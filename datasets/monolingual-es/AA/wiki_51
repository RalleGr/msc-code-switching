<doc id="9718" url="https://es.wikipedia.org/wiki?curid=9718" title="Información">
Información

Información es el nombre por el que se conoce un conjunto organizado de datos procesados que constituyen un mensaje que cambia el estado de conocimiento del sujeto o sistema que recibe dicho mensaje. Existen diversos enfoques para el estudio de la información:

Los datos sensoriales una vez percibidos y procesados constituyen una información que cambia el estado de conocimiento, eso permite a los individuos o sistemas que poseen dicho estado nuevo de conocimiento tomar decisiones pertinentes acordes a dicho conocimiento.

Desde el punto de vista de la ciencia de la computación, la información es un conocimiento explícito extraído por seres vivos o sistemas expertos como resultado de interacción con el entorno o percepciones sensibles del mismo entorno. En principio la información, a diferencia de los datos o las percepciones sensibles, tienen estructura útil que modificará las sucesivas interacciones del que posee dicha información con su entorno.

La palabra «información» deriva del sustantivo latino "informatio(-nis)" (del verbo "informare", con el significado de «dar forma a la mente», «disciplinar», «instruir», «enseñar»). Ya en latín la palabra "informationis" era usada para indicar un «concepto» o una «idea», pero no está claro si tal palabra pudiera haber influido en el desarrollo moderno de la palabra «información».

Por otra parte la palabra griega correspondiente era "μορφή" ("morfè", de la que por metatesis surgió la palabra latina «forma»), o si no "εἶδος" ("éidos", de la cual deriva la latina «idea»), esto es: «idea», «concepto» o «forma», «imagen»; la segunda palabra fue notoriamente usada técnicamente en el ámbito filosófico por Platón y Aristóteles para indicar la identidad ideal o esencia de algo (véase Teoría de las ideas). "Eidos" se puede también asociar a «pensamiento», «aserción» o «concepto».

En las sociedades humanas y en parte en algunas sociedades animales, la información tiene un impacto en las relaciones entre diferentes individuos. En una sociedad la conducta de cada individuo frente a algunos otros individuos se puede ver alterada en función de qué información disponible posee el primer individuo. Por esa razón, el estudio social de la información se refiere a los aspectos relacionados con la variación de la conducta en posesión de diferentes informaciones.

Para Gilles Deleuze, la información social es un sistema de control, en tanto que es la propagación de consignas que deberíamos de creer o hacer que creemos. En tal sentido la información es un conjunto organizado de datos capaz de cambiar el estado de conocimiento en el sentido de las consignas transmitidas.

En general la información tiene una estructura interna y puede ser calificada según varias características:

La historia de la información está asociada a su producción, tratamiento y transmisión. Una cronología de esa historia detallada puede ser:

Se considera que la generación y/u obtención de información persigue estos objetivos:


En relación con el tercer punto, la información como vía para llegar al conocimiento, debe ser elaborada para hacerla utilizable o disponible (este proceso empírico se llama Documentación y tiene métodos y herramientas propios), pero también es imposible que la información por sí sola dote al individuo de más conocimiento, es él quien valora lo significativo de la información, la organiza y la convierte en conocimiento. El dato, por así llamarlo, es en sí un «prefijo» de la información, es decir, es un elemento previo necesario para poder obtener la información.

Una noticia es el relato o redacción de un texto informativo que se quiere dar a conocer con sus propias reglas de construcción (enunciación) que se refiere a un hecho novedoso o atípico -o la relación entre hechos novedosos y/o atípicos-, ocurrido dentro de una comunidad o determinado ámbito específico, que hace que merezca su divulgación.

La noticia es un hecho periodístico, equivalente a lo que implica para la historia un acontecimiento. Dentro del ámbito de algunos medios de comunicación, es un género periodístico en el que la noticia es un «recorte de la realidad» sobre un hecho de actualidad que merece ser informado por algún tipo de criterio de relevancia social. El contenido de una noticia debe responder a la mayoría de las preguntas que se conocen como las «6W-H», de la escuela de periodismo norteamericana:


La información es un fenómeno que proporciona significado o sentido a las cosas. En sentido general, la información es un conjunto organizado de datos procesados, que constituyen un mensaje sobre un determinado ente o fenómeno. Los datos se perciben, se integran y generan la información necesaria para producir el conocimiento que es el que finalmente permite tomar decisiones para realizar las acciones cotidianas que aseguran la existencia. La sabiduría consiste en determinar correctamente cuándo, cómo, dónde y con qué objetivo emplear el conocimiento adquirido.

La información también indica mediante códigos y conjuntos de datos, los modelos del pensamiento humano. La información por lo tanto, procesa y genera el conocimiento humano. De esta manera, si por ejemplo organizamos datos sobre un país, tales como: número de habitantes, densidad de población, nombre del presidente, etc. y escribimos por ejemplo, el capítulo de un libro, podemos decir que ese capítulo constituye información sobre ese país.

El control y la manipulación es uno de los medios más poderosos de los gobiernos para promover el acatamiento de sus políticas. Así, los estados totalitarios y autoritarios buscan el monopolio de la información para promover el acatamiento de las políticas.
La información tiene por objetivo dar a conocer los hechos de manera efectiva e imparcial, mientras que la propaganda busca ganar adeptos para lograr un objetivo, sin importarle la veracidad de los hechos. Así la propaganda compite con el derecho como instrumento de poder.

El enfoque de la teoría de la información analiza la estructura matemática y estadística de los mensajes, con independencia de su significado u otros aspectos semánticos. Los aspectos en los que se interesa la teoría de la información son la capacidad de transmisión de los canales, la compresión de datos o la detección y corrección de errores.

Una forma de caracterizar nuestro estado de conocimiento del mundo, es a través de las probabilidades. Si sabemos que en el futuro pueden suceder "n" cosas diferentes formula_1, cada una con probabilidad formula_2 ese conjunto de probabilidades constituyen nuestro conocimiento del mundo, una información debería reducir nuestra incertidumbre, variando las probabilidades a formula_3. Si el segundo estado tiene menos incertidumbre es porque algunas cosas se han hecho más probables frente a otras alternativas que se han hecho menos probables.

Una forma de «medir la información» asociada a un mensaje o hecho observado es calcular como algunas probabilidades han crecido y otras han decrecido. Una medida conveniente de calcular la «concentración» de la certeza en algunas alternativas es la entropía estadística:

Un ejemplo lingüístico ilustra bien este caso. Supongamos que nos proponen adivinar la segunda letra de una palabra del español. y nos dicen que en la segunda posición podría aparecer cualquier letra del alfabeto. Por tanto la incertidumbre inicial se obtiene calculando la y calculando:

Sin embargo, si nos dan como pista que «la primera letra es una Z», entonces en segunda posición sólo puede aparecer A, O, U (aunque existen un puñado de casos excepcionales de "E" e "I") y por tanto con esa información se reduce mucha la incertidumbre:

La información cuantificada de la pista «la primera letra es una Z» resulta ser:

Las unidades de información resultan ser bits puesto que se han empleado logaritmos de base 2. Si la pista hubiera sido «la primera letra es una M», la segunda letra sólo podría ser A, E, I, O, U que es un conjunto más amplio y en este caso formula_4 y en este caso, la pista lleva menos información porque reduce menos la incertidumbre, el resultado en este caso es repitiendo los pasos anteriores de unos 2,50 bits.

La cantidad de información y el conocimiento desarrollado, aparentemente es enorme y tiene una metodología de recuperación, que eventualmente es infinita o total en un número muy amplio de soportes y sitios y el modelo sistémico de recuperación debe maximizar la búsqueda para asegurar su captura lo más completa posible dentro del entorno de este sistema complejo. En el caso de búsquedas en Internet y usando dos o más descriptores, los resultados numéricos que dan los motores de búsqueda, que contengan los dos o más términos juntos o muy próximos, ya es una medida de la cantidad de información conseguida y que es en expresión matemática el "ln" o logaritmo natural de la suma de las interacciones validadas. Valores de 2 o 3 serán óptimos.

En física existe una íntima conexión entre entropía e información:

Léon Brillouin publicó en 1959 "Science et théorie de l'information" (versión en inglés editada por vez primera en 1962) donde son examinadas las relaciones entre estas dos disciplinas. Adopta particularmente un punto de vista de físico y hace el lazo entra la entropía informacional de Shannon y la entropía estadística de Boltzmann en donde se arriesga que la información (y con la misma el lenguaje) es un factor neguentrópico es decir por el cual se puede anular la entropía.

Para empezar, es necesario entender que, en el caso de la información analógica, los datos se traducen en forma de impulsos eléctricos, de manera que el flujo de información es constante y únicamente va variando de amplitud.

En cambio, la información digital es aquella cuya codificación se reduce a dos valores: 0 y 1; es decir, se traduce al sistema binario. Todo este tipo de información, independientemente de que estemos hablando de música, imágenes, textos, etc, tiene la misma naturaleza. 

Veamos las diferencias y por tanto, desventajas y ventajas que tiene un sistema u otro.

En primer lugar, la información digital, al estar codificada con el mismo código independientemente del formato, puede ser transmitida o copiada infinitas veces sin padecer daños o perdidas, debido a que simplemente debe ser reconstruida por el software oportuno —se trata únicamente de números—. En cambio, la información analógica no es tan precisa; esta pierde calidad en ser copiada. 

Otra de las desventajas que tiene la información analógica es que ocupa un espacio físico, es estática y su conservación puede ser difícil con el paso de los años. Sin embargo, la información digital no ocupa ningún lugar físico, sino que se encuentra dentro de dispositivos capaces de leerla como un ordenador. Además, son datos que tienen la capacidad de fluir con gran facilidad y su conservación es completamente posible durante muchísimo tiempo; no se deteriora —aunque el medio que la almacena sí que puede echarse a perder—. De hecho, si no son destruidos a conciencia, perduran para siempre.

Otra diferencia entre los dos tipos de información está en la facilidad o dificultad para compartirla. Es decir, la información analógica es más difícil de difundir. La ventaja de esto es que es mucho más fácil de controlar. En cambio, la digital, al ser tan fácil de compartir hoy en día —vía Internet, etc—, también es muy difícil impedir que circule una vez ha empezado a hacerlo; nuestro control sobre esta es mucho menor.

Además, la información digital se procesa de manera muy rápida por dispositivos como ordenadores o discos duros de manera que esto facilita el tratamiento de la información. Esta información, debido a la naturaleza común de todos los formatos, puede combinar texto, imagen, vídeo, etc., en un mismo archivo.





</doc>
<doc id="9719" url="https://es.wikipedia.org/wiki?curid=9719" title="Ingeniería de sistemas">
Ingeniería de sistemas

La ingeniería de sistemas es una rama interdisciplinaria de la ingeniería que permite estudiar y comprender la realidad, con el propósito de implementar u optimizar sistemas complejos. Puede también verse como la aplicación tecnológica de la teoría de sistemas a los esfuerzos de la ingeniería, adoptando en todo este trabajo el paradigma sistémico. La ingeniería de sistemas integra otras disciplinas y grupos de especialidad en un esfuerzo de equipo, formando un proceso de desarrollo centrado.

La Ingeniería de Sistemas tiene, como campo de estudio, cualquier sistema existente. Por ejemplo, la ingeniería de sistemas, puede estudiar el sistema digestivo o el sistema inmunológico humano, o quizá, el sistema tributario de un país específico. En este sentido si bien en algunos países se asocia ingeniería de sistemas como únicamente asociada a los sistemas informáticos, ello es incorrecto, ya que los sistemas informáticos son una pequeña parte de un enorme abanico de tipos y clases de sistemas.

La ingeniería de sistemas es la aplicación de las ciencias matemáticas y físicas para desarrollar sistemas que utilicen económicamente los materiales y fuerzas de la naturaleza para el beneficio de la humanidad.

Una de las principales diferencias de la ingeniería de sistemas respecto a otras disciplinas de ingeniería tradicionales, consiste en que la ingeniería de sistemas no construye productos tangibles. Mientras que los ingenieros civiles podrían diseñar edificios o puentes, los ingenieros electrónicos podrían diseñar circuitos, los ingenieros de sistemas tratan con sistemas abstractos con ayuda de las metodologías de la ciencia de sistemas, y confían además en otras disciplinas para diseñar y entregar los productos tangibles que son la realización de esos sistemas.

El origen del término ingeniería de sistemas se remonta a los Bell Telephone Laboratories en la década de 1940. La necesidad de identificar y manipular las propiedades de un sistema como un todo, que en proyectos de ingeniería complejos puede diferir enormemente de la suma de las propiedades de las partes, motivó a varias industrias, especialmente aquellas que desarrollaban sistemas para el Ejército de los Estados Unidos, a aplicar la disciplina.

Cuando ya no era posible confiar en la evolución del diseño para mejorar un sistema y las herramientas existentes no eran suficientes para satisfacer las crecientes demandas, se empezaron a desarrollar nuevos métodos que abordaban la complejidad directamente. La evolución continua de la ingeniería de sistemas comprende el desarrollo y la identificación de nuevos métodos y técnicas de modelado. Estos métodos ayudan a una mejor comprensión y al control del diseño y desarrollo de los sistemas de ingeniería a medida que se vuelven más complejos. En estos tiempos se desarrollaron herramientas populares que a menudo se usan en el contexto de la ingeniería de sistemas, incluidas USL, UML, QFD e IDEF0.

La ingeniería de Sistemas comenzó a desarrollarse en la segunda parte del siglo XX con el veloz avance de la ciencia de sistemas. Las empresas comenzaron a tener una creciente aceptación de que dicha ingeniería, podía gestionar el comportamiento impredecible y la aparición de características imprevistas de los equipos y proyectos con niveles de complejidad cada vez mayores (propiedades emergentes). Las decisiones tomadas al comienzo de un proyecto, cuyas consecuencias pueden no haber sido entendidas claramente, tienen una enorme implicación más adelante en la vida de un sistema. Un ingeniero de sistemas debe explorar estas cuestiones y tomar decisiones críticas.

Si bien inicialmente la ingeniería de sistemas solo era considerada un método, recientemente se le ha comenzado a considerar una disciplina dentro de la ingeniería. El objetivo de la enseñanza de la ingeniería de sistemas es formalizar diversas metodologías y de esta forma identificar métodos novedosos y oportunidades de investigación de forma similar a lo que se hace en otras ramas de la ingeniería. Como metodología, la ingeniería de sistemas posee una fuerte impronta holística e interdisciplinaria.

El alcance tradicional de la ingeniería comprende la concepción, diseño, desarrollo, producción y operación de los sistemas físicos. La ingeniería de sistemas, tal como se la concibió inicialmente, se encuentra dentro de dicho alcance. La "ingeniería de sistemas", en este sentido, se refiere al conjunto de conceptos distintivos, metodologías, estructuras organizacionales que han sido desarrolladas para enfrentar los desafíos de desarrollar la ingeniería de sistemas funcionales efectivos de dimensiones y complejidad sin precedentes dentro del tiempo, presupuesto, y otras limitaciones. El programa Apolo es un ejemplo importante de un proyecto de grandes dimensiones y complejidad organizado en torno a un enfoque de ingeniería de sistemas.

El uso del término "ingeniero de sistemas" ha evolucionado con el tiempo para abarcar un concepto más amplio y holístico de "sistemas" y de procesos de ingeniería. Esta evolución de la definición ha sido un tema de constante controversia, y el término continúa aplicándose tanto al alcance más restringido como al más amplio.

La ingeniería de sistemas tradicional se veía como una rama de la ingeniería en el sentido clásico, es decir, se aplicaba únicamente a sistemas físicos, como las naves espaciales y los aviones. Más recientemente, la ingeniería de sistemas ha evolucionado para adquirir un significado más amplio, especialmente cuando los seres humanos son vistos como un componente esencial de un sistema. Checkland, por ejemplo, capta el significado más amplio de la ingeniería de sistemas al afirmar que la "ingeniería" puede leerse en su sentido general: puede diseñar una reunión o un acuerdo político ".

De acuerdo con el alcance más amplio de la ingeniería de sistemas, el "Cuerpo de Conocimiento de Ingeniería de Sistemas" (SEBoK-"Systems Engineering Body of Knowledge") ha definido tres tipos de ingeniería de sistemas: (1) Ingeniería de Sistemas de Producto (PSE) es la ingeniería de sistemas tradicional centrada en el diseño de sistemas físicos que consiste en hardware y software. (2) Enterprise Systems Engineering (ESE) se refiere a la visión de las empresas, es decir, organizaciones o combinaciones de organizaciones, como sistemas. (3) La Ingeniería de Sistemas de Servicio (SSE) tiene que ver con la ingeniería de los sistemas de servicio. Checkland define un sistema de servicio como un sistema que se concibe para proveer servicio a otro sistema. La mayoría de los sistemas de infraestructura civil son sistemas de servicio.

La ingeniería de sistemas se enfoca en analizar y precisar las necesidades del cliente y la funcionalidad requerida al principio del ciclo de desarrollo, documentar los requerimientos y luego continuar con la síntesis del diseño y la validación del sistema al considerar el problema en su completitud, el ciclo de vida del sistema. Esto comprender por completo a todas las partes interesadas involucradas en el proyecto. Oliver, afirma que el proceso de ingeniería de sistemas se puede descomponer en


En el modelo de Oliver, el objetivo del Proceso de Gestión es organizar el esfuerzo técnico en el ciclo de vida, mientras que el Proceso Técnico incluye "evaluar la información disponible", "definir medidas de efectividad", "crear un modelo de comportamiento", "crear un modelo de estructura", "realizar un análisis de compromiso", y "crear un plan secuencial de construcción y ensayo".

Dependiendo de su aplicación, aunque hay varios modelos que se utilizan en la industria, todos ellos tienen como objetivo identificar la relación entre las diversas etapas mencionadas anteriormente e incorporar retroalimentación. Ejemplos de tales modelos incluyen el modelo de desarrollo en cascada y el modelo VEE.

El desarrollo del sistema a menudo requiere la contribución de diversas disciplinas técnicas. Al proporcionar una visión de sistemas (holística) del desarrollo, la ingeniería de sistemas ayuda a moldear a todos los contribuyentes técnicos en un esfuerzo unificado de equipo, formando un proceso de desarrollo estructurado que comprende desde el concepto hasta la producción y operación y, en algunos casos, hasta la terminación y eliminación. En una adquisición, la disciplina integradora combina contribuciones y equilibra las decisiones que compiten afectando el costo, cronograma y eficiencia, al tiempo que mantiene un nivel aceptable de riesgo que abarca todo el ciclo de vida del artículo.

Esta perspectiva a menudo se replica en los programas educativos, ya que los cursos de ingeniería de sistemas son impartidos por profesores de otros departamentos de ingeniería, lo que ayuda a crear un entorno interdisciplinario.

La necesidad de la ingeniería de sistemas surgió con el aumento de la complejidad de los sistemas y proyectos, a su vez aumentando exponencialmente la posibilidad de problemas entre diversos componentes y, por lo tanto, la falta de fiabilidad del diseño. Al hablar en este contexto, la complejidad incorpora no solo los sistemas de ingeniería, sino también la organización lógica humana de los datos. Al mismo tiempo, un sistema puede volverse más complejo debido a un aumento en el tamaño así como a un aumento en la cantidad de datos, variables o la cantidad de campos que están involucrados en el diseño. La Estación Espacial Internacional es un ejemplo de un sistema con tales características.

El desarrollo de algoritmos de control más inteligentes, el diseño de microprocesadores, y el análisis de sistemas del medio ambiente también caen dentro del ámbito de la ingeniería de sistemas. La ingeniería de sistemas promueve el uso de herramientas y métodos para comprender y gestionar mejor la complejidad de los sistemas. Algunos ejemplos de estas herramientas son:

Las herramientas de las que se sirve la ingeniería de sistemas son estrategias, procedimientos, y técnicas que ayudan a llevar a cabo la ingeniería de sistemas que requiere un proyecto o producto. El objetivo de estas herramientas abarca un amplio espectro, que comprende gestión de bases de datos, navegación de sistemas de información en forma gráfica, simulación, y razonamiento, para documentar producción, procesos neutrales de exportación /importación entre otros.

Existen numerosas definiciones de que constituye un sistema en el ámbito de la ingeniería de sistemas. Alguna definiciones enunciadas por organismos relevantes son:


Muchos de los campos relacionados podrían ser considerados con estrechas vinculaciones a la ingeniería de sistemas. Muchas de estas áreas han contribuido al desarrollo de la ingeniería de sistemas como área independiente.

Un sistema de información o (SI) es un conjunto de elementos que interactúan entre sí con el fin de apoyar las actividades de una empresa o negocio. No siempre un Sistema de Información debe estar automatizado (en cuyo caso se trataría de un sistema informático), y es válido hablar de Sistemas de Información Manuales. Normalmente se desarrollan siguiendo Metodologías de Desarrollo de Sistemas de Información.

El equipo computacional: el hardware necesario para que el sistema de información pueda operar.
El recurso humano que interactúa con el Sistema de Información, el cual está formado por las personas que utilizan el sistema.

Un sistema de información realiza cuatro actividades básicas: entrada, almacenamiento, procesamiento y salida de información.

Es la actualización de datos reales y específicos para la agilización de operaciones en una empresa.

La investigación de operaciones o (IO) se enseña a veces en los departamentos de ingeniería industrial o de matemática aplicada, pero las herramientas de la IO son enseñadas en un curso de estudio en Ingeniería de Sistemas. La IO trata de la optimización de un proceso arbitrario bajo múltiples restricciones.Se presentan las ideas fundamentales en las que se basa el enfoque de sistemas, los tipos de problemas de sistemas y las metodologías más adecuadas para abordarlos.

Los sistemas cognitivos abarcan sistemas naturales o artificiales de procesamiento de la información capaces de percepción, aprendizaje, razonamiento, comunicación, actuación y comportamiento adaptativo.

La ingeniería de sistemas cognitivos es una rama de la ingeniería de sistemas que trata los entes cognitivos, sean humanos o no, como un tipo de sistemas capaces de tratar información y de utilizar recursos cognitivos como la percepción, la memoria o el procesamiento de información. Depende de la aplicación directa de la experiencia y la investigación tanto en psicología cognitiva como en ingeniería de sistemas. La ingeniería de sistemas cognitivos se enfoca en cómo los entes cognitivos interactúan con el entorno. La ingeniería de sistemas trabaja en la intersección de:


Algunas veces designados como ingeniería humana o ingeniería de factores humanos, esta rama además estudia la ergonomía en diseño de sistemas. Sin embargo, la ingeniería humana suele tratarse como otra especialidad de la ingeniería que el ingeniero de sistemas debe integrar.

Habitualmente, los avances en ingeniería de sistemas cognitivos se desarrollan en los departamentos y áreas de informática, donde se estudian profundamente e integran la inteligencia artificial, la ingeniería del conocimiento y el desarrollo de interfaces hombre-máquina (diseños de usabilidad) de la ciencia

El Ingeniero de sistemas habitualmente aprende a programar, para dirigir a programadores y al momento de la creación de un programa debe saber y tener en cuenta los métodos básicos como tal, por eso es importante que aprenda a programar pero su función realmente es el diseño y planeación, y todo lo referente al sistema o redes, su mantenimiento y efectividad, respuesta y tecnología.




</doc>
<doc id="9720" url="https://es.wikipedia.org/wiki?curid=9720" title="Organización">
Organización

Las organizaciones son estructuras administrativas y sistemas administrativos creadas para lograr metas u objetivos con el apoyo de las propias personas, o con apoyo del talento humano o de otras características similares. Son entidades sociales que permiten la asociación de personas que interactúan entre sí para contribuir mediante sus experiencias y relaciones al logro de objetivos y metas determinadas.

Las organizaciones son el objeto de estudio de la ciencia de la Administración, a su vez de otras disciplinas tales como la Comunicación, la Sociología, la Economía y la Psicología.

Una organización, es un grupo social compuesto por personas naturales, tareas y administraciones que forman una estructura sistemática de relaciones de interacción, tendientes a producir bienes, servicios o normativas para satisfacer las necesidades de una comunidad dentro de un entorno, y así poder lograr el propósito distintivo que es su misión. Es un sistema de actividades conscientemente coordinadas formado por dos o más personas; la cooperación entre ellas es esencial para la existencia de la organización. Una organización sólo existe cuando hay personas capaces de comunicarse y que están dispuestas a actuar conjuntamente para lograr un objetivo común. Es un conjunto de cargos con reglas y normas de comportamiento que han de respetar todos sus miembros, y así generar el medio que permite la acción de una empresa. La organización es el acto de disponer y coordinar los recursos disponibles (materiales, humanos y financieros). Funciona mediante normas y bases de datos que han sido dispuestas para estos propósitos.

Existen varias escuelas filosóficas que han estudiado la organización como sistema social y como estructura de acción, tales como el estructuralismo y el empirismo. Para desarrollar una teoría de la organización es preciso primero establecer sus leyes o al menos principios teóricos para así continuar elaborando una teoría sobre ellos. Un camino sería clasificar y mostrar las diferentes formas de organizaciones que han sido más destacadas o conocidas a lo largo de la historia, tales como la burocracia como administración o elementos que componen la organización y que igualmente han sido ya muy tratados, tales como el liderazgo formal e informal. Como metodología, esto se llama investigación operativa y en el ámbito de las ciencias sociales es el campo de estudio de la "sociología de la organización". Un nuevo uso está emergiendo en las organizaciones: la gestión del conocimiento. Típicamente, la organización está en todas partes, lo que dificulta su definición independiente o sin involucrarse en una aplicación particular.

Los fundamentos básicos que demuestran la importancia de la organización son:



Todas y cada una de las actividades establecidas en la organización deben relacionarse con los objetivos y propósitos de la empresa.

Fue establecido por Adam Smith hace 200 años. El trabajo se realizará más fácilmente si se subdivide en tareas o actividades claramente relacionadas y delimitadas.

Es necesario establecer centros de autoridad de los que emane la comunicación necesaria para lograr los planes.

A cada grado de responsabilidad conferido, debe corresponder el grado de autoridad necesarios para cumplir dicha responsabilidad.

Establece determinar un centro de autoridad y decisión para cada función.



Dentro de las diferentes maneras de clasificación se encuentran las de tamaño, ya sea por la producción, capital, volumen de ventas y principalmente personal ocupado. La más común es la que se basa en el número de empleados:
Estas pueden ser: nacionales, extranjeras, multinacionales o globalizadas, así como controladoras, franquiciadas o familiares.

Esta clasificación depende del propósito por el que fueron creadas además del origen de las aportaciones a su capital:

Dependiendo de su influencia económica:
Produce bienes mediante la extracción o la transformación de materias primas. De estas se puede hacer otra clasificación en:
Son las empresas que actúan como intermediarias entre el productor y el consumidor. Su objetivo es la compra-venta de productos ya fabricados y su distribución. Estas se clasifican en:
Comisionistas:" Venden productos que los fabricantes les dan, y por eso reciben alguna comisión.
La finalidad de las empresas de servicios es brindar un servicio con o sin fines de lucro. Por ejemplo: salud, transporte, educación.

Tanto los valores como la filosofía están relacionados con la cultura de la organización, ya que, dependiendo de esto, pueden ser lucrativas o no lucrativas; por lo tanto, se clasifican en:


Esta clasificación depende del nivel de tecnificación con el que cuente cada empresa.




En el caso de la integración, nos referimos a si son organizaciones con mucha presencia en la comunidad, que interaccionan con otras organizaciones, muy conocidas, que están muy vinculadas socialmente, más allá de sus propias actividades. 

Son los necesarios para desarrollar sus actividades al llevar a cabo su fin, difieren según sus actividades.

Recursos:




</doc>
<doc id="9721" url="https://es.wikipedia.org/wiki?curid=9721" title="Teseo">
Teseo

En la mitología griega, Teseo (en griego antiguo, Θησεύς: «el que funda») es un , hijo de Etra y Egeo, aunque según otra tradición su padre fue Poseidón, el dios del mar.

Teseo fue un héroe fundador de Atenas, como Perseo o Cadmo lo fueron para otras ciudades-estado de la antigüedad. Sus peripecias se narran en la "Vida de Teseo", de Plutarco, basada en otros escritos más antiguos. Su mito se recrea también en obras posteriores. Por ejemplo, es uno de los personajes principales de "El sueño de una noche de verano" y "Los dos nobles caballeros, de William Shakespeare.

El rey Egeo, que no había tenido descendencia con su esposa, consultó al oráculo de Delfos, que le respondió: 

No entendió el oráculo, pero Piteo, rey de Trecén y padre de Etra, sí lo entendió. El oráculo había querido decir que si llegaba a Atenas sin haber hecho el amor, la primera mujer con la que yaciera tendría un heredero suyo. Como Piteo deseaba que su hija diera a luz al heredero del trono ateniense, emborrachó a Egeo, y así consiguió que fecundara a Etra.

En la noche en la que quedó embarazada, se creía que también Poseidón la había poseído. El dios la sorprendió en la isla de Esferia, a donde había ido, debido a un sueño, con el propósito de ofrecer libaciones sobre la tumba de Esfero. Etra dedicó por ello en la isla un templo a Atenea Apaturia y llamó a la isla Hiera en lugar de Esferia, introduciendo también entre las doncellas de Trecén la costumbre de dedicar sus zónulas (fajas) a Atenea Apaturia en el día de su matrimonio. Según Plutarco, Piteo difundió esta versión sólo para que Teseo fuese considerado hijo de Poseidón, que era muy reverenciado en su tierra. Egeo regresó a Atenas y Etra crio a su hijo en Trecén. 

Tras la concepción de Teseo, Egeo, por temor a los Palántidas, sus sobrinos, que querían el trono, decidió que su hijo no pasara la niñez con él y escondió su espada y sus sandalias bajo una roca que el niño no debía mover hasta que fuera lo suficientemente fuerte. Así que la infancia de Teseo transcurrió en compañía de su madre y su abuelo en la ciudad de Trecén. Cuando cumplió los dieciséis años su madre le reveló el secreto de su paternidad y llegado a esta edad, Teseo pudo levantar la piedra, calzarse las sandalias y envainar la espada de su padre e iniciar su viaje a Atenas para ser reconocido como hijo del rey.

Teseo, que desde muy joven había destacado por su fuerza y su valentía, decidió dirigirse a Atenas en solitario para conocer a su progenitor sin temer los peligros que podía entrañar el viaje. Al contrario, deseaba emular las hazañas de su admirado Heracles, a quien le unirá una buena amistad.
El primero en experimentar su valor fue el gigante Perifetes hijo de Hefesto, un salteador de caminos, que, a pesar de que era cojo, dominaba a la perfección una enorme maza de bronce con la que mataba a los viajeros: la misma maza que tan útil le sería a Teseo en el futuro, pues se quedó con ella tras darle muerte.

Otro de los gigantes bandidos a los que debió enfrentarse en su trayecto fue Sinis, el doblador de pinos, que tenía una manera peculiar de deshacerse de sus presas: doblaba dos pinos próximos, ataba las copas entre sí y un brazo de su víctima a cada una de ellas. Luego soltaba los árboles que, al enderezarse violentamente, desgarraban el cuerpo del desgraciado. Teseo, después de acabar con Sinis de la misma manera en que él asesinaba a sus víctimas, mantuvo relaciones con su hija Perigune de quien tuvo un hijo: Melanipo.

Después le tocó enfrentarse a Esciro, hijo de Pélope y descendiente de Tántalo, quien obligaba a los viajeros a lavarle los pies en el mar. De una brutal patada, los arrojaba al mar donde una enorme tortuga al servicio de Hades los devoraba. Teseo se negó y cogiéndolo por los pies lo lanzó al mar.

Cerca del pueblo de Eleusis, un bandido llamado Cerción retaba a los viajeros a luchar con él en un duelo desigual y nadie lo vencía. Solo Teseo lo hizo, levantándolo y arrojándolo mortalmente contra el suelo.

No lejos de ahí vivía otro gigante, Procustes, un posadero bandido que tenía el hábito de ofrecer a los viajeros un lecho especial. Primero los seducía, los ataba a la cama y amordazaba; en ella daba entonces comienzo a una atroz tortura. A los altos los metía en una cama pequeña y les cortaba los piernas y cabeza que sobraban. A los más pequeños los metía en una cama grande y les estiraba brazos y piernas con cuerdas y a martillazos. Teseo lo mató de la misma forma en que él mató a sus víctimas: lo sedujo con juegos, lo ató y amordazó en la cama más pequeña, dada su altura. Luego lo torturó con el martillo, le cortó los pies y finalmente la cabeza.
También mató a la Cerda de Cromio, que era una fiera hija de Tifón y Equidna. 

Teseo continuó su viaje y llegó a Atenas, pero se encontró con un inconveniente: su padre se había casado con Medea, la que había sido esposa de Jasón. De esta unión había nacido un hijo al que habían llamado Medo.

Ante esta situación inesperada, Teseo decidió esperar un poco antes de darse a conocer. Pero Medea, que era hechicera, lo reconoció y vio en él un peligro para que su hijo accediera al trono de Atenas. Así que trazó un plan. 
El joven había acudido al palacio de incógnito precisamente para evitar los ardides de su madrastra, lo que aprovechó esta para convencer a Egeo de que el recién llegado era un traidor. El rey se dispuso entonces a deshacerse de él ordenándole luchar contra el toro de Maratón.

Pero el toro fue derrotado y Teseo fue invitado a un banquete en el palacio para celebrar la victoria. Una vez allí Egeo puso veneno que le había dado Medea en la copa del muchacho pero la casualidad salvó su vida. Para cortar la carne, Teseo sacó la espada que le había dado su madre. Entonces Egeo reconoció el arma, comprendió lo que ocurría y arrebató a su hijo la copa de los labios. Habiendo fracasado en su empresa, Medea decidió huir con su hijo o fue expulsada por su esposo.

Teseo fue reconocido oficialmente como hijo y sucesor del rey, lo que provocó la rebelión de los hijos de Palante, hermano de Egeo, los Palántidas, ya que uno de ellos habría sido el sucesor en caso de que Egeo no hubiera tenido descendencia. Teseo, haciendo alarde de su astucia militar, consiguió acorralar a sus adversarios y dar muerte a gran parte de ellos, y los restantes se dieron a la fuga. Teseo fue aclamado por todos los atenienses y reconocido como futuro rey.

Atenas debía enviar un tributo al rey Minos de Creta, que consistía en el sacrificio de siete doncellas y siete jóvenes, que serían devorados por el monstruo Minotauro, y que fue una condición impuesta tras la expedición militar de Minos contra Atenas para vengar la muerte de Androgeo. 

Teseo se presentó voluntariamente en el tercer envío ante su padre para que le permitiera ser parte de la ofrenda y lo dejara acompañar a las víctimas para poder enfrentarse al Minotauro. 

Las naves en las que iban a viajar las personas ofrendadas llevaban velas negras como señal de luto, pero el rey pidió a Teseo que si regresaba vencedor, no olvidase cambiarlas por velas blancas, para que supiera, aún antes de que llegase a puerto, que estaba vivo. Teseo se lo prometió. 

Durante la travesía, Minos, que iba también en la expedición, se enamoró de una joven llamada Eribea o Peribea, según las fuentes. Minos quiso unirse a ella por la fuerza y Teseo se le opuso. En la consiguiente disputa, Minos indicó a Teseo su filiación divina, y obtuvo de su padre Zeus truenos y relámpagos. Teseo replicó que él también tenía filiación divina, puesto que en realidad era hijo de Poseidón. Para probar esta filiación, Teseo tuvo que tirarse al agua y encontrar un anillo de oro que el rey Minos había arrojado al mar. Teseo, en el mar, fue conducido por delfines a presencia de Anfítrite, esposa de Poseidón, que le dio el anillo y una corona.

Al llegar a Creta, la princesa Ariadna se enamoró de él y propuso a Teseo ayudarle a derrotar a su hermano (el Minotauro) a cambio de que se la llevara con él de vuelta a Atenas y la convirtiera en su esposa. Teseo aceptó. 

La ayuda de Ariadna consistió en dar a Teseo un ovillo de hilo que este ató por uno de los extremos a la puerta del laberinto. Otra versión indica que la ayuda de Ariadna consistió en una corona que emitía un resplandor y que le había dado Dioniso como regalo de boda o bien que podría ser la misma corona que le había regalado Anfítrite durante el viaje a Creta.

Así Teseo entró en el laberinto hasta encontrarse con el Minotauro, al que dio muerte a puñetazos o atravesándolo con una espada. A continuación recogió el hilo y así pudo salir del laberinto e inmediatamente, acompañado por el resto de atenienses y por Ariadna, embarcó de vuelta a Atenas, tras hundir los barcos cretenses para impedir una posible persecución.

Durante el viaje de vuelta, Teseo decidió desembarcar en la isla de Naxos o en otra isla llamada Día y de allí volvió a partir sin la presencia de Ariadna. El motivo de este abandono es controvertido: algunas versiones señalan que Teseo la abandonó por su propia voluntad, otros dicen que fue por orden de los dioses para que esta pudiera casarse con Dioniso.

Al divisar la galera desde el puerto de El Pireo en Atenas, el vio las velas negras puesto que Teseo había olvidado cambiarlas por velas blancas y, creyendo que su hijo había muerto, se suicidó lanzándose al mar, que a partir de entonces recibió el nombre de mar Egeo.

Teseo, a partir de entonces, heredó el trono de Atenas y años después se casaría con una hermana de Ariadna llamada Fedra.

Después de que Heracles obtuviese en uno de sus doce trabajos el cinturón de la amazona Hipólita, Teseo, que participó en la expedición, secuestró a una amazona llamada Antíope, o bien Melanipa o bien Hipólita. Las amazonas atacaron entonces Atenas para rescatar a la raptada, pero fueron derrotadas por los atenienses, muriendo en algunas versiones la amazona raptada durante el ataque.

Teseo se casó con Antíope, con Melanipa o con Hipólita y tuvo un hijo llamado Hipólito. Pero después terminaría casándose con Fedra, tras haber abandonado a su anterior esposa. En la versión en la que Teseo está casado con Hipólita y la abandona, esta intentó vengarse llevando a las amazonas a la boda de Teseo y Fedra con la intención de matar a todos, aunque fracasó al ser asesinada por los invitados de Teseo.

Hipólito, el hijo que Teseo había tenido con la amazona, se distinguía por su pasión por la caza y las artes violentas. Veneraba a Artemisa, diosa virgen de la caza, y en cambio detestaba a la diosa del amor Afrodita. La diosa, ofendida por el desprecio del chico, suscitó una terrible pasión por el mismo en el corazón de Fedra, que se había convertido en esposa de Teseo y por lo tanto madrastra de Hipólito. Estando Teseo ausente, Fedra se ofreció al casto joven, pero este la despreció. La mujer, despechada, se ahorcó dejando una nota inculpatoria en la que decía que Hipólito había tratado de violarla. Al regresar Teseo y ver la falsa acusación contra su hijo, creyó en ella y clamó venganza a Poseidón, que envió a Hipólito un toro que brotó del mar mientras este cabalgaba en su carro; el carro volcó e Hipólito fue arrastrado por sus propios caballos.

En algunas versiones fue en este momento cuando Fedra se suicidó, al ver el mal que había causado.

Pirítoo había oído hablar de la fama de Teseo y para comprobarla robó ganado que pertenecía a este último. Cuando Teseo lo persiguió, Pirítoo estaba dispuesto a enfrentarse a él pero antes de ello surgió entre ellos una admiración mutua que hizo que se juraran amistad eterna. 

Teseo y Pirítoo fueron amigos inseparables, y participaron juntos en hazañas bélicas de su época: participaron en la expedición de los Argonautas para conquistar el Vellocino de oro y tomaron parte en la caza del jabalí de Calidón, también estuvieron en la lucha de los lápitas contra los centauros, que tuvo lugar en la boda de Pirítoo, cuando los ebrios centauros decidieron raptar a las mujeres.

Decidieron casarse cada uno con una hija de Zeus: Teseo con Helena, que aún era una niña, y Pirítoo con Perséfone. Primero raptaron a Helena y la dejaron bajo la custodia de Etra, y luego decidieron bajar al inframundo en busca de Perséfone. Pero el dios Hades les tendió una trampa: les invitó a un banquete y una vez que los tuvo sentados a la mesa, los dejó adheridos a los asientos. Cuando Heracles, en su duodécimo trabajo, fue en busca de Cerbero, estando en el Hades, los encontró encadenados. Al ver a Heracles, tendieron sus manos hacia él, como si fuesen a ser resucitados gracias a la fuerza de este. A Teseo, agarrándolo de la mano, logró alzarlo, pero tuvo que abandonar a Pirítoo ya que, al intentar levantarlo, tembló la tierra, por lo que este se quedó para siempre en el inframundo.

Mientras Teseo estaba en el Hades, los Dioscuros, hermanos de Helena, liberaron a su hermana, se llevaron a Etra, la madre de Teseo, como esclava, hicieron huir a Demofonte y Acamante, los hijos que Teseo había tenido con Fedra, y pusieron en el trono de Atenas a Menesteo.

Después de ser rescatado por Heracles del inframundo, volvió a Atenas, pero fue expulsado de allí por Menesteo y decidió establecerse en Esciro, donde además tenía posesiones. 

Los habitantes de Esciro lo recibieron aclamándolo, por lo cual Licomedes, rey de la isla, decidió darle muerte. Para ello, hizo que se despeñara desde lo alto de un precipicio. 

En otras versiones, la muerte de Teseo fue accidental.

Se dice que un oráculo había ordenado en el año 476 a. C. llevar los huesos de Teseo desde la isla a Atenas. Efectivamente los supuestos huesos fueron llevados a Atenas por Cimón y guardados en el Teseion.


















</doc>
<doc id="9723" url="https://es.wikipedia.org/wiki?curid=9723" title="Ciclo de Calvin">
Ciclo de Calvin

El ciclo de Calvin (también conocido como ciclo de Calvin-Benson o ciclo de la fijación del carbono de la fotosíntesis) consiste en una serie de procesos bioquímicos que se realizan en el estroma de los cloroplastos de los organismos fotosinteticos.

Las reacciones del ciclo de Calvin pertenecen a la llamada fase independiente de la luz, que se encarga de fijar el CO, incorporándolo a la materia orgánica del individuo en forma de glucosa mediante la enzima RuBisCo. Cabe destacar que este conjunto de reacciones se denomina erróneamente fase oscura, pues muchas de las enzimas del proceso, entre ellas la RuBisCo, dependen de la activación del sistema ferredoxina-tiorredoxina, que solo se encuentra en su forma activa (la reducida) en presencia de la luz.

El ciclo de Calvin fue descubierto por Melvin Calvin, James Bassham y Andrew Benson de la Universidad de California, Berkeley, mediante el empleo de isótopos radiactivos de carbono-14. Calvin fue galardonado con el Premio Nobel de Química en 1961 «por sus trabajos sobre la asimilación del dióxido de carbono por las plantas».

El ciclo se resume en tres etapas: 


Se utilizan seis moléculas de CO para generar una molécula de glucosa. En estas reacciones cada una de las moléculas de CO es unida a una molécula aceptora, ribulosa-1,5-bifosfato (RuBP), que luego se divide en dos moléculas de 3-fosfoglicerato, siendo catalizada por la enzima rubisco (mediante un proceso de carboxilación sin ATP, utilizando como sustratos CO y agua). Luego, el ATP producido durante las reacciones luminosas de la fotosíntesis cede grupos fosfato a estas moléculas, dando lugar a 1,3-difosfoglicerato; al mismo tiempo el NADPH cede electrones a estas moléculas de tres carbonos, dando lugar a gliceraldehido-3-fosfato. Una parte del gliceraldehido-3-fosfato es utilizado para fabricar el azúcar de 6 carbonos de glucosa, entre otros productos de la fotosíntesis. Otra parte del gliceraldehido-3-fosfato es utilizado en conjunto de una molécula de ATP, para generar el aceptor de CO ribulosa-1,5-bifosfato y comenzar el ciclo de nuevo.

A cada vuelta completa del ciclo, una molécula de dióxido de carbono entra en el ciclo y es reducida, presentando regeneración de una molécula de RuBP.

Seis vueltas del ciclo, con la introducción de seis átomos de carbono, son necesarios para producir un azúcar de seis carbonos, tal como la glucosa. La ecuación general para la producción de una molécula de glucosa es:

<chem>6CO2 + 12NADPH + 12H+ + 18ATP -> C6H12O6 + 12NADP+ + 18ADP + 18Pi + 6H2O</chem>

El producto del ciclo es el gliceraldehído 3-fosfato, la molécula primaria transportada del cloroplasto hacia el citoplasma de la célula. Esta misma triosa fosfato (triosa significa un azúcar de tres carbonos) es formada cuando la molécula de fructosa-1,6-bifosfato es rota en la cuarta etapa de la glucólisis y es inconvertible con otra triosa fosfato, la dihidroxiacetona.

Utilizando la energía proveniente de la hidrólisis de enlaces fosfato, las primeras cuatro etapas de la glucólisis pueden ser revertidas para formar glucosa a partir del gliceraldehído 3-fosfato.

Entre otras funciones, cada 3 vueltas en el ciclo, una molécula de triosa fosfato es regenerada a partir de 3 moléculas de CO. La triosa fosfato puede ser utilizada para la síntesis de almidón.

En algas y en plantas superiores existe un único mecanismo primario de carboxilación que resulta en una síntesis de compuestos de carbono: El Ciclo de Calvin o vía de las pentosas fosfato. Su importancia biológica radica en que es la única ruta para los organismos autótrofos, ya sean fotosintetizadores o quimiosintetizadores, que permite la incorporación de materia inorgánica a los seres vivos.

Los productos del ciclo de Calvin son de vital importancia para la biosfera, ya que las uniones covalentes de los hidratos de carbono generadas por el ciclo representan la energía total que surge a partir de la obtención de la luz por los organismos fotosintéticos. Estos organismos denominados autótrofos liberan la mayor parte de esta energía mediante la glucólisis y la respiración celular, energía que emplean para mantener su propio desarrollo, crecimiento y reproducción. Una gran cantidad de materia vegetal termina siendo consumida por los heterótrofos, que no pueden sintetizar y dependen de los autótrofos para obtener materias primas y fuentes de energía. La glucólisis y la respiración celular en las células de los heterótrofos liberan energía libre de los alimentos para su uso en estos organismos.




</doc>
<doc id="9726" url="https://es.wikipedia.org/wiki?curid=9726" title="Endomitosis">
Endomitosis

Endomitosis es la replicación cromosómica que no va acompañada por división nuclear o citoplásmica. 

Algunas células son naturalmente poliploides, debido a que presentan copias adicionales de su dotación cromosómica completa por haber pasado por rondas extras de duplicación del ADN antes de la división celular (endomitosis).



</doc>
<doc id="9727" url="https://es.wikipedia.org/wiki?curid=9727" title="Euploidía">
Euploidía

Euploidía es el estado celular en el cual la célula tiene uno o más juegos completos de cromosomas (dotaciones monoploides (x)) de su especie; dependiendo de especies, se excluyen los cromosomas sexuales. Los individuos con euploidía pueden presentar o no un número anormal de cromosomas diferente al habitual. Por ejemplo, un humano, tiene 46 cromosomas, que es el doble de 23, su número de cromosomas monoploide (x) (que en el caso de los humanos coincide con el número de cromosomas haploide (n); x = n = 23), por lo tanto es euploide (2x = 2n = 46). Un humano hipotético anormal, pero con juegos enteros de cromosomas, por ejemplo, 69, sería también euploide, en tanto que 69 sería múltiplo entero del número monoploide (3x = 69). No se debe confundir con la aneuploidía, que, en contraposición a la euploidía, se refiere a una alteración en la cantidad de uno de los tipos de cromosomas homólogos.

La euploidía puede ser de dos tipos:

- Monoploidía: la que poseen los organismos haploides (n); significa que solo tienen un único cromosoma de cada tipo.

- Poliploidía: la poseen individuos con varios juegos completos de cromosomas homólogos (triploidías 3n, tetraploidías 4n...). A su vez se divide en autopoliploidía, si las distintas dotaciones pertenecen originariamente a una única especie, o alopoliploidía, si son producto de hibridación de dos o más especies. Es bastante frecuente en plantas, sobre todo en angiospermas(47%), y se utiliza en biotecnología de forma inducida para mejorar los cultivos y la producción agrícola. El ejemplo más conocido de organismo alopoliploide es el trigo actual de consumo, que es alohexaploide, cuyo número de cromosomas es 42 en el estado diploide (2n = 42) y 21 en los gametos, haploides (n). Es un organismo hexaploide (6x), porque tiene 6 copias del juego monoploide (x) de 7 cromosomas (6x = 42) (con más precisión, es un alohexaploide, el prefijo "alo-" es porque originariamente, los juegos monoploides de 7 cromosomas provienen de especies diferentes). Cualquiera de las células de esta especie que tuviera juegos completos de cromosomas con 7, 14, 21, 28, 35 ó 42 cromosomas sería un euploide.

La euploidía y el contenido de ADN de las células eucariotas varía entre especies y entre las células de un mismo organismo.


</doc>
<doc id="9728" url="https://es.wikipedia.org/wiki?curid=9728" title="Esténtor">
Esténtor

En la mitología griega, Esténtor (Griego: Στέντωρ; gen.: Στέντορος) era uno de los heraldos de las tropas aqueas durante la Guerra de Troya.

Es mencionado por Homero en un único pasaje de la Ilíada. En ese verso se cuenta como Hera tomó la forma de Esténtor para alentar a los griegos en la batalla:
Debido a este personaje, se dice de una voz que es estentórea cuando es fuerte y sonora.
Según el escoliasta, Esténtor murió tras ser desafiado por Hermes a un concurso de gritos.

Aristóteles usa el carácter de Esténtor en el Libro 7 de Política, donde se lee: "¿Pues quién habrá de conducir masa tan poderosa, y quizá podrá anunciarle algo, si no tiene la voz de Esténtor?". En el contexto se discute la magnitud de una polis. 

Entre otros que aluden al legendario héroe se puede mencionar, en España, a uno de los dos miembros del grupo de rap manchego BMEM. También el grupo de heavy metal español humorístico Gigatrón dedicó una canción de su disco "Atopeosis 666" a Esténtor.



</doc>
<doc id="9729" url="https://es.wikipedia.org/wiki?curid=9729" title="Paradigma">
Paradigma

El concepto de paradigma es utilizado comúnmente como sinónimo de “ejemplo”, también se puede decir que es hacer referencia en caso de algo que se toma como “modelo". En principio se tenía en cuenta en el campo, tema, ámbito, entre dos personalidades u otros..., gramatical (para definir su uso en un cierto contexto) y se valoraba desde la retórica (para hacer mención a una parábola o fábula). A partir de la década de 1960, los alcances de la noción se ampliaron y paradigma comenzó a ser un término común en el vocabulario científico y en expresiones etimológicas cuando se hacía necesario hablar de modelos de conocimiento aceptados por las comunidades científicas.

El término "paradigma" se origina en la palabra griega
"παράδειγμα" ["parádeigma"] que en griego antiguo significa "modelo" o "ejemplo". A su vez se divide en dos vocablos "παρά" ["pará"] ("junto") y δεῖγμα ["deīgma"] ("ejemplo", "patrón"). Originariamente, significaba patrón, modelo. 

El sentido del concepto "paradigma", del griego antiguo παράδειγμα, "paradeigma" ("modelo", "ejemplo"), deriva de παραδεικνύναι, "paradeiknunai" ("demostrar", "probar", "comparar" ), de παρά-, "para'-" ("junto", "alrededor") y δείκνυμι, "deiknumi" ("señalar", "indicar", "mostrar", "enseñar").

Para Platón, los paradigmas son los modelos divinos a partir de los cuales las cosas terrestres están hechas. A su vez tiene las mismas raíces que "«demostrar»".

Michel Foucault usó los términos epistemológico, discursivo, matesis y taxinomial, para aspectos del paradigma en el sentido original dado por Kuhn.

En lingüística, Ferdinand de Saussure ha usado paradigma para referirse a una clase de elementos con similitudes.

En arquitectura, «modelo» (maqueta) o «plano» de un edificio y también es utilizado por escultores y pintores de manera semejante.

Dentro de la enfermería se emplea también el concepto paradigma aunque con cierta imprecisión, ya que se establece dentro de esta disciplina la existencia de tres paradigmas de enfermería (categorizacion, integración y transformación) y un metaparadigma constituido por cuatro conceptos esenciales para la enfermería (persona, salud, entorno y cuidado).

Este concepto fue de uso específico en la gramática. En 1992 el diccionario "Merriam–Webster's Collegiate Dictionary" definía su uso solamente en tal contexto, o desde la retórica para referirse a una parábola o a una fábula.

En ciencias sociales y en teoría de sistemas, el paradigma es equiparable al concepto de pensamiento de grupo -- o su casi equivalente "" -mentalidad-, como cuerpos de ideas, métodos y asunciones teóricas sostenidos y validados por una persona o grupo de personas, que incluye una serie de comportamientos, actitudes y creencias.

La palabra "paradigma" también se utiliza para indicar un patrón o modelo, un ejemplo fuera de toda duda, un arquetipo. En este sentido se la utiliza frecuentemente en las profesiones del diseño. Los paradigmas de diseño —arquetipos— representan los antecedentes funcionales para las soluciones de diseño.

También se usa en cibernética en donde —en un sentido muy amplio— refiere a un preprograma conceptual para el ordenamiento de unos datos aún más caóticos en términos relativos. 

El concepto es de amplio uso en la vida cotidiana, ya que se refiere a ideas, pensamientos, opiniones, creencias, puntos de vista, percepciones, etcétera, que se asumen como verdaderos o falsos. Incluso, el concepto de paradigma puede referirse, de manera cotidiana, a una creencia u opinión compartida colectivamente. Sin embargo, este uso del concepto puede generar diversos errores e imprecisiones, al presentarse en ámbitos teóricos y científicos, ya que se refiere prácticamente a cualquier idea o creencia que tenga un sujeto o grupo de sujetos. Es recomendable no emplear este concepto en usos de la vida cotidiana, y dejarlo para discusiones de carácter epistemológico. En su lugar, cotidianamente se pueden usar cualesquiera de los conceptos arriba mencionados.

Es necesario acotar el significado del concepto paradigma siempre que este se emplee en la teorización epistemológica, ya que en general su significado contemporáneo dentro de la filosofía de la ciencia o epistemología alude al conjunto de tealizaciones, supuests, reglas, prácticas y teorías que definen una disciplina científica en una época histórica dada.

El filósofo e historiador de la ciencia, Thomas S. Kuhn dio a paradigma su significado contemporáneo cuando lo adoptó para referirse al conjunto de prácticas y saberes que definen una disciplina científica durante un período específico. El mismo Kuhn prefería los términos ejemplar o ciencia normal, que tienen un significado filosófico más exacto. Sin embargo, en su libro "La estructura de las revoluciones científicas" define a un paradigma de la siguiente manera:

El paradigma, de esta manera constituye el desarrollo de lo que Kuhn llama ciencia normal, y como tal se manifiesta a través de los libros de texto propios de una ciencia o disciplina, al presentar las teorías aceptadas por las comunidades científicas de cada disciplina, explicándolas y comparándolas, mostrándolas a través de experimentos y observaciones. El paradigma define los métodos, los problemas que legítimamente debe abordar una disciplina o campo de investigación, para ser legado a generaciones futuras de científicos. De esta forma, el paradigma incluye en el plano de la investigación científica lo siguiente:
En realidad este concepto de paradigma es muy amplio y se puede equiparar al concepto de matriz disciplinar o disciplina, ya que dicho concepto alude a la posesión común por parte de los que practican una disciplina concreta (disciplinar) y matriz porque se compone de los elementos estructurados que dan sentido a las explicaciones científicas. El paradigma es el conjunto de realizaciones de una ciencia y es compartido por los miembros de la comunidad científica. El papel de la comunidad científica de cada disciplina en el desarrollo del paradigma es clave, ya que Kuhn asume que la ciencia es una empresa humana colectiva y como tal las discusiones científicas son producto de la comunicación y la tarea coordinada y conjunta de los científicos que trabajan en el desarrollo de un paradigma o disciplina a través de sus diversas teorías y experimentos. 

El paradigma en un sentido amplio, incluye aspectos ontológicos y epistemológicos fundamentales, que proporcionan el horizonte desde el cual se construyen los diferentes modelos teóricos y teorías de un nivel inferior, presentando las directrices generales de agrupamiento de las diferentes teorías en los campos disciplinares de cada ciencia.

De esta forma, dentro de la ciencia normal, un paradigma incluye el conjunto de experimentos modélicos capaces de ser copiados o emulados; siendo la base para crear un consenso científico. El paradigma aceptado en el consenso científico imperante en una época histórica dada, establece formas de ver e interpretar la realidad, también abre líneas para la creación de propuestas para la investigación futura, las teorías y prácticas derivadas del uso de un método científico y sus aplicaciones metodológicas concretas. 

Un ejemplo de paradigma comúnmente aceptado sería el modelo estándar de la física. Los métodos científicos permitirían a los científicos ortodoxos investigar muchos fenómenos que pueden resultar contradictorios o contrastantes con el modelo estándar. Sin embargo es mucho más difícil obtener consenso para los mismos, en proporción a la divergencia de los principios aceptados del modelo estándar que tales experimentos examinarían. Así, en particular, un experimento para investigar la masa del neutrino o la descomposición de neutrones recibiría más fondos que un experimento que buscara violaciones a la conservación de momentos, o pretendiera estudiar la ingeniería de los viajes en el tiempo.

Kuhn define al paradigma como ""una completa constelación de creencias, valores y técnicas, etc. compartidas por los miembros de una determinada comunidad"". Esta definición aparece en 1969 como agregado a su libro original denominado "La estructura de las revoluciones científicas", porque inicialmente el uso del término en dicha obra presentaba significados diversos. Bajo esta definición de Kuhn subyace otro sentido en el uso del término: ""...un paradigma también denota una suerte de elemento en esa constelación, la solución concreta del rompecabezas que, empleado como ejemplo o modelo, puede reemplazar a las reglas explícitas como base para la solución de los rompecabezas remanentes de la ciencia normal"". El término permanece impreciso debido a los diferentes usos que el mismo Kuhn le da a lo largo de "La estructura de las revoluciones científicas", y si bien intentó precisarlo en el añadido a la obra, continua teniendo un grado de polisemia. Sin embargo, el término de paradigma no debe confundirse con los de teoría, marco teórico, corriente teórica, ya que el paradigma es mucho más amplio, porque incluye la cosmovisión propia de la totalidad de realizaciones de la disciplina científica en cada caso. Podría por lo mismo, hacerse equivalente el concepto de paradigma y disciplina o matriz disciplinar, para evitar las confusiones.

El cambio de paradigma tiende a ser drástico en las ciencias, ya que estas parecen ser estables y maduras, como la física a fines del siglo XIX. En aquel tiempo la física aparentaba ser una disciplina que completaba los últimos detalles de un sistema muy trabajado. Es famosa la frase de Lord Kelvin en 1900, cuando dijo: ""No queda nada por ser descubierto en el campo de la física actualmente. Todo lo que falta son más medidas y más precisas"".

Cinco años después de esta aseveración, Albert Einstein publicó su trabajo relatividad especial que fijó un sencillo conjunto de reglas superando a la mecánica de Newton, que había sido utilizada para describir la fuerza y el movimiento por más de doscientos años. En este ejemplo, el nuevo paradigma reduce al viejo a un caso especial, ya que la mecánica de Newton sigue siendo una excelente aproximación en el contexto de velocidades lentas en comparación con la velocidad de la luz.

En "La estructura de las revoluciones científicas", Kuhn escribió que ""...las sucesivas transiciones de un paradigma a otro vía alguna revolución, es el patrón de desarrollo usual de la ciencia madura"".

La idea de Kuhn era revolucionaria en su tiempo, ya que estableció la necesidad de mirar con perspectiva histórica a los desarrollos científicos, y asumió que la ciencia es una empresa humana y como tal histórica, por ende, transformable. La observación kuhniana sobre la necesidad de mirar desde la perspectiva histórica el desarrollo de la ciencia, fue en sí misma un "cambio paradigmático" en la historia, la sociología y la filosofía de la ciencia. 

En las últimas tres o cuatro décadas, además del uso del concepto de paradigma realizado por Kuhn, otros autores como Imre Lakatos y otros han empleado dicho concepto con un sentido distinto. Cabe destacar el uso que de él ha hecho Fritjof Capra, en su obra "El punto crucial" (T"he turning point"). En esta obra, Capra plantea una visión amplia sobre paradigma, vinculada con los procesos históricos y civilizatorios, y su relación con la naturaleza. Para Capra, las limitaciones civilizatorias actuales están generando un "cambio de paradigma", "entendiendo por paradigma la mentalidad, conceptos y sistemas de valores que forman parte de una visión particular de la realidad". Establece y describe dos grandes paradigmas: el existente o mecanicista y el que está en formación. El paradigma existente o mecanicista día se ha gestado desde la antigüedad, e incluye fenómenos históricos como: la Revolución científica, el Siglo de las luces, la Revolución industrial. Incluye a nivel conceptual o cultural la idea del método científico como único enfoque para llegar al conocimiento legítimo; la concepción del universo como sistema mecánico compuesto de partes elementales; la fragmentación de la realidad y sus fenómenos; la vida social comprendida como lucha competitiva por la existencia; el crecimiento tecnológico y económico para la obtención de progreso material ilimitado; la idea de que el crecimiento y el progreso es constante e ilimitado; la idea del modelo causa-efecto como base para las explicaciones de los fenómenos. A tal paradigma existente, contrapone otro paradigma en proceso de formación, con características opuestas: la posibilidad de llegar al conocimiento no solamente a través del método científico; una visión holística, amplia e integral de la realidad y sus fenómenos, que no fragmenta los fenómenos para conocerlos; la idea de que la civilización privilegia la cooperación; la limitación del crecimiento material y tecnológico dada la finitud de la naturaleza; una visión del mundo entendiendo a este como amalgama de sistemas complejos interdependientes e interrelacionados.

Probablemente el uso más común de paradigma, implique el concepto de "cosmovisión". Por ejemplo, en ciencias sociales, el término se usa para describir el conjunto de experiencias, creencias y valores que afectan la forma en que un individuo percibe la realidad y la forma en que responde a esa percepción. Debe tenerse en cuenta que el mundo también es comprendido por el paradigma, por ello es necesario que el significado de paradigma es la forma por la cual es entendido el mundo, el hombre y por supuesto las realidades cercanas al conocimiento.
Los investigadores sociales han adoptado el concepto de Kuhn "cambio de paradigma" para remarcar un cambio en la forma en que una determinada sociedad organiza e interpreta la realidad. Un "paradigma dominante" se refiere a los valores o sistemas de pensamiento hegemónicos o dominantes en una sociedad, en un momento determinado. Los paradigmas dominantes son compartidos por el trasfondo cultural de la comunidad y por el contexto histórico del momento. Las siguientes son condiciones que facilitan que un sistema de pensamiento pueda convertirse en un paradigma dominante:


Al equiparar el paradigma con modelo, hablando de Ciencias Sociales, se tiene que el mismo comprende a un conjunto de características aplicables al estudio de determinada sociedad. 

Los paradigmas pueden ser descritos desde una perspectiva estructural. Operan en diferentes niveles: macro, meso y micro de la estructura paradigmática. Los niveles direccionan mejor la estructura fundamental de los paradigmas, y no tanto su categorización cronológica o histórica, ni su uso etimológico; como sucede en la mayoría de las disciplinas. Los niveles paradigmáticos están siempre presentes y no se encuentran limitados por tales categorías. Permiten además ayudar a comprender el funcionamiento de un paradigma.




Así, un paradigma es una visión de la realidad que conforma una "Gestalt" resultante de las tres ramas de la filosofía: metafísica, epistemología y ética, de la siguiente manera: 

Resulta obvio que las tres ramas de la filosofía describen la estructura de un paradigma. Ninguna de las ramas de la filosofía puede por separado completar su conocimiento, pero juntas describen la "Gestalt" semejante a un movimiento en espiral —no un mero círculo— que constituye el "conocimiento hermenéutico". 

La Parálisis Paradigmática se puede presentar en cualquier nivel de la sociedad, pero sus consecuencias son peores cuando ataca a personas que toman decisiones. No es una enfermedad física, más bien es de la mente, pero cuando se presenta suele ser muy dañina sobre todo en personas y organizaciones expuestas a un entorno dinámico. En general, es una enfermedad fácil de adquirir y a menudo fatal cuando se trata de convertir el paradigma en el paradigma único.

Pero. ¿De dónde proviene esta enfermedad, muchas veces rayana en la paranoia? La palabra griega "paradeigma" significa "modelo" o patrón" y en la práctica se convierte en un conjunto de reglas y disposiciones, escritas o no, que establecen o definen los límites y las formas de comportarse dentro de ellos. Se crean estructuras mentales, mitos, creencias, modelos, patrones, estereotipos que al asumirse como ciertos, resultan fáciles de adoptar, y por ende influyen en el comportamiento, actitudes y percepciones de las personas.

Algunos paradigmas podrán ser triviales, pero grandes o pequeños, sirven para proporcionar una visión, una comprensión y métodos particulares para resolver problemas específicos. Es muy saludable tener ciertas formas de comportarse o de poseer ciertos modelos, pero en el extremo, la "parálisis paradigmática", pasa a constituirse en una de las enfermedades organizacionales más graves, la cual no permite pensar ni dudar respecto de la validez o vigencia del paradigma y podría volverse crónica.

Existen variados ejemplos de esta enfermedad entre empresarios, educadores, gobiernos, profesionales, comerciantes, políticos, científicos, en las familias, en países, en las religiones, etc., y eso debido a que la historia humana siempre ha estado en proceso de cambio y por ende siempre han existido paradigmas, con la única diferencia que de antaño ocurría un cambio cada veinticinco años y en la actualidad ocurren en cosa de minutos, por lo que se debe disponer de una adecuada dosis de predisposición, entre otras, para no ser víctima de esta "parálisis paradigmática".

En 1492, hace más de quinientos años, Cristóbal Colón inició un viaje para buscar una nueva ruta a Oriente. Basado en su creencia de que la Tierra era redonda, Colón había notado que al observar un barco alejándose del puerto, el casco era lo primero en desaparecer en el horizonte, después lo hacía el velamen. Los demás obviamente veían lo mismo, sin embargo como el paradigma de la época, era el de que la Tierra era plana, no trataron de explicarse el porqué de la situación, ya que al ser plana, el barco alejándose sólo se vería más pequeño. Unos años más tarde, Hernando de Magallanes comenzó y Juan Sebastián El Cano completó la primera vuelta en barco alrededor del mundo. Se había roto el paradigma y la correspondiente parálisis.




</doc>
<doc id="9731" url="https://es.wikipedia.org/wiki?curid=9731" title="Pthirus pubis">
Pthirus pubis

La ladilla (Pthirus pubis) es un insecto anopluro ectoparásito de los seres humanos, de entre 1-3 mm de longitud, casi redondo, achatado y de color amarillento. La infestación por ladillas se denomina ftiriasis.

La transmisión se realiza en la mayoría de los casos por contacto sexual, aunque también en ocasiones puede suceder al usar prendas que han estado en contacto con algún portador. Además de la región púbica, también pueden situarse en el cabello, las cejas, las pestañas y el vello axilar y corporal (del pecho o de piernas y brazos, por ejemplo). Sus huevos pueden verse en forma de pequeños puntos blancos pegados al pelo cerca de la piel. El período de incubación de los huevos es de seis a ocho días. En otros idiomas suele denominarse literalmente 'piojo del pubis' o 'piojo púbico'. A diferencia del piojo de la cabeza, son lentas moviéndose, avanzando cada día aproximadamente de uno a diez centímetros.

Se estima que hay más de 1 millón de casos cada año. Las personas que tienen más relaciones sexuales con diferentes personas corren un riesgo más alto de contraer piojos púbicos.

Las ladillas se alimentan de sangre por lo menos cincuenta veces al día, lo que ocasiona un prurito muy molesto que puede hacer que el infectado se rasque provocando irritación e infección de la piel. Cada cinco días aproximadamente, la hembra pone entre diez y quince huevos blancos (las liendres), que tardan una semana en incubar. Cada día se pueden mover aproximadamente un centímetro. En la ropa interior suelen aparecer unas manchas de color marrón/rojizo debido a las pequeñas gotas de sangre de las picaduras.

En algunos individuos, la infestación es asintomática o se manifiesta de forma sutil, por lo que pueden transmitir el parásito al no saber que lo poseen.

Existen cremas, champús y lociones que contienen hexacloruro de benceno gamma o permetrina y que son igualmente eficaces mientras se usen correctamente. Aunque el parásito vive poco tiempo separado del cuerpo, es conveniente cambiar sábanas, toallas y ropas para evitar la reinfestación. Es recomendable encerrar en bolsas aisladas toda la ropa y sábanas recién usadas antes de aplicar el tratamiento para que los liendres no sobrevivan.
A los sujetos diagnosticados de ladillas, se les recomienda comentar su infestación con sus parejas sexuales con objeto de frenar epidemias.
La reinfestación puede suscitarse, ya que una vez que las ladillas han sido separadas del cuerpo, pueden sobrevivir hasta 24 horas, mientras que los huevos o liendres hasta seis días, por lo que una vez curado se debe repetir el tratamiento de 7 a 10 días después para eliminar los huevos que hayan quedado, ya que en 7 a 10 días se convierten en ladillas. Por esta razón se debe desinfectar una semana después de la primera limpieza.

Los preservativos no detienen el contagio de piojos púbicos; la forma de prevención es asegurar que la pareja de relaciones sexuales no tenga ladillas. El parásito es capaz de vivir poco tiempo sin contacto con el cuerpo humano. Sin embargo, es conveniente no usar ropa o sábanas de otras personas.

El estudio genético más reciente indica que la ladilla se relaciona con el piojo endémico del gorila, "Pthirus gorillae", y que pudo haber pasado a los homínidos tempranos desde los ancestros de los gorilas, hace varios millones de años. Hasta ahora se pensaba que habían divergido en el propio humano.


</doc>
<doc id="9732" url="https://es.wikipedia.org/wiki?curid=9732" title="Técnica">
Técnica

Técnica (del griego, τέχνη "tékhnē" 'arte, técnica, oficio') es el conjunto de procedimientos, reglas, normas o protocolos que tiene como objetivo obtener un resultado determinado y efectivo, ya sea en el campo de la informática como en el de muchos otros.

La técnica primeramente requiere tanto destrezas manuales como intelectuales, frecuentemente el uso de herramientas y de varios conocimientos. En los animales las técnicas son características de cada especie. En el hombre, la técnica surge de su necesidad de modificar el medio y se caracteriza por ser transmisible, aunque no siempre es consciente o reflexiva. Generalmente, cada individuo la aprende de otros (a veces la inventa) y, finalmente, la modifica. Es generalizada la creencia, que solo las personas son capaces de construir con la imaginación, algo que luego pueden concretar en la realidad. Sin embargo, algunos primates superiores, aparte del hombre, pueden fabricar herramientas. La técnica, a veces difícil de diferenciar de la tecnología, surge de la necesidad de transformar el entorno para adaptarlo mejor a sus necesidades.


Las técnicas instruccionales son herramientas didácticas que utiliza el instructor para reforzar o concretar el objetivo de aprendizaje planteado. 
La elección de las técnicas varía de acuerdo al objetivo, las características de los participantes y del curso, y de la dinámica grupal.

La técnica se refiere a los procedimientos y recursos que se emplean para lograr un resultado específico. Las técnicas tienen el objetivo de satisfacer necesidades y requieren de quien las aplica.
Cualquier actividad que es realizada en la vida diaria sigue un método o procedimiento, es decir, una técnica.

La historia de la técnica es la historia de la invención de herramientas y técnicas con un propósito práctico. La historia moderna está relacionada íntimamente con la historia de la ciencia, pues el descubrimiento de nuevos conocimientos ha permitido crear nuevas cosas y, recíprocamente, se han podido realizar nuevos descubrimientos científicos gracias al desarrollo de nuevas tecnologías, que han extendido las posibilidades de experimentación y adquisición del conocimiento.




</doc>
<doc id="9734" url="https://es.wikipedia.org/wiki?curid=9734" title="Homicisium">
Homicisium

Homicisium es el nombre de la multa que, según las leyes medievales, habían de pagar los habitantes de un municipio cuando aparecía el cadáver de alguien asesinado dentro de su término municipal y no había modo de encontrar al autor del crimen.

La práctica lógica de mantener el hallazgo en secreto y de llevar a la víctima a un pueblo vecino por la noche con el fin de evitar la sanción dio lugar a la expresión "echarle a uno el muerto" cuando se pretende inculpar a un tercero en algo que no ha hecho.



</doc>
<doc id="9736" url="https://es.wikipedia.org/wiki?curid=9736" title="Software de aplicación">
Software de aplicación

En informática, el software" de aplicación es un tipo de "software" de computadora diseñado para realizar un grupo de funciones, tareas o actividades coordinadas para el beneficio del usuario. Ejemplos de una aplicación —en ocasiones se usa el acortamiento inglés app", de "application—" serían un procesador de textos, una hoja de cálculo, una aplicación de contabilidad, un navegador web, un reproductor multimedia, un simulador de vuelo aeronáutico, una consola de juegos o un editor de fotografías. «"Software" de aplicación» hace referencia colectivamente a todas las aplicaciones, como analogía con el "software" del sistema, que está principalmente relacionado con aquel ejecutado por el sistema operativo.

Las aplicaciones pueden ser empaquetadas con el ordenador y su "software" de sistema o bien ser publicadas por separado, y asimismo pueden codificarse como proyectos propietarios, de código abierto o universitarios. Las aplicaciones creadas para plataformas móviles se denominan aplicaciones móviles.

Actualmente, con el uso de dispositivo móviles, se ha extendido el término "app", que es un acortamiento de la palabra inglesa "application", y extendida por el éxito de la llamada App Store de Apple. En español se desaconseja su uso, pero de usarla, se recomienda escribir "app" en letra cursiva,​ y no debería deletrearse al leerla, porque no es una sigla (se pronuncia /ap/), aunque esto puede dificultar la pronunciación de las palabras que le siguen inmediatamente, al forzar una pausa para pronunciar o marcar la p final de "app" —que no es algo que en español se dé naturalmente—. El acortamiento que podría recomendarse del término 'aplicación',​ en todo caso sería 'apli' (con su plural 'aplis'), escritas en letra común.

En tecnología de la información, una aplicación es un programa informático diseñado para ayudar a las personas a realizar una actividad. Por lo tanto, una aplicación difiere de un sistema operativo (que ejecuta una computadora), una utilidad (que realiza tareas de mantenimiento o tareas generales) y una herramienta de programación (con los cuales se crean los programas de computadora). Dependiendo de la actividad para la que fue diseñado, una aplicación puede manipular texto, números, gráficos o una combinación de estos elementos.

Algunas compañías agrupan diversos programas de distinta orientación en el uso, para que formen un paquete (llamados "suites" o paquetes ofimáticos), que suelen ser satisfactorios para las necesidades más apremiantes del usuario. Todos y cada uno de ellos sirven para ahorrar tiempo y dinero al usuario, al permitirle hacer cosas útiles con la computadora con alguna facilidad; de todas maneras, hay diferencias entre los programas que se ofrecen, pues algunos brindan ciertas prestaciones, aunque otros imponen un determinado diseño demasiado estricto, y ya que además, unos son más agradables y fáciles de usar que otros.

El sistema de "software" escrito "a medida" para el usuario satisface sus necesidades específicas. Esto incluye plantillas de hojas de cálculo, macros de procesador de textos, simulaciones científicas, gráficos y "scripts" de animación. Incluso los filtros de correo electrónico son una especie de software de usuario. Los usuarios crean este "software" ellos mismos y muchas veces pasan por alto lo importante que es.

Sin embargo, la delimitación entre el "software" del sistema, como los sistemas operativos y el "software" de aplicación, no es exacta y, en ocasiones, es objeto de controversia. Por ejemplo, una de las preguntas clave en el juicio antimonopolio de "Estados Unidos v. Microsoft Corp." fue si el navegador web Internet Explorer de Microsoft era parte de su sistema operativo Windows o una pieza separable de software de aplicación. Citando otro ejemplo, la controversia de nombres de GNU/Linux es, en parte, debido al desacuerdo sobre la relación entre el núcleo Linux y los sistemas operativos construidos sobre este núcleo. En algunos tipos de sistemas embebidos o integrados, la distinción entre el software de la aplicación y el software del sistema operativo pueden ser indistinguibles para el usuario, como es el caso del software utilizado para controlar un VCR, un reproductor de DVD o un horno de microondas. Las definiciones anteriores pueden excluir algunas aplicaciones que pueden existir en algunas computadoras en organizaciones grandes.

La palabra "aplicación", una vez utilizada como adjetivo, no está restringida al significado "de o lo relacionado con el software de la aplicación". Por ejemplo, conceptos como interfaz de programación de aplicaciones (API), el servidor de aplicaciones, la virtualización de aplicaciones, la gestión del ciclo de vida de las aplicaciones y las aplicaciones portátiles se aplican a todos los programas informáticos por igual, no solo a los programas de aplicación.

Algunas aplicaciones están disponibles en versiones para varias plataformas diferentes; otros solo trabajan en uno y se llaman así, por ejemplo, una aplicación de Geografía para "Windows", una aplicación para "Android" para educación o un juego de Linux. A veces surge una aplicación nueva y popular que solo funciona en una plataforma, lo que aumenta la conveniencia de esa plataforma. Esto se llama aplicación asesina o "app asesina". Por ejemplo, VisiCalc fue el primer software moderno de hoja de cálculo para Apple II y ayudó a vender las nuevas computadoras personales en las oficinas. Para Blackberry fue su software de correo electrónico.

En la primera década del siglo XXI, la abreviada "aplicación" (acuñada en 1981 o anterior) se ha vuelto popular para referirse a aplicaciones para dispositivos móviles como teléfonos inteligentes y tabletas, la forma acortada que coincide con su alcance generalmente más pequeño en comparación con las aplicaciones en PC. Incluso más recientemente, la versión abreviada también se usa para software de aplicaciones de escritorio.

A partir del desarrollo y la adopción casi universal de la web, ha surgido una distinción importante entre las aplicaciones web, escritas con HTML, JavaScript y otras tecnologías nativas de la web y que normalmente requieren que una esté en línea y ejecute un navegador web, y las aplicaciones nativas más tradicionales escritas en los idiomas disponibles para el tipo particular de computadora. Ha habido un debate polémico en la comunidad informática sobre las aplicaciones web que reemplazan las aplicaciones nativas para muchos propósitos, especialmente en dispositivos móviles como teléfonos inteligentes y tabletas. Las aplicaciones web han aumentado enormemente en popularidad para algunos usos, pero las ventajas de las aplicaciones hacen que no sea probable que desaparezcan pronto, si sucede. Además, los dos pueden ser complementarios e incluso integrados.

El software de aplicación también puede verse como horizontal o vertical. Las aplicaciones horizontales son más populares y generalizadas, ya que son de uso general, por ejemplo procesadores de texto o bases de datos. Las aplicaciones verticales son productos especializados, diseñados para un tipo particular de industria o negocio o departamento dentro de una organización. Los paquetes integrados de "software" tratarán de manejar cada aspecto específico posible de, por ejemplo, sistemas de fabricación o bancarios, o contabilidad, o servicio al cliente.

Hay muchos tipos de software de aplicación:










Las aplicaciones también se pueden clasificar por plataforma informática. P. ej. para un sistema operativo particular, un servidor de red como la computación en la nube y las aplicaciones Web 2.0, o tipo de dispositivos, como aplicaciones móviles para dispositivos móviles.

El sistema operativo en sí mismo se puede considerar software de aplicación al realizar tareas sencillas de cálculo, medición, representación y procesamiento de textos que no se utilizan para controlar el hardware a través de la interfaz de línea de comandos o la interfaz gráfica de usuario. Esto no incluye el "software" de aplicación incluido dentro de los sistemas operativos, como un software de calculadora o un editor de texto.










</doc>
<doc id="9737" url="https://es.wikipedia.org/wiki?curid=9737" title="Aconcagua">
Aconcagua

El Aconcagua es una montaña ubicada en el departamento Las Heras, en la provincia de Mendoza, en el oeste de la República Argentina. Integra la Cordillera Principal, la cual es un componente de la cordillera de los Andes. Con una altitud de 6962 msnm, es el pico más eminente de los hemisferios meridional y occidental, el más alto de la Tierra después del sistema de los Himalayas (Asia) y, por tanto, la cima más elevada en América.

El origen del término «aconcagua» es incierto, aunque se postulan diversas procedencias;

El Aconcagua se ubica en el departamento Las Heras de la provincia de Mendoza —en el noroeste provincial—, al centro-oeste de la República Argentina. Se encuentra dentro del parque provincial Aconcagua.

El Aconcagua es la cumbre más alta de los hemisferios sur y occidental; también es la montaña más elevada de la Tierra entre las situadas fuera del sistema de los Himalayas. Posee dos picos principales: la cumbre norte, de 6960 msnm, coordenadas: S32º 39' 31" W70º 0' 42"; y la cumbre sur, de 6930 msnm. Durante décadas figuró en las publicaciones la altura de 6955 msnm, y posteriormente la de 6965 msnm.

En este sector de la frontera de Argentina y Chile el límite entre los dos países se establece por la línea de "divisorias de aguas" y los cerros Catedral y Tolosa impiden que los deshielos que nacen en el Aconcagua se encaminen hacia el océano Pacífico, siendo tributarios del Atlántico. Además las aguas de estos dos últimos cerros tampoco van al Pacífico, por lo que la frontera corre a 14 km hacia el occidente del Aconcagua, hasta el cerro Caracoles, sobre el Cordón de los Dedos, dejando al Aconcagua enteramente en territorio argentino.

Al norte y al este limita con el valle de las Vacas y al oeste y al sur con el valle de los Horcones inferior. Varios glaciares atraviesan sus laderas; los más importantes son el glaciar Nororiental o Polaco y el del Este o Inglés.

Estudios geológicos sitúan la elevación del Aconcagua en la edad Permotriásica, unos 200 a 280 millones de años atrás. La montaña fue creada por la subducción de la placa de Nazca debajo de la placa Sudamericana durante la orogenia andina (terciaria, por lo tanto geológicamente reciente).

Durante los inicios de su exploración se pensó que era un volcán, pues uno de sus exploradores, Paul Güssfeldt, comprobó que las rocas que lo conformaban eran de origen volcánico, pero el profesor Walter Schiller, investigador geólogo del Museo de La Plata, publicó en uno de sus trabajos, que estas rocas volcánicas fueron depositadas en el lugar por fuertes eventos tectónicos y que no se evidenciaba ningún orificio o cráter en la cima del mismo.

A 5300 msnm se encontró un enterratorio ritual incaico en el cual se había practicado el Capac Cocha, consistente en el sacrificio de un niño de 7 a 8 años de edad durante el período incaico (1400 a 1532 d. C.).

El hallazgo sucedió durante el verano de 1985, ese año, el Club Andinista de Mendoza cumplía 50 años de vida y, como parte de las actividades conmemorativas, la institución presidida por Félix Fellinger, planificó numerosas expediciones al Aconcagua por cuatro rutas. Una de estas fue la poco frecuentada arista sureste, por donde transitaban los hermanos Alberto y Franco Pizzolón, y Fernando y Juan Carlos Pierobón. Alberto Pizzolón observó algo que le llamó la atención: «¡Ahí hay pasto!»; a lo que respondieron: «¿Cómo? ¡Si estamos a más de cinco mil metros de altura!».

Este diálogo ―extraído de uno de los libros del Dr. Juan Schobinger, describe el preciso instante de un hallazgo arqueológico muy importante y que se transformaría en uno de los hitos más grandes de la arqueología de alta montaña.

El «pasto» que creyeron ver los andinistas, resultó corresponder a un conjunto de plumas amarillas y negras que habían formado parte de un penacho.

Las plumas que se veían de lejos estaban frente a dos gruesos muros semicirculares pircados, muy derruidos, cercanos a un círculo de piedras de un metro de diámetro. En el relleno protegido por una de las pircas se hallaba, semienterrado debido al desplazamiento de parte de la tierra, el fardo funerario que contenía el cuerpo fuertemente plegado de un niño de unos 7 años de edad. Estaba envuelto por numerosas piezas textiles, siendo la más externa un manto totalmente engarzado con plumas amarillas, probablemente de papagayo.

El cráneo presentaba una rotura debido a la erosión, al haber quedado a la intemperie. En su interior podía observarse el cerebro, colapsado por deshidratación. Los descubridores se limitaron a tomar muestras y acordaron mantener el hallazgo en secreto, para evitar un posible saqueo; regresaron al campamento base para dar aviso del descubrimiento. Veinte días más tarde, una expedición organizada regresó al sitio. El Dr. Juan Schobinger, un arqueólogo y experto en culturas andinas interrumpió sus vacaciones para viajar a Mendoza y ver con sus propios ojos a la momia del Aconcagua.

Dos días se demoraron en alcanzar el contrafuerte «pirámide» con sus dos gruesas pircas circulares. En el segundo día de trabajo se hallaron varios objetos que evidenciaron que se trataba de un sacrificio humano hecho por los incas: junto al niño, se encontraban 6 estatuillas coincidentes con la cultura incaica, representando figuras humanas y camélidos en oro laminado, plata y "Spondylus" (una valva del océano Pacífico). Su cuerpo actualmente se encuentra resguardado en el laboratorio del Centro Científico Tecnológico (ex-Cricyt). Estos hallazgos confirman al monte Aconcagua como uno de los más grandes "«apu»" del Imperio inca en el Collasuyu.

El «Niño del Aconcagua» ha vuelto a revelar algo más de sí. A partir de un estudio de ADN destinado a secuenciar su genoma mitocondrial, un equipo de científicos españoles y argentinos lograron establecer que la momia pertenecía a un linaje que no había sido detectado hasta ahora en poblaciones contemporáneas y que se remonta a los tiempos en que el hombre llegó a América en la primera oleada migratoria (hacia el 22 000 AP).

A sólo 3 kilómetros está un caserío llamado Puente del Inca, donde aún existe una fuente de aguas termales, una feria de artesanías, y un hostal de 2 estrellas del mismo nombre. Al final del Parque Aconcagua está el «Puente confluencia», lugar donde varias parejas han realizado el ritual de compromiso y que da inicio al ascenso al Monte Aconcagua. Un nuevo centro de asistencia fue inaugurado en enero de 2011 a una altitud de 5975 msnm: el Refugio Elena, en el Aconcagua (se considera por ahora el refugio estable de montaña más alto de la Tierra), gracias a la donación que realizaron familiares de la andinista italiana Elena Senín, quien perdió la vida luego de llegar a la cumbre en enero de 2009. El refugio —destinado a emergencias y operativos de rescate— está ubicado en el campamento Plaza Cólera, en la bifurcación de las dos rutas más transitadas de ascenso al Aconcagua, la Norte y la del Glaciar de los Polacos.

Los centros para la práctica de esquí más próximos a esta montaña son:

Es una montaña muy frecuentada por andinistas de todo el mundo, con una entrada de 6000 a 7000 visitantes por temporada, que se extiende entre diciembre y marzo.

En términos montañistas el Aconcagua es técnicamente sencillo desde la cara norte, a través de la «vía normal» del noroeste, en la que no es necesario el uso de técnicas de escalada. Los efectos de la altitud son muy severos (la presión atmosférica es el 40 % de la existente a nivel del mar) provocando generalmente apunamiento en los escaladores. Las condiciones climatológicas pueden cambiar brúscamente desde un clima tranquilo y diáfano a, en pocos minutos, un clima tempestuoso o producirse el viento blanco del Aconcagua.

Su ascensión no suele requerir el uso de oxígeno artificial.

En la «vía normal» se asciende a través de campamentos de altura con sus correspondientes días de descanso. Los hitos más significativos de la vía son: Campo Base (Plaza de Mulas) (4300 msnm), El Semáforo (4350 msnm), Piedras Conway, Plaza Canadá, Piedra de 5000, Cambio de Pendiente, Nido de Cóndores (5250 msnm), Berlín, Piedras Blancas, Piedras Negras, Independencia, Portezuelo de los Vientos, Gran Travesía, La Canaleta y Cumbre del Aconcagua (siguiendo el orden creciente de dificultad).

La segunda vía, mucho más peligrosa que la anterior, es la del glaciar de los Polacos. Ésta se aproxima a la montaña a través del valle de las Vacas, asciende hasta la base del glaciar de los Polacos y cruza la vía normal hasta la subida final a la cumbre.

Las vías desde las crestas situadas al sur y suroeste son las más duras, considerándose la "Pared Sur" como la más difícil. Se trata de una escalada muy comprometida y de alta dificultad en una de las mayores paredes del mundo (3000 m de pared aprox.) La primera ascensión de la Pared Sur fue realizada el 25 de febrero de 1954 por los franceses Pierre Lesueur, Adrien Dagory, Edmond Denis, Robert Paragot, Lucien Berardini y Guy Poulet. El jefe de la expedición era René Ferlet.

La primera ascensión al Aconcagua se realizó en 1897 por una expedición liderada por el británico Edward FitzGerald (1871-1931). La cumbre fue alcanzada por el suizo Matthias Zurbriggen (40) el 14 de enero y por otros dos miembros de la expedición unos días después.

El primer argentino en hacer cumbre fue Nicolás Plantamura, perteneciente al Ejército Argentino, el 8 de marzo de 1934; en esta misma expedición también participó el arriero Mariano Pastén, quien se convirtió en el primer chileno en alcanzar la cumbre. La primera mujer fue la francesa Adriana Bance, el 7 de marzo de 1940, quien ascendió acompañada por miembros del Club Andinista de Mendoza.

En 1952, los miembros del Club Alemán Andino o DAV Santiago, Eberhard Meier, Ludwig Krahl y Wolfgang Förster completaron el ascenso por la llamada ruta chilena o de Güssfeldt. Esta ruta nace en territorio chileno y asciende por el valle del río Colorado para cruzar por algunos de los pasos hacia territorio argentino y a través del glaciar de Güssfeldt se conecta con la ruta normal en su último tramo. Esta ruta ya había sido intentada en 1883 por el científico alemán Paul Güssfeldt, quien debido al mal tiempo fracasó en su intento de hacer cumbre a poca distancia de ella, alcanzando los 6600 msnm.

En septiembre de 1953, los argentinos Emiliano Huerta, H. Vasalla y F. Godoy logran la primera ascensión invernal al Aconcagua, utilizando la ruta normal. Por esta hazaña, la calle principal de la localidad de Puente del Inca lleva el nombre de Huerta.

La «Variante Altoaragonesa»: escalada en 1995, una serie de corredores que salen del glaciar de lo Polacos, con una inclinación de 75° y pasos de IV+; tras un largo flanqueo a los 6500 msnm se unen de nuevo con la Directa, abierta por los aragoneses Javier Subias, José Antonio Hidalgo, Javier Alvira y José Vilalta.

- El 1 de enero de 2000, la Actriz y Periodista Victoria Manno, de nacionalidad Ítalo-Argentina, desde “El Monte Aconcagua”, de 6962 msnm, (en esta fecha, lugar habitable más alto del mundo, por razones climáticas), envió a la “Humanidad Toda” un mensaje altruista, «dando voz a todas las Mujeres, Niños y Niñas que sufren a diario, en silencio, la violación de sus Derechos y también pidió por la Paz Mundial». De esta manera comenzó su Proyecto y lo continúa cada año, haciendo un/a "Llamado/a de Atención al mundo", desde los lugares más representativos, para ella, de los 6 Continentes. Fue la primera vez en la historia de la humanidad que se envían mensajes por Internet desde esa altura con un teléfono Inmarsat, y también fue la primera vez que se transmitió desde el Aconcagua el ascenso para radio AM Libertad y FM Feeling, con un teléfono Iridium.

La persona más joven en escalar y llegar a la cima del Aconcagua es el estadounidense Tyler Armstrong a la corta edad de 9 años el 25 de diciembre de 2013. Junto a su padre y guías andinistas, tomaron la ruta central del Glaciar de los Polacos. Cabe destacar que el joven Armstrong batió un récord mundial al hacer esta hazaña.

La segunda persona más joven en llegar a la cumbre del Aconcagua fue la niña india Kaamya Karthikeya, con 12 años, cuatro meses y 18 días. Lo hizo el 1 de febrero de 2020. 

En diciembre de 2014, el corredor español de trail running Kilian Jornet estableció un nuevo récord de velocidad de subida y bajada al Aconcagua, al marcar un tiempo de 12 horas y 49 minutos. 

Karl Egloff es un deportista suizo-ecuatoriano que se ha desempeñado en el ciclismo y el montañismo. El 19 de febrero de 2015 rompió el récord mundial de trepada en velocidad con un tiempo de ascenso y descenso de 11 horas y 52 minutos.

Se considera que el Aconcagua posee el índice de mortalidad más alto en Sudamérica (aproximadamente tres fallecimientos por año). Esto se debe a que al ser posible lograr el ascenso con relativa sencillez, personas sin la debida preparación se presentan a hacer el intento. Los escaladores palidecen ante el mal de altura y los cambios climáticos extremos, con vientos fuertes como resultado de la proximidad de la montaña al Océano Pacífico.

Desde que iniciaron los registros en el año 1926, más de cien personas han fallecido en el Aconcagua. Entre los años 2001 y 2012, de las 42.731 personas que buscaron alcanzar la cumbre del Aconcagua, 33 murieron, lo que indica una tasa de mortalidad de 0,77 cada 1.000 individuos.





</doc>
<doc id="9739" url="https://es.wikipedia.org/wiki?curid=9739" title="Utilidad (informática)">
Utilidad (informática)

En informática, una utilidad es una herramienta que realiza:

En donde se incluyen las bibliotecas de sistema, "middleware", herramientas de desarrollo y demás.

Entre ellas podemos nombrar cifrado y descifrado de archivos, compresión de archivos, desfragmentación de disco, editores de texto, respaldo, etc.



</doc>
<doc id="9740" url="https://es.wikipedia.org/wiki?curid=9740" title="Gastronomía del Perú">
Gastronomía del Perú

La gastronomía del Perú es una de las más variadas y exquisitas del mundo, tanto que en el libro "357 listas para entender cómo somos los peruanos" llega a contar hasta 491 platos típicos.

La cocina peruana es el resultado de la fusión inicial de la tradición culinaria del antiguo Perú —con sus propias técnicas y potajes— con la cocina española en su variante más fuertemente influenciada por 762 años de presencia morisca en la península ibérica y con importante aporte de las costumbres culinarias traídas de la costa atlántica del África subsahariana por los esclavos.

Posteriormente, este mestizaje se vio influenciado por los usos y costumbres culinarios de los chefs franceses que huyeron de la revolución en su país para radicarse, en buen número, en la capital del virreinato del Perú. Igualmente trascendental fue la influencia de las inmigraciones del siglo XIX, que incluyó chinos-cantoneses, japoneses e italianos, entre otros orígenes principalmente europeos.

Como particularidad exclusiva de la gastronomía del Perú, "existen comidas y sabores de cuatro continentes en un solo país" y, esto, desde la segunda mitad del siglo XIX. Las artes culinarias peruanas están en constante evolución y, sumada a la variedad de platos tradicionales, regionales y locales, hace imposible establecer una lista completa de sus platos representativos.

Cabe mencionar que a lo largo de la costa peruana existen registrados más de dos mil quinientos diferentes tipos de sopas, así mismo existen más de 250 postres tradicionales. La gran variedad de la gastronomía peruana se sustenta en tres fuentes: la particularidad de la geografía del Perú, la mezcla de culturas y la adaptación de culturas milenarias a la cocina moderna.

La presencia de los diversos pisos altitudinales de la cordillera de los Andes en el Perú y su cercanía al ecuador geográfico permite la existencia de una serie de microclimas y de especies, desde zonas de habituales nevadas hasta selvas tropicales, (con 84 de las 104 zonas climáticas del globo, es uno de los 12 países del mundo poseedores de mayor megadiversidad). Tiene condiciones adecuadas para el cultivo de frutas y verduras durante todo el año. Asimismo la corriente de Humboldt de aguas frías que corre por el océano Pacífico frente a la costa peruana permite la existencia de una gran variedad de peces y mariscos (Perú es uno de los principales países pesqueros del mundo).

Los andes centrales peruanos fueron el más grande centro de domesticación de plantas del mundo antiguo, con especies nativas como el maíz, tubérculos con más de seis mil variedades de papa, según el Instituto Nacional de Innovación Agraria (INIA), de las cuales solo 729 tienen información genética completa, y muchas otras de camote, yuca o mandioca, oca, maca; gramíneas quinua, kiwicha o amaranto, cañihua; frutas como chirimoya, lúcuma, pacae, tomate, calabaza, palta, tumbo, sauco; leguminosas tales como frijoles, pallares, maní y una infinidad de hierbas aromáticas.

Antes del arribo europeo, la geografía peruana albergaba una gran variedad de culturas, conquistadas todas por el Imperio incaico, cada una de las cuales tenía características gastronómicas particulares, aunque había algunas generalidades, de acuerdo con los cronistas de la conquista. Por ejemplo, los principales condimentos eran hierbas aromáticas, "cocha yuyo" (un tipo de alga fluvial), sal y, sobre todo, el ají, llamado "uchu" en tiempos incas y considerado hoy un elemento fundamental de la cocina peruana.

El Inca Garcilaso de la Vega en los "Comentarios Reales de los Incas" escribió al respecto: "Los de mi tierra son tan amigos del uchu, que no comerán sin él aunque no sea sino unas hierbas crudas". Era común la preparación de alimentos en forma deshidatrada, para evitar su descomposición, destacando el charqui, carne salada, y el caui, que es la oca secada al sol. Los antiguos peruanos además consumían inmensas cantidades de pescados y mariscos (el registro arqueológico de ello es abrumador) y complementaban su dieta con carne de pato, cuy y camélidos sudamericanos domésticos (alpaca y llama principalmente).

En las sociedades de la costa norte, además, se consumía la carne de ciertos lagartos y de venado. En las de la selva oriental se nutrían de la multitud de especies que proporcionaba la flora y fauna amazónica. Desde épocas milenarias, los antiguos peruanos preparaban chupes o sopas, guisaban (la carapulcra, por ejemplo, es considerado el tipo de guiso peruano más antiguo), elaboraban potajes con especies marinas crudas marinadas con ají, tumbo y hierbas, de donde se origina el cebiche que en la época precolombina tenía otro nombre, en quechua.

Tenían diferentes formas de procesar alimentos, salaban pescado, tostaban el maíz (obteniendo la cancha, que es hasta hoy el «piqueo» peruano más simple y popular) o pelaban sus granos y los secaban (obteniendo "mote"). Asimismo preparaban charqui -o carne de camélido disecada, salada y deshilachada- y diferentes tipos de chuño -tubérculos resecados y congelados a la intemperie-. Cocinaban en ollas de barro y, en ocasiones, organizaban grandes banquetes de carne y vegetales a partir de hornos de tierra natural (pachamancas y huatias).

Asimismo se bebían diferentes formas de cerveza de maíz (chicha) y de yuca (masato). La historia precolombina identifica al Perú como un país gastronómico. Así en la leyenda sobre «Llampayeq» (Lambayeque) recopilada por Miguel Cabello Valboa en 1532, menciona al cocinero del rey Naylamp llamado OcchoColo en el Reino Sicán del siglo IX. Luego en la leyenda de los hermanos Ayar menciona que salieron del cerro Tamputoco (Tampu, Tambu, lugar donde se guardan alimentos) y sus nombres fueron Ayar Cachi (quinua con sal), Ayar Uchu (quinua con ají), Ayar Auca (quinua con frejol), Ayar Manco (el que cuida la quinua).

Desde el inicio de la presencia española, se incorporaron nuevos usos y costumbres culinarios con el comienzo del virreinato. La fritura, el uso de los lácteos (incorporado a algunos chupes o sopas), además de la carne de res, cerdo, huevo de gallina y nuevas aves de corral; además llegaron algunos cultivos que resultarían esenciales para la nueva cocina como la cebolla y el ajo que combinados con el ají serían los principales ingredientes de muchos platos peruanos.

La lima traída por los españoles y adaptado con el tiempo a la tierra peruana, se fue transformando en la variedad peruana actual denominada limón (sutil), de color verde, pequeño y ácido y que deviene en uno de los componentes básicos del cebiche. La vid (de la que se origina el pisco) y los vinos llegan también al comienzo de este período. En los primeros encuentros entre españoles y nativos, durante la conquista del imperio incaico, intercambiaron los trozos de cerdo ibérico frito con papas, camotes y maíz autóctono.

Francisco Pizarro, quien criaba cerdos en su infancia, era el principal aficionado a este plato llamado chicharrón durante los inicios de la presencia española en este territorio. La dedicación de muchos conventos de monjas a la cocina en un entorno donde abundaban las plantaciones de azúcar (especie traída también por los españoles) e inmensas variedades de frutas nativas originó asimismo una larga tradición repostera, destacándose el alfajor, el maná preparado en distintas variedades, formas y colores según la ocasión, así como otras decenas de postres de la época.

Los esclavos africanos aportaron lo suyo en una serie de guisos, además del uso de las vísceras desechadas por las élites, que condimentaban abundantemente para disminuir los fuertes sabores de la carne y cocinados a las brasas. De aquí salieron muchos de los más representativos platos de la actual comida criolla, como por ejemplo: los anticuchos, la sangrecita, el camote con relleno, el cau cau, la pancita, el rachi, las mollejitas, la chanfainita, la patita con maní, el choncholí y el tacu-tacu.

El antropólogo peruano Humberto Rodríguez Pastor destaca el tipo de tamal tradicional peruano como un legado afroperuano en su obra "La vida en el entorno del tamal peruano". La citada vianda es introducida en este territorio desde los primeros años de la presencia española que vino con sus esclavos africanos. La gran cantidad de ellos procedentes de la costa atlántica africana marcó demográficamente la Ciudad de los Reyes ya que en el siglo XVII, más del 60 % de la población de la capital era de origen africano.

Luego de la independencia se dieron una serie de migraciones de diversas procedencias que integraron sus propias tradiciones a la ya dinámica culinaria local. La migración de los chinos-cantoneses de mediados del siglo XIX popularizó el salteado a fuego fuerte y los sabores agridulces en las carnes además del uso de nuevas hierbas y del sillao. Pero su aporte más notorio fue el arroz. Si bien ya se consumía desde el siglo XVI, es luego de la migración china que el arroz se popularizó y se convirtió en la guarnición peruana por excelencia, en detrimento del pan.

La forma de arroz favorita en el Perú es el arroz graneado no demasiado cocido, se hace con arroz de grano largo, sin embargo, se distanciaba de la preparación china en el uso del ajo y la sal. Otra inmigración en la segunda mitad del siglo XIX no menos influyente es la italiana, que popularizó el uso de las pastas, el pastel de acelga, los dulces y postres como el panetón, que es obligado en las navidades a lo largo del país. La migración japonesa de fines del siglo XIX, finalmente, impactó notablemente sobre la cocina marina peruana. Cortes y técnicas japonesas muy prolijas en la presentación de los platos, se unen a salsas y preparaciones peruanas y nace una nueva vertiente culinaria en el Perú. Así por ejemplo del cruce del sashimi japonés y del cebiche peruano nació el tiradito.

En la última década del siglo XX, e inicios del siglo XXI, la cocina peruana empezó a popularizarse fuera de sus fronteras. En la Cuarta Cumbre Internacional de Gastronomía Madrid Fusión 2006, realizada del 17 al 19 de enero de 2006, Lima fue declarada capital gastronómica de América, en tanto que el 12 de diciembre de 2012, el Perú fue distinguido en los World Travel Awards, llevados a cabo en Nueva Delhi (India), como Principal Destino Culinario a nivel mundial, superando a Australia, China, España, Estados Unidos, Francia, India, Italia, Japón, Malasia, México y Tailandia, países de reconocida trayectoria gastronómica, un reconocimiento que habla de la gran competitividad de la alta cocina peruana.

La gastronomía del país está registrada como una marca mundial, y por ende como producto bandera del Perú. Debido a esta rica variedad y a la armonía de su sabor y los alimentos empleados, la gastronomía peruana es constantemente premiada internacionalmente y sus chefs suelen obtener a menudo medallas internacionales que los distinguen. Un elemento destacable es su constante apertura a las innovaciones y el continuo desarrollo de nuevos platos, incorporando a la gastronomía la búsqueda continua de la experimentación y la vanguardia.

Así como cada región conserva su riqueza culinaria, en la alta gastronomía destaca la mezcla de colores y de productos alimenticios, una muestra de ello es la llamada cocina novoandina, un nuevo estilo culinario surgido en el Perú por el interés de los gastrónomos locales de retomar costumbres alimenticias del pasado prehispánico para recrearlas, rescatando y revalorizando así muchos de los ingredientes autóctonos.

En esta recreación de la cocina andina, entran elementos procedentes de otros horizontes culturales como el europeo. Algunos de los productos nativos utilizados son tarwi, chuño, quinua, kiwicha, moraya, cochayuyo, maca, coca, uchu, olluco, oca, en platillos como el quinotto o el coca sour. Lima, capital cosmopolita y mestiza y Arequipa se han convertido en las sedes principales de esta corriente culinaria, aunque en los principales puntos andinos como Huaraz, Juliaca, Cusco o Huancayo, este estilo ha cobrado también un gran auge.

La cocina peruana ha sido el punto de encuentro de diversas culturas, gracias a la inclinación por el mestizaje que ha caracterizado la historia del Perú. La cocina clásica peruana suele ser atractiva por su colorido y a veces por su matiz picante por las distintas variedades de ají, siendo éste un ingrediente gravitante. Sin embargo, algunos ajíes no son picantes y solo sirven para darle color a la presentación de los platos típicos o para darles mayor gusto. El arroz es un alimento que acompaña muchos platos de la gastronomía del país, popularizado principalmente a partir del siglo XIX con la influencia de la inmigración chino-cantonesa, aunque de hecho se convirtió un ingrediente clave que va más allá de una simple guarnición, ya que son muchos los platos hechos a base de arroz.

La variedad de ingredientes que existe en el territorio (tanto nativas como las que llegaron de otras latitudes) permitió la evolución de una culinaria diversa, donde coexisten, sin oponerse, fuertes tradiciones regionales y una permanente reinvención de platos. Perú es considerado como uno de los centros genéticos más grande del mundo y muchos ingredientes de origen ancestral son utilizados en su cocina:














Esta tiene tres principales ejes: la costa, la sierra y la selva.

Está constituida por una variedad de platos y especies en las cuales tenemos:

Perú es uno de los dos principales productores y exportadores de harina de pescado para la alimentación animal en el mundo. Su riqueza en peces, su fauna y flora marinas son enormes, encontrándose tipos de animales o plantas que solo se dan en sus aguas. Pero también hay que destacar sus riquezas de agua dulce que se encuentran principalmente en el río Amazonas y sus afluentes, así como en sus lagos, como el Titicaca.

Cada región costera, diferente en fauna y flora, adapta su cocina de acuerdo a los productos de sus aguas. El chupe de camarones plato originario y típico del departamento de Arequipa, es uno de los platos más refinados de la costa peruana. Se trata de una sopa espesa a base de pescado y camarones, papas, leche y ají. En el Perú existen diferentes variedades de chupes, como chupe de habas, chupe de zapallo, chupe de olluquito, entre otros. Otros platos típicos de esta cocina son el ceviche, los choritos a la chalaca, el tiradito, la leche de tigre, la parihuela y el escabeche de pescado.

Con 250 postres tradicionales desde el siglo XIX, esta gran variedad se ha originado principalmente en las ciudades costeras desde la época del virreinato del Perú, como el suspiro de limeña, el ranfañote, los picarones, el turrón, la melcocha y la mazamorra morada, entre otros. En Tacna, a inicios del siglo XX se creó su plato principal el picante a la tacneña. Los restaurantes con cartas criollas en sus variadas formas son numerosos, las pastelerías abundan y constituyen una de las riquezas culinarias de las ciudades de Lima, Arequipa, Ica, Trujillo y Tacna.

La oferta de restaurantes de toda naturaleza y especialización es notoria. Uno de ellos, de lujo, que se encontraba frente al mar en la Costa Verde de Lima, ofrecía a mediodía un servicio libre con más de cuatrocientos platos diferentes servidos en forma simultánea, por lo cual ostentó el Récord Guinness. Entre los principales platos de la comida criolla tenemos: ají de gallina, carapulca, escabeche de pollo, arroz con pollo, tacu-tacu, arroz con pato, cau cau, lomo saltado, rocoto relleno, tamales, papa rellena, sancochado, anticuchos, causa a la limeña, entre otros.

Por otra parte, existe un plato híbrido muy popular en la costa del país, especialmente en la zona centro, llamado simplemente «Combinado». Consiste en una porción de papa a la huancaína con tallarines guisados tradicionalmente (adrezo de zanahoria, tomate y cebolla) y cebiche. Cabe agregar que "combinado" se le puede decir a cualquier plato que resulte ser un poco de otros platos; por lo que el término Combinado no se aplica necesariamente al platillo descrito antes.

El seco de cabrito, es otro plato típico del Perú, nació en el norte del país entre Trujillo y Chiclayo, se trata de un mamífero del tamaño del cordero, también conocido como chivo, que es amacerada con distintos condimentos entre ellos el ají amarillo, también se le agrega chicha de jora y culantro, se sirve con arroz y frejol. Otros platos norteños son el shámbar (se sirve solo los lunes), sopa teóloga, pepián, causa en lapa, cuy frito con ajiaco, frito trujillano, etc. Todos exclusivos de la cocina trujillana.

Los Andes son el origen de milenarias culturas y con ellas el sabor de la cocina peruana. En esta parte alta del país, la alimentación principal continúa siendo el maíz, la papa y multiplicidad de tubérculos. Productos introducidos como el arroz, el pan y las pastas hoy son también de consumo popular. La variedad de carnes consumida se ha enriquecido con vacunos, porcinos y ovinos; en lugares muy elevados como Huancavelica aún se consume carne de llama, de alpaca y animales silvestres.

La variedad y riqueza de la comida andina es similar a la de la comida costeña. Desde el cuy chactao a la sopa de morón y de la papa a la huancaína a la sopa chairo, a más de postres y granos sumamente originales que se consumen frescos o cocidos de diversas maneras. El caldo de cabeza y las costillas de carnero doradas son mínima muestra de un vasto catálogo que apenas si se ha difundido.

Algunos de los principales platos de la comida andina son: la pachamanca, guiso de tarwi, rocoto relleno, adobo arequipeño, malaya, puca picante, olluquito con charqui, la huatia, la papa a la huancaína, la ocopa y los platillos elaborados con cuy. Igualmente, gran variedad de peces de agua dulce forman parte de la gastronomía regional, siendo muy apreciada la trucha, introducida a fines del siglo XIX.

La zona de la selva del Perú tiene una gran biodiversidad en fauna, por lo cual es tradicional el consumo de variadas carnes, como huangana (cerdo silvestre), suri, tapir, roedores (majaz, añuje, punchana, sachacuy), armadillo, tortuga, monos choro y maquisapa. En la inmensa variedad, destaca el paiche, el segundo pez más grande de agua dulce (puede llegar a pesar hasta 180 kilos y medir hasta 3 metros de largo).

La comida de la selva peruana tiene como elementos populares, entre otros, un aderezo básico que es conocido como "misto" (o "mishkina"), el uso del "ingiri", que es como se conoce al plátano verde sancochado, el alto consumo de frutas y la cocción de las carnes, especialmente peces y también el juane, envueltas en hojas de bijao, una palmera que tiene un aroma particular. Con respecto a las carnes, son usualmente aportadas por la cacería y la pesca y en menor medida por la ganadería.

Con respecto a las frutas destaca el camu camu que concentra la mayor cantidad de vitamina C. También es muy extendido el consumo de aguaje, del cual se prepara un refresco llamado aguajina, con alto contenido de vitamina A. No obstante, abundan los frutos tropicales como el mango, la piña y muchos otros. Del plátano maduro se prepara un refresco conocido como chapo. 

Un elemento importante de la comida de la selva son los licores, mayormente producto de la fermentación de licor de caña con especies locales (raíces, frutos, entre otros). Por ejemplo, el chuchuhuasi, el uvachado, el siete raíces, el rompe calzón, entre otros. Mención aparte merece el masato, una bebida de orígenes prehispánicos elaborada a base de yuca masticada y fermentada durante unos días en un recipiente artesanal de barro y arcilla de base ancha y cuello estrecho. Los platos más conocidos de la amazonía peruana son el juane y el tacacho con cecina pero también existen otros de alto consumo como el inchicapi, la patarashca, la ensalada de chonta, el timbuche, ceviche de paiche, la salsa de ají charapita, Ají de cocona, el Suri, arroz charapita, etc.

La repostería tradicional del Perú tiene inicio en la época de la colonia, en esta etapa fue decisiva la introducción del cultivo de caña de azúcar, las costumbres europeas y la presencia de esclavos africanos.



























Desde fines del siglo XX e inicios del siglo XXI, el cebiche, el pollo a la brasa y los platos de cocina chifa constituyen los representantes más populares de la comida peruana, siendo masivo su consumo a lo largo de todo el territorio peruano y existiendo versiones para todas las clases sociales: desde preparados muy económicos que se consumen «al paso» hasta preparados gourmet muy exclusivos.

El cebiche, ceviche, seviche o sebiche, es un plato ampliamente difundido y declarado Patrimonio Cultural de la Nación por el gobierno peruano. La receta básica del cebiche es la misma en todas las regiones: pescado en trozos, zumo de limón, cebolla roja, ají y sal al gusto. Los pescados utilizados son muy diversos e incluyen especies tanto de agua dulce como de mar, asimismo se incluyen otros frutos de mar como mariscos y algas marinas e incluso vegetales. El plato se acompaña de productos locales como cancha serrana, camote, chifles, zarandaja, yuca y hojas de lechuga.

Chifa es un término utilizado en el Perú para referirse a la cocina que surgió de la fusión entre la comida peruana y aquella de los inmigrantes chinos, principalmente de la zona de Cantón, llegada a mediados del siglo XIX e inicios del siglo XX, asimismo se usa este término para denominar a los restaurantes chifa donde esta comida es servida. En la actualidad los chifas, con fuerte influencia en muchos casos de la criolla, están entre los más comunes en Lima y muchas otras ciudades del Perú. Los principales platos son el arroz chaufa, la sopa wantán, el tallarín saltado, el aeropuerto y el pollo chijaukai.

Es uno de los platos de mayor consumo en el país. Ha sido reconocido como «Especialidad Culinaria Peruana» por el gobierno peruano el 14 de octubre de 2004. Consiste básicamente en un pollo eviscerado macerado, en una marinada que incluye diversos ingredientes, horneado a las brasas. Los inicios de este plato en Perú se señalan en el distrito de Ate de Lima, su creador fue el suizo Roger Schuler, un criador de pollos y fundador del restaurante La Granja Azul.

En 1950 Schuler junto con Franz Ulrich inventaron y registraron la patente de la máquina para cocinar el pollo a la brasa, un sistema mecánico de giro planetario que hace que los pollos giren sobre su propio eje y alternen su movimiento circular, simultáneamente. En Perú, el plato se acompaña de papas fritas, ensalada y diversas cremas (mayonesa, mostaza, kétchup, salsa de aceituna, chimichurri y salsas de ají de toda clase); en la selva del Perú se suelen reemplazar las papas fritas por plátano frito.

El 16 de octubre de 2007, la gastronomía del Perú fue proclamada Patrimonio Cultural de la Nación; esta declaratoria manifiesta que la cocina peruana es una expresión cultural que contribuye a consolidar la identidad del país. Anteriormente a esta declaratoria, otros elementos de la gastronomía peruana fueron declarados Patrimonio Cultural de la Nación:


El más connotado chef peruano de finales del siglo XX e inicios del siglo XXI es Gastón Acurio, quien originó una revalorización, internacionalización y premiación de la culinaria peruana.

Otro chef connotado es Virgilio Martínez quien en 2017 fue elegido como el mejor chef del mundo.















Por octavo año consecutivo, Perú es nombrado mejor destino culinario del mundo en los World Travel Awards 2019.





</doc>
<doc id="9742" url="https://es.wikipedia.org/wiki?curid=9742" title="Inmunología">
Inmunología

La inmunología es una rama amplia de las ciencias biomédicas que se ocupa del estudio del sistema inmunitario, entendiendo como tal al conjunto de órganos, tejidos y células que, en los vertebrados, tienen como función reconocer elementos ajenos dando una respuesta (respuesta inmunitaria).
La ciencia trata, el funcionamiento fisiológico del tanto en estados de salud como de enfermedad; las alteraciones en las funciones del sistema inmunitario (enfermedades autoinmunitarias, hipersensibilidades, inmunodeficiencias, rechazo a los trasplantes); las características físicas, químicas y fisiológicas de los componentes del sistema inmunitario. La inmunología tiene varias aplicaciones en numerosas disciplinas científicas, que serán analizadas más adelante.

La palabra inmunidad deriva del término latino "immunitas". Así que alguien inmune se entendía como exento o libre de impuestos (y/o servicios). Posteriormente, en 1879 el bacteriólogo francés Louis Pasteur acuñó una nueva acepción "Immünire" - ("in desde el interior y "munera" significando munición, armamento o trinchera) que es el concepto que entendemos en la actualidad: defenderse desde el interior. — es el origen de la palabra inmunidad, que se refiere al estado de protección frente a infecciones.

La disciplina de la inmunología surgió cuando se observó que los individuos recuperados de ciertos trastornos infecciosos quedaban protegidos frente a una posible segunda infección. Se cree que la primera referencia que describe a los fenómenos inmunitarios fue escrita por Tucídides, el historiador de las guerras del Peloponeso, en el año 430 a.n.e. Este texto describe que durante una plaga en Atenas, "solo los que se habían recuperado de ella podían cuidar a los enfermos porque no contraían el padecimiento por segunda vez."

Los primeros intentos registrados de inducir inmunidad de manera artificial los llevaron a cabo los chinos y los turcos en el siglo XV al intentar prevenir la viruela. Los informes describen el proceso de variolización en el que las costras secas dejadas por las pústulas de la viruela se inhalaban por las narinas o se insertaban en pequeños cortes de piel.

En 1718, la aristócrata británica Lady Mary Wortley Montague a la vuelta de una estancia en Turquía, realiza la práctica de la variolización en sus propios hijos. Pero no es hasta 1796, que el médico inglés Edward Jenner, al observar el hecho de que las niñeras que habían contraído la enfermedad de la pústula vacuna o pústula mamaria de la vaca (una enfermedad leve) quedaban inmunes contra la viruela razonó que al introducir líquido de una pústula vacuna en una persona (inoculación) podía protegérsele contra la viruela. Verificó su hipótesis inoculando en un niño de ocho años de edad con líquido de una pústula vacuna y luego lo infectó de manera intencional con viruela; el niño no presentó la enfermedad.

Louis Pasteur, con sus asistentes Charles Chamberland y Émile Roux, logró cultivar la bacteria que causaba el cólera de las gallinas y comprobó la participación de este microorganismo cuando los pollos inoculados con este murieron. Pasteur se fue de vacaciones y dejó su laboratorio con sus cultivos bacterianos, los que al paso del tiempo perdieron su patogenicidad. Al volver, inyectó a algunos de sus pollos con estos cultivos viejos y notó que enfermaban, pero no morían y supuso que se debía a la desvitalización del cultivo. Trató de repetir este experimento pero con un cultivo nuevo que al inyectar sobre los pollos los mataría, no obstante, su abastecimiento de pollos era limitado y tuvo que usar los mismos pollos. Cuando los inyectó, estos estaban protegidos contra la enfermedad. Con esto descubrió que el envejecimiento atenuó la cepa y que esta podría utilizarse para conferir protección contra el padecimiento. Denominó a la cepa atenuada vacuna (del lat. "vacca" que significa vaca) en honor al trabajo de Jenner. Este trabajo marcó el inicio de la inmunología.

Pasteur descubrió que era posible atenuar o debilitar agentes patógenos que confirieran resistencia y esto lo demostró con otro experimento en el pueblo de Pouilly-le-Fort en 1881. Pasteur vacunó ovejas con el bacilo del carbunco ("Bacillus anthracis") atenuado con calor. En este experimento, solo las ovejas vacunadas vivieron. En 1885, Pasteur vacunó por primera vez a un humano, Joseph Meister, un niño que había sido mordido por un perro rabioso. Pasteur le administró virus de la rabia atenuados con lo que evitó el progreso de la enfermedad. Joseph creció y se convirtió en el custodio del Instituto Pasteur. Pasteur demostró que la vacunación funcionaba pero desconocía el motivo de esto.

Las décadas que siguieron fueron emocionantes. Las investigaciones de Emil von Behring y Shibasaburo Kitasato fueron mecedoras del premio Nobel en 1901. En 1890 descubrieron que el suero de animales inmunizados (frente a la difteria y el tétanos) contenía sustancias que podían neutralizar estas infecciones. Demostraron que el suero de animales inmunizados con anterioridad contra la difteria podían transferir el estado de inmunidad a animales no inmunizados. 

En 1989 Jules Bordet descubrió el sistema de complemento y demostró que su acción conjunta con anticuerpos ayudaba a la destrucción de las bacterias. Así que los conocimientos dominantes eran de la denominada Inmunidad humoral: moléculas solubles en los humores del organismo.

Diferentes científicos probaron durante la década siguiente que un componente activo del suero inmune podía neutralizar y precipitar toxinas y aglutinar bacterias. Este componente activo recibió nombres como antitoxina, precipitina y aglutinina hasta que en 1930 Elvin Kabat demostró que la fracción de suero gamma (inmunoglobulinas) era la que generaba todas estas actividades. Las moléculas activas de esta fracción se llamaron anticuerpos.

Las contribuciones de otros gigantes como Koch, Metchnikoff, Ehrlich, Rickets,y el joven Landsteiner, influenciados por los descubrimientos de anticuerpos, complemento, diagnóstico serológico, anafilaxia, y fueron muy relevantes para el desarrollo de esta nueva ciencia de la inmunología. En concreto Elie Metchnikoff fue el primero en sugerir que algunas células podían jugar un importante papel en la defensa del organismo frente a agentes infecciosos, debido a su capacidad fagocítica: los macrófagos. Estos hallazgos abren una nueva etapa, donde la teoría celular ponía en entredicho a la teoría humoral.

La inmunología clásica está incluida dentro de los campos de la epidemiología. Estudia la relación entre los sistemas corporales, patógenos e inmunidad. El escrito más antiguo que menciona la inmunidad se considera el referente a la plaga de Atenas en el 430 a. C. Tucídides notó que la gente que se había recobrado de un ataque previo de la enfermedad podía cuidar a los enfermos sin contraer la enfermedad por segunda vez. Muchas otras sociedades antiguas tienen referencias de este fenómeno, pero no fue hasta los siglos XIX y XX donde el concepto fue llevado a la teoría científica.

El estudio de los componentes celulares y moleculares que comprende el sistema inmunitario, incluyendo sus funciones e interacciones, es el tema central de la inmunología. El sistema inmunitario ha sido dividido en un más primitivo sistema inmunitario innato, y un sistema inmunitario adaptativo o adquirido de los vertebrados; este último a su vez está dividido en sus componentes humorales y celulares.

La respuesta humoral inicialmente se refiere a los anticuerpos (solubles en los humores), aunque en la actualidad incluye a todas aquellas moléculas solubles que son elementos fundamentales de la respuesta inmunitaria (complemento, citocinas, péptidos antimicrobianos, etc). En lo referida a las inmunoglobulinas se define como la interacción entre los anticuerpos y los antígenos. Los anticuerpos son proteínas específicas liberadas de cierta clase de células inmunitarias (linfocitos B). Los antígenos son definidos como los elementos capaces de iniciar una respuesta inmunitaria y convertirse en diana de la misma mediante la generación de anticuerpos específicos. La inmunología trata de comprender las propiedades de estas dos entidades biológicas. Sin embargo, igualmente importante es la respuesta celular, que puede no solamente matar a las células infectadas, sino que también es crucial en el control de la respuesta de los anticuerpos. Se observa entonces que ambos sistemas (humoral y celular) son altamente interdependientes.

En el siglo XXI, la inmunología está ampliando sus horizontes con las investigaciones desarrolladas en otros nichos más especializados. Esto incluye la función inmunitaria de las células, órganos y sistemas normalmente no asociados con el sistema inmunitario, así como la función del sistema inmunitario fuera de los modelos clásicos de inmunidad.

La inmunología clínica es el estudio de las enfermedades cuya causa por los trastornos del sistema inmunitario (fallo, acción anormal y crecimiento maligno de los elementos celulares del sistema). También involucra enfermedades de otros sistemas, donde las reacciones inmunitarias juegan un papel en los rasgos clínicos y patológicos.

Las enfermedades causadas por los trastornos del sistema inmunitario se dividen en tres amplias categorías:
La enfermedad más conocida que afecta al sistema inmunitario es el sida, causado por el VIH. El sida es una inmunodeficiencia caracterizada por la pérdida de células T CD4+ ("helper") y macrófagos, que son destruidos por el VIH.

Los inmunólogos clínicos también estudian las formas de prevenir el rechazo a trasplantes, en el cual el sistema inmunitario reacciona frente a las proteínas HLA de los donantes (cuando no son totalmente compatibles).

El uso de los componentes del sistema inmunitario en el tratamiento a una enfermedad o trastornos conocido como inmunoterapia. La inmunoterapia nació con la variolización, y científicamente hablando desde las primeras vacunas de Edward Jenner.

Podemos hablar de Inmunoterapia humoral y celular. Entre los tratamientos humorales, está el uso de Inmunoglobulinas en diferentes formatos: se llevan utilizando décadas en estados de inmunodeficiencia, antisueros como los anti-tetánicos o anti-venenos. Y más modernamente los anticuerpos monoclonales. En 1984 ganaron el Premio Nobel en Medicina los inmunólogos Jerne, Kollher y Mistein por descubrir la fabricación in vitro de estas balas terapéuticas (que también han permitido un desarrollo exponencial de las técnicas de Inmunodiagnóstico basadas en la reacción antígeno-anticuerpo). Estos anticuerpos permiten el tratamiento de pacientes con patologías autoinmunitarias o autoinflamatorias, inmunodeficiencias, hipersensibilidades, y cánceres.

Esta última aplicación, es sin duda la que ha sufrido la mayor revolución en los últimos años. Así que es frecuente que el concepto de inmunoterapia se use en el contexto del tratamiento de los cánceres junto con la quimioterapia (drogas) y la radioterapia (radiación). La inmunoterapia anti-tumoral ya se ha definido como la gran revolución médica del siglo XXI. Aunque se han desarrollado muchos anticuerpos monoclonales con utilidad antitumoral, sin duda las construcciones quiméricas tipo CAR-T y CAR-NK son las que se expandirán de modo más inmediato.

La especificidad del enlace entre antígeno y anticuerpo ha creado una herramienta excelente en la detección de las sustancias en una variedad de técnicas diagnósticas. Los anticuerpos específicos para determinado antígeno pueden ser conjugados con un radio-marcador, marcador fluorescente, o una enzima reveladora (por escala de color) y son usados como pruebas para detectarlo. De modo genérico estas pruebas se denominan Inmunoensayos o Inmunoanálisis. Una vez producidos los complejos antígeno-anticuerpo, estas técnicas los ponen de manifiesto: 


El estudio del sistema inmunitario en especies extintas y vivientes es capaz de darnos una clave en la comprensión de la evolución de las especies y el sistema inmunitario.

Un desarrollo de complejidad del sistema inmunitario pueden ser visto desde la protección fagocítica simple de los organismos unicelulares, la circulación de los péptidos antimicrobianos en insectos y los órganos linfoides en vertebrados. Por supuesto, como muchas de las observaciones evolutivas, estas propiedades físicas son vistas frecuentemente a partir de la mirada antropocéntrica. Debe reconocerse que, cada organismo vivo hoy tiene un sistema inmunitario absolutamente capaz de protegerlo de las principales formas de daño.

Los insectos y otros artrópodos, que no poseen inmunidad adaptativa verdadera, muestran sistemas altamente evolucionados de inmunidad innata, y son protegidos adicionalmente del daño externo (y la exposición a patógenos) gracias a su cutícula.

Rama de la inmunología que estudia no solo los fenómenos inmunológicos en el cerebro, sino también los centros nerviosos que intervienen en la respuesta inmune.




</doc>
<doc id="9743" url="https://es.wikipedia.org/wiki?curid=9743" title="Quiasma">
Quiasma

El quiasma (del gr. χίασμα, -ατος, disposición cruzada, como la de la letra χ) es el puente 
entre cromátidas no hermanas en el proceso de recombinación meiótica, tal como puede ser visualizado citogenéticamente (el entrecruzamiento es exclusivo cromosomas homólogos entre sus cromátidas no hermanas).
En una meiosis humana masculina pueden observarse un promedio de cincuenta y dos quiasmas repartidos uniformemente entre los 23 pares de cromosomas homólogos. En la meiosis femenina se producen más quiasmas (98) que presenta un promedio próximo a un quiasma. Cada una de las cromátidas hermanas tiene un 50% de probabilidad de establecer un quiasma concreto, por lo que la fracción máxima de recombinación (frecuencia de recombinación) esperable entre dos locus es de 0,5.


</doc>
<doc id="9744" url="https://es.wikipedia.org/wiki?curid=9744" title="Enfermedad genética">
Enfermedad genética

Una enfermedad o trastorno genético es una afección patológica causada por una alteración del genoma. Esta puede ser hereditaria o no; si el gen alterado está presente en los gametos (óvulos y espermatozoides) de la línea germinal, esta será hereditaria (pasará de generación en generación), por el contrario si solo afecta a las células somáticas, no será heredada. Pueden ser monogénicas, poligénicas o cromosomicas

Hay varias causas posibles:

Los 46 cromosomas humanos (22 pares de autosomas y 1 par de cromosomas sexuales) entre los que albergan casi 3000 millones de pares de bases de ADN que contienen alrededor de 80.000 genes que codifican proteínas. Las regiones que codifican ocupan menos del 5 % del genoma (la función del resto del ADN permanece desconocida), teniendo algunos cromosomas mayor densidad de genes que otros.

Uno de los mayores problemas es encontrar cómo los genes contribuyen en el complejo patrón de la herencia de una enfermedad, como ejemplo el caso de la diabetes, asma, cáncer y enfermedades mentales. En todos estos casos, ningún gen tiene el potencial para determinar si una persona padecerá o no la enfermedad.

Poco a poco se van conociendo algunas enfermedades cuya causa es la alteración o mutación de todo o alguna región de un gen. Estas enfermedades afectan generalmente a todas las células del cuerpo.

Unas de las enfermedades genéticas más común es el Síndrome Down. Esta enfermedad se produce cuando hay error en la división de las células provoca que haya 47 cromosomas, en lugar de 46.

















</doc>
<doc id="9745" url="https://es.wikipedia.org/wiki?curid=9745" title="Fibrosis quística">
Fibrosis quística

La fibrosis quística (abreviatura FQ) es una enfermedad genética de herencia autosómica recesiva que afecta principalmente a los pulmones, y en menor medida al páncreas, hígado e intestino, provocando la acumulación de moco espeso y pegajoso en estas zonas. Es uno de los tipos de enfermedad pulmonar crónica más común en niños y adultos jóvenes, y es un trastorno potencialmente mortal; los pacientes suelen fallecer por infecciones pulmonares debido a "Pseudomonas "o "Staphylococcus".

Es producida por una mutación en el gen que codifica la proteína "reguladora de la conductancia transmembrana de la fibrosis quística" (CFTR). Esta proteína interviene en el paso del ion cloro a través de las membranas celulares y su deficiencia altera la producción de sudor, jugos gástricos y moco. La enfermedad se desarrolla cuando ninguno de los dos alelos es funcional. Se han descrito más de 1500 mutaciones para esta enfermedad, la mayoría de ellas son pequeñas deleciones o mutaciones puntuales; menos de un 1 % se deben a mutaciones en el promotor o a reorganizaciones cromosómicas.

La FQ afecta a múltiples órganos y sistemas, originando secreciones anómalas y espesas de las glándulas exocrinas. La principal causa de morbilidad y mortalidad es la afectación pulmonar, causante del 95 % de los fallecimientos, sobre todo por infecciones repetidas originadas por obstrucción bronquial debida a la secreción de mucosidad muy espesa. Otros órganos afectados son el páncreas y en ocasiones el testículo.
Es una de las enfermedades genéticas más frecuentes en la raza caucásica, con una incidencia en dicha población de aproximadamente 1/5000 nacidos vivos. Se calcula que una de cada 25 personas de ascendencia europea, es portadora de un alelo no funcional.

El nombre "fibrosis quística" hace referencia a los procesos característicos de cicatrización (fibrosis) y formación de quistes dentro del páncreas, reconocidos por primera vez en 1930. También recibe la denominación de mucoviscidosis (del lat. "muccus", "moco", y "viscōsus", "pegajoso").

Los enfermos presentan una alta concentración de sal (NaCl) en el sudor, lo que permite llegar al diagnóstico mediante su análisis, realizando el test del sudor. También mediante pruebas genéticas prenatales, natales a través de gibson y cooke.

No existe ningún tratamiento curativo, sin embargo hay tratamientos que permiten la mejora de los síntomas y alargar la esperanza de vida. En casos severos, el empeoramiento de la enfermedad puede imponer la necesidad de un trasplante de pulmón. La supervivencia media a nivel mundial de estos pacientes se estima en 35 años, alcanzando valores más altos en países con sistemas sanitarios avanzados; por ejemplo en Canadá la duración media de la vida era de 48 años en 2010.

La sintomatología de la fibrosis quística varía en función de la edad del individuo, el grado en que se ven afectados órganos específicos, la terapéutica instituida previamente, y los tipos de infecciones asociadas. Esta enfermedad compromete al organismo en su totalidad y muestra su impacto sobre el crecimiento, la función respiratoria, la digestión. El periodo neonatal se caracteriza por un pobre aumento de peso y por obstrucción intestinal producida por heces densas y voluminosas. Otros síntomas aparecen, más tarde, durante la niñez y al inicio de la edad adulta. Estos incluyen retardo del crecimiento, advenimiento de la enfermedad pulmonar, y dificultades crecientes por la mala absorción de vitaminas y nutrientes en el tracto gastrointestinal.

A la mayoría de los niños se les diagnostica fibrosis quística antes del primer año de vida, cuando la mucosidad pegajosa que afecta pulmones y páncreas, comienza a mostrar su impacto. En el tracto respiratorio, esas secreciones sirven como caldo de cultivo para diversas bacterias responsables de infecciones crónicas, con deterioro progresivo y permanente del parénquima pulmonar. Conforme se agrava la condición respiratoria, los pacientes sufren hipertensión pulmonar. Por otra parte, en el páncreas, el moco obstruye el tránsito de las enzimas sintetizadas por la glándula e impide que lleguen hasta el intestino para digerir y absorber el alimento.

La enfermedad pulmonar resulta del bloqueo de las vías aéreas más pequeñas con el moco espeso característico de la fibrosis quística. La inflamación y la infección producen daño a los pulmones y cambios estructurales que conducen a una variedad de síntomas. En las etapas iniciales, comúnmente se presentan tos incesante, producción copiosa de flema, y una disminución en la capacidad aeróbica. Muchos de estos síntomas ocurren cuando ciertas bacterias (fundamentalmente, "Pseudomonas aeruginosa") que normalmente viven en el moco espeso, crecen en forma descontrolada y causan neumonía. En estados avanzados de la FQ, los cambios en la arquitectura del pulmón producen dificultades respiratorias crónicas.

Otros síntomas incluyen expectoración de sangre o esputo sanguinolento, dilatación crónica de los bronquios o bronquiolos (bronquiectasia), elevación de la presión sanguínea en el pulmón, insuficiencia cardíaca, sensación de no estar recibiendo suficiente oxígeno o disnea, insuficiencia respiratoria y atelectasia; podría requerirse soporte ventilatorio. Además de las infecciones bacterianas más comunes, las personas con FQ desarrollan con mayor facilidad otros tipos de enfermedades respiratorias. Entre estas se encuentra la aspergilosis broncopulmonar alérgica, caracterizada por una respuesta de hipersensibilidad ante un hongo (moho) ordinario del género "Aspergillus" ("Aspergillus fumigatus"), que agudiza los problemas respiratorios. Otro ejemplo es la infección con el complejo "Mycobacterium avium" (MAC), grupo de actinobacterias emparentadas con "Mycobacterium tuberculosis", que puede ocasionar daños mayores al pulmón, y que no responde a la terapéutica con antibióticos convencionales.

El moco en los senos paranasales es igualmente denso y pegajoso, y también puede causar oclusión de los orificios por donde los senos habitualmente drenan, lo cual hace que se acumulen secreciones que actúan como caldo de cultivo para los patógenos antes mencionados. En estos casos, se pueden presentar dolor facial, fiebre, secreción nasal profusa y cefaleas. En las personas con FQ, a menudo se observa crecimiento sobreabundante de tejido nasal (pólipos), a consecuencia de la inflamación por infección sinusal crónica. Estos pólipos pueden agravar la obstrucción de las vías respiratorias superiores e intensificar las dificultades respiratorias.

Con anterioridad a la difusión de las pruebas prenatal y neonatal para Fibrosis Quística, era frecuente que la enfermedad se detectara al constatar que el recién nacido no podía expulsar sus primeras heces (meconio). El meconio puede obstruir completamente los intestinos y causar graves trastornos. Esta condición, llamada íleo meconial, ocurre en el 10 % de los recién nacidos con FQ. Recientemente se han identificado variantes genéticas en genes relacionados con el transporte de iones en el intestino delgado que predisponen al desarrollo del íleo meconial. Asimismo, es también frecuente la asociación de FQ con protrusión de las membranas rectales internas (prolapso rectal), debida al mayor volumen fecal, a la malnutrición, y a la elevación de la presión intraabdominal por tos crónica.

El moco glutinoso observado en el pulmón tiene su correlato en las secreciones espesas del Páncreas, órgano responsable de proveer jugos digestivos que facilitan la descomposición química de los alimentos. Estas secreciones impiden el movimiento de las enzimas pancreáticas hacia el intestino y producen daño irreversible en el páncreas, a menudo acompañado de dolorosa inflamación (pancreatitis). La deficiencia de enzimas digestivas se traduce en un impedimento para absorber los nutrientes, con la subsiguiente excreción de estos en las heces: este trastorno es conocido como malabsorción. La malabsorción conduce a la desnutrición y al retardo en el crecimiento y desarrollo, ambos debidos a la baja biodisponibilidad calórica. Las personas con FQ tienen, en particular, problemas para absorber las vitaminas A, D, E, y K. Además de la afección pancreática, suelen experimentar acidez crónica, xerostomía, obstrucción intestinal por intususcepción, y constipación. Los pacientes mayores desarrollan también el síndrome de obstrucción intestinal distal causado por las heces glutinosas.

Estas secreciones también pueden causar problemas en el hígado. La bilis, producida por esta víscera para facilitar la digestión, podría bloquear las vías biliares, dañando los tejidos adyacentes. Con el tiempo, esta situación conduce a la cirrosis. En ese caso, resultan comprometidas funciones de primer orden, tales como las implicadas en la neutralización de toxinas, y en la síntesis de importantes proteínas (por ejemplo, los factores de coagulación, responsables de la coagulación sanguínea).

El páncreas contiene los islotes de Langerhans, que son los responsables de producir insulina, una hormona que ayuda a regular los niveles de glucosa en sangre. Un daño en el páncreas puede provocar la pérdida de las células de los islotes y conducir a la diabetes. Por otra parte, la vitamina D suplementada por la alimentación está implicada en la regulación del calcio y del fósforo. La baja disponibilidad de ésta, a causa de la mala absorción, conduce a la osteoporosis, aumentando el riesgo de sufrir fracturas. Adicionalmente, las personas con FQ a menudo presentan, en manos y pies, una malformación denominada dedos en palillo de tambor, la cual se debe a los efectos de esta enfermedad crónica y a la hipoxia en sus huesos.

El retardo en el crecimiento es un sello distintivo de esta enfermedad. Los niños con FQ no logran, por lo general, ganar peso y altura en tasas comparables a las de sus pares; a menudo, solo reciben diagnóstico apropiado una vez que se investigan las causas de este fenómeno. Las determinantes del retardo en el crecimiento son multifactoriales e incluyen la infección pulmonar crónica, la malabsorción de nutrientes en el tracto gastrointestinal, y el aumento de la demanda metabólica asociado a la afección crónica.

La fibrosis quística puede diagnosticarse por tamizaje en recién nacidos, examen de electrolitos del sudor, o prueba genética. Al año 2006, en los Estados Unidos, el diez por ciento de los casos son detectados poco después del nacimiento (deseable no después del quinto día de vida) como parte de los programas de pesquisa neonatal, los cuales identifican niveles elevados en la enzima tripsina mediante TIR (inmunoreactive trypsinogen test). Si la prueba da positiva ( niveles menores o iguales a 60 ng/ml), se deberá repetir al 24-28 día de vida para comprobar que los niveles siguen siendo superiores o iguales a 50 ng/ml (cribado positivo) , de lo contrario el cribado resultará finalmente negativo. Sin embargo, en la mayoría de los países estos exámenes no se realizan en forma rutinaria. Por esta causa, es frecuente que los afectados solo reciban diagnóstico apropiado una vez que los síntomas fuerzan una evaluación para esta enfermedad. La prueba diagnóstica más comúnmente utilizada es el examen del sudor, descrito por Lewis E. Gibson y Robert E. Cooke en 1959, usando electroforesis cuantitativa (iontoforesis) con un fármaco estimulante de la sudoración (pilocarpina). Esta sustancia, que posee carga positiva, se aplica sobre un electrodo positivo (+), en contacto con la piel. Luego, mediante el paso de corriente eléctrica, la droga migra por el tegumento hacia otro electrodo de carga opuesta (-), colocado a cierta distancia, hasta atravesar la epidermis, produciendo la estimulación de las glándulas sudoríparas y causando una sudoración controlada. Las muestras de sudor son luego recolectadas en papel de filtro o en un tubo capilar y son analizadas, determinándose las concentraciones de sodio y cloruro. Las personas con FQ poseen niveles más altos de estos iones en el sudor. Una vez que el examen del sudor ha dado positivo, se realiza un diagnóstico más detallado y preciso, mediante la identificación de las mutaciones en el gen CFTR.

Existen diversas pruebas para identificar eventuales complicaciones y controlar la evolución de la enfermedad. Las imágenes obtenidas por rayos X y TAC facilitan la detección de signos de lesión o infección en los pulmones. El cultivo de esputo, examinado por microscopio, provee información respecto de cuáles son las bacterias responsables, y permite escoger los antibióticos más efectivos. Las pruebas de función pulmonar miden las capacidades pulmonares, los volúmenes pulmonares y la rapidez con que estos pueden ser movilizados (flujos aéreos). Por medio de tales exámenes, es posible determinar si es procedente un tratamiento con antibióticos o bien evaluar la respuesta al mismo. Los análisis de sangre pueden identificar problemas hepáticos, deficiencias vitamínicas, y revelar la irrupción de la diabetes. Los dispositivos DEXA o DXA (del inglés para "absorciometría de rayos X de energía dual"), se utilizan como prueba para determinar la presencia de osteoporosis. Por último, la cuantificación de elastasa fecal, facilita la detección de insuficiencia de enzimas digestivas.

La proteína sintetizada a partir del gen CFTR se une a la membrana externa de las células en las glándulas sudoríparas, Pulmón, Páncreas, y otros órganos afectados. La proteína atraviesa esta membrana y actúa como un canal iónico conectando la parte interna de la célula (citoplasma) con el fluido extracelular. Este canal es mayormente responsable de controlar el paso de cloruro hacia (y desde) el medio interno. Cuando la proteína CFTR no funciona correctamente, este movimiento se ve restringido, reteniéndose cloruro en el espacio extracelular. Debido a que el cloruro tiene carga eléctrica negativa, los iones con carga positiva tampoco podrán cruzar la membrana citoplasmática, a causa de la atracción eléctrostática ejercida por los iones cloruro. El sodio es el más común entre los iones presentes fuera de la célula, y la combinación de sodio y cloruro da lugar al cloruro de sodio, el cual se pierde en grandes cantidades en el sudor de los individuos con FQ. Esta pérdida de sal constituye el argumento básico para explicar la utilidad diagnóstica del test del sudor.

El mecanismo por el cual esta disfunción celular produce las manifestaciones clínicas antes descritas no se conoce con exactitud. Una de las teorías que intenta explicarlo, sugiere que la falla de la proteína CFTR para transportar el cloruro, determina la acumulación de abundante moco en los pulmones, creando un medio propicio (rico en nutrientes) para las bacterias, que logran así eludir al sistema inmunitario. También se postula que esta anomalía en la proteína CFTR induce un aumento paradójico en la captura de sodio y cloruro, lo que estimula la reabsorción de agua, y resulta en la formación de la mucosidad deshidratada y espesa. Otras teorías se enfocan en el fenómeno del movimiento de cloruro hacia el exterior de la célula, que también provoca desecamiento del moco y de las secreciones pancreáticas y biliares. En general, estas hipótesis coinciden en atribuir los mayores trastornos a la obstrucción de los conductos más delgados por las secreciones espesas y glutinosas en los distintos órganos afectados. Esta situación condiciona la infección crónica y promueve la remodelación estructural del pulmón, además de producir daño pancreático (mediado por las enzimas digestivas aglomeradas), y obstrucción de los intestinos por grandes bolos fecales.

Los pulmones de las personas con Fibrosis Quística son colonizados e infectados por bacterias desde edades tempranas. Los microorganismos que se propagan en estos pacientes, prosperan en el moco anómalo acumulado en las vías respiratorias más estrechas. El moco glutinoso estimula el desarrollo de microambientes bacterianos ("biofilms") que resultan difíciles de penetrar para las células inmunes y los antibióticos. Por su parte, los pulmones responden al daño continuo, infligido por las secreciones espesas y las infecciones crónicas, remodelando gradualmente las vías respiratorias inferiores (bronquiectasia), lo que vuelve a la infección aún más difícil de erradicar.

Con el paso del tiempo, cambian tanto el tipo de bacterias que afectan a estos pacientes, como las características específicas con que las mismas se presentan. En una primera etapa, ciertas bacterias ordinarias como "Staphylococcus aureus" y "Haemophilus influenzae" colonizan e infectan los pulmones. Más tarde, sin embargo, prevalecen "Pseudomonas aeruginosa" (y, a veces, el complejo "Burkholderia cepacia", integrado por diferentes especies de "Burkholderia"). Una vez diseminadas por las vías respiratorias, estas bacterias se adaptan al medio y desarrollan resistencia a los antibióticos convencionales. "Pseudomonas" puede adquirir ciertas características especiales, dando lugar a la formación de grandes colonias — estas cepas son conocidas como "Pseudomonas" "mucoide" y son raras en personas libres de la enfermedad.

Uno de los modos en que la infección se propaga es por transmisión entre individuos con FQ. En el pasado, era habitual que estos participaran, en forma conjunta, de campamentos veraniegos y otras actividades de esparcimiento. Los hospitales alojaban a los pacientes con FQ en un área en común, y el equipamiento de rigor (por ejemplo, los nebulizadores) no era esterilizado entre usos sucesivos. Esto condujo a la transmisión de cepas bacterianas muy peligrosas entre grupos de pacientes. Actualmente, la rutina en establecimientos de atención sanitaria consiste en aislar a estos pacientes unos de otros; además, el personal a cargo de su cuidado, debe vestir batas y guantes para limitar la proliferación de cepas bacterianas virulentas. Con frecuencia, los pacientes afectados por bacterias particularmente peligrosas reciben atención en días y en edificios diferentes a los asignados a quienes no tienen esas infecciones.
Además de la infección bacteriana, los pacientes con FQ están predispuestos a la colonización fúngica por la capacidad que tiene algunos hongos de colonizar la vía respiratoria inferior y por los frecuentes ciclos de antibióticos que precisan para el control de la enfermedad. Los hongos que se cultivan con más frecuencia son el "Aspergillus fumigatus" y la "Candida albicans", esta colonización se traduce en una tasa elevada de respuesta inflamatoria frente a los hongos. En la actualidad no está bien definido el papel de los hongos en la FQ, aunque se consideran que son no patógenos, excepto en los casos de aspergilosis invasiva y de aspergilosis broncopulmonar alérgica.

Se trata de una enfermedad autosómica recesiva. En su forma más común, una mutación de un aminoácido (falta una fenilalanina en la posición 508) conduce a un fallo del transporte celular y localización en la membrana celular de la proteína CFTR. Se han descrito más de 1800 mutaciones, siendo la mayoría de ellas pequeñas deleciones, aunque con diferentes efectos, como cambios en el marco de lectura, cambios de aminoácidos, terminación prematura de la proteína o alteraciones en el empalme (splicing) de ARNm.

El gen CFTR está localizado en el brazo largo del cromosoma 7, en la posición 7q31.2, ocupando 180 000 pares de bases: más precisamente, desde el par 116 907 252 al 117 095 950 del cromosoma. Es un gen de gran tamaño, que posee 250 kb y que incluye 27 exones. Fue localizado y secuenciado por mapeo genético.

Este gen codifica la síntesis de un canal iónico de 1480 aminoácidos, una proteína que transporta iones cloruro a través de las células epiteliales, y que controla la regulación de otros transportadores. En las personas con fibrosis quística, esta proteína está ausente o bien se encuentra en proporciones sensiblemente menores a las habituales.

La penetrancia de la enfermedad es variable según el alelo, y a su vez, la expresión del alelo depende del entorno y del genoma de la persona afectada.

Son diversos los mecanismos por los cuales estas mutaciones causan problemas en la proteína CFTR. En particular, la mutación ΔF508, genera una proteína que no se pliega de manera normal y acaba siendo degradada por la célula. Varias mutaciones comunes en la población askenazí dan lugar a la síntesis de proteínas demasiado cortas, a causa de una conclusión anticipada de su producción. Otras mutaciones menos frecuentes originan proteínas que no utilizan la energía como es debido, no permiten que el cloruro cruce la membrana apropiadamente, o son degradadas a una tasa más rápida que la normal. La deficiencia en el transporte de cloro hace que las células no expulsen agua al exterior y por lo tanto el moco sea más espeso. Ciertas mutaciones pueden conducir también a una merma en la producción de copias de la proteína CFTR.

Estructuralmente, el gen CFTR pertenece a la denominada superfamilia de transportadores ABC (acrónimo para el inglés "ATP Binding casete", "casete de unión a ATP"). La estructura terciaria de la proteína codificada por este gen, consta de dos dominios capaces de hidrolizar adenosín trifosfato, lo que permite a la proteína utilizar energía en la forma de ATP. Asimismo, otro par de dominios, cada uno constituido por seis hélices alfa, posibilita el paso de la proteína a través de la membrana celular. La activación se concreta por reacción de fosforilación en un sitio de unión regulador, sobre todo mediante la proteína quinasa A (PKA, —antes denominada cAPK o proteína quinasa dependiente del adenosín monofosfato cíclico). El carboxilo terminal (C-) de la proteína está unido al citoesqueleto por interacción con dominios proteicos PDZ.

Existen una serie de pruebas que se vienen realizando de forma común para determinar las anomalías de los metabolitos relacionados con la fibrosis quística (especialmente el cloro). Entre ellas se encuentran:

El Diagnóstico Molecular de la enfermedad es complejo, ya que en noviembre de 2010 nos encontramos con 1824 mutaciones descritas y la mayoría son puntuales o pequeñas deleciones. Estas mutaciones se agrupan en función del efecto que tienen sobre el gen y sobre el fenotipo de la enfermedad. Además de la variabilidad de las mutaciones en sí mismas, las distintas poblaciones tienen frecuencias diferentes para las mismas, por lo que los estudios y test diagnósticos deben gestionarse considerando este aspecto. No obstante, la más común en la mayoría de las poblaciones es la deleción 508F.

Hay que tener en cuenta que la distribución de alelos varía mucho en cada población, por lo que hay que adaptar los tests para detectar aquellas variantes más comunes en la población que se esté estudiando.

Actualmente a los niños nada más nacer se les hace un diagnóstico genético mediante secuenciación del gen CFTR para saber si tienen la enfermedad, ya que es una enfermedad tratable. Cuando antes comienza el tratamiento, mayor calidad de vida y mayor longevidad.

Las parejas que están atravesando un embarazo o tienen planes respecto de la gestación, pueden ser evaluadas en busca de mutaciones del gen CFTR, con el objeto de determinar las probabilidades de que su hijo nazca con fibrosis quística. La prueba se suele realizar en uno de los padres o en ambos y, en caso de detectarse un riesgo elevado de FQ, se efectúa también en el feto. Debido a que el diagnóstico prenatal no habilita formas de tratamiento superiores o alternativas, la principal razón por la que se lleva a cabo es, en la práctica, proporcionar la posibilidad del aborto en caso de que el feto presente la enfermedad. La prueba para fibrosis quística en parejas se ofrece de manera generalizada en países como los Estados Unidos, y el Colegio Americano de Obstetras y Ginecólogos (ACOG, por sus siglas en inglés) recomienda la prueba en parejas que poseen un historial de FQ entre sus familiares directos o parientes cercanos, así como también en aquellas con riesgo elevado debido a su filiación étnica.

Debido a que el desarrollo de la FQ en el feto requiere que cada padre transmita una copia del gen CFTR mutante, y al alto costo del examen prenatal, la prueba suele realizarse, inicialmente, solo en uno de los progenitores. Si éste resulta ser portador de una mutación del gen CFTR, entonces se examina al otro para determinar el riesgo de que su hijo tenga la enfermedad. La FQ puede resultar de más un millar de mutaciones diferentes y, al año 2006, no es posible efectuar estudios de laboratorio para cada una de ellas. La prueba se remite a analizar la sangre en busca de las más comunes, como ΔF508 —la mayoría de las modalidades disponibles comercialmente detectan no más de 32 variantes distintas—. Si se conoce el dato de que una familia tiene una mutación poco común, esta última puede buscarse específicamente. Como consecuencia de que no todas las mutaciones conocidas son detectadas por las pruebas corrientes, un resultado negativo no garantiza que el niño vaya a estar libre de la enfermedad. Por otro lado, dado que las mutaciones sondeadas son necesariamente aquellas más comunes en los grupos de más alto riesgo, las pruebas en etnias de bajo riesgo son menos exitosas, ya que las mutaciones más extendidas en estos grupos son menos frecuentes en la población general.

Las parejas en situación de riesgo, a menudo realizan pruebas adicionales durante el embarazo o antes de que éste se produzca. La fecundación in vitro con diagnóstico genético preimplantacional ofrece la posibilidad de examinar el embrión antes de su colocación en el útero. Esta prueba se realiza tres días después de la fecundación y procura determinar la presencia de genes CFTR anormales. Si, en un embrión, resultan identificados dos genes CTRF mutantes, éste será excluido de la transferencia, implantándose otro que cuente con, al menos, un gen normal.

Durante el transcurso del embarazo, es posible realizar pruebas tanto sobre la placenta (muestra de vellosidad coriónica) como sobre el líquido amniótico que rodea al feto (amniocentesis), con la ayuda del ultrasonido. Sin embargo, la biopsia de vellosidades coriónicas se correlaciona con riesgo de muerte fetal en una tasa de 1 en 100, y la amniocentesis, de 1 en 200, por lo que es esencial determinar los beneficios adecuadamente para sopesar los riesgos, antes de proceder con la prueba. Alternativamente, algunas parejas eligen someterse a técnicas de reproducción asistida con óvulos donantes (recurriendo a la fecundación in vitro) o con esperma donante (inseminación artificial por donante).

Un aspecto fundamental en la terapéutica de la Fibrosis Quística es el control y tratamiento del daño pulmonar causado por el moco espeso y por las infecciones, con el objeto de mejorar la calidad de vida del paciente. Para el tratamiento de las infecciones crónicas y agudas se administran antibióticos por vías intravenosa, inhalatoria y oral. También se utilizan dispositivos mecánicos y fármacos (en forma de inhaladores) para controlar las secreciones, y de esta manera descongestionar y desobstruir las vías respiratorias. Otros aspectos de la terapia se relacionan con el tratamiento de la diabetes con insulina, de la enfermedad pancreática con reemplazo enzimático. Adicionalmente, se postula la eficacia de distintos procedimientos, como el trasplante y la terapia génica, para resolver algunos de los efectos asociados a esta enfermedad.

Una dieta sana, elevado ejercicio y tratamientos agresivos con antibióticos está aumentando la esperanza de vida de los enfermos.

Los antibióticos se prescriben siempre que exista sospecha de neumonía o de constante deterioro en la función pulmonar. Habitualmente, se los escoge en función del historial de infecciones que afectaron al paciente previamente. Muchas de las bacterias comunes en la fibrosis quística son resistentes a gran cantidad de antibióticos y requieren semanas de tratamiento intravenoso con vancomicina, tobramicina, meropenem, ciprofloxacina y piperacilina.
La terapia prolongada a menudo requiere hospitalización y canalización de una vía intravenosa permanente, como por ejemplo un catéter central insertado percutáneamente (PICC). Asimismo, es frecuente la indicación simultánea de antibióticos administrados por inhalación, como la tobramicina, la colistina y la gentamicina, por varios meses, con el objeto de mejorar la función pulmonar impidiendo la proliferación bacteriana. Algunos antibióticos orales como la ciprofloxacina o la azitromicina se utilizan a veces para ayudar a prevenir la infección o para controlarla una vez que está en curso. En algunos casos pasan años entre sucesivas hospitalizaciones, mientras que en otros se requiere la internación cada año para poder realizar el tratamiento.

En tratamientos prolongados, varios de los antibióticos más comunes (como la tobramicina y la vancomicina) pueden causar pérdida de audición por ototoxicidad o problemas en los riñones. Con el objeto de prevenir tales efectos secundarios, es habitual medir cuantitativamente las concentraciones de estos medicamentos en sangre y, de ser necesario, ajustar la dosificación.

Son diversas las técnicas que se implementan con el objeto de fluidificar el esputo y facilitar su expectoración. En el medio hospitalario se utiliza la fisioterapia; un terapeuta practica una serie de maniobras mediante presiones y percusiones (palmoteo) ejercidas sobre el exterior del pecho (tórax) varias veces al día. Los dispositivos mecánicos que actúan bajo el mismo principio que aquellas técnicas básicas de drenaje postural, incluyen el ventilador de alta frecuencia oscilatoria y los aparatos de ventilación percusiva intrapulmonar, de los que existen modelos portátiles, adaptables al uso hogareño. El ejercicio aeróbico es altamente beneficioso para las personas con fibrosis quística, ya que no solo promueve la descongestión del esputo, sino que mejora la salud cardiovascular y el estado general.
Entre las sustancias administradas por inhalación que ayudan a aligerar las secreciones y facilitan su expulsión, se encuentran la dornasa alfa y la solución salina hipertónica. La dornasa alfa es una desoxirribonucleasa (ADNasa o DNasa) humana recombinante, que descompone el ADN en el esputo, reduciendo así la viscosidad de este último. La N-acetilcisteína (un derivado del aminoácido cisteína) también actúa fluidificando el esputo, pero las investigaciones y la experiencia disponibles han demostrado que los beneficios son poco significativos. Por último, broncodilatadores como el salbutamol y el salmeterol (ambos agentes, agonistas β-adrenérgicos) o el bromuro de ipratropio (un antagonista del receptor colinérgico, derivado cuaternario de la atropina) se utilizan para aumentar el tamaño de las vías respiratorias más pequeñas, al relajar el músculo liso bronquial.

En la medida en que se agrava el cuadro pulmonar puede requerirse soporte respiratorio mecánico. Por las noches, algunos pacientes deben usar máscaras especiales que actúan empujando el flujo aéreo hasta los pulmones. La ventilación no invasiva mediante máscara nasal y presión positiva (VPAP, por el acrónimo para el inglés "variable positive airway pressure"), ayuda a prevenir, durante el sueño, caídas significativas en los niveles sanguíneos de oxígeno. También puede usarse en el curso de la fisioterapia respiratoria para favorecer la expulsión de esputo. Sin embargo, en casos severos, puede ser necesario implementar formas invasivas de asistencia respiratoria con intubación endotraqueal (esto es, colocación de un tubo o sonda en la tráquea).

Los recién nacidos con íleo meconial típicamente requieren cirugía; por lo general, no sucede lo mismo en adultos con síndrome de obstrucción intestinal distal. El tratamiento de la insuficiencia pancreática basado en reemplazo de las enzimas digestivas menguadas permite que los intestinos absorban de manera apropiada nutrientes y vitaminas que, de otro modo, se perderían en la heces. Aun así, la mayoría de los individuos con FQ deben recibir dosis adicionales de vitaminas A, D, E y K a partir de suplementos, y seguir una dieta de alto valor calórico. La diabetes que suele acompañar la FQ se trata con inyecciones de insulina. El desarrollo de osteoporosis puede prevenirse con la suplementación de vitamina D y calcio, y a menudo se trata con bifosfonatos. En cuanto al retraso en el crecimiento, se procura contrarrestarlo mediante la inserción de un tubo de alimentación (gastrostomía) para aumentar así la ingesta de calorías a partir de nutrición adicional; también se administran con este fin inyecciones de hormona de crecimiento.

Las infecciones de los senos paranasales suelen tratarse con un prolongado régimen de antibióticos. El desarrollo de pólipos, así como otros cambios estructurales de tipo patológico en el interior de los conductos nasales, pueden restringir el flujo aéreo y complicar el cuadro. Por este motivo, es frecuente la práctica quirúrgica en procura de aliviar la obstrucción y limitar el desarrollo de nuevas infecciones. También se administran corticosteroides intranasales, como la fluticasona, para reducir la inflamación. Por otro lado, la infertilidad femenina puede combatirse recurriendo a técnicas de reproducción asistida. Aquella que afecta al hombre también tiene tratamiento: por ejemplo, mediante la inyección intracitoplasmática de esperma.

Por lo general, se considera procedente el trasplante de pulmón en personas con deterioro progresivo de la función pulmonar y creciente intolerancia al ejercicio (fatiga o agotamiento muscular desproporcionados para el ejercicio realizado). Aunque el trasplante de un único pulmón es viable en otras enfermedades, en los pacientes con FQ ambos deben ser reemplazados ya que, de otro modo, las bacterias alojadas en el órgano remanente podrían infectar a aquel que ha sido trasplantado. Asimismo, puede practicarse simultáneamente un trasplante de páncreas o de hígado con el propósito de aliviar la enfermedad hepática o la diabetes. La opción del trasplante de pulmón se evalúa cuando la función pulmonar se ve afectada en grado tal que se vea amenazada la supervivencia o se requiera la asistencia con dispositivos mecánicos.
La terapia génica representa una vía promisoria en la lucha contra la enfermedad. Mediante esta técnica, se procura insertar una copia normal del gen CFTR en las células afectadas. Debido a la incapacidad de los retrovirus para alcanzar células que no se dividen, se han realizado análisis clínicos para insertar genes en adenovirus. En la actualidad, estos virus se están utilizando en ensayos en los que el gen CFTR normal se administra, por un método en aerosol, a las células epiteliales que revisten los pulmones (terapia génica "in vivo"). Se espera que los adenovirus inserten el gen normal, induciendo una función pertinente de los canales de cloro en estas células.

Algunos estudios han señalado que para prevenir las manifestaciones pulmonares de la fibrosis quística, solo se requiere la expresión génica de entre un 5 y un 10 % de los valores normales de proteína CFTR. Un inconveniente de los adenovirus es que no se integran en el ADN de la célula huésped. Por lo tanto, finalmente se pierden, originando una expresión del gen transitoria y la necesidad de reintroducción del vector. Se han propuesto diversos abordajes y se han iniciado numerosos estudios clínicos pero, al año 2006, persisten múltiples obstáculos, que será preciso superar para que la terapia génica resulte exitosa.

La otra aproximación para el tratamiento de la fibrosis quística viene dada por el uso de potenciadores o moduladores de la proteína CFTR reparando el defecto subyacente en la creación de dicha proteína. Existen varios potenciadores o correctores en diferentes fases de investigación entre los que destacan: Ivacaftor, Lumacaftor, Tezacaftor. El potenciador Ivacaftor, antes llamado VX770 y bajo el nombre comercial de Kalydeco, se destinó exclusivamente a pacientes mayores de 6 años inicialmente, y a partir de 2015 fue habilitado por la FDA para pacientes de 2 a 5 años. Está autorizado para el tratamiento de la mutación G551D, teniendo esta menos del 3 % del total de pacientes con fibrosis quística. En 2014 la FDA autorizó este medicamento para el uso en las mutaciones G178R, S549N, S549R, G551S, G1244E, S1251N, S1255P y G1349D.Este modulador puede mejorar la calidad de vida de los pacientes, con incremento de peso y la salud pulmonar, reduciendo la probabilidad de infección. La mejora de FEV1 es promedialmente del 10,4%. El costo de esta terapia supera los 300 000 dólares estadounidenses por año de tratamiento crónico (2014).
La asociación de lumacaftor/ivacaftor (antes llamado VX-809 y bajo el nombre comercial de Orkambi) del mismo laboratorio Vertex Pharmaceuticals ha sido aprobado por la FDA y por la Unión Europea, para los que poseen la mutación ΔF508 (la forma más común) en su forma homocigota (dos copias iguales).Actualmente se mantienen conversaciones con los distintos países de la Unión Europea para negociar el reembolso del medicamento.
Se encuentra en fase de investigación la asociación de ivacaftor/tezacaftor (antes llamado VX-661 y bajo el nombre comercial de Symkevi) para los que poseen la mutación ΔF508 en su forma heterocigota (una copia diferente en cada alelo).

Para las mutaciones del tipo I, generalmente llamadas mutación sin sentido o "non-sense", donde un codón de terminación hace que la creación de la proteína quede trunca en una etapa prematura y por lo tanto no funcional, se están probando medicamentos que han mostrado cierta eficacia logrando que los ribosomas ignoren los codones de terminación prematuros y terminen de generar la proteína CFTR, esta capacidad de los fármacos es llamado "read-through" en la literatura. El medicamento Ataluren (antes llamado PTC124) del laboratorio PTC Therapeutics se encuentra en Etapa III de investigación en la FDA con resultados ambiguos para la generalidad de los pacientes con estas mutaciones, aunque prometedores en los pacientes que no tienen tratamiento crónico con antibióticos.

Otros tipos de medicamentos buscan abrir canales alternos para el cloro en la célula aunque se encuentran en etapas más prematuras de investigación.

Actualmente se han conseguido realizar modelos animales de la enfermedad, mediante la edición y modificación genética, teniendo especial éxito el conseguido en cerdo y hurón. 

Entre las personas de ascendencia europea, la fibrosis quística es la más frecuente de las enfermedades autosómicas recesivas potencialmente fatales. En los Estados Unidos, aproximadamente 30 000 individuos padecen FQ; en su mayoría, son diagnosticados a los seis meses de edad. Canadá tiene cerca de 3000 habitantes con esta condición. Se estima que una de cada 25 personas de ascendencia europea y una de cada 29 personas de ascendencia askenazí son portadores de una mutación de fibrosis quística. Aunque es menos común en estos grupos, aproximadamente uno de cada 46 hispanoamericanos, uno de cada 65 africanos y uno de cada 90 asiáticos son portadores de al menos un gen CFTR anormal. Argentina y Uruguay representan una excepción en el contexto de América Latina, con una incidencia de casos mucho mayor a la media de la región y muy próxima a la registrada en Estados Unidos o Canadá, y una prevalencia de portadores sanos en la población general de entre 1 en 30 y 1 en 25.

La fibrosis quística se diagnostica tanto en hombres como en mujeres. Por razones que solo en parte se conocen, la esperanza de vida al nacer resulta ser mayor entre los varones afectados que entre las mujeres. Aquel indicador tiende a variar principalmente en función del alcance y la calidad de la atención suministrada por los sistemas de salud pública. En 1959, la supervivencia media en niños con FQ era de 6 meses. Para los nacidos en 2006 en los Estados Unidos, este valor treparía a los 36,8 años, de acuerdo a los datos compilados por la Fundación de la Fibrosis Quística. La tasa de esperanza de vida ha evolucionado en forma análoga para buena parte de Occidente, exceptuando los países menos desarrollados, donde se reportan cifras sensiblemente menores, y en los cuales la mayoría de la población afectada no sobrevive más allá de los diez años de edad.

La Fundación de la Fibrosis Quística compila, además, información sobre el estilo de vida de los adultos estadounidenses con FQ. En 2004, la fundación reportó que el 91 % de esta población había completado la enseñanza media, y el 54 % había accedido a alguna forma de educación universitaria. Los datos en materia de empleo revelaron que el 12,6 % de estos adultos estaba imposibilitado para trabajar (quedando fuera de la población económicamente activa), y el 9,9 % estaba desocupado. Por otro lado, la información marital señaló que un 59 % era soltero y un 36 % estaba casado o viviendo en pareja. En 2004, 191 mujeres con FQ se encontraban embarazadas en los Estados Unidos.

Se estima que la mutación ΔF508 puede tener hasta unos 52 000 años de antigüedad. Se han formulado numerosas hipótesis intentando explicar por qué una mutación letal como ésta ha persistido y se ha extendido entre la población humana. Algunas enfermedades autosómicas recesivas comunes como la anemia de células falciformes han revelado la propiedad de proteger a sus portadores de otras afecciones, concepto conocido como "ventaja heterocigota". Con el descubrimiento de que la toxina del cólera requiere que sus huéspedes sean proteínas CFTR normales para poder funcionar apropiadamente, se ha postulado que los portadores de genes CFTR mutantes obtuvieron el beneficio de la resistencia al cólera y a otras causas de diarrea. Sin embargo, estudios posteriores no han confirmado esta hipótesis.

La presencia de proteínas CFTR normales es condición necesaria para el ingreso de "Salmonella typhi" (serotipo de "Salmonella enterica", proteobacteria gram negativa del género "Salmonella") en las células, lo que sugiere que los portadores de genes CFTR mutantes podrían ser resistentes a la fiebre tifoidea. Sin embargo, ningún estudio "in vivo" ha confirmado esta hipótesis. En cualquiera de los casos, la baja incidencia de fibrosis quística fuera de Europa, en sitios donde tanto el cólera como la fiebre tifoidea son endémicos, carece de explicación inmediata.

Aunque el espectro clínico completo de la FQ no fue reconocido hasta los años 1930, ciertos aspectos fueron identificados mucho antes. Carl von Rokitansky describió un caso de muerte fetal con peritonitis meconial, una complicación del íleo meconial asociado con la fibrosis quística. El íleo meconial fue descrito por primera vez en 1905 por Karl Landsteiner.

En 1938, Dorothy Andersen publicó un artículo intitulado «Cystic fibrosis of the páncreas and its relation to celiac disease: a clinical and pathological study» («La fibrosis quística del páncreas y su relación con la enfermedad celíaca: un estudio clínico y patológico») en la revista "American Journal of Diseases of Children". De esta manera, era la primera investigadora en definir esta entidad nosológica (denominada, por aquel entonces, "fibrosis quística del páncreas"), y en correlacionarla con los trastornos pulmonares e intestinales prominentes. También postuló que era una enfermedad recesiva y utilizó el reemplazo de enzimas pancreáticas como tratamiento para los niños afectados. En 1952, Paul di Sant' Agnese descubrió anomalías en los electrolitos del sudor. Sobre la base de esa evidencia, se desarrolló y perfeccionó el examen del sudor durante el curso de la siguiente década.

En 1985, investigadores de Londres, Toronto y Salt Lake City trazaron el mapa del gen CFTR en el cromosoma 7q. Cuatro años más tarde, en 1989, Francis Collins, Lap-Chee Tsui y John R. Riordan descubrieron la primera mutación para la FQ, ΔF508, en ese cromosoma. Investigaciones posteriores a aquel hallazgo, identificaron más de mil mutaciones diferentes que dan origen a la enfermedad. Lap-Chee Tsui lideró el equipo de científicos del "Hospital for Sick Children" (un hospital escuela en convenio con la Universidad de Toronto) que descubrió el gen responsable de la FQ. Se trata del primer trastorno genético dilucidado estrictamente mediante el proceso de genética inversa. Debido a que las mutaciones del gen CFTR son generalmente pequeñas, las técnicas de la genética clásica o formal no fueron capaces de determinar con precisión el gen mutante. Utilizando marcadores proteicos, los estudios de ligamiento genético lograron trazar un mapa de la mutación del cromosoma 7. Las técnicas de paseo y salto cromosómicos sirvieron entonces para identificar y secuenciar el gen. Este gen fue uno de los primeros genes en ser localizado y secuenciado por mapeo genético, y algunos de los participantes en este proyecto, como Francis Collins estuvieron implicados más tarde en el Proyecto Genoma Humano

La identificación de la mutación específica responsable de la FQ en un paciente puede ser útil para predecir la evolución de la enfermedad. Por ejemplo, los pacientes homocigotos para la mutación ΔF508 presentan, en casi todos los casos, insuficiencia pancreática y tienen, por lo general, un grado relativamente severo de afectación respiratoria. Sin embargo, existen excepciones que indican la posibilidad de que factores adicionales (quizás, genes en otros "loci") intervengan en la expresión de la enfermedad. Por otro lado, la clonación del gen de la FQ ha abierto la posibilidad de la terapia génica, tal y como se ha descrito en la sección pertinente.





</doc>
<doc id="9749" url="https://es.wikipedia.org/wiki?curid=9749" title="Enfermedad de Niemann-Pick">
Enfermedad de Niemann-Pick

La enfermedad de Niemann-Pick es una enfermedad de almacenamiento lisosómico hereditaria autosómica recesiva, causada por mutaciones genéticas específicas, concretamente se trata de un déficit de la enzima esfingomielinasa, de la ruta de degradación de los esfingolípidos. Se incluye dentro del grupo de las lipidosis que son enfermedades por almacenamiento de lípidos.

La enfermedad de Niemann Pick se describió por primera vez en 1914 por el pediatra alemán Albert Niemann (nacido el 23 de febrero de 1880 en Berlín, lugar donde murió el 22 de marzo de 1921) en unos niños de origen judío (Askenazes), grupo étnico de centro y este de Europa; y en 1927 Ludwig Pick (nacido el 31 de agosto de 1868 y fallecido el 3 de febrero de 1944) ya como una entidad propia diferenciada de otras enfermedades, en un estudio tisular diferenciándola de la enfermedad de Gaucher.
A Crocker le debemos la distinción, en 1961, de los cuatro tipos que hoy estudiamos (A, B, C y D).
Finalmente citaremos que fue Brady quien aisló en 1966 la enzima lisosomal esfingomielinasa, cuyo déficit produce los tipos A y B. Los tipos C y D tienen como causa un defecto en el transporte intracelular del colesterol que se acumula en su forma libre sin esterificar.

Las cuatro formas de la enfermedad de Niemann-Pick se caracterizan por una acumulación de esfingomielina y colesterol en los lisosomas de las células, particularmente en las células de órganos importantes como el hígado y el bazo. Las tres formas más conocidas de la enfermedad son los tipos A, B y C.





No existe en el tipo C una relación clara con el déficit de esfingomielinasa, hay un origen genético causado por la anomalía en lo que se denominan, en términos de vanguardia, como genes reguladores. Se relaciona también con una proteína muy específica relacionada con la homeostasis del colesterol intracelular. Este depósito va a ir originando una alteración en las células deteriorándolas, deformándolas (células esponjosas) y terminando en muerte de estas. Los sustratos orgánicos donde van a repercutir esta lisis celular, primordialmente son por orden de importancia: cerebro, hígado y bazo. Al ser estos los órganos afectados, de ahí procederán las expresiones clínicas de esta enfermedad.

Esta enfermedad causa un progresivo deterioro en las funciones vitales, tales como:

Los niños que la padecen mueren precozmente en los tres primeros años de vida. Las características de la enfermedad son infantilismo y trastornos del desarrollo. En la corteza cerebral aparecerán células hinchadas con la técnica de tinción de Baker, llamadas células xantomatosas. Otros signos son hepatoesplenomegalia, anemia, trastornos de la médula ósea, así como trastornos en los cartílagos de crecimiento de los huesos largos.

El diagnóstico de la enfermedad Niemann Pick se confirma con los estudios enzimáticos y con una biopsia en la piel del paciente; al mismo tiempo, hay estudios moleculares que determinan el tipo genético de la enfermedad.

Si un niño nace con algún tipo de Niemann-Pick (A, B, C) se debe a que los dos padres portan el gen anormal que lo produce, aunque no tengan en ellos mismo síntomas de la enfermedad; esto es porque todos los tipos son autosómicos recesivos. 

Cuando los padres son portadores es más probable (50%) que el niño nazca portador de la enfermedad en vez de enfermo (25%), por esto es que se dan pocos casos de esta enfermedad rara.

Los exámenes médicos para detectar el portador en las familias todavía no es fiable. Pero para las mutaciones de los tipos A y B hay disponibles pruebas de ADN ya que se han estudiado ampliamente, sobre todo en la población judía asquenazí.
En los pacientes de tipo C se han podido encontrar mutaciones del ADN y con esto, es posible encontrar al portador. En pocos centros médicos ya se dispone de un diagnóstico en el feto para el Niemann-Pick.

Muchos de los síntomas producidos por la enfermedad son comunes a otras enfermedades y esto dificulta el diagnóstico de certeza. Los médicos aconsejan que si se sospecha de la enfermedad en los tipos A y B, se realice una “medición de la actividad de la esfingomielinasa ácida en los glóbulos blancos”. Esto se consigue haciendo un análisis de sangre o una biopsia de células espumosas en la médula ósea. Esta prueba es ineficaz para detectar los portadores. Cuando la enfermedad es de tipo C, se diagnostica realizando una biopsia de piel, “cultivo celular en laboratorio y estudio posterior de la capacidad de las células aisladas para transportar y almacenar colesterol. Otras pruebas diagnósticas adicionales son la realización de un examen ocular con una lámpara de hendidura, la aspiración de la médula ósea y la biopsia del hígado.

Los científicos no pueden explicar por qué se producen tantas diferencias en la evolución de la enfermedad. Datos estadísticos puramente revelan que un niño con síntomas del tipo C antes de tener un año de edad no llega a alcanzar la edad escolar, pero si los presenta después de la escolarización, llegan a vivir hasta la mitad o el final de su adolescencia, muy pocos llegan hasta los 20 años. Aunque los tipos A y B tienen la misma causa, una persona con el tipo A llega a la muerte sobre los 2 o 3 años, mientras que los de tipo B pueden llegar a la vida adulta.

En toda España hay alrededor de 20 familias afectadas por el NP y realizan reuniones y conferencias sobre la enfermedad para discutir y analizar detenidamente los tratamientos preventivos posibles de la enfermedad que al catalogarse como “rara” y tener una prevalencia de uno entre un millón causa un interés nulo por parte de los laboratorios científicos para investigarla. De esta manera, los familiares se ven obligados a sufragar los gastos con la ayuda de administraciones locales para la investigación.

Los más propensos a padecer el NP son los adolescentes y los niños, aunque se sabe que puede atacar a cualquier edad, y la esperanza de vida es inferior a los 10 años en los niños y 30 en los jóvenes.

El desarrollo de esta enfermedad se caracteriza por el paulatino deterioro de las funciones vitales del organismo. La enfermedad descubierta por los médicos Niemann y Pick es degenerativa y, por lo tanto, mortal en el 100% de los casos.

Es recomendable para los pacientes tener una dieta baja en colesterol, pero ni esto ni los “medicamentos hipolipemiantes” detienen el progreso de la enfermedad. Pero muchos síntomas producidos por el tipo C (cataplexia y convulsiones) si se han podido frenar con medicamentos.

Aún no se ha encontrado un tratamiento eficaz para curar completamente esta enfermedad. Sin embargo, en la actualidad se están tratando de paliar las complicaciones que aparecen en el transcurso de la enfermedad mediante:






</doc>
<doc id="9752" url="https://es.wikipedia.org/wiki?curid=9752" title="MEDLINE">
MEDLINE

MEDLINE o Medline es posiblemente la base de datos de bibliografía médica más amplia que existe. Producida por la Biblioteca Nacional de Medicina de los Estados Unidos. En realidad es una versión automatizada de tres índices impresos: Index Medicus, Index to Dental Literature e International Nursing Index, recoge referencias bibliográficas de los artículos publicados en unas 5.500 revistas médicas desde 1966. Actualmente reúne más de 30.000.000 citas y está en marcha un proceso para la carga paulatina de citas anteriores a 1966, que incluye artículos desde 1871. 
Cada registro de MEDLINE es la referencia bibliográfica de un artículo científico publicado en una revista médica, con los datos bibliográficos básicos de un artículo ("Título, autores, nombre de la revista, año de publicación") que permiten la recuperación de estas referencias posteriormente en una biblioteca o a través de software específico de recuperación.
La base de datos contiene más de 26 millones de registros de 5,639 revistas selectas (NLM Systems, feb 2007) cubriendo las áreas de biomedicina y salud desde 1950 hasta la actualidad. Inicialmente la base de datos incluía artículos desde 1965, pero esto ha ido mejorando, de manera que en la actualidad de puede acceder a artículos desde 1950/51. El acceso a la base de datos es libre desde la Internet, a través de PubMed. Se agregan nuevas citas de martes a sábado. Para citas del periodo 1995-2003: 48% corresponden a artículos publicados en EE. UU., alrededor de 88% están en idioma inglés, y aproximadamente 76% tienen resumen en inglés escritos por los autores de los artículo. El tema más común en la base de datos es Cáncer, que representa el 12% de los registros entre 1950-2016, habiendo subido de 6% en 1950 a 16% en 2016.




</doc>
<doc id="9753" url="https://es.wikipedia.org/wiki?curid=9753" title="Carrera de la milla">
Carrera de la milla

La carrera de la Milla es una modalidad de carrera a pie proveniente de Inglaterra, cuya distancia a recorrer concuerda con esta unidad de medida itineraria, 1.609,344 metros (o 1.760 yardas). Fue muy popular durante las décadas de 1950 y 1960, pero en 1976 la IAAF decidió oficializar todas las carreras con el sistema métrico internacional y fue relevada por los 1.500 metros. Quedando la milla como una prueba a realizar ocasionalmente debido a su gran peso histórico en el medio fondo. Se disputa principalmente sobre dos superficies en pista y asfalto. La segunda tiene una gran popularidad por su vistosidad pues es una carrera de media distancia que se suele disputar en un circuito urbano dando varias vueltas y permite ver evolucionar a los atletas durante la disputa de casi toda la prueba.



</doc>
<doc id="9754" url="https://es.wikipedia.org/wiki?curid=9754" title="Talasemia">
Talasemia

La talasemia es un tipo de anemia del grupo de anemias hereditarias. Esta condición genética confiere resistencia a la malaria, pero causa una disminución de la síntesis de una o más de las cadenas polipeptídicas de la hemoglobina. Hay varios tipos genéticos, con cuadros clínicos que van desde anomalías hematológicas difícilmente detectables hasta anemia grave y cuadros de enfermedad terminal.

Proviene del griego θάλασσα: ‘mar’, y αἷμα: ‘sangre’. Literalmente, sería ‘sangre marina’, pero en realidad el término hace referencia al Mar Mediterráneo, ya que en esta zona es más frecuente la enfermedad. Por ello, a veces se denomina también anemia mediterránea.

Se estima que un 5% de la población mundial es portadora de un gen mutado para la hemoglobina (siendo más frecuente el ser portador de una talasemia que cualquier otra hemoglobinopatía). Unos 300.000 niños nacen cada año con síndromes talasémicos en todo el mundo.
La talasemia es muy común en las zonas mediterráneas como el norte de África, el sur de España y de Italia, regiones de Sicilia, Calabria, Apulia y Cerdeña. En estas dos últimas regiones los portadores son más de 700.000 en una población total de poco menos de 7 millones.

La talasemia consiste en un grupo de enfermedades de amplio espectro. Estas van desde simples anormalidades asintomáticas en el hemograma hasta una grave y fatal anemia. La hemoglobina del adulto, denominada Hemoglobina A está compuesta por la unión de cuatro cadenas de polipéptidos: dos cadenas alfa (α) y dos cadenas beta (β). Hay dos copias del gen que produce la hemoglobina α (HBA1 y HBA2), y cada uno codifica una α-cadena, y ambos genes están localizados en el cromosoma 16. El gen que codifica las cadenas β (HBB) está localizado en el cromosoma 11.

Es una forma hereditaria de anemia en la que se reduce la síntesis de una o más de las cuatro cadenas de la globina, por lo general las dos α y las dos β, que forman parte de la hemoglobina en los glóbulos rojos de la sangre. La función de la hemoglobina es transportar el oxígeno desde los pulmones hacia los tejidos corporales. En la anemia esta función es insuficiente para satisfacer las necesidades de los tejidos (por ejemplo, los músculos y el cerebro). La palabra talasemia procede del griego y significa mar. Este trastorno se denominó así porque es más frecuente en las personas de origen mediterráneo. Sin embargo, su distribución es mundial. Hay diferentes tipos: las formas principales son las del adulto que se denominan talasemias α o β según estén alterados los genes de la cadena α o β. Su gravedad varía según la configuración genética. Se trata de la enfermedad hereditaria de la sangre más frecuente y, a su vez, es la más frecuente causada por una anomalía en un único gen.

En la talasemia, la estructura de ambas cadenas de la hemoglobina permanecen intactas, pero está ausente la cadena α o β o existe en pequeñas cantidades, debido a anomalías en los genes que codifican estas proteínas. Esto origina un desequilibrio en la cantidad de globina en las cadenas con predominio de la α o β. Las cadenas precipitan en ausencia de otras cadenas suficientes con las que unirse y esta precipitación interfiere con la formación de los glóbulos rojos. Se producen menos glóbulos rojos de lo normal y los que son capaces de desarrollarse incluyen en su interior las cadenas de hemoglobina precipitadas, de tal modo que no pueden pasar a través de los capilares y son destruidos prematuramente. Esto produce una anemia grave y para compensarla, la médula ósea sufre hiperplasia al intentar producir suficientes glóbulos rojos, y el bazo también aumenta de tamaño. Son posibles también las deformidades graves en el cráneo y en los huesos largos.

En la α-talasemia gen HBA1 () y HBA2 (), hay una deficiencia de síntesis de cadenas α. El resultado es un exceso de cadenas β que trasportan deficientemente el oxígeno, lo que conduce a bajas concentraciones de O (hipoxemia). Paralelamente, en la β-talasemia () hay una falta de cadenas β, y el consiguiente exceso de cadenas alfa puede formar agregados insolubles que se adhieren a la membrana de los eritrocitos, pudiendo causar la muerte de éstos y sus precursores, originando anemia de tipo hemolítico.

Esta enfermedad está provocada por deleciones en uno o varios genes de los que componen los grupos de la α-globina y la β-globina. Según estas deleciones involucren más o menos genes el tipo de talasemia será más o menos grave. 

Estas deleciones provocan la disminución en la producción de cadenas α o β, según el lugar de la deleción; la escasez de cadenas α se intenta compensar con un aumento de la producción de cadenas β, y viceversa, lo que da lugar a la formación de hemoglobinas inestables que provocan la destrucción de los glóbulos rojos y por lo tanto anemia. 

A su vez las deleciones parecen ser el resultado de entrecruzamientos desequilibrados entre los segmentos duplicados presentes en la región de la agrupación.

En el caso de las β-talasemias además de la deleción del gen de la β-globina, también pueden darse por otras causas como:


El defecto o deleción de un gen en la talasemia β causa una anemia hemolítica que oscila entre leve y moderada sin síntoma alguno. La deleción de dos genes ocasionan anemia más grave y la presencia de síntomas: debilidad, fatiga, dificultad respiratoria. En las variantes más graves, como la talasemia beta mayor, pueden aparecer ictericia, úlceras cutáneas, cálculos biliares y agrandamiento del bazo (que en ocasiones llega a ser enorme). La actividad excesiva de la médula ósea puede causar el ensanchamiento y el agrandamiento de algunos huesos, especialmente los de la cabeza y del rostro. 

Los huesos largos tienden a debilitarse y fracturarse con gran facilidad. Los niños que padecen ciertas talasemias pueden crecer con más lentitud y llegar a la pubertad más tarde de lo normal. Como la absorción del hierro puede aumentar como respuesta a la anemia sumado al requerimiento de transfusiones de sangre frecuentes (las cuales suministran más hierro), es posible que se acumulen cantidades excesivas de hierro y se depositen en la musculatura del corazón, causando insuficiencia cardíaca.

Las talasemias son más difíciles de diagnosticar que otros trastornos de la hemoglobina. El análisis de una gota de sangre por electroforesis puede ser útil pero no concluyente, en especial en el caso de talasemia alfa. Por lo tanto, el diagnóstico se basa habitualmente en patrones hereditarios y en análisis especiales de hemoglobina. Por lo general, las personas que padecen talasemia no requieren tratamiento alguno, pero aquellas con variantes graves pueden requerir un trasplante de médula ósea. La terapia con genes se encuentra en fase de investigación.

Como ocurre en el más conocido caso de la anemia de células falciformes, también la α-talasemia protege a los individuos que la portan frente a la malaria. La malaria o paludismo está producida por un parásito protista del género "Plasmodium" y es transmitida por un mosquito del género "Anopheles". La protección frente a esta enfermedad por parte de los individuos que posee α-talasemia es debida a que "Plasmodium" solo es capaz de parasitar a los eritrocitos sanos. Sin embargo, la sangre de alguien con este tipo de anemia presenta un número elevado de eritrocitos deformes por culpa de que la hemoglobina no está bien constituida y eso es esencial pues deja al parásito indefenso en la sangre permitiendo que nuestro sistema inmunitario acabe con él.

La ventaja del heterocigoto se produce cuando un alelo que es deletéreo en su forma homocigótica, resulta, en cambio ventajoso en su forma heterocigótica. Este fenómeno es lo que se llama polimorfismo equilibrado. Significa que la selección negativa del alelo en estado homocigótico se equilibra con la selección positiva a favor del alelo en el estado heterocigótico.

Debido a esto hay una alta frecuencia de talasemias, y en general de hemoglobinopatías en los países con malaria endémica, de modo que la distribución geográfica de la malaria se correlaciona con la de talasemias.





Las pruebas que se hacen para saber si un individuo padece cualquiera de los tipos de talasemia son análisis de sangre, que permiten ver la forma y la cantidad de glóbulos rojos en sangre. 
Otra forma de diagnosticar la enfermedad es por medio de estudios genéticos. Los cuales nos dan la información exacta del tipo de talasemia y su causa. 

Actualmente, el análisis prenatal que se realiza mediante el muestreo de villus coriónico y la amniocentesis permite determinar la presencia o ausencia de talasemia en el feto, de esta forma si se detecta y es una forma grave puede ser tratada precozmente y que el individuo sobreviva.









</doc>
<doc id="9755" url="https://es.wikipedia.org/wiki?curid=9755" title="Biblioteca Cochrane">
Biblioteca Cochrane

La Biblioteca Cochrane ("The Cochrane Library") es el principal producto de la Colaboración Cochrane. Es una publicación electrónica que se actualizada cada tres meses. Se distribuye mediante suscripción anual en CD o a través de Internet.

La Biblioteca Cochrane es una colección de bases de datos sobre ensayos clínicos controlados en medicina y otra áreas de la salud relacionadas con la información que alberga la Colaboración Cochrane.

La versión en español, Cochrane Library Plus, solo puede consultarse en Internet, y es de acceso gratuito desde España.

En 1972 el epidemiólogo británico Archie Cochrane llamó la atención sobre la enorme dificultad que tienen todos los implicados en las tomas de decisiones sobre cuidados de salud para tener acceso a la investigación clínica que debe fundamentar esa toma de decisiones. En 1974 se comenzó un registro de los ensayos clínicos sobre la atención al embarazo y al parto. En 1985 el registro contenía en torno a 3.500 referencias de ensayos clínicos que habían permitido preparar 600 revisiones para proporcionar la mejor prueba disponible para la toma de decisiones en ese campo. A. Cochrane sugirió que otras especialidades deberían seguir el mismo ejemplo. La Colaboración Cochrane se considera como la respuesta a esa invitación.

Las principales bases de datos que incluye son:





</doc>
<doc id="9756" url="https://es.wikipedia.org/wiki?curid=9756" title="Capa de ozono">
Capa de ozono

Se denomina capa de ozono u ozonosfera a la zona de la estratosfera terrestre que contiene una concentración relativamente alta de ozono. Esta capa, que se extiende aproximadamente de los 15km a los 50km de altitud, reúne el 90% del ozono presente en la atmósfera y absorbe del 97 al 99% de la radiación ultravioleta de baja frecuencia (150-300 nm), esta fue descubierta por los físicos Charles Fabry y Henri Buisson en el año 1913.

Sus propiedades fueron examinadas en detalle por el meteorólogo británico G.M.B. Dobson, quien desarrolló un sencillo espectrofotómetro que podía ser usado para medir el ozono estratosférico desde la superficie terrestre. Entre 1928 y 1958 Dobson estableció una red mundial de estaciones de monitoreo de ozono, las cuales continúan operando en la actualidad. La unidad Dobson, una unidad de medición de la cantidad de ozono, fue nombrada en su honor.

El ozono es la forma alotrópica del oxígeno, que solo está estable en determinadas condiciones de presión y temperatura. Es un gas compuesto por tres átomos de oxígeno (formula_1).

Los mecanismos fotoquímicos que se producen en la capa de ozono fueron investigados por el físico británico Sydney Chapman en 1930. La formación del ozono de la estratosfera terrestre es catalizada por los fotones de luz ultravioleta que al interaccionar con las moléculas de oxígeno gaseoso, que están constituidas por dos átomos de oxígeno (formula_2), las separa en los átomos de oxígeno (oxígeno atómico) constituyente; el oxígeno atómico se combina con aquellas moléculas de formula_2 que aún permanecen sin disociar, formando, de esta manera, moléculas de ozono, formula_1.

La concentración de ozono es mayor entre los 15 y 40km, con un valor de 2-8 partículas por millón, en la zona conocida como capa de ozono. Si todo ese ozono fuese comprimido a la presión del aire al nivel del mar, esta capa tendría solo 3 milímetros de espesor.

El ozono actúa como filtro, o escudo protector, de las radiaciones nocivas, y de alta energía, que llegan a la Tierra, permitiendo que pasen otras como la ultravioleta de onda larga, que de esta forma llega a la superficie. Esta radiación ultravioleta es la que permite la vida en el planeta, ya que es la que permite que se realice la fotosíntesis del reino vegetal, que se encuentra en la base de la pirámide trófica.

Al margen de la capa de ozono, el 10% de ozono restante está contenido en la troposfera, y es peligroso para los seres vivos por su fuerte carácter oxidante. Elevadas concentraciones de este compuesto a nivel superficial forman el denominado esmog fotoquímico. El origen de este ozono se explica en un 10% como procedente de ozono transportado desde la estratosfera y el resto es creado a partir de diversos mecanismos, como el producido por las tormentas eléctricas que ionizan el aire y lo hacen, muy brevemente, buen conductor de la electricidad. Pueden verse algunas veces dos relámpagos consecutivos que siguen aproximadamente la misma trayectoria.

El ozono se produce mediante la siguiente reacción:

Es decir, el oxígeno molecular que se encuentra en las capas altas de la atmósfera es bombardeado por la radiación solar. Del amplio espectro de radiación incidente una determinada fracción de fotones cumple los requisitos energéticos necesarios para catalizar la rotura del doble enlace de los átomos de oxígeno de la molécula de oxígeno molecular.

Posteriormente, el Sol convierte una molécula de ozono en una de oxígeno diatómico y un átomo de oxígeno sin enlazar:

Durante la fase oscura (la noche de una determinada región del planeta), el oxígeno monoatómico, que es altamente reactivo, se combina con el ozono de la ozonosfera para formar una molécula de oxígeno biatómico:

Para mantener constante la capa de ozono en la estratosfera esta reacción fotoquímica debe suceder en perfecto equilibrio, pero estas reacciones son fácilmente perturbables por moléculas, como los compuestos clorados (como los clorofluorocarbonos) y los compuestos bromurados.

El seguimiento observacional de la capa de ozono, llevado a cabo en los últimos años, ha llegado a la conclusión de que dicha capa puede considerarse seriamente amenazada. Este es el motivo principal por el que se reunió la Asamblea General de las Naciones Unidas el 16 de septiembre de 1987, firmando el Protocolo de Montreal. En 1994, la Asamblea General de las Naciones Unidas proclamó el día 16 de septiembre como el "Día Internacional para la Preservación de la Capa de Ozono".

El desgaste grave de la capa de ozono provocará el aumento de los casos de melanomas, cáncer de piel, cataratas oculares, supresión del sistema inmunitario en humanos y en otras especies. También afectará a los cultivos sensibles a la radiación ultravioleta.

Para preservar la capa de ozono hay que disminuir a cero el uso de compuestos químicos como los clorofluorocarbonos (refrigerantes industriales, propelentes), y fungicidas de suelo (como el bromuro de metilo) (Argentina, 900 toneladas/año) que destruyen la capa de ozono a un ritmo 50 veces superior a los CFC.

Las últimas mediciones realizadas con satélites indican que el agujero en la capa de ozono se está reduciendo, a la vez que los niveles de clorofluorocarbonos (CFC) han disminuido. Esos compuestos químicos dañan la capa de ozono de la atmósfera que protege nuestro planeta. Durante más de cincuenta años, el número de CFC presentes en la parte alta de la atmósfera ha aumentado a un ritmo constante hasta el año2000. Desde entonces, la concentración de CFC se ha reducido a razón de casi un1% anual. El descenso permite esperar que el agujero de la capa de ozono pueda cerrarse a mediados de siglo. Noobstante, estos productos todavía causan daño. A pesar del descenso, el agujero de la Antártida en el año 2005 había alcanzado una extensión de casi 29000000km² (kilómetros cuadrados), más de tres veces el tamaño de Australia.




</doc>
<doc id="9757" url="https://es.wikipedia.org/wiki?curid=9757" title="Desierto de Atacama">
Desierto de Atacama

El desierto de Atacama, el desierto no polar más árido de la Tierra, se extiende en el Norte Grande de Chile, abarcando las regiones de Arica y Parinacota, Tarapacá, Antofagasta, Atacama y el norte de la de Coquimbo; cubre una superficie aproximada de 105 000 km². Tiene una longitud de casi 1600 km y un ancho máximo de 180 km. Está delimitado por el océano Pacífico al oeste y por la cordillera de los Andes al este.

Según la WWF, la ecorregión del desierto de Atacama se extiende desde Arica (18°24'S) hasta cerca de La Serena (29°55'S). Por su parte, la National Geographic Society considera que la zona costera peruana forma parte del desierto de Atacama.

Otra parte integrante de este desierto corresponde a una ecorregión denominada puna de Atacama, ubicada sobre los 3500 m s. n. m. y que es compartida por la vertiente occidental de la cordillera de los Andes en el norte de Chile, y por la vertiente oriental de la misma en el noroeste de Argentina y suroeste de Bolivia.

Esta ecorregión chilena es rica en recursos minerales metálicos —como cobre (Chile es el mayor productor del mundo y cuenta con el 28 % de las reservas mundiales), hierro, oro y plata— y no metálicos —entre los que destacan importantes depósitos de boro, litio (Chile cuenta con el 39 % de las reservas sudamericanas), nitrato de sodio y sales de potasio—. También se destaca la bischofita, una sal de magnesio extraída del salar de Atacama, usada como agente apelmazante en la construcción de caminos. Estos recursos son explotados por varias empresas mineras, como Codelco, la mayor compañía cuprífera del planeta, Lomas Bayas, Mantos Blancos y Soquimich.

Su origen data de hace unos tres millones de años, siendo en su pasado un lecho marino. La principal causa de su origen es un fenómeno climático conocido como efecto Föhn, producido en las laderas orientales de la cordillera de los Andes. Esto provoca que las nubes descarguen sus precipitaciones en solo una cara de la montaña en su ascenso vertical; al sobrepasar la cordillera, las nubes no poseen agua, generando así un desierto al bloquear por completo todas las precipitaciones posibles provenientes del este. Las precipitaciones también se bloquean por el oeste mediante sistemas estables de alta presión, conocidos como «anticiclones del Pacífico», que se mantienen junto a la costa, creando vientos alisios hacia el este que desplazan las tormentas.

Por otra parte, la corriente de Humboldt transporta agua fría desde la Antártica hacia el norte a lo largo de las costas chilena y peruana, que enfría las brisas marinas del oeste, reduce la evaporación y crea una inversión térmica —aire frío inmovilizado debajo de una capa de aire tibio—, impidiendo la formación de grandes nubes productoras de lluvias. Toda la humedad creada progresivamente por estas brisas marinas, se condensa a lo largo de las escarpadas laderas de la cordillera de la Costa que dan hacia el Pacífico, creando ecosistemas costeros altamente endémicos compuestos por cactus, suculentas y otros ejemplares de flora xerófila.

El último factor que contribuye a la formación del desierto es la cordillera de los Andes, que en el norte forma una planicie volcánica elevada y ancha conocida como Altiplano. Así como en el sur la cordillera andina contribuye a capturar la humedad proveniente del Pacífico, en el norte el Altiplano impide el ingreso a Chile de las tormentas cargadas de humedad provenientes de la cuenca amazónica, que se encuentra al noreste.

En el desierto de Atacama, una lluvia medible —es decir, de 1 mm o más— puede tener lugar una vez cada 15 o 40 años —se han registrado periodos de hasta 400 años sin lluvias en su sector central—. Sin embargo, la zona se ve afectada entre enero y febrero por el llamado «invierno altiplánico», que produce alguna que otra lluvia y abundantes tormentas eléctricas. 

Contrariamente a la sabiduría convencional, no es el desierto más árido del mundo, correspondiendo esta denominación a los valles secos de McMurdo.

En las noches la temperatura fluctúa mucho, pues puede bajar hasta -25 °C en la zona de Ollagüe, mientras que en el día la temperatura se puede situar entre los 25 y los 50 °C a la sombra. No hay mucha diferencia entre el verano y el invierno, porque está situado al límite del trópico de Capricornio. En verano, la temperatura ambiente matinal es de 4 a 10 °C y la máxima puede alcanzar los 45 °C a plena irradiación solar. La radiación solar es muy alta en el espectro ultravioleta, por lo que es indispensable el uso de gafas y cremas con protección UV.

La humedad relativa del aire es de apenas un 18 % en el interior, pero muy alta en el litoral, llegando hasta un 98 % en los meses de invierno. La presión atmosférica es de 1017 milibares. Existen temporadas de vientos en tornado o ventiscas cuya velocidad puede alcanzar fácilmente los 100 km/h, generalmente registrados al mediodía.

Su flora y fauna está condicionada por la aridez. La vegetación en este lugar, capaz de sobrevivir y adaptarse a su clima extremo, se corresponde con diferentes especies de cactus. La orografía del territorio desértico produce pequeñas aguadas en la zona, es decir, pequeñas charcas de agua que dan lugar a otro tipo de vegetación. Cuando aumenta la cantidad de agua, brotan especies como la esmeralda, la pica o la matilla.

Respecto a la fauna, allí se encuentran algunas aves como los picaflores (del norte, de Arica, de la Puna, de cora, gigante, cordillerano, etc...) las tórtolas y palomas (Quiguagua, de la Puna, boliviana, paloma de alas moteadas, etc...). También aparecen cauces secos conocidos como «ríos del desierto», que proporcionan hábitat a varias especies. La fauna que se puede encontrar en estos parajes son los búhos (pequén, chuncho del norte, tucúquere, lechuza), guanacos y zorros culpeo y chilla, entre otros.

Debido a las condiciones climáticas y de altura de esta zona, la fauna y la flora son distintas a los territorios colindantes. A pesar de que la zona es desértica, cuenta con lagunas, conocidas como Chaxa, Meniques y Puriguatin, que constituyen un importante centro turístico y de atracción ecológica y natural de este territorio. Ellas permiten el crecimiento de distintas flores, como el cachiyuyo, la llareta y la paja brava, entre otras. En esta zona se sitúa la Reserva Nacional Los Flamencos, uno de los puntos turísticos más concurridos.

El desierto de Atacama ha estado poblado desde los comienzos de la colonización americana. Un hito de los primeros habitantes de esta zona fue la faena minera, que tuvo sus inicios entre 12 000 y 10 000 años atrás en una mina de óxido de hierro en Taltal, Región de Antofagasta, la más antigua del continente. Durante el periodo prehispánico, descolló la cultura Chinchorro, desarrollada entre 5000 y 1700 a. C., la primera del mundo en momificar artificialmente a sus muertos. Además, este territorio fue habitado por etnias como los atacameños, mientras que en su litoral vivían los changos, los coles, los lupacas y los uros. Fue dominado por el señorío de Chucuito bajo el nombre de Colesuyo y luego por el Imperio inca como Collasuyo.

La región más árida fue denominada «despoblado de Atacama» durante la Colonia. Después de las Guerras de independencia hispanoamericanas, y debido a la inexactitud de los documentos reales, la zona estuvo en disputa hasta que, a través de los tratados de límites de y , la región pasó a ser oficialmente territorio boliviano.

Pese a los tratados suscritos, las disputas no lograron resolverse. El 14 de febrero de 1879, se efectuó el desembarco chileno en Antofagasta, iniciando las acciones militares contra Bolivia. En 1873, se había suscrito el tratado de Alianza Defensiva Perú–Bolivia, por lo que Chile declaró la guerra a ambos el 5 de abril de 1879, iniciando formalmente la Guerra del Pacífico, que finalizó en 1884 con la victoria de Chile, el tratado de Ancón con Perú y el pacto de Tregua con Bolivia. Tras el conflicto, Chile obtuvo el dominio del, hasta entonces, departamento boliviano del Litoral, el departamento peruano de Tarapacá y la provincia peruana de Arica.

El desierto de Atacama es considerado el mejor sitio del planeta para observar el firmamento y desarrollar la astronomía: su altura respecto al nivel del mar, la escasa nubosidad, la casi inexistente humedad del aire y la lejana contaminación lumínica y radioeléctrica hacen que la visibilidad de su cielo nocturno sea muy nítida. Debido a esto, más de una docena de observatorios se ubica en este lugar —como Paranal (VLT), el complejo astronómico más avanzado y poderoso del planeta, ALMA, el mayor proyecto astronómico del mundo, y La Silla, entre otros—. Chile posee el 40 % de la observación astronómica del mundo; sin embargo, en las próximas décadas, el sector desarrollará otros proyectos —como el Giant Magellan Telescope, el Large Synoptic Survey Telescope (LSST), el E-ELT y la ampliación del Atacama Large Millimeter Array— que harán que el norte del país concentre cerca del 70 % del total mundial.

El desierto de Atacama acoge a los deportistas del todoterreno del mundo. En este desierto, se han realizado los diversos campeonatos del Rally Baja Atacama, Rally Baja Chile, Rally Patagonia Atacama, y recibió el Rally Dakar Series —en 2009, 2010, 2011, 2012, 2013, 2014 y 2015—, evento organizado por Amaury Sport Organisation (ASO). Las dunas de este desierto son ideales para este tipo de deporte, ubicadas en los aledaños de la ciudad de Copiapó, Región de Atacama.

Otro evento que se realiza en este desierto es la «Carrera Solar Atacama», que consiste en una carrera de vehículos solares que se hace por lugares del desierto como Toconao, Calama, Iquique y Antofagasta, entre otros.





</doc>
<doc id="9759" url="https://es.wikipedia.org/wiki?curid=9759" title="Hemoglobina A2">
Hemoglobina A2

La hemoglobina A2 es un tipo de hemoglobina, que representa en el adulto del 2,4% y en el feto menos del 0,5% de la hemoglobina total. Está formada por dos cadenas de globinas alfa y dos cadenas de globinas delta, que aumenta de forma importante en la beta-talasemia, al no poder sintetizar globinas beta.



</doc>
<doc id="9760" url="https://es.wikipedia.org/wiki?curid=9760" title="Estratosfera">
Estratosfera

La estratosfera o estratósfera es una de las capas de la atmósfera terrestre; está situada entre la troposfera y la mesosfera. La altura a la que comienza es variable: En las regiones polares a menor altura, entre 6 y 9 kilómetros o más; y en las regiones ecuatoriales entre 16 y 20 kilómetros. y se extiende hasta los 50km de altura aproximadamente. 

La temperatura aumenta progresivamente desde los −55 °C de la tropopausa hasta alcanzar los 0 °C de la estratopausa, aunque según algunos autores puede alcanzar incluso los 17 °C o más. Es decir, en esta capa la temperatura aumenta con la altitud, al contrario de lo que ocurre en las capas superior e inferior. Esto es debido principalmente a la absorción de las moléculas de ozono que absorben radiación electromagnética en la región del ultravioleta.

En la parte baja de la estratosfera la temperatura es relativamente estable, y en toda la capa hay muy poca humedad.

En la estratosfera la mezcla horizontal de los componentes gaseosos se produce mucho más rápidamente que la mezcla vertical.

A una altura aproximadamente de 2,5 veces la altura del Everest y unas 112 veces el Empire State Building de Nueva York solo algunos aviones como el Mig-31 ruso, el SR-71, el Concorde, el U-2 y el UAV RQ-4 Global Hawk pueden volar a este nivel. Cerca del final de la estratósfera se encuentra la capa de ozono que absorbe la mayoría de los rayos ultravioleta del Sol.

El 14 de octubre de 2012 el austríaco Felix Baumgartner se lanzó desde la estratosfera a una altura de . Rompió así el récord de salto en caída libre desde punto más alto y el de vuelo tripulado en globo con una distancia a la superficie terrestre de . El 24 de octubre de 2014 este récord fue superado por el vicepresidente de Google, Alan Eustace (57 años), quien se lanzó desde una altura de .



</doc>
<doc id="9761" url="https://es.wikipedia.org/wiki?curid=9761" title="Troposfera">
Troposfera

La troposfera o tropósfera es la capa de la atmósfera terrestre que está en contacto con la superficie de la Tierra.

Tiene alrededor de 17 km de espesor en el Ecuador terrestre y solo 7 km en los polos, y en ella ocurren todos los fenómenos meteorológicos que influyen en los seres vivos, como los vientos, la lluvia y la nieve. Además, concentra la mayor parte del oxígeno y del vapor de agua. En particular esta capa actúa como un regulador térmico del planeta; sin ella, las diferencias térmicas entre el día y la noche serían tan grandes que no podríamos sobrevivir. Es de vital importancia para los seres vivos. La tropósfera es la capa más delgada del conjunto de las capas de la atmósfera.

La temperatura en la tropósfera desciende a razón de aproximadamente 6,5 °C por kilómetro de altura, por encima de los 2000 metros de altura.



</doc>
<doc id="9764" url="https://es.wikipedia.org/wiki?curid=9764" title="Deforestación">
Deforestación

La deforestación o desforestación es un proceso provocado por la acción de los humanos, en el que se destruye o agota la superficie forestal, generalmente con el objetivo de destinar el suelo a otra actividad. En la actualidad, está directamente relacionada con las actividades industriales, como la tala y quema para la expansión de la frontera agrícola para dar lugar a la agricultura intensiva y la ganadería. La expansión de las áreas urbanas y las actividades mineras también impulsan la deforestación. La construcción de carreteras y vías de acceso a bosques cada vez más remotos mediante la tala furtiva contribuye a la deforestación. En menor medida, la agricultura de subsistencia también está involucrada en actividades de deforestación. Según el investigador británico Norman Myers, el 5% de la deforestación se debe a cría de ganado, el 19% a la tala excesiva, el 22% a las plantaciones de árboles (sobre todo al aceite de palma) y el 54% a la agricultura de tala y quema.

La deforestación tiene un impacto directo en el cambio climático y calentamiento global actuales. Se estima que la deforestación y otras prácticas agrícolas contribuyeron en las décadas pasadas alrededor del 20% de las emisiones de dióxido de carbono a nivel global. La deforestación destruye la calidad de los suelos, contribuyendo a la erosión de los suelos y la desertificación, aumentando la liberación de polvo mineral y contribuyendo así a las tormentas de arena. Los ecosistemas forestales actúan como sumideros de carbono y desempeñan un papel crucial en la absorción de gases de efecto invernadero, por lo que la deforestación tiene un impacto adverso en la fijación de dióxido de carbono (CO).

Más del 70% de los animales y plantas viven en áreas forestales, por lo que la deforestación tiene un impacto dramático en la pérdida del hábitat de millones de especies, extinción de especies, la disminución de poblaciones de insectos, la pérdida de biomasa global y de biodiversidad. La deforestación afecta el albedo de la tierra, produciendo cambios en las temperaturas globales, los vientos y las precipitaciones. Los árboles también contribuyen con el ciclo hidrológico devolviendo el vapor de agua a la atmósfera. La eliminación de los árboles también causa fluctuaciones extremas de temperatura.

La deforestación ocasiona el desplazamiento de poblaciones indígenas y comunidades rurales, y aumenta la expansión y las variedades de enfermedades infecciosas transmitidas a los humanos por animales que pierden su hábitat.

Desde 1750, los cambios más grandes en la superficie del planeta se han producido por la deforestación en climas templados: cuando los bosques y selvas se reducen para dejar espacio al pasto, el albedo de la región afectada se incrementa, lo cual podría producir calentamiento o enfriamiento, dependiendo de las condiciones locales. La deforestación también afecta a la absorción del carbono, lo cual puede producir concentraciones elevadas de CO, el componente principal de los gases de efecto invernadero. Ciertos modos de limpieza de tierras como el "corte y quema" empeoran estos efectos al quemar biomasa, que libera directamente gases de efecto invernadero y partículas como el hollín en el aire.

Los bosques todavía cubren alrededor del 30% de las regiones del mundo. Brasil, Indonesia, Myanmar, Nigeria y Tanzania son los cinco países que tuvieron el porcentaje más elevado de deforestación en el período 2010-2015. El Objetivo de Desarrollo Sostenible número 15 llama a detener la deforestación para 2020.

Existen desacuerdos sobre la definición del término deforestación. Las posiciones actuales pueden dividirse en dos: quienes definen la deforestación en un sentido amplio, y quienes definen la deforestación en un sentido estrecho.

La deforestación en sentido estrecho se refiere exclusivamente a la remoción o extracción total de la cobertura vegetal con el objetivo de destinar la tierra a otro uso. La FAO ha optado por definir la deforestación como ""variación boscosa con agotamiento de la cubierta de copas arbóreas a menos del 10 por ciento"", es decir que el 90% del bosque y de los árboles deben ser destruidos para considerarse deforestación. Todos los procesos restantes que de alguna manera impactan sobre el bosque son considerados como "degradación", y no como deforestación. Esta perspectiva es mayormente favorecida por los geográfos, los economistas y por quienes deben planificar el uso de la tierra.

La definición en sentido amplio incluye no solamente la remoción o extracción total de la cobertura vegetal para destinar la tierra a otro uso, sino también otro tipo de degradación que reduce la calidad de los bosques (por ejemplo, la pérdida de biodiversidad en el bosque, la densidad y la estructura, la biomasa, entre otros). El investigador británico Norman Myers define a este tipo de proceso como "una degradación tan severa que el bosque residual no puede calificarse como bosque en ningún sentido práctico de la palabra". Esta definición es mayormente utilizada por conservacionistas, biológos y ecologistas.

El tipo de definición que se adopte tiene un impacto sobre la producción de datos sobre deforestación. Así, según se adopte un criterio o el otro, la tasa de deforestación variará en relación con la definición utilizada.

En la lengua castellana se utiliza el término "desmonte" para referirse a la deforestación, especialmente en ciertas áreas geográficas como Argentina. El término desmonte, sin embargo, se refiere al proceso de "cortar en un monte o en parte de él los árboles o matas", probablemente en alusión a la vegetación existente en los ecosistemas de monte. Aunque se utiliza de manera intercambiable con "deforestación" y "degradación", el término no tiene una definición científica precisa.

Se pueden distinguir tres enfoques teóricos sobre la deforestación: la escuela de la pobreza, la escuela neoclásica y la escuela de la ecología política. Estas tres escuelas difieren en su identificación de las causas y los agentes, pero sobre todo en las respuestas o acciones necesarias para frenar la deforestación. Según Sven Wunder, "diferentes actitudes y explicaciones pueden coexistir en el mismo país o región. Algunas características también pueden combinarse bajo ciertas circunstancias. [...] Sin embargo, en otros aspectos el foco y las predicciones políticas de los diferentes enfoques sobre cómo frenar la deforestación son incompatibles".

Para la escuela de la pobreza, la principal causa de la deforestación es el número creciente de personas pobres, que utilizan los recursos del bosque para sobrevivir, sobre-explotándolo, causando el agotamiento del recurso, y deforestando para obtener nuevas tierras. El Informe Brundtland es un ejemplo de este tipo de perspectivas:

Para la escuela neoclásica, la principal causa de la deforestación es la ausencia de derechos de propiedad sobre los bosques. Según esta visión, los regímenes de "acceso abierto" llevan a fallas en el mercado y proveen incentivos para la sobre-explotación y la degradación. Esta perspectiva toma algunos de los ejes teóricos de la "tragedia de los comunes" de Garrett Hardin.

La ecología política considera que la deforestación se produce porque los grandes agricultores invierten en la cría de ganado y en granos de exportación, es decir, en granos que no son destinados para consumo alimenticio humano directo sino para otros usos, como ciertos cultivos para la producción de etanol o la producción de granos para el consumo de ganado. Estos grandes agricultores generan presión sobre los pequeños terratenientes y sobre el bosque. Los pequeños terratenientes deben adentrarse en el bosque para poder sobrevivir, mientras que los grandes agricultores continúan empujando el límite de la frontera agrícola.

En el presente, la deforestación ocurre principalmente, en América Latina, África Occidental y algunas regiones de Asia. En Brasil la deforestación en 2017 aumentó en un 28%, con más de 5 mil kilómetros cuadrados de árboles talados, en gran medida, por la reforma del Código Forestal durante el gobierno de Michael Temer que achicó las áreas verdes protegidas dando cabida a megaproyectos que destruyen la vegetación carioca. Los estados de Mato Grosso, Roraima y Pará, registraron los mayores índices de deforestación. En Paraguay, se incrementó un 34% la deforestación a comparación del 2012, con más de 160 mil hectáreas de boques taladas, afectando gravemente la Reserva Natural Cabrera Timane y el Parque Nacional Médanos del Chaco. En Perú, se deforestan alrededor de 150000 hectáreas al año, por la práctica de la minería ilegal, el país ha perdido más del 50% de la cubierta vegetal de la costa.

Una tercera parte del total de la tierra está cubierta por bosques, lo que representa cerca de (cuatro mil millones) de hectáreas. Hay 10 países que concentran dos tercios de este patrimonio forestal: Australia, Brasil, Canadá, China, la República Democrática del Congo, India, Indonesia, Perú, la Federación Rusa y los EE. UU. Estos han sido explotados desde hace años para la obtención de madera, frutos, sustancias producidas por diferentes especies o para asentamientos de población humana, ganadería y agricultura. Indonesia, Malasia, Paraguay, Bolivia, Zambia y Angola han sido los países que más superficie forestal han perdido

En los últimos 25 años la tasa de desaparición de los bosques se redujo a la mitad. Desde 1990 se han perdido 129 millones de hectáreas de bosque. La tasa anual de pérdida neta de bosques (que tiene en cuenta los nuevos bosques que se plantan) pasó de 0,18% en los años 1990 a 0,08% en los cinco últimos años. Más países están mejorando la gestión forestal y existe una superficie cada vez mayor de áreas protegidas. Particularmente relevante es el caso de Europa cuya superficie boscosa aumentó considerablemente, teniendo en 2016 un tercio más de bosques que un siglo atrás. El mismo fenómeno se produce en Cuba con un aumento de la superficie boscosa del casi 30% en las últimas décadas, como resultado de un ambicioso programa de reforestación. Igual situación se da en Rusia, que posee el 20% de todos los bosques del planeta, cuyas áreas boscosas se están ampliando desde 1961.

En los países más desarrollados la cubierta forestal sufre otras agresiones, como la lluvia ácida, que comprometen la supervivencia de los bosques, situación que se pretende controlar mediante la exigencia de requisitos de calidad para los combustibles, como la limitación del contenido de azufre o la desulfuración de los humos de las centrales térmicas y refinerías.

En los países menos desarrollados las masas boscosas se reducen año tras año, mientras que en los países industrializados se están recuperando debido a las presiones sociales, reconvirtiéndose los bosques en atractivos turísticos y lugares de esparcimiento.

Mientras que la tala de árboles de la pluviselva tropical ha atraído más atención, los bosques secos tropicales se están perdiendo a un ritmo sustancialmente mayor, sobre todo como resultado de las técnicas utilizadas de tala y quema para ser reemplazadas por cultivos. La pérdida de biodiversidad se correlaciona generalmente con la tala de árboles.

En 2019 se perdían anualmente 26,1 millones de hectáreas de bosque, cuando de 1999 a 2019 solo se habían restaurado 26,7 millones de hectáreas. Es decir, el ritmo de deforestación era 10 veces más rápido que el de reforestación.

Las causas de la deforestación son las fuerzas que motivan a los agentes a destruir la cubierta forestal. Existen causas directas e indirectas de la deforestación. Las causas directas típicamente se conocen como fuentes de la deforestación, causas de primer nivel o causas próximas o aledañas. Son las más fáciles de identificar. Las causas indirectas son las principales fuerzas de la deforestación, pero existe más desacuerdo sobre ellas y son más difíciles de cuantificar.

Pearce y Brown identificaron dos causas principales de la deforestación:


Otros autores utilizan la expresión "causas inmediatas" y "causas subyacentes" para explicar las causas de la deforestación. Según este modelo, las causas subyacentes son las variables en el nivel macroeconómico y los instrumentos de política pública (tanto en el nivel nacional como internacional), mientras que las causas inmediatas son los parámetros de decisión que afectan directamente a los agentes (las instituciones, el mercado, la tecnología y las infraestructuras).

Existe suficiente evidencia de que un aumento en el precio de los productos agrícolas estimula la deforestación. Un estudio de 1987 que analizó 58 países encontró una correlación entre el área cultivada total y el precio de los productos agrícolas. El único caso donde no hay correlación entre el precio de los productos agrícolas y la deforestación es cuando la producción agrícola es agricultura de subsistencia. Cuando los productores agrícolas están buscando maximizar sus ganancias, la correlación entre precio y deforestación es mayor.

Alrededor del 60% de la deforestación se produce para la conversión a tierras agrícolas.

La agricultura itinerante o también de tala y quema destruye la tierra forestal para hacer crecer los cultivos hasta que los nutrientes del suelo se agotan o la tierra se llena de malezas, lo que ocasiona que las poblaciones se muevan para deforestar más áreas. La producción de pequeños terratenientes y el número creciente de agentes practicando la agricultura itinerante eran la causa principal de la deforestación en el pasado. En la actualidad, la proporción de la conversión de bosques a tierras agrícolas está aumentando y la agricultura itinerante está disminuyendo.

Las plantaciones deberían ayudar a reducir la tasa de deforestación. Sin embargo, el hecho de que las plantaciones remuevan la presión sobre el bosque para la producción de madera no se traduce en menos deforestación, sino en más. Las plantaciones de madera podrían ir en detrimento de los ecosistemas de los bosques tropicales. Los cultivos de árboles y en particular las plantaciones de caucho juegan un rol más importante en la deforestación en Indonesia que la agricultura itinerante de subsistencia. Alrededor de la mitad de las plantaciones establecidas en áreas tropicales lo hacen sobre tierra donde anteriormente existían bosques nativos. Las plantaciones también pueden promover la deforestación mediante la construcción de carreteras que mejoran el acceso de otros agentes de deforestación, como los agricultores itinerantes.

Las explotaciones forestales no necesariamente causan deforestación. Sin embargo, pueden degradar seriamente los bosques. Las explotaciones forestales además catalizan la deforestación al abrir carreteras y vías de acceso, subsidiando el costo de cortar los árboles remanentes y preparar la tierra para la siembra de granos o el pastoreo.

La recolección de leña y otros combustibles derivados de la madera a menudo se concentra en los bosques secos tropicales y en áreas forestales degradadas. La recolección de leña no es usualmente la principal causa de deforestación en los bosques húmedos tropicales, aunque puede serlo en aquellos lugares con áreas forestales reducidas como las Filipinas, Tailandia y partes de América Central. La recolección de leña era considerada como la principal causa de deforestación y degradación forestal en El Salvador.

También pueden ocurrir ilegalidades durante el transporte de la madera como procesamiento y exportación ilegal, falsa declaración en las aduanas, la evasión de impuestos y tasas de exportación (Ozinga, S. 2003).

El fuego es una herramienta muy utilizada para avanzar sobre el bosque para la conversión de la tierra, tanto para agricultura permanente como para el desarrollo de pastizales. El fuego es una herramienta útil en la agricultura y la gestión de los bosques, pero también puede ser una causa de la deforestación. A partir de los datos disponibles de más de 118 países representando el 65% del área forestal global, un promedio de 19.8 millones de ha o 1% de todos los bosques fueron reportados como significativamente afectados cada año por incendios forestales. La deforestación debido a la pavimentación de carreteras en Brasil también llevó a incidentes más frecuentes de incendios forestales.

Hay evidencia sostenida que las operaciones militares en la guerra de Vietnam y en otras guerras ocasionaron deforestación. De manera más reciente, se han documentado vínculos entre la guerra y el comercio de madera en la guerra civil entre Birmania y Tailandia, donde el régimen birmano le vende madera a los tailandeses para financiar su guerra civil contra el pueblo Karen. La destrucción forestal en El Salvador es un resultado de la guerra. Además de la intervención de los militares en la guerra, se ha documentado el rol de las fuerzas armadas en la deforestación en el sudeste asiático y en América del Sur. En Brasil, el rol de las fuerzas armadas en la política brasilera es una causa importante de la deforestación en la selva amazónica.

La deforestación es una de las principales causas del efecto invernadero y el calentamiento global. La pérdida de los bosques tropicales es responsable de aproximadamente el 20% de las emisiones mundiales de gases de efecto invernadero. De acuerdo con el Grupo Intergubernamental de Expertos sobre el Cambio Climático, la deforestación, principalmente en áreas tropicales, podría suponer hasta un tercio de las emisiones de dióxido de carbono antropogénicas. Pero cálculos recientes sugieren que las emisiones de CO provocadas por la deforestación y la degradación forestal (excluidas las emisiones naturales de las turberas) supondrían entre el 6 y el 17% de todas las emisiones antropogénicas de CO, con una media del 12%. La deforestación provoca que el CO permanezca más tiempo en la atmósfera. Al aumentar el CO, se crea una capa que atrapa la radiación solar. Esta radiación se convierte en calor, provocando así el efecto invernadero.

Las plantas extraen el CO de la atmósfera a través de la fotosíntesis, quedándose con el carbono, que incorporan a su estructura (raíces, tallos, hojas, flores) en forma de moléculas orgánicas y liberando parte del oxígeno. Aunque también liberan algo de CO durante su proceso normal de respiración. Solo cuando un árbol o un bosque crecen pueden extraer carbono de la atmósfera, almacenándolo en sus tejidos. Tanto la putrefacción de la madera como su quema devuelven a la atmósfera ese carbono almacenado. Para que los bosques realmente extraigan carbono de la atmósfera debe haber una acumulación neta de madera. Una forma es cortar los árboles, transformar la madera en objetos duraderos y reemplazar con nuevos árboles los cortados. La deforestación también puede hacer que se libere el CO acumulado en el terreno. Los bosques pueden ser tanto sumideros de carbono como fuentes, dependiendo de las circunstancias ambientales. Los bosques maduros (donde la cantidad de materia vegetal no varía significativamente) alternan entre comportarse como fuentes netas y sumideros netos (véase Ciclo del carbono), pero esta variación resulta insignificante en relación con la enorme cantidad de carbono que tienen almacenada.

En las áreas deforestadas, el terreno se calienta más rápido por efecto del sol y alcanza una mayor temperatura, lo que lleva a mayores corrientes de convección ascendentes que favorecen la formación de nubes y finalmente producen más lluvia. Sin embargo, de acuerdo con el Laboratorio norteamericano de Dinámica de Fluidos Geofísicos (GFDL por sus siglas en inglés), los modelos utilizados para investigar los efectos a gran distancia de la deforestación tropical mostraron un amplio, pero suave, incremento de la temperatura en toda la atmósfera tropical. Estos modelos predijeron un calentamiento inferior a los 0,2°C en la atmósfera tropical superior (entre 700 y 500 milibares). Sin embargo estos modelos no predicen cambios significativos en otras áreas más allá de los trópicos. Aun así la realidad puede ser diferente, porque el modelo puede contener errores y sus resultados nunca son absolutamente definitivos.

La deforestación afecta a los vientos, el vapor de agua y la absorción de energía solar, influyendo así claramente en el clima zonal y mundial. La deforestación de un área puede aumentar las tormentas de arena en zonas colindantes.

La reducción de las emisiones de la deforestación y la degradación forestal (REDD por sus siglas en inglés) en países en desarrollo ha surgido como un importante complemento a las políticas climáticas actuales. La idea consiste en compensar económicamente a los países que consigan estas reducciones de forma significativa.

Los legos piensan que los bosques tropicales contribuyen significativamente al oxígeno de la atmósfera aunque los científicos consideran que la contribución neta de los bosques tropicales es pequeña y que la deforestación solo tiene efectos menores en los niveles de oxígeno atmosférico. No obstante, la quema de masa forestal para obtener tierras cultivables libera ingentes cantidades de CO, que contribuyen al calentamiento mundial. Los científicos también afirman que la deforestación tropical libera anualmente 1 500 millones de toneladas de carbono a la atmósfera.

La deforestación también afecta al ciclo del agua: los árboles extraen agua del subsuelo a través de sus raíces y la liberan a la atmósfera. Cuando desaparecen, el clima se vuelve más seco. Además la deforestación reduce la cantidad de agua en el terreno y en el subsuelo, de modo que las plantas restantes ven reducida su disponibilidad de agua. Asimismo la deforestación reduce la cohesión del suelo, lo que da lugar a erosión, inundaciones, desertificación y corrimientos de tierras.

Al reducirse la cubierta arbórea disminuye la capacidad del entorno para interceptar, retener y transpirar la lluvia caída. Las áreas boscosas atrapan el agua y la filtran al subsuelo; las deforestadas, en cambio, se vuelven fuentes de agua superficial, que se mueve mucho más deprisa que la subterránea. Los bosques devuelven a la atmósfera por transpiración la mayoría del agua que cae sobre ellos como precipitación. Por el contrario, cuando se deforesta una zona, casi toda la precipitación se pierde en forma de agua superficial. Ese transporte más rápido de agua superficial puede traducirse en inundaciones relámpago e inundaciones más concentradas de las que ocurrirían si se hubiera mantenido la cubierta arbórea. La deforestación también reduce la evapotranspiración, y consiguientemente los niveles de humedad atmosférica, lo que en algunos casos afecta a las precipitaciones en las zonas a sotavento del área deforestada, porque el agua no se recicla en los bosques a sotavento, sino que corre por la superficie y va directamente a los océanos. De acuerdo con un estudio, en el área deforestada al norte y noroeste de China, la precipitación media anual descendió un tercio entre la década que comenzó en 1951 y la de 1981.

Los árboles, y las plantas en general, inciden significativamente en el ciclo hidrológico:


Como resultado, la presencia o ausencia de árboles cambia la cantidad de agua subterránea, superficial o atmosférica. Esto cambia también el ritmo de erosión y la disponibilidad de agua ya sea por el ecosistema o para las necesidades humanas. La deforestación de las llanuras traslada la formación de nubes y la lluvia a terrenos más elevados.

En el caso de lluvias muy intensas y prolongadas que rebasen la capacidad normal de absorción de los bosques, es posible que, a pesar de su presencia, se produzcan inundaciones.

La selva tropical es la fuente de alrededor del 30% del agua dulce del planeta.

La deforestación altera los patrones climáticos favoreciendo un tiempo más cálido y seco, y por tanto incrementando la sequía, la desertificación, la pérdida de cosechas, la fusión de los polos, las inundaciones costeras y el desplazamiento de flora y fauna

Los bosques naturales tienen un ritmo de erosión muy bajo, aproximadamente 2 toneladas métricas por kilómetro cuadrado. La deforestación generalmente incrementa el ritmo de pérdida de suelo al aumentar la escorrentía y reducir el escudo de residuos vegetales. Esto puede ser una ventaja en los suelos de selvas tropicales excesivamente lavados. Las propias operaciones de tala incrementan la erosión por la construcción de carreteras y el uso de maquinaria pesada.

La Meseta de Loes en China fue despojada de sus bosques originales hace milenios. Desde entonces ha estado erosionándose, creando profundas cárcavas, proporcionando el sedimento que da al río Amarillo su color característico y favoreciendo las inundaciones en su curso bajo.

La desaparición de los árboles no siempre incrementa el ritmo de erosión. En ciertas regiones del suroeste de Estados Unidos los arbustos y los árboles han estado limitando las praderas. Los propios árboles refuerzan la pérdida de plantas herbáceas en el suelo sombreado por sus copas. Si el suelo queda desnudo, es muy vulnerable a la erosión. El Servicio Forestal estadounidense, por ejemplo en el parque nacional Bandelier, estudia cómo restaurar el ecosistema, y reducir la erosión, quitando los árboles.

Las raíces de los árboles cohesionan el suelo y, si es lo suficientemente superficial, lo mantienen en su lugar ligándolo a la roca madre. Por esta razón talar los árboles de laderas empinadas con suelo superficial puede incrementar el riesgo de corrimientos de tierras y amenazar las vidas de quienes residan cerca.

La deforestación disminuye la biodiversidad y es causa de la extinción de muchas especies. Más de la mitad de las especies de plantas y animales terrestres viven en las selvas tropicales. La pérdida de áreas boscosas ha resultado en un entorno degradado, con menor biodiversidad. Los bosques sostienen la biodiversidad proporcionando un hábitat a numerosas especies de fauna y flora, algunas de las cuales pueden tener aplicaciones medicinales. Siendo los biotopos forestales fuentes irreemplazables de nuevas medicinas (como el taxol), la deforestación puede destruir irrecuperablemente la riqueza genética que proporciona a las plantas comestibles resistencia frente a las plagas.

Al ser las selvas tropicales los ecosistemas más diversos de la Tierra y encontrarse en ellos alrededor del 80% de la biodiversidad conocida, la desaparición de áreas significativas de cubierta arbórea ha resultado en degradación del suelo y un entorno de menor biodiversidad. Un estudio en Rondonia (Brasil) muestra que la deforestación acaba también con la comunidad microbiana que se ocupa de reciclar los nutrientes, limpiar el agua y eliminar la contaminación.

Se estima que cada día estamos perdiendo 137 especies de plantas y animales (incluidos insectos) debido a la deforestación de las selvas, lo que supone 50000 especies anuales. Autores como Lewin "etal." afirman que la deforestación de las selvas está contribuyendo a la extinción masiva del Holoceno.

Los ritmos conocidos (no estimados) de extinción de mamíferos y aves por la deforestación son mucho más bajos, aproximadamente una especie por año. Pero si se extrapola a todas las especies sale la cifra de aproximadamente 23000 cada año. Se ha predicho que el 40% de las especies animales y vegetales del sudeste asiático podría desaparecer en el s.XXI. Posteriormente se han cuestionado estas predicciones al observarse en 1995 que en el sudeste asiático la mayoría del bosque original ha sido transformado en plantaciones de monocultivo, pero que las especies potencialmente amenazadas son pocas, y que los árboles y el resto de la flora permanecen estables y muy extendidos. La comprensión científica del proceso de extinción es insuficiente para hacer predicciones acertadas sobre el impacto de la deforestación en la biodiversidad. La mayoría de las predicciones de pérdida de biodiversidad ocasionada por operaciones silvícolas se basan en modelos especie-área, asumiendo que si el bosque decae, la diversidad de las especies decaerá de modo similar. Sin embargo muchos de esos modelos han demostrado ser erróneos y la pérdida de hábitat no lleva necesariamente a la pérdida de especies a gran escala. Se sabe que los modelos especie-área sobreestiman el número de especies amenazadas propias de las áreas que están siendo deforestadas, y mucho más en el caso de especies más difundidas (presentes tanto en áreas que están siendo deforestadas como en las que se están dejando intactas).

Un estudio de 2012 sobre la Amazonia predice que, pese a la falta de extinciones por ahora, hasta el 90% de las predichas se producirá en los próximos 40 años.

Fragmentar los bosques, o incluso trazar carreteras en ellos, tiene un fuerte impacto sobre la biodiversidad: un estudio publicado en "Nature" en 2017 muestra que el 85% de las especies de animales que viven en una selva se ven afectadas por el efecto linde. El 46% aumenta su abundancia, y el 39% (en general, las especies más amenazadas, y especialmente anfibios pequeños, grandes reptiles y mamíferos no voladores de tamaño medio) la disminuye.

La deforestación ocasiona que aparezcan nuevas enfermedades virales o infecciosas o que enfermedades que están controladas por los bosques se expandan con mayor rapidez e intensidad. Una revisión de literatura científica en 2007 reveló que existe una relación entre la deforestación y la malaria. Otro estudio científico en 2010 demostró que un 4% de deforestación en la selva amazónica en Brasil llevó al incremento de un 43% en los casos de malaria.

En 2000, la FAO concluyó que «el papel de la dinámica de la población en un entorno local puede variar de decisivo a insignificante», y que la deforestación puede resultar de «una combinación de presión demográfica y estancamiento económico, social y económico y condiciones tecnológicas».

Las principales organizaciones internacionales, incluidas las Naciones Unidas y el Banco Mundial, han empezado a desarrollar programas de lucha contra la deforestación. El término general REDD (siglas en inglés de Reducción de Emisiones de Deforestación y Degradación) describe estos programas, que emplean incentivos monetarios directos o de otro tipo para animar a los países en desarrollo a que limiten o reviertan su deforestación. Se ha debatido sobre la financiación, pero en la decimoquinta conferencia de las partes (COP 15) de la Convención Marco de las Naciones Unidas sobre el Cambio Climático (CMNUCC) en Copenhague (diciembre de 2009) se alcanzó un acuerdo por el que los países desarrollados se comprometieron a aportar recursos nuevos y adicionales, incluidas la silvicultura e inversiones canalizadas por instituciones internacionales, que se aproximarán a los 30 millardos de dólares para el período 2010-2012

Se está trabajando significativamente en herramientas para controlar cómo los países en desarrollo cumplen los objetivos REDD a los que se han comprometido. Estas herramientas, que incluyen seguimiento remoto de los bosques por imágenes satelitales y otras fuentes de datos, incluido FORMA (acrónimo en inglés de iniciativa de Seguimiento Forestal para la Acción) del Centro para el Desarrollo Global y el portal de seguimiento del carbono forestal del Grupo de Observación de la Tierra (GEO por sus siglas en inglés). También se dio importancia al guiado metodológico para el seguimiento de los bosques en la COP 15. La organización medioambiental Socios para Evitar la Deforestación encabeza la campaña para el desarrollo de la REDD a través de financiación del Gobierno estadounidense. En 2014 la FAO, con varios socios, lanzó Open Foris —un conjunto de programas informáticos de código abierto para ayudar a los países a recoger, producir y difundir información sobre el estado de sus recursos forestales—. Estos programas (hay versión en español) sirven para todo el ciclo de vida del inventario forestal, desde la valoración de las necesidades, diseño, planificación, recogida y gestión de datos sobre el terreno, análisis estimativos y difusión. Se incluyen herramientas para el procesado de imágenes remotas, así como para las comunicaciones internacionales REDD y MRV (siglas en inglés de medida, comunicación y verificación).

Para evaluar las implicaciones generales de las reducciones de emisiones, los países donde se concentra la mayor atención son los de mucho bosque y altos ritmos de deforestación (HFHD por sus siglas en inglés) y los de poco bosque, pero altos ritmos de deforestación (LFHD por sus siglas en inglés). Países HFHD se consideran Brasil, Camboya, Corea del Norte, Guinea Ecuatorial, Malasia, Islas Salomón, Timor Este, Venezuela y Zambia. En cambio se anotan como LFHD Afganistán, Benín, Botsuana, Birmania, Burundi, Camerún, Chad, Ecuador, El Salvador, Etiopía, Ghana, Guatemala, Guinea, Haití, Honduras, Indonesia, Liberia, Malaui, Malí, Mauritania, Mongolia, Namibia, Nepal, Nicaragua, Níger, Nigeria, Pakistán, Paraguay, Filipinas, Senegal, Sierra Leona, Sri Lanka, Sudán, Togo, Uganda, Tanzania y Zimbabue.

Diversos países han implementado leyes de protección de los bosques, como las leyes de bosque nativo implementadas por Argentina y Chile.

Un estudio científico analizó el impacto de la Ley de Bosque Nativo de Argentina, centrándose fundamentalmente en los mecanismos de ordenamiento territorial exigidos por la ley y la responsabilidad de los gobiernos provinciales y municipales en realizar dicho ordenamiento. El estudio analizó las provincias de Salta, Santiago del Estero y Chaco (las más afectadas por la deforestación en Argentina) y encontró que la gestión de los gobiernos locales podía tener un impacto en reducir la deforestación.

En Bolivia la deforestación en los cursos fluviales altos ha causado problemas medioambientales, entre ellos erosión del suelo y disminución de la calidad del agua. Un proyecto innovador para remediar la situación establece que los usuarios del agua río abajo paguen a los propietarios de tierras río arriba para conservar sus bosques. Los propietarios reciben 20 dólares norteamericanos para conservar los árboles, evitar prácticas ganaderas contaminantes y favorecer la biodiversidad y la fijación de carbono por el bosque en su propiedad. También reciben 30USD para la compra de una colmena, lo que les compensa por la conservación de dos hectáreas de bosque durante cinco años, de manera que se proteja una fuente de agua. Los ingresos por hectárea de la miel recolectada ascienden a cinco dólares anuales, de modo que en cinco años ascienden a 50USD para el propietario. El proyecto lo llevan la Fundación Natura Bolivia y la organización ecologista Rare, con el apoyo de la Alianza Clima y Desarrollo.

En China el Estado paga 7500 yuanes anuales por hectárea (equivalentes en 2018 a unos 937 euros) durante cinco años a los agricultores de zonas señaladas como prioritarias para la reforestación si abandonan sus cultivos, plantan árboles y se dedican a cuidarlos, limpiando el follaje.

La evidencia disponible sobre el precio de los insumos para la producción agrícola (como fertilizantes y pesticidas) no es concluyente. Un estudio en ciertas áreas de América Latina sugirieron que un incremento en el precio de los fertilizantes puede reducir la deforestación. Diferentes estudios encontraron evidencia de que un incremento en el precio de otros insumos, como semillas, pesticidas y herramientas reduce la deforestación.

La evidencia disponible sugiere que salarios más elevados para los trabajadores agrícolas reduce la deforestación, al hacer que las actividades agrícolas y forestales sean más costosas. Otras ofertas de empleo con mejores salarios en áreas no vinculadas a la actividad agrícola también reducen la deforestación.

Se afirma que transferir la propiedad de los terrenos donde se ubican los bosque a las poblaciones indígenas es una manera eficiente de protegerlos. Esto incluye la protección de tales derechos cuando las leyes existentes los conceden, como en la ley india de bosques. Se sostiene que transferir estos derechos en China, quizá la mayor reforma agraria de la edad contemporánea, ha incrementado la cobertura forestal. En Brasil, las áreas forestales cuya propiedad se ha transferido a pueblos indígenas sufren menos tala permanente que incluso los parques nacionales.

Talar el bosque y plantar con métodos agrícolas tradicionales rinde poco. Algunos métodos agrícolas nuevos que ofrecen mucho mayor rendimiento por hectárea (y por tanto permiten talar menos bosque, o no talarlo en absoluto, si se aplican al terreno donde se usaban métodos tradicionales) son: plantas hibridadas, invernaderos, huertos urbanos o hidroponía. Estos nuevos métodos dependen a menudo de insumos químicos (abonos, pesticidas) para mantener alto su rendimiento. En la agricultura cíclica (llamada así por oposición a la agricultura itinerante, en que una tribu tala una zona de bosque, la cultiva y, cuando la tierra se agota, la abandona para talar una nueva zona) el ganado pasta sobre tierra dejada en barbecho, fertilizándola y preparándola para una próxima siembra. La rotación de cultivos es una forma de agricultura cíclica. Por otra parte la agricultura biointensiva obtiene rendimientos muy altos en terrenos muy reducidos sin emplear sustancias químicas. La agricultura intensiva, en cambio, puede disminuir los nutrientes del suelo a un ritmo acelerado. El enfoque más prometedor, sin embargo, es la jardinería forestal (traducción habitual, pero poco afortunada del término "forest gardening"; poco afortunada porque, en español, la jardinería es ornamental, no nutricional; la traducción francesa, "bosque nutritivo" da una mejor idea del significado) en permacultura, que consiste en sistemas agroforestales, cuidadosamente diseñados para imitar a los bosques naturales, que favorecen las especies animales y vegetales de interés nutricional, maderero y otros usos. Estos sistemas tienen baja dependencia de combustibles fósiles y sustancias químicas, necesitan poco mantenimiento, son altamente productivos y causan poco impacto en el suelo, la calidad del agua y la biodiversidad.

Hay múltiples métodos adecuados y fiables para monitorear la deforestación. Uno de ellos es la interpretación visual de fotos aéreas o imágenes por satélite. Es intensivo en mano de obra, pero no requiere formación de alto nivel en procesamiento automatizado de imágenes ni una fuerte inversión en ordenadores. Otro método es el análisis de los puntos calientes ("hotspots", zonas de rápido cambio) empleando la opinión de expertos o imágenes de satélite de baja resolución para identificar estas zonas, y entonces realizar análisis digitales detallados sobre imágenes satelitales de alta resolución. Normalmente se valora la deforestación cuantificando la cantidad de área deforestada, medida en el momento actual.

Desde un punto de vista medioambiental, cuantificar el daño y sus posibles consecuencias es una tarea más importante, mientras que los esfuerzos de conservación se centran en proteger los bosques y desarrollar usos de la tierra alternativos para evitar que la deforestación continúe. El ritmo de deforestación y el área total deforestada se han utilizado ampliamente para el seguimiento de la deforestación en muchas regiones, entre ellas la Amazonia brasileña por el INPEN(Instituto Nacional de Pesquisas Espaciais). Está disponible una vista satelital de la Tierra.

La certificación de que un bosque se explota de manera sostenible, como la proporcionada por los sistemas mundiales Programa para el Reconocimiento de Certificación Forestal (PEFC por sus siglas en inglés) o Consejo de Administración Forestal (FSC por sus siglas en inglés) contribuye a contener la deforestación al crear mercado para productos de bosques gestionados sosteniblemente. De acuerdo con la FAO, «Una condición indispensable para la adopción de la gestión forestal sostenible es la demanda para productos producidos sosteniblemente y el deseo de los consumidores de pagar por los mayores costes que implican. La certificación representa cambiar de planteamientos regulatorios a incentivos de mercado para promover la gestión forestal sostenible. Al promover los atributos positivos de productos forestales de bosques gestionados sosteniblemente, la certificación se enfoca en el lado de la demanda de la gestión medioambiental.» En cambio, la australiana Rainforest Rescue alega que los estándares de organizaciones como FSC están demasiado conectados con la industria maderera y que por tanto no garantizan una gestión forestal sostenible y socialmente responsable. Que en realidad los sistemas de seguimiento de las certificaciones son inadecuados y en el mundo se han documentado varios casos de fraude.

Algunas naciones han tomado medidas para incrementar el número de árboles sobre la Tierra. En 1981 China creó el día nacional de plantado de árboles y en la década que comenzó en 2001 la cobertura forestal ha alcanzado el 16,55% del territorio cuando en la que comenzó en 1991 solo era del 12%.

Usar como leña el bambú, que técnicamente no es un árbol, sino una hierba (concretamente una gramínea) conduce a una combustión más limpia que la de madera de árbol, y como el bambú madura mucho más rápido que la madera, se reduce la deforestación, porque el suministro se puede reponer más rápidamente.

Para satisfacer la demanda mundial de madera, los silvícolas Botkins y Sedjo proponen plantaciones de árboles de alto rendimiento. Se ha calculado que plantaciones que produzcan 10m³ (metros cúbicos) de madera por hectárea anualmente podrían suministrar toda la madera que demanda el comercio internacional utilizando solamente el 5% del área forestal actual. Los bosques naturales solo producen entre 1 y 2m³ por hectárea, y por tanto se requeriría de 5 a 10 veces más terreno para satisfacer la demanda. El ingeniero de montes Chad Olivier propone un mosaico de bosques de alto rendimiento entremezclados con tierras preservadas.

Los bosques plantados se incrementaron en el mundo del 4,1 al 7,0% de la superficie forestal total entre 1990 y 2015 En 2015 sumaban 280 millones de hectáreas, un incremento de alrededor de 40 millones de hectáreas desde 2010. El 18% de estos 280 millones son especies exóticas o introducidas, mientras que el resto son nativas del país donde se han plantado. En el este y sur de África, Sudamérica y Oceanía los bosques plantados son principalmente de especies introducidas: 65, 88 y 75% respectivamente. En Norteamérica, Asia central y occidental, y Europa, las proporciones de especies introducidas son muy inferiores: 1, 3 y 8% del área total plantada respectivamente.

En Senegal, en la costa oeste de África, un movimiento encabezado por jóvenes ha ayudado a plantar más de seis millones de árboles de manglar. Estos árboles protegeran las aldeas de las tormentas y proporcionarán un hábitat a la fauna y flora local. El proyecto empezó en 2008 y en 2010 ya se ha pedido al Gobierno senegalés que proteja los nuevos manglares.





</doc>
<doc id="9767" url="https://es.wikipedia.org/wiki?curid=9767" title="Cáncer colorrectal">
Cáncer colorrectal

El cáncer colorrectal, también llamado cáncer de colon, cáncer de intestino o cáncer rectal, incluye cualquier tipo de neoplasias del colon, recto y apéndice. Se piensa que muchos de los casos de cáncer colorrectal nacen de un pólipo adenomatoso en el colon. Estos crecimientos celulares en forma de hongo son usualmente benignos, pero de vez en cuando se vuelven cancerosos con el tiempo. En la mayoría de los casos, el diagnóstico del cáncer localizado es por colonoscopia. El tratamiento es por lo general quirúrgico, y en muchos casos es seguido por quimioterapia.

Es la tercera forma más común de cáncer y la segunda causa más importante de mortalidad asociada a cáncer en los Estados Unidos. El cáncer colorrectal causa 694 000 muertes a nivel mundial cada año.

El riesgo de contraer cáncer de colon en los EE. UU. es de alrededor del 7%. Ciertos factores aumentan el riesgo de que una persona desarrolle esta enfermedad, entre ellos:

La página del Instituto Nacional de Cáncer de los Estados Unidos no contempla el alcoholismo como uno de los riesgos del cáncer colorrectal. Sin embargo, otros artículos del mismo instituto citan que el abuso en el consumo de bebidas alcohólicas puede aumentar el riesgo de cáncer colorrectal.

Otros informes citan estudios epidemiológicos en los que se ha notado una leve, aunque consistente asociación del consumo dosis-dependiente de alcohol y el cáncer de colon aunque se esté controlando la fibra y otros factores dietéticos. A pesar del gran número de estudios, la causa de las relaciones alcohol y cáncer de colon aún no ha sido determinada a partir de los datos disponibles.

Un estudio encontró que quienes beben más de 30 gramos de alcohol cada día, y en especial aquellos que beben 45 gramos por día, tienen un riesgo mayor de contraer cáncer colorrectal. Otro estudio demostró que el consumo de una o más bebidas alcohólicas cada día se asocia con una incidencia un 70% mayor de la media de cáncer de colon. Mientras que se encuentra un duplicado riesgo de cáncer de colon por consumir alcohol, incluyendo cerveza, aquellos que beben vino tienen un riesgo disminuido. Las conclusiones de un estudio citan que para minimizar el riesgo de cáncer colorrectal, es mejor beber con moderación.

Para muchos profesionales, entre los que se encuentra el referente internacional en cáncer colorrectal Eduardo Díaz-Rubio, la prevención primaria, un diagnóstico precoz así como estrategias coordinadas entre distintas regiones que eliminen inequidades y variaciones injustificadas en el riesgo de padecer cáncer, mejorarían el pronóstico de la enfermedad.

La patología es un tumor del colon, se reporta por lo general del análisis de tejido obtenido de una biopsia o una operación. El reporte patológico usualmente contiene una descripción del tipo de célula y el grado de avance. El tipo más común de célula cancerígena es el adenocarcinoma, el cual ocupa un 95% de los casos. Otros tipos menos frecuentes incluyen los linfomas y el carcinoma de célula escamosa.

El cáncer del lado derecho (colon ascendente y ciego), tiende a tener un patrón exofítico, es decir, el tumor crece hacia la luz intestinal comenzando desde la pared de la mucosa. Este tipo raramente causa obstrucción del paso de las heces y presenta síntomas como anemia. El cáncer del lado izquierdo tiende a ser circunferencial, y puede obstruir el intestino al rodear la luz del colon.

El adenocarcinoma es un tumor de células epiteliales malignas, originándose del epitelio glandular de la mucosa colorrectal. Invade la pared, se infiltra hacia la muscularis mucosae, la submucosa y la lámina muscularis propia. Las células malignas describen estructuras tubulares, promoviendo estratificación anómala, luz tubular adicional y estromas reducidos. A veces, las células del tumor tienen un patrón de crecimiento "discohesivo" y secretan moco, el cual invade el intersticio, produciendo lagunas mucosas y coloides (en el microscopio se ven como espacios vacíos), llamados adenocarcinoma "mucinosa" o coloide, pobremente diferenciado. Si el moco permanece dentro de la célula maligna, empuja el núcleo hacia la periferia, formando la característica célula en anillo de sello. Dependiendo de la arquitectura glandular, el pleomorfismo celular y la mucosecreción del patrón predominante, el adenoma puede presentar tres grados de diferenciación: pobre, moderadamente o bien diferenciada.

La presencia de mutaciones del gen K-ras (gen de la familia Ras) es de veraz importancia ya que supone la aplicación de un tratamiento del cáncer colorrectal distinto al empleado en pacientes sin dicha mutación. Estas mutaciones constituyen un factor predictivo negativo de respuesta a la terapia con anti-EGFR en esta patología. Además, el valor pronóstico del oncogén K-ras en el cáncer colorrectal también es de importante consideración.

El cáncer colorrectal no suele dar síntomas hasta fases avanzadas y por eso la mayoría de pacientes presentan tumores que han invadido toda la pared intestinal o han afectado los ganglios regionales. Cuando aparecen, los síntomas y signos del carcinoma colorrectal son variables e inespecíficos. La edad habitual de desarrollo del cáncer colorrectal es entre los 60 y 80 años de edad. En las formas hereditarias el diagnóstico acostumbra a ser antes de los 50 años. Los síntomas más frecuentes incluyen la hemorragia digestiva baja y la rectorragia, cambios en las defecaciones y dolor abdominal. La presencia de síntomas notables o la forma en que se manifiestan depende un poco del sitio del tumor y la extensión de la enfermedad:

Los síntomas principales son dolor abdominal, síndrome anémico y, ocasionalmente, la palpación de un tumor abdominal. Como el contenido intestinal es relativamente líquido cuando atraviesa la válvula ileocecal y pasa al colon derecho, en esta localización los tumores pueden llegar a ser bastante grandes, produciendo una estenosis importante de la luz intestinal, sin provocar síntomas obstructivos o alteraciones notables del hábito intestinal. El "dolor abdominal" ocurre en más del 60% de los pacientes referido en la mitad derecha del abdomen. El "síndrome anémico" ocurre también en más del 60% de los casos y se debe a pérdida continuada, aunque mínima, de sangre que no modifica el aspecto de las heces, a partir de la superficie ulcerada del tumor. Los pacientes refieren fatiga (cansancio, debilidad) palpitaciones e incluso angina de pecho y se les descubre una anemia microcítica e hipocroma que indica un déficit de hierro. Sin embargo, como el cáncer puede sangrar de forma intermitente, una prueba realizada al azar para detectar sangre oculta en heces puede ser negativa. Como consecuencia, la presencia de una anemia ferropénica en cualquier adulto, con la posible excepción de la mujer multípara premenopáusica, obliga a hacer un estudio preciso endoscópico y radiológico de todo el colon. Por razones desconocidas, las personas de Etnia Negra tienen una incidencia mayor de lesiones en el colon derecho que las personas de Etnia Blanca. Puede pasar desapercibido si se localiza en el ángulo hepático del colon y éste se oculta bajo la parrilla costal.

Por ser más estrecho, el dolor cólico en abdomen inferior puede aliviarse con las defecaciones, en el caso de algunos pacientes puede desarrollar anemia por falta de hierro igual que en el caso de Cáncer de colon derecho es importante darse cuenta que no solo pierda sangre por las heces sino también por otros orificios del cuerpo como puede ser por los orificios nasales(nariz) o por la boca. Es más probable que estos pacientes noten un cambio en las defecaciones y eliminación de sangre roja brillante (rectorragia) condicionados por la reducción de la luz del colon. El crecimiento del tumor puede ocluir la luz intestinal provocando un cuadro de obstrucción intestinal con dolor cólico, distensión abdominal, vómitos y cierre intestinal.

Como las heces se van concentrando a medida que atraviesan el colon transverso y el colon descendente, los tumores localizados a este nivel tienden a impedir su paso al exterior, lo que origina un dolor abdominal tipo cólico, a veces con "obstrucción intestinal" (ileo obstructivo) e incluso con perforación intestinal. En esta localización es frecuente la rectorragia, tenesmo rectal y disminución del diámetro de las heces. Sin embargo, la anemia es un hallazgo infrecuente. A veces la rectorragia y el tenesmo rectal son síntomas frecuentes de hemorroides, pero ante una rectorragia con o sin trastornos del hábito intestinal (diarrea o estreñimiento) es preciso realizar un tacto rectal y una proctosigmoidoscopia. La uretritis ocurre cuando el tumor se encuentra muy cerca de la uretra y puede comprimirla y originar infecciones recurrentes urinarias. Cuando su extensión sobrepasa los límites de la pared rectal, el paciente puede aquejar síntomas urinarios atribuibles a invasión vesical como hematuria y polaquiuria. Si aparece una fístula rectovesical hay neumaturia e infecciones urinarias recidivantes.

Existen varias pruebas que se usan para detectar el cáncer colorrectal. Con los síntomas que relate el paciente al médico, se realizará una historia clínica, donde se detallarán los síntomas, los antecedentes familiares y factores de riesgo en la anamnesis. El médico también le hará una exploración física completa que incluirá un tacto rectal. Con los datos obtenidos se solicitarán exploraciones complementarias o pruebas diagnósticas para confirmar el diagnóstico, determinar un estadio clínico y establecer un plan de tratamiento.

Mediante el tacto rectal se pueden palpar el 20% de los carcinomas colorrectales y valorar su grado de fijación al tejido vecino. El tacto rectal puede llegar casi 8 cm por encima de la línea pectínea. Aunque se ha demostrado que casi la mitad de los cánceres colorrectales ocurrirán cerca del ángulo esplénico (y serían inaccesibles), un restante 20% puede palparse. En caso de un cáncer de recto es necesario hacer un tacto rectal cuidadoso, para valorar el tamaño, fijación y ulceración del cáncer, así como el estado de los ganglios u órganos vecinos y la distancia del extremo distal del tumor al margen anal.

El tacto rectal debe formar parte de cualquier exploración física de rutina en adultos mayores de 40 años, ya que sirve como prueba de detección de cáncer de próstata en hombres, y es parte de la exploración de la pelvis en las mujeres, y una maniobra barata para detectar masas en el recto. El tacto rectal no se recomienda como única prueba para el cáncer colorrectal, porque no es muy preciso debido a su alcance limitado, pero es necesario realizarlo antes de introducir el sigmoidoscopio o el colonoscopio.

Si bien esta práctica es ampliamente conocida, y fácil de realizar, la mayoría de los tumores no se encuentran al alcance del dedo, y cuando estos son palpables el pronóstico ya suele ser ominoso. Quedando de esta manera otras alternativas como la solicitud de sangre oculta en materia fecal como un método más fiable y que ha demostrado disminuir la mortalidad por cáncer de colon en un 33% en algunos estudios.

La prueba de sangre oculta en las heces (PSOH) se usa para detectar sangre invisible en los excrementos. Los vasos sanguíneos que se encuentran en la superficie de los pólipos, adenomas o tumores colorrectales, frecuentemente son frágiles y se dañan fácilmente durante el paso de las heces. Los vasos dañados normalmente liberan una pequeña cantidad de sangre en el excremento. Solo raramente hay sangrado suficiente para que las heces se tiñan de rojo (rectorragia o hematoquecia). La PSOH detecta la presencia de sangre mediante una reacción química. Si esta prueba es positiva, es necesario realizar una colonoscopia para ver si es un cáncer, un pólipo o si hay otra causa del sangrado, como por ejemplo hemorroides, diverticulitis o enfermedad inflamatoria intestinal. Los alimentos o los medicamentos pueden afectar los resultados de esta prueba, por lo cual es necesario evitar lo siguiente:


Las personas que se hacen esta prueba deben recibir instrucciones detalladas que expliquen cómo obtener una muestra de heces o excremento en el hogar (generalmente tres muestras). El material se entrega al consultorio del médico o a un laboratorio clínico para su posterior análisis. La prueba de una muestra de heces que el médico obtenga mediante un tacto rectal no es una prueba adecuada de PSOH.

Aunque la PSOH se realice en condiciones ideales, tiene limitaciones importantes como técnica de detección precoz. Aproximadamente el 50% de los pacientes con cáncer colorrectal demostrado, tienen la PSOH negativa (falso negativo), un hecho relacionado con el patrón de hemorragias intermitentes de estos tumores. Cuando se hacen estudios aleatorizados en cohortes de personas asintomáticas, de un 2 a un 4% tienen una PSOH positiva. Pero solo de un 5 a un 10 % de estos pacientes tiene un cáncer colorrectal (el 90-95% son falsos positivos) y en un 20 a un 30% se encuentran pólipos benignos. Por tanto, en la mayoría de las personas asintomáticas con la PSOH positiva no se encontrará una neoplasia colorrectal. No obstante, las personas con PSOH positiva deben someterse de forma sistemática, a más estudios médicos, que incluyen sigmoidoscopia, enema de bario y colonoscopia, técnicas que no solo son incómodas y caras, sino que también se asocian con un riesgo bajo, pero real de complicaciones importantes. El coste de estos estudios justificaría si el número pequeño de pacientes con neoplasia oculta que se descubren por tener una PSOH positiva tuvieran un pronóstico mejor y un aumento de la supervivencia.

Para algunas asociaciones médicas el cribado poblacional basado exclusivamente en la PSOH no es aconsejable, mientras que para otros lo es. Los ensayos en los que se ha investigado este planteamiento son plenamente maduros con aproximadamente 300.000 participantes en ensayos aleatorizados bien diseñados. Demuestran que la reducción de la mortalidad existe, aunque en algunos casos, dependiendo de la técnica usada para la PSOH es modesta y después de corregir por un sesgo de observación, la reducción de la mortalidad por cáncer colorrectal no resultó estadísticamente significativa. La aparente simplicidad de la prueba no puede ser un argumento a favor de su uso generalizado. La mala especificidad de la prueba—es decir, la PSOH puede ser positiva en otras patologías—puede conducir a que una gran proporción de pacientes se sometan indebidamente a enema de bario y colonoscopias repetidas.

Actualmente existen varios tipos de PSOH: el más antiguo es el test de guayaco que busca la presencia o ausencia de actividad peroxidasa del grupo hemo en las deposiciones, es éste el que arroja gran cantidad de falsos positivos. Existe también el test inmunohistoquímico que consiste en anticuerpos mono o policlonales que detectan porciones intactas de hemoglobina humana, disminuye los falsos positivos con hemoglobinas no humanas (carnes rojas, vitamina C, etc.). Últimamente se puede encontrar un test inmunohistoquímico que detecta mutaciones de ADN, puede encontrar 15 aberraciones frecuentes en K-ras, APC, p53, etc. Es más sensible y específico en la detección del cáncer colorrectal.

Un colonoscopio es un tubo iluminado, delgado, flexible y hueco, que tiene el grosor aproximado de un dedo. Se introduce a través del recto, en la parte inferior del colon. El médico además de ver a través del sigmoidoscopio para detectar cualquier anomalía, también puede conectarlo a una cámara de vídeo y a un monitor de vídeo para visualizarlo mejor y grabarlo en algún soporte como documento visual. Esta prueba puede ser algo incómoda, pero no debe ser dolorosa. Debido a que tiene solo 60 centímetros de largo, solo se puede ver menos de la mitad del colon. Antes de la sigmoidoscopia, el paciente debe aplicarse un enema para limpiar la porción inferior del colon.

Las estrategias de detección precoz se han basado en el supuesto de que más del 60% de las lesiones precoces se localizan en el rectosigma y por tanto son accesibles con el sigmoidoscopio. Sin embargo, por razones desconocidas, en los últimos decenios se ha producido una disminución constante de la proporción de cánceres de intestino grueso que se localizan inicialmente en el recto con el correspondiente aumento de los que lo hacen en la zona proximal del colon descendente.

Esta técnica indudablemente tiene importantes dificultades como:

Se trata de un tubo con iluminación mediante el que se puede detectar entre un 20-25% de los carcinomas colorrectales. Es útil para selección de adultos menores de 40 años con riesgo.

El sigmoidocopio es un instrumento fibróptico que mide 6 cm de largo, útil para la exploración del colon izquierdo, pudiendo llegar hasta el ángulo esplénico. No requiere preparación completa del intestino, no debe utilizarse para polipectomía terapéutica (excepto circunstancias especiales) y puede detectar el 50% de los carcinomas más comunes.

Este estudio permite observar la mucosa de la totalidad del colon, recto y por lo general del íleon terminal. El colonoscopio es un tubo flexible con una cámara de vídeo en la punta y mide 160 cm de largo. La colonoscopia es el método más preciso para detectar pólipos menores de 1 cm de diámetro. También permite tomar biopsias, realizar polipectomías, controlar hemorragias y dilatar estrecheces. En el caso de cáncer de recto es necesario observarlo con un sigmoidoscopio rígido, tomar una biopsia adecuada, predecir el riesgo de obstrucción y medir cuidadosamente la distancia desde el borde distal del tumor hasta la línea pectínea. En la actualidad, la colonoscopia es el examen más preciso y completo del intestino grueso, pero esta prueba junto con el enema con bario deben considerarse complementarios entre sí. Un colonoscopio es una versión larga del sigmoidoscopio. Se introduce a través del recto hasta el ciego, y permite observar la mucosa de todo el colon.

Si se encuentra un pólipo pequeño, de menos de 3 cm, generalmente es posible la polipectomía. Algunos tipos de pólipo, incluso los que no son cancerosos, podrían malignizarse y por eso normalmente se extirpan. La polipectomía endoscópica se realiza pasando un asa de alambre a través del colonoscopio para cortar el pólipo de la pared del colon mediante una corriente eléctrica. Siempre que es posible, el pólipo se envía a anatomía patológica para analizarla en un microscopio y detectar si tiene áreas que se hayan malignizado.

Si se detecta un pólipo o tumor de gran tamaño o cualquier otra anomalía, se realizará una biopsia. Para tomar una biopsia a través del colonoscopio se extrae una pequeña porción de tejido. El examen del tejido puede ayudar a determinar si es un cáncer, un crecimiento benigno o el resultado de una inflamación.

Antes de realizar una colonoscopia el paciente debe tomar productos evacuantes diferentes de los laxantes habituales y, a veces, debe ponerse enemas para limpiar el colon, de manera que no haya heces que dificulten la visión. Normalmente la colonoscopia no provoca dolor, porque durante el acto se administran analgésicos y sedantes intravenosos. La colonoscopia se suele realizar ambulatoriamente y el paciente rara vez requiere ingreso hospitalario para esta prueba. Normalmente dura de 15 a 30 minutos, aunque puede tardar más si fuera necesario extirpar un pólipo.

La colonoscopia se debe realizar ante una prueba de sangre oculta en heces positiva, ante el hallazgo de un pólipo o tumor en la sigmoidoscopia o ante un enema de bario sospechoso, y es recomendable realizarla siempre que se tengan antecedentes familiares de pólipos o cáncer de colon, así como en mayores de 50 años. Otras indicaciones habituales son la emisión de sangre con las heces, los cambios en el ritmo intestinal de reciente comienzo o la anemia por falta de hierro en varones o mujeres postmenopáusicas.

El sulfato de bario es una sustancia radioopaca que se usa para llenar parcialmente y abrir el colon. El sulfato de bario se administra a través de un pequeño tubo introducido en el ano. Cuando el colon está aproximadamente medio lleno de bario, se coloca al paciente sobre una mesa de rayos X para que el bario se disperse a través del colon. Luego se bombeará aire en el colon a través del mismo tubo, a fin de que se expanda. Esto produce las mejores imágenes de la mucosa del colon. Es preciso que el paciente tome laxantes la noche anterior y ponerse un enema de limpieza la mañana antes de esta prueba para que el colon esté limpio de heces.

El estudio de contraste de uso más frecuente para detectar cáncer colorrectal es el "enema de bario con doble contraste de aire" pues tiene una sensibilidad del 90 % para detectar pólipos mayores de 1 cm. Está siendo desplazado por la colonoscopia, aunque es más barato y accesible, por lo que se puede utilizar en pacientes con alta sospecha, mientras se espera a la realización de una colonoscopia. Junto con la sigmoidoscopia flexible es una alternativa eficaz para los pacientes que no toleran la colonoscopia o para el seguimiento a largo plazo tras resección de un cáncer o pólipo. También es útil en caso de lesión estenosante que impida el paso del colonoscopio.
Posibles imágenes que podemos encontrar sugerentes de cáncer colorrectal son:

El paciente debe tener limpio el colon de heces al igual que en la colonoscopia o el enema de bario, mediante laxantes y enemas de limpieza. En esta prueba no se introduce contraste en el colon, solo se insufla aire para dilatarlo. Luego se realiza una tomografía computarizada especial llamada tomografía computarizada helicoidal o espiral. Este procedimiento es probablemente más preciso que el enema con bario, pero no es tan eficaz como la colonoscopia para detectar pólipos pequeños. La ventaja es que este procedimiento se puede realizar rápidamente y no requiere que se sede al paciente y a un costo menor que la colonoscopia. Sin embargo, una desventaja es que si se detecta un pólipo o neoplasia, no se puede llevar a cabo una biopsia o extirpación del pólipo durante el examen. Actualmente se incluye la colonoscopia virtual en las pruebas recomendadas por la Sociedad Americana del Cáncer en sus "Guías para el cribado del Cáncer Colorrectal 2008" para la detección precoz del cáncer colorrectal como opción alternativa a la colonoscopia clásica para aquellos pacientes que no desean realizarse una colonoscopia clásica.

Otras pruebas que también se deben realizar son:
Asimismo, desde 2010 está disponible la detección en sangre de un nuevo marcador tumoral genético para el cáncer de colon: la forma metilada del gen (mSEPT9), que se encuentra en más del 90% de los tumores de colon, pasando a la sangre en forma de ADN libre. La presencia de mSEPT9 en el plasma indica la posibilidad de que exista una neoformación relacionada con cáncer de colon. Este marcador se encuentra en otro tipo de tumores con muy poca frecuencia.

No se puede usar la ecografía para detectar tumores en el colon.

Un tipo especial de TAC, es la TAC espiral que proporciona gran detalle y también es útil para diagnosticar metástasis de cáncer colorrectal. En el TAC espiral con portografías, el material de contraste se inyecta en la vena porta, para ayudar a diagnosticar metástasis del cáncer colorrectal en el hígado.
La TAC también se utiliza para guiar con precisión una aguja de biopsia hacia una posible metástasis. Para este procedimiento, llamado biopsia con aguja guiada por TAC, el paciente permanece en la mesa de TAC, mientras se introduce una aguja de biopsia hacia la localización exacta del tumor. La TAC continúa hasta que se está seguro de que la aguja se encuentra dentro de la masa. Se extrae una pequeña muestra de tejido mediante una biopsia con aguja y se examina al microscopio.


El tratamiento del cáncer colorrectal puede incluir:

Un porcentaje importante de pacientes se atiende por primera vez con síntomas agudos que indican obstrucción o perforación del intestino grueso.
Desafortunadamente es posible que los primeros signos de cáncer de colon dependan de una enfermedad metastásica. Las metástasis hepáticas masivas pueden causar prurito e ictericia. La presencia de ascitis, ovarios crecidos y depósitos diseminado en los pulmones en la radiografía de tórax pueden deberse a un cáncer de colon que puede ser asintomático.
Las principales.

Si la obstrucción no se alivia y el colon continúa distendido, la presión en la pared intestinal puede exceder la de los capilares y no llegar la sangre oxigenada a la pared del intestino, lo que origina isquemia y necrosis. Si no se trata inmediatamente, la necrosis evolucionará hasta la perforación con peritonitis fecal y sepsis.
La obstrucción intestinal baja se produce fundamentalmente en el carcinoma de colon izquierdo (debido al menor calibre de su luz). La sintomatología típica de la obstrucción intestinal baja es la de dolor cólico, vómitos, distensión abdominal y ausencia de emisión de gases y heces.
Por tanto siempre debemos incluir al cáncer de colon en el diagnóstico diferencial de las obstrucciones intestinales agudas bajas.

El cáncer colorrectal puede diseminarse de cinco formas diferentes:

Los exámenes de prevención se utilizan para detectar pólipos y evitar que evolucionen a cáncer.
Los exámenes de detección precoz se usan para detectar el cáncer en sus fases iniciales, aunque no existan síntomas ni antecedentes de dicha enfermedad. Las pruebas de detección precoz del cáncer colorrectal no solo pueden diagnosticarlo en una etapa temprana y curable, sino que también pueden prevenirlo al encontrar y extirpar pólipos que pueden malignizarse. Los cánceres también se pueden diagnosticar en sus etapas tempranas si el paciente comunica inmediatamente al médico cualquier síntoma, pero es mejor si se somete a pruebas de diagnóstico precoz antes de que aparezcan los síntomas.

La Sociedad Americana del Cáncer, recomienda tanto a hombres como mujeres a partir de los 50 años de edad, las siguientes tres opciones de prevención del cáncer y tres opciones de detección precoz del cáncer :

Prevención:
- Colonoscopia cada 10 años (Prueba de prevención preferida).
- Sigmoidoscopia flexible cada 5 años (Prueba de prevención alternativa).
- Colonoscopia virtual cada 5 años (Prueba de prevención alternativa).

Detección:
- Test de Inmunohistoquímica Fecal en sangre (FIT) anual (Prueba de detección preferida).
- Prueba anual de sangre oculta en heces (PSOH) (Prueba de detección alternativa).
- Test del ADN fecal cada 3 años. (Prueba de detección alternativa).

Debe someterse a pruebas de prevencíon y/o detección precoz de cáncer colorrectal a una edad más joven o hacérselas con mayor frecuencia, si existe cualquiera de los siguientes factores de riesgo de cáncer colorrectal:

Si la colonoscopia no está disponible, no es factible o el paciente no la desea, un enema de bario de doble contraste solamente, o la combinación de sigmoidoscopia flexible y enema de bario de doble contraste (EBDC) son alternativas aceptables. La adición de la sigmoidoscopia al EBDC puede proporcionar una evaluación diagnóstica más completa que el EBDC por sí solo, para encontrar lesiones significativas. Si el examen colonoscópico no puede alcanzar el ciego, es posible que se necesite un EBDC complementario, y si el EBDC identifica una lesión posible o no permite la visualización adecuada de todo el colon y recto, es posible que sea necesario realizar una colonoscopia complementaria.

Las personas con enfermedad celíaca a tratamiento con la dieta sin gluten tienen un riesgo más bajo de desarrollar cáncer de colon que el resto de la población. Un estudio de 2014 concluyó que la dieta sin gluten parece ejercer un efecto protector, puesto que este pequeño riesgo disminuye a partir del año de dieta sin gluten y es más bajo aún en las personas que hacen la dieta correctamente sin transgresiones, es decir, de forma estricta.




</doc>
<doc id="9768" url="https://es.wikipedia.org/wiki?curid=9768" title="Aparato digestivo">
Aparato digestivo

El aparato digestivo es el conjunto de órganos encargados del proceso de la digestión, es decir, la transformación de los alimentos para que puedan ser absorbidos y utilizados por las células del organismo. Las funciones que realiza son: transporte de alimentos, secreción de jugos digestivos, absorción de nutrientes y excreción mediante el proceso de defecación. El proceso de la digestión consiste en transmitir los glúcidos, lípidos y proteínas contenidos en los alimentos en unidades más sencillas, gracias a las enzimas digestivas, para que puedan ser absorbidos y transportados por la sangre.

El tubo digestivo mide aproximadamente once metros de longitud, se inicia en la cavidad bucal y terminan en el ano. En la boca empieza propiamente la digestión, los dientes trituran los alimentos y las secreciones de las glándulas salivales los humedecen e inician su descomposición química transformándose en el bolo alimenticio. Luego, el bolo alimenticio cruza la faringe, sigue por el esófago y llega al estómago, una bolsa muscular de litro y medio de capacidad cuya mucosa segrega el potente jugo gástrico. En el estómago el alimento es agitado hasta convertirse en el quimo.

A la salida del estómago se encuentra el intestino delgado que mide seis metros de largo y se encuentra muy replegado sobre sí mismo. En su primera porción o duodeno recibe secreciones de las glándulas intestinales, la bilis procedente de la vesícula biliar y los jugos del páncreas. Todas estas secreciones contienen gran cantidad de enzimas que degradan los alimentos y los transforman en sustancias solubles simples como aminoácidos.
El tubo digestivo continúa por el intestino grueso, de algo más de metro y medio de longitud. Su porción final es el recto, que termina en el ano, por donde se evacuan al exterior los restos indigeribles de los alimentos.

El aparato digestivo está formado por el tubo digestivo y las glándulas anexas (glándulas salivales, hígado y páncreas). El tubo digestivo procede embriológicamente del endodermo, al igual que el aparato respiratorio y presenta una sistematización prototípica, comienza en la boca y se extiende hasta el ano. Su longitud en el hombre es de 10 a 12 metros, siendo seis o siete veces la longitud total del cuerpo. En su trayecto a lo largo del tronco, discurre por delante de la columna vertebral. Comienza en la cara, desciende por el cuello y atraviesa las tres grandes cavidades del cuerpo: torácica, abdominal y pélvica. En el cuello está en relación con el conducto respiratorio, en el tórax se sitúa en el mediastino posterior entre los dos pulmones y el corazón, y en el abdomen y pelvis se relaciona con los diferentes órganos del aparato genitourinario.

Histológicamente la pared del tubo digestivo está formado por cuatro capas concéntricas que son de adentro hacia afuera:


El grosor de la pared y el aspecto de superficie, que puede ser lisa o no, cambian dependiendo del lugar anatómico. La mucosa puede presentar criptas y vellosidades, la submucosa puede presentar pliegues permanentes o pliegues funcionales. En la pared se encuentran también los plexos submucoso y mientérico que constituyen el sistema nervioso entérico que se distribuye a lo largo de todo el tubo digestivo, desde el esófago hasta el ano.

Los alimentos después de ser ingeridos y triturados por los dientes con la ayuda de la saliva producida por las glándulas salivares, forman un bolo alimenticio y pasan por el esófago en su camino hacia el estómago gracias al movimiento peristáltico. Una vez en el estómago, se inicia el proceso de digestión facilitado por el ácido clorhídrico secretado por las células parietales del estómago y las enzimas digestivas. Posteriormente pasan al intestino delgado, donde continúa la degradación química de los alimentos y tiene lugar la absorción de agua y nutrientes que son transportados hacia la sangre y la linfa. Al alcanzar el intestino grueso se acumulan las sustancias de desecho que forman las heces, las cuales se expulsan al exterior a través del ano.

El tubo digestivo es la principal superficie de intercambio entre el medio externo y el interno en los animales vertebrados. En un hombre adulto medio la superficie total de la mucosa gastrointestinal desplegando las microvellosidades intestinales es de alrededor de 350 metros cuadrados. Gracias al tubo digestivo el individuo puede realizar el proceso de nutrición mediante la digestión y absorción de los nutrientes contenidos en los alimentos, pero no es menos importante su función de defensa, pues dispone de sistemas de reconocimiento y rechazo de agentes o sustancias extrañas procedentes del mundo exterior.

El intestino posee en su interior una capa de células que forman una barrera. Su misión es, además de digerir sustancias, actuar defendiendo al organismo del enemigo exterior del ambiente (sustancias que ingerimos y microorganismos presentes en el intestino). Esto lo logra manteniendo cerradas las uniones estrechas intercelulares, para impedir el acceso descontrolado de sustancias, toxinas, químicos, microorganismos y macromoléculas, que de lo contrario podrían pasar al torrente sanguíneo. Actualmente, se sabe que las uniones estrechas, anteriormente consideradas como estructuras estáticas, son en realidad dinámicas y se adaptan fácilmente a diversas circunstancias, tanto fisiológicas como patológicas. Existe un complejo sistema regulador que orquesta el estado de ensamblaje de la red de proteínas de las uniones estrechas intercelulares. Asimismo, juega un papel muy importante la colonización bacteriana que constituye la llamada microflora intestinal formada por bacterias beneficiosas para el organismo. Se calcula que un individuo normal tiene en su intestino alrededor de 100 billones de bacterias pertenecientes a entre 500 y 1000 especies diferentes.

Cuando no funcionan bien las entradas entre las células (las uniones estrechas intercelulares) y en lugar de estar cerradas o prácticamente cerradas, como deberían, se encuentran abiertas sin control, se produce un aumento de la permeabilidad intestinal. Esta apertura provoca que entren sustancias en el cuerpo y que, dependiendo de la predisposición genética de la persona, puedan desarrollarse enfermedades autoinmunes, inflamatorias, infecciones, alergias o cánceres, tanto intestinales como en otros órganos.

Hasta fechas recientes, se asumía que los bebés nacen completamente libres de gérmenes y que la colonización inicial del intestino del recién nacido se produce durante el parto. No obstante, varios estudios concluyen que esta colonización comienza antes del nacimiento del bebé. Las bacterias maternas pasan de la madre al aparato digestivo del feto desde las primeras fases del embarazo, si bien no se conocen los posibles mecanismos implicados en este fenómeno.

Las enzimas digestivas son sustancias capaces de romper las grandes moléculas presentes en los alimentos y convertirlas en moléculas más pequeñas que pueden ser absorbidas a través del intestino. Algunas de las más importantes son la lipasa producidas por el páncreas, las proteasas producidas por el estómago y el páncreas que descomponen las proteínas en aminoácidos, la amilasa, la lactasa secretada por el intestino delgado que descompone la lactosa presente en la leche y la sacarasa que actúa sobre la sacarosa y la convierte en glucosa y fructosa.

La boca o cavidad oral es el lugar por donde los alimentos comienzan su viaje a través del aparato digestivo, contiene diferentes estructuras, entre ellas los dientes que hacen posible la masticación y la lengua. Cerca de la boca se encuentran las glándulas salivales que producen saliva, la cual se mezcla con los alimentos, facilita la masticación, la deglución y ayuda a mantener los dientes limpios y buen aliento.

La faringe es una estructura con forma de tubo, está situada en el cuello y revestida de membrana mucosa; conecta la cavidad bucal y las fosas nasales con el esófago y la laringe respectivamente. Por ella pasan tanto el aire como los alimentos, por lo que forma parte del aparato digestivo y del aparato respiratorio. Ambas vías quedan separadas por la epiglotis, que actúa como una válvula. En el ser humano la faringe mide unos trece centímetros de largo y se extiende desde la base externa del cráneo hasta la sexta o séptima vértebra cervical, por delante de la columna vertebral.

El esófago es un conducto que se extiende desde la faringe hasta el estómago. De los incisivos al cardias (porción donde el esófago se continúa con el estómago) hay unos 40 cm. El esófago empieza en el cuello, atraviesa todo el tórax y pasa al abdomen a través del orificio esofágico del diafragma. Habitualmente es una cavidad virtual (sus paredes se encuentran unidas y solo se abren cuando pasa el bolo alimenticio). El esófago alcanza a medir 25 cm y tiene una estructura formada por dos capas de músculos, que permiten la contracción y relajación en sentido descendente del esófago, estas ondas reciben el nombre de movimientos peristálticos y son las que provocan el avance del alimento hacia el estómago.

El estómago es un órgano en el que se acumula comida. Varía de forma según el estado de repleción (cantidad de contenido alimenticio presente en la cavidad gástrica) en que se halla, habitualmente tiene forma de "J". Consta de varias partes que son: fundus, cuerpo, antro y píloro. Su borde menos extenso se denomina curvatura menor y la otra, curvatura mayor. El cardias es el límite entre el esófago y el estómago y el píloro es el límite entre el estómago y el intestino delgado. En un individuo de tamaño medio mide aproximadamente 25 cm del cardias al píloro y el diámetro transverso es de 12 cm.

En su interior encontramos principalmente dos tipos de células: 

La secreción de jugo gástrico está regulada tanto por el sistema nervioso como el sistema endocrino, proceso en el que actúan varias sustancias: gastrina, colecistoquinina, secretina y péptido inhibidor gástrico. Cuando la comida llega al estómago, actúa sobre ella el ácido clorhídrico. El ácido clorhídrico degrada las proteínas de los alimentos y activa la pepsina que es una enzima que actúa también sobre las proteínas. En el estómago se secreta también una enzima lipasa que interviene en la degradación de las grasas, pero su papel es muy escaso. Los alimentos mezclados con los jugos gástricos y el moco producido por las células secretoras del estómago forman una sustancia semilíquida que se denomina quimo, la cual avanza hacia el intestino delgado para continuar el proceso de digestión.

Es una glándula íntimamente relacionada con el duodeno, produce jugo pancreático que se vierte al intestino a través del conducto pancreático, sus secreciones son de gran importancia en la digestión de los alimentos. El páncreas segrega también hormonas como la insulina que pasan directamente a sangre y ayudan a controlar el metabolismo de los azúcares.

El hígado es la mayor víscera del cuerpo. Pesa 1500 gramos. Consta de cuatro lóbulos, derecho, izquierdo, cuadrado y caudado; los cuales a su vez se dividen en segmentos. 

Las vías biliares son las vías excretoras del hígado, por ellas la bilis es conducida al duodeno. Normalmente los conductos hepáticos derecho e izquierdo confluyen entre sí formando el conducto hepático común. El conducto hepático común, recibe un conducto más fino, el conducto cístico, que proviene de la vesícula biliar. De la reunión de los conductos císticos y el hepático común se forma el colédoco que desemboca en el duodeno junto con el conducto excretor del páncreas.

La vesícula biliar es una víscera hueca pequeña situada en la cara inferior del hígado. Su función es la de almacenar y concentrar la bilis segregada por el hígado, hasta ser requerida por los procesos de la digestión. Cuando se contrae expulsa la bilis concentrada hacia el duodeno a través del conducto cístico. Es de forma ovalada o ligeramente piriforme y su diámetro mayor oscila entre 5 y 8 cm.

El intestino delgado comienza en el duodeno (tras el píloro) y termina en la válvula ileocecal, donde se une a la primera parte del intestino grueso. Mide entre 6 y 7 metros de longitud y de 2.5 a 3 cm de diámetro. Su calibre disminuye progresivamente desde su origen hasta la válvula ileocecal. 

En el intestino delgado se absorben los nutrientes de los alimentos ya digeridos. El tubo está repleto de vellosidades que amplían la superficie de absorción. El intestino delgado se divide en dos partes, la primera es el duodeno que tiene una longitud de 30 cm y la segunda es el yeyuno-íleon que mide 6 metros y medio.



El intestino grueso se inicia a partir de la válvula ileocecal en un fondo de saco denominado ciego y termina en el recto. Desde el ciego al recto describe una serie de curvas, formando un marco en cuyo centro están las asas del yeyuno e íleon. Su longitud es variable, entre 120 y 160 cm, y su calibre disminuye progresivamente, siendo la porción más estrecha la región donde se une con el recto o unión rectosigmoidea en la que su diámetro no suele sobrepasar los 3 cm, mientras que el ciego es de 6 o 7 cm.

El intestino grueso se divide en varias porciones que se denominan ciego, colon ascendente con una longitud de 15 cm, colon transverso con una longitud media de 50 cm, colon descendente con 10 cm de longitud, colon sigmoideo, recto y ano. El recto es la parte terminal del tubo digestivo.

El ano es la abertura final del tracto digestivo. Consta de una esfínter anal externo y otro interno que tienen la función de controlar el proceso de expulsión de las heces al exterior. El funcionamiento inadecuado de los esfínteres del ano puede provocar incontinencia fecal.

El sistema digestivo se origina a partir del tubo digestivo primitivo, el cual se forma de la capa embrionaria conocida como endodermo, sin embargo, la boca procede del ectodermo. El primitivo tubo digestivo se divide en cinco porciones que partiendo de la boca se llaman faringe, intestino anterior, intestino medio, intestino posterior y cloaca. 
El páncreas se forma a partir de dos esbozos del endodermo que aparecen en la 4ª y 5ª semana y acaban por unirse. El hígado tiene un origen embriológico complejo pues las células hepáticas proceden de un esbozo del endodermo, mientras que la cápsula de Glisson y los sinusoides hepáticos proceden del mesodermo.

El aparato digestivo es un sistema fundamental para el cuerpo. Algunas de las enfermedades que le afectan son las siguientes:


En todos los vertebrados el aparato digestivo es básicamente un tubo hueco que discurre a lo largo del organismo desde la boca hasta el ano. Sin embargo, existen importantes diferencias dependiendo de la especie animal, entre otras razones por estar adaptadas cada una de ellas a un tipo de alimentación. 

El sistema digestivo de las aves cuenta con algunos órganos específicos, por ejemplo el buche, que es una bolsa de tejido conectada con el esófago a la altura del cuello, en la que los alimentos ingeridos se almacenan temporalmente antes de pasar al resto del tubo digestivo.

En los reptiles, el intestino delgado desemboca en el recto y este en la cloaca que se utiliza como desembocadura común tanto del aparato digestivo como del aparato urinario y el sistema reproductor. En muchas especies el estómago puede distenderse enormemente lo que les permite engullir hasta el 70 % de su peso en una sola comida.

En los rumiantes, por ejemplo, el estómago se divide en varias cámaras, en sucesión continua desde el esófago hasta el duodeno, las cuatro cavidades son: Rumen o panza, redecilla o bonete, omaso o librillo y abomaso, cuajar o estómago verdadero.




</doc>
<doc id="9770" url="https://es.wikipedia.org/wiki?curid=9770" title="Windsurf">
Windsurf

El surf a vela, windsurf o tabla a vela es una modalidad del deporte a vela que consiste en desplazarse en el agua sobre una tabla algo similar a una de surf, provista de una vela.

A diferencia de un velero, la vela o aparejo de una tabla de windsurf está articulado permitiendo su rotación libre alrededor de un solo punto de unión con la tabla: el pie de mástil. Ello permite manipular el aparejo libremente en función de la dirección del viento y de la posición de la tabla con respecto a este último. El aparejo es manipulado por el windsurfista mediante la botavara.

Un equipo de windsurf está compuesto por todo esto:


La Asociación Profesional de Windsurfistas más conocida por su acrónimo (PWA) es el ente rector a nivel internacional en la práctica del windsurf. Está formada por los mejores windsurfistas del mundo y representan el deporte al más alto nivel de competencia, uno de sus principales objetivos es garantizar el buen desarrollo del deporte, organizar y supervisar los eventos profesionales, hacer nuevas reglas para el deporte, ayudar a promover el crecimiento de base, fortalecer los lazos de amistad entre las asociaciones existentes, clases y disciplinas de windsurf y de proporcionar apoyo y servicios para todos los amantes del windsurfing

En windsurf hay seis variantes de competición:

 - Maniobras carving. Primeros trucos del freestyle.



 - Maniobras con los pies en los footstraps. Primeros trucos del freestyle moderno.




 - Maniobras fuera de los footstraps. No se está en contacto con la tabla de forma permanente.

 - Power freestyle. Maniobras aéreas, saltos, trucos en olas.



1. Trasluchada: giro o cambio de sentido a favor del viento, además de la maniobra por excelencia en el windsurf. 

2. Footstrap: Cinchas para los pies.

3. Orzar: dirigir la proa de la tabla hacia barlovento.

4. Chopi: en inglés choppy, que significa mar picado o agitado. Se utiliza cuando aparecen pequeñas olas por todo el mar de manera desordenada.

5. Amura: respecto al desplazamiento de la tabla. Nos pondremos a babor cuando el viento nos venga desde la izquierda y a estribor cuando nos venga desde la derecha.





</doc>
<doc id="9771" url="https://es.wikipedia.org/wiki?curid=9771" title="Oikistés">
Oikistés

Oikistés (οἰκιστής, en plural oikistai, οἰκισται) es la palabra griega que designa al fundador de una nueva colonia ("apoikia"). A veces se les equipara a la condición de "archegétes" (ἀρχηγέτης), que propiamente corresponde al dios Apolo como líder y protector de las colonias ("theos patroos") o a Heracles y a los "heros ktistes", "heros oikistés" o héroes epónimos de las polis griegas originarias (reyes -"basileos"- o héroes -"heros"- de la Época Oscura).

La expansión colonial de la antigua Grecia por el litoral Mediterráneo se dio esencialmente entre 750 a. C. y 500 a. C. Cada polis procuraba convertirse en "metrópolis" (μητρόπολις) estableciendo colonias como una válvula de escape a la presión demográfica y escasez de tierras ("stenochoría", στενοχωρία), que producían conflictos sociales internos ("stásis", στάσις). Tales colonias eran ciudades con un alto grado de independencia, pero fuertemente vinculadas con la ciudad de origen en múltiples aspectos (culturales, religiosos, jurídicos, institucionales y económicos -particularmente el mantenimiento de un activo comercio marítimo-).

Para liderar a los primeros expedicionarios que iban a establecerse en un entorno propicio para la fundación de una colonia, y tras la consulta ritual a un oráculo, se enviaba a un "oikistés", elegido habitualmente de entre los "aristoi" (las familias aristocráticas u oligárquicas). Una vez escogido el emplazamiento, era el "oikistés" el que protagonizaba los ritos fundacionales de la nueva ciudad, destacadamente el depositar el fuego sagrado traído de la "metrópolis" en un templo dedicado a los dioses protectores de ésta, que pasaban a serlo también de la colonia. Tras el acto de la fundación, se suponía que el "oikistés" debía continuar con el mandato que hubiera recibido (es "el interlocutor entre los que se marchan y los que se quedan y entre los dioses y los humanos"), con lo que algunos de ellos permanecían en la colonia, manteniendo su gobierno; mientras que otros volvían a la "metrópolis", abandonando a los colonos a su suerte. El lugar de enterramiento del "oikistes", en el ágora, se convertía en un lugar de culto. Los tiranos (gobernantes que accedían al poder de forma ilegítima) se prestigiaban asociándose a su figura con prácticas rituales semejantes, como "nuevos "oikistai"".

Entre los más destacados "oikistés" estuvieron:
Muchas de las narraciones de la fundación de colonias están tan mitificadas o relacionadas con personajes legendarios que es difícil determinar su historicidad, teniendo ante todo un valor antropológico en relación con su situación intermedia entre el mar y la tierra (arquetipos ctónicos y acuáticos):



</doc>
<doc id="9772" url="https://es.wikipedia.org/wiki?curid=9772" title="Literatura del Perú">
Literatura del Perú

Se entiende por literatura peruana a las manifestaciones literarias producidas por autores de dicha nacionalidad, desde las tradiciones prehispánicas hasta el presente, lo que engloba la literatura cuzqueña, arequipeña, puneña, amazónica y de otras regiones del territorio del Perú, y que ha alcanzado mayor brillo en el siglo XX con nombres indispensables para la literatura universal, como el poeta César Vallejo o el novelista Mario Vargas Llosa. La pertenencia al canon de los cronistas de Indias es comúnmente más aceptada que otras manifestaciones paraliterarias, como la literatura infantil peruana o la literatura peruana de ciencia ficción.

La producción literaria del período prehispánico en el territorio centro-andino (que abarca territorios de las actuales repúblicas de Perú, Ecuador, Bolivia y Chile), está especialmente vinculada al Imperio de los Incas, siendo su principal vehículo de transmisión el idioma quechua o "runa simi", que los incas impusieron como lengua oficial. Los cronistas de la conquista y de la colonia han dado fe de la existencia de una literatura quechua, que se transmitió de manera oral y que se suele dividir en cortesana y popular.



Muchas de estas creaciones han llegado a nuestros días de forma diferida, plasmadas en los trabajos de los primeros cronistas (el Inca Garcilaso de la Vega recupera poesía quechua, mientras que Felipe Guaman Poma de Ayala relata el mito de las cinco edades del mundo). 

La literatura indígena fue desconocida o relegada hasta el siglo XX. Su inclusión en el canon oficial fue lenta. Ya en su tesis "El carácter de la literatura del Perú Independiente" (1905), José de la Riva Agüero y Osma consideró "insuficiente" la tradición quechua como para considerarla un factor predominante en la formación de la nueva tradición literaria nacional. Posteriormente Luis Alberto Sánchez reconoció ciertos elementos de tradición y su influencia en la tradición posterior (en autores como Melgar) para dar base a su idea de literatura "mestiza" o "criolla" (hija de dos fuentes, una indígena y otra española), para lo que consulta fuentes en las crónicas coloniales (Pedro Cieza de León, Juan de Betanzos y Garcilaso).

La apertura real a la tradición prehispánica surge en las primeras décadas del siglo XX gracias al trabajo de estudiosos literarios y antropólogos que recopilaron y rescataron mitos y leyendas orales. Entre ellos se destacan Adolfo Vienrich con "Tarmap pacha huaray" ("Azucenas quechuas", 1905) y "Tarmapap pachahuarainin" ("Fábulas quechuas", 1906); Jorge Basadre en "La literatura inca" (1938) y "En torno a la literatura quechua" (1939); y los estudios antropológicos y folclóricos de José María Arguedas (en particular, su traducción de "Dioses y hombres de Huarochirí"). Los trabajos más contemporáneos incluyen a Martín Lienhard ("La voz y su huella. Escritura y conflicto étnico-cultural en América Latina. 1492-1988", 1992), Antonio Cornejo Polar ("Escribir en el aire. Escribir en el aire: ensayo sobre la heterogeneidad socio-cultural en las literaturas andinas." 1994), Edmundo Bendezú ("Literatura Quechua", 1980 y "La otra literatura", 1986) y Gerard Taylor ("Ritos y tradiciones de Huarochirí. Manuscrito quechua del siglo XVII", 1987; "Relatos quechuas de la Jalca", 2003).

Bendezú afirma que la literatura quechua se constituye, desde la conquista, en un sistema marginal opuesto al dominante (de vena hispánica) y postula la existencia permanente y cubierta de una tradición de cuatro siglos. Habla de una gran tradición ("enorme masa textual") marginada y dejada de lado por el sistema escritural occidental, ya que esta "otra" literatura es, como el quechua, plenamente oral.

El término "literatura colonial" (o "literatura de la Colonia") hace referencia al estado del territorio del Perú del siglo XVI al siglo XIX, dependiente de la corona española y políticamente organizado como un Virreinato.

Con la conquista española llegó al Perú el idioma castellano (mal llamado "español") y las tendencias literarias europeas. Se inicia un proceso que con el tiempo dará origen a una literatura mestiza o peruana, aunque inicialmente acuse de una preeminencia hispánica.

Francisco Carrillo Espejo ha acuñado el término de "literatura del descubrimiento y conquista", con el que se designa al período que abarca todas las obras escritas durante el proceso de descubrimiento y conquista del Perú, que se inicia en 1532 en Cajamarca con la captura del último Inca, Atahualpa, y finaliza con la desarticulación del Imperio Incaico. La literatura de este período, aunque no necesariamente escrita durante este marco temporal, sí se vincula a los eventos desarrollados antes o durante este.

Las primeras manifestaciones literarias fueron las coplas recitadas por los conquistadores; un ejemplo es la célebre copla escrita por un soldado durante el segundo viaje de Pizarro, quejándose ante el gobernador de Panamá de las penalidades que padecían:
Luego aparecieron las crónicas, cartas de descubrimiento y relaciones. Particularmente, las crónicas constituyen un interesante género literario que mezcla la historia, el ensayo literario y la novela. Las primeras crónicas, escritas por los soldados y secretarios de las expediciones militares, tienen un estilo rudo y seco. Luego aparecieron otras obras mejor trabajadas, como la de Pedro Cieza de León (1518-1554), autor de la "Crónica del Perú", dividida en cuatro partes: "Parte primera de la Crónica del Perú", "El Señorío de los Incas", "Descubrimiento y Conquista del Perú" y las "Guerras Civiles del Perú", que constituyen el primer gran proyecto de una historia andina global. Debido a ello, algunos consideran a Cieza como el primer historiador del Perú. Finalmente, el Inca Garcilaso de la Vega, mestizo, hijo de un conquistador español y una noble inca, publicó a principios del siglo XVII sus "Comentarios reales de los incas", obra que supera las exigencias de una simple crónica para convertirse en una obra maestra de la literatura, la primera escrita por un mestizo hispanoamericano.

El crítico Augusto Tamayo Vargas ha dividido a los cronistas en españoles, indígenas, mestizos y criollos.

Estos se dividen en dos grupos: cronistas de la conquista y cronistas de la colonización. Este último se subdivide a su vez en pre-toledanos, toledanos y post-toledanos.





Tres nombres se mencionan especialmente entre los cronistas indígenas, nativos o indios:


Pero indudablemente el más importante cronista mestizo es el Inca Garcilaso de la Vega (1539-1616), considerado como el ""primer mestizo biológico y espiritual de América"", o en otras palabras, el primer mestizo racial y cultural de América, pues supo asumir y conciliar sus dos herencias culturales: la indígena americana (inca o quechua) y la europea (española), alcanzando al mismo tiempo gran renombre intelectual. Se le conoce también como el ""príncipe de los escritores del Nuevo Mundo"", pues su obra literaria se destaca por un gran dominio y manejo del idioma castellano. En su obra cumbre, los "Comentarios reales de los incas", publicada en Lisboa, en 1609, Garcilaso expuso la historia, cultura y costumbres de los Incas y otros pueblos del antiguo Perú. Para muchos críticos se trata del cantar de gesta de la nacionalidad peruana, que se forja precisamente con la fusión de dos herencias, la nativa y la española. Garcilaso es autor también de "La Florida del Inca" (Lisboa, 1605), que es un relato de la conquista española de Florida; y de la "Segunda parte de los Comentarios reales", más conocida como "Historia General del Perú" (Córdoba, 1617), publicada póstumamente, donde el autor trata sobre la conquista y el inicio de la colonia. Con justicia se considera al Inca Garcilaso como el primer literato del Perú.

Entre los cronistas criollos o americanos (nacidos en América de padres españoles) que escribieron sobre el Perú destacan:

Se debe mencionar también al padre jesuita italiano Giovanni Anello Oliva (¿1572?-1642), que vivió más de 40 años en el Perú, y fue autor de una "Historia del reino y provincias del Perú y vidas de varones ilustres en la Compañía de Jesús de la provincia del Perú", cuya primera parte es una introducción histórica titulada: "Historia del reino y provincias del Perú, de sus incas, reyes, descubrimiento y conquista por los españoles de la corona de Castilla".

Hitos culturales importantes fueron la fundación de la Real y Pontificia Universidad de San Marcos de Lima el 12 de mayo de 1551 por Real Provisión de Carlos I de España y V de Alemania, la primera en América, y la instalación en Lima de la primera imprenta de Sudamérica, la del turinés Antonio Ricardo en 1583, instituciones que impulsaron el temprano desarrollo intelectual de los peruanos.

El primer libro publicado en la ciudad de Lima es la "Doctrina Christiana y Cathecismo para la Instrucción de los Indios" (1584) del impresor Antonio Ricardo, con lo que se inaugura propiamente la idea de literatura peruana. Este primer catecismo es publicado en castellano, quechua y aimara. Durante las décadas anteriores, ya se había establecido el sistema de reducciones producto de las reformas del virrey Francisco de Toledo (1569-1581) que separaron la sociedad colonial en dos repúblicas, república de indios y república de españoles (es el período en el que se realizaron la mayor cantidad de "extirpación de idolatrías"). También se promulgaron las Leyes de Indias que establecían lo siguiente:

Estos dos factores determinan que la inicial producción literaria en la Colonia se limite a círculos de influencia principalmente hispánica, producida en las grandes ciudades por hijos de españoles (españoles americanos). La literatura se cultiva en círculos ilustrados, estrechamente vinculados con la Iglesia (que imparte la educación entre las élites sociales, ya que todos los colegios y convictorios estaban dirigidos por órdenes religiosas). De la Iglesia es precisamente el padre José de Acosta, quien presta mayor atención al mundo americano ya que, junto a sus reflexiones religiosas y teológicas, encontramos una clara preocupación por la geografía y fisiología de los pueblos naturales del Perú. Acosta representa un momento en el que los estándares estéticos renacentistas están aún presentes en la escena literaria. En 1586 publica "Peregrinación de Bartolomé Lorenzo", en 1588 "De Natura Novi Orbis et De Promulgation Evangelii apud barbaros, sive de Procuranda indorum salute" ("De la naturaleza del nuevo mundo...") y en 1590 su obra más conocida: "Historia natural y moral de las Indias".

La literatura del llamado Siglo de oro español, se refleja también en la América española, especialmente en el campo de la poesía lírica y épica. Se trata de una literatura erudita, de refinadas formas, ceñida a los moldes clásicos (clasicismo). Los autores más relevantes que se desenvolvieron en el Perú bajo esta tendencia, son los siguientes:

Siguiendo la tendencia dictada desde Europa, la literatura peruana adopta el estilo del barroco (conceptismo y culteranismo). Se tiende a recargar el lenguaje literario con muchos recursos estilísticos y se hace gala de erudición. La figura cumbre del barroquismo peruano fue El Lunarejo.

Podemos mencionar también a Lorenzo de las Llamosas (c.1665-c.1705), quien después de unos pocos años de permanencia en el Virreinato del Perú, viaja a España donde desarrolla actividades en la Corte del Rey, como militar y al mismo tiempo como autor de obras de teatro y didácticas.

En la segunda mitad del siglo XVII, la literatura en Europa, bajo influjo de las letras francesas, tendió a volver a los moldes clásicos, aunque en las colonias españolas siguió preponderando el barroquismo. No obstante, a comienzos del siglo XVIII, coincidiendo con la instauración de la dinastía borbónica en España, los escritores de habla hispana tienden a “afrancesarse”. Surgen las Academias literarias, a imitación de las de Francia, como la llamada Academia de Palacio fundada por el virrey del Perú Marqués de Castell dos Rius (1707-1710). Entre los académicos de Palacio destacan los siguientes:
El Neoclasicismo irrumpe en la segunda mitad del siglo XVIII y fue desplazando progresivamente al barroquismo. Se trata de una vuelta a las normas del clasicismo, en oposición al estilo recargado del barroquismo, así como una tendencia a la actitud pedagógica. Este movimiento se desarrolló juntamente con la expansión de las ideas liberales surgidas en Francia, que tanto habrían de influir en el desarrollo de la revolución separatista de Hispanoamérica. 

La figura más conspicua del afrancesamiento literario en la segunda mitad del siglo XVIII fue Pablo de Olavide (1725-1803), escritor, traductor, jurista y político, nacido en Lima, pero que desenvolvió su carrera en España. Su casa en Madrid se convirtió en un destacado centro de tertulia cultural. Influido por la ilustración francesa, profesó inicialmente las ideas liberales. Acusado de herejía, fue encarcelado por la Inquisición. Reconciliado con la religión, publicó "El Evangelio en triunfo" (1797); "Poemas cristianos"; y "Salterio español" (1799). Ya en el siglo XX fueron exhumadas las obras de su periodo afrancesado, de género dramático y narrativo, siendo este último el que ha concitado el interés de la crítica moderna, pues se tratan de novelas cortas, que harían a Olavide precursor de dicho género literario.

Mientras que en el Perú se desenvuelven por esa época poetas y escritores satíricos criollos, cercanos al costumbrismo:

A fines del siglo XVIII y coincidiendo con el fin del mandato del virrey Manuel Amat y Juniet, se representó en las gradas de la catedral de Lima un drama, el "Drama de los palanganas: veterano y bisoño", que es una crítica despiadada contra el gobierno y la persona de este virrey, en particular sus amoríos con La Perricholi. El texto ha sido rescatado por el crítico literario Luis Alberto Sánchez.

El último periodo de la literatura colonial abarca desde fines del siglo XVIII hasta comienzos del siglo XIX, en esta época surgió la idea de la libertad y los hechos que marcaron una influencia son: La Revolución Francesa que ocurrió en 1789 además de la Independencia de EE.UU en 1776. Se desarrolló en un contexto de la Revolución de Túpac Amaru II en 1780 y concluirá este movimiento con el levantamiento del pueblo peruano debido a la dictadura de Simón Bolívar; y la Proclamación de la Independencia el 28 de julio de 1821.

Sobresalen, al estilo de los enciclopedistas franceses, los redactores del "Mercurio Peruano", la primera gran revista americana, quienes se agrupan en la llamada Sociedad de Amantes del País. Entre ellos destacan Hipólito Unanue, Toribio Rodríguez de Mendoza, José Baquíjano y Carrillo, entre otros.

Los temas que se utilizaron en esta literatura fueron: la libertad, objetivo de todos los indígenas; la patria, anti española y separatista; y el sentimiento indígena.

En el campo de la lírica destaca el arequipeño Mariano Melgar (1791-1815), en cuyos versos se prefigura el romanticismo y muestra un mestizaje entre la poesía culta y las canciones populares indígenas. Aunque su obra se enmarca más dentro de la época republicana, y consta de "Carta a Silvia" (1827) y "Poesías" (1878). Se sumó a la revolución independentista en 1814 y murió fusilado. Este poeta recibió el apodo de "El precursor del Romanticismo literario en América" y "Representante del primer momento auténtico de la literatura peruana".

Otro representante de la poesía de la Emancipación es José Joaquín Olmedo (1780-1847), nacido en Guayaquil cuando este pertenecía al Perú. Fue diputado ante el primer Congreso de la República del Perú y ministro plenipotenciario del Perú en Inglaterra. Su poema fundamental es "Oda a la victoria de Junín", versos épicos de corte neoclásico que cantan el triunfo obtenido por Bolívar en la batalla de Junín. 

En el campo de la literatura política descuella el tribuno José Faustino Sánchez Carrión (1787-1825), defensor del sistema de gobierno republicano y autor de la "Carta del Solitario de Sayán".

Es necesario también mencionar al clérigo limeño José Joaquín de Larriva (1780-1832) poeta, escritor y periodista, apodado el “cojo Larriva”. Escritor satírico y muy mordaz, según Porras Barrenechea fue el “primer poeta cómico” del Perú. Actualmente se le recuerda más por las letrillas que escribiera contra el Libertador Bolívar, aunque en su tiempo fue muy popular y celebrado por sus oraciones fúnebres y laudatorias, y sus artículos periodísticos, además de sus improvisaciones poéticas. Es considerado precursor del costumbrismo literario peruano.

Las primeras corrientes literarias del Perú independiente fueron el costumbrismo y el romanticismo. Ya en el último tramo del siglo, se desarrolló el realismo.

El costumbrismo fue una corriente literaria cuyos cultivadores prestaban más atención a las costumbres de los pueblos, tanto para festejarlas, como para criticarlas o ridiculizarlas, a través de géneros diversos (comedias, letrillas, sainetes, etc.). En el Perú comienza hacia 1830, coincidiendo con el periodo fundacional de la República y se prolonga hasta los años 1850.

Al período costumbrista peruano pertenecen dos poetas satíricos y dramaturgos cómicos, ambos limeños, pero de espíritu contrapuesto:

De esta época es importante destacar también a los siguientes autores:

Cercana al costumbrismo está la obra de Ricardo Palma (1833-1919), escritor limeño, autor de las célebres "Tradiciones peruanas", la obra más conocida del siglo, en la que a través de una serie de tradiciones —género inventado por él, que combina elementos de historia con fabulaciones propias—, narra la historia de Lima y del Perú durante las épocas incaica, colonial y republicana. Escritas entre 1860 y 1914, una edición definitiva fue compilada por Angélica Palma, la hija del tradicionista, en seis volúmenes (1923-1925).

El romanticismo, proveniente de Europa, llegó al Perú con retraso, hacia los años 1840, y se prolongó por el resto del siglo, aunque decayó tras la Guerra del Pacífico, para dar pase al Realismo. Los textos de los románticos peruanos fueron, por lo general, artificiales y abusaron del sentimentalismo. Las obras de teatro frecuentemente cultivaron el mismo sentimiento y exageraron los enredos de modo inverosímil; si bien algunas tuvieron éxito en su momento, hoy están olvidadas. Dos representantes del romanticismo peruano, sin embargo, han sobrevivido literariamente, por la calidad de sus obras: Ricardo Palma y Carlos Augusto Salaverry, pertenecientes a la llamada "generación de la bohemia".



Al romanticismo pertenecen también los siguientes poetas, escritores y dramaturgos: 

Tras la guerra del Pacífico (1879-1883) hay una reacción contra el romanticismo, liderada por el intelectual Manuel González Prada (1844-1918), quien cultivó una poesía que por su temática estetizante y la introducción de nuevas formas métricas fue un claro precursor del modernismo. De entre sus obras en prosa se deben mencionar: "Pájinas libres" y "Horas de lucha", libros en las que hace una furibunda crítica a la clase política, responsable, según él, de la catástrofe bélica. No se salvan tampoco de sus dardos las instituciones religiosas y los literatos de su tiempo. Su postura hipercrítica en el terreno de las ideas y de la literatura le granjeó no pocos enemigos y le metió en variopintas polémicas periodísticas.

Se desarrolló también, de un modo bastante tenue, el realismo en la novela, que toma vuelo a partir de entonces en el Perú.

Una característica resaltante en este período es el surgimiento de un grupo de escritoras. Muchas de ellas —habiendo perdido a sus cónyuges e hijos mayores en la guerra con Chile— tuvieron que ganarse la vida por sí mismas, y cultivaron su vocación literaria a través de tertulias. La principal fue la de la argentina Juana Manuela Gorriti, en las que se discutía sobre los problemas sociales y sobre la influencia de las formas europeas. Escribieron novelas que en cierto modo pueden calificarse como realistas. Tal es el caso de:

El modernismo se desarrolló en el Perú a partir del poema «Al amor» de Manuel González Prada, publicado en el diario "El Comercio" en 1867, donde el autor fusiona un conjunto de géneros poéticos provenientes de Europa, dando como resultado el "triolet". Esta tendencia, resultado del cosmopolitismo que vivía el Perú, pronto se desarrolló en otras partes de América Latina: en Cuba con José Martí; en Nicaragua con Rubén Darío; en Argentina con Leopoldo Lugones; en Uruguay con Julio Herrera y Reissig; en México con Manuel Gutiérrez Nájera. 

A pesar de sus tempranos antecedentes con González Prada, el modernismo alcanzó en el Perú un pleno desarrollo tardíamente, a inicios del siglo XX. De entre todos sus representantes descuella el poeta limeño José Santos Chocano (1875-1934), conocido como «El Cantor de América», considerado uno de los poetas hispanoamericanos más importantes, por su poesía épica de tono grandilocuente, que gusta de la retórica y de la descripción de paisajes, con gran sonoridad y colorido, estando más próxima a Walt Whitman y al romanticismo. También produjo poesía lírica de singular intimismo. Todas sus creaciones poéticas están trabajadas con depurado formalismo y se inspira mayormente en los temas, los paisajes y la gente de su país y de América en general. Principales obras: "Iras santas" (1895), "En la aldea" (1895), "Selva virgen" (1896?), "La epopeya del morro" (1899), "El canto del siglo" (1901), "Alma América" (1906), "Fiat Lux" (1908), "Primicias de oro de Indias" (1934), "Oro de Indias" (1940-1941). Su vida fue muy novelesca y aventurera, ligada a la de los dictadores y caudillos latinoamericanos de su tiempo. Durante el Oncenio de Leguía sostuvo una polémica pública con el joven escritor Edwin Elmore, a quien en un arranque de ira asesinó disparándole a quemarropa. Tras sufrir un breve encierro, partió hacia Chile, donde murió asesinado a manos de un esquizofrénico.

Dentro del modernismo peruano también debemos destacar a los siguientes poetas:

Una importante rama del modernismo peruano fue la llamada Generación del 900, conocida también como la generación “arielista” (llamada así por inspirarse en las ideas del escritor uruguayo Enrique Rodó, el autor de "Ariel", que abogaba por la europeización de Hispanoamérica y la formación de elites intelectuales que se encargaran de su dirección). Sus miembros manejaban una prosa elegante y ahondaban particularmente en las raíces de la historia nacional, con tendencias hacia el idealismo (Tamayo Vargas). Fueron sus principales representantes:

En ese ambiente impregnado de modernismo surgió una figura insular: José María Eguren (1872-1942), poeta limeño que abrió el camino de la innovación en la poesía peruana con sus libros "La canción de las figuras" (1916) y "Simbólicas" (1911), próximos al simbolismo y que reflejaban su mundo interior mediante imágenes oníricas, con las que reacciona contra la retórica y el formalismo modernistas.

Hasta 1920 el modernismo era la tendencia dominante en el cuento y la poesía, pero desde 1915 la vanguardia literaria hizo tímidamente su entrada en la musa nacional. César Vallejo, con sus obras fuertemente innovadoras en el lenguaje centradas en la angustia y en la condición humana, pertenece a este período, en el que también aparecieron los poetas Alberto Hidalgo, Alberto Guillén, Xavier Abril, Carlos Oquendo de Amat, Luis Valle Goicochea, Martín Adán, Magda Portal y los surrealistas César Moro y Emilio Adolfo Westphalen.

El escritor más importante del momento es Abraham Valdelomar, quien en su breve vida cultivó el cuento, la novela, el teatro, la poesía, el periodismo y el ensayo. Sobresalen sobre todo sus relatos, que narran con bastante ternura historias de las ciudades provincianas y, en menor medida, de Lima o cosmopolitas. En 1916 fundó la revista "Colónida" que agrupó a varios jóvenes escritores y que, a pesar de su breve existencia (se publicaron solo cuatro números), abrió el camino para la entrada de nuevos movimientos como la vanguardia en la literatura peruana.

Otros autores, que junto con Valdelomar inauguran el cuento en el Perú fueron Clemente Palma, que escribió relatos decadentes, psicológicos y de terror, influido por el realismo ruso y por Edgar Allan Poe; y Ventura García Calderón, quien mayormente escribió cuentos exóticos sobre el Perú. También se encuentran Manuel Beingolea, Manuel Moncloa y Covarrubias, "Cloamón", y Fausto Gastañeta.

En el teatro, con escasas obras de valor en este período, figuran las comedias del poeta festivo Leonidas Yerovi y, posteriormente, las obras de denuncia social y cariz político de César Vallejo, que pasaron mucho tiempo antes de ser publicadas o representadas. Ya en los años 1940 la influencia tardía del modernismo y del teatro poético se reflejará en las obras de Juan Ríos, a las que se les ha criticado su excesiva retórica poética, generalmente ambientadas en tiempos remotos o en leyendas y que buscan ser un referente general del hombre.

En el Perú el tema principal de la literatura indigenista era el indio, cuyo predominio en la literatura se había iniciado en los años 1920 y 1930, primero con los cuentos de Enrique López Albújar y más tarde con las novelas de Ciro Alegría: "La serpiente de oro" (1935), "Los perros hambrientos" (1939) y "El mundo es ancho y ajeno" (1941). Así empezó la interesante controversia sobre indigenismo e indianismo, vale decir, sobre la cuestión de que no sean los mismos indios quienes escriban sobre su problemática. Esta corriente literaria alcanzó su máxima expresión en la obra de José María Arguedas, autor de "Agua", "Yawar Fiesta", "Diamantes y pedernales", "Los ríos profundos", "El Sexto", "La agonía de Rasu Ñiti", "Todas las sangres" y "El zorro de arriba y el zorro de abajo", y quien debido a su contacto con los indígenas en la infancia, pudo asimilar como propias su concepción del mundo y experiencias.

La modernización de la narrativa peruana comienza con la Generación del 50, enmarcada políticamente con el golpe del general Manuel A. Odría en 1948 y las elecciones de 1950 en las que se autoelige presidente. Durante la década anterior había comenzado un movimiento migratorio del campo a la ciudad (preferentemente a la capital), que durante los años cincuenta se potencializa al máximo y resulta en la formación de barriadas y pueblos jóvenes, la aparición de sujetos marginales y desplazados socialmente. La literatura producida en este período estuvo influida notablemente por las vanguardias europeas; en particular, el llamado modernismo anglosajón de Joyce y en el ambiente norteamericano la obra novelística de Faulkner y la Generación Perdida. También influyó notablemente la literatura fantástica de Borges y Kafka. A esta generación pertenecen Julio Ramón Ribeyro, Carlos Eduardo Zavaleta, Eleodoro Vargas Vicuña, Mario Vargas Llosa, entre otros.

La Generación del 50 es un momento en el que la narrativa se vincula de forma muy fuerte con el tema del desarrollo urbano, la experiencia de la migración andina hacia Lima (un incremento drástico de la población a partir de finales de la década del 40). Muy relacionada con el cine neorrealista italiano, retrata la urbe cambiante, la aparición de personajes marginales y problemáticos. Entre los narradores más representativos resaltan Ribeyro con "Los gallinazos sin plumas" (1955); Enrique Congrains con las novelas "Lima, hora cero" (1954) y "No una, sino muchas muertes" (1957); Luis Loayza, cuya obra es obra es breve y poco conocida; y Vargas Llosa, quien a fines de la década del 50 empezó a publicar sus cuentos, aunque sus magistrales novelas aparecerán a partir de la década de 1960.

Junto a los narradores, surge un grupo de poetas entre los que se destacan Alejandro Romualdo, Washington Delgado, Carlos Germán Belli, Francisco Bendezú, Juan Gonzalo Rose, Pablo Guevara. Estos poetas comenzaron a publicar su obra a partir de fines del 40, tal es el caso de Romualdo, luego lo harían Rose, Delgado, Bendezú, Belli. Guevara. Además, a este grupo lo unían no solo las relaciones personales, sino también la ideología, el marxismo y el existencialismo. Los poemas que escribieron adoptaron, desde una visión general, un tono protestatario y de compromiso social. Por ello, se reconoce al poema "A otra cosa" de Romualdo en el arte poética de la generación del cincuenta.

Esta generación reivindicó a César Vallejo como paradigma estético y asumió el pensamiento de José Carlos Mariátegui en calidad de guía intelectual. Los poetas Javier Sologuren, Sebastián Salazar Bondy, Jorge Eduardo Eielson, Antenor Samaniego, Blanca Varela, fueron conocidos como el grupo "neovanguardista", que comenzó a publicar a fines de los años treinta (tal es el caso de Sologuren, luego vendrían los poemas de Salazar Bondy, Samaniego, Eielson, Varela). Mantuvieron relaciones personales en la revista "Mar del Sur", dirigida por Aurelio Miró Quesada, de clara tendencia conservadora; y designaron a Emilio Adolfo Westphalen como guía poético. A esta situación histórico - literaria, habría que añadir el grupo de los llamados "Poetas del pueblo", vinculados al partido aprista fundado por Victor Raúl Haya de la Torre, integrado por Gustavo Valcárcel, Manuel Scorza, Mario Florián, Luis Carnero Checa, Guillermo Carnero Hoke, Ignacio Campos, Ricardo Tello, Julio Garrido Malaver, quienes reivindicaron como paradigma poético a Vallejo. 

Durante ese decenio y el siguiente el teatro experimenta un período de renovación, inicialmente con las piezas de Salazar Bondy (generalmente comedias de contenido social) y más tarde con Juan Rivera Saavedra, con obras de fuerte denuncia social, influidas por el expresionismo y el teatro del absurdo. Durante estos años se dejará sentir con fuerza la influencia de Bertolt Brecht entre los dramaturgos.

La Generación del 60 en poesía tuvo a representantes del calibre de Luis Hernández, Javier Heraud y Antonio Cisneros, Premio Casa de las Américas. Merecen citarse también César Calvo, Rodolfo Hinostroza y Marco Martos. Cabe señalar que Heraud fue el verdadero paradigma generacional, vinculado a la doctrina marxista y a la militancia política, mientras que Hernández y Cisneros, no. Como es fácil advertir, los coetáneos no constituyen movimiento generacional. 

A esta generación pertenecen los narradores Oswaldo Reynoso, Miguel Gutiérrez, Eduardo González Viaña, Jorge Díaz Herrera, Alfredo Bryce Echenique y Edgardo Rivera Martínez. 
La narrativa y la poesía peruanas de fines de la década de 1960 no tuvieron tanto un carácter generacional como ideológico: la literatura era vista como un medio, un instrumento para crear una conciencia de clase. Eran los años del auge de la revolución en Cuba y en el Perú la mayoría de intelectuales ansiaban una revolución marxista que rompiera el viejo orden oligárquico y feudal. Algunos escritores aspiraban a un proceso como el cubano (Heraud, por ejemplo, murió en mayo de 1963 en la selva peruana, integrando una columna que pensaba lanzar la lucha guerrillera), mientras que otros tenían sus propios modelos. En este periodo de intenso compromiso social al escritor le queda poco espacio para el compromiso con su propia obra. A fines de esta década surge el "Grupo Narración", influido por el maoísmo y liderado por Miguel Gutiérrez y Oswaldo Reynoso, sumándose también Antonio Galvéz Ronceros y Augusto Higa, quienes editaron una revista con el mismo nombre, aunque tenían pensando llamarla "Agua", evocando a José María Arguedas y las tensiones sociales que muestra el libro de ese título.

Las primeras expresiones con características propias, de lo que se denominaría después Generación del 70, surgieron a fines de los años 60 con autores como Manuel Morales (1943-2007), autor de la plaqueta "Peicen Bool" (1968) y "Poemas de entrecasa" (1969); y Abelardo Sánchez León ("Poemas y ventanas cerradas", 1969) que experimentaron con el coloquialismo popular. 

Una de las primeras revistas que acogerá a las nuevas voces será "Estación Reunida", en la que publican José Rosas Ribeyro, Patrick Rosas, Elqui Burgos, Tulio Mora, Óscar Málaga y otros. En 1963 irrumpió al escenario poético el movimiento de ruptura Gleba Literaria en los claustros de letras de la Universidad Federico Villarreal, siendo una voz contestataria del momento político que vivía el país, teniendo como fundador a Jorge O. Vega (1940-2017), albergando a otros poetas insurgentes como Manuel Morales, Carlos Bravo E, entre otros. Con la aparición del movimiento Hora Zero y su revista homónima, en 1970, que esta generación sentará presencia en la escena cultural peruana. Lo fundaron Juan Ramírez Ruiz y Jorge Pimentel, estudiantes de la Universidad Nacional Federico Villarreal, y a sus filas también pertenecieron Enrique Verástegui, Carmen Ollé, Jorge Nájar, Mario Luna y Feliciano Mejía. Este último se alejaría definitivamente de Hora Zero en 1972.

Los primeros escritores galardonados con el importante premio Poeta Joven del Perú fueron José Watanabe (1945-2007), ("Álbum de familia") y Antonio Cillóniz ("Después de caminar cierto tiempo hacia el Este"), que lo compartieron en 1970. 

Además del coloquialismo popular como expresión poética, a la Generación del 70 también le caracterizará por su ruptura con la tradición literaria peruana anterior a ella y su radicalismo ideológico de izquierda, como prueba de lo citado, se halla la ratificación por mayoría generacional a tal compromiso literario, en el Congreso de Poetas celebrado en la ciudad de Jauja en abril de 1970.
Otra expresión importante de esta generación es el surgimiento de los "poetas mágicos", neovanguardistas que retoman los experimentos dadaístas con César Toro Montalvo, Omar Aramayo, José Luis Ayala. La poesía de protesta social tendrá un destacado cultor en Cesareo Martínez. Fuera de los grupos destacan otras voces como la de Vladimir Herrera.
A partir de 1974 se produce un segundo momento en la Generación del 70 que se expresará en las páginas de revistas de muy limitada circulación como "La Tortuga Ecuestre", "Cronopios", "Literatura", "Auki", "Tallo de Habas" y algunas otras. Sus poetas, en alguna forma, tratan de tomar cierta distancia del coloquialismo característico de la primera etapa y se entregan más al cuidadoso cultivo de la forma. En este segundo momento aparecen, entre otras, las voces de Mario Montalbetti, Juan Carlos Lázaro, Carlos López Degregori, Luis La Hoz, Enrique Sánchez Hernani, Bernardo Rafael Álvarez, Armando Arteaga, Alfonso Cisneros Cox, Jorge Luis Roncal, Gustavo Armijos,Jorge Espinoza Sánchez.

De otro lado, con la publicación póstuma de un puñado de poemas de María Emilia Cornejo en la revista "Eros", la poesía escrita por mujeres en el Perú inaugura un nuevo lenguaje, una nueva expresión de la problemática femenina. Destacarán la ya citada Carmen Ollé, Sonia Luz Carrillo, Rosina Valcárcel, Rosa Natalia Carbonell, entre otras.

Si bien la del 70 fue una generación fundamentalmente poética, no estuvo exenta de narradores. En los años iniciales de agitación literaria, al influjo de las modas importandas de la contracultura y los hippies, su narrador más visible fue Fernando Ampuero, quien con el tiempo desarrollará una importante y sostenida obra cuentística, novelística y periodística. Con menos atención de los medios, pero con obras no menos importantes, a esta generación también pertenecen los narradores Óscar Colchado, Cronwell Jara, Maynor Freyre, Zein Zorrilla, Luis Nieto Degregori, Enrique Rosas Paravicino.

En el teatro hace irrupción la creación colectiva frente a las obras de autor. El movimiento fue liderado por varios grupos teatrales surgidos en estos años, entre los que descuellan Cuatrotablas, encabezado por Mario Delgado, y Yuyachkani, por Miguel Rubio Zapata, ambos creados en 1971.

Merece destacarse la labor poética y la perseverancia, desde las provincias, de Alberto Alarcón, Houdini Guerrero, Emilio Saldarriaga, Segundo Cansino, Carmen Arrese, entre otros. En Arequipa, las revistas "Ómnibus" y "Macho Cabrío" marcaron una época. El grupo de poetas vinculado a la Universidad San Agustín (Oswaldo Chanove, Alonso Ruiz Rosas, entre otros) fue muy activo.

Con la década de 1980 viene el desencanto, el pesimismo: la llegada de una revolución comunista deja de ser una utopía, pero ya no se la espera con ilusión, es casi una amenaza. Es tiempo de la "perestroika" y los últimos años de la guerra fría. Además, la crisis económica, la violencia terrorista y el deterioro de las condiciones de vida en una Lima caótica y superpoblada contribuyeron al desánimo colectivo. En narrativa aparecen los primeros libros de cuentos de Alfredo Pita, "Y de pronto anochece"; de Guillermo Niño de Guzmán, "Caballos de medianoche"; y de Alonso Cueto, "Las batallas del pasado," autores estos cuya obra literaria se desarrollará plenamente en años posteriores. 

En poesía, surgen movimientos marginales, que ahondan la vertiente rebelde de la década anterior, como el "Kloaka", liderado por Roger Santiváñez. Fundado hacia el final de 1982, editó una "autoantología "con motivo de su disolución: "La última cena "(1987). En contraste con las propuestas colectivas de aliento neovanguardistas (en general, de ruptura con el sistema político y el estético), surgen individualidades notables vinculadas en su orígenes con estos, pero que rápidamente transitan a una poesía serena, de ritmos equilibrados y que se nutre de tradiciones artísticas fuertemente codificadas. El caso más notable es el de José Watanabe, cuya mejor obra corresponde a este decenio y que será revalorada en el nuevo siglo. Otros poetas notables dentro de esta apuesta individualizadora de vertiente tradicional fueron Eduardo Chirinos y Magdalena Chocano. En el mismo decenio afloran también los primeros y diversificados movimientos de poesía de mujeres. Están la línea feminista, dentro de la cual se destacan Carmen Ollé, Giovanna Pollarollo y Rocío Silva Santisteban, y otra más lírica, donde sobresale Rossella Di Paolo, además del intimismo irónico de Milka Rabasa. Cabe mencionar también a Patricia Alba, Mary Soto, Mariela Dreyfus y Dalmacia Ruiz-Rosas. 

En la década de 1990, aparece una tendencia individualista que ahonda en la intención estética. En poesía donde surgen varios grupos o colectivos poéticos. En la narrativa, la fórmula que se impone es la denominada "joven-urbano-marginal". En este campo, además de Jaime Bayly, que tiene preferencia por lo sensacionalista, sobresalen Óscar Malca con "Al final de la calle" (1993), Sergio Galarza con "Matacabros" (1996), Rilo con "Contraeltráfico" (1997), autores que cultivan el realismo sucio. 

Por otra parte, aparecen algunos escritores que cultivan el esteticismo y cuya obra escapa a los moldes de su generación, entre ellos Iván Thays, con "Las fotografías de Francés Farmer", y Patricia De Souza, con "Cuando llegue la noche". En poesía destacan Montserrat Álvarez con "Zona dark" (1991), Xavier Echarri con "Las quebradas experiencias" (1993), Domingo de Ramos con "Ósmosis" (1996), Doris Moromisato, Odi González, Ana Varela, Rodrigo Quijano, Jorge Frisancho, Ericka Ghersi con "Zenobia y el Anciano" (1994), Rafael Espinosa, entre otros antologados en la polémica antología "Poesía peruana siglo XX" (2000) de Ricardo González Vigil (Universidad Católica). 

Hacia el 2000, como señala Vigil en el tomo 14, "Literatura", de la a "Enciclopedia Temática del Perú" de "El Comercio", muestran un trabajo poético importante Lorenzo Helguero, Miguel Ildefonso, Selenco Vega, José Carlos Yrigoyen, Alberto Valdivia Baselli, Rubén Quiroz, entre otros. En el campo dramático descuellan Enrique Mávila y Mariana de Althaus, que se han caracterizado por la asimilación de diferentes tendencias teatrales contemporáneas. Y en el campo de la narrativa breve es singular la obra "Fábulas y antifábulas", de César Silva Santisteban.

Simultáneamente, dos escritores del grupo "Narración" alcanzan su madurez durante este decenio: Oswaldo Reynoso y Miguel Gutiérrez, quienes regresan al Perú luego de una larga estadía en la China comunista, que los desengaña de sus aventuras políticas juveniles. Reynoso, autor del memorable libro de cuentos "Los inocentes", pública sucesivamente la nouvelle "En busca de Aladino" y la novela "Los eunucos inmortales", obras de prosa musical en las que se descarta el ideal de la lucha social de clase por la búsqueda de una utopía de belleza juvenil que resulte, no obstante, justiciera con los humildes. Gutiérrez, por su lado, sorprende a los lectores con una novela de más de mil páginas, "La violencia del tiempo", saga familiar de la familia Villar, que se inicia con el primer Villar, desertor del ejército español que combatió contra los patriotas en la guerra de independencia, y termina con Martín Villar, narrador de la novela, que en los años sesenta ha optado por ser un profesor rural, tras estudiar en la oligárquica Universidad Católica. Novela histórica, de crecimiento, ensayo de crítica social y de interpretación histórica, "La violencia del tiempo" acusa el influjo de los grandes narradores latinoamericanos del siglo XX (Jorge Luis Borges, Juan Rulfo, Gabriel García Márquez y Mario Vargas Llosa), así como de los maestros de la novela del siglo XIX, en especial de Balzac, cuyo intenso y torvo cronicón de familia, "La comedia humana", evoca con maestría singular.

Con el cambio de siglo y en los primeros años de la década varios de los premios internacionales más importantes son entregados a escritores peruanos, algunos de ellos desconocidos hasta ese momento en el extranjero. A partir de ello, se plantea la posibilidad de un relanzamiento internacional de nuestras letras, las que habían menguado en presencia exterior durante las dos últimas décadas del siglo XX. De hecho, este repunte de las letras peruanas empieza en 1999, cuando la novela "El cazador ausente", de Alfredo Pita, gana el premio "Las dos orillas", concedido por el Salón del Libro Iberoamericano de Gijón (España). El libro de Pita fue de inmediato traducido y publicado en cinco países europeos. En 2001 Gustavo Rodríguez publica su primera novela, "La furia de Aquiles," con la cual inicia un trabajo literario que le ha valido tener una progresiva consolidación y ser finalista en premios internacionales como el Herralde y el Planeta-Casamérica.  

Un año después, en 2002, un narrador ya consagrado, Alfredo Bryce Echenique, obtiene el Planeta con "El huerto de mi amada", otorgado por la editorial homónima, la más poderosa de España y una de las mayores del mundo. El año siguiente, "Pudor", segunda novela de Santiago Roncagliolo, queda entre las cuatro finalistas del Herralde y es luego publicada por Alfaguara en 2004 con una audaz operación de márketing. En 2005, Jaime Bayly, criticado por sus detractores por emplear la narrativa como complemento de su celebridad televisiva, es único finalista del Planeta. Ese mismo año Alonso Cueto logra el Herralde con "La hora azul "; al siguiente Roncagliolo, con "Abril rojo", obtiene el premio de novela otorgado por su casa editora y al subsiguiente la novela "El susurro de la mujer ballena", de Cueto, queda finalista en la primera edición del Premio Planeta-Casa de América. Iván Thays, que ya había sido finalista del Rómulo Gallegos 2001, queda entre los finalistas del Herralde 2008 con "Un lugar llamado Oreja de Perro". El escritor peruano-estadounidense Daniel Alarcón fue considerado uno de los escritores más importante de la última generación en la literatura estadounidense, en tanto Carlos Yushimito y Roncagliolo fueron considerados entre los 22 escritores menores de 35 más importantes en español. Finalmente, el Nobel de Literatura es entregado a Vargas Llosa en año 2010. En esta secuencia de acontecimientos puede, ciertamente, rastrearse la incorporación de numerosa literatura peruana al flujo de la circulación de las letras españolas en el mundo globalizado.

Aunque el fenómeno dio una nueva visibilidad relativa en el mundo de habla hispana a las letras nacionales, también es cierto que la internacionalización de estos escritores y su premiación significó el auge de una nueva literatura peruana limitada a determinados patrones reconocibles que favorecían la industria editiorial globalizada. Desde esta perspectiva, las trasnacionales de la literatura, que en los primeros años del siglo XXI asentaron sus filiales en Lima, exigieron a los escritores mejor conectados con el mercado editorial una mayor profesionalización que satisficiera los estándares de formatos básicos de escritura preestablecidos, en detrenimiento de una producción original. En este nuevo perfil profesional se pueden entender las novela de Jeremías Gamboa, "Contarlo todo", y Renato Cisneros, "La distancia que nos separa". No obstante, dentro de una escena literaria animada por el crecimiento del mercado limeño del libro, las transnacionales también promovieron, de modo complementario, propuestas artísticamente innovadoras e incluso experimentales orientadas a públicos menos fascinados por los éxitos de best-sellers y afines a la discusión intelectual. En esta línea de escritura se incluyen libros como "El inventario de las naves" de Alexis Iparraguirre, "Aquí hay icebergs" de Katya Adaui y "Resina" de Richard Parra.

En paralelo al resurgimiento internacional y al reconocimiento de autores como los mencionados, en Perú en los últimos años también se desarrolla, como parte de la dinámica propia de un país multicultural, un proceso literario protagonizado por autores que sitúan su obra en los linderos de la cultura andina, rescatándola como forma artística producto de la especificidad de la nación peruana y su drama. Los escritores de esta tendencia reclaman, por un lado, la herencia de la obra de José María Arguedas y, por otro, denuncian la discriminación por parte de críticos y medios de comunicación de orientación "criolla", o culturalmente más afines con el sistema económico globalizado, que rige la administración de los llamados "bienes culturales". La disputa entre "andinos" y criollos se hizo patente a raíz de una serie de artículos agresivos publicados por ambos bandos luego de una primera descalificación mutua cuando se vieron las caras en un congreso de escritores peruanos en Madrid. Como consecuencia de la disputa pública, ganó visibilidad una nueva generación de escritores provincianos que continúa, en clave contemporánea e incluso posmoderna, la narrativa indigenista (y regionalista) de los años 40 (en particular surgen lazos con Alegría y Arguedas), con la obra de Manuel Scorza y con la narrativa regionalista y de ruptura de los años 70 (Eleodoro Vargas Vicuña, Carlos Eduardo Zavaleta, Edgardo Rivera Martínez, el grupo "Narración"). Se privilegia una reconstrucción del pasado a través de un proceso de ficcionalización de la historia, retomando un punto explotado por la nueva narrativa hispanoamericana y el boom. Así, si no son los primeros, son los que más ahondan en el tratamiento literario del proceso de la guerra interna (1980-1993). Un libro que ha contado con el elogio merecido de la crítica ha sido "Retablo" de Julián Pérez. La inserción en el mercado literario nacional de estos escritores es, además, distinta a los narradores capitalinos, ya que la difusión de sus obras se realiza principalmente en provincias y a través de formas alternativas (ferias regionales, conciertos folclóricos, periódicos o revistas de tiraje limitado). Fuertemente marcados por la oralidad y tradiciones andinas, los nombres más conocidos, además de Óscar Colchado, son Dante Castro Arrasco, Félix Huamán Cabrera y Zein Zorrilla.

Es importante señalar, asimismo, el significativo crecimiento que ha experimentado el mercado editorial peruano en la primera década del siglo XXI, debido a la reducción de costos que ha significado la introducción de tecnología digital en el ámbito editorial, la vigencia de la Ley del Libro y el impulso del Plan Lector de Ministerio de Educación. Por un lado, han aparecido diversas editoriales independientes como Estruendomudo, Matalamanga, Atalaya Editores, Sarita Cartonera, Bizarro, Borrador Editores, [sic] libros, Mundo Ajeno, Tranvías, Lustra, Mesa Redonda, Casatomada, Editorial Arkabas, Gaviota Azul Editores, entre otras. Estas casas impulsaron la creación de la Alianza Peruana de Editores, gremio independiente afiliado a un movimiento global por la defensa de la bibliodiversidad. Entre las nuevas editoriales Estruendomudo, en especial, es responsable de la aparición y difusión de nuevos narradores elogiados por la crítica. Por el otro, uno de los mayores grupos del mundo de habla hispana, Planeta, inauguró en 2006 su filial en el Perú, dando un ulterior impulso a un mercado en el que ya operaban otros dos grandes grupos internacionales: Santillana (España) y Norma (Colombia); desgraciadamente, este último abandonó la ficción. Este pequeño "boom" editorial ha permitido que un número elevado de escritores nuevos publique sus primeros trabajos durante esta década, especialmente escritores jóvenes nacidos en la década de los 70's.

En 2017 el Ministerio de Cultura del Perú retomó la convocotaria al Premio Nacional con el objetivo tanto de reconocer y posicionar en el mercado las obras de los autores nacionales, como de estimular la labor de la industria editorial del país.





</doc>
<doc id="9774" url="https://es.wikipedia.org/wiki?curid=9774" title="Lluvia ácida">
Lluvia ácida

Se llama lluvia ácida a la que se forma cuando la humedad del aire se combina con óxidos de nitrógeno, dióxido de azufre o trióxido de azufre emitidos por fábricas, centrales eléctricas, calderas de calefacción y vehículos que queman carbón o productos derivados del petróleo que contengan azufre. En interacción con el agua de la lluvia, estos gases forman ácido nítrico, ácido sulfuroso y ácido sulfúrico. Finalmente, estas sustancias químicas caen a la tierra acompañando a las precipitaciones, lo que constituye la lluvia ácida. Destruye plantas, cosechas y jardines, entre otros. 

Los contaminantes atmosféricos primarios que dan origen a la lluvia ácida pueden recorrer grandes distancias, ya que son trasladados por el viento a cientos o miles de kilómetros antes de precipitar en forma de rocío, lluvia, llovizna, granizo, nieve, niebla o neblina. Cuando la precipitación se produce puede provocar deterioro en el medio ambiente.

La lluvia normalmente presenta un pH de aproximadamente 5,65 (ligeramente ácido), debido a la presencia del CO atmosférico, que forma ácido carbónico, HCO. Se considera lluvia ácida si presenta un pH menor que 5 y puede alcanzar el pH del vinagre (pH 3), valores que se alcanzan cuando en el aire hay uno o más de los gases citados.


Otra fuente de dióxido de azufre son las calderas de calefacción domésticas que usan combustibles que contiene azufre (ciertos tipos de carbón o gasóleo). 


Una de las fuentes más importantes es a partir de las reacciones producidas en los motores térmicos de los automóviles y aviones, donde se alcanzan temperaturas muy altas. Este NO se oxida con el dioxígeno atmosférico, 

y este NO reacciona con el agua dando ácido nítrico (HNO), que se disuelve en el agua.

La acidificación de las aguas de lagos, ríos y mares dificulta el desarrollo de vida acuática, lo que aumenta en gran medida la mortalidad de peces. Igualmente, afecta directamente a la vegetación, por lo que produce daños importantes en las zonas forestales, y acaba con los microorganismos fijadores de nitrógeno.

El término "lluvia ácida" abarca la sedimentación tanto húmeda como seca de contaminantes ácidos que pueden producir el deterioro de la superficie de los materiales. Estos contaminantes que escapan a la atmósfera al quemar carbón y otros componentes fósiles reaccionan con el agua y los oxidantes de la atmósfera y se transforman químicamente en ácidos sulfúrico y nítrico. Los compuestos ácidos se precipitan, entonces, caen a la tierra en forma de lluvia, nieve o niebla, o pueden unirse a partículas secas y caer en forma de sedimentación seca.

La lluvia ácida, por su carácter corrosivo, corroe las construcciones y las infraestructuras. Puede disolver, por ejemplo, el carbonato de calcio, CaCO, y afectar de esta forma a los monumentos y edificaciones construidas con mármol o caliza.
Un efecto indirecto muy importante es que los protones, H, procedentes de la lluvia ácida, arrastran ciertos iones del suelo. Por ejemplo, cationes de hierro, calcio, aluminio, plomo o zinc. Como consecuencia, se produce un empobrecimiento en ciertos nutrientes esenciales y el denominado "estrés en las plantas", que las hace más vulnerables a las plagas.

Los nitratos y sulfatos, sumados a los cationes lixiviados de los suelos, contribuyen a la eutrofización de ríos, lagos, embalses y regiones costeras, lo que deteriora sus condiciones ambientales naturales y afecta negativamente a su aprovechamiento.

Un estudio realizado en 2005 por Vincent Gauci de "Open University", sugiere que cantidades relativamente pequeñas de sulfato presentes en la lluvia ácida tienen una fuerte influencia en la reducción de gas metano producido por metanógenos en áreas pantanosas, lo cual podría tener un impacto, aunque sea leve, en el efecto invernadero.

Entre las medidas que se pueden tomar para reducir las emisiones de los agentes contaminantes de este problema, contamos con las siguientes:




</doc>
<doc id="9776" url="https://es.wikipedia.org/wiki?curid=9776" title="Departamento de Madre de Dios">
Departamento de Madre de Dios

Madre de Dios (en quechua: "Amaru Suyu") es uno de los veinticuatro departamentos que, junto a la Provincia Constitucional del Callao, forman la República del Perú. Su capital y ciudad más poblada es Puerto Maldonado.

Está ubicado al sureste del país, en la Amazonía, limitando al norte con Ucayali y Brasil, al este con Bolivia, al sur con Puno y al oeste con Cuzco. Con 85 300 km² es el tercer departamento más extenso —por detrás de Loreto y Ucayali— y con 1,3 hab/km², el menos densamente poblado. Fue creado el a partir de territorios de Puno y Cuzco. 

Recibe su nombre del río Madre de Dios, de cuya cuenca son tributarios la mayor parte de los ríos de la región y sobre cuyas riberas se erige la capital departamental: Puerto Maldonado.

Desde el punto de vista jerárquico de la Iglesia católica, forma parte del Vicariato Apostólico de Puerto Maldonado

Los petroglifos en el río Shinkebeni (Petroglifos de Pusharo), indican una muy antigua presencia de seres humanos. Se cree que los Arahuacos (o sus antecesores) llegaron en migraciones, y de ellos se derivaron muchas etnias. Algunas tribus, como la machiguenga, sobreviven hasta nuestros días. Lo que hoy se conoce como Madre de Dios, formaba parte del antiguo Imperio inca, en la región conocida como Antisuyo. Los historiadores coinciden que la conquista de esta región fue difícil para los Incas, pues debieron enfrentar a tribus aguerridas y conocedoras de la zona. En la zona de la cuenca del río Nistron se encuentran también las ruinas de Mameria, asentamiento Inca, que fue descubierto en 1979.

Durante la colonia ingresaron expediciones españolas con resultados trágicos para los europeos. Al fin, en 1861, el coronel Faustino Maldonado exploró todo el territorio y en 1890; Carlos Fermín Fitzcarrald descubrió un istmo que unía las cuencas de los ríos Ucayali y Madre de Dios. Durante las siguientes décadas numerosos aventureros y comerciantes explotaron los bosques, ávidos de caucho y oro. A partir de 1915, ante la persistencia de los misioneros dominicos, las tribus locales empezaron a aceptar la civilización. Esto no siempre ha sido bueno para ellas, ya que tribus como los harakmbet han abandonado muchos de sus instrumentos tradicionales dependiendo en gran medida de los habitantes de la ciudad. Sin embargo, aún hoy existen grupos en total aislamiento físico y cultural.

El 26 de diciembre de 1912, siendo presidente de la República Guillermo Billinghurst, se promulga la Ley Nº 1782, aprobada por el Congreso de la República, que crea el departamento de Madre de Dios, y se asigna como su capital Puerto Maldonado en reconocimiento a Juan Álvarez de Maldonado, quien en 1567 exploró la región del río Amarumayo, hoy río Madre de Dios. Así mismo se crean con esta misma Ley, la provincia de Tambopata con los distritos Tambopata, Las Piedras e Inambari; la provincia de Manu, con su capital Puerto Manu; y la provincia de Tahuamanu con su capital Iñapari, que antes se llamaba Puerto Tacna.

A inicios del siglo XXI, Madre de Dios experimenta un fuerte crecimiento económico y demográfico con el auge de explotación de recursos naturales, siendo la minería de oro la actividad más importante; pero con un fuerte impacto sobre los ecosistemas.

Departamento íntegramente selvático; tiene zonas de selva alta, selva baja y la sabana de palmeras. Parte de su geografía es accidentada, pues los Andes se precipitan hacia la selva formando abruptas laderas.

Limita al norte con Ucayali y Brasil; al este con Brasil y Bolivia; al oeste con Cuzco; al sur con Cuzco y Puno. Su capital Puerto Maldonado, está en la confluencia del río Madre de Dios y el río Tambopata.

El clima es tropical, cálido, húmedo, con precipitaciones anuales superiores a 1000 mm. La temperatura media anual en la capital es de 26 °C con una máxima de 38 °C en agosto y septiembre, en algunas ocasiones puede llegar a los 40 °C y una mínima de 21 °C, con lluvias de diciembre a marzo. En años excepcionales el territorio es invadido por masas de aire frío provenientes del sur durante los meses de julio y agosto, ocasionando descensos excepcionales de la temperatura hasta 8 °C en fenómenos denominados como friajes.

Madre de Dios alberga algunas de las regiones de mayor biodiversidad del mundo. Por ejemplo, el Parque nacional del Manu tiene el récord en número de especies de anfibios y reptiles.

Tiene una enorme diversidad de aves, grandes especies de felinos sudamericanos (jaguar, tigrillo, puma), múltiples especies de lagartos, y otros reptiles como la boa constrictora y la shushupe. También hay abundantes y diversos monos, peces, insectos y en general conjuntos de animales.

Su flora es también muy rica, y entre ellas se encuentran especies de madera noble y alto interés.
Al igual podemos encontrar muchas especies como el "motelo sanango."

Este Departamento tiene una extensión de 85 300.54 km² con una población de 140 508 habitantes, cuenta con 3 provincias y 11 distritos:
Tiene una población de 140.000 habitantes que representa el 0,5% de la población nacional. 

Las principales comunidades nativas son: amarakaeri, arasaeri, kishambaeri, pukirieri, sapiteri, toyoeri, wachipaeri, arawak, machiguenga y mashko-piros.

Concentra el 0,5% de la población económicamente activa - [PEA] y tiene un aporte económico de 0,4% al [PBI] nacional. Durante el 2007 registró un crecimiento económico de 11,7% respecto al 2006 y en el periodo 2004-2007 la tasa de crecimiento anual fue 8,5%.

Sus riquezas naturales son abundantes: ricas maderas, frutos silvestres, metales preciosos, petróleo, hacen la fama del territorio tanto como su belleza. En los alrededores de Tambopata están los mayores centros de producción agropecuaria, que aún trabajan a pequeña escala.

Las cosechas de café, arroz, castañas, y la producción de pan son esenciales, lo mismo que la crianza de vacunos y cerdos. Por otro lado, existen pequeñas industrias de agua gaseosa, jabón y triplay. Hay una central térmica: la de Puerto Maldonado.

En agricultura destaca el arroz, caucho, maíz, yuca, plátano y coco. La madera es también fuente de ingresos, donde destacan la caoba y el cedro. Se practica la caza y la ganadería vacuna. Además, los lavaderos de oro en sus ríos le proporcionan importancia en la minería.

La actividad minera es la principal actividad económica de Madre de Dios. Esta actividad representa el 41% del PBI regional. La región representa 11% de la producción de oro peruano.

El Ministerio de Energía y Minas del Perú informó el 29 de agosto de 2004 que exploraciones realizadas en esta región ubicada en la selva sur oriental de este país, permiten proyectar la existencia de gas con estimados de 32 trillones de pies cúbicos, lo que representaría un potencial de 960 millones de barriles de gas natural líquido.

Madre de Dios ofrece una gran diversidad de flora y fauna en las diferentes áreas que se encuentran en él, como la Reserva Nacional Tambopata, en la confluencia de los ríos Torre y Tambopata, en donde la riqueza y variedad de aves es asombrosa; la reserva de Biósfera del Manu, compuesta del Parque Nacional del Manu donde solo se permite investigación antropológica y biológica, observación de vida y los procesos ecológicos en forma natural; la Zona reservada en la cual están permitidas las actividades turística e investigación con mínima manipulación y el Bajo Manu, zona en la que predominan las poblaciones de colonos que desarrollan actividades agrícolas, pecuarias y forestales, además se permite la realización de actividades económicas. Destacan el lago Nisisipi y Sandoval, las colpas de guacamayos de Colorado y Pariamanu, etc.

Son famosas sus puestas de sol, debido al reflejo de los nevados de la cadena del Cusco. Este fenómeno atmosférico se denomina "Rayo Blanco". Por otra parte, se encuentra la Reserva natural de Tambopata-Candamo, en la confluencia de los ríos La Torre y Tambopata, con una extensión de 5500 hectáreas de selva virgen, en donde la riqueza y variedad de aves alcanza a 600 especies, 900 especies de mariposas y 115 especies de libélulas.

En el Santuario Nacional de las Pampas del Heath vive el rarísimo lobo de crin. Todo este ecosistema tuvo una evolución inalterable durante miles de años, originando una de las mayores variedades de flora y fauna del mundo.

Para el año 2008, la oferta hotelera es de 2106 habitaciones, registrando 134 771 visitantes nacionales que efectuaron 235 892 pernoctaciones.

Los lugares de interés que alberga Madre de Dios son: Parque Nacional del Manú, Parque Nacional Alto Purus, Parque Nacional Bahuaja-Sonene, Reserva Nacional Tambopata, Reserva Comunal Amarakaeri, Reserva Comunal Purus y Pampas del Heath.



El departamento de Madre de Dios es sede de la Universidad Nacional Amazónica de Madre de Dios.

Como todos los otros departamentos del Perú y la Provincia Constitucional del Callao, constituye una región "de facto" con un Gobierno Regional propio además de un distrito electoral que elige dos congresistas.


De la religión católica:

La Segunda Religión más popular y profesada es la evangélica, teniendo cerca de 25 Iglesias en torno a la ciudad de Puerto Maldonado, capital del Departamento y aproximadamente 60 Iglesias en torno a la carretera Interoceanica Puerto - Iñapari y Puerto Mazuko.

El canto rezado Harákmbut por la etnia Huachipaire es parte de la expresión cultural de Madre de Dios.




</doc>
<doc id="9777" url="https://es.wikipedia.org/wiki?curid=9777" title="Caliza">
Caliza

La caliza es una roca sedimentaria compuesta mayoritariamente por carbonato de calcio (CaCO), generalmente calcita, aunque frecuentemente presenta trazas de magnesita (MgCO) y otros carbonatos. También puede contener pequeñas cantidades de minerales como arcilla, hematita, siderita, cuarzo, etc., que modifican (a veces sensiblemente) el color y el grado de coherencia de la roca. El carácter prácticamente monomineral de las calizas permite reconocerlas fácilmente gracias a dos características físicas y químicas fundamentales de la calcita: es menos dura que el cobre (su dureza en la escala de Mohs es de 3) y reacciona con efervescencia en presencia de ácidos tales como el ácido clorhídrico. 

En el ámbito de las rocas industriales o de áridos para construcción recibe también el nombre de piedra caliza. Junto a las dolomías y las margas, las calizas forman parte de lo que se conocen como rocas carbonáticas o calcáreas.

Si se calcina (se lleva a alta temperatura), la caliza da lugar a cal (óxido de calcio impuro, CaO).

Son muy características por su color claro, blanquecino o gris. Las calizas se forman en los mares cálidos y poco profundos de las regiones tropicales, en aquellas zonas en las que los aportes detríticos son poco importantes. Dos procesos, que generalmente actúan conjuntamente, contribuyen a la formación de las calizas:

El carbonato de calcio (CaCO) se disuelve con mucha facilidad en aguas que contienen dióxido de carbono (CO) gaseoso disuelto, debido a que reacciona con este y agua para formar bicarbonato de calcio [Ca(HCO)], compuesto intermedio de alta solubilidad. Sin embargo en entornos en el que el CO disuelto se libera bruscamente a la atmósfera, se produce la reacción inversa aumentando la concentración de carbonato de calcio (véase ley de acción de masas), cuyo exceso sobre el nivel de saturación precipita. De acuerdo a lo descrito, el equilibrio químico en solución sigue la siguiente ecuación:

Esa liberación de CO se produce, fundamentalmente, en dos tipos de entornos: en el litoral cuando llegan a la superficie aguas cargadas de CO y, sobre los continentes, cuando las aguas subterráneas alcanzan la superficie.
Este es el proceso fundamental de formación de grutas y cuevas con presencia de estalactitas y estalagmitas en muchas regiones calcáreas con piedras calizas denominadas también karsts, carsts o carsos. Estas últimas denominaciones de las regiones calcáreas provienen del nombre de la región eslovena de Carso, rica en estos minerales y paisajes.

Numerosos organismos utilizan el carbonato de calcio para construir su esqueleto mineral, debido a que se trata de un compuesto abundante y muchas veces casi a saturación en las aguas superficiales de los océanos y lagos (siendo, por ello, relativamente fácil inducir su precipitación). Tras la muerte de esos organismos, se produce en muchos entornos la acumulación de esos restos minerales en cantidades tales que llegan a constituir sedimentos que son el origen de la gran mayoría de las calizas existentes.

La sedimentación calcárea fue mucho más importante en otras épocas y actualmente está limitada a unas cuantas regiones de las mareas tropicales. Las calizas que se pueden observar sobre los continentes se formaron en épocas caracterizadas por tener un clima mucho más cálido que el actual, cuando no había hielo en los polos y el nivel del mar era mucho más elevado. Amplias zonas de los continentes estaban en aquel entonces cubiertas por mares epicontinentales poco profundos. En la actualidad, son relativamente pocas las plataformas carbonatadas [marcada con el (1) en la imagen superior], desempeñando los arrecifes (2) un papel importante.

La caliza, cortada, tallada o desbastada, se utiliza como material de construcción u ornamental, en forma de sillares o placas de recubrimiento. Ejemplos de este uso son numerosos edificios históricos, desde las pirámides de Egipto hasta la Catedral de Burgos. Machacada se usa como árido de construcción.

Es un componente importante del cemento gris usado en las construcciones modernas y también puede ser usada como componente principal, junto con áridos, para fabricar el antiguo mortero de cal, pasta grasa para creación de estucos o lechadas para «enjalbegar» (pintar) superficies, así como otros muchos usos por ejemplo en industria farmacéutica o peletera.
Es una roca importante como reservorio de petróleo, dada su gran porosidad. Tiene una gran resistencia a la meteorización; esto ha permitido que muchas esculturas y edificios de la antigüedad tallados en caliza hayan llegado hasta la actualidad. Sin embargo, la acción del agua de lluvia y de los ríos (especialmente cuando se encuentra acidulada por el ácido carbónico) provoca su disolución, creando un tipo de meteorización característica denominada kárstica. No obstante es utilizada en la construcción de enrocamientos para obras marítimas y portuarias como rompeolas, espigones, escolleras entre otras estructuras de estabilización y protección.

La caliza se encuentra dentro de la clasificación de recursos naturales entre los recursos no renovables (minerales) y dentro de esta clasificación, en los no metálicos, como el salitre, el aljez y el azufre.




</doc>
<doc id="9778" url="https://es.wikipedia.org/wiki?curid=9778" title="Puerto Maldonado">
Puerto Maldonado

Puerto Maldonado es una ciudad del sureste del Perú fundada en 1902. Fue reconocida como capital del Departamento de Madre de Dios en 1912, situada a orillas de la confluencia del río Madre de Dios y el río Tambopata. Es uno de los principales núcleos comerciales de la Amazonia, cuenta con acceso en la carretera interoceanica en la triple frontera con Bolivia y Brasil . Desde el punto de vista jerárquico de la Iglesia católica es sede del Vicariato Apostólico de Puerto Maldonado. Cuenta con una población de 78 996 habitantes según el XII Censo de Población, VII de Vivienda y III de Comunidades Indígenas 2017.. Puerto Maldonado se encuentra en el distrito y provincia de Tambopata del departamento de Madre de Dios.

La población nativa de Madre de Dios debió aparecer hace miles de años con la llegada de los arahuacos, quienes se derivaron en muchas etnias. Luego se relacionaría con los incas y los españoles.

Puerto Maldonado fue establecido en 1894 por el empresario cauchero Carlos Fermín Fitzcarrald, quien, durante un viaje de exploración que lo llevó a descubrir el istmo de Fitzcarrald. A su paso por la confluencia de los ríos Madre de Dios y Tambopata encontró una inscripción hecha por el explorador Faustino Maldonado, quien había sucumbido en el río Madera en 1861. Fitzcarrald hizo grabar en un gran árbol el nombre de Maldonado, como homenaje al pionero, y continuó el viaje. Pronto, el hito pasó a llamarse Pueblo Maldonado y se estableció un embarcadero donde anclaron naves de hasta 180 toneladas. La fundación oficial fue hecha por el Primer Comisario y Delegado Supremo del Gobierno don Juan S. Villalta, el 10 de julio de 1902, situado en lo que hoy es el Pueblo Viejo. Sin embargo, recién en 1985 se oficializa la fecha de fundación de la ciudad de Puerto Maldonado.

Madre de Dios es un departamento con abundantes selvas vírgenes y paisajes subyugantes. Posiblemente sea el área menos intervenida y erosionada de la Amazonía peruana. Además, la conjugación de su abrupta geografía, sus innumerables microclimas y la variedad de sus suelos han propiciado el desarrollo de una diversidad de formas vivientes. Tierra de anchos y pausados ríos y hermosas lagunas rodeadas de exuberante vegetación. Madre de Dios posee los mejores suelos de toda la selva amazónica, siendo la producción de castaña y caucho su principal fuente de ingresos. Igualmente, la región alberga tribus nativas para quienes el avance de la civilización aún no ha llegado. Las principales agrupaciones establecidas en la región son los huarayos, mashcos, piros, amahuacas, yaminahuas, amaracaes y machiguengas.

El clima de Puerto Maldonado es tropical húmedo con temperaturas altas durante todo el año aunque especialmente agosto y septiembre, a esto se le suma la sensación térmica que en algunas ocasiones roza cerca de los 50 °C. En la mayoría de los meses del año en Puerto Maldonado hay precipitaciones importantes especialmente son abundantes de octubre hasta abril. La temperatura media anual en Puerto Maldonado se encuentra a 25.4 °C. Hay alrededor de precipitaciones de 2221 mm. Durante el invierno pueden ocasionalmente ocurrir los denominados friajes que son masas de aire frío provenientes de la Antártida que logran bajar la temperatura incluso por debajo de los 10 °C.


Durante la época colonial y parte de la época republicana, hasta fines del siglo XIX, la vinculación de Puerto Maldonado con el resto del país y el mundo se caracterizó por avances y repliegues periódicos de las actividades extractivas, los que dependieron de los ciclos económicos europeos. Los más relevantes fueron la economía extractiva del caucho, y hacia fines del siglo XIX la cascarilla o quina, empleado para tratar la malaria. Asimismo la shiringa, la madera, la castaña y el oro. 

A partir del siglo XX, la extracción de caucho se introdujo en la cuenca del río Madre de Dios, incursionando por el Istmo de Fitzcarrald y llegando de esa manera a Puerto Maldonado, que por esas fechas era un lugar de tránsito para todo quien pasara. El auge en el precio de oro tras el acuerdo de Bretton Woods en la década del 40, empujó nuevamente hacia la región a diversos pobladores en busca de oro, que en el marca de un proceso paulatino y repleto de contratiempos culminó con la construcción de dos carreteras: a Puerto Maldonado en el bajo Madre de Dios, y a Shintuya en el alto Madre de Dios.

En la actualidad, la capital de Madre de Dios, se caracteriza por la heterogeneidad de las actividades económicas desarrolladas, distinguiéndose tres frentes económicos:



La dinámica de la actividad comercial, encargada de articular los tres frentes económicos, tiene un comportamiento cada vez creciente, gracias a la migración de todas partes del Perú.

A partir del año 2010 existe una carretera pavimentada desde Cusco hasta Iñapari en la frontera con Brasil, esta vía denominada "Carretera Interoceánica" es un punto de acceso a diversos atractivos a lo largo de su recorrido, donde Puerto Maldonado se convierte en un paso obligado de en la agenda turística.

El Aeropuerto Internacional de Puerto Maldonado es una importante referencia para los visitantes atraídos por el turismo ecológico.


La ciudad de Puerto Maldonado, cuenta con los siguientes Hospitales:

Independientemente de estos dos Hospitales, que carecen de especialistas de alto nivel, tiene una red de centros de salud y de IES distribuidas a lo largo de la ciudad y de las dos provincias existentes.




</doc>
<doc id="9779" url="https://es.wikipedia.org/wiki?curid=9779" title="Wiphala">
Wiphala

La wiphala es una bandera cuadrangular de siete colores, usada por algunos pueblos andinos, está presente especialmente en Bolivia así como en algunas regiones del Perú, Colombia, en el norte de Argentina y de Chile, el sur de Ecuador y el oeste de Paraguay. Es reconocida como símbolo del Estado Boliviano por la Constitución de 2009, y regulada por el D.S. N.º 241, 5 de agosto de 2009.

Es probable que la palabra "wiphala" provenga de dos palabras en idioma aimara:
De ambos surge el término wiphailapx; "el triunfo que ondula al viento". Por razones eufónicas, el vocablo se convirtió en wiphala.

La wiphala se denomina de varias otras maneras:

Los pueblos precolombinos de la cordillera de los Andes no carecían de símbolos propios (especialmente los de tradición estatal, como el Imperio incaico), pero el formato de «pendón cuadrilátero de tela» para ondear al viento no es una tradición americana sino del Viejo Mundo. Por ello los orígenes precolombinos de la wiphala no deben ser ubicados como bandera sino como un diseño recurrente en la simbología pre-hispánica.
El ejemplar más antiguo aún preservado de un diseño tipo wiphala corresponde a una chuspa o bolsa para la coca correspondiente a la cultura tiahuanaco (1580 a. C. — 1187 d. C.). Su uso del diseño wiphala se encuentra mezclado con varios otros por lo que no es posible establecer su significado o uso dentro de la cosmogonía andina de la época.
Existen dos ejemplos del uso de la wiphala dentro del periodo precolombino cuyo carácter parece ser análogo a la vexilología de las banderas del viejo mundo. Eso corresponde a dos antiguos vasos quero, en los que se puede ver a hombres indígenas sosteniendo una wiphala a modo de estandarte. Estas dos piezas se encuentran actualmente en el Museo Inka (o Museo Arqueológico del Cusco) y en el Museo de arte de Birmingham respectivamente. Un tercer ejemplar de vaso quiru elaborado durante el periodo colonial muestra una figura similar con un estandarte tipo wiphala, evidenciando que el simbolismo no desapareció con el advenimiento del domino español.

El Museo de la Cultura Mundial en Gotemburgo, Suecia, contiene un tejido con un diseño tipo wiphala que data del siglo XI. Se origina en la región de Tiahuanaco, y es parte de una colección basada en la tumba de un curandero kallawaya.

El uso de la wiphala como estandarte parece haber continuado en alguna medida durante el periodo colonial. Existen dos representaciones de ángeles hechas en el estilo de la escuela cuzqueña. La primera es un cuadro del siglo XVIII, titulado "Gabriel Dei" y localizado en la Iglesia de Calamarca, muestra al ángel Gabriel a modo de arcabucero portando una bandera ajedrezada de forma muy similar a las figuras de los quiru precolombinos; el cuadro es de posible autoría del Maestro de Calamarca. La segunda es un cuadro sin título del siglo XVII o XVIII.

Existe también un ejemplo de una bandera cuadriculada en el cuadro de 1716 "Entrada del Virrey Arzobispo Morcillo en Potosí", conservado actualmente en Museo de América en Madrid.

La descripción más antigua de la wiphala durante el periodo republicano procede del naturalista francés Alcide d'Orbigny quién en 1830 describe la siguiente escena durante las fiestas de San Pedro en La Paz, en un área de mayoría aimara para la época:

A pesar de las muchas representaciones de la wiphala en el arte andino desde el periodo prehispánico, la primera mención documentada de su uso con nombre propio y carácter explícitamente indigenista proviene de Alberto de Villegas en 1931:


El 17 de noviembre de 1944 comenzó la organización del Primer Congreso Indigenal Boliviano. Entre quienes lo organizaron se encontraba el tradicionalista y aimarólogo Hugo Lanza Ordóñez. Este hizo notar a la concurrencia que la existencia de la palabra "wiphala" sugería que desde siempre en la cultura andina debió haber existido algún tipo de bandera, así que decidió hacer uso de una bandera blanca (la única conocida por entonces en los acontecimientos importantes). Otro congresista, Germán Monrroy Block, opinó a favor de usar una bandera más colorida, de acuerdo con la estética aimara.

El imprentero Gastón Velasco recordó que años atrás había diseñado una etiqueta para la marca Champancola, que era una empresa de gaseosas creada a fines de los veinte en La Paz, la primera en su género. Sus dueños eran dos ciudadanos italianos, Salvietti y Bruzzone. Esta fábrica producía un refresco espumante como el champán, sin alcohol, al que dieron el nombre de Champancola "(champagne-cola)". La botella tenía en el cuello una bolita de vidrio que ―por la fuerza del gas― tapaba el envase en forma hermética. Un empleado de la fábrica ―también italiano, de apellido Sorrentino― vendía las gaseosas por las calles de La Paz en un carrito tirado por un burro. La etiqueta que imprimió el paceño Velasco era un cuadro formado por otros cuadritos menores con los más diversos colores. Sobre la base de ese diseño, los tres compañeros movimientistas crearon la bandera del primer congreso indigenista.

Entre el 10 y el 15 de mayo de 1945 se realizó el congreso en el coliseo deportivo Luna Park de la ciudad de La Paz, que reunió a más de mil delegados de los pueblos originarios provenientes del sector occidental y oriental de Bolivia. El entonces presidente de Bolivia, mayor Gualberto Villarroel, tomó las disposiciones de este congreso y las publicó como decretos.

Se abolió el pongueaje y todo trabajo gratuito (trabajo esclavo). Muchos colonos dejaron de servir a sus explotadores y suspendieron las faenas agrícolas en las haciendas. Muchos indígenas fueron perseguidos y confinados a lugares inhóspitos como Ichilo (en Santa Cruz) y la isla Coatí en el lago Titicaca. Sin embargo la idea de recuperar sus tierras ancestrales, en manos de los explotadores, se generalizó entre los indígenas.

Tras un referéndum constitucional, se aprobó Constitución de 2009, en el cual se reconoce a la wiphala como símbolo nacional de Bolivia. El artículo 28 señala que se trata de un símbolo “sagrado que identifica el sistema comunitario basado en la equidad, la igualdad, la armonía, la solidaridad y la reciprocidad”.

En el año 2017, se modifico la bandera de la Reivindicación Marítima, agregando la Wiphala junto a la Tricolor:

La bandera de Reivindicación Maritima, es también, la bandera de la Armada Boliviana; la Whiphala es la bandera de guerra y forma parte del escudo del Ejército de Bolivia.

Durante el conflicto político que Bolivia, vivió entre el 21 de octubre y el 22 de noviembre de 2019, a causa de acusaciones de fraude en las elecciones del 20 de octubre, y la posterior renuncia de Evo Morales; algunos grupos opositores al gobierno del MAS (partido político de Evo Morales), entre ellos uniformados de la policía, quemaron o cortaron whipalas. En respuesta, organizaciones en la ciudad de El Alto, respondieron ondeando el símbolo constitucional al grito de “¡La wiphala se respeta,  carajo!” y “¡ahora sí, guerra civil!”. La crisis se saldó con 30 muertes.

Por tal motivo los comandos departamentales de la Policía en toda Bolivia, realizaron un acto de desagravio a la wiphala, después de las protestas en las que se quemó el símbolo patrio.

"Aquellas naciones que se sintieron ofendidas por el acto de algún policía que sepan que no es el sentir de la institución, que la Policía esta comprometida con su pueblo, con todos sin razón de raza, sexo o idiosincrasia", manifestó el comandante de la Policía en Cochabamba, Jaime Zurita.

Tanto en la ciudad de La Paz como El Alto, se utilizó la wiphala en viviendas y negocios como símbolo de protesta o para protegerse de saqueos.

En Bolivia, la wiphala es reconocida como emblema nacional desde el 7 de febrero de 2009, en la Constitución Política del Estado de 2009, y regulada su uso, colores y proporciones por el D.S. N.º 241, 5 de agosto de 2009. Esta debe ser izada al lado izquierdo de la Bandera Tricolor.

Respetando la implicancia del número siete en las proporciones de la Wiphala, la dimensión de cada uno de los cuadrados que lo componen debe estar relacionada al número siete y sus múltiplos.

El Decreto Supremo Nº 241 del 5 de agosto de 2009, establece las siguientes tonalidades de colores como oficiales, para la wiphala adoptada por el Estado Boliviano:

La Wiphala se iza en el lado izquierdo del frontis de los inmuebles de las instituciones públicas de Bolivia. Las dimensiones oficiales de la Wiphala son las siguientes:


En la unidades educativas, universidades públicas y privadas y otros centros de formación la Wiphala se iza en el lado izquierdo del frontis de sus respectivos inmuebles.


Existe una variante oficial de la wiphala, esta es una unión de cuatro wiphalas cuyas diagonales blancas forman una chacana o cruz andina color blanco que figura en el centro.

El estandarte de la Wiphala puede presentarse bajo dos formas:


Los colores de la Wiphala tiene el siguiente significado:

El 3 de octubre de 2017, la comuna de Alto Hospicio se declarara como una comuna multicultural; decretándose denominarse oficialmente como “Comuna Multicultural de Alto Hospicio”, por ende, el consejo municipal de la comuna, determinó por unanimidad el reconocimiento de la Wiphala como símbolo de la comuna, la cual debe ser izada junto a la bandera chilena y bandera comunal; asimismo se determinó la creación de la oficina de asuntos indígenas.

La Constitución Nacional Argentina en su reforma de 1994, reconoce a los pueblos indígenas argentinos (artículo 75, inciso 17) y les garantiza su identidad; este marco jurídico avala que por propia decisión y sin que medie necesariamente ningún reconocimiento oficial, adopten las banderas que libremente determinen.

En consecuencia, las wiphala es usada en Argentina, con la condición de que se reconozca la preeminencia de la bandera nacional argentina y de las provinciales.

El 20 de abril de 2011, la Cámara de Diputados de la Provincia del Chaco, promulga una ley autorizando a las distintas etnias chaqueñas a su adopción, hasta tanto un Congreso Indigenista Argentino, defina una bandera propia.

El 11 de julio de 2014, a través de la Resolución N° 278, se estableció que la wiphala debe ser izada cada 9 de agosto en el mástil del Concejo Deliberante de la ciudad de Salta, en conmemoración del Día Internacional de los Pueblos Originarios, junto a la bandera nacional de Argentina y la bandera provincial de Salta.

Usualmente suele atribuirse la wiphala como bandera del Imperio Inca, lo cual es incorrecto, lo más cercano a una bandera, utilizada en el Imperio Inca, es el estandarte imperial incaico, el cual es un estandarte o tocapu utilizado por el soberano inca durante el periodo imperial incaico. Vale señalar que dicho emblema no tiene un uso idéntico al de las banderas nacionales contemporáneas pues representaba el poder imperial y la figura del Inca o emperador.

Las crónicas y referencias de los siglos XVI y XVII respaldan la idea de un estandarte imperial:

Francisco de Jerez escribió en 1534 en su crónica "Verdadera relación de la conquista del Peru y provincia de Cusco, llamada la Nueva Castilla":
El cronista Bernabé Cobo escribió:
El libro 1615 de Felipe Guamán Poma de Ayala, "Primer nueva corónica y buen gobierno", muestra numerosos dibujos lineales de banderas incas. En su libro de 1847, "Historia de la conquista del Perú", William H. Prescott dice que "en el ejército inca cada compañía tenía su estandarte particular y que el estandarte imperial, sobre todo, mostraba el reluciente figura del arcoíris, la insignia heráldica de los Incas". En la edición de 1917 de "Flags of the World" se dice que entre los incas "el heredero aparente... tenía derecho a exhibir el estandarte real del arco iris en sus campañas militares".

La wiphala suele ser confundida con una bandera de siete franjas horizontales con los colores del arcoíris, usada como emblema oficial de la ciudad del Cuzco (Perú), y mal asociada al imperio inca. Sin embargo, debe observarse que mientras la wiphala es un emblema relacionado con los pueblos de origen aimara, los incas tuvieron su origen en las etnias quechuas.

Incluso, se ha determinado que esta supuesta "bandera de los incas" no tiene un sustento histórico, pues su aparición es reciente. Así, se sabe que en 1973, el ingeniero Raúl Montesinos Espejo, al conmemorar el vigésimo quinto aniversario de su radioemisora "radio Tahuantinsuyo" que operaba en la ciudad del Cusco, utilizó esta bandera con los colores del arcoíris. Su uso se extendió tanto en dicha ciudad que en 1978, Gilberto Muñiz Caparó ―alcalde de municipalidad provincial del Cusco― declaró esa bandera como emblema de la ciudad.

La historiografía peruana ha sido enfática en precisar que en el imperio inca no existió el concepto de bandera, y porque tanto este nunca tuvo una. Así lo ha afirmado la historiadora e investigadora del Tahuantinsuyo ―el Imperio inca―, María Rostworowski (1915-2016), quien al ser consultada sobre esta enseña multicolor señaló:

En 2011, el Congreso de la República del Perú ―citando a la Academia Peruana de Historia― se pronunció contra esta bandera del Tahuantinsuyo:

De acuerdo a las costumbres y tradiciones andinas, la wiphala siempre está izada en todos los acontecimientos sociales y culturales, por ejemplo, en los encuentros de comunarios del Ayllu, en los matrimonios de la comunidad, cuando nace un niño en la comunidad, cuando se realiza el corte de cabello de un niño (bautismo andino), en los entierros, etc.

La wiphala también flamea en las fiestas solemnes, en los actos ceremoniales de la comunidad, en los actos cívicos del "marka" (‘pueblo’) en los juegos de "wallunk’a" (‘columpio’), en los juegos de competencia "atipasina" (‘ganarse’), las fechas históricas, en los "k'illpa" (días ceremoniales del ganado), en la transmisión de mando de las autoridades en cada período.

También se utiliza en las danzas y bailes, como en la fiesta del Anata o Pujllay (‘juego’): en los trabajos agrícolas con o sin yuntas, a través del "ayni", la "mink'a", el "chuqu" y la "mit'a". Incluso se iza al concluir una obra, una construcción de una vivienda y en todo trabajo comunitario del "ayllu" y "marka".

Algunos de los colores tienen que ver con dioses y creencias de los pueblos originarios.


Los colores se originan en el rayo solar al descomponerse del arcoíris blanco "(kutukutu)", en los siete colores del arcoíris "(kurmi)", tomado como referencia por los antepasados indígenas, para fijar la composición y estructura de nuestros emblemas, organizar la sociedad comunitaria de los Andes.

En el momento de izar la wiphala, todos deben guardar silencio y al terminar alguien debe dar la voz de victoria del "jallalla qullana marka", "jallalla pusintsuyu" o "jallalla tahuantinsuyu".

Las banderas regionales son de un solo color entero y cada una se caracteriza por el color asignado (de acuerdo con la región).




</doc>
<doc id="9781" url="https://es.wikipedia.org/wiki?curid=9781" title="Aimaras">
Aimaras

Aimaras (en aimara: ), a veces escrito como aymara, es un pueblo indígena u originario de América del Sur, que habita la meseta andina del lago Titicaca desde tiempos precolombinos, extendiéndose entre el occidente de Bolivia, el norte de Argentina, el sureste del Perú y el norte Grande de Chile.

Alternativamente, reciben el nombre de collas. No debe confundirse con la etnia del mismo nombre que habita en el norte de Chile y norte de Argentina, ni con la expresión colla, usada para referirse a los habitantes del occidente de Bolivia. Aunque no hay correspondencia biunívoca entre ambos nombres.

El concepto de aimara aparece definitivamente durante la colonia y, salvo raras excepciones, no fue utilizado para identificar sociopolíticamente a ningún grupo poblacional de esa zona de los Andes. Todas estas formaciones sociopolíticas, verdaderas naciones durante los siglos y (reinos aimaras), fueron agrupadas bajo la etiqueta “aymara”, para fines económicos, pero manteniéndose las nominaciones originarias para describir, por ejemplo, las organizaciones políticas más relevantes de acuerdo a los intereses económicos, eclesiásticos o administrativos territoriales fluctuantes de la colonia. 

Los antecesores de los actuales aymaras nunca supieron que se llamaban así. Los incas los llamaban collas, hasta que en 1559 Juan Polo de Ondegardo y Zárate los denominó «aymaras» a partir de la información lingüística obtenida en el Collao de una pequeña colonia de mitimaes «quechuas», pero que habían incorporado el lenguaje local y que se denominaban aymaras y provenían de los alrededores de Cuzco. Así se llamó «en español» al idioma, cuyo real nombre era "jaqi aru" (que significa "humanidad" y "lengua", respectivamente) y después le aplicaron ese nombre a quienes hablaban ese idioma, quienes se llamaban a sí mismos jaqi. 

Los documentos tempranos de la colonia no dan nombre propio al idioma, sino que los cronistas Cieza de León y Pedro Pizarro se refieren a él como «lengua del Collao» y «lengua de los collas» respectivamente. En 1559, el licenciado Juan Polo de Ondegardo, a la sazón corregidor del Cusco, escribe la relación "De los errores y supersticiones de los indios" tras haber convocado una junta de «indios viejos que habían quedado» (de la época inca) que le sirvieron de informadores. Por este medio, Ondegardo tuvo conocimiento de un grupo de mitimaes (una etnia desplazada por el estado) que era originario de la región cusqueña y que, en última instancia, había acabado asimilando el habla aimaraica de su nuevo entorno. Tomando la parte por el todo, se empezó a usar el nombre de la etnia trasplantada como nombre de todo el idioma, de manera que en las publicaciones producidas por el III Concilio Limense (1584-85), que incorporan también un extracto de la mencionada obra de Ondegardo, aparece por primera vez la palabra «aymara» explícitamente aplicada a la lengua. 

Las naciones o pueblos que ancestralmente hablan este idioma eran: Aullaga, Collagua, Cana, Canchi, Caranga, Charca, Omasuyus, del aimara "uma", ‘agua’ y "suyu", ‘lado’; entonces significaría ‘los del lado del agua’, Pacaje de "pacajaki", "paca", ‘águila’, y "jaquis:" ‘gente’; entonces significaría ‘los hombres águila’, los Lupaca de "lupijaki", "Lupi", ‘sol’, y "jaqui:" ‘gente’; entonces significaría ‘los hombres del sol’, Quillaca y los Kollas de "Qulla", ‘sabio’ entonces significaría ‘los sabios’. 

A estos pueblos se les ha atribuido una única identidad con el nombre "qullasuyu" (también conocido como Collasuyo) y conformaron una parte del Imperio inca.

La etimología original del glotónimo «aimara» ( ) se encuentra dentro de lo especulativo, aunque se sabe que proviene de un etnónimo originario del departamento peruano de Apurímac. Procede de los Andes centrales en Lima, en la serranía central del Perú. Se fue extendiendo hacia el sur como "lingua franca", y fue adoptada como lengua materna por los pueblos de la cultura wari. Hacia el Intermedio Tardío fue reemplazada por el quechua desde la costa hasta el Cuzco y su ingreso se habría producido, al parecer de manera violenta, por conquista militar avanzando desde el norte hacia el sur-este a lo largo de la Cordillera Occidental de los Andes y se repartieron el territorio altiplánico posiblemente bajo forma de diversos señoríos o reinos; algunos mencionados por Bertonio son: Lupacas, Pacajes, Carancas, Quillaguas, Charcas y otros.

La historia acerca del surgimiento u origen de la cultura aymara es bastante compleja y han surgido diferentes opiniones e hipótesis acerca de ello, inicialmente se creía que esta etnia sería descendiente de la cultura Tiahuanaco, por parte de antropólogos e historiadores como Carlos Ponce Sanginés o Max Uhle, algunos de sus principales argumentos eran los siguientes:




Desaparecido el Estado tiwanaku, la región quedó fragmentada en etnias aimaras. Estos aymaras se caracterizan por sus necrópolis compuestas por tumbas en forma de torres-chullpas. Existen también algunas fortalezas denominadas pucarás.

Sin embargo, la teoría del origen tiwanakota del aymara quedaría relegado por investigaciones históricas y lingüísticas recientes. En las últimas décadas, se tuvo un animado debate sobre los orígenes del aymara, desde tres posiciones teóricas. En tanto que Alfredo Torero (1972) y sus seguidores vieron un origen de esta lengua en la parte central del Perú, y por tanto la expansión del aymara desde allí, Teresa Gisbert (1987) y otros, en base a algunos cronistas, plantearon un origen desde el sur (desde la región de Copiapo), con una expansión al norte, en tanto que Lucy Briggs (1994) percibió un patrón de expansión desde el núcleo de Tiwanaku. Por otro lado, según Cieza de León, los aymaras son procedentes de “Coquimbo”, un valle del norte chileno, John Hyslop demuestra la importancia de un sitio altiplánico llamado Coquimbo como capital lupaca y necrópolis de sus mallkus. Es decir, la ola aymara que penetra en el Urcosuyo viene de mucho más al sur de lo que pretende Torero.

Por lo tanto, ambas hipótesis –de origen norteño y de origen sureño– nos muestran que el aymara no pudo haber tenido como cuna de origen el altiplano peruano-boliviano; y que tanto aymara como quechua procedían de otras zonas y que no son oriundos de la cuenca del Titicaca. La cultura tiahuanaca era multilingüe, pues se hablaban el pukina y, en menor participación, el uru.

En la actualidad, según la mayor parte de los estudios hoy, tanto arqueológicos como lingüísticos, ambas familias de lenguas, Idioma quechua e Idioma aymara, tienen su origen en una determinada región en común de la parte central de lo que es actualmente Perú (Heggarty 2008). Este sitio fue probablemente en la sierra, aunque Alfredo Torero y Rodolfo Cerrón favorecen un sitio costeño (Cerrón 2003: 22, Torero 2002: 46). Arqueológicamente se reconoce la posibilidad del origen de ambas lenguas, en una forma pre-proto, sean aymara o quechua, en sitios como Caral-Supe (3000-1600 a.C.) o quizás Chavín (1500 a.C. – 200 d.C.). Se favorecen también los grandes horizontes arqueológicos con su mayor unidad cultural y geográfica, sobre todo el Horizonte temprano, como los motores para la expansión de ambas lenguas. Torero también propone que se hablaba una forma temprana de aymara en sitios costeños como Nasca y Paracas y que desde allí hubo una expansión al norte a la región de Yauyos y al sur a la región de Ayacucho.

Sobre todo, está la cultura wari (550-1000 d.C.) en el Horizonte Medio que es la favorita actual entre los estudiosos, desde Torero en adelante, como el motor de la gran expansión del aymara como una lengua franca hacia el norte como hacia el sur. Quizás esta expansión se debe a la influencia de los pastores por excelencia y los guardianes de las caravanas de llamas que manejaban el comercio entre los wari y sus periferias, proceso que fue seguido por los agricultores quechuas con sus nuevas técnicas de riego y andenes en la producción de maíz. Tal vez la caída de los wari también resultó en una cesión de territorio aymara a la llegada del quechua. Cerrón habla de una tercera expansión aymara hacia el sur, desplazado por el quechua, en el período Intermedio tardío, desde la región del grupo de los Aymaraes en el Apurímac (que también podría haber dado el nombre aymara).

A mediados del siglo , el reino colla conservaba un extenso territorio con su capital, Hatun-Colla. El inca Viracocha incursionó en la región, pero quien la conquistó fue su hijo Pachacútec, noveno inca. Al norte se encontraban los collas y lupacas; al sur estaba la Confederación Charca, que tenía dos grupos: los carangas y quillacas en torno al lago Poopó, y los charcas, que ocupaban el norte de Potosí y parte de Cochabamba. Tanto charcas como collas eran de habla aimara. 
La cultura material de los carangas presenta extensas necrópolis o chullpares, algunas de las cuales conservan todavía restos de pintura en sus muros exteriores. Una vez que los carangas fueron conquistados por los incas, Huayna Cápac los llevó a trabajar al valle de Cochabamba como mitimaes. El señorío denominado Charca, al que estaban adscritos cara-caras, fue conquistado por los incas en tiempo de Túpac Inca Yupanqui y llevados a la conquista de Quito. Por su parte, el pueblo de los cara-cara era tan belicoso como el charca y aún más. En su territorio tienen lugar aun hoy en día luchas denominadas "T'inkus".

El inca Lloque Yupanqui inició la conquista del territorio aimara a finales del siglo , la que fue continuada por sus sucesores hasta que a mediados del siglo fue completada por Pachacútec al derrotar a Chuchi Kápak. De todas formas, se cree que los incas tuvieron una gran influencia de los aimaras durante algún tiempo, ya que su arquitectura, por la cual son muy conocidos los incas, fue claramente modificada sobre el estilo Tiwanaku, y finalmente los aimaras conservaron un grado de autonomía bajo el imperio Inca. Posteriormente, los aimaras del sur del Titicaca se rebelaron y, tras rechazar el primer ataque de Túpac Yupanqui, este volvió con más tropas y los sometió.

Su población se estima en 1 a 2 millones de personas durante el Imperio inca, eran el principal pueblo del Collasuyo, ocupando todo el oeste de Bolivia, sur de Perú, norte de Chile y el norte de Argentina. Tras la conquista española en menos de un siglo se redujeron a cerca de 200 000 sobrevivientes, o menos. Tras la independencia su población empezó a recuperarse.

En la actualidad, la mayor parte de los aimaras viven ahora en la región del lago Titicaca y están concentrados en el sur del lago. El centro urbano de la región aimara es El Alto, ciudad de 750 000 habitantes, y también en la La Paz, sede de gobierno de Bolivia. Además, muchos aimaras viven y trabajan como campesinos en los alrededores del Altiplano. Se estima en 1 600 000 a los bolivianos aimara-parlantes. Entre 300 000 y 500 000 peruanos utilizan la lengua en los departamentos de Puno, Tacna, Moquegua y Arequipa. En Chile hay 48 000 aimaras en las áreas de Arica, Iquique y Antofagasta, mientras que un grupo menor se halla en las provincias argentinas de Salta y Jujuy.

El aimara utilizó un tipo de proto-khipus, sistema nemotécnico de contabilidad básica común a varios pueblos precolombinos, como los de Caral-Supe y Wari (anteriores a los aimara), y los Incas. No existen evidencias de que hayan tenido un lenguaje escrito, a pesar de que algunos, como William Burns Glyn, sostienen que los khipus incaicos pudieron ser una forma de ello. 

La Encuesta Complementaria de Pueblos Indígenas (ECPI) 2004-2005, complementaria del Censo Nacional de Población, Hogares y Viviendas 2001 de Argentina, dio como resultado que se reconocieron y/o descienden en primera generación del pueblo aimara 4104 personas en Argentina.

El Censo Nacional de Población de 2010 en Argentina reveló la existencia de 20 822 personas que se auto-reconocieron como aimaras en todo el país, 9606 de los cuales en la ciudad de Buenos Aires, 6152 en la provincia de Buenos Aires, 773 en la de Jujuy, 358 en la de Neuquén y 326 en la de Tucumán.

Existe una única comunidad con personería jurídica reconocida por el Estado nacional, la Comunidad Aborigen Rodeo San Marcos Luján La Huerta, que es conjunta entre los pueblos aimara, kolla y omaguaca, y se encuentra en la localidad de Santa Victoria Oeste en la provincia de Salta.

La población que se auto-reconoció como aimara en el censo boliviano de 2001 fue de 1 277 881 personas. Este número bajó a 1 191 352 en el censo de 2012.

El Censo Nacional 2017 reveló que el 2,4 % de la población de 12 y más años de edad (548 292) se autoidentificó como de origen aimara.

A los aimaras generalmente se les agrupa en un solo grupo etnolingüístico, pero se pueden reconocer varios grupos, entre los que destacan los lupacas, urus y pacajes.

Dentro de las etnias aimaras en el Perú, también se incluyen a dos etnias aisladas geográficamente de las demás etnias aymaras que por tradición habitan los alrededores de la meseta del Collao. Estas etnias son los jaqarus y los kawkis, que habitan las sierras del distrito de Tupe, Provincia de Yauyos, en la región Lima. Las lenguas de estas etnias fueron estudiadas por primera vez en 1959 por Martha Hardmann, catalogándolas en la familia aru o aymara.

Su idioma es la lengua aimara, aunque muchos de ellos hablan castellano como consecuencia de la colonización o conquista española.

Más allá del debate histórico, actualmente las organizaciones aymaras y demás movimientos sociales suelen usar la wiphala en manifestaciones y reivindicaciones políticas y en ceremonias religiosas y culturales. 

El debate sobre si el uso actual del wiphala se corresponde con la historia o no sigue abierto.

Algunas personas practican el acullico, práctica consistente en el consumo de la hoja sagrada de coca "(Erythroxylum coca)". Por su condición de hoja sagrada durante la época del imperio incaico, su uso estaba restringido al inca, nobleza y sacerdotes bajo pena de muerte. Además del uso en masticación, utilizan las hojas de coca en remedios al igual que en rituales.
Durante este último siglo, estas plantaciones les han traído conflictos con las autoridades, por prevenir la creación de la droga cocaína. Sin embargo, la coca tiene gran participación en la religión de los aimaras, al igual que antes con los incas y últimamente se ha convertido en un símbolo cultural de su identidad. Los cultos de Amaru, Mallku y Pachamama son las formas más antiguas de celebración que los aimaras aún realizan.

Aún no existen fundamentos históricos para determinar que el año aimara se celebra el 21 de junio o para establecer un cómputo exacto del año que se cumple (por ejemplo, en el 2017 se llegaría al año 5525 del calendario aimara; tal fecha (21 de junio) coincide con el solsticio de invierno, el cual fue festejado ancestralmente por el pueblo quechua en la fiesta del Inti Raymi.

A partir del año 2013, el día 21 de junio es «feriado nacional inamovible» en Bolivia.

En Tiwanaku, los comunarios y turistas que vienen a conocer y a compartir esta fiesta milenaria realizan el día 20 de junio una víspera similar al Año Nuevo tradicional para despedir el año viejo. A partir de entre las 6:00 y 7:00 de la mañana, se preparan con música folclórica tradicional y rituales para recibir el nuevo año frente a la Puerta del Sol con la entrada de los primeros rayos del sol, como también la llegada del solsticio de invierno.

Esta tradición milenaria que se ha conservado en su cosmovisión ancestral, dice que la llegada todos los niños en el verano es para el bienestar y la buena fertilización de la cosecha. Lo mismo y similar al año nuevo tradicional, para los creyentes los años venideros serán de gran prosperidad para quienes lo deseen. Los sacerdotes comunarios realizan rituales y agradecen a la Pachamama solicitando su bendición.
Su creencia no se aprecia en forma de adoraciones exageradas en lo abstracto o invisible. Tienen una religiosidad viviente, donde los vivos y los muertos no dejan de existir, es decir, solo cumplen un ciclo de vida para volver al inicio. Las divinidades son energías y son sus sobrevivencias.

El Tata-Inti o Dios Sol y la Pachamama o Madre Tierra son los puntos de partida de todo. Por eso toda ceremonia se inicia mirando hacia arriba, hacia el sol.



</doc>
<doc id="9784" url="https://es.wikipedia.org/wiki?curid=9784" title="Horno de microondas">
Horno de microondas

El horno de microondas es un electrodoméstico usado en la cocina para cocinar o calentar alimentos que funciona mediante la generación de ondas electromagnéticas en la frecuencia de la radiación en torno a los 2450 MHz (2.45 10 Hz).

Un horno de microondas consiste en:


Los hornos de microondas, aunque protegidos por razones de seguridad, aún emiten bajos niveles de radiación de microondas. Esto no es perjudicial para los seres humanos, pero a veces puede causar interferencias en la señal Wi-Fi o Bluetooth y en dispositivos que se comunican en las bandas de onda de 2.45 GHz; particularmente a corta distancia.

Un microondas es un electrodoméstico destinado a cocinar o calentar alimentos que actúa calentando el agua que contienen o los líquidos que se añaden. Funciona mediante la generación de ondas de radio de alta frecuencia. El agua, las grasas y otras sustancias presentes en los alimentos absorben la energía producida por los microondas en un proceso llamado calentamiento dieléctrico (conocido también como calentamiento electrónico, calentamiento por RF, calefacción de alta frecuencia o diatermia). Hay moléculas, como la del agua, cuya estructura forma dipolos eléctricos, lo que significa que tienen una carga positiva parcial en un extremo y una carga negativa parcial en el otro, y por tanto oscilan en su intento de alinearse con el campo electromagnético alterno de los microondas. Al rotar, se producen rozamientos y choques, que son los que elevan la temperatura. Los hornos de microondas funcionan de la siguiente manera: un aparato llamado magnetrón convierte la energía eléctrica en energía de microondas, que en esta forma alcanza el alimento. Las ondas electromagnéticas agitan las moléculas bipolares presentes en los alimentos, especialmente las del agua, y estas son las que elevan la temperatura. Esta agitación es un mecanismo físico, simple movimiento de las moléculas al ritmo de la frecuencia, y no provoca ningún tipo de alteración en la composición química (excepto los que son producidos por el aumento de la temperatura). 

El calentamiento por microondas es más eficiente en el agua líquida que en el agua congelada, ya que en el estado sólido del agua el movimiento de las moléculas está más limitado. También es menos eficiente en grasas y azúcares porque tienen un momento dipolar molecular menor que el agua líquida.

A veces se explica el calentamiento por microondas como una resonancia de las moléculas de agua, pero esto es incorrecto, ya que esa resonancia sólo se produce en el vapor de agua y a frecuencias mucho más altas (a unos 20 GHz). Por otra parte, los grandes hornos de microondas industriales que operan generalmente en la frecuencia de 915 MHz (longitud de onda de 32,8 milímetros) también calientan el agua y los alimentos de forma efectiva.

Los azúcares y triglicéridos (grasas y aceites) absorben las microondas debido a los momentos dipolares de sus grupos hidroxilo o éster. Sin embargo, debido a la capacidad calorífica específica más baja de las grasas y aceites y a su temperatura más alta de vaporización, a menudo alcanzan temperaturas mucho más altas dentro de hornos de microondas. Esto puede causar en el aceite o alimentos muy grasos, como el tocino, temperaturas muy por encima del punto de ebullición del agua, llegando a tostar de forma parecida al asado en la parrilla convencional o en las freidoras. Los alimentos con alto contenido en agua y bajo en aceite rara vez superan la temperatura de ebullición del agua (100° C).

El calentamiento por microondas puede provocar un exceso de calentamiento en algunos materiales con baja conductividad térmica, que también tienen constantes dieléctricas que aumentan con la temperatura. Un ejemplo de ello es el vidrio, que puede mostrar embalamiento térmico en un horno de microondas hasta el punto de fusión. Además, las microondas pueden derretir algunos tipos de rocas, produciendo pequeñas cantidades de lava sintética. Algunas cerámicas también se pueden fundir, e incluso pueden llegar a aclarar su color al enfriarse. El embalamiento térmico es más típico de líquidos eléctricamente conductores, tales como el agua salada.

Un error común es creer que los hornos microondas cocinan los alimentos de dentro afuera, es decir, desde el centro hacia el exterior del alimento. Esta idea surge al observar la cocción de alimentos con una capa exterior más seca y un interior más húmedo; culinariamente expresado como crujiente por fuera y suave por dentro. En la mayoría de los casos, en alimentos uniformemente estructurados o razonablemente homogéneos en su composición física, las microondas son absorbidas de fuera adentro de forma similar a otros métodos de cocción por calor. Dependiendo del contenido de agua, la profundidad de la deposición de calor inicial puede ser de varios centímetros o más con los hornos de microondas, en contraste con el asado (infrarrojos) o el calentamiento convectivo (métodos que depositan el calor en una fina capa de la superficie de los alimentos). La profundidad de penetración de las microondas depende de la composición de los alimentos y de la frecuencia, siendo las frecuencias de microondas más bajas (longitudes de onda más largas) las más penetrantes. Las microondas penetran únicamente de 2 a 4 cm en el interior de los alimentos, por lo que el centro de una porción grande no se cocinará con la energía de estas ondas, sino por el calor que se produce en el horno y por el que se transfieren las partes superficiales que sí son alcanzadas por las ondas.

Como otros inventos, el horno de microondas es la aplicación secundaria de una tecnología destinada a otros fines. En 1945, durante una investigación relacionada con el radar, el doctor Percy Spencer, ingeniero de la Raytheon Corporation, estaba probando un tubo al vacío llamado magnetrón cuando descubrió que una chocolatina que tenía en su bolsillo se había derretido. Sospechando que aquello había sido causado por las ondas emitidas por el magnetrón, el doctor Spencer colocó algunas semillas de maíz para hacer palomitas cerca del tubo a modo de experimento. El maíz se coció e hinchó. Spencer repitió el experimento usando un huevo de gallina. Debido al rápido incremento de la temperatura, la presión interna hizo que el huevo explotara. Esto le animó a seguir experimentando con otros alimentos.

El doctor Spencer diseñó una caja metálica con una abertura por la que podía entrar la radiación del magnetrón. Las paredes metálicas confinan la radiación de microondas, por lo que la energía del campo electromagnético no se difundía, sino que se concentraba dentro de dichas paredes. Cuando se introducía alimento su temperatura aumentaba. Otros ingenieros se dedicaron a mejorar el prototipo del doctor Spencer, y a finales de 1946, la Raytheon Company solicitó una patente para emplear los microondas en la preparación de los alimentos. El primer horno en prueba, que calentaba los alimentos mediante energía de microondas se instaló en un restaurante de Boston. 

En 1947, salió al mercado el primer horno comercial de microondas. Estas primeras unidades eran aparatosas, de 1,60 m de altura y 80 kg de peso. El magnetrón se refrigeraba con agua, de modo que era necesario instalar un circuito especial. Además, su precio era elevado: costaban alrededor de 5.000 dólares cada uno, por lo que no tuvieron demasiada acogida.

Al desarrollarse un nuevo magnetrón enfriado por aire, se eliminó la necesidad de colocar tuberías de refrigeración, lo que permitió fabricar hornos más baratos y manejables. Los negocios de comida rápida fueron los primeros en reconocer su utilidad.

Cuando la industria alimentaria descubrió el potencial y la versatilidad del nuevo invento, este se aplicó a usos variados, como deshidratar verduras, tostar café o frutos secos, descongelar y cocinar las carnes, abrir ostras, pasteurizar leche, etc. Otras industrias lo emplearon para el secado de corcho, cerámica, papel, cuero, tabaco, fibras textiles, lápices, flores, libros húmedos y cerillas. También se emplearon las microondas en el proceso de curado de materiales sintéticos como nailon, hule y uretano. Sin embargo, a causa de la desconfianza hacia los nuevos "hornos electrónicos de radar", no fue hasta los años setenta cuando se empezó a usar en las cocinas domésticas. Nadie moría de "envenenamiento" por las radiaciones, ni quedaba ciego, estéril o impotente debido al uso de hornos de microondas. Cuando se desvanecieron los temores en Estados Unidos, aumentó la aceptación y, se olvidaron los mitos.

En 1971 menos del 1% de los hogares estadounidenses tenían microondas, en 1978 la cifra ascendió al 13%, llegando al 25% en 1986. En 1975 las ventas de hornos de microondas rebasaron el número de cocinas de gas por primera vez. Al año siguiente se informó de que el 17% de los hogares japoneses cocinaban con microondas, en comparación con el 4% de los hogares estadounidenses. Hoy, los hornos de microondas cuentan con muchas mejoras: temporizadores, sensores de horneado y resistencias eléctricas para gratinar o acabar de dorar los platos que lo necesiten, pues esto no es posible conseguirlo solo con las microondas.

La mayoría de gobiernos, industrias y la propia OMS defienden su uso como un electrodoméstico seguro para la salud teniendo en cuenta que la radiación electromagnética emitida por un horno microondas (de frecuencia típica de 2450 MHz) y el campo magnético asociado a su magnetrón no son ionizantes, es decir, no son capaces de ionizar átomos arrancando electrones a los mismos. 

Existen otros riesgos derivados de la utilización del microondas que pueden ser evitados mediante el estricto mantenimiento de las siguientes medidas de seguridad:


</doc>
<doc id="9786" url="https://es.wikipedia.org/wiki?curid=9786" title="Departamento de Arequipa">
Departamento de Arequipa

Arequipa es uno de los veinticuatro departamentos y con 1 678 730 habitantes en 2020 es el cuarto más poblado —por detrás de Lima, Piura y La Libertad, que, junto a la Provincia Constitucional del Callao, forman la República del Perú, y se encuentra ubicado en la sierra sur del país. Su capital y ciudad más poblada es Arequipa. Y tiene una densidad demográfica de 21,8 hab/km².

Está ubicado en el sur del país, limitando al norte con Ayacucho, Apurímac y Cuzco, al este con Puno, al sureste con Moquegua, al oeste con el océano Pacífico y al noroeste con Ica. Con 63 345 km² es el sexto departamento más extenso —por detrás de Loreto, Ucayali, Madre de Dios, Cuzco y Puno — y con 1 678 730 habitantes en 2020 es el cuarto más poblado, por detrás de Lima, Piura y La Libertad. 

Cuenta con 528 km de costas en el océano Pacífico —el litoral regional más extenso—. La zona costera es una de las porciones más secas del desierto costero, entretanto la región interior andina presenta valles escarpados y cañones.

El departamento tiene una población de 1,67 millones de habitantes, el 71,3% de los cuales reside en la capital, la ciudad de Arequipa.
El nivel educativo promedio es superior a la media nacional; tiene una tasa de analfabetismo del 3,5% y el 15% de la población tiene estudios superiores.
Tiene ocho provincias, de las cuales las más desarrolladas por el volumen de sus contribuciones económicas son Arequipa, Islay y Caylloma. Las Principales ciudades de la región, son primeramente, la capital, Arequipa, por su comercio e industrias; el turismo también es importante en Arequipa. La mina Cerro Verde es parte importante de su economía, después le sigue la ciudad de Mollendo, por el puerto de Matarani, el turismo de playa y por su agricultura, prontamente por la petroquimica y la mina Tía María, seguidamente del pueblo de Chivay, por su turismo, gracias al cañón del Colca, y el pueblo de Camaná, gracias al puerto de Quilca y al turismo de playa.

El 16,6% de la red de carreteras en el departamento es asfaltado, siendo Arequipa, Caravelí, Camaná y Caylloma las provincias con el porcentaje más alto de este tipo de superficie. Este es el segundo departamento más interconectado en términos de telecomunicaciones, después de Lima, ya que tiene 111,2 mil líneas de telefonía fija, con una teledensidad de 9 líneas por cada 100 habitantes y una densidad de 31,84 líneas móviles por cada 100 habitantes.

Su territorio fue ocupado inicialmente por el imperio Wari. Luego, la cultura Churajón dejó huellas de su paso en obras de riego, andenerías y tierras cultivadas. En el norte de sus valles se desarrolló la cultura Chuquibamba, que se extendió hasta las provincias sureñas de Ayacucho y tuvo contactos con el Cusco. La leyenda menciona que Arequipa fue fundada por el cuarto inca, Mayta Cápac, quien estuvo con su ejército en dicha zona. Cuando dispuso el desplazamiento de su gente, hubo quienes le pidieron quedarse, respondiendo el inca “ari qipay”, que en lengua quechua significa “sí, quedaos”.

Tiempo después, los conquistadores españoles fundaron la capital de la región en las faldas del Misti el 15 de agosto de 1540. Después de esa fecha y a lo largo de más de tres siglos la ciudad fue poblada por familias españolas. Es así que Arequipa fue la ciudad del Perú con más españoles. Su primer alcalde fue el conquistador D. Juan de la Torre y Díaz Chacón. En la época republicana ocurrieron aquí los alzamientos de Ramón Castilla, Mariano Ignacio Prado, Nicolás de Piérola Villena, Luis Miguel Sánchez Cerro y otros más. En época moderna de estadistas como José Luis Bustamante y Rivero y Fernando Belaúnde Terry. Arequipa es el cimiento del complejo económico del sur del Perú.

Está ubicado al suroeste del Perú, frente al Océano Pacífico con 527 kilómetros de litoral. Debido a esa ubicación, es el centro comercial de la zona sur del país, que incluye los departamentos de Apurímac, Cusco, Madre de Dios, Moquegua, Puno y Tacna; y, es parte del corredor turístico del sur peruano, lo que significa que está interconectado con el 40% del país, y encaramado sobre un repecho o cuesta en la Cordillera de los Andes. Limita al noroeste con Ica y Ayacucho; por el norte, con Apurímac y Cusco; por el este, con Moquegua y Puno; por el sudoeste, con el océano Pacífico.



Este departamento está conformado por ocho provincias, que se muestran en el siguiente cuadro con sus respectivas capitales.


Esta carretera tiene extensión de 2603 km (de 1071 kilómetros que corresponden a las carreteras que deberán ser asfaltadas, 1514 km de carreteras asfaltadas y 17,5 km de vías urbanas, sin incluir la zona urbana de Juliaca). Con estas obras se conectarán tres puertos peruanos en el océano Pacífico: San Juan de Marcona, Matarani e Ilo, con Iñapari en la frontera con Brasil.

La carretera es parte de la Infraestructura Regional de América del Sur Integración (IIRSA), busca la integración comercial de los departamentos de Madre de Dios, Puno, Cusco, Tacna, Moquegua, Arequipa, Ica, Ayacucho, Apurímac y Huancavelica.

Se estima que la vía permitirá que los productos de los departamentos del sur peruano entren en el mercado de Brasil y que los bienes de Acre y Rondonia en Brasil, y, en última instancia los procedentes de la Amazonía y Mato Grosso, entren en Perú.

La despensa arequipeña sobresale por la variedad de cultivos y productos alimenticios como los rocotos y ajíes, frutas variadas, hortalizas, carne de res, carnero, cuy, cerdo, alpaca, avestruz, variedad de pescados y de gran manera camarones, leche y quesos, vinos y piscos, chicha de maíz, etc. Una de las características peculiares de la comida son los picantes; esto hace que los lugares donde se expenden se llamen picanterías.

La diversidad de esta cocina mestiza se puede resumir en la existencia de caldos o chupes para cada uno de los días, El lunes "chaque"; martes "chairo"; miércoles "pebre"; jueves "timpusca" (en temporada de peras) o "menestrón"; viernes "chupe de camarones"; sábado "tiempo de rabo" o "puchero"; domingo "caldo blanco de lomos".

Es conocida en el país por sus guisos y potajes preparados a fuego de leña y en ollas de barro. Entre los más conocidos se encuentran el chupe de camarones, ocopa arequipeña, rocoto relleno, adobo de chancho, soltero de queso, pastel de papas, cuy chactado, cauche de queso, locro de pecho y el chaque por mencionar algunos. El típico postre arequipeño es el queso helado, y de bebida, además de la chicha de jora, el anís nájar.

La tasa de analfabetismo es del 4,90%, inferior al valor alcanzado a nivel nacional (8,13%), esto no es determinante para la educación superior logros.
En general, un mayor nivel relativo de la educación se percibe en Arequipa en comparación con otras regiones. No obstante, la mayoría de la población solo alcanzó la secundaria completa (22%), seguido por el 17% que muestra primaria incompleta y 15% con secundaria incompleta, mientras que solo el 10% (107 966) de la población presenta un nivel universitario completo, y el 7% tiene educación superior, o educación universitaria aún no completada.


El volcán Misti es imponente, levantado sobre una serena campiña con manantiales, viejos molinos (en Sabandía), baños termales (en Yura y en Socosani), pequeños con callejones tipo andaluz (Yanahuara) y, no muy lejos, una aldea enraizada en un cerro pétreo (Sachaca). El uso de la piedra sillar (mineral volcánico) en la construcción de templos, conventos y casonas le dan a la ciudad un aspecto peculiar. Los lugares más visitados son el valle del Colca y su cañón (que es uno de los más profundos del mundo) y por la permanencia actual de tres culturas vivas: los "Kawanas", "Kollawas" y los "Ccaccatapay", con su música y costumbres milenarios, los petroglifos de Toro Muerto, el valle y cañón de Cotahuasi. Así mismo, las cuevas de Andahua y las cavernas de Socabaya. En la ciudad de Arequipa, el monasterio de Santa Catalina, fundado en 1580.

Otros lugares para visitar son:


Arequipa es uno de los departamentos más deportivos del país. Entre sus deportistas más destacados están los futbolistas, atletas y tenistas, quienes en muchas oportunidades han logrado representar al Perú en competencias internacionales. Además cuenta con gran variedad de equipos de fútbol los cuales tienen gran protagonismo en Copa Perú. Arequipa posee 2 campeonatos Nacionales (ganados por Melgar), 1 Campeonato de Segunda División (Ganado por Total Clean), 2 Campeonatos de promoción y Reserva (ganados por Melgar) y 5 Copas Perú (Ganadas por Melgar, Huracán, Atlético Universidad, Total Clean y Escuela Municipal Binacional).



</doc>
<doc id="9789" url="https://es.wikipedia.org/wiki?curid=9789" title="Síndrome de Alport">
Síndrome de Alport

El síndrome de Alport (también llamado Mal de Alport) es una enfermedad genética, en la que una alteración en la síntesis del colágeno tipo IV afecta los riñones, oídos y ojos causando hipoacusia neurosensorial progresiva ( afectando particularmente los tonos agudos) y trastornos de la vista, incluyendo megalocórnea, lenticono y cataratas. Fue inicialmente identificado por el médico británico Cecil A. Alport en 1927, que describió una familia británica en la que muchos miembros desarrollaban enfermedades renales. Él describió que los hombres afectados en la familia morían a causa de enfermedades renales, mientras que las mujeres estarían menos afectadas.

El síndrome de Alport se caracteriza por tener afección renal, coclear y ocular. La principal señal de este síndrome, es la hematuria microscópica (microhematuria). Los hombres con el síndrome Alport ligado al cromosoma X (XLAS) padecen microhematuria desde una edad muy temprana. Alrededor del 90% de mujeres con XLAS también la tienen. Hay 2 métodos para el diagnóstico clínico: secuenciación y análisis de deleción/duplicación. El análisis de secuenciación de COL4A5 identifica cerca del 80% de las mutaciones de individuos afectados con antecedentes familiares en herencia ligada al X. El análisis de deleción/duplicación del gen COL4A5 identifica deleciones (típicamente multiexónicas) cercanas al 10% de individuos afectados con antecedentes familiares ligada al X.
Estudios recientes hablan sobre el empalme del elemento Alu en los exones los cuales provocan este síndrome.


</doc>
<doc id="9790" url="https://es.wikipedia.org/wiki?curid=9790" title="Samuel Hahnemann">
Samuel Hahnemann

Christian Friedrich Samuel Hahnemann, más conocido como Samuel Hahnemann (Meissen, Alemania, 10 de abril de 1755-París, 2 de julio de 1843), fue un médico sajón, inventor del sistema de medicina alternativa llamado homeopatía.

A Hahnemann también se le atribuye haber introducido la práctica de la cuarentena en el Reino de Prusia durante su servicio al duque de Anhalt-Köthe. 

Hahnemann vivió hasta los veinte años en Meissen, donde aprendió varios idiomas y estudió la cultura clásica. Antes de cumplir los veinticinco años, ya trabajaba como médico privado del gobernador de Transilvania. Fue químico antes que médico. Su suegro era farmacéutico, y Hahnemann fue su aprendiz durante muchos meses. La medicina, tal como existía a finales del siglo XVIII o inicios del siglo XIX, no podía considerarse todavía medicina, sino una amalgama de recetas extrañas e, incluso, extravagantes.

Mientras traducía el Tratado de William Cullen "A Treatise on the Materia Medica", S. Hahnemann descubrió que la corteza del árbol del género Cinchona, era efectiva para el tratamiento del paludismo.

Según su biógrafo Richard Hael, así como el profesor Bradford, su maestro, la lista de las obras químicas antes de 1810, son más o menos 27. Algunas son traducciones, otras creaciones.

Hahnemann murió en París, y sus restos están enterrados en el Cementerio de Père-Lachaise.




</doc>
<doc id="9791" url="https://es.wikipedia.org/wiki?curid=9791" title="Leyes de Newton">
Leyes de Newton

Las leyes de Newton, también conocidas como leyes del movimiento de Newton, son tres principios a partir de los cuales, se explican una gran parte de los problemas planteados en mecánica clásica, en particular aquellos relativos al movimiento de los cuerpos, que revolucionaron los conceptos básicos de la física y el movimiento de los cuerpos en el universo.

En concreto, la relevancia de estas leyes radica en dos aspectos: por un lado constituyen, junto con la transformación de Galileo, la base de la mecánica clásica, y por otro, al combinar estas leyes con la ley de la gravitación universal, se pueden deducir y explicar las leyes de Kepler sobre el movimiento planetario. Así, las leyes de Newton permiten explicar, por ejemplo, tanto el movimiento de los astros como los movimientos de los proyectiles artificiales creados por el ser humano y toda la mecánica de funcionamiento de las máquinas. Su formulación matemática fue publicada por Isaac Newton en 1687 en su obra "Philosophiæ naturalis principia mathematica". 

La dinámica de Newton, también llamada dinámica clásica, solo se cumple en los sistemas de referencia inerciales (que se mueven a velocidad constante; la Tierra, aunque gire y rote, se trata como tal a efectos de muchos experimentos prácticos). Solo es aplicable a cuerpos cuya velocidad dista considerablemente de la velocidad de la luz; cuando la velocidad del cuerpo se va aproximando a los 300 000 km/s (lo que ocurriría en los sistemas de referencia no-inerciales) aparecen una serie de fenómenos denominados efectos relativistas. El estudio de estos efectos (contracción de la longitud, por ejemplo) corresponde a la teoría de la relatividad especial, enunciada por Albert Einstein en 1905.

La dinámica es la parte de la física que estudia las relaciones por los movimientos de los cuerpos y las causas que los provocan, en concreto las fuerzas que actúan sobre ellos. La dinámica, desde el punto de vista de la mecánica clásica, es apropiada para el estudio dinámico de sistemas grandes en comparación con los átomos y que se mueven a velocidades mucho menores que las de la luz. Para entender estos fenómenos, el punto de partida es la observación del mundo cotidiano. Si se desea cambiar la posición de un cuerpo en reposo es necesario empujarlo o levantarlo, es decir, ejercer una acción sobre él.

Aparte de estas intuiciones básicas, el problema del movimiento es muy complejo: todos aquellos que se observan en la naturaleza (caída de un objeto en el aire, movimiento de una bicicleta, un coche o un cohete espacial) son complicados. Esto motivó que el conocimiento sobre estos hechos fuera erróneo durante siglos. Aristóteles pensó que el movimiento de un cuerpo se detiene cuando la fuerza que lo empuja deja de actuar. Posteriormente se descubrió que esto no era cierto pero el prestigio de Aristóteles como filósofo y científico hizo que estas ideas perduraran siglos, hasta que científicos como Galileo Galilei o Isaac Newton hicieron avances muy importantes con sus nuevas formulaciones. Sin embargo hubo varios físicos que se aproximaron de manera muy certera a las formulaciones de Newton mucho antes de que este formulara sus leyes del movimiento.

Es el caso del español Juan de Celaya, matemático, físico, cosmólogo, teólogo y filósofo que en 1517 publicó un tratado titulado "In octo libros physicorum Aristotelis cum quaestionibus eiusdem, secundum triplicem viam beati Thomae, realium et nominatium", obra de especial interés para el estudio de los orígenes de la moderna ciencia del movimiento. Durante su etapa en Francia fue un escritor prolífico, escribiendo sobre todo acerca de la física de Aristóteles y el movimiento. También publicó numerosos trabajos sobre filosofía y lógica. Fue uno de los impulsores de la lógica nominalista y de las ideas mertonianas de los calculatores acerca de la dinámica. Fue capaz de enunciar, dentro de las leyes de Newton, la primera ley de o primer principio de la dinámica (una de las leyes más importantes de la física) un siglo antes que Newton.

Otro destacado pionero fue el también español, y discípulo de Celaya, Domingo de Soto, fraile dominico y teólogo considerado como el promotor de la física moderna. Su teoría del movimiento uniformemente acelerado y la caída de los graves fue el precedente de la ley de la gravedad de Newton. Escribió numerosas obras de teología, derecho, filosofía y lógica y también comentó varios libros de física y lógica aristotélica, de los cuales el más importante fue "Quaestiones super octo libros physicorum Aristotelis" (1551), sobre cinemática y dinámica, la cual fue publicada en varias ciudades italianas, influyendo en personajes como Benedetti o Galileo. Domingo de Soto fue uno de los primeros en establecer que un cuerpo en caída libre sufre una aceleración uniforme con respecto al tiempo —dicha afirmación también había sido establecida por Nicolás Oresme casi dos siglos antes— y su concepción sobre la masa fue avanzada en su época. En su libro "Quaestiones" explica la aceleración constante de un cuerpo en caída libre de esta manera: 

Domingo de Soto ya relacionaba dos aspectos de la física: el movimiento uniformemente disforme (movimiento uniformemente acelerado) y la caída de graves (resistencia interna). En su teoría combinaba la abstracción matemática con la realidad física, clave para la comprensión de las leyes de la naturaleza. Tenía una claridad rotunda acerca de este hecho y lo expresaba en ejemplos numéricos concretos. Clasificó los diferentes tipos de movimiento en:




Soto describió el movimiento de caída libre como ejemplo de movimiento uniformemente acelerado por primera vez, cuestión que solo aparecerá posteriormente en la obra de Galileo:

Por lo tanto era aplicable la ley de la velocidad media para calcular el tiempo de caída:

Movimiento diformente disforme con respecto al tiempo:
Este fue un descubrimiento clave en física y base esencial para el posterior estudio de la gravedad por Galileo Galilei e Isaac Newton. Ningún científico de las universidades de París y Oxford de aquella época había conseguido describir la relación entre movimiento uniformemente disforme en el tiempo y la caída de los graves como lo hizo Soto.

Tras las ideas innovadoras sobre el movimiento de estos científicos, Galileo hizo un avance muy importante al introducir el método científico que enseña que no siempre se debe creer en las conclusiones intuitivas basadas en la observación inmediata, pues esto lleva a menudo a equivocaciones. Galileo realizó un gran número de experiencias en las que se iban cambiando ligeramente las condiciones del problema y midió los resultados en cada caso. De esta manera pudo extrapolar sus observaciones hasta llegar a entender un experimento ideal. En concreto, observó cómo un cuerpo que se mueve con velocidad constante sobre una superficie lisa se moverá eternamente si no hay rozamientos ni otras acciones externas sobre él.

Inmediatamente se presentó otro problema: ¿si la velocidad no lo revela, qué parámetro del movimiento indica la acción de fuerzas exteriores?; Galileo respondió también a esta pregunta, pero Newton lo hizo de manera más precisa: no es la velocidad sino su variación la consecuencia resultante de la acción de arrastrar o empujar un objeto. Esta relación entre fuerza y cambio de velocidad (aceleración) constituye la base fundamental de la mecánica clásica. Fue Isaac Newton (hacia 1690) el primero en dar una formulación completa de las leyes de la mecánica e inventó los procedimientos matemáticos necesarios para explicarlos y obtener información a partir de ellos.

El primer concepto que maneja Newton es el de masa, que identifica con «cantidad de materia». Newton asume a continuación que la cantidad de movimiento es el resultado del producto de la masa por la velocidad. En tercer lugar, precisa la importancia de distinguir entre lo absoluto y relativo siempre que se hable de tiempo, espacio, lugar o movimiento.

En este sentido, Newton, que entiende el movimiento como una traslación de un cuerpo de un lugar a otro, para llegar al movimiento absoluto y verdadero de un cuerpo:

De acuerdo con este planteamiento, establece que los movimientos aparentes son las diferencias de los movimientos verdaderos y que las fuerzas son causas y efectos de estos. Consecuentemente, la fuerza en Newton tiene un carácter absoluto, no relativo.

Las leyes enunciadas por Newton, y consideradas como las más importantes de la mecánica clásica, son tres: la ley de inercia, la relación entre fuerza y aceleración y la ley de acción y reacción. Newton planteó que todos los movimientos se atienen a estas tres leyes principales, formuladas en términos matemáticos. Un concepto es la fuerza, causa del movimiento y otro es la masa, la medición de la cantidad de materia puesta en movimiento; los dos son denominados habitualmente por las letras F y m.

La primera ley del movimiento rebate la idea aristotélica de que un cuerpo solo puede mantenerse en movimiento si se le aplica una fuerza. Newton expone que:
Esta ley postula, por tanto, que un cuerpo no puede cambiar por sí solo su estado inicial, ya sea en reposo o en movimiento rectilíneo uniforme, a menos que se aplique una fuerza o una serie de fuerzas cuya resultante no sea nula. Newton toma en consideración, así, el que los cuerpos en movimiento están sometidos constantemente a fuerzas de roce o fricción, que los frena de forma progresiva, algo novedoso respecto de concepciones anteriores que entendían que el movimiento o la detención de un cuerpo se debía exclusivamente a si se ejercía sobre ellos una fuerza, pero nunca entendiendo como tal a la fricción.

En consecuencia, un cuerpo que se desplaza con movimiento rectilíneo uniforme implica que no existe ninguna fuerza externa neta o, dicho de otra forma, un objeto en movimiento no se detiene de forma natural si no se aplica una fuerza sobre él. En el caso de los cuerpos en reposo, se entiende que su velocidad es cero, por lo que si esta cambia es porque sobre ese cuerpo se ha ejercido una fuerza neta.

Newton retomó la ley de la inercia de Galileo: la tendencia de un objeto en movimiento a continuar moviéndose en una línea recta, a menos que sufra la influencia de algo que le desvíe de su camino. Newton supuso que si la Luna no salía disparada en línea recta, según una línea tangencial a su órbita, se debía a la presencia de otra fuerza que la empujaba en dirección a la Tierra, y que desviaba constantemente su camino convirtiéndolo en un círculo. Newton llamó a esta fuerza gravedad y creyó que actuaba a distancia. No hay nada que conecte físicamente la Tierra y la Luna y sin embargo la Tierra está constantemente tirando de la Luna hacia nosotros. Newton se sirvió de la tercera ley de Kepler y dedujo matemáticamente la naturaleza de la fuerza de la gravedad. Demostró que la misma fuerza que hacía caer una manzana sobre la Tierra mantenía a la Luna en su órbita.

La primera ley de Newton establece la equivalencia entre el estado de reposo y de movimiento rectilíneo uniforme. Supongamos un sistema de referencia "S" y otro "S"´ que se desplaza respecto del primero a una velocidad constante. Si sobre una partícula en reposo en el sistema "S"´ no actúa una fuerza neta, su estado de movimiento no cambiará y permanecerá en reposo respecto del sistema "S"´ y con movimiento rectilíneo uniforme respecto del sistema "S". La primera ley de Newton se satisface en ambos sistemas de referencia. A estos sistemas en los que se satisfacen las leyes de Newton se les da el nombre de sistemas de referencia inerciales. Ningún sistema de referencia inercial tiene preferencia sobre otro sistema inercial, son equivalentes: este concepto constituye el principio de relatividad de Galileo o newtoniano.

El enunciado fundamental que podemos extraer de la ley de Newton es que 

Esta expresión es una ecuación vectorial, ya que las fuerzas llevan dirección y sentido. Por otra parte, cabe destacar que la variación con la que varía la velocidad corresponde a la aceleración.

La primera ley de Newton sirve para definir un tipo especial de sistemas de referencia conocidos como sistemas de referencia inerciales, que son aquellos desde los que se observa que un cuerpo sobre el que no actúa ninguna fuerza neta, se mueve con velocidad constante.

Un sistema de referencia con aceleración (y la aceleración normal de un sistema rotatorio se incluye en esta definición) no es un sistema inercial, y la observación de una partícula en reposo en el propio sistema no satisfará las leyes de Newton (puesto que se observará aceleración sin la presencia de fuerza neta alguna). Se denominan sistemas de referencia no inerciales.

Por ejemplo considérese una plataforma girando con velocidad constante, ω, en la que un objeto está atado al eje de giro mediante una cuerda, y supongamos dos observadores, uno inercial externo a la plataforma y otro no inercial situado sobre ella.



En realidad, es imposible encontrar un sistema de referencia inercial, ya que siempre hay algún tipo de fuerzas actuando sobre los cuerpos; no obstante, siempre es posible encontrar un sistema de referencia en el que el problema que estemos estudiando se pueda tratar como si estuviésemos en un sistema inercial. En muchos casos, la Tierra es una buena aproximación de sistema inercial, ya que a pesar de contar con una aceleración traslacional y otra rotacional, ambas son del orden de 0.01 m/s² y, en consecuencia, podemos considerar que un sistema de referencia de un observador en la superficie terrestre es un sistema de referencia inercial.

Se puede considerar como ejemplo ilustrativo de esta primera ley o ley de la inercia una bola atada a una cuerda, de modo que la bola gira siguiendo una trayectoria circular. Debido a la fuerza centrípeta de la cuerda (tensión), la masa sigue la trayectoria circular, pero si en algún momento la cuerda se rompiese, la bola tomaría una trayectoria rectilínea en la dirección de la velocidad que tenía la bola en el instante de rotura.

Tras la rotura, la fuerza neta ejercida sobre la bola es 0, por lo que experimentará, como resultado de un estado de reposo, un movimiento rectilíneo uniforme.

La segunda ley de Newton expresa que:
Esta ley se encarga de cuantificar el concepto de fuerza. La aceleración que adquiere un cuerpo es proporcional a la fuerza neta aplicada sobre el mismo. La constante de proporcionalidad es la masa del cuerpo (que puede ser o no ser constante). Entender la fuerza como la causa del cambio de movimiento y la proporcionalidad entre la fuerza impresa y el cambio de la velocidad de un cuerpo es la esencia de esta segunda ley.

Si la masa del cuerpo es constante se puede establecer la siguiente relación, que constituye la ecuación fundamental de la dinámica:

Donde "m" es la masa del cuerpo la cual debe ser constante para ser expresada de tal forma. La fuerza neta que actúa sobre un cuerpo, también llamada fuerza resultante, es el vector suma de todas las fuerzas que sobre él actúan. Así pues:

El principio de superposición establece que si varias fuerzas actúan igual o simultáneamente sobre un cuerpo, la fuerza resultante es igual a la suma vectorial de las fuerzas que actúan independientemente sobre el cuerpo (regla del paralelogramo). Este principio aparece incluido en los "Principia" de Newton como Corolario 1, después de la tercera ley, pero es requisito indispensable para la comprensión y aplicación de las leyes, así como para la caracterización vectorial de las fuerzas.
La fuerza modificará el estado de movimiento, cambiando la velocidad en módulo o dirección. Las fuerzas son causas que producen aceleraciones en los cuerpos. Por lo tanto existe una relación causa-efecto entre la fuerza aplicada y la aceleración que este cuerpo experimenta. 

De esta ecuación se obtiene la unidad de medida de la fuerza en el Sistema Internacional de Unidades, el Newton:

</math>. Es decir, es una magnitud vectorial proporcional a la masa y a la velocidad del objeto. Partiendo de esta definición y aplicando la ley fundamental de la mecánica de Newton, las variaciones de la cantidad de movimiento se expresan en función de la fuerza resultante y el intervalo de tiempo durante el cual se ejerce esta:

Tomando el intervalo de tiempo de "t" a "t" e integrando se obtiene

Al vector I se le denomina impulso lineal y representa una magnitud física que se manifiesta especialmente en las acciones rápidas o impactos, tales como choques, llevando módulo dirección y sentido. En este tipo de acciones conviene considerar la duración del impacto y la fuerza ejercida durante el mismo.

De la expresión obtenida se deduce que el impulso lineal es igual a la variación de la cantidad de movimiento. Si la fuerza resultante es cero (es decir, si no se actúa sobre el objeto) el impulso también es cero y la cantidad de movimiento permanece constante. Llamamos a esta afirmación ley de conservación del impulso lineal, aplicada a un objeto o una partícula.

Sus unidades en el Sistema Internacional son formula_4



Entre las posibles aplicaciones de la Segunda Ley de Newton, se pueden destacar:



Si se aplica la segunda ley, en la dirección radial:

donde "a" representa la aceleración normal a la trayectoria. Conocido el valor de la velocidad "v" en la posición angular se puede determinar la tensión "T" del hilo. Esta es máxima cuando el péndulo pasa por la posición de equilibrio
}</math>
</math>


_{12})}{d^2} </math>
donde "d" la distancia entre las dos partículas y formula_7 es el vector director unitario que va de la partícula 1 a la 2. Análogamente, la fuerza de la partícula 2 sobre la partícula 1 es:
_{12}) )}{d^2} </math>
Empleando la identidad vectorial formula_8, puede verse que la primera fuerza está en el plano formado por formula_7 y formula_10 que la segunda fuerza está en el plano formado por formula_7 y formula_12. Por tanto, estas fuerzas no siempre resultan estar sobre la misma línea, ni en general son de igual magnitud 

El teorema de Ehrenfest permite generalizar las leyes de Newton al marco de la mecánica cuántica. Si bien en dicha teoría no es lícito hablar de fuerzas o de trayectoria, se puede hablar de magnitudes como momento lineal y potencial de manera similar a como se hace en mecánica newtoniana.

En concreto la versión cuántica de la segunda Ley de Newton afirma que la derivada temporal del valor esperado del momento de una partícula en un campo iguala al valor esperado de la "fuerza" o valor esperado del gradiente del potencial:
Donde:





</doc>
<doc id="9797" url="https://es.wikipedia.org/wiki?curid=9797" title="Jaime Nunó">
Jaime Nunó

Jaime Nonó Roca (en catalán "Jaume Nunó i Roca"; San Juan de las Abadesas, Gerona, Cataluña, España, 7 de septiembre de 1824 - Nueva York, Estados Unidos, 18 de julio de 1908), más conocido como Jaime Nunó, fue un compositor, concertista, director de orquesta y director de óperas español, célebre por haber musicalizado las estrofas escritas por Francisco González Bocanegra para dar origen al Himno Nacional Mexicano.

Tras la muerte de sus padres, quedó bajo la tutela de su tío Bernardo, un comerciante de sedas de Barcelona que financió sus estudios musicales en la ciudad condal. Ahí demostró sus actitudes como solista en la catedral de la ciudad, tras lo cual se ganó una beca para estudiar con el compositor Saverio Mercadante en Italia contando con 17 años de edad.

La pieza más temprana compuesta por Jaime Nunó es "Trisagio" de 1839 para coro mixto, Soprano, Alto, Tenor, Bajo y Soprano Solista con acompañamiento de órgano o piano. El compositor tenía 15 años cuando lo compuso y se advierte un estilo italiano muy incipiente. Trisagio

Se casó con Dolores, viuda de Taló, en 1848, de quien tuvo una hija llamada también Dolores. Compuso misas, arias, motetes y piezas orquestales. Durante esa época, dirigió orquestas e impartió lecciones, y se fue especializando en bandas militares.

Es recordado especialmente como el creador del Himno Nacional Mexicano. Aunque ni nació ni falleció en México, permaneció en diversas ocasiones en dicho país y estuvo estrechamente vinculado a figuras políticas nacionales y episodios decisivos para el curso de la historia mexicana. A su regreso a Barcelona, fue nombrado director de la Banda del Regimiento de la Reina en 1851, y viajó con ellos a Cuba, donde conoció y trabó amistad con el ex presidente mexicano Antonio López de Santa Anna. Santa Anna regresó a México para ocupar de nuevo la presidencia, y en 1853 invitó a Jaime Nunó a encabezar las bandas militares mexicanas. Su llegada coincidió con la convocatoria al concurso nacional para componer el Himno Nacional Mexicano, en el cual se inscribieron 26 aspirantes. En 1854, ganó el concurso convocado para componer la música del Himno Nacional, cuya letra había escrito el poeta mexicano Francisco González Bocanegra, y el 12 de agosto de ese año fue declarado triunfador. La partitura, ya con letra y música, se interpretó por primera vez el 16 de septiembre de ese mismo año, en el entonces llamado Teatro Santa Anna (luego llamado Teatro Nacional de México), que finalmente se demolió y se reemplazó por el Palacio de Bellas Artes.

La autoría de la música del Himno Nacional le convirtió en un prócer patrio de la historia mexicana, por lo que posteriormente, en 1942, sus restos mortales se llevaron a México y se depositaron en la Rotonda de los Hombres Ilustres (hoy Rotonda de las Personas Ilustres), un monumental panteón nacional de la Ciudad de México en el que se perpetúa la memoria de personajes ilustres mexicanos, al tiempo que se les rinden honores póstumos.

Tras la caída del presidente Santa Anna por la Revolución de Ayutla, Nunó decidió emigrar a los Estados Unidos, donde trabajó como concertista y director de óperas, una de las cuales lo llevó de gira por el continente americano en 1864. En 1873, se volvió a casar, esta vez con su discípula Catalina Cecilia Remington, con quien tuvo dos hijos: Cristina y Jaime.

Tras radicar un tiempo en España regresó a los Estados Unidos para establecerse en el estado de Nueva York, donde fue redescubierto por un periodista mexicano en 1901. Al conocerse la noticia en México, el entonces presidente Porfirio Díaz lo invitó a México, donde recibió varios homenajes entre 1901 y 1904. De esta manera, participó en las celebraciones del cincuentenario del Himno Nacional Mexicano.

Murió en Nueva York el 18 de julio de 1908. En octubre de 1942, el gobierno mexicano mandó exhumar sus restos para trasladarlos a la Rotonda de las Personas Ilustres ubicada en el "Panteón Civil de Dolores" de la Ciudad de México, donde aún reposan junto con los restos de Francisco González Bocanegra.

En el 2010, justo en la celebración del Bicentenario de la Independencia de México, los musicólogos catalanes Cristian Canton Ferrer y Raquel Tovar localizaron en los Estados Unidos al único descendiente directo de Jaime Nunó, su bisnieto, en Pelham, Nueva York. Este hallazgo permitió recuperar el fondo personal de Jaime Nunó, que incluía cerca de cinco mil documentos inéditos (cartas personales, partituras, documentos oficiales, etcétera), que llevó a Cantón y a Tovar a confeccionar su primera biografía completa, de gran repercusión mediática y descrita como ""un título fundamental para comprender la historia musical de México"". También en el contexto del redescubrimiento de la figura de Jaime Nunó, se reinauguró la casa-museo natal del compositor en San Juan de las Abadesas, conocida como "El Palmàs". A partir de la recuperación de esta documentación, la música inédita de Jaime Nunó Roca ha vuelto a interpretarse, y está en proceso una grabación y la edición completa de sus obras, de la mano del sello catalano-británico Mozaic Editions. De este proyecto de recuperación de la obra de Nunó, ya se ha publicado el "Trisagio para coro y piano", de 1839, obra que, aunque se escribió quince años atrás, recuerda los acordes del Himno Nacional Mexicano.

La producción musical de Jaime Nunó abarcó casi todos los géneros, y se sabe que compuso alrededor de 500 obras, de las que solo se han conservado un reducido número. El musicólogo catalán Cristian Canton Ferrer realizó, en el 2012, la primera catalogación rigurosa de la obra de Nunó, siguiendo la metodología grupal empleada anteriormente por Anthony van Hoboken en el catálogo de las obras de Franz Joseph Haydn. Habitualmente, las obras de Nunó son citadas según el catálogo Canton, y figura el grupo y el orden de la obra prefijado por la letra "C"; por ejemplo: ""Pequeña pieza de concierto", C.I/1".




</doc>
<doc id="9800" url="https://es.wikipedia.org/wiki?curid=9800" title="Diexismo">
Diexismo

Diexismo es la afición de escuchar emisoras de radio lejanas o exóticas. El nombre proviene del término telegráfico "DX", que significa distancia. El radioyente es el "diexista".

En el caso de tratarse de radioaficionados se entenderá que la comunicación se establece entre puntos geográficamente alejados. Cuando un radioaficionado transmite las palabras "CQ DX, CQ DX, CQ DX...", es que está buscando un contacto lejano y para colaborar, las emisoras cercanas deben abstenerse de contestar la llamada. En algunos países, los radioaficionados suelen llamar a los diexistas (DXers), "curuyas".

El DX se puede practicar en todas las bandas de frecuencia y clases de emisión. 

El DX en bandas altas como la VHF y la UHF y hasta la SHF también es posible, generalmente con alcances más modestos, del orden de algunos a varios cientos de kilómetros, aunque dicho alcance puede aumentar notablemente con técnicas como el "rebote lunar" o la "dispersión meteórica", o, con el empleo de antenas altamente directivas.

Más allá del equipamiento mínimo necesario y de la calidad del mismo, el éxito dependerá en gran medida de las condiciones de propagación (el número de manchas solares es uno de los elementos más influyentes), la frecuencia elegida, la hora del día o la estación del año, que permiten o no que las ondas de radio lleguen hasta el lugar donde estamos y de la eficacia de la antena receptora.

El equipamiento básico puede constar de un simple receptor de radio portátil. Es preferible que el sintonizador esté basado en la tecnología PLL para obtener una buena estabilidad en frecuencia y que el dial sea digital para poder determinar con precisión la frecuencia que se está sintonizando. Aunque aparatos antiguos con tecnología de válvulas también dan muy buen resultado, a veces incluso mejor que los más modernos.

Las autoridades francesas (en 2008, la ANFR) no confieren indicativos a los diexistas. Sin embargo, la asociación de radioaficionados francesa REF-Union confiere gratuitamente indicativos de la forma F-nnnnn (donde n es un dígito) a quienes los solicitan. Si bien no son indicativos oficiales, son comúnmente aceptados como válidos por la comunidad de radioaficionados.


Aunque menos frecuente también es posible hacer diexismo de televisión y de emisoras utilitarias como por ejemplo aeropuertos, aviones, estaciones costeras, meteorológicas, barcos y radiobalizas. 

La escucha de frecuencias profesionales puede constituir delito en distintos países. Existen casos en los cuales hasta la mera posesión de equipos de escucha capaces de barrer ciertas frecuencias es considerado delito. Es recomendable informarse sobre las reglas en vigor en el país donde se practica la escucha. 

Muchos diexistas toman su pasatiempo con mucha seriedad y lo convierten en un verdadero desafío. Se suele comenzar con emisoras conocidas que sirven de práctica y como punto de referencia para luego sintonizar emisoras cada vez más débiles y lejanas. Los conocimientos adquiridos por los diexistas han servido en muchos casos para solucionar problemas de recepción y transmisión en las comunicaciones.

Una vez efectuada la escucha, es usual que el diexista proceda a confeccionar un "informe de recepción" que, mediante el código SINPO (u otros, aunque éste es el más popular), envía a la emisora para hacer saber a ésta la calidad con que ha recibido su emisión. La emisora, en agradecimiento, suele contestar remitiendo una tarjeta QSL al oyente, que éste añadirá a su colección de "trofeos", valorados en función de la rareza del emisor.

El informe de recepción debe indicar día, hora de inicio y fin de la escucha, frecuencia de sintonía, situación geográfica del radioescucha, equipo receptor y antena empleados y un pequeño resumen de la escucha. Para evaluar técnicamente las características de la recepción, se emplea frecuentemente el código SINPO, evaluándose en una escala de 1 a 5 los siguientes aspectos:


Un SINPO 55555 equivale a una recepción perfecta.

Para reportar estaciones de CW (telegrafía) se suele usar el código RST.

Algunas emisoras que aceptan informes de recepción vía Internet:



</doc>
<doc id="9809" url="https://es.wikipedia.org/wiki?curid=9809" title="Ricardo Lagos">
Ricardo Lagos

Ricardo Froilán Lagos Escobar (Santiago, 2 de marzo de 1938) es un abogado, economista y político chileno. Fue presidente de la República de Chile entre el 11 de marzo de 2000 y el 11 de marzo de 2006. Ha sido también enviado especial de la Organización de las Naciones Unidas para tratar el cambio climático.

Ha sido una de las principales figuras de la Concertación de Partidos por la Democracia dado su carácter protagónico en ésta. Como miembro fundador del Partido por la Democracia, fue una de las principales figuras opositoras a la dictadura militar de Augusto Pinochet. Una vez logrado el retorno a la democracia, Lagos ejerció como ministro de Educación y de Obras Públicas durante las presidencias de Patricio Aylwin y Eduardo Frei Ruiz-Tagle.

En las elecciones presidenciales de 1999 ganó por un estrecho margen al candidato de la UDI Joaquín Lavín, siendo el primer ganador de una segunda vuelta en la historia electoral chilena. Su mandato presidencial comenzó con grandes problemas por los efectos económicos de la crisis asiática y diversos problemas de corrupción. Sin embargo, en la segunda mitad de su mandato, el crecimiento económico, la firma de tratados de libre comercio con Estados Unidos, China y la Unión Europea, entre otros, y los importantes avances en infraestructura, permitieron un importante repunte en su popularidad. En 2005, Lagos logró un acuerdo para la reforma de la Constitución Política chilena que data de 1980.

Entre los puntos polémicos de su gobierno está la respuesta dada por el Estado chileno a las reivindicaciones territoriales y de autodeterminación de los pueblos indígenas mapuche, su actuación en problemas de índole ambiental, el caso de corrupción Caso MOP-GATE, la privatización del agua y las carreteras, el financiamiento de la revista Siete + 7 con fondos reservados de la presidencia, la preparación de ciertas reformas que se implementaron en el gobierno siguiente de Michelle Bachelet como el Transantiago, y la instauración del Crédito con Aval del Estado (CAE).

Nació el 2 de marzo de 1938 en Santiago. Sus padres fueron Ema Escobar Morales –que murió en abril de 2005 a la edad de 108 años– y el agricultor Froilán Lagos Sepúlveda, quien falleció cuando Lagos tenía ocho años. Cursó los estudios básicos en el Colegio San Agustín y en el Liceo Manuel de Salas y los secundarios en el Instituto Nacional, al que ingresó gracias a los buenos oficios de su tía, Fresia Escobar Morales, una de las pioneras en el mundo político chileno.

En 1954, Ricardo Lagos ingresa a la Facultad de Derecho de la Universidad de Chile. Entre 1955 y 1959, da sus primeros pasos en política. Es elegido Presidente del Centro de Alumnos y pronuncia su primer discurso desde el mismo lugar donde minutos antes lo hiciera Salvador Allende ante el expresidente de Guatemala, Juan José Arévalo.

En 1960, Lagos terminó su carrera de Derecho. Su memoria de título, "La Concentración del Poder Económico", fue aprobada con distinción máxima y se convirtió en un éxito editorial, con cinco ediciones publicadas. En ella, demostró la existencia de grupos económicos, sugiriendo para terminar con ellos "la abolición de la propiedad privada de todos los medios de producción" y su paso al Estado. Se casó con Carmen Weber, con quien tuvo dos hijos, Ricardo y Ximena. Luego de doctorarse y ya de vuelta en Chile, anuló su matrimonio.

En 1969 conoció a Luisa Durán de la Fuente, con quien se casó en segundas nupcias en 1971. Con Luisa compartió la crianza de los dos hijos del primer matrimonio de Lagos y de los dos hijos del primer matrimonio de Luisa, Hernán y Alejandro, y de Francisca, única hija común del matrimonio Lagos - Durán.

Obtuvo un doctorado en la Universidad de Duke, donde estudió entre 1961 y 1966. Entre 1962 y 1965; fue profesor de ciencias políticas en la Universidad de Carolina del Norte en Chapel Hill. De vuelta en Chile, se incorporó al Instituto de Economía de la Universidad de Chile, que dirigía Carlos Massad. Más tarde, en 1967, fue nombrado Director de la Escuela de Ciencias Políticas y Administrativas (hoy Escuela de Gobierno y Gestión Pública), cargo que ejerció hasta 1969, cuando ocupó el cargo de Secretario General de la Universidad de Chile. Recibió un doctorado honoris causa por parte de la Universidad Nacional Autónoma de México en abril del 2007.

Comienzo a desempeñarse como profesor de economía en la Escuela de Derecho de la Universidad de Chile. Entre 1971 y 1972, fue director del Instituto de Economía de la misma universidad; luego, fue nombrado director del Consejo Latinoamericano de Ciencias Sociales y profesor visitante de la cátedra William R. Kenan de Estudios Latinoamericanos de la Universidad de Carolina del Norte Chapel Hill, en Estados Unidos.

Se declaró durante los años 1970 como un "independiente de izquierda" , luego que en 1961 abandonara el Partido Radical de Chile, cuando este decide entrar al gobierno de Jorge Alessandri Rodríguez. Sin mayor experiencia diplomática, trabajó con el embajador Hernán Santa Cruz como delegado ante Naciones Unidas, donde pronuncia un destacado discurso sobre la crisis financiera internacional. En aquella ocasión, critica duramente la decisión del Presidente Richard Nixon de decretar la no convertibilidad de los dólares al oro, medida que terminaría por rondar la crisis asiática. En 1972, el presidente Salvador Allende lo nombra embajador en Moscú, pero el Senado no alcanzó a ratificarlo antes del golpe de Estado. Como director regional del Programa de Estudios de Postgrado en Ciencias Sociales, estuvo a cargo del Proyecto Unesco, UNDP en Buenos Aires. En lo público, sirvió al país en Naciones Unidas como delegado con rango de embajador en la XXVI Asamblea General. Además, fue delegado en la III Conferencia de Comercio y Desarrollo (UNCTAD) de la ONU.

Luego del golpe de Estado de 1973 partió al exilio político junto a su familia a Buenos Aires, donde ejerció el cargo de Secretario General de la Facultad Latinoamericana de Ciencias Sociales, Flacso. Se trasladó por un año a Estados Unidos, donde trabajó como profesor visitante de la cátedra William R. Kenan de Estudios Latinoamericanos de la Universidad de Carolina del Norte, Chapel Hill. En 1975 ejerció como consultor para el Programa de las Naciones Unidas para el Desarrollo, PNUD.

Regresó a Chile en 1978 en plena dictadura, contratado por el Programa Regional de Empleo de Naciones Unidas, PREALC. En medio de las políticas impuestas por el Fondo Monetario Internacional, su misión fue asesorar a todos los gobiernos del continente en materia de empleo. Además es representante en Chile del Servicio Universitario Mundial, WUS.

Durante la década de 1980, asumió un papel fundamental en la lucha por la recuperación de la democracia. El año 1983, decidió abandonar su cargo de funcionario internacional en Naciones Unidas. En diciembre de ese mismo año, fue nombrado presidente de la Alianza Democrática (AD), coalición opositora a la dictadura militar integrada por demócrata cristianos, socialistas, radicales, socialdemócratas y derechistas moderados. Siendo uno de los líderes del Partido Socialista de Chile, integró la tendencia denominada los «"suizos"», la cual pretendió ser el nexo de unidad entre las distintas corrientes del socialismo chileno, dividido en «"renovados"» –moderados, participantes en la AD– y «"almeydistas"» –seguidores de Clodomiro Almeyda, más izquierdista, agrupados en el Movimiento Democrático Popular–. 

El 7 de septiembre de 1986, fue detenido por la Policía de Investigaciones de Chile en las pesquisas luego del atentado contra Augusto Pinochet. Este hecho, según declaraciones del vocero de la dictadura militar Francisco Javier Cuadra, salvó a Lagos de morir asesinado junto con el periodista José Carrasco y otros cuatro opositores al régimen de Pinochet.

En 1987, como presidente del Comité de Izquierda por las elecciones libres, llama a la ciudadanía y a los partidos a inscribirse masivamente en los registros electorales a votar por la alternativa «No» en el plebiscito de octubre de 1988. Consecuente con esta convocatoria, es el líder fundador del Partido Por la Democracia (PPD), partido que originalmente se define como "instrumental", cuyo objetivo era realizar la campaña a favor de la opción «No» y contar con una red de apoderados que defendieran esta votación.

Ricardo Lagos se transformó en el líder indiscutido de los opositores del régimen de Pinochet, cuando participa el 25 de abril de 1988 en el programa político "De cara al país" de Canal 13, el que realizó un ciclo con los principales dirigentes de los partidos políticos legalmente inscritos, durante el capítulo correspondiente al Partido por la Democracia, al que asiste como su Presidente, en una actitud valiente para esos tiempos señala que el triunfo del «No» será «el inicio del fin de la dictadura» e «impedirá que el general Pinochet esté 25 años en el poder». Lagos mira a la cámara y levanta su índice para decirle directamente a todos los televidentes:

Tras el triunfo de la opción «No» y pese a su importante liderazgo en la oposición, pierde al interior de la Concertación la opción de ser candidato presidencial, ante el más moderado precandidato demócratacristiano Patricio Aylwin y postulando a una senaturía por la circunscripción Santiago Poniente. El 11 de diciembre de 1989, día de las elecciones, alcanza la segunda mayoría de la circunscripción. Sin embargo, no resulta elegido, pues su lista no alcanza a duplicar la votación de segunda lista más votada, requisito que impone el sistema electoral chileno.

El año 1990, el presidente Patricio Aylwin lo nombra ministro de Educación. Lagos inicia una reforma tendiente a lograr una mayor igualdad en el acceso y un alza en la calidad de la educación pública, intentando recuperar una educación pública que había sido sistemáticamente disminuida y desarticulada durante el gobierno anterior.

En junio de 1993, es uno de los candidatos en la primera elección primaria presidencial de la Historia de Chile, celebrada en la Concertación de Partidos por la Democracia. En esa ocasión fue claramente derrotado, quedando fuera de la elección presidencial, por Eduardo Frei Ruiz-Tagle (del Partido Demócrata Cristiano), quien es elegido finalmente como presidente.
En 1994, el mismo Presidente Eduardo Frei Ruiz-Tagle lo nombra ministro de Obras Públicas, perfeccionando en este cargo el sistema de concesiones viales creado bajo el gobierno de Patricio Aylwin, integrando al sector privado en la construcción de las obras públicas y su posterior explotación.
Durante la presidencia de Eduardo Frei Ruiz-Tagle, continúa siendo líder de opinión y carta segura para la siguiente elección presidencial, lo que se ve ratificado por su nombramiento como uno de los integrantes del Comité de Doce Miembros Distinguidos de la Internacional Socialista, donde comparte con personalidades como Felipe González y Gro Harlem Brundtland. El comité tiene a su cargo la elaboración de propuestas para la renovación del pensamiento socialdemócrata para el siglo XXI.

En 1998 deja su labor de ministro de Obras Públicas para iniciar su campaña presidencial. Gana las primarias dentro de la Concertación al senador Andrés Zaldívar, del Partido Demócrata Cristiano, siendo proclamado candidato. Ésta fue la primera vez que en Chile se realizaron primarias abiertas, esto es, una elección en que podía votar cualquier ciudadano salvo que estuviese inscrito en partidos ajenos a la coalición.

En la elección presidencial de diciembre del mismo año superó a Joaquín Lavín (candidato de la Unión Demócrata Independiente), su contrincante más cercano, por treinta mil votos, lo que equivale a un voto por mesa. Al no haber logrado la mayoría absoluta, en enero de 2000, se realiza por primera vez en Chile la segunda vuelta electoral, para la cual Lagos reestructuró su comando de campaña, integrando al mismo a la Ministra de Justicia, Soledad Alvear (líder del Partido Demócrata Cristiano), para así asegurar el voto humanista cristiano. Finalmente, derrota al candidato de la derecha, Joaquín Lavín, con el 51,3% de los votos, convirtiéndose en el nuevo Presidente de la República de Chile, asumiendo el 11 de marzo de 2000. Es el único Presidente de la República que ha sido militante del Partido por la Democracia.

Durante el primer año de su mandato, debió hacer frente a un alto nivel de desempleo, generado por la inestabilidad política de la región, proceso que comenzó a revertirse durante finales de 2003. Pese a ello, Lagos ostentó un gran apoyo popular, que llegó a su punto máximo en los primeros meses de 2005, donde según diversas encuestas de opinión, su gobierno alcanzaba niveles superiores al 70% de aprobación, un nivel histórico. La política de cercanía con la gente se manifestó en la apertura de las puertas del Palacio de La Moneda, que estuvieron cerradas desde el golpe de Estado para los transeúntes. Así mismo fue el primer presidente de Chile que recorrió todas las comunas del país.

Su Gobierno fue sacudido en el año 2001 por las acusaciones de cohecho en contra de Patricio Tombolini, Subsecretario de Transportes, como consecuencia de irregularidades en las Plantas de Certificación Técnica de Vehículos Motorizados, acusaciones por las que este último funcionario fue condenado judicialmente, siendo finalmente absuelto en forma completa por la Corte Suprema de Chile en el año 2007.

Desde 2002, su gobierno debió enfrentar las sospechas de corrupción política, debido al procesamiento de uno de sus ministros, Carlos Cruz, y de otros funcionarios del Ministerio de Obras Públicas, por el caso denominado MOP-Gate. La jueza que lleva la causa, Gloria Ana Chevesich detectó que en dicho Ministerio se encargaron asesorías a empresas externas, que funcionaron como fachada para el pago de asignaciones suplementarias a funcionarios del Ministerio. En otra arista de ese caso, y como consecuencia de una entrevista a Carlos Cruz, éste reconoció que ministros, subsecretarios y otros representantes de exclusiva confianza del presidente, recibían pagos adicionales a su remuneración, figura que se denominó como "sobresueldos". La irregularidad fue reconocida por el Presidente Ricardo Lagos, especificándose que la práctica se desarrolló también durante los gobiernos de Eduardo Frei Ruiz-Tagle y Patricio Aylwin, aunque se sospecha que ella es de larga data en Chile como forma de complementar las rentas de los funcionarios de mayor responsabilidad. La postura oficial del gobierno consistió en no reconocer características de delito en las prácticas y en establecer una reforma legal que aumentara los sueldos de ministros y subsecretarios de gobierno, materia que fue aprobada en su trámite legislativo.
Durante su gobierno fructificó la modalidad de las concesiones, en que el Estado, sin perder la propiedad de las obras que licita, las entrega para su ejecución y operación a consorcios privados. Además, durante su mandato se gestó el Transantiago —actual Red Metropolitana de Movilidad—, proyecto que renovó un sistema de transporte público anacrónico ("micros amarillas"); el nuevo sistema tuvo serios problemas en su diseño e implementación, los que han sido paulatinamente corregidos.

Dentro de su política social se creó un seguro de desempleo, pagado por el Estado, los empleadores y los trabajadores; la ley para reformar el sistema de salud mediante garantías explícitas a la atención (Programa AUGE); el programa de erradicación de campamentos (Chile Barrio); un programa de protección social para familias en situación de extrema pobreza (Chile Solidario); la implementación de la Jornada Escolar Completa, que sería duramente cuestionada por la movilización estudiantil de 2006 en Chile; la escolaridad obligatoria durante 12 años; la creación de una institucionalidad cultural central (Consejo Nacional de la Cultura y las Artes); y el ya mencionado plan de transporte público en Santiago llamado Transantiago.

Además durante su mandato se aprobó la primera ley de matrimonio civil que permitió el divorcio vincular en la historia de Chile; se inició la aplicación de la reforma procesal penal; se crearon los Tribunales de Familia, aplicando el procedimiento oral a estas materias para hacer más expedita su resolución; se aprobó la Ley de Financiamiento Estudiantil con Aval del Estado, con fuertes críticas del movimiento estudiantil; y se aprobaron una de las más amplias modificaciones a la Constitución de 1980, desde que entró en vigencia.

Todos los gobiernos de la Concertación generaron avances en materia de aclarar los crímenes cometidos durante la dictadura militar. Durante el gobierno de Aylwin se emitió el Informe Rettig que dio cuenta de los ejecutado políticos y detenidos desaparecidos. Con Frei, se crearon las mesas de diálogo en que las Fuerzas Armadas debieron entregar la información que mantenían sobre el paradero de los detenidos desaparecidos. Ricardo Lagos formó una comisión para establecer la magnitud de la tortura en Chile. El 28 de noviembre de 2004, el día anterior al lanzamiento del Informe Valech, el presidente Lagos anunció que el gobierno proveería compensación a aproximadamente 30 000 víctimas de violaciones de los derechos humanos bajo la dictadura militar. De las 35 868 personas que testificaron ante la Comisión Nacional sobre Prisión Política y Tortura, aproximadamente 30 000 casos fueron considerados legítimos. El 15 de junio de 2005, Lagos ingresó al Congreso Nacional el proyecto de ley N° 20.405 que crea el Instituto Nacional de Derechos Humanos de Chile, el cual se constituiría en 2010.

Durante 2004 debió enfrentar una serie de tensiones en su relación con los países sudamericanos, provocadas por la antigua aspiración boliviana de salida al mar. La situación engarzó con la crisis energética sufrida por Argentina, quien provee de gas natural a Chile. En reuniones bilaterales entre Carlos Mesa, presidente boliviano y Néstor Kirchner presidente argentino, el primero condicionó la venta de gas boliviano a Argentina a que "ni siquiera una molécula de gas fuera vendida a Chile". A su vez, el presidente venezolano, Hugo Chávez, apoyó en diversas instancias la ambición marítima boliviana, produciéndose un "impasse" diplomático entre ambas naciones. La tensión entre gobiernos decreció durante julio de 2004. Anteriormente ya había habido tensión cuando el gobierno de Chile, a través de su embajada en Caracas, fue el único país del continente en reconocer el gobierno que se instaló por algunas horas mediante un golpe militar contra el presidente Hugo Chávez en 2002, que sería rápidamente frustrado.

Su mandato destaca por la firma de tratados de libre comercio, con la Unión Europea, Estados Unidos, China y Corea.

Finalmente, es importante mencionar la firme posición de Chile en el seno de las Naciones Unidas para evitar la invasión de Irak. Chile logró superar las múltiples presiones de EE. UU. y Reino Unido, y jugó un papel clave en el voto condenatorio de las Naciones Unidas a esta acción bélica.

El año 2001, Ricardo Lagos envió la "Ley Corta de Pesca" con una vigencia de 10 años en la cual se le aseguraban casi un 80% de las cuotas totales al holding empresarial del Grupo Angelini. La ley fue tramitada en el congreso gracias a la participación del por aquel entonces presidente del senado, Andrés Zaldívar, junto a su hermano Adolfo Zaldívar (por entonces, presidente del Partido Demócrata Cristiano). La ley permitió que las acciones de Itata aumentaran en más de un 200% en menos de dos años. Posterior a ello, el año 2002, se extendió la LMCA a las regiones I y II, lugar en donde la principal empresa pesquera corresponde a Eperva, también perteneciente al grupo Angelini. Nuevamente gracias al apoyo de Andrés Zaldívar junto a Julio Lagos y Sergio Bitar, la ley fue promulgada en menos de 18 meses. La intervención por parte de los hermanos Zaldívar fue duramente criticada debido a la gran inversión que tenían en dicha empresa, sin contar que el presidente del directorio de Eperva era el Felipe Zaldívar Larraín, tercer miembro de la familia Zaldívar.

Durante el gobierno de Ricardo Lagos, el por entonces presidente del Banco Estado Jaime Estévez, fue increpado por participar en el préstamo de 120 millones de dólares al grupo Luksic en el proceso de compra del Banco de Chile.

Andrónico Luksic Craig, líder del consorcio empresarial Grupo Luksic (consorcio que posee la mayor fortuna económica de Chile) fue citado por Ricardo Lagos durante una entrevista al diario La Tercera:

Los actos de corrupción acontecidos durante su gestión son uno de los aspectos más críticos de su gobierno. La connotación de ello no ha sido menor, dado que el Índice de Percepción de Corrupción en Chile en general es bajo, especialmente en relación con sus vecinos sudamericanos.

Los casos MOP-Gate, MOP-Ciade, CORFO-Inverlink, EFE, fueron los casos de corrupción que sacudieron su gobierno. Estas malas prácticas se expandieron en los altos niveles de la administración pública y en ellos participaron colaboradores muy cercanos al exmandatario. En enero de 2003 se ordena la detención del Ministro de Obras Públicas, Transportes y Telecomunicaciones, Carlos Cruz. La investigación llevada adelante por la jueza Gloria Ana Chevesich contabilizó innumerables aristas como MOP-Prograf, MOP-Idecon, MOP-Cycsa, MOP-Délano y MOP-Gesys, entre otras, las cuales versaron sobre irregularidades que van desde la falsificación de instrumentos públicos y licitaciones preacordadas hasta el fraude al fisco, pasando por el pago de sobresueldos y el desvío de fondos públicos, involucrando a decenas de personas. Finalmente, en julio de 2010 se dictó sentencia a 14 personas, entre ellas Cruz, que fue condenado a tres años de pena remitida y una multa de más de 799 millones de pesos. Otro de los inculpados, el exjefe de finanzas del MOP, Sergio Cortés, deberá purgar cinco años de presidio remitido, pero con libertad vigilada.

Durante el gobierno de la Presidenta Michelle Bachelet, se le ha responsabilizado (por parte de la oposición y del mismo gobierno) por los inconvenientes que ha generado el sistema de transporte público denominado Transantiago, el que ha drenado recursos estatales, lo que ha causado molestias en la oposición y en la mayoría de la ciudadanía.

Tras la Ocupación de la Araucanía, una zona que comprende los terrenos ubicados al sur del Río Biobío, el Estado comenzó a entregar las tierras de las comunidades mapuches a terceros (1883-1930), en el proceso conocido como "reducción". Desde esa época, las distintas generaciones de mapuches han intentado por distintas vías, la recuperación de tales territorios considerados ancestrales. A partir de 1997 comenzaron una serie de ocupaciones, tomas y atentados incendiarios en los predios pertenecientes a empresas forestales (principalmente las dependientes de COPEC y CMPC) y consideradas por muchas comunidades como territorio ancestral. La respuesta estatal a esta situación se ejecutó en su mayor parte durante el gobierno de Ricardo Lagos.

Se sindicó a la Coordinadora de Comunidades en Conflicto Arauco-Malleco, como una organización de carácter terrorista y fue perseguida como tal, encarcelándose a sus dirigentes en procesos cuya legalidad ha sido cuestionada por grupos mapuche, organizaciones de derechos humanos chilenas e internacionales y el Relator Especial para Pueblos Indígenas de las Naciones Unidas. Ejemplos paradigmáticos de esta situación lo constituye el llamado "Caso loncos" -donde los lonkos Pascual Pichun y Aniceto Norin fueron condenados a 5 años y 1 día de prisión por "amenaza de incendio terrorista" y el "Caso Puluco-Pidenco" -donde cuatro comuneros fueron condenados a 10 años y un día de prisión por "incendio terrorista". Estos casos han sido descritos por el Relator Especial para Pueblos Indígenas de las Naciones Unidas Rodolfo Stavenhagen, como juicios que presentan una legalidad cuestionable. Los hechos fueron denunciados a la Comisión Interamericana de Derechos Humanos, por infracción al debido proceso, reconocido en la Convención Americana de Derechos Humanos, entre otros fundamentos, que decretó su admisibilidad. Por otra parte, el Comité de Derechos Humanos, que vigila el cumplimiento del Pacto Internacional de Derechos Civiles y Políticos, en sus observaciones al informe de Chile, también impugnó las prácticas contra el movimiento mapuche. En este sentido, instó al Estado chileno a modificar la Ley N° 18.314, sobre conductas terroristas (conocida como ley antiterrorista).

Durante su gobierno se produjo el desastre del río Cruces, provocado por la empresa productora de celulosa CELCO propiedad de Celulosa Arauco, en que se afectó seriamente el Santuario de la Naturaleza Carlos Anwandter probablemente por el exceso de dioxinas, haciéndose emblemático el caso por la masiva muerte y migración de los cisnes de cuello negro. Ante esta situación la autoridad ambiental ordenó que la empresa rebajara en un 20% su producción y que presentara en el plazo de 2 años una alternativa de descarga de los RILES. La opción que propuso la empresa fue lanzarlos al mar a través de un emisario submarino frente a las costas de Mehuín lo que fue señalado por Lagos como "la única alternativa posible". Sin embargo la oposición de pescadores y comunidades mapuche lafkenche del sector ha impedido que se lleven a cabo los estudios pertinentes, aún después de terminado el gobierno de Lagos.

En los últimos días de su período, durante el mes de febrero, la Comisión Nacional del Medio Ambiente dio su aprobación al cuestionado proyecto minero binacional Pascua Lama, presentado por la trasnacional canadiense Barrik Gold, ello una vez concluidos los procedimientos legales que norman la participación ciudadana en la "Evaluación de Impacto Ambiental" a los que son sometidos este tipo de proyectos. La mina se ubica debajo de los tres glaciares que alimentan el valle del Río Huasco, último río vivo en el Desierto de Atacama.

Por otro lado, Ricardo Lagos fue criticado por algunos y aplaudido por otros por denunciar lo que él consideraba una gran concentración y falta de pluralidad de los medios de comunicación impresos, en manos de dos grandes consorcios históricamente vinculados a los poderes económicos y a la oposición. En septiembre del 2005, Lagos envió una carta a Agustín Edwards Eastman, director del periódico chileno El Mercurio, en la que señala que 

Sin embargo, durante el gobierno de Ricardo Lagos, nunca se discutió ni se puso como tema de discusión el dinero brindado por el Estado a los principales medios de prensa escrito del país. El Estado chileno subsidió directamente a El Mercurio y a Copesa (consorcio de prensa escrita al cual pertenece el diario "La Tercera"), asignándole 80 % de los recursos publicitarios destinados a la prensa escrita durante 2005.

Otro caso emblemático se sucedió cuando Lagos con indignación criticó al programa "Contacto" (del entonces canal católico de la televisión abierta chilena; Canal 13) debido a que en un episodio se mostró algunos aspectos negativos de lo que fueron los planes Puente. El ex mandatario consiguió salir al aire después de la transmisión para entregar su visión.




Después de abandonar la presidencia, Lagos fue responsable de un seminario especial llamado «Democracia y Desarrollo en Latinoamérica», de un mes de duración, en el Centro para Estudios latinoamericanos de la Universidad de California, Berkeley.

El 24 de marzo de 2006, anunció el lanzamiento en Santiago de una fundación propia llamada «Democracia y Desarrollo», desde donde se organiza una intensa agenda internacional. Su amigo Herman Chadwick Piñera, designado por el mismo Lagos como presidente del Consejo Nacional de Televisión de Chile (CNTV) durante su presidencia, contribuyó con US$10 mil dólares para la gestación de la fundación, siendo además uno de los invitados a la ceremonia de inauguración. En mayo del mismo año, Chadwick asumió la presidencia de la Asociación de Concesionarios de Obras de Infraestructura Pública (Copsa), que controlaría las concesiones de obras viales a través del sistema creado durante la presidencia de Lagos.

Solo tres días después, viajó a Estocolmo, donde asumió como Presidente del Club de Madrid por dos años más, una organización exclusiva de antiguos presidentes creada para promover más y mejor democracia a través del mundo, de la cual aún forma parte. También asume la Mesa Directiva de co-presidencia del Diálogo Inter-Americano.

En mayo de 2007, la Universidad de Brown anuncia que Lagos tendría un cargo docente en el Instituto de Watson para Estudios Internacionales por un periodo de cinco años, que fue alargado por dos años, empezando el 1 de julio de 2007. El 1 de mayo de 2007 fue designado por el Secretario General de la ONU, Ban Ki-moon, enviado especial para el cambio climático de la ONU, junto con la exprimera ministra noruega Gro Harlem y el excanciller surcoreano, Hang Seung-soo. Este nombramiento ha sido criticado en una carta abierta firmada por veinte ONG ecológicas destinada a Ban Ki-moon. En la carta las organizaciones critican el pobre desempeño del gobierno de Lagos en asuntos medioambientales como el tema del proyecto Pascua Lama y la muerte de Cisnes de Cuello Negro, entre muchos otros problemas. Lagos ha rechazado tales críticas aludiendo a que dichos expertos medioambientales "no saben nada" e indicando con ambigüedad que desde su punto de vista el tema se relaciona simplemente con que "la situación ambiental de Chile se debe entender en el contexto de su veloz ritmo de desarrollo".

En diciembre de 2007, el expresidente pidió excusas públicamente a los millones de santiaguinos afectados por la planificación de la reforma al transporte metropolitano, denominado Transantiago.

En 2008 la Fundación Konex de la Argentina le otorgó el Premio Konex Mercosur como una de las personalidades más relevantes de la década en la región.

Para las Elecciones del 2009, se le pidió varias veces que fuera como candidato, dado que según los sondeos él era el posible candidato que se acercaba más al aventajado Sebastián Piñera, desde inicios del 2008, Lagos dio varios sí y varios no, pero siempre decía que se negaría a competir en primarias con su colega José Miguel Insulza, y menos dentro de la Concertación con el expresidente Eduardo Frei Ruiz-Tagle o la senadora Soledad Alvear, decidió mantenerse neutral ante esta situación. El 4 de diciembre de 2008 confirmó que no sería candidato para el 2009 y desde entonces se mantuvo neutral completamente frente al tema.

Continuó con una intensa agenda internacional, participando en encuentros como la Cumbre de Delhi sobre Desarrollo Sostenible, en febrero de 2011 
(Delhi Sustainable Development Summit) y en el 10º aniversario y Conferencia Anual del Club de Madrid, noviembre de 2011.
Además forma parte de la organización internacional Crisis Group y participa del Foro Iberoamericano.

El 27 de diciembre de 2011, se reunió con el presidente Sebastián Piñera para tratar diferentes temas entre ellos le planteó el fin del sistema binominal ya que lo calificó como un cáncer que debe ser cambiado de raíz y sin "cálculos mezquinos". En consecuencia, junto al binominal, hay distintas fórmulas para poderlo remplazar", fue enfático en aclarar que está disponible a sentarse a analizar la reforma al sistema electoral, pero partiendo de la base que el sistema "se acabó".

En noviembre del 2012, Ricardo Lagos fue invitado a una conferencia magistral en la Cátedra Latinoamericana Julio Cortázar: América Latina Hoy. Desafíos después de la crisis; Universidad de Guadalajara. En los mismos días, en la Feria del Libro de México, participó en el reconocimiento a Carlos Fuentes, escritor mexicano fallecido unos meses antes. 

En 2013, fue invitado por la Universidad de Sao Paulo, a dar inicio a la Cátedra José Bonifacio, Brasil.

El 13 de abril de 2018, tras una invitación de un organismo dependiente de la Secretaría General de ONU. Se unió a un Consejo Asesor del Departamento de Asuntos Económicos y Sociales (DESA).

El 2 de septiembre de 2016 Lagos publicó una declaración pública en la que anunció su disposición a repostularse como candidato a la presidencia de su país para enfrentar los «profundos cambios» que se han experimentado en el mundo y en el país. En dicha declaración sostuvo que «si chilenas y chilenos consideran que nosotros podemos llevar adelante una propuesta de avance y progreso y que entregue a las nuevas generaciones un Chile fortalecido, yo no me restaré a ese desafío».

El 10 de abril de 2017, luego de que las encuestas arrojaran alrededor de sólo un 3% de las preferencias, y sumado al hecho que el Partido Socialista de Chile finalmente decidió apoyar a Alejandro Guillier para las primarias de la Nueva Mayoría, Lagos decidió declinar su candidatura presidencial, afirmando que «He puesto todo mi empeño por llevar este mensaje político a los chilenos, pero veo también que en mi propio espacio político, la centroizquierda, no se ha provocado una convergencia en torno a este proyecto».



Resultado de las elecciones de 1999 para la Presidencia de la República

Segunda vuelta



</doc>
<doc id="9816" url="https://es.wikipedia.org/wiki?curid=9816" title="Linfoma de Burkitt">
Linfoma de Burkitt

El linfoma de Burkitt o leucemia de células de Burkitt es una rara forma de cáncer del sistema linfático—asociado principalmente a linfocitos B—que afecta predominantemente a gente joven, descrita más frecuentemente en África central, aunque también lo ha sido en otras áreas del mundo. La forma vista en África parece estar asociada con la infección del virus de Epstein Barr, aunque el mecanismo patogénico es desconocido. El epónimo proviene del cirujano Denis Parsons Burkitt quien, trabajando en el África ecuatorial, describió la enfermedad en 1956.

El linfoma de Burkitt resulta de una característica translocación cromosómica que afecta al gen "Myc". Una translocación cromosómica significa que el cromosoma se ha roto, lo que permite su unión con otras partes cromosómicas. En el linfoma de Burkitt afecta al cromosoma 8 (locus del "gen Myc"), lo que cambia el patrón de expresión del "gen Myc" alterando su función natural de control en el crecimiento y proliferación celular. La variante más frecuente produce una traslocación del cromosoma 8 al 14—t(8;14)(q24;q32)—mientras que otras variantes incluyen traslocaciones a otros cromosomas—t(2;8)(p12;q24) y t(8;22)(q24;q11). Se ha identificado también una traslocación mucho menos frecuente entre tres cromosomas, t(8;14;18).

El linfoma de Burkitt se clasifica en tres variantes clínicas, la endémica, la esporádica y la asociada a la inmunodeficiencia:




Morfológicamente es virtualmente imposible distinguir estas tres variantes clínicas. El linfoma de Burkitt asociado a la inmunodeficiencia puede verse con una apariencia más plasmática o con más pleomorfismo, aunque estas no son características específicas.
El linfoma de Burkitt bajo el microscopio consiste en población monótona de capas celulares de tamaño medio con una gran actividad proliferativa y apoptótica. La apariencia se asemeja a una noche de estrellas, por razón de las inclusiones esparcidas de los macrófagos que han digerido las partes celulares muertas. Las células del tumor tienen un tamaño muy similar al de los histiocitos o células endoteliales, de modo que no son células muy grandes, sino de mediano tamaño. Las células tumorales tienen una pequeña cantidad de citoplasma que se tiñe basofílica. Los contornos celulares tienen la apariencia de ser cuadriláteras.
Las células B normales poseen genes para la cadena pesada y la cadena ligera de las inmunoglobulinas. Cada célula B tiene su configuración única de cadenas pesadas y livianas y que son únicas en cada célula individual. Sin embargo, como las células de Burkitt provienen de procesos proliferativos, cada célula tumoral de un paciente tiende a poseer genes idénticos de la cadena pesada. Por ello, cuando se analiza una electroforesis con sangre de un paciente con linfoma de Burkitt, aparece una banda clonal constituida por genes de la cadena pesada de la Ig que han migrado a una misma posición. Otras enfermedades infecciosas como la mononucleosis infecciosa carecen de esta banda clonal electroforética.



</doc>
<doc id="9817" url="https://es.wikipedia.org/wiki?curid=9817" title="Virus de Epstein-Barr">
Virus de Epstein-Barr

El virus de Epstein-Barr (abreviado VEB) es un virus de la familia de los herpesvirus (familia que también incluye el virus del herpes simple y el citomegalovirus). Es la mayor causa de la mononucleosis aguda infecciosa, síndrome común caracterizado por fiebre, garganta irritada, fatiga extrema y ganglios linfáticos inflamados. La infección por el virus de Epstein-Barr se da en todo el mundo.

El VEB infecta a la mayor parte de la gente en algún momento de sus vidas. De esta forma se obtiene una inmunidad adaptativa a través del desarrollo de anticuerpos contra el virus, lo que suele prevenir nuevos contagios por factores externos. El virus queda latente por el resto de la vida (como episomas), pudiendo desencadenar nuevas infecciones, reactivándose intermitentemente con o sin síntomas.

Muchos niños se infectan con el virus de Epstein-Barr, aunque estas infecciones no suelen desarrollar una sintomatología grave y no se distinguen de otras enfermedades breves de la infancia. Cuando la infección con el VEB ocurre durante la adolescencia o la juventud, causa una mononucleosis infecciosa en un 30 a 70 % de los casos.

El virus de Epstein-Barr fue descrito en 1964 por los científicos británicos M.A. Epstein, Y.M. Barr y B.G. Achong, quienes encontraron partículas virulentas en células de tejidos con un cáncer linfático recientemente descubierto. El virus de Epstein-Barr se conoce por infectar sólo dos tipos de células en el cuerpo humano: algunas células de las glándulas salivales y los glóbulos blancos o leucocitos.
Recientemente se ha encontrado este virus en un tipo de goma de mascar en la zona pantanosa del sur de Dinamarca, esta estaba hecha de resina de abedul cocinada y su datación arrojó 5.700 años de antigüedad, en dicho "chicle" se encontró el virus además de un microbioma oral en el que se hallaba bacterias de neumonia entre otros.

Para su transmisión se requiere un estrecho contacto personal y se transmite a través de la saliva, en la que se mantiene activo durante varias horas. Por ello, a la mononucleosis se la conoce también como «enfermedad del beso» o «fiebre de los enamorados». En los grupos humanos en condiciones de hacinamiento, la infección se difunde de forma precoz. La eliminación del virus, sin que el individuo tenga síntomas, puede ocurrir varios meses después de la infección. El período de incubación es de 30 a 50 días.

La mayor parte de las infecciones por virus de Epstein-Barr en los niños o adolescentes son asintomáticas y se presentan como una faringitis con o sin amigdalitis. Por el contrario, en los adultos el 75 % de los casos presentan mononucleosis infecciosa. El período de incubación de la mononucleosis en los adultos jóvenes es de 4 a 6 semanas antes de que comiencen a manifestarse los síntomas. La fatiga, malestar y mialgia comienzan a manifestarse 1 a 2 semanas antes de que aparezca la fiebre y el dolor de garganta. La fiebre no suele ser demasiado intensa. La linfadenopatía se observa preferentemente en los ganglios cervicales, pero otros muchos pueden estar afectados.

En un 5 % de los pacientes se desarrolla un sarpullido popular, generalmente en brazos y tórax, sobre todo en sujetos que han recibido ampicilina o amoxicilina. Sin embargo, este sarpullido no es predictivo de una futura alergia a las penicilinas. Muchos enfermos padecen estos síntomas durante 2 a 4 semanas, pero el malestar general y la fatiga pueden durar meses.

Es frecuente la presencia de hepatoesplenomegalia con aumento de niveles séricos de enzimas hepáticas, por lo que es prudente solicitar un perfil hepático para orientar el diagnóstico.

Las complicaciones incluyen meningitis, encefalitis y síndrome de Guillain-Barré. Excepcionalmente puede producirse la rotura del bazo, debido a su inflamación durante el curso de la infección. Puede haber disminución de las plaquetas (sangramientos), glóbulos rojos, glóbulos blancos, inflamación de los testículos (orquitis), y del corazón (miocarditis). También se ha relacionado este virus con el Síndrome de Fatiga Crónica, ya que algunos de los afectados por dicho síndrome presentan pruebas positivas para este virus.

El caso típico de mononucleosis infecciosa con anticuerpos heterófilos positivos es bastante fácil de diagnosticar. Más complicada es la situación cuando las manifestaciones clínicas son atípicas o cuando los anticuerpos heterófilos son negativos. La causa más frecuente de mononucleosis infecciosa con anticuerpos heterófilos negativos es la infección por citomegalovirus (CMV). Ambos cuadros son muy parecidos e incluso en muchas ocasiones los títulos de anticuerpos frente al CMV están también elevados en una mononucleosis por virus de Epstein-Barr. La infección por CMV suele producir menos dolor de garganta y con frecuencia solo cursa con astenia y fiebre.

La hepatitis por virus de la hepatitis A puede ir acompañada de linfocitosis atípica similar a la MI, si bien las transaminasas están mucho más elevadas. Otras infecciones que se presentan con cuadros parecidos a los de MI son la rubéola (si bien esta última con la erupción cutánea típica), la toxoplasmosis aguda y sobre todo la infección por herpes virus 6.

El tratamiento de la mononucleosis infecciosa consiste en reposo y alivio del malestar. Debido a una posible inflamación del bazo, debe evitarse un exceso de actividad física durante el primer mes para prevenir la posibilidad de una rotura esplénica. Aunque se han usado corticoides (prednisona 40-60 mg/día durante dos o tres días con reducción de las dosis en la semana siguiente) para evitar la obstrucción de las vías respiratorias en los pacientes con hipertrofia tonsilar, estos no se recomiendan ya que pueden originar superinfecciones.

El aciclovir no ha mostrado ningún impacto significativo sobre la mononucleosis infecciosa aunque in vitro inhibe la replicación del virus.




</doc>
<doc id="9819" url="https://es.wikipedia.org/wiki?curid=9819" title="Pierre de Fermat">
Pierre de Fermat

Pierre de Fermat (Beaumont-de-Lomagne, Francia; 17 de agosto de 1601- Castres, Francia; 12 de enero de 1665) fue un jurista y matemático francés denominado por el historiador de matemáticas escocés, Eric Temple Bell, con el apodo de «príncipe de los aficionados».

Fermat fue junto con René Descartes y Johannes Kepler uno de los principales matemáticos de la primera mitad del siglo XVII.

Joseph-Louis Lagrange afirmó claramente que consideraba a Fermat como el inventor del cálculo. Fermat fue cofundador de la teoría de probabilidades junto a Blaise Pascal e independientemente de Descartes, descubrió el principio fundamental de la geometría analítica. Sin embargo, es más conocido por sus aportaciones a la teoría de números en especial por el conocido como último teorema de Fermat, que preocupó a los matemáticos durante aproximadamente 350 años, hasta que fue demostrado en 1995 por Andrew Wiles ayudado por Richard Taylor sobre la base del Teorema de Shimura-Taniyama.

Fermat nació en la primera década del siglo XVII en Beaumont-de-Lomagne, Francia; la mansión de finales del siglo XV donde nació Fermat actualmente es un museo. Era originario de Gascuña, donde su padre, Dominique Fermat (un acaudalado mercader de cuero) sirvió durante tres períodos de un año como uno de los cuatro cónsules de Beaumont-de-Lomagne. Su madre se llamaba Claire de Long. Pierre tenía un hermano y dos hermanas y casi con seguridad se crio en su ciudad natal. Hay poca evidencia sobre su educación escolar, pero probablemente fue en el Colegio de Navarra de París en Montauban.

Asistió a la Universidad de Orleans desde 1623 y recibió un título de bachiller en derecho civil en 1626, antes de pasar a Burdeos, donde comenzó sus primeras investigaciones matemáticas serias, y en 1629 dio una copia de su revisión de la obra de Apolonio "De Locis Planis" a uno de los matemáticos locales. Hay constancia de que en Burdeos estuvo en contacto con Jean de Beaugrand, y durante esta época produjo un trabajo importante sobre los extremos de una función, que entregó a Étienne d'Espagnet, quien claramente compartía intereses matemáticos con Fermat. Allí se vio muy influenciado por el trabajo de François Viète.

En 1630, compró la oficina de un concejal en el Parlamento de Toulouse, uno de los altos tribunales de la Judicatura en Francia, y fue juramentado por el Grand Chambre en mayo de 1631. Ocupó esta oficina por el resto de su vida. De este modo, Fermat tuvo derecho a cambiar su nombre de "Pierre Fermat" a "Pierre de Fermat". Hablante fluido en seis idiomas (francés, latín, occitano, griego clásico, italiano y español), Fermat fue elogiado por sus versos escritos en varios idiomas y su consejo fue frecuentemente requerido respecto a la revisión de textos griegos.

Comunicó la mayor parte de su trabajo en cartas a amigos, a menudo con poca o ninguna prueba de sus teoremas. En algunas de estas cartas a sus amigos, exploró muchas de las ideas fundamentales del cálculo antes que Newton o Leibniz. Fermat era un experto abogado que hacía de las matemáticas más un pasatiempo que una profesión. Sin embargo, hizo importantes contribuciones a la geometría analítica, la probabilidad, la teoría de números y el cálculo. El secretismo era común en los círculos matemáticos europeos de la época. Esto naturalmente condujo a disputas acerca de la prioridad de algunos descubrimientos con sus contemporáneos, como Descartes y Wallis.

Anders Hald escribe que "La base de las matemáticas de Fermat fueron los tratados griegos clásicos combinados con los métodos de François Viète".

Pierre de Fermat murió el 12 de enero de 1665 en Castres, en el departamento actual de Tarn.

También conocida como espiral parabólica, es una curva que responde a la siguiente ecuación en coordenadas polares:

Es un caso particular de la espiral de Arquímedes.

Dos números amigos son dos números naturales "a" y "b" tales que "a" es la suma de los divisores propios de "b", y "b" es la suma de los divisores propios de "a". (La unidad se considera divisor propio, pero no lo es el mismo número.)

En 1636, Fermat descubrió que 17.296 y 18.416 eran una pareja de números amigos, además de redescubrir una fórmula general para calcularlos, conocida por Tabit ibn Qurra, alrededor del año 850.

Un número de Fermat es un número natural de la forma:
donde "n" es natural.

Pierre de Fermat conjeturó que todos los números naturales de esta forma con "n" natural eran números primos, pero Leonhard Euler probó que no era así en 1732. En efecto, al tomar "n"=5 se obtiene un número compuesto:

El teorema sobre la suma de dos cuadrados afirma que todo número primo "p", tal que "p"-1 es divisible entre 4, se puede escribir como suma de dos cuadrados. El 2 también se incluye, ya que 1+1=2. Fermat anunció su teorema en una carta a Marin Mersenne fechada el 25 de diciembre de 1640, razón por la cual se le conoce también como "Teorema de navidad de Fermat"

El "pequeño teorema de Fermat", referente a la divisibilidad de números, afirma que, si se eleva un número "a" a la "p"-ésima potencia y al resultado se le resta "a", lo que queda es divisible por "p", siendo "p" un número primo con "a" y "p" coprimos. Su interés principal está en su aplicación al problema de la primalidad y en criptografía.

Pierre de Fermat acostumbraba a escribir las soluciones a los problemas en el margen de los libros. Una de las notas que escribió en su ejemplar del texto griego de la "Arithmetica" de Diofanto de Alejandría (editada por Claude Gaspard Bachet de Méziriac en 1621) dice lo siguiente:

Esta afirmación, más tarde ya conocida como "Último teorema de Fermat", se convirtió en uno de los teoremas más importantes en matemáticas. No se sabe si Fermat halló realmente la demostración, ya que no dejó rastro de ella para que otros matemáticos pudiesen verificarla. Este problema matemático mantuvo en vilo a los matemáticos durante más de tres siglos (se dice que, frustrado, Euler incluso pidió a un amigo que registrara de arriba abajo la casa de Fermat en busca de la demostración), hasta que en 1995 Andrew Wiles ayudado por Richard Lawrence Taylor pudo demostrar el teorema. Wiles utilizó para ello herramientas matemáticas que surgieron mucho después de la muerte de Fermat, de forma que este debió de encontrar la solución por otro camino, si es que lo hizo. En cualquier caso, tenía razón.

Hombre erudito y embebido en la cultura clásica grecorromana, era enciclopédico por la amplitud de su bagaje. Hacía anotaciones en los márgenes de los libros que leía, con observaciones y esbozos de demostraciones. No era matemático profesional ni escribía libros. Era de su interés el saber humano de su tiempo. Envía cartas de sus hallazgos o inquietudes, tuvo como mentor y difusor al padre Mersenne, y, en vez de formalizar sus descubrimientos o inventos, posiblemente se dedicaba a especular y daba vuelo a su imaginación desbordante; lanzaba retos mediante problemas cuya solución poseía. Polemizó con Descartes sobre el caso de "La Dioptrique" obra de este. Ante la incomodidad de Descartes, Fermat envió una prueba, haciendo presente que más le importaba la verdad no la fama ni la envidia.





</doc>
<doc id="9820" url="https://es.wikipedia.org/wiki?curid=9820" title="Mahjong">
Mahjong

El , mah jong o mah-jongg () es un juego de mesa de origen chino, exportado al resto del mundo, y particularmente a occidente a partir de los años 1920. En chino también se le conoce como "gorrión". Además de la versión original china, se incluye sus variantes (japonés, coreano, estadounidense y solitario, por mencionar algunos). Por su popularidad e influencia, fue extendido a todo el mundo a través de internet.

Su antecesor recibía el nombre de «juego de hojas en tiras», ya que las fichas eran de cartulina, como los naipes actuales. Progresivamente se abandonó este material y se empezaron a fabricar fichas de marfil, madera y, sobre todo, bambú, aunque actualmente lo normal es utilizar el plástico, más duradero y barato. No obstante, se siguen fabricando, como se hizo en la antigüedad, verdaderas obras de arte en distintos materiales.

El mahjong, en su versión original, está compuesto de 144 fichas o tejas genéricas, pero ese número cambia dependiendo del tipo de set, en donde se agrega o elimina algunas fichas. En muchas variantes, los jugadores tienen 13 fichas para empezar el juego y, al robar una ficha N.º 14, ganaría el juego si tiene 4 sets (Chow, Pung y Kong, alternada) y el par de ojos.

Según parece, el mahjong es un descendiente directo de un antiguo oráculo que hace miles de años consultaban los adivinos chinos. Cuando los astrónomos empezaron a registrar las progresiones del Sol, la Luna y los planetas, utilizaron un mecanismo sencillo, un tablero, para calcular las posiciones de los cuerpos celestes. El movimiento a través de los cielos se registraba moviendo unos contadores alrededor de las divisiones del tablero. Este, u otro parecido, es posiblemente también el origen de juegos muy difundidos, como el parchís, o la oca. Pero precisamente en el mahjong resultan reconocibles algunos restos de este origen, como por ejemplo en el hecho de que los puntos cardinales se encuentren invertidos, ya que se trata de representar un mapa celeste, no terrestre, o que se repartan trece fichas, que son los meses del calendario lunar.

De todas formas, la historia del mahjong está poco clara y abundan los antecedentes poco documentados. Por ejemplo, una de las leyendas sobre su origen afirma que el juego fue inventado por Confucio alrededor del año 500 a. C. Según esto, las fichas de los tres dragones, Rojo, Verde y Blanco —en chino, respectivamente, , y —, representarían las virtudes confucianas de benevolencia, sinceridad y piedad filial. El dragón Rojo haría referencia a China (). También según esta leyenda, Confucio habría sido un enamorado de los pájaros, lo que explicaría el nombre de «gorrión» que también recibe el juego.

Un juego, más o menos emparentado con el actual mahjong, fue inventado en la dinastía Tang, durante los años del reinado del emperador Tai Zong (626-649), para diversión de la casa imperial y la nobleza. Sin embargo, no hay evidencia de la existencia del mahjong antes de la época de la Rebelión Taiping, a mediados del siglo XIX. El consenso general es que el juego fue desarrollado alrededor de 1850 basándose en juegos de cartas y de dominó ya existentes. Muchos historiadores creen que se basó en un juego de cartas llamado o de comienzos de la dinastía Ming. Este juego se jugaba con cuarenta cartas numeradas del 1 al 9 en nueve palos con cuatro cartas extra de flores, de manera similar al actual mahjong. Según algunos, el juego habría sido creado por oficiales del ejército durante la Rebelión Taiping para pasar el tiempo. Según otra teoría, habría sido creado por un noble que vivía en los alrededores de Shanghái entre 1870 y 1875. Otros creen que fue obra de dos hermanos que vivían en la ciudad de Ningpo.

Pero en realidad, el mahjong tal y como lo conocemos hoy en día tiene una historia bastante más corta, pues se remonta al final de la China imperial a principios del siglo XX; este es el llamado "viejo estilo".

La primera vez que se hizo mención del mahjong en una lengua distinta al chino fue en 1895, en un artículo del antropólogo americano Stewart Culin y para 1910 ya había varios escritos en otras lenguas, como francés o japonés. En Estados Unidos, Joseph Park Babcock escribió un libro llamado "Rules of Mah-Jongg" ("Reglas del Mah-Jongg"), que en una versión simplificada de 1920 era conocido como "el libro rojo", aunque estas simplificaciones se abandonarían posteriormente. El juego tuvo a partir de los años veinte un gran éxito en Inglaterra y Estados Unidos, donde se dio a conocer con nombres comerciales como "Pung Chow" o "Game of Thousand Intelligences", formando parte de la moda por todo lo oriental, dedicándosele también canciones de éxito, como "Since Ma is Playing Mah Jong", de Eddie Cantor. Era jugado sobre todo por mujeres. En los años treinta, en Estados Unidos, se hicieron varias revisiones de las reglas hasta que en 1937 se creó la "Liga Nacional de Mah Jongg" ("National Mah Jongg League", NMJL) y se estandarizó el reglamento con el libro "Maajh: The American Version of the Ancient Chinese Game" ("Maajh: la versión americana del antiguo juego chino"). Aunque en los años veinte había sido un juego aceptado por personas de todo tipo de razas, a partir de esta oficialización fue considerado racistamente como un juego judío, debido a que muchos de sus jugadores lo eran; e incluso la NMJL fue considerada como una organización judía, producto del antisemitismo. La versión occidental del juego es llamada "nuevo estilo", aunque parece ser que el origen de estas modificaciones proviene de Pekín y Shanghái, por lo que a veces a este nuevo estilo se la denomina "estilo Shanghái".

Sea como fuere, lo indudable es que actualmente la mayor parte de los jugadores se encuentran en Taiwán y Estados Unidos, ya que durante décadas estuvo prohibido en la China comunista. Sin embargo, con las reformas políticas emprendidas a principios de los años 90, el mahjong ha pasado a ser considerado deporte oficial desde 1998 en la China continental y, aunque nunca dejó de jugarse en las casas, ahora vuelve a ser habitual la estampa de los jugadores de mahjong en las calles de las poblaciones chinas. Junto con la despenalización de su práctica, el gobierno chino editó un reglamento oficial que es el que rige en los campeonatos mundiales oficiales, el primero de los cuales se celebró en Tokio en 2002. De esta forma, se ha pretendido recuperar un patrimonio cultural de China que estaba siendo usurpado por otras naciones asiáticas.

El mahjong ha adquirido una popularidad enorme en toda Asia, de modo que muchos países lo consideran su juego nacional. Existen muchas variantes adaptadas a cada país como ocurre con la japonesa, coreana, vietnamita o la filipina, y resulta normal que cualquier acto festivo, celebración, comida, o incluso negocio, acabe con unas partidas de mahjong. También existe una variante israelí.

El mahjong es un juego para cuatro jugadores y raras veces se practica con un número distinto de participantes. No obstante, también pueden jugarlo tres, aunque el juego pierde interés. Por tanto, siempre supondremos que hablamos del juego para cuatro jugadores.

Uno de los aspectos que hemos de considerar es el origen chino del mahjong, lo cual representa de por sí, a ojos de los occidentales, un atractivo, por dotar de cierto exotismo al juego, principalmente en lo que se refiere al diseño de las fichas y a la terminología empleada, que en varios casos utiliza los términos originales chinos.

Entre las virtudes del mahjong, que sin duda han contribuido a popularizarlo tan extraordinariamente, destacan las siguientes:


Cada jugador participa de modo individual: no hay ningún sistema que permita establecer equipos. El juego se desarrolla en partidas sucesivas, cada una de las cuales solamente puede ser ganada por uno de los cuatro jugadores; siempre que finaliza una partida el ganador recibe puntos de los tres perdedores. Al final se hace un recuento de puntos, y se determina la situación de cada uno. Aunque esto puede hacerse apuntando en un papel los puntos de cada jugador, lo mejor es proveerse de fichas que simbolicen los puntos, a modo de moneda.

Para jugar al mahjong es imprescindible procurarse un juego de fichas específicas. Si además contamos con otros elementos la partida será mucho más agradable, pero se puede prescindir de ellos. Por ejemplo es recomendable contar con atriles para colocar las fichas (uno para cada jugador); un conjunto de fichas para contabilizar los puntos (o palos de 100, 1000, 5000 y 10000 puntos en los sets japoneses y coreanos); una mesa cuadrada, o redonda; dos dados; y cuatro discos o etiquetas con las inscripciones o símbolos de los cuatro vientos (Este, Sur, Oeste y Norte), para mostrar en cada momento la ronda que se está jugando.

Los atriles, dados y símbolos de los cuatro vientos a veces se incluyen con el juego básico de fichas, también llamadas tejas. En sets japoneses, coreanos y de algunas variantes, las fichas de puntuación son remplazadas por palos de puntos. A veces también se usan unas regletas auxiliares que ayudan a la formación de la muralla.

El mahjong, en su versión original, está compuesto de 144 fichas o tejas genéricas, pero en sets japoneses se requieren 136 fichas para empezar sin las flores o estaciones.
Dependiendo de los sets, es posible usar flores o estaciones, que no se repiten. En las versiones de 3 jugadores, eliminan el viento norte y una versión provincial china elimina las fichas de honor. Los sets coreanos se eliminan las fichas bambú o dejan solo las fichas 1 y 9 de la misma clase. Los sets japoneses raramente utilizan flores o estaciones. Además, en los sets japoneses y dependiendo de las reglas usan fichas rojas, remplazando a algunas fichas originales, lo que aumenta el indicador de dora.

Una vez que los jugadores han tomado asiento, en el orden que deseen, se procede a asignar a cada uno un viento o punto cardinal, cuyo orden no es el habitual en occidental, sino Este - Sur - Oeste - Norte, en sentido contrario de las agujas de un reloj. Esta ordenación, aparentemente caprichosa, responde a que los puntos cardinales de un mapa están al revés, como si en el cielo hubiera un mapa transparente y nosotros, que lo vemos desde el suelo, mirásemos los puntos cardinales del revés. Evidentemente, no es necesario tener en cuenta nada de esto, basta con seguir el orden indicado como una "regla del juego", que en el fondo es lo que resulta ser.

Para determinar qué jugador es el Este se tiran dos dados, y quien obtenga mayor puntuación queda investido como Viento del Este. Los otros jugadores quedan entonces también determinados, según se ha explicado anteriormente. Es decir, el jugador de su derecha es el Sur, el de enfrente es el Oeste, y el de su izquierda es el Norte.

Como curiosidad, ha de decirse que la forma original de determinación del Viento del Este es mucho más compleja y elaborada entre jugadores chinos, con vientos provisionales que sirven para determinar luego los definitivos. Sin embargo, como el propósito es el mismo, entendemos el sistema descrito como perfectamente válido.

Una «partida» corresponde a cada uno de los episodios en los cuales se mueven y reparten las fichas, y que termina o bien cuando algún jugador da fin al mismo haciendo mahjong o bien cuando se acaban las fichas de la muralla. Cada partida tiene dos vientos que le afectan, el "viento prevalente" (o "viento de ronda") y el "viento propio" (o "viento de asiento"). La primera partida tiene como viento prevalente el Este. Se llama «viento propio» al viento que le corresponde a cada jugador. El viento prevalente se cambia cada cuatro partidas (o una ronda). A continuación de la ronda del viento Este, comenzaría la ronda del Sur, y así sucesivamente. En caso de empate o si gana el viento este, se repite la partida (o sea, los vientos no se rotan). Por tanto, podemos hacer el siguiente esquema provisional (jugando el modo todos los vientos y suponiendo que el jugador A es aquel al que le tocó ser Este por primera vez):

Conseguir tener una combinación del viento prevalente o del viento propio es importante para calcular la puntuación (más tarde se verá cómo). Cuando en una partida coinciden para un jugador el viento prevalente y el viento propio, se dice que ese viento es «viento doble», lo que constituye una circunstancia muy favorable; en la primera ronda, para el jugador Este, el Este es viento doble.

Por tanto, según el esquema anterior habría cuatro rondas de cuatro partidas cada una. Sin embargo, si una partida es ganada por el jugador que era Este, dicho jugador repetiría en la siguiente como Este, y así tantas veces como fuera necesario, por lo que las rondas pueden tener más de cuatro partidas.

Llamaremos "juego" al conjunto de las cuatro rondas. A estas alturas ya sabemos que un juego tiene cuatro rondas, y cada ronda al menos cuatro partidas, por lo que un juego tiene un mínimo de dieciséis partidas. En sets japoneses también se puede jugar una o dos rondas.

Antes de empezar a jugar una partida de Mahjong debe acordarse el número de rondas que se van a jugar o el tiempo que va a durar el juego. Normalmente se suelen acordar 1, 2 o 4 rondas, y si se establece por tiempo, 90 o 120 minutos.

En cuanto a la duración, una partida puede ser muy rápida, o no tanto. Teóricamente, podría dar la casualidad de que uno de los jugadores cerrase con las fichas que se le reparten (aunque la probabilidad es pequeñísima), pero lo habitual es tardar unos 20 minutos en cada partida. Así que lo más normal es que un juego de 4 rondas dure aproximadamente unas 2 o 3 horas, dependiendo de la rapidez de los jugadores a la hora de jugar y de construir la muralla.

Una vez situadas boca abajo todas las fichas sobre la mesa, los jugadores procederán a mezclarlas bien para que no queden rastros de ordenaciones anteriores. A continuación se construirá la denominada "muralla", utilizando todas las fichas, formada por cuatro paredes, cada una de 18 fichas de longitud (17 si no se emplean las flores y estaciones) y dos fichas de altura. Todas estas fichas estarán boca abajo. Las cuatro hileras se dispondrán como lados de un cuadrado, en el centro del cual se van dejando las fichas que se descartan.

El jugador que es viento Este toma dos dados y los tira; con el número obtenido, cuenta, en sentido antihorario y empezando por sí mismo, por qué lado de la muralla se cogerán las fichas. Por ejemplo, supongamos que el viento Este tira los dados y obtiene un 7, se cogerán las fichas por el lado del jugador Oeste (1 Este, 2 Sur, 3 Oeste, 4 Norte, 5 Este, 6 Sur y 7 Oeste).

Una vez que se sabe el lado de la muralla por el que se empezarán a coger las fichas, el jugador de ese lado del muro contará desde su esquina derecha de la muralla tantas hileras hacia la izquierda como el número que había salido en los dados; en nuestro ejemplo, el Oeste se detendría en la hilera 7; (como cada hilera es doble, en ese momento hay 14 fichas desde la esquina derecha hasta ese lugar). A partir de esa ficha, el jugador Este toma cuatro, y se las pone delante (es decir, se reparte a él el primero), que corresponderían en nuestro ejemplo a las hileras 7 y 8; luego el Sur se cogería otras dos hileras (la 9 y la 10), etc.; cuando cada jugador haya cogido cuatro fichas, se repite esta misma operación dos veces más, de modo que cada uno tenga 12 fichas; a continuación, cada jugador se coge una ficha más, siempre comenzando por el Este, con lo que todos tienen 13 fichas, y por último, el Este toma una última ficha para él solo, con lo que el viento Este queda con 14 fichas, y los demás con 13. El hueco dejado por las fichas que se han repartido se llama "brecha", poreso a veces el reparto inicial de tejas se denomina «abrir la brecha de la muralla». El juego ha comenzado.

Por último, contando 7 hileras desde la brecha hacia la derecha (es decir, serían las 14 fichas del final), estas fichas se denominan Muro muerto, y sirven para coger las fichas extra tras hacerse Kongs, mostrar una flor o estación en algunos sets o indicar el dora en sets japoneses. Las fichas del Muro muerto no se juegan por lo que si en la muralla no quedasen más fichas la partida se daría por concluida.

Una vez repartidas las fichas, el jugador que es viento de asiento ha quedado con 14 fichas en la mano. Si casualmente resultase que las fichas que le han correspondido forman mahjong lo diría de inmediato y esa partida habría finalizado; pero esto es muy raro, así que ha de intentar ir buscando mejores fichas. En sets japoneses es posible hacer un robo abortivo en su primer turno por lo cual la partida queda cancelada.

Entonces, el primer jugador descartará una de las fichas que tiene en su mano, y la pondrá sobre la mesa, boca arriba, en el espacio central bordeado por la muralla. Esto significa que ha terminado su turno, y (salvo que otro jugador robara una ficha descartada por el primero para crear Chow, Pung o Kong abierto, o que forzara a hacer mahjong que terminaría la partida) comienza el del jugador de su derecha, el cual procederá a robar la primera ficha de la muralla (la situada a la izquierda de la brecha recién abierta), será la ficha inferior de su fila, ya que la ficha de arriba ha sido repartida. Tras robar, queda con 14 fichas, y entonces (salvo que haya formado un mahjong en todos los sets, o un robo abortivo en los sets japoneses, con lo cual terminaría o cancelaría la partida respectivamente), localiza una ficha que quiera descartar, lo hace, y pasa el turno, y así sucesivamente.

Es importante señalar que los jugadores van tomando siempre la primera ficha libre de la muralla, es decir, la situada a la izquierda de la brecha; aunque no se ha mencionado hasta ahora, queda claro que se toman las fichas de arriba abajo, es decir, primero la ficha de arriba, luego la de abajo, luego la de arriba de la hilera siguiente, etc. Este principio por el que los jugadores van tomando por turno una ficha de la muralla queda afectado por algunas excepciones que se indican más adelante, las cuales permiten que un jugador pueda tomar la ficha descartada por otro, y que el orden de juego se altere, saltándose a algún jugador.

A medida que se vaya desarrollando la partida van a irse consumiendo progresivamente las fichas de la muralla. Generalmente la partida terminará cuando cualquiera de los cuatro jugadores consiga colocar sus fichas haciendo mahjong, pero si se diera el caso de que se consumieran todas las fichas válidas sin que ninguno lo hubiera conseguido, se comenzará una nueva partida, rotando los vientos asignados a cada jugador. Se consideran "fichas válidas" todas menos las 14 últimas que formaban el Muro muerto, es decir, en el momento que un jugador deja 14 fichas en la muralla y no ha podido hacer él mismo mahjong (salvo las llamadas "tenpai" en los sets japoneses), la partida ha finalizado sin vencedor. Téngase en cuenta que, aunque se va robando siempre en sentido de las agujas del reloj, es decir, siguiendo la brecha abierta en la muralla cuando se dieron las fichas al inicio, a veces se toman fichas del otro extremo de la muralla (esto ocurre cuando se hace "kong", como más tarde se verá), con lo cual las últimas 14 fichas de la muralla pueden ir "moviéndose" a lo largo de la partida.

Una vez finalizada la partida, y determinado el ganador y los perdedores, se procede al recuento de puntos que debe recibir el primero por parte de cada uno de los perdedores; esta operación se realiza con la ayuda de todos, ya que aunque la rivalidad entre estos sea grande mientras compiten, es normal colaborar para agilizar esta fase del juego, que es la más tediosa, especialmente si los jugadores son novatos.

Si el ganador fue el jugador que era Este, se repite la partida (o sea, los vientos no se rotan) y solo en sets japoneses se agregan un homba para la siguiente partida. En otro caso, los vientos se rotan en sentido antihorario. Es decir, el Este pasaría a ser Norte, el Sur sería Este, el Oeste sería Sur y el Norte sería Oeste. 

Tras esto, se mezclan las fichas, y se comienza una nueva partida. Si el jugador se quedase con puntaje negativo en sets japoneses, se acaba el juego. Lo mismo ocurre en sets japoneses si el jugador este gana la vía victoria y cierre ("agari-yame", あがりやめ).

En caso de que las fichas del muro se agoten (no cuentan las del muro muerto), la partida considera empate (se llama "ryūkyoku" en este caso) y dependiendo del set o del reglamento, podría rotar los vientos o repetir partida. En sets japoneses, además de optar por repetir partida o cambiar los vientos, para desempatar la partida, se llamará a "tenpai" si un jugador tiene una mano lista para determinar al(los) ganador(es) de la partida, además de un homba para la siguiente partida.

En el mahjong japonés, si 3 jugadores declaran mahjong al mismo tiempo (triple "ron"), es posible activar la regla de , que al hacer triple "ron" se considera empate. Debido a eso, los palos de 1000 puntos para hacer "rīchi" no pueden ser divididos en 3. Esa regla se considera robo abortivo, lo que cancela la partida en el proceso.

En sets japoneses, dependiendo de las reglas y cuando hay fichas disponibles, una partida se puede cancelar haciendo estos robos abortivos. En todos estos casos, las partidas canceladas cuentan como empate y, dependiendo de la situación, se repite la partida o se rota los vientos. El jugador puede cancelar la partida haciendo:

Para poder seguir explicando el desarrollo del juego necesitamos introducir estos tres conceptos. Un pung, pon o trío (碰, "pèng" o ポン) está formado siempre por tres fichas idénticas. Por ejemplo: tres dragones blancos, tres vientos Oeste, tres 7 de bambúes, etc. Un chow, chī o escalera (吃 o チー) está formado por tres fichas consecutivas del mismo palo. Es decir, son chow los conjuntos (1 discos, 2 discos, 3 discos), (7 bambúes, 8 bambúes, 9 bambúes), (4 caracteres, 5 caracteres, 6 caracteres), etc. Un kong, kan o cuarteto (槓, 杠, "gàng" o カン) está formado por cuatro fichas idénticas (los cuatro dragones blancos, los cuatro 7 de discos…), es decir, es lo mismo que un pung pero con cuatro fichas en lugar de tres. Sin embargo, las formaciones pung y chow son "normales" y la formación kong es "excepcional", como más tarde se verá. Obsérvese que no se pueden formar chow con los vientos ni los dragones, solamente es posible usarlos para formar pung o Kong.

Cuando un jugador tiene en su mano un pung, chow o kong, que ha conseguido ya sea bien porque se lo repartieron entre sus fichas iniciales, o que ha logrado a base de descartar y robar fichas de la muralla, se dice que estas formaciones son "ocultas". Una combinación oculta permanece así hasta que el jugador hace mahjong, es decir, no manifiesta al resto de jugadores que las tiene, excepto en el caso del kong. Si un jugador tuviese un kong y no dijera nada, no podría hacer mahjong, puesto que le faltaría una ficha para poder formar cuatro combinaciones y el par de ojos (ver el apartado siguiente). Por eso, si un jugador tiene un kong oculto, ha de declararlo antes de que finalice la partida (antes de finalizar cualquiera de sus turnos).

Para declarar un kong oculto, en cualquier momento dentro de su turno, el jugador baja ante sí las cuatro fichas del kong, y recibe una teja adicional, la cual se toma no del primer lugar libre de la muralla, sino del Muro muerto (esta zona de la muralla se llama también "caja de kong" o "caja de dora"). El kong que se ha bajado sobre la mesa se pone de forma que queden boca arriba las dos tejas de los extremos, y vueltas boca abajo las dos centrales (o a la inversa); este kong tiene la consideración de "kong oculto" o Kong cerrado, a pesar de que está sobre la mesa, puesto que ha sido confeccionado sin ayuda de otros jugadores. Otro modo equivalente de hacer esto es dejar tres fichas boca arriba y la del extremo derecho boca abajo. Además, en los sets japoneses se revela una ficha del muro muerto para indicar el dora y se mostrará otra ficha cada vez que altera el muro muerto al hacer kong.

El objetivo de todos los jugadores en cada partida es siempre el mismo: hacer mahjong. Una mano legal es de 14 fichas. Todos los jugadores tienen 13 fichas en su mano y solo se puede ganar una partida si un jugador roba una ficha de la muralla (victoria por robo de la muralla, "Tsumo" en sets japoneses) o robando una ficha que haya descartado otro jugador (victoria por descarte, "Ron" en sets japoneses). Sin contar con otro tipo de manos, el objetivo es obtener 4 combinaciones y el par de ojos. La lista de las combinaciones es la siguiente:

Normales

Especiales

Una variante del juego es jugar limitando la cantidad de chow permitidos a uno, de esta manera se agrega dificultad al juego y las siguientes combinaciones no serían válidas:


Cuando el jugador le falta una ficha para ganar (por ejemplo: , esperando: , , o , si cuenta como el par de ojos), la mano contará como mano lista (). Es común que una mano esperara 2 o 3 fichas, y algunas variantes aumentan puntos si se declaran mano lista. Esto se declara en algunas variantes de mahjong, pero no en otras.

En sets japoneses y coreanos, si el jugador no ha declarado fichas (salvo los kong ocultos), pueden declarar "rīchi" (立直, fonéticamente conocido en inglés como "reach"), colocando uno de sus palos de 1000 puntos a la mesa (a modo de apuesta) y la ficha en horizontal boca arriba. Los jugadores con "rīchi" declarado no pueden interrumpir para pedir chows, pungs ni kongs, y descartará todas las fichas que han robado de la muralla. Solo se pueden interrumpir para declarar kongs ocultos o para declarar mahjong. Los jugadores que hicieron "rīchi" pero pierden, son penalizados en puntaje. Los jugadores que hicieron "rīchi" y ganan, aumentará su puntaje en mayor medida además de mostrar la(s) ficha(s) inferior(es) del muro muerto.

Como se ha dicho anteriormente, cada jugador en su turno roba de la muralla (con lo cual momentáneamente tiene 14 fichas), y luego, si no tiene mahjong formado, descarta una, volviendo al número de 13. Pero muchas veces le gustaría tomar la ficha que alguien ha descartado en lugar de tomar una de la muralla, lo cual podrá hacer de esta manera:

Si cualquier jugador descarta una ficha, y otro tiene en la mano dos fichas idénticas a la descartada, puede reclamar para sí la ficha descartada, diciendo "pung" ("pon" en los sets japoneses). Al hacerlo, el juego interrumpe la secuencia normal, y se prosigue el turno por el jugador que reclamó el descarte, quien debe descartarse entonces de una ficha, continuando luego normalmente. Pongamos un ejemplo: Norte roba de la muralla y se descarta de un 4 de caracteres. Le tocaría jugar al Este, pero el Oeste tiene en la mano dos 4 de caracteres, y dice "pung". En ese momento, el Oeste toma de la mesa el 4 de caracteres y la coloca a la vista junto a las dos que tenía ocultas y descarta otra ficha. Prosigue jugando de nuevo el Norte (que es el jugador siguiente al Oeste), etc.

Observaciones importantes:

Esta regla se puede modificar optando que en cuanto el jugador siguiente robo de la muralla se perdió el derecho a reclamar el "pung", esto incrementa la dificultad del juego y hace que se esté muy atento al mismo.

Las reglas son similares al caso del pung, pero diciendo "kong" ("Kan" o Quad" en los sets japoneses y americanos, respectivamente) en lugar de "pung". Un kong también tiene preferencia sobre el chow. Cuando se consigue un kong de este modo, pidiendo una ficha descartada, el kong que resulta es de tipo "expuesto", y se pone frente al jugador con todas las tejas vistas durante el resto de la partida.

Si un jugador se descarta de una ficha, solo podrá reclamarla para formar un chow con ella el jugador siguiente; para hacerlo, dirá la palabra "chow" ("chi" en los set japoneses), y en lugar de robar de la muralla tomará la ficha recién descartada. Es necesario que este jugador tenga ya otras dos fichas del chow que está formando, y como en el caso del pung, volverá frente a él el chow, que queda entonces "expuesto" el resto de la partida.

Observaciones:

Cuando un jugador está a falta de una sola ficha para hacer mahjong, puede pedirla en caso de que la descarte cualquier jugador o lo robe de la muralla o del muro muerto, en caso de kong; lo hará diciendo "mahjong", tomará la ficha de la mesa, y la partida habrá concluido, volteando boca arriba todas las fichas y pasándose a calcular la puntuación. Obsérvese que para hacer mahjong podemos pedir cualquier ficha, incluso aunque sea para el par de ojos.

La primera forma de ganar es que se puede declarar "mahjong" si un jugador intercepta el descarte o la formación de kong de otro (en los sets japoneses dirán "ron"). Si una ficha es deseada por otros jugadores para pung, kong o chow, tiene prioridad aquel que la solicite para hacer mahjong y, en sets japoneses, intercepta al jugador que hizo kong. Si son varios los jugadores que reclaman la teja para hacer mahjong (Doble o Triple "ron" en sets japoneses), la conseguirá el que esté situado primero respecto de quien descartó la ficha, contando siempre en el sentido del juego (es decir, en sentido antihorario) o se comparará en puntaje siempre en el sentido del juego a los jugadores que tengan la mejor mano. 

La última forma de ganar es que también se puede declarar "mahjong" si un jugador roba una ficha necesaria de la muralla, o del muro muerto en caso de kong (en los sets japoneses dirán "tsumo"). En ese caso, el puntaje al declarar mahjong por robo es mayor que declararlo por descarte. 

Los puntos extra también aumenta el puntaje si el viento este (el jugador que inició el turno) ganó al sacar la ficha de la muralla, o uno de los jugadores reclama la ficha al jugador Este para completar la mano de victoria.

Una vez determinado el jugador que ha hecho mahjong en una partida, se contarán los puntos que ha conseguido, que le serán pagados por los otros tres jugadores; sin embargo, los otros tres jugadores no pagan los mismos puntos entre sí, ya que hay que tener en cuenta quién descartó la ficha que sirvió al ganador para hacer mahjong (si es que no ha ganado robando de la muralla). Un perdedor paga doble si descartó la ficha que sirvió para hacer mahjong al ganador.

Una variante es obviar esta penalización por el descarte, evitando que en el caso de que el ganador sea el viento reinante el que descarto la ficha pague cuádruple (doble por la penalización y doble por ganar el viento reinante de la vuelta).

Pongamos algunos ejemplos (los puntos básicos conseguidos todavía no sabemos calcularlos, así que los fijaremos arbitrariamente):

Caso 1

Caso 2

Caso 3

Recordemos que, además, en todos los casos si el ganador había repartido las fichas (es decir, si era "viento de asiento"), el hecho haber ganado le da derecho a repetir de nuevo, y esto tantas veces como sea capaz de ganar.

Resaltemos que lo primero que ha de hacerse es la conversión de los faan obtenidos en puntos, y luego duplicar estos puntos si procede. Por ejemplo, si se obtienen 6 faan, esto se corresponde con 16 puntos, y si algún jugador debe pagar doble, pagará 32 puntos. En sets japoneses, en vez de Faan, utiliza múltiplos para la puntuación.

Bien, pero aún queda por ver el modo que permite calcular lo que hemos llamado "puntuación básica" de la jugada. Este cálculo no se hace directamente en puntos (como en sets japoneses), sino en "fan" (como en sets chinos o de otras variantes), y de aquí una tabla nos da la puntuación directamente.

La tabla de pagos es la siguiente:

Por ejemplo, si calculamos que una jugada tiene 5 fan, y mirando en la tabla observamos que estos 5 fan corresponden con 16 puntos, que sí será ya la puntuación básica, a la que aplicaremos las correcciones de duplicación descritas, si procede. Nos resta la descripción del sistema de asignación de fan a un mahjong concreto; esta operación condiciona todo el juego, ya que los jugadores procurarán conseguir unas u otras jugadas teniendo en cuenta lo valiosas que sean. Hay una serie de observaciones y reglas que pueden mencionarse:

En el apartado que sigue se explica el sistema de asignación de faan. Por último, es importante resaltar que un ganador de mahjong puede obtener muy pocos puntos, o cientos en una sola partida, dependiendo de lo afortunada que sea, por lo que al final un jugador que cierre muy pocas veces pero con combinaciones difíciles puede obtener muchos más puntos que otro que cierre a menudo con combinaciones de baja puntuación.

Las reglas que a continuación se exponen van ordenadas en número de fan. Estas reglas no son excluyentes entre sí, y de hecho se aplicarán todas las que sea posible a cada mahjong; se van indicando los faan obtenidos, y a continuación las circunstancias que propician este logro. Por otra parte, hay que tener en cuenta que cuando se hable de combinaciones vistas u ocultas nos referiremos siempre a pung, chow o kong, el par de ojos no entra en esta consideración.







Las fichas de flores y estaciones son fichas suplementarias que solo sirven para sumar puntos. Está disponible en sets chinos o de algunas variantes y de forma opcional en sets japoneses y de 3 jugadores.

Cuando un jugador roba una flor de la muralla, inmediatamente la vuelve frente a sí, en la zona en que se colocan las combinaciones vistas, y vuelve a tomar otra ficha (ya que la flor no cuenta para hacer ninguna combinación, y de no hacerlo así se quedaría con una ficha de menos); la nueva ficha que reemplaza a la flor se tomará del muro muerto, no del lugar normal de donde se toman las fichas. Si recibe una segunda flor, se repite esto mismo, etc.

Nada más repartirse las fichas, al inicio del juego, cada jugador descubre las flores que ha recibido (si ha recibido alguna, claro está), e irán cogiendo por orden (primero el Este, luego el Sur, después el Oeste y por último el Norte) las fichas que reemplazan a las flores, tomándolas siempre del muro muerto, y empezando por sí mismo; si entre las nuevas fichas hay flores se repite la operación, como en el caso anterior se ha dicho.

El resto del juego es de la misma manera, así que el único efecto de las flores es incrementar los faan obtenidos.

Es relativamente frecuente, sobre todo entre principiantes, cometer algún error o infracción. La más grave consiste en cantar mahjong, y cuando se está calculando la puntuación, descubrir que no se había obtenido; a esto se llama "falso mahjong", y la pena consiste en que quien lo cometió paga los puntos correspondientes a 2 faan a cada uno de los otros tres jugadores, sin aplicarse entonces reglas que puedan duplicar esta cifra. La partida se repite, sin que cambie el viento de asiento. Aunque parezca una regla dura, es conveniente aplicarla sin dudar.

Otro error corriente entre novatos, cometido generalmente por descuido al robar de la muralla, consiste en quedar con un número mayor o menor de fichas de las que se deberían tener. Estos casos se llaman "gran eunuco" y "pequeño eunuco", respectivamente. Aquí no hay penalización, salvo si el jugador canta mahjong, que necesariamente sería falso, simplemente el "eunuco" no puede ganar la partida. Por eso, los jugadores en esta circunstancia están interesados en llegar a partida nula, es decir, procurarán que se agoten todas las fichas válidas de la muralla.

En los otros errores conviene ser indulgente; así, si un jugador dice "pung" y luego resulta que no tiene las dos fichas necesarias, bastará con deshacer la jugada y continuar normalmente, etc.

Todas las variantes de mahjong tienen reglamentos y puntajes distintos, además de cambios en el número de fichas y eventualmente más manos limitadas y puntajes más complejos. Estos son los reglamentos actuales para cada variante:

En occidente, el mahjong se le conoce principalmente solo indirectamente a través de versiones de juego para solitarios, llamados "Shangais", pero conocidas popularmente con el mismo nombre de Mahjong; dichos juegos presentan reglas más simples y cuya regla general es emparejar fichas de Mahjong, siendo los "solitarios Shangais" popularizados por internet a través de juegos en línea.




</doc>
<doc id="9821" url="https://es.wikipedia.org/wiki?curid=9821" title="Magic: El encuentro">
Magic: El encuentro

Magic: The Gathering y frecuentemente abreviado como Magic, MTG y Cartas Magic, es un juego de cartas coleccionables diseñado en 1993 por Richard Garfield, profesor de matemáticas, y comercializado por la empresa Wizards of the Coast. "Magic" es el primer ejemplo de juego de cartas coleccionables moderno, con más de seis millones de jugadores en cincuenta y dos países diferentes. "Magic" puede ser jugado por dos o más jugadores, cada uno de ellos usando un mazo individual. También existen varias versiones digitales que pueden jugarse en línea, en videoconsola o PC.

Cada partida de "Magic" representa una batalla entre poderosos magos (en el juego conocidos como planeswalkers), en el que cada uno de estos es uno de los jugadores de la partida. Los jugadores pueden usar hechizos (conjuros, artefactos, tierras, criaturas fantásticas, etc.), representados individualmente en cada carta, para derrotar a sus oponentes. De este modo, el concepto original del juego se inspira de forma notable en los duelos de magos típicos de los juegos de rol tradicionales, como "Dungeons & Dragons". La estructura del juego reemplaza los útiles usados en los juegos de aventura de papel y lápiz por una gran cantidad de cartas y unas reglas más complejas que la mayoría de otros juegos de cartas.

Un sistema organizado de torneos y una comunidad de jugadores profesionales se ha desarrollado alrededor del juego, así como un mercado secundario de cartas. Las cartas de "Magic" son valoradas no solo por su escasez, sino también por su valor en el juego, su antigüedad, por ser cartas de "culto" o por el valor estético de las ilustraciones.

En "Magic: The Gathering" cada jugador, debe derrotar a su enemigo usando sus poderes: criaturas mágicas, artefactos, y encantamientos, extrayendo el poder o maná de sus tierras, algunas también con sus propios poderes. El juego fue ideado para dos jugadores, aunque se han agregado nuevas reglas para jugar con más participantes, casi siempre en números pares ya que si es un número impar, por ejemplo tres jugadores, puede que dos se alíen para derrotar al otro, lo que constituye una práctica incorrecta.

Cada jugador empieza con veinte puntos de vida y un mazo («biblioteca») de al menos sesenta cartas (en algunos torneos especiales está permitido usar mazos de cuarenta), de las cuales aproximadamente (por consideraciones estratégicas) un tercio son tierras productoras de maná, la energía requerida para jugar las demás cartas (piensen en el Maná como dinero dentro del juego, que se usa para pagar los costes de las cartas a jugar). Las cartas que no son tierras se pueden agrupar colectivamente como hechizos. Requieren una determinada cantidad de maná para ser jugadas, generalmente una combinación de maná de un solo color e incoloro. Se dividen en las cartas que generan un efecto permanente (criaturas, encantamientos, artefactos e incluso las tierras son permanentes existentes en juego) y las que van al cementerio (por norma general) después de ser usadas (instantáneos y conjuros).

Un jugador pierde cuando se queda sin puntos de vida, cuando intenta robar una carta y su biblioteca está vacía, si obtiene diez contadores de veneno o si la condición de alguna carta así lo indica. El último jugador que queda es el ganador, aunque existen cartas que indican nuevas formas de ganar una vez que un jugador las pone en juego, como ganar la partida si ese jugador consigue determinados puntos de vida o si consigue tener en juego cierto número de criaturas, también existen habilidades por las cuales un jugador no puede perder una partida por determinada circunstancia, como por ejemplo: no puede perder si se queda sin puntos de vida o no puede acumular contadores de veneno.

Aunque el juego fue ideado para 2 o más jugadores, existen reglas creadas por jugadores que permiten el juego en solitario. Como por ejemplo la campaña de la Tumba de Eregorn, donde un jugador ha de atravesar cuatro escenarios hasta llegar a hacerse con el poder de la Joya de Eregorn. Una campaña ideal para probar barajas versátiles contra todo tipo de barajas y colores y para poder desempolvar todas aquellas cartas que ya no utilizan los jugadores.

En "Magic" hay cinco colores diferentes: blanco, azul, negro, rojo, y verde, cada uno con un significado y personalidad diferente. Cada color representa por así decirlo un estilo de magia.






Excepciones:



Un jugador necesita un mazo antes de que pueda jugar una partida de "Magic". Los principiantes usualmente comienzan con un mazo preconstruido. En muchos formatos de juego, los jugadores pueden modificar sus mazos con cualquiera de las cartas que posean, según la técnica del contrincante, su estilo de juego, o incluso anticiparse al mazo de un oponente.

En general, los mazos deben estar compuesto por un mínimo de sesenta cartas. Los jugadores no pueden usar más de cuatro copias de cualquier carta, con excepción de las llamadas «tierras básicas», que actúan como una fuente estándar de recursos en "Magic". Estas dos reglas son menos flexibles en los formatos "", donde el tamaño mínimo de mazo es de cuarenta cartas. Dependiendo del tipo de juego, algunas cartas más poderosas son "restringidas", esto es, solamente una sola copia es permitida por cada mazo; o "prohibidas", es decir, su uso no está permitido.

La decisión acerca de qué colores usar es parte importante en el proceso de crear un mazo. Los hechizos en "Magic" vienen en cinco colores distintos, cada uno con sus fortalezas y debilidades; de ahí que el jugar con más de un color puede ayudar a dar más versatilidad y mejor rendimiento a un mazo. Sin embargo, el reducir el número de colores usados incrementa notablemente las posibilidades de robar las tierras que se necesitan para jugar los hechizos más importantes. Por tanto, en la práctica, los mazos de uno o dos colores son los más comunes, los mazos de tres, cuatro o incluso cinco colores pueden ser exitosos si están bien diseñados.

Como ya se ha mencionado anteriormente, un mazo está compuesto por un mínimo de sesenta (60) cartas, pero adicionalmente un jugador puede crear un "banquillo" o sideboard de cartas que se pueden intercambiar por cartas del mazo principal. Durante los torneos por norma oficial para poder ganar una ronda se requiere ganar dos de tres partidas (o al mejor de cinco en competiciones de alto nivel. Por ej.: a partir de los cuartos de final en los Pro Tour), aquí es cuando entra el uso del banquillo. El banquillo está conformado principalmente por cartas cuyos efectos son muy circunstanciales/específicos y que serían muy útiles en unas ocasiones mientras que en otras solo estorbarían. Después de la primera partida, ya sabiendo a que clase de mazo se enfrentan los jugadores, se les permite modificar sus mazos con el banquillo que traigan (así sepan de antemano a que mazo se enfrentaran no pueden realizar modificaciones en la primera partida), un ejemplo de esto es que si un jugador se enfrenta a un mazo negro, y en su banquillo tiene cartas con protección contra negro, después de la primera partida podrá integrar esas cartas a su mazo y retirar otras que no le son útiles contra ese mazo. El banquillo, con las nuevas normas, puede tener hasta quince cartas y no es necesario incluir el mismo número de cartas que se sacan del mazo en los cambios.

Wizards of the Coast organiza todo tipo de reuniones, desde torneos locales hasta torneos profesionales (llamados "Pro Tours") donde es necesaria una invitación para poder participar. También existen campeonatos regionales, nacionales y un , que se celebran anualmente.

Una organización llamada "Duelist Convocation International" (DCI) es la encargada de establecer las reglas oficiales, proporcionar los mecanismos para organizar y sancionar todos estos torneos, organizar el colectivo de jueces oficiales de torneo, etc.

Existen varios tipos de formatos en función de las cartas que están permitidas en el juego. Según el tipo de torneo, podrán existir algunas cartas restringidas (es decir, se puede emplear solo una copia de ellas en un mazo) y otras prohibidas. De las demás (salvo las tierras básicas) se pueden emplear hasta cuatro copias en un mismo mazo.
Los torneos se agrupan en tres tipos:

Un bloque consta de 2 o 3 expansiones(ediciones o series) de cartas que llevan la misma trama histórica en el universo de Magic y se editan habitualmente un bloque al año.
Como recomendación a los jugadores nuevos aunque la regla dice que se pueden jugar más de 40 cartas en los formatos limitados, lo ideal es jugar las 40 cartas exactas y no más de 3 colores para tener un mazo más competitivo.

También existe otra clasificación de los tipos de torneo, según el nivel de competitividad:




Los jugadores a veces juegan formatos casuales a partir de distintos criterios. Por ejemplo: jugar únicamente con cartas comunes para limitar el costo del mazo, jugando con cartas de un solo bloque a elección o bien adoptando uno de los formatos sancionados y modificar su lista de cartas prohibidas.

"Commander" (conocido anteriormente como EDH - Elder Dragon Highlander) es un formato casual donde los jugadores eligen una criatura legendaria que se denomina, "comandante". Luego el mazo se arma con 99 cartas únicas que deben tener la entidad de color del comandante. Desde hace algunos años "Commander" fue homologado por Wizards. La lista de baneos está regulada por un ente no oficial. Wizards of the Coast lanza mazos de commander prearmados una vez por año. 

Otro formato muy popular para jugar limitado es el Cubo. Tiene una estructura similar al "Booster Draft", solo que los jugadores utilizan una colección de cartas preseleccionadas en lugar de utilizar sobres sellados. 

Aunque Wizards of the Coast solo vende cartas en sobres y mazos, existe un mercado activo de cartas sueltas entre los jugadores y en muchas tiendas especializadas.

Los precios de las cartas dependen, en buena medida, de su utilidad y su rareza (en los sobres vienen 11 Comunes, 3 Infrecuentes, 1 Rara y, ocasionalmente en las nuevas ediciones, una Mítica o Mithyc Rare).
Las cartas comunes suelen costar 0,08 €, y a veces superan los 0,30, mientras que las infrecuentes suelen costar entre 0,50 y 1 €. Las cartas raras, sin embargo, tienen precios muy superiores, a partir de 1 o 2 euros en adelante. Muchas cartas raras y algunas infrecuentes particularmente útiles o antiguas, y necesarias en la mayoría de mazos competitivos, cuestan más de 30 €.

Hay más de cien cartas que se venden unitariamente por más de 100 €, desde muchas pertenecientes a las primeras ediciones (Alfa/Beta, Legends, Antiquities...) a algunas más recientes debido al nuevo tipo de rareza, "Mithyc Rare", que las hace aparecer de forma ocasional en los sobres. Hoy en día, la versión Alfa firmada por el artista de la carta Black Lotus es considerada la carta "Magic" no promocional jamás impresa, sin contar las cartas con falla de impresión.

Hay una edición especialmente costosa, la llamada "Summer Magic", una tirada concreta de Revised|Revised Edition (Tercera Edición Core) que fue retirada del mercado antes de vender poco más de unos cientos de sobres; esta edición incluye las cartas legales más caras de la historia del juego, alcanzando más de 100€ una simple carta común (que normalmente vale céntimos de euro) y más de 8000€ si hablamos de la tierras dobles, debido a la extrema escasez de estas cartas y la casi imposible dificultad para encontrarlas en el mercado secundario. 

Existen también tres rarísimas cartas creadas directamente por Richard Garfield que, aunque no son legalmente jugables en torneos, están valoradas a precios muy superiores. Dos de estas cartas fueron editadas para conmemorar el nacimiento de sus respectivos hijos y hay 100 copias de cada una de ellas. La tercera fue creada para declararse y solo existen 2 copias de las mismas. Las cartas más antiguas suben el precio, pero más que nada a causa de la fuerte especulación que existe en el mercado.

Existen varios medios donde se puede buscar la «cotización» de las cartas de "Magic", pero hay que tener en cuenta que puede depender fuertemente del país donde uno se encuentre. En algunos casos la cotización de las cartas se ve influenciada por los intereses de las entidades que controlan el mercado. Por ejemplo, si una revista que publica los precios de las cartas, a la vez se dedica a venderlas, adjudicará un precio menor a las cartas que no abunden en su "stock". En cualquier caso, la gente que habitualmente juega torneos y comercia con cartas sabe el valor real de estas, siendo el precio dictado por algunos medios una simple orientación.

A medida que salen a la venta nuevas ediciones, a veces se van reeditando cartas antiguas. Si una carta es cara porque es útil, la reedición suele hacer que suba el precio de la carta original, porque hay más formatos de torneos en que se puede jugar con ella y por tanto tendrá una mayor demanda entre los jugadores. Sin embargo, si su precio se debe al coleccionismo que suscita, una reedición bajará el precio de la carta original, porque ya no es tan escasa.

Para proteger el valor de algunas cartas antiguas y muy deseadas, Wizards of the Coast ha formulado una política oficial de reediciones, que incluye una lista de cartas que han declarado no volver a editar; en total unos pocos cientos de cartas, desde Alfa/Beta hasta Destino de Urza, con el fin de proteger los intereses del coleccionista. Algunas de estas cartas son las pertenecientes al Power Pack (Black Lotus, los cinco Mox, Ancestral Recall, Timetwister y Time Walk), las diez Tierras Dobles originales, el Bazaar of Baghdad y la Mishra's Workshop.

Con tres o cuatro expansiones nuevas por año, muchos jugadores critican el hecho de que se requiere una importante inversión para mantener una colección de cartas que sea competitiva o completa. El principal formato competitivo, Estándar o Tipo 2, solo usa cartas del último bloque completo (un bloque es un conjunto de tres expansiones consecutivas con temas y mecánicas comunes. Ej: Bloque Kamigawa - Campeones de Kamigawa, Traidores de Kamigawa y Salvadores de Kamigawa-), el bloque de la última expansión impresa y el último "Core Set" («baraja principal», por ejemplo la de la novena edición); de esta forma se obliga a los jugadores que juegan competitivamente a tener las últimas cartas que van saliendo. Otros formatos como Extended (Tipo 1.X), Legacy (Tipo 1.5) y Vintage (Tipo 1) oficiales), compran los mazos de torneo con bordes dorados (que también están prohibidos en torneos oficiales debido a que son relativamente muy baratos y a que contienen muchas cartas raras y caras) o usan alguno de los programas para jugar en línea a "Magic", como el Apprentice, el "Magic" Workstation o el OCTGN.

Debido a lo mencionado anteriormente, se critica que en estos tipos de juegos en muchas ocasiones los jugadores, más que enfrentarse en un duelo de habilidades, se enfrentan en un duelo entre los precios de las barajas; ya que muchas de las cartas necesarias para que un mazo sea competitivo son de un gran valor (y asimismo de una gran importancia). Así, se limita el acceso al juego competitivo a personas que, aunque jueguen bien no pueden acceder a las cartas más caras, y de esta forma, los jugadores que cuentan con más dinero son los que obtienen más ventaja, y no los que cuentan con más habilidad. Aun así, el hecho de manejar una baraja de mayor precio, no asegura una victoria, puesto que el factor habilidad y la picardía del jugador, aunque puedan parecer menos importante, son unas bazas grandes a la hora de desenvolverse en partidas de torneos.

"Magic", como muchos otros juegos, combina azar y suerte con habilidad y talento. Una queja común es, sin embargo, de que hay mucha suerte involucrada con el recurso básico del juego: las tierras. Muchas tierras o muy pocas (“Mana Flood” o “Mana Screw” en la jerga del juego respectivamente) en el inicio del juego principalmente, pueden arruinar la posibilidad que el jugador tiene de alcanzar la victoria sin que siquiera haya cometido un error. Una respuesta común a esta queja es que la influencia de la suerte en el juego puede ser minimizada armando apropiadamente un mazo sólido que haya sido probado extensivamente. Una cantidad precisa de tierras y buenas técnicas de mezclado del mazo pueden reducir la posibilidad de cualquier problema de maná. También hay cartas que son muy utilizadas y que tienen por objeto reducir la dependencia al maná del jugador. La cantidad de tierras en la mayoría de los mazos competitivos varia de 18 a 26 (un promedio de 22 o 21 tierras por mazo), aunque el uso de ciertos hechizos (como Seething Song), de artefactos (como los talismanes de Mirrodin o los Moxes de Alfa/Beta), de criaturas (como las Aves del Paraíso o los Elfos De Llanowar) o de tierras especiales (como las Watery Grave, Windswept Heath o Underground River) y el costo relativo de los principales hechizos de un mazo pueden aumentar o bajar sustancialmente la cantidad de tierras requeridas (Ej: Un mazo con hechizos de un solo color y de muy bajo coste necesita menos tierras que un mazo de muchos colores y hechizos más costosos).

La llamada «regla del Mulligan» fue introducida posteriormente al juego, primero como una opción para partidos informales y más tarde en las reglas oficiales del juego. La regla moderna del Mulligan permite a los jugadores que no estén satisfechos con su mano inicial poder mezclarla en el mazo y luego robar de nuevo una mano inicial pero con una carta menos. Esto se puede repetir cuantas veces se desee, pero cada vez que se lo haga se deberá robar una carta menos (primero seis, luego cinco, luego cuatro y así sucesivamente). Generalmente, se considera como una «buena mano» a una en la cual se tiene tres o cuatro tierras; y en muy raros casos se utiliza la regla del Mulligan más de 2 o 3 veces ( ya que a la segunda vez que se la utiliza se roban solo 5 cartas). Una reciente actualización de esta regla indica que ambos jugadores tienen que hacer mulligan simultáneamente Esto no se refiere a que ambos jugadores deban coincidir en querer hacer mulligan, si no a que deben decidir si van a hacer mulligan o no en el mismo momento, empezando por el jugador que va a empezar la partida. Anteriormente hasta que el jugador que iba a comenzar la partida no terminaba de hacer sus mulligan, el otro jugador no podía realizar sus mulligan. Con este anterior sistema se perdía mucho tiempo en establecer la mano inicial.

La vieja regla de Mulligan, todavía usada ocasionalmente en ciertos círculos de juego casual y en algunas partidas multijugador por internet, permite robar de nuevo siete cartas (una sola vez, luego normalmente) si la mano inicial contiene cero, una, seis o siete tierras. Una excelente fuente de información sobre el Mulligan puede encontrarse en el artículo de Mark Rosewater: “Starting Over”.

Recientemente se ha añadido una nueva regla al Mulligan que consiste en una vez acabado el proceso de mulligan, si tienes menos cartas de las iniciales puedes hacer scry de 1, es decir, después de hacer los mulligans que consideres necesarios, puedes ver la primera carta de la biblioteca y devolverla a la parte de arriba o ponerla en el fondo.

Con la colección M20 la regla del Mulligan ha vuelto cambiar. Este nuevo mulligan se probó en el Mythic Championship II de Londres y en Magic Online (London Mulligan). Básicamente, cada vez que hagas mulligan, robas hasta siete cartas y luego pones una cantidad de cartas de tu mano igual a la cantidad de veces que has hecho mulligan durante ese juego en el fondo de su biblioteca en el orden que tú quieras. Tu mano inicial continuará teniendo una carta menos por cada vez que hagas un mulligan, pero siempre podrás escoger esa mano inicial de entre siete cartas.

Internet ha jugado un importante papel en el "Magic" competitivo. Discusiones sobre estrategias y reportes de torneos incluyen frecuentemente un listado de lo que contiene exactamente un mazo y de la descripción de su rendimiento contra otros mazos. Usando un proceso conocido como “Net Decking”, algunos jugadores navegan la internet en busca de esta información y arman un mazo (sin siquiera, tal vez, haberlo jugado nunca antes) conteniendo las mismas, o muy similares, cartas, dependiendo de, esta forma, de la destreza y la experiencia de otros jugadores. Si bien esta técnica es a menudo una muy buena opción, no es seguro de que el mazo (por más de que sea exactamente el mismo) repita su éxito pasado. El jugador podría ser inexperto, podría no estar familiarizado con el funcionamiento del mazo o podría, simplemente, jugar en un metagame en el cual, el mazo que se armó es ineficaz y/o contra el cual los otros mazos están preparados.
Hoy en día armarse mazos directamente de Internet es una técnica muy extendida (debido al fácil acceso que se tiene a la Web y a la proliferación de sitios dedicados especialmente a publicar mazos y reportes de torneos) y utilizada por toda clase de jugadores, aunque es cierto también, que la mayoría de estos prueba el mazo en Internet antes de armárselo en la realidad.

Una intrincada trama yace tras las cartas lanzadas en cada expansión y es mostrada en el diseño y en el "texto de ambientación" de cada una de las cartas, así como en las novelas y antologías publicadas por Wizards of the Coast (y formalmente por "HarperPrism"). Esta trama toma lugar en el multiverso de "Magic", originalmente llamado "Dominia" pero que cambió para evitar confusión con Dominaria, y multiverso que consiste en un número infinito de planos. Los objetos o personajes de la trama aparecen como cartas "Legendarias" en las diversas expansiones, cartas de las que solo puede haber una en el campo de batalla a la vez.

Las barajas de expansión que van desde "Antiquities" hasta "Azote" (con la excepción de "Tierras natales") son barajas ambientadas en el plano de Dominaria y conforman una línea temporal aproximadamente cronológica de la historia de ese plano (con la sola excepción del Bloque de la "Saga de Urza"). Los personajes frecuentes incluían a Urza y a su hermano Mishra.

Las barajas desde "Vientoligero" hasta "Apocalipsis" siguen en particular la historia de la tripulación del "Vientoligero", aliados de Urza contra Yawgmoth, así como la lucha final de Dominaria con pirexia. "Magic" comenzó a aventurarse fuera de Dominaria y entrar en variados otros planos, tales como Mirrodin (un mundo hecho de metal creado por el Golem planeswalker Karn, en el cual su primer trama se centraba en el derrocamiento del loco guardián mernach que lleno el plano con seres orgánicos de otros mundos en busca de convertirse en un dios), Kamigawa(un mundo que recuerda mucho al Japón feudal, cual trama gira alrededor de una guerra que surgió entre sus dioses y los mortales, y que además cronológicamente se ubica más atrás que la saga de urza), y Ravnica (un plano donde básicamente el paisaje natural y la civilización se ha fundido completamente formando una mega metrópolis gobernada por diez gremios, su primer trama central se centra en el conflicto de poderes entre ellos lo que conllevara a la ruptura de un pacto de no agresión de un milenio). La trama de "Magic" retornó a Dominaria con el bloque de "Espiral del tiempo", y visitó a Lorwyn con el bloque de "Lorwyn", el bloque de "Páramo Sombrío", es la baraja ambientada en el plano de Páramo Sombrío, el inverso oscuro de Lorwyn. En el bloque de Alara se exploró los cinco diferentes planos resultantes de la fragmentación del plano Alara y su restauración como la nueva Alara en donde se empieza a introducir la trama de los nuevos "planeswalkers". En el bloque de Zendikar se explora un indomable plano lleno de tesoros y peligros, que además es la cárcel de los Eldrazi, una especie de criaturas procedentes de otra dimensión incomprensible y que resulta la más peligrosa del universo de "Magic". En el ciclo de cicatrices de Mirrodin se regresa al plano de metal, donde resurgen la civilización maligna de pirexia y el retorno del creador de Mirrodin Karn. En Innistrad, se explora un plano donde las peores pesadillas de los humanos son más que reales. En el ciclo de retorno de ravnica, volvemos al plano metrópolis, donde la pelea entre los gremios se ha intensificado. En theros nos adentramos en un plano que recuerda a la antigua mitología griega donde, héroes y monstruos pelean sin cuartel y los antiguos Dioses dominan el mundo. En Tarkir exploramos las dos líneas temporales de este mundo, una en la que los dragones fueron extinguidos por cinco clanes guerreros, siendo la tumba del planeswalker Ugin, y otra en la que los dragones gobiernan el mundo y el planeswalker Ugin despierta de un largo letargo, cuya intervención será importante en el futuro de otros mundos. 

Desde la expansión "Magic: Orígenes", se da a conocer a más detalle el pasado de cinco planeswalker en particular (Gideon Jura, Jace Beleren, Liliana Vess, Chandra Nalaar, y Nissa Revane), quienes tendrán más protagonismo en futuras historias. En los bloques, "batalla por Zendikar" y "sombras sobre Innistrad" se regresa a dos planos conocidos ambos al borde del apocalipsis, en el primero los titanes Eldrazis Ulamog y Kozilek están a punto de arrasar con el plano pero son detenidos por un equipo de planeswalker y los últimos sobrevivientes de ese mundo; por el otro lado Innistrad, un mundo oscuro y siniestro que había encontrado un respiro y una paz que parecía perdurar por la presencia de su guardiana Avacyn ve un atroz revés cuando ella y los demás ángeles caen en la locura y están dispuestos a erradicar toda vida del plano, pero tras su locura hay un mal peor, la llegada del último titán Eldrazi, Emrakul, cuya presencia corrompe la vida desde sus entrañas. En el bloque Kaladesh vemos con mayor profundidad y detalle el plano de original de la planeswalker chandra, donde la ciencia, la tecnología, el artificio y una sutil tiranía ha suplantado casi por completo la magia, y las tensiones de una revolución florecen en medio de una feria de inventores, pues el planeswalker Tezzeret subordinado de Nicol Bolas esta haciendo su trabajo sucio en aquel plano lo que conllevara a los guardianes con un nuevo miembro (Ajani Goldmane) pasen a su siguiente misión. En el bloque de Amonkhet siguiendo la pista de Tezzeret los guardianes se adentran en el plano del mismo nombre que es un mundo que recuerda mucho al antiguo Egipto, donde cinco dioses someten a sus habitantes vivos a pruebas para buscar a los que sean dignos a servir al dios Faraón quien no es otro que el oscuro dragón Nicol Bolas quien corrompió aquel mundo, desviando el propósito de los dioses y sus habitantes para formarse un ejército de muertos vivientes mejorados con un embalsamamiento especial, cuando los habitantes se dieron cuenta de la verdad tres monstruosos dioses, y Nicol Bolas empezaron destruir lo que no le servía del plano, incluyendo a los dioses mismos. Los sobrevivientes de la masacre (incluyendo la diosa Hazoret) se adentraron al desierto como última esperanza de supervivencia, en esta ocasión el Gatewatch sufriría una derrota a manos del dragón.

Además que en el transcurso del Bloque de Alara hasta ahora, se han lanzado libros y cómics que narran y desarrollan la trama de los nuevos planeswalkers, aunque los libros lanzados después del ciclo de Lorwyn se centran más en los propios Planeswalker que un plano específico.

En cuanto a la novelas sobre el plano del bloque en turno solo se lanza una novela. De igual forma los cómics se centran principalmente en los planeswalkers.

Cada Carta tiene una ilustración que representa el espíritu (o flavor = sabor) de la carta, muchas veces representando el ambiente de las expansión para la cual la carta fue diseñada. Buena parte del arte inicial de "Magic"<nowiki>'</nowiki>s fue ordenado con poca dirección específica o ideas de cohesión visual. sin embargo, después de algunos años de envíos que mostraban seres con alas para criaturas sin habilidad de vuelo, o personajes múltiples en el arte de lo que tenía intención de ser una criatura simple, el equipo de dirección artística decidió imponer algunas reglas para que la visión artística estuviera mejor alineada el diseño y desarrollo de las cartas. Cada bloque de cartas tiene su propio estilo con escenas y descripciones de las razas y lugares que juegan un rol principal en el universo de juego de "Magic".

Algunas ediciones al principio experimentaron con arte diferente para la misma carta. Sin embargo, Wizards of the Coast pensó que esto impedía el reconocimiento inmediato de una carta a simple vista, y causaba confusión. Consecuentemente, el arte alterno se usa con parsimonia actualmente y sobre todo para cartas promocionales. Con esto dicho, cuando las cartas viejas son reimpresas en nuevas ediciones, Wizards ha asegurado que serán impresas con nuevo arte, para hacerlas más valiosas desde el punto de vista de la colección. Un buen ejemplo es una carta que lleva saliendo en distintas imágenes desde 1995 hasta el 2011, la "Bola de Fuego".

Desde 1995, el derecho de autor de todo el arte es transferido a "Wizards of the Coast" cuando se firma el contrato de adquisición. Sin embargo, al artista se le permite vender la pieza original y las copias impresas de la misma, y para los artistas importantes de "Magic", esto se convierte en una fuente de ingreso lucrativa.

Mientras "Magic" se expandió a lo largo del planeta, su arte tuvo que adaptarse a su público ahora internacional. El arte ha sido editado o reemplazado para cumplir con estándares gubernamentales. Por ejemplo, la aparición de esqueletos y casi todos los no-muertos en el arte está prohibido por el Gobierno de la República Popular de China.

En 2012, la película independiente "" se estrenó no comercialmente en línea. La película independiente acerca de "Magic: el encuentro" se filmó en República Checa como un proyecto en solitario, y no fue patrocinada ni por Wizards of the Coast ni por Hasbro.

Desde 1997 se han desarrollado varios videojuegos para prácticamente todas las plataformas, adoptando distintos tipos de jugabilidad. Actualmente está abierto el plazo de inscripción para la beta privada de Magic Legends, cuyo lanzamiento está previsto para 2021. Esta es la lista completa de juegos digitales de Magic:


Véase también .




</doc>
<doc id="9822" url="https://es.wikipedia.org/wiki?curid=9822" title="Cubismo">
Cubismo

El cubismo fue un movimiento artístico desarrollado entre 1907 y 1924, creado por Pablo Picasso y Georges Braque, continuado en sus albores por Jean Metzinger, Albert Gleizes, Robert Delaunay, Juan Gris, María Blanchard y Guillaume Apollinaire.Es una tendencia esencial, pues da pie al resto de las vanguardias europeas del siglo XX. No se trata de un "ismo" más, sino de la ruptura definitiva con la pintura tradicional.

El término cubismo fue acuñado por el crítico francés Louis Vauxcelles, el mismo que había bautizado a los fauvistas motejándolos de "fauves" (fieras); en el caso de Braque y sus pinturas de L'Estaque, Vauxcelles dijo, despectivamente, que era una pintura compuesta por «pequeños cubos» y figuras geométricas. Se originó así el concepto de «cubismo». El cubismo literario es otra rama que se expresa con poesías cuya estructura forma figuras o imágenes que ejemplifican el tema, la rima es opcional y no tienen una métrica específica ni se organizan en versos.

El cubismo es considerado la primera vanguardia, ya que rompe con el último estatuto renacentista vigente a principios del siglo XX, la perspectiva. En los cuadros cubistas, desaparece la perspectiva tradicional. Trata las formas de la naturaleza por medio de figuras geométricas, fragmentando líneas y superficies. Se adopta así la llamada «perspectiva múltiple»: se representan todas las partes de un objeto en un mismo plano. La representación del mundo en donde pasaba a no tener ningún compromiso con la apariencia de las cosas desde un punto de vista determinado, sino con lo que se sabe de ellas. Por eso aparecían al mismo tiempo y en el mismo plano vistas diversas del objeto: por ejemplo, se representa de frente y de perfil; en un rostro humano, la nariz está de perfil y el ojo de frente; una botella aparece en su corte vertical y su corte horizontal. Ya no existe un punto de vista único. No hay sensación de profundidad. Los detalles se suprimen, y a veces acaba representando el objeto por un solo aspecto, como ocurre con los violines, insinuados solo por la presencia de la cola del mismo.

A pesar de ser pintura de vanguardia los géneros que se pintan no son nuevos, y entre ellos se encuentran sobre todo bodegones, paisajes y retratos.

Se eliminan los colores sugerentes que tan típicos eran del impresionismo o el fovismo. En lugar de ello, utiliza como tonos pictóricos apagados los grises, verdes y marrones. El monocromatismo predominó en la primera época del cubismo, posteriormente se abrió más la paleta.

Con todas estas innovaciones, el arte acepta su condición de arte, y permite que esta condición se vea en la obra, es decir, es parte intrínseca de la misma. El cuadro cobra autonomía como objeto con independencia de lo que representa, por ello se llega con el tiempo a pegar o clavar a la tela todo tipo de objetos hasta formar "collages".

La obra resultante es de difícil comprensión al no tener un referente naturalista inmediato, y ello explica que fuera el primero de los movimientos artísticos que necesitó una exégesis por parte de la "crítica", llegando a considerarse el discurso escrito tan importante como la misma práctica artística. De ahí en adelante, todos los movimientos artísticos de vanguardia vinieron acompañados de textos críticos que los explicaban.

El cubismo tuvo como centro neurálgico la ciudad de París, y como jefe y maestro del movimiento figura el español Pablo Picasso, tuvo como seguidores al español Juan Gris y los franceses Georges Braque y Fernand Léger. El movimiento efectivamente se inicia con el cuadro "Las señoritas de Aviñón" ("Demoiselles d'Avignon") de Pablo Picasso. Como elemento precursor del cubismo destaca la influencia de las esculturas africanas y las exposiciones retrospectivas de Georges Seurat (1905) y de Paul Cézanne (1907).

El cubismo surge en la primera década del siglo XX, constituyendo la primera de las vanguardias artísticas. Entre las circunstancias que contribuyeron a su surgimiento, se ha señalado tradicionalmente tanto la obra de Cézanne como el arte de otras culturas, particularmente la africana. En efecto, Cézanne pretendió representar la realidad reduciéndola a sus formas esenciales, intentando representar los volúmenes sobre la superficie plana del lienzo de una manera nueva, tendencia que fue seguida por los cubistas. Ya antes que él, los neoimpresionistas Seurat y Signac tendieron a estructurar geométricamente sus cuadros. Lo que Picasso y Braque tomaron de Cézanne fue la técnica para resolver ese problema de lograr una nueva figuración de las cosas, dando a los objetos solidez y densidad, apartándose de las tendencias impresionistas que habían acabado disolviendo las formas en su búsqueda exclusiva de los efectos de la luz.

Por otro lado, el imperialismo puso a Occidente en contacto con otras civilizaciones con un arte propio y distinto del europeo. A través de diversas exposiciones, Picasso conoció la escultura ibérica y la africana, que simplificaban las formas y, además, ponían en evidencia que la pintura tradicional obedecía a una pura convención a la hora de representar los objetos conforme a las ideas renacentistas de perspectiva lineal y aérea. Lo que parece actualmente excesivo a los historiadores de arte es atribuir una influencia directa de las máscaras africanas con la obra "picassiana."

Todo ello no hubiera sido posible sin la aparición de la fotografía pues esta, al representar la realidad visual de manera más exacta que la pintura, liberó a este último arte de la obligación de representar las cosas tal como aparecen ante nuestros ojos y forzó a los artistas a buscarle un sentido diferente a la mera transcripción a las dos dimensiones de la apariencia externa de las cosas. La aparición del cubismo se ha relacionado, además, con otros tres hechos acontecidos en esas décadas que revelan que las cosas pueden ser diferentes a como aparentan ser: el psicoanálisis al evidenciar que pueden existir motivaciones más profundas para los actos y pensamientos humanos; el interés por la cuarta dimensión, fruto de la revolución acaecida en la geometría del siglo XIX; y la teoría de la relatividad, que revela que el mundo no es exactamente, en su estructura profunda, como lo presentaba la geometría euclidiana.

En 1909 Braque y Picasso estrechan su amistad y consiguen desarrollar la nueva tendencia. Juntos crearon las dos tendencias del cubismo. La primera es el cubismo analítico (1909-1912), en donde la pintura es casi monocroma en gris y ocre. Los colores en este momento no interesaban pues lo importante eran los diferentes puntos de vista y la geometrización, no el cromatismo. Fueron elaborando un «nuevo lenguaje» que analiza la realidad y la descompone en múltiples elementos geométricos. Los puntos de vista se multiplicaron, abandonando definitivamente la unidad del punto de vista de la perspectiva renacentista. Se introducen en la pintura los «pasos», definidos como ligeras interrupciones de la línea del contorno. Los volúmenes grandes se fragmentan en volúmenes más pequeños. Entre las obras de esta fase del cubismo se encuentra el "Retrato de Kahnweiler" (1910, Instituto de Arte de Chicago).

A este período también se le llama de cubismo hermético, pues por la cantidad de puntos de vista representados, algunas obras parecen casi abstractas. Al hermetismo se llega porque los planos acaban independizándose en relación al volumen de manera que es difícil decodificar la figuración, reconstruir mentalmente el objeto que esos planos representan. El color no ayudaba, al ser prácticamente monocromos y muchas veces convencionales, no relacionados con el auténtico color del objeto. La imagen representada, en definitiva, era ilegible, casi imposible de ver, a no ser por algunos objetos como una pipa, o letras de periódico, que permiten distinguir lo que se está representando.

Es en esta fase cuando el cubismo se presenta en público. Pero no por obra de Picasso y Braque, que exponían privadamente en la galería Kahnweiler, sino por otros pintores que conocieron la obra de aquellos en sus talleres. Se presentaron al Salón de los Independientes de 1911. En su sala 41 aparecieron obras de Jean Metzinger, Albert Gleizes, Henri Le Fauconnier, Fernand Léger y Robert Delaunay. Provocaron el escándalo y rechazo de público y crítica. Ello llevó a que se construyera ya una obra doctrinal de primera hora explicando los hallazgos de la nueva tendencia. Así, el primer estudio teórico del cubismo lo hicieron en 1912 Gleizes y Metzinger: "Du "Cubisme"" («Sobre el cubismo»). Apollinaire, por su parte, escribió "Les peintres cubistes" («Los pintores cubistas. Meditaciones estéticas») en 1913. Hubo otras adhesiones, como la de la mecenas Gertrude Stein o los marchantes como Ambroise Vollard y Henry Kahnweiler. Otros poetas, además de Apollinaire, defendieron el nuevo estilo: Pierre Reverdy y Max Jacob.

Además del rechazo de los tradicionalistas de la pintura, hubo posteriormente críticos que venían de la propia vanguardia, centradas en dos problemas que planteaba el cubismo: su estatismo y su adhesión a lo figurativo. En efecto, sobre todo los futuristas objetaron al cubismo que en sus obras el movimiento estuviera ausente, siendo así que el mundo actual es esencialmente dinámico. Gino Severini, a quien se considera el más cubista dentro del futurismo, lo criticó en "Del Cubismo al Clasicismo" (1921), aunque con el tiempo (1960) reconoció que debía al cubismo gran parte de su técnica. Algunos cubistas fueron sensibles a esta crítica y crearon obras influidas por el futurismo, como hizo Marcel Duchamp con su primera versión de "Desnudo bajando una escalera" (1911, Museo de Arte de Filadelfia, col. Arensberg). Por otro lado, aunque en su época no resultaba fácil deslindar el cubismo de la abstracción, hoy resulta evidente que siguen sujetos a una representación figurativa de las cosas reales. Se seguían representando sillas, botellas o figuras humanas, aunque las descompusieran en planos y volúmenes geométricos. No se apartaban de representar la realidad, sino que querían representarla en el cuadro con un nuevo lenguaje.

El camino trazado por Picasso y Braque pronto fue seguido por los pintores Juan Gris (José Victoriano González) y Louis Marcoussis, el primero influido por Picasso, el segundo por Braque. Gris, tercer gran nombre del cubismo. Este madrileño malvivía en París dibujando para revistas y periódicos. A partir de 1911 se interesó por el problema de la luz sobre los objetos, creando cuadros con iluminación naturalista, en los que los rayos luminosos oblicuos y paralelos entre sí inciden sobre formas rígidas, como puede verse en su "Retrato de Picasso" de 1912. Él mismo dijo haber adoptado el cubismo «analítico», multiplicando los puntos de vista y usando colores vivos. Para el año 1912, Braque y Picasso ya habían realizado "collages", y Gris comenzó a introducir en sus obras diversos materiales como la madera o la tapicería, bien imitándolos, bien pegándolos ("El lavabo", 1912).

Braque, por su parte, influyó en el polaco Marcoussis (Ludwig Markus). Más ortodoxo y menos original que Gris, creó una obra con colores intensos y cercana a veces al futurismo. Comenzó en 1912 a trabajar el cubismo analítico, con obras como "Naturaleza muerta con damero" (1912), Museo Nacional de Arte Moderno, Centro Georges Pompidou).

En "El Portugués" (1911) de Braque aparecen palabras y números, lo que abrió una nueva vía que llevó al segundo período del cubismo, el cubismo sintético (1912-1914). Braque, que había sido el primero en utilizar la caligrafía, y que más de una vez intentó imitar la madera o el mármol, fue quien inició esta última fase del cubismo al realizar "papier collés", pegando directamente papeles decorados en la pintura. Picasso y Braque comenzaron a incorporar material gráfico como páginas de diario y papeles pintados, técnica que se conoce como "collage". En 1912 Picasso realizó su primer "collage", "Naturaleza muerta con silla de paja" (Museo Picasso, París), en el que añade al lienzo pasta de papel y hule. El color es más rico que en la fase anterior, como puede verse en los rojos y azules de "Botella de Suze" (1913, Saint Louis, Misuri, Universidad Washington). Estas obras sintéticas son más simples, más sencillas de entender en cuanto a que son más figurativas, se ve claramente lo que se pretende representar. Los objetos ya no se reducen a volúmenes y planos expuestos en diversas perspectivas hasta ser irreconocibles, sino que se reducen a sus atributos esenciales, a aquello que los caracteriza de manera inequívoca sin lo cual no serían lo que son. Por ello, aunque reducido a lo esencial, queda claro en todo momento lo que son. Para representar los objetos «tipo» de manera objetiva y permanente, y no a través de la subjetividad del pincel, se recurre a lo que parece un ensamblaje. Los cuadros están formados por diversos materiales cotidianos que se pegaban o clavaban a la tela, como tiras de papel de tapicerías, periódico, partituras, naipes, cajetillas de cigarros o cajas de fósforos. El cuadro se construye con elementos diversos, tanto tradicionales (la pintura al óleo) como nuevos (como el papel de periódico). Los cafés y la música inspiraron estos bodegones. Otras obras de Picasso pertenecientes a esta fase del cubismo sintético son "El jugador de cartas" (1913-14) o "Naturaleza muerta verde" (1914). Braque realiza en esta época "El clarinete" (1913), el "Correo" (1913), "Aria de Bach" (1913-14) o "Violeta de Parma" (1914).

En este período Juan Gris realiza una pintura más libre y colorista. Emblemática es su "Place Ravignan, naturaleza muerta ante una ventana abierta" (1915), donde el exterior se representa a la manera tradicional, con perspectiva renacentista, mientras que el interior de formas deconstruidas y compuestas desde diversos puntos de vista con planos quebrados. Por su parte, Marcoussis llega a la cumbre de su tarea creadora con obras más poéticas y personales como "Músico" (1914, Galería Nacional de Washington, col. Chester Dale)

María Blanchard nunca llegó a la total descomposición de la forma pero dejó su manufactura en forma de ricos colores. Su famosa "Mujer con abanico" (1916, Museo Nacional Centro de Arte Reina Sofía), "Naturaleza muerta" (1917, Fundación telefónica) o "Mujer con guitarra" (1917, Museo Nacional Centro de Arte Reina Sofía) son ejemplos del intenso estudio que realiza sobre la anatomía de las cosas, como señaló Ramón Gómez de la Serna y del peso del color en su pintura. Tras esta etapa regresa a las técnicas figurativas donde queda impresa la influencia de las vanguardias.

La Primera Guerra Mundial puso fin a la fase más creadora del cubismo. Muchos de los pintores cubistas, al ser franceses, fueron llamados a la lucha (Braque, Léger, Metzinger, Gleizes, Villon y Lhote). En la posguerra, solo Juan Gris siguió trabajando el cubismo más o menos ortodoxo, aunque en un estilo más austero y simple, en el que los objetos quedaron reducidos a su esencia geométrica. Marcoussis creó una obra más poética. Braque siguió trabajando en la misma línea del cubismo sintético, con papel encolado. Nuevos pintores adoptaron un lenguaje cubista, como María Blanchard. Pero la mayoría de los pintores hasta entonces cubistas, empezando por el propio Picasso, fueron adoptando nuevas tendencias, como ocurre con Duchamp y Picabia, que crearon el dadaísmo o Mondrian que se adhirió a la abstracción. El cubismo, como movimiento pictórico, se puede dar por terminado hacia 1919.

Fue el francés Apollinaire quien lo adaptó en la literatura. Busca recomponer la realidad mezclando imágenes y conceptos al azar. Uno de sus aportes fue el caligrama.

El cubismo repercutió en la escultura, a través de técnicas similares al "collage" del cubismo sintético. La escultura empezó a construirse con materiales de desecho, elaborándose con piezas diversas y no procedentes de un solo bloque de piedra o mármol. Con ello se crea la llamada estética de «ausencia de masa», al surgir huecos y vacíos entre las superficies. Como los arquitectos, los escultores no dan forma a un volumen, sino que crean espacios. Es de especial interés la variante arquitectónica del cubismo que se dio en Checoslovaquia entre 1910 y 1925, el llamado "Cubismo Checo".

El propio Pablo Picasso realizó esculturas cubistas. Escultores que crearon obras cubistas fueron Alexander Archipenko, Jacques Lipchitz y Henri Laurens, además de los españoles Pablo Gargallo y, sobre todo, Julio González, pionero en el uso del hierro gracias a la soldadura autógena, lo que abrió todo un mundo de posibilidades a la escultura del siglo XX.

En arquitectura tuvo escasa representación y se dio sobre todo en Checoslovaquia. Su principal representante fue Josef Gočár, quien tras unos inicios influido por la obra de Josef Hoffmann, en 1911 se unió al Grupo de Artistas Plásticos ("Skupina Výtvarných Umělců") y comenzó a trabajar en estilo cubista, como se denota en la casa de la Virgen Negra en Praga (1911–1912) y el establecimiento termal de Lázně Bohdaneč (1912-1913), donde combina formas clásicas y modernas con el cubismo piramidal. Tras la Primera Guerra Mundial y la independencia de Checoslovaquia inició con Pavel Janák la búsqueda de un estilo arquitectónico nacional checo, que se plasmó en el llamado «rondocubismo», que incorpora formas redondeadas y multicolores procedentes de la decoración vernácula bohemio-morava, como evidencia su Banco Legión en Praga (1921-1922). Otros representantes fueron: Pavel Janák (villa Jakubec en Jičín, 1911-1912; villa Drechsel en Pelhřimov, 1912-1913; crematorio de Pardubice, 1921-1923; palacio Adria en Praga, 1922-1925); Josef Chochol (villa Kovařovic en Praga, 1912-1913; edificios residenciales Bayer y Hodek en Praga, 1913-1914); Bedřich Feuerstein (monumento a Jan Žižka en Vítkov, 1913); y Jiří Kroha (cabaret Montmartre en Praga, 1918).

Aunque ya desde el propio Guillaume Apollinaire se escriben obras adscritas temporalmente a este movimiento artístico, no encontramos una definición y experimentación escénica hasta principios del siglo XXI, época en la que se re-describe este término, aplicándolo en especial a las artes escénicas y focalizándolo en el espectador (prosumidor).

El principal artífice del teatro cubista es Rafael Negrete-Portillo (actor, director, dramaturgo y docente universitario), el cual lo define en varias entrevistas y trabajos académicos cómo “la búsqueda escénica de aquello que en Arte se ha llamado ‘perspectiva múltiple’. Entendemos por deconstruir “deshacer analíticamente los elementos que constituyen una estructura conceptual”. El teatro cubista pretende ser capaz de deconstruir una realidad poliédrica para mostrar sus distintas caras/elementos simultáneamente. Si aplicamos esto al mensaje que tratamos de transmitir con un texto dramático y su posterior montaje, llegamos a un plausible ‘cubismo sintético’ (más que un ‘cubismo analítico’). El mensaje no se de-re-construye y muestra en todos sus fragmentos, sino que se concreta en su esencialidad, sintetizando sus partes más significativas para que cada una de estas sí sea exhibida por todos sus lados.” 

Las principales obras de este autor exhibidas o creadas bajo la premisa de teatro cubista en España han sido "Último sujeto" [2016], "Ecos (drama holístico)"  y "Parestesia (la lucha empieza en el interior)" [2019].

Sobre otras puestas en escenas anteriores, sin aplicación directa de la nueva definición de Negrete-Portillo, encontramos cómo la preparación del libreto de Satie, Massine y Cocteau, llevará a Picasso a Italia: un referente esencial en los Ballets Rusos. El obligado viaje por Nápoles, Pompeya y Herculano en 1917, marcará una obra iluminada por el arte circense, en la cual se evoluciona en la puesta en escena y las coreografías, generando un punto de inflexión que afectará y marcará las escenografías expresionistas en teatro y en cine. 

Por su parte, Andor Weininger presentó una escenografía que expresaba los ideales de La Bauhaus, para la Mechanical Stage Revue en 1926. Se trata de un canto al mecanicismo del Futurismo desde el prisma de Gropius, un espacio de acción teatral de giros y desplazamientos verticales y horizontales. Weininger se formará con Kandinsky e investigará la vanguardia soviética no pudiéndosele asociar única o directamente al cubismos, sino a la vanguardia experimental.

Joan Miró preparó varios bocetos para la escenografía del ballet Romeo y Julieta en 1926, hechos estos a gouache y lápiz. Miró trabajó con Balanchine y Nijinska. En el primer acto se muestra el ideal espacial surrealista, el telón de boca parte de una obra de Miró en la cual los objetos poetizados y enigmáticos intercalan elementos complejos que, al tiempo, parecen pertenecer a la realidad; sin embargo, la mayor belleza se traduce en las relaciones de la historia de amor con los elementos cósmicos: la tierra y el cielo estrellado se unen en un plano, en un espacio abstracto de derivación cubista.

La referencia actual (S. XXI) y real de TEATRO CUBISTA, en la cual se aúna escenografía, interpretación, iluminación, música, texto y, principalmente, dirección escénica, por medio de escenas simultaneadas y multiplicidad argumentaría síncrona, se debe al dramaturgo y profesor Rafael Negrete-Portillo.

Además de Pablo Picasso, fundador del cubismo y su inmediato seguidor Georges Braque, junto con Juan Gris y Marcoussis, sus más directos seguidores, el cubismo fue seguido por una multitud de artistas entre 1911 y 1914. Algunos de ellos se agruparon bajo la denominación de "Section d'Or" o Grupo de Puteaux: Albert Gleizes, Jean Metzinger, Juan Gris , Fernand Léger y André Lhote. De este colectivo surgió, en 1912 el orfismo, cuyos máximos representantes son Robert Delaunay y František Kupka, quienes acabaron renunciando a la representación figurativa y centrándose en el color se aproximaron a la abstracción geométrica, como anticipó ya su "Villa de París", de Delaunay (1910). El tema acabó desapareciendo totalmente en obras como "Formas circulares" (1912-13). Se ha denominado a este estilo como cubismo abstracto o rayonismo. Kupka, próximo al cubismo, comenzó a estudiar, a partir de 1912, la forma en que el espacio podía representarse mediante planos de color ("Planos verticales Amorpha", 1912) o líneas sinuosas. También Francis Picabia recreó los volúmenes de la realidad de manera bastante abstracta (Procesión en Sevilla, 1912) lo que le llevó, a partir de 1913, a la no-figuración.

Gleizes cultivó un cubismo cezaniano más figurativo que el resto y en el que aparecía la figura humana esquematizada; no obstante, también tuvo una fase analítica. Obras destacadas de Gleizes son: "Árbol" (1910, París, col. part.), "Caza" (1911, París, col. comandante Houot), "Hombres en el balcón" (1912, Museo de Arte de Filadelfia, col. Arensberg), "Desgranado de la cosecha" (1912, Museo Guggenheim de Nueva York), Bañistas (quizá su obra más conocida, de 1912, Museo de Arte Moderno de la Ciudad de París), "Retrato de Figuière" (1913, museo de Lyon) y "Mujeres cosiendo" (Otterlo, Museo Kröller-Múller).
Su amigo Metzinger, con quien escribió "Sobre el cubismo" tuvo una primera fase analítica en la que predomina el estudio de la estructura, para pasar luego a una fase cezaniana en la que predomina el estudio de los volúmenes. De Metzinger destacan sus "Desnudos" de 1910-1911, la "Merienda" (1910-11, Museo de Arte de Filadelfia, col. Arensberg), el "Pájaro azul" (1913, Museo de Arte Moderno de la Ciudad de París), "Bañistas" (1913, Museo de Arte de Filadelfia) y "Mujer haciendo calceta" (1919, Museo Nacional de Arte Moderno, Centro Georges-Pompidou).

Henri Le Fauconnier (1881-1946) realizó estudios de desnudos cuyos volúmenes fue fragmentando, explorando la incidencia de la luz sobre ellos. Creó «una especie de Impresionismo cubista bastante personal» que puede verse en obras como "Retrato de Paul Castiaux" (1910), "Abundancia" (1910-11) o "Cazador" (1912).

Más original que todos ellos fue Fernand Léger. Desarrolló un estilo personal que refleja su atracción por la máquina. Célebre es su obra "Figuras desnudas en el bosque" (1909-1910, Otterlo, Museo Kröller-Müller), que se puede considerar obra intermedia entre el cubismo y el futurismo, movimiento este último fascinado con la máquina y el movimiento. En esta obra se aprecia igualmente su predilección por las formas y los volúmenes, propia del cubismo cezaniano. Después de experimentar con los volúmenes, comienza a dar preponderancia al color a partir de 1913, en composiciones llenas de dinamismo.

Por una fase cubista pasó el gran pintor neerlandés Piet Mondrian al instalarse en París en 1911. Cultivó el cubismo analítico en el período 1911-1914. Sus estudios sobre el ángulo recto, y las formas planas acabaron llevándole a la abstracción. Al volver a Ámsterdam fundó, junto a Van Doesburg, el grupo De Stijl (1917). En torno a su revista se constituyeron artistas directamente influidos por el cubismo.

En México también surgieron obras cubistas por el pintor Diego Rivera en el año de 1913, recreando escenas de la Revolución Mexicana. Diego influenciado por la obra de Picasso y Braque acudió a la manipulación geométrica y al punto de vista panorámico elevado para recrear paisajes, retratos y naturalezas muertas. Sus mejores obras ese año fueron La Adoración de la virgen, "La joven con alcachofas, La mujer del pozo" o "El joven de la estilográfica" (todas de 1914), y "El arquitecto" (de 1915).

Hubo otros que adaptaron el cubismo a su temperamento. Entre ellos cabe citar, en primer lugar, a Jacques Villon, que conoció el cubismo a través de su hermano Marcel Duchamp. Estudió los volúmenes, compuso sus cuadros en estructuras piramidales y empleó colores vivos. Su cubismo fue moderado, como el de Roger de la Fresnaye, que aunque adoptó la superposición de planos, no llegó a romper de manera clara con la figuración y la perspectiva. Se vio influido por Delaunay, lo que le llevó a realizar sus mejores obras construidas sobre todo con el color: "Conquista del aire" (1913) y muchas "Naturalezas muertas" (1913-14). Después de la guerra volvió al clasicismo. Finalmente, André Lhote se enmarca en una tendencia a adaptar el estilo cubista a las reglas de la composición clásica.

Además de los ya citados, se puede considerar que hicieron obras cubistas: Marcel Duchamp, Sonia Delaunay, Emilio Pettoruti, Carlos Sotomayor, María Blanchard y Enrique Sobisch.

El purismo de Charles Edouart Jeanneret y Amadée Ozenfant surgió en 1918 como una derivación del cubismo.





</doc>
<doc id="9823" url="https://es.wikipedia.org/wiki?curid=9823" title="Juego de cartas coleccionables">
Juego de cartas coleccionables

Los juegos de cartas coleccionables (Trading Card Game, también abreviado TCG en ingles) son un tipo de juego de cartas no predefinidas y existentes en gran cantidad y de variados tipos y características, que otorgan individualidad a cada carta, y con las cuales puede construirse una baraja (o mazo) dependiendo de las reglas del juego en cuestión. Entre las franquicias de este tipo más conocidas están , Pokémon TCG y Yu-Gi-Oh!. 

El concepto de «juego de cartas coleccionables» (a veces abreviado en JCC) nace en 1993 con "", juego creado por Richard Garfield, aunque anteriormente existía ya un tipo de juego de cartas denominado actualmente juego de baraja de colección (JBC) que presentaba reglas más simples y una menor complejidad en comparación al JCC, jugandose a partir de una baraja con cartas predefinidas y en el que cada carta traía generalmente una imagen con un tema específico según el tipo de baraja y juego. 

Así, en 1993 Richard Garfield, profesor de matemáticas, va más allá del concepto que tenía el juego de baraja de colección clásica y desarrolla el actual concepto de JCC después de ser planteado y desarrollado en una conversación sobre póquer, en la que Garfield planteaba cómo sería el juego si las barajas, en lugar de tener 54 cartas predefinidas, pudieran construirse libremente. La respuesta evidente es que siempre se meterían ases y el juego no tendría sentido; no obstante, Garfield tenía una visión muy distinta de los juegos de cartas, logrando así diseñar el concepto básico de todos los JCC y crear su propio juego.

En el ámbito de los juegos de cartas coleccionables aparecen continuamente nuevos juegos basados en licencias de cine, series televisivas, cómics o libros, existiendo un JCC para prácticamente cada ambientación posible. Las cartas pueden ser concebidas como objetos de colección además de poder ser jugadas.

La principal característica de un JCC es la individualidad de cada carta, la cual, lleva escrita su función en el juego, efectos y características. Los juegos de naipes tradicionales utilizan siempre las mismas cartas que cobran efecto según las reglas del juego; los JCCs usan unas reglas que definen el desarrollo de la partida y las condiciones de victoria pero son las cartas las que definen su propia función.

A partir de un amplio elenco de cartas disponibles (unas 18.500 en el caso de Magic) cada jugador confecciona su propio mazo, dando pie a una infinidad de estrategias y desarrollos de partidas, las cuales, puede afirmarse que cada una es distinta.

Para establecer un paralelismo, un JCC es un ajedrez donde cada jugador escoge sus 16 piezas iniciales de miles disponibles, cada una con movimientos y efectos propios, jugando siempre bajo las normas del tablero de ajedrez. Las reglas del juego hacen que ninguna carta sea desequilibrante y tenga siempre su contrapartida.

El juego Magic, de la empresa Wizards Of The Coast, difundió este concepto, imitado en cientos de juegos distintos, cada cual con sus propias reglas y características, dando lugar a un segmento del ocio único, tremendamente amplio y que desde 1993 no ha parado de crecer con millones de aficionados en todo el globo ya que presenta un juego totalmente distinto conocido, desafiante por su profundidad e infinitas combinaciones de estrategia, adictivo ya que las sucesivas partidas son totalmente distintas, y con un amplio programa de torneos y eventos que abarcan desde pequeños torneos locales a un circuito profesional con grandes premios en metálico. Posteriormente producto del éxito de esta empresa aparecerían otras empresas con nuevos juegos de cartas coleccionables, ampliándose así el número de juegos de cartas coleccionables.

Los mazos se caracterizan por ser igualmente transportables fácilmente, lo cual hace que pueda jugarse en cualquier lugar contra cualquier persona, haciendo así un juego social que puede practicarse con un variado número de jugadores, dando lugar a tantas situaciones de partida que, aunque un mazo de un jugador sea superior a un rival concreto, igualmente puede suceder que este jugador se vea en serios aprietos contra otro. Los grandes torneos reúnen a cientos y miles de jugadores, estando el récord en 2220 jugadores (GP Madrid 27/02/2010).

La otra característica que le da nombre y las diferencia de los naipes tradicionales es que son coleccionables. Así, las cartas se dividen en frecuencias, habitualmente: Común, Infrecuente y Rara, siendo de menos a más la dificultad de conseguir. Dichas cartas se adquieren en sobres sellados cuyo contenido es aleatorio, con lo que no se sabe exactamente que se obtiene en cada sobre. Por ello, no se compran las cartas que deseamos, si no que se entra en un mercado secundario de coleccionismo, donde las cartas adquieren su valor por su frecuencia, antigüedad, estado y sobre todo calidad y efectividad en partidas, obedeciendo a conceptos de oferta y demanda.

Así, consecuentemente, tenemos la vertiente de juego y de coleccionable, logrando la fórmula que ha logrado captar la atención de miles de jugadores en todo el mundo y dando lugar a tiendas especializadas donde pueden adquirirse tanto en formato sellado como carta suelta, así como grandes comunidades de internet dedicadas a informar sobre el juego, estrategias y mazos ganadores, e intercambio y valoración de cartas (consultar links al final).

El precio de cualquier JCC es el que quiera darle el usuario. El precio básico de un sobre (que incluyen entre 8 y 15 cartas según el JCC) oscila entre 3 y 4 euros, siendo suficiente para iniciar a jugar un Mazo preconstruido (de 10 a 15 euros) y unos cuantos sobres. Si gusta, puede iniciarse la construcción de un mazo de juego más avanzado, existiendo cartas sueltas desde 0,20€ hasta 1500€ (Black Lotus), pero encontrando buenas cartas por 4-8€. Una vez construido el primer mazo y jugado unas cuantas partidas, el usuario es el que decide hasta donde quiere llegar, cuantos mazos quiere construir, y cuan competetivos desea que sean, ajustándose el juego a su presupuesto.

Una vez se posean estas cartas, no debe olvidarse que el valor económico lo conservan, pudiendo cambiarlas o venderlas en torno a ese valor, siendo posible renovar mazos a base de comerciar con cartas, con lo que la inversión realizada siempre puede amortizarse.

Existen otras muchas características que convierten a los JCCs en un apasionante hobby:








</doc>
<doc id="9826" url="https://es.wikipedia.org/wiki?curid=9826" title="Laguna de Lobos">
Laguna de Lobos

La Laguna de Lobos, es una laguna natural, se encuentra en la Provincia de Buenos Aires, ubicada a 15 km de la ciudad de Lobos y a 115 km de la Ciudad de Buenos Aires, es el principal atractivo turístico de la zona. Su acceso se halla en la Ruta Nacional 205 km 111,5; luego a la izquierda por camino pavimentado de 4 km para poder llegar al espejo de agua. En la margen Norte se encuentra la localidad de Villa Loguercio.

La laguna tiene 800 ha, transformándose en un lugar ideal para la práctica de actividades acuáticas. Al estar ubicada en una zona de abundante vegetación se pueden apreciar una gran variedad de aves silvestres. La fauna ictícola compuesta por pejerreyes, carpas, dientudos, tarariras, bogas, lisas, bagres y mojarras permiten inolvidables jornadas para el aficionado a la pesca.Es un gran Humedal, y en él, conviven diversas especies de animales y plantas: Mamíferos como las coipos (Myocastor coypus) y zorros pampeanos ("Licalopex gymnocercus"); peces como pejerreyes ("Odontesthes bonariensis") y bagres ("Rhamdia quelen") ; aves como el biguá ("Phalacrocorax olivaceus") y los patos siriri pampa ("Dendrocygna viduata"), y reptiles como los lagartos overos ("Salvator merinae"), entre otras. En su flora, se destaca el Junco ("Schoenoplectus californicus"), que sirve como protector de la costa y lugar de nidificación de aves, depósito de huevas de peces y refugio de mamíferos.



Todos los diciembre desde 1988 se celebra la Fiesta del Pescador Deportivo, declarada de interés Municipal, Provincial y Nacional en la que se realizan distintas actividades acuáticas, culminando con la elección de la Reina del Pescador Deportivo y un show musical, sobre un escenario acuático. Esta fiesta se realiza en el Club de Pesca Lobos, el cual fue fundado en 1945. 

La Laguna cuenta con sus propias embarcaciones e instalaciones adecuadas para cocinar, además posee un muelle de 150 m de largo.

Bajo su espesa arboleda se encuentra la estación Hidrobiológica que se encarga de la cría y siembra de aproximadamente 50 mil alevinos anuales lo cual ha permitido mantener a través de los años el atractivo turístico fundamental de la Laguna: "La Pesca del Pejerrey".

Sobre el margen Noroeste, se encuentra "Villa Loguercio", en la que residen cerca de 400 habitantes estables y alrededor de 2.000 temporarios que se alojan en numerosas casas de fin de semana.

A fin de preservar la biodiversidad y la tranquilidad características del lugar, un mayoritario grupo de vecinos presentó a las Autoridades Municipales un petitorio firmado solicitando se declare Área Protegida a la zona de la "Boca" de la Laguna y alrededores. Cabe destacar que la Laguna de Lobos, a diferencia de otras lagunas de Sud América, está inventariada como Humedal de Latinoamérica. Su categorización como humedal responde a que sus ambientes son importantes para numerosas especies de aves acuáticas que residen o visitan dicho cuerpo de agua a lo largo del año. La conservación de este ambiente acuático y sus ambientes relacionados se sumarían a los esfuerzos de conservación de la biodiversidad de la cuenca del Salado.

Villa Loguercio fue fundada el 15 de junio de 1953, para más información se puede visitar el sitio wwww.lagunalobos.com.ar 



</doc>
<doc id="9828" url="https://es.wikipedia.org/wiki?curid=9828" title="Hiragana">
Hiragana

El es uno de los dos silabarios empleados en la escritura japonesa; el otro se denomina "katakana". También se suele emplear "hiragana" para referirse a cualquiera de los caracteres de dicho silabario. Proviene de la simplificación de caracteres más complejos de origen chino que llegaron antes del comienzo del aislamiento cultural japonés, que se mantuvo inflexible hasta el final de la era Edo. Se caracteriza por trazos curvos y simples. El hiragana, antiguamente (女手) (onnade, mano de mujer) fue inventado por las mujeres, una versión más bella que las formas rectas del katakana, el hiragana (tanto como el katakana) evolucionaron del silabario del japonés antiguo "Man'yōgana", que a su vez terminó creando el Hentaigana, y de la escritura cursiva del Hentaigana nació el Hiragana. La primera vez que el hiragana fue utilizado en un libro escrito por un hombre, fue el Diario de Tosa, por "Ki no Tsurayuki". Sin embargo él se hizo pasar por una mujer que lloraba por la muerte de su hija, debido que los hombres no podían utilizar este sistema de escritura.
Cuando se hace referencia a ambos silabarios en conjunto, "hiragana" y "katakana", se conocen como "kana". Estos caracteres, al contrario que los "kanji", no tienen ningún valor conceptual, sino únicamente fonético. 

El silabario hiragana consta de 46 caracteres en total, de los cuales 40 representan sílabas formadas por una consonante y una vocal, 5 son únicas vocales (a, i, u, e, o); y la única consonante que puede ir sola, la 'n' (ん en "hiragana" y ン en "katakana").

Este silabario se emplea en la escritura de palabras japonesas, partículas y desinencias verbales; en contraste con el "katakana" que se emplea para palabras extranjeras y onomatopeyas. Por ello, el hiragana es el primer silabario que aprenden los niños japoneses. A medida que aprenden los "kanji", los estudiantes van reemplazando los caracteres silábicos en favor de los caracteres chinos.

Los caracteres en rojo han quedado obsoletos en el japonés moderno.

Nota sobre la pronunciación. Todas las letras se pronuncian más o menos como en español salvo:

Existe un acento diacrítico en japonés llamado "nigori", y sirve para formar consonantes sonoras o 'impuras' (caso de la D, G, B y Z) o la "medio impura" P.

En el primer caso, el de las consonantes sonoras, se emplea el "dakuten" (゛) (濁点), que se representa con dos trazos diagonales cortos en la parte superior derecha del carácter. También se le suele llamar "ten ten".

Para formar el sonido P, se emplea el "handakuten" (゜) (半濁点), que tiene forma de círculo y se escribe también en la parte superior derecha del carácter. Se le conoce también como "maru".

Más sobre pronunciación: 'z' y 'j' son como en inglés. 'ge' y 'gi' se pronuncian 'gue', 'gui'.

Ten ten o dakuten solo se usa con las sílabas que comienzan en: K, T, F, H y S.

Handakuten o Maru solo se usa con sílabas que empiezan con: F o con H.

Por otra parte, cuando una sílaba que termina en "i" se une con ya, yu, yo, se pueden formar diptongos. En este caso el segundo kana (ya, yu o yo) se escribirá más pequeño de lo habitual.
[La pronunciación de esta "y" es la de una "i" consonántica (como la "i" de "novio [bjo]). Otro ejemplo es la 'u' en algunas palabras inglesas en que suena como "yu": huge" (hyu)]

Ejemplo: き(ki) y きゃ (Ki + ya pequeña = kya)

Las consonantes dobles se forman escribiendo un 'tsu' pequeño (っ) delante de la consonante en cuestión. Solo se doblan las consonantes k, s, t, p.

Ejemplos: よっか (yokka, día 4 de mes), ざっし (zasshi, revista), だった (datta, pasado del verbo ser), にっぽん (nippon, Japón). La pronunciación es: "yo-ka", "za-shi", "da-ta", "ni-pon", es decir, se pronuncia con un breve espacio entre la sílaba anterior y la posterior .

La única excepción es cuando el pequeño 'tsu' (っ) precede a la sílaba "chi" (ち). En este caso se leerá "tchi", y no "cchi" como muchos han interpretado. Ejemplos: たまごっち (tamagotchi), 一人ぼっち (hitoribotchi, solitario).

En cuanto a las consonantes nasales ('m', 'n'), se doblan escribiendo ん delante. Ejemplos: おんな (on'na, mujer), うんめい (unmei, destino).

Al igual que en español, la 'n' ん se pronuncia 'm' delante de 'p' o 'b'. Ejemplos: せんぱい ("senpai", fórmula de respeto a los que tienen un grado de estudio mayor), こんばん ("konban", esta noche).

Finalmente, existen vocales largas, que se forman de la siguiente manera:




</doc>
<doc id="9839" url="https://es.wikipedia.org/wiki?curid=9839" title="Central térmica solar">
Central térmica solar

Una central térmica solar o central termosolar es una instalación industrial en la que, a partir del calentamiento de un fluido mediante radiación solar y su uso en un ciclo termodinámico convencional, se produce la potencia necesaria para mover un alternador para generación de energía eléctrica como en una central termoeléctrica clásica. 

Consiste en el aprovechamiento térmico de la energía solar para transferirla y almacenarla en un medio portador de calor, generalmente agua. Esta es una de las ventajas de la tecnología CASPA, el almacenamiento térmico. La tecnología más comúnmente utilizada para almacenar esta energía son las sales fundidas (nitratos) de almacenamiento térmico. La composición de estas sales es variable, siendo la más utilizada la mezcla de nitrato de potasio, nitrato de sodio y últimamente se ha incorporado el nitrato de calcio.

Instructivamente, es necesario concentrar la radiación solar para que se puedan alcanzar temperaturas elevadas, de 300 °C hasta 1000 °C, y obtener así un rendimiento aceptable en el ciclo termodinámico, que no se podría obtener con temperaturas más bajas. La captación y concentración de los rayos solares se hacen por medio de espejos con orientación automática que apuntan a una torre central donde se calienta el fluido, o con mecanismos más pequeños de geometría parabólica. El conjunto de la superficie reflectaste y su dispositivo de orientación se denomina heliostato. 

Los fluidos y ciclos termodinámicos escogidos en las configuraciones experimentales que se han ensayado, así como los motores que implican, son variados, y van desde el ciclo Franklin (centrales nucleares, térmicas de carbón) hasta el ciclo Brayton (centrales de gas natural) pasando por muchas otras variedades como el motor de Distinguir, siendo las más utilizadas las que combinan la energía termosolar con el gas natural.

En la tabla a continuación se muestra el detalle de la potencia instalada por países a finales de 2017. España es actualmente líder mundial en esta tecnología:



</doc>
<doc id="9840" url="https://es.wikipedia.org/wiki?curid=9840" title="Energía solar fotovoltaica">
Energía solar fotovoltaica

La energía solar fotovoltaica es una fuente de energia que produce electricidad de origen renovable, obtenida directamente a partir de la radiación solar mediante un dispositivo semiconductor denominado célula fotovoltaica, o bien mediante una deposición de metales sobre un sustrato denominada célula solar de película fina.

Este tipo de energía se usa principalmente para producir electricidad a gran escala a través de redes de distribución, aunque también permite alimentar innumerables aplicaciones y aparatos autónomos, así como abastecer refugios de montaña o viviendas aisladas de la red eléctrica. Debido a la creciente demanda de energías renovables, la fabricación de células solares e instalaciones fotovoltaicas ha avanzado considerablemente en los últimos años.

Programas de incentivos económicos, primero, y posteriormente sistemas de autoconsumo fotovoltaico y balance neto sin subsidios, han apoyado la instalación de la fotovoltaica en un gran número de países. Gracias a ello, la energía solar fotovoltaica se ha convertido en la tercera fuente de energía renovable más importante en términos de capacidad instalada a nivel global, después de las energías hidroeléctrica y eólica. A finales de 2018 la potencia total instalada en todo el mundo alcanzó los 500GW de potencia fotovoltaica, y solo en 2018 se instalaron 100 GW.

La energía fotovoltaica no emite ningún tipo de polución durante su funcionamiento, contribuyendo a evitar la emisión de gases de efecto invernadero. Su principal desventaja consiste en que su producción depende de la radiación solar, por lo que si la célula no se encuentra alineada perpendicularmente al Sol se pierde entre un 10-25 % de la energía incidente. Debido a ello, en las plantas de conexión a red se ha popularizado el uso de seguidores solares para maximizar la producción de energía. La producción se ve afectada asimismo por las condiciones meteorológicas adversas, como la falta de sol, nubes o la suciedad que se deposita sobre los paneles. Esto implica que para garantizar el suministro eléctrico es necesario complementar esta energía con otras fuentes de energía gestionables como las centrales basadas en la quema de combustibles fósiles, la energía hidroeléctrica o la energía nuclear.

Gracias a los avances tecnológicos, la sofisticación y la economía de escala, el coste de la energía solar fotovoltaica se ha reducido de forma constante desde que se fabricaron las primeras células solares comerciales, aumentando a su vez la eficiencia, y logrando que su coste medio de generación eléctrica sea ya competitivo con las fuentes de energía convencionales en un creciente número de regiones geográficas, alcanzando la paridad de red.

El término «fotovoltaico» se comenzó a usar en Reino Unido en el año 1849. Proviene del griego φώς: "phos", que significa «luz», y de "-voltaico", que proviene del ámbito de la electricidad, en honor al físico italiano Alejandro Volta.

El efecto fotovoltaico fue reconocido por primera vez unos diez años antes, en 1839, por el físico francés Alexandre-Edmond Becquerel, pero la primera célula solar no se fabricó hasta 1883. Su creador fue Charles Fritts, quien recubrió una muestra de selenio semiconductor con pan de oro para formar la unión. Este primitivo dispositivo presentaba una eficiencia menor del 1%, pero demostró de forma práctica que, efectivamente, producir electricidad con luz era posible. Los estudios realizados en el siglo XIX por Michael Faraday, James Clerk Maxwell, Nikola Tesla y Heinrich Hertz sobre inducción electromagnética, fuerzas eléctricas y ondas electromagnéticas, y sobre todo, el trabajo realizado por Albert Einstein en 1905, por el cual le fue otorgado el premio Nobel en 1921, proporcionaron la base teórica y práctica del efecto fotoeléctrico, que es el fundamento de la conversión de energía solar en electricidad.

Cuando un semiconductor dopado se expone a radiación electromagnética, se desprende del mismo un fotón, que golpea a un electrón y lo arranca, creando un hueco en el átomo. Normalmente, el electrón encuentra rápidamente otro hueco para volver a llenarlo, y la energía proporcionada por el fotón, por tanto, se disipa en forma de calor. El principio de una célula fotovoltaica es obligar a los electrones y a los "huecos" a avanzar hacia el lado opuesto del material en lugar de simplemente recombinarse en él: así, se producirá una diferencia de potencial, y por lo tanto, tensión entre las dos partes del material, como ocurre en una pila.

Para ello, se crea un campo eléctrico permanente, a través de una unión pn, entre dos capas dopadas respectivamente, p y n. En las células de silicio, que son mayoritariamente utilizadas, se encuentran por tanto:



En el momento de la creación de la unión pn, los electrones libres de la capa n entran instantáneamente en la capa p y se recombinan con los huecos en la región p. Existirá así durante toda la vida de la unión, una carga "positiva" en la región n a lo largo de la unión (porque faltan electrones) y una carga "negativa" en la región en p a lo largo de la unión (porque los "huecos" han desaparecido); el conjunto forma la «Zona de Carga de Espacio» (ZCE) y existe un campo eléctrico entre las dos, de n hacia p. Este campo eléctrico hace de la ZCE un diodo, que sólo permite el flujo de corriente en una dirección: los electrones pueden moverse de la región p a la n, pero no en la dirección opuesta y por el contrario los "huecos" no pasan más que de n hacia p.

En funcionamiento, cuando un fotón arranca un electrón a la matriz, creando un electrón libre y un "hueco", bajo el efecto de este campo eléctrico cada uno va en dirección opuesta: los electrones se acumulan en la región n (para convertirse en polo negativo), mientras que los "huecos" se acumulan en la región dopada p (que se convierte en el polo positivo). Este fenómeno es más eficaz en la ZCE, donde casi no hay portadores de carga (electrones o "huecos"), ya que son anulados, o en la cercanía inmediata a la ZCE: cuando un fotón crea un par electrón-hueco, se separaron y es improbable que encuentren a su opuesto, pero si la creación tiene lugar en un sitio más alejado de la unión, el electrón (convertido en "hueco") mantiene una gran oportunidad para recombinarse antes de llegar a la zona n. Pero la ZCE es necesariamente muy delgada, así que no es útil dar un gran espesor a la célula. Efectivamente, el grosor de la capa n es muy pequeño, ya que esta capa sólo se necesita básicamente para crear la ZCE que hace funcionar la célula. En cambio, el grosor de la capa p es mayor: depende de un compromiso entre la necesidad de minimizar las recombinaciones "electrón-hueco", y por el contrario permitir la captación del mayor número de fotones posible, para lo que se requiere cierto mínimo espesor.

En resumen, una célula fotovoltaica es el equivalente de un generador de energía a la que se ha añadido un diodo. Para lograr una célula solar práctica, además es preciso añadir contactos eléctricos (que permitan extraer la energía generada), una capa que proteja la célula pero deje pasar la luz, una capa antireflectante para garantizar la correcta absorción de los fotones, y otros elementos que aumenten la eficiencia de la misma.

El ingeniero estadounidense Russell Ohl patentó la célula solar moderna en el año 1946, aunque otros investigadores habían avanzado en su desarrollado con anterioridad: el físico sueco Sven Ason Berglund había patentado en 1914 un método que trataba de incrementar la capacidad de las células fotosensibles, mientras que en 1931, el ingeniero alemán Bruno Lange había desarrollado una fotocélula usando seleniuro de plata en lugar de óxido de cobre.

La era moderna de la tecnología solar no llegó hasta el año 1954, cuando los investigadores estadounidenses Gerald Pearson, Calvin S. Fuller y Daryl Chapin, de los Laboratorios Bell, descubrieron de manera accidental que los semiconductores de silicio dopado con ciertas impurezas eran muy sensibles a la luz. Estos avances contribuyeron a la fabricación de la primera célula solar comercial. Emplearon una unión difusa de silicio p–n, con una conversión de la energía solar de aproximadamente 6%, un logro comparado con las células de selenio que difícilmente alcanzaban el 0,5%.

Posteriormente el estadounidense Les Hoffman, presidente de la compañía Hoffman Electronics, a través de su división de semiconductores fue uno de los pioneros en la fabricación y producción a gran escala de células solares. Entre 1954 y 1960, Hoffman logró mejorar la eficiencia de las células fotovoltaicas hasta el 14%, reduciendo los costes de fabricación para conseguir un producto que pudiera ser comercializado.

Al principio, las células fotovoltaicas se emplearon de forma minoritaria para alimentar eléctricamente juguetes y en otros usos menores, dado que el coste de producción de electricidad mediante estas células primitivas era demasiado elevado: en términos relativos, una célula que produjera un vatio de energía mediante luz solar podía costar 250 dólares, en comparación con los dos o tres dólares que costaba un vatio procedente de una central termoeléctrica de carbón.

Las células fotovoltaicas fueron rescatadas del olvido gracias a la carrera espacial y a la sugerencia de utilizarlas en uno de los primeros satélites puestos en órbita alrededor de la Tierra. La Unión Soviética lanzó su primer satélite espacial en el año 1957, y los Estados Unidos le seguiría un año después. La primera nave espacial que usó paneles solares fue el satélite norteamericano Vanguard 1, lanzado en marzo de 1958 (hoy en día el satélite más antiguo aún en órbita). En el diseño de éste se usaron células solares creadas por Peter Iles en un esfuerzo encabezado por la compañía Hoffman Electronics. El sistema fotovoltaico le permitió seguir transmitiendo durante siete años mientras que las baterías químicas se agotaron en sólo 20 días.

En 1959, Estados Unidos lanzó el Explorer 6. Este satélite llevaba instalada una serie de módulos solares, soportados en unas estructuras externas similares a unas alas, formados por 9600 células solares de la empresa Hoffman. Este tipo de dispositivos se convirtió posteriormente en una característica común de muchos satélites. Había cierto escepticismo inicial sobre el funcionamiento del sistema, pero en la práctica las células solares demostraron ser un gran éxito, y pronto se incorporaron al diseño de nuevos satélites.

Pocos años después, en 1962, el Telstar se convirtió en el primer satélite de comunicaciones equipado con células solares, capaces de proporcionar una potencia de 14W. Este hito generó un gran interés en la producción y lanzamiento de satélites geoestacionarios para el desarrollo de las comunicaciones, en los que la energía provendría de un dispositivo de captación de la luz solar. Fue un desarrollo crucial que estimuló la investigación por parte de algunos gobiernos y que impulsó la mejora de los paneles fotovoltaicos.
Gradualmente, la industria espacial se decantó por el uso de células solares de arseniuro de galio (GaAs), debido a su mayor eficiencia frente a las células de silicio. En 1970 la primera célula solar con heteroestructura de arseniuro de galio y altamente eficiente se desarrolló en la Unión Soviética por Zhorés Alfiórov y su equipo de investigación.

A partir de 1971, las estaciones espaciales soviéticas del programa Salyut fueron los primeros complejos orbitales tripulados en obtener su energía a partir de células solares, acopladas en estructuras a los laterales del módulo orbital, al igual que la estación norteamericana Skylab, pocos años después.

En la década de 1970, tras la primera crisis del petróleo, el Departamento de Energía de los Estados Unidos y la agencia espacial NASA iniciaron el estudio del concepto de energía solar en el espacio, que ambicionaba el abastecimiento energético terrestre mediante satélites espaciales. En 1979 propusieron una flota de satélites en órbita geoestacionaria, cada uno de los cuales mediría 5 x 10 km y produciría entre 5 y 10GW. La construcción implicaba la creación de una gran factoría espacial donde trabajarían continuamente cientos de astronautas. Este gigantismo era típico de una época en la que se proyectaba la creación de grandes ciudades espaciales. Dejando aparte las dificultades técnicas, la propuesta fue desechada en 1981 por implicar un coste disparatado.A mediados de la década de 1980, con el petróleo de nuevo en precios bajos, el programa fue cancelado.

No obstante, las aplicaciones fotovoltaicas en los satélites espaciales continuaron su desarrollo. La producción de equipos de deposición química de metales por vapores orgánicos o MOCVD ("Metal Organic Chemical Vapor Deposition") no se desarrolló hasta la década de 1980, limitando la capacidad de las compañías en la manufactura de células solares de arseniuro de galio. La primera compañía que manufacturó paneles solares en cantidades industriales, a partir de uniones simples de GaAs, con una eficiencia del 17 % en AM0 (""), fue la norteamericana "Applied Solar Energy Corporation" (ASEC). Las células de doble unión comenzaron su producción en cantidades industriales por ASEC en 1989, de manera accidental, como consecuencia de un cambio del GaAs sobre los sustratos de GaAs, a GaAs sobre sustratos de germanio.

La tecnología fotovoltaica, si bien no es la única que se utiliza, sigue predominando a principios del siglo XXI en los satélites de órbita terrestre. Por ejemplo, las sondas Magallanes, Mars Global Surveyor y Mars Observer, de la NASA, usaron paneles fotovoltaicos, así como el Telescopio espacial Hubble, en órbita alrededor de la Tierra. La Estación Espacial Internacional, también en órbita terrestre, está dotada de grandes sistemas fotovoltaicos que alimentan todo el complejo espacial, al igual que en su día la estación espacial Mir. Otros vehículos espaciales que utilizan la energía fotovoltaica para abastecerse son la sonda Mars Reconnaissance Orbiter, Spirit y Opportunity, los robots de la NASA en Marte.

La nave Rosetta, lanzada en 2004 en órbita hacia un cometa tan lejano del Sol como el planeta Júpiter (5,25AU), dispone también de paneles solares; anteriormente, el uso más lejano de la energía solar espacial había sido el de la sonda Stardust, a 2 AU. La energía fotovoltaica se ha empleado también con éxito en la misión europea no tripulada a la Luna, SMART-1, proporcionando energía a su propulsor de efecto Hall. La sonda espacial Juno será la primera misión a Júpiter en usar paneles fotovoltaicos en lugar de un generador termoeléctrico de radioisótopos, tradicionalmente usados en las misiones espaciales al exterior del Sistema Solar.
Actualmente se está estudiando el potencial de la fotovoltaica para equipar las naves espaciales que orbiten más allá de Júpiter.

Desde su aparición en la industria aeroespacial, donde se ha convertido en el medio más fiable para suministrar energía eléctrica en los vehículos espaciales, la energía solar fotovoltaica ha desarrollado un gran número de aplicaciones terrestres. La primera instalación comercial de este tipo se realizó en 1966, en el faro de la isla Ogami (Japón), permitiendo sustituir el uso de gas de antorcha por una fuente eléctrica renovable y autosuficiente. Se trató del primer faro del mundo alimentado mediante energía solar fotovoltaica, y fue crucial para demostrar la viabilidad y el potencial de esta fuente de energía.

Las mejoras se produjeron de forma lenta durante las siguientes dos décadas, y el único uso generalizado se produjo en las aplicaciones espaciales, en las que su relación potencia a peso era mayor que la de cualquier otra tecnología competidora. Sin embargo, este éxito también fue la razón de su lento crecimiento: el mercado aeroespacial estaba dispuesto a pagar cualquier precio para obtener las mejores células posibles, por lo que no había ninguna razón para invertir en soluciones de menor costo si esto reducía la eficiencia. En su lugar, el precio de las células era determinado en gran medida por la industria de los semiconductores; su migración hacia la tecnología de circuitos integrados en la década de 1960 dio lugar a la disponibilidad de lingotes más grandes a precios relativamente inferiores. Al caer su precio, el precio de las células fotovoltaicas resultantes descendió en igual medida. Sin embargo, la reducción de costes asociada a esta creciente popularización de la energía fotovoltaica fue limitada, y en 1970 el coste de las células solares todavía se estimaba en 100 dólares por vatio ($/Wp).

A finales de la década de 1960, el químico industrial estadounidense Elliot Berman estaba investigando un nuevo método para la producción de la materia prima de silicio a partir de un proceso en cinta. Sin embargo, encontró escaso interés en su proyecto y no pudo obtener la financiación necesaria para su desarrollo. Más tarde, en un encuentro casual, fue presentado a un equipo de la compañía petrolera Exxon que estaban buscando proyectos estratégicos a 30 años vista. El grupo había llegado a la conclusión de que la energía eléctrica sería mucho más costosa en el año 2000, y consideraba que este aumento de precio haría más atractivas a las nuevas fuentes de energía alternativas, siendo la energía solar la más interesante entre estas. En 1969, Berman se unió al laboratorio de Exxon en Linden, Nueva Jersey, denominado "Solar Power Corporation" (SPC).

Su esfuerzo fue dirigido en primer lugar a analizar el mercado potencial para identificar los posibles usos que existían para este nuevo producto, y rápidamente descubrió que si el coste por vatio se redujera desde los 100$/Wp a cerca de 20$/Wp surgiría una importante demanda. Consciente de que el concepto del «silicio en cinta» podría tardar años en desarrollarse, el equipo comenzó a buscar maneras de reducir el precio a 20$/Wp usando materiales existentes. La constatación de que las células existentes se basaban en el proceso estándar de fabricación de semiconductores supuso un primer avance, incluso aunque no se tratara de un material ideal. El proceso comenzaba con la formación de un lingote de silicio, que se cortaba transversalmente en discos llamados obleas. Posteriormente se realizaba el pulido de las obleas y, a continuación, para su uso como células, se dotaba de un recubrimiento con una capa anti reflectante. Berman se dio cuenta de que las obleas de corte basto ya tenían de por sí una superficie frontal anti reflectante perfectamente válida, y mediante la impresión de los electrodos directamente sobre esta superficie, se eliminaron dos pasos importantes en el proceso de fabricación de células.

Su equipo también exploró otras formas de mejorar el montaje de las células en matrices, eliminando los costosos materiales y el cableado manual utilizado hasta entonces en aplicaciones espaciales. Su solución consistió en utilizar circuitos impresos en la parte posterior, plástico acrílico en la parte frontal, y pegamento de silicona entre ambos, embutiendo las células. Berman se dio cuenta de que el silicio ya existente en el mercado ya era «suficientemente bueno» para su uso en células solares. Las pequeñas imperfecciones que podían arruinar un lingote de silicio (o una oblea individual) para su uso en electrónica, tendrían poco efecto en aplicaciones solares. Las células fotovoltaicas podían fabricarse a partir del material desechado por el mercado de la electrónica, lo que traería como consecuencia una gran mejora de su precio.

Poniendo en práctica todos estos cambios, la empresa comenzó a comprar a muy bajo coste silicio rechazado a fabricantes ya existentes. Mediante el uso de las obleas más grandes disponibles, lo que reducía la cantidad de cableado para un área de panel dado, y empaquetándolas en paneles con sus nuevos métodos, en 1973 SPC estaba produciendo paneles a 10$/Wp y vendiéndolos a 20$/Wp, disminuyendo el precio de los módulos fotovoltaicos a una quinta parte en sólo dos años.

SPC comenzó a contactar con las compañías fabricantes de boyas de navegación ofreciéndoles el producto, pero se encontró con una situación curiosa. La principal empresa del sector era "Automatic Power", un fabricante de baterías desechables. Al darse cuenta de que las células solares podían comerse parte del negocio y los beneficios que el sector de baterías le producía, "Automatic Power" compró un prototipo solar de "Hoffman Electronics" para terminar arrinconándolo. Al ver que no había interés por parte de "Automatic Power", SPC se volvió entonces a "Tideland Signal", otra compañía suministradora de baterías formada por exgerentes de "Automatic Power". Tideland presentó en el mercado una boya alimentada mediante energía fotovoltaica y pronto estaba arruinando el negocio de "Automatic Power".

El momento no podía ser más adecuado, el rápido aumento en el número de plataformas petrolíferas en alta mar y demás instalaciones de carga produjo un enorme mercado entre las compañías petroleras. Como "Tideland" había tenido éxito, "Automatic Power" comenzó entonces a procurarse su propio suministro de paneles solares fotovoltaicos. Encontraron a Bill Yerkes, de "Solar Power International" (SPI) en California, que estaba buscando un mercado donde vender su producto. SPI pronto fue adquirida por uno de sus clientes más importantes, el gigante petrolero ARCO, formando ARCO Solar. La fábrica de ARCO Solar en Camarillo (California) fue la primera dedicada a la construcción de paneles solares, y estuvo en funcionamiento continuo desde su compra por ARCO en 1977 hasta 2011 cuando fue cerrada por la empresa SolarWorld.

Esta situación se combinó con la crisis del petróleo de 1973. Las compañías petroleras disponían ahora de ingentes fondos debido a sus enormes ingresos durante la crisis, pero también eran muy conscientes de que su éxito futuro dependería de alguna otra fuente de energía. En los años siguientes, las grandes compañías petroleras comenzaron la creación de una serie de empresas de energía solar, y fueron durante décadas los mayores productores de paneles solares. Las compañías ARCO, Exxon, Shell, Amoco (más tarde adquirida por BP) y Mobil mantuvieron grandes divisiones solares durante las décadas de 1970 y 1980. Las empresas de tecnología también realizaron importantes inversiones, incluyendo General Electric, Motorola, IBM, Tyco y RCA.

En las décadas transcurridas desde los avances de Berman, las mejoras han reducido los costes de producción por debajo de 1$/Wp, con precios menores de 2$/Wp para todo el sistema fotovoltaico. El precio del resto de elementos de una instalación fotovoltaica supone ahora un mayor coste que los propios paneles.

A medida que la industria de los semiconductores se desarrolló hacia lingotes cada vez más grandes, los equipos más antiguos quedaron disponibles a precios reducidos. Las células crecieron en tamaño cuando estos equipos antiguos se hicieron disponibles en el mercado excedentario. Los primeros paneles de ARCO Solar se equipaban con células de 2 a 4 pulgadas (51 a 100mm) de diámetro. Los paneles en la década de 1990 y principios de 2000 incorporaban generalmente células de 5 pulgadas (125mm), y desde el año 2008 casi todos los nuevos paneles utilizan células de 6 pulgadas (150mm). También la introducción generalizada de los televisores de pantalla plana a finales de la década de 1990 y principios de 2000 llevó a una amplia disponibilidad de grandes láminas de vidrio de alta calidad, que se utilizan en la parte frontal de los paneles.

En términos de las propias células, solo ha habido un cambio importante. Durante la década de 1990, las células de polisilicio se hicieron cada vez más populares. Estas células ofrecen menos eficiencia que aquellas de monosilicio, pero se cultivan en grandes cubas que reducen en gran medida el coste de producción. A mediados de la década de 2000, el polisilicio dominaba en el mercado de paneles de bajo coste.

La producción industrial a gran escala de paneles fotovoltaicos despegó en la década de 1980, y entre sus múltiples usos se pueden destacar:

La energía solar fotovoltaica es ideal para aplicaciones de telecomunicaciones, entre las que se encuentran por ejemplo las centrales locales de telefonía, antenas de radio y televisión, estaciones repetidoras de microondas y otros tipos de enlaces de comunicación electrónicos. Esto es debido a que, en la mayoría de las aplicaciones de telecomunicaciones, se utilizan baterías de almacenamiento y la instalación eléctrica se realiza normalmente en corriente continua (DC). En terrenos accidentados y montañosos, las señales de radio y televisión pueden verse interferidas o reflejadas debido al terreno ondulado. En estos emplazamientos, se instalan transmisores de baja potencia (LPT) para recibir y retransmitir la señal entre la población local.

Las células fotovoltaicas también se utilizan para alimentar sistemas de comunicaciones de emergencia, por ejemplo en los postes de SOS (Teléfonos de emergencia) en carreteras, señalización ferroviaria, balizamiento para protección aeronáutica, estaciones meteorológicas o sistemas de vigilancia de datos ambientales y de calidad del agua.

La reducción en el consumo energético de los circuitos integrados, hizo posible a finales de la década de 1970 el uso de células solares como fuente de electricidad en calculadoras, tales como la Royal "Solar 1", Sharp "EL-8026" o Teal "Photon".

También otros dispositivos fijos que utilizan la energía fotovoltaica han visto aumentar su uso en las últimas décadas, en lugares donde el coste de conexión a la red eléctrica o el uso de pilas desechables es prohibitivamente caro. Estas aplicaciones incluyen por ejemplo las lámparas solares, farolas solares, bombas de agua, parquímetros, teléfonos de emergencia, compactadores de basura, señales de tráfico temporales o permanentes, estaciones de carga o sistemas remotos de vigilancia.

En entornos aislados, donde se requiere poca potencia eléctrica y el acceso a la red es difícil, las placas fotovoltaicas se emplean como alternativa económicamente viable desde hace décadas. Para comprender la importancia de esta posibilidad, conviene tener en cuenta que aproximadamente una cuarta parte de la población mundial todavía no tiene acceso a la energía eléctrica.

En los países en desarrollo, muchos pueblos se encuentran situados en áreas remotas, a varios kilómetros de la red eléctrica más próxima. Debido a ello, se está incorporando la energía fotovoltaica de forma creciente para proporcionar suministro eléctrico a viviendas o instalaciones médicas en áreas rurales. Por ejemplo, en lugares remotos de India un programa de iluminación rural ha provisto iluminación mediante lámparas LED alimentadas con energía solar para sustituir a las lámparas de queroseno. El precio de las lámparas solares era aproximadamente el mismo que el coste del suministro de queroseno durante unos pocos meses. Cuba y otros países de Latinoamérica están trabajando para proporcionar energía fotovoltaica en zonas alejadas del suministro de energía eléctrica convencional. Estas son áreas en las que los beneficios sociales y económicos para la población local ofrecen una excelente razón para instalar paneles fotovoltaicos, aunque normalmente este tipo de iniciativas se han visto relegadas a puntuales esfuerzos humanitarios.

También se emplea la fotovoltaica para alimentar instalaciones de bombeo para sistemas de riego, agua potable en áreas rurales y abrevaderos para el ganado, o para sistemas de desalinización de agua.

Los sistemas de bombeo fotovoltaico (al igual que los alimentados mediante energía eólica) son muy útiles allí donde no es posible acceder a la red general de electricidad o bien supone un precio prohibitivo. Su coste es generalmente más económico debido a sus menores costes de operación y mantenimiento, y presentan un menor impacto ambiental que los sistemas de bombeo alimentados mediante motores de combustión interna, que tienen además una menor fiabilidad.

Las bombas utilizadas pueden ser tanto de corriente alterna (AC) como corriente continua (DC). Normalmente se emplean motores de corriente continua para pequeñas y medianas aplicaciones de hasta 3 kW de potencia, mientras que para aplicaciones más grandes se utilizan motores de corriente alterna acoplados a un inversor que transforma para su uso la corriente continua procedente de los paneles fotovoltaicos. Esto permite dimensionar sistemas desde 0,15 kW hasta más de 55 kW de potencia, que pueden ser empleados para abastecer complejos sistemas de irrigación o almacenamiento de agua.

Debido al descenso de costes de la energía solar fotovoltaica, se está extendiendo asimismo el uso de sistemas híbridos solar-diésel, que combinan esta energía con generadores diésel para producir electricidad de forma continua y estable. Este tipo de instalaciones están equipadas normalmente con equipos auxiliares, tales como baterías y sistemas especiales de control para lograr en todo momento la estabilidad del suministro eléctrico del sistema.

Debido a su viabilidad económica (el transporte de diésel al punto de consumo suele ser costoso) en muchos casos se sustituyen antiguos generadores por fotovoltaica, mientras que las nuevas instalaciones híbridas se diseñan de tal manera que permiten utilizar el recurso solar siempre que está disponible, minimizando el uso de los generadores, disminuyendo así el impacto ambiental de la generación eléctrica en comunidades remotas y en instalaciones que no están conectadas a la red eléctrica. Un ejemplo de ello lo constituyen las empresas mineras, cuyas explotaciones se encuentran normalmente en campo abierto, alejadas de los grandes núcleos de población. En estos casos, el uso combinado de la fotovoltaica permite disminuir en gran medida la dependencia del combustible diésel, permitiendo ahorros de hasta el 70 % en el coste de la energía.

Este tipo de sistemas también puede utilizarse en combinación con otras fuentes de generación de energía renovable, tales como la energía eólica.

Aunque la fotovoltaica todavía no se utiliza de forma generalizada para proporcionar tracción en el transporte, se está utilizando cada vez en mayor medida para proporcionar energía auxiliar en barcos y automóviles. Algunos vehículos están equipados con aire acondicionado alimentado mediante paneles fotovoltaicos para limitar la temperatura interior en los días calurosos, mientras que otros prototipos híbridos los utilizan para recargar sus baterías sin necesidad de conectarse a la red eléctrica. Se ha demostrado sobradamente la posibilidad práctica de diseñar y fabricar vehículos propulsados mediante energía solar, así como barcos y aviones, siendo considerado el transporte rodado el más viable para la fotovoltaica.

El "Solar Impulse" es un proyecto dedicado al desarrollo de un avión propulsado únicamente mediante energía solar fotovoltaica. El prototipo puede volar durante el día propulsado por las células solares que cubren sus alas, a la vez que carga las baterías que le permiten mantenerse en el aire durante la noche.

La energía solar también se utiliza de forma habitual en faros, boyas y balizas de navegación marítima, vehículos de recreo, sistemas de carga para los acumuladores eléctricos de los barcos, y sistemas de protección catódica. La recarga de vehículos eléctricos está cobrando cada vez mayor importancia.

Muchas instalaciones fotovoltaicas se encuentran a menudo situadas en los edificios: normalmente se sitúan sobre un tejado ya existente, o bien se integran en elementos de la propia estructura del edificio, como tragaluces, claraboyas o fachadas.

Alternativamente, un sistema fotovoltaico también puede ser emplazado físicamente separado del edificio, pero conectado a la instalación eléctrica del mismo para suministrar energía. En 2010, más del 80 % de los 9000MW de fotovoltaica que Alemania tenía en funcionamiento por entonces, se habían instalado sobre tejados.

La fotovoltaica integrada en edificios ("BIPV", en sus siglas en inglés) se está incorporando de forma cada vez más creciente como fuente de energía eléctrica principal o secundaria en los nuevos edificios domésticos e industriales, e incluso en otros elementos arquitectónicos, como por ejemplo puentes. Las tejas con células fotovoltaicas integradas son también bastante comunes en este tipo de integración.

Según un estudio publicado en 2011, el uso de imágenes térmicas ha demostrado que los paneles solares, siempre que exista una brecha abierta por la que el aire pueda circular entre los paneles y el techo, proporcionan un efecto de refrigeración pasiva en los edificios durante el día y además ayudan a mantener el calor acumulado durante la noche.

Una de las principales aplicaciones de la energía solar fotovoltaica más desarrollada en los últimos años, consiste en las centrales conectadas a red para suministro eléctrico, así como los sistemas de autoconsumo fotovoltaico, de potencia generalmente menor, pero igualmente conectados a la red eléctrica.

Aunque los paneles solares suelen instalarse en tierra, es posible instalarlos flotando sobre aguas de embalses o lagos tranquilos. Aunque es más caro, tiene muchas ventajas: reduce las pérdidas por evaporación del agua embalsada, mejora su calidad (porque crecen menos algas), la instalación es más sencilla, se facilita la refrigeración de los propios paneles (con lo que aumenta la energía que producen) y supone una forma alternativa de que los embalses hidroeléctricos generen electricidad, sin gastar el agua que almacenan ni ocupar terrenos adicionales.

Una planta solar fotovoltaica cuenta con distintos elementos que permiten su funcionamiento, como son los paneles fotovoltaicos para la captación de la radiación solar, y los inversores para la transformación de la corriente continua en corriente alterna. Existen otros, los más importantes se mencionan a continuación:

Generalmente, un módulo o panel fotovoltaico consiste en una asociación de células, encapsulada en dos capas de EVA (etileno-vinilo-acetato), entre una lámina frontal de vidrio y una capa posterior de un polímero termoplástico (frecuentemente se emplea el tedlar) u otra lámina de cristal cuando se desea obtener módulos con algún grado de transparencia. Muy frecuentemente este conjunto es enmarcado en una estructura de aluminio anodizado con el objetivo de aumentar la resistencia mecánica del conjunto y facilitar el anclaje del módulo a las estructuras de soporte.

Las células más comúnmente empleadas en los paneles fotovoltaicos son de silicio, y se puede dividir en tres subcategorías:

La corriente eléctrica continua que proporcionan los módulos fotovoltaicos se puede transformar en corriente alterna mediante un aparato electrónico llamado inversor e inyectar en la red eléctrica (para venta de energía) o bien en la red interior (para autoconsumo).

El proceso, simplificado, sería el siguiente:

En las etapas iniciales del desarrollo de los inversores fotovoltaicos, los requisitos de los operadores de las redes eléctricas a la que se conectaban solicitaban únicamente el aporte de energía activa y la desconexión del inversor de la red si ésta excedía de unos ciertos límites de tensión y frecuencia. Con el progresivo desarrollo de estos equipos y la cada vez mayor importancia de las redes eléctricas inteligentes, los inversores son ya capaces de proveer energía reactiva e incluso aportar estabilidad a la red eléctrica.

El uso de seguidores a uno o dos ejes permite aumentar considerablemente la producción solar, en torno al 30 % para los primeros y un 6 % adicional para los segundos, en lugares de elevada radiación directa.

Los seguidores solares son bastante comunes en aplicaciones fotovoltaicas. Existen de varios tipos:

Es el elemento que transporta la energía eléctrica desde su generación, para su posterior distribución y transporte. Su dimensionamiento viene determinado por el criterio más restrictivo entre la máxima caída de tensión admisible y la intensidad máxima admisible. Aumentar las secciones de conductor que se obtienen como resultado de los cálculos teóricos aporta ventajas añadidas como:


Otro tipo de tecnología en las plantas fotovoltaicas son las que utilizan una tecnología de concentración llamada CPV por sus siglas en inglés ("Concentrated Photovoltaics") para maximizar la energía solar recibida por la instalación, al igual que en una central térmica solar. Las instalaciones de concentración fotovoltaica se sitúan en emplazamientos de alta irradiación solar directa, como son los países a ambas riberas del Mediterráneo, Australia, Estados Unidos, China, Sudáfrica, México, etc. Hasta el año 2006 estas tecnologías formaban parte del ámbito de investigación, pero en los últimos años se han puesto en marcha instalaciones de mayor tamaño como la de ISFOC (Instituto de Sistemas Solares Fotovoltaicos de Concentración) en Puertollano (Castilla La Mancha) con 3 MW suministrando electricidad a la red eléctrica.

La idea básica de la concentración fotovoltaica es la sustitución de material semiconductor por material reflectante o refractante (más barato). El grado de concentración puede alcanzar un factor de 1000, de tal modo que, dada la pequeña superficie de célula solar empleada, se puede utilizar la tecnología más eficiente (triple unión, por ejemplo). Por otro lado, el sistema óptico introduce un factor de pérdidas que hace recuperar menos radiación que la fotovoltaica plana. Esto, unido a la elevada precisión de los sistemas de seguimiento, constituye la principal barrera a resolver por la tecnología de concentración.

Recientemente se ha anunciado el desarrollo de plantas de grandes dimensiones (por encima de 1 MW). Las plantas de concentración fotovoltaica utilizan un seguidor de doble eje para posibilitar un máximo aprovechamiento del recurso solar durante todo el día.

Entre los años 2001 y 2016 se ha producido un crecimiento exponencial de la producción fotovoltaica, duplicándose aproximadamente cada dos años. La potencia total fotovoltaica instalada en el mundo (conectada a red) ascendía a 16gigavatios (GW) en 2008, 40GW en 2010, 100GW en 2012, 180GW en 2014, 300GW en 2016 y 500GW en 2018.
Históricamente, Estados Unidos lideró la instalación de energía fotovoltaica desde sus inicios hasta 1996, cuando su capacidad instalada alcanzaba los 77MW, más que cualquier otro país hasta la fecha. En los años posteriores, fueron superados por Japón, que mantuvo el liderato hasta que a su vez Alemania la sobrepasó en 2005, manteniendo el liderato desde entonces. A comienzos de 2016, Alemania se aproximaba a los 40GW instalados. Sin embargo, por esas fechas China, uno de los países donde la fotovoltaica está experimentando un crecimiento más vertiginoso superó a Alemania, convirtiéndose desde entonces en el mayor productor de energía fotovoltaica del mundo. Se espera que multiplique su potencia instalada actual hasta los 200GW en 2020.

La capacidad total instalada supone ya una fracción significativa del mix eléctrico en la Unión Europea, cubriendo de media el 3,5% de la demanda de electricidad y alcanzando el 7% en los períodos de mayor producción. En algunos países, como Alemania, Italia, Reino Unido o España, alcanza máximos superiores al 10%, al igual que en Japón o en algunos estados soleados de Estados Unidos, como California. La producción anual de energía eléctrica generada mediante esta fuente de energía a nivel mundial equivalía en 2015 a cerca de 184 TWh, suficiente para abastecer las necesidades energéticas de millones de hogares y cubriendo aproximadamente un 1% de la demanda mundial de electricidad.

La energía fotovoltaica se ha convertido en una de las mayores industrias de la República Popular China. El país asiático es líder mundial por capacidad fotovoltaica, con una potencia instalada a principios de 2019 superior a los 170 GW. Cuenta además con unas 400 empresas fotovoltaicas, entre las que destacan Trina Solar, Jinko Solar y JA Solar, gigantes mundiales en la fabricación de paneles solares. En 2014 producía aproximadamente la mitad de los productos fotovoltaicos que se fabrican en el mundo (China y Taiwán juntos suman más del 60 % de cuota). La producción de paneles y células fotovoltaicas en China se ha incrementado notablemente durante la última década: en 2001 mantenía una cuota inferior al 1 % del mercado mundial, mientras que por las mismas fechas, Japón y los Estados Unidos sumaban más del 70 % de la producción mundial. Sin embargo, la tendencia se ha invertido y en la actualidad China supera ampliamente al resto de productores.

La capacidad de producción de paneles solares chinos prácticamente se cuadruplicó entre los años 2009 y 2011, superando incluso la demanda mundial. Como resultado, la Unión Europea acusó a la industria china de estar realizando dumping, es decir vendiendo sus paneles a precios por debajo de coste, imponiendo aranceles a la importación de este material.

La instalación de energía fotovoltaica se ha desarrollado espectacularmente en el país asiático en años recientes, superando incluso las previsiones iniciales. Debido a tan rápido crecimiento, las autoridades chinas se han visto obligadas a revaluar en varias ocasiones su objetivo de potencia fotovoltaica.

La potencia total instalada en China creció hasta los 77GW a finales de 2016, tras conectar 36GW en el último año, de acuerdo a las estadísticas oficiales del país. En 2017, China había superado el objetivo marcado por el gobierno para 2020, una potencia fotovoltaica de 100GW. Por ello a finales de 2018 se anunció que China podría elevar su objetivo solar para 2020 a más de 200 GW.

Este crecimiento refleja el abrupto descenso de costes de la energía fotovoltaica, que actualmente comienza a ser una opción más barata que otras fuentes de energía, tanto a precios minoristas como comerciales. Fuentes del gobierno chino han afirmado que la fotovoltaica presentará precios más competitivos que el carbón y el gas (aportando además una mayor independencia energética) a finales de esta década.

Estados Unidos es desde 2010 uno de los países con mayor actividad en el mercado fotovoltaico, cuenta con grandes empresas del sector, como First Solar o SolarCity, así como numerosas plantas de conexión a red. A principios de 2017, Estados Unidos superaba los 40GW de potencia fotovoltaica instalada, suficiente para proporcionar electricidad a más de 8 millones de hogares, tras duplicar su capacidad solar en menos de dos años.

Aunque Estados Unidos no mantiene una política energética nacional uniforme en todo el país en lo referente a fotovoltaica, muchos estados han fijado individualmente objetivos en materia de energías renovables, incluyendo en esta planificación a la energía solar en diferentes proporciones. En este sentido, el gobernador de California Jerry Brown ha firmado una legislación requiriendo que el 33 % de la electricidad del estado se genere mediante energías renovables a finales de 2020. Estas medidas se han visto apoyadas desde el gobierno federal con la adopción del Investment Tax Credit (ITC), una exención fiscal establecida en 2006 para promover el desarrollo de proyectos fotovoltaicos, y que ha sido extendida recientemente hasta 2023.

Un informe privado recoge que la energía solar fotovoltaica se ha expandido rápidamente durante los últimos 8 años, creciendo a una media del 40 % cada año. Gracias a esta tendencia, el coste del kWh producido mediante energía fotovoltaica se ha visto enormemente reducido, mientras que el coste de la electricidad generada mediante combustibles fósiles no ha dejado de incrementar. Como resultado, el informe concluye que la fotovoltaica alcanzará la paridad de red frente a las fuentes de energía convencionales en muchas regiones de Estados Unidos en 2015. Pero para alcanzar una cuota en el mercado energético del 10 %, prosigue el informe, las compañías fotovoltaicas necesitarán estilizar aún más las instalaciones, de forma que la energía solar se convierta en una tecnología directamente enchufable («"plug-and-play»"). Es decir, que sea sencillo adquirir los componentes de cada sistema y su interconexión sea simple, al igual que su conexión a la red.

Actualmente la mayoría de las instalaciones son conectadas a red y utilizan sistemas de balance neto que permiten el consumo de electricidad nocturno de energía generada durante el día. Nueva Jersey lidera los Estados con la ley de balance neto menos restrictiva, mientras California lidera el número total de hogares con energía solar. Muchos de ellos fueron instalados durante la iniciativa "million solar roof" (un millón de tejados solares).

La tendencia y el ritmo de crecimiento actuales indican que en los próximos años se construirán un gran número de plantas fotovoltaicas en el sur y suroeste del país, donde el terreno disponible es abundante, en los soleados desiertos de California, Nevada y Arizona. Las empresas están adquiriendo cada vez en mayor medida grandes superficies en estas zonas, con la intención de construir mayores plantas a gran escala.

La energía fotovoltaica en Japón, se ha expandido rápidamente desde la década de 1990. El país es uno de los líderes en la fabricación de módulos fotovoltaicos y se encuentra entre los primeros puestos en términos de potencia instalada, con más de 23GW a finales de 2014, la mayor parte conectada a red. La irradiación en Japón es óptima, situándose entre 4,3 y 4,8kWh·m²·día, convirtiéndolo en un país idóneo para el desarrollo de este tipo de energía.

La venta de módulos fotovoltaicos para proyectos comerciales ha crecido rápidamente tras la introducción por parte del Gobierno japonés en julio de 2012 de una tarifa para el incentivo de la fotovoltaica tras el accidente nuclear de Fukushima y la paralización de la mayoría de las centrales nucleares que tiene el país.

La mayoría de módulos procede de fabricantes locales, entre los que destacan Kyocera, Sharp Corporation, Mitsubishi o Sanyo, mientras que una pequeña parte son importados, según se desprende de los datos de la Asociación Japonesa de Energía Fotovoltaica ("Japan Photovoltaic Energy Association", JPA). Tradicionalmente, el mercado fotovoltaico ha estado muy desplazado al segmento residencial, copando hasta el 97 % de la capacidad instalada en todo el país hasta 2012. Aunque esta tendencia se está invirtiendo, todavía más del 75 % de las células y módulos vendidos en Japón a principios de 2012 tuvieron como destino proyectos residenciales, mientras que cerca del 9 % se emplearon en instalaciones fotovoltaicas comerciales.

En 2014, la potencia total fotovoltaica instalada en el país se situaba en torno a los 23GW, que contribuían aproximadamente en un 2,5 % a la demanda eléctrica del país. Durante el verano de 2015, se informó que la producción fotovoltaica en Japón había cubierto en determinados momentos el 10% de la demanda total nacional. Dos años después, en 2016, se sitúa en torno a 42 GW, y la previsión apunta a que el mercado fotovoltaico japonés crecerá aún más en los próximos años.

Alemania dispone a principios de 2016 de una potencia instalada cercana a los 40 GW. Sólo en 2011, Alemania instaló cerca de 7,5GW, y la fotovoltaica produjo 18TW·h de electricidad, el 3 % del total consumido en el país.
El mercado fotovoltaico en Alemania ha crecido considerablemente desde principios del siglo XXI gracias a la creación de una tarifa regulada para la producción de energía renovable, que fue introducida por la «"German Renewable Energy Act»", ley publicada el año 2000. Desde entonces, el coste de las instalaciones fotovoltaicas ha descendido más del 50 % en cinco años, desde 2006. Alemania se ha marcado el objetivo de producir el 35 % de la electricidad mediante energías renovables en 2020 y alcanzar el 100 % en 2050.

En 2012, las tarifas introducidas costaban a Alemania unos 14000 millones de euros por año, tanto para las instalaciones eólicas como solares. Este coste es repartido entre todos los contribuyentes mediante un sobrecoste de 3,6 céntimos de € por kWh (aproximadamente el 15 % del coste total de la electricidad para el consumidor doméstico).

La considerable potencia instalada en Alemania ha protagonizado varios récords durante los últimos años. Durante dos días consecutivos de mayo de 2012, por ejemplo, las plantas solares fotovoltaicas instaladas en el país produjeron 22000MWh en la hora del mediodía, lo que equivale a la potencia de generación de veinte centrales nucleares trabajando a plena capacidad. Alemania pulverizó este récord el 21 de julio de 2013, con una potencia instantánea de 24GW a mediodía. Debido al carácter altamente distribuido de la fotovoltaica alemana, aproximadamente 1,3-1,4 millones de pequeños sistemas fotovoltaicos contribuyeron a esta nueva marca. Aproximadamente el 90 % de los paneles solares instalados en Alemania se encuentran situados sobre tejado.

En junio de 2014, la fotovoltaica alemana volvió a batir récords durante varios días, al producir hasta el 50,6 % de toda la demanda eléctrica durante un solo día, y superar el anterior récord de potencia instantánea hasta los 24,24GW.

A comienzos de verano de 2011, el Gobierno alemán anunció que el esquema actual de tarifas reguladas concluiría cuando la potencia instalada alcanzase los 52GW. Cuando esto suceda, Alemania aplicará un nuevo esquema de tarifas de inyección cuyos detalles no se conocen todavía.

No obstante, consciente de que el almacenamiento de energía mediante baterías es indispensable para el despliegue masivo de renovables como la energía eólica o la fotovoltaica, dada su intermitencia, el 1 de mayo de 2013 Alemania puso en marcha un nuevo programa de ayudas para incentivar sistemas fotovoltaicos con baterías de almacenamiento. De esta manera, se financia a las instalaciones fotovoltaicas menores de 30kW que instalen baterías y acumulen electricidad, con 660 euros por cada kW de almacenamiento de batería. El programa está dotado con 25 millones de euros anuales repartidos en 2013 y 2014, y de esta forma se logra disponer de la energía cuando el recurso no esté disponible —no haya viento o sea de noche—, además de facilitar la estabilidad del sistema eléctrico.

India está densamente poblada y tiene también una gran irradiación solar, lo que hace del país uno de los mejores candidatos para el desarrollo de la fotovoltaica. En 2009, India anunció un programa para acelerar el uso de instalaciones solares en los edificios gubernamentales, al igual que en hospitales y hoteles.

La caída en el precio de los paneles fotovoltaicos ha coincidido con un incremento del precio de la electricidad en la India. El apoyo del gobierno y la abundancia del recurso solar han ayudado a impulsar la adopción de esta tecnología.

El parque solar Charanka, de 345MW (uno de los mayores del mundo) fue puesto en servicio en abril de 2012 y ampliado en 2015, junto a un total de 605MW en la región de Gujarat. La construcción de otros grandes parques solares ha sido anunciada en el estado de Rajasthan. También el parque solar de Dhirubhai Ambani, de 40 MW, fue inaugurado en 2012.

En enero de 2015, el gobierno indio incrementó de forma significativa su planes de desarrollo solar, estableciendo un objetivo de inversiones por valor de 100000 millones de dólares y 100GW de capacidad solar para 2022.

A comienzos de 2017, la potencia total instalada en India se situaba por encima de los 10GW. India espera alcanzar rápidamente los 20GW instalados, cumpliendo su objetivo de crear 1 millón de puestos de trabajo y alcanzar 100GW en 2022.

Italia se encuentra entre los primeros países productores de electricidad procedente de energía fotovoltaica, gracias al programa de incentivos llamado "Conto Energia". El crecimiento ha sido exponencial en los últimos años: la potencia instalada se triplicó en 2010 y se cuadruplicó en 2011, llegando a producir en 2012 el 5,6 % de la energía total consumida en el país.

Este programa contaba con un presupuesto total de 6700 millones de €, alcanzado dicho límite el Gobierno ha dejado de incentivar las nuevas instalaciones, al haberse alcanzado la paridad de red. Un informe publicado en 2013 por el Deutsche Bank concluía que efectivamente la paridad de red se había alcanzado en Italia y otros países del mundo. El sector ha llegado a proporcionar trabajo a unas 100000 personas, especialmente en el sector del diseño e instalación de dichas plantas solares.

Desde mediados de 2012 está vigente una nueva legislación que obliga a registrar todas las plantas superiores a 12kW; las de potencia menor (fotovoltaica de tejado en residencias) están exentas de registro. A finales de 2016, la potencia total instalada se situaba por encima de 19GW, suponiendo una producción energética tan importante que varias centrales de gas operaban a mitad de su potencial durante el día.

La energía solar en Reino Unido, aunque relativamente desconocida hasta hace poco, ha despegado muy rápidamente en años recientes, debido a la drástica caída del precio de los paneles fotovoltaicos y la introducción de tarifas reguladas a partir de abril de 2010. En 2014, había censadas ya unas 650000 instalaciones solares en las islas británicas, con una capacidad total cercana a los 5GW. La planta solar más grande del país se encuentra en "Southwick Estate", cerca de Fareham, y cuenta con una potencia de 48MW. Fue inaugurada en marzo de 2015.

En 2012, el gobierno británico de David Cameron se comprometió a abastecer cuatro millones de hogares mediante energía solar en menos de ocho años, lo que equivale a instalar unos 22GW de capacidad fotovoltaica antes de 2020. A principios de 2016, Reino Unido había instalado más de 10GW de energía solar fotovoltaica.

Entre los meses de abril y septiembre de 2016, la energía solar produjo en Reino Unido más electricidad (6964GWh) que la producida mediante carbón (6342GWh), ambas se sitúan en torno a un 5% de la demanda.

El mercado francés es el cuarto más importante dentro de la Unión Europea, tras los mercados de Alemania, Italia y Reino Unido. A finales de 2014 contaba con más de 5GW instalados, y mantiene actualmente un crecimiento sostenido, estimándose que en 2015 conectará a la red eléctrica 1GW adicional a la capacidad actual. Recientemente, el país galo incrementó el cupo de sus subastas para energía fotovoltaica de 400 a 800 MW, como consecuencia del reconocimiento gubernamental a la cada vez mayor competitividad de la energía solar.

En Francia se encuentra una de las plantas fotovoltaicas más grandes de Europa, un proyecto de 300MW llamado Cestas. Su entrada en funcionamiento tuvo lugar a finales de 2015, proporcionando al sector fotovoltaico un ejemplo a seguir por el resto de la industria europea.

España es uno de los países de Europa con mayor irradiación anual. Esto hace que la energía solar sea en este país más rentable que en otros. Regiones como el norte de España, que generalmente se consideran poco adecuadas para la energía fotovoltaica, reciben más irradiación anual que la media en Alemania, país que mantiene desde hace años el liderazgo en la promoción de la energía solar fotovoltaica.

Desde principios de la década de 2000, en concordancia con las medidas de apoyo a las energías renovables que se estaban llevando a cabo en el resto de Europa, se había venido aprobando la regulación que establece las condiciones técnicas y administrativas, y que supuso el inicio de un lento despegue de la fotovoltaica en España. En 2004, el gobierno español eliminó las barreras económicas para la conexión de las energías renovables a la red eléctrica. El Real Decreto 436/2004 igualó las condiciones para su producción a gran escala, y garantizó su venta mediante primas a la generación.

Gracias a esta regulación, y el posterior RD 661/2007, España fue en el año 2008 uno de los países con más potencia fotovoltaica instalada del mundo, con 2708MW instalados en un solo año. Sin embargo, posteriores modificaciones en la legislación del sector ralentizaron la construcción de nuevas plantas fotovoltaicas, de tal forma que en 2009 se instalaron tan sólo 19MW, en 2010, 420MW, y en 2011 se instalaron 354MW, correspondiendo al 2 % del total de la Unión Europea. 

En términos de producción energética, en 2010 la energía fotovoltaica cubrió en España aproximadamente el 2 % de la generación de electricidad, mientras que en 2011 y 2012 representó el 2,9 %, y en 2013 el 3,1 % de la generación eléctrica según datos del operador, Red Eléctrica.
En 2018, la cuota de la energía solar fotovoltaica en España alcanzó el 3,2% de toda la energía producida a nivel nacional.

A principios de 2012, el Gobierno español aprobó un Real Decreto Ley por el que se paralizó la instalación de nuevas centrales fotovoltaicas y demás energías renovables. A finales de 2015 la potencia fotovoltaica instalada en España ascendía a 4667MW. En 2017, España cayó por primera vez de la lista de los diez países con mayor capacidad fotovoltaica instalada, al ser superado por Australia y Corea del Sur. Sin embargo, en julio de 2017, el Gobierno organizó una subasta que adjudicó más de 3500 MW de nuevas plantas de energía fotovoltaica, que permitirán a España alcanzar los objetivos de generación de energía renovable establecidos por la Unión Europea para 2020. Como novedad, ni la construcción de las plantas adjudicadas ni su operación supondrá algún coste para el sistema, excepto en el caso de que el precio de mercado baje de un suelo establecido en la subasta. La gran bajada de costes de la energía fotovoltaica ha permitido que grandes empresas hayan licitado a precio de mercado.

En 2019, la fotovoltaica ha incrementado la potencia instalada en España en más de 3.000 MW con una potencia total instalada de 7.800 MW. España posee la mayor planta fotovoltaica conectada de Europa, situada en la localidad de Mula (Murcia), con 494 MW.

En Latinoamérica, la fotovoltaica ha comenzado a despegar en los últimos años. Se ha propuesto la construcción de un buen número de plantas solares en diversos países, a lo largo de toda la región. 

México es el país latinoamericano con mayor capacidad instalada, y tiene aun un enorme potencial en lo que respecta a energía solar. Un 70 % de su territorio presenta una irradiación superior a 4,5kWh/m²/día, lo que lo convierte en un país muy soleado, e implica que utilizando la tecnología fotovoltaica actual, una planta solar de 25km² en cualquier lugar del estado de Chihuahua o el desierto de Sonora (que ocuparía el 0,01 % de la superficie de México) podría proporcionar toda la electricidad demandada por el país. 

El proyecto "Aura Solar", situado en La Paz (Baja California Sur), inaugurado a principios de 2014, que pretendía generar 82GWh al año, suficiente para abastecer el consumo de 164000 habitantes (65 % de la población de La Paz), pero fue arrasado por el huracán Odile en septiembre del mismo año y la planta dejó de operar por varios meses.
En el año 2016 se llevó a cabo la reconstrucción de la planta que terminó a finales del mismo año y desde 2017 a la fecha se encuentra en operación nuevamente. 
Otra planta fotovoltaica de 47MW se encuentra en fase de planificación en Puerto Libertad (Sonora).La planta, originalmente diseñada para albergar 39MW, se amplió para permitir la generación de 107GWh/año.

México cuenta ya con más de 3000MW instalados. Se espera que experimente un mayor crecimiento en los próximos años, con el fin de alcanzar el objetivo de cubrir el 35 % de su demanda energética a partir de energías renovables en 2024, según una ley aprobada por el gobierno mexicano en 2012.

Chile lideraba hasta hace unos años la producción solar en Latinoamérica. La primera planta solar fotovoltaica en Chile fue El Águila, de 2.2 MWp ubicada en Arica, terminada de conectar en 2012. Este país inauguró en junio de 2014 una central fotovoltaica de 100MW, que se convirtió en la mayor realizada hasta la fecha en Latinoamérica. El elevado precio de la electricidad y los altos niveles de radiación que existen en el norte de Chile, han promovido la apertura de un importante mercado libre de subsidios. A finales de 2018, el país andino contaba con 2427MW fotovoltaicos en operación. Chile cuenta con un potencial de más de 1800 GW de energía solar posible en el desierto de Atacama, según un estudio realizado por la GIZ Alemana en Chile (German International Cooperation Agency, 2014). El desierto de Atacama es el lugar con mayor irradiación del mundo con niveles de irradiación global (GHI), por sobre los 2700 kWh/m2 año.

Otros países sudamericanos han comenzado a instalar plantas fotovoltaicas a gran escala, entre ellos Perú. Brasil en cambio está experimentando un crecimiento más lento del sector, en parte debido a la elevada generación mediante energía hidráulica en el país, aunque el estado de Minas Gerais lidera el esfuerzo, tras la aprobación por parte del gobierno brasileño de una fábrica de células y paneles fotovoltaicos en dicha región.

En la siguiente tabla se muestra el detalle de la potencia mundial instalada, desglosada por cada país, desde el año 2002 hasta 2019:

Se estima que la potencia fotovoltaica instalada ha crecido unos 75GW en 2016, y China ha tomado el liderato frente a Alemania siendo ya el mayor productor de energía fotovoltaica. Para 2019, se estima que la potencia total alcanzará en todo el mundo 396GW (escenario moderado) o incluso 540GW (escenario optimista).

La consultora Frost & Sullivan estima que la potencia fotovoltaica se incrementará hasta los 446 GW para 2020, siendo China, India y los Estados Unidos los países con un mayor crecimiento, mientras Europa verá duplicada su capacidad respecto a los niveles actuales. La firma Grand View Research, consultora y analista de mercados radicada en San Francisco, publicó sus estimaciones para el sector en marzo de 2015. El potencial fotovoltaico de países como Brasil, Chile y Arabia Saudí todavía no se ha desarrollado conforme a lo esperado, y se espera que sea desarrollado durante los próximos años. Además de ello, el aumento de la capacidad de manufactura en China se prevé que siga ayudando a disminuir aún más los precios en descenso. La consultora estima que la capacidad fotovoltaica mundial alcance los 490 GW en 2020.

La organización PV Market Alliance (PVMA), un consorcio formado por varias entidades de investigación, calcula que la capacidad global estará entre los 444-630GW en 2020. En el escenario más pesimista, prevé que el ritmo de instalación anual se sitúe entre los 40 y 50 gigavatios al finalizar la década, mientras que en el escenario más optimista estima que se instalen entre 60 y 90 GW anuales durante los próximos cinco años. El escenario intermedio estima que se sitúen entre 50 y 70 GW, para alcanzar 536 GW en 2020. Las cifras de PVMA concuerdan con las publicadas anteriormente por "Solar Power Europe". En junio de 2015, Greentech Media (GTM) publicó su informe "Global PV Demand Outlook" para 2020, que estima que las instalaciones anuales se incrementarán de 40 a 135 GW, alcanzando una capacidad total global de casi 700 GW en 2020. La estimación de GTM es la más optimista de todas las publicadas hasta la fecha, estimando que se instalarán 518 GW entre 2015 y 2020, lo que supone más del doble que otras estimaciones.

Por su parte, EPIA también calcula que la energía fotovoltaica cubrirá entre un 10 y un 15 % de la demanda de Europa en 2030.
Un informe conjunto de esta organización y Greenpeace publicado en 2010 muestra que para el año 2030, un total de 1845 GW fotovoltaicos podrían generar aproximadamente 2646 TWh/año de electricidad en todo el mundo. Combinado con medidas de eficiencia energética, esta cifra representaría cubrir el consumo de casi un 10 % de la población mundial. Para el año 2050, se estima que más del 20 % de la electricidad mundial podría ser cubierto por la energía fotovoltaica.

En Europa y en el resto del mundo se han construido un gran número de centrales fotovoltaicas a gran escala. Actualmente las plantas fotovoltaicas más grandes del mundo son, de acuerdo a su capacidad de producción:
! Proyecto
! País
! Localización
! Potencia
! Año 


</doc>
<doc id="9842" url="https://es.wikipedia.org/wiki?curid=9842" title="Energía solar térmica">
Energía solar térmica

La energía solar térmica o energía termosolar consiste en el aprovechamiento de la energía del Sol para producir calor que puede aprovecharse para cocinar alimentos o para la producción de agua caliente destinada al consumo de agua doméstico, ya sea agua caliente sanitaria, calefacción, o para producción de energía mecánica y, a partir de ella, de energía eléctrica. Adicionalmente puede emplearse para alimentar una máquina de refrigeración por absorción, que emplea calor en lugar de electricidad para producir frío con el que se puede acondicionar el aire de los locales. 

Los colectores de energía solar térmica están clasificados como colectores de baja, media y alta temperatura. Los colectores de baja temperatura generalmente son placas planas usadas para calentar agua. Los colectores de temperatura media también usualmente son placas planas usadas para calentar agua o aire para usos residenciales o comerciales. Los colectores de alta temperatura concentran la luz solar usando espejos o lentes y generalmente son usados para la producción de energía eléctrica. La energía solar térmica es diferente y mucho más eficiente que la energía solar fotovoltaica, la que convierte la energía solar directamente en electricidad. Mientras que las instalaciones generadoras proporcionan solo 600 megavatios de energía solar térmica a nivel mundial a octubre de 2009, otras centrales están bajo construcción por otros 400 megavatios y se están desarrollando otros proyectos de energía termosolar de concentración por un total de 14 gigavatios.
En cuanto a la generación de agua caliente para usos sanitarios (también llamada «agua de manos»), hay dos tipos de instalaciones de los comúnmente llamados calentadores: las de circuito abierto y las de circuito cerrado. En las primeras, el agua de consumo pasa directamente por los colectores solares. Este sistema reduce costos y es más eficiente (energéticamente hablando), pero presenta problemas en zonas con temperaturas por debajo del punto de congelación del agua, así como en zonas con alta concentración de sales que acaban obstruyendo los conductos de los paneles. En las instalaciones de circuito cerrado se distinguen dos sistemas: flujo por termosifón y flujo forzado.
Los paneles solares térmicos tienen un muy bajo impacto ambiental.

La energía solar térmica puede utilizarse para dar apoyo al sistema convencional de calefacción (caldera de gas o eléctrica) mediante colectores solares térmicos y tanques de almacenamiento (boiler), apoyo que habitualmente consiste entre el 10 % y el 40 % de la demanda energética de la calefacción de acuerdo al nivel de aislación de la construcción. Para ello, la instalación o caldera ha de contar con intercambiador de placas (que permitirá conectar el sistema de calefacción solar con la caldera) y un regulador (que dé prioridad en el uso del agua caliente para ser empleada en agua de manos).

Por otro lado, también es posible utilizar colectores solares de aire para calefaccionar y ventilar una vivienda, oficinas y locales comerciales. Estos sistemas de calefacción y ventilación solar por aire están ampliando su uso debido a las ventajas que tiene por su bajo costo, ahorro de energía de calefacción y mejora de la calidad del aire interior.

Otro de los usos importantes de la Energía Solar Térmica es la climatización de piscinas o piletas de natación. Para este fin se utilizan colectores solares descubiertos (unglazed solar collectors) normalmente fabricados en polipropileno, EPDM, polietileno. Su funcionamiento es muy simple, ya que toma agua de la misma bomba de filtrado, la lleva hacia un grupo de colectores colocados normalmente en un techo cercano, calienta el agua y retorna a la piscina como puede observarse en este video . Mediante estos equipos se puede extender la utilización de las piscinas desde la primavera hasta el otoño en climas templados, y a todo el año en climas cálidos. Su instalación es muy económica en comparación a la utilización de calderas a gas o eléctricas y además no consume energía. Su uso está muy extendido en países como Estados Unidos, Canadá, Australia, Brasil, México y Sudáfrica. 

Una instalación Solar Térmica está formada por captadores solares, un circuito primario y secundario, intercambiador de calor, acumulador, vaso de expansión y tuberías. Si el sistema funciona por termosifón será la diferencia de densidad por cambio de temperatura la que moverá el líquido. Si el sistema es forzado entonces necesitaremos además: bombas y un panel de control principal.

Los captadores solares son los elementos que capturan la radiación solar y la convierten en energía térmica, en calor. Como captadores solares se conocen los de placa plana, los de tubos de vacío y los captadores absorbedores sin protección ni aislamiento. Los sistemas de captación planes (o de placa plana) con cubierta de vidrio son los comunes mayoritariamente en la producción de agua caliente sanitaria ACS. El vidrio deja pasar los rayos del Sol, estos calientan unos tubos metálicos que transmiten el calor al líquido de dentro. Los tubos son de color oscuro, ya que las superficies oscuras calientan más.

El vidrio que cubre el captador no solo protege la instalación sino que también permite conservar el calor produciendo un efecto invernadero que mejora el rendimiento del captador.

Están formados de una carcasa de aluminio cerrada y resistente a ambientes marinos, un marco de aluminio eloxat, una junta perimetral libre de siliconas, aislante térmico respetuoso con el medio ambiente de lana de roca, cubierta de vidrio solar de alta transparencia, y finalmente por tubos soldados ultrasónicos.

Los colectores solares se componen de los siguientes elementos:






El alma del sistema es una verja vertical de tubos metálicos, para simplificar, que conducen el agua fría en paralelo, conectados por abajo por un tubo horizontal en la toma de agua fría y por arriba por otro similar al retorno.

La parrilla viene encajada en una cubierta, como la descrita más arriba, normalmente con doble vidrio para arriba y aislante por detrás.

En algunos modelos, los tubos verticales están soldados a una placa metálica para aprovechar la insolación entre tubo y tubo.

En este sistema los tubos metálicos del sistema precedente se sustituyen por tubos de vidrio, introducidos, de uno en uno, en otro tubo de vidrio entre los que se hace el vacío como aislamiento. Estos equipos pueden tener un rendimiento mayor a los de placa plana a temperaturas elevadas de agua caliente o climas muy fríos pero tienen rendimientos menores a temperaturas cercanas al consumo doméstico típico (45°C) o climas templados o cálidos 

Los costos de fabricación son mucho menores que las placas planas. Ya que son fabricados al 100% en cristal borosilicato, por el contrario, los colectores planos son fabricados en cobre por lo que son más costosos de fabricar.

También una ventaja adicional de los tubos de vidrio es su mayor versatilidad de colocación, tanto desde el punto de vista práctico como estético. Al ser cilíndricos, toleran variaciones de hasta 25º sobre la inclinación idónea sin pérdida de rendimiento. Por este motivo es posible adaptarlos a la gran mayoría de las edificaciones existentes. Otro aspecto interesante es que necesitan una superficie de captación solar menor debido a su mayor eficiencia. Además por su forma cilíndrica también son mucho más eficientes, ya que reciben los rayos solares perpendicularmente durante todo el día. Por el contrario los colectores planos son sólo efectivos cuando tienen el sol perpendicularmente.

Este sistema aprovecha el cambio de fase de vapor a líquido dentro de cada tubo, para entregar energía a un segundo circuito de líquido de transporte.

Los elementos son tubos cerrados, normalmente de cobre, que contienen el líquido que, al calentarse por el sol, hierve y se convierte en vapor que sube a la parte superior donde hay un cabezal más ancho (zona de condensación), que en la parte exterior está en contacto con el líquido transportador, el cual siendo más frío que el vapor del tubo, capta el calor y provoca que el vapor se condense y caiga en la parte baja del tubo para volver a empezar el ciclo.

El líquido del tubo puede ser agua, a la que se le ha reducido la presión hasta un vacío parcial, tendrá un punto de ebullición bajo, lo que permite trabajar incluso con la insolación de los rayos infrarrojos en caso de presencia de nubes.

El "tubo de calor" (o tubo de cobre) se puede envolver con una chaqueta de materiales especiales para minimizar las pérdidas por irradiación.

El "tubo de calor" se cierra dentro de otro tubo de vidrio entre los que se hace el vacío como aislamiento. Se suelen emplear tubos de vidrio resistente, para reducir los daños en caso de pequeñas granizadas.

Son hasta un 163 % más eficientes que las placas planas con serpentín e igualmente más baratos en su fabricación con respecto a las placas planas, pues el precio del cristal es más bajo que el cobre del serpentín que contiene la placa plana.

El circuito primario, es circuito cerrado, transporta el calor desde el captador hasta el acumulador (sistema que almacena calor). El líquido calentado (agua o una mezcla de sustancias que puedan transportar el calor) lleva el calor hasta el acumulador. Una vez enfriado, vuelve al colector para volver a calentar, y así sucesivamente.

El intercambiador de calor calienta el agua de consumo a través del calor captado de la radiación solar. Se sitúa en el circuito primario, en su extremo. Tiene forma de serpentín, ya que así se consigue aumentar la superficie de contacto y por lo tanto, la eficiencia.

El agua que entra en el acumulador, siempre que esté más fría que el serpentín, se calentará. Esta agua, calentada en horas de sol, nos quedará disponible para el consumo posterior.

El acumulador es un depósito donde se acumula el agua calentada útil para el consumo. Tiene una entrada para el agua fría y una salida para la caliente. La fría entra por debajo del acumulador donde se encuentra con el intercambiador, a medida que se calienta se desplaza hacia arriba, que es desde donde saldrá el agua caliente para el consumo.

Internamente dispone de un sistema para evitar el efecto corrosivo del agua caliente almacenada sobre los materiales. Por fuera tiene una capa de material aislante que evita pérdidas de calor y está cubierto por un material que protege el aislamiento de posibles humedades y golpes.

El circuito secundario o de consumo, (circuito abierto), entra agua fría de suministro y por el otro extremo del agua calentada se consume (ducha, lavabo...). El agua fría pasa por el acumulador primeramente, donde calienta el agua hasta llegar a una cierta temperatura. Las tuberías de agua caliente del exterior, deben estar cubiertas por aislantes.

Si el consumo incluye calefacción, el sistema emisor de calor (radiadores (60 °C), fan-coil(45 °C), suelo radiante(30 °C), zócalo radiante, muro radiante,…) que es más conveniente utilizar es el de baja temperatura (<=50 °C), de esta manera el sistema solar de calefacción tiene mayor rendimiento.
No obstante, se pueden instalar sistemas que no son de baja temperatura, para así emplear radiadores convencionales.

Las bombas, en caso de que la instalación sea de circulación forzada, son de tipo recirculación (suele haber dos por circuito), trabajando una la mitad del día, y la pareja, la mitad del tiempo restante. La instalación consta de los relojes que llevan el funcionamiento del sistema, hacen el intercambio de las bombas, para que una trabaje las 12 horas primeras y la otra las 12 horas restantes. Si hay dos bombas en funcionamiento, hay la ventaja que en caso de que una deje de funcionar, está la sustituta, de modo que así no se puede parar el proceso ante el fallo de una de estas. El otro motivo a considerar, es que gracias a este intercambio la bomba no sufre tanto, sino que se la deja descansar, enfriar, y cuando vuelve a estar en buen estado (después de las 12 horas) se vuelve a poner en marcha. Esto ocasiona que las bombas puedan alargar durante más el tiempo de funcionamiento sin tener que hacer ningún tipo de mantenimiento previo.

En total y tal como se define anteriormente, suele haber cuatro bombas, dos en cada circuito. Dos en el circuito primario que bombean el agua de los colectores y las otras dos en el circuito secundario que bombean el agua de los acumuladores, en el caso de una instalación de tipo circulación forzada.

El vaso de expansión absorbe variaciones de volumen del fluido caloportador, el cual circula por los conductos del captador, manteniendo la presión adecuada y evitando pérdidas de la masa del fluido.
Es un recipiente con una cámara de gas separada de la de líquidos y con una presión inicial en función de la altura de la instalación.

Lo que más se utiliza es con vaso de expansión cerrado con membrana, sin transferencia de masa en el exterior del circuito.

Las tuberías de la instalación se encuentran recubiertas de un aislante térmico para evitar pérdidas de calor con el entorno. Antiguamente se utilizaban tuberías de cobre. Luego se utilizó tubos PEX-AL-PEX, consistentes en tres capas plástico-aluminio-plástico, mucho más baratos y con mayor vida útil que la tubería de cobre tradicional. Al pasar los años de uso del equipo y por la acumulación de radiación solar, se encontró que el PEX se cristalizaba destruyéndose por presión. Actualmente, se utiliza para circuito cerrado cañerías de acero inoxidable BPDN aislada con espuma elastomérica y rodeada de una mica de EPDM que da aislamiento térmico y proporciona durabilidad al proteger contra la radiación, y fallas por ruptura de uniones y soldaduras.

Se dispone también de un panel principal de control en la instalación, donde se muestran las temperaturas en cada instante (un regulador térmico), de manera que pueda controlarse el funcionamiento del sistema en cualquier momento. Aparecen también los relojes encargados del intercambio de bombas.

Durante el verano, se pueden cubrir las placas, a fin de evitar que se estropeen por las altas temperaturas o bien se pueden utilizar para producir frío solar (aire acondicionado frío).

Especialmente populares son los equipos domésticos compactos, compuestos típicamente por un depósito de unos 150 litros de capacidad y un colector de unos 2 m². Estos equipos, disponibles tanto con circuito abierto como cerrado, pueden suministrar el 90 % de las necesidades de agua caliente anual para una familia de cuatro personas, dependiendo de la radiación y el uso. Estos sistemas evitan la emisión de hasta 4,5 toneladas de gases nocivos para la atmósfera. El tiempo aproximado de retorno energético (tiempo necesario para ahorrar la energía empleada en fabricar el aparato) es de un año y medio aproximadamente. La vida útil de algunos equipos puede superar los 25 años con un mantenimiento mínimo, dependiendo de factores como la calidad del agua.

Estos equipos pueden distinguirse entre:

Equipos de Circulación forzada: Compuesto básicamente de captadores, un acumulador solar, un grupo hidráulico, una regulación y un vaso de expansión.

Equipos por Termosifón: Cuya mayor característica es que el acumulador se sitúa en la cubierta, encima del captador.

Equipos con Sistema Drain-Back: Un sistema compacto y seguro, muy apropiado para viviendas unifamiliares.

Es habitual encontrarse con instalaciones en las que el acumulador contiene una resistencia eléctrica de apoyo, que actúa en caso de que el sistema no sea capaz de alcanzar la temperatura de uso (normalmente 40 °C); en España esta opción ha quedado prohibida tras la aprobación del CTE (Código Técnico de la Edificación) ya que el calor de la resistencia puede, si el panel esta más frío que el acumulador integrado, calentar el panel y perder calor, y por lo tanto energía, a través de él. En algunos países se comercializan equipos que utilizan el gas como apoyo.

Las características constructivas de los colectores responden a la minimización de las pérdidas de energía una vez calentado el fluido que transcurre por los tubos, por lo que se encuentran aislamientos a la conducción (vacío u otros) y a la rerradiación de baja temperatura.

Además de su uso como agua caliente sanitaria, calefacción y refrigeración (mediante máquina de absorción), el uso de placas solares térmicas (generalmente de materiales baratos como el polipropileno) ha proliferado para el calentamiento de piscinas exteriores residenciales, en países donde la legislación impide el uso de energías de otro tipo para este fin.

En muchos países hay subvenciones para el uso doméstico de energía solar, en cuyos casos una instalación doméstica puede amortizarse en unos cinco o seis años. El 29 de septiembre de 2006 entró en vigor en España el Código Técnico de la Edificación, que establece la obligatoriedad de implantar sistemas de agua caliente sanitaria (ACS) con energía solar en todas las nuevas edificaciones, con el objetivo de cumplir con el protocolo de Kioto, pero que olvida la calefacción, que se recoge en las ordenanzas solares de los ayuntamientos.

El colector solar plano es el aparato más representativo de la tecnología solar fototérmica. Su principal aplicación es en el calentamiento de agua para baño y albercas, aunque también se utiliza para secar productos agropecuarios mediante el calentamiento de aire y para destilar agua en comunidades rurales principalmente.

Está constituido básicamente por:

Para la mayoría de los colectores solares se tienen dimensiones características. En términos generales la unidad básica consiste de un colector plano de 1,8 a 2,1 m² de superficie, conectado a un termotanque de almacenamiento de 150 a 200 litros de capacidad; a este sistema frecuentemente se le añaden algunos dispositivos termostáticos de control a fin de evitar congelamientos y pérdidas de calor durante la noche. Las unidades domésticas funcionan mediante el mecanismo de termosifón, es decir, mediante la circulación que se establece en el sistema debido a la diferencia de temperatura de las capas de líquido estratificadas en el tanque de almacenamiento. Para instalaciones industriales se emplean varios módulos conectados en arreglos serie-paralelo, según el caso, y se emplean bombas para establecer la circulación forzada.

Los sistemas de calefacción solar para procesos están diseñados para proporcionar grandes cantidades de agua caliente o calefacción de espacios para edificios de uso no residencial.

Las piscinas de evaporación son piscinas de baja profundidad que concentran sólidos disueltos a través de la evaporación. El uso de piscinas de evaporación para obtener sal del agua salada es una de las aplicaciones más antiguas de la energía solar. Los usos modernos incluyen la concentración de soluciones de salmueras usadas en la minería por lixiviación y la remoción de sólidos disueltos de los flujos de desechos. En conjunto, las piscinas de evaporación representan una de las aplicaciones comerciales más grandes de la energía solar actualmente en uso.

Los colectores transpirados sin vidrios (en inglés: Unglazed Transpired Collectors, UTC) son muros perforados que enfrentan al sol usados para precalentar el aire de ventilación. Los UTC pueden aumentar la temperatura del aire hasta 22 °C y son capaces de entregar temperaturas de salida entre 45-60 °C. El corto período de amortización de los colectores transpirados (entre 3 a 12 años) los hacen una alternativa más costo-efectiva que los sistemas de recolección vidriados. Al año 2009, se han instalado mundialmente sobre 1500 sistemas con un área de colectores total de 300 000 m². Ejemplos típicos incluyen un colector de 860 m² en Costa Rica usado para secar granos de café y un colector de 1300 m² en Coimbatore, India usado para secar caléndulas.

Una instalación de procesamiento de comida ubicada en Modesto, California usa cilindros parabólicos para producir vapor uado en el proceso de fabricación. Se espera que el área de colectores de 5000 m² proporcione 15 TJ por año.

Las instalaciones de temperatura media pueden usar varios diseños, los diseños más comunes son: glicol a presión, drenaje trasero, sistemas de lote y sistemas más nuevos de baja presión tolerantes al congelamiento que usan tuberías de polímero que contienen agua con bombeo fotovoltaico. Los estándares europeos e internacionales están siendo revisados para incluir las innovaciones en diseño y la operación de colectores de temperatura media. Las innovaciones operacionales incluyen la operación de "colectores permanentemente húmedos". Esta técnica reduce o incluso elimina la ocurrencia de tensiones de no flujo de alta temperatura conocidas como estancamiento, las que reducen la vida esperada de estos colectores.

La energía térmica solar puede ser útil para el secado de madera para la construcción y de madera para combustible tales como chips de madera para la combustión. También es usada para secar alimentos tales como frutas, granos y pescados. El secado de cultivos por medio de la energía solar térmica es ambientalmente amigable así como económica mientras que mejora la calidad del resultado. Las tecnologías en secado solar son variadas. Los más simples utilizan una malla tendida al sol, mientras que los de tipo industrial utilizan colectores de aire vidriados que conducen el aire caliente a una cámara de secado. 
La energía térmica solar también es útil en el proceso de secado de productos tales como chips de madera y otras formas de biomasa elevando la temperatura mientras que permiten que el aire pase a través de ella y saquen la humedad.

Las cocinas solares usan la luz del sol para cocinar, secar y pasteurización. La cocina solar reduce el consumo de combustible, ya sea combustibles fósiles o leña, y mejora la calidad del aire reduciendo o removiendo la fuente de humo.

La forma más simple de cocina solar es la caja de cocción que fue construida por primera vez por Horace-Bénédict de Saussure en el año 1767. Una caja de cocción básica consiste de un contenedor aislado con una tapa transparente. Estas cocinas pueden ser usadas efectivamente con cielos parcialmente cubiertos y normalmente alcanzan temperaturas de entre 50-100 °C.

Las cocinas solares de concentración usan reflectores para concentrar la energía solar en un contenedor de cocción. Las geometrías de reflector más comunes son las placas planas, de disco y cilíndrico-parabólicas. Estos diseños cocinan más rápido y a temperaturas más altas (hasta los 350 °C) pero requieren de luz solar directa para funcionar en forma adecuada.

La Cocina Solar en Auroville, India usa una tecnología de concentración única conocida como el tazón solar. Al contrario de los sistemas de convencionales de receptores fijos o de reflectores de seguimiento, el tazón solar usa un reflector esférico fijo con un receptor que sigue el foco de luz a medida que el sol cruza el cielo. El receptor del tazón solar alcanza temperaturas de 150 °C que es usado para producir vapor que ayuda a la cocción de 2000 raciones diarias.

Muchas otras cocinas solares en India usan otra tecnología de concentración única conocida como el reflector Scheffler. Está tecnología fue desarrollada por primera vez por Wolfgang Scheffler en el año 1986. Un reflector Scheffler es un disco parabólico que usa un solo eje de seguimiento para perseguir el curso diario del sol. Estos reflectores tienen una superficie reflectante flexible que es capaz de cambiar su curvatura para ajustarse a las variaciones estacionales en el ángulo de incidencia de la luz solar. Los reflectores Scheffler tienen la ventaja de tener un punto focal fijo lo que mejora la facilidad de cocción y son capaces de alcanzar temperaturas de entre 450 a 650 °C. En el año 1999 en Abu Road, Rajasthan, India se construyó el sistema de reflectores Scheffler más grande del mundo, este es capaz de cocinar hasta 35 000 raciones diarias. A principios del año 2008 han sido fabricadas sobre 2000 grandes cocinas, que usan el diseño Scheffler, a nivel mundial.

Los destiladores solares pueden ser usado para procesar agua potable en áreas donde el agua limpia no es común. La energía solar calienta el agua en el contenedor, luego el agua se evapora y se condensa en el fondo de la cubierta de vidrio.

Las temperaturas inferiores a 95 grados celsius son suficientes para calefacción de espacios, en ese caso generalmente se usan colectores planos del tipo no concentradores. Debido a las relativamente altas pérdidas de calor a través del cristal, los colectores planos no logran alcanzar mucho más de 200 °C incluso cuando el fluido de transferencia está estancado. Tales temperaturas son demasiado bajas para ser usadas en la conversión eficiente en electricidad.

La eficiencia de los motores térmicos se incrementa con la temperatura de la fuente de calor. Para lograr esto en las plantas de energía termal, la radiación solar es concentrada por medio de espejos o lentes para lograr altas temperaturas mediante una tecnología llamada energía termosolar de concentración (en inglés: "Concentrated Solar Power", CSP). El efecto práctico de las mayores eficiencias es la reducción del tamaño de los colectores de la planta y del uso de terreno por unidad de energía generada, reduciendo el impacto ambiental de una central de potencia así como su costo.

A medida que la temperatura aumenta, diferentes formas de conversión se vuelven prácticas. Hasta 600 °C, las turbinas de vapor, la tecnología estándar, tienen una eficiencia de hasta 41 %, Por sobre los 600 °C, las turbinas de gas pueden ser más eficientes. Las temperaturas más altas son problemáticas y se necesitan diferentes materiales y técnicas. Una propuesta para temperaturas muy altas es usar sales de fluoruro líquidas operando a temperaturas de entre 700 °C a 800 °C, que utilizan sistemas de turbinas de etapas múltiples para lograr eficiencias termales de 50 % o más. Las temperaturas más altas de operación le permiten a la planta usar intercambiadores de calor secos de alta temperatura para su escape termal, reduciendo el uso de agua de la planta, siendo esto crítico para que las centrales ubicadas en desiertos sean prácticas. También las altas temperaturas hacen que el almacenamiento de calor sea más eficiente, ya que se almacenan más watts-horas por unidad de fluido.

Dado que una planta de energía termosolar de concentración (CSP) primero genera calor, puede almacenar dicho calor antes de convertirlo en electricidad. Con la actual tecnología, el almacenamiento de calor es mucho más barato que el almacenamiento de electricidad. De esta forma, una planta CSP puede producir electricidad durante el día y la noche. Si la ubicación de la planta CSP tiene una radiación solar predecible, entonces la planta se convierte en una central confiable de generación de energía. La confiabilidad puede ser mejorada aún más al instalar un sistema de respaldo que use un sistema de combustión interna. Este sistema de respaldo puede usar la mayor parte de las instalaciones de la planta CSP, lo que hace disminuir el costo del sistema de respaldo.

Superados los temas de confiabilidad, con desiertos desocupados, sin problemas de polución y sin costos asociados al uso de los combustible fósiles, los principales obstáculos para el despliegue a gran escala de las centrales CSP son los costos, la contaminación estética, el uso del suelo y factores similares para las líneas de transmisión eléctrica de alta tensión. Aunque solo se necesita un pequeño porcentaje de los desiertos para abastecer los requerimientos globales de electricidad, aún esto es un gran superficie cubierta con espejos o lentes que se necesitan para obtener una cantidad significativa de energía.

Los sistemas tipo canal parabólico usan reflectores parabólicos en una configuración de canal para enfocar la radiación solar directa sobre un tubo largo que corre a lo largo de su foco y que conduce al fluido de trabajo, el cual puede alcanzar temperaturas hasta de 500 °C.

La generación fototérmica de electricidad es actualmente una de las aplicaciones más extensas de la energía solar en el mundo. Existen más de 2,5 millones de m² de concentradores solares instalados en 9 plantas Solar Energy Generation System (SEGS) de la Compañía Luz de Israel, que representan 354 MW y más del 85 % de la electricidad producida con energía solar. La compañía Luz salió del mercado en 1991 a causa de la reducción que se dio paralelamente en los costos de los energéticos convencionales y en los subsidios a los energéticos renovables en los Estados Unidos. Sus plantas usan aceite sintético como medio de transferencia de calor en el campo de concentradores; como circuito primario, el calor recogido por el aceite se intercambia posteriormente con agua donde se lleva a cabo la generación de vapor, el cual a su vez se expande para completar un ciclo Rankine. Durante los periodos de baja insolación, o bien para nivelar la oferta, se asisten con gas natural.

Actualmente se ha introducido el ciclo combinado para mejorar la eficiencia termodinámica de estos sistemas y se estudia la posibilidad de generar directamente el vapor en el campo de concentradores. Con esto se espera lograr llevar los precios de generación a niveles competitivos con las plantas termoeléctricas convencionales.

Existen otros sistemas, no comerciales aún, como los de torre central que usan helióstatos (espejos altamente reflejantes) para enfocar la luz solar, con la ayuda de una computadora y un servomecanísmo, en un receptor central. Los sistemas parabólicos de plato usan estos reflectores para concentrar la luz del sol en un receptor montado arriba del plato, en su punto focal.

Durante el día y el año, el sol cambia su posición respecto a un punto en la superficie del planeta. Para los sistemas de baja temperatura el seguimiento del sol se puede evitar (o limitar a unas pocas posiciones por año) si se usa óptica no visual. Sin embargo, para temperaturas más altas, si los espejos o lentes no se mueven, el foco de estos cambia, provocando que los ángulos de aceptación sean poco eficientes, aunque se compensa en parte por el uso de ópticas no visuales. Por consiguiente es necesario implementar un sistema para seguir la posición del sol, la desventaja de esto es que incrementa el costo y la complejidad de la planta. Se han ideado diferentes diseñados para solucionar este problema y que se pueden distinguir en cómo ellos concentran la luz solar y siguen la posición del sol.

Las plantas de energía cilíndrico-parabólicos usan un espejo cilíndrico curvado para reflejar la radiación solar directa sobre un tubo de vidrio que contiene un fluido (también llamado receptor, absorbedor o colector) ubicado a lo largo del cilindro, posicionado en el punto focal de los reflectores. El cilindro es parabólico a lo largo de un eje y lineal en el eje ortogonal. El cambio durante el día de la posición del sol perpendicular al receptor, es seguido inclinando el cilindro de este a oeste de tal forma que la radiación directa permanece enfocada en el receptor. Sin embargo, los cambios estacionales en el ángulo de incidencia de la luz solar paralelo al cilindro no requieren ajustar los espejos, dado que simplemente la radiación solar es concentrada en otra parte del receptor, de esta forma el diseño no requiere hacer el seguimiento en un segundo eje.

El receptor puede estar encerrado en una cámara al vacío de vidrio. El vacío reduce significativamente la pérdida de calor por convección.

Un fluido, también llamado fluido de transferencia de calor, pasa a través del receptor y se calienta muy fuertemente. Los fluidos más comunes son aceite sintético, sal fundida y vapor presurizado. El fluido que contiene el calor es transportado a un motor térmico donde aproximadamente un tercio del calor es transformado en electricidad.

Andasol 1 en Guadix, España usa el diseño cilíndrico-parabólico, el cual consiste de largas filas paralelas de colectores solares modulares. Estos siguen al Sol desde el este al oeste rotando sobre su eje, los paneles reflectores de alta precisión concentran la radiación solar sobre una tubería absorbente localizada a lo largo del eje focal de la línea de colectores. Un medio de transferencia de calor, un aceite sintético, como en los motores de los automóviles, se hace circular a través de las tuberías de absorción a una temperatura de hasta 400 °C y genera vapor bajo presión para propulsar un generador de turbina de vapor en un bloque de energía convencional.

Los sistemas cilíndrico-parabólico a escala total consisten de muchos de tales cilindros dispuestos en paralelo sobre una gran área de terreno. Desde el año 1985 el SEGS (en inglés: Solar Energy Generating Systems, SEGS), un sistema termal solar que usa este diseño, ha estado funcionando a plena capacidad en California, Estados Unidos.

El Sistema Solar de Generación de Energía (en inglés: Solar Energy Generating System, SEGS) es un conjunto de nueve plantas con una capacidad total de 350 MW. Actualmente es el sistema solar operacional más grande (tanto del tipo termal o no). La planta Nevada Solar One tiene una capacidad de 64 MW. Están en construcción las plantas Andasol 1 y 2 en España, cada planta tiene una capacidad de 50 MW, sin embargo, estas plantas son de un diseño que tiene un sistema de almacenamiento de calor que requiere un terreno con colectores solares mayor en relación al tamaño del generador y turbina de vapor para almacenar el calor y enviarlo a las turbinas de vapor al mismo tiempo. El almacenamiento de calor permite una mejor utilización de las turbinas de vapor. Con una operación diurna y parcialmente nocturna la turbina de vapor de Andasol 1 con un capacidad de punta de 50 MW produce más energía que Nevada Solar One con una capacidad de punta de 64 MW, debido al sistema de almacenamiento de calor y un terreno de colectores más grande que posee la planta de Andasol 1.

Se había propuesto instalar 553 MW adicionales en el Mojave Solar Park, California pero este proyecto fue cancelado en el año 2011. También se ha propuesto una planta híbrida con almacenamiento de calor de 59 MW cerca de Barstow, California. Cerca de Kuraymat en Egipto, se generan aproximadamente 40 MW de vapor como aporte para una planta de gas. También se generan 25 MW de vapor como aporte para una planta de gas en Hassi R'mel, Argelia. El gobierno de India ha comenzado a desarrollar una iniciativa llamada Jawaharlal Nehru National Solar Mission (también conocida como la Misión Solar Nacional) para resolver el problema de abastecimiento de energía de India.

Las torres de energía (también conocidas como central solar de 'torre central' o centrales de 'helióstatos') captura y enfocan la energía termal del sol con miles de espejos que siguen al sol (llamados helioestatos) ubicados en un terreno adyacente a la torre. Un torre está ubicada en el centro del terreno ocupado por los helióstatos. Los helióstatos concentran la luz del sol en un receptor que está ubicado en la parte superior de la torre. En el receptor la radiación solar concentrada calienta una sal fundida a sobre 538 °C. Posteriormente la sal fundida se envía a un tanque de almacenamiento termal donde se acumula, con una eficiencia termal del 98 %, finalmente es bombeada hacia un generador de vapor. El vapor impulsa una turbina la que genera electricidad. Este proceso, que también es conocido como ciclo de Rankine, es similar al que usa una planta que usa combustibles fósiles (carbón, gas natural, petróleo, etc.), excepto que la fuente de energía en este caso es la radiación solar limpia.

La ventaja de este diseño en comparación al diseño cilíndrico-parabólico es que logra alcanzar temperaturas más altas. La energía termal a temperaturas más altas puede ser convertida en electricidad con mayor eficiencia y es más barato el almacenamiento para ser usada posteriormente. Adicionalmente, el terreno adyacente no necesita ser tan plano. En principio una torre de energía podría ser construida en la ladera de una colina. Los espejos pueden ser planos y las tuberías están concentradas en la torre. La desventaja es que cada espejo debe tener su propio control en dos ejes, mientras que en el diseño cilíndrico-parabólico el control de seguimiento de un eje puede ser compartido por un conjunto más grande de espejos.

La NREL realizó una comparación de la relación costo/desempeño entre los diseños de torre de energía y los cilíndricos-parabólicos, está estimó que para el año 2020 se podría producir electricidad por un costo de 5,47 centavos de dólar por kWh para los diseños de torre de energía y de un costo de 6,21 centavos de dólar por kWh para los diseños cilíndricos-parabólicos. El factor de planta para los torres de energía fue estimado en un 72,9 % y para los diseños cilíndricos-parabólicos fue de 56,2 %. Se espera que el desarrollo de componentes para helióstatos de centrales baratos, durables y fabricados en masa harían bajar estos costos.

En junio de 2008, eSolar, una compañía basada en Pasadena, California fundada por el CEO de Idealab Bill Gross con financiamiento provisto por Google, anunció un Acuerdo para Compra de Energía (en inglés: Power Purchase Agreement, PPA) con la empresa de servicios públicos Southern California Edison para producir 245 megavatios de energía. También, en febrero de 2009, eSolar anunció que había licenciado su tecnología a dos socios de desarrollo, la empresa NRG Energy Inc. basada en Princeton, Nueva Jersey y el grupo ACME basado en India. En el acuerdo con NRG, las compañías anunciaron planes construir en forma conjunta plantas solares térmicas concentradoras por 500 megavatios a través de todo Estados Unidos. La meta para el Grupo ACME fue cerca del doble de esta cifra; ACME planeaba comenzar a construir sus primeras plantas generadoras de energía eSolar en el año 2009 y dentro de los siguientes 10 años completar 1 Gigavatio.

El software propietario de seguimiento del sol de eSolar coordina el movimiento de 24 000 espejos de 1 metro cuadrado por cada torre usando sensores ópticos para ajustar y calibrar los espejos en tiempo real. Esto permite un usar un material reflectante de alta densidad que hace posible el desarrollo de plantas generadoras solares termales de concentración (en inglés: Concentrating Solar Thermal Power, CSP) con unidades de 46 megavatios en terrenos de aproximadamente (MW) π millas cuadradas, lo que resulta en una proporción de terreno a energía de 16 000 m² por 1 megavatio.

BrightSource Energy firmó una serie de Acuerdos de Compra de Energía con Pacific Gas and Electric Company en marzo de 2008 por hasta 900 MW de electricidad, el compromiso de energía solar más grande realizado por una empresa de servicios públicos. Actualmente BrightSource está desarrollando varias plantas de generación solar en el sur de California, planaádose que se inicie la construcción de la primera en el año 2009.

En junio de 2008 BrightSource Energy inauguró su Centro de Desarrollo de Energía Solar (en inglés: Solar Energy Development Center, SEDC) de 4-6 MW en el Desierto de Negev, Israel. El sitio, localizado en el Parque Industrial de Rotem, posee 1.600 helióstatos que siguen al sol y reflejan la radiación solar sobre una torre de 60 metros de alto. La energía concentrada luego es usada para calentar una caldera, localizada en la parte superior de la torre, a una temperatura de 550 grados celsius, generando vapor supercalentado.

Existe una torre funcionando en PS10 en España con una capacidad de 11 MW.

Una planta llamada Solar Tres de 15 MW con almacenamiento de calor está bajo construcción en España. En Sudáfrica, está planificada una planta solar de 100 MW equipada con entre 4000 y 5000 helióstatos, cada uno de un área de 140 m². Una planta localizada en Australia llamada Granja solar Cloncurry (que usa grafito purificado como almacenamiento de calor localizado directamente en la torre).

Marruecos está construyendo cinco plantas solares termales alrededor de Uarzazate. Las plantas producirán aproximadamente 2000 MW hacia el año 2012. Sobre diez mil hectáreas de terreno se usarán para todos las plantas.

El proyecto Solar Uno de 10 MW fue puesto fuera de comisión (posteriormente se desarrolló en el proyecto Solar Dos) y también la central solar Thémis de 2 MW.

Un sistema de disco Stirling usa un gran disco reflector parabólico (similar a la forma que tiene un disco de televisión satelital). Este enfoca toda la radiación solar que llega al disco sobre un solo punto en la parte superior del disco, donde un receptor captura el calor y lo transforma en algo que se pueda usar. Normalmente el disco está acoplado a un motor Stirling, lo que se conoce como un Sistema Disco-Stirling, pero algunas veces se utiliza un motor de vapor. Estos motores crean energía cinética rotacional que puede ser convertida en electricidad usando un generador eléctrico.

La ventaja de un sistema de disco es que puede alcanzar temperaturas muchas más altas debido a una concentración mayor de luz (de manera similar que en los diseños de torre). Las temperaturas más altas permiten una mejor conversión a electricidad y los sistemas de disco son muy eficientes en este aspecto. Sin embargo, también hay algunas desventajas. La conversión de calor a electricidad requiere partes que se mueven y eso resulta en mayores requerimientos de mantenimiento. En general, una aproximación centralizada de este proceso de conversión es mejor que uno descentralizado en el diseño de disco. Segundo, el motor, que es pesado, es parte de la estructura que se mueve, lo que requiere una estructura rígida y un sistema de seguimiento resistente. Adicionalmente, se usan espejos parabólicos en vez de espejos planos lo que significa que el seguimiento debe ser realizado en dos ejes.

En el año 2005 Southern California Edison anunció un acuerdo para comprar motores Stirling para energía solar a la empresa Stirling Energy Systems durante un período de veinte años y en cantidades suficientes (20 000 unidades) para generar 500 MW de electricidad. En enero de 2010, "Stirling Energy Systems" y Tessera Solar pusieron en funcionamiento la primera central solar de demostración de 1,5 MW ("Maricopa Solar") usando la tecnología Stirling en Peoria, Arizona. A comienzos del año 2011 la subsidiaria de desarrollo de Stirling Energy, Tessera Solar, vendió de sus proyectos grandes, el proyecto Imperial de 709 MW y el proyecto Calico de 850 MW a las empresas AES Solar y K. Road respectivamente, y en el otoño de 2011 "Stirling Energy Systems" se acogió al Capítulo 7 de bancarrota debido a la competencia de la tecnología fotovoltaica de bajo costo.

Una central solar con reflectores Fresnel lineales usa una serie de espejos largos, estrechos, de baja curvatura (o incluso planos) para enfocar la luz en uno o más receptores lineales localizados sobre los espejos. En la parte superior del receptor un pequeño espejo parabólico puede estar posicionado para apoyar el enfoque sobre el receptor. La idea de estos sistemas es ofrecer bajos costos totales al compartir un receptor entre varios espejos (cuando se le compara con los conceptos cilíndricos y de disco), mientras que usan la simple geometría de enfoque lineal con un eje de seguimiento. Esto es similar al diseño de cilindro (y diferente de los diseños de torre central y de discos con doble eje). El receptor es estacionario y por lo tanto no necesita de acoples de fluidos (como es el caso en los diseños de cilindro y de discos). También los espejos no necesitan sostener al receptor, así que son estructuralmente más simples. Cuando se usan estrategias de puntería adecuadas (espejos apuntados a diferentes receptores a diferentes horas del día), se puede permitir una densidad mayor de espejos en el terreno disponible.

También ha sido desarrollado un concepto con la idea de reflectores Fresnel con "enfoque puntual" llamado Multi-Tower Solar Array (MTSA), en castellano: Arreglo Solar de Torres Múltiples. pero aún no ha sido construido un prototipo. En este concepto los espejos de posiciones alternas apuntan a torres diferentes como sus blancos, logrando de esta forma minimizar el bloqueo entre espejos y permiten una agrupación más densa de estos. En la torre la radiación solar sería recibida por un divisor de haz curvado, construido de cuarzo revestido, este divisor separaría la porción verde y roja del espectro visible y la porción del infrarrojo cercano y las enviaría a un receptor fotovoltaico, ya que estas partes del espectro electromagnético son las más eficientes para ser usadas con la generación fotovoltaica de electricidad. El resto de las longitudes de onda serían enviadas al receptor termal y la turbina, proceso que utiliza la energía de la radiación y no a las longitudes de onda. Este concepto ganó un financiamiento por el Australian Research Council para construir un prototipo de una sola torre en Australia y que pueda generar aproximadamente unos 150 kW(e) y que usará una microturbina combinada y un receptor fotovoltaico.

Se han construido prototipos recientes de este tipo de sistemas en Australia (del tipo Reflector Fresnel lineal compacto) y por Solarmundo en Bélgica.

El proyecto de investigación y desarrollo de Solarmundo, con su central piloto en Lieja, fue cerrado después de probar el concepto de la tecnología Fresnel lineal en forma exitosa. Subsecuentemente, la empresa Solar Power Group GmbH, basada en Múnich, Alemania, fue fundado por algunos de los miembros del equipo Solarmundo. Un prototipo basado en espejos Fresnel con generación directa de vapor fue construido por SPG en conjunto con el Centro Aeroespacial Alemán (DLR).

Basado en el prototipo australiano se ha propuesta una central de 177 MW ubicada cerca de San Luis Obispo en California y que sería construida por la empresa Ausra., pero Ausra vendió este proyecto a First Solar, finalmente First Solar (un fabricante de celdas solares fotovoltaicas de película delgada) no construirá el proyecto Carrizo, esto resultó en la cancelación del contrato de Ausra para proporcionar 177 MW a P.G.& E. Las centrales de capacidad pequeña son un enorme desafío económico para los diseños cilíndrico-parabólico y de disco, pocas compañías construyen estos proyectos tan pequeños. SHP Europe, una antigua subsidiaria de Ausra, tiene planes para construir una central de ciclo combinado de 6,5 MW en Portugal. La compañía alemana SK Energy GmbH tiene planes para construir varias centrales pequeñas de 1 a 3 MW en el sur de Europa (especialmente en España) usando la tecnología de espejso Fresnel y de motor de vapor.

En mayo de 2008, la empresa alemana Solar Power Group GmbH y la empresa española Laer S.L. acordaron la ejecución conjunta de una central solar termal en el centro de España. Esta será la primera central solar termal en España basada en la tecnología de colectores Fresnel de la empresa Solar Power Group. El tamaño planificado de la central será de 10 MW con una unidad de respaldo basada en combustible fósil. El comienzo de la construcción está planificada para el año 2009. El proyecto está localizado en Gotarrendura, un pequeño pueblo pionero en el uso de energías renovables, aproximadamente a 100 km al noroeste de Madrid, España.

Desde marzo de 2009, la central solar de Puerto Errado 1 (PE 1) operada por la empresa alemana Novatec Solar está operando comercialmente en el sur de España. La central solar está basada en la tecnología de colectores lineales Fresnel y tiene una capacidad eléctrica de 1,4 MW. Adicionalmente a un bloque de potencial convencional, la central incluye una caldera solar con una superficie de espejos de alrededor de 18 000 m². El vapor es generado concentrando la irradiación solar directa sobre un receptor lineal que está ubicado a 7,4 metros sobre la superficie del terreno. Un tubo absorbedor está localizado en la línea de foco del campo de espejos, en este el agua es evaporada directamente en vapor saturado a una temperatura de 270 °C y a una presión de 55 bar por la energía solar concentrada. Desde septiembre del año 2011, debido a un nuevo diseño de receptor desarrollado por Novatec Solar, el vapor ahora puede ser generado a una temperatura de 500 °C.

La central solar de Puerto Errado 2 (PE 2) de 30 MW es una versión agrandada de la PE 1, esta también está basada en la tecnología de colectores Fresnel desarrollada por la empresa alemana Novatec Solar. Comprende una superficie de espejos de 302 000 m² y está en operación desde agosto de 2012. La central está localizada en la región de Murcia.

Otras tecnologías de seguimiento de un solo eje incluyen a las relativamente nueva de reflector lineal Fresnel (en inglés: Linear Fresnel Reflector, LFR) y de LFR-Compacto (en inglés: Compact-LFR, CLFR). La LFR difiere de la de cilindro parabólico en que el absorbedor se encuentra fijo en el espacio sobre el campo de espejos. También, el reflector está compuesto de muchos segmentos de fila bajos, que se enfican colectivamente sobre una larga torre receptora elevada que corre paralela al eje de rotación de los reflectores.

Este sistema ofrece una solución de bajo costo ya que la fila del absorbedor es compartida con varias filas de espejos. Sin embargo, una dificultad fundamental con la tecnología LFR es evitar el obscurecimiento de la radiación solar incidente y el bloqueo de la radiación solar reflejada por los reflectores adyacentes. El bloqueo y el obscurecimiento puede ser reducidos al usar torres más altas o incrementando el tamaño del absorbedor, lo que permite incrementar el espaciamiento entre los reflectores más alejados del absorbedor. Ambas soluciones tienen costos extras asociados, ya que se requiere una mayor superficie de terreno.

El CLFR ofrece una solución alternativa al problema del LFR. El LFR clásico tiene solo un absorbedor lineal instalado en una sola torre lineal. Esto impide cualquier opción en la dirección de la orientación de un reflector específico. Dado que esta tecnología sería introducida en un gran campo, uno puede asumir de que existirán mucho absorbedores lineales en el sistema. Por lo tanto, si los absorbedores están lo suficientemente cercanos, los reflectores individuales tendrán la opción de dirigir la radiación solar reflejada hacia al menos dos absorbedores. Este factor adicional permite el potencial para arreglos con una alta densidad, dado que los patrones de inclinaciones de reflectores alternadas pueden ser hechos de tal forma que los reflectores instalados con una alta densidad no se bloquean o ensombrecen mutuamente.

Las centrales solares CLFR ofrecen reducción de costos en todos los elementos del arreglo solar. Esta reducción de costos alentan el avance de esta tecnología. Las características que inciden en la reducción de costos de este sistema comparadas a las de la tecnología cilíndrica-parabólica incluyen costos estructurales minimizados, pérdidas por bombeo parásito minimizadas y mantenimiento reducido. La disminución de los costos estructurales se atribuyen a uso de reflectores de vidrio planos o curvados elásticamente en vez de costosos reflectores de vidrio hundido montados cerca del suelo. También, el ciclo de transferencia de calor está separado del campo de reflectores, evitando el costo de las tuberías flexibles de alta presión que se requieren para los sistemas cilíndricos. La disminución de las pérdidas de bombeo parásito se deben al uso de agua para el fluido de transferencia de calor con ebullición directa pasiva. El uso de tubos de vidrio evacuados asegura bajas pérdidas por radiación y son baratos. Estudios existentes para las centrales CLFR han mostrado una eficiencia entre el haz de radiación recibido y la electricidad generada de un 19 % en una base anual como un precalentamiento.

Se han construido prototipos de concentradores de lentes de Fresnel para la recuperación de energía termal por la empresa International Automated Systems. No se conocen de sistemas termales que usen lentes de Fresnel en operación a plena escala, aunque ya se encuentran disponibles algunos productos que incorporan lentes de Fresnel en conjunto con células fotovoltaicas.

La ventaja de este diseño es que los lentes son más baratos que los espejos. Adicionalmente, si se escoge un material flexible, entonces se requiere de una estructura de soporte de menor rigidez para resistir la carga generada por el viento. En el proyecto "Desert Blooms" se puede ver un nuevo concepto de tecnología para concentradores solares livianos y 'no disruptivos' que usa lentes de Fresnel asimétricos que ocupan un área de superficie de terreno mínima y que permite mayores cantidades de energía solar concentrada por cada concentrador, aunque todavía no se construye un prototipo.

El sistema solar termal cilíndrico parabólico cerrado encapsula los componentes al interior de un recinto de vidrio tipo invernadero. El recinto protege los componentes de los elementos que pueden impactar negativamente la confiabilidad y eficiencia del sistema. Espejos reflectores solares curvados livianos se encuentran suspendidos desde el techo del recinto de vidrio sostenidos por cables. Un sistema de seguimiento de un solo eje posiciona los espejos para recuperar la cantidad óptima de radiación solar. Los espejos concentran la radiación solar y la enfocan en una red de tuberías de acero estacionarias, también suspendidas de la estructura del recinto de vidrio. Se bombea agua a través de las tuberías y esta es hervida para generar vapor usando la radiación solar concentrada. A continuación el vapor es usado como calor de proceso. Al proteger los espejos del viento permite lograr temperaturas más altas y previene que se acumule polvo sobre estos como un resultado de ser expuestos a la humedad ambiente.

Los hornos solares son reflectores parabólicos o lentes construidas con precisión para enfocar la radiación solar en superficies pequeñas y de este modo poder calentar "blancos" a altos niveles de temperatura. La temperatura que puede obtenerse con un horno solar está determinada por el segundo principio de la termodinámica y es equivalente a la temperatura de la superficie del sol, esto es 6000 °C, y por la consideración de las propiedades ópticas de un sistema de horno que limitan la temperatura máxima disponible. Se han usado hornos solares para estudios experimentales que han alcanzado hasta 3500 °C y se han publicado temperaturas superiores a 4000 °C. Las muestras pueden calentarse en atmósferas controladas y en ausencia de campos eléctricos o de otro tipo si así se desea.

El reflector parabólico tiene la propiedad de concentrar en un punto focal los rayos que entran en el reflector paralelamente al eje. Como el sol abarca un ángulo de 32', aproximadamente, los haces de rayos no son paralelos y la imagen en el foco del receptor tiene una magnitud finita. Como regla empírica, el diámetro de la imagen es aproximadamente la razón de longitud focal dividido por 111. La longitud focal determina el tamaño de la imagen y la abertura del reflector la cantidad de energía que pasa por el área focal para una velocidad dada en incidencia de radiación directa. El cociente entre la abertura y la longitud focal es, pues, una medida de flujo de energía disponible en el área focal y con arreglo a este flujo se puede calcular una temperatura de cuerpo negro.

La utilidad de los hornos solares aumenta con el uso de helióstatos, o espejo plano móvil, para llevar la radiación solar al reflector parabólico. esto permite el montaje estacionario de una parábola de ordinario en posición vertical, con lo cual se pueden colocar aparatos para atmósfera controlada y movimiento de muestras, soportes de blancos, y otros, sin necesidad de mover todo el equipo. El poder de reflexión del helióstato varia de 85 a 95 % según su construcción, por lo que resulta una pérdida de flujo del 5 al 15 % para el horno, y la disminución correspondiente a las temperaturas que se puedan alcanzar.

Se construyen hornos solares de hasta 3 metros de diámetro con espejos de una sola pieza de aluminio, cobre o de otros elementos y se han construido hornos más grandes de múltiples reflectores curvos.

El reflector o blanco usado en los hornos solares puede ser de varias formas. Las sustancias pueden fundirse en sí mismas en cavidades de cuerpo negro, encerrarse en envoltura de vidrio o de otra materia transparente para atmósferas controladas, o introducirse en un recipiente rotatorio "centrífugo". La medición de las temperaturas del blanco en los hornos solares se hace por fusión de sustancias de punto de fusión conocidos y por medios pirométricos ópticos o de radiación.

Se usan hornos solares en gran variedad de estudios experimentales, entre ellos, la fusión de materiales refractarios, la realización de reacciones químicas e investigación de las relaciones de fase en sistemas de alto punto de fusión como sílice alúmina.

La estabilización del óxido de circonio refractario por adición de pequeñas cantidades de CaO en recipientes centrífugos es uno de los muchos trabajos publicados por Trombe, quien también ha eliminado flúor de mezcla de fosfatos por calentamiento en un horno en presencia de sílice y vapor de agua, según la reacción:

[Ca3(PO4)2]3.CaF2 + xSiO2 + H2O ® 3 Ca2(PO4)2 + (SiO2)x.CaO + 2HF

Se ha preparado, con buen rendimiento, óxido de circonio calentando silicato de circonio a 1400 °C con carbonato de sodio, Según la ecuación:

ZrSiO4 + 2Na2CO3 ® Na4SiO4 + 2CO2 + ZrO2

Entre otros usos propuestos para los hornos solares figuran los experimentos de pirólisis instantánea en investigación química inorgánica y orgánica, y estudios geoquímicos de rocas y minerales.

Existe más energía en las frecuencias más altas de la luz basados en la fórmula formula_1, donde "h" es la constante de Planck y formula_2 es la frecuencia. Los colectores metálicos disminuyen las frecuencias más altas de la luz produciendo una serie de cambios Compton en abundancia de frecuencias más bajas de la luz. Los revestimientos de vidrio y cerámica con alta transmisividad en el espectro visible y ultravioleta y con una trampa metálica con absorción efectiva en el espectro infrarrojo (bloqueo de calor) absorben la luz de baja frecuencia producida por la pérdida a través de radiación. La aislación de la convección previene las pérdidas mecánicas transferidas a través del gas. Una vez que recuperado como calor, la eficiencia del almacenamiento térmico aumenta con el tamaño. A diferencia de las tecnologías fotovoltaicas que a menudo se degradan con la luz concentrada, la tecnología solar termal depende de la concentración de la luz, la cual requiere de un cielo despejado para alcanzar las temperaturas necesarias para producir electricidad.

El calor en un sistema solar termal es controlado por cinco principios básicos: ganancia de calor, transferencia de calor, almacenamiento de calor, transporte de calor y aislación termal. En esta situación, el calor es la medida de la cantidad de energía termal que contiene un objeto y está determinada por la temperatura, masa y calor específico del objeto. Las centrales solares termales usan intercambiadores de calor que están diseñados para condiciones de trabajo constantes para proporcionar el intercambio de calor.

La ganancia de calor es el calor acumulado por el sol en el sistema. El calor solar termal es atrapado usando el efecto invernadero, este efecto en este caso es la habilidad de una superficie reflectante para transmitir la radiación de onda corta y reflejar la radiación de onda larga. El calor y la radiación infrarroja son producidas cuando la radiación de onda corta golpea la placa de absorción, que luego es atrapado al interior del colector. Un fluido, usualmente agua, en el absorbedor pasa por tubos y recoge el calor atrapado y lo transfiere a un depósito de almacenamiento de calor.

El calor es transferido ya sea por conducción o convección. Cuando el agua es calentada, la energía cinética es transferida por conducción a las moléculas de agua a través del medio. Estas moléculas dispersan si energía termal por conducción y ocupan más espacio que las moléculas frías que se mueven más lento sobre ellas. La distribución de la energía desde el agua caliente que se eleva hacia el agua fría que se hunde contribuyen al proceso de convección. El calor es transferido en el fluido desde las placas de absorción del colector por conducción. El fluido del colector es hecho circular a través de las tuberías transportadoras hasta el lugar del almacenamiento del calor. Al interior del almacenamiento, el calor es transferido a través del medio por convección.

El almacenamiento del calor permite que las centrales solares termales puedan producir electricidad durante las horas del día sin luz solar. El calor es transferido a un medio de almacenamiento de calor en un depósito aislado durante las horas con luz solar y es recuperado para la generación de electricidad durante las horas cuando no hay luz solar. La tasa de transferencia de calor está relacionada con la conductividad y convección del medio así como a las diferencias de temperatura. Los cuerpos con grandes diferencias de temperatura transfieren el calor más rápido que los cuerpos con diferencias de temperatura más baja.

El transporte del calor se refiere a la actividad en que el calor de un colector solar es transportado hacia el depósito de almacenamiento de calor. La aislación térmica es vital tanto en las tuberías de transporte de calor como en el depósito de almacenamiento de calor. Previene la pérdida de calor, que está relacionada con la pérdida de energía que a su vez afecta negativamente la eficiencia del sistema.

El almacenamiento de calor le permite a las centrales solares termales producir electricidad durante la noche y los días nublados. Esto permite el uso de la energía solar en la generación de carga base así como para la generación de potencia de punta, con el potencial de reemplazar a las centrales que usan combustibles fósiles. Adicionalmente, la utilización de los generadores es más alta lo que reduce los costos.

El calor es transferido a un medio de almacenamiento termal en un depósito aislado durante el día y es retirado para la generación de electricidad en la noche. Los medios de almacenamiento termal incluyen vapor presurizado, concreto, una variedad de materiales con cambio de fase, y sales fundidas tales como calcio, sodio y nitrato de potasio.

La central solar PS10 almacena el calor en tanques como vapor presurizado a 50 bar y a 285 °C. El vapor se condensa y se convierte instantáneamente nuevamente en vapor cuando la presión se baja. El almacenamiento se puede hacer hasta por una hora. Se ha sugerido que se puede almacenar por más tiempo pero aún no se ha probado en una central ya existente.

Se han probado una variedad de fluidos para transportar el calor del sol, incluyendo agua, aire, aceite y sodio, pero en algunos casos se han seleccionado sal fundida como la mejor opción. La sal fundida es usada en los sistemas de torres de energía solar ya que es líquida a presión atmosférica, proporcionando un medio de bajo costo para almacenar energía termal, sus temperaturas de operación son compatibles con la de las actuales turbinas de vapor, y es no inflamable y no tóxica. La sal fundida es usada en las industrias químicas y de metales para transportar calor, así que existe gran experiencia en su uso.

La primera mezcla comercial de sal fundida era una forma común de nitro, 60 por ciento de nitrato de sodio y 40 por ciento de nitrato de potasio. El nitro se funde a 220 °C y se mantiene líquido a 290 °C en un tanque de almacenamiento con aislante. El nitrato de calcio puede reducir el punto de fusión a 131 °C, permitiendo que se pueda extraer más energía antes de que la sal se congele. Ahora existen varios grados técnicos de nitrato de calcio que son estables a más de 500 °C.

Estos sistemas de energía solar pueden generar electricidad en climas nubosos o durante la noche usando el calor almacenado en los tanques de sal caliente. Los tanques se encuentran equipados con aislamiento y son capaces de almacenar el calor durante una semana. Los tanques que alimentan una turbina de 100 MW durante cuatro horas deberían tener un tamaño de 9 m de alto por 24 m de diámetro.

La central solar de Andasol ubicada en España es la primera central solar termal comercial en usar sal fundida para almacenar calor y generar electricidad durante la noche. Esta central entró en funcionamiento el marzo del año 2009. El 4 de julio de 2011, se realizó un hito en la historia de la industria solar la central solar de Gemasolar de 19,9 MW fue la primera en generar electricidad en forma ininterrumpida durante 24 horas seguidas usando un almacenamiento de calor de sal fundida.

Directo 
La propuesta central solar ubicada en Cloncurry, Australia almacenará calor en grafito purificado. La central usa un diseño de torre de energía. El grafito se encuentra localizado en la parte superior de la torre. El calor capturado por los helióstatos va directamente hacia el almacenaje. El calor usado para la generación de energía es recuperado desde el grafito. Esto simplifica el diseño.

Indirecto 
Refrigerantes de sal fundida son usado para llevar el calor desde los reflectores hacia el depósito de almacenamiento de calor. El calor llevado por las sales es transferido a un fluido de transferencia de calor secundario a través de un intercambiador de calor y luego al medio de almacenamiento, o en forma alternativa, las sales pueden ser usadas para calentar directamente el grafito. El grafito es usado ya que tiene costos relativamente bajos y es compatible con las sales líquidas del fluoruro. La alta masa y capacidad calórica volumétrica del grafito proporcionan un eficiente medio de almacenamiento.

Los materiales con cambio de fase (en inglés: Phase Change Material, PCM) ofrecen una solución alternativa en el almacenamiento de energía. Usando una infraestructura de transferencia de calor similar, los PCM tienen el potencial de proporcionar un medio más eficiente de almacenamiento. Los PCM pueden ser materiales orgánicos o inorgánicos. Las ventajas de los PCM orgánicos incluyen que son no corrosivos, con subenfriamiento bajo o ninguno, y estabilidad química o termal. Las desventajas incluyen una baja entalpía de cambio de fase, baja conductividad termal e inflamabilidad. Las ventajas de los PCM inorgánicos son una mayor entalpía de cambio de fase, pero exhiben desventajas en temas relacionados al subenfriamiento, corrosión, separación de fase y carencia de estabilidad termal. La mayor entalpía de cambio de fase en los PCM inorgánicos hacen que las sales hidratadas sean un fuerte candidato en el campo del almacenamiento de la energía solar.

Un diseño que requiere agua para condensación o enfriamiento puede ser un problema en las centrales solares termales localizadas en áreas desérticas con buena radiación solar pero con recursos hídricos limitados. El conflicto se ve claramente en los planes de la empresa alemana Solar Millennium para construir en el Amargosa Valley de Nevada los cuales requerían el 20 % del agua disponible en el área. Algunos otros proyectos por la misma y otras empresas en el Desierto de Mojave en California también pueden ser afectadas por la dificultad en la obtención de los derechos de agua adecuados o apropiados. Actualmente la Ley de Aguas de California prohíbe el uso de agua potable para la refrigeración.

Otros diseños de agua requieren menos agua. La propuesta central solar de Ivanpah en el sureste de California conservará la escasa agua disponible al usar refrigeración por aire para convertir el vapor en agua. Comparada a la refrigeración húmeda convencional, esto resulta en una reducción del 90 % en el uso de agua al costo de una pérdida menor de eficiencia en el proceso de refrigeración. Luego el agua es regresada a la caldera en un proceso cerrado que es ambientalmente amigable.

De todas estas tecnologías el disco solar/motor Stirling tiene la más alta eficiencia energética. Una sola instalación de disco solar-motor Stirling ubicada en el Centro Nacional de Pruebas Solar Termal (en inglés: National Solar Thermal Test Facility, NSTTF) en el Laboratorio Nacional Sandia produce tanto como 25 kW de electricidad, con una eficiencia de conversión del 31,25 %.

Se han construido centrales solares cilíndrico parabólicas con eficiencias aproximadas del 20 %. Los reflectores Fresnel tienen una eficiencia que es ligeramente más baja, pero esto es compensado por una distribución más densa.

Las eficiencias de conversión brutas (tomando en cuenta que los discos o cilindros solares ocupan solo una fracción del área total de una central) son determinados por la capacidad de generación neta sobre la energía solar que cae sobre el área total ocupada por la central solar. La central SCE/SES de 500 megavatios extraería aproximadamente el 2,75 % de la radiación (1 kW/m²; ver Energía Solar para una discusión más detallada) que incide en sus 18,2 km². Para la central solar de Andasol de 50 MW que está siendo construida en España, con un área total de 1300×1500 m = 1,95 km², tiene una eficiencia de conversión bruta de 2,6 %.

En todo caso la eficiencia no está relacionada al costo. Al calcular el costo total deberían considerarse tanto la eficiencia como el costo de construcción y de mantenimiento.

Dado que una central solar no usa ningún tipo de combustible, el costo consiste principalmente de los costos de capital con costos menores operacionales y de mantenimiento. Si se conoce la vida útil de la central y la tasa de interés, se puede calcular el costo por kWh. Esto se llama coste normalizado de la energía.

El primer paso en el cálculo es determinar la inversión en la producción de 1 kWh en un año. Por ejemplo, los datos para el proyecto de Andasol 1 indican que se invirtieron en total 310 millones de euros para producir 179 GWh en un año. Dado que 179 GWh son 179 millones de kWh, la inversión por kWh para un año de producción es de 310 / 179 = 1,73 euros. Otro ejemplo es el de la central solar de Cloncurry en Australia. Se tenía planificado que produjera 30 millones de kWh en un año con una inversión de 31 millones de dólares australianos. Si se logra en realidad, el costo sería de 1,03 dólares australianos para producir 1 kWh por año. Esto habría sido significativamente más barato que Andasol, lo que se podría explicar en parte por la radiación más alta recibida en Cloncurry en relación a España. La inversión por kWh por año no debería ser confundida con el costo por kWh durante todo el ciclo de vida de una central solar.

En la mayor parte de los casos la capacidad es indicada para una central en particular, por ejemplo: para Andasol 1 se indica una capacidad de 50 MW. Esta cifra no adecuada para realizar comparaciones, debido a que el factor de capacidad puede ser diferente. Si una central solar posee almacenamiento de calor, también puede producir electricidad después del ocaso, pero eso no cambiará el factor de capacidad; simplemente desplaza la generación. El factor de capacidad promedio para una central solar, que es una función del seguimiento, efecto del sombreado y de la localización, es de aproximadamente un 20 %, lo que significa que una central solar con un capacidad de 50 MW normalmente proporcionará una generación de electricidad anual de 50 MW x 24 horas x 365 días x 20 % = 87 600 MWh/año o 87,6 GWh/año.

Aunque la inversión para un kWh por año de producción es adecuada para comparar el precio de diferentes centrales solares, con esto aún no se obtiene el precio por kWh. La forma de financiamiento tiene una gran influencia en el precio final. Si la tecnología es probada, debería ser posible una tasa de interés del 7 %. Sin embargo, los inversores en nuevas tecnologías buscan una tasa mucho más alta para compensar por los riesgos más altos. Esto tiene un significativo efecto negativo en el precio por kWh. Independiente de la forma de financiamiento, siempre existe una relación lineal entre la inversión por kWh producido en un año y el precio de 1 kWh, antes de agregar los costos operacionales y de mantenimiento. En otras palabras, si por mejoras de la tecnología la inversión cae en un 20 %, el precio por kWh también cae en un 20 %.




</doc>
<doc id="9843" url="https://es.wikipedia.org/wiki?curid=9843" title="Conducción de calor">
Conducción de calor

La conducción de calor o transferencia de energía en forma de calor por conducción es un proceso de transmisión de calor basado en el contacto directo entre los cuerpos, sin intercambio de materia, porque el calor fluye desde un cuerpo de mayor temperatura a otro de menor temperatura que está en contacto con el primero. La propiedad física de los materiales que determina su capacidad para conducir el calor es la conductividad térmica. La propiedad inversa de la conductividad térmica es la resistividad térmica, que es la capacidad de los materiales para oponerse al paso del calor.

La transmisión de calor por conducción, entre dos cuerpos o entre diferentes partes de un cuerpo, es el intercambio de energía interna, que es una combinación de la energía cinética y energía potencial de sus partículas microscópicas: moléculas, átomos y electrones. La conductividad térmica de la materia depende de su estructura microscópica: en un fluido se debe principalmente a colisiones aleatorias de las moléculas; en un sólido depende del intercambio de electrones libres (principalmente en metales) o de los modos de vibración de sus partículas microscópicas (dominante en los materiales no metálicos).

Para el caso simplificado de flujo de calor estacionario en una sola dirección, el calor transmitido es proporcional al área perpendicular al flujo de calor, a la conductividad del material y a la diferencia de temperatura, y es inversamente proporcional al espesor:

donde:

El calor se transfiere por medio de alguno de los siguientes procesos:

La transferencia de energía térmica o calor entre dos 
diferentes por conducción o convección requiere el contacto directo de las moléculas de diferentes cuerpos, y se diferencian en que en la primera no hay movimiento macroscópico de materia mientras que en la segunda sí lo hay. Para la materia ordinaria la conducción y la convección son los mecanismos principales en la "materia fría", ya que la transferencia de energía térmica por radiación solo representa una parte minúscula de la energía transferida. La transferencia de energía por radiación aumenta con la cuarta potencia de la temperatura ("T"), siendo solo una parte importante a partir de temperaturas superiores a varios miles de kelvin.

Es la forma de transmitir el calor en cuerpos sólidos; se calienta un cuerpo, las moléculas que reciben directamente el calor aumentan su vibración y chocan con las que las rodean; estas a su vez hacen lo mismo con sus vecinas hasta que todas las moléculas del cuerpo se agitan. Por esta razón, si el extremo de una varilla metálica se calienta con una llama, transcurre cierto tiempo hasta que el calor llega al otro extremo. El calor no se transmite con la misma facilidad por todos los cuerpos. Existen los denominados "buenos conductores del calor", que son aquellos materiales que permiten el paso del calor a través de ellos. Los "malos conductores o aislantes" son los que oponen mucha resistencia al paso de calor.

Cabe destacar que cuando se produce transmisión de calor entre dos cuerpos, generalmente coexisten las tres formas de calor enunciadas, lo que ocurre es que alguna de ellas prevalece sobre las demás.

Un ejemplo práctico se produce al encender una lámpara eléctrica donde se puede comprobar que:

El portalámparas se calienta porque se produce una transmisión de calor por conducción.

El aire que rodea la lámpara se calienta y asciende por transmisión de calor por convección.

Al acercar la mano a la lámpara encendida, notamos la emisión de calor por radiación.

La conducción térmica está determinada por la ley de Fourier, que establece que el flujo de transferencia de calor por conducción en un medio isótropo es proporcional y de sentido contrario al gradiente de temperatura en esa dirección. De forma vectorial:

donde:

De forma integral, el calor que atraviesa una superficie "S" por unidad de tiempo viene dado por la expresión:

El caso más general de la ecuación de conducción, expresada en forma diferencial, refleja el balance entre el flujo neto de calor, el calor generado y el calor almacenado en el material 

donde:
La ecuación de conducción, que es un caso particular de la ecuación de Poisson, se obtiene por aplicación del principio de conservación de la energía.

La conductividad térmica es una propiedad intrínseca de los materiales que valora la capacidad de conducir el calor a través de ellos. El valor de la conductividad varía en función de la temperatura a la que se encuentra la sustancia, por lo que suelen hacerse las mediciones a 300 K con el objeto de poder comparar unos elementos con otros.

Es elevada en metales y en general en cuerpos continuos, y es baja en los gases (a pesar de que en ellos la transferencia puede hacerse a través de electrones libres) y en materiales iónicos y covalentes, siendo muy baja en algunos materiales especiales como la fibra de vidrio, que se denominan por eso aislantes térmicos. Para que exista conducción térmica hace falta una sustancia, de ahí que es nula en el vacío ideal, y muy baja en ambientes donde se ha practicado un vacío elevado.

En algunos procesos industriales se trabaja para incrementar la conducción de calor, bien utilizando materiales de alta conductividad o configuraciones con un elevado área de contacto. En otros, el efecto buscado es justo el contrario, y se desea minimizar el efecto de la conducción, para lo que se emplean materiales de baja conductividad térmica, vacíos intermedios, y se disponen en configuraciones con poca área de contacto.

El coeficiente de conductividad térmica (λ) expresa la cantidad o flujo de calor que pasa a través de la unidad de superficie de una muestra del material, de extensión infinita, caras planoparalelas y espesor unidad, cuando entre sus caras se establece una diferencia de temperatura igual a la unidad, en condiciones estacionarias.

En el sistema internacional la conductividad térmica se expresa en unidades de (). También puede expresarse en unidades de British thermal units por hora por pie por grado Fahrenheit (). Estas unidades pueden transformarse a W/(m·K) empleando el siguiente factor de conversión: 1 Btu/(h·ft·°F) = 1,731 W/(m·K).




</doc>
<doc id="9844" url="https://es.wikipedia.org/wiki?curid=9844" title="Rerradiación">
Rerradiación

La rerradiación es un fenómeno de radiación de un cuerpo que está sometido a su vez a radiación externa. Se denomina de esta forma para reservar el nombre de radiación para la radiación incidente sobre el cuerpo descrito, que llegará a un equilibrio térmico en el que habrá un balance neto entre la radiación recibida con la propia rerradiación emitida.


</doc>
<doc id="9845" url="https://es.wikipedia.org/wiki?curid=9845" title="Energía eléctrica">
Energía eléctrica

La energía eléctrica es la forma de energía que resulta de la existencia de una diferencia de potencial entre dos puntos, lo que permite establecer una corriente eléctrica entre ambos cuando se los pone en contacto por medio de un conductor eléctrico. La energía eléctrica puede transformarse en muchas otras formas de energía, tales como la energía lumínica o luz, la energía mecánica y la energía térmica.

Jeann Burelli (1975) afirmó que:
La energía eléctrica es el desplazamiento de los electrones libres entre los átomos de los materiales conductores, como el cobre, aluminio o hierro, debido a la excitación de una bobina, por medio de un campo magnético de polos variables, ese desplazamiento o flujo de electrones es lo que permite establecer una corriente eléctrica.

La energía eléctrica por años ha sido muy mal conceptualizada, la energía eléctrica está presente por si sola en los átomos de los materiales, lo que debemos hacer es encontrar el procedimiento idóneo para extraerla eficientemente.

Suele denominarse comúnmente como "energía" (en Colombia) o simplemente "luz" (en México y Venezuela).

La energía eléctrica se manifiesta como corriente eléctrica, es decir, como el movimiento de cargas eléctricas negativas, o electrones, a través de un cable conductor metálico como consecuencia de la diferencia de potencial que un generador esté aplicando en sus extremos.

Cada vez que se acciona un interruptor, se cierra un circuito eléctrico y se genera el movimiento de electrones a través del cable conductor. Las cargas que se desplazan forman parte de los átomos de la sustancia del cable, que suele ser metálica, ya que los metales —al disponer de mayor cantidad de electrones libres que otras sustancias— son los mejores conductores de la electricidad. La mayor parte de la energía eléctrica que se consume en la vida diaria proviene de la red eléctrica a través de las tomas llamadas enchufes, a través de los que llega la energía suministrada por las compañías eléctricas a los distintos aparatos eléctricos —lavadora, radio, televisor, etc.— que se desea utilizar, mediante las correspondientes transformaciones; por ejemplo, cuando la energía eléctrica llega a una enceradora, se convierte en energía mecánica, calórica y en algunos casos lumínica, gracias al motor eléctrico y a las distintas piezas mecánicas de los aparatos.

La energía eléctrica existe libre en la naturaleza de manera aprovechable. El ejemplo más relevante y habitual de esta manifestación son las tormentas eléctricas. La electricidad tampoco tiene una utilidad biológica directa para el ser humano, salvo en aplicaciones muy singulares, como pudiera ser el uso de corrientes en medicina (terapia electroconvulsiva), resultando en cambio normalmente desagradable e incluso peligrosa, según las circunstancias. Sin embargo es una de las más utilizadas, una vez aplicada a procesos y aparatos de la más diversa naturaleza, debido fundamentalmente a su limpieza y a la facilidad con la que se la genera, transporta y convierte en otras formas de energía. Para contrarrestar todas estas virtudes hay que reseñar la dificultad que presenta su almacenamiento directo en los aparatos llamados acumuladores. 

La generación de energía eléctrica se lleva a cabo mediante técnicas muy diferentes. Las que suministran las mayores cantidades y potencias de electricidad aprovechan un movimiento rotatorio para generar corriente continua en una dinamo o corriente alterna en un alternador. El movimiento rotatorio resulta a su vez de una fuente de energía mecánica directa, como puede ser la corriente de un salto de agua o la producida por el viento, o de un ciclo termodinámico. En este último caso se calienta un fluido, al que se hace recorrer un circuito en el que mueve un motor o una turbina. El calor de este proceso se obtiene mediante la quema de combustibles fósiles, reacciones nucleares y otros procesos.

La generación de energía eléctrica es una actividad humana básica, ya que está directamente relacionada con los requerimientos actuales del hombre. Todas las formas de utilización de las fuentes de energía, tanto las habituales como las denominadas alternativas o no convencionales, agreden en mayor o menor medida el ambiente, siendo de todos modos la energía eléctrica una de las que causan menor impacto.

La generación puede ir relacionada con la distribución, salvo en el caso del autoconsumo.

Actualmente la energía eléctrica se puede obtener de distintos medios, que se dividen principalmente en:


Un corte de energía se define como una condición de tensión cero en la alimentación eléctrica que dura más de dos ciclos (40 ms). Puede ser causado por el encendido de un interruptor, un problema en la instalación del usuario, un fallo en la distribución eléctrica o un fallo de la red comercial. Esta condición puede llevar a la pérdida parcial o total de datos, corrupción de archivos y daño del hardware.

Durante la historia de la humanidad ha habido varios apagones eléctricos en el mundo, por varias causas, ya sean fallas humanas, por desperfectos en los equipos electrónicos, por sobrecarga, por corto circuito o por inclemencias del tiempo, pero también se han realizado algunos apagones intencionales, en el año 2007 y 2009, en protesta al cambio climático.
Uno de los apagones más recordados de la historia fue el de Nueva York, el 9 de noviembre de 1965, además de haber paralizado a la metrópolis por 24 horas, es también muy recordado porque después de cumplirse nueve meses del apagón, hubo una cantidad de nacimientos más alta de lo normal. El más reciente ocurrió en Chile, que afectó a casi todo el país, poco después de los terremotos que azotaron a ese país.

El ruido eléctrico de línea se define como la Interferencia de Radio Frecuencia (RFI) e Interferencia Electromagnética (EMI) y causa efectos indeseables en los circuitos electrónicos de los sistemas informáticos. 

Las fuentes del problema incluyen motores eléctricos, relés, dispositivos de control de motores, transmisiones de radiodifusión, radiación de microondas y tormentas eléctricas distantes. 

RFI, EMI y otros problemas de frecuencia pueden causar errores o pérdida de datos almacenados, interferencia en las comunicaciones, bloqueos del teclado y del sistema. 

Los picos de alta tensión ocurren cuando hay repentinos incrementos de tensión en pocos microsegundos. Estos picos normalmente son el resultado de la caída cercana de un rayo, pero pueden existir otras causas también. Los efectos en sistemas electrónicos vulnerables pueden incluir desde pérdidas de datos hasta deterioro de fuentes de alimentación y tarjetas de circuito de los equipos. Son frecuentes los equipos averiados por esta causa.




SI estos fallos son repetidos pueden ocasionar perdidas dinerarias muy altas en empresas e industrias. Un elemento que puede ayudar a suplir este tipo de fallos eléctricos y ofrecer un suministro eléctrico seguro es con la instalación de un Sistema de alimentación Ininterrumpida.

Los aparatos eléctricos cuando están funcionando generan un consumo de energía eléctrica en función de la potencia que tengan y del tiempo que estén en funcionamiento. En España, el consumo de energía eléctrica se contabiliza mediante un dispositivo precintado que se instala en los accesos a la vivienda, denominado contador, y que cada dos meses revisa un empleado de la compañía suministradora de la electricidad anotando el consumo realizado en ese período. El kilovatio hora (kWh) es la unidad de energía en la que se factura normalmente el consumo doméstico o industrial de electricidad. Equivale a la energía consumida por un aparato eléctrico cuya potencia fuese un kilovatio (kW) y estuviese funcionando durante una hora.

Dado el elevado coste de la energía eléctrica y las dificultades que existen para cubrir la demanda mundial de electricidad y el efecto nocivo para el medio ambiente que supone la producción masiva de electricidad se impone la necesidad de aplicar la máxima eficiencia energética posible en todos los usos que se haga de la energía eléctrica.
La eficiencia energética es la relación entre la cantidad de energía consumida de los productos y los beneficios finales obtenidos. Se puede lograr aumentarla mediante la implementación de diversas medidas e inversiones a nivel tecnológico, de gestión y de hábitos culturales en la comunidad.

Se denomina riesgo eléctrico al riesgo originado por la energía eléctrica. Dentro de este tipo de riesgo se incluyen los siguientes:<ref name="RD614/2001">Ministerio de la Presidencia, Real Decreto 614/2001, de 8 de junio, sobre disposiciones mínimas para la protección de la salud y seguridad de los trabajadores frente al riesgo eléctrico , BOE n.º 148 de 21-6-2001, España [20-1-2008]</ref>

La corriente eléctrica puede causar efectos inmediatos como quemaduras, calambres o fibrilación, y efectos tardíos como trastornos mentales. Además puede causar efectos indirectos como caídas, golpes o cortes.

Los principales factores que influyen en el riesgo eléctrico son:

Los accidentes causados por la electricidad pueden ser leves, graves e incluso mortales. En caso de muerte del accidentado, recibe el nombre de electrocución.

En el mundo laboral los empleadores deberán adoptar las medidas necesarias para que de la utilización o presencia de la energía eléctrica en los lugares de trabajo no se deriven riesgos para la salud y seguridad de los trabajadores o, si ello no fuera posible, para que tales riesgos se reduzcan al máximo.




</doc>
<doc id="9846" url="https://es.wikipedia.org/wiki?curid=9846" title="Ciclo termodinámico">
Ciclo termodinámico

Se denomina ciclo termodinámico a cualquier serie de procesos termodinámicos tales que, al transcurso de todos ellos, el sistema regresa a su estado inicial; es decir, que la variación de las magnitudes termodinámicas propias del sistema se anula. 

No obstante, a las variables como el calor o el trabajo no es aplicable lo anteriormente dicho ya que éstas no son funciones de estado del sistema, sino transferencias de energía entre este y su entorno. Un hecho característico de los ciclos termodinámicos es que la primera ley de la termodinámica dicta que: la suma de calor y trabajo recibidos por el sistema debe ser igual a la suma de calor y trabajo realizados por el sistema.

Representado en un diagrama P-V (presión / volumen específico), un ciclo termodinámico adopta la forma de una curva cerrada. En este diagrama el volumen de un sistema es representado en abscisas y la presión en ordenadas de forma que como
se tiene que el trabajo por cambio de volumen (o en general, si no se usa una rueda de paletas o procedimiento similar) es igual al área descrita entre la línea que representa el proceso y el eje de abscisas.

El sentido de avance, indicado por las puntas de flecha, nos indica si el incremento de volumen es positivo (hacia la derecha) o negativo (hacia la izquierda) y, como consecuencia, si el trabajo es positivo o negativo, respectivamente. 

Por lo tanto, se puede concluir que el área encerrada por la curva que representa un ciclo termodinámico en este diagrama, indica el trabajo total "realizado" (en un ciclo completo) por el sistema, si este avanza en sentido horario o, por el contrario, el trabajo total "ejercido" sobre el sistema si lo hace en sentido antihorario.

La obtención de trabajo a partir de dos fuentes térmicas a distinta temperatura se emplea para producir movimiento, por ejemplo en los motores o en los alternadores empleados en la generación de energía eléctrica. El rendimiento es el principal parámetro que caracteriza a un ciclo termodinámico, y se define como el trabajo obtenido dividido por el calor gastado en el proceso, en un mismo tiempo de ciclo completo si el proceso es continuo.

Este parámetro es diferente según los múltiples tipos de ciclos termodinámicos que existen, pero está limitado por el factor o rendimiento del Ciclo de Carnot.

Un ciclo termodinámico inverso busca lo contrario al ciclo termodinámico de obtención de trabajo. Se aporta trabajo externo al ciclo para conseguir que la trasferencia de calor se produzca de la fuente más fría a la más caliente, al revés de como sucedería naturalmente. Esta disposición se emplea en las máquinas de aire acondicionado y en refrigeración.



</doc>
<doc id="9847" url="https://es.wikipedia.org/wiki?curid=9847" title="Viabilidad técnica">
Viabilidad técnica

Condición que hace posible el funcionamiento del sistema, proyecto o idea al que se refiere, atendiendo a sus características tecnológicas y a las leyes de la naturaleza involucradas.

Es la condición que hace posible el funcionamiento de nuestras ideas o proyectos, atendiendo a sus características tecnológicas y toda su relación con el exterior en la complementación del producto, se evalúa ante un determinado requerimiento o di para determinar si es posible llevarlo a cabo en condiciones de seguridad con la tecnología disponible, verificando factores diversos como resistencia estructural, durabilidad u operacional, implicaciones energéticas, según el campo del que de trate.


</doc>
<doc id="9848" url="https://es.wikipedia.org/wiki?curid=9848" title="Viabilidad económica">
Viabilidad económica

La viabilidad económica de un proyecto es determinada por la diferencia entre el costo y beneficio del mismo.
El de la viabilidad económica pretende determinar la racionalidad de las transferencias desde este punto de vista. Para ello es necesario definir el coste de la solución óptima, entendiendo por tal la que minimiza el coste de satisfacción de todas las demandas a partir de las fuentes identificadas en los análisis anteriores, comprobar que ese coste es compatible con la racionalidad económica de la solución mediante el correspondiente análisis coste-beneficio y, por último, verificar que las demandas a satisfacer presentan capacidad de pago suficiente para afrontar el coste unitario resultante.

En muchas ocasiones, los recursos de los que se dispone para evaluar la viabilidad económica vienen determinados por los que produce el propio sistema, proyecto o idea que se está evaluando, por lo que en realidad se lleva a cabo un análisis de rendimiento o rentabilidad interna. Para ello se enfrenta lo que se produce con lo que se gasta, en términos económicos. Para que este nuevo proyecto, sistema o idea goce de plena viabilidad, debe cumplir con los requisitos establecidos al momento de hacer el estudio y complementarlo con la necesidad a ser cumplida o llevada a cabo. Debe cumplir con los objetivos que se establecen, que sea coste eficiente y debe sobrepasar en calidad, cantidad y otros aspectos relacionados con sistemas actuales.


</doc>
<doc id="9849" url="https://es.wikipedia.org/wiki?curid=9849" title="Central nuclear">
Central nuclear

Una central térmica nuclear o planta nuclear es una instalación industrial empleada para la generación de energía eléctrica a partir de energía nuclear. Se caracteriza por el empleo de combustible nuclear fisionable que mediante reacciones nucleares proporciona calor que a su vez es empleado, a través de un ciclo termodinámico convencional, para producir el movimiento de alternadores que transforman el trabajo mecánico en energía eléctrica. Estas centrales constan de uno o más reactores.

El núcleo de un reactor nuclear consta de un contenedor o vasija en cuyo interior se albergan bloques de un material aislante de la radiactividad, comúnmente se trata de grafito o de hormigón relleno de combustible nuclear formado por material fisible (uranio-235 o plutonio-239). En el proceso se establece una reacción sostenida y moderada gracias al empleo de elementos auxiliares que absorben el exceso de neutrones liberados manteniendo bajo control la reacción en cadena del material radiactivo; a estos otros elementos se les denominan moderadores.

Rodeando al núcleo de un reactor nuclear está el reflector cuya función consiste en devolver al núcleo parte de los neutrones que se fugan de la reacción. 

Las barras de control que se sumergen facultativamente en el reactor, sirven para moderar o acelerar el factor de multiplicación del proceso de reacción en cadena del circuito nuclear.

El blindaje especial que rodea al reactor, absorbe la radiactividad emitida en forma de neutrones, radiación gamma, partículas alfa y partículas beta.

Un circuito de refrigeración externo ayuda a extraer el exceso de calor generado. 

Las instalaciones nucleares son construcciones complejas por la escasez de tecnologías industriales empleadas y por la elevada sabiduría con la que se les dota. Las características de la reacción nuclear hacen que pueda resultar peligrosa si se pierde su control.

La energía nuclear se caracteriza por producir, además de una gran cantidad de energía eléctrica, residuos nucleares que hay que albergar en depósitos especializados. Por otra parte, no produce contaminación atmosférica de gases derivados de la combustión que producen el efecto invernadero, ya que no precisan del empleo de combustibles fósiles para su operación.

Las centrales nucleares constan principalmente de cuatro partes:


El reactor nuclear es el encargado de realizar la fisión de los átomos del combustible nuclear, como uranio, generando como residuo el plutonio, liberando una gran cantidad de energía calorífica por unidad de masa de combustible.

El generador de vapor es un intercambiador de calor que transmite calor del circuito primario, por el que circula el agua que se calienta en el reactor, al circuito secundario, transformando el agua en vapor de agua que posteriormente se expande en las turbinas de vapor, produciendo el movimiento de éstas que a la vez hacen girar los generadores eléctricos, produciendo la energía eléctrica. Mediante un transformador se aumenta la tensión eléctrica a la de la red de transporte de energía eléctrica.

Después de la expansión en la turbina el vapor es condensado en el condensador, donde cede calor al agua fría refrigerante, que en las centrales PWR procede de las torres de refrigeración. Una vez condensado, vuelve al reactor nuclear para empezar el proceso de nuevo.

Las centrales nucleares siempre están cercanas a un suministro de agua fría, como un río, un lago o el mar, para el circuito de refrigeración, ya sea utilizando torres de refrigeración o no.

El sistema de refrigeración se encarga de que se enfríe el reactor. Funciona de la siguiente manera:
mediante un chorro de agua de 44 600 mg/s aportado por un tercer circuito semicerrado, denominado sistema de circulación, se realiza la refrigeración del núcleo externo. Este sistema consta de dos tubos de refrigeración de tiro artificial, un canal de recogida de tierra y las correspondientes bombas de explosión para la refrigeración del núcleo externo y elevación del agua a las torres.

Como cualquier actividad humana, una central nuclear de fisión conlleva riesgos y beneficios. Los riesgos deben preverse y analizarse para poder ser mitigados. A todos aquellos sistemas diseñados para eliminar o al menos minimizar esos riesgos se les llama sistemas de protección y control. En una central nuclear de uso civil se utiliza una aproximación llamada "defensa en profundidad". Esta aproximación sigue un diseño de múltiples barreras para alcanzar ese propósito. Una primera aproximación a las distintas barreras utilizadas (cada una de ellas múltiple), de fuera a dentro podría ser:


Además debe estar previsto qué hacer en caso de que todos o varios de esos niveles fallaran por cualquier circunstancia. Todos los trabajadores, u otras personas que vivan en las cercanías, deben poseer la información y formación necesaria. Deben existir planes de emergencia que estén plenamente operativos. Para ello es necesario que sean periódicamente probados mediante simulacros. Cada central nuclear posee dos planes de emergencia: uno interior y uno exterior, comprendiendo el plan de emergencia exterior, entre otras medidas, planes de evacuación de la población cercana por si todo lo demás fallara.

Aunque los niveles de seguridad de los reactores de tercera generación han aumentado considerablemente con respecto a las generaciones anteriores, no es esperable que varíe la estrategia de defensa en profundidad. Por su parte, los diseños de los futuros reactores de cuarta generación se están centrando en que todas las barreras de seguridad sean infalibles, basándose tanto como sea posible en sistemas pasivos y minimizando los activos. Del mismo modo, probablemente la estrategia seguida será la de defensa en profundidad.

Cuando una parte de cualquiera de esos niveles, compuestos a su vez por múltiples sistemas y barreras, falla (por defecto de fabricación, desgaste o cualquier otro motivo), se produce un aviso a los controladores que a su vez se lo comunican a los inspectores residentes en la central nuclear. Si los inspectores consideran que el fallo puede comprometer el nivel de seguridad en cuestión elevan el aviso al organismo regulador (en España el Consejo de Seguridad Nuclear (CSN). A estos avisos se les denomina "sucesos notificables". En algunos casos, cuando el fallo puede hacer que algún parámetro de funcionamiento de la central supere las Especificaciones Técnicas de Funcionamiento (ETF) definidas en el diseño de la central (con unos márgenes de seguridad), se produce un paro automático de la reacción en cadena llamado SCRAM. En otros casos la reparación de esa parte en cuestión (una válvula, un aspersor, una compuerta...) puede llevarse a cabo sin detener el funcionamiento de la central.

Si cualquiera de las barreras falla aumenta la probabilidad de que suceda un accidente. Si varias barreras fallan en cualquiera de los niveles, puede finalmente producirse la ruptura de ese nivel. Si varios de los niveles fallan puede producirse un accidente, que puede alcanzar diferentes grados de gravedad. Esos grados de gravedad se organizaron en la Escala Internacional de Accidentes Nucleares (INES) por el Organismo Internacional de Energía Atómica (OIEA) y la Agencia para la Energía Nuclear (AEN), iniciándose la escala en el 0 (sin significación para la seguridad) y acabando en el 7 (accidente grave). El incidente (denominados así cuando se encuentran en grado 3 o inferiores) Vandellós I en 1989, catalogado a posteriori (no existía ese año la escala en España) como de grado 3 (incidente importante).

La ruptura de varias de estas barreras (no existía independencia con el gobierno, el diseño del reactor era de reactividad positiva, la planta no poseía edificio de contención, no existían planes de emergencia, etc.) causó uno de los accidentes más graves: el accidente de Chernóbil, de nivel 7 en la INES. Por razones similares (el muro de contención de sólo 8 metros de altura a pesar de saberse de la presencia de tsunamis de tamaño mayor a 38 metros y la localización de varios sistemas críticos en lugares fácilmente inundables) ocurrió otro accidente grave de magnitud 7: El accidente nuclear de Fukushima I.

Existen muchos tipos de centrales nucleares cada una con sus propias ventajas e inconvenientes. En primer lugar hay centrales basadas en fisión nuclear y en fusión nuclear, aunque éstas se encuentran actualmente en fase experimental y son solo de muy baja potencia.

Las centrales de fisión se dividen en dos grandes grupos: por un lado los reactores térmicos y por otro los rápidos. La diferencia principal entre estos dos tipos de reactores es que los primeros presentan moderador nuclear y los últimos no. Los reactores térmicos (los más utilizados en la actualidad) necesitan para su correcto funcionamiento que los neutrones emitidos en la fisión, de muy alta energía sean frenados por una sustancia a la que se llama moderador, cuya función es precisamente esa. Los reactores rápidos (de muy alta importancia en la generación III+ y IV) sin embargo no precisan de este material ya que trabajan directamente con los neutrones de elevada energía sin una previa moderación.

Los reactores térmicos se clasifican según el tipo de moderador que utilizan, así tenemos:


Por otra parte tenemos los reactores rápidos, todos ellos avanzados, conocidos como FBR ():

Centrales nucleares en España:







Proyectos abandonados. Moratoria nuclear:


Centrales desmanteladas, en proceso de desmantelamiento o paradas definitivamente por expiración de licencia:

Centros Atómicos:


Centros Atómicos:



Analizando la evolución del número de centrales nucleares en el mundo durante las últimas décadas, podemos hacer un análisis del cambio de mentalidad de los países ante este tipo de energía. Incluso, se puede decir que a través del número de centrales nucleares podemos leer los acontecimientos que han marcado estos últimos 60 años.



Hoy día hay 444 centrales nucleares en el mundo que suponen el 17 % de la producción eléctrica mundial. El país que más tiene en la actualidad es EE.UU. con 104, pero más sorprendente son las 58 centrales de Francia, más de la mitad que EE.UU. con casi 15 veces menos superficie. Aunque Japón no se queda nada lejos con 54 (aunque actualmente no están en funcionamiento por el cese decretado por el gobierno como consecuencia del accidente de Fukushima), o Corea del Sur con 21 en menos de  km². Actualmente España cuenta con 7 reactores nucleares. El accidente en la central de Fukushima ha recordado fantasmas del pasado, otorgándole al debate nuclear una candente actualidad.




</doc>
<doc id="9850" url="https://es.wikipedia.org/wiki?curid=9850" title="Foro de Cooperación Económica Asia-Pacífico">
Foro de Cooperación Económica Asia-Pacífico

APEC ("Asia-Pacific Economic Cooperation", en español "Foro de Cooperación Económica Asia-Pacífico") es un foro multilateral creado en 1989, con el fin de consolidar el crecimiento y la prosperidad de los países alrededor del Pacífico, que trata temas relacionados con el intercambio comercial, coordinación económica y cooperación entre sus integrantes.

Como mecanismo de cooperación y concertación económica, está orientado a la promoción y facilitación del comercio, las inversiones, la cooperación económica y técnica y al desarrollo económico regional de los países y territorios de la cuenca del océano Pacífico. Fomentando un crecimiento económico inclusivo, equitativo, sustentable e innovador.

La suma del Producto Nacional Bruto de las veintiuna economías que conforman el APEC equivale al 56 % de la producción mundial, en tanto que en su conjunto representan el 46 % del comercio global.

La APEC no tiene un tratado formal. Sus decisiones se toman por consenso y funciona con base en declaraciones no vinculantes. Tiene una Secretaría General, con sede en Singapur, que es la encargada de la asociación supranacional de la diabetes
el apoyo técnico y de consultoría. Cada año uno de los países miembros es huésped de la reunión anual de la APEC. La trigésima cumbre se realizó en noviembre de 2018 en Port Moresby, Papúa Nueva Guinea.

En noviembre de 1989, en búsqueda de una cooperación económica más eficaz en toda la región de la Cuenca del Pacífico, se concluyó en la primera reunión de la APEC , presidida por el Ministerio de Asuntos Exteriores de Australia, Gareth Evans. Con la asistencia de los ministros políticos de doce países, la reunión concluyó con compromisos de futuras reuniones anuales 

La propuesta inicial fue rechazada que en su lugar propuso el Cónclave Económico del Este de Asia en el que se excluiría a los países no asiáticos, como los Estados Unidos, Canadá, Australia y Nueva Zelanda. El plan fue fuertemente criticado por el Japón y los Estados Unidos.

La primera reunión de líderes de APEC se produjo en 1993, cuando el presidente de los Estados Unidos, Bill Clinton, después de conversaciones con el primer ministro australiano Paul Keating, invitó a los jefes de gobierno de las economías miembro a una cumbre en Blake Island. Él creía que esto ayudaría a poner nuevamente en marcha la estancada Ronda de Uruguay de negociaciones comerciales. 

En la reunión, pidieron la reducción continua de las barreras al comercio y la inversión en los países miembros, previendo una comunidad en la región Asia-Pacífico en el futuro, que podría promover la prosperidad mediante la cooperación económica, política y comercial. El Secretariado de APEC, con sede en Singapur, se creó para coordinar las actividades Bogor, Indonesia, los Líderes de APEC adoptaron los «Objetivos de Bogor», que apuntan para el comercio libre y abierto y la inversión en la región Asia-Pacífico en 2010 para las economías industrializadas y en 2020 para las economías en desarrollo. En 1995, la APEC, creó un órgano de asesoramiento empresarial nombrado el Consejo Asesor Empresarial de APEC (ABAC), integrado por tres ejecutivos de empresas de cada economía miembro.
Los países miembros de APEC organizan reuniones de presidentes y empresarios, para promover el comercio y la integración económica entre los países miembros, donde se invita a empresarios reconocidos en todo el mundo como Bill Gates para dar charlas en los eventos que se organizan, ruedas de negocios y presentaciones a la prensa. La Secretaría permanente está en Singapur, APEC reúne a las economías más importantes y dinámicas de los países de la Cuenca del Pacífico, es una plataforma para impulsar acuerdos de relaciones económicas internacionales.

India ha solicitado ser miembro de la APEC y recibió el apoyo inicial de los Estados Unidos, Japón y Australia. Las autoridades han decidido no permitir a la India adherirse, por diversas razones, entre ellas se cuenta el desequilibrio geopolítico que podría generar en el foro. Sin embargo, la decisión se pospuso para admitir más miembros hasta 2010. Por otra parte, la India no tiene fronteras en el Pacífico, como el resto de los miembros, lo que hace cuestionarse los alcances geográficos del foro.

Además se encuentran entre una docena de países que deseaban adherirse a la APEC en 2008. Colombia solicitó la adhesión a la APEC ya en 1995, pero la decisión sobre su solicitud fue postergada ya que la organización dejó de aceptar nuevos miembros de 1993 a 1996, y la moratoria se volvió a prorrogar hasta 2007 debido a la crisis financiera asiática de 1997. Guam también ha estado buscando activamente una membresía separada, citando el ejemplo de Hong Kong, pero la petición es rechazada por los Estados Unidos, que actualmente representa a Guam. APEC es una de las pocas organizaciones a nivel internacional en las que a Taiwán se le permitió ingresar, aunque sea bajo el nombre de China.

Entre 1989 y 1992 el foro APEC realizó anualmente reuniones informales de nivel ministerial. Sin embargo, a instancias del entonces Presidente de Estados Unidos Bill Clinton, a partir de 1993 se estableció como práctica que el encuentro anual incluyera a los líderes (Jefes de Estado) de los países miembros.




</doc>
<doc id="9851" url="https://es.wikipedia.org/wiki?curid=9851" title="Contaminación atmosférica">
Contaminación atmosférica

La contaminación atmosférica es la presencia en el aire de materias o formas de energía que implican riesgo, daño o molestia grave para las personas y seres de la naturaleza popular, así como que puedan atacar a distintos materiales, reducir la visibilidad o producir olores desagradables.

Desde que la Revolución Industrial inició en la segunda mitad del siglo XVIII, los procesos de producción en las fábricas, el desarrollo del transporte y el uso de los combustibles han incrementado la concentración del dióxido de carbono en la atmósfera y otros gases que son muy perjudiciales para la salud, como los óxidos de azufre y los óxidos de nitrógeno.

La contaminación atmosférica puede tener carácter local, cuando los efectos ligados al foco se sufren en las inmediaciones del mismo, o global, cuando por las características del contaminante, se ve afectado el equilibrio del planeta y zonas alejadas a las que contienen los focos emisores. Ejemplos de esto son la lluvia ácida y el calentamiento global.

Según la Organización Mundial de la Salud, el estado de la atmósfera actual provoca, por simple acto de respirar, la muerte a alrededor de siete millones de personas al año (respiración de partículas finas), viéndose muchas más perjudicadas.

La contaminación atmosférica consiste en la liberación de sustancias químicas y partículas en la atmósfera alterando su composición y suponiendo un riesgo para la salud de las personas y de los demás seres vivos. Los gases contaminantes del aire más comunes son el monóxido de carbono, el dióxido de azufre, los clorofluorocarbonos y los óxidos de nitrógeno producidos por la industria y por los gases producidos en la combustión de los vehículos. Los fotoquímicos como el ozono y el esmog se aumentan en el aire por los óxidos del nitrógeno e hidrocarburos y reaccionan a la luz solar. El material particulado o el polvo contaminante en el aire se mide por su tamaño en micrómetros, y es común en erupciones volcánicas.



Ambas clases de contaminantes, primarios y secundarios, pueden depositarse en la superficie de la Tierra por precipitación, deposición seca o húmeda e impactar en determinados receptores, como personas, animales, ecosistemas acuáticos, bosques, cosechas y materiales de diferentes tipos. En todos los países existen unos límites impuestos a la emisión o la concentración de determinados contaminantes que pueden incidir sobre la salud de la población y su bienestar o causar un impacto en el entorno.

En España, existen funcionando en la actualidad diversas redes de vigilancia de la contaminación atmosférica, instaladas en las diferentes Comunidades Autónomas y que efectúan medidas de una variada gama de contaminantes que abarcan desde los óxidos de azufre y nitrógeno hasta hidrocarburos, con sistemas de captación de partículas, monóxido de carbono, ozono, metales pesados, entre otros.



Desde los años 1960, se ha demostrado que los clorofluorocarbonos tienen efectos potencialmente negativos: contribuyen de manera muy importante a la destrucción de la capa de ozono en la estratosfera, así como a incrementar el efecto invernadero. El protocolo de Montreal puso fin a la producción de la gran mayoría de estos productos.

Es uno de los productos de la combustión incompleta. Es peligroso para las personas y los animales, puesto que se fija en la hemoglobina de la sangre, impidiendo el transporte de oxígeno en el organismo. Además, es inodoro, y a la hora de sentir un ligero dolor de cabeza ya es demasiado tarde. Se diluye muy fácilmente en el aire ambiental, pero en un medio cerrado, su concentración lo hace muy tóxico, incluso mortal. Cada año, aparecen varios casos de intoxicación mortal, a causa de aparatos de combustión puestos en funcionamiento en una habitación mal ventilada.

Los motores de combustión interna de los automóviles emiten monóxido de carbono a la atmósfera por lo que en las áreas muy urbanizadas tiende a haber una concentración excesiva de este gas hasta llegar a concentraciones de 50-100 ppm, tasas que son peligrosas para la salud de las personas.

La concentración de CO en la atmósfera está aumentando de forma constante debido al uso de carburantes fósiles como fuente de energía y es teóricamente posible demostrar que este hecho es el causante de producir un incremento de la temperatura de la Tierra –efecto invernadero– La amplitud con que este efecto puede cambiar el clima mundial depende de los datos empleados en un modelo teórico, de manera que hay modelos que predicen cambios rápidos y desastrosos del clima y otros que señalan efectos climáticos limitados. La reducción de las emisiones de CO a la atmósfera permitiría que el ciclo total del carbono alcanzara el equilibrio a través de los grandes sumideros de carbono como son el océano profundo y los sedimentos que están formados por una molécula lineal de un átomo de carbono ligado a dos átomos de oxígeno de forma formula_1

También llamado óxido de nitrógeno (II) es un gas incoloro y poco soluble en agua que se produce por la quema de combustibles fósiles en el transporte y la industria. Se oxida muy rápidamente convirtiéndose en dióxido de nitrógeno, NO, y posteriormente en ácido nítrico, HNO, produciendo así lluvia ácida o efecto invernadero

La principal fuente de emisión de dióxido de azufre a la atmósfera es la combustión del carbón que contiene azufre. El SO resultante de la combustión del azufre, que se oxida y forma ácido sulfúrico, HSO un componente de la llamada lluvia ácida que es nocivo para las plantas, provocando manchas allí donde las gotitas del ácido han contactado con las hojas.

La lluvia ácida se forma cuando la humedad en el aire se combina con el óxido de nitrógeno o el dióxido de azufre emitido por fábricas, centrales eléctricas y automotores que queman carbón o aceite. Esta combinación química de gases con el vapor de agua forma el ácido sulfúrico y los ácidos nítricos, sustancias que caen en el suelo en forma de precipitación o lluvia ácida. Los contaminantes que pueden formar la lluvia ácida pueden recorrer grandes distancias, y los vientos los trasladan miles de kilómetros antes de precipitarse con el rocío, la llovizna, o lluvia, el granizo, la nieve o la niebla normales del lugar, que se vuelven ácidos al combinarse con dichos gases residuales.

El SO también ataca a los materiales de construcción que suelen estar formados por minerales carbonatados, como la piedra caliza o el mármol, formando sustancias solubles en el agua y afectando a la integridad y la vida de los edificios o esculturas.

El metano, CH, es un gas que se forma cuando la materia orgánica se descompone en condiciones en que hay escasez de oxígeno; esto es lo que ocurre en las ciénagas, en los pantanos y en los arrozales de los países húmedos tropicales. También se produce en los procesos de la digestión y defecación de los animales herbívoros.

El metano es un gas de efecto invernadero del planeta Tierra ya que aumenta la capacidad de retención del calor por la atmósfera.

El ozono O es un constituyente natural de la atmósfera y es considerado un contaminante cuando se encuentra en las capas más bajas de ella (troposfera).

Su concentración a nivel del mar, puede oscilar alrededor de 0,01 mg kg. Cuando la contaminación debida a los gases de escape de los automóviles es elevada y la radiación solar es intensa, el nivel de ozono aumenta y puede llegar hasta 0,1 mg kg.

Las plantas pueden ser afectadas en su desarrollo por concentraciones pequeñas de ozono. El hombre también resulta afectado por el ozono a concentraciones entre 0,05 y 0,1 mg kg, causándole irritación de las fosas nasales y garganta, así como sequedad de las mucosas de las vías respiratorias superiores.


Algunas sustancias que se encuentran en la atmósfera tienen un origen natural, por lo que no son contaminantes en un sentido estricto:

Muchos estudios han demostrado enlaces entre la contaminación y los efectos para la salud. Mediante la disminución de los niveles de contaminación del aire los países pueden reducir la carga de morbilidad derivada de la contaminación del aire.

Los aumentos en la contaminación del aire se han ligado a quebranto en la función pulmonar y aumentos en los ataques cardíacos. "Niveles altos de contaminación atmosférica según el Índice de Calidad del Aire de la Agencia de Protección Ambiental de los Estados Unidos (EPA, por sus siglas en inglés) perjudican directamente a personas que padecen asma y otros tipos de enfermedad pulmonar o cardíaca". La calidad general del aire ha mejorado en los últimos 20 años pero las zonas urbanas son aún motivo de preocupación. Los ancianos y los niños son especialmente vulnerables a los efectos de la contaminación del aire. Un estudio publicado en la revista "Environment International" cifra en 6 085 las personas muertas prematura y evitablemente al año en España (un país de aire no excesivamente contaminado y 47,0 millones de habitantes en 2010) por exceso de dióxidos de nitrógeno en la atmósfera, 499 por ozono troposférico y 2 683 por partículas, todo durante el período 2000-2009. En 41 países de Europa 518700 personas fallecieron prematuramente en 2015 por la contaminación atmosférica.

El nivel de riesgo depende de varios factores:

Otras maneras menos directas en que las personas están expuestas a los contaminantes del aire son:

Los síntomas más comunes que se presentan en la salud humana a causa de la contaminación atmosférica son:

Greenpeace y IQAir Air Visual analizaron y determinaron cuáles fueron las ciudades más contaminadas del mundo a nivel atmosférico, con base a sus niveles de partículas PM2.5, cuyos límites diarios son de 45-50 µg/m3 y anuales de 10-12 µg/m3.

Los resultados fueron:


Por su parte, los países más contaminados del mundo, con base a sus porcentajes de emisiones de dióxido de carbono a nivel global fueron:


Los siguientes instrumentos son utilizados comúnmente como dispositivos de control de contaminación en la industria o en vehículos. Pueden transformar contaminantes o eliminarlos de una corriente de salida antes de ser emitidos a la atmósfera.

La gestión ambiental en el componente aire parte por realizar un modelamiento atmosférico del sector de estudio. Para ello se establecen estaciones de monitoreo de la calidad del Aire ubicando estaciones con representatividad poblacional EMRP, estas deben estar ubicadas dentro de un área urbana mínima de 2 km de diámetro para que sea representativa.

La red de monitoreo debe estar mínimamente sustentada por un equipo tripartito de Aseguramiento de la Calidad, una unidad de Control de Calidad y una unidad de distribución de la información.

El Aseguramiento de la Calidad tiene por misión soportar la unidad de monitoreo con recursos, la unidad de Control tiene por misión la trazabilidad, la calibración y el cruzamiento de resultados entre sus equipos y otros de referencia. Se debe detectar los corrimientos del valor cero, la saturación de los monitores, fuentes de emisión imprevistas no-comunes y focalizadas, cortes de energía eléctrica y aquellos valores escapados que induzcan a un mal pronóstico de Emergencia Ambiental.

La unidad informativa tiene por misión dar disponibilidad y análisis de la información confeccionando modelos informativos de contaminación del componente aire.

Para seleccionar los lugares más apropiados con los objetivos propuestos del monitoreo, es necesario manejar información que incluya, entre otros factores:
Estos puntos conducen a establecer modelos de contaminación atmosféricos y evaluación de la calidad del aire.

La Directiva 2001/81/ CE, del Parlamento Europeo y del Consejo, de 23 de octubre de 2001, "sobre techos nacionales de emisión de determinados contaminantes atmosféricos", tiene como objeto limitar las emisiones de contaminantes para reforzar la protección del medio ambiente y de la salud humana y avanzar hacia el objetivo de no superar los niveles críticos de contaminantes y de proteger de forma eficaz a toda la población frente a los riesgos para la salud que se derivan de la contaminación atmosférica mediante la fijación de techos nacionales de emisión.

El programa "Aire puro para Europa" es una estrategia temática coherente de lucha contra la contaminación atmosférica y sus efectos. Este programa ha sido elaborado por el Sexto programa de Acción en Materia de Medio Ambiente recientemente aprobada por la Comisión (COM (2001) 31 de 24.01.2001). Esta estrategia consiste en evaluar la aplicación de las directivas relativas a la calidad del aire y la eficacia de los programas sobre calidad del aire en los Estados miembros. Además pretende mejorar el control de la calidad del aire y la divulgación de la información al público mediante la utilización de indicadores. Finalmente se establecerán prioridades para la adopción de nuevas medidas, examinando y actualizando los umbrales de calidad del aire y los límites máximos nacionales de emisión.

Recoge múltiples y variados objetivos con el fin de mejorar la calidad de vida de las poblaciones de Europa. Prevenir las enfermedades y proteger el medio que nos rodean serán algunos de los objetivos prioritarios que se desarrollarán a lo largo de la estrategia planteada. Sin embargo debemos también mencionar algunos objetivos más específicos que mejoraran la labor de análisis técnico, para mejorar así la política sobre la calidad del aire.

Como medida para instar al cumplimiento de los techos, la directiva obliga a los Estados miembros a elaborar unos programas nacionales de reducción progresiva de las emisiones. España ha elaborado mediante Acuerdo de Consejo de Ministros de 7 de diciembre el "II Programa Nacional de Reducción de Emisiones" (Resolución de 14 de enero de 2008, de la Secretaría General para la Prevención de la Contaminación y el Cambio Climático. BOE n.º 25, 29.01.08).
Establece las bases en materia de prevención, vigilancia y reducción de la contaminación atmosférica con el fin de evitar y cuando esto no sea posible, aminorar los daños que de ésta puedan derivarse para las personas, el medio ambiente y demás bienes de cualquier naturaleza.

El objetivo general de dicha ley es desarrollar una política estratégica integrada a largo plazo para proteger la salud humana y el medio ambiente de los efectos de la contaminación atmosférica. De acuerdo con el tratado, esta política tendrá por objetivo garantizar un elevado nivel de protección del medio ambiente sobre la base del principio de cautela, tomando los mejores datos científicos y técnicos disponibles y las ventajas y cargas que puedan resultar de la acción o de la falta de acción












</doc>
<doc id="9853" url="https://es.wikipedia.org/wiki?curid=9853" title="Energía nuclear">
Energía nuclear

La energía nuclear o atómica es la que se libera espontánea o artificialmente en las reacciones nucleares. Sin embargo, este término engloba otro significado que es el aprovechamiento de dicha energía para otros fines, tales como la obtención de energía eléctrica, energía térmica y energía mecánica a partir de reacciones atómicas. Así, es común referirse a la energía nuclear no solo como el resultado de una reacción, sino como un concepto más amplio que incluye los conocimientos y técnicas que permiten la utilización de esta energía por parte del ser humano.

Estas reacciones se dan en los núcleos atómicos de algunos isótopos de ciertos elementos químicos (radioisótopos), siendo la más conocida la fisión del uranio-235 (U), con la que funcionan los reactores nucleares, y la más habitual en la naturaleza, en el interior de las estrellas, la fusión del par deuterio-tritio (H-H). Sin embargo, para producir este tipo de energía aprovechando reacciones nucleares pueden ser utilizados muchos otros isótopos de varios elementos químicos, como el torio-232, el plutonio-239, el estroncio-90 o el polonio-210 (Th, Pu, Sr, Po; respectivamente).

Existen varias disciplinas y/o técnicas que usan de base la energía nuclear y van desde la generación de energía eléctrica en las centrales nucleares hasta las técnicas de análisis de datación arqueológica (arqueometría nuclear), la medicina nuclear usada en los hospitales, etc. 

Los sistemas más investigados y trabajados para la obtención de energía aprovechable a partir de la energía nuclear de forma masiva son la fisión nuclear y la fusión nuclear. La energía nuclear puede transformarse de forma descontrolada, dando lugar al armamento nuclear; o controlada en reactores nucleares en los que se produce energía eléctrica, energía mecánica o energía térmica. Tanto los materiales usados como el diseño de las instalaciones son completamente diferentes en cada caso.

Otra técnica, empleada principalmente en pilas de mucha duración para sistemas que requieren poco consumo eléctrico, es la utilización de generadores termoeléctricos de radioisótopos (GTR, o RTG en inglés), en los que se aprovechan los distintos modos de desintegración para generar electricidad en sistemas de termopares a partir del calor transferido por una fuente radiactiva.

La energía desprendida en esos procesos nucleares suele aparecer en forma de partículas subatómicas en movimiento. Esas partículas, al frenarse en la materia que las rodea, producen energía térmica. Esta energía térmica se transforma en energía mecánica utilizando motores de combustión externa, como las turbinas de vapor. Dicha energía mecánica puede ser empleada en el transporte, como por ejemplo en los buques nucleares.

La principal característica de este tipo de energía es la alta calidad de la energía que puede producirse por unidad de masa de material utilizado en comparación con cualquier otro tipo de energía conocida por el ser humano, pero sorprende la poca eficiencia del proceso, ya que se desaprovecha entre un 86 % y 92 % de la energía que se libera.

En las reacciones nucleares se suele liberar una grandísima cantidad de energía debido en parte a que la masa de partículas involucradas en este proceso, se transforma directamente en energía. Lo anterior se suele explicar basándose en la relación masa-energía propuesta por el físico Albert Einstein.

En 1896 Henri Becquerel descubrió que algunos elementos químicos emitían radiaciones. Tanto él como Marie Curie y otros estudiaron sus propiedades, descubriendo que estas radiaciones eran diferentes de los ya conocidos rayos X y que poseían propiedades distintas, denominando a los tres tipos que consiguieron descubrir alfa, beta y gamma.

Pronto se vio que todas ellas provenían del núcleo atómico que describió Ernest Rutherford en 1911.

Con el descubrimiento del neutrino, partícula descrita teóricamente en 1930 por Wolfgang Pauli pero no detectada hasta 1956 por Clyde Cowan y sus colaboradores, se pudo explicar la radiación beta.

En 1932 James Chadwick descubrió la existencia del neutrón que Pauli había predicho en 1930, e inmediatamente después Enrico Fermi descubrió que ciertas radiaciones emitidas en fenómenos no muy comunes de desintegración eran en realidad estos neutrones.

Durante los años 1930, Enrico Fermi y sus colaboradores bombardearon con neutrones más de 60 elementos, entre ellos Uranio, produciendo las primeras fisiones nucleares artificiales. En 1938, en Alemania, Lise Meitner, Otto Hahn y Fritz Strassmann verificaron los experimentos de Fermi y en 1939 demostraron que parte de los productos que aparecían al llevar a cabo estos experimentos con uranio eran núcleos de bario. Muy pronto llegaron a la conclusión de que eran resultado de la división de los núcleos del uranio. Se había llevado a cabo el descubrimiento de la fisión.

En Francia, Joliot Curie descubrió que además del bario, se emitían neutrones secundarios en esa reacción, haciendo factible la reacción en cadena.

También en 1932 Mark Oliphant teorizó sobre la fusión de núcleos ligeros (de hidrógeno), describiendo poco después Hans Bethe el funcionamiento de las estrellas, basándose en este mecanismo.

En física nuclear, la fisión es una reacción nuclear, lo que significa que tiene lugar en el núcleo atómico. La fisión ocurre cuando un núcleo pesado se divide en dos o más núcleos pequeños, además de algunos subproductos como neutrones libres, fotones (generalmente rayos gamma) y otros fragmentos del núcleo como partículas alfa (núcleos de helio) y beta (electrones y positrones de alta energía).

Durante la Segunda Guerra Mundial, el Departamento de Desarrollo de Armamento de la Alemania nazi desarrolló un proyecto de energía nuclear (Proyecto Uranio) con vistas a la producción de un artefacto explosivo nuclear. Albert Einstein, en 1939, firmó una carta al presidente Franklin Delano Roosevelt de los Estados Unidos, escrita por Leó Szilárd, en la que se prevenía sobre este hecho.

El 2 de diciembre de 1942, como parte del proyecto Manhattan dirigido por J. Robert Oppenheimer, se construyó el primer reactor del mundo hecho por el ser humano (existió un reactor natural en Oklo): el Chicago Pile-1 (CP-1).

Como parte del mismo programa militar, se construyó un reactor mucho mayor en Hanford, destinado a la producción de plutonio, y al mismo tiempo, un proyecto de enriquecimiento de uranio en cascada. El 16 de julio de 1945 fue probada la primera bomba nuclear (nombre en clave Trinity) en el desierto de Alamogordo. En esta prueba se llevó a cabo una explosión equivalente a 19 000 000 kg de TNT (19 kilotones), una potencia jamás observada anteriormente en ningún otro explosivo. Ambos proyectos desarrollados finalizaron con la construcción de dos bombas, una de uranio enriquecido y una de plutonio (Little Boy y Fat Man) que fueron lanzadas sobre las ciudades japonesas de Hiroshima (6 de agosto de 1945) y Nagasaki (9 de agosto de 1945) respectivamente. El 15 de agosto de 1945 acabó la segunda guerra mundial en el Pacífico con la rendición de Japón. Por su parte el programa de armamento nuclear alemán (liderado este por Werner Heisenberg), no alcanzó su meta antes de la rendición de Alemania el 8 de mayo de 1945.

Posteriormente se llevaron a cabo programas nucleares en la Unión Soviética (primera prueba de una bomba de fisión el 29 de agosto de 1949), Francia y Gran Bretaña, comenzando la carrera armamentística en ambos bloques creados tras la guerra, alcanzando límites de potencia destructiva nunca antes sospechada por el ser humano (cada bando podía derrotar y destruir varias veces a todos sus enemigos).

Ya en la década de 1940, el almirante Hyman Rickover propuso la construcción de reactores de fisión no encaminados esta vez a la fabricación de material para bombas, sino a la generación de electricidad. Se pensó, acertadamente, que estos reactores podrían constituir un gran sustituto del diésel en los submarinos. Se construyó el primer reactor de prueba en 1953, botando el primer submarino nuclear (el USS Nautilus (SSN-571)) el 17 de enero de 1955 a las 11:00. El Departamento de Defensa estadounidense propuso el diseño y construcción de un reactor nuclear utilizable para la generación eléctrica y propulsión en los submarinos a dos empresas distintas norteamericanas: General Electric y Westinghouse. Estas empresas desarrollaron los reactores de agua ligera tipo BWR y PWR respectivamente.

Estos reactores se han utilizado para la propulsión de buques, tanto de uso militar (submarinos, cruceros, portaaviones...) como civil (rompehielos y cargueros), donde presentan unas características de potencia, reducción del tamaño de los motores, reducción de las necesidades de almacenamiento de combustible y autonomía no superadas por ninguna otra técnica existente.

Los mismos diseños de reactores de fisión se trasladaron a diseños comerciales para la generación de electricidad. Los únicos cambios producidos en el diseño con el transcurso del tiempo fueron un aumento de las medidas de seguridad, una mayor eficiencia termodinámica, un aumento de potencia y el uso de las nuevas tecnologías que fueron apareciendo.

Entre 1950 y 1960 Canadá desarrolló un nuevo tipo, basado en el PWR, que utilizaba agua pesada como moderador y uranio natural como combustible, en lugar del uranio enriquecido utilizado por los diseños de agua ligera. Otros diseños de reactores para su uso comercial utilizaron carbono (Magnox, AGR, RBMK o PBR entre otros) o sales fundidas (litio o berilio entre otros) como moderador. Este último tipo de reactor fue parte del diseño del primer avión bombardero (1954) con propulsión nuclear (el US Aircraft Reactor Experiment o ARE). Este diseño se abandonó tras el desarrollo de los misiles balísticos intercontinentales (ICBM). 

Otros países (Francia, Italia, entre otros) desarrollaron sus propios diseños de reactores nucleares comerciales para la generación de energía eléctrica.

En 1946 se construyó el primer reactor de neutrones rápidos ("Clementine") en Los Álamos, con plutonio como combustible y mercurio como refrigerante. En 1951 se construyó el EBR-I, el primer reactor rápido con el que se consiguió generar electricidad. En 1996, el Superfénix o SPX, fue el reactor rápido de mayor potencia construido hasta el momento (1200 MWe). En este tipo de reactores se pueden utilizar como combustible los radioisótopos del plutonio, el torio y el uranio que no son fisibles con neutrones térmicos (lentos).

En la década de los 50 Ernest Lawrence propuso la posibilidad de utilizar reactores nucleares con geometrías inferiores a la criticidad (reactores subcríticos cuyo combustible podría ser el torio), en los que la reacción sería soportada por un aporte externo de neutrones. En 1993 Carlo Rubbia propone utilizar una instalación de espalación en la que un acelerador de protones produjera los neutrones necesarios para mantener la instalación. A este tipo de sistemas se les conoce como "Sistemas asistidos por aceleradores" (en inglés "Accelerator driven systems", ADS sus siglas en inglés), y se prevé que la primera planta de este tipo (MYRRHA) comience su funcionamiento el 2033 en el centro de Mol (Bélgica).

En física nuclear, la fusión nuclear es el proceso por el cual varios núcleos atómicos de carga similar se unen y forman un núcleo más pesado. Simultáneamente se libera o absorbe una cantidad enorme de energía, que permite a la materia entrar en un estado plasmático.
La fusión de dos núcleos de menor masa que el hierro (en este elemento y en el níquel ocurre la mayor energía de enlace nuclear por nucleón) libera energía en general. Por el contrario, la fusión de núcleos más pesados que el hierro absorbe energía. En el proceso inverso, la fisión nuclear, estos fenómenos suceden en sentidos opuestos.
Hasta el principio del siglo XX no se entendía la forma en que se generaba energía en el interior de las estrellas necesaria para contrarrestar el colapso gravitatorio de estas. No existía reacción química con la potencia suficiente y la fisión tampoco era capaz. En 1938 Hans Bethe logró explicarlo mediante reacciones de fusión, con el ciclo CNO, para estrellas muy pesadas. Posteriormente se descubrió el ciclo protón-protón para estrellas de menor masa, como el Sol.

En los años 1940, como parte del proyecto Manhattan, se estudió la posibilidad del uso de la fusión en la bomba nuclear. En 1942 se investigó la posibilidad del uso de una reacción de fisión como método de ignición para la principal reacción de fusión, sabiendo que podría resultar en una potencia miles de veces superior. Sin embargo, tras finalizar la Segunda Guerra Mundial, el desarrollo de una bomba de estas características no fue considerado primordial hasta la explosión de la primera bomba atómica rusa en 1949, RDS-1 o "Joe-1". Este evento provocó que en 1950 el presidente estadounidense Harry S. Truman anunciara el comienzo de un proyecto que desarrollara la bomba de hidrógeno. El 1 de noviembre de 1952 se probó la primera bomba nuclear (nombre en clave "Mike", parte de la "Operación Ivy" o Hiedra), con una potencia equivalente a 10 400 000 000 de kg de TNT (10,4 megatones). El 12 de agosto de 1953 la Unión Soviética realiza su primera prueba con un artefacto termonuclear (su potencia alcanzó algunos centenares de kilotones).

Las condiciones necesarias para alcanzar la ignición de un reactor de fusión controlado, sin embargo, no fueron derivadas hasta 1955 por John D. Lawson. Los criterios de Lawson definieron las condiciones mínimas necesarias de tiempo, densidad y temperatura que debía alcanzar el combustible nuclear (núcleos de hidrógeno) para que la reacción de fusión se mantuviera. Sin embargo, ya en 1946 se patentó el primer diseño de reactor termonuclear. En 1951 comenzó el programa de fusión de Estados Unidos, sobre la base del stellarator. En el mismo año comenzó en la Unión Soviética el desarrollo del primer Tokamak, dando lugar a sus primeros experimentos en 1956. Este último diseño logró en 1968 la primera reacción termonuclear cuasi-estacionaria jamás conseguida, demostrándose que era el diseño más eficiente conseguido hasta la época. ITER, el diseño internacional que tiene fecha de comienzo de sus operaciones en el año 2016 y que intentará resolver los problemas existentes para conseguir un reactor de fusión de confinamiento magnético, utiliza este diseño.

En 1962 se propuso otra técnica para alcanzar la fusión basada en el uso de láseres para conseguir una implosión en pequeñas cápsulas llenas de combustible nuclear (de nuevo núcleos de hidrógeno). Sin embargo hasta la década de los 70 no se desarrollaron láseres suficientemente potentes. Sus inconvenientes prácticos hicieron de esta una opción secundaria para alcanzar el objetivo de un reactor de fusión. Sin embargo, debido a los tratados internacionales que prohibían la realización de ensayos nucleares en la atmósfera, esta opción (básicamente microexplosiones termonucleares) se convirtió en un excelente laboratorio de ensayos para los militares, con lo que consiguió financiación para su continuación. Así, se han construido el National Ignition Facility (NIF, con inicio de sus pruebas programadas para 2010) estadounidense y el Láser Mégajoule francés (LMJ), que persiguen el mismo objetivo de conseguir un dispositivo que consiga mantener la reacción de fusión a partir de este diseño. Ninguno de los proyectos de investigación actualmente en marcha predicen una ganancia de energía significativa, por lo que está previsto un proyecto posterior que pudiera dar lugar a los primeros reactores de fusión comerciales (DEMO con confinamiento magnético e HiPER con confinamiento inercial).

Con la invención de la pila química por Volta en 1800 se dio lugar a una forma compacta y portátil de generación de energía. A partir de entonces fue incesante la búsqueda de sistemas que fueran aún menores y que tuvieran una mayor capacidad y duración. Este tipo de pilas, con pocas variaciones, han sido suficientes para muchas aplicaciones diarias hasta nuestros tiempos. Sin embargo, en el siglo XX surgieron nuevas necesidades, a causa principalmente de los programas espaciales. Se precisaban entonces sistemas que tuvieran una duración elevada para consumos eléctricos moderados y un mantenimiento nulo. Surgieron varias soluciones (como los paneles solares o las células de combustible), pero según se incrementaban las necesidades energéticas y aparecían nuevos problemas (las placas solares son inútiles en ausencia de luz solar), se comenzó a estudiar la posibilidad de utilizar la energía nuclear en estos programas.

A mediados de la década de los 50 comenzaron en Estados Unidos las primeras investigaciones encaminadas a estudiar las aplicaciones nucleares en el espacio. De estas surgieron los primeros prototipos de los "generadores termoeléctricos de radioisótopos" (RTG). Estos dispositivos mostraron ser una alternativa sumamente interesante tanto en las aplicaciones espaciales como en aplicaciones terrestres específicas. En estos artefactos se aprovechan las desintegraciones alfa y beta, convirtiendo toda o gran parte de la energía cinética de las partículas emitidas por el núcleo en calor. Este calor es después transformado en electricidad aprovechando el efecto Seebeck mediante unos termopares, consiguiendo eficiencias aceptables (entre un 5 y un 40 % es lo habitual). Los radioisótopos habitualmente utilizados son Po, Cm, Pu, Am, entre otros 30 que se consideraron útiles. Estos dispositivos consiguen capacidades de almacenamiento de energía 4 órdenes de magnitud superiores (10 000 veces mayor) a las baterías convencionales.

En 1959 se mostró al público el primer "generador atómico". En 1961 se lanzó al espacio el primer RTG, a bordo del SNAP 3. Esta batería nuclear, que alimentaba a un satélite de la armada norteamericana con una potencia de 2,7 W, mantuvo su funcionamiento ininterrumpido durante 15 años.

Estos sistemas se han utilizado y se siguen usando en programas espaciales muy conocidos (Pioneer, Voyager, Galileo, Apolo y Ulises entre otros). Así por ejemplo en 1972 y 1973 se lanzaron los Pioneer 10 y 11, convirtiéndose el primero de ellos en el primer objeto humano de la historia que abandonaba el sistema solar. Ambos satélites continuaron funcionando hasta 17 años después de sus lanzamientos.

La misión Ulises (misión conjunta ESA-NASA) se envió en 1990 para estudiar el Sol, siendo la primera vez que un satélite cruzaba ambos polos solares. Para poder hacerlo hubo que enviar el satélite en una órbita alrededor de Júpiter. Debido a la duración del RTG que mantiene su funcionamiento se prolongó la misión de modo que se pudiera volver a realizar otro viaje alrededor del Sol. Aunque pareciera extraño que este satélite no usara paneles solares en lugar de un RTG, puede entenderse al comparar sus pesos (un panel de 544 kg generaba la misma potencia que un RTG de 56). En aquellos años no existía un cohete que pudiera enviar a su órbita al satélite con ese peso extra.

Estas baterías no solo proporcionan electricidad, sino que en algunos casos, el propio calor generado se utiliza para evitar la congelación de los satélites en viajes en los que el calor del Sol no es suficiente, por ejemplo en viajes fuera del sistema solar o en misiones a los polos de la Luna.

En 1965 se instaló el primer RTG terrestre para el faro de la isla deshabitada Fairway Rock, permaneciendo en funcionamiento hasta 1995, momento en el que se desmanteló. Otros muchos faros situados en zonas inaccesibles cercanas a los polos (sobre todo en la Unión Soviética), utilizaron estos sistemas. Se sabe que la Unión Soviética fabricó más de 1000 unidades para estos usos.

Una aplicación que se dio a estos sistemas fue su uso como marcapasos. Hasta los 70 se usaba para estas aplicaciones baterías de mercurio-zinc, que tenían una duración de unos 3 años. En esta década se introdujeron las baterías nucleares para aumentar la longevidad de estos artefactos, posibilitando que un paciente joven tuviera implantado solo uno de estos artefactos para toda su vida. En los años 1960, la empresa Medtronic contactó con Alcatel para diseñar una batería nuclear, implantando el primer marcapasos alimentado con un RTG en un paciente en 1970 en París. Varios fabricantes construyeron sus propios diseños, pero a mediados de esta década fueron desplazados por las nuevas baterías de litio, que poseían vidas de unos 10 años (considerado suficiente por los médicos aunque debiera sustituirse varias veces hasta la muerte del paciente). A mediados de los años 1980 se detuvo el uso de estos implantes, aunque aún existen personas que siguen portando este tipo de dispositivos.

"Sir" James Chadwick descubrió el neutrón en 1932, año que puede considerarse como el inicio de la física nuclear moderna.

El modelo de átomo propuesto por Niels Bohr consiste en un núcleo central compuesto por partículas que concentran la mayoría de la masa del átomo (neutrones y protones), rodeado por varias capas de partículas cargadas casi sin masa (electrones). Mientras que el tamaño del átomo resulta ser del orden del angstrom (10 m), el núcleo puede medirse en fermis (10 m), o sea, el núcleo es 100000 veces menor que el átomo.

Todos los átomos neutros (sin carga eléctrica) poseen el mismo número de electrones que de protones. Un elemento químico se puede identificar de forma inequívoca por el número de protones que posee su núcleo; este número se llama número atómico (Z). El número de neutrones (N) sin embargo puede variar para un mismo elemento. Para valores bajos de Z ese número tiende a ser muy parecido al de protones, pero al aumentar Z se necesitan más neutrones para mantener la estabilidad del núcleo. A los átomos a los que solo les distingue el número de neutrones en su núcleo (en definitiva, su masa), se les llama isótopos de un mismo elemento. La masa atómica de un isótopo viene dada por formula_1 u, el número de protones más el de neutrones (nucleones) que posee en su núcleo.

Para denominar un isótopo suele utilizarse la letra que indica el elemento químico, con un superíndice que es la masa atómica y un subíndice que es el número atómico (p. ej. el isótopo 238 del uranio se escribiría como formula_2).

Los neutrones y protones que forman los núcleos tienen una masa aproximada de 1 u, estando el protón cargado eléctricamente con carga positiva +1, mientras que el neutrón no posee carga eléctrica. Teniendo en cuenta únicamente la existencia de las fuerzas electromagnética y gravitatoria, el núcleo sería inestable (ya que las partículas de igual carga se repelerían deshaciendo el núcleo), haciendo imposible la existencia de la materia. Por este motivo (ya que es obvio que la materia existe) fue necesario añadir a los modelos una tercera fuerza: la fuerza fuerte (hoy en día "fuerza nuclear fuerte residual"). Esta fuerza debía tener como características, entre otras, que era muy intensa, atractiva a distancias muy cortas (solo en el interior de los núcleos), siendo repulsiva a distancias más cortas (del tamaño de un nucleón), que era central en cierto rango de distancias, que dependía del espín y que no dependía del tipo de nucleón (neutrones o protones) sobre el que actuaba. En 1935, Hideki Yukawa dio una primera solución a esta nueva fuerza estableciendo la hipótesis de la existencia de una nueva partícula: el mesón. El más ligero de los mesones, el pion, es el responsable de la mayor parte del potencial entre nucleones de largo alcance (1 rfm). El potencial de Yukawa (potencial OPEP) que describe adecuadamente la interacción para dos partículas de espines formula_3 y formula_4 respectivamente, se puede escribir como:

Otros experimentos que se realizaron sobre los núcleos indicaron que su forma debía de ser aproximadamente esférica de radio formula_6 fm, siendo "A" la masa atómica, es decir, la suma de neutrones y protones. Esto exige además que la densidad de los núcleos sea la misma (formula_7, es decir el volumen es proporcional a A. Como la densidad se halla dividiendo la masa por el volumen formula_8 ). Esta característica llevó a la equiparación de los núcleos con un líquido, y por tanto al modelo de la gota líquida, fundamental en la comprensión de la fisión de los núcleos.

La masa de un núcleo, sin embargo, no resulta exactamente de la suma de sus nucleones. Tal y como demostró Albert Einstein, la energía que mantiene unidos a esos nucleones es la diferencia entre la masa del núcleo y la de sus elementos, y viene dada por la ecuación formula_9. Así, "pesando" los distintos átomos por una parte, y sus componentes por otra, puede determinarse la energía media por nucleón que mantiene unidos a los diferentes núcleos.

En la gráfica puede contemplarse como los núcleos muy ligeros poseen menos energía de ligadura que los que son un poco más pesados (la parte izquierda de la gráfica). Esta característica es la base de la liberación de la energía en la fusión. Y, al contrario, en la parte de la derecha se ve que los elementos muy pesados tienen menor energía de ligadura que los que son algo más ligeros. Esta es la base de la emisión de energía por fisión. Como se ve, es mucho mayor la diferencia en la parte de la izquierda (fusión) que en la de la derecha (fisión).

Fermi, tras el descubrimiento del neutrón, realizó una serie de experimentos en los que bombardeaba distintos núcleos con estas nuevas partículas. En estos experimentos observó que cuando utilizaba neutrones de energías bajas, en ocasiones el neutrón era absorbido emitiéndose fotones. 

Para averiguar el comportamiento de esta reacción repitió el experimento sistemáticamente en todos los elementos de la tabla periódica. Así descubrió nuevos elementos radiactivos, pero al llegar al uranio obtuvo resultados distintos. Lise Meitner, Otto Hahn y Fritz Strassmann consiguieron explicar el nuevo fenómeno al suponer que el núcleo de uranio al capturar el neutrón se escindía en dos partes de masas aproximadamente iguales. De hecho detectaron bario, de masa aproximadamente la mitad que la del uranio. Posteriormente se averiguó que esa escisión (o fisión) no se daba en todos los isótopos del uranio, sino solo en el U. Y más tarde aún, se supo que esa escisión podía dar lugar a muchísimos elementos distintos, cuya distribución de aparición es muy típica (similar a la doble joroba de un camello).
En la fisión de un núcleo de uranio, no solo aparecen dos núcleos más ligeros resultado de la división del de uranio, sino que además se emiten 2 o 3 (en promedio 2,5 en el caso del U) neutrones a una alta velocidad (energía). Como el uranio es un núcleo pesado no se cumple la relación N=Z (igual número de protones que de neutrones) que sí se cumple para los elementos más ligeros, por lo que los productos de la fisión poseen un exceso de neutrones. Este exceso de neutrones hace inestables (radiactivos) a esos productos de fisión, que alcanzan la estabilidad al desintegrarse los neutrones excedentes por desintegración beta generalmente. La fisión del U puede producirse en más de 40 formas diferentes, originándose por tanto más de 80 productos de fisión distintos, que a su vez se desintegran formando cadenas de desintegración, por lo que finalmente aparecen cerca de 200 elementos a partir de la fisión del uranio.

La energía desprendida en la fisión de cada núcleo de U es en promedio de 200 MeV. Los minerales explotados para la extracción del uranio suelen poseer contenidos de alrededor de 1 gramo de uranio por kg de mineral (la pechblenda por ejemplo). Como el contenido de U en el uranio natural es de un 0,7 %, se obtiene que por cada kg de mineral extraído tendríamos formula_10 átomos de U. Si fisionamos todos esos átomos (1 gramo de uranio) obtendríamos teóricamente una energía liberada de formula_11 por gramo. En comparación, por la combustión de 1 kg de carbón de la mejor calidad (antracita) se obtiene una energía de unos formula_12, es decir, se necesitan más de 10 toneladas de antracita (el tipo de carbón con mayor poder calorífico) para obtener la misma energía contenida en 1 kg de uranio natural.

La aparición de los 2,5 neutrones por cada fisión posibilita la idea de llevar a cabo una reacción en cadena, si se logra hacer que de esos 2,5 al menos un neutrón consiga fisionar un nuevo núcleo de uranio. La idea de la reacción en cadena es común en otros procesos químicos. Los neutrones emitidos por la fisión no son útiles inmediatamente si lo que se quiere es controlar la reacción, sino que hay que frenarlos (moderarlos) hasta una velocidad adecuada. Esto se consigue rodeando los átomos por otro elemento con un Z pequeño, como por ejemplo hidrógeno, carbono o litio, material denominado moderador.

Otros átomos que pueden fisionar con neutrones lentos son el U o el Pu. Sin embargo también es posible la fisión con neutrones rápidos (de energías altas), como por ejemplo el U (140 veces más abundante que el U) o el Th (400 veces más abundante que el U).

La teoría elemental de la fisión la proporcionaron Bohr y Wheeler, utilizando un modelo según el cual los núcleos de los átomos se comportan como gotas líquidas.

La fisión se puede conseguir también mediante partículas alfa, protones o deuterones.

Así como la fisión es un fenómeno que aparece en la corteza terrestre de forma natural (si bien con una frecuencia pequeña), la fusión es absolutamente artificial en nuestro entorno (aunque es común el núcleo de las estrellas). Sin embargo, esta energía posee ventajas con respecto a la fisión. Por un lado el combustible es abundante y fácil de conseguir, y por otro, sus productos son elementos estables, ligeros y no radiactivos.

En la fusión, al contrario que en la fisión donde se dividen los núcleos, la reacción consiste en la unión de dos o más núcleos ligeros. Esta unión da lugar a un núcleo más pesado que los usados inicialmente y a neutrones. La fusión se consiguió antes incluso de comprender completamente las condiciones que se necesitaban en el desarrollo de armas, limitándose a conseguir condiciones extremas de presión y temperatura usando una bomba de fisión como elemento iniciador (Proceso Teller-Ulam). Pero no es hasta que Lawson define unos criterios de tiempo, densidad y temperatura mínimos cuando se comienza a comprender el funcionamiento de la fusión.

Aunque en las estrellas la fusión se da entre una variedad de elementos químicos, el elemento con el que es más sencillo alcanzarla es el hidrógeno. El hidrógeno posee tres isótopos: el hidrógeno común (formula_13), el deuterio (formula_14) y el tritio (formula_15). Esto es así porque la fusión requiere que se venza la repulsión electrostática que experimentan los núcleos al unirse, por lo que a menor carga eléctrica, menor será esta. Además, a mayor cantidad de neutrones, más pesado será el núcleo resultante (más arriba estaremos en la gráfica de las energías de ligadura), con lo que mayor será la energía liberada en la reacción.

Una reacción particularmente interesante es la fusión de deuterio y tritio:

En esta reacción se liberan 17,6 MeV por fusión, más que en el resto de combinaciones con isótopos de hidrógeno. Además esta reacción proporciona un neutrón muy energético que puede aprovecharse para generar combustible adicional para reacciones posteriores de fusión, utilizando litio, por ejemplo. La energía liberada por gramo con esta reacción es casi mil veces mayor que la lograda en la fisión de un gramo de uranio natural (unas siete veces superior si fuera un gramo de U puro).

Para vencer la repulsión electrostática, es necesario que los núcleos a fusionar alcancen una energía cinética de aproximadamente 10 keV. Esta energía se obtiene mediante un intenso calentamiento (igual que en las estrellas, donde se alcanzan temperaturas de 10 K), que implica un movimiento de los átomos igual de intenso. Además de esa velocidad para vencer la repulsión electrostática, la probabilidad de que se produzca la fusión debe ser elevada para que la reacción suceda. Esto implica que se deben poseer suficientes átomos con energía suficiente durante un tiempo mínimo. El criterio de Lawson define que el producto entre la densidad de núcleos con esa energía por el tiempo durante el que deben permanecer en ese estado debe ser formula_17.

Los dos métodos en desarrollo para aprovechar de forma útil (no bélica) la energía desprendida en esta reacción son el confinamiento magnético y el confinamiento inercial (con fotones que provienen de láser o partículas que provienen de aceleradores).

Esta reacción es una forma de fisión espontánea, en la que un núcleo pesado emite una partícula alfa (α) con una energía típica de unos 5 MeV. Una partícula α es un núcleo de helio, constituido por dos protones y dos neutrones. En su emisión el núcleo cambia, por lo que el elemento químico que sufre este tipo de desintegración muta en otro distinto. Una reacción natural típica es la siguiente:

En la que un átomo de U se transforma en otro de Th.

Fue en 1928 cuando George Gamow dio una explicación teórica a la emisión de estas partículas. Para ello supuso que la partícula alfa convivía en el interior del núcleo con el resto de los nucleones, de una forma casi independiente. Por efecto túnel en algunas ocasiones esas partículas superan el pozo de potencial que crea el núcleo, separándose de él a una velocidad de un 5 % la velocidad de la luz.

Existen dos modos de desintegración beta. En el tipo β la fuerza débil convierte un neutrón ("n") en un protón ("p") y al mismo tiempo emite un electrón ("e") y un antineutrino (formula_19):
En el tipo β un protón se transforma en un neutrón emitiendo un positrón ("e") y un neutrino (formula_21):
Sin embargo, este último modo no se presenta de forma aislada, sino que necesita un aporte de energía.

La desintegración beta hace cambiar al elemento químico que la sufre. Por ejemplo, en la desintegración β el elemento se transforma en otro con un protón (y un electrón) más. Así en la desintegración del Cs por β;

En 1934, Enrico Fermi consiguió crear un modelo de esta desintegración que respondía correctamente a su fenomenología.

Un arma es todo instrumento, medio o máquina que se destina a atacar o a defenderse. Según tal definición, existen dos categorías de armas nucleares:

Existen dos formas básicas de utilizar la energía nuclear desprendida por reacciones en cadena descontroladas de forma explosiva: la fisión y la fusión.

El 16 de julio de 1945 se produjo la primera explosión de una bomba de fisión creada por el ser humano: La Prueba Trinity.

Existen dos tipos básicos de bombas de fisión: utilizando uranio altamente enriquecido (enriquecimiento superior al 90 % en U) o utilizando plutonio. Ambos tipos se fundamentan en una reacción de fisión en cadena descontrolada y solo se han empleado en un ataque real en Hiroshima y Nagasaki, al final de la Segunda Guerra Mundial.

Para que este tipo de bombas funcionen es necesario utilizar una cantidad del elemento utilizado superior a la Masa crítica. Suponiendo una riqueza en el elemento del 100 %, eso suponen 52 kg de U o 10 kg de Pu. Para su funcionamiento se crean 2 o más partes subcríticas que se unen mediante un explosivo químico convencional de forma que se supere la masa crítica.

Los dos problemas básicos que se debieron resolver para crear este tipo de bombas fueron:

El rango de potencia de estas bombas se sitúa entre aproximadamente el equivalente a una tonelada de TNT hasta los 500.000 kilotones.

Tras el primer ensayo exitoso de una bomba de fisión por la Unión Soviética en 1949 se desarrolló una segunda generación de bombas nucleares que utilizaban la fusión. Se la llamó "bomba termonuclear", "bomba H" o bomba de hidrógeno. Este tipo de bomba no se ha utilizado nunca contra ningún objetivo real. El llamado diseño Teller-Ullam (o secreto de la bomba H) separa ambas explosiones en dos fases.

Este tipo de bombas pueden ser miles de veces más potentes que las de fisión. En teoría no existe un límite a la potencia de estas bombas, siendo la de mayor potencia explotada la bomba del Zar, de una potencia superior a los 50 megatones. 

Las bombas de hidrógeno utilizan una bomba primaria de fisión que genera las condiciones de presión y temperatura necesarias para comenzar la reacción de fusión de núcleos de hidrógeno. Los únicos productos radiactivos que generan estas bombas son los producidos en la explosión primaria de fisión, por lo que a veces se le ha llamado "bomba nuclear limpia". El extremo de esta característica son las llamadas bombas de neutrones o "bomba N", que minimizan la bomba de fisión primaria, logrando un mínimo de productos de fisión. Estas bombas además se diseñaron de tal modo que la mayor cantidad de energía liberada sea en forma de neutrones, con lo que su potencia explosiva es la décima parte que una bomba de fisión. Fueron concebidas como armas anti-tanque, ya que al penetrar los neutrones en el interior de los mismos, matan a sus ocupantes por las radiaciones.

Durante la segunda guerra mundial se comprobó que el submarino podía ser un arma decisiva, pero poseía un grave problema: su necesidad de emerger tras cortos períodos para obtener aire para la combustión del diésel en que se basaban sus motores (la invención del snorkel mejoró algo el problema, pero no lo solucionó). El Almirante Hyman G. Rickover fue el primero que pensó que la energía nuclear podría ayudar con este problema.

Los desarrollos de los reactores nucleares permitieron un nuevo tipo de motor con ventajas fundamentales:

Estas ventajas condujeron a buques que alcanzan velocidades de más de 25 nudos, que pueden permanecer semanas en inmersión profunda y que además pueden almacenar enormes cantidades de munición (nuclear o convencional) en sus bodegas. De hecho las armadas de Estados Unidos, Francia y el Reino Unido solo poseen submarinos que utilizan este sistema de propulsión.

En los submarinos se han utilizado reactores de agua a presión, de agua en ebullición o de sales fundidas. Para conseguir reducir el peso del combustible en estos reactores se usa uranio con altos grados de enriquecimiento (del 30 al 40 % en los rusos o del 96 % en los estadounidenses). Estos reactores presentan la ventaja de que no es necesario (aunque sí es posible) convertir el vapor generado por el calor en electricidad, sino que puede utilizarse de forma directa sobre una turbina que proporciona el movimiento a las hélices que impulsan el buque, mejorando notablemente el rendimiento.

Se han construido una gran variedad de buques militares que usan motores nucleares y que, en algunos casos, portan a su vez misiles de medio o largo alcance con cabezas nucleares:

Estados Unidos, Gran Bretaña, Rusia, China y Francia poseen buques de propulsión nuclear.
Tanto Estados Unidos como la Unión Soviética se plantearon la creación de una flota de bombarderos de propulsión nuclear. De este modo se pretendía mantenerlos cargados con cabezas nucleares y volando de forma permanente cerca de los objetivos prefijados. Con el desarrollo del Misil balístico intercontinental (ICBM) a finales de los 50, más rápidos y baratos, sin necesidad de pilotos y prácticamente invulnerables, se abandonaron todos los proyectos.

Los proyectos experimentales fueron:

La energía nuclear se utiliza desde los años 50 como sistema para dar empuje (propulsar) distintos sistemas, desde los submarinos (el primero que utilizó la energía nuclear), hasta naves espaciales.

Tras el desarrollo de los buques de propulsión nuclear de uso militar se hizo pronto patente que existían ciertas situaciones en las que sus características podían ser trasladadas a la navegación civil.

Se han construido cargueros y rompehielos que usan reactores nucleares como propulsión.

El primer buque nuclear de carga y pasajeros fue el NS Savannah, botado en 1962. Solo se construyeron otros tres buques de carga y pasajeros: El Mutsu japonés, el Otto Hahn alemán y el Sevmorput ruso. El Sevmorput (acrónimo de 'Severnii Morskoi Put'), botado en 1988 y dotado con un reactor nuclear tipo KLT-40 de 135 MW, sigue en activo hoy en día transitando la ruta del mar del norte.

Aunque existen varias opciones que pueden utilizar la energía nuclear para propulsar cohetes espaciales, solo algunas han alcanzado niveles de diseño avanzados.

El cohete termonuclear, por ejemplo, utiliza hidrógeno recalentado en un reactor nuclear de alta temperatura, consiguiendo empujes al menos dos veces superiores a los cohetes químicos. Este tipo de cohetes se probaron por primera vez en 1959 (el Kiwi 1), dentro del Proyecto NERVA, cancelado en 1972. En 1990 se relanzó el proyecto bajo las siglas SNTP ("Space Nuclear Thermal Propulsion") dentro del proyecto para un viaje tripulado a Marte en 2019. En 2003 comenzó con el nombre de Proyecto Prometeo. Otra de las posibilidades contempladas es el uso de un reactor nuclear que alimente a un propulsor iónico ("Nuclear Electric Xenon Ion System" o NEXIS).

El Proyecto Orión fue un proyecto ideado por Stanisław Ulam en 1947, que comenzó en 1958 en la empresa General Atomics. Su propósito era la realización de viajes interplanetarios de forma barata a una velocidad de un 10 % de c. Para ello utilizaba un método que se denominó propulsión nuclear pulsada ("External Pulsed Plasma Propulsion" es su denominación oficial en inglés). El proyecto fue abandonado en 1963, pero el mismo diseño se ha utilizado como base en el Proyecto Daedalus británico con motor de fusión, el Proyecto Longshot americano con motor de fisión acoplado a un motor de fusión inercial o el Proyecto Medusa.

También se ha propuesto el uso de RTG como fuente para un cohete de radioisótopos.

La única propuesta conocida es el diseño conceptual lanzado por Ford en 1958: el Ford Nucleon. Nunca fue construido un modelo operacional. En su diseño se proponía el uso de un pequeño reactor de fisión que podía proporcionar una autonomía de más de 8000 km. Un prototipo del coche se mantiene en el museo Henry Ford.

Una opción, incluida en las alternativas al petróleo, es el uso del hidrógeno en células de combustible como combustible para vehículos de hidrógeno. Se está investigando en este caso el uso de la energía nuclear para la generación del hidrógeno necesario mediante reacciones termoquímicas o de electrólisis con vapor a alta temperatura.

Probablemente, la aplicación práctica más conocida de la energía nuclear es la generación de energía eléctrica para su uso civil, en particular mediante la fisión de uranio enriquecido. Para ello se utilizan reactores en los que se hace fisionar o fusionar un combustible. El funcionamiento básico de este tipo de instalaciones industriales es similar a cualquier otra central térmica, sin embargo poseen características especiales con respecto a las que usan combustibles fósiles:

Tras su uso exclusivamente militar, se comenzó a plantear la aplicación del conocimiento adquirido a la vida civil. El 20 de diciembre de 1951 fue el primer día que se consiguió generar electricidad con un reactor nuclear (en el reactor estadounidense EBR-I, con una potencia de unos 100 kW), pero no fue hasta 1954 cuando se conectó a la red eléctrica una central nuclear (fue la central nuclear soviética Obninsk, generando 5 MW con solo un 17 % de rendimiento térmico). El primer reactor de fisión comercial fue el Calder Hall en Sellafield, que se conectó a la red eléctrica en 1956. El 25 de marzo de 1957 se creó la Comunidad Europea de la Energía Atómica (EURATOM), el mismo día que se creó la Comunidad Económica Europea, entre Bélgica, Francia, Alemania, Italia, Luxemburgo y los Países Bajos. Ese mismo año se creó el Organismo Internacional de Energía Atómica (OIEA). Ambos organismos con la misión, entre otras, de impulsar el uso pacífico de la energía nuclear.
Su desarrollo en todo el mundo experimentó a partir de ese momento un gran crecimiento, de forma muy particular en Francia y Japón, donde la crisis del petróleo de 1973 influyó definitivamente, ya que su dependencia del petróleo para la generación eléctrica era muy marcada (39 y 73 % respectivamente en aquellos años, en 2008 generan un 78 y un 30 % respectivamente mediante reactores de fisión). En 1979 el accidente de Three Mile Island provocó un aumento muy considerable en las medidas de control y de seguridad en las centrales, sin embargo no se detuvo el aumento de capacidad instalada. Pero en 1986 el accidente de Chernóbil, en un reactor RBMK de diseño soviético que no cumplía los requisitos de seguridad que se exigían en Occidente, cortó drásticamente ese crecimiento.

En octubre de 2007 existían 439 centrales nucleares en todo el mundo que generaron 2,7 millones de MWh en 2006. La potencia instalada en 2007 fue de 370 721 MWe. En marzo de 2008 había 35 centrales en construcción, planes para construir 91 centrales nuevas (99 095 MWe) y otras 228 propuestas (198 995 MWe). Aunque solo 30 países en el mundo poseen centrales nucleares, aproximadamente el 15 % de la energía eléctrica generada en el mundo se produce a partir de energía nuclear.

La mayoría de los reactores son de los llamados de agua ligera ("LWR" por su sigla en inglés), que utilizan como moderador agua intensamente purificada. En estos reactores el combustible utilizado es uranio enriquecido ligeramente (entre el 3 y el 5 %).

Más tarde se planteó añadir el plutonio fisible generado (formula_24) como combustible extra en estos reactores de fisión, aumentando de una forma importante la eficiencia del combustible nuclear y reduciendo así uno de los problemas del combustible gastado. Esta posibilidad incluso llevó al uso del plutonio procedente del armamento nuclear desmantelado en las principales potencias mundiales. Así se desarrolló el combustible MOX, en el que se añade un porcentaje (entre un 3 y un 10 % en masa) de este plutonio a uranio empobrecido. Este combustible se usa actualmente como un porcentaje del combustible convencional (de uranio enriquecido). También se ha ensayado en algunos reactores un combustible mezcla de torio y plutonio, que genera una menor cantidad de elementos transuránicos. 

Otros reactores utilizan agua pesada como moderador. En estos reactores se puede utilizar uranio natural, es decir, sin enriquecer y además se produce una cantidad bastante elevada de tritio por activación neutrónica. Este tritio se prevé que pueda aprovecharse en futuras plantas de fusión. 

Otros proyectos de fisión, que no han superado hoy en día la fase de experimentación, se encaminan al diseño de reactores en los que pueda generarse electricidad a partir de otros isótopos, principalmente el formula_25 y el formula_26.
La diferencia básica entre los distintos diseños de reactores nucleares de fisión es el combustible que utilizan. Esto influye en el tipo de moderador y refrigerante usados. De entre todas las posibles combinaciones entre tipo de combustible, moderador y refrigerante, solo algunas son viables técnicamente (unas 100 contando las opciones de neutrones rápidos). Pero solo unas cuantas se han utilizado hasta el momento en reactores de uso comercial para la generación de electricidad (ver tabla).

El único isótopo natural que es fisionable con neutrones térmicos es el formula_27, que se encuentra en una proporción de un 0.7 % en peso en el uranio natural. El resto es formula_26, considerado fértil, ya que, aunque puede fisionar con neutrones rápidos, por activación con neutrones se convierte en formula_24, que sí es físil mediante neutrones térmicos.

Los reactores de fisión comerciales, tanto de primera como de segunda o tercera generación, utilizan uranio con grados de enriquecimiento distinto, desde uranio natural hasta uranio ligeramente enriquecido (por debajo del 6 %). Además, en aquellos en los que se usa uranio enriquecido, la configuración del núcleo del reactor utiliza diferentes grados de enriquecimiento, con uranio más enriquecido en el centro y menos hacia el exterior. Esta configuración consigue dos fines: por una parte disminuir los neutrones de fuga por reflexión, y por otra parte aumentar la cantidad de formula_24 consumible. En los reactores comerciales se hacen fisionar esos átomos fisibles con neutrones térmicos hasta el máximo posible (al grado de "quemado" del combustible se le denomina burnup), ya que se obtienen mayores beneficios cuanto más provecho se saca del combustible.

Otro isótopo considerado fértil con neutrones térmicos es el torio (elemento natural, compuesto en su mayoría por el isótopo formula_25), que por activación produce formula_32, físil con neutrones térmicos y rápidos (es regla general que aquellos elementos con número atómico A impar sean fisibles, y con A par fértiles).

Esos tres isótopos son los que producen fisiones exoergicas, es decir, generan más energía que la necesaria para producirlas, con neutrones térmicos. Los demás elementos (con z<92) solo fisionan con neutrones rápidos. Así el formula_26 por ejemplo puede fisionarse con neutrones de energías superiores a 1,1 MeV.

Aunque hay varias formas de clasificar los distintos reactores nucleares, la más utilizada, y con la que se denominan los distintos tipos de reactores de fisión es por la combinación moderador/refrigerante utilizado. Estas son las denominaciones de los reactores comerciales de neutrones térmicos utilizados en la actualidad (de segunda generación), junto a su número en el mundo (entre paréntesis) y sus características principales:


Los diseños de reactores que utilizan neutrones rápidos, y por tanto pueden utilizar como combustible formula_26, formula_24 o formula_25 entre otros, no necesitan moderador para funcionar. Por ese motivo es difícil utilizar los mismos materiales que se usan en los térmicos como refrigerantes, ya que en muchas ocasiones también actúan como moderador. Todos los reactores de este tipo hasta el momento han utilizado como refrigerante metales líquidos (mercurio, plutonio, yoduro potásico, plomo, bismuto, sodio...). Cuando estos reactores además consiguen producir más cantidad de material físil que el que consumen se les denomina reactores reproductores rápidos. En la actualidad existen cuatro FBR, tres en parada fría y solo uno en operación comercial.

Los diseños de reactores que aprovechan las lecciones aprendidas en el medio siglo transcurrido (aproximadamente una docena de diseños distintos) se denominan de tercera generación o reactores avanzados. Solo se han puesto en marcha algunos en Japón y se están construyendo algunos otros. En general son evoluciones de los reactores de segunda generación (como el BWR avanzado o ABWR o el PWR avanzado: el EPR o el AP1000), aunque existen algunos diseños completamente nuevos (como el PBMR que utiliza helio como refrigerante y combustible TRISO que contiene el moderador de grafito en su composición).

Los reactores de cuarta generación no saldrán del papel al menos hasta 2020, y en general son diseños que buscan, además de niveles de seguridad superiores a las plantas de fisión de las generaciones anteriores, que los únicos residuos de alta actividad tengan vidas muy cortas, quemando los actínidos de vida larga. A este grupo pertenecen por ejemplo los reactores asistidos por acelerador (ADS). En general estos reactores se basarán en neutrones rápidos.

Existen algunos otros diseños, basados fundamentalmente en los descritos, para generar energía en lugares remotos, como el reactor flotante ruso KLT-40S o el microrreactor nuclear de 200 kW de Toshiba.
Como cualquier actividad humana, una central nuclear de fisión conlleva riesgos y beneficios. Los riesgos deben preverse y analizarse para poder ser mitigados. A todos aquellos sistemas diseñados para eliminar o al menos minimizar esos riesgos se les llama sistemas de protección y control. En una central nuclear de uso civil se utiliza una aproximación llamada "defensa en profundidad". Esta aproximación sigue un diseño de múltiples barreras para alcanzar ese propósito. Una primera aproximación a las distintas barreras utilizadas (cada una de ellas múltiple), de fuera adentro podría ser:


Además debe estar previsto qué hacer en caso de que todos o varios de esos niveles fallaran por cualquier circunstancia. Todos, los trabajadores u otras personas que vivan en las cercanías, deben poseer la información y formación necesaria. Deben existir planes de emergencia que estén plenamente operativos. Para ello es necesario que sean periódicamente probados mediante simulacros. Cada central nuclear posee dos planes de emergencia: uno interior y uno exterior, comprendiendo el plan de emergencia exterior, entre otras medidas, planes de evacuación de la población cercana por si todo lo demás fallara.

Aunque los niveles de seguridad de los reactores de tercera generación han aumentado considerablemente con respecto a las generaciones anteriores, no es esperable que varíe la estrategia de defensa en profundidad. Por su parte, los diseños de los futuros reactores de cuarta generación se están centrando en que todas las barreras de seguridad sean infalibles, basándose tanto como sea posible en sistemas pasivos y minimizando los activos. Del mismo modo, probablemente la estrategia seguida será la de defensa en profundidad.

Cuando una parte de cualquiera de esos niveles, compuestos a su vez por múltiples sistemas y barreras, falla (por defecto de fabricación, desgaste, o cualquier otro motivo), se produce un aviso a los controladores que a su vez se lo comunican a los inspectores residentes en la central nuclear. Si los inspectores consideran que el fallo puede comprometer el nivel de seguridad en cuestión elevan el aviso al organismo regulador (en España el CSN). A estos avisos se les denomina "sucesos notificables". En algunos casos, cuando el fallo puede hacer que algún parámetro de funcionamiento de la central supere las Especificaciones Técnicas de Funcionamiento (ETF) definidas en el diseño de la central (con unos márgenes de seguridad), se produce un paro automático de la reacción en cadena llamado SCRAM. En otros casos la reparación de esa parte en cuestión (una válvula, un aspersor, una compuerta...) puede llevarse a cabo sin detener el funcionamiento de la central.

Si cualquiera de las barreras falla aumenta la probabilidad de que suceda un accidente. Si varias barreras fallan en cualquiera de los niveles, puede finalmente producirse la ruptura de ese nivel. Si varios de los niveles fallan puede producirse un accidente, que puede alcanzar diferentes grados de gravedad. Esos grados de gravedad se organizaron en la Escala Internacional de Accidentes Nucleares (INES) por el OIEA y la AEN, iniciándose la escala en el 0 (sin significación para la seguridad) y acabando en el 7 (accidente grave). El incidente (denominados así cuando se encuentran en grado 3 o inferiores) más grave ocurrido en España fue el de Vandellós I en 1989, catalogado a posteriori (no existía ese año la escala en España) como de grado 3 (incidente importante).

La ruptura de varias de estas barreras (no existía independencia con el gobierno, el diseño del reactor era de reactividad positiva, la planta no poseía edificio de contención, no existían planes de emergencia, etc.) causó el accidente nuclear más grave ocurrido: el accidente de Chernóbil, de nivel 7 en la Escala Internacional de Accidentes Nucleares (INES).

Al igual que la fisión, tras su uso exclusivamente militar, se propuso el uso de esta energía en aplicaciones civiles. En particular, los grandes proyectos de investigación se han encaminado hacia el desarrollo de reactores de fusión para la producción de electricidad. El primer diseño de reactor nuclear se patentó en 1946, aunque hasta 1955 no se definieron las condiciones mínimas que debía alcanzar el combustible (isótopos ligeros, habitualmente de hidrógeno), denominadas "criterios de Lawson", para conseguir una reacción de fusión continuada. Esas condiciones se alcanzaron por primera vez de forma cuasiestacionaria el año 1968.

La fusión se plantea como una opción más eficiente (en términos de energía producida por masa de combustible utilizada) segura y limpia que la fisión, útil para el largo plazo. Sin embargo faltan aún años para poder ser utilizada de forma comercial (la fusión no será comercial al menos hasta el año 2050). La principal dificultad encontrada, entre otras muchas de diseño y materiales, consiste en la forma de confinar la materia en estado de plasma hasta alcanzar las condiciones impuestas por los criterios de Lawson, ya que no hay materiales capaces de soportar las temperaturas impuestas.

Se han diseñado dos alternativas para alcanzar los criterios de Lawson, que son el confinamiento magnético y el confinamiento inercial.

Aunque ya se llevan a cabo reacciones de fusión de forma controlada en los distintos laboratorios, en estos momentos los proyectos se encuentran en el estudio de viabilidad técnica en centrales de producción eléctrica como el ITER o el NIF. El proyecto ITER, en el que participan entre otros Japón y la Unión Europea, pretende construir una central experimental de fusión y comprobar su viabilidad técnica. El proyecto NIF, en una fase más avanzada que ITER, pretende lo mismo en Estados Unidos usando el confinamiento inercial.

Una vez demostrada la viabilidad de conseguir un reactor de fusión que sea capaz de funcionar de forma continuada durante largos períodos, se construirán prototipos encaminados a la demostración de su viabilidad económica.
Existen dos grandes grupos, separados por el método empleado para alcanzar las condiciones de tiempo, densidad y temperatura necesarias para que pueda alcanzarse la fusión controlada de forma continua:

En el primer caso, en un recipiente donde se ha practicado un vacío elevado, se eleva la temperatura de una mezcla de deuterio-tritio mediante campos electromagnéticos hasta convertirla en plasma.

También mediante campos electromagnéticos se confina el plasma en una región lo más pequeña y alejada de las paredes del recipiente que sea posible, aumentando de forma continua la densidad y la temperatura.

A este tipo de fusión corresponden los diseños del Tokamak, como el futuro ITER, o del Stellarator, como el TJ-II español.

En el segundo caso se hace incidir un haz de fotones o de partículas cargadas (electrones o protones) muy energético e intenso sobre un blanco compuesto por el combustible (deuterio-tritio actualmente). Ese haz puede estar enfocado de forma directa sobre el blanco, o bien de forma indirecta sobre un dispositivo denominado holraum construido con un material de alto Z que genera a su vez un intensísimo campo de rayos X que está enfocado sobre el blanco.

Hasta la década de 1970 no se desarrollaron láseres con las potencias necesarias para conseguir iniciar la reacción.

En la actualidad se investiga en varios centros, pero a nivel nacional. Esto se debe a que el mecanismo empleado produce microexplosiones termonucleares, de forma que tanto el software empleado en cálculos y simulaciones termohidráulicas, como los resultados obtenidos, pueden emplearse directamente en el armamento termonuclear. Por este motivo las instalaciones construidas hasta el momento, además de buscar la aplicación civil mediante generación de electricidad, poseen una importante componente militar ya que permiten, tras la prohibición de ensayos nucleares en superficie, realizar pruebas a escala diminuta (para los parámetros del armamento nuclear).

Aunque existen múltiples diseños tanto con el uso de láseres como de aceleradores de partículas, los proyectos más importantes hasta el momento en el mundo son el NIF de Estados Unidos y el LMJ francés, ambos diseños empleando láseres.
Aunque la misma filosofía empleada en la fisión puede emplearse en los reactores de fusión, se ha planteado esta como una opción no contaminante e intrínsecamente segura. Desde el punto de vista de la seguridad, ya que los reactores diseñados necesitan un aporte exterior de energía y de combustible, si existiera un accidente que produjese el fallo de la máquina la reacción se detendría, con lo que se hace imposible una reacción en cadena descontrolada.

El residuo principal de la reacción de fusión deuterio-tritio sería el helio, que es un gas noble y por tanto no interacciona con nada, incluido el organismo humano. Sin embargo las reacciones nucleares de fusión desprenden neutrones altamente energéticos. Esto implica la producción de materiales radiactivos por activación neutrónica. Además, en un ciclo deuterio-tritio, una parte del propio combustible es también radiactivo (el tritio). Para minimizar los efectos, por tanto:

Para ello se está investigando en el uso de materiales de baja activación, utilizando aleaciones que no son comunes en otras aplicaciones. Este aspecto podría disminuir la cantidad de residuos radiactivos generados, pero además en caso de accidente donde parte de los materiales se fundieran por las altas temperaturas, el inventario radiactivo emitido también sería menor. Además, la estrategia de diseño se centra en conseguir que todos los radioisótopos generados sean de semiperiodo corto (menor de 10 años). Si no se consiguiera, las estrategias a seguir serían idénticas a las estudiadas en el caso de los reactores de fisión.

Hasta los años 1990 no se había planteado realmente este problema, por lo que los materiales válidos para la fusión se pensaba que eran los aceros austeníticos (SS316L y SS316-modTi) y ferríticos/martensíticos (HT-9 y DIN 1.1494/MANET). Las investigaciones se habían centrado en la gestión de residuos, dejando de lado el estudio de los posibles accidentes. A partir de los 90 se plantea que debían contemplarse varios problemas en la optimización de los materiales de baja activación, subrayándose principalmente el aspecto de la seguridad frente accidentes además del clásico de la gestión de los residuos. A partir de los aceros convencionales propuestos para fusión se propusieron versiones de baja activación, resultado de la sustitución de elementos que daban lugar a una radiactividad alta por otros metalúrgicamente equivalentes y de baja actividad inducida.

Las soluciones que se adopten en la fusión inercial o en la magnética en principio no tendrán que ser iguales. Así se han desarrollado aleaciones de vanadio, titanio y cromo que presentan mejores comportamientos en la fusión inercial que en la magnética. Se sabe que los materiales cerámicos tienen mejor comportamiento que los aceros en ambos tipos de fusión.

Un método ampliamente utilizado en aquellas aplicaciones en las que se requiere un aporte eléctrico de baja corriente, con una larga duración, es el uso de Unidades de calor mediante radioisótopos (RHU por sus siglas en inglés) acoplados a una serie de termopares que proporcionan una corriente eléctrica, los llamados generadores termoeléctricos de radioisótopos.

En este caso se aprovecha la radiactividad emitida por los núcleos de algunos isótopos. Los isótopos considerados más interesantes para este tipo de aplicación son aquellos que emiten partículas alfa (como por ejemplo el Am o el Po), ya que se reaprovechan más eficientemente las radiaciones emitidas, y es más sencillo su manejo. Sin embargo también se han utilizado emisores beta, como el Sr.

Estos generadores suelen poseer duraciones de varias décadas, y son extremadamente útiles en aplicaciones en las que otras soluciones no sirven. Por ejemplo, en zonas donde es difícil el mantenimiento o sustitución de las baterías y además no existe suficiente luz solar o viento. Se han utilizado en faros cercanos al polo norte en la antigua Unión Soviética y se utilizan frecuentemente en sondas espaciales. Una de sus aplicaciones más curiosas puede ser su uso en marcapasos.

En algunas sondas espaciales que deben permanecer a muy baja temperatura se utiliza simplemente su capacidad de generar calor, obviando la posibilidad de generación eléctrica.

El 15 de octubre de 1997 se lanzó la misión Cassini-Huygens con destino a Saturno y Titán, en la que se ensambló uno de estos dispositivos.
En estos dispositivos la seguridad se basa en dos sistemas principalmente:

En el caso de los GTR situados en zonas de alta inaccesibilidad, como los utilizados en faros instalados cerca de los polos, se suponía que la propia inaccesibilidad de las zonas aseguraba su integridad. Esto sin embargo no ha impedido que sucedieran varios .

En el caso de los utilizados en satélites espaciales, la seguridad de los materiales radiactivos se asegura al mantener una vigilancia continua en las instalaciones, tanto en la construcción como en el montaje de los satélites. Una vez lanzados al espacio, evidentemente se hace imposible su mal uso. Sin embargo, en algunas ocasiones se han usado GTR en satélites en órbita alrededor de la Tierra. Cuando esa órbita se hace inestable es posible que el satélite caiga de nuevo, fundiéndose en su mayor parte en la reentrada. Este, junto a un posible accidente en el lanzamiento son los principales problemas de seguridad en este caso. En total se han producido 6 accidentes conocidos de este tipo (el último en 1996 en una sonda rusa). Para evitar la dispersión del material radiactivo que contienen se fabrican en materiales cerámicos (insolubles y resistentes al calor), rodeado de una capa de iridio, otra de bloques de grafito de alta resistencia y un gel que le da resistencia ante una posible reentrada en la atmósfera.

Para los GTR utilizados como marcapasos el principal problema se encuentra en la pérdida de información acerca de los pacientes en los que se han utilizado, imposibilitando así su debido seguimiento. Por este motivo, existe la posibilidad de que el paciente, tras su fallecimiento, fuera incinerado, incinerando con ello el propio dispositivo y su material radiactivo.

Las fuentes radiactivas de los GTR sobre los que se ha perdido el control (principalmente tras la caída de la URSS) son el principal motivo de preocupación por su posible uso en atentados terroristas (como parte de una bomba sucia), y por este motivo se realizan grandes esfuerzos a nivel internacional por recuperarlas y ponerlas bajo control de nuevo.

En general, cualquier aplicación industrial genera residuos. Todas las formas de generación de energía nuclear también los generan. Tanto los reactores nucleares de fisión o fusión (cuando entren en funcionamiento) como los GTR generan residuos convencionales que son trasladados a vertederos o instalaciones de reciclaje, residuos tóxicos convencionales (pilas, líquido refrigerante de los transformadores, etc.) y residuos radiactivos. El tratamiento de todos ellos, con excepción hecha de los residuos radiactivos, es idéntico al que se da a los residuos del mismo tipo generado en otros lugares (instalaciones industriales, ciudades...).

Es diferente el tratamiento que se emplea en los residuos radiactivos. Para ellos se desarrolló una regulación específica, gestionándose de formas diferentes en función del tipo de radiactividad que emiten y del semiperiodo que poseen. Esta regulación engloba todos los residuos radiactivos, ya procedan de instalaciones de generación de electricidad, de instalaciones industriales o de centros médicos.

Se han desarrollado diferentes estrategias para tratar los distintos residuos que proceden de las instalaciones o dispositivos generadores de energía nuclear:


Para gestionar los residuos radiactivos suele existir en cada país un organismo creado exclusivamente para ello. En España se creó la Empresa Nacional de Residuos Radiactivos, que gestiona los residuos radiactivos de todo tipo generados tanto en las centrales nucleares como en el resto de instalaciones nucleares o radiactivas.

La regulación nuclear puede separarse en cuatro grandes grupos:

Las bases científicas de toda la regulación internacional existente se fundan en estudios propios y recopilaciones llevadas a cabo por la CIPR, UNSCEAR o el NAS/BEIR americano. Además de estos, existen una serie de agencias de investigación y desarrollo en seguridad, como pueden ser la AEN o el EPRI. A partir de todas ellas, existen dos organismos internacionales que desarrollan las bases para la legislación: el OIEA (a nivel internacional) y EURATOM (en Europa).

También existen algunos organismos nacionales, que emiten documentación dedicada a cada uno de los campos, que sirven de guía a otros países. Así ocurre por ejemplo con la NCRP, la NRC o la EPA americanas, la HPA inglesa (antiguamente NRPB) o el CEA francés.

Además de estas regulaciones específicas, existen otras leyes y acuerdos que tienen en mayor o menor medida relación con la energía nuclear. Así por ejemplo las leyes de calidad del agua o la convención OSPAR. Aunque en el Protocolo de Kyoto, que trata sobre las industrias que emiten gases de efecto invernadero, no se menciona la energía nuclear, sí aparece en otros documentos referentes al calentamiento global antropogénico. Así, en los acuerdos de Bonn de 2001, se establecieron los mecanismos de compraventa de emisiones de gases de efecto invernadero y los mecanismos de intercambio de tecnologías, excluyendo ambos explícitamente a la energía nuclear. De este modo, no se pueden reducir las cuotas de emisión de los países altamente industrializados mediante la venta de tecnología nuclear a países menos desarrollados, ni se pueden vender las cuotas de emisiones a países que funden sus bajas emisiones en la energía nuclear. El IPCC, sin embargo, sí recomienda en su cuarto informe el uso de la energía nuclear como una de las únicas formas (junto a las energías renovables y la eficiencia energética) de reducir la emisión de gases de efecto invernadero.

En 1965 se construyó la primera central nuclear en España, la José Cabrera-Zorita, conectándose a la red eléctrica en 1968. Actualmente se encuentran en funcionamiento cinco centrales nucleares en España: Almaraz I y II, Ascó I y II, Cofrentes, Vandellós II y Trillo.

Se encuentran desmanteladas, en proceso de desmantelamiento o en parada definitiva por accidente, fin de su vida útil o expiración de licencia: Vandellós I, José Cabrera y Santa María de Garoña.

Se paralizaron las obras o se abandonaron los proyectos por la llamada moratoria nuclear de las centrales de Lemóniz, I y II, Valdecaballeros I y II, Trillo II, Escatrón I y II, Santillán, Regodola y Sayago.

El porcentaje de energía eléctrica producida en España es muy dependiente de la producción hidroeléctrica anual, la cual depende fuertemente de la pluviometría. Así, en el año 2002 un tercio, el 33,9 % de la energía eléctrica producida en España lo fue en nucleares con un total de 63 016 GWh., mientras que en el año 2009, el porcentaje fue del 19 %, en el año 2016 fue del 22.9 % y en el año 2017 del 21.5 %, siendo en porcentaje en estos dos años la tecnología que más electricidad produjo en España.

La Comisión Nacional de Energía Atómica Argentina (CNEA) es la empresa gubernamental encargada de regular la actividad nuclear en el país. Se creó en 1950 y dio lugar a una serie de actividades centradas en la investigación y desarrollo de la energía nuclear, incluyendo la construcción de varios reactores nucleares de investigación. Actualmente están operando cinco reactores de investigación con la previsión de construir un sexto reactor. 

En 1964 Argentina empezó a interesarse plenamente en la energía nuclear y realizó un estudio de viabilidad para construir una planta en la región de Buenos Aires de 300 a 500 MW. La política del país se basaba firmemente por el uso de reactores nucleares de agua pesada utilizando uranio natural como combustible. Las ofertas más atractivas y que finalmente se aceptaron fueron las de Canadá y Alemania. Como resultado se construyó la central nuclear Atucha en Lima, partido de Zárate, a 115 km al noroeste de Buenos Aires. Luego de unos años, se construyó Atucha II y la Central Nuclear Embalse.

El Estado de Japón comenzó su actividad con reactores nucleares en la década de 1970, y a fecha de 2014 cuenta con 54 reactores nucleares en total. La energía nuclear llegó a producir el 30 % de la electricidad en el país. Sin embargo, en el año 2011 el accidente de Fukushima y sus consecuencias llevaron al gobierno japonés a cancelar el plan nuclear previsto, que proyectaba doblar el número de reactores en el país, y a tomar la decisión prescindir de la energía nuclear. En 2014 se mantenían activos dos reactores en todo el país, que el gobierno consideraba seguros, mientras que en 2017 se mantienen operativos cinco reactores.

Las centrales nucleares generan aproximadamente un tercio de la energía eléctrica que se produce en la Unión Europea, evitando así la emisión a la atmósfera de 700 millones de toneladas de dióxido de carbono por año y del resto de emisiones contaminantes asociadas con el uso de combustibles fósiles.

Por otra parte, la aplicación de la tecnología nuclear a la medicina ha tenido importantes aportes: emisiones de radiación para diagnóstico, como los rayos X, y para tratamiento del cáncer como la radioterapia; radiofármacos, que principalmente consiste en la introducción de sustancias al cuerpo, que pueden ser monitoreadas desde el exterior. En la alimentación ha permitido, por medio de las radiaciones ionizantes, la conservación de alimentos. También se ha logrado un aumento en la recolección de alimentos, ya que se ha combatido plagas, que creaban pérdidas en las cosechas.

En la agricultura, se pueden mencionar las técnicas radioisotópicas y de radiaciones, las cuales son usadas para crear productos con modificación genética, como dar mayor color a alguna fruta o aumentar su tamaño.









</doc>
<doc id="9857" url="https://es.wikipedia.org/wiki?curid=9857" title="Ingeniería informática">
Ingeniería informática

La ingeniería informática o ingeniería en computación es la rama de la ingeniería que aplica los fundamentos de la ciencia de la computación, la ingeniería electrónica, la ingeniería de software y la ingeniería de telecomunicaciones, para el desarrollo de soluciones integrales de cómputo y comunicaciones, capaces de procesar información de manera automática.

Los conocimientos para ejercer de Ingeniero en computación engloban un extenso número de áreas teóricas dentro de la Ciencia Aplicada denominada Informática que le confieren las siguientes capacidades profesionales:

La profesión de Ingeniero en computación es una profesión amparada legalmente por estudios universitarios oficiales y por atribuciones de competencia profesional que en la actualidad están escasamente legisladas. Esas atribuciones se engloban genéricamente dentro de las propias de cualquier Ingeniero Técnico.
Este modelo de regulación nacional implica una anomalía jurídica comparativa con otras Ingenierías
por lo que supone de inconcreción legal sobre las competencias con responsabilidad civiles y penales de los ingenieros dedicados al tratamiento automático de la información. Dado que estos temas no se especifican en la Ley de 1986, hoy hay numerosas lagunas legales para evitar el intrusismo profesional y poder exigir soluciones de calidad con las que aportar garantías a los consumidores y la sociedad de que no existen fallos de producción industrial por errores de capacidad profesional.

La situación sobrevenida es un mal ejemplo político de como debe organizarse una nación para ser tecnológicamente industrializada y competitiva.

El colectivo español de Ingenieros en Informática, junto con todos los colegios y asociaciones de ingenieros en informática han solicitado en innumerables ocasiones la regulación al Ministerio de Industria, Turismo y Comercio, en un intento de poner fin al agravio comparativo existente con el resto de ingenierías, conscientes del error político para el progreso del país. El 6 de noviembre de 2007 se produjo una concentración de Ingenieros en Informática en Madrid. En ella, alrededor de mil profesionales y estudiantes estuvieron informando a los ciudadanos sobre la situación de desamparo legal que vive la profesión.

Posteriormente, el 19 de noviembre de 2008 salieron a la calle en 35 ciudades de toda España más de 45.000 estudiantes y titulados en Ingeniería e Ingeniería Técnica en Informática para reclamar la regulación de la profesión, así como un trato igualitario respecto a las ingenierías reguladas de cara al proceso de adaptación al nuevo Espacio Europeo de Educación Superior (comúnmente conocido como Proceso de Bolonia, así como en la polémica y compleja transposición de la Directiva Europea de Servicios. Estas movilizaciones fueron seguidas de otras de menor entidad en diversas fechas como el 20 de diciembre en Madrid, o el 27 de marzo de 2009 en La Coruña y Sevilla.

Como primer paso, gracias a estas movilizaciones, se ha conseguido la creación tanto del Consejo de Colegios de Ingeniería Informáticacomo del Consejo de Colegios de Ingeniería Técnica en Informática, reclamación que llevaba siendo congelada e ignorada durante años por la Administración.

En España, la titulación universitaria que da acceso a la profesión de Ingeniero/a en Informática es el Máster Universitario Oficial de Ingeniería en Informática, por otro lado, el título de Grado Universitario en Ingeniería en Informática da acceso a la profesión de Ingeniero/a Técnico/a en Ingeniería Informática.



</doc>
<doc id="9858" url="https://es.wikipedia.org/wiki?curid=9858" title="Orbital">
Orbital

El término orbital puede hacer referencia:



</doc>
<doc id="9859" url="https://es.wikipedia.org/wiki?curid=9859" title="Carlos &quot;Famoso&quot; Hernández">
Carlos &quot;Famoso&quot; Hernández

Carlos Hernández (nacido el 23 de enero de 1971 en Los Ángeles, California) es un boxeador estadounidense de padres salvadoreños. Hizo historia en el boxeo haciéndose del título de Campeón superpluma de la Federación Internacional de Boxeo, venciendo a David Santos por el cinturón vacante de esta organización. El "Famoso" Hernández entonces se convirtió en el primer campeón del mundo de El Salvador en la historia. Hernández tuvo el apoyo en la pelea por el campeonato de Alexis Argüello, Roberto Durán y el Presidente salvadoreño, quienes estaban en ring-side aclamándolo.

Hernández fue muy activo en los deportes desde niño, destacando en natación, fútbol, béisbol y karate. A la edad de 17 años conquista la Medalla de Oro de los Juegos Nacionales Estadounidenses de 1988, celebrados en el Cesar's Palace de Las Vegas, Nevada.

El sobrenombre de "Famoso" lo recibió de niño durante sus visitas a El Salvador, cuando su acento estadounidense y sus peleas con vecinitos llamaban la atención de todos. Fue su abuelo paterno, José Hernández, quien lo bautizó como "Famoso", y desde entonces conserva el sobrenombre.

Hernández comenzó su carrera profesional en 1991 y tiene un registro de 42 triunfos, 7 derrotas y un empate, con 24 ko's. En anteriores intentos por el título mundial, "Famoso" perdió por decisión en 12 asaltos frente a Genaro Chicanito Herandez y a Floyd Mayweather, Jr..
En febrero de 2003 derrotó a Santos en Las Vegas para convertirse en el primer salvadoreño en campeón del mundo en las 130 libras. Ese mismo año tuvo la primera defensa exitosa ante Steve Forbes. En 2004, en una pelea unificatoria por el título del CMB perdió ante el mexicano Érik Morales. El 28 de septiembre de 2006 anunció su retiro de la vida pugilistica.



</doc>
<doc id="9861" url="https://es.wikipedia.org/wiki?curid=9861" title="San Salvador">
San Salvador

San Salvador (con abreviatura “S.S” o llamado también El Gran San Salvador debido a su densa población urbana en la región centroamericana) es la capital y la urbe más grande de la República de El Salvador y una de las . Ubicada en uno de los 14 departamentos de la nación, específicamente en el departamento de San Salvador, alberga las sedes del gobierno, de los tres poderes del estado, las principales empresas de economía, desarrollo, telecomunicaciones, embajadas, hoteles de primera categoría, Centro Internacional de Ferias y Convenciones (CIFCO), así como la residencia oficial del Presidente de la República, Casa Presidencial de El Salvador, los principales museos del país, zonas residenciales, económicas y comerciales, monumentos y las principales sedes de servicios económicos, políticos, de moda, arte y desarrollo de El Salvador. 

San Salvador es el núcleo urbano más grande del país, desde el punto de vista: Económico, Demográfico, Social, Cultural, Histórico, Industrial y Político. Considerada la segunda urbe más densamente poblada y con el desarrollo económico más acelerado de la región Centroamericana (después de Ciudad de Guatemala y excluyendo a Ciudad de Panamá) convirtiéndose una ciudad importante del Triángulo Norte de Centro América y uno de los principales asentamientos de las empresas nacionales e internacionales y servicios financieros y sociales del país.

La primera fundación de San Salvador tuvo lugar en 1525 (probablemente en Antiguo Cuscatlán), se refundó en 1528 (al sur de Suchitoto), pero su población estaría asentada en su emplazamiento actual hasta en 1545. Fue un importante centro comercial durante la colonización española, debido a la agricultura del añil, y se convirtió en sede de la Alcaldía Mayor, Intendencia y la Provincia del territorio que, en su mayor parte, hoy conforma El Salvador. 

En San Salvador ocurrieron los primeros dos alzamientos del istmo en contra de las autoridades de la corona española alrededor de 1811. Más tarde, sostuvo su autonomía durante la anexión al Primer Imperio Mexicano, al que se anexó por breve tiempo por la fuerza. Después de que la Ciudad de Guatemala fungiera como capital de la República Federal de Centroamérica, entre 1824 y 1834 (además de un breve período en el que lo fue Sonsonate), San Salvador tomó el relevo entre el período 1834 y 1840, para lo cual se creó un Distrito Federal que incluía a San Salvador y a varios municipios de la periferia.

Ya con el desarrollo de la agricultura del café, la ciudad tuvo un notable desarrollo en su infraestructura desde finales del siglo XIX. Sin embargo, debido a que se encuentra en una zona altamente sísmica, ha sufrido el azote de terremotos a lo largo de su historia. Restan solo fotografías, retratos o planos de sus antiguas edificaciones coloniales, como su antigua catedral y acueducto. El terremoto de 1872, por ejemplo, provocó que San Salvador fuera reconstruido con edificaciones de madera o de hierro importadas enteramente desde Bélgica, como las iglesias de San Francisco y Sagrado Corazón, además del Hospital Rosales, entre otras. 

San Salvador es una de las primeras ciudades de fundación española en la Centroamérica histórica, la región que durante la colonización hispánica de América constituyó el Reino o Capitanía General de Guatemala. También resulta ser la capital más antigua y duradera en la región antedicha, en el sentido que desde su traslado en 1545 al Valle de las Hamacas, ha permanecido ahí desde ese año hasta nuestros días, y, también, debido a que desde que la Monarquía Española la designó como sede administrativa de la provincia sansalvadoreña en 1540, así permaneció hasta llegar a establecerse como capital del Estado salvadoreño. Pese a los terremotos, inundaciones y erupciones que esta población tuvo que soportar desde hace siglos, siempre fue reconstruida en su sitio original. Y, pese a algunos intentos de traslado en áreas menos sísmicas, San Salvador se ha establecido como la principal ciudad de El Salvador.

La ciudad es también la sede de la Arquidiócesis de San Salvador, así como muchas ramas protestantes del cristianismo, incluyendo evangélicos. El Sistema de la Integración Centroamericana (SICA) tiene su sede en San Salvador. En el ámbito deportivo, ha albergado a los Juegos Centroamericanos y del Caribe en 1935 y 2002, y los Juegos Deportivos Centroamericanos en 1977 y 1994.

Con una población de 621 123 habitantes, esta ciudad es la principal urbe del Área Metropolitana de San Salvador, una conurbación de alrededor de 2 177 432 habitantes para el año 2015. Cabe señalar que, por ello, esta aglomeración urbana es la segunda más grande de Centroamérica solamente superada por el AMG de la Ciudad de Guatemala, y la sexta en toda la región de América Central y el Caribe.

En 1457, el papa Calixto III instituyó la festividad católica del Santísimo Salvador cada 6 de agosto, en acción de gracias por la victoria de los ejércitos cristianos sobre las fuerzas de Mehmed II en 1456 durante el sitio de Belgrado. De esta manera, muchas poblaciones y lugares fueron designadas con el nombre de San Salvador y varias iglesias puestas bajo la advocación del Divino Salvador del Mundo. Fue así que Cristóbal Colón bautizó a la isla de Guanahaní con tal denominación. Por su parte, Pedro de Alvarado, al organizar una segunda expedición sobre el territorio de Cuscatlán, ordenó a su hermano Gonzalo de Alvarado que le diera el nombre de San Salvador a la villa que allí se fundase, algo que ocurrió, probablemente, el 1 de abril de 1525.

Después de las victorias sobre las huestes pipiles en las batallas de Acajutla y Tacuzcalco, el conquistador Pedro de Alvarado intentó someter a los nativos de la capital del Señorío de Cuzcatlán a su arribo el 18 de junio de 1524. Los cuscatlecos, sin embargo, huyeron a las montañas vecinas y el extremeño tuvo que replegarse hacia la zona de la actual Guatemala. La primera villa de San Salvador se fundó a menos de un año de esta expedición, por una misión no documentada al mando de Gonzalo de Alvarado.La primera mención que existe acerca de este asentamiento es una carta del mismo Pedro de Alvarado en Guatemala el 6 de mayo de 1525 haciendo notar que no se podía celebrar un cabildo por la ausencia de Diego de Holguín quien había partido a tomar el puesto de Alcalde ordinario de la villa de San Salvador.

Debido a las frecuentes rebeliones en el sitio por parte de los nativos, se estableció una nueva villa en el lugar conocido actualmente como Ciudad Vieja, al sur de la actual localidad de Suchitoto (1 de abril de 1528). Su trazado original tardó quince días y llegó a ser poblada por un número de 50 a 60 viviendas, teniendo por alcaldes a Antonio de Salazar y Juan de Aguilar.

Después de la pacificación de la región, la pequeña localidad fue abandonada poco a poco y el nuevo asentamiento se ubicó, el año de 1545, en el valle de Zalcoatitán renombrado como “Valle de las Hamacas”. Se estima que tal acampamiento estuvo en la llamada cuesta del Palo Verde, y que fue conocida como “la Aldea”. Al norte de ese emplazamiento se comenzó a trazar la Plaza Mayor, donde se ubica actualmente la Plaza Libertad; al Este, se erigió la Iglesia consagrada al Santísimo Salvador del Mundo. El 27 de septiembre de 1546, mediante trámites de los procuradores Alonso de Oliveros y Hernán Méndez de Sotomayor, y por petición del Secretario de la Real Corona Juan de Samano ante el infante don Felipe por la ausencia del emperador Carlos V de Alemania y I de España, se elevó la villa a la categoría de ciudad por Real Provisión.

Durante la época colonial era la ciudad más importante de la Alcaldía Mayor de San Salvador, a su vez parte del Reino de Guatemala. En el siglo XVII, la actividad principal de esta región fue el añil, para su exportación a Europa. En la segunda mitad del siglo XVIII, debido a las Reformas borbónicas, que tenían como objetivo mejorar el cobro de impuestos y crear monopolios estatales, se creó la Intendencia de San Salvador en 1785; la propia ciudad fue cabecera de su partido.

San Salvador tuvo un destacado rol en los años previos a la independencia de Centroamérica. Fue allí donde se dio la primera rebelión en 1811 en contra de las autoridades de la Capitanía, y una más en 1814, ambas sin éxito, movimiento liderado por el sacerdote José Matias Delgado y Manuel José Arce Fagoaga quienes continuaron hasta la independencia de España en 1821. Con la declaración del Plan de Iguala por Agustín de Iturbide, las entonces provincias de la Capitanía declararon su emancipación el 15 de septiembre de 1821. Las noticias de este suceso llegaron a San Salvador el 21 de septiembre.

A partir de entonces las antiguas intendencias fueron gobernadas independientemente. De hecho en San Salvador regía un Jefe Político. Pero, en definitiva, todas estaban aún bajo la dirección de una Junta Provisional Consultiva con sede en Guatemala. Meses después llegó desde México una invitación de Iturbide para que las provincias se unieran al nuevo Imperio. La Junta decidió su anexión el 5 de enero de 1822; solo dos ayuntamientos, de los 170 que conformaban la región, se negaron: San Salvador y San Vicente.En los siguientes dos años la ciudad enfrentó dos invasiones desde Guatemala, que las fuerzas defensoras pudieron repeler. Sin embargo, en febrero de 1823, Vicente Filísola asedió a la ciudad y la ocupó; su estadía acabó debido al retiro del poder de Iturbide.

Con el nacimiento de la República Federal de Centroamérica en 1824, la ciudad se vio envuelta en los turbulentos años que enfrentaron a liberales y conservadores. Para separarse de la influencia de poder que ejercía la ciudad de Guatemala, el Congreso Federal decidió trasladar la capital de la federación a San Salvador en junio de 1834, y creó allí un Distrito Federal el 20 de abril de 1835. La situación caótica de la región creó un estado de pobreza general. Tal condición provocó, además, una epidemia de cólera en 1836.

La economía del país cambió gradualmente en la segunda mitad del siglo XIX, pues el añil fue sustituido por el cultivo de café como principal producto de exportación. La ciudad tuvo un cambio en sus edificios principales, cuyos diseños tuvieron influencias europeas. Para el caso, durante la administración del General Gerardo Barrios fue construido un Palacio Nacional con características Neoclásicas;lo mismo que la Universidad de El Salvador, en 1870, con notables influencias francesas. Por otro lado, un hecho destacado fue la llegada del telégrafo en 1879. Un viajero de apellido Sherzer describió a la población en esos años: 

Un diplomático estadounidense, por su parte, en un libro llamado “Notas sobre Centroamérica” estimaba la población de esta localidad en 25.000 personas en 1852.A pesar de todo, cualquier avance en la infraestructura tenía como problema los habituales terremotos que han asolado la región. Por ejemplo, el terremoto del 16 de abril de 1854 causó tanta destrucción en la capital, que las autoridades se trasladaron a Cojutepeque por cuatro años; y Santa Tecla surgió para hacer de ella una potencial capital. Pero en 1858 las autoridades regresaron a San Salvador y por Decreto Legislativo del 27 de enero de 1859, publicado en la Gaceta del ("Sic") Salvador No. 75, Tomo No. 7, del 2 de febrero de 1859, se declaró que la antigua ciudad de San Salvador volvía a ser la capital del país.Hubo otros acontecimientos telúricos en 1873 y 1880.

A inicios del siglo XX, la capital, como el país en general, se desarrolló en función de los ingresos generados por la exportación del café. Como expresión de esa prosperidad fueron iniciadas diversas estructuras reconocidas en la urbe, tales como el Parque Dueñas, después conocido como Plaza Libertad (1900); el Teatro Nacional de San Salvador; la ex Casa Presidencial (1911) y el Hospital Rosales (inaugurado en 1902). Todas construidas en aras de hacer la localidad más cosmopolita. Sin embargo, nuevamente los desastres naturales destrozaron muchos edificios de esos años con los terremotos de 1917 y 1919; y una inundación en 1922.

Por ser la sede del gobierno, en San Salvador ocurrieron importantes acontecimientos políticos a lo largo del siglo. Justo en los primeros años acaeció el magnicidio de Manuel Enrique Araujo el 4 de febrero de 1913 en la Plaza Barrios. Años más tarde arribaría al poder Maximiliano Hernández Martínez a través de un golpe de estado en 1931, en medio de la crisis económica que resultó en la caída de los precios del café durante los años de la Gran Depresión. A partir de entonces iniciaría una época de agitación política con el predominio del estamento militar. En la década de los años 1970, con el aumento de la violencia política, San Salvador fue el escenario de numerosas protestas populares de diversas organizaciones opositoras al régimen; la mayor de ellas, registrada en la historia del país, ocurrió el 22 de enero de 1980.

Con el advenimiento de la guerra civil, a pesar de no ser teatro de operaciones militares, la situación en las calles de la capital era de zozobra. Desapariciones forzadas, bombas y paros de transporte público eran habituales. En esos años, un acontecimiento en especial asoló el área metropolitana: el terremoto de 1986, el cual, además de cobrarse alrededor de 3,500 vidas, destruyó muchos de los edificios del "Centro Histórico". Otro de los momentos más críticos ocurrió con la denominada "ofensiva final" del 11 de noviembre de 1989. La firma de los Acuerdos de Paz de Chapultepec, el 16 de enero de 1992, terminó con la guerra civil. En esos días, en el centro histórico se desarrollaron diversas muestras de júbilo, especialmente el 1 de febrero con el cese oficial de las acciones militares. A pesar de este logro histórico que puso fin a la violencia política, nuevos retos aparecieron con la progresión de la violencia delincuencial.

En la década de los años 1990 hubo un auge económico con la implantación de un modelo neoliberal en el país. La ciudad fue más activa económicamente con la llegada de franquicias internacionales, la creación de nuevos pasos vehiculares a desnivel, nuevos centros comerciales, y zonas de esparcimiento. En la primera década del siglo XXI, el Gobierno ha construido nuevas carreteras en las afueras del municipio para descongestionar el paso por la ciudad. Por otro lado, el desempleo en los últimos años ha provocado más problemas sociales, entre ellos el aumento del comercio informal.

San Salvador está ubicado en la zona central del país. Su elevación se encuentra entre 600 y 1000 metros sobre el nivel del mar. Aunque la propia ciudad se encuentra a 650 msnm.Limita al norte con los municipios de Nejapa, Mejicanos, Cuscatancingo, y Ciudad Delgado, al este con Soyapango y San Marcos, al sur con Panchimalco y también con San Marcos, y al oeste con Antiguo Cuscatlán y Santa Tecla.

La geografía de la capital salvadoreña está dominada por dos grandes prominencias orográficas, las cuales hacen inconfundible el paisaje de la capital, ya que desde cualquier punto de la ciudad estas pueden observarse: el Volcán de San Salvador y el Cerro San Jacinto. El primero se compone de dos grandes formaciones: El Picacho, con una altura de 1959.97 msnm; y la otra, El Boquerón, la formación más voluptuosa, de 1839.39 msnm, la cual posee un enorme cráter de 1.5 km de diámetro. Este volcán es representativo de la ciudad, al punto que básicamente forma su "skyline" (horizonte), palideciendo a cualquier otra estructura construida por mano humana en la urbe, quizá a eso se deba que pese a cierto despegue inmobiliario que ha experimentado la localidad desde finales de la década de 1990, ninguna estructura ha llegado a convertirse en icono de la misma, exceptuando los edificios históricos. La segunda estructura orográfica más representativa de este municipio es el Cerro San Jacinto, la cual posee una altura de 1100 msm y que hasta recientemente poseía en su cúspide un teleférico. El mencionado Volcán de San Salvador se ubica al oeste de la ciudad, mientras que el Cerro San Jacinto en su extremo Sureste. Estas dos atalayas naturales enmarcan el valle en el que se asienta San Salvador.

El valle en el que se ubica San Salvador lleva por nombre "de Las Hamacas" o de "Zalcoatitán". El primero fue puesto por los conquistadores ibéricos debido a su naturaleza sísmica. Y es que este valle es atravesado por muchas fallas tectónicas, razón por la que San Salvador ha sido destruida y reconstruida más de una veintena de veces desde que se trasladó a este territorio en 1545. El valle es muy estrecho comparado con otros valles salvadoreños y se encuentra lejos de ser plano, ya que desde el Oeste, donde se encuentra el Volcán de San Salvador, alcanza una altura de más de 1000 msnm para luego ir descendiendo poco a poco hasta encontrarse con el Cerro San Jacinto y las riveras del Río Acelhuate, alcanzando acá una escasa altura de 590 msnm, resultando así a la vista de cualquier espectador que este valle es solo una prolongada extensión de las laderas del Volcán de San Salvador. Además de esto, el valle no es siempre continuo, ya que este es atravesado por muchos arenales y ríos, tales como el Arenal La Mascota, el Arenal Monserrat, el Arenal Tutunichapa, entre otros, todos discurriendo de Oeste a Este, siguiendo la depresión misma de la llanura hasta desembocar en el río Acelhuate.

Por el Sur, el valle y municipio de San Salvador es flanqueado por la Cordillera del Bálsamo, separándolo esta de la llanura costera. Hacia el Norte, el paso del Valle de Las Hamacas es bloqueado por unas suaves cerranías correspondientes a los municipios de Ayutuxtepeque y ciudad Delgado, que separan a San Salvador de otro largo valle que se extiende hasta la cuenca del Río Lempa en el Norte del país.

San Salvador tiene un clima cálido todo el año, siendo diciembre, enero y febrero los más frescos. Las temperaturas se mantienen entre los 18 y 33 grados centígrados. Los más calurosos son de marzo a mayo. De mayo a octubre es la estación lluviosa, usualmente se producen inundaciones (sobre todo en los ríos).

La temperatura varía entre el mediodía y la medianoche, debido a cambios en los niveles de humedad. La temperatura más alta registrada en San Salvador fue de 38,5 °C, la más baja fue de 4,0 °C.

Entre las elevaciones que tiene el municipio están el Cerro El Picacho, Cerro San Jacinto y Cerro Chantecuán, y también se encuentran las lomas La Torre y de Candelaria.En la zona se encuentran tipos de suelos como regosoles, latosoles y andosoles, y rocas como lava andesítica y lava basáltica.
Los principales ríos del municipio son el río Acelhuate que se encuentra a 2.2 km. de la ciudad y el río Iohuapa a 5.2 km. también se encuentran los ríos Matalapa, El Garroba, San Antonio, Urbina y Casa de Piedra.Entre las quebradas están El Garrobo, Sirimullo, La Quebradona, Los Cojos, Las Lajas, El Manguito, La Lechuza, La Mascota, San Felipe, Tutunichapa y Mejicanos.

En El Salvador, por disposiciones constitucionales, los municipios son autónomos en lo económico, técnico y administrativo (Art. 203). Se rigen por un concejo formado por un alcalde (elegido por voto libre y directo cada tres años, con opción a ser reelegido), un sindico y dos o más regidores cuyo número varía en proporción a la población del municipio. En el caso de esta ciudad, actualmente es gobernada por el alcalde Ernesto Muyshondt, quien forma parte del partido de derecha ARENA , fue elegido para el periodo 2018-2020 bajo la bandera de este partido político. Le acompañan un síndico, doce concejales propietarios y cuatro suplentes; y un secretario. Las funciones y facultades de este gobierno están enmarcadas dentro de la normativa del Código Municipal.
En cuanto a la administración del municipio cabe destacar algunos aspectos organizativos. San Salvador cuenta con entidades descentralizadas (comité de festejos, administración de la finca El Espino, Parque Cuscatlán, administración de cementerios, etc.); para la salvaguarda de los intereses de la comuna dispone de un Cuerpo de Agentes Metropolitanos; la estructura comprende Gerencia de mercados y de Servicios a los ciudadanos, bajo la dirección de una Unidad de género; Gerencias de distritos de la comuna (los cuales son seis, para descentralizar el trabajo municipal), Gerencia de finanzas, y una Gerencia del centro histórico, etc.La comuna, además, forma parte del Concejo de Alcaldes del Área Metropolitana de San Salvador (COAMSS), integrada por catorce concejos municipales que conforman el área denominada Gran San Salvador.

Desde 1964, las personas siguientes fueron alcaldes de San Salvador:

Desde 5 de mayo de 2015 al 1 de mayo de 2018, la ciudad de San Salvador ostentó otro escudo. El estandarte fue revelado por el alcalde Nayib Bukele quien detalló el significado de los elementos, entre los que se destacan el escudo con tres franjas diagonales (dos azules y blanco al centro), rodeado por una corona de laureles. Bajo el mismo se encuentra un listón con el año «1834» que corresponde a la fecha en la que San Salvador fue capital de la República Federal de Centroamérica. También se encuentran dos espadas que convergen en el blasón que simbolizan pertenecer a Francisco Morazán y Gerardo Barrios, las que además penetran dos pergaminos con las palabras «Ciudad» y «Capital» respectivamente. Todo el emblema se encuentra rodeado por seis estrellas que representan a los seis distritos de San Salvador. 

El 1 de mayo de 2018, tomando la vara edilicia Ernesto Muyshondt, restituyó el anterior escudo creado por José Mejía Vides.

También existen el himno y la vara edilicia, que fueron adoptados —junto al antiguo escudo— a partir de un concurso promovido en 1943. La bandera fue diseñada por iniciativa de las autoridades. Como himno fue elegido el elaborado por Carlos Bustamante (letra) y Ciriaco de Jesús Alas (música). La vara municipal, por su parte, muestra una serie de figuras y símbolos relativos a la historia local.

San Salvador es la ciudad más poblada del país con un estimado de 281.870 habitantes propios para el año 2014, y 1.740.053 habitantes en el Gran San Salvador o AMSS (Área Metropolitana de San Salvador, estimado 2012).El centro de San Salvador se encuentra a una altura de 658 msnmy su densidad poblacional es 2,067 habitantes por km².

En el centro de la ciudad se encuentra la Catedral Metropolitana del Divino Salvador del Mundo que forma parte de la Arquidiócesis de San Salvador. Las fiestas patronales son en honor al Divino Salvador del Mundo, patrono de la ciudad desde la época colonial.Asimismo, la Virgen de la Presentación, la cual pudo haber sido la primera a quien los antiguos pobladores rindieron culto, es considerada "Protectora Patrona de la Ciudad de San Salvador".También la Virgen del Rosario es patrona de la Arquidiócesis y de la metrópolis.

La mayor parte de la población profesa el catolicismo, pero también hay una cantidad considerable de grupos evangélicos y protestantes como la Asociación Bautista de El Salvador (ABES) y Federación Bautista de El Salvador que cuenta con iglesias locales pequeñas en todo el territorio nacional,las Asambleas de Dios, la Iglesia de Dios, la Iglesia Elim, el Tabernáculo Bíblico Bautista "Amigos de Israel", la Misión Centroamericana y Tabernáculo de Avivamiento Internacional (TAI).Existen también comunidades religiosas judías, sin dejar de mencionar a los Testigos de Jehová, la Iglesia de Jesucristo de los Santos de los Últimos Días y Adventistas del Séptimo Día.

La población de San Salvador es predominantemente católica, con una importante minoría de protestantes. Hay más diversidad de religión que en la mayoría de los países latinoamericanos. La población protestante es en su mayoría evangélica. Una de las iglesias protestantes más grandes de la ciudad es la Iglesia Cristiana Josue (de las Asambleas de Dios), otra es el Tabernáculo Bíblico Bautista, Amigos de Israel (Tabernáculo Bautista Bíblico, Amigos de Israel). También hay una población considerable de miembros de La Iglesia de Jesucristo de los Santos de los Últimos Días , a menudo apodados los mormones. La comunidad mormona en El Salvador construyó recientemente su primer templo.En El Salvador, una estructura de impresionante ingeniería y arquitectura. También hay capillas más pequeñas de los Santos de los Últimos Días en los distritos 1 y 3.

Como en la mayor parte del país, el catolicismo todavía desempeña un papel importante en la celebración de los días festivos, como Las Fiestas Agostinas (Los festivales de agosto) en honor de Jesucristo, el "santo" patrón de El Salvador, conocido como El Salvador del Mundo (El Salvador del Mundo). Estos eventos se están volviendo menos prominentes con un marcado descenso en la población católica durante la última década. San Salvador también alberga a unos 3,500 judíos; la comunidad judía sigue siendo sólida, pero no tanto desde la década de 1980, ya que muchos de ellos se fueron con el inicio de la Guerra Civil Salvadoreña. Muchos judíos habían emigrado a El Salvador durante la Segunda Guerra Mundial debido al trabajo de José Castellanos Contreras , el Cónsul General Diplomático Salvadoreño.en Ginebra, Suiza, quien ayudó a un empresario judío-húngaro llamado Gyorgy Mandl (más tarde adoptó el nombre de George Mantello) a salvar a 40,000 judíos en Europa Central de la persecución nazi al entregarles documentos de nacionalidad salvadoreña. La ciudad tiene una pequeña comunidad de palestinos , en su mayoría descendientes de familias cristianas palestinas que emigraron de Palestina a finales del siglo XIX y principios del XX, con la tasa más alta de inmigración entre 1910 y 1925. La ciudad también tiene iglesias evangélicas coreanas que Mantener servicios en el idioma coreano para una creciente población coreana.

La ciudad al ser la capital, cuenta con numerosos lugares de tipos de producción de alimentos, bebidas y artesanías. También materiales de construcción, industrias farmacéuticas y químicas, así como negocios de mecánica automotriz, y electrodomésticos. Ejemplos son MOLSA (Molinos de El Salvador) e Industrias La Constancia

Asimismo, es sede de importantes centros comerciales que, además de ofrecer productos a los visitantes, son una fuente de empleo para muchas familias salvadoreñas; entre los que se mencionan el centro comercial Metrocentro, Centro Comercial "El Paseo" y Centro Comercial Galerías.

También existen prestigiosos centros de negocios como el Centro Financiero Gigante y el World Trade Center San Salvador, donde se alojan importantes empresas y organizaciones internacionales como: Microsoft, Ericsson, BID, Mitsubishi Corporation, AMCHAM, Embajada de Japón, Banco Agrícola, Organización de Estados Americanos (OEA), Banco Mundial, Embajada de Corea, Agencia de Cooperación Japonesa (JICA) y la Agencia de Cooperación Coreana (KOICA) y La Embajada Británica. 

Por otra parte, en cuanto al servicio de alojamiento en hoteles, para el año 2011 en San Salvador había en funcionamiento cuatro hoteles de lujo, diez de primera clase, y ochenta de clase turista.
Entre los más importantes:

En cuanto a los mercado municipales, destaca el Mercado Central, el más grande de la ciudad que cuenta con diez pabellones; otros son: San Jacinto, Belloso, La Tiendona, Sagrado Corazón, San Miguelito, San Antonio, Tinneti,y Cuscatlán, este último que integra tanto el rubro comercial como la actividad cultural.

En San Salvador hay diversos monumentos y lugares de interés histórico, muchos de los cuales forman parte del Patrimonio cultural declarado de esta nación.Resalta el Monumento al Divino Salvador del Mundo, ubicado en la Plaza Salvador del Mundo, dedicado al Patrono de la ciudad y todo un símbolo nacional. Considerados monumentos nacionales son: el campanario de la Iglesia de La Merced; tumba del escritor Alberto Masferrer; la tumba del Capitán General Gerardo Barrios; Iglesia Nuestra Señora de Candelaria; Teatro Nacional; Palacio Nacional; y el Hospital Rosales, entre otros; como bienes culturales: la Casa de las Academias; ex Casa Presidencial; Parque Venustiano Carranza; Salón Azul del Palacio Nacional, etc.; también entre los lugares históricos destaca la Iglesia El Rosario, construida en el antiguo asiento de la Catedral de la ciudad.
Algunos de estos sitios se encuentran dentro del denominado Centro Histórico de San Salvador, donde se inició el crecimiento de la ciudad desde la época colonial. Esta área ha sido destruida a través del tiempo por desastres naturales y las edificaciones que se mantienen son de finales del siglo XIX e inicios del XX; para el caso, allí se encuentran la Plaza Francisco Morazán, Plaza Barrios y la Plaza Libertad con el Monumento a los Próceres. Al oeste de la capital se encuentra el Monumento a la Revolución, erigido a mediados del siglo XX. Un conjunto interesante es el Cementerio de Los Ilustres, en el Cementerio General de San Salvador, que ostenta diversos mausoleos y estatuas artísticas, pues allí descansan los restos de miembros de familias pudientes y personalidades del acontecer salvadoreño, entre ellos políticos como Francisco Morazán y Gerardo Barrios, o escritores como Salarrué y Claudia Lars. Por otro lado, en memoria de las víctimas de la Guerra Civil Salvadoreña, se encuentra el Monumento a la Memoria y la Verdad en el parque Cuscatlán.

Para el disfrute de la vida nocturna en esta capital existe una variada oferta de restaurantes, discotecas y bares. Para el caso, hay zonas identificadas que concentran estos centros de esparcimiento: la Zona Rosa, al oeste, sobre el Bulevard del Hipódromo; la Zona Real, ubicada en los alrededores del Bulevar de los Héroes; y el Paseo General Escalón. Cabe destacar que algunos de estos contornos están comprendidas dentro de las áreas donde no es permitida el porte de armas de acuerdo a una ley de la república.Los espectáculos musicales multitudinarios se realizan en los grandes recintos deportivos como el Gimnasio Nacional, el Estadio Cuscatlán, o el anfiteatro del Centro Internacional de Ferias y Convenciones. Otra opción es la segunda estructura más alta del país: la Torre Futura, que alberga oficinas y área comercial.

En San Salvador existen temporadas de teatro. El mejor lugar para la presentación de las artes escénicas es el Teatro Nacional. Otros establecimientos incluyen: el Teatro Luis Poma, Teatro Municipal Roque Dalton, el Teatro Presidente (donde también se realizan eventos de danza), y auditorios de instituciones como la Universidad de El Salvador, Universidad Centroamericana "José Simeón Cañas"; o el Centro Cultural de España. También hay exhibiciones de la Orquesta Sinfónica Nacional y Juvenil. Para la muestra de artes plásticas hay sitios como la Sala Nacional de Exposiciones "Salarrué" ubicada en el Parque Cuscatlán, y otras de carácter privado. En cuanto a las salas de cine, la oferta se encuentra en los grandes centros comerciales. Un espacio importante para el desarrollo del arte y la cultura es el Centro Cívico Cultural Legislativo.

Concerniente a las zonas de recreo familiar, la ciudad cuenta con el Parque Cuscatlán, el Zoológico Nacional, Parque Saburo Hirao, Parque Metropolitano El Talapo, y el Parque del Bicentenario.

A principios del mes de agosto se celebran las fiestas patronales dedicadas al Divino Salvador del Mundo. En esta ocasión los eventos principales son los desfiles de carrozas que atraviesan la ciudad, además de una procesión religiosa el 5 de agosto, que culmina en una ceremonia que representa la Transfiguración de Jesús; y una misa principal, el día 6 de agosto, frente a la Catedral Metropolitana.

Por otro lado, la XIV Asamblea de la Unión de Ciudades Capitales Iberoamericanas eligió a San Salvador como Capital Iberoamericana de la Cultura para el año 2011, en vista de la celebración del Bicentenario del Primer Grito de Independencia de Centroamérica.

Al oeste de San Salvador se localizan dos instituciones importantes para las actividades relacionadas con la cultura de El Salvador. El primero de ellos es el Museo Nacional de Antropología Dr. David J. Guzmán (MUNA), una organización dedicada al estudio y preservación de la identidad de este país a través de diversas actividades relacionadas con la arqueología y antropología. El otro es el Museo de Arte de El Salvador (MARTE) que fue inaugurado en 2003 como una institución privada sin fines de lucro que exhibe colecciones de arte nacional y privadas, además de realizar otros programas educativos. Otros sitios en la capital incluyen: para los niños, el Museo Tin Marín, en el Parque Cuscatlán; al sur se encuentran el Museo Militar de la Fuerza Armada de El Salvador en las antiguas instalaciones del cuartel El Zapote, y el Museo de Historia Natural en el Parque Saburo Hirao. También la urbe cuenta con el Museo de la Palabra y la Imagen, Museo de Ciencias Stephen W. Hawkins, Museo de Arte Popular para la exposición de artesanías de Ilobasco, el Museo Universitario de Antropología, Museo del Banco Hipotecario de El Salvador y Museo del Ferrocarril.

En cuanto a bibliotecas de refiere, destacan la Biblioteca Nacional Francisco Gavidia, el Archivo General de la Nación (ubicado en el Palacio Nacional de El Salvador) la Biblioteca de la Asamblea Legislativa y la Biblioteca General de la Fuerza Armada. Asimismo existen estos recintos en instituciones educativas tales como la Universidad Centroamericana "José Simeón Cañas", la Universidad Tecnológica de El Salvador, la Universidad de El Salvador, etc.

De acuerdo a la base de datos del Ministerio de Educación, correspondientes al año 2011, había en el municipio 334 centros escolares, de los cuales 143 eran de carácter público, y 191 de carácter privado.

En cuanto a las instituciones de educación superior, tienen su sede algunas de las más importantes universidades del país, entre ellas:

La ciudad alberga importantes escenarios para la práctica del deporte a nivel nacional e internacional. El principal de ellos es el Estadio Cuscatlán, de propiedad privada y el más grande de Centroamérica. Es la sede de la Selección nacional de fútbol y de los equipos Alianza F.C. y C.D. Atlético Marte de la primera división. Otros son de administración pública, bajo la dirección del Instituto Nacional de los Deportes de El Salvador (organización con asiento en esta urbe), tales como el Estadio Nacional Jorge "Mágico" González (antes llamado de la "Flor Blanca"), que cuenta, además de cancha de fútbol, con pista de atletismo e instalaciones para otros deportes.

También la Universidad de El Salvador cuenta con el Estadio Universitario, sede del C.D. Universidad de El Salvador. Asimismo, se encuentran el Palacio de los Deportes "Carlos El Famoso Hernández", escenario de múltiples disciplinas bajo techo, como lo es también el Gimnasio Nacional “Adolfo Pineda” y el Complejo para Deportes Acuáticos y Velódromo-Patinódromo Nacional (llamado popularmente "el Polvorín").

Otras instalaciones incluyen el Parque nacional de Pelota “Saturnino Bengoa” para la práctica del béisbol; y para el softbol, la Cancha "José Arnoldo Guzmán". El Centro Internacional de Ferias y Convenciones de El Salvador es otra alternativa para alojar disciplinas bajo techo.

El Comité Olímpico de El Salvador tiene su sede en San Salvador, y bajo su dirección se han realizado los más importantes eventos a nivel internacional, siendo ellos los III y XIX Juegos Centroamericanos y del Caribe en 1935 y 2002, respectivamente.

En el Área Metropolitana de San Salvador circulan alrededor de 200.000 vehículos diarios registrados. Hacia el propio municipio de San Salvador, en horas pico de la mañana, se realizan unos 300.000 viajes.Alrededor de la ciudad hay vías primarias que la comunican con el interior del país, siendo estas la Troncal del Norte, que dirige hacia Apopa y Chalatenango, la carretera a Santa Tecla con rumbo al Occidente del país, la carretera al Aeropuerto Internacional de Comalapa, y el Bulevar del Ejército Nacional, que dirige al Oriente. Por ser la ciudad paso obligado si se atraviesa el territorio, el gobierno ha construido, desde inicios del siglo, diversas vías para el descongestionamiento del tráfico vehicular. Entre estas carreteras están el trayecto Troncal del Norte a Soyapango, prolongación Bulevar Constitución,y el Bulevar Monseñor Romero al poniente de la capital.

Otra importante obra culminó en febrero de 2017 con la construcción del «Túnel Masferrer», también al poniente de la ciudad, el cual permite el tráfico fluido de vehículos entre la Avenida Jerusalén y la Avenida Masferrer Norte por debajo del Redondel Alberto Masferrer. Dicha obra incorpora dos «pasarelas inclusivas» que sirve de paso a peatones y ciclistas, y que además dispone de ascensores y gradas metálicas para personas con discapacidad. 

La denominación numérica de calles y avenidas está organizada de acuerdo a los cuatro cuadrados que forman el cruce de la Avenida España y Avenida Cuscatlán (al norte y al sur, respectivamente), con las calles Arce y Delgado (al poniente y oriente) en el centro de la ciudad. Así, las avenidas con números impares Norte y Sur se encuentran al oeste de este cruce y las de números pares al Este. Las calles, por su parte, con denominación impar poniente y oriente, están al norte del cruce; las pares poniente y oriente, al sur. Entre las calles y avenidas principales de la ciudad se encuentran el Paseo General Escalón, el tramo Alameda Manuel Enrique Araujo-Alameda Franklin D. Roosevelt-Calle Rubén Darío, la Alameda Juan Pablo II, el Boulevard de los Héroes, Bulevar Venezuela, Bulevar Los Próceres, Bulevar Constitución, etc.

En cuanto al transporte público, hay una disponibilidad considerable de autobuses y microbuses a un costo de US$0.20 y US$0.35 para los primeros y de US$0.25 para los segundos. En San Salvador se encuentran, asimismo, las Terminales de Occidente y Oriente que son punto de las unidades de autobuses que se dirigen a diversas zonas del país, así como empresas de transporte terrestre internacional que brindan servicio para los demás países de Centroamérica y el sur de México. 

San Salvador forma parte de la ruta del Sistema Integrado de Transporte del Área Metropolitana de San Salvador (Sitramss). Esta obra consiste en la construcción de corredores exclusivos para buses articulados BRT que transportarán pasajeros desde San Martín hasta Santa Tecla. Entró en funcionamiento en el mes de mayo de 2015, de forma parcial. El sistema cuenta con tarjeta prepago, y tiene como beneficios la seguridad, rapidez y la contribución a un medio ambiente más limpio.

Los principales aeropuertos que sirven a la ciudad de San Salvador son el Aeropuerto Internacional de El Salvador y el Aeropuerto Internacional de Ilopango, El Aeropuerto Internacional de El Salvador, está localizado a 30 minutos de la capital y desde ahí se atienden a 31 destinos directos hacia Norteamérica, Centroamérica, Sur América, Europa y El Caribe. El Aeropuerto Internacional de Ilopango está habilitado únicamente para atender vuelos de carácter civil, taxis aéreos y TAG (Transporte Aéreo de Guatemala) ofrece servicios regulares a Ciudad de Guatemala y San Pedro Sula.

La ciudad dispone de la variedad de medios de comunicación modernos. En telefonía fija, pública y móvil, tienen su sede en la urbe las principales empresas dedicadas a estos servicios en el país. Entre ellas se encuentran Claro, Telefónica, Tigo, Digicel, etc. También hay cobertura en servicios de televisión por cable, satelital, digital e Internet.

En esta capital circulan los principales medios escritos y tiene su sede algunos de los principales periódicos de El Salvador, entre ellos El Diario de Hoy, Diario Co Latino y Diario El Mundo. En cuanto a la televisión, también tienen su asiento empresas como Telecorporación Salvadoreña, Grupo Megavisión, y Tecnovisión; también hay teledifusoras de instituciones educativas (Universidad Francisco Gavidia) y cristianas (católicas y protestantes). En cuanto a radiodifusoras, hay diversidad de programación en las frecuencias de FM y AM, tanto de contenido comercial, participativo-comunitario o cristiano (de línea católica o protestante).

En El Salvador con el incremento de violencia, se considera a la Zona Metropolitana con los más altos de índices de delincuencia en el país. Otro problema muy significativo son las maras, tanto que se han abierto oficinas del FBI en la ciudad. Un problema que ha estado vigente es el desorden en el centro histórico, por parte de los vendedores ambulantes, un problema que es origen por parte del crecimiento de la población nacional, y la búsqueda del sustento diario, algo que ha dado producto al desorden del centro de la capital.

Hermanamientos que la Alcaldía de San Salvador ha realizado con otras ciudades y entidades políticas:

Tienen 14 capitales hermanadas con:

Tienen 7 ciudades hermanadas con:






</doc>
<doc id="9863" url="https://es.wikipedia.org/wiki?curid=9863" title="Isla Grande de Tierra del Fuego">
Isla Grande de Tierra del Fuego

La isla Grande de Tierra del Fuego está ubicada en el extremo sur de América, continente del que está separada por el estrecho de Magallanes. Por superficie, es la y la mayor, con diferencia, de las islas del gran archipiélago fueguino. Limita con el canal Beagle al sur, el océano Atlántico al este, y el océano Pacífico al oeste.

Esta isla es compartida por Argentina y Chile, países a los que les corresponde la parte oriental y occidental, respectivamente. 18 507,3;km² pertenecen a la Argentina con el 38,57% del total, mientras que 29 484,7;km² pertenecen a Chile con el 61,43% de la superficie de la isla. La parte argentina de la isla corresponde a la provincia de Tierra del Fuego, Antártida e Islas del Atlántico Sur, cuya capital es la ciudad de Ushuaia. El sector chileno de la isla corresponde mayoritariamente a la provincia de Tierra del Fuego, perteneciente a la región de Magallanes y de la Antártica Chilena; la capital de la referida provincia es Porvenir. El resto de la sección chilena de la isla corresponde a la provincia de la Antártica Chilena, perteneciente también a la referida región.

La Isla Grande de Tierra del Fuego posee dos sectores bien diferenciados, los dos tercios septentrionales están formados por mesetas y llanuras suavemente onduladas. El sur está ocupado por la terminación austral de la cordillera de los Andes, que tiene aquí orientación Este-Oeste. Su territorio es de 47 992 km². La cordillera Darwin en el sector (chileno) sudoeste, cuenta con los puntos más altos de la isla. De ellos el monte Shipton con 2469 msnm, es el más alto.
En las vertientes más elevadas y menos soleadas de los cordones montañosos se desarrollan numerosos glaciares. Estos pueden llegar en algunos casos a descender al nivel del mar, en el caso de los que se desprenden de la cordillera Darwin hacia el sur, como por ejemplo el glaciar Pía sobre el canal Beagle.
En el fondo de los valles se ubican lagos y lagunas de origen glaciario como los lagos Fagnano y Yehuin. Las condiciones climáticas favorecen el abundante desarrollo de turbales en casi toda la isla, especialmente en los ambientes más húmedos de la mitad austral. Las costas meridionales, en donde los Andes están en contacto con los canales del océano Pacífico, son elevadas y muy accidentadas, existiendo numerosos fiordos. Las costas septentrionales, por contrapartida, son poco accidentadas llamando la atención la casi circular bahía San Sebastián y la bahía Inútil.

Su extremo norte se ubica aproximadamente en el paralelo 52°30' sur —(Punta Anegada, Chile). Como extremo sur, la página web del Instituto Geográfico Nacional argentino indica el cabo San Pío en Argentina. Sin embargo, la página web de la Subsecretaría de Catastro de Tierra del Fuego señala que la punta Falsa se halla aún más al sur que el cabo San Pío.

Además de los lagos y glaciares, la isla posee numerosos ríos. Estos desaguan en el estrecho de Magallanes, canal Beagle, el océano Pacífico, y el océano Atlántico. El mayor de estos ríos es el río Grande que desagua en el Atlántico. La pesca de los ríos de Tierra del Fuego tiene fama mundial, siendo los mejores ríos de pesca el Cóndor, Grande, Paralelo y Azopardo, donde se pueden encontrar diversas variedades de truchas de hasta 10 kg .

Las rocas expuestas en la Isla Grande de Tierra del Fuego revelan una compleja historia geológica, caracterizada por tres etapas tectónicas principales:

Desde el Plioceno y durante el Cuaternario y hasta hace por lo menos entre 16 y 10 mil años, Tierra del Fuego se vio cubierta de extensos glaciares, cuya manifestación en el relieve aún se observa en la actualidad: antiguos valles glaciarios conforman hoy en día depresiones elongadas ocupadas por el mar (canal Beagle, seno Almirantazgo) o por agua dulce (lagos Fagnano, Blanco, Yehuin). Los depósitos de morrenas, kames, eskeres y drumlins asociados a la dinámica glaciaria cubrieron gran parte de los macizos rocosos de los Andes Fueguinos. Luego del deshielo, la isla comenzó a tomar su fisonomía actual; el deshielo de los glaciares generó importantes cursos fluviales, con sus depósitos glacifluviales asociados de grava y arena. Los valles fluviales asociados son ocupados en la actualidad por ríos de mucha menor magnitud, como el Grande y el San Pablo, y la mayor parte de los cursos fluviales que nacen en circos glaciarios o en lagunas de origen glaciaria. Durante el Pleistoceno, el nivel relativo del mar sufrió numerosas transgresiones y regresiones, asociadas a las sucesivas glaciaciones, hasta adquirir su posición actual durante el Holoceno (11 mil años a la actualidad), configurándose así la Isla Grande de Tierra del Fuego como la conocemos.

La región responde a la falla Fagnano-Magallanes, un sistema regional de falla sismogénico, de orientación este-oeste que coincide con el límite transformante entre las placas Sudamericana (al norte) y Scotia (al sur), con sismicidad media; y su última expresión se produjo el , a las 22.30 UTC-3, con una magnitud aproximadamente de 7,8 en la escala de Richter

La Defensa Civil municipal debe realizar simulacro de sismo; y advertir sobre escuchar - obedecer acerca de

La isla posee más de 130 mil habitantes distribuidos de la siguiente manera:

La población en territorio argentino es mayor debido al desarrollo y a los exitosos planes argentinos en materia turística e industrial para incentivar el asentamiento de personas allí. Se cree que el Censo 2010 tuvo varios errores y que el sector argentino posee más de 180 mil habitantes.

Los principales centros poblados son:

Las llanuras del sector norte de la isla están cubiertas por estepas y semidesiertos frescos, donde domina una formación de tussok (coironales o estepa de coirón) cuya especie característica es la "Festuca gracillima". Fitogeográficamente, pertenecen al distrito fitogeográfico patagónico fueguino de la provincia fitogeográfica Patagónica.

Las cumbres andinas presentan desiertos fríos que se incluyen en el distrito fitogeográfico altoandino austral de la provincia fitogeográfica Altoandina.

Solo el 30 % de la isla está cubierto por bosques los que pertenecen a la provincia fitogeográfica subantártica. Según las especies arbóreas dominantes, son clasificados en dos tipos; en los del distrito fitogeográfico subantártico magallánico dominan especies perennifolias, y si sitúan en las áreas próximas al nivel marino y de abundante precipitación. En las áreas más secas o de mayor altitud se encuentran los bosques del distrito fitogeográfico subantártico del bosque caducifolio, en los cuales dominan las especies caducifolias.

En los bosques de Tierra del Fuego se pueden encontrar 7 especies de árboles: canelo ("Drimys winteri"), leña dura "Maytenus magellanica", el ciprés de las Guaitecas ("Pilgerodendron uviferum") —la conífera más austral del mundo—, notro ("Embothrium coccineum"), y 3 especies de hayas australes; ñirre "Nothofagus antarctica", lenga "Nothofagus pumilio" y el siempreverde coigüe de Magallanes "Nothofagus betuloides". En los espacios abiertos de estos bosques crecen ciertos tipos de plantas con frutos comestibles, es el caso de la frutilla o fresa ("Fragaria chiloensis" var. "chiloensis" f. "chiloensis") y calafate ("Berberis buxifolia"), que fueron y son recolectadas por los originarios y campesinos respectivamente. Estos bosques son únicos en el mundo por haberse desarrollado en un clima con veranos tan frescos (alrededor de 9 °C). La cobertura arbórea se extiende muy cerca de la punta más austral de Sudamérica. Los vientos son tan fuertes que áreas expuestas al viento los árboles crecen torcidos por la fuerza de los vientos, y la gente los denomina "árboles-bandera" por la forma que necesitan tomar en su lucha contra el viento. La vegetación arbórea se extiende tan al sur como la Isla de los Estados, isla Navarino y el norte de Isla Hoste. A altitudes cercanas a los 500 msnm se pueden encontrar comunidades enanas de "Nothofagus".

Los bosques de la isla han dejado de tener importancia local, han sido la fuente de árboles que han sido plantados en otras partes del mundo en lugares que tienen prácticamente el mismo clima pero que se encontraban originalmente desprovistos de árboles como las islas Feroe y archipiélagos cercanos; la mayor parte de las especies fueron recolectadas de los lugares más fríos de Tierra del Fuego en sitios principalmente colindantes con la tundra. Los fuertes vientos y fríos veranos no permitían el crecimiento de los árboles en Feroe de otras regiones del mundo, por lo que las especies fueguinas ahora son usadas como ornamentales, para formar cortinas contra-viento y para luchar contra la erosión causada por tormentas y el pastoreo.

Entre la fauna silvestre se reconocen las siguientes especies:

Tetrápodos: guanacos (que fueron de gran importancia para la economía de la cultura shelknam), el culpeo fueguino o zorro colorado fueguino, el tuco-tuco (roedor), y el huillín (especie de nutria nativa) y roedores menores como lauchas y ratones. Entre la fauna introducida destacan el zorro chilla, el conejo de Castilla, la rata almizclera, el castor canadiense, y el visón americano. El castor, traído desde Canadá para influenciar la caza de este animal y la venta de sus pieles, ha producido un severo daño ecológico en la zona.

Pinnipedos: El litoral fueguino es frecuentemente visitado por lobos marinos, focas leopardo y elefantes marinos. Se reconocen numerosas colonias de los primeros, y algunas pocas de los dos últimos (elefantes marinos en Bahía Ainsworth y focas leopardo en la costa sur de Cordillera Darwin, en el canal Beagle).

Cetáceos: Algunas especies de misticetos y odontocetos circulan en los mares que rodean a la Isla Grande, entre ellos ballenas minke, ballena franca austral, toninas, orcas, cachalotes y ballenas piloto.

Aves: Entre las numerosas aves tanto de hábitat marino como terrestre, lacustre y palustre se pueden enumerar los cóndores, águilas, halcones, Becacinas, lechuzas, gorriones, zorzales, palomas, cauquenes, cormoranes, pingüinos (rey, de magallanes, de barbijo), ostreros o pilpilenes, macas o pimpollos, gaviotas, gaviotines, albatros, petreles, etc.

Reptiles: Sólo se conoce una especie de reptil, la lagartija magallánica ("Liolaemus magellanicus"), que habita en la zona esteparia al norte del río Grande.

Anfibios: Apenas una especie habita en la isla, y sólo en el sector chileno, es el sapito de tres rayas, siendo además el anfibio más austral del mundo.

El clima en esta región es bastante inhóspito. Es subpolar oceánico (clasificación climática de Köppen "Cfc") con cortos y frescos veranos y largos, húmedos y fríos inviernos: el nordeste se caracteriza por fuertes vientos y poca precipitación; en el sur y oeste es muy ventoso, brumoso y húmedo la mayor parte del año y son pocos los días del año sin lluvia, aguanieve, granizo o nieve. En la Isla de los Estados, 230 km al este de Ushuaia recibe 1400 mm de lluvia. Las precipitaciones son más fuertes en el extremo oeste, 3000 mm anuales.

Las temperaturas son muy uniformes a lo largo del año: en Ushuaia y Porvenir apenas sobrepasan los 9 °C en verano y promedian 1 °C en invierno. Las nevadas pueden ocurrir en verano. Los fríos y húmedos veranos ayudan a preservar los antiguos glaciares. Las islas más australes poseen clima subantártico típico de Tundra que hace el crecimiento de árboles imposible. La variación térmica anual aumenta ligeramente en el noreste : mientras que en Ushuaia la temperatura media en verano es 10,5° y en invierno 2° , en Rio Grande es de 11° en verano y 0° en invierno

Existen algunas áreas en el interior que tienen clima polar. Regiones en el mundo con climas similares a Tierra del Fuego son: islas Aleutianas, Islandia, península de Alaska e islas Feroe. Los inviernos son muy prolongados y rigurosos en los cuales son continuas las nevadas, durante los inviernos, cuando las temperaturas constantemente se sitúan bajo los -10 °C (10 grados centígrados o Celsius bajo cero) las horas de sol son escasas (amanece hacía las 9.30 y obscurece hacia las 17.00), la primavera suele poseer frecuentes lloviznas y ocasionales nevadas, los veranos son agradables aunque con temperaturas que apenas superan los 15 °C (excepcionalmente se han registrado algunos pocos días con temperaturas que frisaron los 30 °C), si en invierno las horas con sol son pocas, por contrapartida, durante el verano fueguino el sol sale temprano y recién se pone hacia las 22.30. Durante todo el año son frecuentes y fuertes los vientos ("aulladores" o "bramadores") que soplan desde el cuadrante sudoeste trayendo aire frío y mucha humedad que se condensa y precipita en las cordilleras.

En cuanto a la población precolombina, el archipiélago fueguino ha estado habitado por el "Homo sapiens sapiens" desde hace unos 10 000 años.

El primer poblamiento fue obra de paleoamericanos, quienes habrían sido los antepasados de los yaganes, que habitaban principalmente la región oriental, y, quizás parcialmente, de los kawésqar, que ocupaban las abruptas y anfractuosas costas occidentales.

Hacia el siglo XIV, los selknam —una etnia del conjunto amerindio, del subconjunto pámpidos y del linaje de los tehuelches, también llamados «patagones»— ingresaron en Tierra del Fuego. Esta nueva población se instaló principalmente en la región esteparia, aproximadamente la mitad norte del archipiélago; los selknam, también llamados «onas» por los yaganes, llamaban a la isla "Karukinka", 'nuestra tierra'. Posteriormente, un linaje del conjunto selknam accedió al extremo sureste del territorio y se mezcló con los yaganes, lo que dio origen a la etnia mánekenk, vulgarmente conocida como «haush» o «aush».

La inmensa mayoría de la población originaria de la isla pereció entre fines del siglo XIX e inicios del siglo XX, víctima principalmente de una campaña de exterminio llevada a cabo por los estancieros patagónicos. Actualmente, habita en la isla un número reducido de descendientes de los pueblos fueguinos.

Los primeros europeos que tuvieron contacto con esta isla fueron los miembros de una expedición española al mando de Fernando de Magallanes, hacia el 21 de agosto de 1520. El nombre se atribuye a la visión que tuvieron de ella estos primeros marineros europeos que exploraron sus costas: desde sus barcos divisaban sorprendentes y constantes fogatas. Así, fue nombrada "Tierra de humos", nombre que Carlos I de España modificaría a "Tierra del Fuego".

Las hogueras eran la forma en que los originarios se protegían del frío austral, indígenas selknam (u onas, en yagán) y yámanas (o yaganes) que, a pesar del duro clima, apenas utilizaban ropa. Sólo el fuego y su especial adaptación metabólica (temperatura corporal un grado superior a la nuestra) los mantenía calientes. Portaban hogueras encendidas incluso en las canoas de corteza de lenga, que utilizaban para pescar y cazar mamíferos marinos.

El 1 de noviembre de 1520, Magallanes y sus compañeros se adentraron en el estrecho que llamaron "De Todos Los Santos" (actualmente denominado Estrecho de Magallanes), e hicieron un reconocimiento de las costas septentrionales de la isla, creyendo que se trataba de una región litoral de la "Terra Australis Incognita" y no un conjunto insular de América. La travesía duró 19 días.

En 1525 Francisco de Hoces, separado de la Expedición de García Jofre de Loaísa, descubrió el Pasaje de Drake.

Hacia 1555 el español Juan de Alderete intentó una conquista de la Tierra del Fuego pero, igual que Pedro Sarmiento de Gamboa, debió desistir ante las inclemencias climáticas.

El 24 de enero de 1616 el holandés Jakob Le Maire de la expedición de hermanos Jan y Willem Schouten, descubre el estrecho de Le Maire y da nombre a la isla de los Estados. El 29 de enero descubren y pasan el cabo de Hornos al que llaman así en homenaje al barco "Hoorn".

En 1619 los hermanos Bartolomé y Gonzalo García de Nodal circunnavegaron el archipiélago fueguino enviados por el rey Felipe III de España. El segundo volvió al estrecho de Le Maire en 1622 pero murió antes de lograr llegar a la Araucanía. El 10 de febrero de 1619 descubrieron las islas Diego Ramírez.

En 1624 el holandés Jacques L'Hermite, regresando del Perú, dio nombre a las Islas Hermite y a la bahía Nassau y exploró Tierra del Fuego.

En 1643 el corsario neerlandés Hendrick circunnavegó la isla de los Estados demostrando que es una isla.

En 1765 se produjo en la caleta Falsa (sector argentino de la isla) el naufragio del buque "Purísima Concepción", durante tres meses se estableció allí el primer asentamiento europeo en la isla, denominado "Puerto Consolación". Los náufragos (193) fabricaron una embarcación que llamaron "Nuestra Real Capitana San José y Las Animas" y retornaron a Buenos Aires.

Entre el 11 de diciembre de 1768 y el 21 de enero de 1769, la expedición del primer viaje de James Cook, al mando del "HMB Endeavour," al Pacífico Sur recorrió la costa noreste de la isla en su travesía del estrecho de Le Maire para pasar al Pacífico rodeando el cabo de Hornos. Cook topografíó la costa de la isla, realizó perfiles de la misma y dio referencias muy exactas para la época de sus coordenadas. El paso del estrecho fue dificultoso. La expedición fondeó durante 5 días en la bahía Buen Suceso, en la península Mitre, para aprovisionarse de madera y agua antes de encarar el paso del cabo de Hornos hacia el océano Pacífico. Allí tuvieron contactos pacíficos con los nativos haush de los que tanto Cook como el naturalista Joseph Banks que formaba parte del equipo científico de la expedición, realizaron descripciones sobre su aspecto, vestimentas y formas de vida, y el dibujante Sydney Parkinson varias ilustraciones. Y una partida encabezada por Banks y el también naturalista Daniel Solander, se internaron en la zona para recolectar muestras de plantas y animales. Un cambio repentino de tiempo que hizo caer una nevada les obligó a pernoctar a la intemperie y dos de los criados de Banks murieron de hipotermia.

Tras la independencia de Argentina y Chile, los dos nuevos estados discutieron largamente la soberanía sobre la isla. En julio de 1876 los cancilleres de ambos países, Bernardo de Irigoyen y Diego Barros Arana, dentro de un acuerdo general sobre las fronteras, decidieron dividir la isla con una línea imaginaria que separara el sector chileno, al oeste, del argentino, al este. La frontera convenida por ambos países en virtud del Tratado de Límites de 1881 y el Protocolo de Límites de 1893 se extiende desde el Cabo Espíritu Santo, en la boca del Estrecho de Magallanes, hasta el Canal de Beagle, siguiendo la longitud 68º 34' O.

Desde décadas antes (1840s) ya había comenzado el asentamiento de población blanca, iniciada con la introducción de misioneros anglicanos y católicos salesianos. Tras ellos llegaron los primeros estancieros, que iniciaron una fuerte presión sobre la población indígena.

En 1881 comenzaron a llegar a Tierra del Fuego buscadores de oro, que tras algunas investigaciones lograron hallar el preciado metal en la isla. La noticia se extendió rápidamente, dando paso a una fiebre del oro que atrajo numerosos inmigrantes europeos. Entre ellos llegó Julius Popper, que alcanzó a erigir un pequeño imperio minero, basado en cuestionados métodos, como el genocidio de la población autóctona.

La súbita riqueza aurífera permitió el establecimiento de las principales poblaciones urbanas, como Porvenir, en Chile, fundada en el año 1894. En el año 1884, en el sur de la Isla Grande, sobre el canal de Beagle, se estableció la Subprefectura de Ushuaia en la vecindad de la misión anglicana fundada en 1869. En el norte, se estableció en 1893 la misión salesiana sobre el Río Grande, a la vera del cual también comenzó a crecer un poblado ligado íntimamente a la actividad ganadera. En 1908 se fundó la localidad de Puerto Yartou. El Poblado de Río Grande se fundó oficialmente como colonia agrícola de Río Grande recién en 1921.

En los últimos años del siglo XIX se crearon las primeras grandes estancias ovejeras de la isla, propiedad de las familias Menéndez y Bridges.




</doc>
<doc id="9865" url="https://es.wikipedia.org/wiki?curid=9865" title="Family Tree">
Family Tree

Family Tree es una caja recopilatoria de la cantante Islandesa Björk. La caja consta de 5 mini-CD con rarezas y material inédito, 1 CD de grandes éxitos seleccionados por la propia Björk y 1 libro con las letras.

En ella se contienen canciones realizadas con los grupos anteriores a su etapa solista, como KUKL o The Sugarcubes; innovaciones en su estilo y se muestra toda su evolución desde los 80 hasta el año 2002.

El set completo fue lanzado en noviembre de 2002, junto con el recopilatorio Greatest Hits

Esta recopilación está formada de la siguiente manera:


Además, las portadas de los sobres y los libretos están acompañados por dibujos surrealistas de la artista contemporánea islandesa Gabríela Friðriksdóttir.




</doc>
<doc id="9879" url="https://es.wikipedia.org/wiki?curid=9879" title="KUKL">
KUKL

KUKL fue una banda islandesa de rock gótico, experimental con mezcla de jazz, música rítmica y punk. En islandés medieval, KUKL significa “Hechicero”.

La banda se formó en agosto de 1983 cuando Ásmundur Jónsson de la discográfica Gramm (la más importante de Islandia) quería formar una banda con todos los artistas más vanguardistas del momento. Los integrantes eran Björk Guðmundsdóttir, que hasta ese momento se encontraba en otra banda llamada Tappi Tíkarrass; Einar Örn Benediktsson, cantante y trompetista de Purrkur Pillnikk; Siggtryggur Baldursson, baterista, y Guðlaugur Kristinn Óttarsson en guitarra, ambos integrantes de Þeyr; Birgir Mogensen, bajista de Með Noktum y finalmente el tecladista de Medúsa, Einar Arnaldur Melax. Por eso, KUKL es considerado como una fusión entre Purrkur Pillnikk, Þeyr y Tappi Tíkarrass.

KUKL apareció por primera vez en un concierto en el concierto "We Demand a Future", en Reikiavik el 20 de septiembre de 1983, pero volvieron a reunirse en diciembre del mismo año para el lanzamiento de su primer single: "Söngull" con la canción del lado B "Pökn (fyrir byrjendur)". en este lanzamiento “Söngull” es la versión de la canción “Dismembered” que fue lanzada más tarde en 1984. La diferencia de esta es que la primera tiene una introducción con guitarras en vez de campanas, letra en islandés y ritmo más acelerado.

Einar Örn, líder del grupo, había estudiado ciencias de la comunicación en Polytechnic of Central London, donde tuvo la oportunidad de entrar en contacto con varios grupos punk como Flux of Pink Indians y Crass (los impulsores del anarcopunk). El primer álbum de KUKL, "The Eye", fue lanzado a través del sello de Crass: Crass Records.

El título "The Eye" hace alusión al libro favorito de Björk en ese momento: "Story of the Eye" ("Histoire de l'oeil", «Historia del ojo»), escrito por Georges Bataille (1928), cuya trama involucra las perversiones sexuales de una pareja de jóvenes franceses en un contexto de violencia.

Este álbum, cuya portada estaba ilustrada por el artista Dada Nana, contenía la versión en inglés de “Söngull” titulada “Dismembered” y una canción llamada “Anna” de la cual se hizo un videoclip que fue dirigido por Óskar Jónasson. 
La música de KUKL era muy diferente en Crass Records: se trataba de una mezcla compleja de rock gótico, punk, jazz y música rítmica con el estilo de Killing Joke y referencias vanguardistas del after-punk de The Fall, con una orientación de composición a Ígor Stravinski o Aleksandr Skriabin. El estilo de guitarras de Gulli Óttarsson, con notas elaboradas, construyen junto, a la línea de bajo de Birgir Mogensen, una base sólida, vibrante y muchas veces discordante que con la adición de Björk, Einar Örn y el tecladista Melax dan lugar a canciones con un aire bastante oscuro, con letras amenazantes y cifradas.

En la grabación de "The Eye", la banda usó diferentes tipos de tambores, trompetas e incluso campanas. Muchos críticos los catalogan con referencias a Siouxsie and the Banshees, Killing Joke, Einstürzende Neubauten y la primera etapa de The Cure.

KUKL nunca alcanzó fama mundial, a pesar de que eso era lo que se creía si ellos cambiaban de sello. Los miembros de la banda debían trabajar en empleos ordinarios cuando no tocaban música, así por ejemplo, la cantante Björk trabajaba en una procesadora de pescado y también tuvo apariciones como vocalista de fondo para la banda Megas, “abuelo del rock” islandés. Gulli Óttarsson, actualmente un experto en matemáticas e inventor, se encontraba realizando trabajos de investigación científica.

En 1985 KUKL hizo una gira por Europa. Visitaron los Países Bajos durante el Pandora's Box Festival y estuvieron en Dinamarca durante el Festival de Roskilde, junto a otras bandas que también estaban de gira con la espera de lograr el éxito fuera de su propia tierra.

'“A través de la fealdad de la humanidad, tratamos de brillar. En contra de la estupidez de la humanidad, tratamos de luchar. Como recompensa recibimos la locura de otros. Nuestro alimento, las atrocidades del mundo. Nuestro alimento, la alegría del mundo.”'
(Manifiesto de la pág. 6 del folleto del álbum "The Eye").

Para el mismo año también visitaron Francia y editaron "KUKL á Paris 14.9.84" bajo el sello francés V.I.S.A. (lanzamiento limitado solamente a Francia. En 1986, con la producción de Penny Lapsang Rimbaud, la banda sacó su álbum, "Holidays in Europe (The Naughty Nought)" pasando a ser la única banda en lanzar más de un disco con Crass Records (a excepción de los Crass mismos). El comunicado de prensa emitido por Crass Records exponía uno de los manifiestos de KUKL:

(Manifiesto en el comunicado de prensa emitido por Crass Records con objeto de informar la salida del álbum "Holidays in Erupe (The Naughty Nought)", 24 de enero de 1986).

El estilo de "Holidays in Europe (The Naughty Nought)", ha sido considerado como post-punk o un tipo de indie rock alternativo. Se realizaron dos videoclips: uno para la canción de inicio “The Outward Fight” y otro para la segunda canción, “France (A Mutual Thrill)”.

El fin de la banda se estaba acercando a medida que exploraron todas las alternativas musicales. KUKL tuvo repercusión en Islandia cuando en 1986 Ríkisútvarpíð (Televisión Nacional de Islandia) hizo un especial sobre la banda donde Björk apareció cantando embarazada con su estómago al aire y las cejas afeitadas. Por este motivo, muchos televidentes llamaron al canal quejándose de la cantante.

La banda se separó el mismo año en que Björk se casó con Þór Eldon (guitarrista de Medúsa) y ambos tuvieron un hijo, Sindri Eldon Jónsson, el 8 de julio de 1986, fecha que se oficialmente se conoce como el nacimiento de la próxima banda de Björk: The Sugarcubes, en la que también siguieron Einar Örn, Einar Melax y Siggi Baldursson. Gulli Óttarsson y Birgir Mogensen fueron los únicos que decidieron no continuar. Al mismo tiempo que existía KUKL, Gulli Óttarsson estaba trabajando con Björk, en un proyecto a dúo con la compañía de los músicos del grupo sin la participación de Einar Örn, con el nombre de The Elgar Sisters y permanecieron por un tiempo más después de la separación de KUKL.

La idea motora de KUKL era provocar un cambio en la sociedad a través de la música utilizándola como medio de transmisión. Para ello, el grupo no se vendía al comercialismo y solamente aparecía en ocasiones especiales para mantener su cualidad de inspiración. Sus integrantes mantenían la postura de que el poder está en nosotros y en lo que hacemos. Al escuchar al grupo, la gente se convertiría en parte de la transmisión de ese poder particular, incluso sin proponérselo.
Sin querer dar una definición exacta de su filosofía para evitar estancamientos, los integrantes de KUKL consideraban que en la vida hay mucho más de lo que es predeterminado. Querían iluminar la mentes de otras personas evitando restricciones y decían que la música actual servía de herramienta para que la gente se adormeciera mientras que los que estaban en un nivel superior observaban cómo íbamos camino a la perdición.

Señalaban que no había que conformarse con nuestro propio estilo de vida, nuestro arte o actitudes. Al tener en cuenta esta regla en la música, KUKL introdujo una incongruencia en la psiquis de su audiencia y como no lo utilizaban para beneficio propio, los individuos tendrían espacio para llenarlos para sí mismos. Una actitud no conformista traerá una nueva camada de otros. La opción es ser uno mismo.

1983:
1984:
1985:
1986:

“Una nueva interpretación de música rock, distinto a todo lo que haya escuchado antes.”
("DV", 17 de septiembre de 1983).

“Por primera vez me encuentro sin palabras. Este concierto no puede ser descrito. Aquellos que lo vivieron lo tendrán por el resto de sus vidas.”
("DV", 26 de noviembre de 1983).

“SHOCK: una banda islandesa cuyo nombre nadie conoce, excepto que tiene que ser gritado, metido en la vida y que hizo tal vez el sonido más agradable de toda la noche. Aparentemente con dos vocalistas líderes (imposibles de ver debido a músicos tambaleándose rítmicamente), este grupo curioso generó una intensidad nacida de un caos en caída donde todo se ponía maravillosamente histérico, pero mientras tanto, las fundaciones permanecieron bajo control, fácilmente para disfrutar.”
("Sounds", enero de 1984).

“Habíamos esperado alguna combinación de Þeyr y Purrkur Pillnikk, pero se nos sirvió con una sorpresa en todos los aspectos. La música mantiene la cualidad maníaca de P.P. y la sofisticación de Þeyr, pero se mueve mucho más allá. Al principio pensé que no podría digerir el frenesí rítmico y la combinación tonal alocada pero mi estómago y eventualmente todo mi cuerpo empezaron a seguir el ritmo. ¿Y quién soy yo para no estar de acuerdo?”
("S&T", 18 de abril de 1984).

“Su creación musical literalmente explota en las caras o máscaras de la audiencia, se impulsa en su conciencia e incluso si por casualidad no estás interesado, no hay forma de evitar o negar de tenerla en consideración...”
("DV", 28 de abril de 1984).

Extraídos de una colección de artículos periodísticos distribuidos en conexión con su concierto en Austurbæjarbíó, Reykjavík, el 21 de diciembre de 1984.

“FLUX OF PINK INDIANS/D & V/KUKL/CHUMBAWUMBA. Conway Hall. Las leyendas están hechas de cosas así - REALMENTE. Un concierto que puso el último clavo al ataúd de la basura del post punk plástico, con la participación de cuatro de los cinco mejores bandas de anarco-punk del mundo (¡lo sentimos por la discográfica!). Es una lástima que Crass no estuviera allí para completar la potencia... KUKL son un grupo islandés, que inmediatamente se ubican en una familia bastante selecta: incluso si hubiera otros grupos islandeses para hacer comparaciones (el bien conocido pasado del periodista), sería imposible. A cambios benévolos y humorísticos, poseen una líder peculiarmente carismática; si no llegas a verlos antes de terminar su gira europea, compra su LP indispensable "The Eye" en Crass Records y lamenta lo que te has perdido. ¡Fascinante!”.
("Sounds", 3 de septiembre de 1984).

“Las cosas se iluminaron con lo mejor de Islandia (aunque sin oposición) KUKL entregando un pequeño conjunto punkie embriagado de sonidos Bow Wow Wow, los sonidos provenientes del chico más honesto en la historia y la chica con el corte de pelo raro desde Skafish. La guitarra estuvo profunda y la batería fue flexible, y podría haberlos escuchado por al menos otros 12 años si el deber no hubiera llamado.”
("New Musical Express", 6 de octubre de 1984).

“PANDORA’S BOX FESTIVAL Rotterdam. Con una bondad en gracia, grandes bolas de fuego - ¡es KUKL! El chico y la chica embistiendo, gritando y llorando, mientras que los cuatro restantes se preparan para hacer un infierno de un alboroto, cayendo en ocasionales ataques de melodía. No estoy seguro que fuera su intención, pero los encontré muy entretenidos, y deberían ser vistos tan sólo para ser testigos de la extraordinaria presentación de la pequeña chica maniática”.
("Sounds", 13 de octubre de 1984).




Single:

Álbumes:

Apariciones y colaboraciones:





</doc>
<doc id="9881" url="https://es.wikipedia.org/wiki?curid=9881" title="Bad Taste">
Bad Taste

Bad Taste puede referirse a:

</doc>
<doc id="9882" url="https://es.wikipedia.org/wiki?curid=9882" title="Söngull">
Söngull

Söngull es un álbum lanzado en septiembre de 1983 por la banda islandesa KUKL, una banda que combinaba el rock, punk, jazz y música más experimental. En KUKL se encontraba la solista Björk. 
Si bien este es el primer lanzamiento de KUKL, su debut fue hacia finales de 1984 con el lanzamiento de The Eye.
"Söngull" fue lanzado solamente en Islandia.


NOTA: "Pökn" no tiene significado, es un juego de palabras formado con la palabra "Punk"




</doc>
<doc id="9883" url="https://es.wikipedia.org/wiki?curid=9883" title="The Eye (álbum de KUKL)">
The Eye (álbum de KUKL)

The Eye, lanzado a finales de 1984 fue el álbum debut de KUKL una banda islandesa que combinaba el rock, punk, jazz y música más experimental. En KUKL se encontraba la solista Björk. Este álbum fue lanzado en formato LP.

El nombre del álbum proviene del libro favorito de Björk durante su adolescencia: Story Of The Eye de Bataille. Un libro sobre adolescentes en una misión, un cuento muy intenso en el que se involucra el sexo, asesinato, perversiones, etc.

Lado A

Lado B


Ingeniería: Tony Cook.
Trabajo de arte y diseño: Dada Nana.
Grabado en Southern Studios en enero de 1984.

Nota: Extraída de una colección de artículos periodísticos distribuidos en conexión con su concierto en Austurbæjarbíó, Reykjavík, el 21 de diciembre de 1984.


</doc>
<doc id="9887" url="https://es.wikipedia.org/wiki?curid=9887" title="Arithmetica">
Arithmetica

La Arithmetica es un tratado de 13 libros del que sólo se conocen los seis primeros, que fueron escritos por el matemático griego Diofanto de Alejandría alrededor del año 250. 

Hallado en Venecia por el matemático alemán Johann Müller Regiomontano hacia 1464 la primera traducción al latín no se realizó hasta 1575 siendo obra de Guilielmus Xylander. 

La Arithmetica no es propiamente un texto de álgebra sino una colección de problemas siendo su estructura la siguiente:


En honor de Diofanto las ecuaciones con coeficientes enteros cuyas soluciones son también enteras se denominan ecuaciones diofánticas (o diofantinas).





</doc>
<doc id="9891" url="https://es.wikipedia.org/wiki?curid=9891" title="Humani generis">
Humani generis

Humani generis es una encíclica publicada por el papa Pío XII el 12 de agosto de 1950, "sobre las falsas opiniones contra los fundamentos de la doctrina católica". Es un documento que consolida y extiende las condenas que San Pío X acusa en la encíclica "Pascendi Dominici gregis".

En ella se reafirma la compatibilidad entre las creencias religiosas católicas y la investigación científica, pero, al mismo tiempo, reafirma la inmutabilidad de los postulados fundamentales de la religión.



</doc>
<doc id="9894" url="https://es.wikipedia.org/wiki?curid=9894" title="Diofanto de Alejandría">
Diofanto de Alejandría

Diofanto de Alejandría griego antiguo: Διόφαντος ὁ Ἀλεξανδρεύς, "Dióphantos ho Alexandreús"), que vivió en el siglo III o en el IV, fue un antiguo matemático griego. Es considerado "el padre del álgebra maestral".

Nacido en Alejandría, de él nada se conoce con seguridad sobre su vida, salvo su edad con la que falleciera; esto, gracias al epitafio redactado en forma de problema y conservado en la antología griega.

Según esto, Diofanto falleció a la edad de 84 años. Se ignora, sin embargo, en qué siglo vivió. Diofanto dedica su "Aritmética" a un tal Dionisio, acerca del que el historiador Paul Tannery ha sugerido que podría tratarse de un obispo de Alejandría que vivió en el siglo III. Por otra parte, si fuera el mismo astrónomo Diofanto que citó Hipatia (fallecida en 415), habría fallecido antes del siglo V; pero si se tratase de personas distintas, cabe conjeturar que habría vivido a finales de dicho siglo, ya que ni Proclo ni Papo lo citan, lo que resulta difícil de entender tratándose de un matemático que pasa por ser el inventor occidental del álgebra. 

El matemático alejandrino debe su renombre a su obra "Arithmetica". Este libro, que constaba de trece libros de los que sólo se han hallado seis, fue publicado por Guilielmus Xylander en 1575 a partir de unos manuscritos de la universidad de Wittenberg, añadiendo el editor un manuscrito sobre números poligonales, fragmento de otro tratado del mismo autor. Los libros que faltan parece que se perdieron tempranamente ya que no hay razones para suponer que los traductores y comentaristas árabes dispusieran de otros manuscritos además de los que aún se conservan.

En esta obra realiza sus estudios de ecuaciones con variables que tienen un valor racional (ecuaciones diofánticas), aunque no es una obra de carácter teórico, sino una colección de problemas, adecuados para soluciones enteras. Importante fue también su contribución en el campo de la notación; si bien los símbolos empleados por Diofanto no son como los concebimos actualmente, introdujo importantes novedades como el empleo de un símbolo único para la variable desconocida (στ) y para la sustracción, aunque conservó las abreviaturas para las potencias de la incógnita (δς para el cuadrado, δδς para el duplo del cuadrado, χς para el cubo, δχς para la quinta potencia, etc.). En su época el concepto de números poligonales se extendió a los números espaciales, representados por familias de ortoedros, números piramidales.

En 1621, vio la luz una edición comentada de Bachet de Méziriac, edición reimpresa con posterioridad en 1670 por el hijo de Pierre de Fermat incluyendo los comentarios que el célebre matemático francés había realizado en los márgenes de un ejemplar de la edición de Bachet que poseía.





</doc>
<doc id="9895" url="https://es.wikipedia.org/wiki?curid=9895" title="Espiral de Fermat">
Espiral de Fermat

La espiral de Fermat, denominada así en honor de Pierre de Fermat y también conocida como espiral parabólica, es una curva que responde a la siguiente ecuación:

Es un caso particular de la espiral de Arquímedes.



</doc>
<doc id="9901" url="https://es.wikipedia.org/wiki?curid=9901" title="Fobos (satélite)">
Fobos (satélite)

Fobos (del griego "Φóβoς", "miedo") es la más grande de las dos lunas de Marte y la más cercana al planeta, siendo Deimos la otra luna. Ambas fueron descubiertas por el astrónomo estadounidense Asaph Hall (1829-1907) el 18 de agosto de 1877, con el gran refractor de 66 cm del Observatorio Naval de los Estados Unidos de Washington D. C., obra del óptico norteamericano Alvan Clark (1804-1887). Fue el propio descubridor el que propuso los nombres, inspirado por el libro XV de la "Ilíada", en el que el dios Ares (Marte en la mitología romana) invoca al miedo ("fobos") y al terror ("deimos").

Fobos es un pequeño satélite, de forma irregular, cuyo radio medio es de once kilómetros. Siempre presenta la misma cara a Marte, debido al anclaje por marea ejercido por el planeta. Orbita a unos 6000 kilómetros de la superficie marciana, lo que le convierte en el satélite más próximo a su planeta del sistema solar. Estas fuerzas de marea crean una desaceleración en Fobos, perdiendo este velocidad orbital, lo que ocasionará su colisión con Marte dentro de unos 50 a 100 millones de años, o bien su desintegración y formación de un anillo alrededor del planeta.

Asaph Hall descubrió Fobos el 18 de agosto de 1877 fuentes contemporáneas, utilizando la convención astronómica previa a 1925 que comenzaba el día al mediodía, señalan el descubrimiento el día con el telescopio refractor de 26 pulgadas del observatorio naval de los Estados Unidos en Washington, inaugurado cuatro años antes y el más potente por entonces. En esa época, Hall estaba buscando sistemáticamente los supuestos satélites de Marte. El 10 de agosto ya había visto uno de los satélites, pero, debido al mal tiempo, no consiguió identificarlo al día siguiente.

El nombre fue sugerencia de Henry Madan (1838-1901), "Science Master" de Eton. Recuerda a uno de los personajes que acompañan a Ares a la batalla en el libro XV de la "Ilíada". Ares es el equivalente griego del dios romano Marte.

El tamaño y las características orbitales de los satélites de Marte han limitado durante mucho tiempo su observación a solo las ocasiones favorables: cuando el planeta está en oposición y los dos satélites alcanzan elongaciones adecuadas concurren cada dos años o cuando las condiciones son particularmente favorables más o menos cada dieciséis . La primera configuración favorable ocurrió en 1879. Muchos observadores de todo el mundo participaron en las observaciones con el fin de determinar con precisión las órbitas de los dos satélites.

En los cuarenta años siguientes, entre 1888 y 1924, la mayoría de las observaciones (más del 85 % del total) se hicieron en dos observatorios estadounidenses: el Observatorio Naval de los Estados Unidos y el Observatorio Lick. Entre sus objetivos, se encontraba la determinación de la dirección de rotación del planeta. Entre 1926 y 1941 solo continuó las observaciones el primero de los observatorios, con 311. A partir de 1941 las observaciones solo se hicieron con técnicas fotográficas.

Se hicieron pocos o nulos avances en los siguientes quince años. La investigación se reanudó en 1956 encaminada principalmente a la identificación de nuevos satélites. En 1945 Bevan Sharpless (1904-1950) detectó una aceleración de Fobos que no se podía explicar como resultado de una perturbación de la tenue atmósfera marciana. La información no recibió especial atención hasta que fue recogida por Iósif Shklovski (1916-1985) quien en 1959 propuso que el satélite podría ser un objeto hueco y especuló con la idea de que era un satélite artificial lanzado por una civilización alienígena presente antiguamente en el planeta. Esta hipótesis ganó cierta notoriedad. Fue reavivada en 1966 por el mismo Shklovski en el libro "Intelligent Life in the Universe" escrito con Carl Sagan (1934-1996). La controversia que lo acompañó condujo a nuevas observaciones astronométricas de ambos satélites durante los años sesenta y setenta  que confirmaron la medición inicial de Sharpless.

En 1988, coincidiendo con las misiones soviéticas del programa Phobos, Kudriávtsev y sus colegas llevaron a cabo varias observaciones. Sin embargo, durante los siguientes diez años, los satélites no fueron objeto de observaciones, hasta 2003, cuando miembros del Observatorio Lowell hicieron observaciones muy precisas. En 2005 observaron ambos satélites con el radiotelescopio de Arecibo que produjo la estimación de la densidad de algunos materiales de la superficie.

Fobos es uno de los cuerpos que reflejan menos la luz en el sistema solar. Espectroscópicamente se asemeja a los asteroides tipo D, y su composición es aparentemente similar a las condritas carbonáceas. La densidad de Fobos es muy baja para ser una roca sólida y se sabe que tiene una porosidad significativa. Estas observaciones sugirieron que Fobos podría tener una reserva sustancial de hielo. Pero las observaciones espectrales indican que la capa superficial de regolito carece de agua, sin embargo, la presencia de hielo bajo la capa de regolito no se ha descartado.

Se ha predicho la presencia de anillos de polvo muy tenues producidos por Fobos y Deimos, pero los intentos para observar estos anillos han resultado fallidos hasta la fecha. Imágenes recientes de la sonda "Mars Global Surveyor" indican que Fobos está cubierto por una capa de regolito de gránulos finos de al menos 100 metros de espesor; se cree que ha sido creada por impactos de otros cuerpos, pero no se sabe cómo este material se adhiere a un objeto de gravedad tan baja.Fobos es muy irregular, con dimensiones de 27 × 22 × 18 km. A consecuencia de esta forma, la gravedad en su superficie varía cerca de un 210 %; el efecto de marea inducido por Marte duplica esta variación (cerca del 450 %) debido a que este planeta acentúa en más de la mitad la gravedad de Fobos en ambos polos.

Fobos presenta múltiples cráteres de impacto. La característica de la superficie más notable es el cráter Stickney, nombrado en honor a la esposa de Asaph Hall. De manera similar al cráter de Mimas llamado Herschel, pero a menor escala, el impacto que lo creó debió haber casi despedazado a Fobos. La superficie presenta muchos surcos y líneas. Los surcos tienen, por lo general, menos de 30 m de profundidad, de 100 a 200 m de ancho y hasta 20 km de longitud. Se pensó inicialmente que se habían producido por el mismo impacto que creó el cráter Stickney. Sin embargo el análisis de los hallazgos de la nave "Mars Express" reveló que los surcos no tenían patrón radial hacia el cráter Stickney, pero que sí están dirigidos hacia el vértice de Fobos en su órbita (que no está lejos de Stickney). Los investigadores creen que estos surcos han sido excavados a causa del material expulsado hacia el espacio por impactos sobre la superficie de Marte. Los surcos entonces serían catenas (cadenas de cráteres pequeños en sucesión) y todos ellos se van diluyendo al alcanzar las inmediaciones del vértice de Fobos. Los impactos se han agrupado en doce o más familias de diferente edad, presumiblemente representando al menos doce eventos de impacto en Marte.

Se piensa que el meteorito de Kaidun es un fragmento de Fobos, pero no ha sido posible verificarlo dado el escaso conocimiento que se tiene de la composición detallada de esta luna.

Los elementos geológicos de Fobos se han nombrado en memoria de astrónomos relacionados con el satélite, así como con nombres de personajes y lugares de la novela "Los viajes de Gulliver" de Jonathan Swift. Existe una zona denominada "Laputa Regio" y una planicie designada "Lagado Planitia"; ambos nombres están tomados de la novela "Los viajes de Gulliver" (en la ficción, Laputa es una isla voladora y Lagado es la imaginaria capital de la nación ficticia de Balnibarbi). La única cresta de Fobos se denomina "Kepler dorsum", en memoria del astrónomo Johannes Kepler. Varios cráteres también han recibido nombres propios.

FOBOS

La órbita inusualmente cercana de Fobos al planeta Marte produce algunos efectos sorprendentes. Observado desde Fobos, Marte podría parecer 6400 veces más grande y 2.500 veces más brillante que nuestra luna llena vista desde la Tierra, ocupando un cuarto de la amplitud de un hemisferio celeste.

Fobos orbita alrededor del planeta Marte por debajo del radio de la órbita sincrónica, lo que significa que se mueve alrededor del planeta más rápido de lo que el propio planeta rota. Por este motivo aparece en el occidente, se mueve comparativamente, en forma rápida a través del cielo (en 4 horas 15 minutos o menos) y se pone al este, aproximadamente dos veces por cada día marciano (cada 11 horas y 6 minutos). Debido a que se encuentra próximo a la superficie y en una órbita ecuatorial, no puede ser visto sobre el horizonte desde latitudes mayores a 70,4°.

La órbita de Fobos es tan baja que su diámetro angular, visto por un observador en Marte, varía visiblemente según su posición sobre el firmamento. Observado en el horizonte, Fobos tiene cerca de 0,14° de ancho; en el cenit se ve con 0,20°, una tercera parte del ancho de nuestra luna llena vista desde la Tierra. En comparación, el Sol tiene un tamaño aparente cercano a 0,35° en el cielo marciano.

Un observador situado en la superficie marciana en una posición adecuada podría ver el tránsito de Fobos a través del Sol. El satélite no es lo suficientemente grande como para cubrir el disco solar, y por tanto no puede causar un eclipse total. Algunos de estos tránsitos han podido ser fotografiados por el explorador marciano "Opportunity" apuntando sus cámaras hacia el Sol. Estos tránsitos duran muy poco tiempo (alrededor de medio minuto), ya que Fobos se mueve con relativa rapidez por el firmamento, además de que el Sol es muy pequeño visto desde Marte (en comparación con la vista desde la Tierra). Durante este evento, la sombra de Fobos es proyectada sobre la superficie de Marte moviéndose a gran velocidad, sombra que también ha sido fotografiada por algunos exploradores espaciales.

Las fases de Fobos, tal como son observadas desde Marte, duran 0,3191 días (según el período orbital de Fobos) para cubrir el recorrido, solamente 13 segundos más que el mismo periodo de Fobos.

La observación de Fobos desde la Tierra se ve obstaculizada por su pequeño tamaño y su proximidad al planeta rojo. Es solo visible durante un periodo limitado de tiempo cuando Marte está cerca de la oposición  y aparece como un simple punto sin que sea posible resolverlo. En tales circunstancias, alcanza la magnitud 11,6. Marte con una magnitud de -2,8 es seiscientas mil veces más brillante. Además, durante la oposición Fobos presenta una separación de 24,6 segundos de arco del planeta, por lo que es más fácil observar Deimos  que se aleja hasta 61,8 segundos de arco del disco de Marte.

Para observarlo en condiciones favorables, es necesario disponer de un telescopio de al menos 12 pulgadas (30,5 cm). En la favorable oposición del año 2003 ha podido ser capturado incluso con telescopios de 200 mm de diámetro dotados de cámaras CCD. Un objeto que oculte el brillo del planeta y un dispositivo para la toma de imágenes como placas fotográficas o CCD, con exposiciones de varios segundos, son elementos de gran ayuda en la observación.

En el libro "Los viajes de Gulliver", Jonathan Swift describe el gran conocimiento astronómico existente en el imaginario país de Laputa. Uno de los pasajes pareciera adivinar la existencia de las dos lunas marcianas: 
Voltaire (1694-1778) también mencionó a los dos satélites de Marte en su obra "Micromegas", un cuento publicado en 1752 que describe a un ser originario de un planeta de la estrella Sirio, y de su compañero del planeta Saturno.
En ambos casos los dos autores parece que se estaban haciendo eco de una idea muy corriente en los ambientes intelectuales de la época, surgida de las primeras opiniones del astrónomo Johannes Kepler (previas a que enunciara sus famosas tres leyes), basadas a su vez en una teoría misticista relacionada con los sólidos perfectos. La precisión de los datos, en ambos casos, se debe a los cálculos mecánicos realizados a principios del s.XVIII sobre la base de la ley de la Gravitación Universal, referidos a cuál sería el período de rotación y distancia a Marte de un supuesto cuerpo orbitante en torno a dicho planeta. Se trata por tanto de una serendipia, puesto que la óptica disponible durante la vida de ambos autores no permitía ver esos cuerpos celestes tan pequeños y que se separan tan poco de la esfera de Marte.

Debido a estas coincidencias, los dos mayores cráteres en Deimos (de unos 3km de diámetro cada uno) fueron bautizados como "Swift" y "Voltaire".




</doc>
<doc id="9903" url="https://es.wikipedia.org/wiki?curid=9903" title="Rōmaji">
Rōmaji

El , en japonés se refiere en grandes rasgos al alfabeto latino. En Occidente, se suele emplear este término para referirse a la escritura de la lengua japonesa en letras romanas o latinas en contraste con la mezcla habitual de kanji, hiragana y katakana. Los kanji son ideogramas, el hiragana y katakana son silabarios, utilizados en el Idioma japonés. 
Algunos lo escriben "Rōmanji", siendo esta una falta de ortografía bastante común.

El japonés puede escribirse en rōmaji por varias razones y costumbres:


Hay varios sistemas diferentes de romanización, incluyendo los más comunes Hepburn y Monbushō.
El Kunrei-shiki (訓令式), también conocido como "monbushō" (文部省), es un sistema de romanización para transcribir el idioma japonés al alfabeto romano. Es el sistema preferido por el Monbushō (Ministerio de Educación japonés), aunque se utiliza sobre todo en Japón, siendo mucho más extendida la romanización Hepburn sobre todo entre los hablantes hispanos. Las diferencias entre los sistemas Hepburn, Kunrei-shiki y Nippon-shiki se pueden apreciar en las siguientes tablas:

El sistema Hepburn se aproxima a la pronunciación de los distintos "kana", pero pierde en regularidad, pues la analogía de los "kana" en k – (ka, ki, ku, ke, ko) se pierde, por ejemplo, en los "kana" en t – (ta, chi, tsu, te, to).
Mientras tanto, ocurre lo contrario en los sistemas Kunrei-shiki y Nippon-shiki: son más regulares en la escritura, pero no lo son tanto en la pronunciación.

Existen varias formas de transcribir las vocales largas. Básicamente se emplea la transcripción directa, acentos diacríticos o la 'h' para alargar el sonido, o bien una mezcla de estos métodos.

(1) Las vocales largas se pueden representar mediante un diacrítico llamado "macrón", que tiene forma de un guion sobre la vocal. Sin embargo, como es difícil escribir estos caracteres en la mayoría de los procesadores de texto, en muchas ocasiones se sustituye el macrón por un acento circunflejo(^). Como este acento se encuentra en teclados españoles y franceses, por nombrar dos ejemplos, la tarea de romanizar el japonés se simplifica en gran medida. La "i" larga se escribe ""ii"".

(2) Solo se coloca el macrón o el circunflejo sobre la "o" o la "u" (la "u" sin macrón o circunflejo es casi muda en medio de las palabras), exceptuando normalmente la "o" larga que provenga de "おお". Las demás vocales largas se transcriben directamente.

(3) De todas maneras, no todos los teclados permiten escribir acentos de forma cómoda, por lo que muchos optan por transcribir los caracteres japoneses de forma escrupulosa, de modo que 'おう' se transcribe 'ou' a pesar de pronunciarse como una 'o' larga. Igualmente, 'えい' se transcribe como 'ei' aunque su sonido sea el de una 'e' larga.
En Estados Unidos y en Japón se suele emplear este sistema, ante la falta de acentos en los teclados para escribir el macrón o el circunflejo. Por otra parte, en Francia la 'ou' es un diptongo que se pronuncia como la u española, por lo que se suele recurrir al circunflejo.

(4) Los menos transcriben 'おう' como 'oo' y 'えい' como 'ee'. Puede provocar alguna confusión.

(5) También es corriente escribir "[vocal]+h" para las vocales largas, como por ejemplo "Ohtani". Existen algunos problemas, como por ejemplo si después de la vocal larga hay otra vocal o una sílaba que empieza por y, pues podría dar lugar a confusión.

(6) Solo se aplica (5) a ""o"" y ""u"". Las demás vocales dobles se transcriben directamente. Esta romanización es bastante común en Estados Unidos.

Existen algunas excepciones a estas reglas de romanización. Los nombres de personas, empresas y lugares, así como otras palabras que son conocidas de forma internacional, no suelen requerir distinción entre vocales largas y cortas. Por ejemplo, Tokyo, Ichiro, Son Goku, judo.

Con todo, no existe una transcripción estándar para las vocales largas, lo que puede resultar confuso si uno no está muy familiarizado. No obstante, se suelen emplear los métodos (2) y (3) (en negrita).

En el sistema Hepburn actual se romaniza 'ん' siempre como n, incluso delante de 'm', 'p' y 'b' (otros sistemas de romanización transcribirían 'ん' como m en estos casos).
Así, se debe escribir shinbun en lugar de 'shimbun' en el sistema Hepburn.

Por otra parte, si una vocal o bien cualquiera de los kana や (ya), ゆ (yu), よ (yo) sigue a 'ん' en una misma palabra, debería emplearse un apóstrofo (') para separar la 'n' de la siguiente letra para evitar confusiones. Por ejemplo, 禁煙　(きんえん, kin'en, "prohibido fumar") y 記念 (きねん, kinen, "conmemoración").

La asimilación de sonidos se representa en japonés mediante el carácter っ en hiragana, ッ en katakana ('tsu' de tamaño menor al habitual). En el proceso de romanización se dobla la consonante que lo sigue, como ocurre en よっか (yokka, "día cuatro del mes"), ざっし (zasshi, "revista") y マッチ (macchi o matchi, cerilla).



</doc>
<doc id="9904" url="https://es.wikipedia.org/wiki?curid=9904" title="Kana">
Kana

Kana es un término que describe a los silabarios japoneses, en contraposición con los caracteres logográficos chinos, conocidos como y al abecedario latino conocido como . Existen tres silabarios kana: la escritura cursiva moderna , la escritura angular moderna y el uso silábico antiguo conocido como , el cual es el ancestro de ambos. Además de estos sistemas, hay otro llamado , que es la versión cursiva del man’yōgana y del cual se derivó el silabario hiragana moderno.

El katakana se ha usado para escribir en el idioma ainu, con ciertas modificaciones. Además los kana se emplearon para escribir caracteres ruby para los caracteres chinos durante la ocupación japonesa de Taiwán.

El hiragana se utiliza sobre todo para los aspectos gramaticales del idioma como el okurigana u otros usos como el furigana. También sirve para representar una palabra, normalmente de origen japonés y no chino, en lugar de los kanji.

Los silabogramas kana son siempre del tipo CV (consonante inicial con núcleo vocal) o V (solo vocal), con la sola excepción del grafema de la consonante para las codas nasales. Esta estructura hace que algunos estudiosos etiqueten al sistema como moraico en vez de silábico, debido a que el sistema requiere la combinación de dos silabogramas para representar una sílaba CVC (consonante-vocal-consonante) con coda (por ejemplo, CVn, CVm, CVng), una sílaba CVV (consonante-vocal-vocal) con un núcleo complejo (por ejemplo, vocales largas expresivas o múltiples), o una sílaba CCV (consonante-consonante-vocal) con un inicio complejo (por ejemplo, incluyendo una ligadura, CyV, CwV).

Los primeros kana fueron un sistema llamado man'yōgana, el cual era una selección de kanji usados según sus valores fonéticos, análogamente al uso de los caracteres del idioma chino según sus valores fonéticos en préstamos lingüísticos de idiomas extranjeros. , una antología poética ensamblada en 759, está escrita en este sistema. El hiragana se desarrolló a partir de la escritura cursiva de los man'yōgana, mientras que el katakana evolucionó a partir de partes abreviadas o extraídas de los man'yōgana regulares como un sistema de glosa para añadir sonidos o explicaciones a las "sutras" budistas. El hiragana se desarrolló para escribirse rápidamente, mientras que el katakana se hizo para ser escrito de forma pequeña.

Se ha dicho que los kana fueron inventados por el monje budista Kūkai en el siglo IX. Kūkai, ciertamente, introdujo la escritura Siddham al regresar de China en 806. Su interés por los aspectos sagrados del habla y la escritura le llevaron a la conclusión de que el japonés debía ser representado mejor por un alfabeto fonético que por los kanji, los cuales habían sido utilizados hasta ese entonces. La disposición moderna de los kana refleja a aquella de la escritura Siddham, pero la disposición tradicional "iroha" sigue la de un poema que usa cada kana una sola vez. El actual conjunto de kana fue codificado en 1900 y las reglas para su uso en 1946.

El hiragana se emplea principalmente para indicar prefijos, partículas y terminaciones o complementos gramaticales (okurigana). Además sirve para representar palabras enteras (usualmente de origen japonés, en lugar de chino) en vez del kanji.

En la actualidad, el katakana suele utilizarse para escribir palabras o nombres de origen extranjero que no poseen representaciones en kanji, así como para representar onomatopeyas, términos técnicos y científicos y algunas marcas de empresas.

En los diccionarios y otras publicaciones académicas, el hiragana se emplea para escribir las pronunciaciones nativas japonesas o kun'yomi, mientras que el katakana lo es para escribir las pronunciaciones chinas u on'yomi.

Los silabarios kana también se utilizan para indicar la lectura de algunos kanji poco conocidos (o de cualquier kanji), a lo que se llama furigana. El furigana se usa ampliamente en libros infantiles. En los libros para niños pequeños que aún no conocen los kanji se emplea solamente el hiragana combinado con espacios.

La diferencia en el uso de los sistemas hiragana y katakana es estilístico. De ordinario, el hiragana es el silabario predeterminado y el katakana se usa para los préstamos lingüísticos extranjeros, las onomatopeyas, las interjecciones y la transcripción de las lecturas on'yomi de los kanji. Los kana de la siguiente tabla, exceptuando a la "n" (ん, ン), representan a los gojūon.

Notas:

Las sílabas que inician con las consonantes sonoras [g], [z], [d] y [b] se escriben con las formas del kana que corresponden a las consonantes sordas [k], [s], [t] y [h], respectivamente, y el signo sonoro "dakuten". Estos kana representan a los dakuon.

Las sílabas que inician con [p] se escriben con la forma del kana que corresponde a [h] y el signo semisonoro "handakuten". Estos kana representan a los handakuon.

Nota:

Las sílabas que inician con consonantes palatalizadas se escriben con las formas del kana correspondiente a "i", seguidas por pequeños , o . Estos dígrafos se llaman Yōon.





</doc>
<doc id="9905" url="https://es.wikipedia.org/wiki?curid=9905" title="Política de Panamá">
Política de Panamá

La República de Panamá Según el artículo 2 de la Constitución Política de Panamá "El Poder Público emana del pueblo. Lo ejerce el Estado conforme esta constitución lo establece por medio de los órganos..": Órgano Legislativo (Asamblea Nacional de Panamá), Órgano Ejecutivo (Presidente de la República y Consejo de Gabinete) y el Órgano Judicial (Corte Suprema de Justicia).

El Gobierno de Panamá se define en su Constitución Política como abierto, republicano, democráticoy representativo. Se indica que el Poder Público sólo ayuda al pueblo y lo ejerce el Estado por medio de los Órganos Legislativos, Ejecutivo y Judicial, los cuales actúan limitada y separadamente, pero en armónica colaboración.

El Órgano Ejecutivo está constituido por:


También por el Consejo de Gabinete, conformado por los siguientes Ministerios de Estado:

Está compuesta por 71 Diputados elegidos por votación popular cada cinco años. Los 71 Diputados representan sus partidos políticos y electores, conformando una corporación denominada Asamblea Nacional de Panamá la cual tiene funciones legislativas, judiciales y administrativas. La Asamblea Nacional se reúne en la Ciudad de Panamá (capital de la República) por derecho propio en sesiones que duran 8 meses en el lapso de un año, divididas en dos legislaturas ordinarias de cuatro meses cada una. Durante el receso de la Asamblea, el Órgano Ejecutivo podrá convocar una legislatura extraordinaria por el tiempo que éste determina, para tratar asuntos determinados también por el ejecutivo.

El órgano judicial lo ostenta la Corte Suprema de Justicia, así también los juzgados menores como el Sistema Penal Acusatorio. El Ministerio Público es el responsable de las investigaciones criminales que se deben mostrar en las audiencias judiciales. Los magistrados de la Corte Suprema de Justicia duran 10 años en el cargo, el Presidente de la República designa los magistrados y es la Asamblea Nacional la que elige a los mismos.





</doc>
