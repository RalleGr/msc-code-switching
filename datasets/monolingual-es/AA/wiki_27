<doc id="4898" url="https://es.wikipedia.org/wiki?curid=4898" title="GPS">
GPS

El Sistema de Posicionamiento Global (GPS; ), y originalmente NAVSTAR GPS, es un sistema que permite determinar en toda la Tierra la posición de cualquier objeto (una persona, un vehículo) con una precisión de hasta centímetros (si se utiliza GPS diferencial), aunque lo habitual son unos pocos metros de precisión. El sistema fue desarrollado, instalado y empleado por el Departamento de Defensa de Estados Unidos, y actualmente es propiedad de la Fuerza Espacial de los Estados Unidos. Para determinar su posición, un usuario utiliza cuatro o más satélites y utiliza la trilateración. 

El GPS funciona mediante una red de como mínimo veinticuatro satélites en órbita sobre el planeta Tierra, a aproximadamente 20.000 km de altura, con órbitas distribuidas para que en todo momento haya al menos cuatro satélites visibles en cualquier punto de la tierra. Cuando se desea determinar la posición tridimensional, el receptor que se utiliza para ello localiza automáticamente como mínimo cuatro satélites de la red, de los que recibe unas señales indicando la identificación y hora del reloj de cada uno de ellos, además de información sobre la constelación. Con base en estas señales, el aparato sincroniza su propio reloj con el tiempo del sistema GPS y calcula el tiempo que tardan en llegar las señales al equipo, y de tal modo mide la distancia al satélite. Mediante el método de trilateración inversa, computa su propia posición. Se calcula también con una gran exactitud en el tiempo, basado en los relojes atómicos a bordo cada uno de los satélites y en el segmento terreno de GPS.

La antigua Unión Soviética construyó un sistema similar llamado GLONASS, ahora gestionado por la Federación Rusa.

La Unión Europea desarrolló el sistema de navegación Galileo. En diciembre de 2016 la Comisión Europea, propietaria del sistema, informó que el sistema de navegación Galileo comenzó sus operaciones y que los satélites ya envían información de posicionamiento, navegación y determinación de la hora a usuarios de todo el mundo.

La República Popular China está implementando su propio sistema de navegación, el denominado Beidou, que está previsto que cuente con 12 y 14 satélites entre 2011 y 2015. Para 2020, ya plenamente operativo deberá contar con 30 satélites. En diciembre de 2012 tenían 14 satélites en órbita.

En la década de 1960, el sistema de navegación terrestre OMEGA, basado en la comparación de fase de las señales emitidas a partir de pares de estaciones terrestres, se convirtió en el primer sistema mundial de radio de navegación. Las limitaciones de estos sistemas impulsaron la necesidad de una solución de navegación más universal con más precisión. 

La armada estadounidense aplicó esta tecnología de navegación utilizando satélites para proveer a los sistemas de navegación de sus flotas observaciones de posiciones actualizadas y precisas. El sistema debía cumplir los requisitos de globalidad, abarcando toda la superficie del globo; continuidad, funcionamiento continuo sin afectarle las condiciones atmosféricas; altamente dinámico, para posibilitar su uso en aviación y precisión. Esto llevó a producir diferentes experimentos como el Timation y el sistema 621B en desiertos simulando diferentes comportamientos. 

Así surgió el sistema TRANSIT, que quedó operativo en 1964, y hacia 1967 estuvo disponible, además, para uso comercial militar. TRANSIT estaba constituido por una constelación de seis satélites en órbita polar baja, a una altitud de 1074km. Tal configuración conseguía una cobertura mundial, pero no constante. La posibilidad de posicionarse era intermitente, pudiéndose acceder a los satélites cada 1,5 horas. El cálculo de la posición requería estar siguiendo al satélite durante quince minutos continuamente.

En 1967, la U.S. Navy desarrolló el satélite Timation, que demostró la viabilidad de colocar relojes precisos al espacio, una tecnología requerida por el GPS.

Posteriormente, en esa misma década y gracias al desarrollo de los relojes atómicos, se diseñó una constelación de satélites, portando cada uno de ellos uno de estos relojes y estando todos sincronizados con base en una referencia de tiempo determinado.

En 1973 se combinaron los programas de la Armada y de la Fuerza Aérea de los Estados Unidos (este último consistente en una técnica de transmisión codificada que proveía datos precisos usando una señal modulada con un código de PRN ("Pseudo-Random Noise": ruido pseudoaleatorio), en lo que se conoció como "Navigation Technology Program" (programa de tecnología de navegación), posteriormente renombrado NAVSTAR GPS.

Entre 1978 y 1985 se desarrollaron y lanzaron once satélites prototipo experimentales NAVSTAR, a los que siguieron otras generaciones de satélites, hasta completar la constelación actual, a la que se declaró con «capacidad operacional inicial» en diciembre de 1993 y con «capacidad operacional total» y utilidad civil en abril de 1995.

En 2009, el gobierno de los Estados Unidos ofreció el servicio normalizado de determinación de la posición para apoyar las necesidades de la OACI, y ésta aceptó el ofrecimiento.

El Sistema Global de Navegación por Satélite lo componen:


Cada satélite GPS emite continuamente un mensaje de navegación a 50 bits por segundo en la frecuencia transportadora de microondas de aproximadamente 1600 MHz. La radio FM, en comparación, se emite a entre 87,5 y 108,0 MHz y las redes Wi-Fi funcionan a alrededor de 5000 MHz y 2400 MHz. Más concretamente, todos los satélites emiten a 1575,42 MHz (esta es la señal L1) y 1227,6 MHz (la señal L2).

La señal GPS proporciona la “hora de la semana” precisa de acuerdo con el reloj atómico a bordo del satélite, el número de semana GPS y un informe de estado para el satélite de manera que puede deducirse si es defectuoso. Cada transmisión dura 30 segundos y lleva 1500 bits de datos codificados. Esta pequeña cantidad de datos está codificada con una secuencia pseudoaleatoria (PRN) de alta velocidad que es diferente para cada satélite. Los receptores GPS conocen los códigos PRN de cada satélite y por ello no solo pueden decodificar la señal sino que la pueden distinguir entre diferentes satélites.

Las transmisiones son cronometradas para empezar de forma precisa en el minuto y en el medio minuto tal como indique el reloj atómico del satélite. La primera parte de la señal GPS indica al receptor la relación entre el reloj del satélite y la hora GPS. La siguiente serie de datos proporciona al receptor información de órbita precisa del satélite.


El programa GPS III persigue el objetivo de garantizar que el GPS satisfaga requisitos militares y civiles previstos para los próximos 30 años. Este programa se está desarrollando para utilizar un enfoque en tres etapas (una de las etapas de transición es el GPS II); muy flexible, permite cambios futuros y reduce riesgos. El desarrollo de satélites GPS II comenzó en 2005, y el primero de ellos estará disponible para su lanzamiento en 2012, con el objetivo de lograr la transición completa de GPS III en 2017. Los desafíos son los siguientes:


El sistema ha evolucionado y de él han derivado nuevos sistemas de posicionamiento, como sistemas de posicionamiento dinámicos, un sistema de captura de datos, que permite al usuario realizar mediciones en tiempo real y en movimiento, el llamado Mobile Mapping.
Este sistema obtiene cartografía móvil 3D basándose en un aparato que recoge un escáner láser, cámaras métricas, un sensor inercial (IMU), sistema GNSS y un odómetro a bordo de un vehículo.
Se consiguen grandes precisiones, gracias a las tres tecnologías de posicionamiento: IMU + GNSS + odómetro, que trabajando a la vez dan la opción de medir incluso en zonas donde la señal de satélite no es buena.

La información que es útil al receptor GPS para determinar su posición se llama efemérides. En este caso cada satélite emite sus propias efemérides, en la que se incluye la salud del satélite,su posición en el espacio, su hora atómica, información doppler, etc.

Mediante la trilateración se determina la posición del receptor:

Debido al carácter militar del sistema GPS, el Departamento de Defensa de los EE.UU. se reservaba la posibilidad de incluir un cierto grado de error aleatorio, que podía variar de los 15 a los 100 m. La llamada disponibilidad selectiva (S/A) fue eliminada el 2 de mayo de 2000. Aunque actualmente no aplique tal error inducido, la precisión intrínseca del sistema GPS depende del número de satélites visibles en un momento y posición determinados.

Si se capta la señal de entre siete y nueve satélites, y si éstos están en una geometría adecuada (están dispersos), pueden obtenerse precisiones inferiores a 2,5 metros en el 95 % del tiempo. Si se activa el sistema DGPS llamado SBAS (WAAS-EGNOS-MSAS), la precisión mejora siendo inferior a un metro en el 97 % de los casos. Estos sistemas SBAS no se aplican en Sudamérica, ya que esa zona no cuenta con este tipo de satélites geoestacionarios. La funcionabilidad de los satélites es por medio de triangulación de posiciones para proporcionar la posición exacta de los receptores (celulares, vehículos, etc.).

La posición calculada por un receptor GPS requiere en el instante actual la posición del satélite y el retraso medido de la señal recibida. La precisión es dependiente de la posición y el retraso de la señal.

Al introducir el retraso, el receptor compara una serie de bits (unidad binaria) recibida del satélite con una versión interna mediante (un motor de correlación cableado en un chip especializado, basado en la patente Gronemeyer'216). Cuando se comparan los límites de la serie, las electrónicas pueden fijar la diferencia a 1 % de un tiempo BIT, o aproximadamente 10 nanosegundos por el código C/A. Desde entonces las señales GPS se propagan a la velocidad de luz, que representa un error de 3 metros. Este es el error mínimo posible usando solamente la señal GPS C/A.

La precisión de la posición se mejora con una señal P(Y). Al presumir la misma precisión de 1 % de tiempo BIT, la señal P(Y) (alta frecuencia) resulta en una precisión de más o menos 30 centímetros. Los errores en las electrónicas son una de las varias razones que perjudican la precisión (ver la tabla).

Puede también mejorarse la precisión, incluso de los receptores GPS estándares (no militares) mediante software y técnicas de tiempo real. Esto ha sido puesto a prueba sobre un sistema global de navegación satelital (GNSS) como es el NAVSTAR-GPS. La propuesta se basó en el desarrollo de un sistema de posicionamiento relativo de precisión dotado de receptores de bajo costo. La contribución se dio por el desarrollo de una metodología y técnicas para el tratamiento de información que proviene de los receptores.


Factores que Afectan la Calidad de los Datos:

Errores Propios del Satélite.

Se refiere a los errores que afectan la calidad de los resultados obtenidos en una medición GPS.

Errores orbitales (efemérides): Debido a que los satélites no siguen una órbita kepleriana normal por causa de las perturbaciones, se requieren mejores estimadores de órbitas, lo que implica un proceso que está obstaculizado por conocimientos insuficientes de las fuerzas que actúan sobre los satélites. Estos errores afectan la determinación de la posición del satélite en un instante determinado con respecto a un sistema de referencia seleccionado. Para disminuir el error en vez de utilizar las efemérides captadas en el receptor se utilizan efemérides precisas calculadas por el IGS y NASA días después de la medición.

Errores del reloj: Se refieren a las variaciones en el sistema de tiempo del reloj del satélite, producidas por la deriva propia de los osciladores y las originadas por la acción de los efectos relativísticos. Dichos errores conllevan a que exista un diferencial entre el sistema de tiempo del satélite y del sistema GPS, el cual no va a ser constante para todos los satélites sino que varia de uno a otro, debido a que la frecuencia estándar de los osciladores de los satélites tiene valores definidos para cada satélite.

Errores de la configuración geométrica: las incertidumbres en un posicionamiento son consecuencia de los errores de las distancias asociadas con las geometrías de los satélites utilizados, cuatro o más. El efecto de la geometría queda expresado por los parámetros de la denominada "Dilución de Precisión Geométrica" (GDOP), el cual considera los tres parámetros de posición tridimensional y tiempo. El valor de GDOP es una medida compuesta que refleja la influencia de la constelación de satélites sobre la precisión combinada de las estimaciones de un tiempo y posición de la estación. 

Al efecto se consideran: PDOP: Dilución de precisión para la posición. HDOP: Dilución de precisión para la posición. VDOP: Dilución de precisión vertical. TDOP: Dilución de precisión para el tiempo.

Errores provenientes del medio de propagación.

Errores de refracción ionosférica: En la frecuencia GPS, el rango del error por refracción en la ionósfera va desde 50 metros (máxima, al mediodía, un satélite cerca del horizonte) hasta 1 metro (mínima, en la noche, un satélite en el zenit). Debido a que la refracción ionosférica depende de la frecuencia, el efecto es estimado comparando mediciones realizadas en dos frecuencias diferentes (L1=1575.42MHz. y L2=1227.60MHz.). Usando dos estaciones, una con coordenadas conocidas. Podemos corregir errores de tiempo. El retardo del tiempo de viaje en la ionosfera depende de la densidad de electrones a lo largo del camino de la señal y de la frecuencia de la misma. Una fuente influyente sobre la densidad de los electrones es la densidad solar y el campo magnético terrestre. Por lo tanto la refracción ionosférica depende de la hora y del sitio de medición.

Errores de refracción troposférica: La refracción troposférica produce errores comprendidos entre 2 metros (satélite en el zenit) y 25 metros (satélite a 5º de elevación). La refracción troposférica es independiente de la frecuencia, por lo tanto una medición de dos frecuencias no puede determinar el efecto pero este error puede ser compensado usando modelos troposféricos. 

Multipath: Es el fenómeno en el cual la señal llega por dos o más trayectorias diferentes. La diferencia en las longitudes de las trayectorias causa interferencia de las señales al ser recibidas. El multipath se nota usualmente cuando se está midiendo cerca de superficies reflectoras, para minimizar sus efectos se utiliza una antena capaz de hacer discriminaciones en contra de las señales que llegan de diferentes direcciones. 

Errores en la recepción.

Estos errores dependen tanto del modo de medición como del tipo de receptor que se utiliza.

Ruido: Como la desviación estándar del ruido en la medición es proporcional a la longitud de onda en el código.El ruido en las medidas de fase de la portadora condiciona la cantidad de datos y el tiempo de seguimiento requeridos para alcanzar un determinado nivel de precisión, resultando crucial el seguimiento y las mediciones continuas para asegurar dicha precisión.

Centro de fase de la antena: Este puede cambiar en función del ángulo de elevación del azimut (figura 15). El aparente centro de fase eléctrico de la antena GPS es el punto preciso de navegación para trabajos relativos. Si el error del centro de fase de la antena es común para todos los puntos durante la medición, estos se cancelan. En mediciones relativas se usan todas las antenas de la red alineadas en una misma dirección (usualmente el norte magnético) para que el movimiento del centro de fase de la antena sea común y se cancele con una primera aproximación.

El DGPS (Differential GPS), o GPS diferencial, es un sistema que proporciona a los receptores de GPS correcciones de los datos recibidos de los satélites GPS, con el fin de proporcionar una mayor precisión en la posición calculada. Se concibió fundamentalmente debido la introducción de la disponibilidad selectiva (SA).

El fundamento radica en el hecho de que los errores producidos por el sistema GPS afectan por igual (o de forma muy similar) a los receptores situados próximos entre sí. Los errores están fuertemente correlacionados en los receptores próximos.

Un receptor GPS fijo en tierra (referencia) que conoce exactamente su posición basándose en otras técnicas, recibe la posición dada por el sistema GPS, y puede calcular los errores producidos por el sistema GPS, comparándola con la suya, conocida de antemano. Este receptor transmite la corrección de errores a los receptores próximos a él, y así estos pueden, a su vez, corregir también los errores producidos por el sistema dentro del área de cobertura de transmisión de señales del equipo GPS de referencia.

En suma, la estructura DGPS quedaría de la siguiente manera:


Existen varias formas de obtener las correcciones DGPS. Las más usadas son:


En los mensajes que se envían a los receptores próximos se pueden incluir dos tipos de correcciones:


El error producido por la disponibilidad selectiva (SA) varía incluso más rápido que la velocidad de transmisión de los datos. Por ello, junto con el mensaje que se envía de correcciones, también se envía el tiempo de validez de las correcciones y sus tendencias. Por tanto, el receptor deberá hacer algún tipo de interpolación para corregir los errores producidos.

Si se deseara incrementar el área de cobertura de correcciones DGPS y, al mismo tiempo, minimizar el número de receptores de referencia fijos, será necesario modelar las variaciones espaciales y temporales de los errores. En tal caso estaríamos hablando del GPS diferencial de área amplia.

Con el DGPS se pueden corregir en parte los errores debidos a:


Para que las correcciones DGPS sean válidas, el receptor tiene que estar relativamente cerca de alguna estación DGPS; generalmente, a menos de 1000km. Las precisiones que manejan los receptores diferenciales son centimétricas, por lo que pueden ser utilizados en ingeniería. Permite determinar en todo el mundo la posición de un objeto, una persona o un vehículo con una precisión hasta de centímetros.


Actualmente dentro del mercado de la telefonía móvil la tendencia es la de integrar, por parte de los fabricantes, la tecnología GPS dentro de sus dispositivos. El uso y masificación del GPS está particularmente extendido en los teléfonos móviles smartphone, lo que ha hecho surgir todo un ecosistema de software para este tipo de dispositivos, así como nuevos modelos de negocios que van desde el uso del terminal móvil para la navegación tradicional punto-a-punto hasta la prestación de los llamados Servicios Basados en la Localización (LBS).

Un buen ejemplo del uso del GPS en la telefonía móvil son las aplicaciones que permiten conocer la posición de amigos cercanos sobre un mapa base. Para ello basta con tener la aplicación respectiva para la plataforma deseada (Android, Bada, IOS, WP, Symbian) y permitir ser localizado por otros.

Las nuevas tendencias en el mundo del fitness, el deporte y el ocio han llevado a la aparición de smartwatches con capacidades GPS, hay distintos tipos de dispositivos wearables que usan la tecnología GPS y suelen funcionar combinados con el teléfono inteligente si se trata de dispositivos sin pantallas como son los relojes deportivos de marcas como Garmin con su gama Forerunner o Polar y las pulseras inteligentes como las de Xiaomi o de manera independiente en los relojes inteligentes como el Apple Watch o el Samsung Gear S3.

Como con los teléfonos inteligentes las utilidades son las aplicaciones que permiten conocer la posición de amigos cercanos sobre un mapa base o el seguimiento del recorrido realizado. Para ello basta con tener la aplicación respectiva para la plataforma deseada (Android, Bada, IOS, WP, Symbian). Algunas aplicaciones no requieren conectar con el teléfono inteligente y proporcionan detalles de manera independiente en la pantalla del dispositivo.

Los relojes en los satélites GPS requieren una sincronización con los situados en tierra para lo que hay que tener en cuenta la teoría general de la relatividad y la teoría especial de la relatividad. Los tres efectos relativistas son: la dilatación del tiempo, cambio de frecuencia gravitacional, y los efectos de la excentricidad. La desaceleración relativista del tiempo debido a la velocidad del satélite es de aproximadamente 1 parte de 10, la dilatación gravitacional del tiempo hace que el reloj del satélite gira alrededor de 5 partes entre 10 más rápido que un reloj basado en la Tierra, y el efecto Sagnac debido a rotación con relación a los receptores en la Tierra. Si no se tuviese en cuenta el efecto que sobre el tiempo tiene la velocidad del satélite y su gravedad respecto a un observador en tierra, se produciría un corrimiento de 38 microsegundos por día, que a su vez provocarían errores de varios kilómetros en la determinación de la posición.

De acuerdo con la teoría de la relatividad, debido a su constante movimiento y la altura relativa respecto, aproximadamente, un marco de referencia inercial no giratorio centrado en la Tierra, los relojes de los satélites se ven afectados por su velocidad. La relatividad especial predice que la frecuencia de los relojes atómicos moviéndose a velocidades orbitales del GPS, unos v = 4 km/s, marcar más lentamente que los relojes terrestres fijos en un factor de formula_1, o resultar un retraso de unos 7 μs/día, siendo c = velocidad de la luz en el vacío.

El efecto de desplazamiento de frecuencia gravitacional sobre el GPS, la relatividad general predice que un reloj más cercano a un objeto masivo será más lento que un reloj más alejado. Aplicado al GPS, los receptores están mucho más cerca de la Tierra que los satélites, haciendo los relojes del GPS ser más rápido en un factor de 5 × 10 , o alrededor de 45,9 μs/día.

Al combinar la dilatación del tiempo y desplazamiento de frecuencia gravitacional, la discrepancia es de aproximadamente 38 microsegundos por día, una diferencia de 4,465 partes de 10. Sin corrección, los errores en la pseudodistancia inicial se acumularía aproximadamente unos 10 km/día. Este error en la pseudodistancia inicial se corrige en el proceso de resolución de las ecuaciones de navegación. Además las órbitas de los satélite son elípticas, en lugar de perfectamente circulares, lo que causa que los efectos de la dilatación del tiempo y desplazamiento de la frecuencia gravitacional varíen con el tiempo. Este efecto excentricidad hace que la diferencia de velocidad de reloj entre un satélite GPS y un receptor aumente o disminuya en función de la altitud del satélite.

Para compensar esta discrepancia, al patrón de frecuencia a bordo de cada satélite se le da una tasa de compensación antes del lanzamiento, por lo que marcha un poco más lento que la frecuencia de trabajo en la Tierra. Concretamente, trabaja a 10.22999999543 MHz en lugar de 10,23 MHz Dado que el reloj atómico a bordo de los satélites GPS se ajusta con precisión, hace que el sistema sea una aplicación práctica de la teoría científica de la relatividad en un ambiente del mundo real. Friedwardt Winterberg propuso colocar relojes atómicos en satélites artificiales para poner a prueba la teoría general de Einstein en 1955.

El procesamiento de la observación GPS también debe compensar el efecto Sagnac. La escala de tiempo del GPS se define en un sistema inercial, pero las observaciones se procesan en un sistema centrado en la Tierra, fijo a la Tierra (co-rotación), un sistema en el que la simultaneidad no está definida de forma única. Se aplica una transformación de Lorentz, pues, para convertir del sistema de inercia al sistema ECEF. El recorrido señal resultante de corrección de tiempo tiene signos algebraicos opuestos de los satélites en los hemisferios celestes oriental y occidental. Haciendo caso omiso de este efecto se producirá un error de este a oeste en el orden de cientos de nanosegundos, o decenas de metros de su posición.






</doc>
<doc id="4900" url="https://es.wikipedia.org/wiki?curid=4900" title="Ontología (informática)">
Ontología (informática)

En ciencias de la computación y ciencias de la comunicación, una "ontología" es una definición formal de tipos, propiedades, y relaciones entre entidades que realmente o fundamentalmente existen para un dominio de discurso en particular. Es una aplicación práctica de la ontología filosófica, con una taxonomía.

Una ontología cataloga las variables requeridas para algún conjunto de computación y establece las relaciones entre ellos.
En los campos de la inteligencia artificial, la Web Semántica, ingeniería de sistemas, ingeniería de software, informática biomédica, bibliotecología y arquitectura de la información se crean ontologías para limitar la complejidad y para organizar la información. La ontología puede entonces ser aplicada para resolver problemas.

El término "ontología" tiene su origen en la filosofía y ha sido aplicado en muchas formas diferentes. Proviene de "onto-" de Griego "ὤν, ὄντος", (""lo que se es"), presente participio del verbo "εἰμί" ("ser"). El significado en ciencia de la computación es un modelo para describir el mundo que consiste en un conjunto de tipos, propiedades y relaciones entre tipos. También se espera que lo representado por modelo en una ontología sea lo más semejante posible al mundo real. (en relación con el objeto).
Lo que muchas ontologías tienen en común, tanto en Informática como en Filosofía, es la representación de entidades, ideas y eventos, junto con sus propiedades y relaciones, de acuerdo con su sistema de categorización. En ambos campos hay trabajo considerable sobre los problemas relativos a la ontología (e.g., Quine y Kripke en Filosofía, Sowa y Guarino en Informática), y debates correspondientes a que si la ontología normativa es viable (Ej., debates sobre fundamentalismo en filosofía, y sobre el proyecto Cyc en inteligencia artificial). La diferencia entre los dos es en la manera como se enfocan. Los informáticos están más preocupados por un establecimiento fijo y vocabularios controlados, mientras que los filósofos están más preocupados por los principios, es decir si existen cosas tales como una esencia fija o si las entidades deben primar ontológicamente sobre los procesos.

Las ontologías provienen de la rama de la filosofía conocida como metafísica, la cual tiene que ver con la naturaleza de la realidad  – de lo que existe. Esta rama se preocupa por el análisis de varios tipos o modos de existencia, frecuentemente con especial atención en las relaciones entre lo particular y lo universal, entre las propiedades intrínsecas y extrínsecas y entre la esencia y la existencia. El objetivo tradicional del análisis ontológico es de dividir el mundo "en conjuntos" para descubrir aquellas categorías o tipos fundamentales en el cual los objetos del mundo están naturalmente. 

Durante la segunda mitad del siglo XX, los filósofos debatieron exhaustivamente los posibles métodos o aproximaciones para construir ontologías sin ser realmente "construidas" en cualquier ontología elaborada por ellos. En contraste, los científicos de la computación fueron construyendo algunas ontologías grandes y robustas, tales como WordNet y Cyc, con debates de "como" deberían ser construidas.

Desde mediados de los 1970, investigadores en el campo de la inteligencia artificial (IA) habían reconocido que capturar el conocimiento es la clave para construir grandes y poderosos sistemas de IA. Los investigadores de IA argumentaron que ellos podrían crear nuevas ontologías como modelos computacionales que permitan cierto grado de razonamiento automático. En los años 80, la comunidad de IA comenzó a usar el término "ontología" para referirse a la teoría del mundo modelado y a una componente de los sistemas de conocimiento. Algunos investigadores, se inspiraron de algunas ontologías filosóficas, viendo una ontología computacional como un tipo de filosofía aplicada.

A principios de los 90, una página web y un artículo muy citados "Toward Principles for the Design of Ontologies Used for Knowledge Sharing" por Tom Gruber fue reconocido como una definición deliberada de " ontología " como un término técnico en ciencia de la computación. Gruber introdujo el término para referirse a una especificación de una conceptualización: Una ontología es una descripción (como una especificación formal de un programa) de los conceptos y relaciones que pueden formalmente existir para un agente o comunidad de agentes. Esta definición es consistente con el uso de ontología como un conjunto de definiciones conceptuales, pero más generales. Y esto es un sentido diferente de la palabra ontología utilizada en filosofía.

De acuerdo con Gruber (1993): Ontologías son frecuentemente asociadas con jerarquías taxonómicas de las clases, definición de clases y las relaciones, pero ontologías necesitan no ser limitadas a estas formas. Ontologías no son limitadas tampoco a las definiciones conservadoras — o sea, definiciones en el sentido lógico tradicional que solamente introduce terminología y no añade ningún conocimiento acerca del mundo. Para especificar una conceptualización , se necesitan establecer axiomas que limiten las posibles interpretaciones para los términos definidos .En 1997, Borst hizo más específica la definición de Gruber al afirmar que una ontología es "una especificación formal de una conceptualización compartida". Por lo que la conceptualización ontológica debe expresar una visión compartida entre varias partes, un consenso más que un punto de vista individual. Además, tal conceptualización debe estar expresada en un lenguaje formal para que pueda ser procesada por una computadora.

Las ontologías contemporáneas comparten muchas similitudes estructurales, indiferente al lenguaje en el cual ellos fueron expresados. Como se mencionó arriba, la mayoría de las ontologías describen individuos (instancias), clases (conceptos), atributos y relaciones. En esta sección cada uno de estos componentes será discutido:

Los componentes más comunes de una ontología son:

Las ontologías se suelen codificar usando lenguajes de ontologías.

La ontología de dominio (u ontología de dominio específico) representa conceptos que pertenecen a una parte del mundo. El significado particular de un término aplicado a ese dominio es proporcionado por el dominio de la ontología. Por ejemplo, la palabra "tarjeta" tiene muchos significados. Una ontología acerca del dominio banco podría modelar el significado a "tarjeta de crédito", mientras que una ontología acerca del dominio de hardware de computadoras podría modelar los conceptos a "tarjeta de red" y "tarjeta gráfica".

Como las ontologías de conceptos representan conceptos de manera muy específica, normalmente son muy incompatibles. Como sistemas que dependen de ontologías de dominio expandidas, normalmente necesitan mezclar ontologías de dominio dentro de una representación más general. Esto representa un reto para el diseño de una ontología. Diferentes ontologías en el mismo dominio son hechas en diferentes lenguajes, diferentes intentos de uso de la ontología y diferentes percepciones del dominio (basados en la formación cultural, educación, ideología, etc.).

Actualmente, mezclar ontologías que no están desarrolladas desde una ontología común básica, es un proceso manual muy caro y largo. Las ontologías de dominio que usan la misma ontología básica que provee un conjunto de elementos básicos con los cuales especificar el significado de los elementos de la ontología de dominio puede ser mezclado automáticamente. Hay estudios en técnicas generalizadas para mezclar ontologías, pero esta área sigue siendo muy teórica.

Representan conceptos generales que no son específicos de un dominio. Por ejemplo, ontologías sobre el tiempo, ontologías de conducta, de causalidad, etc. Pueden reutilizarse a través de diferentes dominios.

Proporcionan el vocabulario para describir términos involucrados en los procesos de resolución de problemas los cuales pueden estar relacionados con tareas similares en el mismo dominio o en dominios distintos. Incluyen nombres, verbos, frases y adjetivos relacionados con la tarea (“objetivo”, “planificación”, “asignar”, “clasificar”, etc.).

Especifican los términos que son usados para representar conocimiento en el universo de discurso. Suelen usarse para unificar vocabulario en un dominio determinado (contenido léxico y no semántico). Conocidas también como ontologías lingüísticas.

Especifican la estructura de almacenamiento de bases de datos. Ofrecen un marco para el almacenamiento estandarizado de información (estructura de los registros de una BD).

Especifican conceptualizaciones del conocimiento. Poseen una rica estructura interna y suelen estar ajustadas al uso particular del conocimiento que describen (términos y semántica).

Un estudio de técnicas visualización de ontologías está presentada por Katifori et al. Una evaluación de las dos técnicas de visualización de ontologías más usadas: árboles y grafos es discutido en. Un lenguaje visual para ontologías representadas en OWL es especificada por Visual Notation for OWL Ontologies (VOWL).

Ingeniería de Ontología (o construcción de ontologías) es una rama de ingeniería del conocimiento. Estudia el proceso de desarrollo de la ontología, su ciclo de vida, los métodos y las metodologías para construir ontologías, así como las herramientas y lenguajes que los soportan.

La ingeniería de ontología tiene como objetivo hacer explícito el contenido dentro de las aplicaciones de software, y dentro de los procedimientos de negocios y empresas para un dominio en particular. La ingeniería de ontología ofrece una dirección hacia la solución de los problemas interpretativos traídos por los obstáculos de la semántica, tales como los relacionados con las definiciones de términos de negocios y las clases de software. La ingeniería de ontología es un conjunto de tareas relacionadas con el desarrollo de las ontologías en un dominio específico.

El aprendizaje de una ontología es una creación automático o semiautomático de ontologías, incluyendo extraer término de un dominio de un texto en lenguaje natural. Como construir manualmente una ontología es una labor intensamente compleja y consume mucho tiempo, hay una motivación para automatizar el proceso.
Extracción de información y métodos de minería de datos han sido explotados para unir automáticamente ontologías con documentos, ej. En el contexto de los retos BioCreative. 

Un lenguaje de ontología es un lenguaje formal usado para codificar una ontología. Hay un gran número de dichos lenguajes:


El desarrollo de ontologías para la web ha conducido a que emerjan servicios proveyendo listas o directorios de ontologías con facilidad de búsqueda. Tales directorios han sido llamados bibliotecas de ontología.
Algunos ejemplos:
Los siguientes son directorios y motores de búsqueda al mismo tiempo. Ellos incluyen búsqueda con crawlers.


Werner Ceusters ha notado la confusión causada por los diferentes significados de la palabra ontología cuando es usado en filosofía comparada con el uso de la palabra en ciencia de la computación, y aboga por la gran precisión en el uso de la palabra por los que miembros de varias disciplinas usan varias definiciones de la palabra. Escribió: “Antes de contestar la pregunta '¿qué es una ontología?', se debería primero responder a la pregunta ' ¿qué significa la palabra ontología?'.
Tampoco nos es muy claro cómo las ontologías encajan dentro de las bases de datos NoSQL .





</doc>
<doc id="4907" url="https://es.wikipedia.org/wiki?curid=4907" title="Antónimo">
Antónimo

Los antónimos son palabras que tienen significados opuestos o contrarios entre sí. Deben pertenecer (al igual que los sinónimos) a la misma categoría gramatical. Por ejemplo, antónimos de "alegría" son: "tristeza", "depresión", "melancolía"...; antónimos de "grande" son "pequeño" o "chico".

Existen tres clases de antónimos:

Existen diccionarios especializados en antónimos.





</doc>
<doc id="4913" url="https://es.wikipedia.org/wiki?curid=4913" title="Transbordador STS">
Transbordador STS

El Transbordador espacial STS ("Space Transport System"- Sistema de Transporte Espacial) era una nave espacial de la NASA parcialmente reutilizable para órbita baja terrestre. Su nombre se deriva de un plan de 1969 para un sistema de naves espaciales reutilizables de las cuales sólo el Transbordador fue financiado. El 10 de marzo de 1981 se realizó el primero de cuatro vuelos orbitales de prueba, abriendo paso a vuelos operativos en 1982. Los Transbordadores espaciales fueron utilizados en 135 misiones entre 1981 y 2011, lanzados desde el Centro Espacial Kennedy (KSC) en Florida, EE.UU. Misiones operacionales lanzaron varios satélites, sondas interplanetarias y el Telescopio Espacial Hubble (HST); realizaron experimentos científicos en órbita; y participaron en la construcción y el servicio de la Estación Espacial Internacional (ISS). El tiempo total de misión de la flota de Transbordadores fue 722 días, 19 horas, 21 minutos y 23 segundos.

Los componentes de los Transbordadores incluían el Vehículo Orbital (OV), un par de cohetes aceleradores sólidos recuperables (SRBs) y el Tanque Externo (ET) desechable que contenía hidrógeno y oxígeno líquido. Los Transbordadores se lanzaban verticalmente, como un cohete convencional, con dos SRBs operando en paralelo con los tres motores principales del OV, los cuales obtenían su combustible del ET. Los SRBs eran expulsados antes de que el vehículo entrara en órbita. El ET se expulsaba inmediatamente antes de la inserción orbital, lo cual utilizaba los dos motores del Sistema de Maniobras Espaciales (OMS). Al final de la misión el transbordador disparaba su OMS para desorbitar y reentrar a la atmósfera. El transbordador planeaba a una pista de aterrizaje ubicada en el Rogers Dry Lake en la Base de la Fuerza Aérea Edwards en California o en las Instalaciones de Aterrizaje de Transbordadores en el KSC. Tras aterrizar en Edwards, el transbordador era llevado por el Avión Transportador de Transbordadores de vuelta al KSC. Este avión era un Boeing 747 modificado.

El primer transbordador, "Enterprise", fue construido para pruebas de aproximación y aterrizaje y no tenía capacidad orbital. El nombre de este Transbordador proviene de la nave espacial del mismo nombre de la serie de ciencia ficción "Star Trek" (Viaje a las Estrellas). Cuatro orbitadores operacionales fueron construidos en un inicio: "Columbia"", Challenger, Discovery" y "Atlantis". De estos, "Challenger" y "Columbia" fueron destruidos en accidentes durante sus misiones en 1986 y 2003, respectivamente. En total 14 astronautas murieron. Un quinto transbordador operativo, "Endeavour", fue construido en 1991 para reemplazar a Challenger. El Transbordador espacial fue retirado del servicio al final de la última misión de "Atlantis" el 21 de julio de 2011.

Los Transbordadores espaciales eran vehículos de vuelo espacial humano parcialmente reutilizables capaces de llegar a órbita baja. Fueron utilizados por la NASA entre 1981 y 2011. Fue el resultado de estudios de diseño de Transbordadores realizados por la NASA y la Fuerza Aérea de los Estados Unidos en los años 60. Inicialmente fue propuesto para el desarrollo como parte de una segunda generación de Sistemas de Transporte Espacial (STS) ambiciosa que seguiría al Programa Apolo. En un informe de septiembre de 1969 de un grupo de trabajo espacial comandado por el Vice Presidente Spiro Agnew. El corte de presupuesto a la NASA por parte del entonces presidente Richard Nixon tras la conclusión del Programa Apolo causó que se tuvieran que retirar todos los componentes del sistema a excepción del Transbordador mismo, al cual la NASA le aplicó el nombre STS.

El vehículo consistía en un avión espacial para órbita y reentrada alimentado por tanques desechables de hidrógeno y oxígeno líquido, con cohetes aceleradores sólidos reutilizables que se podían separar del cuerpo principal. La primera de cuatro pruebas orbitales se realizó en 1981 y llevó a cabo vuelos operativos comenzando en 1982, todos lanzados desde el Centro Espacial Kennedy (KSC) en Florida, EE. UU.. El sistema fue retirado de servicio en el 2011 después de 135 misiones, con el "Atlantis" realizando el último despegue del programa el 8 de julio de 2011. El programa terminó cuando el "Atlantis" aterrizó en el KSC el 21 de julio de 2011. Misiones importantes incluyeron lanzar varios satélites y sondas interplanetarias, realizando experimentos científicos, y ayudando con la construcción y servicio de estaciones espaciales. El primer vehículo orbitador, "Enterprise", fue construido para las pruebas iniciales de aproximación y aterrizajes y no tenía motores, escudo de calor u otros sistemas necesarios para vuelos orbitales. Un total de cinco transbordadores operativos fueron construidos. De estos, dos fueron destruidos en accidentes.

Los Transbordadores fueron usados para misiones orbitales espaciales por la NASA, el Departamento de Defensa de E.E.U.U (DoD), la Agencia Espacial Europea (ESA), Japón y Alemania. Estados Unidos financió el desarrollo y las operaciones de los Transbordadores a excepción de los módulos de Spacelab usados en D1 y D2 por Alemania. SL-J fue parcialmente financiado por Japón.

Para el lanzamiento, el Transbordador consistía en el "Stack", incluyendo el Tanque Externo (ET) naranja (en los dos primeros lanzamientos el ET era blanco); dos cohetes aceleradores sólidos (SRBs) delgados, blancos; y el Vehículo Orbital (OV), el cual contenía la tripulación y la carga útil. Algunas cargas eran lanzadas a órbitas mayores con una de dos etapas superiores desarrolladas para el STS (Módulo de Asistencia de Carga de una etapa o Etapa Inercial Superior de dos etapas). El Transbordador espacial era montado en el Edificio de Construcción de Vehículos y luego posicionado sobre una plataforma de lanzamiento móvil sostenida por tuercas frágiles en cada SRB que se detonaban durante el lanzamiento.
El Transbordador ensamblado se lanzaba verticalmente como un cohete convencional. Despegaba con el poder de sus dos SRBs y sus tres motores principales, los cuales eran alimentados por hidrógeno y oxígeno líquido desde el ET. El Transbordador espacial tenía un ascenso de dos etapas. Los SRBs proporcionaban empuje adicional durante el despegue y la primera etapa del vuelo. Aproximadamente dos minutos después del despegue, las tuercas frágiles eran detonadas para soltar los SRBs, los cuales descendían con el apoyo de paracaídas al océano y ser recuperados por barcos para ser reparados y reutilizados. El transbordador y el ET continuarían ascendiendo en un plan de vuelo cada vez más horizontal bajo el poder de los motores principales. Al alcanzar una velocidad de 7.8 km/s los motores principales se apagarían. El ET, conectado por dos tuercas frágiles, era expulsado y se quemaba en la atmósfera. Tras expulsar el tanque externo, los motores del Sistema de Maniobras Espaciales (OMS) se usaban para ajustar la órbita. El transbordador posicionaba astronautas y cargas como satélites o partes de estaciones espaciales en una órbita baja terrestre. Normalmente entre cinco y siete miembros de la tripulación viajaban en el transbordador . Dos miembros de la tripulación, el comandante y el piloto eran suficiente para un vuelo mínimo como en los cuatro primeros vuelos de prueba de STS-1 a STS-4. La capacidad de carga habitual era alrededor de 50,045 lb (22,700 kg) pero podía ser aumentada dependiendo de la configuración de lanzamiento elegida. El orbitador llevaba su carga en un compartimiento especial con puertas que se abrían a lo largo de su lado superior, característica que distinguió al Transbordador Espacial de otras naves. Esta característica hizo posible el despliegue de grandes satélites como el Telescopio Espacial Hubble, al igual que la captura y regreso a la Tierra de cargas pesadas.

Cuando el transbordador terminaba su última misión disparaba sus propulsores OMS para bajar de su órbita y reentrar a la atmósfera. Durante el descenso, el orbitador pasaba por varias capas de la atmósfera y desaceleraba de velocidades hipersónicas principalmente por aerofrenado. En la atmósfera baja y en la etapa de aterrizaje se asemejaba más a un planeador pero controlado con motores del sistemas de control de reacción (RCS) y superficies de vuelo activadas hidráulicamente controladas por fly-by-wire . Aterrizaba en una pista larga como un avión convencional. La forma aerodinámica era un balance entre las demandas de velocidades radicalmente diferentes y presión aérea durante la reentrada, vuelo hipersónico y subsónico. Como consecuencia el orbitador tenía una tasa de hundimiento alta en altitudes bajas, y pasaba de usar impulsores RCS en altitudes altas a superficies de vuelo en la atmósfera baja.

El diseño formal de lo que llegaría a ser el Transbordador Espacial comenzó con el contrato de estudios de diseño de "Fase A" emitido a finales de los años 60. La concepción había empezado dos décadas antes, incluso antes del Programa Apolo de los años 60. Uno de los lugares de los cuales surgió el concepto de una nave espacial que pudiera regresar del espacio y realizar un aterrizaje horizontal fue desde la NACA, en 1954, en la forma de un experimento de investigación aeronáutica llamado X-15. La propuesta de la NACA fue presentada por Walter Dornberger.

En 1958 el concepto del X-15 llevó a que se propusiera lanzar un X-15 al espacio, y otro avión espacial de la serie X fue propuesto, el X-20 Dyna-Solar, junto a otros conceptos y estudios de aviones espaciales. Neil Armstrong fue seleccionado como piloto tanto para el X-15 como para el X-20. A pesar de que el X-20 jamás fue construido, un avión espacial de diseño similar si se construyó varios años después y fue entregado a la NASA en enero de 1966 bajo el Nombre HL-10.

A mediados de los años 60, la Fuerza Aérea americana realizó estudios clasificados en sistemas de transporte espacial de nueva generación y concluyeron que diseños semi-reutilizables eran los más baratos. Propuso un programa de desarrollo con un inicio inmediato en un vehículo de Clase I con motores desechables, seguido por un desarrollo más lento de un diseño semi-reutilizable de Clase II y posiblemente un diseño reutilizable de Clase III. En 1967, George Mueller visitó la base de la NASA para estudiar las opciones. 8 personas atendieron y presentaron una gran variedad de diseños, incluyendo diseños anteriores de la Fuerza Aérea como el X-20 Dyna-Solar.

En 1968 la NASA empezó oficialmente a trabajar en lo que entonces se conocía como el Vehículo de Lanzamiento y Reentrada Integrado (ILRV). Al mismo tiempo, la NASA tuvo una competición alternativa para el Motor Principal del Transbordador Espacial (SSME). Las oficinas de la NASA en Houston y Hunstville realizaron una Solicitud de Propuesta (RFP) conjunta para estudios del ILRV para diseñar una nave que podría mandar una carga a órbita pero también reentrar a la atmósfera y volar así de vuelta a la Tierra. Por ejemplo, una de las respuestas era para un diseño de dos etapas que tenía un impulsor grande y un orbitador pequeño, llamado el DC-3, uno de varios diseños de Transbordador de Fase A. Al terminar los estudios de Fase A, las fases B, C, y D evaluaron profundamente los diseños hasta 1972. En el diseño final, la parte inferior consistía en cohetes aceleradores sólidos recuperables, y la parte superior era un tanque externo desechable.

En 1969, el presidente Richard Nixon decidió apoyar la continuación del desarrollo del programa de Transbordadores Espaciales. Una serie de programas de desarrollo y análisis refinaron el diseño básico antes de entrar al desarrollo final y las pruebas. En agosto de 1973, el X-24B mostró que un avión espacial sin energía podría reentrar a la atmósfera terrestre para realizar un aterrizaje horizontal.

Ministros Europeos se vieron en Bélgica en 1973 para autorizar el proyecto orbital tripulado de Europa del Oeste y su contribución más grande al programa de Spacelab y los Transbordadores Espaciales. Spacelab proporcionaría un laboratorio espacial orbital multidisciplinario y más equipo espacial para el Transbordador.

El Transbordador espacial fue el primer orbitador operativo diseñado para ser reutilizado. Llevaba varias cargas a una órbita baja terrestre, proporcionaba una rotación de tripulación y suministros a la Estación Espacial Internacional (ISS) y le ofrecía mantenimiento a satélites. El orbitador podría incluso recuperar satélites y otras cargas para traerlas de vuelta a la Tierra. Cada Transbordador estaba diseñado para poder durar 100 lanzamientos o 10 años de vida útil, aunque esto luego fue extendido. La persona a cargo de diseñar el STS fue Maxime Faget, quien también ayudó con el diseño de naves espaciales previas como Mercury, Gemini y Apolo. El tamaño y la forma del Transbordador fue determinado principalmente por la necesidad de poder acomodar los satélites comerciales y militares más grandes, al igual que poder tener un rango-cruzado de recuperación de 1,000 millas para poder cumplir los requisitos de la USAF para misiones clasificadas de poder abortar de un lanzamiento y entrar en una órbita polar. Esta fue la razón principal del inmenso tamaño de las alas del Transbordador en comparación con diseños comerciales modernos, los cuales tienen pocas superficies de control para planear. Otros factores que influyeron en la decisión de optar por cohetes sólidos y un tanque desechable incluyeron el deseo del Pentágono por obtener un vehículo de alta capacidad de carga para el lanzamiento de satélites, y el deseo de la administración de Nixon a reducir el coste de la exploración espacial por medio de naves espaciales con componentes reutilizables.

Cada Transbordador espacial era un sistema de lanzamiento reutilizable compuesto por tres partes principales: el OV reutilizable, el ET desechable y dos SRBs reutilizables. Únicamente el OV entraba en órbita tras la expulsión de los impulsores y el tanque. El vehículo era lanzado verticalmente como un cohete convencional, y el orbitador planeaba hasta hacer un aterrizaje horizontal, como un avión. Al final se podía reparar cualquier daño y dejarlo listo para el siguiente lanzamiento. Los SRBs descendían con un paracaídas hasta caer en el océano, donde podían ser remolcados a la costa y reparados para ser reutilizados.
Cinco OVs fueron construidos: "Columbia" (OV-102), "Challenger" (OV-099), "Discovery" (OV-103), "Atlantis" (OV-104), y "Endeavour" (OV-105). Un modelo, "Inspiration", actualmente se encuentra a la entrada del Salón de Fama de Astronautas. Una nave adicional, "Enterprise" (OV-101), fue construido para las pruebas atmosféricas de planeo y aterrizaje; originalmente se planeó prepararlo para operaciones orbitales al final de las pruebas, pero se determinó que sería más barato preparar el artículo de pruebas estructurales STA-099, el cual se volvió el "Challenger". En 1986 el Transbordador espacial "Challenger" se desintegró 73 segundos después del despegue, y se utilizaron componentes que sobrevivieron para construir el "Endeavour" como sustituto. La construcción del "Endeavour" costó aproximadamente $1.7 mil millones de dólares. El "Columbia" sufrió un accidente catastrófico por encima de Texas durante la reentrada en el 2003. Cada lanzamiento de los Transbordadores costaba alrededor de $450 millones de dólares.

Roger A. Pielke, Jr. ha estimado que el programa de los Transbordadores Espaciales hasta 2008 costó alrededor de $170 mil millones de dólares (valor del dólar de 2008). El coste promedio por vuelo era de $1.5 mil millones de dólares. Dos misiones fueron pagadas por Alemania, Spacelab D1 y D2 con un centro de control de carga en Oberpfaffenhofen. D1 fue la primera vez que el control de carga de una misión STS no estaba bajo control americano.

A veces el propio orbitador era referido como el Transbordador Espacial. Esto no era técnicamente correcto, ya que el "Transbordador Espacial" era la combinación del orbitador, el tanque externo y los dos cohetes aceleradores sólidos. Estos componentes, una vez ensamblados en el Edificio de Ensamblaje Vertical, eran conocidos como el "Stack".

La responsabilidad de los componentes del Transbordador estaba repartida entre múltiples centros de la NASA. El Centro Espacial Kennedy era responsable del despegue, aterrizaje y operaciones de regreso para órbitas ecuatoriales, la Base de la Fuerza Aérea de Vandenberg era responsable del despegue. aterrizaje y operaciones de regreso para órbitas polares (aunque esto nunca fue usado)., el Centro Espacial Johnson sirvió como el punto central de todas las operaciones de Transbordadores, el Centro de Vuelos Espaciales Marshall era responsable de los motores principales, tanque externo y los cohetes aceleradores sólidos, el Centro Espacial John C. Stennis condujo las pruebas de los motores principales, y el Centro de Vuelo Espacial Goddard la red de rastreo global.

El orbitador es visualmente similar a una aeronave convencional, con alas doble-delta barridas a 81° en el lado interno y de 45° del externo. Su estabilizador vertical está barrido hacia atrás 50°. Los cuatro flaps montados en las partes traseras de las alas, el freno de velocidad ubicado en la parte trasera del estabilizador, junto con la solapa de cuerpo controlaban al orbitador durante el descenso y aterrizaje.

La bahía de carga del orbitador medía 4,6 por 18 m, comprendiendo la mayoría del fuselaje. Información desclasificada en el 2011 mostró que la bahía de carga estaba diseñada por la Oficina Nacional de Reconocimiento específicamente para acoplar el satélite espía KH-9 HEXAGON. Dos puertas parcialmente simétricas que abrían a lo largo de la bahía abarcaban toda la parte superior. Las cargas generalmente se cargaban horizontalmente mientras que el orbitador estaba vertical en la plataforma de lanzamiento, y descargadas verticalmente en microgravedad por el brazo robótico remoto (controlado por astronautas), astronautas en EVA o con energía de la misma carga.

Los tres Motores Principales del Transbordador STS (SSMEs) están montados a los lados del fuselaje trasero del orbitador en un patrón triangular. Las cabezas del motor pueden girar 10.5° verticalmente y 8.5° horizontalmente durante el ascenso para cambiar la dirección de la propulsión y así maniobrar la nave. La estructura del orbitador está compuesta principalmente por una aleación de aluminio, aunque la estructura del motor es principalmente una aleación de titanio.

Los orbitadores operacionales que fueron construidos fueron el OV-102 "Columbia", OV-099 "Challenger", OV-102 "Discovery", OV-104 "Atlantis" y OV-105 "Endeavour."

La función principal del tanque externo (ET) del Transbordador Espacial era proporcionar combustible de oxígeno e hidrógeno líquido a los motores principales. También era la columna del vehículo de lanzamiento, proporcionando puntos de conexión para los dos cohetes aceleradores sólidos y el orbitador. El ET era la única parte del Transbordador STS que no se reutilizaba. A pesar de que los ETs siempre eran expulsados, pudo haber sido posible llevarlos a órbita y reutilizarlos (como para incorporarse a una estación espacial).

Dos cohetes aceleradores sólidos (SRBs) daban 12,500 kN de empuje al despegar, esto era el 83% del impulso total durante el despegue. Los SRBs eran expulsados dos minutos después del despegue a una altura de aproximadamente 45,72 km, abriendo paracaídas y cayendo al océano para ser recuperado. La cobertura de los SRBs estaba hecha de acero de 13 mm de grosor. Los SRBs eran reutilizados varias veces; el encapsulamiento usado en las pruebas de motor del Ares I en 2009 consistía en coberturas de motores que ya habían volado, colectivamente, en 48 misiones de Transbordador, incluyendo la STS-1.

Los astronautas que han volado en varias naves cuentan que el Transbordador proporciona un viaje más movido que el cohete del programa Apollo o la nave Soyuz. Las vibraciones adicionales eran causadas por los cohetes aceleradores sólidos, ya que el combustible sólido no quema tan constantemente como el líquido. La vibración se calma cuando los cohetes aceleradores sólidos se desprenden.

El orbitador podía utilizarse en conjunto con una gran variedad de complementos, dependiendo de la misión. Esto incluía laboratorios orbitales, cohetes aceleradores extra para lanzar cargas hacia el espacio y muchas funciones más dadas por el Orbitador de Duración Extendida, los módulos Logísticos de múltiples fines o el Canadarm (SRMS). Una etapa superior llamada la Etapa de Transferencia de Órbita se utilizó una vez. Otro tipo de sistemas eran parte del sistema modular Spacelab- pallets, igloo, IPS, etc., los cuales también apoyaban a misiones especiales como la SRTM.

Un componente principal el Programa de Transbordadores STS era el Spacelab, principalmente contribuido por una unión de varios países Europeos, y operaba en conjunción con EE.UU. y socios internacionales. Basado en un sistema de módulos presurizados, paletas y sistemas, las misiones del Spacelab se ejecutaban en ciencias multidisciplinarias, logísticas orbitales y cooperación internacional. Más de 29 misiones volaron en temas variando desde astronomía, microgravedad, radar y ciencias de la vida, entre otras. Componentes físicos del Spacelab también sirvieron para dar apoyo al Hubble (HST) y el reabastecimiendo de estaciones espaciales. En la STS-2 y la STS-3 se hicieron pruebas, y la primera misión completa, la Spacelab-1 (STS-9), despegó el 28 de noviembre de 1983.

El programa Spacelab empezó formalmente en 1973 tras una junta en Bruselas, Bélgica, por líderes Europeos. Dentro de la década, el Spacelab entró en órbita y proporcionó un taller orbital y sistemas de hardware para Europa y E.E.U.U. Se logró cooperación internacional, ciencia y exploración en el Spacelab.

El Transbordador fue una de las primeras naves en usar un sistema de control computerizado de vuelo por "vuelo por cable" digital"." Esto significa que no existía conexión mecánica o hidráulica entre la palanca de control del piloto y las superficies de control o los motores hetes RCS. El algoritmo de control se basaba en una metodología clásica de Integral Derivada Proporcional (PID).

Una preocupación de utilizar sistemas digitales de vuelo por cable era la posibilidad de que fallara. Se investigó bastante sobre el sistema computacional del Transbordador. El Transbordador usaba cinco ordenadores de uso general (GPCs) redundantes de 32-bits de IBM, modelo AP-1'1, constituyendo un tipo de sistema embebido. Cuatro ordenadores usaban software especializado llamado el "Sistema de Software de Aviónica Primario" (PASS). Un quinto ordenador de respaldo utilizaba un software separado llamadel "Sistema de Vuelo de Respaldo" (BFS). Se refería colectivamente a los ordenadores como "Sistema de Procesamiento de Datos" (DPS).

La meta del diseño del DPS del Transbordador era una confianza de fallo-operacional/fallo-seguro. Después de un solo fallo, el Transbordador podría continuar la misión. Después de dos fallos aún podría aterrizar a salvo.

Los cuatro GPCs operaban en marcha, revisándose entre ellos. Si uno daba un resultado distinto al de los otros tres (por ejemplo, si uno fallaba), los tres ordenadores funcionales 'votaban' para sacarlo del sistema. Esto le aislaba del control vehicular. Si un segundo ordenador fallaba, los dos aún funcionales votaban para sacarlo también. Un fallo muy improbable hubiera sido que dos de los ordenadores proporcionaran un resultado 'A', y los otras dos un resultado 'B'. En este caso improbable uno de los dos pares era elegido al azar.

El BFS era software desarrollado por separado usado en el quinto ordenador, usado únicamente si el sistema primario de cuatro ordenadores fallaba. El BFS fue creado porque a pesar de que los cuatro ordenadores primarios eran redundantes en hardware, todos utilizaban el mismo software; así que un problema genérico en el software podía tirar todo el sistema. El software de sistemas embebidos aviónico fue desarrollado bajo condiciones totalmente diferentes a las de un software comercial: el número de líneas de código era diminuto a comparación a un producto comercial público, los cambios del sistema eran infrecuentes y con muchas pruebas, y muchas personas de programación y pruebas trabajaban en una pequeña cantidad de código computacional. A pesar de esto, en teoría aún hubiera podido fallar, y el BFS existía para cubrir esa contingencia. Mientras que el BFS podría correr en paralelo al PASS, el BFS nunca tomaba control sobre el PASS durante una misión.

El software para los ordenadores de vuelo del Transbordador fue escrito usando un lenguaje de alto nivel llamado HAL/S (Lenguaje de Ensamblaje de Alto-nivel/ Transbordador), similar a PL/I (Lenguaje de Programación/Uno). Fue diseñado específicamente para un entorno de sistemas embebidos de tiempo real.

Los ordenadores IBM AP-101 originalmente tenían aproximadamente 424 KB de memoria magnética central cada uno. La CPU podía procesar alrededor de 400,000 instrucciones por segundo, no tenían un disco duro, y cargaban su software desde cartuchos de cinta magnética.

En 1990 los ordenadores originales fueron reemplazados por un modelo mejorado AP-101S, el cual tenía alrededor de 2,5 veces la capacidad de memoria (alrededor de 1MB) y tres veces la velocidad de procesado (alrededor de 1,2 millones de instrucciones por segundo). La memoria fue cambiada de un centro magnético a un semiconductor con respaldo de batería.

Las primeras misiones mediante Transbordador, comenzando en noviembre de 1983, llevaron con ellas un Grid Compass, considerado uno de los primeros ordenadores portátiles. El GRiD recibió el nombre de SPOC (Prdemador Portátil a Bordo del Transbordador). Su uso en el Transbordador requería modificaciones de hardware y software que se incorporaron a versiones posteriores al producto comercial. Fue usado para supervisar y mostrar la posición terrestre del Transbordador, el camino de las siguientes dos órbitas, dónde el Transbordador tendría comunicación de línea de vista con estaciones terrestres, y para determinar puntos para observaciones de la Tierra en puntos específicos. El Comprass no vendió bien, ya que el coste era por lo menos $8,000 USD, pero ofreció un rendimiento incomparable para su tamaño y peso. La NASA era uno de sus clientes principales.

El orbitador prototipo "Enterprise" originalmente tenía una bandera de EE. UU. en la superficie superior del ala izquierda y las letras "USA" en negro en el ala derecha. El nombre "Enterprise" estaba pintado en negro en las puertas de la bahía de carga encima de la bisagra y detrás del módulo de tripulación; en el extremo de popa de las puertas de la bahía estaba el logotipo de la NASA conocido como "worm" (gusano) en gris. Debajo de las puertas de la bahía en la parte trasera estaba el texto "United States" en negro con una bandera de EE. UU. delante.

El primer orbitador operativo, el "Columbia", tenía originalmente las mismas marcas que el "Enterprise", pero las letras "USA" en el ala derecha eran ligeramente más grandes y más separadas. El "Columbia" también tenía marcas negras que el "Enterprise" no tenía en su módulo RCS superior, alrededor de las ventanas de la cabina y en su estabilizador vertical.

El "Challenger" estableció un esquema de marcas modificado para la flota de Transbordadores que se reutilizó por el "Discovery, el "Atlantis" y el "Endeavour". Las letras "USA" en negro encima de la bandera americana se movieron al ala izquierda, con el logotipo "worm" de la NASA en gris centrado encima del nombre del orbitador en el ala derecha. El nombre del orbitador se movió a una posición en el fuselaje delantero justo debajo y detrás de las ventanas de la cabina. Esto permitió que el nombre del Transbordador se pudiera ver cuando se fotografiara en el espacio con las puertas de la bahía abiertas.

En 1983, las marcas del "Enterprise" fueron modificadas para que combinara con las del "Challenger", y el logotipo de la NASA se repintó de negro. Se agregaron algunas marcas negras a la nariz, cabina, ventanas y cola vertical para asemejarse más a los vehículos de vuelo, pero el nombre "Enterprise" se mantuvo en su posición en las puertas de la bahía, ya que no había necesidad de abrirlas. El nombre del "Columbia" se movió a la parte delantera del fuselaje para combinar con los demás vehículos después de la STS-61-C, durante el paréntesis de la flota de 1986 a 1988 tras la pérdida del "Challenger", pero mantuvo sus marcas de ala originales hasta su última remodelación (después de STS-93).
Desde 1998, las marcas de los vehículos cambiaron para incorporar el logotipo tradicional de la NASA conocido como la insignia "meatball" (albóndiga). El logotipo "worm", el cual la agencia había eliminado, fue quitado de las puertas de la bahía de carga y la insignia "meatball" fue agregada a popa del texto "United States" en el fuselaje inferior. La insignia "meatball" también se colocó en el ala izquierda, con la bandera americana encima del nombre del orbitador, situado a la izquierda en vez de centrado, en el ala derecha. Los tres vehículos de vuelo supervivientes, el "Discovery", el "Atlantis," y el "Endeavour", aún tienen estas marcas en sus respectivos museos. El "Enterprise" se volvió propiedad del Instituto Smithsoniano en 1985 y no estaba bajo el control de la NASA cuando se realizaron estos cambios, por lo que aún tiene las marcas de 1983.

El Transbordador espacial inicialmente fue desarrollado en los años 70, pero recibió muchas actualizaciones y modificaciones para mejorar su desempeño, fiabilidad y seguridad. Internamente, el Transbordador se mantuvo muy similar a su diseño original, a excepción de los ordenadores de vuelo mejorados. Aparte de las mejoras de los ordenadores, los instrumentos primarios análogos originales fueron reemplazados con monitores planos modernos a todo color, llamada la cabina de cristal, la cual es similar a la de los aviones modernos. Para facilitar la construcción de la ISS, las esclusas de aire de todos los orbitadores a excepción del "Columbia" fueron reemplazados por un sistema de atraque externo para tener para tener mayor espacio para las cargas que se guardarían en la cubierta media del Transbordador durante misiones de reabastecimiento de la estación.

Los Motores Principales del Transbordador Espacial (SSMEs) fueron mejorados varias veces para aumentar su poder y fiabilidad. Esto explica frases como "Motores principales acelerando a 104%." Esto no significaba que los motores estaban empujando más allá de su límite. La figura del 100% era el nivel de poder original. Durante el programa de desarrollo, Rocketdyne determinó que el motor era capaz de operar segura y fiablemente a 104% de su aceleración original. La NASA decidió utilizar notación superior al 100% ya que reescalar el número significaría modificar el software de los ordenadores, entre otras cosas.

Durante las primeras dos misiones, la STS-1 y la STS-2, el tanque externo estaba pintado de blanco para proteger el aislamiento que cubría la mayoría del tanque, pero mejoras y pruebas determinaron que no era necesario. El peso ahorrado en no pintar el tanque resultó en una capacidad de carga mayor. Se ahorró más peso al quitar algunos "stringers" internos en el tanque de hidrógeno que resultaron ser innecesarios. El tanque externo de bajo peso que resultó de eliminar estos componentes voló por primera vez en la STS-6 y se utilizó para la mayoría de las misiones de Transbordadores. Una versión mejorada se instaló y fue utilizada por primera vez en la STS-1. Esta versión estaba hecha de una aleación aluminio-litio 2195. Pesaba 3,4 toneladas métricas menos que los tanques anteriores, permitiendo que el Transbordador llevara elementos pesados a la órbita alta de la ISS. Ya que el Transbordador no podía ser operado sin tripulación, estas mejoras se probaron por primera vez en vuelos operativos.

Los aceleradores sólidos fueron mejorados también. Ingenieros de diseño agregaron una tercera junta tórica a la articulación entre los segmentos después del desastre del Transbordador Espacial "Challenger".

Se planearon muchas más actualizaciones al SRB para mejorar su desempeño y seguridad, pero nunca se realizaron. Estas culminaron en el cohete sólido de aceleración considerablemente más sencillo, barato, seguro y de mejor desempeño. Estos cohetes entraron en producción a inicios de los años 90 para apoyar a la Estación Espacial, pero se cancelaron para ahorrar dinero tras un gasto de $2,2 mil millones de dólares. La pérdida del programa ASRB resultó en el desarrollo del Tanque Externo superligero (SLWT), el cual mejoró la capacidad de carga a pesar de no mejorar la seguridad. Además de esto, la Fuerza Aérea Americana desarrolló un diseño propio de un cohete SRB mucho más ligero utilizando un sistema de filamentos, pero esto también fue cancelado.

La misión STS-70 fue retrasada en 1995 cuando unos pájaros carpinteros hicieron agujeros en la espuma aislante del Tanque Externo del "Discovery". Desde entonces, la NASA ha instalado búhos falsos que se deben quitar antes del lanzamiento. La naturaleza delicada de la espuma de aislamiento había sido la causa del daño al Sistema de Protección Térmica, el escudo de calor y la envoltura térmica del orbitador. La NASA afirmó que este daño, a pesar de ser responsable del desastre del Transbordador Espacial "Columbia" el 1 de febrero de 2003, no afectaría a la conclusión de la ISS en el tiempo previsto.

Una variación del Transbordador que únicamente llevaría cargas al espacio fue propuesto y rechazado desde los años 80s. Era conocido como el Transbordador-C, y cambiaría reutilizabilidad por capacidad de carga, con un ahorro potencial grande al reutilizar tecnología diseñada para los Transbordadores Espaciales. Otra propuesta era convertir la bahía de carga en un área para pasajeros, con versiones variando desde 30 a 74 asientos, tres días en órbita y un precio de $1,5 millones de dólares por asiento.

En la primera de cuatro misiones de los Transbordadores, los astronautas usaron trajes de gran altitud a presión-completa de la Fuerza Aérea Americana modificados, los cuales incluían un casco de presión-completa durante el ascenso y descenso. Desde el quinto vuelo hasta la pérdida del "Challenger", trajes azules de nomex de una pieza de color azul claro con cascos de presión-parcial fueron utilizados. Una versión de los trajes de gran altitud modificados a presión-parcial y para ser menos estorbosos fue reincorporado cuando los vuelos reiniciaron en 1988. Este traje, conocido como el Traje de Despegue-Entrada terminó su vida de servicio a finales de 1955, y fue reemplazado por el Traje de Escape de Tripulación Avanzado (ACES), el cual se parecía al traje espacial de las misiones Gemini en diseño, pero mantenía el color naranja del traje de Despegue-Entrada.

Para extender la duración que los orbitadores podían mantenerse anclados a la ISS, el Sistema de Transferencia de Energía de Estación-a-Transbordador (SSPTS) fue instalado. El SSPTS permitía que estos orbitadores usaran energía proporcionada por la ISS para preservar sus consumibles. El SSPTS fue utilizado exitosamente por primera vez en STS-118.

Orbitador (para el "Endeavour", OV-105)

Tanque externo (para SLWT)

Cohetes impulsores sólidos

Pilar del sistema

Todas las misiones del Transbordador Espacial se lanzaban desde el Centro Espacial Kennedy (KSC). El criterio de clima para el despegue incluía, pero no se limitaba a: precipitación, temperaturas, cubierta de nubes, pronóstico de truenos, viento y humedad. El Transbordador no se lanzaba bajo condiciones donde le podría caer un rayo. Aeronaves comúnmente reciben rayos sin efectos adversos ya que la electricidad se dispersa sobre su estructura conductora y la aeronave no está conectada a tierra eléctrica. Al igual que la mayoría de las aeronaves, el Transbordador estaba construido primariamente de aluminio conductor, el cual normalmente protegería los sistemas internos. Sin embargo, durante el despegue el Transbordador dejaba atrás una línea de vapor que podría conectar a la nave a tierra eléctrica. La Regla de Yunque de la NASA para el despegue de un Transbordador decía que no podía aparecer una nube de yunque dentro de una distancia de 10 millas náuticas. El Oficial del Clima de Despegue del Transbordador monitoreaba las condiciones del clima hasta que se anunciara la decisión final de cancelar el despegue. Además de esto, para el despegue, las condiciones climáticas tenían que ser aceptables en uno de los sitios de Aterrizaje de Aborto Transatlántico (uno de tantos modos de aborto para los Transbordadores Espaciales) al igual que el área de recuperación de los cohetes aceleradores sólidos. Mientras que el Transbordador quizá habría podido recibir un rayo, una situación de ese tipo causó problemas en Apolo 12, así que para asegurar la seguridad de la tripulación y de la misión, la NASA decidió no despegar el Transbordador si existía posibilidad de que cayeran rayos (NPR8715.5).

Históricamente, el Transbordador no se lanzaría si su vuelo duraría entre diciembre y enero (transición de año, o YERO). Su software de vuelo, diseñado en los años 70, no fue diseñado para esto, y podría requerir que las computadoras del orbitador se resetearan en el cambio de año, lo cual podría causar un problema dentro de la órbita. En el 2007, ingenieros de la NASA crearon una solución para que los Transbordadores pudieran cruzar la frontera de fin de año.

Después de la pausa final en la cuenta regresiva, en T-menos 9 minutos, el Transbordador pasaba por sus preparaciones finales para el lanzamiento, y la cuenta regresiva se controlaba automáticamente por el Secuenciador de Lanzamiento Terrestre (MCG), el software en el Centro de control de lanzamiento, que detenía el conteo si se detecta un problema crítico con cualquiera de los sistemas a bordo del Transbordador. El GLS cedía la cuenta a las computadoras a bordo del Transbordador a T-menos 31 segundos, en un proceso llamado secuencia de inicio automático.

En T-menos 16 segundos, El sistema de supresión de sonidos masivos (SPS) comenzaba a empapar la Plataforma de Lanzamiento Móvil (MLP) y las zanjas de los SRB con 1,100 metros cúbicos de agua para proteger al Orbitador de daño causado por la energía acústica del cohete reflejada del MLP durante el despegue.

En T-menos 10 segundos, encendedores de hidrógeno eran activados en cada campana del motor para sofocar el gas estancado dentro de los conos antes de la ignición. El no quemar estos gases podría encender los sensores a bordo y crear la posibilidad de una presión excesiva, e incluso de la explosión del vehículo durante la fase de del encendido. Las bombas principales del cohete también comenzaban a cargar las cámaras de combustión con hidrógeno y oxígeno líquido. Las computadoras respondían a esta acción permitiendo que las computadoras redundantes iniciaran la fase de quemado.

Los tres cohetes principales (SSMEs) iniciaban en T-6.6 segundos. Los cohetes principales se encendían secuencialmente por medio de las computadoras de propósito general (GPCs) a intervalos de 120ms. Los tres SSMEs eran requeridos para llegar a 90% de la fuerza máxima dentro de 3 segundos, en caso contrario las computadoras a bordo iniciarían un aborto RSLS. Si los tres cohetes indicaban un desempeño nominal para T-3 segundos, recibían el comando para entrar en la configuración de despegue y se enviaría el comando para preparar los SRBs para el encendido a T-0. Entre T-6.6 segundos y T-3 segundos, mientras que los SSMEs se disparaban pero los SRBs seguían conectados a la plataforma, la diferencia de fuerza causaba que la pila de lanzamiento girara 25.5 pulgadas medidas a la punta del tanque externo. El retraso de 3 segundos después de la confirmación de operación del SSME era para permitir que la pila regresara casi a su posición vertical. En T-0 segundos, las 8 tuercas frágiles sujetando los SRBs a la plataforma se detonaban, los SSMEs se aceleraban al 100% de fuerza, y los SRBs eran encendidos. Para T+0.23 segundos, los SRBs acumulaban suficiente fuerza para comenzar el despegue, y llegaban a su presión interna máxima en T+0.6 segundos. El Centro de Control de Misión de JSC asumía control del vuelo una vez que los SRBs estaban libres de la torre de lanzamiento.

Poco después del despegue, los cohetes principales del Transbordador se aceleraban a 104.5% y el vehículo comenzaba una maniobra de giro que lo posicionaba en la trayectoria correcta (azimuth) para la inclinación orbital planeada, y en una posición boca-abajo con las alas a nivel. El Transbordador Espacial volaba de cabeza durante el ascenso para permitir un ángulo ataque recortado que era favorable para las cargas aerodinámicas durante la región de alta presión dinámica, resultando en un factor de carga neto positivo, al igual que proporcionando a la tripulación con una vista del horizonte que podrían usar como referencia visual. El vehículo escalaba en un arco progresivamente más plano, acelerando mientras que la masa de los SRBs y del ET disminuía (al utilizar combustible). Para alcanzar una órbita baja se requiere una aceleración horizontal mucho mayor que la vertical.

Alrededor de 30 segundos dentro del ascenso, se reducía la potencia de los SSME - usualmente al 72%, aunque esto variaba - para reducir las fuerzas aerodinámicas actuando sobre el Trasbordador en un punto llamado Máx Q. Adicionalmente, el diseño de grano de propulsor de los SRBS causaba que su fuerza disminuyera por 30% a 50 segundos de iniciado el ascenso. Una vez que la dirección del Orbitador verificaba que Máx Q estaría dentro de los límites estructurales del Transbordador, se aceleraría de vuelta a 104.5%; la acción de disminuir y luego aumentar la fuerza era conocido como la "cubeta de fuerza" ("thrust bucket"). Para maximizar el desempeño, el nivel de fuerza y la sincronización de la cubeta de fuerza era formado para llevar al Transbordador tan cerca a sus límites aerodinámicos como era posible.

Alrededor de T+126 segundos, ajustadores pirotécnicos expulsaban a los SRBs y cohetes de separación pequeños los empujaban lateralmente lejos del vehículo. Los SRBs descendían con paracaídas hacía el océano para ser reutilizados. El Transbordador luego continuaría acelerando hacia su órbita con sus cohetes principales. La aceleración en este punto típicamente era alrededor de 0.9 G, y el vehículo tomaría un ángulo ligeramente por encima del horizonte - usaba sus tres cohetes principales para aumentar y luego mantener su altitud mientras que aceleraba horizontalmente hacia su órbita. Alrededor de 5.75 minutos dentro del ascenso, las líneas de comunicación directa con la tierra se desvanecían, por lo cual el Transbordador giraba boca-arriba para reconectar sus líneas de comunicación al sistema de Satélites de Rastreo y Relevo de Información.

Alrededor de 7 minutos y medio dentro del ascenso, la masa del vehículo era lo suficientemente baja para que los motores necesitaran disminuir su potencia para limitar la aceleración del vehículo a 3 . El Transbordador mantendría esta aceleración durante el siguiente minuto, y el corte al cohete principal (MECO) sucedía alrededor de 8 minutos y medio después del lanzamiento. Los cohetes principales eran apagados antes de que se acabara por completo el propulsor, ya que si se acabara se podrían dañar los cohetes. El suministro de oxígeno se apagaba antes del suministro de hidrógeno, ya que los SSMEs reaccionaban de forma negativa a otros modelos de apagar. Unos segundos después de MECO, el ET era expulsado por ajustadores pirotécnicos.

A este momento del vuelo, el ET y el Transbordador tenían trayectorias suborbitales, flotando hacia su apogeo. Una vez ahí, alrededor de media hora después del MECO, los cohetes del OMS del Transbordador se disparaban para aumentar su perigeo y llegar a órbita, mientras que el ET caía de vuelta a la Tierra hacia el Océano Índico o el Océano Pacífico, dependiendo del perfil de lanzamiento, y se quemaba en la atmósfera. El ET estaba diseñado de forma que se rompería, explotaría y desintegraría durante su re-entrada para garantizar que cualquier pedazo que cayera a la Tierra fuera pequeño.

El Transbordador era monitoreado durante su ascenso por medio de rastreo de alcance corto (10 segundos antes del despegue hasta 57 segundos después), alcance medio (7 segundos antes del despegue hasta 110 segundos después), y largo alcance (7 segundos antes hasta 165 segundos después).

Cámaras de corto alcance incluían 12 cámaras de 16 mm en la MLP y 8 de 16 mm en la Estructura Fija de Servicio, 4 cámaras fijas de alta velocidad ubicadas en el perímetro del complejo de lanzamiento además de 42 cámaras fijas con películas cinematográficas de 16 mm.

Cámaras de alcance medio incluían cámaras de rastreo operadas remotamente en el complejo de lanzamiento además de 6 sitios a lo largo de la costa al norte y sur inmediato de la plataforma, cada una con un lente de 800 mm y cámaras de alta velocidad corriendo a 100 cuadros por segundo. Estas cámaras sólo corrían por 4-10 segundos dadas las limitaciones en la cantidad de cinta disponible.

Cámaras de largo alcance incluían aquellas montadas en el Tanque Externo (ET), los SRBs y el Orbitador mismo, las cuales transmitían video en vivo hacia la tierra, proporcionando información valiosa sobre cualquier escombro que cayera durante el ascenso. Cámaras de rastreo de largo alcance con cintas de 400-pulgadas y lentes de 200-pulgadas eran operadas por un fotógrafo en la Playa Playalina al igual que en 9 otros sitios desde 38 millas al norte en el Ponce Inlet hasta 23 millas al sur en la Base de la Fuerza Aérea Patrick (PAFB), y cámaras de rastreo óptico móviles adicionales se podían colocar en Merritt Island durante lanzamientos. Un total de 10 cámaras de alta definición eran utilizadas tanto para obtener información de ascenso para ingenieros como para redes televisivas como NASA TV y HDNet. El número de cámaras aumentó considerablemente y cámaras existentes fueron mejoradas bajo recomendación de la Mesa de Investigación del Accidente del Columbia para proporcionar mayor información sobre escombros durante el lanzamiento. Adicionalmente, durante los dos primeros vuelos después de la pérdida del "Columbia" y su tripulación, un par de aeronaves de reconocimiento WB-57 de la NASA equipados con video de alta definición y cámaras de infrarrojo volaron a 60,000 pies (18,288 m) para proporcional más vistas del ascenso de despegue. El KSC también invirtió casi $3 millones de dólares en mejoras para los sistemas de análisis de video en apoyo al rastreo de escombros.

Una vez en órbita, el Transbordador usualmente volaba a una altitud de 320 km con respecto al nivel del mar, y ocasionalmente hasta 650 km. En los años 80 y 90, muchos vuelos involucraban misiones de ciencia en el Spacelab de NASA/ESA, o el lanzamiento de varios tipos de satélites y sondas científicas. Para los años 90 e inicios de siglo, el enfoque había cambiado más al mantenimiento de la ISS, con menos lanzamientos de satélites. La mayoría de las misiones involucraban quedarse en órbita durante varios días o incluso semanas, aunque misiones más largas eran posibles usando un componente llamado "Orbitador de Duración Extendida" o manteniéndose unido a una estación espacial.

El vehículo comenzaba su re-entrada disparando los cohetes del OMS, mientras que volaba de cabeza, parte trasera hacia delante, en dirección opuesta a su órbita por aproximadamente tres minutos, lo cual reducía la velocidad del Transbordador por alrededor de 322 km/h. Esto causaba que el perigeo de la órbita del Transbordador se disminuyera a la atmósfera superior de la Tierra. El Transbordador luego se volteaba.

El vehículo comenzaba a atravesar aire cada vez más denso en la termosfera baja a alrededor de 122 km, mientras viajaba a casi mach 25 (8,200 m/s). El vehículo era controlado por una combinación de impulsores RCS y superficies de control, para volar a un ángulo de 40° con la nariz hacia arriba, produciendo un arrastre muy grande el cual no solo disminuía la velocidad del Transbordador, sino que también reducía el calentamiento por la re-entrada. En lo que el vehículo se encontraba con aire progresivamente más denso, iniciaba una transición de nave espacial a aeronave. En una línea recta, su ángulo de 40° hacia arriba causaría que su ángulo de descenso se aplanara o incluso subiera. Para evitar que el vehículo ascendiera de nuevo, este realizaba una serie de giros en forma-S, cada uno durando varios minutos y teniendo un ángulo de 70° horizontalmente, mientras que mantenía su ángulo vertical de 40°. De esta forma disipaba velocidad horizontalmente en vez de verticalmente. Esto ocurría durante la fase más caliente de la re-entrada, cuando el escudo de calor brillaba rojo y las fuerzas-G estaban en un máximo. Para el final del último giro, la transición a aeronave estaba casi completa. El vehículo nivelaba su alas, bajaba su nariz a un clavado ligero y comenzaba su aproximación a la zona de aterrizaje.

La razón máxima planeo/levante-a-arrastre del orbitador varía considerablemente con su velocidad, yendo desde 1:1 a velocidades hipersónicas, 2:1 a velocidades supersónicas y llegando a 4.5:1 a velocidades subsónicas durante la aproximación y el aterrizaje.

En la atmósfera baja, el orbitador vuela casi como un planeador convencional, a excepción de su razón de descenso mucho mayor, a más de 50 m/s. A aproximadamente mach 3, dos sondas de aire, ubicadas del lado izquierdo y derecho del fuselaje inferior del orbitador, son desplegadas para detectar la presión del aire relacionado con los movimientos del vehículo en la atmósfera.

Cuando la aproximación y la fase de aterrizaje comenzaba, el orbitador estaba a 3,000m de altura y 12 km de la pista de aterrizaje. Los pilotos aplicaban frenos aerodinámicos para ayudar a frenar al vehículo. La velocidad del orbitador disminuía de 682 a 346 km/h, aproximadamente, al tocar la pista (comparado con 260 km/h para un avión comercial). Para apoyar con los frenos de velocidad, un paracaídas de frenado de 12 metros se desplegaba cuando el vehículo (ya habiendo tocado la pista) estuviera a aproximadamente 343 km/h. El paracaídas era expulsado cuando el orbitador se alentaba a 110 km/h.

Después de aterrizar, el vehículo se quedaba en la pista durante varias horas para que el orbitador pueda enfriarse. Equipos al frente y atrás del orbitador hacían pruebas buscando la presencia de hidrógeno, hidracina, monometilhidracina, tetróxido de dinitrógeno y amoniaco. Si se detectaba hidrógeno, se declaraba una emergencia, se apagaba el orbitador y equipos evacuarían el área. Un escolte de 25 vehículos especialmente diseñados y 150 ingenieros y técnicos entrenados se acercarían al orbitador. Líneas de purga y ventilación se conectarían para remover gases tóxicos de las líneas de combustible y la bahía de carga alrededor de 45-60 minutos después de aterrizar. Un cirujano de vuelo abordaba el orbitador para revisiones médicas iniciales de la tripulación antes de desembarcar. Una vez que la tripulación del orbitador habían desembarcado, la responsabilidad del vehículo se transfería del JSC al KSC

Si la misión terminaba en la Base de la Fuerza Aérea Edwards en California, Puerto Espacial White Sands en Nuevo México o cualquier otra pista de aterrizaje que podría utilizar un orbitador en una emergencia, el orbitador era cargado arriba del Aeronave de Transporte del Transbordador Espacial, un 747 modificado, para transportar al Transbordador de vuelta al KSC, aterrizando en las Instalaciones de Aterrizaje de Transbordadores. Una vez que el Transbordador se encontraba en las Instalaciones de Aterrizaje de Transbordadores, el orbitador era remolcado 2 millas (3.2 km) por un camino especial y caminos normalmente sólo accesado por camiones de visita y empleados del KSC hasta llegar a las Instalaciones de Procesamiento del Orbitador, donde comenzaba un proceso de preparación que duraba meses para estar listo para su siguiente misión.

La NASA prefería que los Transbordadores Espaciales aterrizaran en el KSC. Si condiciones climáticas no lo permitían, el Transbordador podría retrasar su aterrizaje hasta que las condiciones fueran favorables, aterrizar en la Base de la Fuerza Aérea Edwards, California, o usar uno de varios sitios de aterrizaje alternativos alrededor del mundo. Un aterrizaje en cualquier lugar que no fuera el KSC significaba que después de aterrizar, el Transbordador tendría que ser conectado a la aeronave que la transportaría de regreso al Cabo Cañaveral. El Transbordador Espacial "Columbia" (STS-3) una vez aterrizó en el Puerto Espacial White Sands, Nuevo México; esto se veía como un último recurso ya que los científicos de la NASA creían que la arena podría dañar el exterior del Transbordador.

Había varios sitios de aterrizaje alternativos que jamás se utilizaron.

Un ejemplo de un análisis de riesgo técnico para una misión STS es SPRS iteración 3 contribuyentes de mayor riesgo para STS-133:

Una evaluación de riesgos interno de la NASA (realizado por la Oficina de Garantía de la Seguridad del Programa de Transbordadores Espaciales en el JSC) publicada a finales del 2010 o inicios del 2011 concluyó que la agencia había seriamente subestimado los riesgos involucrados con operar el Transbordador. El reporte evaluó que había una probabilidad de 1 en 9 de una falla catastrófica durante los primeros 9 vuelos del Transbordador, pero que las mejoras del Transbordador que se realizaron posteriormente mejoraron el riesgo a 1 en 90.

Esta es una lista de eventos principales en la flota de orbitadores de los Transbordadores Espaciales.
Fuentes: manifiesto de lanzamiento de NASA, archivo de Transbordadores Espaciales de NASA

El 28 de enero de 1986, el "Challenger" se desintegró 73 segundos después de su lanzamiento, debido al fracaso del SRB derecho, y mató a los siete astronautas que se encontraban a bordo. Este desastre fue causado por la deficiencia a baja temperatura de una junta tórica, en un sello crítico de la misión entre los segmentos de la carcasa de los SRB. El fracaso de una junta tórica del interior permitió que la combustión de gases calientes explotara, las repetidas advertencias por parte de los ingenieros de diseño que  expresaban su preocupación por la falta de evidencia de las juntas tóricas cuando la temperatura estaba por debajo de 53° F (12 °C) habían sido ignoradas por los administradores de la NASA.

El 1 de febrero de 2003, el "Columbia" se desintegró durante la re entrada debido a los daños causados durante el lanzamiento en el borde de ataque del ala fabricada de un compuesto de carbón-carbón, matando a su equipo de siete personas. Los ingenieros de control habían hecho tres solicitudes separadas de imágenes de alta resolución tomadas por el Departamento de Defensa que habrían proporcionado una comprensión de la magnitud de los daños, mientras que el jefe de defensa del Sistema de Protección Térmica (TPS), el ingeniero pidió que a los astronautas a bordo del "Columbia" se les permitiera dejar el vehículo para que se pudieran inspeccionar los daños. Los gerentes de la NASA intervinieron para detener la asistencia del Departamento de Defensa y se les negó la solicitud de la caminata espacial, por lo tanto la viabilidad de escenarios para la reparación de astronautas o de rescate por "Atlantis" no fue considerada por la dirección de la NASA en el momento.

La NASA retiró el transbordador espacial en el 2011, después de 30 años de servicio. El Transbordador fue originalmente concebido y se presentó al público como un "camión espacial¨, el cual debería, entre otras cosas, ser utilizado para construir una estación espacial de los Estados Unidos en órbita Terrestre baja a principios de 1990. Cuando la estación espacial de Estados Unidos se convirtió en el proyecto de la ISS, la cual sufrió de largos retrasos y cambios de diseño antes de que pudiera ser completada, la vida de servicio del transbordador espacial fue extendido varias veces hasta el 2011, sirviendo por lo menos 15 años más de lo que originalmente estaba diseñado. El "Discovery" fue el primero de los tres Transbordadores Espaciales restantes de la NASA en ser retirado.

La última misión del Transbordador Espacial fue originalmente programada para finales del 2010, pero el programa se extendió más tarde a julio del 2011, cuando Michael Suffredini del programa de la ISS, dijo que se necesitaba un viaje adicional en el  2011 para entregar piezas a la Estación Espacial Internacional. La misión final del Transbordador consistía en sólo cuatro astronautas - Christopher Ferguson (comandante) , Douglas Hurley (piloto) , Sandra Magnus (especialista de misión 1) , y Rex Walheim (especialista de misión 2); se llevó a cabo la misión del Transbordador espacial número 135 y la último a bordo del "Atlantis", que fue lanzada el 8 de julio de 2011 y aterrizó con seguridad en el Centro Espacial Kennedy el 21 de julio de 2011 a las 5:57 p. m. EDT (09:57 GMT).

La NASA anunció que transferiría los orbitadores a instituciones educativas o museos al concluir el programa de los Transbordadores Espaciales. Cada museo o institución es responsable por cubrir el precio de preparar y transportar cada vehículo para mostrarlo. 20 museos de alrededor de EUA enviaron propuestas para recibir uno de los orbitadores retirados. La NASA también hizo bloques del sistema de protección térmica del Transbordador Espacial disponibles a escuelas y universidades por menos de $25 USD cada uno. Alrededor de 7,000 bloques estaban disponibles, limitado a uno por institución.

El 12 de abril del 2011, la NASA anunció la selección de ubicaciones para los orbitadores de Transbordadores restantes:


Hardware de vuelo y de entrenamiento se tomarán desde el Centro Espacial Johnson e irán al Museo Nacional del Aire y del Espacio, y el Museo Nacional de la Fuerza Aérea de Estados Unidos. La maqueta del fuselaje completo, que incluye la bahía de carga y la sección de popa, pero sin alas, irá al Museo de Vuelo en Seattle. el simulador fijo de la Institución de Entrenamiento y Simulación de Misiones irá al Planetario Adler de Chicago, y el simulador de movimiento irá al Departamento de Ingeniería Aeroespacial de Texas A&M en College Station, Texas. Otros simuladores utilizados en el entrenamiento de astronautas de los Transbordadores irán al Museo de la Aviación Wings of Dreams en Starke, Florida y el Centro Espacial y Aéreo de Virginia, en Hampton Virginia.

En agosto del 2011, la Oficina del Inspector General de la NASA (OIG) publicó una "reseña de la NASA sobre la Selección del monitor de localizaciones para el Orbitador del Transbordador Espacial", la revisión tenía cuatro conclusiones principales:
La NASA OIG tuvo tres recomendaciones, diciendo que la NASA debería: 

En septiembre del 2011, el CEO y dos miembros de la junta del Museo de Vuelo de Seattle se reunieron con el administrador de la NASA Charles Bolden, señalando "los errores significativos para decidir dónde colocar sus cuatro Transbordadores Espaciales retirados"; los errores alegados incluyen información inexacta sobre la asistencia al Museo de Vuelo, así como la disposición del sitio de exhibición del Museo Espacio-Mar-Aire Intrepid.

Hasta que otra nave espacial tripulada de los Estados Unidos estuviera lista, los equipos viajarán hacia y desde la Estación Espacial Internacional (ISS) exclusivamente a bordo de la nave espacial rusa Soyuz.

Un sucesor planeado para el Transbordador STS fue el "Shuttle II", durante los años 80 y 90, y más tarde el programa Constellation durante el período del 2004-2010. La CSTS era una propuesta para continuar operando comercialmente los Transbordadores STS, después de la NASA. En septiembre del 2011, La NASA anuncio la selección del diseño para el nuevo STS que lanzará la nave espacial Orion y más hardware para misiones que van más allá de una órbita baja Terrestre.

El programa de Servicios Comerciales de Transporte Orbital inició en el 2006 con el propósito de crear vehículos de carga no tripulados operados comercialmente para dar servicio a la ISS. El programa de Desarrollo de Tripulación Comercial (CCDev) se inició en el 2010 para crear naves espaciales comerciales, capaces de entregar al menos cuatro miembros de la tripulación a la ISS, para permanecer conectados durante 180 días, y luego regresar de nuevo a la Tierra. Estas naves hubieran sido operacionales para la década del 2010.

Transbordadores Espaciales han aparecido tanto en obras de ficción como en obras de no ficción, desde películas de niños hasta documentales. Ejemplo tempranos incluyen la película de James Bond de 1979, "Moonraker"; el videojuego de Activision de 1982 "Space Shuttle: A Journey Into Space"; y la novela de G. Harry Stine de 1981 "Shuttle Down". En la película de 1986 "SpaceCamp", el "Atlantis" es lanzado al espacio accidentalmente con un grupo de participantes del Campamento Espacial de EUA a bordo. En el vídeo juego The Dig de 1995, desarrollado por la compañía "LucasArts", el transbordador Atlantis participó en la misión de transporte a los astronautas hacia el asteroide. La película de 1998 "Armageddon" mostró una tripulación combinada de trabajadores petroleros y militares de EUA que pilotean dos Transbordadores modificados para evitar la destrucción de la Tierra por un asteroide. La película del 2013 "Gravity" presenta al Transbordador Espacial ficticio "Explorer", cuya tripulación sufre un accidente bajo una lluvia de basura espacial viajando a alta velocidad. Además, la franquicia Star Trek llamó "Discovery" a la nave principal de la serie en agradecimiento a que la NASA llamó al primer transbordador "Enterprise", al igual que la famosa nave.

El Transbordador Espacial también ha sido convertido en varios juguetes y modelos; por ejemplo, un modelo del Trasbordado Espacial de Lego fue construido por visitas al Centro Espacial Kennedy, y modelos más pequeños han sido vendidos comercialmente como un set estándar de ""LegoLand"". El Transbordador Espacial también aparece en varios simuladores de vuelo y simuladores de vuelo espacial como, por ejemplo: "Microsoft Space Simulator, Orbiter, FlightGear," y "X-Plane".
El servicio Portal de EUA ha publicado varias versiones postales que muestran al Transbordador Espacial. La primera de estas estampas se publicó en 1981, y están en exhibición en el Museo Postal Nacional.








</doc>
<doc id="4914" url="https://es.wikipedia.org/wiki?curid=4914" title="Connochaetes">
Connochaetes

Los ñus o ñúes (Connochaetes) son un género de mamíferos artiodáctilos de la subfamilia Alcelaphinae. Son antílopes de África que incluyen al ñu negro ("Connochaetes gnou") y al ñu azul ("Connochaetes taurinus") como sus únicas especies. El género pertenece a la familia de los bóvidos.

El nombre científico de este animal, "Connochaetes", proviene de las palabras griegas κόννος, "kónnos", barba, y χαίτη|, "jaítē", "cabello suelto", (al modo de la melena del león). 
El nombre ñu, o "gnu" en la transcripción de otras lenguas, parece originado en "t'gnu" el nombre joisán de estos animales o de "!nu", la forma san del mismo nombre. 

Los ñus tienen un aspecto desgarbado que recuerda a un extraño cruce entre vaquilla y antílope, con crines largas y desgreñadas y cola empenachada de caballo. Poseen largas barbas que cuelgan a lo largo de todo el cuello desde la barbilla hasta el pecho, y patas muy largas de pezuñas afiladas. Se cuentan entre los herbívoros que mejor prosperan en África y forman los rebaños más numerosos de la Tierra. En el parque nacional Serengueti, en Tanzania, más de un millón de estos ejemplares migran con los cambios de estaciones en busca de hierba fresca. 
Su evidente parecido con el ganado vacuno le da su nombre holandés de "wildebeest", que podría traducirse por "ganado salvaje".

Las hembras alumbran al principio de la estación de las lluvias y las crías pueden correr a los pocos minutos de nacer; durante su juventud, los ñus son incluso más rápidos en la carrera que sus adultos, pudiendo alcanzar los 80 km/h y los más viejos alcanzan los 60 km/h.

Los ñus son antílopes sociales que se reúnen en grupos familiares de hembras con sus crías y jóvenes, mientras que los machos adultos se reúnen en grupos aparte. No obstante, esos pequeños grupos tienden a asociarse entre sí y con los de otras especies de ungulados de la sabana, como cebras y gacelas de Thomson, de forma que se producen grandes aglomeraciones de individuos. En contra de lo que pueda parecer, no se produce rivalidad por la comida entre las diferentes especies, ya que las cebras prefieren las hierbas altas y fibrosas, las gacelas prefieren las cortas y secas, y los ñus las que se encuentran en un término medio. Los ñus además, se aprovechan de la vista de las gacelas y el olfato de las cebras para descubrir a los depredadores.

Durante la época de celo, que coincide con la época de las grandes migraciones, los machos adultos mantienen parcelas de terreno en donde intentan mantener a un grupo de hembras con las que aparearse. Esto los obliga a estar en constante movimiento para poder reunirlas y agruparlas, y al mismo tiempo defenderlas de otros machos que buscan una hembra de la que apropiarse. El constante trasiego de individuos ayuda a evitar la consanguinidad y a seleccionar a los ejemplares más fuertes y resistentes para la reproducción.

Los ñus son presa de muchos carnívoros tales como licaones, guepardos, leopardos, leones, cocodrilos, etc. Estos últimos suelen ser los depredadores más comunes de los adultos. Sus crías son presa, por lo general, de chacales, servales y guepardos. Tanto machos como hembras están armados con peligrosos cuernos ganchudos, pero no suelen emplearlos para defenderse, prefiriendo la huida a la carrera para escapar de sus cazadores.

Se han descrito las siguientes especies:
Ambas especies son realmente abundantes, pero el ñu negro o de cola blanca, de color oscuro, crines erectas y cola cubierta de crines blancas, ha estado a punto de compartir el mismo final que otros muchos ungulados oriundos de Sudáfrica, y su número es bastante menor al de sus parientes más norteños; si la especie se ha preservado ha sido gracias a la acción de granjas privadas que los han mantenido cautivos.

En las ruinas de la antigua Hieracómpolis, en Egipto, se ha hallado una loseta con un animal tallado similar al ñu, que podría fecharse a 3000 años a. C.

El ñu es el animal simbólico o mascota del Proyecto GNU (pronunciación similar a ñu, en español)




</doc>
<doc id="4915" url="https://es.wikipedia.org/wiki?curid=4915" title="Africano (desambiguación)">
Africano (desambiguación)

Africano puede hacer referencia a:


</doc>
<doc id="4918" url="https://es.wikipedia.org/wiki?curid=4918" title="Organización Internacional de Normalización">
Organización Internacional de Normalización

La Organización Internacional de Normalización, también llamada Organización Internacional de Estandarización (originalmente en inglés: "International Organization for Standardization", conocida por la abreviación 
ISO) es una organización para la creación de estándares internacionales compuesta por diversas organizaciones nacionales de normalización.

Fundada el , la organización promueve el uso de estándares privativos, industriales y comerciales a nivel mundial. Su sede está en Ginebra (Suiza) y hasta 2015 trabajaba en 196 países.

La Organización Internacional de Normalización (ISO) es una organización independiente y no-gubernamental formada por las organizaciones de normalización de sus 164 países miembros. Es el mayor desarrollador mundial de estándares internacionales voluntarios y facilita el comercio mundial al proporcionar estándares comunes entre países. Se han establecido cerca de veinte mil estándares cubriendo desde productos manufacturados y tecnología a seguridad alimenticia, agricultura y sanidad.

El uso de estándares facilita la creación de productos y servicios que sean seguros, fiables y de calidad. Los estándares ayudan a los negocios a aumentar la productividad a la vez que minimizan los errores y el gasto. Al permitir comparar directamente productos de diferentes fabricantes, facilita que nuevas compañías puedan entrar en nuevos mercados y ayudar en el desarrollo de un comercio global con bases justas. Los estándares también sirven para proteger a los consumidores y usuarios finales de productos y servicios, asegurando que los productos certificados se ajusten a los mínimos estandarizados internacionalmente.

Los tres idiomas oficiales de ISO son inglés, francés y ruso. El nombre de la organización en francés es "Organisation internationale de normalisation", "International Organization for Standarization" en inglés y "Международная организация по стандартизации" en ruso. Según ISO, debido a que su nombre en diferentes idiomas tendría diferentes siglas ("IOS" en inglés, "OIN" en francés, etc.), la organización adoptó "ISO" como sus siglas en referencia a la palabra griega "isos" (, traducido como "igual") Sin embargo, durante las reuniones fundacionales de la nueva organización, esta palabra nunca fue mencionada, así que esta explicación podría haber sido imaginada posteriormente.

Tanto el nombre "ISO" como el logo son marcas registradas, y su uso está restringido.

La organización conocida hoy en día como ISO nació en 1926 como la Federación Internacional de Asociaciones de Estandarización Nacionales (ISA). Fue suspendida en 1942 durante la Segunda Guerra Mundial, pero tras la guerra se le propuso por parte del Comité Coordinador de Estándares de las Naciones Unidas (UNSCC) formar un nuevo cuerpo de estándares globales. En octubre de 1946, delegados de ISA y de UNSCC de 25 países se reunieron en Londres y decidieron unir fuerzas para crear la nueva Organización Internacional de Normalización; la nueva organización comenzaría oficialmente a operar en febrero de 1947. 

ISO es una organización voluntaria cuyos miembros son autoridades reconocidas en estandarización, cada uno representando a un país. Los miembros se reúnen anualmente en la Asamblea General para discutir los objetivos estratégicos de ISO. La organización está coordinada por un Secretariado Central con sede en Ginebra.

Un Consejo rotativo de 20 miembros proporcionan guía y gobierno, incluyendo el establecimiento de los presupuestos anuales del Secretariado Central.

La Junta de Administración Técnica es la responsable de cerca de 250 comités técnicos, quienes desarrollan los estándares ISO.

ISO ha formado varios comités conjuntos con la Comisión Electrotécnica Internacional (IEC) para desarrollar estándares y la terminología relacionados con áreas de tecnología eléctrica y electrónica.

El Comité Conjunto Técnico ISO/IEC 1 (JTC 1) fue creado en 1987 para "desarrollar, mantener, promover y facilitar los estándares relacionados con la Tecnología de la Información".

El Comité Conjunto Técnico ISO/IEC 2 (JTC 2) se creó en 2009 con el propósito de «normalizar el campo de la eficiencia energética y las fuentes de energías renovables».

ISO tiene 164 países miembros, de un total de 206 países en el mundo.

ISO tiene tres categorías de miembros:

Los miembros participantes son llamados miembros "P", en contraposición a los miembros observadores, que son llamados miembros "O".

ISO está financiada por una combinación de:

Los principales productos de ISO son sus estándares internacionales. ISO también publica informes técnicos, especificaciones técnicas, especificaciones disponibles públicamente, erratas técnicas, y guías.





Son metaestándares que cubren «materias relacionadas con la normalización internacional». Son nombradas utilizando el formato ""ISO[/IEC]Guide N:yyyy: Título""Por ejemplo:

Un estándar publicado por ISO/IEC es la última etapa en un largo proceso que normalmente comienza con la propuesta de un nuevo trabajo en un comité. Aquí se presentan algunas abreviaturas usadas para marcar un estándar cuando está en este estado:


Abreviaturas usadas para enmiendas:

Otras abreviaturas:

Los Estándares Internacionales son desarrollados por los comités técnicos de ISO (TC) y subcomités (SC) por un proceso con seis etapas: 

Los TC y SC pueden establecer grupos de trabajo (WG) de expertos para la preparación de borradores de trabajo. Los Subcomités pueden tener varios grupos de trabajo, los cuales a su vez pueden tener varios Subgrupos (SG).

Es posible omitir ciertas etapas, si hay algún documento con un cierto grado de madurez al principio del proyecto de estandarización, por ejemplo un estándar desarrollado por otra organización. Las directrices de ISO/IEC también permiten el llamado "Procedimiento abreviado". En este procedimiento el documento es enviado directamente para aprobación como un Borrador de Estándar Internacional (DIS) a los cuerpos miembros de ISO o como un Borrador Final de Estándar Internacional (FDIS) si el documento fue desarrollado por un cuerpo internacional de estandarización reconocido por el Consejo de ISO.

El primer paso -una propuesta de trabajo (Nueva Proposición)- es aprobado el subcomité o comité técnico relevante (por ejemplo, SC29 y JTC1 respectivamente en el caso de Moving Picture Experts Group - ISO/IEC JTC1/SC29/WG11). Un grupo de trabajo (WG) de expertos es establecido por el TC/SC para la preparación de un borrador de trabajo. Cuando el objetivo de un nuevo trabajo está lo suficientemente claro, alguno de los grupos de trabajo (por ejemplo, MPEG) normalmente hace una petición abierta de proposiciones -conocido como "petición de propuestas". El primer documento que es producido por ejemplo para los estándares de codificación de audio y vídeo es llamado un modelo de verificación (VM) (anteriormente también llamado un "modelo de simulación y prueba"). Cuando se alcanza la suficiente confianza en la estabilidad del estándar en desarrollo, se produce un borrador de trabajo (WD). Tiene la forma de un estándar, pero se mantiene internamente para ser revisado por el grupo de trabajo. Cuando un borrador de trabajo es lo suficientemente sólido y el grupo de trabajo está seguro que de ha desarrollado la mejor solución técnica para el problema tratado, éste se convierte en un borrador de comité (CD). Si es necesario, es entonces cuando es enviado a los miembros P del TC/SE (los cuerpos nacionales) para votación. 

El CD pasa a ser un borrador final de comité (FCD) si el número de votos positivos está por encima del quorum. Varios borradores de comité pueden ser evaluados hasta que se alcance un consenso en su contenido técnico. Cuando se alcanza, el texto es finalizado para ser enviado como un borrador de Estándar Internacional (DIS). El texto es entonces enviado a los cuerpos nacionales para votación y ser comentado en un periodo de cinco meses. Es aprobado como un borrador final de Estándar Internacional (FDIS) si un las dos terceras partes de los miembros P del TC/SC están a favor y no más de un cuarto del total de votos emitidos son negativos. ISO celebrará entonces una votación con los Cuerpos Nacionales donde no se podrán proponer cambios técnicos al texto (una votación se sí/no), en un periodo de dos meses. Es aprobado como un Estándar Internacional (IS) si las dos terceras partes de los miembros P del TC/SC están a favor y no más de un cuarto de los votos emitidos son negativos. Tras la aprobación, solo se introducirán cambios menores editoriales en el texto. El texto final se envía al Secretariado Central de ISO, el cual lo publica como un Estándar Internacional.

El hecho de que muchos de los estándares creados por ISO son ubicuos ha llevado, en ocasiones, al uso de "ISO" para llamar al producto en sí que se adecúa a un estándar. Algunos ejemplos de ello son:

A excepción de un pequeño número de estándares aislados, los estándares ISO no están disponibles gratuitamente, cuyo coste ha sido visto por algunos sectores como demasiado elevado para proyectos pequeños software de código abierto.

Los procedimientos abreviados del ISO/IEC JTC1 (usado por Office Open XML y OpenDocument) han cosechado críticas en relación a la estandarización de Office Open XML. Martin Bryan, convocante del ISO/IEC JTC1/SC34 WG1, dijo al respecto:

El empresario en seguridad e inversor de Ubuntu, Mark Shuttleworth, comentó en el proceso de estandarización de Office Open XML que "cree que devalúa la confianza de la gente en el procedimiento de creación de estándares" y alegó que ISO no estaba llevando a cabo sus responsabilidades. También señaló que Microsoft ha presionado activamente a muchos países que tradicionalmente no han participado en ISO y formado comités con empleados de Microsoft, proveedores de soluciones y distribuidores afines a Office Open XLM.




ISO 45001 de Seguridad y salud en el trabajo


</doc>
<doc id="4921" url="https://es.wikipedia.org/wiki?curid=4921" title="Gráficos vectoriales escalables">
Gráficos vectoriales escalables

Gráficos vectoriales escalables, o gráficos vectoriales redimensionables (del inglés Scalable Vector Graphics) o SVG es un formato de gráficos vectoriales bidimensionales, tanto estáticos como animados, en formato XML, cuya especificación es un estándar abierto desarrollado por el W3C desde 1999.

SVG se convirtió en una recomendación del W3C en septiembre de 2001, por lo que ya ha sido incluido de forma nativa en el navegador web del W3C Amaya. Las versiones 1.5 y posteriores de Mozilla Firefox soportan gráficos hechos con SVG, así como el navegador Opera que desde su versión 8 ha implementado SVG 1.1 Tiny en su núcleo. Navegadores como Google Chrome, Safari e Internet Explorer 9 también son capaces de mostrar imágenes en formato SVG sin necesidad de complementos externos. Otros navegadores web como versiones anteriores a la 9 de Internet Explorer, necesitan un conector o "plug-in".

En 1996, Chris Liley redactó para el W3C un documento con directrices sobre los posibles requerimientos para integrar un formato estándar de archivo para describir elementos gráficos vectoriales. Para 1998 varias empresas habían turnado propuestas a este organismo, de las cuales dos principalmente sirvieron como base para los borradores que constituirían el SVG: el VML desarrollado por Microsoft para su formato de documentos RTF y el PGML desarrollado entonces por Adobe en coordinación con IBM, Netscape y SUN. Tras la publicación del estándar SVG, Microsoft disoció el VML arguyendo que era un producto ya en el mercado, mientras que Adobe celebró abiertamente estandarización del PGML; el soporte para SVG fue integrado a distintos productos de "software" de esta última compañía como Adobe Illustrator y el visualizador de archivos ASV.

La especificación de SVG permite tres tipos de objetos gráficos:


Los objetos gráficos pueden ser agrupados, transformados y compuestos en objetos previamente renderizados, y pueden recibir un estilo común. El texto puede estar en cualquier espacio de nombres XML admitido por la aplicación, lo que mejora la posibilidad de búsqueda y la accesibilidad de los gráficos SVG. El conjunto de características incluye las transformaciones anidadas, las trayectorias de recorte, las máscaras alfa, los filtros de efectos, las plantillas de objetos y la extensibilidad.

El dibujado de los SVG puede ser dinámico e interactivo. El modelo Document Object Model (DOM) para SVG, que incluye el DOM XML completo, permite animaciones de gráficos vectoriales sencillas y eficientes mediante ECMAScript o SMIL. Un conjunto amplio de manejadores de eventos, como "onMouseOver" y "onClick", puede ser asignado a cualquier objeto SVG. Debido a su compatibilidad y relación con otros estándares de Web, características como el scripting pueden ser aplicadas a elementos SVG y a otros elementos XML desde distintos espacios de nombre XML simultáneamente y dentro de la misma página web. Un ejemplo extremo de esto es un juego completo de Tetris realizado como un objeto SVG. 

Si el espacio de almacenamiento es un problema, las imágenes SVG pueden ser guardadas como archivos comprimidos con gzip, en cuyo caso pasan a ser imágenes SVGZ. Debido a la verbosidad del XML, este tiende a comprimirse muy bien, y estos ficheros pueden ser mucho más pequeños.

El fichero o archivo vectorizado original (SVG) es más pequeño que la versión de mapa de bits exceptuando cualquier programa que fuerza a una vectorialización ya que vectorializa detalles en general de poca utilidad, aproximables por otros métodos o invisibles, a los cuales inserta información oculta y firmas de todo tipo.

Complejidad
Por ser un lenguaje vectorial, SVG permite crear imágenes complejas.

Los elementos geométricos son objetos provistos de atributos genéricos básicos y optativos o por defecto.

Todos los objetos se encuentran enmarcados en una ventana, con un ancho (width) y un alto (height) determinados con números enteros. Esta ventana posee un sistema de coordenadas cuyo origen está situado en la parte superior izquierda y en el cual los valores positivos de x y y se orientan a la derecha y hacia abajo respectivamente. Cada valor está determinado con números enteros o con un porcentaje respecto del área de trabajo.

Un sistema de referencia coordenado o un objeto determinado en el estándar se puede modificar mediante transformaciones. El ejemplo que sigue muestra dos transformaciones: una de posición (traslado) del origen de coordenadas, mediante la orden "translate" y otra de escala, usando la orden "scale".

<svg width="200" height="200" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
</svg>

Todos los objetos comparten algunos atributos genéricos:





El objeto línea consta básicamente de dos puntos, (x1, y1) y (x2, y2), entre los cuales se dibuja un segmento recto:

codice_17

El objeto rectángulo consta básicamente de un punto cuya esquina superior izquierda u origen es (x, y), y sus dimensiones están representadas por os parámetros height y width:

codice_18

El objeto círculo consta de un punto cuyo centro tiene las coordenadas (cx, cy), y su radio es r:

codice_19

El objeto elipse consta de un punto cuyo centro tiene las coordenadas (cx, cy), un radio horizontal, rx, y un radio vertical, ry:

codice_20

El objeto mixtilínea consta básicamente de puntos unidos por líneas que pueden cerrar o no un espacio. Estos puntos forman cadenas iniciadas con M o m que pueden unirse con otras para formar un todo. El uso de mayúsculas se refiere a valores absolutos de coordenadas y de minúsculas para los valores relativos, es decir, considera el último punto dado como el origen de coordenadas.

Enlaces entre puntos:






Aberturas:

Para generar una abertura en un recinto con interior(fill distinto de "none") se usa una cadena con orientación contraria al borde del recinto (hay dos posibles órdenes: el horario y el antihorario), esto generará un recinto cuyo interior tendrá una sola orientación, en caso contrario no hay abertura.

Se puede añadir texto, a partir de: un punto base x e y y se configuran las características de la cadena de texto con los parámetros: font-family (tipo de letra), font-size (tamaño de letras ) y estilo de letra por font-style (con uno de cuatro valores posibles: normal, inherit, oblique o italic):

codice_23

Se pueden agrupar objetos para que compartan atributos por defecto y una disposición fija en el espacio para poder duplicarlos cómodamente:

codice_24

Para duplicar objetos tanto internos como externos se tiene que incluir en la etiqueta <svg> el siguiente fragmento:

codice_25

Se puede duplicar objetos identificados previamente con id="...", donde el espacio de puntos indica el nombre.

codice_26

Se pueden añadir o incrustar enlaces a imágenes del tipo *.png, *.jpg o *.svg dentro de un rectángulo de parámetros x, y, width y height:

codice_27

Esta opción no es soportada por Wikimedia Commons.

Se pueden recortar objetos mediante rellenos de otros objetos, por ejemplo, un círculo mediante un recinto triangular. En el ejemplo que sigue, la orden clipPath con el parámetro "id" identifica el nombre del área a recortar. La orden path siguiente con el parámetro clip-rule indica si un punto queda dentro del área de relleno (nonzero) o fuera (evenodd, como en el caso presentado). Luego de cerrar la orden con </clipPath>, la orden siguiente traza un círculo al cual se aplica el área recortada con el parámetro clip-path="url(#nombre del área)".
<clipPath id="cortador">
</clipPath>
<circle r="80" cx="12" cy="130" stroke="#000" fill="#999" clip-path="url(#cortador)"/>

Las transformaciones permiten efectuar semejanzas sobre imágenes mediante la matriz:

formula_1

codice_28


codice_29


codice_30


codice_31


codice_32


Actualmente hay muchos programas de diseño gráfico que lo soportan. Entre los que son software libre está Inkscape. La Suite Ofimática libre LibreOffice, incorpora Draw, programa exclusivo para trabajar con SVG y otros archivos gráficos vectoriales. También hay editores en línea que permiten abrir archivos o crearlos como Method Draw, Vector Paint, Draw SVG o svgreal.





</doc>
<doc id="4922" url="https://es.wikipedia.org/wiki?curid=4922" title="UEFA">
UEFA

La Unión de Federaciones Europeas de Fútbol (), referida comúnmente por su acrónimo UEFA, es la confederación europea de asociaciones nacionales de fútbol y máximo ente de este deporte en el continente. Agrupa en la actualidad a 55 asociaciones y es una de las seis confederaciones pertenecientes a la Federación Internacional de Fútbol Asociación (FIFA), máximo rector en el mundo.

Fundada el 15 de junio de 1954, su sede central se encuentra en Nyon, Suiza, y es la encargada de organizar los distintos campeonatos de naciones de Europa, además de promover, desarrollar, controlar y velar por el fútbol, sus cometidos, finanzas, reglamentos y medios del mismo, siendo la Eurocopa, oficialmente Campeonato de Europa de Naciones, su principal torneo masculino, y la Eurocopa Femenina, oficialmente Campeonato de Europa Femenino, su homólogo de mujeres. De igual modo es quien trata las diferentes cuestiones de las federaciones nacionales del territorio europeo, así como su fútbol de formación organizando también competiciones para dichas categorías conformando un total de 15 torneos entre todas las disciplinas.

Es la asociación continental más laureada del ámbito FIFA pues suma entre todas las selecciones de sus miembros un total de , donde destacan 41 títulos mundiales, además de ser la más reconocida. Entre ellas destaca Alemania (DFB), que es de ser la más galardonada de Europa con 34 títulos; España (RFEF), que es su miembro más premiado en el continente con 29 títulos; y Francia (FFF), que es la vigente campeona del mundo y la más condecorada en competiciones mundiales con seis trofeos.

La principal competición en lo que se refiere a los clubes es la Liga de Campeones, tanto en categoría masculina —disputada por primera vez en 1955—, como en femenina —establecida en 2001—. En ellas dominan los clubes españoles con 18 títulos y los alemanes con 9 respectivamente.

Es la tercera confederación continental más antigua, y la que más miembros posee siendo el último en incorporarse la Federación de Fútbol de Kosovo (FFK) —como estado parcialmente reconocido internacionalmente— el 3 de mayo de 2016.

La Unión Europea de Asociaciones de Fútbol () fue fundada el 15 de junio de 1954 en Basilea, Suiza, debido a las necesidades imperantes en el fútbol europeo continental, hasta entonces sin un organismo rector. Es por ello que surge tras un consenso de tres de las federaciones más reconocidas de la época, la Federación Italiana (FIGC), la Federación Francesa (FFF) y la Federación Belga (KBVB/URBSFA). Hasta la fecha eran las propias federaciones de cada país las que velaban por dicho deporte en cada región siguiendo las normativas FIFA, y que empezaba a internacionalizarse en el territorio merced sobre todo a partidos de índole amistosa y a las primeras competiciones de clubes.

Entre ellas destacaron la Copa de Europa Central y la Copa Latina de Europa del Sur, ambas consideradas predecesoras de la actual Liga de Campeones (originalmente Copa de Europa) , que fue la primera competición oficial lanzada por el organismo en 1955. Sin embargo, ya se encontraba inmersa desde su fundación en la creación de una competición europea de selecciones que finalmente vio la luz en 1960, la Copa de Europa de Naciones, más conocida como Eurocopa. Ambas son en la actualidad dos de las competiciones futbolísticas más prestigiosas del mundo, y precisamente el promotor de esta última, Henri Delaunay, fue el primer secretario general del organismo mientras que Ebbe Schwartz fue su primer presidente.

Su sede se ubicó en París, Francia, hasta que en 1959 fue trasladada a Berna y posteriormente en 1995 a Nyon, Suiza, donde continúa localizada. Conocida simplemente por su acrónimo UEFA está definido en el estatuto de la organización como la abreviación de su nombre en francés, sin embargo es comúnmente asociado con el inglés "Union of European Football Associations".

El organismo es una de las más fuertes y reconocidas de las seis confederaciones mundiales, debido a la importancia de sus miembros, entre los que se encuentran varias de las principales asociaciones, ligas y selecciones de fútbol del mundo. Un ejemplo de esto es que la FIFA otorgó a la UEFA 13 de los 32 cupos de participantes para la Copa Mundial de 2014 que se celebró en Brasil.

A nivel organizativo promueve competiciones de clubes, como la ya mencionada Liga de Campeones y la Liga Europa (originalmente Copa de la UEFA), o de selecciones nacionales, siendo la principal la Eurocopa (oficialmente Campeonato de Europa de Naciones), todas a nivel masculino. En categoría de mujeres organiza también la Liga de Campeones y la Eurocopa en claro reconocimiento y legitimidad al auge que tiene en la actualidad el fútbol femenino.

Son 55 las asociaciones de fútbol que pertenecen a la UEFA. Las representadas son todas las naciones geográficamente dentro de Europa a excepción de Vaticano y Mónaco, que tampoco están representadas ni en la FIFA ni en otra Confederación Continental de Fútbol. A estas se suman Armenia, Azerbaiyán y Georgia (ubicadas en el Cáucaso, se discute su pertenencia geográfica a Europa o Asia); Israel y Chipre (geográficamente en Asia, aunque políticamente siempre asociadas a Europa); y Rusia, Turquía y Kazajistán (mayormente en Asia y con una porción menor en Europa).

Es importante mencionar que los cuatro países constituyentes del Reino Unido (Inglaterra, Escocia, Gales e Irlanda del Norte) participan de forma separada (esta excepcionalidad solo se da en fútbol, rugby, críquet y otros deportes cuyo origen sean de los países de la Commonwealth, incluido Reino Unido, o de Irlanda), y que el archipiélago autónomo de Islas Feroe compite de manera independiente a Dinamarca. Israel formaba parte de la Confederación Asiática de Fútbol hasta ser expulsada en 1974 por el conflicto árabe-israelí. Kazajistán también fue en el pasado miembro de la AFC. Gibraltar y Kosovo (estado parcialmente reconocido) fueron aceptados como miembros de la UEFA el 3 de mayo de 2016, y fueron aceptados como miembros de la FIFA el 13 de mayo de 2016.

<nowiki>*</nowiki> <br>
<nowiki>#</nowiki> 


Hasta la fecha solamente cuatro clubes europeos (Juventus FC en 1985, AFC Ajax en 1992, Bayern de Múnich en 1996 y Chelsea FC en 2013) han conquistado, al menos en una ocasión, cada una de las tres principales (Copa Europea de Clubes Campeones/Liga de Campeones de la UEFA, Copa Europea de Ganadores de Copa y Copa de la UEFA/Liga Europa de la UEFA). Al club italiano, en calidad de primer club en la historia del fútbol europeo en lograr dicho suceso, le fue conferida en reconocimiento la «Placa de la UEFA» (en. The UEFA Plaque) de parte de la Unión Europea de Asociaciones de Fútbol en Ginebra, Suiza, el 12 de julio de 1988.

La Juventus es, además, el único club en Europa que ha conquistado todas las organizadas por la confederación europea de fútbol.

La clasificación mundial de la FIFA del 25 de julio de 2019 muestra a las siguientes selecciones como las mejores de la UEFA:
La clasificación mundial de la FIFA del 25 de julio de 2019 muestra a las siguientes selecciones como las mejores de la UEFA:

Coeficientes UEFA a 25 de julio de 2019:
Coeficientes UEFA a 25 de julio de 2019:

A continuación se listan los clubes y selecciones vencedoras de las competiciones oficiales del fútbol asociación, tanto masculinas como femeninas. Entre el palmarés no se incluye la Copa Intertoto, pese a estar reconocido, por ser un torneo clasificatorio para otra de sus competiciones, la Liga Europa, ni las competiciones "amateurs" o de fútbol sala, al no estar recogido bajo mismas normativas.

De entre todas las ligas de primera división, y en cómputo global, es el Rangers Football Club escocés quien suma más títulos con 54, seguido de los 52 del Linfield Football Club norirlandés, los 48 del Celtic Football Club escocés, los 44 del Olympiakós Pireás griego, 36 del Sport Lisboa e Benfica portugués, los 34 logrados por el Royal Sporting Club Anderlecht belga, los 33 del Amsterdamsche Football Club Ajax neerlandés, del Athltic Club Sparta Praha checo, del Real Madrid Club de Fútbol español, y de la Juventus Football Club italiana, los 32 del Sportklub Rapid Wien austríaco y los 31 del Centralen Sporten Klub na Armiyata Sofiya búlgaro, siendo los únicos clubes en haber conquistado más de una treintena de campeonatos.





</doc>
<doc id="4927" url="https://es.wikipedia.org/wiki?curid=4927" title="15 de febrero">
15 de febrero

El 15 de febrero es el 46.º (cuadragésimo sexto) día del año en el calendario gregoriano. Quedan 319 días para finalizar el año y 320 en los años bisiestos.






















</doc>
<doc id="4928" url="https://es.wikipedia.org/wiki?curid=4928" title="16 de febrero">
16 de febrero

El 16 de febrero es el 47.º (cuadragésimo séptimo) día del año en el calendario gregoriano. Quedan 318 días para finalizar el año y 319 en los años bisiestos.







</doc>
<doc id="4930" url="https://es.wikipedia.org/wiki?curid=4930" title="18 de febrero">
18 de febrero

El 18 de febrero es el 49.º (cuadragésimo noveno) día del año en el calendario gregoriano. Quedan 316 días para finalizar el año y 317 en los años bisiestos.























</doc>
<doc id="4931" url="https://es.wikipedia.org/wiki?curid=4931" title="19 de febrero">
19 de febrero

El 19 de febrero es el 50.º (quincuagésimo) día del año en el calendario gregoriano. Quedan 315 días para finalizar el año y 316 en los años bisiestos.









</doc>
<doc id="4932" url="https://es.wikipedia.org/wiki?curid=4932" title="20 de febrero">
20 de febrero

El 20 de febrero es el 51.º (quincuagésimo primer) día del año en el calendario gregoriano. Quedan 314 días para finalizar el año y 315 en los años bisiestos.








</doc>
<doc id="4933" url="https://es.wikipedia.org/wiki?curid=4933" title="21 de febrero">
21 de febrero

El 21 de febrero es el quincuagésimo segundo día del año según el calendario gregoriano. Quedan 313 días para finalizar el año y 314 en los años bisiestos.









</doc>
<doc id="4934" url="https://es.wikipedia.org/wiki?curid=4934" title="22 de febrero">
22 de febrero

El 22 de febrero es el 53.º (quincuagésimo tercer) día del año en el calendario gregoriano. 312 días para finalizar el año y 313 en los años bisiestos.





















l* San Maximiano de Ravena, obispo italiano.



</doc>
<doc id="4935" url="https://es.wikipedia.org/wiki?curid=4935" title="23 de febrero">
23 de febrero

El 23 de febrero es el 54.º (quincuagésimo cuarto) día del año en el calendario gregoriano. Quedan 311 días para finalizar el año y 312 en los años bisiestos.










</doc>
<doc id="4936" url="https://es.wikipedia.org/wiki?curid=4936" title="25 de febrero">
25 de febrero

El 25 de febrero es el 56.º (quincuagésimo sexto) día del año en el calendario gregoriano. Quedan 309 días para finalizar el año y 310 en los años bisiestos.








</doc>
<doc id="4937" url="https://es.wikipedia.org/wiki?curid=4937" title="27 de febrero">
27 de febrero

El 27 de febrero es el 58.º (quincuagésimo octavo) día del año en el calendario gregoriano. Quedan 307 días para finalizar el año y 308 días en los años bisiestos.









</doc>
<doc id="4941" url="https://es.wikipedia.org/wiki?curid=4941" title="Morfeo">
Morfeo

En la mitología griega, Morfeo (en griego antiguo Μορφεύς, de μορφή "morphê", ‘forma’) es el dios de los sueños, hijo de la personificación del sueño (Hipnos), y encargado de llevar sueños a reyes y emperadores. Según ciertas teologías antiguas, es el principal de los Oniros, los mil hijos engendrados por Hipnos (el Sueño) y Nix (la Noche, su madre), o por Hipnos con Pasítea.

Era representado con alas que batía rápida y silenciosamente, permitiéndole ir a cualquier rincón de la Tierra. Morfeo se encargaba de inducir los sueños de quienes dormían y de adoptar una apariencia humana para aparecer en ellos, especialmente la de los seres queridos (de ahí su nombre), permitiendo a los mortales huir por un momento de las maquinaciones de los dioses.

Fue castigado por Zeus por haber revelado secretos a los mortales a través de sus sueños. De su nombre procede la expresión «estar en los brazos de Morfeo», que significa ‘soñar’ y por extensión ‘dormir’ o viceversa.

Los médicos griegos rindieron culto a Morfeo especialmente en los santuarios de los grandes oráculos y en los templos de Esculapio, dios de la medicina, donde era invocado por medio de complejos rituales donde se empleaban baños, ayunos, quema de inciensos, música y cantos que inducían al sueño.







En muchas culturas existen diferentes figuras que son similares a Morfeo, en ciertas ocasiones se considera como el mismo a Hypnos y Morfeo. En la cultura anglosajona se tiene como figura representante del sueño a Sandman. En la mitología egipcia existía el dios Tot y el dios Tutu que representaban los sueños. En la cultura japonesa existían unos seres llamados Baku que se comían las pesadillas y los malos espíritus. En el hinduismo aparece el dios Vishru quién entre otras cosa representa los sueños.







</doc>
<doc id="4945" url="https://es.wikipedia.org/wiki?curid=4945" title="World Wide Web Consortium">
World Wide Web Consortium

El Consorcio WWW, en inglés: World Wide Web Consortium (W3C), es un consorcio internacional que genera recomendaciones y estándares que aseguran el crecimiento de la "World Wide Web" a largo plazo.

Este consorcio fue creado en octubre de 1994, y está dirigido por Tim Berners-Lee, el creador original del URL ("Uniform Resource Locator", Localizador Uniforme de Recursos), del HTTP ("HyperText Transfer Protocol", Protocolo de Transferencia de HiperTexto) y del HTML ("Hyper Text Markup Language", Lenguaje de Marcado de HiperTexto), que son las principales tecnologías sobre las que se basa la Web.

El W3C fue creado el 1 de octubre de 1994, por Tim Berners-Lee en el Instituto Tecnológico de Massachusetts (MIT), actual sede central del consorcio. Uniéndose posteriormente, en abril de 1995, INRIA en Francia, reemplazado por el ERCIM en 2003 como el huésped europeo del consorcio y la Universidad de Keiō (Shonan Fujisawa Campus) en Japón como huésped asiático, en septiembre de 1996. Estos organismos administran el W3C, que está integrado por:

La oficina española del W3C, establecida en 2003, está albergada por la Fundación CTIC en el Parque Científico Tecnológico de Gijón (Principado de Asturias).

A veces, cuando una especificación se hace demasiado grande, se divide en módulos independientes que pueden madurar a su propio ritmo. Ediciones posteriores de un módulo o especificación se conocen como niveles y se denotan por el primer número entero en el título (por ejemplo CSS3 = Nivel 3). Las revisiones posteriores en cada nivel se indican mediante un número entero después de un punto decimal (por ejemplo CSS2.1 = Revisión 1).

El proceso de formación de estándar del W3C se define en el documento de proceso de W3C, que presenta cuatro niveles de madurez a través del cual cada nueva norma o recomendación deben progresar.

Después de que suficiente contenido ha sido recopilado por el 'editor de borradores' y de la discusión, puede ser publicado como borrador de trabajo (WD, del inglés "Working Draft") para su revisión por la comunidad. Un documento WD es la primera forma de un estándar que está disponible públicamente. Comentarios de prácticamente cualquier persona son aceptados, aunque no se hacen promesas con respecto a actuar sobre algún elemento en particular comentado.

En esta etapa, el modelo de documento puede tener diferencias significativas respecto a su forma final. Como tal, cualquier persona que implementa estándares WD deben estar dispuestos a modificar significativamente sus implementaciones como el estándar madura.

Una recomendación candidata (en inglés: "Candidate recommendation") es una versión de una norma que es más madura que el WD. En este punto, el grupo responsable de la norma está convencido de que la norma cumple con su objetivo. El propósito de la CR es obtener ayuda de la comunidad de desarrollo en cuanto a la implementación de la norma.

El documento estándar puede cambiar aún más, pero en este momento, se decidió en su mayoría características significativas. El diseño de estas características aún puede cambiar debido a los comentarios de los ejecutores conforme el estándar madura.

Una recomendación propuesta (en inglés: "Proposed recommendation") es la versión de una norma que ha pasado los dos niveles anteriores. Los usuarios de la norma proporcionan entrada. En esta etapa, el documento se presentó al Consejo Asesor del W3C para su aprobación final.

Si bien este paso es importante, rara vez causa cambios significativos en un estándar a medida que pasa a la siguiente fase.

Tanto candidatos y propuestas pueden entrar en "última llamada" para indicar cualquier información adicional que deba ser tenida en cuenta.

Una recomendación de W3C  (en inglés: "W3C recommendation") es la etapa más madura de desarrollo. En este punto, la norma ha sido objeto de amplia revisión y pruebas, tanto en condiciones teóricas y prácticas. La norma está respaldada por el W3C, lo que indica su disposición para su despliegue al público, y fomentar un apoyo más generalizado entre los ejecutores y los autores.

Recomendaciones a veces se pueden implementar de forma incorrecta, en parte, o nada en absoluto, pero muchas normas definen dos o más niveles de conformidad que deben seguir los desarrolladores si se desean etiquetar su producto como compatible con W3C.

Una recomendación puede ser actualizada o ampliada por separado con erratas publicadas o editor de borradores no técnicos, hasta que haya suficientes cambios sustanciales se acumulan para producir una nueva edición o nivel de la recomendación. Además, el W3C publica diversos tipos de notas informativas que se van a utilizar como referencias.

A diferencia de la ISOC y otros organismos de normalización internacionales, el W3C no tiene un programa de certificación. El W3C ha decidido, por ahora, que no es adecuado para iniciar un programa de este tipo, debido al riesgo de crear más inconvenientes para la comunidad que beneficios.

En 2012 y 2013, el W3C empezó a considerar la inclusión de algunos Encrypted Media Extensions (EME) para DRM en HTML5, lo que ha sido criticado por estar en contra de la neutralidad e interoperabilidad, que distingue a los sitios construidos usando solo estándares del W3C sin incluir plugins propietarios como Flash.

Estándares publicados por el W3C/IETF (sobre el Internet protocol suite):




</doc>
<doc id="4949" url="https://es.wikipedia.org/wiki?curid=4949" title="Estándar">
Estándar

Estándar o standard puede hacer referencia a:




</doc>
<doc id="4953" url="https://es.wikipedia.org/wiki?curid=4953" title="CSS (desambiguación)">
CSS (desambiguación)

CSS es una sigla que puede hacer referencia a:

</doc>
<doc id="4954" url="https://es.wikipedia.org/wiki?curid=4954" title="Document Object Model">
Document Object Model

Document Object Model o DOM ('Modelo de Objetos del Documento' o 'Modelo en Objetos para la Representación de Documentos') es esencialmente una interfaz de plataforma que proporciona un conjunto estándar de objetos para representar documentos HTML, XHTML y XML, un modelo estándar sobre cómo pueden combinarse dichos objetos, y una interfaz estándar para acceder a ellos y manipularlos. A través del DOM, los programas pueden acceder y modificar el contenido, estructura y estilo de los documentos HTML y XML, que es para lo que se diseñó principalmente.

El responsable del DOM es el World Wide Web Consortium (W3C).

El DOM permite el acceso dinámico a través de la programación para acceder, añadir y cambiar dinámicamente contenido estructurado en documentos con lenguajes como ECMAScript (JavaScript).

La historia del DOM está entrelazada con la historia de "la guerra de los navegadores" a finales de la década de 1990 entre Netscape Navigator y Microsoft Internet Explorer, así como con la de JavaScript y JScript, los primeros lenguajes de scripting que se implementaron ampliamente en los motores JavaScript de los navegadores web.

El DOM define la manera en que objetos y elementos se relacionan entre sí en el navegador y en el documento.

Puede utilizarse cualquier lenguaje de programación adecuado para el diseño web. En el caso de JavaScript, cada objeto tiene un nombre, el cual es exclusivo y único. Cuando existe más de un objeto del mismo tipo en un documento web, estos se organizan en un vector.

Es posible asignarle una identificación a un objeto, y luego usarla para hacer referencia a éste, por ejemplo:
<div id="Juan">...</div>
Para hacer referencia a elementos del mismo tipo, los cuales, como se ha dicho, están organizados en un vector, se pueden utilizar la notación punto de la siguiente manera.
document.div[0]
document.div["Juan"]
document.div.Juan
Donde el elemento «Juan» es el primero del vector de elementos del tipo <nowiki><div></nowiki>.

También se puede usar la función "getElementById".
document.getElementById("Juan")
Los objetos computacionales, de la misma forma que cualquier objeto de la vida real, tienen propiedades. Algunos ejemplos de propiedades de objetos de la vida real son dimensiones, color y peso.

En la mayoría de los objetos computacionales algunas propiedades se pueden determinar de la siguiente manera:
Objeto.propiedad = valor;

//por ejemplo para el objeto «Vaso»

Vaso.color = rojo;
La manipulación de objetos sigue los mismos principios que en el lenguaje de programación que se esté utilizando. Una de las características de estos objetos es la "función" para la cual están diseñados, de hecho en la mayoría de ocasiones tienen más de una función. En JavaScript, muchas funciones para cada uno de los objetos, incluyendo el navegador y la ventana que lo contiene, han sido definidas previamente; adicionalmente, el usuario puede definir funciones de acuerdo a sus necesidades, por ejemplo el código:
function comeLaLetraA(Texto){

Añade una nueva función al documento utilizado para crear una página web.

Un evento desde el punto de vista computacional ocurre cuando cambia alguna situación en la computadora como, por ejemplo, la posición del ratón, la pulsación de alguna tecla, los contenidos de alguna de las memorias, la condición de la pantalla, etc. En la creación de páginas web estos eventos representan la interacción del usuario con la computadora.

Cuando alguno de estos eventos ocurre, como por ejemplo la presión de algún botón del ratón, es deseable que la computadora responda de alguna manera. Esta es la razón por la que existen "event handlers" ('encargados de manejar eventos') los cuales son objetos que responden a eventos. Una manera de añadir eventos en el DOM utilizando javascript es:
<element onevent="script">...</element>
Por ejemplo:
<div id="midivision" onClick="javascript:comeLaLetraA('bar');">
Aquí va otro texto
</div>
Otra forma de manipular eventos en JavaScript, al crear páginas web, es tratándolos como propiedades de los elementos que forman la página, por ejemplo:
object.event = funcion;
//como puede ser:
document.midivision.onclick = hazAlgo;
// también:
document.getElementById("midivision").onclick = hazAlgo;
En DOM se considera que un evento se origina en el exterior de la página web y se propaga de alguna manera hasta los elementos internos de la página. Un posible ejemplo de esta propagación es:

Siguiendo esta idea, se establecen tres etapas: "captura", la cual se da cuando el evento se está trasladando a su destino. "Blanco", que ocurre cuando llega al blanco, o sea que llega a su destino. Este destino es el objeto en el cual se va a crear una reacción a este evento. Finalmente la etapa de "burbujeo" que ocurre cuando el evento «regresa» a su posición original.

Ciertos objetos pueden estar pendientes de ciertos eventos. Para hacer esto el objeto añade un «oyente de eventos» con la función addEventListener. Cuando el evento ocurra, alguna función determinada se lleva a cabo. En este proceso se indica en qué momento la función se lleva a cabo, ya sea en la etapa de "captura" o en la etapa de "burbujeo". Este momento se indica con la palabra "true" si debe ocurrir en la etapa de captura o "false" si debe ocurrir en la etapa de burbujeo. En JavaScript se escribe de la siguiente manera:
objeto.addEventListener(evento, funcion, momento);

por ejemplo:

document.getElementById("mydivision").addEventListener("click", hazAlgo, false);


</doc>
<doc id="4957" url="https://es.wikipedia.org/wiki?curid=4957" title="Unidad de control">
Unidad de control

La unidad de control (UC), en inglés: "control unit" (CU), es uno de los tres bloques funcionales principales en los que se divide una unidad central de procesamiento (CPU). Los otros dos bloques son la unidad de proceso y el bus de entrada/salida.

Su función es buscar las instrucciones en la memoria principal, decodificarlas (interpretación) y ejecutarlas, empleando para ello la unidad de proceso.

Existen dos tipos de unidades de control: las cableadas, usadas generalmente en máquinas sencillas, y las microprogramadas, propias de máquinas más complejas. En el primer caso, los componentes principales son el circuito de lógica secuencial, el de control de estado, el de lógica combinacional y el de emisión de reconocimiento de señales de control. En el segundo caso, la microprogramación de la UC se encuentra almacenada en una micromemoria, a la cual se accede de manera secuencial para posteriormente ir ejecutando cada una de las microinstrucciones.


Las salidas de la UC se encargan de controlar la actividad del resto del dispositivo. Se puede pensar en una UC como una máquina de estado finito.

La UC es la circuitería que controla el flujo de datos a través del procesador, y coordina procesador, que a su vez controla el resto de la computadora.

Algunos ejemplos de dispositivos que requieren una UC son las CPU y las GPU. La edad de la información moderna no sería posible sin diseños complejos de la UC.

En un tiempo, las unidades de control para los CPU eran lógica "ad hoc", y eran difíciles de diseñar. Estas pueden identificarse como la parte principal de la computadora y del dispositivo principal que ayuda al computador a funcionar de una manera apropiada. Es construida de puertas lógicas, circuitos biestables, circuitos codificadores, circuitos decodificadores, contadores digitales y otros circuitos digitales. Su control está basado en una arquitectura fija, es decir, que requiere cambios en el cableado si el conjunto de instrucciones es modificado o cambiado. Esta arquitectura es preferida en las computadoras RISC, pues consiste en un conjunto de instrucciones más pequeño.

Las unidades de control usadas para invocar esas respuestas. Estas instrucciones son evidentes en el diseño de la arquitectura, pero también pueden ser representadas de otras maneras.

En 1951, por M. V. Wilkes, fue introducida la idea de microprogramación como un nivel intermediario para ejecutar instrucciones de programa de computadora (véase también microcódigo). Los microprogramas fueron organizados como una secuencia de microinstrucciones y almacenados en una memoria del control especial. El algoritmo para la unidad de control microprogramada es usualmente especificado por la descripción de un diagrama de flujo. La ventaja principal de la unidad de control microprogramada es la simplicidad de su estructura. Las salidas del controlador son organizadas en microinstrucciones y pueden ser reemplazadas fácilmente.

Las funciones realizadas por la unidad de control varían grandemente por la arquitectura interna de la CPU, pues la unidad de control realmente implementa esta arquitectura. En un procesador regular que ejecuta nativamente las instrucciones x86, la unidad de control realiza las tareas de leer ("fetch"), decodificar, manejo de la ejecución y almacenamiento de los resultados. En un procesador x86 con un núcleo RISC, la unidad de control tiene considerablemente más trabajo que hacer. Ella maneja la traducción de las instrucciones x86 a las microinstrucciones del RISC, maneja la planificación de las microinstrucciones entre las varias unidades de ejecución, y maneja la salida de estas unidades para cerciorarse de que terminen donde supuestamente deben ir. En uno de estos procesadores la unidad de control está dividida en otras unidades debido a la complejidad del trabajo que debe realizar (tales como una unidad de planificación para manejar la planificación y una unidad de retiro para ocuparse de los resultados que vienen de la tubería o "pipe"). Almacena los datos más utilizados de modo que se buscan primero en la computadora y luego en la RAM.




</doc>
<doc id="4958" url="https://es.wikipedia.org/wiki?curid=4958" title="Unidad aritmética lógica">
Unidad aritmética lógica

En computación, la unidad aritmética lógica o unidad aritmético-lógica, también conocida como ALU (siglas en inglés de "arithmetic logic unit"), es un circuito digital que realiza operaciones aritméticas (suma, resta) y operaciones lógicas (SI, Y, O, NO) entre los valores de los argumentos (uno o dos)

Por mucho, los circuitos electrónicos más complejos son los que están construidos dentro de los chips de microprocesadores modernos. Por lo tanto, estos procesadores tienen dentro de ellos un ALU muy complejo y potente. De hecho, un microprocesador moderno (y los mainframes) puede tener múltiples núcleos, cada núcleo con múltiples unidades de ejecución, cada una de ellas con múltiples ALU.

Muchos otros circuitos pueden contener en el interior una unidad aritmético lógica: unidades de procesamiento gráfico como las que están en las GPU modernas, FPU como el viejo coprocesador matemático 80387, y procesadores digitales de señales como los que se encuentran en tarjetas de sonido, lectoras de CD y los televisores de alta definición. Todos estos tienen en su interior varias ALU potentes y complejas.

El matemático John von Neumann propuso el concepto de la ALU en 1945, cuando escribió un informe sobre las fundaciones para un nuevo computador llamado EDVAC (Electronic Discrete Variable Automatic Computer) (Computador Automático Variable Discreto Electrónico). Más adelante, en 1946, trabajó con sus colegas diseñando un computador para el Princeton Institute of Advanced Studies (IAS) (Instituto de Princeton de Estudios Avanzados). El IAS computer se convirtió en el prototipo para muchos computadores posteriores. En esta propuesta, von Neumann esbozó lo que él creyó sería necesario en su máquina, incluyendo una ALU.

Von Neumann explicó que una ALU es un requisito fundamental para una computadora porque necesita efectuar operaciones matemáticas básicas: adición, sustracción, multiplicación, y división. Por lo tanto, creyó que era "razonable que una computadora debería contener los órganos especializados para estas operaciones".

Una ALU debe procesar números usando el mismo formato que el resto del circuito digital. Para los procesadores modernos, este formato casi siempre es la representación del número binario de complemento a dos. Las primeras computadoras usaron una amplia variedad de sistemas de numeración, incluyendo complemento a uno, formato signo-magnitud, e incluso verdaderos sistemas decimales, con diez tubos por dígito.

Las ALU para cada uno de estos sistemas numéricos mostraban diferentes diseños, y esto influenció la preferencia actual por el complemento a dos, debido a que ésta es la representación más simple, para el circuito electrónico de la ALU, para calcular adiciones, sustracciones, etc.

La ALU se compone básicamente de: Circuito Operacional, Registros de Entradas, Registro Acumulador y un Registro de Estados, conjunto de registros que hacen posible la realización de cada una de las operaciones.

La mayoría de las acciones de la computadora son realizadas por la ALU. La ALU toma datos de los registros del procesador. Estos datos son procesados y los resultados de esta operación se almacenan en los registros de salida de la ALU. Otros mecanismos mueven datos entre estos registros y la memoria.

Una unidad de control controla a la ALU, al ajustar los circuitos que le señala a la ALU qué operaciones realizar.

En la imagen se detalla una ALU de 2 bits con dos entradas (operandos) llamadas A y B: A[0] y B[0] corresponden al bit menos significativo y A[1] y B[1] corresponden al bit más significativo.

Cada bit de la ALU se procesa de manera idéntica, con la excepción del direccionamiento del bit del acarreo. El manejo de este bit es explicado más adelante.

Las entradas A y B van hacia las cuatro puertas de la derecha, de arriba abajo, XOR, AND, OR. Las tres primeras puertas realizan las operaciones XOR, AND, y OR sobre los datos A y B. La última puerta XOR es la puerta inicial de un sumador completo.

El paso final de las operaciones sobre cada bit es la multiplexación de los datos. La entrada OP de 3 bits, OP[0], OP[1] y OP[2] (desde la unidad de control) determina cual de las funciones se van a realizar:


Claramente se ve que las otras cuatro entradas del multiplexor están libres para otras operaciones (sustracción, multiplicación, división, NOT A, NOT B, etc). Aunque OP[2] actualmente no es usada en este montaje (a pesar de estar incluida y conectada), ésta sería usada en el momento de realizar otras operaciones además de las 4 operaciones listadas arriba.

Los datos de acarreo de entrada y acarreo de salida, llamados flags (banderas), son típicamente conectados a algún tipo de registro de estado.

La mayoría de las ALU pueden realizar las siguientes operaciones:


Un ingeniero puede diseñar una ALU para calcular cualquier operación, sin importar lo compleja que sea; el problema es que cuanto más compleja sea la operación, tanto más costosa será la ALU, más espacio usará en el procesador, y más energía disipará, etc.

Por lo tanto, los ingenieros siempre calculan un compromiso, para proporcionar al procesador (u otros circuitos) una ALU suficientemente potente para calcular rápido, pero no de una complejidad de tal calibre que haga una ALU económicamente prohibitiva.
"Imagina que necesitas calcular, digamos, la raíz cuadrada de un número; el ingeniero digital examinará las opciones siguientes para implementar esta operación:"


Las opciones superiores van de la más rápida y más costosa a la más lenta y económica. Por lo tanto, mientras que incluso la computadora más simple puede calcular la fórmula más complicada, las computadoras más simples generalmente tomarán un tiempo largo porque varios de los pasos para calcular la fórmula implicarán las opciones #3, #4 y #5 de arriba.

Los procesadores complejos como el Pentium IV y el AMD Athlon 64 implementan la opción #1 para las operaciones más complejas y la más lenta #2 para las operaciones extremadamente complejas. Eso es posible por la capacidad de construir ALU muy complejas en estos procesadores.

Las entradas a la ALU son los datos en los que se harán las operaciones (llamados operandos) y un código desde la unidad de control indicando qué operación realizar. Su salida es el resultado del cómputo de la operación.

En muchos diseños la ALU también toma o genera como entradas o salidas un conjunto de códigos de condición desde o hacia un registro de estado. Estos códigos son usados para indicar casos como acarreo entrante o saliente, overflow, división por cero, etc.

Una unidad de coma flotante, "Floating Point Unit" (FPU), también realiza operaciones aritméticas entre dos valores, pero lo hace para números en representación de coma flotante, que es mucho más complicada que la representación de complemento a dos usada comúnmente en una ALU. Para hacer estos cálculos, una FPU tiene incorporados varios circuitos complejos, incluyendo algunas ALU internas.

Generalmente los ingenieros llaman ALU al circuito que realiza operaciones aritméticas en formatos de número entero (como complemento a dos y BCD), mientras que los circuitos que calculan en formatos más complejos como coma flotante, números complejos, etc., reciben generalmente un nombre más específico, como FPU.





</doc>
<doc id="4959" url="https://es.wikipedia.org/wiki?curid=4959" title="Memoria (informática)">
Memoria (informática)

En informática, la memoria es el dispositivo que retiene, memoriza o almacena datos informáticos durante algún periodo de tiempo.La memoria proporciona una de las principales funciones de la computación moderna: el almacenamiento de información y conocimiento. Es uno de los componentes fundamentales de la computadora, que interconectada a la unidad central de procesamiento ("CPU", por las siglas en inglés de "Central Processing Unit") y los dispositivos de entrada/salida, implementan lo fundamental del modelo de computadora de la arquitectura de Von Neumann.

En la actualidad, «memoria» suele referirse a una forma de almacenamiento de estado sólido, conocida como memoria RAM (memoria de acceso aleatorio; RAM por sus siglas en inglés, de "random access memory"), y otras veces se refiere a otras formas de almacenamiento rápido, pero temporal. De forma similar, se refiere a formas de almacenamiento masivo, como discos ópticos, y tipos de almacenamiento magnético, como discos duros y otros tipos de almacenamiento, más lentos que las memorias RAM, pero de naturaleza más permanente. Estas distinciones contemporáneas son de ayuda, porque son fundamentales para la arquitectura de computadores en general. 

Además, se refleja una diferencia técnica importante y significativa entre «memoria» y «dispositivos de almacenamiento masivo», que se ha ido diluyendo por el uso histórico de los términos «almacenamiento primario» (a veces «almacenamiento principal»), para memorias de acceso aleatorio, y «almacenamiento secundario», para dispositivos de almacenamiento masivo. Esto se explica en las siguientes secciones, en las que el término tradicional «almacenamiento» se usa como subtítulo, por conveniencia.

Los componentes fundamentales de las computadoras de propósito general son la CPU, el espacio de almacenamiento y los dispositivos de entrada/salida. La habilidad para almacenar las instrucciones que forman un programa de computadora y la información que manipulan las instrucciones es lo que hace versátiles a las computadoras diseñadas según la arquitectura de programas almacenados. 

Una computadora digital representa toda la información usando el sistema binario. Texto, números, imágenes, sonido y casi cualquier otra forma de información puede ser transformada en una sucesión de bits, o dígitos binarios, cada uno de los cuales tiene un valor de 1 o 0. La unidad de almacenamiento más común es el byte, igual a 8 bits. Una determinada información puede ser manipulada por cualquier computadora cuyo espacio de almacenamiento sea suficientemente grande como para que quepa el dato correspondiente o la representación binaria de la información. Por ejemplo, una computadora con un espacio de almacenamiento de ocho millones de bits, o un megabyte, puede ser usada para editar una novela pequeña.

Se han inventado varias formas de almacenamiento basadas en diversos fenómenos naturales. No existe ningún medio de almacenamiento de uso práctico universal y todas las formas de almacenamiento tienen sus desventajas. Por tanto, un sistema informático contiene varios tipos de almacenamiento, cada uno con su propósito individual.

La memoria primaria, está directamente conectada a la CPU del ordenador. Debe estar presente para que la CPU efectúe cualquier función. El almacenamiento primario consta de la memoria primaria del sistema; contiene los programas en ejecución y los datos con que operan. Se puede transferir información muy rápidamente (típicamente en menos de 100 ciclos de reloj) entre un registro del microprocesador y localizaciones del almacenamiento principal. En las computadoras modernas se usan memorias de acceso aleatorio basadas en electrónica del estado sólido, que está directamente conectada a la CPU a través de buses de direcciones, datos y control.

El almacenamiento lleva por principal requisito que cualquiera de sus localidades debe ser "directamente direccionable", esto es, todo dato contenido en memoria debe poder encontrarse basándose en su dirección. Es por esto que los registros del procesador no pueden considerarse almacenamiento primario. Las referencias a estos se efectúan "por nombre", de forma directa, y no por dirección. Los registros representan el "estado actual" del cómputo y los datos utilizados inmediatamente, pero no pueden almacenar un programa (sólo apuntar al lugar de ejecución actual).

La gran diferencia de velocidad entre el procesador y la memoria primaria dio origen a la memoria caché. Esta es una memoria de muy alta velocidad, típicamente entre 10 y 100 veces más que la memoria primaria, y se emplea para mejorar la eficiencia o rendimiento del CPU. Parte de la información de la memoria principal se duplica en la memoria caché. Comparada con los registros, la caché es ligeramente más lenta, pero de mayor capacidad. Sin embargo, es más rápida, aunque de mucha menor capacidad que la memoria principal. 

Algunos autores presentan a la memoria caché como una jerarquía aparte, sin embargo, al no ser memoria directamente direccionable (guarda estrictamente copias de la información disponible en la memoria principal), es común presentarla como parte funcional del almacenamiento primario.

La memoria secundaria requiere que la computadora use sus canales de entrada/salida para acceder a la información y se utiliza para almacenamiento a largo plazo de información persistente. Sin embargo, la mayoría de los sistemas operativos usan los dispositivos de almacenamiento secundario como área de intercambio para incrementar artificialmente la cantidad aparente de memoria principal en la computadora (a esta utilización del almacenamiento secundario se le denomina memoria virtual). La memoria secundaria también se llama de «almacenamiento masivo». Un disco duro es un ejemplo de almacenamiento secundario. 

Habitualmente, la memoria secundaria o de almacenamiento masivo tiene mayor capacidad que la memoria primaria, pero es mucho más lenta. En las computadoras modernas, los discos duros suelen usarse como dispositivos de almacenamiento masivo. El tiempo necesario para acceder a un byte de información dado almacenado en un disco duro de platos magnéticos es de unas milésimas de segundo (milisegundos). En cambio, el tiempo para acceder al mismo tipo de información en una memoria de acceso aleatorio (RAM) se mide en mil-millonésimas de segundo (nanosegundos).

Esto ilustra cuan significativa es la diferencia entre la velocidad de las memorias de estado sólido y la velocidad de los dispositivos rotantes de almacenamiento magnético u óptico: los discos duros son del orden de un millón de veces más lentos que la memoria (primaria). Los dispositivos rotantes de almacenamiento óptico (unidades de CD y DVD) son incluso más lentos que los discos duros, aunque es probable que su velocidad de acceso mejore con los avances tecnológicos.

Por lo tanto, el uso de la memoria virtual, que es cerca de un millón de veces más lenta que memoria “verdadera”, ralentiza apreciablemente el funcionamiento de cualquier computadora. Muchos sistemas operativos implementan la memoria virtual usando términos como memoria virtual o «fichero de caché». La principal ventaja histórica de la memoria virtual es el precio; la memoria virtual resultaba mucho más barata que la memoria real. Esa ventaja es menos relevante hoy en día. Aun así, muchos sistemas operativos siguen implementándola, a pesar de provocar un funcionamiento significativamente más lento.

La memoria terciaria es un sistema en el que un robot industrial brazo robótico, montará, conectará o desmontará (desconectará) un medio de almacenamiento masivo fuera de línea (véase el siguiente punto) según lo solicite el sistema operativo de la computadora. La memoria terciaria se usa en el área del almacenamiento industrial, la computación científica en grandes sistemas informáticos y en redes empresariales. Este tipo de memoria es algo que los usuarios de computadoras personales normales nunca ven de primera mano.

El almacenamiento fuera de línea ("off-line") es un sistema donde el medio de almacenamiento puede ser extraído fácilmente del dispositivo de almacenamiento. Estos medios de almacenamiento suelen usarse para transporte y archivo de datos. En computadoras modernas son de uso habitual para este propósito los disquetes, discos ópticos y las memorias flash, incluyendo las unidades USB. También hay discos duros USB que se pueden conectar rápidamente. Los dispositivos de almacenamiento fuera de línea usados en el pasado son cintas magnéticas en muchos tamaños y formatos diferentes, y las baterías extraíbles de discos Winchester.

El almacenamiento de red es cualquier tipo de almacenamiento de computadora que incluye el hecho de acceder a la información a través de una red informática. Discutiblemente, el almacenamiento de red permite centralizar el “control de información” en una organización y reducir la duplicidad de la información. El almacenamiento en red incluye:

La división entre primario, secundario, terciario y fuera de línea, se basa en la jerarquía de memoria o distancia desde la CPU. Hay otras formas de caracterizar a los distintos tipos de memoria.


Dependiendo de la habilidad para acceder a información contigua o no, se puede clasificar en:



Memorias de mayor capacidad son el resultado de la rápida evolución en tecnología de materiales semiconductores. Los primeros programas de ajedrez funcionaban en máquinas que utilizaban memorias de base magnética. A inicios de 1970 aparecen las memorias realizadas por semiconductores, como las utilizadas en la serie de computadoras IBM 370.

La velocidad de los computadores se incrementó, multiplicada por 100.000 aproximadamente y la capacidad de memoria creció en una proporción similar. Este hecho es particularmente importante para los programas que utilizan tablas de transposición: a medida que aumenta la velocidad de la computadora se necesitan memorias de capacidad proporcionalmente mayor para mantener la cantidad extra de posiciones que el programa está buscando. 

Se espera que la capacidad de procesadores siga aumentando en los próximos años; no es un abuso pensar que la capacidad de memoria continuará creciendo de manera impresionante. Memorias de mayor capacidad podrán ser utilizadas por programas con tablas de Hash de mayor envergadura, las cuales mantendrán la información en forma permanente. 




La memoria de semiconductor usa circuitos integrados basados en semiconductores para almacenar información. Un chip de memoria de semiconductor puede contener millones de minúsculos transistores o condensadores. Existen memorias de semiconductor de ambos tipos: "volátiles" y "no volátiles". En las computadoras modernas, la memoria principal consiste casi exclusivamente en memoria de semiconductor volátil y dinámica, también conocida como memoria dinámica de acceso aleatorio o más comúnmente RAM, su acrónimo inglés. Con el cambio de siglo, ha habido un crecimiento constante en el uso de un nuevo tipo de memoria de semiconductor no volátil llamado memoria flash. Dicho crecimiento se ha dado, principalmente en el campo de las memorias fuera de línea en computadoras domésticas. Las memorias de semiconductor no volátiles se están usando también como memorias secundarias en varios dispositivos de electrónica avanzada y computadoras especializadas y no especializadas.

Las memorias magnéticas usan diferentes patrones de magnetización sobre una superficie cubierta con una capa magnetizada para almacenar información. Las memorias magnéticas son "no volátiles". Se llega a la información usando uno o más cabezales de lectura/escritura. Como el cabezal de lectura/escritura solo cubre una parte de la superficie, el almacenamiento magnético es de "acceso secuencial" y debe buscar, dar vueltas o las dos cosas. En ‘computadoras modernas’, la superficie magnética es de alguno de estos tipos:


En las ‘primeras computadoras’, el almacenamiento magnético se usaba también como memoria principal en forma de memoria de tambor, memoria de núcleo, memoria en hilera de núcleo, memoria película delgada, memoria de Twistor o memoria de burbuja. Además, a diferencia de hoy, las cintas magnéticas se solían usar como memoria secundaria.

Las memorias en disco óptico almacenan información usando agujeros minúsculos grabados con un láser en la superficie de un disco circular. La información se lee iluminando la superficie con un diodo láser y observando la reflexión. Los discos ópticos son "no volátil" y de "acceso secuencial". Los siguientes formatos son de uso común:


Se han propuesto los siguientes formatos:


Los discos magneto-ópticos son discos de memoria óptica donde la información se almacena en el estado magnético de una superficie ferromagnética. La información se lee ópticamente y se escribe combinando métodos magnéticos y ópticos. Las memorias de discos magneto ópticos son de tipo "no volátiles", de "acceso secuencial", de escritura lenta y lectura rápida. Se usa como memoria terciaria y fuera de línea.

Las tarjetas perforadas fueron utilizados por primera vez por Basile Bouchon para el control de telares textiles en Francia. En 1801 el sistema de Bouchon fue perfeccionado por Joseph Marie Jacquard, quien desarrolló un telar automático, conocido como telar de Jacquard. Herman Hollerith desarrolló la tecnología de procesamiento de datos de tarjetas perforadas para el censo de Estados Unidos de 1890 y posteriormente fundó la "Tabulating Machine Company", una de las precursoras de IBM. IBM desarrolló la tecnología de la tarjeta perforada como una potente herramienta para el procesamiento de datos empresariales y produjo una línea extensiva de "máquinas de registro" que utilizaban papel perforado para el almacenamiento de datos y su procesado automático. 

En el año 1950, las tarjetas IBM y las unidades máquinas de registro IBM se habían vuelto indispensables en la industria y el gobierno estadounidense. Durante los años 1960, las tarjetas perforadas fueron gradualmente reemplazadas por las cintas magnéticas, aunque su uso fue muy común hasta mediados de los años 1970 con la aparición de los discos magnéticos. La información se grababa en las tarjetas perforando agujeros en el papel o la tarjeta. La lectura se realizaba por sensores eléctricos (más tarde ópticos) donde una localización particular podía estar agujereada o no. 

Para almacenar información, los tubos Williams usaban un tubo de rayos catódicos y los tubos Selectrón usaban un gran tubo de vacío. Estos dispositivos de memoria primaria tuvieron una corta vida en el mercado ya que el tubo de Williams no era fiable y el tubo de Selectrón era caro.

La memoria de línea de retardo usaba ondas sonoras en una sustancia como podía ser el Mercurio para guardar información. La memoria de línea de retardo era una memoria "dinámica volátil", "ciclo secuencial" de lectura/escritura. Se usaba como memoria principal.

La memoria de cambio de fase usa las fases de un material de cambio de fase para almacenar información. Dicha información se lee observando la resistencia eléctrica variable del material. La memoria de cambio de fase sería una memoria de lectura/escritura "no volátil", de "acceso aleatorio" podría ser usada como memoria primaria, secundaria y fuera de línea.
La memoria holográfica almacena ópticamente la información dentro de cristales o fotopolímeros. Las memorias holográficas pueden utilizar todo el volumen del medio de almacenamiento, a diferencia de las memorias de discos ópticos, que están limitadas a un pequeño número de superficies en capas. La memoria holográfica podría ser "no volátil", de "acceso secuencial" y tanto de escritura única como de lectura/escritura. Puede ser usada tanto como memoria secundaria como fuera de línea. 

La memoria molecular almacena la información en polímeros que pueden almacenar puntas de carga eléctrica. La memoria molecular puede ser especialmente interesante como memoria principal.

Recientemente se ha propuesto utilizar el spin de un electrón como memoria. Se ha demostrado que es posible desarrollar un circuito electrónico que lea el spin del electrón y lo convierta en una señal eléctrica.



</doc>
<doc id="4960" url="https://es.wikipedia.org/wiki?curid=4960" title="Microcontrolador">
Microcontrolador

Un microcontrolador (abreviado µC, UC o MCU) es un circuito integrado programable, capaz de ejecutar las órdenes grabadas en su memoria. Está compuesto de varios bloques funcionales que cumplen una tarea específica. Un microcontrolador incluye en su interior las tres principales unidades funcionales de una computadora: unidad central de procesamiento, memoria y periféricos de entrada/salida.

Algunos microcontroladores pueden utilizar palabras de cuatro bits y funcionan a velocidad de reloj con frecuencias tan bajas como 4 kHz, con un consumo de baja potencia (mW o microwatts). Por lo general, tendrá la capacidad de mantenerse a la espera de un evento como pulsar un botón o de otra interrupción; así, el consumo de energía durante el estado de reposo (reloj de la CPU y los periféricos de la mayoría) puede ser solo de nanowatts, lo que hace que muchos de ellos sean muy adecuados para aplicaciones con batería de larga duración. Otros microcontroladores pueden servir para roles de rendimiento crítico, donde sea necesario actuar más como un procesador digital de señal (DSP), con velocidades de reloj y consumo de energía más altos.

Cuando es fabricado el microcontrolador, no contiene datos en la memoria ROM. Para que pueda controlar algún proceso es necesario generar o crear y luego grabar en la EEPROM o equivalente del microcontrolador algún programa, el cual puede ser escrito en lenguaje ensamblador u otro lenguaje para microcontroladores; sin embargo, para que el programa pueda ser grabado en la memoria del microcontrolador, debe ser codificado en sistema numérico hexadecimal que es finalmente el sistema que hace trabajar al microcontrolador cuando este es alimentado con el voltaje adecuado y asociado a dispositivos analógicos y discretos para su funcionamiento.

El primer microprocesador fue el Intel 4004 de 4 bits, lanzado en 1971, seguido por el Intel 8008 y otros más capaces. Sin embargo, ambos procesadores requieren circuitos adicionales para implementar un sistema de trabajo, elevando el costo del sistema total.

El Instituto Smithsoniano dice que los ingenieros de Texas Instruments Gary Boone y Michael Cochran lograron crear el primer microcontrolador, TMS 1000, en 1971; fue comercializado en 1974. Combina memoria ROM, memoria RAM, microprocesador y reloj en un chip y estaba destinada a los sistemas embebidos.

Debido en parte a la existencia del TMS 1000, Intel desarrolló un sistema de ordenador en un chip optimizado para aplicaciones de control, el Intel 8048, que comenzó a comercializarse en 1977. Combina memoria RAM y ROM en el mismo chip y puede encontrarse en más de mil millones de teclados de compatible IBM PC, y otras numerosas aplicaciones. El en ese momento presidente de Intel, Luke J. Valenter, declaró que el microcontrolador es uno de los productos más exitosos en la historia de la compañía, y amplió el presupuesto de la división en más del 25%.

La mayoría de los microcontroladores en aquel momento tenían dos variantes. Unos tenían una memoria EPROM reprogramable, significativamente más caros que la variante PROM que era solo una vez programable. Para borrar la EPROM necesita exponer a la luz ultravioleta la tapa de cuarzo transparente. Los chips con todo opaco representaban un coste menor.

En 1993, el lanzamiento de la EEPROM en los microcontroladores (comenzando con el Microchip PIC16x84) permite borrarla eléctrica y rápidamente sin necesidad de un paquete costoso como se requiere en EPROM, lo que permite tanto la creación rápida de prototipos y la programación en el sistema. El mismo año, Atmel lanza el primer microcontrolador que utiliza memoria flash. Otras compañías rápidamente siguieron el ejemplo, con los dos tipos de memoria.

El costo se ha desplomado en el tiempo, con el más barato microcontrolador de 8 bits disponible por menos de 0,25 dólares para miles de unidades en 2009, y algunos microcontroladores de 32 bits a 1 dólar por cantidades similares. En la actualidad los microcontroladores son baratos y fácilmente disponibles para los aficionados, con grandes comunidades en línea para ciertos procesadores.

En el futuro, la MRAM podría ser utilizada en microcontroladores, ya que tiene resistencia infinita y el coste de su oblea semiconductora es relativamente bajo.

Los microcontroladores están diseñados para reducir el costo económico y el consumo de energía de un sistema en particular. Por eso el tamaño de la unidad central de procesamiento, la cantidad de memoria y los periféricos incluidos dependerán de la aplicación. El control de un electrodoméstico sencillo como una batidora utilizará un procesador muy pequeño (4 u 8 bits) porque sustituirá a un autómata finito. En cambio, un reproductor de música o vídeo digital (MP3 o MP4) requerirá de un procesador de 32 bits o de 64 bits y de uno o más códecs de señal digital (audio o vídeo). El control de un sistema de frenos ABS (Antilock Brake System) se basa normalmente en un microcontrolador de 16 bits, al igual que el sistema de control electrónico del motor en un automóvil.

Los microcontroladores representan la inmensa mayoría de los chips de computadoras vendidos, sobre un 50% son controladores "simples" y el restante corresponde a DSP más especializados. Mientras se pueden tener uno o dos microprocesadores de propósito general en casa (Ud. está usando uno para esto), usted tiene distribuidos seguramente entre los electrodomésticos de su hogar una o dos docenas de microcontroladores. Pueden encontrarse en casi cualquier dispositivo electrónico como automóviles, lavadoras, hornos microondas, teléfonos, etc.

Un microcontrolador difiere de una unidad central de procesamiento normal, debido a que es más fácil convertirla en una computadora en funcionamiento, con un mínimo de circuitos integrados externos de apoyo. La idea es que el circuito integrado se coloque en el dispositivo, enganchado a la fuente de energía y de información que necesite, y eso es todo. Un microprocesador tradicional no le permitirá hacer esto, ya que espera que todas estas tareas sean manejadas por otros chips. Hay que agregarle los módulos de entrada y salida (puertos) y la memoria para almacenamiento de información.

Un microcontrolador típico tendrá un generador de reloj integrado y una pequeña cantidad de memoria de acceso aleatorio o ROM/EPROM/EEPROM/flash, con lo que para hacerlo funcionar todo lo que se necesita son unos pocos programas de control y un cristal de sincronización. Los microcontroladores disponen generalmente también de una gran variedad de dispositivos de entrada/salida, como convertidor analógico digital, temporizadores, UARTs y buses de interfaz serie especializados, como IC y CAN. Frecuentemente, estos dispositivos integrados pueden ser controlados por instrucciones de procesadores especializados. Los modernos microcontroladores frecuentemente incluyen un lenguaje de programación integrado, como el lenguaje de programación BASIC que se utiliza bastante con este propósito.

Los microcontroladores negocian la velocidad y la flexibilidad para facilitar su uso. Debido a que se utiliza bastante sitio en el chip para incluir funcionalidad, como los dispositivos de entrada/salida o la memoria que incluye el microcontrolador, se ha de prescindir de cualquier otra circuitería.

Básicamente existen dos arquitecturas de computadoras, y por supuesto, están presentes en el mundo de los microcontroladores: Von Neumann y Harvard. Ambas se diferencian en la forma de conexión de la memoria al procesador y en los buses que cada una necesita.

La arquitectura Von Neumann utiliza el mismo dispositivo de almacenamiento tanto para las instrucciones como para los datos, siendo la que se utiliza en un ordenador personal porque permite ahorrar una buena cantidad de líneas de E/S, que son bastante costosas, sobre todo para aquellos sistemas donde el procesador se monta en algún tipo de zócalo alojado en una placa madre. También esta organización les ahorra a los diseñadores de placas madre una buena cantidad de problemas y reduce el costo de este tipo de sistemas.

En un ordenador personal, cuando se carga un programa en memoria, a este se le asigna un espacio de direcciones de la memoria que se divide en segmentos, de los cuales típicamente tendremos los siguientes: código (programa), datos y pila. Es por ello que podemos hablar de la memoria como un todo, aunque existan distintos dispositivos físicos en el sistema (disco duro, memoria RAM, memoria flash, unidad de disco óptico…).

En el caso de los microcontroladores, existen dos tipos de memoria bien definidas: memoria de datos (típicamente algún tipo de SRAM) y memoria de programas (ROM, PROM, EEPROM, flash u de otro tipo no volátil). En este caso la organización es distinta a las del ordenador personal, porque hay circuitos distintos para cada memoria y normalmente no se utilizan los registros de segmentos, sino que la memoria está segregada y el acceso a cada tipo de memoria depende de las instrucciones del procesador.

A pesar de que en los sistemas integrados con arquitectura Von Neumann la memoria esté segregada, y existan diferencias con respecto a la definición tradicional de esta arquitectura; los buses para acceder a ambos tipos de memoria son los mismos, del procesador solamente salen el bus de datos, el de direcciones, y el de control. Como conclusión, la arquitectura no ha sido alterada, porque la forma en que se conecta la memoria al procesador sigue el mismo principio definido en la arquitectura básica.

Algunas familias de microcontroladores como la Intel 8051 y la Z80 implementan este tipo de arquitectura, fundamentalmente porque era la utilizada cuando aparecieron los primeros microcontroladores.

La otra variante es la arquitectura Harvard, y por excelencia la utilizada en supercomputadoras, en los microcontroladores, y sistemas integrados en general. En este caso, además de la memoria, el procesador tiene los buses segregados, de modo que cada tipo de memoria tiene un bus de datos, uno de direcciones y uno de control.

La ventaja fundamental de esta arquitectura es que permite adecuar el tamaño de los buses a las características de cada tipo de memoria; además, el procesador puede acceder a cada una de ellas de forma simultánea, lo que se traduce en un aumento significativo de la velocidad de procesamiento. Típicamente los sistemas con esta arquitectura pueden ser dos veces más rápidos que sistemas similares con arquitectura Von Neumann.

La desventaja está en que consume muchas líneas de E/S del procesador; por lo que en sistemas donde el procesador está ubicado en su propio encapsulado, solo se utiliza en supercomputadoras. Sin embargo, en los microcontroladores y otros sistemas integrados, donde usualmente la memoria de datos y programas comparten el mismo encapsulado que el procesador, este inconveniente deja de ser un problema serio y es por ello que encontramos la arquitectura Harvard en la mayoría de los microcontroladores.

Por eso es importante recordar que un microcontrolador se puede configurar de diferentes maneras, siempre y cuando se respete el tamaño de memoria que este requiera para su correcto funcionamiento.

En los años 1970, la electrónica digital no estaba suficientemente desarrollada, pero dentro de la electrónica ya era una especialidad consagrada. En aquel entonces las computadoras se diseñaban para que realizaran algunas operaciones muy simples, y si se quería que estas máquinas pudiesen hacer cosas diferentes, era necesario realizar cambios bastante significativos al hardware.

A principios de la década de 1970, una empresa japonesa le encargó a una joven compañía norteamericana que desarrollara un conjunto de circuitos para producir una calculadora de bajo costo. Intel se dedicó de lleno a la tarea y entre los circuitos encargados desarrolló uno muy especial, algo no creado hasta la fecha: el primer microprocesador integrado.

El Intel 4004 salió al mercado en 1971, es una máquina digital sincrónica compleja, como cualquier otro circuito lógico secuencial sincrónico. Sin embargo, la ventaja de este componente está en que aloja internamente un conjunto de circuitos digitales que pueden hacer operaciones corrientes para el cálculo y procesamiento de datos, pero desde una óptica diferente: sus entradas son una serie de códigos bien definidos, que permiten hacer operaciones de carácter específico cuyo resultado está determinado por el tipo de operación y los operandos involucrados.

Visto así, no hay nada de especial en un microprocesador; la maravilla está en que la combinación adecuada de los códigos de entrada, su ejecución secuencial, el poder saltar hacia atrás o adelante en la secuencia de códigos sobre la base de decisiones lógicas u órdenes específicas, permite que la máquina realice gran cantidad de operaciones complejas, no contempladas en los simples códigos básicos.

Hoy estamos acostumbrados a los sistemas con microprocesadores, pero en el "lejano" 1971 esta era una forma de pensar un poco diferente y hasta escandalosa, a tal punto que Busicom, la empresa que encargó los chips a Intel, no se mostró interesada en el invento, por lo que Intel lo comercializó para otros que mostraron interés; el resto es historia: una revolución sin precedentes en el avance tecnológico de la humanidad.

Es lógico pensar que el invento del microprocesador integrado no fue una revelación divina para sus creadores, sino que se sustentó en los avances, existentes hasta el momento, en el campo de la electrónica digital y las teorías sobre computación. Pero sin lugar a dudas fue la gota que colmó la copa de la revolución científico-técnica, porque permitió desarrollar aplicaciones impensadas o acelerar algunas ya encaminadas.

Ahora comenzaremos a ver cómo es que está hecho un procesador, no será una explicación demasiado detallada porque desde su invención este ha tenido importantes revoluciones propias, pero hay aspectos básicos que no han cambiado y que constituyen la base de cualquier microprocesador.
En la Figura 'Esquema de un microcontrolador' podemos ver la estructura típica de un microprocesador, con sus componentes fundamentales, claro está que ningún procesador real se ajusta exactamente a esta estructura, pero aun así nos permite conocer cada uno de sus elementos básicos y sus interrelaciones.

Son un espacio de memoria muy reducido pero necesario para cualquier microprocesador, de aquí se toman los datos para varias operaciones que debe realizar el resto de los circuitos del procesador. Los registros sirven para almacenar los resultados de la ejecución de instrucciones, cargar datos desde la memoria externa o almacenarlos en ella.

Aunque la importancia de los registros parezca trivial, no lo es en absoluto. De hecho una parte de los registros, la destinada a los datos, es la que determina uno de los parámetros más importantes de cualquier microprocesador. Cuando escuchamos que un procesador es de 4, 8, 16, 32 o 64 bits, nos estamos refiriendo a procesadores que realizan sus operaciones con registros de datos de ese tamaño, y por supuesto, esto determina muchas de las potencialidades de estas máquinas.

Mientras mayor sea el número de bits de los registros de datos del procesador, mayores serán sus prestaciones, en cuanto a poder de cómputo y velocidad de ejecución, ya que este parámetro determina la potencia que se puede incorporar al resto de los componentes del sistema, por ejemplo, no tiene sentido tener una ALU de 16 bits en un procesador de 8 bits.

Por otro lado un procesador de 16 bits, puede que haga una suma de 16 bits en un solo ciclo de máquina, mientras que uno de 8 bits deberá ejecutar varias instrucciones antes de tener el resultado, aun cuando ambos procesadores tengan la misma velocidad de ejecución para sus instrucciones. El procesador de 16 bits será más rápido porque puede hacer el mismo tipo de tareas que uno de 8 bits, en menos tiempo.

Esta unidad es de las más importantes en el procesador, en ella recae la lógica necesaria para la decodificación y ejecución de las instrucciones, el control de los registros, la ALU, los buses y cuanta cosa más se quiera meter en el procesador.

La unidad de control es uno de los elementos fundamentales que determinan las prestaciones del procesador, ya que su tipo y estructura determina parámetros tales como el tipo de conjunto de instrucciones, velocidad de ejecución, tiempo del ciclo de máquina, tipo de buses que puede tener el sistema, manejo de interrupciones y un buen número de cosas más que en cualquier procesador van a parar a este bloque.

Por supuesto, las unidades de control son el elemento más complejo de un procesador y normalmente están divididas en unidades más pequeñas trabajando de conjunto. La unidad de control agrupa componentes tales como la unidad de decodificación, unidad de ejecución, controladores de memoria caché, controladores de buses, controlador de interrupciones, pipelines, entre otros elementos, dependiendo siempre del tipo de procesador.

Como los procesadores son circuitos que hacen básicamente operaciones lógicas y matemáticas, se le dedica a este proceso una unidad completa, con cierta independencia. Aquí es donde se realizan las sumas, restas, y operaciones lógicas típicas del álgebra de Boole.

Actualmente este tipo de unidades ha evolucionado mucho y los procesadores más modernos tienen varias ALU, especializadas en la realización de operaciones complejas como las operaciones en coma flotante. De hecho en muchos casos le han cambiado su nombre por el de «coprocesador matemático», aunque este es un término que surgió para dar nombre a un tipo especial de procesador que se conecta directamente al procesador más tradicional.

Su impacto en las prestaciones del procesador es también importante porque, dependiendo de su potencia, tareas más o menos complejas, pueden hacerse en tiempos muy cortos, como por ejemplo, los cálculos en coma flotante.

Son el medio de comunicación que utilizan los diferentes componentes del procesador para intercambiar información entre sí, eventualmente los buses o una parte de ellos estarán reflejados en los pines del encapsulado del procesador.

En el caso de los microcontroladores, no es común que los buses estén reflejados en el encapsulado del circuito, ya que estos se destinan básicamente a las E/S de propósito general y periféricos del sistema.

Existen tres tipos de buses:


Aunque no aparezca en el esquema, no podíamos dejar al conjunto o repertorio de instrucciones fuera de la explicación, porque este elemento determina lo que puede hacer el procesador.

Define las operaciones básicas que puede realizar el procesador, que conjugadas y organizadas forman lo que conocemos como software. El conjunto de instrucciones vienen siendo como las letras del alfabeto, el elemento básico del lenguaje, que organizadas adecuadamente permiten escribir palabras, oraciones y cuanto programa se le ocurra.

Existen dos tipos básicos de repertorios de instrucciones, que determinan la arquitectura del procesador: CISC y RISC.

CISC, del inglés "Complex Instruction Set Computing", Computadora de conjunto de instrucciones complejo. Los microprocesadores CISC tienen un conjunto de instrucciones que se caracteriza por ser muy amplio y que permiten realizar operaciones complejas entre operandos situados en la memoria o en los registros internos. Este tipo de repertorio dificulta el paralelismo entre instrucciones, por lo que en la actualidad, la mayoría de los sistemas CISC de alto rendimiento convierten las instrucciones complejas en varias instrucciones simples del tipo RISC, llamadas generalmente microinstrucciones.

Dentro de los microcontroladores CISC podemos encontrar a la popular familia Intel 8051 y la Z80, aunque actualmente existen versiones CISC-RISC de estos microcontroladores, que pretenden aprovechar las ventajas de los procesadores RISC a la vez que se mantiene la compatibilidad hacia atrás con las instrucciones de tipo CISC.

RISC, del inglés "Reduced Instruction Set Computer", Computadora con Conjunto de Instrucciones Reducido. Se centra en la obtención de procesadores con las siguientes características fundamentales:


Una de las características más destacables de este tipo de procesadores es que posibilitan el paralelismo en la ejecución, y reducen los accesos a memoria. Es por eso que los procesadores más modernos, tradicionalmente basados en arquitecturas CISC, implementan mecanismos de traducción de instrucciones CISC a RISC, para aprovechar las ventajas de este tipo de procesadores.

Los procesadores de los microcontroladores PIC son de tipo RISC.

Anteriormente se ha visto que la memoria en los microcontroladores debe estar ubicada dentro del mismo encapsulado, esto es así la mayoría de las veces, porque la idea fundamental es mantener el grueso de los circuitos del sistema dentro de un solo integrado.

En los microcontroladores la memoria no es abundante, aquí no encontrará Gigabytes de memoria como en las computadoras personales. Típicamente la memoria de programas no excederá de 16 K-localizaciones de memoria no volátil (flash o eprom) para contener los programas.

La memoria RAM está destinada al almacenamiento de información temporal que será utilizada por el procesador para realizar cálculos u otro tipo de operaciones lógicas. En el espacio de direcciones de memoria RAM se ubican además los registros de trabajo del procesador y los de configuración y trabajo de los distintos periféricos del microcontrolador. Es por ello que en la mayoría de los casos, aunque se tenga un espacio de direcciones de un tamaño determinado, la cantidad de memoria RAM de que dispone el programador para almacenar sus datos es menor que la que puede direccionar el procesador.

El tipo de memoria utilizada en las memorias RAM de los microcontroladores es SRAM, lo que evita tener que implementar sistemas de refrescamiento como en el caso de las computadoras personales, que utilizan gran cantidad de memoria, típicamente alguna tecnología DRAM. A pesar de que la memoria SRAM es más costosa que la DRAM, es el tipo adecuado para los microcontroladores porque éstos poseen pequeñas cantidades de memoria RAM.

En el caso de la memoria de programas se utilizan diferentes tecnologías, y el uso de una u otra depende de las características de la aplicación a desarrollar, a continuación se describen las cinco tecnologías existentes, que mayor utilización tienen o han tenido:


Lo más habitual es encontrar que la memoria de programas y datos está ubicada toda dentro del microcontrolador, de hecho, actualmente son pocos los microcontroladores que permiten conectar memoria de programas en el exterior del encapsulado. Las razones para estas “limitaciones” están dadas porque el objetivo fundamental es obtener la mayor integración posible y conectar memorias externas consume líneas de E/S que son uno de los recursos más preciados de los microcontroladores.

A pesar de lo anterior existen familias como la Intel 8051 cuyos microcontroladores tienen la capacidad de ser expandidos en una variada gama de configuraciones para el uso de memoria de programas externa. En el caso de los PIC, estas posibilidades están limitadas solo a algunos microcontroladores de la gama alta, la Figura 5 muestra algunas de las configuraciones para memoria de programa que podemos encontrar en los microcontroladores. La configuración (a) es la típica y podemos encontrarla casi en el 100% de los microcontroladores. La configuración (b) es poco frecuente y generalmente se logra configurando al microcontrolador para sacrificar la memoria de programas interna, sin embargo el Intel 8031 es un microcontrolador sin memoria de programas interna. La configuración (c) es la que se encuentra habitualmente en los microcontroladores que tienen posibilidades de expandir su memoria de programas como algunos PIC de gama alta.

Cuando se requiere aumentar la cantidad de memoria de datos, lo más frecuente es colocar dispositivos de memoria externa en forma de periféricos, de esta forma se pueden utilizar memorias RAM, FLASH o incluso discos duros como los de los ordenadores personales, mientras que para los cálculos y demás operaciones que requieran almacenamiento temporal de datos se utiliza la memoria RAM interna del microcontrolador. Esta forma de expandir la memoria de datos está determinada, en la mayoría de los casos, por el tipo de repertorio de instrucciones del procesador y porque permite un elevado número de configuraciones distintas, además del consiguiente ahorro de líneas de E/S que se logra con el uso de memorias con buses de comunicación serie.

Las interrupciones son esencialmente llamadas a subrutina generadas por los dispositivos físicos, al contrario de las subrutinas normales de un programa en ejecución. Como el salto de subrutina no es parte del hilo o secuencia de ejecución programada, el controlador guarda el estado del procesador en la pila de memoria y entra a ejecutar un código especial llamado "manejador de interrupciones" que atiende al periférico específico que generó la interrupción. Al terminar la rutina, una instrucción especial le indica al procesador el fin de la atención de la interrupción. En ese momento el controlador restablece el estado anterior, y el programa que se estaba ejecutando antes de la interrupción sigue como si nada hubiese pasado. Las rutinas de atención de interrupciones deben ser lo más breves posibles para que el rendimiento del sistema sea satisfactorio, porque normalmente cuando una interrupción es atendida, todas las demás interrupciones están en espera.

"Imagine que está esperando la visita de un amigo, al que llamaremos Juan. Usted y Juan han acordado que cuando él llegue a su casa esperará pacientemente a que le abra la puerta. Juan no debe tocar a la puerta porque alguien en la casa duerme y no quiere que le despierten."

"Ahora usted ha decidido leer un libro mientras espera a que Juan llegue a la casa, y para comprobar si ha llegado, cada cierto tiempo detiene la lectura, marca la página donde se quedó, se levanta y va hasta la puerta, abre y comprueba si Juan ha llegado, si éste todavía no está en la puerta, esperará unos minutos, cerrará la puerta y regresará a su lectura durante algún tiempo."

Como verá este es un método poco eficiente para esperar a Juan porque requiere que deje la lectura cada cierto tiempo y vaya hasta la puerta a comprobar si él ha llegado, además debe esperar un rato si todavía no llega. Y por si fuera poco, imagine que Juan no llega nunca porque se le presentó un problema, tuvo que cancelar la cita y no pudo avisarle a tiempo, o peor, que Juan ha llegado a la puerta un instante después que usted la cerraba. Juan, respetando lo acordado, espera un tiempo, pero se cansa de esperar a que le abran y decide marcharse porque cree que ya usted no está en la casa o no puede atenderlo. A este método de atender la llegada de Juan lo llamaremos encuesta.

Veamos ahora otro método. En esta ocasión simplemente se recuesta en el sofá de la sala y comienza a leer su libro, cuando Juan llegue debe tocar el timbre de la puerta y esperar unos momentos a que le atiendan. Cuando usted oye sonar el timbre, interrumpe la lectura, marca la página donde se quedó y va hasta la puerta para atender a la persona que toca el timbre. Una vez que Juan o la persona que ha tocado el timbre, se marcha, usted regresa a su asiento y retoma la lectura justo donde la dejó.
Este último es un método más eficiente que el anterior porque le deja más tiempo para leer y elimina algunos inconvenientes como el de que Juan nunca llegue o se marche antes de que usted abra la puerta. Es, en principio, un método simple pero muy eficaz y eficiente, lo llamaremos atención por interrupción.

El primero de ellos, la encuesta, es un método eficaz, pero poco eficiente porque requiere realizar lecturas constantes y muchas veces innecesarias del estado del proceso que queremos atender. Sin embargo, es muy utilizado en la programación de microcontroladores porque resulta fácil de aprender, la implementación de código con este método es menos compleja y no requiere de hardware especial para llevarla adelante. Por otra parte, la encuesta, tiene muchas deficiencias que con frecuencia obligan al diseñador a moverse hacia otros horizontes

El mundo está lleno de situaciones; de las cuales no podemos determinar ni cuando, ni como ni por qué se producen, en la mayoría de los casos lo único que podemos hacer es enterarnos de que determinada situación, asociada a un proceso, ha ocurrido. Para ello seleccionamos alguna condición o grupo de condiciones que nos indican que el proceso que nos interesa debe ser atendido, a este fenómeno, en el cual se dan las condiciones que nos interesa conocer, lo llamaremos evento. En el segundo ejemplo vemos que para atender a Juan, este debe tocar el timbre, por tanto, la llegada de Juan es el proceso que debemos atender y el sonido del timbre es el evento que nos indica que Juan ha llegado.

El método de atención a procesos por interrupción, visto desde la óptica del ejemplo que utilicé para mostrarlo, es más simple que el de la encuesta, pero no es cierto, el método se complica porque requiere que el microprocesador incorpore circuitos adicionales para registrar los eventos que le indican que debe atender al proceso asociado y comprender estos circuitos y su dinámica no es una tarea sencilla.

Los circuitos para la atención a las interrupciones y todas las tareas que debe realizar el procesador para atender al proceso que lo interrumpe son bastante complejos y requieren una visión diferente de la que estamos acostumbrados a tener de nuestro mundo.

Los seres humanos no estamos conscientes de las interrupciones, en nuestro organismo existen mecanismos que nos interrumpen constantemente, para ello tenemos a nuestro sistema sensorial, pero no somos conscientes del proceso de interrupción, aunque sí de la atención a las interrupciones. Eso es porque incorporamos mecanismos que nos sacan rápidamente de la tarea que estemos haciendo para atender una situación que no puede o no debe esperar mucho tiempo. Bien, esa misma es la idea que se incorpora en los microprocesadores para atender procesos que no pueden esperar o que no sabemos cuando deben ser atendidos porque ello depende de determinadas condiciones.

La cosa se complica en la secuencia de acciones a realizar desde el momento en que se desencadena el proceso de interrupción, hasta que se ejecuta el programa que lo atiende, y en la secuencia de acciones posteriores a la atención. Piense en cuantas cosas debe hacer su organismo ante una interrupción, utilicemos el segundo ejemplo para atender la llegada de Juan. Piense en cuantas cosas su cerebro hace a espaldas de su conciencia, desde el momento en que suena el timbre hasta que usted se encuentra listo (consciente de que es probable que Juan ha llegado) para abrir la puerta, y todo lo que su cerebro debe trabajar para retomar la lectura después que Juan se ha marchado. Todo eso, excepto abrir la puerta y atender a Juan, lo hacemos de forma “inconsciente” porque para ello tenemos sistemas dedicados en nuestro organismo, pero en el mundo de los microcontroladores debemos conocer todos esos detalles para poder utilizar los mecanismos de interrupción.

Los procesos de atención a interrupciones tienen la ventaja de que se implementan por hardware ubicado en el procesador, así que es un método rápido de hacer que el procesador se dedique a ejecutar un programa especial para atender eventos que no pueden esperar por mecanismos lentos como el de encuesta.

En términos generales, un proceso de interrupción y su atención por parte del procesador, tiene la siguiente secuencia de acciones:


Como podemos observar, el mecanismo de interrupción es bastante complicado, sin embargo tiene dos ventajas que obligan a su implementación: la velocidad y su capacidad de ser asíncrono. Ambas de conjunto permiten que aprovechemos al máximo las capacidades de trabajo de nuestro procesador.

Los mecanismos de interrupción no solo se utilizan para atender eventos ligados a procesos que requieren atención inmediata sino que se utilizan además para atender eventos de procesos asíncronos.

Las interrupciones son tan eficaces que permiten que el procesador actúe como si estuviese haciendo varias cosas a la vez cuando en realidad se dedica a la misma rutina de siempre, ejecutar instrucciones una detrás de la otra.

Cuando observamos la organización básica de un microcontrolador, señalamos que dentro de este se ubican un conjunto de periféricos. A continuación describiremos algunos de los periféricos que con mayor frecuencia encontraremos en los microcontroladores.

También conocidos como puertos de E/S, generalmente agrupadas en puertos de 8 bits de longitud, permiten leer datos del exterior o escribir en ellos desde el interior del microcontrolador, el destino habitual es el trabajo con dispositivos simples como relés, LED, o cualquier otra cosa que se le ocurra al programador.

Algunos puertos de E/S tienen características especiales que le permiten manejar salidas con determinados requerimientos de corriente, o incorporan mecanismos especiales de interrupción para el procesador.

Típicamente cualquier pin de E/S puede ser considerada E/S de propósito general, pero como los microcontroladores no pueden tener infinitos pines, ni siquiera todos los pines que queramos, las E/S de propósito general comparten los pines con otros periféricos. Para usar un pin con cualquiera de las características a él asignadas debemos configurarlo mediante los registros destinados a ellos.

Son circuitos sincrónicos para el conteo de los pulsos que llegan a su poder para conseguir la entrada de reloj. Si la fuente de un gran conteo es el oscilador interno del microcontrolador es común que no tengan un pin asociado, y en este caso trabajan como temporizadores. Por otra parte, cuando la fuente de conteo es externa, entonces tienen asociado un pin configurado como entrada, este es el modo contador.

Los temporizadores son uno de los periféricos más habituales en los microcontroladores y se utilizan para muchas tareas, como por ejemplo, la medición de frecuencia, implementación de relojes, para el trabajo de conjunto con otros periféricos que requieren una base estable de tiempo entre otras funcionalidades. Es frecuente que un microcontrolador típico incorpore más de un temporizador/contador e incluso algunos tienen arreglos de contadores. Como veremos más adelante este periférico es un elemento casi imprescindible y es habitual que tengan asociada alguna interrupción. Los tamaños típicos de los registros de conteo son 8 y 16 bits, pudiendo encontrar dispositivos que solo tienen temporizadores de un tamaño o con más frecuencia con ambos tipos de registro de conteo.

Como es muy frecuente el trabajo con señales analógicas, éstas deben ser convertidas a digital y por ello muchos microcontroladores incorporan un conversor analógico-digital, el cual se utiliza para tomar datos de varias entradas diferentes que se seleccionan mediante un multiplexor.

Las resoluciones más frecuentes son 8 y 10 bits, que son suficientes para aplicaciones sencillas. Para aplicaciones en control e instrumentación están disponibles resoluciones de 12bit, 16bit y 24bit. También es posible conectar un convertidor externo, en caso de necesidad

Este periférico está presente en casi cualquier microcontrolador, normalmente en forma de UART ("Universal Asynchronous Receiver Transmitter") o USART ("Universal Synchronous Asynchronous Receiver Transmitter") dependiendo de si permiten o no el modo sincrónico de comunicación.

El destino común de este periférico es la comunicación con otro microcontrolador o con una PC y en la mayoría de los casos hay que agregar circuitos externos para completar la interfaz de comunicación.
La forma más común de completar el puerto serie es para comunicarlo con una PC mediante la interfaz EIA-232 (más conocida como RS-232), es por ello que muchas personas se refieren a la UART o USART como puerto serie RS-232, pero esto constituye un error, puesto que este periférico se puede utilizar para interconectar dispositivos mediante otros estándares de comunicación.
En aplicaciones industriales se utiliza preferiblemente RS-485 por sus superior alcance en distancia, velocidad y resistencia al ruido.

Este tipo de periférico se utiliza para comunicar al microcontrolador con otros microcontroladores o con periféricos externos conectados a él, por medio de una interfaz muy sencilla. Hay solo un nodo controlador que permite iniciar cualquier transacción, lo cual es una desventaja en sistemas complejos, pero su sencillez permite el aislamiento galvánico de forma directa por medio de optoacopladores.

Cumple las mismas funciones que el SPI, pero requiere menos señales de comunicación y cualquier nodo puede iniciar una transacción. Es muy utilizado para conectar las tarjetas gráficas de las computadoras personales con los monitores, para que estos últimos informen de sus prestaciones y permitir la autoconfiguración del sistema de vídeo.

Los microcontroladores son los que han permitido la existencia de este sistema de comunicación. Es un sistema que trabaja por polling (monitorización) de un conjunto de periféricos inteligentes por parte de un amo, que es normalmente un computador personal. Cada modo inteligente está gobernado inevitablemente por un microcontrolador.

Es el sistema más extendido en el mundo para redes de área local cableadas. Los microcontroladores más poderosos de 32 bits se usan para implementar periféricos lo suficientemente poderosos como para que puedan ser accesados directamente por la red. Muchos de los enrutadores caseros de pequeñas empresas están construidos sobre la base de un microcontrolador que hace del cerebro del sistema.

Este protocolo es del tipo CSMA/CD con tolerancia a elevados niveles de tensión de modo común y orientado al tiempo real.
Este protocolo es el estándar más importante en la industria automotriz (OBD). También se usa como capa física del "field bus" para el control industrial.

Hay una enorme cantidad de otros buses disponibles para la industria automotriz (linbus) o de medios audiovisuales como el i2s, IEEE 1394. El usuario se los encontrará cuando trabaje en algún área especializada.

Son circuitos analógicos basados en amplificadores operacionales que tienen la característica de comparar dos señales analógicas y dar como salida los niveles lógicos ‘0’ o ‘1’ en dependencia del resultado de la comparación. Es un periférico muy útil para detectar cambios en señales de entrada de las que solamente nos interesa conocer cuando está en un rango determinado de tensión.

Los PWM (Pulse Width Modulator) son periféricos muy útiles sobre todo para el control de motores, sin embargo hay un grupo de aplicaciones que pueden realizarse con este periférico, dentro de las cuales podemos citar: inversión DC/AC para UPS, conversión digital analógica D/A, control regulado de luz (dimming) entre otras.

Muchos microcontroladores han incorporado estos tipos de memoria como un periférico más, para el almacenamiento de datos de configuración o de los procesos que se controlan. Esta memoria es independiente de la memoria de datos tipo RAM o la memoria de programas, en la que se almacena el código del programa a ejecutar por el procesador del microcontrolador.

Muchos de los microcontroladores PIC incluyen este tipo de memoria, típicamente en forma de memoria EEPROM, incluso algunos de ellos permiten utilizar parte de la memoria de programas como memoria de datos no volátil, por lo que el procesador tiene la capacidad de escribir en la memoria de programas como si ésta fuese un periférico más.

Los microcontroladores más comunes en uso son:

Observación: Algunas arquitecturas de microcontrolador están disponibles por tal cantidad de vendedores y en tantas variedades, que podrían tener, con total corrección, su propia categoría. Entre ellos encontramos, principalmente, las variantes de Intel 8051 y Z80.



</doc>
<doc id="4963" url="https://es.wikipedia.org/wiki?curid=4963" title="Lógica">
Lógica

La lógica es la ciencia formal y rama tanto de la filosofía como de las matemáticas que estudia los principios de la demostración y la inferencia válida, las falacias, las paradojas y la noción de verdad.

La lógica matemática es la que estudia la inferencia mediante sistemas formales como la lógica proposicional, la lógica de primer orden y la lógica modal. La lógica computacional es la aplicación de la lógica matemática a las ciencias de la computación. La lógica filosófica utiliza los métodos y resultados de la lógica moderna para el estudio de problemas filosóficos.

Los orígenes de la lógica se remontan a la Edad Antigua, con brotes independientes en China, India y Grecia. Desde entonces, la lógica tradicionalmente se considera una rama de la filosofía, pero en el siglo XX la lógica ha pasado a ser principalmente la lógica matemática, y por lo tanto ahora también se considera parte de las matemáticas, e incluso una ciencia formal independiente.

La palabra «lógica» deriva del griego antiguo λογική "logikḗ", que significa «dotada de razón, intelectual, dialéctica, argumentativa» y que a su vez viene de λόγος ("lógos"), «palabra, pensamiento, idea, argumento, razón o principio».

En el lenguaje cotidiano, expresiones como «lógica» o «pensamiento lógico» aportan también un sentido alrededor de un «pensamiento lateral» comparado, haciendo los contenidos de la afirmación coherentes con un contexto, bien sea del discurso o de una teoría de la ciencia, o simplemente con las creencias o evidencias transmitidas por la tradición cultural.

Del mismo modo existe el concepto sociológico y cultural de lógica como, p.e. «la lógica de las mujeres», «lógica deportiva», etc. que, en general, podríamos considerar como «lógica cotidiana» - también conocida como «lógica del sentido común».

En estas áreas la «lógica» suele tener una referencia lingüística en la pragmática.

Un argumento en este sentido tiene su «lógica» cuando resulta convincente, razonable y claro; en definitiva cuando cumple una función de eficacia. La habilidad de pensar y expresar un argumento así corresponde a la retórica, cuya relación con la verdad es una relación probable.




</doc>
<doc id="4971" url="https://es.wikipedia.org/wiki?curid=4971" title="Wikimedia Meta-Wiki">
Wikimedia Meta-Wiki

Wikimedia Meta-Wiki (llamada Meta-Wiki o simplemente Meta) es un sitio web dedicado a coordinar los proyectos de la Fundación Wikimedia, como Wikipedia. Tiene el formato wiki. 

Durante la primera fase de existencia de Wikipedia, las discusiones sobre el proyecto de Wikipedia y en especial, sobre las políticas a seguir, tenían lugar frecuentemente en la propia Wikipedia, aunque dicha metadiscusión no era de contenido enciclopédico. 

Además, algunos usuarios querían un foro donde no tuvieran que estar limitados por la política del punto de vista neutral de Wikipedia, de forma que pudieran escribir ensayos expresando sus propias opiniones sobre los asuntos que cubría la enciclopedia. 

En respuesta a estas preocupaciones, los escritores de Wikipedia crearon la en noviembre de 2001. 

La Meta-Wiki sirve actualmente como una de las tres mayores vías de discusión para los s, junto con las listas de correo y las páginas de discusión de artículos concretos (que están reservados a la discusión de ese artículo en particular).

Originariamente enfocada hacia la versión en lengua inglesa de la Wikipedia, Meta-Wiki se ha convertido desde su actualización a la de Wikipedia, en un foro de discusión multilingüe utilizado por todas las comunidades lingüísticas de los proyectos Wikimedia. 


</doc>
<doc id="4976" url="https://es.wikipedia.org/wiki?curid=4976" title="Memoria de acceso aleatorio">
Memoria de acceso aleatorio

La memoria de acceso aleatorio ("Random Access Memory", RAM) se utiliza como memoria de trabajo de computadoras y otros dispositivos para el sistema operativo, los programas y la mayor parte del software.
En la RAM se cargan todas las instrucciones que ejecuta la unidad central de procesamiento (procesador) y otras unidades del computador, además de contener los datos que manipulan los distintos programas.

Se denominan «de acceso aleatorio» porque se puede leer o escribir en una posición de memoria con un tiempo de espera igual para cualquier posición, no siendo necesario seguir un orden para acceder (acceso secuencial) a la información de la manera más rápida posible.

Durante el encendido de la computadora, la rutina POST verifica que los módulos de RAM estén conectados de manera correcta. En el caso que no existan o no se detecten los módulos, la mayoría de tarjetas madres emiten una serie de sonidos que indican la ausencia de memoria principal. Terminado ese proceso, la memoria BIOS puede realizar un test básico sobre la memoria RAM indicando fallos mayores en la misma.

Uno de los primeros tipos de memoria RAM fue la memoria de núcleo magnético, desarrollada entre 1949 y 1952 y usada en muchos computadores hasta el desarrollo de circuitos integrados a finales de los años 60 y principios de los 70. Esa memoria requería que cada bit estuviera almacenado en un toroide de material ferromagnético de algunos milímetros de diámetro, lo que resultaba en dispositivos con una capacidad de memoria muy pequeña. Antes que eso, las computadoras usaban relés y líneas de retardo de varios tipos construidas para implementar las funciones de memoria principal con o sin acceso aleatorio.

En 1969 fueron lanzadas una de las primeras memorias RAM basadas en semiconductores de silicio por parte de Intel con el integrado 3101 de 64 bits de memoria y para el siguiente año se presentó una memoria DRAM de 1024 bytes, referencia 1103 que se constituyó en un hito, ya que fue la primera en ser comercializada con éxito, lo que significó el principio del fin para las memorias de núcleo magnético. En comparación con los integrados de memoria DRAM actuales, la 1103 es primitiva en varios aspectos, pero tenía un desempeño mayor que la memoria de núcleos.

En 1973 se presentó una innovación que permitió otra miniaturización y se convirtió en estándar para las memorias DRAM: la multiplexación en tiempo de la direcciones de memoria. MOSTEK lanzó la referencia MK4096 de 4096 bytes en un empaque de 16 pines, mientras sus competidores las fabricaban en el empaque DIP de 22 pines. El esquema de direccionamiento se convirtió en un estándar de facto debido a la gran popularidad que logró esta referencia de DRAM. 
Para finales de los 70 los integrados eran usados en la mayoría de computadores nuevos, se soldaban directamente a las placas base o se instalaban en zócalos, de manera que ocupaban un área extensa de circuito impreso. Con el tiempo se hizo obvio que la instalación de RAM sobre el impreso principal, impedía la miniaturización , entonces se idearon los primeros módulos de memoria como el SIPP, aprovechando las ventajas de la construcción modular. El formato SIMM fue una mejora al anterior, eliminando los pines metálicos y dejando unas áreas de cobre en uno de los bordes del impreso, muy similares a los de las tarjetas de expansión, de hecho los módulos SIPP y los primeros SIMM tienen la misma distribución de pines.

A finales de los 80 el aumento en la velocidad de los procesadores y el aumento en el ancho de banda requerido, dejaron rezagadas a las memorias DRAM con el esquema original MOSTEK, de manera que se realizaron una serie de mejoras en el direccionamiento como las siguientes:

"Fast Page Mode RAM" (FPM-RAM) fue inspirado en técnicas como el "Burst Mode" usado en procesadores como el Intel 486. Se implantó un modo direccionamiento en el que el controlador de memoria envía una sola dirección y recibe a cambio esa y varias consecutivas sin necesidad de generar todas las direcciones. Esto supone un ahorro de tiempos ya que ciertas operaciones son repetitivas cuando se desea acceder a muchas posiciones consecutivas. Funciona como si deseáramos visitar todas las casas en una calle: después de la primera vez no sería necesario decir el número de la calle únicamente seguir la misma. Se fabricaban con tiempos de acceso de 70 ó 60 ns y fueron muy populares en sistemas basados en el 486 y los primeros Pentium.

"Extended Data Output RAM" (EDO-RAM) fue lanzada al mercado en 1994 y con tiempos de accesos de 40 o 30 ns suponía una mejora sobre FPM, su antecesora. La EDO, también es capaz de enviar direcciones contiguas pero direcciona la columna que va a utilizar mientras que se lee la información de la columna anterior, dando como resultado una eliminación de estados de espera, manteniendo activo el búfer de salida hasta que comienza el próximo ciclo de lectura.

"Burst Extended Data Output RAM" (BEDO-RAM) fue la evolución de la EDO-RAM y competidora de la SDRAM, fue presentada en 1997. Era un tipo de memoria que usaba generadores internos de direcciones y accedía a más de una posición de memoria en cada ciclo de reloj, de manera que lograba un 50 % de beneficios, mejor que la EDO. Nunca salió al mercado, dado que Intel y otros fabricantes se decidieron por esquemas de memoria sincrónicos que si bien tenían mucho del direccionamiento MOSTEK, agregan funcionalidades distintas como señales de reloj.

Las dos formas principales de RAM moderna son:

La expresión memoria RAM se utiliza frecuentemente para describir a los módulos de memoria utilizados en las computadoras personales y servidores.

La RAM es solo una variedad de la memoria de acceso aleatorio: las ROM, memorias Flash, caché (SRAM), los registros en procesadores y otras unidades de procesamiento también poseen la cualidad de presentar retardos de acceso iguales para cualquier posición.

Los módulos de RAM son la presentación comercial de este tipo de memoria, que se compone de circuitos integrados soldados sobre un circuito impreso independiente, en otros dispositivos como las consolas de videojuegos, la RAM va soldada directamente sobre la placa principal.

Los módulos de RAM son tarjetas o placas de circuito impreso que tienen soldados chips de memoria DRAM, por una o ambas caras.

La implementación DRAM se basa en una topología de circuito eléctrico que permite alcanzar densidades altas de memoria por cantidad de transistores, logrando integrados de cientos o miles de megabits. Además de DRAM, los módulos poseen un integrado que permiten la identificación de los mismos ante la computadora por medio del protocolo de comunicación "Serial Presence Detect" (SPD).

La conexión con los demás componentes se realiza por medio de un área de pines en uno de los filos del circuito impreso, que permiten que el módulo al ser instalado en un zócalo o ranura apropiada de la placa base, tenga buen contacto eléctrico con los controladores de memoria y las fuentes de alimentación.

La necesidad de hacer intercambiable los módulos, y de utilizar integrados de distintos fabricantes, condujo al establecimiento de estándares de la industria como los "Joint Electron Device Engineering Council" (JEDEC).


La tecnología de memoria actual usa una señal de sincronización para realizar las funciones de lectura/escritura de manera que siempre está sincronizada con un reloj del bus de memoria, a diferencia de las antiguas memorias FPM y EDO que eran asíncronas.

Toda la industria se decantó por las tecnologías síncronas, porque permiten construir integrados que funcionen a una frecuencia superior a 66 MHz.

Tipos de DIMM según su cantidad de contactos o pines:

Memoria síncrona, con tiempos de acceso de entre 25 y 10 ns y que se presentan en módulos DIMM de 168 contactos. Fue utilizada en los Pentium II y en los Pentium III , así como en los AMD K6, AMD Athlon K7 y Duron. 
Está muy extendida la creencia de que se llama "SDRAM" a secas, y que la denominación SDR SDRAM es para diferenciarla de la memoria DDR, pero no es así, simplemente se extendió muy rápido la denominación incorrecta. El nombre correcto es "SDR SDRAM" ya que ambas (tanto la SDR como la DDR) son memorias síncronas dinámicas. Los tipos disponibles son:

Se presentan en módulos RIMM de 184 contactos. Fue utilizada en los Pentium 4 . 
Era la memoria más rápida en su tiempo, pero por su elevado costo fue rápidamente cambiada por la económica DDR.
Los tipos disponibles son:

Memoria síncrona, envía los datos dos veces por cada ciclo de reloj. De este modo trabaja al doble de velocidad del bus del sistema, sin necesidad de aumentar la frecuencia de reloj. Se presenta en módulos DIMM de 184 contactos en el caso de ordenador de escritorio y en módulos de 144 contactos para los ordenadores portátiles. 

La nomenclatura utilizada para definir a los módulos de memoria de tipo DDR (esto incluye a los formatos DDR2, DDR3 y DDR4) es la siguiente: DDRx-yyyy PCx-zzzz; donde x representa a la generación DDR en cuestión; yyyy la frecuencia aparente o efectiva, en Megaciclos por segundo (MHz); y zzzz la máxima tasa de transferencia de datos por segundo, en Megabytes, que se puede lograr entre el módulo de memoria y el controlador de memoria. La tasa de transferencia depende de dos factores, el ancho de bus de datos (por lo general 64 bits) y la frecuencia aparente o efectiva de trabajo. La fórmula que se utiliza para calcular la máxima tasa de transferencia por segundo entre el módulo de memoria y su controlador, es la siguiente:

Tasa de transferencia en MB/s = (Frecuencia DDR efectiva) × (64 bits / 8 bits por cada byte)

Por ejemplo:

1 GB DDR-400 PC-3200: Representa un módulo de 1 GB (Gigabyte) de tipo DDR; con frecuencia aparente o efectiva de trabajo de 400 MHz; y una tasa de transferencia de datos máxima de 3200 MB/s.

4 GB DDR3-2133 PC3-17000: Representa un módulo de 4 GB de tipo DDR3; frecuencia aparente o efectiva de trabajo de 2133 MHz; y una tasa de transferencia de datos máxima de 17000 MB/s.

Los tipos disponibles son:

Las memorias DDR 2 son una mejora de las memorias DDR "(Double Data Rate)", que permiten que los búferes de entrada/salida trabajen al doble de la frecuencia del núcleo, permitiendo que durante cada ciclo de reloj se realicen cuatro transferencias. Se presentan en módulos DIMM de 240 contactos. Los tipos disponibles son:

Las memorias DDR 3 son una mejora de las memorias DDR 2, proporcionan significantes mejoras en el rendimiento en niveles de bajo voltaje, lo que lleva consigo una disminución del gasto global de consumo. Los módulos DIMM DDR 3 tienen 240 pines, el mismo número que DDR 2; sin embargo, los DIMMs son físicamente incompatibles, debido a una ubicación diferente de la muesca. Los tipos disponibles son:


Dentro de la jerarquía de memoria, la RAM se encuentra en un nivel después de los registros del procesador y de las cachés en cuanto a velocidad.

Los módulos de RAM se conectan eléctricamente a un controlador de memoria que gestiona las señales entrantes y salientes de los integrados DRAM. Las señales son de tres tipos: direccionamiento, datos y señales de control. En el módulo de memoria esas señales están divididas en dos buses y un conjunto misceláneo de líneas de control y alimentación. Entre todas forman el bus de memoria que conecta la RAM con su controlador:
Algunos controladores de memoria en sistemas como PC y servidores se encuentran embebidos en el llamado puente norte ("North Bridge") de la placa base. Otros sistemas incluyen el controlador dentro del mismo procesador (en el caso de los procesadores desde AMD Athlon 64 e Intel Core i7 y posteriores). En la mayoría de los casos el tipo de memoria que puede manejar el sistema está limitado por los "sockets" para RAM instalados en la placa base, a pesar de que los controladores de memoria en muchos casos son capaces de conectarse con tecnologías de memoria distintas.

Una característica especial de algunos controladores de memoria, es el manejo de la tecnología canal doble o doble canal ("Dual Channel"), donde el controlador maneja bancos de memoria de 128 bits, siendo capaz de entregar los datos de manera intercalada, optando por uno u otro canal, reduciendo las latencias vistas por el procesador. La mejora en el desempeño es variable y depende de la configuración y uso del equipo. Esta característica ha promovido la modificación de los controladores de memoria, resultando en la aparición de nuevos "chipsets" (la serie 865 y 875 de Intel) o de nuevos zócalos de procesador en los AMD (el 939 con canal doble , reemplazo el 754 de canal sencillo). Los equipos de gamas media y alta por lo general se fabrican basados en "chipsets" o zócalos que soportan doble canal o superior, como en el caso del zócalo ("socket") 1366 de Intel, que usaba un triple canal de memoria, o su nuevo LGA 2011 que usa cuádruple canal.

Existen dos clases de errores en los sistemas de memoria, las fallas ("Hard fails") que son daños en el hardware y los errores ("soft errors") provocados por causas fortuitas. Los primeros son relativamente fáciles de detectar (en algunas condiciones el diagnóstico es equivocado), los segundos al ser resultado de eventos aleatorios, son más difíciles de hallar.
En la actualidad la confiabilidad de las memorias RAM frente a los errores, es suficientemente alta como para no realizar verificación sobre los datos almacenados, por lo menos para aplicaciones de oficina y caseras. En los usos más críticos, se aplican técnicas de corrección y detección de errores basadas en diferentes estrategias:

Por lo general, los sistemas con cualquier tipo de protección contra errores tiene un coste más alto, y sufren de pequeñas penalizaciones en desempeño, con respecto a los sistemas sin protección. Para tener un sistema con ECC o paridad, el "chipset" y las memorias deben tener soporte para esas tecnologías. La mayoría de placas base no poseen dicho soporte.

Para los fallos de memoria se pueden utilizar herramientas de software especializadas que realizan pruebas sobre los módulos de memoria RAM. Entre estos programas uno de los más conocidos es la aplicación Memtest86+ que detecta fallos de memoria.

Es un tipo de módulo usado frecuentemente en servidores, posee circuitos integrados que se encargan de repetir las señales de control y direcciones: las señales de reloj son reconstruidas con ayuda del PLL que está ubicado en el módulo mismo. Las señales de datos se conectan de la misma forma que en los módulos no registrados: de manera directa entre los integrados de memoria y el controlador.
Los sistemas con memoria registrada permiten conectar más módulos de memoria y de una capacidad más alta, sin que haya perturbaciones en las señales del controlador de memoria, permitiendo el manejo de grandes cantidades de memoria RAM. Entre las desventajas de los sistemas de memoria registrada están el hecho de que se agrega un ciclo de retardo para cada solicitud de acceso a una posición no consecutiva y un precio más alto que los módulos no registrados.
La memoria registrada es incompatible con los controladores de memoria que no soportan el modo registrado, a pesar de que se pueden instalar físicamente en el zócalo. Se pueden reconocer visualmente porque tienen un integrado mediano, cerca del centro geométrico del circuito impreso, además de que estos módulos suelen ser algo más altos.

Durante el año 2006 varias marcas lanzaron al mercado sistemas con memoria FB-DIMM que en su momento se pensaron como los sucesores de la memoria registrada, pero se abandonó esa tecnología en 2007 dado que ofrecía pocas ventajas sobre el diseño tradicional de memoria registrada y los nuevos modelos con memoria DDR3.


</doc>
<doc id="4977" url="https://es.wikipedia.org/wiki?curid=4977" title="RAM (desambiguación)">
RAM (desambiguación)

RAM, Ram o R.A.M. pueden referirse a:










</doc>
<doc id="4978" url="https://es.wikipedia.org/wiki?curid=4978" title="Memoria programable de solo lectura">
Memoria programable de solo lectura

La memoria programable de solo lectura o PROM (del inglés "programmable read-only memory") es una memoria digital donde el valor de cada bit depende del estado de un fusible (o antifusible), que puede ser quemado una sola vez. Por esto la memoria puede ser programada (pueden ser escritos los datos) una sola vez a través de un dispositivo especial, un programador PROM. Estas memorias son utilizadas para grabar datos permanentes en cantidades menores a las ROM, o cuando los datos deben cambiar en muchos o todos los casos.

Pequeñas PROM han venido utilizándose como generadores de funciones, normalmente en conjunción con un multiplexor. A veces se preferían a las ROM porque son bipolares, habitualmente Schottky, consiguiendo mayores velocidades. 

Una PROM común se encuentra con todos los bits en valor 1 como valor por defecto de las fábricas; el quemado de cada fusible, cambia el valor del correspondiente bit a 0. La programación se realiza aplicando pulsos de altos voltajes que no se encuentran durante operaciones normales (12 a 21 voltios). El término "read-only" (solo lectura) se refiere a que, a diferencia de otras memorias, los datos no pueden ser cambiados (al menos por el usuario final).

La memoria PROM fue inventada en 1956 por Wen Tsing Chow, trabajando para la «División Arma», de la American Bosch Arma Corporation en Garden City, Nueva York. La invención fue concebida a petición de la Fuerza aérea de los Estados Unidos, para conseguir una forma más segura y flexible para almacenar las constantes de los objetivos en la computadora digital del MBI "Atlas E/F".

La patente y la tecnología asociadas fueron mantenidas bajo secreto por varios años mientras el "Atlas E/F" era el principal misil de Estados Unidos. El término «quemar», refiriéndose al proceso de grabar una PROM, se encuentra también en la patente original, porque como parte de la implementación original debía quemarse literalmente los diodos internos con un exceso de corriente para producir la discontinuidad del circuito. Las primeras máquinas de programación de PROMs también fueron desarrolladas por ingenieros de la División Arma bajo la dirección del Sr. Chow y fueron ubicados el laboratorio Arma de Garden City, y en la jefatura del Comando estratégico aéreo de las Fuerzas Aéreas.

Wen Tsing Chow y otros ingenieros de la División Armada continuaron con este suceso diseñando la primera «memoria de solo lectura no destruible» ("non-destructive read-only memory"', "NDROM") para aplicarlo a misiles guiados, fundamentado en una base de doble abertura magnética. Estas memorias, diseñadas originalmente para mantener constantes de objetivos, fueron utilizadas para sistemas de armas de misiles balísticos intercontinentales y de rango medio móvil.

La principal motivación para este invento fue que la Fuerza Aérea Estadounidense necesitaba reducir los costes de la fabricación de plaquetas de objetivos basadas en PROMs que necesitaban cambios constantes a medida que llegaba nueva información sobre objetivos del bloque de naciones comunistas. Como estas memorias son borrables, programables y re-programables, constituyen la primera implementación de una producción de memorias EPROM y EEPROM, de fabricación anterior al 1963.

Debe observarse que los términos modernos de estos dispositivos, PROM, EPROM y EEPROM, no fueron creados hasta un tiempo después de que las aplicaciones de misiles nucleares guiados hayan estado operacionales. Las implementaciones originales de Arma se refieren a las PROMs como "matriz de almacenamiento de constantes"; y a las EPROMsy EEPROM simplemente eran denominadas «memorias NDRO».

Las modernas implementaciones comerciales de las PROM, EPROM y EEPROM basadas en circuitos integrados, borrado por luz ultravioleta, y varias propiedades de los transistores, aparecen unos diez años después. Hasta que esas nuevas implementaciones fueron desarrolladas, fuera de aplicaciones militares, era más barato fabricar memorias ROM que utilizar una de las nuevas caras tecnologías desarrolladas y fabricados por los contratistas de misiles de las fuerzas aéreas.

De todas formas, en misiles, naves espaciales, satélites y otras aplicaciones de mucha confiabilidad, siguen en uso muchos de los métodos de la implementación original de los años 1950.



</doc>
<doc id="4981" url="https://es.wikipedia.org/wiki?curid=4981" title="Perissodactyla">
Perissodactyla

Los perisodáctilos (Perissodactyla) son un orden de mamíferos placentarios. Son mamíferos ungulados que se caracterizan por la posesión de extremidades con un número impar de dedos terminados en pezuñas, y con el dedo central, que sirve de apoyo, más desarrollado que los demás. Son herbívoros. En la actualidad solo incluye a los caballos, asnos, cebras (suborden Hippomorpha), los tapires y los rinocerontes (suborden Ceratomorpha).

Los primeros perisodáctilos eran muy similares a los otros ungulados primitivos. Las patas desarrollan la condición mesaxónica, es decir, el dedo central (tercero) se hace más grande y los demás se reducen, al mismo tiempo que la región inferior de las extremidades se alarga y la superior se acorta, con reducción de la ulna (cúbito) y la fíbula (peroné).

Un rasgo distintivo de los perisodáctilos es la organización del carpo y del tarso. En las patas anteriores (a veces referidas como ""manos"" en el habla popular), un carpiano distal, el hueso grande, se ensancha y se articula con los carpianos proximales, mientras que en el pie, el ectocuneiforme se transforma en un gran hueso plano que transmite el empuje del astrágalo a través de un navicular aplanado; el astrágalo tiene la superficie inferior aplanada y no a modo de tróclea como en los artiodáctilos.

El aparato digestivo está menos especializado que en los artiodáctilos. Los incisivos se conservan y se usan para segar la hierba, los caninos están reducidos o ausentes y, a menudo, hay un diastema. Los molares de las formas primitivas permanecieron bunodontos y de corona baja, pero en los rinocerontes y caballos modernos tienen una elaborada superficie trituradora; los premolares se molarizan, con lo que se consigue un gran superficie trituradora.

El tubo digestivo es más simple que el de los artiodáctilos. El estómago no presenta cámaras. La digestión de la celulosa se realiza en el ciego y el intestino grueso, que puede estar muy desarrollado.

Los perisodáctilos son macrosmáticos, es decir, predomina el sentido del olfato, con la porción sensorial de la nariz muy desarrollada.

Los perisodáctilos más primitivos se habían separado muy poco de los condilartros. "Hyracotherium", del Eoceno tenía el tamaño de un perro pequeño y se parecía al condilartro "Phenacodus"; tenía una dentición completa, con premolares trituberculados y molares cuadrados bunodontos con dos crestas transversales. La locomoción era digitígrada en las formas primitivas.

Los primeros perisodáctilos conocidos datan del Eoceno, aunque posiblemente surgieron en Asia durante el Paleoceno Superior, menos de 10 millones de años después de la extinción masiva del Cretácico-Terciario en la cual se extinguieron los dinosaurios y otros muchos organismos. A principios de Eoceno, hace 55 millones de años, ya estaban diversificados y ocupaban varios continentes.

Los caballos y tapires evolucionaron en Norte América mientras que los rinocerontes parecen haber evolucionado en Asia a partir de animales similares al tapir y luego haber recolonizado América durante el Eoceno Medio (hace unos 45 millones de años). Existieron 15 o 16 familias, de las cuales solo tres sobreviven en la actualidad.

Dichas familias fueron muy diversas en apariencia y tamaño; algunas incluían animales gigantescos (Brontotheriidae) y extraños (Chalicotheriidae). El mayor perisodáctilo fue un rinoceronte asiático ("Paraceratherium") que, con 11 toneladas, fue más de dos veces mayor que los elefantes actuales.

Los perisodáctilos fueron el grupo de grandes mamíferos herbívoros dominante durante el Oligoceno. No obstante, la expansión de las praderas durante el Mioceno (hace unos 20 millones de años) favorecieron a los artiodáctilos que, con sus estómagos provistos de cámaras se adaptaron mejor a una dieta tan pobre en nutrientes y pronto les arrebataron la supremacía. A pesar de ello, muchas especies de perisodáctilos sobrevivieron y prosperaron hasta el final del Pleistoceno (hace solo 10.000 años), cuando no pudieron soportar la presión de los cazadores humanos ni el nuevo hábitat.

La siguiente clasificación (hasta el nivel de familia) está basada en el estudio de McKenna & Bell (1997)

Orden Perissodactyla



Cladograma según Hooker y Dashzeveg de 2004:


</doc>
<doc id="4982" url="https://es.wikipedia.org/wiki?curid=4982" title="Autollamador">
Autollamador

Un autollamador es un dispositivo electrónico que puede llamar automáticamente a números de teléfono para comunicar entre dos puntos cualesquiera de las redes telefónicas, celulares (móviles) y de buscas.

Una vez que se ha establecido la comunicación (mediante un intercambio telefónico), el autollamador anunciará mensajes verbales o transmitirá datos digitales, por ejemplo, mensajes de texto en formato "servicio de mensajes cortos" (SMS) a la parte llamada.



</doc>
<doc id="4985" url="https://es.wikipedia.org/wiki?curid=4985" title="Alarma autollamadora">
Alarma autollamadora

Las alarmas autollamadoras tienen un transmisor accionado por un pulsador, que se lleva puesto para caso de necesitar ayuda sea cual sea el lugar de la casa donde la persona que lo necesita se encuentre.

Estos transmisores portátiles envían una señal a la unidad de alarma. El transmisor está diseñado para ser colocado alrededor del cuello o de la muñeca, o para ser pinzado en la ropa. Algunos modelos ofrecen métodos adicionales para fijar la alarma, por ejemplo, un pulsador en la propia unidad de alarma, botones de pared o cordones de tirado. 

Una vez que se ha presionado el botón, la unidad de alarma contactará automáticamente con un familiar o amigo o con un centro de control especial. La mayoría de los sistemas de alarma permiten a la persona que contesta a la llamada el oír y hablar con el , aunque esto no es imprescindible cuando ha saltado la alarma mediante la pulsación del botón.

Si la alarma ha conectado con el centro de control, aparecerán automáticamente los datos del llamante en la pantalla del operador. El operador tratará generalmente de hablar con el llamante, principalmente mediante un canal de intercomunicación que se ha abierto cuando se ha activado la alarma y si el llamante necesita ayuda, o si el operador no obtiene respuesta, enviará una persona ( personal sanitario, bomberos, etc.. ) que realice una visita.

Una alarma que llame directamente a la casa de un familiar o amigo, estará generalmente programada para llamar a más de un número, para aumentar las posibilidades de encontrar a alguien.

A continuación detallamos las ventajas y desventajas de los sistemas de alarmas autollamadores o auto dialers, como también se conocen.






</doc>
<doc id="4986" url="https://es.wikipedia.org/wiki?curid=4986" title="Radioastronomía">
Radioastronomía

La radioastronomía es la rama de la astronomía que estudia los objetos celestes y los fenómenos astrofísicos midiendo su emisión de radiación electromagnética en la región de radio del espectro. Las ondas de radio tienen una longitud de onda mayor que la de la luz visible. En la radioastronomía, para poder recibir buenas señales, se deben utilizar grandes antenas, o grupos de antenas más pequeñas trabajando en paralelo. La mayoría de los radiotelescopios utilizan una antena parabólica para amplificar las ondas, y así obtener una buena lectura de estas. Esto permite a los astrónomos observar el espectro de radio de una región del cielo. La radioastronomía es un área relativamente nueva de la investigación astronómica, que todavía tiene mucho por descubrir.

En la actualidad, existen gigantescos radiotelescopios, permitiendo observaciones de una resolución imposible en otras longitudes de onda. Entre los problemas que la radioastronomía ayuda a estudiar, se encuentran la formación estelar, las galaxias activas, la cosmología, etc.

Una de las primeras investigaciones de ondas de radio de origen extraterrestre fue llevada a cabo por Karl Guthe Jansky, un ingeniero de Bell Telephone Laboratories, en los comienzos de 1930. El primer objeto detectado fue el centro de la Vía Láctea, seguido por el Sol. Estos primeros descubrimientos fueron confirmados por Grote Reber en 1938. Después de la Segunda Guerra Mundial, en Europa y los Estados Unidos, se desarrollaron importantes mejoras en la radioastronomía, y el campo de la radioastronomía comenzó a florecer.

Uno de los desarrollos más notables vino en 1946 con la introducción de la radio interferometría por Martin Ryle de Cavendish Astrophysics Group en Cambridge (quien obtuvo el Premio Nobel por esto, y su trabajo de apertura sintética), también el espejo interferómetro de Lloyd desarrollado independientemente por Joseph Pawsey's en 1946 en la Universidad de Sydney. Dos temas, uno astronómico y uno técnico, dominaron la investigación en Cambridge desde fines de 1940 por más de treinta años. ¿Cuál era la naturaleza de las fuentes de radio discretas, o "estrellas de radio"? ¿Dónde estaban?, ¿cuáles eran ellas?, ¿cuáles eran sus características?, ¿cuántas existían ahí afuera?, ¿cómo funcionaban y cuál era su significado en el universo? De importancia paralela era el rompecabezas de cómo idear las nuevas clases de radiotelescopio que aclararían estas preguntas astronómicas.

La radioastronomía ha llevado a un importante incremento en el conocimiento astronómico, particularmente con el descubrimiento de muchas clases de nuevos objetos, incluyendo los púlsares, quásares y las galaxias activas. Esto es debido a que la radioastronomía nos permite ver cosas que no son posibles de detectar en la astronomía óptica. Tales objetos representas algunos de los procesos físicos más extremos y energéticos en el universo.

La radioastronomía es también, en parte responsable por la idea de que la materia oscura es una importante componente de nuestro universo; las mediciones de radio de la rotación de las galaxias sugiere que hay muchas más masa en las galaxias que la que ha sido observada directamente. La radiación de fondo de microondas (CMB) fue detectada por primera vez utilizando radiotelescopios. Los radiotelescopios también han sido utilizados para investigar objetos mucho más cercanos a la tierra, incluyendo observaciones del Sol, la actividad solar y mapeos por radar del los planetas del Sistema Solar.

Los radiotelescopios pueden ser ahora encontrados por todo el mundo. Radiotelescopios muy distanciados unos de otros, son utilizados frecuentemente en combinación utilizando una técnica llamada interferometría para obtener observaciones de alta resolución que no pueden ser obtenidas utilizando un solo receptor. Inicialmente radiotelescopios distanciados por unos pocos kilómetros eran combinados usando interferometría, pero a partir de 1970, radiotelescopios alrededor de todo el mundo (incluso orbitando la tierra) son combinados para realizar mapeos interferómetros de gran tamaño (Very Long Baseline Interferometry (VLBI)).
La dificultad de adquirir altas resoluciones con simples radiotelescopios llevaron a la radiointerferometría, desarrollada por los radioastrónomos británico Martin Ryle, al ingeniero, y radiofísico australiano Joseph Lade Pawsey y a Ruby Payne-Scott en 1946. Sorprendentemente, este primer uso de la radiointerferometría para observaciones astronómicas fue llevado a cabo por Payne-Scott, Pawsey y Lindsay McCready el 26 de enero de 1946 usando la radioantena SINGLE convertida en antena radar (arreglo de emisor) a 200 MHz, cerca de Sídney, Australia. Este grupo usó el principio de la interferometría con base al mar en donde su antena (formalmente un radar WWII) observando el sol al amanecer con interferencia, alcanzada por la radiación directa solar y la reflejada desde el mar. Con estas referencias de al menos ondas de 200 m, los autores determinaron que la radiación solar durante la fase de día, siendo mucho más pequeña que el disco solar. Y ese grupo australiano comenzó a trabajar con los principios de la apertura sintética en sus artículos de mediados de 1946 y publicados en 1947. Ese uso del interferómetro de mar fue exitosamente demostrado por numerosos grupos en Australia y en el Reino Unido durante la segunda guerra mundial, quienes observaron refracciones interferométricas (la radiación directa de retornos de radar y la señal reflejada del mar) desde aeronaves.

El grupo de Cambridge de Ryle y Vonberg observaron el sol en 175 MHz a mediados de julio de 1946 con un interferómetro Michelson consistente de dos radioantenas con espaciados desde decenas de metros a 240 metros. Todos mostraron que la radiorradiación era más pequeña que "10 arc min" en tamaño y detectaron una polarización circular del Tipo I bursts. Otros grupos habían detectado también polarización circular al mismo tiempo: David Martyn en Australia y Edward Appleton con J. Stanley Hey en Reino Unido. 

Un moderno radiointerferómetro consiste en radiotelescopios ampliamente separados que observan el mismo objeto y se conectan juntos usando cable coaxial, guía de ondas, fibra óptica, u otro tipo de línea de transmisión. Eso no solo incrementó las recolecciones totales de señales, sino pudo también usarse en el proceso llamado apertura sintética que vastamente incrementó la resolución. Esta técnica trabaja superponiendo las (interferencias) de las señales de ondas de los diferentes telescopios en un principio donde las ondas se hacen coincidir con las mismas fases que añadirán unas a otras mientras dos ondas que tiene fases opuestas se cancelarán entre sí. Así se crea un telescopio combinado que tiene el tamaño de las antenas más apartadas en el arreglo. Para producir una imagen de alta calidad, se requiere un gran número de diferentes separaciones entre diferentes telescopios (separaciones proyectadas entre cualesquieras dos telescopios se llaman línea de base) - y con muchas diferentes líneas de base, como sea posible, se requiere para buenas calidades de imágenes. Por ejemplo el Very Large Array tiene 27 telescopios que dan 351 líneas de base independientes.

A comienzos de los años 1970, se producen mejoras en la estabilidad de los receptores de radiotelescopios permite telescopios en todo el mundo (y aún en órbita terrestre) combinando los Very Long Baseline Interferometry. En vez de conexiones físicas en las antenas, los datos recibidos en cada antena es apareada con información del tiempo, usando un reloj atómico local, y almacenando para posteriores análisis en cinta magnética o en disco duro. En los últimos años, los 
datos se correlacionan con datos de otras antenas similarmente registrados, para producir imágenes. Usando este método es posible sintetizar una antena que tiene efectivamente el tamaño de la Tierra. Las largas distancias entre los telescopios permiten resoluciones de mucha amplitud angular, más grandes que en otros campos de la astronomía. A altísimas frecuencias, es posible que los rayos sintetizados tienen menos de 1 miliarco segundo.

Los arreglos preeminentes VLBI que operan hoy son los Very Long Baseline Array (con telescopios en Norteamérica) y la (telescopios en Europa, China, Sudáfrica, Puerto Rico). Cada arreglo usualmente opera separadamente, y ocasionales proyectos se unen produciendo incrementos en la sensibilidad, y se referencia como "Global VLBI". Hay también una red VLBI: eL "Long Baseline Array", operando en Australia.

Luego de su acopio, los datos registrados en hard media han sido el único modo de desarrollar esos datos de cada telescopio para posteriores correlaciones. Sin embargo, la disponibilidad hoy mundialmente, de redes de fibra ópticas de banda muy ancha hace posible hacer VLBI en tiempo real. Esa técnica (referida como e-VLBI) fue primero usada por EVN (acrónimo en inglés Red Europea VLBI) que actualmente está incrementando el número de científicos en proyectos e-VLBI por año.

La emisión en radio se puede presentar en dos formas: radio continuo y líneas espectrales. En el radio continuo la emisión se extiende en una región ancha del espectro electromagnético mientras que las líneas espectrales se hallan centradas en una frecuencia específica. Estas formas dependen del origen físico de la radiación. 

En las galaxias el radio continuo proviene de tres mecanismos: radiación sincrotrón, emisión libre-libre y emisión térmica. La radiación sincrotrón es emitida en su mayor parte por electrones relativistas confinados en los campos magnéticos de las galaxias. También una parte de esta emisión proviene directamente de los remanentes de supernova, los núcleos de galaxias activas, los púlsars y los microquasares. La emisión libre-libre o bremsstrahlung proviene en su mayor parte de las regiones de formación estelar mientras que, la emisión térmica tiene su origen a estas longitudes de onda en cuerpos relativamente fríos, en su mayoría el polvo del medio interestelar.

A escalas más pequeñas las estrellas más potentes y cercanas pueden ser observadas en radio continuo, en particular nuestro Sol. Y, en escalas mayores la principal emisión en radio continuo es la radiación de fondo de microondas.

Las diferentes especies químicas que se hallan en el universo y en sus objetos emiten o absorben luz en diferentes líneas espectrales, siguiendo las leyes de la mecánica cuántica. En región de radio del espectro electromagnético se suelen encontrar líneas de transición, rotacionales y vibracionales de los átomos y moléculas más comunes en el universo. Estas líneas suelen observarse en emisión pero también pueden observarse en absorción sobre un fondo de radio continuo. Algunas de estas líneas son:


También se observan otras líneas como el NH, OH, HCN, etc, que trazan distintas propiedades físicas y químicas de las distintas regiones y objetos del universo.





</doc>
<doc id="4990" url="https://es.wikipedia.org/wiki?curid=4990" title="Invento">
Invento

Invento o invención (del latín "invenire", "encontrar" -véase también -) es un objeto, técnica o proceso que posee características novedosas y transformadoras. Sin embargo, algunas invenciones también representan una creación innovadora sin antecedentes en la ciencia o la tecnología que amplían los límites del conocimiento humano. 

En ocasiones, se puede obtener protección legal por medio del registro de una patente, siempre que la invención sea realmente novedosa y no resulte obvia. El registro representa una concesión temporal por parte del Estado para la explotación de la patente, conformándose en la práctica un monopolio que limita la competencia. No obstante, dicho registro no necesariamente es indicativo de la autoría de la invención.

Los primeros inventos datan de la prehistoria y fueron elementos realizados en piedra, toscos y rústicos, que fueron evolucionando a través de los tiempos. En la opinión de muchos autores el mayor, antes de la documentación histórica, es el sistema de signos para comunicarse: el lenguaje. En opinión de otros el lenguaje, no es un invento, sino una forma de expresión natural, entendiendo por expresión como el uso de una capacidad innata. Los perros se comunican entre sí, los delfines y los insectos... nuestra incapacidad de comprender dichos lenguajes, no es motivo para descalificarlos como tal, pues es autodemostrativo que se comunican usando su propio lenguaje, si el lenguaje se considera un invento, entonces muchos autores consideran que también se debe considerar inventores a todas las especies de animales que tienen lenguaje sea este más o menos tosco o limitado.

En lo que todo el mundo parece estar de acuerdo es en que los primeros inventos fueron los utensilios para el procesado de comida, la caza , la salud y la medicina. La ropa y por supuesto la rueda que posiblemente sea el invento más sustancial en la prehistoria desde el punto de vista de la tecnología.

Si bien un objeto o método innovador y útil se puede desarrollar para satisfacer un propósito específico, la idea original puede que nunca se realice como invención de trabajo, quizás porque el concepto sea de cierta manera poco realista o impráctica. Como a "castillos en el aire" se puede referir a una idea creativa que no alcance su objetivo debido a consideraciones prácticas. La historia de la invención está llena de tales casos, pues las invenciones no surgen necesariamente en el orden que sea más útil. Por ejemplo, el diseño del paracaídas fue resuelto mucho antes de la invención del vuelo autónomo. Otros inventos simplemente solucionan problemas para los cuales no hay incentivo económico que proporcione una solución.

Por otra parte, cualquier barrera a la puesta en práctica puede simplemente ser adjudicada a limitaciones de la ingeniería o la tecnología que eventualmente se pudiesen superar a través de avances científicos. La historia está también repleta de ejemplos de ideas que han llevado un cierto tiempo para hacerse realidad física, según lo demostrado por las varias ideas atribuidas originalmente a Leonardo da Vinci y a las que ahora se ve su aplicación diaria en forma práctica.




</doc>
<doc id="4996" url="https://es.wikipedia.org/wiki?curid=4996" title="Prolepsis">
Prolepsis

Prolepsis (del griego "prolambanein", anticipación) puede referirse a:



</doc>
<doc id="4998" url="https://es.wikipedia.org/wiki?curid=4998" title="Zenón de Elea">
Zenón de Elea

Zenón de Elea (en griego clásico: Ζήνων ὁ Ελεάτης) fue un filósofo griego nacido en Elea, perteneciente a la escuela eleática (). Fue discípulo directo de Parménides de Elea. No estableció ni conformó ninguna doctrina positiva de su propia mano. Famoso por sus intrincadas paradojas que discuten la pluralidad de entes y en algunos casos el movimiento —entre otras cosas—. Aristóteles lo llamó el inventor de la dialéctica y el matemático Bertrand Russell le describió como "inmensamente sutil y profundo".

Como sucede con la mayoría de los filósofos presocráticos, la vida de Zenón de Elea permanece en gran parte desconocida. 

Se piensa que pasó toda su vida en Elea y se ocupó de la educación de los hombres en la virtud. En principio fue pitagórico y Estrabón le atribuyó una actividad política. En el único contexto en el que es citado varias veces, es un relato de su participación en un complot contra un tirano, hay tanta divergencia de detalles que es imposible reconstruir lo sucedido.

Las fuentes que brindan luz al respecto son el diálogo Parménides de Platón, la obra Vida de los filósofos ilustres del historiador y filósofo antiguo Diógenes Laercio y Física de Aristóteles.

En el diálogo de Platón, se dice que Zenón tiene cerca de 40 años y que Parménides roza los 65 en el momento en que ambos se encuentran con un Sócrates "muy joven"; dato que nos puede servir para situar su nacimiento alrededor del año 480 o 490 a. C. Platón lo describe como "alto y bello a la mirada", así como estimado por su maestro.

Diógenes Laercio indica que fue hijo natural de un hombre llamado Telentágoras, pero que Parménides lo tomó en adopción. Laercio subraya así mismo su destreza a la hora de analizar los dos lados de cada cuestión o dilema, capacidad que le hizo recibir el título de "inventor de la dialéctica" de la mano de Aristóteles.

Como su maestro, tuvo probablemente una gran actividad política: el mismo Laercio afirma que Zenón apoyaba el derrocamiento del tirano eleata que gobernaba, bajo peligro de muerte:

Laercio no concreta la identidad del tirano, pues indica que podría tratarse tanto de Nearco como de Diomedón, dando además dos finales posibles a la historia: en uno el tirano es finalmente lapidado por el pueblo que se rebela y en otro es Zenón quien resulta ejecutado. Tertuliano informa varios siglos más tarde sobre la muerte de Zenón:

En el pasaje Tertuliano se equivoca de tirano, ya que es imposible que Zenón, filósofo nacido en el siglo V a.C. fuese torturado por Dionisio I, tirano de Siracusa un siglo más tarde.

Las obras de Zenón se han perdido. Platón escribe que durante su juventud ya había escrito para defender las teorías de su maestro, pues tales documentos fueron llevados a Atenas con motivo de la visita con Parménides; fueron allí robados y publicados posteriormente sin su consentimiento.

Como es habitual en el ámbito presocrático, la mayor y casi única fuente de la que podemos extraer información sobre su obra y pensamiento es la cita de autores posteriores, en particular del propio Aristóteles.

Del citado escrito de Platón, se infiere que lo escrito por Zenón constituía un volumen que consistía en una colección de argumentos, cada uno de los cuales atentaban contra tesis de la pluralidad de entes; sin embargo, en los fragmentos citados, Zenón no los muestra ni como antinomias ni como dirigidos explícitamente contra la pluralidad de entes. Más que nada, las paradojas de movimiento que vemos en los escritos de Aristóteles.

En resumidas cuentas no sabemos con exactitud cuál fue el principio de organización que siguió Zenón para la ordenación de sus argumentos. Los estudiosos G. S. Kirk, J. E. Raven y M. Schofield, ensayan tres posibilidades:

A: Zenón escribió una obra, de la manera en la que la describe Platón, pero Zenón escribió al menos otra obra, que incluía otras paradojas que no atacaban la pluralidad de las cosas.

B: El filósofo escribió solo un libro, pero Platón no describe bien sus argumentos ni su forma.

C: Zenón escribió una sola obra, y todas las paradojas, en su origen arremetían contra la pluralidad de entes, y han sido deformadas por los diferentes autores y su interpretación a lo largo del tiempo.

No se sabe con certeza si la obra de Zenón precedió e influyó la filosofía de Meliso y Anaxágoras o inversamente. Es evidente influyó fuertemente sobre el atomismo de Leucipo y Demócrito. La curiosa obra de Gorgias.

Cuando Protágoras intercede por la construcción de argumentos contradictorios sobre cualquier materia, es evidente que obtuvo inspiración en Zenón.

El interés filosófico de Platón por Zenón quedó plasmado en sus antinomias que colman las treinta últimas páginas del Parménides con razonamientos en torno al movimiento, el lugar y el tiempo, los cuales estimularían luego a Aristóteles...

En nuestra época es cuando con mayor intensidad se han discutido -y discuten- las paradojas de Zenón, quien es el presocrático que ejerce más influjo. 

Zenón, en la línea de su maestro, intenta probar que el ser tiene que ser homogéneo, único y, en consecuencia, que el espacio no está formado por elementos discontinuos sino que el cosmos o universo entero es una única unidad.

Sus aporías están diseñadas bajo los siguientes ejes argumentativos:

Aplicando este esquema se le ha considerado el primero en utilizar la demostración llamada ad absurdum (reducción al absurdo), que toma por hipótesis lo contrario de lo que se considera cierto (en su caso, las afirmaciones del adversario) y muestra las incongruencias que se derivan de una consideración de esto como verdadero, obligando al interlocutor a rechazar las premisas y a aceptar las tesis opuestas, que eran las que se querían demostrar en un principio. Este procedimiento lo lleva a cabo mediante sus aporías.

Como ejemplo de (1) tenemos en Fr. 3, Simplicio, "Fís"., donde probablemente argumenta que, para que una cosa sea una y no otra, entre ambos miembros dentro de un conjunto, debe haber algo que los separe. Podríamos refutar alegando que el enunciado es válido solo si se aplicase a conjuntos sistemáticamente ordenados -como series de puntos- y que en la realidad, siendo objetos en tres dimensiones se comportan de una manera muy distinta, pero Zenón bien podría seguir “indagando” sobre esto, y haciéndonos preguntas que no podríamos responder hasta que hayamos resuelto mediante una reflexión filosófica qué es lo que hace a una cosa ella misma y no otra ni muchas.

Aquiles y la tortuga, Aristóteles, "Fís.," es un claro ejemplo de (3), pero también nos crea dificultades en (2) y (4); consiste en una carrera entre Aquiles -el de los pies ligeros- contra una tortuga, pero empezando la tortuga desde una distancia más adelantada…

El problema consiste en mostrar cómo es imposible que el corredor más rápido adelante al más lento, pues para poder siquiera alcanzar Aquiles a la tortuga debería antes pasar por la mitad de la distancia de ese recorrido, y para alcanzar esa distancia, antes debería alcanzar la mitad de ese recorrido, y así infinitamente, en una serie de números que convergen hacia cero, donde no solo plantea un problema de cómo medimos y percibimos el tiempo y el espacio, sino que hace racionalmente imposible la idea de movimiento.

Los razonamientos de Zenón constituyen el testimonio más antiguo que se conserva del pensamiento infinitesimal desarrollado muchos siglos después en la aplicación del cálculo infinitesimal que nacerá de la mano de Leibniz y Newton en 1666. No obstante, Zenón era ajeno a toda posible matematización, presentando una conceptualización de tal estilo como un instrumento necesario para poder formular sus paradojas.







</doc>
<doc id="4999" url="https://es.wikipedia.org/wiki?curid=4999" title="John Maynard Keynes">
John Maynard Keynes

John Maynard Keynes (5 de junio de 1883 – 21 de abril de 1946) fue un economista británico, considerado como uno de los más influyentes del . Sus ideas tuvieron una fuerte repercusión en las teorías y políticas económicas.

La principal novedad de su pensamiento radicaba en considerar que el sistema capitalista no tiende al pleno empleo ni al equilibrio de los factores productivos, sino hacia un equilibrio que solo de forma accidental coincidirá con el pleno empleo. Keynes y sus seguidores de la posguerra destacaron no solo el carácter ascendente de la oferta agregada, en contraposición con la visión clásica, sino además la inestabilidad de la demanda agregada, proveniente de los "shocks" ocurridos en mercados privados, como consecuencia de los altibajos en la confianza de los inversores. La principal conclusión de su análisis es una apuesta por la intervención pública directa en materia de gasto público, que permite cubrir la brecha o déficit de la demanda agregada. Está considerado también como uno de los fundadores de la macroeconomía moderna.

Keynes fue un personaje muy polifacético. Además de ser un economista teórico que cambió la consideración de la macroeconomía en el , desempeñó también múltiples puestos en el mundo económico, fue profesor en la Universidad de Cambridge desde 1908, editor del "Economic Journal" desde 1912, secretario de la Royal Economic Society, alto funcionario de la Administración británica y negociador internacional en nombre de Inglaterra en diferentes ocasiones. También trabajó en el sector empresarial, en la dirección de inversiones de una compañía de seguros y de asesor financiero del King's College, del Banco de Inglaterra y del propio gobierno británico. Dentro también del mundo de la economía, fue gran aficionado a la historia económica y biógrafo de grandes economistas. Fuera del mundo económico, durante sus estudios en la Universidad de Cambridge se interesó por las matemáticas, estadística, filosofía, literatura y solo finalmente por la economía. Fue también director y principal accionista del Teatro de las Artes de Cambridge y mecenas del grupo de Bloomsbury, coleccionista de pintura moderna y bibliófilo de literatura científica. Fue primer barón Keynes.

John Maynard Keynes nació el 5 de junio de 1883 en la ciudad de Cambridge, en una familia acomodada de reputado nivel cultural. Sus padres eran John Neville Keynes, profesor de economía y filosofía en la Universidad de Cambridge y Florence Ada Brown, una de las primeras mujeres que logró estudiar en las universidades británicas, autora y precursora de la asistencia social que llegó a ser alcaldesa de Cambridge en 1932. Su hermana Margaret (1885-1974) contrajo matrimonio con el ganador del premio Nobel en fisiología Archibald Hill y su hermano menor Geoffrey Keynes (1887-1982) fue cirujano y gran bibliófilo.

En 1897 obtuvo una beca para estudiar en el Colegio Eton. En 1902 ingresó en el King's College en la Universidad de Cambridge, donde estudió matemáticas y teoría de probabilidades, orientándose luego hacia la economía por consejo de sus maestros Alfred Marshall y Arthur Pigou. 

Durante su estancia como estudiante universitario en Cambridge, frecuentó el grupo de los llamados Apóstoles y de ahí pasó al denominado Círculo de Bloomsbury, grupo intelectual y artístico que proponía un nuevo orden social, contrario a los principios morales victorianos y del que formaban parte, entre otros, el escritor Lytton Strachey, Leonard y Virginia Woolf y el pintor Duncan Grant, con quien Keynes mantuvo una relación durante varios años. 

La fascinación homoerótica de Keynes por Sergéi Diaghilev le llevó a conocer en octubre de 1918 a Lidia Lopujova, una bailarina rusa de su compañía, con quien contrajo matrimonio luego de un viaje por Taiwán.

Tras un breve periodo trabajando en el servicio administrativo británico para la India, en 1909 entró como profesor en el King's College de Cambridge, donde enseñaría economía hasta su muerte.

En 1906 fue nombrado funcionario público del Home Civil Service. Fue destinado a la Indian Office, donde acumuló un profundo conocimiento del sistema financiero indio. Producto de su estancia en este departamento escribió en 1913 "Moneda y finanzas en la India". Después de considerar decepcionante su trabajo en esta oficina, en junio de 1908 renunció a su puesto para trabajar en la Universidad de Cambridge en teoría de probabilidades. En 1909 consiguió una plaza de profesor en la cátedra de Pigou de esta universidad. En 1911 fue nombrado editor de la revista económica "Economics Journal" y durante los años 1913 y 1914 fue miembro de la Royal Commission on Indian Finance and Currency.

En 1916 comenzó a trabajar como consejero del Ministerio de Hacienda británico (HM Treasury). Entre sus responsabilidades se encontraba el diseño de los contratos crediticios entre el Reino Unido y sus aliados continentales durante la guerra, y los sistemas de adquisiciones exteriores. En 1919, tras finalizar la primera guerra mundial, formó parte de la delegación británica en la Conferencia de Paz de París, puesto del que dimitió ese mismo año por estar disconforme con el régimen abusivo de indemnizaciones y reparaciones que se imponían a Alemania, que consideraba una «Paz cartaginesa». Sobre este tema escribió, en 1919, el libro "Las consecuencias económicas de la paz", en el que anunciaba las implicaciones y consecuencias de las condiciones económicas impuestas a Alemania en el Tratado de Versalles.

En 1919, regresó a la Universidad de Cambridge, pero compartía su tiempo entre sus estancias en el domicilio familiar de la calle Harvey en Cambridge y el 46 de Gordon Square en Londres, ciudad donde ejercía un intenso conjunto de actividades. Fue miembro de varios consejos de administración de empresas financieras y aseguradoras, dirigía el semanario Nation and Athenaeum y Economic Journal y participaba en el Consejo asesor económico del primer ministro británico. Keynes también fue un inversor de éxito, logrando hacerse con una gran fortuna (después de afianzarse como economista, ya que en su juventud sufrió grandes pérdidas que debieron ser cubiertas por su padre). Durante el Crack del 29 casi queda en bancarrota, pero pudo recuperarse en poco tiempo. Se le considera un gran bibliófilo, en especial por lo que respecta a las ediciones originales de las obras de Isaac Newton. Estaba interesado en la literatura en general y el drama en particular. Se constituyó en empresario del Teatro de Artes de Cambridge, labor en la que puso gran entusiasmo (cierto día que el portero estaba ausente se le pudo ver a él mismo cortando las entradas en el hall). Gracias a esto, la institución se convirtió durante un tiempo en el más importante escenario británico fuera de Londres. En 1925 contrajo matrimonio con la bailarina rusa Lidia Lopujova.

Bertrand Russell dijo de Keynes: «Es la mente más aguda y más clara que jamás conocí. Cuando discutía con él, sentía que mi vida pendía de un hilo y raramente terminaba sintiéndome algo muy diferente a un estúpido». Otro conocido comentario que Keynes hizo a su mujer fue que había «encontrado a Dios en el tren de las 5:15» cuando recibía a Ludwig Wittgenstein, el protegido de Russell, para su estancia en Cambridge.

Keynes murió el 21 de abril de 1946 a causa de un infarto. Sus problemas cardíacos se agravaron por la presión de su trabajo en los problemas financieros internacionales de la posguerra. El hermano de Keynes, "Sir" Geoffrey Keynes (1887-1982), fue un distinguido cirujano, profesor universitario y bibliófilo. Sus sobrinos fueron el fisiólogo Richard Keynes (1919-2010) y Quentin Keynes (1921-2003), aventurero y bibliófilo.

Keynes publicó su "Tratado sobre la probabilidad" en 1920, una contribución a las bases matemáticas y filosóficas de la teoría de la probabilidad.

Atacó las políticas deflacionarias de los años 20 en un "Tratado sobre la reforma monetaria" de 1923, una argumentación sobre por qué los países deberían apuntar a la estabilidad de los precios domésticos al tiempo de proponer el uso de tipos de cambio flexibles. En el "Tratado sobre el dinero" de 1930 (en dos volúmenes) expone su teoría de tipo Wickselliana sobre ciclo de crédito.

Su obra central, la "Teoría general del empleo, el interés y el dinero", desafió el paradigma económico dominante cuando se publicó en 1936. En este libro, Keynes presenta una teoría basada en la noción de demanda agregada para explicar las variación general de actividad económica, tales como las observadas durante la Gran Depresión. Según su teoría, el ingreso total de la sociedad está definido por la suma del consumo y la inversión; y en una situación de desempleo y capacidad productiva no utilizada, «solamente» pueden aumentarse el empleo y el ingreso total incrementando primero los gastos, sea en consumo o en inversión.

La cantidad total de ahorro en la sociedad es determinada por el ingreso total y, por tanto, la economía podría alcanzar un incremento del ahorro total, aun si las tasas de interés se bajaran para estimular los gastos en inversión. El libro abogaba por políticas económicas activas por parte del gobierno para estimular la demanda en tiempos de elevado desempleo, por ejemplo a través de gastos en obras públicas. El libro se considera a menudo como el fundador de la macroeconomía moderna. Los historiadores concuerdan en que Keynes influyó en el New Deal del presidente estadounidense Franklin Delano Roosevelt, pero discuten aún sobre el grado de dicha influencia. Una política de gasto deficitario como la emprendida en el New Deal comenzó en 1938, que había sido llamada "pump priming" en 1932 por el presidente Herbert Hoover. Pocos economistas renombrados en los Estados Unidos comulgaron con las ideas de Keynes durante los años 30. Con el tiempo, sin embargo, sus ideas fueron más ampliamente aceptadas.

En 1942, Keynes era ya un economista ampliamente reconocido, como evidencia su admisión en la Cámara de los Lores con el título de barón Keynes de Tilton en el Condado de Sussex, ubicándose en la bancada del Partido Liberal. Durante la Segunda Guerra Mundial, Keynes argumentó en "¿Cómo pagar la guerra?" (1940) que el esfuerzo bélico debería financiarse mayoritariamente mediante el aumento de colonias en África y por mayores impuestos, en lugar de gasto deficitario, para de esa manera evitar la inflación. Es claro que este algoritmo tendría sus ventajas para Inglaterra, no así para los países a los que les tocara en suerte ser "colonias" (William R. Catton Jr. Overshoot ("Sobrecarga"), 1980). A medida que la victoria aliada parecía más segura, Keynes estuvo muy involucrado en las negociaciones que establecieron el sistema Bretton Woods, en su papel de líder de la delegación británica y presidente de la comisión del Banco Mundial. El plan de Keynes, referente a una Unión Internacional de Compensación propuesta para un sistema de administración de divisas, involucraba un banco central mundial que sería responsable de una unidad mundial única de cambio, el Bancor. Sin embargo, el peso de los EE. UU. en las negociaciones fue determinante para que el resultado final estuviera más acorde a los planes de Harry Dexter White, estableciéndose el uso del dólar estadounidense como moneda de reserva, dando inicio a su presencia dominante en las finanzas globales.

Keynes escribió "Ensayos en biografía" y "Ensayos en persuasión", el primero aportando retratos de economistas y otras personas notables, mientras que el segundo presenta algunos de los intentos del autor de influir en los formadores de políticas durante la Gran Depresión. Keynes fue editor jefe para el "Economic Journal" desde 1912.

La brillante actuación de Keynes como inversor bursátil está demostrada por la información pública del fondo que administró en nombre del King's College de Cambridge.

Desde 1928 a 1945, a pesar de recibir un gran golpe durante el Crack de Wall Street de 1929, el valor del fondo de Keynes mostró un considerable crecimiento anual promedio de 13,2%, comparado con un nivel general en el mercado del Reino Unido que se redujo en un promedio del 0,5% anual.

El enfoque generalmente adoptado por Keynes con sus inversiones lo resumió él mismo en:


Keynes argumentó que:

En su obra principal, "Teoría general del empleo, el interés y el dinero", Keynes escribió sus opiniones en lo referente al empleo, teoría monetaria, y el ciclo de comercio, entre otros temas. Su obra dedicada al empleo se oponía a todo lo que los economistas clásicos habían enseñado. Keynes decía que la causa real del desempleo era el insuficiente gasto en inversión. Creía que la cantidad de trabajo entregada es diferente cuando el decremento en los salarios reales (el producto marginal del trabajo) se debe al decremento del salario monetario, que en el caso cuando se debe a un incremento del nivel de precios, asumiendo que el salario monetario se mantenga constante.

Se puede sintetizar su aporte en el concepto de que cuando la demanda deviene transitoriamente más pequeña, ello puede tener como consecuencia, en determinados contextos institucionales, el que la oferta también se contraiga; con lo que resultaría un nuevo equilibrio del mercado, pero habiendo perdido el mercado mismo cierta magnitud entre ambos momentos. 

En su teoría, el desencadenante de esos movimientos en la demanda y la oferta es el mercado de capital. La demanda de capital transitoriamente deviene menor, a partir de lo cual la oferta de capital le sigue mímicamente a la baja, en vez de mantenerse transitoriamente o aumentar transitoriamente. 

Al resolverse ambos movimientos, el de la demanda de capital y el de la oferta de capital, ambos a la baja, el mercado como un todo vuelve a un nuevo equilibrio. Pero en éste, la cantidad de capital aplicado será menor que antes, por lo cual la nueva proporción resultante entre los demás factores de producción —trabajo y recursos— y el capital últimamente en el mercado, se alterará. Al reducirse o retenerse parte del capital o ahorro de antaño, una parte de los otros dos factores resultará excedente y no podrá más que quedar fuera del mercado; se realiza como un creciente stock involuntario de estos otros dos factores. Todo esto sucede en el contexto de cierta inflexibilidad en la información que se disemina y comunica, a partir de un marco institucional dado; que queda más o menos anacrónico o extemporáneo a los giros en el mercado de capital, que desencadenan luego el desempleo o la formación involuntaria de stocks de factores.

En su "Teoría del dinero", Keynes dijo que los ahorros e inversión estaban determinados en forma independiente. La cantidad destinada a ahorro tenía poco que ver con las variaciones en las tasas de interés, que a su vez tenían poco que ver con cuanto se destinaba a inversión. Keynes pensó que los cambios en la cantidad destinada a ahorro dependían en la predisposición para consumir que resultaba de cambios incrementales, marginales, al ingreso. Por tanto, la cantidad destinada a inversión estaba determinada por la relación entre la tasa esperada de retorno sobre la inversión y la tasa de interés.

Las teorías de Keynes fueron tan influyentes, aun siendo disputadas, que hoy en día todo un subcampo de la macroeconomía llamada economía keynesiana continúa desarrollando y discutiendo sus teorías y sus aplicaciones. John Maynard Keynes se interesó en diversos campos de la cultura y fue una figura central del llamado grupo de Bloomsbury, conformado por prominentes artistas y escritores del Reino Unido. Sus ensayos autobiográficos "Two Memoirs" se publicaron en 1949.

Su obra de 1930 "Tratado sobre el dinero" ("Treatise on Money") en dos volúmenes fue vista como el mejor trabajo de Keynes por uno de sus más frecuentes oponentes intelectuales, Milton Friedman. Friedman y otros monetaristas han argumentado que los economistas keynesianos no prestan suficiente atención a la estanflación.







</doc>
<doc id="5000" url="https://es.wikipedia.org/wiki?curid=5000" title="Aguascalientes">
Aguascalientes

Aguascalientes (), oficialmente llamado Estado Libre y Soberano de Aguascalientes, es uno de los treinta y un estados que, junto con la Ciudad de México, forman México. Su territorio, bajo jurisdicción de la ciudad homónima y siendo el núcleo de su formación, estuvo siempre ligada a Zacatecas, primero siendo esta, provincia del Reino de Nueva Galicia entre 1575 y 1786; y posteriormente como Intendencia (1786-1821), y finalmente como Estado o Departamento (1821-1857), hasta su separación de la misma y la erección como entidad federativa en 1857. Su capital y ciudad más poblada es Aguascalientes.

Colinda al norte con Zacatecas y al sur con Jalisco. Tiene un territorio de 5,618 km². Es el , con el 0.3% de la superficie de total del país. Se divide en . Tiene una población de 1,513,544 hab. (estimación del 2015), concentrada preponderantemente en la zona metropolitana de Aguascalientes, que abarca tres de sus municipios. Es el y el con 210.93 hab/km².

La presencia humana en el territorio data del 20000 a.C. Posteriormente, habitaron las civilizaciones chichimecas, sobre todo las naciones caxcan, guamare y los chichimecas blancos. El primer intento de conquista española data de 1531. En el Virreinato de la Nueva España el territorio actual que conforma Aguascalientes pertenecía a la Provincia de Nueva Galicia. Tras la Independencia de México, el 23 de mayo de 1835 se creó el Territorio de Aguascalientes por orden de Antonio López de Santa Anna, separándolo del Estado de Zacatecas. El 5 de febrero de 1857, con la promulgación de la Constitución de 1857 fue elevado a entidad federativa como el vigésimo cuarto estado de la federación. Durante el Porfiriato experimentó una transformación económica, social y artística desde entidad agropecuaria a polo industrial, debido a la presencia de los Talleres Generales del Ferrocarril Central Mexicano. Su capital fue la sede de la Soberana Convención Revolucionaria de 1914. Actualmente alberga al Instituto Nacional de Estadística y Geografía, así como las fábricas más importantes de Nissan Motor Company en Latinoamérica. La capital también es conocida por su Feria Nacional de San Marcos.

Tiene un índice de desarrollo humano de 0.798, el . Su PIB nominal es de $169,885/hab. (preliminar de 2015), siendo el y su PIB per cápita es de $130,102/hab. (estimación de 2015), siendo el .

Algunos sitios en el estado forman parte del Camino Real de Tierra Adentro, patrimonio de la humanidad de la UNESCO. Específicamente, la Antigua Hacienda de Peñuelas, la Antigua Hacienda de Cieneguilla, el conjunto histórico de la ciudad de Aguascalientes y la Antigua Hacienda de Pabellón de Hidalgo.

El nombre de “Aguascalientes” es debido a su capital homónima, nombrada así por las aguas termales que los primeros pobladores españoles descubrieron en la zona.

Pedro Almíndez Chirino fue el primer español que se internó en este territorio, tal vez a finales de 1530 o principios de 1531, en cumplimiento de instrucciones dadas por Nuño de Guzmán.

Las bélicas tribus chichimecas hicieron el acceso y tránsito por la zona especialmente difícil a los conquistadores. De hecho, la total ocupación de las tierras del Bajío fue una labor que tardaría alrededor de dos siglos. Al respecto, el Virrey Luis de Velasco ofrecía beneficios municipales a quienes establecían poblados para hacer frente a los chichimecas. Y por su parte, el virrey Gastón de Peralta decidió enfrentarlos de forma directa, lo que no le dio buenos resultados.

Se edificaron algunos fuertes o presidios a raíz de la interacción con los denominados chichimecas guachichiles. Sistema que ideó Martín Enríquez de Almansa siguiendo la estrategia que se había venido desarrollando en España durante todo el periodo de la Reconquista. Ello, para proteger el Camino de la Plata, que se extendía entre Zacatecas y México, creándose así los tres presidios, fundados por el combatiente de indios Juan Domínguez, que son: el de las Bocas, más tarde denominado de las Bocas de Gallardo, situado en la frontera de Aguascalientes, en lo que era la jurisdicción de la alcaldía mayor de Teocaltiche, actualmente frontera de Aguascalientes y Zacatecas; el de Palmillas, que estuvo ubicado cerca de lo que ahora es Tepezalá; y el de Ciénega Grande, este último fundado hacia el año de 1570, y que estuvo ubicado en lo que ahora son las calles de Moctezuma y Victoria, aunque algunos historiadores lo ubican en la calle 5 de Mayo (otrora Camino Real) y Moctezuma, precisamente frente a la Plaza de Armas. Presidio que tenía como finalidad la protección del Valle de los Romero y el camino a Zacatecas, entrando así a asegurar el paso a los convoyes cargados de plata y otros metales.

La fundación de Aguascalientes como villa devino de la orden que el rey Felipe II diera al oidor de la audiencia de la Nueva Galicia, Gerónimo de Orozco, en la que estableció que debería de buscar un hombre rico que se asentara en el territorio, con la finalidad de expulsar a los chichimecas y asegurar el paso seguro. Gerónimo de Orozco, en ejecución de dicha orden, buscó a alguien que aceptase el encargo del rey, encontrando en la ciudad de Santa María de los Lagos a un hombre de nombre Juan de Montoro, quien aceptó el encargo y se dirigió al territorio, acompañado de otras once personas, fundándose así el 22 de octubre de 1575 la Villa de las Aguas Calientes. Se ha señalado que originalmente se denominó como San Marcos, cambiando de nombre a partir del 18 de agosto de 1611 por el de Villa de Nuestra Señora de la Asunción de las Aguas Calientes. Y, finalmente, a partir del 2 de junio de 1875, se le denominó Villa de Nuestra Señora de la Asunción de las Aguas Calientes; cambiando posteriormente por ciudad de Aguascalientes, nombre que conserva hasta la actualidad.

En el mismo acto de su erección, a la Villa de San Marcos (Aguascalientes) le fue adjudicada la jurisdicción de alcaldía mayor dependiente del Reino de Nueva Galicia. A partir del 4 de diciembre de 1786, con motivo de la expedición de la “Ordenanza de Intendentes”, se transformó en subdelegación de intendencia.

El 24 de abril de 1789, por disposición de la Junta Superior de Real Hacienda, la subdelegación de Aguascalientes pasó a depender de Zacatecas.

Tras la llegada de Cortés y la caída de Tenochtitlán en 1521, la conquista llevó a muchos españoles aventureros a avanzar hacia el norte en busca de fortuna. Estas tierras, además de ser más áridas que las que habían encontrado en Veracruz y en el Valle de México, estaban habitadas por indígenas que en su mayoría eran nómadas y que pronto se convirtieron en el terror de los todavía mal trazados caminos que conectaban a la Nueva España con esta otra parte del territorio, al cual se le dio el nombre de la Nueva Galicia.

La audiencia de la Nueva Galicia –institución encargada del gobierno y la administración de los nuevos territorios— se asentó en Guadalajara, al tiempo que se descubrían yacimientos de plata en el cerro de la Bufa, donde no tardarían en aparecer las minas de Zacatecas. Estas minas llegaron a ser el motor económico de la Nueva Galicia y su explotación hizo necesario abrir caminos no solamente entre Guadalajara y Zacatecas, sino también entre las minas y la ciudad de México, Querétaro y Michoacán, a donde se llevaba buena parte de la plata extraída.

Desde el principio los caminos fueron asaltados por grupos de chichimecas (que así se les llamaba a los indios de la frontera norte de la Nueva España) y debido a la constancia de los ataques, comenzó una guerra que se extendió durante toda la segunda mitad del siglo XVI. Por esta razón los españoles establecieron villas a lo largo de los caminos. Estas eran a la vez puestos fortificados y lugares de descanso para los viajeros. De esta manera nació Santa María de los Lagos en 1563 y algunos años más tarde salieron de allí los colonos que fundarían la ciudad de Aguascalientes. Estos pobladores eran gente de origen más bien humilde, que salieron de la villa de Lagos (hoy Lagos de Moreno) debido a los abusos de las autoridades locales y también por el deseo que tenían de obtener nuevas mercedes de tierra.

Se ha creado confusión respecto a la fecha exacta en que Aguascalientes se separó formalmente del territorio de Zacatecas. Ello, en virtud de que, de facto, después de derrotar al gobierno liberal de Zacatecas por levantarse contra el gobierno central, el presidente Antonio López de Santa Anna pasó por Aguascalientes, donde fue bien recibido por el pueblo quien hacía tiempo deseaba separarse de Zacatecas. El 26 de marzo de 1835, se promulgó una ley para disminuir las milicias estatales. Esto resultó en la oposición del gobierno de Zacatecas y el estado decretó medidas para asumir la resistencia. El exgobernador de Zacatecas y comandante de las milicias estatales Francisco García Salinas encabezó un ejército de unos cuatro mil hombres en contra del gobierno. El general Antonio López de Santa Anna fue autorizado para dirigir al ejército federal en la campaña contra Zacatecas. Llegó a la ciudad el viernes 1 de mayo donde las calles se limpiaron y las casas se adornaron para recibirlo. Las autoridades lo condujeron al templo donde se llevó a cabo un "Te Deum" en su honor y después lo llevaron al alojamiento que se le había preparado en la casa de Don Pedro García Rojas. Al día siguiente, fue depuesto el jefe político zacatecano, José María Sandoval, y nombrado por el cabildo para sustituirlo Don Pedro García Rojas.

Según a la leyenda de la libertad por un beso, fue gracias a la intervención de doña María Luisa Fernández Villa de García Rojas (esposa de Don Pedro García Rojas) que Santa Anna pronunció que Aguascalientes ya no pertenecería a Zacatecas durante una cena en la casa de los García Rojas . Días después, Francisco García Salinas fue sorprendido por el general Santa Anna el 11 de mayo y derrotado ahí mismo en el campo de Guadalupe durante la Batalla de Zacatecas. Aprovechando este los ánimos independentistas de los aguascalentenses, y a modo de castigo para Zacatecas por apoyar la causa de la Revolución en su contra, por Decreto Federal del General Antonio López de Santa Anna fechado el 23 de mayo de 1835, dentro de su artículo tercero, ordenó que Aguascalientes continuase separado del territorio de Zacatecas, sin otorgarle al territorio ninguna categoría específica, recayendo el nombramiento de jefe político, en Pedro García Rojas.

Al respecto, hay que mencionar que dicho decreto no se formalizó según los requisitos legales para el efecto, ya que era necesario para ello que las dos terceras partes de las cámaras, tanto de Senadores como de Diputados aprobasen el decreto, además de que se requería que las dos terceras partes de las legislaturas de los estados también lo aprobaran, no cumpliéndose el segundo requisito, toda vez que se convocó a congreso constituyente para elaborar la constitución centralista que se conoció más tarde como las Siete Leyes; constitución que no reconoció a Aguascalientes el rango de departamento, ya que cabe mencionar que se eliminaron los estados, junto con el régimen federal, sustituyendo a los Estados por Departamentos, por lo que continuó perteneciendo a Zacatecas; lo que se puede constatar, ya que en la constitución local de Zacatecas de 1825, se contempló a Aguascalientes como integrante de dicho Estado.

Fue el general José Mariano Salas, quien el 5 de agosto de 1846, se pronunció por restablecer el federalismo, convocando un congreso constituyente que declaró vigente la constitución de 1824, pero aún no se consideró a Aguascalientes como estado. Posteriormente, el 18 de mayo de 1847 se aprobó el acta de reformas a la Constitución de 1824, pero tampoco se le concedió a Aguascalientes el rango de estado, lo que propició una guerra entre Aguascalientes y Zacatecas, trayendo como consecuencia que Zacatecas se hiciera a la fuerza de los Partidos, ahora municipios, de Calvillo y Rincón de Romos. En julio de 1848, Aguascalientes aceptó anexarse pacíficamente a Zacatecas, pero continuó por medio de Miguel García Rojas, haciendo gestiones para separarse. Fue hasta el 10 de diciembre de 1853 que López de Santa Anna, en uso de sus facultades extraordinarias, expidió un decreto declarando a Aguascalientes como departamento, tomando como base para ello los decretos del 30 de diciembre de 1836 y del 30 de junio de 1838, sin nunca hacer referencia al del 23 de marzo de 1835. Finalmente, en el proyecto de lo que sería la Constitución de 1857, que fue presentado el 16 de junio de 1856, se incluyó a Aguascalientes como estado en el artículo 43, aprobándose por unanimidad de los 79 diputados presentes la subsistencia del estado de Aguacalientes, el 10 de diciembre de 1856, entrando en vigor dicha constitución el 16 de septiembre de 1857, recayendo el cargo de gobernador constitucional del Estado en el Lic. Jesús Terán Peredo.

Durante el Segundo Imperio Mexicano del archiduque de Austria Maximiliano, los Altos de Jalisco forman parte del Departamento de Aguascalientes, como una continuación natural, tomando en cuenta aspectos culturales y geográficos.

Las divisiones territoriales a través de la historia de México, generalmente han estado ligadas a cambios políticos y no a una distribución espacial tendiente a mejorar el desarrollo administrativo, económico y social del territorio nacional. El 3 de marzo de 1865 apareció uno de los decretos más importantes del gobierno de Maximiliano para la primera división del territorio del nuevo Imperio y que fue publicado en el Diario del Imperio el 13 de marzo del mismo año. Dicha misión le fue encomendada a don Manuel Orozco y Berra (1816-1881) y esta división fue realizada según las bases siguientes:


El cambio del progreso comenzó a verse desde 1880 y al mismo tiempo se empezaría la construcción de una vía del Ferrocarril Central Mexicano y la edificación del Teatro Morelos. Al parecer llegaba a la ciudad el progreso que tanto anhelaba la élite local, pero por falta de presupuesto o inestabilidad política aplazaban estos proyectos. A principios de 1882, el gobernador Rafael Arellano destinaba recursos a lo que serían las nuevas obras que traerían importantes beneficios a la ciudad. Pese a las carencias que se tenían en el cabildo, las obras fueron elaboradas con cierta rapidez tales que en 1883, Rafael Arellano declararía en su informe de gobierno que la cimentación del teatro concluiría en un año. La importancia del teatro era tal que fomentaría la cultura y colocando a la ciudad en un estatus junto con ciudades como Zacatecas y San Luis Potosí. El 25 de agosto de 1885 se dio la apertura del Teatro Morelos. La inauguración del recinto significaba los nuevos aires de modernidad que imperaban en la capital del Estado, además en dicho espacio se dio lugar "lo más selecto de la sociedad", periodistas que no se cansaban de admirar lo bello del edificio y funcionarios satisfechos con la función que ahí se desarrollaba. La inauguración del Teatro acaparaba a todos los medios haciéndole mención como un lugar culto y digo para una ciudad que se encontraba en progreso. Aguascalientes ya contaba con lugares como corridas de toros y verbenas de barriada, pero la clase alta carecía de un espacio de entretención, por lo que el nuevo teatro llegó a ser un espacio sofisticado para las clases privilegiadas. 

Durante 1881 el progreso ya se comenzaba a presentar en la capital de Aguascalientes con la apertura de vías ferroviarias, además de construir más vetas en las orillas de la Sierra Fría. En conclusión, los cambios que se estaban presentado en el Estado eran de optimismo, pues cada trabajador de los sectores diferentes se mantenía en progreso y la construcción de vías de ferrocarril conectaba a la ciudad, esperando que estas trajesen grandes beneficios al Estado, como el camino que iba de Aguascalientes a Tampico que se esperaba que fuera de mayor provecho para el Estado de Aguascalientes. 

La sociedad que estaba en contra del constante progreso que experimentaba el Estado se haría presente. Manuel Jacinto Guerra, dueño de una fábrica de jabón, declaró que las vías de ferrocarril acarrearían la ruina de empresas de corte artesanal . en 1884 había llegado a Aguascalientes el primer tren de pasajeros, que significó uno de los acontecimientos más importantes del siglo XIX. La construcción de vías y la llegada del ferrocarril acarreó la ilustración a la ciudad, el gobierno ordenaría la reparación de banquetas y empedrados , el mejoramiento de alumbrado público y el arreglo de jardines y paseos existentes. El día que el tren hizo su entrada triunfal hubo fiesta y fuegos artificiales en Aguascalientes. 

Años después de la apertura de la estación del tren en la capital se inauguró otra en Chicalote que conectaba a las ciudades de San Luis Potosí y Tampico, toda esta línea conectaba con el sistema ferroviario del sudeste norteamericano, convirtiendo a Aguascalientes en uno de los nudos más importantes de la red mexicana de vías férreas.  

Silvestre Dorador, Román Morales, Pedro Vital, Alfonso Guerrero Aguilera y Alberto Fuentes Dávila fueron precursores de la Revolución en esta entidad. Pues al estallar el movimiento maderista, abrazaron la causa en compañía de algunos otros coterráneos, y quedó formalizada la acción rebelde del pueblo de la comarca.

El 25 de enero de 1983 se creó, por decreto presidencial, el Instituto Nacional de Estadística y Geografía (INEGI), que integró en su estructura a: la Dirección General de Estadística (en funciones desde 1882, cuando pertenecía a la Secretaría de Fomento, Colonización, Industria y Comercio), la Dirección General de Geografía, establecida en 1968 y que estaba adscrita a la Secretaría de la Presidencia, la Dirección General de Política Informática y la Dirección General de Integración y Análisis de la Información. Desde 1985, el Instituto se desconcentró para ubicar su sede en la ciudad de Aguascalientes.

Aguascalientes consta de once municipios:


Los municipios de El Llano y San Francisco de los Romo son de reciente creación; habiéndoseles otorgado tal distinción en 1992 y 1991, respectivamente.

Se localiza en el centro geográfico del país, a unos 480 km al noroeste de la ciudad de México. Al norte, noreste y oeste limita con el estado de Zacatecas, con el que tiene más de la mitad de sus límites, y al sur y sureste con el estado de Jalisco, su otro vecino. El estado de Aguascalientes es muy pequeño, y solo colinda con otros dos estados. Tiene una extensión territorial de 5,471 km², lo que representa el 0.03 % de la superficie total de México. En el estado atraviesa parte de la Sierra Madre Occidental.

El estado es atravesado por un corredor norte-sur denominado valle de Aguascalientes. Este continúa por el sur en los Altos de Jalisco y el Bajío, y por el norte se entremete más allá de Cosío, hasta el estado de Zacatecas. Las principales elevaciones que se cuentan en la entidad son: Cerro de la Ardilla, en la Sierra Fría (3,050 msnm); Cerro de la Antorcha, en la Sierra del Laurel (2,760 msnm); Cerro del Mirador (2,700 msnm); Cerro del Muerto (2,660 msnm); Cerro de Altamira, en la Sierra de Asientos (2,650 msnm); Cerro de San Juan (2,530 msnm); Cerro de Juan el Grande (2,500 msnm), Cerro El Picacho (2,420 msnm) y el Cerro de Los Gallos (2,340 msnm).

Su principal río es el San Pedro (o río Aguascalientes), afluente del río Santiago. Atraviesa el estado por el centro. En su ribera oriental se encuentra la ciudad capital (Aguascalientes). Su cauce solo lleva agua unos meses al año durante la temporada de lluvias, y no de continuo, debido al elevado número de presas construidas en su cuenca (donde la principal es la presa Plutarco Elías Calles, en el distrito de riego 01).

Otro río importante es el Calvillo, en el municipio del mismo nombre, cuyo suelo no permite ni muchas ni grandes presas. Ambas subcuencas llenan prácticamente todo el estado, y desaguan en el río Santiago.

Otras presas que tiene el estado son El Cedazo, Gral. Abelardo L. Rodríguez, Jocoque, Malpaso y Niágara.

Los climas que se presentan en la región son: templado semiseco en 62 % de la superficie, templado subhúmedo y semicálido semiseco en 25 %, con lluvias en verano. Las precipitaciones se concentran de junio a octubre, contando con un promedio de 500 mm anuales. El clima frío, con temperaturas bajas casi todo el año 12 %, lo representa la Sierra Fría, con temperaturas promedio anuales entre 12 y 16 °C, y precipitación promedio anual de más de 700 mm.

La última vez que nevó en el estado de Aguascalientes fue el 9 de marzo de 2016, en algunas zonas.

La amplitud térmica entre las mínimas y las máximas es muy alta durante todo el año.

La temperatura media anual es de 17 a 18 °C. La temperatura más alta (30 °C o más), se presenta en los meses de mayo y junio. La más baja es de alrededor de 4 °C, en el mes de enero. Las lluvias son escasas y se presentan durante el verano. La precipitación total anual es de 526 mm, por lo que la práctica agrícola requiere de riego.

De acuerdo con climate-data.org, la tabla climática de Aguascalientes es la siguiente:

En la montaña: puma, venado cola blanca, jabalí, lechuza, gato montés, guajolote, coyote, lobo, águila real, conejo, zorro y ardilla.

En los valles: coyote, mapache, liebre, codorniz pinta, lechuza, paloma pinta, jabalí, venado cola blanca, conejo y águila real.

Entre las especies vegetales se encuentran, principalmente, árboles de pino, encino, álamo, laurel y mezquite, huizache, sabino, pirul y nopal en las partes bajas.

Según las cifras que arrojó el "II Censo de Población y Vivienda" realizado por el Instituto Nacional de Estadística y Geografía en 2010, el estado de Aguascalientes contaba hasta entonces con un total de 1,184,996 habitantes. De dicha cantidad, 576,638 eran hombres y 608,358 eran mujeres. La tasa de crecimiento anual para la entidad durante el periodo 2005-2010 fue del 2.2%.

El incremento observado durante la década equivale a los residentes actuales de los municipios de Jesús María, Calvillo, Rincón de Romos, Pabellón de Arteaga, San Francisco de los Romo y Cosío, considerados de manera conjunta.

Así, durante la década pasada y los cinco primeros años de la presente, se incorporaron a la sociedad aguascalentense 22,045 personas en promedio cada año, entre recién nacidos e inmigrantes, y habiendo descontado a los fallecidos y a los emigrantes.

Por su población, la Zona Metropolitana de Aguascalientes ocupa la décimo tercera posición nacional con 968,119 habitantes. Como estado, es el vigésimo séptimo entre los más poblados del país.

El estado cuenta con 4 médicos por cada 1,000 habitantes, mientras que la media nacional es de 1.28, según el [Consejo Nacional de Población] (Conapo).

La población sumada por municipios son 1,213,445 en 2011.

La Zona Metropolitana de Aguascalientes se conforma por 3 municipios:


Por el acelerado crecimiento, el próximo municipio a integrarse a la zona metropolitana será Pabellón de Arteaga.

El estado se caracteriza por su intensa actividad industrial, ganadera y comercial. Actualmente su principal fuente de ingresos es la construcción. Aguascalientes cuenta con importantes vías de comunicación que enlazan al estado con las zonas económicas más importantes del país. Importantes empresas nacionales e internacionales tienen presencia en el estado.

Por tres años consecutivos, el Banco Mundial (BM) ha reconocido a Aguascalientes como el estado con el mejor clima de negocios, armonía laboral y estado de derecho. Aguascalientes tiene un excelente clima de negocios; donde es más fácil, rápido y económico abrir un
negocio.

En el 2014 se realizó un censo económico, para verificar y recoger la información económica de todos los negocios en el estado de Aguascalientes, registrando 55463 establecimientos activos en esa fecha.

Las actividades primarias solo son el 4.65 % del PIB estatal..

La producción agrícola estriba en cultivo de maíz, trigo, soya, sorgo, papa, frijol, chile verde, chile seco, tomate, alfalfa,
ajo, aguacate, y demás árboles frutales. La producción de uva y guayaba es de importancia cultural para el estado de Aguascalientes. La zona agrícola más importante es el valle de Aguascalientes.

En la minería encontramos producción de cemento, cal, oro, plata, estaño y plomo.

La ganadería la hay vacuna (gran cuenca lechera), equina, lanar, caprina, porcina, mular y asnal.

Las actividades secundarias generan el 40.18 % del PIB. En la entidad se ha establecido Nissan, una de las armadoras de automóviles más importantes a nivel mundial. Es la armadora de inversión japonesa más grande instalada en todo el país, además de algunas otras empresas que son proveedores de equipos para esta. También se han establecido importantes empresas de tecnologías, lo cual ha hecho que el estado sea uno de los más avanzados del territorio mexicano. Asimismo, están establecidas cuatro importantes cadenas de hoteles de cinco estrellas, uno de ellos clasificado como cinco estrellas un diamante. En cuanto a servicios profesionales, en la entidad se han establecido las cuatro grandes firmas de contadores públicos a nivel mundial.

Las actividades terciarias generan el 55.17 % del PIB estatal.

Existe en el estado una oferta educativa de todos los niveles y
especialidades. Los principales centros académicos son, sin ningún
orden en particular:


La ciudad de Aguascalientes celebra anualmente a finales del mes de abril y principios de mayo la Feria Nacional de San Marcos, la más importante del país; a la que se promociona con epítetos tales como "La Feria de Ferias" o "La Feria de México". La Feria de San Marcos tiene orígenes en una feria principalmente agrícola y ganadera, iniciada en 1828 y que se celebraba originalmente en el mes de noviembre. Tenía su recinto en los portales de un antiguo parián o mercado a medio construir.

En 1848, se dispuso que se celebrara en el Jardín de San Marcos, en las proximidades del templo de la Virgen del Carmen, pero más ubicado como el Templo de San Marcos, santo que se encuentra en dicho recinto y por quien es nombrado el jardín. Asimismo, para que coincidiera con la celebración del Patrono de Aguascalientes, el 25 de abril, y para aprovechar la cosecha de primavera, la feria se dispuso para el mencionado mes.

Cada año se invita un país y un estado de la república diferente para que muestre a los turistas y ciudadanos su cultura, gastronomía entre otras cosas. La feria incluye la participación de varios artistas de renombre, con actuaciones de cada uno, diariamente en el palenque, corridas de toros, presentaciones artísticas gratuitas en el Teatro del Pueblo, exposición agrícola y ganadera en la Megavelaria y la coronación de la Reina de la Feria, que es el acto inicial de la misma.

Por el mes de julio, en el municipio de Jesús María se celebra la Feria de los Chicahuales; segunda en importancia en el estado.

También en el municipio de Aguascalientes, durante la primera quincena del mes de agosto y particularmente el día 15 de agosto se lleva a cabo la Romería de la Asunción, y comienzan las peregrinaciones a la catedral. Estas peregrinaciones se organizan en la diócesis de Aguascalientes que comprende el estado de Aguascalientes, parte de Zacatecas y Jalisco, y una pequeña población de Guanajuato. La diócesis tiene un poco más de 1 millón de feligreses y es una de las más grandes de México; de hecho, Aguascalientes está catalogado como el tercer estado con mayor porcentaje de católicos en el país, siendo un 93 % de su población la que profesa esta religión, después de Guanajuato y Jalisco.

Se celebra de forma especial el Día de Muertos, el cual es amplificado por el Festival de las Calaveras que abarca desde los últimos días de octubre y primeros días de noviembre. Se homenajea al famoso grabador y caricaturista aguascalentense, José Guadalupe Posada, creador de la mundialmente conocida Catrina.

Entre los principales centros culturales cabe destacar la Casa de la Cultura y su anexo Centro de Artes Visuales, los cuales se encuentran dentro del centro histórico de la capital; así mismo se encuentra la Casa Terán, que recibe su nombre de Jesús Terán Peredo, además de las diversas bibliotecas, como la Biblioteca Central del Estado "Jaime Torres Bodet" y la Biblioteca "Enrique Fernández Ledezma".

Debido a su posición geográfica, la gastronomía del estado se ve influenciada por la de sus estados vecinos y de otros de la región. Así mismo, la cocina va variando en cada uno de sus municipios.

En el municipio capital pueden encontrarse una gran variedad de platillos. Son típicos los establecimientos conocidos como "cenadurías", donde, como su nombre lo indica, se ofrecen "antojitos" para la hora de la cena. En estos lugares pueden encontrarse las tradicionales flautas, tacos dorados, las enchiladas muy típicas del estado, acompañadas con encurtidos de cerdo y verduras. También ofrecen tamales, atoles, tostadas, pozole, sopes y demás. Existe, también en la capital, el tradicional mercado Juárez, donde pueden encontrarse las mejores birrierías del estado. Es una tradición gastronómica que el menudo sea desayunado los domingos. Son también de gran difusión los tacos de colores, que lo son de guisados muy variados.

El municipio de Jesús María es famoso por sus gorditas chiqueadas, rellenas de guisos como chicharrón, tinga, moronga, frijoles, chile relleno, mole, arroz, etc. Se pueden encontrar puestos en los portales de su plaza principal.

El municipio de San Francisco de los Romo posee una gran tradición en la elaboración de carnitas, donde se puede encontrar buche, nana, costilla, chamorro, maciza, patitas, y demás delicias acompañadas de tortillas de comal y salsas como guacamole, pico de gallo, de tomatillo, etc.

En el municipio de Calvillo son famosas sus preparaciones con la guayaba, al ser uno de los productos propios de la región. Se pueden encontrar ates, dulces, pasteles, mermeladas y licores de dicho fruto. Debido a la presencias de presas, son ya típicos los restaurantes de mariscos.

Por su parte, el municipio de Pabellón de Arteaga ofrece los tradicionales "burritos", que son tacos rellenos con diferentes guisos, y hechos con tortillas de harina.

En cuanto a la panadería, Aguascalientes cuenta con una enorme tradición. El pan es un elemento muy presente a la hora del desayuno o de la cena, tanto dulce como salado. Son panes típicos las cemitas, las sanjuaneras, los ladrillos, los puerquitos, los chamucos, las coronas, las conchas, los calvitos, el bolillo (que los hay también de nata y de manteca), así como las gorditas de nata. Resaltan los condoches, que son elaborados en hornos de barro o piedra a la leña.

Este estado cuenta con atractivos arqueológicos como el El Ocote,
con pinturas rupestres hechas por nativos hace miles de años.
El municipio de San José de Gracia
aporta varios sitios naturales de interés, como lo son la Sierra Fría,
donde se puede acampar y practicar la pesca deportiva; los diversos
cañones de la región, como Boca de Túnel de bellos paisajes naturales, 
que combinados con una imponente su presa, hacen de este lugar un
espacio único que invita desde la contemplación hasta los deportes
extremos y acuáticos.

También se dice que la entidad es uno de los mejores sitios del mundo
para presenciar un atardecer. Cielos claros durante el día
frecuentemente dan paso a puestas de coloración más bien infernal.

Destino cultural obligado para todo viajero que visita el estado de
Aguascalientes es el recorrido por las antiguas haciendas de la zona.
Algunas de ellas fueron construidas en el siglo XVI. Representaron el
motor de la economía regional durante siglos, primero con la actividad
minera y luego con la agrícola y ganadera. Entre ellas se destacan las
haciendas de El Soyatal, Santiago (también conocida como Garabato),
San Blas, Palo Alto, El Ocote, San Bartolo, Cieneguilla y Peñuelas.

El estado posee tres de los llamados
"Pueblos Mágicos" del país: Calvillo, Real de Asientos y
San José de Gracia.

En el municipio de Calvillo también hay grandes atracciones
turísticas, como son su centro histórico, la presa de Malpaso, y
lugares ecoturísticos como la Sierra del Laurel y la Sierra Fría. En
estos lugares se dispone de cabañas para rentar al público, y
actividades que realizar como rápel, ciclismo de montaña, caminatas,
motociclismo, gotcha, etc. Calvillo es uno de los mejores lugares para
visitar en el estado de Aguscalientes.

El estado actualmente tiene proyectos estratégicos para el desarrollo
del turismo, como lo ha sido el Cristo Roto de la presa Calles. Este monumento se
ha convertido ya en un icono de Aguascalientes, y un destino obligado
de los turistas, por ser la tercera estatua religiosa más grande de
México.

El turismo en el municipio epónimo se da principalmente en abril,
mes en el que se lleva a cabo la Feria Nacional de San Marcos, con
visitas nacionales y también internacionales que llegan hasta 8
millones de turistas por verbena. Es considerada la mejor feria de
México, por su afluencia, exposiciones, serial taurino, la
exposición ganadera más grande de México; y su reconocimiento
fuera de la entidad de por sí.

Antes, durante y después del Día de Muertos la ciudad celebra el
Festival de las Calaveras.

También en la ciudad de
Aguascalientes son de interés los
barrios que dieron origen a la ciudad: el de San Marcos,
el de la Estación, el de la Salud, el del Encino y el de Guadalupe. Todos estos abrazan el centro histórico de la ciudad, cuya
relevancia se da por hecho.

Lo que en algún tiempo fue el taller de ferrocarriles más importante de México ha sido parcialmente restaurado para dar lugar al complejo ferrocarrilero Tres Centurias, con museos y otros atractivos. Hoy solo un legado, los talleres del ferrocarril marcaron el primer "boom" de la villa. 

Aguascalientes cuenta con tradición en el deporte del motociclismo, que data desde los años 60, con personajes como Joaquín de la Torre Velázquez, Max de la Torre, El famosísimo Suzuki, Los Huerta, Salvador González, Jesús y Francisco Rosales, Marcelino García entre otros. Este deporte tiene muy buenos representantes a nivel nacional e internacional, El estado se ha llevado varios campeonatos nacionales con pilotos como Jesús Rosales Calvillo, René de la Torre Medina, etc. Actualmente se cuentan con varias pistas en el estado, desde la clásica del parque México hasta la moderna y tecnificada pista RCLand.

El nuevo Autódromo Internacional de Aguascalientes está localizado al sur de la capital, cerca de su Aeropuerto Internacional. Es considerado como internacional porque comparte características únicas con los mejores autódromos en el mundo, las dimensiones de la pista son de 7/8 de milla, es una pista de concreto, lo que lo convierte en un autódromo único en Latinoamérica. La primera carrera se llevó a cabo el 12 de abril de 2009 con un lleno total. También albergó la clausura de la NASCAR Corona Series el mes de noviembre de 2009, con una asistencia de más de 35,000 personas. En el año 2010 también se llevó a cabo la carrera final de la NASCAR Corona Series.

Los Gallos de Aguascalientes fue un equipo de fútbol profesional de México. Fundado en la ciudad de Aguascalientes, participó en la Segunda División de México y en Primera División "A".

Club Necaxa, fundado en la Ciudad de México en 1923. Es uno de los clubes de más tradición e historia del fútbol mexicano. En 2003, ante la baja asistencia que tenían en la capital a la sombra del América y de los equipos populares Cruz Azul y Pumas de la UNAM, se decidió mudarlo a la ciudad de Aguascalientes donde incluso se construyó el moderno Estadio Victoria, el cual cuenta con todas las comodidades modernas incluidos un restaurante y fuentes danzantes.

Inicialmente el Club Necaxa inició bien su estancia en Aguascalientes. Incluso calificó a liguilla un par de torneos y jugó una Copa Libertadores. Pero los malos manejos provocaron su declive deportivo que los llevó a descender a la Liga de Ascenso de México en 2009.

Necaxa rápidamente se coronó en su primer torneo en la Liga de Ascenso de México, campeonato que refrendó en el siguiente torneo, por lo que logró el regreso a Primera División de México en solo un año. Sin embargo, regresó a la liga de Ascenso de México en el Apertura 2011. 

Real Aguascalientes FC es un equipo de fútbol profesional de México. Fundado en la ciudad de Aguascalientes, participa en la Tercera División de México.

Pretende continuar con la tradición del desaparecido Gallos de Aguascalientes, ya que el presidente del club es hijo del antiguo presidente de los Gallos de Aguascalientes, el único club en ser totalmente originiario de la ciudad.

Es sede de las Panteras de Aguascalientes que juega en la Liga Nacional de Baloncesto Profesional de México. Actualmente las Panteras de Aguascalientes juegan en el gimnasio Hermanos Carreón.

El estado es sede del equipo de béisbol profesional Rieleros de Aguascalientes que juega en la Liga Mexicana de Béisbol. Actualmente juega en el Parque Alberto Romo Chávez.

Aguascalientes se caracteriza también por tener una población regularmente activa, cuenta con diferentes clubes sociales y complejos deportivos, privados y estatales, que están constantemente en formación de personas para competencias organizadas en el estado, y otras más importantes como son campeonatos regionales, nacionales e internacionales.

Las principales instalaciones deportivas de Aguascalientes son:

- Gimnasio multidisciplinario

- Alberca Olímpica

- CECADI

- Unidad deportiva IV Centenario

- Complejo deportivo bicentenario

- Parque Ferrocarrilero

Entre los deportes que Aguascalientes se caracteriza por producir deportistas competentes, son:
Natación Femenil y Varonil
Baloncesto Femenil
Tae kwon do
Arquería

Artistas

Actores y cantantes populares:

Escritores:

Locutores:

Académicos:
Deportistas

El Estado ha firmado los siguientes acuerdos de hermanamiento:



Eslabones del 



</doc>
<doc id="5001" url="https://es.wikipedia.org/wiki?curid=5001" title="QNX">
QNX

QNX (pronunciado Q.N.X. o Quiu-nex) es un sistema operativo de tiempo real de tipo Unix que cumple con la norma POSIX, desarrollado por QNX Software Systems, una empresa canadiense que fue adquirida por BlackBerry en abril de 2010, convirtiéndose así en subsidiaria de esta última. Es desarrollado principalmente para su uso en sistemas embebidos, y está disponible para las arquitecturas x86, MIPS, PowerPC, SH4 (incluida la videoconsola Dreamcast con una versión muy limitada), ARM, StrongARM, xScale y BlackBerry Playbook. QNX está basado en una arquitectura de kernel micronúcleo que proporciona características de estabilidad avanzadas de memoria protegida frente a fallos de dispositivos, aplicaciones, etc.

Photon o Photon microGUI es el sistema de ventanas (servidor y cliente) de QNX, aunque también funciona una versión X Window. En lo que refiere a las implementaciones para trabajar la interfaz gráfica, QNX es compatible con pipelines para combinar varias tecnologías en una misma imagen. Puede trabajar con OpenGL, HTML5, y Qt 5 en una misma pantalla.

Los sistemas operativos de tiempo real son interesantes para situaciones donde sea absolutamente necesaria una toma continua de, por ejemplo, muestras de datos. Basándose en este interés, existen diversos proyectos para crear versiones en tiempo real de otros sistemas.

El microkernel de QNX, llamado Neutrino, está implementado en 4 variantes que desarrolla y comercializa la compañía:
Esta versión es la más completa y robusta pensada para cumplir los requerimientos de sistemas embebidos. Es un microkernel real de arquitectura modular.
Está diseñada para cumplir con las normas ISO 26262 en ASIL D y las normas IEC 61508 en SIL3. Provee de un sistema diseñado sobre una base segura, para implementar en sistemas críticos como automóviles, trenes y automatización industrial.
Cumple las normas IEC 62304 y está diseñado para reducir el esfuerzo en el desarrollo de dispositivos médicos que requieren de aprobaciones regulativas.
Es un RTOS de características completas, certificado en norma ISO/IEC 15408 EAL 4+.



</doc>
<doc id="5002" url="https://es.wikipedia.org/wiki?curid=5002" title="BeOS">
BeOS

BeOS fue un sistema operativo para PC desarrollado por Be Incorporated en 1990, orientado principalmente a proveer alto rendimiento en aplicaciones multimedia. A pesar de la creencia común fomentada por la inclusión de la interfaz de comandos Bash en el sistema operativo, el diseño de BeOS no estaba basado en UNIX.

BeOS cuenta con un micronúcleo modular propio, el cual ha sido altamente optimizado para trabajo con audio, video y gráficos, y animaciones en tres dimensiones. A diferencia de UNIX, BeOS es un sistema operativo monousuario. Su arquitectura de núcleo avanzada ofrece capacidad para múltiples procesadores, un rendimiento alto, ancho de banda de entrada/salida modular, multihilo generalizado, multitarea apropiativa, flexibilidad gráfica y respuesta en tiempo real. Posee un sistema de archivos con registro por diario e índice optimizados para 64 bits llamado BFS, pero en vez de utilizar una base de datos, BeOS confía en su bajo tiempo de espera para registrar y recuperar atributos de archivos en menor tiempo. Una nueva interfaz gráfica de usuario multihilo fue desarrollada bajo los principios de claridad y un diseño simple y ordenado. La interfaz API fue escrita en C++ para simplicidad de programación. Posee compatibilidad POSIX y una interfaz de línea de comandos basada en Bash.

Originalmente (1995-1996) el sistema operativo corría sobre su propio hardware, conocido como BeBox. Más tarde (1997) fue extendido a la plataforma PowerPC y finalmente (1998) se añadió compatibilidad con procesadores x86. La intención original de Be era venderle el sistema operativo a Apple para reemplazar Mac OS, pero los planes de venta fracasaron y el sistema nunca alcanzó gran popularidad en el mercado. En el año 2001 la propiedad intelectual de Be fue vendida a la empresa Palm que la pasó a palmSource y es ahora de Access Co.. El último lanzamiento del sistema fue su versión 5 en el año 2000, aunque existía en proyecto una versión nueva y mejorada antes de la bancarrota.

En febrero de 2001 Be Incorporated tomó acción penal contra Microsoft por sus prácticas anticompetitivas. Durante años Microsoft utilizó contratos de licencia cerrados con fabricantes de hardware, los cuales impedían la fabricación de equipos con más de un sistema operativo preinstalado, es decir, con cualquier cosa que no fuera Windows. Esta táctica finalmente dejó a Be fuera del mercado. Como medida desesperada, el presidente ejecutivo de Be (Jean-Louis Gassée) ofreció distribuir BeOS gratuitamente a cualquier fabricante de hardware que aceptara instalar BeOS junto con Windows, pero ninguno aceptó. El 5 de septiembre de 2003, el juicio fue cerrado con el pago de 23,2 millones de dólares a Be, tras lo cual Microsoft dejó de ser acusada de prácticas indebidas.

Pese a su poco renombre, BeOS posee una fiel comunidad de usuarios y aficionados la cual quedó comprensiblemente decepcionada con el fracaso comercial de Be. Hacia 2002, algunos proyectos de software libre fueron formados para recrear BeOS 5 y a partir de allí extender sus capacidades (sin la inclusión de código propietario cerrado). La naturaleza de micronúcleo de BeOS hace posible su recreación en partes, permitiendo probar cada módulo con el núcleo ya existente del sistema BeOS oficial. La interfaz Gráfica de BeOS (OpenTracker/OpenDeskbar), ha sido íntegramente recreada bajo licencia MIT.

El sistema de archivos, BeFS (de 64 bits, Journaled, orientado a base de datos) ha inspirado a:

La réplica de código abierto del BeFS está disponible en Sourceforge.net, y para mejor, su autor, Domenic Giampaolo, ha liberado el libro "Practical filesystem design".


El rumor dice que la extinta YellowTAB poseía los derechos para utilizar el código base del inconcluso BeOS R6, de nombre clave "Dano", el cual se supone utilizaron para su producto Zeta. Pero Access Co. Ltd. negó que yellowTab o Magnussoft poseyeran dichos derechos.




</doc>
<doc id="5009" url="https://es.wikipedia.org/wiki?curid=5009" title="WAI">
WAI

WAI o wai, hace referencia a:


</doc>
<doc id="5010" url="https://es.wikipedia.org/wiki?curid=5010" title="William Shakespeare">
William Shakespeare

William Shakespeare ( ; Stratford-upon-Avon, Warwickshire, Reino de Inglaterra, c. - "ib.", ) fue un dramaturgo, poeta y actor inglés. Conocido en ocasiones como el Bardo de Avon (o simplemente el Bardo), Shakespeare es considerado el escritor más importante en lengua inglesa y uno de los más célebres de la literatura universal.

Según la "Encyclopædia Britannica", «Shakespeare es generalmente reconocido como el más grande de los escritores de todos los tiempos, figura única en la historia de la literatura. La fama de otros poetas, tales como Homero y Dante Alighieri, o de novelistas tales como León Tolstoy o Charles Dickens, ha trascendido las barreras nacionales, pero ninguno de ellos ha llegado a alcanzar la reputación de Shakespeare, cuyas obras hoy se leen y representan con mayor frecuencia y en más países que nunca. La profecía de uno de sus grandes contemporáneos, Ben Jonson, se ha cumplido por tanto: "Shakespeare no pertenece a una sola época sino a la eternidad"».

El crítico estadounidense Harold Bloom sitúa a Shakespeare junto a Dante Alighieri, en la cúspide de su «canon occidental»: «Ningún otro escritor ha tenido nunca tantos recursos lingüísticos como Shakespeare, tan profusos en "Trabajos de amor perdidos" que tenemos la impresión de que, de una vez por todas, se han alcanzado muchos de los límites del lenguaje. Sin embargo, la mayor originalidad de Shakespeare reside en la representación de personajes: Bottom es un melancólico triunfo; Shylock, un problema permanentemente equívoco para todos nosotros; pero "sir" John Falstaff es tan original y tan arrollador que, con él, Shakespeare da un giro de ciento ochenta grados a lo que es crear a un hombre por medio de palabras».

Jorge Luis Borges escribió sobre él: «Shakespeare es el menos inglés de los poetas de Inglaterra. Comparado con Robert Frost (de New England), con William Wordsworth, con Samuel Johnson, con Chaucer y con los desconocidos que escribieron, o cantaron, las elegías, es casi un extranjero. Inglaterra es la patria del "understatement", de la reticencia bien educada; la hipérbole, el exceso y el esplendor son típicos de Shakespeare».

Shakespeare fue poeta y dramaturgo venerado ya en su tiempo, pero su reputación no alcanzó las altísimas cotas actuales hasta el siglo XIX. Los románticos, particularmente, aclamaron su genio, y los victorianos adoraban a Shakespeare con una devoción que George Bernard Shaw denominó «bardolatría».

En el siglo XX, sus obras fueron adaptadas y redescubiertas en multitud de ocasiones por todo tipo de movimientos artísticos, intelectuales y de arte dramático. Las comedias y tragedias shakespearianas han sido traducidas a las principales lenguas, y constantemente son objeto de estudios y se representan en diversos contextos culturales y políticos de todo el mundo. Por otra parte, muchas de las citas y aforismos que salpican sus obras han pasado a formar parte del uso cotidiano, tanto en inglés como en otros idiomas. Y en lo personal, con el paso del tiempo, se ha especulado mucho sobre su vida, cuestionando su sexualidad, su filiación religiosa, e incluso la autoría de sus obras.

Existen muy pocos hechos documentados en la vida de Shakespeare, aunque es probable que naciera el 23 de abril de 1564. Lo que sí se puede afirmar es que fue bautizado en la Iglesia de la Santísima Trinidad en Stratford-upon-Avon, Warwickshire, el 26 de abril de ese mismo año y falleció en ese mismo lugar el 23 de abril de 1616, según el calendario juliano (3 de mayo en el gregoriano), el supuesto día en el que cumplió 52 años.

William Shakespeare (también deletreado Shakspere, Shaksper y Shake-speare, porque la ortografía en tiempos isabelinos no era ni fija ni absoluta) nació en Stratford-upon-Avon, en abril de 1564. Fue el tercero de los ocho hijos que tuvieron John Shakespeare, un próspero comerciante que llegó a alcanzar una destacada posición en el municipio, y Mary Arden, que descendía de una familia de abolengo.

En el momento de su nacimiento su familia vivía en la calle Henley de Stratford-upon-Avon. No se conoce el día exacto de su nacimiento, puesto que en aquella época solo se hacía el acta del bautismo, el 26 de abril en este caso, por lo que es de suponer que nacería algunos días antes (2 o 3 días) y no más de una semana, según era lo corriente. La tradición ha venido fijando como fecha de su nacimiento el 23 de abril, festividad de San Jorge, tal vez por analogía con el día de su muerte, el 23 de abril de 1616, según el calendario juliano, pero esta datación no se sustenta en ningún documento, aunque es la fecha más probable.

El padre de Shakespeare, que se encontraba en la cumbre de su prosperidad cuando nació William, cayó poco después en desgracia. Acusado de comercio ilegal de lana, perdió su posición destacada en el gobierno del municipio. Se ha apuntado también que tal vez tuvo que ver en su procesamiento una posible afinidad con la fe católica, por ambas partes de la familia.

William Shakespeare probablemente cursó sus primeros estudios en la escuela primaria local, la Stratford Grammar School, en el centro de su ciudad natal, lo que debió haberle aportado una educación intensiva en gramática y literatura latinas. A pesar de que la calidad de las escuelas gramaticales en el período isabelino era bastante irregular, existen indicios en el sentido de que la de Stratford era bastante buena. La asistencia de Shakespeare a esta escuela es mera conjetura, basada en el hecho de que legalmente tenía derecho a educación gratuita por ser el hijo de un alto cargo del gobierno local. No obstante, no existe ningún documento que lo acredite, ya que los archivos parroquiales se han perdido. En esa época estaba dirigida por John Cotton, maestro de amplia formación humanística y católico; una "grammar school" (equivalente a un estudio de gramática del XVI español o al actual bachillerato) impartía enseñanzas desde los 8 hasta los 15 años y la educación se centraba en el aprendizaje del latín; en los niveles superiores el uso del inglés estaba prohibido para fomentar la soltura en la lengua latina; prevalecía el estudio de la obra de Esopo traducida al latín, de Ovidio y de Virgilio, autores estos que Shakespeare conocía.

El 28 de noviembre de 1582, cuando tenía 18 años de edad, Shakespeare contrajo matrimonio con Anne Hathaway, de 26, originaria de Temple Grafton, localidad próxima a Stratford. Dos vecinos de Anne, Fulk Sandalls y John Richardson, atestiguaron que no existían impedimentos para la ceremonia. Parece que había prisa en concertar la boda, tal vez porque Anne estaba embarazada de tres meses. Tras su matrimonio, apenas hay huellas de William Shakespeare en los registros históricos, hasta que hace su aparición en la escena teatral londinense. El 26 de mayo de 1583, la hija primogénita de la pareja, Susanna, fue bautizada en Stratford. Un hijo, Hamnet, y otra hija, Judith, nacidos mellizos, fueron asimismo bautizados poco después. A juzgar por el testamento del dramaturgo, que se muestra algo desdeñoso con Anne Hathaway, el matrimonio no estaba bien avenido.

Los últimos años de la década de 1580 son conocidos como los 'años perdidos' del dramaturgo, ya que no hay evidencias que permitan conocer dónde estuvo, o por qué razón decidió trasladarse de Stratford a Londres. Según una leyenda que actualmente resulta poco creíble, fue sorprendido cazando ciervos en el parque de "sir" Thomas Lucy, el juez local, y se vio obligado a huir. Según otra hipótesis, pudo haberse unido a la compañía teatral Lord Chamberlain's Men a su paso por Stratford. Un biógrafo del siglo XVII, John Aubrey, recoge el testimonio del hijo de uno de los compañeros del escritor, según el cual Shakespeare habría pasado algún tiempo como maestro rural.

Hacia 1592 Shakespeare se encontraba ya en Londres trabajando como dramaturgo, y era lo suficientemente conocido como para merecer una desdeñosa descripción de Robert Greene, quien lo retrata como «un grajo arribista, embellecido con nuestras plumas, que con su corazón de tigre envuelto en piel de comediante se cree capaz de impresionar con un verso blanco como el mejor de vosotros», y dice también que «se tiene por el único sacude-escenas del país» (en el original, Greene usa la palabra "shake-scene", aludiendo tanto a la reputación del autor como a su apellido, en un juego de paronomasia).

En 1596, con solo once años de edad, murió Hamnet, único hijo varón del escritor, quien fue enterrado en Stratford el 11 de agosto de ese mismo año. Algunos críticos han sostenido que la muerte de su hijo pudo haber inspirado a Shakespeare la composición de "Hamlet" (hacia 1601), reescritura de una obra más antigua que, por desgracia, no ha sobrevivido.

Hacia 1598 Shakespeare había trasladado su residencia a la parroquia de St. Helen, en Bishopsgate. Su nombre encabeza la lista de actores en la obra "Cada cual según su humor" ("Every Man in His Humour"), de Ben Jonson.

Pronto se convertiría en actor, escritor, y, finalmente, copropietario de la compañía teatral conocida como Lord Chamberlain's Men, que recibía su nombre, al igual que otras de la época, de su aristocrático mecenas, el lord chambelán (Lord Chamberlain). La compañía alcanzaría tal popularidad que, tras la muerte de Isabel I y la subida al trono de Jacobo I, el nuevo monarca la tomaría bajo su protección, pasando a denominarse los King's Men (Hombres del rey).

En 1604, Shakespeare hizo de casamentero para la hija de su casero. Documentación legal de 1612, cuando el caso fue llevado a juicio, muestra que en 1604, Shakespeare había sido arrendatario de Christopher Mountjoy, un artesano hugonote del noroeste de Londres. El aprendiz de Mountjoy, Stephen Belott, tenía intenciones de casarse con la hija de su maestro, por lo que el dramaturgo fue elegido como intermediario para ayudar a negociar los detalles de la dote. Gracias a los servicios de Shakespeare, se llevó a efecto el matrimonio, pero ocho años más tarde Belott demandó a su suegro por no hacer entrega de la totalidad de la suma acordada en concepto de dote. El escritor fue convocado a testificar, mas no recordaba el monto que había propuesto.

Existen varios documentos referentes a asuntos legales y transacciones comerciales que demuestran que en su etapa londinense Shakespeare se enriqueció lo suficiente como para comprar una propiedad en Blackfriars y convertirse en el propietario de la segunda casa más grande de Stratford.

Shakespeare se retiró a su pueblo natal en 1611, pero se vio metido en diversos pleitos, como por ejemplo un litigio respecto al cercado de tierras comunales que, si por un lado fomentaba la existencia de pasto para la cría de ovejas, por otro condenaba a los pobres arrebatándoles su única fuente de subsistencia. Como el escritor tenía cierto interés económico en tales propiedades, para disgusto de algunos tomó una posición neutral que solo aseguraba su propio beneficio. En marzo de 1613 hizo su última adquisición, no en su pueblo, sino en Londres, comprando por 140 libras una casa con corral cerca del teatro de Blackfriars, de cuya suma solo pagó en el acto sesenta libras, pues al día siguiente hipotecó la casa por el resto al vendedor. Por cierto que Shakespeare no hizo la compra a su solo nombre, sino que asoció los de William Johnson, John Jackson y John Hemynge, este último uno de los actores que promovieron la edición del "First folio". El efecto legal de este procedimiento, según escribe el gran biógrafo de Shakespeare Sidney Lee, «era privar a su mujer, en caso de que sobreviviera, del derecho de percibir sobre esta propiedad el dote de viuda»; pero pocos meses después aconteció un desastre: se incendió el Teatro del Globo, y con él todos los manuscritos del dramaturgo, junto con su comedia "Cardenio", inspirada en un episodio de "Don Quijote de La Mancha"; se sabe de esta obra porque el 9 de septiembre de 1653 el editor Humphrey Maseley obtuvo licencia para la publicación de una obra que describe como "Historia de Cardenio", por Fletcher y Shakespeare; el citado Sidney Lee dice que ningún drama de este título ha llegado hasta nuestros días y que probablemente haya que identificarlo con la perdida comedia llamada "Cardenno" o "Cardenna", que fue representada dos veces ante la Corte por la compañía de Shakespeare, la primera en febrero de 1613, con ocasión de las fiestas por el matrimonio de la princesa Isabel, y la segunda en 8 de junio, ante el embajador del Duque de Saboya, esto es, pocos días antes de incendiarse el teatro de El Globo.

En las últimas semanas de la vida de Shakespeare, el hombre que iba a casarse con su hija Judith —un tabernero de nombre Thomas Quiney— fue acusado de promiscuidad ante el tribunal eclesiástico local. Una mujer llamada Margaret Wheeler había dado a luz a un niño, y afirmó que Quiney era el padre. Tanto la mujer como su hijo murieron al poco tiempo. Esto afectó, no obstante, a la reputación del futuro yerno del escritor, y Shakespeare revisó su testamento para salvaguardar la herencia de su hija de los problemas legales que Quiney pudiese tener.

Shakespeare falleció el 23 de abril de 1616, según el calendario juliano (3 de mayo en el gregoriano). Estuvo casado con Anne hasta su muerte, y le sobrevivieron dos hijas, Susannah y Judith. La primera se casó con el doctor John Hall. Sin embargo, ni los hijos de Susannah ni los de Judith tuvieron descendencia, por lo que no existe en la actualidad ningún descendiente vivo del escritor. Se rumoreó, sin embargo, que Shakespeare era el verdadero padre de su ahijado, el poeta y dramaturgo William Davenant.

Siempre se ha tendido a asociar la muerte de Shakespeare con la bebida, —murió, según los comentarios más difundidos, como resultado de una fuerte fiebre, producto de su estado de embriaguez—. Al parecer, el dramaturgo se habría reunido con Ben Jonson y Michael Drayton para festejar con sus colegas algunas nuevas ideas literarias. Investigaciones recientes llevadas a cabo por científicos alemanes afirman que es muy probable que el escritor inglés padeciera de cáncer.

Los restos de Shakespeare fueron sepultados en el presbiterio de la iglesia de la Santísima Trinidad ("Holy Trinity Church") de Stratford. El honor de ser enterrado en el presbiterio, cerca del altar mayor de la iglesia, no se debió a su prestigio como dramaturgo, sino a la compra de un diezmo de la iglesia por 440 libras (una suma considerable en la época). El monumento funerario de Shakespeare, erigido por su familia sobre la pared cercana a su tumba, lo muestra en actitud de escribir, y cada año, en la conmemoración de su nacimiento, se le coloca en la mano una nueva pluma de ave.

Era costumbre en esa época, cuando había necesidad de espacio para nuevas sepulturas, vaciar las antiguas, y trasladar sus contenidos a un osario cercano. Tal vez temiendo que sus restos pudieran ser exhumados, según la "Enciclopedia Británica", el propio Shakespeare habría compuesto el siguiente epitafio para su lápida:

Una leyenda afirma que las obras inéditas de Shakespeare yacen con él en su tumba. Nadie se ha atrevido a comprobar la veracidad de la leyenda, tal vez por miedo a la maldición del citado epitafio.

Se desconoce cuál entre todos los retratos que existen de Shakespeare es el más fiel a la imagen del escritor, ya que muchos de ellos son falsos y pintados "a posteriori" a partir del grabado del "First folio". El llamado "Retrato Chandos", que data de entre 1600 y 1610, en la National Portrait Gallery (en Londres), se considera el más acertado. En él aparece el autor a los cuarenta años, aproximadamente, con barba y un aro dorado en la oreja izquierda.

Resulta curioso que todo el conocimiento que ha llegado a la posteridad sobre uno de los autores del canon occidental no sea más que un constructo formado con las más diversas especulaciones. Se ha discutido incluso si Shakespeare es el verdadero autor de sus obras, atribuidas por algunos a Francis Bacon, a Christopher Marlowe (quien, como espía, habría fingido su propia muerte) o a varios ingenios; la realidad es que todas esas imaginaciones derivan del simple hecho de que los datos de que se dispone sobre el autor son muy pocos y contrastan con la desmesura de su obra genial, que fecunda y da pábulo a las más retorcidas interpretaciones.

Casi ciento cincuenta años después de la muerte de Shakespeare en 1616, comenzaron a surgir dudas sobre la verdadera autoría de las obras a él atribuidas. Los críticos se dividieron en «stratfordianos» (partidarios de la tesis de que el William Shakespeare nacido y fallecido en Stratford fue el verdadero autor de las obras que se le atribuyen) y «anti-stratfordianos» (defensores de la atribución de estas obras a otro autor). La segunda posición es, en la actualidad, muy minoritaria.

Los documentos históricos demuestran que entre 1590 y 1620 se publicaron varias obras teatrales y poemas atribuidos al autor William Shakespeare, y que la compañía que representaba estas piezas teatrales, Lord Chamberlain's Men (luego King's Men), tenía entre sus componentes a un actor con este nombre. Se puede identificar a este actor con el William Shakespeare del que hay constancia que vivió y murió en Stratford, ya que este último hace en su testamento ciertos dones a miembros de la compañía teatral londinense.

Los llamados «stratfordianos» opinan que este actor es también el autor de las obras atribuidas a Shakespeare, apoyándose en el hecho de que tienen el mismo nombre, y en los poemas encomiásticos incluidos en la edición de 1623 del First Folio, en los que hay referencias al «Cisne de Avon» y a su «monumento de Stratford». Esto último hace referencia a su monumento funerario en la iglesia de la Santísima Trinidad, en Stratford, en el que, por cierto, aparece retratado como escritor, y del que existen descripciones hechas por visitantes de la localidad desde, al menos, la década de 1630. Según este punto de vista, las obras de Shakespeare fueron escritas por el mismo William Shakespeare de Stratford, quien dejó su ciudad natal y triunfó como actor y dramaturgo en Londres.

Los llamados «anti-stratfordianos» discrepan de lo anteriormente expresado. Según ellos, el Shakespeare de Stratford no sería más que un hombre de paja que encubriría la verdadera autoría de otro dramaturgo que habría preferido mantener en secreto su identidad. Esta teoría tiene diferentes bases: supuestas ambigüedades y lagunas en la documentación histórica acerca de Shakespeare; el convencimiento de que las obras requerirían un nivel cultural más elevado del que se cree que tenía Shakespeare; supuestos mensajes en clave ocultos en las obras; y paralelos entre personajes de las obras de Shakespeare y la vida de algunos dramaturgos.

Durante el siglo XIX, el candidato alternativo más popular fue "sir" Francis Bacon. Muchos «anti-stratfordianos» del momento, sin embargo, se mostraron escépticos hacia esta hipótesis, aun cuando fueron incapaces de proponer otra alternativa. El poeta estadounidense Walt Whitman dio fe de este escepticismo cuando le dijo a Horace Traubel, «Estoy con vosotros, compañeros, cuando decís “no” a Shaksper ("sic"): es a lo que puedo llegar. Respecto a Bacon, bueno, veremos, veremos». Desde los años 1980, el candidato más popular ha sido Edward de Vere, decimoséptimo conde de Oxford, propuesto por John Thomas Looney en 1920, y por Charlton Ogburn en 1984. El poeta y dramaturgo Christopher Marlowe se ha barajado también como alternativa, aunque su temprana muerte lo relega a un segundo plano. Otros muchos candidatos han sido propuestos, si bien no han conseguido demasiados seguidores.

La posición más extendida en medios académicos es que el William Shakespeare de Stratford fue el autor de las obras que llevan su nombre.

Sin embargo, recientemente el rumor sobre la autoría de Shakespeare se ha acrecentado tras las declaraciones de los actores Derek Jacobi y Mark Rylance. Ambos han divulgado la denominada "Declaración de Duda Razonable" sobre la identidad del famoso dramaturgo. La declaración cuestiona que William Shakespeare, un plebeyo del siglo XVI criado en un hogar analfabeto de Stratford-upon-Avon, escribiera las geniales obras que llevan su nombre. El comunicado argumenta que un hombre que apenas sabía leer y escribir no pudo poseer los rigurosos conocimientos legales, históricos y matemáticos que salpican las tragedias, comedias y sonetos atribuidos a Shakespeare.

A lo largo del tiempo han existido teorías que subrayan que William Shakespeare era tan solo un alias tras los que podían esconderse otros ilustres nombres como Christopher Marlowe (1564-1593), el filósofo y hombre de letras Francis Bacon (1561-1626) o Edward de Vere (1550-1604), decimoséptimo conde de Oxford. Jacobi asegura inclinarse por Edward de Vere, que frecuentó la vida cortesana en el reinado de Isabel I (1533-1603), y lo califica como su «candidato» preferido, dadas las supuestas similitudes entre la biografía del conde y numerosos hechos relatados en los libros de Shakespeare.

¿Cuál es una de las razones principales por la que se cuestionó la autoría de Shakespeare? El "World Book Encyclopedia" señala «la negativa a creer que un actor de Stratford on Avon hubiese podido escribir tales obras. Su origen rural no cuadraba con la imagen que tenían del genial autor». La citada enciclopedia añade que la mayoría de los supuestos escritores «pertenecían a la nobleza o a otro estamento privilegiado». Así pues, muchos de los que ponían en tela de juicio la paternidad literaria de Shakespeare creen que «solo pudo haber escrito las obras un autor instruido, refinado y de clase alta». Con todo, muchos especialistas creen que Shakespeare sí las escribió.

Se ha opinado mucho sobre la vida personal del autor y sobre su presunta homosexualidad, especulación que encuentra su base principal en una originalísima colección de sonetos que fue publicada, al parecer, sin su consentimiento. También se ha sospechado la existencia de alguna o algún amante que hiciera desgraciado su matrimonio, ya que la que fue su mujer y madre de sus tres hijos era bastante mayor que él y se encontraba embarazada antes de la boda. Esta sospecha se asienta en una famosa cita de su testamento: «Le dejo mi segunda mejor cama», pasaje que ha suscitado las más dispares interpretaciones y no pocas especulaciones. La más general tiene que ver con que la relación de la pareja no era del todo satisfactoria. Pero otra apunta en sentido contrario, ya que el dramaturgo le habría dedicado un soneto a su señora esposa titulado "The World's Wife" («La esposa del mundo»).

Se ha seguido muy de cerca, además, la crueldad de Shakespeare con respecto a la figura femenina en sus sonetos y, en consecuencia, de la ingenuidad del hombre que cae atrapado en sus redes. Los temas de la promiscuidad, lo carnal y la falsedad de la mujer —descrita y criticada humorísticamente por el dramaturgo— son pruebas suficientes para los que parten de la base de que tendría cierta predilección por los hombres y un repudio hacia la coquetería de las damas, en todo caso, siempre mencionadas en alusión a su superficialidad e intereses materialistas.

Véase parte del siguiente fragmento de su soneto 144:

Se puede apreciar claramente la dura crítica shakesperiana hacia el papel de una mujer que, a primera vista, parece interponerse entre el romance del dramaturgo y su mecenas.
Quienes desmienten este supuesto, lo hacen objetando que la voz poética del soneto no tiene por qué coincidir con la personalidad del autor.

Lo cierto es que Shakespeare parodia su perspectiva, como vemos en la cita:

Toda esta problemática se enturbia si nos detenemos por un instante a analizar algunos de sus más afamados pasajes teatrales. En una de sus comedias, titulada "Como gustéis", Shakespeare pone de manifiesto la corrupción del mundo masculino y la capacidad de una mujer –Rosalinda – para restaurar el orden inicial y llegar a la paz. Sin embargo, a pesar de que la heroína de la trama es una figura femenina, ésta se arma de valor y es capaz de grandes hazañas recién cuando asume el papel de un hombre, Ganimedes —personaje de la mitología, amante masculino de Júpiter—.

Adentrándonos en la tragedia, el caso del "Rey Lear" es también muy representativo. Aquí el autor destaca la ceguedad de los hombres, sobre todo de Lear, que destierra a su hija Cordelia por ser la única de las tres hermanas en expresar su honestidad.
Estudios feministas apuntarían a que Shakespeare atacaba a su sociedad contemporánea, y que utilizaría nombres y lugares ficticios para huir de persecuciones de la corte.

Defiende a la mujer y le hace ver a los hombres que el silenciarla terminaría en catástrofe, como así sucede en el desenlace de "Lear".
Otras opiniones sobre la obra expresan que la mujer no podía acceder al trono, según el dramaturgo, porque esto implicaría caos y controversias. Cuando el rey Lear adjudica el poder a sus dos hijas mayores, Goneril y Regan, éstas cambian su conducta bruscamente para con su padre y lo someten a una agobiante tortura que irá consumiendo su vida poco a poco. El gobierno se deteriora y el séquito real se desmorona hasta que un hombre reasume el mando.

En 1559, cinco años antes del nacimiento de Shakespeare, durante el reinado de Isabel I, la Iglesia de Inglaterra se separó definitivamente, tras un período de incertidumbre, de la Iglesia católica. Por esa razón, los católicos ingleses fueron presionados para convertirse al anglicanismo, y se establecieron leyes para perseguir a los que rehusaban convertirse. Algunos historiadores sostienen que durante la época de Shakespeare existió una oposición importante y muy extendida a la imposición de la nueva fe. Algunos críticos, apoyándose en evidencias tanto históricas como literarias, han argumentado que Shakespeare era uno de estos opositores, si bien no han conseguido demostrarlo fehacientemente. Lo cierto es que Shakespeare se encontró más cómodo bajo el reinado del filocatólico Jacobo I que bajo el de Isabel I, anticatólica.

Hay indicios de que algunos miembros de la familia del dramaturgo fueron católicos. El más importante es un folleto firmado por John Shakespeare, padre del poeta, en el que, supuestamente, este hacía profesión de fe de su secreto catolicismo. El texto, hallado en el interior de una de las vigas de la casa natal de Shakespeare en el siglo XVIII, fue analizado por un destacado estudioso, Edmond Malone. Sin embargo, se ha perdido, por lo que no puede demostrarse su autenticidad. John Shakespeare figuraba también entre los que no asistían a los servicios eclesiásticos, pero supuestamente esto fue "por temor a ser procesado por deudas", según los comisionados, y no por no aceptar la religión anglicana.

La madre de Shakespeare, Mary Arden, pertenecía a una conocida familia católica de Warwickshire. En 1606, su hija Susannah fue una de las pocas mujeres residentes en Stratford que rehusaron tomar la comunión, lo que podría sugerir ciertas simpatías por el catolicismo. El archidiácono Richard Davies, un clérigo anglicano del siglo XVIII, escribió supuestamente de Shakespeare: «Murió como un papista». Además, cuatro de cada seis maestros de la escuela de Stratford a la que se cree que asistió el escritor durante su juventud, eran simpatizantes católicos, y Simon Hunt, probablemente uno de los profesores de Shakespeare, terminó haciéndose jesuita.

Aunque ninguna de estas teorías prueba de modo fehaciente que Shakespeare fuese católico, la historiadora Clare Asquith es de la opinión de que las simpatías de Shakespeare por el catolicismo son perceptibles en su escritura. Según Asquith, Shakespeare utiliza términos positivos, como «alto» ("high"), «luminoso» ("light") o «justo» ("fair"), para aludir a personajes católicos; y términos negativos —«bajo» ("low"), «oscuro» ("dark")— para los protestantes.

Aunque es mucho lo que se desconoce sobre la educación de Shakespeare, lo cierto es que el artista no accedió a una formación universitaria y su amigo Ben Jonson, que sí la tenía, lamentó en alguna ocasión «su escaso latín y aún menos griego», lo que no fue óbice para que le llamara, además, «dulce cisne del Avon» y añadiera que «no es de un siglo, sino de todos los tiempos». En cierta manera, su no tan escasa instrucción (en Stratford había una buena escuela, y Shakespeare pudo conocer en ella a bastantes clásicos latinos) fue una ventaja, ya que su cultura no se moldeó sobre el patrón común de su tiempo; como autodidacta, William Shakespeare, según señaló un experto conocedor y traductor de su obra completa, Luis Astrana Marín, tuvo acceso a fuentes literarias sumamente raras gracias a la amistad que sostuvo con un librero. Los análisis de sus escritos revelan que fue un lector voraz; algunos de ellos son auténticos centones de textos extraídos de las fuentes más diversas. Pero son fundamentalmente cuatro las fuentes de sus obras.

En primer lugar, los historiadores ingleses, en especial la segunda edición de las "The Chronicles of England, Scotlande, and Irelande", publicada en 1587, de Raphael Holinshed, como fuente de algunos de sus dramas históricos, de la trama de "Macbeth" y de partes de "El rey Lear" y "Cimbelino"; las "Vidas paralelas" de Plutarco en la retraducción desde la versión francesa de Jacques Amyot realizada por su amigo Thomas North (1573), de donde sacó su "Tito Andrónico", su "Julio César", su "Coriolano" y su "Antonio y Cleopatra", y los "Ensayos" de Montaigne en la traducción de John Florio (1603), que moldearon algunos pasajes de "La tempestad". 

En segundo lugar hay que mencionar como fuente de inspiración los "novellieri" (de Mateo Bandello proviene la historia de "Como gustéis" y la de "Romeo y Julieta", que también inspiró "Castelvines y Monteses" de Lope de Vega y "Los bandos de Verona" de Francisco Rojas Zorrilla; de Giambattista Giraldi Cinthio la de "Otelo"; de Giovanni Boccaccio "A buen fin no hay mal tiempo" y de Giovanni Fiorentino "El mercader de Venecia" y "Las alegres comadres de Windsor"; también Chaucer inspira algunas obras) y misceláneas de todo tipo, algunas de ellas españolas, como las "Noches de invierno" de Antonio de Eslava o la "Silva de varia lección" de Pero Mexía. 

En tercer lugar, también se inspiró en la producción dramática inglesa anterior a él, de la que extrajo abundantes argumentos, personajes y principios de composición. A veces incluso rehízo obras enteras (por ejemplo, hubo un "Hamlet" anterior al suyo atribuido a Thomas Kyd, de 1589, que fue un éxito y no se ha conservado, pero inspiró el posterior de Shakespeare). Citó o evocó textos de muchas obras, siendo especialmente sensible al modelo de Christopher Marlowe en sus primeras obras. Este ansia imitativa no pasó desapercibida por su contemporáneo Robert Greene, quien lo tomó por un plagiario y escribió en 1592, aludiendo a una conocida fábula citada por Horacio, lo siguiente:

Por último, Shakespeare estaba también muy versado en mitología (conocía muy bien las "Metamorfosis" de Ovidio) y en retórica, si bien su estilo unas veces rehúye conscientemente las rígidas y mecánicas simetrías de esta última y otras se muestra demasiado jugador del vocablo, como correspondía entonces a la moda conceptista del Eufuismo, difundido por John Lyly y a su vez procedente del estilo de Antonio de Guevara, si bien Shakespeare se pronunció contra los excesos de ese estilo.

Shakespeare reconoció ser un gran asimilador (el poder de síntesis caracteriza a grandes poetas, como también por ejemplo a Dante Alighieri) y lo declaró en su "Soneto LXXVI"; pero también afirmó en este soneto ser capaz de superar a sus modelos convirtiendo en nuevo algo radicalmente viejo, insuflándole nueva vida. En vez de inventar o apelar a la originalidad, tomaba historias preexistentes, como la de "Hamlet", y les otorgaba cuanto les faltaba para la eminencia. Sin embargo, y pese a todo, se mostró además completamente original instalándose algunas veces deliberadamente al margen de toda tradición, como en sus "Sonetos", que invierten todos los cánones del petrarquismo elaborando un cancionero destinado a un hombre y donde se exige, ni más ni menos, el abandono del narcisismo del momento para engendrar la trascendencia de la eternidad por el amor, lo que puede parecer bastante abstracto, pero es que son así de abstractos y enigmáticos estos poemas, cada uno de los cuales encierra siempre un movimiento dramático, una invocación a la acción

Cuando Shakespeare se inició en la actividad teatral, esta se encontraba sufriendo los cambios propios de una época de transición. En sus orígenes, el teatro en Inglaterra era un espectáculo de tipo popular, asociado a otras diversiones extendidas en la época como el "bear baiting" (pelea de un oso encadenado contra perros rabiosos). Sus raíces se encuentran en la etapa tardomedieval, en una triple tradición dramática: los "milagros " o "misterios" ("mystery plays"), de temática religiosa y destinados a solemnizar las festividades de los diferentes gremios; las moralidades u "obras morales" ("morality plays"), de carácter alegórico y representadas ya por actores profesionales: y los "interludios" cortesanos, piezas destinadas al entretenimiento de la nobleza.

Los nobles más destacados patrocinaban grupos de actores que llevaban sus nombres. Así surgieron, en la época isabelina, compañías como The Hundson Men (luego Lord Chamberlain's Men), The Admiral's Men, y The Queen's Men, entre las más relevantes. En ciertas ocasiones, estas compañías teatrales realizaban sus representaciones en el palacio de sus protectores aristocráticos. The King's Men, por ejemplo, después del apadrinamiento de la compañía por el rey Jacobo I, actuaban en la corte una vez al mes. Contar con el respaldo de un mecenas era fundamental para asegurar el éxito de la obra en el futuro.

Las obras se representaban al principio en los patios interiores de las posadas. Todavía en época de Shakespeare algunos de estos lugares continuaban acogiendo representaciones teatrales. Sin embargo, no resultaban muy adecuados para las representaciones, ya que a veces la actividad de la posada llegaba a dificultar las representaciones. Además contaban con la oposición de las autoridades, preocupados por los desórdenes y reyertas que allí se originaban, así como por las «malvadas prácticas de incontinencia» que allí tenían lugar. Estaba también en contra el factor de la higiene: la peste era muy frecuente y las reuniones multitudinarias no fomentaban precisamente la salud.

Por esos motivos fue surgiendo paulatinamente una legislación que regulaba la actividad teatral, y se fue haciendo más difícil conseguir licencias para realizar representaciones en las posadas. Esto propició la construcción de teatros fijos, más salubres, en las afueras de la ciudad, y la consolidación y profesionalización de la carrera de actor. El primer teatro, denominado simplemente The Theatre, se construyó en 1576. Más adelante se construyeron otros: The Curtain, The Rose, The Swan y The Globe. Este último, construido en 1599 y ubicado, como el resto, fuera de la ciudad, para evitar problemas con el Ayuntamiento de Londres, era el más famoso de todos, y fue el preferido de la compañía de la que formó parte William Shakespeare.

Todos estos teatros fueron construidos siguiendo el modelo de los patios de las posadas. Ninguno se conserva en su estado primitivo, pero existe la posibilidad de conocer con cierta aproximación su forma, gracias a algunas referencias de la época. Eran recintos de forma hexagonal u octogonal (hay excepciones) con un escenario medianamente cubierto que se internaba un poco hacia el centro de un arenal al aire libre circundado por dos o tres pisos de galerías. La plataforma constaba de dos niveles, uno a poco más de un metro respecto a la arena, techado y sujeto por columnas, y otro un poco más alto con un tejado en el que se ocultaba el aparato necesario para manejar la tramoya y maniobrar la puesta en escena. Podía llevar una bandera e incluso simular una torre.

Estos teatros tenían un aforo muy respetable. Se ha calculado, por ejemplo, que The Globe podía acoger a alrededor de 2000 espectadores.

En un principio, la condición social de los cómicos, en especial de la de los más humildes, no se distinguía fácilmente de la de un vagabundo o un mendigo. Con el tiempo, sin embargo, gracias a la apertura de los nuevos teatros, los actores de época isabelina fueron alcanzando mayor consideración social.

La rudimentaria escenografía hacía al intérprete cargar con la responsabilidad mayor de la obra, por lo cual su técnica tendía a la sobreinterpretación en lenguaje, gesticulación y llamativa vestimenta. Como las mujeres tenían prohibido subir al escenario, los papeles femeninos se encomendaban a niños o adolescentes, lo cual se prestaba al juego cómico de la ambigüedad erótica. La palabra era muy importante, y el hecho de que el escenario se adelantara algo en el patio acotaba ese lugar para frecuentes monólogos. La ausencia de fondos pintados hacía frecuente que el actor invocase la imaginación del público y el escritor recurriera a la hipotiposis. El público era abigarrado y heterogéneo, y en consecuencia se mezclaban desde las alusiones groseras y los chistes procaces y chocarreros a la más culta y refinada galantería amorosa y la más retorcida pedantería eufuista.

La audiencia acudía al teatro pagando un precio variable según la comodidad del puesto ofrecido. La entrada más barata exigía estar a pie y expuesto a los cambios meteorológicos; las menos asequibles favorecían a la nobleza y a la gente pudiente, que podía tomar asiento a cubierto y a salvo del sol.

El oficio de autor dramático no estaba bien remunerado y todos los derechos sobre las obras pasaban a poder de las empresas que las representaban; por ello las obras sufrían con frecuencia múltiples refundiciones y adaptaciones por parte de varias plumas, no siempre diestras ni respetuosas, por no hablar de los cortes que sufrían a merced del capricho de los actores. El nombre del autor solo se mencionaba (y frecuentemente con inexactitud) dos o tres años más tarde, por lo que los escritores no disfrutaban del fruto de su trabajo, a menos que poseyeran acciones en la compañía, como era el caso de Shakespeare y otros dramaturgos que trabajaban conjuntamente y se repartían las ganancias.

Una de las características más importantes del teatro isabelino, y del de Shakespeare en particular, es la multitud de niveles en las que giran sus tramas. Lo trágico, lo cómico, lo poético, lo terreno y lo sobrenatural, lo real y lo fantástico se entremezclan en mayor o menor medida en estas obras. Las transiciones entre lo melancólico y lo activo son rápidas y, frecuentemente, se manifiestan a través de duelos y peleas en escena que debían de constituir una animada coreografía muy del gusto de la época.

El bufón (en inglés, "fool") es un personaje importante para la obra shakespeariana, ya que le da libertad de expresión y soltura. Se reconocía en él una insuficiencia mental o carencia física que le permitía decir cosas u opinar sobre cuestiones polémicas que habrían sido prohibidas en boca de personajes de mayor fuste. Sin duda esta estratagema era ideal para el autor inglés, puesto que cualquier crítica a la realeza podría ser justificada adjudicándosela a un personaje que no piensa como la generalidad de las otras personas dadas las insuficiencias que padece.

Ante la falta de manuscritos hológrafos y de fechas precisas de composición, se hace muy difícil el establecer una cronología bibliográfica shakespeariana. El "First Folio", que reagrupa la mayor parte de su producción literaria, fue publicado por dos actores de su compañía, John Heminges y Henry Condell, en 1623, ocho años después de la muerte del autor. Este libro dividía su producción dramática en Historias, Comedias y Tragedias, y de él se hicieron 750 copias, de las que han llegado a nuestros días la tercera parte, en su mayoría incompletas. Gracias a esta obra se conservó la mitad de la obra dramática del autor, que no había sido impresa, pues Shakespeare no se preocupó en pasar a la historia como autor dramático.

El "First Folio" recoge exclusivamente obras dramáticas (no se encuentra en la edición ninguno de sus poemas líricos), en número de 36: 11 tragedias, 15 comedias y 10 obras históricas. No incluye algunas obras tradicionalmente atribuidas a Shakespeare, como las comedias "Pericles" y "Los dos nobles parientes", ni la obra histórica "Eduardo III". Mientras que en el caso de "Pericles", parece bastante segura la participación de Shakespeare, no ocurre lo mismo con las otras dos obras, por lo que el número de títulos incluidos en el canon shakesperiano oscila, según las versiones, entre las 37 y las 39.

Al igual que muchas tragedias occidentales, la de Shakespeare suele describir a un protagonista que cae desde el páramo de la gracia y termina muriendo, junto a una ajustada proporción del resto del cuerpo protagónico. Se ha sugerido que el giro que el dramaturgo hace del género, es el polo opuesto al de la comedia; ejemplifica el sentido de que los seres humanos son inevitablemente desdichados a causa de sus propios errores o, incluso, el ejercicio irónicamente trágico de sus virtudes, o a través de la naturaleza del destino, o de la condición del hombre para sufrir, caer, y morir... En otras palabras, es una representación con un final necesariamente infeliz.

Shakespeare compuso tragedias desde el mismo inicio de su trayectoria: una de las más tempranas fue la tragedia romana de "Tito Andrónico", siguiendo unos años después "Romeo y Julieta". Sin embargo, las más aclamadas fueron escritas en un período de siete años entre 1601 y 1608: "Hamlet", "Otelo", "El rey Lear", "Macbeth" (las cuatro principales), y "Antonio y Cleopatra", junto a las menos conocidas "Timón de Atenas" y "Troilo y Crésida".

Muchos han destacado en estas obras al concepto aristotélico de la tragedia: que el protagonista debe ser un personaje admirable pero imperfecto, con un público capacitado para comprender y simpatizar con él. Ciertamente, cada uno de los personajes trágicos de Shakespeare es capaz de ejercer el bien y el mal. La representación siempre insiste en el concepto del libre albedrío; el (anti) héroe puede degradarse o retroceder y redimirse por sus actos. El autor, en cambio, los termina conduciendo a su inevitable perdición.

A continuación se listan las tragedias completas de Shakespeare, ordenadas según la fecha aproximada de su composición:


Entre las características esenciales de la comedia shakesperiana encontramos la "vis" cómica, la dialéctica de un lenguaje lleno de juegos de palabras, el contraste entre caracteres opuestos por clase social, sexo, género o poder (un ejemplo representativo sería "La fierecilla domada", también traducida a veces como "La doma de la bravía"); las alusiones y connotaciones eróticas, los disfraces y la tendencia a la dispersión caótica y la confusión hasta que el argumento de la historia desemboca en la recuperación de lo perdido y la correspondiente restauración en el marco de lo natural. El panorama de la comedia supone además la exploración de una sociedad donde todos sus integrantes son estudiados por igual de forma muy distinta a como es vista la sociedad en sus obras históricas, montadas sobre la persecución maquiavélica del poder («una escalera de arena», a causa de su vaciedad de contenido) y el trastorno del orden cósmico divino que el rey representa en la tierra. Como galería de tipos sociales la comedia es, pues, un espacio más amplio en Shakespeare que el trágico y el histórico y refleja mejor la sociedad de su tiempo, si bien también resalta en este campo el talento del autor para crear personajes especialmente individualizados, como en el caso de Falstaff.

Si bien el tono de la trama es con frecuencia burlesco, otras veces se encuentra latente un inquietante elemento trágico, como en "El mercader de Venecia". Cuando trata temas que pueden desencadenar un trágico desenlace, Shakespeare trata de enseñar, a su modo habitual, sin tomar partido, proponer remedios ni moralizar o predicar en absoluto, los riesgos del vicio, la maldad y la irracionalidad del ser humano, sin necesidad de caer en la destrucción que aparece en sus tragedias y deja a la Naturaleza el orden restaurador y reparador.

Los finales de las comedias son, por lo general, festivos y placenteros. Debe tenerse en cuenta que el lenguaje vulgar y de doble sentido, así como la magnitud de diversos puntos de vista, los cambios de suerte y el trastorno de las identidades, aportan un ingrediente infaltable que suele estar acompañado de sorprendentes coincidencias. La parodia del sexo, el papel del disfraz y el poder mágico de la naturaleza para reparar los daños y heridas ocasionados por una sociedad corrupta y sedienta de codicia son elementos trascendentes en la comedia shakespeariana.

El hombre cambia totalmente su forma de pensar y de actuar al refugiarse en lo salvaje y huir de la civilización, prestándose al juego de oposiciones. Cabe destacar, por último, que la esfera social que Shakespeare utiliza en sus obras es quizás algo más reducida que la que encontramos en la mayor parte de las comedias.

Tal como se ha dicho antes, el bufón —que era un personaje muy popular en la corte de la época— es el elemento inquebrantable sobre el cual el dramaturgo se siente más libre de expresar lo que piensa, teniendo en cuenta que las opiniones de una persona con estas características nunca eran consideradas como válidas —excusa perfecta para explayarse—.

Se estima que la fecha de composición de las comedias de Shakespeare ha de girar en torno a los años 1590 y 1612, como punto de partida y culminación de su labor como escritor. La primera y menos elaborada fue "Los dos hidalgos de Verona", seguida de "El mercader de Venecia", "Mucho ruido y pocas nueces", "Como gustéis","Cuento de invierno", "La tempestad", y otras tantas que se enumeran a continuación:

Es importante dejar en claro que "La tempestad", "Cuento de invierno", "Cimbelino" y "Pericles" son consideradas por muchos fantasías poéticas (en inglés se emplea el término "romance"), dado que poseen características que las diferencian del resto de las comedias.

En el "First Folio" se clasifican como «obras históricas» (en inglés, "histories") exclusivamente las relacionadas con la historia, relativamente reciente, de Inglaterra. Otras obras de tema histórico, como las ambientadas en la antigua Roma, o incluso "Macbeth", protagonizada por un auténtico rey de Escocia, no se clasifican en este apartado. Son once en total (o diez, si se excluye "Eduardo III", modernamente considerada apócrifa). La fuente utilizada por el dramaturgo para la composición de estas obras es bien conocida: se trata de las "Crónicas" de Raphael Holinshed.

A continuación se ofrece una lista de estas obras ordenadas según la fecha aproximada de su composición.


Existen serias dudas sobre la autoría de la primera de la lista, "Eduardo III". De la última, "Enrique VIII", se cree que fue escrita en colaboración con John Fletcher, quien sustituyó a Shakespeare como principal dramaturgo de la compañía "King's Men".

Dentro del conjunto de sus obras históricas, se suelen agrupar la decena que escribió sobre los reyes ingleses, conocido como el «Ciclo de Historia», que Shakespeare dedicó a siete reyes ingleses. Este ciclo excluye las obras sobre el rey Lear (un rey legendario) y Macbeth (basado en la vida del rey escocés, Macbeth de Escocia) y una obra sobre Edward III (aunque hay cada vez más indicios de que fuera escrito por Shakespeare, al menos en parte, no se ha podido establecer su autoría). Este Ciclo exluye, por no seguir la secuencia histórica, a "El rey Juan" y a "Enrique VIII".

Ocho de estas obras están agrupadas en dos tetralogías cuyo orden de escritura no coincide con el orden cronológico de los acontecimientos históricos reflejados. La primera de estas tetralogías está formada por las tres dedicadas al reinado de Enrique VI (1422-1461), junto con la consagrada al ambicioso y terrible Ricardo III (que reinó en el período 1483-1485). Todas ellas fueron compuestas con toda probabilidad entre 1590 y 1594.

La segunda tetralogía, formada por "Ricardo II", las dos partes de "Enrique IV" y "Enrique V", retrocede en el tiempo. Se centra en los reinados de Ricardo II (1377-1399), Enrique IV (1399-1413) y Enrique V (1413-1422). Todas estas obras fueron compuestas en el período 1594-1597.

Habida cuenta de que gran parte del público era analfabeto, estas obras representaban una buena forma de comunicar la historia y fomentar, consecuentemente, el patriotismo y el amor por la cultura inglesa, así como de inculcar un sentimiento de rechazo hacia las guerras civiles. Además de brindar entretenimiento, las obras históricas reafirmaban y justificaban el poder de la monarquía ante quienes pudieran poner en cuestión su legitimidad. En el teatro de Shakespeare, el rey, como en la obra dramática de Lope de Vega, es el representante del orden cósmico en la tierra. Esto es lo que más tarde analizarían académicos de la talla de Greenblatt, centrándose en el discurso imperante y en la capacidad del teatro isabelino para asentar la autoridad real, mantener el orden y desalentar la subversión.

Dada la dependencia de las compañías teatrales con respecto de sus patrocinadores aristocráticos (y, en el caso de The King's Men, de la autoridad real), es lógico que se escribieran y representaran obras protagonizadas por personajes histórico pertenecientes a la nobleza y relevantes en la historia de Inglaterra. Es el caso de Enrique V, vencedor en la batalla de Agincourt de las tropas de Francia, la sempiterna rival de Inglaterra. Retomando hechos históricos destacados, obviando derrotas y exagerando el heroísmo de la victoria —que se atribuía al monarca reinante—, estas obras lograban que se acrecentase la devoción popular hacia la corona.

En los comienzos de la dramaturgia shakesperiana, la finalidad era legitimar la autoridad de la dinastía Tudor, entronizada en 1485, precisamente tras el derrocamiento de Ricardo III, uno de los personajes más abominables del teatro shakesperiano. La subida al trono de los Tudor había despertado ciertos recelos, tanto debido a su origen galés como a lo problemático de sus derechos al trono (aparentemente, Enrique VII, primer monarca de la dinastía, fundamentaba sus derechos en ser descendiente de la princesa francesa Catalina, viuda de Enrique V, que se volvió a casar unos años más tarde con Owen Tudor, un noble galés poco influyente en el ámbito de la monarquía nacional.)

No obstante, existen críticos que opinan que las obras históricas de Shakespeare contienen críticas veladas hacia la monarquía, disimuladas para evitar posibles problemas con la justicia.

Las narraciones caballerescas escritas en prosa o verso eran un género de fantasía heroica muy común en Europa desde la Edad Media hasta el Renacimiento; los libros de caballerías en inglés, francés, español, italiano y alemán podían contener además mitos artúricos y leyendas celtas y anglosajonas; también intervenían en ellos la magia y la fantasía, y era además perceptible la nostalgia por la pérdida mitología precristiana de hadas y otras supersticiones. Esta narrativa legendaria, cuya última expresión y obra maestra fue acaso "La muerte de Arturo" de "sir" Thomas Malory, se había convertido ya en algo alternativo y popular, identificado con las lenguas vernáculas frente a una narrativa más moralizante de carácter cristiano, vinculada al ámbito eclesiástico, para un público más selecto y en latín. Para definir este tipo de contenidos populares se escogió la denominación de lo "romantic" o "novelesco".

En Gran Bretaña, a fines del siglo XVI y comienzos del XVII, el "romance" se erigió como un género fantástico en el que, además de seguirse unas convenciones características (caballero con poderes especiales, magia, brujería, alteración de la realidad, cortejo de la figura femenina, hazañas y arriesgadas aventuras), se añadía el hecho de la conquista de América: un crisol de razas y culturas bárbaras que servía de inspiración para muchos viajeros y dramaturgos. En William Shakespeare, la obra que reúne todas las susodichas convenciones y las plasma en una producción teatral tan interesante como irreal es "La tempestad", considerada el testamento dramático de Shakespeare porque fue probablemente su última obra.

Se representó por primera vez en 1611 y tuvo una segunda puesta en escena hacia febrero de 1613 con motivo de la boda de Isabel Estuardo, hija del rey Jacobo I, con el príncipe Frederick de Heidelberg. En la pieza pueden hallarse no pocos paralelismos con las figuras más destacadas del período jacobino: la máscara nupcial que Próspero crea para el disfrute de Miranda y Ferdinando se corresponde con las figuras divinas de Ceres y Juno, auspiciando un dichoso porvenir si la feliz pareja prometía guardar castidad hasta después del matrimonio. Esto podría haberle sentado muy bien al monarca, tan conocido por el rigor de su moral tradicional como por su morboso interés por la magia y la brujería, que también tienen lugar importante en la obra. En efecto, estas prácticas motivaron en la época la quema de mujeres entre los siglos XVI y XVIII y Jacobo I sentenciaba sin vacilar a muerte a todas aquellas personas que estuvieran bajo mera sospecha de llevar a cabo este tipo de ceremonias. La temática de "La tempestad" no podría menos, pues, que manifestarse en un monarca —Próspero— interesado en acabar con el maleficio de una vieja bruja, que acechaba con irrumpir en el orden social de la isla. El mundo mágico propio de esta época reaparece sin embargo en otras comedias novelescas y fantásticas de la última época de Shakespeare, como son:


Se considera que "La tempestad" es el testamento dramático de Shakespeare. Al parecer inspirada en una de las "Noches de invierno" de Antonio de Eslava, el príncipe Próspero náufrago en una isla, semihumano y semidivino por sus poderes mágicos, rompe al final su varita al reflexionar sobre su limitado poder, y resulta casi imposible no poner sus palabras en boca del mismo Shakespeare:

Algunas de las obras que Shakespeare escribió con John Fletcher se han perdido, por ejemplo "Cardenio", inspirada en un episodio del "Don Quijote de La Mancha" de Miguel de Cervantes, o "Los dos nobles caballeros" (1613), que fue registrada en el "Quarto" hacia 1637; como esta última obra no se incluyó en el "First Folio", muchos lectores cuestionan la autoría del dramaturgo en la misma. Por otro lado, y en vista de las vicisitudes que presentan muchas de las producciones shakespearianas, hay quienes sostienen que la mitad de ellas se ajustarían más bien al perfil y al estilo de Fletcher.

Shakespeare posee, al igual que todos los grandes poetas, un gran poder de síntesis; escribía con todo el idioma y contaba con un léxico matizado y extensísimo. Cuidó la estilización retórica de su verso blanco, con frecuencia algo inserto en la tradición conceptista barroca del Eufuismo, por lo que en la actualidad es bastante difícil de entender y descifrar incluso para los mismos ingleses; rehuyó sin embargo conscientemente las simetrías retóricas, las oposiciones demasiado evidentes de términos; el idioma era entonces una lengua proteica y los significados de las palabras no estaban todavía fijados con claridad por repertorios léxicos. Si su trabajadísimo lenguaje es y solía ser (y lo era incluso cuando Voltaire atacó en sus "Cartas inglesas" las hinchazones anticlásicas de su estilo) un impedimento para apreciar la obra del autor, también es cierto que es el asiento sobre el que reposa su fama y prestigio como pulidor de metáforas e inventor de neologismos comparables a los de otros dramaturgos y poetas de su época de renombrada trayectoria, como los españoles Miguel de Cervantes, Lope de Vega, Francisco de Quevedo y Luis de Góngora.

En líneas generales, la crítica ha destacado sobre todo dos aspectos de la obra dramática de William Shakespeare.

En primer lugar, una indiferencia y distanciamiento casi inhumanos del autor respecto a la realidad de sus personajes, que comparte asimismo con la mayor introspección y profundización en la creación de su psicología. Shakespeare no moraliza, no predica, no propone fe, creencia, ética ni solución alguna a los problemas humanos: plantea, y lo hace mejor que nadie, algunas de las angustias fundamentales de la condición humana (ser o no ser, la ingratitud, sea filial "(El rey Lear)" o no, la ambición vacía), pero nunca da respuestas: no sabemos qué pensaba Shakespeare, al que el espectáculo del mundo le trae al fresco, por más que su visión de fondo sea pesimista y sombría ante la posición miserable y mínima que ocupa un hombre hecho de la misma materia que los sueños en un universo misterioso, profundo, inabarcable y sin sentido. Mientras que el teatro barroco español privilegia lo divino sobre lo humano, Shakespeare reparte por igual su temor (o, más exactamente, su maravilla) ante lo celeste y ante lo terrenal:

Porque Shakespeare está abierto a todo, no se impone ningún límite religioso, ético ni filosófico; hace decir a Julio César que «de todas las maravillas que he oído, la que más extraña me parece es el que los hombres tengan miedo» y en todo caso solo se puede tener miedo «del miedo que tienen los demás».

Alguna vez la crítica ha señalado en su obra el hilo constante de la misantropía y, por otra parte, solo un cósmico distanciamiento ante todo lo divino y lo humano es capaz de acuñar frases como esta:

O bien:

En segundo lugar, la crítica ha destacado el extraordinario poder de síntesis del "Cisne de Avon" como lírico; su fantasía es capaz de ver un universo en una cáscara de nuez; como creador de personajes, cada uno de ellos representa en sí mismo una cosmovisión, por lo cual se le ha llamado "Poet's poet" (poeta de poetas). Son auténticas creaciones Ricardo III, Hamlet, Otelo, Bruto, Macbeth, lady Macbeth, Falstaff... Sin embargo, y por eso mismo, se le han hecho también algunos reproches: los personajes de sus obras parecen autistas, no saben escucharse y permanecen cerrados en su mundo a toda comprensión profunda del "otro". ¿Qué simpatía existe entre Hamlet y su pobre y torturada novia Ofelia? ¿Se han "escuchado" alguna vez Marco Antonio y Cleopatra, quienes, a pesar de ser amantes, desconfían patológicamente el uno del otro? El crítico Harold Bloom ha señalado esto como una de las diferencias más notables y sensibles entre Shakespeare y Cervantes. En este último existe empatía, amistad y conexión humana entre sus personajes, de forma que estos aprenden de los demás y evolucionan, mientras que los autistas personajes trágicos de Shakespeare son incapaces de comprenderse y realizar este humano acercamiento.

El estudio de Shakespeare ha sido abordado desde muy diferentes perspectivas. En un primer momento, el historicismo analizó su obra desde un punto de vista histórico y externo, focalizando su atención en lo extraliterario. Como reacción, el neocriticismo se decantó más por el análisis de la obra en sí misma, prescindiendo de todo elemento extraliterario. El principal exponente de esta escuela crítica fue Stephen Greenblatt.

En años recientes, han cobrado cierto auge en medios académicos los estudios de Shakespeare desde una perspectiva feminista, duramente criticados por autores como Bloom.

Fuera de ser un dramaturgo de incuestionable importancia, Shakespeare fue también poeta y sonetista, y se cree generalmente que él mismo se valoraba más como lírico que como autor dramático y solamente como tal esperaba perdurar a su tiempo. Aunque escribió sobre todo poemas extensos narrativos y mitológicos, se le recuerda especialmente como un excepcional autor de sonetos puramente líricos.

La primera mención de estos últimos se halla en el "Palladis Tamia (Wit's Treasury)" (Londres, 1598) del bachiller en Artes por Cambridge Francis Meres, quien alaba a Shakespeare por sus "sonetos de azúcar"; esta mención demuestra que circulaban copias manuscritas de los mismos entre sus amigos íntimos por esas fechas:

Poco después, en 1599, algunos de sus sonetos, el 138 y el 144, más tres incluidos en su comedia "Trabajos de amor perdidos", salieron impresos (con numerosas variantes respecto a las ediciones posteriores) en una colección de poesías líricas intitulada "El peregrino apasionado", una miscelánea de varios autores falsamente atribuida en su integridad al Cisne del Avon y que incluye entre sus otros sonetos otros ocho que le han sido asignados con bastante fundamento por razones de estilo y contenido. Solamente en 1609 apareció una misteriosa edición completa, seguramente sin el permiso de su autor, por parte de un tal T. T. (Thomas Thorpe, un editor amigo de escritores y escritor él mismo). La dedicatoria es a un tal Sr. W. H.

No hay forma de establecer con justeza la identidad oculta tras esas iniciales y se han barajado distintas teorías sobre el personaje que se esconde tras ellas; lo más probable es que fuese cualquiera de los habituales mecenas del poeta y la gran mayoría de los críticos se inclina por pensar que las siglas están invertidas y se trata de Henry Wriothesley (1573), conde de Southampton, ya que Shakespeare ya le había expresado públicamente su aprecio con dedicatorias de otros poemas: "Venus y Adonis" y "La violación de Lucrecia". Pero otro posible candidato y tan verosímil como el anterior es William Herbert, conde de Pembroke e hijo de Mary Herbert, hermana de "sir" Philip Sidney, el famoso poeta que compuso "La Arcadia"; en favor de este último cuenta también que le poseía una intensa devoción por el teatro y fue patrón de los King’s Men, la compañía teatral de Shakespeare. Ambos eran nobles apuestos y dedicados al mecenato del arte y las letras, y bastante más jóvenes que el poeta, requisitos que debe cumplir cualquier verdadero destinatario de los poemas.

El orden establecido por la edición de Thorpe ha consagrado una peculiar estructura muy diferente a la habitual del italianizante cancionero petrarquista; en efecto, no hay composiciones en otros metros que rompan la monotonía, la métrica es muy diferente a la del soneto clásico (se trata de tres serventesios y un pareado, el llamado soneto shakespeariano) y está consagrada en su mayor parte a la amistad (o amor) de un hombre, al que interpela frecuentemente para que cree su propia imagen y semejanza:

Se instala, pues, en una tradición completamente renovada y original, y el propio poeta era irónicamente consciente de ello:

Puede dividirse en dos series sucesivas de sonetos: una de 126, que celebra a un amigo rubio y bien parecido de alta alcurnia, mecenas del poeta, al que propone que deje la soledad, el narcisismo y los placeres y engendre herederos, y los 28 últimos, que conciernen a una mujer morena, que se hallaba casada, como se infiere de una alusión del soneto 152, y seguramente era un amujer instruida, ya que sabía tocar la espineta o clavecín. Dos de los sonetos se consideran aparte, pues son versiones de un mismo epigrama de la "Antología griega".

Por otra parte, aparece también y ocasionalmente, en el trío formado por Shakespeare, el enigmático destinatario y la dama morena, un poeta rival, hecho que complica todavía más la historia de un amor que en la lengua de la época podía entenderse también como amistad o como ese tipo especial de dilección que se establece entre un poeta y su mecenas. Los expertos (William Minto, seguido después por Edward Dowden, Tylor y Frederick Furnivall) sostienen en su mayoría que este poeta era el helenista George Chapman, ya que se le identifica como autor de alejandrinos, versos entonces bastante raros en la métrica inglesa y que solo utilizaba por entonces tal autor.

Los temas de los "Sonetos" son el amor y el tiempo, de alguna forma contrapuestos; en este último tema se profundiza en lo que se refiere a la fugacidad, llegándose a veces a lo metafísico. Cada soneto contiene también un movimiento dramático; se aprecia además en su lectura, sobre todo, el valor moral y espiritual del mensaje y la filosofía que nos deja: aprovechar el escaso tiempo que la vida nos depara para entregarse de fondo a ella. Claudio Guillén, además señala que "Shakespeare se atreve a decir cosas nuevas, completamente nuevas, como la no diferencia entre la amistad y el amor y, también, la no diferencia esencial entre el amor a la mujer y al hombre". 

La cronología de los sonetos es difícil de establecer, pero se conjetura que fueron compuestos entre 1592 y 1597.


Cada época histórica ha primado determinadas obras según las preocupaciones e intereses imperantes. El concepto de «justicia poética» que prevaleció en el siglo XVIII provocó el rechazo de muchas de las tragedias de Shakespeare, ya que según sus criterios el teatro debía promover ejemplos de virtud. El crítico inglés Samuel Johnson (1709-1784) no aceptó el desenlace del "Rey Lear", que consideró cruel e innecesario, y la versión de 1681 de Nahum Tate sustituyó a la de Shakespeare hasta mediados del siglo XIX, asombrando con su gran éxito al público lector: en ella hay un final feliz en el que Cordelia y Lear consiguen triunfar sobre los obstáculos, y la protagonista se casa con Edgardo, legítimo heredero del conde de Gloucester.

En 1731 el famosísimo actor David Garrick (1717-1779) apareció por vez primera encarnando al personaje del jorobado Ricardo III en la escena de un teatro de los arrabales de Londres y cosechó un éxito clamoroso. Al hacerle cargo después de la dirección del elegante teatro Drury Lane desencadenó con su deslumbrante actuación una verdadera «shakespearemanía» que llegó a su culmen al celebrarse en Stratford (1769), organizado por el mismo Garrick, el primer jubileo en honor al poeta, acto que promovió tal entusiasmo que bajo su signo incluso Irlanda llegó a exhumar falsos documentos literarios atribuidos a Shakespeare. Hacia 1772 Garrick modificó buena parte de "Hamlet" al suprimir la escena de los sepultureros y eximir a Laertes de toda culpa referente al veneno que portaba en su espada. Es más, la reina Gertrudis consigue sobrevivir para llevar una vida de arrepentimiento, lo que no ocurre en el original. Simultáneamente, la fama del dramaturgo se propagó por toda Europa; Voltaire lo dio a conocer en sus "Cartas de Inglaterra" y Jean-François Ducis lo introdujo en la escena parisina al representar por vez primera su adaptación de "Hamlet" (1769); en este mismo año Gotthold Ephraim Lessing publicaba en Alemania su "Hamburgische Dramaturgie", una colección de estudios críticos teatrales en los que propugnaba el repudio de la tragedia francesa clásica y el nuevo encumbramiento de Shakespeare, tal como haría Johann Gottfried Herder en 1771 en sus "Blättern von Deutscher Art und Kunst". Como Garrick en Inglaterra, Friedrich Ludwig Schröder, actor y director teatral, contribuyó con su primera representación del "Hamlet" en Alemania (1777) a hacer vivir a Shakespeare en la escena de aquel país. Goethe representó piezas de Shakespeare y Calderón en Weimar cuando le encargaron dirigir el teatro del principado y él mismo y Friedrich von Schiller experimentaron el influjo del genio inglés en sus propias obras teatrales. En España, Ramón de la Cruz tradujo el "Hamleto" en 1772 y Leandro Fernández de Moratín en 1798.

El renacimiento de Shakespeare (así como el de Pedro Calderón de la Barca) fue para la historia europea del teatro un hecho decisivo ya que favoreció la llegada del Prerromanticismo y al mismo tiempo hizo factible la aparición del drama nacional alemán y posteriormente del drama romántico francés de Víctor Hugo.

En 1807 Thomas Bowdler publicó "Family Shakespeare", una versión modificada para hacerla, según su criterio, más apta para mujeres y niños, que no pudiese «ofender a la mente virtuosa y religiosa». Esta adaptación dio origen a la palabra inglesa "bowdlerize", que designa a la censura puritana.

En época victoriana, las representaciones se caracterizaron en general por el intento arqueológico de reconstruir una época y los montadores y actores estaban obsesionados por el realismo histórico según la metodología documentalizante del positivismo. También las Vanguardias artísticas afectaron al dramaturgo: Gordon Craig intentó hacer un "Hamlet" cubista en 1911. Su concepción iconoclasta del escenario abrió la vía para diversas revisiones estéticas de las obras de Shakespeare en el siglo XX. En 1936, Orson Welles montó un "Macbeth" innovador en Harlem, transponiendo no solo la época de la obra sino empleando también actores afroamericanos. El largometraje de Laurence Olivier, "Enrique V", filmado en honor a los combatientes de la Segunda Guerra Mundial, hizo que determinados pasajes fueran resaltados para animar el patriotismo británico; el más significativo fue la arenga del monarca a sus tropas antes de la batalla de Agincourt contra las tropas francesas. Lo mismo cabe decir sobre innúmeras adaptaciones teatrales y cinematográficas hasta estas mismas fechas.

Así pues, la adaptación, interpretación y retorsión de la obra shakespeariana fue durante largo tiempo el producto de unos intereses morales, políticos y estéticos concretos, y escamotearon la sombría concepción de la vida que ofrece genuinamente Shakespeare.

En lo que concierne a su influencia sobre otras culturas, y la hispana en concreto, Shakespeare fue siempre una caudalosa fuente de inspiración para escritores modernos y contemporáneos, pero no llegó a dejarse notar verdaderamente hasta el siglo XIX. En Hispanoamérica autores como Rubén Darío y en particular el ensayista José Enrique Rodó leyeron con especial interés "La tempestad". Rodó, por ejemplo, articuló en su conocido ensayo "Ariel" (1900) toda una interpretación de América sobre los mitos de dos de sus personajes principales, Ariel y Calibán.

Pero su coronación como autor de la Literatura universal debió esperar en España hasta fines del siglo XVIII, cuando Voltaire suscitó entre los ilustrados españoles cierta curiosidad por el autor inglés a través de lo que dijo de él en sus "Cartas inglesas"; Ramón de la Cruz tradujo el "Hamleto" en 1772 desde la reducción en francés de Jean-François Ducis (1733-1816), quien había adaptado traducciones francesas de las tragedias de Shakespeare al verso sin saber inglés según los gustos del Neoclasicismo y eliminando el final violento, entre otros retoques. Esta traducción, sin embargo, no llegó a publicarse. Por el contrario Leandro Fernández de Moratín sí llegó a imprimir la suya, también desde la mala versión francesa de Ducis, acumulando a las de su modelo otras deficiencias (Madrid: Villalpando, 1798).

Hubo otras versiones de obras sueltas ("Otelo", 1802, traducción de Teodoro de la Calle desde la versión francesa de Ducis; "Macbé ó Los Remordimientos", 1818, por Manuel García, también desde la versión francesa de Ducis), pero solamente se emprendieron esfuerzos globales de traducción de toda la obra del autor en la segunda mitad del siglo XIX, empresas sin duda espoleadas por el prestigio que había alcanzado el autor con los elogios sin tasa que le prodigó el Romanticismo alemán.

1872 fue un año fundamental en la recepción española de Shakespeare. Se editan las primeras traducciones directas desde el inglés: "Obras de William Shakspeare trad. fielmente del... inglés con presencia de las primeras ediciones y de los textos dados á luz por los más célebres comentadores del inmortal poeta", Madrid, 1872-1877 (Imp. Manuel Minuesa, R. Berenguuillo). La traducción es de Matías de Velasco y Rojas, marqués de Dos Hermanas, pero no pasó de tres volúmenes; el segundo y el tercero se imprimieron en 1872, el primero con sus poemas y sonetos, el segundo con "El Mercader de Venecia" y el tercero con "Julieta y Romeo".

Entre 1872 y 1876 Jaime Clark tradujo "Romeo y Julieta"; "Hamlet"; "Otelo"; "Rey Lear"; "El mercader de Venecia"; "Como gustéis"; "Noche de Reyes" y "La tempestad". En 1873, el gibraltareño Guillermo Mcpherson empezó a imprimir su traducción de 23 obras en endecasílabo blanco, provistas de importantes prólogos.

Por otra parte, de 1872 a 1912, menudearon las representaciones de sus obras en Madrid; Shakespeare aparece incluso como personaje en "Un drama nuevo" de Manuel Tamayo y Baus, aunque ya había aparecido como tal en el drama de Enrique Zumel "Guillermo Shakespeare" (Granada: José María Zamora, 1853). Del mismo modo, la crítica española emprendió por primera vez el estudio en profundidad de Shakespeare; fueron los primeros Guillermo Macpherson y su amigo el gaditano Eduardo Benot (1885) y especialmente Eduardo Juliá Martínez (1918), quien aprovechó la fecha de centenario para divulgar la figura de Shakespeare con una especie de biografía novelada que, bajo el título "Shakespeare y su tiempo: historia y fantasía" (1916), pretendía exponer "verdades entre las apariencias del entretenimiento" (p. xii). La obra está bien documentada, como reflejan la caudalosa anotación y los apéndices finales (281–331), que son con mucho lo más sustancioso de la obra; tras esto escribió Juliá su interesante "Shakespeare en España" (1918), que sirvió de base a la obra homónima de Alfonso Par. Este tradujo, entre otras piezas dramáticas, "King Lear" al catalán y al castellano. En 1916, coincidiendo con el tercer centenario de la muerte del dramaturgo, escribió en catalán "Vida de Guillem Shakespeare", que apareció en castellano en 1930, y en este mismo año "Contribución a la bibliografía española de Shakespeare"; su dedicación se verá coronada con dos obras colosales, una publicada en 1935, "Shakespeare en la literatura española", en dos volúmenes, y otra al año siguiente, la póstuma "Representaciones shakespearianas en España", también en dos volúmenes. También hay que señalar aquí a otro estudioso español de Shakespeare, Ricardo Ruppert y Ujaravi (1920), al escritor del Realismo Juan Valera y a miembros de la Generación del 98 cuales Miguel de Unamuno y Valle-Inclán, que dedicaron algunos ensayos al Cisne del Avon.

Entre las traducciones, sobresalen las obras completas en ocho volúmenes del ya citado Guillermo Macpherson (1885-1900), con sus correspondientes introducciones. También ocupan un lugar privilegiado las "Obras completas de Shakespeare" de Rafael Martínez Lafuente, aunque muy probablemente son retraducciones desde el francés, pues recogen en su prólogo fragmentos de los ensayos de Víctor Hugo sobre la vida y obra del dramaturgo que precedió a una traducción francesa. Ya comprende la obra entera, e incluso los títulos atribuidos, la versión de Luis Astrana Marín en prosa, entre 1920 y 1930, que fue muy leída por Federico García Lorca; compuso además Astrana una biografía que reeditó ampliada y realizó un estudio de conjunto sobre su obra que puso como introducción a su monumental edición. Son asimismo dignas de mencionarse las traducciones y adaptaciones llevadas a cabo por los simbolistas Antonio Ferrer y Robert ("Macbeth", 1906); "La fierecilla domada" por Manuel Matoses (1895); "Noche de Epifanía" (1898) y "El Rey Lear" (1911) por Jacinto Benavente; "Romeo y Julieta" (1918) y "Hamlet" (1918) por Gregorio Martínez Sierra. Una apreciable cifra de estudios y traducciones utilizados y acumulados por William Macpherson y Rafael Martínez Lafuente pueden asimismo encontrarse en la Biblioteca del Ateneo de Madrid.

Entre las traducciones modernas, fuera de la famosa y ya citada de Luis Astrana Marín en prosa, hay que señalar las "Obras completas" de José María Valverde (Barcelona: Planeta, 1967), también en prosa, y las ediciones bilingües con versión española en verso blanco realizadas por el Instituto Shakespeare de Valencia, consagrado por entero a este empeño desde 1980 bajo la dirección de Manuel Ángel Conejero y Jenaro Talens. Notables son también las versiones realizadas de algunas obras por el más importante de los trágicos españoles de la segunda mitad del siglo XX, Antonio Buero Vallejo. Asimismo, Ángel Luis Pujante ha emprendido una nueva traducción de sus obras completas para Editorial Espasa-Calpe desde 1986.

Por último, en la Universidad de Murcia se ha creado en línea una base de datos con los textos de todas las traducciones de textos históricas de Shakespeare al español, cinco biografías del autor, materiales complementarios y la bibliografía confeccionada por Ángel-Luis Pujante y Juan F. Cerdá "Shakespeare en España. Bibliografía anotada bilingüe / Shakespeare in Spain. An Annotated Bilingual Bibliography", Murcia/Granada: Universidad de Murcia & Universidad de Granada, 2000-2014.

Entre las versiones cinematográficas de la biografía shakesperiana destaca "Shakespeare in Love" (1998) dirigida por John Madden, "Miguel y William", 2007, de la directora y guionista Inés París sobre Miguel de Cervantes y Shakespeare y "Anonymous" (2011) dirigida por Roland Emmerich que plantea una posible respuesta sobre la autoría de sus obras en el seno de un complot político.

Se han producido unas 250 películas basadas en textos de Shakespeare, lo cual demuestra la enorme influencia de la obra de este escritor. La obra más veces llevada a la pantalla es "Hamlet", con 61 adaptaciones al cine y 21 series de televisión entre 1907 y 2000.
Algunas películas basadas en obras de Shakespeare son las siguientes:










</doc>
<doc id="5011" url="https://es.wikipedia.org/wiki?curid=5011" title="Luis Buñuel">
Luis Buñuel

Luis Buñuel Portolés (Calanda, Teruel, España; 22 de febrero de 1900-Ciudad de México, 29 de julio de 1983) fue un director de cine español, que tras el exilio de la guerra civil española se naturalizó mexicano.A pesar de los hitos cinematográficos logrados en su país natal con "Viridiana" (1961) y "Tristana" (1970), la gran mayoría de su obra fue realizada o coproducida en México y Francia, debido a sus convicciones políticas y a las dificultades impuestas por la censura franquista para filmar en España. Es considerado uno de los más importantes y originales directores de la historia del cine.

Nació en la localidad turolense de Calanda el 22 de febrero de 1900. Su padre, Leonardo Manuel Buñuel González, originario del mismo pueblo, donde tenía un negocio de ferretería y armas, había conseguido una pequeña fortuna en Cuba y en 1898, al estallar la guerra hispano-estadounidense, liquidó sus negocios y volvió a su pueblo natal, donde se casó el 10 de abril de 1899 con María Portolés Cerezuela, de diecisiete años, veintiocho más joven que él, con la que tuvo siete hijos: Luis (1900), María (1901), Alicia (1902), Concepción (1904), Leonardo (1907, pediatra y radiólogo), Margarita (1912) y Alfonso (1915), este último arquitecto y diseñador con inquietudes artísticas que destacó como autor de "collages" surrealistas.

A los tres años del nacimiento de su primogénito, la familia se trasladó a vivir a Zaragoza y a partir de entonces pasó a repartir sus vacaciones entre Calanda (donde regresaban en Semana Santa y a veranear) y en ocasiones a San Sebastián. En una de estas ocasiones, cuando contaba con dieciséis años, le presentaron en un baile a una jovencísima Concha Méndez, dos años mayor que él, y que por entonces también pasaba los veranos en San Sebastián, con la que inicia un noviazgo que durará siete años. Así deja constancia la escritora del 27 en unas grabaciones registradas en 1981 depositadas en la fonoteca de México: "Un verano en San Sebastián, conocí a un chico aragonés que me presentó, en uno de los bailes, a otro chico, que resultó ser Luis Buñuel, el director de cine. En aquel tiempo este solo se interesaba por los insectos. Y nos pusimos de relaciones. (...) estuvimos juntos siete años." Una relación de la que conocemos pocos detalles porque Buñuel siempre la mantuvo en la sombra y se negaba a que Concha conociera a todos sus amigos, como sigue relatando ella misma en las grabaciones: "Nunca nos reunimos juntos con los chicos de la Residencia de Estudiantes. (...) Me hablaba de ellos, pero nunca me los presentó. Me preguntó cómo podía conciliar ambos mundos; uno más frívolo, nuestra vida en común, y el otro artístico, en el que se filtraban ya rasgos surrealistas."
Luis pasó toda su infancia y adolescencia en Zaragoza, donde cursó la educación primaria y secundaria, primero en Corazonistas (con mayoría de franceses) y en 1908, durante siete años, en el colegio jesuita de El Salvador, al comienzo del paseo de la Constitución, donde hoy se encuentra la sede principal de Ibercaja, cerca de la plaza de Aragón; como alumno a media pensión, no vestía el uniforme completo de los internos, sino solamente la gorra con un galón. Sus notas eran generalmente excelentes.
Lo que se sabe sobre las primeras películas que vio procede de las declaraciones del propio Buñuel, y son imprecisas y contradictorias. En 1975 dijo a Pérez Turrent y José de la Colina que había visto de niño «cine parlante y en colores, en la sala Coine [sic], de Zaragoza», aludiendo al cinema parlante que Ignacio Coyne Lapetra regentó entre 1905 y 1909; recordaba una película donde «se veía un cerdo, con faja de comisario de policía y sombrero de copa, cantando una canción. Era un dibujo animado con colores muy malos que salían de las figuras, y el sonido venía de un gramófono», pero también a los mismos autores les comunicó que en la primera película que vio había un asesinato cruento. Por otro lado, en sus memorias, tituladas "Mi último suspiro", afirmaba que en 1908 asistió por primera vez al cine Farrucini [sic], que remite a la barraca de feria Nuevo Metensmograf Cinematógrafo Farrusini del feriante barcelonés Enric Farrús, quien se estableció en Zaragoza en 1908 al calor de la Exposición Hispano-Francesa de ese año que conmemoraba el centenario de los Sitios de Zaragoza. Recordaba, asimismo, haber visto en esa época muchas películas cómicas de André Deed, que en España era conocido como Toribio, y el "Viaje a la Luna" de Georges Méliès.

En Calanda daba funciones con un teatrillo de personajes de cartón que sus padres habían comprado en París y espectáculos de sombras chinescas con una linterna mágica. Acudía regularmente al teatro y a la ópera, pues los Buñuel tenían, como familia acomodada que era, palco en abono en el Principal, uno de los cuatro que entonces había en la capital aragonesa. Su niñera le llevaba también al teatro Circo que ofrecía comedias, dramas de detectives, melodramas, farsas, sainetes y zarzuelas; posiblemente allí contemplaría una opereta basada en "Los hijos del capitán Grant", que Buñuel tenía como uno de sus mejores recuerdos, por la espectacularidad de su escenografía. Ya de adolescente, en 1915, asiste en el teatro Principal a numerosas funciones de teatro y ópera "La vida es sueño", "El alcalde de Zalamea", "Don Álvaro o la fuerza del sino", "La Favorita", "Lucía de Lammermoor", el "Fausto" de Gounod, "Rigoletto", "El barbero de Sevilla", "Carmen"...

Desde los diez o doce años comenzó a tocar el violín y a estudiarlo desde los trece. Al año siguiente salió por primera vez de Aragón y viajó a Vega de Pas (Cantabria) y San Sebastián, donde veranearía a menudo. En 1915 fue expulsado por los jesuitas del colegio y se matriculó en el Instituto de Enseñanza Media de Zaragoza (más tarde llamado «Goya») como alumno libre. En esa época leyó "El origen de las especies", de Darwin, además de libros de la nutrida biblioteca de su padre Leonardo, como el "Jean-Christophe" de Romain Rolland, obras de los librepensadores franceses Rousseau, Diderot o Voltaire y clásicos españoles como Quevedo o Benito Pérez Galdós, además de novelas de detectives (Nick Carter, Dick Turpin) y una novela que le dejará huella: "Robinson Crusoe".

A los diecisiete años, terminado el bachillerato, partió a Madrid para cursar estudios universitarios. En la capital se alojó en la recién creada Residencia de Estudiantes, fundada por la Junta para la Ampliación de Estudios, heredera del espíritu del krausismo pedagógico y la Institución Libre de Enseñanza, donde permaneció siete años. Su propósito, inducido por su padre, era estudiar Ingeniería Agrónoma. En esta época se interesó por el naturismo y llevó una alimentación y vestimenta espartanas, gustando de lavarse con agua helada. Tomó parte en las actividades del cine-club de la Residencia y trabó amistad, entre otros, con Salvador Dalí, Federico García Lorca, Rafael Alberti, Pepín Bello y Juan Ramón Jiménez. También participó en las tertulias ultraístas y, todos los sábados desde 1918 hasta 1924, en las del Café Pombo, dirigidas por Ramón Gómez de la Serna.

En 1920 inició, con el doctor Ignacio Bolívar, estudios de entomología, que abandonó para matricularse en Filosofía y Letras, rama de Historia, ya que se había informado de que varios países ofrecían trabajo como lector de español a licenciados en Filosofía y Letras, lo que suponía una oportunidad de cumplir su deseo de salir de España.

Con sus compañeros de la Residencia hizo sus primeros ensayos de puesta en escena, con versiones delirantes del "Don Juan Tenorio" en las que actuaban Lorca, Dalí y otros compañeros.

En 1921 visitó por primera vez Toledo, ciudad que causó una profunda impresión en Buñuel y sus amigos. También tuvo conocimiento en estos años de las tendencias internacionales más importantes del pensamiento y del arte, y mostró interés por el Dadaísmo y la obra de Louis Aragon y André Bretón. Y, por supuesto, siguió asistiendo con regularidad al cine.

Desde 1922 escribe poemas, prosas poéticas y cuentos en diversas revistas literarias de la época, fundamentalmente aquellas que sirvieron de vehículo para el ultraísmo y la generación del 27, como "Vltra", "Horizonte", "Alfar", "Helix" o "La Gaceta Literaria". De esta época son también sus primeros proyectos cinematográficos. En 1926, la Junta Magna del Centenario de Zaragoza le encargó una película sobre el pintor Francisco de Goya. El guion fue finalmente desechado por la Junta, aduciéndose cuestiones económicas. En 1927 Buñuel le propuso a Ramón Gómez de la Serna la realización de una película. Esta se inspiraría en ocho cuentos del escritor, que quedarían unidos a través de diferentes noticias publicadas en un periódico leído por el protagonista.

En 1923 murió su padre en Zaragoza, inició el servicio militar y publicó su primer artículo, al que siguieron cuentos y poemas en revistas de vanguardia e incluso preparó un libro que los recopilaba bajo el título "Un perro andaluz". Muchas de las imágenes de sus escritos de estos años, previos al surrealismo francés, pasaron a su cine. El día de San José de ese mismo año de 1923 fundó la paródica Orden de Toledo y se nombró a sí mismo condestable. Para ser caballero había que emborracharse y estar toda la noche sin dormir. A ella pertenecieron, entre otros, Dalí, Pepín Bello, Alberti...

En 1924, año en que Dalí le realiza su primer retrato, se licenció en Historia y renunció al doctorado, decidido a marcharse a París, la que por entonces era capital cultural de occidente.

En enero de 1925, después de asistir a la conferencia que dio Louis Aragon en la Residencia de Estudiantes, Buñuel abandonó Madrid rumbo a París. En la capital francesa asistió a las tertulias de los inmigrantes españoles y se acerca cada vez más al grupo surrealista. Su afición por el cine se intensificó y veía habitualmente tres películas al día, una por la mañana (generalmente proyecciones privadas, gracias a un pase de prensa), otra por la tarde en un cine de barrio y otra por la noche.

El pianista Ricardo Viñes le propuso la dirección escénica de "El retablo de Maese Pedro", de Manuel de Falla, que, estrenada en Ámsterdam el 26 de abril de 1926 y representada también al día siguiente, supuso un importante éxito. Esta experiencia le llevó a escribir una pieza de teatro de cámara de vanguardia titulada "Hamlet" en 1927, que fue representada en el "Café Sélect" de París.

Su conversión total al cine se produjo tras ver la película "Las tres luces" ("Der müde Tod"), de Fritz Lang. Varias semanas después se presentó en un rodaje al conocido director de cine francés Jean Epstein y se ofreció a trabajar en cualquier labor a cambio de aprender todo lo que pudiera acerca del cine, y Epstein acabó permitiéndole desempeñar el cargo de ayudante de dirección en el rodaje de sus películas mudas "Mauprat" (1926) y "La caída de la casa Usher" ("La chute de la maison Usher"), de 1928.

Comenzó a colaborar como crítico en varias publicaciones de cine y arte, en las que dejó constancia de sus iniciales concepciones cinematográficas, como la francesa "Cahiers d'Art" y la española "La Gaceta Literaria", de la que fue director de su sección de cine desde 1927. El director de esta revista, Ernesto Giménez Caballero, le propuso fundar un cine-club en la
Residencia de Estudiantes. La idea se llevó a cabo y Buñuel, viajando ocasionalmente a Madrid, promovió en España el cine de vanguardia y las ideas surrealistas.

También en estos años colaboró como actor en pequeños papeles, como el de contrabandista en la película "Carmen" (Jacques Feyder, 1926) con Raquel Meller, y en "La sirène des tropiques" (Henri Étiévant y Mario Nalpas, 1927) con Joséphine Baker. Todo este bagaje le familiarizó con el oficio cinematográfico y le permitió conocer a buenos profesionales y actores que después habrían de colaborar con él en "Un perro andaluz" y "La edad de oro", sus dos primeras películas. Como crítico, elogió el cine de Buster Keaton y atacó, por considerarla pretenciosa, la vanguardia cinematográfica francesa, en cuyas filas militaba el propio Jean Epstein. Es conocida su ruptura con este al negarse el aragonés a trabajar en el nuevo proyecto del más reputado de los directores vanguardistas franceses, Abel Gance, de cuyo "Napoleón" había escrito Buñuel recientemente una crítica muy dura.

Cada vez más interesado por el grupo surrealista de Breton, comenzó a trasladar a sus compañeros de la Residencia de Estudiantes las novedades de esta tendencia, escribiendo poemas de un surrealismo ortodoxo e instando a Dalí a que se trasladase con él a París para conocer el nuevo movimiento. En 1927 escribía un libro de poesía surrealista, que no llegó a editar, cuyo título inicialmente era "Polismos" y más tarde "Un perro andaluz", que fue el que más tarde recibiría su primera película.

En 1928 preparó un guion cinematográfico sobre Francisco de Goya con motivo del centenario de su fallecimiento, patrocinado por una comisión zaragozana. El proyecto no llegó a buen término por falta de presupuesto, como tampoco otro basado en un guion de Ramón Gómez de la Serna que iba a titularse "El mundo por diez céntimos", en el que el hilo conductor iban a ser los sucesivos cambios de dueño de una moneda, o bien, diferentes narraciones que se hallaban en un periódico. Gómez de la Serna le envió finalmente el guion de "Caprichos" (su nuevo título), aunque no se ha conservado, ya que entre octubre y noviembre de ese año abandonaría el proyecto al comenzar a trabajar con Dalí en el de "Un perro andaluz".

En enero de 1929, Buñuel y Dalí, en estrecha colaboración, ultimaron el guion de un film cuyo proyecto se titularía sucesivamente "El marista en la ballesta", "Es peligroso asomarse al interior" y, por fin, "Un perro andaluz", una vez desechada la publicación con este título de su proyectado poemario surrealista. La película se comenzó a rodar el 2 de abril con un presupuesto de 25 000 pesetas aportadas por la madre de Buñuel. Se estrenó el 6 de julio en el Studio des Ursulines, un cine-club parisino, en el que alcanzó un clamoroso éxito entre la intelectualidad francesa, y permaneció en exhibición nueve meses consecutivos en el Studio 28.

A partir de la proyección de "Un chien andalou", Buñuel fue admitido de lleno en el grupo surrealista, que se reunía diariamente en el Café Cyrano para leer artículos, discutir sobre política y escribir cartas y manifiestos. Allí, Buñuel forjó amistad con Max Ernst, André Breton, Paul Éluard, Tristan Tzara, Yves Tanguy, Magritte y Louis Aragon, entre otros.

A fines de 1929 se volvió a reunir con Dalí para escribir el guion de lo que sería más tarde "L'Âge d'or", pero la colaboración ya no resultó tan fructífera, pues entre los dos se interpone el gran amor de Dalí, Gala Eluard. Buñuel comenzó el rodaje de la película en abril de 1930, cuando el pintor se encontraba disfrutando de unas vacaciones con Gala en Torremolinos. Cuando descubrió que Buñuel ya había acabado la película con el sustancioso mecenazgo de los Vizcondes de Noailles, que deseaban producir una de las primeras películas sonoras del cine francés, Dalí se sintió marginado del proyecto y traicionado por su amigo, lo que originó un distanciamiento entre ellos que se fue incrementando en el futuro. A pesar de aquello, felicitó a Buñuel por el largometraje, asegurando que le había parecido «una película americana». El estreno tuvo lugar el 28 de noviembre de 1930. Cinco días más tarde, grupos de extrema derecha atacaron el cine donde se proyectaba, y las autoridades francesas prohibieron la película y requisaron todas las copias existentes, comenzando una larga censura que duraría medio siglo, pues no sería distribuida hasta 1980 en Nueva York y un año después en París.

En 1930, Buñuel viajó a Hollywood, contratado por la Metro Goldwyn Mayer, como «observador», con el fin de que se familiarizara con el sistema de producción estadounidense. Allí conoció a Charles Chaplin y Serguéi Eisenstein. En 1931 llegó a España, en vísperas de la proclamación de la Segunda República. "La edad de oro" se proyectó en Madrid y Barcelona. En 1932 asistió a la primera reunión de la Asociación de Escritores Revolucionarios (AERA), se separó del grupo surrealista y se afilió al Partido Comunista Francés. Contratado por la Paramount, regresó a España y trabajó como responsable de sincronización.

En abril de 1933, financiado por su amigo Ramón Acín, comenzó a filmar "Las Hurdes, tierra sin pan", un documental sobre esa comarca extremeña. La derecha y la Falange Española comenzaban a rebelarse y la película fue censurada por la Segunda República por considerarla denigrante para España. Ese mismo año firmó un manifiesto contra Hitler con Federico García Lorca, Rafael Alberti, Sender, Ugarte y Vallejo.

En 1934 visitó en París a Dalí, ya casado con Gala. Dalí se mostró indiferente con Buñuel, con lo que se incrementó su distanciamiento. El 23 de junio se casó con Jeanne Rucar, a la que había conocido en casa de su amigo Joaquín Peinado en 1925 cuando estudiaba anatomía en París, y que había sido medalla de bronce de gimnasia artística en las Olimpiadas de París de 1924. La boda se celebró en la alcaldía del distrito XX de la capital francesa, sin invitar a la familia, con tres testigos improvisados (uno de ellos, un transeúnte desconocido) y, después de comer, Buñuel volvió a Madrid, ya que había aceptado trabajar para la Warner Brothers como director de doblaje. La pareja tendría dos hijos, Jean Louis, nacido en París, y Rafael, que lo haría en Nueva York.

En 1935, con ayuda de algún dinero familiar, fundó, junto a Ricardo Urgoiti, la productora Filmófono, que competía con la Cifesa de los hermanos Casanova, principal productora española de los años treinta. Filmófono produjo películas como "Don Quintín el amargao", donde debutó en el cine la gran bailaora Carmen Amaya, "La hija de Juan Simón", "¿Quién me quiere a mí?" o "¡Centinela alerta!", y la única condición de Buñuel para producirlas era, curiosamente, no aparecer en la ficha técnica, pues a sus ojos no eran más que "melodramas baratos". Todos estos largometrajes fueron rentables y supusieron la consolidación de la industria cinematográfica española de los años treinta. Sin embargo, la Guerra Civil abortó este proyecto.

El golpe de Estado
franquista sorprendió a Buñuel en Madrid. Así como Dalí se alineó con Franco y simpatizó con el bando sublevado, Buñuel siempre permaneció fiel a la Segunda República. No obstante, no dejó por ello de ayudar a amigos suyos del bando franquista cuando estuvieron en peligro de muerte; así, logró que liberasen a José Luis Sáenz de Heredia (primo hermano de José Antonio Primo de Rivera, fundador de (Falange), que simpatizaba con Franco, pues habían trabajado juntos en Filmófono. El 18 de agosto de 1936 fue asesinado Lorca.

En septiembre de 1936 salió de Madrid en un tren abarrotado hacia Ginebra, vía Barcelona. Allí lo había citado para una entrevista Álvarez del Vayo, ministro de Asuntos Exteriores de la República, quien lo mandó a París como hombre de confianza de Luis Araquistáin, embajador en Francia, para realizar diferentes misiones, principalmente de inteligencia. Supervisó y escribió junto a Pierre Unik el documental "España leal en armas". Realizó su bautismo aéreo en varios viajes relámpago a España, en misiones de guerra.

Durante 1937 se encargó de supervisar para el Gobierno republicano el pabellón español de la Exposición Internacional de París. Dalí le pintó su segundo y último retrato: "El sueño". El 16 de septiembre de 1938, ayudado en los gastos de viaje por sus amigos Charles Noailles y Rafael Sánchez Ventura, viajó a Hollywood de nuevo, esta vez encargado por el Gobierno republicano de la supervisión, como consejero técnico e histórico, de dos películas acerca de la Guerra Civil que se iban a rodar en Estados Unidos.

En 1941, ya terminada la guerra, cuando comenzaba el rodaje de "Cargo of Innocents", la asociación general de productores estadounidenses prohibió toda película en contra de Franco, lo que significó el fin del proyecto en el que estaba implicado Buñuel. Sin trabajo y con poco dinero, y ya con su mujer e hijos reunidos con él, aceptó el encargo que le ofrece el Museo de Arte Moderno (MOMA) de Nueva York, como productor asociado para el área documental y supervisor y jefe de montaje de documentales para la Coordinación de Asuntos Interamericanos, que dirigía Nelson Rockefeller. Su misión era seleccionar películas de propaganda antinazi; tenía despacho propio y personal a su cargo. Su primer trabajo para el MoMA consistió en la reedición de "El triunfo de la voluntad", de Leni Riefenstal, con el fin de hacerla más breve y accesible a miembros del Gobierno de Estados Unidos para que viesen el potencial del cine como instrumento propagandístico. Pero fue despedido en 1943 a raíz de la publicación del libro "La vida secreta de Salvador Dalí", donde el pintor tachaba a Buñuel de ateo y hombre de izquierdas. Un periodista del "Motion Pictures Herald" atacó a Buñuel en un artículo donde advertía acerca de lo peligroso que resultaba la presencia de este español en un museo tan prestigioso. Buñuel se reunió con Dalí en Nueva York para pedirle explicaciones y esa entrevista significó la ruptura de sus relaciones.

Volvió a Hollywood y se puso a trabajar para la Warner Brothers como jefe de doblaje de versiones españolas para América Latina. Acabada la colaboración con la Warner en 1946, se quedó en Los Ángeles en busca de un trabajo relacionado con el cine y en espera de que le concedieran la nacionalidad estadounidense, que había solicitado.

Cuando Luis Buñuel aún estaba viviendo del dinero que había ahorrado el año anterior, la casualidad quiso que en una cena en casa del cineasta francés René Clair se encontrara con Denise Tual, la viuda del actor ruso Pierre Batcheff (protagonista de "Un perro andaluz", quien se había suicidado en 1932). La mujer, que se había vuelto a casar con el productor francés Ronald Tual, le ofreció trabajar en el nuevo proyecto que tenía intención de realizar: "La casa de Bernarda Alba", que dirigiría Buñuel. Tual, que había llegado a Los Ángeles con el interés de conocer mejor la industria estadounidense del cine, tenía intención de realizar la película entre París y México, para lo cual aprovechó su regreso a París para hacer escala en México y concretar algunos asuntos con el productor francés de origen ruso Oscar Dancigers, exiliado en ese país. Una vez allí se enteraron de que los derechos de la obra habían sido vendidos a otra productora que había pujado más alto.

Truncado el proyecto, Luis Buñuel tuvo la suerte de que Dacingers le ofreciera otro trabajo: dirigir "Gran Casino", una película comercial con el conocido cantante mexicano Jorge Negrete y la primera figura argentina Libertad Lamarque. Buñuel aceptó y, una vez arreglados todos los papeles de residencia e instalado con su esposa y sus hijos, ingresó en la industria mexicana del cine. Esta primera película de su nueva etapa constituyó un rotundo fracaso y durante los tres siguientes años se vio obligado a mantenerse del dinero que le enviaba su madre todos los meses.

En 1949, a punto de abandonar el cine, Dacingers le pidió que se hiciera cargo de la dirección de "El gran calavera", ya que Fernando Soler no podía ser a la vez director y protagonista. El éxito de esta película y la concesión de la nacionalidad mexicana animaron a Buñuel a plantear a Dacingers un nuevo proyecto más acorde con sus deseos como cineasta, proponiéndole, bajo el título "¡Mi huerfanito, jefe!", un argumento sobre las aventuras de un joven vendedor de lotería. A esta oferta siguió una mejor respuesta por parte de Dacingers, la realización de una historia sobre los niños pobres mexicanos.

Así, en 1950 Buñuel realizó "Los olvidados", película con fuertes vínculos con "Las Hurdes, tierra sin pan", y que en un primer momento no gustó a los mexicanos ultranacionalistas (Jorge Negrete el primero), ya que retrataba la realidad de pobreza y miseria suburbana que la cultura dominante no quería reconocer. No obstante, el premio al mejor director que le otorgó el Festival de Cannes de 1951 supuso el reconocimiento internacional de la película, y el redescubrimiento de Luis Buñuel, y la rehabilitación del cineasta por parte de la sociedad mexicana. Actualmente, "Los olvidados" es una de las tres únicas películas reconocidas por la Unesco como Memoria del Mundo.

En 1951 filmó "Susana" y "Él"; esta última constituyó un fracaso comercial pero sería valorada en los años siguientes. En 1952 salió de Ciudad de México para filmar "Subida al cielo", cinta simple donde un sueño del protagonista da el toque surrealista de Buñuel y que le valió ir nuevamente a Cannes. Ese mismo año filmó "Robinson Crusoe", primera película que se rodó en Eastmancolor (todos los días se enviaban las copias a California para comprobar los resultados), y, junto con "La joven", que dirigió en 1960, una de las dos únicas películas que rodó en inglés y con coproducción estadounidense. En 1953 dirigió "La ilusión viaja en tranvía", una de las películas consideradas "menores" pero que por su frescura y sencillez, y respaldada por escritores como José Revueltas y Juan de la Cabada, sobrevive al paso de los años.
En 1954 dirigió "El río y la muerte" y es elegido miembro del jurado del Festival Internacional de Cine de Cannes. En 1955, año en que filmó "Así es la aurora" en Francia (lo que le brinda la oportunidad de visitar a su madre en Pau), fiel a sus ideas, firmó un manifiesto en contra de la bomba atómica estadounidense, lo que, unido a su apoyo a la revista antifascista "España Libre" (posicionada en contra de EE. UU.), supuso su inclusión en la lista negra estadounidense hasta 1975. A partir de ese momento, cada vez que pasaban por EE. UU., tanto él como su familia eran interrogados. No obstante, Buñuel dijo que EE. UU. era la tierra más hermosa que había conocido. Cuando alguien le preguntaba si era comunista siempre contestaba que era un español republicano.

Tras "Ensayo de un crimen" (1955), realizó "La muerte en el jardín" (1956), con guion de Luis Alcoriza y Raymond Queneau, que adaptaba la novela homónima de Lacour. The National Film Theatre of London realizó una retrospectiva de su obra. "Nazarín" (1958), Premio Internacional del Festival de Cannes de 1959, es la primera de las tres películas que realizaría con el actor Paco Rabal. Ese mismo año rodó "Los ambiciosos", cine de compromiso político y social. En 1960 dirigió por última vez una obra teatral, "Don Juan Tenorio", en México, y realizó y estrenó en EE. UU. "La joven". 

Fue ganador del "Premio Nacional de Bellas Artes", otorgado por el Gobierno de México en 1977.

En 1962 rodó "El ángel exterminador", una de sus películas más importantes y personales, en la que aludía a varias bromas privadas de su época de la Residencia de Estudiantes y del periodo surrealista transcurrido en Francia.

Durante esta etapa, a Buñuel y a Jean-Claude Carrière les ofrecieron adaptar la novela de Malcolm Lowry "Bajo el volcán" en dos ocasiones. Sin embargo, en ambas el cineasta y el guionista rechazaron la oferta luego de leer la novela y "no encontrar una película detrás del libro".

En 1960 Buñuel regresó a España para dirigir "Viridiana", coproducción hispano-mexicana con guion escrito junto a Julio Alejandro. La película fue producida por Gustavo Alatriste (por parte mexicana) y por Pere Portabella y Ricardo Muñoz Suay, por parte de las productoras españolas UNINCI (Unión Industrial Cinematográfica) y Films 59. Estuvo protagonizada por Silvia Pinal, Francisco Rabal y Fernando Rey.

"Viridiana" fue presentada a concurso en el festival de Cannes de 1961 como representante oficial de España y obtuvo la Palma de Oro, que recogió el entonces director general de Cinematografía, José Muñoz Fontán. Sin embargo, después de que el periódico vaticano "L'Osservatore Romano" condenara la cinta, a la que tachaba de blasfema y sacrílega, las autoridades españolas la condenaron a una "muerte administrativa", denegándole el llamado permiso definitivo de rodaje, lo cual a su vez dio lugar a un conflicto económico entre los productores españoles y los mejicanos. "Viridiana" no se pudo proyectar oficialmente en España hasta 1977.

En 1970 Buñuel volvió a rodar en España, esta vez en régimen de coproducción hispano-franco-italiana; se trataba de "Tristana", protagonizada por Catherine Deneuve, que ya había desempeñado el papel principal en "Belle de jour", y Fernando Rey.

En 1977, Buñuel puso el colofón a su obra con "Ese oscuro objeto del deseo" ("Cet Obscur Objet du Désir"), coproducción hispano-francesa parcialmente rodada en España, que recibió el premio especial del Festival de Cine de San Sebastián. En la película, que revisa temas tratados anteriormente en "Viridiana" o "Tristana", Carole Bouquet y Ángela Molina interpretan al alimón el personaje femenino que da réplica a Fernando Rey.

Además de esas estancias profesionales en España, Buñuel solía, entre los años 1960 y 1980, pasar temporadas en Madrid donde residía en un apartamento en la Torre de Madrid. En este edificio una placa recuerda a Luis Buñuel como «Figura clave del siglo XX».

Ya en su etapa mexicana, Buñuel había rodado varias películas de producción francesa tras las elogiosas críticas europeas de "Ensayo de un crimen", "Así es la aurora" o "La muerte en el jardín", pero su verdadera reentrada en la cinematografía francesa se produjo en 1963 con "Diario de una camarera" ("Le Journal d'une Femme de Chambre"), adaptación de la novela de Octave Mirbeau. Comienza así su cooperación con el productor Serge Silberman y el guionista Jean Claude Carrière.

En 1964 filmó su última película mexicana, "Simón del desierto", que no acabó como estaba proyectada por falta de presupuesto. Aun así, obtuvo el León de Plata de la Mostra de Venecia al año 1965, año en que, junto a Carrière, preparó las adaptaciones de "El monje" y "Là-bas".

En 1966, Dalí le telegrafió desde Figueras ofreciéndole preparar la segunda parte de "Un perro andaluz". Ese mismo año se estrenó "Belle de jour", que obtuvo en 1967 el León de Oro en la Mostra de Venecia. Esta película obtuvo en Francia un extraordinario éxito de público y a partir de entonces los estrenos de Buñuel se convirtieron en acontecimientos culturales, lo que motivó que Silberman le concediera completa libertad creativa y los recursos suficientes para la producción de sus filmes, lo que caracterizó la etapa final de su obra. En 1969 la Mostra le otorgó el gran premio de homenaje por el conjunto de su obra.

En 1972 se convirtió en el primer director español en conseguir el Óscar a la mejor película de habla no inglesa, por "El discreto encanto de la burguesía" ("Le Charme Discret de la Bourgeoisie"), película que se iba a rodar en España, lo cual resultó imposible debido a la censura. Esta película, junto con "La Vía Láctea" ("La Voie Lactée", 1968) y "El fantasma de la libertad" ("Le Fantôme de la Liberté", 1974), conforman una especie de trilogía que ataca los cimientos del cine de narrativa convencional y el concepto causa-consecuencia, abogando por la exposición del azar como motor de la conducta y del mundo. Ese mismo año de 1972 visitó Los Ángeles, donde vivía su hijo Rafael, y George Cukor ofreció en su casa una cena en honor de Buñuel a la que asistieron, además de su hijo Rafael y Carrière, importantes cineastas como Alfred Hitchcock, Billy Wilder, G. Stevens, William Wyler, R. Mulligan, Robert Wise o Rouben Mamoulian.

En 1980 realizó su último viaje a España y fue operado de próstata. En 1981, cincuenta años después de haber sido prohibida, se reestrenó en París "La edad de oro", fue hospitalizado por problemas de la vesícula, Agustín Sánchez Vidal publicó su obra literaria, el Centro Georges Pompidou de París organizó un homenaje en su honor y "Un perro andaluz" se proyectó en una pantalla colocada en el techo de este centro cultural.

En 1982 publicó sus memorias, escritas en colaboración con Carrière y tituladas "Mi último suspiro".

Luis Buñuel falleció en Ciudad de México el día 29 de julio de 1983 de madrugada, a causa de una insuficiencia cardíaca, hepática y renal provocada por un cáncer. Sus últimas palabras fueron para su mujer Jeanne: "Ahora sí que muero". Ese mismo año había sido nombrado doctor honoris causa por la Universidad de Zaragoza. Se mantuvo fiel a su ideología hasta el final: no hubo ninguna ceremonia de despedida, siendo en 1997, cuando finalmente fueron esparcidas sus cenizas en el monte Tolocha, situado en su pueblo natal, Calanda.

Entre 1929 y 1977 dirigió un total de treinta y dos películas. Además, en 1930 rodó "Menjant garotes" ("Comiendo erizos"), una película muda de únicamente cuatro minutos, con la familia Dalí como protagonista.


Aparte de las películas que realizó como director o actor, o en las que colaboró de una u otra forma, también hubo una serie de proyectos que no pudo realizar.





Luis Buñuel realizó varias incursiones en diversos campos (teatro, literatura y poesía) antes y después de dedicarse al mundo del cine, si bien su más relevante aportación fueron los poemas y prosas surrealistas escritos entre 1922 y 1929 durante su estancia en la Residencia de Estudiantes de Madrid. Sus textos de esta época pueden inscribirse entre las aportaciones más interesantes, junto con las de Juan Larrea (1895-1980), de la introducción del surrealismo como componente clave de la Generación del 27. Pero los textos literarios de esta época no solo están influidos por el surrealismo francés, sino que también traslucen los rasgos de la greguería de Ramón Gómez de la Serna y del ultraísmo de la vanguardia madrileña.

Muchos de estos textos iban a conformar un libro de textos poéticos y prosísticos surrealistas del que da noticias desde 1926 y se iba a titular inicialmente "Polismos". Todavía en 1929, en carta escrita a Pepín Bello el 10 de febrero, tiene la intención de publicarlo, aunque ahora con el título "Un perro andaluz", que finalmente se convirtió en el de su primera película.

Los textos más significativos son:


Los diez poemas del libro inédito, que inicialmente se iba a titular "Polismos", fueron escritos hacia 1927. Algunos fueron publicados posteriormente.


Luis Buñuel dirigió desde 1927 la sección cinematográfica de la revista "La Gaceta Literaria". En ella escribió varios artículos, aunque también publicó en revistas francesas, como "Cahiers d'Art" o su suplemento "Feuilles Volantes".





</doc>
<doc id="5013" url="https://es.wikipedia.org/wiki?curid=5013" title="Murcia (desambiguación)">
Murcia (desambiguación)

Murcia hace referencia a varios artículos:







</doc>
<doc id="5016" url="https://es.wikipedia.org/wiki?curid=5016" title="Álgebra lineal">
Álgebra lineal

El álgebra lineal es una rama de las matemáticas que estudia conceptos tales como vectores, matrices, espacio dual, sistemas de ecuaciones lineales y en su enfoque de manera más formal, espacios vectoriales y sus transformaciones lineales.

Es un área activa que tiene conexiones con muchas áreas dentro y fuera de las matemáticas, como el análisis funcional, las ecuaciones diferenciales, la investigación de operaciones, las gráficas por computadora, la ingeniería, etc.

La historia del álgebra lineal moderna se remonta a 1843, cuando William Rowan Hamilton (de quien proviene el uso del término "vector") creó los cuaterniones inspirado en los números complejos; y a 1844, cuando Hermann Grassmann publicó su libro "Die lineare Ausdehnungslehre" ("La teoría lineal de extensión").

De manera más formal, el álgebra lineal estudia conjuntos denominados espacios vectoriales, los cuales constan de un conjunto de vectores y un conjunto de escalares que tiene estructura de campo, con una operación de suma de vectores y otra de producto entre escalares y vectores que satisfacen ciertas propiedades (por ejemplo, que la suma es conmutativa).

Estudia también transformaciones lineales, que son funciones entre espacios vectoriales que satisfacen las condiciones de linealidad:
A diferencia del ejemplo desarrollado en la sección anterior, los vectores no necesariamente son "n"-adas de escalares, sino que pueden ser elementos de un conjunto cualquiera (de hecho, a partir de todo conjunto puede construirse un espacio vectorial sobre un "campo" fijo).

Finalmente, el álgebra lineal estudia también las propiedades que aparecen cuando se impone estructura adicional sobre los espacios vectoriales, siendo una de las más frecuentes la existencia de un producto interno (una especie de producto entre dos vectores) que permite introducir nociones como longitud de vectores y ángulo entre un par de los mismos.

Dentro de los espacios vectoriales de dimensión finita, son de amplio uso los tres tipos siguientes de espacios vectoriales:
Este espacio vectorial está formado por el conjunto de vectores de n dimensiones (es decir con n número de componentes). Podemos encontrar un ejemplo de ellos en los vectores R, que son famosos por representar las coordenadas cartesianas: (2,3), (3,4)...

La matriz es un arreglo rectangular de números, símbolos o expresiones, cuyas dimensiones son descritas en las cantidades de filas (usualmente "m") por las de columnas ("n") que poseen. Los arreglos matriciales son particularmente estudiados por el álgebra lineal y son bastante usados en ciencias e ingeniería.

Un ejemplo de espacio vectorial está dado por todos los polinomios cuyo grado es menor o igual a 2 con coeficientes reales sobre una variable "x".

Ejemplos de tales polinomios son:
La suma de dos polinomios cuyo grado no excede a 2 es otro polinomio cuyo grado no excede a 2:
El campo de escalares es naturalmente el de los números reales, y es posible multiplicar un número por un polinomio:

donde el resultado nuevamente es un polinomio (es decir, un vector).

Un ejemplo de transformación lineal es el operador derivada "D", que asigna a cada polinomio el resultado de derivarlo:
El operador derivada satisface las condiciones de linealidad, y aunque es posible demostrarlo con rigor, simplemente lo ilustramos con un ejemplo la primera condición de linealidad:

y por otro lado:
Cualquier espacio vectorial tiene una representación en coordenadas similar a formula_2, lo cual se obtiene mediante la elección de una base (álgebra) (es decir, un conjunto especial de vectores), y uno de los temas recurrentes en el álgebra lineal es la elección de bases apropiadas para que los vectores de coordenadas y las matrices que representan las transformaciones lineales tengan formas sencillas o propiedades específicas.

Puesto que el álgebra lineal es una teoría muy exitosa, sus métodos se han proliferado por otras áreas de la matemática: en la teoría de módulos, que remplaza al cuerpo en los escalares por un anillo; en el álgebra multilineal, uno lidia con 'múltiples variables' en un problema de mapeo lineal, en el que cada número de las diferentes variables se dirige al concepto de tensor, e incluso en el ámbito de la programación ya que hoy en día la indexación de páginas web se basa en métodos del álgebra lineal; en la teoría del espectro de los operadores de control de matrices de dimensión infinita, aplicando el análisis matemático en una teoría que no es puramente algebraica. En todos estos casos las dificultades técnicas son mucho más grandes.



</doc>
<doc id="5017" url="https://es.wikipedia.org/wiki?curid=5017" title="Apache OpenOffice">
Apache OpenOffice

OpenOffice es una "suite" ofimática libre, de código abierto, que incluye procesador de textos, hoja de cálculo, presentaciones, herramientas para el dibujo vectorial y base de datos. Soporta numerosos formatos de archivo, incluyendo como predeterminado el formato estándar ISO/IEC OpenDocument (ODF), entre otros formatos comunes, y se enfoca en mantener compatibilidad con el estándar OpenOffice XML, el formato de Microsoft, así como también soporta más de 110 idiomas, desde febrero del año 2010. Apache OpenOffice es uno de los sucesores del proyecto OpenOffice.org e integra características de otras suites ofimaticas como IBM Lotus Symphony.

La suite ofimática está disponible para varias plataformas, tales como Microsoft Windows, GNU/Linux, BSD, Solaris y Mac OS X, además de diversos ports realizados a otros sistemas operativos. El software es distribuido bajo la licencia Apache. La primera versión lanzada por Apache fue la 3.4.0, el 8 de mayo de 2012. Desde esa primera versión, se han realizado diversas bifurcaciones como por ejemplo LibreOffice (desarrollado por The Document Foundation) y el proyecto Go-OO.

Apache OpenOffice desciende de OpenOffice.org, un proyecto que tiene como base inicial a StarOffice, una suite ofimática desarrollada por StarDivision y adquirida por Sun Microsystems en agosto de 1999. El desarrollo de la suite estaba liderado por Sun Microsystems y con posterioridad abandonado por Oracle Corporation. El código fuente de la aplicación está disponible bajo la Licencia pública general limitada de GNU (LGPL) versión 3 hasta la versión 3.4.0 Beta 1. Después de la adquisición de Sun en 2010, Oracle Corporation dejó de apoyar el desarrollo comercial y en junio de 2011 donó la suite a la Incubadora de Apache para convertirse en un proyecto de la Apache Software Foundation. Posteriormente, en diciembre de 2011, la Apache Software Fundación anunció que el nombre del proyecto se convertiría en Apache OpenOffice.

Las aplicaciones incluidas en la suite ofimática Apache OpenOffice son las siguientes:
OpenOffice incluye de manera nativa varias fuentes para ser usadas como una alternativa a las fuentes de Microsoft (Calibri, Arial, Times New Roman), las cuales son:

Apache OpenOffice incorpora OpenOffice Basic, un lenguaje de programación interpretado muy similar al lenguaje de macros Microsoft Visual Basic for Applications (VBA). Apache OpenOffice provee soporte limitado a las macros escritas en Microsoft VBA. Las macros escritas en OpenOffice Basic pueden ser utilizadas en Writer, Calc y Base.

OpenOffice.org y StarOffice aseguran tener el 14 % del mercado de las grandes empresas en 2004. El sitio web reportó más de 100 millones de descargas.

OpenOffice.org ha sido adoptado por una gran cantidad de instituciones en el sector público y privado. Algunas de las razones de su adopción son el racionamiento en el gasto de licencias de software y el uso que hace la suite de formatos de fichero estándares e independientes de un único proveedor. Entre los usuarios de OpenOffice.org se encuentran el Ministerio de Defensa de Singapur, el Ayuntamiento de Bristol, el Ayuntamiento de Zaragoza, el Ayuntamiento de Castellón de la Plana y la Gendarmería Francesa.

El 4 de octubre de 2005, Sun y Google anunciaron una alianza estratégica mediante la cual Sun agregaría una barra de búsqueda de Google en OpenOffice.org, Sun y Google colaborarían en actividades de marketing así como en investigación y desarrollo comunes, y Google ayudaría a distribuir OpenOffice.org. En México, OpenOffice.org se incluye en la computadora "YooBook". OpenOffice.org suele aparecer preinstalado en gran cantidad de distribuciones Linux.

La gran mayoría de las características que se criticaban han sido corregidas en la versión 3. Algunas de las carencias que se criticaban en las versiones antiguas 2.x eran la falta de plantillas incorporadas y asistentes automatizados (hay muchas plantillas que pueden descargarse desde Internet, aunque todavía son relativamente pocas las que están traducidas al español); capacidad limitada para personalizar las gráficas (sin embargo, fue la principal preocupación de la versión 2.4, la cual mejoró este problema) y falta de un solucionador de cálculos multivariados en Calc.

Algunos usuarios experimentaban lentitud en el arranque cuando se lanza por primera vez OpenOffice.org en versiones previas a la 3.0. Esto puede mejorarse notablemente, si se desea continuar utilizando versiones antiguas, cambiando la configuración de manejo de memoria de OpenOffice.org. Estos cambios permiten a OpenOffice arrancar mucho más rápidamente, a costa de aumentar el consumo de memoria. Otra posibilidad es usar el programa de arranque rápido, una aplicación que se ejecuta al arrancar el S.O., y que carga parte del software necesario para OpenOffice. Esto ralentiza el arranque del ordenador, pero hace el arranque de OpenOffice mucho más rápido.

El jefe de licencias de Microsoft dijo durante una entrevista realizada por la revista Fortune en 2007 que su empresa se ve perjudicada por programas libres que estarían violando 235 patentes de su propiedad. OpenOffice.org estaría violando 45 patentes de Microsoft. En cualquier caso, Microsoft no ha informado cuáles son esas patentes, a pesar de que puede disponer del código fuente de OpenOffice.org para inspeccionarlo. Esta acción se interpretó por parte de muchas personas como FUD de Microsoft con el fin de desalentar el uso de OpenOffice.org y el software libre. Un punto a considerar es que muchos países no permiten otorgar patentes a los programas informáticos, por lo que en ellos, de realizarse esta denuncia, no tendría relevancia.
Existen varias suites ofimáticas derivadas de OpenOffice.org. La mayoría de ellas están desarrolladas bajo licencia SISSL (que es válida hasta OpenOffice.org 2.0 beta). En general, su objetivo es el mercado local, con añadidos propietarios como un módulo de reconocimiento del habla, conexión a base de datos automática, o mejor soporte CJK. Uno de estos derivados es la suite de oficina propietaria Lotus Symphony. Otra aplicación derivada es NeoOffice para Mac. La versión de OpenOffice 3 para Mac está totalmente integrada en ese entorno.

Sun Microsystems ayuda al desarrollo de OpenOffice.org y lo utiliza como base para el desarrollo de su propia versión comercial denominada StarOffice. Las versiones de StarOffice desde la 6.0 han estado basadas en el código fuente de OpenOffice.org. La diferencia entre ambas suites radica en que StarOffice incluye algunos componentes propietarios adicionales, tales como fuentes adicionales (en especial las fuentes de idiomas asiáticos); plantillas de documento adicionales; filtros de archivos adicionales y herramientas de migración (Enterprise Edition).

Go-OO es una bifurcación de OOo que desarrolla un conjunto de parches que corrigen errores y añaden funcionalidades a la suite ofimática. OpenOffice y las correcciones realizadas por el proyecto Go-OO vienen incluidas en varias distribuciones Linux, entre ellas Debian, Mandriva, openSUSE, Gentoo y Ubuntu.

Después de la adquisición de Sun Microsystems (patrocinador del desarrollo de OpenOffice.org) por parte de Oracle, StarOffice y StarSuite recibieron el nuevo nombre de Oracle Open Office.

El 28 de septiembre de 2010, miembros de la comunidad de desarrollo del proyecto OpenOffice.org formaron un nuevo grupo llamado The Document Foundation, poniendo a disposición una bifurcación de OpenOffice.org llamada LibreOffice. La fundación declaró que coordinará y vigilará el desarrollo de LibreOffice. Oracle fue invitada a convertirse en miembro de The Document Foundation y se pidió que donara la marca OpenOffice.org al proyecto.
The Document Foundation recibió apoyo de la comunidad de desarrolladores independientes de Open Office, y también de las empresas Novell, RedHat, Canonical y Google. El objetivo es producir una suite ofimática independiente de cualquier empresa, con soporte ODF y sin la obligación de asignar la autoría del código a Oracle. Oracle Open Office impone que la autoría del código se asigne a Oracle (anteriormente la obligación era a Sun).


Otras suites ofimáticas:



</doc>
<doc id="5022" url="https://es.wikipedia.org/wiki?curid=5022" title="PLC">
PLC

Las siglas PLC o plc pueden referirse a:


</doc>
<doc id="5024" url="https://es.wikipedia.org/wiki?curid=5024" title="Tiempo universal coordinado">
Tiempo universal coordinado

El tiempo universal coordinado o UTC (un intermedio entre la versión en inglés "Coordinated Universal Time" CUT y la versión en francés "Temps universel coordonné" TUC) es el principal estándar de tiempo por el cual el mundo regula los relojes y el tiempo. 

Es uno de los varios sucesores estrechamente relacionados con el tiempo medio de Greenwich (GMT). Para la mayoría de los propósitos comunes, UTC es sinónimo de GMT, ya que GMT ya no es el estándar definido para la comunidad científica.

El UTC se obtiene a partir del Tiempo Atómico Internacional, un estándar de tiempo calculado a partir de una media ponderada de las señales de los relojes atómicos, localizados en cerca de 70 laboratorios nacionales de todo el mundo. Debido a que la rotación de la Tierra es estable pero no constante y se retrasa con respecto al tiempo atómico, UTC se sincroniza con el tiempo medio de Greenwich (obtenido a partir de la duración del día solar), al que se le añade o quita un segundo intercalar cuando resulta necesario, siempre a finales de junio o diciembre. La decisión sobre los segundos intercalares la determina el Servicio Internacional de Rotación de la Tierra y Sistemas de Referencia, basándose en sus mediciones de la rotación de la Tierra.

El UTC presenta problemas para sistemas informáticos como Unix, que guardan el tiempo como un número de segundos a partir de un tiempo de referencia. Debido a los segundos intercalares, es imposible determinar qué representación va a tener una fecha futura, debido a que el número de segundos intercalares que se han de incluir en la fecha es aún desconocido.

UTC es el sistema de tiempo utilizado por muchos estándares de Internet y la World Wide Web. En particular, se ha diseñado el Network Time Protocol como una forma de distribuir el tiempo UTC en Internet.

El propio servidor de Wikipedia utiliza UTC como base para referenciar el momento en que se registran las actualizaciones de los artículos.

La hora GMT se basa en la posición media del Sol y fue definida por primera vez a partir del mediodía de Greenwich, pero el 1 de enero de 1925 se adoptó la convención de que la jornada comenzase a la media noche, atrasando aquel día 12 horas y desde entonces el GMT se sigue definiendo a partir de la medianoche de Greenwich. Esta hora carece de cierta fiabilidad ya que se basa en el movimiento medio del Sol. Fue por esto por lo que se definió la hora UTC, que tiene una gran precisión, ya que está dada por relojes atómicos.

Las zonas horarias de todo el mundo se expresan como desviaciones positivas o negativas de UTC, tomando como referencia el meridiano cero o meridiano de Greenwich.

Puesto que la Tierra gira de oeste a este, al pasar de un huso horario a otro en dirección Este hay que sumar una hora. Por el contrario, al oeste hay que restar una hora. El meridiano de 180°, conocido como línea internacional de cambio de fecha, marca el cambio de día.

UTC no cambia con un cambio de las estaciones, pero la hora local puede cambiar si una jurisdicción utiliza el sistema de horario de verano. Por ejemplo, la hora oficial es de cinco horas atrasadas (hora local) en la Costa Este de los Estados Unidos, durante el invierno, con respecto al Meridiano de Greenwich; pero es de cuatro horas atrasadas durante el horario de verano en esa zona, con respecto al Meridiano de Greenwich.

ZULU u Hora Zulú designa en usos militares, y en la navegación aérea, el Tiempo Universal Coordinado. Zulú representa la letra Z en el código Interco. Su principal mérito es que permite usar como referencia una hora en común y no las horas locales con las cuales se requeriría un proceso de transformación.

Cada huso horario tiene una letra correlativa como identificación, comenzando por Greenwich. Cuando una hora se expresa en CUT, en UTC o en Zulú, es en realidad la hora en la longitud 0° que atraviesa Greenwich, Inglaterra. Todos los husos horarios del planeta están establecidos en referencia a la longitud 0° conocida también como el meridiano de Greenwich.

Como ejemplo, si estamos en Perú y la hora aeronáutica es 22:30Z, entonces la hora local será 17:30R (R de ROMEO en el alfabeto fonético aeronáutico y diferencia de -05:00 horas).




</doc>
<doc id="5026" url="https://es.wikipedia.org/wiki?curid=5026" title="Literatura medieval española">
Literatura medieval española

Se entiende por literatura medieval española el corpus de obras literarias escrito en castellano medieval entre, aproximadamente, comienzos del siglo y finales del siglo. Las obras de referencia para esas fechas son, por un lado, el "Cantar de mio Cid", cuyo manuscrito más antiguo sería de 1207, y "La Celestina", de 1499, obra de transición hacia el Renacimiento.

Dado que, como demuestran las glosas utilizadas en Castilla para explicar o aclarar términos latinos, hacia finales del siglo el latín hablado se había distanciado enormemente de sus orígenes (empezando a dar paso a las distintas lenguas romances peninsulares), hay que sobreentender que la literatura oral estaría siendo producida en castellano desde bastante antes que la literatura escrita.

Así lo demuestra, por otro lado, el hecho de que distintos autores de entre mediados del siglo y fines del pudiesen incluir, al final de sus poemas en árabe o hebreo, versos que en algunos casos constituían muestras de lírica tradicional en lengua romance, lo que se conoce con el nombre de "jarchas".

La composición literaria en lengua castellana (y, en general, en lengua romance) se hizo en sus comienzos en verso. Dos son las razones principales de ese hecho: por un lado, su carácter de "literatura oral-popular" (lo que implicaba su recitado con frecuente acompañamiento musical); por otro, que la escritura en prosa exigía una tradición en el uso del castellano (sobre todo para la consolidación de su sintaxis) que, dado el dominio culto del latín hasta bien avanzada la Edad Media, no pudo darse hasta el siglo, cuando Alfonso X, "el Sabio", decidió hacer del castellano una lengua de uso tanto para los asuntos de la administración del reino, como para la composición de sus obras historiográficas y de otros tipos. 
Así, pues, los primeros géneros que hay que considerar son la "lírica tradicional" y la poesía "épica" ("cantares de gesta" y "romances"), que, habiéndose recogido por escrito a partir del siglo , serían testimonios de composiciones orales anteriores en el tiempo; ambos géneros conforman lo que se denomina la "literatura del mester de juglaría", esto es, literatura compuesta para ser recitada. Además, hay que contar con el "primitivo teatro castellano". 

Este teatro parece remontarse al siglo, en forma de representaciones relacionadas con temas religiosos. Así ocurre con el primer texto teatral en castellano, la "Representación de los Reyes Magos", cuya única copia data de los años de tránsito entre el siglo y , y que, por la lengua, puede datarse a mediados del . Posteriormente, y hasta "La Celestina" (cuya adscripción al género teatral es discutible) los ejemplos de teatro en castellano son siempre indirectos, a través de referencias en otras obras. 

Dentro ya de los géneros escritos, dado que la lengua de prestigio para la lírica culta (o cortesana) durante la Edad Media fue el gallego-portugués, la lírica culta en castellano no empezó a cultivarse hasta mediados del siglo, apareciendo su figura más relevante, Jorge Manrique, en el siglo .

En cuanto a la prosa, 
Con todo, ya en el mismo siglo , durante el obispado de Raimundo, se tiene constancia de que en el proceso de traducción de diversas obras de géneros variados (matemáticas, astronomía, medicina, filosofía...) al latín, se daba en muchas ocasiones el paso intermedio de traducirlas oralmente al castellano: primero de la lengua original a este y después, lo que tiene una singular importancia, del castellano al latín; tal proceso suponía que la lengua romance ya estaba plenamente constituida para expresar ideas abstractas o elevados cálculos.

Pero la plena consolidación del castellano como lengua escrita a todos los niveles se produjo en el siglo . Esto posibilitó por un lado, la aparición de las obras del llamado "mester de clerecía" (poesía narrativa en verso de tipo culta: "Milagros de Nuestra Señora", de Berceo y "Libro de buen amor", de Juan Ruiz) y por otro, al lado de las obras de tipo "ensayístico", de las primeras obras literarias narrativas en prosa: "cuentos" que, en principio, eran traducciones/adaptaciones realizadas por el taller de Alfonso X, y que ya en el siglo pasaron a ser creaciones originales (aunque con un importante trasfondo popular), bien en forma de relatos de aventuras de ficción próximos ya al género "novela" ("Libro del caballero Zifar"), bien en forma de colecciones de cuentos, como es el caso de "El conde Lucanor" de don Juan Manuel.

Hasta bien entrado el siglo las lenguas de erudición fueron el latín, el árabe y el hebreo, en las que se escribía todo lo que tenía que ver con la religión, la historia y la ciencia. Durante el reinado de Fernando III de Castilla (1217-1252), el castellano se fue convirtiendo en lengua escrita-literaria.

Como se ha señalado antes, el origen de la literatura castellana está en verso, y no en prosa, porque la técnica de enseñanza de la lengua se basaba en la imitación de los textos literarios clásicos, los cuales estaban en verso. Luego, cuando se produce la consolidación de las técnicas poéticas y en pleno desarrollo de sus posibilidades expresivas (con el "mester de clerecía"), los asuntos que antes se escribían en verso se traspasan al dominio formal de la prosa. Esto está, también, en relación directa con la maduración del sistema político y social: la prosa, más difícil que el verso, tiene mayor capacidad para relacionar las distintas unidades lógicas y dialécticas del pensamiento humano.

Así, el contenido de las primeras obras que se escriben en prosa castellana es, principalmente, de tipo histórico y van apareciendo a lo largo del siglo . En primer lugar, están las "Corónicas" (h. 1186) del Fuero general de Navarra, breves narraciones en forma de anales. En segundo lugar, aparecen unos escuetos "Anales toledanos primeros" (muy impregnados de mozarabismos). Después, el "Liber regum" (h. 1196-1209), originalmente en navarroaragonés y traducido a principios del al castellano. Hay, también, diversos contratos y diplomas, de carácter particular, que, al usar el castellano, reflejan las dificultades de comprensión que planteaba el latín escrito, algo que quedaba manifestado en el continuo uso de glosas a partir del siglo . 

Consecuentemente, desde finales del siglo y por razones políticas, se fijan por escrito normas jurídicas en una lengua comprensible para la mayoría: el castellano. Y, poco a poco, se van desarrollando ciertos recursos narrativos en los textos jurídicos: por ejemplo, los "exempla" o cuentecillos ilustrativos de distintos casos. Además, en el desarrollo de la prosa en castellano son muy importantes las traducciones, que fueron iniciadas por el arzobispo Raimundo en Toledo (con la llamada "escuela de traductores"), pues se trataba de un ejercicio lingüístico muy beneficioso, entre otras cosas, para flexibilizar la sintaxis del castellano.

Con todo, la figura esencial de la cultura en castellano de esta época es Alfonso X; su actividad

Tanto él como, después, su hijo Sancho IV, promovieron como reyes de Castilla y León la elaboración de un considerable número de obras de muy distintos géneros ensayísticos. 

A la labor historiográfica es a la que le debe su mayor prestigio Alfonso X; su producción en este ámbito está compuesta por dos títulos: la "Estoria de España" y la "General Estoria".

Otras obras y autores vinculados a la historia son:

Especialmente, la historiografía en el siglo está protagonizada por Enrique de Villena (1384-1434). Su texto más importante es "Los doce trabajos de Hércules" (1417), previamente escrito en catalán. Se trata de una obra compleja en la que, partiendo de la mitología clásica y a través de un método interpretativo, expone su visión de la sociedad de su época. La producción de Enrique de Villena supuso una innovación en la prosa española, por su erudición y restauración de la sintaxis latinizante —imitadora de la latina—.

Las obras medievales de contenido religioso son, básicamente, del siglo , en concreto las derivadas de la traducción a lenguas romances de la Biblia y de la redacción de una literatura doctrinal o catecismos.

Las obras encaminadas a la enseñanza de algún tipo de conocimiento se materializaron, en primer lugar, en la llamada literatura sapiencial, que se desarrolló a lo largo del siglo en forma de colecciones de sentencias, bien originales, bien de versiones de originales en árabe.

Dentro de la didáctica, deben incluirse también los "sermones", cuya técnica, dada la supremacía de los religiosos como autores literarios, fue de una enorme influencia. Había dos tipos de sermones: los cultos (en latín) y los populares, en lengua romance. Este segundo, dado el tipo de auditorio al que iba dirigido (mezcla de laicos y letrados), abundó en el uso de recursos como los "exempla" (cuentos ilustrativos extraídos de la Biblia y otras historias, reales o ficticias con finalidad moralizadora); además de los "exempla", los sermones utilizaban también las "sententiae", o dichos de hombres famosos, originadas en la retórica y el cristianismo primitivo. 

A mediados del siglo se tradujeron del árabe textos de carácter moralizante o didáctico. Entre ellos están el "Libro de los buenos proverbios", los "Bocados de oro", el "Libro de los cien capítulos" y las "Flores de filosofía".

En el siglo se compuso también una obra singular: los "Proverbios morales" (1355-1360) del judío Santob de Carrión. Muy vinculados con las enseñanzas judías, los proverbios están dedicados a Pedro I de Castilla y están escritos en cuartetos heptasilábicos o dípticos alejandrinos con rima interna; su contenido expresa un relativismo moral muy pesimista basado en la contemplación de la vida cotidiana.

Además de estas colecciones de proverbios, en la Edad Media se dieron también obras destinadas a la educación de príncipes e infantes. A esta tradición pertenece obras trasladadas desde el árabe como "Calila e Dimna", el "Barlaam y Josafat" y el "Sendebar", que aunque más tarde fueron leídas como compilaciones de cuentos, habían sido concebidas en origen como textos para el adoctrinamiento de príncipes.

A la prosa doctrinal pertenece, también, un tratado de Alfonso Martínez de Toledo (1398-1468), capellán de Juan II y de Enrique IV, titulado "El Arcipreste de Talavera" o "El Corbacho".

La práctica textual vinculada al derecho tiene sus primeras muestras en castellano con los "fueros" y las "cartas pueblas", documentos de alcance específico en Castilla y León que, por un lado, pretendían recopilar los privilegios de cada localidad y, por otro, legislar sobre la repoblación de los terrenos fronterizos.

La llegada al trono de Fernando III conllevó la búsqueda de una legislación unificada; el primer paso fue la traducción del "Liber iudicum": el "Fuero juzgo" se instauró, así, como obra de referencia legal para el territorio conquistado bajo su reinado. El segundo paso fue, ya, original, en el sentido de iniciar un nuevo corpus legal, el "Setenario".

Alfonso X, por su parte, no solo termina el "Setenario", sino que, apoyándose en él, redacta las "Siete partidas", obra que refleja su interés por imponerse en sus territorios. 

El concepto de «lo científico» era muy amplio en la Edad Media, e incluía astronomía, astrología, tratados sobre las propiedades de las piedras ("El lapidario"), las plantas y la magia.

El interés de Alfonso X por la astrología lo puso en contacto con sabios judíos y árabes, de quienes aprovechó sus traducciones latinas o encargó nuevas versiones romanceadas. Con ellas, elabora textos como el "Libro del saber de astrología", colección de tratados sobre temas astronómicos, el "Libro complido en los judizios de las estrellas", adaptación del tratado de Ali ibn ar-Rigal (Ali ben Ragel), o el "Libro de la ochava esfera". También escribió tratados sobre instrumentos de medición o unas "tablas astronómicas", pues su objetivo era descubrir el porvenir ("astrología judiciaria"). Por ello consultaba a sus estrelleros al tomar decisiones, lo que le valió el recelo y desconfianza de clérigos e intrigantes cortesanos. Se acercó a temas relacionados con la magia, en su "Libro de las formas et de las imágenes" o en su versión, parcialmente conservada, del "Picatrix" árabe.

La lírica popular medieval comprende una variada tradición de composiciones propias del acervo popular, predominantemente rural, utilizadas preferentemente durante el trabajo y las fiestas, por lo que, a menudo, eran canciones asociadas al baile (también, hay canciones de camino, rimas infantiles, etc.). Así, pues, considerados como textos puestos por escrito, hay que tener en cuenta que bajo tal versión aparecen como textos poéticos aislados de su primitiva unidad artística, que reunía letra y música.

Desde finales del siglo muchas de estas composiciones fueron fijadas textualmente e incluidas en los grandes cancioneros de los siglos y .

La lírica popular castellana comparte una serie de elementos que resultan una constante en la expresión literaria de diferentes tradiciones europeas, de ahí, por ejemplo, que muchos de sus textos recuerden a las cantigas de amigo gallego portuguesas

Los contenidos, casi siempre vinculados al amor (la muerte por amor, la pena por la separación, etc.), se centran en motivos tales como la descripción de la mujer (por ejemplo, fijándose en sus cabellos, muchas veces símbolo de virginidad), las localizaciones en ámbitos naturales donde hay agua (que simboliza la cita amorosa y el erotismo) o flores (también de simbología sexual), o con la presencia del aire o el viento, símbolos de la comunicación amorosa. 

En muchas ocasiones, la voz lírica es una voz femenina, que lamenta ante un confidente (generalmente la madre, la hermana, la amiga o la naturaleza) la distancia respecto al ser amado por motivos que abarcan la ausencia, la pérdida o el duelo.

Derivados de esos contenidos, es posible aislar una serie de temas frecuentes en la lírica popular: el amor y la naturaleza, entrelazados y confundidos; la niña enamorada que no quiere ser monja; el elogio de la propia belleza por parte de la voz lírica femenina; el rechazo del matrimonio; los malos que enturbian la relación amorosa; la caza de amor; etc. 

Formalmente, suelen ser composiciones breves, de dos a cuatro versos de arte menor (habitualmente, de seis a ocho sílabas), irregulares y con rima asonante. Dada su raigambre oral, son muy ricas en recursos fónicos (repetición de vocales, disposición regular de los acentos, etc.) y paralelísticos. 

En cuanto a su forma estrófica, hay predominancia de los pareados, tercetos, cuartetas, etc. A veces, presentan una glosa que desarrollan o bien desdoblan el estribillo, con una narración más objetiva. El villancico es la estrofa característica: dos o tres versos, variables silábicamente aunque preferiblemente de ocho a seis sílabas, y con un esquema rítmico "abb". Se estima que existieron en Castilla desde el siglo .

También del zéjel, composición poética de origen árabe, hay ejemplos en las "Cantigas" de Alfonso X, en el "Libro de buen amor" y en varios poetas cultos del , como Juan Álvarez Gato y Gómez Manrique. 

Estilísticamente, la expresión es sencilla y elemental, reflejando una actitud emocional ingenua y misteriosamente irracional; hay una ausencia casi total de metáforas, prefiriéndose las imágenes visuales que denotan impresiones directas de una realidad exterior frecuentemente subjetivizada y cargada de un simbolismo ancestral; por último, la expresión de los sentimientos amorosos se realiza de forma abierta, patética, con énfasis y de forma reiterada.

La llamada lírica culta castellana es la poesía elaborada en las cortes de los reyes medievales Juan II de Castilla, Enrique IV de Castilla y Reyes Católicos por parte de los caballeros que vivían en ellas (reyes, políticos, magnates...) y que nos ha llegado a través de los "cancioneros" del siglo. Se extiende a lo largo de siglo y medio, desde los primeros poemas del Cancionero de Baena (h. 1370), hasta la segunda edición del "Cancionero geral" (1516) de García de Resende. Se la puede considerar como "la más impresionante muestra de poesía cortesana de toda la Europa medieval. Los grandes poetas cultos castellanos de esta época fueron Pero López de Ayala, el Marqués de Santillana, Juan de Mena y Jorge Manrique.

Las características más sobresalientes de la lírica culta castellana son herencia de la lírica gallegoportuguesa: fundamentalmente, la terminología métrica y la concepción del amor cortés (en la que el "goig" o alegría del amor provenzal ha sido sustituido por la "coita" o pena).

Se trata de una poesía esencialmente "social", y no tan subjetiva, íntima, como la tradicional. Esta función social se ejemplifica en los diversos temas tratados: la política, la moral, la filosofía, la teología, el amor cortés, etc. A diferencia de lo que ocurría en la lírica tradicional, la lírica culta ya no asocia de forma radical la letra y la música; así, aparecen las primeras composiciones líricas destinadas solo a la lectura y no al canto, con lo que la composición hubo de responder a otras necesidades y objetivos: posibilidad de mayor extensión, búsqueda de nuevos niveles de significación con la alegoría, fijación de géneros (canciones y villancicos), etc.

Las estrofas comienzan a definirse y a centrarse en diferentes formas, tomando, como base, el verso de ocho sílabas y el de doce. 

Los temas de esta poesía derivan, básicamente, de la poesía provenzal de los trovadores occitanos: el amor y sus variaciones. En la Península se añaden algunas características, como las alegorías -personajes basados en ideas abstractas-, los juegos de palabras complejos, la falta de paisaje y de descripción física, la aceptación de la desgracia por parte del amante, etc. 

Esta poesía suele recogerse en libros de poemas llamados habitualmente "cancioneros". Destacan tres: 


Para completar el panorama de la poesía de esta época, se pueden añadir otras obras muy diversas en su forma y géneros: 

La "épica" es un subgénero narrativo compuesto en verso y en lengua romance, cuyos orígenes datan del primer tercio del siglo. Las narraciones épicas están protagonizadas por héroes que representan, por sus valores, a toda una sociedad; suelen centrarse en acontecimientos relevantes dentro de la historia de un pueblo, por lo que esos héroes terminan por ser considerados símbolos para los mismos.

Es frecuente, además, que el argumento de estas historias gire alrededor de algún problema del protagonista con el valor social de la honra, que constituía la base de todo el sistema ético-político de relaciones vasalláticas en la Edad Media.

La épica castellana toma sus temas, fundamentalmente, de dos acontecimientos históricos:

En este sentido,
Así las cosas, por influencia de la épica francesa (a través del Camino de Santiago y de la presencia del mundo occitano en el noreste peninsular), la épica castellana solo tomó algunos temas de esta, como por ejemplo la figura de Carlomagno, en el único texto que presenta huellas del llamado "ciclo carolingio", el fragmento conservado del "Cantar de Roncesvalles".

El poema épico se denomina propiamente "cantar de gesta". De los cantares de gesta se dice que son obras que pertenecen al mester de juglaría, pues eran transmitidos y recitados de memoria por los juglares que actuaban en las plazas de los pueblos y ciudades, en los castillos o en las estancias de la corte, a cambio de un pago por sus servicios. Sabían danzar, tocar instrumentos, recitar y realizar ejercicios acrobáticos y circenses. Consecuentemente, los cantares de gesta se representaban con apoyatura musical ante el público, haciendo uso de una monodia: una ligera cadencia final en cada uno de los versos que era subrayada en el primero y último de cada tirada (entonación y conclusión). 

El objetivo de este recitado público era doble: entretener e informar al auditorio, aunque sin propósitos moralizantes ni pedagógicos (propósitos que sí serían propios de las obras del "mester de clerecía"). 

Se han conservado muy pocos debido a esta transmisión oral. Además del "Cantar de mio Cid", que se conserva casi completo, nos han llegado fragmentos del "Cantar de Roncesvalles" y del "Cantar de las Mocedades de Rodrigo". De otros cantares de gesta nos han llegado noticias gracias a las "crónicas históricas", que los utilizaron como fuente (por ejemplo, el "Cantar de los siete infantes de Lara", que aparece en la "Segunda Crónica General" —"Crónica de 1344", de Pedro de Barcelos— y que está vinculado al ciclo de temas relativo a los Condes de Castilla).

Algunas características de los cantares de gesta de la literatura española son: 


Los cantares de gesta fueron tomados como documentos históricos en muchas ocasiones, porque algunos fueron prosificados y así fueron incluidos en crónicas medievales (como la "Estoria de España" o "Primera crónica general" de Alfonso X); gracias a esto, algunos se han podido conservar parcialmente.

La obra española más importante (y única completa) de este género es el "Cantar de mio Cid", que se conserva en una copia manuscrita del siglo de un códice de 1207 copiado por Per Abbat de un original fechado entre 1195 y 1207. La fecha de redacción del original se sitúa, por tanto cerca de 1200. 

La obra ha sido dividida por los editores modernos en tres cantares:


Los hemistiquios oscilan entre las tres y las once sílabas, con claro predominio, en este orden, de heptasílabos, octosílabos y hexasílabos, lo que da versos de longitud variable que se cifra entre 14 y 16 sílabas métricas, y estos se organizan en series o tiradas de un número indefinido de versos asonantes entre sí.
Aparecen, sistemáticamente, a lo largo del poema fórmulas —grupos de palabras que se repiten con ligeras variaciones—. Esto apunta al carácter oral de este género, ya que en el origen de la poesía épica, facilitaría la improvisación y la memorización de los versos. De entre estas fórmulas destacan la omisión de verbos de decir —dijo, preguntó, respondió...— y los epítetos, adjetivos generalmente aplicados a personas o lugares caracterizados positivamente.

La palabra "romancero", en el contexto de la literatura medieval, hace referencia al conjunto o "corpus" de poemas denominados "romances" que han sido conservados, ya sea por escrito, ya a través de la tradición oral. Compuestos anónimamente a partir del siglo , fueron recogidos por escrito en el y conforman lo que se denomina "romancero viejo", en contraposición al "romancero nuevo", con autores ya reconocidos, compuesto a partir del . Los músicos españoles del Renacimiento utilizaron algunos como texto para sus composiciones.

Los romances derivan, con bastante probabilidad, de los cantares de gesta: ante las actitudes y demandas del público, los juglares y recitadores debieron comenzar a resaltar determinados episodios de esos cantares que destacaban por su interés y singularidad; al aislarlos del conjunto del cantar, se crearían los romances. Este carácter esencial de los mismos, llevaba a que fuesen cantados al son de instrumentos en bailes grupales o en reuniones de entretenimiento o trabajo común.

Formalmente, se trata de poemas no estróficos de carácter épico-lírico; esto quiere decir que, aparte de ser narrativos como los cantares de gesta, presentan ciertos aspectos que los aproximan a la poesía lírica, como la frecuente aparición de la subjetividad emocional.

Al derivar de la épica, los versos son largos, de entre 14 y 16 sílabas, y con rima asonante; estos versos presentan lo que se denomina "cesura interna", de forma muy marcada, que tiende a dividirlos en dos partes o hemistiquios con cierta independencia sintáctica. En la evolución del género, estos hemistiquios fueron ganando aún más autonomía, por lo que quedaron fijados en las ocho sílabas, aproximadamente. De ahí que, en ocasiones, y por la influencia de la poesía lírica que utilizaba siempre versos cortos, los romances apareciesen como tiradas de versos octosílabos con rima asonante solo en los versos pares. 

Su temática y naturaleza son muy variadas. Un grupo importante —acaso el más antiguo— pertenece al género épico y podría derivar de cantares de gesta fragmentados y hoy perdidos en su casi totalidad. Otra parte considerable la forman romances líricos de personajes o situaciones muy diversas.

Existen diversas propuestas de clasificación temática; con todo, existen categorías constantes que serían las siguientes:

Estilísticamente, se suelen clasificar en:

Otros rasgos literarios son:

El siglo admiró estas composiciones y no dudó en imitarlas y revitalizarlas. Autores como Lope de Vega, Góngora o Quevedo escribieron romances al modo de los antiguos, formando lo que hoy se conoce como Romancero nuevo.

Se denomina "mester de clerecía" a la técnica literaria (una manera de componer textos literarios) que desarrollaron en el siglo una serie de escritores vinculados a la universidad y a la erudición (la "clerecía"), y que aplicaron a la creación de obras narrativas en verso. 

Al comienzo del siglo las lenguas vernáculas de la península, y concretamente el castellano, habían alcanzado un grado de madurez relativamente alto. Así, tras una fase dedicada al estudio de su gramática, sobre la base del latín, los "clérigos", conocedores además del francés, pudieron elevar al castellano al rango de lengua literaria, o sea, de lengua culta, apta para la escritura de todo tipo de obras. Por otro lado, hacia 1200 la mayoría de la población ya no entendía el latín. En estas circunstancias, debió de parecer inútil seguir usando una lengua solo entendida por una minoría en obras que, por el interés de su contenido histórico, didáctico, moral o religioso, convenía que fuesen conocidas y entendidas por todos. 

El modelo literario que sirvió de punto de referencia para estos escritores fue el "Libro de Alexandre", sobre todo en lo que se refiere al uso de la estrofa que caracteriza sus obras: la "cuaderna vía". Con todo, el "Alexandre" es una adaptación libre al castellano de la "Alexandreis" (h. 1182), obra en latín del francés Gautier de Châtillon, que servía de lectura escolar en las primeras universidades españolas; de ahí la fuerte impronta de la prosodia latina en el "Alexandre" y, por ejemplo, la proscripción de la sinalefa para obligar a una lectura cuidadosa y despaciosa del texto, característica general de las obras del "mester".

De forma sintética, los rasgos definitorios de las obras del "mester de clerecía" serían los siguientes:

Las obras más importantes del mester de clerecía son "Milagros de Nuestra Señora", de Gonzalo de Berceo, y "El Libro de buen amor", de Juan Ruiz, arcipreste de Hita. Otras obras también relevantes son "El Libro de Alexandre" y "El Libro de Apolonio" y "Rimado de Palacio".

Se trata de una obra narrativa en verso compuesta por un prólogo y por 25 relatos independientes que tratan sendos milagros llevados a cabo por la Virgen. No son historias enteramente originales de Berceo, por cuanto lo que hace es seguir lo escrito en un manuscrito latino que él recrea.

La intención de la obra es presentar un conjunto de ejemplos morales, pero que ante todo sea un tratado, literario y doctrinal, sobre la Virgen María, en el que sobre todo destaque su carácter de mediadora de todas las gracias.

También conocido con el título de "Libro del arcipreste", es una narración autobiográfica en verso, ya del segundo cuarto del siglo . Trata, fundamentalmente, del amor. 

Con la excusa del relato de sus propias aventuras amorosas, casi siempre frustradas, el narrador pretende, en última instancia, advertir y aconsejar al lector u oyente sobre el peligro de los pecados de la carne.

Con todo, el libro presenta una estructura muy heterogénea: no solo está inspirado en tradiciones cultas (latinas) y populares a la vez, sino que alterna partes narrativas con otros didácticas, proverbiales y líricas, y pasa del tono humorístico al moralizante de forma continua.

Su interpretación es objeto de controversia entre los especialistas.

La prosa en castellano se inició con los géneros de carácter didáctico o moralizante. La prosa de ficción en castellano surgió a mediados del siglo , aunque en aquellos momentos se trataba de obras cuyos modelos remontaban al mundo oriental, aunque no siempre. 

Se trata de colecciones de cuentos o recopilaciones de "exempla" como el "Calila y Dimna" (la primera colección vernácula, basada en una colección hindú de fábulas animales) y el "Libro de los engaños e los asayamientos de las mugeres", conocido como "Sendebar" (cuyo título original pudo haber sido "Los assayamientos de las mugeres").

Luego, tras la época de Alfonso X, la prosa, beneficiándose del prestigio adquirido en las obras sobre todo historiográficas, empezará a aparecer como herramienta para componer novelas. De esta manera, las obras novelísticas de la Edad Media son transformaciones de la historiografía, como lo demuestra el hecho de que sus primeras muestras sean adaptaciones libres de temas procedentes de la antigüedad considerados históricos. 

Así, pues, al principio, los personajes son siempre individuos de dignidad regia o similar, abriéndose paulatinamente a otros sectores sociales, pero siempre mostrando preferencia por personajes con rasgos atractivos. Consecuentemente, la novela caballeresca se convierte en el género narrativo más abundante de la Edad Media.

En el grupo de novelas de contenido más histórico destaca "La gran conquista de Ultramar", sobre las cruzadas del siglo (y en la que aparece la famosa historia del "Caballero del cisne"). 

El siglo se abre con el "Libro del cavallero Çifar", primer libro de caballerías hispánico. Su elaboración se inicia en tiempos de Sancho IV y su estructura se enriquece a lo largo del siglo . Comienza como una adaptación de la vida de san Eustaquio, sobre la que se ensamblan diversos elementos. La redacción que nos ha llegado se compone de dos prólogos y cuatro partes. Las dos primeras partes —«El caballero de Dios» y «El rey de Mentón»— siguen una historia de separación y encuentro de los miembros de una familia. En ellas se entretejen colecciones de ejemplos y sentencias. La tercera parte, titulada “Castigos del rey de Mentón”, recoge los consejos que Zifar —ya rey de Mentón— da a sus hijos Garfin y Roboán. La cuarta narra la historia de Roboán desde que abandona el reino de Mentón hasta que consigue ser coronado emperador. 

El aumento de la presencia de los episodios amorosos en las novelas de caballerías dio como resultado la aparición, entre mediados del siglo y 1548, del género de la "ficción sentimental". Aún teniendo como fondo relatos propiamente caballerescos, el ambiente ahora es el mismo que se refleja en la poesía cancioneril: la vida cortesana. Las tramas suelen ser dobles, y se centran en la separación de los amantes; abundan en esta novelas los recursos tendentes a conferir verosimilitud a lo narrado, especialmente el autobiografismo y el uso del discurso directo de los personajes (cartas, intervenciones...). Todos estos rasgos se encuentran fijados en la novela de Juan Rodríguez del Padrón, "Siervo libre de amor", y en la obra maestra del género, "Cárcel de amor" (h. 1488), de Diego de San Pedro. 

El gallego Juan Rodríguez del Padrón nace a finales del siglo y viaja por Europa, antes de tomar el hábito franciscano en 1441 en Jerusalén. La primera de sus obras es la más importante, por inaugurar el nuevo género de la ficción sentimental, que culminará con el fin de siglo: se trata del "Siervo libre de amor" (1439). Con estilo latinizante narra, en su primera parte, cómo la amada desprecia al amante por confiar a un falso amigo su pasión. El Entendimiento, personaje alegórico, disuade en la segunda parte al protagonista de la idea del suicidio e introduce la Estoria de dos amadores —amor trágico de Ardanlier y Liesa, que termina con la muerte de ambos—. Se establece una tercera parte en que el autor, solo y desesperado de amor, encuentra una extraña nave que lo aguarda.
La ficción sentimental alcanza su mayor éxito con Diego de San Pedro y su "Cárcel de amor". El argumento es el siguiente: Leriano consigue del Autor que la princesa Laureola corresponda a su amor, respondiendo una carta suya. Denunciada a su padre, el rey, Laureola es condenada a muerte y salvada por Leriano, que, al ver su amor rechazado, se quita la vida bebiendo las cartas de Laureola disueltas en veneno. 

Al canciller de Castilla, Pero López de Ayala (1332-1407), debemos la "Crónica del rey don Pedro", a la que siguieron las de Enrique II, Juan I y Enrique III. Son unas narraciones que presentan personajes y situaciones vividas por él, con puntos de vista y justificaciones de su actitud no siempre clara.

Por último, a finales del siglo aparece la novela dialogada "La Celestina", obra de transición hacia el Renacimiento.

Don Juan Manuel (1282-1348), sobrino de Alfonso X, es el prosista de más personalidad del siglo . 

Su primer libro debió escribirlo entre 1320 y 1324: es la "Crónica abreviada", resumen de una de las derivadas de las de Alfonso X. El "Libro de los estados", escrito entre 1327 y 1332, es un desahogo de sus preocupaciones y amarguras. En él expone la realidad política y social de su tiempo. 

Su obra más conocida es el "Libro de los enxiemplos del Conde Lucanor e de Patronio", compuesto en 1335. Consta de dos prólogos y cinco partes, la primera de las cuales es la más célebre por sus cincuenta y un ejemplos o cuentos, tomados de fuentes diversas: árabes, latinas o de crónicas castellanas. 

Todas las narraciones de esta primera parte tienen la misma estructura:

"La Celestina" es el título por el que se conoce la "Comedia o Tragicomedia de Calisto y Melibea", la cual fue publicada en dos versiones diferentes: una en 1499, que constaba de 16 actos; y otra, en 1508, que tiene 21. Pertenece al género de la comedia humanística, género inspirado en la comedia latina, que estaba destinado a ser leído y no representado.

El autor es Fernando de Rojas, nacido en La Puebla de Montalbán (Toledo), hacia 1475, de familia conversa (judíos convertidos al cristianismo), que estudió leyes en Salamanca y fue alcalde de Talavera de la Reina. Murió en 1541.

La obra cuenta cómo Calisto, joven noble, entra en un jardín para recobrar su halcón perdido, y allí conoce a Melibea, de la que se enamora y que le rechaza inicialmente. Calisto, por consejo de su criado Sempronio, contrata los servicios de Celestina para alcanzar los favores de la muchacha. Aquella consigue con sus trucos concertar una cita entre Calisto y Melibea y, como premio, recibe del enamorado una cadena de oro. Sempronio y Pármeno, criados de Calisto y socios de Celestina en el negocio, reclaman su parte. La anciana se niega al reparto y ambos la asesinan, crimen por el que son ajusticiados. Sus compañeras, Elicia y Areúsa, deciden vengarse por lo sucedido en las personas de los amantes contratando a Centurio. Una noche, estando Calisto con Melibea, al oír los ruidos provocados por Centurio y sus acompañantes, el amante resbala de una escala y muere. Melibea, desesperada, se arroja al vacío desde una torre de la casa de su padre, Pleberio, quien cierra la obra con un lamento por su hija muerta.

El rasgo más llamativo de la obra es su "realismo", al retratar el ambiente burgués y la crisis de los ideales heroicos y religiosos frente a la importancia que adquiere el dinero.

Como declara Fernando de Rojas en los dos prólogos de la obra, el tema de la misma es advertir contra la corrupción que ocasionan los malos y lisonjeros sirvientes y contra los males que provoca el amor profano; por otra parte, en un plano superior, el tema es la concepción de la vida como una lucha a la manera de Heráclito: "Todas las cosas son criadas a manera de contienda o batalla". De ahí que se enfrenten siempre los estamentos sociales de los señores y los siervos, los sexos y aun el mismo lenguaje, que por un lado abunda en rasgos populares (exclamaciones, palabras patrimoniales, refranes, frases cortas, diminutivos, sintaxis suelta) y por otro en rasgos cultos y cortesanos (expresiones engoladas y latinizantes, cultismos, sentencias y apotegmas de autor conocido, periodos largos, hipérbaton). 

Los personajes celestinescos también muestran una perfecta caracterización y el autor los suele agrupar en parejas para construir mejor por contraste su psicología: los criados Pármeno (joven y aún idealista) y Sempronio (más viejo y cínico); Tristán y Sosia, los criados que les sustituyen; las prostitutas Elicia y Areusa, una más independiente que la otra; los privilegiados Calisto y Melibea, Pleberio y Alisa... Solamente dos personajes aparecen más o menos aislados: Celestina, que representa la subversión del placer sexual, y la criada de Melibea, Lucrecia, que encarna la represión y el resentimiento. 

Melibea es una mujer enérgica y que toma sus propias decisiones. Es arrogante, apasionada, hábil para improvisar y con un carácter fuerte. 

Calisto se muestra débil de carácter, que olvida sus obligaciones y solo piensa en sí mismo y en el interés sexual por Melibea. 

Celestina se presenta como una persona vital, movida fundamentalmente por la codicia. 

Los criados no guardan fidelidad a su amo y buscan su propio beneficio también. Esta actitud la muestra Sempronio desde el principio y Pármeno una vez que sus advertencias sobre Celestina son despreciadas por Calisto y Celestina lo corrompe con ayuda de una pupila suya. 

El lenguaje se muestra también con total realismo. Así, se utiliza el lenguaje culto (lleno de figuras retóricas, especialmene antítesis y geminaciones, hipérbaton, homoteleuton, cultismos, etc.) y el lenguaje vulgar (repleto de obscenidades, palabras malsonantes, amenazas, refranes, etc.). Cada personaje utiliza el nivel del lenguaje que le es propio. Celestina utilizará el que más le interese en función del personaje con el que hable.

El teatro medieval castellano cuenta con testimonios confusos, escasos e irregulares, hasta el punto de haberse puesto en duda su existencia hasta finales del siglo . 







</doc>
<doc id="5027" url="https://es.wikipedia.org/wiki?curid=5027" title="Augusto Monterroso">
Augusto Monterroso

Augusto Monterroso Bonilla (Tegucigalpa, Honduras, 21 de diciembre de 1920-Distrito Federal, 7 de febrero de 2003), también conocido como Tito Monterroso, fue un escritor hondureño, nacionalizado guatemalteco y exiliado en México, conocido por sus relatos breves.

Al estallar en 1944 las vueltas contra el dictador Jorge Ubico, Monterroso desempeñó un activo papel, lo que le llevó a la cárcel al tomar el poder el general Federico Ponce Vaides, pero en septiembre logró escapar de prisión y pidió asilo en la embajada de México. Tras la revolución de octubre en Guatemala, encabezada por Jacobo Arbenz, Monterroso fue designado para un cargo en el consulado de Guatemala en México, donde permaneció hasta 1953. Tras la caída de Arbenz se exilió en Chile,luego, para retornar a México en 1956, país en el que iba a establecerse definitivamente.

Narrador y ensayista, empezó a publicar sus textos a partir de 1959, año en que se publicó la primera edición de "Obras completas y otros cuentos", conjunto de incisivas narraciones donde comienzan a notarse los rasgos fundamentales de su narrativa: una prosa concisa, breve, aparentemente sencilla que sin embargo está llena de referencias cultas, así como un magistral manejo de la parodia, la caricatura, y el humor negro.

Tito, como lo llamaban sus allegados, el gran escritor de cuentos y fábulas breves, falleció de un paro cardíaco el 7 de febrero de 2003. Estuvo casado con la escritora de origen libanés Bárbara Jacobs, quien en el 2008 donó su legado artístico a la Universidad de Oviedo.
Es considerado como uno de los maestros de la minificción y, de forma breve, aborda temáticas complejas y fascinantes, con una provocadora visión del mundo en el universo, haciendo habitual la sustitución del nombre por el apócope. Entre sus libros destacan, además: "La oveja negra y demás fábulas" (1969), "Movimiento perpetuo" (1972), la novela "Lo demás es silencio" (1978); "Viaje al centro de la fábula" (conversaciones, 1981); "La palabra mágica" (1983) y "" (1987). En 1998 publicó su colección de ensayos "La vaca".

Su composición "Cuando despertó, el dinosaurio todavía estaba allí" se consideraba el microrrelato más breve de la literatura universal hasta la aparición de «El emigrante», de Luis Felipe Lomelí. Se ha incluido en una docena de antologías y se ha traducido a varios idiomas, además de tener una edición crítica de Lauro Zavala titulada "El dinosaurio anotado". Con razón, Monterroso aseveró sobre este microrrelato que «sus interpretaciones eran tan infinitas como el universo mismo». En 1970, ganó el premio Magda Donato; en 1975, el Premio Xavier Villaurrutia, por "Antología personal", y en 1988 se le entregó la condecoración del Águila Azteca, por su aporte a la cultura de México. Fue galardonado con el Premio FIL de Literatura en Lenguas Romances (México) en 1996. En 1997 el Ministerio de Cultura y Deportes de Guatemala le otorgó el Premio Nacional de Literatura Miguel Ángel Asturias. En 2000 le fue concedido el Premio Príncipe de Asturias de las Letras en reconocimiento a toda su carrera. En las palabras del jurado: «su obra narrativa y ensayística constituye todo un universo literario de extraordinaria riqueza ética y estética, del que cabría destacar un cervantino y melancólico sentido del humor. (...) Su obra narrativa ha transformado el relato breve».


=Cervantes=



</doc>
<doc id="5031" url="https://es.wikipedia.org/wiki?curid=5031" title="Horario de verano en el mundo">
Horario de verano en el mundo

Existen dos tipos de horario diferentes: el horario de verano y el horario de invierno (u horario estándar). El cambio de hora que algunos países o sus subdivisiones aplican una vez al año hace que del horario estándar (o de invierno) se pase al horario de verano. La primera vez que se aplicó este cambio de hora fue durante la Primera Guerra Mundial.
Desde entonces, nunca más hubo cambio de hora hasta la crisis del petróleo de 1973, a partir de la cual algunos países modificaron su horario oficial con el objetivo declarado de aprovechar mejor la luz solar, de manera que se consumía menos electricidad.

El origen de esta idea se remonta a 1784, cuando Benjamin Franklin —entonces embajador de los Estados Unidos en Francia— envió una carta al diario "Le Journal" de París donde proponía algunas medidas para el ahorro energético.

Franklin propuso tres medidas:

Estas propuestas no se tomaron en serio, pero al poco tiempo comenzaron los primeros experimentos de iluminación con gas, cuya peligrosidad hizo plantearse seriamente el tema del ahorro energético.

Así, poco a poco, las ideas de Franklin fueron retomándose y evolucionando hasta llegar a la conclusión de que lo más conveniente era cambiar la hora, una medida que no se instauró plenamente sino hasta 1974.

Desde 2010 Argentina usa como hora oficial el todo el año en todo el territorio. El mismo que Uruguay y el este de Brasil. 

Geográficamente el país se extiende por las longitudes correspondientes a los husos horarios UTC −4 o UTC −5. 

Argentina adhiere a la Convención Internacional del Huso Horario en 1920 y en ese momento se adoptó como hora oficial el huso horario UTC-4, el huso donde está ubicada de Buenos Aires. Esto se mantendría hasta principios de la década de 40 momento en el que comienza una transición que dura hasta 1969. Durante esta transición la hora oficial (o de invierno) es UTC-4 unos años y otros, UTC -3. Desde 1970 la hora oficial (o de invierno) es UTC-3, situación que se mantiene hasta hoy. 

Argentina ha usado el horario de verano en poco menos del 30% de los años desde 1920 a 2020. A veces durante 10 años, a veces solo un verano. En cada momento, el horario de verano implica adoptar como huso horario, el ubicado más al este. Hasta 1969 el horario de verano era UTC-3. Desde 1974, es UTC-2. La primera vez que se usa el horario de verano es durante la década de 1930, se usa por toda la década durante la cual se cambiaba la hora por 4 meses. En la década de 1960, destaca un largo retorno de 9 años del horario de verano. Esta vez el periodo de horario cambiado se extiende por 5 o 6 meses. En enero de 1974 se aplica por primera vez UTC - 2 como horario de verano, pero solo por los 3 meses de ese verano. Entre fines de la década de 1980 y comienzos de la de 1990, e iniciada en el marco de una crisis eléctrica, vuelve el horario de verano con el esquema de 1974. La última vez que se retoma el horario de verano es en los años 2008 y 2009, esta vez, incluso con diferencias de horario entre provincias. Pero para marzo de 2010, todo el territorio vuelve al UTC-3 todo el año.
En Belice se adoptó el horario de verano por primera vez en su historia en 1918, pasando de UTC −6 a UTC −5:30, y así estuvo conformado el horario de verano hasta 1973, cuando se decidió aplicarlo de UTC −6 a UTC −5, y así se conformó hasta 1983, año en que se dejó de utilizar el horario de verano como medida de ahorro energético. Actualmente, Belice está en UTC −6 durante todo el año.
En Bolivia se adoptó por primera vez el horario de verano en octubre de 1931, pasando de UTC −4:30 (el huso horario oficial de aquel entonces) a UTC −3:30. Así se mantuvo hasta el 20 de marzo de 1932, cuando a las 0:00 se retrasó solamente media hora, cambiando su huso horario oficial, de UTC −4:30 a UTC −4 (este último vigente hasta la actualidad). Desde este entonces nunca más se utilizó el horario de verano.

El 1 de septiembre de 2011, Bolivia iba a implementar por segunda vez en su historia el horario de verano, pasando de UTC −4 a UTC −3, pero sus habitantes se opusieron a la medida, ya que tocaba "madrugar más". Aun así, Roberto Peredo (Viceministro de Hidrocarburos y Energía de Bolivia) dijo que el horario de verano en septiembre fue un intento fallido, pero que probablemente saldría un decreto donde se implementaría el horario de verano (de septiembre a marzo de cada año). José Luis Gutiérrez (Ministro de Hidrocarburos y Energía) afirmó con mensajes contradictorios que el 80% de la población estaba de acuerdo con el horario de verano en Bolivia. Actualmente, está cancelado el uso del horario de verano, pues se dijo que empezaría el 1 de septiembre de 2011, y luego el 1 octubre de 2011; sin embargo, el ministro de Hidrocarburos y Energía declaró que en 2011 no se aplicará el horario de verano. Además, él informó el 30 de septiembre de 2011 que el cambio de huso horario, con una hora de adelanto, recién se aplicará el 2012, debido a que "falta concientizar" a la población boliviana, ya que esta medida se implementará por primera vez en la historia de Bolivia para la generación actual, y por segunda vez en la historia de Bolivia para los que vivieron la medida en 1931. Así, el 1 de diciembre de 2011 se afirma que la propuesta de usar el horario de verano en Bolivia queda postergada, y que en 2012 posiblemente podría aplicarse dicha medida. Sin embargo, el gobierno boliviano estableció seguir con su huso horario oficial (UTC −4) sin horario de verano para cada año.

Brasil adoptó el horario de verano en 1931, pero hoy, sólo las regiones centro-oeste, sur y sudeste lo utilizan.

La aplicación del horario de verano se decidía por decreto presidencial anual. En el período 2008/2009, el horario de verano inició el domingo 19 de octubre a las 00:00 horas, y finalizó el domingo 15 de febrero a las 00:00 horas. Los estados que lo aplicaron fueron: Río Grande del Sur, Santa Catarina, Paraná, San Pablo, Río de Janeiro, Espíritu Santo, Minas Generales, Goiás, Mato Grosso del Sur, Mato Grosso y Distrito Federal.

A partir del 2018, el gobierno de Brasil estableció la fecha fija para el comienzo y el final del horario de verano: desde el primer domingo de noviembre, a las 00:00 horas, hasta el tercer domingo de febrero del año siguiente, a las 00:00 horas. Esta orden fue creada con una excepción: si el tercer domingo del mes de febrero se produjere cerca de la fiesta de carnaval, el horario de verano se aplazará hasta el domingo siguiente.

El horario de verano en los estados del centro-oeste, sur y sudeste de Brasil, en el 2018-2019, va del 4 de noviembre al 17 de febrero, tomando en cuenta la excepción anteriormente mencionada. En 2019, después de que estudios indicaran la disminución de la eficacia del horario de verano en la reducción del consumo de energía eléctrica, en el 2019 gobierno federal decidió dejar de usar el horario.

Geográficamente, a Chile le corresponde como hora estándar la perteneciente a la zona (huso horario) UTC −5; sin embargo, por conveniencia para el país, se adoptó como hora estándar aquella de la zona UTC −4.

El 1 de marzo de 1894, la primera señal horaria oficial comenzó a operar en Valparaíso, a 4 horas 46 minutos y 36 segundos menos respecto a la hora de Greenwich. En 1903 comenzó a operar otra señal horaria oficial en Coquimbo, que estaba sincronizada a 4 horas 45 minutos y 20,7 segundos menos con respecto a Greenwich. El 10 de enero de 1910, Chile adoptó el UTC −5 como su hora oficial. Posteriormente, el 1 de julio de 1916, la hora oficial se fijó según la hora del meridiano del observatorio Astronómico de la Quinta Normal de Santiago —es decir, a 4 horas, 42 minutos y 46,3 segundos menos con respecto a Greenwich—. Más tarde, el 10 de septiembre de 1918, se adoptó el UTC −4 como hora estándar.

Mediante la ley 8522, de 1946, se estableció que la hora oficial para toda la República, desde el 1 de septiembre al 31 de marzo de cada año sería GMT -4, y se denominaría «hora de verano», y desde el 1 de abril al 31 de agosto sería GMT -5, y se denominaría «hora de invierno».
Posteriormente, por la ley 8777, de 1947, se dispuso que la hora oficial sería GMT -4, derogándose la ley 8522.

Durante la gran sequía de 1968, el gobierno de Eduardo Frei Montalva decidió implantar el huso horario de UTC −4 a UTC −3 (horario de verano).

Desde 1970, el cambio se llevó a cabo de la siguiente manera:

El horario de verano se adelantó hasta en 3 semanas en 1988, con motivo del plebiscito del 5 de octubre de ese año, y durante las sequías de 1989 y 1998. Ese último año, los participantes del Encuentro Continental de Jóvenes efectuado en Chile, aprovecharon los beneficios del adelantamiento de dicho horario.

En 2008, debido a las sequías, el horario de verano se aplazó hasta el último domingo de marzo; lo mismo había ocurrido en las sequías de 1989, 1997 y 1999, pero en el caso de 1989, se cambió el horario en abril de 1990, debido a que la fecha coincidió con el cambio de mando presidencial entre Augusto Pinochet y Patricio Aylwin Azócar. Sólo en 1987, el cambio de hora se aplazó hasta el segundo domingo de abril, debido a la visita del papa Juan Pablo II, efectuada entre el 1 y el 5 de abril de ese año. En 2010, debido al gran terremoto que azotó al país a fines de febrero, se decidió aplazar el cambio de horario en tres semanas, es decir, al primer domingo de abril.

Durante los últimos años, varios sectores de la prensa chilena, especialmente el diario "El Mercurio", han publicado editoriales contra el cambio de hora.

En 2009-2010, el horario de verano en Chile insular y Chile continental se inició el 10 de octubre de 2009 y finalizó el 3 de abril de 2010, debido al terremoto ocurrido a fines de febrero de 2010.

Para 2011, el horario de verano debió finalizar el domingo 13 de marzo a las 00:00, pero por una fuerte sequía y ante la escasez energética, el Ministerio de Minería y Energía dijo que finalizaría el horario de verano el próximo 3 de abril de 2011 a las 00:00, pasando de UTC −3 a UTC −4. Sin embargo, el 28 de marzo de 2011, el biministro Laurence Golborne anunció un plan piloto para dicho año, donde se reducirá el horario de invierno, adjudicándole razones sociales, energéticas y de seguridad ciudadana. Por tanto, el horario de verano, que finalizó el 7 de mayo de 2011, inició nuevamente el 20 de agosto del mismo año y terminará el sábado 10 de marzo de 2012, volviendo así a UTC −4. Sin embargo, el horario de verano no terminó en marzo de 2012, sino el 28 de abril de dicho año.

El sábado 1 de septiembre de 2012, a las 23h 59min 59s, los relojes volvieron a adelantarse 60 minutos. Sin embargo, por nueva resolución, el horario de invierno comenzó a la medianoche del 27 de abril de 2013, cuando los relojes se retrasaron 60 minutos. El horario de verano 2013-2014 comenzó a las 23:59:59 del 7 de septiembre, adelantándose los relojes una hora (las 00:00 se convertirán en 01:00 del 8 de septiembre de 2013). Esta medida se mantuvo hasta las 23:59:59 del sábado 8 de marzo de 2014.

El 28 de enero de 2015, durante el segundo gobierno de Michelle Bachelet, se estableció la extensión del horario de verano (UTC-3) hasta marzo de 2017. Sin embargo, el día 13 de marzo de 2016 el gobierno decide dar pie atrás con dicha medida, y restablece el horario de invierno (UTC-4) durante tres meses, desde el segundo sábado de mayo hasta el segundo sábado de agosto. 

Por lo tanto, el horario de verano en Chile va desde las 00:00 del segundo domingo de agosto (adelantándose una hora), hasta las 00:00 del segundo domingo de mayo (atrasándose una hora). Para el periodo 2018-2019, va del domingo 12 de agosto, al domingo 12 de mayo.

Históricamente, Colombia sólo ha utilizado una vez horario de verano en su territorio. Entre 1992 y 1993 durante el gobierno del entonces presidente César Gaviria Trujillo, la reducción en las reservas de agua en los embalses, agravada por el fenómeno de El Niño, condujo a una crisis energética nacional que forzó a las autoridades a implementar una serie de apagones eléctricos programados que duraron más de un año. Para contrarrestar los efectos negativos de los cortes de energía eléctrica, el gobierno tomó la decisión de adelantar una hora los relojes en el país, pasando de UTC −5 a UTC −4 a la medianoche del 2 de mayo de 1992. La medida, conocida informalmente como "Hora Gaviria", duró nueve meses. Desde entonces, Colombia no ha vuelto a adoptar el horario de verano y mantiene su horario oficial: UTC −5. La hora legal de Colombia es mantenida y coordinada por el Instituto Nacional de Metrología.
En Costa Rica se adoptó por primera vez el horario de verano en 1979, pasando de UTC −6 a UTC −5. Desde este entonces se aplicó en 1980 (desde febrero hasta mayo), en 1991 (desde enero hasta junio) y en 1992 (desde enero hasta marzo) para ahorrar energía. A partir de entonces nunca más se empleó el horario de verano en Costa Rica, y actualmente está en su horario oficial: UTC −6.
Cuba ha adoptado el horario de verano por varias veces. La primera fue en 1928, pero desde allí se empezó a utilizar comúnmente, pasando de UTC −5 a UTC −4. Según las autoridades, ""la aplicación del horario de verano constituye una medida de alta incidencia en el ahorro energético"". Así lo afirmó la Unión Eléctrica (estatal).

El horario de verano inicia el segundo domingo de marzo, y finaliza el primer domingo de noviembre.

Cuba genera el 98% de su electricidad en termoeléctricas, que queman una parte importante de los 158 000 barriles diarios que consume la isla, 100 000 de ellos importados de Venezuela con facilidades de pago, y el resto de producción nacional. Nunca ha pagado esa deuda ni anticipos.

En 2011, muchos cubanos fueron interrogados por la televisión local sobre la aplicación del horario de verano, y muchos de los encuestados lamentaron tener que levantarse cuando aún está oscuro, pero otros celebraron que los días sean más largos para realizar deportes y actividades al aire libre.
En El Salvador, la Comisión Ejecutiva Hidroeléctrica del Río Lempa (CEL) aplicó el horario de verano durante el periodo de mayo a septiembre de 1987, pasando de UTC −6 a UTC −5. Esta medida se aplicó para resolver un momento de crisis energética.

El cambio de hora produjo una reducción del 3,15% en la generación bruta; es decir, 25 106 728 kWh, y la demanda máxima se redujo en 6,62% en promedio; 7% en los días hábiles, 3,3% en los sábados, y casi 9% en los domingos. El ahorro en combustible fue notable en el periodo de mayo-junio, en el que se ahorraron 9 566 kWh, equivalentes a 1 471 715 galones de diésel, con un costo de 5,4 millones de colones.

Desde entonces nunca más se volvió a aplicar el horario de verano en aquel país, y su horario oficial y actual es el UTC −6.
Guatemala empezó a usar horario de verano por primera vez en noviembre de 1973, pasando de UTC −6 a UTC −5. Así se mantuvo hasta 1974, cuando volvió a su hora oficial. Desde allí no se usó más el horario de verano hasta mayo de 1983, cuando se decidió aplicar el horario de verano hasta septiembre de ese mismo año, cuando volvió a su hora oficial. Nuevamente observó el horario de verano en marzo de 1991 hasta septiembre de ese mismo año. Por último, otra vez lo empleó desde abril hasta septiembre de 2006. Desde aquel entonces nunca más se volvió a aplicar, pues el presidente de aquella época (Óscar Berger) decidió cancelarlo, ya que no contraía medidas de seguridad, sino que el cambio de hora resultaba en amenaza para el país, pues hacía que madrugaran más los ciudadanos.

En la actualidad Guatemala está en su horario oficial: UTC −6.
En Honduras se utilizó el horario de verano en varias ocasiones para ahorrar energía. Se empezó a utilizar en 1987 y 1988, desde mayo hasta septiembre de aquellos años. Después se dejó de utilizar, pero volvió a observarlo en el año 2006, desde mayo hasta agosto.

Es rara la vez que se cambia el horario, pero en estos momentos Honduras está en su zona horaria oficial (UTC −6) y en caso de volverlo a utilizar, pasará de UTC −6 a UTC −5.

En México, el horario de verano empezó a aplicarse desde 1996, con la idea de que así se evitaba el desfase económico que había con Estados Unidos dos veces por año (ya que este país fronterizo con México ya aplicaba el horario de verano desde antes). Así se evitarían algunos desórdenes como las operaciones financieras y los vuelos internacionales.

En un principio, todo el territorio mexicano adoptó el horario de verano, pero el estado de Sonora dejó de utilizarlo en 1998, debido a lo poco beneficioso que resultó a causa de las temperaturas tan altas registradas en dicha región, y para mantenerse a la par del horario del estado de Arizona, Estados Unidos, donde no se utiliza el horario de verano. Esto causa que los estados de Baja California y Sonora empaten sus horarios en UTC −7 durante el verano septentrional.

En México, cada año el horario de verano inicia el primer domingo de abril (adelantando una hora a los relojes de las a las ) y termina el último domingo de octubre (atrasando una hora los relojes de la a las 1:00 a. m.). Solamente en el año 2001, se inició el primer domingo de mayo y terminó el último domingo de septiembre, volviendo en 2002 a las fechas originales, las cuales habían sido adoptadas en 1996 porque en ese entonces coincidían con la aplicación del horario de verano en los Estados Unidos.

A partir de 2007, Estados Unidos modificó su aplicación del horario de verano iniciando el segundo domingo de marzo y terminando el primer domingo de noviembre, seguido de Canadá y otros países de la región. Sin embargo, México se ha rehusado desde entonces a realizar dicha modificación a pesar del desfase que se ocasiona por no hacerlo. Inicialmente, se mantuvo el horario de verano sin cambios. En 2009, las ciudades fronterizas con Estados Unidos solicitaron nuevamente al congreso el cambio de fechas para que el horario de verano en México volviera a quedar a la par con el de Estados Unidos. Así se cambió el decreto original promulgado en 2002 por el H. Congreso de la Unión (la nueva versión fue publicada en el "Diario Oficial de la Federación" el 6 de enero del 2010 por el Poder Ejecutivo), agregando un párrafo con la regla de aplicación del horario de verano modificada únicamente para la franja fronteriza del norte del país según el siguiente texto:

""En los municipios fronterizos de Tecate y Mexicali en Baja California; Ciudad Juárez y Ojinaga en Chihuahua; Acuña y Piedras Negras en Coahuila; Anáhuac en Nuevo León; y Nuevo Laredo, Reynosa y Matamoros en Tamaulipas, la aplicación de este horario estacional surtirá efecto desde las dos horas del segundo domingo de marzo, y concluirá a las dos horas del primer domingo de noviembre (adoptando el horario de verano estadounidense).""

Y hasta la actualidad sigue vigente esa regla excepcional, desfasando a las ciudades fronterizas respecto al resto del país durante cuatro o cinco semanas cada año. En septiembre de 2015 se presentó nuevamente al congreso la iniciativa de modificar el horario de verano en el resto del país para hacerlo coincidir con Estados Unidos, y eliminar con ello la necesidad de un párrafo de excepción para la frontera. La propuesta fue desechada el 29 de junio de 2016 siendo esta la tercera negativa del congreso mexicano a modificar el horario de verano a escala nacional en un periodo de casi diez años.

Desde el 1 de febrero del 2015, el estado de Quintana Roo cambió su huso horario estándar al UTC −5, y ya no aplica más el horario de verano.
Nicaragua adoptó por primera vez el horario de verano en 1973, luego del siniestro terremoto en la ciudad de Managua, en la Navidad del año anterior, como una medida para ahorrar energía y dinero; así pasó de UTC −6 a UTC −5. Esta medida permaneció hasta el año 1975, en que se regresó a la hora solar prolongando el horario de verano por 2 años. Así permaneció hasta el año 1979, en que el gobierno somocista volvió a adelantar la hora por un periodo de tan sólo 4 meses. El siguiente año se aplicó el cambio horario con la misma duración. Durante varios años, la propuesta del cambio horario quedó descartada hasta el año 1980. También se utilizó en el año 1992, cuando debido a una fuerte crisis económica del país, el gobierno se vio nuevamente obligado a utilizar este recurso. Por segunda vez la hora oficial quedó adelantada por 2 años desde el 1 de enero de 1992 hasta el 20 de febrero de 1994. Actualmente se ha aplicado el horario de verano en los años 2005 (del domingo 10 de abril al domingo 2 de octubre) y 2006 (del domingo 30 de abril al domingo 1 de octubre), pero nunca más se volvió a utilizar.
Paraguay utiliza el horario de verano desde el año 1975, pasando de UTC −4 a UTC −3. En realidad, se utilizó desde 1972, cuando aún no se había declarado como oficial el cambio de horario.

La legislación vigente al respecto, en todo el país, es el Decreto 1264 del 24 de febrero de 2014, 
haciendo que el horario de verano en Paraguay comience el primer domingo de octubre, y termine el cuarto domingo de marzo. Con la reglamentación anterior, iniciaba el primer domingo de octubre y finalizaba el segundo domingo de abril.

Para el periodo 2018-2019, el horario de verano en Paraguay va del 7 de octubre al 24 de marzo.
Perú ha utilizado el horario de verano en varias ocasiones (1938, 1939, 1985, 1986, 1987, 1990 y 1994). Debido a las sequías e inundaciones causadas por "El Niño" entre 1982 y 1983, el segundo gobierno de Fernando Belaúnde Terry decidió adoptar el horario de verano a fines de ese último año. La última vez que se utilizó ese horario fue en 1994, pasando de UTC −5 a UTC −4. Esa disposición fue utilizada en todo el país. A partir de entonces nunca más se volvió a aplicar el horario de verano.
De 1942 a 1945 Puerto Rico utilizó el horario de verano pasando de UTC-4 a UTC-3. A las 00:00 tenían que adelantar una hora para el ahorro de energía. Comenzando el segundo domingo de marzo y terminando el primer domingo de noviembre. Al ver que este método no tuvo éxito en el país decidieron quitar el horario de verano y nunca más ha sido utilizado. El huso horario actual de Puerto Rico es UTC-4.
En Uruguay, el horario de verano empezó a observarse desde 1923, cuando su hora oficial, desde 1920, era el UTC −4. En 1942 se decretó la hora de verano en UTC −2, y la hora oficial en UTC −3.

El motivo de adelantar una hora a las 2:00 del primer domingo de octubre, se debió a un proyecto presentado por el gobierno uruguayo, con el fin de ahorrar energía eléctrica a causa de la sequía estival, y la consecuente escasez de agua en la represas que sustentan este servicio. Se entiende que, al oscurecer una hora más tarde, el consumo disminuirá sutilmente, ahorrando miles de dólares a la economía del país. Una idea similar se sostuvo en algunas provincias argentinas, pero no consiguió mayores beneficios.

Sin embargo, a partir de octubre de 2015 no se observa más el cambio horario, debido al reclamo por parte de organizaciones turísticas del Uruguay, en el que manifestaron que el cambio del uso horario afectaba al sector turístico (principalmente al gastronómico). A raíz del planteamiento hecho por el Presidente de la Cámara Uruguaya de Turismo, al Presidente de la República Oriental del Uruguay, se resolvió poner fin al horario de verano después de 11 años consecutivos en que se modificaba la hora legal del país.




</doc>
<doc id="5035" url="https://es.wikipedia.org/wiki?curid=5035" title="Orson Scott Card">
Orson Scott Card

Orson Scott Card (24 de agosto de 1951) es un escritor estadounidense. Escribe ciencia ficción y otros géneros literarios. Su obra más conocida es "El juego de Ender".

Nacido en Richland, Washington, Card creció en California, Arizona y Utah. Vivió en Brasil dos años como misionero para La Iglesia de Jesucristo de los Santos de los Últimos Días (Iglesia mormona). Es licenciado por la Universidad Brigham Young en 1975 y la Universidad de Utah en 1981. Actualmente vive en Greensboro, Carolina del Norte.

Escritor prolífico, Card es autor de numerosas novelas individuales ("Niños perdidos", "El cofre del tesoro") y diversas sagas como "La Saga del Retorno" o las historias de "Alvin el Hacedor".

Ha ganado numerosos premios Hugo y Nébula, como el Nébula de 1985 y el Hugo de 1986 a la mejor novela por "El juego de Ender" y el Nébula de 1986 y Hugo de 1987 por "La voz de los muertos".

Card ha incursionado dentro del mundo de los cómics al escribir el guion entre los años 2005 y 2006 de la miniserie "Ultimate Iron Man".

Card es uno de los seis hijos de Willard y Peggy Card, y hermano mayor de Arlen Card. Nació en Richland, Washington, y creció en Santa Clara, California así como en Mesa, Arizona y Orem, Utah. Sirvió como misionero para La Iglesia de Jesucristo de los Santos de los Últimos Días en Brasil y se graduó en la Universidad Brigham Young y en la Universidad de Utah; también estuvo un año realizando un programa de doctorado en la Universidad de Notre-Dame. Actualmente vive en Greensboro, Carolina del Norte un ambiente que ha jugado un papel importante en "El juego de Ender" y en muchas otras obras suyas.

Card y su esposa Kristine Allen han tenido cinco hijos, cada uno bautizado con el nombre de uno o más autores que él y su esposa admiran. Los nombres de sus hijos son Michael Geoffrey (Geoffrey Chaucer), Emily Janice (Emily Brontë y Emily Dickinson), Charles Benjamin (Charles Dickens), Zina Margaret (Margaret Mitchell) y Erin Louisa (Louisa May Alcott). Charles, que tenía parálisis cerebral, murió poco después de cumplir los 17 años y su hija Erin murió el mismo día que nació. Orson Scott Card y su esposa viven con su hija más joven, Zina, en Greensboro, North Carolina.

La vida de su hijo Charles ha influenciado las novelas de Orson Scott Card, especialmente la serie "Homecoming". Su hija Emily, junto con otros dos escritores adoptó las historias cortas de su padre "Clap Hands and Sing", "Lifeloop" y "A Sepulchre of Song" para el teatro en "Posing as People".

En el año 2008 Orson Scott Card apareció en el cortometraje "The Delivery", que protagonizaba su hija Emily. Interpreta a un escritor que lee un audiolibro en el cortometraje que ganó el primer puesto de Fantasía en el Dragon*Con Film Festival. Escribió una historia original, "The Emperor of the Air", específicamente para el cortometraje, dirigido por Gabrielle de Cuir y Stefan Rudnicki.

Orson Scott Card es un fan de la serie de televisión "Firefly" y aparece en un documental sobre la serie. También ha actuado como jurado en varios programas de televisión y organizaciones. and the National Organization for Marriage (2009–2013).

La práctica de Card de la religión mormona es un elemento importante de su vida. Uno de sus tatarabuelos era Brigham Young, un importante líder de La Iglesia de Jesucristo de los Santos de los Últimos Días, y todos sus antepasados en las últimas tres generaciones han sido mormones. Entre sus ancestros se incluyen otros mormones destacados como Charles Ora Card, fundador de la colonia mormona de Cardston, en la provincia canadiense de Alberta. Su fe ha sido una fuente de inspiración que ha influenciado sus obras y opiniones personales.

En 2006, un día antes de las elecciones presidenciales en los Estados Unidos, Card escribió un artículo de opinión en RealClearPolitics, en la que a pesar de ser un demócrata, animaba a los votantes a apoyar al partido republicano.
En otro artículo escrito para el "Mormon Times", condenaba la decisión de cualquier gobierno que aprobara el matrimonio gay afirmando que "sin importar la ley, el matrimonio sólo tiene una definición, y cualquier gobierno que intente cambiarla es mi enemigo mortal. Actuaré para destruir ese gobierno y acabar con él..." En el año 2009 Card se convirtió en miembro de la National Organization for Marriage, un grupo que trata de impedir la legalización de los matrimonios entre el mismo sexo. Card ha pedido que las leyes contra la conducta homosexual permanezcan en los libros y un regreso a los valores de América en la década de 1960.

Aunque apoya la investigación de fuentes de energía alternativas y la superación del uso de combustibles fósiles, Card ha criticado con frecuencia las acciones para evitar el calentamiento global y afirma que los presuntos peligros del calentamiento global "se han convertido en una ortodoxia científica que obstaculiza la investigación y el debate".
Su relato corto "Angles" muestra a unos científicos que temen continuar su investigación porque iría en contra del dogma científico establecido. También, quizá por sus creencias religiosas, ha mostrado su desconfianza respecto al darwinismo en oposición a la teoría del Diseño inteligente (de la que también desconfía, pero por razones diferentes). Card critica a los científicos que afirman que el darwinismo explica por completo el funcionamiento de la evolución y también ha dicho que "la ciencia nunca ha demostrado ni podrá demostrar nunca el Diseño Inteligente."


Dentro de su extensa obra destaca La saga de Ender formada por seis libros: 


Aunque Scott Card afirmó que no proseguiría la saga de Ender, ha escrito una nueva saga paralela a la Saga de Ender, formada hasta el momento por cinco libros: 









</doc>
<doc id="5037" url="https://es.wikipedia.org/wiki?curid=5037" title="2003">
2003

2003 () fue un año común comenzado en miércoles según el calendario gregoriano. Fue designado:




















































Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.






</doc>
<doc id="5039" url="https://es.wikipedia.org/wiki?curid=5039" title="Memoria flash">
Memoria flash

La memoria flash permite la lectura y escritura de múltiples posiciones de memoria en la misma operación. Gracias a ello, la tecnología "flash", mediante impulsos eléctricos, permite velocidades de funcionamiento superiores frente a la tecnología EEPROM primigenia, que solo permitía actuar sobre una única celda de memoria en cada operación de programación. 

Se trata de la tecnología empleada en las memoria USB , unidades de estado sólido y las actuales BIOS.

La historia de la memoria flash siempre ha estado muy vinculada con el avance del resto de las tecnologías a las que presta sus servicios como "routers", módems, BIOS de las PC, "wireless", etcétera. En 1984, fue Fujio Masuoka quien inventó este tipo de memoria como evolución de las EEPROM existentes por aquel entonces. Intel intentó atribuirse la creación de esta sin éxito, aunque sí comercializó la primera memoria flash de uso común.


</doc>
<doc id="5040" url="https://es.wikipedia.org/wiki?curid=5040" title="EEPROM">
EEPROM

EEPROM o E²PROM son las siglas de "Electrically Erasable Programmable Read-Only Memory" (ROM programable y borrable eléctricamente). Es un tipo de memoria ROM que puede ser programada, borrada y reprogramada eléctricamente, a diferencia de la EPROM que ha de borrarse mediante un aparato que emite rayos ultravioleta. Son memorias no volátiles.

Las celdas de memoria de una EEPROM están constituidas por un transistor MOS, que tiene una compuerta flotante (estructura SAMOS), su estado normal está cortado y la salida proporciona un 1 lógico.

Aunque una EEPROM puede ser leída un número ilimitado de veces, sólo puede ser borrada y reprogramada entre 100.000 y un millón de veces.

Estos dispositivos suelen comunicarse mediante protocolos como I²C, SPI y Microwire. En otras ocasiones, se integra dentro de chips como microcontroladores y DSPs para lograr una mayor rapidez.

La memoria flash es una forma avanzada de EEPROM creada por el Dr. Fujio Masuoka mientras trabajaba para Toshiba en 1984 y fue presentada en la Reunión de Aparatos Electrónicos de el IEEE de 1984. Intel vio el potencial de la invención y en 1988 lanzó el primer chip comercial de tipo NOR.



</doc>
<doc id="5041" url="https://es.wikipedia.org/wiki?curid=5041" title="Memoria EPROM">
Memoria EPROM

EPROM son las siglas de "Erasable Programmable Read-Only Memory" (ROM programable borrable). Es un tipo de chip de memoria ROM no volátil inventado por el ingeniero Dov Frohman de Intel. Está formada por celdas de FAMOS (Floating Gate Avalanche-Injection Metal-Oxide Semiconductor) o "transistores de puerta flotante", cada uno de los cuales viene de fábrica sin carga, por lo que son leídos como 1 (por eso, una EPROM sin grabar se lee como FF en todas sus celdas).

Las memorias EPROM se programan mediante un dispositivo electrónico, como el Cromemco Bytesaver, que proporciona voltajes superiores a los normalmente utilizados en los circuitos electrónicos. Las celdas que reciben carga se leen entonces como un 0.

Una vez programada, una EPROM se puede borrar solamente mediante exposición a una fuerte luz ultravioleta. Esto es debido a que los fotones de la luz excitan a los electrones de las celdas provocando que se descarguen. Las EPROM se reconocen fácilmente por una ventana transparente en la parte alta del encapsulado, a través de la cual se puede ver el chip de silicio y que admite la luz ultravioleta durante el borrado.

Como el cuarzo de la ventana es caro de fabricar, se introdujeron los chips OTP ("One-Time Programmable", programables una sola vez). La única diferencia con la EPROM es la ausencia de la ventana de cuarzo, por lo que no puede ser borrada. Las versiones OTP se fabrican para sustituir tanto a las EPROM normales como a las EPROM incluidas en algunos microcontroladores. Estas últimas fueron siendo sustituidas progresivamente por EEPROMs (para fabricación de pequeñas cantidades donde el coste no es lo importante) y por memoria flash (en las de mayor utilización).

Una EPROM programada retiene sus datos durante diez o veinte años, y se puede leer un número ilimitado de veces. Para evitar el borrado accidental por la luz del sol, la ventana de borrado debe permanecer cubierta. Las antiguas BIOS de los ordenadores personales eran frecuentemente EPROM y la ventana de borrado estaba habitualmente cubierta por una etiqueta que contenía el nombre del productor de la BIOS, su revisión y una advertencia de copyright.

Las EPROM pueden venir en diferentes tamaños y capacidades. Así, para la familia 2700 se pueden encontrar:

Pines de la EPROM 2764

Una memoria EPROM puede ser borrada con una lámpara de luz UV, del tipo UV-C, que emita radiación en torno a los 2537 Å (Angstrom) o 254nm, a una distancia de unos 2,5 cm de la memoria. La radiación alcanza las células de la memoria a través de una ventanilla de cuarzo transparente situada en la parte superior de la misma.

Para borrar una EPROM se necesita que la cantidad de radiación recibida por la misma se encuentre en torno a los 15 W/cm^2 durante un segundo. El tiempo de borrado real suele ser de unos 20 minutos debido a que las lámparas utilizadas suelen tener potencias en torno a los 12 mW/cm² (12 mW x 20 x 60 s = 14.4 W de potencia suministrada). Este tiempo también depende del fabricante de la memoria que se desee borrar. En este tiempo todos sus bits se ponen a 1.

Es importante evitar la sobreexposición del tiempo de radiación a las EPROM; es decir, la potencia luminosa suministrada a la memoria, pues se produce un envejecimiento prematuro de las mismas.

Debido a que la radiación solar e incluso la luz artificial proveniente de tubos fluorescentes borra la memoria lentamente (de una semana a varios meses), es necesario tapar dicha ventanilla con una etiqueta opaca que lo evite, una vez que son grabadas.
Se debe aclarar que una EPROM no puede ser borrada parcial o selectivamente; de ahí que por muy pequeña que fuese la eventual modificación a realizar en su contenido, inevitablemente se deberá borrar y reprogramar en su totalidad.

Un borrador de EPROM es una caja opaca ópticamente, con una fuente de luz UV del tipo C, la cual también es utilizada para esterilizar instrumentos quirúrgicos y/o como germicida.
Para borrar las EPROM no se puede utilizar las luz "UV Negra", (que es comúnmente utilizada para verificar billetes, tickets, etc.), que emiten en la región UV-A, (365 nm). La única luz que funciona es la UV-C, (254 nm), la cual emite "luz peligrosa" o "germicida", (mata gérmenes). Es "luz peligrosa" porque la exposición prolongada puede causar cataratas a largo plazo y daño en la piel; sin embargo una exposición breve, unos 5 segundos continuos en la piel, no debería de causar más que una leve resequedad, por lo que es necesario tomar todas las precauciones para evitar estos problemas. Dado que este tipo de luz UV-C se encuentra en la luz solar, si se deja una EPROM directamente bajo esta, en algunos días o semanas se borraran; por lo que se requiere proteger las EPROM una vez se hayan programado.

Se puede utilizar una lámpara de tubo normal de 4 W del tipo F4T5 (4 watt, 5 pulgadas) que da luz blanca (ver foto). También un tubo de luz G4T5 "Germicidal UVC", que tiene el vidrio claro, para borrar las EPROM. La "G" es para germicidas, lo mismo que la "F" es para fosforescentes (aunque no tengan fósforo). Otro tipo de lámpara comúnmente utilizada es la PHILIPS TUV 4W-G4T5-240805D-4WTUV.
Las lámparas UV que tienen el vidrio morado o lila son para el espectro UV-Visible o "UV Negra", normalmente están marcadas como U4T5 o similar y no funcionan para borrar las EPROM.

Un tubo de luz fluorescente de luz blanca, tiene una cubierta de fósforo en el interior del vidrio. La Luz UV del mercurio excita el fósforo, el cual re-radia la energía en el rango visible. Las lámparas UV para borradores de EPROM o germicidas usan directamente la luz del vapor de mercurio. El vidrio se debe de hacer de cuarzo, en lugar de vidrio ordinario, para evitar que el vidrio absorba la mayor parte de los rayos UV. El cuarzo es más transparente en las longitudes UV del mercurio. 

También podrían ser borrados si son expuestos a la luz de la soldadura eléctrica (de electrodo), con el riesgo que una "chispa" queme el chip, debido a que se debe de acercar la EPROM como a unos 10 o 15 cm para que reciban la suficiente radiación para borrarlos.(en cuyo caso se podría proteger con una barrera de vidrio trasparente, vaso, ventana etc).; En teoría también se pueden borrar con rayos X, "tomando radiografías del EPROM", el tiempo de borrado dependerá de la calibración/emisión del equipo de rayos X utilizado.

La única diferencia entre los 27256 y los 27C256 es que los 27256 usan NMOS mientras los 27C usan tecnología CMOS. CMOS sólo consume potencia apreciable cuando una señal está cambiando. NMOS usa canal N FET's con elementos resistores, mientras CMOS evita las resistencias que desperdician energía por utilizar ambos canales N y P FET. Además los CMOS evitan la producción de calor, permitiendo arreglos más compactos de transistores de los que los NMOS son capaces. La alta densidad de elementos de los CMOS reduce las distancias de interconexión lo cual incrementa la velocidad. Además CMOS brilla cuando hay una cantidad limitada de energía como cuando se utiliza un sistema alimentado por baterías.

Se presentan algunos problemas en las EPROM CMOS usando programadores viejos, debido a las diferencias en los voltajes de programación, (CMOS tiene 12,5 Vpp). EPROM CMOS también requieren una fuente de voltaje, (Vcc), de exactamente 6 Voltios. CMOS son fáciles de borrar pero tienden a dañarse si son sobre expuestos a la luz UV.


Las siguientes partes pueden ser la misma - (de acuerdo a las guías de referencia de los fabricantes), allí pueden haber algunas diferencias incluyendo el algoritmo usado para programarlos.



</doc>
<doc id="5042" url="https://es.wikipedia.org/wiki?curid=5042" title="Servicio general de paquetes vía radio">
Servicio general de paquetes vía radio

El servicio general de paquetes vía radio, en inglés: General Packet Radio Service (GPRS), fue creado en la década de los 80
Una conexión GPRS está establecida por la referencia a su nombre de punto de acceso (APN). Con GPRS se pueden utilizar servicios como "Wireless Application Protocol" (WAP), servicio de mensajes cortos (SMS), "Multimedia Messaging System" (MMS), Internet y para los servicios de comunicación, como el correo electrónico y la "World Wide Web" (WWW). Para fijar una conexión de GPRS para un módem inalámbrico, un usuario debe especificar un APN, opcionalmente un nombre y contraseña de usuario, y muy raramente una dirección IP, todo proporcionado por el operador de red. La transferencia de datos de GPRS se cobra por volumen de información transmitida (en kilo o megabytes), mientras que la comunicación de datos a través de conmutación de circuitos tradicionales se factura por minuto de tiempo de conexión, independientemente de si el usuario utiliza toda la capacidad del canal o está en un estado de inactividad. Por este motivo, se considera más adecuada la conexión conmutada para servicios como la voz que requieren un ancho de banda constante durante la transmisión, mientras que los servicios de paquetes como GPRS se orientan al tráfico de datos. La tecnología GPRS como bien lo indica su nombre es un servicio orientado a radio-enlaces que da mejor rendimiento a la conmutación de paquetes en dichos radio-enlaces.

El acceso al canal utilizado en GPRS se basa en divisiones de frecuencia sobre un dúplex y TDMA. Durante la conexión, al usuario se le asigna un canal físico, formado por un bloque temporal en una portadora concreta. Ese canal será de subida o bajada dependiendo de si el usuario va a recibir o enviar datos. Esto se combina con la multiplexación estadística en el dominio del tiempo, permitiendo a varios usuarios compartir el mismo canal físico, ya sea de subida o de bajada. Los paquetes tienen longitud constante, correspondiente a la ranura de tiempo del GSM. El canal de bajada utiliza una cola de tipo "First in, first out" (FIFO; primero en entrar, primero en salir) para los paquetes en espera, mientras que el canal de subida utiliza un esquema similar al de ALOHA con reserva.

En resumen, se utiliza un sistema similar al ALOHA ranurado durante la fase de contención, y TDMA con una cola FIFO durante la fase de transmisión de datos.

La tecnología GPRS mejora y actualiza a GSM con los servicios siguientes:


La tecnología GPRS se puede utilizar para servicios como el acceso mediante el Protocolo de Aplicaciones Inalámbrico (WAP), el SMS y MMS, acceso a Internet y correo electrónico.

El método de cobro típico para transferencias de datos usando GPRS es el pago por megabytes de transferencia, mientras que el pago de la comunicación tradicional mediante conmutación de circuitos se cobra por tiempo de conexión, independientemente de si el usuario está utilizando el canal o este se encuentra inactivo. Este último método es poco eficiente debido a que mantiene la conexión incluso cuando no se están transmitiendo datos, por lo que impide el acceso al canal a otros usuarios. El método utilizado por GPRS hace posible la existencia de aplicaciones en las que un dispositivo móvil se conecta a la red y permanece conectado durante un periodo prolongado de tiempo sin que ello afecte en gran medida a la cantidad facturada por el operador.

Existen tres clases de dispositivos móviles teniendo en cuenta la posibilidad de usar servicios GSM y GPRS simultáneamente:
Estos dispositivos pueden utilizar simultáneamente servicios GPRS y GSM.
Solamente pueden estar conectados a uno de los dos servicios en cada momento. Mientras se utiliza un servicio GSM (llamadas de voz o SMS), se suspende el servicio GPRS, que se reinicia automáticamente cuando finaliza el servicio GSM. La mayoría de los teléfonos móviles son de este tipo.
Se conectan alternativamente a uno u otro servicio. El cambio entre GSM y GPRS debe realizarse de forma manual.

Para que un dispositivo de clase A pueda transmitir en dos frecuencias a la vez, necesitaría dos radios. Para resolver este costoso problema, un móvil con GPRS suele implementar la característica conocida como "modo de transferencia dual" ("Dual Transfer Mode", DTM). Un móvil DTM puede usar a la vez el canal de datos y el de voz, puesto que es la red la que coordina y se asegura de que no se requiera transmitir en dos frecuencias diferentes a la vez. Los móviles DTM se consideran de clase A, pero simplificados.

Dependiendo de la tecnología utilizada, la velocidad de transferencia varía sensiblemente. La tabla siguiente muestra los datos de subida y bajada para cada tipo de tecnología.

Para comparar GPRS con GSM se utiliza normalmente la velocidad de transmisión de SMS. Sobre una red GPRS se pueden enviar aproximadamente 30 SMS por minuto, frente a los 6 o 10 SMS que permite GSM.


Cuando se establece la comunicación entre los 2 terminales, se crea un paso físico o lógico entre ellos (ocupando así un canal de comunicación), que se mantiene reservado hasta que se libera la conexión. Estas redes de circuitos conmutados se encargan de llevar los bits desde su punto de origen al de destino, sin identificar PDUs (Protocol Data Unit, paquetes de datos) ni ocuparse de su estructura.

Conmutación de Paquetes (GPRS)

La entidad transmisora segmenta el mensaje a transmitir en PDUs (paquetes de datos) independientes, de tamaño apropiado. La entidad receptora se encarga de reconstruirlos (reensamblarlos) hasta obtener el mensaje original completo. Cada paquete de datos se transfiere de un nodo a otro como una sola unidad. Contienen información de control (direcciones de origen y destino, identificador, etc.) que permite su manejo en la red.

El PDU se almacena temporalmente en cada uno de los nodos por los que pasa mientras espera ser enviado al siguiente. Esto conlleva un aumento del retardo en función del volumen de tráfico existente y de la capacidad del enlace. Todos los PDUs que componen los datos están relacionados unos con otros, pero la forma en que viajan y son reagrupados varía. La propia red puede fragmentar los PDUs si la longitud de estos es mayor que la unidad máxima de transferencia (MTU) de la red.




</doc>
<doc id="5043" url="https://es.wikipedia.org/wiki?curid=5043" title="Enhanced Data Rates for GSM Evolution">
Enhanced Data Rates for GSM Evolution

EDGE es el acrónimo para "Enhanced Data Rates for GSM Evolution" (tasas de Datos Mejoradas para la Evolución del GSM) y también conocida como "Enhanced GPRS" (EGPRS) o GPRS Mejorado.

Es una tecnología de telefonía móvil celular, que actúa como puente entre las redes 2G y 3G. EDGE se considera una evolución del GPRS ("General Packet Radio Service"). Esta tecnología funciona con redes GSM ("Global System for Mobile communications"). Aunque EDGE funciona con cualquier GSM que tenga implementado GPRS, el operador debe implementar las actualizaciones necesarias, actualmente todos los teléfonos móviles soportan esta tecnología.

EDGE, o EGPRS, puede ser usado en cualquier transferencia de datos basada en conmutación por paquetes ("Packet Switched"), como lo es la conexión a Internet. Los beneficios de EDGE sobre GPRS se pueden ver en las aplicaciones que requieren una velocidad de transferencia de datos, o ancho de banda altos, como video u otros servicios multimedia.

Además de usar GMSK ("Gaussian Minimum-Shift Keying"), EDGE usa 8PSK ("8 Phase Shift Keying") para los cinco niveles superiores de nueve esquemas totales de modulación y codificación. En los cuatro primeros niveles se utiliza GPRS propiamente dicho. La utilización de 8PSK produce una palabra de 3 bits por cada cambio en la fase de la portadora. Con esto se triplica el ancho de banda disponible que brinda GSM. El nivel del esquema que se utilice para transmitir depende de la relación C/I (portadora/interferente), el cual será más alto cuanto más grande sea el valor de C/I. Al igual que GPRS, EDGE usa un algoritmo de adaptación de tasas, que adapta el esquema de modulación y codificación (MCS) usado para la calidad del canal de radio y así el índice binario ("bit rate") y la robustez de la transmisión de datos. EDGE agrega una nueva tecnología que no se encuentra en GPRS, la Redundancia Incremental, la cual, en vez de retransmitir los paquetes de información alterados, envía más información redundante que se combina en el receptor, lo cual incrementa la probabilidad de decodificación correcta.

EDGE puede alcanzar una velocidad de transmisión de 384 Kbps en modo de paquetes, con lo cual cumple los requisitos de la Unión Internacional de Telecomunicaciones (UIT o ITU) para una red 3G, también ha sido aceptado por la ITU como parte de IMT-2000, de la familia de estándares 3G. También mejora el modo de circuitos de datos llamado HSCSD, aumentando el ancho de banda para el servicio. EDGE fue estrenado en las redes GSM de Estados Unidos en el año 2003.

Aunque la tecnología UMTS es de mayor capacidad de transferencia y cronológicamente más reciente, sus altos costes de implementación y poco apoyo, hacen que una buena cantidad de operadores de telefonía móvil celular tengan implementada la tecnología EDGE, dominando el mercado global de las comunicaciones GSM/GPRS.

Para la implementación de EDGE por parte de un operador, la red principal, o "core network", no necesita ser modificada, sin embargo, las estaciones base, BTS, sí deben serlo. Se deben instalar tranceptores compatibles con EDGE, además de nuevos terminales (teléfonos) y un software que pueda decodificar/codificar los nuevos esquemas de modulación.

La definición de EDGE, si es de 2G o 3G, depende de su implementación. Mientras la Clase 3 e inferiores, claramente no son 3G, la Clase 4 y superiores presentan un ancho de banda superior a otras tecnologías consideradas 3G (como 1xRTT). En Clase 10, con un ancho de banda superior a 230 Kbps, EDGE logra trascender las definiciones comunes de 2G y 3G.

EDGE/EGPRS se implementa como una mejora de "bolt-on" de 2,5 G GSM/GPRS, por lo que es más fácil para los actuales operadores de GSM para actualizar a la misma. EDGE es un superconjunto de GPRS y puede funcionar en cualquier red con GPRS desplegado en ella, siempre y cuando el transportista implementa la actualización necesaria.

EDGE no requiere ningún cambio de hardware o software que se hizo en las redes GSM básicos. Para que EDGE sea compatible, en las unidades transceptoras debe estar instalado y el subsistema de estación base tiene que ser actualizado para soportar EDGE. Si el operador tiene ya en este lugar, que a menudo es el caso hoy en día, la red se puede actualizar a EDGE mediante la activación de una función de software opcional. La última actualización es esta tecnología es compatible con todos los principales fabricantes de chips para redes GSM y WCDMA/HSPA.



</doc>
<doc id="5045" url="https://es.wikipedia.org/wiki?curid=5045" title="Sistema global para las comunicaciones móviles">
Sistema global para las comunicaciones móviles

El sistema global para las comunicaciones móviles (del inglés "Global System for Mobile communications", GSM, y originariamente del francés "groupe spécial mobile") es un sistema estándar, libre de regalías, de telefonía móvil digital.

Un cliente GSM puede conectarse a través de su teléfono con su computador y enviar y recibir mensajes por correo electrónico, faxes, navegar por Internet, acceder con seguridad a la red informática de una compañía (red local/Intranet), así como utilizar otras funciones digitales de transmisión de datos, incluyendo el servicio de mensajes cortos (SMS) o mensajes de texto.
GSM se considera, por su velocidad de transmisión y otras características, un estándar de segunda generación (2G). Su extensión a 3G se denomina UMTS y difiere en su mayor velocidad de transmisión, el uso de una arquitectura de red ligeramente distinta y sobre todo en el empleo de diferentes protocolos de radio (W-CDMA).

La Asociación GSM (GSMA o "GSM Association)," dice que GSM es el estándar en telecomunicaciones móviles más extendido en el mundo, con un 82% de los terminales mundiales en uso. GSM cuenta con más de 3000 millones de usuarios en 159 países distintos, siendo el estándar predominante en Europa, América del Sur, Asia y Oceanía, y con gran extensión en América del Norte.

La ubicuidad del estándar GSM ha sido una ventaja tanto para consumidores (beneficiados por la capacidad de itinerancia y la facilidad de cambio de operador sin cambiar de terminal, simplemente cambiando la tarjeta SIM) como para los operadores de red (que pueden elegir entre múltiples proveedores de sistemas GSM, al ser un estándar abierto que no necesita pago de licencias).

El GSM se implementó por primera vez el servicio de mensajes cortos de texto (SMS), que posteriormente fue extendido a otros estándares. Además, en GSM se define un único número de emergencias a nivel mundial, el 112, que facilita que los viajeros de cualquier parte del mundo puedan comunicar situaciones de emergencia sin necesidad de conocer un número local.

La interfaz de radio de GSM se ha implementado en diferentes bandas de frecuencia.

El estándar GSM fue desarrollado a partir de 1982. En la conferencia de telecomunicaciones CEPT de ese año fue creado el grupo de trabajo "Groupe Spécial Mobile" o "GSM," cuya tarea era desarrollar un estándar europeo de telefonía móvil digital. Se buscó evitar los problemas de las redes analógicas de telefonía móvil, que habían sido introducidos en Europa a fines de los años 1950, y no fueron del todo compatibles entre sí a pesar de usar, en parte, los mismos estándares. En el grupo GSM participaron 26 compañías europeas de telecomunicaciones.

En 1990 se finalizaron las especificaciones para el primer estándar "GSM-900", al que siguió "DCS-1800" un año más tarde. En 1991 fueron presentados los primeros equipos de telefonía GSM como prototipos. De manera paralela, se cambió el nombre del grupo a "Standard Mobile Group" (SMG) y las siglas GSM a partir de este momento se usaron para el propio estándar.

En 1992 las primeras redes europeas de GSM-900 iniciaron su actividad, y el mismo año fueron introducidos al mercado los primeros teléfonos móviles GSM, siendo el primero el Nokia 1011 en noviembre de este año. En los años siguientes, el GSM compitió con otros estándares digitales, pero se terminó imponiendo también en América Latina y Asia.

En 2000, el grupo de trabajo para la estandarización del GSM se pasó al grupo TSG GERAN "(Technical Specification Group GSM EDGE Radio Access Network)" del programa de cooperación 3GPP, creado para desarrollar la tercera generación de telefonía móvil (3G). El sucesor del GSM, UMTS, fue introducido en 2001, sin embargo su aceptación fue lenta, por lo que gran parte de los usuarios de telefonía móvil en 2010 siguen utilizando GSM.

Al diseñar la estructura de red para un sistema de telefonía móvil, el problema a encarar es el de la limitación en el rango de frecuencias disponibles. Cada "conversación" (o cada cliente de tráfico de datos) requiere un mínimo de ancho de banda para que pueda transmitirse correctamente. A cada operador en el mercado se le asigna cierto ancho de banda, en ciertas frecuencias delimitadas, que debe repartir para el envío y la recepción del tráfico a los distintos usuarios (que, por una parte, reciben la señal del otro extremo, y por otra envían su parte de la “conversación”). Por tanto, no puede emplearse una sola antena para recibir la señal de todos los usuarios a la vez, ya que el ancho de banda no sería suficiente; y además, deben separarse los rangos en que emiten unos y otros usuarios para evitar interferencias entre sus envíos. A este problema, o más bien a su solución, se le suele referir como reparto del espectro o control de acceso al medio.
El sistema GSM basa su división de acceso al canal en combinar los siguientes modelos de reparto del espectro disponible. El primero es determinante a la hora de especificar la arquitectura de red, mientras que el resto se resuelve con circuitería en los terminales y antenas del operador:

La BSS, capa inferior de la arquitectura (terminal de usuario – BS – BSC), resuelve el problema del acceso del terminal al canal. La siguiente capa (NSS) se encargará, por un lado, del enrutamiento (MSC) y por otro de la identificación del abonado, tarificación y control de acceso (HLR, VLR y demás bases de datos del operador). Este párrafo con tantas siglas se explica a continuación con más calma, pero sirve de resumen general de la arquitectura de red empleada.

Por otra parte, las comunicaciones que se establezcan viajarán a través de distintos sistemas. Para simplificar, se denomina canal de comunicaciones a una comunicación establecida entre un sistema y otro, independientemente del método que realmente se emplee para establecer la conexión. En GSM hay definidos una serie de canales lógicos para el tráfico de llamadas, datos, señalización y demás propósitos.

Esta capa de red se ocupa de proporcionar y controlar el acceso de los terminales al espectro disponible, así como del envío y recepción de los datos.

El sistema debe ser capaz de soportar una gran carga de usuarios, con muchos de ellos utilizando la red al mismo tiempo. Si solo hubiera una antena para todos los usuarios, el espacio radioeléctrico disponible se saturaría rápidamente por falta de ancho de banda. Una solución es reutilizar las frecuencias disponibles. En lugar de poner una sola antena para toda una ciudad, se colocan varias, y se programa el sistema de manera que cada antena emplee frecuencias distintas a las de sus vecinas, pero las mismas que otras antenas fuera de su rango. A cada antena se le reserva cierto rango de frecuencias, que se corresponde con un cierto número de canales radioeléctricos (cada uno de los rangos de frecuencia en que envía datos una antena). Así, los canales asignados a cada antena de la red del operador son diferentes a los de las antenas contiguas, pero pueden repetirse entre antenas no contiguas.

Además, se dota a las antenas de la electrónica de red necesaria para comunicarse con un sistema central de control (y la siguiente capa lógica de la red) y para que puedan encargarse de la gestión de la interfaz radio: el conjunto de la antena con su electrónica y su enlace con el resto de la red se llama estación base (BTS," Base Transceiver Station"). El área geográfica a la que proporciona cobertura una estación base se llama celda o célula (del inglés "cell", motivo por el cual el sistema GSM pertenece a la familia de sistemas celulares). 

El empleo de celdas requiere de una capa adicional de red que es novedosa en el estándar GSM respecto a los sistemas anteriores: es el controlador de estaciones base, o BSC, "(Base Station Controller)" que actúa de intermediario entre la red troncal y las estaciones base. Entre otras funciones, se encarga de la asignación de los recursos radio (radiocanal y ranura temporal) a los usuarios, el control de su potencia o la gestión del procedimiento de traspaso. El conjunto de estaciones base coordinadas por un BSC proporcionan el enlace entre el terminal del usuario y la siguiente capa de red, ya la principal, que veremos más adelante. Como capa de red, el conjunto de BSs + BSC se denomina subsistema de estaciones base, o BSS "(Base Station subsystem)."

Una estación base GSM puede alcanzar un radio de cobertura a su alrededor desde varios cientos de metros ("en estaciones urbanas") hasta un máximo práctico de 35 km (en zonas rurales), según su potencia y la geografía del entorno. Sin embargo, el número de usuarios que puede atender cada BS está limitado por el ancho de banda (subdividido en canales) que el BSC asigna a cada estación, y aunque podría pensarse que las estaciones base deberían tener una gran potencia para cubrir mayor área, tienen una potencia nominal de 320 W como máximo (frente a las antenas de FM o televisión, que poseen potencias de emisión de miles de Watts, un valor casi despreciable) y de hecho siempre emiten al menor nivel de potencia posible para evitar interferir con celdas lejanas que pudieran emplear el mismo rango de frecuencias, motivo por el cual es raro que se instalen modelos de más de 40 W. Es más, en zonas urbanas muy pobladas o túneles se instala un mayor número de BSs de potencia muy limitada (menor que 2,5 W) para permitir la creación de las llamadas pico y microceldas, que permiten mejor reutilización de las frecuencias (cuantas más estaciones, más reutilización de frecuencias y más usuarios admisibles al mismo tiempo) o bien dan cobertura en lugares que una BS normal no alcanza o precisan de gran capacidad (túneles de metro o de carreteras, espacios muy concurridos, ciudades muy pobladas).

Por tanto, en zonas donde exista una gran concentración de usuarios, como ciudades, debe instalarse un gran número de BSs de potencia muy limitada, y en zonas de menor densidad de uso, como áreas rurales, puede reducirse el número de estaciones y ampliar su potencia. Esto asegura además mayor duración de la batería de los terminales y menor uso de potencia de las estaciones base.

Además, el terminal no se encuentra emitiendo durante el transcurso de toda la llamada. Para ahorrar batería y permitir un uso más eficiente del espectro, se emplea el esquema de transmisión TDMA "(Time Division Multiple Access," o acceso múltiple por división del tiempo). El tiempo se divide en unidades básicas de 4,615 ms, y éstas a su vez en 8 "time slots" o ranuras de tiempo de 576,9 μs. Durante una llamada, se reserva el primer "time slot" para sincronización, enviada por la BS; unos "slots" más tarde, el terminal emplea un "slot" para enviar de terminal a BS y otro para recibir, y el resto quedan libres para el uso de otros usuarios en la misma BS y canal. Así se permite un buen aprovechamiento del espectro disponible y una duración de batería superior, al no usar el emisor del terminal constantemente sino solo una fracción del tiempo.

Al mismo tiempo, la comunicación no debe interrumpirse porque un usuario se desplace ("roaming", deambular) y salga de la zona de cobertura de una BS, deliberadamente limitada para que funcione bien el sistema de celdas. Tanto el terminal del usuario como la BS calibran los niveles de potencia con que envían y reciben las señales e informan de ello al controlador de estaciones base o BSC "(Base Station Controller)." Además, normalmente varias estaciones base al mismo tiempo pueden recibir la señal de un terminal y medir su potencia. De este modo, el controlador de estaciones base o BSC puede detectar si el usuario va a salir de una celda y entrar en otra, y avisa a ambas MSCs ("Mobile Switching Center", Central de Conmutación Móvil) y al terminal para el proceso de salto de una BS a otra: es el proceso conocido como handover o traspaso entre celdas, una de las tres labores del BSC, que permite hablar aunque el usuario se desplace.

Este proceso también puede darse si la estación más cercana al usuario se encuentra saturada –es decir, si todos los canales asignados a la BS están en uso–. En ese caso el BSC remite al terminal a otra estación contigua, menos saturada, incluso aunque el terminal tenga que emitir con más potencia. Por eso es habitual percibir cortes de la comunicación en zonas donde hay muchos usuarios al mismo tiempo. Esto nos indica la segunda y tercera labor del BSC, que son controlar la potencia y la frecuencia a la que emiten tanto los terminales como las BTSs para evitar cortes con el menor gasto de batería posible.

Además del uso para llamadas del espectro, reservando para ello los canales precisos mientras se estén usando, el estándar prevé que el terminal envíe y reciba datos para una serie de usos de señalización, como por ejemplo el registro inicial en la red al encender el terminal, la salida de la red al apagarlo, el canal en que va a establecerse la comunicación si entra o sale una llamada, la información del número de la llamada entrante... Y prevé además que cada cierto tiempo el terminal avise a la red de que se encuentra encendido para optimizar el uso del espectro y no reservar capacidad para terminales apagados o fuera de cobertura.

Este uso del transmisor, conocido como ráfagas de señalización, ocupa muy poca capacidad de red y se utiliza también para enviar y recibir los mensajes cortos SMS sin necesidad de asignar un canal de radio. Es sencillo escuchar una ráfaga de señalización si el teléfono se encuentra cerca de un aparato susceptible de captar interferencias, como un aparato de radio o televisión.

En GSM se definen una serie de canales para establecer la comunicación, que agrupan la información a transmitir entre la estación base y el teléfono. Se definen los siguientes tipos de canal:


El subsistema de red y conmutación ("network and switching system" o NSS), también llamado núcleo de red ("core network"), es la capa lógica de enrutamiento de llamadas y almacenamiento de datos. Notemos que, hasta el momento, solo teníamos una conexión entre el terminal, las estaciones base BS y su controlador BSC, y no se indicaba manera de establecer conexión entre terminales o entre usuarios de otras redes. Cada BSC se conecta al NSS, y es este quien se encarga de tres asuntos:


La central de conmutación móvil o MSC ("mobile switching central") se encarga de iniciar, terminar y canalizar las llamadas a través del BSC y BS correspondientes al abonado llamado. Es similar a una centralita telefónica de red fija, aunque como los usuarios pueden moverse dentro de la red realiza más actualizaciones en su base de datos interna.

Cada MSC está conectado a los BSCs de su área de influencia, pero también a su VLR, y debe tener acceso a los HLRs de los distintos operadores e interconexión con las redes de telefonía de otros operadores.

El HLR ("home location register", o registro de ubicación base) es una base de datos que almacena la posición del usuario dentro de la red, si está conectado o no y las características de su abono (servicios que puede y no puede usar, tipo de terminal, etcétera). Es de carácter más bien permanente; cada número de teléfono móvil está adscrito a un HLR determinado y único, que administra su operador móvil.

Al recibir una llamada, el MSC pregunta al HLR correspondiente al número llamado si está disponible y dónde está (es decir, a qué BSC hay que pedir que le avise) y enruta la llamada o da un mensaje de error.

El VLR ("visitor location register" o registro de ubicación de visitante) es una base de datos más volátil que almacena, para el área cubierta por un MSC, los identificativos, permisos, tipos de abono y localizaciones en la red de todos los usuarios activos en ese momento y en ese tramo de la red. Cuando un usuario se registra en la red, el VLR del tramo al que está conectado el usuario se pone en contacto con el HLR de origen del usuario y verifica si puede o no hacer llamadas según su tipo de abono. Esta información permanece almacenada en el VLR mientras el terminal de usuario está encendido y se refresca periódicamente para evitar fraudes (por ejemplo, si un usuario de prepago se queda sin saldo y su VLR no lo sabe, podría permitirle realizar llamadas).

Tengamos en cuenta que el sistema GSM permite acuerdos entre operadores para compartir la red, de modo que un usuario en el extranjero –por ejemplo— puede conectarse a una red (MSC, VLR y capa de radio) de otro operador. Al encender el teléfono y realizar el registro en la red extranjera, el VLR del operador extranjero toma nota de la información del usuario, se pone en contacto con el HLR del operador móvil de origen del usuario y le pide información sobre las características de abono para permitirle o no realizar llamadas. Así, los distintos VLRs y HLRs de los diferentes operadores deben estar interconectados entre sí para que todo funcione. Para este fin existen protocolos de red especiales, como SS7 o IS-41; los operadores deciden qué estándar escoger en sus acuerdos bilaterales de "roaming" (itinerancia) e interconexión.

Además, los MSC están conectados a otros sistemas que realizan diversas funciones.

Por ejemplo, el AUC (authentication user center, centro de autentificación del usuario) se encarga del cifrado de las señales y de la identificación de usuarios dentro del sistema; el EIR (equipment identification register, registro de identificación de equipo) guarda listas de permiso de acceso al terminal, al que identifica unívocamente mediante su número de serie o IMEI, para evitar que los terminales robados y denunciados puedan usar la red; los SMSCs o centros de mensajes cortos; y así varios sistemas más, entre los que se incluyen los de gestión, mantenimiento, prueba, tarificación y el conjunto de transcodificadores necesarios para poder transferir las llamadas entre los diferentes tipos de red (fija y diferentes estándares de móvil).

Todos los códigos señalados deben ejecutarse pulsando LLAMAR, CALL, SEND o su equivalente para llamar a un número.

Activación de envío u ocultación del número al realizar o recibir una llamada.
Estos códigos dependen de la habilitación del servicio por parte de la proveedora del mismo. En algunos países, como Argentina, las empresas Personal, Claro, Perú (Movistar) y Venezuela (Digitel) ignoran los códigos y la activación/desactivación del servicio debe ser realizada desde el menú de cada teléfono.

Al realizar una llamada:

Al recibir

Temporal (solo para una llamada)




Para desactivar los desvíos, se puede utilizar el mismo código principal, como por ejemplo #67# para desactivar el desvío si ocupado. Para desactivar todos los desvíos sin importar el que esté activo, podemos usar ##002#.


Los códigos específicos de restricción de red como voz y SMS pueden consultarse usando el mismo código de estado y pueden desactivarse de la misma manera. La palabra PASS debe ser reemplazada por un código de 4 dígitos (llamada contraseña de restricción), el cual está en poder del operador de telefonía, aunque puede usarse como código de red 0000.

Una de las características principales del estándar GSM es el módulo de identidad del suscriptor, conocida comúnmente como tarjeta SIM. La tarjeta SIM es una tarjeta inteligente desmontable que contiene la información de suscripción del usuario, parámetros de red y directorio telefónico. Esto permite al usuario mantener su información después de cambiar su teléfono. Paralelamente, el usuario también puede cambiar de operador de telefonía, manteniendo el mismo equipo simplemente cambiando la tarjeta SIM. Algunos operadores introducen un bloqueo para que el teléfono utilice un solo tipo de tarjeta SIM, o solo una tarjeta SIM emitida por la compañía donde se compró el teléfono, esta práctica se conoce como bloqueo de sim, y es ilegal en algunos países.

En Australia, América del Norte y Europa, muchos operadores móviles bloquean los terminales que venden. Esto se hace porque el precio de la telefonía móvil es típicamente subvencionado con los ingresos procedentes de suscripciones, y los operadores para tratar de evitar subvencionar los móviles de la competencia pueden recurrir a esta práctica. Los abonados pueden ponerse en contacto con el operador, para eliminar el bloqueo o bien utilizar servicios privados para retirar el mismo, o hacer uso de software y sitios web para desbloquear el teléfono por sí mismos. Si bien la mayoría de los sitios web ofrecen el desbloqueo a un costo fijo, algunos lo hacen de manera gratuita. El bloqueo se aplica al teléfono, identificado por su identidad internacional del equipo móvil (IMEI) número y no a la cuenta (que se identifica con la tarjeta SIM).

En algunos países como Bangladesh, Bélgica, Chile, Costa Rica, Cuba, Indonesia, Malasia, Hong Kong y Pakistán, se venden los teléfonos desbloqueados. Sin embargo, en Bélgica, es ilegal que los operadores ofrezcan cualquier forma de subvención en el precio del teléfono. Este fue también el caso en Finlandia hasta el 1 de abril de 2006, cuando la venta de combinaciones de teléfonos subvencionados y los números se convirtió en legal, aunque los operadores por obligación tienen que desbloquear los teléfonos de forma gratuita después de un período determinado (pudiendo ser un máximo de 24 meses). Parecido es el caso de España, en el que los operadores también están obligados a la liberación a petición una vez finalizado el contrato, si bien, dicha operadora podría repercutir el gasto de la operación sobre el cliente.

La tecnología móvil en España comenzó en 1976 con un servicio para vehículos limitado a Madrid y Barcelona llamado Teléfono automático en vehículo. Este servicio fue evolucionando para dar cabida a más usuarios con tecnologías como TMA-450 y posteriormente TMA-900, llegando hasta 900.000 en 1996.

En 1995 dada la inferioridad tecnológica del servicio analógico respecto al digital proporcionado por GSM, se creó la primera red digital móvil llamada Movistar. Posteriormente, se concedieron licencias para una segunda operadora móvil llamada Airtel (actualmente Vodafone). En 1999 se crea una tercera operadora llamada Amena (actualmente Orange).

A esta última se le asignaron frecuencias únicamente en la banda de 1800 MHz lo que suponía tener que desplegar más celdas que si se emplease la banda de 900 MHz para conseguir dar cobertura a una misma zona. Ya en 2005, el gobierno asignó a Amena nuevas frecuencias en la banda de 900 MHz, pero Movistar y Vodafone siguieron contando con un mayor número de frecuencias en esta banda.

A principios del año 2000, empezaron los cierres de las redes analógicas y la asignación de licencias para la tecnología 3G, a la que años más tarde seguiría la tecnología 3,5G. Ese mismo año se concede licencia a la cuarta operadora llamada Xfera (actualmente Yoigo), aunque no empezaría a operar hasta 2006.

Actualmente convivimos con tecnología 2G/3G/3,5G y, aunque 3,5G sea superior tecnológicamente, compañías como Vodafone utilizan red dual para ofrecer una mayor cobertura (si no hay cobertura 2G o 3G, el terminal móvil puede que tenga cobertura 3,5G y viceversa) y maximizar la duración de la batería de sus móviles.

Según los datos ofrecidos por la Comisión del Mercado de las Telecomunicaciones de España correspondientes al año 2009 se puede apreciar que el número de estaciones base GSM es considerablemente mayor que el de estaciones 3G/UMTS.

Tras mucho revuelo durante el año 2013, las 4 grandes compañías Españolas (Vodafone, Orange, Movistar y Yoigo) han implementado los primeros 4G entre los grandes núcleos urbanos.

En mayo de 2013 se inició una "guerra" entre Yoigo y Orange para ver quién lanza más rápido la línea 4G, Movistar estaba a un lado prometiendo 4G a finales de año (sin cambios) y entre Vodafone y Yoigo para mediados de verano. La gran sorpresa la hizo Vodafone, quién sin decir nada, a la primera semana de mayo dijo que implementaría el servicio 4G para 7 grandes urbes en junio de este mismo año, siendo así la primera operadora en ofrecer 4G y una velocidad móvil de 150Mbps en España. Las urbes que disfrutaron de 4G a partir de junio fueron: Madrid, Barcelona, Valencia, Bilbao, Sevilla, Málaga y Palma de Mallorca. La adaptación a nuevas líneas 4G en éstas 7 ciudades costó una inversión de 12.000 millones de euros por parte de Vodafone, Orange y Yoigo. Orange lo lanzó el 8 de julio de 2013 y Yoigo el 18 de julio de 2013, mientras que Vodafone estuvo disponible desde el 3 de junio de 2013. Finalmente, Movistar anunció en octubre de 2013 la disponibilidad del 4G a través de la red de Yoigo hasta que se liberase por parte del Gobierno la frecuencia de los 800 MHz

De acuerdo con las cifras suministradas por la organización “3G Américas”, en Colombia el 89 por ciento de los teléfonos móviles operan bajo el estándar GSM, mientras que en Argentina esta cifra llega al 97 por ciento (al 2008 los operadores Movistar, Personal, y Claro solo operan con GSM), en Chile (primer país de Latinoamérica en operar redes GSM ya desde 1997) el 100% de los móviles operan bajo GSM y la primera compañía de telefonía móvil chilena que implementó y debutó bajo la tecnología GSM fue ENTEL PCS, en México al 80 por ciento, en Brasil al 65 por ciento, en Paraguay y en Uruguay al 100 por ciento y en Venezuela Digitel al 100% puesto que fue el operador que empezó con esta tecnología, Movistar está en fase de ampliar al 100% su red GSM, y Movilnet opera en dualidad CDMA/GSM, países como Cuba que comenzó por TDMA, a partir de enero de 2009 emplea exclusivamente la tecnología GSM a través de la empresa estatal Cubacel.

En Colombia la Comisión Reguladora de Comunicaciones (CRC), señaló que a partir del 1° de octubre las empresas de telefonía móvil están obligadas a entregar los celulares con las bandas abiertas (desbloqueados) para que éstos puedan funcionar con cualquier operador. Con esta medida el Gobierno busca promover la competencia en el mercado de telefonía celular, en la cual el ganador será el usuario final y evitar el hurto y tráfico ilegal de móviles no solo en Colombia, si no a nivel Latinoamérica, según diálogos entre los distintos gobiernos.

En Chile, se utilizan dos modalidades de proveer los terminales; Venta (principalmente para los abonados de prepago, aunque hay clientes de postpago que prefieren comprar el terminal) y Arriendo con opción de compra (modalidad muy difundida en la modalidad postpago, ya que el terminal resulta a un precio más económico); Por ley, todos los equipos, tanto prepago como postpago se encuentran desbloqueados para todas las operadoras. Los equipos antiguos, que era entregados bloqueados por las operadoras pueden ser desbloqueados gratuitamente.





</doc>
<doc id="5046" url="https://es.wikipedia.org/wiki?curid=5046" title="Digital">
Digital

Digital puede designar:


</doc>
<doc id="5048" url="https://es.wikipedia.org/wiki?curid=5048" title="Satmódem">
Satmódem

Un satmódem (por "satellite modem", en inglés, abreviadamente SM) es un módem que se emplea para establecer una transmisión de datos con un satélite de comunicaciones.

El satélite ofrece la enorme ventaja frente a las conexiones terrestres de banda ancha, que suele tener bastante más cobertura (ej. en toda la Unión Europea) y que es más fácil el cambiar de emplazamiento con el mismo equipo y con el mismo abono.

Existen dos tipos diferentes de módems satelitales:


La integridad de este puede dañarse de por vida si se densistala constantemente es decir cortar la señal apagando el mismo

Un satmódem no es el único dispositivo necesario para establecer un canal de comunicación. Otro de los equipos esenciales para la creación de un enlace por satélite son antenas de satélite y convertidores de frecuencia. 

Los datos a transmitir se transfieren a un módem de equipo terminal de datos (por ejemplo, un ordenador). El módem en general tiene Frecuencia intermedia de salida (IF), es decir, 50 a 200 MHz, pero, a veces la señal se modula directamente a L-banda. En la mayoría de casos la frecuencia se convierte usando un "upconverter" antes de la amplificación y la transmisión.

Una señal modulada es una secuencia de símbolos, piezas de datos representadas por un estado de señal correspondiente, por ejemplo, se utilizan un bit o unos pocos bits, dependiendo del esquema de modulación. 

De la misma manera, una señal recibida desde un satélite es convertida en primer lugar (esto se hace mediante un convertidor de bajo ruido bloquear LNB), entonces es demodulada por un módem, y finalmente es gestionada por unos equipos terminales de datos. El LNB es accionado generalmente por el módem a través del cable de señal con 13 o 18 V DC.

Las principales funciones de un módem de satélite son las de modulación y desmodulación. Los estándares de comunicación por satélite también definen los códigos de corrección de errores y formatos de encuadernación. 

Tipos de modulación populares que se usan para las comunicaciones por satélite:

- Modulación por desplazamiento de fase binaria (BPSK)

- Cuadratura para desplazamiento de fase (QPSK)

- Modulación por desplazamiento de fase en cuadratura ortogonal (OQPSK)

- 8PSK

- Modulación de amplitud en cuadratura (QAM), especialmente 16QAM

Los códigos de corrección de errores por satélite populares incluyen:

Los códigos convolucionales: 

- Con longitud de restricción de menos de 10, generalmente descodificada utilizando un algoritmo de Viterbi (mirar descodificador Viterbi)

- Con longitud de restricción más de 10, en general descodificada usando un algoritmo de Fano (mirar descodificador secuencial) 

Códigos Reed-Solomon generalmente contactan con los códigos convolucionales con una intercalación.

Los nuevos módems compatibles con los códigos de corrección de errores (superiores códigos turbo y los códigos LDPC)

Formatos de trama que son compatibles con varios módems satelitales incluyen:

- Servicio al Intelsat (SII) encuadernación 

- Velocidad de datos intermedia (IDR) encuadernación 

- MPEG-2 framing transporte (usada en DVB)

- E1 y T1 framing

Módems de alta gama también incorporan algunas características adicionales:

- Múltiples interfaces de datos (como RS-232, RS-422. V.35, G.703, LVDS, Ethernet)

- Distante de extremo incrustado monitor y Control (Edmac), que permite controlar el módem de extremo distante

- Automatic Power Control de enlace ascendiente (AUPC), es decir, el ajuste de la potencia de salida para mantener una señal constante a sonido al extremo distante

- Caer y función de inserir un flujo multipixelado, permite remplazar algunos canales en el mismo.

Los principales fabricantes y modelos de terminales DVB-RCS son:


SatLabs es una organización internacional sin ánimo de lucro, para la adopción del estándar DVB-RCS/2 a larga escala. El objetivo principal de SatLabs es asegurar la ineroperabilidad entre terminales (módems satelitales) y sistemas DVB-RCS y obtener soluciones de bajo coste.

El Programa de Cualificación de SatLabs (SatLabs Qualification Program) tiene por objeto proporcionar un proceso de certificación independiente. Cuando un terminal ha pasado con éxito la prueba definida en el "SatLabs Qualification Program", se otorga un Certificado de Conformidad ("Certificate of Compliance") y el terminal queda definido como un Producto Cualificado ("Qualified Product").

DVB-RCS ha ganado atractivo en Mercados como Rusia, India y China, donde las agencias gubernamentales están mandando estándares abiertos. India incluso ha mandado DVB-RCS específicamente para su red de educación a distancia Edusat, así como para su previsto sistema de telemedicina. Sudamérica también se está dirigiendo decididamenteen la dirección de DVB-RCS.




</doc>
<doc id="5049" url="https://es.wikipedia.org/wiki?curid=5049" title="Digital Video Broadcasting">
Digital Video Broadcasting

Digital Video Broadcasting (DVB) es una organización que promueve estándares de televisión digital, aceptados internacionalmente, en especial para HDTV y televisión vía satélite, así como para comunicaciones de datos vía satélite (unidireccionales, denominado DVB-IP, y bidireccionales, llamados DVB-RCS).

El acceso unidireccional, no es de banda ancha, ya que se realiza combinando el acceso a Internet tradicional, vía RTB/RDSI, más el módem univía de acceso satelital DVB.
Se creó en el año 1993 fruto de la alianza entre varias empresas privadas europeas que buscaban normalizar los trabajos que se habían hecho en la televisión digital. Así se crearon nuevos estándares en un sector como el de la radiodifusión digital que hasta entonces era poco práctico y costoso.

De esta manera se comenzó un proceso en el que rápidamente se añadieron compañías de todo el mundo para acabar creando y desarrollando conjuntamente los diferentes estándares " 'DVB' ". El primer sistema acordado fue el DVB-S (transmisión vía satélite) en el año 1994, el cual fue utilizado por un operador francés poco después. El mismo año apareció el DVB-C (transmisión por cable), mientras que el sistema DVB-T (transmisión terrestre) fue posterior surgiendo en 1997. Las primeras difusiones en terrestre fueron en 1998 en Suecia y el Reino Unido.

De esta manera los estándares abiertos del DVB se convirtieron en un referente, al ser utilizados por casi todo el mundo relacionado con el sector. Actualmente la organización está formada por unas 270 organizaciones y empresas de 30 países.

DVB es un organismo encargado de crear y proponer los procedimientos de estandarización para la televisión digital compatible. Está constituido por más de 270 instituciones y empresas de todo el mundo. Los estándares propuestos han sido ampliamente aceptados en Europa y casi todos los continentes, con la excepción de Estados Unidos, Canadá y Japón donde coexisten con otros sistemas propietarios. Todos los procedimientos de codificación de las fuentes de vídeo y audio están basados en los estándares definidos por MPEG. No obstante, hemos visto que los estándares MPEG sólo cubren los aspectos y metodologías utilizados en la compresión de las señales de audio y vídeo y los procedimientos de multiplexación y sincronización de estas señales en tramas de programa o de transporte. Una vez definida la trama de transporte es necesario definir los sistemas de modulación de señal que se utilizarán para los distintos tipos de radiodifusión (satélital, cableada y terrestre), los tipos de códigos de protección frente a errores y los mecanismos de acceso condicional a los servicios y programas.
El DVB ha elaborado distintos estándares en función de las características del sistema de radiodifusión. Los estándares más ampliamente utilizados en la actualidad son el DVB-S y el DVB-C que contemplan las transmisiones de señales de televisión digital mediante redes de distribución satelital y por videocable respectivamente. La transmisión de televisión digital a través de redes de distribución terrestres utilizando los canales UHF convencionales se contempla en el estándar DVB-T, que actualmente se está implantando en la mayor parte de los países europeos.
Además de estos estándares también están especificados sistemas para la distribución de señales de televisión digital en redes multipunto, sistemas "Satellite Master Antenna Televisión" (SMATV).
También existen estándares que definen las características de la señalización en el canal de retorno en sistemas de televisión interactiva, la estructura de transmisión de datos para el cifrado y descifrado de programas de acceso condicional, la transmisión de subtítulos, y la radiodifusión de datos (nuevos canales de teletexto) mediante sistemas digitales.

Los sistemas DVB distribuyen los datos por:

Estos estándares definen la capa física y la capa de enlace de datos de un sistema de distribución. Los dispositivos interactúan con la capa física a través de una interfaz paralela síncrona (SPI), una interfaz serie síncrona (SSI) o una interfaz serie asíncrona (ASI). Todos los datos se transmiten en flujos de transporte MPEG-2 con algunas restricciones adicionales (DVB-MPEG). Se está experimentando en varios países un estándar para distribución comprimida en el tiempo (DVB-H) para distribución a dispositivos móviles.

Estos estándares se diferencian principalmente en los tipos de modulación utilizados, debido a las diferentes restricciones técnicas:

Además de la transmisión de audio y vídeo, DVB también define conexiones de datos (DVB-DATA - EN 301 192) con canales de retorno (DVB-RC) para diferentes medios (DECT, GSM, RTB/RDSI, satélite, etc.) y protocolos (DVB-IPTV: protocolo de Internet; DVB-NPI: protocolo de red independiente).

Para facilitar la conversión, estos estándares también soportan las tecnologías existentes tales como el teletexto (DVB-TXT) y el sincronismo vertical (DVB-VBI). Sin embargo, para muchas aplicaciones hay disponibles alternativas más avanzadas como, por ejemplo, DVB-SUB para los subtítulos.

Si bien DVB es el estándar más universal para la transmisión y recepción de televisión digital, también están disponibles en el mercado internacional los estándares: ATSC de origen estadounidense, y el ISDB de origen japonés, principalmente en el formato de Televisión Digital Terrestre.



</doc>
<doc id="5051" url="https://es.wikipedia.org/wiki?curid=5051" title="Valentín de Roma">
Valentín de Roma

Valentín de Roma es uno de los tres santos mártires de existencia discutida, que vivieron en la antigua Roma. La festividad de san Valentín era celebrada por la Iglesia católica cada 14 de febrero en el calendario litúrgico tradicional, hasta que en 1960, en el Concilio Vaticano II, se reorganizó el santoral y se retiró su celebración. No obstante, universalmente se sigue considerando el 14 de febrero como memorial de San Valentín.

La fiesta de San Valentín fue declarada por primera vez alrededor del año 498 por el papa Gelasio I. Según la "Enciclopedia Católica", el santo cuya festividad cayó en la fecha conocida hoy como día de San Valentín fue posiblemente uno de los tres mártires ejecutados en tiempos del Imperio Romano, los dos primeros en la segunda mitad del siglo III, durante el reinado del emperador Claudio II “el Gótico”:


Se cree que el sacerdote y el obispo Valentín están enterrados en la "Via Flaminia" en las afueras de Roma. En el siglo XII, la puerta de la ciudad conocida en tiempos antiguos como la "Porta Flaminia" (ahora conocida como "Porta del Popolo") era conocida como la "Puerta de San Valentín". Un cráneo atribuido a San Valentín Mártir se conserva dentro de una urna de cristal, a la vista de los fieles, en la Basílica de Santa Maria in Cosmedin en Roma.
Sin embargo, poco se sabe sobre las vidas de estos tres hombres.

Muchas de las leyendas que los rodean actualmente muy probablemente se inventaron durante la Edad Media en Francia e Inglaterra, cuando el día festivo 14 de febrero empezó asociarse con el amor, a raíz de la historia de San Valentín, quien habría sido ejecutado un 14 de febrero al no querer renunciar al cristianismo y haber casado a soldados en secreto después de que el matrimonio de soldados profesionales fuera prohibido por el emperador Claudio II. Otra leyenda dice que es patrono de los enamorados porque su fiesta coincide con el momento del año en que los pájaros empiezan a emparejarse.

La festividad se borró del calendario eclesiástico por la Iglesia católica en 1969, como parte de un intento por eliminar santos de un origen posiblemente legendario, aunque siguen celebrándola algunas parroquias locales. También es venerado como santo por la Iglesia ortodoxa y por la Iglesia anglicana, así como por la iglesia luterana.

En el 2014, el papa Francisco decidió participar en la celebración de san Valentín, en un intento por devolverle el sentido religioso a esta festividad surgida en principio para contrarrestar a las lupercales, consideradas paganas por la Iglesia católica. Un relato muy popular sobre este santo cuenta que le devolvió la vista a una jovencita ciega y esta en agradecimiento sembró sobre la tumba del santo un rosal que según la tradición florecía cada 14 de febrero.




</doc>
<doc id="5052" url="https://es.wikipedia.org/wiki?curid=5052" title="Bluetooth">
Bluetooth

Bluetooth es una especificación industrial para redes inalámbricas de área personal (WPAN) creado por Bluetooth Special Interest Group, Inc. que posibilita la transmisión de voz y datos entre diferentes dispositivos mediante un enlace por radiofrecuencia en la banda ISM de los 2.4 GHz. Los principales objetivos que se pretenden conseguir con esta norma son:

Los dispositivos que con mayor frecuencia utilizan esta tecnología pertenecen a sectores de las telecomunicaciones y la informática personal, como teléfonos móviles, computadoras portátiles, computadoras personales, impresoras, altavoces inalámbricos, auriculares inalámbricos o semi inalámbricos o cámaras digitales.

El nombre procede del rey danés y noruego Harald Blåtand, cuya traducción al inglés es Harald Bluetooth. El rey es conocido por unificar las tribus danesas y convertirlas al cristianismo. Este nombre fue propuesto por Jim Kardach, quien desarrolló un sistema que permitiría a los teléfonos móviles comunicarse con los ordenadores y unificar la comunicación inalámbrica.

El logo de Bluetooth combina las runas Hagall () y Berkana (), que corresponden a las iniciales de Harald Blåtand.


Se denomina Bluetooth al protocolo de comunicaciones diseñado especialmente para dispositivos de bajo consumo, que requieren corto alcance de emisión y basados en transceptores de bajo coste.

Los dispositivos que incorporan este protocolo pueden comunicarse entre sí cuando se encuentran dentro de su alcance. Las comunicaciones se realizan por radiofrecuencia de forma que los dispositivos no tienen que estar alineados y pueden incluso estar en habitaciones separadas si la potencia de transmisión es suficiente. Estos dispositivos se clasifican como "Clase 1", "Clase 2", "Clase 3" o "Clase 4" en referencia a su potencia de transmisión, siendo totalmente compatibles los dispositivos de una caja de ordenador.

En la mayoría de los casos, la cobertura efectiva de un dispositivo de clase 2 se extiende cuando se conecta a un transceptor de clase 1. Esto es así gracias a la mayor sensibilidad y potencia de transmisión del dispositivo de clase 1, es decir, la mayor potencia de transmisión del dispositivo de clase 1 permite que la señal llegue con energía suficiente hasta el de clase 2. Por otra parte la mayor sensibilidad del dispositivo de clase 1 permite recibir la señal del otro pese a ser más débil.

Los dispositivos con Bluetooth también pueden clasificarse según su capacidad de canal:

Para utilizar Bluetooth, un dispositivo debe implementar alguno de los perfiles Bluetooth. Estos definen el uso del canal Bluetooth, así como canalizar al dispositivo que se quiere vincular.


La utilidad Bluetooth fue desarrollada, como reemplazo de cable, en 1994 por Jaap Haartsen y Mattisson Sven, que estaban trabajando para Ericsson en Lund, Suecia.
La utilidad se basa en la tecnología de saltos de frecuencia de amplio espectro. En sus inicios, la tecnología Bluetooth podía transmitir datos a una velocidad de 720 kbs, una capacidad increíble para la década de los noventa pero que hoy parece muy limitada. Después de más de dos décadas de mejoras, los diferentes tipos de Bluetooth han llegado a contar con velocidades de hasta 50Mbs. Además, el rango de conexión es otro de los aspectos que ha mejorado mucho. Bluetooth ha pasado de funcionar en distancias menores a un metro a los más de 100 metros que pueden alcanzar hoy en día.

Las prestaciones fueron publicadas por el Bluetooth Special Interest Group (SIG). El SIG las anunció formalmente el 20 de mayo de 1998. Fue creado por Ericsson, IBM, Intel, Toshiba y Nokia, y posteriormente se sumaron muchas otras compañías. Hoy cuenta con una membresía de más de 20.000 empresas en todo el mundo. Todas las versiones de los estándares de Bluetooth están diseñadas para la retro compatibilidad, que permite que el último estándar cubra todas las versiones anteriores.

Las versiones 1.0 y 1.kb han tenido muchos problemas, y los fabricantes tenían dificultades para hacer sus productos interoperables. Las versiones 1.0 y 1.0k incluyen de forma obligatoria en el hardware la dirección del dispositivo Bluetooth (BD_ADDR) en la transmisión (el anonimato se hace imposible a nivel de protocolo), lo que fue un gran revés para algunos servicios previstos para su uso en entornos Bluetooth.


Esta versión es compatible con USB 1.1 y las principales mejoras son las siguientes:

Esta versión de la especificación Core Bluetooth fue lanzada en 2004 y es compatible con la versión anterior 1.2. La principal diferencia está en la introducción de una tasa de datos mejorada (EDR: Enhanced Data Rate, en inglés) para acelerar la transferencia de datos. La tasa nominal de EDR es de 3 Mbit/s, aunque la tasa de transferencia de datos práctica sea de 2,1 Mbit/s. EDR utiliza una combinación de modulación por desplazamiento de frecuencia gausiana o GFSK (en inglés Gaussian Frequency Shift Keying) y modulación por desplazamiento de fase o PSK (en inglés Phase Shift Keying) con dos variantes, π/4-DQPSK y 8DPSK. EDR puede proporcionar un menor consumo de energía a través de un ciclo de trabajo reducido.

La especificación se publica como "Bluetooth v2.0 + EDR", lo que implica que EDR es una característica opcional. Aparte de EDR, hay otras pequeñas mejoras en la especificación 2.0, y los productos pueden anunciar el cumplimiento de Bluetooth v2.0 sin incluir la mayor tasa de datos. Al menos un dispositivo comercial se anuncia "sin EDR Bluetooth v2.0" en su ficha técnica.

La versión 2.1 de la especificación Bluetooth Core + EDR es totalmente compatible con 1.2, y fue adoptada por el Bluetooth SIG ("Bluetooth Special Interest Group") el 26 de julio de 2007.

La función de titular de la 2.1 es Secure Simple Pairing (SSP): se mejora la experiencia de emparejamiento de dispositivos Bluetooth, mientras que aumenta el uso y la fuerza de seguridad. Para más detalles, véase la sección de enlace de abajo.

2.1 permite a otras mejoras, incluida la "respuesta amplia investigación" (EIR), que proporciona más información durante el procedimiento de investigación para permitir un mejor filtrado de los dispositivos antes de la conexión, y oler subrating, lo que reduce el consumo de energía en modo de bajo consumo.

La versión 3.0 + HS de la especificación Core Bluetooth fue aprobada por el Bluetooth SIG el 21 de abril de 2009. El bluetooth 3.0+HS soporta velocidades teóricas de transferencia de datos de hasta 24 Mbit/s entre sí, aunque no a través del enlace Bluetooth propiamente dicho. La conexión Bluetooth nativa se utiliza para la negociación y el establecimiento mientras que el tráfico de datos de alta velocidad se realiza mediante un enlace 802.11.

Su principal novedad es AMP (Alternate MAC/PHY), la adición de 802.11 como transporte de alta velocidad. Inicialmente, estaban previstas dos tecnologías para incorporar en AMP:. 802.11 y UWB, pero finalmente UWB no se encuentra en la especificación.

En la especificación, la incorporación de la transmisión a alta velocidad no es obligatoria y por lo tanto, los dispositivos marcados con "+ HS" incorporan el enlace 802.11 de alta velocidad de transferencia de datos. Un dispositivo Bluetooth 3.0, sin el sufijo "+ HS" no soporta alta velocidad, sino que solo admite una característica introducida en Bluetooth 3.0 + HS (o en CSA1).

Permite el uso de alternativas MAC y PHY para el transporte de datos de perfil Bluetooth. La radio Bluetooth está siendo utilizada para la detección de dispositivos, la conexión inicial y configuración del perfil, sin embargo, cuando deben enviarse grandes cantidades de datos, se utiliza PHY MAC 802.11 (por lo general asociados con Wi-Fi) para transportar los datos. Esto significa que el modo de baja energía de la conexión Bluetooth se utiliza cuando el sistema está inactivo, y la radio 802.11 cuando se necesitan enviar grandes cantidades de datos.
Datos de los permisos de servicio para ser enviado sin establecer un canal L2CAP explícito. Está diseñado para su uso en aplicaciones que requieren baja latencia entre la acción del usuario y la reconexión/transmisión de datos. Esto solo es adecuado para pequeñas cantidades de datos.
Control de energía mejorada.

Actualización de la función de control de potencia para eliminar el control de lazo abierto de energía y también para aclarar las ambigüedades en el control de energía presentado por los esquemas de modulación nuevo añadido para EDR. Control de potencia mejorada elimina las ambigüedades mediante la especificación de la conducta que se espera. Esta característica también añade control de potencia de bucle cerrado, es decir, RSSI filtrado puede empezar como se recibe la respuesta. Además, un "ir directamente a la máxima potencia" solicitud ha sido introducido. Con ello se espera abordar el tema auriculares pérdida de enlace normalmente se observa cuando un usuario pone su teléfono en un bolsillo en el lado opuesto a los auriculares.

La alta velocidad (AMP), característica de la versión 3.0 de Bluetooth se basa en 802.11, pero el mecanismo de AMP se diseñó para ser utilizado también con otros radios. Originalmente, fue pensado para UWB, pero la WiMedia Alliance, el organismo responsable por el sabor de la UWB destinado a Bluetooth, anunciado en marzo de 2009 que fue la disolución.
El 16 de marzo de 2009, la WiMedia Alliance anunció que iba a firmar un acuerdo de transferencia de tecnología para la WiMedia Ultra-Wideband (UWB) especificaciones. WiMedia ha transferido todas las especificaciones actuales y futuras, incluido el trabajo sobre el futuro de alta velocidad y la optimización de las implementaciones de energía, el Bluetooth Special Interest Group (SIG), Wireless USB Promoter Group y el Foro de Implementadores USB. Después de la finalización con éxito de la transferencia de tecnología, marketing y relacionados con cuestiones administrativas, la WiMedia Alliance dejará de operar.

En octubre de 2009, el Bluetooth Special Interest Group suspendió el desarrollo de UWB como parte de la alternativa MAC / PHY, Bluetooth 3.0 + HS solution. Un número pequeño, pero significativo, de antiguos miembros de WiMedia no tenían y no iban a firmar acuerdos necesarios para la transferencia de propiedad intelectual. El SIG de Bluetooth se encuentra ahora en el proceso de evaluar otras opciones para su plan de acción a largo plazo.

El SIG de Bluetooth ha completado la especificación del Núcleo de Bluetooth en su versión 4.0, que incluye al Bluetooth clásico, el Bluetooth de alta velocidad y los protocolos Bluetooth de bajo consumo. El bluetooth de alta velocidad se basa en Wi-Fi, y el Bluetooth clásico consta de protocolos Bluetooth preexistentes. Esta versión ha sido adoptada el 30 de junio de 2010.
El bluetooth de baja energía ("Bluetooth Low Energy" o BLE) es un subconjunto de Bluetooth v4.0 con una pila de protocolo completamente nueva para desarrollar rápidamente enlaces sencillos. Como alternativa a los protocolos estándar de Bluetooth que se introdujeron en Bluetooth v1.0 a v4.0 está dirigido a aplicaciones de muy baja potencia alimentados con una pila de botón. Diseños de chips permiten dos tipos de implementación, de modo dual, de modo único y versiones anteriores mejoradas.
El 12 de junio de 2007, Nokia y Bluetooth SIG anunciaron que Wibree formará parte de la especificación Bluetooth, como una tecnología Bluetooth de muy bajo consumo.

El 17 de diciembre de 2009, el Bluetooth SIG adoptó la tecnología Bluetooth de bajo consumo como el rasgo distintivo de la versión 4.0. Los nombres provisionales Wibree y Bluetooth ULP ("Ultra Low Power") fueron abandonados y el nombre BLE se utilizó durante un tiempo. A finales de 2011, se presentaron los nuevos logotipos ""Smart Bluetooth Ready"" para los anfitriones y ""Smart Bluetooth"" para los sensores como la cara pública general de BLE.

A mediados de 2016, Bluetooth Special Interest Group (SIG) anuncia la llegada de Bluetooth 5 para finales del año 2016 o principios de 2017 en su página oficial www.bluetooth.com. Afirman que tendrá el doble de velocidad, mejor fiabilidad y rango de cobertura; además de que contará con 800% mayor capacidad que su versión anterior. 

En enero de 2019 se presentó la versión 5.1. Entre las principales novedades que presenta está el que se podrán saber la ubicación de otros dispositivos a los que estén conectados. Esta detección no será 100% precisa como el caso del GPS, pero sí podrá determinar una ubicación con un margen de unos cuantos centímetros.

El 6 de enero de 2020 el Bluetooth Special Interest Group presentó la versión 5.2 del protocolo Bluetooth con mejoras importantes en el modo de radiofrecuencia Bluetooth LE (Low Energy). Entre otras novedades, se presenta el nuevo perfil EATT (Enhanced Attribute Protocol) que mejora el rendimiento cuando hay varios dispositivos BLE conectados de forma simultánea; se aumenta la seguridad al hacer las conexiones cifradas por defecto bajo el perfil EATT; se disminuye el consumo y se aumenta la estabilidad de la señal al permitir optimizar dinámicamente la potencia de la transmisión (LE Power Control); y se permite enviar audio sincronizado a múltiples dispositivos de manera sincronizada (LE Isochronous Channels).

La especificación de Bluetooth define un canal de comunicación a un máximo 720 kbit/s (1 Mbit/s de capacidad bruta) con rango óptimo de 10 m (opcionalmente 100 m con repetidores).

Opera en la frecuencia de radio de 2,4 a 2,48 GHz con amplio espectro y saltos de frecuencia con posibilidad de transmitir en Full Duplex con un máximo de 1600 saltos por segundo. Los saltos de frecuencia se dan entre un total de 79 frecuencias con intervalos de 1 MHz; esto permite dar seguridad y robustez.

La potencia de salida para transmitir a una distancia máxima de 10 metros es de 0 dBm (1 mW), mientras que la versión de largo alcance transmite entre 20 y 30 dBm (entre 100 mW y 1 W).

Para lograr alcanzar el objetivo de bajo consumo y bajo costo se ideó una solución que se puede implementar en un solo chip utilizando circuitos CMOS. De esta manera, se logró crear una solución de 9×9 mm y que consume aproximadamente 97% menos energía que un teléfono celular común.

El protocolo de banda base (canales simples por línea) combina conmutación de circuitos y paquetes. Para asegurar que los paquetes no lleguen fuera de orden, los slots pueden ser reservados por paquetes síncronos, empleando un salto diferente de señal para cada paquete. La conmutación de circuitos puede ser asíncrona o síncrona. Cada canal permite soportar tres canales de datos síncronos (voz) o un canal de datos síncrono y otro asíncrono. Cada canal de voz puede soportar una tasa de transferencia de 64 kbit/s en cada sentido, la cual es suficiente para la transmisión de voz. Un canal asíncrono puede transmitir como mucho 721 kbit/s en una dirección y 56 kbit/s en la dirección opuesta. Sin embargo, una conexión síncrona puede soportar 432,6 kbit/s en ambas direcciones si el enlace es simétrico.

El hardware que compone el dispositivo Bluetooth está compuesto por dos partes:

El LC o Link Controller se encarga del procesamiento de la banda base y del manejo de los protocolos ARQ y FEC de la capa física; además, se encarga de las funciones de transferencia tanto asíncrona como síncrona, la codificación de audio y el cifrado de datos.

La CPU del dispositivo se encarga de las instrucciones relacionadas con Bluetooth en el dispositivo anfitrión, para así simplificar su operación. Para ello, sobre la CPU corre un software denominado Link Manager cuya función es la de comunicarse con otros dispositivos por medio del protocolo LMP.

Bluetooth está definido como un protocolo de arquitectura de capa que está formado por unos protocolos centrales, protocolos de reemplazo de cable, protocolos de control de telefonía, y protocolos adoptados. Como mínimo, toda pila de protocolos de Bluetooth debe tener los siguientes protocolos: LMP, L2CAP y SDP. Además, los dispositivos que se comunican por Bluetooth pueden usar casi siempre los protocolos HCI y RFCOMM.

El protocolo de control de enlace ("Link Management Protocol", LMP) se usa para el establecimiento y control del enlace de radio entre dos dispositivos. Está implementado en el controlador.

El protocolo de control y adaptación del enlace lógico ("Logical Link Control and Adaptation Protocol", L2CAP) es usado para multiplexar múltiple conexiones lógicas entre dos dispositivos que usan diferentes protocolos de nivel superior. Proporciona segmentación y reemsamblado de los paquetes.

En su modo básico, L2CAP proporciona a los paquetes una carga útil que se puede configurar hasta 64 kB, y con una MTU por defecto de 672 bytes.

En los modos de Retransmisión y control de flujo, L2CAP puede configurarse para datos isócronos o para un canal de datos fiables mediante la retransmisión y la comprobación de CRC.

El apéndice 1 de la especificación de Bluetooth añade dos modos adicionales a L2CAP. Estos nuevos modos dejan obsoletos los anteriores modos de retransmisión y control de flujo:
La confiabilidad en cualquiera de estos modos es opcionalmente garantizada por la capa inferior BDR/EDR mediante la configuración del número de retransmisiones y el tiempo de espera antes de descartar paquetes. La capa inferior garantiza que los paquetes lleguen en orden.

El protocolo de descubrimiento de servicio ("Service Discovery Protocol", SDP) permite a un dispositivo descubrir servicios que ofrecen otros dispositivos y sus parámetros asociados. Por ejemplo, cuando usas un teléfono móvil con unos auriculares Bluetooth, el teléfono usa SDP para determinar qué perfil de Bluetooth pueden usar los auriculares y los ajustes del protocolo de multiplexación necesarios para que el teléfono pueda conectarse con los auriculares. Cada servicio está identificado por un UUID ("Universally Unique Identifier").

RFCOMM ("Radio Frequency Communications") es un protocolo de reemplazo de cable usado para generar un flujo de datos virtual en serie. RFCOMM ofrece transporte de datos binarios y emula las señales de control de EIA-232 a través de la capa de banda base de Bluetooth.

RFCOMM ofrece un flujo de datos confiable y sencillo para el usuario, similar a TCP. Es utilizado por muchos perfiles relacionados con la telefonía.

Muchas aplicaciones Bluetooth utilizan RFCOMM debido a su amplio soporte y la posibilidad de encontrar API públicas en la mayoría de sistemas operativos. Además, las aplicaciones que usen el puerto serie para comunicarse, podrán ser portadas a RFCOMM fácilmente.

El protocolo de encapsulación de red de Bluetooth ("Bluetooth Network Encapsulation Protocol", BNEP) se usa para transferir datos de otra pila de protocolos a través de un canal L2CAP. Su principal propósito es la transmisión de paquetes IP en un perfil de red de área personal. BNEP realiza una función parecida a la que hace SNAP en las redes inalámbricas de área local.

El protocolo de control de transporte de audio y vídeo ("Audio/Video Control Transport Protocol", AVCTP) es usado por el perfil de control remoto para transferior órdenes de control de audio/vídeo a través de un canal L2CAP. Los botones de control en unos aurículares estéreo usan este protocolo para controlar el reproductor de música.

El protocolo de distribución de transporte de audio y vídeo ("Audio/Video Distribution Transport Protocol", AVDTP) se usa para el perfil de destribución avanzada de audio para transferir música a los auriculares estéreo a través de un canal L2CAP pensado para la distribución de video.

El protocolo de control de telefonía binario ("Telephony Control Protocol - Binary", TCS BIN) es el protocolo orientado a bits que define la señalización del control de llamadas para el establecimiento de las llamadas de voz y datos entre dispositivos Bluetooth. 

Los protocolos adoptados son aquellos que han sido definidos por otras organizaciones de estandarización y han sido incorporados en la pila de protocolos de Bluetooth, permitiendo a Bluetooth codificar protocolos solamente cuando sea necesario. Los protocolos adoptados incluyen:

Protocolo estándar de Internet para transportar datagramas IP en un enlace punto a punto.

Protocolo base de la suite de protocolos TCP/IP.

Protocolo de la capa de sesión para el intercambio de objetos, proporcionando un modelo para la representación de los objetos y las operaciones.

WAE especifica un marco de aplicación para los dispositivos inalámbricos y WAP es un estándar abierto que permite a los usuarios móviles acceder a los servicios de información y telefonía.

Entre las tareas realizadas por el LC y el Link Manager, destacan las siguientes:

Bluetooth se utiliza principalmente en un gran número de productos tales como teléfonos, impresoras, tabletas, teléfonos inteligentes, altavoces y auriculares. Su uso es adecuado cuando puede haber dos o más dispositivos en un área reducida sin grandes necesidades de ancho de banda. Su uso más común está integrado en teléfonos y tabletas, bien por medio de unos auriculares Bluetooth o en transferencia de ficheros. además se puede realizar y confeccionar enlaces o vincular distintos dispositivos entre sí.

Bluetooth simplifica el descubrimiento y configuración de los dispositivos, ya que estos pueden indicar a otros los servicios que ofrecen, lo que permite establecer la conexión de forma rápida (sólo la conexión, no la velocidad de transmisión).

Puede compararse la efectividad de varios protocolos de transmisión inalámbrica, como Bluetooth y Wi-Fi, por medio de la capacidad espacial (bits por segundo y metro cuadrado).

Bluetooth y Wi-Fi cubren necesidades distintas en los entornos domésticos actuales: desde la creación de redes y las labores de impresión a la transferencia de ficheros entre tabletas, teléfonos inteligentes y ordenadores personales. Ambas tecnologías operan en las bandas de frecuencia no reguladas (banda ISM).

Wifi es similar a la red Ethernet tradicional y como tal el establecimiento de comunicación necesita una configuración previa. Utiliza el mismo espectro de frecuencia que Bluetooth con una potencia de salida mayor que lleva a conexiones más sólidas. A veces se denomina al Wi-Fi la “Ethernet sin cables”. Aunque esta descripción no es muy precisa, da una idea de sus ventajas e inconvenientes en comparación a otras alternativas. Se adecua mejor para redes de propósito general: permite conexiones más rápidas, un rango de distancias mayor y mejores mecanismos de seguridad.

Wi-Fi Direct es un programa de certificación que permite que varios dispositivos Wi-Fi se conecten entre sí sin necesidad de un punto de acceso intermedio.

Cuando un dispositivo ingresa al rango del anfitrión Wi-Fi Direct, éste se puede conectar usando el protocolo ad hoc existente, y luego recolecta información de configuración usando una transferencia del mismo tipo de la de Protected Setup. La conexión y configuración se simplifican de tal forma que algunos sugieren que esto podría reemplazar al Bluetooth en algunas situaciones. Puesto que sus ventajas son una mayor velocidad de transferencia (11 Gbps de 802.11ax contra 50 Mbps de Bluetooth 5.0), cubre una distancia mayor (100 metros contra 10 metros de Bluetooth) y una mayor seguridad teórica al emplear un cifrado de 256 bits contra los 128 bits de Bluetooth. Sin embargo las ventajas que ofrece la tecnología Bluetooth en contrapartida son un menor consumo de energía, la posibilidad de usar más de un dispositivo a la vez y que la distancia reducida minimiza el riesgo de interferencias.




</doc>
<doc id="5053" url="https://es.wikipedia.org/wiki?curid=5053" title="Kilobit por segundo">
Kilobit por segundo

Un kilobit por segundo es una unidad de medida que se usa en telecomunicaciones e informática para calcular la velocidad de transferencia de información a través de una red. Su abreviatura y forma más corriente es kbps. El símbolo estándar internacional es kbit/s.
La abreviatura kb/s corresponde a kilobit por segundo

Equivale a 1000 bits por segundo = 1000 bit/s.

Ejemplo:

64 kbit/s = 1000 x 64 = 64 000 bit/s

En algunas ocasiones los datos se expresan en bytes por segundo (B/s), entonces un kB/s es igual a un kilobyte por segundo, un MB/s es igual a un megabyte por segundo y un GB/s es igual a un gigabyte por segundo.

Debe tenerse en cuenta que las unidades de velocidad de transferencia de información se relacionan en potencias de base 10.

En el caso de las unidades de información referidas a almacenamiento/procesamiento se suele utilizar la misma abreviatura para indicar potencias de base 2, este es un error común, porque para designar potencias de base 2 se deben utilizar los prefijos binarios.

Ejemplo 1: 1 kbit = 1000 bit = 10 bit

Ejemplo 2: 1 Kibit = 1024 bit = 2 bit.

Ejemplo 3: 1 kB = 1000 B = 10 B.

Ejemplo 4: 1 KiB = 1024 B = 2 bytes = (2)·8 bit = 2 bit.
Kilobits por segundo se refiere a la velocidad a la que se transfiere 1 kilobit en un segundo.[1] Un kilobit se refiere a 1.000 bits de información y se toma de las palabras "kilo" y "bit". Kilo es un prefijo común usado con unidades métricas de medida para denotar grupos de 1000. El bit es la unidad de información más pequeña de un sistema digital. Esto pertenece a un valor binario.[^02]

Kbps no debe confundirse con KBps ya que la pequeña "b" se utiliza para denotar bits mientras que la "B" mayúscula significaría un byte, que es un grupo de 8 bits. Por lo tanto, un kilobit se referiría a 1.000 bits mientras que un kilobyte se referiría a 8.000 bits.[^03] Los datos de Internet se indican en bytes, pero las velocidades se miden en bits por segundo, ya que la transmisión de datos se realiza 1 bit a la vez.

La tecnología anterior utilizaba Kbps para medir la velocidad de transmisión de datos, sin embargo, las recientes innovaciones y mejoras en la industria han llevado a velocidades más rápidas, eliminando el uso común de Kbps y sustituyéndolo por velocidades de datos medidas con Mbps, e incluso Gbps. 1 Mbps equivale a 1.000 Kbps o 1.000.000 bps. 1 Gbps es igual a 1.000.000.000 Kbps o 1.000 millones de bps. Los sistemas actuales cuyas velocidades corren a Kbps incluyen:[^04]

Módem de 28.8K - 28.8Kbps
Módem 36.6K - 36.6 Kbps
Módem 56K - 56 Kbps
RDSI - 128 Kbps
DSL / Cable Modems - 512 Kbps (extremo inferior)
ADB - 256 Kbps


</doc>
<doc id="5056" url="https://es.wikipedia.org/wiki?curid=5056" title="Voltio">
Voltio

El voltio o volt, por símbolo V, es la unidad derivada del Sistema Internacional para el potencial eléctrico, la fuerza electromotriz y la tensión eléctrica. Recibe su nombre en honor a Alessandro Volta, quien en 1800 inventó la pila voltaica, la primera batería química.

El voltio se define como la diferencia de potencial a lo largo de un conductor cuando una corriente de un amperio consume un vatio de potencia.

Así mismo, el voltio se define de forma equivalente como la diferencia de potencial existente entre dos puntos tales que hay que realizar un trabajo de 1 J para trasladar del uno al otro la carga de 1 C.

Puede ser expresado en las unidades básicas del SI (m, kg, s, y A) como:

formula_1

El instrumento de medición para medir la tensión eléctrica es el voltímetro.

El voltio estándar se define utilizando un oscilador de unión Josephson, cuya frecuencia de oscilación (extraordinariamente estable) viene dada por:

formula_2

La relación entre la frecuencia y el voltaje a través de la unión, depende únicamente de las constantes fundamentales e (carga del electrón) y h (constante de Planck). Para un microvoltio aplicado a la unión, la frecuencia es:

formula_3

"El voltio estándar se define como el voltaje necesario para producir una frecuencia de 483,5979 MHz en un oscilador de unión Josephson."

En 1800, como resultado de un desacuerdo profesional sobre la respuesta galvánica propugnada por Luigi Galvani, Alessandro Volta desarrolló su propia pila, que a la postre se convertiría en precursora de la batería, que produjo una corriente eléctrica constante. Volta había determinado la más eficaz manera de utilizar metales para producir electricidad, estos metales eran el zinc y la plata. 

En la década de 1880, el Congreso Internacional de electricidad, ahora conocida como Comisión Electrotécnica Internacional (IEC), aprobó el voltio como unidad para medir la fuerza electromotriz. En ese momento, el voltio estaba definido como la diferencia de potencial a través de un conductor eléctrico cuando una corriente de un amperio disipa un vatio de potencia. Antes de la evolución del sistema de medición de la tensión de voltaje estándar de Josephson, en los laboratorios el voltio de referencia se medía mediante baterías especiales debidamente calibradas y construidas al efecto.

La analogía hidráulica se utiliza en muchas ocasiones para explicar el funcionamiento de los circuitos eléctricos: la corriente eléctrica se suele comparar con el agua en las tuberías. El voltaje se asemeja a la presión del agua, ya que en los fluidos esta presión es la que determina su velocidad; el fluido circulante se asemeja a la intensidad de los electrones en el circuito eléctrico. La corriente (en amperios), en la misma analogía, es una medida del caudal de agua que fluye a través de un determinado punto, y la potencia total se mide en vatios. 

Se tiene una ecuación para unir los tres criterios: formula_4.

A continuación figura una tabla de los múltiplos y submúltiplos del voltio en el Sistema Internacional de Unidades:




</doc>
<doc id="5058" url="https://es.wikipedia.org/wiki?curid=5058" title="Acrónimo">
Acrónimo

Un acrónimo (del griego ἄκρος ―transliterado como "akros"― ‘extremo’, y ὄνομα ―trasliterada como "ónoma"― ‘nombre’), en lingüística moderna puede ser una sigla que se pronuncia como una palabra ―y que por el uso acaba por incorporarse al léxico habitual en la mayoría de casos, como "láser" (Light Amplification by Stimulated Emission of Radiation") o "radar" (radio detection and ranging")― o también puede ser un vocablo formado al unir parte de dos palabras. Este último tipo de acrónimos funden dos elementos léxicos tomando, casi siempre, del primer elemento el inicio y del segundo el final, como "bit" (Binary digit) o "transistor" (Transfer resistor).

El significado de un acrónimo es la suma de los significados de las palabras que lo generan. Por ejemplo, el término telemática procede de telecomunicación e informática, que a su vez es acrónimo de información y automática.

Lo contrario de un acrónimo sería un retroacrónimo esto es un tipo de acrónimo que empieza siendo una palabra de uso común y, que después, acaba siendo interpretada como un acrónimo o unas siglas. Por ejemplo, el nombre del proyecto socioeducativo uruguayo Plan Ceibal (un ceibal es un lugar plantado de ceibos, la flor nacional de Argentina y de Uruguay) es un retroacrónimo que significa "Plan de Conectividad Educativa de Informática Básica para el Aprendizaje en Línea". Varias leyes estadounidenses son retroacrónimos, como la ley USA PATRIOT y la PROTECT IP Act.

Los acrónimos y las siglas se escriben sin puntos entre cada una de las letras (a diferencia de las abreviaturas, que sí llevan un punto final).

Los acrónimos pueden componerse de dos sustantivos: "motel", del inglés "motor-hotel"; de un adjetivo y un sustantivo: "docudrama", de "documental dramático"; o de dos adjetivos: "spanglish", de "spanish" e "english".

Los acrónimos siempre se escriben y pronuncian como una palabra normal, y su género es el del elemento principal.

En el español, el proceso de acronimia no es muy habitual, pero sí antiguo. Un ejemplo de esto fue el cambio en el siglo XV de la forma de respeto "vos" por "vuestra merced", que dio origen al acrónimo "usted".

A veces, el acrónimo genera confusión en su significado porque usa partes de términos que ya se usaban independientemente como raíces cultas; por ejemplo "eurocracia", que significa ‘burocracia europea’ y no ‘poder europeo’, como en los términos con -"cracia" (‘poder’); por ejemplo: "autocracia" (‘poder de uno’) o "democracia" (‘poder del pueblo’).







</doc>
<doc id="5059" url="https://es.wikipedia.org/wiki?curid=5059" title="Organización de las Naciones Unidas">
Organización de las Naciones Unidas

La Organización de las Naciones Unidas (ONU), o simplemente las Naciones Unidas (NN. UU.), es la mayor organización internacional existente. Se creó para mantener la paz y seguridad internacionales, fomentar relaciones de amistad entre las naciones, lograr la cooperación internacional para solucionar problemas globales y servir de centro que armonice las acciones de las naciones. Su sede está en Nueva York (Estados Unidos) y está sujeta a un régimen de extraterritorialidad. También tiene oficinas en Ginebra (Suiza), Nairobi (Kenia) y Viena (Austria).

La ONU se rige por la Carta de las Naciones Unidas, que entró en vigor el 24 de octubre de 1945 y se firmó el 25 de junio del mismo año en la ciudad estadounidense de San Francisco, por 51 países, pocos meses antes del final de la Segunda Guerra Mundial. En el preámbulo de la Carta se mencionan explícitamente las dos guerras mundiales.

La ONU se financia por las contribuciones voluntarias de los Estados miembros. Sus principales objetivos son garantizar el cumplimiento del derecho internacional, el mantenimiento de la paz internacional, la promoción y protección de los derechos humanos, lograr el desarrollo sostenible de las naciones y la cooperación internacional en asuntos económicos, sociales, culturales y humanitarios.

Los de las Naciones Unidas y otros organismos vinculados deliberan y deciden acerca de temas significativos y administrativos en reuniones periódicas celebradas durante el año. Los principales órganos de la ONU son la Asamblea General, el Consejo de Seguridad, el Consejo Económico y Social, la Secretaría General, el Consejo de Administración Fiduciaria y la Corte Internacional de Justicia.

La figura pública principal de la ONU es el secretario general. El actual es António Guterres de Portugal, que asumió el puesto el 1 de enero de 2017, reemplazando a Ban Ki-moon.

Los idiomas oficiales de la ONU son seis: árabe, chino mandarín, español, francés, inglés y ruso.

La ONU reemplazó a la Sociedad de Naciones (SDN), fundada en 1919, ya que dicha organización había fallado en su propósito de evitar otro conflicto internacional.

El término «Naciones Unidas» se pronunció por primera vez en plena Segunda Guerra Mundial por el entonces presidente de los Estados Unidos Franklin Roosevelt, en la Declaración de las Naciones Unidas, el 1 de enero de 1942 como una alianza de 26 países en la que sus representantes se comprometieron a defender la Carta del Atlántico y para emplear sus recursos en la guerra contra el Eje Roma-Berlín-Tokio.

La idea de la ONU fue elaborada en la declaración emitida en la conferencia de Yalta celebrada por los aliados en febrero de 1945. Allí Roosevelt sugirió el nombre de "Naciones Unidas".

Aunque inspirada en la Sociedad de Naciones, la ONU se diferencia de esta tanto en su composición como en su estructura y funcionalidad. Por un lado, va a aumentar su universalización, lo que va a permitir la ampliación de la organización por medio de las grandes potencias, de los nuevos estados surgidos tras la descolonización, o de los que surgirán tras el desmembramiento de la Unión Soviética, Yugoslavia y Checoslovaquia en Europa oriental. La Sociedad de Naciones no contaba con las grandes potencias como estados miembros dificultando así el respeto mismo a su autoridad. La ONU al contar con dichas naciones recalca su propia universalidad y autoridad obligando así a los estados miembros respetar las leyes establecidas por la misma organización, evitando repercusiones importantes.

De agosto a octubre de 1944, representantes de Francia, la República de China, el Reino Unido, los Estados Unidos y la Unión Soviética celebraron la conferencia de Dumbarton Oaks para esbozar los propósitos de la organización, sus miembros, los organismos, y las disposiciones para mantener la paz, seguridad y cooperación internacional. La actual organización refleja parcialmente esta conferencia, ya que los cinco miembros permanentes del Consejo de Seguridad (que tienen poder de veto en cualquier resolución de ese Consejo) son dichos estados, o sus sucesores (República Popular China que reemplazó a la República de China-Taiwán y Rusia que sucedió a la Unión Soviética).
El 25 de abril de 1945 se celebró la conferencia de San Francisco (la Conferencia de las Naciones Unidas sobre Organización Internacional). Además de los gobiernos, fueron invitadas organizaciones no gubernamentales. El 26 de junio las cincuenta naciones representadas en la conferencia firmaron la Carta de las Naciones Unidas. Polonia, que no había estado representada en la conferencia, añadió su nombre más tarde entre los signatarios fundadores, para un total de 51 Estados.

La ONU comenzó su existencia después de la ratificación de la Carta por la República de China, Francia, la Unión Soviética, el Reino Unido y los Estados Unidos y la gran mayoría de los otros 46 miembros. El primer período de sesiones de la Asamblea General se celebró el 10 de enero de 1946 en Central Hall Westminster (Londres). La Sociedad de Naciones se disolvió oficialmente el 18 de abril de 1946 y cedió su misión a las Naciones Unidas.

En 1948 se proclama de la Declaración Universal de los Derechos Humanos, uno de los logros más destacados de la ONU.

Los fundadores de la ONU manifestaron tener esperanzas en que esta nueva organización sirviera para prevenir nuevas guerras. Estos deseos no se han hecho realidad en muchos casos. Desde 1947 hasta 1991, la división del mundo en zonas hostiles durante la llamada Guerra Fría hizo muy difícil este objetivo, debido al sistema de veto en el Consejo de Seguridad. Desde 1991 las misiones de paz de la ONU se han hecho más complejas abarcando aspectos no militares que asegurasen un adecuado funcionamiento de las instituciones civiles, como en las elecciones.

En la actualidad, no permanecen las condiciones internacionales que impulsaron la gestación de la ONU; debido a que, el sistema internacional está en constante cambio, los problemas han tomado nuevas formas, han surgido nuevas amenazas, entre las más sobresalientes están: narcotráfico, terrorismo, armas biológicas y químicas, proliferación de armas nucleares, degradación de medio ambiente y las pandemias (Valdés, 2007: 09); así como, nuevas formas de cooperación internacional y temas de relevancia social tales como la brecha digital. Ajustar a la ONU a la nueva realidad internacional ha sido la principal razón de la comunidad internacional y de esa manera evitar que la ONU se convierta en un organismo internacional obsoleto.

Recientemente ha habido numerosas llamadas para la reforma de la ONU. Algunos desean que esta juegue un papel mayor o más efectivo en los asuntos mundiales, otros desean que su papel se reduzca a la labor humanitaria. Ha habido también numerosos llamamientos a ampliar la composición del Consejo de Seguridad para reflejar la situación geopolítica actual (esto es, más miembros de África, América Latina y Asia) y para que se modifique el procedimiento de elección del Secretario General.

Desde 2011 y después de la adhesión de Sudán del Sur, el número de estados miembros es de 193. Están incluidos todos los estados reconocidos internacionalmente, menos:

El último país en ser admitido fue la República de Sudán del Sur, el 14 de julio de 2011.

Los llamados «casos especiales», únicos territorios no miembros, sin calidad de miembro observador y con gobierno propio son:
Ambos territorios están actualmente en libre asociación con Nueva Zelanda. Sin embargo, cada uno podría declarar su independencia solicitando su ingreso a la ONU. Esto ya ha sucedido, por ejemplo, con los Estados Federados de Micronesia, las Islas Marshall y Palaos, todos Estados en libre asociación con Estados Unidos y miembros de las Naciones Unidas.

El artículo 4, del capítulo 2 de la Carta de las Naciones Unidas establece los requisitos para ser estado miembro:

China, representada por el gobierno de la República de China (ROC), fue uno de los cinco miembros fundadores de la ONU en 1945 y formó parte de la ONU como miembro original el 24 de octubre de 1945. Sin embargo, como resultado de la Guerra Civil China, el gobierno de la ROC controlado por el Kuomintang huyó a la isla de Taiwán en 1949, y el gobierno comunista de la República Popular China (RPC), declarada el 1 de octubre de 1949, tomó el control de la mayor parte del territorio de China. Representantes del gobierno de la ROC continuaron representando a China en la ONU, a pesar del pequeño tamaño de la jurisdicción en Taiwán de la ROC (y otras islas no consideradas parte de la provincia de Taiwán) comparado con la jurisdicción en China continental de la RPC, hasta que el 25 de octubre de 1971, cuando la Asamblea General aprobó la resolución 2758, reconociendo al Gobierno de la RPC como el único representante legítimo de China en la ONU, expulsando al representante de Chiang Kai-shek como representante legítimo de China y reconociendo en cambio a la RPC. Esto, en efecto, transfirió el escaño de China en la ONU (incluyendo su asiento permanente en el Consejo de Seguridad) de la ROC a la RPC.

Desde 1991, la ROC ha solicitado repetidamente volver a participar en la ONU, únicamente como representante del pueblo de Taiwán, y no como representante de toda China, utilizando la designación de «República de China en Taiwán», «República de China (Taiwán)» o simplemente «Taiwán». Sin embargo, en 2007 un comité clave de la ONU rechazó por decimoquinta vez consecutiva la solicitud de la ROC. Al consultarle al secretario general Ban Ki-moon sobre los motivos del rechazo dijo que era legalmente imposible, debido a la resolución de la asamblea que expulsó a los nacionalistas chinos en 1971.

En la actualidad, 23 estados miembros de la ONU, además de la Santa Sede, mantienen relaciones diplomáticas con la ROC. La República Popular China, que considera Taiwán como una provincia rebelde, se opone a que la isla-estado sea miembro de la ONU.

Todos los estados miembros de la Unión Europea (UE) forman parte a su vez de la ONU. La UE, a pesar de ser miembro de otras organizaciones internacionales, como la OMC, no forma parte de la ONU. Sin embargo ha desarrollado misiones por encargo de la ONU en diferentes partes del mundo. Tal es el caso de la EUFOR.

Al ratificarse el Tratado de Lisboa, la UE tiene personalidad jurídica única en la sociedad internacional, desde diciembre de 2009. El tratado especifica en lo referente a su acción en la escena internacional y las relaciones con la ONU:

El régimen jurídico de la sede de la ONU está regido por un tratado entre esta y los Estados Unidos de América (Acuerdo relativo a la sede de las Naciones Unidas, del 31 de octubre de 1947), y la Convención sobre Prerrogativas e Inmunidades de las Naciones Unidas, de 1946.

Por razones de seguridad, todo correo recibido es esterilizado. La Administración Postal de las Naciones Unidas emite sellos, con los que deben ser franqueados todos los artículos enviados desde el edificio. Los periodistas acreditados, cuando informan desde el complejo, no deben utilizar «Nueva York» como identificación de su localización en reconocimiento de su estatus de extraterritorialidad.

El complejo diseñado por un equipo internacional de arquitectos incluye los siguientes edificios: la Secretaría (una torre de 39 pisos), la Asamblea General, la Biblioteca Dag Hammarskjöld y el área de conferencias. También hay jardines y esculturas exteriores.

Aunque la sede principal está en el complejo sobre suelo neoyorquino, la ONU y sus organismos especializados y regionales tienen otras sedes, como en: Ginebra, Suiza; La Haya, Países Bajos; Viena, Austria; Montreal, Canadá; Copenhague, Dinamarca; Bonn , Alemania; Nairobi, Kenia; París, Francia; Santiago, Chile; Adís Abeba, Etiopía; Valencia, España; San José, Costa Rica y Buenos Aires, Argentina.

La ONU tiene como idiomas de trabajo al árabe, chino mandarín, español, francés, inglés y ruso.

El Servicio de Radio de Naciones Unidas emite, además de los seis idiomas oficiales, en bengalí, portugués y suajili.

El personal y personal asociado de la ONU se encuentra protegido por la Convención sobre la Seguridad del Personal de las Naciones Unidas y del Personal Asociado, aprobada el día 9 de diciembre de 1994 por la Asamblea General de la ONU.

La financiación de las Naciones Unidas y de algunas de sus agencias especializadas está asegurada por las contribuciones obligatorias de los estados miembros. En el caso de algunas agencias especializadas, su financiación proviene de contribuciones voluntarias de estados miembros, organizaciones, empresas o particulares.

La Asamblea General establece en el presupuesto ordinario las contribuciones obligatorias durante dos años (1 924 840 250 USD en 2006) y determina la aportación de cada miembro basándose en la capacidad de pago de los países, calculado del ingreso nacional por habitante; no obstante, para mantener un nivel de independencia, el nivel máximo de contribución está fijado en el 22 % (el nivel mínimo es un 0,01 % del total). Es importante señalar que las contribuciones obligatorias no siempre son satisfechas por los países y conforme al artículo 19 de la Carta de las Naciones Unidas se le puede quitar el derecho al voto en la Asamblea General al estado miembro cuyos atrasos de pago igualen o superen la cantidad que debiera haber contribuido en los dos años anteriores.

Las celebraciones de la ONU tienen como objetivo contribuir, en todo el mundo, al cumplimiento de los objetivos de la Carta de las Naciones Unidas y sensibilizar al público acerca de temas políticos, sociales, culturales, humanitarios, o relacionados con los derechos del hombre. Son ocasiones para promover acciones nacionales e internacionales y despertar el interés sobre los programas y actividades de las Naciones Unidas.

Se realiza una reunión cada año y cuando un tema es considerado particularmente importante de tratar en ese momento, la Asamblea General puede recomendar al Consejo de Seguridad una conferencia internacional y el Consejo de Seguridad decide si se debe hacer o no para centrar atención global y construir un consenso para una acción unificada se realiza una reunión cada año. Un ejemplo sería la Conferencia de Naciones Unidas sobre el Medio Ambiente y el Desarrollo (Cumbre de la Tierra), del 3 al 14 de junio de 1992, cuyos acuerdos dieron lugar a la adopción del programa Agenda 21 por 179 países.

En este mismo sentido de centrar la atención en temas importantes de interés internacional, la ONU declara celebraciones internacionales, como días, meses, años, etc., para promover, movilizar y coordinar eventos en todo el mundo.

La Carta de las Naciones Unidas en su artículo 26, concibió la posibilidad de un sistema de regulación de los armamentos que aseguraría «la menor desviación posible de los recursos humanos y económicos del mundo hacia los armamentos». La aparición de las armas nucleares ocurrió semanas después de la firma de la Carta y esto supuso un impulso inmediato en el desarrollo de la noción de control de armamento y de desarme. De hecho, la Asamblea General de la ONU adoptó en su primera resolución (febrero de 1946), se refería a los usos pacíficos de la energía atómica y a la eliminación de armas atómicas de destrucción masiva.

La ONU ha establecido varios foros para dirigir los temas del desarme. El principal es el Primer Comité de la Asamblea General de Naciones Unidas sobre Desarme y Seguridad internacional, en cuya agenda se ha tomado en cuenta la prohibición completa de los ensayos nucleares, la prohibición de armas químicas, la no proliferación de las armas nucleares, el establecimiento de zonas libres de armas nucleares, el prevenir, combatir y erradicar el tráfico ilícito de armas pequeñas y ligeras en todos sus aspectos, la exploración y utilización del espacio ultraterrestre con fines pacíficos, el mantenimiento de la seguridad internacional…

En junio de 1978, el primer periodo extraordinario de sesiones de la Asamblea General dedicado al desarme estableció una Comisión de Desarme como un órgano subsidiario de la Asamblea, compuesto por todos los Estados Miembros de las Naciones Unidas. Fue creado como un órgano de deliberación, con la función de considerar diferentes problemas en la esfera del desarme y hacer recomendaciones al respecto y con la de dar seguimiento a las decisiones y recomendaciones pertinentes del periodo extraordinario de sesiones. Desde el año 2000 su agenda se ocupa sólo de dos temas sustantivos. Esta Comisión presenta un informe anual a la Asamblea General.

Las Fuerzas de paz de las Naciones Unidas (los «cascos azules») son enviadas a varias regiones donde han cesado recientemente conflictos armados, para de este modo, hacer cumplir los acuerdos de paz y disuadir a los combatientes de reanudar las hostilidades. Debido a que la ONU no mantiene un ejército independiente, los efectivos son suministrados por los Estados miembros, y su participación es opcional. La autoridad para enviar o retirar a los contingentes de mantenimiento de la paz está en manos del gobierno que los aporta, al igual que la responsabilidad en relación con la paga y cuestiones disciplinarias y de personal.

El Consejo de Seguridad normalmente establece y define las operaciones de mantenimiento de la paz; para ello asigna un mandato a la misión, es decir, una descripción de sus tareas. Para establecer una nueva misión de mantenimiento de la paz o modificar el mandato de una misión existente, nueve de los 15 Estados miembros del Consejo tienen que votar a favor; sin embargo, la propuesta fracasa si alguno de los cinco miembros permanentes del Consejo de Seguridad (China, Estados Unidos, Rusia, Francia y Reino Unido) vota en contra.

La primera operación de mantenimiento de la paz, fue la UNSCOB (United Nations Commission for the Balkans), dispuesta por la Asamblea General de las Naciones Unidas, por Resolución Nro.109(II)del 21 de octubre de 1947. Se llevó a cabo en Grecia entre octubre de 1947 y febrero de 1952.

Todos los estados miembros tienen la obligación legal de pagar la parte que les corresponde del costo de las actividades de mantenimiento de la paz en el marco de una fórmula compleja que ellos mismos establecieron, que incluye una sobrecarga para los cinco miembros permanentes de Consejo de Seguridad. Los países que aportan voluntariamente personal uniformado a las operaciones de mantenimiento de la paz son reembolsados por las Naciones Unidas a una tasa fija de un poco más de 1000 USD por soldado por mes. Las Naciones Unidas también reembolsan a los países por el equipo que aportan.

Durante el segundo mandato de Javier Pérez de Cuéllar como Secretario General, las Fuerzas de Paz de la ONU recibieron en 1988 el . En 2001, la ONU y su Secretario General Kofi Annan ganaron el premio Nobel de la Paz «por su trabajo por un mejor mundo organizado y más pacífico».

La ONU concede las Medallas de las Naciones Unidas a los miembros del servicio militar que hacen cumplir los acuerdos de la Organización.

La preocupación por los derechos humanos fue una de las razones principales para la creación de las Naciones Unidas. Las atrocidades y el genocidio de la Segunda Guerra Mundial contribuyeron a un consenso para que la nueva organización debiera trabajar para prevenir tragedias similares en el futuro. En este sentido se creó un marco jurídico para considerar y actuaba sobre quejas referidas a violaciones de los derechos humanos.

La Carta de la ONU (arts. 55 y 56) obliga a todos sus miembros a promover "el respeto universal a los derechos humanos y a las libertades fundamentales de todos" y para tomar "medidas conjunta o separadamente, en cooperación con la Organización" para tal fin. La Declaración Universal de los Derechos Humanos, aunque no legalmente vinculante, fue adoptada por la Asamblea General en 1948 como un patrón de realización para todos; y consecuentemente, la Asamblea se ocupa regularmente de las cuestiones referidas a los derechos humanos. Así el 15 de marzo de 2006 la Asamblea General de la ONU votó de forma abrumadora para sustituir la Comisión de Derechos Humanos de las Naciones Unidas (UNCHR) por el Consejo de Derechos Humanos de la ONU. Su propósito es tratar violaciones de los derechos humanos. El UNCHR había sido criticado en varias ocasiones por los miembros que la componían, concretamente, varios de sus miembros, como Sudán o Libia, poseían un dudoso historial de respeto de los derechos humanos, incluyendo a los representantes elegidos para presidir la comisión.

La Carta Internacional de Derechos Humanos, dispuso la creación de siete organismos entre los que se destacan el Comité de Derechos Humanos (HRC) y al Comité para la Eliminación de la Discriminación contra la Mujer (CEDAW). El soporte de la Secretaría General se proporciona a través de la Oficina del Alto Comisionado de las Naciones Unidas para los Derechos Humanos (OHCHR), excepto del CEDAW, que lo recibe de la División para el Adelanto de la Mujer (DAW).

Las Naciones Unidas y sus agencias son fundamentales en mantener y aplicar los principios en emanados de la Declaración universal de los Derechos Humanos; por ejemplo, el apoyo de la ONU para los países en transición a la democracia ha contribuido significativamente a la democratización por todo el mundo, y se ha manifestado en la asistencia técnica para posibilitar elecciones libres y justas, en mejorar las estructuras judiciales, en redactar constituciones, en formar funcionarios, o en transformar los movimientos armados en partidos políticos. Esto se ha visto recientemente en Afganistán y Timor Oriental.

Naciones Unidas es también un foro para apoyar los derechos de la mujer para participar plenamente en la vida política, económica y social de sus países. La ONU contribuye a elevar el significado del concepto de derechos humanos a través de sus tratados y su atención a los abusos específicos con sus resoluciones de la Asamblea General o del Consejo de Seguridad o los fallos de la Corte Internacional de Justicia (ICJ).

La ONU conjuntamente con otras organizaciones como la Cruz Roja, proporciona comida, agua potable, refugio y otros servicios humanitarios a las poblaciones que los necesitan, sean desplazados por guerra, o afectados por otros desastres. Las agencias humanitarias más importantes de la ONU son la Oficina de las Naciones Unidas para la Coordinación de Asuntos Humanitarios (OCHA): Organismo perteneciente al Secretariado General de ONU, encargado de realizar acciones de coordinación humanitaria. Apoya organismos como el Comité Permanente Interagencial (IASC por sus siglas en inglés), los Equipos Humanitarios Nacionales o locales; hace la secretaría técnica a INSARAG, grupo especializado en asesorar grupos de búsqueda y rescate; administra los fondos CERF y ERF; realiza acciones de incidencia por los afectados y afectadas, y propone políticas de atención a estos afectados, así como de prevención. Adicionalmente suministra servicios y recursos de información para fortalecer la toma de decisiones.
El Programa Mundial de Alimentos (PMA), que en 2004 repartió comida a unos 100 millones de personas, el Alto Comisionado de las Naciones Unidas para los Refugiados (ACNUR), que hasta 2001 había contribuido a reasentar a por lo menos 25 millones de personas en diferentes países. También se destacan el Programa de las Naciones Unidas para el Desarrollo (UNDP) que es la mayor organización internacional para garantizar asistencia técnica en el mundo, las organizaciones como ONUSIDA, OMS y el Fondo Mundial de Lucha contra el sida, la Tuberculosis y la Malaria (también llamado "Fondo Mundial"), que combaten las enfermedades en el mundo, especialmente en países pobres, y que han ayudado a reducir la mortalidad infantil y maternal. Siguiendo estas iniciativas, en diciembre de 2005, la Asamblea General creó el Fondo de respuesta a emergencias (CERF), adminstrado por OCHA, como un sistema que mejorara la coordinación de la ayuda humanitaria, haciéndola más oportuna y responsable de las víctimas de desastres naturales o hechos por el hombre.

Naciones Unidas publica anualmente el Índice de Desarrollo Humano (IDH), como una forma de ordenar comparativamente los países por su pobreza, la instrucción, la educación, la esperanza de vida, y otros factores como el gasto militar.

Los Objetivos de Desarrollo del Milenio ya aparecen en la Declaración del Milenio, adoptada por la Asamblea General y firmada por 192 países miembros de la ONU el 8 de septiembre de 2000, tras la Cumbre del Milenio; y en este sentido, en la Cumbre mundial de 2005 (14-16 de septiembre de 2005), los representantes de los entonces 191 miembros de la ONU, los reafirmaron como ocho objetivos a alcanzar para el año 2015.

Objetivo 1: Erradicar la pobreza extrema y el hambre:

Objetivo 2: Lograr la enseñanza primaria universal.

Objetivo 3: Promover la igualdad entre los géneros y la autonomía de la mujer.

Objetivo 4: Reducir la mortalidad infantil.


Objetivo 5: Mejorar la salud materna.

Objetivo 6: Combatir el VIH/SIDA, el paludismo y otras enfermedades.

Objetivo 7: Garantizar la sostenibilidad del medio ambiente.

Objetivo 8: Fomentar una asociación mundial para el desarrollo.

Según una investigación hecha por el Centro de Estudios latinoamericanos, publicada en la "Revista Electrónica Iberoamericana" (Vol. 1 n. 1) expone que América Latina, en la primera conclusión del examen, no es positiva, porque si bien se han logrado avances significativos en los puntos 4, 5 y 6 aún falta mucho camino por recorrer para llegar al fin deseado.

La realidad se ve acentuada por una América Latina llena de un sin número de contrastes, en donde se tienen a los hombres más ricos del mundo, por un lado, pero también se encuentran zonas en donde la gente no recibe los servicios más básicos, dichos ámbitos, en los que se hizo un mayor énfasis, fueron: Pobreza extrema, Mortalidad materna, educación primaria universal y cobertura de saneamiento. Sin embargo no todo es desilusión, ya que ha habido grandes avances, esto debido a una reducción de la mortalidad infantil.

Dicha investigación concluye haciendo una advertencia para que los gobiernos en América Latina presten mayor para que de una manera colaborativa, se logren las estrategias necesarias para la reducción de las cifras negativas, así mismo se pide no dejar de lado a los derechos humanos, estos, por ser unos de los mayores logros alcanzados por el hombre, es su lucha por alcanzar la felicidad de todos los ciudadanos.

El artículo 7 de la Carta de las Naciones Unidas indicaba que los órganos principales de la organización eran:

Además la Carta posibilitaba que cada órgano del poder pudiera establecer los organismos subsidiarios que estimara necesarios para el desempeño de sus funciones.

Una de las características singulares del sistema de la ONU es la duplicación de la responsabilidad. Por ejemplo, la UNODOC (Oficina de las Naciones Unidas contra la Droga y el Delito) informa a la Secretaría General, la Asamblea General supervisa el UNICRI (Instituto Interregional para Investigaciones sobre la Delincuencia y la Justicia), pero el Comité Económico y Social tiene dos comisiones orgánicas distintas, la de estupefacientes por un lado, y la de prevención del delito y justicia penal por el otro.

El "Sistema de las Naciones Unidas" está organizado de la siguiente manera (aunque las siglas varían según los idiomas oficiales de este organismo internacional):




Desde su fundación, ha habido muchos llamados para la reforma de las Naciones Unidas pero poco consenso sobre cómo hacerlo. Algunos quieren que la ONU desempeñe un mayor papel o más eficaz en los asuntos mundiales, mientras que otros quieren que su papel se reduzca al trabajo humanitario. También se han hecho numerosos llamados para que se incremente la composición del Consejo de Seguridad de la ONU, las diferentes formas de elegir al Secretario General de la ONU y una Asamblea Parlamentaria de las Naciones Unidas. Jacques Fomerand afirma que la división más duradera en las opiniones de la ONU es "la división Norte-Sur" entre las naciones ricas del Norte y las naciones pobres del Sur. Las naciones del sur tienden a favorecer una ONU más potenciada con una Asamblea General más fuerte, permitiéndoles una mayor voz en los asuntos mundiales, mientras que las naciones del Norte prefieren una ONU económicamente laissez-faire que se centre en amenazas transnacionales como el terrorismo. Algunos críticos perciben que las Naciones Unidas solo están al servicio de los gobiernos de sus países miembros (especialmente los más poderosos) y no de los ciudadanos del común.

Después de la Segunda Guerra Mundial, el Comité Francés de Liberación Nacional fue tardíamente reconocido por Estados Unidos como el gobierno de Francia, por lo que el país fue excluido inicialmente de las conferencias que crearon la nueva organización. El futuro presidente francés, Charles de Gaulle, criticó a la ONU, considerándola una "machin" (artilugio), y no estaba convencido de que una alianza de seguridad global ayudaría a mantener la paz mundial, prefiriendo tratados de defensa directa entre países. Durante la Guerra Fría, tanto Estados Unidos como la Unión Soviética acusaron repetidamente a la ONU de favorecer a la otra superpotencia. En 1953, la Unión Soviética obligó a renunciar al secretario general Trygve Lie, debido a su negativa a tratar con él, mientras que en los años cincuenta y sesenta una popular pegatina de parachoques estadounidense decía: "No se puede escribir comunismo sin la ONU" En una declaración a veces mal citada, el presidente estadounidense George W. Bush declaró en febrero de 2003 (refiriéndose a la incertidumbre de la ONU ante las provocaciones iraquíes bajo el régimen de Saddam Hussein) que "las naciones libres no permitirán que las Naciones Unidas se desvanezcan en la historia como una sociedad de debate inefectiva e irrelevante". En cambio, el presidente francés, François Hollande, declaró en 2012 que "Francia confía en las Naciones Unidas y sabe que ningún Estado, por poderoso que sea, puede resolver problemas urgentes, la lucha por el desarrollo y poner fin a todas las crisis... Francia quiere que las Naciones Unidas sean el centro de la gobernanza mundial". También han surgido críticas hacia la atención de la ONU al tratamiento de los palestinos por parte de Israel, el cual es considerado sesgado en contra de Israel.En septiembre de 2015, Faisal bin Hassan Trad, de Arabia Saudita, ha sido elegido Presidente del panel del Consejo de Derechos Humanos de las Naciones Unidas que nombra a expertos independientes, una medida criticada por grupos de derechos humanos.

Los críticos también han acusado a la ONU de ineficiencia burocrática, desperdicio y corrupción. También se ha acusado a la ONU de problemas con la planificación de sus operaciones y la contratación y manejo del personal. En 1976, la Asamblea General creó la Unidad Conjunta de Inspección para buscar ineficiencias dentro del sistema de las Naciones Unidas. Durante los años noventa, Estados Unidos retuvo las cuotas citando la ineficiencia y sólo comenzó a pagar con la condición de que se introdujera una importante iniciativa de reformas. En 1994, la Oficina de Servicios de Supervisión Interna (OSSI) fue establecida por la Asamblea General para actuar como organismo de control de la eficiencia. En 1994, el ex Representante Especial del Secretario General de las Naciones Unidas en Somalia Mohamed Sahnoun publicó "Somalia: The Missed Opportunities", un libro en el que analiza las razones del fracaso de la intervención de las Naciones Unidas en Somalia en 1992, mostrando que entre el comienzo de la guerra civil somalí en 1988 y la caída del régimen de Siad Barre en enero de 1991, la ONU perdió por lo menos tres oportunidades para evitar grandes tragedias humanitarias; Cuando las Naciones Unidas trataron de prestar asistencia humanitaria, fueron totalmente superadas por las ONG, cuya competencia y dedicación contrastaban marcadamente con la excesiva cautela de la ONU y las ineficiencias burocráticas. Si no se emprendiera una reforma radical, advirtió Mohamed Sahnoun, entonces la ONU seguiría respondiendo a esa crisis con improvisación inepta. En 2004, la ONU se enfrentó a acusaciones de que su recién terminado Programa Petróleo por Alimentos -en el cual a Irak se le permitió comercializar petróleo por necesidades básicas para aliviar la presión de las sanciones- había sufrido corrupción generalizada, incluyendo miles de millones de dólares en sobornos. Una investigación independiente creada por la ONU encontró que muchos de sus funcionarios habían estado involucrados, además de plantear preguntas "importantes" sobre el papel de Kojo Annan, el hijo de Kofi Annan.

También se han hecho críticas a varios organismos de la ONU. Una fuente de críticas radica en el poder de veto de los 5 miembros permanentes del Consejo de Seguridad, el cual ha sido utilizado para proteger los intereses geopolíticos de dichos países, impidiendo la acción de la ONU para salvaguardar la paz y seguridad internacional. Así mismo, se han hecho reproches al hecho de que la Asamblea General tenga un poder limitado y al hecho que las labores del secretario general no estén claramente definidas.

Varios organismos e individuos asociados con la ONU han ganado el Premio Nobel de la Paz en reconocimiento a su trabajo. Dos secretarios generales, Dag Hammarskjöld y Kofi Annan, recibieron cada uno el premio (en 1961 y 2001, respectivamente), al igual que Ralph Bunche (1950), un negociador de la ONU; René Cassin (1968), contribuyente a la Declaración Universal de Derechos Humanos; y el Secretario de Estado estadounidense Cordell Hull (1945) por su papel en la fundación de la organización. Lester B. Pearson, ministro de Asuntos Exteriores de Canadá, recibió el premio en 1957 por su papel en la organización de la primera fuerza de paz de la ONU para resolver la crisis de Suez. UNICEF ganó el premio en 1965, la Organización Internacional del Trabajo en 1969, las Fuerzas de Mantenimiento de la Paz de las Naciones Unidas en 1988, el Organismo Internacional de Energía Atómica (que depende de la ONU) en 2005 y la Organización para la Prohibición de Armas Químicas en 2013. El Alto Comisionado de las Naciones Unidas para los Refugiados fue galardonado en 1954 y 1981, convirtiéndose en uno de los dos únicos recipientes que ganó el premio dos veces. La ONU en su conjunto recibió el premio en 2001, compartiéndolo con Annan.




</doc>
<doc id="5061" url="https://es.wikipedia.org/wiki?curid=5061" title="E.164">
E.164

La E.164 es una recomendación de la Unión Internacional de Telecomunicaciones (UIT) que asigna a cada país un código numérico (código de país) usado para las llamadas telefónicas internacionales.

E.164 es el nombre del documento que especifica el formato, la estructura y la jerarquía administrativa de los números telefónicos. La UIT concede códigos de país a las naciones con soberanía y la administración de los números de teléfono de cada país los efectúa el regulador del país correspondiente. Un número E.164 está compuesto por el código de país, código de zona o ciudad y un número telefónico. Sin embargo, en algunos países, la marcación dentro de una zona o de una ciudad puede ser abreviada, sin necesidad de tener que marcar el código de zona o ciudad.
Lista de indicativos por país y página de la UIT:




</doc>
<doc id="5065" url="https://es.wikipedia.org/wiki?curid=5065" title="Friedrich Schelling">
Friedrich Schelling

Friedrich Wilhelm Joseph (von) Schelling (Leonberg, Wurtemberg, 27 de enero de 1775 - Bad Ragaz, Suiza, 20 de agosto de 1854) fue un filósofo alemán, uno de los máximos exponentes del idealismo y de la tendencia romántica alemana.

Nació el 27 de enero de 1775 en Leonberg (Wurtemberg), hijo de Josephus Friedrich Schelling (1732-1812) y Gottliebin Marie Schelling (nacida Cleß, 1746-1818). Su padre, un pastor protestante —profesión arraigada en la familia Schelling—, gozaba de cierto renombre, ya que había realizado escritos sobre teología; poseía una gran cultura y profundos conocimientos de las lenguas semíticas. En 1777, en el monasterio de Bebenhausen, Joseph Friedrich Schelling se convierte en predicador y pastor del Seminario Superior. En este lugar es donde inicia el primogénito de los Schelling, Friedrich Wilhelm Joseph, sus primeros estudios, los cursa brillantemente, con una anticipación de dos años con respecto a sus demás compañeros.

A los ocho años comienza a aprender letras clásicas. Su precoz madurez intelectual causa admiración a sus profesores. En otoño de 1790 ingresa, con sólo dieciséis años de edad, en el famoso seminario de Tubinga, donde tiene por condiscípulos a Friedrich Hölderlin y Hegel, mayores que él. Se dedica primeramente a la teología, a la exégesis y a las lenguas antiguas. Más tarde tiene lugar su súbita pasión por la filosofía.

El seminario tenía una política de carácter conservador y feudal; en él había un ambiente marcado por la tradición teosófica-mística que tenía sus bases en el Renacimiento. Aunque este instituto intentaba mantener un control de los estudios y las tendencias e ideas políticas de sus alumnos, estos leían, a escondidas, autores que no estaban permitidos, como era el caso de Kant, Lessing, Rousseau, Schiller y Herder; gracias a estas lecturas los estudiantes pudieron criticar los conocimientos tradicionalmente aceptados.

En 1792 se graduó con una tesis escrita en latín sobre el origen del mal humano, titulada "Un intento de explicación crítica y filosófica de los más antiguos filosofemas de Génesis III sobre el primer origen de la maldad humana" ("Antiquissimi de prima malorum humanorum origine philosophematis Genes. III. explicandi tentamen criticum et philosophicum"), donde trata de encontrar una explicación histórica del mal, explicando que la humanidad, en su origen, vivió en una época feliz, y que a causa de una caída cayó en la infelicidad. En este escrito ya se encuentran algunas ideas del futuro movimiento romántico.

En 1793 escribe un segundo trabajo, "Sobre mitos, leyendas históricas y filosofemas del mundo más antiguo", ahora sobre el estudio de los mitos, buscando su esencia y su función dentro de las culturas primitivas. Define el mito como una forma muy particular de filosofar que lleva a cabo el hombre que no ha evolucionado lo suficiente, para después llegar a la abstracción y a la conceptualización.

Una breve estancia en Dresde y en Leipzig (1795) le introduce en el círculo del primer romanticismo, y experimenta un sentimiento romántico a favor de la naturaleza; además realiza estudios de matemáticas, ciencias y medicina. En este mismo año pasa una temporada con su familia y toma la decisión de dejar el seminario, de la misma manera que sus compañeros y amigos Hegel y Hölderlin. También se orienta hacia el estudio del derecho y de las ciencias naturales. Reside generalmente en Leipzig, pero empieza a entablar relaciones en Jena, en esa época la patria de los intelectuales alemanes. En 1795 publica "Del Yo como principio de la filosofía o sobre lo incondicionado en el saber humano", el escrito más destacado de la fase inicial de su filosofía donde, claramente influido por Fichte, aborda la búsqueda de un primer principio incondicionado del saber humano que encontrará en el Yo absoluto. Ese mismo año también ven la luz sus "Cartas filosóficas sobre dogmatismo y criticismo".

Su reputación empieza a crecer, había llamado la atención gracias a sus primeras obras publicadas a partir de 1792, y a propuesta de Fichte («Schelling veía en él al adalid de la causa de la libertad y al que había llevado a su perfeccionamiento la filosofía kantiana, y consideraba que era el campeón de la lucha contra la corrupción del verdadero espíritu crítico, que se había llevado a cabo en los ambientes dogmáticos de Tubinga») y de Goethe, fue nombrado profesor en la Universidad de Jena. Al año siguiente Fichte abandona Jena a causa de una acusación de ateísmo realizada en su contra, y Schelling lo sustituye; a los 23 años toma posesión de la cátedra de Filosofía en la ciudad intelectual más importante del momento: Jena. En esa época Schelling se enamora de Caroline Schlegel (esposa de A. W. Schlegel), doce años mayor que él, y entabla amistad con Schiller y Goethe. Con ella, gran intelectual también, terminará casándose, y abrirá un importante salón en Jena. Caroline le ayudó a corregir sus escritos y a preparar sus ediciones y las de sus amigos y convirtió a su salón el corazón intelectual de Jena.

En 1800 Schelling publicó el "Sistema del idealismo trascendental", donde se materializa un giro crucial en su pensamiento, ya que se aparta abiertamente del idealismo subjetivo de Fichte y se decanta por un idealismo objetivo. Ahora cambia de perspectiva y pone el énfasis, ya no en la naturaleza, sino en el Yo. Esta obra es considerada como la más sistemática y acabada de su primera producción filosófica. Después, en poco tiempo, cambió otra vez de etapa y desarrolló lo que se denominará la filosofía de la identidad, en donde «el énfasis que antes se había puesto respectivamente en la naturaleza y en el yo se pone ahora en un absoluto indiferenciado, raíz común de ambos».

En enero de 1801 le llegó una ayuda en su carrera intelectual en la persona de su antiguo compañero Hegel, con quien editó el "Diario crítico de filosofía". En 1802 publicó "Bruno o sobre el principio divino y natural de las cosas. Un diálogo", uno de sus trabajos centrales de la fase conocida como de la identidad. Desde 1798 hasta 1803 se llevó a cabo su maduración intelectual, «en los que Schelling se dedica a introducir la naturaleza en el idealismo subjetivo fichteano».

En junio de 1803 se casó con Caroline Schlegel tras haberse divorciado esta de su anterior marido. Schelling continuó con sus estudios de ciencias naturales, y en la Universidad de Landshut se graduó en medicina por ese mismo año. En 1806 Schelling fue llamado a Múnich, donde Maximiliano I había fundado la Academia de Ciencias de Múnich, en el que entró como miembro y después secretario, y erigió enseguida una Academia de Bellas Artes, de la que Schelling fue secretario perpetuo.

En septiembre de 1809 Caroline murió de disentería. A consecuencia de la muerte de su esposa, Schelling cambió sus ideas fundamentales, rompió gradualmente con el idealismo y abrió su pensamiento al problema de la libertad y un nuevo punto de vista filosófico-teológico, con su libro "Investigaciones filosóficas sobre la esencia de la libertad humana y los objetos con ella relacionados", considerado por muchos como su escrito más relevante e influyente. La publicación de esta obra dio lugar a una polémica epistolar con Eschenmayer, autor con el que ya había polemizado anteriormente. A partir de ese año, clave en su vida y en su evolución intelectual, Schelling inició un largo periodo de silencio editorial, que no de escritura y reflexión, que se prolongó hasta su muerte y que solo se vio roto por la publicación de textos de escasa relevancia.

Entre 1811 y 1815 redactó tres versiones de "Las edades del mundo" ("Die Weltalter"), escrito que debía constar de tres libros: el pasado, el presente y el fututo. Schelling solo llegó a escribir el primero. Esta obra es considerada por muchos como su texto más difícil y enigmático, uno de sus libros más oscuros, pero también de los más profundos y fascinantes.

Tres años después, en 1812, volvió a casarse (con muchas reticencias a causa del dolor que le ocasionó la muerte de Caroline), ahora con Paulina Gotter. En 1820 salió de Múnich hacia Erlangen, donde se dedicó a dar lecciones públicas durante casi siete años. En 1827 regresó a Múnich para dar clases en la nueva Universidad, donde antes se encontraba la de Landshut. En esta ciudad fue alabado por el rey de Baviera Luis I, quien lo nombró «presidente de la Academia, conservador de colecciones públicas y consejero privado; el rey también le ennobleció con el título de Von Schelling».

En 1841 es llamado por el rey Federico Guillermo IV de Prusia para que vaya a Berlín a ocupar la cátedra que había sido de Hegel, fallecido diez años antes. Se le había llamado para combatir precisamente a Hegel y su panteísmo. En este momento da sus lecciones sobre la Filosofía de la mitología y la Filosofía de la religión. En estas conferencias, entre sus oyentes se encuentran tres jóvenes que llegarían a ser muy importantes: Sören Kierkegaard, Mijaíl Bakunin y Friedrich Engels; sin mencionar a todo el medio intelectual más importante de Berlín. Continúa con la enseñanza hasta 1845. Sus últimos años transcurren en Berlín, en medio de un olvido creciente, entre el cuidado de sus enfermedades, los consuelos familiares, las sesiones académicas y la preparación dificultosa de la filosofía racional, destinada a coronar el edificio del sistema. Muere el 20 de agosto de 1854 en Ragaz, Suiza.

La primera de ellas, que correspondería a su juventud influida por Fichte, estaría en torno a 1795, año en que publica "Del Yo como principio de la filosofía o Sobre lo incondicionado en el saber humano" ("Vom Ich als Princip der Philosophie oder über das Unbedingte im menschlichen Wissen"), probablemente el escrito más relevante de esta fase inicial de su producción filosófica. 

Distinguiríamos, además, una segunda etapa donde su interés se centra en la filosofía de la naturaleza y que se inicia alrededor de 1796, cuando se traslada a estudiar a la Universidad de Leipzig. 
En 1800, se sitúa el periodo donde expone su filosofía trascendental y cuya obra representativa es el "Sistema del idealismo trascendental" ("System des transzendentalen Idealismus"), uno de sus ensayos más importantes y logrados, tanto en la forma como en el contenido. 

Posteriormente, vendría la fase llamada de la identidad, que llegaría hasta 1809, fecha en la que se inicia una época conocida como de la libertad, y cuyo texto paradigmático son las "Investigaciones filosóficas sobre la esencia de la libertad humana y los objetos con ella relacionados" ("Philosophische Untersuchungen über das Wesen der menschlichen Freiheit und die damit zusammenhängenden Gegenstände"). 

Finalmente, podríamos hablar de dos fases más: la primera de ellas la situaríamos en los años donde se redactan las diversas versiones de "Las edades del mundo" ("Die Weltalter") (1811-1815) y la segunda estaría dominada por la distinción entre filosofía positiva y filosofía negativa, llegando hasta 1854, año de la muerte de este pensador.











</doc>
<doc id="5069" url="https://es.wikipedia.org/wiki?curid=5069" title="Níger">
Níger

El Níger, oficialmente República del Níger (en francés: "République du Niger"), es un país sin litoral de África occidental. Limita al sur con Nigeria y Benín, al oeste con Burkina Faso y Malí, al norte con Argelia y Libia, y al este con Chad. Sus zonas septentrional y central se encuentra en las áreas desérticas del Sahara y el Sahel.

Níger fue una colonia europea del África Occidental Francesa, que accedió a la independencia en 1960. Su economía es considerada por algunas fuentes como la más desfavorecida de África y se estima que aproximadamente dos tercios de su población viven bajo el umbral de la pobreza. Así, su índice de desarrollo humano en 2016 fue de 0,353 y ocupando el puesto 187, lo convierte en el , solo por detrás de República Centroafricana. Su democracia es inestable y ha sufrido varios golpes de Estado en las últimas décadas, el último en febrero de 2010, cuando se depuso al presidente Tandja Mamadou tras 10 años en el poder. Hasta abril de 2011 el país fue gobernado por una Junta Militar encabezada por el oficial Salou Djibo, hasta la entrega del poder al vencedor de las elecciones del año 2011, el actual presidente Mahamadou Issoufu.

Tiene una superficie de , y una población de (2015), mayoritariamente musulmana, que se concentra en la franja meridional, en particular en la región suroccidental a orillas del río Níger. Está dividido en siete departamentos y el distrito de su ciudad capital, Niamey. Es un país rico en minerales, entre los que se destaca el uranio, un producto valioso cuyo precio sin embargo registra fuertes fluctuaciones. Sin embargo, apenas el 3,9% de su territorio es apto para la agricultura, lo que sumado a las sequías y a la desertificación de su territorio, lo hace vulnerable a las hambrunas. El gobierno asimismo ha iniciado la exploración y los proyectos de explotación de yacimientos de oro y de petróleo.

El territorio nigerino ha estado habitado por homínidos desde hace más de noventa milenios, según la evidencia arqueológica encontrada. La evidencia botánica, climática y geológica indica que en esos tiempos el proceso de desertificación de toda la región norte del país aún no había comenzado, o era incipiente.

Grupos que vivían del pastoreo dejaron pinturas rupestres de una abundante vida silvestre, animales domésticos y carretas, así como una compleja cultura que se remonta por lo menos al X milenio a. C., que se destacó por sus obras en cerámica así como por su técnica en el desarrollo de arcos y flechas.De ese periodo quedan asimismo huellas de una activa hidrografía, cuando durante el neolítico el clima se hizo clemente, y el río Níger y el lago Chad registraron sus máximos niveles.

Aunque hace seis mil años las regiones septentrionales situadas en el Sahara seguían siendo fértiles, el desierto ya había avanzado notablemente en el actual Níger hacia el siglo XXV a. C.
Desde principios del siglo XII, los tuareg construyeron grandes federaciones, que expandieron su influencia hacia el sur, en las montañas de Air, desplazando a sus antiguos habitantes hacia territorios aún más meridionales. En su momento de mayor poderío, esta entidad controló el actual Níger y ejercía un gran peso en la zona septentrional de Nigeria.

El Imperio songhay se expandió en el actual Níger desde principios del siglo XIV, controlando hasta Agadez antes de su colapso en 1591, entidad de la cual los pueblos zarma y songhai guardan trazas. Tras la caída algunas partes del Imperio y refugiados del actual Malí conformaron una serie de estados Songhai, de los cuales el reino dendi fue el más poderoso.

En el siglo XVII los fulani se desplazaron a la región oriental de Liptako. Los pequeños reinos Zarma y varios estados hausa, que llegaron en el siglo X y se convirtieron al islam en el XIV, chocaron la expansión fulani de Sokoto en el sur. En el siglo XVII controlaban las rutas comerciales entre el Mediterráneo y Ghana y Nigeria. La frontera con Nigeria Británica se basó en parte en la ruptura entre el califato Sokoto al sur, y la dinastía hausa reinante que había abandonado el norte. En el extremo oriental, en la cuenca del lago Chad, la expansión sucesiva de los imperios Imperio Kanem-Bornu difundió étnicamente a los Kanuri y a los Toubou hasta regiones tan occidentales como los oasis de Zinder y Kaouar del siglo X al XVII.

En el siglo XIX el contacto con Occidente comenzó mediante las exploraciones, en particular las del naturalista Mungo Park y el explorador alemán Heinrich Barth, al servicio del Imperio británico. De hecho, los poderes coloniales estaban interesados en conectar sus enclaves orientales y occidentales en el continente africano, por lo que la existencia de una fuente de agua como el lago Chad intensificó aún más el interés por la región.

Los británicos y los franceses se dividieron la región sobre el papel, fijando el límite más al norte de la frontera final, que fue extendida por el oficial Parfait-Louis Monteil en la década de 1890. Aunque los esfuerzos franceses por controlar la región comenzaron antes de 1900, varios grupos no fueron sometidos sino hasta 1922, cuando se consolidó la colonia.

La colonización francesa se realizó a finales del siglo XIX. Las fuerzas coloniales encontraron más oposición de la que esperaban, lanzando la expedición punitiva Voulet-Chanoîne, arrasando el sur del país entre 1898 y 1899, dejando a su paso toda una serie de atrocidades. El límite definitivo entre las colonias francesas y británicas se fijó en 1904, el cual seguía el curso del río Níger, desde Tombuctú hasta el lago Chad.

Durante su dominio colonial, los franceses favorecieron a la etnia zarma así como a los practicantes de la religión musulmana, pues presentaba más concordancias con su sistema que las estructuras indígenas animistas. Algunas prácticas religiosas que fueron percibidas como amenazantes por la administración local fueron en efecto suprimidas.

Aunque las revueltas de los tuareg continuaron, tras el sitio de Agadez en 1916 y 1917 los franceses controlaron la zona. A partir de entonces el actual territorio de Níger pasó a formar parte del África Occidental Francesa. La capital se encontraba en Dakar, Senegal, con un gobernador local en Niamey. En 1931 decenas de miles de personas murieron debido a una hambruna, que llevó a muchos otros habitantes a huir a Nigeria. Tras un corto período de prosperidad, las condiciones alimentarias difíciles regresaron en 1937 y 1940.

Tras la Conferencia de Brazzaville de 1944, además de conceder la nacionalidad francesa a los habitantes de esos territorios, en 1946 la Constitución francesa promovió la descentralización. En 1956 el sistema colonial trató una vez más de adaptarse mediante la Ley de Reforma ("Loi Cadre") del 23 de julio. Tras el establecimiento de la Quinta República Francesa en 1958, Níger pasó a ser un estado autónomo dentro de la Comunidad Francesa, cuando prefirió esa posibilidad a la independencia en un referendo que, sin embargo, despertó sospechas de fraude.

Dos años después el país obtuvo su independencia. El primer presidente fue Hamani Diori, que logró su reelección en las elecciones de 1965 y 1970. Con la ayuda de Francia comienza la explotación de las minas de uranio, al norte, cerca de la frontera con Argelia.

Sin embargo, las grandes sequías que se sucedieron a partir de 1968 y se intensificaron en la década de 1970 generaron un estado de inquietud social y de inestabilidad de gobierno, que condujeron a un primer golpe de Estado militar en 1974 dirigido por el coronel Seyni Kountché, que derrocó a Diori. Kountché sufrió a su vez varios intentos golpistas frustrados, pero también fue afortunado por los descubrimientos de uranio. La explotación del mineral, sin embargo, también acarreó una fuerte inflación.

Kountché falleció y fue reemplazado por su camarada Ali Seibou, que acrecentó su base de poder en la década de 1980. La principal preocupación de sus gobiernos fue desarrollar y diversificar la débil base económica y productiva del país, intentando desprenderse de la dependencia de uranio, entonces la única fuente de ingresos económicos por la exportación. Seibou constituyó el Movimiento Nacional para el Desarrollo Social (MNSD), que sería el único partido político legal. En 1983 se presentó una intensa sequía, a lo cual se sumó el desplome de los precios del uranio y a la concentración del poder político. A finales de la década de 1970 y el primer lustro de la década de 1980 se presentó una hambruna que según algunos cálculos se cobró la vida de 2 millones de personas.

Mientras se extendía la pobreza entre amplias capas de la sociedad nigerina y las sequías amenazaban generar hambrunas, el MNSD comenzó a solicitar préstamos al Fondo Monetario Internacional y al Banco Mundial. Esas entidades exigieron cambios estructurales macroeconómicos, entre ellos el congelamiento de los salarios de los empleados públicos durante dos años, o los impuestos a productos básicos de la canasta familiar, generando mayores niveles de pobreza.

Las organizaciones estudiantiles y obreras se opusieron mediante manifestaciones, huelgas y motines en todo el país, exigiendo una apertura política y el abandono del unipartidismo. En muchos casos hubo una respuesta gubernamental violenta.
Finalmente este país sufre con serias tasas de pobreza extrema.

En 1990 la presión popular llevó a Seibou a adoptar medidas aperturistas, convocando una Conferencia Nacional bajo la dirección de Seku Amadu, quien debía preparar un gobierno de transición. En 1993 se promulgó la Constitución y se celebraron las primeras elecciones libres, vencidas por la Alianza de Fuerza para el Cambio (AFC) — una coalición de seis partidos de oposición — que obtuvo cincuenta de los ochenta y tres escaños en juego. El MNSD perdió, asimismo, la presidencia, que pasó a manos de Mahamane Ousmane, candidato de un de los partidos integrantes de la AFC.

Con el nuevo gobierno se registró un alzamiento de la etnia tuareg de la zona septentrional. Las repetidas sequías habían acabado con el ganado de este pueblo nómada, que fue obligado a convertirse en sedentario mientras reclamaba al Gobierno una solución al problema. Este prometió entregarles tierras, pero incumplió su promesa, lo que generó un conato de guerra civil que duró hasta bien entrado el año siguiente. Por su parte, se registraron manifestaciones estudiantiles. Las discrepancias condujeron a la ruptura de la coalición en 1994, así como a la renuncia del primer ministro. En 1995 se formó una nueva coalición que alcanzó la mayoría en el Congreso. Esta ventaja de poder le permitió exigir la renuncia de todo el poder ejecutivo y la formación de un nuevo gobierno, que quedó en manos de Hama Amadou.

En enero de 1996, sin embargo, el coronel Ibrahim Baré Mainassara dio un nuevo golpe de Estado y suspendió la vigencia de la Constitución, cuyo imperio no ha sido restaurado. Mainassara prometió devolver el gobierno al poder civil y cumplió en 1999, año en que las primeras elecciones municipales dieron el triunfo a la oposición. Esa circunstancia provocó descontento en amplios sectores militares, cuya reacción fue hacer asesinar a Mainassara a manos de su propia guardia. En diciembre el poder regresó a manos de civiles al elegirse presidente a Mamadou Tandja y a Hama Amadou como primer ministro, nombrado en enero de 2000.

La sequía en 2004 provocó una reducción de la producción de grano del 40% y una etapa de hambruna que afecta directamente a cuatro millones de personas, de las que 800.000 son atendidas exclusivamente a través de las Naciones Unidas en campos de emergencia.

En junio de 2005 se alertó por la ONU de un riesgo potencial de hambruna para algo más de 3 millones y medio de nigerinos en el segundo semestre del año. En julio se estableció por las mismas Naciones Unidas un fondo de ayuda de 65 millones de dólares de emergencia para paliar la situación.

El 18 de febrero de 2010 ocurre un nuevo golpe de Estado militar, quedando suspendidas la Constitución y sus instituciones. Al mando del Estado se ha colocado el "Consejo Supremo para la Restauración de la Democracia", liderado por el militar Salou Djibo. Luego del golpe, en las emisoras de radio de todo el país se dejó de escuchar la programación habitual y se empezó a escuchar música militar.

En Niamey las calles quedaron vacías y se vivió un estado de zozobra entre sus habitantes a la espera de información sobre los destinos políticos del país y el depuesto presidente Mamadou Tandja y su gabinete, que se encontraban en el Palacio de Gobierno.

Posteriormente, el presidente Tandja fue llevado a juicio y encarcelado. Las elecciones presidenciales fueron llevadas a cabo en 2011 resultando vencedor Mahamadou Issoufu.

La República de Níger se regía por la Constitución Nacional aprobada en enero de 1993, pero la vigencia de la misma fue suspendida el 27 de enero de 1996 tras el golpe militar del coronel Ibrahim Baré Mainassara. Según ese marco, era una república presidencialista, con un presidente elegido cada cinco años por sufragio libre y universal. Un primer ministro y un consejo de ministros son además del presidente los elementos del poder ejecutivo. El poder legislativo, según el mismo sistema, descansaba en una Asamblea Nacional de ciento trece miembros, elegidos también por un período de cinco años. La justicia era administrada por una Corte Suprema, una Corte Superior y una Corte de Seguridad del Estado.
El mandato de su actual Presidente, Mamadou Tandja, debía finalizar en diciembre de 2009. No obstante, durante su mes de agosto, decidió disolver el Parlamento y orquestar una reforma constitucional con la finalidad de prorrogarlo cinco años, para lo cual convocó un referendo. La iniciativa fue condenada dentro y fuera del país, incluso con sanciones internacionales, pues eliminaba muchos de los obstáculos de su autoridad, abolía los límites de los mandatos, y le daba tres años adicionales en la silla presidencial sin celebrar elecciones. El Tribunal Constitucional declaró esa consulta ilegal, a lo que Tandja respondió con la abolición de ese organismo y remplazó a sus miembros con sus propios candidatos. En 2009 esas decisiones políticas ya habían llevado al país a ser expulsado de la Comunidad Económica de Estados de África Occidental.

Hubo mucha tensión política durante unos meses, que desembocó el mediodía del 18 de febrero de 2010 en un golpe de Estado. Mientras el Presidente y sus Ministros se encontraban reunidos en el Palacio Presidencial celebrando un Consejo de Ministros extraordinario, en Niamey, un grupo de soldados venidos de fuera de la ciudad con vehículos blindados, artillería pesada y ametralladoras de gran calibre, liderados por el mayor Adamou Harouna, han protagonizado un levantamiento militar, reteniendo al Gobierno en pleno no obstante los intentos de frustrarlo por la Guardia Presidencial.

Aunque debido al golpe la Unión Africana expulsó a Níger, ni los inversionistas franceses y ni los chinos han cambiado sus planes, que sumados registran más de seis mil millones de dólares en instalaciones para la extracción minera.

Hasta abril de 2011 el país fue gobernado por una Junta Militar encabezada por el oficial Salou Djibo, año en el que se produjo la entrega del poder al vencedor de las elecciones, el actual presidente Mahamadou Issoufu.

Níger aplica una política exterior moderada y mantiene relaciones amistosas con Occidente y el mundo islámico, así como con los países no alineados. Pertenece a la ONU y a sus principales organismos especializados y en 1980-81 fue miembro del Consejo de Seguridad de la ONU. Níger mantiene una relación especial con la antigua potencia colonial Francia y tiene estrechas relaciones con sus vecinos de África Occidental.

Es miembro fundador de la Unión Africana y de la Unión Monetaria del África Occidental y también pertenece a la Autoridad de la Cuenca del Níger y a la Comisión de la Cuenca del Lago Chad, a la Comunidad Económica de los Estados de África Occidental, al Movimiento de los Países No Alineados, a la Organización de Cooperación Islámica y a la Organización para la Armonización del Derecho Mercantil en África (OHADA). Las regiones más occidentales del Níger se unen a las regiones contiguas de Malí y Burkina Faso en el marco de la Autoridad de Liptako-Gourma.

La controversia fronteriza con Benin, heredada de la época colonial y relativa, entre otras cosas, a la isla de Lété en el río Níger, fue resuelta por la Corte Internacional de Justicia en 2005 en beneficio del Níger.

Las divisiones administrativas de Níger son 7 regiones y un distrito capital. A su vez, las regiones están divididas en 36 departamentos, compuestos por su parte por 275 comunas. Estas pueden ser rurales o urbanas, correspondiendo en el primer caso a municipios o poblados, y en el segundo a barrios de las ciudades. La región con más departamentos es la de Tahoua.

Agadez es con mucho el mayor departamento de Níger, con una superficie de 634.209 km², lo cual equivale a dos veces el tamaño de Polonia. Zinder, Diffa y Tahoua también presentan áreas considerables, de más de 100.000 km². Otros departamentos son Dosso, Maradi, Tillabéri y el distrito capital de Niamey, que es de lejos el de menor extensión con 670 km², comparable al área de Singapur.

Las ciudades de Níger con una población superior a los 100.000 (2006) habitantes son Niamey, Zinder, Maradi, Agadez y Tahoua.

Níger es el del mundo, con un área de . Limita con Argelia (956 km), Benín (), Burkina Faso (), Chad (), Libia (), Malí (821 km) y Nigeria (1497 km).

Níger es un estado sin litoral situado en África occidental. Más de tres cuartas partes de su territorio se encuentran en el desierto del Sahara, situándose la mayoría del país entre 200 y 500 msnm. Difícilmente habitables, constituyen todo el norte del país. La zona más desarrollada es por ende la meridional, en la franja que sigue la frontera desde Nigeria hasta Malí, en particular a orillas del río Níger. En esa área existen grandes sabanas donde es posible criar ganado y llevar a cabo una producción agrícola. Es en esta última región donde se encuentra la capital, Niamey, y la mayoría de los demás centros poblados, como Zinder, Maradi y Tillabéri.

La zona desértica alcanza su máximo rigor en el Teneré. En ella se encuentran macizos montañosos como el del Air, localizado en el norte del departamento de Agadez, que alcanzan su pico máximo en el monte Bagzane, a 2.022 msnm. El área fue declarada Patrimonio de la Humanidad por la Unesco, y en 1992 fue incluida en la Lista del Patrimonio de la Humanidad en peligro. Las ciudades más importantes de esta zona del país son Tahoua y Agadez.

El clima nigerino es uno de los más duros del mundo, presentando las temperaturas más elevadas de todo el planeta. El país ha sufrido un fuerte proceso de desertificación durante el último medio siglo, perdiendo en promedio más de 100 mm de agua de lluvia.
El territorio es principalmente muy caliente y muy seco, con muchas áreas desérticas. En el extremo sur hay un clima tropical en los bordes de la cuenca del río Níger. El terreno es predominantemente formado por llanuras desérticas y dunas de arena, con una sabana plana a ondulada en el sur y colinas en el norte.

La principal cuenca es la del río Níger, que es crucial para el comercio, la agricultura, la pesca, y el turismo. Su principal afluente es el río Benue. Con 4.200 km de distancia es el tercero a escala continental, después del Nilo y del Congo. Nace en la frontera entre Sierra Leona y Guinea, fluye de occidente a oriente, pasando antes de Níger por Malí. Tras atravesar el país entra a Nigeria, donde desemboca en un amplio delta en el Atlántico. En ese sentido constituye la principal fuente de comunicación con el exterior, así como su principal canal para la exportación.

En el extremo suroriental se encuentra una parte del lago Chad, que Níger comparte con Chad, con Nigeria y con Camerún. En esta área habitan los Beri Beri. Junto al río Níger, el lago suministra la mayor parte del agua potable del país.

Las altas temperaturas y el clima desértico ponen al territorio nigerino en permanente peligro a causa de las sequías periódicas. El poco riego artificial y la escasa superficie de tierras cultivables conllevan otros problemas, como el sobrepastoreo y la desertificación antrópica.

Otros problemas ecológicos del país son la caza furtiva, la presión sobre las numerosas especies en peligro de extinción, la erosión y la deforestación.

Los principales biomas presentes en Níger son el desierto, al norte, y la sabana, al sur. Según WWF, las principales ecorregiones presentes en Níger son, de norte a sur el desierto del Sahara, la estepa y sabana arbolada del Sahara meridional, la sabana de acacias del Sahel y la sabana sudanesa occidental.

Además, están presentes el monte xerófilo del Sahara occidental en las montañas de Air, y la sabana inundada del lago Chad, alrededor del lago Chad, en el sureste.

La economía de Níger es una de las menores de los países periféricos y está basada en el pastoreo y la agricultura. La explotación mineral del uranio corresponde por su parte al 31% de los ingresos por exportaciones, siendo el tercer productor mundial de ese material.

El norte y este de Níger, constituido por el altiplano del Djad y parte del desierto de Teneré, es aprovechado por comunidades nómadas para el pastoreo de ganado bovino y caprino. El sur y el oeste, con mayores precipitaciones, está constituido por población sedentaria dedicada a la agricultura del mijo y el sorgo, que constituyen el alimento básico de la población. El cacahuete se dirige a la exportación. Apenas quedan restos de bosques que hasta mediados del siglo XX ocupaban la parte sur del territorio y que fueron talados para usarse como leña.

La escasa actividad agropecuaria está limitada por los apenas 660 km de tierras con regadío y el apenas 3,9% de la superficie nacional que es apta para los cultivos. Además de los cultivos mencionados, Níger produce camote, maíz, arroz, plátanos y tomates.

En 2000 Níger fue reconocido como un país en vías de desarrollo altamente endeudado o HIPC.

El principal producto minero es de lejos el uranio, una industria que ha recibido grandes inversiones extranjeras, en particular de la compañía francesa Areva, que en 2009 estuvo a la cabeza de una inversión de más de mil quinientos millones de dólares para la construcción de la segunda mina más grande del mundo de este mineral.

Otros recursos mineros son carbón, hierro, fosfato, oro y petróleo. La industria está casi exclusivamente vinculada a la agricultura en forma de transformación y envasado de productos agrícolas. En 2002 ocupa el tercer puesto como exportador mundial de mineral de uranio que constituye el 80 % del valor total de las exportaciones del país.

Otros productos minerales son estaño, cinc, molibdeno, hierro y el fosfato de sodio. Las minas de sal de Agadez y Bilma son explotadas de forma artesanal por las caravanas que la transportan a las grandes ciudades de Níger y Benín.

La moneda de Níger es el franco CFA, que en 2005 tenía una paridad con el dólar estadounidense de 525,85. Su renta, de 900 dólares per cápita (2004), es una de las más bajas del mundo. La inflación ronda el 3% anual (2002).
Níger arrastra una deuda pública externa de 1600 millones de dólares (2002). Las inversiones extranjeras son escasas, salvo las derivadas de la explotación del uranio. El país está altamente endeudado con el Fondo Monetario Internacional y el Banco Mundial.

La balanza de pagos es desfavorable, manteniéndose un comercio activo con Francia que importa buena parte del uranio, y con los países vecinos del sur como Nigeria y Ghana. Exporta uranio, ganado y garbanzos a Francia (43,4% de las exportaciones), Nigeria (35%), España (4,5%) y Estados Unidos (3,9%), e importa principalmente maquinarias, cereales y petróleo provenientes de Francia (18,6% de las importaciones), Costa de Marfil (13,4%), Estados Unidos (9,6%) y Nigeria (7,6%). Las importaciones se centran en maquinaria, petróleo y algodón.

Níger es el del mundo, con una población estimada en 2020 de 22,4 millones de habitantes.
La composición étnica del país en 2001 era de un 55,4 % de hausas, un 21 % de zarma, un 9,3 de tuaregs, un 8,5% de peulh-fula, y un 4,7% de kanouri- manga, perteneciendo el resto a otras minorías como los árabes, los gourma, o los tubu. 

Se calcula que viven unos 10.000 franceses en el país. El idioma oficial es el francés, aunque solo una pequeña parte de la población (y concentrada prácticamente en su totalidad en Niamey) lo habla. La principal lengua vernácula es el hausa, que sirve de lengua franca entre los distintos grupos étnicos, y es hablada como idioma materno por el 60 % de los nigerinos.

La esperanza de vida es de 44 años y el promedio de hijos por mujer es de 7,37, la segunda tasa más alta del mundo, lo cual está provocando un aumento poblacional nunca visto en la historia de este pobre país. Del mismo modo, su tasa de natalidad es la más alta del mundo, con 51,6 nacimientos por 1.000 habitantes, aunque la de mortalidad es asimismo elevada, ubicándose en el puesto dieciocho a escala mundial con 14,83 muertes por mil habitantes. La mortalidad infantil es de las mayores del mundo, con 116,66 fallecimientos por 1.000 habitantes, y su esperanza de vida una de las más bajas. Este crecimiento demográfico, no obstante, es frenado por la emigración a países más desarrollados en búsqueda de mejores condiciones de vida.

Tan sólo el 17,6 % de la población está alfabetizada. De acuerdo a estimaciones de 2012, seis localidades nigerinas sobrepasaban los 100 000 residentes; éstas eran: la capital Niamey (1 058 847 habitantes), Zinder (263 766), Maradi (188 008), Arlit (128 807), Agadez (118 647) y Tahoua (110 046).

Níger ha presentado durante los últimos ciento veinte años la siguiente evolución demográfica:

El islam llegó al país en el siglo X.Entre el 80 % y el 90 % de la población es musulmana,de la cual un 95 % es sunnita y sólo un 5 % chiita. El resto de los nigerinos son animistas locales y católicos. No se dispone de datos sobre los ateos.

Níger constituye un verdadero mosaico de grupos étnicos, costumbres y culturas muy distintas. Habitan el centro y sureste los haussa con un alto índice de mestizaje. En las fronteras con Benín y Mali viven los songhay. Los tuaregs ocupan el Macizo de Air. En las llanuras entre Tibesti y Chad moran los Tubu. Mientras que los peul se ubican en las regiones meridionales. Todas estas etnias sobreviven en uno de los territorios más pobres del continente africano. La esperanza de vida de los nigerinos ronda los 46 años, la mortalidad infantil es muy elevada y únicamente 15 habitantes de cada 100 están alfabetizados.
Esta sociedad multiétnica se distribuye entre nómadas y sedentarios, está fuertemente influenciada por el comercio como forma de vida. Para los viajeros de las caravanas el ayuno durante el Ramadán está perdonado. Las costumbres no obedecen solo a la religión sino a una manera de vivir que les permite a enfrentar condiciones adversas determinadas por el clima y la geografía pero empeoradas por las fluctuaciones de su economía y la pobreza.

La Selección de fútbol de Níger es el equipo que representa al país en las competiciones oficiales internacionales. Su organización está a cargo de la Federación Nigerina de Fútbol, perteneciente a la Confederación Africana de Fútbol. No ha participado en la Copa Mundial de Fútbol ni en esa disciplina de los Juegos Olímpicos, pero logró clasificar a las últimas dos ediciones de la Copa Africana de Naciones.

La selección juega sus partidos en el estadio Seyni Kountché en Niamey, con capacidad para 35.000 espectadores, nombrado así en recuerdo de Seyni Kountché. También disputan en sus canchas encuentros de liga los equipos Sahel SC, Olympic FC de Niamey, Zumunta AC y JS du Ténéré.


En los Juegos Olímpicos Níger está representado por el Comité Olímpico de Níger. En Múnich 1972 el boxeador Issaka Dabore logró la en la categoría peso wélter.




</doc>
<doc id="5074" url="https://es.wikipedia.org/wiki?curid=5074" title="Demografía de Alemania">
Demografía de Alemania

La demografía de Alemania es supervisada por la "Statistisches Bundesamt" (Oficina Federal de Estadística de Alemania). Según el primer censo desde la reunificación, la población de Alemania se había contado para ser 79 778 000 en noviembre de 2015, lo que lo convierte en el segundo país europeo más poblado (solo después de Rusia) y el decimosexto país más poblado de el mundo.

La densidad demográfica media ronda los 223 hab./km² (una de las mayores del continente), siendo la . Alemania tiene una serie de grandes ciudades, siendo Berlín la más poblada, sin embargo la mayor aglomeración urbana es la región Rin-Ruhr, que se contabiliza dentro de las zonas más densamente pobladas a nivel global (unos 1,334 hab/km²).

La pirámide de población de Alemania es de contracción:

Esto significa que es un país muy desarrollado con baja natalidad y mortalidad, con alta esperanza de vida (81 años), buena atención médica, y buen sistema educativo.

Alemania

Fronteras históricas.

Fronteras de 1871.






El alemán es el idioma oficial del país y lo hablan casi todos los ciudadanos. Existen varios dialectos regionales, algunos de los cuales difieren sustancialmente del alemán estándar.

La minoría lingüística más grande está constituida por unos 100 000 sorabos (descendientes de las tribus eslavas denominadas wendos o vendos por los alemanes en época medieval) que viven en la región de Lusacia, en la que se encuentran las ciudades de Cottbus y Bautzen, y hablan una lengua eslava.

Unos 10 000 habitantes en Schleswig Meridional hablan algunas variedades del danés como lengua materna.

En el curso de la inmigración desde 1960, más de 1,5 millones de habitantes tienen el turco como lengua materna, muchos de los cuales no tienen ningún conocimiento o solo muy limitado del alemán.

Alemania ha sido históricamente un país de emigrantes e inmigrantes ("véase "Gastarbeiter"").
El constante envejecimiento que la población alemana ha experimentado en los últimos decenios, obliga al Estado a adoptar nuevas formas para lograr un mayor flujo inmigratorio; sin embargo, Alemania tiene un sistema de leyes menos desarrollado respecto al flujo migratorio que otras naciones como Francia o el Reino Unido.

En Alemania viven 7 millones de extranjeros, igual a un 8,5 % de la población. Esta cifra ha disminuido aproximadamente en 600.000 individuos respecto a los últimos años, ya que aumentó el número de extranjeros que adquirieron un pasaporte alemán y disminuyó el flujo de inmigración debido al endurecimiento de las leyes de asilo.

Más de un millón de personas obtuvieron un pasaporte alemán entre 2000 y 2005. De éstos, 200 000 son niños nacidos en Alemania de padres extranjeros. Uno de cada cuatro recién nacidos tiene un padre o una madre extranjeros y cada quinto matrimonio es mixto.

Independientemente de que tengan o no pasaporte alemán, 15 millones de personas son inmigrantes o hijos de inmigrantes y uno de cada cinco matrimonios es binacional en Alemania.

Por otra parte, el programa llamado Green Card lanzado en el año 2000 fue un fracaso y no logró impulsar la inmigración de técnicos cualificados que pretendía.

Alemania no reconoce muchos de los estudios realizados en universidades de países del Tercer mundo, aun si el nivel académico sea igual o incluso superior al de las universidades alemanas. Tal es el caso de ingenieros informáticos provenientes de la India.

Tan solo en el 2004 Alemania registró un incremento de cerca del 20 % en la tasa de emigración.

A los clásicos destinos de emigración como Estados Unidos, Canadá y Australia se les suman España, Reino Unido, Francia, Sudáfrica y Suecia.

La mayor parte de los emigrantes son hombres de entre 25 y 44 años, personas de educación media con profesiones técnicas.

Según el Informe 2003 sobre Migración, en Alemania hay un mínimo de 90 000 ilegales, procedentes en su mayoría de países de la Europa del Este, en especial polacos y rusos, así como africanos y latinoamericanos. La mitad de ellos en Berlín. Se trata de un cálculo conservador hecho a partir de los casos detectados por la policía.

Los ilegales sufren la exclusión de los servicios de sanidad y del sistema escolar. Sin embargo, hay colegios que no se interesan por el estado civil del estudiante y anteponen el derecho a la educación del niño.

Un estudio de la "Oficina para Conflictos Interculturales e Integración Social" (AKI) dice que los ilegales en Alemania no son un problema que crezca rápidamente, pues quienes llegan en pateras a las costas europeas, no alcanzan Alemania; por tanto, los ilegales en este país llegan por tierra.

Alemania no promueve ninguna campaña de legalización de emigrantes ilegales.

Los trabajadores en los sectores de la construcción y la gastronomía están obligados en todo momento a portar la tarjeta de seguro social, en la que constan sus datos personales, así como sus contribuciones al seguro social. Este tipo de controles trata de inspeccionar la situación de los trabajadores "in situ", lo que facilita el control sobre el trabajo ilegal y el salario mínimo. La policía alemana hace redadas en empresas sospechosas de emplear a inmigrantes ilegales, y en caso de confirmarse la sospecha, no solo son apresados los inmigrantes ilegales, sino que se multa también a la empresa.


Página en alemán de estadística religiosa


</doc>
<doc id="5075" url="https://es.wikipedia.org/wiki?curid=5075" title="Babieca">
Babieca

Babieca fue el legendario caballo que las fuentes literarias, a partir del "Cantar de mio Cid" (escrito hacia 1200), y la tradición posterior, atribuyen al noble castellano Rodrigo Díaz, conocido como El Cid Campeador, quien llegó a dominar prácticamente todo el oriente de la península ibérica a finales del siglo XI.

Antes de ser nominado en el "Cantar de mio Cid", el caballo del héroe castellano estaba caracterizado sin nombre en el "Carmen Campidoctoris" (compuesto c. 1190) como un caballo norteafricano comprado por mil dinares, de gran agilidad y velocidad, algo especialmente valorado en los caballos de guerra, que eran robustos, pesados y relativamente lentos. 

Sin embargo, en el mismo "Cantar" a Babieca se le presenta, después de la toma de Valencia y cuando el Cid va a recibir a su mujer e hijas, como un trofeo de guerra que las posteriores prosificaciones cronísticas del poema atribuyen concretamente a la victoria sobre el rey de la Taifa de Sevilla, relatada poco antes en el poema. 

En el siglo XIII, se documenta la tradición posterior que explicó el nombre del caballo aparecido en el "Cantar de mio Cid" a partir del significado que entonces tenía el término «babieca», que solo significaba 'necio' o 'tonto'. A partir de esa acepción se forjó la leyenda explicativa del nombre, documentada en la "Crónica particular del Cid", e imaginada en la infancia del héroe:
El vicario del Arciprestazgo de Campo, que incluía "Cañizal" (Cañizar de Amaya), en el oeste de Burgos, Manuel José del Hoyo, afirmaba que, según tradición, el caballo del Cid era de "Cañizal".

Aunque se ha buscado el porqué del nombre del "Cantar de mio Cid", no se ha encontrado una explicación satisfactoria. La hipótesis mejor fundada fue postulada por Martín de Riquer en 1953 y sostenía que el nombre de «Babieca» fue tomado por analogía con el caballo de Guillermo de Orange, del ciclo épico francés, que se llamaba "Bauçan", ya que en castellano medieval «bausán» significaba 'necio' o 'tonto', lo mismo que «babieca».

Otras explicaciones para la razón de este nombre han sido ofrecidas por Menéndez Pidal, que atribuía a un uso jocoso el sobrenombre del caballo cidiano, aunque esta tesis no concuerda con la caracterización del caballo ni con el tono del poema épico.

Sin soporte documental, se ha pensado que «babieca» signifique 'babeador', pero no existe testimonio alguno de que la palabra «babieca» haya tenido ese sentido, ni es posible relacionar etimológicamente «babieca» con «babeador».

Por último, se ha propuesto que el nombre provenga del uso de algunas hablas aragonesas en las que el término "babieca" (o "babueca") significa 'búho' o 'lechuza', sentido igualmente documentado en dialectos catalanes del Ampurdán con la acepción de 'autillo'.

Según la "Leyenda de Cardeña", elaborada en torno al Monasterio de San Pedro de Cardeña hacia 1270, fue el caballo sobre el que la esposa de El Cid montó el cadáver de éste para hacer creer a sus enemigos que seguía vivo. Después, Babieca no volvió a ser montado y murió dos años más tarde a la inusual edad de cuarenta años. Según esta tradición, fue enterrado en algún lugar del Monasterio de San Pedro de Cardeña, a diez kilómetros de Burgos, en el término municipal de Castrillo del Val y junto a las localidades de Cardeñajimeno y Carcedo.

En la explanada situada frente a la fachada principal, en la que aparece una imagen ecuestre del Cid Campeador, hay una estatua del Sagrado Corazón, y a la izquierda un monolito con leyenda alusiva al caballo Babieca. Coincide con el lugar donde, según la tradición, fue sepultado el fiel animal, aunque las excavaciones arqueológicas financiadas por el Duque de Alba en el año 1949 no obtuvieron resultados.




</doc>
<doc id="5082" url="https://es.wikipedia.org/wiki?curid=5082" title="Costumbre">
Costumbre

Costumbre es un hábito o tendencia adquirida por la práctica frecuente de un acto. Las costumbres de la vida cotidiana son distintas en cada grupo social conformando su idiosincrasia distintiva, que, en el caso de grupos definidos localmente, conforman un determinado carácter nacional, regional o comercial.

Las costumbres son formas de comportamiento particular que asume toda una comunidad y que la distinguen de otras comunidades; como sus danzas, fiestas, comidas, idioma o artesanía.

Estas costumbres se van transmitiendo de una generación a otra, ya sea en forma de tradición oral o representativa, o como instituciones. Con el tiempo, estas costumbres se convierten en tradiciones.

Generalmente se distingue entre las que cuentan con aprobación social, y las consideradas "malas costumbres", que son relativamente comunes, pero que no cuentan con la aprobación social, y suelen promulgarse leyes para tratar de modificar las costumbres.

La palabra griega "ethos" (ἔθος) y la latina "mores" designan el mismo concepto. De la primera derivan términos como "ética" y "etología" y de la segunda "moral".

"O tempora, o mores" ("¡oh tiempos, oh costumbres!") es un tópico literario que proviene de las Catilinarias de Cicerón, para referirse a la corrupción de las costumbres de su tiempo.

Para Aristóteles los hábitos correspondientes son principalmente algo atinente a la voluntad y algo moral ya que la virtud es el conjunto de hábitos voluntarios buenos en cambio el vicio se debe al conjunto de voluntarios malos hábitos. 

David Hume considera que la costumbre es equiparable al hábito sin embargo a diferencia de Aristóteles quien considera que la costumbre se debe a la moral, Hume considera que las costumbres se originan en repeticiones que sirven para explicar "nuestras" creencias en la existencia del mundo exterior o en las relaciones causales. Así es que, según Hume, las costumbres sirven para explicar al mundo.

Costumbre en Derecho, es "la forma de actuar uniforme y sin interrupciones que, por un largo período de tiempo, adoptan los miembros de una comunidad, con la creencia de que dicha forma de actuar responde a una necesidad jurídica, y es obligatoria". Lo cual está formada por los valores universalmente aceptados, y también la moral, siendo esta última particularmente aceptada con sus respectivos cambios en el tiempo y lugar, por determinados grupos sociales. 

Usualmente las leyes son codificadas de manera que concuerden con las costumbres de las sociedades que rigen y, en defecto de ley, la costumbre puede constituir una fuente del derecho. Sin embargo, en algunos territorios, como los españoles de derecho foral (Navarra), y otros países, la costumbre es fuente de derecho primaria y como tal se aplica antes (o a la vez) que la ley.

En el Derecho penal no funciona la costumbre como una norma porque se abarca más a las Leyes, en donde las costumbres no pueden ser contrarias a la ley es valida y puede ser alegada, y en tal caso se debe probar conforme lo dispone el artículo 178 del código general del proceso: ¨ Los usos y costumbres aplicables conforme a la ley sustancial deberán acreditarse con documentos¨. 

En Derecho internacional, la costumbre es una práctica generalizada y repetitiva de los Estados y de otros sujetos de derecho internacional, aceptada como norma y obligatoria a través de lo denominado como expectativa de derecho. Tiene tanta validez como los tratados internacionales, y no existe ninguna prelación de fuentes entre ellas.

No obstante, hay que tener en cuenta los hechos que llevan a una práctica general y uniforme a ser considerada derecho por los sujetos del Derecho internacional. Para que cristalice la "opinio iuris" o elemento subjetivo, resultan de vital importancia las acciones u omisiones que realizan Estados que son significativos de la comunidad internacional. 

En sociología y antropología se estudian los "usos y costumbres" (José Ortega y Gasset), que son componentes de la cultura en los sistemas de acción, como adaptación instrumental y, por tanto, parte de la estructura social en el funcionalismo. También es asimilable por similitud con conductas en psicología social y en la teoría sistémica. Como componente de nuevas leyes impuestas a una comunidad, es la resistencia popular más importante contra el cambio.

avanzada


</doc>
<doc id="5087" url="https://es.wikipedia.org/wiki?curid=5087" title="Semana">
Semana

Se conoce como semana (del latín tardío "septimāna", y este del latín "septem", ‘siete’) al ciclo compuesto por siete jornadas seguidas; es decir al período de 7 días naturales con carácter de consecutivos, que de acuerdo a la norma ISO 8601 adoptada por la mayoría de los países del mundo, comienza el lunes y finaliza el domingo, aunque hay países que consideran el primer día de la semana al domingo y el último al sábado, de acuerdo con la semana litúrgica cristiana, o que la hacen comenzar en sábado, en algunos países musulmanes. 

La semana es el período de tiempo estándar utilizado para los ciclos de días de trabajo y de descanso en la mayoría de las partes del mundo.

Los siete días que forman la semana son:

Se supone que en las primeras épocas de la humanidad, cuando los seres humanos descubrieron el ciclo solar (la regularidad de la aparición del verano y del invierno), se dieron cuenta de que se podía medir el tiempo transcurrido y la edad de una persona por la cantidad de pasos del invierno a la primavera que había vivido. Cuando se conoció más el ciclo anual, se pudo dividir en 4 estaciones trimestrales (más o menos convencionales, ya que las estaciones nunca duraban la misma cantidad de tiempo ni eran exactamente iguales).

En algún momento (antes o después del descubrimiento anterior) se descubrió el ciclo de las fases lunares. La Luna pasa por cuatro momentos fáciles de discriminar:
Entre dos fases lunares hay aproximadamente una semana de siete días.

Contando desde la luna nueva, los babilonios celebraban un ciclo de siete días dedicados a los siete "planetas" o deidades. En cada uno de ellos, se prestó atención (y se hicieron ofrendas, para ilustrar también a los menos capaces de pensar en abstracto) a un aspecto diferente de la realidad, lo que la ilusión llamó dioses: Marduk e Ishtar el día 7, Ninlil y Nergal el día 14, Sin y Shamash el día 21, y Enki y Mah el día 28. Las tablillas de los reinados del siglo VI a. C. de Ciro el Grande y Cambises II indican que estas fechas eran a veces aproximadas. El inicio de la lunación era a veces aproximado; el final del ciclo, contenía entre uno y tres días días de descanso, y en ocasiones hasta una última semana de ocho o nueve días inclusive, rompiendo el ciclo continuo de siete días. 

En la religion cristiana, como en el judaísmo, se sostiene la idea de que el ciclo semanal de siete dias se debe a que YHWH, el Dios creador, demoro siete dias de 24 horas en terminar la creación del mundo. En el ciclo semanal descrito por las escrituras hebreas se considera el domingo como el primer día del ciclo semanal y el shabbat (en hebreo) o sábado como el septimo y último dia de la semana tal como se describre en el libro de Genesis "fueron pues acabados los cielos y la tierra y todo el ejercito de ellos, y acabo Dios el dia septimo la obra que hizo" ("Génesis" 1:1 - 2:1-3).

El origen de estos nombres está en la observación del cielo por los antiguos. Durante el año, la inmensa mayoría de los astros visibles no cambiaban de posición unos con respecto a otros. Sin embargo, aquellos seres humanos observaron a simple vista siete cuerpos celestes que sí variaban de posición: el Sol, la Luna, y los cinco planetas que pueden verse a simple vista: Marte, Mercurio, Júpiter, Venus y Saturno.

En hebreo simplemente se numeran (primer día, segundo día, tercer día, etc.) contando desde el domingo, excepto el séptimo y último, que se llama "shabbat".

En árabe también se numeran excepto el sexto "al-Jum'ah" (día de la reunión en la mezquita) y el séptimo "asSabt."

En griego moderno también se numeran excepto el primero "kyriakí" (día del Señor), el sexto "paraskeví" (día de la preparación) y el séptimo "sávato",

En portugués los días de lunes a viernes se llaman "segunda-feira", "terça-feira", "quarta-feira", "quinta-feira", y "sexta-feira", y los dos restantes se llaman como en español, "sábado" y "domingo".

Mientras que los idiomas mediterráneos orientales reflejan la numeración de los días de la semana, los idiomas de Europa Occidental (excepto el portugués) reflejan los nombres de los astros móviles del firmamento: Luna, Marte, Mercurio, Júpiter, Venus, Saturno, Sol. Estos siete cuerpos celestes dieron sus nombres a los días de la semana: lunes, martes, miércoles, jueves, viernes. En español, sábado procede de la palabra hebrea "shabbat" (día de descanso), y domingo de la palabra latina "domínica" (día del Señor). No obstante, en algunos idiomas (como el inglés, por ejemplo), se mantienen los nombres originales de estos dos días: "saturday" (día de Saturno) y "sunday" (día del Sol); y en otros idiomas se sustituyen los dioses grecorromanos con los dioses germánicos más o menos correspondientes. Así, el dios germánico de la guerra Tiw ("tuesday") sustituye al marcial grecorromano Marte, el principal dios germánico Woden ("wednesday") al dios secundario Mercurio, el importante dios guerrero Thor ("thursday") al importantísimo Júpiter, la diosa de la fertilidad Freya o Frigg ("friday") a la diosa del amor Venus.
Los nombres latinos de los dioses relacionados con los astros móviles del firmamento son meras transliteraciones de los nombres griegos, los cuales a su vez son transliteraciones de los nombres babilónicos, los cuales se remontan a los sumerios. Sin embargo, hubo una mala interpretación, son ejemplos: Nergal es el dios de la guerra pero también de la pestilencia y especialmente del infierno, de esta manera se superpone con el griego Hades. Mientras Cronos es el padre de Zeus, Ninurta es el hijo de Enlil.

Samuel A. Goudsmit, en "El tiempo" (Nueva York, 1966, pág. 24), prueba que los egipcios dividían cada uno de los 12 meses de 30 días (de su año de 360 días) en tres semanas de 10 días. Lo mismo hacían los griegos de esa época. No se sabe en qué momento cambiaron ese calendario por la adoración de los planetas, pero debe haber sido antes del siglo IV a. C., ya que Heródoto, en "La historia" (2.82), escribió: «Estos son algunos de los hallazgos de los egipcios. Descubrieron que [...] cada día le pertenece a un dios».

Stephen Herbert Langdon, en "La mitología de todas las razas" (Nueva York, 1964, pág. 154) prueba que los seguidores del culto a Sin (en Harrán), a quienes los escritores árabes y sirios conocían como arranianos o sabeanos les habían puesto los nombres de los planetas a sus días. Como los hebreos y otros pueblos, consideraban que el día dedicado a Saturno era el séptimo día, así que comenzaban la semana con un día dedicado al Sol. Para el resto de los días utilizaban el mismo orden que los egipcios.

Steven L. Renshaw, en "El sistema solar y los nombres de los días", demuestra que esos mismos astros del sistema solar, y en la misma secuencia, se usaron para nombrar los días en India, Tíbet y Birmania. También sucedió lo mismo en Japón, pero esa costumbre se ha podido rastrear solo hasta mil años atrás.

Los soldados romanos estacionados en Egipto se acostumbraron a la semana pagana de siete días y poco a poco la introdujeron en su país, reemplazando la semana oficial de ocho días (octaviano, César Augusto) y los siguientes gobernantes romanos toleraron esta práctica, que se oficializó con Constantino I el Grande en el año 321 de nuestra era.

La hipótesis más conocida acerca del origen del orden de los planetas es la siguiente: si se disponen los planetas de acuerdo al conocimiento erróneo ―desde una astronomía geocéntrica― que los antiguos tenían de sus respectivas distancias a la Tierra ―en realidad de cuánto tiempo tardaban en dar un ciclo completo en relación al fondo de estrellas―, el orden (de lejano a cercano, o de más lento a más rápido) sería:


Algunos pueblos mediterráneos pensaban que cada hora del día era regida por el Sol, la Luna o uno de los cinco planetas conocidos en aquel entonces. Los cuales eran dioses que giraban eternamente alrededor de la Tierra. La secuencia en que ellos se gobernaban correspondía al orden inverso de sus distancias a la Tierra.

Según Michael Macrone, en su libro "¡Por Júpiter!" (1992), en esa época los egipcios pensaban que el planeta más distante era Saturno. Por lo tanto creían que la primera hora era regida por Saturno, la segunda hora por Júpiter, y así por el estilo. También creían que después de que pasaban las primeras siete horas (regidas por los siete astros conocidos) la cuenta se repetía.

Según la "Enciclopedia católica", en el artículo «Domingo», los antiguos egipcios creían que el planeta que regía la primera hora también regía el período completo de 24 horas, y daba su nombre a ese día.

Elias Joseph Bickerman, en "Cronología del mundo antiguo" (Universidad Cornell, 1968) afirma que Celso había escrito que esta misma doctrina formaba parte de la cosmogonía persa.

Hace 600 años, Chaucer describió estas mismas creencias (que él creía de origen griego) en su "Tratado acerca del astrolabio" (en el capítulo «Declaración especial acerca de las horas de los planetas»). El texto de Chaucer es traducción de un manuscrito griego mucho más antiguo.

Según Vetio Valente, quien vivió en el siglo II d. C. y es la autoridad más conocida sobre astrología en el mundo antiguo, la primera hora del día comenzaba al atardecer, lo cual era tradicional entre griegos y babilonios. Dice también que las mitades diurnas y nocturnas del día eran presididas por los astros que corresponden a la primera hora de cada mitad. Esto se confirma por un grafiti pompeyano que nombra el 6 de febrero del 60 un domingo, cuando actualmente se diría que fue miércoles. Al parecer, la cuenta de los días de la semana tras la primera hora diurna constituía una semana alterna a la ordinaria, como se desprende de las cartas pascuales del obispo Atanasio, y en una tabla de fechas pascuales para los años 311-369 que sobrevive en una copia etíope.

Es muy probable que los antiguos sumerios utilizaran este sistema para generar el orden de los días de la semana; lo que no se sabe es por qué lo hicieron, o sea, cuál era el mito que sostenía este ordenamiento.

En 1988 se firmó la norma ISO 8601, que es la convención internacional que indica el orden de los días de la semana. Esta norma establece que la semana comienza el lunes y finaliza el domingo, siendo la norma que se sigue en la inmensa mayoría de los países del mundo.
Sin embargo, en los calendarios litúrgicos y en algunos países, la semana comienza el domingo.

En 2012, en Reino Unido, la mayoría de las agendas y calendarios utilizan el lunes como comienzo de la semana; sin embargo algunos utilizan el domingo.

En Brasil y Portugal, aunque la semana comienza el lunes desde los años 1990, los nombres de los días sugieren que tradicionalmente la semana comenzaba el domingo: el lunes se llama "segunda-feira" (‘segunda feria’), el martes "terça-feira" (‘tercera feria’), etc. De todas maneras se identifica al sábado y al domingo como fin de semana.

En el judaísmo, el comienzo y fin de la semana se basa en el escrito "...porque en seis días hizo Dios a los cielos y a la tierra, y en el día séptimo descansó" (Éxodo 31-16), dicho escrito va pegado a la creencia que los días comienzan con el atardecer y terminan con la caída del sol del día siguiente; “... y fue el anochecer y fue el amanecer de un día" (Génesis 1-5). Por eso, en Israel la semana comienza el sábado luego de la caída del sol (aunque la mayoría de las personas comienzan a trabajar el domingo, muchos negocios vuelven a iniciar labores desde el sábado por la noche).

Las culturas con una fuerte herencia europea ―y algunas otras― toman el viernes, sábado y el domingo como el fin de semana.
Varios países musulmanes toman como fin de semana el sábado y el domingo (todos incluyen el viernes).
En 1976, cuando Argelia se independizó del Imperio francés, para diferenciarse de sus conquistadores cambió el fin de semana de sábado-domingo a jueves-viernes (que respeta el día sagrado musulmán del viernes).
A mediados de agosto de 2009, Argelia ―para aumentar el comercio con otros países― volvió a cambiar el fin de semana de jueves-viernes a viernes-sábado.

En Europa existe una tradición de reforma desde la época precristiana, prueba de ello son los calendarios solares y lunisolares como el calendario helénico, el calendario romano, el calendario juliano y hasta el actual calendario gregoriano.

Los soldados romanos estacionados en Egipto se acostumbraron a la semana pagana de siete días y poco a poco la introdujeron en su país, reemplazando la semana oficial de ocho días (octaviano, César Augusto) y los siguientes gobernantes romanos toleraron esta práctica, que se oficializó con Constantino I el Grande en 321 de nuestra era.

La revolución francesa instauró una semana de diez días, abolida por Napoleón.

La revolución rusa cambió a una semana de seis días, cinco laborables y uno de descanso, que perduró hasta 1940.

Los importantes cambios y reformas de calendario casi nunca interrumpieron el ciclo de siete días.

Hector Santos ―en «The day no filipinos were born» (‘El día en que no nació ningún filipino’); en "Sulat sa Tansô", 3 de abril de 1997)― explica que Filipinas tenía la fecha española debido a que su contacto con Europa era a través del este (por América). Este desarreglo fue compuesto cuando el arzobispo de Manila decretó que no existiría el martes 31 de diciembre de 1844, y que el lunes 30 de diciembre de 1844 sería seguido por el miércoles 1 de enero de 1845.

El 30 de marzo de 1867, el Imperio ruso vendió Alaska a Estados Unidos por 7,2 millones de dólares estadounidenses). Por eso su calendario juliano se cambió al gregoriano. El cambio requirió la pérdida de 12 días: el 6 de octubre de 1867 dio paso al 18 de octubre de 1867.

En algunos calendarios revolucionarios la duración de la semana cambia, y cambia también el nombre de la misma, ya que etimológicamente el término «semana» proviene de «siete» ("septem" en latín).







</doc>
<doc id="5090" url="https://es.wikipedia.org/wiki?curid=5090" title="Biotecnología">
Biotecnología

La biotecnología (del griego βίος [bíos], «vida», τέχνη [-tecne-], «destreza» y -λογία [-logía], «tratado, estudio, ciencia») se refiere a toda aplicación tecnológica que utilice sistemas biológicos y organismos vivos o sus derivados para la creación o modificación de productos o procesos para usos específicos. Dichos organismos pueden o no estar modificados genéticamente por lo que no hay que confundir Biotecnología con Ingeniería Genética. Sus bases son la ingeniería, física, química, biología, medicina y veterinaria; y el campo de esta ciencia tiene gran repercusión en la farmacia, la medicina, la ciencia de los alimentos, el tratamiento de residuos sólidos, líquidos, gaseosos, la industria y la agricultura.

Probablemente el término fue acuñado por el ingeniero húngaro Károly Ereki, en 1919, cuando lo introdujo en su libro "Biotecnología en la producción cárnica y láctea de una gran explotación agropecuaria".

Según el Convenio sobre Diversidad Biológica de 1992, la biotecnología podría definirse como «toda aplicación tecnológica que utilice sistemas biológicos y organismos vivos o sus derivados para la creación o modificación de productos o procesos para usos específicos».

El Protocolo de Cartagena sobre Seguridad de la Biotecnología del Convenio sobre la Diversidad Biológica define la "biotecnología moderna" como la aplicación de:

La biotecnología tiene aplicaciones en importantes áreas industriales, como la atención de la salud, con el desarrollo de nuevos enfoques para el tratamiento de enfermedades; la agricultura, con el desarrollo de cultivos y alimentos mejorados; usos no alimentarios de los cultivos, por ejemplo plásticos biodegradables, aceites vegetales y biocombustibles, y cuidado medioambiental a través de la biorremediación, como el reciclaje, el tratamiento de residuos y la limpieza de sitios contaminados por actividades industriales. A este uso específico de plantas en la biotecnología se le llama biotecnología vegetal. Además, se aplica en la genética para modificar ciertos organismos.

Las aplicaciones de la biotecnología son numerosas, y suelen clasificarse en:


La biotecnología ha aportado nuevas herramientas diagnósticas, especialmente útiles para los microorganismos que son difíciles de cultivar, ya que permiten su identificación sin necesidad de aislarlos. Hasta hace muy poco tiempo, todos los métodos se basaban en el cultivo microbiológico, la tinción histológica o las pruebas químicas y determinaciones en suero, algunos métodos en general largos y tediosos que requieren mucha mano de obra y son muy difíciles de manejar. El desarrollo de los inmunodiagnósticos con los anticuerpos monoclonales y de las técnicas que analizan el material genético como la hibridación y secuenciación del ADN o ARN, con la inestimable ayuda técnica de la PCR, han sido un logro biotecnológico importante y decisivo para introducir el concepto del diagnóstico rápido, sensible y preciso. Además, se tiene en cuenta que esta metodología permite su robotización y automatización en el futuro del diagnóstico molecular y genético, que es muy esperanzador.

La biotecnología ha proporcionado herramientas para el desarrollo de una nueva disciplina, la patología molecular, que permite establecer un diagnóstico del cáncer basado no en la morfología del tumor, como hace la anatomía patológica clásica (microscopía combinada con histoquímica), sino en sus características patogénicas debidas a las alteraciones genéticas y bioquímicas. La patología molecular ha incorporado técnicas de inmunohistoquímica y análisis genético al estudio de las proteínas o de los ácidos nucleicos extraídos de los tumores. Estas técnicas han permitido la detección precoz de las células malignas y también su clasificación. Un tumor que se ha detectado en sus fases iniciales y que está bien clasificado puede eliminarse con facilidad antes de que se produzca su diseminación a otros lugares del organismo, de manera que su detección y clasificación precoz puede salvar más vidas que el desarrollo de nuevas terapias.






La biorremediación es el proceso por el cual se utilizan microorganismos para la limpieza de un sitio contaminado. Los procesos biológicos desempeñan un papel importante en la eliminación de contaminantes y la biotecnología aprovecha la versatilidad catabólica de los microorganismos para degradar y convertir dichos compuestos. En el ámbito de la microbiología ambiental, los estudios basados en el genoma abren nuevos campos de investigación "in silico" ampliando el panorama de las redes metabólicas y su regulación, así como pistas sobre las vías moleculares de los procesos de degradación y las estrategias de adaptación a las cambiantes condiciones ambientales. Los enfoques de genómica funcional y metagenómica aumentan la comprensión de las distintas vías de regulación y de las redes de flujo del carbono en ambientes no habituales y para compuestos particulares, que sin duda acelerarán el desarrollo de tecnologías de biorremediación y los procesos de biotransformación.

Los entornos marítimos son especialmente vulnerables, ya que los derrames de petróleo en las regiones costeras y en mar abierto son difíciles de contener y sus daños difíciles de mitigar. Además de la contaminación a través de las actividades humanas, millones de toneladas de petróleo entran en el medio ambiente marino a través de filtraciones naturales. A pesar de su toxicidad, una considerable fracción del petróleo que entra en los sistemas marinos se elimina por la actividad de degradación de hidrocarburos llevada a cabo por comunidades microbianas, en particular, por las llamadas bacterias hidrocarbonoclásticas (HCB). Además, varios microorganismos, como "Pseudomonas", "Flavobacterium", "Arthrobacter" y "Azotobacter", pueden utilizarse para degradar petróleo. El derrame del barco petrolero Exxon Valdez, en Alaska en 1989, fue el primer caso en el que se utilizó biorremediación a gran escala de manera exitosa: se estimuló la población bacteriana, suplementándole nitrógeno y fósforo, que eran los limitantes del medio.

Se ha propuesto el uso de procesos biológicos para la destoxificación de residuos y remediación de sitios afectados, debido a que han demostrado ser más prácticos y económicamente factibles para el manejo y tratamiento de diferentes tipos de residuos de las actividades de exploración y producción de petróleo. Los métodos de tratamiento biológico dependen de la capacidad de los microorganismos para degradar residuos aceitosos a productos inocuos (dióxido de carbono, agua y biomasa) a través de reacciones bioquímicas. Sin embargo, existen algunas limitantes que dificultan su aplicabilidad como, por ejemplo, la disponibilidad de nutrientes, el alto contenido de arcillas, aireación y la disponibilidad del contaminante, sin mencionar la edad de la contaminación. Estudios realizados recientemente en el Instituto Mexicano del Petróleo demostraron el potencial de aplicación de las tecnologías de biorremediación en sitios contaminados con lodos y recortes de perforación mediante la aplicación de la tecnología de composteo en biopilas.

El uso de nuevas tecnologías para las aplicaciones diarias como el bioplástico, con menor tiempo de degradación, contribuye al mejoramiento del ambiente, disminuyendo la utilización del PET, uno de los principales contaminantes.

La ingeniería biológica o bioingeniería es una rama de la ingeniería que se centra en la biotecnología y en las ciencias biológicas. Incluye diferentes disciplinas, como la ingeniería bioquímica, la ingeniería biomédica, la ingeniería de procesos biológicos, la ingeniería de biosistemas, la ingeniería bioinformática, etcétera. Se trata de un enfoque integrado de los fundamentos de las ciencias biológicas y los principios tradicionales de la ingenierías clásicas como la química o la informática.

Los bioingenieros con frecuencia trabajan llevando procesos biológicos de laboratorio a escalas de producción industrial. Por otra parte, a menudo atienden problemas de gestión, económicos y jurídicos. Debido a que las patentes y los sistemas de regulación (por ejemplo, la FDA en los Estados Unidos) son cuestiones de vital importancia para las empresas de biotecnología, los bioingenieros a menudo deben conocer estos temas.

Existe un creciente número de empresas de biotecnología, y muchas universidades de todo el mundo proporcionan programas en bioingeniería y biotecnología de forma independiente. Entre ellas, destacan las de la especialidad en ingeniería bioinformática.

Este es un campo interdisciplinario que se ocupa de los problemas biológicos usando técnicas computacionales propias de la ingeniería informática. Esa interdisciplinariedad hace que sea posible la rápida organización y análisis de los datos biológicos. Este campo también puede denominarse biología computacional, y puede definirse como "la conceptualización de la biología en término de moléculas y, a continuación, la aplicación de técnicas informáticas para comprender y organizar la información asociada a estas moléculas, a gran escala". La bioinformática desempeña un papel clave en diversas áreas, tales como la genómica funcional, la genómica estructural y la proteómica, y forma un componente clave en el sector de la biotecnología y la farmacéutica.

Entre las principales ventajas de la biotecnología se tienen:




La aplicación de la biotecnología presenta riesgos que pueden clasificarse en dos categorías diferentes: los efectos en la salud de los humanos y de los animales y las consecuencias ambientales. Además, existen riesgos de un uso éticamente cuestionable de la biotecnología moderna. (ver: Consecuencias imprevistas).

Entre los riesgos para el medio ambiente cabe señalar la posibilidad de polinización cruzada, por medio de la cual el polen de los cultivos genéticamente modificados (GM) se difunde a cultivos no GM en campos cercanos, por lo que pueden dispersarse ciertas características como resistencia a los herbicidas de plantas GM a aquellas que no son GM. Esto que podría dar lugar, por ejemplo, al desarrollo de maleza más agresiva o de parientes silvestres con mayor resistencia a las enfermedades o a los estreses abióticos, trastornando el equilibrio del ecosistema.

Otros riesgos ecológicos surgen del gran uso de cultivos modificados genéticamente con genes que producen toxinas insecticidas, como el gen del Bacillus thuringiensis. Esto puede hacer que se desarrolle una resistencia al gen en poblaciones de insectos expuestas a cultivos GM. También puede haber riesgo para especies que no son el objetivo, como aves y mariposas, por plantas con genes insecticidas.

También se puede perder biodiversidad, por ejemplo, como consecuencia del desplazamiento de cultivos tradicionales por un pequeño número de cultivos modificados genéticamente".

En general los procesos de avance de la frontera agrícola en áreas tropicales y subtropicales suelen generar impactos ambientales negativos, entre otros: procesos de erosión de los suelos mayor que en áreas templadas y pérdida de la biodiversidad.

Existen riesgos de transferir toxinas de una forma de vida a otra, de crear nuevas toxinas o de transferir compuestos alergénicos de una especie a otra, lo que podría dar lugar a reacciones alérgicas imprevistas.

Existe el riesgo de que bacterias y virus modificados escapen de los laboratorios de alta seguridad e infecten a la población humana o animal.

Los agentes biológicos se clasifican, en función del riesgo de infección, en tres grupos:

Los procesos de modernización agrícola, además del aumento de la producción y los rendimientos, tienen otras consecuencias.

La regulación nacional relacionada con la bioseguridad se había centrado en aspectos de prevención y control de posibles riesgos del uso y aplicación de OGMs para la salud humana, la sanidad vegetal y animal y el medio ambiente, aspectos en el ámbito de competencia de las Secretarías de Salud (SS), Secretaría de Agricultura, Ganadería, Desarrollo Rural, Pesca y Alimentación (SAGARPA) con base en la Ley General de Salud; Ley Federal de Sanidad Vegetal; Ley sobre Producción, Certificación y Comercio de Semillas y en la NOM-FITO-056. Por lo que respecta al ambiente, la Secretaría del Medio Ambiente, Recursos Naturales (SEMARNAT), se rige por la Ley General del Equilibrio Ecológico y la Protección al Ambiente y el reglamento en materia de impacto ambiental. Otras dependencias gubernamentales, relacionadas con los OGMs son la Secretaría de Hacienda y Crédito Público (SHCP), aplica la normatividad relacionada con el control sobre movimientos transfronterizos de bienes, aduanas, imposición tributaria, etc.; la Secretaría de Economía, responsable del comercio exterior, políticas comerciales, tratados internacionales; el IMPI, a cargo de los aspectos relativos a la propiedad industrial (patentes, marcas, etc.) y la Secretaría de Educación Pública (SEP) y el Consejo Nacional de Ciencia y Tecnología (CONACYT) indirectamente relacionadas estos dos últimos indirectamente con la bioseguridad al aplicar normas jurídicas vinculadas con la elaboración de políticas educativas y de investigación.

En el terreno específico de la bioseguridad de las actividades de la biotecnología moderna, la regulación vigente en el país requiere una revisión e integración sistematizada y armónica que le permita ser congruente con criterios internacionales, que cuente con los elementos operativos adecuados para darle eficacia gracias a la evaluación y al monitoreo de los riesgos biotecnológicos, que garanticen la seguridad jurídica de quienes realizan actividades de investigación, producción, comercialización y, en general, el manejo de los organismos genéticamente modificados y de los productos obtenidos de los mismos.

El 30 de abril de 2002, el Senado de la República ratificó el Protocolo de Cartagena sobre la Seguridad de la Biotecnología del Convenio sobre la Diversidad Biológica, que entró en vigor el 11 de septiembre de 2003, noventa días posteriores a la ratificación por 50 países. Si bien el origen y la naturaleza del Protocolo es ambiental, su contenido y la forma en que se asimile legalmente en nuestro país para su aplicación tendrá importantes repercusiones en la investigación, producción y comercialización de OGMs y de productos que los contengan, así como un efecto en la organización y participación de distintas autoridades gubernamentales. Además también es importante recordar que el Congreso de la Unión aprobó en diciembre de 2001, una modificación al artículo 420 Ter del Código Penal Federal, la cual pudiera traer por consecuencia que cualquier individuo, si maneja, utiliza o transporta transgénicos, puede incurrir en la comisión de un delito y, por lo tanto, ser sujeto de un procedimiento penal.

Con base en lo anterior, el Senado de la República en el 2002, solicitó a la Academia Mexicana de Ciencias (AMC) el apoyo técnico para la elaboración de la Iniciativa de la Ley de Bioseguridad de Organismos Genéticamente Modificados (ILBOGMs).





</doc>
<doc id="5095" url="https://es.wikipedia.org/wiki?curid=5095" title="DSP">
DSP

DSP puede referirse a:


</doc>
<doc id="5097" url="https://es.wikipedia.org/wiki?curid=5097" title="Microbótica">
Microbótica

Ya existen en el mercado aspiradores móviles que limpian estancias automáticamente. También se ven por los jardines más microbots cortacésped que recorren sin parar su superficie manteniendo la hierba a una altura fija. En fin cada vez hay más productos que contienen un microcontrolador y desarrollan tareas y trabajos que parece tienen algo de “inteligencia”. Los microcontroladores son circuitos integrados en cuyo chip se aloja un pequeño pero completo computador. Según el programa que ejecute procesará la información de entrada que le suministremos y generará unos resultados de salida que gobernará los actuadores del sistema. El microbot es un pequeño robot móvil gobernado por un microcontrolador y destinado a realizar sencillas tareas que frecuentemente las hace el ser humano. Limpia, vigila, corta el césped, transporta piezas, inspecciona tuberías, acompaña y ayuda a inválidos e incluso juega al fútbol.

Microbots nacieron gracias a la aparición del microcontrolador en la última década del siglo 20, y la aparición de sistemas mecánicos en miniatura en el silicio (MEMS), aunque muchos microbots no utilizan silicio para componentes mecánicos distintos sensores. La primera investigación y el diseño conceptual de este tipo de pequeños robots se llevó a cabo a principios de 1970 en la investigación (continuación) clasificada para las agencias de inteligencia de Estados Unidos. Aplicaciones previstas en ese momento incluyen prisionero de guerra y la asistencia de rescate misiones de intercepción electrónica. Las tecnologías de apoyo a la miniaturización subyacentes no se han desarrollado plenamente en ese momento, por lo que el progreso en el desarrollo de prototipos no estaba inmediatamente próxima de este conjunto inicial de los cálculos y diseño de concepto. A partir de 2008, los microrobots más pequeños utilizan un rasguño del accionamiento de ajuste. 

El desarrollo de las conexiones inalámbricas, especialmente Wi-Fi (es decir, en las redes domóticas) ha aumentado en gran medida la capacidad de comunicación de microbots, y en consecuencia su capacidad de coordinar con otros microbots para llevar a cabo tareas más complejas. De hecho, muchas investigaciones recientes se han centrado en la comunicación microrobot, incluyendo un enjambre 1,024 robot de la Universidad de Harvard que reúne en sí en varias formas; y la fabricación de microbots en el SRI Internacional para "MicroFactory para los productos de macro" de la DARPA programa que se puede construir de peso ligero, de alta estructuras -strength.




</doc>
<doc id="5100" url="https://es.wikipedia.org/wiki?curid=5100" title="Wifi">
Wifi

El wifi (escrito también wi fi) es una tecnología que permite la interconexión inalámbrica de dispositivos electrónicos. Los dispositivos habilitados con wifi (tales como ordenadores personales, teléfonos, televisores, videoconsolas, reproductores de música, etcétera) pueden conectarse entre sí o a Internet a través de un punto de acceso de red inalámbrica.

Wi-Fi es una marca de la Alianza Wi-Fi, la organización comercial que cumple con los estándares 802.11 relacionados con redes inalámbricas de área local. Su primera denominación en inglés fue Wireless Ethernet Compatibility Alliance.

El término "wifi", sustantivo común escrito normalmente en redonda (sin comillas ni cursiva), proviene de la marca comercial Wi-Fi. La WECA, el consorcio que desarrolló esta tecnología, contrató a una empresa de publicidad para que le diera un nombre a su estándar, de tal manera que fuera fácil de entender y recordar. Phil Belanger, miembro fundador de WECA, actualmente llamada Alianza Wi-Fi, apoyó el nombre "Wi-Fi":
La Wi-Fi Alliance utilizó el eslogan publicitario "The Standard for Wireless Fidelity" ("El estándar para la fidelidad inalámbrica") por un corto tiempo después de que se creara la marca y, aunque el nombre nunca fue oficialmente "fidelidad inalámbrica", como la Wi-Fi Alliance también se denominó "Wireless Fidelity Alliance Inc" - Alianza de la Fidelidad Inalámbrica incorporada - en algunas publicaciones y en la web del IEEE se define que "WiFi es un nombre corto para Wireless Fidelity" se puede concluir que Wi-Fi se entiende como abreviatura de Fidelidad Inalámbrica, para nombrar un conjunto de protocolos y hardware de red inalámbrica con inspiración mercadotécnica en el uso de Hi-Fi para "High Fidelity" ("Alta Fidelidad") para nombrar un conjunto de sistemas de audio de altas prestaciones.

Esta tecnología surgió por la necesidad de establecer un mecanismo de conexión inalámbrica que fuese compatible entre distintos dispositivos. Buscando esa compatibilidad, en 1999 las empresas 3Com, Airones, Intersil, Lucent Technologies, Nokia y Symbol Technologies se unieron para crear la "Wireless Ethernet Compatibility Alliance", o WECA, actualmente llamada Alianza Wi-Fi. El objetivo de la misma fue diseñar una marca que permitiese fomentar más fácilmente la tecnología inalámbrica y asegurar la compatibilidad de equipos.

De esta forma, en abril de 2000 WECA certifica la interoperabilidad de equipos según la norma IEEE 802.11b, bajo la marca Wi-Fi. Esto quiere decir que el usuario tiene la garantía de que todos los equipos que tengan el sello Wi-Fi pueden trabajar juntos sin problemas, independientemente del fabricante de cada uno de ellos.

En el año 2002, la asociación WECA estaba formada ya por casi 150 miembros en su totalidad. La familia de estándares 802.11 ha ido naturalmente evolucionando desde su creación, mejorando el rango y velocidad de la transferencia de información, su seguridad, entre otras cosas.

La norma IEEE 802.11 fue diseñada para sustituir el equivalente a las capas físicas y MAC de la norma 802.3 (Ethernet). Esto quiere decir que en lo único que se diferencia una red wifi de una red Ethernet es en cómo se transmiten las tramas o paquetes de datos; el resto es idéntico. Por tanto, una red local inalámbrica 802.11 es completamente compatible con todos los servicios de las redes locales (LAN) de cable 802.3 (Ethernet).

Existen diversos tipos de wifi, basados cada uno de ellos en un estándar IEEE 802.11. Son los siguientes:

Uno de los problemas a los cuales se enfrenta actualmente la tecnología wifi es la progresiva saturación del espectro radioeléctrico, debido a la masificación de usuarios; esto afecta especialmente en las conexiones de larga distancia (mayor de 100 metros). En realidad el estándar wifi está diseñado para conectar ordenadores a la red a distancias reducidas, y cualquier uso de mayor alcance está expuesto a un excesivo riesgo de interferencias.

Un elevado porcentaje de redes se instalan sin tener en consideración la seguridad, convirtiéndose así en redes abiertas (completamente accesibles a terceras personas), sin proteger la información que por ellas circulan. De hecho, la configuración por defecto de muchos dispositivos wifi es muy insegura ("routers", por ejemplo), dado que a partir del identificador del dispositivo se puede conocer la contraseña de acceso de este y, por tanto, se puede conseguir fácilmente acceder y controlar el dispositivo .

El acceso no autorizado a un dispositivo wifi es muy peligroso para el propietario por varios motivos. El más obvio es que pueden utilizar la conexión. Pero, además, accediendo al wifi se puede supervisar y registrar toda la información que se transmite a través de él (incluyendo información personal, contraseñas, etcétera). La forma de hacerlo seguro es seguir algunos consejos:

Existen varias alternativas para garantizar la seguridad de estas redes. Las más comunes son la utilización de protocolos de cifrado de datos para los estándares wifi como el WEP, el WPA o el WPA2, que se encargan de codificar la información transmitida para proteger su confidencialidad, proporcionados por los propios dispositivos inalámbricos. La mayoría de las formas son las siguientes:







La seguridad de una red wifi puede ser puesta a prueba mediante una auditoría de wifi. Sin embargo, no existe ninguna alternativa totalmente fiable, ya que todas ellas son susceptibles de ser vulneradas.

Existen varios dispositivos wifi, los cuales se pueden dividir en dos grupos: dispositivos de distribución o de red, entre los que destacan los enrutadores, puntos de acceso y repetidores; y dispositivos terminales, que en general son las tarjetas receptoras para conectar a la computadora personal, ya sean internas (tarjetas PCI) o bien USB.


En relación con los manejadores de dispositivo, existen directorios de circuito integrado auxiliar de adaptadores inalámbricos.

Las redes wifi poseen una serie de ventajas, entre las cuales se pueden destacar:



Aun así, como red inalámbrica, la tecnología wifi presenta los problemas intrínsecos de cualquier tecnología inalámbrica. Algunos de ellos son:





</doc>
<doc id="5101" url="https://es.wikipedia.org/wiki?curid=5101" title="Móstoles">
Móstoles

Móstoles es un municipio y una villa española de la Comunidad de Madrid. Con habitantes (INE ), es el vigésimo séptimo municipio más poblado del país y el , solo superado por la capital, Madrid.

El municipio se sitúa a 18 km al suroeste del centro de la ciudad de Madrid, en el área metropolitana de Madrid. Geográficamente se encuentra en el centro de la península ibérica y de la Meseta Central, en el valle del río Guadarrama perteneciente a la cuenca del Tajo.

Su cercanía a Madrid capital ha propiciado un acusado desarrollo demográfico en el transcurso de las últimas décadas. Móstoles, en cuarenta años y gracias al urbanismo salvaje, pasó de ser un núcleo rural de casi 4000 habitantes, a mediados de los años 1960, a una ciudad satélite de la capital con más de 200 000 en los comienzos del siglo , integrada dentro del área metropolitana de Madrid.

Aunque a partir de la década de los 1970 tenía un carácter eminente de ciudad dormitorio, Móstoles alberga el campus tecnológico y la sede central de la Universidad Rey Juan Carlos, así como el Aula Universitaria de la UNED y es uno de los Centros de Exámenes de la Jefatura Provincial de Tráfico. La primera base operativa de las BESCAM (Brigadas Especiales de Seguridad de la Comunidad Autónoma de Madrid) se ubicó también en esta localidad en 2004, tras un convenio suscrito con el gobierno regional.

El topónimo es de origen incierto. Se ha propuesto como posibilidad que pudiera tener alguna relación con el término latino "monasterium" (que para llegar al topónimo actual tendría que partir del ablativo plural "monisteriis") que aludiría a una posible presencia de uno o más monasterios previos a la conquista musulmana.

El escudo, acabado en pico, está dividido verticalmente en dos partes. La izquierda se divide a su vez horizontalmente; en la parte superior, con fondo de oro, una Virgen María con el Niño Jesús en sus brazos, sostenida por una luna creciente de plata y rodeada por una letra "D" mayúscula de gules. En la parte inferior la invocación en latín: "Tota pulchra est", seguida del anagrama de María en letras de oro sobre gules.

El cuartel derecho se compone de cuatro fajas de gules (fondo rojo) y respectivamente llevan cargadas en letra de oro las sílabas “ON”, “PHE”, “LIPE”, y “II” aunque históricamente esta cuarta sílaba se representa en el filete de planta. Junto con la 'D' de la parte izquierda forma el texto Don Phelipe II en referencia al monarca que le otorgó el privilegio de villazgo, segregándola de Toledo. Cada faja de gules lleva abajo un filete de planta, en oro sobre azur.

El blasón concreto, que corresponde al acuerdo de rehabilitación del escudo heráldico el 29 de abril de 2004 es el siguiente:
Finalmente, encima se coloca la Corona Real de España, y tiene una banda que lo rodea con la inscripción “Excelentísimo e Ilustrísimo Ayuntamiento de la Villa de Móstoles”, títulos otorgados respectivamente por los reyes Alfonso XIII en 1908 y su padre Alfonso XII en 1882.

De diseño muy reciente (1 de abril de 2004), la bandera de Móstoles incluye los siguientes colores: el rojo carmesí, que simboliza la pertenencia a la Comunidad de Madrid y que a su vez representa su pertenencia a Castilla; el azul Prusia y el amarillo-gualda (oro). Sobre la franja central, a un tercio del asta, se sitúa el escudo municipal de la villa.

La descripción textual de la bandera es la siguiente:

La localidad se ubica 17 km al suroeste de Madrid y forma parte del área metropolitana de la capital de Madrid. Concretamente, la salida correspondiente a Móstoles desde la A-5 es en el kilómetro 16,3.

El proyecto independiente AUDES5, que define una serie de subáreas metropolitanas en Madrid, considera a Móstoles cabecera de una de estas subáreas, la cual engloba a localidades limítrofes como Alcorcón, Navalcarnero, Moraleja de Enmedio o Arroyomolinos y cuenta con una extensión de 315,1 km² y una población superior a los 430 000 habitantes.

Su término municipal limita, al norte y oeste con el de Villaviciosa de Odón, al noreste con el de Alcorcón, al este con el de Fuenlabrada, al sur con los de Arroyomolinos y Moraleja de Enmedio y al suroeste con el de Navalcarnero.


Móstoles se encuentra en el valle del río Guadarrama. Este río constituye el límite occidental de la localidad. Además de este cauce, destacan el arroyo de Los Combos y el arroyo del Soto, que recorren de norte a sur y este a oeste, respectivamente, el término municipal.

La localidad goza de un clima mediterráneo continentalizado muy parecido al de Madrid por su proximidad; sus temperaturas medias máximas son 32 °C en verano y 11 °C en invierno, y sus mínimas 16 °C en verano y 2 °C en invierno. La precipitación anual media es de aproximadamente 447 mm, las épocas más lluviosas son la primavera, el otoño y el comienzo del invierno aunque hay una gran variabilidad de un año a otro.

Pese a que el desarrollo de Móstoles como gran ciudad es muy reciente (último tercio del siglo ), la zona ha permanecido habitada (aunque se desconoce si con continuidad) desde épocas remotas.

El curso bajo del arroyo del Soto es rico en hallazgos arqueológicos; además de restos fósiles de animales primitivos, se han encontrado indicios de la presencia de humanos prehistóricos, tales como armas y utensilios de sílex de época paleolítica.

La fundación de Móstoles como asentamiento estable podría datar de la época de la dominación romana, o incluso ser anterior, a juzgar por la aparición de fíbulas de bronce celtibéricas en la mencionada zona.

Durante la época romana imperial pudo haber un núcleo urbano en el actual centro de la localidad, donde se han documentado una necrópolis, un horno de cerámica y otros restos. La Carta Arqueológica del municipio incluye otros restos encontrados en prospecciones en las vegas de los arroyo del Soto y de la Reguera. Los primeros mostoleños con nombre conocido pudieron ser Fortunata y Flavianus y pudieron haber vivido en la segunda mitad del s. II d. C. La lápida encargada por la primera en honor de su difunto marido apareció en 2002 en la zona del arroyo del Soto y, aunque esta se encontró fuera de su contexto original, lo más probable es que su primer emplazamiento fuera muy cercano. El arqueólogo y profesor de historia local Jesús Rodríguez Morales considera que, además de un poblamiento desde época romana, Móstoles debió ser encrucijada de importantes vías terrestres.

Durante la dominación visigoda e islámica Móstoles o sus alrededores permanecieron poblados. Las pruebas se corresponden sobre todo con restos de cerámica de dichas épocas, halladas en diversos puntos, y en especial en el centro urbano, donde también se han hallado hasta siete viajes de agua subterráneos que datan probablemente de época islámica.

La primera referencia documental segura de Móstoles data de la Edad Media. Un documento del año 1144, que señala que el rey Alfonso VII donó la aldea de Freguecedos (luego Fregacedos, ubicada en el emplazamiento de la actual urbanización Loranca en Fuenlabrada) al obispo de Segovia, menciona a "Turrem de Monsteles" como población próxima.

Para el siglo Móstoles era un nudo de comunicaciones cuya importancia radicaba en ser la encrucijada de varias vías importantes. La actual disposición radial de las principales calles de la localidad es un vestigio de estas rutas. En 1565 Móstoles se independizó de Toledo, comprando su propia jurisdicción al monarca Felipe II.

En este siglo también se produjo la aparición de una talla de la Virgen, por la cual se construyó la ermita de Los Santos. La presencia de viajeros y mercaderes propiciaba que la actividad hostelera fuera importante, con varios mesones, posadas y ventas. Las tabernas contaban además con el atractivo de los "órganos de Móstoles", un pintoresco sistema ideado por los lugareños para preservar fresco el vino y que da nombre a una zarzuela del siglo escrita por Juan de Alba. En aquella época el concejo mostoleño tenía que surtir de paja, pan y cebada a la Corte y además exportaba gran cantidad de vino y aceite. Móstoles era cabeza del arciprestazgo de Canales, que englobaba 57 pueblos y aldeas.

En el siglo , la Guerra de Sucesión Española afectó negativamente al pueblo. Las políticas viarias de los Borbones dispusieron que todas las rutas que atravesaban Móstoles fueran abandonadas para pasar por la capital, quedando únicamente el Camino Real de Extremadura (que salía de Madrid) como vía importante que pasara por el pueblo.

El 2 de mayo de 1808 se redactó el llamado Bando de los alcaldes de Móstoles, que se considera popularmente una declaración de guerra contra los franceses que inició la Guerra de la Independencia. En realidad, este fue redactado por el aristócrata Juan Pérez Villaamil (los alcaldes Andrés Torrejón y Simón Hernández solo lo firmaron), y estaba destinado a avisar a los pueblos de la carretera de Extremadura y de su entorno para que acudiesen a socorrer al pueblo de Madrid, y llamaba al levantamiento general contra los franceses, usurpadores del trono español.

En 1891 se inaugura el Ferrocarril Madrid-Villa del Prado, que pasaba por Móstoles y que llegó finalmente, en 1901, hasta Almorox (Toledo). Parte de esta antigua línea de ferrocarril de vía métrica (1000 mm), clausurada en 1970, daría origen a la actual línea C-5 de Cercanías de RENFE.

En 2008 el Ayuntamiento recibió el Premio Bioenergía 2008 de Oro y en 2010 recibe la "Escoba de Oro" por el soterramiento de los contenedores de recogida de envases y fracción orgánica mejorando así la calidad ambiental del municipio. En 2016 se produjo una alteración del término municipal, tras segregarse una superficie de m² dividida en tres zonas para incorporarla al término municipal de Fuenlabrada y, a su vez, agregarse a Móstoles una superficie de m² dividida en 7 zonas y perteneciente hasta entonces a Fuenlabrada.

La ciudad de Móstoles creció rápida y desordenadamente para dar alojamiento a los emigrantes que venían a Madrid desde zonas rurales de Extremadura, León, las dos Castillas y Galicia, entre otras regiones. Su población se estabiliza en los 90, pero, a partir de finales de esa década vuelve a crecer con la llegada de una nueva inmigración (desde África y Europa del Este, especialmente). Actualmente la tasa de inmigración (ciudadanos de nacionalidad no española) supera el 13% de la población.

El municipio, que tiene una superficie de 45,36 km², cuenta según el padrón municipal para del INE con habitantes y una densidad de  hab./km². La mayor parte de la población se agrupa en torno al casco urbano de Móstoles; existen otros núcleos menos poblados como Parque Coímbra y Colonia Guadarrama. En lo relativo a la inmigración, en el año 2013 habría nacido fuera de España 34 250 personas INE 2013), lo que representaba el 16,59% de la población y 28 436 tenían nacionalidad extranjera (13,77% de la población municipal). Como se puede apreciar en el gráfico adjunto, el crecimiento de la población fue vertiginoso en los años 70 y 80, como consecuencia de la emigración desde las áreas rurales hacia las grandes ciudades.

El gentilicio más usado es mostoleños/as, aunque también son correctos mostolenses, y mostoleros/ras aunque este está en desuso.

Desglose de población según el Padrón Continuo por Unidad Poblacional del INE.

Desde 2000, Móstoles se organiza en distritos, los cuales cuentan con órganos de participación vecinal propios llamados Juntas de Distrito. Estos fueron en un principio cuatro, pero en 2004 la corporación municipal decidió escindir las urbanizaciones de Parque Coímbra y Colonia Guadarrama del Distrito Oeste.







Durante ocho años, entre 1995 y 2003, gobernó José María Arteta, del Partido Socialista Obrero Español, en coalición con Izquierda Unida, aunque el PP era el partido con mayor número de concejales desde 1995. Le sucedería Esteban Parro, del PP, alcalde entre 2003 y 2012. Daniel Ortiz Espejo (PP) ejerció como alcalde tras la renuncia de Parro para dedicarse en exclusiva a su labor en el Senado.

En 2015 entró en la alcaldía David Lucas Parrón, del PSOE de Madrid, que gobernó con el apoyo de Ganar Móstoles e IUCM-LV. En enero de 2018 renunció al cargo, que pasó a manos, de forma interina, de la primera teniente de alcalde, Jessica Antolín Manzano, también del partido socialista, hasta la elección de Noelia Posse como alcaldesa el 7 de febrero de 2018.

El partido ganador en estas elecciones fue Partido Socialista Obrero Español (PSOE) con 10 escaños, en segunda posición ha quedado el Partido Popular (PP) con 6 escaños, en tercera posición Ciudadanos-Partido de la Ciudadanía (Cs) con 5 concejales, cuarta posición para Más Madrid-Ganar Móstoles (MMGM) con 2 concejales, quinta posición para Vox (Vox) con 2 concejales y el último partido con representación en el ayuntamiento es Podemos (Podemos) con otros 2 concejales. (El orden de los partidos en con el número obtenido en votos en las elecciones municipales 2019 )

El concepto de deuda viva contempla solo las deudas del Ayuntamiento de Móstoles con cajas y bancos relativas a créditos financieros, valores de renta fija y préstamos o créditos transferidos a terceros, excluyéndose, por tanto, la deuda comercial.

La deuda viva municipal por habitante en 2014 ascendía a 588,37 €, siendo en 2017 de 528,67 €.

Aunque durante las tres últimas décadas Móstoles ha sido ciudad dormitorio de Madrid, actualmente cuenta con varios e importantes polígonos industriales, un parque tecnológico y la Plataforma Logística Puerta del Atlántico (aún en construcción). Los polígonos más destacados son Arroyomolinos, La Fuensanta, Regordoño, Los Rosales, Las Nieves y Expansión. También existen zonas industriales dispersas: Las Pajarillas, Las Monjas y Móstoles Industrial. En la primera década del siglo comenzó la construcción el polígono industrial de La Fuensanta II. En casi todos los casos, el tejido industrial está formado por medianas y pequeñas empresas y la actividad económica predominante es la electro-sidero-metalúrgica, pero también son relevantes la industria textil y la papelera.

Móstoles cuenta con una delegación municipal de promoción económica y empleo, desde 2015 llamada "Móstoles Desarrollo", cuyo objetivo es facilitar el asentamiento de industrias en el municipio. En este sentido, se considera positivo para el municipio la puesta en funcionamiento de la Universidad Rey Juan Carlos y la implantación del Centro de Investigación y Desarrollo Tecnológico de Repsol. Este está integrado en un parque industrial llamado Móstoles Tecnológico.

La asociación de empresarios más representativa en el municipio es Móstoles Empresa. Una asociación global que integra a autónomos, jóvenes, emprendedores, grandes empresas, PYMES, comercios, etc. Es un proyecto renovado de AJE Móstoles (Asociación de Jóvenes Empresarios de Móstoles). La pequeña y mediana empresa (pyme) es predominante dentro del comercio local: tiendas de barrio y supermercados. Hay una gran superficie: Carrefour, pero, paradójicamente, la localidad está rodeada de grandes centros comerciales ubicados en los municipios vecinos. En el término de Arroyomolinos, pero a tan solo 250 metros de la urbanización Parque Coimbra de Móstoles, se encuentra el centro comercial Madrid Xanadú (que incluye el mayor parque de nieve artificial de Europa). En Alcorcón están las zonas de Parque Oeste y Tres Aguas.

El "Centro Comercial Abierto Las Avenidas" es un conjunto de calles comerciales que se encuentra en el centro de la localidad. Se articula en torno a la avenida del Dos de Mayo y la avenida de la Constitución. La creación de este "centro comercial abierto" se engloba dentro del esfuerzo del comercio local y de las autoridades municipales para mantener la actividad minorista, frente a la proliferación de grandes superficies comerciales en los alrededores. Además de Carrefour, se encuentra el Centro Comercial La Fuensanta, con una superficie de 24000 m², en el que encontramos el supermercado Mercadona y el gimnasio Virgin Active. Otras grandes superficies son los centros comerciales Ecomóstoles y Dos de Mayo.

Móstoles cuenta también con un Centro de Investigación y Desarrollo Tecnológico de Repsol, el Parque Empresarial Móstoles Tecnológico y el Instituto IMDEA Energía.

En Móstoles se encuentra el CA2M Centro de Arte Dos de Mayo. El edificio, de más de 5800 m² de superficie, se encuentra en el centro de Móstoles y se levanta sobre la fachada de La Casona, un inmueble tradicional del siglo que ha sido rehabilitado. Su colección, de más de 1300 obras, consta principalmente de obras de fotografía, pintura y escultura de artistas españoles nacidos a partir de la década de 1940. Desde mayo de 2014, también alberga la Colección Fundación ARCO, compuesta por 300 obras de 224 artistas contemporáneos. Además de la exposición de sus fondos, el CA2M lleva a cabo otras actividades no expositivas como conferencias, proyecciones, debates, talleres, seminarios, cursos y actuaciones musicales. También cuenta con salas dedicadas al estudio y la creación artística.

El Museo de la Ciudad es un nuevo recurso cultural pensado para proporcionar un mejor conocimiento de Móstoles, el aprendizaje y la participación de sus visitantes. El Museo se ubica en la antigua Casa de Postas, inmueble de estilo neomudéjar. La Casa-Museo de Andrés Torrejón es la casa donde vivió y murió el famoso alcalde de Móstoles. Una pequeña casa de época, decorada con elementos del siglo de estilo rústico.

La Biblioteca Municipal de Móstoles tiene seis puntos de servicio (biblioteca central y cinco sucursales de barrio). Para obtener información acerca de la ubicación, fondos, servicios ofertados y actividades.

El Teatro del Bosque, inaugurado en 2003, es uno de los edificios más representativos de la localidad. Está levantado con vistas al Parque Finca Liana. Su jardín perimetral, caracterizado por parterres muy verticales, fue diseñado por Javier Mariscal. Este teatro alberga los espectáculos de teatro, danza y música promovidos por el Ayuntamiento. Tiene un aforo para 646 espectadores y un escenario con una superficie de 342 metros cuadrados; asimismo, el foso de orquesta tiene capacidad para unos 50 músicos. Las instalaciones se completan con una cafetería, seis camerinos, cabinas de traducción simultánea, que suman una superficie de 3000 metros cuadrados distribuidos en tres plantas.

Existe una gran comunidad de teatro amateur en la localidad, de hecho, la Confederación Estatal de Teatro Amateur ESCENAMATEUR inició sus primeros pasos en esta localidad. Sus principales focos son el Centro Sociocultural El Soto, Centro Sociocultural Caleidoscopio, los institutos municipales y la Universidad Rey Juan Carlos. Una de las compañías de teatro más relevante de la ciudad es Melpómene Teatro, dirigida por el mostoleño Alejandro Cavadas, que lleva desde el año 2004 haciendo un trabajo teatral desde y para sus vecinos, acumulando más de treinta premios nacionales.

También existen centros de juventud y centros culturales (Villa de Móstoles, El Soto, Caleidoscopio, Joan Miró y Norte-Universidad).

La localidad acoge, desde 2005, un festival de cine, impulsado por el ayuntamiento, en colaboración con la Universidad Rey Juan Carlos y el gobierno de la Comunidad de Madrid. El evento, llamado Festival Internacional de Cine Madrid Móstoles, se celebra en el Teatro del Bosque e integra ciclos, sección oficial con las películas que entran a concurso y actividades paralelas a precio reducido (2,40 euros en la edición de 2005).

Ocho ediciones del Festimad, un festival de música rock, desde 1996 hasta 2004, tuvieron lugar en el Parque del Soto de Móstoles. Tras la decisión del ayuntamiento de Móstoles de no celebrar más eventos musicales en el Parque del Soto, tras la llegada del Partido Popular al Ayuntamiento y por el temor a que la multitud que se citaba allí cada año a disfrutar de los conciertos y acampar pudiese dañar el entorno natural, el festival se trasladó en 2005. Desde entonces se celebró en otras localidades del sur de Madrid, pese a que la intención del consistorio mostoleño era mantener el evento en la localidad pero en otro espacio. En 2017 volvió a celebrarse en Mostóles.

Cuentan con una edición local para Móstoles las siguientes publicaciones: "El Cuaderno de Móstoles" (nace en junio de 2012 hasta la actualidad), "Sur Madrid "(el más veterano a fecha de 5 del 2010 con 15 años de antigüedad en Móstoles), "Voces de Pradillo" (fundado en mayo de 2012), "Mercado Móstoles" (cierra como periódico en el 2008), "Información al día" (cierra en el 2008), "Paso a paso" (cierra en el 2008), "El Universo de Móstoles" (cierra en el 2008), "Domingol" (abierto en la actualidad 5 del 2010), "El Iceberg" (abandona su cabecera en Móstoles en el 2008)...

"SER Madrid Oeste" 102.3 FM (antes llamada SER Móstoles y SER Suroeste). Existió hasta 2002 una emisora municipal con el nombre "Onda Móstoles" 96.2 FM. Actualmente, a través de este dial realiza sus emisiones "Móstoles FM", emisora local, de cuya gestión se encarga el Ayuntamiento de Móstoles. Una radio de información y entretenimiento. Los estudios se encuentran en el Polígono Arroyomolinos de Móstoles, en las instalaciones de la Empresa de Promoción Económica (EMPESA).

Desde los 90 ha habido varias emisoras locales de televisión, la mayoría proyectos privados que no han tenido continuidad. Por orden cronológico fueron "CTM", "Telemóstoles", "Móstoles Televisión" (de titularidad pública), "Canal 30 Móstoles" y "Localia Móstoles", que cesó sus emisiones tras el desmantelamiento de la empresa llevado a cabo por el Grupo PRISA. En el impas entre el cierre de Localia y la concesión de las licencias de TDT nace una televisión local por Internet en el 2004 Sur-Madrid TV que posteriormente en el 2008 se convierte en el primer canal de TDT en dar contenidos Locales con una extensa programación más de 100 horas semanales en información local del sur de Madrid. "CTM", primero (1992-1993) y, después, "Telemóstoles" (1994-1996), tuvieron una trayectoria parecida, con unas 6 horas de emisión al día y audiencias aceptables dada la falta de medios. Trayectoria que, en ambos casos, se vio interrumpida a los dos años por diferentes problemas: pocos ingresos, fuga de profesionales y actuaciones legales promovidas por el Ayuntamiento dado que las dos emisoras carecían de licencia. 
Tras la elección del socialista José María Arteta como alcalde, en 1995, se aprobó la creación de un Instituto municipal de Comunicación, con emisoras propias de titularidad pública en radio y televisión. La emisora "Móstoles Televisión" comenzó a emitir en 1996, con novedosos equipamientos digitales. El proyecto, una vez más, no fue rentable y no tuvo continuidad más allá del cambio de siglo. La cadena pública convivió con una nueva iniciativa privada, "Canal 30 Móstoles", una emisora ambiciosa que alcanzó un notable reconocimiento. Sin embargo, con el paso del tiempo "Localia", la marca de televisión local impulsada por el Grupo PRISA, tomó el control de la cadena hasta 2009, año en el que, con una muy limitada producción propia, el canal desapareció con la disolución de la compañía. En mayo de 2010, existían tres canales de TDT (Televisión Digital Terrestre) LD Sur-Madrid, Popular TV y 8madrid, de los cuales solo LD Sur-Madrid proporciona información local, con una apuesta indudable por este tipo de información y sede en las poblaciones de cobertura.

Una de los periódicos papel veteranos puso en marcha en 1996 una web de información local. La web oficial del Ayuntamiento ofrece información corporativa y ciudadana.

En lo que a equipos profesionales se refiere, tras la desaparición del FS Móstoles, en la élite deportiva permanece el Fútbol Sala Femenino Móstoles, el equipo de atletismo (Club Asociación Atlética de Móstoles), el equipo de natación (Agrupación Deportiva Natación Móstoles) y todo lo relacionado con artes marciales.

En 1955 se fundó el C.D. Móstoles, desaparecido en 2012 por problemas económicos. Vestía íntegramente de azul y jugaba sus partidos en el campo del Soto. En dos ocasiones logró ascender a Segunda División B, por lo que la Tercera División es su categoría habitual.

El club de fútbol más representativo es el CD Móstoles URJC, originalmente, fundado en 1996 como C.D. Juventud Móstoles-El Soto, que, con 57 equipos y más de 900 jugadores, es el segundo club más grande de la Comunidad de Madrid y por extensión, uno de los más grandes de España. Su primer equipo compite actualmente en el grupo VII de la Tercera División Nacional, tras haber conseguido el ascenso proclamándose campeón de Grupo y absoluto de la Preferente madrileña. El filial del CD Móstoles URJC compite en la Regional Preferente, grupo segundo. El CD Móstoles URJC representa el fútbol mostoleño en las categorías más altas del fútbol autonómico, militando su Juvenil 'A', en Liga Nacional, Cadete 'A' en Primera División autonómica, el Infantil 'A' en División de Honor (campeón de la misma en la 2015/2016), etc. Recientemente se ha renovado la concesión del Estadio Municipal El Soto, propiedad del ayuntamiento, por diez años más.

En Preferente también compiten otros equipos como el Móstoles CF, fundado en 2013 representando a la antigua directiva con la que quebró el CD Móstoles que juega en el campo Andrés Torrejón, Móstoles Balompié. Otros equipos de fútbol son el Internacional de Móstoles, el Fátima, Entiergal, el C.F. Torremar... que disputan sus encuentros en los campos Iker Casillas.

En Fútbol Sala destaca el equipo Fútbol Sala Femenino Móstoles fundado en 1988 y que desde 1995 juega en 1ª División nacional, donde es el club con más temporadas disputadas, su palmarés suma 2 Campeonatos de Liga (2000-01 y 2005,06), 4 Copas de España (2000, 2010, 2011 y 2012) y 2 Supercopas (2006 y 2010); disputa sus partidos en el Pabellón del Polideportivo Villafontana.

En el apartado relacionado con el baloncesto está el Club Baloncesto Ciudad de Móstoles, que destaca por su política de cantera y con presencia en la Liga Nacional.
Relacionado con el balonmano está el Club Balonmano Móstoles, el más antiguo de la zona Oeste de Madrid, fundado en 1989, cuyo equipo femenino en varias ocasiones ha militado en División de Honor Plata nacional, la última vez en 2018, recientemente reestructurado hacia una política basada principalmente en la cantera; disputa sus partidos en el Pabellón del Polideportivo de Villafontana.

El patinaje artístico sobre ruedas está representado en la Ciudad por el Club de Patinaje Artístico Móstoles (CPA Móstoles). Fundado en 1992, cuenta con una trayectoria plagada de éxitos deportivos, entre los que destacan la participación de sus patinadores en campeonatos autonómicos y nacionales, habiendo sido el más reciente su participación en el XXV Campeonato de España Sénior / Júnior 2017, en la categoría sénior femenino. En la actualidad cuenta con 166 deportistas entre patinadores federados, cantera, escuela municipal y colegios. En 2017 celebrará el 25 aniversario.

Existen los campos de fútbol Iker Casillas, el Estadio Municipal "El Soto", los Polideportivos Andrés Torrejón, Los Rosales y Villafontana, las piscinas de verano de El Soto y Villafontana y las piscinas cubiertas de Las Cumbres, Villafontana y El Soto. Además de algunas áreas de deportes en distintos barrios.

El Centro Deportivo Andrés Torrejón tiene capacidad para 6700 espectadores. Se encuentra paralizada su construcción a la espera de que mejoren las finanzas municipales. 
El Estadio Municipal El Soto alberga al equipo de fútbol de la ciudad, CD Móstoles URJC y tiene una capacidad para 14 000 asistentes. Existen además diversos polideportivos: Villafontana, Los Rosales, La Loma, Parque Coimbra y Río Guadiana.

En Móstoles hay 24 guarderías (10 públicas, de ellas cuatro de la Comunidad de Madrid y dos de gestión privada, y 14 privadas), 36 colegios públicos de educación infantil y primaria, 18 institutos de educación secundaria y 6 colegios privados (con y sin concierto). En 2013, debido a los recortes por la crisis y la subida de tasas impuesta por el gobierno municipal, las escuelas infantiles públicas perdieron parte del alumnado y como un tercio de su personal. Otras dotaciones de Móstoles destinadas a la educación son una Escuela Oficial de Idiomas y un colegio público de educación especial.

En el Campus de Móstoles se encuentra la sede central de la Universidad Rey Juan Carlos. Pertenecen a este campus la Escuela Superior de Ciencias Experimentales y Tecnología (ESCET) y la Escuela Técnica Superior de Ingeniería Informática (ETSII). Entre las instalaciones se encuentran un edificio para el Rectorado (rector, vicerrectores, PAS, salón de actos, ampliación de rectorado), un edificio de Gestión, tres aularios y tres laboratorios, dos departamentales, el CAT (Centro de Apoyo Tecnológico) y una Biblioteca Central. Se accede por carretera por la A-5 E-90 (salida 14 Móstoles DGT- Universidad), la línea 12 del Metro de Madrid (Universidad Rey Juan Carlos), en la Línea C-5 de Cercanías (Móstoles El Soto) o en autobús (522, 525, 526 y 529H).

Móstoles cuenta con Aula Universitaria de la Universidad Nacional de Educación a Distancia (UNED), integrado en el Centro Asociado Madrid Sur. La presencia de la UNED en Móstoles se remonta al curso 1979-80. Sus instalaciones están ubicadas en el IES Velázquez, que dispone de quince aulas, biblioteca, secretaría y sala de informática. En el Centro Madrid Sur hay más de 8000 alumnos, unos 2000 matriculados en el Aula de Móstoles. La UNED imparte en Móstoles tutorías de: Curso de Acceso a mayores de 25 y 45 años, Turismo (Fuenlabrada y Móstoles), Educación Social (Móstoles, Parla), Ingeniería en Técnica de la Información (Móstoles), Ingeniería Informática (Móstoles), Geografía e Historia (Móstoles), Pedagogía (Móstoles, Parla y Valdemoro), Antropología Social y Cultural (Móstoles) y Psicología (Móstoles y Parla).

El Conservatorio Rodolfo Halffter se creó en 1986 como conservatorio de Grado Elemental. En el año 1995, la Comunidad de Madrid otorgó una autorización al centro para impartir así mismo las enseñanzas correspondientes al Grado Medio, circunscrita a los alumnos matriculados en el conservatorio. Por estas fechas, el conservatorio estrenó además un edificio propio en la zona del llamado Cuartel Huerta, en el centro de la localidad, que cuenta con una biblioteca especializada, una sala de cámara y un auditorio con capacidad para 170 espectadores.

Desde 1998, una nueva autorización permite el acceso de cualquier estudiante de música, previa superación de las pruebas de selección correspondientes y en junio de 2002, se establece definitivamente como Conservatorio de Grado Medio, gracias a un Convenio suscrito con la Comunidad de Madrid. Ha participado en importantes actos como una gala benéfica en beneficio del "Centro de Promoción Femenina de Ségou" (Mali), o el acto de entrega de medallas de la Comunidad de Madrid, celebrado el 2 de mayo del año 2007.

En 1983 se inauguró el Hospital Universitario de Móstoles, de gestión pública y perteneciente al Servicio Madrileño de Salud. Este hospital en origen atendía a Móstoles y Alcorcón, así como a municipios cercanos, tenía una población de referencia de 400 000 personas y contaba con una dotación de 400 camas. En 1998 se abrió el Hospital Fundación Alcorcón, uno de los primeros en modelo de fundación (existen otros hospitales con este modelo en Verín y en Mallorca) en una empresa pública sanitaria. La apertura de este centro sanitario en Alcorcón supuso el cierre del Hospital Hermanos Laguna que se encontraba en Alcorcón, un hospital de 60 camas que contaba con las especialidades de Medicina Interna, UCI/Coronarias y Cardiología. En el Hospital Universitario de Móstoles se integraron los profesionales que hasta entonces habían trabajado en el centro Hermanos Laguna.
Tras la inauguración de la Fundación Hospital Alcorcón en 1998 el Área Sanitaria 8 queda dividida en dos con una población de referencia de aproximadamente de 200 000 habitantes cada una. Con la apertura en 2012 del Hospital Rey Juan Carlos el número de pacientes a atender es de 155 000 personas. Esto le ha permitido reducir del número de camas y así conseguir que sus habitaciones sean individuales en muchos de sus servicios. Por ejemplo en Obstetricia, todas las habitaciones cuentan con una cama (para la madre) y un sofá-cama para el acompañante.

En 2012 se inauguró el Hospital Universitario Rey Juan Carlos que gestiona la empresa privada IDC-Capio, en una concesión de treinta años, si bien el edificio pertenece al Servicio Madrileño de Salud. La diferencia fundamental entre este modelo y el público es la diferente financiación que recibe de la administración (Consejería de Sanidad). Este centro recibe una cantidad económica por cada paciente asignado; y esa cantidad le es descontada si el paciente elige otro centro distinto. Sin embargo en la práctica su presupuesto ha aumentado en su segundo año de existencia; mientras que el del hospital público se ha reducido en 5 millones de euros. Otra de las diferencias fundamentales entre los sistemas es que algunos de los profesionales a los que da empleo el centro privado tienen sueldos más bajos que en el hospital público; en algunos casos hasta un 15% menos, con menos días libres de descanso.

Existen también el Hospital HM Universitario Puerta del Sur —inaugurado en noviembre de 2014, un centro privado perteneciente al grupo HM Hospitales— y el Centro Base de minusválidos, gestionado por la Comunidad de Madrid.




El vertiginoso crecimiento de Móstoles ha hecho necesario reforzar las infraestructuras de transporte a lo largo de los últimos años para evitar un colapso circulatorio. La construcción de la autovía de circunvalación M-50 y de la línea de Metro de Madrid número 12 (Metrosur) son dos de las principales actuaciones llevadas a cabo en este sentido a lo largo de la pasada década. Ambas vías conectan entre sí núcleos periféricos de la zona metropolitana y, a su vez, sirven para facilitar el acceso a Madrid de forma indirecta.

A continuación se enumeran las principales vías de comunicación de la localidad:


Existen cinco estaciones de la línea 12 (o Metrosur) de Metro de Madrid:


La línea C-5 (Móstoles-Atocha-Fuenlabrada/Humanes) de tren de Cercanías de Madrid cuenta con dos estaciones: Móstoles (Móstoles Central para el metrosur) y Móstoles-El Soto.

Móstoles cuenta con 22 líneas de autobuses que la conectan con las localidades limítrofes y con la capital. Entre ellas hay nueve rutas a Madrid (521, 522, 523, 524, 528, 534 y 539 y las nocturnas N501 y N503), seis a Navalcarnero, dos a Fuenlabrada, dos a Villaviciosa de Odón y una a Alcorcón.

Estas líneas son, en su mayoría, explotadas por la Arriva-DBlas, radicada en Alcorcón y de titularidad privada. Esta misma compañía ostenta la concesión de las seis líneas locales de Móstoles, numeradas del 1 al 6 y con distintivo urbano rojo. Los vehículos son apodados cariñosamente "las Blasas" en alusión a la familia De Blas, que fundó la empresa que presta este servicio.

Además, Móstoles es el centro de numerosas rutas interurbanas hacia la zona suroeste de Madrid y norte de Toledo.

En la localidad hay 12 paradas de auto-taxi, con una capacidad total para 63 vehículos.

Está situado a unos 35 km del aeropuerto de Madrid-Barajas. El aeropuerto de Cuatro Vientos está a unos 9 km.

Uno de los inmuebles con valor arquitectónico es la iglesia de Nuestra Señora de la Asunción, el edificio más antiguo de Móstoles. Se conservan un ábside semicircular de estilo mudéjar y una torre del mismo estilo. Se encuentra en el número 1 de la plaza Ernesto Peces.

La ermita de Nuestra Señora de los Santos consta de una sola nave y tiene un retablo del siglo (restaurado en 2005), a cuyos lados están las imágenes modernas del Sagrado Corazón de Jesús y María. Contiene también espejos del siglo , cornucopias, cuadros de marcos tallados y un gran decorado. La Virgen de los Santos (patrona de Móstoles) está situada en un camarín. Existe también una figura de barro que representa a San Juan y el Niño Jesús, de la escultora Luisa Roldán "la Roldana".
En la plaza del Pradillo y alrededores se encuentran las principales esculturas y fuentes. Entre ellas se encuentra la Fuente de los peces —con dos peces de bronce, de cuya boca mana el agua—, inaugurada en la primavera de 1852. También se ubica cerca el monumento a Andrés Torrejón, inaugurado en 1908 por el rey Alfonso XIII, con motivo de cumplirse el primer centenario del levantamiento del Dos de mayo de 1808 en Madrid y el bando de los alcaldes de Móstoles. El monumento tiene una base de granito sobre la que reposa un trozo de montaña de roca de Segovia que representa los Pirineos. Por encima del peñón, traspone un águila imperial representando a Francia que, con una de sus garras, pretende arrancar la corona real del escudo de España. Al pie de un cañón aparece la estatua del alcalde de Móstoles. A su lado, un jinete al galope tendido: el postillón que lleva apresurado el célebre bando, cuyo texto figura en una placa de bronce. Otros hitos son el monumento a La Barbacana, una escultura realizada por la escultora Virtudes Jiménez Torrubia que se encuentra justo en el lugar donde antes había una barbacana (muro bajo que separaba la plaza ajardinada de la carretera y que constituía un punto de encuentro para la juventud de los años 60 y 70), y el "Homenaje a los escritores más importantes", en la parte superior del pórtico de la plaza del Pradillo, con diversos retratos de literatos.

El Monumento del Bicentenario de los acontecimientos del 2 de mayo de 1808 está Situado en la plaza del Sol del nuevo barrio de Móstoles, metro Manuela Malasaña. El Monumento homenaje al maestro consiste en un grupo escultórico realizado por la escultora Virtudes Jiménez Torrubia. Se encuentra situado en lo que fue el patio de las antiguas escuelas municipales, actualmente la plaza de Ernesto Peces. El Monumento al Peñero representa la figura de un peñero lanzando un cohete en homenaje a la gran cantidad de peñeros que hay en el municipio. Durante los carnavales de 2016 se adjuntó un monolito en memoria de Rafael González Cruceta fallecido en diciembre de 2015 y uno de los más carismáticos peñeros de la ciudad.

También existen una fuente y escultura al titiritero ubicada en la Plaza de la Cultura; la glorieta del 2 de mayo, que rinde homenaje a la mujer trabajadora; la Puerta de Madrid en el barrio de 'Los Rosales'; la Glorieta héroes de la libertad, un homenaje a todas las víctimas del terrorismo; y el Jardín de los Planetas, con diversas esculturas que representan a los planetas del Sistema Solar.

Móstoles sería una de las ciudades de España con más árboles por habitante: nueve árboles por cada veinte habitantes aproximadamente. Entre sus parques y zonas verdes se encuentran:
También existen el parque Prado Ovejero, parque Galicia, parque La Mancha, parque Canarias, parque Andalucía y el parque lineal Arroyo del Soto

Entre las plazas destaca la del Pradillo, la principal y más conocida de Móstoles, se encuentra en el centro urbano. También cuenta con la plaza de España, que alberga la casa consistorial, y la plaza de la Cultura, con el Centro Cultural Villa de Móstoles y la estatua al titiritero.

Una de las principales arterias de la ciudad es la avenida de Portugal, una larga vía con zonas ajardinadas y carril bici. Era parte de la antigua carretera N-V (Madrid-Frontera portuguesa) hasta que en 1980 se abrió al tráfico la circunvalación que rodea la ciudad, (actual A-5). La avenida de la Constitución comunica la avenida de Portugal con la plaza de Pradillo: se trata de una zona comercial y es sede del CA2M. La avenida del Dos de Mayo es continuación de la anterior hacia el barrio del Hospital, ambas formaban parte de la primitiva carretera de Extremadura que atravesaba la localidad, hasta que en 1953 se desvió por la actual avenida de Portugal; también tiene un marcado carácter comercial.

Es tradicional la representación conmemorativa de los hechos de mayo de 1808. Cada 2 de mayo se representan los hechos que acaecieron en Madrid y en Móstoles en torno a aquel día. A cargo de unos 300 vecinos, miembros de asociaciones y peñas, ataviados con trajes de época y dirigidos por Miguel Nieto (actualmente lo dirige Claudio Pascual junto con Juan Polanco). Antes de 2004 se celebraba en la plaza del Pradillo, en el centro de la localidad.

Durante las fiestas navideñas, es costumbre hacer una representación de un "Belén viviente" en la plaza de Ernesto Peces. Tras el muy recordado programa de Martes y Trece en la Nochevieja de 1986, se ha popularizado la idea de que en Móstoles, las empanadillas son un plato tradicional de la Nochevieja siendo esta idea equivocada.

Uno de los centros de mayor relevancia dentro del llamado movimiento okupa a escala estatal está localizado en esta población, se trata de La Casika, un inmueble que permanece ocupado desde 1997 por varios colectivos de extrema izquierda. Móstoles ha venido siendo uno de los lugares de referencia para el grafiti en la Comunidad de Madrid, a lo largo de los últimos años. Sin embargo la política de la presente corporación municipal se opone a esta actividad, por lo que se han abandonado experiencias que se desarrollaron hasta 2004, tales como el certamen de grafiti municipal, que se celebraba anualmente, o el encargo de la decoración de las empalizadas de colegios e institutos a los "grafiteros".

Las zonas donde hay más ambiente nocturno están situadas en la zona de Los Rosales, la zona de Princesa/Salones President, más conocida como la zona de "arriba", la zona del Hospital y algún sitio del centro más para raciones y cañas aunque también hay un par de discotecas. En menor medida, la zona denominada "Estoril" también tiene locales de copas.












</doc>
<doc id="5102" url="https://es.wikipedia.org/wiki?curid=5102" title="Provincia de Salta">
Provincia de Salta

Salta, oficialmente Provincia de Salta (tal y como figura en su Constitución provincial), es una de las 23 provincias de la República Argentina. A su vez, es uno de los 24 estados autogobernados o jurisdicciones de primer orden que conforman el país, y uno de los 24 distritos electorales legislativos nacionales. Su capital es la homónima Salta. Está ubicada al noroeste del país, N.O.A. , limitando al norte con la Provincia de Jujuy y con el Departamento de Potosí y el Departamento de Tarija en Bolivia hasta el trifinio Hito Esmeralda, donde comienza su frontera con el Departamento de Boquerón en Paraguay (hacia el noreste), al este con Formosa y Chaco, al sur con Santiago del Estero, Tucumán y Catamarca, y al oeste con la Región de Antofagasta en Chile. Con 155 488 km² es la sexta jurisdicción de primer orden más extensa, por detrás de la Provincia de Buenos Aires, Santa Cruz, Chubut, Río Negro y Córdoba.

El ser humano empezó a poblar la región unos 10 000 años antes de Cristo, mediante migraciones sucesivas provenientes del norte.

Hacia el siglo XV, poco antes de la llegada de los españoles, el territorio valliserrano y andino estaba y está habitado por los Atacamas y calchaquíes. Estos últimos constituían una diversidad de pueblos que poseían una misma lengua en común, el cacán. En la Puna habitaban y habitan los Atacamas (o más exactamente autodenominados Likanantai) de habla Kunza, cuya cultura era semejante a la quechua pese a ser enemigos, ya que los quechuas eran un grupo originario de la sierra sur del Perú que por ese tiempo se encontraba en plena expansión. Los likanatai llamados también luego "atacamas" eran sedentarios, se dedicaban a la agricultura, con avanzadas técnicas de riego. La ciudad de Tastil, de origen lickan-antay, se convirtió en un activo centro de comercio, quienes instalaron una especie de centro administrativo en "Sikuani" (la actual Chicoana); Tastil fue destruida junto a los cementerios ancestrales en el momento de la expansión quechua.

En el Chaco Salteño, en tanto, habitaban y habitan: wichís, chorotes, qom'lek (comúnmente llamados por los guaraníes: «"tobas"») y vilelas, dedicados a la caza y la pesca.

Los primeros españoles en explorar la zona fueron Diego de Almagro (1535), seguido de Diego de Rojas (1542), quien pasó por el sur de la provincia.

Durante los siglos XVI y XVII, los españoles hicieron dos intentos por establecer una población en la actual zona central de Salta; el primero frustrado, y el segundo con éxito. En 1566, los capitanes Jerónimo de Olguín, Diego de Heredia y Juan de Barzocana, amotinados contra el gobernador Francisco de Aguirre, fundaron una ciudad a la que llamaron "Cáceres", sobre la margen izquierda del río Pasaje (actual departamento de Anta). Aunque refundada varias veces, con diferentes denominaciones —"Nuestra Señora de Talavera del Esteco" (1567); "Madrid de las Juntas" (1592): "Talavera de Madrid" (1609)— se generalizó el nombre "Esteco" en recuerdo a la anterior "Nuestra Señora de Talavera de Esteco", de donde provenían los habitantes que poblaron las ciudades posteriores. Constantemente asediada por los originarios, acabó destruida por un terremoto el 13 de septiembre de 1692.

Mejor suerte tuvo otra ciudad, más al oeste, en las sierras subandinas. Hernando de Lerma, siguiendo órdenes del virrey Francisco Álvarez de Toledo, estableció el primer poblado permanente, llamado "San Felipe de Lerma del Valle de Salta", el 16 de abril de 1582. Pero por diversos motivos, su nombre fue abreviado a "San Felipe de Valle de Salta" y posteriormente a "Salta".

La región formó parte del Virreinato del Perú hasta 1776, en que la corona española creó el Virreinato del Río de la Plata. El nuevo virreinato fue luego subdividido, y Salta quedó ubicada, en un principio, en la Gobernación Intendencia de San Miguel de Tucumán (1782) y posteriormente en la Gobernación Intendencia de Salta del Tucumán (1783), junto con Catamarca, Santiago del Estero, Jujuy, Tucumán y la Puna de Atacama, con capital en Salta (desde 1792). El resto del territorio formó la de Gobernación Intendencia de Córdoba del Tucumán, actuales Córdoba, San Luis, Mendoza, San Juan, La Rioja y pequeños sectores occidentales de la actual provincia de Santa Fe.

En 1794 se fundó la estratégica ciudad de San Ramón de la Nueva Orán, nexo entre Salta y Tarija. 

En 1807 por Real Orden, Tarija fue incorporada a la Intendencia de Salta del Tucumán en los aspectos militares y eclesiásticos, pero continuaba dependiendo de Potosí en lo administrativo al momento de la Revolución de Mayo, hecho que dio lugar al litigio posterior entre Argentina y Bolivia sobre la posesión de la ciudad y su territorio.

En junio de 1810, el cabildo de Salta se sumó a la Revolución de Mayo, llegando en 1811 el primer contingente del Ejército del Norte.

Durante la guerra de la Independencia Argentina, la ciudad de Salta fue invadida varias veces por los realistas: 29 de enero-10 de marzo de 1812; 15 de abril-4 de mayo de 1817, por José de la Serna (Invasión de De la Serna a Jujuy y Salta); 31 de mayo-fines de junio de 1820, por Juan Ramírez Orozco; 7 de junio-14 de julio de 1821 por Pedro Antonio de Olañeta. Durante estas ocupaciones, el caudillo Martín Miguel de Güemes organizó la resistencia y las ofensivas patriotas, y lanzó una guerra de guerrillas popularmente llamada guerra gaucha, hasta su muerte, en 1821.

Con la decisiva batalla de Salta (20 de febrero de 1813), Manuel Belgrano logró que todo el noroeste quedase libre, aunque se mantuvieron esporádicos ataques realistas desde el Alto Perú hasta 1826.

Consolidada la independencia, Salta se hundió, junto con el resto del país, en el torbellino de luchas entre unitarios y federales, siendo alternativamente gobernada por ambos bandos.

Muy poco tiempo después de la Revolución de Mayo, en 1814, la antigua Intendencia de Salta del Tucumán empezó a desintegrarse, y se inició un largo proceso durante el cual la provincia de Salta fue formando su territorio, en medio de disputas con provincias vecinas, disputas entre la Argentina y países vecinos, guerras con las tribus del Chaco, hasta que en 1943 la provincia adquirió la forma y los límites que actualmente tiene.

Por decreto del 8 de octubre de 1814, el Director Supremo Gervasio Posadas dividió la Intendencia de Salta del Tucumán en dos:


El 26 de agosto de 1826, una revuelta popular separó a Tarija no solo de Salta sino de toda la Argentina, anexándola a Bolivia. Facilitaron ello la Guerra argentino brasileña, las luchas civiles en Argentina y los intentos de ciertos gobernantes salteños por mantener a Tarija como sufragánea de Salta. El Congreso nacional, por ley del 30 de noviembre de 1826, elevó a Tarija a la categoría de provincia, aunque no volvió ya a territorio argentino.

Bajo el mando del Gobernador, el Coronel Juan Solá, en 1879 el ejército argentino lanzó una campaña militar en el Chaco, a fin de someter y a los indígenas de la región. Como resultado de la misma, el Chaco Central y Austral fueron puestos bajo la órbita del Estado Nacional. En la ofensiva fueron muertos millares de indígenas, y a las tribus sobrevivientes se les despojó de sus tierras, que fueron entregadas a colonos, origen del actual conflicto entre criollos y aborígenes en el Chaco salteño. Posteriormente, por Ley N° 1.532 de "Organización de los Territorios Nacionales" (16 de octubre de 1884), el Estado Nacional estableció los límites entre Salta y los Territorios Nacionales del Chaco y de Formosa. La provincia se ensancha hacia el este.

Por el tratado del 10 de mayo de 1889 con Bolivia, Argentina renunciaba a su reclamo sobre Tarija. En compensación, Bolivia cedía la Puna de Atacama —territorio que, por otra parte, había sido incorporado a Salta ya en 1816 por Martín Miguel de Güemes—, que se encontraba en poder de Chile luego de la guerra del Pacífico (1879-1880). Esta acción boliviana le otorgaba a la Argentina un territorio que formó parte del Virreinato del Río de la Plata, pero que de hecho estaba en manos de Chile, buscando forzar una guerra entre Chile y Argentina. Como Chile se negara a entregar los territorios cedidos por Bolivia, se decidió someter la cuestión al arbitraje del estadounidense William Buchanan, que en 1899 otorgó a Argentina el 75% del territorio en disputa y el resto a Chile. También por el tratado de 1889 Argentina cedía un territorio que se consideraba hasta entonces salteño: las Juntas de San Antonio. Las concesiones argentinas favorables a Bolivia prosiguieron con las rectificaciones de 1904 (Esmoraca y Estarca) y Yacuiba, Yacuiba recién fue aceptada como boliviana (en territorio tarijeño) por el Tratado de límites Carrillo-Díez de Medina (julio de 1925). 

Por la Ley N° 3906 (9 de enero de 1900) se creó el Territorio de Los Andes. Por decreto del 12 de mayo de 1900 el Poder Ejecutivo Nacional dividió al Territorio de Los Andes en tres departamentos administrativos: Susques (norte), que limitaba con la Provincia de Jujuy; Pastos Grandes (centro), lindante con la Provincia de Salta; y Antofagasta de la Sierra (sur), limítrofe con la Provincia de Catamarca.

En 1902 la Provincia de Salta cedió una parte del Departamento de La Poma, más precisamente el sector de San Antonio de los Cobres (aproximadamente 5500 km²) por Ley N° 4059, para que pasara a ser la capital del territorio, formándose con ella un cuarto departamento en el Territorio de Los Andes. A propósito de esta decisión, cabe señalar que en la actualidad, en la división departamental de la Provincia de Salta se advierte una extraña intrusión del territorio del actual Departamento de Los Andes en el Departamento de La Poma, imprimiendo en ese sector una suerte de "cuello de botella", resultante del hecho que la localidad de San Antonio de los Cobres jamás volvió a pertenecer a La Poma, sino que, una vez repartido el Territorio Nacional de Los Andes entre las provincias de Jujuy, Salta y Catamarca (como se señala más adelante), la parte que le correspondió a Salta -que pasó a llamarse Departamento de Los Andes- conservó a dicha localidad como su cabecera.

En 1925 la Argentina reconoce la soberanía de la localidad de Yacuiba a Bolivia, a pesar de estar al sur del paralelo que porta el límite internacional acordado, debido a que Bolivia necesitaba conservar una población en el área del Chaco.

Como consecuencia de la rectificación fronteriza debida al tratado de límites entre Argentina y Bolivia firmado el 9 de julio de 1925, y puesto en vigor el 11 de octubre de 1938 la provincia de Salta incorporó en 1941 el área de Los Toldos, hasta entonces boliviano.

En 1943 el Gobierno Nacional resolvió disolver el Territorio Nacional de Los Andes. Los departamentos de San Antonio de Los Cobres y Pastos Grandes fueron fusionados y reintegrados a la provincia de Salta, constituyendo el actual Departamento Los Andes. (Susques pasó a pertenecer a la Jujuy y Antofagasta de la Sierra a la Catamarca).

El límite con la provincia de Tucumán fue fijado mediante la Ley Nacional N° 22264 dictada por el gobierno militar y publicada en el Boletín Oficial el 12 de agosto de 1980.

 La Convención Constituyente reunida en Santa Fe, que redactó la Constitución de 1853, fue presidida por un salteño, el Dr. Facundo de Zuviría. Durante la segunda mitad del Siglo XIX se establecieron y consolidaron las instituciones del Estado Salteño (legislatura, corte, etc.) y se definieron y separaron los roles del gobierno de la Provincia de Salta y el de la ciudad de Salta, proceso en el cual se estableció la Municipalidad de la Ciudad de Salta.

A fines del Siglo XIX y principios del XX llegan a la Argentina millones de inmigrantes italianos y españoles, pero muy pocos se radican en Salta. Sí, en cambio, es establecen grupos de sirios y libaneses, quienes dan una nueva dinámica a la economía local.

A mediados del Siglo XX, los vientos del peronismo soplan con fuerza en todo el país. En la ciudad de Salta, es expropiada la sede del elitista club Club 20 de Febrero, cuyo edificio es declarado de utilidad pública y pasa a pertenecer al gobierno provincial, como así también algunas haciendas en los Valles Calchaquíes, pertenecientes a influyentes terratenientes. Durante las décadas de '60 y '70 la provincia (y toda la región noroeste del país) era observada con recelo desde el gobierno nacional, ante la posibilidad del surgimiento de movimientos de extrema izquierda, como el Ejército Guerrillero del Pueblo.

Salta, al igual que el resto del país, vivió la dictadura militar (1976-1983) y la violencia política que le precedió. Desaparecieron unos 200 salteños, entre ellos el exgobernador justicialista Miguel Ragone.

A partir de la reinstauración de la democracia (1983) la provincia ha disfrutado de cierta calma política, mientras otras provincias del norte han sufrido constantes y repentinos cambios de gobierno. Los plazos constitucionales se han cumplido y cada cuatro años se renueva la administración. Se han sucedido los siguientes gobernadores:


La Provincia se rige por la Constitución de 1998, que reemplazó a la anterior, de 1986.

En 2003 se reformó un artículo, el referido a la elección del gobernador. Hay tres poderes: ejecutivo, legislativo y judicial:

Es ejercido por el gobernador, elegido por voto popular por un período de 4 años, y que puede ser reelecto hasta por un tercer mandato consecutivo (según la reforma de 2003). En la misma fórmula también se elige al vicegobernador, quien ejerce la presidencia del Senado provincial en representación del Poder Ejecutivo. El gobernador es asistido en sus funciones por un gabinete de 10 ministros.

Es ejercido por una legislatura bicameral:

Está integrado por una corte de siete magistrados (un presidente y seis miembros).

Desde el año 2013, el 100% del padrón de la Provincia vota con el Sistema de Boleta Única Electrónica. Esto transforma a Salta, en la primera provincia de la Argentina en utilizar exitosamente este sistema, el cual también se utiliza desde 2015 en la Ciudad de Buenos Aires.

A través de este sistema se emite un voto que cuenta con respaldo electrónico y respaldo físico (en papel). La autoridad de mesa entrega una boleta al votante, la cual está en blanco y tiene un chip sin información. El votante inserta la boleta en una computadora que presenta las listas de candidatos. El votante selecciona su voto y el mismo se imprime en la boleta y se guarda en el chip. El votante puede verificar que su voto se haya guardado correctamente con un lector de chip que presenta la máquina y viendo la impresión.

El sistema de boleta única electrónica evita el robo de boletas, reduce enormemente la cantidad de votos impugnados y reduce la posibilidad de realizar fraudes como el "voto hormiga" o "voto cadena". Por tanto, resulta en un ahorro importante de infraestructura para los partidos políticos, tanto para generar boletas como para fiscalizar las mismas. También da más transparencia a la elección teniendo una doble verificación.

La capital de la provincia es la ciudad de Salta.
La provincia se divide administrativamente en 23 departamentos, subdivididos en 60 municipios.

"Véase:" "" y "Organización municipal de la provincia de Salta"

Salta forma parte de la Región del Norte Grande Argentino, cuya creación fue suscripta en la ciudad de Salta el 9 de abril de 1999 entre las provincias de Catamarca, Corrientes, Chaco, Formosa, Jujuy, Misiones, Tucumán, Salta y Santiago del Estero a fines de promover la ""integración de las provincias del NOA y el NEA, a los efectos de lograr en la realidad un sistema efectivo de consenso y acción conjunta entre los estados partes"".

Evolución de la población hasta 1869:

Población actual:

La población se encuentra desigualmente distribuida. La zona más densamente poblada de la provincia es el Valle de Lerma, donde se encuentra la ciudad de Salta. La densidad allí supera los 20 hab./km² . La misma desciende a menos de 1 hab./km² en los departamentos de Los Andes y La Poma.

Analizando los censos de 1869 hasta ahora, se observa una tendencia preocupante a la concentración de la población en la capital provincial. En 1869, de un total de 88.933 habitantes en la provincia, solo 11.716 (13,17 %) vivían en la ciudad de Salta y sus alrededores. En tanto que el último censo, de 2001, de un total 1.079.051 personas en la provincia, 468.583 habitaban en el Gran Salta, lo que representa el 43,42%.

Salta, al igual que otras provincias del norte del país, es a menudo descrita como una "provincia feudal", debido a que aún arrastra ciertos problemas de desarrollo que la alejan de las provincias pampeanas, más industriales y modernas.


No obstante algunos progresos realizados en los últimos 20 años, las estadísticas de educación en Salta aún están por debajo de los índices nacionales.
Sucesivos gobiernos han declarado una lucha contra el analfabetismo y la deserción escolar, pero las iniciativas chocan contra el problema del trabajo infantil, muy común en las zonas rurales,y tolerado por las mismas autoridades.

Para la educación superior existen cuatro universidades: la Universidad Nacional de Salta (sede en Salta Capital, subsedes en Orán, Tartagal, San José de Metan y Rosario de la Frontera) , una universidad privada de orientación católica, la Universidad Católica de Salta, la Universidad Provincial de Administración Pública (primera en su tipo en toda la Argentina, y en proceso de organización siendo su rector el Dr. Roberto Luciano Robino); y la universidad empresarial siglo 21, con sede en Córdoba. Existe también un importante número de establecimientos de educación superior no universitaria (terciarios), que ofrecen numerosas carreras, algunos de los cuales han alcanzado un excelente nivel académico. Sin contar las universidad a nivel nacional con sede en la provincia

Al igual que en el resto del país, la gran mayoría de la población habla castellano. Tras 400 años de contacto con el español, los idiomas indígenas siguen conviviendo en la actualidad con el castellano pero su uso mayoritario depende de la zona de la provincia - el cacán, utilizado por los calchaquíes, el quechua y el aimara. Por otra parte se han preservado muchos topónimos indígenas en toda la provincia, como así también algunas palabras, utilizadas en el habla cotidiana por todas las clases sociales: "yapa" (aumentar, agregar), "chango" (joven, niño), "guagua" (bebé), etc.

En la región oriental de Salta, limítrofe con Chaco y Formosa, que hace relativamente poco tiempo fue incorporada a la provincia, hay miles de indígenas que aún hablan sus idiomas ancestrales. Los dos principales son el wichí (hablado por unas 30.000 personas) y el chiriguano, un dialecto del guaraní (utilizado por unas 20.000 personas). Hay también otras cinco lenguas indígenas, habladas por unas 15.000 personas.

La mayoría de la población (alrededor del 91%) adhiere a la iglesia católica. Pero mientras que las clases alta y media profesan el catolicismo formal, los habitantes de zonas rurales suelen profesar un catolicismo sincrético, hibridado con antiguas creencias ancestrales indígenas. Aún perviven, en la provincia, el culto a la Pachamama, y no es inusual ver, en la sala de algunas casas, la figura de un Ekeko.

Los inmigrantes sirios y libaneses llegados a principios del siglo XX introdujeron el cristianismo ortodoxo del rito antioquiano, que cuenta con un importante número de adeptos en Salta, Tartagal y otras ciudades importantes.

Los protestantes, en sus diferentes expresiones, tienen también una presencia destacable. Hay anglicanos, metodistas, luteranos, pentecostales, Adventistas del Séptimo Día etc.

La congregación cristiana de los Testigos de Jehová representa 0,46% de devotos de la provincia, lo cual es minoría del total de la población. Sus centros de reuniones se centran el la capital pero también hay en el interior de la provincia, organizan asambleas para invitar a los habitantes y hacen predicación de casa en casa con publicaciones en idiomas originarios como el quechua y aimara para las personas que no hablan el español. 

También hay un número considerable de mormones, los cuales no se reconocen como "Protestantes"

La comunidad judía salteña está bien organizada, y cuenta con un rabino para sus oficios religiosos. La comunidad judía es muy antigua, sus orígenes se remontan a la época colonial.

Existen también pequeños grupos de musulmanes (entre inmigrantes del Medio Oriente radicados recientemente), budistas (inmigrantes de origen chino y coreano), y en Rosario de la Frontera, donde se radicaron numerosos inmigrantes de la India, hay, incluso, una pequeña comunidad sikh.

Los principales centros urbanos de la provincia, de acuerdo con los datos del 2019, son:

También superan los 20.000 habitantes: Rosario de la Frontera (27.819 h.) y Embarcación (ente 80 mil y 60 mil h.).

Con más de 10.000 habitantes: Pichanal (20.773 h.), Rosario de Lerma (19.871 h.), Profesor Salvador Mazza (18.068 h.), Colonia Santa Rosa (15.399 h.), Joaquín V. González (15.376 h.), General Mosconi (14.118 h.) y Cafayate (13.785 h.).

Salta, al igual que el resto del país, ha recibido una gran cantidad de inmigrantes, aunque a menor escala que otras provincias y con diferentes fuentes de inmigración. Mientras que otras provincias recibieron una mayoría de inmigrantes europeos, en Salta se produjo una ola migratoria principalmente de origenhispanoamericano destacándose la comunidad boliviana y la de origen árabe, principalmente siria y libanesa. Asimismo se encuentran presentes las comunidades europeas mayoritarias en el país, tanto la española como la italiana.

La comunidad boliviana en la provincia de Salta ronda las 200.000 personas.

En Salta, al igual que en el resto de la Argentina, el fútbol es el deporte más popular. Los clubes más importantes de la provincia son: Gimnasia y Tiro, Central Norte, y Juventud Antoniana; quienes en algún momento de su historia llegaron a participar de la Primera División.
El estadio de mayor envergadura es el Gigante del Norte, con capacidad máxima de 30.000 espectadores. En el mismo se enfrentó la Selección Argentina contra Marruecos, partido en el que jugó Diego Maradona. En 2001 se inauguró el Estadio Padre Ernesto Martearena; con capacidad para 23.408 espectadores; ha albergado la Copa Mundial de Fútbol Juvenil de 2001, y la Copa América 2011.
Si bien varios jugadores salteños llegaron a integrar parte de el Seleccionado, Alberto Chividini es el único que ha logrado consagrarse campeón con Argentina (Campeonato Sudamericano 1929), y el único en disputar un mundial (Uruguay 1930).
A mediados del siglo XX, el béisbol empezó a ganar gran cantidad de adeptos. Salta es potencia en este deporte, al punto de que la mayoría de los jugadores del Seleccionado Argentino son salteños. Algunos talentos incluso, han probado suerte en el exterior, con notable éxito.

Otro de los deportes populares de la provincia, es el rugby. A pesar de que la provincia tiene pocos equipos, la selección de la Unión de Rugby de Salta es una de las más fuertes del país, estando en la Zona Campeonato del Campeonato Argentino de Rugby. Los equipos más representativos a nivel clubes de la provincia son Gimnasia y Tiro (campeón del Torneo Regional del Noroeste en 1999), Jockey Club, Universitario Rugby Club (Salta) y Tiro Federal. Tanto en Natación, como en Ciclismo, se considera a Salta como una potencia regional destacándose el ciclista Daniel Diaz . Actualmente en natación, se destaca la presencia de Robert Strelkov, quien participó en los , y en los . En ciclismo, se debe destacar el desarrollo de la Competencia "Clásica 1° de Mayo", certamen desarrollado anualmente, y que cuenta con la participación de deportistas internacionales de destacada trayectoria.

Recientemente, el Baloncesto creció en popularidad. Salta Basket, fundado en 2014, participó en La Liga (primera división), y llegó a competir en torneos internacionales. Actualmente disputa La Liga Nacional (segunda división). El Tribuno Basket, se encuentra en tercera división.

El Autódromo de Salta, inaugurado en 1974, ha albergado carreras de los principales campeonatos nacionales de automovilismo, tales como el Turismo Carretera, TC 2000, Top Race, y Turismo Nacional. En motociclismo, se destaca Kevin Benavides, quien participó en el Campeonato Mundial de Enduro, y en el Rally Dakar.

La fisiografía del territorio, hace que los deportes extremos de aventura sean muy practicados.
El volcán Llullaillaco, en el límite con Chile, es la máxima elevación de la provincia, y la sexta del país. El Socompa también sobrepasa los 6000 msnm, y hay otras 10 montañas que superan los 5000 msnm. Los cultores del montañismo, por lo tanto, son muchos, y se nuclean en torno a clubes muy bien organizados. El salteño Mariano Merani escaló hasta la cima del Cho Oyu, en el Himalaya en 1998, y otro salteño, Christian Vitry, llegó hasta la cumbre del Dhaulagiri, en la misma cordillera, en 2008.

En el cañón del río Juramento se puede practicar el ráfting, sin embargo esta actividad está bastante más difundida entre los turistas.
La caza, y la pesca deportiva, están estricamente reguladas; y los aficionados deben informarse oportunamente con las autoridades sobre licencias y períodos de veda.

Salta tiene la forma de una herradura, y en su extenso territorio hay gran diversidad de relieve, climas, flora y fauna. Sus límites están precisos, en gran parte, salvo por algunas disputas de límite que aún mantiene con las provincias de Catamarca (Salar del Hombre Muerto) y Jujuy.
Conocer el Relieve de Salta

Se distinguen cuatro paisajes diferenciados, de oeste a este:
También, en el nexo entre la Puna y la región valliserrana se ubica la gran Quebrada del Toro donde se encuentra la localidad de San Antonio de los Cobres.

La actividad sísmica del área de Salta es frecuente y de intensidad baja, y un silencio sísmico de terremotos medios a graves cada 40 años.


Se observan climas y paisajes contrastados principalmente según la altitud: en la región occidental (la más elevada) predomina un clima árido y frío con bruscas variaciones térmicas entre el día y la noche (e incluso si se pasa de un lugar asoleado a un lugar bajo sombra), durante el siglo XX la desertificación se ha agravado quedando la vegetación natural casi reducida a manchones de plantas xerófilas y psamófilas como la achaparrada tola que forma "colchones" o los grandes cactos llamados cardones, en la Puna y en las quebradas más secas se encuentran remanentes de un antiguo bosque de árboles bajos (churquis y queñoas), en la zona de los valles Calchaquíes apenas quedan vestigios de los densos bosques de algarrobos criollos (o “tacos”).

En la zona de los valles latitudinales, los vientos húmedos del Océano Atlántico señalan una transición hacia el clima tropical húmedo, formándose en las laderas orientales una densa nimbosilva y pluvisilva que corresponde al bioma de yungas, con una enorme variedad de especies, entre las que se destacan los jacarandás (o tarcos), tipas, cebiles, molles, zapallos caspis, urundeles, guayabos etc, mientras que el este, ya en la región del Chaco Salteño se forman bosques de árboles caducifolios adaptados a las alternancias estacionales de sequías (en invierno) y “temporada lluviosa” (en verano) con ejemplares de chañares, lapachos, quebrachos, guayacán, yuchán, ñandubay, vinal y palmas o palmeras como la timbó y caranday.

En algunas regiones de Salta, en particular en las yungas, es común la práctica de la deforestación para la utilización agrícola de las tierras. Por este motivo, algunas organizaciones ecologistas en Argentina actualmente buscan la creación de leyes que regulen la utilización de las zonas selváticas a nivel nacional.

Con excepción de algunas zonas de la Puna, que forman cuencas endorreicas, la mayor parte de la provincia de Salta se encuentra dentro de la cuenca del Plata.

Merced a las elevadas altitudes (que rondan los 6000 msnm) de los cordones andinos con, pese a la latitud, nieves eternas y a la condesación de la humedad atmosférica en forma de una bruma sobre las laderas más orientales, la provincia de Salta posee importantes cursos de agua, aunque heterogéneamente distribuidos en el territorio. Los tres ríos más importantes y caudalosos son el Pilcomayo, el Bermejo, (tributarios del Paraguay) y el Juramento (llamado Salado en la vecina Santiago del Estero, tributario del Paraná). Afluentes de estos son el río Grande de Tarija, Itaú, Yocavil, Horcones, el Metán, etc. A ellos se les suman enorme cantidad de ríos menos prolongados, arroyos y arroyuelos, que descienden de los faldeos orientales andinos. Aprovechando la presencia de tal red fluvial se han construido algunos embalses con usinas hidroeléctricas como en Cabra Corral y el dique El Tunal.

Además de los ríos y arroyos, la provincia de Salta posee importantes lagunas, especialmente en el sector puneño, pero la extendida desertificación ha transformado a gran parte de tales lagunas en extensos salares como los de Arizaro, Pocitos, Barreal, Quirón, Tolillar, Rincón, Incahuasi, Antofalla, Ratones, Llullaillaco etc. Algunas de las singularidades de tales lagunas–salares son la frecuentemente altísima alcalinidad de sus aguas y el hecho de que constituyan el núcleo de cuencas endorreicas. Por otra parte, en el Chaco Salteño existen amplios humedales como los bañados de Los Colorados y los esteros del Quirquincho.

Las actividades económicas de la provincia representan, en conjunto, aproximadamente el 1% del PBI de la Argentina. Pero si se considera que la población de Salta es algo superior al 3% de la población del país, ello significa que el PBI "per cápita" de la provincia está muy por debajo de la media nacional. Uno de los principales problemas de Salta es la llamada "economía informal" o "en negro." En las zonas rurales, muchos habitantes trabajan en negocios o explotaciones agrícolas no registrados. Y en la capital provincial pueden verse millares de "vendedores ambulantes." Según estadísticas del INDEC de marzo de 2009, el empleo en negro en Salta es el cuarto más alto del país.
En los dos últimos años aumentó en un 49% la radicación de empresas en Salta, empresas principalmente argentinas y europeas que se basan en la metalmecánica, alimentos, textiles, azucareras y del calzado.

La base de la economía está dada por cultivos industriales como el cacao café, tabaco, chirimoya, caña de azúcar, banana, mango, papaya, cítricos (pomelo), legumbres (porotos y soja), hortalizas (las que se producen todo el año), vid, ajíes, cebollas, papas y algodón, mientras que aún se mantiene a nivel de agricultura de subsistencia un promisorio cultivo para la región puneña: el de la quinoa.

La ganadería se encuentra representada por la cría de ganado vacuno en la región del valle de Lerma y las zonas despejadas de la región chaqueña, por otra parte es frecuente la presencia de caprinos en las zonas montañosas y se mantiene como un valioso recurso la cría de auquénidos (en particular la vicuña) en las zonas de mayor altitud y aridez, mientras que la cría de caballos es reducida aunque los ejemplares de caballo salteño suelen tener merecida fama por su resistencia.

La minería y la producción de hidrocarburos (petróleo, butano) tienen gran importancia, sobre todo en el norte del Chaco Salteño, contando con una de las mayores reservas mundiales de gas de lutita en el Yacimiento gasífero Los Monos. En la región de la Puna hay ricos yacimientos de diversos minerales (oro, cobre, plomo, plata, estaño, litio, bórax, salitre, potasio, etc.), aún sin explotar o que recién se han comenzado a explotar.

Las pocas industrias existentes están directamente relacionadas con las actividades agrícola-ganaderas locales: azúcar y sus subproductos (en el norte de la provincia), vino (en los valles calchaquíes), cervezas, lácteos y pastas (en el valle de Lerma), etc. En años recientes, la radicación de numerosas firmas extranjeras ha introducido tecnología de punta en la actividad vitivinícola. Hoy, por ejemplo, el azúcar y el vino se exportan a Europa y los Estados Unidos, y en el caso del vino salteño, goza de una excelente reputación a nivel internacional.

También guardan relación las manufacturas locales con las actividades mineras: en el norte hay refinerías de petróleo y gas natural. El bórax que se extrae en el Valle de Lerma se utiliza en la fabricación de detergente, etc.

Muchas industrias aún tienen un carácter muy artesanal, y se destinan al mercado local: queso de cabra, quesillos, indumentaria de cuero, muebles de madera, etc.

En la capital provincial y las principales localidades gran parte de la población se dedica al sector de servicios: comercio, bancos y actividades financieras, educación, salud, transporte, comunicaciones, gastronomía y entretenimiento, etc. El turismo ha cobrado gran importancia, y se han abierto numerosos hoteles, algunos de gran categoría.


El servicio eléctrico se encuentra desde el año 1996 a cargo de la empresa distribuidora EDESA, empresa perteneciente al Grupo DESA (Desarrolladora Energética S.A.), cuyo CEO y presidente es el empresario Rogelio Pagano. Debido a la geografía de la provincia, existen zonas rurales dispersas a las que los sistemas energéticos de EDESA no tienen acceso. Es por esto que se creó ESED S.A. – Empresa de Sistemas Eléctricos Dispersos- compañía concesionaria controlada por EDESA, encargada de brindar este nuevo servicio público, en forma exclusiva, a todos los habitantes de Salta que lo soliciten.

Dada la excepcional ubicación de la provincia como punto de enlace internacional, las rutas que recorren su territorio son muy transitadas por vehículos de varias nacionalidades.

La provincia está unida al resto del país por varias rutas nacionales. Las rutas 9 y 34 se dirigen, al sur, hacia las grandes ciudades pampeanas, y al norte, hacia Jujuy y Bolivia. La [[Ruta Nacional 50, en tanto, une la ciudad de [[San Ramón de la Nueva Orán|Orán]] con la localidad boliviana de [[Bermejo (Tarija)|Bermejo]]. Por el este, las rutas [[Ruta Nacional 16 (Argentina)|16]] y [[Ruta Nacional 81 (Argentina)|81]] unen a Salta con [[provincia del Chaco|Chaco]] y [[Provincia de Formosa|Formosa]] respectivamente. Hacia el oeste, la [[Ruta Nacional 51 (Argentina)|ruta 51]] va hacia San Antonio de los Cobres, y empalma, en territorio [[Chile|chileno]], con la carretera CH23 que llega hasta la ciudad de [[Antofagasta]], en la costa del [[Océano Pacífico|Pacífico]].

En el interior de la provincia, la [[Ruta Nacional 68 (Argentina)|ruta 68]] une a la ciudad de Salta con la localidad de [[Cafayate]]. La famosa [[Ruta Nacional 40 (Argentina)|ruta 40]], ""la ruta más larga del país"", recorre la zona occidental de la provincia, desde el [[Departamento Los Andes]], limítrofe con Jujuy, hasta el [[Departamento Cafayate]], limítrofe con [[Tucumán]], la que alcanza en el [[Abra del Acay]] (en el km 4601) su punto de mayor altitud: 5061 [[msnm]] (16 604 pies), en las coordenadas geográficas: . 

[[Archivo:Polvorilla.jpg|left|750px|thumb|El [[Tren a las nubes]], que circula a través de los picos de la [[Cordillera de los Andes]] mostrando impactantes paisajes, alcanza en su recorrido una altura de 4.200 msnm, lo que lo convierte en uno de los trenes que mayor altura alcanzan en el mundo.]]

[[Archivo:Viaducto La Polvorilla, Salta, Argentina.jpg|thumb|310x310px|[[Viaducto La Polvorilla]], punto final del [[Tren a las nubes]] en la línea ferroviaria que une a Salta con la ciudad chilena de [[Antofagasta]].]]
El [[Ferrocarril General Belgrano]] enlaza a Salta con todo el norte de la [[Argentina]] y con países vecinos.

El puente [[Viaducto La Polvorilla]] es una estructura de vigas de acero de 222,4 m de longitud, una altura promedio de 70 m respecto al suelo y 1600 toneladas de peso, sobre un terreno ubicado a 4.200 [[msnm]] —constituyendo así uno de los puentes y tramos ferroviarios más altos del mundo respecto al nivel del mar—. Desde su inauguración alrededor de 1930 fue considerada una obra monumental de ingeniería, transformándose en un [[atractivo turístico]]. En la década de 1970 la empresa [[Ferrocarriles Argentinos (1949-1993)|Ferrocarriles Argentinos]] ideó un servicio de pasajeros turístico, el [[Tren de las Nubes]], con término poco más allá de [[La Polvorilla]].

Por este puente pasaron muchos trenes de pasajeros que enlazaban Salta Capital con [[Antofagasta]] como el expreso 1721 - 1722 de la empresa Ferrocarriles Argentinos. Este servicio dejó de correr definitivamente en agosto de 1977; desde esa fecha tan sólo corrían trenes de carga con 4 coches de Materfer al final del tren conocido como el Mixto. El Mixto dejó de correr junto por falta de fondos de la municipalidad de [[San Antonio de los Cobres]] en el año 2005. 

El [[Aeropuerto Internacional de Salta Martín Miguel de Güemes|aeropuerto Martín Miguel de Güemes]], de la [[ciudad de Salta]], es el más activo del norte de [[Argentina]]. En él operan varias líneas aéreas que unen Salta con [[Buenos Aires]], [[Córdoba (Argentina)|Córdoba]], [[Ciudad de Mendoza|Mendoza]] e [[Puerto Iguazú|Iguazú]]; con la ciudad boliviana de [[Santa Cruz de la Sierra]], el puerto chileno de [[Iquique]], la ciudad de [[Lima]], [[Perú]] y con [[Asunción]], la capital de [[Paraguay]], proporcionándole estos destinos, mediante las conexiones necesarias, comunicación aérea permanente con el resto del mundo. 
Las ciudades de [[San Ramón de la Nueva Orán|Orán]], [[General Mosconi (Salta)|Gral. Mosconi]] y [[Cafayate]] también cuentan con aeropuertos y hay pistas de aterrizaje en [[Cachi (localidad)|Cachi]], [[Rosario de la Frontera]], [[Metán]], [[Los Toldos (Salta)|Los Toldos]], [[Rivadavia Banda Sur]] y [[Santa Victoria Este]]. El gobierno de la Provincia mantiene vuelos regulares, de carácter sanitario, desde la capital provincial a las poblaciones más remotas del territorio salteño.

[[Archivo:Bandera de la Provincia de Salta.svg|thumb|right|200px|Bandera de la Provincia de Salta]]

Mediante la ley N.º 6946 se adoptó como símbolo provincial en 1996 la [[Bandera de Salta]], luego del concurso al que se convocó para su diseño y que ganaron los alumnos de 7.º «A» de la escuela Nicolás Avellaneda. En la bandera observa el color del tradicional [[poncho salteño]] (semejante al de los [[Infernales]] que acaudillara [[Martín Miguel de Güemes]]) y se ven representados los 23 departamentos mediante estrellas doradas como las de las espuelas gauchas llamadas nazarenas que rodean el escudo de la provincia.

[[Archivo:Escudo de de Salta.gif|miniaturadeimagen|267x267px|Escudo de la Provincia de Salta]]
El [[Escudo de la Provincia de Salta]] es un una imagen de forma oval, siguiendo la forma general del [[escudo de Argentina|escudo nacional]], con una estrella y un sol al centro. El escudo está diseñado sobre tres conceptos básicos: el ideal [[Martín Miguel de Güemes|güemesiano]], la lucha de los hombres y mujeres que dieron su vida por la [[independencia de Argentina]] y la identidad salteña.

La Provincia de Salta ha utilizado diversos escudos a lo largo de su historia, hasta adoptar el escudo definitivo en 1946 por Ley N.º 749.

Gloria a Salta es oficialmente el [[Himno de la Provincia de Salta]], [[Argentina]]. Es obligatoria su entonación en todos los establecimientos escolares dependientes del gobierno provincial y en todo acto oficial después del [[Himno Nacional Argentino]].
La letra fue compuesta por [[Sara Solá de Castellanos]] y la música por [[Amy Paterson]].

Popularmente la zamba [[La López Pereyra]] es reconsiderada el himno de la Provincia. La música fue compuesta en 1901 por [[Artidorio Cresseri]] y la letra por Juan Francia, René Ruiz, Arturo Gambolini, José Gambolini, Carlos López Pereyra y [[Artidorio Cresseri]].

[[Categoría:Provincia de Salta]]

</doc>
<doc id="5103" url="https://es.wikipedia.org/wiki?curid=5103" title="Salta">
Salta

Salta es una ciudad del norte de Argentina, capital de la provincia homónima. Posee una población de 533.303 habitantes (censo 2010) , siendo la ciudad más poblada de la provincia, la segunda del NOA y la séptima del país. Constituye un importante polo cultural y turístico. Se encuentra ubicada al este de la cordillera de los Andes, en el valle de Lerma, a 1187 m, muy cerca del nacimiento del río Salado —un importante río que desagua en el río Paraná— y cruzada por el río Arenales que la divide en norte y sur.

Es miembro de la red Mercociudades, junto a otras 180 urbes de los países miembros del Mercosur.

El nombre de la ciudad de Salta procede del nombre de la tribu indígena de los "Salta", que habitaban allí cuando el español Hernando de Lerma fundó la ciudad que originalmente llamó "Ciudad de San Felipe y Santiago del Lerma en el valle de Salta, provincia del Tucumán".

El escudo de armas de la ciudad de Salta es el escudo oficial que utilizan las diferentes áreas y dependencias de la municipalidad de la ciudad de Salta. El Concejo Deliberante de Salta adoptó en 1934 mediante la ordenanza N.º 239/34 un escudo en el que aparecía la figura de un indígena amenazando al conquistador español con arco y flecha, en 1938 la ordenanza municipal N.º 470 retiró la figura por ser considerada "discriminatoria" hacia los pueblos indígenas.

Sobre el campo único aparece un soldado español con armadura de plata casaca y bombacho de gules que sostiene con su brazo diestro una alabarda de plata y con el siniestro las correas de un perro. Un río azul celeste, y detrás de este aparecen dos colinas y tres árboles en fila foliados en sinople. Por detrás de las sierras hay dos nubes de plata sobre un cielo de azur celeste. Ostenta los títulos de "Muy Noble" y "Muy Leal" .

Posterior a la primera guerra calchaquí, en marzo de 1576, el virrey del Perú, el español Francisco Álvarez de Toledo, con el claro objetivo de llevar a cabo una vez más su idea que las provincias estuvieran conectadas y anexadas de tal forma que pudieran salvaguardarse de los levantamientos, a sabiendas de que una provincia podía auxiliar a otra en caso de ser necesario, le escribió al rey Felipe II de España que “envió gente a hacer la población del Valle Calchaquí y Salta, para que aquella provincia de Tucumán se pudiera unir, juntar y comerciar con la de la ciudad de la Plata”, llamada a lo largo de su historia como: Charcas-La Plata-Chuquisaca-Sucre.

Cumpliendo las órdenes que le dio el virrey Álvarez de Toledo, el español Hernando de Lerma fundó la ciudad de Salta, el 16 de abril de 1582.

Las claras instrucciones de Álvarez de Toledo a Lerma, en las expresiones “ordeno y mando” y “bajo apercibimiento” demuestran que el autor de la fundación de Salta fue el Virrey Toledo y su ejecutor Hernando de Lerma.

Otras dos causas convencieron al virrey de la necesidad de fundar la ciudad de Salta. La primera de ellas era mitigar la fuerte resistencia que la tribu de los indios chiriguanos oponía al avance español desde el este de la nueva ciudad. La segunda causa fue la de crear un centro poblacional que fuera escala en las comunicaciones entre la ciudad de Lima, la capital virreinal, y la lejana Buenos Aires, ciudad cuya segunda fundación habían llevado a cabo los españoles en 1580, en las costas del estuario del Río de la Plata, llave de entrada preferencial hacia el interior virreinal desde el Océano Atlántico.

El nombre "Salta" es de origen indígena pero su traducción al castellano ha resultado difícil para los lingüistas y sobre su significado exacto se han propuesto las más disímiles teorías.

Durante la época virreinal la población prosperó rápidamente pues era abastecedora de materias primas para la opulenta Potosí. Formó parte del Virreinato del Perú hasta 1776, cuando la Corona de España creó el Virreinato del Río de la Plata. En 1783 fue designada capital de la Intendencia de Salta del Tucumán.

En tiempos de la Revolución de Mayo, la ciudad fue cuartel general de las expediciones al Alto Perú y en la lucha contra los realistas, se destacaron los escuadrones de gauchos al mando del general Martín Miguel de Güemes.

El 20 de febrero de 1813 las tropas de las Provincias Unidas del Río de la Plata, a las órdenes del general Manuel Belgrano, lograron una decisiva segunda victoria sobre los realistas en la batalla de Salta, suceso bélico que dejó libre al actual territorio argentino.

Como consecuencia del triunfo de Salta, el mayor general Eustoquio Díaz Vélez, gobernador militar de la Intendencia de Salta del Tucumán de las Provincias Unidas del Río de la Plata, inmediatamente colocó la Bandera de la Argentina en el balcón del Cabildo de Salta por lo que le cabe la característica de haber sido la primera ciudad en que las autoridades revolucionarias enarbolaron por primera vez la bandera celeste y blanca.

Tras la Declaración de independencia de la Argentina, en 1816, la ciudad quedó económicamente arruinada y se sumergió en un período de decadencia por buena parte del siglo XIX. Aunque hacia la década de 1890, con la llegada del ferrocarril y la radicación de numerosos inmigrantes españoles, germanos, italianos y árabes (sirios y libaneses en particular) la economía local adquirió nuevo vigor.

Según el censo 2001, Salta contaba con , lo que representa un crecimiento del 25,7 % frente a los del censo anterior, en tanto que su área metropolitana, denominada Gran Salta, con , se constituye hasta el momento como la 8.ª urbanización de Argentina por magnitud poblacional. Para el Censo 2010, Salta contaba con .

A principios de 1580, existían tres ciudades de españoles en el actual noroeste argentino: Santiago de Estero, Tucumán y Nuestra señora de Talavera. Estas se encontraban ubicadas en las llanuras, lejos de los habitantes hostiles de esas tierras. En 1582 se funda la Ciudad de Lerma en el Valle de Salta, cuyo objetivo era la conquista de los habitantes que se rebelaban contra la corona. Más adelante en el 1770, el objetivo de la ciudad de Salta pasaría a ser más importante, actuando como pasaje obligado al Alto Perú durante el Virreinato.

La traza de la ciudad fue un damero de nueve por cuatro manzanas, con la plaza ubicada al norte del centro geométrico. Este trazado urbano se fijó para proteger la ciudad al norte con el pantano o del Tineo, actual Avenida Belgrano, y al sur con el río Sauce, actual Avenida San Martín.

Para referirnos a la arquitectura de este periodo, que va desde los primeros años de la ciudad hasta el 1700, nos concentraremos en los edificios icónicos de ese momento, como ser el edificio del Cabildo, a partir del cual se toman ciertos elementos estilísticos que dan a la unidad de conjunto de la arquitectura colonial en Salta.

El principal edificio civil en la ciudad de Salta es, sin dudas, el Cabildo. Tiene la importancia especial de ser el cabildo virreinal argentino que se ha mantenido y conservado en el mejor estado.

La estructura general del edificio es la típica dos pisos de arquería con torre al centro. Detrás del volumen delantero de dos pisos de arcos se abren dos patios rodeados por galerías de arcos. Su expresión arquitectónica se caracteriza por la teja la baldosa cerámica, la madera de algarrobo en ménsulas y vigas.

El estilo, los detalles y los materiales utilizados en el Cabildo, fueron modelo para las construcciones siguientes, como lo fueron la aparición temprana, siglo XVII, de muchas viviendas de calidad, rasgo particular de la Salta virreinal.

Durante el periodo del Virreinato, comprendido entre 1778 y 1810, la capital de Salta fue un actor primordial como paso obligado entre el puerto de Buenos Aires y el Alto Perú. Era sede militar y centro de provisiones para las campañas. 

Al ser un centro de paso, el caudal de gente que por allí pasaba era grandísimo. Así fue como la ciudad comenzó a conocerse por el resto de las provincias argentinas y por los países limítrofes que comerciaban entre sí. 

En estos tiempos, la arquitectura de la ciudad se mantuvo intacta, con su coherencia estilística y unidad de conjunto, con la repetición de volúmenes, proporciones, el uso de materiales como la teja, la madera y el ladrillo.

Sucedió así por una regulación del gobernador en ese momento que pautaba la preservación de las edificaciones tal y como estaban desde sus primeros años, manteniendo las fachadas y la estampa colonial de la ciudad. Esa regulación, años más tarde, seria llevada al Reglamento Municipal de Construcciones, premiando a quienes cumplían y sancionando a quienes decidían cambiar el aspecto de sus casas.

De este decreto surge el apodo de la ciudad como «La Linda», ya que el haber preservado su aspecto como ciudad unificada bajo un estilo arquitectónico, la volvía distinta, más bella, que las demás provincias de Argentina.

Hasta el momento se analizó la evolución arquitectónica, hasta el 1800, de esta ciudad, cuyos rasgos fueron preservados casi en su totalidad hasta el día de hoy. Ya hemos hablado de que su estilo primordial es el colonial, pero a partir de mediados siglos XIX y principios del XX, surge un movimiento o nuevo estilo, el neo-colonial. 

Como lo indica su nombre, sus rasgos característicos tenían mucho que ver con la arquitectura colonial, por lo que fue confundido y se le adjudicó ese nombre. Pero cabe aclarar, que luego de estudios y de observaciones a obras que correspondían a este periodo, se llegó a la conclusión de que no hubo tal estilo, sino que fue la apropiación de estilos europeos que se tomaron como colonial por la semejanza del uso de elementos y materiales. Dice el Presidente de la Comisión de Preservación de Patrimonio Arquitectónico y Urbanístico, Guillermo Matach":" Salta tiene muchos rasgos de la arquitectura de la colonia, pero no es puramente colonial, como mucha gente cree"."

Ahora bien, podemos definir a lo que llaman arquitectura neo-colonial. El arquitecto Roque Gómez la define como el conjunto de teorías, proyectos y realizaciones que se concretaron en las primeras décadas del siglo XX, tomando como modelos, en principio, las obras producidas durante la dominación española en América y en la propia España.

En función de esto, indica que más que como un estilo es necesario definirlo como un movimiento historicista, con un sustento ideológico, producto de la visión de un determinado momento y también de elementos dictados por la moda. Frente a ello, se afirma el carácter necesariamente ecléctico de este movimiento que parte del intento de recuperación del hispanismo. Así se puede considerar a esta arquitectura como una mera copia del pasado.

En la ciudad de Salta fue eso exactamente lo que sucedió, y así se construyeron edificios de gran porte y valor como La Basílica de San Francisco en 1880, ya perteneciente también al movimiento neo renacentista.

A partir del siglo XX, se introduce un nuevo movimiento arquitectónico en gran parte de América, denominado racionalismo, cuyos principios básicos fueron: la predilección por las formas geométricas simples, con criterios ortogonales, empleo del color y del detalle constructivo en lugar de la decoración sobrepuesta, concepción dinámica del espacio arquitectónico, uso de materiales como el acero, el hormigón o el vidrio. Un gran maestro de esta arquitectura fue Le Corbusier, y de allí es donde surge el primer ejemplo de la introducción de este estilo arquitectónico en Salta, generando un cambio, un choque, o tal vez insertando una nueva perspectiva sobre la convivencia de estilos de ayer y hoy.

En la actualidad existen numerosas instituciones que cultivan las letras, la pintura, la escultura, danzas, la música y otras expresiones artísticas.

A mediados del siglo XX había en Salta un interesante movimiento literario cuyos más famosos exponentes fueron Vicente Solá y los poetas Juan Carlos Dávalos, Manuel J. Castilla, José Ríos y Walter Adet. En el área de la música se puede citar a Jaime Dávalos, Eduardo Falú, Gustavo "Cuchi" Leguizamón, Dúo Salteño, Los Chalchaleros, Los Fronterizos, Los Cantores del Alba, Hernán Figueroa Reyes, Dino Saluzzi, Daniel Tinte, Julia Elena Dávalos, y Los Nocheros, como algunos de los exponentes de la música local. Un grupo de artistas y escritores dan recitales poéticos callejeros, organizados por el movimiento Joaquina Cultural, en la Avenida San Martín al 1200, una zona típicamente bohemia. Los poetas Lucio Walter Erazú y Aníbal Aguirre son el motor principal de este grupo, entre otros autores salteños. En esta zona también se encuentra el mítico Boliche Balderrama y el Paseo de los Poetas, obra realizada sobre el canal de la calle Esteco. Por otra parte, en el ámbito de la música clásica, destacó a lo largo de décadas la figura del profesor de piano Alberto Prevot, que formó a generaciones de pianistas en la ciudad.

La Orquesta Sinfónica de Salta es otra expresión cultural de la ciudad. El Centro Cultural América y el Museo de Arte Contemporáneo (MAC), ambos frente a la plaza principal de la ciudad, ofrecen un variado calendario de exposiciones de artistas plásticos y otros eventos a lo largo del año.

Existen numerosos grupos de teatro y títeres en la Ciudad de Salta, a continuación algunos de ellos: Teatro de Títeres Leomar, Stress, Teatro Comunitario Alas, La Sardinera del Norte, Sensaciones, Manicomedia, Peso Neto Teatro, El Cofre, La Banda de los Notables Cuchufletos, El Ucumar, La Patota Teatral, Las Tablas, El Eje Teatro, Espacio Inverso, Arpi, Majarete, Bajofondo, Identikit, Espacio, La Suripanta, El GIT, NN Desaparecidos, El Umbral, El Altillo, La Faranda, Los Títeres de Gabriel Castilla, Títeres de Guiñol y Boca, La Tiendita Mágica, La Luna Mimosa, A punto de ser pez.

Existen tres generaciones definidas en este campo artístico, la primera integrada por actores como Rafael Monti, Mari Gerbino, Ana María Parodi, Eduardo Siuffi, José Antonio Lázzari, Elsa Mamaní, Cecilia Sutti, Alma Canobio, Nena Córdoba entre muchos otros; la segunda integrada por Antonio Muñoz, Danny Veleizan, Claudia Mendia, Cristina Idiarte, Carol Beltran, Pablo Aguierre, Pablo Dragone, Luciana Rajal, Graciela Cruz, Cristian Villareal entre muchos otros; y finalmente la de los jóvenes, integrada por: Pía Carballo, Marité Cervera, Rocío Paredes, Manuela de la Cruz, Nicolás Obregón, Bernabé Bustos, Mariano Madrazo, Esteban Trejo, Carolina Córdoba entre muchos otros. 

En el campo del arte cinematográfico se debe destacar la nueva camada de directores jóvenes, entre ellos Lucrecia Martel directora de "La ciénaga" (2001) y "La niña santa" (2003), y "Zama" (2017), entre otras y Rodrigo Moscoso, director de "Leo 16" (1997) y "Modelo 73" (2001).

También cuenta con espacios de arte independiente, como "Fedro", Galería de Arte, coordinado por los artistas plásticos María Laura Bucciantti y Roly Arias; "Mamoré", Galería de Arte, dirigida por Juan Blanco. "La Ventolera" es un espacio donde se realizan espectáculos de teatro, títeres, exposiciones, música en vivo y talleres, llevan adelante el proyecto Andrea García, Marianela Torino, Carmen Ruíz de los Llanos, Daniela Ulm, Ana Azurmendi, Juan de la Cruz, Cecilia Toconás y Silvia Martínez. "La Guarda", en tanto, se dedica a las artes visuales, conducido por Ana María Benedetti y Roxana Ramos. También se encuentra en la ciudad el Museo del Niño LEOMAR, que es itinerante e interactivo y que no posee sede física, sino que sus muestras van rotando por diversos museos, principalmente por el Museo Casa de Arias Rengel, Peatonal La Florida 20 y Museo Casa de Hernández, en Peatonal La Florida 97, el director es Marcelo Barrios y la productora Paola Vargas.

Todos los años, en ocasión del aniversario de fundación de la ciudad, se organiza el "Abril Cultural salteño", con exposiciones de arte, conciertos, veladas literarias y otros eventos de primer nivel.

En la ciudad de Salta existen dos centros de enseñanza universitaria.
La Universidad Nacional de Salta, fundada en 1973 y con más de 70.000 estudiantes, fue catalogada en 2009 como la tercera universidad de Argentina de mejor nivel de programas educativos.
Asimismo, la Universidad Católica de Salta, ubicada en el norte de la ciudad, erigida en 1963 y a la que asisten más de 40.000 estudiantes.

Salta cuenta con 2 centros de convenciones : El Centro de convenciones del centro cívico Grand Bourg y El Centro de convenciones de Salta que posee un salón principal que puede subdividirse en salones de menor tamaño gracias a placas divisorias especialmente diseñadas para tal fin; cuenta además con otras 6 salas de usos múltiples y un predio ferial de 200 hectáreas.

Al igual de lo que sucede en toda la República Argentina, por el art. 2 de la Constitución de la Nación Argentina el estado reconoce un carácter preeminente a la Iglesia católica que cuenta con un estatus jurídico diferenciado respecto al del resto de iglesias y confesiones y, a su vez, por el art. 14, la libertad de culto de todo habitante está garantizada.

Salta, basándose en la encuesta proporcionada por el CONICET respecto a la situación religiosa de acuerdo a las diferentes regiones argentinas en 2008, al ser parte del Noroeste argentino se destaca porque es la región donde más se profesa la religión católica de la Argentina con un 91,7% favorable y la menos atea con un 1,8%. 

Muchísimo más atrás, en segundo lugar se ubica el evangelismo con casi el 4%, región donde tiene menos peso.

La mayor parte de los salteños capitalinos se declaran profesantes de la religión católica. El arzobispo de Salta, actualmente es monseñor Mario Antonio Cargnello, titular de la Arquidiócesis de Salta.

Entre las fechas religiosas más importantes de Salta, además de las nacionales, están las fiestas de sus patronos El Señor y la Virgen del Milagro, festividad que se celebra el 13 y 15 de septiembre.

Siendo un importante aglomerado urbano del país, también cuenta con la presencia de otras comunidades religiosas como las iglesias evangélicas, la ortodoxa, y las comunidades judía e islámica, entre otras.

La comunidad judía cuenta con su propia kehilá, la Asociación Alianza Israelita Salta, así también con su sinagoga, escuela hebrea y cementerio judío.

El patrimonio histórico y monumental y los diversos espacios escénicos y culturales convierten a Salta en una ciudad receptora de turismo nacional e internacional.

El Aeropuerto Internacional Martín Miguel de Güemes es el principal punto de llegada de turistas extranjeros y nacionales a la provincia. Su segundo, a través de la Terminal de ómnibus de la Ciudad de Salta.

El alojamiento se realiza principalmente en la capital provincial desde donde pueden adquirirse con mucha facilidad diferentes tours (que se realizan en el día) a cualquier punto de la provincia, inclusive a la Quebrada de Humahuaca o Purmamarca (Provincia de Jujuy) y a la Ciudad de San Salvador de Jujuy (capital provincial).

Entre los hospedajes de la capital salteña se destacan por su categoría el Sheraton Salta Hotel, el Hotel Alejandro I (edificio más alto de la ciudad, con 70 m) y el clásico Hotel Salta, impulsado por la Secretaría de Turismo de la Nación e inaugurado en 1941 para fomentar el turismo en la región.

La Oficina de Informes del Ministerio de Turismo de la Provincia se ubica en la calle Buenos Aires N.º 93 (a 90 m de la plaza central); en el aeropuerto y terminal de ómnibus.

La mejor manera de recorrer los hermosos paisajes de Salta la linda es hacerlo en auto. Si no es auto propio se puede alquilar un auto ya que ésta provincia hay muchas empresas que ofrecen este tipo de servicio.



Para otros eventos con costumbres y tradiciones salteñas debe consultarse el calendario de actividades del Ministerio de Turismo de la Provincia a través de su sitio web oficial.

El Poder Ejecutivo municipal es ejercido por la Municipalidad de la Ciudad de Salta, cuya cabeza es el intendente capitalino, que se elige por votación mayoritaria. Actualmente el cargo es ejercido por Bettina Romero, que asumió el 10 de diciembre de 2019, sucediendo a Gustavo Sáenz, quien asumió como gobernador de la provincia ese mismo día. 

Por su parte, el Concejo Deliberante tiene a su cargo el Poder Legislativo. Al mismo lo conforman 21 concejales que se renuevan por votación cada dos años.

Salta está unida al resto del país por las rutas nacionales 9 y 34, en dirección a las provincias pampeanas y Buenos Aires, la 16, en dirección al Chaco, y la 51, que va hacia la cordillera de los Andes, y se conecta, en territorio chileno, con la ruta 23, en dirección a Antofagasta, en la costa del Pacífico.

En la Terminal de Ómnibus de la ciudad de Salta operan unas 20 empresas, con servicios diarios o semanales que unen a Salta con casi todas las provincias de la Argentina excepto Tierra del Fuego, como así también con ciudades de Bolivia, Paraguay, el sur de Brasil y el norte de Chile. El edificio comenzó a construirse en 1999, se paralizó durante un par de años y su primera etapa se inauguró en 2005. En enero de 2012 la Terminal sufrió el derrumbe de la plataforma por la que ingresan los colectivos, con varios heridos como resultado. Debido a la tragedia, se pudo saber que las columnas de la obra habían sido apoyadas sobre un viejo canal de desagüe. En febrero de 2013 fue inaugurada la reconstrucción de la estación, que cambió de concesionario.

Salta es terminal del Ferrocarril General Belgrano, que por el sur llega desde Buenos Aires por el Ramal C en Tucumán y al este se extiende hasta el puerto de Barranqueras, en Resistencia por el Ramal C y luego el RamalC12. Por el norte, el ferrocarril enlaza con el RamalC18 hasta La Paz (Bolivia) y por el oeste, enlaza con el RamalC14 a Antofagasta, Chile, en la costa del Pacífico

La mayoría de estas líneas dejaron de prestar servicios ferroviarios de pasajeros tras la Privatización ferroviaria en Argentina de 1991, realizada por Carlos Menem, pero continuando por unos años más de forma activa los trenes de carga, conservándose activa únicamente la línea a Buenos Aires y el Ramal C14 con pocos trenes de cargas por el Paso Socompa (Salta) y con fines turísticos hasta el Viaducto de la Polvorilla con el famoso "Tren a las Nubes". En diciembre de 2007 se reactivó la línea a Barranqueras con uno o dos trenes semanales, cuando en los 80' eran más de 40, y el gobierno provincial realiza intensas gestiones para reactivar la línea a Buenos Aires con trenes de pasajeros que, esta bastante deteriorada. 

Desde el mes de julio de 2012 hay un servicio frecuente desde la Estación Central Salta hasta la Estación de Gral. Martín Miguel de Guemes, utilizando dos duplas ferroviarias "Apolo" adquiridas a España por el gobierno argentino con 4 frecuencias diarias, dos ascendentes y otras dos descendentes. En junio de 2015 se anunció la construcción de dos triplas para aumentar dicho servicio encargadas a la ferroviaria cordobesa Materfer.

El Aeropuerto Internacional de Salta es el aeropuerto de mayor tráfico del Noroeste Argentino y el cuarto en el interior del país después de los de Córdoba, Mendoza y Bariloche. Cuenta con servicios regulares desde y hacia Buenos Aires (Aeroparque, Ezeiza, El Palomar (EPA)), Córdoba, Rosario, Jujuy, Mendoza, Bariloche, e Iguazú. Cuenta con frecuencias regulares internacionales a Santa Cruz de la Sierra(Bolivia), Panamá, Lima (Perú) y Asunción del Paraguay. Este abanico de ofertas se incrementa a través de vuelos chárters internacionales en temporadas de alta demanda a Floreanópolis, Maceio, Brasil, Puerto Seguro, San Salvador de Bahía, Iquique y Punta del Este. El aeropuerto de Salta se encuentra en estos momentos en funcionamiento las 24 horas. Durante el 2019, la aerolínea de bajo costo Flybondi suma a su frecuencia diaria a El Palomar, frecuencias regulares low cost desde y hacia Córdoba y Rosario (Santa Fe). En ese mismo año, la flamante Norwegian Argentina anuncia una frecuencia diaria de bajo costo con destino Aeroparque, servicio que presta con su cuarto Boeing 737-800.

El sistema de transporte urbano e interurbano en la ciudad y el su área metropolitana está concesionado a la empresa provincial SAETA. Sus unidades cuentan con seguimiento por GPS, lectoras de tarjetas para el pago. y pantallas LED en algunas de ellas. Además, en Salta hay un servicio de taxis y una gran cantidad remises.

La ciudad se divide en numerosos barrios con una marcada diferencia social distribuidos en:

Es el centro histórico, financiero y comercial de la ciudad. Lugar donde se originó la ciudad en el año 1582, es allí también donde se encuentran los edificios más antiguos como el Cabildo, la Iglesia Principal, la Basílica Menor de San Francisco, el Convento de San Bernardo (Salta),la "Casa de la Cultura (Salta)", entre muchos otros. Su forma casi perfecta de damero se ve alterada tan solo por la sinuosa Avenida Belgrano, trazada así pues por allí corría antiguamente el curso de un río. Las calles peatonales Alberdi, Florida y Caseros son esencialmente comerciales. La calle Balcarce ubicada entre la Estación Central del Ferrocarril y la Avenida Entre Ríos es, actualmente, el centro de la vida nocturna con incontable cantidad de reconocidas discos, restaurantes y bares.

Tres Cerritos, ubicado al noreste, llamado así por tres montículos naturales de tierra, de los cuales tan solo queda uno en Chachapoyas, Parque General Belgrano, El Huaico, Universitario ubicado alrededor de la Universidad Nacional de Salta, Castañares y Ciudad del Milagro. 
Barrio San Antonio, uno de los más antiguos de la ciudad, conocido por su club de fútbol homónimo, y por su cercanía al centro de la ciudad. Barrio Hernando de Lerma, famoso por su club de fútbol, Juventud Antoniana; Villa Cristina, célebre por sus comparsas y murgas de carnaval;Barrio Municipal, visitada por su ex matedero, actual centro municipal cultural; Villa las Rosas, conocida por su anual celebración del pesebre viviente; Barrio Casino, donde se encuentra el Balneario Municipal; el conjunto de barrios Miguel Ernesto Aráoz, El Tribuno, Intersindical, Limache, Santa Ana, surgidos al sur del Río Arenales, modernos y bien desarrollados; San Carlos, Bancario barrio recientemente creado donde antiguamente se ubicaban los terrenos de una fábrica de bórax, Juan Pablo 2, etc.

El Portezuelo, con su espectacular entrada arbolada a la ciudad, tras ascender por una rampa natural; allí comienza también el camino de ascenso al Cerro San Bernardo; aquí se encuentra el Autódromo donde todos los domingos hay carreras de autos; Villa Las Rosas, en donde se encuentra el penal. Aquí también se localiza el mítico Monumento a Güemes, la Cruz del Congreso Eucarístico y el distinguido Club 20 de Febrero. 

Plaza Alvarado, San Cayetano (con su santuario), Grand Bourg, El Tipal Country Club y La Almudena Country Club. En Grand Bourg funciona desde 1987 el Gobierno de la Provincia, que trasladó sus oficinas al lugar para descongestionar el centro salteño.

Salta posee grandes variedades de revistas y periódicos provinciales de diferentes ámbitos y, diferentes programas y canales de televisión.

En la ciudad de Salta, diariamente se edita el matutino "El Tribuno". También el "Nuevo Diario", un matutino independiente, y "Diario Punto Uno". Hay varias revistas mensuales, como Revista Emprender"Jaque", "Revista Tradición", "Revista Raíces" (del Club 20 de Febrero) y "ABC revista", así como un número considerable de sitios de noticias en línea, como "La Gaceta Salta", "El Intransigente" ", Primero Salta" y "Salta al Día".

Los canales de televisión abierta son (el primer canal de televisión de la provincia) Canal 7 Salta, Canal 3 "El aire de Salta", Canal 7, Canal 9, (surgidos recientemente) (Canal local: Somos Salta), Cablexpress y Salta Cable Color prestan servicios de televisión por cable. Hay canales de Televisión analógica que transmiten desde la Capital Federal son LW 82 TV Canal 11 (Telefe Salta) que pertenece a ViacomCBS y está afiliada a la gran parte de la programación de su señal de LS 84 TV Canal 11 (Telefe).


Las radios en FM son numerosas, con contenidos variados y dedicadas a diferentes públicos o géneros musicales. En AM, destacan LV9 Radio Salta y LRA4 Radio Nacional Salta Y FM destacan FM Aries,FM Identidad, FM Cielo, FM La Estación, FM Profesional, FM Pacifico, FM Universidad nacional de Salta, FM Capital, FM Noticias, entre otras.

El valle de Lerma, donde se encuentra la ciudad, goza de clima Templado con invierno seco y verano suave (Cwb en clasificación climática de Köppen) , con gran pluviosidad entre diciembre y febrero (promedio anual de 754,7 mm). A pesar de la creación de embalses masivos en la región donde se ha notado un importante aumento en la humedad del aire, no existe el más mínimo atisbo de aumento de las temperaturas; al contrario, las máximas temperaturas de la región son de las décadas de 1900 a 1940. En cambio, en el interior de las urbes salteñas, se han generado "oasis artificiales" debido a la isla de calor, producto de la urbanización. Entre el 16 y el 18 de julio de 2010, tuvo lugar la mayor nevada ocurrida en la historia de Salta desde 1582, con temperaturas mínimas de hasta -7 °C y máximas que no superaban los 0 °C.

En Salta, al igual que en el resto de la Argentina, el fútbol es el deporte más popular. Los principales clubes son Central Norte que participa en el Torneo Federal A y Gimnasia y Tiro y Juventud Antoniana en el Torneo Regional Federal Amateur. Otros clubes importantes son Mitre , San Antonio y Club Atlético Libertad. 
La Liga Salteña de Fútbol, es el ente que rige este deporte en la ciudad y cuenta actualmente con 18 equipos afiliados.

El segundo deporte más popular es el rugby. La selección de la Unión de Rugby de Salta es una de las más fuertes del país; y entre los equipos más representativos se destacan Gimnasia y Tiro (campeón del Torneo Regional del Noroeste en 1999), Universitario Rugby Club (Salta) , Jockey Club de Salta , y Tiro Federal de Salta.

Otros deportes con gran número de seguidores son: el béisbol (los jugadores locales figuran entre los mejores del país), hockey, básquet, vóley, natación, ciclismo y montañismo.

El equipo de Salta Basket, fue fundado en 2014.
En 2017 participó en La Liga(primera división del baloncesto) y en 2018 descendió. 
Actualmente disputa la segunda división nacional(segunda division del baloncesto argentino).

El equipo de vóley masculino de Central Norte, disputa la Serie A2 (Segunda División Argentina), luego de lograr el ascenso en 2015.

Otro deportista olímpico nacido en la ciudad, es el ciclista Daniel Diaz.

En cuanto al ámbito automovilístico, en el Autódromo de Salta se suelen disputar carreras como las del Turismo Carretera, TC 2000, Top Race y Turismo Nacional. En motociclismo, se destaca la presencia de Kevin Benavides, deportista reconocido internacionalmente.

Los fines de semana, millares de salteños ascienden a la cima del cerro San Bernardo para practicar aerobismo, ejercitarse y admirar el panorama de la ciudad. La ciudad cuenta con muchos circuitos para realizar actividades deportivas, con aparatos para realizar ejercicios en forma muy completa, son muy populares los que se encuentran a lo largo de la Avenida Virrey Toledo, continuación Avenida Reyes Católicos, a la par de Avenida Patrón Costas (acceso zona norte de la ciudad), Avenida Juan Domingo Perón (camino al municipio de San Lorenzo) al igual que la mayoría de las plazas que se encuentran fuera de la zona centro de la ciudad.

Salta cuenta con 5 estadios y 1 anfiteatro a cielo abierto:

El Estadio Padre Ernesto Martearena fue construido para ser una de las sedes de la Copa Mundial de Fútbol Juvenil del año 2001. Tiene una capacidad para 20.408 personas y es una de las sedes de los torneos de verano. En 2011, fue también designado como una de las sedes de la Copa América.

El Estadio Doctor Luís Güemes perteneciente al Club Atlético Central Norte, cuenta con una capacidad para 6000 personas, pero no está habilitado por AFA. Se inauguró en 1959.

El estadio del Club de Gimnasia y Tiro de Salta:, también denominado Monumental de la Vicente López, cuenta con una capacidad de 30.000 personas. Las Tribunas de la Bandeja y ex Virrey Toledo fueron reinauguradas el 20 de abril de 1994 y contó con la presencia de la Selección de fútbol de Argentina que se encontraba en etapa de preparación para la Copa Mundial de Fútbol de 1994.

El Estadio Fray Honorato Pistoia: perteneciente al Centro Juventud Antoniana. Cuenta con una capacidad de 10.000 personas y fue inaugurado en julio de 1930 en un partido Estudiantes de La Plata vs. Juventud Antoniana, el cual ganó Juventud.

El Estadio Delmi: (abreviación Del Milagro o conocido también como Polideportivo Ciudad de Salta), administrado por el gobierno provincial e inaugurado en 1986. Con capacidad para 10.000 personas sentadas se encuentra ubicado en la Avenida Ibazeta al 1400 en el cruce con calle O’Higgins (A su lado, cuenta además con el 
Microestadio cerrado con capacidad para 5.000 personas).

El Anfiteatro Eduardo Falú ubicado en Plaza España, es un anfiteatro a cielo abierto con capacidad para 8000 personas. El mismo cuenta con 2 canchas de básquet internas, cabinas de transmisión para periodistas, vestuarios, sanitarios y patio de comidas.

La trama de la ciudad se origina en las cinco por cinco manzanas del trazado hipotético original, donde se ubicaba la plaza central, la iglesia matriz y el edificio del Cabildo, aún en pie, luego de numerosas intervenciones a lo largo de siglos. Desde el centro la ciudad fue creciendo en dos sentidos fundamentales. Hacia el Norte, conectándose al camino que llevaba al Alto Perú, por la actual calle Balcarce. Hacia el Sur, vía de conexión con el Valle de Lerma y los Valles Calchaquíes, por la actual calle Florida.

El crecimiento de la trama respetó el damero hasta fines del siglo XIX, cuando comienzan a aparecer tramas quebradas, debido a las condiciones topográficas de la ciudad. Otro factor que afectará la trama de ciudad será para esta misma época la aparición del tendido ferroviario, que atraviesa en diagonal el macrocentro. Ya adentrado el siglo XX, aparecen barrios con tramas orgánicas, inspirados en los modelos de las ciudades jardines inglesas y americanas, en las laderas de los cerros, como el barrio Tres Cerritos. Hacia la década de 1960 y 1970, los conjuntos habitacionales hacen su debut en la ciudad de mano de los mandatos urbanísticos racionalistas, ubicándose ya en general en las afueras del macrocentro de la ciudad. Excelente ejemplo de esto es el Barrio Casino, al sur de la ciudad. Las ciudad se estructura a través de varias vías principales en sentido norte-sur en el cual tiene una extensión de aproximadamente 25 km.
Otro grave problema es el tránsito. La flota de más 600 colectivos, que a pesar de ser la más moderna de la Argentina, circula en partes de sus recorridos en estrechas calles; a lo que se suma la presencia de numerosos taxis y remises (más de 2100), el mal estado de algunas arterias, y el enorme caudal de autos en horas pico, provenientes principalmente del este y oeste por avenida Entre Ríos, desde el norte por Vicente López, Avenida Bicentenario de la Batalla de Salta y avenida Sarmiento y, desde el sur por avenida Monseñor Tavella y Pellegrini.

La sismicidad del área de Salta es frecuente y de intensidad baja, y un silencio sísmico de terremotos medios a graves cada 40 años.







</doc>
<doc id="5104" url="https://es.wikipedia.org/wiki?curid=5104" title="Cachopo">
Cachopo

El cachopo es un plato característico de la gastronomía de Asturias (España). Consiste en dos filetes de ternera grandes, entre los cuales se coloca jamón serrano y queso. El conjunto se come frito y caliente tras ser empanado en huevo, harina y pan rallado y se suele servir con guarnición de patatas, pimientos o champiñones. 

El término "cachopo" proviene de la semejanza que estos filetes empanados guardan con los troncos huecos de árbol, llamados "cachopos" (del latín "caccabum", recipiente), que se usaban como recipiente para guardar herramientas de labranza.

Las primeras referencias sobre el cachopo asturiano las recoge el doctor Gaspar Casal, a primeros del siglo . En el libro "El libro de cocina", publicado por la gastrónoma Adela Garrido en 1938, el plato aparece denonimado como "filete a la asturiana". Su popularidad se produce en la década de 1950, a raíz de que en 1947 el restaurante Pelayo de Oviedo— lo incorpora a su carta. Como dice el crítico gastronómico José Ignacio Gracia Noriega, «el cachopo era un plato antiguo y de sobra conocido por la burguesía asturiana de comienzos del siglo pero su prestigio definitivo no llega hasta finales de los cincuenta o principios de los sesenta gracias al buen hacer de algunos restaurantes de Oviedo y Grado, fundamentalmente».

A partir de esta forma original han surgido múltiples variantes de cachopos de pescado, pollo o cerdo y rellenos de marisco, cecina, setas, pimientos, quesos, espárragos, etc. 

Es habitual en los últimos años la publicación de guías y rutas de restaurante donde mejor se sirven los cachopos. También es habitual la realización de jornadas del cachopo, de entre las que destacó el mes del cachopo realizado en Madrid en noviembre de 2016.

Desde 2012 se vienen celebrando distintos campeonatos regionales de cachopos en varias regiones de España, de entre los que destaca el concurso de cachopos de Asturias, organizado por la "Guía del cachopo".




</doc>
<doc id="5105" url="https://es.wikipedia.org/wiki?curid=5105" title="San Ramón de la Nueva Orán">
San Ramón de la Nueva Orán

San Ramón de la Nueva Orán, más conocida como Orán, es una comunidad autónoma del norte de Argentina, en la provincia de Salta. Es cabecera del departamento Orán, en el norte de la provincia. En el censo de 2010 registró una población de 82.413 habitantes, de los cuales 76.070 habitan la zona urbana. Este hace que la convierta en el 2º mayor centro urbano de la provincia y uno de los . Fue fundada el 31 de agosto de 1794 por el español Ramón García de León y Pizarro, quien la bautizó como "San Ramón de la Nueva Orán" por ser esa fecha el día de San Ramón Nonato, y por haber nacido él mismo en la ciudad argelina de Orán.

Orán está situada en una zona de clima tropical (se halla 33 km al norte del Trópico de Capricornio), de grandes ríos, principalmente dedicada a los cultivos comerciales.

El centro urbano se encuentra a 32 km al sur de la frontera boliviana, a unos 3 km al oeste del caudaloso Río Bermejo, y a escasos 20 km al sur de la confluencia del Río Bermejo con el Río Grande de Tarija, siendo atravesado el ejido de la ciudad por la ruta nacional RN 50.

Los primeros habitantes de la zona fueron los churumatas, mataguayos (emparentados con los wichís) y quechuas, dedicados a la caza, la pesca en los ríos circundantes y el cultivo de algunas especies. En ocasiones se unían en alianzas y en otras se producían violentos conflictos por el control de los recursos. 

Durante el siglo XVII y gran parte del siglo XVIII los españoles hicieron denodados esfuerzos por apoderarse de la zona estableciendo una población, enfrentándose a la tenaz resistencia de los pueblos originarios.

Así fue que en 1625, en el lugar llamado "El Ramal" (o "La Enramada" o "Junta de los Ríos"), los españoles hicieron un primer intento de asentamiento poblacional estableciendo la efímera villa de Santiago de Guadalcázar. Pero el poblado fue atacado y destruido por pobladores de la nación aymara en 1629. 

Más de un siglo después, en 1779, fue fundada en la zona una reducción por parte de franciscanos procedentes de San Bernardo de Tarija; los misioneros llamaron a la nueva localidad "Nuestra Señora de las Angustias del (río) Zenta" e introdujeron el cultivo de la vid, la caña de azúcar, los cítricos y otras especies frutícolas. Adjunto a la reducción las autoridades españolas establecieron el fortín "San Andrés". Este primer núcleo persiste con el nombre de Misión Zenta a unos 5 km al oeste del centro comercial de la ciudad. Esta segunda fundación resultó también destruida por los indígenas aproximadamente diez años después.

El apoderamiento definitivo ocurrió el 31 de agosto de 1794, cuando el militar y noble español Ramón García de León y Pizarro, mientras era gobernador de la Intendencia de Salta del Tucumán, fundó San Ramón de la Nueva Orán, dedicando el nombre de la ciudad a San Ramón Nonato, cuyo santoral se celebraba ese día, y a la ciudad norteafricana de Orán, en donde García de León y Pizarro había nacido. La nueva población reforzaría así el nexo directo entre la ciudad de Salta y la villa de Tarija. En la Biblioteca del Congreso de los Estados Unidos se conserva un mapa de la región de Orán al momento de fundarse la ciudad, en 1794, probablemente confeccionado por García de León y Pizarro o alguien de su entorno.
Una Cédula Real aprobó la fundación el 4 de mayo de 1797, asignándole como su territorio jurisdiccional:
En 1810 estando aún vivo García de León y Pizarro las fuerzas patriotas argentinas liberaron la ciudad y debieron encarcelarlo, ya que se mantenía realista; García de León y Pizarro fue confinado en la ciudad que fundara, y falleció luego en Chuquisaca en 1815. Su fidelidad al rey de España le significó el confinamiento y las peores penurias. 

Durante la Guerra Gaucha librada contra los realistas, San Ramón de la Nueva Orán fue uno de los principales puntos de operaciones, esto hasta inicios de 1825.

En 1836 la población fue atacada por las tropas de la Confederación Perú-boliviana, durante la Guerra entre la Confederación Argentina y la Confederación Perú-Boliviana. 

Hacia 1880 el gobierno central argentino estudió la propuesta, originada en Salta, de crear un territorio nacional separado de la provincia de Salta con capital en San Ramón de la Nueva Orán, el que incluiría sectores del Chaco central y austral, pero la iniciativa no prosperó.

A pesar de que el territorio en donde se encuentra edificada San Ramón de la Nueva Orán es de media sismicidad, en la década de 1870 sufrió dos importantes sismos: el del 9 de octubre de 1871, que devastó la localidad de Orán, con numerosas víctimas, y el del 6 de julio de 1874, que la destruyó nuevamente y causó el éxodo de parte de su población.

Tras estas catástrofes, la localidad dedicó las últimas décadas del siglo XIX a su recuperación, y bien entrado el siglo XX el progreso se hizo rápido e inexorable. Posteriormente llegó el ferrocarril, se construyó la catedral, se inauguró el aeródromo y se creó la sede regional de la Universidad Nacional de Salta. Era la tercera urbe más populosa después de la ciudad de Salta y Tartagal, hasta que en el censo de 1980 superó a esta última, posicionándose desde entonces como la segunda ciudad de la provincia. Ese mismo año fue inaugurado el Hospital San Vicente de Paul, que con su más de 1 ha se encarga desde entonces de la atención de pacientes de la ciudad de Orán.

El clima es tropical con estación seca. El clima de Orán es del tipo clima subtropical húmedo con invierno seco (verano cálido) "Cwa", de acuerdo con la clasificación climática de Köppen. En verano las altas temperaturas y humedad constantes tornan el clima muy desconfortable; durante el invierno se produce la estación seca con escasas precipitaciones y temperaturas frescas a confortables. Las precipitaciones rondan cerca de los 1000 mm anuales. En cuanto a las temperaturas, las máximas promedian los 32.4 °C en el mes de enero (verano) y las mínimas llegan a 8.9 °C en julio, durante el invierno.
La Ciudad de Orán (como comúnmente se la conoce) es el centro geopolítico más importante del norte de la provincia de Salta. Posee oficinas de la AFIP, ANSES, Banco Nación, INTA, Juzgado Federal, entre otros, lo que la convierte en un punto de referencia de toda la Región del Bermejo. Tiene un activo centro comercial y es punto de tránsito hacia la frontera boliviana. En la actualidad existe una autopista hacia el sur que la conecta con la ciudad de Pichanal.

San Ramón de la Nueva Orán es centro de una importante región agroindustrial: caña de azúcar, destinada en su mayor parte a la producción de azúcar en el ingenio azucarero del Tabacal, localidad cercana a la ciudad; cítricos, principalmente naranjas y pomelos, siendo los primeros utilizados para la fabricación de jugo concentrado en la empresa frutícola Zenta, que también realiza la venta de las frutas al natural; pimientos, bananas, chirimoyas, papayas y mangos. En la zona también se cultiva café, que en su momento fue explotado por la empresa Salta Café, aunque con una producción relativamente pequeña. También la soja es de las más importantes.

Rodeada naturalmente de forestas ecotónicas entre las selvas de yungas y el bosque tropical chaqueño el entorno de San Ramón de la Nueva Orán ha sido y es una importante área forestal con industria maderera.

En el departamento Orán se encuentra la producción de porotos más grande del país y recientemente se incorporó la producción de soja, todo esto con visión, junto con la producción de caña de azúcar y maíz, a fin de producir biocombustibles principalmente para uso agroindustrial, debido en gran parte a la deficiencia en el abastecimiento de gasoil en la zona.

La ganadería ocupa también un renglón de importancia. Históricamente ha sido una escala en una de las rutas de ganado vacuno y equino hacia Bolivia.

Aunque San Ramón de la Nueva Orán tiene ya más de 200 años de antigüedad, posee pocos edificios antiguos, por la destrucción sísmica. Un colegio de estilo colonial y la catedral, moderno edificio de sobrias líneas resaltadas por el alto y esbelto campanil, son las construcciones más interesantes. Llama también la atención del visitante el arbolado público de la ciudad, con especies autóctonas muy floridas. Se destacan los lapachos, seguidos de guayabos, tarcos, yuchanes, tipas, entre otros, a los que se suman los naranjos.

En un radio de 50 km a la redonda, el turista puede visitar las ruinas de la antigua misión de la ciudad de Orán, y los numerosos arroyos y ríos que le son próximos, donde es posible practicar la pesca deportiva en el río Bermejo y en territorio boliviano. Todos aptos para el turismo de aventura, encontrándose la ciudad en el linde oriental de la reserva de biosfera de las Yungas del programa MaB, región propicia para la práctica de ciclo turismo, mountain bike y moto enduro.

La ciudad es sede de un centro regional de la Universidad Nacional de Salta. La "Casa de la Cultura", creada en 1983, es también un importante centro de divulgación cultural, dedicado al teatro, la música y todo tipo de exposiciones y conferencias.

Esta ciudad es sede del distrito judicial del norte, creado en 1957. A este primer distrito se le designaron cuatro magistrados judiciales de amplia trayectoria en la Justicia de Salta: Simón Ernesto Yazlle (juez de Primera Instancia en lo Civil y Comercial), magistrado de amplia actuación social que ocupó cargos relevantes en la justicia; Alfredo Ricardo Amerisse (juez en Primera Instancia en lo Penal), de trayectoria judicial, que llegara a cargos importantes como el de juez de Cámara en la Ciudad de Salta; Germán Bernard (fiscal en lo Civil y Comercial), catedrático de amplia trayectoria judicial dentro de la provincia de Salta; y José Durval García Luna Taboada (juez defensor de Pobres, Menores, Incapaces y Ausentes), amigo personal de Robustiano Patrón Costas, de ideología conservadora, descendiente del mismo fundador de esta ciudad y del general Antonino Taboada, ocupó el cargo de asesor letrado, fiscal y juez en lo Penal y el Crimen en la Ciudad de Salta, así también como director del Registro Civil de Salta. El 23 de mayo de 1959 el Senado de la Nación brindó los acuerdos respectivos como aval de las designaciones y conformación del Distrito. Todos los años en esta ciudad se conmemora la creación de esta institución y es feriado judicial.

San Ramón de la Nueva Orán posee un Aero Club, que anteriormente recibía vuelos provenientes de la Ciudad de Salta. Además posee una terminal de ómnibus para conexiones terrestres nacionales e internacionales.

Contaba con , lo que representa un incremento del 36% frente a los del censo anterior.
La sismicidad del área de Salta es frecuente y de intensidad baja, y un silencio sísmico de terremotos medios a graves cada 40 años.

En el ámbito deportivo cuenta con un equipo en el Torneo Argentino B de cuarta división organizado por el Consejo Federal de Fútbol, órgano interno de la AFA que nuclea a los clubes indirectamente afiliados a la AFA, provenientes de las Ligas Regionales. El actual club es el denominado Club Deportivo Tabacal.
Otros equipos conocidos son el Club Atlético River Plate de Embarcación, club donde supo jugar Luciano Colombo, uno de los jugadores más destacados de esa ciudad junto a Gerardo Yecerotte, y el Club Atlético Independiente de Hipólito Irigoyen. Estos dos últimos son integrantes del Torneo Argentino C, más conocido como Torneo del Interior.

En cuanto a la jurisdicción eclesiástica católica, la ciudad es sede de la diócesis de la Nueva Orán (sufragánea de la Arquidiócesis de Salta). Sus parroquias son Nuestra Señora del Valle, San Cayetano, San José, Catedral de San Ramón Nonato, Santa Teresita del Niño Jesús y San Antonio de Padua.




</doc>
<doc id="5106" url="https://es.wikipedia.org/wiki?curid=5106" title="Tabacal">
Tabacal

Tabacal puede referirse a:



</doc>
<doc id="5109" url="https://es.wikipedia.org/wiki?curid=5109" title="Conferencia Europea de Administraciones de Correos y Telecomunicaciones">
Conferencia Europea de Administraciones de Correos y Telecomunicaciones

La Conferencia Europea de Administraciones de Correos y Telecomunicaciones (CEPT, siglas de su nombre en francés "Conférence européenne des administrations des postes et des télécommunications") es un organismo internacional que agrupa a las entidades responsables en la administración pública de cada país europeo de las políticas y la regulación de las comunicaciones, tanto postales como de telecomunicaciones. 

Fue fundada el 26 de junio de 1959. En aquella época las comunicaciones se prestaban en régimen de monopolio, casi siempre por una entidad pública que operaba tanto los servicios postales como los de telecomunicación. Eran las "PTT"s, Administraciones de Correos, Telégrafía y Teléfonía ("Poste, Télégraphe, Téléphone"). La CEPT agrupaba a estas entidades, que a través de ella generaban las normas que estandarizaban los aspectos comerciales, operativos, regulatorios y técnicos de su actividad. Aspiraba a unificar la posición europea ante los organismos mundiales de referencia, la Unión Internacional de las Telecomunicaciones (UIT, ITU en inglés) y la Unión Postal Universal (UPU).

Con la liberalización de las telecomunicaciones, estas actividades de la CEPT perdieron en gran parte su sentido. La separación de correos y telecomunicaciones se lo quitó tanto a las siglas PTT como CEPT, y la privatización de los operadores y la apertura de los mercados a la libre competencia se lo quitaron a la cooperación en aspectos comerciales, operativos o incluso técnicos, pues los nuevos operadores empezaban a ser potenciales competidores en sus mercados.

En 1988, la CEPT transfirió a ETSI (European Telecommunications Standards Institute) todas las tareas de normalización. Dejaban de llevarse en un club de operadores estatales, y pasaban a un organismo de estandarización más similar al de otros sectores productivos. 

Y en 1992, los nuevos operadores de correos y de telecomunicaciones europeos, ya separada una cosa de la otra, privatizados muchos de los de telecomunicación, y abiertos los mercados a la concurrencia, crearon sus propios foros de armonización: Post Europe para el correo, y ETNO (European Public Telecommunications Network Operators' Association, Asociación Europea de Operadores de Redes Públicas de Telecomunicación) para las telecomunicaciones.

La CEPT cambió entonces radicalmente sus objetivos y su composición. Dejó de ser un club de operadores para convertirse en un foro de organismos de regulación y política de las telecomunicaciones. Actualmente reúne a los que en cada país fijan las normas legales, la regulación del mercado y las políticas en materia de comunicación. En septiembre de 1995, una reunión plenaria definió los nuevos objetivos, que apuntan a armonizar a escala europea las actividades de los que normalizan y regulan el mercado, igual que antes aspiraban a hacerlo con las de los que operaban las redes y servicios.

Actualmente la CEPT tiene 45 países miembros, tras incorporar hacia 1995 a los de la antigua órbita socialista.


</doc>
<doc id="5110" url="https://es.wikipedia.org/wiki?curid=5110" title="ITU">
ITU

Itu o ITU puede hacer referencia a:

ITU Instituto de Teoría de la Arquitectura y Urbanismo, Facultad de Arquitectura Diseño y Urbanismo. Uruguay
Más información en 
http://www.fadu.edu.uy/itu/

</doc>
