<doc id="15421" url="https://es.wikipedia.org/wiki?curid=15421" title="Ciberpunk">
Ciberpunk

El ciberpunk (del original en inglés cyberpunk y cuya pronunciación es /'saɪbəʳpʌŋk/) es un subgénero de la ciencia ficción, conocido por reflejar visiones distópicas del futuro en las cuales se combinan la tecnología avanzada con un bajo nivel de vida. Originalmente el término "cyberpunk" fue utilizado para referirse al movimiento literario encabezado por Bruce Sterling, William Gibson y John Shirley que surgió durante la década de 1980 en el seno de la literatura de ciencia ficción, siendo empleado por primera vez en ese sentido por Gardner Dozois en 1984. Dozois probablemente se inspiró en el título de un relato de Bruce Bethke. El ciberpunk recibe su nombre de la adjunción del prefijo "ciber-" (relacionado con redes informáticas) al vocablo punk (en referencia a su carácter rebelde). En él, la ciencia (y sobre todo la informática y la cibernética) suele generar o interaccionar con algún tipo de cambio de paradigma social o cultural.

En las tramas del género ciberpunk, el argumento suele estar centrado en los hipotéticos conflictos entre "hackers", inteligencias artificiales y megacorporaciones, todo ello situado en un futuro cercano del planeta tierra. Este contexto se opone al de muchas narraciones clásicas de la ciencia ficción, como "Fundación" de Isaac Asimov o "Dune" de Frank Herbert, que generalmente se sitúan en un futuro distante y en planetas y estrellas extrasolares. Las distopías posindustriales del ciberpunk, sin embargo, acostumbran a estar marcadas por un desarrollo cultural extraordinario, y por la subversión en el uso de las tecnologías, que son explotadas en ámbitos nunca previstos por sus creadores. Según las palabras de William Gibson en la novela "Quemando cromo", «la calle encuentra sus propios usos para las cosas». La atmósfera del género también se inspira en el cine negro, y reutiliza técnicas comunes en las novelas policíacas. Entre sus primeros escritores notables se pueden contar William Gibson, Bruce Sterling, Pat Cadigan, Rudy Rucker y John Shirley. El término ciberpunk fue acuñado en los años 1980, y aún es utilizado hoy en día.

A diferencia de la ciencia ficción de la Nueva ola, que importó en su seno las técnicas y las preocupaciones estilísticas preexistentes en la literatura y la cultura, el origen primigenio del ciberpunk se sitúa en la ciencia ficción, antes de que aumentara su popularidad. A comienzos y a mediados de los años ochenta, el ciberpunk se convirtió en uno de los temas de modas de los círculos académicos, donde comenzó a ser objeto de investigación por parte del postmodernismo. Durante ese mismo período, el cine de Hollywood tomó interés por el género, incorporándolo a sus producciones de ciencia ficción. En las películas más influyentes del ciberpunk, como "Blade Runner", "The Terminator", "Ghost in the Shell (1995)", "RoboCop", "Total Recall", "The Matrix" o "Akira", se puede apreciar la continuación de los temas y estilos más importantes del género. Los videojuegos , los juegos de mesa y los juegos de rol ciberpunk, tales como "Shadowrun" o el apropiadamente nombrado "Cyberpunk 2020", ofrecen a menudo guiones fuertemente influenciados por las películas y la literatura ciberpunk. A partir de los años 1990, ciertas tendencias en los campos de la moda y de la música fueron calificadas como ciberpunk.

En el mismo periodo en el que escritores muy diversos comenzaron a trabajar con conceptos del ciberpunk, nuevos sub-géneros emergieron, centrándose en la tecnología y en sus efectos sociales de una manera diferente. Entre sus ejemplos están el steampunk, iniciado por Tim Powers, Kevin Wayne Jeter y James Blaylock, y el biopunk (o alternativamente ribofunk), en el que destaca Paul Di Filippo. Asimismo, algunas personas consideran novelas como "La era del diamante" de Neal Stephenson como el inicio de la categoría postciberpunk.

Los escritores ciberpunk tienden a emplear elementos de la novela policíaca "dura", del cine negro y de la prosa postmoderna para describir las características del lado clandestino de sus sociedades dominadas por la tecnología. Su visión de un futuro imperfecto puede ser vista como la antítesis del porvenir utópico que se anunciaba en las historias de la "Edad de Oro de la ciencia ficción", populares en los años 1940 y 1950.)

En la escritura ciberpunk la mayor parte de la acción ocurre en línea, en el ciberespacio; atenuando cualquier frontera entre la realidad y la realidad virtual. Un tropo típico en estas obras es la conexión directa entre el cerebro humano y un sistema de cómputo. El mundo dominado por los sistemas informáticos es representado como un lugar oscuro, siniestro, donde las redes de comunicación controlan todos los aspectos de la vida. Las corporaciones multinacionales gigantes han tomado el papel de los gobiernos como centros del poder político, económico y militar. La lucha entre un personaje marginalizado y un sistema totalitario es un tema común en la ciencia ficción (por ejemplo, la novela "1984" de George Orwell) y particularmente en el ciberpunk, aunque en la ciencia ficción convencional los sistemas totalitarios tienden a ser estériles, ordenados y controlados por el Estado.

Los protagonistas de la escritura ciberpunk generalmente son hackers, quienes son moldeados frecuentemente en la idea de héroe solitario que combate la injusticia: vaqueros, rōnin, etc. A menudo son individuos marginalizados que se encuentran envueltos en situaciones extraordinarias, más que científicos brillantes o capitanes estrella buscando intencionalmente avances o aventura, y no siempre son verdaderos “héroes”.

Uno de los personajes prototípicos del género ciberpunk es Case, de la novela "Neuromante" de William Gibson. Case es un "vaquero de la consola", un hacker brillante, que traiciona a sus socios del crimen organizado. Robado su talento con una lesión que lo deja lisiado; infligida en venganza por sus socios criminales, Case recibe una inesperada única oportunidad en la vida de ser curado con asistencia médica experta; pero a cambio de su participación en otra empresa criminal con un nuevo equipo. Como Case muchos protagonistas ciberpunk son manipulados, puestos en situaciones donde tienen poca o ninguna opción, y aunque ellos pueden verse en esto, no necesariamente llegan a estar más lejos de lo que previamente estaban. Estos anti-héroes –“criminales, parias, visionarios, desertores e inadaptados”– no experimentan el “camino de héroe” de Campbell como un protagonista de la epopeya homérica o una novela de Alexandre Dumas.
Ellos en cambio, traen a la memoria el investigador privado de la novela policíaca, que podría solucionar los casos más complejos, pero nunca recibir una recompensa justa. Este énfasis sobre los inadaptados y descontentos -que Thomas Pynchon llama el ""pretérito"" y Frank Zappa el ""olvido de la Gran Sociedad""- es el componente "punk" del ciberpunk.

El ciberpunk se sitúa como un defensor de la libre circulación de la información. Decididamente opuesto a los derechos de propiedad intelectual. Se trata de un acérrimo defensor de las tecnologías de cifrado para garantizar la privacidad así como del dinero electrónico y de todas las modernas tecnologías digitales, en general.

La literatura ciberpunk suele servir frecuentemente como una metáfora de las preocupaciones actuales sobre los efectos y el control de las corporaciones sobre las personas, la corrupción de los gobiernos, la enajenación y la vigilancia tecnológica. El ciberpunk puede ser entendido como una inquietud a los lectores y un llamado a la acción. Esto a menudo expresa el sentido de rebelión, sugiriendo que uno podría describirlo como un tipo de ciencia ficción contracultural. En las palabras del autor y crítico David Brin,

Las historias ciberpunk se han considerado a veces como pronósticos ficticios de la evolución del Internet. El mundo virtual conocido actualmente como Internet, aparece a menudo bajo varios nombres, incluyendo "ciberespacio", "la Red", "el Metaverso" o "la Matriz". En este contexto es importante observar que las descripciones más tempranas de una red global de comunicaciones aparecieron mucho antes de que la World Wide Web se incorporara al conocimiento popular, aunque no antes de que los escritores tradicionales de la ciencia ficción tales como Arthur Charles Clarke y en algunos comentaristas sociales como James Burke comenzaran a predecir que tales redes eventualmente se formarían.

El ciberpunk es también un movimiento contracultural. Como tal, tiene su origen en una tradición libertaria y una profunda desconfianza en el uso de las nuevas tecnologías que, si bien pueden proporcionar mayores niveles de comodidad y progreso, también pueden alienar al individuo y ayudar a controlarlo.

Del mismo modo que la fuerza estética del ciberpunk ha influido en otros géneros más allá de la ciencia ficción, la fuerza de sus futuros, claramente distópicos, ha influido en la sociedad modificando nuestro punto de vista acerca de las nuevas tecnologías. Así, siendo una de las funciones de la ciencia ficción alertar a la sociedad de los peligros de sus actitudes y de sus creaciones, el ciberpunk ha sido uno de los movimientos más exitosos dentro del género.

Sin embargo, el ciberpunk no es un movimiento reaccionario. No se posiciona contra la tecnología, sino contra determinados usos de la misma. Así, del mismo modo que los poderosos se valen de la tecnología para mantener su control sobre las masas, cualquier acción en contra de ellos deberá también contar con el uso de tecnologías sofisticadas.

Además de posicionarse contra las implicaciones negativas de la ciencia y la tecnología, el ciberpunk muestra situaciones que se producen en un escenario económico controlado por organizaciones cada vez más poderosas e influyentes, a la vez que alejadas de la ciudadanía. Se denuncia así una fractura social en la que los ricos y poderosos se valen de su dinero y poder para manipular a la sociedad mediante el control de la información.

Algo a tener en cuenta al analizar el ciberpunk como corriente social es que sus autores no se posicionan contra algo que será, sino contra algo que está siendo. Es esta cercanía de los contenidos lo que ha hecho este movimiento tan inquietante.

El editor de ciencia ficción Gartner Dozois es generalmente conocido como la persona que popularizó el uso del término "ciberpunk" como un tipo de literatura. El escritor Bruce Bethke acuñó el término en 1980 para su historia corta "Cyberpunk", aunque la historia no se publicó hasta noviembre de 1983, en "Amazing Stories", Volumen 57, Número 4.

El término fue rápidamente acogido como una etiqueta aplicada a los trabajos de William Gibson, Bruce Sterling, John Shirley, Rudy Rucker, Michael Swanwick, Pat Cadigan, Lewis Shiner, Richard Kadrey y otros. De estos, Sterling inició el movimiento, liderando la ideología, gracias a su fanzine "Cheap Truth" ("Verdad barata"). (Véase también los artículos de John Shirley sobre Sterling y Rucker).

Los elementos del ciberpunk están presentes en "Los Cantos de Hyperion" de Dan Simmons; el planeta Lusus posee muchas características del mundo distópico de Neuromante ("Neuromancer") y los niveles cibernéticos de la vida y la existencia de inteligencia artificial tienen obvias influencias de los trabajos de Gibson.

William Gibson con su novela "Neuromancer", es probablemente el más famoso escritor conectado con el término. El estilo enfático, la fascinación con la superficie, la «apariencia y sensación» de futuro y la atmósfera ya tradicional en la ciencia ficción son vistos como la ruptura y a veces como «el trabajo arquetípico del ciberpunk». Neuromancer fue galardonada con los premios Hugo, Nébula y Philip K. Dick. De acuerdo con el archivo de la jerga "La total ignorancia de Gibson acerca de computadoras y la cultura hacker actual le permitieron especular sobre el rol de las computadoras y hackers en el futuro de modo que ambas son desde entonces irritantemente ingenuas y tremendamente estimulantes".

Tempranamente, el ciberpunk fue aclamado como una ruptura radical de los estándares de la ciencia ficción y una nueva manifestación de vitalidad, sin embargo poco tiempo después surgieron muchos críticos para cambiar su estatus a «movimiento revolucionario». Estos críticos dicen que la ciencia ficción de la "Nueva ola" de los años 60 era mucho más innovadora en cuanto a estilo y técnicas narrativas. Además mientras el narrador de "Neuromancer" pudo haber tenido una “voz” inusual para la ciencia ficción, se pueden encontrar muchos otros ejemplos anteriores a este: la voz narrativa de Gibson, por ejemplo se asemeja a la del actualísimo Raymond Chandler en su novela El Gran Sueño (1939). Otros consideran que los rasgos considerados únicos del ciberpunk, de hecho se pueden encontrar en trabajos más antiguos de otros escritores, de los que podemos citar James Graham Ballard, Philip K. Dick, Harlan Ellison, Stanisław Lem, Samuel R. Delany e incluso William Burroughs. Por ejemplo los trabajos de Philip K. Dick contienen temas recurrentes de decaimiento social, inteligencia artificial, paranoia y líneas ocultas entre la realidad y una especie de realidad virtual; la película ciberpunk "Blade Runner" está basada en uno de estos libros. Humanos vinculados con máquinas son el cimiento de la novela "Wolfbane" de Frederik Pohl y Cyril M. Kornbluth (1959) y "Criaturas de luz y oscuridad" de Roger Zelazny (1986).

En 1994 el académico Brian Stonehill insinuó que la novela "El arco iris de gravedad" de Thomas Pynchon «no solo insulta, sino plagia a los precursores del ciberespacio».
Otros importantes predecesores incluyen a dos novelas muy celebradas de Alfred Bester, "El hombre demolido" y "Las estrellas mi destino", así como la novela de Vernor Vinge "Nombres verdaderos". En esta década el escritor brasileño Fausto Fawcett publica sus primeras novelas.

El escritor de ciencia ficción David Brin describe el ciberpunk como «(...) la campaña de promoción gratuita más fina emprendida a nombre de la ciencia ficción». Esto pudo no haber atraído a los «verdaderos "punks"», pero atrajo a muchos nuevos lectores y dispuso la clase de movimiento que la literatura postmodernista buscaba comentar (una ilustración de esto es el "Manifiesto Cyborg" de Donna Haraway, un intento de construir un «mito político» usando cyborgs como metáforas de la «realidad social» contemporánea). El ciberpunk hizo más atractiva la ciencia ficción para los académicos, argumenta Brin. Además hizo a la ciencia ficción más lucrativa para Hollywood y las artes visuales en general. Aun cuando su «importancia retórica y quejas de persecución» por parte de los aficionados al ciberpunk era irritante en el peor y chistoso en el mejor de los casos, Brin declara que «Los rebeldes pusieron las cosas patas arriba; estamos en deuda con ellos [...]». Pero, el pregunta "¿Fueron ellos originales?".

El futuro ciberpunk inspiró a muchos escritores profesionales que no se encontraban entre los ciberpunk "originales" al incorporar ideas ciberpunk en sus propios trabajos, tales como Walter Jon Williams con "Hardwired" y "Voz del torbellino", y George Alec Effinger con su obra "Cuando la gravedad falla". Mientras nuevos escritores y artistas empezaron a experimentar con ideas ciberpunk, nuevas variedades de ficción emergieron, a veces manejando el mismo nivel de crítica que las historias del ciberpunk original.Lawrence Person escribió en un ensayo publicado en el foro de Internet Slashdot:
El ensayo de Person aboga usando el término "postciberpunk" para etiquetar los nuevos trabajos que estos escritores producen. En esta visión, las historias típicas del postciberpunk continúan enfocándose en una atmósfera de datos ubicua de información computarizada y el aumento cibernético en el cuerpo humano, pero sin asumir la distopía. Buenos ejemplos pueden ser "La era del diamante" de Neal Stephenson o "Transmetropolitan" de Warren Ellis y Darick Robertson. Como todas las categorías incluidas en la ciencia ficción, los límites del postciberpunk son susceptibles de cambiar o ser mal definidos. Para complicar el asunto, hay un mercado continuo de novelas ciberpunk "puras" fuertemente influenciadas por el trabajo temprano de Gibson, como "Carbono alterado" de Richard Morgan.

En 1965, Jean-Luc Godard estrena Alphaville, un film de ciencia-ficción con elementos de novelas de ese mismo género, en la cual aparece un futuro distópico propio del ciberpunk, basado, probablemente en el que aparece en "Un mundo feliz" de Aldous Huxley.

La película "Blade Runner" (1982), adaptada del libro" ¿Sueñan los androides con ovejas eléctricas?" de Philip Kindred Dick, se ubica en una distopía futura en la cual seres manufacturados llamados replicantes (en la novela, «andys» o «andrillos» dependiendo de la traducción) son usados como esclavos en colonias del espacio, y en la Tierra presa de varios cazadores de recompensas, quienes se encargan de "retirarlos" (matarlos). Aunque "Blade Runner" no fue un éxito en su lanzamiento, encontró un gran nicho en el mercado de alquiler de películas. Puesto que la película omite los elementos religiosos y míticos de la novela de Dick (por ejemplo, cajas de empatía y Wilbur Mercer), cae más estrictamente dentro del género ciberpunk que la novela. William Gibson revelaría después que la primera vez que vio la película, se había sorprendido mucho de cómo la apariencia de esta película era similar a su visión cuando estaba trabajando en "Neuromancer". Aunque no fue hasta principios de los noventa cuando se consagró como un género de denominación popular, gracias a numerosas películas, entre las que destacan "Hardware" o "Death Machine".

Según lo mencionado anteriormente, la serie de televisión "Max Headroom" también expandió el ciberpunk, quizá con un éxito más popular que los primeros trabajos escritos del género.

El número de películas de este género, o por lo menos de uno de sus elementos ha crecido constantemente desde "Blade Runner". Varios de los trabajos de Philip Kindred Dick se han adaptado a la pantalla gigante, con elementos ciberpunk llegando a ser típicamente dominantes, los ejemplos incluyen "Screamers" (1996), "Minority Report" (2002), "Paycheck" (2003) y "Una mirada a la oscuridad" (2006).

Pero desafortunadamente para el argumento original, la película "Johnny Mnemonic" (1995) fue un fracaso, comercialmente y para la crítica. Los fanes de Gibson reclaman que el argumento se desvió sustancialmente del trabajo original, aun cuando Gibson mismo escribió el guion final.

El director Darren Aronofsky ubica su opera prima "π" (1998) en una Nueva York actual, pero construyó el libreto con influencias de la estética ciberpunk. De acuerdo con comentarios del DVD, él hizo esta producción usando deliberadamente máquinas antiguas (como el disquete de 5-¼ de pulgada), imitando el estilo tecnológico de "Brazil" (1985), para crear una "sensación" ciberpunk. Aronofsky describe el Chinatown, donde se ubica la película, como "el vecindario ciberpunk después de Nueva York".

Se encuentran otras películas prácticamente coetáneas a "Blade Runner", que también reflejan este mundo cyberpunk, como por ejemplo, "Cielo líquido" (1982), pues tiene un argumento de ciencia-ficción urbanita con protagonistas bastante marginales, también merece mención la película "Max Headroom" (1985), ya que en una época como los años 80, un argumento como el de esta película era muy llamativo, en ella, una inteligencia artificial tenía el papel protagonista, se convertiría, además, en una serie televisiva. Por otro lado, también encontramos la película "Días extraños (1995)" tratando un apocalipsis de realidad virtual en 1999, aunque fue un filme con muy poca acogida, lo mismo que ocurrió con la ya comentada película "Johnny Mnemonic." 

La serie "Robocop" se ajusta más al futuro cercano donde hay por lo menos una corporación, Omni Productos de Consumo, que es una empresa todopoderosa en la ciudad de Detroit. "Hasta el fin del mundo" (1991) muestra otro ejemplo donde el ciberpunk es el tema de fondo, y una estrategia de argumento, para verla de otro modo y dirigir el personaje de la historia. "Gattaca" (1997) dirigida por Andrew Niccol es un filme negro futurista cuyo empapado modo distópico provee un buen ejemplo del biopunk.

La serie "The Matrix", que inicio en 1999 con "The Matrix" (conformada también por "The Matrix Reloaded", "The Matrix Revolutions" y "The Animatrix") usan una amplia variedad de elementos ciberpunk.

El estilo ciberpunk y el diseño futurista han encontrado una gran acogida (y vasta exposición) en el anime, incluyendo "Akira" (primer referente anime del género) es un manga en el que también se basa la película homónima de animación japonesa. Ambas obras tuvieron un reconocimiento instantáneo como clásicos dentro de sus respectivos géneros. El manga, de más de dos mil páginas, fue escrito y dibujado por Katsuhiro Otomo entre los años 1982 y 1993 obteniendo un éxito significativo en Japón y en el resto del mundo. Premiada con el Premio Kōdansha al mejor manga en 1984 en la categoría general (一般部門). El largometraje homónimo se separa de la línea argumental del manga por causas claras: la película fue estrenada cinco años antes de la conclusión del manga. Akira se ambienta en la ciudad futurista de Neo-Tokio, representada con profundo detalle en la película de animación (se invirtieron cerca de siete millones de dólares solo en los decorados). Otros animes en abordar esta temática son: "Ghost in the Shell", este anime es uno de los que más importancia presenta dentro de este subgénero, presentando conexiones con el cine negro y reflexiones existencialistas sobre un mundo donde distinguir máquina de humano es cada vez más difícil.
Al lado de este potencial encontramos otras obras como "Cyber City Oedo 808", "Battle Angel Alita", "Bubblegum Crisis", "Armitage III", "Armitage Dual Matrix", "Silent Möbius", "Serial Experiments Lain", "Texhnolyze", "Appleseed", "Ergo Proxy", "" y "Psycho-Pass", siendo esta última la que más ha influenciado la juventud contemporánea japonesa que vive con una relativa cercanía a la ambientación de la serie, que muestra un Japón con tecnologías punta y que advierte sobre los riesgos que puede causar esto ante una posible perdida de identidad humana.

El anime también ha proporcionado ejemplos de los subgéneros steampunk, como es el caso del manga de CLAMP Clover, también en muchos de los trabajos de Hayao Miyazaki, pero también notablemente en "Last Exile" (2003), creado por el estudio Gonzo y dirigido por Koichi Chigira, que ofrece una curiosa mezcla de sociedad victoriana y batallas futuristas entre naves aéreas.

Este subgénero también sigue plasmándose en las películas y series de la actualidad, un ejemplo de ello es la película "Inception" del año 2010 dirigida por Christopher Nolan e interpretada por Leonardo Di Caprio, se puede relacionar con este subgénero ya que este versátil director sin tener que tratar la socorrida realidad virtual, hace entrar al espectador en una realidad lejana a la nuestra haciendo que el protagonista junto con sus secuaces se introduzcan en sueños ajenos, algo que encaja con los patrones de este subgénero a tratar.

"Blade Runner 2049", dirigida por Denis Villeneuve, es un claro ejemplo de ciberpunk en la actualidad, siendo una secuela de la aclamada "Blade Runner". Esta película nos sitúa treinta años después y nos muestra un futuro distópico y lleno de referencias a la cultura ciberpunk.

También puede destacarse la película "Dredd" del año 2012, dirigida por Pete Travis, ya que a pesar del poco éxito comercial, es un buen ejemplo de estas claves del ciberpunk establecidas, pues se representa un futuro violento y distópico.

El término «música ciberpunk» puede referirse a dos categorías algo superpuestas. Primero puede denotar la amplia gama de los trabajos musicales que las películas ciberpunk utilizan como banda sonora. Estos trabajos varían en género desde la música clásica y el jazz –usada en "Blade Runner", y que por otra parte evoca el ambiente del cine negro— hasta el noise y la música electrónica. Típicamente las películas hacen uso de la electrónica, electronic body music, música industrial, noise, futurepop, rock alternativo, rock gótico, Synthpop, retrowave, synthwave, vaporwave e intelligent dance music, derivados y fusiones para crear la sensación «apropiada». El mismo principio se aplica a los videojuegos. Por supuesto, mientras los trabajos escritos no están asociados a bandas sonoras con tanta frecuencia como las películas, la alusión a trabajos musicales es usada para el mismo efecto. Por ejemplo la novela gráfica "Kling Klang Klatch" (1992), una fantasía oscura sobre un mundo de juguetes vivos, donde un oso de peluche amargado tiene una adicción hacia el azúcar y una predilección por el jazz.

La «música ciberpunk» también describe los trabajos asociados con la tendencia de la moda que emergió del desarrollo de la ciencia ficción. El libro "Future Shock" de Alvin Toffler influyó tanto en los creadores del techno en Detroit a principios de los 80, como Juan Atkins y su grupo Cybotron, como a los pioneros europeos del sintetizador Kraftwerk, produciendo canciones de clara inspiración distópica. La banda canadiense de thrash/punk/progressive metal Voivod fue una de las primeras en autodenominarse ciberpunk. En los 1990, la cultura popular comenzó a incluir un movimiento en la música y en la moda que llamaron también "ciberpunk" y que llegó a ser particularmente asociada con las subculturas rave y techno. Con el nuevo milenio llegó un nuevo movimiento de bandas industriales que hacían música de "portátil". Punks y okupas se armaron con equipo digital y fusionaron la tecnología con sonidos callejeros. La subcultura hacker documentada en lugares como el archivo de la jerga contempla este movimiento con sentimientos encontrados, desde los autoproclamados ciberpunks que están frecuentemente "inclinados" hacia el cuero negro y el cromo quienes hablan entusiasmados de tecnología en lugar de aprender o verse involucrados en esto. ("La actitud no sustituye a la capacidad", entrada del Archivo). Sin embargo estos autoproclamados ciberpunks al menos están "emocionados con las cosas correctas" y típicamente respetan a las personas que actualmente trabajan con esto de "la naturaleza hacker".

El español José María Ávalos Oliveros, en su tesis de máster para la carrera de postproducción digital de la Universidad Politécnica de Valencia, llamada "Distopía musical: La música en el Cyberpunk" se sostiene que ningún compositor se amolda a unas reglas concretas para escribir o componer música ciberpunk. No hay un estilo preestablecido ni pautas a seguir. Cada músico aporta una novedad distinta, por lo general, en cada producción en la que hayan participado. También hay que tener en cuenta factores como la época en la que se compone la música, las canciones, en caso de haber, que se utilizan en la banda sonora junto al carácter comercial y la influencia que puede tener sobre otras producciones del género. 

Ciertos géneros musicales como el drum and bass fueron directamente influenciados por el ciberpunk, incluso generando un subgénero completo llamado neurofunk. Un claro ejemplo de la influencia ciberpunk en la música son la banda Sigue Sigue Sputnik y el video del tema de Duran Duran "Union of the Snake". El álbum de 1982 del grupo electrónico The Cassandra Complex, se llama "Cyber Punk". En la actualidad podemos decir que el género que representa a plenitud el espíritu ciberpunk es el Futurepop, de la mano de bandas como Mind.In.A.Box, VNV Nation, Rotersand, Covenant, Colony 5 y bandas de Synthpop como Neuroactive, Neuroticfish y Seabound. Estos grupos destacan por el intenso uso del Vocoder (sintetizador de voz) en sus canciones, ritmos bailables entre 120-140 bpm, letras futuristas, y melodías pegadizas que provoca un efecto adecuado a la atmósfera Cyberpunk

En Brasil destaca Fausto Fawcett, también escritor. Comenzó su carrera musical en 1986, por sugestión de uno de sus amigos de la facultad, el cineasta Cacá Diegues, y firmó con la Warner Music Group para lanzar su álbum debut, "Fausto Fawcett e os Robôs Efêmeros" (Fausto fawcett y los Robots Efímeros) al año siguiente. Descrito como una "obra conceptual sobre una Copacabana "Blade Runner"".

Los videojuegos usan frecuentemente el Ciberpunk como fuente de inspiración, algunos de estos como "Blade Runner" o Enter the Matrix, están basados en películas del género, mientras que otros como Deus Ex y System Shock, Final Fantasy VII, Mega Man o Snatcher son trabajos originales.Algunas franquicias multimedia entran al terreno de los videojuegos como el caso de Shadowrun o Ghost in the Shell, mientras que otras tienden a incluir ambiente y temas del género ciberpunk como un elemento más en torno a la construcción de su mundo y narrativa, como es el caso de la saga .hack, la trilogía de Xenosaga, algunos títulos de las franquicias Metal Gear y Megami Tensei, a la vez que novelas visuales como VA-11 Hall-A recurren al género como un medio tanto de homenaje como sátira a sus tropos narrativos. CD Projekt actualmente está desarrollando un título dedicado al género llamado Cyberpunk 2077.

Existen varios juegos de rol titulados "Ciberpunk": por ejemplo "Ciberpunk 2013", Cyberpunk 2020 y "Ciberpunk V.3" son las tres ediciones de un mismo juego, publicado por Talsorian Games, y existe también un suplemento para el sistema genérico GURPS ("GURPS Ciberpunk"), publicado por Steve Jackson Games.
Cyberpunk 2020 fue diseñado con el argumento de los escritos de William Gibson en mente, y hasta cierto punto con su aprobación, diferente de la aproximación (quizá más creativa) hecha por la "FASA" en la producción del juego Shadowrun. Ambos juegos se ambientan en un futuro cercano, en un mundo donde la cibernética es prominente.
"Netrunner" es un juego de cartas coleccionables introducido en 1996, basado en el juego de rol Cyberpunk 2020; fue lanzado junto a un popular juego de realidad alternativa en línea llamado "Webrunner", que permite a los jugadores ingresar al mainframe de una perversa organización futurista. También Iron Crown Enterprises lanzó un juego de rol, titulado Cyberspace, ahora descatalogado.

En 1990, en una inusual unión entre la realidad y la ficción del ciberpunk, el Servicio Secreto de los Estados Unidos llegó a las instalaciones de Steve Jackson Games y confiscaron todas sus computadoras bajo la "Operación Sundevil", que fue un masivo golpe a los hackers y crackers de computadoras. Esto se debió a que – supuestamente – el libro de "GURPS Ciberpunk" podría ser usado para preparar crímenes por ordenador. Esta, en efecto, no fue la principal razón para la redada, pero tras el evento ya fue muy tarde para corregir la impresión del público.
Más tarde "Steve Jackson Games" ganó el juicio contra el Servicio Secreto, ayudados por la Electronic Frontier Foundation, de mente más amplia. Este evento alcanzó algo de notoriedad, lo que se extendió también al libro. Todas las ediciones publicada de "GURPS Ciberpunk" contienen una cita en la cubierta que dice ""¡El libro que fue decomisado por el Servicio Secreto de los Estados Unidos!"". En su interior el libro provee un resumen de la redada y sus consecuencias.

El 2004 trajo numerosas publicaciones nuevas de juegos de rol Ciberpunk, destacó entre ellas "Ex Machina", un juego más cinematográfico con cuatro escenarios completos y enfocado en actualizar el lado lúdico del género a temas corrientes dentro de la ficción Ciberpunk. Estos cambios incluyen un mayor ángulo político, transfiriendo la alineación del género e incluso incorporando temas transhumanos. El 2006 vio la largamente esperada publicación de "Ciberpunk V.3" de Talsorian Games', la secuela de Cyberpunk 2020, sin embargo muchos la vieron más como una edición transhumanista o Postciberpunk que realmente Ciberpunk.

Los juegos de rol también han producido una de las más originales tomas del género en la forma de la serie de juegos "Shadowrun" de 1989. Aquí, el escenario es un distópico futuro cercano; sin embargo, también incorpora elementos de la fantasía y la literatura, como magia, espíritus, duendes y dragones. Las facetas ciberpunk de "Shadowrun" fueron modeladas en gran parte basadas en los escritos de William Gibson, y la FASA, quienes lo publicaron originalmente, han sido acusados por algunos de copiar el trabajo de Gibson sin siquiera mencionar su influencia. Gibson, mientras tanto, ha mostrado su desagrado por la inclusión de elementos de fantasía dentro de los escenarios que él ayudó a desarrollar. Sin embargo, "Shadowrun" ha introducido a muchos al género, y sigue siendo popular entre los jugadores.

El juego de rol "Torg", publicado por West End Games también incluyó una variante del escenario (o cosmos) ciberpunk llamado "Cyberpapado". Este escenario fue inicialmente una distopía religiosa medieval que repentinamente sufrió un surgimiento tecnológico. En vez de corporaciones y gobiernos corruptos, el Cyberpapado fue dominado por el “Falso Papado de Avignon”. En lugar de la Internet, los hackers navegan por la "GodNet", una red común de computadoras con directo simbolismo religioso, hogar de ángeles, demonios, y otras figuras bíblicas. Otro “cosmos” aparte del juego Torg fue "Nippon Tech", el cual incorporaba otros aspectos del ciberpunk como corporaciones dominantes con asesinos profesionales, sin embargo no incluye redes de computadores como parte fundamental del escenario.

El ciberpunk también ha sido usado en videojuegos de aventura para computadoras, destacan el ahora freeware Beneath a Steel Sky, publicado por Revolution Software, Neuromancer, publicado por Interplay en 1988, BloodNet, publicado por Microprose en 1993 y , por GameTek en 1994. También el ahora abandonware, Flashback. El videojuego de acción y aventura "Neuromancer" está basado directamente en la novela, incluyendo Chiba City, algunos de los personajes, hacking de bases de datos y plataformas ciberespaciales.

Este estilo llegó a influenciar el desarrollo de videojuegos y mods de disparos en primera persona. Algo que se puede apreciar, por ejemplo, en Neotokyoº , un mod del viedeojuego Half-Life 2 (cuya historia y complejidad lo clasificaron como un mod independiente), situado en un Japón futurista y donde se aprecian referencias a Akira, Ghost in the Shell y distintos rasgos que definen al género ciberpunk.

El primer libro cubano ciberpunk fue "Nova de Cuarzo" (1999), de Vladimir Hernández Pacín. Otra novela "cyber" publicada fue "Dioses de neón" (2002), de Michel Encinosa Fú. Uno de los exponentes más claros del Ciberpunk en Chile es Jorge Baradit, quien ha escrito las novelas Ygdrasil, Kalfukura y Synco, además de participar o promover proyectos artísticos como , Ucronía Chile y Lluscuma. Uno de los grupos españoles que se autodenomina ciberpunk aparece en Berlín en 1989 con autores de diversos fanzines underground que, en 1996, pasarían a publicar en la Web uno de los primeros ezines españoles. Tras constituirse como asociación en 2002 sus publicaciones evolucionarán hacia el ciberactivismo abandonando prácticamente la publicación de relatos. Literariamente la única aportación reconocida de este grupo han sido las primeras novelas escritas en español para teléfonos móviles: "Lía, MAD phreaker", de David de Ugarte y "BCN No Future" de Javier Lorente. En un contexto más futurista está "2123 El año de Moebius", con booktráiler de Ángel De Aluart. "El sueño del Rey Rojo", del autor asturiano Rodolfo Martínez, suele considerarse también dentro del género. El filósofo y escritor Jonás Barnaby, bajo el seudónimo Albert Mut, puede contarse entre las emergentes personalidades del género en los últimos años, con relatos claramente distópicos y tecnológicos como "La gallina temporal" o "Phobos B-101".
En el cine, pocas son las películas españolas que hacen uso de este estilo. Un ejemplo lo encontramos en "Natalie_Net", dirigida por Chico Morera que cuenta la historia de una videoblogger de fama que empieza a desarrollar características de computadora. La película muestra un ambiente insano, en un entorno distópico y atemporal en el que la tecnología, los virus informáticos y las relaciones humanas toman el protagonismo y muestran su lado más oscuro. 

En cuanto al desarrollo del movimiento en México, se considera que éste se introdujo por medio de la literatura y de allí partió para encontrar otros medios de expresión más populares, como la música. La primera obra literaria escrita en México y que puede enmarcarse dentro del ciberpunk es el cuento "La red" de Isidro Ávila. Sin embargo, la obra que se considera que originó el movimiento en México, fue una novela publicada un par de años después que el cuento de Ávila. "La primera calle de la soledad" (1994), del entonces joven Gerardo Horacio Porcayo, sirvió de ancla para que muchos escritores de ciencia ficción tomaran al género como algo suyo, y aunque el ciberpunk mexicano nunca terminó por germinar completamente, ha perdurado más de una década después de su nacimiento.

La primera novela de ciencia ficción que podría considerarse ciberpunk en Paraguay es "La Sociedad de las Mentes" (2001), de Juan de Urraza, que si bien contiene elementos utópicos que resultan disonantes con el género, en realidad los une al mundo virtual, sobre todo si se tiene en cuenta como un todo y se mira como unidad con su segunda novela "Yronia" (2005), que es la continuación de la misma.

Entre los subgéneros del ciberpunk está el steampunk que se ubica en una era victoriana ucrónica pero con una visión negra del mundo. El término fue acuñado originalmente en 1987 como broma para describir algunas de las novelas de Tim Powers, James Blaylock y Kevin Wayne Jeter, pero con el tiempo William Gibson y Bruce Sterling ingresaron en el subgénero con su novela en colaboración "La máquina diferencial", y el término fue empezado a tomarse en serio. El silkpunk sería un derivado de este último, con la diferencia de que se enfoca en un contexto ambientado en la China de la Dinastía Han. Siendo Ken Liu y su libro "La Gracia de los Reyes" un referente de esta tendencia.

Otro subgénero similar de aun muy reciente clasificación es el que se ha venido a llamar wirepunk, heredero del steampunk, que en lugar de tomar como partida el siglo XIX, se centra en la tecnología del siglo XX, ahora que ya supone un tiempo pasado. Un ejemplo claro es la saga literaria de Jeanne DuPrau iniciada con "City of Ember".

Los inicios de 1990 vieron el nacimiento del biopunk, un estilo derivado construido no sobre la base de la tecnología sino sobre la biología. En estas historias la gente es cambiada de varias formas, pero no por medios mecánicos, sino por manipulación genética de varios de sus cromosomas. Paul di Filipo es visto como el más prominente escritor biopunk, aunque "Shaper/Mechanist" de Bruce Sterling es su mayor influencia.

El género emergente llamado postciberpunk continúa preocupándose por los efectos de los ordenadores, pero sin dar por supuesta la distopía ni dar tanta importancia a los implantes cibernéticos. También heredero del ciberpunk podemos considerar el concepto de singularidad tecnológica utilizado en la ciencia ficción más reciente, que recoge su preocupación por el desarrollo de la inteligencia artificial hasta el extremo, y el rol que los humanos podríamos adoptar en tales circunstancias.



</doc>
<doc id="15422" url="https://es.wikipedia.org/wiki?curid=15422" title="Economía de Bélgica">
Economía de Bélgica

La moderna economía de mercado de Bélgica se beneficia de la privilegiada localización geográfica del país en Europa, por una red de transportes bastante desarrollada y por una base industrial y comercial diversificada. La industria está concentrada principalmente en la región de Flandes, al norte.

Con pocos recursos naturales, el país importa grandes cantidades de materias primas (MP) y exporta principalmente manufacturados. El resultado es una economía muy dependiente de los mercados mundiales.

Cerca de 3/4 del comercio del país es hecho con otros países de la Unión Europea. En 2009 la economía del país sufrió una retracción de 2,7%, el desempleo creció ligeramente y el déficit presupuestario empeoró debido a la ayuda en gran escala al sector financiero. El déficit presupuestario creció para 4,8% del PIB en 2010, mientras la deuda pública superaba los 100% del PIB el mismo año.

El país es lo 18º en el ranking de competitividad del Foro Económico Mundial.

Su moneda anterior era el franco belga; desde el 1 de enero de 2002 es el euro, moneda de la Unión Europea.


Población ocupada

Población ocupada por sectores

(Estimaciones 199

Población bajo el umbral de la pobreza

Se presentan a continuación las mercancías de mayor peso en las importaciones de Bélgica para el período 2010-hasta abril 2015. Las cifras están expresadas en dólares estadounidenses valor FOB.

Se presentan a continuación los principales socios comerciales de Bélgica para el periodo 2010-hasta abril 2015.La mayoría de sus importadores están en Europa salvo Estados Unidos, India y China. Las cifras expresadas son en dólares estadounidenses valor FOB.



</doc>
<doc id="15423" url="https://es.wikipedia.org/wiki?curid=15423" title="Relación de indeterminación de Heisenberg">
Relación de indeterminación de Heisenberg

En mecánica cuántica, la relación de indeterminación de Heisenberg o principio de incertidumbre establece la imposibilidad de que determinados pares de magnitudes físicas observables y complementarias sean conocidas con precisión arbitraria. Sucintamente, afirma que no se puede determinar, en términos de la física cuántica, simultáneamente y con precisión arbitraria, ciertos pares de variables físicas, como son, la posición y el momento lineal (cantidad de movimiento) de un objeto dado. En otras palabras, cuanta mayor certeza se busca en determinar la posición de una partícula, menos se conoce su momento lineal y, por tanto, su masa y velocidad. Este principio fue enunciado por el físico teórico alemán Werner Heisenberg en 1927. 

Existe en la actualidad un leve error provocado por el pensamiento clásico tan arraigado en el razonamiento humano, se tiende a creer que la indeterminación se debe a la intervención experimental a la hora de medir una propiedad. Sin embargo, lo que el principio de indeterminación sugiere es que las propiedades de la partícula se encuentran en estado de superposición y por tanto tienen atribuidos a la vez diferentes valores de posición y de momento lineal. En la intervención, a la hora de medir, obligamos a una de las magnitudes a tomar un valor, colapsando su función de onda, y dándonos así un resultado preciso para esta, por lo que aumenta irremediablemente la indeterminación en la otra medida. 

El principio de indeterminación no tiene un análogo clásico y define una de las diferencias fundamentales entre física clásica y física cuántica. Desde un punto de vista lógico es una consecuencia de axiomas corrientes de la mecánica cuántica y por tanto estrictamente se deduce de los mismos.

La explicación «divulgativa» del principio de indeterminación afirma que las variables dinámicas como posición, momento angular, momento lineal, etc. se definen de manera "operacional", esto es, en términos relativos al procedimiento experimental por medio del cual son medidas: la posición se definirá con respecto a un sistema de referencia determinado, definiendo el instrumento de medida empleado y el modo en que tal instrumento se usa (por ejemplo, midiendo con una regla la distancia que hay de tal punto a las referencias).

Sin embargo, cuando se examinan los procedimientos experimentales por medio de los cuales podrían medirse tales variables resulta que la medida siempre acabará perturbada. En efecto, si por ejemplo pensamos en lo que sería la medida de la posición y velocidad de un electrón, para realizar la medida (para poder «ver» de algún modo el electrón) es necesario que un fotón de luz choque con el electrón, con lo cual está modificando su posición y velocidad; es decir, por el mismo hecho de realizar la medida, el experimentador modifica los datos de algún modo, introduciendo un error que es imposible de reducir a cero, por muy perfectos que sean nuestros instrumentos.

NOTA: Si la posición se mide, determinando la perturbación que genera la partícula en el campo gravitacional que le rodea, puede reducirse el error a cero. Debido a que toda partícula es afectada en diferentes medidas por los campos generadas por otras.

Esta descripción cualitativa del principio, sin ser totalmente incorrecta, es engañosa en tanto que omite el principal aspecto del principio de indeterminación: el principio de indeterminación establece el límite de aplicabilidad de la física clásica. La física clásica concibe sistemas físicos descritos por medio de variables perfectamente definidas en el tiempo (velocidad, posición...) y que en principio pueden conocerse con la precisión que se desee. Aunque en la práctica resultara imposible determinar la posición de una partícula con una precisión infinitesimal, la física clásica concibe tal precisión como alcanzable: es posible y perfectamente concebible afirmar que tal o cual partícula, en el instante de tiempo exacto 2 s, estaba en la posición exacta 1,57 m. En cambio, el principio de indeterminación, al afirmar que existe un límite fundamental a la precisión de la medida, en realidad está indicando que si un sistema físico real se describe en términos de la física clásica, entonces se está haciendo una aproximación, y la relación de incertidumbre nos indica la calidad de esa aproximación.

Por motivos culturales y educativos, las personas se suelen enfrentar al principio de incertidumbre por primera vez estando condicionadas por el determinismo de la física clásica. En ella, la posición formula_1 de una partícula puede ser definida como una función continua en el tiempo, formula_2". Si la masa de esa partícula es formula_3 y se mueve a velocidades suficientemente inferiores a la de la luz, entonces el momento lineal de la partícula se define como masa por velocidad, siendo la velocidad la primera derivada en el tiempo de la posición: formula_4. 

Dicho esto, atendiendo a la explicación habitual del principio de incertidumbre, podría resultar tentador creer que la relación de incertidumbre simplemente establece una limitación sobre nuestra capacidad de medida que nos impide conocer con precisión arbitraria la posición inicial formula_5 y el momento lineal inicial formula_6 . Ocurre que si pudiéramos conocer formula_5 y formula_6 , entonces la física clásica nos ofrecería la posición y la velocidad de la partícula en cualquier otro instante; la solución general de las ecuaciones de movimiento dependerá invariablemente de formula_5 y formula_6 . Esto es, resolver las ecuaciones del movimiento lleva a una familia o conjunto de trayectorias dependientes de formula_5 y formula_6 ; según qué valor tomen formula_5 y formula_6 , se tendrá una trayectoria dentro de esa familia u otra, pero la propia resolución de las ecuaciones limita el número de trayectorias a un conjunto determinado de ellas. Según se ha razonado, de acuerdo con el principio de incertidumbre formula_5 y formula_6 no se pueden conocer exactamente, así que tampoco podrán conocerse formula_17 y formula_18 en cualquier otro instante con una precisión arbitraria, y la trayectoria que seguirá la partícula no podrá conocerse de manera absolutamente exacta. Este razonamiento es, sin embargo, incorrecto, pues en él subyace la idea de que, pese a que formula_5 y formula_6 no se pueden conocer exactamente, es posible continuar usando la descripción clásica en virtud de la cual una partícula seguirá una trayectoria definida por la solución general de las ecuaciones de movimiento, introduciendo la noción añadida de que las condiciones iniciales formula_5 y formula_6 no pueden conocerse al detalle: esto es, no podemos conocer exactamente qué trayectoria va a seguir la partícula, pero estaremos aceptando que, de facto, va a seguir una. 

Esta forma de proceder es, sin embargo, totalmente incorrecta: el principio de incertidumbre conlleva un desvío completo de las concepciones clásicas, haciendo que la noción clásica de trayectoria debe ser desechada: preguntar cuáles son simultáneamente los valores de formula_17 y formula_18 es un absurdo. Así dicho, podría resultar paradójico que primero se establezca una relación de incertidumbre en términos de posición formula_25 y momento lineal formula_26 , para luego afirmar que formula_25 y formula_26 , que aparecen en dicha relación, no tienen sentido: si no tienen sentido, ¿qué sentido puede tener una relación que las emplee? Ocurre que, en física cuántica, es posible introducir una serie de entidades matemáticas formula_25 y formula_26 que se correspondan en muchos aspectos con la posición y el momento clásicos. Dichas entidades no son, no obstante, exactamente iguales a la posición y el momento clásicos: el principio de incertidumbre sencillamente indica que si interpretamos esas entidades como posición y momento lineal -y por tanto interpretamos el movimiento de una forma clásica-, entonces existe un límite fundamental en la precisión con que dichas variables pueden ser conocidas; esto es, si intentamos introducir variables clásicas e intentamos interpretar el movimiento de forma clásica, la precisión con que estas variables pueden ser especificadas está limitada.

Este principio supone un cambio básico en la naturaleza de la física, ya que se pasa de un conocimiento absolutamente preciso (en teoría aunque no en la práctica), al conocimiento basado solo en probabilidades. Aunque debido a la pequeñez de la constante de Planck, en el mundo macroscópico la indeterminación cuántica es casi siempre completamente despreciable, y los resultados de las teorías físicas deterministas, como la teoría de la relatividad, siguen teniendo validez en todos casos prácticos de interés.

Las partículas, en mecánica cuántica, no siguen trayectorias definidas. No es posible conocer exactamente el valor de todas las magnitudes físicas que describen el estado de movimiento de la partícula en ningún momento, sino solo una distribución estadística. Por lo tanto no es posible asignar una trayectoria a una partícula. Sí se puede decir que hay una determinada probabilidad de que la partícula se encuentre en una determinada región del espacio en un momento determinado.

Comúnmente se considera que el carácter probabilístico de la mecánica cuántica invalida el determinismo científico. Sin embargo, existen varias interpretaciones de la mecánica cuántica y no todas llegan a esta conclusión. Según puntualiza Stephen Hawking, la mecánica cuántica es determinista en sí misma, y es posible que la aparente indeterminación se deba a que realmente no existen posiciones y velocidades de partículas, sino solo ondas. Los físicos cuánticos intentarían entonces ajustar las ondas a nuestras ideas preconcebidas de posiciones y velocidades. La inadecuación de estos conceptos sería la causa de la aparente impredecibilidad. Otros fenómenos deducibles o conectados con el principio de indeterminación de Heisenberg son:

Si se preparan varias copias idénticas de un sistema en un estado determinado, como puede ser un átomo, las medidas de la posición y de la cantidad de movimiento variarán de acuerdo con una cierta distribución de probabilidad característica del estado cuántico del sistema. Las medidas del objeto observable sufrirán desviación estándar Δ"x" de la posición y el momento Δ"p". Verifican entonces el principio 
de indeterminación que se expresa matemáticamente como:

donde la "h" es la constante de Planck (para simplificar, formula_31 suele escribirse como formula_32 )

El valor conocido de la constante de Planck es:

En la física de sistemas clásicos esta indeterminación de la posición-momento no se manifiesta puesto que se aplica a estados cuánticos del átomo y "h" es extremadamente pequeño. Una de las formas alternativas del principio de indeterminación más conocida es la indeterminación tiempo-energía que puede escribirse como:

Esta forma es la que se utiliza en mecánica cuántica para explorar las consecuencias de la formación de partículas virtuales, utilizadas para estudiar los estados intermedios de una interacción. Esta forma del principio de indeterminación es también la utilizada para estudiar el concepto de energía del vacío.

Además de las dos formas anteriores existen otras desigualdades como la que afecta a las componentes "J" del momento angular total de un sistema:

Donde "i", "j", "k" son distintos y "J" denota la componente del momento angular a lo largo del eje "x".

Más generalmente si en un sistema cuántico existen dos magnitudes físicas "a" y "b" representadas por los operadores u observables denotados como formula_34, en general no será posible preparar una colección de sistemas todos ellos en el estado formula_35, donde las desviaciones estándar de las medidas de "a" y "b" no satisfagan la condición:
La expresión general de la relación de indeterminación se deduce de los postulados I y III de la mecánica cuántica. La demostración más particular de que existen magnitudes que no pueden conocerse con precisión arbitraria usa también y de manera crítica el postulado VI.

Para probar el principio de indeterminación de Heisenberg supongamos dos observables formula_36 y formula_37 cualesquiera y supongamos un estado formula_38 tal que formula_39. En esa situación puede demostrarse que:

Donde:
Definiendo a partir de formula_36 y formula_37, los operadores autoadjuntos:

Se puede construir la función real:

Y desarrollando el producto escalar anterior:

Teniendo en cuenta que:
La ecuación puede ser reescrita como:

Como formula_48 es un operador hermítico los coeficientes de la función polinómica anterior son reales, y como la expresión anterior es real para todo valor de formula_49 necesariamente el discriminante del polinomio asociado debe ser negativo:

Reordenando y obteniendo raíces cuadradas en la ecuación anterior se obtiene precisamente la ecuación . Si se particulariza la ecuación tomando formula_50:
Mediante el principio de incertidumbre es posible estimar la energía del punto cero de algunos sistemas. Para ello supondremos que en tales sistemas el punto cero cumple que la partícula estaría clásicamente en reposo (a nivel cuántico significa que el valor esperado del momento es nulo). Este método del cálculo de energías tan solo da una idea del orden de magnitud del estado fundamental, nunca siendo un método de cálculo del valor exacto (en algún sistema puede resultar que el valor obtenido sea el exacto pero ello no deja de ser más que una simple casualidad).
La interpretación física del método es que debido al principio de incertidumbre, la localización de la partícula tiene un coste energético (el término de la energía cinética), de modo que cuanto más cerca del centro de fuerzas esté la partícula más energía tendrá el sistema debido a las fluctuaciones cuánticas, de modo que en el nivel fundamental el sistema minimizará su energía total.

A continuación se estimará la energía fundamental de un átomo monoelectrónico.
Por el principio de indeterminación se tiene que:
Empleando como estimación que para el nivel fundamental se cumple:
La energía total es la suma de cinética más potencial. Dado que el valor medio del momento radial es nulo, su valor cuadrático esperado será igual a su desviación y se aproximará el valor esperado del inverso del radio al inverso de su desviación.
En el nivel fundamental la energía ha de ser mínima de modo que:
El valor obtenido es casualmente idéntico al radio de Bohr y sustituyendo en la estimación obtenida para la energía se obtiene:
Casualmente este es exactamente la energía del estado fundamental de un átomo hidrogenoide. El objetivo del método es la estimación del valor, si bien en este ejemplo particular obtenido es idéntico al calculado formalmente.

Empleando como estimación:
Tomando que el valor medio de la posición y momento son nulos debido a la simetría del problema se tiene que la energía total es:
Minimizando la energía:
Sustituyendo el valor en la energía se obtiene:
Como se puede observar el valor obtenido es el doble del punto cero del oscilador armónico, de modo que aunque el valor obtenido no sea exacto el orden de magnitud sí es el correcto.

Sea una partícula que se encuentra confinada en un pozo infinito de anchura 2a. Dado que las únicas posiciones posibles de la partícula se encuentran dentro del pozo se puede estimar que:
La energía cinética será por tanto:
Como se observa el resultado obtenido difiere en un factor algo superior a 2 del valor real, pero de nuevo el orden de magnitud es el correcto. Este cálculo da una idea de las energías que hay que aportar para confinar una cierta párticula en una región, tal como puede ser un nucleón en el núcleo.





</doc>
<doc id="15427" url="https://es.wikipedia.org/wiki?curid=15427" title="Anillo (desambiguación)">
Anillo (desambiguación)

El término anillo puede referirse a:








</doc>
<doc id="15429" url="https://es.wikipedia.org/wiki?curid=15429" title="TeX">
TeX

TeX, estilizado como formula_1, es un sistema de tipografía escrito por Donald E. Knuth, muy popular en el entorno académico, especialmente entre las comunidades de matemáticos, físicos e informáticos. Ha conseguido sustituir con creces a troff, otro programa de tipografía habitual en Unix.

TeX se considera generalmente la mejor forma de componer fórmulas matemáticas complejas pero, especialmente en la forma de LaTeX y otros paquetes de macros, se puede usar para otras tareas de composición.

Knuth empezó a escribir TeX porque se sentía molesto con la calidad cada vez menor de la tipografía en los volúmenes I a III de su obra "El arte de programar ordenadores". Empezó por ello a diseñar su propio lenguaje de tipografía. Pensó que podría acabarlo en su año sabático, 1978; se equivocó por tan solo ocho años. El lenguaje se finalizó y congeló (no se hicieron más modificaciones) alrededor de 1985.

Guy Steele coincidió en Stanford en el verano de 1978, cuando Knuth estaba desarrollando su primera versión de TeX. Cuando volvió al MIT a finales de año, reescribió la entrada/salida de TeX para que se ejecutase en el ITS.

La primera versión de TeX se escribió usando el lenguaje de programación SAIL que se ejecutaba en una PDP-10 en el sistema operativo WAITS de la Universidad de Stanford. Para las versiones posteriores de TeX, Knuth inventó el concepto de programación literaria, una forma de producir código fuente compilable y documentación con referencias de alta calidad (por supuesto, escrito en TeX) partiendo del mismo archivo original. El lenguaje usado se llama WEB y produce programas en Pascal.

TeX tiene un sistema de numeración de versiones peculiar. Desde la versión 3, las actualizaciones se indican añadiendo una cifra decimal extra al final, de modo que el número de versión se aproxime asintóticamente a π. La versión más reciente es la 3,14159265 y por ser muy estable solo se prevén pequeñas actualizaciones.

Knuth ha indicado que el "último cambio final (hecho después de mi muerte)" será cambiar el número de versión a π, y que en ese momento todos los errores que queden serán considerados características.

Las órdenes de TeX empiezan con una barra invertida ("\") y sus argumentos se indican mediante
llaves ("{}"). Sin embargo, casi todas las propiedades sintácticas
de TeX pueden cambiarse sobre la marcha, con lo que la entrada de TeX es
algo difícil de analizar salvo por el propio TeX. TeX es un lenguaje
basado en órdenes básicas y macros: muchas órdenes, incluidas la
mayoría de las que definen los usuarios, se sustituyen sobre la marcha hasta
que solo quedan órdenes básicas, que entonces se ejecutan. La sustitución en sí
misma está libre de efectos secundarios. La recursión de macros no consume
memoria y asimismo se dispone de construcciones if-then-else. Todo ello hace
de TeX un lenguaje Turing completo incluso al nivel de sustitución.

El sistema TeX tiene un conocimiento preciso de los tamaños de los caracteres
y símbolos, y usando esta información calcula el alineamiento óptimo de letras
por línea y de líneas en cada página. Posteriormente produce un archivo DVI (de las siglas en inglés "device independent", independiente del dispositivo) que contiene la posición final de todos los caracteres. El archivo dvi se puede imprimir directamente usando un controlador
de impresora adecuado, o puede convertirse a otros formatos. Actualmente,
pdfTeX se usa para generar archivos PDF saltándose la generación del DVI.

La mayor parte de la funcionalidad viene dada por diversas macros: las
originales de Knuth englobadas en lo que se llama plainTeX, LaTeX (mayoritario
en las ciencias técnicas) y ConTeXt (usado principalmente para
publicaciones).

La referencia principal de TeX son los dos primeros volúmenes de la obra "Computers and Typesetting" de Knuth: `"The TeXbook"'´ y `"TeX: The Program"´ (éste incluye el código fuente de TeX completo y documentado).

La organización de los directorios en una instalación de TeX está normalizada en un árbol llamado texmf.

La licencia de TeX permite la distribución y modificación libres, pero exige que cualquier versión modificada no se llame TX, TeX o algo similar, que pueda ser confundido con la versión original. La licencia da derechos similares a aquellos de una marca registrada.

Aunque está bien escrito, TeX es tan grande (y tan lleno de técnica avanzada) que se dice haber descubierto al menos un error en cada sistema Pascal en el que se ha compilado, ya que TeX se ejecuta en la mayoría de los sistemas operativos.

Knuth ofrece recompensas monetarias para la gente que encuentre e informe de un error en el programa. El premio por error empezó con un centavo y se doblaba cada año hasta que quedó congelado en su valor actual de 327,68 dólares. Esto, sin embargo, no ha arruinado a Knuth, porque se han encontrado muy pocos errores y
en cualquier caso el cheque que prueba que el propietario encontró un error en TeX se suele enmarcar en vez de cobrarlo.

Donald Knuth explica en su obra "The TeXbook" que la palabra "technology" ("tecnología") tiene raíz griega y esta comienza por las letras τεχ. Por tanto, el nombre TeX en español se tiene que pronunciar [tej], y no [teks]. Ello se debe a que TeX no quiere decir TEX sino τεχ, acabado en la letra griega χ <nowiki>[</nowiki>ji<nowiki>]</nowiki>. La misma palabra griega τέχνη (ΤΕΧΝΗ – technē) significa "arte", una referencia a que la técnica no está reñida con el arte ni con la presentación elegante.

Cuando se está escribiendo un archivo en TeX y se quiere hacer referencia al
nombre se dispone de la orden \TeX, definida así:

O así en formula_2:

y que fue creada por Knuth para demostrar lo que es posible hacer con TeX. La letra "E" queda por
debajo de la línea base y más unida a la T; en los otros sistemas se escribe
usando la aproximación "<nowiki>TeX</nowiki>".

Varios sistemas de procesamiento de documentos están basados en TeX; destacan entre ellos:


Todos estos sistemas están escritos en el lenguaje de programación TeX (algunos con complementos en otros lenguajes de programación). Además, hay programas que extienden el lenguaje de programación con nuevas órdenes y capacidades:

Además, hay programas asociados como BibTeX para el manejo de
bibliografías, MakeIndex y xindy para los índices alfabéticos
y Metafont para gráficos.

Todas las extensiones están disponibles en el
CTAN, ("Comprehensive TeX Archive Network").

En sistemas compatibles Unix, TeX se distribuye bajo la forma teTeX. En sistemas Windows existen MiKTeX y fpTeX. En sistemas Mac OS X existe MacTeX con utilidades como TeXShop.

El editor de texto TeXmacs es un editor de textos científicos WYSIWYG que pretende ser compatible con TeX. Usa las tipografías de Knuth y puede generar un archivo TeX. Otra herramienta similar es LyX.

Un ejemplo simple en TeX: 
crea un archivo llamado miprimer.tex que contenga lo siguiente:

Abre un intérprete de órdenes y escribe

TeX creará un archivo llamado "miprimer.dvi". Usa un programa adecuado para visualizarlo. Por ejemplo, MiKTeX incluye el visor yap

El visor muestra "hola" en una página. "\bye" es la orden Tex que marca el final de un archivo y no se muestra en la salida final.

El archivo dvi puede ser impreso directamente desde el visor o convertido a un formato más común tal como PostScript usando el programa dvips.

Es posible crear directamente archivos PDF usando pdfTeX:

pdfTeX se creó originalmente porque al convertir los PostScript generados en PDF se obtenía una visualización de las tipografías de baja calidad, aunque la impresión era buena. La causa es que TeX usa de forma nativa tipografías Tipo 3 de mapas de bits, que no se visualizan tan bien como las tipografías Tipo 1 escalables.

Es posible actualmente hacer que dvips use las tipografías escalables con un poco de configuración (versiones recientes de Ghostscript lo permiten), pero una conversión directa a PDF tiene otros beneficios: es un proceso en un solo paso, en lugar de dos, y pdfTeX incluye cosas tales como marcadores e hipervínculos, ausentes en PostScript.

Para ver a TeX en acción, prueba a escribir la conocida fórmula de la ecuación cuadrática:

Con el texto de arriba deberías obtener algo que se viese como esto

En un documento, para entrar en el "modo matemático" se escribe un signo $, a continuación la fórmula de manera que la entienda TeX y se cierra con otro signo $. Otro modo de presentación, que deja la fórmula centrada en una nueva línea, se consigue usando $$. Por ejemplo, la fórmula anterior se escribiría
y se vería como
formula_4

Aplicación de Fórmula del cociente

Aplicación de Fórmula del cociente






</doc>
<doc id="15430" url="https://es.wikipedia.org/wiki?curid=15430" title="Ingeniería civil">
Ingeniería civil

La ingeniería civil es la disciplina de la ingeniería que emplea conocimientos de cálculo, mecánica, hidráulica y física para encargarse del diseño, construcción y mantenimiento de las infraestructuras emplazadas en el entorno, incluyendo carreteras, ferrocarriles, puentes, canales, presas, puertos, aeropuertos, diques y otras construcciones relacionadas. La ingeniería civil es la más antigua después de la ingeniería militar, de ahí su nombre para distinguir las actividades no militares con las militares. Tradicionalmente ha sido dividida en varias subdisciplinas incluyendo ingeniería ambiental, ingeniería sanitaria, ingeniería geotécnica, geofísica, geodesia, ingeniería estructural, ingeniería del transporte, ciencias de la Tierra, urbanismo, ordenación del territorio, ingeniería hidráulica, ciencia de materiales, gestión costera, , e ingeniería de la construcción. El ingeniero civil ocupa puestos en prácticamente todos los niveles: en el sector público desde el ámbito municipal al gubernamental y en el ámbito privado desde los pequeños consultores autónomos que trabajan en casa hasta los contratados en grandes compañías internacionales.

La ingeniería ha sido un aspecto de la vida desde el inicio de la existencia humana. Las prácticas más tempranas de la ingeniería civil podrían haber comenzado entre el 4000 y el en el Antiguo Egipto y Mesopotamia cuando los humanos comenzaron a abandonar la existencia nómada, creando la necesidad de un cobijo. Durante este tiempo, el transporte empezó a incrementar su importancia, lo que llevó al desarrollo de la rueda y de la navegación.

Hasta la Edad Contemporánea no hay una distinción clara entre ingeniería civil y arquitectura, y el término ingeniero y arquitecto sufrió variaciones refiriéndose a la misma persona, incluso intercambiándose. La construcción de las Pirámides de Egipto entre el 2700 y el podría considerarse las primeras muestras de construcciones de gran tamaño. Otras construcciones históricas incluyen el sistema de gestión de aguas de Qanat, el Partenón por Ictino en la Grecia Antigua (), la vía Apia por los ingenieros romanos o la Gran Muralla China en el , o los trabajos de irrigación en Anuradhapura. De todas las civilizaciones antiguas quizás la más desarrollada en ingeniería civil fueron los romanos que fueron pioneros en la construcción de una red de calzadas, acueductos, puertos, puentes, presas y alcantarillados.

En el siglo XVIII el término ingeniería civil fue acuñado para incorporar toda la ingeniería para usos civiles en oposición de la ingeniería militar (artillería, balística, construcción de defensas...). En 1747 se crea la escuela de ingeniería civil más antigua del mundo, la École nationale des ponts et chaussées en París, que aún hoy perdura. El primer ingeniero civil autoproclamado fue John Smeaton que construyó el faro de Eddystone. En 1771 Smeaton y algunos colegas formaron la "Smeatonian Society of Civil Engineers", un grupo de profesionales que se reunían diariamente para debatir sobre su profesión. A través de estos encuentros se formaron las sociedades profesionales que conocemos hoy en día.

En España se consideró la necesidad de crear un cuerpo de ingenieros específico que se encargara de las obras públicas, por eso se funda la Escuela Oficial del Cuerpo de Ingenieros de Caminos dirigida por Agustín de Betancourt en 1802. Por aquel entonces México ya había establecido el primer instituto de investigación especializado en la ingeniería civil y en 1857 se instituyen las enseñanzas de ingeniero civil en la Academia de San Carlos basándose en los planes de estudios europeos.

Los ingenieros civiles cuentan con un título académico en ingeniería civil. El tiempo de estudio es de entre cuatro y cinco años para el "título de grado en ingeniería" ("bachelor de ingeniería" en los países anglosajones), que es necesario para poder cursar posteriormente los estudios de posgrado (títulos de "máster en ingeniería" y "doctor en ingeniería").

En la mayoría de los países, el título universitario representa el primer paso a la certificación profesional y el programa de la titulación en sí mismo está certificado por un colegio profesional. Después de completar un programa de titulación certificada el ingeniero debe satisfacer una serie de requerimientos (incluyendo experiencia laboral y un examen) antes de ser certificado. Una vez certificado, el ingeniero es designado con el título de ingeniero profesional (en Estados Unidos, Canadá y Sudáfrica), o ingeniero colegiado (en la mayoría de los países de la Commonwealth), ingeniero profesional colegiado (en Australia y Nueva Zelanda) o ingeniero europeo (para algunos países de la Unión Europea). Existen acuerdos internacionales entre colegios de ingenieros que permiten a ingenieros de otros países ejercer fuera de sus fronteras. En España cualquier persona que completa la carrera puede ejercer y colegiarse, sin ningún otro requisito adicional como experiencia o examen.

Las ventajas de la certificación varían dependiendo del sitio. Por ejemplo, en Estados Unidos y Canadá “sólo un ingeniero profesional licenciado puede preparar, firmar y sellar, y entregar un proyecto de ingeniería a una autoridad pública para su aprobación, o sello para clientes públicos o privados”. En el estado de Quebec, en Canadá, esto es así. En Reino Unido no existe una legislación tan restrictiva ni en España si bien existen colegios que pueden expulsar a sus miembros por mala praxis y así no poder ejercer. Se supone que todos los ingenieros deben respetar un código ético y que si no lo cumplen se les puede culpar por negligencia.

En España existe actualmente el grado en "Ingeniería Civil", "Obras Públicas" o "Civil y Territorial" (entre otros nombres dependiendo de la universidad que lo otorga) de 4 años y 240 ECTS), así como el máster en "Ingeniería de Caminos, Canales y Puertos" de entre 1 y 2 años más y 66 a 120 ECTS). Anteriormente existían las titulaciones oficiales de "Ingeniero Técnico de Obras Públicas" e "Ingeniero de Caminos, Canales y Puertos", que se corresponden con las profesiones reguladas actuales homónimas. Dichas titulaciones generalmente incluyen unidades que cubren física, matemáticas, gestión de proyectos, construcción, diseño y temas específicos de la ingeniería civil. Normalmente en el inicio de la titulación las asignaturas cubren la mayoría, si no todas, las subdisciplinas de la ingeniería civil. Los estudiantes entonces eligen especializarse en la parte final de la titulación en una o más subdisciplinas en vistas a terminar sus titulaciones. Además del "Máster Universitario en Ingeniería de Caminos, Canales y Puertos", que atribuye las competencias específicas de dicha profesión regulada, las universidades generalmente ofrecen además másteres de especialización para mejorar los conocimientos del ingeniero civil en un área de particular interés dentro de la ingeniería civil.

Actualmente se estudia 5 o 6 años para el “Título en Ingeniería Civil”, el cual es necesario para cursar estudios de postgrado, como “máster o Magíster en ciencias de la ingeniería ”, después el “doctorado”. Las materias necesarias para la obtención del Título de ingeniero civil se deben cursar materias tales como Ciencias básicas, Ciencias de Ingeniería, Ingeniería Aplicada, Ciencias Económicas y Administrativas.

Su campo de aplicación es muy amplio. Estarían, por ejemplo, las infraestructuras del transporte:

Las obras hidráulicas:

La intervención sobre problemas de estabilidad del terreno.

Las estructuras que componen las obras anteriores:

En general, las obras de ingeniería civil implican el trabajo una gran cantidad de personas (en ocasiones cientos y hasta miles) a lo largo de lapsos que abarcan desde unas pocas semanas o meses hasta varios años.

Debido al elevado costo de los trabajos que se acometen (piénsese en el coste de una autovía o de una línea de ferrocarril) buena parte de los trabajos que se realizan son para el Estado, o bien para grandes compañías que pretenden la explotación de una infraestructura a largo plazo (autopistas y túneles de peaje, compañías de ferrocarril, etcétera). Sin embargo, sus técnicas son también aplicadas para obras semejantes a las anteriores pero de más pequeña escala, como podrían ser:

Además, son también competencia de un ingeniero civil:

De esta forma, un ingeniero civil no se limita a las grandes obras de infraestructura, muy raras debido a su elevado coste.

Hay una serie de subdisciplinas dentro del amplio campo de la ingeniería civil. Los ingenieros civiles generales trabajan en estrecha colaboración con agrimensores e ingenieros civiles especializados para diseñar nivelación, drenaje, pavimento, suministro de agua, servicio de alcantarillado, presas, suministro eléctrico y de comunicaciones. La ingeniería civil general también se conoce como ingeniería de sitio, una rama de la ingeniería civil que se enfoca principalmente en convertir una extensión de tierra de un uso a otro. Los ingenieros de sitio pasan tiempo visitando los sitios del proyecto, reuniéndose con las partes interesadas y preparando planes de construcción. Los ingenieros civiles aplican los principios de ingeniería geotécnica, ingeniería estructural, ingeniería ambiental, ingeniería de transporte e ingeniería de construcción a proyectos residenciales, comerciales, industriales y de obras públicas de todos los tamaños y niveles de construcción.

El trabajo de un ingeniero civil comienza al presentarse una determinada necesidad (un nuevo dique en un puerto, la ampliación o construcción de una carretera, una presa que dé continuidad y estabilidad al caudal de un río…). En esta etapa de planificación, los ingenieros civiles trabajan en forma integrada con otros profesionales y autoridades nacionales o locales con poder de decisión.

Entra entonces el trabajo de recopilación de los datos necesarios para el diseño de una solución a dicha necesidad, datos que pueden ser topográficos (medición de la superficie real del terreno), hidrológicos (pluviometría de una cuenca, caudal de un río, etc.), estadísticos (aforos de las carreteras o calles existentes, densidades de población), etcétera.

Para esta finalidad los diseños de las obras y sistemas más complejos se hacen en varias etapas. La primera etapa denominada de pre-factibilidad, se encarga de analizar el mayor número de soluciones posibles. Es en esta etapa en la cual los organismos competentes decidirán por ejemplo: el emplazamiento de un puerto, el trazado general de una carretera o tomarán la decisión respecto a si construir una vía férrea para transporte de minerales o un mineroducto. Para la toma de decisiones se consideran, entre otros, los siguientes puntos de vista: dificultad de la obra; costo de la obra; impacto ambiental producido por la obra. El estudio de pre-factibilidad involucra un equipo multidisciplinario de técnicos, donde además de ingenieros civiles participan ingenieros eléctricos, mecánicos, geólogos, economistas, sociólogos, ecologistas. Como resultado de esta fase se escogen 2 o 3 soluciones para detallarlas en la etapa siguiente.

En la siguiente etapa, llamada factibilidad técnico-económica, ya se avanza mucho en los detalles constructivos, en la determinación de los costos, en el cronograma de construcción y en el flujo de caja necesario para la ejecución de la obra. En esta etapa tienen mucho peso las investigaciones de campo para detectar dificultades específicas relacionadas con la geología de las áreas en las que se intervendrá, y se detallarán los impactos ambientales, incluyendo tanto la parte física como la abiótica y la social. En general es en esta fase que se escoge la solución definitiva, que será detallada en la etapa de diseño definitivo o proyecto ejecutivo.

Viene entonces el trabajo real sobre el terreno: acondicionar este para que sea capaz de soportar las estructuras que se van a construir sobre él (llegándose en ocasiones a sustituir el terreno por otro de mayor capacidad portante si el existente no cumple las condiciones necesarias), movimientos de tierras (desmontes y terraplenes), construcción de las estructuras (pilotes, zapatas, pilares, estribos, vigas, muros de contención).

Sin embargo, todos estos pasos rara vez se dan de forma fluida ni, mucho menos, competen a un mismo equipo de ingeniería. Así, a menudo son los ingenieros de la Administración correspondiente los que detectan la necesidad que se tratará de solventar, mientras que en otras ocasiones la obra viene incluida dentro de un plan de actuación político (no siempre con una clara justificación técnica).

Si la obra a acometer es de gran envergadura la Administración no la ejecuta, sino que sus ingenieros elaboran un anteproyecto que es sacado a subasta pública. Entonces son los ingenieros de las diferentes empresas constructoras los que, a partir de las prescripciones técnicas del anteproyecto, elaboran diferentes alternativas. Las alternativas ofrecidas por las constructoras pueden ser muy distintas al anteproyecto y entre sí, pues cada empresa hace uso de la maquinaria y procedimientos que le son más conocidos, y la Administración elegirá la más barata de las opciones que cumplan las exigencias.

Los ingenieros que lleven a cabo la obra no tienen por qué ser (ni, generalmente, son) los que la hayan diseñado. La empresa constructora puede decidir también subcontratar diferentes trabajos a otras empresas, con lo que puede llegar a haber a diferentes empresas para una misma obra (una ejecuta los movimientos de tierras, otra las estructuras de hormigón…) cada una con su correspondiente departamento de ingeniería y su correspondiente equipo de ingenieros en obra.

Muy a menudo, debido a lo imprevisible del terreno se producen problemas a pie de obra que obligan a realizar modificaciones en el proyecto; en otras ocasiones la Administración puede decidir variar algunas condiciones o exigencias a medida que la obra se desarrolla y se observan problemas o posibilidades que no se habían estudiado o que en el momento en que se elaboró el anteproyecto no se consideraron importantes. Puede ocurrir que una nueva infraestructura obligue a hacer modificaciones o surja la posibilidad de que dos obras diferentes, construidas por empresas diferentes (por supuesto con diferentes equipos de ingenieros) sean ejecutadas en conjunto.

Todo esto puede dar idea de la gran cantidad de variables que afectan al trabajo de ingeniería civil. Las obras de gran envergadura son raras, y más frecuentemente el ingeniero civil se limita a la supervisión de la obra y a la toma de decisiones concretas en problemas concretos que no afectan al desarrollo o presupuesto general de la obra. Así, trabajos como la contención de un terreno de características habituales, la colocación de una viga pretensada o la ejecución de un firme, son trabajos rutinarios que no implican cambios significativos en el proyecto.



</doc>
<doc id="15431" url="https://es.wikipedia.org/wiki?curid=15431" title="Linfedema después de una mastectomía">
Linfedema después de una mastectomía

Durante una tumorectomía o una mastectomía suele ser necesario extirpar algunos de los ganglios linfáticos de la axila que recogen la linfa de la región de los antebrazos, de la mayor parte de la mama, de la nuca y de la propia axila. Los ganglios linfáticos procesan los fluidos, los posibles microbios y los residuos de las infecciones. 

El brazo puede sufrir un edema linfático o linfedema cuando se alteran los ganglios o vasos linfáticos. Este daño linfático puede deberse a cirugía, a radioterapia o, más frecuentemente, a una combinación de ambas. La quimioterapia puede también contribuir al edema del brazo. 

La mujer a la que han extirpado los ganglios de la axila tiene un mayor riesgo para desarrollar linfedema, lo que puede ocurrir inmediatamente después de la cirugía, o meses o años después. No todas las mujeres que tienen una mastectomía experimentarán linfedema. 

Hay varios tipos de linfedema. El tipo agudo, temporal y suave de linfedema aparece unos pocos días después de la cirugía y usualmente dura un período corto de tiempo. Otro tipo de linfedema agudo, más doloroso, puede aparecer de 4 a 6 semanas después de la cirugía. Pero el tipo más común de linfedema tiene lugar lentamente y sin dolor, y puede tener lugar de 18 a 24 meses después de la cirugía. 

El síntoma principal del linfedema es el edema del brazo afectado. El grado de edema puede variar. Algunas personas pueden experimentar un edema grave - cuando el brazo afectado está varias pulgadas más grande que el otro brazo. Mientras que otras experimentarán una forma más suave del edema - cuando el brazo afectado está solo un poco más grande que el otro brazo.

Además del edema del brazo afectado, los siguientes son síntomas comunes en el linfedema. 

Sin embargo, cada individuo puede experimentar los síntomas de una forma diferente. Puesto que los síntomas del linfedema pueden parecerse a otras enfermedades el diagnóstico y el tratamiento deberán llevarse a cabo bajo supervisión médica. Un tratamiento conservador de elección de probada eficacia es el drenaje linfático manual.



</doc>
<doc id="15432" url="https://es.wikipedia.org/wiki?curid=15432" title="Benceno">
Benceno

El benceno es un hidrocarburo aromático de fórmula molecular CH, (originariamente a él y sus derivados se le denominaban compuestos aromáticos debido a la forma característica que poseen) también es conocido como benzol. En el benceno cada átomo de carbono ocupa el vértice de un hexágono regular, aparentemente tres de las cuatro valencias de los átomos de carbono se utilizan para unir átomos de carbono contiguos entre sí, y la cuarta valencia con un átomo de hidrógeno. Según las teorías modernas sobre los enlaces químicos, tres de los cuatro electrones de la capa de valencia del átomo de carbono se utilizan directamente para formar los enlaces covalentes típicos (2C-C y C-H) y el cuarto se comparte con los de los otros cinco átomos de carbono, obteniéndose lo que se denomina "la nube π (pi)" que contiene en diversos orbitales los seis electrones. El benceno es un líquido incoloro y muy inflamable de aroma dulce (que debe manejarse con sumo cuidado debido a su carácter cancerígeno), con un punto de ebullición relativamente alto.

El benceno se usa en grandes cantidades en los Estados Unidos. Se encuentra en la lista de los 20 productos químicos de mayor volumen de producción. Algunas industrias usan el benceno como punto de partida para manufacturar otros productos químicos usados en la fabricación de plásticos, resinas, nilón y fibras sintéticas como lo es el kevlar y en ciertos polímeros. También se usa benceno para hacer ciertos tipos de gomas, lubricantes, tinturas, detergentes, medicamentos y pesticidas. Los volcanes e incendios forestales constituyen fuentes naturales de benceno. El benceno es también un componente natural del petróleo crudo y la gasolina. Se encuentra también en el humo de cigarrillo y otros materiales orgánicos que se han quemado. Puede obtenerse mediante la destilación fraccionada del alquitrán de hulla.

Se suele mostrar, en términos de estructura de Lewis, como un hexágono, plano e indeformable, carente de tensiones de anillo (transanulares), en cuyos vértices se encuentran los átomos de carbono, con tres dobles enlaces y tres enlaces simples en posiciones alternas (1=2, 3=4, 5=6; 6-1, 2-3, 4-5; o bien 1=2-3=4-5=6-1). Esta estructura difería de la de Brønsted y Lowry.
Hay que resaltar que, acorde a los resultados de la espectrofotometría infrarroja, el benceno no posee ni simples ni dobles enlaces, sino un híbrido de resonancia entre ambos, de distancia de enlace promedio entre simple y doble (aproximadamente 1,4 Å). Estos resultados coinciden con la previsión de la TOM (teoría de orbitales moleculares), que calcula una distribución de tres orbitales enlazantes totalmente ocupados. A esta especial estabilidad se le llama aromaticidad y a las moléculas (iones o no, estables o intermedios de reacción) se les llama aromáticas.

La molécula de Benceno fue descubierta por Faraday en 1825, quien aisló por primera vez a partir del gas de alumbrado el compuesto, de fórmula empírica CH. Fue Eilhard Mitscherlich quien logró medir su masa molecular a partir de su presión de vapor, estableciéndola en 78 u, lo que correspondía a una fórmula molecular CH. El compuesto se había obtenido de la goma benjuí, lo que llevó a que se denominase bencina, y posteriormente benceno.

Inicialmente se propusieron formas abiertas (alifáticas) para la cadena de benceno, con dos triples enlaces, sin embargo los datos experimentales que se obtenían a partir de sus reacciones eran contradictorios con estos modelos abiertos, dado que presentaba un número inusualmente bajo de isómeros. Así, por ejemplo, la monobromación del compuesto presentaba un único isómero, al igual que ocurría con la nitración. Por otro lado no respondía a las adiciones habituales de nucleófilos a enlaces múltiples.

Esto llevó a que se propusieran diversas estructuras para comprender estos hechos, como la de Dewar, la de Klaus o la de Kekulé.
Sin embargo, la estructura de Kekulé seguía presentando una incompatibilidad con la malformación 1,2 de la molécula dado que deberían formarse dos isómeros (isómeros ortobencénicos), uno de ellos con el bromo sobre un doble enlace y el otro con ambos bencenos sobre un enlace simple. Esto llevó a Kekulé a proponer que el benceno alternaba entre dos formas, en las que los enlaces cambiaban continuamente de posición, por lo que únicamente se detectaría un isómero.

La representación de los tres dobles enlaces se debe a Friedrich Kekulé, quien además fue el descubridor de la estructura anular de dicho compuesto y el primero que lo representó de esa manera.

De todas formas, fue el , Linus Pauling quien consiguió encontrar el verdadero origen de este comportamiento, la resonancia o mesomería, en la cual ambas estructuras de Kekulé se superponen.

Normalmente se representa como un hexágono regular con un círculo inscrito para hacer notar que los tres dobles enlaces del benceno están deslocalizados, disociados y estabilizados por resonancia. Es decir, no "funcionan" como un doble enlace normal sino que al estar alternados, esto es, uno sí y uno no, proporcionan a la molécula sus características tan especiales. Cada carbono presenta en el benceno hibridación sp. Estos híbridos se usarán tanto para formar los enlaces entre carbonos como los enlaces entre los carbonos y los hidrógenos. Cada carbono presenta además un orbital P adicional perpendicular al plano molecular y con un electrón alojado en su interior, que se usará para formar enlaces π.

La reacción típica del benceno es la de sustitución aromática que sigue dos caminos alternativos:

Las reacciones de sustitución aromática más corrientes son las originadas por reactivos electrofílicos. La capacidad del benceno para actuar como un donador de electrones se debe a la polarización del núcleo bencénico. Las reacciones típicas del benceno son las de sustitución. Los agentes de sustitución utilizados con más frecuencia son:

El cloro y el bromo dan derivados por sustitución de uno o más hidrógenos del benceno, que reciben el nombre de haluros de arilo.

formula_1

formula_2

La halogenación está favorecida por las bajas temperaturas y algún catalizador, como el hierro, el tricloruro de aluminio u otro ácido de Lewis, que polariza al halógeno para que se produzca la reacción. En el caso del bromobenceno se utiliza FeBr como catalizador.

Cuando los hidrocarburos bencénicos se tratan con ácido sulfúrico concentrado, que es una mezcla de (HSO) y (SO), se forman compuestos característicos que reciben el nombre ácidos sulfónicos. El electrófilo que reacciona puede ser HSO o SO. Es la única reacción reversible de las que estamos considerando.

El ácido nítrico fumante o una mezcla de ácidos nítrico y sulfúrico, denominada mezcla sulfonítrica, (una parte de ácido nítrico y tres de sulfúrico), produce derivados nitrados, por sustitución. El ácido sulfúrico protona al ácido nítrico que se transforma en el ion nitronio positivo (NO) que es el agente nitrante efectivo:

Este proceso se efectúa haciendo reaccionar el benceno con ácido nítrico y usando como catalizador ácido sulfúrico, mezcla que se conoce como sulfonítrica, generándose el ión nitronio NO, que actúa como agente electrofílico a una temperatura entre 50 a 60 °C, produciéndose en este proceso el nitro benceno y agua

El benceno es inflamable y arde con llama fuliginosa, propiedad característica de la mayoría de los compuestos aromáticos y que se debe a su alto contenido en carbono.

<chem> C6H6 + 15/2 O2 -> 6 CO2 + 3 H2O </chem>

El anillo de benceno puede ser reducido a ciclohexano, con hidrogenación catalítica (por ejemplo, usando níquel Raney) a alta presión, manteniendo así la estructura de la cadena cerrada. La reducción parcial se puede llevar a cabo por medio del método de Birch para formar ciclohexadienos. 

El benceno reacciona con los haluros de alquilo, en presencia de cloruro de aluminio anhidro (AlCl) como catalizador, formando homólogos.

formula_5

El ataque sobre el anillo bencénico por el ion CH es semejante al realizado por el ion Cl en la cloración.

Es una modificación de la de Wortz de la serie grasa. Los homólogos del benceno pueden prepararse calentando una solución etérea de un halogenuro de alquilo y otro de arilo con sodio.
Este método tiene la ventaja sobre el de Friedel–Crafts, de que se conoce la estructura del producto y puede introducirse fácilmente cadenas largas normales.

Cuando se introduce un segundo sustituyente y en un derivado del benceno del tipo CHX, la posición que ocupa Y depende del carácter electrónico del grupo X, que ya está presente en el núcleo. Los productos de la reacción pueden ser orto y para o meta disustituidos y eso depende de la velocidad de la reacción de sustitución en cada una de las tres posiciones.

Hay unas reglas de orientación:

Hay un método sencillo de orientación para los derivados disustituidos que fue establecido por Körner. Frecuentemente es llamado método 2,3,1 de Körner. Se basa en el principio de que la introducción de un tercer sustituyente en un compuesto "para" proporciona un producto trisustituido, en el isómero "orto" dos y en el "meta" tres. Körner aplicó este principio para establecer la orientación de los dibromobencenos isómeros. Nitró cada uno de ellos y examinó el número de productos nitrados. El isómero que dio un solo dibromo-nitrobenceno es el "para"; el que dio dos derivados nitrados, el "orto", y el tercero que dio tres, es el compuesto "meta".

Los hidrocarburos como el tolueno, etilbenceno, fenol, etc., tienen carácter alifático y aromático. El benceno es no polar, lo mismo que el metano, siendo cero el momento dipolar de cada uno de estos compuestos. Sin embargo, el tolueno tiene un pequeño momento dipolar (aproximadamente 0,4 D) con la carga negativa sobre el núcleo y la positiva sobre el grupo metilo. Los alquilbencenos experimentan la cloración y bromación, ya sea en el núcleo o en la cadena lateral, según sean las condiciones de la reacción. Para denominar las posiciones relativas del benceno, véase Patrones de sustitución en hidrocarburos aromáticos.

Respirar niveles de benceno muy altos puede causar la muerte, mientras que niveles bajos pueden causar somnolencia, mareo y aceleración del latido del corazón o taquicardia. Comer o tomar altos niveles de benceno puede causar vómitos, irritación del estómago, mareo, somnolencia o convulsiones y, en último extremo, la muerte.

La exposición de larga duración al benceno se manifiesta en la sangre. El benceno produce efectos nocivos en la médula ósea y puede causar una disminución en el número de hematíes, lo que conduce a padecer anemia. El benceno también puede producir hemorragias y daños en el sistema inmunitario, aumentando así las posibilidades de contraer infecciones por inmunodepresión.

Los efectos nocivos del benceno aumentan con el consumo de bebidas alcohólicas.

Algunos estudios sobre una muestra de mujeres que respiraron altos niveles de benceno durante varios meses han revelado que presentaron menstruaciones irregulares, así como disminución en el tamaño de sus ovarios. No se sabe si la exposición al benceno afecta al feto durante el embarazo. Varios estudios en animales han descrito bajo peso de nacimiento y problemas en la formación de huesos.

El Departamento de Salud y Servicios Sociales de los Estados Unidos (DHHS) ha determinado que el benceno es un reconocido carcinógeno en seres humanos y otros mamíferos lactantes. La exposición de larga duración a altos niveles de benceno en el aire puede producir leucemia así como cáncer de colon.

En el organismo, el benceno es transformado en productos llamados metabolitos. Ciertos metabolitos pueden medirse en la orina o en las heces. Sin embargo, este examen debe hacerse con celeridad después de la exposición y el resultado del análisis no indica a que concentración de benceno se estuvo expuesto, ya que los metabolitos en la orina pueden originarse a partir de otras fuentes.

El benceno se utiliza como constituyente de combustibles para motores, disolventes de grasas, aceites y pinturas; en el grabado fotográfico de impresiones; como intermediario químico en la manufactura de detergentes, explosivos, productos farmacéuticos y pinturas; en la síntesis de otros productos químicos, como el estireno, cumeno (en varias resinas) y ciclohexano (en nailon y fibras sintéticas), en la manufactura de ciertos tipos de caucho, lubricantes y plaguicidas.



</doc>
<doc id="15435" url="https://es.wikipedia.org/wiki?curid=15435" title="Tipos anatomopatológicos de cáncer de mama">
Tipos anatomopatológicos de cáncer de mama

Los carcinomas de mama pueden encontrarse en dos formas principales según su origen. Un noventa por ciento, aproximadamente, tienen su origen en el epitelio ductal. El restante diez por ciento, en las células de los acinos glandulares. El primer tipo, además, puede presentarse en formas variadas que suelen clasificarse como subtipos, existiendo distintos tipos de rasgos anatomopatológicos, macroscópicos y microscópicos, que los distinguen. La clasificación puede presentarse como sigue:

Distribución:




</doc>
<doc id="15436" url="https://es.wikipedia.org/wiki?curid=15436" title="Zarzuela">
Zarzuela

La zarzuela es una forma de música teatral o género musical escénico surgido en España que se distingue principalmente por contener partes instrumentales, partes vocales (solos, dúos, coros...) y partes habladas, aunque existen excepciones en las que estas últimas, las partes habladas, están completamente ausentes. El término «zarzuela», aplicado al género musical y teatral, procede del Palacio de la Zarzuela, palacio real español situado en las proximidades de Madrid y en el que se hallaba el teatro que albergó las primeras representaciones del género.

De tal forma reductora y errónea se ha asimilado la zarzuela a la opereta, género de origen francés, principalmente por contener partes habladas o declamadas, pretendiendo así que «la zarzuela es la opereta española». Pero la zarzuela es históricamente muy anterior y esa característica ya se encontraba en otros géneros europeos, también muy anteriores a la opereta y no necesariamente anteriores a la zarzuela. En realidad en ese sentido la zarzuela sería más bien el equivalente español del "opéra-comique" francés o del "singspiel" alemán. Dichos géneros de Francia y del mundo germánico se caracterizan por producir representaciones teatrales y musicales en las que, a diferencia de la ópera propiamente dicha, se alterna música con partes habladas o declamadas. "La flauta mágica" de Mozart, por ejemplo, no es una ópera sino un "singspiel" y, por consiguiente, tanto sentido tiene decir que «la zarzuela es la opereta española» como decir que «el "singspiel" es la zarzuela vienesa». A pesar de todo, ha habido zarzuelas del género grande que por no tener partes habladas son parecidas al "grand opéra" francés o a la ópera seria italiana. Por lo tanto la zarzuela se definiría de una manera más adecuada, y más simple, como el arte lírico y escénico propiamente hispánico, pues aunque naciera en España, al poco tiempo de su aparición se extendió a la casi totalidad del mundo hispánico.

Parece ser que los primeros autores que aportaron a este nuevo estilo de teatro musical fueron Lope de Vega y Calderón de la Barca. Según las investigaciones, Calderón de la Barca es el primer dramaturgo que adopta el término de zarzuela para una obra suya titulada "El golfo de las sirenas" que se estrenó en 1657 y que representaba la vida de un joven aventurero que emprendía un largo viaje lleno de misterios y peligros.

Lope de Vega escribió una obra que tituló "La selva sin amor", comedia con orquesta. Según el autor era «cosa nueva en España». En el prólogo de la edición de 1629 se lee: «Los instrumentos ocupaban la primera parte del teatro, sin ser vistos, a cuya armonía cantaban las figuras los versos en aquella frondosa selva artificial, haciendo de la misma composición de la música las admiraciones, quejas, iras y demás afectos…». Sin embargo, sólo se conserva la música suficiente en la obra "Los celos hacen estrellas" de Juan Hidalgo de Polanco y Juan Vélez de Guevara, que se estrenó en 1672. Con esta obra se puede tener una idea de cómo era este género en el siglo XVII y como marcó la diferencia para las siguientes doctrinas del género. 

Con el advenimiento de la dinastía de los Borbones, desde principios del siglo XVIII, se pusieron de moda los estilos italianos en diversas manifestaciones artísticas, incluida la música y la danza en los centros de convivencia de la plebe. Las zarzuelas se convirtieron en obras estilísticamente parecidas a las óperas italianas, como por ejemplo las obras de Antonio de Literes. Pero al llegar el reinado de Carlos III, amante de las buenas representaciones teatrales, los problemas políticos provocaron una serie de revueltas contra los ministros italianos llevando el conflicto a la toma de ayuntamientos y disturbios frecuentes (como, por ejemplo, el motín de Esquilache) (i.e. Squillace), hecho que repercutió en las representaciones teatrales y de nuevo se volvió a la tradición popular española representada, en esta ocasión, por los sainetes de don Ramón de la Cruz, cuya primera obra representada de este género fue "Las segadoras de Vallecas" (1768), con música de Rodríguez de Hita.

El auge de la zarzuela y su fama le llegó en el siglo XIX, a partir de 1839, con varios músicos entre los que destacan Francisco Barbieri y Emilio Arrieta. Muchas veces el éxito de la obra se debía a una o más canciones que el público aprende y da a conocer oralmente a los demás por medio de representaciones acústicas, como ocurría con los cuplés. La estructura de la obra siguió siendo la misma: números hablados, cantados, coros, que se aderezan con escenas cómicas o de contenido amoroso que, generalmente, son interpretadas por un dúo. Abundaba el género costumbrista y regionalista y en los libretos se recogía toda clase de modismos, regionalismos y jerga popular para asegurar que la interpretación fuera un éxito.

Al contrario que las escenas españolas, ambientadas en la Corte o en aldeas, la zarzuela cubana describía imágenes y costumbres coloniales, utilizando las suaves cadencias musicales que dan a Cuba tanto reconocimiento mundial. Tema popular era el señorito rico, hijo del dueño del ingenio, que aunque comprometido con una joven de su clase, cortejaba a la joven mulata, zalamera y atrevida, con quien tenía amores prometiéndole matrimonio. El final era por lo general truculento, con desengaños, pasión, celos y lágrimas. Estos impresionantes finales no restaban un ápice a la belleza de la música, antes bien ponían énfasis en las habilidades y talentos histriónicos y musicales de los artistas de la interpretación teatral y musical del Divino Maestro.

Hacia mediados del siglo se adoptan temas costumbristas, populares, cómicos y bailes españoles; algunos músicos respetados de este período son Emilio Arrieta, Federico Chueca, Fernández Caballero, Tomás Bretón y Ruperto Chapí. 

Después de la Revolución de 1868, el país entró en una profunda crisis (sobre todo económica) que se reflejó también en el teatro: el espectáculo teatral se convirtió en un entretenimiento caro, al alcance de pocos bolsillos. Fue entonces cuando el Teatro Variedades de Madrid tuvo la idea de reducir la duración de la representación, para abaratar el precio del espectáculo: la función teatral, que hasta entonces duraba unas cuatro horas, se redujo a una hora, lo que se llamó teatro por horas. La innovación tuvo un gran éxito y los compositores de zarzuelas se acomodaron al nuevo formato, creando obras mucho más cortas cuyo verdadero triunfo tardó diez años, hasta 1879. A las zarzuelas de un solo acto se las clasificó como género chico y a las de dos o más actos, género grande. La zarzuela grande se matuvo en el Teatro de la Zarzuela de Madrid, aunque con poco éxito y poco público. A pesar de esto, en 1873 se abrió un nuevo teatro Apolo de Madrid, que compartió los fracasos con el anterior, por querer hacerle un lugar para el drama y la comedia, hasta que no tuvo más remedio que cambiar el espectáculo al género chico en el que triunfó durante decenios.

 En los primeros años del siglo XX se componen obras de mayor calidad musical como "El puñao de rosas", "La alegría del batallón", "El trust de los tenorios" en el género chico y "Doña Francisquita" de Amadeo Vives, "La calesera" o, un poco antes (en 1898), "Gigantes y Cabezudos" del maestro Manuel Fernández Caballero, que supo ganarse muy bien a la crítica componiendo una obra muy del "gusto popular". 

Paralelamente, se empieza a dar el apelativo de "género ínfimo" a las representaciones conocidas como revistas. Son obras musicales con conexión a algunas ideas de la zarzuela pero más ligeras y atrevidas, con números escénicos que, en la época, se calificaron de «verdes», es decir, pícaros para los tiempos de hoy, que hablaban o ponían sobre la mesa la evolución de la sociedad sobre temas sexuales y con letras de doble intención, en casi todas hay "cuplés". Una de estas obras fue "La corte de Faraón", basada en la opereta francesa "Madame Putiphar". La música se hizo tan popular que algunos de sus números acabaron siendo verdaderos cuplés difundidos por el público.

En el primer tercio del siglo, la zarzuela se va manteniendo con producciones que, a veces, se ajustan a la estructura musical de una ópera italiana, gracias a autores de la talla de Francisco Alonso, José Padilla, Pablo Sorozábal, Federico Moreno Torroba, Tomas Barrera Saavedra, Rafael Calleja, Pablo Luna, José Serrano Simeón y Jacinto Guerrero.

La Guerra Civil abre un paréntesis nefasto que acaba por agravar el mismo problema de los años anteriores, y en la posguerra la decadencia es casi total. No existen apenas nuevos autores de este género y no se renuevan las obras por no cuajar los estrenos como lo hicieron en otras épocas. Por otro lado, la zarzuela preexistente es difícil y costosa de representar y sólo aparece de forma esporádica, por temporadas, durante unos pocos días o semanas.

La zarzuela se cultivó con muchos aciertos al trasladarse a Cuba, donde destacaron los compositores Gonzalo Roig y Ernesto Lecuona, y Rodrigo Prats, Eliseo Grenet, Argentina, en cuya capital hasta en 3 teatros se representaba "La verbena de la Paloma" el año de su estreno y a Venezuela, con José Ángel Montero y Pedro Elías Gutiérrez.

En Argentina, la zarzuela, el sainete y el tango conformaron un nuevo género peculiar de gran éxito popular conocido como sainete criollo.

En Filipinas, la popularidad de las zarzuelas cedió a la indigenización de este género. Durante la colonización norteamericana, las "sarswelas" (la forma indígena) fueron una forma mayor de mostrar resistencia a fuerzas extranjeras. Honorata 'Atang' de la Rama fue conocida como la Reina de la Sarswela Filipina. Esta forma de arte se llama también "zarzuelta" en varios lugares del país.

Los siglos XIX y XX fueron épocas de gran producción de zarzuelas en la Hispanoamérica, en especial en Venezuela, Cuba, México y Argentina, de donde salieron grandes obras que todavía son presentadas internacionalmente como "El cumpleaños de Leonor", de Montero que era la historia de una mujer mayor que al descubrir la traición de su marido buscaba una vida mejor en la gran ciudad; "María la O" de Ernesto Lecuona y "La media naranja" del íberoargentino Antonio Reynoso.

En Argentina la zarzuela comenzó a difundirse en la segunda mitad del siglo XIX en el mismo momento en que se generaba el tango, a partir de la fusión de diversos estilos locales, de origen africano, gaucho e indígena, y otros aportados por contingentes inmigratorios de diferentes partes del mundo que estaban llegando en gran cantidad al país.

En España, como género específico, tuvo mucho éxito popular hasta la segunda mitad del siglo XX. Varios músicos españoles compusieron zarzuelas, como Lópe de vega con "Amalia", "La Pericona" y "Madame Lynch". Antonio Videgain García que vivió allí compuso alguna obra de este corte y Francisco Alonso, por su parte, compuso "Manuelita Rosas" (1941), una zarzuela ambientada en Argentina.

Pero la zarzuela además fue una importante influencia en la gestación del tango. De hecho, la primera vez que se usó la palabra "tango" para nombrar al género musical, fue en una zarzuela, "Justicia Criolla" de Ezequiel Soria.

Pero además la zarzuela fue una de las fuentes del tango, dando lugar al tango "azarzuelado", a la vez que influyó en la creación de un género dramático-musical de Argentina, que adoptó el nombre de "sainete criollo", designado a excepcionalmente también como "zarzuela criolla", que tuvo enorme éxito popular, con obras destacadas como "El conventillo de la Paloma" de Alberto Vacarezza.

La zarzuela llegó a las Filipinas en 1879 o 1880, cuando el grupo de Dario de Céspedes presentó el Juego de fuego en Manila. Desde entonces, varios grupos filipinos comenzaron a hacer su propias zarzuelas en varios idiomas indígenas. Las más populares fueron escritas en tagalo, pampangueño, ilocano, cebuano, panayano y samareño.

La primera sarswela conocida en samareño es "An Pagtabang ni San Miguel" (El Ayuda de San Miguel) de Norberto Romualdez, mientras "Ing Managpe" de Mariano Proceso Pabalan Byron es la primera en pampangueño. El héroe nacional José Rizal, músico además de literato, es autor de la zarzuela llamada "Junto al Pásig".

A la llegada del vodevil, las sarswelas perdieron su popularidad, pero renovaron su éxito con la llegada del cine. Muchas sarswelas, principalmente las tagalas, se filmaron para el cine.

A partir de 1950 la zarzuela pudo sobrevivir en el gusto popular gracias a la discografía, un campo que se mantuvo en auge desde entonces. Se produjeron una serie de grabaciones de gran éxito, la mayoría de ellas dirigidas por el músico español Ataúlfo Argenta colaborando músicos tan respetados como Mary Carmen Alvira o Julián Parera. Las mejores voces del momento aparecieron en estos discos, cantantes mundialmente famosos que profesionalmente se dedicaban a la ópera y a los recitales. Voces como las de Teresa Berganza, Ana María Iriarte, Carlos Munguía, etc., participaron en las grabaciones. Se añadieron los coros del Orfeón Donostiarra y Coro de Cantores de Madrid contribuyendo a darles una gran calidad. Pero por otro lado enturbió el recuerdo de los cantantes del estreno que empezaron a no ser recordados como los participes del éxito de esas obras.

Tras la muerte de Ataúlfo Argenta se incorporaron los directores Indalecio Cisneros, García Asensio, y otros. Incluso hubo grabaciones raras porque fueron dirigidas por el propio autor de la obra por el motivo que argumente anteriormente, como fue el caso de Pablo Sorozábal y Federico Moreno Torroba. En esta etapa participaron en las grabaciones nuevas y grandes voces consagradas: Monserrat Caballé, Alfredo Kraus, Plácido Domingo, Juan Pons, etc.

Durante los años 60, Radio Televisión Española inició la producción de una serie de zarzuelas interpretadas por conocidos actores del momento, tales como José Moreno, Antonio Casal, Juan Luis Galiardo, María Cuadra y María José Alfonso), con buenas direcciones musicales, normalmente a cargo de Federico Moreno Torroba, y utilizando voces o cubriendo sus deficiencias con el doblaje por artistas líricos de reconocido prestigio como Alfredo Kraus o Luis Sagi Vela para los números vocales, grabados con la técnica del "playback". Muchas de ellas las dirigió Juan de Orduña y se emplearon, en lo posible, escenarios naturales para la grabación de las mismas, lográndose obras de notable calidad, especialmente en el apartado musical. Con este sistema se grabaron, por ejemplo:

En los últimos años del decenio de 1970 a 1980 se reaviva el interés por la zarzuela, en especial por su música. En toda Europa se desencadena un renacer de la afición por los espectáculos líricos, sobre todo entre la juventud. Este renacimiento repercute en España que muestra un gran interés por la zarzuela. El empresario José Tamayo pone en escena un espectáculo teatral de gran producción hacía tiempo olvidado, "Antología de la zarzuela", representando los fragmentos más populares del repertorio de zarzuela moderna con cantantes de primera línea, montaje que se mantiene durante decenios renovando los números incluidos. Las casas discográficas ofrecen colecciones cuyos discos van acompañados de un fascículo que contiene la sinopsis de la obra y algunos datos del autor. La radio y la televisión dedican varios espacios a su programación. Los programas que TVE ofreció con el título de "Antología de la Zarzuela", basados en playbacks de las grabaciones de mediados de siglo representados en estudio de TV con vestuarios y baile, gozaron de una gran audiencia. En cuanto a los años más recientes, según datos de la SGAE en 2006 la zarzuela experimentó un aumento de más de un 4%.
Además, la Fundación Autor se ha unido con la Fundación de la Zarzuela para la promoción de la zarzuela entre nuevos públicos con los proyectos "Zarzuela en femenino", con las sopranos Isabel Segarra y Sonia Martínez, presentado en Málaga en 2014 y "Zarzuela en masculino", en producción.








</doc>
<doc id="15437" url="https://es.wikipedia.org/wiki?curid=15437" title="Diagnóstico del cáncer de mama">
Diagnóstico del cáncer de mama

El diagnóstico del cáncer de mama con certeza requiere el examen microscópico de una muestra del tejido mamario sospechoso (biopsia). La biopsia, sin embargo, es tan solo el último escalón en una cadena de procedimientos cuyo objetivo en separar los estudios mamarios en dos grupos principales: los que presentan algún grado de sospecha de cáncer y los que no. 

La anamnesis (interrogatorio) junto con una entrevista clínica y seguidas del examen físico o exploración física de la mama es el primer paso que se da para identificar si hay indicios de enfermedad. 

Dentro del interrogatorio es de suma importancia investigar si la paciente tiene familiares directos que han tenido cáncer de mama (madre, hermana), si ha tenido tumores benignos en mama, si su menstruación fue de inicio temprano (12 años o menos) y su menopausia tardía (mayor de 50 años), si ha tomado anticonceptivos, si fuma; ya que todos estos elementos se han identificado como factores de riesgo del cáncer de mama. Después de eso, se debe averiguar si la paciente ha tenido dolor mamario (mastalgia) o ha presentado alguna tumoración.

Posteriormente sigue el examen de mama que la paciente debe realizarse mensualmente. 

Un médico especialista también debe explorarla al menos cada 6 meses. Para ello examinará ambas mamas y los ganglios linfáticos de las axilas, buscando nódulos, anomalías, tumoraciones, deformidades en piel, en pezones o bien tumoraciones por arriba o abajo de la clavícula o en axila.

A continuación, si ha sido posible obtener algún dato que lo justifique, debe recurrirse a algunas de las siguientes técnicas de diagnóstico por imagen :


La mamografía por rayos X, es un estudio que proporciona imágenes radiológicas en varias proyecciones, obtenidas en un aparato de rayos X que ha sido diseñado especialmente para estudiar las mamas. Otras técnicas, como la galactografía, la neumoquistografía y la neumooncografía, representan variantes de la mamografía en las que se asocian técnicas invasivas para precisar el estudio de determinadas alteraciones.

La ecografía mamaria es una técnica de diagnóstico por imagen que complementa a otras, su principal utilidad consiste en la distinción de la naturaleza sólida o quística de lesiones nodulares identificadas en la mamografía. También es de utilidad en el estudio de mama con un componente glandular importante que condiciona una elevada densidad de la imagen mamogràfica, dificultando la discriminación de posibles lesiones. Permite una medición muy precisa del tamaño de los nódulos mamarios y es de gran utilidad para guiar punciones para obtener material celular o tisular para examen citológico o biópsico que permitan el estudio y diagnóstico histo-patológico. 

El mamógrafo de impedancia eléctrica a través del cálculo de la conductividad eléctrica de los tejidos, proporciona información importante sobre procesos fisiológicos y patológicos en la glándula mamaria, que permite diagnosticar el riesgo de cáncer, FCD [Fibro Cystic Disease, según sus siglas en inglés] (enfermedad fibroquística), mastitis, involución fisiológica, evaluación de la lactancia, etc.

La mamografía de impedancia eléctrica permite ver la distribución de la conductividad del tejido biológico en varias secciones trasversales de la glándula y detectar tumores visualizándolos como área con valores anormales de conductividad eléctrica. El principio es sustentado por una suposición de que los tumores malignos de la glándula mamaria muestran una conductividad eléctrica mucho mayor (habilidad del paso de la corriente eléctrica) que aquella de los tejidos sanos circulantes.
Este tipo de tecnología es sin dolor, sin radiación y se puede utilizar en pacientes de cualquier edad entre otros beneficios, su diagnóstico es de alta eficacia ya que da un resultado tanto cualitativo como cuantitativo.

Es ideal para la detección oportuna del cáncer de mama, ya que aparte de ser rápido en su aplicación es la mejor herramienta en su tipo, para realizar Cribado o Tamizaje de una marea rápida y segura.

La resonancia magnética y la T.E.P. (o P.E.T.) tienen importancia en casos concretos y su empleo, en la actualidad, no es rutinario. Sin embargo, las indicaciones de su empleo van ampliándose cada vez más. Las principales indicaciones de la resonancia son el seguimiento de cambios cicatrizales mamarios intensos post quirúrgicos, el estudio de multicecentricidad del cáncer mamario, la valoración de la extensión local para apoyar o contraindicar el tratamiento conservador y el estudio de complicaciones de prótesis mamarias. 

Cuando se obtiene una mamografía, el radiólogo examina cuidadosamente las imágenes obtenidas buscando ciertos signos radiológicos que son conocidos como indicadores probables de patología. Las imágenes pueden visualizarse de manera analógica, utilizando como soporte una película radiográfica especial para mamografía; o bien de manera digital, utilizando sistemas informáticos.

La biopsia permite realizar un diagnóstico definitivo de cáncer de mama. Para realizar la biopsia, se utiliza una aguja especial guiada por radiografía u otra técnica de diagnóstico por imágenes para extraer una porción muy pequeña de tejido de la zona sospechosa. A menudo el especialista coloca un pequeño marcador metálico dentro de la mama para poder identificar la zona más fácilmente en futuras pruebas de diagnóstico por imágene.

Las muestras de tejido extraídas en la biopsia son analizadas en un laboratorio para determinar si las células son cancerosas. El análisis de la muestra de biopsia determina el tipo de células presentes en el cáncer de mama, el grado del cáncer, y la exisencia de receptores hormonales en las células cancerosas que deban ser tenidos en cuenta a la hora de analizar los posibles tratamiento.



</doc>
<doc id="15440" url="https://es.wikipedia.org/wiki?curid=15440" title="Ilusionismo">
Ilusionismo

El ilusionismo, popularmente denominado magia, es un arte escénico, subjetivo, narrativo y espectáculo de habilidad e ingenio, que consiste en producir artificialmente efectos en apariencia maravillosos e inexplicables mientras se desconoce la causa que los produce. Estos efectos (desapariciones, transformaciones, uniones, lecturas de la mente, etc), que fingidamente hacen parecer realidad lo imposible, se conocen como efectos, juegos de manos, ilusiones y vulgarmente como trucos de magia.

Dentro de la magia caben diversas especialidades: fantasistas, prestidigitadores, prestímanos, cartomagos, mentalistas, escamoteadores y reyes de la evasión con o sin ataduras, que protegen sus trucos con el compromiso del secreto profesional.

Conocido bajo los diversos nombres de "magia simulada", "magia blanca" o "escamoteo", el ilusionismo se remonta a la más lejana antigüedad.

Los primeros datos escritos y documentados de magia vienen de Egipto, hace más de 4000 Años.Un dibujo en la pared de una cámara mortuoria de la ciudad de Beni Hassan —trazado probablemente 2200 años antes de Cristo— representa a dos hombres dedicados a realizar con unos cuencos en forma de copa lo que parece un truco de ilusionismo. Los jeroglíficos que indican salida de debajo dan la impresión de confirmar que debajo de una de las vasijas se encuentra una bola o algún pequeño objeto redondo, a punto de aparecer en forma mágica.

Henry Westcar, un aventurero británico, descubrió en 1825 el papiro Westcar, primer documento que describe una función mágica realizada por el mago Dyedi en la corte real de Khufu (Keops). El papiro ilustra la categoría única y especial de la que gozaba el arte del ilusionismo. Según aquel texto, Dyedi era toda una leyenda entre los hombres. Se le atribuían ciento diez años de edad y unos apetitos que rivalizaban con los dioses. Su mera presencia inspiraba temor a los hombres normales. Hasta el faraón omnipotente solicitó que compareciese ante él.

De su actividad nómada y feriante constan testimonios en manuscritos del siglo X y entre sus primeros grandes especialistas figuran el maestro Gonin (finales del s. XVI), fundador de una dinastía de magos.

Asimilados a los "hombres de ciencia", utilizaron autómatas y otros artefactos antes de que en el s. XVIII incorporasen la electricidad y otros adelantos científicos para ampliar su repertorio de trucos, que presentaban bajo el nombre de "física" y de donde nació la física recreativa. La consolidación de esta profesión se produjo en el s. XVIII, dando lugar a la aparición de los "teatros de magia" (Robertson, Robert-Houdin). En un principio, el repertorio se reducía sobre todo al "escamoteo" (hacer desaparecer un objeto para encontrarlo en otro lugar distinto de aquel en que debiera estar o hacer aparecer otro en su lugar), pero se amplió luego con trucos complejos de "gran magia".

El más célebre mago y escapista del siglo XIX (y posiblemente de todos los tiempos) fue Harry Houdini (1874-1926), tomó su nombre profesional de Harry Keller y del mencionado Robert-Houdin, y desarrolló una serie de ilusiones de magia escénicas, basadas muchas de ellas en el arte del escape.

A finales del siglo XX, el ilusionismo volvió a tener auge de la mano de Doug Henning primero, y David Copperfield después, a través de sus especiales televisivos, espectáculos en Broadway y giras internacionales.



San Juan Bosco (1815-1888) es el patrón de los ilusionistas. Fueron los mismos magos quienes en un congreso internacional celebrado en Segovia (España), lo escogieron como modelo y protector, a mediados del siglo XX.

Don Bosco, como tradicionalmente se le conoce, fue un sacerdote moderno, cercano a los jóvenes más pobres, que supo ganarse la amistad de éstos con técnicas inspiradas en el ilusionismo, así logró evitar que muchos fueran a la cárcel y que tomaran en su vida el buen camino.

Se suele clasificar el ilusionismo según diferentes conceptos en función de la distancia a los espectadores, número de ellos y localización de la presentación:

En función de los objetos utilizados:


Entre los objetos que más comúnmente se han usado para realizar magia destacan: naipes, monedas, animales (palomas, conejos, pájaros...), aros y anillas, agujas, anillos, bastones, billetes, bolas, bombillas, botellas, cigarrillos, corbatas, cubiletes, cubos, cuerdas, dados, dedales, discos, espadas y cuchillos, esponjas (en forma de bolas y figuras), flores, frutas y verduras, fuego, globos, gomas elásticas, hilos, huevos, humo, imperdibles, leche y otros líquidos, navajas, palitas, pañuelos, papeles y periódicos, paraguas, pizarras, relojes, teléfonos móviles, tubos, varitas, vasos y copas, velas, vestidos... además de los aparatos usados en las grandes ilusiones: armarios, baúles, cajas, jaulas, cortinas, etc. Para el escapismo también se suelen usar sogas, cadenas, esposas, camisas de fuerza, candados y cerraduras, etc. y poleas para elevar en el aire al ilusionista.

En función de los tipos de efectos:


Los objetos más comunes a la hora de robar en un espectáculo de pickpocketing son: relojes, carteras, corbatas, cinturones, gafas, pañuelos, teléfonos móviles... es decir, cualquier objeto que pueda llevarse en los bolsillos tanto de la chaqueta o camisa, como de los pantalones.

Por lo general, este tipo de demostraciones suelen ser cómicas y hechas en escenario, por lo que el objeto es devuelto a la supuesta víctima al terminar el número.

Como en toda profesión existe un día en el año para celebrar el ejercicio de la misma, para la magia es el 31 de enero, festividad de San Juan Bosco, día internacional del mago. En Norteamérica también es celebrado el 31 de octubre, National Magic Day, fecha en que se conmemora el fallecimiento de Harry Houdini.

El ilusionista realiza un efecto que el espectador percibe como maravilloso, contrario a las leyes naturales o al sentido común, y cuya causa le parece desconocida e inexplicable. Entre los posibles efectos mágicos que se pueden realizar están:






</doc>
<doc id="15442" url="https://es.wikipedia.org/wiki?curid=15442" title="Johann Strauss (hijo)">
Johann Strauss (hijo)

Johann Strauss (Neubau , 25 de octubre de 1825-Viena, 3 de junio de 1899) fue un compositor austriaco conocido especialmente por sus vals, como "El Danubio azul". Hijo del compositor Johann Strauss y hermano de los compositores Josef Strauss y Eduard Strauss, Johann es el más famoso de la familia Strauss. Fue conocido en su vida como «el rey del vals» y a él se debe en gran medida la popularidad del vals en la Viena del siglo XIX. Lo revolucionó, elevándolo de una danza campesina a una de entretenimiento apta para la Corte Imperial de los Habsburgo. Sus obras gozan de mayor popularidad que las de sus predecesores, como su padre y Josef Lanner. Algunas de sus polcas y marchas son también muy conocidas, así como su opereta "Die Fledermaus" ("El Murciélago").

El único familiar que le prestó su apoyo fue su madre. Por el contrario, al ser descubierto por su padre, Johann recordaría «una desagradable y violenta escena» y que su padre «no quería saber nada de sus planes musicales». Al parecer, en lugar de evitar que Strauss se convirtiera en su rival, el padre quería apartar a su hijo de los rigores de la vida de músico. Fue entonces cuando Strauss padre abandonó a la familia y encontró una amante, Emilie Trampusch, cuando Johann tenía 17 años y había decidido concentrarse plenamente en la carrera de compositor con la ayuda de su madre.

Entonces Strauss comenzó a estudiar contrapunto y armonía con el profesor Joachim Hoffmann, quien poseía una escuela privada de música. Su talento fue reconocido asimismo por el compositor Josef Drechsler (también escrito Drexler), quien le enseñó ejercicios de armonía. Su otro maestro de violín fue Anton Kollmann, rèpètiteur del ballet de la Ópera de la Corte de Viena. Armado con esto, el mismo día que su madre había solicitado el divorcio de su marido, se presentó ante las autoridades vienesas para actuar en público. Inicialmente formó una pequeña orquesta, contratando a algunos músicos de la taberna Zur Stadt Belgrad.

La influencia de Johann Strauss I en varios establecimientos de entretenimiento significó que muchos de ellos fueron cautelosos en ofrecer un contrato al joven Strauss, temiendo el enojo del padre. Strauss hijo fue capaz de persuadir al Casino Dommayer en Hietzing, Viena, para que hiciera su debut. La prensa local se apresuró a divulgar la noticia de Strauss contra Strauss, rivalidad entre padre e hijo. Strauss padre, encolerizado con su hijo y ante la desobediencia del propietario, se negó a tocar nunca más en el Casino Dommayer, que había sido el escenario de sus anteriores triunfos.

Strauss encontró muchas dificultades en sus primeros años como músico, pero pronto ganó audiencia amante de la música, tras haber aceptado comisiones para actuar fuera de Viena. El primer gran reconocimiento para el joven compositor fue la posición de Maestro de Capilla del Segundo Regimiento de Ciudadanos de Viena, que había quedado vacante tras la muerte de Josef Lanner dos años antes. Viena fue asolada por una revolución burguesa el 24 de febrero de 1848, y la rivalidad entre padre e hijo se hizo mucho más evidente.

Johann decidió apoyar a los revolucionarios, como lo ponen de manifiesto los títulos de dos obras que datan de este período, los valses "Freiheitslieder" ("Canciones de Libertad") op. 52 y "Burschenlieder" ("Canciones de los Jóvenes") op. 55, así como las marchas "Revoluciones de Marzo" op. 54 y la agitada "Marcha de los Estudiantes" op. 56. Esta decisión demostró serle desfavorable profesionalmente, ya que la realeza austriaca le negó dos veces la tan codiciada posición de "KK Hofballmusikdirecktor" (Director Musical del Baile de la Corte), posición que fue dada a Johann I en reconocimiento a sus contribuciones musicales. Por otra parte, el joven Strauss también fue apresado por las autoridades vienesas por tocar en público «La Marsellesa», atizando los sentimientos revolucionarios, pero más tarde fue absuelto. Poco después de su absolución, compuso la polca "Geißelhiebe" ("Latigazos") op. 60, que contiene elementos de «La Marsellesa» en su «Trío», como una sección musical en respuesta a su detención. Strauss padre se mantuvo leal a la monarquía del Danubio y compuso su "Marcha Radetzky" op. 228 dedicada al mariscal de campo Joseph Radetzky von Radetz que pasaría a ser su composición más conocida.

Cuando Strauss padre murió de escarlatina en Viena en 1849, el joven Strauss fusionó ambas orquestas y participó en numerosas giras. Posteriormente, también compondría una serie de marchas patrióticas dedicadas al monarca Francisco José I, como la "Kaiser Franz-Josef Marsch" ("Marcha del Emperador Francisco José") op. 67 y la "Kaiser Franz Josef Rettungs Jubel-Marsch" ("Marcha de Júbilo por la salvación del Emperador Francisco José") op. 126, probablemente para congraciarse con el nuevo monarca que subió al trono de Austria tras la Revolución de 1848.

Finalmente superó la fama de su padre, y se convirtió en uno de los más populares compositores de valses de su época, viajando por Austria, Polonia y Alemania con su orquesta. Sería habitual que el público viera una sola representación antes de que se trasladara rápidamente a otro lugar. Sería la primera y última representación en cada uno de esos lugares, y en cuyas placas proclamarían con orgullo «Heut Spielt der Strauss!» o «¡Hoy toca Strauss!».

También hizo visitas a Rusia, donde actúo en Pávlovsk y escribió varias composiciones, que más tarde retituló para que se ajustara al público vienés; a Gran Bretaña donde actuó con su primera esposa Hetty Treffz, en el Covent Garden, a Francia, Italia y más tarde a los Estados Unidos en la década de 1870, donde tomó parte en el Festival de Boston por invitación del maestro de banda Patrick Gilmore y fue el principal director en el "Monster Concert" de más de 1000 músicos, en el que interpretó su vals "El Danubio Azul" op. 314, entre otras piezas de gran éxito.

Entre las más populares piezas de baile que Strauss escribió en este período destacan los valses "Sängerfahrten" op. 41, "Liebeslieder" (Canciones de amor) op. 114, "Nachtfalter" (Mariposa nocturna) op. 157, "Accelerationen" (Aceleraciones) op. 234 y las polcas "Annen" (de Ana) op. 117 y "Tritsch-Tratsch" op. 214.

Se casó con la cantante Hetty Treffz en 1862 y postuló al título de Director Musical del Baile de la Corte, puesto que alcanzó en 1863, después de haberle sido negado en varias ocasiones por sus frecuentes fricciones con las autoridades locales. Su participación en el Baile de la Corte significaba que sus trabajos serían ahora escuchados por la realeza. Su segunda esposa, Angelika Dittrich (actriz), con quien se casó en 1878, no era una ferviente partidaria de su música y la diferencia de edad y opiniones, y sobre todo su indiscreción, llevó a que Johann le pidiera el divorcio.

A Strauss no le concedió la nulidad la Iglesia católica y, por lo tanto, cambió de religión y nacionalidad y se convirtió en ciudadano de Sajonia-Coburgo-Gotha el 28 de enero de 1887. Strauss buscó consuelo en su tercera esposa, Adele (con quien se casó el 15 de agosto de 1882) y en sus últimos años fluyeron sus talentos creativos, lo que resultó en gran parte buena música, como las que se encuentran en las operetas "Der Zigeunerbaron" y "Waldmeister" y los valses "Kaiser-Walzer" op. 437, "Kaiser Jubiläum" op. 434, "Märchen aus dem Orient" op. 444 y "Klug Gretelein" op. 462.

Adele, al igual que el abuelo de Strauss, era de origen judío, hecho que los nazis ocultaron.

Después de crear su primera orquesta antes de la muerte de su padre, fundó muchas otras que tocarían en diversos establecimientos de entretenimiento y de baile, como el «Sperl» y el «Apollo», a quienes les dedicó varias piezas con sus nombres para conmemorar sus primeras actuaciones. Más tarde, aceptó comisiones para tocar en Rusia para el archiduque Michael y el Zar Alejandro II, especialmente en Pavlovsk, donde había sido construida una nueva línea de ferrocarril. Cuando las comisiones se hicieron demasiadas para atenderlas él solo, trató de convencer a sus hermanos Josef y Eduard para que lo sustituyeran en su ausencia, ya fuese por su mala salud o por su apretada agenda. En 1853, incluso tuvo que internarse en un sanatorio, ya que sufría de escalofríos y padecía de neuralgia. Deseosa de que la empresa familiar no se viniera abajo, la madre Anna Strauss ayudó a convencer al reacio Josef para que asumiera el mando de la orquesta Strauss. Los vieneses acogieron con satisfacción a ambos hermanos y eventualmente hubo de admitir que «Josef era el más talentoso de ambos, yo simplemente soy el más popular». Josef dejó su propia marca con sus propios valses y esta nueva rivalidad fue muy propicia para el desarrollo del vals. Johann Strauss II procedió a consolidar su posición como «rey del vals» con su exquisito vals "El Danubio Azul" op. 314, que nació como un vals coral con texto escrito por un poeta local.

Lo más destacado del triunvirato de los Strauss queda de manifiesto en el "Concierto de Música Perpetua" en 1860, donde su acertada broma musical "Perpetuum Mobile" op. 257 se interpretó por los tres hermanos encabezando tres orquestas diferentes. En la misma época, los tres hermanos Strauss también organizaron numerosas actividades durante sus conciertos en el "Volksgarten" en Viena, donde el público podía participar. Por ejemplo, se interpretaba una nueva pieza y el público era invitado a adivinar cuál de los tres la había compuesto.

Aunque el más solicitado compositor de música de baile fue Johann Strauss entre 1860 y 1890, tuvo una dura competencia con Karl Michael Ziehrer y Émile Waldteufel; con el segundo competía desde París. Phillip Fahrbach también le negó al joven Strauss el cargo de Director Musical del Baile de la Corte cuando postuló por primera vez al cargo. El compositor alemán Jacques Offenbach, que se hizo famoso en París, le planteó asimismo un desafío a Strauss en el campo de la opereta. Más tarde, la aparición del maestro de la opereta Franz Lehár marcó el inicio de la Edad de Plata en Viena de este género y desplazó la posición de Strauss en el mundo de la opereta.

Johann fue admirado por otros prominentes compositores. Richard Wagner admitió una vez que amaba el vals "Vino, mujeres y canto", Op. 333. Richard Strauss (sin estar emparentado con la familia) al componer sus valses del El caballero de la rosa dijo en referencia a Johann Strauss hijo: «¿Cómo podría olvidar al sonriente genio de Viena?»

Johannes Brahms fue un amigo personal, a quien Strauss dedicó su vals "Seid umschlungen, Millionen!" ("¡Abrazaos, millones!"), Op. 443, inspirado en la Oda a la alegría de Friedrich Schiller. Una historia se cuenta en las biografías de ambos hombres: la hija de Strauss se acercó a Brahms con la intención de pedirle un autógrafo. Era usual que los compositores escribieran algunos compases de su música más conocida y firmaran con su nombre. Brahms, sin embargo, escribió unos cuantos compases de los valses más conocidos de Strauss y a continuación escribió: «Desafortunadamente, NO por Johannes Brahms».

La mayoría de las operetas de Strauss, sin embargo, no han tenido un éxito perdurable al compararlas con sus piezas de baile, y gran parte del éxito se lo adjudican "Die Fledermaus" ("El Murciélago"), "Una Noche en Venecia" y "El Barón Gitano". A pesar de la falta de popularidad de sus operetas, hay muchas piezas extraídas de ellas que fueron recibidas calurosamente, como el "Vals Cagliostro" op. 370 de la opereta "Cagliostro en Viena", el vals "Oh, hermoso mayo" op. 375 (Príncipe Matusalén), el vals Rosas del Sur op. 388 (El Pañuelo de Encaje de la Reina) y el Vals del Beso op. 400 (La Guerra Divertida). También escribió una ópera, "Ritter Pásmán", que tiene numerosas fallas en el libreto, pero muchos atribuyen su fracaso al uso de valses y polcas, lo que indicaría que era incapaz de escribir música seria. De hecho, para su tercera y más exitosa opereta de todos los tiempos, Die Fledermaus (El Murciélago) de 1874, los críticos de música de Viena profetizaron «que el motivo de las melodías serían valses y polcas». Sin embargo, su mayor crítico e irónicamente firme defensor Eduard Hanslick escribió en el momento de la muerte de Strauss en 1899 que su desaparición supondría el final de los tiempos felices en Viena.

Johann Strauss murió de neumonía en Viena el 3 de junio de 1899 a la edad de 73 años y fue sepultado en el Zentralfriedhof de Viena (Cementerio Central de Viena). Al momento de su muerte, se encontraba trabajando en su ballet "Aschenbrödel" ("Cenicienta").

La música de Strauss es interpretada regularmente en el Concierto de Año Nuevo de la Orquesta Filarmónica de Viena, como resultado de los esfuerzos de Clemens Krauss quien realizó un programa especial dedicado a Strauss en 1929 con la orquesta vienesa. Varios intérpretes de Strauss, como Willi Boskovsky, continuaron la tradición de dirigir violín en mano como era costumbre en la familia Strauss, así como Herbert von Karajan y Riccardo Muti. Además, la Orquesta de Viena Johann Strauss que se formó en 1966 le rinde homenaje durante las giras de esta conocida orquesta.



</doc>
<doc id="15443" url="https://es.wikipedia.org/wiki?curid=15443" title="Ecografía">
Ecografía

La ecografía (del griego «"ἠχώ"» [ēkhō] ‘eco’, y «"γραφία"» [grafía] ‘escribir’), también llamada ultrasonografía o ecosonografía, es un procedimiento de diagnóstico usado en los hospitales y clínicas que emplea el ultrasonido para crear imágenes bidimensionales o tridimensionales. Un pequeño instrumento muy similar a un "micrófono" llamado transductor emite ondas de ultrasonidos. Estas ondas sonoras de alta frecuencia se transmiten hacia el área del cuerpo bajo estudio, y se recibe su eco. El transductor es el responsable de enviar pequeños pulsos de ondas acústicas de alta frecuencia, inaudibles por el oído humano las cuales van hacia al interior del cuerpo. Estas rebotarán sobre órganos, tejidos o fluidos y el aparato registrará los cambios mínimos del sonido. Una computadora convierte este eco en una imagen que aparece en la pantalla. Este proceso ocurre gracias al llamado efecto piezoeléctrico.

La ecografía es un procedimiento sencillo, a pesar de que se suele realizar en el servicio de radiodiagnóstico; y por dicha sencillez, se usa con frecuencia para visualizar fetos que se están formando así como en ecografía musculoesquelética además de otros muchos usos. Es una prueba no invasiva, de bajo coste y sin riesgos a diferencia de otros procedimientos diagnósticos o pruebas de imagen como la radiografía, en los que se emplea radiación nuclear . Al someterse a un examen de ecografía, el paciente sencillamente se acuesta sobre una mesa y el médico mueve el transductor sobre la piel que se encuentra sobre la parte del cuerpo a examinar. Antes es preciso colocar un gel sobre la piel para la correcta transmisión de los ultrasonidos. No obstante, un inconveniente es que la ecografía requiere de una gran periodo de aprendizaje por parte del profesional que la realiza con el fin de interpretar correctamente las imágenes.

La ecografía podría dividirse en dos grupos, con contraste o sin contraste, normalmente la mayoría de las ecografías son con contraste, esta consiste en microburbujas de gas estabilizadas que presenta el fenómeno de resonancia incrementando así la señal que recibe el transductor. Este método de contraste es capaz de diferenciar entre tejidos normales y enfermos, aquellas zonas enfermas se verán más brillantes a la hora de hacer el examen, pero ante todo la experiencia del médico haciendo el examen es primordial para poder interpretar las imágenes de manera correcta. Por ejemplo si hay un tumor o cáncer como ya antes dicho se verá en el monitor más brillante por el aumento del flujo sanguíneo.

Para la mayoría de los exámenes de ultrasonido, el paciente será colocado boca arriba en una camilla se permitirá también mover al paciente de costado o boca abajo, pero en principio dependerá de cada tipo de examen a realizar. Al realizar el test se deberá de colocar un gel a base de agua que ayudará que el transductor haga contacto seguro con el cuerpo del paciente, por tanto este proceso se basa en romper las moléculas de aire que se pueden formar e impedir el paso de las ondas sonoras hacia el tejido, órgano, etc. 

Si es necesario antes de iniciar el examen, dependiendo de la zona que se quisiera ver se hará una inyección que se aplicará con un catéter intravenoso con el material de contraste ya que probablemente dicha zona a estudiar sea difícil de visualizar a través del monitor, se aplicará con un catéter intravenoso.

Los ultrasonidos fueron descubiertos por Lazzaro Spallanzani, mientras desarrollaba su labor de biólogo en 1794 estudiando a los murciélagos.

En 1880 en París Pierre Curie y su hermano Jacques descubrieron el efecto piezoeléctrico.

En 1881 Gabriel Lippman descubrió la reciprocidad del efecto piezoeléctrico lo cual permitía la posibilidad de la recepción y emisión de ultrasonidos.

En 1914 fue construido el primer sonar.

En 1935 fue inventado el primer sistema de radar por el físico Robert Watson-Wat.

En 1940, el americano físico-acustico Floyd Firestone creó el primer generador de imágenes de ultrasonido usando eco, lo llamó “Supersonic Reflectoscope”. En este mismo año, se aplicó por primera vez, energía ultrasonica sobre el cuerpo humano únicamente con propósitos médicos, en Maryland, Estados Unidos.

En 1941, en Austria, el psiquiatra Carl Theodore Dussik intentó identificar los ventrículos cerebrales midiendo la atenuación del ultrasonido a través del cráneo.

En 1947, el doctor Douglas Howry detectó estructuras de tejidos suaves al examinar los reflejos producidos por los ultrasonidos en diferentes interfaces.

En 1949, se publicó una técnica de eco pulsado para detectar cálculos y cuerpo extraños intracorpóreos. También el físico John Wild utilizó por primera vez el ultrasonido para ver la anchura del intestino.

En 1951 hizo su aparición el ultrasonido compuesto, en el cual un transductor móvil producía varios disparos de haces ultrasónicos desde diferentes posiciones y hacia un área fija. Los ecos emitidos se registraban e integraban en una sola imagen. Se usaron técnicas de inmersión en agua con toda clase de recipientes: una tina de lavandería, un abrevadero para ganado y una torreta de ametralladora de un avión B-29.

En 1952, Douglas Howry, Dorothy Howry, Roderick Bliss y Gerald Posakony publicaron imágenes bidimensionales del antebrazo, en vivo.

En 1952, John J. Wild y John Reid publicaron imágenes bidimensionales de carcinoma de seno, de un tumor muscular y del riñón normal. Posteriormente estudiaron las paredes del sigmoide mediante un transductor colocado a través de un rectosigmoideoscopio y también sugirieron la evaluación del carcinoma gástrico por medio de un transductor colocado en la cavidad gástrica.

En 1953, Lars Leksell, usando un reflectoscopio Siemens, detectó el desplazamiento del eco de la línea media del cráneo en un niño de 16 meses. La cirugía confirmó que este desplazamiento era causado por un tumor. El trabajo fue publicado solo hasta 1956. Desde entonces se inició el uso de ecoencefalografía con M-MODE.

En 1954, Ian Donald hizo investigaciones con un detector de grietas, en aplicaciones ginecológicas.

En 1956, Wild y Reid publicaron 77 casos de anormalidades de seno palpables y estudiadas además por ultrasonido, y obtuvieron un 90 por ciento de certeza en la diferenciación entre lesiones quísticas y sólidas.

En 1957, el ingeniero Tom Brown y el Dr. Donald, construyeron un escáner de contacto bidimensional, evitando así la técnica de inmersión. Tomaron fotos con película Polaroid y publicaron el estudio en 1958.

EN 1957, el Dr Donald inició los estudios obstétricos a partir de los ecos provenientes del cráneo fetal. En ese entonces se desarrollaron los cálipers (cursores electrónicos).

En 1958 fue publicado el primer trabajo de ecografía musculoesquelética en la revista "American Journal of Phisical medicine" titulado "meassurements of articular tissue with ultrasound," su autor fue K.T. Dussik.

En 1959, Satomura reportó el uso, por primera vez, del Doppler ultrasónico en la evaluación del flujo de las arterias periféricas.

En 1960, Donald desarrolló el primer escáner automático, que resultó no ser práctico por lo costoso.

En 1960, Howry introdujo el uso del Transductor Sectorial Mecánico ("hand held scanner").

En 1961, el trabajo resultante de David Robinson y George Kossoff dio la posibilidad de crear el primer ultrasonido práctico comercial en Australia.

En 1962, Homes produjo un escáner que oscilaba 5 veces por segundo sobre la piel del paciente, permitiendo una imagen rudimentaria en tiempo real.

En 1963, un grupo de urólogos japoneses reportó exámenes ultrasónicos de la próstata, en el A-MODE.

En 1964 apareció la técnica Doppler para estudiar las carótidas, con gran aplicación en Neurología.

En 1965 La firma austriaca Kretztechnik asociada con el oftalmólogo Dr Werner Buschmann, fabricó un transductor de 10 elementos dispuestos en fase, para examinar el ojo, sus arterias, etc.

En 1966, Kichuchi introdujo la "Ultrasonocardiotomografía sincronizada", usada para obtener estudios en 9 diferentes fases del ciclo cardiaco, usando un transductor rotatorio y una almohada de agua.

En 1967, se inicia el desarrollo de transductores de A-MODE para detectar el corazón embrionario, factible en ese entonces a los 32 días de la fertilización.

En 1968, Sommer reportó el desarrollo de un escáner electrónico con 21 cristales de 1,2 MHz, que producía 30 imágenes por segundo y que fue realmente el primer aparato en reproducir imágenes de tiempo real, con resolución aceptable.

En 1969 se desarrollaron los primeros transductores transvaginales bidimensionales, que rotaban 360 grados y fueron usados por Kratochwil para evaluar la desproporción cefalopélvica. También se inició el uso de las sondas transrectales.

En 1970 Kratochwill comenzó la utilización del ultrasonido transrectal para valorar la próstata.

En 1971 la introducción de la escala de grises marcó el comienzo de la creciente aceptación mundial del ultrasonido en diagnóstico clínico.

1977 Kratochwil combinó el ultrasonido y laparoscopia, introduciendo un transductor de 4.0 MHz a través del laparoscopio, con el objeto de medir los folículos mediante el A-MODE. La técnica se extendió hasta examinar vesícula, hígado y páncreas.

En 1982 Aloka anunció el desarrollo del Doppler en color en imagen bidimensional.

En 1983, Lutz usó la combinación de gastroscopio y ecografía, para detectar CA gástrico y para el examen de hígado y páncreas.

En 1983, Aloka introdujo al mercado el primer Equipo de Doppler en Color que permitió visualizar en tiempo real y en color el flujo sanguíneo.

Aunque ya se obtienen imágenes tridimensionales, el empleo de tal tecnología ha sido desaprovechado pues se ha limitado a usos puramente "estéticos" para estimular a las madres a ver sus hijos en tercera dimensión, pero no para mejorar el diagnóstico.

En 2017, Jan Tesarik introdujo la “Histeroscopia ultrasonográfica virtual” para detectar, en 3 dimensiones, anomalías de la cavidad uterina sin entrar en la matriz, igual de precisa y menos invasiva que la histeroscopia convencional y posteriormente aplicó la misma técnica al estudio de las trompas falopianas (histerosalpingoscopia virtual), la cavidad de folículos ováricos (foliculoscopia virtual) y sacos gestacionales (embrioscopia virtual).

El sonido es una onda mecánica que requiere de un medio para propagárse. El oído humano puede percibirlo con una frecuencia de entre los 20 y 20000 Hz y el ultrasonido es cualquier sonido que supera esta cifra. El ecógrafo funciona mediante un aparato que genera ultrasonidos aprovechando para ello el fenómeno físico llamado efecto piezoeléctro que consiste en que al comprimirse algunos materiales pueden generar una diferencia de potencial eléctrico en su superficie y por lo tanto corriente eléctrica. Este efecto sucede también a la inversa, de manera que al aplicarsele electricidad a los mismos en forma de corriente alterna, generan unas vibraciones que producen ultrasonidos. En el ecógrafo el material piezoeléctrico se encuentra en el cabezal, el cual realiza tanto las funciones de generar ondas ultrasónicas como de recibirlas al rebotar estas en los tejidos que tienen diperente impedancia acústica, para luego volver a convertirlas en corriente eléctrica, que el aparáto transforma en imágenes. La impedancia acústica es la resistencia que ofrece el téjido al paso del sonido, el cual a medida que va avanzando sufre una pérdida de energía debida a los tres fenómenos siguientes:


Todos estos fenómenos posibilitan la generación de la imagen ecográfica al rebotar el sonido en los tejidos y ser recibido por los cristales del receptor los cuales los transforman en corriente eléctrica para ser enviados a la CPU que los procesa.

Los ecógrafos utilizados en la práctica clínica suelen tener frecuencias que oscilan entre los 3 a los 18MHz.

El examen de ultrasonido puede causar en algunos casos y depende del examen, incomodidad pero rara ocasión causará dolor.




Como riesgos, hoy en día no hay ningún riesgo posible que genere un examen por ecografía pero no quiere decir que no vaya a existir a largo plazo, por tanto se establecen una serie de recomendaciones útiles a seguir para evitar esto como por ejemplo controlar el tiempo de adquisición, la frecuencia e intensidad empleada, utilizar la ecografía solo en el caso necesario, emplear una sonda con menor frecuencia, etc. Aunque se ha detectado que los materiales de contraste podrían en pocos de los casos, tener un pequeño porcentaje de riesgo de generar una reacción alérgica sobre el paciente. 

Una ecografía ocular y orbitaria es un examen para observar la zona de los ojos específicamente ver el tamaño y la estructura de este.

Se insensibiliza el ojo con unas gotas anestésicas específicas. El transductor de ultrasonido se coloca contra la superficie frontal del ojo.

En este examen las ondas sonoras viajan desde la superficie del ojo hasta el ordenador donde procesa los datos y genera la imagen.

La ecografía abdominal se realiza para visualizar los órganos internos del abdomen como el hígado, vesícula biliar, páncreas, riñones y bazo. También se puede utilizar para examinar los vasos sanguíneos que van a estos órganos, como la vena cava inferior y la aorta. Se puede utilizar para diagnosticar tumores y muchas otras enfermedades. 

La ecografía vaginal sirve para estudiar el útero, los ovarios, las trompas, el cuello uterino y el área pélvica de la mujer. Se utiliza para evaluar la posición, el tamaño o la presencia de miomas o pólipos en el útero. Se puede estudiar el endometrio, conociendo la fase del ciclo menstrual. Igualmente, puede detectar posibles quistes en los ovarios, embarazos ectópicos o para realizar un recuento folicular. Para el examen, se utiliza una sonda ecográfica que se coloca dentro de la vagina del paciente. Esta prueba puede realizarse anualmente como parte del chequeo ginecológico normal. También se emplea cuando se sospecha de tumores, infertilidad, embarazo ectópico, cuando el paciente presenta sangrado anormal y durante el embarazo.

La ecografía de mama se utiliza para diferenciar nódulos o tumores que pueden ser palpables o aparecer en la mamografía. Su principal objetivo es detectar si el tumor es de tipo sólido o líquido para determinar su benignidad. Las ecografías mamarias son recomendables cuando las mamas son densas o se necesita diferenciar la benignidad del tumor. El sistema BI-RADS establece tres tipos de densidad mamaria 1.- Mama grasa 2 .-Densidad media 3.- Densidad heterogénea 4.- Mama muy densa.

En las mamas grasas son fáciles de detectar tumores en las mamografías, pero en las mamas densas (3-4) (Fibrosas) se necesitan análisis complementarios. La densidad de la mama varía con la edad por lo general, a mayor edad la mama es más grasa.

Mediante este método diagnóstico es posible detectar diferentes problemas de salud que afectan a la glándula tiroides, entre los que se pueden mencionar los quistes, así como los nódulos, definiendo la posibilidad de que uno de estos pueda resultar benigno o maligno mediante la caracterización a través del sistema TI-RADS. Otra aplicación es la detección y el seguimiento de enfermedades que afectan al parénquima tiroideo de manera difusa, como el bocio multinodular y la tiroiditis. Para la biopsia con aguja delgada o fina (BAAF), se emplea el Ultrasonido / ecografía como guía para poder realizar el procedimiento con precisión.

La ecografía médica para el diagnóstico del cáncer de próstata consiste en la introducción de una sonda por el recto que emite ondas de ultrasonido que producen ecos al chocar con la próstata. Estos ecos son captados de nuevo por la sonda y procesados por una computadora para reproducir la imagen de la próstata en una pantalla de vídeo. El paciente puede notar algo de presión con esta prueba cuando la sonda se introduce en el recto. Este procedimiento dura solo algunos minutos y se realiza ambulatoriamente. La ecografía transrectal es el método más usado para practicar una biopsia. Los tumores de próstata y el tejido prostático normal a menudo reflejan ondas de sonido diferentes, por eso se utiliza la ecografía transrectal para guiar la aguja de biopsia hacia el área exacta de la próstata dónde se localiza el tumor. La ecografía transrectal no se recomienda de rutina como prueba de detección precoz del cáncer de próstata. La ecografía transrectal es también imprescindible en el estadiaje del cáncer colorrectal.

La ecografía del escroto es un procedimiento imagenológico para examinar el escroto, este es un conjunto de envolturas que cubren y alojan a los testículos y vías excretoras.

El procedimiento consta de lo siguiente, el paciente se acuesta boca arriba sobre la camilla con las piernas separadas, el médico procederá a colocar una tela por debajo del escroto y el doctor colocará unas tiras de cinta adhesiva para levantar el escroto y poder así realizar el test.El saco escrotal se levanta ligeramente con los testículos ubicados a cada lado, se aplicará un gel transparente sobre el saco escrotal para así ayudar a la transmisión de las ondas sonoras utilizando el transductor conectado a la computadora a la que le llegan los datos y genera la imagen.

La ecografía del pene es el método más utilizado y menos invasivo para realizar la valoración inicial del pene y de sus funciones para ver si presenta ciertas enfermedades o no. Para el estudio de su funcionalidad será utilizada la ecografía Doppler que puede ser dicho examen de dos tipos en reposo o dinámico, este último explicado es para ver porqué ocurren los problemas de la disfunción eréctil. Su realización del test es muy parecida a la del escroto pero en este caso el transductor se coloca encima del pene y se toman las ondas.

Se utiliza para evaluar el sistema circulatorio y ayudar a identificar coágulos sanguíneos, bloqueos en venas y arterias u otras patologías. Este examen generalmente incluye un estudio de ultrasonido Doppler, para evaluar el flujo sanguíneo a través de venas y arterias.
Se utiliza para crear imágenes del corazón que son más detalladas que las imágenes obtenidas por una radiografía simple. Se pueden obtener imágenes en tiempo real en dos o tres dimensiones. Permite evaluar el funcionamiento de las válvulas cardíacas, para el diagnóstico de estenosis o insuficiencia, y para evaluar la contracción del músculo cardíaco, para el diagnóstico de hipertrofia o dilatación de los ventrículos y aurículas.

Se puede hacer una ecocardiografía de esfuerzo para comprobar el funcionamiento del miocardio para bombear la sangre al cuerpo. Se utiliza para detectar una disminución en el flujo sanguíneo al corazón, producido por un estrechamiento de las arterias coronarias.

Se utiliza para producir imágenes del feto mientras está en el útero. para evaluar el crecimiento y desarrollo del bebé y monitorear el embarazo. Por medio de esta prueba se pueden identificar muchas condiciones que pueden ser peligrosas tanto para la madre como para el hijo. Se utiliza para:



Se utiliza para obtener imágenes de los riñones, los uréteres y la vejiga. Se realiza este examen cuando los médicos sospechan de alguna problema renal y este les permite identificar:


Esta técnica diagnóstica permite detectar tumores cutáneos, procesos inflamatorios, alteraciones ungueales, enfermedades del pelo y también es aplicable a la dermoestética. Utilizada por primera vez en Chile por la Dra. Wartsman, en España la técnica ha sido introducida por el Dr. Fernando Alfageme Roldán.

Es la propia máquina de la ecografía donde tiene lugar el procesamiento de la información obtenida mediante los ultrasonidos, consta de tres partes:


El transductor es la pieza más importante del aparato, la cual es responsable de transformar los impulsos eléctricos en ondas ultrasónicas así como de dirigirlos a un punto concreto, utilizando para ello la propiedad física llamada efecto piezoeléctrico. También realiza el mismo procedimiento a la inversa, de manera que transforma los impulsos sonicos que recibe en corriente eléctrica que transmite al sistéma de procesamiento y con las cuales crea las imágenes que se visualizan en el monitor.
Para cumplir con su función, es necesario que esté compuesto por una carcasa hermética al paso del sonido, cristales con propiedades piezoeléctricas y una membrána plástica que se mantiene en contacto con la piel y sobre la zona que se está explorando.

Existen diferentes tipos de transductores según su frecuencia y forma. Aunque el tipo más usado de ellos puede cambiar su cantidad de frecuencia, existen algunos que la presentan baja, media o alta. Suele usarse alta frecuencia para tejidos superficiales y baja para tejidos profundos. Habitualmente el ecógrafo opera en frecuencias de entre 7 y 13 MHz. 

En cuanto a su forma, esta pueden ser lineales, convexas, microconvexas, endocavitarias, sectoriales y 3D.

El equipo médico se puede utilizar en diferentes configuraciones, dependiendo del la finalidad del estudio y del órgano diana. 




La ecografía doppler o simplemente eco-Doppler, es una variedad de la ecografía tradicional, basada por tanto en el empleo de ultrasonidos, en la que aprovechando el efecto Doppler, es posible visualizar las ondas de velocidad del flujo que atraviesa ciertas estructuras del cuerpo, por lo general vasos sanguíneos, y que son inaccesibles a la visión directa. La técnica permite determinar si el flujo se dirige hacia la sonda o si se aleja de ella, así como la velocidad de dicho flujo. Mediante el cálculo de la variación en la frecuencia del volumen de una muestra en particular, por ejemplo, el de un flujo de sangre en una válvula del corazón, se puede determinar y visualizar su velocidad y dirección. La impresión de una ecografía tradicional combinada con una ecografía Doppler se conoce como ecografía dúplex.

La información Doppler se representa gráficamente con un Doppler espectral, o bien como una imagen usando Doppler direccional o un power Doppler (Doppler no-direccional). La frecuencia Doppler cae en el rango audible y puede escucharse utilizando altavoces estéreo, produciendo un sonido pulsátil distintivo.

En los últimos tiempos se ha podido ver una revolución en el campo de la medicina materno-fetal. Esa revolución, además, no solo ha afectado a la medicina en sí misma, sino que ha aportado a la sociedad la posibilidad de establecer una unión emocional con los neonatos mucho más profunda de lo que hasta ahora se creía posible, gracias a una calidad de imagen que permite ver el aspecto del futuro bebé en fotografía (3D) o en imagen en movimiento (4D).

Para lograrlo, mediante el ecógrafo, se emiten los ultrasonidos en cuatro ángulos y direcciones, pasando el emisor suavemente por la barriga del paciente, a la cual se le ha aplicado previamente un gel para mejorar la eficiencia del proceso. Los ultrasonidos rebotan y son captados por el ordenador, que procesa automáticamente la información para reproducir en la pantalla la imagen a tiempo real del bebé.

Desde que se inventó la ecografía se han ido introduciendo algunas de las mejoras tecnológicas en los nuevos aparatos las cuales permiten obtener imágenes de mejor calidad, así como ser más eficientes en su uso diagnóstico. Son las siguientes:


Inicialmente la ecografía ha sido una técnica diagnóstica desarrollada y utilizada por radiólogos, sin embargo, hoy día es utilizada cada vez más en otras especialidades médicas como herramienta diagnóstica: cardiología, ginecología, obstetricia, medicina de urgencias, cuidados intensivos, medicina general, familia, urología o pediatría.



</doc>
<doc id="15444" url="https://es.wikipedia.org/wiki?curid=15444" title="Tomografía axial computarizada">
Tomografía axial computarizada

La tomografía (del griego "τομή", tomé, "corte, sección", y de "γραφή", "grafé", "imagen, gráfico") es la obtención de imágenes de cortes o secciones de algún objeto. La posibilidad de obtener imágenes de cortes tomográficos reconstruidas en planos no transversales ha hecho que en la actualidad se prefiera denominar a esta técnica tomografía computarizada o TC en lugar de TAC. En lugar de obtener una imagen de proyección, como la radiografía convencional, la TC obtiene múltiples imágenes al efectuar la fuente de rayos X y los detectores de radiación movimientos de rotación alrededor del cuerpo. La representación final de la imagen tomográfica se obtiene mediante la captura de las señales por los detectores y su posterior proceso mediante algoritmos de reconstrucción.

En los fundamentos de esta técnica trabajaron de forma independiente el ingeniero electrónico y físico sudafricano nacionalizado norteamericano Allan McLeod Cormack y el ingeniero electrónico inglés Sir Godfrey Newbold Hounsfield, que dirigía la sección médica del Laboratorio Central de Investigación de la compañía EMI. Ambos obtuvieron de forma compartida el , en 1979.

En 1967, Cormack publica sus trabajos sobre la TC siendo el punto de partida de los trabajos de Hounsfield, que diseña su primera unidad. En 1972 comenzaron los ensayos clínicos cuyos resultados sorprendieron a la comunidad médica, si bien la primera imagen craneal se obtuvo un año antes.

Los primeros cinco aparatos se instalaron en Reino Unido y los Estados Unidos; la primera TC de un cuerpo entero se consiguió en 1974.

En el discurso de presentación del comité del Premio Nobel se destacó que previo al escáner, “las radiografías de la cabeza mostraban solo los huesos del cráneo, pero el cerebro permanecía como un área gris, cubierto por la neblina. Súbitamente la neblina se ha disipado”.

En recuerdo y como homenaje a Hounsfield, las unidades que definen las distintas atenuaciones de los tejidos estudiadas en TC se denominan "unidades Hounsfield" o "número TC" ("CT number)," donde el agua corresponde a 0HU, tejidos blandos +30 a+60HU, grasa -40 a -120HU, entre otros que permiten hacer caracterización de tejidos.

El aparato de TC emite un haz colimado de rayos X que incide sobre el objeto que se estudia. La radiación que no ha sido absorbida por el objeto es recogida por los detectores. Luego el emisor del haz, que tenía una orientación determinada (por ejemplo, estrictamente vertical a 90º) cambia su orientación (por ejemplo, haz oblicuo a 95º). Este espectro también es recogido por los detectores. El ordenador 'suma' las imágenes, promediándolas. Nuevamente, el emisor cambia su orientación (según el ejemplo, unos 100º de inclinación). Los detectores recogen este nuevo espectro, lo 'suman' a los anteriores y 'promedian' los datos. Esto se repite hasta que el tubo de rayos y los detectores han dado una vuelta completa, momento en el que se dispone de una imagen tomográfica definitiva y fiable.

Para comprender qué hace el ordenador con los datos que recibe, lo mejor es examinar el diagrama que se aprecia líneas abajo.

Una vez que ha sido reconstruido el primer corte, la mesa donde el objeto reposa avanza (o retrocede) una unidad de medida (hasta menos de un milímetro) y el ciclo vuelve a empezar. Así se obtiene un segundo corte (es decir, una segunda imagen tomográfica) que corresponde a un plano situado a una unidad de medida del corte anterior.

A partir de todas esas imágenes transversales (axiales), un computador reconstruye una imagen bidimensional que permite ver secciones de la pierna (o el objeto de estudio) desde cualquier ángulo. Los equipos modernos permiten incluso hacer reconstrucciones tridimensionales. Estas reconstrucciones son muy útiles en determinadas circunstancias, pero no se emplean en todos los estudios, como podría parecer. Esto es así debido a que el manejo de imágenes tridimensionales no deja de tener sus inconvenientes.

Un ejemplo de imagen tridimensional es la imagen 'real'. Como casi todos los cuerpos son opacos, la interposición de casi cualquier cuerpo entre el observador y el objeto que se desea examinar hace que la visión de este se vea obstaculizada. La representación de las imágenes tridimensionales sería inútil si no fuera posible lograr que cualquier tipo de densidad que se elija no se vea representada, con lo que determinados tejidos se comportan como transparentes. Aun así, para ver completamente un órgano determinado es necesario mirarlo desde diversos ángulos o hacer girar la imagen. Pero incluso entonces veríamos su superficie, no su interior. Para ver su interior debemos hacerlo a través de una imagen de corte asociada al volumen y aun así parte del interior no siempre sería visible. Por esa razón, en general, es más útil estudiar una a una todas las imágenes consecutivas de una secuencia de cortes que recurrir a reconstrucciones en bloque de volúmenes, aunque a primera vista sean más espectaculares.

La TC se basa en el trabajo desarrollado en 1917 por Johann Radon, quien demostró que es posible reconstruir una imagen a partir de múltiples proyecciones de estas a diferentes ángulos, esta operación matemática usada en la TC es conocida como transformada de Radon.

El tubo de rayos X que gira alrededor del objeto a escanear captura diferentes tomas en su rotación, y del número de estas depende en gran parte la calidad la resolución del escaneo (plano XY), el otro factor de hardware que afecta este ítem es el número de detectores (pixeles). Al tiempo que el tubo y el detector giran respecto al paciente, se mueven longitudinalmente para cubrir la superficie a estudiar y las imágenes pueden ser más "gruesas" (>5mm) o "delgadas"(<5mm) (más resolución) según el número de líneas de detectores, que en los equipos más modernos pueden ser superiores a 128.

Las múltiples proyecciones obtenidas son almacenadas en una única matriz llamada sinograma, a la cual se le aplica un algoritmo de reconstrucción llamado retroproyección filtrada que igualmente está basado en la transformada de Radón.

Para aplicarlo a la medicina hubo que esperar al desarrollo de la computación y del equipo adecuado que mezclase la capacidad de obtener múltiples imágenes axiales separadas por pequeñas distancias, almacenar electrónicamente los resultados y tratarlos. Todo esto lo hizo posible el británico G. H. Hounsfield en la década de 1970.

La TC permite estudiar casi todos los órganos internos del cuerpo, desde la cabeza hasta las extremidades, incluyendo los huesos, tejidos blandos, corazón y vasos sanguíneos. La TC es una exploración o prueba radiológica muy útil para el estadiaje o estudio de extensión de los cánceres en especial en la zona craneana, como el cáncer de mama, cáncer de pulmón y cáncer de próstata. Asímismo, la TC es de gran utilidad en los servicios de emergencia, por su gran velocidad de barrido de cuerpo entero, que permite detectar eficazmente fracturas, hemorragias y lesiones de órganos en pocos segundos o minutos. En los últimos años, se ha mejorado su capacidad diagnóstica para el sistema cardiocirculatorio, pudiendo evaluar eficazmente enfermedades agudas y crónicas del corazón y de los vasos sanguíneos.

Otro uso es la simulación virtual y planificación de un tratamiento del cáncer con radioterapia, para lo cual es imprescindible el uso de imágenes en tres dimensiones que se obtienen de la TC.

Las primeras TC se instalaron en España a finales de los años 1970 del siglo XX. Los primeros TC servían solamente para estudiar el cráneo, y fue con posteriores generaciones de equipos cuando pudo estudiarse el cuerpo completo. Al principio era una exploración cara y con pocas indicaciones de uso. Actualmente es una exploración de rutina de cualquier hospital, habiéndose abaratado mucho los costos. Con el desarrollo de la TC helicoidal, los cortes son más finos, incluso submilimétricos y la velocidad de barrido mayor. La nuevas TC multicorte incorporan varios anillos de detectores (típicamente entre 16 y 320), lo que permite la adquisición de múltiples cortes simultáneos en cada rotación del tubo de rayos X, lo aumenta aún más la rapidez, logrando imágenes volumétricas en tiempo real.
Entre las ventajas de la TC se encuentra que es una prueba rápida de realizar y que se encuentra ampliamente disponible en la mayoría de los hospitales de mediana y alta complejidad. Por medio de la visualización a través de la exploración por TC un médico radiólogo experto puede diagnosticar numerosas causas de dolor abdominal con una alta precisión, lo cual permite aplicar un tratamiento rápido y con frecuencia elimina la necesidad de procedimientos de diagnóstico adicionales y más invasivos.

Cuando el dolor se produce a causa de una infección e inflamación, la velocidad, facilidad y precisión de un examen por TC puede reducir el riesgo de complicaciones graves causadas por la perforación del apéndice o la rotura del divertículo y la consecuente propagación de la infección.
Las imágenes por TC son exactas, no son invasivas y no provocan dolor.

Una ventaja importante de la TC es su capacidad de obtener imágenes de huesos, tejidos blandos y vasos sanguíneos al mismo tiempo. A diferencia de los rayos X convencionales, la exploración por TC brinda imágenes detalladas de numerosos tipos de tejido así como también de los pulmones, huesos y vasos sanguíneos.

Los exámenes por TC son rápidos y sencillos; en casos de emergencia, pueden revelar lesiones y hemorragias internas lo suficientemente rápido como para ayudar a salvar vidas.

La TC es menos sensible al movimiento de pacientes que la resonancia magnética, por lo que en los equipos más modernos es posible hacer tomografía cardíaca de alta calidad aún con el movimiento del corazón.

La TC se puede realizar si el paciente tiene implante de dispositivo médico de cualquier tipo, a diferencia de la RM.

En equipos de TC muy avanzados, es posible obtener imágenes en tiempo real, haciendo de este una buena herramienta para guiar procedimientos mínimamente invasivos, tales como biopsias por aspiración y aspiraciones por aguja de numerosas áreas del cuerpo, particularmente los pulmones, el abdomen, la pelvis y los huesos.

Un diagnóstico determinado por medio de una exploración por TC puede eliminar la necesidad de una cirugía exploratoria y una biopsia quirúrgica.

Entre entre sus inconvenientes se cita que algunas veces es necesaria la inyección de medio de de contraste intravenoso, que implica una punción y riesgo de reacciones adversas en pacientes susceptibles. Por otra parte, al utilizar rayos X, se reciben dosis de radiación ionizante, mayores que las obtenidas en exámenes más simples como radiografías. La dosis efectiva de radiación a partir de este procedimiento es diferente según la máquina, los parámetros introducidos por el operador, el tamaño o contextura del paciente y la parte del cuerpo escaneada y varía en algunas máquinas probadas de aproximadamente 1 a 10 mSv. A veces, más de una exploración se realiza a la vez, una con y otra sin agente de contraste, lo que duplica la dosis recibida por el paciente. La dosis efectiva recibida por un adulto en una exploración del abdomen y la pelvis es de aproximadamente la misma proporción que una persona promedio recibe de radiación natural o de fondo en tres años.

Las mujeres siempre deben informar a su médico y al tecnólogo de rayos X o TC si existe la posibilidad de que estén embarazadas. En estos casos, debe realizarse un análisis de beneficio versus riesgo antes de someter al feto a los rayos X.

Una tomografía computarizada, sin o con inyección de medio de contraste no contraindica la lactancia materna, pues no hay paso de radiación ni de volúmenes significativos de medio de contraste a la leche materna.

Antes de realizar un estudio con contraste, el paciente debe de llenar un cuestionario en donde se le realizan preguntas acerca de su historial de salud como: alergias, síntomas y razón por la que se le va a practicar el examen. El riesgo de una reacción alérgica grave al material de contraste yodado, muy rara vez ocurre, y los departamentos de radiología deben de poseer el entrenamiento y los medios necesarios en caso de que ocurra un evento como este . 

Debido a que los niños son más sensibles a la radiación, se les debe someter a un estudio por TC únicamente si es fundamental para realizar un diagnóstico, y no se les debe realizar estudios por TC en forma repetida a menos que sea absolutamente necesario.




</doc>
<doc id="15449" url="https://es.wikipedia.org/wiki?curid=15449" title="Campylobacter fetus">
Campylobacter fetus

Campylobacter fetus es una especie de "Campylobacter", gram negativa, móviles, bacilos oxidasa positiva, con una característica forma de "S", similar a los miembros del género "Vibrio". "C. fetus" está recubierta de una proteína de superficie que funciona similar a una cápsula e interrumpe la adherencia de la molécula del complemento C3b. 

Por lo general es un patógeno oportunista que, diferencia de otras bacterias del mismo género, rara vez causa diarrea, sino que provoca infecciones extraintestinales en pacientes inmunosuprimidos o con enfermedades de base acompañantes como la cirrosis hepática, la diabetes mellitus, cáncer, leucemia, cardiopatía, etc. El cuadro infeccioso se manifiesta por una poco frecuente bacteriemia o septicemia en la que aparece casi siempre la fiebre. Como resultado de la bacteriemia puede haber afectación en distintos órganos, destacando las localizaciones cardiovaculares con endocarditis y pericarditis, tromboflebitis, meningitis y meningoencefalitis, artritis y abortos.

Pueden surgir otros procesos supurados como peritonitis, absceso de pulmón, empiema, celulitis, infecciones del tracto urinario y colecistitis. También el "Campylobacter fetus", puede ocasionar gastroenteritis con sintomatología semejante a la producida por "Campylobacter jejuni". El reservorio natural del "C. fetus" son el ganado y las ovejas.




</doc>
<doc id="15450" url="https://es.wikipedia.org/wiki?curid=15450" title="Phlomis herba-venti">
Phlomis herba-venti

Phlomis herba-venti, popularmente aguavientos o hierba de las moscas, es una especie perteneciente a la familia Lamiaceae. Nativa de la región Mediterránea y el Asia central. 

Es una planta herbácea perenne de hasta 70 cm de altura, con hojas lanceoladas u ovadas, coriáceas, cordadas con pecíolo de 4-8 cm y limbo de 7-20 x 5-10 cm, verdes. Las flores son de color púrpura.

subsp. herba-venti. Del sur de Europa y norte de Marruecos.
subsp. kopetdaghensis (Knorr.) Rech.f., in Fl. Iran. 150: 309 (1982). de Turkmenistán a Irán.
subsp. lenkoranica (Knorring) Rech.f., in Fl. Iran. 150: 309 (1982). De Transcaucasia al sur de Turkmenistán.
subsp. pungens (Willd.) Maire ex DeFilipps, Bot. J. Linn. Soc. 64: 233 (1971). Sudeste de Europa a Irán y noroeste de África.



</doc>
<doc id="15451" url="https://es.wikipedia.org/wiki?curid=15451" title="Mitrídates, rey de Ponto">
Mitrídates, rey de Ponto

Mitrídates, rey de Ponto (título original en italiano, "Mitridate, Re di Ponto") es una ópera seria en tres actos con música de Wolfgang Amadeus Mozart y libreto en italiano de Vittorio Amedeo Cigna-Santi. Lleva por número KV 87. En el último catálogo Köchel, K 74a. Se compuso por encargo del conde Firmian, gobernador de Milán y mecenas. Se estrenó en el Teatro Regio Ducal de Milán el 26 de diciembre de 1770. 

La historia que se cuenta en la ópera transcurre en Ninfea, puerto de Crimea en el Reino del Ponto en el año 63 a. C. El protagonista es el rey Mitrídates VI Eupator (132-63 a. C.) Enzarzado en sus luchas contra los romanos, deja a su prometida Aspasia al cuidado de sus hijos: Farnaces y Sifares. Después de sufrir una severa derrota, Mitrídates es dado por muerto. 

Acto I

"Escena 1"

Arbate, el gobernador de Ninfea, da la bienvenida a Sifares que está enojado con su hermano, Farnaces, debido a los fuertes lazos que lo unen a los romanos, sus enemigos. Arbate jura lealtad a Sifares. Aspasia ruega a Sifares, para que la ayude a resistir los avances de Farnaces. Sifares acepta sus súplicas, y al tiempo revela su amor por ella. Aspasia ama secretamente a Sifares.

"Escena 2"

Farnaces, el primogénito, ofrece su amor a Aspasia, quien lo rechaza con el apoyo de Sifares, que la protege frente a su poderoso hermano. Llegan noticias de que Mitrídates está vivo y se acerca a la ciudad. Arbate insta a los hermanos a sobreponerse a sus diferencias y saludar a su padre. Farnaces conspira con Marzio, tribuno romano, contra Mitrídates.

"Escena 3"

Mitrídates llega a Ninfea con la princesa Ismene, hija del rey de los partos, para ofrecerla como esposa a su hijo Farnaces. Mitrídates quiere que Farnaces se case con Ismene, su prometida. Ismene está enamorada de Farnaces. Arbate le dice a Mitrídates que Farnaces persigue a Aspasia, pero no menciona a Sifares. Celoso, Mitrídates jura vengarse de Farnaces. 

Acto II

"Escena 1"

Farnaces desprecia y amenaza a Ismene, y ésta se lo dice a Mitrídates, quien sugiere que se case con Sifares. Mitrídates pide a Aspasia que se casen inmediatamente, pero ella vacila, lo que demuestra su infidelidad. Aspasia le confiesa su amor a Sifares, pero acuerdan separarse para salvaguardar el honor. Sifares planea marcharse y Aspasia queda preocupada, inmersa en su conflicto entre el amor y el deber. 

"Escena 2"

Mitrídates es consciente del complot de Farnaces y los romanos contra él, y planea vengarse, a pesar de la oferta de paz que Marzio le hace llegar. Detiene a Farnaces, acusado de traición. Ismene salva al príncipe Farnaces lo confiesa todo a su padre y es ingresado en prisión. Aspasia y Sifares declaran su amor y están dispuestos a morir, por temor a Mitrídates. 

Acto III

"Escena 1"

Ismene, todavía enamorada de Farnaces, trata de convencer a Mitrídates para que perdone a Aspasia y Sifares. Los romanos, guiados por Marzio, atacan y Mitrídates se prepara para la batalla. Aspasia piensa en suicidarse ingiriendo veneno, y sólo la intervención de Sifares consigue salvarla. Sifares también quiere morir y une a su padre en la batalla.

"Escena 2"

Marzio libera a Farnaces y le promete el trono de su padre si le ayuda. Pero el príncipe cambia de idea, se arrepiente de su traición y se une al ejército de su padre.

"Escena 3"

Mitrídates es herido en combate y él mismo se arroja sobre su espada para suicidarse, ante la derrota. Antes de morir, perdona a sus hijos y da su bendición a Sifares y Aspasia, mientras Farnaces se declara dispuesto a desposar a Ismene. En el quinteto final Sifares, Aspasia, Farnaces, Ismene y Arbate declaran su intención de vengarse de los romanos y combatir a aquellos que pretenden acabar con la libertad del mundo entero.

Originalmente interpretada con 2 flautas, 2 oboes, 4 cornos, 2 fagotes, 2 trompetas, timbales, cuerdas y bajo continuo.

El texto originario era el "Mithridate" de Racine (1673); fue traducido por el abate Giuseppe Parini. Vittorio Amadeo Cigna-Santi versificó esta traducción para la ópera de Mozart. Este poeta residía en Turín y envió a Milán el libreto por partes. La historia de Mitrídates ya había sido puesta en música con anterioridad, por Quirino Gasparini.


Consta de una obertura, veintiún arias, una cavata, una cavatina, un dúo y un coro final. Mozart intercaló siete recitativos acompañados. De las piezas vocales de esta ópera, destacan: 

Se compuso para inaugurar la temporada musical, por parte del conde Firmian, gobernador y mecenas milanés. Las circunstancias de su composición y estreno pueden seguirse a través de las cartas de Leopold Mozart. Se estrenó con gran éxito en el Teatro Regio Ducal de Milán el 26 de diciembre de 1770.

Esta primera representación suscitó el entusiasmo del público, en Parini y en todos los cantantes. El castrado Pietro Benedetti afirmó que, si el público no quedaba encantado con el dueto final del segundo acto, “se haría castrar por segunda vez". Después de esa primera representación, hubo otras veinte en el mismo escenario, dirigiendo Mozart las cuatro primeras. No se volvió a representar hasta el siglo XX. Se interpretó en Salzburgo en versión de concierto en 1977.

Esta ópera se representa poco; en las estadísticas de Operabase aparece la n.º 163 de las óperas representadas en 2005-2010, siendo la 24.ª en Austria y la decimocuarta de Mozart, con 19 representaciones en el período.

Mozart escribió "Mitridate" mientras se encontraba de viaje por Italia en 1770. Tenía catorce años. Fue su primera experiencia con la ópera seria. Recibió cien florines y la manutención durante los cinco meses que tardó en componerla. Como el resto de sus primeras óperas, sigue muy de cerca el modelo italiano.


Igualmente, está disponible una grabación en DVD por Charles T. Downey (2006, Ionarts).








</doc>
<doc id="15453" url="https://es.wikipedia.org/wiki?curid=15453" title="Caja negra (transporte)">
Caja negra (transporte)

Se denomina caja negra o registrador de vuelo al dispositivo que, principalmente en aeronaves, trenes, barcos y naves espaciales, registra la actividad de los instrumentos y las conversaciones de los tripulantes. Su función es almacenar datos que, en caso de un accidente, permitan analizar lo ocurrido en los momentos previos y establecer sus causas. Los aviones comerciales de gran tamaño llevan dos cajas negras, técnicamente conocidas por sus siglas en inglés como CVR (grabadora de voces de cabina) y FDR (grabadora de datos de vuelo). 

Los buques tienen sistemas similares (derivados de estos mismos) que se denominan Registradores de Datos de la Travesía (VDR)

Los primeros registradores de vuelo se empezaron a usar a finales de los años 1950 y se les llamó "cajas negras", denominación que perduró incluso después de que se pintasen de color naranja para facilitar su localización tras un accidente. El origen de la denominación de "caja negra" no está claro: algunos de los prototipos de la RAF estaban pintados de negro, otros prototipos eran cámaras oscuras con placas fotográficas, y desde un punto de vista de sistemas, se comportan como cajas negras (se pone foco en sus entradas y salidas).

Se denomina caja negra o registrador de vuelo al dispositivo que, principalmente en las aeronaves y coches motores o locomotoras de trenes, registra la actividad de los instrumentos y las conversaciones en la cabina. Su función es almacenar datos que, en caso de un accidente, permitan analizar lo ocurrido en los momentos previos. Según las normas de aviación internacionales, estos aparatos hoy son obligatorios en todos los vuelos comerciales ya que graban los datos del viaje y son clave en las investigaciones sobre accidentes de avión. Gracias a ellos, nueve de cada diez accidentes, se pueden explicar. Por eso se ha puesto tanto empeño a la caja de vuelo MH370 de Malaysia Airlines perdido en marzo del 2014 en vuelo de Kuala Lampur, capital de Malasia, a Pekín, capital de China.

Como ocurre con tantos otros inventos sofisticados, no tiene un único inventor, pero el primer prototipo de caja negra, data del año de 1939 y fue diseñado por el ingeniero francés Francois Hussenot. Se trataba de una rudimentaria caja hecha con film fotográfico calibrada con espejos. Los sensores a bordo lanzaban flashes en el filme fotográfico y así se registraba el historial del vuelo. Consciente de lo importante de su invento, se dice que Hussenot escondió la caja del ejército nazi invasor de su patria, enterrándola cerca de una playa del Océano Atlántico en junio de 1940. Y como también ocurre con tantos avances tecnológicos, la guerra perfeccionó la tecnología, que se extendió a los vuelos comerciales en todo el mundo. Después de la Segunda Guerra Mundial, algunos dispositivos usaban fotografía y otros imprimían los datos en bobinas de aluminio.

Los primeros registradores de vuelo se empezaron a usar a finales de los años 1950 y se les llamó "cajas negras", denominación que perduró incluso después de que se pintasen de color naranja, esto para facilitar su localización tras un accidente. La denominación de "cajas negras" proviene, al igual que en otras situaciones (como día negro) de que en el momento que las cajas negras se hacen necesarias, es porque ha sucedido un accidente aéreo.

La caja negra propiamente dicha es obra del australiano David Warren. En 1953 le pidieron a este químico e ingeniero de aviación que ayudara a descubrir la causa de una serie de accidentes aéreos. Los expertos intentaban entender por qué varios aviones Comet, se habían estrellado sin ninguna explicación, lo que ponía en duda el futuro de los vuelos comerciales. "Me quedé pensando para mis adentros ... Si pudiéramos recuperar esos últimos segundos" dijo en una entrevista en 1985 citada por The New York Times, "se ahorrarían muchas discusiones e incertidumbre". Un año más tarde, Warren propuso instalar un dispositivo de grabación en la cabina del piloto y para 1958 había producido el prototipo de la Unidad de Memoria del Vuelo. Esa primera versión era ligeramente más grande que la mano de un adulto, pero capaz de grabar unas cuatro horas de conversación de cabina y de lecturas de mandos. La versión de Warren grababa el sonido en una bobina de acero magnetizado. Para sorpresa de Warren, el dispositivo fue rechazado por las autoridades de aviación, que le encontraron "poca utilidad directa e inmediata para las aeronaves civiles", mientras que los pilotos dijeron que era como un "Big Brother" (Gran Hermano) que espiaría su trabajo.

Cuando Warren llevó el invento al Reino Unido, fue recibido con entusiasmo y luego de un reportaje de la BBC sobre el aparato, los fabricantes comenzaron a interesarse con el proyecto. Mientras tanto en Estados Unidos, ya había investigaciones sobre el aparato y en 1960, ya se daban los primeros pasos para hacer que los dispositivos fueran obligatorios. A mediados de la década de 1960, los registradores de vuelo -de datos y de voz- eran obligatorios para los aviones comerciales. Actualmente, las computadoras de vuelo, han reemplazado a la cinta magnética, los dispositivos pueden grabar más datos y son mucho más propensos a sobrevivir a un impacto. Debe tener una etiqueta con las letras de al menos 2.5 cm de alto que digan: "REGISTRADOR DE VUELO NO ABRIR" en inglés y francés.

Los registradores actuales emplean microcircuitos de memoria flash, capaces de almacenar datos durante varios años sin alimentación de energía. En la actualidad graban digitalmente las dos últimas horas o los últimos treinta minutos (según el modelo) de todas las conversaciones realizadas en la cabina, tanto las realizadas por los pilotos como las de ambiente, que se captan por medio de un micrófono normalmente instalado en el panel superior (overhead) y que registra todos los sonidos que se producen en cabina (conversaciones, avisos sonoros del avión, etc). Esos registradores contienen también tarjetas de circuito que procesan y comprimen los datos, aunque sólo los microcircuitos de memoria están encerrados en el bloque antichoque de la caja. Ese bloque se cubre con un blindaje grueso de acero para que resista los aplastamientos por impacto. Bajo el acero hay una capa de aislante térmico diseñado para proteger los microcircuitos de memoria de los incendios que suelen ocurrir tras un accidente del reactor.

Todas las aeronaves comerciales de gran tamaño llevan dos cajas: la grabadora de voces de cabina o CVR ("Cabin Voice Recorder - CDR") que recoge las conversaciones de la tripulación de vuelo y los sonidos procedentes de la cabina, y el registrador de datos de vuelo ("Flight Data Recorder - FDR"), que anota la altitud del aparato, su velocidad con respecto al aire, su rumbo y otras lecturas instrumentales. Dada la importancia de esa información, los registradores se diseñan para resistir aceleraciones considerables, además de ubicarse en sitios que suelen ser menos castigados por un impacto como la cola del avión. 

Recientemente se amplió la lista de lecturas instrumentales a almacenar y también se ha propuesto que cada grabadora de voces de cabina esté equipada con una fuente de alimentación de reserva para que pueda seguir funcionando aunque se averíen los circuitos eléctricos de la aeronave.

Las cajas negras más modernas disponen de entradas para almacenar vídeo, dando la posibilidad de grabar las acciones sucedidas en la cabina de vuelo en los momentos previos al accidente.

Las pruebas de certificación que se realizan para comprobar que estén preparadas, son las siguientes:





Una de las mayores deficiencias de las cajas negras es la posibilidad de pérdida -especialmente en accidentes sucedidos en el mar- o destrucción total. Para subsanar esas deficiencias se han propuesto innovaciones como la elaboración de cajas negras flotantes, auto-expulsables en caso de accidente, dotarlas de un localizador GPS, o con capacidad para transmitir los datos vía satélite a medida que se registran. La tecnología de cajas negras auto-expulsables fue utilizada inicialmente en la aviación militar y desde 2015 en algunos aviones comerciales.

La posibilidad de utilizar sistemas de transmisión de datos vía satélite en tiempo real ha sido observada debido al peligro de que personas o empresas ajenas a las autoridades a cargo de la investigación de los accidentes, pudieran acceder a la información para manipularla o utilizarla abusivamente.

Actualmente es obligatoria la instalación de un Registrador de Datos de la Travesía (RDT o VDR por sus siglas en inglés) en todos los buques de nueva construcción, con la excepción de buques que no sean de pasaje cuyo arqueo bruto sea inferior a 3.000 toneladas. Existen versiones simplificadas (S-VDR) para buques más pequeños. 

La filosofía es similar al caso de las aeronaves: se graban los sonidos captados en el puente de mando y los "alerones" (especie de "balcón" que posee el puente, a babor y estribor), audio del sistema VHF y por otro lado se graban datos de los mandos tales como ángulo de timón, velocidad de máquinas, estado de puertas estancas, junto a datos del GPS, de la ecosonda, del radar, u otros. Todo queda registrado en una cápsula hermética que sobreescribe los datos periódicamente dejando siempre legibles las últimos 12 horas (48 horas según MSC.333(90)) anteriores al corte de energía, los que deben conservarse por un mínimo de 30 días posteriores. Las cápsulas pueden quedar fijas al casco o liberarse mediante zafas hidrostáticas como las radiobalizas de emergencia y las balsas salvavidas.



</doc>
<doc id="15454" url="https://es.wikipedia.org/wiki?curid=15454" title="Criptoanálisis">
Criptoanálisis

El criptoanálisis (del griego "kryptós", "escondido" y "analýein", "desatar") es la parte de la criptología que se dedica al estudio de sistemas criptográficos con el fin de encontrar debilidades en los sistemas y romper su seguridad sin el conocimiento de información secreta. En el lenguaje no técnico, se conoce esta práctica como romper o forzar el código, aunque esta expresión tiene un significado específico dentro del argot técnico. A las personas que se dedican al criptoanálisis se llaman criptoanalistas.

Los métodos y técnicas del criptoanálisis han cambiado drásticamente a través de la historia de la criptografía, adaptándose a una creciente complejidad criptográfica. Los sistemas criptográficos han evolucionado desde los métodos de lápiz y papel del pasado, pasando por máquinas como Enigma -utilizada por los nazis durante la Segunda Guerra Mundial-, hasta llegar a los sistemas basados en computadoras del presente. Al aumentar la potencia de cálculo de los sistemas criptográficos, también los esquemas criptográficos han ido haciéndose más complejos. A mediados de los años 1970 se inventó una nueva clase de criptografía: la criptografía asimétrica. Los métodos utilizados para romper estos sistemas son por lo general radicalmente diferentes de los anteriores, y usualmente implican resolver un problema cuidadosamente construido en el dominio de la matemática pura. El ejemplo más conocido es la factorización de enteros.

Los resultados del criptoanálisis han cambiado también: ya no es posible tener un éxito ilimitado al romper un código, y existe una clasificación jerárquica de lo que constituye un ataque en la práctica.

La técnica del criptoanálisis se basa en buscar errores o algún error en el sistema para penetrarlo y hacer daños.

El objetivo del criptoanálisis es encontrar debilidades en los sistemas criptográficos que permitan elaborar ataques (ataques criptoanalíticos) que rompan su seguridad sin el conocimiento de información secreta. Para ello estudia en profundidad el diseño y propiedades de los sistemas criptográficos.

Por ejemplo para un sistema criptográfico de cifrado un estudio criptoanalítico puede consistir por ejemplo en conseguir la clave secreta o simplemente en acceder al texto en claro sin ni siquiera tener dicha clave. Sin embargo el criptoanálisis no solo se ocupa de los cifrados sino que su ámbito es más general estudiando los sistemas criptográficos con el objetivo de sortear la seguridad de otros tipos de algoritmos y protocolos criptográficos.

Sin embargo, el criptoanálisis suele excluir ataques que no tengan como objetivo primario los puntos débiles de la criptografía utilizada; por ejemplo, ataques a la seguridad que se basen en el soborno, la coerción física, el robo, el keylogging y demás, aunque estos tipos de ataques son un riesgo creciente para la seguridad informática, y se están haciendo gradualmente más efectivos que el criptoanálisis tradicional.

Para la consecución de su objetivo, de elaboración de ataques criptoanalíticos que 'rompan' la seguridad de los sistemas criptográficos, los criptoanalistas estudian los sistemas criptográficos con el objetivo de descubrir debilidades que se puedan aprovechar. Para ello estudian los sistemas desde distintos enfoques.

La teoría de la información proporciona herramientas para evaluar la seguridad de los sistemas criptográficos. Por ejemplo, en los sistemas de cifrado se estudia la entropía de la clave, de los criptogramas y de los mensajes en claro. Como el mensaje en claro suele estar expresado en idiomas humanos, también es interesante el estudio de su entropía y en especial su ratio de entropía.

Los criptoanalistas también estudian el secreto de los sistemas criptográficos. Por ejemplo, en los sistemas de cifrado estudian el grado de secreto caracterizando aquellos sistemas que tienen secreto perfecto a nivel teórico. De su estudio se concluye que el secreto perfecto requiere que el número de claves sea al menos tan grande como el número de mensajes. Esto es impracticable excepto para los llamados cifradores de libreta de un solo uso. En la práctica la mayor parte de los sistemas tienen claves finitas. Para caracterizar la seguridad de estos sistemas los criptoanalistas han desarrollado el concepto de distancia de unicidad que es el valor mínimo de caracteres cifrados que hacen que solo haya una clave posible que haya sido utilizada para obtener este criptograma. Para ello se aprovecha el concepto de la entropía condicional del conocimiento de la clave una vez conocido el texto cifrado.

Para un sistema de cifrado hay dos entropías condicionales interesantes desde el punto de vista del criptoanalista:

Para un sistema de cifrado hay una serie de entropías condicionales interesantes:

Supongamos
Entonces:


(K)</math>


Se ha demostrado que se cumple la siguiente relación entre las distintas entropías:

De esta relación podemos sacar una conclusión:

Por ejemplo, la criptografía asimétrica emplea problemas matemáticos "duros" como base para su seguridad, así que un punto obvio de ataque es desarrollar métodos para resolver el problema. Los algoritmos asimétricos se diseñan en torno a la conjeturada dificultad de resolver ciertos problemas matemáticos. Si se encuentra un algoritmo mejorado que puede resolver el problema, el criptosistema se ve debilitado. Ejemplos:

Otra característica distintiva de los algoritmos asimétricos es que, a diferencia de los ataques sobre criptosistemas simétricos, cualquier criptoanálisis tiene la oportunidad de usar el conocimiento obtenido de la clave pública.

Los ataques criptoanalíticos consisten en la aplicación de estudios criptoanalíticos para explotar las debilidades de sistemas criptográficos y así 'romper' su seguridad.

Los ataques criptoanalíticos varían en potencia y en su capacidad de amenaza para los sistemas criptográficos. Se dice que un ataque explota una "debilidad certificacional" si es un ataque teórico que resulta improbable de aplicar en ninguna situación realista; Muchos de los resultados demostrados en la investigación criptoanalítica moderna son de este tipo.

Cada ataque tiene sus propiedades, las cuales lo caracterizan, y que hacen que ese ataque sea más o menos realizable.

No todos los ataques criptoanalíticos tienen como objetivo la ruptura total del sistema. El objetivo de un ataque criptoanalítico es obtener información desconocida sobre el sistema criptográfico de forma que se vaya debilitando su seguridad

Los ataques criptoanalíticos se puede clasificar en función de sus características.

Los ataques se pueden clasificar según la forma de actuar del atacante

En los ataques pasivos el atacante no altera la comunicación, solo la escucha o monitoriza, para obtener información.
Por tanto este tipo de ataques suelen usar técnicas de escucha de paquetes(sniffing) y de análisis de tráfico.
Son difíciles de detectar ya que no implican alteración de los datos.
En algunos casos este tipo de ataques se pueden dificultar cifrando la información posible objetivo de escuchas.

Suponen alguna modificación del flujo de datos o la creación de flujos falsos. Hay muchas técnicas que se usan en este tipo de ataques. Ejemplos:

El criptoanálisis puede realizarse bajo una serie de supuestos sobre cuánto puede observarse o descubrirse sobre el sistema en cuestión antes de realizar el ataque. Como un punto de comienzo básico se supone que, para los propósitos del análisis, el algoritmo general es conocido; esta es la Máxima de Shannon, ""el enemigo conoce el sistema"". Este es un supuesto razonable en la práctica - a lo largo de la Historia, hay incontables ejemplos de algoritmos secretos que fueron conocidos mediante el espionaje, la traición y la ingeniería inversa. (En algunas ocasiones, algunos códigos han sido reconstruidos mediante la pura deducción, por ejemplo, el código Lorenz y el código PURPLE, así como una cierta cantidad de códigos clásicos.)

Otros supuestos se pueden categorizar como sigue:

Estos tipos de ataque difieren evidentemente en la plausibilidad de que ocurran en la práctica. Aunque algunos son más probables que otros, los criptógrafos suelen adoptar un enfoque conservador y asumir el peor caso imaginable cuando diseñan algoritmos, razonando que si un sistema es seguro incluso contra amenazas tan poco realistas, entonces debería resistir también al criptoanálisis en el mundo real.

Los supuestos en los que se basan estos ataques son a menudo más realistas de lo que podría parecer a primera vista. Para obtener un ataque con texto claro conocido, el criptoanalista podría muy bien conocer o ser capaz de inferir una parte que probablemente forma parte del texto claro, como por ejemplo el encabezamiento de una carta cifrada ("Estimado Sr."), o que el inicio de una sesión de ordenador contenga las letras "LOGIN". Un ataque de texto claro escogido es menos probable, pero en algunos casos puede ser plausible: por ejemplo, si convences a alguien para reenviar un mensaje que tú mismo le has mandado antes, pero en forma cifrada. Los ataques de clave relacionada son básicamente teóricos, aunque pueden ser realistas en ciertas situaciones, como por ejemplo al construir funciones hash criptográficas utilizando un cifrado por bloques.

Los resultados de un criptoanálisis también pueden variar en utilidad. Por ejemplo, el criptógrafo Lars Knudsen (Knudsen, 1998) clasificó varios tipos de ataque sobre cifrados por bloques de acuerdo con la cantidad y la calidad de la información secreta que pudiera ser descubierta:


Se pueden aplicar estas categorías a los ataques sobre otros tipos de algoritmos.

Los ataques se pueden categorizar por la cantidad de recursos que requieren. Estos pueden tomar la forma de:


En la criptografía académica, una "debilidad" o una "ruptura" en un algoritmo se definen de una manera bastante conservadora. Bruce Schneier resume esta posición de la siguiente manera: ""Romper un cifrado simplemente significa encontrar una debilidad en el cifrado que puede ser explotada con una complejidad inferior a la de la fuerza bruta. No importa que la fuerza bruta pudiera requerir 2 cifrados; un ataque que requiera 2 cifrados se consideraría una ruptura... puesto de una manera simple, una ruptura puede ser tan sólo una debilidad certificacional: una evidencia de que el código no es tan bueno como se publicita"" (Schneier, 2000).

Hay multitud de métodos de ataque criptoanalíticos. Estos se pueden clasificar en a si están especializado en algún tipo de criptografía o si son más generales. Los principales son los siguientes:


Los ordenadores cuánticos son potencialmente útiles para el criptoanálisis. Debido a que los estados cuánticos pueden existir en una superposición (es decir, estar entrelazados), es posible un nuevo paradigma computacional, en el que un bit no representa tan solo los estados 0 y 1, sino cualquier combinación lineal de estos. Peter Shor de los Laboratorios Bell probó la posibilidad, y varios equipos han demostrado uno u otro aspecto de la computación cuántica en los años transcurridos desde entonces. Por el momento, solo se ha demostrado una muy limitada prueba de posibles diseños. No hay, a fecha de 2006, una perspectiva creíble de un ordenador cuántico real y utilizable.

Sin embargo, de construirse un ordenador cuántico, muchas cosas cambiarían. La computación en paralelo sería probablemente la norma, y varios aspectos de la criptografía cambiarían.

En particular, dado que un ordenador cuántico sería capaz de realizar búsquedas de claves mediante fuerza bruta extremadamente rápidas, tamaños de clave considerados hoy en día más allá de los recursos de cualquier atacante por fuerza bruta quedarían al alcance de este ataque. Los tamaños de clave necesarios para quedar más allá de la capacidad de un ordenador cuántico serían considerablemente más grandes que los actuales. Algunos escritores de divulgación han declarado que ningún cifrado permanecería seguro de estar disponibles los ordenadores cuánticos. Otros aseguran que simplemente añadiendo bits a las longitudes de las claves se evitarán los ataques de fuerza bruta, incluso con ordenadores cuánticos.

Una segunda posibilidad es que el aumento en capacidad computacional pueda hacer posibles otros ataques de búsqueda de claves, más allá de la simple fuerza bruta, contra uno o varios de los algoritmos actualmente inexpugnables. Por ejemplo, no todo el progreso en la factorización mediante números primos se ha debido a una mejora de los algoritmos. Una parte se debe al incremento del poder computacional de los ordenadores, y la existencia de un ordenador cuántico en funcionamiento podría acelerar considerablemente las tareas de factorización. Este aspecto es bastante predecible, aunque no claramente. Lo que no puede ser anticipado es un avance en el campo teórico que requiera la computación cuántica, que pudiera hacer realizables ataques actualmente impracticables o incluso desconocidos. En ausencia de un método para predecir estos avances, solo nos queda esperar.

Se desconoce si existe un método de cifrado en tiempo polinómico que requiera un tiempo exponencial para su descifrado, incluso para un ordenador cuántico.

El criptoanálisis ha evolucionado conjuntamente con la criptografía, y la competición entre ambos puede ser rastreada a lo largo de toda la historia de la criptografía. Las claves nuevas se diseñaban para reemplazar los esquemas ya rotos, y nuevas técnicas de criptoanálisis se desarrollaban para abrir las claves mejoradas. En la práctica, se considera a ambas como las dos caras de la misma moneda: para crear un sistema criptográfico seguro, es necesario tener en cuenta los descubrimientos del criptoanálisis. De hecho, hoy en día se suele invitar a la comunidad científica a que trate de romper las nuevas claves criptográficas, antes de considerar que un sistema es lo suficientemente seguro para su uso.

Aunque la expresión criptoanálisis es relativamente reciente (fue acuñada por William F. Friedman en 1920), los métodos para romper códigos y cifrados son mucho más antiguos. La primera explicación conocida del criptoanálisis se debe al sabio árabe del siglo IX, Yusuf Yaqub ibn Ishaq al-Sabbah Al-Kindi, en su "Manuscrito para Descifrar Mensajes Criptográficos". Este tratado incluye una descripción del método de análisis de frecuencias (Ibraham, 1992).

El análisis de frecuencias es la herramienta básica para romper los cifrados clásicos. En todas las lenguas conocidas, ciertas letras del alfabeto aparecen más frecuentemente que otras; por ejemplo, en español, las vocales son muy frecuentes, ocupando alrededor del 45% del texto, siendo la E y la A las que aparecen en más ocasiones, mientras que la frecuencia sumada de F, Z, J, X, W y K no alcanza el 2%. Igualmente, se pueden reunir estadísticas de aparición de pares o tríos de letras. El análisis de frecuencias revelará el contenido original si el cifrado utilizado no es capaz de ocultar estas estadísticas. Por ejemplo, en un cifrado de substitución simple (en el que cada letra es simplemente substituida por otra), la letra más frecuente en el texto cifrado sería un candidato probable para representar la letra "E".

El análisis de frecuencias se basa tanto en el conocimiento lingüístico como en las estadísticas, pero al volverse cada vez más complicados los cifrados, las matemáticas se convirtieron gradualmente en el enfoque predominante en el criptoanálisis. Este cambio fue particularmente evidente durante la Segunda Guerra Mundial, cuando los esfuerzos para romper los códigos del Eje requirieron nuevos niveles de sofisticación matemática. Más aún, la automatización fue aplicada por primera vez en la Historia al criptoanálisis, bajo la forma de los dispositivos Bomba y Colossus, una de las primeras computadoras.

Aunque la computación fue utilizada con gran éxito durante la Segunda Guerra Mundial, también hizo posible nuevos métodos criptográficos que eran órdenes de magnitud más complejos que los empleados hasta la fecha. Tomada como un todo, la criptografía moderna se ha vuelto mucho más impenetrable al criptoanalista que los métodos de pluma y papel del pasado, y parece que en la actualidad llevan ventaja sobre los métodos del puro criptoanálisis. El historiador David Kahn escribió: ""Son muchos los criptosistemas en venta hoy por parte de cientos de compañías comerciales que no pueden ser rotos por ningún método conocido de criptoanálisis. De hecho, en ciertos sistemas incluso un ataque de texto claro escogido, en el que un fragmento de texto claro seleccionado es comparado con su versión cifrada, no permite conocer el código para romper otros mensajes. En cierto sentido, entonces, el criptoanálisis está muerto. Pero éste no es el final de la historia. El criptoanálisis puede estar muerto, pero, mezclando mis metáforas, hay más de un modo de desollar un gato."" (Observaciones sobre el 50 Aniversario de la National Security Agency, 1 de noviembre de 2002). Kahn menciona a continuación las mayores posibilidades para la intercepción, la colocación de dispositivos grabadores ("bugging"), los ataques de canal lateral y la criptogtafía cuántica como sustitutos de los métodos tradicionales del criptoanálisis.

Kahn podría haberse apresurado demasiado al declarar al criptoanálisis muerto; aún no se han extinguido los cifrados débiles. En medios académicos, se presentan regularmente nuevos diseños, y también son rotos frecuentemente: el cifrado por bloques Madryga, de 1984, demostró ser vulnerable a un ataque con solo texto cifrado disponible en 1998; FEAL-4, propuesto como sustituto para el algoritmo estándar de cifrado de datos DES fue demolido por una avalancha de ataques de la comunidad académica, muchos de los cuales no eran enteramente realizables en condiciones prácticas. En la industria, igualmente, los cifrados no están exentos de fallos: por ejemplo, los algoritmos AS/1, AS/2 y CMEA, usados en la industria de teléfonos móviles, pueden ser rotos en horas, minutos o incluso en tiempo real por equipo informático ampliamente disponible. En 2001, se demostró que el algoritmo WEP, utilizado para proteger redes Wi-Fi, es susceptible de ser atacado mediante un ataque de clave relacionada.

Los criptoanálisis exitosos han influido sin lugar a dudas en la Historia. La capacidad de leer los pensamientos, supuestamente secretos, o los planes de otros puede ser una ventaja decisiva, y nunca con mayor razón que en tiempos de guerra. Por ejemplo, durante la Primera Guerra Mundial, el descifrado del Telegrama Zimmermann fue capital para la entrada de los Estados Unidos en la guerra. En la Segunda Guerra Mundial, el criptoanálisis de los códigos alemanes, incluyendo la máquina Enigma y el código Lorenz, ha sido considerado desde un factor que apenas acortó la guerra en algunos meses en Europa, hasta un elemento crucial que determinó el resultado final (véase ULTRA). Los Estados Unidos también se beneficiaron del criptoanálisis del código japonés PURPLE durante la contienda (véase MAGIC).

Todos los gobiernos han sido conscientes desde antiguo de los potenciales beneficios del criptoanálisis para la inteligencia militar, tanto en lo puramente bélico como en lo diplomático, y han establecido con frecuencia organizaciones dedicadas en exclusiva al descifrado de códigos de otras naciones, por ejemplo GCHQ y NSA, organizaciones americanas todavía muy activas hoy en día. En 2004, surgió la noticia de que los Estados Unidos habían roto los códigos utilizados por Irán: ).

En inglés:

En inglés:


</doc>
<doc id="15455" url="https://es.wikipedia.org/wiki?curid=15455" title="Protestantismo">
Protestantismo

El protestantismo es una de las ramas de reciente aparición en el cristianismo. Aproximadamente 801 millones de cristianos, o el 36,7% de los 2184 millones de cristianos, son protestantes. Los protestantes fueron originariamente grupos de disidentes que, alegando que la Iglesia católica venía incurriendo en numerosos errores teológicos, se separaron de esta en el sigloXVI, en un proceso que se denomina la Reforma protestante. Desde entonces, los protestantes niegan el primado del apóstol Pedro y por consiguiente la sucesión apostólica de los obispos de Roma y la eficacia de los sacramentos. La mayoría de los protestantes creen en el sacerdocio de todos los creyentes, la salvación solamente por la fe y no por las buenas obras y la autoridad suprema de la Biblia por encima de la Sagrada Tradición ("sola scriptura").

El término "protestante" hacía referencia originariamente a los partidarios de las ideas luteranas de la Reforma en Alemania a raíz de su protesta y resistencia a los edictos imperiales que intentaban buscar la uniformidad religiosa de Alemania. Para otros, el apelativo se les atribuyó con ocasión de que los príncipes que seguían a Martín Lutero protestaron por no poder asistir a la Dieta de Espira en 1529, apelando al concilio.

La doctrina luterana (algunos elementos centrales de las propuestas de Martín Lutero, además de en las noventa y cinco tesis del manifiesto colocado en la puerta de la Iglesia de Todos los Santos de Wittenberg, Alemania, el 31 de octubre de 1517, se presentan en sus obras 'Catecismo Mayor' y 'Los Artículos de Esmalcalda') giraría en torno a la idea de que la Biblia es la única autoridad en materia de fe para la Iglesia y en la necesidad absoluta de la gracia de Dios para que el hombre, mediante la sola fe en Cristo y el Evangelio, pueda ser salvado por Dios en un acto de conversión interior.

El protestantismo también defiende las doctrinas de la absoluta depravación del hombre y su necesidad total de Dios, la sola mediación de Cristo, la sacramentalidad única del bautismo (cuando la persona reconoce su naturaleza pecaminosa),y la cena del Señor (cuando no son percibidos como símbolos) y las obras buenas como fruto de la fe sola. Además, rechaza la autoridad del papa, salvación por indulgencias, el bautismo de bebés, el purgatorio, el sacrificio incruento de la misa, la devoción a los santos y veneración a imágenes religiosas, el sacramento de la penitencia, la intercesión de la Virgen María y los santos difuntos.

El protestantismo es muy diverso, y es bastante más heterogéneo que la Iglesia católica y la Iglesia ortodoxa, tanto desde el punto de vista teológico como el eclesiástico. El protestantismo no cuenta con una autoridad suprema ni tiene unidad estructural. Los protestantes desarrollaron la idea de la «Iglesia invisible», que se contrapone a la posición católica, que ve en la Iglesia católica la única Iglesia verdadera, fundada por Jesús.

Debido a la diversidad de grupos que se sumaron al protestantismo y sus diferencias doctrinales, el mismo no se corresponde con el modelo de una sola iglesia ni una doctrina homogénea. A pesar de las coincidencias originales expresadas principalmente en las Cinco Solas, aun en sus orígenes, no se podría hablar de un movimiento sólidamente uniforme en este aspecto. El protestantismo habitualmente se expresa en tres tipos de movimientos o congregaciones:


Existen en el mundo más de 800 millones de protestantes, distribuidos en diferentes denominaciones que siguen diferentes líneas interpretativas de la Biblia.

El término "protestante" deriva del latín "protestari", que significa ‘declaración pública o protesta’, en la protesta de los cinco príncipes electores y 14 ciudades imperiales alemanas contra la decisión de la Dieta de Espira en 1529, que reafirmaba el edicto de la Dieta de Worms de 1521, en el que se proscribía creer y enseñar las doctrinas luteranas. El término "protestante" no se utilizó en su origen para describir a los reformadores, sino posteriormente para describir a los diferentes grupos disidentes de la ortodoxia católica. Desde entonces se ha empleado en diferentes sentidos, siendo común para referirse a aquellos devotos no pertenecientes a la Iglesia católica ni a la ortodoxa.

Se trata de una de las principales divisiones de la cristiandad, junto con las Iglesias ortodoxas orientales, las Iglesias ortodoxas occidentales y la Iglesia católica. Las doctrinas de las diversas ramas protestantes varían, pero son prácticamente unánimes en lo que implica una relación personal directa del individuo con Dios sin ninguna institución de por medio y la Biblia como autoridad última en asuntos de fe, conocido como "sola scriptura".

El 31 de octubre de 1517, Martín Lutero, un fraile agustino alemán, publicó las noventa y cinco tesis, las cuales, de acuerdo con la tradición, clavó en la puerta de la Iglesia del palacio de Wittenberg, práctica común entonces. Las tesis condenaban la avaricia y el paganismo en la Iglesia católica como un abuso, y pedían una disputa teológica en lo que las indulgencias podían dar. Sin embargo, en sus tesis no cuestionaba directamente la autoridad del papa para conceder indulgencias. Lutero criticaba en particular la práctica común por aquel entonces de la venta de indulgencias, de las que la Iglesia católica de LeónX hizo un uso extensivo para recaudar fondos dedicados a la construcción de la Basílica de San Pedro, algo que consideraba contra las enseñanzas bíblicas, poniendo en duda la autoridad del papa y la doctrina del purgatorio. Lutero mantuvo que la salvación se garantizaba por la fe sola, expresando que las buenas obras y los sacramentos administrados por la Iglesia católica no eran necesarios para ser salvado. Lutero envió una copia de las tesis a su obispo, el cual las reenvió a Roma.
Tras ignorar inicialmente a Lutero, el papa LeónX escribió una refutación académica de sus tesis. En ella mantuvo la autoridad papal sobre la Iglesia y condenó cada “desviación” como una apostasía. Lutero replicó, iniciándose una controversia que culminó con la excomunión de Lutero por el papa LeónX el 3 de enero de 1521 mediante la bula "Decet Romanum Pontificem".

Del mismo modo que no se puede hablar de una sola iglesia protestante, tampoco se puede hablar de una sola doctrina protestante coherente y cohesionada. De hecho, la variedad doctrinal que el protestantismo ha ido adoptando a lo largo de su evolución ha sido una de las causas de su fragmentación. Aun con todo, se puede hablar de una doctrina de mínimos que con distinta intensidad sí comparten todas las iglesias herederas de la Reforma. Tradicionalmente se suele resumir esta doctrina común en las “cinco solas”, que desarrolladas comprenden el núcleo de la fe protestante:

Además de las “cinco solas”, el protestantismo, como la mayoría de las corrientes del cristianismo, comparte igualmente las creencias en la Trinidad, la cristología clásica o de los primeros concilios ecuménicos, la celebración de los sacramentos del bautismo y la cena del Señor (eucaristía), aunque con diferencias importantes, la creencia en el Juicio Final y la resurrección de la carne, etc. Algunas de sus iglesias se adhieren a los credos niceno-constantinopolitano y de Atanasio (Iglesia Anglicana, Iglesia Luterana, Iglesia Metodista, Iglesia Presbiteriana y calvinistas en general, etc.).

En cuanto a la eclesiología el protestantismo concibe a la Iglesia como una, santa, universal y apostólica igual que las demás iglesias cristianas. Lo peculiar de su eclesiología es que defiende la idea de una doble dimensión de la iglesia, una invisible y otra visible. Es invisible por cuanto es la reunión de todos los santos en el cuerpo místico de Cristo en todos los tiempos y lugares, la unicidad y santidad de la Iglesia está garantizada, pues, en Cristo. Es visible en la denominación o iglesia local en la que se congrega el creyente. Se acepta la existencia de distintas jurisdicciones en la iglesia y se rechaza la idea de que ésta deba estar gobernada por una sola persona o por una sola institución. El ecumenismo es percibido como la necesidad de buscar la unidad doctrinal en lo esencial y la intercomunión de todos los cristianos, pero no se acepta un ecumenismo que conduzca a la construcción de una sola iglesia gobernada por una sola institución.

Para entender la doctrina protestante hay que tener en cuenta que en su génesis fue un movimiento de reforma de la Iglesia católica. Por este motivo, muchas de las doctrinas protestantes solo tienen sentido y deben su existencia al catolicismo y a la necesidad o intención de corregirlo de lo que fueron percibidos como errores por los reformadores protestantes. Un ejemplo que deja clara esta génesis dialéctica de ciertas doctrinas protestantes es, por ejemplo, la negación del purgatorio. El purgatorio en el protestantismo sencillamente no tiene cabida en su teología ni en sentido alguno en sus desarrollos positivos, pero aun así es doctrina usual en confesiones y catecismos protestantes el decir: “el purgatorio no existe”. Algunas doctrinas protestantes con este origen son:

En un principio, los protestantes expresaron sus posiciones doctrinales por medio de Confesiones de Fe, breves documentos apologéticos. En el luteranismo destaca la Confesión de Augsburgo. En el ámbito de la reforma calvinista, la Confesión Escocesa (1560), La Segunda Confesión Helvética (1531) y la Confesión de Fe de Westminster (1647). En el anglicanismo destacan Los Treinta y Nueve Artículos de Religión de la Iglesia de Inglaterra que concilian posiciones anglicanas y reformadas. Las iglesias bautistas y evangélicas también tienen sus propias declaraciones y confesiones de fe. La declaración teológica de Barmen, contra el régimen nazi, y la breve declaración de fe de la Iglesia presbiteriana en los Estados Unidos son ejemplos de declaraciones recientes.

La enseñanza doctrinal en el protestantismo suele realizarse en la iglesia mediante la predicación y la escuela dominical, predominando el aspecto ético y religioso.

La educación que la Reforma implantó desde sus inicios con Lutero, suponía la lectura de la Biblia, surgiendo la necesidad de enseñar a leer a todos, lo que llevó a que los reformadores se interesaran por la enseñanza popular. Cabe destacar que cada rama del protestantismo tiene características propias en cada país, además de que las doctrinas se comenzaron a impartir en lenguas vernáculas, o sea, en el idioma de cada país.

Diversos autores han destacado el importante papel que tuvo la Reforma protestante en el impulso de la educación y alfabetización pública, si este se compara con la situación en los países católicos, surgiendo como una reacción eclesiástica, pero con un carácter religioso, alentándose con insistencia el estudio bíblico personal y colectivo, así como la participación activa de todos los miembros (laicos y ministros) en la formación para la evangelización. Generalmente es la predicación el medio más usado, aunque existen catecismos como el de Heidelberg y el Mayor en el luteranismo o el de Westminster en el presbiterianismo. Los seminarios y escuelas bíblicas son los centros de estudio teológico superior.

Debido a los errores de la Iglesia católica, que durante mucho tiempo había estado atesorando bienes materiales y se había empeñado en una lucha por el poder terrenal, las capas sociales más bajas, campesinos, artesanos y comerciantes estaban descontentos con las jerarquías eclesiásticas, que se llevaban el diezmo de sus bienes y de los que prácticamente no recibían nada a cambio. La vida de lujo y pecado de los cardenales y obispos en Roma era bien conocida por toda la población de Europa e incluso reyes y emperadores sentían rencor hacia el Papado que interfería frecuentemente en el gobierno. Sin embargo, ya desde el sigloXIII, con Francisco de Asís, se planteaba la cuestión de si la Iglesia debería acumular riquezas o debería repartirlas entre los pobres.

Se denomina “Período de la Prerreforma” al movimiento iniciado por John Wyclif, un peregrino inglés de origen judío que quería que la gente interpretara la Biblia por sí misma en vez de que la Iglesia tomara decisiones en el estilo de vida de esas personas. En el sigloXIV, Wyclif defendió, en su natal Inglaterra, varias opiniones que atentaban contra la autoridad de la Iglesia, criticando las riquezas del papado y las indulgencias mediante las que los ricos podían comprar el perdón para determinados pecados, incluso por anticipado. Asimismo, hizo que la Biblia se tradujera al inglés y encomendó a discípulos suyos, conocidos como «Los Predicadores de los Pobres», que predicaran en inglés, cuando la Santa Sede imponía el latín en todas las predicaciones. Wyclif y William Tyndale pudieron traducir la Biblia al inglés en contra de la Iglesia católica, para que las personas pudieran leerla en su lengua vernácula. De estos libros traducidos se imprimieron muy pocos, alrededor de 6000 ejemplares.

Después de muerto, la Iglesia le consideró hereje e hizo que, 44 años después de su muerte, su cuerpo fuera desenterrado y quemado en la hoguera, pero sus ideas calaron hondo en el ánimo de Jan Hus, un reformista bohemio que inició una campaña contra la Iglesia. Su ejecución por hereje en 1415 provocó una guerra civil en Bohemia que fue sofocada por el emperador y el Papa. A lo largo de todo este tiempo, tanto el Movimiento Lolardo o Wycliffita, como el Movimiento Husita y la protesta místico-evangélica de Girolamo Savonarola, señalaron de manera objetiva y frontal las diferentes opiniones sobre el cristianismo en la Edad Media, dentro de una perspectiva bíblica y evangélica.

El desarrollo de la imprenta a mediados del sigloXV hizo que las ideas anticlericales tuvieran una mayor difusión, y cuando Martín Lutero publicó, en 1517, sus noventa y cinco tesis contra las indulgencias papales, pudo difundir sus ideas mucho más que sus predecesores. Excomulgado por el Papa, condenado por el emperador, perseguido por ejércitos y sacerdotes, Lutero se mantuvo oculto durante más de un año en el castillo de Wartburg traduciendo la Biblia al alemán y escribiendo artículos que eran publicados y distribuidos masivamente. El resultado fue una revuelta de los campesinos que pensaron encontrar una liberación de la tiranía eclesiástica. Lutero, sin embargo, no pretendía desatar una guerra, por lo que publicó un panfleto en el que exhortaba a los campesinos a abandonar las armas. Ante esta actitud conciliadora de Lutero a dicha rebelión, muchos nobles se volvieron partidarios suyos.

Tras el fin de la revuelta, CarlosV concedió que cada Estado pudiera decidir, dentro de su propio territorio, sobre cuestiones religiosas, pero en 1529 la mayoría católica hizo que se derogase esta norma. Los luteranos elevaron su más enérgica protesta, lo que les hizo ganar el antes mencionado apodo de “protestantes”. CarlosV estaba empeñado en acabar con los luteranos, pero distraída su atención por varias guerras contra Francia y el Imperio turco, no pudo enviar tropas hasta quince años más tarde. Para entonces ya era tarde. El luteranismo se había convertido en la fe de más de la mitad de la población de Alemania y, aunque se perdieron batallas al principio, los luteranos consiguieron ganar la libertad religiosa.

En el plazo de dos décadas más, la Reforma se había expandido por la mayor parte del noroeste de Europa. En Inglaterra el rey EnriqueVIII rechazó la autoridad papal sobre la Iglesia, y la Iglesia de Inglaterra entró en una reforma que la volvió una entidad esencialmente protestante (aunque a menudo los anglicanos, también llamados episcopalianos, se clasifican aparte). En Suiza, Francia, partes de Alemania, de Escocia y de los Países Bajos comenzó una segunda corriente de reforma no luterana, influida principalmente por Juan Calvino, el francés convertido en ginebrino, y el líder suizo Ulrico Zuinglio.

Al mismo tiempo apareció un estilo más radical de Protestantismo en el ala izquierda del movimiento. Anabaptistas, menonitas y otros rebautizaron cristianos y los iniciaron en un movimiento que rechazó drásticamente las prácticas católicas, incluso las que el luteranismo, calvinismo y anglicanismo no habían rechazado.

Como se ha mencionado, la Reforma se extendió desde sus bases originales a Escandinavia y la Europa Central, pero apenas penetró en Rusia y en el sudeste de Europa, donde prevalecía la Iglesia ortodoxa, o en la Europa meridional, que seguía firmemente católica. Después de una serie de guerras religiosas desde mediados del sigloXVI hasta mediados del XVII, la mayoría de los protestantes (excepto los radicales) y los católicos adoptaron el principio de que los gobernantes de una región determinarían la religión de esa provincia o Estado. La separación de la Iglesia y el Estado, un principio que otros protestantes vinieron a sostener a fines del sigloXVIII, comenzó a romper la primacía protestante en el noroeste de Europa.

En la última parte del sigloXVIII y a través del sigloXIX y hasta el presente, los misioneros protestantes extendieron el movimiento por casi todo el mundo. Los puntos de penetración protestantes fueron muchas costas asiáticas y africanas, pero hasta hace relativamente poco que el protestantismo no llegó hasta la católica América hispana. A partir de 1607, cuando los anglicanos llegaron a Virginia, y hasta finales del sigloXIX, después de la inmigración en gran escala desde Europa del sur y de Irlanda, se creía que Norteamérica, menos Quebec, era territorio en gran parte protestante.

De una forma algo más pacífica, las ideas protestantes se infiltraron en muchos países europeos, unas veces apoyadas por la burguesía, otras por la nobleza, en ocasiones directamente por la monarquía. Apenas cincuenta años después de morir Lutero, el protestantismo había cambiado por completo el mapa de la sociedad.

La idea fundamental del protestantismo es que la Biblia es la Palabra de Dios pero, al contrario de lo que siempre afirmaron los católicos, cualquiera puede interpretarla y comprenderla. Así, libres de la autoridad eclesiástica, los protestantes pueden leer la Biblia y tras meditar en lo que han leído, pueden sacar sus propias conclusiones, conclusiones que posteriormente podrán ser discutidas con otras personas.

Esta libertad en la interpretación bíblica ha provocado que a lo largo de los años hayan surgido numerosas denominaciones, cada una con una interpretación distinta de diversos pasajes de la Biblia, pero también ha contribuido a darle un valor al pueblo, libre por fin de la autoridad religiosa, que fue el primer paso para las sociedades más democráticas.

La traducción de la Biblia a los diversos idiomas europeos, favorecida también por el auge de la imprenta, ha contribuido a la difusión de la cultura, haciendo que en los países protestantes el analfabetismo descendiera sensiblemente.

Entre los principales y más destacados personajes prerreformadores se señalan los siguientes: John Wyclif (1324-1384), William Tyndale, Jan Hus (1369-1415) y Girolamo Savonarola (1452-1498).

El Renacimiento, con su mentalidad crítica, trajo consigo el cuestionamiento de las enseñanzas y prácticas de la Iglesia, confrontándose principios humanistas con la teología escolástica medieval.

Con la invención de la imprenta como nuevo elemento divulgador, las ideas de los reformadores se expandieron con rapidez. El crecimiento de la ciudad y de su elemento intelectual, la Universidad, fue un catalizador de la Reforma.

El fortalecimiento de las monarquías nacionales europeas creó una palpable fricción entre poderes. La decadencia de los postulados dogmáticos de la reforma eclesiástico-cluniacense, y más concretamente de los papas Gregorio VII, Inocencio III y Bonifacio VIII sobre el poder supremo del papado, así como la corrupción de la máxima cúpula del sistema jerárquico eclesiástico medieval desde principios del sigloXIV con los cismas de Aviñón y de Occidente dieron lugar al surgimiento de exposiciones teológicas como las de Juan Taulero, de Guillermo de Occam y de Marsilio de Padua de un trasfondo antipapal. Surge la tesis conciliarista. El creciente fervor nacionalista europeo llevó a mirar con desconfianza y repudio el dominio papal sobre las diferentes naciones del viejo continente. Muchos monarcas vieron en la Reforma Protestante un modo de afianzar el Estado nacional y su poder monárquico o imperial. En el centro y norte de Europa hubo países, como Suiza o Suecia, donde la Reforma fue uno de los instrumentos más eficaces de la lucha contra los países católicos que los dominaban.

Influyen también la rápida decadencia del escolasticismo y el resurgimiento de la teología agustiniana con unos caracteres renovadores, volviendo con mayor vigor la lectura y el estudio de la teología de san Agustín en detrimento de la teología tomista.

Los principales reformadores, de vasta cultura teológica y humanista, se consideraban a sí mismos fieles cristianos que aspiraban a regresar a las doctrinas apostólicas y a renovar la Iglesia cristiana en la práctica y doctrina.

Juan Calvino estudió en la Sorbona y su padre trabajaba con un obispo; Lutero era monje y profesor universitario de Biblia; Zuinglio era sacerdote y humanista. De acuerdo con el programa de los humanistas, buscaron en las fuentes de la antigüedad cristiana las bases para una renovación. Releyeron las Sagradas Escrituras y a los Padres de la Iglesia, (especialmente a San Agustín), interpretando una visión de la fe y una doctrina más bíblica y cristocéntrica, despreciando, por otro lado, toda la tradición cultural y religiosa acumulada por la Iglesia desde los primeros siglos.

La diseminación de las ideas protestantes fue facilitada por la invención de la imprenta, que hizo posible difundir una amplia literatura apologética, bíblica y devocional y fomentó la edición de nuevas traducciones de la Biblia en lenguas vernáculas. Estas revisiones del texto hicieron patente la débil base de algunas doctrinas medievales. La nueva forma de fundamentar la autoridad, junto con el rechazo de la formulación escolástica ahora sustituida por lenguaje bíblico, hacía difícil a los teólogos católicos rebatirla. En el Concilio de Trento, los obispos católicos partidarios de Roma optarían por limitar el acceso laico a las escrituras, estableciendo que la Vulgata Latina era la única Biblia autorizada y redactando un índice de libros prohibidos.

Como resultado del apoyo de los gobiernos nacionales y locales, la Reforma Protestante logró éxito en amplias áreas de Europa. Se hizo predominante en el norte de Alemania y en Escandinavia, en su forma luterana. En Escocia prosperó la Iglesia presbiteriana de inspiración calvinista. También las Iglesias reformadas fructificaron en los Países Bajos, en las ciudades suizas y en el oriente de Hungría. Con posterioridad retornaron a la influencia del catolicismo Francia, Polonia, Bohemia, Bélgica, Hungría y amplias regiones de Alemania (sobre todo en el sur y el oeste). No obstante, con el desarrollo de los imperios europeos, particularmente el británico, el protestantismo continuó su expansión. Los siglosXIX y XX presenciaron una fuerte labor misionera que lo expandió por Asia, Oceanía, África y América. Hoy en día, cálculos estimativos señalan que más de 500 millones de personas profesarían alguna de las diversas formas del protestantismo moderno.

Los fundadores y colaboradores de la Reforma utilizaron todos los medios a su alcance para la extensión de la misma, valiéndose de cualquier factor que pudiese contribuir favorablemente con su movimiento. En relación a este punto podemos señalar algunos medios analizados y criticados por sus detractores (Iglesia católica):

En Alemania surgió la Iglesia luterana. En la Suiza de habla alemana, Ulrico Zuinglio y otros comenzaron también un intento de reforma de la Iglesia católica. Pero Juan Calvino fue el dirigente más destacado de la Reforma Protestante en Suiza. La Reforma que se había iniciado casi simultáneamente en Zúrich (cantón de habla alemana) y Ginebra (francófona) fue extendiéndose por los países vecinos, llegando a Escocia de la mano de John Knox, que se había formado en Ginebra, dando origen a la Iglesia Presbiteriana.

Mientras tanto, la Iglesia de Inglaterra (anglicana) no se dejó influir en un primer momento por el protestantismo, pero tras su ruptura con la Iglesia de Roma, comenzó un paulatino y vacilante acercamiento hacia los ideales reformados. Actualmente las Iglesias de la Comunión anglicana se declaran abiertamente reformadas. De ellas surgió la Iglesia Metodista, que, junto a los presbiterianos, y a las iglesias bautistas, entre otros, se conocen históricamente como disidentes.

Fuera de ese protestantismo, que muchos estudiosos denominan “magisterial”, se dio otra vertiente, que se distinguió tanto del catolicismo como de las Iglesias protestantes de carácter nacional. Esta corriente recibe el nombre de "Reforma Radical", cuyos integrantes pasaron a conocerse como anabaptistas, que rechazó la unión de la Iglesia cristiana con el Estado y repudiaron el bautismo infantil, constituyéndose en iglesias independientes o segregadas que dieron lugar a corrientes como los menonitas, e influyeron a los fundadores de otras, como las iglesias bautistas.

Desde finales del sigloXIX empezaron a surgir otras diferentes ramas influenciadas por los movimientos de reavivamiento. La principal corriente protestante surgida en ese periodo es el pentecostalismo, que empezó en Estados Unidos y se ha extendido principalmente a Latinoamérica y África. Las iglesias pentecostales dan un énfasis mayor en los dones espirituales descritos en el Nuevo Testamento, principalmente el “hablar en lenguas”.

Cada rama o denominación del protestantismo suele estar subdividida en diversos grupos independientes a los que también se les suele llamar “denominaciones” o “familias denominacionales”. Estos grupos se suelen distinguir entre los de su propia rama por diferencias cuanto al énfasis en determinados puntos doctrinales y aplicación de los textos bíblicos, pero también por estar en diferentes países o incluso regiones de una misma nación. También se suele dar el caso de divisiones provocadas por divergencias administrativas, aunque esto afecta mucho más a iglesias locales que a grupos enteros.

Asimismo existe una infinidad de congregaciones locales o grupos de congregaciones que no poseen vínculo formal con denominaciones instituidas, a las cuales se suele llamar “iglesias independientes”. Este tipo de iglesias ha experimentado una gran proliferación en las últimas décadas del sigloXX especialmente dentro del pentecostalismo, donde el énfasis en revelaciones divinas hace que muchas personas decidan tener su propio ministerio, empezando sus propias congregaciones de forma independiente, es decir, sin vínculo institucional.

Las iglesias independientes suelen mantenerse a nivel local como un solo grupo, aunque experimenten un gran crecimiento numérico, lo cual ha dado lugar a diversas “megaiglesias”, que son iglesias locales con miles de miembros. Aun así, existen iglesias independientes que se subdividen hasta el punto de convertirse ellas mismas en denominaciones, además de otras que se unen en fraternidades de iglesias que acaban transformándose también en denominaciones.








El país con mayor número de protestantes es Estados Unidos, donde pese a la pérdida de peso de los “WASP”, tradicionalmente a favor de otros grupos (especialmente los hispanos, de mayoría católica), la mayor parte de los estadounidenses pertenece a alguna confesión protestante. Entre los de mayor población, el que tiene el mayor porcentaje es el Reino Unido, cuyas confesiones mayoritarias son la Iglesia de Inglaterra (anglicana) y la Iglesia de Escocia (presbiteriana).

Los principales grupos protestantes comenzaron a establecerse en América Latina en el sigloXX. Los presbiterianos se instalaron en Argentina en 1836, en Brasil en 1859, en México en 1872 y en Guatemala en 1882. Los metodistas siguen un itinerario parecido: México en 1871, Brasil en 1886, Antillas en 1890, Costa Rica, Panamá y Bolivia en los últimos años del siglo; mientras que en Ecuador, Colombia y Perú se establecieron primeramente los bautistas y los pentecostales, así como una parte de los metodistas.

Actualmente las comunidades protestantes han ido ganando terreno frente al catolicismo en América Latina en general, ampliando su penetración en diversos países, en especial en Colombia y Centroamérica.


</doc>
<doc id="15456" url="https://es.wikipedia.org/wiki?curid=15456" title="Henri Léon Lebesgue">
Henri Léon Lebesgue

Henri Léon Lebesgue (; Beauvais, 28 de junio de 1875 - París, 26 de julio de 1941) fue un matemático francés.

Nació en Beauvais, Oise, Picardie, Francia. Estudió en la Escuela Normal Superior y en el período 1899 - 1902 impartió clases en el Liceo de Nancy. En 1910 recibió una cátedra en la Universidad de la Sorbona.

Lebesgue es fundamentalmente conocido por sus aportes a la teoría de la medida y de la integral. A partir de trabajos de otros matemáticos como Émile Borel y Camille Jordan, Lebesgue realizó importantes contribuciones a la teoría de la medida en 1901. Al año siguiente, en su disertación "Intégrale, longueur, aire" ("Integral, longitud, área") presentada en la Universidad de Nancy, definió la integral de Lebesgue, que generaliza la noción de la integral de Riemann extendiendo el concepto de área bajo una curva para incluir funciones discontinuas. Este es uno de los logros del análisis moderno que expande el alcance del análisis de Fourier.

También aportó en ramas como la topología, la teoría del potencial y el análisis de Fourier. En 1905 presentó una discusión sobre las condiciones que Lipschitz y Jordan habían utilizado para asegurar que "f(x)" es la suma de su serie de Fourier.

A partir de 1910 no se concentró más en el área de estudio que él había iniciado, debido a que su trabajo era una generalización, y él era temeroso de las mismas. En sus palabras: "Reducida a teorías generales, las matemáticas serían una forma hermosa sin contenido. Morirían rápidamente." A pesar de que desarrollos posteriores demostraron que su temor no tenía fundamentos, éste nos permite entender el curso que siguió su trabajo.

Además de aproximadamente 50 artículos, escribió dos libros: "Leçons sur l'intégration et la recherché des fonctions primitives" (1904) y "Leçons sur les séries trigonométriques" (1906).

Además de los distintos conceptos matemáticos que llevan su nombre, se tiene que:


Biografía


</doc>
<doc id="15458" url="https://es.wikipedia.org/wiki?curid=15458" title="Organización territorial de Cuba">
Organización territorial de Cuba

El territorio de la República de Cuba se divide en 15 provincias y el Municipio Especial Isla de la Juventud. Las provincias, a su vez, se dividen en municipios, 168 en total (incluyendo el Municipio Especial).

La creación de nuevas unidades territoriales y sus límites es materia de ley. Los cambios más recientes fueron aprobados por la Asamblea Nacional en agosto en 2010 (puestos en vigor a partir del 1 de enero de 2011), los cuales consistieron en la creación de dos nuevas provincias: Artemisa y Mayabeque a partir de la segmentación de la Provincia de La Habana, junto con el traspaso de los tres municipios más orientales de la provincia de Pinar del Río. También se extinguió el municipio de Varadero en la Provincia de Matanzas, integrando su territorio al municipio de Cárdenas. La anterior organización en 14 provincias y 169 municipios databa de 1976.

La capital del país es la ciudad de La Habana, que constituye una provincia, cuyo nombre oficial de 1976 a 2010 fue Ciudad de La Habana.

Las provincias y municipios cuentan con personalidad jurídica para todos los efectos de la ley, y los municipios gozan además de autonomía. Tienen sus propios sistemas de gobierno y estructuras administrativas para cumplir las funciones ejecutivo-administrativas, aunque con una estrecha dependencia de las autoridades centrales, al ser un Estado unitario.

En cada localidad existe además un Comité del Partido Comunista de Cuba, el que como fuerza dirigente de la sociedad y el Estado cubano, ejerce una función de fiscalización y control de las políticas trazadas en los aspectos sociales y económicos. Los primeros secretarios provinciales y municipales comparten junto a los principales dirigentes locales del Poder Popular una dirección colectiva y organizada, siendo esta la política del actual gobierno, para fortalecer el papel de las estructuras de gobierno; que en períodos anteriores se veía ensombrecida por las estructuras partidistas. .

El municipio es la sociedad local, organizada por la ley, que consntituye la unidad política-administrativa primaria y fundamental de la organización nacional; goza de autonomía y personalidad jurídica propias a todos los efectos legales, con una extensión territorial determinada por necesarias relaciones de vencindad, económicas y sociales de su población e intereses de la nación, con el propósito de lograr la satisfacción de las necesidades locales. Cuenta con ingresos propios y las asignaciones que recibe del Gobierno de la República en finción del desarrollo económico y social de su territorio y otros fines del Estado. 

La autonomía del municipio comprende la elección o designación de sus autoridades, la facultad de decidir sobre la utilización de sus recursos y el ejercicio de las competencias que le corresponden, así como dictar acuerdos y disposiciones normativas necesarias para el ejercicio de las facultades. Esta autonomía se ejerce de conformidad con los principios de solidaridad, coordinación y colaboración con el resto de los territorios del país, y sin detrimento de los intereses superiores de la nación. 

La Asamblea Municipal del Poder Popular es el máximo órgano representativo municipal. Está integrada por delegados elegidos en cada circunscripción electoral a partir de candidatos propuestos en asambleas populares. La duración del mandato de sus delegados, desde la Constitución del 2019, es de 5 años; anteriormente era de dos años y medio. La Asamblea elige a su presidente, a su vicepresidente y designa a su secretario y a las comisiones de trabajo. El Presidente de la Asamblea Municipal del Poder Popular representa al Estado en su demarcación territorial; existe una nueva ley de orgaización de las AMPP desde el año 2019, que establece con claridad las funciones y atribuciones de este órgano y en particular las fucniones de cada miembro de la mesa directiva. 

La Administración Municipal tiene como objetivo satisfacer las necesidades de la economía, de la salud, asistenciales, educacionales, culturales, deportivas y recreativas del territorio; así como ejecutar las tareas relativas a la prevención y atención social. Las funciones ejecutivo-administrativas es desempeñado por el Consejo de Administración Municipal, el cual es designado por la Asamblea a la que se subordina y rinde cuenta; presidido por el Intendente Municipal, con carácter colegiado y dirige las actividades de gobierno a nivel local. 

Los municipios se dividen, para facilitar las relaciones con los electores, en consejos populares integrados por los propios delegados y presididos por uno de ellos. Los municipios de Santa Clara, Camagüey, Holguín y Santiago de Cuba (municipios de gran población), se dividen además en distritos, donde se agrupan varios consejos populares, con el fin de descentralizar y acercar a la población las diferentes oficinas administrativas.

La provincia tiene personalidad jurídica propia a todos los efectos legales y se organiza por la ley como nivel intermedio entre las estructuras centrales del Estado y los municipios, con una extensión superficial equivalente a la del conjunto de municipios comprendidos en su demarcación territorial, bajo la dirección del Gobierno Provincial del Poder Popular. 

El Gobierno Provincial del Poder Popular representa al Estado y tiene como misión el desarrollo económico y social de su territorio, conforme a los objetivos generales del país y actúa como coordinador entre las estructuras centrales y locales. Estos órganos funcionan en estrecha vinculación con el pueblo, y esta conformado por un Gobernador, un Vicegobernador y un Consejo Provincial. Los Gobiernos Provinciales en el ejercicio de sus funciones y atribuciones no pueden asumir ni interferir en las conferidad a los órganos municipales. 

El Gobernador es el máximo responsable ejecutivo-adminitrativo de la provincia, es elegido por los delegados de las asambleas municipales del Poper Popular correspondiente, en un mismo día y a una misma hora formando colegios electorales especiales a propuesta del Presidente de la República, por un perído de cinco años. Para ser Gobernador se requiere ser ciudadano cubano por nacimiento, no tener otra ciudadanía, haber cumplido 30 años, residir en la provincia y hallarse en pleno goce de los derechos civiles y políticos. Es responsable ante la Asamblea Nacional del Poder Popular, el Consejo de Estado, Consejo de Ministros y Consejo Provincial, a los cuales rinde cuentas de su gestión. Además dirige la Administración Provincial para lo cual se asiste de la entidad administrativa correspondiente. 

El Vicegobernador es elegido de la misma forma y el mismo día que el Gobernador, por igual período; cumple las atribuciones que le delegue o asigne el Gobernador y los sustituye en caso de enfermedad o muerte, conforme los procediminetos previstos en la ley. 

El Consejo Provincial, es un órgano colegiado y deliberativo, presidido por el Gobernador e integrado por el Vicegobernador, los Presidentes y Vicepresidentes de las asambleas locales del Poder Popular y los Intendentes Municipales correspondientes. Cumple con funciones previamente establecidas por ley que se enmarcan en evaluar, controlar, orientar y coordinar la labor de los órganos municipales de estado y gobierno, en función de la provincia y los intereses de la nación.

Los órganos (Asamblea y Administración) del Municipio Especial Isla de la Juventud son los mismos de cualquier municipio pero en estos coinciden competencias municipales y provinciales.



<nowiki>*:</nowiki> Después de la promulgación de la nueva Constitución y la nueva Ley Electoral se eligen las nuevas estructuras por los que resta del mandato que comenzó en el 2018. 


</doc>
<doc id="15460" url="https://es.wikipedia.org/wiki?curid=15460" title="Célula endotelial">
Célula endotelial

Una célula endotelial es un tipo de célula aplanada que recubre el interior de los vasos sanguíneos y sobre todo de los capilares, formando parte de su pared.

El núcleo de las células endoteliales está muy aplanado y por eso aparece elíptico en los cortes visualizados al microscopio. La región nuclear y más gruesa de la célula hace prominencia en la luz. La porción periférica y más delgada de la célula es tremendamente fina, y las membranas que miran a la luz o al tejido están separadas por una capa de citoplasma de un grosor de 0,2 a 0,4 micras. 

Hay en la región cercana al núcleo un complejo de Golgi y unas pocas mitocondrias, mientras que en la región delgada periférica del citoplasma hay elementos tubulares tortuosos del retículo endoplásmico. Son raros los lisosomas, pero no son infrecuentes los cuerpos multivesiculares. 

Un rasgo llamativo de las células endoteliales es la presencia de una numerosa población de vesículas del plasmalema de unos 70 nanómetros de diámetro, de cuello delgado, que están presentes en ambas superficies celulares y que se abren a la luz y al espacio extravascular. 

La superficie luminal de las células es normalmente de perfil liso, pero a menudo los bordes de las células vecinas pueden superponerse y entonces, puede proyectarse hacia la luz por corta distancia una cresta o lengüeta. Faltan de ordinario los desmosomas y la "zonula adherens", pero hay una unión ocluyente de pequeño tamaño que en las preparaciones de criofractura muestra de uno a tres cordones intramembranosos paralelos en la cara E.

En la superficie extraluminal o externa, las células endoteliales están en contancto con la membrana basal y sustancias como colágeno, proteglicanos, heparánsulfato, integrinas; en la parte luminal las células endoteliales en contacto con la sangre poseen mucopolisacáridos, glucoproteínas, fibrinógeno y algo de fibrina.

En el cuerpo humano, el conjunto del endotelio vascular puede pesar unos 1,5 kilogramos y comprende un área de unos 600 metros cuadrados.

Las células endoteliales forman el endotelio vascular que es un epitelio plano simple (de una sola capa de células) que recubre la cara interna de los vasos sanguíneos y el corazón. Las células endoteliales tienen varias funciones en la homeostasis, entre las que figuran las siguientes: 


</doc>
<doc id="15461" url="https://es.wikipedia.org/wiki?curid=15461" title="Sistema educativo de Chile">
Sistema educativo de Chile

La educación en Chile se divide en cuatro fases —parvularia, básica, media y superior—, de los cuales la básica y media son obligatorios. La educación chilena está regida por la Ley General de Educación (LGE) de 2009, sucesora de la Ley Orgánica Constitucional de Enseñanza (LOCE).

Los niveles parvulario, básico y medio del sistema de gobierno —así como los centros de formación técnica de gobierno superior— están regulados y vigilados por el Ministerio de Educación. El Consejo Superior de Educación (CSE) tiene como principales funciones pronunciarse sobre la solicitud de reconocimiento oficial de las universidades e institutos profesionales, verificar su desarrollo, establecer sistemas de examen selectiva y acreditación, recomendar sanciones y realizar estudios sobre la educación superior.

Los derechos a la educación y a la libertad de enseñanza están resguardados en la Constitución Política de la República; sin embargo, para tener reconocimiento legal, los establecimientos particulares deben cumplir con los objetivos fundamentales y contenidos mínimos obligatorios (OF-CMO), prescritos por los artículos 15 a 20 de la LOCE. Dichos requisitos y normas son establecidas por el Ministerio de Educación previo informe del CSE.

En el año 2013 se reportó que más de 800 escuelas municipales cerraron y apenas el 36 % de los alumnos está inscrito en colegios públicos.Y en las últimas décadas, ha habido dos grandes olas de manifestaciones relacionadas con el manejo de la educación en el país: en 2006 y 2011 —esta última se vio inmersa en un año de profunda y activa protesta social en el país en distintos ámbitos—. Sin embargo, según ha sido informado por el Centro Interuniversitario de Desarrollo, en 2016 Chile tiene la tasa de escolarización en educación superior más alta de Latinoamérica equiparable a países como Australia y superior a Estonia o Gran Bretaña, además de figurar como el país con la mayor cobertura del quintil más pobre, y un nivel de desigualdad reducido comparado a países como Uruguay o Brasil.

En la Colonia, la educación estuvo a cargo principalmente de la Iglesia, especialmente las congregaciones religiosas establecidas en el país, destacando los jesuitas y dominicos. En el nivel primario el enfoque estaba en la enseñanza de la escritura y lectura, más algunas lecciones de catecismo y aritmética. También en este periodo las órdenes de los mercedarios y franciscanos formaron escuelas en Concepción, Osorno, La Imperial y Valdivia. Asimismo, debido a la necesidad de convertir a los indígenas a la fe católica, se abrió en Penco un curso de lengua araucana, pero no duró por la escasez de alumnos. También se mandó a hacer una escuela donde los Mapuches aprendiesen castellano, el Colegio de Naturales de Chillán (1697).

En cuanto a la educación superior, en el siglo XVII funcionaron en Chile tres centros de enseñanza superior con categoría de universidades pontificias, que tenían un carácter eminentemente eclesiástico: el Colegio Máximo San Miguel de los jesuitas y la Universidad de Santo Tomás de Aquino de los dominicos, ambas en Santiago; mientras que en Concepción funcionó durante 43 años la Universidad Pencopolitana dirigida por los jesuitas. En 1758, por autorización de Felipe V, se erige la Universidad de San Felipe, antecesora de la actual Universidad de Chile.

Con el advenimiento de la Independencia, la educación se transforma en un tema de interés para los dirigentes patriotas. En 1812 se establece la Biblioteca Nacional y en 1813 se crea el Instituto Nacional. Se establece la obligatoriedad de mantener escuelas de primeras letras por el Estado en los diversos textos constitucionales.

Una vez afianzada la organización del estado, se suceden diversos hitos en materia educacional. Así, en 1842 la U. de San Felipe es convertida en la Universidad de Chile, y se crea la Escuela Normal de Preceptores, la primera en su tipo en Sudamérica. Asimismo, para el desarrollo de la educación técnica, en 1848 se establece la Escuela de Artes y Oficios. En 1860 se establece la Ley Orgánica de Instrucción Primaria, y en 1877 el decreto Amunátegui permite la incorporación de la mujer a la educación superior.

Durante el siglo XIX convivieron los sistemas público y privado de educación, este último dominado por la Iglesia Católica. Con el advenimiento de la República Liberal, surgen en el marco de las "Cuestiones Teológicas" debates sobre la vigilancia estatal en la educación versus la libertad de enseñanza defendida por los colegios confesionales. Uno de los hitos más importantes de este enfrentamiento fue la creación, en 1888, de la Universidad Católica de Chile, primer centro privado de estudios superiores del país. Por otro lado, para la formación de profesores de enseñanza secundaria, se crea el Instituto Pedagógico en 1889.

El siglo XX fue generoso tanto en la creación de establecimientos como en las reformas impulsadas. En cuanto a lo primero, en 1921 se establece la primera universidad fuera de Santiago, la Universidad de Concepción (aunque existieron cursos universitarios sueltos previamente en Valparaíso). Asimismo, se crean otras cuatro universidades privadas en regiones: Católica de Valparaíso (1928), Federico Santa María (1931), Austral de Valdivia (1954) y Católica del Norte (1956). Mientras, en 1947 la Escuela de Artes y Oficios, junto a otros institutos técnicos públicos, se refunden y crean la Universidad Técnica del Estado. Todas las mencionadas anteriormente, además de las universidades de Chile y Católica de Santiago conforman el Consejo de Rectores de las Universidades Chilenas en 1954. Por otro lado, la Universidad de Chile se expandiría por el país creando varias sedes regionales, lo que sería imitado por la UTE y la Católica.
En cuanto a las reformas educacionales, en 1920 la Ley de Instrucción Primaria Obligatoria estableció como nivel mínimo de educación el 3.° año de preparatoria. En 1945 se hicieron planes de reforma parcial de la educación escolar, y en 1953 se crea la Superintendencia de Educación y la JUNAEB. Con todo, en 1965 se inicia bajo el gobierno de Eduardo Frei Montalva una profunda reforma educacional en la que se redujo de 6 a 4 años la enseñanza secundaria, llamada "de humanidades," que pasaría a llamarse "educación media", mientras que la educación preparatoria, que se renombró "educación básica", pasó de 6 a 8 años, que serían obligatorios, y se dividen las jornadas en dos. Por otro lado, las universidades pasarían por el período de la "reforma universitaria", en que se pretendió otorgar un cariz más democrático al gobierno de estas instituciones.

Tras el golpe de estado de 1973, se vivió un proceso de retroceso en materia de democracia educacional, a la vez que se descentralizó o liberalizó algunos aspectos de la educación. En 1974 empieza un proceso que elimina las escuelas normales y traspasa la formación de profesores de enseñanza básica a las universidades. En 1981 las sedes de las universidades de Chile y Técnica del Estado fueron convertidas en universidades regionales, o en otras universidades metropolitanas, y se autorizó la creación de universidades privadas al margen del Consejo de Rectores. Asimismo, los establecimientos escolares son trasferidos desde el Estado a las Municipalidades ("municipalización"). En las postrimerías del régimen, se dicta la Ley Orgánica Constitucional de Enseñanza, que señala las pautas para la educación chilena desde los niveles pre-escolares, hasta la educación superior. Reconoce el derecho a la Educación y la libertad de enseñanza, además fija los requisitos mínimos y objetivos básicos que se deben cumplir.

Tras el retorno a la democracia, la educación vive una serie de cambios curriculares, amén del aumento de las demandas por acceso y calidad de la educación. En 1992 se establece el Estatuto Docente y en 1996 se inicia el programa de Jornada Educacional Completa, con importantes cambios en los contenidos curriculares.

La Revolución Pingüina, producida entre mayo y junio de 2006, coloca a la educación como un tema central de la política y la sociedad chilena. Eso lleva, entre otros, al reemplazo de la LOC de Enseñanza por la Ley General de Educación, que contempla modificaciones importantes en los procesos de admisión, currículum, y reconocimiento oficial de los establecimientos educacionales. Asimismo, se han producido eventos que generan gran impacto en la opinión pública, como el cierre forzoso de la Universidad ARCIS, la Universidad del Mar, la Universidad del Pacífico, Universidades desaparecidas de Chile, y los casos de corrupción que afectaron a los miembros de la Comisión Nacional de Acreditación.

La educación parvularia atiende a la población de niños y niñas entre los 6 meses y los 6 años. Pasó a ser obligatoria el 21 de mayo de 2013, cuando el presidente Sebastián Piñera anunció la aprobación del proyecto de ley que estipulaba la obligatoriedad del kínder, dejando así el prekínder no obligatorio.

La atención parvularia se realiza a través de las sala cunas y jardines infantiles de administración municipal, particular subvencionada, particular, de JUNJI o de la Fundación Nacional de Atención al Menor (Fundación INTEGRA). La educación parvularia está dividida en los siguientes niveles:


El 21 de mayo de 2013, se anunció una reforma constitucional para establecer el segundo nivel de transición (kínder) como obligatorio a partir de 2015, convirtiéndose en requisito para cursar el nivel básico, llegando así a 13 años de educación garantizada.

En 2015 se promulgó una ley que crea un nuevo esquema rector del nivel preescolar, el cual a agosto de 2016 está en fase de implementación.

La Enseñanza Básica desde la reforma de 1965, corresponde al ciclo inicial de estudios escolares. En 1920 la legislación chilena había establecido la obligatoriedad de cursar 4 años de escolaridad mínima. En 1929 este mínimo es aumentado a 6 años. Finalmente, en 1965 se establece la obligatoriedad del nivel básico, cuya duración actual es de 8 años divididos en 2 ciclos y 8 grados (de 6 a 14 años de edad ideal).


La Ley General de Educación de 2009 contempla el cambio a una educación básica de 6 años y la educación media también de seis años, con una renovada estructura curricular. El cambio se efectuará a contar de 2026.

La Enseñanza Media está dividida en Enseñanza Media Científico-Humanista (EMCH), Técnico-Profesional (EMTP), y Artística (desde 2006), con una duración de 4 años.

En el año 2003 el expresidente Ricardo Lagos junto a la ministra de educación otorgaron de carácter obligatoria esta etapa a través de la ley N°19.876.

La Enseñanza Media se organiza como sigue:

Los liceos o colegios que imparten especialidades técnico-profesionales otorgan Títulos de Técnico de Nivel Medio y se les denomina:

En la educación superior de Chile se distinguen cuatro tipos de establecimientos, creados por la reforma de la educación superior 1981. A ellos pueden optar todos los egresados de la educación media:

Quienes ingresan a la educación superior universitaria pueden optar entre universidades tradicionales, que fueron creadas antes de 1981 y que están agrupadas en el Consejo de Rectores de las Universidades Chilenas (CRUCH) o en las universidades "privadas". Las primeras reciben varios tipos de fondos del Estado, como el aporte fiscal directo (AFD) y el aporte fiscal indirecto (AFI). Según la legislación vigente, a todas las universidades chilenas se les considera como organizaciones sin fines de lucro, aunque solo desde el año 2011 se discute un mecanismo de fiscalización de dicha situación.

Las instituciones del CRUCH son 25 y la selección para el ingreso a estas universidades se efectúa a través de la Prueba de Selección Universitaria (PSU), la cual mide los conocimientos de los estudiantes en materias que son parte de los contenidos esperados para el nivel de educación secundaria. Esta prueba permite un sistema integrado, simultáneo y coordinado entre las diversas instituciones. Es controlado actualmente por la Universidad de Chile.

Durante 2011, el CRUCH invitó a universidades privadas a sumarse al sistema de ingreso, de las cuales se sumaron 8.

La Educación es gratuita en su duración formal para el 50 % de la población más vulnerable y que haya elegido una universidad o instituto público y que no tenga ánimo de lucro. En cuanto a las universidades privadas o con ánimo de lucro, el estudiante puede optar a diferentes becas o créditos con condiciones muy diferentes entre sí para el financiamiento de sus estudios. Esto ha provocado críticas al sistema por parte de estudiantes y egresados. Los estudiantes de universidades del CRUCH pueden postular al Fondo Solidario de Crédito Universitario y los demás estudiantes de educación superior solo al Crédito Con Garantía Del Estado (o Crédito con Aval del Estado, CAE). En 2011 se creó una comisión que estudiaría mecanismos para mejorar el financiamiento de la educación superior, la que fue compuesta por 12 académicos.
El arancel promedio de una universidad chilena es de $3 565 645 pesos chilenos. Esto corresponde al 132 % del sueldo mínimo anual, y a 93 % en el caso del "arancel de referencia", que es el utilizado para calcular el monto de crédito otorgado para el CAE.

Desde 2004 se inició un proceso de acreditación de la calidad de la educación superior mediante la Comisión Nacional de Acreditación de Pregrado (CNAP).
Por la Ley N° 20.129 de 2006 se establece un sistema nacional de aseguramiento de la calidad de la educación superior (CFT, IP y universidades) a cargo de la Comisión Nacional de Acreditación (CNA-Chile). Sus principales objetivos son la acreditación institucional y acreditación de carreras y programas de la educación superior. El sistema es voluntario para las instituciones por lo que su impacto es acotado.

La cobertura del sistema educacional chileno es prácticamente universal, como ocurre en países desarrollados, teniendo índices de matrícula que representan esa realidad. La matrícula en Educación Básica (EGB) alcanza al 99,7 % de los niños entre 7 y 14 años. En el caso de la Educación Media la cobertura de la matrícula es de 87,7 %, de los adolescentes entre 15 y 18 años.

Antiguamente, la obligatoriedad escolar abarcaba solo el Ciclo Básico (EGB) de 8 años. Pero, a partir del 7 de mayo del 2003, una reforma constitucional, bajo el gobierno del presidente Ricardo Lagos, estableció la Educación Media gratuita y obligatoria para todos los chilenos hasta los 18 años de edad, entregando al Estado la responsabilidad de garantizar el acceso a ella.
También se distinguen modalidades especiales de la educación básica y media como la educación de adultos y la especial (educación diferencial).

Con la entrada en vigencia de la Ley General de Educación de 2009, se reemplazó el Consejo Superior de Educación por el Consejo Nacional de Educación. Adicionalmente, la Ley de Aseguramiento de la Calidad de la Educación de 2011 separa funciones del ministerio en tres organismos, para lo cual crea dos nuevas instituciones reguladoras del sistema escolar, la Superintendencia de Educación y la Agencia de Calidad de la educación, las que entraron en operación en el segundo semestre de 2012.

En 2017, la Ley N° 21.040 creó un nuevo Sistema de Educación Pública, transfiriendo los establecimientos educacionales de los 345 municipios del país a 70 nuevos Servicios Locales de Educación Pública (SLE). Los SLE serán constituirán servicios públicos descentralizados cuyo objeto único será la provisión del servicio educacional en sus respectivos territorios de competencia, y se conformarán mediante un proceso gradual que durará hasta el año 2025.

Se ha mencionado la creación de equivalentes para el nivel superior, pero a agosto de 2016 estos proyectos no se han concretado.

En el índice de Desarrollo Humano de las Naciones Unidas (Educación), Chile (0.847) está en el puesto número 1 en Latinoamérica. Los países con mejores índices compuestos de Educación (alfabetización, gasto en educación, tasa bruta de matriculación, usuarios de internet por cada 100 personas, años de educación promedio, años esperados de instrucción) en América Latina son Argentina (0.764), y Uruguay (0.731).

En el Informe PISA del año 2013, los estudiantes chilenos lograron el puntaje promedio más alto de los países latinoamericanos, posicionándose en el puesto 52 de 66 países que participaron de la medición. aunque ha descendido desde el puesto 44.Chile se coloca en el puesto número 51 con 423 puntos en matemáticas, por debajo de la media fijada por PISA (de 494), mientras que en lectura obtiene 441 y en ciencia 445. En Lenguaje se obtuvo 441 puntos contra 449 del año 2009 y 494 del promedio OCDE; mientras que en Ciencias se obtuvo 445 bajando dos puntos de la medición anterior.
La Prueba Pisa mide a 66 países, a todos los pertenecientes a la organización OCDE más diferentes países de América Latina, Asia y Europa.

En el ámbito universitario y basándose en la clasificación internacional elaborada por la Universidad Jiao Tong de Shanghái en China, la Universidad de Chile y la Pontificia Universidad Católica de Chile están entre las mejores 500 del mundo. Por otro lado, en el año 2013 el ranking QS World University incluyó a nueve universidades chilenas dentro de las 800 mejores del mundo, entre las que se encontraban, además de las anteriores, la Universidad de Santiago de Chile, Universidad de Concepción, Pontificia Universidad Católica de Valparaíso, Universidad Adolfo Ibáñez, Universidad Austral de Chile, Universidad de Talca y la Universidad Técnica Federico Santa María. A nivel investigativo, y de acuerdo al ranking SIR Global 2013, aparecen 17 universidades chilenas lideradas por la Universidad de Chile, seguida de la Pontificia Universidad Católica de Chile, Universidad de Concepción, Pontificia Universidad Católica de Valparaíso y Universidad Austral de Chile, entre otras.

El actual sistema educativo tiene su origen en las reformas de 1980 que significó el traspaso de la educación pública a la administración municipal, permitiendo la competencia de las escuelas municipales y particulares que recibían subvención escolar del estado, además se permitió que privados crearan universidades, institutos profesionales y centros de formación técnica. Esta institucionalidad profundizó la segmentación social, característica histórica de la educación chilena. 

Antes de la dictadura militar había tres instancias escolares; los estudiantes de clase alta asistían a colegios privados pagados, los de clase media estudiaban preferentemente en liceos públicos y los de extracción popular cursaban solo algunos años de enseñanza básica en las escuelas primarias públicas o privadas subvencionadas. Las universidades, eran gratuitas, estaban reservadas para una pequeña parte de la población que lograba aprobar los exigentes requisitos de acceso y cuyas familias podían seguir manteniéndola, en promedio, solo el 7 % de los jóvenes en edad universitaria estudiaban en la educación superior, es decir 55 000 estudiantes.

La reforma de los años ochenta reforzó la estratificación social, por la diferenciación que se indujo entre escuelas privadas subvencionadas y municipales. A las primeras se les permitió operar como entidades que podían obtener utilidades, a lo que sumó la oferta tradicional de escuelas religiosas con nuevos establecimientos privados. Muchos de ellos buscaron atraer a las familias de clase media ofreciéndoles un ambiente social más homogéneo y símbolos de distinción, infraestructuras deportivas y similares. Los más exitosos pusieron en marcha exámenes de admisión para seleccionar a los estudiantes con mayor capacidad de aprendizaje. De este modo, el sector privado subvencionado empezó a congregar a alumnos de clase media y las escuelas municipales se fueron quedando a cargo de aquellos estudiantes de menor condición socioeconómica y con mayores dificultades para sus aprendizajes.

Llegada la democracia se desarrollaron una serie de iniciativas destinadas a implementar la equidad, igualdad y calidad educativa; ello con alta inversión en infraestructura, proyectos de mejoramiento educativo, jornada escolar completa, etcétera; ha pasado el tiempo y en estos últimos años se ha optado por una reforma educacional de envergadura, destinada a modificar la estructura del sistema vigente. Por eso, a fines del 2016 se inició el proceso legislativo del término de la municipalización y su reemplazo por servicios locales financiados centralmente. También, la ley de inclusión, promulgada en 2015, prohibió la selección en los establecimientos con financiamiento público, poniendo fin al copago en la educación particular subvencionada y prohibió el lucro de estos establecimientos, ello a través de una implementación gradual y que comienza plenamente este año.

A lo largo del período hubo también cambios fundamentales en la institucionalidad de la educación con la creación de organismos para la calidad y la regulación del sector, poniéndose en marcha un sistema de carrera docente, se reformó el currículo, se creó la Subsecretaría de Educación Parvularia y se multiplicó la dotación de establecimientos de ese nivel de enseñanza. La generalidad de estas iniciativas se relacionan con la enseñanza básica y media, claro es, que aún se cuestiona la calidad. Mientras tanto, la educación superior estuvo prácticamente desprovista de atención por parte de la política pública, salvo lo referido al sistema de crédito estudiantil. 

En estos últimos años, se ha verificado como nunca antes en la historia de Chile, una expansión de la matrícula en la enseñanza superior, pero bajo un marco desregulado, sin instancias de coordinación ni resguardos de la calidad de los estudios. La situación empezó a cambiar después de las masivas movilizaciones de protesta del 2011. En el segundo gobierno de Michelle Bachelet se ha impulsado una reforma del sector que incluye la instalación gradual de la gratuidad de la enseñanza, una Subsecretaría de Educación Superior y la creación de más universidades estatales, entre otras iniciativas que deberá afinar, modificar e impulsar el gobierno de Sebastián Piñera. 

Cabe preguntarse, si todas las iniciativas en educación logran o no el objetivo central, que es aportar al crecimiento económico con recursos humanos más calificados y productivos, para igualar oportunidades y reducir las brechas de aprendizaje entre los jóvenes, sirviendo además las demandas sociales. 

El aporte de la educación superior a la movilidad social es un tema relevante si se considera que la cobertura de la educación superior ha aumentado en los últimos años y que entre el 40 % y el 50 % de los jóvenes de los estratos medios bajos y bajos está ingresando a alguna institución de educación superior. Muchos de estos jóvenes son la primera generación de sus familias que se incorporan a la educación superior. 

Pero hay desafíos, un estudio del PNUD en Chile señala que los jóvenes de los estratos medios bajos y bajos, no consiguen los mismos resultados que los jóvenes de origen familiar más acomodado. La mayoría cursa estudios superiores técnico-profesionales o bien va a universidades de menor calidad académica, presentan mayores tasas de deserción y sus ingresos laborales cuando egresan son más bajos, pero más elevados que en el caso de no haber llegado a este nivel de enseñanza. Esto último contribuye a acortar brechas; sin embargo, el número de estos estudiantes ha crecido a tasas mucho mayores que la economía en los últimos diez o quince años, por tanto, un factor crítico es la capacidad actual y futura que tiene nuestra economía de proveer los empleos esperados por los más de un millón de estudiantes que hoy está en la educación superior. Sumamos a ello el efecto que tendrá la creciente automatización de los procesos productivos en empleos. 

En relación a lo anterior, es probable que la cobertura de la educación superior en Chile esté próxima a tocar techo en términos de los empleos que puede proveer la economía para sus egresados. La evidencia internacional señala que no hay país en el mundo que aspire a que toda la población tenga estudios superiores. Es un desafío pendiente para la enseñanza media dotar de competencias a los jóvenes que no ingresarán a la educación superior, así como la instalación de una oferta de educación continua que permita la renovación de conocimientos y competencias a lo largo de la vida.

Los jóvenes de estrato bajo y medio bajo que cursan estudios superiores en centros de formación técnica e institutos profesionales obtienen ingresos que son más bajos que los obtenidos por los jóvenes de hogares acomodados quienes tienden menos a estudiar en esos centros. Ahora bien, la brecha de ingresos entre los profesionales universitarios, según su origen socioeconómico es significativa. Esta diferencia de salario entre profesionales del estrato alto y del estrato bajo se explica por la acumulación de factores a lo largo de la niñez y la juventud, que determina un acceso muy diferenciado a la educación superior en términos de la calidad de la institución y del tipo de carrera. Además, se explica por la valoración diferenciada que hacen los empleadores, especialmente cuando se trata de puestos altos. Para el caso de altos ejecutivos se prefiere a egresados de colegios privados de élite por un tema supuestamente cultural y porque tienen más redes de contacto.

Quizás, un tema no estudiado en términos académicos sea el futuro impacto que tendrán en este cuadro los “nuevos chilenos”, vale decir, los hijos de inmigrantes y los jóvenes extranjeros que se radicarán definitivamente en el país, ellos vendrán a sumarse a la demanda por educación superior y empleo, para lo cual, habrá que adecuar las políticas públicas, tanto para el acceso y financiamiento para la educación como también para la incorporación laboral de todos los jóvenes, será el desafío para el siglo XXI. El mayor acceso a la educación superior ha generado expectativas de movilidad social en los hogares chilenos, y es parte de discurso político situar la educación como centro del desarrollo del país; por eso los resultados del proceso serán muy importantes, no solo para el desarrollo económico y social de Chile sino para la inclusión que tengan las personas respecto del modelo de desarrollo vigente.

Según Francisco Ocaranza Bosio, Director Escuela de Historia y Geografía, Miembro del Programa de Doctorado en Educación de la Universidad Bernardo O’Higgins, señala que «a partir del movimiento estudiantil de 2006, el de los pingüinos, y luego con una frecuencia casi anual, el alumnado nacional, y a veces también los profesores, han manifestado a través de diversos canales y estrategias, su malestar por las condiciones del sistema en relación a aspectos como calidad, financiamiento, inclusión y otros. En 2014, de acuerdo al Ministerio de Educación, Chile contaba con cinco millones de estudiantes. El 76 % repartido entre el nivel de parvularia, básica y media, y el 24 % restante en el superior. De los 762 mil en edad preescolar, el 32 % acude a instituciones de la JUNJI o Integra, mientras que el 68 % restante a establecimientos escolares. A nivel básico hay alrededor de dos millones de alumnos, de los cuales un 53 % asiste a instituciones particulares subvencionadas, 40 % a municipales, y 7 % a particulares pagadas. En el nivel medio, el 51 % a particulares subvencionados, el 37 % a municipales, el 8 % a particulares pagados, y el 5 % a corporaciones de administración delegada».

Respecto de la educación superior, el 27.4 % de los estudiantes asisten a instituciones del CRUCH, el 31 % a universidades privadas fuera del CRUCH, el 29.4 % a institutos profesionales, y el 12.2 % a centros de formación técnica.

En materia de recursos económicos destinados a cada estudiante, Chile destina la mitad que el promedio OCDE (US$ 5092 versus 10 000), 6500 en el nivel parvulario (18 % del PIB per cápita), 4074 en el escolar (19 %), y 7600 (35 %) en el superior.

Las cifras relativas al PIB per cápita también se encuentran bajo el promedio OCDE, donde se alcanza un 22 %, 25 % y 41 % respectivamente. Sin perjuicio de esto, cabe destacar que existe un incremento del 12 % en el gasto destinado a la educación escolar, entre 2008 y 2013, y de un 5 % en educación superior en el mismo período de tiempo.

Asociado a todo esto debe tenerse en cuenta la fuente de financiamiento de la educación, que en Chile cuenta con una importante proporción procedente de la billetera del particular. Así, el gasto privado en la educación escolar asciende al 21 % (solo 9 % promedio OCDE), y 62 % la universitaria (30 % OCDE), lo que tiende a compensar lo expuesto en el párrafo anterior.

El sistema de educación, vigente desde la década de 1980, y ligeramente modificado en 1994 a través de la instalación del sistema particular subvencionado para el nivel básico y medio, trae como una de sus consecuencias más marcadas, un altísimo nivel de segregación, favoreciendo una inevitable reproducción de la estratificación social.

A este respecto es ilustrativo tomar en cuenta los resultados obtenidos en la prueba PISA de 2012, donde Chile ocupa el penúltimo lugar de la OCDE en competencia lectora mínima (68 % frente al 80 %). La diferencia en el rendimiento entre el primer y el quinto quintil es de más de 35 puntos porcentuales, mientras que el promedio OCDE es de 20.

En materia de distribución de recursos en la escuela, o disponibilidad de los mismos en relación a la condición socioeconómica, Chile presenta un ratio de concentración superior a 30 %, existiendo quince países OCDE con rangos que van del 0 % al 10 %, y otros cinco con índice negativo, debido a sus políticas de igualdad de oportunidades a partir de las cuales las familias de menores ingresos reciben mayores recursos con un fin de igualación.

Finalmente cabe considerar la segregación social existente entre escuelas, de acuerdo al índice de Duncan (0 es integración total y 1 segregación absoluta), donde Chile ocupa la última posición del listado.




</doc>
<doc id="15462" url="https://es.wikipedia.org/wiki?curid=15462" title="Lábaro cántabro">
Lábaro cántabro

Lábaro cántabro, también conocido como lábaru cántabru o simplemente lábaro o lábaru, es el nombre que recibe la interpretación moderna y contemporánea de un antiguo estandarte militar conocido por los romanos como cantabrum. En la actualidad, es representado habitualmente como un pendón de tela de color magenta sobre el cual está bordado un círculo rodeado de una decoración geométrica con cuatro medias lunas enfrentadas dos a dos que combina el estandarte militar con la simbología de las estelas cántabras.

En las últimas décadas su uso se ha popularizado dentro de la Comunidad Autónoma de Cantabria siendo en la actualidad muy visible especialmente en eventos deportivos y fiestas regionales. En el año 2016 el Parlamento de Cantabria lo reconoció como "símbolo identitario del pueblo cántabro", aunque sin sustituir a la bandera oficial de Cantabria.

El origen del nombre y del diseño se encuentra en la teoría defendida por diversos autores de una posible relación entre la génesis del labarum y el estandarte militar denominado "cantabrum", con la consiguiente identificación de ambos como una misma cosa; y a la supuesta relación que el Codex Theodosianus establece entre el "labarum" y los cantabrarii, colegio de soldados romanos encargados de portar el "cantabrum".

Su significado etimológico, "el que habla", hace referencia a su uso como estandarte utilizado para enviar órdenes o señales a la tropa durante la batalla.

Los relatos de Tertuliano y Minucio Félix no establecen relación alguna entre el "cantabrum" y el "labarum", dejando únicamente clara la veneración que las tropas romanas hacían de sus cruces, cubiertas por las telas de los "cantabra" y "vexilia":

Según estas teorías, el "cantabrum" es el estandarte que Constantino I el Grande tras su conversión al cristianismo transforma en el "labarum" al incluir el crismón, anagrama que representa a Cristo, consistente en las grafías mayúsculas en griego de las dos primeras letras de su nombre, una "X" sobre la que se superpone una "P".

Se justifica también la relación en la etimología celta del término lábaro procedente de "(p) lab-" hablar, de donde se ha derivado el adjetivo "labaros", orador, ampliamente representado en las lenguas celtas. Galés: "llafar", habla, idioma, voz, orador; antiguo córnico y bretón: "lavar" palabra; antiguo irlandés: "labar" charlatán, "labrad" habla, lenguaje; irlandés: "labhar" locuaz, en voz alta y "labhairt" palabra, habla < célt. "(p) labro-". En latín "Labarum".

Asimismo, el antropónimo "Labaro" ya existía entre los antiguos cántabros, habiendo sido recogido en lápidas funerarias.

El diseño actual, siguiendo igualmente la teoría de ser el "labarum" lo mismo que el "cantabrum", establece para el lábaro cántabro el color rojo púrpura del labarum.

El tetrasquel dorado representa las cuatro crecientes lunares que aparecen representadas en varias estelas cántabras discoideas gigantes. Siendo un símbolo que se ha constatado que usaban los cántabros frecuentemente, como se observa en caetras representadas en monedas acuñadas tras las guerras cántabras.

Además este tipo de estandartes y sus variantes estaban bastante extendidos entre los pueblos célticos, como lo demuestran los relieves del arco de triunfo de Orange. Su diseño entronca con antiguos símbolos celtas como el trisquel y su simbolismo, de tipo religioso, se relaciona con el culto al Sol y a la Luna.

El Pleno del Parlamento de Cantabria, en su sesión del 14 de marzo de 2016, aprobó una resolución como consecuencia de la tramitación de la proposición no de ley, N.º 9L/4300-0056, relativa al reconocimiento del lábaro como símbolo representativo e identitario del pueblo cántabro y los valores que representa.

La interpretación moderna del lábaro cántabro y su posible uso como símbolo oficial o cooficial de la Cantabria actual ha surgido como debate en el seno de esta Comunidad Autónoma, desatando un conjunto de enraizadas disputas dialécticas difundidas en muchos casos a través de los medios de comunicación.

En este diálogo mediático las posturas que más voz ostentan provienen de la Asociación para la Defensa de los Intereses de Cantabria (ADIC), por un lado, y de investigadores que intervinieron en la creación de los símbolos de la actual Comunidad Autónoma enmarcados dentro del Centro de Estudios Montañeses, por el otro.

Desde ciertos colectivos cántabros tanto sociales como políticos, se ha venido reivindicando el uso oficial del estandarte aureomagenta como bandera de Cantabria en representación del legítimo cantabrum, bien en sustitución de la actual, o al menos otorgándole la misma oficialidad. El estandarte ya se venía utilizando durante la celebración de festivales de folclore tradicional, acontecimientos deportivos y de reafirmación de la identidad cántabra. Algunos ayuntamientos legitimaron su utilización ya antes del 14 de marzo de 2016, fecha de la aprobación por parte del Parlamento de Cantabria de la proposición no de ley sobre el carácter simbólico del lábaro.

A partir de esta fecha fueron numerosas las corporaciones locales que izaron esta bandera en las casas consistoriales, promoviendo su uso y difusión como expresión iconográfica del pueblo cántabro.

Existe una serie de expertos como Joaquín González Echegaray, José Luis Casado Soto o Ramón Teja Casuso que defienden la legitimidad histórica de la actual bandera de Cantabria frente al lábaro, al argumentar que el pendón blanquirrojo es el que llevaban los barcos cántabros desde, al menos, el siglo XVIII. Según estos académicos, aunque en los textos antiguos hay alguna referencia a un estandarte denominado "cantabrum", en ningún caso las fuentes clásicas dan una descripción exacta de la forma, colores o símbolos del mismo, siendo aventurado reconstruirlo sin más elementos de juicio.

Frente a tergiversaciones publicadas en determinados medios críticos con el lábaro, González Echegaray en su estudio "Acerca del llamado "Lábaro Cántabro"" se limita a afirmar sobre el lábaro moderno: «Se trata de una creación nueva, que solo puede decirse que se halla vagamente sugerida por algunos de los elementos históricos que de aquí hemos hablado», si bien su opinión respecto a adoptar el lábaro como bandera es negativa.

Para Casado Soto, más crítico, el lábaro no sería sino un invento del regionalismo cántabro, cuya antigüedad no va más allá del periodo preautonómico, y el actual debate en torno a los símbolos regionales sería un intento de destruir el consenso que se alcanzó en el Estatuto de Autonomía.




</doc>
<doc id="15465" url="https://es.wikipedia.org/wiki?curid=15465" title="Narciso López">
Narciso López

Narciso López de Urriola (Caracas, 2 de noviembre de 1797-La Habana, 1 de septiembre de 1851) fue un militar español nacido en Venezuela creador de la bandera y del escudo de Cuba. A partir de una idea de Narciso López, junto a otros exiliados cubanos en Nueva York como el poeta Miguel Teurbe Tolón, José Aniceto Iznaga Borrell, su sobrino José María Sánchez Iznaga, Cirilo Villaverde y Juan Manuel Macías, confeccionaron en 1849 la bandera de Cuba, que es hoy la bandera y pabellón oficial: 2 franjas blancas, tres azules, un triángulo rojo y una estrella solitaria. Sobre ella juraron luchar y ofrendar la vida por hacer de Cuba una república independiente del imperio español. López fue líder de hasta cinco intentos para liberar a Cuba hasta que fuera ejecutado por las autoridades coloniales en La Habana por alta traición mediante garrote vil el 1 de septiembre de 1851.

Algunos historiadores, como Hugh Thomas, argumentan que Narciso López se convirtió en un promotor de la anexión de Cuba a los Estados del sur esclavista de los Estados Unidos. Como corriente política, ese anexionismo de Narciso López fue animado por los intereses expansionistas de los EE.UU. Al fracasar en el intento los estados del sur optaron por el secesionismo del norte industrializado lo cual desembocó en la guerra civil americana.

Narciso López nació en Caracas, Venezuela, en 1797. Sus padres fueron Pedro Manuel López y Ana Paula de Oriola, ambos de origen vasco.

Durante el proceso de emancipación de la América Hispana sirvió en el ejército español, luchando entre otras batallas en las de Las Queseras del Medio en 1819 y Carabobo en 1821 donde dirigió el "Regimiento Guías del General" de la "Quinta División" del ejército del Mariscal Miguel de la Torre. Su última actuación en Venezuela, fue en la Batalla naval del Lago de Maracaibo en 1823. Al ser destrozada la flota española, el coronel Narciso López huyó a Cuba con los restos del ejército realista, incluyendo a Calixto García de Luna e Izquierdo (abuelo de Calixto García) y Marcos Maceo (padre de Antonio Maceo y José Maceo) que lucharon por la independencia de la isla caribeña.

Cuatro años más tarde marchó a España, y allí frecuentó los círculos criollos. Luchó en la guerra civil que se desata en España (primera Guerra Carlista), donde sus méritos militares le elevaron al grado de brigadier en 1836. En 1839 recibió el cargo de gobernador de Valencia y un año más tarde fue ascendido a general. También ocupó el cargo de Gobernador Militar de Madrid y representante en las Cortes por Sevilla. Tomó parte en la revolución española de 1840.

Regresó a Cuba en 1840 con Jerónimo Valdés, que había sido nombrado Capitán General. Este le confió la Gobernación de las Cuatro Villas (Trinidad, Sancti Spíritus, San Juan de los Remedios y Santa Clara (Cuba)), y la Presidencia de la Comisión Militar Ejecutiva y Permanente contrayendo asimismo matrimonio con María de los Dolores de Frías y Jacott, hermana del gran terrateniente e intelectual cubano, Francisco de Frías y Jacott, IV conde de Pozos Dulces.

A la caída de Valdés, el sucesor de este, el Capitán General de Cuba Leopoldo O'Donnell, le destituyó en 1843 de sus cargos, y desde entonces se alineó y comprometió con las causas de los terratenientes cubanos, en línea con el mantenimiento de la esclavitud del Sur de los Estados Unidos.

En contacto con los grupos autonomistas locales dueños de grandes fortunas en Cuba, se embarcó en acciones contra la metrópoli en las serranías de Manicaragua, como la llamada conspiración de la Mina de la Rosa Cubana de 1848, que tenía ramificaciones en toda la isla y tras cuyo fracaso se vio obligado a huir a Estados Unidos, donde recibió la protección del gobernador del estado de Missisipi.

En aquel mismo año, el contacto entre grupos independentistas cubanos (el de Trinidad, dirigido por el propio López, y el aristocrático de La Habana y Camagüey, liderado por el marqués de Santa Lucía), fructificó en la organización de un Consejo Cubano en Nueva York. José Aniceto Iznaga Borrell, Gaspar Betancourt y Alonso Betancourt pasaron a Washington con el propósito de entrevistarse con el presidente de los EE.UU. James Knox Polk partidario decidido de la doctrina expansionista del Destino Manifiesto. Para llegar a esto, solicitaron la intervención de Jefferson Davis, senador por el Estado de Missisipi, y William J. Brown, subsecretario de Comunicaciones. Se presentaron todos en la Casa Blanca el 23 de junio de 1848. Desde aquella plataforma trataron de sensibilizar a los medios políticos estadounidenses, proponiendo al presidente Polk la compra de Cuba a la Corona de España, negociaciones que, al efectuarse directamente con el gobierno federal, podían significar el fin de la esclavitud lo que no convenía a los intereses que López representaba.

Narciso López, por su lado, se dedicó en Nueva York a preparar una expedición para la liberación de Cuba, a la apertura de suscripciones y financiación a través de la familia Iznaga, a actividades de propaganda e incluso, junto a Teurbe Tolón en Nueva York, al diseño de una bandera, a imagen y semejanza de la de Texas para su incorporación a la Unión como nueva estrella, y que luego se convertiría en la actual bandera cubana.

En julio de 1849 López decidió que la expedición partiría desde , Missisipi. En ella participaban exilados cubanos y algunos veteranos norteamericanos de la guerra contra México. Otros iban por la oferta de 1000 dólares y 64 hectáreas de tierras cultivables en Cuba que se les habría hecho efectivas en caso de tener éxito. López ofreció el mando al político sudista Jefferson Davis, quien recomendó al coronel Robert E. Lee por 200.000 dólares. Lee rehusó ante la oposición del gobierno de Washington de romper el "Tratado de Neutralidad con España" de 1818 y consecuentemente, López decidió asumir personalmente la jefatura de la expedición, de varios cientos de hombres. Sin embargo, la expedición fue frustrada en septiembre de 1849 al enviar el presidente de los Estados Unidos Zachary Taylor una fuerza naval para capturar los barcos de López, como consecuencia de un cambio de política con respecto a la anexión de Cuba.

En un segundo intento, López organizó otra expedición, en esta ocasión desde Nueva Orleans. Contó con la ayuda del gobernador de Mississippi , veterano de la guerra de México al que ofreció el mando de la aventura, que declinó. El 19 de mayo de 1850, con 600 voluntarios de Mississippi y Luisiana, desembarcó en Cárdenas, enarbolando por primera vez la que sería tomada en la Asamblea de Guáimaro como la enseña nacional de Cuba, por lo que a Cárdenas se le conoce también con el nombre de Ciudad Bandera. Tras quemar la casa del gobernador López controló la localidad durante varias horas pero la población de la misma no apoyó la revuelta, tras comprobar que el objetivo era mantener la esclavitud. La inferioridad de sus fuerzas y la aproximación de tropas españolas le obligaron a reembarcarse, siendo su barco perseguido por un navío de guerra español hasta Cayo Hueso; a pesar del fracaso, fue recibido como héroe en el Sur de Estados Unidos.

El 4 de julio de 1851 un grupo liderado por Joaquín de Agüero se levantó contra los españoles en Las Tunas y declaró la independencia de Cuba. La revuelta sería aniquilada rápidamente y Agüero fue capturado en Camagüey terminando sus días frente a un pelotón de fusilamiento, pero la noticia no hizo sino abrir las apetencias de los inversionistas especuladores en el futuro cubano, lo cual motivó a López a organizarse una vez más para intentar una nueva y última invasión a la isla.

El 3 de agosto de 1851 salió otra vez desde Nueva Orleans una expedición de 420 hombres, entre los que figuraba un "regimiento" de voluntarios sudistas al mando de William J. Crittenden sobrino del presidente en ejercicio Millard Fillmore. El 12 de agosto de 1851 los mercenarios a bordo de "El Pampero", desacatando las órdenes del gobierno federal, desembarcaron en la isla con la pretensión de establecer una república independiente y su posterior anexión a los Estados Unidos.

El desembarco se produjo en la playita del "Morrillo" de Pinar del Río, actual municipio de Bahía Honda, Provincia de Artemisa. El destacamento invasor fue objeto de persecución por el ejército español enviado desde La Habana. Sostuvo un primer encuentro armado victorioso en el poblado de Las Pozas. No obstante, ante la superioridad numérica de los españoles se vio obligado a replegarse hacia la Sierra del Rosario, en el curso alto del río Bayate. Una parte del destacamento (cincuenta hombres), que había permanecido en el lugar del desembarco bajo el mando del coronel Crittenden, segundo comandante de la expedición, se reembarcó en "El Pampero" y fue apresado por los vapores españoles "Cárdenas" y "El Habanero". Los expedicionarios fueron conducidos a La Habana y fusilados el 13 de agosto. Como respuesta a estas muertes el consulado español en Nueva Orleans fue destruido, mientras que los comercios de varios españoles en la ciudad fueron saqueados por las turbas. Posteriormente el gobierno del presidente Millard Fillmore negoció la liberación del resto de prisioneros estadounidenses en manos españolas.

Días después, los mercenarios de López sostuvieron un combate desastroso contra el general español Manuel de Enna y el brigadier Rosales, aunque el propio general Enna fue herido y falleció posteriormente. Con las fuerzas diezmadas, sin apoyo interno, acusado de piratería y bregando por la Sierra, López sostuvo dos batallas más, la última en la Puerta de La Muralla, cerca de San Cristóbal y fue finalmente capturado en Pinos de Rangel; en total, murieron unos 200 expedicionarios en los combates y el resto fue apresado: 160 de ellos fueron enviados a España.Lopéz fue conducido a La Habana el 31 de agosto y ejecutado por alta traición, mediante garrote vil, en la mañana del 1 de septiembre de ese año en la explanada del castillo de la Punta: se convirtió posiblemente en la figura más controvertida de la historia de Cuba.

La orden de su ejecución fue emitida por el entonces Capitán General de Cuba, José Gutiérrez de la Concha, quien había combatido bajo el mando de López durante el estallido de las guerras carlistas. Las últimas palabras de López fueron ""Mi muerte no cambiará los destinos de Cuba"."

La derrota y muerte de muchos expedicionarios, procedentes de Nueva Orleans, provocó la destrucción del consulado español en aquella ciudad y el cambio de nombre de numerosos habitantes para ocultar su procedencia española. Varios miembros de la familia Iznaga, ante la posibilidad de ser acusados, se trasladaron al estado de Mississippi, donde adquirieron extensiones de tierra para el cultivo de algodón mediante esclavos. Inspirado por las hazañas de López, el filibustero estadounidense William Walker organizó nuevas expediciones para apoderarse de Nicaragua, pero acabó fusilado en 1860 en Trujillo (Honduras). Los estados del Sur cambiaron su política anexionista por la secesionista que condujo a la Guerra civil estadounidense entre 1861 y 1865. Las invasiones de este aventurero venezolano colaboraron a la formación del concepto de América Latina y al antiimperialismo estadounidense.




</doc>
<doc id="15466" url="https://es.wikipedia.org/wiki?curid=15466" title="Escudo de Cuba">
Escudo de Cuba

El escudo de Cuba, conocido como el de "La Palma Real", fue creado en 1849 por Miguel Teurbe Tolón —quien también creó la bandera cubana— a petición del general venezolano Narciso López, para sellar los despachos y bonos que como jefe del gobierno provisional de Cuba emitió entre 1850 y 1851.

La versión actual no es exactamente igual a la original, ya que se suprimieron algunos elementos que contenía aquel y que podían haberse asociado con ideas anexionistas. Las especificaciones de diseño del escudo fueron establecidas mediante decreto por el primer presidente de Cuba, Tomás Estrada Palma, el 21 de abril de 1906 y han permanecido sin modificaciones desde entonces.

Según la ley 42 de la Asamblea Nacional, es el "Símbolo de la Nación". Este escudo tiene la forma de una adarga ojival, y está dividido hacia los dos tercios de su altura donde lo remata una línea horizontal. En su parte superior, la principal, se observa un mar a cuyos lados se ven dos porciones terrestres (Florida y Yucatán), entre los cuales cierra el estrecho una llave de vástago macizo (Cuba), con la palanca hacia abajo y a cuyo fondo un sol naciente esparce sus rayos por todo el cielo del paisaje. Estos elementos simbolizan la importancia geográfica y política de CUBA. La llave representa la entrada del Golfo de México y las significaciones terrestres son de izquierda a derecha, el Cabo Sable en la Florida y el Cabo Catoche en México. Al fondo, el sol aparece semi-hundido en el horizonte, denotando su calor tropical.

El cuartel inferior izquierdo, representa la división de la Isla, o sea los Departamentos en que estaba dividida en esos momentos, Occidente, Centro y Oriente; representándolos con tres franjas azul turquí. Dos blancas, que exponen la pureza de sus patriotas, intercaladas entre las tres azules, cierran el contenido del compartimiento. En el cuartel inferior derecho se yergue una palma real, con el botón de su hoja central en lo más alto, como símbolo de la lozanía y fertilidad de su privilegiado suelo, así como haciendo la exposición de que ha sido el más útil de los árboles a través de la historia de dicho país. Al fondo, en su retaguardia, aparecen dos montañas. Y ligeros celajes enmarcan el paisaje.

A manera de soporte, un haz de varas, asomado por debajo del vértice de la ojiva y aparecido después por la parte superior y central del eje del escudo, coronado con un gorro frigio de color rojo, vuelto hacia la izquierda, en el que se incrusta una estrella pentagonal, orientada hacia arriba. El gorro es un emblema adoptado por la Revolución Francesa, vuelto hacia la derecha, que sobresale por la parte superior. Este gorro se había usado en la antigüedad para ser llevado por los hombres que habían obtenido la libertad; en él aparece en su parte central , una estrella blanca de cinco puntas, con una de ellas orientada hacia la parte superior y, al igual que en la Bandera de la Estrella Solitaria, representa el estado independiente. El haz de varas indica la unión de los cubanos; la estrella, la máxima expresión de libertad.

Y termina la ornamentación del escudo, siempre visto de frente, de izquierda a derecha, sin exceder la altura del mismo, una rama de encina, que representa la paz, y otra de laurel, que representa la victoria ladeando el contorno del mismo.

El escudo, fue ratificado por la Constitución de 1940 y ha sido sucesivamente ratificado en las Constituciones adoptadas tras el triunfo de la Revolución cubana en 1959.


</doc>
<doc id="15468" url="https://es.wikipedia.org/wiki?curid=15468" title="Símbolos nacionales no oficiales de Cuba">
Símbolos nacionales no oficiales de Cuba

Los símbolos nacionales no oficiales de Cuba son aquellos símbolos e iconografías utilizadas profusamente como representación de la identidad de Cuba. Aunque no hayan sido adoptados de una manera oficial son populares y representan la historia, la naturaleza y la idiosincrasia de Cuba y los cubanos. Estos símbolos son la flor de la mariposa, la palma real, el tocororo, y la bandera de yara, aunque también hay otros.

La flor nacional de Cuba es la mariposa, cuyo nombre científico es "Hedychium coronarium" , de la familia de las zingiberaceae (apocináceo). No es originaria de Cuba sino de Asia, pero se ha adaptado maravillosamente al suelo cubano. Fue en 1936 que los botánicos del Jardín de la Paz en Argentina, pidieron a sus homólogos cubanos que determinaran cuál podría ser la flor nacional. El 13 de octubre de ese mismo año fue elegida la mariposa debido a que su blancura representa la pureza de los ideales independentistas y es símbolo de la paz. Además, el blanco es un elemento presente en las franjas de la enseña nacional. La forma de su flores, unidas al tallo central, simbolizan la unión de los cubanos. Es también paradigma de la gracia y la esbeltez de la mujer cubana. Según la tradición oral, se cuenta que durante las guerras de independencia, se escondían mensajes para el ejército libertador en el interior de estas flores, prendidas en velos y matones.

Cuando hablamos del Árbol Nacional, nos referimos al gran árbol que apareció un 6 de octubre del 1953, la palma real, cuyo nombre científico es "Roystonea regia" . Es reconocida por los cubanos como la reina de los campos por la majestuosidad de su estructura, por su peculiar talla, la utilidad que reporta y por ser, además, el más numeroso de los árboles de la Isla. Pertenece a la familia de las "palmáceas", es un árbol elevado, erecto que alcanza generalmente entre cuarenta y cincuenta pies de altura, coronado por un bello penacho de hojas "pinnatisectas", capaz de suscitar tal admiración que muchos poetas y músicos han cantado a su elegancia. Sus frutos florecen durante casi todo el año y desde tiempos inmemoriales fue utilizada, primero por los aborígenes y más tarde por los campesinos cubanos, para satisfacer algunas de sus necesidades más vitales, desde la comida para los animales de crianza hasta la madera para la construcción de las casas y las hojas para cubrir sus techos. Su gallarda presencia en el Escudo Nacional representa la libertad e independencia de la joven república cubana, símbolo de la lozanía y feracidad de su privilegiado suelo. No es exclusiva de Cuba, también se encuentra en Venezuela.

El tocororo, cuyo nombre científico es "Priotelus temnurus" del orden "Trogoniformes" y perteneciente a la familia "Trogonidae". Llamado por los originarios cubanos como "guatini", nombre que continúa siendo utilizado en algunas de las provincias orientales. Habita en lugares boscosos del país, preferentemente de montaña. Es considerado el ave nacional por dos motivos: su espléndido plumaje de vivos colores y por su resistencia al cautiverio, ya que muere cuando se le mantiene cautivo. Considerado como el ave más bella de Cuba, parte de su plumaje en verde recuerda los campos, su pecho de plumas blancas, su vientre de plumaje rojo y las plumas azules de su cabeza completan el claro simbolismo de la enseña nacional.

También conocida como «bandera de Yara» o «La bandera del 10 de octubre», fue creada por Carlos Manuel de Céspedes, el Padre de la Patria cubana, y confeccionada por Candelaria Acosta "Ambula". Según el hijo de Céspedes, su padre se habría inspirado en la bandera de Chile (instaurada el 18 de octubre de 1817) para su creación: «"imaginó una bandera nueva, que luciendo los mismos colores y forma de la de Carreras [sic] y O'Higgins se diferenciase de ésta en la disposición de aquellos"».

Esta bandera ha presidido desde 1868, y continúa presidiendo, junto a la enseña nacional, todas las sesiones del parlamento cubano.

Han sido varios los poetas que han recibido, en distintas épocas, el título de Poeta Nacional de Cuba, entre los que se incluyen José María Heredia (1803-1839), de quien el propio José Martí hiciera grandes elogios por su poesía, de profundo contenido patriótico, entre la que destaca especialmente su "Oda al Niágara" con la descripción de las "palmas que en mi patria se mecen del viento a la sonrisa", Julián del Casal, el poeta cubano que murió riéndose (1863-1893), Agustín Acosta (1886-1979) y por último Nicolás Guillén (1902-1989), conocido especialmente por sus "Motivos del Son" y su poesía negra como contribución al espectro de los componentes de la nación y la nacionalidad cubana, especialmente por la musicalidad de sus versos.

El deporte nacional de Cuba es sin dudas la "pelota" o béisbol. Surgió en los Estados Unidos y se comenzó a practicar en Cuba a finales del siglo XIX.

El danzón fue creado en Matanzas en 1878. Actualmente es uno de los bailes más utilizados al momento de representar el país en sus bailes típicos.


</doc>
<doc id="15471" url="https://es.wikipedia.org/wiki?curid=15471" title="Electromiografía">
Electromiografía

Electromiografía es la técnica de registro gráfico de la actividad eléctrica producida por los músculos esqueléticos.
Esta actividad eléctrica es conocida como el electromiograma o “EMG”.
El EMG puede ser monitoreado a través de electrodos insertados dentro de los músculos (electrodos intramusculares) o a través de electrodos en la superficie de la piel sobre el músculo (electrodos superficiales). 
El EMG es usado por científicos para estudiar el sistema neuromuscular, por médicos para el diagnóstico de enfermedades neuromusculares, y por fisioterapeutas para monitorear la activación de músculos de un paciente.

La fuente eléctrica es el potencial de la membrana muscular de alrededor de -70 mV, midiendo los rangos potenciales de EMG de menores a mayores rangos entre 50 μV hasta 20 o 30 mV, dependiendo del músculo en observación.

El rango típico de repetición de una unidad motora muscular es de alrededor 7–20 Hz dependiendo del tamaño del músculo. El daño a las unidades esperadas puede ser entre rangos de 450 y 780 mV.

El primer material en el que aparece el EMG fue en el de trabajo de Francesco Redi en 1666. Redi descubrió un músculo altamente especializado en la anguila eléctrica Electrophorus electricus que generaba electricidad. En 1773, Walsh pudo demostrar que el tejido muscular de la Raya Eléctrica tenía la capacidad de generar una chispa de electricidad. En 1792, en una publicación titulada "De Viribus Electricitatis in Motu Musculari Commentarius" escrita por Luigi Galvani, aparecía que el autor demostraba que la electricidad podía iniciar contracciones musculares. Seis décadas después, en 1849, Dubois-Raymond descubrió que era también posible llevar un registro de la actividad eléctrica durante la actividad de la contracción muscular. El primer registro real fue hecho por Marey en 1890, quien además introdujo el término de electromiografía. En 1922, Gasser y Erlanger usaron un osciloscopio para mostrar las señales eléctricas de los músculos. Entre 1930 y 1950 los científicos comenzaron a utilizar electrodos mejorados y más sofisticados para los estudios musculares. El uso clínico del EMG de superficie (sEMG) para el tratamiento de desórdenes más específicos comenzó en la década de los 60’. Hardyck y sus colaboradores fueron los primeros (1966) en usar el sEMG. En los comienzos de los 80’s, Cram y Steger introdujeron un método clínico para escanear una variedad de músculos utilizando un dispositivo para el sensado del EMG.

No fue hasta mediados de los 80’s, cuando las técnicas de integración en electrodos fueron lo suficientemente avanzadas para permitir la producción por lotes de la instrumentación y los amplificadores pequeños y livianos requeridos. En el presente, hay un gran número de amplificadores disponibles comercialmente. Investigaciones recientes han resultado en una mejor comprensión de las propiedades del sEMG. La electromiografía de superficie es crecientemente usada para el registro de músculos superficiales en protocoles clínicos o kinesiológicos, mientras que los electrodos intramusculares son utilizados para investigar músculos profundos o actividad muscular localizada.

Hay muchas aplicaciones para el uso de la EMG. El EMG es utilizado clínicamente para el diagnóstico de problemas neurológicos y neuromusculares. Es utilizado diagnósticamente por los laboratorios de marcha y por clínicos entrenados en el uso del biofeedback o el aseguramiento ergonómico. El EMG es también utilizado en muchos tipos de laboratorios de investigación, incluyendo a los que están involucrados en el campo de la biomecánica, el control motor, la fisiología neuromuscular, los desórdenes de movimiento, el control postural y la terapia física.

Hay dos métodos para utilizar el EMG, uno es la superficial, y el otro método es el intramuscular. Para llevar a cabo un EMG intramuscular, se usa una aguja electrodo, se inserta a través de la Piel hasta que entre al tejido muscular. Un profesional entrenado (como un neurofisiólogo, un neurólogo, o un fisiatra), va observando la actividad eléctrica mientras inserta el electrodo. Mientras se va insertando el electrodo provee una información valiosa en cuanto a la actividad muscular como al nervio que inerva ese músculo. Los músculos cuando están en reposo muestran señales normales eléctricas, cuando el electrodo es insertado, por ende la actividad eléctrica se estudia cuando el músculo está en reposo. La actividad anormal espontánea indica un daño en el nervio o en el músculo. Después se le pide al paciente que contraiga el músculo suavemente para poder realizar un análisis con más profundidad. El tamaño, la frecuencia y la forma resultante de la unidad potencial motora son analizados. Posteriormente el electrodo es retirado unos pocos milímetros e insertado nuevamente para analizar la actividad, la cual debe tener unidades por lo menos entre 10–20. Cada trazo del electrodo da una imagen muy local de la actividad del músculo completo. Debido a que el músculo esquelético difiere en su estructura interna, el electrodo debe ser puesto en varias localizaciones para obtener resultados confiables de estudio.

El método Intramuscular EMG puede ser considerado demasiado invasivo o innecesario en algunos casos. En su lugar, el método superficial emplea una superficie en la cual el electrodo se puede utilizar para controlar la imagen general de la activación muscular, a diferencia de la actividad de sólo unas pocas fibras como se observa utilizando un EMG intramuscular. Esta técnica se utiliza en una serie de ajustes, por ejemplo, en la fisioterapia, la activación muscular se controlará mediante EMG superficial y los pacientes tienen un estímulo auditivo o visual para ayudarles a saber cuándo se está activando el músculo (retroalimentación).

Una unidad motora se define como un motor neurona y todas las fibras musculares que inerva. Cuando una unidad motora se activa, el impulso llamado potencial de acción se desplaza de la neurona motora hacia el músculo. El área donde el nervio hace contacto con el músculo se llama unión neuromuscular. Después de que el potencial de acción se transmite a través de la unión neuromuscular, se obtiene un potencial en todas las fibras musculares inervadas por la unidad motora particular. La suma de toda esta actividad eléctrica se conoce como un potencial de acción de la unidad motora (MUAP). La actividad electrofisiológica de las múltiples unidades motoras es la señal que normalmente se evalúa durante un EMG. La composición de la unidad motora, el número de fibras musculares por unidad motora, el tipo metabólico de las fibras musculares y muchos otros factores afectan la forma de los potenciales de unidad motora en el miograma.

Algunos pacientes pueden encontrar el procedimiento doloroso, otros experimentan un pequeño nivel de disconfort cuando la aguja es insertada. Los músculos a los cuales se les realiza el procedimiento pueden quedar adoloridos por uno o dos días después del procedimiento.

Un equipo básico de electromiografía consta de los siguientes elementos:

El tejido muscular en reposo es eléctricamente inactivo. Después de la actividad eléctrica causada por la inserción de las agujas, el electromiógrafo no debe detectar ninguna actividad anormal espontánea (es decir, un músculo en reposo debe estar eléctricamente silencioso, con la excepción del área de la unión neuromuscular, que en circunstancias normales, se activa muy espontáneamente). Cuando el músculo se contrae voluntariamente, los potenciales de acción comienzan a aparecer. Como la fuerza de la contracción muscular aumenta, más y más fibras musculares producen potenciales de acción. Cuando el músculo se contrae completamente, deben aparecer un grupo desordenado de potenciales de acción de tasas y amplitudes variables.

El EMG es utilizado para diagnosticar enfermedades que generalmente están clasificadas en una de las siguientes categorías: neuropatías, enfermedades del empalme neuromuscular y miopatías.

Las Neuropatías se definen desde las siguientes del EMG:

Miopatías definiendo características del EMG:

Los resultados anormales son causados por las siguientes condiciones médicas:

Las señales del EMG se componen principalmente de los potenciales de acción de las unidades motoras superpuestas. La medición de la señales del EMG pueden ser descompuestas en los potenciales de acción de las unidades motoras (PAUMs) constituyentes. Los PAUMs de diferentes unidades motoras pueden tener distintas formas, mientras que los PAUMs registrados por el mismo electrodo de la unidad motora, son típicamente similares. La forma y el tamaño del PAUM dependen notablemente del lugar donde se localice el electrodo con respecto o a las fibras.

Las señales del EMG son usadas en muchas aplicaciones clínicas y biomédica. El EMG es usado como una herramienta para diagnosticar enfermedades neuromusculares, y desórdenes del control motor. Las señales del EMG también son utilizadas para el desarrollo de prótesis de manos, brazos y extremidad inferior.

El EMG también es usado para detectar la actividad muscular en los lugares donde no se produce movimiento. Se puede reconocer el habla de una persona con incapacidad para producir voz mediante la observación de la actividad del EMG, en los músculos asociados con el habla.




</doc>
<doc id="15473" url="https://es.wikipedia.org/wiki?curid=15473" title="Mostaza">
Mostaza

La mostaza hace referencia generalmente al condimento envasado con apariencia externa pastosa y de sabor picante que se elabora de las semillas de varias plantas del género "Sinapis", familia de las crucíferas, que también incluye las coles y los nabos. Asimismo, hace referencia también a la pequeña semilla de mostaza, usada como especia y que se emplea frecuentemente en algunas gastronomías, como por ejemplo: la alemana, la india o la francesa, entre otras.

La mostaza se denominaba en el castellano clásico como jenabe, que a su vez proviene del latín "sinapis", y este del griego con el mismo nombre, de aquí proviene la palabra sinapismos que son las cataplasmas de mostaza aplicadas al pecho como remedio natural de catarros y otras afecciones pulmonares. La denominación, tal y como se conoce hoy en día, aparece por primera vez en Francia posiblemente hacia el año 1220 de una derivación de la palabra latina ""mustum"" y la primera constancia registrada del nombre asociado al condimento es: ‘moutarde’ y se sospecha que provenga del latín vulgar ‘"mustum ardens"’ ("mosto ardiente") por tener los romanos la costumbre de añadir, o diluir, granos de mostaza en el zumo de la uva (mosto). Casi en la misma época aparece registrado en castellano con el nombre de mostaza y en Italia con el de "mostarda".
Las semillas de mostaza están relacionadas desde varios enfoques religiosos como las semillas de fe y de abundancia. Esta semilla se nombra como algo muy pequeño que se multiplica y simboliza la abundancia y el fenómeno de la multiplicidad.

Se cree que fueron los romanos quienes desarrollaron el preparado de mostaza que conocemos hoy. Mezclaban jugo de uva sin fermentar -conocido como "mosto"- con semillas de mostaza -llamadas "sinapis-" para formar el "mustum ardens" o "mosto ardiente". La empleaban como condimento gastronómico; Plinio la menciona como aditamento de los vinos especiados, y -como también confitaban en vinagre sus hojas- se utilizaba en la elaboración del moretum (queso especiado). También se empleaba como planta medicinal aplicada como remedio contra los dolores de cabeza o simplemente como digestivo.
Los griegos la usaban como condimento y Pitágoras recomendaba su consumo, pues creía que mejoraba la memoria y levantaba el ánimo; se sabe también que el botánico Teofrasto la cultivaba en los jardines.

Se puede considerar este periodo como el primero en que tuvo auge esta especia. De hecho, se empieza a emplear como condimento de carnes (sobre todo vacunas) y tal vez para ocultar el sabor de la carne en mal estado. Es en el siglo XIII cuando aparece en casi todos los platos de la gastronomía europea, y su cultivo se intensificó; así se puede comprobar en las ciudades de Cremona en Italia y Dijon en Francia; en esta última la producción continúa hoy en día, y se considera una de las primeras del mundo (una gran parte de la producción mundial proviene de esta región de la Borgoña y, la otra, de Canadá).

Ya en la época moderna nos encontramos numerosas recetas de elaboración diversas por país, en España en el siglo XVII, por ejemplo, el cocinero de los reyes de la Casa de Austria, Francisco Martínez Motiño, menciona una "receta española" de elaboración de la mostaza. Posteriormente en el siglo XX se hace famosa por una simple semejanza de olor con la iperita que tiene el gas mostaza (no tiene ninguna otra cosa en común). 

Hoy en día se emplea como salsa acompañante de salchichas (especialmente los perritos calientes) y es un ingrediente importante de las hamburguesas. También se emplea en algunos sándwiches (principalmente la del tipo americano que viene coloreada con cúrcuma, llamada mostaza preparada). En varios países también acompaña a las comidas rápidas autóctonas tales como la arepa cabimera, tumbarranchos o patacón (en Venezuela) o las sopaipillas (en Chile).

La variedad francesa o tipo Dijon suele emplearse en platos de estilo "gourmet."

La mostaza es una salsa baja en calorías y carece de colesterol al no tener como ingrediente ningún tipo de grasa animal. Su semilla tiene un alto contenido proteico y de minerales. Además posee propiedades antisépticas y digestivas.

La mostaza blanca dulce ("Brassica alba") crece de forma silvestre en el norte de África, el Oriente Medio y la Europa mediterránea, extendiéndose ampliamente por su prolongado cultivo. La mostaza morena ("Brassica juncea"), originaria de las laderas del Himalaya, se cultiva comercialmente en el Reino Unido, Canadá y Estados Unidos. La mostaza negra ("Brassica nigra"), se cultiva en Argentina, Brasil, Estados Unidos y algunos países europeos. Canadá cultiva el 90 % de toda la semilla de mostaza para el comercio internacional.

Se sabe que existen unas cuarenta especies distintas de mostaza, de las cuales sólo tienen interés culinario y médico la denominada mostaza blanca ("Sinapis alba"), la mostaza negra ("Sinapis nigra") y la mostaza salvaje ("Sinapis arvensis"). Se emplea fundamentalmente en gastronomía como condimento de algunos platos y en la elaboración de algunas salsas, como la Cumberland (elaborada con oporto) en la cocina portuguesa y la salsa Robert, inventada por Robert Vinot, en la gastronomía francesa. En algunos países de Europa Oriental se utiliza una mostaza agridulce, hecha a base de dos partes de mostaza por una de mayonesa, sazonada con especias y endulzada con azúcar.

La mostaza es un ingrediente que está presente en numerosos alimentos. Debido a su capacidad de inducir procesos de alergia, de acuerdo con las directivas de la Comunidad Económica Europea, es obligatoria su identificación en los alimentos que la contengan.


</doc>
<doc id="15476" url="https://es.wikipedia.org/wiki?curid=15476" title="LEP/DELPHI">
LEP/DELPHI

DELPHI ("DEtector with Lepton, Photon and Hadron Identification") es el nombre de uno de los cuatro detectores que operaron en el acelerador de partículas LEP.

Fue diseñado como un detector de propósito general con un especial énfasis en la identificación de partículas, incluso en el caso de sucesos complejos. Entre sus características más relevantes se encuentra el uso de detectores de efecto Cherenkov ("Ring Imaging CHerenkov counters", RICH) para la identificación de trazas dentro de jets, la información tridimensional que proporciona tanto para partículas cargadas como neutras, la elevada granularidad de la mayoría de sus componentes y su precisión en la reconstrucción de vértices.

DELPHI se encontraba instalado en una caverna subterránea a 100 m por debajo del nivel del suelo. En la figura inferior puede observarse la estructura general del detector. Constaba de una sección cilíndrica, el "barril", y dos tapas que pueden abrirse axialmente, más un detector de luminosidad situado en el túnel de LEP. DELPHI tenía en conjunto un diámetro de más de 10 m y
un peso total cercano a las 3500 Tm.

Un solenoide superconductor de 7.4 m de longitud y 5.2 m de diámetro interior proporcionaba un campo magnético altamente uniforme de 1.2 T en la dirección del haz, el cual permitía distinguir la carga de las partículas por el sentido de la curvatura de su trayectoria en el seno del detector.

El sistema de detección de trazas se estructuraba alrededor del tubo del haz. En la zona del barril estaba formado por distintos subdetectores optimizados para definir la posición de la partícula a su paso por ellos. Consistía en el detector de microvértices, el detector interno, la cámara de proyección temporal y el detector externo. Las cámaras "forward" A y B contribuyen en la zona de las tapas. Los detectores RICH se hallaban intercalados entre estos detectores y proporcionan una ayuda en la identificación de los tipos de partículas que los atraviesaban.

Los calorímetros electromagnéticos se situaban alrededor de los detectores anteriores tanto en la zona del barril como en la zona "forward". Rodeaban a éste, el retorno del imán y el calorímetro hadrónico en la zona del barril. Finalmente, en las zonas más externas de DELPHI se encontraban las cámaras de muones la cual contribuía de forma decisiva a la identificación de estos leptones.


</doc>
<doc id="15479" url="https://es.wikipedia.org/wiki?curid=15479" title="Lotería">
Lotería

La lotería o loto es un juego que puede ser público mediante billetes y sorteos o un juego de mesa que consiste en cartones y barajas. El coleccionismo de billetes de lotería se denomina loterofilia.

Se supone que el nombre de "lotería" procede del italiano "lotta", "lucha", porque al parecer se establece una lucha entre el jugador, la suerte y los concurrentes: otros suponen que se deriva del alemán "lot", que significa "suerte", porque es lo que uno desea en la lotería y demás juegos de azar. Otra definición proviene del latín "loterus" que se usaba en referencia a la suerte de los individuos.

La evidencia más antigua registrada de loterías son billetes keno de la dinastía china Han de entre 205 y 187 a. C. Keno es un tipo de lotería que aún se juega en los casinos de hoy en día. Hay evidencia que esas loterías chinas iniciales ayudaron a financiar la construcción de la Gran Muralla de China.

Es tan antiguo este juego de azar que en la descripción de las saturnales lo vemos ya usado por los romanos y se sospecha que fueron sus inventores para hacer más agradables dichas fiestas. Empezaban estas por una distribución gratuita de billetes a los convidados, que ganaban algo de importancia o mérito en el caso de ser favorecidos por la suerte: lo que había escrito en los billetes se llamaba "apophaneta". De orden de Augusto se hicieron extracciones de poco valor, mientras Nerón, para halagar al pueblo mandó distribuir hasta mil billetes diarios alguna vez con los cuales se podía hacer la fortuna de algunas familias. Heliogábalo inventó una lotería muy original y consistía en lotes de mucho valor y lotes de muy poco: por ejemplo, doce esclavos y doce garbanzos, seis vasos de plata y seis de barro, una libra de fruta y una de oro. Parece que se debe la reaparición de este juego en la Europa moderna al monje Celestino Galiano, que floreció en el siglo XVIII y se dice que inventó otro juego llamado el "loto", semejante al de la lotería.

En la República de Génova existía la costumbre de echar a la suerte el nombre de los cinco senadores que debían ocupar ciertas plazas. El senado estaba compuesto de noventa miembros y para el sorteo se metían en una caja cincuenta bolas, cinco de ellas marcadas, que eran las de los cargos vacantes. El público, que desconocía el nombre de los noventa senadores, hacía apuestas sobre los que pudieran ser los agraciados, las cuales eran objeto de verdadera especulación. Se autorizó a varios banqueros para verificar operaciones regulares, fundándose con tal motivo una lotería por vez primera en 1629 que en pocos años pasó a las naciones vecinas.

En Francia, la lotería no nació hasta 1776, época en que se constituyó la lotería real, que fue abolida en 1836.

La lotería es un juego de azar que consiste en acertar los números de un billete previamente comprado con los números extraídos de una tómbola o un recipiente que garantice que sean extraídos al azar. Los números de dicho billete pueden ya estar preimpresos o bien ser elegidos por las propias personas. El número de aciertos pueden ser todos o parte de los número del billete. Al ganador o ganadores se les entrega un premio en dinero o especies. Por lo general si no hay ganadores para un sorteo el premio se acumula para el siguiente.

La lotería es un monopolio estatal o una concesión regulada por la leyes. En todos los países existen prohibiciones para que los particulares organicen juegos de lotería que no estén regulados de alguna forma. Una parte de lo recaudado por la venta de los billetes de lotería en general es entregada a obras de beneficencia social o queda en manos del Estado y es destinado a los gastos corrientes del mismo; de ahí que se diga que se trata de: "un impuesto voluntario". En este mismo orden fiscal, se ha detectado el frecuente uso de las loterías como instrumento de fraude fiscal.

Consiste en un grupo de barajas con figuras determinadas y con varios cartones que contienen un número determinado de éstas figuras ordenado al azar (ej. 9, 12, 16). Los jugadores toman cartones y uno de ellos además, previo a haber revuelto perfectamente el mazo, va sacando una a una las barajas y dando su nombre, a esto se le llama en México, "cantar las barajas" o "echar la baraja" o simplemente "cantarlas" o "echarlas". A medida que se van "cantando" las barajas los jugadores apuntan en sus cartones las que van teniendo. Gana el primero que llene un cartón, es decir que todas las figuras de este hayan salido y el jugador se haya dado cuenta, pues si no se dice que "se le pasaron" y el juego continúa hasta que se dé cuenta o alguien más llene su cartón. Es común que existan metas intermedias como el primero que logra una figura al centro del cartón (en caso de que este tenga una figura central ej. cartones de 3x3 o 5x5 figuras), a esto se le llama "Bolazo" y otras metas intermedias pueden ser para el primero en lograr "cuatro esquinas" o "raya" en un cartón particular.

Es común que este juego se juega con dinero, para lo cual se fija una cooperación por cartón y el monto de los premios del bolazo, la raya y las cuatro esquinas, en caso de que se decida que existan además de que el resto del dinero recolectado por los cartones utilizados luego de pagar los premios anteriores es para el ganador.

El año 1763, reinando Carlos III y siendo su ministro el Marqués de Esquilache, se estableció en Madrid la Real lotería "primitiva" o "antigua" en beneficio de varios establecimientos piadosos, habiéndose celebrado su primer sorteo el día 10 de diciembre de aquel año. 

Por un real decreto de 30 de septiembre de 1763, se estableció en la villa de Madrid, a imitación de la corte de Roma y otras, la primera lotería, o sea la extracción de unos números a favor de los hospitales, hospicios y otras obras pías, bajo las seguridades, método y reglas que se creyeron conducentes e imprimieron para gobierno de los empleados.

En 20 de julio por resolución de Carlos III y circular del Consejo de 23 de agosto del siguiente año, se prohibió el establecimiento de ofertas extranjeras en España en atención a haberse introducido abusivamente en varias ciudades y pueblos billetes de varias de ellas que se beneficiaban y despachaban en el reino, para evitar la exportación del numerario bajo la pena de 500 ducados (5,500 rs.) por la primera vez a cada infractor, dividida entre el denunciador, juez y fisco por iguales partes; por la segunda la pena doblada y por la tercera, cuatro años de presidio además de los 1.000 ducados de multa.






</doc>
<doc id="15481" url="https://es.wikipedia.org/wiki?curid=15481" title="Contraste radiológico">
Contraste radiológico

Los contrastes radiológicos son sustancias radiopacas, que aplicadas por diferentes vías de administración (oral, rectal, endovenosa, etc.) pueden ser empleadas, durante un examen de rayos X o radiografías, para facilitar y/o mejorar la visualización de distintos órganos o fluidos de nuestro cuerpo con un fin diagnóstico. También se emplean contrastes radiológicos para las técnicas de resonancia magnética. 

Dichas sustancias mejoran la visualización ya que producen un incremento en la diferencia de densidad entre los diversos tejidos. 

Para la selección de sustancias como medio de contraste radiológico se han definido unas características que debería tener un supuesto medio de contraste ideal: 


Sin embargo, no todos los medios de contraste empleados actualmente en clínica cumplen con todas estas características. 

Distinguimos dos tipos de contrastes: 

Su denominación se debe a que absorben menos las radiaciones que los órganos y fluidos corporales; de tal forma, que los órganos o tejidos se ven más opacos que las cavidades donde se encuentre el medio de contraste. 

Los principales medios de contraste radiológico pertenecientes a este grupo son los gases, solubles en sangre y rápidamente eliminables, como el oxígeno y dióxido de carbono. 

Son empleados fundamentalmente en la visualización del tracto gastrointestinal, pudiendo ser administrados tanto por vía oral como por vía rectal. 

Otros ejemplos de contrastes negativos son: aire (generalmente se administra por vía rectal para la visualización del colon) y polvo efervescente o bicarbonato sódico (empleado por vía oral para la visualización de las estructuras comprendidas entre el esófago y duodeno).

El dióxido de carbono es bien tolerado por los pacientes ya que no se absorbe, y sus indicaciones más frecuentes son en pacientes con insuficiencia renal y/o con antecedentes de alergia al contraste yodado.

Los contrastes radiológicos positivos se caracterizan por ser sustancias más radiopacas que los órganos o fluidos de sus proximidades; esto es debido a que suelen tener un número atómico elevado, que les permite absorber o atenuar gran cantidad de radiación. Dentro de este grupo destacan principalmente los contrastes baritados y yodados. 


Es un elemento químico que forma parte de la familia de los lantánidos. Como medio de contraste es empleado fundamentalmente en resonancia magnética; se caracteriza por ser administrado por vía endovenosa, y se elimina por vía renal. 

Hay que tener en cuenta, que el galidonio en su forma libre es tóxico, por tanto su utilización como medio de contraste requiere la unión a un agente quelante; de tal manera que esta unión lo convierte en una sustancia no tóxica, aunque es verdad que esta unión depende directamente de la estructura que presente el quelante. 

A la dosis habitual de este medio de contraste (0.1 a 0.2 mmol/kg) la mayoría de las reacciones son leves, como por ejemplo sensación de frío o calor, náuseas, cefalea, mareos o picazón.  Reacciones alérgicas como exantema, urticaria o broncoespasmo son muy poco frecuentes. También se ha relacionado el gadolinio con nefrotoxicidad, provocando fibrosis sistémica nefrogénica, a cual se trata de una enfermedad, poco frecuente, pero muy grave, que puede llegar a comprometer la vida del paciente; se caracteriza fundamentalmente, por un aumento en la formación de tejido conectivo en la piel, articulaciones, músculos y órganos internos. 

Muchos fármacos con gadolinio han sido retirados del mercado debido a que se observó que, parte del gadolinio no se eliminaba de forma íntegra, y los que liberaban más cantidad, daban lugar a la formación de depósitos cerebrales. si bien, es cierto, que no se han identificado síntomas ni trastornos relacionados con dichos depósitos. 

Aun así, y por precaución, dichos fármacos fueron retirados por la AEMPS (Agencia Española de Medicamentos y Productos Sanitarios), y los que se han mantenido, se ha recomendado que se usen cuando sea estrictamente necesario, y a la mínima dosis posible. 

También se ha observado que dichos depósitos pueden formarse en otros órganos, y se han relacionado con efectos secundarios que pueden provocar estos fármacos.

El mecanismo por el cual el gadolinio provoca dicho daño, se cree que se debe a su capacidad de provocar la liberación de citosinas a través de la estimulación de macrófagos de la piel (por los iones libres de Gd) o monocitos en sangre periférica (por los complejos de quelante-Gd).

Estos procesos de activación de macrófagos, liberación de citoquinas proinflamatorias provocan la diferenciación de fibrocitos en la sangre y la activación de fibroblastos, consecuentemente se genera una respuesta que crea depósitos de colágeno y fibrosis. Se ha comprobado que la presencia de insuficiencia renal contribuye a la liberación de gadolinio mediante el aumento de la transmetalación. Cuanto mayor sea la proporción de gadolinio no unido a quelante (gadolinio libre) mayor es el daño porque presenta mayor capacidad de activación.

El bario se emplea en forma de polvo de sulfato de bario (BaSO), que se mezcla con agua formando una suspensión. 

En cuanto a sus características principales, destaca la ausencia de actividad (no tiene actividad farmacológica) y toxicidad (ya que no se absorbe), además, tiene una excelente capacidad de opacificación y no produce falsos positivos. 

Se emplea como medio de contraste para la visualización de estructuras en el tracto gastrointestinal, pudiéndose administrar tanto por vía oral, como por vía rectal. 

Esta contraindicado en situaciones en las que se sospeche cualquier alteración del sistema gastrointestinal como perforación intestinal, obstrucción del tracto gastrointestinal,  irritación peritoneal, estenosis pilórica, fístulas, etc. 

El sulfato de bario es que contraste radiológico positivo que menor toxicidad presenta. 

Aunque a priori parezca una paradoja dado que el bario es un metal pesado, cuyos compuestos solubles presentan una elevada toxicidad, estos compuestos prácticamente carecen de toxicidad por su baja solubilidad y consecuentemente la imposibilidad de absorción. De ahí, que esté contraindicado en aquellas situaciones en las que pueda existir la absorción de este compuesto, como por ejemplo en perforación intestinal. Así mismo, las reacciones alérgicas son muy poco frecuentes. 

Se trata del grupo más importante dentro de los medios de contraste, ya que son los más usados. Se caracterizan por una alta densidad de contraste y una baja toxicidad. 

Su estructura química se corresponde con un núcleo benzoico yodado. 

Así se les puede dividir en monoméricos (tienen un solo núcleo benzoico) y diméricos (dos núcleos benzoicos); dando lugar así a cuatro grupos de contrastes yodados: 





La principal diferencia entre los iónicos y los no iónicos es la sustitución del radical carboxilo (-COOH) de los iónicos, por un radical hidroxilo (-OH), lo que hace que no se disocien y por ello presenten menor osmolaridad. 

También, debido a esta característica presentan mejor tolerancia neural, se unen menos a proteínas plasmáticas, y tienen una menor incidencia de efectos adversos; sin embargo, son más costosos. 

Los contrastes yodados se pueden administrar tanto por vía oral, como por vía endovenosa, y dependiendo de la vía de administración, presentaran características farmacocinéticas diferentes. 

Los contrastes yodados administrados por vía oral se absorben muy poco y se eliminan fácilmente por vía rectal, mientras que los administrados por vía endovenosa, se eliminan prácticamente de forma total por vía renal, y es solo un 2% excretado por vía biliar. 

Es necesario distinguir entre los medios de contraste iónicos y no iónicos. 



Son responsables del signo tóxico: nefropatía inducida por contraste 

Es la reacción adversa más importante provocada por los contrastes yodados. Aunque el mecanismo exacto no se conoce del todo bien, se piensa que el mecanismo toxicodinámico es el siguiente:

Como hemos comentado antes la quimiotoxicidad que ejerce el medio de contraste sobre la nefrona genera la producción de ROS (radicales libres de oxígeno) y por tanto un efecto citotóxico que cursa con estrés oxidativo y activación de citosinas proinflamatorias.  Esto conduce a la inhibición de reabsorción tubular de proteínas, a la vacuolización, apoptosis y necrosis. La cascada oxidativa termina en hipóxia en los túbulos renales y pérdida de nefronas. 


</doc>
<doc id="15486" url="https://es.wikipedia.org/wiki?curid=15486" title="Treponema">
Treponema

Treponema es un género de espiroquetas gram negativas, finas y pequeñas (de 0,1 a 0,4 µm de diámetro y 6 a 10 µm de largo), con espiras regulares y apretadas y extremos afilados. En fresco solo pueden observarse con un microscopio de campo oscuro o por contraste de fase.

Los "Treponema" son bacterias helicoidales que se tiñen difícilmente con colorantes de anilina y fácilmente con Giemsa o por impregnación argéntica. Son móviles en medios líquidos por medios de rotación o translación. Tienen de 1 a 5 flagelos, generalmente 3. Son organismos quimioheterótrofos, utilizando una gran variedad de carbohidratos o aminoácidos como fuente de energía y carbono. Realizan una respiración anaerobia estricta o microaerófila.

Las especies patógenas son difíciles de cultivar en laboratorio, siendo más factible visualizar el organismo directamente de la muestra con inmunofluorescencia directa y ciertas tiniciones especializadas. Normalmente son parásitos y patógenos del hombre y animales. Viven en la cavidad oral, el aparato digestivo y órganos genitales.



</doc>
<doc id="15487" url="https://es.wikipedia.org/wiki?curid=15487" title="Spirochaetaceae">
Spirochaetaceae

Familia de microorganismos del orden de los Spirochaetales de la Phylum Spirochaetes, clase Spirochaetes.

Bacterias anaerobias, anaerobias facultativas o microaerofilas, helicoidales con un diámetro comprendido entre 0.1 y 3.0 micrómetros, los extremos no están curvados. En el peptidoglucano poseen L-ornitina. Utilizan carbohidratos o aminoácidos como fuente energética y carbonada.


</doc>
<doc id="15488" url="https://es.wikipedia.org/wiki?curid=15488" title="Daniel Barenboim">
Daniel Barenboim

Daniel Barenboim (; Buenos Aires, 15 de noviembre de 1942) es un pianista y director de orquesta argentino nacionalizado español, israelí y palestino.

Hijo de músicos (tanto Enrique Barenboim como Aída Schuster, sus padres, fueron destacados pianistas), debutó en Buenos Aires a los siete años y fue posteriormente invitado por el Mozarteum de Salzburgo a continuar sus estudios en esa ciudad, en cuyo festival participó tres años después. Luego estudió con Nadia Boulanger, Igor Markevitch y en la Academia de Santa Cecilia de Roma.

Barenboim empezó sus estudios de piano a la edad de cinco años con su madre y continuó con su padre, quien quedó como su único profesor. En agosto de 1950 interpretó su primer concierto en Buenos Aires. Realizó sus estudios primarios en el Instituto Pestalozzi de Belgrano R.

En 1952 la familia Barenboim se trasladó a Israel. Dos años más tarde, sus padres lo enviaron a Salzburgo para que tomara clases de dirección con Igor Markevitch. Allí conoció a Wilhelm Furtwängler, para quien tocó el piano. 

En 1955 estudió armonía y composición con Nadia Boulanger en París.

Barenboim debutó como pianista en el Mozarteum de Salzburgo en 1952, en París ese mismo año, en Londres en 1956 y en Nueva York en 1957, bajo la dirección de Leopold Stokowski. En los años siguientes realizó regularmente conciertos en Europa, Estados Unidos, Sudamérica y el Lejano Oriente.

Realizó su primera grabación en 1954. Más tarde grabó sonatas para piano de Mozart y Beethoven y conciertos de Mozart (interpretando al piano y dirigiendo), Beethoven (con Otto Klemperer), Brahms (con John Barbirolli) y Béla Bartók (con Pierre Boulez).

Tras su debut como director con la Orquesta Filarmónica de Londres en 1967, recibió invitaciones de diversas orquestas sinfónicas europeas y estadounidenses. El 15 de junio de ese mismo año, contrajo matrimonio con la chelista británica Jacqueline du Pré. En el transcurso de los últimos años de la vida de du Pré, Barenboim se instaló en París con la pianista Elena Bashkirova. Un año después de la muerte de du Pré por esclerosis múltiple en 1987, se casó con Bashkirova, con la que tuvo dos hijos, David y Michael.

Su debut como director de ópera tuvo lugar en 1973 con la representación del "Don Giovanni", de Mozart, en el Festival de Edimburgo. Entre 1975 y 1989 fue director musical de la Orquesta de París, donde dirigió numerosas piezas de música contemporánea.

En 1981 debutó en el festival de Bayreuth, realizado anualmente en homenaje a Wagner. Barenboim dirigió regularmente en esa ciudad hasta 1999, donde hizo una lectura completa de "El anillo del nibelungo" y de "Tristán e Isolda", con la mezzosoprano Waltraud Meier y el tenor Siegfried Jerusalem.

Desde 1991 hasta el 17 de junio de 2006, Barenboim fue director musical de la Orquesta Sinfónica de Chicago, cargo al que accedió en sustitución de George Solti.

El 2 de septiembre de 2001 solicitó la nacionalidad española, que le fue concedida el 25 de octubre de 2002. Desde 1980 se presentó con frecuencia en el Palacio de Carlos V con motivo de la celebración del Festival Internacional de Música y Danza de Granada. Por su vinculación con el anterior, le fue entregada la Medalla de Honor del Festival el 9 de julio de 2011.

Es además el director musical general de la Deutsche Staatsoper o Staatsoper Unter den Linden, la Ópera Estatal de Berlín conocida como "Unter den Linden" (Bajo los tilos) desde 1992.
Además de sus actividades como pianista y director de orquesta, Barenboim ha compuesto varios tangos. En diciembre de 2006 dirigió el Concierto de Año Nuevo en Buenos Aires, cuyo repertorio fue "Tango Sinfónico".

En 2008 se presentó por primera vez en el Metropolitan Opera de Nueva York, donde dirigió "Tristán e Isolda" y dio un recital de piano, el primero después de veintidós años, ya que el último había sido dado por Vladimir Horowitz

En 2009 y dirigió el Concierto de Año Nuevo de la Orquesta Filarmónica de Viena.

Fue condecorado con la Legión de honor del gobierno francés.

A partir del 10 de agosto de 2011 es candidato al Premio Nobel de la Paz por sus diversas actividades a favor de la paz y la convivencia en Oriente Próximo.

Desde la década de 1960 ha realizado presentaciones en Buenos Aires en varias ocasiones. Se presentó en el teatro Colón en 1980 con la Orquesta de París, en 1989 interpretó las variaciones Goldberg de Bach, en 1995 con la Staatskapelle Berlín, en 2000 con la Sinfónica de Chicago y en un recital de piano conmemorando 50 años de su debut en Buenos Aires, en 2002 para la integral de sonatas de Beethoven, en 2004 para los dos tomos del Clave bien temperado de Bach, en 2005 con la West-Eastern Divan Orchestra, en 2006 con un concierto multitudinario de fin de año ante 50.000 personas junto a la Filarmónica de Buenos Aires, en 2008 con la Staatskapelle Berlin y en 2010 nuevamente con la West-Eastern Divan interpretando las nueve sinfonías de Beethoven y en un concierto al aire libre para 60.000 personas y con el coro y orquesta del Teatro Alla Scala de Milán en el Teatro Colón con motivo del bicentenario argentino.

El 7 de julio de 2001, Barenboim dirigió la Staatskapelle de Berlín en la representación de la ópera de Wagner "Tristán e Isolda" en el festival de Israel celebrado en Jerusalén. Fue llamado pronazi y fascista por algunos de los presentes.

Barenboim iba a interpretar el primer acto de "La Walkiria" con tres cantantes, entre los que se encontraba el tenor español Plácido Domingo. Sin embargo, las protestas de los supervivientes del holocausto y del gobierno israelí forzaron a la organización del festival a buscar un programa alternativo. Pese a estar en desacuerdo con la decisión, Barenboim accedió a sustituir estas piezas por composiciones de Schumann y Stravinski. Finalizado el concierto, declaró que en el bis iba a interpretar una pieza de Wagner, e invitó a aquellos de los presentes que tuvieran alguna objeción a que abandonaran la sala.

En 1999 junto al escritor estadounidense de origen palestino Edward Said, al que lo unió una gran amistad, fundó la West-Eastern Divan Orchestra, una iniciativa para reunir cada verano un grupo de jóvenes músicos talentosos tanto de origen israelí como de origen árabe o español. Ambos recibieron el premio Príncipe de Asturias de la Concordia por la iniciativa.

En 2004 Barenboim recibió el Premio de la Fundación Wolf de las Artes de Jerusalén.

El 12 de enero de 2008, después de un concierto en Ramala, Barenboim aceptó también la ciudadanía palestina honoraria. Se convirtió así en el primer ciudadano del mundo con ciudadanía israelí y palestina, y dijo que la había aceptado con la esperanza de que sirviera como señal de paz entre ambos

En 2019, se realizó en el Centro Cultural Kirchner de Buenos Aires el Festival Barenboim, donde el músico estuvo acompañado por, entre otros, la pianista Marta Argerich.


Doctorados "honoris causa"

Premios Grammy

Barenboim ha recibido, como director y como pianista, seis premios Grammy: 



</doc>
<doc id="15489" url="https://es.wikipedia.org/wiki?curid=15489" title="Oncología radioterápica">
Oncología radioterápica

La oncología radioterápica es una especialidad médica con un ámbito específico de actividad quirúrgica, dedicada a los aspectos diagnósticos, cuidados clínicos y terapéuticos del enfermo oncológico, primordialmente orientada al empleo de los tratamientos con radiaciones, así como al uso y valoración relativa de los tratamientos alternativos o asociados e investigación y docencia. Emplea rayos X y rayos gamma de alta energía (fotones de alta energía), haces de electrones y otras radiaciones ionizantes para el tratamiento de ciertas clases de cáncer.

Tradicionalmente se ha llamado a esta especialidad y forma de tratamiento, radioterapia.

El enfermo oncológico es considerado en el contexto general de la enfermedad neoplásica; valorando especialmente la integración del tratamiento con radiación y tratamientos alternativos, en la secuencia diagnóstica y terapéutica del abordaje de su enfermedad. El especialista oncólogo radioterapeuta debe poseer una profunda formación clínica y conocimiento de la oncología médica, siendo su competencia la indicación, planificación, control, ejecución y seguimiento del tratamiento con radiaciones y terapias asociadas. Debe ser asimismo competente en el apoyo clínica paliativo del enfermo terminal, y para valoración y seguimiento de los pacientes oncológicos. El campo de acción se enmarca en la asistencia médica especializada, e impone que el especialista tenga acceso directo a la evaluación de pacientes, participe en la asistencia clínica multidisciplinar como son los comités de tumores, y promueva proyectos de investigación y educación postgraduada en aquellas instituciones con especial proyección académica.

La actividad y ámbito de trabajo del especialista abarca los distintos aspectos clínicos y de investigación relacionados con el cáncer y con el efecto biológico de las radiaciones y tratamientos asociados. Su actividad clínica incluye la epidemiología, prevención, patogenia, clínica, diagnóstico, tratamiento y valoración pronóstica de las neoplasias. El campo de acción clínico puede sintetizarse en tres grupos de situaciones que definen la asistencia médica propia de la especialidad:

Tiene su origen en los primeros años del siglo XX. El primer médico en emplear la radioterapia oncológica en España fue el Dr Celedonio Calatayud en 1906. 

El campo de acción instrumental incluye el profundo conocimiento y experta manipulación de todos los elementos tecnológicos que permitan desarrollar una labor asistencial adecuada a la evolución del equipamiento médico:


</doc>
<doc id="15490" url="https://es.wikipedia.org/wiki?curid=15490" title="Mole de caderas">
Mole de caderas

El Mole de Caderas o huaxmole es un platillo tradicional de carne de chivo de la región de Tehuacán, Puebla.

Es considerado uno de los platillos más importantes en los estados de Puebla y Oaxaca, debido a la prolongada crianza y cuidados en la preparación del animal -del cual se aprovecha la totalidad de la carne- y de la celebración del Festival de la Matanza que acompaña y da inicio al sacrificio de animales de crianza para la preparación de los alimentos y para la posterior conservación y curado de la carne.

En la preparación del mole de caderas se emplea la carne y hueso de la cadera, condimentos a base de sal, chile y se da un baño en limón para darle un toque especial, con un caldo de color rojo hervido con la carne de las caderas y ejotes silvestres. El sabor del platillo es característico de la carne de los chivos que son llevados durante un trayecto de un año pastando a través de las regiones del sur del estado de Puebla y del norte de Oaxaca, alimentando al ganado solo con abundantes cantidades de sal, se mantienen hidratados solo por agua. De la práctica de este tipo de crianza se obtiene carne de un sabor fuerte y característico con el cual se preparan los platillos tradicionales.

Guiso tradicional mexicano que lleva como ingredientes distintivos la cadera y el espinazo del chivo. La salsa se elabora con chiles guajillo, costeño y serrano, tomate, jitomate, hoja de aguacate, cilantro y un ejote típico de la región.

Las caderas de se cuecen en agua con cebolla, ajo, y sal; los chiles se tuestan y se preparan en salsa, y ésta se incorpora al caldo junto con hojas tostadas de aguacate; los ejotes se añaden cuando la carne está cocida.

Es típico del estado de Puebla, sobre todo en la capital, Cholula y Tehuacán. Algunos añaden guajes crudos molidos y cilantro, y lo convierten en huaxmole de caderas, aunque no se use este nombre para designarlo. Esta forma de huaxmole también se come en Oaxaca.

En los restaurantes tradicionales de Puebla se anuncia con especial insistencia cuando se prepara este mole, ya que para muchos es muy especial, al grado que un plato de mole de caderas es más caro que el mole poblano. Se elabora donde se celebra una fiesta anual, durante la época de la matanza de chivos, esto es, de octubre a diciembre.

El 20 de octubre de cada año se lleva a cabo en Tehuacán el festival de la matanza, en la que hay bailes y danzas como la denominada "danza de la matanza", donde literalmente se baila a un cabro macho para sacrificarlo al final con un tiro en la frente.

Con esta celebración da inicio la matanza, no sin antes ofrecer una ceremonia por parte de los matanceros en un altar donde se pide para que la matanza sea buena, igual o mejor que la del año pasado. Los matanceros dan paso a los chiteros y estos, a su vez, a los fritangueros de víceras.

Todo el animal es aprovechado: el espinazo y caderas son lo más cotizado por la cocina tradicional de la zona; los huesos se venden para acompañar platillos también asociados con la temporada, como el guasmole o el tesmole; las vísceras se consumen en asadura y con la piel se prepara chicharrón de chivo.

El mole por lo general es un platillo único y se acompaña con tortillas de maíz Se conoce también como mole de chivo, aunque este refiere a un guiso tradicional, pero más usual.

Las referencias históricas señalan como fecha probable del inicio de la elaboración de este mole el año 1800, época en la que hubo un aumento sin precedentes en las cabezas de ganado caprino.

Esta receta mexicana de mole de caderas de Puebla, México, lleva los siguientes ingredientes:

Para preparar mole de caderas hay que cocer las carnes con ajo y cebolla. Poner a hervir los chiles, el tomate y el jitomate. Molerlos con ajo y cebolla; freír en manteca, dejar sazonar. Incorporar la salsa a las carnes, previamente cocidas, con su caldo. Moler el huaje crudo y agregar; cuando el guiso esté hirviendo, añadir hojas de aguacate y cilantro en ramas. Hervir un momento y servir luego. Rinde 10 raciones.


</doc>
<doc id="15491" url="https://es.wikipedia.org/wiki?curid=15491" title="Excitabilidad neuronal">
Excitabilidad neuronal

La excitabilidad neuronal o impulso nervioso es la capacidad de las neuronas de cambiar su potencial eléctrico y transmitir este cambio a través de su axón. La excitación neuronal se produce mediante un flujo de partículas cargadas a través de la membrana, lo cual genera una corriente eléctrica de modo que depende de la existencia de distintas concentraciones de iones a ambos lados de la membrana celular y de la capacidad de transporte activo a través de estas membranas para generar una diferencia de potencial electroquímico dentro y fuera de la célula.

La membrana de las células está polarizada, debido a que hay un reparto desigual de cargas eléctricas entre el interior y el exterior de la célula. Esto crea una diferencia de potencial, siendo el exterior positivo respecto al interior. En el exterior, en el líquido intersticial, el anión más abundante es el cloro. En el citoplasma, los aniones más abundantes son las proteínas, que en el pH celular se ionizan negativamente. 

El catión más abundante en el líquido intersticial es el sodio, y en el citoplasma el potasio y la mayor parte de los cambios en el potencial son debidos al intercambio de estos iones.

La representación gráfica de la variación de potencial respecto al tiempo es el potencial de acción. La cantidad de estímulo necesario para provocar la actividad de una neurona, se denomina umbral de excitabilidad. Alcanzado este umbral, la respuesta es un potencial de acción independiente del estímulo. Es decir, sigue la ley del "todo o nada". Esto es debido a los canales activados por voltaje de sodio.

Durante la despolarización, la neurona no es excitable y se dice que está en periodo refractario absoluto. Durante la hiperpolarización subsiguiente, la neurona es parcialmente excitable, parcialmente refractaria, es decir, que se precisa un estímulo más intenso para provocar un nuevo potencial de acción, ya que ha aumentado el umbral de excitabilidad.




</doc>
<doc id="15494" url="https://es.wikipedia.org/wiki?curid=15494" title="Narrativa española anterior a 1936">
Narrativa española anterior a 1936

El siglo XX se inicia en España con un amplio movimiento de renovación cultural y artística que tiene dos momentos significativos: la Generación de 1898 (Miguel de Unamuno, Azorín, Ramón María del Valle-Inclán, Pío Baroja) y la llamada Generación de 1914. 

Esta renovación, no alcanza muy particularmente al relato novelístico, al que impulsa a ensayar nuevas fórmulas. Así, propicia no sólo el desarrollo de una novela de corte psicológico, sino de una novela lírica en la que predomina la expresión de la subjetividad. Relacionada con esta actitud hay que considerar el escaso interés que los escritores de este periodo muestran hacia el relato tradicional de acontecimientos según un orden cronológico; y ello a pesar del enorme éxito de otro conjunto de narradores que se ciñen a los modos clásicos del relato para ponerlos ya al servicio del entretenimiento o la mera diversión, ya al del impulso reformista y social (Blasco Ibáñez, Felipe Trigo, v.gr.).
La ruptura del relato tradicional se logra mediante una gran variedad de procedimientos estructurales y estilísticos más o menos innovadores: 

Esta línea renovadora la prolongarán los escritores del 14, muy especialmente Ramón Pérez de Ayala, Gabriel Miró y Ramón Gómez de la Serna- sin desistir aún en su afán de encontrar un punto de equilibrio entre el realismo y el experimentalismo aislador. El resultado es la creación de un corpus novelístico que conjuga el acceso a un público potencialmente amplio con una exigencia de valoración estética. Y eso sin que se diluya en su totalidad la marcada preocupación reformista y social que tiñe la actividad de gran parte de los autores e intelectuales del momento.

El clima cultural en el que surge la joven novelística del 27 se caracteriza, pues, por una actitud antirrealista y por un decidido afán experimental. 
Esta nueva narrativa se congregó en la serie Nova Novorum de la Revista de Occidente. Allí se fragua un tipo de relato que ensaya la incorporación a la narración 

Se trata, por tanto, de una novela en la que la narración se libera de la dependencia de la historia, que rompe con la disposición lineal del tiempo, y 
que abre un amplio espacio para el distanciamiento irónico o humorístico. 

Toda la narrativa del 27 se puede ordenar en dos grandes vertientes: la novela lírico-intelectual (Benjamín Jarnés, Antonio Espina, Mauricio Bacarisse, Francisco Ayala, Pedro Salinas) y la humorística (Jardiel Poncela, Edgar Neville).

Sin embargo, la crítica ha ignorado, cuando no despreciado, la importancia de este relevante grupo de escritores que sintoniza perfectamente con las modernas tendencias europeas de la época.

Pese a la repercusión de las Vanguardias, entre finales de la década de los 20 y 1935 surge una generación de narradores que, opuesta al arte deshumanizado, cultiva una novela realista y de finalidad social. Esta nueva generación se propone una manifiesta rehabilitación de lo humano, del valor testimonial y de la trascendencia moral y política de la literatura. Figura clave en esta evolución de la novela es José Díaz Fernández. Junto a él, son considerados precursores de la narrativa comprometida Joaquín Arderíus, Ramón J. Sender y César Arconada, entre otros.

Véase también:
Literatura española contemporánea


</doc>
<doc id="15496" url="https://es.wikipedia.org/wiki?curid=15496" title="Violín">
Violín

El violín (del italiano violino, diminutivo de viola) es un instrumento de cuerda de la familia del violín. Quién lo toca recibe el nombre de violinista.

En los violines antiguos, las cuerdas eran de tripa. Hoy pueden ser también de metal o de tripa entorchada con aluminio, plata o acero; la cuerda en mi, la más aguda ―llamada cantino― es directamente un hilo de acero, y, ocasionalmente, de oro. En la actualidad se están fabricando cuerdas de materiales sintéticos que tienden a reunir la sonoridad lograda por la flexibilidad de la tripa y la resistencia de los metales.
Además del efecto logrado por el arco sobre las cuerdas, se pueden conseguir otros: pizzicato (pellizcando las cuerdas como en el arpa o la guitarra, pero con otra posición), trémolo (moviendo el arco arriba y abajo muy rápido), vibrato (oscilando ligeramente los dedos sobre las cuerdas), glissando (deslizando los dedos de una posición a otra), col legno (tocando con la parte de madera del arco), sul ponticello (tocando cerca del puente), sul tasto (tocando sobre el diapasón), etcétera.

Las partituras de música para violín usan siempre la clave de sol, llamada antiguamente «clave de violín.

Las cuerdas se afinan por intervalos de quintas:


"El número está indicado de acuerdo con el índice acústico internacional, según el cual el do central es un do. Este índice se utiliza en todo el mundo excepto México" (la 440→la) "y los países regidos por el índice acústico franco-belga" (la 440→la).

La cuerda de sonoridad más grave es la de "sol", y luego le siguen, en orden creciente, el "re", "la" y "mi".
En el violín la primera cuerda en ser afinada es la del "la"; esta se afina comúnmente a una frecuencia de 440 Hz, utilizando como referencia un diapasón clásico de metal ahorquillado o, desde finales del siglo XX, un diapasón electrónico. El diapasón ha tendido a subir en los últimos años y se sitúa más comúnmente en los 442 Hz en la actualidad, e incluso más arriba en las orquestas norteamericanas.

El cuerpo del violín posee una forma abombada, con silueta estilizada determinada por una curvatura superior e inferior con un estrechamiento a la cintura en forma de C. Las tapas del violín se modelan con suaves curvas que proporcionan la característica de abovedado. Los aros, que van alrededor del violín dando la silueta, son de poca altura, el mástil posee cierto ángulo de inclinación hacia atrás respecto al eje vertical, longitudinal y se remata por un caracol llamado colocho o voluta. La estructura interna del violín la constituyen dos elementos fundamentales en la producción sonora del instrumento dados por la barra armónica y el alma. La barra armónica corre a lo largo de la tapa justo debajo de las cuerdas graves y el alma está ubicada justo debajo del pie derecho del puente donde se ubican las cuerdas agudas.

El arco es una vara estrecha, de curva suave y construida idóneamente en la dura madera del palo brasil o «de Pernambuco» ("Caesalpinia echinata"), de unos 77 cm de largo, con una cinta de 70 cm constituida por entre 100 y 120 (con un peso de unos 60 gramos según longitud y calibre) crines de cola de caballo, siendo las de mejor calidad las llamadas "Mongolia", que provienen de climas fríos donde el pelo es más fino y resistente. Tal cinta va desde una punta a la otra del arco. Para que las cuerdas vibren y suenen de un modo eficiente, la cinta de cola de caballo del arco debe ser frotada adecuada y regularmente con una resina llamada colofonia (en España se llama ""perrubia"", de "pez-rubia"). También, actualmente ―muchas veces para abaratar costos―, la crin blanqueada de caballo es sustituida por fibras vinílicas. El arco del violín tiene en la parte por la que es tomado un sistema de tornillo que al hacer desplazar la pieza por la cual se aferra un extremo de la cinta de crin hace que se tense o se distienda.

Los violines se clasifican de acuerdo con su tamaño: el 4/4 ―cuya longitud suele ser de 14 pulgadas o 35,5 cm y su ancho máximo de 20 cm, y un alto de 4,5 cm― es el más grande y es el utilizado por los adultos; le siguen violines de tamaño menor, destinados a jóvenes y niños, denominados 3/4, 2/4 y 1/4. Existe también un violín de tamaño 7/8, llamado también "Lady", que es utilizado por algunas mujeres o por varones adultos de manos pequeñas. El tamaño del violín va de acuerdo al tamaño (longitud) de la mano.

La genealogía que lleva al violín actual es más compleja. Se encuentra en el frotamiento de las cuerdas del laúd y el "rebab" ―y su versión europea, el rabel―, instrumentos difundidos en la Europa mediterránea durante la expansión medieval de la cultura árabe. En Italia, a partir de la lira bizantina o el "rebab", surgen los antecedentes más evidentes, tanto del violín como de la llamada viola da gamba; son tales precedentes la viola de arco (nombre que se utilizaba para todo instrumento de cuerda frotada con arco, como el "rebec"o rabel, y que también recibe las denominaciones de viela, vihuela, vihuela de arco, fídula y giga) y la lira o "viola da braccio", está ya muy semejante a un violín o viola primitivos, aunque con el diapasón separando los bordones. Es en el siglo XVI que aparece el violín propiamente dicho, aunque con algunas diferencias respecto a la mayoría de los violines que se vienen fabricando desde el siglo XIX. La tapa superior y las tablas laterales se hacen de madera blanda, mientras que la tapa inferior se hace de madera dura. En el norte de Italia la ciudad de Cremona se hallaba entre un bosque de abetos (madera blanda) y uno de arce (madera dura), por lo que estas maderas eran las usadas por los grandes maestros violeros. El arco ha sufrido muchas modificaciones. El modelo actual data del siglo XIX, cuando François Tourte le dio una curvatura cóncava, que en los modelos más primitivos era convexa, como la del arco de cacería.

Aunque en el siglo XVII el violín ("violino") se encontraba bastante difundido en Italia, carecía de todo prestigio (el laúd, la vihuela, la viela, la viola da gamba, la guitarra, la mandolina eran mucho más considerados). Sin embargo, Claudio Monteverdi es uno de los que descubren la posibilidad de las calidades sonoras del violín, y es por ello que lo usa para complementar las voces corales en su ópera "Orfeo" (1607). Desde entonces el prestigio del violín comienza a crecer. Hacia esa época comienzan a hacerse conocidos ciertos fabricantes de violines (llamados aún luteros o lauderos, o "luthiers" —más frecuentemente que "violeros"— ya que inicialmente se dedicaron a la fabricación de laúdes). Así se hacen conocidos Gasparo Bertolotti de Saló, o Giovanni Maggini de Brescia, o Jakob Steiner de Viena; sin embargo, una ciudad se hará celebérrima por sus lauderos especializados en la confección de violines: Cremona. En efecto, de Cremona son los justamente afamados Andrea Amati, Giuseppe Guarneri, Antonio Stradivari (sus apellidos suelen ser más conocidos en su forma latinizada: "Amatius", "Guarnerius", "Stradivarius") y el mismísimo Claudio Monteverdi. Durante el siglo XIX se destacaron François Lupot y Nicolas Lupot. Es a partir de entonces, y sobre todo con el barroco, que se inicia la Edad de Oro (al parecer de allí en más perpetua) del violín.

Desde entonces el violín se ha difundido por todo el mundo, encontrándose incluso como «instrumento tradicional» en muchos países no europeos, desde América hasta Asia. El violín es un instrumento protagonista en las orquestas, grupos de cámara etc. Especial atención ha recibido en la música árabe, en la que el ejecutante lo toca apoyado en la rodilla cual si fuera un chelo, y en la música celta irlandesa, donde el instrumento recibe el nombre de "fiddle" (derivado del italiano "fidula"), y sus músicas derivadas como, en cierto grado, el "country". 

En cuanto al secreto de la sonoridad típica de los violines realizados por las familias Stradivarius y Guarneri, existen hoy diversas hipótesis que, más bien que excluirse, parecen sumarse; en primer lugar se considera que la época fue particularmente fría, motivo por el cual los árboles desarrollaron una madera más dura y homogénea. A esto se suma el uso de barnices especiales que reforzaban la estructura de los violines. También se supone que los troncos de los árboles eran trasladados por ríos cuyas aguas tenían un pH que reforzaba la dureza de las maderas; también influye un comprobado tratamiento químico (acaso más que con el objetivo de la sonoridad, el de la conservación) de los instrumentos, que reforzó la dureza de las tablas. Por último, ciertos violines Stradivarius tienen en sus partes internas un acabado biselado de los contornos en donde contactan las maderas, el cual parece beneficiar la acústica de estos violines.

El violín consta principalmente de una caja de resonancia que posee elegantes y hermosas formas ergonómicas (de sección oval con dos estrechamientos cerca del centro). Tal caja de resonancia está constituida por dos tablas: la tabla armónica y la tabla del fondo (tradicionalmente hecha con madera de arce), las cubiertas laterales o "aros" y la tabla superior o "tapa armónica" (tradicionalmente de madera de abeto blanco o rojo); la tapa se encuentra horadada simétricamente y casi en el centro por dos aberturas de resonancia llamadas "oídos" o "eses", ya que en el tiempo de su diseño se usaba aún en la escritura o imprenta la S larga, semejante a una "efe" cursiva pero sin el travesaño horizontal, y en desuso a partir del siglo XVIII. Por la misma razón, actualmente se tienden a llamar "efes".

En el interior de la caja se encuentra el poste sonoro o "alma" del violín, que es una pequeña barra cilíndrica de madera dispuesta perpendicularmente entre la "tapa" y la "tabla armónica" del lado derecho del eje de simetría de la caja (esto es: prácticamente abajo, hacia la derecha, de la zona en donde se apoya el "puente"), del lado contrario al "alma", a lo largo de la cara interna de la "tapa", se encuentra adherido con cola un listón llamado "barra armónica". Tanto el "alma" como la "barra armónica" cumplen dos funciones: ser soportes estructurales (el violín sufre mucha tensión estructural) y transmitir mejor los sonidos dentro de la caja de resonancia.

La caja de resonancia tiene, en el violín de orquesta, 35,7 cm de longitud, y se encuentra orlada por rebordes en ambas tablas; tales rebordes cumplen, además de una función decorativa, la función de reforzar el instrumento.

Por fuera, la caja de resonancia se continúa por el "mango" o "astil"; el mástil o "mango" concluye en un "clavijero", oquedad rectangular en la que se insertan las cuerdas anudadas y tensionadas allí mediante sendas "clavijas" para cada cuerda, las clavijas son como llaves simples de sección ligeramente conoidal; luego del clavijero, un remate llamado ―por su forma― "voluta" (aunque en ciertos casos la voluta se encuentra sustituida por otras formas, por ejemplo una cara humana o la figuración de una cabeza de león).

En cierto ángulo, las líneas de la voluta, en perspectiva, hacen una línea recta y continua con las cuerdas, especialmente mi y sol, y se juntan en el horizonte. Esto permite saber, cuando el violín está puesto en el hombro, cuándo se encuentra correctamente recto.

Sobre el mango se ubica el diapasón del violín o "tastiera", este suele ser de ébano ya que esta madera produce ese sonido "maderil" que los instrumentos de cuerda frotada requieren además el ébano es sumamente duro y denso por lo que la fricción de las cuerdas no daña el diapasón. En violines antiguos pueden encontrarse tastieras de marfil.

Sobre la tapa de la caja se encuentra el "ponticello" o "puente" el cual mantiene elevadas las cuatro cuerdas, en la parte posterior de la caja de resonancia, unida a ella por un nervio flexible que se engancha a un botón, se encuentra otra pieza (tradicionalmente de madera de ébano) de forma triangular llamada el "cordal", como su nombre lo indica, el "cordal" sirve para retener las cuatro cuerdas, estas se apoyan en los siguientes puntos: los orificios del cordal, el ponticello, la cejilla ubicada sobre el astil y las clavijas.

Cuando se quiere atenuar el sonido, se aplica sobre el "puente" una especie de tabique llamado "sordina".

Desde fines de siglo XIX es común añadir a la parte trasera de la caja de los violines una "mentonera" o "berbiquí" desmontable, aunque tal aditamento "no" es indispensable (la invención de este añadido se atribuye a Louis Spohr); en cambio sí es de bastante importancia el barniz (Tradicionalmente "gomalaca" diluida en alcohol) con el cual se recubre, en su parte externa, a la mayor parte del violín.

La singular acústica del violín ha sido muy estudiada durante todo el siglo XX, destacándose las investigaciones del alemán Ernst Chladni, del cual deriva toda una formulación llamada "esquema de Chladni".

La manera de sostener tanto el violín como el arco es una parte importante en la enseñanza del instrumento para lograr una buena técnica de ejecución, por lo tanto debe tener una primordial consideración al empezar el estudio del instrumento.
Lo primero a tomar en cuenta en la posición del violín, es que este debe sostenerse de tal manera que los ojos se puedan fijar en la cabeza del violín; y a su vez el brazo izquierdo debe acomodarse ligeramente hacia adelante para que los dedos se coloquen de manera natural y perpendicular al diapasón. De mismo modo es importante la correcta y relajada colocación del instrumento entre el cuello y el hombro, tanto para el logro de un buen sonido y ejecución como para proteger al instrumentista de lesiones por tensiones innecesarias. Debe ser colocado lo más alto posible para que el brazo izquierdo, la mano y los dedos tengan libertad de movimiento para poder cambiar de posición con facilidad. 

Instrumento de singular resistencia, el violín suele requerir de pocos cuidados especiales. Cuando no se usa debe estar guardado en un estuche lo más hermético y acolchado posible, con la caja, la vara del arco y las cuerdas limpias, y las crines del arco levemente distendidas. El violín ha de estar al resguardo en todo lo posible para que no le afecte la humedad ni cambios bruscos de temperatura; por lo demás, solo requiere una habitual limpieza con un paño seco, o bien con productos especialmente diseñados para ello. Las cuerdas suelen romperse por la tensión y la fricción, y por este motivo es conveniente que el violinista tenga un juego de cuerdas de repuesto. También suelen romperse los pelos de cola de caballo (crines) que constituyen la cinta del arco; por este motivo la ejecución frecuente puede obligar a su recambio cuando fuese necesario. Si se ejecuta el violín sin la barbada o mentonera, conviene usar un pañuelo en la parte del cuello y mentón en la cual se apoya el violín para evitar que el instrumento se vea afectado por la transpiración. Suele ocurrir que un violín "viejo" que haya sido bien ejecutado, suene mejor que un violín nuevo o con poco uso.

Es importante en el cuidado del violín que al guardarse durante un período largo de tiempo las cuerdas sean aflojadas para no quedar en tensión. Con esto la estructura del violín quedará protegida de posibles rajaduras por una tensión innecesaria.

Desde la segunda mitad del siglo XX las cuerdas y la cinta del arco, en muchos casos, están siendo fabricadas con materiales sintéticos; y el uso de estos materiales también se ha extendido a otras partes en el caso de los violines fabricados en serie: por ejemplo cordales, mentoneras, tastieras, que están siendo fabricados con material plástico lo cual afecta la sonoridad característica del instrumento, y por ello con cierta detracción de los violinistas profesionales. En el caso de los violines eléctricos, casi todos sus componentes son sintéticos, pero en ellos el sonido (diverso del de los acústicos) es elaborado electrónicamente; tales violines suelen usarse en conjuntos de pop, "rock", "jazz" y afines.

La introducción hacia fines del siglo XVI e inicios del XVII del violín en el ámbito del Cono Sur se debe principalmente a los religiosos jesuitas y franciscanos, muchos de ellos nacidos italianos como Domenico Zipoli, cuyo nombre lleva una famosa escuela de música cordobesa.

Los jesuitas introdujeron la enseñanza musical en las reducciones creadas en territorios que hoy pertenecen a la Argentina, Paraguay, Bolivia y el sur de Brasil, en una región poblada en los citados siglos por indígenas entre los cuales preponderaba la cultura guaraní. La mayor parte de esas pequeñas ciudades fue destruida con la expulsión de los jesuitas, en 1767 en la colonia española, precedida por una decisión del reino de Portugal. Las misiones jesuíticas de Bolivia son las únicas que se salvaron de la destrucción que sobrevino a la expulsión de los religiosos. Se trata de siete ciudades en la región conocida como Chiquitania donde anualmente se realiza un festival de música barroca.

En Brasil, el violín artesanal conocido por el nombre de "rabeca" fue introducido también por los religiosos, especialmente en la zona de las misiones jesuíticas, pero su utilización en la música se desarrolló más intensamente durante la breve presencia colonizadora del holandés Mauricio de Nassau, en Recife, entre 1637 y 1643. Otro importante estímulo representó la instalación de la Corte portuguesa en Río de Janeiro en 1807.

Actualmente, la utilización de la rabeca como instrumento melódico es común en la música de la región nordeste y también en el norte amazónico. En la ciudad amazónica de Bragança, en el estado de Pará, la tradición de la rabeca recibió un notable impulso por parte del poder público que ayudó a instalar una escuela para la enseñanza del instrumento, basada en el conocimiento y la técnica de los maestros locales.

Dentro de los folclores sudamericanos el violín es particularmente relevante en el folclore de Argentina y en zonas aledañas, donde fue utilizado en la música religiosa, aunque rápidamente las poblaciones criollas y autóctonas supieron utilizarlo para músicas profanas. Así es que en gran parte del norte argentino y el sur de Bolivia, el violín (e incluso una variante más rústica que ha mantenido el arcaico nombre de "rebab") es uno de los instrumentos musicales principales, tras la guitarra y el bombo. Con el violín se suelen acompañar los gatos, chacareras, las cuecas bolivianas y en menor medida chamamés, zambas y polcas criollas.Música de origen folclórico, el tango cuenta con el violín como uno de sus principales instrumentos. El violín de tango suele ser el mismo que el violín de concierto para la llamada música clásica, en cambio los violines de las otras músicas mencionadas anteriormente suelen ser violines "criollos", de formas muy semejantes al violín clásico, aunque la gran diferencia se encuentra en las maderas con que están confeccionados (algarrobo criollo y mistol o chañar por ejemplo); en gran parte de Argentina (especialmente en el NOA) a los músicos especializados en tocar el violín "no" se les dice violinistas sino "violinistos" o "violistos", en el noreste es frecuente el término "violinero" (que sin embargo suele aplicarse más al "luthier"). Las etnias de ascendencia directamente aborigen también suelen confeccionar interesantes tipos de "violines", por ejemplo entre los qom'lek (o tobas) son característicos los "violines" fabricados a partir de una lata cuadrangular de aceite comestible a la cual se le aplica un mango de leño, las cuerdas suelen ser realizadas con tripa aunque más modernamente se realizan con los cables de metal que se obtienen de los sistemas de frenos de bicicletas; teniendo tales violines una entonación llamada "m'biké", tal entonación, se considera, es similar a la que poseían los violines europeos en el siglo XVI.

En Venezuela se utiliza principalmente en la región de Los Andes para ejecutar bambucos y valses de la región. 

En México, su uso se extiende al son huasteco, huapango, música calentana, música planeca y mariachi. En España, se utiliza en los verdiales. En los países anglosajones, al violín folclórico se le denomina "fiddle".

En Chile, la única región en la que el violín fue introducido de manera tradicional en la música folckórica es Chiloé, llegando a generarse una variante de este instrumento conocido como "violín chilote", el cual, aparte de incorporar el uso de maderas nativas de la Patagonia chilena en la lutheria de violines, como alerce, coigüe y ciruelillo, presenta una caja acústica más plana y de mayor tamaño que el violín docto, dándole un sonido característico. Una variante del violín chilote que ocupaba tripas de carnero como cuerdas es conocida como "Barraquito", siendo común escuchar ambos instrumentos en danzas como la "Pericona" y en pasacalles en honor a santos y vírgenes. En Chiloé es común encontrar también al rabel como parte de los instrumentos musicales tradicionales.

Nicoló Paganini creó una mixtura muy interesante entre la relación del humano con el violín, cuenta la historia que a su madre Teresa Bocciardo para decirle, que su hijo estaba destinado a ser el más importante violinista del mundo,en los pasillos musicales de Italia se hablaba del "diabólico talento" de Paganini, quien culminó esta etapa de la percepción de su pacto componiendo dos grandes obras mejor conocidas como "24 Caprichos para violín" y "Concierto para violín n.º 1 (Paganini)".




</doc>
<doc id="15497" url="https://es.wikipedia.org/wiki?curid=15497" title="Geografía de Indonesia">
Geografía de Indonesia

Indonesia es un archipiélago formado por más de diez mil islas, de las que 8.844 tienen nombre y 922 están habitadas. Comprende cinco islas principales: Sumatra, Java, Borneo, Sulawesi y Nueva Guinea, dos grupos mayores de islas: las islas menores de la Sonda y las islas Molucas, y sesenta grupos más pequeños de islas. Cuatro de las islas son compartidas con otros países: Borneo con Malasia y Brunéi; Sebatik, al noroeste de Kalimantan, con Malasia; Timor con Timor Oriental, y las recién divididas provincias de Papúa y Papúa Occidental comparten la isla de Nueva Guinea con Papúa Nueva Guinea.

Una extensión de agua relativamente abierta (formada por los mares de Java, Flores y Banda) divide la mayor parte de las islas de Indonesia en dos hileras desiguales de islas: al sur, las islas (comparativamente largas y estrechas) de Sumatra, Java y Timor entre otras, y al norte, Borneo, islas Célebes, el archipiélago de las Molucas y Nueva Guinea.

Una cadena de montañas volcánicas, que alcanza altitudes superiores a los 3.700 m, se extiende de oeste a este, a través de las islas meridionales desde Sumatra hasta Timor. Los puntos más elevados de esta cadena son el Kerinci (3.800 m) en Sumatra, y el Semeru (3.676 m), en Java. Cada una de las islas septentrionales principales tiene una masa montañosa central y llanuras en torno a la costa. Puncak Jaya (5.030 m), en la cadena montañosa Surdiman de Irian Jaya, es la cima más elevada del país. Las zonas con mayor extensión de tierras bajas son Sumatra, Java, Borneo e Irian Jaya. Durante siglos las periódicas erupciones volcánicas de los numerosos volcanes activos han depositado ricos suelos en las tierras bajas, sobre todo en Java. Muchos continúan activos y también se producen . Uno de los más destructivos fue el terremoto del océano Índico de 2004, con epicentro cerca de Sumatra, que afectó a toda la cuenca del océano Índico y provocó la muerte de más de 200.000 personas.

Las islas principales de Sumatra, Java, Madura y Kalimantan se apoyan sobre la placa de la Sonda y se agrupan, junto con Sulawesi, en las islas mayores de la Sonda. En el extremo oriental de Indonesia se encuentra Nueva Guinea, que yace sobre la placa Australiana. La profundidad marina en las placas de Sonda y Sahul tiene una media de 300 m o menos. Entre estas dos placas tectónicas se encuentran Sulawesi, las islas menores de la Sonda y las islas Molucas, las cuales forman un segundo grupo de islas con una profundidad en los mares que las rodean de hasta 4.500 m.
La isla de Sulawesi se halla sobre tres placas separadas, la placa del Mar de Banda, la placa del Mar de las Molucas y la placa de Sunda o de la Sonda. La actividad sísmica y volcánica es elevada en la parte norte, evidenciada por las formaciones volcánicas en la provincia de Célebes Septentrional en el norte de Sulawesi y el arco de islas entre las que se hallan las islas Sangihe y las islas Talaud, al sudoeste de la fosa de Filipinas. 

Las islas menores de la Sonda consisten en dos hileras de islas que se estiran hacia el este desde Bali hacia el sur de Molucas. El arco interior de las islas menores de la Sonda es la continuación del cinturón alpino de montañas y volcanes que se extiende desde Sumatra a través de Java, Sumatra y Flores, y desaparece en las volcánicas islas de Banda, las cuales, junto con las islas Kai, las islas Tanimbar y otras islas pequeñas del mar de Banda son típicos ejemplos de la mezcla Wallacea de plantas y animales de Asia y Australasia. El arco exterior de las islas menores de la Sonda es una extensión geológica de la cadena de islas occidental de Sumatra que incluye Nias, Mentawai y Enggano. Esta cadena resurge en las islas menores de la Sonda en las abruptas montañas de las islas de Sumba y Timor.

Las islas Molucas son geológicamente de las más complejas de las islas indonesias, formada por cuatro placas tectónicas. Están localizadas en el nordeste del archipiélago, rodeadas por el mar de Filipinas al norte, Papua al este y las islas menores de la Sonda al sudoeste. Las islas mayores, Halmahera, Seram y Buru se elevan abruptamente desde un profundo mar y tienen una vegetación Wallacea única. Apenas hay llanuras costeras. Al sur se halla el mar de Banda. La convergencia entre la placa del mar de Banda y la placa Australiana crea una cadena volcánica llamada Arco de Banda.

Los geomorfólogos creen que la isla de Nueva Guinea es parte del continente australiano, ya que ambos están sobre la placa de Sahul y estaban unidos por tierra durante el último periodo glacial. El movimiento tectónico de la placa de Australia ha creado elevadas montañas que forman una cadena en el centro de la isla de este a oeste. y cálidas llanuras aluviales a lo largo de las costas. La Cordillera Central discurre a lo largo de 650 km de este a oeste, formando una espina dorsal entre el norte y el sur de la isla. Debido a estos movimientos, Nueva Guinea experimenta numerosos terremotos y tsunamis, especialmente en las partes norte y septentrional.

La mayor parte de las grandes islas son montañosas, con picos que oscilan entre 2.000 y 3.000 m sobre el nivel del mar en Sumatra, Java, Bali, Lombok, Sulawesi y Seram. Las cimas más altas del país se encuentran en las montañas Jayawijaya, en Nueva Guinea, y en las montañas Sudirman, en Papúa. El pico más alto, el Puncak Jaya, de 4.884 m, se encuentra en las montañas Sudirman. Una fila de volcanes se extiende desde Sumatra a las islas menores de la Sonda, y entonces realiza un giro hacia las islas Banda de las Molucas, al nordeste de Sulawesi. De los 400 volcanes, unos 150 están activos. Dos de las más violentas erupciones en tiempos históricos han tenido lugar en Indonesia; en 1815, una erupción en el monte Tambora, en Sumbawa, mató a 92.000 personas, y en 1883, el Krakatoa mató a 36.000. Mientras la ceniza volcánica resulta ser muy positiva para la agricultura, el riesgo de erupciones y terremotos hace que las condiciones sean impredecibles en muchas zonas.

Indonesia tiene una actividad tectónica y volcánica relativamente elevadas. Yace sobre la convergencia entre las placas de Eurasia, Indoaustraliana, Pacífica y del mar de Filipinas. La megafalla de Sonda es una falla de 5.500 km de longitud localizada frente a las costas sur de Sumatra, Java y las islas menores de la Sonda, donde la placa Índica empuja hacia el nordeste subduciendo la placa de Sonda. El movimiento tectónico en esta placa es responsable de la creación de la fosa de la Sonda o de Java y las cordilleras a través de Sumatra, Java y las islas menores de la Sonda. En la vecindades de la falla se producen numerosos grandes terremotos, como el terremoto del océano Índico de 2004. El monte Merapi, situado en la porción de Java de la megafalla, es el volcán más activo de Indonesia y es conocido como uno de los Volcanes de la Década, debido al riesgo que supone para las pobladas áreas vecinas.

La parte norte de Sulawesi y las islas Molucas yacen en la convergencia de la placa de la Sonda y la placa del Mar de las Molucas, dando lugar a una activa región tectónica con cadenas volcánicas como las de las islas Sangihe y Talaud. El norte de las Molucas y el oeste de Nueva Guinea se hallan en la convergencia de las placas Cabeza de Pájaro, mar de Filipinas y de las Carolinas. También es una activa región volcánica, con el terremoto de Papúa de 2009, de categoría 7,7 Mw como el terremoto más reciente de la región.

El rico suelo volcánico de Indonesia es ideal para el desarrollo de los cultivos; los bosques se extienden por su superficie y cubren aproximadamente dos tercios del territorio. 

Los principales recursos del país son el estaño, la bauxita, el petróleo, el gas natural, el cobre, el níquel y el carbón; también cuenta con pequeñas cantidades de plata, diamantes y rubíes. La pesca es abundante y del mar se obtienen también perlas, conchas (carey) y agar, una sustancia que se extrae de las algas.

Las cálidas aguas alrededor de Indonesia y el hecho de poseer cerca de un millar de islas a lo largo del ecuador, entre el Sudeste Asiático y Australia, con más de 5.000 km de oeste a este, proporcionan un clima suave y ecuatorial, cálido y húmedo todo el año, con tormentas o chaparrones que a veces causan inundaciones. Las temperaturas promedio son de 28 °C a lo largo de las planicies costeras y de 23 a 26 °C en las montañas del centro de las islas. La humedad relativa oscila entre el 70 y el 90%. Los vientos son moderados, el monzón, en general, sopla del sudoeste entre junio y septiembre, y del nordeste entre diciembre y marzo. Los tifones y las tormentas rara vez son un peligro, las corrientes entre las islas son el mayor riesgo para la navegación, especialmente en el estrecho de Lombok.

Dependiendo del lugar, hay una época más o menos seca. El día dura 12 horas todo el año, pero en las zonas montañosas, muy nubosas, apenas se ve los rayos del sol.

Las lluvias son más abundantes en la costa y las vertientes occidentales de Sumatra, las laderas meridionales del oeste de Java, casi todo Borneo (excepto el sudoeste) y gran parte del oeste de Nueva Guinea Occidental. En el sudoeste de Sumatra caen en torno a 4.000 mm al año, y entre octubre y diciembre caen más de 400 mm cada mes. En Padang, capital de Sumatra Occidental, con una temperatura media anual de 27 C, con máximas de 27,5 C y mínimas todo el año de 22,5 C, con una variación mínima de 1 grado entre estas variables, caen 4.040 mm, con un mínimo de 200 mm en septiembre y 500 mm en noviembre.¡, superando los 400 mm entre octubre y noviembre.

En Borneo, el clima también es muy húmedo, y resulta difícil encontrar un mes en que caigan menos de 250 mm. Sin embargo, hace más calor. En Kalimantan caen entre 2.500 y 3.200 mm, pero en Pontianak, por ejemplo, las temperaturas oscilan entre los 32-33 C de media de las máximas y los 23 C de media de las mínimas.

Llueve menos al norte de Sumatra, sudeste de Borneo, extremo norte de Sulawesi, islas Maluku y la parte más occidental de Nueva Guinea, con clima subecuatorial o tropical de sabana (según Köppen(, ya que hay una parte del año en que no se superan los 100 mm mensuales. En Banda Aceh, al norte de Sumatra, caen unos 2.000 mm al año, con dos periodos de unos 100 mm mensuales entre junio y agosto y febrero y marzo. En el norte de Sulawesi, esto sucede entre julio y septiembre, pero en las islas Maluku, se invierte la situación, con más de 400 mm esos meses y en torno a 100 mm de octubre a febrero. En la capital de las Molucas, Ambón, caen 2.615 mm al año, en junio y julio se superan los 400 mm, y entre octubre y febrero apenas se superan los 100 mm.

El lugar más seco de Indonesia, sin embargo, se encuentra en la zona sur, entre Yakarta, las islas de Bali, Komodo y Timor, y el sur de Nueva Guinea. En Yakarta, al norte de Java, con temperaturas entre 24 y 32 C todo el año, caen unos 1.800 mm, con menos de 100 mm entre junio y septiembre, y un mínimo de 45 mm en agosto. En Bandung, también en Java, pero a 700 m de altitud, las temperaturas oscilan entre 17 y 28 C y caen unos 2.160 mm al año, con 68 mm en agosto y 290 mm en diciembre. En la isla de Bali, al este de Java, caen unos 1.700 mm anuales, y las lluvias disminuyen hacia el este: islas de Lombok, Sumbawa, [Isla Komodo|Komodo]], Flores, Sumba, caen unos 1.300-1.400 mm, y en zonas de Sumba, en el norte y el este caen unos 800 mm, con un periodo seco de mayo a noviembre.

En la isla de Timor caen entre 1.200 y 1.500 mm. Al este de Timor, al sur de las islas Molucas, vuelve a llover y se superan los 2.000 mm, con un periodo seco de agosto a octubre. En las islas Tanimbar, de las Molucas, caen 2350 mm anuales, con 50 mm en agosto y septiembre, y más de 300 mm entre diciembre y marzo. Por su parte, en Macasar, en la isla de Célebes, con 2.875 mm, caen menos de 100 mm entre junio y octubre, con 14 mm en agosto, y más de 500 mm entre diciembre y febrero, con casi 700 mm en enero. Las temperaturas, entre 23 y 32 C son muy estables.

En la cadena de volcanes de la isla de Java, que alcanza los 4.884 m en el monte Jaya, el número de tormentas aumenta. En los alrededores de Bogor, apodada la 'ciudad de la lluvia' en las cercanías del monte Salak, de 2.211 m, caen unos 4.000 mm al año y se da el mayor número de tormentas del planeta, hasta 322 por año. Las lluvias se producen en la cara norte del volcán entre diciembre y marzo, y en la cara sur entre junio y septiembre, dependiendo del monzón. Nueve meses superan los 300 mm.

Entre noviembre y mayo, los ciclones pueden afectar el sur de las islas, y entre abril y diciembre, al norte del ecuador, pero la incidencia es pequeña. Cuando se produce el fenómeno de El Niño, hay sequía entre junio y agosto, pero puede prolongarse hasta septiembre e incluso noviembre. La Niña produce un enfriamiento entre junio y agosto.

Las principales islas se recogen en la Tabla que sigue:

Hay además, muchas otras islas de menor importancia, que por área geográfica son: 





El elevado poblamiento y la rápida industrialización presentan serios problemas medioambientales, a los cuales se da escasa prioridad debido a los altos niveles de pobreza y un gobierno débil y escaso de recursos. Los problemas incluyen la deforestación a gran escala (mucha ilegal) y los incendios relacionados que causan un intenso esmog en partes del oeste de Indonesia, Malasia y Singapur; la sobre explotación de recursos marinos, y los problemas medioambientales asociados con la rápida urbanización y el desarrollo económico, incluyendo la contaminación atmosférica, la congestión de tráfico, la gestión de residuos y de las aguas residuales.

La deforestación y destrucción de suelos vegetales convierte a Indonesia en el tercer emisor de gases de invernadero. La destrucción de hábitats amenaza la supervivencia de especies indígenas y endémicas, incluyendo 140 especies de mamíferos identificados por la IUCN como amenazados, y 15 identificados como críticos, incluyendo el orangután de Sumatra.

En 1970, el 15% de los indonesios vivía en ciudades, contra el 30% actual, con la consiguiente presión sobre el medio ambiente urbano. La contaminación industrial es especialmente preocupante en Java, así como el aumento del número de vehículos a motor. Muy pocos indonesios tienen acceso a agua potable de fiar, y la mayoría tiene que hervir el agua antes de beberla.

Según la IUCN, en Indonesia hay 733 áreas protegidas que ocupan unos 231.946 km, el 12,17% del territorio, además de 181.848 km de áreas marinas, el 3,06% de los 5.947.954 km que pertenecen al país. De estas, 49 son parques nacionales, 1 es un área de gestión marina (Kota Batam), 1 es una reserva marina de uso múltiple (Teluk Ambon), 6 son reservas de caza, 9 son parques nacionales marinos, 1 es un parque recreativo, 252 son reservas naturales, 84 son reservas de vida salvaje, 1 es un bosque protegido (Gunung Boliyohuto), 21 son parques marinos recreativos, 4 son parques costeros, 11 son reservas naturales marinas, 2 son parques costeros en islotes, 37 son parques en grandes bosques y 1 es un área de protección de cultivos marinos. 

Por otra parte, en Indonesia hay 7 reservas de la biosfera de la Unesco, 4 sitios patrimonio de la humanidad y 7 sitios Ramsar con una extensión global de 13.730 km.









</doc>
<doc id="15498" url="https://es.wikipedia.org/wiki?curid=15498" title="Política de Indonesia">
Política de Indonesia

Indonesia es una república presidencialista. Al año siguiente el sistema federal de Indonesia fue abolido y el país se convirtió en una república unitaria.

Tres constituciones provisionales definieron la forma del gobierno de Indonesia. La primera fue proclamada en 1945; la segunda fue promulgada en febrero de 1950 y la tercera fue aprobada, en agosto de 1950, por la Cámara de Representantes. En 1959 se restableció la Constitución de 1945 mediante un decreto presidencial.

Según la Constitución de 1945, el principal poder ejecutivo de Indonesia es el presidente, elegido por un plazo de cinco años por el voto popular. Anteriormente era designado por un cuerpo nacional denominado la Asamblea Consultiva del Pueblo, que realiza parte de las funciones parlamentarias del país. 

El presidente, que puede ser elegido durante varios periodos, tiene un amplio poder y puede gobernar por decreto; también nombra y preside el gabinete de ministros.

El poder legislativo en Indonesia reside en el Consejo de Representantes, que debe aprobar todas las leyes y tiene derecho a presentar proyectos de ley para que sean ratificados por el presidente. El Consejo está formado por 400 miembros directamente elegidos y 100 nombrados. La Asamblea Consultiva del Pueblo está compuesta por los miembros del Consejo y 500 miembros más que son delegados regionales y representantes de grupos profesionales (como campesinos, hombres de negocios, intelectuales y mujeres). Las principales funciones de la Asamblea son determinar las líneas generales de la política del gobierno y del Estado. La Constitución requiere que la Asamblea se reúna al menos cada cinco años y que el Consejo se convoque una vez al año.

Los casos criminales y civiles se juzgan en tribunales de distrito distribuidos por toda Indonesia. Las apelaciones se realizan ante los tribunales supremos ubicados en las 14 ciudades más importantes; el tribunal de apelación final es el Tribunal Supremo, que tiene su sede en Yakarta. Las leyes del Código Penal se aplican en toda Indonesia. En los casos de jurisdicción civil, sin embargo, los indonesios son juzgados según una ley consuetudinaria no codificada (Ley de Adat), mientras que los occidentales y asiáticos de origen o antepasados extranjeros están sujetos a un sistema basado en los códigos civiles continentales europeos.

Cada una de las 33 provincias y distritos está administrada por un gobernador y por cuerpos administrativos y legislativos locales.

Indonesia tiene tres partidos políticos importantes. El de mayor entidad es Golongan Karja (GORKA, Grupos Funcionales, fundado en 1964), una alianza de organismos que representan a los trabajadores, campesinos, la juventud y otros grupos económicos. Otras agrupaciones son el Partido de la Unidad para el Desarrollo (1973), que tiene una fuerte orientación musulmana, y el pequeño Partido Democrático de Indonesia (1973), una coalición de grupos cristianos y nacionalistas.

Las Fuerzas Armadas de Indonesia se unificaron en 1967 y se sometieron al control del ministro de Seguridad y Defensa. Desde entonces la institución militar ha ejercido una autoridad decisiva. El Ejército del país, la Armada y las Fuerzas Aéreas cuentan con un total de 274.500 personas.


</doc>
<doc id="15499" url="https://es.wikipedia.org/wiki?curid=15499" title="Cultura de Indonesia">
Cultura de Indonesia

La cultura de indonesia es el resultado de la mezcla de diferentes civilizaciones. Siendo hoy en día un país islámico, las creencias autóctonas, el hinduismo y el budismo de la India ejercieron una profunda influencia y han dejado una importante huella en la arquitectura y escultura del país. Las islas también han sentido la influencia de las culturas polinesia y de Asia suroriental, así como la de chinos y holandeses. La influencia árabe empezó a cobrar más importancia a partir del siglo XIII, sobre todo a través de las enseñanzas del islam. 

En Indonesia hay aproximadamente 20 bibliotecas de gran importancia que se hallan sobre todo en las ciudades de Bandung, Bogor, Yakarta y Yogyakarta. En Yakarta se encuentran los archivos nacionales y la Biblioteca del Museo Nacional (360.000 volúmenes), así como la Biblioteca Nacional (750.000 volúmenes), que alberga varias colecciones especiales. El Museo de Bali está situado en Denpasar.

La Constitución de Indonesia garantiza la religión, siempre que sea una de las cinco religiones oficiales (Islam, Catolicismo, Protestantismo, Budismo, Hinduismo) y bajo el credo de "Pancasila" entre otras cosas, defiende por igual y da el mismo trato a todas las creencias.

El islam en sus diferentes manifestaciones es la fe de aproximadamente el 88% de la población. Entre los demás grupos religiosos se pueden señalar la presencia de más de 17 millones de cristianos, sobre todo protestantes, y más de 1,5 millones de budistas, la mayoría de origen chino. El hinduismo, que en el pasado tuvo una gran importancia, está confinado a la isla de Bali y algún punto remoto del este de Java.

El islam en Indonesia es similar al Islam en los países árabes. Los indonesios son un pueblo extremadamente abierto y pacífico, con una base hindú y budista muy sólida por la cual se acogió el islam hace cinco siglos. 

En general, existe respeto y tolerancia entre aquellos que profesan religiones distintas, aunque en momentos puntuales, siempre motivados políticamente, ha habido enfrentamientos entre musulmanes y cristianos (Célebes central, Molucas). 

En las islas de Java y Sumatra predomina el islam, donde viven casi 200 millones de personas. Bali, por su parte, es el último lugar donde se practica el hinduismo, y en el Este de Indonesia (Flores, Timor, Molucas, Célebes Septentrional) encontramos cristianismo (católicos y protestantes); en estas provincias viven entre 15 y 20 millones de personas.
En Indonesia se hablan más de 100 idiomas, pero la lengua oficial y más hablada es el Bahasa Indonesia. De origen malayo, fue durante mucho tiempo la lengua de los comerciantes de las ciudades costeras, y posee elementos del chino, del indio, del holandés y del inglés.

Según la legislación indonesia, la enseñanza es obligatoria desde los seis años. El sistema de enseñanza del país sigue el sistema holandés con un programa de enseñanza secundaria dividido en matemáticas, lenguaje y ciencias económicas. Aproximadamente el 74% de los indonesios de 15 años o más saben leer y escribir.


A finales de la década de 1980, 26,7 millones de niños indonesios asistieron a escuelas primarias públicas y más de 8,9 millones de estudiantes se inscribieron en institutos de enseñanza secundaria. Además, más de 1,4 millones de estudiantes indonesios acudieron a escuelas de formación del profesorado.


A finales de la década de 1980, acudieron a las instituciones de educación superior casi 1,2 millones de estudiantes por año. Las instituciones con mayor número de alumnos son la Universidad de Indonesia (1950) en Yakarta, la Universidad Estatal de Pajajaran (1957) en Bandung y la Universidad Gajah Mada (1949) en Yogyakarta. Para el año académico de 2008/2009, el Ministerio de Educación Nacional otorgará a 1000 personas a participar en Beca Regular, y Short Course (curso corto). Los programas serán otorgados por el Gobierno de la República de Indonesia para estudiantes extranjeros que tengan interés en estudiar arte, danza típica, música tradicional, idioma Indonesio (bahasa Indonesia) y dialectos Indonesios en 54 universidades y colegios en Indonesia.

A finales de los años ochenta, funcionaban en Indonesia más de 890.000 teléfonos. La emisora estatal, Radio Republic Indonesia, opera en 49 estaciones que llegan a unos 32,8 millones de oyentes.

Un sistema de transmisión de televisión, controlado por el Estado, que comenzó a funcionar en 1962, se estima que llega a unos 7,1 millones de televidentes; la difusión de la televisión comercial privada se inició en 1989. 

Los principales periódicos de gran difusión de Indonesia son The Jakarta Post, Kompas, Pos Kota y Berita Buana, y la revista crítica Tempo, todos publicados en Yakarta.

El archipiélago cultural de Indonesia abarca 18.000 islas, con su propia historia única y distintos temperamentos culturales y artísticos. Esta diversidad en la fibra moral de las numerosas islas ha dado a luz a los centenares de diversas formas de música, que se acompañan a menudo de danza y teatro. Para saber más sobre la música y para bailar en Indonesia, es importante entender las varias influencias culturales que han formado eventualmente el país.
Si bien en sus orígenes fue influida por los sistemas musicales de China, India e Indochina, su evolución es independiente y totalmente diferenciada. La música de Indonesia no tiene sentido separada de la poesía o la danza y es, en general, de carácter mágico y religioso, tomando formas de representación teatral. La unidad de ejecución es el gamelan, orquesta formada por instrumentos diversos según las distintas variedades. La música de Indonesia comprende tres sectores: Java, Bali e islas periféricas.

La música de Java, Sumatra, Bali, Flores y otras islas ha fascinado a muchos. El "estallido" de la música tradicional en el curso de la música popular indonesia y de la música Se basaron en la forma de música de danza que ha sido popular desde mediados de los años setenta. El Qasidah moderno, es una forma de poesía religiosa acompañada por cantos y la percusión y es muy popular entre las audiencias del "estallido".El baile en Indonesia se realiza como en la mayor parte de los artes de ejecución del Oriente. La danza en Indonesia se cree que pudo haber comenzado como una forma de adoración religiosa. Hoy, aunque las influencias modernas continúan entrando calladamente, las viejas tradiciones de la danza y el drama todavía están bien resguardadas. Se están preservando por muchas organizaciones gubernamentaleso o academias de arte y escuelas de danza supervisadas, aparte de las que prosperan en las cortes. En tiempos pretéritos estas manifestaciones musicales fueron realizadas en las cortes reales para entretener, sin embargo, ahora estas danzas han alcanzado a amplios estratos populares de las cortes incluidas, y han incorporado una forma más espontánea de expresión.

Java: La música javanesa conoce dos sistemas de afinación principales, el slendro (pentatónico) y el pelog (heptatónico); de ellos derivan todos los actuales. A cada sistema de afinación corresponde un sistema de representación teatral basado en representaciones con actores o con marionetas y, según cuál sea el estilo, sobre ciclos del Ramayana y Mahabaratha o sobre ciclos autóctonos javaneses. Entre los instrumentos javaneses se pueden mencionar el gambang kayu, especie de xilofón, el bonag panerus, formado por una doble fila de tubos, el ageng, un tipo de gong, etc. Cada uno de ellos tiene una función definida dentro del gamelan y en relación al estilo y escala escogidos. El canto tiene gran importancia, ya que la poesía se canta siempre y no se recita. Existen gran variedad de formas vocales y de metros. El número de versos, su metro y la vocal final de verso están prescritos rigurosamente. Una variedad del gamelan es el kowangan de los pastores de las montañas, que utiliza instrumentos de cuerda, tamboriles e instrumentos de bambú.
Bali: El sistema musical de Bali es, en cuanto a lo que a instrumental y sistemas de afinación se refiere, similar al javanés, pero desde el punto de vista del carácter es fundamentalmente distinto. La base de la música balinesa la dan fuertes contrastes dinámicos y de tempos, una gran espectacularidad y una riqueza ornamental que contrasta con la severidad normativa de la música javanesa. La diferencia originaria estriba en que mientras la música de Java fue una música de corte, o al menos favorecida por las bussy. Modernamente, otros músicos occidentales se han acercado a la música de Bali.
Islas periféricas. Están influidas por la música de Java y Bali así como por la de culturas no indonésicas. La influencia balinesa es más importante en las islas de la Sonda, especialmente en Lombok. Las islas Célebes cuentan con cantos heroicos autóctonos acompañados por el keso-keso, especie de laúd. Borneo pertenece al área de influencia javanesa, salvo en el interior, donde existen cantos autóctonos y un interesante órgano de boca, llamado kledi, originario de Indochina. Por el contrario, la isla de Sumatra mezcla la influencia puramente indonésica con la del mundo islámico. La música de Sumatra muestra influencias arábigas y aun persas, existiendo en ella algunas especialidades instrumentales como el bangsi, especie de flauta, el serunai, especie de oboe, y el gambus, laúd de siete cuerdas.


</doc>
<doc id="15500" url="https://es.wikipedia.org/wiki?curid=15500" title="Arte en Italia">
Arte en Italia

En Italia el enfrentamiento y convivencia con la antigüedad clásica, considerada como un legado nacional, proporcionó una amplia base para una evolución estilística homogénea y de validez general. Por ello, allí, es posible el surgimiento del arte renacentista y precede a todas las demás naciones. En Florencia el desarrollo de una rica burguesía ayudará al despliegue de las fuerzas del Renacimiento, la ciudad se convierte en punto de partida del nuevo estilo, y surgen, bajo la protección de los Médicis, las primeras obras que desde aquí se van a extender al resto de Italia.

Fuera de Italia la Antigüedad Clásica supondrá un caudal académico asimilable, y el desarrollo del Renacimiento dependerá constantemente de los impulsos marcados por Italia. Artistas importados desde Italia o formados allí, hacen el papel de verdaderos transmisores.


</doc>
<doc id="15501" url="https://es.wikipedia.org/wiki?curid=15501" title="Austrodanthonia">
Austrodanthonia

Austrodanthonia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Australia, Nueva Zelanda y Nueva Guinea.

El número cromosómico básico del género es x = 12, con números cromosómicos somáticos de 2n = 24 diploide. 



</doc>
<doc id="15502" url="https://es.wikipedia.org/wiki?curid=15502" title="Historia de los Estados Unidos">
Historia de los Estados Unidos

La fecha del comienzo de la historia de Estados Unidos es tema de debate entre los historiadores. Los libros de texto más antiguos comienzan con la llegada de Cristóbal Colón el 12 de octubre de 1492 o alrededor de 1600, con la llegada del navío "Mayflower". No obstante, en las últimas décadas, las escuelas y universidades estadounidenses han retrocedido en el tiempo para incluir más información acerca de los nativos americanos.

Los Anasazi eran un conjunto de tribus amerindias de la superárea cultural de Oasisamérica. Ocupaban, en varios grupos, la superficie de los estados actuales de Colorado, Utah, Arizona y Nuevo México. Su civilización ha dejado vestigios monumentales y litúrgicos en distintos lugares, de los cuales dos han sido clasificados como Patrimonio de la humanidad por la Unesco. Los restos arqueológicos demuestran conocimiento de la cerámica, el tejido y la irrigación. Además, dibujaban símbolos que no han sido descifrados y observaban los desplazamientos solares. A partir del año 1400, los anasazi se refugian en el Valle del Río Grande y en el centro de Arizona. Se pierden sus huellas poco antes de la llegada de los españoles. Las razones de este éxodo no son conocidas, sin embargo existen varias hipótesis: un cambio climático que amenazó las cosechas, un medio deteriorado que redujo las tierras cultivables disponibles, sobrepoblación, problemas políticos, guerras. No obstante, dada la ausencia de documentos escritos y la limitación de los conocimientos actuales no es posible probar ninguna de dichas hipótesis.

Los indios de las llanuras incluyen a todas las tribus que habitaban las Grandes Llanuras (la tierra ubicada entre las Montañas Rocosas y el río Misisipi). Fueron cazadores-recolectores la mayor parte de su existencia, pero cuando los exploradores españoles introducen en la región los caballos en el siglo XVII, los indios los consiguen y cambian su modo de vida a una civilización nómada, siguiendo las rutas migratorias de los bisontes americanos. Cuando los blancos invadieron y ocuparon las Grandes Llanuras en el siglo XIX, los indígenas participan en una amarga guerra de resistencia que duró desde 1836 hasta 1918. La combinación de las Guerras Indias y la política del gobierno de Estados Unidos de aniquilar a los bisontes americanos dio lugar a un colapso demográfico dramático en la población de los indios de las llanuras. Al cabo de su derrota, los blancos confinaron al resto de los indios en reservas, donde permanecen hoy en día.

Los inuit son un pueblo indígena que tradicionalmente han habitado la región circumpolar del este de Siberia (Rusia), a través de Alaska (Estados Unidos), Canadá y Groenlandia. La cultura más antigua fue la pre-Dorset, plenamente desarrollada, que data de hace 5000 años. Parece que han evolucionado en Alaska de personas que utilizan el arcaico herramientas de tecnología de la pequeña, que probablemente habían emigrado a Alaska de Siberia, al menos, de 2000 a 3000 años atrás, aunque podrían haber sido en Alaska ya en 10 000 a 12 000 años o más. Hay artefactos similares que se encuentran en Siberia, que se remonta quizás a hace 18 000 años.

Los Indios de los Bosques superpoblados habitaron en los bosques entre el océano Atlántico y el río Misisipi. Estas tribus eran generalmente comunales y vivían en aldeas con chozas de madera y carriles. La recepción de los invasores ingleses se mezcló con hechos resultantes en la guerra y el exterminio, mientras que otros fueron pacíficos. Finalmente, la relación entre los ingleses y los Indios de los Bosques fue de hostilidad permanente, tanto que los franceses, que controlaban el valle del río Misisipi, lo utilizaron para su beneficio. Los franceses mantuvieron una política de comercio y de paz con los Indios de los Bosques y eventualmente formaron una alianza militar con ellos.

La más avanzada de las civilizaciones precolombinas en el territorio que ahora es Estados Unidos fue la Confederación Iroquesa. La Confederación Iroquesa, o las Cinco Naciones, fue una liga o confederación de carácter democrático, con características tanto participativas como representativas (combinadas con algunas hereditarias). Se hallaba constituida por tribus amerindias de lengua iroquesa, que habitaban al noreste de Estados Unidos y al sureste de Canadá en la zona de los Grandes Lagos. La Confederación estaba formada originalmente por cinco tribus (seneca, cayuga, oneida, onondaga y mohawk) que se confederaron a mediados del siglo XII, y a las que se sumó tuscarora en 1720. 

El régimen democrático de la Confederación estaba regulado por una constitución de 117 artículos conocida como la Gran Ley de la Paz y gobernada por un Parlamento o Consejo de representantes de la población, considerado como el tercero más antiguo del mundo luego del Althing de Islandia y las Cortes de León (1188). La Gran Ley de la Paz establecía una especie de Estado de Derecho con estrictos límites y restricciones al poder de los gobernantes. Establecía también una división del poder entre hombres y mujeres, estableciendo que ningún hombre podía presidir un clan y ninguna mujer ser jefe militar o sachem. A las jefas de los clanes correspondía elegir a los jefes militares.
Así la Confederación tuvo una influencia directa tanto en la democracia y el constitucionalismo, como en la idea de la igualdad de mujeres y hombres en la sociedad moderna. En especial Benjamín Franklin, quien tuvo trato directo con Haudenosaunee en 1753, destacó en sus obras que el grado de autonomía individual que gozaban los habitantes de la liga era desconocido en Europa y publicó los tratados indios, considerada como una de sus obras más importantes. Para pensadores o historiadores de los movimientos radicales como Howard Zinn, la Confederación de las Seis naciones constituye una muestra de la aplicación de la democracia radical a través de las decisiones asamblearias.

Se sabe que alrededor del año 1000, un grupo de vikingos establecidos en Groenlandia navegaron hacia la costa oriental de América del Norte bajo el mando de Leif Eriksson, arribando a un lugar que llamaron Vinland. En la provincia canadiense de Terranova se han encontrado irrefutables vestigios de una colonia vikinga, en L'Anse aux Meadows. Es probable que los vikingos también visitaran Nueva Escocia y Nueva Inglaterra; sin embargo, no lograron fundar colonias permanentes y pronto perdieron contacto con el nuevo continente.

Cinco siglos más tarde, la necesidad de incrementar el comercio y un error de navegación propiciaron un nuevo encuentro con el continente americano. A finales del siglo XV había en Europa una gran demanda de especias, sedas y tinturas de Asia. Cristóbal Colón creyó erróneamente que podría llegar al Extremo Oriente navegando 6.400 kilómetros hacia el oeste partiendo desde Europa. En 1492 persuadió a los reyes de España para que le financiaran el viaje. Colón navegó hacia occidente pero no llegó a Asia, sino a la isla de Guanahani en el Caribe. Comienza la colonización española de América, incluyendo territorios en los actuales Estados Unidos.

Juan Ponce de León dio nombre a La Florida en 1513, cuando la tomó en nombre de la corona de España. Hasta 1563 los españoles enviaron varias expediciones para explorarla, pero sin llegar a levantar ninguna fortificación estable. Sin embargo, la presencia en 1564 de un nutrido contingente de hugonotes franceses, que alzaron un fuerte en la desembocadura del río San Juan, supuso una seria amenaza que llevó a España a la decisión de establecer una presencia militar permanente en el área. El 28 de agosto de 1565, Pedro Menéndez de Avilés funda la ciudad de San Agustín. Es el asentamiento europeo más antiguo ocupado hoy en EE.UU. Solo San Juan (Puerto Rico) la supera como ciudad más antigua de "los Estados Unidos".

Estados Unidos surgió a partir de la colonización británica de América, protagonizada por oleadas de inmigrantes británicos que fundaron entre los siglos XVII y XVIII Trece Colonias en la costa atlántica del subcontinente norteamericano, al este de los Apalaches. 

En 1583 la Reina Isabel I de Inglaterra otorga una autorización al pirata Walter Raleigh para fundar una colonia al norte de La Florida, a la que llamaría Virginia y que abarcaría más tarde las actuales Carolina del Sur, Carolina del Norte, Virginia y Virginia Occidental. Rápidamente se vio la posibilidad de explotar la zona con cultivos de tabaco, por lo que se creó en 1606 la Compañía de Virginia como sociedad anónima, que financió el primer establecimiento inglés.

En 1607 un grupo de colonos ingleses fundó una pequeña aldea en Jamestown, Virginia. Portadores de una cédula del Rey Jaime I de Inglaterra, fundaron una colonia permanente en los primeros siete meses después de su arribo. La colonia creció con el tiempo y prosperó con el cultivo de tabaco, cuyo primer envío a Inglaterra fue en 1614.

En Nueva Inglaterra, región nororiental del actual Estados Unidos, los puritanos ingleses establecieron varias colonias. Llegaron a América pensando que la Iglesia de Inglaterra había adoptado demasiadas prácticas del catolicismo. Tenían el propósito de fundar una colonia basada en sus propios ideales religiosos. Un grupo de puritanos, conocidos como los peregrinos, cruzaron el Atlántico en un barco llamado "Mayflower" y se establecieron en Plymouth en 1620. Una colonia puritana mucho más grande se estableció en el área de Boston en 1630. Para 1635 algunos colonizadores ya estaban emigrando a la cercana Connecticut.

Con el paso del tiempo, las colonias británicas de América del Norte fueron ocupadas también por muchos grupos de origen no británico: agricultores alemanes se establecieron en Pensilvania; los suecos fundaron la colonia de Delaware; y los primeros esclavos africanos llegaron a Virginia en 1619. En 1626, colonizadores neerlandeses compraron la isla de Manhattan a los jefes indígenas de la región y erigieron la ciudad de Nueva Ámsterdam; en 1664, esta colonia fue tomada por los ingleses y rebautizada con el nombre de Provincia de Nueva York.

No obstante, las dos potencias coloniales más importantes eran España y Francia. La primera no sólo controlaba Florida y otras regiones del sur de Estados Unidos, sino que también había procedido a la colonización de Las Californias. Al inicio la colonización fue tarea exclusiva de la Compañía de Jesús, concentrada exclusivamente en Baja California. La primera misión en Baja California fue la efímera Misión de San Bruno. En 1769 se funda Los Ángeles y en 1776 la ciudad de San Francisco

Francia por su parte controlaba La Luisiana, que era junto a Quebec parte del virreinato de la Nueva Francia. El territorio se extendía desde los Grandes Lagos hasta el golfo de México y desde las montañas de los Apalaches hasta las Rocosas. No obstante, la gran mayoría del territorio se encontraba o bien despoblado o habitado por nativos americanos y no por colonos franceses. La Luisiana estaba dividida en dos regiones, conocidas como Alta Luisiana, que empezaba al norte del río Arkansas, y Baja Luisiana.

Reino Unido y Francia, enemigos históricos, comenzaron a competir por la expansión territorial en el nuevo continente. Esto dio como resultado el estallido de la Guerra franco-india en 1754, parte a su vez de la Guerra de los Siete Años. Los franceses contaban con el apoyo de la España borbónica y de distintas tribus indias como es el caso de los algonquinos, los ottawas o los hurones. Reino Unido contó con el apoyo de la Confederación Iroquesa 

La guerra de los Siete Años terminó en 1763. El 10 de febrero, el Tratado de París ponía fin al imperio colonial francés en América del Norte y consolidaba a Reino Unido como la potencia hegemónica. La corona británica se hace con el control de Quebec y de la Florida española. España pasa a controlar Luisiana.

La ayuda dada a los británicos por parte de los colonos americanos no fue recompensada por el gobierno de Londres. Los impuestos sobre las Trece colonias aumentaron, como es el caso de la polémica Ley del sello de 1765 por el cual todo documento impreso en el territorio americano debía publicarse en papel sellado y producido en Londres, timbrados con un sello fiscal en relieve.. Al igual que los impuestos anteriores, el impuesto a los sellos tenían que ser pagados en moneda británica válida, no en papel moneda colonial. Los colonos se negaban a pagar este nuevo impuesto puesto que no tenían representación alguna en el parlamento británico. Nace así el concepto de "No taxation without representation". Además, estas nuevas leyes eran ilegales según la Declaración de Derechos de 1689 que impuso el Parlamento inglés al príncipe Guillermo de Orange para poder suceder al rey Jacobo II.

Volviendo al siglo XVIII, el parlamento rechazó las protestas coloniales y afirmó su autoridad al aprobar nuevos impuestos, como es el caso del Acta del Té, la cual gravaba la importación proveniente de la metrópoli de distintos productos, incluido el té, para beneficiar así a la Compañía Británica de las Indias Orientales, la cual había comenzado su expansión por Asia, a quien los colonos boicoteaban comprando el té de los Países Bajos. En 1773 tiene lugar Motín del té en Boston cuando un grupo de colonos disfrazados de indios arrojó al mar el cargamento de té de tres buques británicos.

Las tensiones entre metrópoli y colonia eran cada vez mayores. Inspirados por los ideales de la Ilustración, los intelectuales americanos comenzaron a plantear la idea de una América independiente. En 1774 tiene lugar el Primer Congreso Continental, la primera reunión de colonos contra la corona británica y a favor de una patria independiente. Comienzan a formarse milicias de colonos por la independencia.

En Concord un grupo de rebeldes se hizo con el control de un depósito de armas, razón por la cual el 19 de abril de 1775 soldados británicos salieron de Boston en dirección a Concord. En el poblado de Lexington, defendido por milicianos, comenzaron los combates. En mayo de 1775, un Segundo Congreso Continental se reunió en Filadelfia y empezó a asumir las funciones de gobierno nacional. Nombró catorce generales, autorizó la invasión de Canadá y organizó un ejército de campaña bajo el mando de George Washington, un hacendado virginiano y veterano de la guerra franco-india. El 17 de junio de ese mismo año tiene lugar la Batalla de Bunker Hill la cual concluye con una victoria pírrica británica. El 4 de julio de 1776 se firma la Declaración de Independencia de los Estados Unidos. El 4 de julio se acabará convirtiendo en el día nacional de los Estados Unidos.

El desarrollo inicial fue claramente de dominio inglés, pero su curso cambiaría cuando tras la Batalla de Saratoga en 1777, primera gran victoria estadounidense. Francia, al mando del Marqués de La Fayette, y posteriormente España, al mando de Bernardo de Gálvez entraron en guerra apoyando a los independentistas norteamericanos. Por consiguiente, también se realizaron combates en Gibraltar, las Islas Baleares y el subcontinente Indio, donde franceses y británicos competían por el control colonial. Los rebeldes americanos pasaron de ser un grupo desorganizado para convertirse en un ejército en toda regla, el Ejército Continental.

En 1781 los británicos sufren una aplastante derrota en la Batalla de Yorktown. En [[1783], por la [[Paz de Versalles]], Gran Bretaña se ve obligada a reconocer la independencia de las trece colonias británicas, tal y como estas habían redactado en la Declaración de Independencia de [[1776]].

Una vez lograda la independencia, resultó muy complicado poner de acuerdo a todas las antiguas colonias sobre si seguían como estados independientes, o se reunían en una sola nación. Tras varios años de negociaciones, en [[1787]], cincuenta y cinco representantes de las antiguas colonias se reunieron en el [[Congreso de Filadelfia]] con el fin de redactar una [[constitución]]. Se creaba así un [[Federación|gobierno federal]] único, con un [[Presidente de la República]] y dos Cámaras Legislativas ([[Congreso de los Estados Unidos|Congreso]] y [[Senado de los Estados Unidos|Senado]]) como solución intermedia. Se redactó también la [[Constitución de 1787]], y se convocarons elecciones de las que [[George Washington]] fue elegido primer [[Presidente de los Estados Unidos]] bajo la nueva constitución.

Esta constitución estaba inspirada en los principios de [[Igualdad social|igualdad]] y [[libertad]] que defendían los [[Ilustración|ilustrados]] y se configuró como la primera carta magna que recogía los principios del [[liberalismo]] político, estableciendo un régimen republicano y democrático. La independencia y democracia estadounidense causó un notable impacto en la opinión y la política de Europa y [[América Latina]].

[[George Washington]] gobernó con un estilo federalista. Cuando los agricultores de [[Pensilvania]] se negaron a pagar un impuesto federal sobre el [[licor]], Washington movilizó a un ejército de quince mil hombres para sofocar la "Rebelión del [[Whisky]]". Con Alexander Hamilton al frente de la Secretaría de Hacienda, el gobierno federal se hizo cargo de las deudas de cada estado y creó una banca nacional. Estas medidas fiscales fueron concebidas para alentar la inversión y persuadir a la iniciativa privada a que apoyara al nuevo gobierno.

[[Archivo:Desarrollo Económico de USA 1700-1840.jpg|miniaturadeimagen|izquierda|PIB per cápita en los primeros años de la república estadounidense.]]
En [[1797]], a George Washington le sucedió otro federalista, [[John Adams]], quien se vio envuelto en una guerra naval no declarada contra la [[Francia]] de [[Napoleón]], la [[Cuasi-Guerra]]. En una atmósfera de histeria bélica, el Congreso, controlado por los federalistas, aprobó en [[1798]] las Leyes sobre Extranjeros y Sedición. Estas medidas permitieron la deportación o arresto de extranjeros «peligrosos» y prescribieron multas o prisión por publicar ataques «falsos, escandalosos y maliciosos» contra el gobierno. Diez editores [[Partido Demócrata-Republicano de los Estados Unidos|republicanos]] fueron condenados conforme a la Ley de Sedición, la cual fue duramente denunciada por el abogado virginiano y principal autor de la [[Declaración de Independencia de los Estados Unidos|Declaración de Independencia]] [[Thomas Jefferson]]. 

En [[1803]] la joven nación realiza la [[compra de Luisiana]] a [[Francia]], que había recuperado el territorio tras la [[Guerra de Independencia de los Estados Unidos|Guerra de Independencia]]. 

[[File:USS Constitution vs Guerriere.jpg|thumb|300px|El [[USS Constitution]] derrota al [[HMS Guerriere]] durante la [[Guerra de 1812]].]]
En 1807, [[Reino Unido de Gran Bretaña e Irlanda|Gran Bretaña]] introdujo una serie de restricciones comerciales para impedir el comercio estadounidense con [[Francia]], en respuesta al apoyo estadounidense a [[Napoleón Bonaparte]], con quien Gran Bretaña estaba en guerra. Los [[Estados Unidos]] impugnaron estas restricciones como un bloqueo ilegal. El reclutamiento forzoso de ciudadanos estadounidenses en la [[Royal Navy]] y el apoyo militar de Gran Bretaña a los [[nativos americanos]], quienes se oponían a la expansión de la frontera estadounidense en el noroeste, agravó aún más la tensión entre los dos países. Comenzaba así la [[Guerra de 1812]].

Estados Unidos, bajo el mando de [[James Madison]], procedió a la invasión de la colonia británica de [[Canadá]]. Para sorpresa de los estadounidenses, los británicos y colonos canadienses no solo resistieron, sino que expulsaron a los invasores del país. Los Estados Unidos respondieron con una segunda ofensiva en el este de Canadá, pero esta invasión también fue derrotada. El gobernador británico de Canadá, George Provost, ordenó una contraofensiva. Los británicos saquearon la ciudad de [[Detroit]] y se hicieron con el control de todo el estado de [[Maine]].

Gran Bretaña puso en marcha una política de cinco puntos los cuales eran: 


Esta estrategia se basaba en la enorme superioridad de la Armada británica. Los británicos bloquearon con éxito la costa atlántica e invadieron la región de Chesapeake. El ejército estadounidense atacó a los británicos en la batalla de Bladensburg pero fueron derrotados, dejando así un camino de menor resistencia entre la bahía de Chesapeake y Washington. El 24 de agosto de 1814, el ejército británico entró en la capital. El presidente Madison había ordenado que la ciudad fuese evacuada, por lo que una vez más, los británicos no encontraron resistencia armada. El general británico, George Cockburn, ordenó arrasar la ciudad. La [[Casa Blanca]], el [[Capitolio de Estados Unidos]], la sede de la Armada, la [[Biblioteca del Congreso de Estados Unidos|Biblioteca del Congreso]], y el Tesoro de Estados Unidos fueron quemados.

La derrota y el retorno al colonialismo parecía inevitable para los estadunidenses, pero, de repente, la marea de la guerra comenzó a girar. Dos semanas después del saqueo de Washington, el ejército estadounidense rechazó al ejército británico en la batalla de North Point, obligándolo a retirarse hacia el [[Océano Atlántico]]. Los británicos lanzaron una segunda ofensiva en contra de la ciudad portuaria de [[Baltimore]], pero los estadounidenses rechazaron la invasión con éxito.

Madison hizo un llamamiento para la paz. En diciembre de 1814, los funcionarios de los dos países se reunieron en [[Gante]], [[Bélgica]] y acordaron firmar un tratado de paz que resultó en el reconocimiento del "[[statu quo ante bellum]]". Sin embargo la noticia del [[tratado de Gante]] no llegó a los Estados Unidos hasta varios meses después. Mientras tanto, los británicos lanzaron su asalto final sobre las ciudades portuarias de [[Nueva Orleans]] y [[Mobile (Alabama)|Mobile]]. El general estadounidense y futuro presidente, [[Andrew Jackson]], llevó a los estadounidenses a la victoria en la [[batalla de Nueva Orleans]], pero los británicos capturaron con éxito [[Mobile (Alabama)|Mobile]]. Noticias del tratado de paz por fin llegaron a Estados Unidos el 23 de marzo de 1815 y los británicos retiraron todas las tropas y terminaron el bloqueo naval.

Hoy en día, la guerra sigue siendo objeto de acalorado debate entre los estadunidenses, británicos y canadienses, con cada uno de los tres pueblos proclamando la victoria.

Ya desde antes de la Guerra de 1812 en la frontera de la Florida española con el estado de [[Georgia (Estados Unidos)|Georgia]] existían fuertes tensiones debido a que los [[cazadores de esclavos]] estadounidenses perseguían a los [[Semínolas negros]] del norte de colonia española. Estas tensiones aumentaron cuando los británicos ayudaron a los [[Seminola]]s contra los americanos en la Guerra de 1812. En 1814, los estadounidenses invadieron el norte de la Florida española comenzando así las [[guerras Seminolas]]. [[España]] protestó ante esta invasión pero poco o nada pudo hacer por pararla ya que la gran mayoría del ejército español, recién salido de su propia [[Guerra de Independencia de España|guerra de independencia]], se encontraba en [[América Latina]] luchando contra los independentistas. Se buscó una solución pacífica al conflicto, el [[tratado de Adams-Onís]] de 1819 por el cual se comprometía a ceder la Florida española a los [[Estados Unidos]] en 1821.
[[Archivo:Crossing the Mississippi on the Ice by C.C.A. Christensen.png|400px|thumb|"Crossing the Mississippi on the ice" (‘el cruce del Misisipi sobre el hielo’), de C. C. A. Christensen.]]
Después de esta segunda guerra, Estados Unidos gozó de un período de rápida expansión económica, sobre todo a partir de la colonización y expansión hacia el Oeste. Ya a fines del siglo XVIII se había iniciado el avance imparable de los colonos, bien desde los trece estados originales (las antiguas [[trece colonias]] que están representadas en las trece barras de la bandera estadounidense) o directamente desde el continente europeo. Por lo general, se trataba de emigrantes anglosajones (irlandeses, escoceses, ingleses y galeses) y de otros países de la Europa Central y Occidental (principalmente [[Germano-estadounidense|alemanes]]).

Las caravanas de colonos fueron los verdaderos motores de la ocupación progresiva del continente hacia el oeste. Sin embargo, no se trató de la ocupación de áreas desocupadas, ya que gran parte del territorio estaba previamente ocupado por [[pueblos originarios]], colonos franceses procedentes del Canadá francés, así como todas las ciudades fundadas por los españoles antes en los territorios de [[Arizona]], [[Texas]], [[Colorado]], [[Nuevo México]], [[Utah]], [[Nevada]] y [[California]]. Así pues, ciudades como [[Detroit]], [[Dubuque]], [[Saint Louis]], [[Nueva Orleans]], [[Baton Rouge]], [[Des Moines]], [[Louisville]] y muchas otras, ya habían sido fundadas por los franceses bastantes años antes de esa especie de estampida hacia el oeste, y lo mismo podía decirse de las ciudades fundadas por los españoles que procedían de México, como [[Socorro]], [[San Antonio (Texas)|San Antonio]], [[Albuquerque]], [[Santa Fe (Nuevo México)|Santa Fe]], [[El Paso (Texas)|El Paso]], [[San Diego (California)|San Diego]], [[San Bernardino]], [[Los Ángeles]], [[San Francisco (California)|San Francisco]], etc. que se habían fundado durante los siglos XVI y XVII.
Toda esta expansión hacia el [[Lejano Oeste]] ("Far West") se vio dinamizada por dos hechos muy importantes: el [[Fiebre del oro|descubrimiento de oro]] en California ([[1848]]) y la culminación de la red ferroviaria con la [[Primer ferrocarril transcontinental de Estados Unidos|primera línea transcontinental]] en 1869 (el primer [[ferrocarril]] de vapor se había inaugurado en [[Baltimore]] ([[Maryland]]), en [[1830]]). Una red nacional de carreteras y canales recorría el país, buques de vapor surcaban los ríos, y la [[Revolución industrial]] había llegado a Estados Unidos: la región de Nueva Inglaterra contaba con fábricas de textiles y [[Pensilvania]] con fundiciones de hierro. No obstante, el sur del país continuaba siendo una región poco industrializada cuya economía se basaba en la agricultura esclavista.

Entre las décadas de [[Años 1820|1820]] y [[Años 1830|1830]], después de la proclamación de la [[Doctrina Monroe]] de expansión territorial hacia el Pacífico, miles de colonos estadounidenses se establecieron en las comunidades anglosajonas de [[Texas]] (entonces territorio [[México|mexicano]]). En aquel momento el gobierno mexicano se encontraba en una mala situación económica al término de una guerra de independencia con España que duró más de una década, y dio la bienvenida a los colonos. El gobierno mexicano obtuvo fondos vendiendo tierras a estos colonos que prefirieron mudarse a territorio mexicano en vez de pagar altos precios en Luisiana y otros estados del sur. Estos colonos esperaban, además, que Estados Unidos comprara Texas para proveer de más tierra a sus nuevos ciudadanos.

[[Archivo:USA's States that were part of Mexico.png|230px|thumb|Azul oscuro: Estados de EUA que fueron parte de MéxicoAzul claro: Estados de EUA que sólo una pieza de ellos fue parte de México.]]

En [[1820]] un [[empresario]] de [[Misuri]], [[Moses Austin]], había negociado con [[España]] para que se le permitiera llevar trescientos colonos a Texas. Stephen Austin, el hijo (conocido como el padre de la [[República de Texas]]), siguió estos planes con el nuevo gobierno mexicano, escogiendo colonos que fueran buenos trabajadores y que pudieran ser leales al gobierno mexicano. El gobierno mexicano, que había abolido la esclavitud, toleró que los colonos trajeran sus esclavos para trabajar las tierras y venderlos a otros colonos pero se listaban como «sirvientes contratados» ("indentured servants" en [[Idioma inglés|inglés]]). Problemas con el nuevo gobierno del presidente [[Antonio López de Santa Anna]] causaron que los colonos [[Independencia de Texas|se levantaran en armas y lucharan]], con apoyo de los Estados Unidos, para obtener la independencia, ya que para entonces los colonos anglosajones eran más numerosos que los colonos mexicanos. Pese a la aplastante derrota de la [[Batalla de El Álamo]], Texas logró independizarse en [[1836]] a través del [[Tratado de Velasco]], para ser finalmente incorporada a los [[Estados Unidos]]

Texas no era el único territorio mexicano donde existía una población considerablemente grande de estadounidenses. Esto, sumado a la anexión de Texas, al [[Incidente de Thornton]] y a la [[Doctrina Monroe]] provocó el estallido de la [[Guerra Mexicano-Americana]] en 1846. Los estadounidenses [[Asedio de Veracruz|desembarcaron en Veracruz]] y conquistaron la [[Ciudad de México|capital]], tras lo cual los mexicanos se vieron obligados a firmar el [[Tratado de Guadalupe Hidalgo]] en [[1848]], por el cual los Estados Unidos se anexionaron los actuales estados de [[California]], [[Nuevo México]], [[Arizona]], [[Nevada]], [[Utah]], [[Colorado]] y parte de [[Wyoming]]. Los residentes hispanos recibieron plena ciudadanía y los indios mexicanos se convirtieron en indios americanos. 

[[Archivo:Abraham Lincoln head on shoulders photo portrait.jpg|thumb|left|Abraham Lincoln.]]
Desde su nacimiento, Estados Unidos se convirtió en el más importante comprador de [[esclavo]]s para satisfacer la demanda de mano de obra en las pesadas labores agrícolas. Como bien se dijo antes, la esclavitud se extendía entre los estados [[Región Sur (Estados Unidos)|sureños]] cuya economía era prácticamente agrícola. 

La [[Isla de Gorea]], ubicada a unos cuantos kilómetros frente a la costa de [[Senegal]], en el océano Atlántico, fue el lugar desde donde se organizó el tráfico de esclavos hacia Estados Unidos de América, que durante los siglos XVII, XVIII y hasta la [[abolición de la esclavitud]], en el siglo XIX, desplazó a más de veinte millones de personas de África.

En las elecciones presidenciales de 1860, el recién creado [[Partido Republicano (Estados Unidos)|Partido Republicano]] dirigido por [[Abraham Lincoln]], apoyó la prohibición de la esclavitud en todos los territorios de Estados Unidos. Los estados del sur vieron esto como una violación de sus derechos constitucionales y como el primer paso en un gran plan republicano para finalmente abolir la esclavitud. Los tres candidatos a favor de la Unión recibieron una abrumadora mayoría del 82 % de los votos a nivel nacional: los votos del republicano Lincoln se centraron en el norte, los votos del demócrata Stephen A. Douglas se distribuyeron a nivel nacional y los votos del congresista constitucional John Bell se centraron en Tennessee, Kentucky y Virginia.».

De los treinta y cuatro estados de Estados Unidos en febrero de 1861, siete estados esclavistas del sur individualmente declararon su secesión de los Estados Unidos para formar los [[Estados Confederados de América]], o el Sur. La Confederación creció para incluir once estados esclavistas. La Confederación nunca fue diplomáticamente reconocida por el Gobierno de los Estados Unidos, ni fue reconocida por ningún país extranjero.

La guerra estalló en abril de 1861, cuando las fuerzas de los Estados Confederados, presididos por [[Jefferson Davis]] atacaron [[Fort Sumter]] en Carolina del Sur, poco después de que el presidente Abraham Lincoln asumiera su cargo. Los nacionalistas de la Unión proclamaron lealtad a la Constitución de los Estados Unidos. Se enfrentaron a secesionistas de los Estados Confederados, que defendían los derechos de los estados a expandir la esclavitud.

La Unión, nombre dado al gobierno de Lincoln, puso en marcha el [[Plan Anaconda]], un bloqueo naval y fluvial a través del [[Océano Atlántico]], el [[río Mississippi]] y del [[río Tennessee]]. El Sur no disponía de barcos por lo que no podía parar el bloqueo. Se tuvo que improvisar una armada con buques mercantes artillados y barcos de guerra capturados al Norte. El [[8 de marzo]] de [[1862]], el acorazado [[CSS Virginia|CSS "Virginia"]] atacó a los barcos de bloqueo en las costas de Virginia. En un principio cayó la victoria de su lado, pero al día siguiente llegó el nuevo barco de guerra de la Unión, el moderno acorazado en la [[batalla de Hampton Roads]]. La batalla concluyó en un empate, lo que supuso una victoria estratégica para la Unión, ya que se mantuvo el bloqueo.

En julio de 1863, en la [[Batalla de Gettysburg]], la Unión vence a los confederados en la que probablemente sea la batalla más importante de la guerra.

[[File:The Battle of Prairie Dog Creek.jpg|thumb|300 px|Tropas de la caballería estadounidense durante la Batalla de Prairie Dog Creek, en 1867, en el contexto de las [[Guerras indias]].]]
[[File:Chinese railroad workers sierra nevada.jpg|thumb|300 px|Obreros chinos construyendo el [[Primer ferrocarril transcontinental de Estados Unidos]].]]

Entre [[1865]] y [[1877]] tiene lugar el período de la Reconstrucción, el cual se caracterizó por los intentos del gobierno de [[Washington D.C.|Washington]] de sanar las heridas de la guerra. Se modificó la [[Constitución de los Estados Unidos|constitución]] para otorgar libertad y derechos a los negros: A través de la [[Decimotercera Enmienda a la Constitución de los Estados Unidos|Decimotercera Enmienda]] se prohibía la esclavitud, la [[Decimocuarta Enmienda a la Constitución de los Estados Unidos|Decimocuarta Enmienda]] extendía las protecciones legales federales a todos los ciudadanos independientemente de su raza y la [[Decimoquinta Enmienda a la Constitución de los Estados Unidos|Decimoquinta Enmienda]] abolía las restricciones raciales para votar.

La Reconstrucción no fue tarea fácil. Los políticos del sur pusieron en marcha las [[Leyes Jim Crow]] bajo el principio de "[[Separados pero iguales]]". Por consiguiente, se estableció la [[segregación racial]] en todas las instalaciones públicas. Estas leyes afectaron a casi todas las instalaciones públicas y al transporte, incluidos los vagones de los trenes y autobuses interestatales. Las instalaciones para los afroamericanos y los nativos americanos eran consistentemente inferiores y carecían de fondos suficientes en comparación con las instalaciones para los estadounidenses blancos; a veces, no había instalaciones para los negros.

En [[1865]] se funda el [[Ku Klux Klan]], una [[grupo terrorista|organización terrorista]] [[supremacista blanca]] que llevará a cabo violentos ataques contra la comunidad negra, los políticos favorables a la reconstrucción y los "[[Carpetbagger]]s", los inmigrantes que llegaban al sur desde los estados del norte. Si bien el primer Ku Klux Klan fue disuelto por [[Ulysses S. Grant]], a través del Acta de Derechos Civiles de 1871, el Klan reapareció a comienzos del [[Siglo XX]]. Las principales causas que explican el renacimiento de la organización eran la cada vez mayor inmigración, siendo el Klan una organización [[antisemita]] y [[anticatolicismo|anticatólica]], el estreno en [[1915]] de la película [[El nacimiento de una nación]] de "[[D. W. Griffith]]", la cual idealizaba al primer Ku Klux Klan, y el caso [[Leo Frank]], un [[judío]] linchado en 1915 en [[Georgia]] tras ser acusado de violación y asesinato. El asesinato de Frank generó una oleada de antisemitismo por todo el país.

La segunda mitad del [[Siglo XIX]] en Estados Unidos se caracterizó por la [[Revolución industrial]], la inmigración desde el este hacia los nuevos territorios en el oeste y por llegada al país de una gran cantidad de inmigrantes europeos, sobre todo [[Inmigración irlandesa en Estados Unidos|irlandeses]] y [[Inmigración alemana en Estados Unidos|alemanes]]. Al Nuevo mundo también llegaron una cantidad considerable de [[judío]]s que llegaron a América huyendo de los [[pogromo]]s que sufrían en [[Europa]], sobre todo en el [[Imperio ruso]]. No obstante, a Estados Unidos también llegaron inmigrantes asiáticos, sobre todo [[japoneses]] y [[Inmigración china en Estados Unidos|chinos]], siendo estos últimos utilizados como mano de obra semi esclava en la construcción del ferrocarril. A finales del Siglo XIX, y sobre todo a comienzos del XX, el principal grupo inmigrante será el [[Inmigración italiana en Estados Unidos|italiano]]. 

Con excepción de la compra de [[Alaska]] a [[Rusia]] en [[1867]], la expansión territorial de Estados Unidos se había detenido en [[1848]]. No obstante, muchas aéreas que [[de iure]] pertenecían al gobierno estadounidense seguían estando controladas por tribus indias. Caben destacar las [[Guerras Navajo]], las [[Guerras apaches]] y sobre todo la [[Guerra de Black Hills]] contra los [[lakota]]s liderados por [[Toro Sentado]] y [[Caballo Loco]]. Fue durante esta guerra cuando tuvo lugar la [[Batalla de Little Bighorn]] en la cual el [[7.º Regimiento de Caballería (Estados Unidos)|7.º Regimiento de Caballería]] del [[Ejército de los Estados Unidos]], liderado por [[George Armstrong Custer]], sufrió una humillante derrota por parte de los hombres de Toro Sentado.

[[File:Ellis island 1902.jpg|thumb|300 px|left|Inmigrantes europeos en la [[Isla de Ellis]], 1902]]
Alrededor de [[1890]], al mismo tiempo que muchas naciones europeas expandían sus imperios coloniales, un nuevo espíritu animó la política exterior estadounidense, la cual en gran medida seguía las pautas de la Europa septentrional. El comienzo del colonialismo estadounidense suele situarse en el golpe de estado llevado a cabo por estadounidenses en el [[Reino de Hawái]] en el año 1893. La reina [[Liliuokalani]] es forzada a abdicar y se establece república, estado títere de los Estados Unidos, hasta que en 1898 Háwai queda definitivamente incorporado a los estados Unidos

Mientras tanto, en el [[Caribe (región)|Caribe]], [[Cuba]] se sublevaba contra el colonialismo de [[España]]. El gobierno de [[William McKinley]] vio en la rebelión la oportunidad perfecta de establecer un imperio colonial en América Latina. En [[1898]], tras la explosión accidental del [[USS Maine (ACR-1)|USS Maine]] en la bahía de [[la Habana]], de la cual se acusa a los españoles, estalla la [[Guerra hispano-estadounidense]]. Los estadounidenses no sólo atacarán Cuba, también enviarán a la Marina a [[Filipinas]]. En la [[Batalla de Cavite]], en la Bahía de [[Manila]], la flota española es fácilmente vencida por la norteamericana. Poco tiempo después, las tropas americanas desembarcan en [[Puerto Rico]] y Cuba. Tras poco más de tres meses de guerra, los españoles se rinden a los americanos. La paz se firmará a través del [[Tratado de París (1898)|Tratado de París]]. Cuba se independiza de España y Estados Unidos se hace con el control de Puerto Rico, las Filipinas y la isla de [[Guam]] en el Océano Pacífico. 
No obstante, poco tiempo después de la rendición española, estalla una nueva contienda en Filipinas, enfrentando a los estadounidenses con los independentistas filipinos, quienes les habían ayudado contra los españoles en la anterior guerra. Comienza así la [[Guerra Filipino-Estadounidense]], que asoló el archipiélago asiático. 

En la [[década de 1910]] estalla la [[Revolución Mexicana]]. Preocupado por la situación en el país, el presidente [[Woodrow Wilson]] ordena [[Ocupación estadounidense de Veracruz de 1914|ocupar Veracruz]] en [[1914]], en una acción que busca brindar apoyo a [[Venustiano Carranza]], revolucionario adín a los Estados Unidos. Además, en la frontera sur de los Estados Unidos se viven momentos de tensión. Comienza la llamada [[Guerra fronteriza]] que enfrenta a las tropas estadounidenses con las distintas facciones revolucionarias de México especialmente los [[maderistas]], los [[Venustiano Carranza|carrancistas]] y sobre todo los [[Pancho Villa|villistas]]. Estos últimos atacarían directamente a Estados Unidos en [[1916]] en la llamada [[Batalla de Columbus]]. Esta acción por parte de Villa provocó la [[Expedición Punitiva]] encabezada por el general [[John J. Pershing]] que oficialmente iniciaría el 14 de marzo de 1916 y concluiría el 17 de febrero de 1917. El propósito de la intervención de los estadounidenses del norte en México era capturar a Villa para ajusticiarlo en Estados Unidos. La expedición fracasó. 

También es importante explicar la injerencia por parte del [[Imperio alemán]] en México y otros lugares de [[América Latina]]. El interés alemán en [[Haití]] lleva a que en [[1915]], con Alemania ya involucrada en la [[Primera Guerra Mundial]], Estados Unidos [[Ocupación estadounidense de Haití|invada el país]]. Al año siguiente [[Ocupación estadounidense de la República Dominicana (1916-1924)|invaden la República Dominicana]]. La [[República Dominicana]] permanecerá ocupada hasta 1924 y Haití hasta 1934.

[[File:At close grips2.jpg|right|thumb|upright=1.2|Soldados estadunidenses en la Primera Guerra Mundial.]]

Cuando en [[1914]] estalló la [[Primera Guerra Mundial]] la mayoría de los ciudadanos estadounidenses se mostraban contrarios a que Estados Unidos interviniese en la misma. Además, debido a que los principales grupos migratorios eran el alemán y el británico (Tanto ingleses como irlandeses, ya que por aquel entonces Irlanda todavía no se había independizado del Reino Unido) la opinión pública estaba fuertemente dividida. Con el tiempo, especialmente después de los informes sobre las [[Violación de Bélgica|atrocidades en Bélgica]] en 1914 y después del hundimiento del buque de pasajeros [[RMS Lusitania]] en 1915, el pueblo estadounidense llegó a ver cada vez más a Alemania como el agresor.

En [[1917]], Alemania decidió reanudar la [[Guerra submarina indiscriminada|guerra submarina sin restricciones]] contra cualquier barco que se acercara a aguas británicas, incluyendo navíos estadounidenses. Alemania también se puso en contacto con el gobierno de Carranza en México a través del [[Telegrama Zimmermann]], interceptado por la inteligencia británica. En este documento Alemania se comprometía, a cambio de México atacase a Estados Unidos, a prestar asistencia financiera y armamentística para que este país recuperase los territorios de [[Texas]], [[Nuevo México]] y [[Arizona]].

Como consecuencia de esto, el [[6 de abril]] de [[1917]] el presidente [[Woodrow Wilson]] declaró la guerra a las [[Potencias Centrales]]. Por aquel entonces, el [[ejército de los Estados Unidos]] era pequeño y estaba mal equipado si lo comparamos con los ejércitos europeos. Millones de hombres tuvieron que ser reclutados, adiestrados, equipados y enviados a Europa a través de un océano infestado de submarinos. Para comienzos de [[1918]] ya había cerca de un millón de soldados de la [[Fuerza Expedicionaria Estadounidense]] en [[Francia]]. Al mando de esta unidad se encontraba [[John J. Pershing]]. Las fuerzas estadounidenses destacaron durante la [[Kaiserschlacht|Ofensiva de Primavera]] y la [[Ofensiva de los Cien Días]]. Wilson también envió tropas a la [[Rusia soviética]] en el marco de la [[intervención aliada en la Guerra Civil Rusa]], la llamada [[Expedición Oso Polar]] que permaneció en territorio ruso desde septiembre de 1918 hasta julio de [[1919]]

Los soldados estadunidenses llevaron a Europa el virus que provocó la devastadora [[Pandemia de gripe de 1918]], cuyo origen se suele situar en [[Fort Riley]], [[Kansas]]. Cerca de medio millón de estadounidenses fallecieron a causa de la gripe. 

En [[1919]], Wilson viajó a Europa para participar en la [[Conferencia de Paz de París (1919)|Conferencia de Paz de París]], la cual daría como resultado el [[Tratado de Versalles]] y el establecimiento de la [[Sociedad de Naciones]], idea propuesta por el propio Wilson. No obstante, muchos estadounidenses temían que dicha organización mundial arrastrara a Estados Unidos a otra guerra extranjera razón por la cual Estados Unidos nunca ratificó el Tratado de Versalles ni pasó a formar parte de la Sociedad.

[[Imagen:Chicago race riot, five policemen and one soldier.jpg|thumb|left|270 px|Policías y soldados en [[Chicago]] durante el [[Verano Rojo de 1919]].]]
La posguerra en Estados Unidos fue un período convulso. A la devastadora epidemia de gripe habrá que sumar una oleada de [[terrorismo anarquista]] y de fuertes movilizaciones obreras consecuencia de la [[Revolución rusa]]. Comenzaba el período conocido como [[Primer Temor Rojo]] cuyos puntos de máxima tensión fueron los ataques anarquistas de [[Luigi Galleani]] y el [[Muerte de Sacco y Vanzetti|juicio de Nicola Sacco y Bartolomeo Vanzetti]]. El verano de 1919 fue apodado "[[Verano Rojo de 1919|Verano Rojo]]" debido a las constantes protestas y disturbios. No obstante, la mayoría de las tensiones vividas aquel verano fueron disturbios raciales y no obreros. Era la época de la [[Gran Migración Negra]] cuando cerca de dos millones de afroamericanos se trasladaron del campo a la ciudad. Los negros hubieron de competir con los inmigrantes europeos y asiáticos por el trabajo, lo que dio lugar a fuertes protestas.

El [[17 de enero]] de [[1920]] entra en vigor la [[Decimoctava Enmienda a la Constitución de los Estados Unidos]] la cual prohibía "la fabricación, venta o transporte de licores embriagantes dentro de los Estados Unidos y de todos los territorios sometidos a su jurisdicción, así como su importación a los mismos." Comenzaba la [[Ley Seca]]. Esta ley no consiguió reducir el consumo de [[alcohol]] en los Estados Unidos, floreciendo un gigantesco [[mercado negro]] en manos de gánsteres como [[Al Capone]] o [[Lucky Luciano]].

[[Archivo:Chrysler Building, New York.jpg|thumb|right|250px|El [[Edificio Chrysler]] inaugurado en [[1930]] es considerado un símbolo de "los [[Felices años veinte]]".]]
[[Archivo:Lange-MigrantMother02.jpg|thumb|right|250px|"[[Florence Owens Thompson|Madre migrante]]", fotografía de [[Dorothea Lange]], todo un símbolo de la [[Gran Depresión]].]]
La [[década de 1920]] recibió el nombre de "los [[Felices años veinte]]" debido a la prosperidad económica que experimentaba el país. Estados Unidos desplazó a Gran Bretaña del liderazgo económico mundial. En [[Hollywood]], [[California]] la industria del cine genera millones de dólares. Es la edad de oro de la aviación, la era de los aviadores como [[Charles Lindbergh]], [[Amelia Earhart]] o [[Wiley Post]]. Del optimismo y de la bonanza económica también participó la [[Bolsa de valores]], que vivió un prolongado incremento de las cotizaciones, lo que permitió la formación de una burbuja especulativa, financiada por el crédito. Se estaba formando toda una [[burbuja especulativa]].

La burbuja acabó por explotar en el llamado [[Crac del 29]]. La [[Bolsa de Nueva York]] experimentó una brutal caída en el llamado [[Jueves Negro]] (24 de octubre de 1929), seguida por el catastrófico [[Lunes Negro]] y el [[Crack del 29|Martes Negro]] (28 y 29 de octubre de 1929). Una vez iniciado, el derrumbe en los precios de las acciones y de otros valores no pudo detenerse. Hacia [[1932]], miles de bancos y más de cien mil sociedades mercantiles habían [[quiebra|quebrado]]. La producción industrial se redujo a la mitad, el ingreso agrícola decayó en más del 50%, los salarios bajaron un 60%, la inversión nueva se redujo un 90%, y uno de cada cuatro trabajadores estaba desempleado. Comenzaba la [[Gran Depresión]]. Las zonas agrarias del país, como es el caso de los estados de [[Texas]], [[Nebraska]] u [[Oklahoma]] fueron las más perjudicadas, ya que a los devastadores efectos de la crisis hubo que sumar el "[[Dust Bowl]]", una devastadora [[sequía]]. Miles de personas hubieron de emigrar en dirección a zonas más prósperas como es el caso de [[California]]. En las afueras de las grandes ciudades se forman "[[Hooverville]]s", barrios de chabolas llamados así por el entonces presidente [[Herbert Hoover]].

En 1933 asume la presidencia el demócrata [[Franklin D. Roosevelt]], quien pone en marcha la política del "[[New Deal]]" con el objetivo de sacar al país de la crisis. Esta política, acusada de [[socialista]] por la oposición, establece subsidios de desempleo, la [[Ley de Seguridad Social (Estados Unidos)|Ley de Seguridad Social]], la [[Comisión de Bolsa y Valores]] y un mayor control sobre los [[banco]]s. Además, se realizan [[Obra pública|obras públicas]] por todo el país con el objetivo de generar empleo y dotar al país de infraestructuras tales como la [[Presa Hoover]]. Igualmente, en 1933 Roosevelt pone fin a la ley seca a través de la [[Vigesimoprimera Enmienda a la Constitución de los Estados Unidos|Vigesimoprimera Enmienda]]. El fin definitivo de la Gran Depresión llegó en [[1941]] con la entrada de Estados Unidos en la [[Segunda Guerra Mundial]] y consiguiente aumento de la producción industrial.

Cuando en [[1939]] [[Invasión alemana de Polonia de 1939|Alemania invadió Polonia]], dando comienzo a la [[Segunda Guerra Mundial]], la opinión pública estadounidense estaba dividida, igual que durante la [[Primera Guerra Mundial]]. De la misma manera que algunos sectores de la población eran contrarios al [[nazismo]], también existían defensores de esta ideología entre los que cabe destacar la figura de [[Charles Lindbergh]], firme defensor del [[Tercer Reich]] y del [[aislacionismo]] Igualmente existían asociaciones germanófilas, como es el caso de la [[German American Bund]]. A nivel institucional, durante la década de 1930 el Congreso había aprobado distintas [[Leyes de Neutralidad]] que buscaban evitar la guerra. Más hostil era la actitud ante el [[Imperio Japonés]]. La enemistad entre ambas potencias se remonta al [[Tratado naval de Washington]] de [[1922]] por el cual se restringía el número de buques de guerra que podía tener Japón. Tras la [[Invasión japonesa de Manchuria]] en [[1931]] y el comienzo de la [[Segunda Guerra Sino Japonesa]] en [[1937]], en la que tuvieron lugar eventos tales como el ataque nipón al barco estadounidense [[USS Panay (PR-5)|USS Panay]] o la [[masacre de Nankín]] (en la que fueron asesinados más de veinte mil chinos) hicieron crecer el [[sentimiento anti japonés]] en los Estados Unidos. 

Los crímenes japoneses en China, así como la [[Ocupación japonesa de Indochina|invasión de la Indochina francesa]] en [[1940]] provocaron que el gobierno de Roosevelt a imponer un embargo de las exportaciones de petróleo sobre Japón. Esto provocó un aumento de las tensiones entre ambos países y si bien figuras como la del primer ministro [[Fumimaro Konoe]] buscaron solucionar el problema de forma pacífica, el emperador [[Hirohito]] acabó decantándose por las tesis belicistas de [[Hideki Tōjō]], quien buscaba invadir las [[Indias Orientales Neerlandesas]] (actual [[Indonesia]]) ricas en [[petróleo]]. No obstante existía un grave impedimento a la hora de realizar esta acción: La [[Flota del Pacífico de los Estados Unidos]]. Por consiguiente, el [[7 de diciembre]] de [[1941]] la [[Armada Imperial Japonesa]] [[Ataque a Pearl Harbor|atacó la base estadounidense de Pearl Harbor]] en [[Hawái]]. Entre los días 7 y 8 los japoneses realizaron desembarcos en las posesiones estadounidenses de [[Filipinas]], [[Guam]] y [[Isla Wake|Wake]] a la vez que avanzaban en dirección a las Indias Orientales Neerlandesas a través de la [[Malasia británica]]. El [[8 de diciembre]] Estados Unidos declara la guerra a Japón. 

Alemania declaró la guerra a los Estados Unidos el [[11 de diciembre]]. El gobierno estadounidense moviliza a la armada en el Pacífico e incrementa el envío de suministros a [[Reino Unido]] y la [[Unión Soviética]]. El [[FBI]] comienza a perseguir a simpatizantes nazis. Asociaciones como la [[German American Bund]] son ilegalizadas. Se procede a la [[Campos de concentración para japoneses en Estados Unidos|detención e internamiento de japoneses y estadounidenses de origen japonés]]. En [[Filipinas]], las tropas del general [[Douglas MacArthur]] resisten hasta junio de [[1942]]. El [[8 de mayo]] de ese mismo año la amenaza japonesa contra [[Australia]] fue detenida en la [[batalla del Mar del Coral]]. En [[junio]] la principal flota japonesa, que navegaba rumbo a Hawái, fue rechazada en la [[batalla de Midway]], una de las más importantes batallas de la guerra. El ejército estadounidense también ayuda a la [[República de China]] en su lucha contra Japón.

En Europa, las [[Fuerzas Aéreas del Ejército de los Estados Unidos]] comienzan a bombardear Alemania, sus aliados y los territorios ocupados. En noviembre de 1942 tiene lugar la [[Operación Torch]], el desembarco aliado en el [[Protectorado francés de Marruecos]] y la [[Argelia francesa]], en manos del gobierno pro nazi de la [[Francia de Vichy]]. En mayo de [[1943]], el [[Protectorado francés de Túnez]], el último territorio del [[Potencias del Eje en la Segunda Guerra Mundial|Eje]] en [[África]], es conquistado por estadounidenses, británicos y por las tropas de la [[Francia Libre]]. En la [[Conferencia de Casablanca]] se acuerda invadir el [[Reino de Italia (1861-1946)|Reino de Italia]] a través de [[Sicilia]], la llamada [[Operación Husky]] Si bien Sicilia es rápidamente conquistada, la [[Campaña de Italia]] es lenta y sangrienta por lo que se decide llevar a cabo un nuevo desembarco, [[Batalla de Normandía|esta vez en Normandía]]. 

[[Archivo:Raising the Flag on Iwo Jima, larger - edit1.jpg|thumb|left|280px|"[[Alzando la bandera en Iwo Jima]]", fotografía de [[Joe Rosenthal]].]]
En el Océano Pacífico los estadounidenses combaten tanto en la región central ([[Campaña de las islas Gilbert y Marshall]]) como en las cercanías de [[Nueva Guinea]] ([[Campaña de las Islas Salomón]]). En este último frente tiene lugar la sangrienta [[Campaña de Guadalcanal]] entre agosto de [[1942]] y febrero de [[1943]]. Tras conquistar las Gilbert y Marshall, los estadounidenses [[Campaña de las islas Marianas y Palaos|atacan las islas Marianas y Palaos]] en junio de [[1944]]. Los estadounidenses recuperan [[Guam]] y conquistan [[Saipán]] y [[Peleliu]]. Con la conquista de las [[islas Marianas]], los estadounidenses ponen en marcha una dura campaña de bombardeos sobre el archipiélago japonés. 

En Francia, los Aliados avanzan hacia Alemania. Importante fue la intervención americana en la [[Batalla de las Ardenas]] y en la [[Batalla del Bosque de Hürtgen]]. Mientras, las tropas de Douglas MacArthur regresana las [[Filipinas]] en octubre de [[1944]]. La isla japonesa de Iwo Jima, en el Pacífico central, [[Batalla de Iwo Jima|cayó en manos de Estados Unidos]] en marzo de 1945 y la [[Isla de Okinawa]] [[Batalla de Okinawa|fue conquistada]] en junio de 1945. Franklin Roosevelt falleció el [[12 de abril]] a causa de una [[hemorragia cerebral]], siendo sucedido por [[Harry S. Truman]]. En Europa, la guerra finalizó el [[9 de mayo]] cuando los soviéticos se hicieron con el control de [[Berlín]].

La invasión de Japón sería larga y sangrienta por lo que, con la esperanza de llevar la guerra a un rápido fin, el presidente Truman ordenó usar la recién creada [[bomba atómica]] contra las ciudades de [[Hiroshima]] ([[6 de agosto]]) y [[Nagasaki]] ([[9 de agosto]]). El día 8 los soviéticas comienzan la [[Operación Tormenta de Agosto]], la invasión de [[Manchukuo]] y el norte de la [[Península Coreana]], controlados por Japón. Los [[bombardeos atómicos de Hiroshima y Nagasaki]] sumados al inminente avance soviético, provocó que el emperador Hirohito decidiese rendirse a los estadounidenses, mucho menos duros que los soviéticos, el día [[2 de septiembre]] de [[1945]] a bordo del [[acorazado]] "[[USS Missouri (BB-63)|USS Missouri]]". Terminaba así la Segunda Guerra Mundial.

[[File:HA-SC-98-06983-Crew of M24 along Naktong River front-Korean war-17 Aug 1950.JPEG|thumb|250 px|Tripulación de un tanque [[M-24 Chaffee|M-24]] estadounidense en las cercanías del [[Río Nakdong]] durante la [[Guerra de Corea]].]]
En [[1946]] Filipinas se independiza de Estados Unidos. Las negociaciones para la creación de un estado independiente habían comenzado con la [[Ley Tydings-McDuffie]] de [[1934]]

Durante las etapas finales de la [[Segunda Guerra Mundial]], cuando el [[Potencias del Eje en la Segunda Guerra Mundial|Eje]] estaba ya prácticamente derrotado, el gobierno estadounidense parecía más preocupado por la [[Unión Soviética]], potencia que se estaba haciendo con más y más territorios. Pese a que ambos estados colaboraron durante la guerra, la verdad es que ambas naciones eran rivales por no decir enemigas debido a sus sistemas económicos y políticos totalmente antagónicos ([[Capitalismo]] en los Estados Unidos, [[comunismo]] en la URSS). La enemistad entre ambas potencias se remonta hasta la [[Intervención aliada en la Guerra civil rusa]]. Una vez derrotados el [[Tercer Reich]] y el [[Imperio de Japón]], la rivalidad resurgió. Comenzaba así la [[Guerra Fría]]

Los primeros lugares donde se apreció la rivalidad entre potencias fueron la [[República de China]] y [[Corea]]. En la primera, una vez derrotado Japón, se reanudó la [[guerra civil china|guerra civil]] entre el [[Kuomintang]], de ideología capitalista y apoyado por Estados Unidos, y el [[Partido comunista chino]], apoyado por el gobierno de Stalin. Pese al apoyo de Estados Unidos, en [[1949]] los comunistas, liderados por [[Mao Zedong]] ganaron la guerra. En Corea, tras la [[Operación Tormenta de Agosto]], los americanos lanzan la [[Operación Cuarenta Lista Negra]], la invasión del sur. Por consiguiente, [[División de Corea|Corea queda dividida en dos]]. La ocupación soviética del norte terminó en 1948, creándose así [[Corea del Norte]], gobernada por [[Kim Il-sung]]. La ocupación estadounidense del sur termina ese mismo año. [[Syngman Rhee]] se hace con el poder en [[Corea del Sur]]. Ambos dictadores desean unificar Corea bajo su mando. En [[1950]] el norte comunista ataca al sur. Comienza así la [[Guerra de Corea]].

La [[ONU]] autoriza el envío una fuerza internacional al sur de Corea, coalición liderada por [[Douglas MacArthur]], gobernador del [[Ocupación de Japón|Japón ocupado por los estadounidenses]]. Las tropas estadounidenses [[Batalla de Inchon|desembarcan en Inchon]], liberan [[Seúl]] e invaden Corea del Norte hasta la frontera con [[China]] en el [[Río Yalu]]. [[Mao Zedong]] reacciona enviando al [[Ejército Popular de Liberación]], que derrota a los estadounidenses en la [[Batalla del embalse de Chosin]] y envía a las tropas de la ONU de vuelta a la frontera con el sur donde los combates durarías hasta el [[Acuerdo de Armisticio de Corea]] de [[1953]]. La guerra provocó la caída de [[Harry Truman]] y la llegada al poder del héroe de la [[Segunda Guerra Mundial]], el general [[Dwight D. Eisenhower]]. Douglas MacArthur por su parte fue destituido cuando propuso utilizar armas atómicas contra Corea del Norte y China. La [[Ocupación de Japón]] finalizó en [[1952]] con la firma del [[Tratado de San Francisco]]. Japón se convertiría junto a Corea del Sur, Filipinas y [[Tailandia]] en el principal aliado de Estados Unidos en Asia.

En la [[Europa]] de posguerra la situación era igualmente tensa. Los territorios liberados por la Unión Soviética se habían convertido en dictaduras comunistas mientras que los liberados por los Aliados occidentales ahora eran democracias capitalistas. Alemania [[Ocupación aliada de Alemania|
había quedado dividida]] en cuatro sectores: El francés, el británico, el soviético y el estadounidense. [[Berlín]], si bien quedaba dentro del sector soviético, había sido igualmente dividido. Estados Unidos pone en marcha el [[Plan Marshall]] que busca ayudar a la reconstrucción de Europa mediante ayudas económicas por valor de unos 14 000 millones de dólares de la época. Esto es visto como una amenaza por el gobierno de Stalin, que considera al Plan Marshall como un soborno para que los países europeos se posicionen del bando estadounidense. Como respuesta, ordena el [[Bloqueo de Berlín]] en julio de [[1948]]. El bloqueo de la parte occidental de la ciudad, la controlada por los Aliados occidentales, fue un fracaso debido al [[puente aéreo]] llevado a cabo por británicos y estadounidenses.

En [[1947]] se funda la [[Agencia Central de Inteligencia]] (CIA) la principal agencia de inteligencia del gobierno estadounidense. Durante la Guerra Fría llevaría a cabo misiones encubiertas contra movimientos comunistas alrededor de todo el mundo. Durante las [[Elecciones generales de Italia de 1948]] ayudó a la [[Democracia Cristiana (Italia)|Democracia Cristiana]] a ganar al [[Partido Comunista Italiano]] mediante una intensa campaña de propaganda. Igualmente, apoyó a los franceses en su lucha contra el [[Viet Minh]] durante la [[Guerra de Indochina]] y estuvo junto a la [[United Fruit Company]] detrás del [[Golpe de Estado en Guatemala de 1954]]. El gobierno de Eisenhower intervino en la [[Crisis del Líbano de 1958]], la primera intervención militar de Estados Unidos en [[Oriente Próximo]].

El [[1 de enero]] de [[1959]] en [[Cuba]], país aliado de Estados Unidos prácticamente desde su independencia, el dictador pro estadounidense [[Fulgencio Batista]] [[Revolución cubana|es derrocado]] por los comunistas del [[Movimiento 26 de Julio]], liderado por [[Fidel Castro]]. Supone un durísimo golpe para Estados Unidos debido a la escasa distancia entre Cuba y Florida. En abril de [[1961]] un grupo de disidentes cubanos llevan a cabo la [[Invasión de bahía de Cochinos]] con ayuda de la [[CIA]], un desesperado intento de derrocar a Castro que acaba en fracaso.. Ese mismo año en [[Berlín]] tiene lugar una nueva crisis cuando el gobierno de la [[República Democrática Alemana]] (Estado socialista creado por los soviéticos en su sector) levanta el [[Muro de Berlín]] para evitar la huida de ciudadanos en dirección a la parte capitalista de la ciudad, perteneciente a la [[República Federal Alemana]], creada a partir de los sectores británico, estadounidense y francés. 

[[File:Humanitarian G.I.'s. Firefight where G.I. pushes little kid under jeep for protection, Santo Domingo, May 5., 1965 - NARA - 541806.tif|thumb|250 px|left|Soldados estadounidenses durante la [[Ocupación estadounidense de la República Dominicana (1965-1966)|intervención estadounidense]] en [[República Dominicana]].]]
Una nueva crisis tendrá lugar en Cuba en [[octubre]] de [[1962]] cuando aviones espía [[Lockheed U-2]] de Estados Unidos descubrieron [[Misil balístico|misiles balísticos]] [[R-12 Dvina]] de fabricación soviética en la isla. Comenzaba la [[Crisis de los misiles en Cuba]], uno de los períodos de crisis más agudos de toda la [[Guerra Fría]]. Estados Unidos y la Unión Soviética estuvieron al borde de la [[guerra nuclear]]. Finalmente, [[John Fitzgerald Kennedy]] y [[Nikita Kruschev]] llegaron a un acuerdo. La URSS retiraría sus misiles de Cuba si Estados Unidos se comprometía a no invadir la isla y retiraba sus misiles estacionados en [[Turquía]], que era el principal aliado de los Estados Unidos en Oriente Próximo junto a [[Israel]] y al [[Reino de Irán]].

Durante la presidencia de Kennedy un número cada vez mayor de asesores militares había sido enviado a [[Vietnam del Sur]], país capitalista que se encontraba en guerra con [[Vietnam del Norte]], de ideología comunista y aliado con la URSS y China. Aún a día de hoy se sigue debatiendo si el gobierno estadounidense estuvo implicado en el [[arresto y asesinato de Ngô Đình Diệm]], el impopular dictador que gobernaba Vietnam del Sur, en [[1963]]. Pocos días después de la muerte de Diêm, [[Asesinato de John F. Kennedy|Kennedy es asesinado]]. Su sucesor, [[Lyndon B. Johnson]] aumentará la presencia estadounidense en Vietnam tras el [[Incidente del Golfo de Tonkín]], una [[operación de falsa bandera]] orquestada por la [[CIA]] en la que se fingió un ataque al [[destructor]] americano [[USS Maddox (DD-731)|USS Maddox]]. Comenzaba así la [[Guerra de Vietnam]]. Johnson igualmente [[Ocupación estadounidense de la República Dominicana (1965-1966)|ordenó invadir la República Dominicana]], país que atravesaba una fuerte crisis política y que los estadounidenses temían que se convirtiese en una "Segunda Cuba"

[[Archivo:Joseph McCarthy adjusted.jpg|right|thumb|El senador [[Joseph McCarthy]], impulsor del [[macartismo]].]]
En el ámbito interior, la Guerra Fría se manifestó en forma de un fuerte [[anticomunismo]], el segundo [[temor rojo]]. Entre 1950 y 1956 el senador [[Joseph McCarthy]] desencadenó un extendido proceso de declaraciones, acusaciones infundadas, denuncias, interrogatorios, procesos irregulares y listas negras contra personas sospechosas de ser comunistas: El [[Macartismo]]. Se investigó a actores de Hollywood como [[Humphrey Bogart]], [[Lauren Bacall]] o [[Gregory Peck]], a científicos como [[Robert Oppenheimer]], el creador de la bomba atómica, y a figuras del gobierno y del ejército estadounidense. Uno de los casos más famosos de este período fue el de los "[[Diez de Hollywood]]", diez guionistas y directores que se negaron a declarar ante el [[Comité de Actividades Antiestadounidenses]], un órgano del congreso estadounidense fundado en [[1938]] que no tenía que ver con Joseph McCarthy, quien era senador Junto a McCarthy y al Comité de Actividades Antiestadounidenses, el [[FBI]] de [[J. Edgar Hoover]], fue el principal responsable de las campañas anticomunistas que caracterizaron la [[década de 1950]] en Estados Unidos.
La década de 1950 fue además la época del [[Baby Boom]], un significativo aumento de la población consecuencia de la prosperidad económica que experimentaba Estados Unidos tras la Segunda Guerra Mundial. Para evitar que la época de bonanza acabase desencadenando en una nueva depresión económica en [[1944]] se aprobaron los [[Acuerdos de Bretton Woods]] a través del cual se establecía un [[Nuevo Orden Económico Internacional]], se sustituía el [[patrón oro]] por el [[patrón dólar]], lo que convertía a la economía estadounidense en la más importante del mundo. La vida cotidiana experimentó grandes cambios gracias a avances tecnológicos como la [[televisión]]. A mediados de la década de 1950 nace el estilo musical conocido como "[[rock and roll]]" con músicos estadounidenses como [[Elvis Presley]], [[Little Richard]] o [[Chuck Berry]] a la cabeza. Supuso un gigantesco cambio social que evolucionaría durante la década siguiente y que se convertiría en uno de los máximos exponentes de la [[cultura popular]] estadounidense.

La situación de mejoría económica y social sin embargo dejaba de lado a los [[afroamericanos]] y a otras minorías étnicas. A mediados de la década de 1950 nace el [[Movimiento por los derechos civiles en Estados Unidos|Movimiento por los derechos civiles de los negros]], el cual buscaba finalizar con las desigualdades producto de las [[Leyes Jim Crow]]. El nacimiento del movimiento suele situarse en el [[Boicot de autobuses de Montgomery]] de [[1955]] cuando una mujer negra llamada [[Rosa Parks]] fue detenida por negarse a ceder su asiento en un autobús a un blanco. El caso llegó hasta la [[Corte Suprema de los Estados Unidos]] que acabó declarando inconstitucional las ley que exigía la segregación en los autobuses. 

En [[1957]] tiene lugar la crisis de los [[Little Rock Nine]] en [[Little Rock]], [[Arkansas]], cuando nueve estudiantes negros trataron de acceder a un [[instituto]] blanco. El gobernador de Arkansas, [[Orval Faubus]] llamó a la [[Guardia Nacional]] de Arkansas el 4 de septiembre para evitar que los estudiantes accediesen al instituto. [[Eisenhower]] respondió enviando a la [[101.ª División Aerotransportada]] para escoltar a los estudiantes. El Movimiento por los derechos civiles fue ganado fuerza, apareciendo líderes como el pastor protestante [[Martin Luther King]] o el musulmán [[Malcolm X]], más agresivo que King. 

El [[28 de agosto]] de [[1963]] Martin Luther King pronuncia su famoso discurso "[[Yo tengo un sueño]]" en [[Washington DC]]. Las Leyes Jim crow van siendo eliminadas poco a partir de leyes tales como la [[Ley de Derechos Civiles de 1964]] o la [[Ley de derecho de voto de 1965]], ambas promulgadas por [[Lyndon B. Johnson]]. La lucha por los derechos civiles no fue fácil, ya que vino acompañada por un resurgimiento del [[Ku Klux Klan]]. El caso más mediático tuvo lugar el [[20 de junio]] de [[1964]] cuando los activistas Michael Schwerner, Andrew Goodman y James Chaney fueron asesinados en [[Missisipi]]. Igualmente, en los estados del sur gana popularidad el gobernador de [[Alabama]] [[George Wallace]] quien se presentará a las [[Elecciones presidenciales de Estados Unidos de 1968]] por el [[Partido Independiente Americano]] bajo el lema de "Segregación ahora y para siempre".

[[Archivo:JFK limousine.png|right|thumb|250 px|[[John Kennedy]] momentos antes de ser asesinado en [[1963]].]]
El [[22 de noviembre]] de [[1963]] [[Asesinato de John F. Kennedy|Kennedy es asesinado]] cuando se encontraba de visita en [[Dallas]], [[Texas]]. TrSi bien las investigaciones oficiales declararon culpable a [[Lee Harvey Oswald]], quien fue asesinado dos días después por un miembro de la [[mafia]] llamado [[Jack Ruby]], aún a día de hoy la muerte de Kennedy es uno de los eventos más polémicos de la historia de Estados Unidos.

Mientras en territorio americano la lucha por los derechos civiles continuaba, en [[Vietnam]] los estadounidenses no lograban realizar avances significativos. Debían hacer frente a un enemigo invisible, el [[Viet Cong]], la guerrilla comunista que operaba en Vietnam del Sur con apoyo del norte, gobernado por [[Hồ Chí Minh]]. La invasión terrestre de Vietnam del Norte era peligrosa pues podría provocar la entrada de [[China]] y la [[URSS]] en la guerra. Por consiguiente se decidió realizar una agresiva campaña aérea, la [[Operación Rolling Thunder]]. Pese a haber impulsado leyes que buscaban acabar con la segregación racial y un seguro de salud para los ancianos ([[Medicare]]) y para los pobres ([[Medicaid]]), la Guerra de Vietnam perjudicó la imagen de Johnson por lo que decidió no presentarse a la reelección.
Todo parecía apuntar a que [[Robert F. Kennedy]], hermano del difunto John Fitzerald Kennedy, iba a ser el sucesor de Johnson. Sin embargo, Robert fue asesinado el [[6 de junio]] de [[1968]] por un inmigrante palestino, [[Sirhan Sirhan]]. Los motivos de su asesinato siguen siendo un misterio. En líneas generales [[1968]] fue un año negro para los Estados Unidos. A la muerte de Robert habrá que sumar la de [[Martin Luther King]]. Además, en Vietnam las tropas del [[Ejército de Vietnam del Norte]] y el [[Viet Cong]] llevan a cabo un potente ataque contra el sur, la [[Ofensiva del Tet]]. La guerra tiene dividida a la opinión pública estadounidense, más cuando se descubrieron los [[crímenes de guerra]] perpetrados por soldados estadounidenses como es el caso de la [[Masacre de My Lai]] en la que entre trescientos y quinientos civiles vietnamitas fueron asesinados. La segunda mitad de la [[década de 1960]] fue la época de la [[contracultura]] y del [[movimiento hippie]]. Las protestas antibelicistas se extendían por todo el país a la vez que se vivían tensos disturbios raciales como [[Disturbios de Watts|los vividos en Watts en 1965]] o en [[Disturbios de Detroit en 1967|Detroit en 1967]]. El uso de drogas como la [[heroína]] o el [[LSD]] se populariza.

Mientras tanto, la [[Carrera espacial]] sigue su curso. El [[20 de julio]] de [[1969]], el programa espacial de los Estados Unidos logra un gran éxito técnico y propagandístico al [[Apolo 11|conseguir mandar astronautas y traerlos de vuelta sano y salvo a la Tierra]]. Los tripulantes [[Neil A. Armstrong]], [[Edwin Aldrin]] y [[Michael Collins (astronauta)|Michael Collins]] son recibidos como héroes. 

[[Archivo:UH-1D helicopters in Vietnam 1966.jpg|miniatura|Soldados estadounidenses y [[helicópteros]] [[Bell UH-1 Iroquois]] durante la [[Guerra de Vietnam]]|300 px]]
En Vietnam la situación es crítica. Si bien el presidente [[Richard Nixon]] había prometido retirar a las tropas, en [[1970]] ordena la [[invasión de Camboya]], invasión acompañada de un [[golpe de estado]] que provoca la caída del rey [[Norodom Sihanouk]] y la llegada al poder del dictador derechista [[Lon Nol]]. Igualmente, desde [[1969]] la [[Fuerza Aérea de los Estados Unidos]] lleva [[Operación Menú|bombardeando Camboya bajo el máximo secreto]]. Los sangrientos bombardeos provocan que gran parte de la población camboyana se una a la [[guerrilla]] [[maoísta]] de los [[Jemeres rojos]], liderada por el sangriento [[Pol Pot]]. Igualmente, en [[1971]] apoya a Vietnam del Sur en su [[Operación Lan Som 719|intento de invadir Laos]]. Nixon también ordenará la [[Operación Linebacker]] y la [[Operación Linebacker II]] en [[1972]], campañas de bombardeo sobre Vietnam del Norte las cuales tuvieron lugar mientras los [[Acuerdos de paz de París]] en los que se negociaba la paz. En [[1973]] Estados Unidos, derrotado, se retira de Vietnam. 

[[Archivo:President Nixon meets with China's Communist Party Leader, Mao Tse- Tung, 02-29-1972 - NARA - 194759.tif|miniatura|[[Richard Nixon]] se reúne con [[Mao Zedong]] en [[1972]]|300 px]]
El [[Consejero de Seguridad Nacional]] [[Henry Kissinger]] además llevó a cabo un acercamiento diplomático con la [[República Popular China]], acercamiento que se vio consolidado con la visita llevada a cabo por Nixon en 1972. Estados Unidos aceptó oficialmente el postulado de "[[Una sola China]]" que el gobierno de [[Pekín]], cortando relaciones diplomáticas con la [[República de China|China nacionalista]], gobernada por [[Chiang Kai-shek]]. El acercamiento con China tiene sus orígenes en la enemistad que surgió entre los gobiernos de Pekín y Moscú a partir de la muerte de [[Stalin]], la [[Ruptura sino-soviética]]. La enemistad entre ambas naciones llegó a su límite en [[1969]] con el [[Conflicto fronterizo sino-soviético]], que se saldó con varias decenas de muertes. Puesto que [[Vietnam]] era un estado comunista afín a Moscú, China apoyó a la [[Kampuchea Democrática]]. Cuando el gobierno de los Jemeres Rojos cayó en [[1978]], China y Estados Unidos colaboraron con los guerrilleros maoístas que se encontraban en la frontera con Tailandia. Los Jemeres Rojos utilizaron [[minas antipersona]] de fabricación estadounidense en su lucha contra [[Vietnam]].

En [[Latinoamérica]], la administración Nixon apoya distintos golpes de estado, siendo el más famoso [[Golpe de Estado en Chile de 1973|el que tuvo lugar en Chile en 1973]], el cual derrocó a [[Salvador Allende]] y trajo al poder al militar [[Augusto Pinochet]]. La [[CIA]] prestará asistencia a distintas dictaduras de América Latina a través de la [[Operación Cóndor]]. Las principales dictaduras pro estadounidenses fueron las de [[Dictadura militar (Chile)|Chile]], [[Proceso de Reorganización Nacional|Argentina]], [[Dictadura militar en Brasil|Brasil]], [[Alfredo Stroessner|Paraguay]], [[Dictadura cívico-militar en Uruguay (1973-1985)|Uruguay]], [[Hugo Banzer Suárez|Bolivia]].

El apoyo estadounidense a [[Israel]] durante la [[Guerra del Yom Kipur]] provoca que la [[Organización de Países Árabes Exportadores de Petróleo]] corte el suministro de [[petróleo]] a Estados Unidos y sus aliados. Comienza la [[Crisis del petróleo de 1973]]. Estados Unidos, con una economía débil producto de la Guerra de Vietnam, se ve duramente afectada. Ciudades como [[nueva York]] se encuentran al borde de la quiebra. Para colmo, el gobierno de Nixon atraviesa un grave escándalo, el [[Escándalo Watergate|Caso Watergate]], llamado así por el [[Complejo Watergate]], sede del Comité Nacional del Partido Demócrata de Estados Unidos. Las investigaciones de los periodistas [[Bob Woodward]] y [[Carl Bernstein]] del [[Washington Post]] descubrieron la intervención de la Administración Nixon en un caso de espionaje a rivales políticos. El [[Congreso de los Estados Unidos]] inició una investigación, pero la resistencia del gobierno de [[Richard Nixon]] a colaborar en ésta condujo a una crisis institucional. El escándalo destapó múltiples abusos de poder por parte del gobierno de Nixon, que se saldó con la dimisión de éste como [[Presidente de Estados Unidos]] en agosto de 1974. El escándalo salpicó a un total de 69 personas, de las cuales 48 fueron encontradas culpables y encarceladas; muchas de ellas habían sido altos funcionarios del gobierno de Nixon. Nixon fue sucedido por [[Gerald Ford]], quien le concedió el perdón presidencial. 

En [[1979]] tiene lugar una [[Crisis del petróleo de 1979|nueva crisis del petróleo]], resultado de la [[revolución iraní]] que derrocó al [[sah]] [[Mohammad Reza Pahleví]], aliado de Estados Unidos que se ve obligado a huir de [[Irán]]. El clérigo [[Ruhollah Jomeiní]] se convierte en el nuevo líder del país. En noviembre de 1979 [[Crisis de los rehenes en Irán|estudiantes islámicos asaltan la embajada estadounidense]], comenzando así una grave crisis diplomática. El gobierno de [[Jimmy Carter]] trató de rescatar a los rehenes en la llamada [[Operación Garra de Águila]] en [[1980]], que resulta en un humillante fracaso. Los rehenes finalmente serán liberados en [[1981]]. Ese mismo año [[Ronald Reagan]] asume la presidencia. Sesenta y nueve días después de asumir la presidencia, [[Intento de asesinato de Ronald Reagan|Reagan sufre un intento de asesinato]].

[[1979]] también es el año en el que la [[Unión Soviética]] entra en la [[República Democrática de Afganistán]], país comunista de [[1978]] y sumido en una guerra civil contra los [[Muyahidines]], integristas islámicos opuestos al comunismo. Durante las presidencias de Carter, y sobre todo la de Reagan, la [[CIA]] financiará a los muyahidines. Entre los líderes de la guerrilla se encuentra un millonario llamado [[Osama Bin Laden]]. [[Afganistán]] no fue el único lugar donde el gobierno de Reagan financió a movimientos anticomunistas. En [[Nicaragua]] se suministró apoyo, ilegal puesto el Congreso así lo había prohibido, a la [[Contras|Contra]], la cual buscaba derrocar a [[Frente Sandinista de Liberación Nacional]]. Además de [[Implicación de la CIA en el tráfico de drogas|recurrir al narcotráfico]], la CIA obtuvo dinero a través de la venta de armas a [[Irán]], [[guerra Irán-Irak|en guerra con el vecino Irak]]. En [[1986]] la venta ilegal de armas fue descubierta, comenzando así el escándalo [[Irán-Contra]].

[[Archivo:Carter Brezhnev sign SALT II.jpg|miniaturadeimagen|250px|left|[[Jimmy Carter]] y [[Leonid Brezhnev]] firmando el acuerdo "SALT II".]]
[[File:US Army Rangers parachute into Grenada during Operation Urgent Fury.jpg|miniaturadeimagen|250px|left|Paracaidistas estadounidenses durante la [[Invasión de Granada]].]]
Durante el gobierno de Reagan las tensiones de la [[Guerra Fría]] volvieron a aparecer. Los gobiernos de Nixon y Carter habían firmado los [[Acuerdos SALT]] con la Unión Soviética, acuerdos para limitar el número de sistemas de [[Misil antibalístico|misiles antibalísticos]] (ABM) en un intento de rebajar el clima de tensión entre ambas naciones. Reagan por el contrario aumentó las tensiones con la [[Iniciativa de Defensa Estratégica]], apodada "[[Guerra de las galaxias]]", un programa de investigación y tecnología para el establecimiento de un escudo defensivo ante un ataque soviético con armas balísticas estratégicas. La idea original era la de establecer una defensa anti-misiles desde el espacio que detectara la trayectoria de misiles balísticos y que pudiera destruirlos en diversos puntos de su trayectoria. El programa fue polémico no solo por su elevado coste, sino también porque suponía el primer paso para la militarización del espacio. La administración Reagan también recurrió a la acción militar directa. Estados Unidos formó parte, junto a [[Francia]], [[Reino Unido]] e [[Italia]], de una fuerza multinacional que intervino en la [[Guerra civil libanesa]] en [[1982]]. Esto provocó que el partido islamista [[Hezbolá]] [[Atentado contra la embajada de Estados Unidos en el Líbano de 1983|atacase la embajada estadounidense]] y [[Atentado contra los cuarteles en Beirut en 1983|cuarteles americanos y franceses]] en [[1983]], lo cual provoca la retirada de la Fuerza Internacional de Líbano. En un intento de levantar la moral, Reagan [[Invasión de Granada|ordena invadir la isla de Granada]], la cual estaba regida por un gobierno comunista afín a [[Cuba]].

En el campo económico bajo el gobierno de Reagan la situación económica mejoró considerablemente, pese a altibajos tales como el [[Lunes negro (1987)|Lunes negro de 1987]]. La economía estadounidense es considerablemente más fuerte que la soviética, en crisis debido a la [[guerra afgano-soviética]] y al [[Accidente de Chernobyl]]. La URSS no puede competir contra Estados Unidos en la nueva carrera armamentística. En [[1987]] tiene lugar la [[Cumbre de Reikiavik]] en un intento por frenar la carrera de armamentos. Las conversaciones fracasaron en el último minuto, pero el progreso logrado se materializó finalmente en 1987 en el [[Tratado INF|Tratado de Fuerzas Nucleares de Alcance Intermedio]]. El gobierno soviético de [[Mijaíl Gorbachov]] pone en marcha la [[Doctrina Sinatra]], una política que consistía en permitir a los países vdel Pacto de Varsovia resolver sus asuntos internos y fijar su evolución política. Los problemas económicos de la URSS repercutieron en todo el [[Bloque Oriental]], especialmente en [[Polonia]] y en la [[República Democrática Alemana]]. Comienzan las llamadas [[Revoluciones de 1989]], que acaban los regímenes comunistas del este de Europa. El [[9 de noviembre]] de [[1989]] tiene lugar la [[Caída del Muro de Berlín]]. Termina así la Guerra Fría.

El [[2 de agosto]] de [[1990]], [[Irak]] invade [[Kuwait]], país aliado de los Estados Unidos. Se forma una [[coalición de la guerra del Golfo|coalición internacional]] amparada por la [[ONU]] y liderada por Estados Unidos la cual es trasladada a [[Arabia Saudí]]. La [[Guerra del Golfo|guerra para expulsar a las tropas iraquíes de Kuwait]] comenzó con un bombardeo aéreo y naval el [[17 de enero]] de [[1991]], que continuó durante cinco semanas, la Operación Tormenta del Desierto. Esto fue seguido por un asalto terrestre el 24 de febrero. Esta fue una victoria decisiva para las fuerzas de la coalición, que liberaron a Kuwait y avanzaron hacia el territorio iraquí. La coalición cesó su avance y declaró un alto el fuego cien horas después de que comenzara la campaña terrestre.

[[File:ANG40InfantryDivisionLosAngelesRiot1992.jpg|thumb|300px|Dos soldados de la [[Guardia Nacional]] patrullando durante los [[disturbios de Los Ángeles de 1992]].]]
El [[29 de abril]] de [[1992]], en [[Los Ángeles]] tiene lugar una de las sentencias más polémicas de la historia de los Estados Unidos, cuando un jurado compuesto casi completamente por blancos absolvió a los cuatro agentes de policía que aparecieron en unas grabaciones tomadas por el videoaficionado George Holliday mientras propinaban una paliza al taxista negro [[Rodney King]], que incumplió las limitaciones de velocidad. Comenzaban así los [[Disturbios de Los Ángeles de 1992]], los más violentos en la historia de Estados Unidos desde la [[década de 1960]]. Se trató de un conflicto a tres bandas entre manifestantes [[afroamericanos]], incluyendo integrantes de pandillas como los [[Bloods]] y los [[Clips]]; el [[Departamento de Policía de Los Ángeles]] y los dependientes de la ciudad, en su mayoría de origen [[Etnia coreana|coreano]], que buscaban salvar sus negocios de los [[saqueo]]s y que no dudaban en utilizar [[armas de fuego]]. La situación de violencia era tal que la [[Guardia Nacional]] hubo de intervenir, al igual que otras unidades militares como es el caso de la [[1.ª División de Marines]]. Los disturbios se saldaron con cincuenta y cuatro muertos.

En [[1995]] tiene lugar el [[Atentado de Oklahoma City]], en el que un coche bomba mata a ciento sesenta y ocho personas en un edificio del gobierno federal. Los terroristas, [[Timothy McVeigh]] y [[Terry Nichols]], eran dos [[ultraderechista]]s que querían vengar el [[Asedio de Waco]] de [[1993]].

[[Archivo:National Park Service 9-11 Statue of Liberty and WTC fire.jpg|thumb|250px|La mañana de los atentados del 11 de septiembre.]]

En la mañana del [[11 de septiembre]] de [[2001]], dos de los cuatro aviones secuestrados por [[Al-Qaeda]] impactaron en las dos torres del [[World Trade Center]] en Nueva York, el tercero en el [[El Pentágono|Pentágono]] y el cuarto, que tenía como objetivo el [[Capitolio de los Estados Unidos]], se estrelló cerca de [[Shanksville, Pensilvania]]. Los motivos de [[Osama Bin Laden]] eran el [[apoyo militar estadounidense a Israel]], la presencia de tropas estadounidenses en [[Arabia Saudí]] y el intervencionismo estadounidense en conflictos tales como la [[guerra civil somalí]]. El ataque provocó la muerte a más de tres mil personas, convirtiéndose en el peor [[atentado terrorista]] en la historia estadounidense. Tras los ataques, el presidente [[George W. Bush]] puso en marcha [[Guerra en Afganistán (2001-presente)|invasión a Afganistán]] con el propósito de derrocar el [[Talibán|régimen talibán]] y sus conexiones terroristas, lográndolo en menos de un mes iniciado el conflicto. No obstante, la insurgencia Talibán continúa a día de hoy. Igualmente se recortaron la libertades públicas de Estados Unidos y se aprobaron polémicas leyes como la [[Ley USA PATRIOT]].

El [[20 de marzo]] de [[2003]] comienza la [[Guerra de Irak]], la invasión estadounidense de este país. La principal justificación para esta operación que ofreció el [[presidente de los Estados Unidos]], George W. Bush y sus aliados en la coalición, fue la afirmación de que [[Irak]] poseía y estaba desarrollando [[armas de destrucción masiva]] (ADM), violando un convenio de [[1991]]. Funcionarios de los Estados Unidos sostuvieron que Irak representaba una inminente, urgente e inmediata amenaza a los Estados Unidos, a su pueblo y a sus aliados, así como a sus intereses. Se criticó ampliamente a los servicios de información, y los inspectores designados al efecto [[Armas de destrucción masiva en Irak|no encontraron pruebas]] de que existieran las pretendidas armas de destrucción masiva. Después de la invasión, el Grupo de Investigación en Irak llegó a la conclusión de que Irak había terminado sus programas para desarrollar dichas armas en 1991 y no había ninguna en el momento de la invasión, pero que tenían la intención de reanudar la producción siempre y cuando se levantaran las sanciones. La inexistencia de dicahas armas no evito la destitución de [[Saddam Hussein]] y su posterior ejecución. Finalmente, en la guerra el bando encabezado por Estados Unidos logró algunas cosas: ganar la invasión, ocupar el país, derrocar al gobierno, e implantar un nuevo régimen con elecciones supuestamente democráticas. Lo que el bando encabezado por Estados Unidos no logró fue estabilizar el país, que se sumergió en una crisis socio-política y de guerra civil interna con presencia de grupos terroristas como Al Qaeda. No se puede decir que algún bando resultara victorioso o derrotado, pero lo cierto es que las tropas estadounidenses tuvieron que regresar en 2014 debido a la amenaza del [[Estado Islámico]] que ganó el terreno de un Irak consumido por la insurgencia rebelde al gobierno pro-estadounidense y las guerras civiles. Sumándose a la amenaza del Estado Islámico por petición del gobierno iraquí, EE. UU. decidió intervenir una vez más en la denominada [[guerra contra el Estado Islámico]]. Además de Irak, el Estado Islámico también ocupó parte del país vecino Siria

Desde [[2004]] la carrera del senador [[Barack Obama]] fue meteórica. Sus promesas de cambio y su famoso lema "Yes, we can " le dieron fama mundial y le llevarían a la [[Casa Blanca]], tras ganar con una ventaja considerable las elecciones de [[2008]], convirtiéndose en uno de los [[presidentes de Estados Unidos]] que más fuerza consiguió en las urnas. El candidato demócrata Obama tuvo que afrontar la [[crisis financiera de 2008]], la tensión con [[Irán]], la resolución de las guerras de [[Irak]] y [[Afganistán]] y los problemas del medio ambiente. También, Barack Obama tuvo entre los objetivos que alcanzó relativamente la mejora de la política exterior con [[Europa]] y el diálogo con todos los gobiernos del mundo.


[[Categoría:Historia de Estados Unidos| ]]

</doc>
<doc id="15504" url="https://es.wikipedia.org/wiki?curid=15504" title="Escultura de Italia">
Escultura de Italia

Las esculturas etruscas son principalmente en terracota o bronce.
Modelaron las figuras de los muertos, que aparecían recostados sobre los sarcófagos.
Con la escultura etrusca apareció el retrato realista, saliendo del idealismo del arte griego.

Tiene cierta semejanza con la primitiva escultura griega y cierta influencia mesopotámica.

Las principales obras de este período son la Quimera de Arezzo, la Loba capitolina, el Apolo de Veyes entre otras.

La escultura romana no tuvo un estilo propio hasta pasado cierto tiempo.

Sus primeras influencias fueron los etruscos. Más adelante, según aumentaba el territorio romano y avanzaban las conquistas, grandes cantidades de esculturas griegas llegaron a Roma como botín de guerra y también llegaron los escultores griegos. Realizaron de estas esculturas miles de copias que adornarían los jardines y los edificios públicos romanos.
De los etruscos heredaron el realismo de las imágenes de cera que realizaban a sus difuntos, de los griegos el idealismo. De época republicana destacan los retratos de Julio César, Cicerón y Pompeyo.
El idealismo griego se puede apreciar en las obras del principio del imperio (s.I a.c) como el Augusto de Prima Porta, o los retratos de Calígula y Tiberio.

Posteriormente la época de los flavios y durante la anarquía militar del s. III d.c predominó la corriente más propia del realismo. Durante el reinado de los Antoninos el retrato tiende al barroquismo. Muestra de ello son los retratos de Cómodo, Antonio Pío y la Estatua ecuestre de Marco Aurelio.

En Roma también se esculpieron relieves, las influencias fueron las mismas, siendo el realismo una tendencia más popular y el idealismo más aristocrático. En los relieves, los artistas romanos hicieron uso de recursos pictóricos como las perspectivas. Y detalles anecdóticos.
La influencia más clara de Grecia se aprecia en los relieves del Ara Pacis de Augusto, esta tendencia idealista se fue perdiendo con el tiempo, aunque se mantiene en la Columna de Trajano o el Arco de Tito, pero es más débil en la Columna de Marco Aurelio en que sus relieves representan el horror de la guerra.

Las obras más destacadas de la escultura bizantina son las labores ornamentales de los capiteles con motivos vegetales y animales afrontados, como son los de San Vital de Rávena o los sarcófagos de la misma ciudad, en los que se representan los temas del Buen Pastor.

Pero las obras capitales de la escultura bizantina son las pequeñas obras, dípticos y cajas talladas en marfil, destacando el díptico Barberini, Museo del Louvre, del siglo V, o la célebre Cátedra del Obispo Maximiano, en Rávena, tallada hacia el año 533 sobre placas de marfil con minucioso trabajo.

Durante el románico, en el resto de Europa la escultura estuvo subordinada a la arquitectura, siendo una simple ornamentación, principalmente en las puertas de las iglesias y catedrales.
Pero en la mayor parte del territorio italiano, la decoración escultórica no existía, en el arte románico particular italiano se le dio más importancia al color, por lo que la decoración de las fachads no era esculpida si no que era pintada o utilizaba mármoles de diferentes colores.
Pero en general el románico italiano, al igual que el gótico fue más clasicista que en el resto de Europa.

La escultura gótica italiana se desarrolla principalmente en la Toscana y el norte de la península.
Son los lugares donde Nicola Pisano esculpió los relieves del púlpito del baptisterio de la Catedral de Pisa y de la Catedral de Siena.
Nicola Pisano tuvo una tendencia marcadamente clasicista que prácticamente se anticipa al renacimiento.
Por otro lado, su hijo Giovanni está más influido por la corriente internacional, tomando características propias del gótico francés como del alemán.

Finalmente con Lorenzo Ghiberti termina el gótico, conserva ciertos rasgos de la escultura gótica aunque volviendo en cierto modo al clasicismo lo que conducirá al renacimiento.




</doc>
<doc id="15505" url="https://es.wikipedia.org/wiki?curid=15505" title="Beckmannia">
Beckmannia

Beckmannia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del Norte de Eurasia y América del Norte. 
El género fue descrito por Nicolaus Thomas Host y publicado en "Icones et Descriptiones Graminum Austriacorum" 3: 5. 1805. La especie tipo es: "Beckmannia eruciformis" (L.) Host 
Beckmannia: nombre genérico que fue nombrado en honor de Johann Beckmann.

El número cromosómico básico del género es x = 7, con números cromosómicos somáticos de 2n = 14 (generalmente), o 16. 2 ploide. Cromosomas relativamente «grandes». 



</doc>
<doc id="15507" url="https://es.wikipedia.org/wiki?curid=15507" title="Sputnik 1">
Sputnik 1

El lanzado el 4 de octubre de 1957 por la Unión Soviética, fue el primer satélite artificial de la historia.

El Sputnik 1 fue el primero de varios satélites lanzados por la Unión Soviética en su programa Sputnik, la mayoría de ellos con éxito. Le siguió el Sputnik 2, como el segundo satélite en órbita y también el primero en llevar a un animal a bordo, una perra llamada Laika. El primer fracaso lo sufrió el Sputnik 3.

La nave Sputnik 1 fue el primer intento exitoso de poner en órbita un satélite artificial alrededor de la Tierra. Se lanzó desde el Cosmódromo de Baikonur en Tyuratam, 371 km al suroeste de la pequeña ciudad de Baikonur, en Kazajistán (antes parte de la Unión Soviética). La palabra "sputnik" en ruso significa "compañero de viaje" ("satélite" en astronáutica). El nombre oficial completo, se traduce sin embargo como "Satélite Artificial Terrestre" (ISZ por sus siglas en ruso).

El Sputnik 1 fue el primero de una serie de cuatro satélites que formaron parte del programa Sputnik de la antigua Unión Soviética y se planeó como una contribución al Año Geofísico Internacional (1957-1958), establecido por Organización de las Naciones Unidas. Tres de estos satélites (Sputnik 1, Sputnik 2 y Sputnik 3) alcanzaron la órbita terrestre. El Sputnik 1 se lanzó con el vehículo de lanzamiento R-7 y se incineró durante su reentrada el 4 de enero de 1958.

La secuencia real de toma de decisiones en lo que respecta a la forma del Sputnik 1 fue enrevesada. Inicialmente el Académico Mstislav Kéldysh ideó un satélite de 1,5 t en forma de cono, con la capacidad de hacer muchas mediciones físicas en el espacio, pero cuando los soviéticos leyeron que el proyecto estadounidense Vanguard tenía diseñados, y planeados dos satélites, uno pequeño tan sólo para ver si podían poner algo en órbita, los rusos decidieron hacer lo mismo, realizando lo que se traduce como "el satélite más simple", que tenía un centímetro más de diámetro y era bastante más pesado que el Vanguard. Ellos tuvieron que ver si las condiciones en órbita terrestre baja podían permitir a un satélite mayor permanecer allí durante el tiempo necesario. Cuatro meses después del lanzamiento del Sputnik 1, fue puesto en órbita el satélite de prueba Vanguard, Jruschev lo ridiculizó comparándolo con un "pomelo". Una vez que los soviéticos descubrieron que también podían poner en órbita satélites de prueba, pensaron en poner en órbita el satélite y laboratorio espacial Keldysh como Sputnik 3, haciéndolo tras un primer lanzamiento fallido.

El Sputnik 1 tenía una masa aproximada de 83 kg, contaba con dos transmisores de radio (20,007 y 40,002 MHz) y orbitó la Tierra a una distancia de entre 938 km en su apogeo y 214 km, en su perigeo. El análisis de las señales de radio se usó para obtener información sobre la concentración de los electrones en la ionosfera. La temperatura y la presión se codificaron en la duración de los pitidos de radio que emitía, indicando que el satélite no había sido perforado por un meteorito.

El satélite artificial Sputnik 1 era una esfera de aluminio de 58 cm de diámetro que llevaba cuatro largas y finas antenas de 2,4 a 2,9 m de longitud. Las antenas parecían largos bigotes señalando hacia un lado. La nave obtuvo información perteneciente a la densidad de las capas altas de la atmósfera y la propagación de ondas de radio en la ionosfera. Los instrumentos y fuentes de energía eléctrica estaban alojadas en una cápsula que también incluía transmisores de radio operando a 20,007 y 40,002 MHz. (alrededor de 15 y 7,5 m en longitud de onda), las emisiones se realizaron en grupos alternativos de 0,3 s de duración. El envío a tierra de la telemetría incluía datos de temperatura dentro y sobre la superficie de la esfera.

Debido a que la esfera estaba llena de nitrógeno a presión, el Sputnik 1 dispuso de la primera oportunidad de detectar meteoritos, aunque no detectó ninguno. Una pérdida de presión en su interior, debido a la penetración de la superficie exterior, se habría reflejado en los datos de temperatura.

El Sputnik 1 fue el primero de una serie de cuatro satélites que formaron parte del programa Sputnik de la antigua Unión Soviética y se planeó como una contribución al Año Geofísico Internacional, establecido por Organización de las Naciones Unidas. Lanzado desde el Cosmódromo de Baikonur en Kazajistán, antes parte de la Unión Soviética. El Sputnik 1 se lanzó en un cohete R-7 y se incineró durante su reentrada el 4 de enero de 1958. Tres de estos satélites (Sputnik 1, Sputnik 2 y Sputnik 3) alcanzaron la órbita terrestre.

Los transmisores funcionaron durante tres semanas, hasta que fallaron las baterías químicas a bordo, y fue monitorizado con gran interés a lo largo de todo el mundo. La órbita del entonces satélite inactivo fue observada más tarde ópticamente, hasta caer 92 días después de su lanzamiento (4 de enero de 1958), después de haber completado alrededor de 1440 órbitas a la Tierra, acumulando una distancia de viaje, de aproximadamente unos 70 millones de km. El apogeo de la órbita decayó de 947 km tras el lanzamiento hasta 600 km el 9 de diciembre.

El cohete auxiliar de lanzamiento del Sputnik 1 también alcanzó la órbita terrestre y fue visible de noche, desde la Tierra, como un objeto de primera magnitud, mientras que la pequeña pero pulida esfera, apenas era visible en sexta magnitud, por lo que era más difícil seguirla desde Tierra. Varias réplicas del satélite Sputnik 1 pueden verse en museos de Rusia; hay otra junto a la embajada de Rusia en Madrid en España y una más está expuesta en el Smithsonian "National Air and Space Museum" (Museo Nacional Smithsonian del Aire y del Espacio) en Washington D. C.

En el 2003 una unidad de reserva del Sputnik 1, llamada "modelo PS-1" se vendió en eBay (sin la radio, que fue extraída durante los años 60 al ser clasificada como material militar). Había estado en exposición en un instituto de ciencias cerca de Kiev. Se estima que se construyeron de cuatro a veinte modelos con propósitos de prueba. Un modelo del Sputnik 1 se entregó como regalo a las Naciones Unidas y ahora decora el vestíbulo de entrada de sus oficinas centrales en Nueva York.

En el 2001 se publicó el libro "Sputnik: The shock of the century", de Paul Dickson.




</doc>
<doc id="15509" url="https://es.wikipedia.org/wiki?curid=15509" title="Programa espacial de la Unión Soviética">
Programa espacial de la Unión Soviética

Se engloban bajo la etiqueta del programa espacial soviético las iniciativas astronáuticas desarrolladas por la URSS desde 1957 hasta el momento de su disolución en 1991. Las ambiciones espaciales rusas empezaron en el siglo XIX, tuvieron sus primeros estudios teóricos en el inicio del siglo XX y se desarrollaron principalmente durante la Guerra Fría en la Unión Soviética. Los soviéticos fueron pioneros en la carrera espacial al ser los primeros en lanzar un satélite a la órbita terrestre, el Sputnik 1; en enviar un ser vivo al espacio, la perra Laika; a un ser humano, Yuri Gagarin; y a la primera mujer, Valentina Tereshkova; en realizar un paseo espacial, Alekséi Leónov; en enviar a la primera mujer que realizó un paseo espacial, Svetlana Savítskaya; en poner en órbita una estación espacial, la Saliut; y en lanzar las primeras sondas interplanetarias a Marte, la Mars 1, y a Venus, la Venera 1.

El 4 de octubre de 1957, el satélite Sputnik 1 fue lanzado con éxito por un cohete R-7. El primer satélite artificial puesto en órbita sorprendió a los estadounidenses, que crearon la NASA para desarrollar su programa espacial e intentar alcanzar a los soviéticos.

El principal ingeniero a cargo fue Serguéi Koroliov. Tuvo un gran equipo, otro miembro destacado fue Borís Yevséyevich Chertok, que estuvo a cargo del desarrollo de sistemas de guía y control.

Un mes después, el 3 de noviembre de 1957, la perra Laika fue enviada al espacio a bordo del Sputnik 2. El objetivo de la misión fue estudiar los efectos de un viaje al espacio en un ser vivo. Laika fue la primera criatura viva en orbitar la Tierra.

Tras el colapso de la Unión Soviética, Rusia y Ucrania heredaron el programa espacial. Rusía creó la Agencia de Aviación y del Espacio Rusa, actualmente conocida como Agencia Espacial Federal Rusa, mientras que Ucrania creó la Agencia Espacial Estatal de Ucrania (NSAU).

El 12 de julio de 2007, Rusia celebró el 100 aniversario del nacimiento de Serguéi Koroliov, el presidente Putin entregó reconocimientos y flores a la hija del padre del programa espacial.

Las ideas de la exploración espacial ya existían en el Imperio ruso aún antes de la Primera Guerra Mundial. En sus trabajos pioneros, Konstantín Tsiolkovski había escrito y hablado sobre esto explicando el concepto de cohetes con múltiples etapas.

El primer cohete soviético, llamado GIRD, fue lanzado el 18 de agosto de 1933. Luego, el 25 de noviembre de 1933, se lanzó un cohete híbrido de combustible especial llamado GIRD-X. Ya para la época de 1940-41 se llegó a otro avance en la propulsión de cohetes para producir en serie los cohetes para el sistema múltiple Katyusha.

Otra contribución para el avance del programa soviético lo constituyó el trofeo de Guerra los V-2, el encargado del proyecto fue Dmitri Ustínov, y el diseñador e Ingeniero en jefe Serguéi Koroliov también contaron la ayuda de planos capturados y del científico alemán Helmut Gröttrup lograron construir una réplica del V-2 y lo llamaron Cohete R-1

Pero el peso de las primeras cabezas nucleares soviéticas requería un propulsor más poderoso, después de varias pruebas con otros modelos Koroliov construyó el R-7 que logró llevar una carga a una distancia de 7000 km., convirtiéndose en ese momento como el cohete más avanzado de la época.

Años más tarde el programa espacial soviético entró en un plan quinquenal y obtuvo también apoyos del ejército soviético, en enero de 1956 se aprobó el plan para desarrollar Satélites que orbitaran el planeta y obtener más conocimientos del ambiente espacial (Sputnik) y también para ganar experiencia militar espacial (Zenit).

Después del éxito mundial con el Spútnik, a Koroliov se le pidió marchas forzadas para el desarrollo de un programa tripulado y así producir la nave espacial Vostok.

Después de la muerte de Koroliov en 1966, Kerim Kerímov quedó a cargo de la construcción del Vostok 1. Kerímov fue nombrado como Jefe la comisión de Vuelos tripulados y estuvo en ese cargo por más de 25 años (1966-1991). Él supervisó cada una de las etapas de desarrollo y operación de vuelos tripulados y misiones de sonda espaciales de la Unión Soviética. Uno de sus más grandes logros fue la puesta en órbita de la estación espacial Mir en 1986.

El programa espacial soviético estaba atado al Plan Quinquenal la URSS y desde su comienzo dependía del apoyo de los militares soviéticos. Aunque Koroliov deseaba conseguir su sueño de los viajes espaciales, guardaba estos deseos en secreto mientras trabajaba en proyectos militares ya que muchos se mofaban de la idea de los lanzamientos de satélites y las naves tripuladas. Sin embargo, el primer cohete soviético con animales a bordo fue lanzado en julio de 1951. Los dos perros que volvieron a salvo después de alcanzar los 101 km de altitud. Este hito y los subsiguientes vuelos dieron a los soviéticos una experiencia valuiosa en el campo de la medicina espacial.

Las capacidades del cohete R-7, como su alcance globan y soportar la carga de aproximadamente cinco toneledas, no solo lo convertían en un efectivo medio para el transporte de cabezas nucleares si no también la base perfecta para un vehículo espacial. El anuncio por parte de Estados Unidos de lanzar un satélite durante el Año Geofísico Internacional en julio de 1957 benefició enormemente a Koroliov a la hora de persuadir al líder soviético Nikita Jrushchov para que apoyase sus planes y así anticiparse a los americanos. Por tanto se aprobaron planes para el uso de satélites orbitales terrestres (Sputnik) para obtener conocimiento sobre el espacio, y cuatro satélines militares no tripulados de reconocimiento (Zenit). Además, se desarrollaron planes futuros para vuelos tripulados en la órbita de la tierra programados para 1964 y una misión lunar no tripulada en una fecha más cercana.
Después del éxito del primer Sputnik, se le encargó a Koroliov - cuya identidad no se conocía públicamente- fue encargado con acelerar el programa tripulado, diseño el cuál se combinó con el programa Zenit para dar lugar a la nave espacial Vostok. Influenciado por Tsiolkovsky - el cuál había elegido Marte como el logro más importante en los viajes espaciales- a comienzos de los años 60 el programa soviético bajo el mando de Koroliov creó substanciales planes de viajes tripulados a Marte entre 1968 y 1970. 

El programa espacial soviético era secundario en cuanto a la financiación militar de Tropas de Misiles de Designación Estratégica de la Federación Rusa. Mientras que en el oeste se creía que era Jrushchov quien personalmente pedía cada nueva misión espacial con objetivo de hacer propaganda, más bien él enfatizaba la creación de misiles que las misiones espaciales y no tenía ningún interés en competir con la misión Apolo.

Mientras el gobierno y el Partido Comunista usaban los éxitos del programa espacial como herramientas de propaganda, planes para misiones basadas en razones políticas no fueron muy comunes. Una excepción fue Valentina Tereshkova la primera mujer en el espacio en 1963, a bordo del Vostok 6. Las misiones se planeaban basándose en la disponibilidad de cohetes y otras razones específicas, más que con objetivos científicos. Por ejemplo, en febrero de 1962 solicitaron de forma abrupta una ambiciosa misión que involucraba el lanzamiento de dos Vostoks simultáneamente a la órbita "en un plazo de diez días" para eclipsar el Mercury-Atlas 6 de John Glenn este mismo mes. El programa, sin embargo, no puedo ser lanzado hasta agosto, con el Vostok 3 y el Vostok 4.

Al contrario del programa espacial americano que tenía a la NASA como única estructura coordinadora dirigida por su administrados, James Webb durante la mayor parte de los años 60, el programa de la USSR se dividía entre varios grupos de diseño que competían entre ellos. A pesar de los éxitos de los Sputniks entre 1957 y 1961 y los Vostoks entre 1961 y 1964, después del 1958 el equipo de Koroliov se enfrentó a la competición de sus jefes de diseño rivales, Mijaíl Yánguel, Valentín Glushkó y Vladimir Chelomél. Koroliov planeaba continuar con la nave Soyuz y el cohete N-1 que sería la base permantete de la estación espacial tripulada y la exploración tripulada a la Luna. Sin embargo, Ustinov le dirigió para que se centrase en misiones cercanas a la Tierra usando la viable nave espacial Vosjod, una modificación de la Vostok, al igual que musión no tripuladas interplanetarias a planetas cercanos tales como Venus o Marte.

Yánguel había sido el asistente de Koroliov pero con el apoyo de los militares se le obsequió de su propio gabinete de diseño en 1954 para trabajar principalmente en el programa espacial militar.

Glushkó fue el jefe de diseño de cohetes pero su fricción personal con Koroliov provocó que se negase a desarrollar los motores de una única cámara criogénica que necesitaba Koroliov para construir cohetes más potentes. 

Cheloméi se benefició del amparo de Jrushchov y en 1960 se le fue concedido el trabajo de desarrollo de un cohete para enviar una nave tripulada a la Luna y una estación espacial militar tripulada. Debido a su limitada experiencia espacial, sus desarrollos fueron lentos. 

El progreso del programa Apolo alarmó a los diseñadores jefes, cada cuál defendió su proyecto como respuesta. Múltiples proyectos, que se pisaban entre ellos, recibieron el visto bueno y nuevas propuestas amenazaron proyectos que ya habían sido aprobados. Dada la "persistencia singular" de Koroliov, en agosto de 1984 - más de tres años después de la declaración de intenciones de los Estados Unidos- la Unión Soviética decidió finalmente competir por la Luna. Se marcó el hito del aterrizaje lunar en 1967. - el 50 aniversario de la Revolución de Octubre - o 1968. En una primera etapa al comiendo de los años 60 el programa soviético del espacio desarrolló 30 proyectos de naves espaciales y lanzaderas. Con la caída de Nikita Jrushchov en 1964, se le dio a Koroliov el completo control del programa tripulado del espacio.

Koroliov falleció en enero de 1966 tras una operación en la que se descubrió su cáncer de colón y tras complicaciones con una enfermedad del corazón y sveras hemorragias. Kerim Kerímov, que había sido el arquitecto del Vostok 1, fue designado presidente de la comisión estatal de vuelos tripulados y mantuvo el cargo durante 25 años (1966-1991). Él supervisó cada etapa de desarrollo y operación de vuelos espaciales tripulados y estaciones intraplanetarias no tripuladas de la Unión Soviética. Uno de los grandes logros de Kerímov fue el lanzamiento del Mir en 1986.

El liderazgo del gabinete de diseño del OKB-1 fue concedido a Vasili Mishin, el cuál tenía la tarea de enviar el hombre a la Luna en 1967 y que aterrizase uno en 1968. Bajo presión Mishin aprobó el lanzamiento del Soyuz 1 en 1967, pese a que la nave núncia había conseguido ser testada sastifactoriamente en un vuelo no tripulado. La misión fue lanzada con conocimiento de fallos de diseño y terminó con el vehículo estrellándose contra el suelo, matándo a Vladímir Komarov. Esta fue la primera fatalidad en vuelo en cualquier programa espacial.

Dado el desastre y bajo nuevas presiones, Mishin desarrolló un problema de alcoholismo. Los soviéticos fueron vencidos en la carrera por enviar el primer vuelo tripulado alrededor de la Luna en 1968 por el Apolo 8, pero Mishin continuó con el desarrollo del problemático y superpesado N1 con la esperanza de que los americanos se tomasen un respiro, dejando suficiente tiempo para hacer funcionar el N1 y ser los primeros en aterrizar el hombre en la Luna. Tuvieron un éxito con el vuelo conjunto del Soyuz 4 y Soyuz 5 en enero de 1969 que probó las técnicas de encuentro, atraque y transferencia de tripulación que serían usadas en el aterrizaje, y el módulo de aterrizaje LK fue testado satisfactoriamente en la órbita terrestre. Pero tras cuatro lanzamientos fallidos de N1 no tripulados, el cohete fue abandonado y con ellos las posibilidades de los soviéticos de aterrizar hombres en la Luna en un único lanzamiento. 

Aparte de los aterrizajes tripulados, el abandonado programa soviético de la Luna incluía una base lunar polivalente Zvezdá. El posterior programa lunar tripulado propuesto, "Vulkan-LEK", no fue adoptado por razones económicas.

Tras los contratiempos, Chelomei convenció a Ustinov para aprobar el programa en 1970 para avanzar su estación espacial militar, Almaz, a razón de batir a la anunciada estadounidense Skylab. Mishín continuó en control del proyecto que posteriormente se convirtió en Saliut pero la decisión respaldada por Mishín de volar una tripulación de tres hombres sin trajes presurizados más que una tripulación de trajes similares a los de Saliut 1 en 1971 provocó una fatalidad cuando la reentrada en la cápsula despresurizada mató a la tripulación en la vuelta a la Tierra. Mishín fue destituido de muchos proyectos, dejando a Chelomei con el control de Saliut. 

A pesar de los problemas en sus primeros programas tripulados lunares, la URSS obtuvo éxitos en sus operaciones remotas a la Luna, obteniendo dos históricos hitos con el programa Lunojod y la obtención de muestras de la Luna. Además, el Programa Mars fue continuado con pequeños éxitos, mientras que la exploración de Venus y el cometa Halley por el programa Venera y el programa a la estrella Vega fueron más efectivos.

El programa espacial soviético había ocultado información sobre sus proyectos anteriores al éxito del Sputnik, el primer satélite artificial del mundo. De hecho, cuando el programa Sputnik fue aprobado por primera vez, una de las primeras acciones que tomó el Politburó fue qué información anunciar al mundo con respecto a este evento. La ITAR-TASS estableció un precedente por todos los anuncios oficiales del programa espacial soviético. La información que finalmente se revelaron no ofrecían detalles de quien construyó y lanzó el satélite o porque fue lanzado. 

El secretismo del programa soviético sirvió como herramienta tanto para prevenir el filtrado de información clasificada entre países y además creó una barrera de misterio entre el programa espacial y la población soviética. La naturaleza del programa contenía mensajes ambiguos sobre sus objetivos, logros y valores. El programa en sí era tan secreto que un ciudadano soviético normal nunca conseguiría una imagen concreta de él, Más que una versión superficial de su historia, de sus actividades presentes y de sus futuros intentos. Los lanzamientos no eran anunciados hasta que no se producían. Los nombres de los cosmonautas no eran difundidos hasta que habían volado. Los detalles de las misiones eran escasos. No conocemos el tamaño o forma de los cohetes las cabinas o la mayoría de tus naves espaciales, a excepto de los primeros Sputniks, las sondas lunares y las sondas de Venus.

Sin embargo, la influencia militar en el programa espacial soviético quizá es la explicación más validad para el secretismo del programa. Las prácticas militares que conocerían al desarrollo de armas, tales como los proyectos de misiles balísticos intercontinentales, se mantenían clandestinas. Los oficiales industriales militares soviéticos construyendo una forma de etiquetado de las armas que se basaba en un sistema alfanumérico aleatorio. Incluso los trabajadores en las fábricas que construían y enviaban partes para la construcción de naves espaciales tenían una pequeña concepción de la imagen completa.

El programa espacial soviético llevó a cabo un gran número de proyectos, incluyendo:


Dos días después de que los Estados Unidos anunciase su intención de lanzar un satélite artificial, el 31 de julio de 1956, la Unión Soviética anunció su intención de hacer lo mismo. El Sputnik 1 fue lanzado el 4 de octubre de 1957, venciendo a los Estados Unidos e impresionando a la gente de todo el mundo.

El programa espacial soviético fue pionero en muchos aspectos de la exploración espacial:




</doc>
<doc id="15513" url="https://es.wikipedia.org/wiki?curid=15513" title="Agoyo">
Agoyo

En ciertas regiones de Guinea, se rendía la máxima veneración al fetiche "Agoyo", especie de ídolo de buen agüero que se conservaba en la cabaña del brujo principal. Su forma era rarísima, casi inconcebible: una talla de unos cuarenta centímetros de altura, medio hombre y medio sapo, adornada con cintas rojas lo mismo que la vasija invertida que le servía de pedestal. En la cabeza llevaba un extraño tocado terminado en un dardo, constituido por un lagarto bajo una media luna, otro más pequeño y horizontal, un trozo de lanza, plumas, serpientes y más lagartos, todo esto se colocaba en una mesa con tres cuencos y dieciocho bolitas de barro.

Para consultar a este ídolo era preciso hacerle un sacrificio acompañado de un espléndido regalo al brujo mayor, que tenía la exclusiva de ese oráculo; si, al verter las bolitas varias veces en los cuencos, salía un número impar, la respuesta era afirmativa y, en caso contrario, negativa.



</doc>
<doc id="15515" url="https://es.wikipedia.org/wiki?curid=15515" title="Rock en español">
Rock en español

Rock en español es la música rock compuesta e interpretada en castellano. A diferencia del rock en inglés, el rock en español ha logrado tener éxito mundial en pocas ocasiones, y muchas veces ni siquiera entre países de habla hispana. Por eso el rock en castellano se ha desarrollado de manera heterogénea en las distintas naciones hispanohablantes; quedando así la música de muchas bandas para uso casi exclusivamente nacional hasta la llegada de la globalización de contenidos multimedia acentuada por la generalización de internet y desarrollada especialmente en el siglo XXI.

A pesar de esto, todas las escenas hispanohablantes siempre tuvieron al rock anglosajón como marco de referencia común, haciendo que el desarrollo de los diversos estilos de rock en cada país fueran relativamente contemporáneos. Aunque cabe añadir, que hubo épocas de represión del género en algunos países que atrasaron el desarrollo del rock en sus escenas locales.

El "rock and roll", también transcrito al español como rocanrol, es un estilo musical creado por la comunidad afroamericana de Estados Unidos a partir del final de la Segunda Guerra Mundial en 1945. En la década de 1950, músicos caucásicos como Elvis Presley y Bill Haley, impulsaron la masificación del género que se transformó en una cultura global.

La producción de rock en español se inició en la segunda mitad de la década de los cincuenta, con bandas musicales que en su gran mayoría, interpretaban en castellano los éxitos del rock and roll estadounidense. Aunque es objeto de controversia afirmar cuándo comienza exactamente a desarrollarse, es cierto que en ocasiones se suele señalar «El relojito» de Gloria Ríos en 1956 como punto de partida para el rock interpretado en castellano.

Sin embargo, el primer gran éxito internacional vino de la mano del saxofonista Danny Flores con la banda The Champs, alcanzando gran popularidad en Estados Unidos (y buena parte del mundo) al llegar al primer lugar de las listas del Billboard en marzo de 1958 con su éxito «Tequila».

Poco después también entró en las listas de éxitos de EE. UU., en ese mismo año de 1958, «La bamba», una canción regional mexicana interpretada con ritmo de rock y cantada en español por Ritchie Valens.

Los primeros años del rock en español, por lo general, se basaron sobre todo en la aparición de agrupaciones que tendían a versionar éxitos del rock anglosajón traduciéndolos al castellano. Ya durante la transición a la nueva década conjuntos como Los Llopis -que eran en realidad una banda centrada en la canción melódica y en géneros más estandarizados- o los Los Teen Tops -que obtuvieron éxito tanto en Hispanoamérica como en España- vertieron al español algunos temas del primer rock and roll estadounidense.

Durante los primeros años de los sesenta, el rock empezó a generalizarse y los medios de comunicación comenzaron a darle cada vez mayor cobertura entre las distintas regions hispanoparlantes. Vista su enorme capacidad de convocatoria entre el público juvenil, los locales de ocio y los promotores musicales comenzaron a programar sistemáticamente conciertos de grupos de rock; de forma que en poco tiempo el género se convirtió en un verdadero fenómeno de masas.

Sin embargo, conforme el rock cada vez obtenía más atención, poco a poco fueron apareciendo diversos grupos y solistas que interpretarían sus temas originales en castellano, dejando paulatinamente de versionar éxitos estadounidenses. También es destacable la influencia de otros países europeos no angloparlantes, con la llegada del fenómeno Yeyé de la que salieron artistas como Raphael.

Por otra parte cabe mencionar que en algunos países sudamericanos se acuñó el término nueva ola para englobar a aquellos artistas que adoptaron la influencia musical del rock de Estados Unidos y de los patrones de la cultura pop de Europa. Este estilo de pop mezclado con twist, beat y rock llegó a cosechar gran popularidad en Latinoamérica. Sin embargo el estallido que vendría con la llegada de la ola inglesa que se dio a mediados de la década sería determinante para el desarrollo del rock en español.

Es justo entonces cuando los británicos The Beatles se convirtieron en un éxito mundial; a ellos les siguieron un gran número de grupos de su misma nacionalidad. Este fenómeno, la denominada invasión británica, afectó al mundo hispanohablante y a todo el planeta. Los nuevos sonidos venidos del Reino Unido (y también de los EE.UU.) como la música beat, el rythm and blues, la psicodelia, el soul, el folk-rock o el pop se impusieron por todo el mundo, ejerciendo una notable influencia en los países de habla hispana e impulsando el desarrollo del rock en sus respectivas escenas.
La influencia de la música beat, el pop o la psicodelia se hizo presente enseguida con Los Brincos, El Kinto, Los Gatos, The Speakers, y algunos artistas de éxito que se expresaban principalmente en inglés también interpretaron en castellano ocasionalmente como Los Bravos o más anecdóticamente como Los Shakers. Aunque en los pocos casos que las bandas tuvieron éxito mundial fueron mayoritariamente con temas interpretados en inglés (como sucedió con Los Bravos o Miguel Ríos). También notable fue el caso de Los Saicos, una de las bandas pioneras del proto-punk en el mundo, siendo reivindicada posteriormente como ejemplo de garage rock primigenio en español. Y es que no todo se basó en adaptar los sonidos anglosajones al entorno hispano, sino que el rock fue más allá dejándose influenciar por la música autóctona de cada país. Así es como la banda Santana obtuvo un gran éxito mundial con su mezcla de rock y sonidos latinoamericanos, un estilo que dio en llamarse rock latino. Aunque es digno de mencionar que su repertorio se expresaba mayoritariamente en inglés, quedó algún éxito en castellano como «Oye cómo va». Esta tendencia del rock latino seguiría desarrollándose durante el final de la década y la siguiente con bandas como Malo así como la de mezclar sonidos folclóricos de otras regiones con el rock. Pero este hecho no quiere decir que la escena hispana dejara de ser influenciada por la anglosajona ya que durante el cambio de década seguirían siendo determinantes estilos como el rock ácido, el blues rock o el hard rock; así como el rock progresivo donde ya se dejaba entrever en bandas como Almendra cuyo cantante Luis Alberto Spinetta sería una pieza clave en el rock interpretado en castellano durante la década de los setenta.

Bajo la amalgama de influencias que se había producido durante la década anterior, gran parte de las agrupaciones de origen hispano todavía seguían interpretando mayoritariamente en inglés con anecdóticas incursiones en el rock en castellano como ocurrió con La Revolución de Emiliano Zapata —que acabarían por abandonar su estatus de banda de rock psicodélico tras los primeros setenta—.

En cambio sí hubo varios exponentes claros del rock psicodélico en español como quedó patente con Los Dug Dug's, Pescado Rabioso o la influencia «blusera» y progresiva de Vox Dei junto a otras estrechamente relacionadas como Manal —aunque más asiduamente emparentada con el blues rock—.

Conforme se fue sucediendo la década, emergían con más fuerza las apuestas de rock progresivo con menores influencias psicodélicas como hizo Invisible, Sui Generis —con una línea más marcada hacia el folk— o Témpano mientras que otras optaron por adherir nuevas influencias a su propuesta estilística como Los Jaivas —con su mezcla de rock y música de raíz folclórica andina— o Triana —con una alta influencia del flamenco—.

Y es que Triana fue una banda pionera en lo que dio en llamarse rock andaluz, una derivación del rock progresivo y del rock sinfónico con una fuerte presencia del flamenco, estilo que quedaba fusionado a la música rock de manera totalmente homogénea. Aunque ese tipo de fusiones, en ocasiones, fueron todavía más allá, dejando de lado el sinfonismo y mezclando el flamenco a otros tipos de rock que anclaban sus raíces en el rock and roll tradicional, el blues o el folk, como hizo la banda Veneno.

Análogamente a lo que había ocurrido al final de la anterior década en el Reino Unido, buena parte de las bandas emparentadas con la psicodelia y el blues —incluyendo algunas ya mencionadas— se fueron abriendo paso a un sonido más endurecido incurriendo en el hard rock. Diversas agrupaciones fueron abrazando el estilo alejándose paulatinamente de las influencias psicodélicas como Pappo's Blues o los ya mentados Vox Dei. Muchas de sus contemporáneas siguieron ese camino creando una escena musical de sonidos más pesados y, sobre todo, alumbrando nuevas corrientes musicales. Entre ellas destacó un movimiento que se conoció como rock urbano (de España), claramente adscribible al hard rock, pero también influido por estilos como el rock progresivo y el blues rock; y cuyo máximo estandarte fue el grupo Leño.
Pero no todo fueron buenas noticias para el rock en español ya que en dos de las escenas nacionales más potentes y representativas del género ocurrieron una serie de acontecimientos extramusicales (de carácter socio-político) que tuvieron consecuencias bastante negativas en México y Argentina.

En México, a principios de los setenta, el rock sufrió algo parecido a un persecución cuando el gobierno federal (en teoría, como consecuencia de los sucesos originados en un festival celebrado en la localidad de Avándaro), prohibió la celebración de conciertos, restringió su difusión mediática (radio y televisión) y presionó a las compañías discográficas para reducir la publicación de discos del género. 
En cuanto a Argentina, el golpe de estado de marzo de 1976 abrió también un período de represión y censura en el que, sin llegar ni mucho menos a la prohibición, el rock fue visto como algo sospechoso y subversivo, capaz de agitar y movilizar a la juventud contra el sistema establecido. Como consecuencia, varios músicos y bandas argentinas dejarían el país e irían a Europa —especialmente España, país que acababa de salir de la dictadura franquista— o a Estados Unidos, 

Esta migración de músicos argentinos hacia España dio lugar a una fugaz conexión entre dos de las escenas más potentes del género hasta el momento en el mundo de habla hispana. De hecho, los artistas argentinos exiliados se unieron a la escena española con rapidez y se adaptaron —en unos casos con mayor y en otros con menor éxito— a su país de acogida. Hubo casos en los que se formaron grupos con componentes de ambas nacionalidades como fue el caso de Tequila mientras otros desarrollaban su carrera como solistas, tal y como hizo Moris.
Sea como sea, esa conexión de alguna manera pudo ayudar a consolidar una nueva escena musical alejada del rock progresivo y de los sonidos duros y sinfónicos que entonces eran predominantes a ambos lados del Atlántico. Una nueva escena heredera del rock and roll clásico, del glam rock, del rythm and blues y del pop y el rock de los años sesenta en la que ya destacaban bandas como Burning y grupos difícilmente encasillables como La Orquesta Mondragón (que combinaba el rock con cierta teatralidad paródica) que, de alguna forma, era equivalente al pub rock británico que, por aquellas fechas, surgía en el Reino Unido; y que, en pocos años, terminaría dando lugar al surgimiento de nuevas corrientes como el punk y la new wave).

En cualquier caso, estas represiones de carácter político no impidieron que todavía sonara el rock en la escena "underground", y buena prueba de ello es que todavía surgirían bandas reconocidas durante este período como Serú Girán, proyecto de Charly García quien ya estuvo en la banda Sui Generis.

Cuando ya el rock progresivo estaba en declive en el resto del mundo, en la escena hispana aún daba sus últimos aciertos con Chac Mool o Frágil, con sus últimas grandes aportaciones a la escena progresiva durante los primeros años de los ochenta.

Precisamente a finales de la década anterior el mundo se vio influenciado por el advenimiento de la escena punk que si bien ya tuvo pioneros durante el mismo período en la escena hispanohablante, sería a lo largo de ésta cuando resultaría ser más prolífica; justo cuando en el mundo angloparlante se encontraba en declive. Precisamente el movimiento estaba en pleno auge en el mundo hispanoparlante con bandas como La Polla Records —grupo que se englobó dentro del rock radical vasco—, Siniestro Total —con cierto tono humorístico— o Los Violadores.

Sin embargo en el punk anglosajón había derivado en nuevas tendencias como la new wave y el post-punk, algo que tampoco pasó desapercibido para la escena en español. De la movida madrileña, un movimiento contracultural que captó la atención internacional, surgieron numerosas bandas en esa línea como Alaska y Dinarama, Radio Futura, La Unión o Gabinete Caligari; aunque lejos de limitarse a ella, englobaba a otras de diferentes propuestas estilísticas como el rock and roll tradicionalista de Loquillo y los Trogloditas, el pop rock de Nacha Pop o el synth pop de Aviador Dro y Mecano.

El alcance de la new wave y el post-punk llegaría a influenciar también a bandas fuera de este movimiento (aunque muchas de ellas acabarían por cambiar su registro o estaban sometidas también a otras influencias) como Sumo, Sentimiento Muerto, Los Prisioneros, Los Abuelos de la Nada, Virus, Caifanes, Patricio Rey y sus Redonditos de Ricota —comúnmente abreviado a "Los Redondos"— o Soda Stereo. 

Sería precisamente Soda Stereo uno de los máximos abanderados en la internacionalización del rock en español. Y es que pese a que la escena había tenido ciertos contactos, nunca había tenido una difusión masiva. Aunque no fuera ni mucho menos un equivalente a la invasión británica ya que no todos los estilos de rock recibieron la misma difusión, sí que sirvió para que se iniciara un ciclo de internacionalización del rock en castellano.

Bajo este escenario, durante la segunda mitad de la década, el español Miguel Ríos organizó los "encuentros de rock latinoamericano" y enseguida comenzó una campaña de difusión denominada «Rock en tu idioma» donde se promocionaron varios de los grupos mencionados junto a otros de diversa índole como Duncan Dhu, Maldita Vecindad y los Hijos del Quinto Patio, Los Toreros Muertos, Hombres G, Enanitos Verdes, Miguel Mateos, entre otros. Desde artistas de pop rock como El Último de la Fila o el «guacarrock» de Botellita de Jerez hasta cantautores como Fito Páez, Joaquin Sabina, Juan Carlos Baglietto y Rodrigo González mostraban que el rock estaba viviendo una época de bonanza. Rodrigo González sería una de las semillas de la corriente que se denominó rock urbano mexicano, que tenía algunas características afines al rock urbano de España aunque anclaba sus raíces en un estilo más influenciado por el rock and roll, el blues, y el rhythm and blues dejando a un lado las del punk y el rock progresivo, siendo El Tri uno de los máximos exponentes de la corriente.

Por otro lado, la escena de heavy metal —totalmente ajena a la escena del «Rock en tu idioma»— estando influida por el hard rock de la década anterior y del movimiento que se conoció como la nueva ola del heavy metal británico (abreviado en inglés "NWOBHM"), iba tomando forma rápidamente con bandas como Riff —formada por el líder de Pappo's Blues—, Kraken o Barón Rojo —que además obtuvo cierto éxito en Europa al versionar algunos de sus temas al inglés—. Aunque algunas bandas de metal encontrarían su cúspide de popularidad durante los años venideros.

Precisamente Ángeles del Infierno que había desarrollado su carrera en la década anterior comenzaría a ofrecer actuaciones en Hispanoamérica a partir de los noventa. Y es que por aquel entonces en la escena anglosajona, el heavy metal estaba sufriendo una recesión en favor de nuevos estilos como el grunge. Sin embargo, tardarían algo más los estilos de rock alternativos en ser dominantes en la escena hispanohablante, pues justo cuando transcurría la eclosión del grunge en EE.UU, el heavy metal en castellano alcanzó nuevas cotas de popularidad con el éxito de Rata Blanca. Por supuesto otros estilos menos clásicos surgieron como el death metal de Brujería, el thrash metal de Ekhymosis o Transmetal —que su estilo se ubicaba entre ambos géneros—.

También fue notorio el resurgimiento del punk en el mundo anglosajón, al que se unieron rápidamente bandas (aunque la productiva era del punk en castellano quedaba muy cerca) como Attaque 77, Todos tus muertos —apostando más por el hardcore punk— o Ska-P —centrada en el ska punk—. Es notable decir que el ska fue un estilo que tuvo bastante presencia también con otras bandas como Desorden Público, King Changó —que catalogaría su estilo de "latin ska"—, Tijuana No! o Los Rabanes.

Por otro lado, muchas de las bandas del «Rock en tu idioma» estaban en su etapa de esplendor; y a ellas no tardaron en unirse otras de una onda similar, generalmente de base pop rock (muchas nacidas a finales de la década anterior) como Jarabedepalo, Los Rodríguez (y, tras su disolución, Andrés Calamaro como solista), Celtas Cortos —con su rock de raíces celtas—, Julieta Venegas, Juanes —que cambió su registro radicalmente desde su salida de Ekhymosis—, Ely Guerra, Los Pericos —entonando su estilo hacia el reggae—, Shakira, Maná o Héroes del Silencio—aunque estos últimos en una línea más dura y "rockera" que los otros—.

A pesar de las incursiones de Héroes del Silencio en el hard rock, no es habitual catalogar a la banda como fiel representante de dicho género. No obstante, éste seguía teniendo presencia con otros grupos que pueden reconocerse más claramente en la etiqueta; hablamos de La Renga —como parte del rock barrial—, Cuca —con una propuesta humorística— o el máximo artífice del renacimiento del viejo rock urbano español Extremoduro.

Aun así, sería demasiado aventurado afirmar que el rock en castellano gozara de fama internacional o mundial. El territorio hispanohablante era visto principalmente como fuente de éxitos de verano y canciones de baile. Resulta irónico que precisamente el productor de Ricky Martin, Draco Rosa, iniciara su carrera dejándose influenciar del rock alternativo. Y es que el rock alternativo se hizo patente en la escena hispanohablante surgiendo así diversas bandas que se dejaron influenciar por su sonido como Caramelos de Cianuro, Libido, La Gusana Ciega, Santa Sabina, La Barranca, Los Tres, Lucybell, Divididos —con gran afinidad al hard rock e incluso el funk—, Zurdok —banda origen del movimiento pluriestilístico «Avanzada regia»—, El Otro Yo o Babasónicos. También caben señalar subescenas derivadas como la del indie rock, de la cual podemos destacar a Los Planetas.

Sin embargo, tal como lo hizo el clásico rock latino, la nueva tendencia del rock alternativo se dejó influenciar por la música autóctona latinoamericana y otros ritmos como el reggae o el ska, formando así el alterlatino. Lejos de ser algo anecdótico, la escena fue muy prolífica con numerosas bandas de éxito como el artista Manu Chao y su banda Mano Negra —que intercalaba diversos idiomas a sus lanzamientos—, Los Fabulosos Cadillacs, Fobia, Café Tacvba, Aterciopelados, Jaguares como también lo fue la banda Caifanes de la cual se radicó, Bersuit Vergarabat, Amparanoia, Los Piojos, Los Auténticos Decadentes, La Ley o Molotov —con una propuesta estilística basada en el rap rock— son algunos de los más representativos en dicha vertiente musical.
Tal y como había ocurrido en el mundo anglosajón, la escena quedó plagada de propuestas de carácter alternativo en comparación a lo que se había venido componiendo antiguamente como también lo hizo la escena "metálica". El metal alternativo se hizo patente con el nu metal (entre otros estilos) de Resorte, Puya —mezclando su sonido con la salsa— o A.N.I.M.A.L. —también de apuesta groove metal—.

A pesar de esta internacionalización, muchas bandas de gran popularidad nacional aún continuaban sin expandir sus fronteras debido a la falta de promoción en otros países. La globalización de contenidos acentuada por la generalización de internet empezó a ayudar a unificar —relativamente— más las distintas escenas regionales. Esto era, en general, un agravante para escenas concretas como, por ejemplo, las propuestas de "rock" duro y pesado. Sin duda, de toda la escena del "heavy metal" de habla hispana de los primeros años del siglo XXI, la que más trascendencia llegó a lograr fue la formación de folk metal Mägo de Oz.

Estilísticamente, el siglo comenzó como una extensión más de lo que había ocurrido durante la década de los noventa. Los sonidos del rock alternativo o del alterlatino seguían dejándose ver con Ozomatli, No Te Va Gustar, Panteón Rococó —inclinándose hacia el ska—, Kevin Johansen —en una línea más de cantautor—, Catupecu Machu, Los Bunkers, La Vida Bohème -más inclinado hacia el indie rock-, El Guincho —con su tropicalismo neopsicodélico—, La Vela Puerca, Zoé, Macaco y Ojos de Brujo —adheriendo influencias del flamenco y encasillándose así en lo que se conoce como «mestizaje»—, Mon Laferte —incurriendo en el folk y en la canción de autor—, Jumbo o Kinky —incorporando a su estilo elementos de la música electrónica—.

Precisamente la música electrónica sería la base de otras que anclarían sus raíces en el rock alternativo como el trip hop de Plastilina Mosh, el electrofolk de Juana Molina, el electropop de Belanova y Miranda! o el synthpop oscuro de los primeros álbumes de Saiko.

Y es que si tuviéramos que destacar alguna tendencia nueva, sin duda, sería la de incorporar la electrónica; incluso el antiguo cantante de Soda Stereo, Gustavo Cerati, se dejó influenciar por el sonido.

Sin embargo, los estilos más «clásicos» no cayeron en el olvido, y es que si Enrique Bunbury en un principio inició su carrera en solitario dejándose influenciar por las nuevas tendencias alternativas e incluso electrónicas, pronto se desmarcaría de ellas experimentando con diversos estilos en una línea de cantautor y de pop rock.
Alejados de esas vertientes, en la escena pop rock podemos nombrar a Estopa —también vinculables a la rumba flamenca—, Bacilos —una apuesta de pop latino—, Natalia Lafourcade —con estilo más propio del folk y el pop barroco—, Fito & Fitipaldis —con sonido más declinado hacia el rock and roll—, El Cuarteto de Nos —con ciertos dotes humorísticos—, u otros más cercanos a la canción de autor como Nacho Vegas, Jorge Drexler o Fiel a la Vega. También notable es el funk latino de Los Amigos Invisibles, estilo que se desmarca de todos los mencionados.




</doc>
<doc id="15516" url="https://es.wikipedia.org/wiki?curid=15516" title="Supertramp">
Supertramp

Supertramp es un grupo británico de "rock" progresivo fundado en 1969 por el músico Rick Davies. Con el apoyo financiero de Stanley August Miesegaes, la primera formación de Supertramp, integrada por Roger Hodgson, Richard Palmer y Robert Millar, publicó un álbum homónimo con las características del "rock" progresivo de escaso éxito comercial, seguido de un segundo trabajo, "Indelibly Stamped", en el que Palmer y Millar fueron sustituidos por Frank Farrell, Kevin Currie y Dave Winthrop.

A pesar del escaso éxito inicial y de que Miesegaes cortó la financiación del grupo, Davies y Hodgson crearon una nueva formación, integrada por Dougie Thomson, John Helliwell y Bob Siebenberg, cuyos trabajos incluyeron un sonido más orientado al pop manteniendo el estilo progresivo con un uso predominante del piano Wurlitzer, del saxofón y de otros sonidos orquestales. El primer álbum de esta formación, "Crime of the Century", favoreció el auge comercial de Supertramp, consolidado con sencillos como «Dreamer» y «Bloody Well Right». "Crime of the Century" fue seguido de trabajos como "Crisis? What Crisis?" y "Even in the Quietest Moments", que labraron la reputación de Supertramp como una banda de directo.

El álbum "Breakfast in America" consolidó a Supertramp como una banda de éxito al alcanzar el primer puesto en la lista de los discos más vendidos de países como Alemania, Australia, Canadá, Estados Unidos, España y Francia, entre otros. Tres de sus sencillos —«Goodbye Stranger», «Take the Long Way Home» y «The Logical Song»— fueron "top 20" en los Estados Unidos, donde el álbum vendió más de cuatro millones de copias. El éxito de "Breakfast in America" continuó con "Famous Last Words", tras el cual Hodgson anunció su abandono para emprender una carrera en solitario.

Con Davies como líder "de facto" de Supertramp, el grupo publicó "Brother Where You Bound" (1985), alejándose del "rock" progresivo, y "Free as a Bird" (1987), que incorporó elementos del "dance" y contó con la incorporación de Mark Hart. Tras una gira posterior, Supertramp estuvo inactivo durante casi una década.

En 1996, Davies volvió a reunir al grupo, ampliado con la presencia de Carl Verheyen, Cliff Hugo, Lee Thornburg y Jesse Siebenberg, para publicar "Some Things Never Change", seguido de una gira documentada en el álbum "It Was the Best of Times" (1999) y de "Slow Motion" (2002), su último álbum hasta la fecha. Tras ocho años de inactividad, Davies reformó nuevamente el grupo para realizar la gira 70-10 Tour, con motivo del 40º aniversario de la formación de Supertramp, seguida en 2015 de la gira Supertramp Forever Tour.

En 1969, Stanley 'Sam' August Miesegaes, un millonario holandés, dejó de apoyar económicamente a una banda llamada The Joint debido a su decepción con ellos. Sin embargo, ofreció a Rick Davies, miembro del grupo y de quien sentía que su talento se había visto «empantanado» en The Joint, una oportunidad para formar su propia banda, de nuevo con su respaldo financiero. Después de colocar un anuncio en el semanario musical "Melody Maker", Davies formó una banda con Roger Hodgson (bajo y voz), Richard Palmer-James (guitarras) y Keith Baker (percusión).

Davies y Hodgson tenían orígenes e inspiraciones musicales radicalmente diferentes: Davies tenía un origen humilde y sus principales influencias musicales eran el "blues" y el "jazz", mientras que Hodgson había comenzado a trabajar en la industria musical y era aficionado al pop y a la música psicodélica. A pesar de ello, comenzaron a escribir canciones juntos, con Palmer como tercer miembro del equipo compositivo. Dado que ningún otro miembro de la banda se mostró dispuesto, Palmer escribió todas las letras.
El grupo se denominó inicialmente como Daddy. Baker fue al poco tiempo reemplazado por Robert Millar, y después de varios meses ensayando en una casa de campo de West Hythe, la banda viajó a Múnich para ofrecer una serie de conciertos en el P.N. Club. Una actuación de diez minutos de la canción "All Along The Watchtower" fue filmada por Haro Senft durante un concierto. Los ensayos fueron poco productivos, y su repertorio inicial consistía en solo cuatro canciones, dos de ellas versiones de otros artistas. Para evitar confusiones con la banda Daddy Longlegs, el grupo cambió su nombre por el de Supertramp, un apodo inspirado en el libro de W. H. Davies "Autobiografía de un súper vagabundo" ("The Autobiography of a Super-Tramp").

Supertramp fue uno de los primeros grupos en firmar con la rama británica de A&M Records, y su primer álbum, "Supertramp", fue publicado en julio de 1970 en el Reino Unido y Canadá (en los Estados Unidos no fue publicado hasta 1977). Estilísticamente, el álbum incluyó "rock" progresivo característico de la época y un sonido similar al del grupo . A pesar de obtener buenas reseñas, el álbum no atrajo a una gran audiencia.

Dave Winthrop (flauta y saxofón) se unió al grupo tras el lanzamiento del primer disco, y poco después, Supertramp tocó en el Festival de la Isla de Wight. La formación continuó cambiando en los seis meses siguientes a la publicación del álbum: Palmer abandonó el grupo debido a conflictos personales con Davies y Hodgson, seguido de Millar, quien sufrió una crisis nerviosa después de una gira por Noruega.

En su siguiente álbum, "Indelibly Stamped", publicado en junio de 1971, Frank Farrell (bajo) y Kevin Currie (percusión) reemplazaron a Palmer y Millar, mientras que Hodgson pasó a tocar la guitarra y Davies comenzó a ser el segundo vocalista. Con la salida de Palmer, Hodgson y Davies comenzaron a escribir las canciones del resto de trabajos del grupo. "Indelibly Stamped" obtuvo ventas inferiores a su predecesor, lo cual provocó que todos los miembros, a excepción de Davies y Hodgson, abandonaran el grupo, y que Miesegaes les retirase su apoyo financiero en octubre de 1972.

Una búsqueda de nuevos miembros llevó a bordo a Dougie Thomson, que había realizado conciertos con el grupo casi un año antes de reanudar las audiciones. En 1973, el grupo se amplió con Bob Siebenberg (batería) y John Helliwell (saxofón e instrumentos de viento y teclados), que completaron la nueva formación, vigente durante los siguientes diez años. Hodgson también comenzó a tocar el piano de cola y el teclado wurlitzer además de que en directo dispusieron de otros teclados con los cuales crearon las ambientaciones orquestales de su música.

Mientras tanto, el vínculo entre Davies y Hodgson comenzó a debilitarse paulatinamente. En julio de 1972, Hodgson consumió LSD por primera vez y le ofreció a Davies, quien se negó a consumirlo. En una carta a Miesegaes, Hodgson describió la experiencia como «el día más feliz de mi vida» y expresó su ansiedad por el hecho de que Davies no la tomara. Años después, Hodgson describió esta divergencia en sus experiencias como la raíz de la ruptura entre ellos. Durante la historia de Supertramp, la relación entre Davies y Hodgson fue amistosa pero con estilos de vida e inclinaciones musicales cada vez más distantes y menos solapadas. En este sentido, su asociación compositiva fue diluyéndose poco a poco y, aunque todas las canciones fueron acreditadas oficialmente a ambos, la mayoría fueron composiciones escritas individualmente por Davies o por Hodgson.

"Crime of the Century", publicado en septiembre de 1974, fue el primer gran éxito comercial y de crítica del grupo tras llegar al puesto cuatro en la lista "UK Albums Chart" y a la primera posición en la lista de discos más vendidos de Canadá. "Crime of the Century" subrayó la ambición del grupo, con muchas canciones fuertemente orquestadas y temas donde Davies y Hodgson compartieron la voz principal, como «School» y «Dreamer». La segunda, publicada como sencillo, llegó al "top 20" en el Reino Unido, mientras que su cara B, «Bloody Well Right», alcanzó el "top 40" en los Estados Unidos.

Con un álbum exitoso en su haber, las presiones sobre Supertramp aumentaron, y su sucesor, "Crisis? What Crisis?", fue grabado en los pocos meses de descanso entre dos giras. Como consecuencia, la mayoría del material consistió en descartes de "Crime of the Century", y décadas más tarde fue considerado por el propio grupo como uno de sus peores momentos. A pesar de las dudas del grupo, "Crisis? What Crisis?" fue bien recibido por la crítica y llegó al puesto veinte en la lista británica "UK Albums Chart" y al 44 en la estadounidense "Billboard 200".

El siguiente álbum, "Even in the Quietest Moments", fue publicado en abril de 1977 junto al sencillo «Give a Little Bit», número quince en los Estados Unidos y veintinueve en el Reino Unido. Al igual que con anterioridad, el álbum obtuvo un mayor éxito que los sencillos, y "Even in the Quietest Moments" llegó al puesto dieciséis en la lista "Billboard 200" y al doce en la lista de discos más vendidos del Reino Unido. Durante este periodo, el grupo trasladó su residencia a Los Ángeles, California.

El enfoque de un sonido más orientado al pop permitió que Supertramp alcanzara su mayor éxito con "Breakfast in America", publicado en marzo de 1979, que alcanzó el puesto tres en el Reino Unido y llegó a lo más alto de las listas de países como Canadá y los Estados Unidos. "Breakfast in America" produjo también cuatro sencillos: «The Logical Song», «Goodbye Stranger», «Take the Long Way Home» y «Breakfast in America», todos ellos "top 20" en los Estados Unidos. En marzo, el grupo se embarcó en una gira de 120 conciertos que rompió todos los récords de asistencia con respecto a conciertos anteriores tanto en Norteamérica como en Europa. Al finalizar la gira, los miembros del grupo decidieron tomar un descanso por un tiempo.
Para evitar un largo espacio entre álbumes durante su descanso, el grupo publicó "Paris", un doble disco en directo grabado en su mayoría en el Pavillon de París, Francia que alcanzó el puesto ocho en la lista estadounidense "Billboard 200". La versión en directo del sencillo «Dreamer» reportó al grupo su segundo número uno en Canadá después de «The Logical Song» y entró en el "top 20" en la lista estadounidense "Billboard Hot 100".

Durante este periodo, Hodgson trasladó a su familia a la zona montañosa del norte de California, donde construyó una casa y un estudio de grabación. Además, comenzó a pasar más tiempo con su familia y a mostrar interés por la espiritualidad, grabando de forma paralela "Sleeping with the Enemy", un primer álbum en solitario nunca publicado. El distanciamiento geográfico amplió la separación con respecto al resto del grupo, y durante la grabación de su siguiente trabajo, "...Famous Last Words...", Davies y Hodgson encontraron dificultades para conciliar sus respectivas ideas musicales. "...Famous Last Words..." fue publicado en octubre de 1982 y llegó a los puestos cinco y seis en las listas de discos más vendidos de los Estados Unidos y el Reino Unido respectivamente. Tras una gira mundial en 1983, Hodgson anunció públicamente que no iba a continuar con el grupo. Según el propio músico, su salida de Supertramp estuvo motivada por el deseo de pasar más tiempo con su familia y emprender una carrera en solitario, y que nunca tuvo problemas personales o profesionales reales entre Davies y él.

Tras la marcha de Hodgson, Davies pasó a liderar "de facto" Supertramp como único compositor y publicó "Brother Where You Bound" en 1985. El álbum fue un distanciamiento deliberado con respecto al pop de sus dos últimos trabajos y llegó al puesto veintiuno en la lista estadounidense "Billboard 200" y al veinte en el Reino Unido. "Brother Where You Bound" incluyó el sencillo «Cannonball», "top 30" en los Estados Unidos, junto con la canción principal, una larga exposición con temática de la Guerra Fría con un solo de guitarra del guitarrista de Pink Floyd David Gilmour.

Dos años después, el grupo publicó "Free as a Bird", un álbum marcado por la experimentación con sintetizadores y por incorporar elementos del pop y del "dance". Davies definió el álbum como «un experimento para tratar de ser moderno y construirlo con ordenadores y máquinas de ritmo, y que la gente viniese de uno en uno, lo cual te hacía perder el espíritu de grupo un poco». Aunque el sencillo «I'm Beggin' You» llegó al primer puesto en la lista "Hot Dance Music/Club Play" de "Billboard", "Free as a Bird" obtuvo un escaso éxito comercial. 

Tras la marcha de Hodgson, y con motivo de la gira de "Brother Where You Bound", el grupo decidió no interpretar canciones del músico, siguiendo un supuesto acuerdo verbal entre Davies y él. Sin embargo, el público se mostró poco entusiasmado por la omisión de estas canciones, por lo que la gira de "Free as a Bird" volvió a incluir composiciones de Hodgson interpretadas por Mark Hart que provocaron la marcha de Dougie Thomson. Tras la gira de 1988, el grupo se separó temporalmente. Al respecto, Davies comentó: «Hemos estado ahí cerca de veinte años entre grabaciones y giras y parecía el momento de tomar un descanso sin idea de cómo o cuándo volveremos. En realidad decidimos no decir nada, como si fuéramos viejos soldados desvaneciéndonos».

En 1993, Davies y Hodgson volvieron a tocar juntos por primera vez en diez años en un homenaje a Jerry Moss, cofundador de A&M Records, en el Beverly Hills Hilton, donde tocaron «The Logical Song» y «Goodbye Stranger». Tras el evento, ambos colaboraron durante seis meses ensayando canciones como «You Win I Lose» y «And the Light», antiguas composiciones de Davies. Sin embargo, el encuentro no fructificó y ambos siguieron caminos por separado.

Poco después, Davies retomó el proyecto de grabar un nuevo álbum y reformó Supertramp con John Helliwell, Bob Siebenberg y Mark Hart, presente desde la grabación de "Free as a Bird". Ampliando el grupo con otros cuatro músicos de sesión, Davies publicó "Some Things Never Change", un álbum con un retorno al sonido habitual de Supertramp, en marzo de 1997. "Some Things Never Change" llegó al "top" 10 en países europeos como Suiza, Francia y Alemania pero no entró en la lista estadounidense "Billboard 200".

"Some Things Never Change" fue seguido de la gira It's About Time Tour documentada en el álbum en directo "It Was the Best of Times", grabado en el Royal Albert Hall de Londres y publicado en abril de 1999. Tres años después, el grupo publicó "Slow Motion", su último álbum de estudio hasta la fecha. "Slow Motion" fue grabado en el nuevo hogar de Davies en Hampton Bays, después de residir durante más de dos décadas en Los Ángeles, usando por primera vez Pro Tools, y fue seguido de una nueva gira mundial, tras la cual el grupo volvió a permanecer inactivo.

Tras el lanzamiento de "Slow Motion", todos los miembros de Supertramp a excepción de Rick Davies realizaron proyectos paralelos al grupo. John Helliwell formó el grupo Créme Anglaise junto a Mark Hart y publicó un álbum homónimo en 2005, mientras que Bob Siebenberg formó parte del grupo Todd Hannigan And The Heavy 29’s junto a su hijo Jesse y trabajó en "The Glendale River", un álbum de estudio.
En 2005, con motivo de la publicación de "Retrospectacle - The Supertramp Anthology", Davies y Hodgson mantuvieron varias reuniones para intentar reformar Supertramp que no llegaron a fructificar. Tres años después, ambos músicos volvieron a encontrarse con vistas a una posible reunión del grupo que tampoco llegó a concretarse. 

A pesar de la negativa de Hodgson, Davies reformó Supertramp para la gira 70-10 Tour con motivo del cuadragésimo aniversario del grupo. La inclusión de canciones de Hodgson en la gira, de forma similar a las dos anteriores, provocó el enfado de Roger, quien dijo que rompía un acuerdo verbal entre él y Rick de no interpretarlas a cambio de mantener Davies el nombre de Supertramp. En respuesta, Davies explicó: «La única realidad es que existen 600 páginas de documentos contractuales que determinan lo que podemos hacer y lo que no. Por lo que a mí respecta, yo cumplo con mi parte de ese acuerdo e interpreto canciones de Supertramp. Eso tiene que ver con todo lo que publicamos juntos e interpretamos juntos sobre un escenario. Para mí, eso también es música de Supertramp».

La gira 70-10 Tour, que contó con la ausencia de Mark Hart y su sustitución por Gabe Dixon, comenzó en septiembre en Alemania y se extendió con una treinta de conciertos por Europa. Un año después, el grupo ofreció una etapa por Canadá antes de regresar a Francia, donde ofreció su concierto número 1 000.

En agosto de 2012, a pesar de la reprobación de Hodgson y de Davies, el grupo publicó el DVD "Live in Paris '79", con material audiovisual inédito filmado durante los conciertos de París en la gira de "Breakfast in America". El lanzamiento, promovido por el resto de miembros de Supertramp, obtuvo un notable éxito comercial al alcanzar el primer puesto en las listas de DVD más vendidos de países como Alemania, Bélgica, Noruega, Austria, Países Bajos y Suiza. 

En 2015, el grupo volvió a reformarse con motivo de la gira Supertramp Forever Tour, que finalmente fue suspendida por problemas de salud de Rick Davies.





</doc>
<doc id="15517" url="https://es.wikipedia.org/wiki?curid=15517" title="Roger Hodgson">
Roger Hodgson

Charles Roger Pomfret Hodgson (Portsmouth, Hampshire, Inglaterra, Reino Unido 21 de marzo de 1950), más conocido como Roger Hodgson, es un músico y compositor británico, fundador junto a Rick Davies de la banda de rock progresivo Supertramp y compositor de gran parte del catálogo musical del grupo hasta su marcha en 1983. Es también reconocido por su voz aguda, marca distintiva de la música de Supertramp, así como por la temática de sus canciones, que habitualmente relatan temas espirituales y filosóficos. 

Tras abandonar Supertramp en 1983, Hodgson inició una carrera en solitario con la publicación del álbum "In the Eye of the Storm", grabando hasta la fecha tres álbumes de estudio. A pesar de varios intentos por retomar la colaboración con Rick Davies, su antiguo compañero en Supertramp, Hodgson centró desde 1997 su carrera artística en ofrecer conciertos periódicos durante extensas giras anuales, en las que combina actuaciones en solitario con conciertos respaldados por una banda de apoyo.

Hijo de Charles Hodgson y Jill Hodgson, Roger Hodgson nació el 21 de marzo de 1950 en Portsmouth, Inglaterra y se crio en Oxford, en una familia de clase media. Fue a la escuela Woodcote House, cerca de Wallingford, donde aprendió a tocar la guitarra eléctrica, y posteriormente acudió a la escuela Stowe School, cerca de Buckingham. A los 12 años su padre le regaló su primera guitarra y aprendió tres acordes básicos de su profesor en la escuela. Poco tiempo después comenzó a componer su propia música, y con 13 años ofreció su primer concierto en el colegio interpretando nueve canciones propias.

Hodgson formó su primera banda en el colegio, llamada H-Bombs y formada por él a la guitarra y Roy Hoby tocando una caja. Con 19 años, entró por primera vez en un estudio de grabación como guitarrista para el grupo People Like Us, que formó poco después de graduarse del internado. El grupo grabó dos canciones, «Duck Bound» y «Send Me No Flowers», que nunca fueron publicadas.

Tras la separación de People Like Us, Hodgson participó en una audición para el sello discográfico Island Records, gracias a la ayuda del representante del grupo Traffic. Island le usó como vocalista del grupo Argosy, formado por Reginald Dwight (más tarde conocido como Elton John), Caleb Quaye y Nigel Olsson. La única grabación del grupo fueron dos canciones, «Mr. Boyd» e «Imagine», compuestas por Hodgson y publicadas como sencillo en 1969 por dos sellos independientes, DJM en el Reino Unido y Congress en Estados Unidos. «Mr. Boyd» fue versionada en 1997 por Jake Shillingford y su grupo My Life Story en su álbum "The Golden Mile".

Tras la ruptura de Argosy, Hodgson respondió a un anuncio puesto en "Melody Maker" por Rick Davies, que buscaba un guitarrista para un nuevo grupo de rock progresivo bajo el nombre de Supertramp. Hodgson obtuvo en un primer momento el puesto, pero la llegada al día siguiente de Richard Palmer y su contratación como guitarrista obligó a Hodgson a aprender a tocar el bajo.

A pesar de que las canciones del primer álbum del grupo, "Supertramp", publicado en 1970, fueron acreditadas a Hodgson, Palmer y Davies, las letras fueron compuestas por Palmer. Sin embargo, la temprana marcha de Palmer del grupo permitió a Hodgson volver a la guitarra y centrarse, junto a su compañero Davies, en la composición de los temas de Supertramp a partir de su segundo álbum, "Indelibly Stamped". 

Con una formación fija y compuesta por John Helliwell al saxofón, Bob Siebenberg a la batería y Dougie Thomson al bajo, Supertramp obtuvo su primer éxito comercial con el álbum "Crime of the Century", que alcanzó el primer puesto en las listas de éxitos canadienses y el puesto 38 en la lista "Billboard 200". El éxito del grupo continuó con sus posteriores trabajos, "Crisis? What Crisis?" e "Even in the Quietest Moments", y alcanzó su apogeo con la publicación en 1979 de "Breakfast in America", que alcanzó el primer puesto en la lista "Billboard 200" y ha vendido hasta la fecha más de 20 millones de copias a nivel mundial. 
Entre 1974 y 1983, todas las canciones de Supertramp fueron legalmente fijadas con créditos compartidos entre Davies y Hodgson. Sin embargo, ambos compositores nunca escribieron como tándem, y en términos generales cada uno escribió la mitad del catálogo musical del grupo, perteneciendo a Hodgson canciones como «Give a Little Bit», «Breakfast in America», «The Logical Song», «Take the Long Way Home» e «It's Raining Again», entre otras.

El uso por parte de Davies de las canciones compuestas por Hodgson, generalmente más comerciales y reconocidas como marca de Supertramp, se convirtió desde la marcha de Hodgson en una de las principales disputas con su antiguo compañero de grupo. No obstante, a lo largo de la historia de Supertramp, la relación de amistad entre Davies y Hodgson se fue distanciando prematuramente a medida que las inclinaciones musicales y sus respectivos estilos de vida coincidían cada vez menos.

Tras la gira de promoción de "Breakfast in America" y la publicación en 1980 del álbum "Paris", Hodgson cambió su residencia y se trasladó desde Los Ángeles hasta las montañas del norte de California, donde construyó una casa y un estudio de grabación y comenzó a centrar su actividad en su familia y en la vida espiritual. La distancia geográfica separó aún más a Hodgson del grupo, y durante la grabación de "...Famous Last Words...", Davies y Hodgson encontraron dificultades en conciliar sus respectivas ideas musicales. Según declaró Bob Siebenberg en relación a los planteamientos musicales de Davies y Hodgson: «Al final, ambos cambiaron sus formatos y la imagen de cómo tendría que ser el álbum. Se convirtió en una versión diluida de lo que habían pensado».

Publicado en 1982, "...Famous Last Words..." se convirtió en el último trabajo de Supertramp con Hodgson, y fue seguido de una gira de promoción en 1983 donde Hodgson anunció que no iba a continuar con Supertramp. Según declaró Hodgson, su marcha estuvo motivada por el deseo de estar más tiempo con su familia y publicar trabajos en solitario, y que no hubo nunca problemas personales o profesionales entre Davies y él.

Desde su salida de Supertramp en 1983, Hodgson grabó tres álbumes en su estudio de grabación privado, el primero de ellos al poco tiempo de abandonar el grupo. Titulado "Sleeping With The Enemy", el álbum fue grabado en los meses entre la publicación de "...Famous Last Words..." y su posterior gira de promoción, y fue mezclado durante los ensayos con Supertramp con la esperanza de poder promocionar alguna canción durante los conciertos con el grupo. Sin embargo, en el último minuto Hodgson tuvo dudas sobre la calidad del álbum y decidió frenar su publicación, dedicando más tiempo a las nuevas canciones tras finalizar su última gira con Supertramp.

El resultado final fue "In the Eye of the Storm", publicado en 1984 y autoproducido en su mayoría por el propio Hodgson, tanto a nivel instrumental como compositivo. A pesar de la promoción como el primer trabajo de un antiguo miembro de Supertramp, "In the Eye of the Storm" no obtuvo un éxito comercial destacado ni en Reino Unido ni en Estados Unidos. Con un sonido más orientado hacia la música pop a diferencia de sus anteriores trabajos con Supertramp, el primer sencillo, «Had a Dream (Sleeping With the Enemy)», alcanzó el puesto 48 en Estados Unidos, mientras que otros sencillos como «In Jeopardy» y «Hooked On A Problem» no entraron en las listas de éxitos.

Aún no logrando el éxito comercial de sus anteriores trabajos bajo el nombre de Supertramp, "In the Eye of the Storm" se convirtió en el mayor éxito de crítica de su carrera en solitario. Bret Adams escribió para "Allmusic" una reseña positiva en la que afirmó que la calidad del álbum se debía al uso de la mayoría de los instrumentos por parte de Hodgson, y que aunque la música carecía de elementos de rock progresivo, «el espíritu de la experimentación del género musical está vivo en el álbum, con cinco de siete canciones que exceden los seis minutos».

Su segundo trabajo, "Hai Hai", fue publicado en 1987 y marcó un cambio de sonido hacia tendencias musicales orientadas hacia el synthpop, aún manteniendo la habitual línea compositiva de Hodgson. Sin embargo, previo a la publicación del álbum, Hodgson sufrió un accidente doméstico padeciendo la fractura de sus dos muñecas, lo que le impidió promocionar su trabajo. Como resultado, "Hai Hai" alcanzó solo el puesto 163 en la lista "Billboard 200", y Hodgson decidió tomar un descanso de giras y grabaciones y pasar más tiempo con sus hijos mientras se recuperaba de las lesiones.

En 1990, el grupo Yes ofreció a Hodgson entrar como vocalista, pero rechazó la oferta. Sin embargo, colaboró con Trevor Rabin componiendo la canción «Walls», publicada en el álbum de Yes "Talk". Una versión de la canción, con Hodgson y Rabin en la voz, fue publicada en el álbum "90125", con demos y trabajos de estudio de Rabin.

El primer intento por rehacer la formación clásica de Supertramp tuvo lugar en 1993, tras coincidir con Davies en un concierto homenaje a Jerry Moss, fundador de A&M Records, en el que interpretaron «The Logical Song» y «Goodbye Stranger». Tras el concierto, Hodgson y Davies colaboraron en el estudio desarrollando canciones como «In The Light» y «You Win I Lose», posteriormente publicadas en el álbum "Some Things Never Change". Sin embargo, la colaboración no dio el resultado esperado y Hodgson prefirió continuar en solitario.

Tras un largo descanso, Hodgson emprendió en 1994 su primera gira en diez años y publicó en 1997 "Rites of Passage", un álbum en directo con canciones interpretadas durante la gira. "Rites of Passage" fue grabado en Nevada City (California) e incluyó una banda de respaldo con su hijo Andrew y su compañero en Supertramp John Helliwell. A pesar de no obtener un éxito comercial destacado en Reino Unido y Estados Unidos, el álbum alcanzó el puesto 34 en Alemania. 

Dos años después, Hodgson interpretó el papel de Rey Arturo en la ópera rock "Excalibur: La Legende Des Celtes", un proyecto liderado por Alan Simon y publicado en 1999, y apareció en dos canciones del álbum, «The Elements» y «The Will of God». Además, contribuyó en los coros de la canción «The Moon Says Hello» en el álbum "Mayo Longo" del músico español Carlos Núñez.

Su tercer álbum en solitario, "Open the Door", fue publicado en el año 2000, siguiendo la estela de sus anteriores trabajos, y cuenta con la colaboración de Alan Simon y Trevor Rabin. En agosto del mismo año, Hodgson participó en la Fairport Convention interpretando los temas «Breakfast In America», «The Logical Song», «Open The Door» y «Give A Little Bit».
Durante 2001, Hodgson salió de gira como miembro del grupo All-Starr Band de Ringo Starr. Desde 2004 y hasta la actualidad, Hodgson ha emprendido giras anuales en las que combina actuaciones en solitario, interpretando canciones con la guitarra o con el piano, y conciertos con una banda de apoyo e incluso, en ocasiones, con orquesta, y dejando de lado cualquier trabajo en el estudio de grabación. Su gira de 2004 le llevó a escenarios de Centroeuropa y Canadá, mientras que en 2005 amplió sus conciertos a Estados Unidos y ofreció su primer concierto en veinte años en Londres, grabado para un futuro lanzamiento en DVD que se desechó. En su lugar, el concierto ofrecido en el Place Des Arts de Montreal el 6 de junio de 2006 fue publicado en el DVD "Take The Long Way Home - Live in Montreal", certificado como disco de platino por la Canadian Recording Industry Association. El DVD fue publicado a nivel global por Eagle Vision en 2007 y fue certificado como disco de oro en Alemania y Francia.

En mayo de 2006, Hodgson fue honrado por la ASCAP en reconocimiento de su canción «Give A Little Bit», por ser una de las más interpretadas del catálogo de ASCAP en 2005. Dos años después fue premiado nuevamente por la ASCAP por la canción de Gym Class Heroes «Cupid’s Chokehold», un "remake" de la canción «Breakfast in America» con amplia difusión comercial en 2007.

Durante su gira de 2007, Hodgson participó en el "Concert for Diana" organizado el 1 de julio en el Estadio de Wembley, interpretando un popurrí de sus canciones más conocidas: «The Logical Song», «Dreamer», «Breakfast in America» y «Give A Little Bit».
En 2009 ofreció conciertos en Europa y Norteamérica, visitó por primera vez países como Ecuador, Venezuela y participó en el Festival Internacional de la Canción de Viña del Mar en Chile y en el Festival Cultural Zacatecas de México. En 2010 organizó una nueva gira mundial con nuevos conciertos en Norteamérica, Sudamérica y Europa, visitando por primera vez países como Panamá.

En 2010, su compañero Rick Davies volvió a reformar Supertramp para emprender una gira con motivo del 40º aniversario del grupo. La promoción de la gira con canciones de Hodgson provocó el enfado de Roger, según el cual el uso de sus canciones rompe un acuerdo verbal entre él y Rick de no interpretarlas a cambio de mantener Davies el nombre de Supertramp. Ante esta situación, Hodgson mantuvo la línea de años anteriores y ofreció conciertos en solitario a nivel mundial, y publicó en octubre "Classics Live", un álbum en formato digital con canciones interpretadas durante la gira de 2010.

El 10 de abril de 2012, Hodgson comenzó la gira "Breakfast In America Tour" en Sudamérica, pasando por países como Ecuador, Perú, Chile, Argentina, Brasil y México. El 4 de mayo de 2012 fue nombrado Caballero de la Orden de las Artes y las Letras por el Ministerio de Cultura de Francia en reonocimiento a su contribución al mundo del arte.


https://www.facebook.com/rogerhodgsonspain/

Roger Hodgson Spain


</doc>
<doc id="15518" url="https://es.wikipedia.org/wiki?curid=15518" title="Hortaliza">
Hortaliza

Las hortalizas son un conjunto de plantas cultivadas generalmente en huertos o regadíos, que se consumen como alimento, ya sea de forma cruda o preparadas culinariamente, y que incluye las verduras y las legumbres (las habas, los guisantes, etc.). Las hortalizas no incluyen a las frutas ni a los cereales.

Sin embargo, esta distinción es arbitraria y no se basa en ningún fundamento botánico. La Real Academia Española no reconoce esta taxonomía, y circunscribe esta acepción a los cultivos realizados en un huerto.


Valor calórico. La mayor parte de las hortalizas son hipocalóricas. Por ejemplo, 100g de acelgas solo contienen quince calorías. La mayoría no superan las cincuenta calorías por 100g, excepto las alcachofas y las papas. Debido a este bajo valor calórico, las hortalizas deberían estar presentes en un gran porcentaje en una dieta contra la obesidad.

Todas estas propiedades hacen que sea recomendable consumirlas con bastante frecuencia y diariamente: se recomienda una ración en cada comida y de la forma más variada posible. Por eso las hortalizas ocupan el segundo piso, junto con las frutas, en la pirámide de los alimentos. Vale aclarar que esta pirámide es solo una de las teorías existentes en la alimentación humana: existen otras pirámides nutricionales, como las que plantean el vegetarianismo, el veganismo o el crudiveganismo.

Las hortalizas frescas deben conservarse adecuadamente hasta el momento del consumo. Las condiciones y duración del almacenamiento influyen mucho en el aspecto y valor nutritivo. La mayoría de las hortalizas deben conservarse a temperaturas bajas con una alta humedad ambiental, por lo que el verdulero del refrigerador es el lugar más recomendable. Se aconseja ponerlas en bolsas agujereadas o con láminas de aluminio, y evitar que el envase sea hermético. En el frigorífico se pueden conservar algunos días, según la clase de hortaliza. Por ejemplo, las espinacas, la lechuga, etc, no conviene tenerlas más de tres días; sin embargo, las zanahorias, los nabos y la remolacha son menos sensibles, y se conservan durante más tiempo. Algunas hortalizas, como las cebollas y los ajos secos, no precisan ser conservadas en el refrigerador, sino que es más adecuado un lugar seco y aireado.

Las verduras son partes de las plantas herbáceas que son idóneas para el consumo humano. Estos componentes comestibles de la planta pueden ser tallos, hojas, raíces, flores y frutos. El valor nutritivo de las verduras define la presencia de esas sustancias esenciales que son importantes para mantener la vida. Los científicos categorizan las verduras como nutracéuticos, porque son una mezcla de nutrición y farmacéutica: ciertas sustancias químicas presentes en los vegetales tienen un gran valor medicinal.

Las hortalizas se han de lavar o cepillar cuidadosamente antes de ser consumidas, según se trate de hojas, raíces o tubérculos. Cuando no se puedan pelar, hay que limpiarlas mucho, sobre todo si tienen la piel rugosa o peluda. Las hortalizas que se coman crudas deberían sumergirse en agua con unas gotas de lejía diluida durante unos cinco minutos, y después limpiarlas con agua corriente. Se debe hacer esto porque las hortalizas se riegan a veces con aguas no potables que pueden contener numerosas bacterias, y el agua de riego entra en contacto con la hortaliza, que suele estar a ras de suelo.

Las vitaminas de las hortalizas se destruyen con la exposición a la luz, el aire y el calor. Las sales minerales se disuelven en el agua al cocer las hortalizas. Para poder beneficiarse de las vitaminas, de los minerales y del sabor, es preciso cocinar las hortalizas con poca agua (o, mejor, con vapor) y de una forma muy rápida, sumergiéndolas directamente en agua hirviendo. El recipiente de cocción debe mantenerse tapado y evitar moverlo (o moverlo lo menos posible). El agua de cocción debería aprovecharse para hacer sopas, consomés y otro tipo de caldos, porque en el agua de cocción es donde se concentran los minerales. Las hortalizas cocidas que no se vayan a consumir en el momento deben enfriarse y guardarse en el refrigerador. Después se pueden volver a calentar, pero durante poco tiempo.


</doc>
<doc id="15519" url="https://es.wikipedia.org/wiki?curid=15519" title="Etnografía de Venezuela">
Etnografía de Venezuela

La etnografía de Venezuela se caracteriza por ser el resultado de la mezcla de tres grupos étnicos principales:, europeos, amerindios y africanos subsaharianos. Según el Censo de 2011, las personas de raza mixta representan la mayoría de la población, seguido por los blancos, negros e indígenas. Otros grupos, como los asiáticos, se han incorporado recientemente a la etnografía venezolana.

De acuerdo con el censo de 2011, cuando se inquirió a las personas acerca de su condición étnica o racial, con las opciones: «moreno», «blanco», «negro», «afrodescendiente» u «otro», un 51,6% de la población dijo ser morena, 43,6% se identificó como blanca, un 2,8% dijo ser negra, 0,7% dijo ser afrodescendiente y el resto (1,2%) mentó que es de otra raza. Los indígenas fueron contados de forma independiente, sin tener en cuenta el color de la piel, y un 2,7% de los venezolanos se identificó como tal.

En el censo realizado en 2011 se define a Morena/Moreno como "toda persona cuyas características fenotípicas son menos marcadas o pronunciadas que de la persona definida como negra o negro. Es un término que en algunos contextos puede ser utilizado para suavizar las implicaciones discriminatorias que conlleva ser una persona negra." 

Se aplica el término "moreno" a personas con una apariencia intermedia entre los estereotipos indígenas, europeos y africanos. Los morenos están distribuidos en toda Venezuela. El mestizaje en Venezuela comenzó en el siglo XVI cuando los conquistadores y colonos españoles se unían con mujeres indígenas o africanas, debido a la escasez de mujeres españolas en el país. También había enlaces entre esclavos africanos y mujeres indígenas, dando origen a los zambos.

Los inmigrantes europeos eran al principio colonos españoles. En el siglo XX llegaron miles de personas provenientes de España, Portugal, Italia, Alemania y Europa oriental, etc., debido a la Segunda Guerra Mundial, la Guerra Civil Española y el crecimiento económico del país, aportando a la sociedad venezolana nuevas formas culturales como gastronomía, infraestructura, etc. En el área metropolitana de Caracas se concentran la mayoría de los europeos migrados después de la Segunda Guerra Mundial.

La población negra africana fue traída como esclava, sobre todo en las tierras bajas costeras, comenzando a principios del siglo XVI, y continuando hasta el siglo XIX. Actualmente representan el 3,5% de la población venezolana.

Muchos de los pueblos indígenas fueron absorbidos por el mestizaje, pero aún existen grupos indígenas que mantienen su cultura e idiomas propios. De acuerdo al censo de 2011, la población indígena del país ascendía a personas, lo que representa el 2,7% del total nacional. El 36,74% de los indígenas todavía vivía en comunidades indígenas de carácter rural, especialmente en los de Zulia, Bolivar, Amazonas y Delta Amacuro. Dentro de la población que se autorreconoció como originaria, un 57,3% dijo ser de la etnia wayúu; 6,7% warao, 4,7% kariña, 4,2% pemón; el 3% cada uno, jivi, cumanagoto, añu y piaroa; 2% chaima, pume y yukpa y 1,3% yanomami.

Entre la población de origen asiático, los chinos son el grupo más numeroso, con alrededor de viviendo en territorio venezolano.

Los judíos venezolanos han abandonado el país de forma masiva desde el inicio de la Revolución Bolivariana. Se estima que actualmente hay judíos en Venezuela, frente a los que vivían en el país en 1998.

Según la Federal Research Division, el 68% de los venezolanos son mestizos (mezcla de razas), el 21% son blancos, el 10% afrodescendientes y el 1% indígenas.

El antropólogo mexicano Francisco Lizcano Fernández estima que el 37,7% de los venezolanos son mestizos, el 37,7% mulatos, el 16,9% blancos, el 2,8% negros, el 2,7% indígenas y el 2,2% asiáticos.

En la encuesta Latinobarómetro de 2016, el 33% de los venezolanos se identificó como mestizo, el 32% como blanco, el 21% como mulato, el 8% como negro y el 4% como indígena.

Según el Censo de 2011, la composición étnica autorreconocida por Estado era la siguiente:

En el Censo de 2011 los indígenas fueron contados como una categoría independiente del color de la piel. Los resultados por Estado fueron los siguientes:

Se han realizado algunos estudios genéticos para determinar la composición étnica del individuo venezolano, los cuales han arrojado los siguientes resultados:

(Simmons et al, 2007) definió la composición genética en dos regiones de Venezuela:
(Castro de Guerra et al, 2011) recopila datos de investigaciones anteriores, de modo que se registra la siguiente composición genética de las regiones venezolanas:
También comprendió mezclas raciales en dos regiones venezolanas:
Y en el noreste del país, encontró que la composición racial se distribuía de la siguiente forma:
Una investigación genética realizada por (López-Carmelo et al, 1996), en 35 hospitales de 13 ciudades suramericanas, 4 de ellas venezolanas, arrojó los siguientes resultados:
Según el estudio de (Salsano y Sanz, 2014), la composición genética en algunas ciudades del occidente venezolano es la siguiente:
Según la recopilación de (Castro de Guerra et al, 2011), la mezcla racial de los grupos étnicos de Venezuela es la siguiente:
Según algunas investigaciones recopiladas en (Castro de Guerra et al, 2011), la composición genética en localidades de mayoría afrovenezolana se distribuye así:
Otras investigaciones, indagaron por la mezcla racial de las localidades de mayoría blanca, encontrando los siguientes resultados (Castro de Guerra et al, 2011):
Según un estudio realizado por (Galanter et al, 2012) sobre poblaciones mestizas e indígenas de Venezuela, la mezcla genética es la siguiente:
(Martínez et al, 2007) encontró que la mezcla racial en Caracas, de acuerdo al nivel socioeconómico, era la siguiente:


</doc>
<doc id="15521" url="https://es.wikipedia.org/wiki?curid=15521" title="Laika">
Laika

Laika (en ruso "Лайка", ‘ladradora’; Moscú, Unión Soviética, 1954 - Sputnik 2, Órbita baja terrestre, 3 de noviembre de 1957) fue una perra espacial soviética que se convirtió en el primer ser vivo terrestre en orbitar la Tierra. Lo hizo a bordo de la nave soviética Sputnik 2, el 3 de noviembre de 1957, un mes después que el satélite Sputnik 1. También fue el primer animal que murió en órbita.

Como se sabía poco sobre los efectos que los vuelos espaciales podían producir sobre los seres vivos en el momento de la misión de Laika y, la tecnología suborbital no se había desarrollado todavía, no se tenía ninguna expectativa de que Laika sobreviviera. Algunos científicos creían que los humanos no podrían sobrevivir al lanzamiento o a las condiciones del espacio exterior, por eso los ingenieros de vuelo vieron a los vuelos de animales como los precursores necesarios para las misiones humanas. Laika, una perra callejera, originalmente llamada Kudryavka (Кудрявка, ‘pequeña de pelo rizado’), fue sometida a entrenamiento con otros dos perros, y finalmente fue elegida como la tripulante de la nave espacial soviética Sputnik 2, lanzada al espacio exterior el 3 de noviembre de 1957.

Laika murió horas después del lanzamiento por sobrecalentamiento, que probablemente fue ocasionado por un fallo del sustentador de la central R-7, que forma parte del sistema térmico de la nave, al separarse de la carga útil. La verdadera causa y tiempo de su muerte no fue revelada sino hasta 2002; en cambio, fue ampliamente informado que había muerto al sexto día, se quedó sin oxígeno, o como el gobierno soviético alegó inicialmente, fue sometida a eutanasia antes del agotamiento del oxígeno. El experimento demostró que es posible que un pasajero vivo sobreviva al ser puesto en órbita y soportar la microgravedad, allanando el camino para los vuelos espaciales humanos y proporcionando a los científicos algunos de los primeros datos sobre cómo los organismos vivos reaccionan a los entornos de los vuelos espaciales. Tras Laika, la URSS envió doce perros más al espacio, de los cuales cinco regresaron con vida a la Tierra.

El 11 de abril de 2008, las autoridades rusas desvelaron un monumento a Laika. Este pequeño monumento en su honor fue construido cerca del centro de investigación militar en Moscú que preparó el vuelo de Laika al espacio. Cuenta con la figura de un perro que se coloca en la parte superior de un cohete.

Tras el éxito del Sputnik 1, el líder soviético Nikita Jrushchov solicitó que se lanzara un segundo satélite artificial al espacio para el día del cuadragésimo aniversario de la revolución bolchevique, el 7 de noviembre de 1957. Cuando se recibió esta solicitud, ya se estaba construyendo un satélite más sofisticado, pero que no estaría listo sino hasta un mes después de la fecha requerida, por lo que fue descartado. El satélite descartado, sería el Sputnik 3.

Para cumplir con la fecha límite de noviembre, tendría que construirse una nueva nave. Específicamente, Kruschev quería ofrecerle a sus ingenieros un "espacio espectacular", una misión que repetiría el triunfo del Sputnik I, aturdiendo al mundo con proezas soviéticas. Los planes se asentaron en un vuelo orbital con un perro. Los ingenieros soviéticos de cohetes habían previsto con antelación una órbita "canina" antes de intentar vuelos espaciales humanos; desde 1951, habían lanzado 12 perros al espacio suborbital en vuelos balísticos, trabajando gradualmente hacia una misión orbital posiblemente en algún momento de 1958. Para satisfacer las demandas de Jruschov, el vuelo orbital canino fue acelerado para su lanzamiento en noviembre.

Según fuentes rusas, la decisión oficial de lanzar el Sputnik 2 se realizó el 10 o 12 de octubre, dejando al equipo solo cuatro semanas para diseñar y construir la nave espacial. El Sputnik 2, por lo tanto, tenía algo de un trabajo urgente, con la mayoría de los elementos de la nave espacial siendo construidos basándose en bocetos. Además de la misión principal de envío de un pasajero para vivir en el espacio, el Sputnik 2 también contenía instrumentación para la medición de la radiación solar y rayos cósmicos.

La nave estaba equipada con un sistema de soporte vital que consistía en un generador de oxígeno y aparatos para evitar envenenamiento por oxígeno, también conocido como "efecto de Paul Bert", y para absorber dióxido de carbono. Se añadió un ventilador, diseñado para activarse cuando la temperatura de la cabina superaba los 15 °C, con el propósito de mantener la temperatura del animal. Se proporcionó suficiente comida (en forma gelatinosa) para un vuelo de siete días, y la perra fue equipada con una bolsa para recoger los residuos. Adicionalmente, se diseñó un arnés para ser colocado al animal, por lo que no había cadenas para restringir sus movimientos al sentarse, ponerse de pie o acostarse; ya que en la cabina no había espacio para dar vueltas. Un electrocardiograma monitorizaba la frecuencia cardíaca y la instrumentación adicional medía la frecuencia respiratoria, la presión arterial máxima y los movimientos de la perra.

Laika fue encontrada como una perra callejera vagando por las calles de Moscú. Los científicos soviéticos optaron por utilizar perros callejeros de Moscú ya que se asumía que estos animales ya habían aprendido a soportar las condiciones extremas de frío y de hambre. Este espécimen era una hembra mestiza de 5 kg (11 libras) de aproximadamente tres años de edad. Otro relato informó que pesaba alrededor de 6 kg (13 lb). El personal soviético le dio varios nombres y apodos, entre ellos Kudryavka ("rizadita"), después Zhuchka ("bichito"), y luego Limonchik ("limoncito"). Laika, el nombre ruso de semejantes al husky, sería el nombre popularizado en todo el mundo. La prensa norteamericana la apodó Muttnik ("mutt" + el sufijo -nik) como un juego de palabras sobre el Sputnik, o también se refería a ella como "Curly". Su verdadero pedigrí es desconocido, aunque en general se acepta que fue parte husky u otra raza nórdica, y posiblemente parte terrier. Una revista rusa describió su temperamento como "flemático", argumentando que no se peleaba con otros perros. Vladimir Yazdovsky, quien dirigió el programa de perros de prueba utilizados en cohetes, en una publicación tardía escribió que «"Laika era tranquila y encantadora"».

Antes del lanzamiento del Sputnik 2, tanto la Unión Soviética como Estados Unidos, ya habían lanzado animales vivos en vuelos suborbitales. Para el vuelo del Sputnik 2, fueron entrenados tres perros: Albina, Mushka, y Laika. Los científicos soviéticos de vida espacial Vladimir Yazdovsky y Oleg Gazenko fueron los encargados de entrenar a los perros.

Adaptar los perros al confinado espacio de la pequeña cabina del Sputnik 2 requirió que permanecieran en compartimientos cada vez menores, por espacios de hasta 20 días. El extenso confinamiento causó que dejasen de orinar o defecar, haciéndolos inquietos, y causando que se deteriorase su estado general. Los laxantes que les suministraron no mejoraron su condición, por lo que los investigadores encontraron que lo único que resultaba eficaz eran los largos periodos de entrenamiento. Los perros fueron luego colocados en centrifugadoras que simulaban la aceleración del lanzamiento de un cohete y se colocaron en máquinas que simulaban los ruidos de la nave espacial. Esto hizo que sus impulsos cardíacos se duplicasen y su presión arterial aumentase un 30-65 torr. Los perros fueron también entrenados para comer un gel especial de alta nutrición que sería su comida en el espacio.

Antes de la puesta en marcha, uno de los científicos llevó a Laika a su casa para que jugase con sus hijos. En un libro que relata la historia de la medicina espacial soviética, el Dr. Vladimir Yazdovsky escribió: "Quería hacer algo bueno por ella: Le quedaba tan poco tiempo de vida."

Vladimir Yazdovsky hizo la selección final de perros y designó sus roles. Laika iba a ser la "perra voladora" —un sacrificio a la ciencia en una misión de ida al espacio. Albina, que ya había volado dos veces en un cohete de prueba a gran altura, se designó como reserva de Laika. El tercer perro, Mushka, era un "perro de control" -se quedaría en tierra y era usada para probar la instrumentación y el soporte vital.

Antes de partir hacia el cosmódromo de Baikonur, Yazdovsky y Gazenko realizaron una cirugía en los perros para conectar los cables de los transmisores a los sensores que medían la respiración, el pulso y la presión arterial.

Debido a que la pista de aterrizaje existente en Turatam, cerca del cosmódromo resultaba pequeña, los perros y la tripulación tuvieron que volar primero a bordo de un avión Tu-104 a Tashkent. Desde allí, un Il-14 más pequeño y ligero los llevó a Turatam. El entrenamiento de perros continuó a su llegada, y uno tras otro fueron colocados en las cápsulas para familiarizarse con el sistema de alimentación.

Según los documentos de la NASA, Laika fue colocada en la cápsula del satélite el 31 de octubre de 1957 —tres días antes del inicio de la misión. En esa época del año, las temperaturas en el sitio de lanzamiento eran extremadamente bajas, por lo que se usó una manguera conectada a un calentador para mantener caliente el contenedor. Dos asistentes estaban encargados de vigilar constantemente a Laika antes del comienzo de la misión. Justo antes del despegue, el 3 de noviembre de 1957, se limpió el pelaje de Laika con una solución de etanol, y le pintaron con yodo aquellas áreas donde la perra llevaría sensores para vigilar sus funciones corporales.

Uno de los técnicos que preparó la cápsula antes del despegue final declaró que "después de la colocación de Laika en el contenedor y antes de cerrar la escotilla, le besamos la nariz y le deseamos buen viaje, sabiendo que no iba a sobrevivir al vuelo."

La hora exacta del despegue varía de una fuente a otra, pero se menciona que fue a las 05:30:42 o a las 7:22 hora de Moscú. Al alcanzar la máxima aceleración después del despegue, el ritmo respiratorio de Laika aumentó de tres a cuatro veces lo normal, y su frecuencia cardiaca pasó de 103 a 240 latidos por minuto. Al alcanzar la órbita, se desprendió exitosamente la punta cónica del Sputnik 2. La otra sección de la nave que debía desprenderse (el "Blok A") no lo hizo, impidiendo que el sistema de control térmico funcionara correctamente. Se desprendió parte del aislamiento térmico, permitiendo que la cápsula alcanzara una temperatura interior de 40 °C. Tras tres horas de microgravedad, el pulso de Laika había descendido a 102 latidos por minuto; este descenso en la frecuencia cardíaca había tomado tres veces más tiempo que lo experimentado durante el entrenamiento, lo cual indicaba el estrés bajo el que estaba la perra. Los datos telemétricos iniciales mostraban que, aunque Laika estaba agitada, estaba comiendo. La recepción de datos vitales se detuvo entre cinco y siete horas después del despegue.

Los científicos soviéticos planearon sacrificarla con comida envenenada, que Laika consumiría después de diez días. Durante muchos años, la Unión Soviética dio explicaciones contradictorias sobre la muerte de Laika, diciendo a veces que la perra había muerto por falta de oxígeno cuando fallaron las baterías, o que había recibido eutanasia. En 1999, fuentes rusas aseguraron que Laika sobrevivió por lo menos cuatro días, y después pereció por el sobrecalentamiento de la nave. En octubre de 2002, el científico Dimitri Malashenkov, quien participó en el lanzamiento del Sputnik 2, reveló que Laika había muerto entre cinco y siete horas después del despegue, debido al estrés y sobrecalentamiento. De acuerdo a un artículo que presentó en el Congreso Mundial del Espacio en Houston:
El Sputnik 2 orbitó la Tierra 2.570 veces, durante 163 días. La nave se desintegró al entrar en contacto con la atmósfera el 14 de abril de 1958.

Debido al problema de la opacidad por la carrera espacial entre los Estados Unidos y la Unión Soviética, las cuestiones éticas planteadas por este experimento pasaron, en gran medida, sin abordarse durante algún tiempo. La prensa de 1957 estaba más preocupada en informar del impacto desde el punto de vista político, mientras que la salud y la recuperación —o la ausencia— de Laika se convirtió solo en un problema menor.

El Sputnik 2 no fue diseñado para ser recuperable, y siempre se tuvo la intención de que Laika muriera. La misión desencadenó un debate mundial sobre el maltrato y los experimentos con animales para avanzar en la ciencia. En el Reino Unido, la Liga Nacional de Defensa Canina ("NCDL", actualmente "Fundación para los Perros") pidió que los dueños de perros guardaran un minuto de silencio en honor a Laika. La Real Sociedad para la Prevención de la Crueldad contra los Animales (RSPCA) recibió protestas incluso antes de que Radio Moscú hubiera terminado de anunciar el lanzamiento. Varios grupos protectores de los derechos animales protestaron frente a embajadas soviéticas. Otros se manifestaron frente a las Naciones Unidas en Nueva York; sin embargo, algunos científicos estadounidenses ofrecieron apoyo a sus colegas soviéticos, por lo menos antes de que se anunciara la muerte de Laika.

Dentro de la Unión Soviética hubo menos controversia. Ni los medios de comunicación, ni los libros de los años siguientes, ni el público cuestionaron abiertamente la decisión de enviar un perro al espacio. No fue sino hasta 1998, después del colapso del régimen soviético, que Oleg Gazenko, uno de los científicos responsables del envío de Laika al espacio, expresó su pesar por permitir que muriese:

En otros países del Pacto de Varsovia era difícil realizar protestas abiertas del programa espacial soviético debido a la censura política. Sin embargo, hubo casos notables de críticas en los círculos científicos polacos. Una revista científica polaca, "Kto, Kiedy, Dlaczego", publicada en 1958, se refirió a la misión del Sputnik 2. En la sección de la revista dedicada a la astronáutica Krzysztof Boruń la describió no trayendo a Laika a la Tierra con vida como "lamentable" y "sin duda, una gran pérdida para la ciencia".

Laika es conmemorada en la forma de una estatua y placa en la Ciudad de las Estrellas, el centro ruso de Entrenamiento de Cosmonautas.

Las futuras misiones espaciales que llevasen perros serían diseñadas para ser recuperadas. Los únicos otros perros que murieron en una misión espacial soviética fueron Pchyolka y Mushka, que murieron cuando el Sputnik 6 fue destruido intencionalmente con una carga explosiva, a su reingreso, con el fin de evitar que las potencias extranjeras inspeccionaran la cápsula, debido a una trayectoria de reentrada atmosférica descontrolada, el 1 de diciembre de 1960.

El viaje de Laika la convirtió en uno de los perros más famosos del mundo.
En 1997, en la Ciudad de las Estrellas, fue desvelada una placa en homenaje a los cosmonautas caídos. Laika está representada en una esquina de la placa, espiando por entre las piernas de uno de los cosmonautas.
En el "Monumento a los Conquistadores del Espacio" (1964), en Moscú, Laika y Lenin son los únicos personajes que se pueden reconocer por su nombre, de entre todos los personajes que aparecen esculpidos en el monumento.
En distintos países se crearon sellos de correo con la imagen de la perra Laika, conmemorando su vuelo. Marcas de chocolates y cigarros fueron nombradas en su memoria, y una gran colección de souvenirs de Laika todavía aparece en subastas actualmente. 
El 9 de marzo de 2005, un área de terreno en el planeta Marte fue llamada "Laika", aunque no oficialmente, por los controladores de la misión del Mars Exploration Rover. El lugar se localiza cerca del cráter "Vostok" en Meridiani Planum.

Laika ha aparecido en numerosas obras literarias, mayormente de ciencia ficción o también de fantasía, que frecuentemente narran historias sobre su rescate o supervivencia. La novela "Intervention" (Intervención) de Julian May, relata que Laika fue rescatada por extraterrestres. En la novela "Weight" (Peso) de Jeanette Winterson, el titán griego Atlas encuentra la cápsula en órbita, y adopta al animal. En la serie Doctor Who se narró una historia sobre su funeral. En un capítulo de la historieta "Flash Gordon" aparece Laika rescatada por una raza de extraterrestres lunares con aspecto perruno.

Los nombres de varios grupos musicales están inspirados en Laika, entre ellos "Laika", "Laika Dog" y "Laika and the Cosmonauts". La perra apareció en la cubierta de los primeros tres álbumes del grupo "Laika". Laika es también el nombre de varias canciones, producidas por las bandas Arcade Fire, Moxy Früvous, The Cardigans. Massacre compuso un tema llamado "Laika se Va" donde relata el viaje desde la perspectiva de la perra. En 1988, el grupo español Mecano, en su álbum Descanso Dominical, incluyó una canción llamada ""Laika"" que relata el lanzamiento del Sputnik 2. La banda alemana "C.C.C.P." lanzó un álbum llamado "Cosmos" en 1996, cuya temática giraba alrededor del programa espacial soviético. En dicho álbum, la canción "Laika, Laika", tiene un coro militar ruso. El álbum "Laika Come Home" (2002) es un remix que el grupo Spacemonkeyz hizo del primer álbum de la banda Gorillaz. El título es una mezcla del nombre de la perra rusa, con el título de la primera película de la perra Lassie ("Lassie come home"). Antonio Arias en su disco "Multiverso" (2010) le dedica una canción, "Laika". Además, la perra ha sido tema de otros artistas como Akino Arai, György Kurtág y Åge Aleksandersen entre muchos otros.

El 11 de abril de 2008 fue inaugurado un monumento en honor a la perra Laika en el centro de Moscú.
Dicho monumento fue colocado en un centro comercial cerca del Instituto de Medicina Militar, donde medio siglo atrás ocurrieron los experimentos científicos con la participación de la célebre perra. La figura de bronce, de dos metros de altura, representa uno de los segmentos de un cohete espacial, que se transforma en una mano humana, sobre la cual está el cuerpo de Laika.




</doc>
<doc id="15523" url="https://es.wikipedia.org/wiki?curid=15523" title="Gases nobles">
Gases nobles

Los gases nobles son un grupo de elementos químicos con propiedades muy similares: por ejemplo, bajo condiciones normales, son gases monoatómicos inodoros, incoloros y presentan una reactividad química muy baja. Se sitúan en el grupo 18 (VIIIA)de la tabla periódica (anteriormente llamado grupo 0). Los siete gases son helio (He), neón (Ne), argón (Ar), kriptón (Kr), xenón (Xe), el radiactivo radón (Rn) y el sintético oganesón (Og).

Las propiedades de los gases nobles pueden ser explicadas por las teorías modernas de la estructura atómica: a su capa electrónica de electrones valentes se la considera "completa", dándoles poca tendencia a participar en reacciones químicas, por lo que solo unos pocos compuestos de gases nobles han sido preparados hasta 2008. El xenón reacciona de manera espontánea con el flúor (debido a la alta electronegatividad de este), y a partir de los compuestos resultantes se han alcanzado otros. También se han aislado algunos compuestos con kriptón. Los puntos de fusión y de ebullición de cada gas noble están muy próximos, difiriendo en menos de 10°C; consecuentemente, solo son líquidos en un rango muy pequeño de temperaturas.

El neón, argón, kriptón y xenón se obtienen del aire usando los métodos de licuefacción y destilación fraccionada. El helio es típicamente separado del gas natural y el radón se aísla normalmente a partir del decaimiento radioactivo de compuestos disueltos del radio. Los gases nobles tienen muchas aplicaciones importantes en industrias como iluminación, soldadura y exploración espacial. La combinación helio-oxígeno-nitrógeno (trimix) se emplea para respirar en inmersiones de profundidad para evitar que los buzos sufran el efecto narcótico del nitrógeno. Después de verse los riesgos causados por la inflamabilidad del hidrógeno, este fue reemplazado por helio en los dirigibles y globos aerostáticos.

"Gas noble" es una traducción del nombre alemán , usado por primera vez en 1898 por Hugo Erdmann, para indicar su extremadamente bajo nivel de reactividad. El nombre hace una analogía con el término «metales nobles», como el oro, asociado con riqueza y nobleza, y que tiene también una baja reactividad. También se ha dado a los gases nobles el nombre "gases inertes", pero esta etiqueta ha sido desaprobada a medida que los gases nobles se han ido conociendo más. "Gases raros" es otro término que se ha utilizado, pero también es incorrecto porque el argón conforma una parte bastante considerable (0,94 % por volumen, 1,3 % por masa) de la atmósfera terrestre.

Pierre Janssen y Joseph Norman Lockyer fueron los primeros en descubrir un gas noble el 18 de agosto de 1868 cuando examinaban la cromosfera del Sol, y lo llamaron helio a partir de la palabra griega para el Sol, (""). Anteriormente, en 1784, el químico y físico inglés Henry Cavendish había descubierto que el aire contenía una pequeña proporción de una sustancia menos reactiva que el nitrógeno. Un siglo más tarde, en 1895, lord Rayleigh descubrió que las muestras de nitrógeno del aire son de diferente densidad que las del nitrógeno como consecuencia de reacciones químicas. En colaboración con William Ramsay, científico del University College de Londres, Lord Rayleigh postuló que el nitrógeno extraído del aire se encontraba mezclado con otro gas y ejecutó un experimento que consiguió aislar exitosamente un nuevo elemento: el argón, palabra derivada del griego ("argós"), "inactivo". A partir de este descubrimiento, notaron que faltaba una clase completa de gases en la tabla periódica. Durante su búsqueda del argón, Ramsay también consiguió aislar el helio por primera vez, al calentar cleveíta, un mineral. En 1902, después de aceptar la evidencia de la existencia de los elementos helio y argón, Dmitri Mendeléyev incluyó estos gases nobles como Grupo 0 en su clasificación de elementos, que posteriormente se convertiría en la tabla periódica.

Ramsay continuó con la búsqueda de estos gases usando el método de la destilación fraccionada para separar aire líquido en varios componentes. En 1898, descubrió el kriptón, el neón y el xenón, llamados así a partir del griego (', "oculto"), (', "nuevo"), y ("", "extraño"), respectivamente. Por su parte, el radón fue identificado por primera vez en 1898 por Friedrich Ernst Dorn, y se le llamó "emanación de radio", pero no fue considerado como un gas noble hasta 1904, cuando se determinó que sus características eran similares a las de los otros gases nobles. Ese mismo año, Rayleigh y Ramsay recibieron el Premio Nobel de Física y Química, respectivamente, por el descubrimiento de los gases nobles.

El descubrimiento de los gases nobles ayudó a la compresión de la estructura atómica. En 1895, el químico francés Heri Moissan intentó infructuosamente producir una reacción entre el flúor, el elemento más electronegativo, y el argón, uno de los gases nobles, con el fin de aislar de la atmósfera aquellos gases caracterizados por su extraordinaria inercia química, comenzando por el que está en mayor abundancia relativa, y de crear nuevos elementos o compuestos. Los científicos fueron incapaces de producir compuestos de argón hasta fines del siglo XX, pero sus intentos ayudaron a desarrollar nuevas teorías de la estructura atómica. Basándose en estos experimentos, el físico danés Niels Bohr propuso en 1913 que los electrones en los átomos se encontraban ordenados en capas electrónicas en torno al núcleo y que en el caso de los gases nobles, exceptuando al helio, la capa exterior siempre contenía ocho electrones. En 1916, Gilbert N. Lewis formuló la regla del octeto, la cual concluye que la configuración más estable para cualquier átomo es contar con ocho electrones en la capa exterior; esta configuración produce elementos que no reaccionan con otros, ya que no necesitan más electrones para completar su capa exterior.

En 1962 Neil Bartlett descubrió el primer compuesto químico de un gas noble, el hexafluoroplatinato de xenón. Compuestos de otros gases nobles fueron descubiertos poco después: en 1962, el fluoruro de radón, y en 1963, el difluoruro de kriptón (KrF). El primer compuesto estable de argón se reportó en 2000 cuando se formó el fluorohidruro de argón a una temperatura de 40 K (−233,2 °C; −387,7 °F).

En diciembre de 1998, científicos del Joint Institute for Nuclear Research trabajando en Dubna, Rusia, bombardearon plutonio (Pu) con calcio (Ca) para producir un único átomo del elemento 114, bajo el nombre Flerovio (Fl). Experimentos químicos preliminares indican que este elemento puede ser el primer elemento transuránico en mostrar propiedades anormales y parecidas a las de los gases nobles, aun cuando es miembro del grupo 14 en la tabla periódica. En octubre de 2006, científicos del Joint Institute for Nuclear Research y del Lawrence Livermore National Laboratory sintetizaron exitosamente el oganesson (Og), el séptimo elemento en el Grupo 18, al bombardear californio (Cf) con calcio (Ca). Como curiosidad cabe indicar que la discusión científica sobre la posibilidad de licuar estos gases dio lugar al descubrimiento de la superconductividad por el físico holandés Heike Kamerlingh Onnes.

Los gases nobles cuentan con fuerzas intermoleculares muy débiles y, por lo tanto, tienen puntos de fusión y de ebullición muy bajos. Todos ellos son gases monoatómicos bajo condiciones estándar, incluyendo aquellos que tienen masas atómicas mayores que algunos elementos que se encuentran normalmente en estado sólido. El helio tiene varias propiedades únicas con respecto a otros elementos: tanto su punto de ebullición como el de fusión son menores que los de cualquier otra sustancia conocida; es el único elemento conocido que presenta superfluidez; de la misma manera no puede ser solidificado por enfriamiento bajo condiciones estándar, sino que se convierte en sólido bajo una presión de 25 atm (2500 kPa; 370 psi) y 0,95 K (−272,20 °C; −457.960 °F). Los gases nobles hasta el xenón tienen múltiples isótopos estables. El radón no tiene isótopos estables; su isótopo de mayor duración tiene un periodo de semidesintegración de 3,8 días que puede formar helio y polonio.

El radio atómico de los gases nobles aumenta de un periodo a otro debido al incremento en el número de electrones. El tamaño del átomo se relaciona con varias propiedades. Por ejemplo, el potencial de ionización disminuye a medida que aumenta el radio ya que los electrones de valencia en los átomos más grandes se encuentran más alejados del núcleo y, por lo tanto, no se encuentran ligados tan fuertemente por el átomo. Los gases nobles tienen los mayores potenciales de ionización de cada periodo, lo cual refleja lo estable que es su configuración electrónica y genera su falta de reactividad química. Sin embargo, algunos de los gases nobles más pesados tienen potenciales de ionización lo suficientemente bajos para ser comparables a los de otros elementos y moléculas. El químico Neil Bartlett, intentando crear el compuesto de un gas noble, notó que el potencial de ionización del xenón era similar al de la molécula de oxígeno, por lo que intentó oxidar xenón usando hexafluoruro de platino, un agente oxidante tan fuerte que es capaz de reaccionar con oxígeno. Los gases nobles no pueden aceptar un electrón para formar aniones estables. Esto quiere decir que poseen una afinidad electrónica negativa.

Las propiedades físicas macroscópicas de los gases nobles están determinadas por las débiles fuerzas de Van der Waals que se dan entre átomos. Las fuerzas de atracción aumentan con el tamaño del átomo como un resultado del incremento en la polarizabilidad y el descenso del potencial de ionización. Esto lleva a tendencias grupales sistemáticas. Por ejemplo, a medida que se baja en los grupos de la tabla periódica, el radio atómico y las fuerzas interatómicas aumentan. De igual forma, se adquieren mayores puntos de fusión y de ebullición, entalpía de vaporización y solubilidad. El aumento de densidad se debe al incremento en masa atómica.

Los gases nobles se comportan como gases ideales bajo condiciones normales de presión y temperatura, pero sus tendencias anormales a la ley de los gases ideales proporcionan claves importantes para el estudio de las fuerzas e interacciones moleculares. El potencial de Lennard-Jones, usado frecuentemente para modelar fuerzas intermoleculares, fue deducido en 1924 por John Lennard-Jones a partir de datos experimentales del argón antes de que el desarrollo de la mecánica cuántica proporcionara las herramientas necesarias para entender las fuerzas intermoleculares a partir de primeros principios. El análisis teórico de estas fuerzas se volvió viable debido a que los gases nobles son monoatómicos, y por tanto isótropos (independientes de la dirección).

En los seis primeros periodos de la tabla periódica, los gases nobles son exactamente los miembros del grupo 18 (8A) de la tabla (anteriormente conocido como grupo 0). Sin embargo, esto ya no es cierto en el séptimo periodo (debido a efectos relativistas): el siguiente miembro del grupo 18, el oganesson, probablemente no es tan gas noble. En cambio, el miembro del grupo 14 Flerovio presenta propiedades similares a las de los gases nobles.

Los gases nobles son incoloros, inodoros, insípidos y no inflamables en condiciones normales. Antiguamente se les asignaba el grupo 0 de la tabla periódica porque se creía que tenían una valencia cero, es decir, que sus átomos no se pueden combinar con otros elementos para formar compuestos. Sin embargo, más tarde se descubrió que algunos sí forman compuestos, haciendo que se abandonara esta denominación. Se conoce muy poco sobre las propiedades del miembro más reciente del grupo 18, el oganesson (oganesson). Los gases nobles tienen capas llenas de electrones de valencia. Los electrones de valencia son los electrones que se encuentran más al exterior de los átomos y normalmente son los únicos que participan en los enlaces químicos. Los átomos con capas de valencia llenas de electrones son extremadamente estables y por tanto no tienden a formar enlaces químicos y tienen poca tendencia a ganar o perder electrones. Sin embargo, los gases nobles más pesados, como el radón, están unidos menos firmemente por la fuerza electromagnética que los más ligeros, como el helio, haciendo que sea más fácil retirar electrones exteriores de los gases nobles pesados. Debido a que dicha capa está completa, los gases nobles se pueden utilizar de acuerdo con la notación de configuración electrónica para dar lugar a una "notación de gases nobles". Para ello, primero se escribe el gas noble más cercano que precede al elemento en cuestión, y se continúa la configuración electrónica a partir de ese punto. Por ejemplo, la notación electrónica del carbono es 1s² 2s² 2p², y su notación de gas noble es [He] 2s² 2p². Esta notación hace que resulte más fácil identificar elementos, y es más corta que escribir toda la notación de orbitales atómicos.

Los gases nobles tienen una reactividad extremadamente baja; a pesar de ello, se han formado una gran cantidad de compuestos de gases nobles. No se han formado compuestos neutros en los que el helio y el neón estén presentes en los enlaces químicos (aunque hay pruebas teóricas de algunos compuestos de helio), mientras que el xenón, el kriptón y el argón solo presentan una reactividad baja. La reactividad sigue el orden Ne < He < Ar < Kr < Xe < Rn.

En 1933, Linus Pauling argumentó que los gases nobles más pesados podían formar compuestos con el flúor y el oxígeno. De igual forma, arguyó la existencia del hexafluoruro de kriptón (KrF) y el hexafluoruro de xenón (XeF), y especuló que el XeF podría existir como compuesto inestable, sugiriendo también que el ácido xénico (HXeO) podía formar sales de perxenato. Se ha demostrado que estas predicciones eran generalmente precisas, salvo que actualmente se cree que el XeF es termodinámica y cinéticamente inestable. Los compuestos de xenón son los más numerosos de los compuestos de gas noble que se han formado. La mayoría de ellos tienen el átomo de xenón en el estado de oxidación +2, +4, +6 o +8 unido a átomos muy electronegativos como el flúor o el oxígeno, como en el fluoruro de xenón (XeF), el tetrafluoruro de xenón (XeF), el hexafluoruro de xenón (XeF), el tetraóxido de xenón (XeO) y el perxenato de sodio (NaXeO). Algunos de estos compuestos han sido utilizados en la síntesis química como agentes oxidantes; el XeF, en particular, está disponible comercialmente y se puede utilizar como agente fluorador. En 2007, se habían identificado unos quinientos compuestos de xenón unidos a otros elementos, incluyendo compuestos organoxenones (unidos con carbono), así como xenón unido a nitrógeno, cloro, oro, mercurio y al propio xenón. También se han observado compuestos de xenón unido a boro, hidrógeno, bromo, yodo, berilio, azufre, titanio, cobre y plata, pero solo a temperaturas bajas en matrices de gases nobles, o en "jet streams" de gases nobles.

En teoría, el radón es más reactivo que el xenón, y por tanto debería formar enlaces químicos más fácilmente que el xenón. Sin embargo, debido a la gran radiactividad y la corta semivida de los isótopos del radón, en la práctica solo se han formado unos pocos fluoruros y óxidos de radón. El kriptón es menos reactivo que el xenón, pero se han observado diversos compuestos con el kriptón en el estado de oxidación +2. El difluoruro de kriptón es el más notable y fácil de caracterizar. También se han caracterizado compuestos en que el kriptón forma un enlace único con nitrógeno y oxígeno, pero solo son estables por debajo de −60 °C y −90 °C, respectivamente. Se han observado átomos de kriptón unidos químicamente a otros no metales (hidrógeno, cloro, carbono), así como algunos metales de transición tardíos (cobre, plata, oro), pero solo o bien a temperaturas bajas. Se utilizaron condiciones similares para obtener los primeros pocos compuestos de argón en el 2000, como el fluorohidruro de argón (HArF), y algunos unidos a los metales de transición tardíos. En 2007 no se conocían moléculas neutras estables con átomos de helio o neón con enlaces covalentes.

Los gases nobles, incluyendo el helio, pueden formar iones moleculares estables en fase gaseosa. El más simple es el hidrohelio, HeH, descubierto en 1925. Al estar compuesto por los dos elementos más abundantes del universo, el hidrógeno y el helio, se cree que se da naturalmente en el medio interestelar, aunque aún no ha sido detectado. Además de estos iones, hay muchos excímeros neutros conocidos de estos gases. Hay compuestos como ArF y KrF que solo son estables cuando se encuentran en un estado electrónico excitado, y algunos de ellos se emplean en los láseres de excímeros.

Además de los compuestos en que un átomo de gas noble está implicado en un enlace covalente, los gases nobles también forman compuestos no covalentes. Los clatratos, descritos por primera vez en 1949, consisten en un átomo de gas noble atrapado dentro de cavidades de la estructura cristalina de determinadas sustancias orgánicas e inorgánicas. La condición esencial para que se formen es que los átomos invitados (los del gas noble) deben tener el tamaño adecuado para encajar en las cavidades de la estructura cristalina del huésped. Por ejemplo, el argón, el kriptón y el xenón forman clatratos con la hidroquinona, pero el helio y el neón no, pues son demasiado pequeños o tienen una polarizabilidad insuficiente para ser retenidos. El neón, el argón, el kriptón y el xenón también forman hidratos de clatratos; esto quiere decir que los gases nobles quedan atrapados dentro de la capa de helio de dichos compuestos.

Los gases nobles pueden formar compuestos fulerenos endoédricos, en los que el átomo de gas noble está atrapado dentro de una molécula de fullereno. En 1993, se descubrió que cuando se expone C, una molécula esférica compuesta de 60 átomos de carbono, gases nobles a una presión elevada, se pueden formar complejos como He@C (@ indica que He se encuentra contenido dentro de C, pero que no está unido covalentemente). En 2008 se obtuvieron complejos endohédricos con helio, neón, argón, kriptón y xenón. Estos compuestos se utilizan en el estudio de la estructura y la reactividad de los fulerenos mediante la resonancia magnética nuclear del átomo de gas noble.

Se considera que los compuestos de gases nobles, como el difluoruro de xenón (XeF), son hipervalentes, pues violan la regla del octeto. Se puede explicar los enlaces en estos compuestos con un modelo de tres centros y cuatro electrones. Este modelo, propuesto por primera vez en 1951, considera la unión de tres átomos colineales. Por ejemplo, los enlaces de XeF se describen por un conjunto de tres orbitales moleculares derivadas de los orbitales p de cada átomo. Los enlaces resultan de la combinación de un orbital p de Xe con un orbital p medio lleno de cada átomo de F, resultando en un orbital de enlace lleno, un orbital de enlace no lleno, y un orbital de antienlace. El orbital molecular ocupado más alto se encuentra en los dos átomos terminales. Esto representa una localización de la carga facilitada por la alta electronegatividad del flúor. La química de los gases nobles más pesados, el kriptón y el xenón, está bien determinada. La de los más ligeros, el helio y el argón, aún se encuentra en un estado temprano, mientras que aún no se ha identificado algún compuesto de neón.

La abundancia de los gases nobles en el universo disminuye a medida que aumenta su número atómico. El helio es el elemento más común en el universo después del hidrógeno, con una proporción de masa de aproximadamente el 24 %. La mayoría del helio del universo se formó durante la nucleosíntesis primordial, pero la cantidad de helio aumenta constantemente debido a la fusión de hidrógeno en la nucleosíntesis estelar (proceso realizado mediante reacciones nucleares que tiene su origen en las estrellas durante su proceso evolutivo, y que antecede a una supernova por colapso gravitatorio). La abundancia en la Tierra muestra tendencias diferentes; por ejemplo, el helio es solo el tercer gas noble más abundante de la atmósfera. El motivo es que no hay helio primordial en la atmósfera, ya que debido a la pequeña masa de este átomo, el helio no puede ser retenido por el campo gravitatorio terrestre. El helio de la Tierra deriva de la desintegración alfa de elementos pesados como el uranio o el torio de la corteza terrestre, y tiende a acumularse en yacimientos de gas natural. Por otro lado, la abundancia del argón crece como resultado de la desintegración alfa del potasio-40, que también se encuentra en la corteza terrestre, para formar argón-40, que es el isótopo del argón más abundante de la Tierra a pesar de ser relativamente raro en el sistema solar. Este proceso es la base del método de datación por potasio-argón. El xenón tiene una abundancia relativamente baja en la atmósfera, lo que se ha dado a conocer como el «problema del xenón desaparecido»; una teoría es que el xenón que falta podría estar atrapado en minerales dentro de la corteza terrestre. El radón se forma en la litosfera por la desintegración alfa del radio. Se puede filtrar en edificios a través de los cimientos y acumularse en áreas mal ventiladas. Debido a su gran radiactividad, el radón supone un riesgo significativo para la salud; solo en Estados Unidos, está asociado con unas 21.000 muertes por cáncer de pulmón cada año.

El neón, el argón, el kriptón y el xenón se obtienen a partir del aire utilizando los métodos de licuefacción de gases, para convertir los elementos a un estado líquido, y de destilación fraccionada, para separar las mezclas en sus componentes. El helio se produce generalmente separándolo del gas natural, y el radón se aísla de la desintegración radioactiva de los compuestos de radio. El precio de los gases nobles está influido por su abundancia natural, siendo el argón el más barato y el xenón el más caro. Lo ilustra la tabla de la derecha, con los precios en USD de 2004 por cantidades de laboratorio de cada gas.

Los gases nobles tienen un punto de ebullición y de fusión muy bajos, lo que los hace útiles como refrigerantes criogénicos. En particular, el helio líquido, que hierve a 4,2K, se utiliza para imanes superconductores, como los que se emplean para la imagen por resonancia magnética y la resonancia magnética nuclear. El neón líquido, aunque no llega a temperaturas tan bajas como el helio líquido, también tiene aplicaciones en la criogenia, pues tiene una capacidad de refrigeración más de 40 veces superior a la del helio líquido y más de tres veces superior a la del hidrógeno líquido.

El helio se utiliza como componente de los gases respirables para sustituir al nitrógeno, gracias a su baja solubilidad en fluidos, especialmente en lípidos. Los gases son absorbidos por la sangre y los tejidos corporales cuando hay presión, como en el submarinismo, lo que provoca un efecto anestésico conocido como "mal de profundidad". Debido a su baja solubilidad, entra poco helio en las membranas celulares, y cuando se utiliza helio para sustituir parte de los gases respirables, como en el trimix o el heliox, se consigue una reducción del efecto narcótico del gas en profundidad. La baja solubilidad del helio ofrece más ventajas para el trastorno conocido como enfermedad por descompresión. A menor cantidad de gas disuelto en el cuerpo significa que se forman menos burbujas de gas durante la reducción de la presión durante el ascenso. Otro gas noble, el argón, es considerado la mejor opción como gas de inflación del traje seco en el submarinismo.

Desde el desastre del Hindenburg de 1937, el helio ha sustituido al hidrógeno como gas de sustentación en los dirigibles y globos, gracias a su ligereza e incombustibilidad, pese a una reducción en la flotabilidad de un 8,6 %. En muchas aplicaciones, los gases nobles se utilizan para formar una atmósfera inerte. El argón se utiliza en la síntesis de compuestos sensibles al aire que al mismo tiempo, son sensibles al nitrógeno. El argón sólido también se utiliza para estudiar compuestos muy inestables, como intermedios reactivos, atrapándolos en una matriz inerte a temperaturas muy bajas. El helio es utilizado como medio portador en la cromatografía de gases, como gas de relleno en los termómetros, y en aparatos para medir la radiación, como el contador Geiger y la cámara de burbujas. Tanto el helio como el argón se utilizan habitualmente para proteger arcos de soldadura y el metal base circundante de la atmósfera durante la soldadura y la ablación, así como en otros procesos metalúrgicos y la producción de silicio para la industria de los semiconductores.

Los gases nobles se usan habitualmente para la iluminación debido a su falta de reactividad química. El argón, mezclado con nitrógeno, se utiliza como gas de relleno de las bombillas incandescentes. El kriptón se usa en bombillas de alto rendimiento, que tienen una temperatura de color más elevada y una mayor eficacia, pues reduce la velocidad de evaporación del filamento más que el argón, las lámparas de halógeno, en particular, utilizan kriptón mezclado con pequeñas cantidades de compuestos de yodo o bromo. Los gases nobles lucen con colores característicos cuando se les utiliza en lámparas de descarga, como los faros de neón, que producen un color naranja-rojo. El xenón es utilizado habitualmente en faros de xenón que, debido a su espectro casi continuo que se asemeja a la luz del día, se usan en proyectores de películas y como faros de automóvil.

Los gases nobles se usan en láseres de excímeros, que se basan en moléculas excitadas electrónicamente de vida corta conocidas como excímeros. Los excímeros utilizados en los láseres pueden ser dímeros de gases nobles como Ar , Kr o Xe , o más habitualmente, el gas noble es combinado con un halógeno en excímeros como ArF, KrF, XeF o XeCl. Estos láseres producen una luz ultravioleta que, debido a su longitud de onda corta (193 nm por ArF y 248 nm para KrF), permite una imagen de alta precisión. Los láseres de excímeros tienen muchos usos industriales, médicos y científicos. Se utilizan en la microlitografía y la microfabricación, esenciales para la manufactura de circuitos integrados y por cirugía láser, incluyendo la angioplastia láser y la cirugía ocular. Algunos gases nobles tienen un uso directo en la medicina. A veces se usa el helio para mejorar la facilidad de respiración de los pacientes con asma. El xenón se utiliza como anestésico debido a su alta solubilidad en lípidos, que lo hace más potente que el habitual óxido nitroso, y como es eliminado fácilmente por el cuerpo, permite un restablecimiento más rápido. La captación de imágenes hechas a través de la resonancia magnética nuclear utiliza el xenón en combinación con otros gases. El radón, que es muy radiactivo y solo está disponible en cantidad mínimas, sirve en el tratamiento por radioterapia.

El color de la emisión de descarga del gas depende de varios factores, incluyendo los siguientes:





</doc>
<doc id="15525" url="https://es.wikipedia.org/wiki?curid=15525" title="Yang Liwei">
Yang Liwei

Yang Liwei (杨利伟) (nacido el 21 de junio de 1963) es un piloto de combate chino, y también el primer taikonauta ("astronauta en chino") que China logró enviar al espacio exterior. China es pues el tercer país que concreta esta hazaña, después de la Unión Soviética y de los Estados Unidos.

El cohete, un lanzador Larga Marcha 2F, despegó el día 15 de octubre de 2003 de la base de Jiuquan (Mongolia Interior), en el desierto de Gobi. El aterrizaje tuvo lugar 21 horas después del despegue, en un punto cercano a la base.

Durante ese tiempo, el astronauta a bordo de la cápsula Shenzhou 5 ("nave divina"), dio 14 vueltas a la Tierra a 340 kilómetros de altura.


</doc>
<doc id="15528" url="https://es.wikipedia.org/wiki?curid=15528" title="Nobel">
Nobel

Nobel puede referirse a:

</doc>
<doc id="15533" url="https://es.wikipedia.org/wiki?curid=15533" title="Ácido ascórbico">
Ácido ascórbico

El ácido ascórbico o vitamina C es un cristal incoloro, inodoro, sólido, soluble en agua, con un sabor ácido. Es un ácido orgánico, con propiedades antioxidantes.

En humanos, primates y cobayas, entre otros, la vitamina C (enantiómero L del ácido ascórbico) no se sintetiza, por lo que debe ingerirse a través de los alimentos. Esto se debe a la ausencia de la enzima L-gluconolactona oxidasa, que participa en la ruta del ácido úrico.

El enantiómero L (levógiro) de este ácido comúnmente se conoce como vitamina C. El nombre "ascórbico" proviene del prefijo "a-" ("sin") y del latín "scorbuticus" ("escorbuto"), procede de su propiedad de prevenir y curar el escorbuto.


El ácido ascórbico y sus sales de sodio, potasio y calcio se utilizan de forma general como antioxidantes. Estos compuestos son solubles en agua, por lo que no protegen las grasas de la oxidación. Para este propósito, pueden utilizarse los ésteres del ácido ascórbico solubles en grasas con ácidos grasos de cadena larga (palmitato y estearato de ascorbilo).

Desde mediados del siglo XVIII, ya se había observado que el jugo de limón podía prevenirles a los marineros padecer escorbuto. Al principio, se suponía que las propiedades ácidas eran responsables de este beneficio; sin embargo, pronto se hizo evidente que otros ácidos en la dieta, como el vinagre, no tenían tales beneficios. En 1907, dos médicos noruegos informaron de un compuesto esencial en los alimentos para prevenir la enfermedad, distinto del que impedía el beriberi. Estos médicos estaban investigando enfermedades por deficiencias dietéticas mediante el nuevo modelo animal de cobayas, susceptibles al escorbuto. Al recién descubierto factor alimentario se le llamó finalmente vitamina C.

Entre 1928 y 1932, el equipo de investigación húngaro dirigido por Albert Szent-Györgyi, y el del investigador norteamericano Charles Glen King, identificaron el factor antiescorbútico como una particular y sencilla sustancia química. En la Clínica Mayo, Szent-Györgyi había aislado químicamente el ácido hexurónico a partir de las glándulas suprarrenales de animales; sospechaba que era el factor antiescorbútico pero no podía demostrarlo sin un ensayo biológico. Tal ensayo se llevó a cabo por fin en la Universidad de Pittsburgh, usando conejillos de indias en el laboratorio de King, quien había trabajado en el problema durante años. A finales de 1931, el laboratorio de King obtuvo indirectamente de Szent-Györgyi ácido hexurónico renal y, utilizando su modelo animal, demostró a principios de 1932 que era la vitamina C.

Este fue el último de los compuestos de origen animal; pero luego, en ese mismo año, el grupo de Szent-Györgyi descubrió que la pimienta paprika, una especia común en la dieta de Hungría, era una rica fuente de ácido hexurónico. Envió algunos de los productos químicos, ahora más disponibles, a Walter Norman Haworth, un químico británico experto en el azúcar. En 1933, en colaboración con el entonces director adjunto de Investigación (y posteriormente sir) Edmund Hirst y sus equipos de investigación, Haworth dedujo la estructura correcta y la naturaleza isómerica-óptica de la vitamina C, y en 1934 informó de la primera síntesis de la vitamina. En honor de las propiedades antiescorbúticas del compuesto, Haworth y Szent-Györgyi propusieron entonces para el compuesto el nuevo nombre de «ácido a-escórbico» ("a-scorbic acid"). Finalmente ellos mismos lo nombraron ácido L-ascórbico cuando su estructura fue probada por síntesis.

En 1937, el Premio Nobel de Química le fue otorgado a Haworth por su trabajo en la determinación de la estructura del ácido ascórbico (compartido con Paul Karrer, quien recibió su premio por el trabajo sobre las vitaminas), y el premio de Fisiología o Medicina de ese mismo año fue para Szent-Györgyi por sus estudios sobre las funciones biológicas del ácido L-ascórbico. El médico estadounidense Fred R. Klenner promovió la vitamina C como una cura para muchas enfermedades en la década de 1950 elevando las dosis en gran medida hasta decenas de gramos de vitamina C al día mediante inyección. Desde 1967, otro ganador del premio Nobel Linus Pauling recomienda elevadas dosis de ácido ascórbico (él mismo tomaba 18 gramos al día) como prevención contra el resfriado y el cáncer. Los resultados de Klenner han sido controvertidos por el momento, ya que sus investigaciones no cumplen con los estándares metodológicos modernos.




</doc>
<doc id="15535" url="https://es.wikipedia.org/wiki?curid=15535" title="Nanómetro">
Nanómetro

El nanómetro es la unidad de longitud del Sistema Internacional de Unidades (SI) que equivale a una mil millonésima parte de un metro (1 nm = 10 m) o a la millonésima parte de un milímetro.

El símbolo del nanómetro es nm.

El nombre combina el prefijo "nano" (del griego , ', «enano») con la unidad "metro" (del griego , ', «unidad de medida»).

Puede escribirse en notación científica como 1 nm = 10 m y es simplemente formula_1 metros.

Un nanómetro equivale a 10 ángstroms.

Recientemente la unidad ha cobrado notoriedad en el estudio de la nanotecnología, área que estudia materiales que poseen dimensiones de unos pocos nanómetros.

Se usa para describir los tamaños en las sucesivas generaciones de la industria de los semiconductores y microprocesadores.

El nanómetro se usa para expresar dimensiones en la escala atómica:

También se utiliza para medir la longitud de onda de la radiación ultravioleta, radiación infrarroja y la luz.
La luz visible va desde 400 a 700 nm.

Los pulmones humanos solo pueden retirar partículas superiores a 200 nanómetros. Las partículas de tamaño inferior pueden llegar a cualquier parte del cuerpo y producir daños y tumores.

El diámetro del cabello humano va de 70 µm (70 000 nm) a 80 µm (80 000 nm).




</doc>
<doc id="15536" url="https://es.wikipedia.org/wiki?curid=15536" title="TAC">
TAC

TAC puede referirse a:










</doc>
<doc id="15550" url="https://es.wikipedia.org/wiki?curid=15550" title="Richard Nixon">
Richard Nixon

Richard Milhous Nixon (Yorba Linda, California; 9 de enero de 1913-Nueva York, 22 de abril de 1994) fue el presidente de los Estados Unidos entre 1969 y 1974, año en que se convirtió en el único presidente en dimitir del cargo. Anteriormente, Nixon había sido vicepresidente de los Estados Unidos durante la presidencia de Dwight D. Eisenhower de 1953 a 1961 y antes de ello miembro en la Cámara de Representantes de Estados Unidos (por el 12.º distrito de California) y del Senado de Estados Unidos (por California).

Nixon nació en Yorba Linda, California. Después de completar su licenciatura de estudios en Whittier College se graduó en leyes en la Duke University School en 1937 y regresó a California a su práctica. Él y su esposa Pat se mudaron a Washington en 1942 para trabajar con el Gobierno Federal. Trabajó posteriormente en el servicio activo de la Marina de los Estados Unidos durante la Segunda Guerra Mundial. Nixon fue elegido a la Casa de los Representantes en 1946 y al Senado en 1950. Su búsqueda del Caso Hiss fortaleció su reputación como un líder anti-comunista y lo elevó a una importancia nacional. Fue compañero de la fórmula electoral con Dwight D. Eisenhower en la nominación presidencial por el partido republicano en la elección de 1952. Nixon permaneció 8 años como vicepresidente, siendo el segundo vicepresidente más joven en la historia de EEUU, a los 40 años. No tuvo el éxito deseado en la campaña presidencial de 1960, siendo derrotado por el candidato demócrata John F. Kennedy, además de perder la elección para gobernador de California ante Pat Brown en 1962. En 1968, volvió otra vez a la carrera presidencial y fue elegido, derrotando al vicepresidente titular Hubert Humphrey.

Nixon terminó la intervención de los Estados Unidos en la Guerra de Vietnam en 1973 y trajo de regreso a casa a prisioneros de guerra (POW) y suspendió el servicio militar. En 1972 visitó a la República Popular de China para el inicio eventual de relaciones diplomáticas entre las dos naciones y el haber iniciado la detención de los Misiles Antibalísticos con la firma de un tratado con la Unión Soviética ese mismo año. Su administración generalmente transfirió el poder de Washington D.C. a los estados. Congeló los salarios y impuso el control de precios por noventa días, suspendiendo el patrón oro y transformando al dólar estadounidense en una moneda fiduciaria. A nivel social obligó a la integración racial en las escuelas sureñas, estableciendo la Agencia de Protección al Medio Ambiente y el inicio de la Guerra contra el Cáncer. Nixon también precedió el alunizaje de la Misión Apolo 11, con lo cual terminaba la carrera espacial. Fue reelecto en una de las más disputadas campañas electorales en la historia de los Estados Unidos, en 1972 cuando derrotó a George McGovern. 

En su segundo período presidencial, Nixon ordenó un puente aéreo para reabastecer las pérdidas israelíes en la Guerra del Yom Kippur, resultando en el reinicio de la paz en el Medio Oriente y de la crisis del petróleo en casa. La administración de Nixon patrocinó el golpe de estado en Chile contra el gobierno socialista del presidente Salvador Allende apoyando al general Augusto Pinochet a subir al poder pero un año antes, ocurrió un escándalo en el Complejo Watergate el cual empezó a crecer, costando a Nixon mucho apoyo político. El 9 de agosto de 1974 aceptó su participación por lo cual renunció a la presidencia siendo la única vez que un presidente de los Estados Unidos ha renunciado. Después de dicha renuncia le fue otorgado un controvertido perdón por su sucesor Gerald Ford. A los 20 años de su retiro, Nixon escribió nueve libros y emprendió muchos viajes al extranjero, ayudando a la rehabilitación de su imagen como la de un antiguo hombre de estado y experto mundial en asuntos exteriores. Sufrió un ataque cerebral extenso el 18 de abril de 1994 y falleció cuatro días después a la edad de 81 años.

Nixon nació en Yorba Linda (California). Tras completar sus estudios de pregrado en Whittier College, se graduó en la Escuela de Derecho de la Universidad de Duke en Carolina del Norte y regresó a California para ejercer la abogacía. Se mudó a Washington D. C. para trabajar para el gobierno federal en 1942 junto a su mujer, Pat Nixon. En esta época, sirvió activamente en la Reserva Marina de Estados Unidos durante la Segunda Guerra Mundial. Nixon fue elegido a la Cámara de Representantes en 1946 y al Senado en 1950, donde se consolidó como líder anticomunista tras el papel clave que desempeñó en el caso de Alger Hiss. Fue el candidato a la Vicepresidencia de Estados Unidos en las elecciones de 1952 por el Partido Republicano. Ejerció como vicepresidente durante ocho años. Posteriormente, libró una campaña presidencial sin éxito en 1960, perdiendo contra John F. Kennedy, y más tarde perdió las elecciones para gobernador de California de 1962. Nixon fue elegido Presidente en las elecciones presidenciales de Estados Unidos de 1968, derrotando a Hubert Humphrey. 
Nixon sigue siendo un personaje de gran interés para los historiadores.

Nació en el seno de una familia de agricultores, metodista el padre, y cuáquera la madre, ambos de origen humilde. El padre se convirtió al cuaquerismo tras la boda, después de haber servido en la Armada de los Estados Unidos durante la Primera Guerra Mundial. Pronto se trasladaron a la localidad californiana de Whittier cuando el joven Richard tenía nueve años. Allí alternó sus estudios de primaria con su trabajo en la tienda de comestibles y en la gasolinera que regentaban sus padres. Se graduó en 1934 por el Whittier College de California, a 19 km de Los Ángeles, y en 1937 por la Duke University Law School.

Alistado en la Marina en 1942, sirvió en el Pacífico sur durante la Segunda Guerra Mundial.
Nixon estaba exento de realizar el servicio militar por dos motivos, ser cuáquero y por su trabajo en la Oficina de Administración de Precios, pero decidió no acogerse a ninguno de ellos y entró al servicio de la Armada de Estados Unidos en agosto de 1942. Fue entrenado en la Estación Naval Aérea de Quonset Point (Rhode Island), asignado a la Estación Naval Aérea de Ottumwa, Iowa, durante siete meses y reasignado como controlador de pasajeros aéreos navales del Comando de Transporte Aéreo de combate del Pacífico Sur, apoyando la logística de las operaciones del Teatro del Pacífico suroriental. Tras solicitar tareas que presentaran mayores desafíos, se le fueron asignando unidades en comando. Nixon regresó a Estados Unidos con dos medallas (aunque jamás estuvo en combate) y una recomendación, por lo que pasó a ser el Oficial administrativo de la Estación Naval Aérea Alameda. En enero de 1945, fue transferido al Bureau of Aeronautics en Filadelfia para ayudar a negociar la conclusión de los contratos de guerra. Además, ayudó a revisar los documentos capturados a nazis y japoneses. Recibió por ello otra carta de recomendación, esta vez del Secretario de la Marina James Forrestal. En octubre de 1945, fue promovido a teniente comandante y renunció a su comisión de servicios el día de año nuevo de 1946.

Allen Dulles llegó a un acuerdo con el joven oficial de la marina reclutado por Prescott Bush, por un anuncio de periódico en 1941, que estaba revisando los archivos de la Konti. Nixon le ayudaría a enterrar los archivos de la Konti. Como retribución, Allen Dulles «ayudó a financiar la primera campaña parlamentaria de Nixon contra Jerry Voorhis».

En las elecciones de noviembre de 1946, Nixon fue elegido Representante (diputado) republicano a la Cámara de Representantes del Congreso de los Estados Unidos por el 12.º distrito congresional de California (un distrito que para esa época ocupaba territorio del Condado de Los Ángeles) para el período 1947-1949; fue reelegido para un segundo período (1949-1951), pero faltando menos de dos meses para concluir ese segundo mandato renunciaría para ser senador. En 1948 y 1949 se hace famoso como miembro del Comité de Actividades Antiamericanas durante la investigación del caso Alger Hiss. Su forma de actuar en el caso le permitió a Richard Nixon ser elegido para elaborar, conjuntamente con otros representantes, el Plan Marshall de ayuda económica a la Europa de posguerra. En las elecciones de noviembre de 1950 fue elegido senador por California al Senado de los Estados Unidos para el período 1951-1957.

Para obtener más información sobre las campañas electorales de Nixon en el Congreso, vea las elecciones de 1946 distritos electorales de California y la elección del Senado de los Estados Unidos de 1950 en California .

En 1945, los republicanos en el distrito 12 del Congreso de California se vieron frustrados por su incapacidad para derrotar al congresista demócrata Jerry Voorhis y buscaron un candidato por consenso que realizara una fuerte campaña contra él. Formaron un "Comité de los 100" para decidir sobre un candidato con la esperanza de evitar las distensiones internas que previamente habían conducido a las victorias de Voorhis. Después de que el comité no pudo atraer a candidatos de alto perfil, Herman Perry, gerente de la sucursal del Whittier's Bank of America, sugirió a Nixon, un amigo de la familia con quien había servido en el Consejo de Administración de Whittier College antes de la guerra. Perry le escribió a Nixon en Baltimore. Después de una noche de conversaciones emocionadas entre los Nixons, el oficial naval respondió a Perry con entusiasmo. Nixon voló a California y fue seleccionado por el comité. Cuando dejó la Armada a principios de 1946, Nixon y su esposa regresaron a Whittier, donde Nixon comenzó un año de campaña intensiva. [49] [50] Sostuvo que Voorhis había sido ineficaz como congresista y sugirió que el respaldo de Voorhis por un grupo vinculado a los comunistas significaba que Voorhis debía tener puntos de vista radicales. [51] Nixon ganó la elección, recibiendo 65,586 votos contra los 49,994 de Voorhis. [52]

En el Congreso, Nixon apoyó la Ley Taft-Hartley de 1947, una ley federal que controla las actividades y el poder de los sindicatos, y formó parte del Comité de Educación y Trabajo. Formó parte del Comité Herter, que viajó a Europa para informar sobre la necesidad de ayuda externa de los Estados Unidos. Nixon era el miembro más joven del comité y el único occidental.[53] La defensa por parte de los miembros del Comité Herter, incluyendo Nixon, condujo a la aprobación del Congreso del Plan Marshall.[54]

En sus memorias, Nixon cuenta que se unió al Comité de Actividades No Americanas (HUAC) de la Cámara de Representantes "a fines de 1947". Sin embargo, él ya era miembro de la HUAC a principios de febrero de 1947, cuando escuchó a "Enemy Number One" Gerhard Eisler y su hermana Ruth Fischer. El 18 de febrero de 1947, Nixon se refirió a la beligerancia de Eisler hacia HUAC en su discurso inaugural ante la Cámara. También a principios de febrero de 1947, el representante de los Estados Unidos, Charles J. Kersten, le presentó al padre John Francis Cronin en Baltimore, quien compartió con Nixon su documento de 1945 de circulación privada "El problema del comunismo estadounidense en 1945" [55] con mucha información de William C. del FBI (quien en 1961 encabezaría la inteligencia doméstica bajo J. Edgar Hoover ).[56]

En mayo de 1948, Nixon co-patrocinó un " Proyecto de ley de Mundt-Nixon " para implementar "un nuevo enfoque para el complicado problema de la subversión comunista interna ... Proporcionó el registro de todos los miembros del Partido Comunista y requirió una declaración de la fuente de todo el material impreso y emitido por las organizaciones que se consideraron frentes comunistas ". Se desempeñó como gerente de piso para el Partido Republicano. El 19 de mayo de 1948, el proyecto de ley fue aprobado por la Cámara de Representantes por 319 a 58, pero no fue aprobado por el Senado. [57] (La Biblioteca Nixon cita el pasaje de este proyecto de ley como la primera victoria significativa de Nixon en el Congreso. [58] )

Nixon ganó atención nacional por primera vez en agosto de 1948 cuando su persistencia como miembro de HUAC ayudó a romper el caso de espionaje de Alger Hiss. Si bien muchos dudaron de las afirmaciones de Whittaker Chambers de que Hiss, un exfuncionario del Departamento de Estado, había sido un espía soviético, Nixon creyó que eran verdad y presionó para que el comité continuara su investigación. Bajo demanda por difamación presentada por Hiss. Chambers presentó documentos que corroboraban sus acusaciones. Estas incluían copias en papel y microfilm que Chambers entregó a los investigadores de la Cámara de Representantes después de haberlas escondido durante la noche en un campo; se hicieron conocidos como los "Papeles de calabaza" [59] Hiss fue condenado por perjurio en 1950 por negar bajo juramento que hubiera pasado documentos a las cámaras.[60] En 1948, Nixon se presentó con éxito como candidato en su distrito, ganó las dos primarias principales del partido[61] y fue reelegido cómodamente.[62]

En 1949, Nixon comenzó a considerar postularse para el Senado de los Estados Unidos contra el titular demócrata, Sheridan Downey, [63] y entró en la carrera en noviembre. [64] Downey, Enfrenta a una batalla amarga primaria con la representante Helen Gahagan Douglas, anunció su retirada en marzo de 1950. [65] Nixon y Douglas ganó las elecciones primarias [66] y se involucraron en una campaña en litigio donde la actual guerra de Corea fue un problema importante. [67] Nixon intentó centrar la atención en el récord de votación liberal de Douglas. Como parte de ese esfuerzo, una " Hoja Rosada" fue distribuida por la campaña de Nixon, lo que sugiere que, dado que el historial de votaciones de Douglas fue similar al del congresista de Nueva York Vito Marcantonio (que algunos creen que es comunista), sus opiniones políticas deben ser casi idénticas. [68] Nixon ganó la elección por casi veinte puntos porcentuales. [69] Durante esta campaña, los oponentes de Nixon lo llamaron "Tricky Dick" por sus tácticas de campaña.[70]

En el Senado, Nixon tomó una posición prominente al oponerse al comunismo global, viajando con frecuencia y hablando en contra de él.[71] Mantuvo relaciones amistosas con su compañero anti-comunista, el polémico senador de Wisconsin Joseph McCarthy, pero tuvo cuidado de mantener cierta distancia entre él y las acusaciones de McCarthy.[72] Nixon también criticó la gestión de la Guerra de Corea llevada a cabo por el presidente Harry S. Truman.[71] Apoyó la condición de estados de Alaska y Hawái, votó a favor de los derechos civiles de las minorías y apoyó la ayuda federal en casos de desastre para la India y Yugoslavia. [73]Votó en contra de los controles de precios y otras restricciones monetarias, los beneficios para los inmigrantes ilegales y el poder público. [73]

El general Dwight D. Eisenhower fue nominado para presidente por el partido Republicano en 1952. No tenía fuerte preferencia para un candidato para la vicepresidente, pero las oficinas principales y los oficiales del partido Republicano se reunieron en una sala llena de humo y recomendaron a Nixon al general, quien aceptó la selección del senador. La juventud de Nixon (tenía 39 años), su postura ante el comunismo y su base política en California (uno de los más grandes estados en donde todos ven como voto ganador para los líderes). Entre los candidatos considerados contra Nixon estaba el senador de Ohio Robert A. Taft, el gobernador de New Jersey Alfred Driscoll y el senador por Illinois Everett Dirksen. En los preámbulos de la campaña, Eisenhower habló de sus planes para el país, dejando la campaña negativa a su compañero de fórmula presidencial. 
A mediados de septiembre, el boleto Republicano mostró una mayor crisis. Los medios reportaban que Nixon tenía un fondo político mantenido por sus patrocinadores, que le reembolsaba sus gastos políticos. Con un fondo que no es ilegal, expuso a Nixon a reclamaciones de posible conflicto de intereses. Con la presión construida para Eisenhower. Nixon tuvo que resignar su boleto de participación cuando el senador fue a la televisión a pronunciar su discurso dirigido a la nación el 23 de septiembre de 1952. El discurso, más tarde denominada como la Discurso de Checkers, fue escuchada por 60 millones de estadounidenses, siendo esta la audiencia más grande en la televisión. Nixon defendió emocionalmente que su fondo no era un secreto, que no tenía donadores que recibieran favores especiales. Se describió como un hombre modesto (su esposa no tenía un abrigo de visón) y era un patriota. La charla es recordada como un regalo que Nixon había recibido, pero del cual no quería devolver: "un pequeño perro cocker spaniel entregado en un viaje a Texas. Nuestra hijita Tricia, de 6 años, le ha llamado Checkers. La charla provocó un gran apoyo para Nixon. Eisenhower decidió retenerlo con lo cual salieron victoriosos en la elección de noviembre.

Eisenhower le dio a Nixon más responsabilidades durante su periodo como vicepresidente de las que se habían dado antes a otros vicepresidentes. Nixon asistió a las reuniones del Gabinete Ciudadano de Seguridad Nacional, conociendo y charlando cuando Eisenhower estaba ausente. En el viaje de 1953 hacia el Far West hubo un exitoso crecimiento hacia la buena voluntad en los Estados Unidos promoviendo Nixon a la región como un centro industrial. Visitó Saigón y Hanoi todavía como colonia francesa (Indochina). A su regreso a los Estados Unidos a fines de 1953, Nixon incrementó su tiempo en las relaciones extranjeras.

Presidió la mayor parte de las reuniones del Gobierno y de los líderes del Congreso, a la par que asumió en tres ocasiones (1955, 1956 y 1957) las funciones presidenciales debido a la crónica dolencia cardíaca que padecía el presidente. Destacó como embajador extraordinario de su país por todo el mundo, en calidad de lo cual visitó un total de 55 estados, incluida la Unión Soviética.

En mayo de 1958 durante una visita a Caracas, Venezuela, su vehículo fue atacado a pedradas por una multitud enardecida. Relatando el episodio en su obra "Seis Crisis", Nixon afirmó que se salvó milagrosamente.

Permaneció como vicepresidente con Eisenhower durante todo el tiempo que este fue , ocho años en total, pues fueron reelegidos en 1956, hasta enero de 1961.

Finalizado el segundo mandato presidencial de Ike Eisenhower, Nixon consiguió que el partido republicano le eligiera candidato a presidente en 1960. Sin embargo John Fitzgerald Kennedy, candidato del partido demócrata, con el que mantuvo cuatro debates presidenciales televisados (los primeros en la historia de ese país), lo venció en las elecciones presidenciales de noviembre de 1960 por un estrecho margen de votos. En 1962 intentó ser elegido gobernador de California, sin éxito.

En 1960, Nixon lanzó su primera campaña para Presidente de los Estados Unidos. Encaró poca oposición en las elecciones primarias de los republicanos y eligió al exsenador por Massachusetts Henry Cabot Lodge Jr. como su compañero de fórmula. Su oponente demócrata en la carrera presidencial fue John F. Kennedy. La campaña de Nixon estaba basada en su experiencia, pero Kennedy llamaba a la nueva sangre y reclamaba que la administración de Eisenhower-Nixon habían permitido que la Unión Soviética sobrepasara a los Estados Unidos en los misiles balísticos (Missile gap). Un nuevo medio de política fue introducido en la campaña: los debates presidenciales televisados. El primero de cuatro debates, Nixon apareció pálido, sombrío y con una barba naciente, en un agudo contraste con el fotogénico Kennedy que parecía un actor de cine. En su presentación en el debate, Nixon fue percibido en la opinión televisiva como un mediocre, mientras mucha gente lo escuchaba por la radio daban a Nixon como ganador. Nixon perdió la elección con Kennedy por solo 112,827 votos (0.2%) del voto popular.
En donde hubo cargos de fraude en las votaciones fue en Texas y en Illinois, ambos estados ganados por Kennedy. Nixon se negó a considerar la impugnación de la elección, centrando una controversia que hizo que disminuyera la credibilidad de los Estados Unidos ante los ojos del mundo y el incierto daño a los intereses estadounidenses. Al final de todo, dejó la oficina como vicepresidente en enero de 1961. Nixon y su familia regresaron a California, donde practicó leyes y escribió un libro, "Six Crises", donde incluía la cobertura del caso Hiss, el ataque cardíaco a Eisenhower y la Crisis de los Fondos, mientras se había resuelto por el discurso de las damas.

Los lectores republicanos tanto locales como nacionales alentaron a Nixon contra Pat Brown para Gobernador de California en la elección de 1962. Después de la renuencia inicial, Nixon entró a la carrera. La campaña tuvo nubarrones por la sospecha pública que Nixon vería esto como una oficina para otra carrera presidencial. Hubo algunas oposiciones en el partido por lo cual finalmente perdió el interés en la campaña para ser Gobernador de California. Nixon tenía la esperanza que en la carrera por la elección su estatus fuera un liderazgo activo para la nación de los políticos Republicanos y asegurar como el mayor jugador en la política nacional. Pero perdió con Brown por un poco menos del 5% en puntos, siendo su derrota ampliamente creída que pudo haber sido el fin de su carrera política. En una conferencia improvisada a la mañana siguiente de la elección, Nixon culpó a los medios en favorecer a su oponente, diciendo: "No tendrá Nixon a patear más porque, caballeros, esta es mi última conferencia de prensa". La derrota en California fue destacada el 11 de noviembre de 1962 en el episodio de Howard R. Smith en la cadena ABC: Noticias y comentarios. titulado "El obituario político de Richard M. Nixon" Alger Hiss apareció en el programa y muchos miembros de la política se quejaron que era un linchamiento político dado a un convicto al aire, un ataque a un exvicepresidente. La ira llevó a Smith y su programa del aire a originar que la simpatía pública creciera hacia Nixon. 

En 1963 la familia de Nixon viajó a Europa, donde dio conferencias de prensa y se entrevistó con los líderes de los países que visitó. La familia se mudó a Nueva York, donde Nixon tenía un compañero en un equipo de abogados, bajo la firma Nixon, Mudge, Rose, Guthrie & Alexander. Cuando anunció su campaña en California, Nixon se comprometió que no competiría para la presidencia en 1964, debido a lo difícil que creía derrotar a Kennedy pero después de su asesinato a su sucesor Lyndon B. Johnson.

En 1964 apoyó al Senador por Arizona Barry Goldwater para la nominación republicana para ser presidente de los Estados Unidos. Cuando Goldwater ganó la nominación, Nixon fue seleccionado para participar en la convención. Aunque pensó a diferencia de Goldwater que le gustaría ganar, Nixon lo acompañó y fue leal. La elección fue un desastre para los Republicanos. Goldwater perdió en forma aplastante ante Johnson y con pérdidas importantes para el partido en los escaños del Congreso y entre los gobernadores estatales. 

Nixon fue uno de los pocos líderes republicanos que no fue culpado por los desastrosos resultados y buscó la reconstrucción en las elecciones del Congreso en 1966. Hizo campaña con los republicanos, con la visión puesta en recuperar los escaños perdidos en la aplastante derrota infringida por Johnson, recibiendo el crédito y la ayuda de los Republicanos habiendo mejores ganancias ese año.

Artículo principal: La campaña presidencial de Richard Nixon en los Estados Unidos en 1968
A fines de 1967, Nixon les dijo a su familia que planeaba competir en la elección presidencia por segunda ocasión. Esto no alegró a su esposa Pat Nixon por su vida privada (estaba embarazada y tenía que dar un discurso al comité de damas) pero soportaba las ambiciones políticas de su esposo. Nixon creía que con los Demócratas entregados y distraídos con la Guerra de Vietnam, con todas las protestas emitidas, un Republicano tenía una buena oportunidad de ganar aunque creía que esta elección iba a ser tan cerrada como la de 1960.

A principios del año 1968 Nixon presentó su pre-candidatura presidencial para competir por la nominación oficial del Partido Republicano. Los principales rivales internos de Nixon en la carrera por la candidatura presidencial eran Nelson Rockefeller (en ese momento gobernador de Nueva York) y Ronald Reagan (entonces gobernador de California). Rockefeller representaba al ala liberal republicana (centroizquierdista), Reagan al ala conservadora republicana (derechista) y Nixon al sector o ala moderada republicana (centrista). Nixon tenía una maquinaria electoral más poderosa y derrotó con relativa facilidad a Reagan y a Rockefeller en las elecciones primarias internas de la mayoría de los Estados; y por eso, cuando se reunió la Convención Nacional Republicana en Miami Beach el 5 de agosto de 1968, Nixon obtuvo en la primera votación el voto de 692 delegados contra 277 de Rockefeller y 182 de Reagan, quedando elegido como candidato presidencial del Partido Republicano.
Una de las más tumultuosas elecciones primarias, en este proceso electoral, comenzó precisamente cuando se inició la Ofensiva del Tet en enero de 1968. El presidente Johnson se retiró de la candidatura en marzo, después de una pobre elección primaria no esperada en New Hampshire. En junio, el Senador Robert F. Kennedy, el candidato Demócrata, fue asesinado momento después de su victoria en las primarias de California. Por el lado Republicano, su principal oposición fue el Gobernador de Míchigan, George Romney, así como El Gobernador de Nueva York, Nelson Rockefeller y el Gobernador por California, Ronald Reagan, quienes mantenían la esperanza de su nominación en una convención negociada. Nixon aseguró su nominación en la primera boleta. Seleccionó al gobernador de Maryland, Spiro Agnew como su compañero de fórmula, una elección en la cual creía Nixon daría la unidad del partido, atractivo para ambos: los moderados del Noroeste y los desafectados del Suroeste con los Demócratas.

Por su parte, el Partido Demócrata elige en la Convención Nacional Demócrata de 1968 al entonces vicepresidente de los Estados Unidos, Hubert Humphrey como su candidato presidencial. Pero un sector del partido, el de los demócratas racistas de los Estados del sur, se separó del partido molesto por las políticas a favor de la igualdad entre blancos y negros que impulsaban Humphrey y el presidente Lyndon B. Johnson. Este grupo de demócratas disidentes fundaron un nuevo partido llamado el American Independent Party; y lanzaron la candidatura presidencial de George Wallace, para ese entonces exgobernador de Alabama y conocido racista el cual había tenido fricciones importantes durante la Presidencia de John F. Kennedy cuando se implementaron los derechos civiles y la entrada de afroamericanos a la Universidad.
Nixon inicia su campaña para la presidencia con una nueva imagen que le presentaba como un elemento más moderado; pero al mismo tiempo prometía ""Ley y Orden"" (el lema principal de su campaña) para restablecer el orden en una sociedad sacudida por violentos disturbios y protestas contra la guerra y contra la segregación racial. Esta promesa era hecha en términos ambiguos en un esfuerzo para atraer el voto de los blancos del sur. Por eso la campaña de Nixon reclutó a antiguos demócratas sureños que se habían pasado al Partido Republicano, descontentos con las medidas a favor de los derechos civiles promovidas por Johnson. Pero el esfuerzo de conquistar el sur se estrellaba con el mensaje mucho más radical de Wallace, que resultaba más atractivo para los racistas. Sus discursos favorecían la segregación racial.

Pero a la larga la estrategia moderada en el norte y el oeste, y más derechista en el sur le dio resultado a Nixon; y la Guerra de Vietnam fue un grave obstáculo para Humphrey. El 5 de noviembre de 1968 se celebraron las elecciones presidenciales; Nixon obtuvo 31 783 783 votos populares, equivalentes al 43,42 % de los sufragios emitidos; Humphrey obtuvo 31 271 839 votos populares, que equivalían al 42,72 % de los votos; y Wallace obtuvo 9 901 118 votos populares, equivalentes al 13,53 % de los sufragios. Otros candidatos minoritarios obtuvieron en total 243 258 votos populares, un 0,33 % de los votos, por lo que Nixon derrotó a Humphrey en el sufragio popular por una diferencia de un poco más de 500 000 votos; pero como Nixon ganó en 32 estados, Humphrey en 13 estados (y el Distrito de Columbia) y Wallace en 5 estados, al final Nixon tuvo 301 electores en el Colegio Electoral, contra 191 de Humphrey y 46 de Wallace.Nixon fue el presidente electo y tomó posesión el 20 de enero de 1969. Había heredado grandes problemas siendo el principal la Guerra de Vietnam.

Casi un año después de ser elegido, el 3 de noviembre de 1969, se dirige a la nación estadounidense en uno de los mensajes presidenciales más famosos de la historia, el denominado discurso de la "Mayoría Silenciosa".

Junto con el secretario de estado Kissinger, redefinió el papel de Estados Unidos en el escenario mundial. Se realizó una retirada gradual de los 500 000 soldados estadounidenses que combatían en Vietnam del Sur, aunque la retirada se prolongó durante cuatro años. Su mayor logro fue su aproximación y apertura de relaciones con la República Popular de China. Nixon también viajó a Moscú para negociar el primer paso para un acuerdo sobre limitación de armas estratégicas. En Oriente Próximo, estableció relaciones con Egipto manteniendo los compromisos con Israel. Las políticas de Nixon se centraron en combatir la inflación. En el momento en que Nixon asumió el cargo en 1969, la inflación fue del 4,7 por ciento, la tasa más alta desde la Guerra de Corea. Con la inflación sin resolver en agosto de 1971, y un año electoral que se avecina, Nixon convocó una cumbre de sus asesores económicos en Camp David. Luego anunció controles temporales de precios y salarios, permitió al dólar flotar frente a otras monedas, y puso fin a la convertibilidad del dólar en oro.

Después de ganar la reelección, la inflación volvió a acelerarse, Nixon volvió a imponer controles de precios en junio de 1973. Los controles de precios se hizo impopular con el público y los empresarios, que vio a los poderosos sindicatos como preferibles a la burocracia tabla de precios Los controles produjeron la escasez de alimentos, la carne desapareció de las tiendas de comestibles y los agricultores ahogaron pollos en lugar de venderlos a pérdida a pesar de la falta de control de la inflación, los controles fueron poco a poco habrá terminado, y el 30 de abril de 1974, su autorización legal caducado.El apoyo prestado a Israel en la Guerra de Yom Kipur llevó a un boicot árabe al petróleo, lo que resultó en la crisis del petróleo de 1973. El embargo causó escasez de gasolina y el racionamiento en los Estados Unidos a finales de 1973 y disparó aún más la inflación. La crisis provocó que las condiciones de vida se volvieran muy adversas para los desempleados, los grupos sociales marginados, algunos trabajadores de mayor edad, y cada vez más, para los trabajadores más jóvenes. Las escuelas y oficinas tuvieron que cerrar a menudo para ahorrar el combustible de la calefacción, las fábricas tuvieron que reducir la producción y despedir trabajadores. Esta escasez llevó al racionamiento de gasolina por lo que los automovilistas se enfrentaron a largas colas en las gasolineras.

El Nixon Shock fue una serie de medidas económicas llevadas a cabo por Estados Unidos el presidente Richard Nixon en 1971, el más importante de los cuales fue la cancelación unilateral de la directa convertibilidad del dólar de los Estados Unidos para el oro. En ese momento, los EE. UU. también tenía una tasa de desempleo del 6,1 % (agosto de 1971) y una inflación del 5,84 % (1971), las cifras más altas desde la Segunda Guerra.la principal influencia de la experiencia de la recesión 1973 llegó en la forma del concepto de estanflación, es decir, la inflación durante un período de recesión

En 1972 Nixon era un presidente muy popular, por lo que su reelección parecía fácil; aun así dos representantes republicanos, uno liberal (izquierdista) y otro conservador (derechista) compitieron contra él por la candidatura oficial del partido republicano. Pero Nixon ganó las elecciones primarias con facilidad y por ello, cuando la Convención Nacional Republicana se reunió nuevamente en Miami Beach del 21 de agosto hasta el 23 de agosto de 1972, Nixon fue elegido candidato del partido con los votos de 1347 delegados contra un solo voto para su rival izquierdista y ninguno para el conservador.

El Partido Demócrata, por su parte, tuvo que elegir entre 10 precandidatos en una dura elección interna; finalmente el elegido fue George McGovern, para ese entonces senador por el estado de Dakota del Sur.

McGovern fue sin duda el candidato presidencial demócrata de ideas más cercanas al socialismo en toda la historia de los Estados Unidos hasta ese momento. Su plataforma electoral estaba muy orientada al gasto social, con propuestas para aumentar los impuestos, el gasto público y la burocracia. Igualmente su programa pretendía aumentar el tamaño del Estado y su intervención en la economía; e iniciar un desarme unilateral en plena Guerra Fría. Por eso los republicanos lo atacaron presentándolo como un radical peligroso y "medio loco"; mientras la popularidad de Nixon iba en aumento gracias a la buena situación económica.

El 7 de noviembre de 1972 se celebraron las elecciones presidenciales; Nixon obtuvo 47 168 710 votos populares, que equivalían al 60,67 % de los sufragios populares emitidos; McGovern obtuvo 29 173 222 votos populares, equivalentes al 37,52 % de los sufragios; John Schmitz, del American Independent Party, obtuvo 1 100 868 votos populares que equivalían a un 1,42 %; y otros candidatos minoritarios sumaron 301 227 votos populares, un 0,39 %. Nixon ganó en 49 estados y McGovern en apenas un estado (Massachusetts) y el Distrito de Columbia; en el Colegio Electoral, Nixon obtuvo 520 Electores contra 17 para McGovern y uno para un candidato minoritario. Fue una de las victorias electorales más aplastantes de la historia estadounidense.El 6 de octubre de 1973, Israel entra en guerra contra una coalición árabe dirigido por Egipto y Siria. Israel en lo que se conoce como la Guerra de Yom Kippur. Israel sufrió grandes pérdidas y Nixon ordenó un puente aéreo para reabastecer las pérdidas israelíes. Debido a la victoria de Israel fue en gran parte debido al apoyo de Estados Unidos, los países de la OPEP árabe respondieron al negarse a vender petróleo crudo a los EE. UU., lo que resultó en la crisis del petróleo de 1973. El embargo causó escasez de gasolina y el racionamiento en los Estados Unidos a finales de 1973 y disparó aún más la inflación. Especialmente en Estados Unidos, la crisis provocó que las condiciones de vida se volvieran muy adversas para los desempleados, los grupos sociales marginados, algunos trabajadores de mayor edad, y cada vez más, para los trabajadores más jóvenes. Las escuelas y oficinas en EE. UU. tuvieron que cerrar a menudo para ahorrar el combustible de la calefacción, y las fábricas tuvieron que reducir la producción y despedir trabajadores. La crisis se agravó aún más a causa del control de los precios en Estados Unidos, que limitó el precio del "petróleo antiguo" (ya descubierto), mientras permitía que el petróleo recién descubierto pudiera ser vendido a un precio más elevado, lo que supuso una retirada del petróleo antiguo del mercado y una escasez artificial. El objetivo era promover las prospecciones petrolíferas. Esta escasez llevó al racionamiento de gasolina (que también se produjo en muchos otros países). Los automovilistas se enfrentaron a largas colas en las gasolineras.

En Estados Unidos, los conductores de vehículos cuyas matrículas acabaran en número impar (o matrículas personalizadas) fueron autorizados a adquirir carburante solo en los días impares del mes, y la misma norma se aplicó a los propietarios de vehículos con matrículas pares.

No obstante, meses antes se había producido un extraño caso de allanamiento de la sede central del Partido Demócrata (en el edificio de oficinas Watergate), que el 17 de junio de 1972 destapó un método de realizar escuchas ilegales por hombres contratados por algunos colaboradores del presidente. Su situación comenzó a complicarse durante el juicio contra los acusados, cuando confesaron ante el juez Sirica, encargado de la investigación, que habían sido enviados por altos responsables del Partido Republicano. Para agravar los problemas de Nixon, su vicepresidente, Spiro T. Agnew, fue acusado de soborno y tuvo que dimitir de su cargo (Richard Nixon lo sustituyó por otro destacado congresista republicano, Gerald R. Ford, que se convertiría en Presidente tras la renuncia de su mentor). Paulatinamente se fue desvelando un plan preconcebido desde el entorno presidencial, en el cual se vieron implicados varios altos cargos, como John Mitchell, ministro de Justicia; John Dean, consejero presidencial; H. R. Haldeman, jefe de Personal de la Casa Blanca, o John Ehrlichman, asesor especial de la Casa Blanca para Asuntos Nacionales. 

En marzo de 1974 el Gran Jurado federal consideró al presidente copartícipe, sin cargos formales, en una conspiración para obstruir la acción de la justicia en la investigación del escándalo Watergate. En la tarde del 8 de agosto, Nixon anunció su dimisión. El 9 de agosto, Gerald Ford prestaba juramento del cargo.

Retirado en su rancho californiano de San Clemente, Nixon intentó volver a la práctica de la abogacía sin poder conseguirlo, ya que fue expulsado del Colegio de Abogados además de que fue incapacitado para el desempeño de su profesión en todo el territorio estadounidense. En 1978 plasmó sus experiencias como presidente en la obra "Mis memorias", libro por el que obtuvo importantes ganancias económicas. En el año 1986 volvió a publicar otra exitosa obra, "No más Vietnam".

Nixon sufrió un derrame cerebral el 18 de abril de 1994 y murió cuatro días más tarde a la edad de 81 años el 22 de abril. A su funeral asistieron el presidente Bill Clinton junto a su mujer, la primera dama, Hillary Clinton, el vicepresidente Spiro Agnew y los expresidentes Gerald Ford junto a Betty Ford, Jimmy Carter junto a Rosalynn Carter, Ronald Reagan junto a Nancy Reagan y George H. W. Bush junto a Barbara Bush. El funeral de Nixon se convirtió también en la última aparición del presidente Reagan, antes de padecer alzheimer.





</doc>
<doc id="15554" url="https://es.wikipedia.org/wiki?curid=15554" title="Julio I">
Julio I

Julio I (s. III - Roma, 352) fue el trigésimo quinto papa de la Iglesia católica, rigiendo entre el 6 de febrero de 337 y el 12 de abril de 352, cuando falleció. Es venerado como santo por la Iglesia Católica y su fiesta se celebra el 12 de abril.

Julio era hijo del prominente ciudadano romano Rústico. Nació en el siglo III, probablemente en Roma, de donde era oriundo su padre.

Julio fue elegido el 6 de febrero del 337, después de que el trono papal permaneció en vacancia por la prematura muerte del Papa electo, Marcos, hecho ocurrido el 7 de octubre del 336. La Iglesia duró cuatro meses con la silla papal vacía. Son desconocidos los motivos de la vacancia tan prolongada.

Confirmó en su puesto a dos obispos cristianos a quienes los arrianos habían hecho abdicar. En el otoño de 341, Julio I convocó un concilio al que asistieron 50 obispos con el propósito de pronunciarse de nuevo en contra del arrianismo y condenar a quienes quitaban obispos a su antojo.

A la muerte de Constantino I el Grande, el imperio se dividió entre sus tres hijos, uno de ellos, Constantino II, pronto desapareció de la historia y quedaron como emperadores sus otros dos hijos, Constancio II, en Oriente y Constante en Occidente. Mientras que Constante era católico, Constancio era arriano. En 350, Constante fue asesinado y el Imperio se reunificó bajo el mando de Constancio. El emperador desató entonces una terrible persecución contra la Iglesia.

Julio I fijó para la Iglesia de Occidente la solemnidad de Navidad el 25 de diciembre, en vez del 6 de enero, junto con la Epifanía. Tomó esta fecha porque, en el calendario juliano, el solsticio de invierno ocurría en ese día, siendo este acontecimiento festejado por muchos pueblos del Hemisferio Norte como un nuevo renacer del ciclo de la vida.

Los inicios del cristianismo en la era romana fueron difíciles, se producían constantes revueltas entre la población y enfrentamientos entre paganos y romanos recién convertidos al cristianismo. La celebración de ciertas fiestas paganas era también motivo de disputas sociales y políticas. Con el propósito de pacificar dichos enfrentamientos, y cristianizar las fiestas, el emperador Constantino el Grande con el apoyo del Papa Julio I decidió hacer coincidir las fiestas paganas de las Saturnales con la celebración del nacimiento del Mesías. De este modo se celebraría en esta fecha el nacimiento del Hijo de Dios el Cristo.

Se le considera el fundador del Archivo de la Santa Sede, porque ordenó la conservación de los documentos.

Julio es venerado como santo desde la misma fecha de su muerte. Falleció el 12 de abril del 352, en Roma, de causas naturales. Fue enterrado en las catacumbas del Calepodio en la Vía Aurelia. Luego sus restos se trasladaron a Santa María de la Trastevere, donde reposan, y que él mismo se hizo cargo de construir.


</doc>
<doc id="15556" url="https://es.wikipedia.org/wiki?curid=15556" title="Etnografía de Colombia">
Etnografía de Colombia

La etnografía de Colombia se caracteriza por ser el resultado de la mezcla de tres grupos principales: europeos, indígenas y africanos. Al mestizaje de estos tres grupos se sumó un gran número de inmigrantes procedentes de muchos países de Europa y del Medio Oriente. En el censo general de población de 2018, el 11,04% de la población se autoidentificó como afrocolombiana (incluyendo raizales y palenqueros), el 4,31% como indígena, y el como 0,01% gitana. El 87,58% fue clasificado sin pertenencia étnica, categoría que engloba al resto de las poblaciones que habitan el país, las cuales incluyen mestizos, blancos, árabes, judíos y otros grupos que no aparecen oficialmente en el censo.


A pesar de varios trabajos en etnohistoria, se desconoce exactamente cuántos indígenas habitaban el actual territorio de Colombia a la llegada de los europeos, debido a que la información prehispánica era de tradición oral, y por tanto se carece de documentos escritos que sirvan para calcular la población de la época. No obstante, si se sabe que tras la llegada de los españoles hubo una gran mortandad de la población indígena (el 90 %) propiciada por las enfermedades traídas por los europeos, las guerras y combates esporádicos que mantuvieron con estos últimos y los trabajos forzados y semiesclavitud a la que fueron sometidos los pueblos indígenas por los colonizadores españoles. No obstante, la población indígena del territorio colombiano ya era de por sí escasa, lo que explica en parte la necesidad de los españoles de importar de esclavos africanos para utilizarlos como mano de obra, aunque la razón principal fue el exterminio de la mayor parte de los indígenas.

Los primeros esclavos africanos llegaron en el año 1504, pero la necesidad era tal que a partir de 1520 entraban en el país aproximadamente 4000 esclavos africanos al año. Desde finales del s.XVI, muchos esclavos negros lograban huir (cimarrones) y fundaban y establecían pueblos libres negros (Palenques), como el famoso Palenque de San Basilio. El punto de entrada de los esclavos era Cartagena, que junto con Mompox era el principal punto de compra-venta de estos. Desde allí eran desplazados por los ríos Cauca y Magdalena hasta otros centros secundarios de comercio esclavista, como Popayán, Honda (Tolima), Anserma (Caldas) y Cali. Durante las primeras décadas se importaban principalmente esclavos varones jóvenes, pero luego se comenzó a introducir mujeres jóvenes para autoabastecer de nuevos esclavos al territorio. Los esclavos realizaban todo tipo de labores, principalmente en minería, agricultura, ganadería y servicio doméstico. Los principales grupos lingüísticos de los esclavos eran el bantú y el sudanés.Además los esclavos debían ser instruidos en la fe católica para ser reconocidos en la nueva sociedad. Recibir el sacramento del bautismo era una condición indispensable para entrar a la América hispánica, según las normas de la corona española, que prohibía la entrada a judíos, herejes y paganos.

Las primeras exploraciones europeas fueron realizadas por Alonso de Ojeda, Juan de la Cosa y Américo Vespucio, llegando hasta la Península de la Guajira. En 1501 Rodrigo de Bastidas descubrió las bocas del río Magdalena y la bahía de Cartagena, acompañado del propio Juan de la Cosa. La primera carta del litoral fue levantada por Juan de la Cosa entre 1492 y 1510. En 1511 Vasco Núñez de Balboa descubrió el río Atrato y contempló las aguas del Pacífico desde la sierra panameña de Darién. En 1522 Pascual de Andagoya, descubridor del Perú, llegó por el Pacífico hasta las bocas del río San Juan. Los españoles invirtieron unos veinte años en explorar las costas colombianas, fundaron varias ciudades y factorías y después avanzaron hacia el interior del país. Los primeros colonos españoles comenzaron a establecerse en el territorio inmediatamente después de su conquista por parte de Gonzalo Jiménez de Quesada, alrededor del año 1540.

En 1528, la familia de banqueros Welser consiguió de Carlos V la exclusividad para la conquista y colonización del territorio comprendido entre el Cabo de la Vela (actual Colombia) y Maracapana (actual Venezuela), siendo los primeros europeos no latinos que iniciaron el proceso colonizador en América Latina. Algunos de los exploradores más importantes fueron Ambrosius Ehinger, Nikolaus Federmann, Georg Hohermut von Speyer o Philipp von Hutten, pero su presencia finalizó en 1546, tras ser retirada la concesión por el Consejo de Indias luego de los reiterados intentos poco exitosos de los gobernadores enviados por los Welser para establecer un gobierno estable en sus territorios, el descontento de los castellanos que habitaban Coro y acusaciones de diversa índole. Las razones para la retirada del contrato fue el incumplimiento del contrato de arrendamiento, donde se incluía la fundación de varias ciudades y varios fuertes, y también falló en la parte del contrato donde se estipulaba la obligatoriedad de extender el cristianismo entre los indígenas. Durante este corto período, pequeños grupos de colonos alemanes se establecieron en el territorio, pero el clima, el calor y las enfermedades acabaron con la vida de muchos de ellos y otros regresaron a Alemania, quedándose muy pocos.

Durante el siglo XVI y principios del XVII, los colonos españoles no eran más que soldados varones al servicio de los conquistadores que después se asentaban en el territorio. Las mujeres españolas tardarían en llegar y cuando lo hicieron sus números siempre fueron relativamente escasos. Esto unido a que los españoles varones eran jóvenes y generalmente habían llegado a América en busca de aventuras, riquezas y por los relatos acerca de que las mujeres indígenas iban desnudas, tenían grandes cantidades de hijos con las mujeres nativas y las esclavas africanas, a las cuales con frecuencia abandonaban. De este modo comenzó el mestizaje racial y en parte de los casos cultural, y en pocos años la población mestiza se alzó como la mayoría de la población, y conforme siguieron mezclándose europeos (sobre todo españoles), americanos y africanos, surgieron diversas variedades de razas, siendo denominadas las más importantes, como: mestizo (blanco-cobrizo), castizo (blanco-mestizo), moreno (blanco-negro), zambo (negro-cobrizo).

La sociedad colonial se caracterizó por dividirse en clases sociales étnicas. Así pues, la clase gobernante eran los criollos (denominación a los españoles y descendientes sin mezcla de éstos establecidos en América) y algunas variedades (castizos y mestizos de aspecto blanco), la clase media la formaban los mestizos y algunas variedades (algunos castizos y mestizos de pocos o medios rasgos indígenas), la clase baja la formaban los indígenas y algunas variedades (mestizos de aspecto predominante indígena) y en lo más bajo se hallaban los esclavos negros y algunas variedades (mulatos de aspecto predominante negro).

Desde tiempos de la colonia hasta tiempos incluso actuales, la población blanca generalmente ha alcanzado los principales y más importantes puestos, cargos y trabajos de la sociedad, teniendo un estatus económico y un nivel de bienestar social medio-alto, en contraste con las personas de otras etnias. Por ejemplo, la gran mayoría de presidentes del país han sido de raza blanca, las ciudades más grandes y desarrolladas del país actualmente Bogotá y Medellín tienen una mayor porcentaje de habitantes blancos, mientras que las zonas más atrasadas son aquellas donde la presencia cobriza o negra es mayor; como la costa pacífica o la región amazónica. Esto ha generado controversias internas sobre el papel de la raza blanca y su responsabilidad en ese atraso; desembocando en la "ley antidiscriminación" del 2011, en donde se establece: "El que arbitrariamente impida, obstruya o restrinja el pleno ejercicio de los derechos de las personas por razones de su raza, etnia, religión, nacionalidad, ideología política o filosófica, sexo u orientación sexual, incurrirá en prisión de 12 a 36 meses". 

A partir de la independencia del país, se sumaron a la mezcla pequeños grupos de inmigrantes árabes, judíos, europeos no españoles (italianos, alemanes...), chinos y otros asiáticos, aunque no tuvieron un impacto significativo en la composición étnica y la cultura del país.

Constituyen el principal grupo étnico, representando el 53,2% de la población total del país. El mestizaje en Colombia comenzó poco después de que se establecieran los primeros colonizadores en el territorio. Es resultado directo de la escasez de mujeres europeas en algunos sectores del reino durante la conquista, debido a que durante todo el período colonial la mayoría de los inmigrantes europeos eran varones. Los españoles entonces se unían principalmente con mujeres nativas de los distintos grupos étnicos, indígenas o africanos. Los mestizos se hallan prácticamente en todo el territorio del país y su población es la más grande en Colombia, siendo el aporte europeo casi exclusivo por parte paterna, pues más del 80% de los colombianos descienden de un europeo por vía paterna, mientras que el 85 % de los colombianos provienen de una indígena por vía materna.

La ascendencia de los caucásicos colombianos es principalmente española y árabe, con algunos aportes italianos, franceses, alemanes y eslavos. Según fuentes externas, la cantidad de blancos en Colombia está entre el 20% y el 37% de la población.

En lo que era la Nueva Granada se presentó una gran cantidad de españoles que comenzaron a llegar al territorio como colonos poco después de la conquista en grandes números (en comparación con la población nativa del territorio por aquellos tiempos), pero eran principalmente varones solteros. El mayor ejemplo lo proporciona la región Andina, por ejemplo, en Antioquia las investigaciones genéticas encontraron que los haplogrupos del cromosoma Y muestran una ascendencia vía masculina 97% europea, 2% africana y 1% indígena y por el contrario, los haplogrupos del ADN mitocondrial revelan una ascendencia por vía materna 50% indígena, 2% africana y 48% europea.

Tras la independencia del país se abrieron las puertas a inmigrantes europeos, a pesar de que el gobierno no la motivó ni la incentivó. Para entonces el país era política, social y económicamente muy inestable, produciéndose poco después de la independencia una serie de conflictos internos, guerras civiles y golpes de estado que lo desestabilizaron casi por completo; suponiendo una desmotivación para los inmigrantes europeos. A pesar de todo, pequeños grupos de españoles, italianos, alemanes, franceses, británicos, rusos, polacos (entre otros), llegaron al país principalmente a través del puerto de Barranquilla, estableciéndose mayormente en las principales ciudades. Una excepción importante a esta tendencia es el departamento de San Andrés y Providencia, el cual fue colonia inglesa y la población blanca desciende de colonos escoceses e ingleses principalmente.

Históricamente, la población blanca ha desempeñado un papel influyente en la historia de Colombia, como lo es en la creación de las instituciones gubernamentales, la constitución, el ejército, el himno nacional, la construcción de infraestructura, creaciones en el arte, la arquitectura y las ciencias.

Según el último censo del país, corresponden al 6,75% de la población, incluyendo a mulatos, raizales y palenqueros, sin embargo otros estudios señalan hasta un 25% de afrocolombianos en total, conformados por un 21,1% de mulatos y garífunas, y un 3,9% de negros, en el caso de Lizcano-Fernández (2005), o un 21% de afrocolombianos, con 17% de mulatos y 4% de negros, según Schwartzman (2008).

Dentro de los afrocolombianos se pueden diferenciar cuatro grupos importantes: los que se ubican en el corredor del Pacífico colombiano, los raizales del Archipiélago de San Andrés Providencia y Santa Catalina, la comunidad de San Basilio de Palenque y otros palenques, y la población que reside en las cabeceras municipales o en las ciudades capitales como Cali. Los departamentos con mayor porcentaje de afrocolombianos son Chocó (73,83%), San Andrés y Providencia (55,64%), Cauca (19,74%), Nariño (17,45%), Valle del Cauca (17,09%), Bolívar (16,73%), Cesar (12,97%) y Sucre (11,91%). Según el censo de 2005, el 29,2% del total de afrocolombianos se concentra en las ciudades de Cartagena de Indias, Barranquilla, Cali, Medellín y Bogotá. En Bogotá, la ciudad del país con más personas que no declaran pertenencia étnica, residen 100 mil afrocolombianos, que representan el 1,5% de la población del Distrito. El 29% de esta población nació en la ciudad, mientras que el 17% llegó desde el Chocó.La Constitución colombiana reconoce los derechos, cultura, costumbres, tradiciones y territorios de la población afrocolombiana y se ha titulado como tierras colectivas de comunidades negras un total de 15 717 269 hectáreas que corresponde al 16,13% de las superficie del país. La población afrocolombiana es mayoritariamente joven, pero está experimentando un progresivo envejecimiento, traduciéndose en aumento en el número adultos mayores. Además, y a diferencia de los indígenas, presenta en su estructura y distribución de género un comportamiento más similar al del total de la población del país. El 86% de la población afrocolombiana está alfabetizada, siendo ligeramente mayor el porcentaje en mujeres (88%) que en hombres (86%). En cuánto a educación, el 41% posee estudios básicos primarios, un 21% no posee estudios en ningún grado y un 16% posee estudios básicos secundarios. El 47% de la población es soltera. La población afrocolombiana posee una alta tasa de natalidad, siendo la media de hijos por mujer de 2,7, estando por encima de la media nacional (2,1), siendo 2,4 en entornos urbanos y 3.5 en entornos rurales, en ambos casos también por encima de la media nacional (1,9 y 3,1 respectivamente).

A pesar de haber constituido un segmento importante en el pasado (en 1852 los indígenas eran el 17,8% de la población total), la población indígena constituye actualmente apenas un 4,31% del total. Tras haber sido víctimas de abusos, semiesclavitud, duras condiciones de vida y trabajos forzados durante siglos, la Constitución de 1991 reconoció los derechos fundamentales de los pueblos indígenas, y además ratificó el Convenio 169 de la OIT que regula internacionalmente sus derechos. El gobierno colombiano reconoce la existencia de 87 pueblos indígenas: Achagua, Amorúa, Andoke, Arhuaco, Arzario, Awá, Bara, Barasana, Barí, Betoye, Bora, Cañamomo, Carapana, Chimila, Chiricoa, Cocama, Coreguaje, Coconuco, Coyaima, Desano, Dujo, Emberá, Emberá Chamí, Emberá Katío, Eperara Siadipara, Guambiano, Guanaca, Guane, Guayabero, Hitnü, Inga, Kawiyarí, Kamëntsa, Kankuamo, Karijona, Kichwa, Kofán, Kogui, Kubeo, Kuiba, Kurripako, Letuama, Makaguaje, Makuna, Masiguare, Matapí, Miraña, Mokaná, Muisca, Nasa, Nonuya, Nunak, Ocaina, Pasto, Piaroa, Piratapuyo, Pisamira, Puinave, Sáliba, Senú, Sikuani, Siona, Siriano, Taiwano, Tanimuka, Tariano, Tatuyo, Tikuna, Totoró, Tsiripu, Tucano, Tule, Tuyuka, Tzase, Uitoto, Umbrá, U'wa, Wanano, Waunan, Wayuu, Yagua, Yanacona, Yaruro, Yauna, Yuko, Yukuna, Yuri y Yurutí. Los departamentos con mayor proporción de indígenas son Vaupés (81,68%), Guainía (74,9%), Vichada (58,16%), Amazonas (57,72%), La Guajira (47,82%), Cauca (24,81%), Putumayo (17,9%) y Nariño (15,46%). En el Censo de 2005, los departamentos de La Guajira, Cauca y Nariño concentraron aproximadamente la mitad de los indígenas del país. De acuerdo con la Constitución Nacional, las lenguas indígenas son también oficiales en sus territorios, aparte del castellano. En el país, se hablan 64 lenguas amerindias y una diversidad de dialectos que se agrupan en 13 familias lingüísticas.





Según una investigación llevado a cabo con 1,659 muestras colombianas, la mayoría de los individuos estudiados presentaron ojos negros o cafés (64%), seguidos por quienes tenían ojos castaños (16% de los hombres y 15% de las mujeres), color miel (11% de las mujeres y 10% de los hombres), verdes (8%), y azules o grises (2%). Asimismo, la mayoría tenían cabello negro o café (85% de las mujeres y 81% de los hombres), seguidos por aquellos de cabello castaño (16% de los hombres y 12% de las mujeres), rubio (2%) y rojizo (1% de los hombres). El cabello fue descrito principalmente como ondulado (39% de los hombres y 38% de las mujeres) y lacio (39% de los hombres y 33% de las mujeres), seguido por el cabello rizado (27% de las mujeres y 20% de los hombres) y crespo (2%). La mezcla genética promedio para el total de la muestra fue 60% europea, 29% indígena y 11% africana.



</doc>
<doc id="15559" url="https://es.wikipedia.org/wiki?curid=15559" title="Budismo en Japón">
Budismo en Japón

La influencia del budismo en Japón se ve reflejada en muchos aspectos de su sociedad a lo largo de la historia; desde su cultura, arte y arquitectura, pasando por su sistema de valores, su filosofía y su espiritualidad llegando a forjar su carácter.

La gran mayoría de los japoneses practica de manera simultánea el budismo y el Shinto, la religión autóctona del país. Según datos de 2010 unos 45.820.000 de habitantes se declaran budistas, el equivalente al 36.2% de la población.

Tras las enseñanzas de Siddhartha Gautama, el budismo se extiende en la India y otros países asiáticos dando lugar a distintas interpretaciones sobre los textos que promulgan su mensaje. Nacen dos corrientes principales que tratan el budismo de forma diferente: la Theravada, que se establece en Sri Lanka, Birmania, Laos, Camboya y Tailandia y se basa en el seguimiento de la doctrina de Buda poniendo especial atención en el estudio de los preceptos y la vida monástica; y la Mahayana, que nace como movimiento laico que interpreta los textos más como un método que como una filosofía sin dar a Buda un trato de deidad y que se expandiría a partir del siglo I d.E.C. en Asia Central, China, Corea y Japón.

Los primeros misioneros Mahayana penetran en China durante la Dinastía Han por vía marítima, llegando a las regiones sureñas del río Yangtzé y Huai, y a través de la Ruta de la Seda, alcanzando las regiones del este hasta llegar a la capital Han de Luoyang, donde en el año 68 d.E.C. se establecería el Templo del Caballo Blanco. Habiendo sido casi coetáneo de Confucio, el budismo no sería ampliamente aceptado en el país hasta la caída de los Han, que propiciaría la necesidad de la población china de acoger la nueva fe extranjera.

La llegada desde la India en el siglo V de Bodhidharma, supondría un cambio en la percepción de los preceptos budistas y el nacimiento del budismo Chan. Al contrario que otros misioneros anteriores, Bodhidharma no pretende ser recibido con honores de iluminado sino que cuestiona las escrituras y doctrinas establecidas.

Tras su llegada al sur de China, Bodhidharma es invitado por el emperador Wu, que busca su bendición después de haber realizado grandes inversiones en la difusión del budismo. Bodhidharma le daría a conocer el error que supone buscar la salvación por medio de la adoración de lo sagrado y viendo que su empresa no podría llevarse a cabo en el estado de los Liang, continuaría su marcha hasta llegar al estado de los Wei para finalmente establecerse en Shaolin. La forma de enseñar de Bodhidharma, basándose en formular preguntas que ayudaban a encontrar la iluminación en lugar de explicar problemas, supondría una de las bases del budismo zen. Posteriormente, las enseñanzas de Bodhidharma fueron difundidas por los diferentes patriarcas que le sucedieron que, fusionando a lo largo del tiempo los principios del budismo Mahayana primitivo con las ideas taoístas, dieron lugar al budismo Chan.

En el Libro de Liang de Las Veinticuatro Historias, texto chino que supone la única referencia de la época ya que los japoneses aún no dominaban la escritura, se tiene constancia de asentamiento de budistas en Japón.

En el año 552, Syong-Myong, rey de Paekche (uno de los tres reinos que formaban Corea), hace llegar una serie de regalos a Kinmei como muestra de agradecimiento por su colaboración en la guerra contra Silla. Estos se basaban en una imagen de Buda fundida en oro y cobre y textos con sutras escritos en sánscrito, y adjuntaban una carta donde el rey Syong-Myong mostraba su admiración por el budismo y la conveniencia política de adoptar la nueva religión.

El emperador Kinmei se mantuvo al margen respecto al budismo y cedió la imagen a Soga no Iname, encargado de gestionar las inversiones de inmigrantes acaudalados y miembro de la familia Soga que mantenía grandes relaciones con las cortes coreanas. Estos abrazaron el budismo y reunieron a tres monjas que tras recibir enseñanza en Corea, se encargaría de gestionar el templo budista que Soga no Umako hizo construir.

Hasta entonces, la religión practicada en Japón era el Shinto y algunas familias de la élite japonesa como la de los Mononobe y Nakatomi, que basaban su linaje en una supuesta descendencia de los "kami" sintoístas, se vieron amenazados por el budismo. Las tensiones entre la familia Soga y las de Mononobe y Nakatomi darían como resultado un conflicto armado en el que los Soga saldrían victoriosos y conseguirían el control de la familia imperial con la se emparentaría. 

Con la toma del poder por parte de la Emperatriz Suiko, sobrina de Soga no Umako, y gracias a la dedicación del Príncipe Regente Shōtoku, el budismo se establecería definitivamente en Japón. La consideración al emperador de descendiente directo de Amaterasu, hace que las dos religiones, budismo y sintoísmo, se integren sin detrimento de ninguna. Shōtoku sería considerado una divinidad tras su muerte, y su vida sería contada con muchos paralelismos con la de Siddharta Gautama.

En este Periodo el número de templos experimentó un gran crecimiento y los emperadores Tenmu y Monmu, tras abrazar el budismo, sentaron las bases del patrocinio estatal. Este patrocinio dio lugar a seis escuelas: Ritsu, Jōjitsu y Kusha pertenecientes al budismo Theravada y Sanron, Hossō y Kegon que seguían directrices del budismo Mahayana. 

Los gobernantes encontraron especialmente atractivo el ideal budista de que el mandato benevolente de un monarca traía el paraíso en la tierra, concepto que añadía legitimidad a su gobierno.

Ya en el año 627, en el Japón había 46 templos budistas, 816 monjes budistas y 569 monjas budistas.

Durante el Periodo Heian se crean las escuelas Tendai y Shingon.

En el año 794 Saicho funda en Hiei la escuela Tendai, que basa su filosofía en el "Sutra del Loto" y los tratados Tiantai chinos sobre técnicas de meditación. Se cree que la idea de que Kyoto fuese el asentamiento como nueva capital se debe él tras aplicar técnicas de "feng shui."

Kūkai funda en el año 816, con la ayuda del Emperador Saga, un monasterio en el monte Koya y establece la secta Shingón. Fue uno de los emisarios enviados a China junto a Saicho y sigue las doctrinas del budismo esotérico Vajrayana. En 921 se le concede el título póstumo de Kōbō-Daishi “"Gran maestro que expande el budismo por doquier"”.

En esta época aparecen nuevas sectas muy diferentes a las aparecidas en Nara, con doctrinas más sencillas que facilitarían su llegada a las clases más populares. Estas nuevas formas de budismo resultan mucho más personal e íntimo, y reivindica el valor de las mujeres garantizándoles la igualdad de posibilidades de salvación religiosa, hasta entonces negada. Surgen dos grandes corrientes: el budismo de la Tierra Pura y el budismo Zen.

El budismo de la Tierra Pura se fundamenta en el "nenbutsu" o culto al Buda Amida (bodhisttva Dharmakara), héroe del Sutra de La Tierra Pura ("Daimuryoju-kyo" en japonés). fue el principal artífice de la expansión del culto de Amida llegado desde China en el año 847.

Honen (1133-1212) fue el fundador de la secta de la Tierra Pura. Tras el estudio del "Sutra de la visualización de la Tierra Pura," escrito por Shandao, Honen llega a la convicción de que la recitación constante del "nenbutsu" es la clave para la salvación como promete el Buda Amida. Actualmente, esta secta cuenta con 6.500.000 seguidores en Japón y es, junto a la secta de la Verdadera Tierra Pura, la que poseen mayor número de fieles.

Shinran (1173-1262), discípulo de Honen y formado también en la disciplina Tendai impartida en el monte Hiei, fue el fundador de la secta de la Verdadera Tierra Pura. Basaba su diferencia en que no creía en la conveniencia de la repetición del "nenbutsu", sino que consideraba que una sola muestra de devoción sincera hacia el buda Amida era suficiente. Determinó que los sacerdotes podían contraer matrimonio y el liderazgo asumió carácter hereditario. A día de hoy, esta secta cuenta con 13.000.000 de seguidores.

 (1234–1289) fundó la tercera de las escuelas del budismo de la Tierra pura. De familia acomodada, renunció a su fortuna para convertirse en predicador itinerante después de recibir en sueños un oráculo de Gongen, divinidad "kami" y manifestación de la esencia de Buda. Basó su existencia en emular a Buddha Shakyamuni, renunciando a todos los bienes materiales, desapegándose de la existencia egoísta. Atrajo a gran número de adeptos en poco tiempo y estos formaron la secta “Tempestiva” o Ji-shu. Actualmente cuenta con entre 300 y 400.000 adeptos.

El budismo Zen fue la otra gran corriente religiosa que gozó de una gran difusión en este periodo y actuó como un puente entre las sectas tradicionales y las nuevas. Contó con algunas sectas muy importantes como fueron la Rinzai y la Sōtō, que ejercieron una gran influencia sobre la filosofía samurái. 

Mientras en la secta de la Tierra Pura se pone énfasis en el culto a Amida y la recitación del "nenbutsu", las escuelas Zen se centran en el esfuerzo personal individual para alcanzar la iluminación ("satori") por medio de la meditación ("zazen"). Según la secta Sōtō, es la práctica de la meditación la que proporciona la iluminación. La secta Rinzai mira de acelerar este proceso añadiendo un "koan" (problema destinado a ser resuelto con el uso de la meditación). Se calcula que los seguidores de las diferentes sectas Zen suponen el diez por ciento de los budistas registrados en Japón.

Durante la regencia Hojo de Kamakura se patrocina el Zen, ya que se consideraba que transmitía la revitalizante cultura china y éste elevaba el nivel cultural del shogunato sobre la corte imperial de Kyoto.

El linaje Rinzai del Zen llegó a Japón procedente de China de mano de Eisai (1141-1215). Formado en el monte Hiei, tras peregrinar a China se propuso construir el primer templo Zen en Kyoto, encontrándose con la negativa de la secta Tendai. Escribió la "Propagación del Zen para la seguridad del país (Kozen gokokuron)," donde se afirmaba que el Zen era el camino para conocer la verdadera naturaleza de la conciencia y contenía la enseñanza de como eliminar el ego; promocionar el Zen era fomentar la falta de egoísmo, lo que según Eisai facilitaría la creación de una sociedad pacífica. Finalmente, en 1202 el shogunato le concedió permiso para construir el templo Zen de Ken’ninji en Kyoto, donde acudiría a practicar zazen un joven Dogen.

Dogen (1200-1253)fue ordenado en el monte Hiei de la secta Tendai con trece años y un año después se convirtió en discípulo de Eisai en Ken’ninji. En 1225, después de viajar a China, Dogen toma como maestro a Rujing ("Nyojyo") con el que establece una estrecha relación y bajo su guía alcanza la iluminación. En 1228 regresa a Japón y en 1233 funda el templo de Koshoji en Uji introduciendo la tradición del linaje Caodong (Sōtō en japonés) de su maestro Rujing. La redacción de sermones y directrices dirigidas a sus discípulos acabaría convirtiéndose en el "Tesoro de la visión del verdadero Dharma (Shōbōgenzō)."

Dogen era de la creencia que la meditación no se realizaba con el fin de alcanzar la iluminación, sino que la práctica del zazen constituía el fin en sí mismo, por lo que no veía necesario el uso de los "koan". Practicar la meditación de forma adecuada ayudaba a estar en consonancia con la naturaleza del buda original.

Al igual que otros líderes religiosos coetáneos, Nichiren (1222-1282) se formó en el monte Hiei, donde forjó su convicción de que el "Sutra del Loto" contenía las verdaderas enseñanzas de Buda. Predicador carismático, atrajo a un gran número de seguidores, especialmente acaudalados terratenientes. En 1257, tras una serie de calamidades naturales y la crisis social y política que vivía el país, Nichiren concluye que se deben a la desaparición de la práctica de la perseverancia y el sacrificio tal y como muestra el "Sutra del Loto", atribuyendo esas desgracias a la práctica del "nenbutsu" de la Tierra Pura. Tras los intentos de invasión mongoles de 1268 y 1281, sus acusaciones contra las sectas de la Tierra Pura van en aumento y es desterrado en varias ocasiones.

El Periodo Muromachi vio como el sintoísmo y el budismo se fusionaban. Las sectas Tendai y Shingon, cuyos fundadores Saicho y Kukai profesaban un profundo respeto por los "kami" autóctonos de Japón, propiciaron la incorporación de las divinidades sintoístas en su marco institucional. Las victorias obtenidas contra las invasiones mongolas de 1274 y 1281 por sendos tifones, hizo creer que Japón vivía bajo la protección de las divinidades "kami" que enviaban el viento divino (kamikaze).

Algunas sectas del Periodo Kamakura crearon sus propias milicias religiosas cuando el país quedó en manos de los señores de la guerra. Las sectas Nichiren y de la Tierra Pura entran en conflicto entre sí y con las sectas de Nara; es normal que los templos posean unidades militares propias formadas por monjes-soldado. 

En 1549 hace su introducción en la religión japonesa el cristianismo de la mano de Francisco Javier, misionero jesuita que llegaba a la isla con el objetivo de evangelizarla. En la campaña de unificación de Japón, Oda Nobunaga encuentra la férrea oposición de la comunidad de la Tierra Pura y decide arrasar el monte Hiei matando a quien se encuentra en su camino. Su repulsa a las sectas budistas provocó que patrocinara al cristianismo.

Toyotomi Hideyoshi, sucesor de Nobunaga, adopta el cristianismo en primera instancia pero algunas costumbres y prácticas llevadas a cabo por algunos conversos y la acumulación de poder de la que hacían gala, hacen que publique un edicto en el que se ordena abandonar Japón a los misioneros. Tras ser apaciguado por Valigniano, la prohibición no es llevada a cabo, pero el conocimiento de las intenciones de conquistar Japón por parte de los monarcas españoles y portugueses, y la entrada clandestina de franciscanos en el país, hacen desatar la ira de Hideyoshi y castiga a los cristianos residentes empleando técnicas de la Inquisición.

El sistema "danka", que obliga a las familias a convertirse en protectoras de su templo local como prueba del rechazo al cristianismo, convirtió a los templos budistas en instituciones. Unos 1600 templos recibieron el apoyo incondicional de la población. Además, existía la obligación de colaborar en los gastos del templo y en asistir a sus actos.

Ingen Ryuki (1592-1673), maestro Chan erudito e iluminado, llega a Japón tras ser invitado y después de una audiencia ante el shogun, éste queda tan impresionado que decide cederle terrenos para que pueda construir un monasterio. Su doctrina combinaba el nenbutsu con el zazen. Sus seguidores actualmente ascienden a 350.000 y forman el 8,5 por ciento de los budistas japoneses.

Con el traspaso de poderes del shogunato al emperador, algunos políticos influyentes promueven la separación del sintoísmo y el budismo, desatando una ola de violencia antibudista en varias zonas de Japón. Ante los actos vandálicos de los sintoístas, el gobierno determinó que “separación” no equivalía a “destrucción”.

Mientras se instauraba el Shinto Estatal, las sectas budistas se vieron obligadas a adaptarse. El Rinzai Zen y el Soto Zen se modernizaron adoptando ideas occidentales pero manteniendo su identidad japonesa. Se aprobaron edictos que permitían a los monjes comer carne y casarse. Estas medidas, junto a la difusión del Shinto Estatal, fueron definitivas para desacralizar al budismo y marginarlo. 

Después de la Restauración Meiji se registraron 13 escuelas budistas que se dividían en 56 ramas, durante la II Guerra Mundial se redujeron a 28. La ley que reguladora fue revocada y actualmente se permite su regreso

Fundador: Xuanzang (玄奘 "Genjo" en japonés), China, c. 630 dC
Nombre en chino: Faxiang (法相), "El carácter del dharma"
Llega al Japón: con Dosho, 654 dC
Influencias principales: Sanron, Zen
Doctrina: "Yuishiki" ("sólo la conciencia")
Texto fundamental: "Jo yuishikiron" (成唯識論) 

Fundador: Dushun (杜順, "Dojun" en japonés), China, c. 600 dC
Nombre en chino: Huayan (華厳)
Llega al Japón: con Bodhisena, 736 dC
Influencias principales: Hosso
Doctrina: "Shihōkai" (四法界)
Texto fundamental: Avatamsaka Sutra ("Kegonkyo" 華厳経)
Fundador: Daoxuan (道宣, "Dosen" en japonés), China, c. 650 dC
Nombre en chino: Lü (律), "Vinaya"
Llega al Japón: con Ganjin (鑑真), 753 dC
Doctrina: Vinaya (las reglas monásticas en el Tripitaka)
Texto fundamental: Dharmaguptavinaya ("Shibunritsu" 四分律)

Las escuelas monásticas (密教, "mikkyo" en japonés) pertenecen a la escuela Vajrayāna (Vehículo de Diamante) del budismo, también conocido como budismo tántrico.

Fundador: Zhiyi (智顗, "Chigi" en japonés), China, c. 550 dC
Nombre en chino: Tiantai (天台), nombrado en honor al templo fundacional
Llega al Japón: con Saichō (最澄), 807 dC
Doctrina: "Sandai" (三諦, "Triple Verdad")
Texto fundamental: Sutra del Loto ("Hokkekyo" 法華経)

Fundador: Kukai (空海), Japón, 816 dC
Nombre en japonés: 真言, "Palabra Verdadera"
Influencias principales: Tantra
Doctrina: Vajrayāna/Tantra (diestro, en el sentido de "usar la mano derecha")
Textos fundamentales: Mahavairochana Sutra ("Dainichikyo" 大日経), Sutra del Diamante ("Kongokyo" 金剛経) 

Fundador: Nichiren Daishonin, 1253 dC
Nombre en japonés: 日蓮, "Sol Loto"
Influencias principales: Tendai
Doctrina: "Nam Myoho Renge Kyo" (南無妙法蓮華経)
Texto fundamental: Sutra del Loto ("Hokkekyo" 法華経)

El período Kamakura fue testigo de la llegada de las dos escuelas que, probablemente, han tenido el mayor impacto en el país: la escuela amidista Tierra Pura, que daba énfasis a la salvación a través de la creencia en Amitābha y es hasta nuestros días la escuela budista más grande del Japón (y por toda Asia); y la escuela Zen, de corte más filosófico, que fue rápidamente adoptada por las clases altas y tuvo un profundo impacto en la cultura japonesa.

Fundador: Huiyuan ("Eon" en japonés), China, c. 400 dC
Nombre en chino: "Jingtu" Tierra Pura
Llega al Japón: con Honen, 1175 dC
Doctrina: "Nembutsu" ("oración a Buda")
Texto fundamental: Sutra de la vida infinita ("Muryojukyo")

Fundador: Shinran, 1224 dC
Nombre en japonés: "La verdadera Tierra Pura"
Influencia principal: Jōdō
Doctrina: "shintai zokutai" ("Verdad cierta, verdad común")
Texto fundamental: Sutra de la vida infinita ("Muryojukyo")

Es una escuela de budismo Zen que se centra en la práctica de zazen Shikantaza (tan solo sentarse) para conseguir el satori (iluminación)

Escuela de budismo Zen que propone para conseguir el satori (iluminación) los ejercicios Kōan (enseñanzas en forma de acertijos)


El budismo llegó al Japón el año 572, cuando los coreanos llegan a Nara para presentar las ocho escuelas doctrinarias. Las escuelas de Nara finalmente menguaron en su influencia y las escuelas que aún se mantienen son:




</doc>
<doc id="15562" url="https://es.wikipedia.org/wiki?curid=15562" title="Geografía de Ecuador">
Geografía de Ecuador

Ecuador (nombre oficial: República del Ecuador) es un país situado en la parte noroeste de América del Sur. Ecuador limita al norte con Colombia, al sur y al este con Perú y al oeste con el océano Pacífico. El país tiene una extensión de 283.561 km². Además del territorio continental, Ecuador está formada por el archipiélago de Colón, aparte de otras cercanas al continente, como Puná, Santay, y la Isla de la Plata.

Ecuador se encuentra sobre la línea ecuatorial terrestre por lo cual su territorio se encuentra en ambos hemisferios. Comprende dos espacios distantes entre sí: el territorio continental al noroeste de América del Sur con algunas islas adyacentes a la costa y, el archipiélago o provincia insular de Galápagos, que se encuentra a 1000 kilómetros de distancia del litoral ecuatoriano en el Océano Pacífico.


Las principales unidades del relieve ecuatoriano son la llanura costera al norte del Golfo de Guayaquil, la sección de la Cordillera de los Andes en el centro del país y un extenso sector de la llanura amazónica ubicado al oriente del país.

Hacia el suroeste se ubica el Golfo de Guayaquil, donde desemboca el río Guayas en el Océano Pacífico. Muy cerca de Quito, la capital, sobre la Cordillera de los Andes, se alza el Chimborazo, el volcán activo más alto del mundo.

El punto más alto del Ecuador es el volcán Chimborazo, con 6310 nbsp;msnm y cuya cima es el lugar más lejano al núcleo de la tierra debido a la silueta elíptica del planeta.

Es parte del Chocó biogeográfico. Se ubica al oeste del país; el territorio de la Costa está formado por llanuras fértiles, colinas, cuencas sedimentarias y elevaciones de poca altitud. Por su territorio corren ríos que parten desde los Andes hasta llegar al Océano Pacífico. Cinco provincias cuentan con playas y balnearios muy atractivos para el turista. En esta zona se encuentra la mayor ciudad de Ecuador: Guayaquil y otras importantes ciudades ecuatorianas: Machala, Manta, Portoviejo, Santo Domingo y Quevedo (Ecuador). La Costa está dividida en siete provincias: Esmeraldas, Santo Domingo de los Tsáchilas, Manabí, Guayas, Santa Elena, Los Ríos y El Oro.

Se encuentra ubicada entre el Nudo de los Pastos al norte hasta el de Loja al sur, ocupando una franja de 600 km de largo por 100 km a 120 km de ancho, la altura media es de 4 000 metros.
La estación lluviosa o invierno dura de octubre a mayo, con una temperatura anual promedio que varía de 12 °C a 18 °C. Esta región se caracteriza por sus impresionantes elevaciones montañosas, volcanes y nevados. Entre los más importantes están el Chimborazo y el Cotopaxi. Sus diez provincias cuentan con ciudades de gran importancia histórica como Quito y Cuenca, y centros artesanales como Otavalo. Igualmente, existen varios parques nacionales con flora y fauna muy ricas y variadas. Está conformada por 10 provincias: Carchi, Imbabura, Pichincha, Cotopaxi, Tungurahua, Bolívar, Chimborazo, Cañar, Azuay y Loja.

Comprende las provincias de Orellana, Pastaza, Napo, Sucumbíos, Morona Santiago, Zamora Chinchipe. Se extiende sobre un área de 120.000 km² de exuberante vegetación, propia de los bosques húmedo-tropicales. Sus límites están marcados por la Cordillera de los Andes en la parte occidental de esta región, mientras que Perú y Colombia el límite meridional y oriental, respectivamente. El relieve de la Amazonía está conformado por una serie de colinas que se originan en los Andes orientales y descienden hasta la llanura del Amazonas. Existen dos regiones geográficas: la Alta Amazonía y la Llanura Amazónica. En la primera región se pueden encontrar las cordilleras de Napo Galeras, Cutucú y Cóndor. Los relieves más importantes de la Amazonía se encuentran en la parte norte de la región, cerca al volcán Sumaco, y los más bajos hacia el este de la región.

Las islas Galápagos (también islas de los Galápagos y oficialmente archipiélago de Colón) constituyen un archipiélago del océano Pacífico ubicado a 1.000 km de la costa de Ecuador. Está conformado por 13 grandes islas volcánicas, 6 islas más pequeñas y 107 rocas e islotes, distribuidas alrededor de la línea del ecuador terrestre. Administrativamente, las islas constituyen una provincia de Ecuador, cuya capital es Puerto Baquerizo Moreno (oficialmente, también se le denomina "Región Insular del Ecuador"). El 12 de febrero de 1832, bajo la presidencia de Juan José Flores, las islas Galápagos fueron anexadas a Ecuador. Desde el 18 de febrero de 1973 constituyen una provincia de este país. Se estima que la formación de la primera isla tuvo lugar hace más de 5 millones de años, como resultado de la actividad tectónica. Las islas más recientes, llamadas Isabela y Fernandina, están todavía en proceso de formación, habiéndose registrado la erupción volcánica más reciente en 2009. Todo el archipiélago tiene una extensión total de 8 010 km².

Las principales islas son:

Casi todos los ríos en el del Ecuador nacen en la región de la Sierra y descienden al este hacia el río Amazonas o el oeste hacia el Océano Pacífico. El aumento de los ríos por el deshielo en los bordes de los picos nevados o de las abundantes precipitaciones que caen en las elevaciones más altas. En la región Sierra, los arroyos y ríos son estrechos y el flujo rápidamente en laderas escarpadas. Los ríos pueden lenta y ampliar a medida que cruzan el hoyas llegado a ser rápido una vez más a medida que fluyen desde las alturas de los Andes hasta las elevaciones más bajas de las otras regiones.

En la región de la Costa, la Costa Externa tiene en su mayoría ríos intermitentes que son alimentados por las constantes lluvias de diciembre a mayo y se convierten en cauces vacíos durante la estación seca. Las pocas excepciones son los más largos, ríos perennes que fluyen a lo largo de la Costa del Externa Interna Costa y la Sierra en su camino hacia el Océano Pacífico. La Costa Interna, por el contrario, es atravesado por los ríos perennes que pueden inundarse durante la temporada de lluvias, a veces formando pantanos.

El sistema del río Guayas, que fluye hacia el sur hasta el Golfo de Guayaquil, constituye el más importante de los sistemas de drenaje en el interior de Costa. La Cuenca del Río Guayas, incluida la tierra drenada por sus afluentes, es de 40.000 kilómetros cuadrados de superficie. El río Guayas, de sesenta kilómetros de largo nace, al norte de Guayaquil en la confluencia de los ríos Babahoyo y Daule. En pocas palabras constreñido a Guayaquil por las colinas, el Guayas se amplía al sur de la ciudad y fluye a través de una red de pequeñas islas del delta y los canales. En su desembocadura, el río forma un amplio estuario con dos canales en torno a Isla Puná, la más profunda de lo que se utiliza para la navegación.

El segundo gran sistema fluvial Costa del Esmeraldas, se levanta en la Hoya de Guayllabamba en la Sierra como el río Guayllabamba y fluye hacia el oeste para desembocar en el Océano Pacífico al este de la ciudad de Esmeraldas. El río Esmeraldas es de 320 kilómetros de largo y tiene una cuenca de drenaje de 20.000 kilómetros cuadrados.

Los principales ríos en el Oriente incluyen el Pastaza, Napo y Putumayo. El Pastaza está formado por la confluencia de los ríos Chambo y Patate el, ambos nacen en la Sierra. El Pastaza incluye la cascada de Agoyán, que a los sesenta y un metros es la cascada más alta de Ecuador. El Napo se levanta cerca del monte Cotopaxi y es el río principal utilizado para el transporte en las tierras bajas orientales. El Napo rangos de ancho de 500 a 1.800 metros. En su curso superior, el Napo fluye rápidamente hasta la confluencia con uno de sus principales afluentes, el río Coca, donde se hace más lento y se nivela. El Putumayo forma parte de la frontera con Colombia. Todos estos ríos desembocan en el río Amazonas.

Las Islas Galápagos no tienen ríos importantes. Varias de las islas más grandes tienen, sin embargo, fuentes de agua dulce.



Debido a la presencia de la cordillera de los Andes y según la influencia del mar, el Ecuador continental se halla climatológicamente fragmentado en diversos sectores. Además, a causa de su ubicación ecuatorial, cada zona climática presenta sólo dos estaciones definidas: la húmeda y la seca, llamadas erróneamente «invierno» y «verano» respectivamente, al igual que ocurre en otras regiones del globo donde por sus emplazamientos próximos a la línea ecuatorial, no ocurren verdaderos inviernos y veranos.

Tanto en la Costa como en el Oriente, la temperatura oscila entre los 20 °C y 33 °C, mientras que en la sierra, esta suele estar entre los 3 °C y 26 °C. La estación húmeda se extiende entre diciembre y mayo en la costa, entre noviembre a abril en la sierra y de enero a septiembre en la Amazonía.
Galápagos tiene un clima más bien templado y su temperatura oscila entre 22 y 32 °C, aproximadamente.

Estas estaciones húmedas y secas causan en cada región del país diferentes estaciones climáticas. Son muy variables las temperaturas.

Así, de enero a marzo es principalmente estación seca en la sierra, mientras que en la costa y amazonía es temporada húmeda, con la mayoría de días nublados.

Del modo contrario, de julio a septiembre en la sierra es temporada húmeda, mientras que en la costa, seca.











</doc>
<doc id="15564" url="https://es.wikipedia.org/wiki?curid=15564" title="Arte de la Antigua Grecia">
Arte de la Antigua Grecia

La historiografía del arte ha identificado varios estilos que periodizan el arte de la Antigua Grecia:

El periodo arcaico se inicia a finales del siglo VIII a. C. y abarca hasta comienzos del siglo V a. C. En este periodo se produce una expansión de la polis griega, instaurándose un nuevo orden ciudadano, con la tiranía como marco político principal, sistema que pronto desaparecerá frente al ideal igualitario de ciudadanía del siglo V a. C. La legitimación de este tipo de mandato ciudadano supone la promoción de grandes obras públicas, representativas del prestigio del tirano, quien apoya la creación de edificios civiles y religiosos en las ciudades donde gobierna, para lo cual manda remodelar su entramado urbano. Esta actuación tuvo como objeto otorgar a cada urbe una identidad propia, al tiempo que mostrar su preponderancia sobre el resto de ellas. Consecuentemente, el arte desempeña en esta etapa un nuevo papel propagandístico de la tiranía, cuyos gobernantes lo utilizan para justificar su poder escasamente legitimado. A partir del siglo VI a. C. el centro político de la polis se convierte en un lugar de gran relevancia artística, convirtiéndose la plaza pública o ágora en el corazón de las actividades cívicas de la sociedad. Entre todas ellas sobresale la de la ciudad de Atenas, impulsada por el legislador Solón y monumentalizada en la época de los Pisistrátidas.

El culto religioso desempeñó también un papel fundamental en la sociedad griega de este periodo, de manera que todas aquellas ciudades que dispusieron de medios económicos suficientes promovieron la construcción de edificios religiosos en piedra, los cuales cumplieron un importante papel a la hora de cohesionar las diferentes clases de la nueva sociedad, menos igualitaria que la de siglos anteriores. Se crean ahora santuarios panhelénicos, como Delfos y Olimpia, donde los distintos tiranos realizan grandes ofrendas votivas para exhibir su poder, y se fomentan nuevos cultos populares, al tiempo que surgen mitos relacionados con dioses y héroes locales, lo que incrementa las identidades políticas de las distintas polis que necesitan sentirse independientes y destacar sobre el resto.

La arquitectura griega fijó las formas del templo, que se fue desarrollando en las acrópolis (ακρόπολις) o ciudadelas elevadas de cada ciudad; así como en los santuarios panhelénicos. Los propiamente "panhellénikós" (πανελληνικός -"de todos los griegos"-), celebraban juegos ("agónes" αγώνες -"contienda", "desafío", "disputa"-), donde competían atletas y aurigas en representación de sus polis, en una sublimación de la violencia en lo sagrado que convertía a los vencedores en héroes o semidioses, por lo que adquirían el derecho a ser representados en estatuas; y acumulaban riquísimas ofrendas, guardadas en lujosos edificios, levantados a costa de cada "polis" (los "thesaurós" θησαυρός). Aunque había muchos otros juegos en honor de otras divinidades o en otras polis (como los Panatenaicos de Atenas), se destacaban cuatro, no por el premio ofrecido (unas olivas, o una corona de hojas de laurel), sino por el prestigio que daba la concurrencia periódica (cada dos o cuatro años) de gentes de toda la Hélade: el de Apolo en Delfos (donde se celebraban los oráculo de Dodona), el de Zeus en Olimpia (del que solo quedan ruinas, donde se celebraban los Juegos Olímpicos), el de Poseidón en Istmia (del que solo quedan los cimientos, donde se celebraban los Juegos Ístmicos) y el de Zeus en Nemea (del que quedan unos restos de época helenística, donde se celebraban los Juegos Nemeos). Sin ser estrictamente "panhelénicos", también alcanzaron un enorme prestigio en toda la Hélade otros santuarios: el de Hera en Samos (Ἥραιον, "Heraion", el primer gran ejemplo de orden jónico -Reco y Teodoro de Samos-, donde se celebraba la eclesiástica "hierogamia" ἱερός γάμος) o el de Artemisa en Éfeso (Ἀρτεμίσιον, "Artemision", el segundo gran ejemplo del orden jónico, que entró en el catálogo de las siete maravillas del mundo).

La lista de los templos importantes sería inacabable (templo de las Musas en Helicón -de hecho, todo el monte Helicón estaba dedicado a ellas, al igual que el monte Parnaso, pero de un modo más tangible a la forma en que el monte Olimpo lo estaba a los principales dioses-, templo de Démeter en Eleusis, templo de Apolo en Dídima, templos de Poseidón -en Halicarnaso, en Ege, en Calauria, en Atenas-, templo de Artemisa -en Carje, en Esparta-, templos de Afrodita -en Cnido, en Lindos, en Citerea-, templos de Hermes -en Imbros, en Samotracia, en Lemnos-, templos de Hera -en Micenas, en Argos, en Figalia, en Esparta-, templo de Ares en Esparta, templos de Dionisos -en Naxos, en Chios, en Atenas-, templos de Asclepio -en Cos, en Epidauro, que alcanzarían mucho mayor prestigio en épocas posteriores-), algunos de ellos formando una relación espacial definida, como el "Triángulo Sagrado" entre el "Parthenón" (Παρθενών -templo "de la virgen", es decir, de Atenea-, en la Acrópolis de Atenas), el "Soúnion" (Σούνιονy, en el promontorio desde el que Egeo se arrojó al mar) y el templo de Afaia en Egina.

La forma del templo griego derivaba del "megaron" (μέγαρον) micénico: esencialmente una planta rectangular cubierta con tejado a dos aguas, con los elementos estructurales de madera. Con la misma estructura se han encontrado restos de un templo de la Época Oscura en Lefkandi (Eubea), y los primeros restos encontrados del "Heraion" de Samos (mediados del siglo VIII a. C.) son similares. La "petrificación" de los elementos del templo se fue produciendo paulatinamente (columnas -cuyo fuste mantiene el recuerdo vegetal con las estrías o el acanalamiento-, vigas -que producen los remates exteriores de triglifos y metopas-, arquitrabes, cornisas, etc.), siendo el ejemplo más evidente el "Heraion" de Olimpia (hacia el 600 a. C.). Una de las razones que impulsaron el cambio fue la generalización de las tejas de cerámica en sustitución de la cubierta de paja y ramas, y que se produjo en Corinto en el siglo VII a. C. Uno de los primeros fue el "Thermón" (Θερμον, templo de Apolo en Thermos, Etolia, hacia el 630 a. C.). El peso, muy superior, obligaba a disminuir la pendiente del tejado, y terminó por definir las proporciones definitivas del frontón que resultan tan armónicas. En las distintas zonas de la Hélade se definieron los estilos dórico (más sobrio y macizo) y jónico (más esbelto y decorativo).

La escultura griega de época arcaica, influenciada notablemente por la egipcia, se caracterizó por rasgos originales, como la sonrisa eginética o arcaica (llamada así por exhibirse en la figura de un famoso guerrero moribundo del Templo de Afaia en Egina); que se fueron transformando, al final del periodo (últimas décadas del siglo VI y primeras del V a. C.), en un estilo de transición al clasicismo denominado estilo severo, estimulado finalmente por la necesidad de renovar la decoración escultórica de los templos destruida durante la invasión persa.

Las figuras masculinas ("kuroi", en singular "kuros" κοῦρος) y femeninas ("korai", en singular "kore" κόρη) podían representar tanto a seres humanos como a dioses, muestra de la antropomorfización de estos y de la elevación al rango semidivino o heroico de aquellos (particularmente, del prestigio que alcanzaban los vencedores en los juegos panhelénicos).

Las primeras esculturas eran las "xoana" (ξόανα, en singular "xoanon" ξόανον), de madera, representaciones muy simplificadas del cuerpo humano adaptadas a la forma cilíndrica del tronco de un árbol. Fueron sustituyéndose por figuras talladas en mármol (especialmente prestigiosa fue la cantera del Pentélico) y las fundiciones de bronce. Dada la posibilidad de reutilizar este material tan caro, han sido muy pocas las que se han conservado. De mucho menor coste eran las figurillas de terracota, que se producían a escala industrial, mediante moldes.

Además de las posibilidades texturales que ofrecen los distintos materiales y técnicas de acabado, aprovechadas de forma limitada en la época arcaica, fue la policromía aplicada sobre las esculturas la que las dotó de luminosidad y sensación de vida. Los antiguos griegos no hubieran concebido que una escultura se dejase sin pintar, la considerarían imperfecta o inconclusa. Incluso la inevitable pérdida de los colores por el paso del tiempo, que el gusto romántico considera un incremento del interés estético, era considerada como un deterioro esencial.

Tras un inicial periodo geométrico (siglos IX y VIII a. C.), al que siguió un período orientalizante (siglos VII y primera mitad del VI a. C.) en el que se detecta la influencia asiria y de otras civilizaciones del Antiguo Oriente (por la importancia y difusión que alcanzaron en esta época los talleres de Corinto se habla de estilo protocorintio); la cerámica griega fue evolucionando sus formas, que hacia el final del siglo VI a. C. alcanzaron un alto grado de refinamiento expresivo, respondiendo a un amplio conjunto de necesidades refinadas de la vida cotidiana de las clases altas, y a la demanda de productos de lujo fácilmente exportables a todo el espacio mediterráneo, e incluso a lejanos lugares en el centro de Europa.

La producción en muchas de las colonias fundadas en esos siglos fue tan importante como la de las metrópolis. Además, la influencia de la cerámica griega se dejó notar en la producción local de los pueblos indígenas, especialmente en la cerámica etrusca (que tiene tipologías verdaderamente sincréticas, como es el caso de la hidria ceretana o hidria de Caere) o en la cerámica ibérica.

Se aprovecharon extensamente las posibilidades que las distintas tipologías de vasos daban en ciertas partes de su superficie (fondos de las copas, vientres y cuellos de las ánforas, etc.) para ejercer como soporte para la pintura griega, que se expresó sucesivamente en dos estilos principales, denominados "cerámica de figuras negras" y "cerámica de figuras rojas".

Cada escuela local de ceramistas se distinguió por un estilo local característico, aunque se influyeron mutuamente.

Comenzó a ser común que los ceramistas y, menos frecuentemente, los pintores firmaran sus obras (Clitias, Exequias, Psiax, Eufronio), lo que se interpreta como una valoración social de su trabajo, implicando un concepto muy moderno de la función del arte y del artista, en un momento en que el trabajo manual estaba degradándose en su consideración, vinculada a la de los esclavos. Es habitual que solo se conozca el nombre del ceramista, con lo que el pintor se denomina por este (Pintor de Andócides, Pintor de Amasis, Pintor de Antimenes, Pintor de Taleides). En otras ocasiones solo se ha podido establecer la identidad común de un maestro por sus obras (Pintor de Príamo, Pintor de Neso, Pintor de las cabezas de caballo, Pintor de Aqueloo) o por los lugares donde se han encontrado (Maestro del Dípilon) o los museos y colecciones particulares donde se conservan (Maestro o Pintor de Madrid, Maestro o Pintor de Princeton, Maestro o Pintor de Edimburgo, Maestro o Pintor de Rycroft, Maestro o Pintor de Castellani). A algunos se les agrupa por sus características comunes (Pequeños maestros, Grupo de Leagro, Grupo Perizoma, Grupo de las tres líneas, Grupo pionero -este último ya a comienzos del siglo V a. C.-).

También hubo pintura sobre paneles y muros, que no se ha conservado a excepción de muy pocos restos, como los "Paneles de Pitsa" (descubiertos en una cueva de Sición, la localidad al norte del Peloponeso, cerca del Golfo de Corinto, donde la tradición consideraba que se había inventado la pintura sobre paneles -"pinax" πίναξ, plural "pinakes" πίνακες; de donde viene la palabra "pinacoteca"-). o los frescos de la Tumba del nadador, en Posidonia (Magna Grecia). La influencia etrusca de esta tumba es evidente; aunque a su vez la pintura etrusca había recibido una notable influencia griega durante los siglos VII y VI a. C..

El arte antiguo griego ha perdurado en la forma de esculturas y arquitectura; también en artes menores como el diseño de monedas, el grabado de alfarería y gemas.

Los griegos, como la mayoría de las culturas europeas, consideraban la pintura como una de las formas más altas de arte. Las obras de Polignoto de Tasos, que trabajó en el siglo V a. C., seguían siendo admiradas incluso 600 años después de su muerte, como después ocurrió con las de Leonardo da Vinci o Miguel Ángel, sin embargo en este caso no solo no se han conservado ninguna de sus obras sino tampoco ninguna reproducción.

Los pintores griegos trabajaron generalmente sobre paneles de madera, que se estropeaban rápidamente (a partir del siglo IV a. C.), cuando no eran bien protegidas. Hoy en día no queda casi ninguna pieza de pintura griega, excepto algunos restos de pinturas en terracota y de algunas pinturas en las paredes de tumbas, sobre todo en Macedonia e Italia. De las obras maestras de la pintura griega tenemos solamente algunas copias realizadas en las épocas romanas, la mayoría de ellas son de una calidad inferior.

Con anterioridad a la formación del arte griego en sí hubo en territorios de la antigua Grecia un arte que se ha llamado "prehelénico", conservadas tan solo en ruinas de edificios de la época y sobre estuco, representando paisajes, acciones guerreras y ceremonias cortesanas o religiosas cuyas figuras aunque imperfectas revelan notable expresión y vida. En las decoraciones de vasijas se presenta raras veces la figura humana y siempre estilizada y de escasos detalles.
En cuanto a la pintura griega, el conocimiento de sus artistas se debe casi por entero a los antiguos historiadores, pues no se conserva de ella ni un solo cuadro ni se conoce obra alguna de los famosos Zeuxis, Parrasio y Apeles, considerados desde la antigüedad los pintores por antonomasia. Las obras pictóricas griegas que al presente se conocen y conservan consisten únicamente en decoraciones de ánforas y de otras elegantes vasijas salvo algunos mosaicos de pavimento y placas de arcilla pintadas y sin contar las obras de pintura romana en que intervino mano griega. Consta, no obstante, que los griegos pintaron cuadros excelentes, por lo menos murales (cuyas copias pueden ser algunas decoraciones de las grandes ánforas de lujo) y que emplearon los procedimientos al fresco, al encausto, al temple y quizás al óleo. Los asuntos representados en tales pinturas, a juzgar por lo que se observa en las mencionadas vasijas, fueron escenas de la vida humana y tradiciones o leyendas mitológicas y heroicas.

La gran mayoría de edificios griegos no han perdurado, debido a varias razones: fueron destruidos en guerras, saqueados para obtener materiales de construcción o abatidos por terremotos. Solamente un puñado de templos, tales como el Partenón y el templo de Hefesto en Atenas. De las cuatro maravillas del mundo creadas por los griegos las cuales ninguna de ellas han perdurado:

La Antigua Grecia destacaba en la arquitectura. Son los templos la construcción más representativa de la arquitectura griega, ya que su principal era brindarle protección o albergue a una deidad o dios. En la parte de adentro se encontraba el efigie de la deidad mientras tanto en la parte exterior se le rendía culto. Los griegos utilizaban el pretexto religioso para justificar con esto sus grandes creaciones de esta forma podían desatar su imaginación y crear maravillosas estructuras. Considerado como una arquitectura sublime esto sirvió de ejemplo para la construcción de otras estructuras griegas; por esto los edificios griegos comparten similares características. 

La diferencia principal del templo arcaico al templo pre-helénico es que se hace el alma de la ciudad, es decir, antes los palacios eran, a su vez, refugios para los ciudadanos en caso de guerra, en la época arcaica son los conjuntos de templos, es decir la acrópolis, es la casa del dios y el refugio de los ciudadanos, ya que estaban situados en una colina y además estaban fortificados, probablemente para las utilidades de antes.

Las ceremonias, cualquiera pese a su importancia, se realizan fuera del templo para que el olor de los sacrificios llegase a la estatua divina para que esta se lo agradeciese y les diese buenas cosechas, etc.

Al principio los templos son muy pequeños y apenas se diferencian de una casa, pero con el tiempo, además de la sustitución de elementos blandos por rocas o sillares, se establecen los órdenes: el dórico, el jónico y el corintio son los tres órdenes, los dos primeros surgen al principio de la época mientras que el corintio se origina después derivando del jónico.
Además la arquitectura griega es adintelada o arquitrabada por la viga que se pone en el pórtico llamada dintel. Ignora los arcos y otros tipos de arquitectura.

Si el templo está rodeado de columnas, se le llama períptero y si las columnas se limitan al pórtico, se llama próstilo, según las columnas que tenga el pórtico, será dístilo (dos columnas), tetrástilo (cuatro), y así sucesivamente siempre siendo pares.


El arquitrabe pasa a tener tres bandas horizontales que se llaman "fasciae" y que rebasan cada una a su inmediata inferior.
El friso es un espacio liso dedicado a realizar esculturas en él. Los demás elementos son iguales que el dórico.

Todas las esculturas y obras de arquitectura que han perdurado, solo son una pequeña muestra de la inmensa colección de obras griegas. Muchas esculturas de dioses paganos fueron destruidas durante la era cristiana. Desgraciadamente, cuando se calcina el mármol se produce la cal, y ese era el destino de muchas obras de mármol griegas durante la Edad Media. Durante ese mismo período, debido a la escasez de metales, la mayoría de las estatuas de bronce eran fundidas.Actualmente muchas de las obras que hoy tenemos son copias romanas.

La escultura de la Antigua Grecia alcanzó el ideal de la belleza artística hasta donde pudo llegar por sí solo el ingenio humano. Aunque Grecia floreció en todas las Bellas Artes, ninguna le distingue tanto como la escultura. 

Cultivó el arte de la Antigua Grecia todos los géneros de escultura, adoptando con predilección el mármol y el bronce como material escultórico y tomando como asuntos principales los mitológicos y los guerreros a los cuales añadió en su última época el retrato de personajes históricos. 

Forman su característica en los mejores tiempos del Arte (los de Fidias) la expresión de la realidad idealizada, la regular proporción orgánica, el alejamiento de lo vago y monstruoso, la precisión en los contornos y detalles, la armonía y belleza en las formas y la finura en la ejecución.

Suele dividirse la escultura griega en cuatro periodos históricos bien delimitados a los cuales precede el protohistórico o minoico y micénico. En este, se desarrolló por espacio de unos veinte siglos (desde el año 3000 al 1100 a. C. aproximadamente) un arte rudimentario pero lleno de vida y movimiento que modeló el barro y trabajó la piedra, el marfil, el hueso e incluso el oro, el plomo y el bronce, produciendo relieve, grabados, entalles mitológicos en piedras finas y pequeñas estatuas e idolillos. Aunque labrados con cierta tosquedad, se presentan a veces con admirable corrección en el dibujo que parece recordar el arte de los cazadores del reno los cuales pudieron tener con la civilización egea algún lazo histórico.

Los cuatro períodos arqueológicos que tras un prolongado silencio artístico siguieron al micénico se distinguen del siguiente modo:


A partir del período arcaico del arte griego, las cerámicas pintadas y las esculturas son casi las únicas formas de arte que han perdurado. La pintura estaba en sus inicios durante aquel período, y ningún ejemplo ha perdurado. Aunque las monedas fueron inventadas en el siglo VII a. C., no eran comunes en la mayor parte de Grecia hasta el siglo V a. C.

De este período destaca la elaboración de cerámicas para uso cotidiano, o de carácter fúnebre, donde se emplearon grandes jarrones. Estos jarrones estaban ornamentados con representaciones lineales, y motivos relacionados con la muerte, como batallas marítimas o terrestres. La mayor parte de la alfarería está compuesta por piezas domésticas, de las que perduraron recipientes tales como las ánforas, pequeñas cráteras e hidrias. Por otra parte, de la cerámica funeraria se han encontrado varias urnas. También se fabricaron figurillas en barro cocido, principalmente para ser depositadas como ofrenda en los templos. Durante el período helenístico, fue elaborada una gran variedad de objetos de alfarería, aunque solo algunas poseen valor artístico.

Durante los períodos más antiguos, hasta las pequeñas ciudades griegas producían objetos de alfarería para el mercado local, siendo sus estilos y modelos muy variados. Entre los años 550 y 480 a. C. el arte en cerámica sufrió una gran transformación; además, los autores incluyeron sus nombres, el nombre del alfarero o del pintor que decoraba aquellas piezas (también existían algunos artistas que practicaban ambos labores). La cerámica ática y cerámica corintia destacaron por sobre las demás. Atenas creó las primeras representaciones del estilo bello: recipientes con figuras rojas sobre fondo negro.

La historia de la cerámica griega antigua está subdividida en los siguientes períodos:

La gama de colores que podía ser utilizada sobre la alfarería fue restringida por las técnicas de cocción: negro, blanco, rojo y color amarillo eran los colores más comunes. Durante los tres primeros períodos, las cerámicas guardaban su color natural claro con algunos motivos negros.

Uno de los signos más fácilmente reconocibles de los logros artísticos griegos es su agraciada arquitectura, caracterizada por las elegantes columnas de piedra y los frontones triangulares esculpidos de los tres estilos arquitectónicos que se desarrollaron entre el 600 y el 300 a. C.

Estos estilos fueron creados para construir más templos a los dioses que eran muy importantes para ellos. Esculpidos en mármol, ellos imitaron las técnicas de corte de la madera de los edificios hechos originalmente en este material.

El estilo dórico es el más antiguo y el más simple, con columnas firmes y frentes cubiertos con esculturas que, al mismo tiempo, podían pintarse de rojo o azul para generar impacto; cabe destacar que no tiene base comparado con otros estilos. El mejor ejemplo superviviente de un templo dórico es el Partenón (438 a. C.) en la Acrópolis de Atenas.

El estilo jónico apareció alrededor del mismo tiempo en las ciudades más ricas de Asia Menor. Produce la sensación de más ligereza y es más decorativo, con columnas esbeltas destacando volutas ensortijadas en cada esquina del capitel. El estilo alcanzó su apogeo en el desaparecido Templo de Artemisa en Éfeso, una de las Siete Maravillas del Mundo. Se puede admirar la arquitectura jónica en el Templo de Atenea Niké en la Acrópolis.

Hacia el año 400 a. C. surgió una nueva versión, más elaborada, de la arquitectura jónica: la corintia. Se caracterizaba por intrincadas hojas espinosas de acanto esculpidas en los capiteles de las columnas, que puede reflejar la influencia del Oriente Medio. La prestancia del estilo corintio lo convirtió en el estilo arquitectónico favorito de la arquitectura del Imperio romano.
Los templos se pueden clasificar por el número de columnas que tienen:

-"In antis", si solo tienen dos en su fachada y muros de la cella.

-Tetrástilo: cuatro.

-Hexástilo: seis.

-Octástilo: ocho.

-Decástilo: diez.

-Próstilo: si solo tiene un pórtico en la parte delantera.

-Anfipróstilo: si lo tiene también en la parte posterior

-Períptero: cuando las columnas exentas rodean la cella.

-Díptero: cuando son dos las filas de columnas.

-Pseudoperíptero: cuando está dispuesto con columnas adosadas a los lados.

-Áptero: si no tiene columnas.

-Hípetro: si no tiene techo.





</doc>
<doc id="15565" url="https://es.wikipedia.org/wiki?curid=15565" title="Arte de la Antigua Roma">
Arte de la Antigua Roma

Arte romano son todas aquellas manifestaciones de las artes visuales que fueron exportadas a todos los territorios del Imperio romano. Las primeras manifestaciones surgieron bajo el influjo del arte etrusco y fueron después influenciadas por el arte griego, que los romanos conocieron en las colonias de la Magna Grecia, ubicadas en el sur de Italia y que conquistaron en el proceso de unificación territorial de la península durante los siglos IV y III  a. C. La influencia griega se acrecienta cuando, en el , Roma ocupa Macedonia y Grecia. 

Hasta cierto punto podría pensarse que el arte de Roma es una imitación y ampliación del arte griego, y por supuesto del arte etrusco, pero el espíritu que animó a los artistas romanos es totalmente distinto de aquellos. La Roma conquistadora y urbanista trató de unir al sentido estético griego, el carácter utilitario y funcional que sus obras requerían.

Desde el punto de vista cronológico, el arte romano se desarrolló con bastante homogeneidad y autonomía desde el hasta el siglo V. Siguiendo las etapas que su devenir histórico marca, destacan al menos la República, hasta el año 27 a. C., y el Imperio, que se extendió desde los tiempos de Augusto hasta la caída de Roma en manos de los bárbaros en el año 476.

A causa del profundo centralismo ejercido por Roma sobre sus provincias en todos los aspectos de la vida, se originó un arte muy uniforme sin que pueda hablarse de escuelas provinciales, al menos durante la época imperial. No obstante, dada la amplitud del Imperio y su constitución en diferentes momentos, no existe una contemporaneidad cronológica, pues en las zonas orientales donde el arte helenístico está más consolidado sus formas artísticas están mucho más evolucionadas que en las provincias occidentales más tardíamente incorporadas a la cultura romana.

La Monarquía romana, República romana e Imperio romano, cubren el periodo desde el siglo VIII a. C. al . Se localiza primero en el Lacio (Italia Central), y se acabará extendiendo por toda la Cuenca del Mediterráneo ("Mare Nostrum"). 

En el periodo anterior a la recepción de la cultura helenística (siglo III a. C.) se desarrolla un arte latino emparentado con otros pueblos itálicos (sabinos y sobre todo etruscos), por ejemplo, la Loba capitolina . 

El período clásico del arte romano dura hasta el triunfo del cristianismo (siglo IV). Asimila y desarrolla la cultura griega (órdenes arquitectónicos, diseño de los templos, concepción escultórica), incorporándole características propias, tanto en materiales de construcción (mortero, cemento y hormigón romanos) como en elementos arquitectónicos (el arco -Arco de triunfo- y la bóveda, orden toscano y orden compuesto, principio de superposición de órdenes) y formas escultóricas (el retrato romano -exigido por el culto a los antepasados y la propaganda política, y que permite datar la evolución estilística y de la moda, sobre todo en la expresión y el peinado- ya el relieve romano, caracterizado por la búsqueda de la profundidad y la perspectiva) y pictóricas (los estilos pompeyanos, decorativos, narrativos o procurando el trampantojo).
Desarrollo arquitectónico con gusto por lo colosal y magnificente, al tiempo que con un acusado sentido práctico y utilitario. (puentes y acueductos -puente de Alcántara, Pont du Gard, Acueducto de Segovia-, calzadas). 

Edificios públicos (termas -termas de Caracalla-, teatro romano -Teatro Marcelo-, circo romano -Circo Máximo-, anfiteatros -Anfiteatro de Capua, Coliseo, Anfiteatro de El Djem-, etc.), religiosos (templo romano -Templo de Vesta, Maison Carrée, Panteón de Agripa-) y civiles (foro romano, basílicas, palacio romano -Domus Aurea de Nerón, construcción original del Palacio de Letrán, luego convertido en residencia papal-, villa romana -Villa romana del Casale- con su versión de villa imperial -Villa Jovis o de Tiberio en Capri, Villa Adriana-, casa romana -domus, vivienda (Roma Antigua)-). Escultura histórica narrativa (frisos corridos en relieve: Ara Pacis, Columna trajana), bustos, estatuas de cuerpo entero (Augusto de Prima Porta) y excepcionalmente ecuestres, reservadas a los emperadores (estatua ecuestre de Marco Aurelio). 



</doc>
<doc id="15568" url="https://es.wikipedia.org/wiki?curid=15568" title="Pintura de Francia">
Pintura de Francia

Con el nombre de pintura francesa puede designarse a toda la pintura realizada en lo que actualmente es Francia. 

Sus primeras manifestaciones se dieron en el arte prehistórico, dentro del arte rupestre. No se presenta en todo el territorio francés, sino que se concentra en determinadas zonas, sino allí donde las piedras calizas y areniscas ofrecen paredes adecuadas. En la zona más rica se localiza en el Périgord, pueden encontrarse las cuevas más célebres: Lascaux, Combarelles, Font-de-Gaume, Gabillou o Rouffignac. Los Sitios prehistóricos y grutas decoradas del valle del Vézère están declaradas Patrimonio de la Humanidad en Europa por la Unesco.

Hay otra zona en los Pirineos franceses, como las de Massar (Ariège), Gourdan (Alto Garona), Bruniquel (Tarn y Garona), y otras en los departamentos de Bajos Pirineos y Altos Pirineos.

Son pinturas realizadas en las cuevas, representando animales (el caballo y el bisonte, principalmente), así como figuras humanas y signos. Su finalidad aún no se ha determinado plenamente, habiéndose interpretado como un arte religioso.

A comienzos del siglo XVII persisten las tendencias de la segunda escuela de Fontainebleau. El retorno de Simon Vouet, príncipe de la Academia de San Lucas de 1624 a 1627, en 1627 marca el comienzo de la recuperación de la pintura francesa. Este pintor es considerado el más propiamente barroco.

El naturalismo de origen caravaggesco queda representado en la obra de Valentin de Boulogne († 1634), el famoso tenebrista Georges de La Tour († 1652) que desarrolla su labor en la corte de Lorena y en las escenas campesinas pintadas a la manera de una escena de género por los hermanos Le Nain: Antoine, Louis y Matheo.

Los grandes maestros del clasicismo son Nicolas Poussin (1594-1665), pintor de temas mitológicos e históricos, y Claude Lorrain (1600-1682), destacado paisajista que influyó en el romanticismo e incluso en los orígenes del impresionismo. Ambos residen en Roma, pero reciben continuos encargos para su país. Trabajan en el problema dominante de la expresión de la perspectiva atmosférica. Poussin desempeña un papel decisivo en la rápida perfección de la escuela francesa en su breve vuelta a París (1640-1642).

En la corte francesa de Luis XIII y Luis XIV se cultivó con profusión el retrato. Inició el género el flamenco Philippe de Champaigne (1602-74), con representaciones de los personajes cortesanos en todo su esplendor y que en sus retratos laicos alcanza una expresión más mundana; fue continuado por retratistas que alcanzan ya el siglo XVIII: Hyacinthe Rigaud (1659-1747) y Nicolas de Largillière (1656-1746), quienes restituyen al retrato su calidad plástica, pero con una búsqueda de suntuosidad y elocuencia que excluye la profundidad de análisis.

En la vida pictórica de este siglo destaca la creación de la Academia Real de Bellas Artes (1648), para superar la vieja corporación de pintores, como un gremio u oficio, propugnando en cambio que se contemple como un "arte liberal". Charles Le Brun fue el pintor académico por excelencia, pintor del rey desde 1664, que ejerce una auténtica tiranía artística. Le Brun alcanza el ideal del pintor gran señor y amigo del soberano. Junto a él cabe mencionar al retratista cortesano Pierre Mignard, que hace tender el retrato hacia una fórmula graciosa y vacía.

Antoine Coypel y Charles de la Fosse († 1716) son los últimos representantes de las tendencias barrocas de inspiración italiana.

En este siglo predomina el rococó, unas pinturas llenas de viveza y encanto típicamente francés, con nombres como los de Watteau, Boucher o Fragonard.

A comienzos de siglo, continúa trabajando Hyacinthe Rigaud, cuyo Retrato de Luis XIV, conservado en el Museo del Louvre suele considerarse la imagen más representativa del Gran Siglo. Le Brun sigue marcando las tendencias desde la Academia, institución que goza de gran estabilidad. Aunque los jóvenes artistas siguen yendo a formarse a Roma, se produce un cierto desplazamiento, fijándose más en las obras que se realizan en Venecia.

Las formas del estilo clásico dan paso, en el reinado de Luis XV, al estilo rococó. Su representante más antiguo es Antoine Watteau (1684-1721), creador del género de las "fêtes galantes" («fiestas galantes»). François Boucher (1703-70) es el pintor de la sensualidad, de los desnudos femeninos. Finalmente, Jean-Honoré Fragonard (1732-1806) compagina la realización de escenas galantes y otras más sentimentales que preludian el romanticismo.

Este tono sentimental y algo lacrimoso se evidencia en la obra de Jean-Baptiste Greuze (1725-1815).

Continúa cultivándose el retrato palaciego, poniéndose de moda la técnica del pastel. Nattier († 1766) es el pintor de las damas de la nobleza, con colores claros y representaciones alegóricas. Maurice Quentin de La Tour († 1788), es el más grande pastelista del siglo, con gran penetración psicológica. Finalmente, Jean Siméon Chardin (1699-1779) cultiva el bodegón, y escenas de inspiración holandesa.

Es el siglo de la gran pintura francesa. París se convierte en un referente artístico de primer orden. Es la ciudad a la que los pintores de toda Europa viajan a formarse, sucediendo de este modo a Roma. Los grandes movimientos artísticos surgieron en la capital gala: romanticismo, realismo, impresionismo, postimpresionismo.
Entre los siglos XVIII y XIX se desarrolla el neoclasicismo, como reacción a los excesos rococós. Encarna los ideales de la Ilustración y se convierte en el arte de la Revolución francesa primero y del Imperio Napoleónico después. 

El artista más destacado es Jacques-Louis David (1748-1825), que en 1784 había presentado "Juramento de los Horacios." Como pintor napoleónico destaca en "La coronación de Napoleón I en Notre Dame" (1805-7).

Antoine-Jean Gros (1771-1835) sigue a Napoleón en sus campañas, siendo su obra más conocida "Napoleón visitando a los apestados de Jaffa" (1804).
Jean Auguste Dominique Ingres (1780-1867) es neoclásico, aunque se nota la influencia del romanticismo en cierta tendencia orientalizante ("La Odalisca"). 

El romanticismo se nota ya en un discípulo de David, François Gérard (1770-1837), que pinta retratos al estilo sentimental de la nueva época. 

Los pintores románticos franceses más destacados fueron Pierre Proudhon (1758-1823); Théodore Géricault (1791-1824), cuya obra más conocida es "La balsa de la Medusa"; y Eugène Delacroix (1798-1863), con obras como "Las matanzas de Quíos" y "La muerte de Sardanápalo". 

Ya desde 1831 se aprecia una evolución hacia el realismo, con obras que reflejan un paisaje realista: Camille Corot (1796-1875), pintor de transición entre el paisaje clásico y el realista. Realistas son también los paisajistas de la Escuela de Barbizon. 

El realismo testimonial, que refleja la vida cotidiana del pueblo, viene representada por autores como:

En 1874 se celebra en Francia la primera exposición colectiva de los impresionistas. Es considerado el movimiento más importante en la pintura de las últimas décadas del siglo XIX. 

Édouard Manet (1822-83) es considerado un precursor del movimiento; su obra más conocida es "Le Dejeuner sur l’herbe (Almuerzo sobre la hierba)". 

El cuadro que dio nombre a este movimiento fue "Impresión: sol naciente", de Claude Monet (1840-1936), presentado en la primera exposición colectiva (1874). Dentro del movimiento impresionista pintaron, además, Renoir (1841-1919), Camille Pissarro; Alfred Sisley (1839-99), más bien paisajista; Edgar Degas, que pinta escenas urbanas con luz artificial; Berthe Morisot y Paul Cézanne (1839-1906).

Con este nombre se conoce a un grupo heterogéneo de artistas que pintan entre 1886 y 1907, entre la última exposición impresionista y el surgimiento del cubismo. 

Los principales artistas postimpresionistas fueron: 

Una corriente particular dentro del postimpresionismo es el puntillismo o “divisionismo”, que aparece por primera vez en el Salón de los Independientes de 1884, encabezado por los pintores neoimpresionistas Georges Seurat (1859-1891) y Paul Signac (1863-1935).

Por su parte, el simbolismo se inició en las últimas dos décadas del siglo, con pintores como Gustave Moreau (1826-1898) y Pierre Puvis de Chavannes (1824-1898).

En esta misma época se agrupan una serie de pintores bajo la denominación "Escuela de Pont-Aven", que toma su nombre de la villa frecuentada por los alumnos de la Escuela de Bellas Artes de París. Vienen a ser una síntesis del impresionismo y el simbolismo. Entre otros pintores de esta escuela cabe mencionar a Emile Bernard y Charles Laval.

A la segunda generación simbolista se les conoce como los nabi, con una concepción estética fundamentalmente decorativa. Dentro de esta corriente puede citarse a Pierre Bonnard y Edouard Vuillard.

En la primera década del siglo, nacen en París el fovismo y el cubismo.

Aunque había ejemplos ya desde 1903, el fovismo es un movimiento que se desarrolló en torno al año 1905, cuando una serie de artistas, agrupados en torno a Matisse, se presentan en el "Salón de otoño de 1905". A pesar del escándalo, volvieron a reunirse en el "Salón de los independientes" en 1906. Hacia 1908 el grupo está disuelto, siguiendo cada artista su propia trayectoria.

Su figura más importante es Matisse (1869-1954), del que puede citarse su obra "Interior en rojo". Otros autores destacados son Albert Marquet (1875-1947), Manguin y Comoin. A este grupo se añaden con posterioridad Derain (1880-1954, "Puerto en Collioure)", Maurice Vlaminck (1876-1958) y una serie de pintores provenientes de Le Havre: Othon Friesz, Raoul Dufy y Georges Braque.
Entre 1907 y 1914 se desarrolla el Cubismo, movimiento artístico que tuvo como principales fundadores al español Pablo Picasso y al francés Georges Braque. El cubismo trata las formas de la naturaleza por medio de figuras geométricas, representando todas las partes de un objeto en un mismo plano. Es considerada la primera vanguardia ya que rompe con el último estatuto renacentista vigente a principios del siglo XX, la perspectiva. Hace su primera aparición colectiva en el Salón de Independientes de 1911. 

Braque se aparta de su inicial afección al fovismo para lanzarse, tras conocer la obra de Picasso, al cubismo. Otros pintores que difundieron el cubismo fueron: Albert Gleizes (1891-1953), Jean Metzinger (1883-1956), Roger de la Fresnaye (1885-1925) y Fernand Léger (1881-1955). 

Derivados del cubismo son otros movimientos artísticos menores, como el "purismo" de Charles Edouard Jeanneret (1887-1966) y Amédée Ozenfant (1886-1966) y el "orfismo" lanzado desde 1912 por obra de Robert Delaunay y František Kupka, ya prenamente abstractos.

Dentro del arte abstracto Robert Delaunay elaboró, desde 1912, a partir de las teorías de Chevreul sobre el contraste simultáneo de los colores, sus ventanas y sus primeras formas circulares cósmicas abstractas, mientras que František Kupka exponía en el Salón de Otoño de 1912 "Amorfa, fuga de dos colores" y en 1913 "Planos verticales azules y rojos".

La abstracción de Fernand Léger ("Contrastes de forme", 1913-1914) y la de Picabia ("Udnie", 1913) utilizaron formas cubistas sin renunciar a la intensidad cromática.

En paralelo a la abstracción constructivista se desarrolló una abstracción llamada biomórfica, que nació de las formas creadas por Jean Arp a finales de la década de 1910.

En el período de entreguerras París sigue siendo centro de atracción de artistas venidos de otros lugares. Pero pierde protagonismo como centro creador de nuevas tendencias. Así, van surgiendo en otros lugares tendencias como el futurismo (Italia), el expresionismo (Alemania), el constructivismo (la URSS) o el neoplasticismo (Países Bajos).

El movimiento Dada se inicia por Tristan Tzara en 1916, y relacionado con él surge el surrealismo, movimiento creado en Francia. Sin embargo, muchos de sus principales representantes son extranjeros, como los españoles Joan Miró y Salvador Dalí. El propio Max Ernst es un pintor alemán nacionalizado francés. Aunque francés de nacimiento, Marcel Duchamp (1887-1968), quien encarna lo más valioso del dadaísmo neoyorquino de origen europeo. 

La pintura surrealista aparece en escena desde la exposición de 1925 en la Galería Pierre. Dentro de los surrealistas abstractos puede citarse a Andrés Masson e Yves Tanguy. 

Como una tendencia artística "menor" del primer tercio de siglo puede citarse el auge del cartel. Siguiendo la línea de Henri de Toulouse-Lautrec, Jules Chéret (1836-1933) es el primero en producir sistemáticamente, desde 1866, grandes carteles litográficos en color. Más adelante, Cassandre (1901-1968) asume el lenguaje formal del constructivismo para crear unos carteles poéticos ("Etoile du Nord", 1927; "Dubo-Dubon-Dubonnet", 1934).

Dentro del expresionismo pueden encontrarse dos pintores franceses: Georges Rouault (1871-1958) y Jules Pascin (1885-1930).

Tras la Segunda Guerra Mundial, París pierde definitivamente su carácter de centro de creación artística. Hay pintores en los distintos movimientos artísticos originados y difundidos en otros lugares del mundo.

El manifiesto del arte concreto, que publicó Theo van Doesburg en París en 1930, dio lugar a la tendencia del mismo nombre que tuvo un gran desarrollo en Suiza con Max Bill y de Richard Paul Lose, en Francia con François Morellet, y en todas las formas de arte sistemático nacidas después de la guerra. Estas tendencias entraron entonces en competencia con las diversas corrientes tachistas y gestuales (Jean Bazaine, Alfred Manessier, Pierre Soulages y Georges Mathieu, entre otros) que el crítico Michel Tapié reagrupó bajo la denominación de arte informal.

Mathieu, que presenta cierta afinidad con Pollock, puede citarse dentro de las tendencias informalistas y matéricas. Henri Michaux, con telas de manchas vibrantes y dibujos con "graffiti"; y Jean Fautrier usa de procedimientos mixtos que lo acercan a la pintura matérica.

El final de la década de 1960 vivió el desarrollo de una abstracción centrada en el análisis de sus propios componentes, con los grupos BMPT y Support(s)-Surface(s).

Hacia 1960, como una reacción contra el informalismo predominante durante los años 1950, surge una neofiguración en todo el mundo. Junto a Francis Bacon, el gran representante de esta tendencia es Jean Dubuffet, creador del "art brut".

Entre los artistas que, sin ser específicamente fotorrealistas, han utilizado la fotografía como medio de expresar la realidad está el francés Christian Boltanski, quien utiliza fotos de álbumes familiares de otras personas que según sus propias palabras, serían, tras haber fallecido, la prueba de su existencia.



</doc>
<doc id="15569" url="https://es.wikipedia.org/wiki?curid=15569" title="Camellia">
Camellia

El género Camellia agrupa entre 100 y 250 especies (hay cierta controversia sobre el número exacto) originarias de las regiones tropicales y subtropicales de Asia sudoriental, China y Japón. Se las encuentra en los bosques situados a media altura sobre el nivel del mar. Un botánico y misionero jesuita del siglo XVII, Georg Josephus Kamel (también conocido como Camellus), las describió y dibujo después de un viaje a Filipinas a bordo de un galeón español, Carlos Linneo nombró a este género en su honor.

Todas las especies son arbustos y árboles que pueden llegar a medir 10 m de altura. De follaje perennifolio, sus hojas son coriáceas, de un verde oscuro lustroso, enteras, puntiagudas y de bordes enteros o ligeramente aserrados.

Las flores son generalmente grandes, con cinco sépalos y cinco pétalos (se han conseguido híbridos con doble o múltiple corola y gran cantidad de pétalos), sus colores varían del blanco al rojo pasando por el rosa y, ocasionalmente, pueden aparecer combinadas en el mismo pie e incluso jaspeadas en esas tonalidades. Hay varias especies, menos populares, de flor amarilla.

Quizás la especie más extendida en jardinería sea "C. japónica" por ser la más frecuentemente utilizada. Es nativa de Japón zona suroriental de China y Corea y de ella se deriva la variedad "Adolphe Audusson", indicada para cultivo en interiores. De las hojas de "C. sinensis" se obtiene el té.



</doc>
<doc id="15570" url="https://es.wikipedia.org/wiki?curid=15570" title="Arquitectura del Barroco">
Arquitectura del Barroco

La arquitectura barroca es un período de la historia de la arquitectura que vino precedida del Renacimiento y del manierismo; se generó en Roma durante el siglo XVII y se extendió hasta mediados del siglo XVIII por los estados absolutistas europeos.

El término barroco, derivado del portugués "barocco", 'perla de forma diferente o irregular', se utilizó en un primer momento de forma despectiva para indicar la falta de regularidad y orden del nuevo estilo. La característica principal de la arquitectura barroca fue la utilización de composiciones basadas en puntos, curvas, elipses y espirales, así como figuras policéntricas complejas compuestas de motivos que se intersecaban unos con otros. La arquitectura se valió de la pintura, la escultura y los estucados para crear conjuntos artísticos teatrales y exuberantes que sirviesen para ensalzar a los monarcas que los habían encargado.

En algunos países europeos como Francia e Inglaterra y en otras regiones de la Europa septentrional se produjo un movimiento más racionalista derivado directamente del Renacimiento que se denominó clasicismo barroco. A lo largo del siglo XVIII se fue desarrollando en Francia un movimiento derivado del Barroco que multiplicaba su exuberancia y se basaba fundamentalmente en las artes decorativas que se denominó rococó y se acabó exportando a buena parte de Europa.

Contrariamente a las teorías según las cuales el movimiento barroco surgió a partir del manierismo, fue el el movimiento que acabó desencadenando en último término el Barroco. De hecho, la arquitectura manierista no fue suficientemente revolucionaria para evolucionar radicalmente, en un sentido espacial y no solo superficial, a partir de los estilos de la antigüedad a los nuevos fines populares y retóricos de la época del contrarreformismo.

Ya en el siglo XVI, Miguel Ángel Buonarroti había anunciado el Barroco de una forma colosal y masiva en la cúpula de la basílica de San Pedro de Roma, así como las alteraciones en las proporciones y las tensiones de los órdenes clásicos expresados en la escalera de acceso a la Biblioteca Laurenciana de roma, del mismo autor, y la enorme cornisa añadida al Palacio Farnese. Estas intervenciones habían suscitado diversos comentarios en su época por su brusca alteración de las proporciones clásicas canónicas. No obstante, en otras obras Miguel Ángel había cedido a la influencia manierista, por lo que fue sólo tras el fin del manierismo cuando se redescubrió a Miguel Ángel como el padre del Barroco.

El nuevo estilo se desarrolló en Roma, y alcanzó su momento álgido entre 1630 y 1670; a partir de entonces el Barroco se extendió por el resto de Italia y de Europa.

La influencia del Barroco no se limitó al siglo XVII; a principios del siglo XVIII se desarrolló el estilo denominado rococó, que no siendo una pura continuación del primero podría ser considerado como la última fase del Barroco.

En 1585 el Papa Sixto V inició las obras para la transformación urbana de Roma, encargando a Domenico Fontana la conexión entre los principales edificios religiosos de la ciudad por medio de grandes ejes viarios rectilíneos. El proyecto, que se basaba en la ratificación de Roma como "ciudad santa", estableció el precedente para las intervenciones que se habrían de llevar a cabo en diversas ciudades europeas.

A la planificación centralizada de la ciudad ideal renacentista se contrapone la visión de la "ciudad capital" barroca, más dinámica y abierta a sus propios límites, y al mismo tiempo punto de referencia para todo el territorio. En Roma, los centros focales del panorama urbano se subrayaron mediante la colocación de antiguos obeliscos egipcios y altas cúpulas, mientras que en París los nodos del sistema viario se definieron por medio de plazas simétricas, en cuyo centro se colocaba la estatua del soberano.

En líneas generales, la plaza barroca cedió su función tradicional cívica y pública para convertirse en un medio de exaltación de la ideología religiosa o política, como en el caso de las "plazas reales" francesas (la Plaza de los Vosgos o la Plaza Vendôme, por ejemplo) o de la Plaza de San Pedro de Roma.

Durante el Renacimiento, la ciudad se encontraba encerrada en sí misma, de manera física y sensible, ya que el habitar se limitaba casi exclusivamente a lo que sucedía dentro de las murallas. En una escala menor, los espacios públicos eran poco comunes y los espacios privados muy frecuentes. El proceso de urbanización del Barroco fue el motor del de la configuración de la ciudad como un todo.

Así, la ciudad comienza a formar parte del paisaje y se adueña del mismo. El exterior se integra al interior como un integrante más del espacio. Lo que antes era una planta cerrada ahora se “abre” para producir una vinculación entre lo artificial y lo natural, provocando puntos de encuentro entre el mundo de la ciudad y el mundo natural del jardín y del paisaje.

La catedral (sede del obispo) representa un importante hito dentro de la historia de las ciudades novohispanas. A nivel del paisaje urbano, por la preeminencia de su volumetría en medio del contexto edificado, y simbólicamente porque además de representar a los poderes religiosos real y civil acompaña prácticamente a lo largo de su edificación a la historia del desenvolvimiento de la ciudad. 

En la mitad del siglo XVII, el desarrollo de una técnica decorativa que incide directamente a favor de los propósitos que configuraron la sensibilidad del barroco: Las yeserías. A partir de modelos copiados tanto de las ilustraciones de libros (grutescos y tarjas fundamentalmente) como de los artesanos renacentistas europeos, los primeros grupos de yeseros provenientes de España se establecieron  en Puebla a partir de la cuarta década del siglo XVII, extendiendo la influencia de su trabajo a la vecina Tlaxcala; el trabajo de argamasa  (mezcla de cal y arena) aplicado generalmente en los marcos de las portadas, como el de los yesos  que vistieron los interiores  de los recintos religiosos, consistió en modelar estos materiales plásticos apoyándose en la estructura de barroco o piedra de muros y bóvedas, e ir creando revestimientos que paulatinamente se apoderan no solo de la totalidad de las superficies,  sino de la calidad particular de los espacios, al establecer una sintonía plástico-expresiva cuya resonancia ambiental aniquila la homogeneidad geométrica de la arquitectura creando un discurso de frenética movilidad aparente.

Entre las iglesias, el punto de partida de la arquitectura barroco puede considerarse la Iglesia del Gesù de Roma, construida a partir de 1568 según el proyecto de Jacopo Vignola. El edificio, que representa una síntesis entre la arquitectura renacentista, manierista y barroca, satisfacía plenamente las nuevas exigencias surgidas tras la Contrarreforma: la disposición longitudinal de la planta permitía acoger al mayor número de fieles, mientras que la planta de cruz latina con numerosas capillas laterales suponía un retorno a la tradición del Concilio de Trento. Así de hecho lo hará constar una figura tan importante como el cardenal Borromeo:
Por otro lado, la presencia de una cúpula subrayaba la centralidad del espacio hacia el fondo de la nave, y presagiaba la búsqueda de una integración entre el esquema longitudinal y el centralizado. También la fachada, construida según el proyecto de Giacomo della Porta, anticipaba los elementos más marcadamente barrocos, comparables a los de los alzados de Santa Susana y San Andrés del Valle.

De este modelo derivaron una serie de iglesias de planta longitudinal centralizada o planta central alargada, caracterizadas por el eje longitudinal y por la presencia de un elemento catalizador de la composición, generalmente una cúpula.se construyó en los años 1985
Si los arquitectos manieristas alteraban la composición rigurosa de las fachadas renacentistas añadiéndoles temas y decoraciones caracterizadas por un intelectualismo refinado, pero sin modificar la lógica planimétrica y estructural de la fachada de los edificios, los arquitectos barrocos modificaron tanto la composición en planta como en fachada, generando una concepción nueva del espacio. Las fachadas de las iglesias dejaron de ser la continuación lógica de la sección interna, para convertirse en organismos plásticos que marcaban la transición entre el espacio exterior y el interior. El espacio interior, por tanto, estaba compuesto a partir de figuras complejas basadas en elipses y líneas curvas, y se definía a través del movimiento de los elementos espaciales, diferenciándose radicalmente de la concepción renacentista que generaba una sucesión uniforme de elementos dispuestos de forma simétrica entre ellos.

En la arquitectura civil del momento se puede distinguir entre dos tipos de construcciones nobles: el palacio, situado generalmente en el interior de la ciudad, y la villa del campo.

El palacio italiano y sus derivados europeos permanecieron fieles a la tipología residencial desarrollada durante el Renacimiento, con un cuerpo edificado cerrado en torno a un patio interno. Se dotó a las fachadas principales de cuerpos centrales resaltados y decorados mediante el uso de órdenes gigantes, que ya habían sido anticipados por Palladio. Se extendieron los ejes de simetría al interior del edificio, donde se abrían el vestíbulo y el patio interno; por ejemplo, el eje longitudinal introducido en el Palacio Barberini de Roma contribuía a la definición de la planta y subrayaba la conexión con el exterior del edificio. Por otro lado, este palacio constituyó un punto importante del desarrollo de la tipología residencial palaciega italiana: la planta se constituía en forma de H, y la entrada se producía mediante un profundo atrio que iba haciéndose más estrecho sucesivamente, hasta llegar a una sala elíptica que servía de centro nodal al palacio entero

En Francia, no obstante, el palacio urbano de la nobleza, denominado "hôtel", recuperó para sí el esquema de los castillos medievales. El clima más duro reclamaba una optimización del soleamiento en las principales estancias, lo que generó fachadas escalonadas y grandes alas laterales. El cuerpo principal se encontraba retrasado respecto a la calle y precedido de la "cour d'honneur", un espacio de transición abierto al exterior que al mismo tiempo separaba el palacio de la ciudad. Un ejemplo de este esquema es el parisino Palacio del Luxemburgo, construido a partir de 1615 por Salomon de Brosse. Aquí, a diferencia de otros edificios del mismo estilo y época, los pabellones angulares no fueron destinados a locales de servicio, sino que contenían estancias principales en cada planta.

Fue notable el desarrollo francés de residencias en el campo, los denominados "châteaux", que llevaron a la realización de extensos complejos de los que partían los ejes viarios principales que ordenaban el entorno. Entre ellos cabe destacar el Palacio de Vaux-le-Vicomte (1656-1659), proyectado por Louis Le Vau, y el Palacio de Versalles, máximo símbolo del absolutismo francés y cuyas labores de reconstrucción fueron iniciadas por el mismo Le Vau por encargo de Luis XIV.

El paisaje ideal de la época barroca halló su expresión más característica en el jardín francés especialmente en las creaciones de André Le Notre. El jardín francés se concebía como un paisaje infinito ordenado geométricamente y centrado en el palacio el cual representa el foco del sistema. Pero la verdadera finalidad es la sensación de espacio infinito que se materializa en un eje longitudinal dominante. Todos los demás elementos están relacionados con ese eje, el cual divide dos mundos: el mundo urbano del hombre y el mundo ampliamente abierto de la naturaleza.

Versalles representa la verdadera esencia del medio ambiente del siglo XVII: dominio, dinamismo y apertura. Hacia fines del siglo, todo el paisaje en torno a París se transformó en una red de sistemas centralizados e infinitamente extendidos. La resolución simbólica que parecían tener las plazas para representar el poder del monarca terminó siendo una resolución paisajística para el usuario.

Así, la ciudad comienza a formar parte del paisaje y se adueña del mismo. El exterior se integra al interior como un integrante más del espacio. Lo que antes era una planta cerrada ahora se “abre” para producir una vinculación entre lo artificial y lo natural, provocando puntos de encuentro entre el mundo de la ciudad y el mundo natural del jardín y del paisaje.

El barroco es un estilo que cruza la historia, nunca pretendió ser entendido por la razón, por la inteligencia, sino captado por los sentidos, buscó en el espectador efectos emocionales, no racionales.

El Barroco surge en el escenario artístico europeo en dos contextos muy claros durante el siglo XVII: de entrada había la sensación de que, con el avance científico representado por el Renacimiento, el clasicismo, aunque hubiera ayudado en este progreso, no estaba en condiciones de ofrecer todas las respuestas necesarias a las dudas del hombre. El Universo ya no era el mismo, el mundo se había expandido y el individuo quería experimentar un nuevo tipo de contacto con lo divino y lo metafísico. Las formas lujuriantes del Barroco, su espacio elíptico, definitivamente antieuclidiano, fueron una respuesta a estas necesidades.

El segundo contexto es la Contrarreforma promovida por la Iglesia católica. Con el avance del protestantismo, el antiguo orden cristiano romano (que, en cierto sentido, había incentivado el advenimiento del mundo renacentista) estaba siendo suplantado por nuevas visiones de mundo y nuevas actitudes ante lo Sagrado. La Iglesia sintió la necesidad de renovarse para no perder los fieles y vio en la promoción de una nueva estética la oportunidad de identificarse con este nuevo mundo. Las formas del barroco fueron promovidas por la institución en todo el mundo (especialmente en las colonias recién descubiertas), haciéndolo el "estilo católico", por excelencia.

Autores como Manuel G. Revilla, José Juan Tablada y Diego Angulo perciben una división tajante de las cualidades formales de nuestra arquitectura barroca según el siglo al pertenezcan; es decir que dividen la arquitectura barroca novohispana en dos siglos: El XVII y el XVIII.

Revilla considera que en el siglo XVII se produce la arquitectura barroca propiamente dicha, caracterizada por la alteración de las proporciones de los elementos arquitectónicos, multiplicación en las formas de los arcos, frontones rotos, abundantes, irregulares y toscas molduras y la aparición de la columna de fuste retorcido o historiado. Aunque todavía se conservan perfiles rectos y entrepaños sin decorar.

Para Revilla es en el siglo XVIII cuando el estilo adquiere madurez y entonces se le puede dar un nombre diferente al de simplemente barroco, se le puede llamar churrigueresco. En la columna se convierte en pilastra cubierta de profusa ornamentación; se decoran todos los entrepaños, las líneas se rompen hasta el infinito y la escultura se convierte en un elemento decorativo más de los edificios.

José Juan Tablada, continúa la misma línea de pensamiento que Revilla y también divide la arquitectura barroca en dos momentos: el barroco hispánico o barroco mexicano, propio del siglo XVII, y el churrigueresco mexicano, desarrollado en el siglo XVIII.

Las características que el autor atribuye al barroco hispánico son: cúpulas y airosos  campanarios, al exterior, en tanto que en el interior, los templos tienen la simple austeridad de las basílicas.

En la España, la afirmación del Barroco se encontró con las dificultades debidas a la decadencia económica del reinado de Felipe III. En la segunda mitad del siglo XVI, Felipe II había mandado construir el importante complejo del Monasterio de El Escorial, construido en su mayor parte según el proyecto de Juan de Herrera (1530-1597). A Herrera se debe también el proyecto de la Catedral de Valladolid, en el que se refuerza el concepto del eje central y que sirvió de modelo para la Catedral de México.

Progresivamente, la arquitectura española del siglo XVII fue evolucionando hacia el estilo barroco, aunque no dejó grandes ejemplos significativos. La mayor parte de las influencias barrocas fueron recogidas de forma exclusivamente decorativa, especialmente en las iglesias. Este lenguaje, que resultaba rápidamente comprensible incluso para el segmento de la población menos instruido, fue exportado con éxito a las colonias americanas.

Entre los edificios religiosos más importantes del siglo XVII en España puede destacarse la Colegiata de San Isidro en Madrid, iniciada en 1629, la iglesia de Santa María Magdalena de Granada (iniciada en 1677 con planta longitudinal derivada de los edificios con esta disposición de la Antigua Roma) y la Basílica de la Virgen de los Desamparados en Valencia, de planta elíptica.

I.- Periodo purista o postherreriano (abarca los dos primeros tercios del siglo XVII). La penetración del barroco -en sus formas arquitectónicas italianas (plantas complicadas, movimiento de fachadas, decoración abundante y creadora de contrastes de luz)- va a ser lenta. La presencia de la ideología religiosa de la Contrarreforma y el prestigio de la monarquía de Felipe II pesan sobre el arte de la época: se prefiere la sobriedad, la sencillez y la uniformidad. Hay una evidente pobreza de materiales –ladrillo, tapial y yeso- junto a una depuración de líneas -al estilo del Escorial-. Así como un escaso desarrollo del movimiento en plantas y alzados; se prefiere la línea recta a la curva; hay un predominio de la Iglesia de nave única con capillas entre contrafuertes -tipo de la iglesia del Gesù de los Jesuitas. Las fachadas expresan la misma sencillez de planos:
"De un espíritu abstracto, los palacios, las Iglesias y conventos son con fachadas de paramentos lisos a base de grandes rectángulos ligeramente resaltados e interiores de diáfana blancura en la que solamente se recortan de manera neta las decoraciones de cuadrados y triángulos geométricos de las bóvedas, resultando conjuntos graves y apaciguados para aquellos que los contemplan al exterior o penetran al interior".

Ejemplos de este tipo de arquitectura lo tenemos en la Colegiata de San Isidro de Madrid (construida por un jesuita: es de planta de cruz latina similar a la del Gesù, o a San Andrés de Mantua de Alberti); la iglesia de la Encarnación (Madrid); la Cárcel de Madrid (hoy ministerio de Asuntos Exteriores), la Casa de la Villa de Madrid, la Plaza Mayor de Madrid, la ciudad de Lerma (Burgos); el palacio del Buen Retiro. Estos cinco últimos edificios siguen la línea llamada "estilo escurialense, caracterizado por la sobriedad de líneas, los volúmenes compactos y torres cuadrangulares en las esquinas, techumbres apiramidadas, agujas en los vértices torres, tejas de pizarra negra. En esta época destacan unas especiales concepciones urbanísticas españolas: las plazas mayores, organizaciones casi cerradas, centro de los espectáculos religioso-políticos (procesiones, autos de fe de la Inquisición, predicaciones, recepciones de reyes), formados por distintos bloques de edificios que se unen dejando, bajo ciertas arcadas, paso a las calles periféricas. La más famosa es la Plaza Mayor de Madrid.
II.- Finales del siglo XVII. Se comienza a complicar la arquitectura; primero penetran las formas decorativas del barroco italiano (columnas de orden gigante y salomónicas, movilidad de planos en las fachadas, etc.), y luego las formas espaciales (plantas ovaladas, o cóncavo-convexas, llenas de movimiento).Destacan: fachada de la Catedral de Granada -de Alonso Cano-, dispuesta a manera de arco de triunfo de tres calles, cubiertas de arcos de medio punto; el Pilar de Zaragoza; la torre de las campanas y la del Reloj (Domingo de Andrade) de Santiago de Compostela. Durante el siglo XVII son escasas las construcciones; ya a finales de siglo se construyen: el presbiterio de la Catedral de Valencia. Las obras más barrocas son la fachada de la Catedral- claro ejemplo de los movimientos de fachadas al estilo de Borromini- : entre el escaso espacio que quedaba entre capilla del santo cáliz y Miguelete, se despliega una fachada a modo de biombo con tres calles plegadas en movimientos sinuosos cóncavo convexo, recargada de decoración en relieve y esculturas. La capilla de la Virgen de los desamparados: de planta ovalada, con espacios de entrada o capillas; destacando el camarín de la Virgen. Otros ejemplos son el museo de Bellas Artes, San Pío V y la torre de Santa Catalina, Palacio del Marques de Dos Aguas.

III.- Corriente nacional: Churrigueresco. Durante el siglo XVIII se acelera la construcción de edificios; resalta la plena asimilación de las formas espaciales de Italia (De Borromini y Bernini) en edificios como: San Marcos de Madrid, las Salesas Reales de Madrid, San Francisco El Grande -Madrid-, Palacio Real de Aranjuez -capilla. Son todos ellos edificios en los que destaca su compleja planta con juegos de curvas y contracurvas, cambitación de formas ovaladas, tangentes y secantes; con alzados en los que las cúpulas, bóvedas, etc. son de gran complejidad (destacan las cúpulas encamonada creadas por Francisco Bautista en e1 siglo XVIII: son un sistema de doble cúpula en el que el intradós es de madera y yeso, mientras que el exterior se despega y separa quedando un espacio hueco para lograr mayor efecto de altura y monumentalidad. Al ser de menor peso permite la constitución de espacios más desahogados).
Por otro lado, la arquitectura del siglo XVIII aumenta la tendencia ornamental hasta límites nunca conseguidos; a este estilo se le llama Churrigueresco: por el nombre de la familia con este apellido que produjo mayores obras. Es una decoración de amontonamiento de formas en ciertos lugares del edificio –puertas, fachada, etc; sobresalen por su monumentalidad y aparatosidad. frente al resto del edificio de líneas más sóbrias-. Destacan: colegios de Anava y Calatrava en Valladolid, plaza Mayor de la ciudad de Salamanca. De Pedro Ribera son el puente de Toledo en Madrid, y el Hospicio de Madrid. Otros edificios de este estilo son: San Telmo en Sevilla. La fachada del Obradoiro en Santiago, etc. Esta fachada de Casas y Novoa sustituye a la románica construida delante del Pórtico de la Gloria; es una monumental fachada estructurada como un grandioso arco de triunfo en diversos planos de profundidad (hasta tres) y de una gran verticalidad.

Otra complicación del barroco español se encuentra en los espacios creados para dar cabida a las imágenes religiosas como: reliquias, sagrario , sacristías e imágenes de gran devoción : vienen a combinarse teatrales efectos en la utilización del espacio, la luz indirecta y de procedencia extraña, la pintura, escultura, etc. Son pequeños lugares en los que el barroquismo estalla en su mayor grado de complicación y teatralidad. Destacan el Transparente de la catedral de Toledo (de Narciso Tomé), el camarín y tabernáculo de la Cartuja del Paular, o el Sagrario de la Cartuja de Granada (Francisco Hurtado Izquierdo). Otra de las grandes escuelas del barroco español, es la fundada a inicios del siglo XVIII por Francisco Hurtado Izquierdo, en Priego de Córdoba. En la que intervinieron, sucesivamente, los hermanos Sánchez de Rueda, Juan de Dios Santaella, Francisco Javier Pedraxas, Remigio del Mármol y José Álvarez Cubero.

Los estudios de arquitectura realizados en Italia por el escenógrafo Inigo Jones y el joven Earl of Arundel constituyeron un impulso inicial que abrió paso a una reorientación fundamental de la arquitectura inglesa, que seguía atrapada en las formas tardomedievales y manieristas. The Queen's House, en Greenwich, pone de manifiesto el brusco cambio de tendencias. El palacio de la reina consta de dos bloques rectangulares unidos entre sí por un puente, conectándolo con el que fue el Greenwich Hospital, hoy conocido como la Old Royal Naval College, declarada Patrimonio de la Humanidad por la UNESCO. Sobre la planta baja almohadillada se levanta el "piano nobile," la planta noble, que se abre al jardín mediante una amplia galería con columnas dóricas. Aparte de Jones hubo muy pocos arquitectos de renombre en este periodo, pero entre ellos cabe citar a Isaac de Caus, que erigió Wilton House, con sus elegantes y fastuosas estancias en forma de caja denominadas "The cube" y "The Double Cube". 
Sin duda si hay un arquitecto inglés que destaque por la maestría de sus obras ese es Sir Christopher Wren, quien consiguió imponer en Inglaterra el clasicismo de cuño romano. En 1666, tras el gran incendio de Londres, se le convocó junto con sus colegas para presentar propuestas destinadas a la reconstrucción y urbanización de la que era una de las ciudades más pobladas de la tierra. La impresionante catedral de Saint Paul, cuya silueta es inconfundible en el horizonte de la ciudad, y 51 iglesias más son obra del maestro Wren. También la ampliación del palacio de Hampton Court por orden de Guillermo III de Inglaterra fue llevada a cabo por el mismo entre los años 1689 y 1692. 

John Vanbrugh y Nicholas Hawksmoor otorgaron al estilo de Wren unas dimensiones aún más monumentales y sobre todo más pintorescas y teatrales. A partir de 1699 tuvieron a su cargo la construcción del imponente Castle Howard al norte de Yorkshire. El recinto entre "cour et jardin" (entre patio y jardín) consta de un ala de aposentos similar a un corredor en cuyo centro destacan el salón abierto al jardín y la gran sala cuadrada abierta al patio. En 1715 y 1717 respectivamente publicaron los dos volúmenes del Vitruvius Britannicus, con grabados de edificios británicos clásicos y la traducción de los "Quatro libri dell'architettura" de Andrea Palladio, lo que provocó un nuevo cambio revolucionario: El neopaladianismo. Esta tendencia tenía como objetivo un retorno a las "reglas nobles y verdaderas" de la Antigüedad tal y como las habían interpretado Palladio e Inigo Jones. El principal protagonista de este movimiento fue Lord Burlington, experto en arte que con su Chiswick House creó un edificio de asombrosa semejanza con las obras de Palladio. Por último cabe destacar otras hermosas obras del barroco británico que se materializan en la residencia de los Duques de Devonshire, conocida como Chattsworth en Derbyshire, Inglaterra, de la mano del arquitecto William Talman en 1694; sin olvidar claro está el monumental Blenheim Palace construido en 1710 por el antes mencionado John Vanbrugh, para el duque de Marlborough de parte de la Reina Ana.

Hay ciertos caracteres que dan personalidad al barroco novohispano, tanto en la composición de los edificios cuanto en el aspecto formal. Los más notorios son los siguientes:















La génesis de la arquitectura barroca se inicia en Italia, con figuras tan determinantes como Gian Lorenzo Bernini y Francesco Borromini. 

En España, la arquitectura barroca va a estar presidida por el gusto por la desornamentación y la sobriedad que había introducido el estilo herreriano, con importantes edificios en los que impera un estilo mesurado y casi clásico.

En América, tras la conquista española, el lenguaje del barroco se desarrolló en forma importante enriqueciéndose con la mano de obra y los conceptos propios de la arquitectura y arte precolombinos, como el uso extensivo de colores brillantes, destacándose en forma especial el barroco mexicano, peruano y el cubano.

En Alemania y en Austria la inspiración italiana combinada con la francesa creará edificios de gran exuberancia decorativa, sobre todo en los interiores, de luminosidad brusca, que darán paso al estilo Rococó (El Rococó se define por el gusto por los colores luminosos, suaves y claros).

En Inglaterra predomina el equilibrio y la austeridad.

La ciudad del barroco se ve como la imagen de su gobernante, cuya importancia se mide por su tamaño y por el número de sus habitantes.

En las cortes más poderosas de Europa, la estructura urbana intentará ostentosamente asentar los valores y la estructura política creada por los dirigentes.

La ciudad se va a estructurar en torno a un centro, como el poder absoluto tiene como centro el Rey, al que confluyen grandes vías, rectas de amplias perspectivas. Las plazas serán uno de los grandes elementos, reflejo y símbolo del poder civil o religioso, entendidas como escenarios de fiestas y representación.

Los cambios se van a reflejar mejor en las pequeñas cortes europeas, donde las realizaciones pueden cambiar y determinar la imagen de toda la ciudad, como es el caso de Würzburg, mientras que en los grandes organismos urbanos como París o Roma, la complejidad y la aparatosidad de los proyectos se va a enfrentar con la ciudad preexistente, que dificulta en gran medida la transformación pretendida, consiguiéndose mejores resultados en las nuevas residencias de los soberanos, fuera de la ciudad, como es el caso de Versalles.

América recibió los conceptos urbanísticos renacentistas primero y barrocos posteriormente, a lo largo de la extensiva urbanización que los colonizadores europeos llevaron a cabo durante los siglos XVI a XIX.





</doc>
<doc id="15573" url="https://es.wikipedia.org/wiki?curid=15573" title="Arte paleocristiano">
Arte paleocristiano

Se denomina arte paleocristiano al estilo de arte que se desarrolla durante los cinco primeros siglos de nuestra era, desde la aparición del cristianismo, durante la dominación romana, hasta la invasión de los pueblos bárbaros, aunque en Oriente tiene su continuación, tras la escisión del Imperio, en el llamado arte bizantino.

En Occidente, Roma es el centro y símbolo de la cristiandad, por lo que en ella se producen las primeras manifestaciones artísticas de los primitivos cristianos o paleocristianos, recibiendo un gran influjo del arte romano tanto en la arquitectura como en las artes figurativas. Lo mismo que la historia del cristianismo en sus primeros momentos, en el arte se distinguen dos etapas, separadas por la promulgación del Edicto de Milán por Constantino en el año 313, otorgando a los cristianos plenos derechos de manifestación pública de sus creencias.

Hasta el año 313, el arte escultórico de los cristianos se centró en la excavación de las catacumbas y el reforzamiento de sus estructuras. Estas eran cementerios romanos, excavados, en un principio, en los jardines de algunas casas de patricias cristianas, como las de Domitila y Priscila en Roma. Más tarde en el siglo V, y ante el aumento de creyentes, estos cementerios se hicieron insuficientes adquiriendo terrenos en las afueras de las urbes donde surgen los cementerios públicos, en los que se excavan sucesivos pisos formando las características catacumbas que ahora conocemos.

La primera vez que se aplicó el término "catacumba" es a la de San Sebastián en Roma, Italia. El cementerio o catacumba (donde se encontraban los cuerpos sin vida) se organiza en varias partes: estrechas galerías ("ambulacrum") con nichos longitudinales ("loculi") en las paredes para el enterramiento de los cadáveres. En algunos enterramientos se destacaba la notabilidad de la persona enterrada, cobijando su tumba bajo un arco semicircular ("arcosolium").

En el siglo IV en el cruce de las galerías o en los finales de las mismas se abrieron unos ensanchamientos ("cubiculum") para la realización de algunas ceremonias litúrgicas. Las catacumbas se completaban en el exterior con una edificación al aire libre, a modo de templete ("cella memoriae"), indicativa de un resto de reliquias que gozaban de especial veneración. Entre las catacumbas más importantes, además de las ya citadas, destacan las de San Calixto en Santa María de Trastevere, Santa Constanza y Santa Inés en Sant'Agnese in Agone, todas ellas en Roma, aunque también las hubo en Nápoles, Alejandría y Asia Menor.

En los templos de culto paganos las procesiones y sacrificios se celebraban al aire libre y en el interior sólo estaba el altar del dios. Estos templos eran muy pequeños. En el Imperio de Constantino surgió la necesidad de utilizar edificios con mayor capacidad para el culto cristiano.
Las nuevas iglesias cristianas necesitaban más espacio para contener a los fieles que se acercaban a orar dentro del templo. Por eso las iglesias no tomaron de modelo los templos paganos sino que tomaron las grandes salas de reuniones públicas que ya eran conocidas con el nombre de basílicas. 
Por eso, a finales del siglo IV y a comienzos del siglo V, comenzaron a suprimirse las iglesias de formas irregulares para reemplazarlas por iglesias de forma regular, es decir, basílicas regulares, de tres naves con un ábside en uno de los lados menores y en el otro lado menor la entrada frente al coro. En todo el Imperio quedó asociado el concepto de iglesia con el de basílica. 

Después del Edicto de Milán, a partir del año 313, la basílica es la construcción eclesiástica más característica del mundo cristiano. Su origen es dudoso, pues se la considera una derivación de la basílica romana, o se la relaciona con algunos modelos de casas patricias, o, incluso, con algunas salas termales. La basílica organiza su espacio, generalmente, en tres naves longitudinales, que pueden ser cinco, separadas por columnas; la nave central es algo más alta que las laterales, sobre cuyos muros se levantan ventanas para la iluminación interior. La cubierta es plana y de madera y la cabecera tiene un ábside con bóveda de cuarto de esfera bajo la que se alberga el altar.

En las grandes basílicas, como la de San Pedro y San Juan de Letrán, en Roma, la estructura de su cabecera se completaba con una nave transversal llamada transepto que buscaba el simbolismo de reproducir la cruz de Cristo en la planta del templo. Al edificio basilical se accede a través del atrio o patio rectangular (antecedente de los claustros), con una fuente en el centro, que conducía hasta el nártex o sala transversal, situada a los pies de las naves, desde donde seguían la liturgia los "catecúmenos". Las basílicas más notables, además de las citadas, son la de Santa María la Mayor, San Pablo Extramuros y la de Santa Inés y San Juan de la Real (Iglesia donde se casó Francisco Franco).

Otros edificios de carácter religioso fueron los baptisterios, edificaciones de planta poligonal, frecuentemente octogonal, que tenían en su interior una gran pila para realizar los bautismos por inmersión. El más conocido es el Baptisterio de San Juan de Letrán, en Roma, construido en tiempos de Constantino. También son de planta central algunos enterramientos que siguen la tradición romana; de planta circular con bóvedas es el Mausoleo de Santa Constanza y de planta de cruz griega es el Mausoleo de Gala Placidia en Rávena.

En el arte paleocristiano oriental se acusa la marcada tendencia a utilizar construcciones de planta de cruz griega, con los cuatro brazos iguales, como la Iglesia de San Simeón el Estilita.

El arte paleocristiano constituye la etapa final de la influencia romana. El cambio cultural que se opera durante los siglos II al IV tuvo en la Península poca vigencia, pues las invasiones de los pueblos germánicos se inician en el año 409. Pese a ello, y cada vez más, han aparecido abundantes testimonios de la vitalidad del arte paleocristiano hispano.

La escultura de la época se halla especialmente representada por los sarcófagos decorados con temas del Crismón, estrígilos, escenas bíblicas y representaciones alegóricas. Entre ellos se destacan el de Leocadius en Tarragona y el de Santa Engracia en Zaragoza. También se conservan algunas estatuas exentas, como varias con el tema del Buen Pastor, laudas sepulcrales y mosaicos que por su técnica y sentido del color siguen los modelos romanos.




Estoy absolutamente convencido que la vida amorosa es siempre un proceso de ensayo y error. No existe la media manzana ni el amor verdadero. Tampoco existe el amor para toda la vida y sobre todas las cosas no existen garantías en el amor.e se consolidan los aspectos formales y espirituales del arte bizantino; es la verdadera etapa creadora y definidora de la estética bizantina. Después del dominio europeo, con la dinastía de los Paleólogos, se da paso a la Tercera Edad de Oro que se centra en el siglo XIV y que finaliza con la toma de Constantinopla en 1453. Después, el arte bizantino florece en los países eslavos, Rusia y sureste de Europa, transmitiéndose hasta nuestros días a través del Monte Athos. El arte bizantino se dividió en cuatro grandes etapas:

Primera Edad de Oro Bizantina: 527 - 726, año en el que aparece la querella iconoclasta. La época dorada de este arte coincide con la época de Justiniano.
La querella iconoclasta se prolongó entre los años 726 - 843 y enfrentó a los iconoclastas contra los iconódulos y fue tan violenta que produjo una crisis artística acentuadísima, especialmente en el arte figurativo.
Segunda Edad de Oro Bizantina: 913 - 1204, momento en que los cruzados destruyen Constantinopla.
Tercera Edad de Oro Bizantina: 1261 - 1453, cuando los turcos toman Constantinopla.
Arquitectura bizantina
Artículo principal: Arquitectura bizantina

</doc>
<doc id="15574" url="https://es.wikipedia.org/wiki?curid=15574" title="Arte Íbero">
Arte Íbero

El término arte íbero se refiere al estilo artístico propio del pueblo íbero, asentado en la península ibérica. Las manifestaciones mejor conservadas son las escultóricas, realizadas en piedra y bronce. Los restos en madera y barro cocido son escasos, por ser materiales más perecederos.

La actividad mejor conocida del arte ibérico es la escultura figurativa, con pequeñas estatuillas de bronce, utilizadas como ofrendas o exvotos, y estatuas de piedra de mayor tamaño. Los yacimientos más importantes son: el santuario del Cerro de los Santos y el del Llano de la Consolación, en Albacete; el santuario del Collado de los Jardines, en Despeñaperros (Jaén); la Fuentecica en Coy y el del Cigarralejo en Murcia.

Entre las esculturas realizadas en piedra, clasificables según su finalidad funeraria o religiosa, se encuentra la Dama de Baza y la Dama de Elche (Museo Arqueológico Nacional de España, en Madrid), que presentan una rica decoración y que sirvieron de urna funeraria. Posterior a las anteriores, y con finalidad religiosa, es la Gran Dama Oferente (del siglo III a. C.), procedente del Cerro de los Santos en Montealegre del Castillo (Albacete), en cuya larga vestimenta de profundos y geométricos pliegues, y en el frontalismo de su estructura, se aprecian las influencias arcaicas de la plástica griega.

De esta misma época es el León de Coy y la Bicha de Balazote (Museo Arqueológico Nacional de Madrid), hallada en la localidad albaceteña que le da nombre y relacionada con los toros antropocéfalos mesopotámicos y seres de aspecto feroz del mundo hitita.

En la orfebrería, destaca el Tesoro de Jávea formado por piezas de oro y plata de delicada labor de influjo griegos.

El área de expansión de la escultura ibérica no es muy amplia, aunque sí muy diversificada, lo que favoreció una gran variedad regional propiciada, en buena medida, por las riquezas naturales y los rasgos culturales de cada zona. Sus manifestaciones se centran en tres áreas: Andalucía, el centro de la península y la zona del Levante.

El área andaluza, de Jaén y Granada, es de una gran complejidad por la influencia cultural de los pueblos colonizadores orientales que se habían instalado en ella con anterioridad (fenicios, griegos, etc.) y por la tradición dejada por los tartesios. La proliferación de restos arquitectónicos y escultóricos, así como muestras de orfebrería y cerámica son los rasgos más distintivos de esta región. Junto a esta corriente oriental se aprecia en Andalucía otra de origen helénico, que se introduce desde las costas alicantinas hacia el sur, presente en el Conjunto de Cerrillo Blanco de Porcuna, el Santuario Heroico del Cerro del Pajarillo (Huelma) y en el Yacimiento de Osuna (del siglo III a. C.).

En el interior, concretamente en la Mancha occidental, destaca la importante ciudad (oppidum) de Alarcos junto al río Guadiana y con importantes restos de calles empedradas, exvotos y figurillas de bronce. Las ruinas de la ciudad ibera (luego romana y visigótica) de Oretum, capital de la antigua Oretania a ambos lados de Despeñaperros apenas están excavadas. Los restos son escasos en esta zona: cerámicas, figurillas de bronce y exvotos en los Santuarios de Despeñaperros y Castellar de Santisteban, En esta zona occidental los restos cerámicos parecen emparentarla con Andalucía.

No ocurre lo mismo con la Mancha oriental y las estribaciones de la Serranía conquense donde se palpa la influencia del estilo artístico ibero-levantino, sobre todo la cerámica. La zona central y meridional de Cuenca constituye el límite septentrional del mundo ibero que conecta con los celtiberos de la sierra. Aquí destacan numerosos yacimientos en la Manchuela conquense como Barchín del Hoyo y, sobre todo, el oppidum de Ikalesken (actual Iniesta) que conserva el único mosaico del arte ibero y uno de los más antiguos del mediterráneo. Este mosaico tiene la particularidad de representar la fusión de las cultura ibera, griega y fenicia. La cultura ibera está representada por el lobo, animal sagrado; la griega por Pegaso y la fenicia por la representación de la diosa Astarté, en el medio de la composición. El mosaico, del siglo VI a. C. aproximadamente, es muy arcaico en su realización pero por el interés artístico y la antigüedad que tiene merece la pena ser considerado como uno de los emblemas del arte ibérico.

Los territorios de la actual provincia de Albacete son especialmente pródigos en muestras diversas de arte ibérico, especialmente escultura, y sorprenden por la profusión de hallazgos, la calidad estilística y la singularidad de sus piezas. Muy sucintamente, se pueden mencionar la gran cantidad de piezas (solo en el Museo Arqueológico Nacional se conservan cerca de tres centenares) halladas en el importante centro de culto del Cerro de los Santos -especialmente la Gran Dama Oferente- y en el Llano de la Consolación. Como piezas únicas destacan la Bicha de Balazote, la Dama de Caudete, la Esfinge de Haches, la Cierva de Caudete, el Sepulcro de Pozo Moro, el León de Bienservida, las Esfinges gemelas de El Salobral, el Caballo de la Losa (Casas de Juan Núñez) o el Jinete de Villares (Hoya Gonzalo) entre otros. En orfebrería destaca el llamado Tesoro de Abengibre, conjunto de vajilla de plata con inscripciones iberas y también la Necrópolis de Los Villares y el camino de la cruz en Hoya Gonzalo que se encuentran en las estribaciones de los Altos de Chinchilla en las inmediaciones de la Vía Heráclea con cerámicas griegas, materiales púnicos, etruscos, etc.
La existencia de grandes oppida en la provincia aún sin estudiar, quizá aumente sensiblemente el ya abultado número de vestigios de arte ibérico. Aunque esta zona siempre es calificada como de paso o de extensión de influencias ibéricas levantinas o andaluzas, es posible que el flujo de extensión fuera, más bien, en sentido inverso y sea ésta una zona nuclear.

En el levante valenciano, en la antigua Edetania, las manifestaciones ibéricas muestran grandes vinculaciones, no solo con la viejas tradiciones de los primeros pobladores del Bronce y del Hierro, como por ejemplo en la incineración como sistema de enterramiento, si no también con las corrientes orientales aportadas por los colonizadores griegos, de los que recogen características propias del período arcaico griego, tratan los mismos temas - esfinges, grifos -, y utilizan decoración geométrica en la cerámica, con fondos amarillentos o ligeramente rojizos.

Esta corriente levantina se transmite a zonas aisladas del valle del Ebro donde se mezcla con los substratos célticos y posteriormente romanos.




</doc>
<doc id="15576" url="https://es.wikipedia.org/wiki?curid=15576" title="Arte gótico">
Arte gótico

Arte gótico es la denominación historiográfica del estilo artístico que se desarrolló en Europa Occidental durante la Edad Media tardía, desde mediados del siglo XII hasta la implantación del Renacimiento (siglo XV para Italia), y bien entrado el siglo XVI en los lugares donde el gótico pervivió más tiempo. Se trata de un amplio período artístico, que surge en el norte de Francia y se expande por todo Occidente. Según los países y las regiones se desarrolla en momentos cronológicos diversos, ofreciendo en su amplio desarrollo diferenciaciones profundas: más puro en Francia (siendo bien distinto el de París y el de Provenza), más horizontal y cercano a la tradición clásica en Italia (aunque al norte se acoge uno de los ejemplos más paradigmáticos, como la catedral de Milán), con peculiaridades locales en Flandes, Alemania, Inglaterra y España.

El arte gótico propiamente dicho coincide en el tiempo con la plenitud y la crisis de la Edad Media.

Si su predecesor, el arte románico, reflejaba una sociedad ruralizada de guerreros y campesinos, el gótico coincide con el resurgimiento de las ciudades, donde se desarrollaron la burguesía y las universidades, y con la aparición de nuevas órdenes religiosas (monásticas como los cistercienses y mendicantes como los franciscanos y los dominicos). También se acentuaron los conflictos y la disidencia (revueltas populares, herejías, desarrollo y crisis de la escolástica, Cisma de Occidente); culminando en los pavorosos espectáculos de la peste negra y la guerra de los Cien Años, un mundo tan cambiante que solo puede entenderse en términos de una mutación fundamental (para la historiografía materialista, la transición del feudalismo al capitalismo).

Frente a las iglesias y monasterios del románico, dicho esto de la forma general, el gótico eleva, como su obra arquitectónica emblemática, prodigiosas catedrales llenas de luz así como con una gran altura, siendo estas sus principales aportaciones técnicas, las cuales se encuentran justificadas en los escritos de Pseudo Dionisio Aeropagita, aunque también se desarrolló una importante arquitectura civil. Otra de sus características es que se comenzó a independizar a otras artes plásticas, como la pintura y escultura, de su subordinación al soporte arquitectónico.

No obstante, hay también muchos elementos de continuidad: este sigue siendo un arte predominante religioso; el monasterio como institución apenas varía excepto en detalles formales y de adaptación a nuevos requerimientos, pero su disposición no presentó variantes, y la planta de las iglesias, mayoritariamente catedrales, siguió siendo predominantemente de cruz latina con cabecera en ábside orientada al este, aunque se complique o varíe (plantas basilicales, colocación del transepto en el centro, complicación de naves, capillas y girolas). Sin duda el principal elemento de continuidad es la concepción intemporal de la obra: en la mayor parte de las construcciones los estilos se suceden y funden al ritmo de los siglos, sabiendo los contemporáneos que hacen una obra que ellos no verían terminada, ni quizá sus hijos o nietos, sino que la construcción de estas edificaciones implica el trabajo varias generaciones. En muchas de ellas, incluso se pone en valor el atrevimiento por comenzar un desafío técnico o económico, a veces por rivalidad política, que cuando se inicia no se ha planificado en su totalidad el proyecto por lo que no se sabe cómo culminarlo, es el caso de las catedrales de Siena y de Florencia.

Los nuevos edificios religiosos se caracterizan por la definición de un espacio que quiere acercar a los fieles, de una manera vivencial y casi palpable, los valores religiosos y simbólicos de la época. El humanismo incipiente liberaba al hombre de las oscuras tinieblas y le invitaba a la luz. Este hecho está relacionado con la divulgación de las corrientes filosóficas neoplatónicas, que establecen una vinculación entre el concepto de Dios y el ámbito de la luz. Como las nuevas técnicas constructivas hicieron virtualmente innecesarios los muros en beneficio de los vanos, el interior de las iglesias se llenó de luz, y la luz conformará el nuevo espacio gótico. Será una luz física, no figurada en pinturas y mosaicos; luz general y difusa, no concentrada en puntos y dirigida como si de focos se tratase; a la vez que es una luz transfigurada y coloreada mediante el juego de las vidrieras y los rosetones, que trasforma el espacio en irreal y simbólico. El color alcanzará una importancia crucial.

La luz está entendida como la sublimación de la divinidad. La simbología domina a los artistas de la época, la escuela de Chartres considera la luz el elemento más noble de los fenómenos naturales, el elemento menos material, la aproximación más cercana a la forma pura.

El arquitecto gótico organiza una estructura que le permite, mediante la utilización de la técnica, emplear la luz, luz transfigurada, que desmaterializa los elementos del edificio, consiguiendo claras sensaciones de elevación e ingravidez.

A nivel arquitectónico, el estilo gótico nació en torno a 1140 en Francia, siendo considerada como el primer monumento de este movimiento la basílica de la abadía real de Saint Denis (edificada por el abad Suger, consejero de Luis VII de Francia).

También desde finales del siglo XII y comienzos del XIII se divulga por los monasterios de la orden del Císter un estilo despojado de ornamentación y reducido a la pureza de los elementos estructurales, expresión de las concepciones estéticas y espirituales de Bernardo de Claraval, que se suele denominar arte cisterciense.

Este arte se ha definido durante mucho tiempo de manera bastante superficial exclusivamente por la utilización de uno de sus elementos, el arco apuntado, al que suele llamarse ojival, del que se deriva la bóveda de crucería que permite desplazar los empujes a contrafuertes externos, que se alejan aún más de los muros mediante el uso de arbotantes. Eso permitió la construcción de edificios mucho más amplios y elevados, y el predominio de los vanos sobre los muros. Los elementos sustentantes (pilares de complicado diseño) quedan mucho más estilizados. Pero la utilización de un elemento no puede definir un estilo de forma global, se trata de un problema más amplio, de una nueva etapa histórica, una nueva concepción del arte y con él del mundo. Un elemento estructural, por importante que sea, no puede resumir un concepto global sobre la vida.

En la escultura gótica las tallas en piedra continúan usándose para la decoración de la arquitectura, además de cumplir la función evangelizadora (el "catecismo de los analfabetos", la inmensa mayoría de la población) pero cada vez se emancipa más (paso del relieve al bulto redondo). La escultura gótica evolucionó desde un estilo alargado y rígido, aún en parte románico, hacia un sentimiento espacial y naturalista a finales del siglo XII y principios del siglo XIII. La influencia de las esculturas griegas y romanas que aún se conservaban se incorporaron al tratamiento de las telas, las expresiones faciales y la pose.

Las esculturas góticas nacieron en los muros de las iglesias, a mediados del siglo XII en la Isla de Francia, cuando el abad Suger hizo construir la abadía de Saint-Denis (h. 1140), considerada el primer edificio gótico, y muy pronto le siguió la catedral de Chartres (h. 1145). Anteriormente, no había tradición escultórica en la Isla de Francia, así que los escultores eran traídos de Borgoña y fueron quienes crearon las revolucionarias figuras que actuaban como columnas en el Pórtico Real de Chartres. Era un invento enteramente nuevo y proporcionaría el modelo para una generación de escultores. 

La escultura gótica se difundió por toda Europa occidental. En España se constata la penetración a través de maestros y obras que llegaron procedentes de Francia; por ejemplo, la influencia del taller de Rieux es bastante evidente en la Virgen del Patrocinio de Cardona. 

La influencia de la escultura francesa se extendió por toda Alemania a partir de 1225 con la catedral de Bamberg, que tiene el más amplio conjunto de escultura del siglo XIII, culminando en 1240 con el "Jinete de Bamberg" (la primera estatua ecuestre en el arte occidental desde el siglo VI, y que se cree que retrataba a Conrado II). 

En Inglaterra la escultura estaba más limitada a monumentos funerarios y decoraciones no figurativas, en parte debido a las limitaciones cistercienses sobre el uso de imágenes. 

En Italia aún persistía la influencia clásica, destacando obras como los púlpitos del baptisterio de Pisa (1269) y la catedral de Siena. Obra maestra tardía de la escultura gótica italiana es el "Arche scaligere" de Verona.

Las técnicas de tallado de madera se hacen cada vez más sofisticadas, llegando a su máximo esplendor en la integración del color y el diseño arquitectónico de complejísimos retablos. La recuperación de la tradición clásica de la fundición del bronce deberá esperar al Renacimiento italiano.

El escultor holandés Claus Sluter y el gusto por el naturalismo marcó el comienzo del fin para la escultura gótica, evolucionando hacia el clasicismo renacentista a finales del siglo XV.

Hasta alrededor de 1200 no apareció un estilo de pintura que pueda llamarse «gótico»; es decir casi 50 años después del comienzo de la arquitectura y la escultura góticas. La transición del románico al gótico es muy imprecisa y no hay un claro corte, pero podemos ver los comienzos de un estilo que es más sombrío, oscuro y emotivo que en el periodo previo. Esta transición ocurre primero en Inglaterra y Francia alrededor de 1200, en Alemania en torno a 1220 e Italia alrededor de 1300.

Es usual indicar que, mientras en el románico las representaciones figurativas son simplificadas e idealizadas, en el gótico se tiende a aumentar el realismo y naturalismo, aproximándose a la imitación a la naturaleza que será el ideal del renacimiento, incluyendo la representación de paisajes, que, no obstante, sigue siendo poco usual.
En el gótico, en correspondencia con las nuevas tendencias filosóficas y religiosas (recuperación de la filosofía de Aristóteles a través del averroísmo, humanismo de San Francisco de Asís) se tendió a aproximar la representación de los personajes religiosos (los santos, los ángeles, la Virgen María, Cristo) en un plano más humano que divino, dejándoles demostrar emociones (placer, dolor, ternura, enojo), rompiendo el hieratismo y formalismo románico.

También hay lentos avances en el uso de la perspectiva y de otras cuestiones técnicas en pintura en cuanto al tratamiento de los soportes (que permiten la mayor difusión de un arte mobiliar), los pigmentos y los aglutinantes.

La pintura, esto es, la representación de imágenes sobre una superficie, durante el periodo gótico, se practicaba en cuatro técnicas principales:

La pintura al óleo sobre lienzo no se hizo popular hasta los siglos XV y XVI y fue el punto de partida del arte renacentista.

Las artes decorativas o artes suntuarias del periodo gótico tuvieron un amplio desarrollo.

El florecimiento del negocio de la lana y los paños, vinculados a las ferias y rutas comerciales que recorren Europa de norte a Sur (de Florencia, Génova y Venecia a Champaña y Flandes, sin olvidar Medina del Campo), producen el nacimiento de un arte singular: el tejido de tapices, que tuvo un prestigio social importantísimo. No para sus autores, que nunca pasaron de la consideración de meros artesanos, sino para sus poseedores. No habiendo una clara separación entre las artes industriales y las que hoy consideramos bellas artes, podría decirse lo mismo de maestros de obras, pintores y escultores, que aunque conservemos el nombre de muchos de ellos, no pasaban de ejercer también uno de los oficios viles y mecánicos, ni siquiera equiparables a las profesiones liberales.

La etiqueta "Gótico internacional" no hace referencia a la totalidad del Gótico como "estilo internacional", sino a una determinada fase o estilo de la pintura y artes decorativas góticas.

En el siglo XIX, el entusiasmo romántico por lo medieval (como reacción frente al neoclasicismo academicista) y el historicismo, llevó a amplias restauraciones de edificios medievales, llegándose a establecer el estilo neogótico, arquitectura realizada a imitación de la gótica medieval. Se sueña a partir de este momento con un renacimiento del arte medieval, llenándose de nuevo contenido al término "gótico" que empieza a distinguirse y separarse claramente del románico.



</doc>
<doc id="15578" url="https://es.wikipedia.org/wiki?curid=15578" title="Arte abstracto">
Arte abstracto

El arte abstracto es una forma de expresión artística que prescinde de toda figuración y propone una nueva realidad distinta a la natural. Usa un lenguaje visual de forma, color y línea para crear una composición que puede existir con independencia de referencias visuales del mundo real. Abarca movimientos como el expresionismo abstracto, el suprematismo, el action painting, "De Stijl" o el constructivismo.

Las primeras innovaciones en el planteamiento artístico se atribuyen a artistas como James McNeill Whistler quien, en su pintura "Nocturne in Black and Gold: The falling Rocket," («Nocturno en negro y oro: el cohete que cae», 1872), puso más énfasis en la maravilla visual que en la representación de los objetos. Asimismo, un "interés objetivo en lo que se ve" aparece en cuadros de John Constable, J. M. W. Turner, Camille Corot y la mayoría de los pintores de plen air de la escuela de Barbizon.

Paul Cézanne había comenzado como impresionista pero su reconstrucción lógica de la realidad desde diferentes puntos espaciales, usando el color para crear módulos y planos, se convirtió en la base de un nuevo arte visual que más tarde desarrolló el cubismo de Georges Braque y Pablo Picasso.

Los pintores expresionistas exploraron el uso grosero de la superficie pictórica, dibujando distorsiones, exageraciones y color intenso. Los expresionistas produjeron pinturas cargadas emocionalmente que eran reacciones y percepciones de la experiencia contemporánea; y reacciones al impresionismo y otras direcciones más conservadoras de la pintura de finales del XIX. Aunque artistas como Edvard Munch y James Ensor se vieron influidos principalmente por la obra de los postimpresionistas fueron decisivos para el advenimiento de la abstracción en el siglo XX con obras como "El grito" y "La entrada de Cristo a Bruselas". 

El posimpresionismo tal como lo practicaron Paul Gauguin, Georges Seurat, Vincent van Gogh y Paul Cézanne tuvo un enorme impacto en el arte del siglo XX y llevó al advenimiento de la abstracción del siglo XX. La herencia de pintores como Van Gogh, Cézanne, Gauguin y Seurat fue esencial para el desarrollo del arte moderno. A comienzos del siglo XX, Henri Matisse y otros jóvenes artistas incluyendo a los precubistas Georges Braque, André Derain, Raoul Dufy y Maurice de Vlaminck revolucionaron el mundo artístico de París con pinturas de paisajes y figuras «salvajes», de mucho colorido y expresivos, que los críticos llamaron fovismo. El crudo lenguaje de color tal como lo desarrollaron los "fauves" directamente influyeron a otro pionero de la abstracción Vasili Kandinski.

Aunque el cubismo al final depende del tema representado fue junto con el fovismo el movimiento artístico que directamente abrió la puerta a la abstracción en el siglo XX. Pablo Picasso hizo sus primeras obras cubistas basándose en la idea de Cézanne de que toda representación de la naturaleza puede reducirse a tres sólidos: cubo, esfera y cono. Con la pintura Las señoritas de Aviñón, de 1907, Picasso creó dramáticamente un cuadro nuevo y radical representando un burdel primitivo y crudo con cinco prostitutas, mujeres violentamente pintadas, que recordaban máscaras tribales africanas y sus nuevas creaciones cubistas. El cubismo analítico fue desarrollado conjuntamente por Pablo Picasso y Georges Braque, desde alrededor de 1908 hasta 1912. El cubismo analítico, la primera manifestación clara del cubismo, fue seguido por el cubismo sintético, practicado por Braque, Picasso, Fernand Léger, Juan Gris, Albert Gleizes, Marcel Duchamp e innumerables artistas hacia los años veinte. El cubismo sintético se caracteriza por la introducción de diferentes texturas, superficies, elementos de "collage", "papier collé" y gran variedad de objetos diversos unidos. Los artistas de "collage" como Kurt Schwitters y Man Ray y otros influidos por el cubismo fueron decisivos para el desarrollo del movimiento llamado dadaísmo.

Desde principios de siglo las conexiones culturales entre artistas de las principales ciudades europeas y norteamericanas se habían vuelto extremadamente activos conforme se esforzaron por crear una forma de arte que igualara las altas aspiraciones del modernismo. Las ideas fueron capaces de influirse mutuamente a través de libros de artistas, exposiciones y manifiestos de manera que muchas fuentes estaban abiertas a la experimentación y formaron la base de la diversidad de modos de abstracción. El siguiente extracto, de "The World Backwards", proporciona alguna impresión de las interconexiones de la cultura de la época:

El conocimiento de David Burliuk de los movimientos de arte moderno debió ser extremadamente actualizados, pues la segunda exposición del movimiento "Sota de Diamantes", celebrada en enero de 1912 (en Moscú) incluyó no sólo pinturas enviadas desde Múnich, sino algunos miembros del grupo alemán Die Brücke, mientras que de París vinieron obras de Robert Delaunay, Henri Matisse y Fernand Léger, así como de Picasso. Durante la primavera David Burliuk impartió dos conferencias sobre el cubismo y planeó una publicación polémica, que "Sota de Diamantes" iba a financiar. Fue al extranjero en mayo y regresó determinado a rivalizar con el almanaque Der Blaue Reiter que había emergido de los impresores mientras el estaba en Alemania.

Algunos acercamientos al arte abstracto tenían conexiones con la música. La música proporciona un ejemplo de una forma de arte que usa los elementos abstractos del sonido y las divisiones del tiempo. El mismo Vasili Kandinski, que también era músico, fue inspirado por la posibilidad de marcas y color asociativo "resonando en el alma." La idea había sido propuesta por Charles Baudelaire, que todos nuestros sentidos responden a diversos estímulos pero los sentidos están conectados en un nivel estético más hondo. 

Íntimamente relacionado con esto, está la idea de que el arte tiene "La dimensión espiritual" y puede transcender la vida de cada día, alcanzando un plano espiritual. La Sociedad Teosófica popularizó la antigua sabiduría de los libros sagrados de la India, China en los primeros años del siglo. Fue en este contexto que Piet Mondrian, Vasili Kandinski, Hilma af Klint y otros artistas trabajando hacia el «estado sin objeto» se vieron interesados en lo oculto como una manera de crear un objeto «interior». Las formas universales e intemporales que se encuentran en geometría: el círculo, el cuadrado y el triángulo se convirtieron en elementos espaciales en el arte abstracto; eran, como el color, sistemas fundamentales que estaban por debajo de la realidad visible.

Cronológicamente, el lituano Mikalojus Konstantinas Čiurlionis está considerado, por sus composiciones no figurativas fechadas en 1904, como el primer pintor abstracto. Pero la abstracción como un estilo moderno internacional, coherente, vio sus verdaderas bases establecidas por Vasili Kandinski. Su obra ilustra la llamada abstracción lírica. Llegó, entre 1910 y 1912, a una abstracción impregnada de sentimiento, idealmente representativa de las aspiraciones de los artistas del grupo expresionista de Múnich Der Blaue Reiter, del que él mismo formaba parte. A partir de 1912, casi todos los artistas europeos hicieron experimentos en esta línea.

Para el año 1911 se habían creado muchas obras experimentales que buscaban el «arte puro». En Francia, Robert Delaunay elaboró, desde 1912, a partir de las teorías de Chevreul sobre el contraste simultáneo de los colores, sus "Ventanas" y sus primeras "Formas circulares cósmicas" abstractas, mientras que Frank Kupka exponía en el Salón de Otoño de 1912 "Amorfa, fuga de dos colores" y en 1913 "Planos verticales azules y rojos". En 1913 el poeta Guillaume Apollinaire llamó orfismo a la obra de Robert y Sonia Delaunay. Lo definió como "el arte de pintar nuevas estructuras a partir de elementos que no han sido tomados prestados de la esfera visual, sino que habían sido creados totalmente por el artista... es arte puro." 

En la misma época, en Rusia, Mijaíl Lariónov y Natalia Goncharova llevaron hasta la abstracción pura su método de transcripción del fenómeno luminoso, al que denominaron rayonismo "(Luchizm)". Sus dibujos usaban líneas como rayos de luz para hacer una construcción. Muchos de los artistas abstractos en Rusia se convirtieron en constructivistas creyendo que el arte no era ya nunca más algo remoto, sino la vida misma. El artista debía convertirse en un técnico, aprendiendo a usar las herramientas y materiales de producción moderna. "¡El arte a la vida!" era el eslogan de Vladímir Tatlin, y de todos los futuros constructivistas. Varvara Stepánova y y otros abandonaron la pintura de caballete y pusieron sus energías en el diseño para teatros y la obra gráfica.

Kazimir Malévich completó su primera obra enteramente abstracta, la suprematista, "Cuadrado negro" en 1915. Otro miembro del grupo suprematista, liubov Popova, creó las Construcciones Arquitectónicas y Construcciones de Fuerza Espacial entre 1916 y 1921. Malévich, Anton Pevsner y Naum Gabo argumentaban que el arte era esencialmente una actividad espiritual; para crear el lugar del individuo en el mundo, no para organizar la vida en un sentido materialista y práctico. Muchos de aquellos que eran hostiles a la idea de producción materialista del arte abandonaron Rusia. Anton Pevsner fue a Francia, Gabo marchó primero a Berlín, luego a Inglaterra y al final a los Estados Unidos. Vasili Kandinski estudió en Moscú luego se marchó a la Bauhaus. A mediados de los años veinte el período revolucionario (de 1917 a 1921) cuando artistas habían sido libres de experimentar, estaba acabado; y para los años treinta solo estaba permitido el arte del realismo social.

En 1906 la artista sueca Hilma Af Klint (1862-1944), pionera del arte abstracto, creó la serie "Pinturas para el templo" en los que abordan la espiritualidad, la evolución del ser humano y las religiones del mundo. La obra de la artista no se hizo pública hasta 1986, veinte años después de su muerte, por expreso deseo de Klint quien consideraba que su obra no sería entendida por el público.

Piet Mondrian fue evolucionando su lenguaje abstracto, de líneas horizontales y verticales con rectángulos de color, entre 1915 y 1919, el neoplasticismo fue la estética que Mondrian, Theo van Doesburg y otros del grupo De Stijl pretendían reformar el medio del futuro. En Italia el futurismo, mezclado con la influencia Bauhaus, guio el camino hacia un arte abstracto con una paleta de color distintivamente cálida como en las obras de Manlio Rho y Mario Radice. 

En el período de entreguerras (1918-1939), Theo van Doesburg, después de haber sido uno de los principales defensores del neoplasticismo, renovó de manera decisiva el arte abstracto al mantener que la creación artística solo debía estar sometida a reglas controlables y lógicas, excluyendo así cualquier subjetividad. 

El manifiesto del arte concreto, que publicó en París en 1931, dio lugar a la tendencia del mismo nombre que tuvo un gran desarrollo en Suiza con Max Bill y de Richard Paul Lose, en Francia con François Morellet, y en todas las formas de arte sistemático nacidas después de la guerra. Estas tendencias entraron entonces en competencia con las diversas corrientes tachistas y gestuales (Jean Bazaine, Alfred Manessier, Pierre Soulages y Georges Mathieu, entre otros) que el crítico Michel Tapié reagrupó bajo la denominación de arte informal.

La tradición abstracta conoció un importante renacer en Estados Unidos a partir de finales de la década de 1940 con la Action Painting (Jackson Pollock, Willem de Kooning, Franz Kline) y con la Colour-Field Painting (Barnett Newman, Mark Rothko, Clyfford Still).

Estas tendencias fueron desbancadas a partir de 1960 por la aparición del arte minimalista, que marcó un nuevo periodo de interés por la geometría y la estructura mientras que en Europa y Latinoamérica el Op Art y el arte cinético conocían sus horas de gloria (Yaacov Agam, Jesús Soto, Carlos Cruz-Diez, Victor Vasarely, Nicolas Schöffer y Bridget Riley, entre otros).

El final de la década de 1960 vivió el desarrollo de una abstracción centrada en el análisis de sus propios componentes, con los grupos BMPT y Support(s)-Surface(s) en Francia, o bien orientada hacia los problemas de definición de la naturaleza de la imagen con Sigmar Polke y Gerhard Richter en Alemania. Las tendencias a la vez neoexpresionistas y neogeométricas que se pusieron de manifiesto durante la década de 1980 mostraron un nuevo periodo de interés por la abstracción, que siguen adoptando hasta nuestros días numerosos artistas, como Ian Davenport, Juan Uslé, Pablo Rey o Jan Maarten Voskuil, inspirados por las más variadas motivaciones.





</doc>
<doc id="15580" url="https://es.wikipedia.org/wiki?curid=15580" title="Arte de China">
Arte de China

Desde los orígenes de la historia china se crearon objetos en bronce, jade, hueso y cuero que recogieron el espíritu y efecto buscado en los rituales chamanistas.

Estas formas de bronce y jade muestran por primera vez uno de los principios esenciales del arte chino: la síntesis entre el espíritu creador artístico y la función social y jerárquica a la que estaban destinados desde su concepción. El primero de ellos se mostraba en la exquisitez de las formas, en el origen de los temas decorativos tomando como paradigma las fuerzas de la naturaleza y su acción sobre el espíritu humano, y en el gran conocimiento técnico de los materiales que ha caracterizado todas las formas artísticas.

Como complemento tanto la diversificación de las formas como la iconografía con la que se adornaban correspondían a los principios de jerarquización social y uso ritual que caracterizó los inicios de la civilización china con la Dinastía Shang y la Dinastía Zhou. En esta última dinastía surgen las escuelas de filosofía que profundizando sobre la relación del individuo con su entorno y la consideración social del mismo, establecerán los fundamentos teóricos sobre los que siglos más tarde se desarrollaría la teoría china del arte.

Nos referimos fundamentalmente al taoísmo y al confucianismo, sin por ello afirmar que existe una clara división entre lo que algunos consideran arte taoísta como manifestación disgregada de un supuesto arte confuciano.

Es cierto que la poesía, pintura y caligrafía representan todas ellas a través del pincel, la esencia misma del pensamiento artístico taoísta, pero no hay que olvidar que incluso estas artes sublimes tuvieron su función social, su jerarquización y en consecuencia participaron del pensamiento confuciano.

Estas eran el arte con mayúsculas, reservado a una clase intelectual formada en los clásicos, y la tradición, donde se reconocía y valoraba al artista y la obra de arte en su unidad y no como producto social. Desde la primera escritura tratada artísticamente y convertida en arte de la caligrafía por Wang Xizhi en el siglo IV d.C. hasta los últimos heterodoxos de la Dinastía Qing, los pintores Zhuda y Shitao, la caligrafía, pintura y poesía han estado unidas en unos mismos principios técnicos y estéticos.,

Los instrumentos básicos -tinta, papel, pincel y tintero-,la formación clásica, y la búsqueda del ritmo, espontaneidad y expresividad basados en el trazo, la pincelada y el vacío han sido los elementos comunes a partir de los cuales se han desarrollado diacrónicamente a lo largo de los siglos.

La palabra, el carácter es considerado como una imagen, como la abstracción de una idea y concepto, y la imagen pictórica en la que se reconoce tanto a un carácter como a un paisaje se lee como una palabra, fusionándose así el pensamiento artístico en poesía-caligrafía-pintura.

Algunos de estos materiales se desarrollaron de una manera casi única en un contexto histórico determinado, mientras que otros se adoptaron a nuevos usos y formas. Así observamos que el bronce y el jade son característicos de las dinastías Shang y Zhou, ligados siempre al ritual y a la representación social.

La laca y la seda coinciden en asociarse con el momento histórico de expansión política y cultural del imperio chino durante la dinastía Han (206 a. C.- 220 d.C.), siendo también los primeros materiales sobre los que se diseña pensando únicamente en la belleza del objeto y no en su uso ritual.

A partir del reinado de Qin Shi Huang la cerámica, cuyas primeras formas aparecieron en el Neolítico sirviendo en muchos casos de referencia a las formas en bronce, adquiere un mayor valor al realizarse con ella la reproducción del gran ejército imperial con el que el emperador quiso proteger bajo tierra su mausoleo.

A partir de entonces, la cerámica (arcilla, terracota, gres y porcelana) se vuelve, gracias a la capacidad de organización laboral de los centros alfareros, las innovaciones técnicas y la habilidad de los artesanos en el material más versátil y polisemántico de todos.

Desde la sencillez del barro cocido y pintado el alfarero chino ha sido capaz mediante la aplicación de vidriados y técnicas decorativas y el control de la cocción una inmensa variedad formal y tipológica, capaz de satisfacer todos los gustos y necesidades.

La carestía de otros materiales (bronce, jade), hizo que a través de los barnices se intentará buscar los efectos cromáticos y plásticos de otros materiales. Así el barniz de óxido de hierro tratado en atmósfera reductora, producía una gama cromática del verde-oliva al azul lavanda con la que se pretendía imitar el aspecto del jade. Esta técnica decorativa conocida en China desde la Dinastía Han perduró hasta el siglo XX, siendo rebautizado el color verde de estas piezas por los europeos con el nombre de celadón.

Las piezas en cerámica funerarias encontraron en la Dinastía Tang su mejor expresión con la aplicación, mediante inmersión y goteo, de tres colores o sancai.

En la Dinastía Song, se transformaron totalmente los usos y las formas de la cerámica. El conocimiento desde el siglo X del caolín -ingrediente necesario para conseguir la porcelana- junto con el desarrollo económico de esta dinastía y la búsqueda de una mayor exquisitez en el diseño de los objetos cotidianos por parte de la clase letrada, permitió la aparición de nuevos tipos cerámicos. Así se diferenciaban los productos destinados a uso imperial, frente aquellos solicitados por los letrados, los comerciantes y las comunidades monásticas.

El cambio del siglo XIII se vio reflejado en el campo artístico en su industrialización y su distribución en el exterior del país. El tipo cerámico azul y blanco, es el más característico de esta transformación, siendo sinónimo de él el nuevo repertorio iconográfico y el paulatino cambio en su distribución y concepción espacial que afectaría a todos los materiales.

La última dinastía mostró al mundo un gusto por la ornamentación, la exuberancia técnica y el alarde formal, en una variedad de formas y materiales, que reflejaban el nuevo gusto y estética de la Dinastía Manchú.

Junto a la delicadeza estética de los materiales señalados, pensados para disfrute particular y en algunos casos también como símbolo de posición social, existieron otras formas de entender el arte.

La escultura en piedra y la arquitectura en madera fueron los cauces a través de los cuales la sociedad se manifestó como colectividad profundamente jerarquizada.

La escultura en piedra se inició como majestuosa y representativa decoración de los caminos funerarios de las tumbas imperiales en la Dinastía Han. Grandes animales reales y mitológicos, representación de los estamentos sociales -letrados, militares, extranjeros, etc.- fueron los temas elegidos para dignificar el poder.

Por ello es un arte anónimo, creación de talleres colectivos, en donde la piedra se tallaba monolíticamente en cuanto material y concepto. De todo ello son muestra las esculturas que flanquean el camino de los espíritus de las dinastías Han, Tang y especialmente las Tumbas Ming, así como la escultura representativa de los palacios imperiales.

Para esto los Chinos hicieron espectaculares esculturas que hoy en día nos impactan

Pero la escultura tuvo también fines religiosos ligados a la difusión del budismo en China. Las grutas de Yungang, Longmen y Dunhuang, muestran el trabajo en piedra, ladrillo y estuco, que dio forma al panteón budista. En ellos se aprecia la influencia extranjera y su transformación o adaptación al gusto y estética chinos, como una de las mayores aportaciones de los intercambios producidos en la Ruta de la Seda.

La arquitectura palaciega, funeraria, religiosa y civil, partió de simples sistemas de construcción y distribución espacial, haciéndose principalmente eco de su carácter de representatividad.

Por ello, tampoco fue considerada como un arte creativo sino como una labor de artesanos, especialmente carpinteros y decoradores, donde no tenían cabida innovaciones en el diseño o en la técnica de construcción.

Entre los ejemplos más significativos de la arquitectura china se encuentran los palacios -Ciudad Prohibida, Palacio de Veracruz, Chengde-, y los templos -Templo del Cielo, Pagoda de la Oca salvaje-, en los que se aprecia la imbricación de todos los materiales artísticos y su doble función artística y representativa.

La arquitectura china se caracteriza por distribuir el espacio en unidades rectangulares que se unen para formar un todo. El estilo chino combina rectángulos de diferentes tamaños y en diferentes posiciones de acuerdo con la importancia de la organización del conjunto, ahí sus construcciones piramidales: utiliza el Feng Shui. Se distinguen claramente los distintos niveles y elementos. El resultado es un aspecto exterior impresionante, pero al mismo tiempo dinámico y misterioso.

En la arquitectura tradicional china, la distribución de las unidades espaciales se rige por los principios de equilibrio y simetría. El eje constituye la estructura principal. Las estructuras secundarias se sitúan a ambos lados del eje formando el patio central y las habitaciones principales. Tanto las viviendas como los edificios oficiales, templos y palacios se ajustan a este principio fundamental. En la distribución del espacio interior se reflejan los valores éticos y sociales de los chinos.

En las viviendas tradicionales, por ejemplo, las habitaciones se asignan según la posición de cada persona en la jerarquía familiar. La cabeza de familia ocupa el cuarto principal, los miembros de mayor edad de la familia de éste viven en la parte de atrás y los más jóvenes, en las alas izquierda y derecha; los mayores en la izquierda y los más jóvenes en la derecha.

La arquitectura china se caracteriza también por el uso de una estructura de vigas y pilares de madera y un muro de adobe que rodea tres de los costados del edificio. La puerta y las ventanas principales se sitúan en el frente. Los chinos llevan usando la madera como uno de sus principales materiales de construcción desde hace miles de años. La madera representa la vida y ésta es la principal idea que la cultura china, en sus múltiples manifestaciones, trata de comunicar. Esta característica ha llegado hasta nuestros días.

El arte chino ha tenido una evolución más uniforme que el occidental, con un trasfondo cultural y estético común a las sucesivas etapas artísticas, marcadas por sus dinastías reinantes. Como la mayoría del arte oriental tiene una importante carga religiosa (principalmente taoísmo, confucianismo y budismo) y de comunión con la naturaleza. Al contrario que en Occidente, los chinos valoraban por igual la caligrafía, la cerámica, la seda o la porcelana, que la arquitectura, la pintura o la escultura, a la vez que el arte está plenamente integrado en su filosofía y cultura. 














</doc>
<doc id="15581" url="https://es.wikipedia.org/wiki?curid=15581" title="Arte islámico">
Arte islámico

Por arte islámico se conoce el estilo artístico desarrollado en la cultura generada por la religión islámica.

El arte islámico tiene una cierta unidad estilística, debido al desplazamiento de los artistas, comerciantes, mecenas y obreros. El empleo de una escritura común en todo el mundo islámico y el desarrollo de la caligrafía refuerzan esta idea de unidad. Concedieron gran importancia a la geometría y a la decoración, que podía ser de tres tipos:


En arquitectura, crearon edificios con funciones específicas tales como mezquitas y madrazas, siguiendo el mismo patrón básico, aunque con diferentes formas. Prácticamente no hay arte de la escultura pero las realizaciones de objetos de metal, marfil o cerámica alcanzan con frecuencia una alta perfección técnica. Existe también una pintura y una iluminación en los libros sagrados y profanos.

Para designarlo también se aplica incorrectamente el término arte árabe. Este error procede de una utilización inexacta de su significado, puesto que de las dos acepciones del término árabe, una es geográfica, aplicable a los naturales de Arabia, mientras que la otra es lingüística, referida los que hablan la lengua árabe de su cultura. El arte musulmán o arte islámico de la península ibérica recibe la denominación de arte hispanomusulmán.

La era islámica, Hégira, comienza en el año 622, fecha en que Mahoma marcha de La Meca a Medina huyendo de la intransigencia mostrada por su predicación. A partir de esa fecha, junto a la fe religiosa, surgieron unas nuevas actitudes sociales y políticas que, en menos de un siglo, se expandieron desde el golfo de Bengala hasta el océano Atlántico.

El islam (‘paz, a través de la obediencia con amor a Dios’) tiene como base espiritual (o metafísica) un libro sagrado, denominado el "Corán", que recoge la palabra de Allah (Dios), revelada de forma directa a Muhammad (Mahoma), el último mensajero del Islam, a lo largo de su vida, a través de pequeños versículos. La comunicación del mensaje divino fue realizada en lengua árabe (debido a que, en aquellos tiempos, el pueblo árabe era uno de los pueblos más nobles, honestos y sinceros que había sobre la faz de la Tierra. No obstante, el mensaje divino ya se había enviado a otros pueblos y en otras lenguas, con anterioridad al pueblo árabe, como la Torá para el pueblo judío y la Biblia para el pueblo cristiano), tras lo cual pasó a convertirse en el idioma oficial y en el vehículo de unidad.

Además del Corán existe otra fuente primordial que se conoce con el nombre de sunna ("costumbre", "hábito" o "manera"), relacionada con la figura del profeta. La "sunna" se configura a base de hadiz o conjunto de actos o dichos de Muhammad, constituyendo una auténtica ciencia de la tradición.

Todo musulmán ("muslim", "creyente") tiene que realizar cinco manifestaciones o actos en las que se recogen básicamente el contenido dogmático de la religión y sus aspectos de culto o rito. Son los conocidos como pilares del islam: profesión de fe, oración, , limosna, ayuno y peregrinación a la Meca. Cada uno de ellos tiene una especial incidencia en las expresiones artísticas. La profesión de fe o "sahada" ("No hay más Dios que Dios y Muhammad su profeta") explicita la no existencia del concepto de encarnación del cristianismo e hinduismo, al mismo tiempo que proclama que Muhammad es sólo el mensajero de Dios. Ello comporta la primacía del mensaje sobre el mensajero, del mismo modo que es, sin duda, la clave para el desarrollo que adquiere la escritura como motivo decorativo -la epigrafía- dentro del arte islámico. Refleja, al mismo tiempo, la tendencia anicónica latente en el islam desde los primeros momentos, si bien, no por ello, la figuración dejó de contar con cierta presencia aunque en ámbitos restringidos. Esta tendencia anicónica propiciará el gran desarrollo de motivos geométricos y vegetales con un grado de abstracción cada vez mayor que, junto a los epigráficos, definirán la ornamentación en el arte islámico.

La oración o "salat" es el precepto según el cual los musulmanes deben orar regularmente cinco veces al día. Ello exige un estado de limpieza ritual o abluciones, un espacio suficiente para prosternarse e inclinar la cabeza hasta el suelo y una correcta orientación hacia La Meca. Consecuencia de estas obligaciones es la existencia de un edificio, la mezquita ("masyid" o "lugar para prosternarse") con un muro qibla donde se halla el mihrab o "nicho" que señala la correcta orientación a La Meca. Las mezquitas suelen contar con un patio ("sahn") en el que existe una fuente ("mida") para las abluciones o limpieza corporal. Otros elementos asociados son el "minbar" o especie de púlpito con gradas para el "jutba" (sermón del viernes), la "maqsura" o acotamiento destinado a las autoridades, el alminar ("manara") desde cuya azotea el "muecín" llama a la oración y también utilizan las alfombras de oración ("sayyada") para mayor limpieza en el desarrollo de la oración.

La obligación de dar limosna ("zakat") produce en el terreno artístico la fundación de instituciones de caridad como "madrasas" o escuelas teológicas donde se enseña el Corán, "maristan" u hospitales, "hamman" o baños y fuentes públicas. El ayuno ("sawn") durante el mes de Ramadán, noveno del calendario lunar islámico, tiene menor trascendencia artística aunque puede concretarse en ciertos objetos realizados para las fiestas de ruptura del ayuno celebradas al final del Ramadán.

El último precepto, la peregrinación a La Meca ("hayy"), al menos una vez en la vida, permite el intercambio de ideas entre los países más alejados, la producción de obras especiales como los paños que el califa envía anualmente para cubrir La Kaaba o los certificados ornamentales de la peregrinación.

La religión, así pues, constituye el gran elemento unificador del amplio territorio y el dilatado marco temporal -siglo VII hasta la actualidad- por el que se ha expandido el islam. No obstante, este desarrollo espacio- temporal ha generado una enorme variedad de manifestaciones artísticas. Lógicamente, las condiciones geográficas - desde desiertos a zonas mesetarias o montañosas- así como los factores históricos y los consiguientes sustratos de civilización preexistentes en cada ámbito cultural han incidido de forma decisiva en las expresiones artísticas, determinando su diferente evolución y sus distintas peculiaridades. Sin embargo, estos condicionamientos y la asimilación de rasgos de todas aquellas culturas con las que ha ido manteniendo contacto, no ha llevado al arte islámico a convertirse en una mera repetición de formas y elementos ajenos. Al contrario, mediante la selección de entre un vasto repertorio y su utilización adecuada a su diferente función, ha logrado un arte profundamente original.

Poco se sabe sobre la arquitectura antes de la dinastía Omeya. El primero y más importante edificio islámico es, sin duda, la "casa del Profeta" en Medina. Esta casa, más o menos mítica, fue el primer lugar donde los musulmanes se reunieron para rezar, aunque la religión musulmana cree que la oración se puede hacer en cualquier lugar.

La casa del Profeta tuvo una gran importancia para la arquitectura islámica, puesto que establece el prototipo de la mezquita de diseño árabe, formada por un patio con una sala de oración hipóstila. Este modelo, adaptado a la oración, no nació de la nada, podría estar inspirado por el templo de Husa ( Yemen, siglo II a. C. ) o por la sinagoga Dura Europos ( renovada en el año 245).Construida con materiales perecederos (madera y barro), la casa del Profeta no sobrevivió por mucho tiempo, pero está descrita con detalle en las fuentes árabes. Actualmente, la Mezquita del Profeta se eleva en el lugar donde supuestamente se encontraba la casa de Mahoma.

Los primeros objetos islámicos son muy difíciles de distinguir de los objetos de épocas anteriores sasánidas y bizantinas, o ya omeyas. De hecho, el islam nació en efecto, en las zonas donde el arte parece haber sido poco abundante, pero rodeadas de imperios notables por su producción artística. Es por ello que, en los inicios del islam, los artistas islámicos utilizaron las mismas técnicas y los mismos motivos que sus vecinos. Se conoce, especialmente, una abundante producción de cerámica sin brillo, como lo demuestra un célebre "tazón" que se conserva en el Museo del Louvre, cuya inscripción nos asegura que su fabricación se remonta a la época islámica. El tazón proviene de uno de los pocos lugares arqueológicos que realiza un seguimiento de la transición entre el mundo preislámico y el islam: El de Susa en Irán.

Entre los Omeyas, la arquitectura religiosa y civil crece con la introducción de nuevos conceptos y diseños. De este modo, el plano árabe, con patio y sala de oración hipóstila, se convierte en un plano-modelo a partir de la construcción, en el lugar más sagrado de la ciudad de Damasco - en el antiguo templo de Júpiter y en el lugar donde estuvo la "Basílica de San Juan Bautista" - de la Gran Mezquita de los Omeyas. El edificio fue un importante hito para que los constructores (y los historiadores del arte) situaran allí el nacimiento del plano árabe. Sin embargo, recientes trabajos de Myriam Rosen-Ayalon nos sugieren que el plano árabe nació un poco antes, con el primer proyecto que se hizo para construir la Mezquita de Al-Aqsa en Jerusalén.

La Cúpula de la Roca en Jerusalén es, sin duda, uno de los edificios más importantes de toda la arquitectura islámica, caracterizado por una fuerte influencia bizantina ( mosaicos con fondo de oro, plano centrado que recuerda el del Santo Sepulcro ), pero que ya tiene elementos puramente islámicos, como el gran friso con inscripciones religiosas del Corán. Su modelo no se propagó, y el que Oleg Grabar considera como "el primer monumento que fue una gran creación estética del islam", quedó sin posteridad.

Los Castillos del desierto en Palestina nos ofrecen mucha información sobre la arquitectura civil y militar de la época, aunque su función exacta está aún en estudio: ¿parada para las caravanas, lugares de descanso, residencias fortificadas, palacios con fines políticos que permitían la reunión entre el califa y las tribus nómadas? Los especialistas se esfuerzan por descubrirla, y parece que su uso ha variado en función del lugar donde se encuentren. Anjar fue una ciudad encontrada completa y que nos informa sobre un tipo de urbanismo aún muy cercano al de la antigua Roma, con cardo y decumano, como en Ramla.

Además de la arquitectura, los artesanos trabajaban la cerámica, a menudo no esmaltada, a veces con un vidriado monocromo transparente, verde o amarillo, y también trabajaron el metal. Sigue siendo muy difícil diferenciar estos objetos de los del período pre-islámico, los artesanos reutilizaron elementos occidentales (follaje vegetal, hojas de acanto, etc) y sasánidas.

En la arquitectura como en las artes mobiliarias , los artistas y artesanos omeyas no inventaron nuevas formas o métodos, sino que reutilizaron de manera espontánea las de la Antigüedad tardía mediterránea e iraní y las adaptaron a su diseño artístico, por ejemplo, mediante la sustitución en la gran mezquita de Damasco de los elementos figurativos que tenían los mosaicos bizantinos, por dibujos de árboles y ciudades. En los "castillos del desierto" se reflejan en particular estos préstamos y adaptaciones. La mezcla de tradición y readaptación de motivos y elementos arquitectónicos, fue creando, poco a poco, un arte típicamente musulmán, palpable sobre todo en la estética de los arabescos, presente a la vez que en los monumentos en los objetos o en las páginas de los Coranes iluminados.

Con el desplazamiento de los centros de poder hacia el este, dos ciudades que serían sucesivamente capitales del califato cobraron gran importancia: Bagdad y Samarra en Irak. La ciudad de Bagdad no ha podido ser excavada porque está cubierta por la ciudad contemporánea. La conocemos por varias fuentes, que la describen como una ciudad circular en cuyo centro se construyeron grandes mezquitas y palacios. Samarra ha sido objeto de varias excavaciones, especialmente de Ernst Herzfeld y más recientemente de Alastair Northedge. Creada por Al-Mutasim, en el año 836, abarca unos treinta kilómetros , y tenía además de muchos palacios, dos grandes mezquitas y varios cuarteles. Abandonada definitivamente a la muerte de Al-Mu'tamid en el año 892 nos ofrece un hito cronológico fiable.

Samarra nos ha proporcionado una gran cantidad de mobiliario, especialmente estuco que servía como decoración arquitectónica y cuyos motivos pueden servir para la datación aproximada de los edificios.El estuco también se encuentra en el arte mobiliario desde el Egipto tulunida hasta Irán, sobre todo acompañando a la madera en la decoración.

El arte de la cerámica conoció por lo menos dos grandes innovaciones: la invención de la "fayenza" y la cerámica "de brillo metálico" que perdurarán durante mucho tiempo después de la desaparición de la dinastía. En el islam, se llama "faience" a una masa de pasta arcillosa, cubierta con un esmalte opaco tratado con óxido de estaño, y decorada. Las imitaciones de porcelana china se multiplicaron entonces gracias al óxido de cobalto, utilizado desde el siglo VIII en Suse, y que permite decorados en azul y blanco. El repertorio de motivos es todavía bastante limitado: motivos vegetales e inscripciones.

El brillo metálico habría nacido en el siglo IX, tal vez por la incorporación a la cerámica de un producto ya existente y que era utilizado en el vidrio. La cronología de esta invención y de los primeros siglos es muy difícil y ha dado lugar a muchas controversias. Los primeros brillos metálicos serían policromados, sin imágenes y a partir del siglo X pasarían a ser figurativos y monocromos, si hemos de creer la opinión más comúnmente aceptada, que se basa, en parte, en el mihrab de la Mezquita de Kairuán.
También se producía vidrio transparente u opaco, decorado por soplado en un molde o mediante la adición de otros elementos. Hay varios ejemplos de tallado de vidrio, el más famoso es probablemente "el tazón de las liebres", que se conserva en el tesoro de San Marcos en Venecia., y la decoración arquitectónica en este material que ha sido hallada en Samarra.

Desde el siglo IX el poder de la dinastía Abbasida es desafiado en las provincias más alejadas del centro de Iraq. La creación de un califato chií rival, el califato de la dinastía fatimí, seguido del califato de los Omeyas de España, dio cuerpo a esta oposición. También aparecieron pequeñas dinastías de gobernadores autónomos en Irán.

La primera dinastía que se instaló en la península ibérica (en "al-Ándalus") fue la de los Omeyas de España. Como su nombre indica, este linaje desciende del de los grandes Omeyas de Siria, diezmado en el siglo IX. La dinastía Omeya en España fue sustituida después de su caída por diversos reinos independientes, los reyes de taifas (1031 - 1091), pero la producción artística en este período no difiere demasiado tras este cambio político. Al final del siglo XI, dos tribus bereberes tomaron sucesivamente el poder en el Magreb y en España, entonces en plena "Reconquista", los almorávides y los almohades del norte de África, que aportaron su influencia magrebí al arte. 

Sin embargo, los reyes cristianos fueron conquistando la España islámica, que quedó reducida a la ciudad de Granada en el siglo XIV con la dinastía Nazarí, que consiguió mantenerse hasta el año 1492.
En el Magreb, los meriníes tomaron la antorcha de los almohades en el 1196. Desde su capital Fez participaron en muchas expediciones militares, tanto en España como en Túnez, de donde no pudieron desalojar a los hafsides, una pequeña dinastía firmemente establecida allí. Los meriníes vieron disminuir su poder a partir del siglo XV y fueron sustituidos de forma definitiva por la dinastía Sharifs en el 1549. La dinastía Hafsides gobernó hasta su desalojo por los turcos Otomanos en el 1574.

"Al-Andalus" fue un lugar de gran cultura en la época medieval. Además de importantes universidades como la de Averroes, que permitió la difusión de la filosofía y la ciencia desconocida para el mundo occidental, este territorio fue también un lugar en el que floreció el arte. En arquitectura, es evidente la importancia de la Gran Mezquita de Córdoba, pero esto no debería eclipsar otros logros como la mezquita de Bab al-Mardum en Toledo o la ciudad califal de Medina Azahara. También es especialmente importante el palacio de la Alhambra en Granada. Varios rasgos caracterizan la arquitectura de España: los arcos de herradura derivados de modelos romanos y visigodos. Los arcos polilobulados, muy habituales y que son típicos de toda la época islámica. La forma del mihrab, como una pequeña habitación, es también un rasgo bastante característico de España.
Entre las técnicas que utilizaron para la fabricación de objetos, el marfil fue ampliamente utilizado para la fabricación de cajas y cofres. La Píxide de Al-Mughira es una obra maestra, con muchas escenas figurativas y difíciles de interpretar.

Los tejidos, de sedas, en particular, fueron en su mayor parte exportados y se pueden encontrar en muchos tesoros de las iglesias occidentales envolviendo los huesos de los santos. En la cerámica, predominaron las "técnicas tradicionales", sobre todo el brillo metálico, que se usó en las baldosas o en una serie de vasos conocida como "vasos de la Alhambra".A partir del reinado de las dinastías magrebíes, también hubo un gusto por trabajar la madera, tallada y pintada: el Minbar de la mezquita de Kutubiyya de Marrakech, datado en 1137, es uno de los mejores ejemplos.

La arquitectura de África del Norte es relativamente desconocida por falta de investigación después de la descolonización. Las dinastías almorávides y almohades se caracterizan por una búsqueda de austeridad que se ejemplifica en las mezquitas con las paredes desnudas. Las dinastías merinides y hafsides patrocinaron una arquitectura muy importante pero poco conocida y un notable trabajo en madera pintada, tallada y taraceada.

La dinastía fatimí, que es una de las pocas dinastías del mundo islámico chiita, gobernó en Egipto entre el 909 y el 1171. Nacida en Ifriqiya en el 909, llegó a Egipto en el 969, donde fundó la ciudad califal de El Cairo, al norte de Fustat, que siguió siendo un importante centro económico. Esta dinastía alumbró una importante arquitectura religiosa y profana, cuyos restos incluyen las mezquitas de al-Azhar y al-Hakim, y las murallas de El Cairo, construidas por el visir al-Badr Jamali. También fue el origen de una rica producción de objetos de arte en una amplia gama de los materiales: madera, marfil, cerámica pintada con esmalte brillante, plata, incrustaciones de metal, vidrio opaco, y sobre todo, cristal de roca. Muchos artistas eran cristianos coptos, como lo demuestran las numerosas obras con iconografía cristiana.
Estos constituían la religión mayoritaria durante el reinado particularmente tolerante de los fatimitas. El arte se caracteriza por una rica iconografía, que explota mucho la figura humana y animal en las representaciones animadas, que tiende a liberarse de elementos puramente decorativos, como las manchas de color en la cerámica esmaltada. Se enriqueció, tanto estilística como técnicamente, a través de sus contactos con las culturas de la cuenca mediterránea, sobre todo Bizancio. La dinastía fatimita fue también la única que produjo escultura, a menudo en bronce.

Al mismo tiempo, en Siria, asumieron el poder los atabegs, es decir, los gobernadores árabes de los príncipes selyúcidas. Muy independientes, se apoyaron en la enemistad entre los príncipes turcos y ayudaron en gran parte a los cruzados francos. En 1171, Saladino tomó el Egipto fatimida, y puso en el trono a la efímera dinastía ayubida.
Este período no fue muy rico en arquitectura, lo que no impidió la renovación y mejora de las defensas de la ciudad de El Cairo. La producción de objetos valiosos no se detuvo. La cerámica pintada con esmaltes brillantes, y con incrustaciones de metal de alta calidad se siguieron produciendo y el vidrio esmaltado surgió a partir del último cuarto del siglo XII, como se ve en una serie de vasos y botellas de este período.

Los Mamelucos arrebataron el poder a los Ayyubidas de Egipto en el año 1250 y se instalaron en el 1261 en Siria, derrotando a los mongoles. No son, estrictamente hablando, una dinastía, porque los soberanos no reinan de padre a hijo: de hecho, los Mamelucos son esclavos turcos liberados, que (en teoría ) comparten el poder entre compañeros de libertad. Este gobierno paradójico se sostuvo casi tres siglos, hasta el 1517, y dio lugar a una arquitectura muy abundante en piedra, compuesta por grandes complejos hechos para los sultanes o emires, especialmente en El Cairo. La decoración se realiza con incrustaciones de piedras de diferentes colores, así como con un exquisito trabajo en madera que consistió en incrustaciones de motivos geométricos radiantes hechos en marquetería. Se utilizó también el esmalte y el vidrio, y lo que es más importante, las incrustaciones de metal: de este período data el Baptisterio de San Luis, uno de los objetos islámicos más famosos, realizado por el oefebre Muhammad ibn al- Zayn

Bajo estos "pequeños khanes", originalmente sometidos al emperador Yuan, pero rápidamente independizados, se desarrolló una rica civilización. La actividad arquitectónica se intensificó a medida que los mongoles se hicieron sedentarios y siguió estando más o menos marcada por las tradiciones de los nómadas, como queda demostrado en la orientación norte - sur de los edificios.Sin embargo, existe una importante influencia persa y la vuelta a las tradiciones ya establecidas, como el plano iraní. La tumba de Oldjaïtou en Sultaniya fue uno de los monumentos más impresionantes de Irán, pero lamentablemente está muy deteriorado y casi destruido. También, durante esa dinastía nació el arte del libro persa, en importantes manuscritos como el "Jami al-tawarikh" mandado hacer por el visir Rashid al-Din.

Aparecieron nuevas técnicas en la cerámica, como la de "lajvardina", y se ven influencias chinas en todas las artes.

El arte de estos nómadas es muy poco conocido. Los investigadores, que apenas están empezando a interesarse en ellos, han descubierto que hubo una planificación urbana y una arquitectura en estas regiones. Se desarrolló también una importante orfebrería y la mayor parte de sus obras muestran una fuerte influencia china. Conservadas en el Museo del Hermitage de San Petersburgo, apenas comienzan a ser estudiadas.

Fue la tercera invasión de los nómadas, la de las tropas de Tamerlán, la que fundó el tercer gran período medieval iraní: el de los Timurides. El desarrollo en el siglo XV de esta dinastía, dio lugar a la cúspide del arte del libro persa, con pintores como Behzad, y muchos mecenas. La arquitectura y el urbanismo persa, a través de monumentos como los de Samarcanda, en particular, experimentaron igualmente una edad de oro. La decoración en cerámica y las bóvedas con mocárabes son particularmente impresionantes. Existe una fuerte influencia del arte del libro y de China en todos los demás ámbitos. Es, en parte, el período Timurida el que dio cohesión al arte persa, permitiéndole florecer más tarde en el gran imperio de los Sefávidas.

Continuando en su impulso, los turcos seldyúcidas continuaron sus conquistas hasta Anatolia. Después de la batalla de Manzikert en 1071 formaron un sultanato independiente del de sus primos iraníes. Su poder parece extenderse desde 1243 hasta las invasiones mongolas, pero las monedas siguieron siendo acuñadas con sus nombres hasta el año 1304. La arquitectura y los objetos sintetizan los distintos estilos, tanto de Irán como de Siria. El arte del trabajo de la madera dará obras maestras,y sabemos de un único manuscrito ilustrado que data de ese periodo.

Los turkmecos, que son nómadas en la región del lago Van, son muy poco conocidos. Se les conocen, sin embargo, varias mezquitas como la Mezquita Azul de Tabriz y tendrán una influencia decisiva tanto en Anatolia, después de la caída de los Seldjoukidas de Rum, como en Irán durante la dinastía Timurida. En efecto, a partir de siglo XIII, Anatolia estaba dominada por pequeñas dinastías turcomanas, que decidieron apropiarse gradualmente de los territorios bizantinos. Poco a poco surge una dinastía: la de los Otomanos, los llamados "primeros Otomanos" antes de 1453. Patrocinaron sobre todo la arquitectura, donde se busca la unificación de los espacios mediante el uso de cúpulas. En la cerámica también se sentaron las bases para lo que se convertiría en el arte otomano propiamente dicho con la "cerámica de Mileto" y los primeros azules y blancos anatolios.

La India, conquistada por los Ghaznévidas y Ghurides en el siglo IX, no se independizó hasta el año 1206 cuando los Muizzî o reyes-esclavos, llegaron al poder, marcando el nacimiento del sultanato de Delhi. Más tarde, surgieron otros sultanatos competidores en Bengala, Cachemira, Guyarat, Jawnpur, Malwa y en el norte del Deccan (Bahmanidas). 

Se alejaron gradualmente de las tradiciones persas, dando nacimiento a una arquitectura y un urbanismo originales teñidos de sincretismo con el arte hindú. La producción de objetos está poco estudiada hasta este momento, pero sabemos de un importante arte del libro. El período de los sultanatos termina con la llegada de los Mogoles que poco a poco conquistaron toda la región.

La Arquitectura adopta muchas formas diferentes en el mundo islámico, a menudo su relación con la religión musulmana: la mezquita es una de ellas, pero la madraza y los lugares de retiro son también edificios típicos de los países del islam adaptados a la práctica del culto.

Los tipos de edificios varían mucho según los períodos y las regiones. Antes del siglo XIII, en la cuna del mundo árabe, es decir, en Egipto, en Siria, en Iraq y en Turquía, casi todas las mezquitas siguen el llamado plano "árabe", con un gran patio y una sala de oración hipóstila, pero que varían enormemente en su decoración e incluso en sus formas: en el Magreb las mezquitas adoptaron un plano en «T» con naves perpendiculares a la qibla, mientras que en Egipto y Siria las naves son paralelas. Irán tiene sus propias especificidades como el uso del ladrillo y la decoración en estuco y cerámica, el uso de formas particulares a menudo tomadas del arte Sasánida como los " Iwan" (porches de entrada abiertos por un gran arco) y el arco persa. En España, hay más bien un gusto por una arquitectura coloreada con el uso de arcos variados (de herradura, polilobulados, etc.).En Anatolia, bajo la influencia de la arquitectura bizantina, pero también debido a evoluciones específicas en el plano árabe en esta región, se construyeron las grandes mezquitas otomanas de cúpula singular y desproporcionada.En la India mogol los planos se fueron alejando gradualmente del modelo iraní, destacando mucho en sus edificios la cúpula bulbosa.

El arte del libro incluye tanto la pintura, la encuadernación, la caligrafía y la iluminación. Es decir, arabescos y dibujos en los márgenes y en los títulos.

Se divide tradicionalmente el arte del libro en tres ámbitos distintos: Árabe para los manuscritos sirios, egipcios, de Jezirah, e incluso otomanos del Maghgreb (pero éstos también pueden ser considerados por separado). Persa para los manuscritos creados en Irán, en particular durante el período mongol. Indio para las obras mogolas. Cada uno de estos ámbitos tiene su propio estilo, dividido en diferentes escuelas, con sus propios artistas y sus convenciones. Las evoluciones son paralelas, aunque parece evidente que ha habido influencias entre las escuelas, e incluso entre zonas geográficas, a través de los cambios políticos y los frecuentes desplazamientos de los artistas.

Son conocidas en Europa como "artes menores" las artes decorativas. Sin embargo, en las tierras del islam, como en muchas culturas de fuera de Europa o antiguas, estas artes se han utilizado ampliamente con fines más artísticos que utilitarios y han alcanzado tal punto de perfección que no se pueden clasificar como "artesanía".Por lo tanto, si los artistas islámicos no se interesaron en la escultura por razones principalmente religiosas, nos dejaron pruebas de un ingenio y una maestría notable en las artes del metal, la cerámica, el cristal, y el cristal de roca; y también en piedras duras como la calcedonia, el tallado en madera, la marquetería y el marfil, ...

Cuando se menciona el término arte islámico, a menudo se piensa en un arte sin imágenes compuesto enteramente de motivos geométricos y arabescos. Sin embargo, hay muchas representaciones de figuras en las artes del islam, particularmente en todo aquello que no está comprendido dentro del ámbito de la religión.

Las religiones han jugado un papel importante en el desarrollo del arte islámico, que a menudo se ha utilizado con fines sagrados. Se piensa, por supuesto, en la religión musulmana. Sin embargo, el mundo islámico no tuvo una mayoría musulmana hasta el siglo XIII y otras creencias también han desempeñado un papel importante en el islam. El cristianismo, particularmente, en un área que va desde Egipto hasta la actual Turquía. El zoroastrismo, especialmente en el mundo irání. El hinduismo y el budismo en el mundo indio y el animismo en todo el Magreb.

Sin embargo, no todo el arte islámico es religioso, y los artistas también utilizaron otras fuentes, entre ellas la literatura. La literatura persa, como el "Shahnamé ", la epopeya nacional compuesta a principios del siglo X por el poeta persa Ferdousí, los Cinco Poemas o "Jamsé" de Nezamí en el (siglo XII), es también una fuente importante de inspiración para muchos motivos que se encuentran tanto en el "arte del libro" como en los objetos (cerámicas, tapices, etc ). Las obras de los poetas místicos Saadi y Djami también han dado lugar a muchas representaciones. El "al-Jami tawarikh ", o "Historia Universal", compuesta por el visir ilkaní Rashid al-Din a comienzos de siglo XIV ha sido la inspiración de numerosas representaciones en todo el mundo islámico.

La literatura árabe no es la única con representaciones; las fábulas de origen indio "Calila y Dimna" o el "Maqamat" de Al-Hariri y otros textos fueron frecuentemente ilustrados en los talleres de Bagdad o Siria.

La literatura científica, como los tratados de astronomía o mecánica también tienen ilustraciones.

Los motivos decorativos son muy numerosos en este arte y muy variados, desde los motivos geométricos hasta los arabescos. La caligrafía en las tierras del islam está considerada como un arte, incluso sagrado, habida cuenta de que las suras del Corán se consideran como palabras divinas y que las representaciones de los seres vivos están excluidas de los libros y lugares religiosos, la caligrafía merece una atención especial, no solo en el ámbito religioso, sino también en las obras profanas.

A menudo se piensa que el arte islámico es totalmente anicónico, sin embargo, se pueden observar numerosas figuras humanas y animales en la cerámica. Las imágenes religiosas del profeta Mahoma, de Jesús y del "Antiguo Testamento" así como de los imanes, también dieron lugar a representaciones que, según épocas y lugares, tienen el rostro velado o no. La cuestión de la representación figurativa en el islam es aún hoy muy compleja.

El arte islámico ha sido durante mucho tiempo conocido en Europa gracias a las numerosas importaciones de materiales preciosos (seda, cristal de roca), que se hicieron en la época medieval. Muchos de estos objetos se han convertido en reliquias y se conservan actualmente en los tesoros de las iglesias del mundo occidental. Sin embargo, la historia del arte islámico como una ciencia es una disciplina muy reciente en comparación, por ejemplo, con la de otras artes antiguas. Por otro lado, las excavaciones de arte islámico han sido víctimas a menudo de arqueólogos que deseosos de acceder rápidamente a los niveles más antiguos, saquearon los niveles más modernos.

Nacida en el siglo XIX e impulsada por el movimiento orientalista, esta disciplina evolucionó marcada por muchos vaivenes, debidos a acontecimientos políticos y religiosos mundiales. La colonización, en particular, fomentó el estudio de algunos países - así como la aparición de colecciones europeas y americanas -, pero períodos enteros de la historia han quedado olvidados. Del mismo modo, la Guerra Fría, ha ralentizado considerablemente el estudio de las artes del islam, impidiendo la difusión de estudios y descubrimientos.

Como sucede a menudo, las grandes colecciones de arte islámico están más bien en el mundo occidental, en el Museo del Louvre, Museo Metropolitano de Arte, Museo Británico y Victoria and Albert Museum en particular. Sin embargo, existen colecciones en otros lugares, entre ellas las del Museo de Arte Islámico de El Cairo, Egipto, o el Museo de Arte Islámico de Doha, Catar. La Fundación Gulbenkian Lisboa y la colección Khalili también conservan numerosas piezas. Los museos americanos, como la Galería Freer de Washington, tienen fondos muy importantes, tanto de objetos como de manuscritos. El Museo Corning del Vidrio de Nueva York posee uno de las colecciones de vidrios islámicos más grande del mundo. En cuanto a los manuscritos, tenemos que señalar grandes bibliotecas como la British Library o la Biblioteca Nacional de Francia, cuyos fondos orientales están bastante completos aunque los museos conservan también páginas ilustradas y manuscritos.

Se están haciendo muchos progresos en el estudio de la producción de objetos y de la arquitectura islámica más antigua, especialmente en Iraq, Samarra o Susa o incluso en el El Cairo. A pesar del contexto actual, los principales yacimientos están siendo excavados en todo el mundo islámico desde Pakistán hasta el Magreb.





</doc>
<doc id="15583" url="https://es.wikipedia.org/wiki?curid=15583" title="Arte hispanomusulmán">
Arte hispanomusulmán

El arte islámico se desarrolló en al-Ándalus (la España musulmana) entre los siglos VIII y XV. Dada la limitación religiosa (aniconismo islámico) que afecta a la escultura y la pintura (a pesar de la cual hay algunos ejemplos), su manifestación principal fue la arquitectura andalusí (hispanomusulmana); aunque las artes suntuarias (o artes decorativas o artes menores –cerámica andalusí, eboraria, orfebrería, textil–) tuvieron un extraordinario desarrollo.

Los monumentos más importantes que han llegado hasta nosotros son la Mezquita de Córdoba y la Alhambra de Granada. La mezquita se construyó aprovechando materiales antiguos e incorporando el doble arco de herradura como solución frente al atirantado. La Alhambra es el palacio real nazarí, y a la vez una alcazaba (castillo). 

Muy vinculados al arte andalusí están dos peculiares estilos artísticos de la Edad Media española: el arte mozárabe (el de los cristianos bajo dominio musulmán, o emigrados desde al-Ándalus a los reinos cristianos del norte, donde influyeron de forma notable en el prerrománico local) y el arte mudéjar (el de los musulmanes bajo dominio cristiano, muchos de ellos especializados en artesanías de la construcción –albañiles, carpinteros, estuquistas–, que caracterizó estilos híbridos denominados románico-mudéjar y gótico-mudéjar, y dejó una gran influencia en estilos de transición al renacimiento español –hispanoflamenco, plateresco, isabelino o "estilo Reyes Católicos" y "estilo Cisneros"–).

La invasión musulmana del reino visigodo (711) significó, en el ámbito artístico y cultural, un cambio de orientación de los modelos, pero también un sincretismo del que la civilización árabe es característica; destacadamente, la reutilización de elementos de iglesias visigodas que se transformaron en mezquitas, lo que implicó la adopción y transformación del arco de herradura.

Hasta 1492, en que desaparece el reino nazarí de Granada, en al-Ándalus se mantuvieron unas condiciones culturales peculiares que le diferenciaron tanto del Islam oriental como del arte europeo. Pero, al mismo tiempo, esta singularidad geográfica y cultural constituyó uno de los factores que repercutieron decisivamente en el despertar de Europa tras los siglos de desunión y letargo que siguieron a la caída del Imperio Romano de Occidente y las invasiones bárbaras.

La conquista musulmana no supuso la extinción de las comunidades cristianas y judías. Unos huyeron al norte, donde formaron un reducto de oposición al nuevo poder instituido en Córdoba y, con el tiempo, constituirían el germen de la posteriormente llamada Reconquista; otros, los cristianos que permanecieron en territorio musulmán, pasaron a ser conocidos con el apelativo de mozárabes. Tanto esta minoría como la judía gozaron de la protección estatal, conformando comunidades numerosas en grandes ciudades como Mérida, Toledo, Valencia, Córdoba, Sevilla, Granada, Almería, Málaga, etc.

Desde el punto de vista artístico, el emirato andalusí emplea un estilo que no difiere en demasía del resto del Califato Omeya. Es decir, la adecuación de fórmulas y elementos de las culturas que les habían precedido, en este caso del mundo romano y visigodo. En ningún momento se produce una repetición literal de motivos y formas; al contrario, su inteligente incorporación y asimilación se traduce en una verdadera eclosión creadora, originándose el momento cúspide del arte califal. En él se funden elementos de la tradición local hispanorromano-visigótica con los elementos orientales, tanto bizantinos, como omeyas o abasíes.

Los edificios artísticos se centran, desde el primer momento, en torno a su capital, Córdoba, en la que se construyó una mezquita congregacional destinada a convertirse en el monumento más importante del occidente islámico. Destacan, entre otras, las obras llevadas a cabo durante el reinado de Abd al-Rahmán II, corte que acogió a numerosos artistas, modas y costumbres orientales; impulsó, entre otras, las construcciones del Alcázar de Mérida así como la del alminar de la iglesia de San Juan en Córdoba e hizo mejorar sus murallas y las de Sevilla. El califa Abderramán III, siguiendo la tradición oriental, (según la cual cada monarca, como signo de prestigio, debía poseer su propia residencia palaciega), decidió fundar la ciudad áulica de Medina Azahara (Medina al-Zahra).

En el resto del territorio peninsular también es patente el florecimiento artístico impulsado por el califato. Entre los de carácter religioso figuran las mezquitas, medersas o madrazas y mausoleos. En la ciudad de Toledo todavía se perciben restos de su fortificación, así como algunos vestigios que definen su alcazaba, medina, arrabales y entorno. De entre ellas destaca la pequeña mezquita del "Cristo de la Luz" o de "Bab al-Mardum". Y obras tan significativas como la rábida de Guardamar del Segura (Alicante), el Castillo de Gormaz (Soria) o la Ciudad de Vascos (Toledo).

El refinamiento imperante en la corte califal propició la creación de toda clase de objetos decorativos que, bajo el patrocinio real, se tradujeron en las más variadas expresiones artísticas. Mención especial merecen los trabajos en marfil, entre los que se encuentran todo tipo de objetos de uso cotidiano minuciosamente tallados: botes y arquetas destinadas a guardar joyas, ungüentos y perfumes; almireces, pebeteros, ataifores, jarras y jofainas de cerámica vidriada etc. En el Museo Arqueológico Nacional, puede contemplarse el Bote de Zamora, destinado a la mujer de al-Hakam II o la arqueta de Leyre, que dan buena muestra de ello.

Los monarcas, igual que en Bagdad y El Cairo, crean su propia fábrica de tejidos o bandas, lo que da lugar al principio de la historia de la producción de tejidos en seda bordada en al-Ándalus. Los motivos vegetales y figurativos geometrizados se inscriben en medallones que forman bandas tal y como aparecen en el velo o almejí de Hisham II que, a modo de turbante, le cubría la cabeza y le colgaba hasta los brazos. 

Asimismo existían los talleres en los que se trabajaba el bronce, tallado con figuras que representaban leones y ciervos con el cuerpo cubierto de círculos tangentes evocando tejidos y que se utilizaban como surtidores en las fuentes. Su paralelismo formal y estilístico con piezas de los fatimis ha conducido a la controversia acerca de la legitimidad de algunas de estas piezas.

La cerámica cuenta con tipos de producción conocida como verde y manganeso. Su decoración a base de motivos epigráficos, geométricos y una destacada presencia de motivos figurativos se consiguen mediante la aplicación del óxido de cobre ("verde") y óxido de manganeso ("morado").

La destrucción de la unidad política llevó a la abolición del califato cordobés en 1031 y a la creación de un mosaico de reinos independientes que fueron denominados taifas (de "tawaifs", partidos, facciones). Las rivalidades entre ellos, reivindicando la herencia del prestigio y la autoridad del Califato, constituyeron la tónica dominante del período. Esta situación se tradujo en el terreno artístico en la emulación de modelos cordobeses.

En este contexto se inserta la arquitectura palatina patrocinada por cada uno de los monarcas. Uno de los mejores testimonios es, sin duda, la Aljafería de Zaragoza, emparentada tipológicamente con el palacio omeya de Msatta (Jordania). Cuenta con organización tripartita donde cada uno de los sectores estaba dedicado a funciones diferenciadas. El sector central, de uso protocolario, está dominado por un patio rectangular cuyos lados menores estaban ocupados por albercas, pórticos y estancias alargadas acotadas en los extremos por alcobas. Este esquema deriva, sin duda, de los modelos palatinos cordobeses. A esta misma tradición responde el repertorio de arcos desplegado en el edificio, entre los que encontramos desde arcos lobulados, mixtilíneos, de herradura semicircular y apuntada, a complejas organizaciones de arcos entrecruzados, superpuestos y contrapuestos. Todos ellos están realizados con materiales pobres, pero revestidos de yeserías con motivos vegetales, geométricos y epigráficos, buscando un efecto de fastuosidad y aparente riqueza.

Las viejas alcazabas de los distintos reinos también sufrieron importantes remodelaciones. En la de Málaga se añadió un doble recinto amurallado con torres cuadradas y un palacio al que corresponden los restos de los llamados Cuartos de Granada. La vieja alcazaba de Granada, conocida como qadima (antigua), situada en la colina del Albaicín, se fortificó con torres cuadradas y redondas y se le añadieron algunas puertas en recodo, como la puerta Monaita y la puerta Nueva. Asimismo, la ciudad conserva unos baños conocidos como El Bañuelo, en la carrera del Darro, organizados en tres estancias de las cuales la central o templada adquiere, por razones de uso, unas mayores dimensiones. Baños muy similares se conservan en Toledo, Baza y Palma de Mallorca. La alcazaba de Almería fue fortificada con muros de tapial, construyéndose en su interior un palacio, "al-Sumadihiyya", rodeado de jardines. En los casos de Toledo y Sevilla, reinos que pujaron más fuertemente por la herencia cordobesa, se conservan deslumbrantes testimonios de las crónicas árabes sobre sus palacios, así como escasos fragmentos generalmente descontextualizados.

Al igual que la arquitectura, las artes suntuarias siguieron la tradición cordobesa aunque el protagonismo fue adquirido por otros centros. Así la producción de marfil se trasladó al taller de Cuenca mientras que el prestigio en los textiles fue adquirido por el taller de Almería. Por lo que respecta a la cerámica, se consolidó una técnica que había aparecido durante el califato pero que en estos momentos adquirió un gran desarrollo. Se trata de la cerámica de "cuerda seca" cuyas piezas se decoran con líneas de óxido de manganeso formando diferentes motivos que se rellenan con vidrio de diferentes colores y tamaños.

Las obras realizadas durante el reinado del monarca Yusuf ibn Tasufin, evidenciaban, todavía, la austeridad y falta de ornamentación impuestas por su fervor religioso. Rigor formal que no mantuvo su hijo Alí ibn Yusuf que, deslumbrado por el refinamiento cortesano de las taifas andalusíes, patrocinó la construcción de varios edificios decorados con los más bellos elementos.

El soporte preferido es el pilar, en sustitución de la columna. Adoptan el arco de herradura y lobulado, a los que añaden arcos de herradura o túmidos, lobulados trebolados, mixtilíneos y lambrequines formados, estos últimos, por pequeñas curvas, ángulos rectos y claves pinjantes. En relación al desarrollo de los arcos aplican, desde el salmer, un motivo en "S" denominado serpentiforme, ya utilizado anteriormente en la Aljafería de Zaragoza. El sistema de tejados preferido es a dos aguas, construyen techos de madera y alcanzan un gran desarrollo en el arte mudéjar, a la vez que realizan extraordinarias cubiertas cupuladas. Unas, representadas por la cúpula del mihrab de la mezquita de Tremecén, seguirán el modelo cordobés: arcos entrecruzados que dejan la clave libre si bien, en este caso, arrancan de trompas angulares de mocárabes y utilizan unos complementos de estuco calado decorados con exuberantes motivos florales. A partir de esta obra, en la que se documenta la introducción en el Magreb del mocárabe, aparecen otros tipos de cúpulas denominadas de mocárabes, como la que puede verse en la mezquita de Qarawiyyin en Fez.

Los trabajos artísticos continuaron vinculados a las tradiciones anteriores. El taller textil de Almería alcanzó su cenit realizando los famosos «attabi». Estos tejidos se caracterizan por la utilización de colores más suaves con toques de oro formando círculos dobles, tangentes o enlazados, dispuestos en filas, en cuyo interior se bordan parejas de animales. La similitud con los tejidos sicilianos permite que se confundan ambos talleres. Un problema similar plantean los marfiles, que contienen inscripciones ambiguas que no acaban de aclarar a cuál de los dos talleres pertenecen. La cerámica, por su parte, continúa desarrollando la técnica de "cuerda seca parcial" o "total" dependiendo de que la decoración cubra toda la superficie o parte de ella. Al mismo tiempo aparecen dos nuevas técnicas aplicadas a la cerámica no vidriada: el esgrafiado y el estampillado, que se generalizarán en la época almohade.

El retorno a la austeridad más extrema condujo, incluso de forma más rápida que en el caso de sus predecesores, los almorávides, a uno de los momentos artísticos de mayor esplendor (ver arte almorávide), de manera particular en lo que atañe a la arquitectura. El arte almohade continuó la estela almorávide consolidando y profundizando en sus tipologías y motivos ornamentales. Construían con los mismos materiales: azulejos, yeso, argamasa y madera. Y mantuvieron, como soporte, los pilares y los arcos utilizados en el período anterior. 

Sus mezquitas siguieron el modelo de la mezquita de Tremecén, con naves perpendiculares al muro de la quibla. En ellas se potenció un esquema en "I" mediante la utilización de cúpulas que son de mocárabes en la mezquita de Tinmal y en la de Kutubiyya de Marrakech. Se caracterizan por su planta cuadrada y su altura compuesta por dos torres, una de ellas alberga otra y, entre ambas, discurre una escalera o rampa, como en el caso de la Giralda de Sevilla. La torre interior está formada por estancias abovedadas y superpuestas que tendrán su repercusión posterior en las construcciones de otras torres-campanario mudéjares, especialmente en las edificadas en Aragón.

La arquitectura palaciega introduce los patios cruzados que ya habían hecho su aparición en Medina al-Zahra, pero que es, en estos momentos, cuando adquieren su mayor protagonismo. Su mejor testimonio se halla representado en el Alcázar de Sevilla, en el que se ha conservado el patio de la "Casa de Contratación" y otro, actualmente subterráneo, conocido como el "Jardín Cruzado" o "Baños de doña María Padilla". Este esquema será aplicado, asimismo, en los patios nazarís y mudéjares. Otra novedad aparece en el "Patio del Yeso" del Alcázar de Sevilla, y tendrá una gran repercusión. Consiste en la colocación de pequeñas aberturas o ventanas cubiertas con celosías de estuco que dan acceso a una estancia y que permiten, de este modo, su iluminación y ventilación.

La arquitectura militar experimenta un enriquecimiento tipológico y se perfecciona su eficacia defensiva que tendrá gran trascendencia, incluso para el ámbito cristiano. Aparecen complejas puertas con recodos a fin de que los atacantes, al avanzar, dejen uno de sus flancos al descubierto; torres poligonales para desviar el ángulo de tiro; torres albarranas separadas del recinto amurallado pero unidas a él por la parte superior mediante un arco, lo cual permite aumentar su eficacia defensiva respecto a una torre normal, como la Torre de Espantaperros de Badajoz o la Torre del Oro de Sevilla; muros reforzados que discurren perpendiculares al recinto amurallado con objeto de proteger una toma de agua, una puerta, o evitar el cerco completo; barbacanas o antemuros y parapetos almenados.

En el terreno decorativo aplicaron un repertorio caracterizado por la sobriedad, el orden y el racionalismo, lo que se traduce en la aparición de motivos amplios que dejan espacios libres en los que triunfan los entrelazados geométricos, las formas vegetales lisas y lo más novedoso: la "sebqa". Otra decoración arquitectónica que aparece en este alminar y en la mezquita de Kutubiyya, es la cerámica, en la que se aplica la técnica del alicatado; es decir piezas recortadas que, combinadas entre sí, componen un motivo decorativo. En otras ocasiones estas manifestaciones artísticas unen el carácter ornamental con el funcional.

Las obras de arte de esta época están peor representadas a causa de la confusión existente entre los diferentes períodos artísticos. Es lo que ocurre, por ejemplo, con los tejidos, que no se distinguen fácilmente de los mudéjares: acusan una práctica ausencia de motivos figurativos en tanto que aumenta la decoración geométrica y epigráfica a base de la repetición insistente de palabras árabes como "bendición" y "felicidad". En cuanto elementos metálicos, destacan los aguamaniles que representan figuras de animales decoradas con incisiones vegetales cinceladas.

El arte nazarí es un estilo surgido en la época tardía de al-Ándalus en el reino nazarí de Granada. Los dos paradigmas del mismo lo constituyen los palacios de la Alhambra y el Generalife. 

La arquitectura militar desarrolla los mismos sistemas generados en la época anterior, dotándola de una mayor complejidad. La arquitectura palaciega emplea dos tipos de organización de patios: uno el patio monoaxial, "patio de los Arrayanes" o de la "Alberca", y otro, el patio cruzado, "patio de los Leones". Las estancias vinculadas a ellos responden, nuevamente, a dos tipologías: una alargada en cuyos extremos están las alcobas, y otra cuadrada rodeada por las habitaciones, por ejemplo, la "Sala de la Barca" y la "Sala de las Dos Hermanas". Los escasos vestigios de arquitectura religiosa permiten pensar en mezquitas que siguen el modelo almohade, con naves perpendiculares al muro de la «qibla». Quizá la única novedad destacable provenga del hecho de la utilización de columnas de mármol cuando el edificio es de cierta relevancia. 

En cuanto al repertorio ornamental utilizan una profusión decorativa que enmascara la pobreza de los materiales, emplean desde zócalos alicatados y yeserías de estuco, a decoración pintada como la que se conserva en la bóveda de la "Sala de los Reyes". Es característica la columna de fuste cilíndrico y el capitel de dos cuerpos, uno cilíndrico decorado con bandas y otro cúbico con ataurique. Los arcos preferidos son los de medio punto peraltado y angrelados. Las techumbres de madera alternan con bóvedas mocárabes realizadas con estuco como los de la Sala de las Dos Hermanas o la de los "Abencerrajes". Asimismo, a los motivos ornamentales habituales (geométricos, vegetales y epigráficos), se une el escudo nazarí que será generalizado por Mohamed V.

En las artes suntuarias destacan las cerámicas de reflejos metálicos y los tejidos de seda a los que pueden añadirse los bronces, las taraceas y las armas. La cerámica de lujo, conocida como de "reflejo metálico" o "losa dorada" se caracteriza por someter, la última cocción, a fuego muy bajo "de oxígeno" y menor temperatura. Con este procedimiento la mezcla de sulfuro de oro y cobre empleada en la decoración llega a la oxidación reduciendo el brillo metalizado. Era frecuente, también, añadir óxido de cobalto con lo que se conseguían unos tonos azules y dorados. Los tejidos se caracterizaban por su intenso colorido así como por los motivos, idénticos a los empleados en la decoración arquitectónica.

El arte mudéjar tuvo lugar entre el siglo XII y el siglo XVI, y fue un fenómeno autóctono y exclusivamente hispánico, realizado por los mudéjares, cristianos y moriscos. Básicamente, es un estilo para cristianos pero que incorpora influencias, elementos o materiales de estilo hispanomusulmán.

En este arte influyó la situación fronteriza en continuo movimiento. El estilo gótico estaba asentado en el norte de la península y, a medida que avanzaba la reconquista, iba progresivamente condicionando el mudéjar. La posterior conquista de al-Ándalus conlleva un mudéjar más joven y con influencias directas de la arquitectura tradicional. El alarife, en su faceta de albañil especializado, utilizaba materiales simples como azulejos, yeso, escayola, mampostería, madera etc., como materia prima básica para crear una obra cargada de imaginación. Como maestro de obras y «experto» en todo tipo de construcciones, y sin competencia entre sus pares cristianos, el alarife descendió en la jerarquía arquitectónica pero continuó siendo indispensable en la obras de iglesias, sinagogas, fortificaciones, palacios, fuentes, etc. 

En el arte mudéjar destacan dos escuelas diferentes:


El último estilo sería el neomudéjar, como fase final y evolutiva en el tiempo.




</doc>
<doc id="15584" url="https://es.wikipedia.org/wiki?curid=15584" title="Arte emiral y califal">
Arte emiral y califal

El arte emiral y califal andalusí comprende sus manifestaciones artísticas desde la conquista musulmana de la península ibérica hasta el surgimiento de los primeros reinos de taifas, es decir, los siglos VIII al X.

Este tipo de arte va a darse lugar especialmente en Córdoba, capital del califato creado por Abderramán III en 929, donde se construyen los edificios más representativos del poder andalusí, no solo la gran mezquita aljama, sino una ciudad califal a las afueras del núcleo urbano: Medina Azahara, de gran lujo y breve existencia, pues fue destruida por la guerra civil al poco de construirse.
En el resto del territorio peninsular sobreviven algunos ejemplos, sobre todo en la ciudad de Toledo, donde todavía queda una puerta islámica del recinto urbano fortificado, la Puerta Antigua de Bisagra o Puerta de Alfonso VI, así como la mezquita de barrio Bab al-Mardum, más conocida tras su conversión en iglesia como ermita del Cristo de la Luz. En estado de restos arqueológicos quedó la rábida de Guardamar del Segura en Alicante o la Ciudad de Vascos de la provincia de Toledo.

La refinada corte de los califas multiplicó las artes decorativas, como los objetos de marfil, cerámica, vidrio, o metal y los tejidos. En el Museo Arqueológico Nacional se conserva el Bote de Zamora, destinado a la mujer de al-Hakam II, o la arqueta de Leyre.

El territorio andalusí se convierte, tras la conquista, en una parte más del Califato Omeya de Damasco como Emirato dependiente, si bien la distancia que le separaba de la sede del califato permitió a sus gobernadores gozar de una relativa autonomía. En 755 llegó a al-Ándalus el futuro Abd al-Rahman I, único sobreviviente omeya de la masacre perpetrada contra esta dinastía por los abbasíes. Un año más tarde se proclamaba emir independiente. A pesar de ello, continuó reconociendo la autoridad religiosa del nuevo califa abbasí cuya corte se había trasladado a Bagdad.

El paso definitivo se consumó con Abd al-Rahman III. Este monarca conjuró los problemas internos y externos, pacificando el levantisco territorio peninsular y enfrentándose a la amenaza del recién instaurado Califato fatimí de El Cairo. Ello le permitió proclamarse califa en el 929, afirmando su autoridad política y religiosa respecto a abbasíes y fatimíes. El Califato de Córdoba constituyó uno de los momentos de mayor esplendor y brillantez cultural aunque su florecimiento fue poco duradero. El comienzo del fin empieza a atisbarse cuando Almanzor relegó a Hixam II (976-1013) y acaparó el poder. A la muerte del hijo y sucesor de Almanzor se desencadenaron las luchas civiles entre facciones para imponer su propio candidato, lo que determinó la independencia de los diferentes territorios y la abolición del califato en el año 1031.

Las empresas artísticas se centraron desde el primer momento en torno a su capital Córdoba que fue dotada de una mezquita congregacional destinada a convertirse en el monumento más importante del occidente islámico. La obra fue iniciada por Abd al-Rahman I sobre el solar de la basílica visigoda de San Vicente que, durante la etapa precedente, habían compartido las dos comunidades: cristiana y musulmana. En el 784 este monarca decide hacer una mezquita de nueva planta de tipo basilical con once naves perpendiculares al muro de la qibla -siguiendo el modelo de la mezquita de Al-Aqsa en Jerusalén, uno de los lugares sagrados más importantes del mundo islámico-. Su rasgo más singular resuelve, al mismo tiempo, problemas técnicos y funcionales. Se trata de la organización de sus arquerías de doble arco superpuesto: un arco de herradura que actúa como de entibo al atirantar una estructura más esbelta formada por un arco de medio punto que soporta, a su vez, el muro que sostiene la techumbre. Tanto este sistema como la alternancia de dovelas, de ladrillo y piedra, cuenta con precedentes en el acueducto de los Milagros en Mérida.

Las sucesivas ampliaciones, llevadas a cabo hasta el siglo X, fueron motivadas por el aumento de población y su necesidad de contar con un lugar adecuado para el culto. De forma que las obras de Abd al-Rahman II, en 833, consistieron en derribar el muro de la qibla prolongando la mezquita hacia el sur. En sentido contrario actuó Abd al-Rahman III, ampliando el patio hacia el norte y levantando un nuevo alminar que todavía permanece, aunque oculto, dentro de la gran torre campanario del siglo XVI. Los esfuerzos anteriores culminan con la intervención de al-Hakam II, hacia 961, en la que amplió, nuevamente, la sala de oración hacia el sur introduciendo diferentes novedades. Establece un esquema en "T", semejante al de la Gran Mezquita de Kairuán, realzado por la utilización de cúpulas cuyos nervios no se cruzan en el centro, arcos lobulados, distintos tipos de arcos entrecruzados y superpuestos así como por capiteles y columnas realizados ex profeso por lo talleres califales.

En los últimos decenios del siglo X, Almanzor amplió todo el costado oriental de la gran mezquita que pasó a contar con diecinueve naves, aunque sin introducir novedades de interés.

De las empresas artísticas acometidas en época emiral sobresalen las ejecutadas durante el reinado de Abderramán II cuya corte acogió a numerosos artistas, modas y costumbres orientales. Impulsó, entre otras construcciones, las obras de la alcazaba de Mérida y del alminar de la iglesia de San Juan de los Caballeros en Córdoba, mejorando las murallas de Córdoba y Sevilla. No obstante, es durante el califato cuando se acometen los más ambiciosos proyectos artísticos. El califa Abd al-Rahman III siguiendo la tradición oriental, según la cual cada monarca construía como símbolo de prestigio su propia residencia palatina, decide fundar en 936 la ciudad aúlica de Medina al-Zahra. Elige para ello, a pocos kilómetros de Córdoba, una suave pendiente del terreno lo que le permite organizar el recinto amurallado en tres terrazas. En ellas dispuso las residencias palatinas, salones de recepción como el denominado Salón Rico, baños, mezquita congregacional, casa de la moneda, talleres califales, jardines y parque zoológico. Estas obras fueron completadas por al-Hakam II, si bien su esplendor fue efímero acabando con la ciudad las primeras revueltas de 1010 que concluyeron con la caída del califato.

En el resto del territorio peninsular también es patente el florecimiento artístico impulsado el califato. Testimonio de ello es la ciudad de Toledo, en la que aún se vislumbran restos de su fortificación así como algunos de los vestigios que definen su alcazaba, medina, arrabales y entorno, como la Puerta Vieja de Bisagra o Puerta de Alfonso VI.

Entre sus construcciones destaca la pequeña mezquita del Cristo de la Luz o de Bab al- Mardum. Su planta cuadrada, organizada en nueve tramos cupulados, presenta una planta y alzado que conecta con el modelo tunecino de la mezquita aglabí de Bu Fatata.

Aparte del carácter excepcional de Toledo, también ocupan un lugar destacado obras como la rábita de Guardamar del Segura (Alicante), el Castillo de Gormaz (Soria) o la ciudad de Vascos (Toledo).

El refinamiento reinante en la corte califal auspició la creación de manufacturas de lujo que, bajo el patrocinio real, se tradujeron en las más variadas expresiones artísticas. Destacan los trabajos en marfil en los que se realizaron objeto de uso palatino como botes y arquetas destinadas a guardar joyas, ungüentos y perfumes entre los que destacan el Bote de Zamora (Museo Arqueológico Nacional), destinado a la esposa de Alhakén II, y la Arqueta de Leyre, (Museo de Navarra). En su profusa trama de vegetación suelen inscribirse escenas de corte al mismo tiempo que bandas con inscripciones epigráficas indican el destinatario e incluso el maestro que ejecutó la pieza.

Las piezas de este material que conocemos hoy día se conservan gracias al aprecio cristiano que las destinó a relicarios para sus monasterios y catedrales, o para guardar joyas, ungüentos y perfumes. Básicamente responden a la tipología de botes y arquetas, siendo los primeros de forma cilíndrica con tapa semiesférica, y las segundas de forma prismática rectangular, con tapa plana o troncopiramidal. Siempre son piezas de pequeño formato, generalmente oscilan entre 11'5 y 7'5 cm.

Los monarcas, al igual que en Bagdad y El Cairo, organizaron su propia fábrica de tejidos o tiraz cuya fundación marca el comienzo de la historia de la producción de tejido de seda en Al-Ándalus. Sus motivos vegetales y figurados, geometrizados, se inscriben en medallones formando bandas tal como aparecen en el velo o almaizar de Hixam II que, a modo de turbante, le cubría la cabeza colgándole hasta los brazos.

Existieron también talleres que trabajaron el bronce cuyas figuras representan leones y ciervos con el cuerpo cubierto de círculos tangentes que evocan tejidos y que, posiblemente, sirvieron como surtidores de fuentes. Su paralelismo formal y estilístico con piezas fatimíes ha motivado controversias sobre la filiación de alguna de las piezas.

La cerámica cuenta con un tipo de producción conocida como "verde y manganeso". Su decoración a base de motivos epigráficos, geométricos y una fuerte presencia de motivos figurados se consigue mediante la aplicación de óxido de cobre (verde) y óxido de manganeso (IV) (morado).




</doc>
<doc id="15585" url="https://es.wikipedia.org/wiki?curid=15585" title="Arte taifa">
Arte taifa

La destrucción de la unidad política en la España medieval llevó a la abolición del califato cordobés en 1031 y a la creación de un mosaico de reinos independientes que fueron denominados taifas (de "tawaifs", partidos, facciones).

Las rivalidades entre ellos, reivindicando la herencia del prestigio y la autoridad del Califato, constituyeron la tónica dominante del período. Esta situación se tradujo en el terreno artístico en la emulación de modelos cordobeses. Sin embargo, la dispersión de los régulos y la pobreza de recursos materiales en comparación con la época del califato, debido en gran medida a las continuas parias que pagaban a los reinos cristianos, produjo un arte de ostentación y de gran dispersión estilística, sin que se pueda determinar de modo preciso, unas constantes artísticas para el periodo taifal.

Dominó la arquitectura civil (palaciega y militar), frente a la religiosa en este periodo, que solo aporta ejemplos de mezquitas menores, como la Mezquita de las Tornerías de Toledo o la torre de la iglesia de San José de Granada, cuyo cuerpo inferior pertenece al alminar de la mezquita de al-Murabittun (ermitaños). Las taifas desarrollaron estilos propios a partir de la propia evolución manierista del arte hispanomusulmán y de influencias exteriores, pues es esta una época de abundante comercio y contactos con Oriente. Sin embargo, los materiales que se utilizaron fueron pobres, en consonancia con el menor poderío económico de los reyes taifas, que emplearon en la arquitectura fundamentalmente el ladrillo, el mampuesto, las yeserías y técnicas mixtas.

Se da una preferencia por lo ornamental: frente a las estructuras de arcos de herradura y peraltados, sostenidos sobre columnas con capiteles de herencia romana, ahora proliferan los arcos mixtilíneos (la gran novedad del periodo), polilobulados, los calados en las yeserías y la decoración de atauriques; capiteles más estilizados y ornamentales y columnas que ya no utilizan tan a menudo el mármol.

No se han conservado demasiados ejemplos de arquitectura del periodo de las primeras taifas. La mayor parte de los restos arquitectónicos existentes corresponden a la arquitectura militar, destacando las alcazabas de Málaga, Almería, Granada o Badajoz, que sin embargo, recibieron posteriormente aportes y remodelaciones, fundamentalmente de época almohade y nazarí. El único ejemplo homogéneo de arquitectura palaciega está representado por La Aljafería de Zaragoza, un palacio de recreo con aspecto fortificado que supuso la culminación del esplendor de la Taifa de Zaragoza.

Emparentada tipológicamente con el palacio omeya de Msatta (Jordania), adopta una organización tripartita donde cada uno de los sectores estaba dedicado a funciones diferenciadas. El sector central, de uso protocolario, está dominado por un patio rectangular cuyos lados menores iban ocupados por albercas, pórticos y estancias alargadas acotadas en los extremos por alcobas. Este esquema deriva, sin duda, de los modelos palatinos cordobeses.

A esta misma tradición responde el repertorio de arcos desplegado en el edificio, encontrando desde arcos lobulados, mixtilíneos, de herradura semicircular y apuntada a complejas organizaciones de arcos entrecruzados, superpuestos y contrapuestos. Todos ellos realizados con materiales de escaso costo pero revestidos de yeserías con motivos vegetales, geométricos y epigráficos buscando un efecto de fastuosidad y aparente riqueza.

Tras la reconquista de Zaragoza en 1118 por Alfonso I el Batallador pasó a ser residencia de los reyes cristianos de Aragón, con lo que la Aljafería se convirtió en el principal foco difusor del mudéjar aragonés. Fue utilizada como residencia regia por Pedro IV el Ceremonioso y posteriormente, en la planta principal, se llevó a cabo la reforma que convirtió estas estancias en palacio de los Reyes Católicos en 1492.

En 1593 experimentó otra reforma que la convertiría en fortaleza militar, primero según diseños renacentistas (que hoy se pueden observar en su entorno, foso y jardines) y más tarde como acuartelamiento de regimientos militares. Sufrió reformas continuas, y grandes desperfectos, sobre todo con los Sitios de Zaragoza de la Guerra de la Independencia hasta que finalmente fue restaurada en la segunda mitad del siglo XX y actualmente acoge las Cortes de Aragón.

Las viejas alcazabas de los distintos reinos también asistieron a importantes remodelaciones.

Según el arquitecto restaurador, Leopoldo Torres Balbás, la Alcazaba de Málaga es el prototipo de la arquitectura militar del periodo taifa, con su doble recinto amurallado y gran cantidad de fortificaciones, siendo su único paralelo el castillo del Crac de los Caballeros, fortaleza levantada en Siria por los cruzados entre los siglos XII y XIII. Cuenta así mismo con un recinto palaciego de la taifa mālaquial que corresponden los palacios nazaríes y el palacio Taifa. Fue construida entre los siglos VIII y XI sirviendo algunos de sus patios como el de la alberca de inspiración para la realización (a mayor escala) de la Alhambra.

Durante los periodos de los reinos de taifas la Alcazaba se convirtió en la sede del poder de la taifa de Málaga donde residían los califas de la dinastía Hammudi que llegó a controlar las coras de Málaga y de Algeciras así como una región al norte de África que incluía las ciudades de Tánger y Ceuta. En la Alcazaba se encuentran ejemplos de arcos de herradura, de medio punto y polilobulados; así como importantes ejemplos de yesería y cerámica decorativa.

La Alcazaba de Almería fue fortificada con muros de tapial, construyéndose en su interior un palacio, "al-Sumadihiyya", rodeado de jardines. En los casos de Toledo, Sevilla y Badajoz, reinos que pujaron más fuertemente por la herencia cordobesa, se conservan deslumbrantes testimonios de las crónicas árabes sobre sus palacios así como escasos fragmentos generalmente descontextualidados.

La vieja alcazaba de Granada, conocida como "qadima" (antigua), situada en la colina del Albaicín, se fortificó con torres cuadradas y redondas añadiéndole algunas puertas en recodo como la puerta Monaita y la puerta Nueva.

Granada conserva unos baños conocidos como El Bañuelo, en la carrera del Darro, organizado en tres estancias de las cuales la central o templada adquiere, por razones de uso, unas mayores dimensiones. Baños muy similares conservan Toledo, Baza y Palma de Mallorca.

Al igual que la arquitectura, las artes suntuarias siguieron la tradición cordobesa aunque el protagonismo fue adquirido por otros centros. Así la producción de marfil se trasladó al taller de Cuenca mientras que el prestigio en los textiles fue adquirido por el taller de Almería.

Por lo que respecta a la cerámica, se consolidó una técnica que había aparecido durante el califato pero que en estos momentos adquirió un gran desarrollo. Se trata de la cerámica de «cuerda seca» cuyas piezas se decoran con líneas de óxido de manganeso formando diferentes motivos que se rellenan con vidrio de diferentes colores.



</doc>
<doc id="15587" url="https://es.wikipedia.org/wiki?curid=15587" title="Arte almohade">
Arte almohade

La laxitud moral y la degradación de costumbres de los almorávides dio lugar a un nuevo movimiento rigorista, los almohades, "al-muwahhidun" "los unitarios". Estaba encabezado por Ibn Túmart quien procedía de un medio tribal del Alto Atlas. Su continuador, Abd al-Mumin, se nombró califa, tomó Marrakech en 1147 y emprendió la conquista del resto de imperio incluyendo Túnez. Al-Ándalus fue incorporada definitivamente por su sucesor Abu Yaqub Yúsuf quien eligió, en 1172, Sevilla como capital del nuevo imperio.

El retorno a la austeridad más extrema se trocó, aún más rápidamente que en el caso de sus predecesores, en uno de los momentos artísticos de mayor brillantez, particularmente en el terreno de la arquitectura. De forma que, el arte almohade (1130-1269) va a continuar la estela almorávide consolidando y profundizando sus tipologías y motivos ornamentales. Va a construir con los mismos materiales: ladrillo, yeso, argamasa y madera. Y, va a mantener como soporte el pilar y los arcos empleados en el período anterior.

Sus mezquitas, excepto la inacabada de Rabat, van a seguir el modelo de la mezquita de Tremecén, con naves perpendiculares al muro de la qibla. En ellas, se potencia un esquema en "T" mediante cúpulas que son de muqarnas en la mezquita de Tinmal y en la Kutubiyya de Marrakech. Asimismo, la Kutubiyya, la de Hasan y la de Sevilla cuentan con alminares muy semejantes entre sí. Se caracterizan por su planta cuadrada y su alzado compuesto por dos torres, una de las cuales alberga a la otra y entre las que discurre una escalera o una rampa en el caso de la Giralda de Sevilla. La torre interior está formada por estancias abovedadas superpuestas que tendrán repercusión posterior en las torres campanarios mudéjares, sobre todo deAragón.

Desarrolla los patios cruceros que ya habían hecho su aparición en Medina al-Zahra aunque es, en estos momentos, cuando adquieren un gran protagonismo. Sus mejores testimonios se hallan en el Alcázar de Sevilla donde se han conservado el patio de la casa de Contratación y otro, actualmente subterráneo, conocido como el Jardín Crucero o los Baños de doña María de Padilla. Estos posiblemente fueron trazados por alarifes que realizaron el patio crucero del Castillejo de Monteagudo, mandado construir por el gobernante bereber del reino independiente de Murcia. Este esquema será retomado en los patios nazaríes y mudéjares. Igual repercusión tendrá otra novedad que aparece en el Patio del Yeso del Alcázar sevillano. Consiste en la colocación de unas pequeñas aberturas o ventanas cubiertas con celosías de estuco sobre el vano de acceso a una estancia para permitir su iluminación y ventilación.

Experimenta un enriquecimiento tipológico y un perfeccionamiento de su eficacia defensiva de gran trascendencia, incluso, para el ámbito cristiano. Aparecen complejas puertas en recodo para que los atacantes al avanzar dejen uno de sus flancos al descubierto; torres poligonales para desviar el ángulo de tiro; torres albarranas separadas del recinto murado pero unido a él en la parte superior mediante un arco superior y cuya proyección hace que aumente su eficacia defensiva respecto a una torre normal; muros corachas que discurren perpendiculares al recinto murado al objeto de proteger una toma de agua, una puerta y evitar el cerco completo; así como barbacanas o antemuros. Entre las fortificaciones destacan las alcazabas de Cáceres, Badajoz, donde se encuentra la torre albarrana de Espantaperros, y Sevilla; a esta última pertenece la famosa torre albarrana poligonal conocida como la Torre del Oro. Los almohades se encuentran, asimismo, entre los primeros en utilizar bóvedas nervadas para cubrir estancias militares, ya que hasta entonces este tipo de cubrición se había utilizado casi exclusivamente en edificios religiosos (como la mezquita de Córdoba y la de Bab al-Mardum en Toledo). Las mejor conservadas en la arquitectura militar están en el castillo de Villena, la muralla almohade de Palma del Río (Córdoba) y el de Biar.

En el terreno decorativo aplicaron un repertorio caracterizado por la sobriedad, el orden y el racionalismo. Ello se tradujo en la aparición de motivos amplios que dejan espacios libres en los que triunfan el entrelazo geométrico, las formas vegetales lisas y el rasgo ornamental más novedoso, la "sebqa". Esta composición que decora la Giralda, consiste en una doble trama romboidal en dos planos compuesta por arcos decorativos superpuestos a partir de la clave de los inferiores. Otra decoración arquitectónica que aparece en este mismo alminar y en la Kutubiyya es la cerámica, en la que se aplica la técnica del alicatado; es decir, piezas recortadas que, combinadas entre sí, componen un motivo decorativo. 

En otras ocasiones, estas manifestaciones aúnan el carácter ornamental como el funcional. Es el caso de la madera con la que se realizaron techumbres de par y nudillo con tirantes cuyo ejemplar más antiguo cubre la nave axial de la Kutubiyya de Marrakech. Estas armaduras estaban llamadas a adquirir un gran protagonismo en el arte mudéjar.

Las producciones artísticas de este período están peor representadas a causa de su confusión con las de otros períodos artísticos. Así sucede con los tejidos que se distinguen con dificultad de los mudéjares. Acusan una práctica ausencia de motivos figurados así como un aumento de decoración geométrica y epigráfica a base de la repetición insistente de palabras árabes como "bendición" y "felicidad". En la metalistería destacan aguamaniles que representan figuras animales decoradas con incisiones vegetales cinceladas como el león de Monzón de Campos que hasta fecha reciente era considerado una pieza califal.



</doc>
<doc id="15588" url="https://es.wikipedia.org/wiki?curid=15588" title="Arte nazarí">
Arte nazarí

El arte nazarí, también llamado arte granadino, constituye la última etapa del arte hispanomusulmán. Se desarrolla durante los siglos XIII, XIV y XV, extendiéndose además de por el Reino Nazarí de Granada, por Berbería y los dominios cristianos de la península ibérica, contribuyendo al surgimiento del arte mudéjar. 

La obra por antonomasia que define a la dinastía nazarí (1237-1492) es la Alhambra, "Qalat al-Amra", "el castillo rojo", verdadera síntesis de arquitectura palatina islámica y de los nuevos elementos de fortificación incorporados a la arquitectura militar. A ella se asocia una almunia o huerta de recreo conocida como el Generalife o "Yannat al-Arif" o "huerta del Arquitecto".

Al debilitarse el imperio almohade surgen en al-Ándalus nuevos pequeños reinos que se hacen con el poder entre los siglos XIII y XV. La batalla de las Navas de Tolosa, en 1212, abre el camino hacia el sur a los conquistadores cristianos. En contraposición, a partir de 1232, los musulmanes de Arjona (Jaén) proclaman sultán a Muhamad ibn Yusuf ibn Nasr. Se inicia así un proceso de reconstrucción territorial mediante el cual se forma en la Andalucía penibética un nuevo reino, el nazarí, cuya capital desde 1237 será Granada. La constante presión cristiana redujo paulatinamente el reino, terminando con la capitulación de Granada el 2 de enero de 1492. De este modo, desaparecía el último bastión islámico de al-Andalus. Surgiendo así el término de 'Arte Nazarí'

La Alhambra es iniciada por el fundador de la dinastía, Muhammad I, que abandona la alcazaba taifa emplazada en el Albaicín y elige esta colina para situar su residencia. El lugar contaba con los restos de una pequeña fortificación del siglo XI que transforma en su propia alcazaba. Para ello, la dota de un doble recinto: uno exterior a modo de barbacaba o antemuro y otro interior reforzado por altas torres; e instala en su interior un barrio castrense con casas, baño y aljibe.

A partir de la alcazaba se desarrolla el recinto amurallado de la ciudad jalonado de torres defensivas, si bien algunas de ellas introducen la novedad de convertirse en viviendas palatinas. A pesar del carácter de estas torres el verdadero núcleo palatino lo constituye la denominada Casa Real Vieja de la Alhambra de la que forman parte los palacios de Comares y de los Leones. Aparte de estos conjuntos residenciales y protocolarios la ciudad también fue dotada con mezquita congregacional, baños, ceca, barrio de servidores, cementerio real, y talleres.

Entre los principales núcleos palatinos hay que destacar en primer lugar el mexuar (maswar) o sala donde se reúne el consejo de ministros o visires. Su construcción se debe a Ismail I, siendo reformado por Muhammad V. A pesar de haber sufrido grandes transformaciones, su sala rectangular cuenta en la parte central con cuatro columnas que soportaban una linterna. En torno a este espacio central cuadrado se disponen otras tantas estancias rectangulares.

Al norte del mexuar se levanta el patio del Cuarto Dorado. Uno de sus lados está cerrado por una gran fachada llamada de Comares por ser el acceso monumental a dicho conjunto palatino. El palacio de Comares, construido por Yusuf I y reestructurado por Muhammad V, debe su nombre al término árabe qamriyya o qamariyya que en Oriente se utiliza para designar las vidrieras de colores. La gran fachada de Comares situada en el patio del Cuarto Dorado, siguiendo la tradición de los monarcas orientales, servía de marco al monarca cuando sentado ante ella concedía audiencias públicas a sus súbditos. Pero, al mismo tiempo, la fachada revela claramente su doble destino, puesto que una de sus puertas sirve de acceso a la zona residencial mientras que la otra introducía en el patio del palacio. Este gran patio rectangular, denominado de los Arrayanes o de la Alberca, está articulado en torno a un eje longitudinal cuya parte central va ocupada por una alberca. Los dos lados mayores del rectángulo estaban ocupados por cuatro viviendas privadas -dos a cada lado- para las cuatro esposas legítimas del sultán. Los lados menores, porticados, acogían: en el meridional la vivienda del príncipe heredero mientras que en el septentrional se hallaba la del sultán. Esta última es una sala rectangular acotada con alcobas a la que se conoce como la sala de la Barca (baraka, bendición). Un pequeño pasillo paralelo a la sala conduce en su extremo derecho a un pequeño oratorio mientras que en el extremo izquierdo una escalera ascendía a otra cámara situada sobre el salón del trono. Finalmente, y alojada en la potente torre de Comares se hallaba el salón del Trono o de Embajadores cuya techumbre de madera ha sido interpretada como los siete cielos del Paraíso coránico. En esta sala el monarca celebraba sus recepciones y actos solemnes.

De forma transversal a este conjunto se encuentra el palacio de los Leones, construido por Muhammad V. De forma tradicional, ha sido considerado la residencia privada del monarca; si bien recientemente se le ha atribuido una función similar al núcleo anterior. De manera que, la sala de las Dos Hermanas actuaría como mexuar mientras que el mirador de Lindaraja sería el salón del Trono. Con independencia de cualquiera de estas posibles funciones, el conjunto se articula en torno a un patio crucero en cuya intersección se encuentra la fuente con doce leones de mármol procedentes de una construcción del siglo XI. Avanzan en los lados menores del patio sendos pabellones mientras que sus cuatro lados están porticados. En el eje de cada uno de ellos se abren las correspondientes estancias conocidas con los nombres de: Sala de los Mocárabes, de los Abencerrajes, de los Reyes.

Así pues, el conjunto de la Alhambra y algunos otros edificios como el Cuarto Real de Santo Domingo y Alcázar Genil, ambos en Granada, los vestigios de la ciudad de Ronda y los numerosos castillos permiten establecer el marco general del arte nazarí que no es otro sino la síntesis del arte hispanomusulmán.

La arquitectura militar desarrolla los sistemas generados en época anterior dotándolos de una mayor complejidad. La arquitectura palatina emplea dos tipos de organización de patios cuyos precedentes se rastrean desde Medinat al-Zahra. Se trata del patio monoaxial -patio de los Arrayanes o de la Alberca- y del patio crucero -patio de los Leones-. Las estancias vinculadas a ellos responden nuevamente a dos tipologías: una alargada con extremos acotados por alcobas, y otra cuadrada rodeada por habitaciones. Sirvan de ejemplo la Sala de la Barca y la Sala de las Dos hermanas.

Los escasos vestigios de arquitectura religiosa permiten pensar en mezquitas que siguen el modelo almohade con naves perpendiculares al muro de la quibla. La única novedad proviene del hecho de utilizar columnas de mármol cuando el edificio tiene cierta relevancia. En cuanto al repertorio ornamental utilizan una profusión decorativa que enmascara la pobreza de los materiales, empleando desde zócalos de alicatado y yeserías de estuco a decoración pintada, como la conservada en la bóveda de la Sala de los Reyes. Es característica la columna de fuste cilíndrico

Por lo que respecta a la arquitectura civil cuenta en la ciudad de Granada con el testimonio de dos edificios: el funduq y el maristan. El funduq, denominado en la actualidad Corral del Carbón, era una especie de albergue o posada destinada al alojamiento de comerciantes foráneos y de sus mercancías, que a veces, estaban especializados en un producto determinado. El maristán u hospital, asociado con posterioridad a manicomio, fue edificado por Muhammad V y demolido en 1843. Ambas fundaciones tenían una estructura cuadrangular de dos pisos en torno a un patio con alberca.
En cuanto al repertorio ornamental utilizan una profusión decorativa que enmascara la pobreza de los materiales empleando: desde zócalos de alicatado y yeserías de estuco, a decoración pintada; como la conservada en la bóveda de la Sala de los Reyes. Es característica la columna de fuste cilíndrico y el capitel de dos cuerpos: uno cilíndrico decorado con cintas y otro cúbico con ataurique. Los arcos preferidos son de medio punto peraltado y angrelados. Las cubiertas de madera alternan con bóvedas de mocárabes realizadas en estuco como las de la Sala de las Dos Hermanas o la de los Abencerrajes. Asimismo, a los motivos ornamentales habituales -geométricos, vegetales y epigráficos- se une el escudo nazarí que será generalizado por Muhammad V.

Similar al esplendor arquitectónico es el adquirido por las artes suntuarias, destacando las cerámicas de reflejo metálico y los tejidos de seda a las que pueden añadirse los bronces, las taraceas y las armas. La cerámica de lujo, conocida como de reflejo metálico o loza dorada, se caracteriza por someter su última cocción a fuego reducido -de oxígeno- y menor temperatura. Con este procedimiento, la mezcla de sulfuro de plata y cobre empleada en la decoración llega a la oxidación produciendo el brillo metalizado. También fue frecuente añadir óxido de cobalto dando lugar a una serie en azul y dorado. Con esta técnica se realizaron los famosos "vasos" o jarrones de la Alhambra.
Los tejidos nazaríes, por su parte, constituyen la última etapa de esplendor de los tejidos de seda andalusíes que serán sustituidos por terciopelos labrados. Se caracterizan por sus intensos colores y la utilización de motivos idénticos a los empleados en la decoración arquitectónica.

Desde al menos mediados del siglo XX se ha tenido constancia de la existencia de pinturas ocultas en el Salón de Embajadores del Palacio de Comares de la Alhambra, en partes solo accesibles para los artesanos nazaríes que participaron en la construcción. En posteriores tareas de restauración del Mirador de Lindaraja, a principios del siglo XXI, fue documentada la existencia de 80 dibujos policromados ocultos realizados por los artesanos que trabajaron en la decoración del palacio. También posteriores trabajos mostraron la existencia de más dibujos en el templete oeste del Patio de los Leones, incluyendo, ambos grupos de dibujos, representaciones humanas ¡algo prohibido en el Islam!.

A partir del siglo XIX, con los revivals, renacimientos o movimientos neo, surge el neoárabe, que aglutina todas los artes musulmanes, incluidos el nazarí, dónde se copian yeserías, azulejos, mocárabes y especialmente, sus columnas, como vemos en el Palacio La Alhambra, el Casino de Murcia, el Kiosco morisco, la Gran Sinagoga de Budapest, la Nueva Sinagoga de Berlín, o la Casa de los Cristales. 


</doc>
<doc id="15589" url="https://es.wikipedia.org/wiki?curid=15589" title="Jaime I de Aragón">
Jaime I de Aragón

Jaime I de Aragón "el Conquistador" (aragonés: "Chaime lo Conqueridor", occitano: "Jacme lo Conquistaire", catalán/valenciano: "Jaume el Conqueridor)" (Montpellier, 2 de febrero de 1208-Alcira, 27 de julio de 1276) fue rey de Aragón (1213-1276), de Valencia (1238-1276) y de Mallorca (1229-1276), conde de Barcelona (1213-1276), conde de Urgel, señor de Montpellier (1219-1276) y de otros feudos en Occitania.

Hijo de Pedro II el Católico y de María de Montpellier, era el heredero de dos importantes linajes: la Casa de Aragón y el de los emperadores de Bizancio, por parte de su madre. Tuvo una infancia difícil. Su padre, que acabaría repudiando a la reina, solo llegó a concebirlo mediante engaño de algunos nobles y eclesiásticos que temían por la falta de un sucesor, y la colaboración de María, haciendo creer a Pedro que se acostaba con una de sus amantes. Estas circunstancias produjeron el rechazo de Pedro II hacia el pequeño Jaime, a quien no conoció sino a los dos años de su nacimiento. A esa edad, el rey hizo un pacto matrimonial para entregar a su hijo Jaime a la tutela de Simón, Señor de Montfort, para casarlo con la hija de este, Amicia, para lo cual el niño iba a ser recluido en el castillo de Carcasona hasta los 18 años.

A la muerte de su padre, durante la cruzada albigense, en la batalla de Muret (1213), Simón de Montfort se resistió a entregar a Jaime a los aragoneses hasta después de un año de reclamaciones y solo por mandato del papa Inocencio III. Durante su minoría de edad, estuvo bajo la tutela de los caballeros templarios en el castillo de Monzón, habiendo sido encomendado a Guillermo de Montredón, junto con su primo de la misma edad, el Conde de Provenza Ramón Berenguer V. Mientras, actuaba como regente del reino el conde Sancho Raimúndez, hijo de Petronila de Aragón y Ramón Berenguer IV y tío abuelo de Jaime. Heredó el señorío de Montpellier a la muerte de su madre (1213).

Huérfano de padre y madre, tenía unos 6 años cuando fue jurado en las Cortes de Lérida de 1214. En septiembre de 1218 se celebraron por primera vez en Lérida unas Cortes generales, en las cuales fue declarado mayor de edad.

En febrero de 1221 contrajo matrimonio en la población soriana de Ágreda, población fronteriza entre Castilla y Aragón, con Leonor de Castilla, hermana de la reina Berenguela de Castilla y tía de Fernando III. Tras la boda la pareja se trasladó a la catedral de Tarazona, donde Jaime fue ordenado caballero. Anulado su primer casamiento por razón de parentesco en 1229, contrajo segundo matrimonio con la princesa Violante (8 de septiembre de 1235), hija de Andrés II, rey de Hungría. Por el testamento de su tío segundo Nuño Sánchez, heredó los condados de Rosellón y Cerdaña y el vizcondado de Fenolleda en Francia (1241).

Durante los quince primeros años de su reinado, mantuvo diversas luchas contra la nobleza aragonesa, que incluso llegó a hacerle prisionero en 1224. En 1227 afrontó un nuevo alzamiento nobiliario aragonés, dirigido por el infante Fernando, tío del rey, que terminó, gracias a la intervención papal a través del arzobispo de Tortosa, con la firma de la Concordia de Alcalá (marzo de 1227). Este tratado marcó el triunfo de la monarquía sobre los levantiscos nobles, dándole la estabilidad necesaria para iniciar las campañas contra los musulmanes. Esta estabilidad logró el apaciguamiento de las reclamaciones de la nobleza y obispos.

Ante los ataques de los piratas mallorquines, los mercaderes de Barcelona, Tarragona y Tortosa pidieron ayuda al monarca para acabar con la amenaza. Así, en una reunión de Barcelona (diciembre de 1228) le ofrecieron sus naves, mientras que los nobles catalanes acordaron participar en la empresa a cambio del botín y dominios territoriales. En otra reunión en Lérida, los nobles aragoneses aceptaron las mismas condiciones, pero sugirieron al rey que la empresa se dirigiera contra los musulmanes de Valencia, por lo que su participación no sería significativa.

El 5 de septiembre de 1229, la escuadra aragonesa, compuesta por 155 naves, 1500 caballeros y 15 000 soldados, zarpó de Tarragona, Salou y Cambrils, para conquistar Mallorca a Abú Yahya, el gobernador almohade semiindependiente de la isla.

Las tropas aragonesas desembarcaron en Santa Ponsa y vencieron a los musulmanes en la batalla de Portopí (13 de septiembre de 1229). Los musulmanes se refugiaron tras las murallas de "Madina Mayurqa" (la actual Palma de Mallorca) y crucificaron a varios soldados aragoneses a la vista de las tropas de Jaime. Estas poco después tomaron y pasaron a cuchillo a la población de la ciudad (diciembre de 1229) y se apoderaron de la isla en pocos meses, salvo un pequeño núcleo de resistencia musulmana que logró mantenerse en la sierra de Tramontana hasta 1232. Los pobladores musulmanes huyeron a África o fueron esclavizados.

Después de pasar a cuchillo la población de "Madina Mayurqa", la cantidad de cadáveres fue tal que se produjo una epidemia que diezmó el ejército de Jaime I. Por añadidura, los nobles catalanes intentaron quedarse con el botín, provocando una revuelta que debilitaría aún más el poder militar de Jaime I.

Mallorca se constituyó como un reino más de la Corona de Aragón bajo el nombre de "Regnum Maioricarum et insulae adiacentes", el cual obtuvo una carta de franqueza en 1230. La institución en 1249 del municipio de Mallorca (actual Palma) contribuiría a la institucionalización del reino.

El monarca aragonés se vio incapacitado para conquistar Menorca a causa de las divisiones internas dentro de su ejército por el botín y la reducción de su ejército debido a unas malas decisiones; aun así, el monarca consiguió por mediación de dos nobles aragoneses (Pedro Maza y Assalido de Gudal), un noble catalán (Bernaldo de Santa Eugenia) y el comendador del Temple de Mallorca (Ramón de Serra) un vasallaje sobre Menorca, rubricado por el Tratado de Capdepera, por el cual los musulmanes menorquines aceptaron su soberanía (1231). El vasallaje sobre Menorca sería transferido al reino de Mallorca como parte del testamento de Jaime I. Alfonso III de Aragón conquistaría de forma efectiva esta isla, después de la capitulación de Abû ‘Umar en 1287. Fue repoblada, aunque quedó una abundante población musulmana, que más tarde fue desterrada.

Por último, cedió la sumisión de Ibiza y Formentera a Guillermo de Montgrí, arzobispo de Tarragona, y su hermano Bernardo de Santa Eugenia, que la hizo efectiva en 1235. La isla se repobló con campesinos de Ampurias (1236).

La conquista de Valencia, a diferencia de la de Mallorca, fue hecha con un importante contingente de aragoneses.
Así, para empezar la conquista, en 1231 Jaime I se reunió con el noble Blasco de Alagón y Hugo de Folcalquier, maestre de la Orden Militar del Hospital, en Alcañiz para fijar un plan de conquista de las tierras valencianas. Blasco de Alagón recomendó asediar las poblaciones en terreno llano y evitar las fortificadas. Sin embargo, lo primero que se tomó fueron dos enclaves montañosos: Morella, aprovechando Blasco la debilidad de su gobierno musulmán; y Ares, lugar cercano a Morella tomado por Jaime I para obligar a Blasco de Alagón a que le entregara Morella.

La conquista de lo que posteriormente se convertiría en el reino de Valencia comienza en 1232, con la toma de Morella. En 1233 se planea la campaña en Alcañiz, que constaría de tres etapas:

En esta última etapa y en los años siguientes, Jaime I tuvo que hacer frente a diversas revueltas de la población mudéjar, encabezadas por el caudillo al-Azraq.

Jaime I obtuvo un gran triunfo sobre la nobleza aragonesa al convertir las tierras conquistadas en Valencia en un reino diferenciado, unido a la Corona de Aragón (1239), gracias a la elaboración legislativa de los Fueros de Valencia, "els Furs". La creación del reino provocó una iracunda reacción de la nobleza aragonesa, que veía así imposibilitada la prolongación de sus señoríos en tierras valencianas.

Mediante el Tratado de Corbeil (1258) Jaime renunció a sus derechos sobre territorios del mediodía francés. En contrapartida, san Luis de Francia renunciaba a sus derechos, como descendiente de Carlomagno, sobre los condados catalanes, herederos de la Marca Hispánica.

Jaime I estuvo presente en el Segundo Concilio Lugdunense, que se celebró en la catedral de Lyon, entre el 7 de mayo y el 17 de julio de 1274. El concilio deliberó sobre la preparación de una nueva cruzada centrándose en los aspectos financieros de la misma, para lo cual se decidió que durante seis años un diezmo de todos los beneficios de la cristiandad deberían destinarse a la cruzada. Jaime I se mostró partidario de iniciarla inmediatamente pero al oponerse los templarios no se tomó ninguna decisión. Ante las indecisiones de los demás asistentes a la asamblea canónica, Jaime I se despidió del Santo Padre (el Papa Gregorio X) y abandonó la reunión con los miembros de su séquito.

Castilla había sometido Murcia a vasallaje (1243), pero los murcianos se rebelaron contra Castilla con el apoyo del Reino nazarí de Granada y los gobernantes del Norte de África (1264). La reina Violante (esposa de Alfonso X el Sabio) pidió ayuda a su padre Jaime I. Entonces, tropas de la Corona de Aragón mandadas por el infante Pedro (el futuro Pedro III el Grande) conquistaron a Muhammad ibn Hûd Biha al-Dawla el reino de Murcia (1265-66), dejando después a más de 10 000 aragoneses en Murcia. En efecto, hay que recordar que según las condiciones del Tratado de Almizra (1244), Murcia pertenecería a Castilla.

En septiembre de 1269 salió de Barcelona con su armada para una expedición a Tierra Santa, pero dispersadas sus naves por las tormentas, tuvo que desembarcar en Aigües-Mortes, cerca de Montpellier, y hubo de renunciar a aquella empresa.

Tras un reinado de sesenta y tres años (el reinado más largo de cualquier monarca en toda la historia de España), murió en Alcira (Valencia) el 27 de julio de 1276. En el trance de su muerte, en la residencia real de esta ciudad, y como había dispuesto, Don Jaime fue amortajado con los hábitos del císter.

Los restos mortales del rey permanecieron depositados en Santa María de Valencia hasta mayo de 1278, en que fueron trasladados al monasterio de Poblet para su sepultura definitiva. No obstante, tras la desamortización de Mendizábal, el monasterio quedó abandonado y el cadáver de Jaime I fue trasladado en 1843 a Tarragona, donde le fue construido un panteón en la parte posterior de la catedral, que fue inaugurado en 1856. En 1952, los restos de Jaime I fueron restituidos a Poblet.

Dictó su biografía, el "Llibre dels feits", que se convirtió en la primera de las cuatro grandes crónicas reales en catalán/valenciano.

De su primera mujer, Leonor de Castilla, tuvo a:


De su segunda esposa, Violante de Hungría, tuvo a:

Tradicionalmente se ha considerado que fue el deseo de Violante de conseguir buenas herencias para sus hijos el motivo por el que Jaime I procedió a la partición de sus reinos. Sin embargo, parece tener más sentido la concepción patrimonial de los reinos que tenía el rey. Así, hizo un primer reparto en su testamento de 1241. Según este testamento, el primogénito Alfonso heredaría Aragón y los condados catalanes, y Pedro, hijo de Violante, Valencia, las islas Baleares, el Rosellón, la Cerdaña y las posesiones occitanas. Dos años después, un nuevo testamento introduce a su tercer hijo en el reparto. Los condados catalanes pasan de Alfonso a Pedro, el cual cede las islas Baleares, Rosellón, Cerdaña y las posesiones occitanas a Jaime. Nuevo testamento en 1248, incluyendo en el reparto al nuevo hijo, Fernando. A la muerte de Alfonso (1260), otorgó nuevo testamento (1262), el cual daría la configuración definitiva de la herencia.

Tras la muerte de Violante (1253) el rey se lanzó a una carrera de amoríos, teniendo múltiples hijos.
De Teresa Gil de Vidaure tuvo a Jaime, señor de Jérica, y a Pedro, señor de Ayerbe. Con Elvira Sarroca tuvo Jaime Sarroca, obispo de Huesca y Pedro del Rey, obispo de Lérida. De sus relaciones amorosas con Blanca de Antillón nació Fernán Sánchez, a quien dio la baronía de Castro; Con Berenguela Fernández tuvo a Pedro Fernández, señor de la baronía de Híjar, mientras que con Berenguela Alfonso, hija del infante Alfonso de Molina, no tuvo descendencia. Estos bastardos reales fueron el origen de algunas de las más importantes casas nobiliarias de Aragón y Valencia.

El reinado de Jaime I marcó el nacimiento de una conciencia territorial en los distintos reinos de la Corona de Aragón, especialmente en Aragón, Reino de Valencia y en los condados catalanes. Dos son los factores que contribuyeron a este hecho: la normalización del Derecho y la transformación de las Cortes en un órgano reivindicativo y representativo de la voluntad del reino, actúan como catalizadores de la creación de una conciencia diferenciadora de cada territorio. Los "Fueros de Aragón" se promulgaron en las cortes de Huesca (1247), sustituyendo a los diferentes códigos locales del reino. Los "Usatges de Barcelona", gracias a la protección real, se extendieron por todos los condados catalanes (mediados del siglo XIII). La situación en Valencia fue diferente, puesto que la oposición de la nobleza aragonesa a la consolidación del reino hizo que los fueros valencianos ("Foris et consuetudines Valentiae"), otorgados por Jaime I en 1240 no triunfaran definitivamente hasta 1329. En 1244, Jaime I establece que el río Cinca sería la divisoria entre Aragón y los condados catalanes. Desde entonces, las Cortes de cada territorio se reunieron de forma separada.

El reinado de Jaime I marcó también el desplazamiento del centro de gravedad de la monarquía hacia la costa mediterránea. Así, la Corte y la cancillería —base del actual Archivo de la Corona de Aragón— se establecieron en Barcelona.

Como elementos positivos de su reinado pueden señalarse:

La infancia de Jaime I transcurrió en el castillo de Monzón, junto al río Cinca, y es donde aprendió el habla viva al cuidado del Maestre de la Orden del Temple. En su autobiografía se aprecia una constante familiaridad con los cuatro brazos del poder de Aragón y la amistad con que trata a "Pere de Muncada" que le recluta caballeros en el territorio fronterizo entre Aragón y los condados catalanes de las poblaciones de Almenar y Tamarite. La doble forma en que escribe el topónimo Monzón es un vestigio de que tanto dominaba el habla viva de la zona en la forma "Monço" como la forma catalana escrita con la grafía "Muntsó" en el manuscrito que se conserva en la Biblioteca Nacional de Madrid.

Como elementos negativos, es preciso advertir que el juicio histórico sobre Jaime I depende del reino en el que se centra el historiador. Para los historiadores aragoneses las conclusiones suelen ser negativas, aduciendo el carácter patrimonial que dio a sus reinos, sin importarle repartir sus dominios entre sus hijos. También es criticada la fijación de la frontera catalano-aragonesa en el Cinca, lo que supuso la adjudicación final de Lérida a los condados catalanes y la separación definitiva de Aragón y los condados catalanes en dos entidades con derecho y Cortes diferentes, tras llevar cien años unidos. La expansión territorial también es enjuiciada negativamente, puesto que con la conquista y creación de los reinos de Mallorca y Valencia, la Corona se convirtió definitivamente en una entidad de carácter confederal, con la monarquía como única institución común y sin ninguna aspiración común entre los diversos reinos.

Del otro lado, para mallorquines y valencianos, la valoración es completamente opuesta: Jaime I es un gran rey, el padre fundador de los reinos, el creador de sus señas de identidad hasta nuestros días: territorio, lengua, fueros, moneda, instituciones, etc.





</doc>
<doc id="15590" url="https://es.wikipedia.org/wiki?curid=15590" title="Arte del Reino Unido">
Arte del Reino Unido

Arte del Reino Unido es una expresión con la que se hace referencia a la producción artística en el Reino Unido. La utilización de las expresiones arte inglés o arte británico es mucho más frecuente, aunque presente la antigüedad de la definición geográfica a la que pueda referirse estrictamente (Inglaterra y Gran Bretaña). Su ubicación cronológica también tiene problemas de ambigüedad, puesto que designa de forma variable (según la intención del que hace la designación) al conjunto del reino de Inglaterra, el reino de Escocia y el principado de Gales, a los que se suman, según la coyuntura histórica, la totalidad o parte de Irlanda (desde 1922 únicamente Irlanda del Norte) y las distintas zonas que han formado parte del Imperio Británico.



</doc>
<doc id="15594" url="https://es.wikipedia.org/wiki?curid=15594" title="Pintura de Rusia">
Pintura de Rusia

La pintura de Rusia tiene una historia que se puede dividir en cinco fases esenciales.

Dado que no han llegado a nuestros días ejemplos de tradición pictórica entre los pueblos eslavos precristianos, la historia de la pintura rusa comienza con la cristianización del Jaganato de Rus, ocurrida en torno al 860, cuando el intercambio cultural con el Imperio bizantino llevó allí la tradición de la pintura de iconos. Hasta el siglo XVIII el género predominante fue la pintura religiosa. La occidentalización del país por Pedro el Grande, creó, en menos de medio siglo, una escuela de pintura enteramente nueva, de carácter profano, relacionada con los finales del Barroco que se desarrollaba en el resto de Europa. La pintura rusa se integró en la evolución general del arte europeo, asimilando nuevas tendencias. A mediados del siglo XIX surgió una nueva escuela nacional. La pintura rusa contribuyó de manera trascendente al arte de Occidente con ocasión de las vanguardias de principios del siglo XX, cuando pintores como Kandinski o Malévich fueron los precursores de la pintura abstracta. Cerca de una década después de la Revolución de 1917, las vanguardias quedaron proscritas y los pintores fueron obligados por el Estado a seguir una estética figurativa populista, originando el estilo conocido como Realismo socialista, que sólo perdió fuerza cuando comenzó a liberalizarse el régimen político a finales del siglo XX. Un grupo de artistas "underground" comenzaron a contestar las fórmulas de arte oficial e introdujeron conceptos contemporáneos en la pintura rusa, diversificando sus horizontes y abriendo el arte local al mundo.

De la Edad Media quedan frescos, miniaturas y, sobre todo, iconos. Existe una fuerte influencia bizantina en el arte de esta época. A veces se transmite directamente por medio de artistas de origen griego como Máximo y, sobre todo, Teófano o Teófanos el Griego (siglo XIV), pintor de frescos e iconos. 
La tradición de la pintura de iconos en Rusia fue importada del Imperio bizantino, que dotó al estado recién cristianizado con los materiales necesarios para la liturgia, incluyendo las representaciones religiosas de santos y mártires de la religión. 
Surgen las primeras escuelas nacionales en torno a la elaboración de iconos. El centro de cultura de entonces era Kiev, hoy perteneciente a Ucrania, y posiblemente los primeros pintores activos en esta ciudad fueran griegos o eslavos bizantinizados, que sirvieron de maestros para la formación de una escuela local de pintura. La primera producción de la que se llamó escuela de Kiev seguía estrechamente el estilo bizantino, pero luego pasó a tener características propias, evidentes en la selección de los colores y en la dimensión de las imágenes, así como en la expresividad de las figuras, de las cuales el Cristo Pantocrator, uno de los modelos formales más importantes de esta época, fue presentado con un aspecto más benevolente y humano que en el patrón original. Obra maestra de la iconografía rusa es la "Virgen de Vladímir", conservada en Moscú. Representa a la la Virgen María con el Niño Jesús que, aun siendo de origen bizantino (fue regalada al gran duque Yuri Dolgoruki de Kiev en torno al año 1131 por el Patriarca griego Lucas Chrysoberges), luego se volvió modelo para incontables copias y variaciones, definiendo una de las tipologías más populares de toda la iconografía sacra rusa y siendo hasta hoy una de las imágenes más veneradas en todo el país. En 1240 Kiev fue tomada y completamente incendiada por los mongoles, y la actividad artística principal se trasladó a Nóvgorod. 

La escuela de Nóvgorod, activa ya en el siglo XI vivió su mejor época entre el siglo XII y el XIV. Ajena a la ocupación mongola, se convirtió en el principal centro artístico del país antes de ser suplantada por Moscú. Su primera fase prefirió iconos en frescos, de los cuales los ejemplos más antiguos están en la Iglesia del Salvador en Nereditsa y en la Iglesia de San Jorge en Stáraya Ládoga. La escuela de Nóvgorod se distingue por la intensidad de los colores, aplicados sin mezcla o gradaciones de tonos, un sombreado mínimo, el dibujo enérgico y preciso, y una preferencia por la composición clara con una simbología simple y fácilmente legible por el pueblo. En el siglo XIII cambia el estilo: los colores se suavizan, la composición gana en dinamismo y espontaneidad y predomina más el aspecto gráfico que lo pictórico. A diferencia de Kiev, más bizantina, la escuela de Nóvgorod asimila elementos del arte folclórico local, el aspecto de sus figuras es menos hierático y más humanizado, parecidas a la gente rusa. Su mirar fijo y penetrante establecía un contacto directo con el espectador, logrando una expresión más soñadora, indirecta e introspectiva. 
En el siglo XII también se formaron otras escuelas regionales en Vladímir, Súzdal, Yaroslavl y Pskov (abadía de Mirozhski, ligadas a la tradición bizantina. En el siglo XIII la invasión mongola devastó Rusia y rompió sus lazos históricos con Bizancio, arruinando muchos de los centros productores de iconos, salvo Nóvgorod y Pskov, donde continuó viva la tradición de pintura de iconos. Menos sofisticada que Nóvgorod, considerada su «hermana mayor», la escuela de Pskov se distinguió por la iconografía formalista y arcaizante, por la intensa expresión emocional de sus figuras y por el uso de tonalidades de color diferenciadas, en especial por lo que se refiere al verde, al naranja y el rojo.

La escuela de Moscú se vio impulsada, junto con otros maestros locales, por Teófano el Griego (h. 1330-h.1410), formado en Constantinopla. Los orígenes de esta escuela se ven oscurecidos por la casi completa inexistencia de ejemplos primitivos, pero se sabe que surgió aproximadamente junto con Nóvgorod y que cuando Teófano llegó ya había una significativa actividad artística. Él, junto con el notable Andréi Rubliov, aunque poseían estilos muy diversos, llevaron esta escuela a su primer florecimiento importante en el siglo XV, en la época en que la influencia de Moscú crecía, tras la expulsión de los mongoles, y se convertía en el centro de la ortodoxia religiosa. Rubliov, nacido hacia el año 1360 y muerto en Moscú h. 1430, es un artista de cuya vida poco se sabe. Se formó en Moscú con Projor de Gorodets y Teófanes el Griego. Vivió como monje en el monasterio de San Sergio. De su obra destaca el icono de la "Trinidad" (actualmente en la Galería Tretiakov de Moscú). Fechado en torno al año 1430, se considera que es el más importante icono bizantino de la escuela rusa. Representa a la Trinidad a través de la escena bíblica llamada «visión de Mambré»: tres ángeles se aparecen al patriarca Abraham. Se caracteriza por el aire melancólico, de intensa espiritualidad. El ángel del centro, con túnica roja, se cree que representa a Cristo con un árbol al fondo. El de la izquierda sería Dios Padre y el de la derecha, el Espíritu Santo. La perspectiva es típica del tipo bizantino, es decir, inversa, abriéndose las líneas conforme se alejan de los ojos del espectador. 

A estos artistas de la escuela de Moscú se debe la definición del iconostasio: una pared cubierta de iconos que se elevó hasta el punto de ocultar completamente el altar, aislándolo de la congregación. Es una alteración significativa respecto al modelo de altar bizantino, y fue introducida en las obras que realizaron en la Catedral Blagoveshchenski de Moscú en torno a 1405. 

La expulsión de los mongoles posibilitó que la tradición de los iconos resurgiese o se iniciase en otras ciudades, como Tver, Suzdal, Rostov o la lejana Kargopol. Entre esos centros menores merece destacarse la escuela de Tver, que se diferenció por el uso de tonos exóticos de azul y turquesa en una paleta más clara.

La pintura de iconos se mantuvo durante toda la Edad Moderna, tomando como referencia estética los caracteres de la pintura bizantina clásica, que se impone a las influencias italianas. A principios del siglo XVI destacó como iconógrafo en Moscú Dionisio o Dionisios (h. 1440-1510). El abad de Volotsk le escribió una carta defendiendo los iconos que se hizo famosa: «Carta a un iconógrafo». A Dionisio y sus discípulos se deben los frescos de la Catedral de la Dormición del Kremlin, además de retratos del zar. 
En el siglo siguiente destacó Simon Ushakov (1626-1686) primer grabador ruso, que trabajó en Moscú, creando un estilo nuevo junto con Vladímirov, en el que se conserva la tradición y la aúna con las novedades de la pintura occidental. Dionisio y Ushakov renovaron el concepto de espacio pictórico, prestaron atención a sutilezas cromáticas y enfatizaron el misticismo en su arte, en detrimento del aspecto dramático. La influencia de la pintura occidental se nota en el uso, especialmente visible en algunas de las obras de Ushakov, de un discreto claroscuro para acentuar la ilusión de tridimensionalidad. Otro elemento distintivo es la aparición del retrato profano, conocido con el nombre de "parsuna," también influido por Occidente pero regido por las convenciones de la pintura religiosa. 

A pesar de estas innovaciones, el arte de iconos fue decayendo en los siglos XVII y XVIII. Cada vez son más realistas y narrativos; la orfebrería decorativa, elemento secundario de los iconos, cobra cada vez más protagonismo, ocupando mayor superficie pictórica. Se pierde contenido místico en favor de lo decorativo. Se prefiere la miniatura, una diversificación de los temas sacros debido a la influencia del florecimiento literario del país, y aparece en la misma ciudad de Moscú la escuela Stróganov. 

Esta escuela prefirió la miniatura, por sus colores y el gran refinamiento y detalle en las imágenes. Dentro de sus maestros estuvieron Prokopii Chirin, Nikifor e Istoma Savin. La escuela Strogánov, cuyo nombre deriva de la rica familia Stróganov, que la patrocinaba, influyó hasta finales del siglo XVII, cuando el arte profano apoyado por el Estado y la nobleza se convirtió en el centro de nuevas tendencias pictóricas. 

Entre las últimas escuelas regionales de iconos estaba la que floreció en torno al Monasterio de la Trinidad y de San Sergio, en Jolui, que inició sus actividades en el siglo XVII y rápidamente ganó mucho prestigio en el norte del país. En 1882 su producción fue organizada por la Hermandad Alejandro Nevski. Iniciaron una gran producción de iconos y frescos en las principales ciudades, de la más alta calidad. Cuando se implantó en Rusia el comunismo, dentro del marco más amplio de la persecución religiosa, se cerró la escuela de Jolui. No obstante, su refinada técnica no se perdió y la escuela de Jolui fue reabierta en 1943, dirigida entonces por un graduado en la Academia de Leningrado, U. A. Kukuliev. Se transformó en una oficina de artes aplicadas, centrada en la producción de miniaturas laqueadas. La rehabilitación de la Iglesia Ortodoxa en Rusia posibilitó el renacimiento de la pintura de iconos, practicándose en la propia Jolui y otros centros como Palej.

Las más completas colecciones de iconos rusos se encuentran en la Galería Tretiakov de Moscú y en el Museo Pushkin de Moscú.

En la Edad Media se pintaron frescos y también miniaturas. En las de carácter popular, como el Psalterio de Kludov (Moscú) abundan las representaciones marginales; en las aristocráticas las miniaturas ocupan toda la página. La tradición de los manuscritos iluminados se inició en Kiev y se desarrolló en paralelo a la pintura de iconos, con la que tiene puntos estilísticos en común. A pesar de depender en gran medida del arte bizantino, están presentes también influencias anglo-normandas, carolingias y otonianas, que llegan a Rusia a través de las rutas comerciales medievales entre Rusia y el resto de Europa. El ejemplo sagrado más antiguo que se conserva es el "Evangeliario de Ostromir", compuesto en torno al año 1056 por el diácono Gregor y su taller, para el patrono Ostromir de Nóvgorod. En sus páginas como ilustraciones se percibe claramente la influencia de modelos bizantinos, que demuestra el elevado nivel que la cultura local había alcanzado por entonces, apenas setenta años después de la introducción de la escritura en la región. 

Otras obras medievales importantes son la "Miscelánea de Sviatoslav", del siglo XII, el "Evangeliario Siiski", de 1339, con diversas escenas en un estilo elegante, el "Evangeliario Fiódorovski", de 1327, o el "Psalterio de Kiev", de 1397, con trescientas tres miniaturas que tratan temas variados, sacros y profanos, animales y vegetales, y el "Evangeliario de Nóvgorod", de 1575, con figuras de los evangelistas e iniciales decoradas.

Pero existen igualmente miniaturas de gran importancia sobre textos profanos, como la gran "Crónica Radzivill", la más antigua y una de las más preciosas en su género, una narrativa ricamente decorada que narra la historia de Rusia entre los siglos V y XIII, producida en el siglo XV. También posee bellas ilustraciones la crónica "Licevoy svod", de 1480, con escenas de batalla.

A diferencia de lo ocurrido en Europa Occidental, la tradición de manuscritos iluminados se prolongó en Rusia durante la Edad Moderna. Así, cabe citar el manuscrito de "La leyenda de la derrota de Mamai", del siglo XVII, un romance histórico, y el "Libro de los títulos de los Zares", de 1672, con una serie de retratos regios y decoraciones realizados por artistas del Kremlin en el siglo XVII. La tradición de miniaturas comenzó a decaer a finales del siglo XVII con su sustitución por libros impresos, aunque puedan encontrarse ejemplares aislados aún en el siglo XIX.

Se produjo un cambio radical en el arte ruso con la subida al trono de Pedro el Grande. En su proyecto de modernizar el país y equipararlo culturalmente a las grandes naciones europeas, las artes tuvieron un especial relieve como forma de ilustrar los avances de la civilización. Atrajo a Rusia a artistas extranjeros y envió a jóvenes rusos con talento para que estudiasen en Italia, Francia, Inglaterra y los Países Bajos. Entre los artistas extranjeros que viajaron a Rusia en el siglo XVIII estuvieron Jean-Baptiste Perronneau, Jean-Baptiste Le Prince, Stefano Torelli, Heinrich Buchholz, Johann Baptist von Lampi, el Viejo, Pietro Rotari, Jean-Louis Voille, Louis Caravaque y Élisabeth Vigée-Le Brun, quien después del arresto de la familia real francesa durante la Revolución huyó del país con su hija menor Julie. En Rusia, le fue muy útil su experiencia tratando con clientela aristocrática. Fue recibida por la nobleza y pintó a numerosos miembros de la familia de Catalina la Grande. Estando allí, Vigée-Le Brun fue nombrada miembro de la Academia de Bellas Artes de San Petersburgo. Para consternación de Vigée-Le Brun, Julie se casó con un noble ruso. 

Ya antes existía alguna tímida influencia de Occidente, como se ve en los "parsuna" y en autores sacros como Karp Zolotariov, que mezcló de forma original la escuela barroca italo-neerlandesa con la tradición ruso-bizantina. Pero el impacto occidental en el reinado de Pedro no tuvo precedentes; se adoptó la estética barroca en la pintura, ahora casi toda ella dedicada a los temas profanos, sin sombra alguna de arcaísmo bizantino pero también con unos pocos rasgos de identidad propia. Los artistas rusos formados en el exterior comprendieron perfectamente los principios técnicos y estilísticos en la pintura occidental.

El primer pintor educado totalmente fuera de Rusia fue Andréi Matvéyev, que estudió en Flandes y en los Países Bajos durante once años. Cuando regresó a Rusia, se convirtió en una de las figuras destacadas en la renovación de la pintura rusa. Iván Nikitin estudió con Tommaso Redi en Florencia, y Alekséi Antrópov con Louis Caravaque. Nikitin y Antrópov ilustran la transición entre la tradición de los "parsuna" y el retratismo típicamente occidental.
La occidentalización se acentúa en los reinados posteriores. Catalina II (reinó 1762-1796), que era una francófila, amante de las artes y ávida coleccionista, estimuló la pintura creando el Museo del Hermitage y otros. Se pasó del Barroco al Rococó, y aparecen signos de la ilustración neoclásica. La pintura occidentalizada se integró desde entonces plenamente en la cultura de las élites. En particular, el retrato elogia a la nobleza, a la que representa a través de posturas y decoraciones típicas. Entre los retratistas rusas destacaron Fiódor Rokotov (1736-1808), Dmitri Levitski (1735-1822) y Vladímir Borovikovski (1757-1825); demostraron originalidad en relación con sus modelos extranjeros y evidenciaron un altísimo grado de calidad a que llega en relativamente poco tiempo la pintura rusa de tradición occidental. Levitski, concretamente, destacó como retratista de Catalina II, aunque murió en la pobreza. También se desarrolló el paisajismo, con Fiódor Alekséyev (1754-1824) y Fiódor Matvéyev (1758-1826), paisajistas pioneros en Rusia, que habían viajado por Italia. Y comienza a tener influencia el «gran estilo» de la pintura de historia. Otros pintores de la misma generación fueron: Alekséi Belski, Iván Argunov, Semión Shchedrin y Antón Losenko. 

En 1757 Se creó la Academia de Bellas Artes de San Petersburgo, que Catalina II llamó luego «Academia Imperial de las Artes». 
Este departamento del gobierno supervisaba todo el sistema artístico en Rusia, organizaba la enseñanza, distribuía premios y becas, contrataba maestros extranjeros, creaba una colección propia con obras extranjeras para ilustración de los alumnos y estimuló el Neoclasicismo. El histórico edificio de la Academia sobre el río Neva en San Petersburgo alberga hoy el "Instituto Académico de San Petersburgo para la Pintura, Escultura y Arquitectura «Iliá Repin»", pero aún informalmente se le conoce como la Academia de Arte de San Petersburgo.

Después de la importación casi literal de estéticas extranjeras en el siglo anterior, en el XIX los avances estilísticos e ideológicos conllevan el establecimiento de una escuela de arte genuinamente rusa. A principios de siglo la Academia Imperial tuvo su apogeo bajo la dirección de Aleksandr Strogánov. El Neoclasicismo también entró en su fase más brillante, siendo la influencia más fuerte la de Ingres, pero luego perdió su fuerza y en seguida se suceden con rapidez, y a lo largo de todo el siglo, las diversas tendencias: Romanticismo, Naturalismo, Realismo y Simbolismo.
El romanticismo propugna la flexibilización del canon neoclásico. Prefirió retratos intimistas de caracterización psicológica, paisajes mediterráneos idílicos que los artisttas academicistas conocen al viajar por Italia o escenas históricas dramáticas. El arte, en general, se extiende a círculos más amplios, alejados de la corte. Silvestre Cedrin (1791-1830) pintó vistas de ruinas clásicas. Karl Briullov (1799-1853), profesor en la Academia, cultivó diversos géneros, pero se le conoce sobre todo por su pintura de historia, siendo muy conocida su "Último día de Pompeya". Aleksandr Ivánov es conocido sobre todo por una obra religiosa a la que dedicó veinte años de su vida: "Aparición de Cristo al pueblo". Otros nombres de transición del neoclasicismo al romanticismo son Maksím Vorobiov, Vasili Tropinin, Orest Kiprenski y Alekséi Venetsiánov. 

La Revuelta de Pugachov de 1773-74, que buscó, sin éxito, abolir la servidumbre. Con motivo de las Guerras Napoleónicas, las clases medias, en particular los oficiales del ejército, conocieron mejor la dura realidad del pueblo, y presenciaron el heroísmo de los soldados. Estas circunstancias hicieron que la vida de las clases inferiores fuera un tema aceptable para el «gran arte». Eso sí, en principio debía presentarse de forma idealizada, diluyendo la cruda realidad de los siervos y campesinos en un bucolismo gentil. Por los mismos motivos, prolifera la escena de género que muestra escenas domésticas de la clase burguesa. La majestosa geografía del país y los tipos humanos y costumbres típicamente locales, llamaron la atención de los artistas, de manera que el retrato y el paisaje fueron haciéndose, poco a poco, más objetivos y naturalistas. Además, surge un nuevo interés por los temas de historia de Rusia, sus batallas y figuras centrales, la vida de los antiguos boyardos, la mitología y religiosidad populares; como resultado, se crean cuadros que reconstruyen visualmente la vida nacional del pasado, pero en una interpretación romántica y medievalista. 

Intelectuales de la época como Nikolái Dobroliúbov y Nikolái Chernishevski sostenían que el arte no debía apartarse de la realidad, sino explicarla y juzgarla. Los pintores consideraron entonces que el arte era algo más que la afirmación de la supremacía de la clase dominante o el retrato ameno y sin compromiso de la vida popular, y que debía usarse para educar moral y socialmente a un público más vasto. Surgió así un arte de crítica social, realista, que los pintores deseaban poder cultivar sin la dirección de la Academia. Así apareció, a mediados de siglo, un nacionalismo artístico que representa el primer momento realmente original de la pintura rusa desde la consolidación de las escuelas de iconos medievales. Esta pintura se centraba en representar al pueblo y el paisaje rusos. 

De esta primera mitad de siglo destaca Pável Fedótov (1815-1852), pintor moscovita que retrata con realismo pero irónicamente la sociedad burguesa de su época en obras como "La petición de matrimonio" y "La joven viuda". Un paisajista a medio camino entre el romanticismo y el realismo fue Lev Lvovich Kamenev (1831-1886), en cuyos cuadros destaca el estudio de la luz sobre la superficie del agua. La nueva pintura fue apoyada por el influyente crítico . 

No obstante, la Academia Imperial seguía atada a convenciones rígidas; prefería los temas históricos y mitológicos, los paisajes italianizantes y los retratos convencionales de la nobleza. Era un arte que empezaba a estar desfasado, aunque hubo maestros de talento entre sus filas, como Aleksandr Litóvchenko. Académicos extranjeros visitantes como Franz Xaver Winterhalter y dejaron en Rusia algunas de sus mejores obras, especialmente en el campo del retrato. La Academia, dada su íntima relación con el poder constituido, no podría abrazar una causa que era en esencia populista y burguesa, aunque entre los nuevos artistas no se observe una disminución de la calidad técnica en relación con los academicistas. 

En 1863, trece artistas, liderados por Iván Kramskói (1837-1887), insatisfechos con la línea académica, crearon la llamada "Peredvízhniki" (Sociedad de exposiciones ambulantes), que pretendía recuperar cierta tradición pictórica rusa, en particular la pintura de iconos, pero con un tratamiento naturalista. Tuvo un enorme éxito y llegaron a un público mucho más amplio. En sus primeros veinticinco años de actividad, la Sociedad de Ambulantes atrajo la atención de los principales artistas rusos, produjo más de tres mil obras y alcanzó a un público de un millón de personas en cerca de quince ciudades. No sólo la pintura profana se vio afectada por las innovaciones de los Ambulantes. También el arte sacro de Mijaíl Vrúbel (1856-1911), Iván Kramskói y mostró la asimilación de sus principios. Vrúbel perteneció al Círculo de Abrámtsevo; fue un dibujante de gran originalidad, que se enmarca en la transición del realismo al simbolismo; trató temas inspirados por la religión ortodoxa rusa, en particular con la figura del demonio. La influencia de la Sociedad de los Ambulantes fue tanta que obligó a la propia Academia a revisar sus posiciones: aceptó la tendencia, a la que llamó realismo ideológico, y más tarde contrató a algunos de sus miembros como profesores. La Sociedad luchó por un arte nacionalista que fuese también un arma de denuncia de las injusticias sociales; fue la avanzadilla de las vanguardias del momento hasta que ella misma, conquistada ya la aceptación de sus ideales por la Academia, comenzó a hacerse más rígida, proscribiendo las experimentaciones más radicales del Modernismo. Aun así, proporcionó la base para la posterior formación del Realismo socialista.
El más destacado de los Ambulantes fue Iliá Repin (1844-1930), a quien se atribuye la introducción del realismo en la pintura rusa. Se formó como pintor de iconos y luego pasó por la Academia de Bellas Artes de San Petersburgo. En sus minuciosos lienzos describió la sociedad de su época, con sus desigualdades ("Los sirgadores del Volga", "Procesión de Pascua en la región de Kursk"), y también episodios de la historia rusa; destacó igualmente como retratista de compositores y escritores. A este grupo de los Ambulantes pertenecieron también Vladímir Makovski (1846-1920), que empezó con escenas sociales y fue adoptando temas cada vez más políticos; Fiódor Vasíliev (1850-1873), uno de los mejores realistas rusos, que realizó numerosos paisajes; Isaac Levitan (1860-1900), el paisajista más destacado de la época, influido tanto por Alekséi Savrásov (1830-1897) como por Camille Corot y que encontró en escritores como Chéjov y Tolstói a grandes defensores de su arte; Valentín Serov (1865-1911), alumno de Repin y de formación académica, cultivó tanto el paisaje como el retrato, y fue escenógrafo para los Ballets rusos de Serguéi Diáguilev. Otros miembros notables de la sociedad de los ambulantes fueron: Abram Arjípov, Nikolái Bogdánov-Belski, Mijaíl Clodt, Nikolái Kasatkin, Arjip Kuindzhi, Rafaíl Levitski, Vasíli Maksímov, Grigori Miasoyédov, Vasili Perov, Illarión Priánishnikov, Konstantín Savitski, Iván Shishkin, Vasili Súrikov, Víktor Vasnetsov y Nikolái Yaroshenko.

A finales del siglo XIX, hubo quienes defendieron unas ideas más liberales que los "Peredvízhniki" (Ambulantes), más abiertas a la influencia del arte occidental. Entre ellos estuvo el grupo llamado Mundo del Arte ("Mir Iskusstva"), fundado en 1889 por el pintor y escenógrafo Léon Bakst (1866-1924), Serguéi Diáguilev y Alexandre Benois. "Mir Iskusstva" es también el nombre de la revista que editaban. Atacaban la obsolescencia de los "Peredvízhniki" (Sociedad de exposiciones ambulantes) y el carácter antinatural de la sociedad industrial, y promovían la individualidad creativa y el espíritu Art Nouveau bajo una bandera positivista. Deseando hacer un arte accesible a todos, escogieron materiales más baratos como el "gouache" y la acuarela y redujeron la escala de sus trabajos. Konstantín Korovin, que además fue decorador y escenógrafo, (1861-1939) formó parte tanto del Círculo de Abrámtsevo como del Mundo del Arte; cultivó el género del paisaje y del retrato. Alumno de Repin fue Borís Kustódiev (1878-1927) que fue miembro del Mundo del Arte, la Sota de Diamantes y, tras la Revolución, de la Asociación de Artistas de la Rusia Revolucionaria. Realizó ilustraciones satíricas para periódicos como "Ádskaya Pochta". Al grupo Mundo del Arte perteneció igualmente (1875-1958), quien había expuesto con los Peredvízhniki, que tuvo una importante labor crítica y teórica; a partir de 1925 se integró en la Asociación de Artistas de la Rusia Revolucionaria. Kuzmá Petrov-Vodkin (1878-1939) también fue miembro del Mundo del Arte, con una obra muy original pionera en la pintura de vanguardia. El grupo Mundo del Arte se disolvió y fue sustituido por la Unión de Artistas Rusos, para después reaparecer con el antiguo nombre.

A principios del siglo XX, la cultura rusa se encontraba en un estado de febril efervescencia. Se adoptaron estilos modernos como el simbolismo o el impresionismo, y también revisitaron estilos históricos a través del neoprimitivismo, neogótico o neorromanticismo. Así, artistas como Konstantín Bogaevski, Nikolái Krýmov, Víktor Borísov-Musátov, Piotr Subbotin-Permiak, Natalia Nésterova, Vasili Denísov, Konstantín Korovin, Mijaíl Nésterov y Abram Arjípov crearon un puente entre la figuración académica y las artes visuales modernas, incluyendo importantes cambios en la técnica pictórica. Después penetraron en Rusia el cubismo, el expresionismo y el futurismo, que se combinaron con elementos nacionalistas para dar lugar a una nueva estética, hacia 1910, a la que se llama vanguardia rusa. En este momento, es la pintura rusa la que contribuye de manera trascendental al arte occidental a través de experiencias como el constructivismo, el suprematismo o el rayonismo, además de contribuir significativamente al nacimiento y teorización de la pintura abstracta con Kandinski.

En Vítebsk (hoy Bielorrusia) nació Marc Chagall, en el seno de una familia judía. Estudió en San Petersburgo y, después de la revolución, fue comisario de arte en Vítebsk. Tuvo un estilo muy original que mezclaba la influencia del judaísmo y la pintura rusa de iconos. Pasó la mayor parte de su vida en Francia, cuya nacionalidad adoptó y donde murió.

La obra de Vasili Kandinski ((Moscú, 1866-Neuilly, 1944) ilustra la llamada abstracción lírica. Estudió en Múnich, y su trabajo se incluyó después en el movimiento "Der Blaue Reiter". Cuando estalló la Primera Guerra Mundial volvió a Moscú. En la URSS organizó la Academia de Ciencias Artísticas. De nuevo en Alemania, impartió clases en la Bauhaus (1922-1932) hasta que la llegada del nazismo le llevó a vivir a Francia, donde vivió hasta su muerte. La obra inicial de Kandinski estuvo dominada por el impresionismo, el fovismo y el cubismo. Pero en torno al año 1910 su obra se hizo cada vez más abstracta, combinando simplemente formas y colores, de manera geométrica. Se reputa a Kandinski ser el fundador de la pintura abstracta, pues se le atribuyen las primeras pinturas occidentales enteramente abstractas. Sus ensayos contribuyeron al desarrollo teórico de la pintura abstracta: "De lo espiritual en el arte" (1910), "Del problema de la forma" y "Punto y línea sobre la superficie" (1926).

Aleksandr Ródchenko (1891–1956), tras haber pintado sus tres monocromos ("Amarillo puro, Azul puro, Rojo puro", 1918) y El Lissitzky, aprovecharon su conocimiento de la forma para ir avanzando hacia una concepción utilitaria del arte. Ródchenko creó el movimiento no-objetivista (1915), análogo al suprematismo. Lázar Lissitzky, llamado El Lissitzky (1890-1941) ilustró libros, luego, inspirado por el constructivismo y el suprematismo, realizó obras de vanguardia como los "Proun." Paralelamente, Vladímir Tatlin (1885-1953) creó con sus relieves abstractos una de las primeras formulaciones de lo que se llamaría el constructivismo. Partió de las formas cubistas, para desarrollar una obra abstracta en tres dimensiones. Otras figuras de renombre serían Vladímir Maiakovski, Varvara Stepánova y Liubov Popova. 

Con Kandinski estudió Aristarc Lentulov (1882-1943), quien formó parte del grupo la Jota de Diamantes y expuso con el Mundo del Arte. En los años treinta, bajo las teorías artísticas de estalinismo dejó la abstracción y se orientó hacia lo figurativo.

El cubismo está representado por Aleksándr Archipenko (Kiev, 1887- Nueva York, 1964), quien inventó la «archipintura», pinturas móviles; desde los años veinte vivió y trabajó en los Estados Unidos.

Por su parte, Mijaíl Lariónov (Tiraspol, cerca de Odesa, 1881-Fontenay-aux-Roses, 1964) y Natalia Goncharova (1881-1962), compañeros sentimentales que trabajaron juntos, llevaron hasta la abstracción pura su método de transcripción del fenómeno luminoso, al que denominaron rayonismo y que empezó en Rusia en el año 1910. El rayonismo partía de otras tendencias precedentes, en particular el cubismo, el orfismo y el futurismo. 

Tras haber sido el principal representante del cubo-futurismo, Kazimir Malévich (Kiev, 1878-Leningrado, 1935) rompió radicalmente con todas las viejas concepciones del arte al pintar en 1915 "Carré noir", que dio lugar al suprematismo. Pasó por el postimpresionismo, el neoprimitivismo, el cubismo y luego contactó con el futurismo y "Der Blaue Reiter". Escribió el "Manifiesto del suprematismo" (1915), así como "Del cubismo al suprematismo". En los años treinta abandonó la abstracción y volvió a la pintura figurativa. Las propuestas suprematistas son las más radicales de la vanguardia rusa, pues llevaron la abstracción geométrica a extremos de simplificación en obras radicales como "Cuadrado negro sobre fondo blanco". Otros participantes fueron Aleksandra Ekster, Olga Rozanova, Nadezhda Udaltsova, Anna Kagan, Iván Kliun, Liubov Popova, Nikolái Suetin, Iliá Chashnik, Lázar Khidekel, Nina Genke-Meller, Iván Puni y Ksenia Boguslavskaya. 

El gran público no recibió con facilidad estas obras, acostumbrado como estaba al arte académico y a un modernismo moderado. Estos movimientos de vanguardia buscaban no sólo una nueva sensibilidad sino también un arte con función social positiva, libre de las convenciones del arte burgués. Por eso, cuando estalló la Revolución de 1917 los apoyó tanto el gobierno revolucionario a través de Anatoli Lunacharski, que encabezaba el Comisariado Popular de Educación, como los movimientos literarios que deseaban colocar el arte al servicio del proletariado, como el Proletkult. En la primera década posterior a la Revolución se originó un extraordinario movimiento de vanguardia en todas las artes, los artistas tuvieron una amplia libertad de acción, el debate sobre el nuevo papel de las artes permanecía candente y la tónica fue el experimentalismo. Pero llegó un momento en que el apoyo oficial cesó. El Partido Comunista, ya firmemente instalado en el poder, consideró necesario crear nuevas reglas para el arte nacional. En 1928 todas las instituciones culturales independientes fueron cerradas, Lunacharski fue destituido y se inició la elaboración de un nuevo programa oficial para la cultura rusa.

Una vez establecido el nuevo régimen, las innovaciones artísticas de la vanguardia rusa se vieron con sospecha, al haber surgido antes de la Revolución, y se consideraba que posiblemente era arte decadente y burgués. Además, muchos miembros del PCUS, como gran parte de la población, no apreciaban las estéticas de vanguardia, rechazaban una abstracción que no comprendían y que no les parecía útil como ilustración doctrinaria. Es significativo que la última exposición de vanguardia se celebrase en Leningrado entre noviembre de 1932 y mayo de 1933: Malévich presentó en ella obras figurativas muy esquematizadas, personas privadas de rostro contra paisajes vacíos.

La nueva política se oficializó en 1932 cuando Stalin promulgó el decreto "Sobre la reconstrucción de las organizaciones literarias y artísticas". Sus directrices se impusieron incluso con violencia, castigándose severamente al que se rebelara. Las restricciones hicieron que muchos intelectuales y artistas emigraran a otros países. Tras la Segunda Guerra Mundial, el arte de Occidente fue declarado de nuevo nocivo y varios pintores enviados al exilio a Siberia. Aunque tras la muerte de Stalin se atenuó el rigor, incluso en los setenta podía, sin previo aviso, clausurarse una exposición y destruirse las obras expuestas.

Lo que se pretendía era glorificar la lucha del proletariado por el progreso y el logro de una sociedad socialista ideal. Según el "Estatuto de la Unión de Escritores Soviéticos", de 1934, el realismo socialista debía ser una representación artística históricamente fiable de la realidad en su desarrollo revolucionario. Debía tener un carácter educativo de transformación ideológica de los trabajadores en el espíritu del socialismo. Gorki, decretó que la obra socialista debía tener cuatro características esenciales. Sería: 

Como el proletario era el centro de esa nueva sociedad, se volvió de nuevo un objeto digno de estudio, como en la época romántica o la de los "Ambulantes". También el líder de ella debía ser elogiado y representado de manera exaltada, junto con ambientes fabriles y campesinos. Siendo ahora el Estado el único mecenas, todo artista se volvía un empleado de la maquinaria estatal, y la pintura, asociándose a las artes gráficas, frecuentemente fue reproducida en gran escala en carteles propagandísticos.

El realismo socialista fue figurativo y de talante optimista. En cuanto a su veracidad histórica, es más bien irreal, incurriendo en excesos a la hora de representar la fuerza, las virtudes o la alegría y contentamiento del proletario. Las opciones eran limitadas, y el estilo pronto se hizo repetitivo, decayendo su calidad. No había lugar a las obras experimentales, que se proscribían por ser consideradas decadentes, obscenas, vulgares, formalistas, pesimistas o degeneradas, y por lo tanto, desde el principio, anti-comunistas.

Muchas son las críticas que se le han hecho al realismo socialistas: la destrucción de la cultura nacional, imponiendo una artificial; la abolición de los lazos orgánicos entre el creador y la obra, sustituidos por un programa apriorístico; la obligatoriedad de tratar unos temas determinados; el aislamiento del arte ruso respecto a las tendencias contemporáneas en el resto de Europa; la supresión de la espontaneidad e individualidad creadora. Pero, por encima de todo, se condena la represión a aquellos que se desviaban de la norma: pequeños detalles podían llevar a un artista al exilio o la muerte.

No obstante, parte de este arte alcanzó un alto nivel, tanto estético como ético y técnico. Muchos de los pintores habían sido formados en la Academia, y muchos también se adhirieron a estos principios de buena voluntad, por encontrarse en sus ideales individuales. Entre sus representantes más típicos estabam Izaak Brodski, Kuzmá Petrov-Vodkin, Georgi Riazski, Borís Ioganson, Aleksándr Gerasimov, Aleksándr Moravov, Iván Vladímirov, Borís Vladímirsky, Karp Trojimenko, Tarás Gaponenko, Aleksándr Laktionov, Piotr Dobrinin, Alekséi Nesterenko, Valentín Lisenkov, Vasili Ivánov, Vladímir Krijatski, Mijaíl Bozhie, Vasili Saicenko y Nikolái Terpsijorov.

A la muerte de Stalin en 1953, el realismo socialista comenzó a ser atacado por el propio Partido Comunista en aspectos como el culto a la personalidad. Artistas que hicieron carrera siguiendo sus dictámenes, como Aleksandr Gerásimov, autor de retratos idealizados de Iósif Stalin, perdieron sus cargos oficiales, y otros que habían sido proscritos, como Kuzmá Petrov-Vodkin, fueron rehabilitados. Pero esta nueva atmósfera de libertad no logró grandes avances, y el realismo socialista siguió siendo la directriz principal en pintura. 

Los artistas disidentes, que fueron conocidos como los inconformistas, siguieron trabajando en gran medida en la oscuridad y el aislamiento, aunque en ese momento se sintieron unidos en torno de un propósito común. No eran un grupo organizado ni tenían una propuesta estética unificada. Pero tenía un objetivo común: desmitificar el idealismo artificial y autoritario del arte estatal, que no reflejaba la realidad como pretendía. Hubo grupos inconformistas en Moscú, San Petersburgo y otras ciudades. En los setenta las autoridades los aceptaron, si bien de manera limitada. No había mercado para ellos, y sólo una mínima parte de sus trabajos lograba exponerse. El coleccionista estadounidense Norton Dodge, con ayuda de diplomáticos extranjeros y autoridades locales tolerantes, adquirió clandestinamente un gran acervo de más de diecisiete mil obras entre 1956 y 1986, que instaló en el Museo de Arte Jane Voorhees Zimmerli de la Universidad de Rutgers, en los Estados Unidos. Entre los inconformistas estuvieron , Lidia Masterkova, Koriún Nahapetián, Vladímir Nemujin, Aleksándr Rappoport, Yevgueni Rujin, Vasili Sítnikov, Oleg Vasíliev, Vladímir Yankilevski y Anatoli Zvérev. En esa misma década de los setenta surgieron los conceptualistas rusos, centrados en Moscú, que trabajaban los principios del arte conceptual. Entre ellos estaban el ya mencionado Erik Bulátov, , , Andréi Monastyrski y Víktor Pivovárov. 

Con la apertura gradual de la política de la Unión Soviética en los ochenta, que finalmente condujo a la desintegración de todo el bloque comunista, todo el programa oficial de arte también se derrumbó. Artistas como R. Bichuns, P. Torda, D. Zhilinski, E. Shteinberg, M. Romadin, M. Leis y V. Kalinin fueron ampliamente reconocidos. Se abrió definitivamente Rusia a los avances del arte occidental contemporáneo y hubo una rápida expansión en el panorama de la pintura rusa, un fenómeno que continúa hasta hoy en día.

Existen en Rusia otras tradiciones pictóricas, apartadas de la pintura convencional, como las miniaturas laqueadas y la pintura primitiva o folclórica.
Las miniaturas laqueadas surgen en el siglo XVIII en torno a la ciudad de Fedóskino, de ahí que también se las conozca como «». Se trata de pinturas, normalmente sobre papel maché, utilizando no sólo óleo sino ricos pigmentos como madreperla, polvo de metales preciosos, de manera que crea el efecto de un brillo plateado. Su momento álgido fue el siglo XIX. Los temas son variados: paisajes, escenas de caza, composiciones florales y retratos, pero sobre todo se preferían escenas de leyendas rusas y escenas de género locales, como bebedores de té con samovares, troikas y escenas de la vida campesina rusa. Otros importantes centros de producción además de Fedóskino son Jolui, Zhóstovo, Mstiora y Pálej. La pintura de Fedóskino contemporánea conserva los rasgos típicos del arte folclórico ruso.
Luego hay una pintura popular, primitiva, ingenua y folclórica que comenzaron a aparecer a principios del siglo XVIII. Los artistas primitivos, anónimos, que estaban fuera de las escuelas estatales, repetían modelos formales precedentes, usando técnicas tradicionales. Tenían algún conocimiento del arte por trabajar muchas veces para la nobleza rural, quienes deseaban imitar a la nobleza urbana dotando de obras de arte sus mansiones. Desarrollaron un retrato derivado de la tradición de los "parsunas". 

Estos artistas populares pueden distinguirse, aunque no con facilidad, de los ingenuos o «naïf», quienes eran artistas cultos, no del pueblo, pero que creaban un estilo único y extravagante que luego repetían como una fórmula. Los naïf aparecen a finales del siglo XIX y la llegada del comunismo tuvo un fuerte impacto sobre ellos, multiplicándose este tipo de obras. 

Todos estos artistas al margen del arte oficial, los primitivos y los naïf, han recibido recientemente mayor atención del gobierno y los coleccionistas, con museos dedicados a la preservación y difusión de estos trabajos.


</doc>
<doc id="15597" url="https://es.wikipedia.org/wiki?curid=15597" title="Acorus calamus">
Acorus calamus

Acorus calamus, el cálamo aromático, es una de las dos especies del género "Acorus" de la familia de las acoráceas. Es conocido vulgarmente como "ácoro dulce", "ácoro aromático" (cálamo), “cálamo acuático” o “ácoro verdadero”, aparece ampliamente distribuido en la zona templada del hemisferio norte, siendo originaria del sudeste asiático.

Se asemeja a los juncos y posee hojas largas lineares de bordes afilados, muy apuntadas, de unos 25mm de anchura. Las flores, pequeñas y de color verde amarillento, se presentan en forma de espiral sobre un espádice desnudo, cuya espata sobresale por encima. El tallo se prolonga bajo tierra en forma de rizomas de largas raíces adventicias, carnosas y fuertemente aromáticas.

Es una planta alta, perennifolia con hojas aromáticas y raíces rizomatosas.
Las hojas, de alrededor de un centímetro de ancho, alcanzan los 10 de largo, y son lanceoladas y de bordes serrados u ondulados. La vena central de la hoja, prominente y fácilmente distinguible de las apenas relevantes venas secundarias, permite distinguirla fácilmente de "A. americanus". Las flores alcanzan los 4 mm, y son estériles, reproduciéndose rizomáticamente; el ovario es vestigial.
Crece en las regiones boreales de todo el globo, aunque es nativa de Europa. Prefiere las tierras húmedas y las costas de aguas lentas o estancadas. Su tallo semileñoso se utilizó en la Antigüedad clásica para fabricar cálamos para la escritura.

La raíz del cálamo, confitada, se usa en Europa en repostería, de ella también se extrae una droga estimulante y carminativa, llamada cálamo.

La maceración de la raíz del cálamo aromático no solo se emplea como tonificante en los casos de debilidad general del aparato digestivo, gases del estómago y de los intestinos, sino que también es un remedio poderoso para combatir las enfermedades de las glándulas así como la gota. Esta tisana contribuye enormemente a calentar el estómago y los intestinos lentos y a fomentar la secreción de mucosa. Se recomienda contra el metabolismo lento, las digestiones lentas, lo mismo que contra la clorosis y la hidropesía.

Se ha utilizado por su fragancia y como droga psicotrópica.
Su raíz se empleaba como anestésico y estimulante contra la fatiga por la asarona que contiene; en dosis mayores, produce efectos alucinógenos. Walt Whitman empleó esta o la raíz del estrechamente emparentado "Acorus americanus". Figuraba en las pociones psicotrópicas preparadas por las brujas europeas durante la Edad Media.
Contiene escasos taninos, aceite esencial (1,5-3,5%), rico en asarona y compuestos sesquiterpénicos, entre los que destaca la acalamona. En las hojas hay trazas de alcaloides. Otras fuentes: Acorina, tanino, colina, esencia con asarona, eugenol, pineno. Ácido cetílico y ácido palmítico, vitamina B1. La esencia se hace viscosa llegando a densidad del 0,36%, el rendimiento está entre 1 y 4%.

Se utiliza como aperitivo, eupéptico, por sus principios amargos; carminativo, diurético, sedante, sudorífico, hipotensor, espasmolítico, anticonvulsivante, antirreumático tópico, por el aceite esencial. Indicado para inapetencia, dispepsias hiposecretoras, gastritis, espasmos gastroduodenales, meteorismo. Ansiedad. Estados en los que se requiera un aumento de la diuresis: afecciones genitourinarias (cistitis, ureteritis, uretritis, oliguria, urolitiasis), hiperazotemia, hiperuricemia, gota, hipertensión arterial, edemas, sobrepeso acompañado de retención de líquidos. La masticación de la raíz combate la halitosis y reafirma las encías sangrantes.

El aceite esencial no debe ser empleado durante el embarazo, la lactancia, ni en niños menores de dos años, es tóxico sobre el sistema nervioso central. También se considera carcinogénico. Se recomiendan tratamientos discontinuos.

"Acorus calamus" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 1: 324. 1753.
Acorus: nombre genérico latino que deriva del griego antiguo άχόρου (áchórou) de Dioscórides (con diferentes versiones del texto que tienen diferentes grafías). La palabra άχόρου en sí se cree que ha derivado de la palabra κόρη ("kori"), que significa pupila (del ojo), debido a que el jugo de la raíz de la planta se utiliza como remedio en enfermedades oculares.

calamus: epíteto latino (que significa "caña") que deriva del griego ΚΆΛΑΜΟΣ (kálamos, que significa "caña"), que es afín al término del latín "culmus" (que significa "acechar") y se derivan del proto-indoeuropeo "kole-mó-" (se cree que significa "hierba" o "caña"). La palabra árabe قلم ("qalam", que significa "pluma") y el sánscrito कलम ("kalama", que significa "caña utilizada como una pluma", y una especie de arroz) se cree que han sido tomadas del griego.




</doc>
<doc id="15601" url="https://es.wikipedia.org/wiki?curid=15601" title="Allium ascalonicum">
Allium ascalonicum

Allium ascalonicum, nombre científico de la chalota, echalote, carlota, chalote o escaloña, es una verdura de la familia de las aliáceas, originaria de Asia Central.

Como la mayoría de las plantas de este género, el echalote, también llamado escalonia, se cultiva a efectos culinarios. La parte comestible de esta planta está en la base de las hojas, que forma bulbos, aovados de forma y sabor entre el ajo y la cebolla. Resulta ideal para las salsas de carne y es aceptada desde hace años por la cocina francesa.

El chalote es un pariente de la cebolla, el sabor parece un poco como a cebolla, pero tiene un dulce y suave sabor. Tiende a ser más caro que la cebolla, especialmente en los Estados Unidos, sin embargo, se puede almacenar durante al menos 6 meses.

A diferencia de las cebollas, donde normalmente cada planta constituye un único bulbo, los chalotes forman grupos de bulbos, y no en la forma de ajo.

Similar a las cebollas, los chalotes crudos liberan productos químicos que irritan los ojos cuando se cortan en rodajas, lo que resulta lacrimógeno.

Indicaciones: es estomacal, nutritivo, digestivo, diurético. Se usa el bulbo.

"Allium ascalonicum" fue descrita por Carlos Linneo y publicado en "Flora Palaestina 17", en el año 1756.

Allium: nombre genérico muy antiguo. Las plantas de este género eran conocidas tanto por los romanos como por los griegos. Sin embargo, parece que el término tiene un origen celta y significa "quemar", en referencia al fuerte olor acre de la planta. Uno de los primeros en utilizar este nombre para fines botánicos fue el naturalista francés Joseph Pitton de Tournefort (1656-1708).

ascalonicum: epíteto geográfico que procede de Ascalón, ciudad de Israel donde se cultivaba.






</doc>
