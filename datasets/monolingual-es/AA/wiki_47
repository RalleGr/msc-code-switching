<doc id="8870" url="https://es.wikipedia.org/wiki?curid=8870" title="Estequiometría">
Estequiometría

En química, la estequiometría (del griego "στοιχειον", "stoikheion", 'elemento' y "μετρον", "métrón", 'medida') es el cálculo de las relaciones cuantitativas entre los reactivos y productos en el transcurso de una reacción química.

Estas relaciones se pueden deducir a partir de la teoría atómica, aunque históricamente no se enunciaron sin hacer referencia a la composición de la materia, según distintas leyes y principios.

El primero que enunció los principios de la estequiometría fue Jeremias Benjamin Richter (1762-1807), en 1792, quien describió la estequiometría de la siguiente manera:

También estudia la proporción de los distintos elementos en un compuesto químico y la composición de mezclas químicas.

Algunos ejemplos son :

2g de H²,reaccionan con 32g de O² para dar 18g de H²O

2 moles de H²,reaccionan con 1 mol de O² para dar 2 moles de H²O

4g de H²,reaccionan con 22.41 en O² en CN para dar 18g de H²O.

Una reacción química se produce cuando hay una modificación en la identidad química de las sustancias intervinientes; esto significa que no es posible identificar a las mismas sustancias antes y después de producirse la reacción química, los reactivos se consumen para dar lugar a los productos.

A escala microscópica una reacción química se produce por la colisión de las partículas que intervienen ya sean moléculas, átomos o iones, aunque puede producirse también por el choque de algunos átomos o moléculas con otros tipos de partículas, tales como electrones o fotones. Este choque provoca que las uniones que existían previamente entre los átomos se rompan y facilite que se formen nuevas uniones, es decir que, a escala atómica, es un reordenamiento de los enlaces entre los átomos que intervienen. Este re ordenamiento se produce por desplazamientos de electrones: unos enlaces se rompen y otros se forman, sin embargo los átomos implicados no desaparecen, ni se crean nuevos átomos. Esto es lo que se conoce como "ley de conservación de la masa", e implica los dos principios siguientes:


En el transcurso de las reacciones químicas las partículas subatómicas tampoco desaparecen, el número total de protones, neutrones y electrones permanece constante. Y como los protones tienen carga positiva y los electrones tienen carga negativa, la suma total de cargas no se modifica. Esto es especialmente importante tenerlo en cuenta para el caso de los electrones, ya que es posible que durante el transcurso de una reacción química salten de un átomo a otro o de una molécula a otra, pero el número total de electrones permanece constante. Esto que es una consecuencia natural de la ley de conservación de la masa se denomina ley de conservación de la carga e implica que:


Las relaciones entre las cantidades de reactivos consumidos y productos formados dependen directamente de estas leyes de conservación, y por lo tanto pueden ser determinadas por una ecuación (igualdad matemática) que las describa. A esta igualdad se le llama ecuación estequiométrica.

Una ecuación química es una representación escrita de una reacción química. Se basa en el uso de símbolos químicos que identifican a los átomos que intervienen y como se encuentran agrupados antes y después de la reacción. Cada grupo de átomos se encuentra separado por símbolos (+) y representa a las moléculas que participan, cuenta además con una serie de números que indican la cantidad de átomos de cada tipo que las forman y la cantidad de moléculas que intervienen, y con una flecha que indica la situación inicial y la final de la reacción. Así por ejemplo en la reacción:

Tenemos los grupos de átomos (moléculas) siguientes:


Los subíndices indican la atomicidad, es decir la cantidad de átomos de cada tipo que forman cada agrupación de átomos (molécula). Así el primer grupo arriba representado, indica a una molécula que está formada por 2 átomos de oxígeno, el segundo a dos moléculas formadas por 2 átomos de hidrógeno, y el tercero representa a un grupo de dos moléculas formadas cada una por 2 átomos de hidrógeno y uno de oxígeno, es decir dos moléculas de agua.

Es el número de moléculas de un determinado tipo que participa en una ecuación química dada en el orden en el que está escrita. En el siguiente ejemplo:

El coeficiente del metano es 1, el del oxígeno 2, el del dióxido de carbono 1 y el del agua 2. Los coeficientes estequiométricos son en principio números enteros, aunque para ajustar ciertas reacciones alguna vez se emplean números fraccionarios.

Cuando el coeficiente estequiométrico es igual a 1, no se escribe. Por eso, en el ejemplo <chem>CH4</chem> y <chem>CO2</chem> no llevan ningún coeficiente delante.

Así por ejemplo


Debe leerse como 1(O) es decir, un grupo de moléculas de oxígeno. Y la expresión:


Debe leerse como 2(HO), es decir dos grupos o moléculas, cada uno de los cuales se encuentra formado por dos átomos de hidrógeno y uno de oxígeno.

Dado que una ecuación química es una representación simplificada o mínima de una reacción química, es importante considerar todos los datos representados; ya que perder de vista a alguno significa no entender realmente la situación representada. Los símbolos y subíndices representan a las especies químicas que participan, y los coeficientes representan al número de moléculas de cada tipo que se encuentran participando de la reacción.

Finalmente la flecha indica cual es el sentido predominante en el cual la reacción química progresa. Así en el ejemplo anterior vemos que CH y O se encuentran en la situación "antes de", es decir del lado de los reactivos y HO y CO se encuentran en la situación de "después de", es decir del lado de los productos. La ecuación completa debería leerse así:

Se dice que una ecuación química se encuentra ajustada, equilibrada o balanceada cuando respeta la ley de conservación de la materia, según la cual la cantidad de átomos de cada elemento debe ser igual del lado de los reactivos (antes de la flecha) y en lado de los productos de la reacción (después de la flecha). 

Para balancear una ecuación, se deben ajustar los coeficientes, y no los subíndices. Esto es así porque cada tipo de molécula tiene siempre la misma composición, es decir se encuentra siempre formada por la misma cantidad de átomos, si modificamos los subíndices estamos nombrando a sustancias diferentes:

HO es agua común y corriente, pero HO es peróxido de hidrógeno una sustancia química totalmente diferente. Al modificar los coeficientes sólo estamos diciendo que ponemos más o menos de tal o cual sustancia.

Por ejemplo, en la reacción de combustión de metano (CH), éste se combina con oxígeno molecular (O) del aire para formar dióxido de carbono (CO) y agua. (HO). La reacción sin ajustar será:

En esta ecuación, las incógnitas son "a", "b", "c" y "d", que son los denominados coeficientes estequiométricos. Para calcularlos, debe tenerse en cuenta la ley de conservación de la materia, por lo que la suma de los átomos de cada elemento debe ser igual en los reactivos y en los productos de la reacción. Existen tres métodos principales para balancear una ecuación estequiométrica, que son, el método de tanteo, el método algebraico y el método de ion-electrón para ecuaciones de tipo redox.

El método de tanteo se basa simplemente en modificar los coeficientes de uno y otro lado de la ecuación hasta que se cumplan las condiciones de balance de masa. No es un método rígido, aunque tiene una serie de delineamientos principales que pueden facilitar el encontrar rápidamente la condición de igualdad.


En el ejemplo, se puede observar que el elemento que participa con un estado de oxidación de mayor valor absoluto es el carbono que actúa con estado de oxidación (+4), mientras el oxígeno lo hace con estado de oxidación (-2) y el hidrógeno con (+1).

Comenzando con el carbono, se iguala de la forma más sencilla posible, es decir con coeficiente 1 a cada lado de la ecuación, y de ser necesario luego se corrige.

Se continúa igualando el oxígeno, se puede observar que a la derecha de la ecuación, así como está planteada, hay 3 átomos de oxígeno, mientras que a la izquierda hay una molécula que contiene dos átomos de oxígeno. Como no se deben tocar los subíndices para ajustar una ecuación, simplemente añadimos media molécula más de oxígeno a la izquierda:

O lo que es lo mismo:

Luego se iguala el hidrógeno. A la izquierda de la ecuación hay cuatro átomos de hidrógeno, mientras que a la derecha hay dos. Se añade un coeficiente 2 frente a la molécula de agua para balancear el hidrógeno:

El hidrógeno queda balanceado, sin embargo ahora se puede observar que a la izquierda de la ecuación hay 3 átomos de oxígeno (3/2 de molécula) mientras que a la derecha hay 4 átomos de oxígeno (2 en el óxido de carbono (II) y 2 en las moléculas de agua). Se balancea nuevamente el oxígeno agregando un átomo más (1/2 molécula más) a la izquierda:

O lo que es lo mismo:

Ahora la ecuación queda perfectamente balanceada. El método de tanteo es útil para balancear rápidamente ecuaciones sencillas, sin embargo se torna sumamente engorroso para balancear ecuaciones en las cuales hay más de tres o cuatro elementos que cambian sus estados de oxidación. En esos casos resulta más sencillo aplicar otros métodos de balanceo.

El método algebraico se basa en el planteamiento de un sistema de ecuaciones en la cual los coeficientes estequiométricos participan como incógnitas, procediendo luego despejar estas incógnitas. Es posible sin embargo que muchas veces queden planteados sistemas de ecuaciones con más incógnitas que ecuaciones, en esos casos la solución se halla igualando cualquiera de los coeficientes a 1 y luego despejando el resto en relación a él. Finalmente se multiplican todos los coeficientes por un número de modo tal de encontrar la menor relación posible entre coeficientes enteros.

En el ejemplo:

para el elemento hidrógeno (H) hay 4·a átomos en los reactivos y 2·d átomos en los productos. De esta manera se puede plantear una condición de igualdad para el hidrógeno:

Y procediendo de la misma forma para el resto de los elementos participantes se obtiene un sistema de ecuaciones:

Con lo que tenemos un sistema lineal de tres ecuaciones con cuatro incógnitas homogéneo:

Al ser un sistema homogéneo tenemos la solución trivial:

Pero debemos buscar una solución que no sea trivial, ya que esta implicaría que no hay "ningún" átomo, y no describe el planteo químico, proseguimos a simplificar:

Si, la tercera ecuación, la cambiamos de signo, la multiplicamos por dos y le sumamos la primera tendremos:

Pasando d al segundo miembro, tenemos:

Con lo que tenemos el sistema resuelto en función de d:

Se trata en encontrar el menor valor de d que garantice que todos los coeficientes sean números enteros, en este caso haciendo d= 2, tendremos:

Sustituyendo los coeficientes estequimétricos en la ecuación de la reacción, se obtiene la ecuación ajustada de la reacción:

Ésta dice que 1 molécula de metano reacciona con 2 moléculas de oxígeno para dar 1 molécula de dióxido de carbono y 2 moléculas de agua.

Al fijar arbitrariamente un coeficiente e ir deduciendo los demás pueden obtenerse valores racionales no enteros. En este caso, se multiplican todos los coeficientes por el mínimo común múltiplo de los denominadores. En reacciones más complejas, como es el caso de las reacciones redox, se emplea el método del ion-electrón.

Las reacciones electroquímicas se pueden balancear por el método ion-electrón donde la reacción global se divide en dos "semirreacciones" (una de oxidación y otra de reducción), se efectúa el balance de carga y elemento, agregando H, OH, HO y/o electrones para compensar los cambios de oxidación.
Antes de empezar a balancear se tiene que determinar en que medio ocurre la reacción, debido a que se procede de una manera en particular para cada medio.

Se explicará por medio de un ejemplo, cuando manganésica reacciona con bismutato de sodio.

También se explicará por medio de un ejemplo, cuando el permanganato de potasio reacciona con el sulfito de sodio.

Ecuación balanceada:

Cuando los reactivos de una reacción están en cantidades proporcionales a sus coeficientes estequiométricos se dice:


Las tres expresiones tienen el mismo significado.

En estas condiciones, si la reacción es completa, todos los reactivos se consumirán dando las cantidades estequiométricas de productos correspondientes.

Si no en esta forma, existirá el reactivo limitante que es el que está en menor proporción y que con base en él se trabajan todos los cálculos.

Ejemplo

La ecuación química que representa la reacción química es:
Se tienen las siguientes equivalencias a partir de la reacción química y las masas atómicas citadas:

Esta última relación es consecuencia de la fórmula química del oxígeno molecular (formula_12)

Entonces para determinar la masa de oxígeno podemos realizar los siguientes "pasos": determinamos las moles de átomos de carbono (primer factor), con estas moles fácilmente determinamos las moles de moléculas de oxígeno (segundo factor a partir de coeficientes de la ecuación química), y finalmente obtenemos la masa de oxígeno (tercer factor)

realizadas las operaciones:

Los cálculos estequiométricos se basan en las relaciones fijas de combinación que hay entre las sustancias en las reacciones químicas balanceadas. Estas relaciones están indicadas por los subíndices numéricos que aparecen en las fórmulas y por los coeficientes. Este tipo de cálculos es muy importante y se utilizan de manera rutinaria en el análisis químico y durante la producción de las sustancias químicas en la industria. 
Los cálculos estequiométricos requieren una unidad química que relacione las masas de los reactantes con las masas de los productos. Esta unidad química es el mol.

Ejemplo de la vida diaria.
La estequiometria la podemos usar por ejemplo cuando vamos al médico porque tenemos un dolor ocasionado por una infección, el doctor debe de sacar la cuenta de nuestro peso con los gramos que contiene el medicamento y sobre la base de esto sacar la medida exacta para saber cuántas pastillas o cuantos mililitros nos tenemos que tomar de dichos medicamentos.




</doc>
<doc id="8873" url="https://es.wikipedia.org/wiki?curid=8873" title="Coregonus lavaretus">
Coregonus lavaretus

La farra o lavareto es un pez de agua dulce, parecido al salmón, perteneciente al género Coregonus, que vive principalmente en los lagos alpinos y en la Europa septentrional. Tiene la cabeza pequeña y aguda, la boca pequeña, la lengua corta, el lomo verdoso y el vientre plateado. Su carne es muy apreciada.

Al no ser una especie presente en España (ni en otros países de lengua española) pueden producirse confusiones con los términos comunes utilizados para la designación de Coregonus lavaretus.

En español, el término farra se ha utilizado más bien para los especímenes de los lagos alpinos (así se encuentra en los diccionarios generalistas, como el Diccionario de la lengua española de la Real Academia o en el María Moliner). El término lavareto se utiliza principalmente para las poblaciones halladas en las aguas de Escandinavia (sobre todo en las traducciones especializadas de la Unión Europea; pero en el Diccionario de la Real Academia Española esta palabra no está registrada). No hay razones científicas para distinguir esas dos poblaciones como especies diferentes; se puede concluir que la farra es un conespecífico del lavareto. 

Dentro de la dificultad de identificar distintas especies dentro del género Coregonus, es posible que haya otras poblaciones de Coregonus conespecíficas de la farra y lavareto.

Por ejemplo, posiblemente es un conespecífico el «corégono» - "Coregonus clupeaformis" Mitchill, 1818-; a menudo hay disputas sobre la correcta clasificación de ambos.

Sin embargo, en los últimos años,se ha registrado una tendencia científica a distinguir como especies diferentes varias poblaciones de Coregonus que anteriormente se consideraban como miembros de una misma especie; esto podría afectar a las designaciones mencionadas.

Los machos pueden alcanzar los 73 cm de longitud total y los 10 kg de peso.

Desovan durante la noche.

Comen crustáceos planctónicos y bentónicos.

Es depredado por el "Coregonus peled" –en Rusia–, el lucio europeo (Esox lucius) -en Inglaterra y Gales-, el lucioperca (Sander lucioperca)" -"en Finlandia- y la" trucha común" (Salmo trutta) -en Finlandia-". "

Vive en áreas de clima templado (entre 4-16 °C).

Se encuentra en Europa y se ha introducido en Irán (1965-1967).


</doc>
<doc id="8874" url="https://es.wikipedia.org/wiki?curid=8874" title="Sarah Bernhardt">
Sarah Bernhardt

Sarah Bernhardt (París, 23 de octubre de 1844-Ib., 26 de marzo de 1923) fue una actriz de teatro y cine francesa.

Sarah Bernhardt nació el 23 de octubre de 1844 en el número 5 de la calle de l'École-de-Médecine, en París. Su nombre real era Rosine Bernardt. Su madre era una mujer de religión judía de origen neerlandés llamada Judith-Julie Bernardt (1821-1876), alias Youle. Se ganaba la vida como cortesana junto con su hermana Rosine. Julie tuvo varias hijas más. En abril de 1843 tuvo dos niñas gemelas que fallecieron a las dos semanas. Tras Sarah, tuvo a Jeanne (fecha de nacimiento desconocida) y a Régine en 1855, que murió de tuberculosis en 1873. Todas fueron hijas de padres distintos y desconocidos. Sarah Bernhardt nunca supo quién era su padre biológico, aunque se cree que era el duque de Morny, medio hermano de Napoleón III.

Sarah pasó los primeros cuatro años de su vida en Bretaña al cuidado de un ama de cría. La primera lengua que Sarah aprendió fue el bretón y por esta razón, al iniciar su carrera teatral, adoptó la forma bretona de su apellido, «Bernhardt». En esta época sufrió un accidente que muchos años después le acarrearía graves problemas de salud. Cayó de una ventana, rompiéndose la rodilla derecha. Aunque sanó sin problemas, la rodilla le quedó delicada para siempre, y en 1914, a causa de una dolorosa inflamación de esa misma rodilla, tuvieron que amputarle la pierna derecha. Tras el accidente, su madre la llevó consigo a París, donde permaneció dos años. A punto de cumplir siete años ingresó en la Institución Fressard, un internado para señoritas próximo a Auteuil. Permaneció allí dos años. En 1853 entró en el colegio conventual Grandchamp, cercano a Versalles. En este colegio participó en su primera obra teatral, "Tobías recupera la vista", escrita por una de las monjas. También aquí fue bautizada e hizo la primera comunión. El ambiente místico del colegio le hizo plantearse el hacerse monja.

Tras abandonar Grandchamp a los 15 años, su madre trató de introducirla en el mundo galante para que se ganara la vida como cortesana. Pero Sarah, influenciada por su educación conventual, se negó repetidamente a ello. Julie Bernard tenía un salón en su piso parisiense donde se reunían sus clientes. Entre ellos estaba el medio hermano de Napoleón III, el duque de Morny. Morny aconsejó que Sarah se inscribiera en el Conservatorio de música y declamación. Gracias a los contactos del duque, Sarah entró sin dificultad en 1859. En 1861 ganó un segundo premio en tragedia y una mención honorífica en comedia. 

Finalizados sus estudios en el Conservatorio, entró, de nuevo gracias a los influyentes contactos de Morny, en la Comédie-Française. Debutó el 11 de agosto de 1862 con la obra "Iphigénie," de Jean Racine. Su fuerte carácter le atrajo problemas con sus compañeros, lo que provocó que abandonara la Comédie por primera vez en 1863. Tres semanas más tarde fue contratada por el Teatro Gymnase, donde hizo siete pequeños papeles en distintas obras. Actuó por última vez el 7 de abril de 1864 con la obra "Un mari qui lance sa femme". 

Ese mismo año conoció a uno de los grandes amores de su vida, Charles-Joseph Lamoral, príncipe de Ligne. Inició una apasionada relación con él, hasta que quedó embarazada y el príncipe la abandonó. El 22 de diciembre de 1864 dio a luz a su único hijo, Maurice Bernhardt. Sin oficio y habiendo fracasado momentáneamente en el mundo del teatro, siguió los pasos de su madre, convirtiéndose en cortesana de lujo. Sarah no abandonó su actividad como cortesana hasta que su carrera teatral se hubo afianzado con éxito y pudo mantenerse sólo con el trabajo que le reportaba el teatro.

Tres años más tarde, en 1867 debutó en el Teatro del Odéon con "Las mujeres sabias" ("Les femmes savantes") de Molière. Ahí empezó su verdadera carrera profesional. Participó en muchos montajes teatrales, alternando la vida teatral con la vida galante. La fama le llegó repentinamente en 1869 con "Le Passant," de François Coppée, una obra en verso de un solo acto. Sarah, además, hizo por primera vez en esta obra un papel masculino, el del trovador Zanetto. Repetiría más veces haciendo de hombre en varias obras más ("Lorenzaccio", "Hamlet" y "L'Aiglon").

En 1870, durante la guerra franco-prusiana, habilitó el Odeón como hospital para convalecientes, donde cuidó con dedicación a los heridos de guerra. En 1871 el improvisado hospital tuvo que ser cerrado por problemas de salubridad.
Tras la derrota francesa y la caída de Napoleón III, muchos intelectuales, exiliados por estar en contra del emperador, pudieron regresar a Francia, entre ellos Victor Hugo. El regreso de Hugo fue trascendental en la vida de Bernhardt, ya que el escritor la eligió para protagonizar el reestreno de su obra "Ruy Blas". Bernhardt además protagonizó otra obra de Hugo, "Hernani". "Ruy Blas" la encumbró a cotas de éxito inimaginables. Regresó a la "Comédie-Française" como una gran estrella y allí afianzó su repertorio y sus múltiples registros como actriz.

El estilo de actuación de Bernhardt se basaba en la naturalidad. Detestaba profundamente las viejas normas del teatro francés, donde los actores declamaban histriónicamente y hacían gestos exagerados. Rompió con todo lo establecido, profundizando en la psicología de los personajes. Estudiaba cada gesto y cada entonación del texto que debía decir, buscando la perfección natural sin que se notara ningún tipo de artificio. Destaca en su arte que, representando siempre a grandes heroínas de tragedia o reinas, huyó de la sobreactuación y de la afectación. Son famosas sus escenas de muerte, en las que en vez de, según sus propias palabras, «ofrecer toda una retahíla de patologías"»" tales como estertores, toses, gemidos agónicos, profundizaba en el acto de morir desde el punto de vista psicológico y sentimental.

Aparte de su profesión de actriz, se interesó por la escultura y la pintura, llegando a exponer varias veces en el Salón de París entre los años 1874 y 1896. Recibió distintos premios y menciones honoríficas en ambas disciplinas. Escribió también tres libros: su autobiografía titulada "Ma double vie, Petite Idole" y "L´art du Théâtre: la voix, la geste, la pronontiation."

Bernhardt se especializó en representar las obras en verso de Jean Racine, tales como "Iphigénie", "Phédre" o "Andromaque". Destacó especialmente, entre muchas otras, en "La Dame aux camélias", de Dumas hijo, "Théodora", de Sardou, "L'Aiglon", de Edmond Rostand, "Izéïl", de Silvestre y Morand, "Macbeth", de Shakespeare y "Jeanne D'Arc," de Jules Barbier.

En 1879 realizó su primera salida de Francia, concretamente a Inglaterra, donde estuvo seis semanas haciendo dos representaciones diarias y obtuvo un éxito rotundo. Al llegar al país fue recibida espectacularmente, lo que indica que su fama había cruzado las fronteras de Francia. En esta primera visita conoció a un joven escritor llamado Oscar Wilde. Años más tarde, en 1893, Bernhardt aceptaría representar su obra "Salomé". Ese mismo año, Sarah fue ascendida a Socio Pleno de la Comédie-Française. Los Socios Plenos son la jerarquía más alta de esta institución.

Tras su espectacular éxito en Inglaterra decidió hacer su primera gira americana. Partió a los Estados Unidos el 15 de octubre de 1880. El éxito fue total. Bernhardt haría repetidas giras por los Estados Unidos (sus famosas «giras de despedida») y también recorrió toda América del Sur, llegando a actuar en Brasil, Perú, Cuba, Argentina, Chile... Viajaba en tren y en barco y llegó a cruzar el cabo de Hornos. En Estados Unidos su éxito era tal que le habilitaron un tren con siete vagones de lujo llamado "Sarah Bernhardt Special", que era de uso exclusivo de la actriz. Sus giras le llevaron a Australia y visitó las islas Hawái y las islas Sandwich. Actuó en Egipto y en Turquía. Asimismo recorrió Europa, actuando en Moscú, Berlín, Bucarest, Roma, Atenas. En su periplo, actuó no solo en grandes teatros, sino también en teatros de ínfima categoría. 

Bernhardt tuvo una agitada vida sentimental, en la que destacan nombres como Louise Abbèma, Gustave Doré, Victor Hugo, Jean Mounet-Sully, Jean Richepin, Philippe Garnier, Gabriele D'Annunzio, Eduardo, Príncipe de Gales, entre otros. Se casó una sola vez, con un oficial griego llamado Jacques Aristidis Damala. Damala era hijo de un rico armador y era adicto a la morfina. Nació en El Pireo en 1842. Bernhardt se casó con él el 4 de abril de 1882 y fue un matrimonio tempestuoso. Sarah intentó convertir en actor a Damala, pero fracasó. La actriz le impartió clases de actuación y le dio el papel de Armand Duval en "La Dame aux Camélias". Se eran infieles mutuamente, y un día Damala, abrumado por el éxito de su mujer, por las constantes burlas de los actores de la compañía de Bernhardt y la mala relación con Maurice Bernhardt, se alistó en la Legión, siendo destinado a Argelia. Meses más tarde regresó con Sarah. Las separaciones y reconciliaciones fueron continuas hasta que Sarah decidió irse de gira por todo el continente americano en 1887 y Damala ya no la acompañó. Era la separación definitiva. Permanecieron casados hasta la muerte de Damala por los efectos secundarios del abuso continuado de morfina, en 1889, a la edad de 42 años. Bernhardt lo enterró en Atenas y adornó la tumba con un busto tallado por ella misma.

Sarah Bernhardt fue también la primera actriz empresaria del mundo del espectáculo. A raíz de una relación muy tensa con el director de la Comédie-Française, Perrin, Bernhardt rompió su contrato y dimitió como Socio Pleno el 18 de marzo de 1880. La Comédie pleiteó contra ella, ganando el juicio. Sarah Bernhardt tuvo que renunciar a su pensión de 43 000 francos que habría tenido de pensión si hubiese permanecido un mínimo de veinte años en la Comédie, y además se la condenó a 100000 francos de multa, que nunca llegó a pagar. Tras su esplendorosa primera gira americana, que le había hecho ganar una gran fortuna, Bernhardt arrendó el teatro Porte-Saint-Martin en 1883. En este teatro produjo y actuó en obras como "Frou-Frou" y "La Dame aux camélias", entre otras. Durante sus giras, el teatro permanecía abierto y se estrenaban obras continuamente con distinto éxito comercial. Bernhardt no dudaba en apoyar el teatro de vanguardia, así que, además del repertorio clásico, en el Porte-Saint-Martin se estrenaban obras de nuevos autores que rompían con el teatro tradicional. Tras unos años, Bernhardt arrendó el Théatre de la Renaissance, donde representó muchas obras de éxito. En 1899 alquiló por veinticinco años el enorme Theâtre des Nations, único teatro donde actuaría en Francia durante los últimos veinticuatro años de su vida.

Su vida familiar no fue sencilla. Tuvo una relación tensa y distante con su madre, Julie. Su progenitora nunca fue una madre cariñosa e interesada, y esto hizo que Sarah siempre buscase su aprobación y su cariño. Julie Bernard sentía predilección tan solo por su hija Jeanne y descuidó totalmente la educación de su hija menor, Régine. Sarah Bernhardt sentía predilección por su hermana pequeña Régine, y cuando logró ser independiente, se la llevó a vivir consigo para alejarla de la madre y de las intenciones de esta de convertirla también en cortesana. Lamentablemente, a causa del abandono afectivo que sufrió y del ambiente del piso de su madre, Régine se prostituyó a los trece años. Falleció a los dieciocho, en 1873, de tuberculosis. Su otra hermana, Jeanne, también fue cortesana durante una época y siempre que tenía necesidad de dinero. Para apartarla de la mala vida, Bernhardt se la llevó consigo con su compañía y la acompañó en varias de sus giras por Estados Unidos y Europa. Era una actriz mediocre, pero hacía pequeños papeles y vivía una vida de lujo junto a su hermana. Se sabe que sufrió crisis de neurosis a causa de su adicción a la morfina y que estuvo ingresada en el hospital de La Pitié-Salpetrière en París, al cuidado del doctor Jean-Martin Charcot. En cambio, el hijo de Sarah, Maurice, siempre estuvo muy unido a su madre. Vivió siempre a su sombra, malgastando auténticas fortunas en el juego, en viajes y en una vida regalada.
El siglo XX empezó con un gran éxito, "L'Aiglon", de Edmond Rostand. La obra fue estrenada el 15 de marzo de 1900 y obtuvo un éxito sin precedentes. Sarah hizo 250 representaciones de "L'Aiglon" y, tras esto, hizo otra gira a Estados Unidos para representarla. En Nueva York representó la obra en el Metropolitan Opera House y cosechó un enorme éxito. Probó suerte también con el recién nacido cine. En 1900 filmó "Le Duel d'Hamlet", haciendo ella de Hamlet. En 1906 rodó "La Dame aux camélias", con Lou Tellegen, su amante de aquel momento, haciendo de Armand Duval. Bernhardt, cuando la vio, se horrorizó y mandó destruir el negativo, que afortunadamente todavía existe. Rodó también "Elisabeth, reine d'Anglaterre", dirigida por Louis Mercanton. En 1913 filmó "Jeanne Doré", dirigida por Tristan Bernard. Esta película se considera la mejor rodada por Bernhardt y donde se puede observar mejor su arte interpretativo. La película se conserva en la Cinématèque de Paris.

En 1914 le fue concedida la Legión de Honor. En 1915 la rodilla derecha, la misma que se había fracturado de niña, había llegado a provocarle dolores insoportables. Para colmo, durante una de sus interpretaciones de la obra dramática "Tosca" —la misma que Puccini hizo triunfar en el género operístico—, en la última escena, cuando la heroína se lanza desde un barranco, no se tomaron las medidas de seguridad pertinentes; Sarah se lanzó y se hirió la pierna. Aunque hacía ya varios años que padecía molestias constantes, durante el año 1914 fue empeorando, hasta que no hubo otro remedio que amputar en febrero de 1915. Una vez recuperada de la amputación, y ya empezada la Primera Guerra Mundial, la actriz decidió hacer una gira tras las trincheras francesas haciendo actuaciones para animar a las tropas. Organizó varias giras con su compañía y recorrió toda Francia. Aun con la pierna amputada, Sarah Bernhardt siguió actuando. Recitaba monólogos, poemas o representaba actos famosos de su repertorio de obras en las que no debía estar de pie. Siguió también participando en películas tras la guerra. Su salud fue empeorando hasta sufrir un grave ataque de uremia que estuvo a punto de matarla. En 1922 vendió su mansión en el campo de Belle-Île-en-Mer, donde había rodado años atrás una película documental sobre su vida. Cuando le llegó la muerte estaba rodando una película, "La Voyante". El rodaje se estaba realizando en su casa, en el Boulevard Péreire, puesto que la actriz estaba ya muy delicada de salud. El 15 de marzo de 1923, tras rodar una escena, quedó totalmente agotada, hasta que se desmayó. Nunca se recuperó. Once días más tarde, el 23 de marzo, fallecía en brazos de su hijo Maurice.

Su entierro fue multitudinario. Unos 150 000 franceses acudieron a despedirla. Fue inhumada en el cementerio parisino del Père-Lachaise.

A pesar de ser llamada «la divina Sarah» por su carácter excéntrico y caprichoso, Sarah Bernhardt trabajó en innumerables proyectos teatrales demostrando un carácter perseverante, una gran profesionalidad y dedicación a su arte.



Carmen Verlichak, "Las diosas de la Belle Époque y de los 'años locos"', Editorial Atlántida, Buenos Aires, 1996 (ISBN 950-08-1599-0)




</doc>
<doc id="8875" url="https://es.wikipedia.org/wiki?curid=8875" title="Alejandro Dolina">
Alejandro Dolina

Alejandro Ricardo Dolina (Morse, 20 de mayo de 1944) es un escritor, músico, conductor de radio y de televisión y actor argentino. Realizó estudios de derecho, música, letras e historia. Es conocido dentro y fuera de su país por sus obras literarias y su clásico programa radial "La venganza será terrible".

Dolina nació en Morse, cerca de Baigorrita, en la provincia de Buenos Aires, y pasó su primera infancia en la localidad bonaerense de Caseros. Su madre, Delfa Virginia Colombo (1922-1994), era maestra. Su padre era contador, ejecutivo de Plavinil Argentina.

Estudió música y literatura desde la juventud. Tuvo diversos empleos. Se sabe que fue operario de ENTEL y estudiante de Derecho.

A los 22 años abandonó la carrera de Derecho y estuvo desempleado. En una fiesta conoció a Manuel Evequoz quien, interesado por la fina inteligencia y el humor de Dolina, trabó amistad y le consiguió trabajo en una agencia publicitaria. Esto significó su introducción en los medios de comunicación y el descubrimiento de su vocación en el ambiente. Dolina fue un gran amigo de Evequoz y en él está inspirado su personaje Manuel Mandeb. Evequoz pertenecía a Montoneros y desapareció durante la dictadura de 1976. El personaje fue creado mientras Evequoz vivía. No obstante, sus textos serían publicados en la década siguiente.

Desde su juventud fue aficionado al tango, a la filosofía y la literatura. La mujer tiene un rol fundamental en su discurso y aun en sus motivaciones, cuando afirma que «todo lo que hago lo hago para levantar minas». Esa cita es erróneamente atribuida a Dolina, ya que en verdad pertenece al humorista Caloi, que lo puso en boca del personaje Alexis Dolinades, inspirado en él. Dolina retoma esta afirmación en su obra "Lo que me costó el amor de Laura" (1998): «Se ha dicho que el hombre hace todo lo que hace con el único fin de enamorar mujeres».

A principios de la década de 1970, Dolina inició su carrera en publicidad y escribió artículos para "Satiricón", una revista que, por medio del humor, comentaba temas de la política, sociedad y estilo de vida del momento. Durante este período trabajó con Carlos Trillo, quien también se dedicaba a la publicidad y se convertiría luego en un exitoso guionista de historietas.

En 1978, después de que la revista "Satiricón" fuese clausurada por la Junta Militar que gobernaba el país, Dolina comenzó a escribir para la revista "Humor". Durante esos años, Dolina se dedicó a escribir sobre el honor, el amor, la amistad y hasta creó cierta mitología centrada en personajes como el Ángel Gris de Flores, el escritor ficticio Manuel Mandeb y otros. Esas historias fueron publicadas en el libro "Crónicas del Ángel Gris" en 1987 y más tarde transformadas en un musical. Estos personajes aparecerían en todos sus libros posteriores.

En 2011 fue una de las voces de las piezas publicitarias del Banco Provincia.

En 1975 hizo sus primeras participaciones radiales en "Mañanitas nocturnas", programa de Carlos Ulanovsky y Mario Mactas que se transmitió por Radio Argentina. Interpretaba a un periodista llamado Gómez. Allí apareció por primera vez el personaje del Sordo Gancé, un músico improvisado, presente hasta hoy en las emisiones de "La venganza será terrible".

El 2 de abril de 1985 Dolina debutó en radio al conducir un programa que se emitía por Radio El Mundo, "Demasiado tarde para lágrimas", junto a Adolfo Castelo.
Bajo el mismo nombre, el programa se trasladó en 1989 a Radio Rivadavia y, brevemente, durante 1991 (apenas un mes) a LRA Radio Nacional. Luego pasó a la radio Viva FM, cambiando su nombre por "El ombligo del mundo". Durante 1993 continuó en FM Tango bautizado, por motivos contractuales, como "La venganza será terrible", llegando a Radio Continental (1994-2000 y 2002-2006), y Radio Del Plata, donde se transmitió solamente durante 2001 mientras, a la misma hora, Radio Continental emitía programas grabados de temporadas anteriores. A finales de 2006, el programa se trasladó a Radio 10, donde permanecería hasta fines de 2009. Desde febrero de 2010 a diciembre de 2011 se emitió por LRA Radio Nacional, con Patricio Barton todas las noches, y Gabriel Schultz y Jorge Dorio en forma alternada. A partir de enero de 2012 el programa se emite, ya sin Schultz, por Radio del Plata en dúplex con 360 TV. Por su trabajo en este programa Dolina ganó, en 1991, el Premio Konex al mejor conductor.

Su programa de radio fue líder en su franja horaria desde el primer año de emisiones, con un encendido superior a la mitad de los radio escuchas de todo el país. Considerado ya un clásico de la radiofonía del Río de la Plata, suele hacer presentaciones en vivo del programa incluso fuera de la Ciudad de Buenos Aires. Una de las últimas funciones de 2012 se llevó a cabo en el excine de Burzaco, donde la gente del partido de Almirante Brown y de los partidos aledaños llenó el recinto y a la finalización del evento, lo ovacionó de pie. Según dice el mismo Dolina «es extraño cómo se sostiene una audiencia numéricamente tan grande en un país donde se supone que no se lee, cuando para entender mi programa al menos hay que haber hojeado dos libros». Su labor diaria es tanto una invitación a la historia y la literatura como al surrealismo. Logra hacer prosa tanto de un fragmento de la Odisea como de un decálogo de consejos para quitar mejor las manchas de la ropa. Su capacidad de improvisación como narrador, actor y músico asombra día a día. En marzo de 2013, realizó un programa especial desde el Espacio Memoria y Derechos Humanos, donde funcionara la ex Escuela de Mecánica de la Armada.

En septiembre de 2016, renuncia a Radio Del Plata y Dolina aseguró que "La venganza será terrible" volvería en otra emisora. Desde el 16 de septiembre se emite el programa por AM 750 en su horario habitual de la medianoche y a las 20, en el que se repite el programa de la medianoche anterior, denominado "La Venganza, el eterno retorno de las ocho".

Luego de "Crónicas del Ángel Gris" (1987), su libro más exitoso hasta el momento, publicó "El libro del Fantasma" (1999), "Bar del Infierno" (2005) (colecciones de cuentos) y su primera novela, "Cartas marcadas" (2012). Aborda temas históricos, filosóficos y costumbristas en torno a los Hombres Sensibles de Flores, sus personajes recurrentes. De clara influencia borgeana, alterna la literatura fantástica (historias de ángeles, demonios, metamorfosis y milagros), el ensayo («Bovarismo descendente» en "El libro..."; «El otro infierno» en "El bar...", entre otros) y el relato histórico («Elisa Brown», «Saint Germain», etc.).

Dolina es cantor y compositor. En sus programas de radio y televisión siempre incluyó segmentos musicales. En 1990 adaptó las "Crónicas" y presentó la comedia musical "El barrio del Ángel Gris". Recibió por ella el premio Argentores. En 1998, grabó su opereta "Lo que me costó el amor de Laura" junto a Mercedes Sosa, Sandro, Joan Manuel Serrat y Ernesto Sabato, entre otros. En 2002, adaptó algunos de sus viejos radioteatros y grabó "Radiocine". En 2004 editó el CD "Tangos del Bar del Infierno".

Protagonizó dos programas: "La barra de Dolina" (1989 por Canal 11, 1990 por ATC) y "Bar del Infierno" (2003, Canal 7). Además participó del programa "Fuga de cerebros" emitido hacia 1991 por el entonces canal ATC, junto a Lalo Mir, Elizabeth Vernaci y Manuel Wirzt.

Se emitió en 2011 por la señal del Canal Encuentro y luego por la Televisión Pública el documental ficticio "Recordando el show de Alejandro Molina", escrito y protagonizado por Dolina bajo de la dirección de Juan José Campanella. Esta serie de trece capítulos de media hora contó con la participación de Ale y Martín Dolina, Patricio Barton, Gillespi, Coco Silly, Gabriel Rolón, Manuel Moreira entre otros.









En 2001 obtuvo el nombramiento de Ciudadano Ilustre de la Ciudad de Buenos Aires. Asimismo, en 2003 fue declarado visitante ilustre de la ciudad de Montevideo, Uruguay.

El 11 de agosto de 2014, en la esquina de Curapaligüe y Sabattini en Caseros, se inaugura el "Paseo del Ángel Gris" en su honor.

El 18 de octubre de 2014 la Universidad Nacional de San Juan, a través de su rector y presidente del Consejo Superior, Oscar Nasisi, aprobó la otorgación del Título Doctor Honoris Causa a Alejandro Dolina.



</doc>
<doc id="8878" url="https://es.wikipedia.org/wiki?curid=8878" title="Intrón">
Intrón

Un intrón es una región del ADN que forma parte de la transcripción primaria de ARN, pero a diferencia de los exones, son eliminados del transcrito maduro, previamente a su traducción. Están presentes en todos los organismos celulares y en virus.

El número y longitud de los intrones varía enormemente entre especies, así como entre los genes de una misma especie. Por ejemplo, el pez globo, "Takifugu rubripes", tiene pocos intrones en su genoma; mientras que los mamíferos y las angiospermas (plantas con flores) suelen presentar numerosos intrones.

La palabra "intrón" se deriva del término "región intragénica", es decir una región dentro de un gen. A pesar de ser a veces llamados "secuencias interventoras" el término puede referirse a cualquiera de las muchas familias de secuencias internas de ácidos nucléicos que no están presentes en el gen final, como lo son las inteínas, las secuencias no traducidas (UTR) y los nucleótidos eliminados en la edición del ARN.

Los intrones fueron descubiertos por Phillip Allen Sharp y Richard J. Roberts, lo que les supuso ganar el en 1993. El término intrón fue introducido por el bioquímico estadounidense Walter Gilbert en 1978.

Los intrones pueden representar un sitio alternativo de splicing, pudiendo dar diferentes tipos de proteínas. El control del splicing está regulado por una amplia variedad de señales moleculares. Los intrones también pueden contener “información antigua”, es decir, fragmentos de genes que probablemente se expresaban pero que actualmente no se expresan.

Tradicionalmente se ha afirmado que los intrones son fragmentos de ADN carentes de información. Sin embargo esta afirmación es cuestionada y actualmente goza de pocos adeptos. Se sabe que los intrones contienen varias secuencias pequeñas que son importantes para un ajuste eficiente.

Algunos intrones del grupo I y II son ribozimas capaces de catalizar su propio splicing fuera del ARN. El descubrimiento de estas propiedades auto-catalíticas supuso el a Thomas R. Cech y Sidney Altman en 1989.

Actualmente se reconocen cuatro clases de intrones:

Los intrones del grupo I, II y III son intrones que sufren de autosplicing mediante reacciones de transesterificación. La frecuencia con la que encontramos estos intrones en el genoma es relativamente rara si la comparamos con la frecuencia de los intrones spliceosomales.

Los intrones del grupo II y III son muy similares y presentan una estructura secundaria altamente conservada. De hecho a veces los intrones del grupo III son identificados como intrones del grupo II debido a su similitud funcional y estructural. 

Los intrones del grupo I están presentes en los genes de ARNr de algunos eucariotas inferiores y en los genes mitocondriales de hongos. Se caracterizan por eliminarse mediante un proceso autocatalítico que requiere de una guanosina o un nucleótido de guanosina libre; así como por carecer de secuencias consenso en los puntos de empalme, aunque pueden tenerlas en su interior.

Los del grupo II y III se eliminan mediante un proceso autocatalítico que requiere de una adenina o de un spliceosoma, respectivamente. En ambos grupos, durante el proceso de empalme de los exones, se forma una estructura en lazo característica denominada lariat.

Los del grupo IV están presentes en los ARNt de los eucariotas y se caracterizan por ser los únicos que se eliminan mediante un corte endonucleótido seguido de un ligamiento en lugar de la reacción de transesterificación

Existen dos modelos, contrapuestos, que explican el origen y la evolución de los intrones nucleares o ayustosomales. Estos modelos se conocen como "intrones tempranos (IE)" o "intrones tardíos (IL)". 

El modelo IE propone que los intrones eran extremadamente numerosos en los ancestros de procariotas y eucariotas; y se fueron perdiendo a lo largo de la evolución. Este modelo se basa en la hipótesis de que los intrones fueron mediadores que facilitaron la combinación de exones, facilitando por tanto la evolución de nuevos genes.

El modelo IL propone que los intrones aparecieron tras la divergencia de procariotas y eucariotas. Este modelo se basa en la observación de que los intrones ayustosomales únicamente se han encontrado en eucariotas








</doc>
<doc id="8880" url="https://es.wikipedia.org/wiki?curid=8880" title="Los Pitufos">
Los Pitufos

Los Pitufos (en el original francés, "Les Schtroumpfs") son unos personajes creados por el dibujante belga Peyo en la historieta "La flauta de los seis pitufos" ("La Flûte à Six Schtroumpfs"), de su serie "Johan y Pirlouit", para el semanario "Le Journal de Spirou" el 23 de octubre de 1958. Tal fue el éxito de estas criaturas azules de pequeño tamaño, equivalentes a gnomos o duendes benignos, que al año siguiente empezaron a protagonizar su propia serie de historietas, así como películas, series de dibujos animados y videojuegos.
El nombre original de los pitufos en francés es "Schtroumpfs". Su nombre en español se le ocurrió a Miguel Agustí, redactor jefe de la revista "Strong", donde fueron publicados por vez primera en castellano en 1969. Durante más de un mes, estuvo buscando un nombre que pudiera conjugarse hasta que recordó el personaje de Patufet, figura emblemática del folclore catalán (y el nombre de una célebre revista infantil de preguerra en catalán). De ahí derivó el nombre de "Los Pitufos", que se mantendría en las siguientes versiones españolas (menos en TBO, donde aparecieron brevemente a mediados de los 70 rebautizados como "Los Tebeítos").

Los pitufos hicieron su aparición, como estrictos secundarios, en el episodio "La Flûte à Six schtroumpfs", publicado en los números 1047 a 1086 del semanario "Le Journal de Spirou", de la serie "Johan y Pirluit". En este episodio, Pirlouit encuentra una flauta mágica que le roban más tarde y, ante la necesidad de hacerse con otra el mago Homnibus, envía a los dos amigos a una tierra desconocida, el País Maldito, en donde viven los Pitufos.

En "La Guerre des Sept Fontaines" (1959), Peyo volvió a introducir a los pitufos de forma prudencial, aún anecdótica. Los pitufos tenían un gran éxito y con "Los pitufos negros" inauguraron en julio de ese año una colección de mini-relatos incluidos con "Le Journal de Spirou":

En enero de 1960, La Flûte à Six Trous se editó en álbum con el título de "La Flûte à Six Schtroumpfs" ("La Flauta de los Seis Pitufos"), prueba de las tendencias del mercado. Peyo creyó poder prescindir de ellos en el siguiente episodio de Johan y Pirlouit, "L'Anneau des Castellac", iniciado en agosto de ese año, pero el experimento no fue satisfactorio, ya que las ventas de los episodios "con pitufos" superaban a las de "sin pitufos", como le hizo notar el editor.
En el duodécimo episodio de Johan y Pirlouit, "Le Pays Maudit" (1964), los Pitufos están omnipresentes en la historia, de principio a fin. Se habían hecho tan populares que provocarían la desaparición casi total de "Johan et Pirlouit", ante la falta de tiempo de su creador para dedicarse a ella por entero.

Sin embargo, no fue hasta 1963, con la historieta "Pitufofonía en do", que los pitufos empezaron a aparecer de forma serializada en "Spirou", además en álbumes publicitarios.

Las nuevas entregas de la serie dejaron entonces de aparecer en la revista "Spirou", pero en noviembre de 1989 se lanzó una nueva revista, "Schtroumpf!", dirigida a los más pequeños.

Tras la muerte de Peyo en 1992, la serie continuó con su hijo Thierry Culliford a cargo de los guiones:

En España, sus historietas aparecieron traducidas al castellano en las revistas "Strong" (1969-1971), "TBO" (1974-1975), "Zipi y Zape", "Zipi y Zape Especial" y "Super Zipi y Zape" (1979-1980), "Pulgarcito" (1981-1982) o "Fuera Borda" (1984-1985). En formato álbum han sido editados sucesivamente por Argos (1969-1971), Bruguera (1979-1983), Grijalbo (1983-1985), Ediciones B (1991-1992), Planeta-DeAgostini (2006-2007) y actualmente Norma.

En el año 2008, se organizaron diversos actos con motivo de su 50 aniversario, incluyendo una exposición retrospectiva en el Centro Belga de la BD con el título de "L'Union fait la Schtroumpf".

Los pitufos constituyen una comunidad secreta de pequeños seres azules que viven en setas u hongos en lo profundo del bosque, durante el Medioevo. No usan nombres propios para tratarse entre sí, y tienen todos el mismo tamaño, apariencia y vestimenta (unos pantalones y un gorro frigio blanco, que son rojos sólo para el Gran Pitufo o Papá Pitufo que los lidera), pero entre ellos sí que parecen distinguirse; a pesar de ello existe una serie de personajes recurrentes, que se distinguen por sus virtudes o defectos, por sus aficiones o por alguna otra peculiaridad, y son llamados por ella: tenemos así al Pitufo Gafotas (Pitufo Filósofo en Hispanoamérica), el Pitufo Bromista, el Pitufo Valiente, la Pitufina, el Pitufo Goloso, el Pitufo Gruñón, el Pitufo Manitas, el Pitufo Vanidoso, el Pitufo Poeta, el Pitufo Simple, el Pitufo Perezoso, el Pitufo Labrador, el Pitufo Deportista...

Su lengua entremezcla palabras humanas normales con la palabra "pitufo", usada tanto como nombre, como adjetivo ("pitufado/a") o como verbo ("pitufar"), y que a los humanos suena siempre igual, pero que ellos parecen distinguir sin problemas; por ejemplo, en la historieta "El País maldito" (1964) un pitufo huye de su hogar para pedir ayuda a Johan y Pirlouit contra "un pitufo que pitufa pitufo", y se genera una discusión tratando de averiguar qué significaban estas palabras, que más tarde revelarían ser "un dragón que echa fuego".

En la mayoría de las historietas protagonizadas por ellos solos, sus enemigos recurrentes y encarnizados son el brujo Gargamel y su gato Azrael.

La banda sonora de la serie de televisión ha recurrido a menudo a notables piezas de música clásica como "Scheherezade" de Nicolái Rimski-Kórsakov o la "Sinfonía n.º 8 (Inacabada)" de Franz Schubert (ambientación de suspense, generalmente en presencia de Gargamel).

La obra de Peyo ha suscitado interpretaciones ideológicas dispares, más o menos fundamentadas.

"Les Schtroumpfs noirs" (1959), por ejemplo, ha sido tachada de racista.

Se ha llegado a afirmar incluso que la misma sociedad pitufa, con su elogio del comunitarismo y el líder supremo, constituye una apología comunista . Según esta interpretación, Gargamel y Azrael serían una clara caricatura del judío y del capitalismo.

En 2011 el francés Antoine Buéno publicó "El pequeño libro azul: análisis crítico y político de la sociedad de los pitufos", donde recoge estas ideas.

Lo que sí es indudablemente cierto es que Peyo reflejó su visión de lo que le rodeaba en "Los pitufos", atreviéndose a retratar la división lingüística de su país en "Schtroumpf vert et Vert Schtroumpf", ya en 1972.









</doc>
<doc id="8883" url="https://es.wikipedia.org/wiki?curid=8883" title="Tejuino">
Tejuino

El tejuino o tesgüino (del náhuatl "tecuin", "latir el corazón") es una bebida fermentada de maíz que consumen diversos grupos étnicos de México, principalmente los del norte del país (yaquis y pimas, de Sonora, y tarahumaras y tubares, de Chihuahua y Durango), en el noroeste y oeste (huichol) y, en menor proporción, en el sur (zapotecas de Oaxaca).

Para los tarahumaras, al igual que para otros grupos étnicos como los huicholes, el tesgüino constituye la bebida preferida en sus eventos sociales, festividades religiosas y deportivas, y en las llamadas tesgüinada, que son reuniones en las que se toman decisiones políticas y económicas importantes para la comunidad, o en las que se realizan trabajos o labores difíciles que requieren de la participación comunitaria de los hombres. Así, el tesgüino es empleado como una forma de pago; bebida embriagante y vehículo para la administración de diversas plantas medicinales; además, mezclado con leche materna o diluido en agua, es consumido por lactantes y niños por lo que puede considerársele como un complemento importante en su dieta.

El proceso de elaboración de la bebida varía de un grupo étnico a otro, aunque generalmente se hace con granos de maíz germinados en la oscuridad, que son molidos en metate y cocidos en suficiente agua durante varias horas hasta obtener un atole amarillento que, una vez frío, se cuela. El líquido recuperado se vacía en olla tesgüineras, se le adiciona el catalizador o fortificador y se deja fermentar de 1 a 10 días o más. Es importante señalar que las ollas tesgüineras nunca se lavan por lo que presentan, adheridos a sus paredes, residuos de fermentaciones previas. El tesgüino no se filtra ni pasteuriza, por lo que contiene los microorganismos vivos que producen la fermentación, las sustancias metabolizadas por ellos y los residuos de los vegetales utilizados.

Por otro lado, el tejuino es mestizo y es una bebida refrescante que también se encuentra en Guadalajara y en Mazatlán, entre otras ciudades. Aunque también se hace con maíz germinado, a este se le agrega piloncillo o azúcar.

Se bebe con limón, sal y chile piquín al gusto o sin agregarle nada, es de sabor agridulce y con un bajo grado de alcohol. Los vendedores ambulantes ofrecen tejuino en los pueblos y ciudades de la región, y es muy poco común encontrarlo en heladerías o neverías.

Hay dos tipos reconocidos: tejuino y tesgüino; el tejuino puede o no tener algo de fermentación no más fuerte que el tepache, el tesgüino es fermentado al máximo para que produzca licor. 

También existen dos tipos de tejuino: el tejuino blanco y el tejuino oscuro, elaborado con piloncillo.

En Nochistlan se llevan a cabo las festividades de San Sebastián. En esta fiesta es tradicional el tejuino, hecho a base de maíz, con la receta de los antiguos caxcanes que poblaban esta región. Cada noche del 17 al 20 de enero se reparte tejuino en cántaros a los asistentes a la fiesta, la cual se realiza en casa de los festejantes. Se hace con un tipo especial de maíz. Tiene un sabor amargo fuerte, es espeso y color café. No se modifica la receta original, se toma natural, es decir, no se le añaden hielos, sal o limón.

Hay quienes elaboran el tejuino fermentando masa (maíz en nixtamal, molido para formar una pasta con la que se hacen las tortillas).

Hay quienes agregan nieve de limón en sustitución del hielo y el limón. También hay quienes agregan hielo raspado (utilizado para los "raspados" o "nieve raspada") en vez de hielo en trozos.

Los Huicholes lo utilizan principalmente en sus festividades y actividades, llevando a cabo un proceso de fermentación (bebida alcohólica).



</doc>
<doc id="8884" url="https://es.wikipedia.org/wiki?curid=8884" title="Chocolate">
Chocolate

El chocolate (del náhuatl: "xocolātl") es el alimento que se obtiene mezclando azúcar con dos productos que derivan de la manipulación de las semillas del cacao: la masa del cacao y la manteca de cacao. A partir de esta combinación básica se elaboran los distintos tipos de chocolate que dependen de la proporción entre estos elementos y de su mezcla, o no, con otros productos ya sea como leche, colorante, y/o frutos secos.

El cacao ha sido cultivado por muchas culturas durante al menos tres milenios en Mesoamérica. La evidencia más temprana del uso del cacao pertenece a la cultura Mokaya de México, con vestigios de bebidas de chocolate que datan de 1900 a. C. Sin embargo, los olmecas de La Venta
en Tabasco fueron los primeros humanos en saborear, en forma de bebida, las habas de cacao molidas, las cuales mezclaban con agua y le añadían diversas especias, hierbas y guindillas y también fueron quienes comenzaron a cultivar el cacao en México. De hecho, la mayoría de la gente mesoamericana hizo bebidas de chocolate, incluidos los mayas y aztecas.

El botánico Carlos Linneo lo llamó Theobroma, que significa alimento de los dioses, llegando hasta el punto de ser objeto de culto para mayas y aztecas.

Actualmente, Ghana y Costa de Marfil son los dos principales productores y exportadores de cacao a nivel global. En varias investigaciones realizadas en estos países se han demostrado multitud de casos relacionados de esclavismo, trata de personas y explotación infantil (véase: Trabajo infantil en la producción de cacao). Muchas asociaciones activistas han querido concienciar sobre las injusticias detrás del chocolate, boicoteando a empresas productoras como Cargill Cocoa u Olam International o empresas compradoras como Nestlé y Hershey's. En julio de 2019, Ghana y Costa de Marfil llegaron a un acuerdo conjunto de fijar un precio mínimo para la venta de cacao, para dignificar la vida de sus trabajadores.

El cultivo, el uso y la elaboración cultural del cacao eran tempranos y extensos en Mesoamérica. Cuando se poliniza, la semilla del árbol de cacao forma finalmente una especie de vaina o mazorca de 10 a 35 cm de largo colgada de las ramas, dentro de la vaina hay de 30 a 40 granos almendrados de color pardo-rojo incrustados en una pulpa dulce y viscosa. Las habas o semillas son amargos debido a los alcaloides dentro de ellos, la pulpa dulce puede haber sido el primer elemento consumido por los seres humanos. La evidencia sugiere que puede haber sido fermentado y servido como una bebida alcohólica ya en 1400 aC.

Mientras que los investigadores no están de acuerdo en que la cultura mesoamericana domesticó por primera vez el árbol del cacao, el uso del frijol fermentado en una bebida parece haber surgido en México. Los científicos han podido confirmar su presencia en vasos de todo el mundo mediante la evaluación de la "huella química" detectable en las muestras de contenidos que quedan. Se ha encontrado un recipiente de cerámica con residuos de la preparación de bebidas de chocolate en sitios arqueológicos que datan del período Formativo Temprano (1900-900 aC). Por ejemplo, una embarcación de este tipo encontrada en un yacimiento arqueológico olmeca en la costa del Golfo de Veracruz (México) data la preparación del chocolate por parte de los pueblos pre-olmecas desde 1750 aC. En la costa del Pacífico de Chiapas, México, un sitio arqueológico de Mokayanan proporciona pruebas de las bebidas de cacao que datan incluso antes, a 1900 aC. 

Hacia el año 1500 a.C., los Olmecas de La Venta en Tabasco, México, fueron los primeros humanos en saborear, en forma de bebida, las habas de cacao molidas, las cuales mezclaban con agua y le añadían diversas especias, hierbas y guindillas, y también fueron quienes comenzaron a cultivar el cacao en México. La evidencia más temprana de la domesticación de la planta de cacao data de la cultura Olmeca desde el período Preclásico. En Los Olmecas lo usaban para rituales religiosos o como bebida medicinal, sin recetas para uso personal. Todavía queda poca evidencia de cómo se procesó la bebida.

En 2008 el Instituto Nacional de Antropología e Historia de México publicó estudios de las Universidades de Columbia, Arizona, Yale, Wisconsin y Kennesaw, en los que los análisis aplicados a una vasija encontrada en las excavaciones de Cerro Manatí, ubicado dentro del ejido del Macayal, en el municipio de Hidalgotitlán, Veracruz, concluyen que el consumo de cacao puede haberse dado 800 años antes de lo que se creía, en el período formativo (1900-900 a. C.). La vasija está datada mediante carbono 14 en 1750 a. C. y contiene restos de teobromina, componente marcador de la presencia de cacao en las vasijas es de alrededor del 1100 a. C. en el sitio arqueológico de Puerto Oculto (noreste de la actual Honduras), más recientes estudios (octubre de 2007) emprendidos por el equipo de arqueólogos dirigidos por John Henderson (Universidad Cornell) y Rosemary Joyce (Universidad de California en Berkeley) no solo ratifican que ya en el 1000 a. C. se consumía el chocolate en la región sino que muy probablemente en ésta el consumo se inició hacia "ca." el 1500 a. C. Se encontró en muestras de cerámica de Belice de entre el 600 al 400  a. C. Según Michael Coe, la bebida fue popularizada en Mesoamérica por los olmecas, pero la evidencia indica una popularidad más temprana.

En los primeros tiempos el consumo parece haber sido en forma de una especie de «cerveza»; es decir, una bebida basada en la fermentación más que de los granos del cacao de la pulpa del mismo. Tal «cerveza de chocolate», cuyos restos se hallan en las vasijas cerámicas de Puerto Escondido, tendría una importante función ritual y muy probablemente se utilizaba en las celebraciones de matrimonios. Bastante posteriormente, los olmecas, mayas y mexicas (entre otras civilizaciones mesoamericanas) comenzaron a consumir el chocolate derivado de la pasta de los granos aliñada o aderezada con chile. En forma semi líquida y líquida, el chocolate solía ser bebida preferida de las realezas, que lo consumían en vasos especiales (jícaras). Igualmente era considerado (con razón) un alimento tonificante o energizante, que se podía consumir mezclado en una masa de harina de maíz mezclada con chiles y miel.

Los mexicas premiaban a los mejores guerreros de la época otorgándoles el derecho de consumir libremente chocolate. También a los soldados se les otorgaban especies de bolitas hechas con polvo de cacao para que pudieran preparar su chocolate a lo largo de la guerra.
De acuerdo a la mitología maya, Kukulkán le dio el cacao a los mayas después de la creación de la humanidad, hecha de maíz (Ixim) por la diosa Xmucané (Bogin 1997, Coe 1996, Montejo 1999, Tedlock 1985). Los mayas celebraban un festival anual en abril, para honrar al dios del cacao, "Ek Chuah", un evento que incluía sacrificios de perros y otros animales con marcas pintadas de chocolate, ofrendas de cacao, plumas, incienso e intercambio de regalos.

El cacao también era utilizado como moneda en las culturas prehispánicas ya que era uno de los productos que se utilizaban para pagar el tributo al "tlatohani".

Durante el siglo XVII, el chocolate era considerado tanto un medicamento como un alimento, y no una bebida de compañía y de placer.




Según la Revista Internacional de Acupuntura, el tipo de cacao más preciado es el criollo, oriundo de Venezuela. Sus granos, muy aromáticos, son menos ácidos y muy poco amargos. Los árboles de la variedad forastero son robustos, resistentes y, por su alto rendimiento representan la parte principal de la producción mundial. Sus granos poseen un sabor más intenso, muy pocos aromas secundarios y son amargos o ácidos.

El movimiento "bean to bar" o "Chocolate del haba a la tableta" aparece a principios del siglo XXI en Estados Unidos y rápidamente se extiende hasta llegar a ser muy popular entre los consumidores de nivel adquisitivo alto. Con la misma velocidad se desvirtúa y las grandes empresas comienzan a utilizar "bean to bar" en sus etiquetas de modo que es preciso un reformulación de los términos del mismo.

El movimiento llega primero a Europa y un poco después a Latinoamérica y Asia. También con la misma velocidad se replantean las bases del "bean to bar" que podríamos resumir así:


Esta corriente de trabajo está aumentando de forma muy importante en todo el mundo aunque no supone aún ni el 10% del total del chocolate producido.

Para muchos, la palabra "chocolate" es una adaptación de la palabra náhuatl "xocolātl", que hacía referencia a una «bebida espumosa hecha de cacao» y cuyo significado literal es "agua agria".
Se postulan por tanto dos etimologías para "xocolātl":


Tras el tratamiento al que se somete a las habas de cacao en las zonas de recolección, estas se envían a las distintas fábricas chocolateras. Al llegar, los granos se examinan y se clasifican.

Lo primero que se realiza es el lavado y tostado de las habas del cacao; el objetivo es aumentar el aroma y favorecer el desprendimiento de la piel de las semillas. Un sistema de cepillado posterior permite eliminar esas pieles y cualquier otra impureza o cuerpo extraño.

A continuación, se realiza la torrefacción de las habas del cacao ya tostadas, un proceso importantísimo para la calidad final del producto. En unas grandes "esfera"s giratorias, las habas se tuestan durante unos pocos minutos a entre 110 y 120 °C., eliminándose la humedad y la acidez, al tiempo que se favorece el desarrollo de los aromas. Cada tipo de grano que formará parte de una determinada mezcla de chocolate se tuesta por separado.

Después de su enfriamiento, las habas, cuyas cáscaras han comenzado a explotar por el efecto de la torrefacción, se llevan a una máquina de descascarillar y cribar, que abre los granos tostados y separa los pellejos, ligeros, de la parte comestible, más pesada.
Las cáscaras y hollejos se reciclan como compost para jardines, o para elaborar mantecas de baja calidad llamadas comercialmente Cocoa.

La cocoa tiene un perfume y un sabor relativamente similar al del chocolate en polvo, pero que carece de las características originales del chocolate hecho a base de cacao. Se consigue mayormente de manera industrializada y es de color marrón oscuro. La cascarilla sirve para hacer bebidas todavía típicas de algunos lugares, solo se pone a macerar un puñado de cascarilla unos minutos, luego esta se hierve con leche y se bebe caliente. Pero esta bebida resultante carece de nutrientes y en algunas ocasiones la cascarilla suele contener cobre en cantidades altas. Por ser muy amarga, la Cocoa es un recurso con gran rendimiento económico sobre todo para las industrias que modifican el sabor de la Cocoa con grandes márgenes de azúcar, que por ser tan amarga necesita.

El siguiente paso es la mezcla. Determinadas cantidades de diferentes variedades de granos son pesadas e introducidas en un depósito cilíndrico, previamente a su paso a las máquinas de molienda. La mezcla de diferentes granos para hacer cacao en polvo es menos exigente que la del chocolate.

A continuación, se muelen las habas del cacao. Las habas trituradas pasan a través de una batería de molinos y se someten a un batido a una temperatura constante de 60-80º; la duración de este tratamiento puede ir de las 18 a las 72 horas. La duración influye en la textura del chocolate resultante: a menos batido, mayor aspereza. Por efecto de la trituración, el tejido celular de las habas, que contiene de un 50 a un 60 % de manteca de cacao, permite la liberación en parte de esta grasa, que luego se licúa por efecto del calor generado por el frotamiento. El resultado es una pasta fluida pero densa, la "pasta de cacao": una suspensión de sustancias con cacao en manteca de cacao.

Para su utilización en los diferentes productos, esta pasta se homogeneiza y se calienta a 100º, para ser luego propulsada en prensas hidráulicas. Se extrae así la mayor cantidad posible de "manteca de cacao", que se filtra y se compacta en grandes bloques. La pasta de cacao, con un porcentaje de grasa reducido entre el 8 y el 22 %, se presenta en forma de pan u hogaza. Esta parte sólida es durísima, pues se solidifica a 600 atmósferas.
El característico crujido y el delicado brillo del buen chocolate es debido a la estructura cristalina de la "manteca de cacao".

La "manteca de cacao", aparte de su utilización en la elaboración de chocolates, se usa en jabones y cosmética, por tener un punto de fusión ligeramente inferior a la temperatura corporal, lo que la convierte en una base perfecta para lápices de labios y otras cremas.

El sabor final del chocolate depende de la selección y mezcla de diversos tipos de granos de cacao. A estos tipos de granos de cacao pueden subdividirse entre las "variedades fuertes" y las "suaves", que se suelen mezclar proporcionalmente:



La elaboración del chocolate pasa por su última fase con la cuidadosa mezcla de la pasta y la manteca de cacao con azúcar, refinando la composición resultante por medio de trituradoras-refinadoras que producen una pasta muy delgada. A continuación, se efectúa la operación más importante, el "conchado" (o "concheado"), que le dará al chocolate toda su finura y su untuosidad.

El "conchado" es un amasado suplementario en artesas que, originalmente, tenían forma de concha. La pasta es batida y estirada en la artesa por unos rodillos, con un lento movimiento de vaivén, durante un periodo de tiempo y a una temperaturas que varían según el producto que se quiera obtener (en todo caso, unas horas y, a menudo, varios días). Todas estas operaciones se realizan a una temperatura superior al punto de fusión de la manteca de cacao que, por lo tanto, se mantiene líquida.

El último paso es el "templado", que consiste en fundir completamente el chocolate a 50 °C para que se rompan las estructuras cristalinas de la manteca de cacao, enfriarlo a 30º para devolverle la estructura, y, finalmente, aumentar ligeramente la temperatura para que los cristales se agrupen de nuevo en pequeñas cadenas.

Normalmente, el chocolate lleva añadida vainilla (o algún derivado como la vainillina) como aromatizante, y lecitina de soja como emulsionante y estabilizante para mejorar la textura y mantener las cualidades del chocolate; en total, ambos productos no superan el 1 % del chocolate.

Los distintos tipos de chocolate se elaboran modificando las proporciones entre sus componentes y añadiendo otros productos a la composición básica de pasta, manteca y azúcar. Su presentación puede ser en forma de "tableta" o en "polvo":

El chocolate negro (llamado también "chocolate fondant"; "chocolate amargo"; "chocolate bitter"; "chocolate amer"; "chocolate duro") es el chocolate propiamente dicho, pues es el resultado de la mezcla de la pasta y manteca del cacao con azúcar, sin el añadido de ningún otro producto (exceptuando el aromatizante y el emulsionante más arriba citados). Las proporciones con que se elabora dependen del fabricante. No obstante, se entiende que un chocolate negro "debe" presentar una proporción de pasta de cacao superior, aproximadamente, al 50 % del producto, pues es a partir de esa cantidad cuando el amargor del cacao empieza a ser perceptible. En cualquier caso, existen en el mercado tabletas de chocolate negro con distintas proporciones de cacao, llegando incluso hasta el 99 %.

Se considera un alimento afrodisiaco porque gracias a su contenido de magnesio ayuda a combatir las contracciones musculares y dolores premenstruales. En lugares como Oaxaca, México se añade almendra a su preparación y es elaborado en piedra, en la actualidad existen grupos de artesanos como los "chocolateconalas" que aún mantienen esta tradición.

En algunos casos, se suele sustituir el azúcar por algún edulcorante (principalmente sucralosa). En este caso se utiliza para regímenes dietéticos.

El chocolate de cobertura es el chocolate que utilizan los chocolateros y los pasteleros como materia prima. Puede ser "negro" o "con leche", pero en todo caso se trata de un chocolate con una proporción de manteca de cacao de alrededor del 30 %, lo que supone el doble que en los otros tipos de chocolate. La cobertura se usa para conseguir un alto brillo al templar el chocolate y porque se funde fácilmente y es muy moldeable.

El chocolate a la taza es el chocolate negro (normalmente, con una proporción de cacao inferior al 50 %), al que se le ha añadido una pequeña cantidad de fécula (normalmente, harina de maíz) para que a la hora de cocerlo aumente su espesor. Suele disolverse en leche. Hay diversidad de gustos en ellos, se puede encontrar amargo, semi-amargo y/o dulce. Hoy en día, es posible encontrar también este chocolate en los comercios en forma ya líquida.

El chocolate con leche es el derivado del cacao más popular. Se trata, básicamente, de un dulce, por lo que la proporción de pasta de cacao suele estar por debajo del 40 %. No obstante, buena parte de las más importantes marcas de chocolate producen tabletas de chocolate con leche con proporciones de cacao inusuales, por encima incluso del 50 %, dirigidas tanto al mercado de los "gourmets" como al negocio de la pastelería. El chocolate con leche, como su nombre indica, lleva leche añadida, en polvo o condensada.

En el caso del chocolate blanco, estrictamente, no se trata de chocolate como tal, pues carece en su composición de la pasta de cacao, que es la materia que aporta las propiedades del cacao. Se elabora con manteca de cacao (por lo menos, el 20 %), leche (en polvo o condensada) y azúcar. Es un producto extremadamente energético y dulce (no posee regusto amargo). Visualmente muy atractivo, es un elemento decorativo muy usado en la repostería.

El chocolate rosa, completamente natural y sin ningún tipo de aditivo ni colorantes añadidos, se obtiene de la semilla del cacao "ruby".

El chocolate, como indica su nombre, es una cubierta de chocolate (en cualquiera de sus variantes y con un peso superior al 25 % del total) que recubre frutos secos (avellanas, almendras...), licores, frutas, etc., así como galletas tipo "wafer".

Degustar el chocolate consiste en experimentar, analizar y apreciar sus características organolépticas con los cinco sentidos. Es importante recordar que la temperatura y humedad del ambiente pueden repercutir en la degustación.

Un buen chocolate tendrá un color marrón muy oscuro y brillante, uniforme, sin ningún tipo de mácula, burbujas o hendiduras.
El tacto debe ser firme, nunca pegajoso, y, al partirlo, debe ofrecer una resistencia mínima; si al partirlo forma astillas, está demasiado seco; y si es difícil de partir está muy ceroso. En la boca, la disolución será fácil, continuada y completa, esto es, sin rastro alguno de granulosidades.
Al partirlo, el sonido debe ser seco pero quebradizo.
Se tendrán en cuenta la olfacción directa y la indirecta (por vía retronasal).

El sabor debe ser básicamente amargo con un punto de acidez y de dulzor, y después puede haber toques de piña, plátano, vainilla, canela, azafrán, etc.
Aunque para disfrutar de un verdadero chocolate es necesario manipular en esencia semilla de cacao con márgenes equilibrados de azúcar, esto no sucede con el chocolate que se conoce comúnmente, ya que todo el chocolate se industrializa a modo de separar manteca y pasta de cacao. Con eso se modifica el sabor y también la calidad. Un buen chocolate es aquel que en esencia de ingredientes está hecho a base de cacao sin modificar sus sustancias naturales. El chocolate artesanal es un buen ejemplo.

El chocolate en polvo tiene por objeto su disolución en leche. Se elabora con una proporción de cacao que oscila entre un 25 y un 32 %, y se presenta más o menos desgrasado.

Es un pieza de chocolate en forma de disco y recubierta con una envoltura de color metálico. 

Los bombones son porciones pequeñas (apropiadas para ser ingeridas en un solo bocado) de una mezcla sólida de chocolate (negro, blanco o con leche) o de una cubierta de chocolate (negro, blanco o con leche) rellena de distintos elementos.

Constituyen, al lado del chocolate en tableta y en polvo, la forma más importante y extendida de presentar comercialmente el chocolate. A diferencia de las otras presentaciones, los bombones están asociados a comportamientos de gratitud, regalo o reconocimiento en las relaciones sociales. Su producción está muy cuidada por prácticamente la totalidad de las industrias chocolateras.


"Para la composición nutricional, véanse al final los enlaces externos sugeridos"

Los dos principales ingredientes del chocolate son calóricos: "la grasa" y "el azúcar".









Es rico en polifenoles-flavonoides, como la epicatequina, potentes antioxidantes que protegen al sistema circulatorio, en especial al corazón, el "chocolate negro" es particularmente rico en polifenoles que entre otros efectos benéficos previene o reduce los efectos del SFC y encefalomielitis miálgica.

Posee un elevado dosaje de promotor de serotonina gracias al triptofano, un aminoácido muy importante en nuestro organismo regulador de neurotransmisores y un buen dosaje de anandamida, ambos psicotrópicos naturalmente existentes en el ser humano y obtenidos en dosis suficientes (mínimas) al consumir chocolate, facilitan una sensación de placer ("sin" caer en la irrealidad o la estupefacción), por su parte, tal sensación de placer refuerza al sistema inmune, también se ha observado que la ingesta de chocolate compensa las inversiones de péptidos que suelen ocurrir en el sistema nervioso central de los seres humanos durante su adolescencia cuando se enamoran.

Las principales contraindicaciones conocidas (marzo de 2007) al consumo de chocolate son las siguientes: exceso de calorías (esto si se realiza una dieta excesiva de chocolate, más aún si este va mezclado con grasas hidrogenadas, azúcares o glúcidos añadidos), cuando el consumo de productos industrializados en base al chocolate es frecuente también es común que se substituya los glúcidos por sacarinas o por ciclamatos los cuales pueden asimismo conllevar riesgos para la salud.

El chocolate "negro" es el que se considera actualmente más benéfico ya que el chocolate blanco es pobre en cacao pero con muchas grasas y glúcidos. Se aconsejan hasta 100 g de chocolate negro por día, esto disminuye el riesgo de accidentes vasculares y de hipertensión, aunque estudios (Druk taubel, Diane Becker, Norma Hollemberg) publicados a inicios de julio de 2007 señalan que el solo consumo de una pequeña barra de chocolate negro por día ya reduce la presión sistólica en un 8 % o 9 %. No obstante, un artículo publicado en diciembre de 2007 en "The Lancet" señala que muchos fabricantes de chocolate quitan los flavonoides por su gusto amargo, añadiendo por contrapartida endulzantes y grasas.

Aunque el chocolate es comúnmente ingerido por placer, existen efectos beneficiosos sobre la salud asociados a su consumo. El cacao o el chocolate negro benefician al sistema circulatorio. Otros efectos beneficiosos sugeridos incluyen efecto anticanceroso, estimulador cerebral, antitusígeno y antidiarreico. Un efecto afrodisíaco aún debe probarse.

Por otro lado el consumo incontrolado de una gran cantidad de cualquier alimento rico en calorías, como el chocolate, incrementa el riesgo de obesidad de no haber un correspondiente aumento en la actividad física. El chocolate crudo es rico en manteca de cacao, un componente graso que es removido durante el refinamiento del chocolate para luego ser añadido nuevamente en proporciones variables durante el proceso de fabricación. Los fabricantes pueden añadir otras grasas, azúcares y leche, todo lo cual incrementa el contenido calórico del chocolate.

Hay riesgo de leve intoxicación por plomo por algunos tipos de chocolate.
El chocolate es tóxico para muchos animales por su insuficiente capacidad para metabolizar la teobromina.

Un estudio difundido por la BBC indicó que el derretir chocolate en la boca produjo un aumento en actividad cerebral y ritmo cardíaco que fue más intenso que el asociado con el beso apasionado y además duró cuatro veces más.

Es importante señalar que el chocolate puede producir reacciones alérgicas en algunas personas, las cuales deben evitar su consumo.

Estudios recientes sugirieron que el cacao o el chocolate negro puede poseer ciertos efectos beneficiosos sobre la salud humana. Esto es principalmente causado por una particular sustancia presente en el cacao llamada epicatechin. El cacao posee una acción significativa como antioxidante, protegiendo contra la oxidación LDL, quizás más que otros alimentos y bebidas ricos en antioxidantes polifenoles. Algunos estudios también observaron una moderada reducción en la presión sanguínea después de ingerir chocolate negro diariamente. Ha habido una dieta llamada "Dieta del chocolate" que enfatiza el comer chocolate y polvo de cacao en cápsulas. Sin embargo, el consumo de chocolate de leche o chocolate blanco, o leche entera con chocolate negro parece negar ampliamente el beneficio en la salud. El polvo de cacao procesado (también llamado chocolate holandés), procesado con álcali, reduce en gran medida la capacidad antioxidante comparado con el polvo de chocolate en crudo. El proceso del cacao con álcali destruye la mayoría de los flavonoides.

Un tercio de la grasa en el chocolate viene en forma de una grasa saturada llamada ácido esteárico y una grasa monoinsaturada llamada ácido oleico. De todas formas, a diferencia de otras grasas saturadas, el ácido esteárico no eleva los niveles de colesterol LDL en el torrente sanguíneo. Consumir niveles relativamente altos de chocolate negro y cacao no parece elevar los niveles séricos de colesterol LDL y algunos estudios indican que podría reducirlos. De hecho, cantidades pequeñas pero regulares de chocolate negro bajan la posibilidad de un ataque cardíaco, un resultado del desequilibrio de colesterol según la hipótesis lipídica.

La creencia popular romántica comúnmente identifica el chocolate como un afrodisíaco. Las propiedades afrodisíacas del chocolate están más frecuentemente asociadas con el simple y sensual placer de su consumo. Aunque no hay prueba de que el chocolate es un afrodisíaco, un regalo de chocolates es un ritual de cortesía familiar.

Estudios sugieren que un tipo especial de cacao podría ser nootrópico y retrasar el declinamiento de la función cerebral que ocurre durante el envejecimiento.

Otra investigación indica que el chocolate puede ser efectivo para prevenir la tos persistente. Se halló que la teobromina fue casi un tercio más efectiva que la codeína, la medicación líder para la tos.

También la teobromina del chocolate (especialmente el negro o "bitter") actúa como antidepresivo natural.

Los flavonoides pueden inhibir el desarrollo de diarrea sugiriendo efectos antidiarréicos del cacao.

La mayor preocupación que tienen los nutricionistas es que aunque el comer chocolate negro puede no afectar el colesterol sérico, la presión arterial o la oxidación LDL, no se sabe aún si afecta favorablemente ciertos marcadores biológicos de la patología cardiovascular. Además, la cantidad necesaria para tener este efecto proveería relativamente grandes cantidades de calorías que de no ser usadas propiciarían un aumento de peso. La obesidad es un factor de riesgo significativo para muchas enfermedades incluyendo las cardiovasculares.

Hay una creencia popular que el chocolate puede causar acné. Esta creencia no está basada en estudios científicos. Varios estudios apuntan no al chocolate sino a la elevada naturaleza glicémica de ciertos alimentos como azúcar, jarabe de maíz y otros carbohidratos simples como causa de acné. El chocolate en sí tiene un bajo índice glucémico. Otras causas dietarias de acné no pueden ser aún excluidas pero se requiere de una investigación más rigurosa.

El chocolate tiene una de las concentraciones más altas de plomo entre los productos que componen la típica dieta occidental, con el potencial de causar intoxicación leve. Estudios recientes han mostrado que si bien los granos absorben poco el plomo, este tiende a unirse a la corteza del cacao y la contaminación puede ocurrir durante el proceso de fabricación. Una publicación reciente encontró cantidades significantes de plomo en el chocolate. En un estudio del USDA de 2004, los niveles medios de plomo en muestras testeadas variaron desde 0.0010 a 0.0965 µg de plomo por gramo de chocolate, pero otro estudio de un equipo de investigación suizo de 2002 encontró que algunos chocolates contenían hasta 0.769 µg por gramo, cercano a los límites internacionales (voluntarios) estándares para plomo en polvo de cacao o granos, que es 1 µg de plomo por gramo. En el 2006, la FDA (administración de drogas y alimentos, la autoridad estadounidense que regula estos productos) bajó en un quinto la cantidad de plomo permitido en caramelos, pero el cumplimiento es solo voluntario. Mientras que estudios muestran que el plomo consumido con el chocolate puede no ser absorbido por el cuerpo humano, no hay ningún umbral para el efecto del plomo en el funcionamiento cerebral de los niños e incluso pequeñas cantidades de plomo pueden causar déficits en el desarrollo neurológico, incluyendo un IQ desigual.

En cantidades suficientes, la teobromina encontrada en el chocolate es tóxica para animales como caballos, perros, loros, roedores pequeños y gatos debido a que estos son incapaces de metabolizar esta sustancia química con efectividad. Si son alimentados con chocolate, la teobromina permanecerá en su torrente sanguíneo hasta 20 horas y estos animales pueden experimentar episodios epilépticos, ataques cardíacos, hemorragias internas y eventualmente la muerte. El tratamiento médico efectuado por un veterinario incluye la inducción del vómito dentro de las dos horas posteriores a la ingestión y la administración de benzodiacepinas o barbitúricos para las convulsiones, antiarrítmicos para las arritmias cardíacas y la diuresis de fluidos.

Un perro típico de 20 kg (40 lb) normalmente experimentará un gran dolor intestinal después de comer menos que 240 gramos de chocolate negro pero no necesariamente presentará bradicardia o taquicardia a menos que coma por lo menos medio kilo (1,1 lb) de chocolate de leche. El chocolate negro tiene 2 o 5 veces más teobromina y por lo tanto es más peligroso para los perros. De acuerdo al Manual Veterinario Merck, aproximadamente 1.3 gramos de chocolate de panadero por kilogramo de peso corporal canino es suficiente para causar síntomas de toxicidad. Por ejemplo, una barra de chocolate de panadero típico de 25 gramos serían suficientes para causar síntomas en un perro de aproximadamente 20 kg.
Por supuesto que el chocolate de panadero raramente se consume directamente debido a su gusto desagradable pero otros chocolates negros pueden tener una toxicidad extrapolada basada en este caso. Como los perros gustan del sabor de productos del chocolate tanto como los humanos y son capaces de encontrar y comer cantidades más grandes que las típicas que se sirven a humanos, deberían ser alejados de su alcance. Existen informes sobre que la moltura hecha de la corteza del grano de cacao es peligrosa para perros y el ganado.

El chocolate contiene una variedad de sustancias, algunas de las cuales tienen un efecto en la química orgánica. Estas incluyen:


El chocolate es un leve estimulante para humanos principalmente debido a la presencia de teobromina.







</doc>
<doc id="8886" url="https://es.wikipedia.org/wiki?curid=8886" title="Exón">
Exón

El exón es la región de un gen que no es separada durante el proceso de corte y empalme y, por tanto, se mantienen en el ARN mensajero maduro. En los genes que codifican una proteína, son los exones los que contienen la información para producir la proteína codificada en el gen. En estos casos, cada exón codifica una porción específica de la proteína completa, de manera que el conjunto de exones forma la región codificante del gen. En eucariotas los exones de un gen están separados por regiones largas de ADN (llamadas intrones) que no codifican. 
El término "exón" fue acotado por el bioquímico estadounidense Walter Gilbert en 1978:

Esta definición fue realizada originalmente para la transcripción del código proteínico, pero luego fue trasladada. El término más tarde llegó a incluir secuencias eliminadas de rRNA y tRNA, y también fue utilizado más tarde para las moléculas de ARN procedentes de diferentes partes del genoma que luego son ligadas por trans-splicing.

Si bien se consideró en un primer momento que los exones (en comparación con los intrones) son los que llevan la "información" dentro de un gen, se ha demostrado que no siempre es así. Así por ejemplo existen pseudogenes que poseen la estructura de un gen activo (incluido sus exones) y sin embargo no se transcriben.

A la derecha se puede observar un diagrama mostrando como los exones y los intrones se localizan de manera intercalada en un gen. A cada extremo de un gen existe una región no traducida del mismo (UTR: UnTraslated Region). La transcripción de un gen a ADN, genera un RNA mensajero inmaduro. Este ARN mensajero lleva a cabo el proceso de splicing, en el que se escinden los intrones y las regiones no traducidas. Una vez que el ARN mensajero ha madurado, puede ser traducido a una proteína.

Es importante mencionar que un mismo gen puede producir diferentes proteínas gracias a un splicing alternativo. Mediante este proceso, algunos exones pueden ser eliminados junto con los intrones que los flanquean. De esa manera se crean diferentes versiones de ARN mensajeros que son traducidas a su vez en diferentes proteínas. Cabe notar que este splicing alternativo, no es de ninguna manera un proceso aleatorio sino que ha evolucionado de manera que las diferentes proteínas así creadas sean todas funcionales. 



</doc>
<doc id="8894" url="https://es.wikipedia.org/wiki?curid=8894" title="Ciclopentanoperhidrofenantreno">
Ciclopentanoperhidrofenantreno

El ciclopentanoperhidrofenantreno (también llamado esterano o gonano) es un hidrocarburo policíclico que puede considerarse un producto de la saturación del fenantreno asociado a un anillo de ciclopentano. Posee 17 átomos de carbono. De esta base estructural derivan los esteroides (el colesterol y sus derivados, como la progesterona, la aldosterona, el cortisol y la testosterona son ejemplos de compuestos que contienen un núcleo de ciclopentanoperhidrofenantreno) derivados del fenantreno.

Las sustancias derivadas de este núcleo muestran grupos metilo -CH, en las posiciones 10 y 13 para integrar los carbonos 18 y 19; generalmente existe una cadena alifática en el carbono 17, la longitud de dicha cadena y la presencia de metilos en el carbono 10 y 13 determina las diferentes estructuras de estas sustancias.

Es el grupo más nuevo y en éste hay cuatro progestágenos importantes: levonorgestrel, desogestrel, gestodeno y norgestimato.

Es un progestágeno usado en las formulaciones anticonceptivas que tiene una actividad biológica aproximadamente 80 veces más potente que la progesterona endógena. Tiene un efecto androgénico porque compite con la testosterona para unirse a la proteína transportadora, y además tiene actividad antiestrogénica. Sin embargo, estos preparados tienen una influencia desfavorable sobre la relación entre el LDL-colesterol y el HDL-colesterol por su efecto androgénico y cuando se administran en combinación con 30 µg de etinilestradiol reducen la fracción LDL y elevan la HDL. El avance más importante que se ha producido en los últimos años ha sido la obtención de tres gestágenos derivados del levonorgestrel y que son los llamados “progestágenos de tercera generación”.

Disponible desde 1982, fue el primero de los progestágenos más selectivos que llegó a estar disponible para uso en los AO. Es 2,8 veces más potente que el levonorgestrel, tiene pocos efectos androgénicos (prácticamente no compite con la testosterona para ligarse a la proteína transportadora) y no tiene influencia sobre la actividad estrogénica, lo que permite emplear bajas dosis de estrógenos en los preparados anticonceptivos y reducir la aparición de efectos secundarios (Dibbelt et al, 1991). Además, limita la penetración de espermatozoides a través del moco cervical y es un poderoso inhibidor de la ovulación.

Fue el segundo progestágeno de tercera generación disponible comercialmente. No necesita del metabolismo hepático para ser biológicamente activo. Es 1,5 veces más potente que el levonorgestrel. No interfiere con el metabolismo de la testosterona y evita, en gran medida, los efectos androgénicos secundarios de los AO que contienen norgestrel y levonorgestrel. A diferencia del desogestrel tiene un importante efecto antiestrogénico lo cual limita la dosis utilizada.

Es el más nuevo de todos. Los ensayos biológicos han demostrado que tiene una alta selectividad progestacional y su afinidad para unirse con los receptores androgénicos es menor que la del gestodeno y el levonorgestrel, lo que le confiere una elevada actividad progestacional con muy baja actividad androgénica (Huber, 1991).

Con los conocimientos actuales, el perfil de un AO no se puede representar simplemente por la suma de ambos componentes; sino por una compleja interacción entre los dos. Esta interacción se puede manifestar con un potente sinergismo entre el estrógeno y el progestágeno o, por el contrario, con un efecto antagónico entre ambos.


</doc>
<doc id="8898" url="https://es.wikipedia.org/wiki?curid=8898" title="Ley del péndulo">
Ley del péndulo

Consideremos un péndulo cuyo brazo mide l, en el campo gravitacional de intensidad g (usualmente: 9,81 m.s), y sujeto a pequeñas oscilaciones.
El período T de oscilación del péndulo es dado por la fórmula:

formula_1

Sea θ el ángulo en radianes que hace el brazo con la vertical y m la masa del péndulo, al extremo de su brazo, que se mueve con la velocidad : v = l·θ'.

La energía cinética del péndulo es: 
Se puede tomar su energía potencial igual a: 
Este sistema no pierde energía, por la suma de energía cinética y potencia es constante 

Al derivar se obtiene:

Se puede simplificar por m·l (no nulos) y por θ' (no idénticamente nulo), lo que da:
Como se supone que θ es siempre pequeño, se puede remplazar sen θ por θ cometiendo un error del orden de θ (porque sin θ = θ + O(θ)).

Entonces equivale a:
Un movimiento oscilatorio sigue la ley 

lo que implica que 

donde formula_2es la velocidad angular de la ley y formula_3 el ángulo máximo.

Identificando y se obtiene 
formula_4, es decir formula_5.

Concluimos recordando que formula_6.


</doc>
<doc id="8899" url="https://es.wikipedia.org/wiki?curid=8899" title="Período">
Período

Periodo o período (del latín "periŏdus") se utiliza regularmente para designar el intervalo de tiempo necesario para completar un ciclo repetitivo, o simplemente el espacio de tiempo que dura algo.<br>
Puede referirse:





</doc>
<doc id="8900" url="https://es.wikipedia.org/wiki?curid=8900" title="Hipótesis de la señal">
Hipótesis de la señal

La hipótesis de la señal relata los mecanismos como el transporte biológico de las proteínas en las células a los orgánulos apropiados por inserción en las membranas o segregadas fuera de la célula. Las proteínas recién sintetizadas están dotadas de una señal intrínseca que debe ser descifrada en los lugares de destino. Günter Blobel sugirió, en 1975, que esa señal determina su capacidad para dirigirse hacia la membrana del retículo endoplásmico y atravesarla, y a esta teoría la llamó "hipótesis de la señal".

La señal consiste en un péptido hidrofóbico, el péptido señal, constituido por unos 20 aminoácidos en orden particular, que son los primeros que aparecen cuando se está sintetizando la cadena polipeptídica. Este péptido parece que señala a la célula que la proteína a la cual está unido debe ser transportada, pues cuando el péptido se une a una proteína citoplasmática esto provoca que la proteína sea secretada. Parece ser que la hidrofobicidad del péptido señal es uno de los factores más importantes para que se produzca el transporte de la proteína. Los principios moleculares descritos por Blobel que constituyen la base de este proceso son universales. los peptidos señal tienen una determinada carga, ya que poseen lys o arg en sus secuencias codificantes, el lado positivo, marcado por estos aminoácidos queda siempre mirando el citosol. Además la translocación estará determinada por esta carga y de ella dependerá también el coste para la célula llevar a cabo la translocación, debido a que la carga debe estar siempre mirando el lado citosólico muchas veces la cadena proteica es más larga por el lado en que esta la carga negativa del péptido señal lo que implicaría un mayor trabajo desplazar esta larga cadena por el translocón (proteína de membrana que cumple la función de translocar la proteína.


</doc>
<doc id="8901" url="https://es.wikipedia.org/wiki?curid=8901" title="Günter Blobel">
Günter Blobel

Günter Blobel (Waltersdorf, Silesia, Alemania; 21 de mayo de 1936-Nueva York, 18 de febrero de 2018) fue un biólogo germano-estadounidense.

En 1987 se nacionalizó estadounidense.

En 1967 se graduó en Oncología, realizó toda su carrera en la Universidad Rockefeller de Nueva York, de la que fue profesor y en cuyo laboratorio de biología celular (en el Instituto Médico Howard Hughes) trabajó.

Ganó el Premio Nobel de Fisiología o Medicina en 1999 por sus trabajos realizados en la década de 1970, al descubrir que las proteínas tienen señales intrínsecas que gobiernan su transporte y situación en la célula. Estas investigaciones abrieron la vía para crear fármacos que se dirigen al lugar del organismo donde deben actuar. Donó la totalidad del premio a la ciudad de Dresde. "Asistí muy de cerca a la destrucción de Desde, y ninguna otra cosa me ha impresionad más", comentó en cuando realizó la donación.






</doc>
<doc id="8902" url="https://es.wikipedia.org/wiki?curid=8902" title="Numeral (lingüística)">
Numeral (lingüística)

Un numeral es un nombre propio para un número, las lenguas naturales en tanto que lenguajes que permiten hacer afirmaciones sobre realidades físicas, disponen de subsistemas lingüístico-cognitivos capaces de nombrar números y contar.

Los sistemas de numeración de las lenguas naturales se basan en la cuenta de dedos. Por eso la práctica totalidad de los sistemas de numeración de las lenguas de la tierra usa sistemas de numeración basados en la base 10 o la base 20.

Las mayoría de las lenguas indoeuropeas utilizan un sistema de numeración decimal, lo que significa que los nombres de los números se agrupan en series de diez, y que existen raíces para los números del uno al nueve, el diez, el cien, el mil y los demás nombres son derivados de las raíces para los numerales citados. Otras familias de lenguas emplean también el sistema decimal y existen familias de lenguas donde se usan sistemas vigesimales (vasco, lenguas mayas, lenguas utoaztecas, etc.). Algunas lenguas tienen subsistemas de base cinco, dentro de un sistema decimal o vigesimal. 

Sin embargo, no se conocen sistemas de cuenta amplios no basados en la base 10 o la base 20. Esta restricción parece relacionada con que la idea original de contar estaba asociada a los dedos de las manos o el conjunto de dedos de manos y pies. Los sistemas no basados en base 10 o 20 son escasos aunque aparecen en lenguas australianas de forma generalizada y —de forma marginal— en algunas lenguas americanas, como el waimirí y el arará (dos lenguas caribes), el resiguaro (una lengua arahuaca), el harákmbet y el andoque (lenguas aisladas), que suelen usar sistemas cuasi-binarios.

Son palabras que expresan orden o cantidad de forma precisa.

Algunas austronesias y melanesias, entre ellas el māorí, algunas lenguas de Sulawesi y algunas de Papúa Nueva Guinea, usan para contar sistemas basados en el 'cuatro'. En estas lenguas el término "asu" o "aso" (derivado del javanés "asu", 'perro'), se usa para 'cuatro' dado que los perros en estas culturas son el cuadrúpedo más abundante. Se ha propuesto que este sistema podría haber surgido en el contexto de granjeros que intercambiaban animales en un mercado, así cincuenta "asu" representaría un conjunto de 200 cosas, si a esa cantidad se le substraen 30 "asu" (120) se llegaría rápidamente a inferir que sólo quedan 20 "asu" (80), y la generalización de la idea de contar las cosas como formando "asu" completos habría dado origen a este curioso sistema de contaje. Nótese que este sistema podría estar cognitivamente relacionado con el uso de docenas como sistema de contaje extendido debido a que la aritmética de la división es simple en docenas.

Los sistemas quinarios que usan la base 5 también están testimoniados. Obviamente dichos sistemas al igual que los sistemas decimales y vigesimales derivan del contaje de dedos (una mano tiene cinco dedos). Un ejemplo de este tipo de sistema se encuentra en la lengua api, una lengua de Vanuatu, donde 'cinco' y 'mano' se dicen simplemente "luna" 'cinco, mano', 'diez' se dice "lua-luna" 'dos-cinco', 'quince' se deice "tolu-luna" 'tres-cinco', etc. El número 'once' se llama "lua-luna tai" 'dos-cinco uno' y 'diecisiete' "tolu-luna lua" 'tres-cinco dos'. Un sistema básicamente idéntico se encuentra en emberá-catio (familia chocó).

Aunque los sistemas puramente quinarios no son frecuentes, si es muy frecuente el uso de la base cinco como "base auxiliar" o "sub-base" empelada para algunos números, así en muchas lenguas del mundo es frecuente que el 6 se llame 'cinco/mano y uno', 7 'cinco/mano y dos ', etc. Por ejemplo el náhuatl tiene una base vigesimal (base, 20) pero emplea 5 como sub-base: 6 "chikasē", 7 "chikōme", 8 "chikēi" y 9 "chiknāwi".

El kanum es un ejemplo exótico de lengua con un sistema de contaje en base 6. Las lenguas sko, sin embargo, usan una base 24, en donde se usa sub-base 6.

El sistema octal usa el número 8 como base. Dicho sistema está testimoniado en el Idioma yuki de California y en las lenguas pame de México central. Esto se debe a que ambos grupos humanos usan los huecos entre los dedos de las manos para contar más que los dedos mismos.

Las lenguas sko usan un sistema de cuenta en base 24, donde el 6 es empleado como sub-base.

El ngiti usa base 32.

El ekari usa una base 60 para contar. En Sumeria parece haberse empleado la base 60 para ciertos usos, aunque siempre con 10 como sub-base, quizá un sistema mixto entre sistemas decimales y duodecimal. El empleo de este curioso sistema en Sumeria está detrás del uso moderno de los grados sexagesimales de la medida de los ángulos en grados, minutos y segundos. Uso del que deriva también la medida del tiempo en minutos y segundos, así como la división de la circunferencia en 360º.

Se ha publicado que el supyire tiene un sistema de cuenta en base 80, en esta lengua se cuenta en veintenas (usando 5 y 10 como sub-bases) hasta 80 y entonces en ochentenas hasta 400, y a partir de ahí de 400 en 400.
799 [i.e. 400 + (4 x 80) + (3 x 20) + {10 + (5 + 4)}]’

Un número importante de lenguas de Melanesa usan (o usaban) sistemas de cuenta basados en las partes del cuerpo en lugar de sobre una base numérica. Este sistema no tiene palabras exclusivas para números, sino que más bien los nombres de ciertas partes del cuerpo se emplean para contar. Por ejemplo entre 1 y 4 se emplean los dedos, 5 se llama 'pulgar', 6 'muñeca [de la mano]', 7 'codo', 8 'hombro', etc. Así a través del cuerpo y luego bajando por el otro brazo, así el meñique opuesto representa el 17 (Islas Torres) o 23 (lenguas eleman). Para números más allá de estos valores, se usa el torso, las piernas y los dedos de los pies, o uno puede contar hacia atrás hasta el otro brazo y de nuevo al primero.

También es un tema bien estudiado, qué principios aritméticos usan los sistemas de numeración de las lenguas del mundo. Por ejemplo, la suma es universal en todas las lenguas ("dieciocho" = "diez" y "ocho" = 10 + 8), para expresar un número que puede expresarse mediante un cierto número de veces la base más un número inferior a la base. La multiplicación es también muy frecuente para expresar ciertos números superiores a la base ("doscientos" = 2 x 100). En algunas lenguas aparece implícitamente la substracción para formar números (latín "duōdēvigintī" '18' = 2 antes de 20 = 20 - 2). En cambio, la división es totalmente marginal en la formación de numerales y generalmente se restringe a fracciones como 1/2.

En muchas lenguas además de los numerales cardinales o numerales comunes usados para contar existen otros tipos adicionales de numerales: ordinales, partitivos, distributivos, multiplicativo, etc. El latín es un ejemplo de lenguas con todos estos tipos de numerales:

En lenguas flexivas estos numerales ordinales son formas derivadas de los lexemas que se usan para formar los numerales comunes.

Los numerales reconstruidos para la protolengua madre de diversas familias lingüísticas. Entre las lenguas de Eurasia se tienen:
Para las lenguas de África se tiene:
Para las lenguas de América se tiene:
Y para las lenguas de Oceanía:



</doc>
<doc id="8903" url="https://es.wikipedia.org/wiki?curid=8903" title="Febe (mitología)">
Febe (mitología)

En la mitología griega, Febe (en griego antiguo Φοίβη - "Phœbē": ‘brillo’ del intelecto), la de la corona de oro, era una de las Titánides originales, hija de Urano y Gea. Febe acudió al lecho de Ceo y de él concibió a Leto y a Asteria. Recibió el control del oráculo de Delfos de Temis, de acuerdo con algunas pocas fuentes, y posteriormente se lo daría a Apolo.

También se aplicaba su nombre como epíteto a Artemisa en su papel de diosa de la luna, que se consideraba femenina.



</doc>
<doc id="8906" url="https://es.wikipedia.org/wiki?curid=8906" title="Charles Babbage">
Charles Babbage

Charles Babbage (Teignmouth, Devonshire, Gran Bretaña, 26 de diciembre de 1791-Londres, 18 de octubre de 1871) fue un matemático y científico de la computación británico. Diseñó y desarrolló una calculadora mecánica capaz de calcular tablas de funciones numéricas por el método de diferencias. También diseñó, pero nunca construyó, la analítica para ejecutar programas de tabulación o computación; por estos inventos se le considera como una de las primeras personas en concebir la idea de lo que hoy llamaríamos una computadora, por lo que se le considera como "El Padre de la computación". En el Museo de Ciencias de Londres se exhiben partes de sus mecanismos inconclusos. Parte de su cerebro conservado en formol se exhibe en el Royal College of Surgeons of England. Sitio en Londres.

Hay un debate sobre la fecha y lugar de nacimiento de Babbage. En primer lugar, según Dictionary of National Biography seguramente nació en el número 44 de Crosby Row, Walworth en Londres, Inglaterra. En relación a su fecha de nacimiento, The Times publicó que Babbage nació el 26 de diciembre de 1792 pero un sobrino suyo afirmó que fue un año antes, 1791. El registro de la parroquia St. Mary's en Newington establece que Babbage fue bautizado el 6 de enero de 1792, lo cual puede confirmar que hubiese nacido un año antes. 

Babbage fue el cuarto hijo de Betsy Plumleigh Teape y Benjamin Babbage (Socio banquero del empresario William Praed en fundar Praed9s & Co.) 

Cuando Babbage tenía 8 años fue enviado a una escuela de día en Alpington para recuperarse de una peligrosa fiebre. Durante un tiempo pudo atender a la escuela de Enrique VI en Totnes, pero su salud le obligó a seguir recibiendo clase de los tutores privados durante un tiempo. 

Fue después de esto cuando Babbage comenzó a acudir a una Academia en Enfield (Londres), donde las clases se impartían por el reverendo Stephen Freeman. La biblioteca de esta academia incentivó la pasión de Babbage por las matemáticas. Antes de dejar esta academia estudió en dos escuelas privadas, la primera se impartía por un clérigo cerca de Cambridge, a través de la cual conoció al evangélico inglés Charles Simeon, pero no eran tutorías lo que Charles necesitaba. La segunda escuela era un tutor de Oxford, gracias al cual Babbage adquirió el nivel suficiente para ser admitido en Cambridge. Fue entonces cuando le trajeron de vuelta a casa para estudiar en una escuela a la edad de 16 o 17.

En octubre de 1810 Babbage llegó a Trinity College (Cambridge), habiéndose formado de forma autodidacta en matemáticas contemporáneas (A partir de las lecturas de Robert Woodhouse, Joseph-Louis Lagrange y Maria Gaetana Agnesi). Consecuentemente se llevó una gran decepción con la forma de impartir las matemáticas de la universidad. 

En 1812, junto con John Herschel, George Peacock y otros amigos, formaron la "Analytical Society " Paralelamente Babbage era miembro de otras asociaciones como The Ghost Club, asociación de investigación de fenómenos paranormales y "The Extractors Club ," centrado en la liberación de los miembros del manicomio, en caso de que alguna vez sucediese. 

En ese mismo año Babbage fue enviado a Peterhouse, Cambridge, donde se encontraba en la élite de las matemáticas pero no se graduó con notas elevadas sino que recibió e cambio el título sin tener que examinarse en 1814. Defendió una tesis que se consideró "blasfemia" en la disputa pública, no obstante no se sabe si esto estuvo relacionado con el hecho de no llevar a cabo un examen. 

Dada su reputación Babbage consiguió progresar rápidamente. Impartió clases de astronomía en el Royal Institution en 1815 y fue elegido Miembro de la Royal Society en 1816. Sin embargo, al presentarse a las oposiciones tras su graduación no fue aceptado. En 1816 fue candidato para ser profesor en "Haileybury College", con cartas de recomendación de James Ivory y John Playfair, pero fue Henry Walter Bates el que se llevó el puesto. Junto con Herschel visitaron Paris y la Sociedad de Arcueil en 1819, conociendo en este viaje a los principales matemáticos y físicos franceses. Ese mismo año aplicó a la Universidad de Edimburgo recomendado por Pierre-Simon Laplace, perdiendo de nuevo el puesto por William Wallace, matemático y astrónomo escocés.

Babbage compró las tablas actuariales de George Barrett, actuario británico, el cual murió en 1821 dejando trabajos sin publicar y estudió el campo en 1836 en "La Visión Comparativa de varias instituciones de seguros de vida" ("Comparative View of the Various Institutions for the Assurance of Lives)." El interés por este proyecto fue continuado por la idea de crear una compañía de seguros, promovida por Francis Baily debatida en 1824 pero nunca llevada a cabo. No obstante, Babbage sí llegó a calcular las tablas actuariales para esa idea utilizando la mortalidad de la sociedad a partir del año 1792. 

Él y Edward Ryan se casaron con las hermanas Whitmore, Babbage en el año 1814. Se construyó una casa en Marylebone, Londres y formó una gran familia. Durante todos estos años fue su padre quien mantenía sus proyectos siempre con cierta reticencia dada su precocidad a la hora de contraer matrimonio. 

En 1827 su padre murió, lo cual le permitió heredar alrededor de £100.000 de aquel entonces, convirtiéndole en una persona independiente y rica. Ese mismo año su mujer falleció y Babbage decidió emprender un viaje de un año de duración. En ese viaje conoció a Leopoldo II de Toscana en Italia, programando un futuro encuentro en Piedmont. En ese viaje recibió la noticia de que había sido aceptado para la vacante de profesor en Cambridge, puesto que le había sido negado en tres ocasiones distintas.

Babbage intentó encontrar un método por el cual se pudieran hacer cálculos automáticamente por una máquina, eliminando errores debidos a la fatiga o aburrimiento que sufrían las personas encargadas de compilar las tablas matemáticas de la época. Esta idea la tuvo en 1812. Tres diversos factores parecían haberlo motivado: una aversión al desorden, su conocimiento de tablas logarítmicas, y los trabajos de máquinas calculadoras realizadas por Blaise Pascal y Gottfried Leibniz. En 1822, en una carta dirigida a "sir" Humphry Davy en la aplicación de maquinaria al cálculo e impresión de tablas matemáticas, discutió los principios de una máquina calculadora. Además diseñó un plano de computadoras. 

Entre 1833 y 1842, Babbage lo intentó de nuevo; esta vez, intentó construir una máquina que fuese programable para hacer cualquier tipo de cálculo, no solo los referentes al cálculo de tablas logarítmicas o funciones polinómicas. Esta fue la máquina analítica. El diseño se basaba en el telar de Joseph Marie Jacquard, el cual usaba tarjetas perforadas para realizar diseños en el tejido.
Babbage adaptó su diseño para conseguir calcular funciones analíticas. La máquina analítica tenía dispositivos de entrada basados en las tarjetas perforadas de Jacquard, un procesador aritmético, que calculaba números, una unidad de control que determinaba qué tarea debía ser realizada, un mecanismo de salida y una memoria donde los números podían ser almacenados hasta ser procesados. Se considera que la máquina analítica de Babbage fue la primera computadora de la historia. Un diseño inicial plenamente funcional de ella fue terminado en 1835. Sin embargo, debido a problemas similares a los de la máquina diferencial, la máquina analítica nunca fue terminada por Charles. En 1842, para obtener la financiación necesaria para realizar su proyecto, Babbage contactó con "sir" Robert Peel. Peel lo rechazó, y ofreció a Babbage un título de caballero que fue rechazado por Babbage. Lady Ada Lovelace, matemática e hija de Lord Byron, se enteró de los esfuerzos de Babbage y se interesó en su máquina. Promovió activamente la máquina analítica, y escribió varios programas para la máquina analítica. Los diferentes historiadores concuerdan que esas instrucciones hacen de Ada Lovelace la primera programadora de computadoras de la historia.

Charles Babbage ha sido considerado por algunos como el padre de las computadoras modernas, pero sin duda también puede ser considerado el padre de las impresoras modernas. Más de 150 años después de sus planos y un trabajo minucioso del Museo de Ciencias de Londres, dieron como resultado la construcción de la "Máquina Analítica". Los planos del matemático y científico incluían un componente de impresión, el cual ha sido reconstruido por el Museo y es funcional. Esta impresora consta de 8000 piezas mecánicas y pesa aproximadamente 2,5 toneladas.

Fue tan innovadora para su época y podemos apreciarlo hoy, que es capaz de imprimir automáticamente los resultados de un cálculo y un usuario puede cambiar parámetros como espacio entre líneas, elegir entre dos tipografías, número de columnas y otros. Su sofisticación llega a tal punto que puede generar (fabricar) los moldes de las impresiones que podrían ser usados por las imprentas aún hoy en día. Esta impresora lamentablemente no lleva un nombre ya que Babbage la incluyó en sus planos de la "Máquina Analítica", pero basta con aludir a ella como la "impresora de Babbage" para reconocer en este hombre un visionario.

Babbage es recordado también por otras realizaciones. La promoción del cálculo infinitesimal es quizás la primera entre ellas. En 1812, Babbage funda la Sociedad Analítica. La tarea primordial de esta sociedad, conducida por el estudiante Robert Woodhouse, era promover el cálculo leibniziano, o cálculo analítico, sobre el estilo de cálculo newtoniano. El cálculo de Newton era torpe y aproximado, y era usado más por razones políticas que prácticas. La Sociedad Analítica incluía a "sir" John Herschel y George Peacock entre sus miembros. En los años 1815-1817 contribuyó en el «cálculo de funciones» de las "Philosophical Transactions" -transacciones filosóficas-, y en 1816 fue hecho miembro de la Royal Society.

Charles Babbage también logró resultados notables en criptografía. Rompió la cifra auto llave de Vigenère, así como la cifra mucho más débil que se llama cifrado de Vigenère hoy en día. La cifra del auto llave fue llamada «la cifra indescifrable», aunque debido a la confusión popular muchos pensaron que la cifra apolialfabética más débil era indescifrable. El descubrimiento de Babbage fue usado en campañas militares inglesas, y era considerado un secreto militar. Como resultado, el mérito por haber descifrado esta clave le fue otorgado a Friedrich Kasiski, quien descifró también este sistema criptográfico algunos años después.

De 1828 a 1839 Babbage fue profesor de matemáticas en Cambridge. Escribió artículos en distintas revistas científicas, y era miembro activo de la Astronomical Society —sociedad astronómica— en 1820 y de la Statistical Society —sociedad estadística— en 1834. Durante los últimos años de su vida residió en Londres, dedicándose a la construcción de máquinas capaces de la ejecución de operaciones aritméticas y cálculos algebraicos.

Propuso el sistema de franqueo postal que utilizamos hoy en día. Hasta entonces el coste de enviar una carta dependía de la distancia que tenía que viajar; Babbage advirtió que el coste del trabajo requerido para calcular el precio de cada carta superaba el coste del franqueo de esta y propuso un único coste para cada carta con independencia del sitio del país al que era enviada.

Fue el primero en señalar que la anchura del anillo de un árbol dependía de la meteorología que había hecho ese año, por lo que sería posible deducir climas pasados estudiando árboles antiguos.

Inventó el "apartavacas", un aparato que se sujetaba a la parte delantera de las locomotoras de vapor para que las vacas se apartasen de las vías del ferrocarril.

Se interesó también por temas políticos y sociales e inició una campaña para deshacerse de los organilleros y músicos callejeros de Londres, aunque estos pasaron al contraataque y se organizaban en torno a su casa tocando lo más alto que podían.





</doc>
<doc id="8909" url="https://es.wikipedia.org/wiki?curid=8909" title="Eduardo Duhalde">
Eduardo Duhalde

Eduardo Alberto Duhalde (Lomas de Zamora, 5 de octubre de 1941) es un político, abogado y notario (escribano) argentino. Ocupó la vicepresidencia de la Nación durante el primer mandato de Carlos Saúl Menem, aunque renunció a este cargo para asumir como gobernador de la provincia de Buenos Aires; y entre 2002 y 2003 fue presidente de Argentina por aplicación de la Ley de Acefalía. Candidato en las elecciones presidenciales de Argentina de 2011, obtuvo el 5,86 % de los votos emitidos.
Está casado con "Chiche" Duhalde (Hilda Beatriz González de Duhalde, 1946-), quien también participó activamente en política y fue elegida diputada y senadora por la provincia de Buenos Aires.

Hijo de don Tomás Duhalde Gorostegui (1907-1977) y de doña María Esther Maldonado Aguirre (1913-2004), Duhalde militó desde temprana edad en el Partido Justicialista. Se recibió de abogado en la Universidad de Buenos Aires en 1970. Cuatro años más tarde, fue elegido concejal de Lomas de Zamora y por acefalía terminaría siendo Intendente de su ciudad natal, luego de que las presiones de la Unión Obrera Metalúrgica (Sindicato argentino) terminaran con la baja de la intendencia del por entonces Intendente de la ciudad, Pedro Pablo Turner, apoyado por la Juventud Peronista. Bandas armadas presionaron a otros dos Concejales para que renunciaran al puesto, mientras la Triple A (Alianza Anticomunista Argentina) llegaba a Lomas de Zamora para asesinar a Turner y a sus partidarios de la Juventud Peronista, fuertes en la universidad local y en algunos barrios. Durante la intendencia de Duhalde, la Triple A y comandos de la policía provincial llevaron a cabo varios asesinatos y secuestros, entre ellos la masacre de Pasco.

Duhalde fue depuesto por el golpe de Estado de 1976. En el retorno democrático 1983 fue elegido intendente de Lomas de Zamora para el período 1983-1987.Al terminar su mandato como intendente fue elegido diputado nacional (1987-1989).

En el año 1989 participó de las elecciones presidenciales, acompañando como candidato a vicepresidente a Carlos Menem, imponiéndose a la postre sobre la fórmula de la Unión Cívica Radical, Eduardo Angeloz-Juan Manuel Casella. Dos años después renunció a la vicepresidencia y se postuló como candidato a gobernador de la provincia de Buenos Aires.

Su gobierno en la provincia tendría un gran apoyo popular, reflejado en victorias holgadas en varias de las elecciones provinciales posteriores en las cuales la provincia se convertiría en un importante sustento electoral de las victorias nacionales del peronismo.

La gestión de Duhalde en la provincia vio un gran aumento en la construcción e inauguración de edificios, establecimientos y obras públicas consecuencia de una distribución de los fondos de la Coparticipación Federal bastante generosa para con la provincia de Buenos Aires.

Dado que la constitución provincial no contemplaba la posibilidad de la reelección, realizó una reforma constitucional en 1994, al tiempo que se reformaba también la nacional. Los principales partidos opositores de entonces, la Unión Cívica Radical, el Frente Grande y el MODIN, unieron a sus constituyentes en un bloque conjunto para impedir que se oficializara la cláusula reeleccionaria. Pero inesperadamente, el Modín desertó a sus aliados y le dio a Duhalde el apoyo que necesitaba para aprobar el cambio, el cual se sometió sin embargo a un plebiscito que decidiría si se permitiría o no la reelección. El duhaldismo se impuso tanto en la aprobación del "Sí" de dicho plebiscito como en las elecciones de gobernador que tuvieron lugar en 1995 por amplia mayoría.

En 1998 fue una de las organizadoras, junto a Cristina Fernández de Kirchner, Néstor Kirchner y Alberto Fernández, del Grupo Calafate, un "think tank" peronista que tuvo como fin generar una alternativa crítica del menemismo, que desempeñó un papel importante en las campañas electorales de Duhalde en 1999 y Kirchner en 2003.

Fue derrotado por Fernando de la Rúa en las elecciones presidenciales de 1999 tras obtener el 38,28 % de los sufragios, frente al 48,37 % de su opositor.

En octubre de 2001 Duhalde había sido elegido senador nacional por amplio margen. Luego accedió a la presidencia durante la crisis subsiguiente a la renuncia de De la Rúa, en el marco de la crisis económica, social y política que tuvo su clímax cuando el ministro de Economía Domingo Cavallo instauró el corralito. Duhalde es acusado de ser promotor y fogonero de los saqueos a comercios y supermercados en la crisis que llevaría a la renuncia del presidente radical Fernando De la Rúa; él por su parte hace responsable de los mismos a piqueteros y partidos de izquierda.

El 2 de enero de 2002 Duhalde fue elegido presidente de la Nación Argentina por la Asamblea Legislativa. Esa decisión fue tomada a través de un amplio consenso en el peronismo y la oposición para que Duhalde condujera el país, sumido en la confusión de una crisis terminal, en el ínterin preelectoral. Duhalde fue investido por los diputados y senadores con 262 votos a favor, 21 en contra y 18 abstenciones, y con mandato hasta el 10 de diciembre de 2003, esto es, hasta agotar el ejercicio cuatrienal para el que había sido elegido De la Rúa. No habría, por tanto, comicios anticipados, siendo la opinión mayoritaria de los legisladores que lo que urgía era obtener un Ejecutivo estable con el máximo apoyo partidista

Duhalde ―que en vísperas de la asunción presidencial había expresado su temor a que se produjera una "guerra civil" en Argentina― empezó por reconocer que el país estaba "quebrado" y "fundido", y anunció un Gobierno de unidad nacional con la triple misión de "reconstruir la autoridad política e institucional, garantizar la paz social y sentar las bases para el cambio del modelo económico y social".

Entre las medidas de su gobierno interino se destacan la búsqueda de la pacificación del país a través de instrumentos como el Diálogo Argentino, de distintas medidas económicas tendientes a la reactivación de una economía argentina que venía de sufrir varios años de recesión: devaluación de la moneda, que dio fin a la Ley de Convertibilidad, la pesificación forzada de los depósitos bancarios en moneda extranjera, y una serie de medidas sociales tendientes a atenuar los efectos de una economía recesiva que había incrementado la pobreza e indigencia hasta índices nunca vistos antes en la Argentina.

Su plan económico productivista permitió que la economía argentina cambiara radicalmente, sobre todo a partir del segundo semestre del 2002. Ya en el arranque de 2003 los efectos positivos del cambio de rumbo económico impulsado por Duhalde y gestionado por Roberto Lavagna ya estaban haciéndose notar. La actividad económica resurgía gracias a que el peso devaluado estaba espoleando el comercio exportador y la producción industrial local en detrimento de las importaciones de bienes, de manera que la caída registrada en 2002 del 10,9 % del PIB, dio paso a un crecimiento del 5 % en el primer trimestre de 2003.

El 26 de junio de 2002, en las inmediaciones de la estación ferroviaria de la ciudad de Avellaneda, en el conurbano de la provincia de Buenos Aires, Argentina se produjo la represión de una manifestación de grupos piqueteros. En la persecución fueron asesinados por efectivos de la Policía Bonaerense los jóvenes activistas Maximiliano Kosteki y Darío Santillán pertenecientes al "Movimiento de Trabajadores Desocupados (MTD) Guernica" y el "MTD Lanús", respectivamente, nucleados en la "Coordinadora de Trabajadores Desocupados Aníbal Verón". Además se registraron 33 heridos por balas de plomo entre los manifestantes. Ante el impacto generado, Duhalde anticipó seis meses el llamado a elecciones presidenciales.

En materia de política exterior, se recuerda el tajante rechazo de su gobierno al golpe de Estado en Venezuela de 2002, las posturas diplomáticas argentina y cubana fueron de importancia para aislar internacionalmente al Gobierno "de facto".

El 27 de agosto de 2002, el "New York Times" publicó una noticia falsa titulada "Algunos en Argentina ven la secesión como una respuesta al peligro económico" ("Some in Argentina See Secession As the Answer to Economic Peril"), afirmando que existía un movimiento para declarar la independencia de la Patagonia, poniendo énfasis en el hecho de que la mayor parte de la tierra patagónica y de sus recursos naturales estaba en manos extranjeras, como los grupos Benetton y Turner. La operación, de acuerdo al "New York Times", se completaría con una base militar de Estados Unidos en Tierra del Fuego y la renuncia a los derechos argentinos en la Antártida, en pago de la deuda externa.

Para las siguientes elecciones presidenciales del 27 de abril de 2003, Duhalde decidió dar su apoyo ―junto con su bastión electoral, el gran Buenos Aires (que concentra al 38% de electores de todo el país)― a Néstor Kirchner, quien resultó electo presidente.

Una vez en la presidencia (desde el 25 de mayo de 2003) y tras un período inicial de cordialidad, Kirchner se enfrentó políticamente a Duhalde, derrotándolo en su bastión de la provincia de Buenos Aires en las elecciones legislativas de octubre de 2005. No obstante, "Chiche" Duhalde (la esposa del expresidente) alcanzó a obtener una banca en el Senado y también logró que varios candidatos afines ingresaran a la Cámara de Diputados.

En diciembre de 2003, Eduardo Duhalde asumió como titular de la Comisión de Representantes Permanentes del Mercosur , tarea que desempeñó hasta el año 2005. Durante su gestión se destaca la creación de la Comunidad Sudamericana de Naciones que luego devengaría en la Unasur.

En las elecciones presidenciales de 2011, Duhalde alcanzó un magro resultado con el respaldo del partido Unión Popular (enfrentado al kirchnerismo). Si bien en las primarias de agosto de 2011 finalizó en tercer lugar con el 12,12% de los votos -apenas unas milésimas por debajo de Ricardo Alfonsín-, luego en las elecciones generales de octubre del mismo año alcanzó apenas un 5,86 %.

Actualmente, Eduardo Duhalde encabeza el Movimiento Productivo Argentino, entidad dedicada al mundo económico con un perfil productivista de la cual es miembro fundador junto a otros políticos como Raúl Alfonsín (1927-2009) y una gran cantidad de empresarios.

En agosto de 2008, Eduardo Duhalde afirmó que el titular del Partido Justicialista, Néstor Kirchner, le hacía acordar «al Führer [por Adolfo Hitler<nowiki>]</nowiki> y a Mussolini».
Recordó lo que llamó su «primer enfrentamiento público» con Kirchner, cuando «él criticaba a Macri y yo le dije que no había que criticarlo a Macri: había que criticar la falta que le hacen al país 400 empresarios como Macri».

Al día siguiente ―tras una retahíla de reacciones contrarias desde todo el espectro político― tuvo que retractarse de sus dichos y «se mostró arrepentido».

Durante el Gobierno de Mauricio Macri, quien asumió el 10 de diciembre de 2015, el expresidente Eduardo Duhalde comenzó a postular la idea de un "cogobierno" que contemple a la oposición. "Parto de la idea de que el esquema gobierno-oposición está caducada y que las convicciones que resume la frase 'El que gana gobierna y el que pierde acompaña' ya no son útiles en nuestros tiempos", postuló en distintas notas de opinión. Su propuesta fue: "Para superar los incontables inconvenientes que encuentran los gobiernos de todo signo, debemos avanzar hacia un sistema donde el ganador de una elección conduce y los otros partidos con representación parlamentaria integran un cogobierno". 

A pesar de que Duhalde escribió dos libros sobre el tema de la lucha contra las drogas ―"Hacia un mundo sin drogas" (1994) y "Política, familia, sociedad y drogas" (1997)―, varias personalidades de la política vincularon a Eduardo Duhalde con el narcotráfico.

En 2005, la diputada Elisa Carrió acusó a Eduardo Duhalde de ser «el mayor responsable político de la droga en el país». Afirmó públicamente que «Duhalde controlaba la droga en la provincia de Buenos Aires». Duhalde demandó a Carrió por calumnias e injurias. En 2009, la jueza federal María Servini de Cubría convocó a ambos contendientes. En su descargo, Carrió afirmó que no había culpado a Duhalde de ser narcotraficante, ni le había imputado un delito, sino que le adjudicó responsabilidad política en el asunto. Durante el encuentro, la diputada ratificó que Duhalde controlaba la droga en la provincia de Buenos Aires, aunque aclaró que si sus dichos afectaron al expresidente, ella le pedía perdón si se había sentido ofendido. Carrió aseguró que no se retractó de sus dichos. La jueza decidió realizar un sobreseimiento en el juicio.

Sin embargo jamás se ha podido demostrar nada al respecto. Los juicios iniciados por el expresidente a cada una de las personas que lo han tratado de vincular con las drogas finalizaron demostrando que no solo Duhalde era inocente, sino que además sus detractores habían mentido de manera orgánica y armónica.

Entre las personas a las cuales les ha iniciado juicio y luego tuvieron que retractarse por decisión judicial figuran Elisa Carrió y Luis D’Elía quien fue condenado a pagarle 150 000 pesos argentinos por sus dichos.

En 2010, Duhalde escribió "Es hora de que me escuchen. El peligro de los narcoestados" (refiriéndose al problema de las drogas en la Argentina).




</doc>
<doc id="8911" url="https://es.wikipedia.org/wiki?curid=8911" title="Presidente de la Nación Argentina">
Presidente de la Nación Argentina

El presidente de la Nación Argentina es el jefe de Estado, jefe de Gobierno y titular del Poder Ejecutivo Nacional, responsable político de la administración general de la República Argentina y comandante en jefe de las Fuerzas Armadas. El actual presidente es Alberto Fernández, de la alianza Frente de Todos, que tomó posesión el 10 de diciembre de 2019.

Entre otros poderes y responsabilidades, el Artículo 99 de la Constitución de la Nación Argentina encarga al presidente «expedir las instrucciones y reglamentos que sean necesarios para la ejecución de las leyes»; hace del presidente el comandante en jefe de las Fuerzas Armadas, lo autoriza a nombrar oficiales ejecutivos y judiciales, lo sitúa al frente de la política exterior de Argentina, le permite conceder indultos o moratorias, aprobar o vetar leyes, introducir legislación mediante decretos de necesidad y urgencia y declarar el estado de sitio y la intervención federal en el país.

El presidente es elegido mediante el sufragio directo con posibilidad de una segunda vuelta electoral para un mandato de cuatro años. Desde la reforma de la Constitución Argentina de 1994 el mandatario tiene la posibilidad de una reelección inmediata, pudiendo repetir nuevamente el mandato después de transcurrido un período. En caso de muerte, destitución o renuncia de un presidente, el vicepresidente asume la presidencia.

Hasta la fecha, hubo un total de cincuenta personas que asumieron el título y cincuenta y tres presidencias —esto debido a que Julio Argentino Roca, Hipólito Yrigoyen y Juan Domingo Perón fueron reelectos transcurrido un periodo presidencial como mínimo—. De ellas, solo dos fueron mujeres: María Estela Martínez de Perón y Cristina Fernández de Kirchner. De las personas elegidas para el cargo doce fueron dictadores que se autodenominaron «presidente», usurpando los poderes ejecutivo y legislativo, tanto nacionales como provinciales y en algunos casos también el poder constituyente, bajo la doctrina de los gobiernos "de facto" de la Corte Suprema. Además, siete vicepresidentes y cinco ciudadanos por la Ley de acefalía —dos presidentes provisorios del Senado, un presidente de la Cámara de Diputados, un gobernador y un senador— asumieron el cargo tras la falta de un presidente. En cuanto al término del mandato por el que fueron elegidos, seis fueron derrocados por golpes de Estado, tres murieron por causas naturales y doce renunciaron.

El primer presidente fue Bernardino Rivadavia con el título de «presidente de las Provincias Unidas del Río de la Plata», creado por ley del Congreso del 6 de febrero de 1826. Después de su renuncia desapareció el cargo en la legislación argentina hasta que en 1853 fue restablecido por una nueva constitución, disponiendo que el mandato tendría una duración de seis años, sin posibilidad de reelección inmediata; la designación se hacía por votación indirecta en un colegio electoral que votaba separadamente en cada provincia y la Municipalidad de Buenos Aires, y si ningún candidato lograba más de la mitad de los votos, decidía el Congreso entre los dos más votados. Justo José de Urquiza fue el primer elegido de acuerdo con el nuevo régimen y desempeñó el cargo como «presidente de la Confederación Argentina», al igual que su sucesor, Santiago Derqui, quien luego de las reformas constitucionales de 1860 asumió como «presidente de la Nación Argentina», título vigente hasta la fecha. Adolfo Rodríguez Saá fue el presidente que menos tiempo permaneció en el cargo, con tan solo 7 días, siendo Julio Argentino Roca el que permaneció por más tiempo en el cargo, con sus 12 años en el puesto.

Los orígenes de la Argentina como nación se remontan a 1776 cuando, en el marco de las llamadas reformas borbónicas, el rey de España creó el Virreinato del Río de la Plata –que abarcaba aproximadamente los territorios de las actuales Argentina, Bolivia, Paraguay, Uruguay y sur de Brasil– separándolo del Virreinato del Perú. El jefe de Estado seguía siendo el rey, representado localmente por el virrey que, en general, eran nacidos en España.

La Primera Junta de Gobierno, oficialmente Junta Provisional Gubernativa de las Provincias del Río de la Plata a nombre del Señor Don Fernando VII, fue la Junta de gobierno surgida el viernes 25 de mayo de 1810 en Buenos Aires, capital del Virreinato del Río de la Plata, como consecuencia del triunfo de la Revolución de Mayo que destituyó al virrey Baltasar Hidalgo de Cisneros. La sede del gobierno fue fijada en el Fuerte de Buenos Aires, que sirviera desde 1776 como residencia de los virreyes y donde hoy se encuentra la Casa de Gobierno. La Primera Junta existió como tal hasta el 18 de diciembre del mismo año, ya que con la incorporación de diputados del interior se transformó en la Junta Grande.

El presidente de la Primera Junta de las Provincias Unidas del Río de la Plata fue Cornelio Saavedra.

Cuando se unieron representantes de otras ciudades del interior y luego el gobierno se delegó primero en un triunvirato y luego en un poder ejecutivo unipersonal con el nombre de director supremo creado por la Asamblea Nacional de 1813.

El cargo de director supremo se mantuvo cuando, luego de declarada la independencia el 9 de julio de 1816 por un Congreso reunido en la ciudad de San Miguel de Tucumán, se aprobó una Constitución en 1819, pero debido a circunstancias políticas, la misma nunca entró en vigor, el poder central se disolvió y el país quedó como una confederación de provincias.

Una nueva constitución aprobada en 1826 creó por primera vez el cargo de presidente, para el cual fue elegido Bernardino Rivadavia, el primer presidente argentino. Debido a la guerra entre Argentina y Brasil, Rivadavia renunció después de un breve período de tiempo, y la oficina se disolvió poco después.

Una guerra civil entre «unitarios» (unitaristas, es decir gobierno central con sede en Buenos Aires) y «federales» (federalistas con plena autonomía de las provincias) se produjo en las décadas siguientes. En este momento, no había una autoridad central y lo más cercano a ello era el representante de relaciones exteriores, por lo general el gobernador de la Provincia de Buenos Aires. El último en llevar este título fue Juan Manuel de Rosas que, en los últimos años de su gobierno, fue elegido jefe supremo de la Confederación Argentina, adquiriendo poder efectivo en el resto del país.

En 1852, Rosas fue derrocado y se convocó a una asamblea constituyente. Esta constitución, aún en vigor, estableció un gobierno federal nacional, con la oficina del presidente, electo mediante el Colegio electoral. El período de mandato se fijó en seis años, sin posibilidad de reelección. El primer presidente elegido en virtud de la Constitución fue Justo José de Urquiza. Después de una breve interrupción en 1860, la sucesión de presidentes se realizó respetando las normas legales hasta que a partir de 1930 fue interrumpida por varios golpes de Estado, con lo cual se intercalaron presidentes "de facto" y otros legalmente elegidos.

En 1930, 1943, 1955, 1962, 1966 y 1976, golpes militares depusieron los presidentes electos. En 1930, 1943 y 1955 las Fuerzas Armadas designaron militares con el título de presidente. Como resultado de los golpes de Estado de 1966 y de 1976, el gobierno federal fue ejercido por una junta militar integrada por los jefes de cada una de las tres ramas de las Fuerzas Armadas —Ejército, Fuerza Aérea y Armada— la que, a su vez, designó a un militar como presidente. En 1962, antes que los militares alcanzaran a nombrar un nuevo presidente, asumió ese cargo el presidente provisional del Senado que era el reemplazante legal del presidente depuesto.

Es discutible si estos jefes de estado militares adecuadamente pueden ser llamados presidentes, dada la ilegitimidad de sus gobiernos. La posición del gobierno argentino actual es que los militares que desempeñaron el Poder Ejecutivo entre 1976 y 1983 no fueron explícitamente presidentes legítimos, por lo cual no se le ha reconocido derecho a una pensión presidencial. La situación de los anteriores presidentes militares no está definida, si bien todos ellos en la actualidad ya han fallecido.

La Constitución de 1826 establecía que el poder ejecutivo es ejercido por una persona bajo el título de "presidente de la República Argentina" (artículo 68). Los requisitos eran ser ciudadano argentino, tener treinta y seis años, nueve como ciudadano y un capital de diez mil pesos (artículos 24 y 69). Tenía un mandato de cinco años y no podía ser reelecto (artículo 71). En caso de enfermedad, muerte, renuncia o destitución el cargo era ejercido por el presidente del senado (artículo 72). Era elegido de la siguiente forma (artículos 73 al 80): En la capital y en cada provincia se formaba una junta de 15 electores quienes votaban cuatro meses antes de que finalice el mandato con «balotas firmadas». Una vez terminada la votación y el escrutinio, el acta iba dirigida al presidente del Senado quien junto a cuatro miembros del congreso hacían el conteo final. El que reunía las dos terceras partes de los votos era proclamado presidente. En caso de que ninguno lo reuniere, era elegido por los 2/3 del congreso. Podía ser destituido por acusación de la Cámara de Representantes por «delitos de traición, concusión, malversación de los fondos públicos, violación de la Constitución, particularmente con respecto a los derechos primarios de los ciudadanos, u otros crímenes que merezcan pena infamante o de muerte». y ser destituido por el senado

Sus atribuciones eran (artículos 81 al 101) publicar y hacer ejecutar las leyes, convocar al congreso, hacer anualmente la apertura de sesiones, ordena las elecciones legislativas, ser el comandante de las fuerzas de mar y tierra necesitando el permiso del congreso para mandar al ejército en persona, proveer la seguridad interior y exterior, tomar medidas para garantizar la paz, hacer tratados con aprobación del senado, nombrar y destituir a sus cinco ministros, nombrar embajadores y demás agentes con aprobación del senado, recibir delegaciones extranjeras, expedir las cartas de ciudadanía, ejercer el patronato general de las iglesias, «Todos los objetos y ramos de Hacienda y Policía, los establecimientos públicos, y nacionales, científicos y de todo género, formados y sostenidos con fondos del Estado las casas de moneda, Bancos nacionales, correos, postas y caminos son de la suprema inspección y resorte del Presidente de la República», aplicar indultos y nombrar jueces de la corte suprema. Además, nombraba a los gobernadores de las provincias a propuesta de un Consejo de Administración y aprobaba sus presupuestos. Todo proyecto de ley debía pasar por el poder ejecutivo quien las aprobaba u objetaba.

Los artículos 71.º a 90.º contenían las estipulaciones relativas al poder ejecutivo. El titular de este era unipersonal, y llevaba el título de "Presidente de la Confederación Argentina". Un vicepresidente, electo juntamente con él, lo supliría en caso de ausencia, inhabilidad o renuncia.

Los requisitos para la elección como presidente eran similares a los exigidos para los senadores; se les añadía la condición de nativo, o de ser hijo de uno en caso de haber nacido fuera del territorio nacional, y la práctica de la religión católica, única concesión a los "montoneros". Su mandato se extendería por un período de seis años, sin posibilidad de reelección hasta que un período completo hubiese pasado; ninguna causa permitía la extensión de este más allá de los seis años cumplidos desde la fecha original de asunción.

El procedimiento para la elección presidencial era indirecto; el electorado de cada provincia escogería un número de delegados, igual al doble de la cantidad total de diputados y senadores que se eligiesen por la misma. Los electores de cada provincia votarían discrecionalmente a los candidatos que juzgasen más convenientes, y remitirían copia sellada de su resolución al Senado de la Nación; una vez recibidas todas las listas, la Asamblea Legislativa realizaría el escrutinio de estas. De haber como resultado mayoría absoluta de un candidato, la proclamación sería automática. En caso de no contar ninguno con la misma, la Asamblea Legislativa elegiría inmediatamente y a simple pluralidad de sufragios entre los dos candidatos más votados, o más en caso de haber empate en el primer o segundo puesto. En este último caso, de no haber candidato con mayoría absoluta en primera instancia, se realizaría balotaje entre los dos candidatos más votados en la primera vuelta. El "quorum" para esta elección era de tres cuartas partes de los congresistas.

De acuerdo con el primer inciso del artículo 90.º, el presidente era la autoridad suprema de la Confederación, en lo que se denomina un régimen "presidencialista": no respondía de sus acciones, dentro del marco impuesto por la Constitución, a ninguna autoridad superior, y no requería de la aprobación del Congreso para el ejercicio de las atribuciones que le competen. Era además el titular del poder ejecutivo de la ciudad designada capital federal, y el jefe de las fuerzas armadas.

El presidente gozaba de facultades legislativas: además de la sanción y promulgación de las leyes dictadas por el Congreso, incluyendo la facultad de veto, estaba a su cargo la expedición de los reglamentos necesarios para la aplicación de la ley, llamados "decretos", aunque respetando el espíritu original de la misma. La firma de tratados con otros estados estaba a su exclusivo cargo, así como la decisión de dar o no trámite a los documentos emitidos por el pontífice católico.

Como autoridad en materia de política exterior, es el encargado del nombramiento de embajadores y otros ministros destinados a la negociación con las potencias extranjeras; la elección y remoción de los titulares de embajada requería acuerdo senatorial —un vestigio de la influencia de la constitución norteamericana, en la que el Senado comparte con el presidente la potestad sobre las relaciones exteriores, sobre los convencionales—, pero la de los funcionarios de rango inferior estaba enteramente a su cargo. Por lo mismo, era la autoridad a cargo de la gestión de los asuntos militares, disponiendo del ejército, designando a los oficiales de este —con acuerdo del Senado, en caso de los puestos superiores del escalafón—, emitiendo patentes de corso, declarando la guerra o decretando el estado de sitio cuando su causa es el ataque de una potencia extranjera.

Su implicación con las tareas del Congreso no se limitaba a la promulgación de las leyes: estaba a cargo del presidente la apertura de las sesiones en Asamblea Legislativa, en la que comunicaba al mismo sus consideraciones acerca de su tarea, y la prórroga o convocatoria a sesiones fuera del período ordinario.

Con respecto al poder judicial, estaba a su cargo la designación de los jueces de los tribunales federales, para lo que requería el acuerdo senatorial; además, contaba con la facultad de indultar a los condenados por delitos de jurisdicción federal, salvo en casos de juicio político. No tenía la facultad de imponer condenas, pero sí de —en estado de sitio— decretar el arresto temporal o el traslado de personas, salvo que estas prefiriesen abandonar el territorio nacional. Si no contaba con el acuerdo del Congreso al dictarlas, estas medidas caducaban automáticamente a los 10 días.

Como encargado de la administración nacional, le estaba encomendada la recaudación de la renta nacional y su aplicación, dentro del marco de la ley de presupuesto; tenía facultad para otorgar el goce de licencias o montepíos, y para recabar cualquier clase de información por parte de la administración nacional.

La Constitución fijaba como ayudantes del presidente a cinco ministros, elegidos por este, en carteras de Interior, de Relaciones Exteriores, de Hacienda, de Justicia, Culto e Instrucción Pública, y de Guerra y Marina. El refrendo ministerial era necesario para los decretos de gobierno. Los ministros estaban además obligados a dar informes al Congreso en la apertura de sesiones, y facultados a tomar parte en los debates de este, aunque sin voto. La tarea era incompatible con el ejercicio del poder legislativo nacional.


El presidente de la Nación tiene las siguientes atribuciones:

El presidente es el jefe supremo de la Nación, jefe de gobierno y responsable político de la administración general del país (inciso 1), y está a la cabeza del poder ejecutivo del gobierno, cuya responsabilidad es «Expedir las instrucciones y reglamentos que sean necesarios para la ejecución de las leyes» (inciso 2). Para llevar a cabo este deber, se le otorga el control de los cuatro millones de empleados del poder ejecutivo federal.

Al presidente le corresponde el nombramiento y remoción de varios miembros del poder ejecutivo. Embajadores (según la ley 20.957, el presidente puede designar hasta 25 embajadores de su confianza sin aprobación del Senado, que deben dejar sus funciones cuando el presidente termina su mandato), ministros plenipotenciarios, encargados de negocios, el presidente del Banco Central de la República Argentina, el procurador General de la Nación y el director de la Agencia Federal de Inteligencia son todos designados por el presidente con el «consejo y consentimiento» de una mayoría del Senado; y por sí solo nombra y remueve a miembros del y otros oficiales federales (inciso 7). Los nombramientos realizados mientras el Senado no está en periodo de sesiones son temporales y expiran al final de la siguiente sesión del Senado (inciso 19).

Supervisa el ejercicio de sus ministros (inciso 10) y puede pedir los informes que crea convenientes (inciso 17). Generalmente, el presidente puede cesar y llenar vacantes a los funcionarios ejecutivos a su discreción (inciso 19).

Quizás el más importante de todos los poderes presidenciales es su posición al frente de las Fuerzas Armadas argentinas como su comandante en jefe (inciso 12). Mientras que el poder de declarar la guerra corresponde constitucionalmente al Congreso (inciso 15), el presidente comanda y dirige a sus ejércitos y es responsable de planear la estrategia militar y concesionar empleos y grados militares (incisos 13 y 14).

Junto con las fuerzas armadas, el presidente también está al frente de la política exterior. A través del Ministerio de Relaciones Exteriores y el Ministerio de Defensa, el presidente es responsable de la protección de los argentinos en el extranjero y de los ciudadanos argentinos en territorio nacional. El presidente decide si hay que reconocer nuevas naciones y nuevos gobiernos, recibe sus ministros y admite cónsules, y negocia tratados con otras naciones, que se hacen vigentes en Argentina cuando son aprobados por las dos terceras partes del Senado (inciso 11). El presidente también puede negociar «acuerdos ejecutivos» con poderes extranjeros que no están sujetos a la confirmación del Senado.

Concede jubilaciones, retiros, licencias y pensiones (inciso 6); y puede ausentarse del territorio nacional, con permiso del Congreso (inciso 18).

En materia de seguridad interior, el presidente está facultado para declarar:

El primer poder conferido al presidente por la Constitución es el poder legislativo del veto presidencial. Cualquier proyecto de ley aprobado por el Congreso deberá ser presentado al presidente antes de que pueda convertirse en ley. Una vez que la norma legal ha sido presentada, el presidente tiene tres opciones:
Luego las promulga y hace publicarlas en el Boletín Oficial. El presidente puede emitir decretos que regulen la organización política del país y reglamenten las leyes aprobadas por el Congreso, firmados junto al jefe de Gabinete y los ministros involucrados. 

El presidente tiene prohibido emitir disposiciones legislativas, excepto en circunstancias excepcionales, a través de: 



El presidente puede desempeñar un papel importante en la conformación del Congreso, sobre todo si el partido político del presidente tiene mayoría en una o ambas Cámaras. Los miembros del poder ejecutivo no pueden ocupar simultáneamente su puesto y un escaño en el Congreso, pero es habitual que redacten la legislación y que un Senador o Diputado la presente por ellos. 

El presidente puede influir de una forma importante en el poder legislativo a través del informe anual, escrito u oral al hacer la apertura de las sesiones ordinarias, que constitucionalmente debe presentar al Congreso reunido en Asamblea, normalmente dictado el 1 de marzo de cada año. Este discurso a menudo perfila la oferta legislativa para el año próximo (inciso 8). De acuerdo con los artículos 63 y 99 inciso 9, el presidente puede convocar a una o a ambas Cámaras del Congreso para una sesión extraordinaria mediante un decreto, indicando los proyectos de ley a tratar.

El presidente también tiene la facultad de proponer jueces federales, incluidos miembros de la Corte Suprema de Argentina, sobre la base de una propuesta vinculante del Consejo de la Magistratura (inciso 4). Sin embargo, estos nombramientos requieren la confirmación del Senado por dos tercios de los miembros presentes y esto puede suponer un escollo importante ante la posibilidad de que un presidente quisiera formar una judicatura federal con una postura ideológica particular. También puede conceder perdones e indultos, pero no intervenir en acusaciones de la Cámara de Diputados (inciso 5).

En la Constitución de 1994, en su artículo 89 de la sección segunda, sobre el poder ejecutivo, marca los requisitos para ser presidente:

La campaña presidencial contemporánea comienza antes de las elecciones primarias, cuando los partidos políticos pueden hacer una selección de candidatos. En las elecciones primarias, quedan habilitados a las elecciones generales quienes pasan el piso del 1,5 % del padrón electoral.

Desde 2015, los candidatos participan en debates televisados a escala nacional, y fueron regulados como obligatorios a partir de 2016. Los candidatos de cada partido habitualmente hacen campaña a lo largo de todo el país para explicar sus programas electorales, convencer a los votantes y solicitar contribuciones a la campaña.

Entre 1853 y 1994, el presidente fue elegido mediante el voto indirecto, es decir, se elegían electores para el Colegio Electoral que permitía ganar por amplias mayorías, ya que, con excepción de la elección de Domingo Faustino Sarmiento en 1868, en todas las demás el candidato ganador había logrado la mayoría en el Colegio en la elección. La elección a presidente era a voto cantado y con fraude, siendo necesario la mayoría absoluta (la mitad más uno de los electores) para ser presidente. La sanción de la Ley Sáenz Peña en 1912 instauró el voto secreto y obligatorio entre hombres mayores de 18 años usando el padrón militar, siendo aplicado en las elecciones de 1916, 1922 y 1928 ganadas por la Unión Cívica Radical, y en 1946 ganadas por las fuerzas que posteriormente formarían el Partido Justicialista.

Tras el golpe de Estado de 1930, las elecciones de 1931 y 1937 se realizaron con fraude electoral y el boicot de la Unión Cívica Radical (partido gobernante antes del golpe y mayoritario en el país), hasta 1935. Con la reforma de 1949, se eliminó el colegio electoral y se instauró el voto directo con la inclusión de las mujeres mediante una ley de 1947, siendo necesario obtener simplemente la mayor cantidad de los votos positivos para resultar electo. Así fue reelecto Juan Domingo Perón en 1951. El golpe militar de 1955 reinstauró el sistema electoral anterior con la proscripción de los partidos Peronista y Comunista, aplicándolo a las elecciones de 1958 y 1963. En 1972, la dictadura militar gobernante instauró el voto directo universal, necesitando el 50 % de los votos afirmativos con posibilidad de balotaje entre las fórmulas que sacaron más del 15 % de los votos y el fin de las proscripciones. Esto se aplicó a las elecciones de marzo y septiembre de 1973. Tras el fin del Proceso de Reorganización Nacional, dictadura que gobernaba desde 1976, las elecciones de 1983 y 1989 se llevaron a cabo mediante el voto indirecto universal con colegio electoral necesitando la mayoría absoluta de los electores para ser presidente.

En 1994 se reformó la Constitución y dispuso, que el candidato es elegido directamente por el pueblo en doble vuelta (Art. 94). La convocatoria deberá hacerse con una anticipación no menor de noventa días y la elección será efectuada dentro de los dos meses anteriores a la finalización del mandato del presidente y vicepresidente saliente (Art. 95). En la primera vuelta, si la fórmula obtiene el 45 % más uno de los votos, u obteniendo 40 % supera por 10 % al segundo, computando únicamente los votos afirmativos, esto es excluyendo los votos en blanco o nulos, sus integrantes serán proclamados como presidente y vicepresidente (Arts. 97 y 98). Las Juntas Electorales dentro de 10 días corridos deberán informar al Presidente del Senado el resultado de la elección, quien convocará a Asamblea Legislativa para proclamar la fórmula electa (Art. 120 del Código Nacional Electoral). Si ninguna fórmula cumple los requisitos luego del anuncio del resultado por la Asamblea Legislativa se llevará a cabo la segunda vuelta entre las dos fórmulas más votadas, a los 30 días posteriores a la elección, que será proclamada por mayoría simple de los votos afirmativos. Las fórmulas deben confirmar su participación a la Junta Electoral de la Capital Federal dentro del quinto día de la proclamación de la Asamblea, de lo contrario la otra fórmula será electa (Arts. 150, 151, 152 C.N.E.).

La Constitución Nacional establece que la toma de posesión del cargo presidencial se realiza ante el Congreso Nacional reunido en Asamblea Legislativa (art. 93). La Constitución establece también que en ese momento, tanto el presidente como el vicepresidente deben prestar juramento, respetando sus creencias religiosas, de "desempeñar con lealtad y patriotismo el cargo" y "observar y hacer observar fielmente la Constitución" (art. 93). Dicho juramento debe ser prestado "en manos del presidente del Senado" (art. 93).

No existen otras normas legales sobre la manera en que debe realizarse la asunción presidencial. La formalización del mismo ha sido tradicionalmente realizada mediante un acta autenticada por el escribano oficial de la Presidencia.

Históricamente, los presidentes han establecido diversas ceremonias adicionales, definidas mediante reglamentos de ceremonial que no tienen valor normativo. La gran cantidad de golpes de Estado sufridos por Argentina en el siglo XX, hizo que muchas de las ceremonias de asunción del poder se realizaran en la Casa Rosada, debido a que el Congreso Nacional había sido clausurado. Luego del golpe de Estado de 1930 y hasta 1989, sólo hubo una transmisión por finalización del mandato constitucional: en 1937 con la transmisión del mando de Agustín P. Justo a Roberto M. Ortiz.

La ceremonia de asunción del mando incluye los denominados "atributos presidenciales", tradición que se remonta a 1813, cuando se realizó la primera asamblea constituyente del naciente estado argentino. Diversos atributos presidenciales se establecieron consuetudinariamente a lo largo de la historia, tres de los cuales han persistido en los últimos años. El primero fue la banda presidencial, tradición que se remonta a 1824. El segundo fue la marcha de Ituzaingó, cuya tradición se remonta a 1827, para conmemorar la victoria argentina en la Guerra contra Brasil. La tercera fue el bastón, tradición que se remonta a 1932.

El momento, el lugar y la persona que entrega los atributos presidenciales ha variado a lo largo del tiempo, e incluso según cada presidente. Ha sido una práctica constante, luego de recuperada la democracia en 1983, que el presidente entrante, inmediatamente después de asumir el mando, pronuncie un discurso ante la Asamblea Legislativa.

De acuerdo con la reforma aprobada en 1994, la duración del mandato del presidente es de "cuatro años con posibilidad de reelección inmediata por otros cuatro años". Una persona que cumplió dos mandatos consecutivos queda habilitada para otra reelección una vez transcurrido al menos un período presidencial desde que dejó el cargo. Estas restricciones se aplican en la misma forma para quienes hayan desempeñado como vicepresidentes en uno o en los dos períodos.

Según la Constitución de 1853 y hasta 1994, el presidente tenía mandato por "seis años", sin posibilidad de reelección consecutiva. La reforma de 1949 permitía la reelección sin limitación alguna, pero fue dejada sin efecto por resolución del gobierno militar surgido en 1955, que ratificó la convención constituyente de 1957, con lo cual se retornó al régimen de 1853. El gobierno surgido del golpe militar de 1966 limitó la duración del mandato a "cuatro años" con una reelección consecutiva mediante un estatuto transitorio en 1972 que solo se aplicó para una elección y luego no fue ratificado.

Sólo hubo cinco personas reelectas en el cargo, Julio Argentino Roca en 1898 e Hipólito Yrigoyen en 1928 por la reforma constitucional de 1898, Juan Domingo Perón en 1952 por la reforma de 1949 y en 1973 por la reforma de 1957, y Carlos Saúl Menem en 1995 y Cristina Fernández de Kirchner en 2011 por la reforma de 1994.

Desde 2015 se encuentra discutido en la Argentina el momento preciso en que comienza y finaliza el mandato presidencial. Hasta ese momento, la práctica era que el mandato presidencial comenzaba en el momento que el presidente electo juraba ante la Asamblea Legislativa y finalizaba cuatro años después, cuando su sucesor realizaba el juramento. La tradición ha sido que el juramento presidencial se realizara durante el día de cambio de mando, de tal modo que el presidente saliente gobernara hasta la entrega del mando y el presidente entrante gobernara desde la entrega del mando.

En 2015 la presidenta saliente, Cristina Fernández de Kirchner, y el presidente entrante, Mauricio Macri, no se pusieron de acuerdo sobre el lugar en que debía realizarse la entrega del bastón de mando. La norma vigente establecía que debía realizarse en el Congreso de la Nación, pero el presidente electo deseaba que se realizara en la Casa Rosada. El hecho tenía un significado simbólico: luego de la Crisis de 2001, el presidente Eduardo Duhalde había decidido realizar en 2003, la entrega del bastón en el Congreso, con el fin de fortalecer la imagen del Poder Legislativo y asociarla a la democracia. Esa decisión fue mantenida por todos los presidentes posteriores.

En 2015 Macri pidió el cambio de la norma, para realizar la entrega del bastón en la Casa Rosada, pero Cristina Fernández denegó el pedido. Si, como había sucedido tradicionalmente, el cargo de presidente se asumía al momento del juramento, la facultad de establecer las formalidades de la transmisión del mando correspondía al presidente saliente. Mauricio Macri inició entonces un proceso judicial solicitándole a la jueza electoral María Romilda Servini que prohibiera a la presidenta Cristina Fernández ejercer la presidencia a partir de las 00:00 horas del 10 de diciembre de 2015.

Apoyada en el dictamen del fiscal Di Lello, la jueza Servini estableció que el mandato presidencial y la asunción del cargo sucedían en momentos distintos. Recurriendo al artículo 6 del Código Civil y Comercial de la Nación, que establece el modo de contar los plazos, la jueza Servini estableció que los mandatos presidenciales comenzaban a las 00:00 horas del día 10 de diciembre y finalizaban cuatro años después, a las 24:00 horas del día 9 de diciembre. Pero aclaró que, pese a que su mandato hubiera comenzado, el presidente electo no podía tomar decisiones hasta que no realizara el juramento establecido por la Constitución, formalidad indispensable para asumir el cargo. En virtud de dicha decisión, la presidencia de Cristina Fernández finalizó doce horas antes de la hora establecida para la asunción de la Presidencia por parte de Mauricio Macri, quedando acéfala.

Para resolver dicha incongruencia la jueza Servini dispuso que el tiempo durante el cual la Presidencia quedaba vacante entre la finalización del mandato de la presidenta saliente y la asunción del sucesor, debía ser ejercido durante ese tiempo, por la persona que hubiera jurado como presidente provisional del Senado, cargo que en ese momento había asumido Federico Pinedo.

Debido a que en Argentina los fallos judiciales sólo valen para el caso concreto, en 2019 reapareció la incertidumbre sobre el momento preciso en que finalizan y comienzan los mandatos presidenciales. De aplicarse el criterio establecido en el fallo de Servini, el mandato de Mauricio Macri habría vencido a las 24:00 del día 9 de diciembre de 2019, quedando la Presidencia acéfala hasta que el presidente Alberto Fernández prestara el juramento al día siguiente. De no aplicarse el fallo de Servini y volverse a la práctica tradicional, el mandato de Mauricio Macri finalizaría al mediodía del día 10 de diciembre de 2019, excediendo el mandato constitucional en doce horas.

La oficina presidencial puede quedar vacante por varias circunstancias: muerte, dimisión y destitución. Hasta la fecha, tres personas murieron ejerciendo el cargo, todas por causas naturales, Manuel Quintana en 1906, Roque Sáenz Peña en 1914 y Juan Domingo Perón en 1974.

En el caso de la renuncia al cargo, el presidente redacta una carta de renuncia dirigida al , la cual debe ser aceptada por el congreso reunido en Asamblea Legislativa. Este hecho ocurrió doce veces en la historia: Bernardino Rivadavia y Vicente López y Planes en 1827, Santiago Derqui y Juan Esteban Pedernera en 1861, Miguel Juárez Celman en 1890, Luis Sáenz Peña en 1895, Roberto Marcelino Ortiz en 1942, Héctor Cámpora en 1973, Raúl Alfonsín en 1989, Fernando de la Rúa y Adolfo Rodríguez Saá en 2001, y Eduardo Duhalde en 2003. 

En cuanto a la destitución, en el artículo 53 de la constitución faculta a la Cámara de Diputados acusar ante el Senado al presidente, vicepresidente, Jefe de Gabinete, ministros y jueces de la Corte Suprema por «mal desempeño o por delito en sus funciones; o por crímenes comunes» por la mayoría de dos terceras partes. Así se inicia el proceso de juicio político (artículos 59 y 60) por parte del Senado presido por el presidente de la Corte Suprema (en el caso de que el presidente sea el acusado), siendo declarado culpable por las dos terceras partes con destitución del cargo e inhabilitación para ejercer cargos públicos y la condena según el Código Procesal Penal.

Según el artículo 153 del Código Nacional Electoral, en caso de muerte o renuncia de cualquiera de los integrantes de la fórmula electa, se aplicará el artículo 88 de la Constitución. Si el presidente electo no puede asumir el cargo por muerte o renuncia, lo reemplazará el vicepresidente electo. En caso de la muerte o renuncia de ambos, se realizarán nuevas elecciones.

En caso de que el presidente en ejercicio no pudiese continuar ejerciendo el cargo, por motivos tales como enfermedad, ausencia, muerte, renuncia o destitución, el cargo es ejercido por el "vicepresidente" (artículo 88 de la "Constitución nacional").

En caso de requerirse un reemplazo para el presidente en una circunstancia en la que no se disponga de un vicepresidente, la Constitución establece en su art. 88 que corresponde al Congreso establecer quien asumirá el cargo. A tal fin se sancionó la Ley 20.972, de Acefalía, estableciendo la línea sucesoria para ese caso: transitoriamente el Poder Ejecutivo debe ser desempeñado por el , a falta de este el presidente de la Cámara de Diputados, y a falta de ambos por el presidente de la Corte Suprema de Justicia. Ese funcionario estará a cargo del Poder Ejecutivo sin asumir el título de «presidente».

Si la vacancia es transitoria estos funcionarios deben ejercer el Poder Ejecutivo hasta el retorno del presidente. Si la vacancia no es transitoria, el Congreso en Asamblea Legislativa (ambas cámaras reunidas, diputados y senadores) por mayoría simple, dentro del plazo de dos días debe elegir un presidente para gobernar, quien deberá terminar el mandato inconcluso o realizar un llamado a nuevas elecciones (artículo 88 de la "Constitución nacional"). Ese funcionario debe ser elegido entre los senadores, diputados o gobernadores. En caso de que ya hayan un presidente y un vicepresidente electos, asumirán el cargo antes para cumplir el mandato. Tales fueron los casos de Vicente López y Planes en 1827 y José María Guido en 1962 como presidentes provisional del Senado, Raúl Lastiri en 1973 como presidente de la Cámara de Diputados, Adolfo Rodríguez Saá en 2001 como gobernador de la Provincia de San Luis, y Eduardo Duhalde en 2002 como senador nacional por la Provincia de Buenos Aires.

El vicepresidente es el compañero de fórmula del presidente, siendo ambos los dos únicos miembros electos del poder ejecutivo argentino. El vicepresidente es el reemplazante del presidente en caso de viajes o licencias. Un caso notable fue el del vicepresidente Marcos Paz, quien reemplazó de manera interina al presidente Bartolomé Mitre, durante cinco años, mientras este último dirigía en el frente las tropas argentinas en la Guerra del Paraguay. Paz murió mientras ejercía la presidencia, lo que obligó a Mitre a retornar a Buenos Aires para reasumir el mando. Es también el reemplazante del presidente en caso de muerte o renuncia, de forma definitiva. Tales fueron los casos de los vicepresidentes Juan Esteban Pedernera en 1861, Carlos Pellegrini en 1890, José Evaristo Uriburu en 1895, José Figueroa Alcorta en 1906, Victorino de la Plaza en 1914, Ramón Castillo en 1942 y María Estela Martínez de Perón en 1974.

Asimismo, también es presidente del Senado de la Nación Argentina, aunque sin derecho a voto salvo caso de empate.

El 29 de marzo de 1962 se produjo un levantamiento militar con el objetivo de derrocar al presidente Arturo Frondizi, del partido Unión Cívica Radical Intransigente, quien se negó a renunciar. Frondizi fue detenido por los militares y llevado a la isla Martín García, previendo los rebeldes que, al día siguiente, el teniente general Raúl Poggi, líder de la insurrección victoriosa, asumiría la presidencia.

La noche del 29 de marzo de 1962, algunas personalidades civiles encabezadas por un miembro de la Corte Suprema de Justicia de la Nación, el doctor Julio Oyhanarte, elaboraron una maniobra para evitar que el quiebre institucional fuera total. Fue así como tomaron la detención de Frondizi como un caso de acefalía que permitía asumir la presidencia a quien estuviera en el primer lugar de la línea sucesoria según la Ley 252, que en el caso era el doctor José María Guido, un senador del mismo partido que Frondizi que presidía provisionalmente la Cámara de Senadores, debido a la renuncia anterior del vicepresidente Alejandro Gómez. Basados en esa interpretación hicieron que esa misma noche Guido jurara ante la Corte Suprema de Justicia como nuevo presidente.

Los militares golpistas terminaron aceptando la situación y convocaron a Guido en la Casa Rosada para comunicarle que sería reconocido como presidente, en tanto y en cuanto se comprometiera por escrito a ejecutar las medidas políticas indicadas por las Fuerzas Armadas, siendo la primera de ellas anular las elecciones en las que había ganado el peronismo. Guido aceptó las imposiciones militares, firmó un acta dejando constancia de ello y fue entonces habilitado por estos para instalarse con el título de presidente, pero clausurando el Congreso Nacional e interviniendo todas las provincias.

De este modo Guido asumió los poderes ejecutivo y legislativo del país, bajo control y supervisión de las Fuerzas Armadas, que se reservaron el derecho de removerlo, pero manteniendo intacto el Poder Judicial.

A raíz de golpes militares de Estado que derrocaron a los gobiernos constitucionales hubo presidentes militares y civil de facto en 1930-1932, 1943-1944, 1955-1958, 1962-1963, 1966-1973 y 1976-1983 que ejercieron además de las facultades propias del presidente también las que correspondían al Congreso. El análisis sobre la validez posterior de sus actos llevó a la formulación posterior de la doctrina de los gobiernos de facto.

Esa doctrina fue dejada sin efecto por la reforma constitucional de 1994 (artículo 36), la que declaró «usurpadores» a quienes hayan interrumpido la observancia de la Constitución por actos de fuerza.

El artículo 29 de la Constitución de 1853 tenía un artículo que consideraba la suma del poder público como «traición a la Patria», pero estaba referida a los gobernantes de jure. Por ese motivo en la reforma constitucional de 1994 se incluyó el artículo 36 que dice:
En síntesis, este artículo establece:

Los atributos presidenciales demuestran la dignidad de la Primera Magistratura de la Nación, siendo símbolos regidos por costumbre que representan al presidente de la República ante los ciudadanos. Tradicionalmente son cinco: la Banda Presidencial, el Bastón de Mando, la Marcha militar Ituzaingó, el Estandarte o Bandera de presencia presidencial y el "sillón de Rivadavia". La entrega de los atributos representan el momento en el cual el mandatario saliente transmite la autoridad presidencial al nuevo presidente. Cada presidente recibe una banda y un bastón nuevos, que suelen conservar como recuerdo de su paso por el cargo una vez que cesan en sus mandatos. 

La banda presidencial es una cinta delgada de tela con los colores de la Bandera bordada con un sol y terminada en una borla de hilos de oro, que se coloca en forma cruzada atravesando el hombro derecho y cayendo hacia el costado izquierdo, sobre la indumentaria para significar que su portador es titular de una dignidad u honor y, en tal carácter, debe ser reconocido por todos.

La Asamblea del Año XIII el 26 de enero de 1814, al instaurar el cargo de Director Supremo de las Provincias Unidas del Río de la Plata, estableció que le correspondían una banda y un bastón, siendo Gervasio Antonio de Posadas la primera persona en utilizarlos. La banda era bicolor, blanca en el centro y azul en los costados, terminada en una borla de oro, hasta la gobernación de Juan Manuel de Rosas en que se cambió al rojo punzó siendo restaurado el color original durante la presidencia de Justo José de Urquiza e incluyendo el bordado con un sol. Durante las presidencias conservadoras fue bordado con el escudo nacional en dorado. El 24 de abril de 1944 el presidente Edelmiro Julián Farrell emitió el Decreto-Ley 10.302/1944; donde instituyó los símbolos patrios. En su artículo 4° legisló sobre la Banda Presidencial, pasando de ser un atributo tradicional a una distinción jurídica:

La marcha es una pieza musical solo de melodía utilizada en los actos oficiales donde se presenta el presidente para indicar su llegada. La partitura fue encontrada en un cofre, entre los trofeos de batalla tras la victoria de Carlos María de Alvear en la Batalla de Ituzaingó durante la Guerra del Brasil. Se cree que fue compuesta por el emperador brasileño Pedro I para el marqués de Barbacena, comandante de sus tropas, en caso de un posible triunfo en Ituzaingó. Se utilizó por primera vez a ese efecto el 25 de mayo de 1827, y —con la excepción de un interludio entre el 26 de enero de 1946 y el 28 de agosto de 1959, en que la reemplazó a ese efecto la marcha San Lorenzo— se ha utilizado desde entonces.

El bastón de mando (o vara de mando, también denominado manípulo) es un complemento protocolario que denota en la persona que lo porta, autoridad o mando sobre un grupo o colectivo identitario. Fue instaurado como atributo presidencial en 1932.

Desde 1983 el orfebre Juan Carlos Pallarols, que proviene de una familia de artesanos cuyo taller data de 1750, confecciona el bastón que consiste en una vara de madera urunday (proveniente de Chaco y Misiones) de noventa centímetros (de acuerdo con la estatura del presidente electo) con una empuñadura de plata adornada por el Escudo Nacional y flores de veinticuatro cardos, una por cada provincia; y tres pimpollos, que representan las Islas del Atlántico Sur. Desde 1932 bajo el gobierno de José Félix Uriburu hasta entonces los fabricaba el artesano Luis Ricciardi como se estilaba: con caña de Malaca, puño de oro 18 quilates adornado con el escudo nacional, regatón de oro y dos borlas. 

El sillón presidencial de Casa Rosada, o mal llamado «sillón de Rivadavia» porque se cree que lo utilizó Bernardino Rivadavia, corresponde a la primera presidencia de Julio Argentino Roca. Fue comprado a la Casa Forest de París en 1885 y está conformado de madera de nogal italiana, siendo decorado con la técnica dorado a la hoja, con lámina de oro. Es utilizado desde entonces por todos los presidentes del país.
El estandarte presidencial es una bandera heráldica de un paño color celeste adornado con el escudo de la Nación Argentina situado en su parte central y acompañados de cuatro estrellas de cinco puntas colocadas en cada uno de sus vértices, utilizada como insignia del presidente de la Nación Argentina y se enarbola en el lugar en que se encuentra el presidente de la Nación.

El presidente tiene su oficina en la sede del Gobierno, la Casa Rosada. Desde 1862, el entonces presidente Bartolomé Mitre se instaló en el antiguo Fuerte de Buenos Aires, que había sido residencia de gobernadores y virreyes españoles, y demás autoridades de los sucesivos gobiernos patrios a partir de 1810. Su sucesor, Domingo Faustino Sarmiento, decidió embellecer la morada del Poder Ejecutivo Nacional, dotándola de jardines y pintando las fachadas de color rosado, con el que, posteriormente, se continuó caracterizando. La construcción de la actual Casa de Gobierno comenzó en 1873, cuando por decreto se ordenó construir el edificio de Correos y Telégrafos en la esquina de Balcarce e Hipólito Yrigoyen. Pocos años después, el presidente Julio Argentino Roca decidió la construcción del definitivo Palacio de Gobierno en la esquina de Balcarce y Rivadavia, edificación similar al vecino Palacio de Correos. Ambos edificios se unieron en 1886 mediante el pórtico que hoy constituye la entrada de la Casa Rosada que da hacia Plaza de Mayo.

La residencia oficial ha ido cambiando a lo largo de la historia. Rivadavia (1826-27) residió en la Casa de los Virreyes, en el antiguo Fuerte de Buenos Aires, mientras que sus sucesores residieron en sus casas particulares. La excepción fue Roque Sáenz Peña quien acondicionó un cuarto en el primer piso en la Casa Rosada debido a que su enfermedad le impedía movilizarse con facilidad. 

A mediados de la década de 1930 el estado adquiere la propiedad de Carlos Madariaga y su esposa Josefina Anchorena, ubicada en la calle Suipacha 1034 de la Ciudad de Buenos Aires, para convertirla en Residencia Presidencial. El primer presidente que la utiliza es Roberto Ortiz junto a su señora María Luisa Iribarne. 

En 1937 (durante la presidencia de Agustín Pedro Justo) el estado adquiere el Palacio Unzué, una residencia construida por Mariano Unzué y Mercedes Baudrix en 1887 ubicada en un espléndido parque diseñado por Carlos Thays, rodeado por las calles Agüero, Alvear (Libertador) y Austria, en el barrio de Recoleta, Buenos Aires. El primer y único presidente que la utilizó de forma permanentemente fue Juan Domingo Perón y su señora Eva Duarte.

La Quinta presidencial de Olivos fue donada por la familia Anchorena Olaguier en 1918 bajo la presidencia de Yrigoyen. Desde 1918 los presidentes la fueron utilizando cómo residencia ocasional y de verano, siendo el primero que la habita de forma permanente Pedro Eugenio Aramburu. Desde entonces se utiliza como residencia oficial y permanente. En la Quinta de Olivos falleció el presidente Juan Domingo Perón el 1 de julio de 1974 mientras ejercía su tercer mandato, siendo hasta ahora el único presidente que murió allí. La Quinta es un gran complejo residencial compuesto de un vasto parque situado en la localidad de Olivos del partido de Vicente López. La residencia presidencial ocupa el edificio principal, de líneas neoclásicas, construido por Prilidiano Pueyrredón en 1854. Las fachadas se conservan como en el siglo XIX, sin embargo, los interiores y el parque sufrieron reformas con el paso de los diferentes presidentes. 

Dispone de una residencia de verano en la localidad de Chapadmalal (provincia de Buenos Aires), la que se denomina Unidad Presidencial de Chapadmalal. La casa de Chapadmalal, construida durante el primer gobierno de Perón, cuenta con una playa privada, un mirador con vistas a la costa y varios jardines. Fue remodelada por última vez en los 90, durante la presidencia de Carlos Menem.

El presidente y vicepresidente disfrutan de un sueldo pagado por el Tesoro de la Nación, que no podrá ser alterado en el período de sus nombramientos. Durante el mismo período no podrán ejercer otro empleo, ni recibir ningún otro emolumento de la Nación, ni de provincia alguna. El sueldo del presidente en bruto es de 173 000 pesos argentinos.

Para desplazarse el mandatario utiliza aviones que forman parte de la Agrupación Aérea Presidencial:


El automóvil presidencial que actualmente se utiliza es una Mercedes Benz Vito, aunque tiene a disposición una Volkswagen Touareg y una Chrysler Town & Country

La Casa Militar es la encargada de la protección del presidente y su familia. Con base en la Casa Rosada, la Casa Militar es conducida por un oficial superior de las Fuerzas Armadas, cuyo cargo es rotativo cada dos años y «debe proveer la seguridad del presidente, de sus familiares directos, como también de la Casa de Gobierno, la residencia presidencial de Olivos y otros lugares de residencia transitoria que disponga el jefe del Estado». Tiene el «control operacional» de tres agrupaciones principales: Coordinación, Logística y Comunicaciones; Aérea, y Seguridad e Inteligencia. Esta última integrada por el histórico Regimiento de Granaderos a Caballo (como escolta presidencial) y la Policía Federal en su división Custodia Presidencial (como custodia personal del presidente y su familia en los desplazamientos terrestres). La Custodia Presidencial tiene su base en la Casa de Gobierno y en la residencia de Olivos, cuyo perímetro y sector externo están a cargo de la policía bonaerense.

El Escuadrón Ayacucho del Regimiento de Granaderos a Caballo, creado en 1812 por el general José de San Martín e instaurado el 15 de julio de 1907 por el presidente José Figueroa Alcorta como escolta del presidente de la Nación —hasta ese momento lo era el Regimiento 8 de Caballería, que llevaba el nombre del general Mariano Necochea y el uniforme histórico de sus Cazadores que hicieron la campaña de los Andes—, cumple las funciones de escolta y seguridad del Presidente de la Nación en la Casa de gobierno, custodia de los restos del General Don José de San Martín en el mausoleo situado en la Catedral Metropolitana, el Izamiento y arrío de la Bandera Oficial de la Nación en la Plaza de Mayo y participan en todos los actos de ceremonial que se realizan en la Casa de Gobierno y en la Catedral Metropolitana. Llevan el uniforme tradicional: botas negras hasta la rodilla, espuelas, una chaquetilla con pechera adornada con botones, sable enfundado en un costado, cuello rígido y el morrión en la cabeza para la famosa Escolta Presidencial. Los granaderos que realizan la custodia del ingreso presidencial y los que custodian los restos de San Martín deben pasar las dos horas que dure su guardia quietos y firmes, en posición de “estatua”.

Los edecanes presidenciales son tres ayudantes militares, uno por cada rama de las Fuerzas Armadas, un teniente coronel —Ejército—, un capitán de fragata —Armada y un vicecomodoro —Fuerza Aérea—, que se distinguen por el cordón dorado que usan encima del uniforme, cuya principal misión es acompañar, proteger y asistir al presidente en todas sus actividades oficiales y representarlo en los eventos protocolares que específicamente les encomiende. El primer edecán que registra la historia argentina fue el capitán Juan María Escobar, quien acompañó al presidente de la Primera Junta, Cornelio Saavedra.

El propio presidente se encarga de su designación, de una lista de candidatos presentada por el Ministerio de Defensa y elaborada por las propias Fuerzas Armadas. En el cumplimiento de sus labores, que prestan las 24 horas del día, coordinan la agenda protocolar del presidente –deben indicar discretamente el fin de una actividad o una audiencia para dar paso a la siguiente–, y reciben y tramitan las instrucciones que les entrega el mandatario. Son también los únicos -además del secretario privado- que tienen acceso directo al despacho presidencial, participan de las reuniones más reservadas y conocen con anticipación los nombramientos que hará el Presidente, porque ellos tendrán que ubicar después al funcionario elegido. En los actos oficiales llevan el discurso que pronunciará el presidente en una carpeta, pero también una copia en el bolsillo. Además, deben ser discretos y estar en condiciones de responder cualquier pregunta del presidente. Los tres edecanes se dividen el trabajo por semanas, quedando uno de turno cada semana, y deben cumplir sus funciones por dos años. En caso de que el presidente asista a una actividad en un recinto militar lo acompaña el edecán respectivo, y para las ceremonias de Estado concurren los tres.

Cada presidente una vez terminado su mandato puede ejercer otros cargos políticos. Algunos presidentes han tenido carreras significativas después de dejar el cargo. Tal es el caso de José Figueroa Alcorta que fue presidente de la corte suprema, siendo el único argentino en presidir los tres poderes, el de Néstor Kirchner que fue secretario general de la UNASUR, y de Cristina Fernández de Kirchner que se desempeña actualmente como vicepresidenta de la Nación. Vicente López y Planes (Buenos Aires en 1852) y Justo José de Urquiza (Entre Ríos entre 1868 y 1870) ejercieron la gobernación de sus provincias natales. Bartolomé Mitre, Julio Argentino Roca, Hipólito Yrigoyen, Marcelo Torcuato de Alvear, Arturo Frondizi, Juan Domingo Perón, Raúl Alfonsín y Néstor Kirchner ejercieron el liderazgo de sus respectivos partidos políticos e incluso se presentaron a elecciones nuevamente, algunos reelegidos con éxito como Roca en 1898 y Perón en 1973, y otros presidentes sirvieron en el Congreso después de abandonar la Casa Rosada como Carlos Pellegrini y Carlos Menem. 

Los exmandatarios poseen tras terminar su mandato la protección vitalicia de la Policía Federal Argentina y perciben una asignación mensual vitalicia equivalente al sueldo de un juez de la Corte Suprema, según la Ley 24.018. Al fallecer, la pensión pasa a la viuda o viudo que cobrará el 75% pero tendrá que renunciar a toda pensión estatal. Para el goce de estos beneficios, deben residir dentro del territorio argentino. Arturo Illia y Raúl Alfonsín donaron todos sus años de jubilación a la caridad.

Tras sus fallecimientos, los presidentes reciben homenajes como decretar tres días de duelo nacional, y el funeral de estado, y sus familias donan sus pertenencias del mandato a museos como el Museo Histórico Sarmiento y el Museo Casa Rosada (creado del antiguo Museo Presidencial Casa Rosada) cuya colección está conformada por objetos personales, retratos, esculturas y documentos de quienes han ocupado el cargo de «presidente» y objetos referentes al contexto social, económico y político de cada etapa presidencial, incluyéndose presidencias recientes.

Como modo de homenaje, toda persona que ejerció la primera magistratura es retratada en mármol de Carrara y depositada en el Hall de Honor de la Casa Rosada junto a todos los presidentes argentinos excepto los de facto, desde Cornelio Saavedra y Bernardino Rivadavia hasta Néstor Kirchner. Los bustos que faltan son Vicente Lopez Y Planes, Santiago Derqui, Juan Esteban Pedernera, Marcos Paz, Luis Saeñz Peña, Jose Figueroa Alcorta, Enrique Martinez, los presidentes de la decada infame (sin contar al presidente de facto Jose Felix Uriburu), Jose Maria Guido (aunque este no sé puede definir si fue o no defacto) Arturo Illia, Raúl Alberto Lastiri, María Estela Martínez de Perón, Ítalo Lúder, Carlos Menem, Fernando de la Rúa, Ramon Puerta Adolfo Rodríguez Saá, Eduardo Camaño y Eduardo Duhalde. Cristina Fernández de Kirchner debería ser agregada a partir del año 2023 y Mauricio Macri a partir del año 2027. 

Los primeros bustos expuestos en ese salón fueron realizados entre 1883 y 1884, encargados por el presidente Julio Argentino Roca. Data de aquella época la tradición de agregar el busto de los primeros mandatarios luego de que finaliza su período de mandato. Estos bustos estuvieron ubicados inicialmente en los Recintos Presidenciales del primer piso, pero en 1973, durante la presidencia de facto de Alejandro Lanusse, se decidió su traslado al Hall de Honor y se dictó el Decreto 4022, que rige la colocación de los Bustos Presidenciales, indicando que esto se hará una vez transcurrido un lapso no menor a dos períodos presidenciales, tras la finalización del mandato correspondiente. En 2016 los 28 bustos fueron reubicados en forma cronológica, además las esculturas de los presidentes "de facto" José F. Uriburu, Pedro Ramírez y Edelmiro Farrell fueron retiradas, y se agregó el de Miguel Juárez Celman, que estaba abandonado en un depósito.




</doc>
<doc id="8912" url="https://es.wikipedia.org/wiki?curid=8912" title="Carlos Menem">
Carlos Menem

Carlos Saúl Menem (Anillaco, Argentina; 2 de julio de 1930) es un abogado y político argentino, presidente de la Nación Argentina entre 1989 y 1999 y en los períodos 1973-1976 y 1983-1989. Desde 2005 ocupa el cargo de senador nacional, representando a la provincia de La Rioja. 

Asumió anticipadamente el cargo presidencial el 8 de julio de 1989 de manos del presidente saliente, Raúl Alfonsín, tras vencer en las elecciones presidenciales de 1989, devolviendo al peronismo al poder después de trece años y protagonizando la primera transferencia pacífica entre dos presidentes democráticos de distintos partidos políticos en la historia argentina tras la última dictadura cívico-militar. Menem ocupó el cargo hasta el 10 de diciembre de 1999, lo que lo convierte en la persona que más tiempo ha detentado la en modo continuo, con . Al período histórico que englobó su presidencia a menudo se le denomina «menemismo» (término también utilizado para referirse al movimiento ideológico en torno a su figura) o incluso «época de Menem».

Durante su primer mandato, asumiendo el cargo en medio de un proceso hiperinflacionario iniciado durante la última etapa del gobierno de Alfonsín, la administración de Menem implementó políticas económicas de corte neoliberal basadas en el Consenso de Washington —si bien su gobierno en gran medida no se plegó al mismo—. Las mismas lograron reducir la inflación a mínimos históricos, y produjeron un marcado crecimiento del producto interno bruto (PIB) y la renta per cápita. Pese a esto, la gran cantidad de privatizaciones de empresas estatales provocaron despidos masivos y un aumento del desempleo y el subempleo. Además, la presidencia de Menem se caracterizó por diversas polémicas y numerosos escándalos de corrupción que salpicaron al gobierno durante todo el período, sucediéndose además atentados terroristas contra la AMIA y la Embajada de Israel en Argentina, así como la repentina muerte de su hijo en un accidente de helicóptero denunciado por su exesposa, Zulema Fátima Yoma (cuyo sonado divorcio en 1991 fue también controvertido), como un atentado criminal. Del mismo modo, se consideraba que el poder judicial carecía de completa independencia con respecto al poder ejecutivo, destacando cinco de los nueve magistrados de la Corte Suprema de Justicia que fueron acusados de funcionar como una «mayoría automática» para el gobierno. También fue destacada la reforma constitucional argentina de 1994. Imposibilitado por la Constitución de 1853 para presentarse a la reelección luego de su primer período de seis años, Menem negoció con su predecesor y líder de la oposición, Alfonsín, el denominado Pacto de Olivos, que allanó el camino para el llamado a una Convención Constituyente a fin de reformar la carta magna. Esta reforma habilitó a Menem para presentarse a la reelección para un mandato más, siendo este acortado a cuatro años y aboliendo el sistema de Colegio Electoral para reemplazarlo por un sistema de elección directa con segunda vuelta o balotaje; mientras que la oposición obtuvo la autonomización de la Capital Federal y la creación del Senador Nacional por la minoría. Entre otros cambios, la reforma introdujo también los derechos de tercera y cuarta generación, normas para defensa de la democracia y la constitucionalidad, las características de los órganos de gobierno, y nuevos órganos de control. 

Tras vencer por abrumador margen en las elecciones de 1995, el segundo mandato de Menem se caracterizó por el inicio de una recesión económica, un aumento de la deuda externa y nuevos escándalos de corrupción, que finalmente lograron el debilitamiento político del menemismo y, por añadidura, del Partido Justicialista. Las escandalosas insinuaciones de Menem de presentarse a un tercer mandato, algo prohibido expresamente por la reforma constitucional, más la precaria situación económica del país, llevaron a la derrota electoral del PJ ante el candidato radical Fernando de la Rúa en 1999, y a que un sector opositor interno del justicialismo más ortodoxo, encabezado por sus ex vicepresidentes Eduardo Duhalde y Carlos Ruckauf, lograra tomar progresivamente el control del partido. 

Después de la crisis de 2001 que llevó a la caída del gobierno de De la Rúa, Menem buscó nuevamente la presidencia en las elecciones de 2003, durante las cuales el Partido Justicialista se dividió y otros dos candidatos (Néstor Kirchner y Adolfo Rodríguez Saá) se presentaron bajo el sello del peronismo. Si bien Menem fue el candidato más votado con el 24,45% de las preferencias, dos puntos por encima de Kirchner, fracasó en lograr la mayoría requerida para ser elegido en primera vuelta, organizándose una segunda vuelta o balotaje entre Menem y Kirchner. Con las encuestas vaticinándole una derrota electoral aplastante, Menem se retiró del balotaje cuatro días antes del mismo, facilitando la elección de Kirchner.

Tras el fin de sus aspiraciones presidenciales, Menem se mantuvo activo en la política como opositor al kirchnerismo dentro del PJ, resultando electo Senador Nacional de la mayoría por La Rioja en las elecciones de 2005, siendo reelegido en 2011. En 2007 contendió por la gobernación riojana por última vez, apoyado por el partido Lealtad y Dignidad, y ubicándose en el tercer puesto detrás de otros dos candidatos justicialistas. En las elecciones de 2017, buscando su segunda reelección como Senador, fue superado en votos por la lista encabezada por el radical Julio Martínez, pero de todas formas logró acceder a la banca por la minoría. A finales de 2019, luego de un breve acercamiento con el gobierno de Mauricio Macri, anunció su adhesión al Frente de Todos, cuya fórmula presidencial era encabezada por Alberto Fernández, con Cristina Fernández de Kirchner como candidata a vicepresidenta. Tras la victoria electoral de la coalición, Menem se unió formalmente al bloque oficialista del Frente de Todos en el Senado.

Carlos Saúl Menem nació en la pequeña localidad de Anillaco, Departamento Castro Barros, provincia de La Rioja, Argentina, el 2 de julio de 1930. Sus padres, Saúl Menehem (1898-1975) y Mohibe Akil (1907-1977), eran ambos de origen sirio y practicantes de la religión musulmana suní, y habían emigrado a la Argentina en la década de 1910. El apellido de su padre, «Menehem», fue castellanizado como «Menem» por las autoridades migratorias cuando arribó al país. Menem cursó sus estudios primarios y secundarios en su provincia natal, en ambos casos en colegios públicos. Practicó la religión islámica hasta que decidió convertirse al cristianismo en su juventud. Por aquel entonces estaba vigente un precepto constitucional que establecía que el presidente de la Nación Argentina debía profesar la fe católica apostólica romana. La reforma constitucional impulsada por su gobierno en 1994, si bien no eliminó en sentido estricto el carácter confesional del Estado argentino, sí abolió la obligatoriedad de que el presidente fuera católico.

Cursó sus estudios universitarios de abogacía en la Facultad de Derecho de la Universidad Nacional de Córdoba entre 1949 y 1955. En 1951, durante un viaje a la Capital Federal del equipo universitario de baloncesto en el que jugaba, Menem conoció por primera vez al presidente Juan Domingo Perón y a su esposa, Eva Duarte, con lo que inició su militancia política en el movimiento peronista o justicialista. Completó sus estudios y se recibió como abogado el 27 de julio de 1955, menos de dos meses antes del golpe de estado que derrocó al gobierno de Perón y proscribió al peronismo de la vida política argentina. Durante la dictadura militar subsiguiente, encabezada por Pedro Eugenio Aramburu, Menem fue detenido por primera vez en 1956, acusado de participar en una conspiración peronista para derrocar al gobierno.

Tras su liberación, en 1957, fundó en la clandestinidad la Juventud Peronista de La Rioja, de la que fue su primer presidente, y contribuyó con asistencia legal a la Confederación General del Trabajo (CGT), principal organización sindical del país, actividad que ejercería hasta 1970. En las elecciones provinciales de 1962 se presentó como candidato a diputado provincial de La Rioja por el Departamento Castro Barros. Dada la proscripción del Partido Justicialista, Menem se presentó bajo la lista del partido Unión Popular (UP). Si bien resultó electo diputado, el golpe de estado que tuvo lugar diez días después de los comicios anuló su victoria y le impidió asumir. Durante su mandato posterior como gobernador, el gobierno riojano reconoció la validez de las elecciones de 1962 y hasta 2003 el gobernador electo en dichos comicios, el radical intransigente Enrique Chumbita, cobraría una jubilación por el mandato gubernativo que no llegó a ejercer.

En 1963 fue elegido presidente del Partido Justicialista de La Rioja. De cara a las elecciones provinciales de julio de ese mismo año, Menem fue postulado como candidato a gobernador de La Rioja bajo el sello de la UP. Sin embargo, la nueva inhabilitación del peronismo para presentar candidaturas llevó a Perón a proclamar la abstención y llamar al voto en blanco, ordenando a los peronistas que evitaran presentarse bajo otras siglas partidarias. Mientras que otras figuras provinciales destacadas, como el chaqueño Deolindo Felipe Bittel, no acataron el llamado, Menem cumplió con la orden de Perón y retiró su candidatura. Más tarde, en 1964, en calidad de presidente del justicialismo riojano, visitó personalmente a Perón en su exilio en España. Ese mismo año viajó a Siria, país natal de sus padres, donde conoció a Zulema Fátima Yoma, musulmana también oriunda de La Rioja, con quien se casaría.

El 17 de noviembre de 1972, Menem fue pasajero en el vuelo que trasladó a Perón de regreso desde España a la Argentina.

Con la legalización del peronismo en 1972 y el llamado a elecciones libres, Menem retornó a la actividad política definitivamente. Sus popularidad dentro de La Rioja y relaciones dentro de la Juventud Peronista nacional lo llevaron a encabezar el Partido Justicialista riojano como candidato gobernador a la relativamente joven edad de cuarenta y tres años. A nivel nacional se había configurado el Frente Justicialista de Liberación (FREJULI), que sostenía a nivel nacional la candidatura presidencial de Héctor José Cámpora, debido a que la junta militar gobernante impidió la candidatura del propio Perón e instauró un sistema de segunda vuelta electoral o balotaje para las elecciones de presidente, gobernadores y senadores, en caso de que ninguna fórmula lograrse la mayoría absoluta de votos en primera instancia. Sin embargo, dicho frente no había llegado a consolidarse a nivel provincial, por lo que Menem concurrió en solitario.

El compañero de fórmula de Menem y candidato a vicegobernador fue Libardo Sánchez, oriundo del Departamento Chilecito y peronista ortodoxo visto como un opositor interno a Menem. Sectores de la Juventud Peronista buscaron imponer la candidatura a vicegobernador del sindicalista combativo Ramón Torres, sin éxito. Sin embargo, se les concedió el primer lugar en la lista para diputados nacionales por el distrito riojano, recayendo la candidatura en la persona de Juana Romero, que fue una de las únicas diecisiete mujeres representantes en el Congreso Nacional durante el período subsiguiente. Debido a esto, la JP dio apoyo a la candidatura de Menem a pesar de no encontrarse representada en la fórmula ejecutiva. 

Menem realizó una campaña personalista, y los medios de comunicación establecieron comparaciones entre él y el caudillo Facundo Quiroga (1788-1835) que había liderado La Rioja durante las guerras civiles argentinas, principalmente debido a su apariencia física, con largas patillas (que serían su marca personal más destacada). En las elecciones provinciales de La Rioja del 11 de marzo de 1973 y a pesar de concurrir con la desventaja de no haberse conformado el FREJULI en el ámbito provincial, la fórmula Menem-Sánchez resultó electa por abrumador margen al obtener el 57,49 % de los votos contra el 29,06 % obtenido por el binomio de la Unión Cívica Radical (UCR). La coalición justicialista obtuvo además 20 de los 25 escaños de la Legislatura Provincial. Menem fue el gobernador más joven de los veintidós elegidos ese año y uno de los únicos siete cuya elección se definió en primera vuelta, sin necesidad de concurrir a un desempate.

Menem asumió la gobernación de La Rioja el 25 de mayo de 1973, en simultáneo con las autoridades nacionales, de manos del interventor "de facto" Julio Luchesi. Sin embargo, se encargó de organizar una masiva celebración de asunción diferenciada el 9 de junio, un par de semanas más tarde, en conmemoración con el decimoséptimo aniversario de la levantamiento de 1956 encabezado por Juan José Valle con el objetivo de derrocar a la dictadura de la Revolución Libertadora. El acto tuvo lugar en San Antonio, localidad natal de Facundo Quiroga, de poco más de doscientos habitantes, y fue sumamente concurrido, destacando la presencia del obispo Enrique Angelelli y el vicepresidente de la Nación Vicente Solano Lima. Durante su discurso enfatizó la lealtad peronista del presidente Cámpora, denunció que el régimen militar le estaba dejando una economía quebrada y realizó varias promesas sociales. Después de su presidencia, el término «Lealtad» dentro del peronismo sería fuertemente asociado con Menem, en parte debido a la lista con la que ganaría posteriormente la conducción del PJ riojano (Lealtad y Unidad), la coalición con la que disputó la presidencia por última vez (Frente por la Lealtad), y el partido provincial que lideraría (Lealtad y Dignidad).

Al momento de asumir Menem la gobernación, La Rioja era considerada una de las provincias más pobres y atrasadas de la Argentina, así como el tercer distrito menos poblado de acuerdo con el censo de 1970 con 136.237 habitantes (solo por delante de Santa Cruz y el entonces Territorio Nacional de Tierra del Fuego). Durante su discurso en el acto de San Antonio, Menem denunció que le habían entregado una provincia «quebrada» con una muy mala situación económica, y se comprometió a emprender una reforma agraria y una socialización de la economía. Los primeros días de mandato de Menem estuvieron marcados por una controversia luego de que enviara un proyecto de ley a la Legislatura Provincial con el objetivo de cumplir su promesa de campaña de expropiar el latifundio Azzallini, ubicado en el poblado de Aminga, para transferirlo a la Cooperativa de Trabajo CODETRAL (Cooperativa de Trabajadores de Aminga Ltda.). La medida, apoyada por el obispo Angelelli como una larga reivindicación de dicho poblado, se vio obstaculizada por la negativa de algunos legisladores del bloque oficialista, con numerosos focos de violencia entre partidarios y detractores de la expropiación entre el 13 de junio y el 29 de julio, incluso entre personas no originarias de Aminga. Finalmente, el compromiso electoral de Menem con la CODETRAL se acabó abandonando, y si bien el territorio fue expropiado con éxito, un sector de la bancada justicialista se unió a la bancada radical y aprobó un proyecto de ley diferenciado, que no otorgaba la concesión del terreno a la cooperativa, siendo este proyecto refrendado por Menem a fines de agosto.

Menem decidió retomar el plan de gobierno de Guillermo Iribarren, interventor federal "de facto" que gobernó desde 1967 hasta su muerte en 1971 (su hermano, Eduardo Menem, que ejerció como ministro de dicho gobierno, ocupó la jefatura de gobierno riojana brevemente hasta la designación de un nuevo interventor). Iribarren fue el primer titular del poder ejecutivo riojano en gobernar la provincia con una planficación: el Plan de Acción Inmediata. El mismo establecía, entre otras disposiciones: la disminución parcial del empleo público, exenciones impositivas, blanqueo de capitales exclusivamente para La Rioja, exención impositiva del 100% para la minería, solución al problema de la tierra, al problema del agua, un Plan de Salud Pública, y la generación de puestos de trabajo. Dicho plan, considerado como detonante de un «milagro riojano», sería la base para la obra de gobierno menemista. La primera gestión de Menem como gobernador de La Rioja fue sumamente popular, facilitando su permanencia al mando del peronismo local por décadas. Sin embargo, una diferencia fundamental con el plan de Iribarren fue el considerable crecimiento del empleo público visto desde la llegada de Menem a la gobernación en adelante, convirtiéndose en la segunda provincia con mayor porcentaje de población dependiente del estado, después de Formosa.

Dentro de la política nacional, Menem mostró una inclinación inicial hacia la denominada tendencia revolucionaria del peronismo (abreviado como la "Tendencia") una corriente ligada a la izquierda política del movimiento justicialista que se encontraba abiertamente enfrentada con el denominado sector ortodoxo, encarnado en la mayoría de los sindicatos peronistas. Sin embargo, a partir de finales de ese año, luego del retorno al poder por abrumador margen de Perón y el viraje político contrario al sector izquierdista por parte de su gobierno (sobre todo después de su muerte y la llegada de su esposa, María Estela Martínez de Perón, apodada Isabel, a la presidencia) Menem abandonó rápidamente su anterior acercamiento. Los demás gobernadores ligados a la "Tendencia": el bonaerense Oscar Bidegain, el cordobés Ricardo Obregón Cano, el mendocino Alberto Martínez Baca, el formoseño Antenor Gauna, el salteño Miguel Ragone y el santacruceño Jorge Cepernic fueron o bien forzados a renunciar o sufrieron la intervención federal a su gobierno a finales de 1973 y luego a lo largo del año 1974. Su distanciamiento con la izquierda llegó a un punto culmine cuando el 9 de agosto de 1975 proclamó públicamente su intención de que Isabel buscara la reelección en 1977.

Con posterioridad a su gobierno, existen serios cuestionamientos dentro del peronismo sobre su relación con Perón, así como con su esposa Isabel, que fue la primera presidenta argentina oriunda de La Rioja, habiendo numerosa evidencia de que su relación no fue realmente buena con ninguno de los dos. De acuerdo con algunas fuentes, Perón se habría referido a Menem como «payaso» o «mamarracho» después de una reunión de gobernadores justicialistas.

Al igual que todas las autoridades electas en 1973, Menem no pudo concluir su mandato debido al golpe de Estado del 24 de marzo de 1976 que desalojó nuevamente al justicialismo del poder, disolvió todas las autoridades legislativas e intervino todos los distritos que aún no habían sido intervenidos para entonces. De acuerdo con el historiador riojano Roberto Rojo, un grupo de militares, entre los cuales se encontraba quien posteriormente sería jefe del Ejército César Milani, ingresó a la casa de gobierno provincial esperando que Menem opusiera resistencia. Sin embargo, de acuerdo con Rojo, el gobernador los recibió pacíficamente, saludó a cada uno de ellos, y fue arrestado sin que se produjeran sobresaltos, afirmando que «volverían a encontrarse» cuando fuera «presidente de la Nación». Menem fue reemplazado por el coronel Osvaldo Battaglia, que asumió como interventor "de facto".

En 1980, con Menem aún detenido por el régimen, el gobierno "de facto" otorgó a los gobernadores despuestos en el golpe de estado una jubilación por su período de gobierno, siendo Menem uno de los beneficiarios. En 2003 se producirían denuncias por la supuesta omisión de Menem de dicha jubilación en su declaración jurada.

Tras su destitución como gobernador provincial, Menem pasó una semana recluido en el Regimiento de Infantería 15 de La Rioja, para luego ser trasladado al barco "33 Orientales", amarrado en Buenos Aires. Durante su estadía en el barco, compartió camarote con Pedro Eladio Vázquez, médico personal de Perón, y se reencontró con varios ministros del gobierno depuesto (Antonio Cafiero, Miguel Unamuno, José Deheza, y Pedro Arrighi), así como con líderes sindicales (Jorge Alberto Triaca, Diego Ibáñez y Lorenzo Miguel). También estaban encarcelados allí el expresidente interino Raúl Lastiri, el diplomático Jorge Vázquez, y el periodista Osvaldo Papaleo. En julio de 1976 fue trasladado a un centro de detención permanente en Magdalena. En 1977 el gobierno "de facto" de Jorge Rafael Videla rechazó su solicitud de salir de prisión para asistir al funeral de su madre, Mohibe Akil, luego de su muerte a la edad de setenta años.

Luego de poco más de dos años detenido, fue finalmente puesto bajo un régimen de «domicilio forzoso» el 29 de julio de 1978. Este sistema establecía que debía residir en forma permanente en una ciudad que no se encontrara en su provincia de origen. Menem eligió la localidad bonaerense de Mar del Plata. Durante su estadía en dicha ciudad, Menem mantuvo vínculos con integrantes del sindicalismo justicialista como Abdul Saravia y Diego Ibáñez, que posteriormente integrarían su espacio político, y mantuvo una relación amistosa con el miembro de la Junta Militar, Emilio Eduardo Massera, que tenía intenciones de lanzarse a la política constitucional (sin éxito). También se juntó con figuras de la farándula como la "vedette" Susana Giménez, el boxeador Carlos Monzón, y el cómico Alberto Olmedo.

Su actividad pública condujo a que el Ministerio del Interior resolviera que no podía permanecer en Mar del Plata y lo forzó a trasladarse nuevamente, esta vez a la ciudad también bonaerense de Tandil, donde tendría la obligación de reportarse diariamente al jefe de policía local, Hugo Zamora. Allí mantuvo una relación personal cercana con Zamora, que sería nombrado jefe de policía de La Rioja tras la llegada de Menem a la gobernación provincial. Finalizó su período de domicilio forzoso en febrero de 1980, luego de diecinueve meses, y se trasladó a Buenos Aires, pero retornó rápidamente a La Rioja. Allí tomó la decisión de reiniciar su actividad política, a pesar de que el gobierno militar se la tenía vedada, lo que provocó que fuera nuevamente detenido en septiembre del mismo año. Con su segunda detención fue nuevamente puesto bajo el sistema de domicilio forzoso, siendo trasladado a la localidad de Las Lomitas, en la provincia de Formosa.

Después de permanecer unos pocos días encerrado en el cuartel local de la Gendarmería, fue alojado en la casa particular de la familia Meza, que se había ofrecido voluntariamente a hospedarlo. Allí mantuvo un romance con una maestra rural veintidós años menor que él, Martha Meza, hija de sus anfitriones, con la cual tuvo un hijo no reconocido, Carlos Nair. Meza tendría una carrera política propia como diputada provincial de Formosa y, posteriormente, nacional, y mantendría un controvertido litigio legal con Menem por la filiación de su hijo, hasta su suicidio el 12 de enero de 2003. Entrando en la década de 1980, luego de que el gobierno militar comenzara a liberar a presos políticos del gobierno constitucional anterior, la dirigencia justicialista presionó legalmente por la situación de Menem, otorgándosele la libertad de movimiento 8 de enero de 1981. Se trasladó finalmente a La Rioja, asentándose en forma definitiva, unos meses más tarde.

Con la derrota del gobierno militar en la guerra de las Malvinas, asumió el presidente "de facto" Reynaldo Bignone, que inició una transición a la democracia. En octubre de 1982, Menem concedió una entrevista a la Revista Extra, en la que describió sus puntos de vista con respecto al verticalismo justicialista, apoyó la conducción legal del PJ por parte de Isabel Perón, aunque se manifestó a favor de que se mantuviera neutral en las tensiones intrapartidarias, cediera poder a Deolindo Bittel y que las líneas divergentes respetaran los resultados de las elecciones internas y apoyaran a los candidatos vencedores, y expresó su deseo de que se investigaran los sucesos ocurridos durante la dictadura militar. No rechazó tampoco una investigación sobre los sucesos ocurridos entre 1973 y 1976. Se mostró a favor de que se volviera temporalmente a la constitución de 1853, pero con el objetivo de buscar una nueva reforma más adelante.

En septiembre de 1983 viajó a Madrid y trató de entrevistarse con Isabel Perón, pero ella se negó a recibirlo.

La transición a la democracia culminó con las elecciones generales de 1983. De cara a las elecciones internas del Partido Justicialista, Menem configuró la lista «Lealtad y Dignidad», en la que había logrado aglutinar con éxito a gran parte de las facciones internas del partido (los sindicatos, la Juventud Peronista, la rama femenina y presos políticos). Con este armado retuvo exitosamente el control del justicialismo riojano con éxito y se postuló nuevamente para la gobernación, con el radical Raúl Alfredo Galván como principal contrincante. Su compañero de fórmula fue Alberto Cavero.
Menem realizó una campaña agresiva, buscando un efecto polarizador utilizando términos despectivos para referirse a sus contrincantes radicales, a quienes calificó de «antipatrias, gorilas y oligarcas». El radicalismo, mientras que centró su campaña en la no violencia y el respeto mutuo, describió a Menem como «un demagogo autoritario». Durante un acto de campaña, Menem relativizó la existencia de una democracia si no había mecanismos que garantizaran la justicia social, con la frase «¿Para qué queremos libertad si no tenemos para comer?». A pesar de su discurso polarizador y diferenciador, Menem ideológicamente se mostró en esencia ambiguo, en un intento por atraer votos tanto a la izquierda como a la derecha del peronismo, y constantemente aludió a electorados de otros partidos, como el socialismo popular y el FIP.

Los comicios tuvieron lugar el 30 de octubre. A nivel nacional, triunfó el candidato presidencial radical Raúl Alfonsín, derrotando por holgado margen al candidato justicialista Ítalo Lúder. En La Rioja, sin embargo, no se replicó el resultado y en un clima de extrema polarización, Menem resultó electo gobernador por segunda vez con el 56,51% de los sufragios (50.466 votos) seguido por el 39,87 % (35 605 votos) obtenido por Galván, imponiéndose en todos los departamentos menos en General Ángel V. Peñaloza. El PJ obtuvo tres cuartos de las bancas en la Cámara de Diputados provinciales con 21 escaños contra 4 de la UCR. Aunque Lúder se impuso en La Rioja, hubo un ligero corte de boleta en favor de Menem: aproximadamente entre 1800 y 3000 personas votaron únicamente por él y no por el resto de la boleta justicialista (2393 no votaron por Lúder en la elección presidencial, 3050 no votaron por la lista de diputados nacionales encabezada por Bernardo Eligio Herrera, y 1881 no votaron por los candidatos a diputados provinciales).

Dos semanas después de su victoria, Menem se entrevistó con Alfonsín, en calidad de presidente y gobernador electos. Una fotografía de la reunión, que fue publicada en medios de comunicación nacionales y provinciales, desató críticas para ambas figuras: políticos del justicialismo criticaron a Menem por reunirse con Alfonsín después de enterarse de su victoria, mientras que el radicalismo local se mostró desconcertado por la reunión y manifestó que esta era un «desmerecimiento» a la militancia radical riojana. El excandidato justicialista a gobernador de Córdoba, Raúl Bercovich Rodríguez, fue uno de los principales críticos a la reunión del gobernador y presidente electos. Menem descartó las críticas, describió la entrevista como «realmente positiva para la democracia y para la provincia de La Rioja», e hizo alusión indirecta a Bercovich, cuya derrota por dieciséis puntos ante el radical Eduardo Angeloz había sido el mayor revés para el justicialismo a nivel gubernativo, afirmando que dialogaba «con los vencedores, no con los derrotados». Durante la reunión, Alfonsín había prometido a Menem (que sería el primer gobernador riojano de signo político distinto al del presidente de turno desde la década de 1920) que no habría discriminación política o económica para las provincias gobernadas por el PJ.

Menem juró el cargo el 10 de diciembre de 1983, recibiendo la gobernación de manos del interventor "de facto" Guillermo Piastrellini. En su asunción dio el siguiente mensaje: «el gobierno defenderá sin concesiones y tomando las medidas que sean necesarias el derecho de todo riojano a una entrada económica que le permita sostener dignamente a su familia, el derecho de todo riojano a poseer un techo adecuado a su condición de ser humano, el derecho de todo riojano de contar con atención calificada a su salud y el derecho de todo riojano a estudiar, a perfeccionarse y a realizarse de acuerdo a su vocación. Ello significa que el equilibrio social será consecuencia natural de la justicia social».

Menem mantuvo en sus cargos a numerosos funcionarios de la administración provincial del régimen militar, así como fue incorporando a dirigentes del radicalismo, el Movimiento de Integración y Desarrollo, el Frente de Izquierda Popular, y el Partido Demócrata Cristiano. Estas medidas, además de consolidar su hegemonía dentro del justicialismo riojano como un líder conciliador, provocaron rápidas fracturas dentro de los partidos no peronistas y, en consecuencia, debilitaron a la oposición provincial.

De su segunda gobernación destaca una visita realizada a la provincia en 1986 por el senador de los Estados Unidos Edward Kennedy, que se manifestó satisfecho con dicha visita y elogió al gobernador. Su llegada respondía supuestamente a preocupaciones del gobierno estadounidense con respecto al crecimiento del poder de Menem dentro del Partido Justicialista y a la retórica antiimperialista que había empleado en sus campañas electorales. Después de la visita de Kennedy, Menem relajó notoriamente sus opiniones con respecto a Estados Unidos, declarando: «Estados Unidos, nos guste o no, es una realidad que pesa sobre Argentina y sobre todos los países del mundo, sin excepción. Con esto quiero decir que Kennedy no es Rockefeller y viceversa, aunque se los suponga componentes de un mismo conjunto». Los medios nacionales destacaron esta última frase («Kennedy no es Rockefeller») con la cual descartó los comentarios sobre la reunión del gobernador con el senador estadounidense.

El estatuto del gobierno militar bajo el que Menem había sido elegido gobernador en 1973 permitía la reelección del gobernador provincial para un segundo mandato consecutivo. Sin embargo, dicho estatuto había perdido vigencia "de jure" en 1981 (con su vigencia "de facto" realmente anulada tras el golpe de 1976), y Menem fue elegido en 1983 bajo la constitución provincial de 1855, que establecía que el gobernador no podía ser reelegido consecutivamente. Al mismo tiempo que se celebraban las elecciones legislativas de medio término, Menem convocó a comicios para una convención constituyente provincial que reformara la carta magna. El justicialismo menemista obtuvo la victoria en dichas elecciones por holgado margen, permitiendo al gobierno de Menem un gran margen de maniobra en la confección de la nueva constitución. Esta ampliaba el número de miembros de la legislatura provincial y habilitaba la reelección del gobernador y el vicegobernador sin límite de mandatos. Con esta reforma y debido a la crisis aguda que enfrentaba el gobierno de Alfonsín, Menem resultó abrumadoramente reelegido en las elecciones de 1987, obteniendo el 62,41% de los votos contra el 33,88% de Enrique Peñaloza Camet, su oponente radical, que hizo una agresiva campaña acusando a Menem de tener un estilo de gobierno muy similar al de su predecesor "de facto", Piastrellini. Menem desfasó las elecciones de gobernador de las de diputados provinciales, que tuvieron lugar meses más tarde, con un arrollador triunfo para el PJ, que retuvo los dos tercios del poder legislativo.

Desde su victoria en las elecciones gubernativas riojanas de 1983, Menem dedicó gran parte de su mandato como gobernador a construir una imagen nacional que le permitiera perfilarse como un posible presidenciable dentro del justicialismo, intención que quedó evidenciada con sus numerosas visitas a otras provincias entre 1984 y 1988, durante las cuales buscó establecer contacto cara a cara con la población local. Dentro de La Rioja encaró sus campañas electorales prometiendo a la población riojana que un proyecto presidencial favorecería a la provincia. Adquirió lentamente algo de poder dentro del PJ nacional debido a su éxito en preservar su hegemonía política personal a pesar de las profundas divisiones que aquejaron al partido durante el período 1983-1987. Menem había formado parte de la denominada «renovación peronista» encabezada a nivel nacional por Antonio Cafiero (elegido en 1987). La renovación aparejaba la democratización partidaria, permitiendo a los afiliados votar en elecciones internas, por lo que a mediados de 1988 se realizó la primera primaria presidencial directa de la historia del peronismo.
Para entonces, Menem y Cafiero eran vistos públicamente como las dos mayores figuras capaces de obtener la candidatura presidencial. Menem aceptó disputar una interna contra Cafiero, pero Julio Mera Figueroa, operador político de Menem, impuso como condición de que la misma se realizara bajo un sistema de «boleta corta», disputándose solo la candidatura presidencial sin dirimir cargos legislativos o gubernativos, propuesta que fue vista por Cafiero como un intento de «licuar» el peso del aparato partidario, mayormente favorable al gobernador de Buenos Aires. El compañero de fórmula de Menem fue Eduardo Duhalde, ex intendente de la localidad bonaerense de Lomas de Zamora, mientras que el de Cafiero fue el excandidato a , José Manuel de la Sota.

Al momento de la elección se consideraba asegurada la victoria de Cafiero debido a su imagen consolidada a nivel nacional, habiendo además obtenido la gobernación de la provincia más poblada del país tan solo unos meses atrás. Buenos Aires y Córdoba, distritos de origen de Cafiero y De la Sota, albergaban juntos más de la mitad del padrón de afiliados al PJ, mientras que La Rioja apenas reunía el 2%. Además, la imagen de Cafiero como un líder renovador parecía contrapesar a Menem, visto como un «populista retrógrado». Sin embargo, durante el período de campaña numerosos hechos favorecieron al gobernador riojano. La participación de Duhalde como precandidato a vicepresidente le concedió al naciente menemismo la aparición paulatina de un peso electoral propio en el crucial conurbano bonaerense. Disfrutó también del apoyo clave del locutor Juan Carlos Rousselot, intendente de Morón. El propio Cafiero escribiría en su diario personal que algunas encuestas le vaticinaban una derrota en el período previo a la realización de la interna.

Menem centró su campaña en perfilarse como un candidato «antisistema», mientras que Cafiero como presidente del Partido Justicialista había mantenido políticas de entendimiento y acercamiento con el gobierno de Alfonsín, lo que provocó que fuera visto también como un continuista de las políticas del alfoninismo debilitado. A pesar de que el propio Menem había mostrado una actitud similar para con Alfonsín durante su primer mandato como gobernador, de cara a las elecciones internas se distanció de él a la par de su alejamiento del sector de Cafiero. Del mismo modo, la relativa estabilidad de La Rioja, así como su escasa importancia en el panorama político permitieron a Menem presentarse como una figura nueva ante la inmensa mayoría del público, mientras que Cafiero se vio desfavorecido por el deterioro de la situación económica, que afectó mayormente a la provincia de Buenos Aires, que él gobernaba. Dos días antes de la realización de la primaria, Menem finalizó su campaña con un acto sorpresivamente concurrido en el Estadio Antonio Vespucio Liberti, Capital Federal, que registró una presencia de 60.000 personas, desconcertando a la dirigencia cafierista.

El sábado 9 de julio de 1988 tuvo lugar la elección primaria con un sorpresivo y holgado triunfo para la fórmula Menem-Duhalde, que logró el 53,94 % de los votos contra el 46,06 % de la fórmula Cafiero-De la Sota. El binomio encabezado por Menem arrasó en el conurbano bonaerense (destacando su victoria en Lomas de Zamora, La Matanza y Morón) y se impuso en dieciocho de los veinticuatro distritos nacionales, mientras que Cafiero triunfó solamente en Capital Federal, Córdoba, Formosa, Misiones, Salta y Santiago del Estero. El mayor apoyo provino de la Patagonia argentina y la región de Cuyo, donde Menem superó cómodamente el 75% de los votos en la mayoría de las provincias, mientras que la elección fue particularmente competitiva en el extremo norte argentino, donde se ubicaban cuatro de los seis distritos en los que la fórmula resultó derrotada. En La Rioja, Menem obtuvo 36.949 votos contra 564 de Cafiero, un 98,50 % de los votos. Cafiero reconoció la derrota en la madrugada del 10 de julio.

De cara a las elecciones presidenciales, el Partido Justicialista resolvió retornar a la estrategia frentista de 1973, configurando una alianza con partidos menores denominada Frente Justicialista de Unidad Popular (FREJUPO). La misma estaba compuesta, además de por el PJ, por el Partido Intransigente (PI), encabezado por Oscar Alende, el Movimiento de Integración y Desarrollo (MID), el Partido Demócrata Cristiano (PDC), el Partido del Trabajo y del Pueblo (PTP), el Partido Nacionalista Constitucional (PNC), Movimiento Línea Popular (MOLIPO) y el Movimiento Patriótico de Liberación (MPL). La distintas listas distritales con candidatos al Colegio Electoral estaban compuestas por miembros de los partidos del frente, además del justicialismo, y muchos destacados dirigentes de los partidos más importantes (como el MID), ocuparían cargos en la posterior gestión de gobierno del PJ. Luego de su derrota en la primar interna, Cafiero aseguró "todo su apoyo" a la candidatura de Menem, y encabezó la lista para electores de presidente en la provincia de Buenos Aires. Juan Carlos Rousselot, César Arias, José Luis Barrionuevo, y Diego Ibáñez, figuras que habían jugado un papel trascendental en configurar el espacio político de Menem fuera de La Rioja, integraron los primeros diez puestos. Poco antes de la elección se desató un escándalo de corrupción y el 19 de abril de 1989, Rousselot fue destituido de su cargo de intendente de Morón por supuestas irregularidades financieras y reemplazado por Arias, aunque volvería a la intendencia en las próximas elecciones.

A pesar de la complicada situación económica del país, y del amplio triunfo del PJ en las elecciones legislativas de 1987, la victoria de Menem en la primaria justicialista llegó a poner en duda brevemente sus posibilidades de triunfo. Prácticamente todas las encuestas electorales de la época vaticinaban una competencia cerrada entre Menem y el candidato del radicalismo, Eduardo Angeloz, que había formado una alianza con partidos políticos provinciales de derecha, y destacaban un alto porcentaje de indecisos, de entre un cuarto y un tercio del electorado. Sin embargo, Menem se mantuvo primero, con un holgado piso de entre el 32.4 y el 39.4% entre abril y mayo. Angeloz, fuertemente antiperonista, y uno de los máximos exponentes del ala neoliberal de la UCR, había alcanzado la candidatura por ser uno de los dos únicos gobernadores radicales (junto al rionegrino Horacio Massaccesi) que habían resistido con éxito la ola de triunfos justicialistas en 1987. Su programa económico consistía en una significativa reducción del gasto público, comprometiéndose a aplicar "Lápiz Rojo". El economista conservador Álvaro Alsogaray, que se postulaba por una coalición de partidos liberales, mantuvo un piso electoral importante, de entre el 7.9 y el 9%.

Menem lanzó su campaña realizando propuestas económicas muy vagas, que le permitieron captar un electorado mucho más amplio que sus contrincantes (en su mayoría liberales). Se trató mayormente de un proselitismo populista, destacando eslóganes como «¡Se viene el salariazo!» o «¡Hagamos la Revolución Productiva!». Sin embargo, probablemente el lema de campaña más famoso de Menem, que sería muy recordado posteriormente, fue «Síganme, no los voy a defraudar» en un afiche con la fotografía de Menem y la frase debajo. Menem apeló también al voto religioso, haciendo uso de citas bíblicas, destacando "Argentina, levántate y anda" y la expresión "a los tibios los vomita Dios". La campaña menemista contó también con el jingle "Valerosos Corazones" compuesto por Litto Nebbia e interpretado por Silvia Garré.
Pese a la discreción de Menem a la hora de tratar temas económicos, la posibilidad de que fuera un candidato «neoliberal encubierto» fue barajada incluso antes de las elecciones por algunos medios de comunicación y figuras de la izquierda política. El periódico trotskista "Prensa Obrera", ligado al Partido Obrero (PO), que postulaba a Jorge Altamira como candidato a presidente, afirmó que Menem era realmente un político conservador, y destacó la frase "Vote un Menem y le saldrá un Alsogaray", en un artículo publicado en marzo de 1989. Al momento de su publicación, el artículo desató críticas y burlas de parte de dirigentes del peronismo, pero posteriormente adquirirían validez cuando Menem iniciara un acercamiento con sectores del neoliberalismo argentino una vez en el poder, entre los que destacarían principalmente Alsogaray y su hija, María Julia.

La tendencia electoral se invirtió enormemente en favor de Menem cuando, a partir de febrero de 1989, se produjeron sorpresivos movimientos financieros que desencadenaron un proceso hiperinflacionario. Después de una fuerte caída en las reservas del Banco Central, el dólar estadounidense tuvo un aumento de alrededor del 40% con respecto al austral argentino a partir del 7 de febrero, momento conocido como el "Martes Negro". El repentino descenso del valor austral amenazó la tenue estabilidad financiera del país y, el Banco Mundial recortó un gran tramo de un paquete de préstamos acordado en 1988, lo que hizo que el valor del austral cayera en picado: el dólar pasó de cotizarse a 17 australes en enero, a cotizarse a más de 100 australes para mediados del año. La inflación, que estaba entre el 5-10% en febrero, aumentó un 78.5% para mayo. En medio de una situación económica prácticamente insostenible, el gobierno de Alfonsín adelantó las elecciones al 14 de mayo, cuando estas deberían haber tenido lugar en octubre.

Un mes antes de las elecciones, el programa de televisión Nuevo Tiempo invitó a Menem y a Angeloz a realizar un debate presidencial el 8 de mayo, una semana antes de los comicios. De haberse realizado, hubiera sido el primer debate presidencial televisado de la historia argentina. Angeloz confirmó inmediatamente su presencia, mientras que Menem se negó a hacerlo y mantuvo una postura vaga. Finalmente, el día del debate, Angeloz estuvo presente y Menem confirmó finalmente su ausencia. El programa, conducido por Bernardo Neustadt, decidió realizar el debate de todas formas, constituyendo simplemente en una entrevista con Angeloz mientras este ocupaba un estrado con su nombre, junto a un estrado vacío destinado a Menem. El radicalismo utilizó la ausencia de Menem en el debate, retratándolo en un spot televisivo como una «silla vacía» que no debatía con Angeloz por no tener «la capacidad para ejercer la presidencia». Sin embargo, la campaña de Menem contraatacó eficazmente la crítica con un nuevo spot, en el que aparecían varias sillas vacías, mientras una voz en off decía que estas representaban debates «que el radicalismo no pudo sostener: con los empresarios, los jubilados, y los obreros», llamando a los argentinos a «cambiar la historia». Al igual que en las anteriores elecciones, los candidatos realizaron masivos actos de cierre de campaña.

El domingo 14 de mayo tuvieron lugar las segundas elecciones presidenciales tras la restauración democrática. Menem obtuvo una amplia victoria con las listas de electores que lo apoyaban en los veinticuatro distritos recogiendo el 48,51% de los votos positivos contra el 37,10% logrado por el total de las listas que apoyaban a Angeloz, y un 7,17% obtenido por Alsogaray. Menem obtuvo mayoría propia en el Colegio Electoral, con 312 votos a su favor contra 234 de Angeloz, 33 de Alsogaray y el resto distribuido en partidos menores. En contraste con las elecciones anteriores, que demostraron una división geográfica entre provincias inclinadas al radicalismo y provincias inclinadas al justicialismo, en esa ocasión el FREJUPO triunfó en casi todo el país, excepto en los bastiones radicales de Córdoba y Capital Federal, así como en Salta y Chubut, donde ganó Angeloz. Logró imponerse en distritos hasta entonces tradicionalmente esquivos al peronismo, como Corrientes y Río Negro, y obtuvo más de dos tercios de los votos en su provincia natal, La Rioja. El PJ y sus aliados electorales lograron además obtener el control del Congreso de la Nación, con 128 diputados sobre 254 y 28 senadores sobre 46.

Tras conocerse su victoria, Menem convocó a «todos los sectores» y emitió su primer mensaje público desde un acto realizado en Catamarca en homenaje al fallecido goberandor Vicente Saadi. A pesar del adelantamiento electoral, Alfonsín había anunciado previamente que completaría su mandato tal y como estaba constitucionalmente establecido, el 10 de diciembre, por lo que todavía faltaban más de siete meses para que el presidente y los diputados electos asumieran. Sin embargo, la contundente derrota electoral desató discusiones internas dentro del radicalismo y presiones externas crecientes para una entrega anticipada de mando. Además, la victoria justicialista no estabilizó la economía como Alfonsín esperaba y la inflación continuó aumentando. El dólar se duplicó en valor durante la siguiente semana y el 29 de mayo disturbios estallaron en las zonas más pobres de varias ciudades. La pobreza comenzó a crecer de modo exponencial: en mayo era del 25% y aumentaría a un 47% a lo largo del año. Los conflictos sociales continuaron aumentando y el 30 de mayo, Alfonsín debió decretar el estado de sitio sobre todo el territorio nacional. Una reunió con distintos grupos empresariales el 9 de junio, que tenía como objetivo recibir ayuda económica para finalizar el mandato, no prosperó. Durante dicha reunión, Héctor Magnetto, que posteriormente mantendría una relación cercana con el menemismo, habría descartado a Alfonsín como un "obstáculo".
Con respecto al período restante para su juramentación, Menem se mostró sumamente vago y discreto con respecto a la gobernabilidad del alfonsinismo. Posteriormente, declaró que estaba «listo para asumir». El 6 de junio, anunció la composición de parte de su gabinete, todavía en gestación, seis meses antes de la entrega prevista de mando, lo que fue visto como una señal de que estaba proponiendo un juramento anticipado. Al momento del anuncio del gabinete, Menem barajaba dos figuras para el clave Ministerio de Economía: el diputado Domingo Cavallo (que posteriormente ejercería el cargo) y el ingeniero Miguel Roig. Roig fue finalmente el elegido, mientras que a Cavallo se le otorgó la cartera de Relaciones Exteriores, lo que evidenció el peso que tendría la economía en la política internacional del nuevo gobierno. Para mediados de junio, el Ministerio de Trabajo era el último por anunciar y se especuló que Menem elegiría a una figura del sindicalismo peronista.

El fracaso de una última reunión con los grupos económicos, que se negaron a dar Alfonsín el apoyo necesario para finalizar su mandato, terminó de debilitar al gobierno y, al día siguiente, el 14 de junio, el presidente emitió un mensaje por cadena nacional, anunciando que había resuelto "resignar" a su cargo a partir del 30 de junio y entregar su cargo a Menem en calidad de presidente electo. El uso de la palabra "resignar" y no "renunciar", como correspondía legalmente, generó confusiones y pujas políticas entre ambas fuerzas.

Poco antes de que Alfonsín anunciara públicamente su "resignación", el dirigente radical Rodolfo Terragno viajó a La Rioja para negociar con Menem de cara al inminente cambio de gobierno. De acuerdo con Terragno, durante la reunión, que tuvo lugar en la casa de gobierno riojana, Menem se mostró en última instancia sorprendido por la intención de Alfonsín de retirarse antes de tiempo y, a pesar de sus declaraciones previas, declaró no estar preparado para asumir tan repentinamente, dado que asumiría con una situación económica y política extremadamente difícil y debería gobernar hasta diciembre con una Cámara de Diputados en la que el radicalismo aún tenía la primera minoría. Luego de barajarse la posibilidad de que su hermano, Eduardo Menem, presidente provisional del Senado, asumiera interinamente la jefatura de estado hasta la juramentación de Menem, se resolvió que se adelantaría la entrega de mando pero, a su vez, se organizaría con algo de tiempo (poco más de un mes) para que Menem pudiera prepararse. Las condiciones para la entrega de mando fueron que la banda presidencial la entregaría el propio Alfonsín para evitar malentendidos y que la UCR aprobaría todas las leyes que el nuevo presidente requiriera hasta que la legislatura electa de mayoría justicialista jurara su cargo en diciembre.
Aunque la entrega originalmente estaba planeada para el 30 de junio, finalmente Menem asumió como presidente de la Nación Argentina el 8 de julio de 1989, ocho días después de lo esperado, pero cinco meses y dos días antes de lo constitucionalmente previsto. La ceremonia tuvo lugar en el Palacio del Congreso de la Nación Argentina, y Alfonsín le colocó la banda presidencial a Menem. Se trató de la primera transición presidencial entre dos presidentes democráticamente electos de distintos partidos políticos, e históricamente se considera que dicho momento consolidó la democracia en la Argentina. El papel del Partido Justicialista, de sus gobernadores (sobre todo el santafesino Víctor Reviglio), o del propio Menem en el adelantamiento del traspaso de mando continúa siendo objeto de controversia. Alfonsín, en una entrevista posterior, se achacó parte de la responsabilidad al admitir que pudo ser un error haber adelantado tanto las elecciones. Años después, durante un discurso, Menem cometió un acto fallido al admitir incidentalmente haber «tumbado» al gobierno de Alfonsín, cuando quería referirse a los golpes de estado de 1955 y 1962 que derrocaron respectivamente a Juan Domingo Perón y a Arturo Frondizi, excusándose en que se confundió porque "los dos [Alfonsín y Frondizi] son radicales".

Carlos Menem asumió la presidencia el 8 de julio de 1989, tras el retiro anticipado de Raúl Alfonsín. Fue por entonces la primera sucesión presidencial entre dos presidentes constitucionales desde 1928, y la primera desde 1916 entre presidentes de diferentes partidos políticos.

El 14 de mayo de 1995 se realizaron las elecciones presidenciales en las que la fórmula que formó con Carlos Ruckauf se impuso con el 49,94% de los votos a los candidatos del FREPASO (29,3%) y de la UCR (16,99%).

El comienzo de una recesión en el tercer trimestre de 1998 y nuevas acusaciones de corrupción tuvieron como consecuencia un descenso en su popularidad: luego de un nuevo intento de reforma constitucional —esta vez fallido—, Menem terminó su gobierno el 10 de diciembre de 1999, traspasándole el mando al presidente electo, el radical Fernando de la Rúa.

El principal problema que debió enfrentar al asumir la presidencia fue el de una economía en crisis con hiperinflación y en una profunda recesión. En el marco de la fuerte espiral hiperinflacionaria que azotaba a la Argentina desde los últimos años de Alfonsín, Menem se reunió el 23 de mayo de 1989 con el directorio de un grupo económico, Bunge y Born, a pocos días de haber ganado las elecciones. Miguel Ángel Roig fue nombrado Ministro de Economía, quien hasta ese momento se desempeñaba como vicepresidente ejecutivo general de Bunge & Born y a partir de su designación se dedicó a diseñar el denominado "Plan BB". Ante el súbito fallecimiento de Roig, Carlos Menem nombró como sucesor al vicepresidente de Bunge & Born, Nestor Rapanelli. El gobierno adoptó parcialmente los principios del Consenso de Washington, para esto introdujo una serie de reformas liberales: se desreguló la economía, reduciendo cupos, aranceles y prohibiciones de importaciones, se estableció la libertad de precios y se produjo la privatización de numerosa empresas estatales. 

Con la aprobación de la Ley de Reforma del Estado en agosto de 1989, fue autorizado a privatizar varias empresas estatales, en la forma que el presidente estimara conveniente. Las primeras privatizaciones efectuadas fueron las de la empresa telefónica Entel y la de Aerolíneas Argentinas. Las mismas, y otras posteriores, se privatizaron rápidamente buscando conseguir con ello réditos mediáticos que instalaran la idea de la voluntad reformista del gobierno, pero dicha rapidez condujo luego a numerosas críticas y denuncias de irregularidades, omisiones y casos de corrupción. Pronto se privatizaron también la red vial, los canales televisivos (con la excepción de ATC, hoy Canal 7), gran parte de las redes ferroviarias, YPF y Gas del Estado. 

Con el aumento de impuestos como el IVA (del 19 al 21 %) y Ganancias aumentó la recaudación fiscal.. Aun así, a pesar de dicho aumento y de los ingresos generados por las privatizaciones, la situación económica se mantenía convulsionada y a fines de 1989, se produjo una segunda hiperinflación. En reemplazo de Rapanelli asumió como Ministro de Economía Erman González, quien impulsó el Plan Bonex (abreviación de BONos EXternos), que consistió en la confiscación de los depósitos a plazo fijo y un cambio de los mismos por bonos de largo plazo en dólares. Así mismo, restringió fuertemente la emisión monetaria y redujo el gasto social. Este plan agravó la recesión económica, pero sirvió para reducir la inflación. Se produjo el cierre de unidades productivas que, en algunas ramas de la actividad, como la textil, fueron masivas, con la subsecuente pérdida de puestos de trabajo. Logró reducir la inflación que terminó en 1990 en 2314 % anual. La inflación disminuyó hasta llegar a valores cercanos al 5 % mensual en el último trimestre del año. La balanza comercial en 1990 obtuvo un superávit extraordinario. Las exportaciones superaron en un 34 % a las del año anterior, mientras que las importaciones sólo fueron un tercio de aquellas. Para 1991 se llevó a delante la eliminación de la actualización monetaria o desindexación, para evitar que se trasladara hacia adelante la inflación pasada Se rebajaron los encajes bancarios de los depósitos en australes y se aumentaron los correspondientes a depósitos en moneda extranjera. 

En enero de 1991 renuncia Erman González y llega el turno al cuarto ministro de economía del presidente Menem, Domingo Cavallo, quien estableció la Ley de Convertibilidad. Este esquema incluyó la creación de un nuevo signo monetario: el peso convertible, que comenzó a circular en el país desde el 1 de enero de 1992, y reemplazó a la entonces vigente moneda nacional, el austral, con una equivalencia de 1 peso a 10.000 australes. Bajo este sistema, el Banco Central estaba obligado a respaldar con sus reservas una relación de cambio en la que un dólar valía lo mismo que un peso; de esta forma, se restringía la emisión de billetes como medio de financiamiento del Estado. La aplicación del régimen de convertibilidad se prolongó hasta la crisis argentina de fines de 2001 y comienzos de 2002. 

En los servicios públicos, las privatizaciones produjeron mejoras de calidad en algunos rubros (electricidad y telefonía, entre otros), mientras que en otros el impacto fue negativo, como en los transportes ferroviarios, este último en particular por el cierre masivo de los servicios de pasajeros de larga distancia, ocurrido el 10 de marzo de 1993. Si bien los servicios de trenes privatizados urbanos del área metropolitana y cargas en general registraron leves mejorías, finalmente con la crisis de 2001 y la posterior devaluación de la moneda desnudaron las frágiles condiciones contractuales que llevaron a las empresas a la quiebra, el posterior vaciamiento de su infraestructura y finalmente a un deterioro del servicio en parte sostenido por subsidios. Al mismo tiempo, los principales inconvenientes económicos generados por esta política fueron una disminución de la competitividad basada en el tipo de cambio y un crecimiento del desempleo.

Todas estas medidas en su conjunto lograron una estabilidad económica sin inflación significativa que ofreció un clima favorable para el surgimiento de inversiones y el ingreso de capitales desde otros países, produciéndose un marcado crecimiento del producto interno bruto (PIB). La estabilidad económica lograda durante el primer mandato de Menem lo impulsaron a su reelección en 1995 con casi el 50% de los votos. La mejora fiscal sirvió para lograr el acuerdo del plan Brady (con los acreedores externos). El PBI aumentó un 50 % en 10 años y llegó a 288.194 millones de dólares en 1998. Producto de políticas macroeconómicas consistentes desde el primer trimestre de 1990; sobre 39 trimestres 30 mostraron expansión económica, revirtiendo la historia de los 15 años previos de estancamiento. Tras la salida de Cavallo, y el ingreso de Roque Fernández, se redujo el déficit fiscal llagando al equilibrio presupuestario para 1995 del orden de un superávit del 0.3 por ciento del pbi, manteniéndose en números positivos hasta 1998.
Sin embargo, el crecimiento económico estuvo caracterizado por el incremento del sector de servicios y agropecuario, mientras la industria local se contraía debido a la reducción de aranceles a bienes importados. La estabilidad económica fue, entonces, a costa de un mayor desempleo. Entre 1990 y 1994, la productividad creció un 7,3% anual, pero la modernización el uso de tecnología, produjo una expansión económica poco intensiva en trabajo. Al asumir Menem el gobierno, los valores de desocupación y subocupación habían alcanzado picos históricos (8,1% y 8,6% de la población económicamente activa, respectivamente). Luego de un período de lenta disminución (6,9% y 8,3% en mayo de 1992), el desempleo y el subempleo volvieron a crecer durante la "crisis del Tequila", hasta alcanzar un pico de 18,4% y 11,3% en mayo de 1995, tras lo cual bajaron levemente hasta 12,4% y 13,6% en octubre de 1998. Para el final de su gobierno, estas cifras eran de 13,8% y 14,3%. Contribuyeron al aumento del desempleo y el subempleo los despidos masivos en las empresas públicas privatizadas, la terciarización de actividades y las sucesivas medidas de flexibilización laboral.

Hacia 1997 y 1998 la pobreza había afectado a más del 36 % de la población (13,4 millones de personas, se encontraban bajo la línea de pobreza). En tanto que el 8,6 % (3,2 millones de personas) vivían en la indigencia. En las regiones del noroeste, nordeste y Cuyo la pobreza superaba el 50 % de la población y la indigencia rozaba el 20 %.Contribuyeron al aumento del desempleo y el subempleo, los despidos masivos en las empresas públicas privatizadas, la terciarización de actividades y las sucesivas medidas de flexibilización laboral.

Entre fin de 1989 y 1995 la deuda pública neta del Estado Nacional bajó 3.765 millones USD, de 96.472 millones a 92.707 millones, pero esta tendencia se revirtió al final de su mandato cuando la deuda externa aumentó hasta us$ 145.000 millones en el 2000.

Durante su gobierno, se modificó por ley del Congreso el número de integrantes de la Corte Suprema de Justicia, elevándolo a nueve miembros. Parte de la prensa denominó a esta corte ampliada la "mayoría automática", aduciendo que en la mayor parte de los casos polémicos los votos de estos cinco jueces coincidían con la posición del gobierno.

Durante su gestión, Argentina fue blanco de dos ataques terroristas: el 17 de marzo de 1992 se produjo el primer atentado contra la embajada de Israel, donde murieron 22 personas y el 18 de julio de 1994 se produjo el Atentado a la AMIA (Asociación Mutual Israelita Argentina), que causó la muerte de 85 personas. La investigación del primer atentado fue realizada por la Corte Suprema de Justicia, sin que fuera elevada nunca a juicio. Respecto del segundo atentado a la AMIA, cuando se realizó el juicio oral entre 2001 y 2003, quedó al descubierto una gigantesca red de encubrimiento que involucraba al juez de la causa, los fiscales, los servicios de inteligencia, el entonces presidente Menem y altos funcionarios de su gobierno, y hasta el presidente de la Delegación de Asociaciones Israelitas Argentinas (DAIA), que generó un segundo juicio por encubrimiento, iniciado en 2015 y que aún no había finalizado a octubre de 2018.

Tras lograr un acuerdo con Raúl Alfonsín, líder de la UCR, impulsó la reforma de la Constitución que fue aprobada por la convención en 1994 y permitió la reelección de Menem al año siguiente.

Suspendió el servicio militar obligatorio tras el escándalo a raíz del Caso Carrasco. Indultó a militares de la última dictadura cívico-militar (1976-1983) y a militantes de organizaciones guerrilleras que habían actuado principalmente durante la década del setenta, lo que sin embargo no calmó el descontento de los militares que amenazaron después con otro intento de golpe.

El 3 de noviembre de 1995 explotaron los depósitos de la Fábrica Militar de Río Tercero. Murieron siete personas y se produjeron daños materiales en la ciudad. Se sospechó que en realidad no fue un accidente, y que lo que se buscaba era ocultar un faltante de armas (ver sección Vida pública después de sus presidencias).
Durante su gestión se reformó el sistema educativo en todos sus niveles con la sanción de la Ley Federal de Educación en 1993 y la Ley de Educación Superior en 1995. Se crearon nueve universidad nacionales, principalmente en el Gran Buenos Aires. Los docentes realizaron una protesta durante dos años, conocida como Carpa Blanca en rechazo a la Ley Federal de Educación.

En 1998, luego de una entrevista con el papa Juan Pablo II en la Ciudad del Vaticano, Carlos Menem aprobó un decreto que declaró el 25 de marzo el Día del Niño por Nacer en la Argentina. Durante la primera celebración, en 1999, el presidente afirmó que "la Argentina ha colocado entre las prioridades de su política exterior, una firme y decidida acción en defensa de la vida". En este contexto fue que Zulema Yoma, exesposa del presidente, decidió contar en una entrevista que se realizó un aborto con el apoyo de Carlos Menem.

En política exterior, desde el inicio mismo de su mandato se promovió un alineamiento automático con los Estados Unidos, de modo tal que Argentina abandonó el Movimiento de Países No Alineados. El Ministro de Relaciones Exteriores, Guido Di Tella, se refirió a dicho alineamiento en forma humorística como las «"relaciones carnales»", pero más adelante el término sería tomado por los críticos de esta política internacional para referirse a la misma en forma denigratoria. Las relaciones que mantuvo el gobierno menemista con los Estados Unidos causaron que Argentina fuera nombrada un aliado importante extra-OTAN en 1998, durante la administración de Bill Clinton.

En otros planos, en 1991, Menem promovió la formación del Mercosur y restableció relaciones diplomáticas con el Reino Unido, interrumpidas desde la Guerra de Malvinas.

Las denuncias de corrupción sobre su gobierno no impidieron que su gestión mantuviera una imagen favorable debido al éxito en la faz económica. En 1993, su Ministro del Interior, Gustavo Béliz, renunció a su cargo y declaró públicamente que el presidente «"estaba rodeado de corruptos»". Poco después de la implementación del Plan Bonex tuvo lugar el "Swiftgate", en el cual la empresa estadounidense "Swift" denunció verse perjudicada en una operación comercial al no aceptar otorgar un soborno. "Swift" recurrió al embajador de los Estados Unidos, Terence Todman, y el propio gobierno estadounidense tomó cartas en el asunto. Finalmente, a principios de 1991, renunció todo el gabinete menemista. Fue durante el mencionado escándalo que José Luis Manzano pronunció su difundida frase «"yo robo para la corona»".

El hijo del presidente, Carlos Menem Jr., «"Carlitos"», murió el 15 de marzo de 1995 junto al corredor de autos Silvio Oltra durante un viaje en helicóptero, a los 26 años de edad. Los peritos determinaron que el aparato cayó al embestir cables de alta tensión pero su madre, Zulema Yoma, insistió siempre en que su hijo había sido atacado por proyectiles y que el propio gobierno estaba ocultando las pruebas del hecho pues, según su versión, la muerte de su hijo fue planeada por el entorno del presidente.

Al principio el presidente no apoyó la teoría de su esposa y, poco después de la muerte de su hijo, Zulema se divorció de él, de quien ya estaba separada de hecho. Luego se presentó como querellante en la causa, abandonando la teoría del accidente.

La causa fue archivada el 16 de octubre de 1998 por el Juez Villafuerte Ruzo, al considerar que se trató de un accidente, pues la nave se estrelló luego de golpear con cables de alta tensión; pero ante el recurso de Zulema fue reconsiderada por la Corte Suprema, que en abril de 2001 decidió rechazar el recurso para reabrirla.

En 2010 la causa fue reabierta. El 8 de julio de 2014 el expresidente Carlos Menem realizó una presentación por escrito en la que manifestó: "Luego de indagar y estudiar los hechos y circunstancias que rodean la causa –aunque inicialmente no fue así-, llegué a la conclusión de que la caída del helicóptero, y la consecuente muerte de mi hijo, fue el resultado de un atentado".

Además de Zulema Yoma. varios sectores de la opinión pública también sospechan que no se trató de un accidente, como se insistió en un principio, sino de un posible ajuste de cuentas o venganza por acuerdos político-mafiosos no cumplidos. Se basan en que el desguace del helicóptero se hizo inmediatamente, sin posibilidad de un nuevo peritaje; en que se produjeron varias muertes por asesinato o causas poco claras de 14 personas relacionadas con la investigación (entre testigos, investigadores y peritos); y en la falta de medidas concretas por parte del gobierno para esclarecer el caso.

En mayo de 2001 contrajo matrimonio con la exmodelo (Miss Universo 1987) y conductora televisiva chilena Cecilia Bolocco, con la cual inició los trámites de divorcio en febrero de 2007. Con ella tiene un hijo: Máximo Menem Bolocco, quien nació en 2003.
El 13 de junio de 2020 el ex presidente Menem fue internado en el Instituto del Diagnóstico, donde suele efectuarse cada seis meses chequeos médicos, impulsado por su médico personal, cardiólogo y amigo Luis De la Fuente.

La noche del 21 de noviembre de 2001, el mismo día en que fue liberado del arresto domiciliario al que estuvo sometido por la causa de la venta ilegal de armas a Ecuador, Menem lanzó su candidatura presidencial en La Rioja, junto a su entonces esposa, Cecilia Bolocco.

En enero de 2003, los justicialistas esperaban dirimir en su interna qué candidato los representaría en las próximas elecciones generales. No obstante, el Congreso Nacional del Partido Justicialista, reunido el 24 de enero en el miniestadio de Lanús, anuló las internas partidarias y aprobó el sistema de "neolemas" mediante el cual autorizó a Carlos Menem, Adolfo Rodríguez Saá y Néstor Kirchner a participar directamente en la elección general convocada para el 27 de abril. Estas nuevas reglas de participación electoral fueron impulsadas por el presidente Eduardo Duhalde, quien impulsaba la candidatura de Kirchner, entonces gobernador de Santa Cruz. Menem, en cambio, quería que hubiera internas. En tanto, la jueza electoral María Servini de Cubría aceptó la apelación presentada por el duhaldismo a la resolución que prohíbe el uso de neolemas para dirimir la fórmula presidencial del Partido Justicialista. Menem no pudo revertir la decisión partidaria mediante la vía judicial.

La fragmentación electoral del peronismo dispersaba el voto dando más posibilidades a un balotaje. En las elecciones del 27 de abril iban a presentarse 19 candidatos, provenientes de un peronismo dividido en tres fracciones.

En las mismas, Menem, candidato de la alianza "Frente por la Lealtad - Ucede", se ubicó en el primer lugar, con 4.740.907 de votos (24,45%), y Kirchner, del "Frente para la Victoria", lo secundó, con 4.312.517 de votos (22,24%). Tercero se ubicó el exministro de Economía radical Ricardo López Murphy, encabezando un frente disidente, con 3.173.475 votos (16,37%); cuarto Adolfo Rodríguez Saa, con 2.735.829 votos (14,11%), y quinta Elisa Carrió. El candidato de la U.C.R., Leopoldo Moreau, quedó relegado a uno de los últimos lugares, con el 2,34% de los votos.

Como ninguno de los candidatos obtuvo la mayoría necesaria para imponerse en la primera ronda, se llegó a segunda vuelta. La reforma de 1994 de la Constitución argentina prescribe, en los artículos 94 y 96, que se debe realizar el balotaje si el triunfador obtiene menos del 45% de los votos y hay una diferencia menor de diez puntos porcentuales con el segundo candidato.

Se preparó entonces una nueva elección en segunda vuelta entre los dos candidatos más votados (el propio Menem y el gobernador Néstor Kirchner), para el 18 de mayo de 2003. Menem decidió renunciar a su candidatura al considerar que la ventaja en votos de su rival era irreversible; de este modo, terminó siendo víctima del mecanismo ideado por su propio equipo. Por aplicación del artículo 155 del Código Electoral Nacional argentino, Ley n° 19.945, que estipula que ""en caso de renuncia de los dos candidatos de cualquiera de las dos fórmulas más votadas en la primera vuelta, se proclamará electa a la otra"", al retirarse del balotaje la fórmula Menem-Romero, quedó proclamada la fórmula Kirchner-Scioli.

El 23 de octubre de 2005 se presentó a elecciones para Senador Nacional por su provincia y obtuvo la banca correspondiente a la minoría; los dos escaños por la mayoría fueron ganados por la fracción del presidente Néstor Kirchner, liderada a nivel local por el exgobernador Ángel Maza. De esta forma, Menem volvió a ocupar un cargo público exactamente seis años después de dejar la Presidencia.

En mayo de 2007, Menem se presentó como opositor al presidente Néstor Kirchner, al que ha denostado públicamente en reiteradas ocasiones. Según su opinión, Kirchner no aplica una política imparcial con respecto al juzgamiento de militares y civiles acusados de haber cometido actos de violación de los derechos humanos en las décadas del 70 y 80, ya que sólo pretende juzgar a los cabecillas de la última dictadura militar, pero no a los militantes de organizaciones guerrilleras.

Se presentó como candidato a gobernador de La Rioja el 19 de agosto de 2007, pero fue derrotado: obtuvo cerca del 22% de los votos, y quedó tercero detrás de Luis Beder Herrera y de Ricardo Quintela. Antes de esta última derrota, Menem se presentaba públicamente como candidato a la presidencia para las elecciones de octubre de 2007. Finalmente, desistió de postularse.
En el año 2008 la empresa Siemens AG manifestó que entre 1998 y 2004 había pagado sobornos a distintos funcionarios, entre los cuales estaba Menem, a fin de obtener un contrato multimillonario con el Estado para fabricar los documentos nacionales de identidad, lo cual fue negado por el imputado.

El 17 de julio de 2008, la figura del Senador Menem fue de gran importancia en la sesión sobre la Resolución 125, relativa a la imposición de retenciones móviles a la exportación de productos agropecuarios, en la cual se dirimió el conflicto que enfrentaba al Gobierno Nacional con el sector agrícola-ganadero. Durante el debate, que se extendió por dieciocho horas, el expresidente estuvo en gran parte ausente a causa de una internación de urgencia por una fuerte neumonía. Esto dio lugar a numerosas especulaciones, ya que con su ausencia votarían 71 de los 72 senadores, permitiéndole al oficialismo vencer por 36 votos contra 35 de la oposición. Finalmente, pasada la medianoche, el Senador se hizo presente en el recinto, pronunció un fuerte discurso en contra de las retenciones móviles, anunció públicamente su proyecto ingresado por mesa de entradas y anticipó que votaría en contra del proyecto oficialista. Esto fue calificado como un acto de coraje cívico por el exsecretario General de la Presidencia, Alberto Kohan. Finalmente, la sesión terminó empatada con 36 votos a favor e idéntico número en contra, y debió desempatar el entonces Vicepresidente, Julio César Cleto Cobos.

Su voto en el proyecto de ley con respecto a la interrupción voluntaria del embarazo del 8 de agosto de 2018 fue negativo.

En diciembre de 2019 se unió a la coalición Frente de Todos.

Menem incursionó en el automovilismo deportivo. Competía asiduamente en "rally" en los años 1980 con un Peugeot 504 primero y luego con un Renault 18, que posteriormente le cedió a su amigo Juan María Traverso.

Después de asumir el cargo como presidente y durante el ejercicio de su mandato, Menem dedicó especial atención a los medios de difusión, con lo cual aumentó su protagonismo público y su perfil mediático.Según algunos medios periodísticos,en oportunidad de brindar una conferencia de prensa, Menem habría afirmado que entre sus lecturas se hallaba una obra escrita por Sócrates. El error de Menem habría consistido en que se desconoce que exista algo escrito por dicho filósofo.

En otra ocasión, al dar comienzo al ciclo lectivo de 1996 en Tartagal (Salta), informó a los niños de una escuela rural que se licitaría un sistema de vuelos estratosféricos «desde una plataforma que quizás se instale en la provincia de Córdoba. Esas naves espaciales van a salir de la atmósfera, van a remontar a la estratósfera y desde ahí elegir el lugar donde quieran ir, de tal forma que en una hora y media podamos, desde Argentina, estar en Japón, en Corea o en cualquier parte». El presidente mencionó en su discurso que había recibido a los empresarios de la compañía de aviación estadounidense Lockheed-Martin. El proyecto en realidad consistía en la restructuración del sistema de aviación militar del país y la construcción de vehículos para viajar al espacio exterior; tanto la NASA como dicha empresa deseaban instalar una base en América del Sur, aunque Argentina tenía dos competidores: Boeing y Northrop Grumman. Con este contrato se privatizó la flota de aviones militares bajo la subsidiaria Lockheed Aircraft Argentina. Menem recibió críticas por ese discurso; el gobernador de Salta, Juan Carlos Romero, afirmó que en su provincia había «por lo menos 50 000 analfabetos». En las siguientes décadas, los medios de comunicación nacionales ridiculizaron las declaraciones de Menem, y con el surgimiento de las redes sociales se han publicado numerosos memes que evocan el incidente.

Otro proyecto impulsado por Menem y duramente criticado por la prensa fue el de construir una «aeroísla», una isla artificial en el Río de la Plata a la cual sería trasladado el Aeroparque Jorge Newbery. El proyecto, apoyado por Álvaro Alsogaray, nunca prosperó.

Indultó a responsables del terrorismo de Estado en Argentina en las décadas de 1970 y 1980, y a integrantes de grupos guerrilleros como el ERP y Montoneros; condecoró a Augusto Pinochet e intentó implantar la pena de muerte. Cuando en 1993 renunció su ministro del Interior, Gustavo Béliz, este lo acusó de elegir en lugar de "funcionarios honestos" a "alcahuetes y mediocres".

En la actualidad encabeza el partido Lealtad y Dignidad, junto a Mercedes Landa.

El 7 de junio de 2001 fue detenido por el escándalo por venta de armas a Ecuador, Croacia y Bosnia sucedida durante su gobierno y quedó bajo arresto domiciliario hasta el 21 de noviembre del mismo año, cuando la Corte Suprema emitió un fallo absolutorio en su favor. Se le acusaba de haber dado las órdenes para desviar grandes cargamentos de armas que, según los decretos por él firmados, debían ir a Panamá y Venezuela, pero terminaron en Ecuador y Croacia. Llevado a juicio oral fue absuelto en 2011 por el Tribunal Oral en lo Penal Económico 3 (TOPE 3). En 2013 la Cámara Federal de Casación Penal revocó la absolución y lo condenó como coautor del contrabando de armas a Croacia y Ecuador; poco después el TOPE 3 estableció la pena en 7 años de prisión.

En 2017, la Corte Suprema por unanimidad (Lorenzetti, Highton de Nolasco, Maqueda, Rosatti y Rosenkrantz) hizo lugar a los recursos extraordinarios de los acusados y ordenó dictar un nuevo fallo en el menor tiempo posible, debido a que no se había cumplido la garantía del “doble conforme” (dos fallos condenatorios). La causa fue enviada a la Sala I de la Cámara Federal de Casación Penal (Liliana Catucci, Carlos Mahiques y Eduardo Riggi) que el 4 de octubre de 2018 decidió absolver a Menem y los demás acusados debido a que no se había cumplido el "principio del plazo razonable" para arribar a una condena firme.

En cuanto a la causa de la voladura de la fábrica de Río Tercero, ocurrida a poco de comenzar su segunda presidencia, la Cámara Federal de Córdoba anuló su indagatoria en la investigación sobre las causas de ese suceso en noviembre de 2008, a 13 años de ocurrido. La razón por la cual se lo desvinculó de la causa es que el entonces fiscal Carlos Stornelli nunca solicitó la indagatoria del exmandatario.

El 1 de diciembre de 2015, el Tribunal Oral Federal 4 condenó a Menem a 4 años y seis meses de prisión por el delito de peculado e inhabilitación perpetua para ejercer cargos públicos.

Su ministro de economía Domingo Cavallo también fue hallado culpable, condenado a 3 años y 6 meses de prisión, inhabilitación perpetua para ejercer cargos públicos, y a un decomiso de 220 868 pesos.

El tribunal también encontró culpable al exministro de Justicia Raúl Granillo Ocampo, por el cobro de sobresueldos, le impuso una pena de tres años y tres meses de cárcel y el decomiso de pesos.

El 28 de febrero de 2019 el Tribunal Oral en lo Criminal Federal 2 compuesto por los jueces Jorge Gorini, Karina Perilli y Néstor Guillermo Costabel declararon su absolución respecto a la acusación de haber encubierto el atentado a la AMIA ocurrido el 18 de julio de 1994.

En marzo de 2019, Menem fue condenado en causa por la venta del Predio Ferial de Palermo a la Sociedad Rural. Los jueces determinaron que esa venta le provocó un perjuicio al estado argentino por aproximadamente cien millones de dólares.




</doc>
<doc id="8915" url="https://es.wikipedia.org/wiki?curid=8915" title="Copa Mundial de Fútbol">
Copa Mundial de Fútbol

La Copa Mundial de la FIFA, también conocida como Copa Mundial de Fútbol, Copa del Mundo o simplemente Mundial, cuyo nombre original fue Campeonato Mundial de Fútbol, es el principal oficial de fútbol masculino a nivel de selecciones nacionales en el mundo.

Este evento deportivo se realiza cada cuatro años desde 1930, con la excepción de 1942 y 1946, en los que se suspendió respectivamente debido al desarrollo y las consecuencias de la Segunda Guerra Mundial. Cuenta con dos etapas principales: un proceso clasificatorio en el que participan en la actualidad cerca de 200 selecciones nacionales y una fase final realizada cada cuatro años en una sede definida con anticipación en la que participan 32 equipos (48 a partir de la edición de 2026) durante un periodo cercano a un mes. El es fabricado por la compañía alemana de equipamiento deportivo Adidas.

La fase final del torneo es el evento deportivo de una sola disciplina más importante del mundo (la final de la Copa Mundial de Fútbol de 2002 fue vista por más de 1100 millones de personas), y el segundo más importante a nivel general después de los Juegos Olímpicos.

Ha sido realizada en 21 ocasiones, en las que ocho de los nueve países futbolizados —corresponden a las — han alzado la copa: es el equipo más exitoso, con cinco victorias; e le siguen con cuatro trofeos; , y la han ganado dos veces, en tanto y se han titulado campeones una vez. El torneo presenta un dominio de los equipos europeos, que han ganado el título en doce ocasiones, mientras que los sudamericanos lo han hecho en nueve. Solo dos equipos de otras confederaciones han llegado a semifinales: en 1930 y en 2002.

El primer encuentro internacional de este deporte se remonta al partido disputado entre y el 30 de noviembre de 1872. El fútbol en ese tiempo era prácticamente desconocido fuera de las islas Británicas, pero lentamente comenzó a desarrollarse en otras partes del mundo. El fútbol debutó como un deporte de demostración en los Juegos Olímpicos de París 1900, experiencia repetida en Saint Louis 1904 y los Juegos Intercalados de 1906 en Atenas. 

El 21 de mayo de 1904 se fundó la Federación Internacional de Asociaciones de Fútbol (FIFA por sus siglas en francés) con el fin de organizar el desarrollo del deporte. Dentro de sus ideas originales surgió la posibilidad de realizar en 1906 un torneo internacional en Suiza, pero finalmente la propuesta fracasó. Sin embargo, la idea se mantuvo y se concretó cuando en Londres se organizaron los IV Juegos Olímpicos en 1908 y se declaró al fútbol como deporte olímpico oficial. Estando a cargo de la organización de la Football Association (no afiliada a la FIFA aún, pero con quien mantenía una estrecha relación), el primer lo ganó el , seguido por y los . 

Con el paso de los años el torneo olímpico de fútbol se mantuvo, pero como un evento amateur. En 1909 Sir Thomas Lipton organizó un torneo profesional entre clubes que representaba a cada país en la ciudad de Turín. Este torneo se denomina a veces como la «primera Copa Mundial». En 1914 la FIFA reconoció el torneo olímpico como un «campeonato mundial de fútbol para amateurs» y decidió hacerse responsable del desarrollo de dicho evento. Tras la Primera Guerra Mundial se realizó el primer torneo intercontinental en los 1920, donde participaron 13 equipos europeos junto al seleccionado de .

En los , el primero organizado por la FIFA, se integraron los equipos sudamericanos. En dicho evento, se coronó campeón, revalidando su título cuatro años más tarde, en Ámsterdam 1928.

Durante los Juegos Olímpicos de 1928, la FIFA organizó un congreso donde se decidió finalmente la realización de un torneo de fútbol profesional de nivel internacional en 1930. Inmediatamente varios países europeos presentaron su candidatura (Italia, Hungría, los Países Bajos, España y Suecia) junto a la de Uruguay. Jules Rimet, presidente de la FIFA en esos años, estaba a favor de la realización en el país sudamericano, tanto por sus éxitos deportivos como porque el país celebraría el centenario de la Jura de la Constitución.

Finalmente, Uruguay salió electo por unanimidad, pero eso no implicó el apoyo europeo a la realización del torneo fuera de su continente. Los países europeos invitados al torneo rechazaron su participación argumentando que no podían costear el largo viaje transatlántico en medio de la crisis económica que había azotado al mundo en esos años. A pesar de que Uruguay se ofreció a solventar los costos, solo , , y acudieron a la cita. Tras el boicot, los organizadores debieron disminuir el número de participantes en el torneo, de 16 a 13.

A pesar de las complicaciones iniciales, el torneo fue un éxito. Para el torneo, la intención de los organizadores era que todos los partidos se disputaran en un solo estadio, el Estadio Centenario, construido especialmente para la celebración de la Copa Mundial y como celebración del centenario de la independencia uruguaya. Fue diseñado por Juan Scasso y Rimet lo llamó el "templo del fútbol". Con una capacidad para 90 000 espectadores, era el mayor estadio del mundo fuera de las Islas Británicas. Sin embargo, las fuertes lluvias acaecidas en Montevideo antes de la inauguración del campeonato impidieron que su construcción fuera finalizada a tiempo. Dada esta situación los organizadores se vieron obligados a buscar otros estadios para celebrar en ellos los primeros partidos, el Gran Parque Central y el Estadio Pocitos, escenarios donde se jugaron de manera simultánea los dos primeros partidos en la historia de la Copa Mundial. El Estadio Centenario fue oficialmente inaugurado el sexto día de competición y a partir de ese momento todos los partidos se jugaron ahí. Finalmente, los equipos del Río de la Plata avanzaron a la final, y se enfrentaron el 30 de julio de 1930 en el recién inaugurado Estadio Centenario. Tras ganar el encuentro por 4:2, los locales se coronaron como los primeros campeones mundiales de fútbol ante 93 000 personas.

Italia organizó la segunda Copa Mundial en 1934. Como respuesta al boicot realizado en 1930 por los países europeos, Uruguay y otros países americanos se retiraron del torneo. La Copa Mundial se había convertido en muy poco tiempo en un gran acontecimiento que recibía las miradas de todo el mundo, por lo que el caudillo fascista Benito Mussolini usó el torneo para la exaltación del nacionalismo, buscando publicitar el poder italiano con una victoria en la competición. Para ello no dudó en asegurar la naturalización de varios jugadores argentinos, como Luis Monti, Raimundo Orsi, Enrique Guaita y Attilio Demaría, y también del brasileño Anfhiloquio Marqués Filo, italianizado como Anfilogino Guarisi. Italia llegó a la final del torneo donde se enfrentó a . Tras una serie de errores arbitrales, Angelo Schiavio anotó el gol del triunfo italiano durante la prórroga, que coronó a Italia como campeona del mundo. Varios jugadores de aquel equipo reconocieron haber jugado la final bajo amenazas del "Duce". El naturalizado Monti declaró:

En los años posteriores el advenimiento de la Segunda Guerra Mundial se hacía cada vez más presente. La Copa Mundial de Fútbol de 1938 realizada en Francia contó con las deserciones de , debido a la Guerra Civil, y , debido al estallido de la Segunda Guerra Sino-japonesa, mientras la clasificada no participó en el torneo al ser incorporada a Alemania tras el "Anschluss". Ya en el torneo propiamente, fue repudiada por el público mientras los jugadores realizaban el saludo nazi. Además, los equipos americanos (a excepción de y ) nuevamente boicotearon el torneo, luego de que fuera otorgada la sede a un país europeo a pesar del compromiso inicial de alternar la sede entre ambos continentes.

En el ámbito deportivo, Italia mostró su capacidad ofensiva llegando a la final del torneo tras derrotar a Brasil de Leônidas, una de las figuras del torneo. Los italianos se enfrentaron a la potente y la derrotaron con 4:2, convirtiéndose en el primer equipo en alcanzar el bicampeonato. Mussolini, al igual que en 1934, no estaba dispuesto a ver perder a su equipo. El seleccionador italiano, Vittorio Pozzo, recibió un telegrama antes del partido final en el que solo podía leerse «Vencer o morir». Además, obligó a sus jugadores a vestir para la final camisetas negras, símbolo del fascismo italiano.

Para el torneo de 1942, Argentina, Brasil y la Alemania nazi presentaron sus candidaturas, pero tras el inicio de la Segunda Guerra Mundial la FIFA decidió la suspensión de todos los eventos mientras el conflicto perdurase, provocando la cancelación de los torneos de y . En ese último año, la FIFA decidió que la Copa Mundial fuera reanudada tan pronto como fuera posible. Como la mayoría de los países europeos estaban devastados por la guerra, ninguno tenía la capacidad para organizar el torneo, por lo que Brasil presentó su candidatura y salió electo por la FIFA para realizar la Copa Mundial de Fútbol de 1950. 
Diversos países se retiraron del torneo, incluida la (por pretender jugar con futbolistas descalzos) y , el múltiple campeón de Sudamérica durante la década de 1940 por decisión interna, reduciendo el número de participantes de 16 a 13. Sin embargo, el evento marcó el ingreso por primera vez de los diversos equipos del Reino Unido a los procesos clasificatorios. Así, participó por primera vez en la Copa Mundial, sin embargo quedó eliminada rápidamente a pesar de su condición de favorita. Tras la primera ronda, , , y se clasificaron a un grupo final de donde saldría el campeón del torneo. El seleccionado brasileño derrotó por sendas goleadas a los equipos europeos, por lo que su victoria parecía asegurada. En el último partido, Brasil se enfrentó a Uruguay, que había tenido una irregular actuación, con una victoria sobre Suecia y un empate ante España. Por lo tanto, aunque no se tratase de una final, el campeón saldría de ese último partido, en el que a Brasil le bastaba un empate. Todo estaba listo en el Estadio Maracaná para las celebraciones del triunfo brasileño ante cerca de 175.000 espectadores, los diarios locales ya habían anunciado el partido como el de la primera victoria mundial de Brasil. Empero, los uruguayos lograron derrotar a los brasileños y coronarse campeones, después de remontar un 1:0 inicial, para acabar con un 1:2. El llamado "Maracanazo" es considerado como una de las más grandes sorpresas en la historia del deporte. En el otro partido, Suecia venció a España con 3:1 obteniendo el tercer puesto y dejando a los ibéricos en cuarta posición.
En 1954, la Copa Mundial regresó a Europa cuando Suiza, país neutral durante la guerra, fue la sede de la V Copa Mundial. Durante el desarrollo del torneo se produjeron tres de los partidos más recordados en la historia de la competición. En los cuartos de final, el "Equipo de oro", nombre con el que se conocía al equipo de , se enfrentó a la selección brasileña, que después del "Maracanazo" decidió cambiar el color blanco de su camiseta por el actual amarillo con ribetes verdes. El partido, que enfrentó a dos de las mejores escuadras del torneo, se convirtió en uno de los encuentros más infames de la historia: la excesiva violencia hizo que fuera conocido tradicionalmente como la "Batalla de Berna", en la que participaron tanto jugadores como entrenadores. En la misma ronda, derrotó a por 7:5, en el encuentro con mayor número de goles anotados en la historia. La final se disputó el 4 de julio de 1954 en el Wankdorfstadion, entre los húngaros, que vencieron en el alargue a en lo que fue la primera derrota uruguaya en los mundiales, y la , equipo que regresaba al torneo después de la prohibición establecida tras la derrota germana en la Segunda Guerra Mundial. Ambos equipos se habían enfrentado en la primera ronda y los magiares habían goleado 8:3 a sus rivales, por lo que una victoria de la Alemania Occidental parecía imposible. Sin embargo, pese a que a los 8 minutos de haber comenzado el encuentro los húngaros empezarían ganando por 0:2, los alemanes alcanzaron la victoria remontándolo con un 3:2 derrotando al combinado liderado por Ferenc Puskás, y alzaron por primera vez el trofeo Jules Rimet. El encuentro conocido como el "Milagro de Berna" se considera como uno de los hechos que marcaron el fin del período de posguerra de Alemania y su renacer, como también uno de las mayores sorpresas en la historia de la competición. Prueba de ello es la película sobre el partido, titulada "El milagro alemán".

Suecia fue el país destinado a realizar la Copa Mundial de Fútbol de 1958. El torneo fue el primero en ser transmitido a través de la televisión, dando así inicio a la expansión del torneo hacia otros continentes. En el ámbito deportivo, alcanzaría el tercer lugar del torneo tras ser derrotados en semifinales por , y el francés Just Fontaine se convertiría el máximo goleador (13 goles) liderando hasta la fecha la clasificación de máximos goleadores en una sola edición de las copas del mundo. Los sudamericanos se enfrentarían en la final al , en el Estadio Råsunda de Estocolmo. Pelé era la gran promesa brasileña, pero eran pocos los que lo conocían. Durante una serie de partidos de preparación frente a clubes italianos previos al inicio del mundial, Pelé sufrió una lesión de rodilla. Estuvo cerca de abandonar la delegación brasileña, pero finalmente acudió a Suecia, donde no pudo debutar hasta el partido de cuartos de final frente a . Suyo fue el único gol del partido, y en semifinales frente a Francia anotó un total de tres. En la final, con un marcador de 5:2, Brasil se coronó campeón del mundo por primera vez en la historia. Aunque los suecos se pondrían en ventaja temprana, la aparición de Vavá y Pelé, con dos goles cada uno, revertiría la situación.

Brasil nuevamente brillaría en el torneo siguiente, realizado en Chile a mediados de 1962. Pelé, ya convertido en uno de los mejores jugadores del momento, no pudo participar debido a una lesión a comienzos del evento, pero la magia de Garrincha llevaría al equipo brasileño a levantar por segunda vez la Copa al derrotar en la final a , frente a más de 60.000 personas instaladas en el Estadio Nacional de Santiago de Chile. Cabe resaltar que en ese torneo, el colombiano Marcos Coll marcó en la portería del legendario arquero Lev Yashin el empate 4:4 de su selección contra la con un gol olímpico, el único marcado en la historia de los mundiales. Mientras que los , después de derrotar a los en el infame partido de fase de grupos conocido como la "Batalla de Santiago", lograron llegar hasta el tercer puesto al derrotar por un gol a la .
En 1966 la Copa sería realizada en Inglaterra, cuna del fútbol. La selección de Brasil quedaría eliminada en la primera ronda después de ser derrotada en violentos partidos por y , este último llegó a semifinales liderado por Eusébio. Uruguay y Argentina tampoco llegaron lejos, luego de quedar eliminados en cuartos de final tras arbitrajes polémicos. , campeona en esos momentos de la Copa de Europa, se vio apeada en la primera ronda de clasificación tras perder contra Alemania y Argentina.

Desde su debut en 1950, no había podido tener una buena actuación, por lo que esta era su oportunidad de demostrar su paternidad. Los locales se enfrentaron a Alemania Federal ante un Estadio de Wembley repleto apoyando a su selección. Tras empatar en el tiempo regular se realizó una prórroga. En el minuto 101, Geoff Hurst disparó contra la portería germana y el balón sería despejado por el guardameta. El tiro sería considerado gol por el árbitro, desatando una polémica que persiste hasta el día de hoy sobre si el balón cruzó completamente la línea de gol. Cuando quedaban segundos para que el partido finalizara y todo el equipo germano intentaba descontar, Bobby Moore atrapó un balón que conectó con Hurst, quien realizó un disparo lejano, anotando el 4:2 final, desatando la alegría en las graderías. Minutos después, Moore recibiría la Copa Jules Rimet de las manos de la reina Isabel II.

A pesar del fracaso de 1966, la escuadra brasileña llegó a México dispuesta a ganar el Mundial de 1970. La "verdeamarela" se enfrentó en primera ronda a los campeones defensores, Inglaterra. Brasil, que incluía en sus filas no solo a Pelé, sino a otros grandes jugadores como Jairzinho, Tostão, Rivelino y Carlos Alberto, derrotó por la cuenta mínima a los ingleses en uno de los encuentros más memorables del torneo.

Brasil avanzó invicto hasta las finales, donde se enfrentaría al ganador del partido entre y . Los italianos habían goleado a la , mientras los germanos se clasificaron tras derrotar a los ingleses en tiempo extra, reeditando la final del torneo previo. El encuentro de semifinal partió con un temprano gol de Roberto Boninsegna. Cuando se jugaban los descuentos, Karl-Heinz Schnellinger anotó y forzó la prórroga, en que cada equipo anotó dos goles más. Alemania, exhausta tras el partido ante Inglaterra y con Franz Beckenbauer lesionado, no pudo aguantar la presión y fue derrotada por 4:3. Una placa instalada posteriormente en el Estadio Azteca, conmemora hasta el día de hoy el llamado "Partido del Siglo", considerado por muchos como el mejor de la historia.

Brasil e Italia se enfrentaron el 21 de junio de 1970 en Ciudad de México para definir cuál de los dos equipos se adjudicaría para siempre el Trofeo Jules Rimet, premio que sería entregado al primer equipo en ganar tres veces el torneo. Durante el primer tiempo ambos equipos estuvieron igualados a un gol, pero la artillería brasileña estallaría en el segundo tiempo, en el que los italianos pagaron el esfuerzo realizado frente a Alemania, anotando tres goles más. Brasil derrotó por 4:1 a Italia, coronándose como tricampeón con una de las escuadras más valoradas en la historia del fútbol. En el partido por el tercer lugar, Alemania Federal derrota a Uruguay 1:0.

Durante la década de los años 60 comenzaron a ser lanzados los primeros sistemas de satélites. En México 1970, y gracias al sistema de Telstar, se transmitieron por primera vez imágenes en color del evento para el resto del planeta. Debido a esto, el evento comenzó a popularizarse con rapidez en el resto del mundo. Prueba de ello es la cantidad de países inscritos para el proceso clasificatorio: en 1962 se inscribieron 56 países y en 1970 fueron 75. Cuando el Mundial regresó a Europa para la Copa Mundial de 1974 organizada por Alemania Occidental había 99 participantes, principalmente de las recién independizadas naciones africanas.

Rápidamente el evento comenzó a convertirse en uno de los principales eventos deportivos, alcanzando la popularidad de los mismísimos Juegos Olímpicos. La Copa Mundial comenzó a volverse en un rentable negocio, que se iniciaría con la primera mascota del torneo: el león "Willie", que representó al mundial realizado por Inglaterra. La empresa deportiva Adidas se convertiría en auspiciante oficial del evento desde 1970 y sería el proveedor oficial de los balones, modernizando notablemente el tradicional deporte.

Después de haber sido derrotado en la final de 1966 y en semifinales de 1970, el comandado por Franz Beckenbauer confiaba en que finalmente lograrían levantar la Copa en su propio país. A pesar de iniciar el torneo de 1974 con una derrota frente a sus rivales de la , los germanos llegaron hasta la final del torneo, realizada en el Estadio Olímpico de Múnich. Su rival en la final fue la selección de los , llamada la "Naranja Mecánica" por el color naranja de la casaca y su facilidad por crear fútbol técnico muy ofensivo y vencer a sus rivales. En la segunda fase se disputaron dos liguillas de cuatro equipos. Alemania se impuso en su grupo, venciendo a la sorprendente , y los Países Bajos quedaron primeros, por delante de Brasil y Argentina. Polonia venció a Brasil en la lucha por el tercer puesto, logrando así su mejor resultado hasta la fecha.

En la final, el Fútbol Total de Johan Cruyff parecía superar a la disciplina de los locales cuando se pusieron en ventaja con el partido recién comenzado. Cruyff forzó un penalti y Johan Neeskens lo convirtió, cuando Alemania todavía no había podido ni siquiera tocar el balón. Pero la marca de estos últimos a la estrella neerlandesa y los goles de Paul Breitner y Gerd Müller finalmente le darían la victoria por 2:1 a Alemania, que sería el primer equipo en levantar el nuevo trofeo del torneo. La hegemonía del fútbol europeo estaba discutida entre Cruyff y el líder alemán, Franz Beckenbauer, ganadores de los últimos Balones de Oro. En referencia a esto, el germano declaró: «Cruyff era mejor jugador que yo, pero yo gané el Mundial».

Tras más de 48 años de espera, finalmente Argentina fue seleccionada para ser sede de la Copa Mundial de 1978. Sin embargo, la organización del torneo se vería afectada por el rechazo internacional a la dictadura militar que se había instalado en el país en 1976 y a las violaciones a los derechos humanos cometidas durante ese período. A pesar de las protestas iniciales ningún país se retiró de la competición, pero los neerlandeses sufrieron la deserción de Cruyff por dichos motivos. Esto no pesaría en el rendimiento de la "Naranja Mecánica", que nuevamente sería finalista después de sobrepasar a Italia y Alemania en la fase grupal de la segunda ronda. Su rival sería la , que clasificaría tras derrotar por 6:0 en un polémico partido a . En la final, disputada en el Estadio Monumental de Buenos Aires, Mario Kempes sería la figura de la victoria sudamericana por 3:1. 
Debido al éxito del torneo, el número de equipos participantes aumentó de 16 a 24 desde la Copa Mundial de 1982 disputada en España, para así darle más oportunidades de participación a equipos de Norteamérica, África, Asia y Oceanía. A diferencia del mundial anterior en que solo participaron en total 3 países de estos continentes, en España participó el doble. A pesar de ello, los nuevos participantes no lograron éxito pues ninguno de ellos clasificó a la segunda ronda, aunque se deben destacar las participaciones de , que quedó eliminado por diferencia de goles al igualar en puntos con Italia, y . La eliminación de este último país generó una fuerte controversia luego de que Alemania derrotara por 1:0 a Austria, cifra necesaria para que ambos países germanohablantes clasificaran en desmedro de los norteafricanos. 

, que contaba con jugadores como Zico, Falcão y Sócrates, fue la sensación de la primera ronda al ganar con facilidad sus tres partidos, mientras se perfilaba como uno de los favoritos junto a su estrella, Michel Platini. Sin embargo, estos dos equipos serían eliminados respectivamente por los eventuales finalistas del torneo: y . Italia clasificaría a semifinales luego de que los tres tantos de Paolo Rossi les dieran la victoria sobre los sudamericanos durante la segunda ronda. En tanto, la dramática semifinal entre franceses y alemanes se definiría tras la primera tanda de penaltis realizada en un Mundial. Luego de que los alemanes remontaran un 3:1 en la prórroga, alcanzarían el pase a la final al ganar por 5:4 desde los once pasos. En la final, los itálicos se impusieron fácilmente alcanzando el tricampeonato; Rossi, la figura del equipo campeón, se quedaría con los dos premios creados ese año: el botín de oro al goleador del torneo y el balón de oro, entregado al mejor jugador.

España, como anfitriona, tuvo una participación modesta: en la primera ronda tras vencer a Yugoslavia y empatar con Honduras. Sin embargo, la floja primera fase le costó su encuadramiento en el grupo de Alemania e Inglaterra en la segunda fase. La eliminación de España supuso la destitución del seleccionador José Santamaría.

Colombia había sido elegida para ser la sede de la XIII Copa Mundial a realizarse en 1986, sin embargo, el país organizador desistió luego de verse imposibilitado de cumplir las fuertes exigencias impuestas por Hermann Neuberger, vicepresidente de la FIFA. Ante la renuncia colombiana, el organismo internacional decidió que México acogiera nuevamente el torneo, debido a que mantenía en gran parte la infraestructura dejada por el torneo de 1970.

La primera ronda del torneo se realizó con normalidad, destacando a como el primer equipo africano que pasó a la segunda ronda. En la segunda ronda, sin embargo, comenzaron a destacarse los equipos favoritos: , que había derrotado a los campeones defensores en octavos de final, enfrentó en un dramático partido a , el cual finalizó con la victoria gala en la ronda de penaltis. Sin embargo, los sueños de Platini se verían nuevamente truncados en semifinales por .

En la otra llave del torneo, avanzaba imparable, en gran parte debido al talento de Diego Armando Maradona. En cuartos de final el equipo albiceleste debía confrontar a , uno de sus más tradicionales rivales, especialmente tras el estallido de la Guerra de las Malvinas cuatro años antes. El enfrentamiento destacó por dos de los goles más recordados en la historia de este deporte: en el minuto 51 Maradona anotó un gol con su mano (conocido como "la mano de Dios") y en el 54 el mismo Maradona recorrió 62 metros en 10 segundos, sobrepasando a 6 ingleses, antes de anotar el denominado "Gol del Siglo", considerado el mejor gol en la historia del fútbol.

La final sería disputada entre alemanes y argentinos en el Estadio Azteca ante más de 110.000 espectadores. Cuando faltaban menos de quince minutos para el final del partido los sudamericanos lideraban por 2:0, pero los dirigidos de Franz Beckenbauer lograron igualar el marcador agregando dramatismo. Sin embargo un gol de Jorge Burruchaga en el minuto 84 definiría la victoria argentina. Maradona, elegido el mejor jugador del torneo, sería el encargado de levantar el segundo título mundial de su país.

La revancha de Alemania se concretaría cuatro años después, cuando fuera Italia la sede de la Copa Mundial de 1990. En este torneo, se convirtió en una de las sorpresas al derrotar en el partido inaugural a la escuadra argentina y avanzar finalmente hasta los cuartos de final, siendo eliminados por Inglaterra en la prórroga. A pesar de ello, este mundial ha sido considerado como uno de los de más baja calidad, debido a un fútbol extremadamente defensivo, lo que se vio reflejado en la baja cifra de goles (la más baja de la historia) y el gran número de partidos definidos en penaltis, entre los que se encontraron las dos semifinales. El torneo finalizaría con una mediocre final entre alemanes y argentinos, caracterizada por los errores arbitrales y la expulsión de dos jugadores de la "Albiceleste". Un solitario gol de penal de Andreas Brehme cinco minutos antes del pitazo final le daría la Copa por tercera vez a la escuadra de Alemania Occidental, algunos meses antes de que se concretara el proceso de reunificación de dicho país.

Con el fin de promover el fútbol en Estados Unidos, la principal potencia mundial tras el fin de la Guerra Fría, la FIFA decidió que la Copa Mundial de 1994 fuera disputada en dicho país, generando amplias críticas debido a la realización del torneo en un lugar donde el fútbol era prácticamente desconocido y donde ni siquiera existía una liga profesional. Esto no impidió que el Mundial fuera un éxito, alcanzando cerca de 3,6 millones de espectadores, una marca imbatida hasta el día de hoy.

El torneo se vio manchado con el asesinato, una vez finalizada la participación del equipo de Colombia, de su defensa Andrés Escobar luego que este accidentalmente cometiera un autogol. También significó el fin de la brillante carrera internacional de Maradona, después que diera positivo su test de dopaje. En el ámbito deportivo, Romário fue el artífice de la impecable campaña de Brasil hasta la final del torneo, en la que se enfrentó a que había llegado a dichas instancias a pesar de haber disputado sufridos encuentros. Los dos tricampeones se enfrentaron en el Rose Bowl, pero ninguno fue capaz de convertir durante el tiempo reglamentario. El campeonato se definiría por primera vez en una tanda de penales. Después que Roberto Baggio fallara en su disparo, Brasil conquistó su tetracampeonato cuando había estado sin levantar la copa durante 24 años.

La nueva generación brasileña comenzó nuevamente a reinar y era la gran favorita para alcanzar el pentacampeonato en Francia 1998, el primer torneo que contó con 32 equipos participantes. A pesar de la ausencia de Romário, Brasil contó con jugadores como Ronaldo y Rivaldo que llevaron a los brasileños a su segunda final consecutiva. En dicho encuentro se enfrentó a la , que había llegado a dicha instancia justo después de derrotar en semifinales a la sorprendente selección de , que en su primera participación en un Mundial había alcanzado el tercer lugar. Aunque los galos habían tenido una irregular campaña durante la segunda ronda, en el encuentro decisivo fueron superiores y el buen juego de Brasil prácticamente se desvaneció. Zinedine Zidane se convirtió en la estrella del partido al anotar dos de los tres goles de "Les Bleus", los cuales les darían el primer título a su país.

Cuatro años más tarde el torneo se disputó por primera vez en tierras asiáticas, cuando Corea del Sur y Japón realizaron conjuntamente el Mundial de 2002. El evento generó una enorme inversión en ambos países, especialmente en cuanto a infraestructura: 18 nuevos estadios fueron construidos en total, con un costo que superó los 4500 millones de dólares y se instaló tecnología de última generación para acoger a las 32 selecciones clasificadas de un total de 199 equipos inscritos, marcando un nuevo hito.

A pesar de haber sufrido en el proceso clasificatorio, nuevamente demostró su poderío, al ganar todos sus partidos durante el torneo. Ronaldo, que había sido opacado en la final de 1998 por Zidane, anotó ocho goles y se convirtió en el jugador con más tantos anotados desde 1970. En la final disputada en Yokohama, los brasileños no tuvieron problemas en superar a . El guardameta alemán Oliver Kahn, que había sido uno de los principales artífices de la campaña de su combinado, recibiendo solo un gol en todo el torneo, no pudo detener dos disparos de Ronaldo que permitieron a Brasil coronarse pentacampeón.

El torneo de 2002 mostró una serie de resultados sorpresivos, entre los que destacaron las eliminaciones en primera ronda de algunos de los equipos favoritos para ganar el torneo, como , y , que se convirtió en el peor campeón defensor de la historia del evento. Otros equipos alcanzaron resultados destacables: se convirtió en el primer equipo asiático en llegar a semifinales junto a la sorprendente , mientras y la debutante accedieron a la ronda de los ocho mejores. Sin embargo, los errores arbitrales marcaron un punto negro en el desarrollo del torneo, hecho que fue reconocido incluso por el propio presidente de la FIFA, Joseph Blatter.
Blatter, que había ascendido a la presidencia de la FIFA con la promesa de llevar el torneo por primera vez a África, sufrió un fuerte revés cuando, por un voto de diferencia, Alemania derrotó a Sudáfrica en la elección de la sede de la Copa Mundial de 2006.

Brasil, que contaba en sus filas con Ronaldinho, era considerado el máximo favorito para levantar el trofeo, pero su desempeño fue ampliamente criticado, aun cuando clasificaron invictos a la segunda ronda y Ronaldo alcanzó el récord de goles anotados en la historia de la competición. Alemania y Francia, que por otro lado casi no albergaban esperanzas de lograr un buen resultado, comenzaron a progresar a medida que avanzaba en el torneo. La primera ronda no presentó grandes sorpresas en general y la mayoría de los favoritos pasaron a la siguiente fase, a excepción de la que fue sobrepasada por y en el denominado "grupo de la muerte".
La supremacía europea se comenzó a manifestar durante la segunda fase. En cuartos de final, los penaltis marcaron el fin de la competición para e , derrotados respectivamente por Alemania y . Francia tuvo un avance imparable, derrotando a Brasil en cuartos de final (rompiendo el invicto que llevaba esa selección luego de la final de 1998) y a Portugal en semifinales. Reeditando la recordada semifinal de 1970, Italia y Alemania se enfrentaron nuevamente en dicha instancia; luego de mantenerse durante gran parte del partido sin anotar, los italianos accederían a la final al marcar dos goles minutos antes de acabar la prórroga. El partido final entre Italia y Francia, disputado en el Estadio Olímpico de Berlín, se desarrolló extremadamente parejo para ambos equipos, que durante los primeros 45 minutos habían anotado un gol cada uno. En la prórroga un polémico incidente provocó la expulsión de Zinedine Zidane al golpear al italiano Marco Materazzi. Sin su capitán, Francia se enfrentó a la definición desde los once pasos. David Trezeguet erró un tiro, lo que permitiría a Italia coronarse como campeona del Mundial por cuarta vez. El torneo de 2006 fue seguido por una audiencia acumulada a lo largo de todo su desarrollo superior a los 32 mil millones de espectadores en 207 países.

Tras el fracaso de la elección de un país africano para la Copa de 2006, la FIFA decidió establecer un sistema de "rotación continental" que permitiera que cada evento fuera organizado al menos una vez por cada confederación continental en un cierto período. África sería el primer continente elegido y Sudáfrica fue ampliamente apoyada como la sede de la Copa Mundial de Fútbol de 2010. De igual forma, el Mundial de 2014 fue asignado a Sudamérica, siendo Brasil el único postulante. 

El evento de 2010, organizado por Sudáfrica, mostró una serie de resultados sorpresivos, donde favoritos para ganar el torneo fueron eliminados en primera fase, como y , o tuvieron que sufrir hasta el último partido para pasar a la segunda fase, como , y . La mayoría de los equipos africanos tuvieron un mediocre desempeño pese a su localía, siendo el primer anfitrión de una Copa del Mundo en no pasar la primera fase. Por otro lado, los cinco miembros de la Conmebol destacaron tanto en primera ronda como en octavos de final, clasificando cuatro a la ronda de los ocho mejores (luego de que fuera eliminado por ). 

Esta dominación sudamericana se derrumbó en la ronda siguiente: , y fueron eliminados por Alemania, los y España. fue la única selección de Sudamérica superviviente, eliminando a , el último representante africano en la competición, en un polémico partido. En el último minuto de la prórroga, el delantero Luis Suárez detuvo un gol con su mano y que hubiera dado la victoria a los ghaneses; tras la expulsión de Suárez, Asamoah Gyan erró el penalti marcado y Ghana fue eliminada en la definición final a penaltis.

En la final, jugada Soccer City de Johannesburgo, se enfrentaron los Países Bajos y España, quienes derrotaron respectivamente a Uruguay y Alemania en semifinales. Tras una histórica racha de malos resultados mundialistas, España logró obtener su primer trofeo mundial luego que Andrés Iniesta anotara el único gol del partido a pocos minutos de que terminara la prórroga.

En 2014, la organización de la Copa Mundial de la FIFA regresó a tierras sudamericanas para disputar su vigésima edición en Brasil. El país había sido seleccionado como parte de la política de rotación continental y la llegada del torneo se sentía justa, considerando que Brasil había conseguido ganar 5 veces el torneo internacional desde que lo acogió en 1950. Sin embargo, el torneo enfrentó serias dificultades en su organización producto de retrasos en las construcciones de las sedes (muchas de las cuales serían utilizadas también para los Juegos Olímpicos de Río de Janeiro 2016) y el aumento en los gastos involucrados, provocó una serie de protestas en Brasil durante el año 2013. Pese a estos problemas, el torneo logró llevarse a cabo casi sin incidentes. 
La primera ronda presentó varios resultados inesperados, incluyendo la eliminación del campeón defensor España, Portugal, Inglaterra e Italia. Mientras ningún país asiático logró pasar a la segunda ronda, África tuvo por primera vez dos representantes en dicha ronda (Nigeria y Argelia), los que fueron eliminados en octavos de final. Sudamérica mostró buenos resultados: cinco de sus seis clasificados llegaron a octavos de final. Los otros cuatro países sudamericanos quedaron enfrentados entre sí sucesivamente. Brasil, por su condición de local y la participación de figuras como Neymar, era uno de los favoritos para ganar el torneo, pero su desempeño fue opaco. En la segunda ronda, debió llegar hasta la ronda de penaltis para eliminar a Chile y luego sufrió para derrotar a Colombia. En semifinales, la crisis futbolística del equipo local estalló con una vergonzosa goleada por 1:7 ante la selección de Alemania, en el partido que la prensa bautizó como "Mineirazo" y que contó con una anotación de Miroslav Klose, quien se convirtió en el máximo goleador de la historia de la Copa Mundial con 16 tantos. De la mano de Lionel Messi, Argentina demostró su carácter de favorito y clasificó hasta la final del torneo. Sin embargo, Alemania, que había tenido una campaña impecable hasta el momento, logró su cuarto trofeo mundial en el Estadio Maracaná tras un parejo partido contra Argentina que se definió con un gol de Mario Götze en la prórroga. Alemania logró por primera vez que un combinado no sudamericano ganara el trofeo en Sudamérica. 

Los problemas en la organización de los torneos de 2010 y 2014 y la presión ejercida por otros países con aspiraciones a organizar el torneo llevaron a un cambio en el sistema de rotación, impidiendo únicamente la postulación de países de un continente anfitrión por dos ediciones tras albergar el evento. El cambio generó un masivo número de postulaciones para las ediciones de 2018 y 2022, que decidieron otorgarse de forma simultánea. La votación dio como resultado a Rusia y Catar como sedes de los siguientes torneos, en detrimento de otras candidaturas consideradas favoritas. El resultado, que incluyó la elección de Catar, país que nunca había clasificado al Mundial ni organizado otro evento deportivo de dicho tamaño, provocó críticas y muchos indicaron la existencia de corrupción y compra de votos. Las investigaciones posteriores demostraron la existencia de una red de corrupción y fraude dentro de la FIFA que provocaron la expulsión de Joseph Blatter y otros altos dirigentes de la institución. 
Pese a las acusaciones y problemas judiciales, Rusia se mantuvo como la sede de la Copa Mundial de 2018, siendo la primera vez que el torneo es organizado en Europa Oriental como también el primero en ser organizado en dos continentes por la ubicación de una de sus sedes en Asia. En la parte deportiva, la gran sorpresa fue la eliminación de Alemania en la fase de grupos, al ser la primera vez desde 1938 que dicha selección no avanza más allá de primera fase en un campeonato mundial; también representaron notoriedad, el hecho de que ningún equipo africano logró avanzar de primera fase y que ningún equipo sudamericano logró avanzar de cuartos de final. La otra gran sorpresa fue la buena participación de Croacia, que le permitió convertirse en la decimotercera selección en lograr llegar a una final. Finalmente Francia se consagró campeón y volvió a conquistar la Copa del Mundo después de 20 años. Con el segundo título galo, se mantiene una hegemonía de los equipos europeos en las fases finales de la Copa Mundial desde la edición de 2006, donde todos los campeones son de dicha zona, incluyendo el hecho que todas las finales desde 2006 han sido disputadas entre equipos europeos exceptuando la edición de 2014. 

En tanto, la Copa Mundial de 2022 será la primera vez que el torneo sea organizado en Oriente Medio. Para el Mundial de 2026 se aprobó una expansión del torneo, el cual presentará a 48 equipos. La sede de este evento será conjunta de Canadá, Estados Unidos y México, tras ganar una votación dejando de lado la candidatura de Marruecos.

La Copa Mundial de Fútbol en la actualidad consta de dos etapas: una fase clasificatoria y una ronda final, considerada esta última usualmente como el evento en sí mismo. El número de participantes en esta ronda final ha variado con el paso de los años: 16 participantes hasta 1978 (a excepción de los mundiales de 1930 y 1950 con 13 participantes cada uno, y el de 1938 con 15), 24 entre 1982 y 1994, 32 desde 1998, y eventualmente serán 48 a partir de 2026.

La fase clasificatoria se ha disputado desde 1934. En ella, las selecciones nacionales que desean participar en el torneo se enfrentan en una serie de encuentros. Para ello, las asociaciones de fútbol que dirigen estas selecciones deben ser miembros plenos tanto de la FIFA como de alguna de las seis confederaciones continentales existentes en la actualidad:

Cada una de estas confederaciones organizan un sistema de elección de sus representantes a través de encuentros deportivos. El número de representantes de cada confederación es definido previamente por la FIFA a través de la entrega de cupos, algunos de los cuales son completos equivalentes a un equipo en la fase final y otros son compartidos, en los que un equipo debe definir su clasificación a la ronda final ante un representante de otra confederación en un proceso denominado generalmente repechaje, repesca o "play-offs".

Por ejemplo, para el Mundial de Rusia 2018, la FIFA estableció la siguiente distribución de los cupos clasificatorios:

A estos cupos se suma el equipo del país organizador del torneo, que desde los orígenes del torneo (a excepción de 1934) ha tenido ese derecho. Los equipos campeones del torneo previo deben en la actualidad participar del proceso clasificatorio, aunque tuvieron el derecho de clasificación automática entre 1938 y 2002.

La fase final del torneo es realizada cada cuatro años y en ella participan los equipos que sortearon exitosamente el proceso clasificatorio y aquellos clasificados por derecho propio. Esta etapa del torneo se realiza a lo largo de un mes exclusivamente en el país organizador designado con anterioridad. Sin embargo, en la Copa Mundial de Fútbol de 2002 el evento fue realizado por Corea del Sur y Japón conjuntamente; aunque la experiencia fue un éxito, el complejo proceso logístico necesario ha hecho que la FIFA considere evitar este tipo de torneos en el futuro.

El país organizador es electo por el Comité Ejecutivo de la FIFA, el cual se reúne seis años antes en Zúrich para poder tomar la decisión. El Comité Ejecutivo está compuesto por diversos representantes de las diferentes confederaciones y es presidido por el . El Comité Ejecutivo realiza una votación simple hasta lograr una mayoría absoluta de votos para determinar el país anfitrión de la Copa. En caso de que haya empate, es el presidente del organismo el encargado de dirimir la situación. En ocasiones anteriores se han logrado acuerdos previos entre los representantes de las candidaturas que han evitado la realización de votaciones o han generado votaciones unánimes. Así, por ejemplo en el 35º Congreso de la FIFA realizado en Londres durante 1966 los representantes de Alemania Occidental, Argentina y España retiraron sus candidaturas al aceptar la propuesta de organizar los torneos de 1974, 1978 y 1982 respectivamente, mientras en 1996 Corea del Sur y Japón aceptaron fusionar sus candidaturas en una sola y así evitar la votación.

La FIFA establece una serie de requisitos para poder organizar el torneo, especialmente en cuanto a infraestructura. En los últimos años las exigencias establecen al menos la existencia en el país de entre 8 a 10 estadios que superen los 40 000 espectadores. En caso de que estas exigencias no sean cumplidas, la FIFA tiene la posibilidad de asignar la sede a otro país.

La elección de la sede ha sido históricamente influida por el poder de las confederaciones continentales. En sus comienzos el torneo fue boicoteado tanto por países europeos como sudamericanos cuando la sede no era elegida en su continente. Para evitar esto, tras el receso producido por la Segunda Guerra Mundial se estableció un sistema de rotación "de facto" entre Europa y Sudamérica, los continentes con mayor tradición en la realización del torneo. Posteriormente, el cupo sudamericano se vería ampliado hacia todo el continente americano, permitiendo la inclusión de México y los Estados Unidos. En 1996 la FIFA insistió en la elección de una sede en Asia y posteriormente lo haría para África. En esta última elección, sin embargo, Alemania se impuso en la elección de la sede de la Copa Mundial de Fútbol de 2006, por lo que la FIFA instituyó una política de rotación continental. Bajo esta premisa, se estipuló la obligatoriedad de candidaturas africanas para 2010 y sudamericanas para 2014. En el caso de la elección de este último torneo, Brasil fue el único candidato por lo que se decidió revisar esta política para evitar este suceso. Así, la FIFA estableció en 2007 modificar este criterio permitiendo la postulación de cualquier país para las copas mundiales de 2018 en adelante, a excepción de aquellos provenientes de confederaciones que han albergado alguno de los dos torneos previos. Así pues, hasta la edición del año 2022, los países europeos habrán sido sede en once ocasiones, los sudamericanos en cinco ocasiones, los norteamericanos en tres ocasiones, los asiátiacos en dos y los africanos en una sola edición.

A lo largo de la historia de la Copa Mundial se han utilizado diversos sistemas de competición para poder determinar al equipo que se coronará como el mejor del mundo. Sin embargo, existe en general el patrón de establecer dos rondas en la competición, a excepción de 1934 y 1938, ediciones en las que se utilizó un formato único de eliminación directa.

Para la primera ronda del torneo los equipos son distribuidos en grupos de cuatro integrantes, aunque previamente, y debido a la retirada de algunos competidores, han existido grupos de hasta dos combinados. Para ello el comité organizador realiza un sorteo previo en el que se establecen a los mejores equipos como cabezas de series y se procura evitar que equipos de la misma confederación se enfrenten durante la primera ronda, a excepción de los equipos de la UEFA que por su mayor número es imposible que queden todos separados.

En cada uno de los grupos todos los equipos se enfrentan en un cuadrangular simple. Cada equipo acumula a lo largo de estos partidos una puntuación en función de sus resultados: 3 puntos por victoria (hasta 1990 eran solo 2), 1 por empate y 0 por derrota. Los cuatro equipos son ordenados de acuerdo a su puntuación en forma descendiente. En caso de que haya dos o más equipos con igual puntuación, existen otros criterios de desempate, que en el presente son:
Los criterios nombrados anteriormente permiten determinar qué equipos se clasifican a la segunda ronda. Cuando el número de participantes es una potencia de dos (8, 16 o 32) clasifican los dos mejores equipos del grupo, pero cuando no es así (por ejemplo, 24 participantes) pueden clasificar algunos de los mejores terceros.

En segunda ronda se han utilizado diversos mecanismos de clasificación para la ronda final. La mayoría de estos corresponden a un sistema de eliminación directa con octavos de final, cuartos de final, semifinales, un partido definitorio del tercer y cuarto lugar, y la final. Algunos torneos, sin embargo, realizaron un nuevo cuadro de grupos para determinar a los finalistas (1974, 1978 y 1982); solo en el torneo de 1950, el título se determinó a través de un sistema grupal entre los cuatro semifinalistas, sin embargo, el resultado de los diversos encuentros hizo que el último partido fuera el que definía al campeón, por lo que puede ser considerado comúnmente como la final.

Los partidos de eliminación directa, a diferencia de los de primera vuelta, no pueden finalizar con un empate. En caso de que los equipos que se enfrentan finalicen el tiempo reglamentario igualados en número de goles, se realiza una prórroga de dos tiempos de quince minutos cada uno (en 1998 y 2002 se utilizó el sistema de "gol de oro"). En caso de que finalizada esta prórroga la igualdad se mantenga, se realiza una tanda de penaltis de cinco tiros, extensible hasta que haya un equipo que logre la victoria.

Esta tabla muestra los principales resultados de la fase final de cada Copa Mundial de Fútbol.

La lista a continuación muestra a los 24 equipos que han estado entre los cuatro mejores de alguna edición del torneo.

En "cursiva", se indica el torneo en que el equipo fue local.

A continuación se muestran las dos confederaciones que se han repartido las copas del mundo.

En "cursiva" se indica el torneo donde la confederación fue local.

Más de 200 equipos diferentes han sido parte de los procesos clasificatorios y 79 han participado a lo largo de la fase final de la Copa Mundial. De ellos, 13 han llegado a la final del torneo y 8 han alcanzado la victoria.

Brasil es el equipo más exitoso, al alcanzar cinco campeonatos, seguido por Italia y Alemania con cuatro. En términos estadísticos, Brasil es el equipo con más victorias, seguido por Alemania e Italia. De los 4 títulos ganados por Alemania, 3 pertenecen a Alemania Federal y el ganado recientemente pertenece a la unificada; también, ha sido la selección con más participaciones en finales, en un total de 8.

Brasil e Italia son, además, los únicos equipos que han ganado dos torneos consecutivamente: Italia lo logró en 1934 y 1938, mientras que los sudamericanos lo lograron en 1958 y 1962. Ambos equipos se han enfrentado en dos finales (1970 y 1994), en ambas ha salido victorioso Brasil. La final de 1970, además, fue la primera en que se coronó a un tricampeón, al cual se le otorgó definitivamente el trofeo Jules Rimet.

De los ocho equipos campeones, todos, a excepción de Brasil y España, han sido campeones al menos una vez cuando el torneo fue organizado en su casa. Por otro lado, Brasil, España y Alemania son los equipos que han ganado un torneo fuera de su continente: en Suecia 1958 y Corea del Sur-Japón 2002 para el primero, en Sudáfrica 2010 para el segundo y en Brasil 2014 para el tercero. En cambio, México y son las únicas selecciones que han sido sede dos veces sin haber obtenido el título.

Alemania contra Argentina es el partido más repetido en finales de la Copa del Mundo; en México 1986; Italia 1990 y Brasil 2014.

En cuanto a las participaciones, Brasil es el único equipo presente en todos los eventos (21 en total), le sigue Alemania con 19, Italia con 18, Argentina con 17, México con 16, y España, Francia e Inglaterra con 15.

Cerca de 6000 jugadores han participado en la Copa Mundial y muchos de ellos han pasado a la historia. De ellos, un grupo selecto ha participado en múltiples oportunidades del evento: solo tres jugadores han disputado encuentros en cinco torneos: el mexicano Antonio Carbajal entre 1950 y 1966, el alemán Lothar Matthäus entre 1982 y 1998 (período en el cual incluso ganó el campeonato) y el mexicano Rafa Márquez entre 2002 y 2018. En cuanto a partidos disputados, Matthäus jugó 25 partidos, récord que se mantiene hasta la actualidad. El italiano Paolo Maldini por otro lado, es el jugador que ha jugado mayor cantidad de minutos, con 2217 minutos en sus cuatro participaciones entre 1990 y 2002.

En cuanto a goles, los dieciséis del alemán Miroslav Klose lo convierten en el jugador que más goles ha marcado en todos los eventos de la Copa. En la Copa Mundial de Fútbol de 1958, el francés Just Fontaine marcó 13 anotaciones, cifra que se ha mantenido como la mayor cantidad de goles alcanzada en un solo evento, siendo sus únicas anotaciones en mundiales.

Roger Milla, futbolista de Camerún se convirtió en el jugador más veterano (42 años) en marcar un gol en un mundial ante Rusia en el Mundial de 1994 disputado en Estados Unidos. En aquel mismo encuentro, el ruso Oleg Salenko anotó cinco tantos, estableciendo el récord de más goles en un partido de la Copa Mundial.

En el Mundial de Rusia 2018 el portero egipcio Essam El-Hadary se convirtió en el futbolista más longevo en disputar un partido en la historia de los mundiales de fútbol, jugando el encuentro entre Egipto y Arabia Saudita. A la fecha tenía 45 años y 161 días. 

Más de 1300 jugadores han anotado en algún partido de la Copa Mundial, totalizando 2548 goles en las 21 ediciones del torneo. El alemán Miroslav Klose es el que más goles ha anotado, con un total de 16 goles en 4 ediciones, mientras el francés Just Fontaine es el que tiene la marca de más goles en un único torneo, con 13 anotaciones.

A continuación se indican los trece jugadores que han anotado 10 o más goles en la historia del torneo. En "cursiva" se indican los jugadores activos seleccionables por su selección.

El entrenador italiano Vittorio Pozzo es el único que ha obtenido en dos ocasiones el campeonato de la Copa Mundial de Fútbol, en las ediciones de Italia 1934 y Francia 1938. En tanto, el alemán Franz Beckenbauer, el brasileño Mário Zagallo y el francés Didier Deschamps han sido los únicos entrenadores que han salido campeones tanto como jugadores como entrenadores.

Todos los entrenadores que han ganado algún campeonato han sido de la misma nacionalidad de las selecciones que dirigieron.

Durante las 21 ediciones de la Copa Mundial disputadas hasta 2018, se han marcado 2548 goles. De estos, 53 han sido autogoles. La Copa Mundial ha sido escenario de algunos de los goles más famosos de la historia del fútbol. Dentro de ellos destacan la llamada "Mano de Dios" y el "Gol del Siglo", ambos marcados por Diego Maradona durante el mismo partido de la Copa Mundial de Fútbol de 1986.

A nivel de torneos, las ediciones de Francia 1998 y la Brasil 2014 son las que han tenido mayor número de goles, con 171 anotaciones en sus 64 partidos disputados cada una, mientras el menor número fue en 1930 y 1934 con 70 goles (aunque la primera edición contó con 18 partidos, uno más que la edición de 1934). Considerando el número de partidos, el mayor número de goles por partido fue en el Mundial de 1954, con 5.38 tantos por encuentro; la cifra menor, en tanto, fue de 2.21 goles por partido en el Mundial de 1990. Esto refleja en general la evolución del fútbol: durante los primeros años del torneo, el fútbol se caracterizaba por su aspecto ofensivo y los partidos tenían un promedio cercano a los 4 goles por partido. Posterior a la Copa Mundial de 1954, sin embargo, el deporte comenzó a desarrollar más los aspectos defensivos, lo que influyó en la disminución de los goles anotados: desde 1958 en adelante, el promedio de goles no superó los 3 por partido.

Los sorteos de los equipos participantes han producido, en ocasiones, encuentros entre selecciones de nivel muy diferente, lo que se ha reflejado en goleadas. Sin embargo, no todos los encuentros con un alto número de goles anotados se deben únicamente a goleadas: el partido con más goles anotados fue el disputado entre y la local en la Copa Mundial de 1954, el cual finalizó con una victoria austríaca por 7:5. La final con más anotaciones, en tanto, fue la disputada en 1958 por y , que terminó con la victoria de los primeros por 5:2. Por otro lado, la final entre e en 1994 finalizó sin goles, por lo que se recurrió a una serie de penaltis, donde los sudamericanos pudieron levantar su cuarta copa mundial. También se destaca el 7:1 de Alemania a Brasil, siendo la mayor goleada recibida por un país anfitrión y una selección campeona del mundo en un mundial.

A continuación se listan las mayores goleadas en la Copa Mundial de Fútbol: 

Durante la realización de la Copa Mundial la organización dispone la entrega de diversos premios de acuerdo a la participación de los equipos y jugadores a lo largo del torneo.

Sin lugar a dudas, el principal premio es el título de campeón del evento. El equipo que logra coronarse como campeón recibe el Trofeo de la Copa Mundial de la FIFA por cuatro años. El equipo recibe además una réplica del trofeo y su nombre es grabado en la base de la original. Esta copa es entregada luego de que la Copa Jules Rimet fuera adjudicada de manera definitiva (tal y como lo establecía el reglamento) a Brasil cuando se coronó campeón por tercera vez en 1970. El equipo ganador además recibe un premio monetario, que en la última edición alcanzó los 16 millones de euros (equivalentes a más de 19 millones de dólares). La copa original fue diseñada por Abel Lafleur. El diseño actual es de Silvio Gazzaniga.

Desde el inicio del torneo, uno de los premios más importantes es al goleador del evento, es decir, el jugador que anota más goles durante la realización de la fase final de cada Copa Mundial. Desde la Copa Mundial de 1982 el premio fue instituido oficialmente como el «Botín de Oro». Desde el Mundial de 2006 fueron además entregados el «botín de plata» y el «botín de bronce», para los jugadores en el segundo y tercer lugar de la estadística de goleadores. Si hay dos o más jugadores con la misma cantidad de goles, cada uno recibe el premio correspondiente, sin tomar en cuenta la cantidad de minutos jugados por cada uno o si los goles fueron anotados en penaltis. Estadísticamente, se destaca el Campeonato de 1962, en el que hubo 6 goleadores, siendo la menor cantidad de goles marcados por un goleador.

El premio «Balón de Oro» es entregado al mejor jugador de cada edición de la Copa Mundial de la FIFA. Este reconocimiento se entrega desde la Copa Mundial de 1982.

Durante la realización del campeonato, la FIFA crea una lista con los 10 mejores jugadores del evento a su juicio. Los jugadores de esta lista son posteriormente votados por los representantes de la prensa especializada. El Balón de Oro es entregado al que haya obtenido más votos, mientras el Balón de Plata y el de Bronce se entregan al segundo y tercer más votados, respectivamente.

El proceso de elección ha sido criticado en las últimas ediciones, pues es realizado previo a la final del campeonato. Esto ha provocado que algunos jugadores hayan sido electos, pero en la final del torneo hay otro que destaca o simplemente el elegido no cumple con las expectativas. Solo en tres ocasiones (Paolo Rossi en 1982, Diego Maradona en 1986 y Romário en 1994), el premio ha sido otorgado a algún jugador del equipo campeón de ese año. El resto ha sido en general a jugadores del equipo derrotado en la final, con excepción de Salvatore Schillaci y Diego Forlán, quienes fueron derrotados en las semifinales de 1990 y 2010, respectivamente. El equipo campeón siempre ha tenido, sin embargo, alguno de sus jugadores entre la terna de jugadores premiados (ya sea Balón de Oro, Plata o Bronce).

Algunos de los otros premios entregados en la Copa Mundial en la actualidad se incluyen:

Además, en cada torneo se elige un "equipo estelar" en que se listan los mejores jugadores de cada evento en cada una de las demarcaciones.

Desde que fue por primera vez televisada en 1954, la Copa Mundial ha sido uno de los eventos deportivos más vistos a lo largo del mundo e incluso ha superado a los Juegos Olímpicos. La Copa Mundial de Fútbol de 2002, por ejemplo, tuvo una audiencia acumulada superior a los 28,8 mil millones de espectadores y solamente la final tuvo 1100 millones en todo el mundo.
Además, es uno de los sucesos más influyentes que existen en la actualidad. Para muchos países la realización del torneo en su patria o incluso la participación del equipo nacional es un hecho histórico de gran relevancia. Por ejemplo, la victoria alemana en la Copa Mundial de Fútbol de 1954 es considerado como uno de los momentos claves para la recuperación de dicho país tras la derrota en la Segunda Guerra Mundial. El torneo también ha sido utilizado con motivos propagandísticos, tanto por el fascismo en Italia 1934 como por la dictadura militar argentina en 1978. Una parte de su impacto cultural, lo da también las tertulias existentes entre los hinchas del fútbol, antes, durante y después de cada partido, que son parte en oficinas, restaurantes y hasta en el transporte público, organizando asados, almuerzos y hasta reuniones familiares o de amigos en las casas, para esperar los partidos, de acuerdo al huso horario del país en que se efectúa la transmisión televisiva, como así también del país que la organiza.

La gran repercusión del torneo a lo largo del mundo ha servido también como plataforma para la difusión de la cultura y representaciones artísticas de los países anfitriones. Una muestra de ello fue el "Walk of Ideas", una serie de estatuas monumentales representando los principales inventos generados en Alemania y que fue construida durante la realización de la Copa Mundial de Fútbol de 2006. La música también ha tenido un lugar de importancia: la mayoría de los torneos han contado con temas oficiales, los que han alcanzado gran popularidad a lo largo del mundo. Ricky Martin, tras el lanzamiento del tema oficial de Francia 1998, "La copa de la vida", pudo dar inicio a su exitosa carrera fuera del mundo hispanohablante.

El desarrollo tecnológico ha sido sumamente importante para que la Copa Mundial pudiera ser el evento que es. Sin lugar a dudas la televisión jugó un rol vital en la difusión del torneo a los diferentes continentes y así convertirlo en un torneo realmente mundial. Los primeros partidos fueron transmitidos durante la Copa Mundial de Fútbol de 1954 debido a la formación algunos años antes de la Unión Europea de Radiodifusión (Eurovisión). Siete partidos fueron transmitidos en vivo a Francia, Italia, Bélgica, los Países Bajos, Dinamarca, el Reino Unido, Alemania y Suiza, anfitriona del torneo. Cuatro años más tarde la cifra de países que recibieron la imagen en blanco y negro aumentó a 63, mientras que la final de Inglaterra 1966 sería el primer encuentro transmitido en color, pero esta tecnología se popularizaría en 1978. Con el lanzamiento de los sistemas de tecnología satelital el evento pudo ser transmitido en directo más fácilmente y en más países, reemplazando los resúmenes compactos que se daban en algunos países. Con el paso de los años, la tecnología permitió una mejor definición de las imágenes y ya desde la Copa Mundial de 2002, Internet se convirtió en una de las principales herramientas de comunicación, permitiendo no solo la utilización del Marcador Virtual que ponen las Páginas Web de los medios de comunicación, sino también permite la transmisión de los partidos completos y si la gente se lo perdió, permite verlo, como si fuera tiempo real o vía on demand. La televisión de alta definición debutaría durante la final de ese mismo torneo y se extendería al evento completo, cuatro años después. Asimismo, el canal que realiza la transmisión de los partidos, suele vender los derechos de los mismos a varias radioemisoras, para efectuar la transmisión parcial o total del Campeonato Mundial de Fútbol. En el caso de la radio, el público al que va dirigida la transmisión de los partidos, es principalmente gente que camina en las calles con reproductores de MP3 o MP4, pueblos donde no alcanza la televisión como cobertura y automovilistas. Con la utilización de la Norma Japonesa de TV Digital, se permite la utilización de TV en HD con dicha norma en vehículos particulares y autobuses.

Otro elemento que experimentaría un gran avance tecnológico de la mano del Mundial es el balón de fútbol. En los primeros eventos se utilizaron balones de cuero rellenados con una vejiga para darle consistencia, pero con el paso de los años fueron evolucionando y mejorando sus características. En México 1970 los balones naranjas de cuero fueron finalmente desechados, dando paso a las tradicionales pelotas de color blanco con cascos negros poligonales. Este nuevo balón fue denominado "Telstar", en honor al satélite que hacía posible la transmisión del evento a diversos rincones del orbe. En Argentina 1978 y España 1982 fue famoso por su difusión el balón "Tango". Cuatro años más tarde se utilizarían por primera vez materiales sintéticos para aumentar la impermeabilidad del balón y en 1986 sería el material principal del balón "Azteca". Con el paso de los años el balón ha ido mejorando progresivamente, haciéndose cada vez más liviano y veloz y perfeccionando su curvatura, hasta llegar en 2006 al "Teamgeist", que con catorce cascos (dieciocho menos que los de su antecesor, "Fevernova") unidos por termosoldadura lo hacen casi esférico en su totalidad. Para el 2010 se utilizó el balón "Jabulani". En el mundial de Brasil, 2014, se utilizó el balón "Brazuca".





</doc>
<doc id="8916" url="https://es.wikipedia.org/wiki?curid=8916" title="Péptido">
Péptido

Los péptidos son un tipo de moléculas formadas por la unión de varios aminoácidos mediante enlaces peptídicos.

Los péptidos, al igual que las proteínas, están presentes en la naturaleza y son responsables de un gran número de funciones, muchas de las cuales todavía no se conocen.

La unión de un bajo número de aminoácidos da lugar a un péptido, y si el número es alto, a una proteína, aunque los límites entre ambos no están definidos. Orientativamente:

Los péptidos se diferencian de las proteínas en que son más pequeños (tienen menos de 10.000 o 12.000 Daltons de masa) y que las proteínas pueden estar formadas por la unión de varios polipéptidos y a veces grupos prostéticos. Un ejemplo de polipéptido es la insulina, compuesta por 51 aminoácidos y conocida como una hormona de acuerdo a la función que tiene en el organismo de los seres humanos.

El enlace peptídico es un enlace covalente entre el grupo amino (–NH) de un aminoácido y el grupo carboxilo (–COOH) de otro.

Para nombrar un péptido se empieza por el aminoácido que porta el grupo –NH terminal, y se termina por el aminoácido que porta el grupo -COOH. En el sistema clásico cada aminoácido se representa por tres letras, y en el moderno, impuesto por la genética molecular, por una letra. Si el primer aminoácido de nuestro péptido fuera alanina y el segundo serina tendríamos el péptido alanil-serina, Ala-Ser, o AS.

Puesto que tienen un grupo amino terminal y un carboxilo terminal, y pueden tener grupos R ionizables, los péptidos tienen un comportamiento ácido-base similar al de los aminoácidos.

Los péptidos, al igual que aminoácidos y proteínas son biomoléculas con un carácter anfótero que permiten la regulación homeostática de los organismos.

Es de destacar este comportamiento en las enzimas, péptidos que funcionan como catalizadores biológicos de las reacciones metabólicas, ya que tienen una capacidad de funcionamiento dentro de ciertos niveles de pH. En caso de superarse se produce una descompensación de cargas en la superficie de la enzima, que pierde su estructura y su función (se desnaturaliza).

Son las mismas que las de los aminoácidos, es decir, las que den respectivamente sus grupos amino, carboxilo y R.
Estas reacciones (sobre todo las del los grupos amino y carboxilo) se han empleado para secuenciar péptidos.

En cuanto a las reacciones del grupo amino, es muy interesante la reacción con el reactivo de Sanger para secuenciar, ya que si tenemos el 2,4-dinitrofenil-péptido y lo hidrolizamos por hidrólisis ácida, se hidrolizarán todos los enlaces peptídicos y obtendremos el dinitrofenil del primer aminoácido de la secuencia, el –NH terminal, más el resto de los aminoácidos disgregados en el medio. 

Con esta reacción Sanger consiguió secuenciar la insulina.

En esta reacción, el núcleo coloreado de dinitrobenceno se une al átomo de nitrógeno del aminoácido para producir un derivado amarillo, el derivado 2,4-dinitrofenil o DNP-aminoácido. El compuesto DNFB reaccionara con el grupo amino libre del extremo amino de un polipéptido, así como también con los grupos amino de los aminoácidos libres. El enlace C–N que se forma es por lo general mucho más estable que un enlace peptídico. De esta forma, haciendo reaccionar una proteína nativa o un polipéptido intacto con el DNFB, hidrolizando la proteína en ácido y aislando los DNP-aminoácidos coloreados, puede identificarse el grupo amino terminal del aminoácido en una cadena polipeptídica. El grupo amino terminal de la lisina y algunos otros grupos funcionales de las cadenas laterales también reaccionarán con el DNFB.

Sin embargo, después de la hidrólisis, solo el derivado del grupo amino terminal del aminoácido original tendrá su grupo α-amino bloqueado; asimismo, tales DNP-α-aminoácidos pueden separarse de otros derivados DNP mediante procedimientos de extracción simples. Con cualquiera de los variados métodos cromatográficos se podrá identificar a los DNP-α-aminoácidos 

Pero este proceso consume mucha energía, ya que, teniendo el primer aminoácido hay que obtener los demás rompiendo por otras zonas.
Esto se evita con la degradación de Edman (también es una reacción de aminoácidos):
Como la ciclación se da en condiciones ácidas suaves, no se rompen los enlaces, y se da la feniltiohidantoína del aminoácido –NH terminal y queda el resto del péptido intacto.

Se separan ambos compuestos y por cromatografía se detecta. Con el resto del péptido se sigue con el mismo procedimiento hasta tener la secuencia completa.

Este método se conoce como degradación de Edman, y es la reacción que usan los secuenciadores automáticos de proteínas. Pero estos secuenciadores solo pueden secuenciar los 20 o 30 primeros aminoácidos, por lo que tendremos que hidrolizar y seguir después. Esto es porque el rendimiento no es del 100% y perdemos péptido poco a poco, y al final no nos queda. Solo las enzimas consiguen un rendimiento al 100%.

También podemos secuenciar empezando por el extremo carboxilo-terminal, para lo que se usan enzimas como la carboxipeptidasa. Es una proteasa que hidroliza los enlaces peptídicos. Ésta en concreto es una exoproteasa (ataca a la proteína por un extremo) que ataca al extremo carboxilo terminal.

Se emplean 2 tipos, la carboxipeptidasa A y B. Catalizan la misma reacción, pero tienen especificidad distinta. La A solo rompe el enlace peptídico si el aminoácido carboxilo-terminal es hidrofóbico. La B lo rompe si es básico.

Hay que controlar muy bien el tiempo de reacción, ya que cuando se libera un carboxilo terminal el siguiente aminoácido se convierte en el carboxilo terminal.

Respecto a las reacciones de los grupos R, existen muchos reactivos que reaccionan de forma específica con determinados grupos R (OH de la serina, tiol de la cisteína...). Esto se usa para ver qué aminoácido es esencial para el funcionamiento de la proteína.

Dentro de las reacciones de los grupos R, una interesante desde el punto de vista de aislamiento y purificación de proteínas es la del grupo tiólico (-SH) de la cisteína, que es fuertemente reductor. En presencia de O tiene mucha tendencia a oxidarse. Si hay dos moléculas de cisteína, en presencia de oxígeno, se oxidan para originar una molécula de cistina:

Esto ocurre frecuentemente en una proteína, cuando se pliega y dos moléculas de cisteína quedan próximas en el espacio, generando un puente disulfuro. El puente disulfuro ocurre de forma natural, y debe formarse para estabilizar la estructura tridimensional de las proteínas.
Sin embargo, puede que no deba ocurrir de forma natural, por ejemplo, si hay cisteínas esenciales expuestas (necesarias para la funcionalidad). 

Cuando aislamos una proteína de su entorno natural, ponemos a la proteína en presencia de oxígeno, con lo que esos grupos tiólicos se pueden oxidar, y la proteína perder su funcionalidad.

Para evitar esto, en los medios de aislamiento y purificación de proteínas añadimos β-mercapto-etanol, cuyo grupo tiólico es más reductor que el de la propia cisteína; tiene más tendencia a oxidarse.

De modo que al añadir β-mercapto-etanol, este se oxida y protege así los grupos tiólicos de la cisteína.

Cuando queremos estudiar la composición de aminoácidos de una proteína tenemos que hidrolizarla completamente, con lo que tenemos una mezcla de todo el conjunto de aminoácidos libres que constituyen dicha proteína.

Para evitar, en toda esta manipulación, que las Cys que tengamos en el medio se oxiden, tenemos que proteger su grupo tiólico añadiendo como reactivo iodoacetato:

Así transformamos la cisteína en carboximetilcisteína.


</doc>
<doc id="8928" url="https://es.wikipedia.org/wiki?curid=8928" title="Alelo">
Alelo

Un alelo o alelomorfo (del griego , lit. «de uno para con el otro») es cada una de las formas alternativas que puede tener un mismo gen que se diferencian en su secuencia y que se puede manifestar en modificaciones concretas de la función de ese gen (producen variaciones en características heredadas como, por ejemplo, el color de ojos o el grupo sanguíneo). Dado que la mayoría de los mamíferos son diploides, poseen dos juegos de cromosomas, uno de ellos procedente del padre y el otro de la madre. Cada par de alelos se ubica en igual locus o lugar del cromosoma.
Por alelo debe entenderse el valor de dominio que se otorga a un gen cuando rivaliza contra otro gen por la ocupación de posición final en los cromosomas durante la separación que se produce durante la meiosis celular. De ese valor de dominación del alelo procreador resultará la trasmisión, idéntica o distinta, de la copia o serie de copias del gen procreado. De acuerdo con esa potencia, un alelo puede ser dominante y expresarse en consecuencia en el hijo solamente con una de las copias procreadoras, por lo tanto si el padre o la madre lo poseen el cromosoma del hijo lo expresará siempre; o bien puede ser un alelo recesivo, por lo tanto se necesitarán dos copias del mismo gen, dos alelos, para que se exprese en el cromosoma procreado, esto es, deberá ser provisto al momento de la procreación por ambos progenitores.

El concepto de "alelo" se entiende a partir de la palabra "alelomorfo" (en formas alelas) es decir, algo que se presenta de "diversas" formas dentro de una población de individuos.

 
Por ejemplo, el gen que regula el color de la semilla del guisante presenta dos alelos: uno que determina el color verde y otro de que determina el color amarillo. Por regla general se conocen varias formas alélicas de cada gen; el alelo más extendido de una población se denomina "alelo normal, salvaje o silvestre", mientras que los alelos correspondientes, es decir, los que se encuentran en la hebra, podrían ser por sí mismos muchos más abundantes, otros más escasos, se conocen como polimorfismos. Así, de forma general, con dos alelos (a y a) podemos tener 3 tipos de combinaciones en diploides:


Los alelos son formas alternas de un gen, que difieren en secuencia o función. 

Toda característica genéticamente determinada depende de la acción de cuando menos un par de genes homólogos, que se denominan alelos.


En función de su expresión en el fenotipo, se pueden dividir en:


La frecuencia de alelos en una población diploide se puede utilizar para predecir las frecuencias de los correspondientes genotipos (véase ley de Hardy-Weinberg). Para un modelo simple, con dos alelos:

donde "p" es la frecuencia de un alelo y "q" es la frecuencia del alelo alternativo, cuya suma da uno. Luego, "p" es la fracción de la población homocigota para el primer alelo, 2"pq" es la fracción de heterocigotas, y "q" es la fracción homocigota para al alelo alternativo. Si el primer alelo es dominante sobre el segundo, entonces la fracción de la población que mostrará un fenotipo dominante es "p" + 2"pq", y la porción con el fenotipo recesivo es "q".

Para tres alelos:

En el caso de alelos múltiples en locus diploide, el número de genotipos posibles (G) según un número de alelos (a) se da por la expresión:



</doc>
<doc id="8929" url="https://es.wikipedia.org/wiki?curid=8929" title="Centrómero">
Centrómero

En genética, el centrómero es la construcción primaria que, utilizando tinciones tradicionales, aparece menos teñida que el resto del cromosoma. Es la zona por la que el cromosoma interacciona con los microtúbulos del huso acromático desde profase hasta anafase, tanto en mitosis como en meiosis, y es responsable de realizar y regular los movimientos cromosómicos que tienen lugar durante estas fases. Además, el centrómero contribuye a la nucleación de la cohesión de las cromátidas hermanas. En la estructura del centrómero intervienen tanto el ADN centromérico como proteínas centroméricas.

En la levadura de gemación ("Saccharomyces cerevisiae") el ADN centromérico consta únicamente de 125 pb y está conservado entre los diferentes cromosomas. Sin embargo, el ADN centromérico en metazoos puede constar de megabases, y no contiene secuencias consenso fácilmente identificables (ver la revisión de Choo en 1997). A pesar de las diferencias entre el ADN centromérico de levaduras y metazoos, el cinetocoro se ensambla en ambos casos sobre nucleosomas centroméricos que contienen una forma especializada de histona H3 (Cse4p en levaduras o su homólogo CENP-A en metazoos).

El ADN centromérico se organiza en forma de heterocromatina constitutiva, que permanece condensada en casi todas las células somáticas de un organismo. Estas regiones son pobres en genes y pueden inducir la represión de la expresión génica de las regiones adyacentes de manera epigenética. Este fenómeno se denomina "variegación por efecto de posición" (PEV, por "Position Effect Variegation"). La aparición ocasional de centrómeros "de novo" (neocentrómeros) sugiere que más que la secuencia del ADN "per se", la característica primaria de los centrómeros es la organización estructural de los dominios centroméricos. La selección del centrómero puede ser también el resultado de un complejo número de parámetros, como el momento de su replicación, la posición dentro del núcleo celular, así como otras características heredables de la estructura de la cromatina.

El centrómero tiene un comportamiento diferente durante la anafase mitótica y la anafase-I de la meiosis, de manera que durante la anafase mitótica las cromátidas hermanas se separan a polos opuestos ("segregación anfitélica") mientras que en la anafase-I de la meiosis lo que se separa a polos opuestos son los cromosomas homólogos completos, cada uno constituido por dos cromátidas ("segregación sintélica").

Cada cromosoma posee dos brazos, uno largo (llamado "q") y otro corto (llamado "p") separados por el centrómero, los cuales se conectan de forma metacéntrica, submetacéntrica, acrocéntrica, holocéntrica o telocéntrica. 

Un cromosoma metacéntrico es un cromosoma cuyo centrómero se encuentra en la mitad del cromosoma, dando lugar a brazos de igual longitud.

Cuatro pares de los cromosomas humanos poseen una estructura metacéntrica, el 1, el 3, el 19 y el 20.

Un cromosoma submetacéntrico es un cromosoma en el cual el centrómero se ubica de tal manera que un brazo es ligeramente más corto que el otro.

La mayor parte de los cromosomas humanos son submetacéntricos excepto los cromosomas 1, 3, 19, 20 y el X que son metacéntricos y 13, 14, 15, 21 y 22 que son acrocéntricos. Además, el cromosoma Y a veces es considerado submetacéntrico aunque otros lo describen como acrocéntrico sin satélite.

Un cromosoma acrocéntrico es un cromosoma en el que el centrómero se encuentra más cercano a uno de los telómeros, dando como resultado un brazo muy corto (p) y el otro largo (q).

De los 23 pares de cromosomas humanos el cromosoma 13, el 14, el 15, el 21 y el 22 son acrocéntricos y actúan como organizadores nucleolares.

Aun cuando el concepto es ampliamente aceptado y distribuido entre la comunidad científica, realmente, un cromosoma telocéntrico como tal no existe. Supuestamente en este tipo de cromosomas el centrómero está localizado en un extremo del mismo, pero la región telocéntrica no permite que molecularmente haya otra estructura finalizando al cromosoma. De hecho, el acortamiento del telomero o su ausencia total causa inestabilidad en los cromosomas y la consecuente Translocación robertsoniana. Por tanto, el término telocéntrico es incorrecto y debe considerarse el término subtelocéntrico, el cual implica que el telómero se ubica al final así no sea visible y que el centrómero esta después invariablemente.

Ninguno de los cromosomas humanos presenta esta característica; pero, por ejemplo, los 40 cromosomas del ratón común son subtelocéntricos.

Los análisis llevados a cabo en "S. cerevisiae" para aislar el ADN centromérico (ADN CEN) de todos sus cromosomas, han establecido que en todos los centrómeros de levaduras estudiados existen tres regiones muy conservadas:


En la cromatina en su forma nativa, el ADN CEN es un segmento de 220-250 pb protegido de la acción de las nucleasas y flanqueado en ambos extremos por sitios hipersensibles al corte y un conjunto de nucleosomas altamente organizados, que contienen la histona especializada Cse4p (el homólogo en levaduras de CENP-A) en lugar de H3. 

Las mutaciones en las regiones I y II reducen pero no inactivan la función del centrómero, mientras que las que ocurren en la región III lo inactivan completamente. En los mutantes por deleción de la región CDEII, la función centromérica puede restablecerse casi completamente insertando una secuencia de ADN compuesta sólo de A-T al azar y de tamaño equivalente. Por tanto, en esta región los factores críticos para una segregación cromosómica correcta son el contenido en A-T y la longitud del ADN, más que la secuencia de nucleótidos, quizás porque influyen en la conformación del ADN centromérico.

CDEIII es la zona de unión del complejo proteico CBF3 (por "Centromere Binding Factor 3"), compuesto por las proteínas Ndc10p, Cep3p, Ctf13p, y Skp1p (esta proteína - también denominada p23- es además parte del complejo SCUL implicado en los procesos de degradación mediados por ubiquitina necesarios para la progresión a través del ciclo celular). En ausencia de CBF3, el cinetocoro no es funcional, tanto "in vivo" como "in vitro", y todas las proteínas del cinetocoro conocidas, incluida Cse4p, presentan alteración de la asociación con el centrómero. Sin embargo, la unión de CBF3 al ADN centromérico "in vivo" no requiere Cse4p. Por tanto, la unión específica de CBF3 a la región CDEIII participa en la definición de la localización del cinetocoro en levaduras. Otra proteína esencial para la viabilidad de las células de levaduras es CBF5p, que co-purifica con el complejo CBF3 en condiciones de baja astringencia. Esta proteína también se une a microtúbulos "in vitro", y parece ser importante en la transición G/S del ciclo celular, ya que la eliminación de CBF5p bloquea la división celular antes de que ocurra la replicación del ADN.

Por otro lado, CDEI es el sitio de unión de un homodímero de Cbf1p. Cbf1p no es esencial para la función del cinetocoro, pero induce el plegado del ADN y por tanto puede contribuir a la estructura de nivel superior del centrómero. Cbf1p tiene similitud estructural e identidad de secuencia limitada a CENP-B, que se une al ADN centromérico de metazoos y también induce el plegado del ADN. 

Mif2p es una proteína esencial en levaduras, similar a CENP-C de metazoos, cuya eliminación produce fallos en la segregación cromosómica, retraso en mitosis y microtúbulos con morfología aberrante. MIF2 interacciona genéticamente con tres de los genes que codifican proteínas centroméricas: CBF1, CBF3a y CBF3b. Aunque la unión de Mif2 al centrómero depende de Ndc10, su localización en el ADN CEN está muy disminuida en mutantes para Cse4. Mif2 presenta un dominio acídico y otro dominio rico en prolina, lo que se denomina un "gancho AT" ("AT hook"), un motivo que es común a las proteínas que se unen a secuencias de ADN ricas en AT (como las proteínas HMGI(Y) de mamíferos), lo que sugiere que Mif2 se une a la región CDEII.

En "S. cerevisiae", el ADN centromérico se replica en una etapa temprana de la fase S, tal vez porque es necesario replicar el ADN centromérico para iniciar el ensamblaje de los cinetocoros hermanos.

El ADN centromérico de "Schizosaccharomyces pombe" es considerablemente más complejo que el de "S. cerevisiae" y comparte algunas propiedades con los centrómeros regionales de los eucariotas superiores. El centrómero de "S. pombe" está constituido por 40-100 kb de ADN organizado en distintos tipos de repeticiones específicas de los centrómeros (las repeticiones tipo K), que están a su vez organizadas en una gran repetición invertida. Como ocurre en eucariotas superiores, la organización de los centrómeros en la levadura de fisión varía considerablemente entre diferentes cromosomas y entre cepas muy próximas. El centro de la repetición invertida, el núcleo central, contiene una secuencia de 4-7 kb que es fundamental para la función centromérica. Las regiones centrales del centrómero de "S. pombe" se organizan en una estructura inusual, que depende de la existencia de un elemento dentro de las repeticiones tipo K (denominado "enhancer" centromérico) y que es fundamental para la función del centrómero. Las regiones K y el núcleo central son las dianas de los mecanismos epigenéticos que afectan a la función del centrómero "in vivo". Por otro lado, existe redundancia funcional tanto entre las repeticiones tipo K como en el núcleo central.

La función precisa de las secuencias repetidas de ADN en el centrómero no está muy clara, pero probablemente tienen una función estructural en el apareamiento y la segregación cromosómica (revisado por Karpen y Allshire en 1997), y sólo indirectamente producen silenciamiento transcripcional. Las secuencias repetidas más externas de los centrómeros de "Schizosaccharomyces pombe" son heterocromáticas y son necesarias para el ensamblaje de un centrómero activo. A partir de estas secuencias repetidas se generan tránscritos que son procesados por los componentes de la maquinaria de RNAi y medían el silenciamiento de la cromatina. (Véase también Ensamblaje de heterocromatina mediante RNAi en "S. pombe").

La determinación de los centrómeros de metazoos constituye una tarea difícil y esquiva. En animales y plantas, los centrómeros están incluidos en regiones de ADN satélite altamente repetido, que resulta difícil de analizar incluso con los métodos de mapeo más potentes. Estas regiones de ADN satélite están embebidas en regiones de heterocromatina constitutiva, que se mantiene silenciada en la mayor parte de las células somáticas de un organismo. La ausencia mayoritaria de genes activos en las regiones centroméricas es una característica que parece que se ha adquirido progresivamente a través de la evolución.

En "Drosophila melanogaster" la secuencia AATAACATAG está repetida en tándem en las regiones próximas al centrómero. Dado que estas secuencias cortas de 10 pb no son representativas del genoma de una especie, suele suceder que su contenido en G+C es diferente al contenido en G+C del resto del genoma. Esto hace que cuando el ADN de una especie eucarionte se centrifuga en gradiente de densidad de cloruro de cesio, aparezca una banda principal que contiene la mayor parte del ADN de la especie y una banda satélite (minoritaria) que está formada por una secuencia corta de ADN repetida en tándem. El ADN satélite en algunas especies tiene mayor densidad que el ADN principal (mayor contenido en G+C) y en otras especies tiene menor densidad y, por tanto, menor contenido en G+C que el ADN principal. Cuando el ADN satélite de ratón se marca radiactivamente y se realiza una hibridación "in situ" con el ADN de cromosomas metafásicos mitóticos, se observa que el marcaje radiactivo (hibridación) se produce en regiones próximas al centrómero. El ADN satélite también se denomina α-satélite y es uno de los componentes del genoma eucariótico que evoluciona más rápidamente. 
A pesar de que en la mayor parte de los casos no se han detectado motivos especialmente definidos, una secuencia candidata en el ADN satélite tendría que estar repetida en cientos de kilobases, ya que éste es el tamaño mínimo de un centrómero funcional que se ha identificado en diferentes organismos. Por ejemplo, en Drosophila la unidad mínima necesaria de repeticiones en tándem es de 420 kb, en maíz se necesitan 500 kb y en humanos la unidad mínima consiste de 100 kb. Una característica interesante de la mayor parte del ADN satélite es su unidad de longitud, ya que aunque no se han detectado secuencias con motivos conservados, la longitud de la unidad que se repite es muy parecida entre organismos. En primates, por ejemplo, la unidad básica que se repite tiene 171 pb, en el pez "Sparus aurata" la repetición centromérica tiene 186 pb, en el insecto "Chironomus pallidivittatus" tiene 155 pb, en "Arabidopsis thaliana" y en el maíz tiene 180 pb, y en el arroz 168 pb. La estrecha variación en longitud de la unidad que se repite en el ADN satélite corresponde aproximadamente al rango de longitud del ADN que rodea a un nucleosoma, y repeticiones más largas, como las que se encuentran en los centrómeros del cerdo (340 pb) corresponden aproximadamente a la longitud de dos nucleosomas. Hay excepciones notorias, como las repeticiones pentaméricas que se encuentran en "Drosophila melanogaster". En general, la selección de la longitud de un nucleosoma podría limitar la evolución del ADN centromérico, de acuerdo con su función estructural en el genoma. 

Estas unidades mínimas (denominadas monómeros) se encuentran normalmente asociadas de forma cabeza-cola. En las regiones centroméricas del núcleo funcional, el ADN satélite se organiza en una unidad repetida que consta de múltiples monómeros. La unidad multimonomérica se repite a su vez muchas veces, generando un vector (array) de nivel superior. Los vectores de nivel superior de ADN satélite son la organización típica de las regiones centroméricas humanas y se extienden a través de megabases de ADN que se encuentran mayoritariamente ininterrumpidos por ningún tipo de inserción o mutación. Por tanto en animales y plantas se presentan centrómeros "regionales", frente a los centrómeros "puntuales" que se encuentran en levaduras.

Sin embargo, a pesar de que el ADN satélite se encuentra en los centrómeros humanos nativos, se han detectado centrómeros humanos generados "de novo" (neocentrómeros) que carecen de α-satélite u otras repeticiones en tándem, lo que indica que el ADN satélite no es fundamental para definir un centrómero funcional.

Una característica conservada y heredable de los centrómeros es la presencia en sus nucleosomas de una variante especial de la histona H3, que se encuentra únicamente en el núcleo de la región centromérica. Esta histona específica de los centrómeros se denomina CENP-A en mamíferos ("centromeric protein A"), Cid ("centromere identifier") en "Drosophila", Cse4 en "S. cerevisiae" y Cnp1 en "S. pombe" (revisado por Choo en 2001). La presencia de esta variante de la histona H3 parece ser fundamental para el ensamblaje del cinetocoro y distingue la placa interna del cinetocoro de la heterocromatina pericéntrica, que contiene la histona H3 normal. CENP-A presenta algunas características que la diferencian de la histona H3 normal, como una cola NH-terminal no canónica, un plegamiento divergente y una región lazo 1 más largo. Aunque la histona H3 está sometida a una fuerte selección evolutiva, las histonas centroméricas son sorprendentemente divergentes. Esta diferencia podría deberse a la necesidad de H3 de interaccionar con todo el genoma, mientras que la variante centromérica sólo necesita interaccionar con el ADN centromérico correspondiente. Este ADN está formado por ADN satélite, que es uno de los componentes del genoma eucariótico que evoluciona más rápidamente. Se ha propuesto que la interacción entre la histona centromérica y el ADN centromérico es responsable de la longitud similar a la que rodea un nucleosoma de las repeticiones del ADN satélite.

Un estudio realizado en "Drosophila" identificó que las regiones centrales de los centrómeros se replican como dominios aislados en estadios tempranos de la fase S, antes de la replicación de la heterocromatina pericéntrica, que se replica de forma tardía. Si en el momento que se replican los centrómeros, la región del núcleo en la que se localizan los centrómeros excluye la histona H3 pero secuestra la histona centromérica, la compartimentalización aseguraría que sólo CENP-A esté disponible para el ensamblaje de la cromatina centromérica. Este modelo se apoya en varias líneas de evidencias en diferentes organismos.

Además de CENP-A, se han identificado otros componentes constitutivos en el centrómero. Uno de ellos es CENP-C, que está conservado evolutivamente, aunque sólo comparte un motivo de 20 aminoácidos con su homólogo en "S. cerevisiae", Mif2. También se han encontrado homólogos en otras especies, como HCP-4 en "C. elegans". La localización centromérica de CENP-C depende de CENP-A y se ha sugerido que CENP-C podría interaccionar con la estructura de la cromatina alterada por CENP-A. Sin embargo, aunque la zona de unión de CENP-C al ADN se ha mapeado en la zona central de la proteína, no se ha identificado una secuencia específica de unión. Parece además que CENP-C presenta la capacidad de unirse a ARN de forma específica, aunque la contribución de estas capacidades a la localización de CENP-C no está clara.

CENP-B es la única proteína centromérica que se une a una secuencia de ADN específica de 17 pb (la "caja CENP-B"), que se encuentra en un subconjunto de monómeros de α-satélite. La función de CENP-B en los centrómeros no está clara, ya que a diferencia con CENP-A o CENP-C, CENP-B no es esencial para la función mitótica (de hecho, el ratón knockout para CENP-B es viable). CENP-B puede estar presente tanto en centrómeros activos como inactivos, lo que sugiere que no está asociada simplemente a la función centromérica. Además, algunos centrómeros funcionales carecen de cajas CENP-B (como el centrómero del cromosoma Y, por ejemplo). Sin embargo, se ha demostrado que la unión de CENP-B aumenta la eficiencia de la unión de CENP-A en cromosomas humanos artificiales. A pesar de no ser esencial en humanos, CENP-B está conservada a través de varios phyla y en "S. pombe" se encuentran tres homólogos que sí son esenciales para la viabilidad celular.

Estas tres proteínas centroméricas están organizadas de forma diferente en el centrómero humano. CENP-B está presente en todo el vector de nivel superior, mientras que CENP-A y CENP-C se encuentran sólo en algunos bloques de unidades repetidas, intercalados con bloques que contienen nucleosomas canónicos (que incluyen histona H3) y con modificaciones de histonas más características de eucromatina que de heterocromatina. Se cree que esos bloques de CENP-A se auto-organizan para presentar una "superficie" combinada que organiza el resto de las proteínas del cinetocoro, que servirá como sitio de anclaje de los microtúbulos. Se considera además que los monómeros pericéntricos que flanquean las repeticiones centroméricas están generalmente desprovistos de proteínas centroméricas, y están empaquetados en nucleosomas canónicos que poseen modificaciones de histonas características de heterocromatina, y unidos a proteínas específicas de heterocromatina como HP1. Esta heterocromatina pericéntrica es importante tanto para definir los límites de los dominios centroméricos como para reclutar las cohesinas que mantendrán unidas las cromátidas hermanas hasta la anafase durante el ciclo celular.

El proceso de segregación cromosómica está sometida a una fuerte presión evolutiva, dado que la pérdida o ganancia de cromosomas (una situación denominada aneuploidía) puede producir importantes alteraciones fenotípicas, como el síndrome de Down en humanos, por ejemplo. Por ello, la maquinaria encargada de distribuir los cromosomas entre las células hijas durante la división celular presenta una gran sofisticación y está sometida a un estricto control (véase checkpoint de mitosis). Los centrómeros son las regiones cromosómicas sobre las que se ensamblan los cinetocoros, que son las estructuras proteicas responsables del anclaje de los cromosomas al huso mitótico, y por ello la zona responsable del movimiento cromosómico y su regulación. Sin embargo, a pesar de ello las secuencias de ADN que definen las secuencias centroméricas están muy poco conservadas y evolucionan rápidamente incluso entre especies muy relacionadas. Esto no quiere decir que las secuencias de ADN del centrómero son hipermutables, sino que las variantes de las secuencias se fijan por expansión y contracción, y pueden aparecer "de novo" en sitios nuevos (neocentrómeros). Estos cambios en el ADN centromérico tienen lugar debido a la existencia de diferentes procesos mutacionales, como errores en la replicación del ADN, intercambio desequilibrado, transposición y escisión. Las proteínas centroméricas también presentan signos inesperados de una rápida evolución. Por todo ello, se ha sugerido que en el núcleo de esta rápida evolución existe un conflicto genético en funcionamiento.

Parece ser que la arquitectura en un vector ("array") de nivel superior que se observa en los centrómeros de humanos podría haber aparecido recientemente en un centrómero en la evolución de los primates (alrededor de la separación gorila-orangután) y se extendió a los otros cromosomas vía transposición. Posteriormente, los intercambios desiguales o conversiones génicas amplificaron los vectores de nivel superior, dando lugar a la arquitectura en vectores centroméricos de nivel superior que es específica de la especie humana y que se observa en diferentes cromosomas humanos. Además, se han generado algunas variantes por mutación que se han fijado en algunos centrómeros. La comparación de unidades monoméricas y unidades vectoriales de nivel superior que se encuentran en los centrómeros de cromosomas ortólogos (por ejemplo, entre chimpancés y humanos) ha llevado al descubrimiento sorprendente de que los vectores centroméricos de diferentes especies son más divergentes entre sí que las unidades pericéntricas. Esta observación es anti-intuitiva, porque el vector de ADN satélite centromérico es el centrómero funcional y está sometido a una fuerte presión selectiva, mientras que las regiones de heterocromatina pericéntrica no lo están. Por tanto, la observación es paradójica: las unidades de ADN satélite que están fuertemente limitadas dentro de una especie han evolucionado rápidamente entre especies. 

Esta paradoja ha llevado a pensar que alguna fuerza selectiva debe dirigir la rápida fijación de las mutaciones en los vectores centroméricos, imponiendo un sesgo para mantener las mutaciones, incrementando de esta forma las tasas de mutación del vector completo. Se ha sugerido que esta fuerza selectiva puede ser la ventaja conferida a los centrómeros durante la meiosis femenina, o "deriva-centromérica": nuevas variaciones en la secuencia de α-satélite, una nueva organización o simplemente un incremento en la cantidad de α-satélite proporciona una mayor oportunidad de incorporación de CENP-A y por tanto una mayor capacidad para la unión de microtúbulos. La asimetría de la tétrada meiótica femenina proporciona una oportunidad para los cromosomas de competir por ser incluido en el núcleo del ovocito mediante una orientación favorable durante la meiosis. Los centrómeros que aprovechan esta oportunidad en meiosis I "ganan", y una ligera ventaja en cada meiosis femenina es suficiente para fijar una variación centromérica favorable. 

Como contrapartida, mientras que la deriva centromérica puede generar una ventaja selectiva en la meiosis femenina, puede producir defectos en la meiosis masculina, pues en este caso un centrómero mutado se apareará con otro normal, generándose una diferencia de tensión que puede activar el checkpoint de mitosis, provocando la muerte celular y con ello una disminución de la fertilidad masculina. Una forma de contrarrestar este efecto en la meiosis masculina sería la aparición de mutaciones en las proteínas centroméricas con alteración en su capacidad de unión al ADN y que equilibraran la tensión centromérica. La proteína candidata más probable es CENP-A.

Si este proceso tiene lugar en dos poblaciones aisladas de la misma especie, las configuraciones del ADN satélite y CENP-A divergirán rápidamente. En cada población, CENP-A evolucionará para suprimir los efectos deletéreos de la evolución del ADN satélite. De esta forma, las nuevas variantes de CENP-A resultarán incompatibles con el ADN satélite de la otra población. Cruces entre ambas poblaciones resultarán en defectos en los híbridos. Por tanto, el proceso evolutivo entre CENP-A y el ADN satélite da lugar al inicio del aislamiento reproductivo entre las dos poblaciones (véase también Mecanismos de aislamiento reproductivo). Esto quiere decir que la evolución centromérica tiene como consecuencia inevitable la especiación.






</doc>
<doc id="8930" url="https://es.wikipedia.org/wiki?curid=8930" title="Terapia génica">
Terapia génica

La terapia génica humana consiste en la inserción de elementos funcionales ausentes en el genoma de un individuo. Se realiza en las células y tejidos con el objetivo de tratar una enfermedad o realizar un marcaje.

La técnica todavía está en desarrollo, motivo por el cual su aplicación se lleva a cabo principalmente dentro de ensayos clínicos controlados, y para el tratamiento de enfermedades severas o bien de tipo hereditario o adquirido. Al principio se planteó solo para el tratamiento de enfermedades genéticas, pero hoy en día se plantea ya para casi cualquier enfermedad.

Entre los criterios para elegir este tipo de terapia se encuentran:

Esto puede hacer un bien ya que puede curar o eliminar alguna enfermedad hereditaria.





Aunque se han utilizado enfoques muy distintos, en la mayoría de los estudios de terapia génica, una copia del gen funcional se inserta en el genoma para compensar el defectivo. Si ésta copia simplemente se introduce en el huésped, se trata de terapia génica de adición. Si tratamos, por medio de la recombinación homóloga, de eliminar la copia defectiva y cambiarla por la funcional, se trata de terapia de sustitución.

Actualmente, el tipo más común de vectores utilizados son los virus, que pueden ser genéticamente alterados para dejar de ser patógenos y portar genes de otros organismos. No obstante, existen otros tipos de vectores de origen no vírico que también han sido utilizados para ello. Así mismo, el ADN puede ser introducido en el paciente mediante métodos físicos (no biológicos) como electroporación, biobalística... Si conocemos la secuencia de ADN que resulta defectuosa en el paciente, podemos retirarla e introducir el material genético para lograr la correcta expresión del gen. Generalmente, se usan un tipo de enzimas que se denominan endonucleasas de restricción. Estas enzimas son capaces de reconocer determinados genes en función de la secuencia de aminoácidos, para posteriormente unirse a ese gen, cortarlo y, posteriormente, introducir el material genético correcto para lograr la expresión del gen. Es un proceso que puede parecer complejo, pero nada más lejos de la realidad ya que solo debemos conocer la secuencia errónea y emplear una endonucleasa de restricción que la reconozca, corte, e introduzca la secuencia génica de interés correcta.

Las células diana del paciente se infectan con el vector (en el caso de que se trate de un virus) o se transforman con el ADN a introducir. Este ADN, una vez dentro de la célula huésped, se transcribe y traduce a una proteína funcional, que va a realizar su función, y, en teoría, a corregir el defecto que causaba la enfermedad.

La gran diversidad de situaciones en las que podría aplicarse la terapia génica hace imposible la existencia de un solo tipo de vector adecuado. Sin embargo, pueden definirse las siguientes características para un "vector ideal" y adaptarlas luego a situaciones concretas:


Los vectores van a contener los elementos que queramos introducir al paciente, que no van a ser solo los genes funcionales, sino también elementos necesarios para su expresión y regulación, como pueden ser promotores, potenciadores o secuencias específicas que permitan su control bajo ciertas condiciones.

Podemos distinguir dos categorías principales en vectores usados en terapia génica: virales y no virales.

Todos los virus son capaces de introducir su material genético en la célula huésped como parte de su ciclo de replicación. Gracias a ello, pueden producir más copias de sí mismos, e infectar a otras células.

Algunos tipos de virus insertan sus genes físicamente en el genoma del huésped, otros pasan por varios orgánulos celulares en su ciclo de infección y otros se replican directamente en el citoplasma, por lo que en función de la terapia a realizar nos puede interesar uno u otro.

Algo común a la mayoría de estrategias con virus es la necesidad de usar líneas celulares "empaquetadoras" o virus helpers, que porten los genes que les eliminamos a nuestros vectores y que permiten la infección.

El genoma de los retrovirus está constituido por ARN de cadena sencilla, en el cual se distinguen tres zonas claramente definidas: una intermedia con genes estructurales, y dos flanqueantes con genes y estructuras reguladoras. Cuando un retrovirus infecta a una célula huésped, introduce su ARN junto con algunas enzimas que se encuentran en la matriz, concretamente una proteasa, una transcriptasa inversa y una integrasa.

La acción de la retrotranscriptasa permite la síntesis del ADN genómico del virus a partir del ARN. A continuación, la integrasa introduce este ADN en el genoma del huésped. A partir de este punto, el virus puede permanecer latente o puede activar la replicación masivamente.

Para usar los retrovirus como vectores víricos para terapia génica inicialmente se eliminaron los genes responsables de su replicación y se reemplazaron estas regiones por el gen a introducir seguido de un gen marcador.

Del genoma vírico quedaban las secuencias LTR; y los elementos necesarios para producir los vectores a gran escala y para transformar las células son aportados desde otros vectores, bien plasmídicos o bien en líneas celulares específicas. En el caso de usar vectores plasmídicos, estrategias como cotransformar con varios plásmidos distintos que codifiquen para las proteínas del retrovirus, y que la transcripción de sus secuencias esté sometida a promotores eucariotas puede contribuir a minimizar el riesgo de que por recombinación se generen virus recombinantes.

Actualmente se buscan estrategias como la anterior para conseguir una mayor seguridad en el proceso. La adición de colas de poliadenina al transgén para evitar la transcripción de la segunda secuencia LTR es un ejemplo de esto.

Los retrovirus como vector en terapia génica presentan un inconveniente considerable, y es que la enzima integrasa puede insertar el material genético en cualquier zona del genoma del huésped, pudiendo causar efectos deletéreos como la modificación en el patrón de la expresión (efecto posicional) o la mutagénesis de un gen silvestre por inserción.

Ensayos de terapia génica utilizando vectores retrovirales para tratar la inmunodeficiencia combinada grave ligada al cromosoma X (X-SCID) representan la aplicación más exitosa de la terapia hasta la fecha. Así, más de veinte pacientes han sido tratado en Francia y Gran Bretaña, con una alta tasa de reconstitución del sistema inmunitario. Sin embargo, ensayos similares fueron restringidos en los Estados Unidos cuando se informó de la aparición de leucemia en pacientes. Hasta hoy se conocen cuatro casos de niños franceses y uno británico que han desarrollado leucemia como resultado de mutagénesis por inserción de los vectores retrovirales, y todos menos uno de estos niños respondieron bien al tratamiento convencional contra la leucemia. En la actualidad, la terapia génica para tratar SCID continúa siendo exitosa en Estados Unidos, Gran Bretaña, Italia y Japón.

 Los adenovirus presentan un genoma de ADN bicatenario, y no integran su genoma cuando infectan a la célula huésped, sino que la molécula de ADN permanece libre en el núcleo celular y se transcribe de forma independiente. Esto supone que el efecto posicional o la mutagénesis por inserción no se dan en estos vectores, lo cual no quiere decir que no tengan otros inconvenientes. Además, debido al hecho de que en su ciclo natural se introducen en el núcleo de la célula, pueden infectar tanto células en división como células quiescentes.

A los vectores de primera generación se les eliminó parte del gen E1, básica para la replicación, y a los de 2.ª, se les eliminaron otros genes tempranos en el ciclo del virus. En ambos casos, cuando se realiza una infección con una concentración elevada de virus, se produce la expresión de otros genes que provocan una respuesta inmune considerable.

Por ello, los últimos vectores basados en adenovirus prácticamente han sido desprovistos de la mayor parte de sus genes, con la excepción de las regiones ITR (regiones repetidas de forma invertida), y la zona necesaria para la encapsidación.

Los AAV son virus pequeños con un genoma de ADN monocatenario. Pueden integrarse específicamente en el cromosoma 19 con una alta probabilidad. Sin embargo, el VAA recombinante que se usa como vector y que no contiene ningún gen viral, solo el gen terapéutico, no se integra en el genoma. En su lugar, el genoma vírico recombinante fusiona sus extremos a través del ITR (repeticiones terminales invertidas), apareciendo recombinación de la forma circular y episomal que se predice que pueden ser la causa de la expresión génica a largo plazo.

Las desventajas de los sistemas basados en AAV radican principalmente en la limitación del tamaño de DNA recombinante que podemos usar, que es muy poco, dado el tamaño del virus. También el proceso de producción e infección resultan bastante complejos. No obstante, como se trata de un virus no patógeno en la mayoría de los pacientes tratados no aparecen respuestas inmunes para eliminar el virus ni las células con las que han sido tratados.

Muchos ensayos con VAA están en curso o en preparación, principalmente en el tratamiento de músculos y enfermedades oculares, los dos tejidos donde el virus parece ser particularmente útil. Sin embargo, se están comenzando a realizar pruebas clínicas, donde vectores basados en el VAA son utilizados para introducir los genes en el cerebro. Esto es posible porque VAA pueden infectar células que no están en estado de división, tales como las neuronas.

Los herpesvirus son virus de ADN capaces de establecer latencia en sus células huésped. Son complejos genéticamente hablando, pero para su uso como vectores tienen la ventaja de poder incorporar fragmentos de DNA exógeno de gran tamaño (hasta unas 30 kb). Además, aunque su ciclo lítico lo realizan en el lugar de infección, establecen la latencia en neuronas, las cuales están implicadas en numerosas enfermedades del sistema nervioso, y son por ello dianas de gran interés.

Los vectores herpesvíricos puestos en marcha han usado dos estrategias principales:



No obstante, el uso de vectores basados en el HSV (virus del herpes simple), solo puede llevarse a cabo en pacientes que no hayan sido infectados previamente por él, pues pueden presentar inmunidad.

Los vectores virales descritos anteriormente tienen poblaciones naturales de células huésped que ellos infectan de manera eficiente. Sin embargo, algunos tipos celulares no son sensibles a la infección por estos virus.

La entrada del virus a la célula está mediada por proteínas de su superficie externa (que pueden formar parte de una cápside o de una membrana). Estas proteínas interaccionan con receptores celulares que pueden inducir cambios estructurales en el virus y contribuir a su entrada en la célula por endocitosis.

En cualquier caso, la entrada en las células huésped requiere una interacción favorable entre una proteína de la superficie del virus, y una proteína de la superficie de la célula. Según la finalidad de una determinada terapia génica, se podría limitar o expandir el rango de células susceptibles a la infección por un vector. Por ello, se han desarrollado vectores conocidos como "pseudotyped", en los cuales la cubierta vírica de proteínas silvestre ha sido remplazada por péptidos de otros virus, o por proteínas quiméricas, que constan de las partes de la proteína vírica necesarias para su incorporación en el virión, así como las secuencias que supuestamente a interaccionar con receptores específicos de proteínas celulares.

Por ejemplo, el vector retrovírico más popular para el uso en pruebas de terapia génica ha sido el virus de la inmunodeficiencia en simios revestido con la cubierta de proteínas G del virus de la estomatitis vesicular. Este vector se conoce como VSV y puede infectar a casi todas las células, gracias a la proteína G con la cual este vector es revestido.

Se ha intentado en numerosas ocasiones limitar el tropismo (capacidad de infectar a muchas células) de los vectores virales. Este avance podría permitir la administración sistemática de una cantidad relativamente pequeña del vector. La mayoría de los intentos han utilizado proteínas quiméricas para la envuelta, las cuales incluían fragmentos de anticuerpos.

Estos métodos presentan ciertas ventajas sobre los métodos virales, tales como facilidades de producción a gran escala y baja inmunogenicidad. Anteriormente, los bajos niveles de transfección y expresión del gen mantenían a los métodos no virales en una situación menos ventajosa; sin embargo, los recientes avances en la tecnología de vectores han producido moléculas y técnicas de transfección con eficiencias similares a las de los virus.

Éste es el método más simple de la transfección no viral. Consiste en la aplicación localizada de, por ejemplo, un plásmido con ADN desnudo. Varios de estos ensayos dieron resultados exitosos. Sin embargo, la expresión ha sido muy baja en comparación con otros métodos de transformación. Además de los ensayos con plásmidos, se han realizado ensayos con productos de PCR, y se ha obtenido un éxito similar o superior. Este logro, sin embargo, no supera a otros métodos, lo que ha llevado a una investigación con métodos más eficientes de transformación, tales como la electroporación, la sonicación, o el uso de la biobalística, que consiste en disparar partículas de oro recubiertas de ADN hacia las células utilizando altas presiones de gas.

El uso de oligonucleótidos sintéticos en la terapia génica tiene como objetivo la inactivación de los genes implicados en el proceso de la enfermedad.

Existen varias estrategias para el tratamiento con oligonucleótidos

Una estrategia, la terapia "antisentido" utiliza oligonucleótidos con la secuencia complementaria al RNAm del gen diana, lo que activa un mecanismo de silenciamiento génico. También se puede usar para alterar la transcripción del gen defectuoso, modificando por ejemplo su patrón de edición de intrones y exones.

También se hace uso de moléculas pequeñas de RNAi para activar un mecanismo de silenciamiento génico similar al de la terapia antisentido

Otra posibilidad es utilizar oligodesoxinucleótidos como un señuelo para los factores que se requieren en la activación de la transcripción de los genes diana. Los factores de transcripción se unen a los señuelos en lugar de al promotor del gen defectuoso, lo que reduce expresión de los genes diana.
Además, oligonucleótidos de ADN monocatenario han sido utilizados para dirigir el cambio de una única base dentro de la secuencia de un gen mutante.

Al igual que los métodos de ADN desnudo, requieren de técnicas de transformación para introducirse en la célula.

La creación de cromosomas humanos artificiales (HACs) estables es una de las posibilidades que se baraja en la actualidad como una de las formas de introducir ADN permanentemente en células somáticas para el tratamiento de enfermedades mediante el uso de la terapia génica. Presentan una elevada estabilidad, además de permitir introducir grandes cantidades de información genética.

El vector de ADN puede ser cubierto por lípidos formando una estructura organizada, como una micela o un liposoma. Cuando la estructura organizada forma un complejo con el ADN entonces se denomina lipoplexe.

Hay tres tipos de lípidos: aniónicos, neutros, o catiónicos. Inicialmente, lípidos aniónicos y neutros eran utilizados en la construcción de lipoplexes para vectores sintéticos. Sin embargo, estos son relativamente tóxicos, incompatibles con fluidos corporales y presentan la posibilidad de adaptarse a permanecer en un tejido específico. Además, son complejos y requieren tiempo para producirlos, por lo que la atención se dirigió a las versiones catiónicas. Éstos, debido a su carga positiva, interaccionan con el ADN, que presenta carga negativa, de tal forma que facilita la encapsulación del ADN en liposomas. Más tarde, se constató que el uso de lípidos catiónicos mejoraba la estabilidad de los lipoplexes. Además, como resultado de su carga, los liposomas catiónicos interactúan también con la membrana celular, y se cree que la endocitosis es la principal vía por la que las células absorben los lipoplexes.

Los endosomas se forman como resultado de la endocitosis. Sin embargo, si los genes no pueden liberarse al citoplasma por rotura de la membrana del endosoma, los liposomas y el ADN contenido serán destruidos. La eficiencia de ese "escape endosomal" en el caso de liposomas constituidos solo por lípidos catiónicos es baja. Sin embargo, cuando “lípidos de ayuda” (normalmente lípidos electroneutrales, tales como DOPE) son añadidos, la eficacia es bastante mayor. Además, ciertos lípidos (lípidos fusogénicos) tienen la capacidad de desestabilizar la membrana del endosoma. El uso de ciertos compuestos químicos, como la cloroquina, permite al ADN exógeno escapar del lisosoma, si bien deben usarse con precaución, ya que es tóxico y debe usarse en dosis pequeñas para no afectar a la célula diana de transfección.

No obstante, los lípidos catiónicos presentan efectos tóxicos dependientes de dosis, lo que limita la cantidad que de ellos se puede usar y por tanto la terapia en sí.

El uso más común de los lipoplexes es la transferencia de genes en células cancerosas, donde los genes suministrados activan genes supresores del tumor en la célula y disminuyen la actividad de los oncogenes.

Estudios recientes han mostrado que lipoplexes son útiles en las células epiteliales del sistema respiratorio, por lo que pueden ser utilizados para el tratamiento genético de las enfermedades respiratorias como la fibrosis quística.

Los complejos de polímeros de ADN se denominan poliplexes y la mayoría consisten en polímeros catiónicos, regulados por interacciones iónicas.

Una gran diferencia entre los métodos de acción de poliplexes y lipoplexes es que algunos poliplexes no pueden liberar su ADN cargado al citoplasma, por lo que requieren de la contransfección con agentes que contribuyan a la liss del endosoma. Existen otros elementos formadores de poliplexes, como el quitosano o la polietilamina, que si son capaces de liberarse del endosoma.

Debido a las deficiencias de muchos de los sistemas de transferencia génica se han desarrollado algunos métodos híbridos que combinan dos o más técnicas. Los virosomas son un ejemplo, y combinan liposomas con el virus inactivado VIH o el virus de la gripe.

Un dendrímero es una macromolécula muy ramificada con forma esférica o variable. Su superficie puede ser funcional de muchas formas y de ésta derivan muchas de sus propiedades. Además, su tamaño, —en la escala nano—, permite su uso en biomedicina.

En particular, es posible construir un dendrímero catiónico, es decir, con carga superficial positiva. De esta forma, interacciona con el ácido nucleico, cargado negativamente, y forma un complejo que puede entrar por endocitosis en la célula. Esto es útil en terapia génica, para introducir genes exógenos.

Los costes de producción son elevados, pero se están desarrollando técnicas que permiten abaratarlo, puesto que se trata de una técnica con una toxicidad muy baja, y su principal desventaja es a nivel productivo.

Las células diana se seleccionan en función del tipo de tejido en el que deba expresarse el gen introducido, y deben ser además células con una vida media larga, puesto que no tiene sentido transformar células que vayan a morir a los pocos días. Igualmente, se debe tener en cuenta si la diana celular es una célula en división o quiescente, porque determinados vectores virales, como los retrovirus, solo infectan a células en división.

En función de estas consideraciones, las células diana ideales serían las células madre, puesto que la inserción de un gen en ellas produciría un efecto a largo plazo. Debido a la experiencia en trasplante de médula ósea, una de las dianas celulares más trabajadas son las células madre hematopoyéticas. La terapia génica en estas células es técnicamente posible y es un tejido muy adecuado para la transferencia ex vivo. Otras dianas celulares con las que se ha trabajado son:


La terapia génica apareció a partir de la década de 1970 para intentar tratar y paliar enfermedades de carácter genético y se dieron las primeras pruebas con virus, las cuales fracasaron. Años más tarde, en la década de 1980, se intentó tratar la talasemia usando betaglobina. En este caso fue un éxito en modelos animales aunque no se pudo usar en humanos.

En 1990, W. French Anderson propone el uso de células de médula ósea tratadas con un vector retroviral que porta una copia correcta del gen que codifica para la enzima adenosina desaminasa, la cual se encuentra mutada. Es una enfermedad que forma parte del grupo de las inmunodeficiencias severas combinadas (SCID). Realizó la transformación ex-vivo con los linfocitos T del paciente, que luego se volvieron a introducir en su cuerpo. Cinco años más tarde, publicaron los resultados de la terapia, que contribuyó a que la comunidad científica y la sociedad consideraran las posibilidades de esta técnica.

No obstante, el apoyo a la terapia fue cuestionado cuando algunos niños tratados para SCID desarrollaron leucemia. Las pruebas clínicas se interrumpieron temporalmente en el 2002, a causa del impacto que supuso el caso de Jesse Gelsinger, la primera persona reconocida públicamente como fallecida a causa de la terapia génica. Su muerte se debió al uso del vector adenoviral para la transducción del gen necesario para tratar su enfermedad, lo cual causó una excesiva respuesta inmune, con un fallo multiorgánico y muerte cerebral. Existe una bibliografía numerosa sobre el tema, y es destacable el informe que la FDA emitió señalando el conflicto de intereses de algunos de los médicos implicados en el caso así como los fallos en el procedimiento. En el año 2002, cuatro ensayos en marcha de terapia génica se paralizaron al desarrollarse en un niño tratado una enfermedad similar a la leucemia. Posteriormente, tras una revisión de los procedimientos, se reanudaron los proyectos en marcha.

Un equipo de investigadores de la Universidad de California, en Los Ángeles, insertó genes en un cerebro utilizando liposomas recubiertos de un polímero llamado polietilenglicol (PEG). La transferencia de genes en este órgano es un logro significativo porque los vectores virales son demasiado grandes para cruzar la barrera hematoencefálica. Este método tiene el potencial para el tratamiento de la enfermedad del Parkinson.

También en ese año se planteó la interferencia por ARN para tratar la enfermedad de Huntington.

Científicos del NIH tratan exitosamente un melanoma metastásico en dos pacientes, utilizando células T para atacar a las células cancerosas. Este estudio constituye la primera demostración de que la terapia génica puede ser efectivamente un tratamiento contra el cáncer.

En marzo de 2006, un grupo internacional de científicos anunció el uso exitoso de la terapia génica para el tratamiento de dos pacientes adultos contagiados por una enfermedad que afecta a las células mieloides. El estudio, publicado en Nature Medicine, es pionero en mostrar que la terapia génica puede curar enfermedades del sistema mieloide.

En mayo de 2006, un equipo de científicos dirigidos por el Dr. Luigi Naldini y el Dr. Brian Brown del Instituto de San Raffaele Telethon para la Terapia Génica (HSR-TIGET) en Milán, informaron del desarrollo de una forma de prevenir que el sistema inmune pueda rechazar la entrada de genes. Los investigadores del Dr. Naldini observaron que se podía utilizar la función natural de los microRNA para desactivar selectivamente los genes terapéuticos en las células del sistema inmunológico. Este trabajo tiene implicaciones importantes para el tratamiento de la hemofilia y otras enfermedades genéticas.

En noviembre del mismo año, Preston Nix de la Universidad de Pensilvania informó sobre VRX496, una inmunoterapia para el tratamiento del HIV que utiliza un vector lentiviral para transportar un DNA antisentido contra la envuelta del HIV. Fue la primera terapia con un vector lentiviral aprobada por la FDA para ensayos clínicos. Los datos de la fase I/II ya están disponibles.

El 1 de mayo de 2007, el hospital Moorfields Eye y la universidad College London´s Institute of Ophthalmology, un año después el Hospital de Niños de Filadelfia anunciaron el primer ensayo de terapia génica para la enfermedad hereditaria de retina. La primera operación (en Inglaterra) se llevó a cabo en un varón británico de 23 años de edad, Robert Johnson, a principios de este año. Mientras que en Filadelfia Corey Haas fue el primer niño en obtener este tipo de terapéutica. La Amaurosis congénita de Leber es una enfermedad hereditaria que causa la ceguera por mutaciones en el gen RPE65. Los resultados de la Moorfields/UCL se publicaron en New England Journal of Medicine. Se investigó la transfección subretiniana por el virus recombinante adeno-asociado llevando el gen RPE65, y se encontraron resultados positivos. Los pacientes mostraron incremento de la visión, y no se presentaron efectos secundarios aparentes. Los ensayos clínicos de esta terapia se encuentran en fase II.

Una de las etapas a realizar es la determinación del tipo molecular que atiñe cada enfermedad (o http://es.wikipedia.org/wiki/Distrofias_de_la_retina). A nivel retiniano se realiza en América Latina por la corporación Virtual Eye Care MD, conocida y de renombre por su calidad a realizar este trabajo de relación entre fenotipo y genotipo. www.virtualeyecaremd.com

Investigadores de la Universidad de Míchigan en Ann Arbor (Estados Unidos) desarrollaron una terapia genética que ralentiza y recupera las encías ante el avance de la enfermedad periodontal, la principal causa de pérdida de dientes en adultos. Los investigadores descubrieron una forma de ayudar a ciertas células utilizando un virus inactivado para producir más cantidad de una proteína denominada receptor TNF. Este factor se encuentra en bajas cantidades en los pacientes con periodontitis. La proteína administrada permite disminuir los niveles excesivos de TNF, un compuesto que empeora la destrucción ósea inflamatoria en pacientes que sufren de artritis, deterioro articular y periodontitis. Los resultados del trabajo mostraron que entre el 60 y el 80 por ciento de los tejidos periodontales se libraban de la destrucción al utilizar la terapia génica.

En septiembre de 2009, se publicó en Nature que unos investigadores de la Universidad de Washington y la Universidad de Florida fueron capaces de proporcionar visión tricromática a monos ardilla usando terapia génica.

En noviembre de ese mismo año, la revista Science publicó resultados alentadores sobre el uso de terapia génica en una enfermedad muy grave del cerebro, la adrenoleucodistrofia, usando un vector retroviral para el tratamiento.

El 2 de noviembre la Comisión Europea autorizó a Glybera, una empresa alemana(Ámsterdam), a lanzar un tratamiento para un extraño desorden genético —la deficiencia de lipoproteinlipasa(LPL)—.

Son numerosas las enfermedades objeto de la terapia génica, siendo las más características las tratadas a continuación:

El primer protocolo clínico aprobado por la FDA para el uso de la terapia génica fue el utilizado en el tratamiento de la deficiencia en adenosín deaminasa (ADA) que provoca un trastorno de la inmunidad, en 1990. En estos pacientes no se ha podido retirar el tratamiento enzimático exógeno necesario para su supervivencia, sino solo disminuirlo a la mitad y se ha detectado la persistencia en la expresión del gen aun después de cuatro años de iniciado el protocolo. Aunque no se haya logrado la completa curación de los pacientes (que consistiría en retirar todo el aporte enzimático exógeno) este constituye un hecho inédito en la historia terapéutica.
En 2009 se hace un nuevo experimento en el que extraen células hematopoyéticas de la médula ósea para la introducción del gen ADA ex vivo mediante un retrovirus modificado (GIADA). Las células modificadas se vuelven a introducir en el paciente. Los resultados de este experimento fueron exitosos porque ninguno de los pacientes desarrollo leucemia (como si había ocurrido con el empleo de retrovirus). Además, todos los pacientes desarrollaron una expresión correcta del gen ADA durante los años de seguimiento que se les hizo y consiguieron un aumento de células sanguíneas. De esta manera 8 de los nueve pacientes no necesita tratamiento enzimático exógeno para complementar la terapia génica.

El tratamiento del cáncer hasta el momento ha implicado la destrucción de las células cancerosas con agentes quimioterapéuticos, radiación o cirugía. Sin embargo, la terapia génica es otra estrategia que en algunos casos ha logrado que el tamaño de tumores sólidos disminuya en un porcentaje significativo. Los principales métodos que utiliza la terapia génica en el cáncer son:

El síndrome de Wiskott-Aldrich (WAS) es una enfermedad recesiva ligada al cromosoma X caracterizada por eczema, trombocitopenia, infecciones recurrentes, inmunodeficiencia así como una gran tendencia a los linfomas y a las enfermedades autoinmunes. También hay una versión más suave de esta enfermedad conocida como trombocitopenia ligada al cromosoma X o XLT caracterizada por microtrombocitopenia congénita con plaquetas de pequeño tamaño. Ambas enfermedades están producidas por mutaciones en el gen WAS que codifica para una proteína multidominio que solo se expresa en células hematopoyéticas, WASP. Por lo tanto, la mayoría de los que padecen este síndrome sufren una muerte prematura debido a una infección, hemorragia, cáncer o anemia grave autoinmune. Actualmente, se han realizado tratamientos eficaces en pacientes con el síndrome de Wiskott-Aldrich por medio de trasplantes de médula ósea o sangre del cordón umbilical de un donante HLA idéntico o compatible.

En 2010 se publica un estudio que muestra importante mejoras en dos niños diagnosticados con la enfermedad. La terapia consistió en extraer las células madre hematopoyéticas y volvérselas a trasferir tras integrarles el gen WAS en el genoma. Tras la terapia génica, detectaron niveles significativos de la proteína WASP en las diferentes células del sistema inmune de los pacientes. El resultado fue que los pacientes tuvieron varias mejoras significativas: uno de ellos se recuperó por completo de la anemia autoinmune y el otro paciente redujo el eczema que sufría.

La β-talasemia constituye un desorden genético con mutaciones en el gen de la β-globulina que reduce o bloquea la producción de esta proteína. Los pacientes con esta enfermedad padecen anemia severa y requieren trasfusiones de sangre a lo largo de toda su vida. La terapia génica tiene como objetivo sanar las células madre de la médula ósea mediante la transferencia de la β-globina normal o gen de β-globina en células madre hematopoyéticas (CMH) para producir de forma permanente los glóbulos rojos normales. Para llevarlo a cabo se pretende emplear lentivirus porque varios estudios muestran la corrección de la β-talasemia en modelos animales. Los objetivos de la terapia génica con esta enfermedad son: optimizar la transferencia de genes, la introducción de una gran cantidad de CMH modificadas genéticamente y reducir al mínimo las consecuencias negativas que pueden derivarse de la integración al azar de los vectores en el genoma.

Un concepto muy importante del que radican algunos aspectos de la seguridad de la terapia génica es el de la barrera Weismann. Se refiere al hecho de que la información hereditaria solo va de células germinales a células somáticas, y no al revés.

La terapia génica en células germinales es mucho más controvertida que en células somáticas, pero aun así, si la barrera Weismann fuera permeable a algún intercambio de información, como algunos autores señalan, incluso la terapia en células somáticas podría tener problemas éticos y de seguridad que antes no habrían sido considerados. Este tipo de aspectos a tener en cuenta se recogen en la Declaración Universal sobre el Genoma Humano.

La naturaleza de la propia terapia génica y sus vectores, implica que en muchas ocasiones los pacientes deben repetir la terapia cada cierto tiempo porque ésta no es estable y su expresión es temporal.

La respuesta inmune del organismo ante un agente extraño como un virus o una secuencia de ADN exógena. Además, esta respuesta se refuerza en las sucesivas aplicaciones de un mismo agente.

Problemas relacionados los vectores virales. Podrían contaminarse tanto por sustancias químicas como por virus con capacidad de generar la enfermedad. Implican también riesgos de respuesta inmune.

Trastornos multigénicos: representan un reto muy grande para este tipo de terapia, ya que se trata de enfermedades cuyo origen reside en mutaciones en varios genes, y aplicar el tratamiento se encontraría con las dificultades clásicas de la terapia multiplicadas por el número de genes a tratar.

Posibilidad de inducir un tumor por mutagénesis. Esto puede ocurrir si el ADN se integra por ejemplo en un gen supresor tumoral. Se ha dado este caso en los ensayos clínicos para SCID ligada al cromosoma X, en los cuales 3 de 20 pacientes desarrollaron leucemia.

El primer ejemplo de terapia génica en mamíferos fue la corrección de la deficiencia en la producción de la hormona del crecimiento en ratones. La mutación recesiva little (lit) produce ratones enanos. A pesar de que estos presentan un gen de la hormona del crecimiento aparentemente normal, no producen mARN a partir de este gen.

El primer paso en la corrección del defecto consistió en la inyección de cinco mil copias de un fragmento de ADN lineal portador de la región estructural del gen de la hormona del crecimiento de la rata fusionado al promotor del gen de la metalotioneína de ratón, en huevos lit. La función normal de la metalotioneína es la destoxificación de los metales pesados, por lo que la región reguladora responde a la presencia de metales pesados en el animal. Los huevos inyectados fueron implantados en hembras. El 1% de los ratones de la descendencia resultaron ser transgénicos, y alcanzaron mayor tamaño.

Se ha creado una tecnología similar para generar variedades transgénicas de salmón del Pacífico con una tasa rápida de crecimiento y, los resultados han sido espectaculares. Se microinyectó en huevos de salmón un plásmido portador del gen de la hormona del crecimiento regulado por el promotor de la metalotioneína y una pequeña porción de peces resultantes fueron transgénicos, pesando once veces más que los no transgénicos.

En series de televisión como "Dark Angel", el tema de la terapia génica se menciona como una de las prácticas realizadas en niños transgénicos y sus madres. También en la serie "Alias", aparece la terapia génica molecular como explicación a dos individuos idénticos.

Es un elemento fundamental en la trama de videojuegos como "Bioshock" o "Metal Gear Solid", y desempeña un papel importante en la trama de películas como "Die Another Day", de James Bond o "Soy leyenda" de Will Smith, "The Bourne Legacy", entre otras muchas.






</doc>
<doc id="8932" url="https://es.wikipedia.org/wiki?curid=8932" title="Enlace peptídico">
Enlace peptídico

El enlace peptídico es un enlace entre el grupo amino (–NH) de un aminoácido (AA) y el grupo carboxilo (–COOH) de otro aminoácido. Los péptidos y las proteínas están formados por la unión de aminoácidos mediante enlaces peptídicos. El enlace peptídico implica la formación de un enlace CO-NH y la deshidratación o pérdida de una molécula de agua (HO), al perder el grupo carboxilo un hidrógeno y un oxígeno y el grupo amino un hidrógeno. Es, en realidad, un enlace amida sustituido. La formación de este enlace requiere aportar energía, mientras que su rotura (hidrólisis) la libera.

Podemos seguir añadiendo aminoácidos al péptido, pero siempre en el extremo COOH terminal (en la Figura 1, abajo a la derecha).

Para nombrar el péptido se empieza por el NH terminal por acuerdo. Si el primer aminoácido de nuestro péptido fuera alanina y el segundo serina tendríamos el péptido alanil-serina.

En las décadas de 1940 y 1950, estudios basados en la difracción de rayos X sobre muestras de cristales de aminoácidos, dipéptidos y tripéptidos realizados por Linus Pauling y Robert Corey ayudaron a comprender la estructura del enlace peptídico, observándose que:

Esta ordenación planar rígida es el resultado de la estabilización por resonancia del enlace peptídico. Por ello, el armazón de un péptido está constituido por la serie de planos sucesivos separados por grupos metileno sustituidos en los que sí puede haber giro. Pero no todos los giros son posibles, lo que impone restricciones importantes al número posible de conformaciones que puede adoptar una proteína.

Si denominamos "Φ" al valor del ángulo que puede adoptar el enlace N-C, y "Ψ" al del enlace C-C, solo existirán unos valores permitidos para Φ y Ψ (ver gráfico de Ramachandran); y dependerá en gran medida del tamaño y características de los grupos R sucesivos.

El enlace peptídico puede romperse por hidrólisis (añadiendo agua). En su presencia se romperá liberando 8–16 kilojulios/mol (2–4 kcal/mol) de energía libre. En la naturaleza este proceso es extremadamente lento (más de 1000 años), pero hay formas de acelerarlo:





</doc>
<doc id="8934" url="https://es.wikipedia.org/wiki?curid=8934" title="Cariotipo">
Cariotipo

El cariotipo (diferente de un idiograma), es el patrón cromosómico de una especie expresado a través de un código, establecido por convenio, que describe las características de sus cromosomas. 
Debido a que en el ámbito de la clínica suelen ir ligados, el concepto de cariotipo se usa con frecuencia para referirse a un cariograma, el cual es un esquema, foto o dibujo de los cromosomas de una célula metafásica ordenados de acuerdo a su morfología (metacéntricos, submetacéntricos, telocéntricos, subtelocéntricos y acrocéntricos) y tamaño, que están caracterizados y representan a todos los individuos de una especie.
El cariotipo es característico de cada especie, al igual que el número de cromosomas; el ser humano tiene 46 cromosomas (23 pares porque somos diploides o 2n) en el núcleo de cada célula, organizados en 22 pares autosómicos y 1 par sexual (hombre XY y mujer XX). Cada brazo ha sido dividido en zonas y cada zona, a su vez, en bandas e incluso las bandas en sub-bandas, gracias a las técnicas de marcado. No obstante, puede darse el caso, en humanos, de que existan otros patrones en los cariotipos, a lo cual se le conoce como aberración cromosómica.

Los cromosomas se clasifican en 7 grupos, de la A a la G, atendiendo a su longitud relativa y a la posición del centrómero, que define su morfología. De esta manera, el cariotipo humano queda formado así: 


Mediante el cariotipo se pueden analizar anomalías numéricas y estructurales, cosa que sería muy difícil de observar mediante genética mendeliana.

Ante todo, deben guardarse las máximas condiciones de esterilidad. Además, debe cumplirse lo siguiente:


El estudio de los cariotipos es posible debido a la tinción. Usualmente un colorante adecuado es aplicado después de que las células hayan sido detenidas durante la división celular mediante una solución de colchicina.
Para humanos los glóbulos blancos son los usados más frecuentemente porque son fácilmente inducidos a crecer y dividirse en cultivo de tejidos. 

Algunas veces las observaciones pueden ser realizadas cuando las células no se están dividiendo 
(interfase). El sexo de un neonato feto puede ser determinado por observación de células en la interfase (ver punción amniótica y corpúsculo de Barr).

La mayoría de (pero no todas) las especies tienen un cariotipo estándar. El ser humano normalmente tiene 22 pares de cromosomas autosómicos y un par de cromosomas sexuales. El cariotipo normal para la mujer contiene dos cromosoma X denominado 46 XX, y el varón un cromosoma X y uno Y, denominado 46 XY. Cualquier variación de este cariotipo estándar puede llevar a anormalidades en el desarrollo.

En los laboratorios de Citogenética se utilizan varias técnicas de bandeo cromosómico. En este sentido, destaca el método de tinción de las bandas de quinacrina (bandas Q). Fue el primero en emplearse, requiere un microscopio de fluorescencia, aunque su uso ya no está tan extendido como el de las bandas de giemsa (bandas G). Para producir estas bandas G se aplica una tinción de Giemsa tras digerir parcialmente las proteínas cromosómicas con tripsina. Las bandas reversas (bandas R) requieren tratamiento por calor y en ellas se invierte el patrón normal blanco y negro que se observa en las bandas Q y G. Este método destaca por su gran utilidad en la tinción de los extremos distales de los cromosomas. Existen otras técnicas de tinción como las bandas C y las NOR (región de organizadores nucleolares), tiñendo estos últimos específicamente ciertas regiones del cromosoma. Así, las bandas C tiñen la heterocromatina constitutiva, que se localiza normalmente cerca de los centrómeros, y la tinción NOR marca los satélites y tallos de los cromosomas acrocéntricos.

Las bandas de alta resolución suponen la tinción de los cromosomas en profase o metafase precoz (prometafase) antes de alcanzar la condensación máxima. Los cromosomas en profase y prometafase están más elongados que los cromosomas en metafase; por este motivo, el número de bandas observadas, para el conjunto de cromosomas, aumenta desde 300-450 hasta casi 800. Ello permite detectar anomalías menos claras, que con las bandas convencionales no suelen apreciarse.
Para obtener este tipo de bandas se necesita añadir otro requisito para la realización del cariotipo. Se trata de un componente utilizado en quimioterapia, el metotrexato que junto con la colchicina se añade antes de realizar la tinción.

Tanto el paso de adición de mitógenos como la adición de colchicina son los pasos críticos para el estudio del cariotipo.
Es necesario realizar un recuento de, al menos, 12-25 células en metafase. Esto es debido a que si por ejemplo, al contar un núcleo le falta el cromosoma 21, puede ser que sea mosaico y que el resto de células sí presenten ese cromosoma. Otra opción, y más probable, es que sea un efecto de la adición del mitógeno, ya que este compuesto altera el proceso normal de división celular, favoreciendo las aneuploidías. Una última opción podría ser que los cromosomas se solapen y al proceder al analizar el cariotipo solo se cuente un cromosoma cuando realmente hay dos. Por todas estas razones se deben contar un mínimo de 12-15 células en metafase que se encuentren bastante separados en el porta.

En el cariotipo clásico se suele utilizar una solución de Giemsa como tinción (específica para los grupos fosfato del ADN) para colorear las bandas de los cromosomas (Bandas-G), menos frecuente es el uso del colorante Quinacridina (se une a las regiones ricas en Adenosina-Timina). Cada cromosoma tiene un patrón característico de banda que ayuda a identificarla.

Los cromosomas se organizan de forma que el brazo corto de este quede orientado hacia la parte superior y el brazo largo hacia la parte inferior.

Algunos cariotipos nombran a los brazos cortos p y a los largos q. Además, las diferentes regiones y subregiones teñidas reciben designaciones numéricas según la posición a la que se encuentren respecto a estos brazos cromosómicos.

Por ejemplo, el síndrome de Cri du Chat implica una deleción en el brazo corto del cromosoma 5. Está escrito como 46, XX, 5p-. La región crítica para este síndrome es la deleción de 15.2, la cual es escrita como 46,XX, del(5)(p15.2)

El análisis espectral de los cariotipos (o SKY) se trata de una tecnología de citogenética molecular que permite el estudio y visualización de los 23 pares de cromosomas en forma simultánea.

Sondas marcadas fluorescentemente son hechas para cada cromosoma al marcar DNA específico de cada cromosoma
con diferentes fluoróforos. Debido a que hay un limitado número de fluoróforos espectralmente distintos, un
método de etiquetado combinatorio es usado para generar muchos colores diferentes.

La diferencias espectrales generadas por el etiquetado combinatorio son capturadas y analizadas usando un interferómetro agregado a un microscopio de fluorescencia.

El programa de procesamiento de imágenes entonces asigna un pseudocolor a cada combinación espectralmente diferente, permitiendo la visualización de cromosomas coloreados.

Esta técnica es usada para identificar aberraciones estructurales cromosómicas en células cancerígenas y otras patologías cuando el bandeo con Giemsa u otras técnicas no son lo suficientemente precisas.

Este tipo de técnicas mejorará la identificación y diagnóstico de las aberraciones cromosómicas en citogenética prenatal así como en células cancerosas.

El cariotipo digital es una técnica utilizada para cuantificar el número de copias de ADN en una escala genómica. Se trata de secuencias de locus de ADN específicos de todo el genoma que son aisladas y enumeradas.

Este método es también conocido como cariotipo virtual.


La variación de estos cromosomas es encontrada frecuentemente:


Levitsky fue el primero en dar una definición a cariotipo como el aspecto fenotípico de los cromosomas somáticos, en contraste con su contenido de genes. Este concepto siguió siendo estudiado con los trabajos de Darlington y White. 
La investigación y el interés por el estudio del cariotipo hizo que se planteara una pregunta : ¿cuántos son los cromosomas que contiene una célula diploide humana?

En 1912, Hans von Winiwarter demostró que el hombre tenía 47 cromosomas en espermatogonia y 48 en oogonia, concluyendo un mecanismo de determinación sexual XX/XO. Años después, en 1922 von Winiwarter no estaba seguro si el número cromosómico del hombre era 46 o 48. Para ello se necesitó un estudio más profundo para poder responder a esta pregunta.

Esto tomó hasta mediados de los años 1950 que fue cuando se dio como generalmente aceptado que el cariotipo de hombre incluye solo 46 cromosomas. En los grandes monos el cariotipo es de 48 cromosomas por lo que se explicó que el cromosoma 2 de los humanos fue formado por una fusión de cromosomas hereditarios, reduciendo así el número de estos.

Aunque la replicación del ADN y la transcripción del ADN están altamente estandarizadas en eucariotas, no puede decirse lo mismo de sus cariotipos, ya que son sumamente variables entre especies en el número de cromosomas y en la organización detallada a pesar de haber sido construidos con las mismas macromoléculas.

Esta variación proporciona la base para una gama de estudios que podría llamarse citología evolutiva.

En algunos casos incluso hay significantes variaciones dentro de las especies. En una revisión del 2000 
Godfrey y Masters concluyen: "En nuestra visión, es poco probable que un proceso o el otro, puedan independientemente contar para el amplio rango de estructuras de cariotipo que son observadas.. Pero usadas en conjunto con otros datos filogenéticos, el fisionamiento cariotípico puede ayudar a explicar dramáticas diferencias en los números diploides entre especies estrechamente relacionadas, que antes fueron inexplicables.

A lo largo del tiempo, algunos de los organismos fueron eliminando la presencia de algunos componentes de su núcleo, así como la heterocromatina.


En "Ascaris suum", todos los precursores de células somáticas experimentan disminución de la cromatina.
Hay veces que se dan casos donde algunos cromosomas son anormales por lo que resulta un trastorno para el nuevo descendiente.

Un ejemplo de la variabilidad entre especies estrechamente relacionadas es el del muntjac (un mamífero de la familia de los cérvidos que vive en la India y el sudeste asiático), que fue investigado por Kurt Benirschke y su compañera Doris Wurster donde demostraron que el número diploide del muntjac Chino (Muntiacus reevesi) resultó ser de 46 y todos telocéntricos.
Cuando se estudió el cariotipo del muntjac Indio (Muntiacus muntjak) vieron que la hembra tenía 6 y el macho 7 cromosomas. 

El número de cromosomas en el cariotipo entre especies no relacionadas es enormemente variable. 

El récord más bajo le pertenece al nematodo "Parascaris univalens", donde el número haploide es n = 1; el récord más alto podría estar en algún lugar entre los helechos, con el helecho Lengua de Adder
"Ophioglossum" adelante con un promedio de 1262 cromosomas.

El récord más alto para animales podría estar entre el esturión de nariz corta "Acipenser brevirostrum" con 372 cromosomas.

La existencia de cromosomas supernumerarios o B significa que el número de cromosomas puede variar incluso dentro de una misma población. (El cromosoma supernumerario se sitúa en el lugar del cromosoma normal 21. La fórmula de este triple cromosoma puede ser XXY o XYY).

La poliploidía (más de dos conjuntos de cromosomas homólogos en las células) se produce principalmente en las plantas. Ha sido de gran importancia en la evolución de estas según Stebbins. 
La proporción de las plantas con flores poliploides es de 30-35% y en el caso de las gramíneas un valor mucho más elevado, alrededor del 70%. 

La poliploidía en plantas inferiores (helechos y psilotales) también es común. Algunas especies de helechos han alcanzado niveles de poliploidía muy por encima de los niveles más altos conocidos en plantas con flores.
La poliploidía en animales es mucho menos común, alcanzando importancia en algunos grupos. En humanos se han registrado casos de embriones y fetos triploides (69, XXX) e incluso tetraploides (92, XXXX)que con un gran porcentaje acababan en aborto natural; en el caso poco frecuente de neonatos con dicha carga cromosómica, sus esperanzas de vida no superaban los pocos días postparto debido a diversas alteraciones en todos sus órganos.

La endopoliploidía se produce cuando los tejidos adultos de las células han dejado de dividirse por mitosis, pero los núcleos contienen más cantidad de cromosomas somáticos originales.

En muchos casos, los núcleos endodiploides contienen decenas de miles de cromosomas (no pueden contarse con exactitud). Las células no siempre contienen exactamente múltiplos (potencias de dos), razón por la cual el aumento en el número de conjuntos de cromosomas causados por la reproducción no es del todo exacto.

Este proceso (sobre todo estudiado en insectos y algunas plantas superiores) puede ser una estrategia de desarrollo para aumentar la productividad de los tejidos que son muy activos en la biosíntesis.

Este fenómeno ocurre esporádicamente a través del reino eucariota desde protozoo hasta el hombre;
Este es diverso y complejo, y sirve a la diferenciación y morfogénesis de muchas formas.
Vea paleopoliploidía para la investigación de duplicación de antiguos cariotipos.

El término es principalmente usado cuando el número de cromosomas varía dentro del cruce poblacional de especies. Esto puede también ser usado dentro de un grupo de especies estrechamente relacionado.

Clásicos ejemplos en plantas son el género "Crepis", donde el número gamético (= haploide) forma las series x = 3, 4, 5, 6, y 7; y "Crocus", donde cada número desde x = 3 hasta x = 15 es representado por al menos una especie. Evidencia de varios tipos muestran que las tendencias de evolución han ido en direcciones diferentes, en diferentes grupos.

Más cerca de casa, los grandes monos tienen 24x2 cromosomas, allí donde los humanos tienen 23x2.

El cromosoma 2 humano fue formado por la mezcla de cromosomas ancestrales, reduciendo el número. La aneuploidía no es considerada normalmente -ploidía sino -somía, tal como la trisomía o monosomía.
Las aneuploidías se denominan de la siguiente manera: número de veces que se repite seguido de la palabra “somía” seguido del número de cromosoma involucrado. El origen de esta mutación puede provenir de la no disyunción en meiosis I o II.

Estas anomalías pueden ser numéricas (presencia de cromosomas adicionales) o estructurales (translocaciones, inversiones a gran escala, supresiones o duplicaciones).

Las anomalías numéricas, también conocidas como aneuploidía, hacen referencia a cambios en el número de cromosomas, que pueden dar lugar a enfermedades genéticas. La aneuploidía se puede observar frecuentemente en células cancerosas. En los animales solo son viables las monosomías y las trisomías, ya que las nulisomías son letales en individuos diploides.

Las anormalidades estructurales a menudo se derivan de errores en la recombinación homóloga. Ambos tipos de anomalías pueden ocurrir en los gametos y, por tanto, estarán presentes en todas las células del cuerpo de una persona afectada, o puede ocurrir durante la mitosis y dar lugar a mosaicos genéticos individuales que tiene normal y anormal algunas células.

Anomalías cromosómicas en humanos:

También se detectó la existencia de la trisomía 8, 9 y 16, aunque por lo general no sobreviven después de nacer. No se han registrado casos en humanos de trisomías en el cromosoma 1, ya que todas acaban en aborto natural y no llegan a nacer.

Hay algunos trastornos que se derivan de la pérdida de un solo trozo de cromosoma, entre ellas:

Estas anomalías cromosómicas también pueden ocurrir en células cancerosas de un individuo genéticamente normales.
Un ejemplo bien documentado es el de Cromosoma Filadelfia o la llamada translocación Filadelfia que es una anormalidad genética asociada a la leucemia mieloide crónica (LMC).

Esta anormalidad afecta a los cromosomas 9 y 22. El 95 por ciento de los enfermos de leucemia mieloide crónica presenta esta anormalidad, mientras el resto de los enfermos padecen translocaciones crípticas invisibles a las preparaciones mediante el método de banda G u otras translocaciones que afectan a otro u otros cromosomas de la misma forma que sucede con los cromosomas 9 y 22.

Partes de dos cromosomas, el 9 y el 22, intercambian sus posiciones. El resultado es que parte del gen de región de fractura (BCR, Breakpoint Cluster Region, en inglés) del cromosoma 22 (región q11) se fusiona con parte del gen ABL del cromosoma 9 (región q34). El gen ABL toma su nombre de «Abelson», el nombre de un virus causante de leucemias precursor de una proteína similar a la que produce este gen.

Desde de 1995 se emplean distintos símbolos para describir la anomalía que sufre un cromosoma en concreto o un cariotipo, siguiendo las reglas que impone el ISCN (siglas inglesas procedentes de Sistema Internacional de Nomenclatura para Citogenética Humana). Es decir, la fórmula cromosómica refleja la descripción simplificada de un cariotipo. En la fórmula cromosómica se registra el número total de cromosomas (incluidos los sexuales) seguido de una coma, tras la cual se escriben los cromosomas sexuales. Si existen aberraciones numéricas o estructurales de los autosomas, éstas se escriben a continuación, tras otra coma. Cuando hay un mosaico, es decir, coexisten dos o más poblaciones celulares diferentes, los cariotipos correspondientes a cada una se escriben separados por una barra; primero se escribe el que tiene menor número de cromosomas y luego sucesivamente los de mayor número. Algunos de los símbolos y abreviaturas usados para describir los cariotipos son:





</doc>
<doc id="8935" url="https://es.wikipedia.org/wiki?curid=8935" title="Ortocentro">
Ortocentro

Se denomina ortocentro al punto donde se cortan las tres rectas que contienen a las tres alturas de un triángulo.

El nombre deriva del término griego "orto", que quiere decir recto, en referencia al ángulo formado entre las bases y las alturas.

El ortocentro se encuentra en el interior del triángulo si este es acutángulo; coincide con el vértice del ángulo recto si es rectángulo, y se halla en el exterior del triángulo si es obtusángulo.

Dado un triángulo cualquiera (excluyendo un triángulo rectángulo), el 'triángulo órtico o triángulo pedal respecto del dado, es el que tiene por vértices los pies de las tres alturas de este, es decir, las proyecciones de los vértices sobre los lados.







</doc>
<doc id="8936" url="https://es.wikipedia.org/wiki?curid=8936" title="Circunferencia circunscrita">
Circunferencia circunscrita

En geometría, la circunferencia circunscrita es la circunferencia que pasa por todos los vértices de un polígono y contiene completamente a dicha figura en su interior. El centro de la circunferencia circunscrita se llama circuncentro 
y su radio circunradio. 

Un polígono que tiene una circunferencia circunscrita se llama polígono cíclico. Todos los polígonos simples regulares, todos los triángulos y todos los rectángulos son cíclicos. En todo polígono cíclico, el circuncentro se halla en el punto de intersección de las mediatrices de los lados del polígono.

Los triángulos son los únicos polígonos que tienen garantizado poseer una circunferencia circunscrita, ya que son siempre polígonos cíclicos. La única excepción son algunos triángulos degenerados que tienen superficie nula.

Los cuadriláteros inscritos poseen propiedades particulares, incluyendo que los ángulos opuestos son suplementarios que se deduce a partir de la generalización del arco capaz. 




</doc>
<doc id="8937" url="https://es.wikipedia.org/wiki?curid=8937" title="Albert Szent-Györgyi">
Albert Szent-Györgyi

Albert Szent-Györgyi de Nagyrápolt (Budapest, -Woods Hole, Massachusetts, ) fue un fisiólogo húngaro, galardonado con el en 1937.

Su padre, Miklós Szent-Györgyi era terrateniente. Su madre, Jozefin, era hija de József Lenhossék y hermana de Mihály Lenhossék, ambos profesores de anatomía en la Universidad de Budapest.

Los trabajos de Szent-Györgyi estuvieron relacionados con la química de la respiración. En la Universidad de Szeged, empleó pimentón como fuente de vitamina C (el L-enantiómero del ácido ascórbico) y se dio cuenta de su actividad contra el escorbuto. Estudió la oxidación celular y descubrió la vitamina C en 1927.

En 1937 recibió el por sus descubrimientos en relación con los procesos de combustión biológica, en especial los referidos a la vitamina C y la catálisis del ácido fumárico.

Nació el 16 de septiembre de 1893 en la ciudad de Budapest, capital de Hungría, que en aquellos momentos formaba parte del Imperio Austrohúngaro. Inició sus estudios de medicina en la Universidad de Budapest, que combinó con sus propias investigaciones en el laboratorio químico de su tío. Debido a la Primera Guerra Mundial, donde sirvió como médico, tuvo que interrumpir sus estudios. Durante la Gran Guerra se disparó en un pie para abandonar el frente, hecho que le permitió finalizar sus estudios el año 1917. 

Inició su investigación científica en la ciudad de Pozsony, hoy en día Bratislava. Cuando la ciudad se convirtió en parted de
Checoslovaquia en enero de 1919, abandonó la villa junto a la mayor parte de población de origen húngaro. Posteriormente desarrolló su investigación en la Universidad de Groningen, centrándose en la química de la respiración celular. Gracias a estos trabajos y a una beca de la Fundación Rockefeller viajó a la Universidad de Cambridge, donde se doctoró en 1927 gracias a su trabajo en el aislamiento del "ácido hexurónico", hoy en día denominado Vitamina C.

Posteriormente, en la Universidad de Szeged utilizó "Capsicum annuum" como fuente de vitamina C (el L-enantiómero del ácido ascórbico) y se dio cuenta de su actividad anti-escorbútica. El año 1937 fue galardonado con el premio Nobel de Medicina y Fisiología «por su descubrimiento relacionado con los procesos de combustión biológica, con especial referencia a la vitamina C y a la catálisis de los ácidos fumáricos».

Durante la Segunda Guerra Mundial participó activamente en la Resistencia húngara. Aunque Hungría se alió con las potencias del Eje, el primer ministro húngaro Miklós Kállay envió a Szent-Györgyi a la ciudad de Estambul el año 1944 para realizar negociaciones secretas con los Aliados.

Adolf Hitler dictó una orden de detención de Szent-Györgyi, pero este se escapó de la Gestapo y permaneció escondido dos años. Después de la guerra recuperó su puesto e incluso se especuló con la posibilidad que los soviéticos lo nombraran primer ministro, por lo que se afilió al Partido Comunista de Hungría y fue elegido miembro del Parlamento a la vez que creó el laboratorio de Bioquímica de la Universidad de Budapest y restableció la Academia de Ciencias de Hungría. Descontento con el gobierno, emigró a los Estados Unidos el año 1947, siendo nombrado ciudadano el año 1955.

Durante su estancia en Norteamérica, creó su propio laboratorio en la ciudad de Woods Hole, situada en el estado de Massachusetts e inició sus investigaciones sobre el cáncer, desarrollando teorías sobre física cuántica aplicadas a la bioquímica de esta enfermedad. Durante la década de 1970 su búsqueda lo condujo a deducir que los radicales libres eran una causa potencial del cáncer. Murió el 22 de octubre de 1986.

Su cita más célebre: "Descubrir algo significa mirar lo mismo que está viendo todo el mundo y percibirlo de manera diferente".




</doc>
<doc id="8938" url="https://es.wikipedia.org/wiki?curid=8938" title="Oráculo de Delfos">
Oráculo de Delfos

El oráculo de Delfos, situado en un gran recinto sagrado consagrado al dios Apolo, fue uno de los principales oráculos de la Antigua Grecia. Estaba ubicado en el valle del Pleisto, junto al monte Parnaso, cerca de la actual villa de Delfos, en Fócida (Grecia), a 700m sobre el nivel del mar y a 9,5km de distancia del golfo de Corinto.

De las rocas de la montaña brotaban varios manantiales que formaban distintas fuentes. Una de las fuentes más conocidas y más antiguas era la fuente Castalia, rodeada de un bosque de laureles consagrados a este mismo dios. 

La leyenda y la mitología cuentan que en el monte Parnaso se reunían las musas, diosas menores del canto y la poesía, junto con las ninfas de las fuentes, llamadas náyades. En estas reuniones, Apolo tocaba la lira y las divinidades cantaban.

Originariamente tenía el nombre de Pito y alcanzó gran notoriedad en el mundo helénico desde mediados del 

Hay diversas propuestas acerca del origen del topónimo de Delfos. Una de ellas propone que viene de Delfino (Δελφινης), que era el nombre del dragón mitológico que custodiaba el oráculo antes de la llegada de Apolo. También se ha escrito que su origen parte de un mito según el cual Apolo se convirtió en delfín para atraer a un barco cretense, del que quería utilizar a la gente como sacerdotes; los cretenses desembarcaron y fundaron Crisa y se les encargó ser sacerdotes del templo y que adorasen al dios bajo el nombre de "Apolo Delfinio" para rememorar su conversión en delfín. Al templo de Apolo se le llamó igualmente "Delfinion" (Δελφίνιoν).

El santuario se construyó en el lugar conocido en la Antigüedad como Pito, nombre que en griego presenta dos formas (ambas femeninas): Πυθώ, -οῦς y Πυθών, -ῶνος (Homero. Il. 2.519 y 9.405; Od.8.80). Este nombre (que carece de etimología aceptada) se relaciona con el de la gran serpiente o dragón que, según la mitología, vigilaba el oráculo primitivo (véase el siguiente apartado). En la Antigüedad se intentó dar una etimología al nombre de Pito que lo relacionara con las funciones del santuario. A estos intentos de etimología popular se refieren su relación con el verbo "pythomai" (πύτωμαι) = "pudrir", que se relacionaría con el hecho de que Apolo habría dejado pudrirse a la serpiente tras haberla matado; o con el verbo pynthanomai (πυνθάνομαι) = "informarse, aprender" que se referiría a las funciones del propio oráculo.

Del término "Pitón" provienen los de "pitia" (Πυθία) o "pitonisa", nombre de las sacerdotisas del templo, que interpretaban las respuestas.

"Véase "Pitia o Pitonisa""

Hay testimonios de ocupación humana cercana al emplazamiento del santuario de Delfos de época arcaica desde el Neolítico, concretamente en una gruta del macizo del Parnaso. Ya en época micénica y en el mismo emplazamiento del santuario hubo primero (c. 1400 a. C.) una pequeña aldea que fue abandonada en algún momento entre 1100 y 800 a. C. El santuario propiamente dicho apareció después de esta fecha con un altar, al que siguió un primer templo. 

Una tradición relatada por Diodoro Sículo indica que un pastor observó como sus cabras se comportaban de un modo extraño cuando se aproximaban a una grieta de donde surgían vapores. Después, el pastor se acercó a ese mismo lugar y empezó a profetizar. Cuando la noticia se extendió, muchas otras personas llegaron al lugar para realizar también profecías, pero a menudo durante el trance saltaban a la grieta y desaparecían por ella. Por ello se decidió nombrar a una mujer para que profetizase por todos, a la que construyeron un trípode para que estuviera segura. Estrabón también menciona los vapores subterráneos que inspiraban a la Pitia y el trípode en la que se situaba. 

Por otra parte, el nombre de Pito se relaciona en la mitología con el de una gran serpiente o dragón Pitón hijo de la diosa Gea (la Tierra) que vigilaba un oráculo consagrado a su madre, o bien era compartido por Poseidón y Gea. Una tradición indica que Gea cedió a Temis su parte y esta lo regaló a Apolo. Por otra parte, Poseidón intercambió la suya con Apolo por Calauria. Sin embargo, la versión más difundida dice que, con el fin de establecer su propio oráculo con el que guiar a los hombres, Apolo mató a Pitón con su arco y tomó posesión del oráculo. Para establecer el culto del nuevo santuario desvió un barco de sacerdotes cretenses (cf. "Himno Homérico a Apolo"). 

Lo cierto es que ningún autor de la Antigüedad, ni siquiera Plutarco en su obra "Diálogos píticos", ha dejado ninguna descripción completa sobre cómo se realizaba una consulta, que además debió ser cambiante a lo largo de los siglos, por lo que la información sobre ello consiste en una recopilación de fuentes de diferentes épocas que a menudo contienen divergencias entre sí.

Se sabe que la elección de este personaje se hacía sin ninguna distinción de clases. A la candidata solo se le pedía que su vida y sus costumbres fueran irreprochables. El nombramiento era vitalicio y se comprometía a vivir para siempre en el santuario. Durante los siglos de apogeo del oráculo fue necesario nombrar hasta tres pitonisas para poder atender con holgura las innumerables consultas que se hacían por entonces. Sin embargo, en los tiempos de decadencia solo hubo una, suficiente para los pocos y espaciados oráculos que se requerían.

Según Diodoro Sículo, originalmente la pitia era una joven virgen, pero a raíz del rapto y violación de una de ellas por un joven de Tesalia se decretó que desde entonces no podría escogerse ninguna con menos de cincuenta años, aunque deberían seguir vistiendo como una doncella.

Los consultantes tenían una entrevista con ella unos días antes del oráculo. Este hecho está perfectamente documentado en las noticias que dan los autores de la Antigüedad. El oráculo se celebraba un día al mes, el día 7 que se consideraba como la fecha del nacimiento de Apolo. Por otra parte, en invierno no había oráculo, porque se creía que Apolo en esa época viajaba al país de los hiperbóreos. 

Los días de consulta, la Pitia se purificaba en la fuente Castalia. A continuación realizaba ofrendas a Apolo. Después, los sacerdotes vertían agua fría sobre una cabra. Si esta tiritaba, era una señal de que Apolo estaba receptivo a las consultas. Entonces se realizaba el sacrificio de la cabra en el altar de Apolo. 

Los consultantes eran de todo tipo, desde grandes reyes hasta gente pobre. En primer lugar se purificaban con agua de las fuentes de Delfos y a continuación se establecía un orden de consulta. El derecho de preferencia del que gozaban algunos de ellos se denominaba "promanteia". Una vez establecido el orden se pagaban las tasas correspondientes, luego ofrecían un sacrificio en el altar que había delante del templo y por último el consultante se presentaba ante la Pitia y hacía sus consultas oralmente, según se cree.

Se conoce muy poco sobre el rito que se seguía en el oráculo. Se sabe que la Pitia se sentaba en un trípode que estaba en un espacio llamado «áditon», al fondo del templo de Apolo Pitio. Αδυτων significa "fondo del santuario" y τo αδυτoν significa "lugar sagrado de acceso prohibido".

Diversos autores tardíos como Diodoro Sículo, Estrabón, Plutarco, Pausanias, Lucano, Orígenes y San Juan Crisóstomo describieron, con algunas diferencias entre sí, el proceso mediante el cual la pitia recibía la inspiración. La imagen dominante que transmiten estas descripciones es que el trípode de la Pitonisa o Pitia se hallaba sobre una grieta muy profunda de la roca. Por esa grieta emanaban unos gases que hacían que la mujer entrara en trance y su cuerpo se agitara. Algunos autores consideraban, en cambio, que la grieta era el espacio físico al que descendía la pitia para profetizar. Según Pausanias, algunos creían que era el agua de la fuente Casotis la que hacía profetizar a la pitia. Luciano menciona que además masticaba hojas de laurel, lo que ayudaba a alcanzar ese estado psicosomático.

Una vez inspirada, la pitia daba respuestas (el verdadero oráculo) y posiblemente —aunque las fuentes no son claras en este aspecto— un sacerdote las interpretaba y escribía en forma de verso, que después se entregaba al consultante.

Se estima que este y otros sistemas de adivinación eran considerados por los griegos de la Antigüedad como medios válidos y útiles de tener una conexión con sus divinidades, por lo que el oráculo fue respetado durante más de mil años.

Los trabajos arqueológicos y geológicos realizados en el siglo XIX por los primeros excavadores en la zona del templo de Apolo no encontraron debajo del templo la grieta profunda de que se habla en la leyenda pero, tras una revisión de la geología del lugar a finales del siglo XX, se ha encontrado que justo debajo del templo de Apolo se cruzan dos fallas geológicas y que por las fisuras que hay en las rocas ubicadas bajo el templo se pueden filtrar gases como etano, metano y etileno que podrían provocar que una persona entrara en un estado parecido al trance.

Tradicionalmente se conocen dos oráculos dados al rey Creso:

Creso (560-546 a. C.) fue el último rey de Lidia. Se cuenta (en Heródoto: "Historia" I, 53 y en Cicerón: "Sobre la adivinación" II, 115, 11) de él que en una ocasión envió una consulta al oráculo, pues se estaba preparando para invadir el territorio persa y quería saber si el momento era propicio. El oráculo fue así: ἤν στρατεύηται ἐπὶ Πέρσας, μεγάλην ἀρχήν μιν καταλύσειν / Croesus Halyn penetrans magnam pervertet opum vim / "Creso, si cruzas el río Halys (que hace frontera entre Lidia y Persia), destruirás un gran imperio". La respuesta se interpretó como favorable y dando por hecho que el gran imperio era el de los persas. Pero el “gran imperio” que se destruyó en aquel encuentro fue el suyo, y Lidia pasó a poder de los persas. Esto es un ejemplo de la ambigüedad en las respuestas. Muchas de ellas fueron recogidas por autores clásicos. En realidad el oráculo no trataba de adivinar los hechos, sino de dar buenos consejos, cosa que no era demasiado difícil, ya que en el santuario se disponía de la última noticia y de los últimos acontecimientos del mundo conocido.

Según Jenofonte, ante una consulta del mismo rey acerca de cómo podría pasar el resto de su vida del modo más felizmente posible se le respondió: "Si te conoces a ti mismo, Creso, realizarás la travesía felizmente". Esta máxima se basa en la idea que para conseguir la felicidad y la autoestima hay que conocer los propios límites y aceptarlos.

Según algunas tradiciones, la primera pitia o pitonisa que actuó en el oráculo de Delfos se llamaba Sibila, y su nombre se generalizó y se siguió utilizando como nominativo de esta profesión. Ni Homero ni Hesíodo hablan de las sibilas; su nombre aparece por primera vez en el siglo VI a. C. y es el filósofo Heráclito de Éfeso (544-) el primer informador de estos personajes. Se pensaba que las sibilas eran oriundas de Asia y que en cierto modo sustituyeron a las antiguas pitias.

La descripción bastante exacta de cómo fue el recinto sagrado se conoce gracias a las informaciones de Pausanias en el siglo II y a la confirmación de esos escritos hecha por las excavaciones arqueológicas.

Una cerca sagrada llamada períbola rodeaba todo el enclave del santuario. En la esquina sureste del recinto comenzaba la vía sacra que iba subiendo montaña arriba, serpenteando y pasando por delante de pequeñas edificaciones llamadas "tesoros" y de diversos monumentos, hasta llegar al templo del oráculo, templo de Apolo y continuando hasta el estadio en lo más alto. El peregrino accedía por la puerta principal de esta vía sagrada.

En el valle pueden verse cientos de olivos plantados, cuya extensión llega hasta el golfo de Corinto. Se dice que es el mayor olivar del mundo.

Los llamados "tesoros" (gr. θεσαυρυς, pronúnciase ""tesaurus"") eran pequeñas capillas donde se guardaban los exvotos y las donaciones que frecuentemente eran muy ricas y valiosas, verdaderas joyas. Se sabe que existían todas estas capillas:


En la terraza que se extendía delante del templo de Apolo estaba situado el altar de los sacrificios. Se construyó además un teatro (en el ) y un estadio, con 7000 plazas para espectadores, para los "juegos píticos" (evento iniciado en el ). También había un hipódromo, que aún está sin localizar.

Al aire libre y salpicadas por todo el recinto se hallaban las estatuas de mármol o de bronce, regalos de reyes o de ciudades, en agradecimiento a los servicios prestados por el oráculo.




El "ónfalos" es el "ombligo del mundo". La leyenda cuenta que el dios Zeus mandó volar a dos águilas desde dos puntos opuestos del Universo. Las águilas llegaron a encontrarse aquí, en Delfos, donde una piedra cónica llamada ónfalos señala el lugar. La piedra, en forma de medio huevo, fue descubierta durante las excavaciones cerca del templo de Apolo. 

Estas piedras que representan el ombligo del mundo eran un símbolo del centro, del lugar donde empezaría la creación del mundo. Al colocarlas en un determinado espacio, lo sacralizaba y lo convertía en el centro religioso. En el caso del ónfalos de Delfos, así fue y este santuario se convirtió en el ombligo o centro religioso de toda Grecia.

En algunas monedas encontradas en el recinto se puede ver la imagen del ónfalos, esquematizada y representada por un punto en el centro de un círculo. La piedra mencionada se halla expuesta en el museo de Delfos.

Por la arqueología y los escritos antiguos se sabe que en el siglo VIII a. C. hubo en este lugar de Delfos edificios sagrados. Pausanias, el historiador griego del siglo II d. C., recoge la tradición y entre otras cosas cuenta que los tres primeros templos fueron construidos, uno con laurel, otro con cera de abeja mezclada con plumas y el tercero con bronce.

La arqueología demuestra que en esta época ya era famoso el nombre de Apolo no solo en el lugar, sino en tierras lejanas. Los exvotos sacados a la luz en las excavaciones son muy significativos: Renombre de Apolo Pitio que era famoso en lugares remotos, caballos de Tesalia, trípodes del Peloponeso, soportes de recipientes de Creta, etc.

Pasado el tiempo fueron aumentando las ofrendas, sobre todo los exvotos de bronce. Se han encontrado escudos cretenses, cascos corintios, calderos con cabezas de grifos llegados desde Samos y el Peloponeso y estatuillas diversas.

A finales del siglo VII a. C. ya se construyen templos especiales para Apolo y Atenea; son de piedra, con columnas dóricas. Sus restos, pasado el tiempo, sirvieron para construir nuevos templos.

A comienzos del siglo VI a. C. tuvieron lugar dos acontecimientos que influyeron bastante en la evolución del santuario de Delfos. Uno fue la instalación en Delfos de la "anfictionía" y el otro, la reorganización de los "Juegos Píticos".

La anfictionía era una liga religiosa que agrupaba 12 pueblos (no ciudades), casi todos de la Grecia central. Tenía sus reuniones en el santuario de Deméter en Antela, cerca de las Termópilas. Como el oráculo de Delfos tenía ya un renombre mayor que el de Deméter, trasladaron allí la sede de esta confederación, sin por ello abandonar el otro santuario. Esta decisión dio lugar a las llamadas guerras sagradas que fueron tres.

Los Juegos Píticos tenían lugar al principio cada 8 años. Después lo acortaron a 4 y se alternaban con los Juegos Olímpicos. Consistían en pruebas atléticas, hípicas y concursos líricos. En Delfos se construyó en esta época un teatro y un hipódromo para la celebración de estos juegos, que se consideraban muy importantes.

Hubo un gran enriquecimiento tras la primera "guerra sagrada", en la que algunas ciudades griegas compitieron por obtener el control y la autoridad del santuario, con lo cual conseguían un reconocimiento de supremacía y prestigio sobre las otras ciudades y sobre algunos reinos extranjeros. Las aportaciones fueron tanto por parte de los griegos como de los pueblos bárbaros. Hay que destacar el regalo que hizo Creso (560-546 a. C.), último rey de Lidia, en esta ocasión: un león de oro sobre una base de lingotes de oro más un cuenco de oro que pesaba un cuarto de tonelada.

En la primera mitad del siglo VI a. C. se hicieron unas 12 fundaciones de tesoros en torno al templo de Apolo. Este viejo templo ardió en el año 548 a. C. y tras el incendio su reconstrucción fue lenta. Hasta el año 505 a. C. no se terminó el nuevo templo, más grande que el anterior y cuya construcción se llevó a cabo gracias a una familia llamada Alcmeónidas, de Atenas. Según cuenta Heródoto, esta familia gestionó la aportación de dinero en todo el mundo griego.

Las aportaciones de exvotos y ofrendas, más las construcciones de tesoros durante esta época, fueron cuantiosas:

Durante este siglo ocurrieron una serie de catástrofes que en nada beneficiaron al santuario de Delfos:

Durante el periodo helenístico, iniciado con los sucesores de Alejandro Magno, se construyó un teatro nuevo y un estadio nuevo.

Los etolios (señores de Delfos) regalaron numerosas ofrendas en forma de columnas y estatuas. Pero los donantes más generosos de esta época fueron los reyes de Pérgamo que en varias ocasiones ofrecieron dinero y mano de obra para el mantenimiento del santuario. El rey de Pérgamo Átalo I regaló un conjunto monumental para celebrar su victoria sobre los gálatas. La donación fue de tal calidad que los etolios de Delfos junto con los componentes de la anfictionía mandaron erigir unas estatuas de Átalo I y de Eumenes II sobre unos pilares y las colocaron junto a la fachada del templo. También Perseo de Macedonia regaló una estatua con su efigie, pero más tarde su vencedor el general romano Lucio Emilio Paulo la mandó quitar para sustituirla por una que le representaba a él.

Son de esta época las inscripciones epigráficas que cubrían los muros de los edificios y del muro poligonal. Pueden leerse textos sobre los "derechos honoríficos" y sobre "la liberación de esclavos". Apolo era quien garantizaba dicha liberación, después de habérsele pagado la suma correspondiente. También es de esta época la epigrafía del tesoro de los atenienses.

Comenzó el declive con la ocupación romana, durante el siglo I a. C. y continuó hasta el siglo III d. C. Durante este período el oráculo, respetado aún, fue sin embargo perdiendo prestigio y visitantes. En el siglo I a. C. fue cuando se hizo la talla de una fuente rupestre en la pared de la garganta Castalia, allá donde desde antiguo se encontraba el manantial sagrado.

Los fondos para el mantenimiento del santuario, de sus monumentos y de sus tesoros fueron menguando a grandes pasos; la hierba crecía entre los edificios, de manera salvaje, la madera se pudría y la suciedad empezaba a notarse. Hubo además un incendio en el templo de Apolo que el emperador Domiciano (81-96) hizo reparar. El escritor griego Plutarco (c. 46-125), que además fue administrador de la anfictionía en los últimos años de su vida, escribió por entonces sus "Diálogos píticos" y en este libro comenta la "impresión de abandono" que le daba el santuario de Delfos.

A pesar de todo, la anfictionía continuaba reuniéndose, organizaba los "Juegos Píticos", levantaba algunas estatuas a los cónsules y emperadores romanos y el oráculo seguía siendo consultado. Pero las peticiones eran ya de otro estilo: ya no se le pedía consejo sobre posibles enfrentamientos, reinados, gobernantes, etc., sino sobre viajes, matrimonios y otros asuntos domésticos. El oráculo dejó de influir en la política y el devenir de los pueblos. Su último momento de algo de esplendor se dio bajo el gobierno de los Antoninos, en el siglo II de nuestra era. Los emperadores siguieron manteniendo una regular correspondencia con el oráculo. Esta correspondencia ha llegado hasta nuestros días grabada sobre los contrafuertes del templo de Apolo.

El emperador romano Adriano (c. 76-138) también visitó Delfos. Allí hizo levantar una estatua (que ha sido hallada en las excavaciones) en homenaje a su favorito Antínoo, que había muerto ahogado misteriosamente en el río Nilo. 

Herodes Ático (101-177), político y orador griego, sofista y protector de las letras, además de poseer una gran riqueza, donó parte de esta a Delfos para reconstruir las gradas del estadio. También mandó erigir estatuas de su familia.

Pero ya por el siglo II d. C. el santuario recibía visitantes que eran más curiosos que fieles. Los viajeros llegaban allí para curiosear y no para utilizar el recinto como lugar sagrado. Pausanias fue uno de estos visitantes que llegó en calidad de hombre culto y amante de las antigüedades y luego contó sus impresiones como historiador. Ya en el año 87 a. C., Sila se había apropiado de muchas riquezas sagradas y de las ofrendas hechas en metales preciosos, lo mismo que el emperador Nerón en el siglo I. En el siglo IV el emperador romano Constantino I el Grande se llevó a Constantinopla una de las pocas piezas grandes que aún quedaban: la columna serpentina que se levantaba exenta y que nadie consideraba de valor después de que los focenses se llevaron 700 años antes su trípode de oro. Todavía se conserva.

En el siglo III los hérulos, godos y bastarnos recorrieron en intensas campañas toda la Grecia Central, Ática y el Peloponeso, arrasando y saqueando. En Delfos destruyeron algunas de las estatuas que quedaban en pie y el resto se vino abajo después del edicto de Teodosio el Grande, emperador romano (c. 346-395), con el que se pretendía acabar oficialmente con todos los "ídolos del paganismo", clausurando así definitivamente el oráculo de Delfos, que cesó su actividad en el año 390. La desolación fue total al cabo de los años y de los centenares de estatuas que antaño poblaron el recinto, no quedó ni una en pie.

El recinto de Delfos nunca llegó a estar deshabitado. Después de que se hubo olvidado por completo la razón de su existencia, sus ruinas se fueron recubriendo y se fue edificando toda una pequeña ciudad.

Tras la ocupación romana y la imposición del monoteísmo cristiano, durante el siglo V de nuestra era, el área de Delfos fue sede de un arzobispado, y para ello se desmanteló el oráculo, construyeron iglesias utilizando como material el mármol de los monumentos; se construyó una basílica, y grandes edificaciones religiosas, borrando así prácticamente toda evidencia del gran oráculo de Delfos. En el siglo XVIII los eruditos se plantearon la duda del lugar exacto en que habría estado el célebre santuario de Apolo. Por los textos antiguos se tenía una idea, pero era casi imposible dar con ningún vestigio hasta que, gracias a un hallazgo fortuito, empezaron los estudios sistemáticos y las excavaciones.

En 1676 Jacques Spon (francés) y George Wheler (inglés) llegaron al emplazamiento del santuario, convertido en un poblado llamado en ese momento "Castri". En su visita por el lugar se fijaron en unas inscripciones en la iglesia de un monasterio que había sido construido justamente sobre los muros del antiguo gimnasio. En estas inscripciones leyeron la palabra Delphi. Lo mismo les ocurrió en algunas casas del poblado. En estos años no pasó de ser una noticia para los historiadores; no hubo excavaciones.

Pasados dos siglos, en 1840, un arqueólogo alemán llamado Karl Otfried Müller trabajó en esta zona y descubrió entre las casas del poblado una parte del gran muro poligonal del recinto del santuario. El descubrimiento fue una llamada a seguir trabajando. Llegaron más arqueólogos franceses y alemanes, que fueron poco a poco descubriendo indicios y vestigios de la joya arqueológica que se escondía en aquel lugar. Pero la tarea era muy difícil pues la presencia del poblado impedía hacer excavaciones en serio. Empezaron entonces los tratos y los proyectos para trasladar a otro sitio todo el poblamiento de Castri, hasta que en 1881 hubo una convención entre el gobierno griego y el gobierno francés (muy interesado en las excavaciones) para expropiar, trasladar y reconstruir el nuevo emplazamiento, que es la ciudad actual llamada Delfí. Tras varios años de negociaciones, entre 1892 y 1901 se realizó una gran actividad arqueológica dirigida por el jefe de la Escuela Francesa de Atenas, Théophile Homolle. Fueron apareciendo piezas, restos de estatuas criselefantinas (es decir, estatuas que tenían la cara, las manos y los pies de marfil y el cabello de oro), piedras de edificios, columnas rotas, etc. En años posteriores vinieron las restauraciones llevadas a cabo por la Escuela francesa de Arqueología más una subvención del Ayuntamiento de Atenas y aportaciones particulares de ciudadanos griegos. De esta forma vieron la restauración:
Muchas de las piezas fueron llevadas al museo de Delfos, entre otras el famoso auriga de bronce de tamaño natural ofrendado por Policelo, la Esfinge de Naxos, los mellizos de Argos y una copia romana del "ónfalos" que era la piedra en forma de huevo que señalaba el centro u "ombligo de mundo" en Delfos y que fue encontrado durante las excavaciones hechas al templo de Apolo.

Un deslizamiento de rocas provocó graves daños al yacimiento arqueológico en 1935 así que, a partir de 1936 se volvieron a realizar excavaciones arqueológicas en el lugar, que pretendieron profundizar más que las anteriores. Por otra parte, durante la Segunda Guerra Mundial y la posterior guerra civil griega muchos objetos arqueológicos fueron enterrados en depósitos para preservarlos y no se desenterraron hasta 1952.

En la década de 1970 se excavó en la cueva Coricia, en la que se encontraron miles de figurillas. Otra campaña de excavaciones tuvo lugar en la década de 1990. En ella se investigó acerca de los primeros tiempos de Delfos y, entre otros hallazgos, se desenterró un hueso de león del siglo VI a. C. También en estas fechas se realizó un estudio geológico del lugar.





</doc>
<doc id="8939" url="https://es.wikipedia.org/wiki?curid=8939" title="Oligonucleótido">
Oligonucleótido

Un oligonucleótido es una secuencia corta de ADN o ARN, con cincuenta pares de bases o menos.

Tienen distintas funciones: se utilizan como cebadores en reacciones de amplificación, como sondas de hibridación y en bloqueos específicos de ARN mensajero. 

Un oligonucleótido es una molécula compleja formada a su vez por varios nucleótidos, cada uno de los cuales está compuesto por una base nitrogenada, un hidrato de carbono y un grupo fosfato.
La síntesis de oligonucleótidos se basa en una serie de reacciones en las que se van protegiendo y desprotegiendo cíclicamente los diferentes centros reactivos de la molécula:

Los oligonucleótidos se emplean en terapia génica como estrategia para el silenciamiento de genes. Se utilizan oligonucleótidos de 12 a 20 pares de bases complementarios a los ARN mensajeros de los genes que queremos silenciar.

Dentro de la célula, el oligonucleótido se une a su ARN mensajero diana y bloquea su traducción, esto es, el ribosoma va a traducir el ARN mensajero hasta que se encuentra el oligonucleótido y el proceso se detiene. Además, el hecho de que la ARNasaH reconozca el híbrido oligo-ARN y lo degrade es otra ventaja de esta técnia. Es importante diseñar el oligonucleótido cerca del inicio de traducción o bien asegurarse de que el posible producto resultante no tenga función biológica, para que el silenciamiento sea efectivo.

Los oligonucleótidos ideales para terapia génica deben tener las siguientes características:

Los oligonucleótidos pueden usarse in vivo, pero poseen una vida media muy corta y una concentración baja. Precisamente porque no tiene efectos duraderos, el principal problema de los oligonucleótidos como terapia génica es que requieren dosis sucesivas.

Se modifican químicamente para tener una vida media más larga, bien en el oxígeno de los puentes fosfato, en las bases o en el fosfato que no se encuentra en los puentes de unión. Se consideran por tanto como una droga/medicamento, ya que tienen no alteran el ADN y terminan por degradarse. 

Las posibles aplicaciones de los oligonucleótidos podrían darse en terapia antiviral, terapia antibacteriana, terapia antiparasitaria, terapia anticáncer ex vivo, tratamiento de enfermedades de la piel, inhibición de la inflamación, supresión de oncogenes o supresión de genes dominantes.

En las mutaciones que provocan un fin anticipado de la traducción, los oligonucleótidos se emplearían para inducir un procesamiento alternativo en el ARN mensajero mutante, sin inlcuir el exón que contiene la mutación. De esta manera, el codón stop prematuro no sería leído por el ribosoma y la proteína podría continuar traduciéndose, luego el producto final sería el mismo que el del gen normal, sin mutación.

Si se une al oligonucleótido una molécula de EDTA-Fe, esto provoca la destrucción activa del ARN mensajero al que se une, con lo que el oligonucleótido puede liberarse para degradar otro ARN mensajero.

Un oligonucleótido también podría unirse a la doble hebra de ADN, dando lugar a una estructura conocida como triple hélice. Si se une a un gen concreto, estaría inhibiendo su transcripción, y por tanto, de nuevo, silenciándolo. Sólo se necesitarían dos oligonucleótidos por célula (uno por cada cromosoma del par), de modo que se evitaría el problema de la dosis. Esta técnica sería útil para mutaciones dominantes. Sin embargo, el superenrollamiento del ADN y su interacción con las histonas dificulta la entrada del oligonucleótido en la doble cadena.


</doc>
<doc id="8940" url="https://es.wikipedia.org/wiki?curid=8940" title="Reacción en cadena de la polimerasa">
Reacción en cadena de la polimerasa

La reacción en cadena de la polimerasa, conocida como PCR por sus siglas en inglés ("polymerase chain reaction") o como RCP, es una técnica de la biología molecular desarrollada en 1986 por Kary Mullis.Su objetivo es obtener un gran número de copias de un fragmento de ADN particular, partiendo de un mínimo; en teoría basta partir de una sola copia de ese fragmento original, o molde.

Esta técnica sirve para amplificar un fragmento de ADN; su utilidad es que tras la amplificación resulta mucho más fácil identificar, con una probabilidad muy alta, virus o bacterias causantes de una enfermedad, identificar personas (cadáveres) o hacer investigación científica sobre el ADN amplificado. Estos usos derivados de la amplificación han hecho que se convierta en una técnica muy extendida, sobre todo en el ámbito de la investigación forense, con el consiguiente abaratamiento del equipo necesario para llevar a cabo dicha técnica.

Esta técnica se fundamenta en la propiedad natural de las ADN polimerasas para replicar hebras de ADN, para lo cual se emplean ciclos de altas y bajas temperaturas alternadas para separar las hebras de ADN recién formadas entre sí tras cada fase de replicación y, a continuación, dejar que las hebras de ADN vuelvan a unirse para poder duplicarlas nuevamente. La reacción en cadena de la polimerasa fue perfeccionada por Kary Mullis perteneciente a la Cetus Corporation en California, en la década de 1980.
Inicialmente la técnica era lenta, ya que las polimerasas se desnaturalizaban al realizar los cambios de temperatura y era necesario agregar nuevas polimerasas en cada ciclo. Puesto que las temperaturas del ciclo (95 °C en las fases de desnaturalización del ADN) suponen la inmediata desnaturalización de toda proteína, se emplean ADN polimerasas termoestables, extraídas de microorganismos adaptados a vivir a esas temperaturas, restrictivas para la mayoría de los seres vivos. Dichos microorganismos, generalmente arqueas, son: "Thermus aquaticus" (polimerasa Taq), "Pyrococcus furiosus" (Pfu), "Thermococcus litoralis" (Vent) y "Thermus thermophilus" (Tth). Generalmente se emplean mezclas de polimerasas muy procesivas (Taq) con otras capaces de hacer corrección de errores (Pfu, Vent).

Hoy, todo el proceso de la PCR está automatizado mediante un aparato llamado termociclador, que permite calentar y enfriar los tubos de reacción para controlar la temperatura necesaria para cada etapa de la reacción. Muchos termocicladores modernos hacen uso del efecto Peltier, que permite tanto calentar como enfriar los tubos simplemente invirtiendo la corriente eléctrica. Los tubos usados para PCR tienen una pared muy fina, lo que favorece una buena conductividad térmica, permitiendo que se alcance rápidamente el equilibrio térmico. Casi todos los termocicladores tienen un sistema que calienta la tapa de cierre con el fin de evitar la condensación sobre los tubos de reacción. Los termocicladores más antiguos carecían de este sistema y solucionaban el problema de la condensación con una capa de aceite en la parte superior de la mezcla de reacción o con un poco de cera dentro de los tubos. Actualmente existen algunos termocicladores que utilizan o pueden utilizar aceite mineral en el tubo de PCR como los termocicladores de nueva generación de flujo de aíre. 

Por lo general, la PCR es una técnica común y normalmente indispensable en laboratorios de investigación médica y biológica para una gran variedad de aplicaciones. Entre ellas se incluyen la clonación de ADN para la secuenciación, la filogenia basada en ADN, el análisis funcional de genes, el diagnóstico de trastornos hereditarios, la identificación de huellas genéticas (usada en técnicas forenses y test de paternidad) y la detección y diagnóstico de enfermedades infecciosas.

Para realizar la técnica se necesitan:






El proceso de PCR por lo general consiste en una serie de 20 a 35 cambios repetidos de temperatura llamados ciclos; cada ciclo suele consistir en 2-3 pasos a diferentes temperaturas. La PCR común se realiza con ciclos que tienen tres pasos de temperatura. Los pasos de ciclos a menudo están precedidos por un choque térmico (llamado "hold") a alta temperatura (> 90 °C), y seguido por otro hold al final del proceso para la extensión de producto final o el breve almacenaje. Las temperaturas usadas y el tiempo aplicado en cada ciclo dependen de gran variedad de parámetros. Estos incluyen la enzima usada para la síntesis de ADN, la concentración de iones divalentes y de los dNTP en la reacción, y la temperatura de unión de los cebadores, así como la longitud del ADN que se desea amplificar.

Actualmente, casi todos los termocicladores dan la opción de realizar la reacción de PCR con la llamada " tapa caliente". Es decir, que el sistema del termociclador aplicará calor a la parte de arriba del vial que contiene la mezcla de PCR. Al comienzo, los laboratorios que empezaron a usar los primeros aparatos que se comercializaron y que no incluían este sistema tenían que poner unas gotas de aceite dentro del vial. El objetivo de este procedimiento, al igual que el de la tapa caliente, es evitar la condensación de la muestra, ya que en el eppendorf se encuentran dos fases:líquido y gas. Al condensarse la muestra, perdemos volumen de la mezcla. Sin embargo, calentando la tapa o poniendo las gotas de aceite evitamos este proceso físico, conservando casi intacto el volumen de la muestra.

Este paso consiste en llevar la reacción hasta una temperatura de 94-96 °C (ó 98 °C si se está usando una polimerasa termoestable extrema), que se mantiene durante 1-9 minutos. Esto solo es necesario para ADN polimerasas que requieran activación por calor.

En primer lugar, se desnaturaliza el ADN (se separan las dos cadenas de las cuales está constituido). Este paso puede realizarse de diferentes modos, siendo el calentamiento (94-95 °C) de la muestra la forma más habitual. La temperatura a la cual se decide realizar la desnaturalización depende, por ejemplo, de la proporción de G+C que tenga la cadena, como también del largo de la misma. Otros métodos, raramente empleados en la técnica de la PCR, serían la adición de sales o agentes químicos capaces de realizar la desnaturalización.

A continuación se producirá la hibridación del cebador, es decir, el cebador se unirá a su secuencia complementaria en el ADN molde. Para ello es necesario bajar la temperatura a 40-68 °C durante 20-40 segundos (según el caso), permitiendo así el alineamiento. Los puentes de hidrógeno estables entre las cadenas de ADN (unión ADN-ADN) solo se forman cuando la secuencia del cebador es muy similar a la secuencia del ADN molde. La polimerasa une el híbrido de la cadena molde y el cebador, y empieza a sintetizar ADN. Los cebadores actuarán como límites de la región de la molécula que va a ser amplificada.

Actúa la polimerasa, tomando el ADN molde para sintetizar la cadena complementaria y partiendo del cebador como soporte inicial necesario para la síntesis de nuevo ADN. La polimerasa sintetiza una nueva hebra de ADN complementaria a la hebra molde añadiendo los dNTP complementarios en dirección 5'→ 3', uniendo el grupo 5'-fosfato de los dNTP con el grupo 3'-hidroxilo del final de la hebra de ADN creciente (la cual se extiende). La temperatura para este paso depende del ADN polimerasa que usemos. Para la polimerasa Taq, la temperatura de máxima actividad está en 75-80 °C (comúnmente 72 °C). El tiempo de extensión depende tanto del ADN polimerasa usada como de la longitud del fragmento de ADN que se va a amplificar. Hay una regla comúnmente usada: en su temperatura óptima, la polimerasa de ADN polimerizará mil bases en un minuto.

Etapa única que se lleva a cabo a una temperatura de 70-74 °C durante 5-15 minutos tras el último ciclo de PCR. Con ella se asegura que cualquier ADN de cadena simple restante sea totalmente ampliado.

Este es un paso que se lleva a cabo a 4-15 °C durante un tiempo indefinido para conservar la reacción a corto plazo.

La PCR normalmente se realiza con un volumen de reacción de 15-100 μL, en pequeños tubos de 0.2-0.5 mL que se colocan en el termociclador.

Para verificar que la PCR ha generado el fragmento de ADN previsto, se emplean técnicas de electroforesis, que separan los fragmentos de ADN generados de acuerdo a su carga, esto es, longitud, y, en menor medida y dependiendo de la matriz empleada, a su tamaño: típicamente se emplean la electroforesis en gel de agarosa, para fragmentos grandes; en acrilamida, para los más pequeños; y, de forma más rápida y aplicable a la PCR asociada a marcaje fluorescente, la electroforesis capilar. El/los tamaño/s de los productos de la PCR vienen determinados por un marcador de
peso molecular de ADN, el cual contiene fragmentos de ADN de tamaño conocido, y que se corre en el gel junto con los productos de PCR.

En la práctica, la PCR puede fallar por varias razones, entre ellas:




Aparte de aspectos como la contaminación y algún fallo en la hibridación de primers, puede haber otras complejidades que afecten a la PCR, como son:

Técnica muy sensible de PCR en la que el producto de una amplificación es utilizado como molde para realizar una segunda amplificación con cebadores que se ubican dentro de la primera secuencia amplificada, es decir, cuando tenemos el primer, como su amplificación se pueden unir los cebadores y se hace de nuevo una amplificación dentro de la amplicón inicial. Este tipo de PCR tiene la ventaja de brindar alta sensibilidad y especificidad. La especificidad aumenta porque como es amplificación de un amplicón obtenido previamente, los cebadores solo van a hibridar en un sitio dentro de la molécula y el resultado será una única banda. Así, evitamos posibles hibridaciones inespecíficas de los cebadores. La desventaja de esta técnica es que no nos permite cuantificar la muestra.

Se introducen cambios de secuencia dentro de fragmentos (clonados) de ADN. Se requieren 2 cebadores (primers) mutagénicos y otros 2. Se amplifica un fragmento 5' y un fragmento 3' que se solapan portando ambos la mutación. Se usan los productos en otra reacción para producir el ADN mutado de longitud completa.

La PCR "in situ" consiste en una reacción de PCR en secciones histológicas o células, donde los productos generados pueden visualizarse en el sitio de amplificación. Es realizada sobre preparaciones fijas en un portaobjetos. En la técnica de PCR "in situ" se realiza una primera amplificación de ADN blanco y luego detección mediante hibridación "in situ" convencional con sondas de ADN/ARN. De esta manera pueden detectarse cantidades pequeñísimas de genoma. Esta tecnología es de gran alcance en la capacidad de amplificar específicamente una población de secuencias de menor representación.

PCR en la cual se amplifica simultáneamente más de una secuencia.
Para ello, se combinan dos o más pares de cebadores en un mismo tubo, junto con el resto de los reactivos de la reacción en cantidades suficientes, para amplificar simultáneamente varios segmentos de ADN.
Ventajas: información sobre varios locus en una sola reacción, menor cantidad de molde para el análisis, menor cantidad de reactivos, rápida construcción de bases de datos. Desventajas: para llevarla a cabo adecuadamente y sin errores, se requiere de una cuidadosa optimización del proceso.

Es una variante de la PCR en la que usamos ARN como molde inicial en vez de ADN, y emplea una transcriptasa inversa (como Tth) para realizar la síntesis de un ADN complementario al ARN (ADNc). De esta forma, el desarrollo inicial de una RT-PCR sería:

Reacción de PCR cuya principal característica es que permite cuantificar la cantidad de ADN o ARN presente en la muestra original, o para identificar con una muy alta probabilidad, muestras de ADN específicas a partir de su temperatura de fusión (también denominado valor "T", del inglés "melting temperature").

Se puede dividir en las técnicas basadas en fluorocromos no específicos y en las técnicas basadas en sondas específicas.

En las técnicas basadas en fluorocromos el ADN, que ve multiplicada su cantidad con cada ciclo, se une al fluorocromo (generalmente SYBR Green) produciendo fluorescencia que es medida por el termociclador apto para PCR en tiempo real. Permite cuantificar solo una secuencia por reacción pero tiene la ventaja de utilizar cebadores normales para su realización. Es mucho más económica que la que usa sondas específicas.

Las técnicas basadas en sondas específicas utilizan una "sonda" unida a dos fluorocromos que hibrida en la zona intermedia entre el cebador directo ("forward") y el inverso ("reverse"); cuando la sonda está intacta, presenta una transferencia energética de fluorescencia por resonancia (FRET). Dicha FRET no se produce cuando la sonda está dañada y los dos fluorocromos están distantes, producto de la actividad 5'-3' exonucleasa de la ADN polimerasa. Esto permite monitorizar el cambio del patrón de fluorescencia y deducir el nivel de amplificación del gen.

La mayoría de estos inconvenientes se han solucionado con la introducción de la PCR realizada en tiempo real (Q-PCR), que elimina cualquier proceso post-PCR puesto que monitoriza la progresión de la amplificación en el momento en que ocurre. A diferencia de la PCR convencional (en punto final), que mide la acumulación del ADN al final de un número predeterminado de ciclos, con Q-PCR esto se hace durante el proceso de amplificación usando fluorescencia, de forma que su aumento es proporcional a la cantidad de ADN formada. El proceso se puede automatizar fácilmente usando un sistema que realice la amplificación (termociclador) y que a su vez sea capaz de leer fluorescencia. Existe una amplia oferta de aparatos en el mercado. La mayoría pueden trabajar con las diversas opciones de marcado fluorescente y son "abiertos", es decir, permiten programar las condiciones de amplificación y lectura de forma que su uso no queda limitado a unos reactivos determinados.
















La técnica de la PCR tiene multitud de aplicaciones: ya en ciencia básica, como herramienta de detección y/o generación de acervos de fragmentos de ADN de interés; ya en ciencia aplicada, como elemento resolutivo en sí mismo, por ejemplo en diagnóstico clínico.

La PCR convencional, se emplea como base para multitud de técnicas en el laboratorio debido a su robustez y rapidez. De este modo, la PCR de punto final permite controlar y detectar los fragmentos de ADN de interés.

Una aplicación de la PCR de extrema importancia es la clonación de secuencias de ADN en vectores, como pueden ser los plásmidos. Para ello, se emplean cebadores que contienen en su extremo 5' una corta secuencia que permite la interacción posterior con otra complementaria situada en el vector de clonación a emplear. Por ejemplo, se puede incluir una diana de restricción en dichos cebadores, de modo que, y si esta no existía previamente en el fragmento y es única en el vector, pueda efectuarse una ligación mediante la ligasa de T4 tras la digestión con la enzima de restricción apropiada de ambos elementos. Otro método asimilable a esta vía es el empleo de la recombinación dirigida; esto es, se adapta al 5' de los cebadores una secuencia que faculta a una recombinasa la recombinación dirigida con un vector dado.

En medicina, la PCR se emplea fundamentalmente como herramienta de diagnosis (Coleman y Tsongalis, 2006):


Los campos de la paleontología, antropología biológica y la medicina y antropología forense se han visto enormemente beneficiados por esta técnica, puesto que todas ellas construyen con frecuencia el conocimiento de sus correspondientes disciplinas gracias a restos o huellas de seres vivos. Uno de los materiales biológicos que más información puede proporcionar es el ADN.
La relativa estabilidad de este permite que, aunque fragmentado, se conserve durante largos períodos si las condiciones son propicias. En ocasiones las muestras intactas con las que se puede contar son extraordinariamente pequeñas o están deterioradas. La PCR soluciona ambos problemas y proporciona cantidades útiles para posteriores pasos de análisis. En primer lugar aumenta la cantidad de material recuperado a partir de muestras escasas, puesto que como ya se dijo anteriormente, en teoría basta una sola molécula para que el proceso pueda tener lugar. También debido a la naturaleza de la técnica y su propósito de amplificación de fragmentos pequeños, esta fragmentación no impide que este ADN pueda ser empleado como molde para una reacción de PCR.


Tal y como la PCR multiplex permite producir huellas genéticas de individuos concretos, dentro del marco de la genética forense, existen métodos basados en la PCR que permiten discernir entre grupos infraespecíficos de cultivos de interés agronómico; por ejemplo, de cultivares. Para ello, se emplean oligonucleótidos de un tamaño lo suficientemente pequeño como para que ceben de forma relativamente inespecífica, aunque siempre de tal forma que produzcan un patrón de bandas discreto e interpretable. De este modo, la pauta obtenida tras la electroforesis de los fragmentos tiende a agrupar a los individuos de mayor semejanza, que poseen un comportamiento similar, de los que divergen.

En 1971, un artículo publicado por Kleppe et al. en "Journal of Molecular Biology" describió por primera vez un método que usaba enzimas para replicar una secuencia pequeña de ADN con cebadores "in vitro". Sin embargo, este temprano ejemplo del principio básico de la PCR no recibió mucha atención, y la invención de la reacción en cadena de la polimerasa en 1983 es generalmente atribuida a Kary Mullis.
Mullis ganó el Premio Nobel por su trabajo en PCR.

Algo muy a tener en cuenta en la PCR es que la ADN polimerasa que se use sea capaz de soportar las altas temperaturas de >90 °C necesarias para la separación de las dos hebras de ADN de la doble hélice tras cada ciclo de replicación. Las ADN polimerasas que se utilizaron originariamente para los experimentos "in vitro" previos a la PCR no eran capaces de soportar estas altas temperaturas, por lo que los primeros procedimientos para replicar el ADN eran muy ineficientes, largos y requerían grandes cantidades de ADN polimerasa.

El descubrimiento en 1968 de la polimerasa Taq, una polimerasa de ADN extraída de la bacteria termófila "Thermus aquaticus" que habita medios de muy alta temperatura (50-80 °C), eliminó los grandes inconvenientes del método de la PCR. Este ADN polimerasa es estable a altas temperaturas, permaneciendo activa hasta después de la desnaturalización del ADN, eliminando la necesidad de añadir a la reacción nueva polimerasa tras cada ciclo. Este descubrimiento permitió automatizar el proceso, antes tan tedioso, acoplándolo al uso del termociclador.

Al mismo tiempo que desarrollaba la PCR en 1983, Mullis trabajaba en Emeryville, California (EE. UU.), para una de las primeras empresas biotecnológicas, Cetus Corporation, donde era responsable de sintetizar cadenas cortas de ADN. Mullis afirma que concibió la idea para la PCR una noche mientras cruzaba la Autopista de la Costa Pacífica (EE. UU.) en su coche. Estaba imaginando una nueva forma de analizar mutaciones en el ADN cuando se percató de que, en lugar de eso, había inventado un método para amplificar regiones específicas de ADN mediante ciclos de duplicación repetidos usando ADN polimerasas. Mullis atribuye la invención de esta técnica a los efectos de la droga psicodélica y alucinógena LSD.

En la revista "Scientific American", Mullis resumió el procedimiento: "Comenzando con una única molécula del material genético ADN, la PCR puede generar 100 billones de moléculas iguales en una tarde. La reacción es fácil de hacer, no requiere más que un tubo de pruebas, unos pocos reactivos simples y una fuente de calor". Fue premiado con el en 1993 por su invención, y siete años después, él y sus colegas del Cetus llevaron a la práctica su propuesta. Sin embargo, han aparecido controversias y diferentes versiones sobre las contribuciones intelectuales y prácticas de otros científicos al trabajo de Mullis, y sobre si él fue el inventor único del principio de la PCR.

La técnica de la PCR fue patentada por Cetus Corporation, donde Mullis trabajaba cuando inventó la técnica en 1983. La enzima polimerasa Taq fue también cubierta de patentes. Tuvieron lugar varios pleitos relacionados con la técnica, incluyendo un pleito fracasado generado por DuPont. La compañía farmacéutica Hoffmann-La Roche adquirió los derechos de las patentes en 1992 y actualmente mantiene las que aún están protegidas.





</doc>
<doc id="8943" url="https://es.wikipedia.org/wiki?curid=8943" title="Torre de servidores">
Torre de servidores

Una torre de servidores es un grupo de servidores, normalmente mantenidos por una empresa o universidad para ejecutar tareas que van más allá de la capacidad de una sola máquina corriente, como alternativa, generalmente más económica, a un superordenador. 

También hace posible la distribución de tareas, de forma que el sistema gana cierta tolerancia a fallos, ya que si uno de los servidores se estropea, el sistema continúa trabajando, notando únicamente una pérdida de rendimiento.

El término usado en inglés es "server farm" y también podrá encontrarlo con su traducción literal: granja de servidores.


</doc>
<doc id="8946" url="https://es.wikipedia.org/wiki?curid=8946" title="Amposta">
Amposta

Amposta es un municipio y localidad española de la provincia de Tarragona, en la comunidad autónoma de Cataluña. Es capital de la comarca del Montsiá. Situado a una altitud de 8 metros sobre el nivel del mar, a orillas del río Ebro, el término municipal cuenta con . De su actividad económica destaca la agricultura (cultivo del arroz) y los servicios. En los últimos años del surgió un pequeño pero importante sector industrial basado en la industria alimentaria, papelera y de embalajes, muebles y multitud de pequeños talleres de maquinaria.

La ciudad está situada en el margen derecho (o sur) del río Ebro, en el límite entre la plataforma continental y el delta del Ebro; de hecho, la parte más oriental de la ciudad está construida sobre una zona pantanosa. El término municipal es el mayor de toda la comarca del Montsià, de la cual Amposta es la capital. La población limita al norte con el río Ebro; al este con San Jaime de Enveija y con el mar Mediterráneo; al sur con el mar Mediterráneo y con San Carlos de la Rápita; y al oeste con Freginals, Masdenverge y Tortosa.

Amposta cuenta con tres entidades de población, la propia ciudad y los núcleos de Balada y Poblenou del Delta, este último fundado en los años 1950 por el Instituto Nacional de Colonización con el nombre de "Villafranco del Delta", en honor al dictador Francisco Franco. 

La principal vía de comunicación con la que cuenta la ciudad es la carretera nacional N-340 y la AP-7.

Existen abundantes vestigios y restos arqueológicos, como un poblado íbero con siete silos de grano en la zona del castillo, otro en la zona del "Pla d'Empuries" y una necrópolis en la "Oriola", que hacen que algunos historiadores como Esteve, Schulten y Bosch apoyen la teoría de que en el actual término municipal de Amposta se ubicaba la ciudad de Hibera o Ibera, capital del territorio de los ilercavones antes de la conquista romana, y el primer gran asentamiento ibérico de la península. La situación de la población de Hibera es disputada por las poblaciones de Amposta, San Carlos de la Rápita y Tortosa.

Durante la segunda guerra púnica se produjo en 215 a. C. la batalla de Dertosa, (algunos expertos la denominan también la batalla de Hibera o de Ibera), que enfrentó a cartagineses y romanos. La ciudad, aliada de los cartagineses fue destruida por las tropas romanas, con lo que los habitantes de la ciudad huyeron y se perdió el rastro de Amposta. La cultura ilercavona se perdió absorbida por la romana.

Los romanos establecieron un poblado cerca de la Torre de la Carrova y también establecieron una posada de vigilancia en una terraza sobre el río Ebro, en el núcleo antiguo de Amposta, de aquí proviene el origen del término Amposta, "Amni Imposita", posada sobre el río.

Durante la conquista árabe, éstos establecieron una fortaleza en el mismo lugar donde estaba la posada romana, hecho confirmado por los posteriores hallazgos arqueológicos.

El conde de Barcelona Ramón Berenguer III, fracasó en el intento de apoderarse de Amposta los años 1095 y 1097, pero pese a ello, infeudó la ciudad al monasterio de San Cugat en 1097. Sin embargo, en 1098, hizo enfeudación a favor de Artal de Pallars a cambio de que este se comprometiera a construir un castillo. Es su hijo Ramón Berenguer IV quien lo consiguió en el año 1148. Como recompensa a su ayuda en sus conquistas, Ramón Berenguer IV dio en el año 1149 (1150 según el calendario actual) el castillo de Amposta y las tierras que lo rodean a la Orden de San Juan de Jerusalén, quienes convirtieron el castillo en el centro y capital de todas la posesiones de los Hospitalarios en la Corona de Aragón. Durante ese período, el castillo alcanzó gran prosperidad e importancia, el título de castellán de Amposta es sinónimo de gran poder dentro de la Corona de Aragón, ya que era el representante de la orden ante el monarca, como lo fue Juan II de Ribagorza. El año 1280, el castillo pasó a control de la corona tras ser cambiado a los Hospitalarios por las villas de Onda y Gallur, por lo que la villa pasó a regirse por los Usatges de Barcelona.

Posteriormente, el sitio dejó de tener tanta importancia, hasta 1461, cuando durante la Guerra civil catalana la ciudad tomó partido por el hijo de Juan II de Aragón, Carlos de Viana contra el otro pretendiente, el que luego seria el rey Fernando el Católico. Por este motivo el castillo sufrió un asedio que empezó el 2 de octubre de 1465, y no fue hasta ocho meses después, el 21 de junio de 1466, cuando el castillo fue tomado, siendo seriamente destruido y la ciudad perdió la categoría de plaza fuerte.

En la edad moderna, ya sin la protección del castillo, la ciudad entró en un período de decadencia, siendo destruida hasta tres veces por piratas turcos y berberiscos que saqueaban las costas a lo largo del . Posteriormente vino un período de lenta recuperación durante los siglos , y , cuando se empieza a explotar las tierras del delta y el puerto de los "Alfacs"

Durante las guerras carlistas, la ciudad fue un importante punto carlista y que las tropas reales llegaron a someterla a asedio varias veces.

Durante el comenzó el crecimiento urbano y demográfico de la ciudad. En 1860 se acordó comenzar a cultivar la zona del delta y cultivar arroz. Amposta también contaba con algunas industrias como molinos de aceite y arroz, y de construcción, aprovechando las materias primas del delta como la caña y la sosa.

Con el saneamiento del delta y el cultivo del arroz, la ciudad alcanzó los 4000 habitantes a principios del . La ciudad empezó su desarrollo con la construcción del puente colgante promovido por el alcalde Joan Palau, las escuelas, abastecimiento de agua y electricidad. El desarrollo de la población sólo se vio interrumpido por la guerra civil, en la que el puente colgante resultó destruido el 10 de marzo de 1938 por un ataque de la aviación italiana. El 18 de abril de 1938 la ciudad fue tomada por el ejército sublevado, comandado por el general Rafael García Valiño.

Tradicionalmente, el sector económico más importante ha sido la agricultura, sobre todo del arroz y los regadíos, así como otras actividades relacionadas como la maquinaria. Sin embargo, a principios del se puede decir que la actividad económica se ha diversificado, y aunque la agricultura sigue existiendo, la baja rentabilidad y la mecanización han hecho que descienda en número de personas empleada en el campo. No obstante, la "Cambra Arrosera del Montsià" es una de las cooperativas más grandes de España y la mayor de Cataluña, además de ser una de las mayores empresas de la ciudad.

La industria ha pasado a ocupar un lugar más importante en la economía. Las industrias que tradicionalmente han existido en Amposta han sido las de talleres de maquinaria, industria papelera y alimentarias. Los últimos años estas empresas han evolucionado de forma notoria y han aparecido nuevas industrias, como las dedicadas al mueble, el metal y la construcción.

El sector terciario también está muy desarrollado, siendo la ciudad un centro importante de servicios, tanto de la comarca como de toda la región. Además de disponer de servicios propios de una ciudad capital de comarca, es un importante centro comercial para la zona y tiene un sector turístico embrionario que empieza a desarrollarse con el Delta y su parque natural y la costa.

Edificios religiosos:

Museos:

Patrimonio civil:

Amposta se distingue por tener un gran número de sociedades culturales y cívicas, pero destacan las sociedades musicales La Lira Ampostina (del 1916) y La Sociedad Musical Unión Filarmónica (de 1917), conocidas por «La Lira» y «La Fila» respectivamente. Ambas tienen bandas de un gran nivel, escuelas de música, y unas sedes sociales que organizan continuamente actos abiertos a la población; las dos, también, han recibido la Cruz de Sant Jordi de la Generalidad de Cataluña.



</doc>
<doc id="8955" url="https://es.wikipedia.org/wiki?curid=8955" title="Seguridad informática">
Seguridad informática

La seguridad informática, también conocida como ciberseguridad o seguridad de tecnología de la información, es el área relacionada con la informática y la telemática que se enfoca en la protección de la infraestructura computacional y todo lo relacionado con esta y, especialmente, la información contenida en una computadora o circulante a través de las redes de computadoras. Para ello existen una serie de estándares, protocolos, métodos, reglas, herramientas y leyes concebidas para minimizar los posibles riesgos a la infraestructura o a la información. La ciberseguridad comprende software (bases de datos, metadatos, archivos), hardware, redes de computadoras y todo lo que la organización valore y signifique un riesgo si esta información confidencial llega a manos de otras personas, convirtiéndose, por ejemplo, en información privilegiada.

La definición de seguridad de la información no debe ser confundida con la de «seguridad informática», ya que esta última solo se encarga de la seguridad en el medio informático, pero la información puede encontrarse en diferentes medios o formas, y no solo en medios informáticos.

La seguridad informática también se refiere a la práctica de defender las computadoras y los servidores, los dispositivos móviles, los sistemas electrónicos, las redes y los datos de ataques maliciosos.

En resumen, la seguridad en un ambiente de red es la habilidad de identificar y eliminar vulnerabilidades. Una definición general de seguridad debe también poner atención a la necesidad de salvaguardar la ventaja organizacional, incluyendo información y equipos físicos, tales como los mismos computadores. Nadie a cargo de seguridad debe determinar quién y cuándo puede tomar acciones apropiadas sobre un ítem en específico. Cuando se trata de la seguridad de una compañía, lo que es apropiado varía de organización en organización. Independientemente, cualquier compañía con una red debe tener una política de seguridad que se dirija a la conveniencia y la coordinación.

La seguridad informática debe establecer normas que minimicen los riesgos a la información o infraestructura informática. Estas normas incluyen horarios de funcionamiento, restricciones a ciertos lugares, autorizaciones, denegaciones, perfiles de usuario, planes de emergencia, protocolos y todo lo necesario que permita un buen nivel de seguridad informática minimizando el impacto en el desempeño de los trabajadores y de la organización en general y como principal contribuyente al uso de programas realizados por programadores.

La seguridad informática está concebida para proteger los activos informáticos, entre los que se encuentran los siguientes:



No solamente las amenazas que surgen de la programación y el funcionamiento de un dispositivo de almacenamiento, transmisión o proceso deben ser consideradas, también hay otras circunstancias no informáticas que deben ser tomadas en cuenta. Muchas son a menudo imprevisibles o inevitables, de modo que las únicas protecciones posibles son las redundancias y la descentralización, por ejemplo mediante determinadas estructuras de redes en el caso de las comunicaciones o servidores en clúster para la disponibilidad.

Las amenazas pueden ser causadas por:

La ingeniería social es la práctica de obtener información confidencial a través de la manipulación de usuarios legítimos. Es una técnica que pueden usar ciertas personas para obtener información, acceso o privilegios en sistemas de información, con resultados similares a un ataque a través de la red, saltándose toda la infraestructura creada para combatir programas maliciosos. Además, es un ataque más eficiente, debido a que es más complejo de calcular y prever.

El principio que sustenta la ingeniería social es el que en cualquier sistema "los usuarios son el eslabón débil".

Existen infinidad de modos de clasificar un ataque y cada ataque puede recibir más de una clasificación. Por ejemplo, un caso de "phishing" puede llegar a robar la contraseña de un usuario de una red social y con ella realizar una suplantación de la identidad para un posterior acoso, o el robo de la contraseña puede usarse simplemente para cambiar la foto del perfil y dejarlo todo en una broma (sin que deje de ser delito en ambos casos, al menos en países con legislación para el caso, como lo es España).

El hecho de conectar un sistema a un entorno externo nos da la posibilidad de que algún atacante pueda entrar en ella y hurtar información o alterar el funcionamiento de la red. Sin embargo el hecho de que la red no esté conectada a un entorno externo, como Internet, no nos garantiza la seguridad de la misma. De acuerdo con el Computer Security Institute (CSI) de San Francisco, aproximadamente entre el 60 y 80 por ciento de los incidentes de red son causados desde dentro de la misma. Basado en el origen del ataque podemos decir que existen dos tipos de amenazas:



El tipo de amenazas según el efecto que causan a quien recibe los ataques podría clasificarse en:

Se pueden clasificar por el "modus operandi" del atacante, si bien el efecto puede ser distinto para un mismo tipo de ataque:

Si en un momento el objetivo de los ataques fue cambiar las plataformas tecnológicas, ahora las tendencias cibercriminales indican que la nueva modalidad es manipular los certificados que contienen la información digital. El área semántica, era reservada para los humanos, se convirtió ahora en el núcleo de los ataques debido a la evolución de la Web 2.0 y las redes sociales, factores que llevaron al nacimiento de la generación 3.0.

Se dice que “la Web 3.0 otorga contenidos y significados de manera tal que pueden ser comprendidos por las computadoras, las cuales -por medio de técnicas de inteligencia artificial- son capaces de emular y mejorar la obtención de conocimiento, hasta el momento reservada a las personas”. Es decir, se trata de dotar de significado a las páginas web, y de ahí el nombre de web semántica o sociedad del conocimiento, como evolución de la ya pasada sociedad de la información.

En este sentido, las amenazas informáticas que viene en el futuro ya no son con la inclusión de troyanos en los sistemas o softwares espías, sino con el hecho de que los ataques se han profesionalizado y manipulan el significado del contenido virtual.

Para no ser presa de esta nueva ola de ataques más sutiles, se recomienda:

El análisis de riesgos informáticos es un proceso que comprende la identificación de activos informáticos, sus vulnerabilidades y amenazas a los que se encuentran expuestos así como su probabilidad de ocurrencia y el impacto de las mismas, a fin de determinar los controles adecuados para aceptar, disminuir, transferir o evitar la ocurrencia del riesgo.

Teniendo en cuenta que la explotación de un riesgo causaría daños o pérdidas financieras o administrativas a una empresa u organización, se tiene la necesidad de poder estimar la magnitud del impacto del riesgo a que se encuentra expuesta mediante la aplicación de controles. Dichos controles, para que sean efectivos, deben ser implementados en conjunto formando una arquitectura de seguridad con la finalidad de preservar las propiedades de confidencialidad, integridad y disponibilidad de los recursos objetos de riesgo.

El proceso de análisis de riesgo genera habitualmente un documento al cual se le conoce como matriz de riesgo. En este documento se muestran los elementos identificados, la manera en que se relacionan y los cálculos realizados. Este análisis de riesgo es indispensable para lograr una correcta administración del riesgo. La administración del riesgo hace referencia a la gestión de los recursos de la organización.
Existen diferentes tipos de riesgos como el riesgo residual y riesgo total así como también el tratamiento del riesgo, evaluación del riesgo y gestión del riesgo entre otras. La fórmula para determinar el riesgo total es:

A partir de esta fórmula determinaremos su tratamiento y después de aplicar los controles podremos obtener el riesgo residual.

El reto es asignar estratégicamente los recursos para cada equipo de seguridad y bienes que intervengan, basándose en el impacto potencial para el negocio, respecto a los diversos incidentes que se deben resolver.

Para determinar el establecimiento de prioridades, el sistema de gestión de incidentes necesita saber el valor de los sistemas de información que pueden ser potencialmente afectados por incidentes de seguridad. Esto puede implicar que alguien dentro de la organización asigne un valor monetario a cada equipo y un archivo en la red o asignar un valor relativo a cada sistema y la información sobre ella. Dentro de los valores para el sistema se pueden distinguir: confidencialidad de la información, la integridad (aplicaciones e información) y finalmente la disponibilidad del sistema. Cada uno de estos valores es un sistema independiente del negocio, supongamos el siguiente ejemplo, un servidor web público pueden poseer la característica de confidencialidad baja (ya que toda la información es pública) pero necesita alta disponibilidad e integridad, para poder ser confiable. En contraste, un sistema de planificación de recursos empresariales (ERP) es, habitualmente, un sistema que posee alto puntaje en las tres variables.

Los incidentes individuales pueden variar ampliamente en términos de alcance e importancia.

Actualmente las legislaciones nacionales de los Estados, obligan a las empresas, instituciones públicas a implantar una política de seguridad. Por ejemplo, en España, la Ley Orgánica de Protección de Datos de carácter personal (LOPD) y su normativa de desarrollo, protege ese tipo de datos estipulando medidas básicas y necesidades que impidan la pérdida de calidad de la información o su robo. También en ese país, el Esquema Nacional de Seguridad establece medidas tecnológicas para permitir que los sistemas informáticos que prestan servicios a los ciudadanos cumplan con unos requerimientos de seguridad acordes al tipo de disponibilidad de los servicios que se prestan.

Generalmente se ocupa exclusivamente a asegurar los derechos de acceso a los datos y recursos con las herramientas de control y mecanismos de identificación. Estos mecanismos permiten saber que los operadores tienen sólo los permisos que se les dio.

La seguridad informática debe ser estudiada para que no impida el trabajo de los operadores en lo que les es necesario y que puedan utilizar el sistema informático con toda confianza. Por eso en lo referente a elaborar una política de seguridad, conviene:

Los derechos de acceso de los operadores deben ser definidos por los responsables jerárquicos y no por los administradores informáticos, los cuales tienen que conseguir que los recursos y derechos de acceso sean coherentes con la política de seguridad definida. Además, como el administrador suele ser el único en conocer perfectamente el sistema, tiene que derivar a la directiva cualquier problema e información relevante sobre la seguridad, y eventualmente aconsejar estrategias a poner en marcha, así como ser el punto de entrada de la comunicación a los trabajadores sobre problemas y recomendaciones en término de seguridad informática.

El activo más importante que se posee es la información y, por lo tanto, deben existir técnicas que la aseguren, más allá de la seguridad física que se establezca sobre los equipos en los cuales se almacena. Estas técnicas las brinda la seguridad lógica que consiste en la aplicación de "barreras y procedimientos" que resguardan el acceso a los datos y solo permiten acceder a ellos a las personas autorizadas para hacerlo.

Cada tipo de ataque y cada sistema requiere de un medio de protección o más (en la mayoría de los casos es una combinación de varios de ellos).

A continuación se enumeran una serie de medidas que se consideran básicas para asegurar un sistema tipo, si bien para necesidades específicas se requieren medidas extraordinarias y de mayor profundidad:

La información constituye el activo más importante de las empresas, pudiendo verse afectada por muchos factores tales como hurtos, incendios, fallas de disco, virus y otros. Desde el punto de vista de la empresa, uno de los problemas más importantes que debe resolver es la protección permanente de su información crítica.

La medida más eficiente para la protección de los datos es determinar una buena política de copias de seguridad o "backups". Este debe incluir copias de seguridad completa (los datos son almacenados en su totalidad la primera vez) y copias de seguridad incrementales (solo se copian los ficheros creados o modificados desde la última copia de seguridad). Es vital para las empresas elaborar un plan de copia de seguridad en función del volumen de información generada y la cantidad de equipos críticos.

Un buen sistema de respaldo debe contar con ciertas características indispensables:

Hoy en día los sistemas de respaldo de información en línea y servicio de respaldo remoto, están ganando terreno en las empresas y organismos gubernamentales. La mayoría de los sistemas modernos de respaldo de información en línea cuentan con las máximas medidas de seguridad y disponibilidad de datos. Estos sistemas permiten a las empresas crecer en volumen de información derivando la necesidad del crecimiento de la copia de respaldo a proveedor del servicio.

Los virus son uno de los medios más tradicionales de ataque a los sistemas y a la información que sostienen. Para poder evitar su contagio se deben vigilar los equipos y los medios de acceso a ellos, principalmente la red.

Tener instalado en la máquina únicamente el "software" necesario reduce riesgos. Así mismo tener controlado el software asegura la calidad de la procedencia del mismo (el software obtenido de forma ilegal o sin garantías aumenta los riesgos). En todo caso un inventario de software proporciona un método correcto de asegurar la reinstalación en caso de desastre. El software con métodos de instalación rápidos facilita también la reinstalación en caso de contingencia.

Los puntos de entrada en la red son generalmente el correo, las páginas web y la entrada de ficheros desde discos, o de ordenadores ajenos como portátiles.

Mantener al máximo el número de recursos de red solo en modo lectura, impide que ordenadores infectados propaguen virus. En el mismo sentido se pueden reducir los permisos de los usuarios al mínimo.

Se pueden centralizar los datos de forma que detectores de virus en modo "batch" puedan trabajar durante el tiempo inactivo de las máquinas.

Controlar el acceso a Internet puede detectar, en fases de recuperación, cómo se ha introducido el virus.

Independientemente de las medidas que se adopten para proteger los equipos de una red de área local y el "software" que reside en ellos, se deben tomar medidas que impidan que usuarios no autorizados puedan acceder. Las medidas habituales dependen del medio físico a proteger.

A continuación se enumeran algunos de los métodos, sin entrar al tema de la protección de la red frente a ataques o intentos de intrusión desde redes externas, tales como Internet.

Las rosetas de conexión de los edificios deben estar protegidas y vigiladas. Una medida básica es evitar tener puntos de red conectados a los "switches". Aun así siempre puede ser sustituido un equipo por otro no autorizado con lo que hacen falta medidas adicionales: norma de acceso 802.1x, listas de control de acceso por "MAC addresses", servidores de DHCP por asignación reservada, etc.

En este caso el control físico se hace más difícil, si bien se pueden tomar medidas de contención de la emisión electromagnética para circunscribirla a aquellos lugares que consideremos apropiados y seguros. Además se consideran medidas de calidad el uso del cifrado (WPA, WPA v.2, uso de certificados digitales, etc.), contraseñas compartidas y, también en este caso, los filtros de direcciones MAC, son varias de las medidas habituales que cuando se aplican conjuntamente aumentan la seguridad de forma considerable frente al uso de un único método.

Proceso lógico y/o físico mediante el cual se elimina información considerada sensible o confidencial de un medio ya sea físico o magnético, sea con el objeto de desclasificarlo, reutilizar el medio o destruir el medio en el cual se encuentra.

Se conoce como hardware confiable a todo dispositivo diseñado para ofrecer una serie de facilidades que permiten manejar de manera segura información crítica. No hay que entender que al ser confiables disponen de mecanismos de seguridad infalibles, tienen sus limitaciones. Lo único que quiere indicar es que aportan ciertas facilidades que mejoran la seguridad y dificultan los ataques. El Trusted Computing Group es un conjunto de empresas que definen especificaciones de hardware con el objetivo de tener plataformas más seguras.

Para mantener un sistema seguro es necesario establecer mecanismos que monitoricen los distintos eventos e informaciones que estén relacionados con la seguridad del sistema. Es muy útil tener una visión centralizada de este tipo de información para así poderla analizar en una sola ubicación. Para ello se han desarrollado sistemas de gestión de información de seguridad (, SIM), encargados del almacenamiento a largo plazo, el análisis y la comunicación de los datos de seguridad, sistemas de gestión de eventos de seguridad ("security event management", SEM), encargados del monitoreo en tiempo real, correlación de eventos, notificaciones y vistas de la consola de la información de seguridad, y finalmente sistemas de gestión de eventos e información de seguridad, los cuales agrupan las funcionalidades de los dos tipos de sistemas anteriores.

Existen organismos oficiales encargados de asegurar servicios de prevención de riesgos y asistencia a los tratamientos de incidencias, tales como el Computer Emergency Response Team Coordination Center del Software Engineering Institute de la Universidad Carnegie Mellon, que es un centro de alerta y reacción frente a los ataques informáticos, destinados a las empresas o administradores, pero generalmente estas informaciones son accesibles a todo el mundo.

El Instituto Nacional de Ciberseguridad (INCIBE) es un organismo dependiente de Red.es y del Ministerio de Energía, Turismo y Agenda Digital de España.

La Comisión Europea ha decidido crear el Centro Europeo de Ciberdelincuencia (EC3) abrió efectivamente el 1 de enero de 2013 y será el punto central de la lucha de la UE contra la delincuencia cibernética, contribuyendo a una reacción más rápida a los delitos en línea. Se prestará apoyo a los Estados miembros y las instituciones de la UE en la construcción de una capacidad operacional y analítico para la investigación , así como la cooperación con los socios internacionales.

El 16 de junio de 2011, el ministro alemán del Interior, inauguró oficialmente el nuevo Centro Nacional de Defensa Cibernética (NCAZ, o "Nationales Cyber- Abwehrzentrum") que se encuentra en Bonn. El NCAZ coopera estrechamente con la Oficina Federal para la Seguridad de la Información ("Bundesamt für Sicherheit in der Informationstechnik", o BSI); la Oficina Federal de Investigación Criminal ("Bundeskriminalamt", BKA); el Servicio Federal de Inteligencia ("Bundesnachrichtendienst", o BND); el Servicio de Inteligencia Militar ("Amt für den Militärischen Abschirmdienst", o MAD) y otras organizaciones nacionales en Alemania. Según el Ministro la tarea primordial de la nueva organización fundada el 23 de febrero de 2011, es detectar y prevenir los ataques contra la infraestructura nacional.

El 1 de mayo de 2009, el senador Jay Rockefeller ( D -WV ) introdujo la "Ley de Seguridad Cibernética de 2009 - S. 773 " (texto completo ) en el Senado , el proyecto de ley, co - escrito con los senadores Evan Bayh (D- IL), Barbara Mikulski (D -MD) , Bill Nelson (D -FL ) y Olympia Snowe (R -ME ) , se remitió a la Comisión de Comercio, Ciencia y Transporte , que aprobó una versión revisada del mismo proyecto de ley (el " Ley de ciberseguridad de 2010 ") el 24 de marzo de 2010. el proyecto de ley busca aumentar la colaboración entre el sector público y el sector privado en temas de ciberseguridad , en especial las entidades privadas que poseen las infraestructuras que son fundamentales para los intereses de seguridad nacionales ( las comillas cuenta John Brennan, el Asistente del Presidente para la seguridad Nacional y Contraterrorismo : " la seguridad de nuestra nación y la prosperidad económica depende de la seguridad, la estabilidad y la integridad de las comunicaciones y la infraestructura de información que son en gran parte privados que operan a nivel mundial " y habla de la respuesta del país a un "ciber - Katrina " .) , aumentar la conciencia pública sobre las cuestiones de seguridad cibernética , y fomentar la investigación y la ciberseguridad fondo. Algunos de los puntos más controvertidos del proyecto de ley incluyen el párrafo 315 , que otorga al Presidente el derecho a " solicitar la limitación o el cierre del tráfico de Internet hacia y desde el Gobierno Federal comprometido o sistema de información de Estados Unidos o de las infraestructuras críticas de la red ". la Electronic Frontier Foundation , una defensa de los derechos digitales sin fines de lucro y la organización legal con sede en los Estados Unidos , que se caracteriza el proyecto de ley como la promoción de un " enfoque potencialmente peligrosa que favorece la dramática sobre la respuesta sobria" .

La UNAM-CERT es un grupo de profesionales que se encargan de evaluar las vulnerabilidades de los sistemas de Información en México.

Las salidas profesionales o laborales de Ciberseguridad son muy variadas y cada vez más demandadas debido a los cambios continuos en plena era digital y debido a los constantes ataques que diariamente sufren empresas, gobiernos y usuarios sobre sus datos. Esto también se ha reforzado debido a las nueva ley de protección de datos de 2018. Entre las salidas profesionales podemos encontrar: 

¿Cuáles son las competencias requeridas para estos puestos?







</doc>
<doc id="8956" url="https://es.wikipedia.org/wiki?curid=8956" title="Rongo rongo">
Rongo rongo

Se conoce con el nombre de rongo rongo a un sistema de escritura descubierto en la isla de Pascua en el s. XIX, tallado primordialmente con puntas de obsidiana, en su mayoría sobre tablillas de madera.

Los habitantes nativos de la isla de Pascua la llamaron también "kohau rongo rongo". La traducción corriente del término "kohau" es madera que sirve para fabricar el casco de las canoas, y "rongo rongo" es ‘gran mensaje’ o ‘gran estudio’. También fue traducido como ‘líneas de recitación’ o ‘báculos recitadores’.

Hay autores que dicen que esta forma de escritura es la única escritura estructurada en toda Oceanía, aunque falta todavía un desciframiento fiable para comprobarlo. Los símbolos o los glifos vienen tallados a lo largo de ranuras hechas con antelación al grabado en los artefactos y son de una altura media entre 9 y 14 mm. Parecen representar gráficamente figuritas de seres antropomórficos en diversas posturas, otras criaturas de fantasía que se asemejan a las aves, a las plantas y a otros animales terrestres y acuáticos, objetos celestes, así como también objetos geométricos, pequeños anzuelos, entre otros.

Los signos que componen los textos están mayormente bien estilizados, tienen casi la misma altura y vienen alineados sin aparente división (espacios blancos o signos de puntuación) entre ellos, formando un tipo de "escritura continua", típica de algunos sistemas de escritura antiguos, p.ej. los textos antiguos de la literatura griega o ciertas muestras del idioma etrusco. Las inscripciones terminan cuando aparece algún "nudo", alguna protuberancia natural u otra irregularidad (por ejemplo fragmentos carcomidos, quemados por el fuego, arruinados por la humedad) sobre la superficie de los objetos o como es de esperar, cuando el espacio físico sobre ellos se agota. El tamaño y la forma de las tablas, cuya edad está aún por determinar con exactitud, son dispares.

Se dice que las tablillas se deben leer a partir de la primera línea del rincón izquierdo del recto y continuar de manera lineal hasta el fin del renglón y luego darle la vuelta para seguir con el próximo. (Sin embargo, el texto inscrito encima del Bastón de Santiago resulta una excepción). No obstante, no está bastante claro si todas las tablillas contienen un documento de carácter unitario o si alguna de ellas podría servir de depósito o colección de documentos diferentes, por lo que el punto de partida de lectura es un asunto pendiente. Observando la fragmentación del texto en secuencias desiguales, hay razones para creer que algunas tablillas retienen esa función. Fischer dice que la tablilla Mamari tiene la apariencia de ser un encadenamiento de varias secuencias de distintas clases Al parecer, Fischer estaba en lo cierto, pues al analizar estructuralmente el texto C, llamado «tablilla Mamari», se observan grupos de secuencias que se repiten en ambos lados del artefacto. Esas distintas secuencias, compuestas en mayor grado de elementos idénticos o semejantes, podrían testificar a favor de «listas», «estribillos» o «fórmulas», tan arraigados y comunes en el folclore antiguo de Rapanui. Varios estudiosos dan cuenta de la posibilidad de listas incluidas en varios objetos "rongo-rongo" en consideración de glifos delimitadores sin valor fonético del tipo 380.1 (3/52), véase Barthel (1958), Horley (2007:28). Verbigracia, esos glifos señalarían el inicio o el fin de oraciones paganas relacionadas con prácticas mágicas, destinadas a capturar prisioneros de guerra y posiblemente tramitar venganza y muerte a los malhechores; no faltarían tampoco secuencias toponímicas u onomásticas insertadas entre dichos delimitadores. Es de esperar que los glifos "rongo-rongo" organizados e incrustados en tales grupos secuenciales, reflejen parte de la cultura pre-cristiana pascuense. Sería descabellado pensar que el escriba derrochara talento, material precioso y escaso —madera— y tiempo para grabar un galimatías de símbolos y bobadas parecidas en la superficie de la tablilla 'Mamari'.

Cuando Jacques B.M. Guy comenta tres tablillas, la de 'Gran Santiago' (Texto H), la de 'Gran Leningrado' (Texto P) y la de 'Pequeña Leningrado' (Texto Q), que él considera que trasmiten «casi exactamente el mismo texto jeroglífico», y las compara con el contenido de la tablilla Tahua (Texto A o ‘el Remo’), observa que esta tiene el aspecto de ser ‘‘una compilación, como colección de textos abreviados, ya perdidos, salvo su principio, encontrados en esas otras tres tablillas’’

Mencionamos de nuevo que el material utilizado para el grabado de los signos "rongo-rongo", el soporte en otros términos, es madera. La elección del soporte está establecida comúnmente por los recursos materiales que el perímetro natural pueda ofrecer a los escribas nativos. En el caso de los antiguos rapanuis, la mayor parte de los signos tallados aparecen sobre madera, y en menor medida sobre material rupestre, como petroglifos, y posiblemente sobre huesos de peces grandes y mamíferos marinos. Si los escribas hubieran grabado en otros materiales perecederos como calabaza o cuero, se espera que con el paso de tiempo, se hubiesen destruido los ‘textos’ que venían allá. En caso de soportes perdurables, como madera dura o piedra, la probabilidad de supervivencia es mayor; no obstante, las condiciones atmosféricas y físicas del entorno combinadas con la violencia ejercida por los humanos, son soberanamente determinantes. Además, habrá que rememorar que el soporte condiciona la forma de los signos. Por tanto, la solidez y la textura de fibras de la leña condicionarían hasta cierto punto la simplificación o sofisticación morfológica de los signos "rongo-rongo" incisos en las tablillas. El material en el que se han grabado los glifos pertenece a varias especies de árboles autóctonos o forasteros. Entre los árboles nativos, se podrían citar el toromiro " (Sophora toromiro) ", el makoi " (Thespesia populnea) ", el hau " (Triumfetta semitriloba) " y el sándalo " (Santalum) ". Así, Métraux (1940:17) comentaba que «la madera de una de las tablillas [rongorongo] en el Museo del Arte Popular en Viena (22869) ha sido analizada y reconocida como Thespesia populnea». En el caso del material ‘forastero’ eso es comprensible, si tenemos en cuenta la escasez de la forestación en la isla de Rapanui, especialmente en un período relativamente tardío de su historia. Los habitantes recogían al azar piezas flotantes de madera a la deriva para poder inscribirlas y seguir así con la tradición.

Para algunos investigadores, estos signos o glifos parecen demostrar la existencia, en el pasado, de una forma de escritura, aparentemente sin antecedente similar en toda Polinesia. No obstante, en Oceanía se da el caso del documento del “Tratado de Waitangi” (Treaty of Waitangi, en inglés) de 1840 firmado por representantes de la monarquía inglesa y un grupo de jefes tribales maoríes, quienes sorprendieron a los presentes mediante «series enteras de símbolos» 1935), describiendo por consiguiente la posibilidad de una escritura emblemática entre los nativos (véase Métraux 1940:400). También existe cierta controversia sobre si la escritura de Rapa Nui surgió de manera independiente —"ex novo"— como en el caso del chino o del sumerio, o la idea de la escritura fue tomada tras contacto con los exploradores europeos, precisamente tras la visita del navío "San Lorenzo" y la fragata "Santa Rosalía", de la Real Armada Española, en noviembre de 1770. Durante aquella expedición española, conocida como Expedición de González de Haedo, los españoles tomaron posesión de la Isla de Pascua, bautizándola como "isla de San Carlos", tras acordarlo con varios jefes indígenas, que firmaron el acta correspondiente con «ciertos caracteres según su estilo», en lo que supuso el primer documento conocido en el que aparece la escritura rongo-rongo.

Se suele teorizar que los signos rongo rongo pueden ser indicadores de un sistema logográfico-fonético, en el cual cada signo o grupo de signos podría representar nombres propios de caciques y su descendencia, distintas actividades bélicas o económicas u otros conceptos relacionados con la cosmogonía pascuense, etc. Los caracteres están grabados en líneas horizontales paralelas. Una de las propiedades de esta escritura es que se trata de inscripciones en «bustrófedon inverso»: mientras en una línea los signos se encuentran en posición normal, en la siguiente se hallan invertidos respecto al renglón previo de modo que, para leer una tablilla, esta debería invertirse cada vez que se inicia una nueva línea. Aunque se desconoce qué significan estos símbolos, se han hecho varios intentos de decodificar lo tallado.

La estudiosa y publicista francesa Catherine Orliac, del Centre National de la Recherche Scientifique (en París), publicó en 2005 en la revista "Archaeology in Oceania" 40:3, el artículo «The rongorongo tablets from Easter Island: botanical identification and 14-C dating» (traducción: ‘las tablillas de rongorongo de la Isla de Pascua: identificación botánica y prueba de radiocarbono’), cuyo significado está vinculado directamente con uno de los asuntos más controvertidos del fenómeno rongo rongo: la edad de las inscripciones. Algunos resultados de su trabajo y la datación realizada en un laboratorio de Miami hicieron saber que la madera de la pequeña tablilla de San Petersburgo tiene datación: 1680-1740, con el método de análisis de las fibras y círculos en la madera y no de carbono 14). Lo que parece favorecer la opinión que las inscripciones preceden la visita de los españoles en 1770, aunque por otro lado se ven menoscabados por la accesibilidad de la autora a todos los documentos diseminados por varios museos del mundo. Así y muy a pesar de los estudiosos, solo uno de ellos, el texto Q, fue analizado mediante la prueba de C-14, imposibilitando como consecuencia la extracción de conclusiones de peso, que podrían obtenerse analizando más manuscritos del corpus.

Seis tablillas y un "reimiro" (pectoral decorativo) grabado con 44 glifos, por ejemplo, los textos G y H, B y C, el texto Q, los textos K y L, siendo el último el reimiro, fueron identificados como incisos en madera de "Thespesia populnea", familia de las Malváceas, conocido en Rapanui con el nombre de "makoi". Esos resultados están en parte conforme con el material de los artefactos, y por otro lado, ellos parecen apoyar lo dicho por Métraux (1940:17) cuando dice que «el árbol es frecuentemente aludido en leyendas y canciones» " (This tree is frequently alluded to in legend and songs) ".

"Thespesia populnea", el árbol-rosal de Oceanía, cuando es joven tiene una ligera coloración rosa y cuando envejece se vuelve de color rojizo oscuro con destellos morados. Puede alcanzar una altura máxima de 15 metros y según el estudioso Zizka (1991:20, 51), citado por Orliac, fue traído a Rapanui por los primeros colonos polinesios en el siglo VIII. Lavachery dice acerca de ese árbol que es ligero, sin consistencia, e igualmente de talla pequeña.

Examinando el ancho de las tablillas para determinar si ellas fueron grabadas en el corte transversal de una rama o de un tronco de "Thespesia populnea", tampoco permitió a la autora llegar a una conclusión. La excepción parece ser la tablilla Mamari que muestra algunos vestigios de los vasos de savia, indicando por tanto que fue incisa en el corte de un tronco de 19,5 cm de diámetro, correspondiendo a un árbol crecido de 15 m de alto. Ese descubrimiento pone interrogativos sobre la edad de dicha tablilla y en un contexto más amplio, sobre la edad de la escritura rongo rongo. Los primeros navegantes europeos, Roggeveen (1722), González (1770), Cook (1774) y La Perouse (1786), no mencionan vegetación alguna de esa altura en sus informes, sino parecen coincidir en la ausencia de árboles grandes en la superficie de Rapanui. Eso podría decirnos que Mamari hubiera sido grabada antes de la desaparición del bosque que antaño cubría la isla, documentada mediante el análisis del carbón de haber ocurrido en la primera mitad del siglo XVII.

El análisis espectrométrico de 20 mg de madera extraídos del objeto en el que viene inscrito el texto Q, alias la tablilla 'Pequeña de San Petersburgo', por la razón que fuera, dio lecturas variadas: a) 1680-1740, b) 1800-1930 y c) 1950-1960. Aquí habría que incluir dos observaciones de la autora: "a") la esperanza de vida de "Thespesia..." llega a los 80 años como máximo y "b") el estado de conservación del árbol en las regiones tropicales no es óptimo debido a la densidad mediana del tronco; por tanto la propensión a la acción de los elementos es muy alta. Métraux (1940:393) también se pregunta en caso de que las tablillas fueran realmente antiquísimas si pudieran haber soportado las condiciones físicas en las que se mantenían. «The wooden tablets could not have been kept for centuries in rain-drenched, thatched huts, or in caves» [Trad: ‘Las tablillas de madera no podían haber sido preservadas durante siglos en chozas con techos hechos de follaje y caladas por la lluvia, o en cuevas’]. En tales circunstancias, los objetos rongo-rongo, solo en condiciones excelentes de conservación, parecidas quizás a las de un museo moderno, podrían gozar de larga o muy larga vida. Otro argumento ofrecido por Orliac respecto a la antigüedad de Mamari, es la presencia del signo 067, llamado en idioma rapanui "«niu»" que en realidad significa ‘nuez de coco’ y que se podría asociar con el árbol "Paschalococos disperta", "naunau opata" en nombre vernáculo, una especie de palmera que tiene el tronco algo hinchado a modo de una botella, emparentada con Jubaea chilensis. La representación de dicho diseño tenía que ser realizado en un tiempo cuando la palmera seguía presente en la isla y estaba ante la vista de los habitantes. Por otra parte, nadie sabe con certeza si dicho signo se relaciona pictográficamente con la palmera, si representa una entidad completamente diferente de la botánica o si implica una sílaba o palabra entera en el lenguaje rapanui.

Lo único cierto de ese trabajo que contiene un análisis xilológico detallado, es la identificación de "Thespesia populnea" como el material usado para las inscripciones. En cuanto a la edad, tras la prueba de radiocarbono, lo dicho por Orliac es desconcertante, «In fact, there is no irrefutable argument enabling one to claim that the small St Petersburg tablet dates to the end of 17th century or the beginning of the 18th, rather than the 19th century» [traducción: ‘de hecho, no hay argumento irrefutable que podría permitir a alguien afirmar que la tablilla de San Petersburgo estuviera fechada a finales del 1600 o a principios del 1700, o durante el 1800’].

La escritura "rongorongo" era conocida por los llamados "tangata rongo rongo" o "maorí rongo rongo", personas bien entrenadas en su canto y lectura. Algunas hipótesis ofrecidas proponen que los signos inscritos en las tablillas servían como ayuda, a modo de "memoria technica" al estilo de los pallares de las tribus confederadas moche, de Perú (Larco Hoyle 2001 [1938]), de los cinturones bordados "wampum" de los amerindios iroqueses, de las conchas "caurí" de los Yoruba de Nigeria, etc, para almacenar y recordar cantos religiosos, tradiciones y genealogías. El conocimiento del significado verdadero de las inscripciones se perdió cuando esclavistas provenientes de Perú se llevaron de Rapa Nui, entre los años 1862 y 1863, a gran parte de los hombres en edad de trabajar para la labor penosa de la extracción y explotación de guano en las islas Chincha de Perú. Sin embargo, su destino final parece haber sido el de braceros y de siervos trabajando para los terratenientes peruanos del continente. Entre ellos, es de suponer, iban los tuhunga tā, los expertos versados en la tradición sabia de kohau rongo-rongo y cuando estos murieron lejos de su tierra ancestral, su conocimiento parece haberse perdido irremisiblemente. De manera paralela, el padre Sebastián Englert (1948) ha comentado «estos conocimientos han bajado a la tumba con los tangata manu, hombres sabios en ciencia antigua».

Los entendidos en la materia dicen que quedan veinticinco objetos de madera auténticos en total, conservados en varios museos del mundo, más la reproducción de un objeto destruido, que contienen signos rongo rongo: catorce tablillas completas, nueve fragmentos de tablillas, dos reimiros (pectorales decorativos), uno siendo el llamado «reimiro de Londres 9295», texto L con unos 44 glifos incisos y el otro, un reimiro de un signo compuesto (un diglifo), conocido como el Reimiro de Londres 6847, texto J, un bastón de cacique, el Bastón de Santiago (alias, the Santiago Staff) que contiene el mayor número de glifos tallados, casi unos 2320 de ellos (según Fischer 1997) y una estatuilla esculpida en madera, conocida como tangata manu, el hombre-pájaro’. Sin embargo, se sabe que hubo muchas más inscripciones porque el primer misionero en Rapa Nui, el Hermano Eugène Eyraud (1820-1868), describió la existencia de centenares de tablillas y varas grabadas en un informe enviado a su superior en diciembre de 1864.

«Dans toutes les cases on trouve des tablettes de bois ou des bâtons couverts de plusieurs espèces de caracteres hiéroglyphiques: ce sont des figures d'animaux inconnues dans l'île, que les indigènes tracent au moyen de pierres tranchantes» [traducción: “En todas las chozas se encuentran tablillas de madera o varas cubiertas de muchos tipos de caracteres jeroglíficos: esas son figuras de animales desconocidos en la isla que los nativos los graban mediante piedras afiladas (puntas de obsidiana) ’].

Los pascuenses, diezmados por las enfermedades y la esclavitud (a finales del siglo XIX quedaban solo unos 200 nativos en la isla), otorgaron poderes mágicos a las tablillas, tanto beneficiosos como malignos, pero algunos misioneros, considerándolas ‘satánicas’ lograron convencer a buena parte de sus poseedores para que las utilizaran como combustible para calentarse o que se deshicieran de ellas de otras maneras. Otras simplemente se pudrieron en las cuevas donde estaban escondidas. Actualmente, la probabilidad de encontrar una auténtica tablilla rongorongo es prácticamente nula. Los objetos originales conservados en los museos son de valor incalculable y una fuente de importancia primaria para el probable desciframiento de ese sistema de escritura único en el mundo.

En teoría, cada escritura concebida por los seres humanos se podría esclarecer por otros humanos. Sin embargo, las cosas no son tan fáciles, puesto que para el desciframiento de un sistema de escritura no hacen falta solo talento, conocimientos profundos, dedicación y mera suerte. A la hora de confrontarse con los “Kohau RongoRongo”, el problema resulta del todo espinoso debido a unos factores de naturaleza objetiva. Las razones que influyen a afirmar lo de arriba, son:


En el curso de 140 años el kohau rongo rongo ha sido objeto de investigación intensa, así como objeto de un debate encendido por parte de mucha gente, tanto de formación sólida académico (lingüístico, epigráfico, etnológico y antropológico), como de gente aficionada a los enigmas y con una imaginación particularmente alta, propensa a juicios temerarios y explicaciones estrafalarias de las más variadas. Teniendo presente la historia de las investigaciones y las disputas relacionadas con el sentido del kohau rongo rongo, es de esperar que las discusiones serias y las anodinas y de poca trascendencia, sigan existiendo también en un futuro cercano. Muchos de los intérpretes, descifradores y estudiosos parecen y tienen una inclinación a disentir de forma constante respecto a la metodología y al significado concreto de los signos rongo rongo. Parece que, al igual que en otros casos de desciframientos exitosos (el Egipcio antiguo, el Lineal B, el Maya) o en los casos de desciframientos aún no acertados (el etrusco, el Lineal A, la escritura del Indo, el meroítico, el Disco de Festos), las disputas y la rivalidad son parte indispensable del proceso.

De momento, se estima que la tarea discreta de entender las inscripciones del kohau rongo rongo desborda los intentos realizados hasta la fecha, si bien inversamente se podría predecir con algo de optimismo que los variados trabajos serios tenderán a contribuir paulatinamente y a la larga a su comprensión.

Abajo, viene una lista incompleta que incluye a las personas que han contribuido con diversos estudios, destinados en principio, a la aclaración y el desciframiento del fenómeno "rongo rongo".

Eugène Eyraud vivió solo cuarenta y ocho años (1820-1868) y sin embargo, su nombre se relaciona históricamente con la información que dio al mundo acerca de la existencia de los objetos "rongorongo" Dicha noticia está registrada en la carta que mandó en diciembre de 1864 al superior de la orden religiosa de la que formaba parte. El Hermano laico Eyraud llegó a la Isla de Pascua a principios de enero de 1864, enviado por la Congregación de los Sagrados Corazones de Jesús y María (SS.CC). A causa del despecho de los habitantes, la labor evangelizadora no dio tantos frutos como él había esperado, viéndose obligado a dejar la isla en nueve meses. Regresó después en compañía del Padre Hippolyte Roussel para extender la misión cristiana. Sucumbió a consecuencia de la tuberculosis en 1868. Los centenares de objetos rongo rongo, sobre los cuales da fe, muestran la existencia de una tradición antigua aún duradera, a pesar de los devastadores efectos de las razzia o correrías de los esclavizadores peruanos, la viruela y otras enfermedades traídas por los repatriados pascuenses y las guerras domésticas entre las tribus. Teniendo presente la carestía de madera en esos años en Rapanui, los centenares de objetos "rongorongo" mencionados —tablillas, bastones, báculos, etc.— por Eyraud, apuntan hacia dos posibilidades, a) la antigüedad de ellos cuando la isla seguía estando cubierta de árboles nativos bien crecidos, suministrando el material para el tallado o el grabado, b) la explotación azarosa de la madera abandonada y/o descarriada por los balleneros, los buques de investigación científica, las embarcaciones de línea y de los navíos militares que solían visitar la Isla de Pascua por aquel entonces (siglos XVIII-XIX). Toda esa riqueza etnográfica y lingüística se reduciría en apenas dos-tres años en dos docenas de objetos que representan el corpus actual de Rongorongo.

En los anales del desciframiento del rongo rongo, Monsignor Jaussen figura como el primer sabio conocido que trató de descubrir el significado que se ocultaba tras sus signos. En 1871 el obispo recopiló ‘el significado’ de muchos de los signos en un cuaderno particular. Para ello, se basó en la lectura de un nativo rapanui que trabajaba entonces como jornalero en Tahití, llamado Metoro Tau’a Ure, quien cantó ante su presencia el contenido supuesto de cuatro tablillas. Las tablillas recitadas eran las de Aruku-Kurenga, Tahua, Mamari y Keiti, que el obispo las tenía desde antes en su posesión. Tras consultar el resultado de las «traducciones», Jaussen quedó algo decepcionado ya que parecían hacer alusiones a la forma exterior de los signos en el mejor de los casos. En relación con las lecturas de Metoro ante el obispo Jaussen, el estudioso suizo Alfred Métraux (1940:396) afirmó que eran "«merely explanatory»" [traducción: ‘meramente explicativas’] pero que "«nevertheless useful for it gives the meaning of designs, the significance of which might otherwise be a puzzle»" [traducción: ‘sin embargo útiles ya que ofrecen el sentido de los signos, cuyo significado, de lo contrario, podría ser un enigma’] (1940: 397). De manera similar, Facchetti (2002:202) es de la opinión que "«Metoro sapeva (in molti casi) riconoscere esattamente l’oggetto raffigurato, ma non era più in grado di dedurne la funzione nel preciso contesto, leggendo tutti i segni come se fossero logogrammi (segni-parola)»" [traducción: ‘Metoro sabía (en muchos casos) reconocer exactamente el objeto configurado, pero no era capaz de deducir la función en el contexto preciso, leyendo todos los signos como si fueran logogramas (signos-palabra)’].

También se podría suponer que Metoro carecía de la predisposición de desvelar el mensaje encerrado en ellas o «lo traducido» podría ser sencillamente habladuría sin sustancia, en el peor de los casos. La competencia de Metoro en el arte del canto de los kohau rongo rongo dejaba mucho que desear. Aún en el presente, dichas lecturas podrían llevar a uno hacia ilusiones que tendrían que ser sustentadas con evidencia indisputable o a un camino tortuoso sin salida si no se procede con extrema precaución a la hora de pedirles consejo.

Otro desciframiento que puede considerarse sin duda alguna inseguro y fracasado es el realizado por el australiano Allen Carroll, médico cirujano de profesión. En 1892, él publicó en la "Revista de la Sociedad Polinesia" el estudio «The Easter Island inscriptions, and the translation and interpretation of them» (‘las inscripciones de la Isla de Pascua, su traducción e interpretación subsiguientes’) en el que ofrecía una presunta traducción tomando por referencia la lengua quechua de los Incas.La 'traducción' implicaba elementos extravagantes y aventureros en la que no presentaba ninguna lista donde contrastase los signos individuales de kohau rongo rongo con su significado real, ni con sus valores fonéticos y sin presentar explicaciones en cuanto a la manera de cómo llegó a descodificarlos.

El estudioso belga y profesor de la Universidad de Louvain Charles Joseph de Harlez de Deulin (1832-1899) dedicó su atención al problema del "rongo rongo" en su libro "L’île de Pâques et ses monuments graphiques" (1895) ["La isla de Pascua y sus monumentos gráficos"] en el que concebía los signos como representación de una escritura jeroglífica al estilo de los glifos maya y los caracteres chinos de las dinastías Shang, Zhou o Dongba. Su parecer, aunque no exagerado, fue sometido a un examen detenido y bastante reservado por los especialistas de la época.

El panorama de la investigación de "rongorongo" no estaría completo sin referirse a un artículo original, escrito en 1904 por el británico Ormonde Maddock Dalton (1866-1945). El ensayo titulado “"Acerca de una Tablilla Inscrita de Madera de la Isla de Pascua"” fue publicado por la revista mensual “MAN,” del Instituto Antropológico de Gran Bretaña e Irlanda.

Teniendo presente un buen número de fantasías y falsedades escritas sobre la naturaleza de la escritura y el contenido de las inscripciones a lo largo de más de un siglo, a uno no le queda más que admirar el estilo escueto de O. M. Dalton a la hora de ofrecer detalles que si bien breves, son correctos y propicios a un análisis científico.

Resumimos algunos de los detalles tras la inspección que realizó O.M. Dalton de la ‘Tablilla de Londres’: 1. Los escribas usaban hasta los bordes biselados de las tablillas para hacer incisiones debido a la escasez de madera. 2. Los ensayos escritos sobre el tema de "rongorongo" están ubicados en distintas publicaciones, a veces de dificultoso acceso. 3. El nivel de estilización de los signos es tal, que uno se pregunta cómo "rongorongo" surgió en un lugar tan solitario y aislado del planeta, i. e. en Rapa Nui. 4. Las tablillas tienen su punto de comienzo de lectura en el rincón izquierdo de abajo. 5. Los canalillos paralelos, tallados sobre la superficie de las tablillas de madera, servían para proteger los signos del desgaste. 6. Reconoce la presencia de elementos reales y abstractos en el inventario de los glifos, sugiriendo su naturaleza “ideográfica.” Se muestra conforme con la presencia de fórmulas, oraciones, genealogías y simples leyendas en el cuerpo de textos y defiende el carácter autóctono polinesio de la escritura. 7. Apoya la hipótesis de la existencia de una serie genealógica en la ‘Tablilla Pequeña de Santiago’ (1904: 3-4), propuesta anteriormente por J. Park Harrison en 1874. Dicha serie fue re-interpretada en 1956 [1957] por los rusos N. Butinov y Y. Knorozov como una secuencia patronímica, estando quizás la conclusión muy cerca de la verdad. 8. Critica el desciframiento del Dr. A. Carroll quien encontró una amalgama de palabras y frases de lenguas americanas en las tablillas, tal como tolteca, quiché y muisca. 9. No es ajeno a la posibilidad de que ciertos grabados rupestres sean similares a los signos "rongorongo", p. ej. el signo del pájaro fragata /600/, alias "MakeMake". 10. Recomienda cautela al referirse a los cantos de Metoro y a las interpretaciones de Ure Vae Iko y respalda el empleo de fotografías de todo el cuerpo disponible de RR y hacer análisis de frecuencia de los signos individuales o de grupos de signos.

Antropóloga y arqueóloga inglesa (1866-1935) que emprendió en compañía de su esposo William Scoresby Routledge una expedición científica a Rapanui, con el fin de estudiar, catalogar y juntar en un compendio, el arte, las costumbres nativas y la ‘escritura’ rongo rongo de los antiguos pascuences.

Durante los años 1914-15 ella consiguió entrevistar a dos informantes ancianos, uno de ellos siendo un leproso llamado Tomenika y el otro, un hombre llamado Kapiera, quienes supuestamente tenían cierto conocimiento de los signos. Las entrevistas, a pesar de su buena voluntad y tenacidad, no fueron muy fructíferas a causa de las contradicciones aparentadas en las palabras de los informantes.

Aun así, Routledge determinó que los rongo rongo eran concebidos como letanías o descripciones repetidas ya que sus sacerdotes-escribas disfrutaban estéticamente de ellas a la hora de ser cantadas. A su juicio, una vez acabada la recolección de datos, los kohau rongo rongo eran una herramienta para poder despertar en la memoria imágenes y oraciones relacionadas con el folclore de los antiguos isleños. Dicho de otro modo, al igual que las cuentas en un rosario o los nudos en un pañuelo, los signos en cuestión ayudaban a una persona particular –y no a cualquiera– a memorizar acontecimientos e historias de antaño. "“No detailed systematic study of the tablets has as yet been possible from the point of view of the Expedition, but it seems at present probable that the system was one of memory, and that the signs were simply aids to recollection, or for keeping count like the beads of a rosary”". [Trad: “Todavía no ha sido posible llevar a cabo un estudio detallado sistemático de las tablillas desde el punto de vista de la Expedición, pero por ahora parece probable que el sistema [de escritura] fuera mnemónico, y que los signos acudieran meramente a la recolección, o para llevar la cuenta tal como en las cuentas de un rosario.”] En otro párrafo, Routledge reitera su idea, diciendo: "“Given, therefore, that it was desired to remember lists of words, whether categories of names or correct forms of prayer, the repetition would be a labor of love, and to draw figures as aids to recollection would be very natural”". [Trad: “Por tanto, ya que era deseado rememorar listas de palabras, fuesen categorías de nombres o formas correctas de oraciones, la repetición sería por amor al arte, y dibujar figuritas que ayudaran a la recolección sería muy natural.”] En 1919, Katherine Routledge publicó los resultados de su investigación en un libro titulado “El Misterio de la Isla de Pascua: La Historia de una Expedición’’.

Algunos investigadores presentes que favorecen la naturaleza mixta logográfica-fonética "escondida" tras los glifos, opinan que su hipótesis es precipitada y necesita ser revisada a rajatabla.

John McMillan Brown publicó en 1924 [1979] el libro "The Riddle of Pacific" [‘el enigma del Pacífico’] como resultado de una estancia de cinco meses en la Isla de Pascua y como resultado de observaciones personales de muchos años con respecto a su cultura. En su obra hay un capítulo entero, pp. 79-96, dedicado a la “"Escritura"” "rongorongo." El Profesor Brown es algo conocido en los círculos académicos lingüísticos y etnográficos por el descubrimiento en la isla de Woleai, Carolinas del Oeste, en 1913, de una forma de escritura desconocida, notablemente silábica, aprovechada por un jefe o cacique de nombre Egilimar (McMillan Brown 1979 [1924]:84) (Riesenberg y Kaneshiro 1960:273-275).

Su descripción del rongo rongo, al igual que el resto de las manifestaciones culturales de la Isla de Pascua, es original, voluble y abigarrada y suscita justificada curiosidad. En resumidas cuentas, los puntos de vista suyos acerca de la “"escritura"” indígena, corresponderían a lo siguiente, a) el aislamiento geográfico, el tamaño medio de la isla y las hostilidades constantes internas serían motivos para que uno no percibiera la necesidad de una escritura como las que se dieron en otras tierras e instituciones dinásticas, p.ej. en Egipto o Mesopotamia, b) la tradición polinesia rebosa de sistemas mnemónicos en las que se registraban cantidades de raciones y unidades alimentarias, plegarias dilatadas, listas larguísimas de divinidades y antepasados, un sinfín de leyendas y tradiciones, muchas de ellas imbuidas de un lenguaje oscuro y metafórico, exigiendo por tanto "una memoria descomunal," c) basándose en los frecuentes patrones de “"komari"” (signo 050 según Barthel 1958) y de las formas múltiples que asume "Make-Make", el dios supremo de los nativos, se inclina a instar “"la sexualidad,"” y la creencia que las tablillas servían de medios para ayudar a la “"concepción"” y a la fertilidad, así como la idea del poder y de la autoridad en dicha isla, reminiscentes de un “"imperio"” antiguo polinesio sumergido y tragado por las olas del Pacífico, d) hace un repaso de los intentos fallidos de desciframiento, e) reproduce el corpus de dos manuscritos importantes ya hechos por Rudolf Philippi, el director del Museo de Santiago de Chili y f) concluye que rongo rongo se relacionaba con un código sacerdotal empleado estrictamente por los prestes para fines ceremoniales. En ese contexto, nos sigue diciendo que se trataba de un sistema de memoria que suscitaba ideas de oraciones, himnos o sortilegios, refiriéndose a la “"repetición perpetua"” de la figura de "Make-Make". Por lo tanto, ese símbolo místico y sagrado provocaría emociones religiosas más que ideas ensambladas y que la escritura sería más bien “"pathográfica"” (que fomenta el pathos) que “"ideográfica".”

Es de interés recordar aquí cuántas de las ideas suyas (algunas bien exageradas) continúan aún debatiéndose entre los estudiosos contemporáneos.

En 1932 el húngaro Vilmos (Guillaume de) Hevesy se dio cuenta de cierto parecido formal entre algunos signos del rongo rongo y los del sistema hipotético de escritura del Valle de Indo (Mohenjo-dāro, Sind, Harappā, Panjāb). En su tiempo, dicha semejanza hasta llegó a atraer la atención del notable orientalista francés Paul Pelliot (1878-1945), quien leyó durante una conferencia ante la Academia francesa de Inscripciones y Caligrafía en 1932, «Note sur les hiéroglyphes de l’Ile de Pâques» (‘apuntes sobre los jeroglíficos de la Isla de Pascua’), el mismo artículo escrito por de Hevesy.

Cabe mencionar que hoy en día esa correlación aparente ha sido descartada por muchos investigadores (Métraux, Guy, Fischer, Facchetti, Parpola, Sproat, Robinson) y ha llegado a ser algo "anecdótico" en la historia del desciframiento arqueológico de los kohau rongo rongo. Considerando la distancia geográfica y el factor tiempo que separan los símbolos gráficos o pictográficos de la ‘escritura’ del Indo y los signos de los kohau rongo rongo de Rapa Nui, resulta dificilísimo, por no decir vano, establecer una conexión significante entre ambos, más allá de sus apariencias.

Por encima, ¿cómo y sobre qué base científica se pueden comparar dos escrituras supuestas que aún permanecen indescifradas? En el caso de la ‘escritura’ del Valle del Indo (alias el proto-índico), aún se desconoce a qué familia lingüística y a qué sustrato pertenece… así que las coincidencias formales entre ellas, por interesantes que sean, resultan injustificadas.

El padre Sebastián Englert (1888-1969) es un caso un tanto peculiar en la historia de Rapa Nui y de los "kohau rongo rongo". Bávaro de nacimiento, entró en el orden capuchino de los frailes franciscanos y en 1935 llegó a la Isla de Pascua para servir en su parroquia. Aunque sin la educación pertinente en antropología y lingüística, sus trabajos son valorados por buena parte de la comunidad científica por su curiosidad, su pasión y la destreza en la lengua rapanui. Entre los más importantes, se podrían mencionar "La Tierra de Hotu Matu’a" de 1940, donde vienen reunidas leyendas antiguas pascuences y otros comentarios interesantes y el "Island at the Center of the World" [‘isla en el centro del mundo’] publicado en 1970 con un capítulo de diez páginas dedicado a las «tablillas inscritas». Englert calificó a "rongo rongo" como «un gran misterio de la isla» (1948) y su postura se puede resumir en los siguientes puntos: 1. era pesimista respecto a su desciframiento por la cantidad mínima de inscripciones auténticas 2. sin el conocimiento del idioma original rapanui, o sea, el proto-rapanui, ya perdido, sería desatinado procurar información de las tablillas y 3. que en ellas no había hechos históricos de relevancia. Hay que mencionar asimismo que el Padre no llevó a cabo ningún análisis estructural de los textos y que personalmente favorecía la idea de la ausencia de sonidos tras los signos.

En su honor se ha edificado en la isla el Museo Antropológico Padre Sebastián Englert para evocar y proteger la rica herencia cultural del pueblo rapanui.

Renombrado etnógrafo, antropólogo y profesor suizo (1902-1963) que llevó a cabo investigaciones acerca de los nativos sudamericanos en varias ocasiones en el transcurso de su vida. En 1934 se embarcó junto con el arqueólogo belga Henry Lavachery en una expedición hacia la isla de Rapanui para estudiar de cerca su cultura y etnografía y comprobar asimismo si la hipótesis del húngaro de Hevesy sobre el presunto vínculo de las ‘escrituras’ del valle del Indo y de Rapa nui, resultaba acertada y válida.

Tras aplicar un método analítico, contando los símbolos rongo rongo y estudiando sus combinaciones
, (1939) llegó a la conclusión que "“If the symbols represented sounds, the same signs would have been combined in the same order whenever a word was repeated. But this seldom happens. The same combinations of the same symbols recur in only very few cases. The individual designs are repeated over and over again but apparently in haphazard order. No clue to a script came from this study."” [Trad: “Si los símbolos representaran sonidos, los mismos signos habrían sido combinados en el mismo orden cuando se repitiera una palabra. Pero eso apenas ocurre. Las mismas combinaciones de los mismos símbolos tienen lugar solo en algunos poquísimos casos. Los diseños individuales se repiten y se repiten aparentemente de manera descuidada. Dicho estudio no dio con ninguna clave de la escritura.”]

Luego, él propuso que las tablillas servían en tanto que recurso "“mnemotécnico, pero que más tarde los nativos se olvidaron de su significado concreto y fueron meramente consideradas como simples ornamentos o símbolos mágicos.”" La idea de que los rongo rongo son una especie de código mnemotécnico que permite registrar información acerca de las tradiciones y los rituales de los antiguos rapanuis, fue apoyada en el segundo decenio del siglo XX por la antropóloga británica Katherine Routledge. Hay que decir que en cierta manera, es posible que Métraux estuviera influido o inspirado en su tiempo por la proposición de Routledge.

Puesto que todavía faltan pruebas fehacientes respecto a la naturaleza verdadera de los kohau rongo rongo, su idea, aunque significante, no viene a parar de momento en beneficio suyo. Su intuición tendría que ser demostrada y justificada mediante pruebas determinantes a su favor o en su contra. Uno se siente obligado a sugerir que el terreno ‘resbaladizo’ en el que tienen lugar las examinaciones y los estudios presentes sobre los kohau rongo rongo, no permite ni confirmar, ni desmentir lo dicho por Métraux.

Más tarde, sin que su hipótesis sirviera de impedimento y viendo el trabajo de los autores rusos
Knorozov y Butinov (1957) respecto a una secuencia que comprendía una lista breve patronímica de ‘aristócratas’ locales rapanuis, Métraux parece haberse retractado de su posición original, a favor de kohau rongo rongo como «un sistema de escritura propio».

José Imbelloni (1885-1967), profesor e investigador italo-argentino de antropología, ha contribuido al estudio de los "rongorongo" con “Las `Tablillas parlantes' de Pascua, monumentos de un sistema gráfico indo-oceánico” publicado en 1951. Lo más positivo de su largo ensayo se podría resumir en la descripción de las "tablillas inscriptas", en notar alomorfos (variantes) en los signos al analizar los textos, restándoles importancia a su significado básico, en observar "un lenguaje del gesto" en algunos de los glifos, en comentar las "tentativas de traducción" y apuntar hacia faltas y flojedades que caracterizaban algunos modelos, mientras enfatizaba justamente el "método combinatorio", o el análisis contextual y estructural de los signos "rongorongo". Sin poder escapar de la propaganda suscitada por De Hevesy (1932) en cuanto al origen de los "rongorongo" del Valle del Indo, Imbelloni dedica tamaño tiempo a esa teoría en las siguientes páginas de su trabajo. Tras aludir sobre varios aspectos y encontrar pruebas que hoy en día carecen de valor científico, termina por mencionar que rongo rongo era una grafía perteneciente al área inmensa indo-oceánica.

Se puede afirmar que el epigrafista y el etnógrafo alemán Thomas S. Barthel es una de las figuras de más prominencia en la historia del desciframiento del rongo rongo. En 1958 se publicó "Grundlagen zur Entzifferung der Osterinselschrift" (fundamentos para el desciframiento de la escritura de la isla de Pascua), su obra fundamental, en la que Barthel registraba los signos en un sistema de nomenclatura, describía las propiedades de los glifos y trataba de lograr un desciframiento factible…

Hay que decir que la técnica de "descifrar" de Barthel se basaba en la descripción de la forma externa de los glifos, haciendo numerosas referencias a los rituales y a la mitología rapanui, y no sobre la base de un análisis sólido contextual, ni comparativo. Esencialmente, Barthel estableció ‘asociaciones mitológicas’ sin ofrecer evidencia explícita acerca de la manera de conseguir un valor fonético para los signos en cuestión. Aparentemente, esa deficiencia tiene que ver con el hecho de que Barthel tomó como punto de referencia la “Lista de Jaussen” arriba mencionada.

A pesar de la óptica metafórica, el "“Fundamentos…”" de Thomas S. Barthel representa un hito notable para todos los descifradores en potencia, debido a su valor heurístico en el descubrimiento del significado de los signos rongo rongo. El catálogo de signos de Barthel (1958) es utilizado aún por los investigadores, si bien algunos de sus rasgos ambiguos, se están mejorando ligeramente o se están reemplazando de modo continuo por investigadores conscientes a lo largo y ancho del mundo.

La investigación de Barthel que le llevó a un número de conclusiones respecto a la escritura antigua pascuence, se podría resumir en los siguientes puntos:


Fue el investigador Colombiano que luego de 20 años de estudio revela la clave ideográfica que le permitió hacer la traducción de los escritos, publicando un libro ""Una Teoría Interpretativa de la Escritura Pascuense"". Al dar a conocer los resultados de sus investigaciones, estas merecieron la acogida del Museo Británico de Londres, el Bishop Musseum de Honolulú y del Museo del Hombre en París, del cual fue miembro titular.

Presentó su trabajo al Primer Congreso Internacional Isla de Pascua y Polinesia Oriental en Hanga Roa, la Isla de Pascua, en 1984.

La “Escuela rusa”, la más productiva, parece haber seguido la pista del trabajo anterior de Mikluho-Makhlai (1846-1936), el gran polifacético y explorador ruso. En el verano de 1940, el joven Boris Kudryavtsev junto con dos de sus amigos durante una visita escolar en el Museo de Leningrado (actualmente, San Petersburgo), llegó a distinguir algunos pasajes que se repetían de forma más o menos idéntica en cuatro tablillas diferentes, en las de “Tahua”, la de “Gran San Petersburgo”, la de “Pequeña San Petersburgo” y la de “Gran Santiago”.

La importancia de dicho descubrimiento en relación con estos grupos compartidos de signos tiene que ver con el lenguaje subyacente y su posible estructura morfo-sintáctica que podrían servir de asistencia en la tarea del desciframiento. De esa manera, las variaciones en las secuencias notadas o en algunos de los signos, sugieren que los escribas rapanuis, aparte de seguir ciertas reglas “sintácticas” a la hora de hacer incisiones en las tablillas, quizás experimentaran con su inventiva y estilo personal en la caligrafía kohau rongo rongo, rechazando la idea de una alineación mecánica y transposición de símbolos de una tablilla a otra/s. Si aceptamos la hipótesis de que la escritura kohau rongo rongo es un sistema mixto logográfico-fonético, los patrones en cuestión con una ortografía ligeramente distinta, "podrían" señalar hacia una lengua modelada conforme una morfología derivativa o flexiva, dando fe de reglas productivas, o señalar hacia una lengua de estructura sintáctica, dando fe de un orden determinado de palabras.

De todos modos, la historia de evolución del diseño de los glifos en los textos conservados de kohau rongo rongo resulta todavía oscura y las fronteras divisorias entre las series de signos son borrosas. Eso hace que uno sea bastante cuidadoso con lo dicho arriba, al menos hasta que se ofrezcan —en un futuro— pruebas indisputables de traducción de documentos rongo rongo.

Las observaciones de Kudryavtsev fueron publicadas en 1947 por su guía académico, D. A. Olderogge, quien opinaba que rongo rongo se parecía "“al sistema de escritura jeroglífico egipcio en una fase temprana”" de desarrollo (Robinson 2002:231).

Los investigadores Yuri Knorozov y Nikolai Butinov publicaron en 1956 en la revista “Etnografía Soviética” un artículo perspicaz sobre una secuencia de signos que aparece en dos renglones del verso de la tablilla de “Pequeña Santiago” que fue segmentada en grupos menores.

Seis de esos grupos se dividen con el signo ‘antropomórfico’ 200 (según el catálogo de Barthel, 1958) a modo de “separador”. Refiriéndose a los documentos de Jaussen acerca de tales patrones comunes en la tradición rapanui, ellos sugirieron con sensatez una posible genealogía en la forma de un patrónimo (apellido familiar heredado del padre), textualmente: "el Rey A, padre de B // el Rey B, padre de C // el Rey C, padre de D// el Rey D." o a la inversa, "el Rey D // D, hijo del Rey C // C, hijo del Rey B // B, hijo del Rey A //."

Es una lingüista con sede en San Petersburgo, cuya meta permanente ha sido la reconstrucción de "“la antigua lengua rapanui”", según la cual se han compuesto presuntamente las inscripciones rongo rongo. El objetivo concreto de Fedorova ha sido establecer un vínculo directo entre dicha lengua y una secuencia en los textos de kohau rongo rongo. Si esto funcionara, se puede suponer que va a ser tan impresionante como el trabajo abstracto de Michael Ventris a la hora de asignar valores fonéticos a los signos del sistema de escritura del “Lineal B”.

Pozdniakov, lingüista y especialista en lenguas africanas afincado en París, ha contribuido a los estudios sobre el rongo rongo con un artículo único: «Les bases du déchiffrement de l'écriture de l'île de Pâques» (Las bases del desciframiento de la escritura de la isla de Pascua), editado en 1996 en "Journal de la Société des Océanistes" (revista de la sociedad de oceanistas). Pretende haber identificado un núcleo de signos (unos 120 signos básicos [1996: 301]) ya mencionados por Barthel (en 1958), que se podrían simplificar o descomponer a continuación hasta llegar probablemente a la mitad de ellos, unos 60 en otras palabras. Ese número parece concordar con el de un "“silabario”" puro y como consecuencia, capacitaría para hallar en los textos rongo rongo, elementos silábicos a modo del ‘Hiragana’ japonés. Lo positivo de su investigación es un análisis estadístico para encontrar una correlación entre la frecuencia de los signos grabados en las tablillas y la frecuencia de las sílabas en la lengua rapanui. Su frase asertiva que "“un parallélisme frappant peut etre établi entre la fréquences des signes dans les texts des tablettes et la fréquences des syllables dans la langue rapanui… (1996:301) ”" [Trad: “un paralelismo llamativo se puede establecer entre las frecuencias de signos en los textos de las tablillas y las frecuencias silábicas en la lengua rapanui…”] podría tener sentido y ser real. A la par, sugiere en la práctica una idea prometedora para entender mejor el significado de las inscripciones de kohau rongo rongo y por tanto, su mensaje implícito.

Pero por otra parte, la coincidencia y la superposición entre las sílabas de “Apai” (un canto de la tradición rapanui grabado entre los otros en 1886, por el pagador del buque estadounidense "Mohican", William Judah Thomson y una serie de glifos rongo rongo extraídos por el mismo Podzniakov —desafortunadamente— todavía no se ha llevado a cabo o al menos no se ha publicado.

Jean-Michel Schwartz publicó en 1973 en francés un libro de proporciones modestas llamado “"Nouvelles Recherches sur L’Ile de Paques"” [Nuevas Investigaciones sobre la Isla de Pascua]. El propósito del libro es descifrar un manuscrito que consta de trece renglones y de unos 250 símbolos aparentes de "rongorongo" poseído y compilado antaño por un informante nativo pascuence de nombre Tomenika.

Según lo que hace constatar Schwartz, los descubrimientos realizados por él son “"impresionantes ",” véase también Fischer (1997:246). En dicho documento, el autor parece haber dado con el significado de las estatuas gigantes “Moái” y su modo de transporte, su orientación topográfica, el culto del “hombre-pájaro,” el origen de los tallados en madera “"moai kavakava, "” el ritual relacionado con el nacimiento de un niño, la procedencia de los pascuences, la transmisión de la escritura, los cantos genealógicos y el culto de los antepasados. Aparte de eso, en el capítulo 8, Schwartz se aventura a comparar la escritura rongo rongo con la antigua escritura china que como él relata “"abre perspectivas inesperadas."” En ese contexto, uno nota la “"analogía formal"” de signos y una “"similitud inquietante del significado"” (Schwartz 1973:106) entre los signos bajo comparación, los "rongorongo" y los chinos y Schwartz organiza su teoría en tres secciones: semejanza de formas con significado diferente; semejanza de formas con significado idéntico; analogía completa. Cita al etnógrafo austríaco Robert von Heine-Geldern quien había llamado la atención respecto a las fuertes semejanzas formales entre algunos caracteres arcaicos chinos de la época Chang y los glifos pascuences. Uno no puede pasar por encima la influencia de lo especulado por de Hevesy años antes en cuanto a la relación de la escritura de la Isla de Pascua y la de la civilización del Valle del Indo.

Jacques B.M. Guy es un políglota y un lingüista francés de relieve. Ha hecho varias contribuciones significativas en ese campo a través de los años, siendo las más importantes a) la mejora de algunos de los signos del sistema de transliteración de Barthel (1958), b) la importancia de identificar la naturaleza de una forma de escritura desconocida (en ese contexto, los ‘kohau rongorongo’) para poder segmentar sus textos indivisos y no incurrir en juicios arbitrarios, ni incidentales, y c) tras la observación perspicaz de Barthel que unos símbolos reiterativos del rongo rongo en una sección de la tablilla “Mamari” tenían que ver con una especie de calendario lunar, Guy ofreció argumentos que esa parte de glifos corresponde en esencia a un canon astronómico que trataba de "“the topic of intercalary nights”" [Trad: “el tema de noches intercaladas”] (Guy 1990:145). Los investigadores serios admiten que esa parte de signos ha sido fijada terminantemente y sin lugar a dudas en todo el corpus de kohau rongo rongo. Guy, también es de la opinión que el desciframiento del rongo rongo parece improbable, dada la escasez de documentos originales y consecutivamente, su contraste significativo y análisis comparativo en un contexto más amplio. No obstante, reconoce todos los intentos realizados sobre una base estrictamente científica y parece apoyarlos a su manera.

Steven R. Fischer, lingüista por formación y descifrador y publicista por vocación, despierta suficiente interés aquí por su aclamado desciframiento de una secuencia particular de signos rongo rongo. Es conocido también por la publicación de una obra mastodóntica titulada "RongoRongo, the Easter Island Script: History, Traditions, Texts" (‘rongorongo: el sistema de escritura de la isla de Pascua; historia, tradiciones, textos’), donde recopila muchísima información valiosa.

Fischer en 1995, en su artículo "Preliminary Evidence for Cosmogonic Texts in Rapanui’s Rongorongo Inscriptions" (Evidencia preliminar sobre textos de cosmogonía en las inscripciones rongorongo de Rapanui”) publicado en la "Revista de la Sociedad Polinesia" expone esa secuencia identificada, conocida como la tríada “X1YZ”, presuntamente hallada en un «recital de cópulas» en el texto inscrito sobre el “Bastón de Santiago”. Esa fórmula particular consiste en
Dicha fórmula, a juzgar por las apariencias, era compatible con un modelo repetitivo encontrado en el canto indígena "Atua-mata-riri" (‘ojos enfadados de Dios’), o sea, ‘tal-y-tal copulando con esa-y-aquella [dieron a luz] a ese/esa o aquel/aquella’. Fischer se aprovechó con perspicacia de tal aparente coincidencia para ver una clara correlación entre lo inscrito en el “Bastón de Santiago” y la recitación “A-M-R”, cuyo tema central son las aventuras amorosas de dioses y diosas nativos y la procreación subsiguiente. Fischer localizó la siguiente estructura triádica 606.076 [‘X1’] + 070 [‘Y’] = 008 [‘Z’], que fue transliterada "“Te manu mau ki ‘ai ki roto ki te ika, [ka pû] te ra‘â”" y seguidamente traducida al inglés: ‘All the birds copulated with the fish: there issued forth the sun.’ [Trad. al español: ‘Todas las aves copularon con los peces, entonces dieron a luz al sol’]. Fischer concluyó que ese modelo procreador estaba presente en la mayoría abrumadora de los textos rongo rongo. Su hipótesis se reforzó de manera adicional mediante otra publicación durante el mismo año en la "Revista de Rapa Nui", «Further Evidence for Cosmogonic Texts in the RongoRongo Inscriptions of Easter Island» (‘Evidencia posterior acerca de textos de cosmogonía en las inscripciones rongo rongo de la Isla de Pascua’). De esa manera, dicha traducción podría servir de clave y de modelo para penetrar una vez y para siempre el resto de los documentos indescifrados rongorongo a modo del «efecto dominó».

A pesar de las buenas ganas de Fischer de contribuir al campo de estudios rongo rongo, parece que hay un buen número de argumentos en contra de su pretensión e hipótesis. De manera similar, hay que decir que varios autores, especialmente Jacques B. M. Guy (1998a, 1998b), han disputado y puesto severas objeciones a lo de Fischer en distintas ocasiones.


El biólogo marino Howard Barraclough Fell, de la Universidad de Harvard, convertido en epigrafista (1911-1994), encontró que "rongorongo" era una especie de escritura secreta. Predijo que las lecturas del informante nativo Metoro Tau‘a Ure se habían realizado en un “"lenguaje artificial de sacerdotes"” y para entenderlas había que reinterpretarlas con la ayuda de las inscripciones halladas en Nueva Zelanda y en otros documentos maoríes [véase Fischer 1997:255-256]. Sin embargo, hay que tener en cuenta que Dr. Fell pretende haber descifrado otros enigmas epigráficos como el Disco de Festos y uno no sabe con certeza por qué los investigadores aún debaten acerca de dichas “"escrituras"” si Dr. Fell ya las hubiera acertado. Uno nota cierta propensión esotérica en su técnica de “'traducción” y eso hace que su trabajo en "rongorongo" se caracterice por la falta de solidez científica.

Otro ‘desciframiento’ ha sido ofrecido por Andis Kaulins. Dice haber identificado en la tablilla de Honolulú 3 (B.3622) una secuencia de signos alineados que «leídos de la derecha a la izquierda» llevan en sí un "“zodíaco astronómico”". Por otro lado, Kaulins reconoce que los signos grabados rongo rongo "“no son ni alfabéticos, ni silábicos, ni puramente jeroglíficos”", sino parte de "“un concepto”". Para dar credibilidad y sustancia a lo que pretende, se refiere al documento ‘firmado’ aparentemente por algunos jefes nativos rapanui y entregado a la partida del capitán español Felipe González y Haedo durante su visita en 1770, quien reclamó el territorio recién descubierto en nombre del monarca Carlos III de España. Kaulins dice que «los pictogramas (o sea los signos rongo rongo) se han vuelto simplemente más y más ‘hieráticos’, o sea ‘cursivos’, como (es de esperar) que la escritura se vuelva así con el paso del tiempo». Si bien teóricamente eso es presumible –recordemos el caso de las tres formas de escritura propias del Egipto faraónico: jeroglífico, hierático y demótico– lo pronosticado por Caulins exigiría pruebas adicionales de documentos kohau rongo rongo ‘suscritos’ o ‘inscritos’ en cursiva para corroborar su derivación de la forma glífica. El estudioso italiano Facchetti (2002:212) da a conocer justamente que "“il fatto che di tale presunto "rongorongo" corsivo non si avrebbe altro esempio all'infuori di questo, il che rende quest'ipotesi un "ad hoc" incredibile.”" [traducción: "el hecho de que de tal presunto "rongorongo" cursivo no haya otro ejemplo salvo éste, hace que dicha hipótesis sea "ad hoc" no creíble.” Facchetti muestra otro argumento cuando dice que el signo 200 o 300 (catálogo de Barthel, 1958) —que conserva su forma original glífica en la suscripción—, contradice el resto de “letras cursivas” en las que viene el documento.

Adicionalmente, hay quienes sugieren que la firma de los indígenas podría ser sencillamente una reacción apurada o un intento de imitar la escritura de los españoles. Otro razonamiento que parece apoyar lo dicho es el siguiente: otras personas profanas o profesionales, podrían ver o captar otros símbolos/mensajes tras “los pictogramas” del documento ‘suscrito’ por el cacique. Con excepción de dos símbolos —el 200 (o 300) y el 51— (catálogo de Barthel, 1958) que figuran también en el conjunto petroglífico de la isla y que son un tanto identificables, el resto se parece más a los garrapatos de un niño y pueden ser interpretados desde numerosas perspectivas. Una persona entrevistada (que desea permanecer en el anonimato), tras visualizar los signos en cuestión, sugirió que "“representan un documento de propiedad de tierras”". Mientras otra persona, sugirió que se trataba de ""un cadáver y de posibles pistas para llevarnos hacia dicho cuerpo muerto”". Dadas las circunstancias, a uno no le queda más remedio que sospechar de la validez de la propuesta de Kaulins.

Martha J. Macri, especialista en antropología lingüística de la Universidad de California en Davis, ha desarrollado una actividad intensa en la materia de las lenguas nativas norteamericanas y los sistemas de escritura de Mesoamérica. Es promotora del MHDP (Maya Hieroglyphic Database Project: proyecto de base de datos de los jeroglíficos mayas). Entre otras cosas, Macri ha publicado un estudio breve titulado «Rongo rongo de la isla de Pascua» en "Sistemas de escritura del mundo" de Peter Daniels y William Bright, donde defiende la hipótesis logográfica-fonética de la caligrafía rongo rongo. Según Macri, el rongo rongo encierra un grupo central de signos —menos de 70— que se combinan entre sí fusionándose para generar la mayor parte del inventario de elementos. Al igual que el ruso Pozdniakov, propone que rongo rongo es, en esencia, un silabario más un número determinado de logogramas, en el que ciertos logogramas como «la luna creciente» o «el lagarto», no forman signos compuestos. Por tanto, es de asumir que los signos se muestren semejantes a los bloques fonéticos (sílabas + vocales), donde cada uno podría representar palabras individuales del idioma antiguo rapanui. Hay que comentar que entre algunos investigadores parece que se ha establecido cierto consenso en relación con esa propuesta que goza de plausibilidad.

Sin embargo Macri, al igual que Pozdniakov, Fischer y otros investigadores, aún no ha ofrecido una lista detallada de valores fonéticos correspondientes a los signos rongo rongo, a la manera de Michael Ventris con el “Lineal B”. Ese terreno de investigación parece prometedor y otros eruditos están concentrando sus esfuerzos para dar con la clave de desentrañar el misterio rongo rongo.

En un artículo editado en el tomo octavo de "Asian and African Studies" (estudios asiáticos y africanos) de 1998 y 1999, titulado "Little Eyes on a Big Trip. Star Navigation as Rongorongo Inscriptions" (‘Ojos pequeños – acerca de un largo trayecto. Navegación estelar en las inscripciones rongo rongo”), Michael H. Dietrich, diseñador y artista gráfico de Estutgardo (Alemania), presenta la hipótesis de que el rongo rongo no contiene textos de naturaleza coherente que impliquen historias y rituales de génesis, cantos religiosos, listas genealógicas, etc. En su lugar, se aventura a predecir que en las tablillas de kohau rongo rongo vienen tallados mapas de orientación de índole astronómica. Los antiguos polinesios tenían fama de ser navegantes experimentados que podrían cubrir enormes distancias en el Pacífico, sin la asistencia de instrumentos y bajo la guía de las estrellas, los vientos, el vuelo de las aves y las corrientes marinas. En ese aspecto, los antiguos rapanuis, al igual que los otros polinesios, pasaban oralmente la información acumulada de una generación a otra. A partir de esos supuestos, Dietrich realizó paralelamente observaciones de cuerpos celestes para dar firmeza a su teoría y luego concluir que las tablillas kohau rongo rongo fueron diseñadas y ejecutadas exclusivamente en tanto que lista de instrucciones para «navegación sideral» a través del Océano Pacífico. El secreto del rongo rongo por tanto, según el investigador alemán, reside en escudriñar la sabiduría polinesia recogida durante los siglos en el campo de la astronomía. 

El investigador alemán de Bremen Egbert Richter-Ushanas, ha ofrecido otra alternativa de desciframiento. Tras analizar dos de los objetos incisos con signos rongo rongo, el ornamento pectoral “Reimiro 2”, alias 'el Reimiro de Londres 9295' y el “Hombre-Pájaro de Nueva York”, nota la presencia de los signos 050 y 051 (catálogo de Barthel, 1958) que tradicionalmente han sido relacionados con símbolos de fertilidad y productividad en el folclore rapanui, representando la vulva o la tierra, en tanto que recipientes del semen y elementos de procreación. A partir de eso y probablemente refiriéndose a lo relatado por Métraux en su informe etnológico (1940:106), «cada muchacha estaba sobre una roca llamada papa-rona, con las piernas separadas y bien abiertas, mientras dos hombres por debajo le examinaban la vulva»" (), Richter-Ushanas se inclina a creer que las secuencias contienen información acerca de "“ceremonias de defloración”", "“circuncisión”" e "“inspección de vulvas”". Todo eso estaba vinculado con actos sagrados de iniciación (conocidos como "take") de las doncellas y jovencitos en estado de pubertad, cumpliendo con su papel designado en la antigua sociedad de Rapa Nui. Abajo viene una muestra de lo traducido:

En muchas sociedades tribales, los ritos simbólicos de iniciación mujeril, relacionados con la toma de consciencia del dolor y de la procreación, eran comunes, elaborados y de suma importancia para su supervivencia entre las adversidades. A medida que los valores fonéticos, asignados por Richter-Ushanas en sólo dos manuscritos particulares rongorongo, no vayan reproduciéndose en el corpus entero para obtener una lectura coherente, se podrá afirmar que la traducción quedará dentro de un ámbito puramente especulativo.

Richard W. Sproat es un experto en lingüística computacional y sistemas de escritura del mundo, primero con base en la Universidad de Illinois en Urbana-Champaign, Estados Unidos, y actualmente trabajando en la Universidad de Sanidad y Ciencias de Oregón, Portland, Oregon, EE.UU. Él es responsable de un estudio importante: "Approximate String Matches in the Rongorongo Corpus". Siguiendo con la tradición de Kudryavtsev y de Guy respecto a la búsqueda de pasajes similares en varias tablillas rongo rongo, descubrió mediante el uso computacional de “datos numéricos de sufijo” [‘suffix array’] correlaciones parciales ulteriores en el corpus de las tablillas rongo rongo existentes. Entre las conclusiones más destacadas de su trabajo, están las siguientes: a) "“…the various forms of the glyph included by Barthel under the same basic numerical code are in fact just variants of the same glyph rather than separate glyphs.”" [Traducción: “varias formas de los glifos incluidos por Barthel en el mismo código básico numérico son, de hecho, sencillamente variantes del mismo glifo, y no glifos separados”. b) "“…the Santiago Staff seems to be an isolate, matching with almost nothing else except itself.”" [Traducción: “el texto del ‘Bastón de Santiago’ “parece ser una reliquia aislada, que no está correlacionada con casi ninguna cosa, excepto consigo misma“]. c) "“…with sufficient assumptions about what may be present, any string can match with any other string, so it's not clear how one would falsify Fischer's claim in the absence of independent evidence.”" [Traducción: “con suficientes suposiciones sobre lo que podría estar presente (en otros textos rongo rongo), cada secuencia puede correlacionarse con cada otra secuencia, así que no queda claro como uno puede falsificar (aceptar o rechazar) la afirmación de Fischer, considerando la ausencia de evidencia independiente”]. Su artículo pone de manifiesto la utilidad de modelos de computación y la relevancia del análisis distribucional, estadístico y comparativo de las secuencias glíficas para avanzar en los estudios rongo rongo. Parece que los éxitos venideros en ese campo estarán enlazados estrechamente y acorde con la aplicación de tales análisis.

Giulio M. Facchetti, profesor e investigador con sede en la Libera Università di Lingue e Comunicazione (IULM), Milán, Italia, publicó en 2002 un libro titulado "Antropologia della scrittura. Con un’appendice sulla questione del rongorongo dell’isola di Pasqua". ("Antropología de la escritura. Con un apéndice sobre el asunto del rongorongo de la Isla de Pascua.”), el cual merece completa atención aquí. Él trata de responder en su apéndice a las exigencias del método científico haciendo todo lo posible para dilucidar algunas cuestiones relacionadas con el kohau rongo rongo. Se puede estimar que tras examinar los pormenores de la interpretación del calendario lunar por Jacques B.M. Guy (1990) y tras encontrar parentescos con otros sistemas de escritura, como el sumerio o el egipcio antiguo, Facchetti (2002:206) es susceptible a creer que "rongorongo" "“sia un sistema di scrittura pienamente sviluppato”", [Trad: “sea un sistema de escritura completamente desarrollado”] en sintonía con tales autores como Guy, Fischer, Pozdniakov y Macri. Otra contribución suya se refiere a las lecturas de Metoro ya recogidas en una lista particular por el obispo Jaussen. A lo largo de los años, la credibilidad de Metoro ha sido debatible dadas las opiniones contrapuestas. Él apoya a Métraux respecto a la aptitud de Metoro para reconocer los signos kohau rongo rongo diciendo "“le letture di Metoro, e soprattutto la lista di segni con relativa interpretazione ricavata da Jaussen …sembrano poter essere utilizzate, in molti casi e con le dovute cautele, per identificare l'oggetto rappresentato dal segno o perfino il suo significato”" (2002:203). [Trad: “las lecturas de Metoro, sobre todo la lista de signos con la relativa interpretación sacada de Jaussen… parecen ser capaces de ser utilizadas, en muchos casos y con la debida precaución, para identificar el objeto representado por el signo o incluso su significado.”] Facchetti realiza una faena especial cuando analiza el trígrafo repetido ‘008.078.711’ (catálogo de Barthel, 1958) que aparece en las secuencias segmentadas por Guy (1990) y etiquetadas con la letra B. Descomponiendo el trígrafo y comparando sus elementos constituyentes con la Lista de Jaussen, él nota que el signo 008 corresponde a ‘ra’à’ [Trad: sol / ‘ahí’: fuego / ‘hetu’u’: estrella, cuerpo celeste] (p. 2); el signo 078 corresponde a ‘higa’ [Trad: caer] (p.10), viendo una "“composición pictográfica… sencillamente para indicar “el desplazamiento de un cuerpo celeste”". Respecto al tercer signo 711, el pez “boca arriba” o “boca abajo”, Facchetti no excluye el valor fonético “hiti” para el, siendo «el nombre de un pez particular» y asimismo «homófono del verbo reaparecer (usado específicamente con la luna y las constelaciones» La coherencia semántica y fonética percibida en ese signo compuesto, empuja a Facchetti a proponer la verificación de ese fonograma en otros contextos.

A diferencia de Guy, Facchetti (2002: 221, 224, 226) se muestra más optimista en cuanto a un desciframiento futuro de los textos kohau rongo rongo. Descartando la idea de una "“clave”" milagrosa, él recurre a cierto paralelismo entre los esfuerzos hechos para comprender la escritura maya y los necesitados para la comprensión y la lectura de kohau rongo rongo. A partir de ello, el estudioso italiano admite un proceso largo y de carácter progresivo donde el análisis combinatorio y comparativo de los signos se da por sentado.

Albert Davletshin del "Centro Knorosov de Estudios Mesoamericanos," de la Universidad Estatal Rusa para Humanidades Moscú, presentó en 2002 un artículo relacionado con la inserción de nombres, apelativos y títulos en tres textos distintivos "rongorongo, " la Tablilla de Pequeña Santiago [verso] (alias «texto G»), el Bastón de Santiago (alias «texto I») y la Tablilla de Honolulú B.3629 (alias «texto T»). Esos textos despliegan secuencias hacinadas en las que se nota la ocurrencia frecuente del signo 076 (véase Barthel 1958). El signo en cuestión apunta hacia un elemento perentorio en un texto o en secciones de texto de carácter homogéneo, cuya naturaleza sigue aún siendo polémica entre los estudiosos.

Se da la circunstancia que las secuencias en los tres artefactos examinados no son idénticas. Según el autor, el Bastón de Santiago presencia el signo 000/199 (la barra vertical que puede tener valor logográfico), casi ausente u omitido en los otros dos textos (véase Melka 2009:38); en los textos del Bastón de Santiago y de la Tablilla de Honolulú B.3629 no aparecen fragmentos de textos no-marcados y en el verso de la Tablilla de Pequeña Santiago, las secuencias glíficas poseyendo el signo 076 vienen distanciadas por segmentos de texto no-marcados por el signo tanteado. Davletshin, bien adiestrado en formas de escritura del mundo, se desvía de las especulaciones inmerecidas y da explicaciones escuetas y estimables sobre los valores del mencionado signo. Tras elaborar la propuesta de Butinov y Knorosov (1957), tras el análisis de las estructuras incrustadas de "rongorongo" y tras referencias a fuentes verídicas académicas, etnográficas e históricas, se ofrece a deliberar que en la Tablilla de Honolulú B.3629 (alias «texto T») hay listas de nombres parcialmente marcados con títulos; en la Tablilla de Pequeña Santiago [verso] (alias «texto G») hay seis sucesiones de nombres, separados por fragmentos textuales no-marcados con el signo 076, mientras que en el documento extenso del Bastón de Santiago (alias «texto I») aparece una lista exclusiva de más de 500 nombres.

Entre otros supuestos del ensayo, el signo 076 viene designado como ‘silábico,’ correspondiendo a “ko,” un marcador de sujeto, siendo revisado así el valor patronímico de Butinov y Knorosov (1957). Ese marcador suele ir en posición inicial, delante de nombres, en genealogías e inventarios onomásticos. Dada la compilación de esos artefactos en donde pueden aparecer tales géneros de kohau rongo rongo como «catálogos de nombres de víctimas y sacrificados», «catálogos de nombres de fugitivos», acatálogos de «nombres de difuntos/sucumbidos"’ y «anales,» lo enunciado por el autor cobra visiblemente fuerza y plausibilidad. En conclusión, hay que decir que la escasez del material "rongorongo" con el signo 076 como atributo de textos distintivos, no obstante, impermeabiliza la identificación rotunda de apelativos y por ende, no posibilita una sostenida autonomía de investigación.

Paul Horley de la Universidad Nacional Yuri Fedkovych Chernivtsi, Ucrania, editó en 2005 en la ‘Revista de Rapa Nui’ el artículo «Allographic Variations and Statistical Analysis of the Rongorongo Script» (traducción: ‘variaciones alográficas y análisis estadístico de la escritura rongorongo’). Él intenta —con denuedo y ambición— abordar los problemas esenciales que presenta el fenómeno kohau rongo rongo hoy en día: 1. simplificación y reorganización del catálogo de Barthel (1958); 2. esclarecimiento de la naturaleza de las inscripciones aportando un total análisis estadístico; 3. teorizar acerca de la edad de las inscripciones y 4. ofrecer sugerencias distintas, p.ej., la forma de grabado y la dirección de la lectura de los glifos; el posible significado del glifo 076 (el supuesto ‘falo’ en la hipótesis de Steven R. Fischer, 1995a, 1995b).


Así, Métraux (1940:355-6) ofrece un modelo de canto folclórico rapanui de ‘Lamentación’.

Lament (Lamentación)

En rapanui,

En inglés,
Traducido al español,

Si bien plausible, esa sugerencia atrevida tiene que ser comprobada por otros estudiosos durante sus investigaciones independientes. Decimos eso, teniendo en cuenta que varios autores, o sea, Knorozov y Butinov (1957), Barthel (1958), Fischer (1997), Guy (1998) han reclamado el significado del signo 076, objeto de discusión permanente en ‘el foro’ de los "rongorongo".

El libro “"Palabras Extraídas de Madera: Propuestas para el Desciframiento de la Escritura de la Isla de Pascua"” de Mari de Laat fue publicado en Holanda en 2009.

El autor ofrece un detallado trabajo en sus 300 páginas sobre la asignación de valores fonéticos y la traducción de tres textos importantes "rongorongo": los de la tablilla ‘"Tahua,"’ de la tablilla ‘"Aruku Kurenga"’ y de la tablilla ‘"Keiti."’ La consideración principal planteada es la de una escritura mayormente silábica, propuesta anteriormente por Macri (1996) y Pozdniakov (1996). La faena en sí misma resulta impresionante, si asumimos el tiempo dedicado y analizamos el tratamiento esmerado del tema. Los resultados obtenidos son otro asunto y su veracidad suscita preguntas difíciles de contestar. Si nos referimos al artículo de Paul Horley (2009a), allá podemos encontrar argumentos claros que le quitan tamaña importancia a lo ofrecido por de Laat (2009).

Otros dos argumentos que se pueden añadir a la materia y que necesitan aclaración son los siguientes: el signo compuesto 380.001 que representa a una ‘persona sentada en perfil llevando una vara’ es leído en la tablilla ‘"Keiti"’ como Taea, un personaje a quien se le imputa el asesinato de su mujer. El compuesto glífico 380.001 ha sido reclamado como determinativo silencioso por varios autores (véase Barthel 1958:304, 309-310; Guy 2006; Horley 2007:27), marcando el inicio de secuencias breves que incluyen listas y otras fórmulas esterotipadas y manifestado en un número de inscripciones rongorongo. Hay siete adicionales textos "rongorongo" en los que se evidencia de modo directo o indirecto, de modo profuso o modesto, el uso del grupo delimitador 380.001. Si la identificación fuera acertada y teniendo en cuenta que las inscripciones existentes de "rongorongo" se deben al puro azar (sea por obsequio, por recolección o por compra), eso significaría que el presunto uxoricida Taea aparece en más del 25% del corpus presente. Si nos remontamos al tiempo del Hermano Eyraud (1864) quien dio a conocer al mundo la existencia de "rongorongo," inscrita posiblemente en centenares de objetos y conservados en las chozas y casas de los nativos pascuences, el hecho discutido, por analogía, se traduciría en lo siguiente: una, o partes de una de cuatro tablillas o copias producidas en las distintas ‘escuelas’ de "rongorongo" implicaría de una forma u otra al impulsivo Taea y sus actos. Dicha propuesta sugiere que Taea era el protagonista de muchos cuentos y narrativa en la antigua Isla de Pascua, amenazándole o incluso arrebatándole protagonismo a la divinidad suprema de los nativos, Makemake, el hacedor del mundo y de muchas de sus criaturas. Resumiendo, los pascuences antiguos tenían mejores cosas que hacer que grabar en un dos por tres el drama de un tal Taea en sus apreciadas piezas de madera, un material bien raro de por sí debido a la deforestación creciente de la superficie de la isla.

El valor fonético ‘"mo"’ asignado al glifo 076, véase de Laat (2009:27-28), asume múltiples traducciones debido al carácter polivalente de la lengua pascuence, condicionada por el inventario reducido de fonemas. Cabe decir que hay tres textos "rongorongo" que forman probablemente un subgrupo dentro del corpus existente, cuya naturaleza distintiva llama particularmente la atención. Son los del "Bastón de Santiago" (texto Ia), el de "Honolulú 3629" (texto T) y el del verso de la "pequeña Tablilla de Santiago" (texto Gv). Esos textos están estructuralmente marcados por repetidas secuencias breves de naturaleza triádica en su mayoría (aunque no faltan para nada las excepciones) que posiblemente concuerden con listas de víctimas y de asesinados en guerras y refriegas (las famosas "kohau îka"), con listas de conjuros y maleficios mágicos (las "timo"), con listas genealógicas, etc. Si reproducimos el supuesto ‘"mo"’ a lo largo y ancho de las tres mencionadas inscripciones en las que aparece 076, uno se pregunta por qué no hay consistencia, ni fonética (según de Laat 2009), ni semántica (conforme el modelo de las tradiciones orales de la isla, véase Thomson 1891, Routledge 1919, Métraux 1940:395, Barthel 1958, Fischer 1997:287, Guy 1998c:109).

La revista "‘Journal of Quantitative Linguistics’" ha publicado un estudio respecto a la detección de género y otros asuntos co-rrelacionados en un número de antiguas inscripciones "rongorongo" de la Isla de Pascua.

Los autores Martyn Harris y Tomi S. Melka (2011a, b) desarrollan un análisis preliminar propuesto por Melka acerca de la posible afinidad (semántica y/o fonética) entre los glifos representando îka (el glifo 700 parecido a un ‘pez’ ["îka"] según la nomenclatura de Barthel 1958) y los ‘pájaros fragata’ (glifo 600 y algunos variantes) en los textos "rongorongo," basándose en una breve secuencia de glifos ornitomorfos en la tablilla "‘Mamari.’" Los estudiosos submiten al análisis textos que contienen listas posibles, tales como "‘Mamari,’" "‘la Menor de Santiago,’" el "‘Bastón de Santiago,’" y la tablilla "‘Honolulú B.3623,’" planteando que fragmentos de esas puede que incorporen listas de "‘víctimas y de asesinados’" (véase Routledge 1919, Knoche 1939, Barthel 1958, Fischer 1997, Guy 1998c, Davletshin 2002) y de conjuros y maleficios conocidos en tiempos pasados como "timo," cuyo propósito era ofrecer amparo de los espíritus malignos y perjudicar y causar la muerte de agresores y malhechores (véase Englert 1948).

Su trabajo adopta una metodología mixta que incluye varias disciplinas científicas, p. ej. etnología, antropología, lingüística de corpus, estadística léxica, y extracción de información para corroborar la hipótesis. Los resultados parecen revalidar su conjetura inicial; no obstante, durante el curso del ensayo ellos se atribuyen la autoría de haber identificado una correlación entre el glifo 6 (el de forma de ‘mano’ ["rima"]) con los glifos 700 y 600, lo cual sugiere un paralelismo con la acción de ‘agarrar’ o ‘capturar,’ si encima se considera el género de esos textos-claves. Es altamente probable que dicho glifo, i.e. 6, corresponda en varios contextos a una partícula posesiva, tal como fue propuesto en un trabajo anterior de Pozdniakov & Pozdniakov (2007).

El artículo es de interés en el sentido de que es multi-disciplinario. De todas maneras, es todavía prematuro afirmar si lo suyo es del todo acertado. Dejemos que el tiempo pase dictamen sobre esa línea de investigación y comprobar si su método es fructífero en términos de contribuir a un desciframiento factible del rongorongo.




</doc>
<doc id="8957" url="https://es.wikipedia.org/wiki?curid=8957" title="William Rivers">
William Rivers

William Halse Rivers (1864 - 1922) fue un psiquiatra y antropólogo inglés, más conocido por su trabajo con los soldados que sufrieron "shell shock" (neurosis de guerra) durante la Primera Guerra Mundial. El paciente más famoso de Rivers fue Siegfried Sassoon, el poeta de guerra inglés. Rivers es famoso también por su trabajo sobre el tema del parentesco.

Rivers nació en 1864 en Kent, en el sureste de Inglaterra. Estudió medicina y después, psicología. Enseñó en la Universidad de Cambridge y se incorporó a la expedición a los estrechos Torres en 1898. Allí, hizo un estudio extenso e importante sobre las poblaciones de Melanesia.

Durante la Primera Guerra Mundial, Rivers trabajó en el Hospital Craiglockhart en Escocia donde usó técnicas psicoanalíticas con los soldados que sufrieron neurosis por la guerra. Sassoon lo conoció en 1917, después se negó a volver a su regimiento, pero lo trataron con compasión hasta que volvió al frente. Rivers se preocupó mucho por la ética de trabajar con los soldados para devolverlos a la guerra y probablemente sus muertes.

Después de la guerra, Rivers publicó los detalles del tratamiento pionero y siguió siendo amigo de Sassoon. Rivers murió repentinamente en 1922.

La vida de W.H.R. Rivers y su encuentro con Sassoon han llegado a libros de Pat Barker, la escritora británica. La trilogía "Regeneration", incluye "Regeneration" (1991), "The Eye of the Door" (1993) y "The Ghost Road" (1995) que ganó el premio Booker el mismo año.



</doc>
<doc id="8964" url="https://es.wikipedia.org/wiki?curid=8964" title="Herramientas de diseño asistido">
Herramientas de diseño asistido

Se denomina herramientas de diseño asistido a un conjunto de herramientas que permiten el diseño asistido por computador. Es frecuente utilizar la sigla CAD, del inglés "Computer Aided Design", para designar al conjunto de herramientas de software orientadas fundamentalmente, pero no exclusivamente, al diseño (CAD), la fabricación (CAM) y el análisis (CAE) asistidos por computadora en los ámbitos científico e industrial.

Inicialmente estos programas se limitaban a pequeñas aplicaciones centradas en el dibujo técnico en dos dimensiones que venían a sustituir el tradicional tablero de dibujo, ya que ofrecía ventajas para la reproducción y conservación de los planos y reducía el tiempo de dibujo, permitiendo además usar elementos repetitivos y agilizar los cambios. Se podría comparar a las ventajas de los primeros procesadores de textos frente a la máquina de escribir.
Sus comienzos se vieron frenados por estar destinados a un grupo de usuarios muy reducido y requerían, además, de un hardware muy potente. Por no hablar de la resistencia de muchos profesionales a adoptar estas tecnologías. Pero su potencial, el incremento de potencia del hardware y la importancia de las empresas que los usaban (entre los que ha destacado la industria de la automoción) permitieron que poco a poco estas herramientas alcanzaran las tres dimensiones y fueran incluyendo curvas complejas, superficies y, finalmente, sólidos. Hasta llegar a los complejos sistemas asociativos y paramétricos que permiten realizar todo el diseño de un automóvil o un avión, someterlos a pruebas de choque, temperaturas, etc., realizar toda la infografía de marketing, realizar prototipos y, por supuesto, fabricarlos, programando y controlando las máquinas que los fabrican y comprobando después los resultados obtenidos. Todo ello en tiempos impensables hace veinte años.

Actualmente estos sistemas están conectados a los sistemas de gestión y producción de tal forma que ya desde la fase de diseño se puede saber el coste del producto final, controlar los stocks de componentes y materiales para su fabricación y, en fin, todo lo que uno pueda imaginar.

Hemos pasado de tener una representación de un plano en pantalla a tener un modelo virtual del que podemos obtener datos, montar en otros modelos, hacerlo adaptativo, imprimirlo, fabricarlo. El siguiente paso fueron los llamados sistemas expertos que permiten recoger reglas y normas de forma que el sistema guía al usuario en la toma de decisiones. Y ahora se persigue recoger el conocimiento y la experiencia del usuario y que el sistema aprenda, teniendo en cuenta estética, ingeniería, fabricación y calidad.

La evolución de estos sistemas ha permitido avances impresionantes en la industria, de los que hoy se benefician desde los satélites hasta las batidoras domésticas.

Importancia de las empresas que los usaban (entre los que ha destacado la industria de la automoción) permitieron que poco a poco estas herramientas alcanzaran las tres dimensiones y fueran incluyendo curvas complejas, superficies y, finalmente, sólidos. Hasta llegar a los complejos sistemas asociativos y paramétricos que permiten realizar todo el diseño de un automóvil o un avión, someterlos a pruebas de choque, temperaturas, etc., realizar toda la infografía de marketing, realizar prototipos y, por supuesto, fabricarlos, programando y controlando las máquinas que los fabrican y comprobando después los resultados obtenidos. Todo ello en tiempos impensables hace veinte años.



</doc>
<doc id="8969" url="https://es.wikipedia.org/wiki?curid=8969" title="Arquitectura de computadoras">
Arquitectura de computadoras

La arquitectura de computadoras es el diseño conceptual y la estructura operacional fundamental de un sistema de computadoras.Es decir, es un modelo y una descripción funcional de los requerimientos y las implementaciones de diseño para varias partes de una computadora, con especial interés en la forma en que la unidad central de proceso (CPU) trabaja internamente y accede a las direcciones de memoria.

También la arquitectura del computador está basado en tres grandes principios que se aplican a todo dispositivo o componente del computador, estos tres principios son: velocidad, capacidad y tipo de conexión.

También suele definirse como la forma de interconectar componentes de hardware, para crear computadoras según los requerimientos de funcionalidad, rendimiento y costo.

La computadora recibe y envía la información a través de los periféricos, por medio de los canales. La CPU es la encargada de procesar la información que le llega a la computadora. El intercambio de información se tiene que hacer con los periféricos y la CPU. Puede considerarse que todas aquellas unidades de un sistema, exceptuando la CPU, se denomina periférico, por lo que la computadora tiene dos partes bien definidas, que son:

La implantación de instrucciones es similar al uso de una serie de montaje en una fábrica de manufacturación. En las cadenas de montaje, el producto pasa a través de muchas etapas de producción antes de tener el producto armado. Cada etapa o segmento de la cadena está especializada en un área específica de la línea de producción y lleva a cabo siempre la misma actividad. Esta tecnología es aplicada en el diseño de procesadores eficientes. 

A estos procesadores se les conoce como "pipeline processors". Estos están compuestos por una lista de segmentos lineales y secuenciales en donde cada segmento lleva a cabo una tarea o un grupo de tareas computacionales. Los datos que provienen del exterior se introducen en el sistema para ser procesados. La computadora realiza operaciones con los datos que tiene almacenados en memoria, produce nuevos datos o información para el uso externo.

Las arquitecturas y los conjuntos de instrucciones se pueden clasificar considerando los siguientes aspectos:


Son las encargadas de procesar la lógica de las instrucciones del sistema. Existen siete tipos básicos diferentes:


La diferencia básica está en el almacenamiento interno de la CPU. Las principales alternativas son: 


Pero antes hay que tomar en cuenta que las informaciones procesadas son de suma importancia.







</doc>
<doc id="8971" url="https://es.wikipedia.org/wiki?curid=8971" title="Cóntigo">
Cóntigo

Un Cóntigo, en inglés Contig (de "contiguous"), son segmentos de ADN superpuestos, que juntos representan una región consenso de ADN. En la secuenciación de "abajo arriba", un cóntigo refiere a la superposición de datos en la secuencia, en la secuenciación de "arriba abajo", refiere a los clones superpuestos que forman un mapa físico del genoma utilizado para guiar la secuenciación y ensamblaje de este. Dependiendo del contexto, cóntigo puede referir tanto a la superposición de secuencia de ADN, como a la superposición de segmentos físicos (fragmentos) contenidos en los clones.

La construcción física del mapa del ADN a menudo incluye el aislamiento de grandes fragmentos de ADN en clones, como los YAC y los BAC. Estos clones se analizan para determinar cuáles contienen ADN en común, o en otras palabras cuales están superpuestos. Estos clones contiguos constituyen un contig o cóntigo donde los clones adyacentes tienen en común parte de su secuencia. Los "contigs" son importantes porque brindan la posibilidad de estudiar un segmento del genoma completo especialmente cuando se buscan genes con ciertas características y cuando se desea establecer la secuencia de grandes fragmentos de un cromosoma.


</doc>
<doc id="8972" url="https://es.wikipedia.org/wiki?curid=8972" title="Amplificación génica">
Amplificación génica

La amplificación génica es aumento en el número de copias de un fragmento de ADN particular. Una célula tumoral amplifica o copia segmentos de ADN en forma aberrante, como resultado de las señales celulares y en ocasiones debido a daños causados por efectos ambientales. También pueden tener su uso en medicina como técnicas de diagnóstico, reacción en cadena de la polimerasa.

El resultado de este proceso es la producción de varias copias de los genes que se encuentran en una región del cromosoma en lugar de sola, este fenómeno se produce de forma natural durante el ciclo vital de algunos insectos y anfibios, pero en el caso de los mamíferos constituye un hecho no planificado que puede ser provocado por inestabilidad genética en la célula que por lo general se asocia con estados avanzados de malignidad del tumor, aunque también aparezca en tumores benignos.

En ocasiones cuando el nivel de amplificación es elevado, se producen tantas copias de la región amplificada que las copias pueden llegar a formar sus propios pseudocromosomas llamados cromosomas dobles diminutos o miniatura que junto a la presencia de un bandeo cromosómico anormal sirve como identificación de la amplificación génica al microscopio.

Los cromosomas dobles diminutos son pequeños minicromosomas que carecen de centrómeros, formados a partir de las copias de la región de ADN; y el bandeo cromosómico anormal se debe a que la región del ADN que se amplificó permanece en el cromosoma por lo que se transmite de una forma estable durante la división celular, al contrario de lo que ocurre con los cromosomas dobles diminutos.

Este proceso es común en células cancerosas, si un oncogén está incluido en la región amplificada, la sobreexpresión que resulta de ese gen puede provocar un crecimiento descontrolado, además de que puede contribuir a la resistencia a los fármacos en el tratamiento del cáncer.

Izquierdo M.; Biología Molecular del cáncer; Madrid, Ed. Síntesis, 245 pp

https://web.archive.org/web/20080918220742/http://www.cicancer.org/elcancer353.php

http://www.cancerquest.org/index.cfm?page=279&lang=spanish


</doc>
<doc id="8974" url="https://es.wikipedia.org/wiki?curid=8974" title="Eva mitocondrial">
Eva mitocondrial

La Eva mitocondrial, según la genética humana, fue una mujer africana que, en la evolución humana, correspondería al ancestro común más reciente femenino que poseía las mitocondrias de las cuales descienden todas las mitocondrias de la población humana actual, según pruebas de tasas de mutación de genoma mitocondrial.

La Eva mitocondrial recibe su nombre de la Eva que se relata en el libro del Génesis de la Biblia. Sin embargo, ni el nombre de "Eva", ni el término "Eva mitocondrial" fueron empleados por Allan Charles Wilson, Mark Stoneking y Rebecca L. Cann, los autores de la investigación original titulada «ADN mitocondrial y evolución humana», publicada en la revista "Nature", del 1 de enero de 1987. Dicho artículo fue acompañado de su respectiva noticia firmada por Jim Wainscoat, con el título «Fuera del jardín del Edén»" que empezó a proyectar el concepto de "Eva" desde los medios de comunicación. Posteriormente, el 26 de enero de 1987, la revista "Time" publicó un artículo de portada, titulado «Madre genealógica de todos: Los biólogos especulan que "Eva" vivió en el África subsahariana». El propio Allan Charles Wilson prefería el término de "One lucky mother", expresando que el uso del nombre de "Eva" era lamentable. El término concreto de "Eva mitocondrial" apareció por primera vez el 2 de octubre de 1987, en un artículo de la revista "Science" escrito por Roger Lewin, titulado "El desenmascaramiento de la Eva mitocondrial". El malentendido terminó por asentarse en la opinión pública cuando la revista "Newsweek" del 11 de enero de 1988, publicó un artículo titulado «La búsqueda de Adán y Eva», con una representación de Adán y Eva en la portada.

Al seguir la línea genealógica por vía materna de cada persona en el árbol genealógico de toda la humanidad, la Eva mitocondrial correspondería a un antepasado femenino común que comparte toda la población actual de seres humanos ("Homo sapiens").

Basándose en la técnica de reloj molecular, investigaciones recientes (2009) estiman que este ancestro vivió hace aproximadamente 200 000 años, lo que corrobora los primeros cálculos proyectados en 1987. La región más probable en que se originó es el África Oriental.

Una comparación del ADN mitocondrial de distintas etnias de diferentes regiones sugiere que todas las secuencias de este ADN tienen envoltura molecular en una secuencia ancestral común. Asumiendo que el genoma mitocondrial sólo se puede obtener de la madre, estos hallazgos implicarían que todos los seres humanos tienen una ascendente femenina común por vía puramente materna cuando ya habrían existido los primeros y más primitivos "Homo sapiens", tales como el "Homo sapiens idaltu."

Uno de los errores más comunes es creer que la Eva mitocondrial era la única mujer viva en el momento de su existencia y que es la única mujer que tuvo descendencia hasta la actualidad. Estudios nucleares de ADN indican que el tamaño de la población humana antigua nunca cayó por debajo de algunas decenas de miles de personas, y, por lo tanto, había muchas otras mujeres con descendientes vivos hasta hoy, pero que en algún lugar en todas sus líneas de descendencia hay por lo menos una generación sin descendencia femenina pero sí masculina, por lo tanto no se mantuvo su ADN mitocondrial pero sí su ADN cromosómico.

Se sabe de esta Eva a causa del genoma contenido en las mitocondrias (orgánulo presente en todas las células) que sólo se transmite de la madre a la prole. Cada mitocondria contiene ADN mitocondrial, y la comparación de las secuencias de este ADN revela una filogenia molecular.

La Eva mitocondrial es, metafóricamente, una bisabuela que todos compartimos: pero no es la única bisabuela de la que descendemos, pues esto hubiera hecho inviable genéticamente la especie, como en los casos de especies amenazadas. Al trazar con mecanismos genéticos los árboles genealógicos de las diversas poblaciones que habitan el planeta, se van encontrando ramas coincidentes (llamadas haplogrupos) en las diferentes poblaciones; hasta que en cierto momento, en todas ellas, se encuentra una rama común. Esta rama, por el estudio de la antigüedad de las mutaciones genéticas, apunta a una ascendencia mitocondrial africana.

Cuanto más pequeña es una población, más rápidamente converge el ADN mitocondrial; las migraciones de pequeños grupos de personas derivan (en lo que se llama deriva genética) tras unas pocas generaciones hacia un ADN mitocondrial común. Esto sirve como sustento a la teoría del origen común, teoría que plantea que los seres humanos modernos "(Homo sapiens)" se originaron en África hace entre 100 000 y 200 000 años.
Así como las mitocondrias se heredan por vía materna, los cromosomas Y se heredan por vía paterna. Por lo tanto es válido aplicar los mismos principios con estos.
El ancestro común más cercano por vía paterna ha sido apodado Adán cromosómico. Los primeros estudios de genética poblacional del cromosoma Y concluyeron que el Adán cromosómico vivió mucho tiempo después que la Eva mitocondrial, alrededor de unos 60 000 a 142 000 mostrando una discrepancia de más de 50 000 años de diferencia entre ambos individuos. Sin embargo, un estudio realizado por la universidad de Stanford acorta sustancialmente la diferencia temporal entre el Adán cromosómico y la Eva mitocondrial.
El equipo de la Universidad de Stanford, secuenció los cromosomas Y de 69 hombres de todo el mundo y descubrieron cerca de 9000 hasta ahora desconocidas variaciones de la secuencia de ADN en el cromosoma Y. Utilizaron estas variaciones para crear un reloj molecular más confiable y encontraron que Adán vivió hace un mínimo de 120 000 años y un máximo de 156 000 años. Un análisis comparativo de secuencias de ADN mitocondrial de los mismos hombres sugirió que Eva vivió hace 99 000 a 148 000 años. Lo que indica que el Adán Cromosómico existió antes que la Eva mitocondrial y probablemente hayan vivido cerca del mismo periodo de tiempo.







</doc>
<doc id="8979" url="https://es.wikipedia.org/wiki?curid=8979" title="Proteoma">
Proteoma

El proteoma celular es la totalidad de proteínas expresadas en una célula particular bajo condiciones de medioambiente y etapa de desarrollo (o ciclo celular) específicas, como lo puede ser la exposición a estimulación hormonal. También están presentes en virus. El término se utilizó por primera vez en 1994 por Marc Wilkins, para referirse al total de proteínas codificadas por un genoma y ha sido aplicado a diferentes escalas en los sistemas biológicos. También se puede hablar del proteoma completo de un organismo, que puede ser conceptualizado como las proteínas de todas las variedades de proteomas celulares. Es aproximadamente, el equivalente proteínico del genoma.

La proteómica es el estudio del proteoma, que se realizan tradicionalmente mediante la técnica de electroforesis en gel de dos dimensiones: en la primera dimensión, las proteínas se separan por isoelectroenfoque, que separa las proteínas con base en su carga eléctrica; y en la segunda dimensión, las proteínas se separan según sea su peso molecular utilizando SDS-PAGE. El gel se tiñe con azul de Coomassie o con nitrato de plata para visualizar las proteínas; las manchas en el gel son las proteínas que han migrado a una localización específica y eso permite identificarlas.

Según varios científicos podemos resumir al proteoma con la realización de 3 actividades: identificar todas las proteínas elaboradas dentro de una célula en específico, tejido u organismo; determinar como estas proteínas forman redes similares a circuitos eléctricos dentro de los organismos; y por último, determinar las estructuras tridimensionales que adoptan estas proteínas. 

El genetista australiano Marc Wilkins acuñó el término« proteoma» ("proteome") en 1994, en un simposio sobre "2D Electrophoresis: from protein maps to genomes" [Electroforesis 2D: de los mapas de proteínas a los genomas], celebrado en Siena en Italia. Apareció impreso en 1995, con la publicación de parte de la tesis doctoral de Wilkins. Wilkins utilizó el término para describir todo el conjunto de proteínas expresadas por un genoma, célula, tejido u organismo.

La importancia del estudio del proteoma a través de la proteómica radica en que a través de esta podemos comprender las interacciones que tienen las proteínas con un organismo.

Conocer el proteoma de un organismo nos ayuda por ejemplo para el desarrollo de medicinas mediante el estudio comparativo de las proteínas presentes en un organismo sano y uno enfermo (como sería en el caso del melanoma). Para poder desarrollar estos medicamentos se realizan los siguientes pasos:

También se puede emplear para evitar medicamentos con efectos secundarios.




</doc>
<doc id="8981" url="https://es.wikipedia.org/wiki?curid=8981" title="Bustrofedon">
Bustrofedon

Bustrófedon, bustrofedon o bustrofedón (, de ‘buey’ y ‘turno, giro’) designa al tipo de escritura o al modo de escribir que consiste en redactar alternativamente un renglón de izquierda a derecha y el siguiente de derecha a izquierda o viceversa (popularmente, «serpiente»). Aparece en numerosas inscripciones arcaicas, entre ellas las griegas.

La voz proviene del término grave latino "bustrofēdon" "(bustrofédon)", y éste del término agudo griego βουστροφηδόν "(boustrofedón):"
Se refiere a la semejanza de esta manera de escribir con la trayectoria formada en las tierras de labor con el arado tirado por bueyes. A pesar de que la palabra griega es un adverbio, en español este vocablo se suele usar como parte de la locución adverbial «[escrito] en bustrófedon».

El "Diccionario panhispánico de dudas" sostiene que la acentuación esdrújula («bustrófedon») surge de la tendencia a hacer esdrújulas muchas palabras cultas.

Este término también se utiliza en ocasiones para describir el movimiento de ciertas impresoras matriciales de ordenador en las que, a pesar de que el cabezal imprime en direcciones opuestas alternativamente, el texto resultante no aparece en bustrófedon.



</doc>
<doc id="8982" url="https://es.wikipedia.org/wiki?curid=8982" title="Cromatina">
Cromatina

La cromatina es la forma en la que se presenta el ADN en el núcleo celular. Es la sustancia de base de los cromosomas eucarióticos, que corresponde a la asociación de ADN, ARN y proteínas que se encuentran en el núcleo interfásico de las células eucariotas y que constituye el genoma de dichas células. Las proteínas son de dos tipos: las histonas y las .

Las unidades básicas de la cromatina son los nucleosomas. Estos se encuentran formados por aproximadamente 146 pares de bases de longitud (el número depende del organismo), asociados a un complejo específico de ocho histonas nucleosómicas (octámero de histonas). Cada partícula tiene una forma de disco, con un diámetro de 11 nm y contiene dos copias de cada una de las cuatro histonas H3, H4, H2A y H2B. Este octámero forma un núcleo proteico, alrededor del cual se enrolla la hélice de ADN (de aproximadamente 1,8 vueltas). Entre cada una de las asociaciones de ADN e histonas existe un ADN libre llamado ADN espaciador, de longitud variable entre 0 y 80 pares de nucleótidos que garantiza flexibilidad a la fibra de cromatina. Este tipo de organización, permite un primer paso de compactación del material genético, y da lugar a una estructura parecida a un "collar de perlas".

Posteriormente, un segundo nivel de organización de orden superior lo constituye la "fibra de 30 nm", compuesta por grupos de nucleosomas empaquetados unos sobre otros adoptando disposiciones regulares gracias a la acción de la histona H1.

Finalmente, continúa el incremento del empaquetamiento del ADN hasta obtener los cromosomas que se observan en la metafase, este es el máximo nivel de condensación del ADN.

La cromatina fue descubierta en 1880 por Walther Flemming, quien le otorgó este nombre debido a su afinidad por los colorantes. Las histonas fueron descubiertas poco después, en 1884, por Albrecht Kossel. Pocos progresos se realizaron en la determinación de la estructura de la cromatina hasta la década de 1970, cuando se pudieron hacer las primeras observaciones de fibras de cromatina por microscopía electrónica, revelando la existencia del nucleosoma, la unidad de base de la cromatina, cuya estructura detallada fue finalmente resuelta por cristalografía de rayos X en 1997.

La cromatina se puede encontrar en dos formas:

La heterocromatina puede ser de dos tipos diferentes, la riqueza en ADN satélite determina tanto la naturaleza permanente o reversible de la heterocromatina, como su polimorfismo y propiedades de tinción.:
Se ha visto que en la formación de heterocromatina frecuentemente participa el fenómeno de ARN interferente. Por ejemplo, en "Schizosaccharomyces pombe", la heterocromatina se forma en el centrómero, telómeros y en el loci "mating-type". La formación de la heterocromatina en el centrómero depende del mecanismo de ARN interferente (ARNi). ARN doble cadena complementarios son producidos de secuencias repetidas localizadas en el centrómero, que inducen ARNi y seguidamente metilación de la lisina 9 histona 3 y enlazamiento de Swi6 (proteína estructural de la heterocromatina, la cual es homóloga a HP1 en mamíferos).

A pesar de las diferencias descritas anteriormente, la heterocromatina constitutiva y la heterocromatina facultativa tienen propiedades muy similares.

1. La heterocromatina está condensada.
Este es, de hecho, lo que define la heterocromatina, y por ello es aplicable tanto a la heterocromatina constitutiva como a la facultativa. Esta elevada condensación la hace fuertemente cromofílica e inaccesible a la DNAsa I y, en general, a otras enzimas de restricción.

2. El ADN de la heterocromatina se replica más tarde.

La incorporación de varios análogos de nucleótidos muestra que el ADN de ambos tipos de heterocromatina se replica tarde. Esto es el resultado, por un lado, de su elevado grado de condensación, que evita que la maquinaria replicativa acceda fácilmente al ADN y, por otro lado, de su localización en un dominio nuclear periférico pobre en elementos activos.

3. El ADN de la heterocromatina se encuentra metilado.

4. En la heterocromatina las histonas se encuentran hipoacetiladas.
Las histonas puede sufrir una serie de modificaciones post-traduccionales en sus extremos N-terminales que pueden afectar a la propia actividad genética de la cromatina.

5. Las histonas de la heterocromatina se encuentran metiladas en la lisina 9.
La metilación de la lisina 9 de la histona H3 (H3-K9) parece que está muy relacionada con el proceso de heterocromatinización del genoma, tanto en la formación de heterocromatina constitutiva como facultativa.

6. La heterocromatina es transcripcionalmente inactiva.

7. La heterocromatina no participa en la recombinación genética.

Durante mucho tiempo el papel concreto de la heterocromatina ha sido un misterio, ya que su polimorfismo no parecía tener ningún efecto funcional o fenotípico.

1. Papel de la heterocromatina en la organización de los dominios nucleares.

2. Papel de la heterocromatina en la función del centrómero.
En la mayor parte de eucariotas, los centrómeros se encuentran rodeados de una considerable masa de heterocromatina. Se ha sugerido que la heterocromatina centromérica sería necesaria para la cohesión de las cromátidas hermanas y que permitiría la disyunción normal de los cromosomas mitóticos.
Se supone que la heterocromatina centromérica podría, de facto, crear un compartimento mediante el incremento de la concentración local de la variante centromérica de las histonas, CENP-A, y mediante la promoción de la incorporación de la CENP-A en lugar de la histona H3 durante la replicación.

3. Papel de la heterocromatina en la represión génica (regulación epigenética)

La expresión génica puede estar controlada a dos niveles:


Mecanismo de inactivación en cis:

Los reordenamientos cromosómicos pueden provocar que una región eucromática se yuxtaponga a una región heterocromática. En el momento en el que el reordenamiento elimina ciertas barreras que protegen la eucromatina la estructura heterocromática es capaz de propagarse en cis a la eucromatina adyacente, inactivando los genes que se encuentran en ella. Este es el mecanismo observado en la variegación por efecto de posición (PEV) en Drosophila y en la inactivación de ciertos transgenes en ratón.

Mecanismo de inactivación en trans:

Durante la diferenciación celular, ciertos genes activos pueden transponerse a un dominio nuclear heterocromático haciendo que se inactiven. Este mecanismo es el que se ha propuesto como explicación para la co-localización en los núcleos de linfocitos de la proteína IKAROS con la heterocromatina centromérica y de los genes cuya expresión controla.

La cromatina es una estructura dinámica que adapta su estado de compactación y empaquetamiento para optimizar los procesos de replicación, transcripción y reparación del ADN, juega un rol regulatorio fundamental en la expresión génica. Los distintos estados de compactación pueden asociarse (aunque no unívocamente) al grado de transcripción que exhiben los genes que se encuentran en esas zonas. La cromatina es, en principio, fuertemente represiva para la transcripción, ya que la asociación del ADN con las distintas proteínas dificulta la procesión de las distintas ARN polimerasas. Por lo tanto, existe una variada cantidad de máquinas remodeladoras de la cromatina y modificadoras de histonas.

Existe actualmente lo que se conoce como "código de histonas". Las distintas histonas pueden sufrir modificaciones post-traduccionales, como ser la metilación, acetilación, fosforilación, generalmente dada en residuos lisina o arginina. La acetilación está asociada con activación de la trascripción, ya que al acetilarse una lisina, disminuye la carga positiva global de la histona por lo cual tiene una menor afinidad por el ADN (que está cargado negativamente). En consecuencia, el ADN se encuentra unido menos fuertemente lo que permite el acceso de la maquinaria transcripcional. Por el contrario, la metilación está asociada con la represión transcripcional y una unión ADN-histona más fuerte (si bien no siempre esto se cumple).
Por ejemplo, en la levadura "S. pombe", la metilación en el residuo de lisina 9 de la histona 3 está asociado con represión de la transcripción en la heterocromatina, mientras que la metilación en el residuo de lisina 4 promueve la expresión de genes.

Las enzimas que llevan a cabo las funciones de modificaciones de histonas son las acetilasas y desacetilasas de histonas, y las metilasas y desmetilasas de histonas, que forman distintas familias cuyos integrantes se encargan de modificar un residuo en particular de la larga cola de las histonas.

Además de las modificaciones de las histonas, existen también maquinarias remodeladoras de la cromatina, como por ejemplo SAGA, que se encargan de reposicionar nucleosomas, ya sea desplazándolos, rotándolos, o incluso desensamblándolos parcialmente, retirando algunas de las histonas constituyentes del nucleosoma y luego volviéndolos a colocar. En general las maquinarias remodeladoras de la cromatina son esenciales para el proceso de transcripción en eucariotas, ya que permiten el acceso y procesividad de las polimerasas.

Otra forma de marcación de la cromatina como "inactiva" puede darse a nivel de la metilación del ADN, en citosinas que pertenezcan a dinucleótidos CpG. En general la metilación del ADN y de la cromatina son procesos sinérgicos, ya que, por ejemplo, al metilarse el ADN, existen enzimas metiladoras de histonas que pueden reconocer citosinas metiladas, y metilan histonas próximas. Del mismo modo, enzimas que metilan el ADN pueden reconocer histonas metiladas, y así seguir con la metilación a nivel de ADN.

Todas estas modificaciones forman parte de la familia de las modificaciones epigenéticas.

La carga de mutaciones es mayor en aquellas zonas que presentan cromatina reprimida y en regiones donde la replicación es tardía, lo cual se ha observado también en cáncer humano. 

Los métodos de captura de la conformación de los cromosomas (normalmente llamados también tecnología 3C) son métodos utilizados en biología molecular para determinar la organización espacial de la cromatina en la célula. Estos métodos cuantifican el número de interacciones de loci genómicos que están cerca en le estructura 3D, pero pueden encontrase lejos respecto a la secuencia nucleotídica.Se han desarrollado muchos métodos para estudiar estos contactos genómicos y todos comparten unos pasos iniciales:
1. Entrecruzamiento del DNA con formaldehído.
2. Corte del genoma mediante endonucleasas
3. Ligación de los fragmentos de DNA al azar. la ligación de los fragmentos entrecruzados está favorecida frente a los fragmentos sueltos debido a la cercanía de los fragmentos entrecruzados.
4. Posteriormente, se analizan los fragmentos obtenidos mediante diferentes técnicas de PCR.

Dip-c: método de estudio de la estructura de la cromatina basado en una modificación de la técnica anterior de conformación de la cromatina, Hi-C, combinado con una amplificación de todo el genoma con alta cobertura por múltiple amplificación de "end-tagging" llamado META. En concreto, se modifica el paso de ligación con biotina del método Hi-C. Este tipo de técnicas requiere un análisis bioinformático posterior. En Dip-C, se establece un algoritmo con base en la hipótesis del vecindario. Ésta postula que dos haplotipo homólogos deben tener diferentes patrones de contacto y que, por tanto, los haplotipos desconocidos de un cromosoma deben contactar en una región cromosómica cercana o vecina. Los autores definen el vecindario como una superelipse con un exponente de 0.5 y un radio de 10 Mb. 
Este método, a diferencia de los anteriores (3C, 4C, 5C y Hi-C), permite analizar la estructura de la cromatina de células diploides basándose en polimorfismos de un único nucleótido o SNPs. De este modo, se consigue establecer qué haplotipo celular está implicado en cada contacto cromosómico. El estudio llevado con este método confirma que detecta más contactos entre los cromosomas y un menor número de falsos positivos.





</doc>
<doc id="8983" url="https://es.wikipedia.org/wiki?curid=8983" title="Titán">
Titán

Titán hace referencia a varios artículos o puede referirse a:







</doc>
<doc id="8984" url="https://es.wikipedia.org/wiki?curid=8984" title="Clonación de computadoras y programas">
Clonación de computadoras y programas

La clonación de computadoras y programas se refiere a cuando IBM sacó su computadora personal (PC) en 1981 y otras empresas como Compaq decidieron sacar un clon de esta computadora mediante una reconstrucción legal realizada con la documentación de la computadora o retroingeniería. Como la mayoría de los componentes con la excepción del BIOS estaban a disposición del público, todo lo que Compaq tenía que hacer era aplicar un proceso de retroingeniería al BIOS. El resultado era que te llevabas una computadora mejor que las computadoras a los que imitaba por el mismo precio.

También se pueden clonar los programas mediante la retroingeniería o reprogramación legal a través de la documentación u otras fuentes. Programas como el editor de líneas EDLIN de MS-DOS y el sistema operativo Unix han sido clonados. Las razones que inducen a la clonación pueden ser el tener que pagar costosas licencias o como proeza, para demostrar que es posible hacerlo.

El término clonación en el vocablo informático viene asociado exactamente a replicar una información que se encuentra en una zona de memoria a otra zona de memoria, a esto también se le llama "copia". En la clonación de sistemas operativos existe una variante y es la copia o clonación de una zona de memoria de una computadora a otra ya sea por medio del sistema operativo en algún dispositivo de almacenamiento o la manera más usada actualmente, vía red usando el Pre-Boot Execution Environment (PXE) de la tarjeta de red y a estos software se les denomina sistemas de instalación remota.

El popular glosario de jerga en inglés Jargon File nos da la siguiente definición:



</doc>
<doc id="8985" url="https://es.wikipedia.org/wiki?curid=8985" title="Computadora">
Computadora

La computadora (del inglés: "computer" y este del latín: "computare", ‘calcular’), también denominada computador u ordenador (del francés: "ordinateur"; y este del latín: "ordinator"), es una máquina digital programable que ejecuta una serie de comandos para procesar los datos de entrada, obteniendo conveniente información que posteriormente se envía a las unidades de salida. Un computador está formado físicamente por numerosos circuitos integrados y variados componentes de apoyo, extensión y accesorios, que en conjunto pueden ejecutar tareas diversas con suma rapidez y bajo el control de un programa ("software"). 

La constituyen dos partes esenciales , el "hardware", que es su estructura física (circuitos electrónicos, cables, gabinete, teclado, etc.), y el "software", que es su parte intangible (programas, datos, información, documentación, etc.).

Desde el punto de vista funcional es una máquina que posee, al menos, una unidad central de procesamiento (CPU), una memoria principal y algún periférico o dispositivo de entrada y otro de salida. Los dispositivos de entrada permiten el ingreso de datos, la CPU se encarga de su procesamiento (operaciones aritmético-lógicas) y los dispositivos de salida los comunican a los medios externos. Es así, que la computadora recibe datos, los procesa y emite la información resultante, la que luego puede ser interpretada, almacenada, transmitida a otra máquina o dispositivo o sencillamente impresa; todo ello a criterio de un operador o usuario y bajo el control de un programa de computación.

El hecho de que sea programable le permite realizar una gran variedad de tareas, esto la convierte en una máquina de propósitos generales (a diferencia, por ejemplo, de una calculadora cuyo único propósito es calcular limitadamente). Es así que, sobre la base de datos de entrada, puede realizar operaciones y resolución de problemas en las más diversas áreas del quehacer humano (administrativas, científicas, de diseño, ingeniería, medicina, comunicaciones, música, etc), incluso muchas cuestiones que directamente no serían resolubles o posibles sin su intervención.

Básicamente, la capacidad de una computadora depende de sus componentes hardware, en tanto que la diversidad de tareas radica mayormente en el software que admita ejecutar y contenga instalado.

Si bien esta máquina puede ser de dos tipos, computadora analógica o sistema digital, el primer tipo es usado para pocos y muy específicos propósitos; la más difundida, utilizada y conocida es la computadora digital (de propósitos generales); de tal modo que en términos generales (incluso populares), cuando se habla de «la computadora» se está refiriendo a una computadora digital. Las hay de arquitectura mixta, llamadas computadoras híbridas, siendo también estas de propósitos especiales.

En la Segunda Guerra Mundial se utilizaron computadoras analógicas mecánicas, orientadas a aplicaciones militares, y durante la misma época se desarrolló la primera computadora digital, que se llamó ENIAC; ella ocupaba un enorme espacio y consumía grandes cantidades de energía, que equivalen al consumo de cientos de computadoras actuales (PC). Las computadoras modernas están basadas en circuitos integrados, miles de millones de veces más veloces que las primeras máquinas, y ocupan una pequeña fracción de su espacio. 

Computadoras simples son lo suficientemente pequeñas para residir en los dispositivos móviles. Las computadoras portátiles, tales como tabletas, "netbooks", "notebooks", "ultrabooks", pueden ser alimentadas por pequeñas baterías. Las computadoras personales en sus diversas formas son iconos de la llamada era de la información y son lo que la mayoría de la gente "considera" como «computadora». Sin embargo, los sistemas embebidos también constituyen computadoras, y se encuentran en muchos dispositivos actuales, tales como reproductores MP4, teléfonos celulares, aviones de combate, juguetes, robots industriales, etc.

Lejos de ser un invento de una persona en particular, la computadora es el resultado evolutivo de ideas de muchas personas relacionadas con áreas tales como la electrónica, la mecánica, los materiales semiconductores, la lógica, el álgebra y la programación.

Los principales hitos en la historia de la computación, desde las primeras herramientas manuales para hacer cálculos hasta las modernas computadoras de bolsillo.

Las tecnologías utilizadas en computadoras digitales han evolucionado mucho desde la aparición de los primeros modelos en los años 1940, aunque la mayoría todavía utiliza la Arquitectura de von Neumann, publicada por John von Neumann a principios de esa década, que otros autores atribuyen a John Presper Eckert y John William Mauchly.

La arquitectura de Von Neumann describe una computadora con cuatro (4) secciones principales: la unidad aritmético lógica, la unidad de control, la memoria primaria, principal o central, y los dispositivos de entrada y salida (E/S). Estas partes están interconectadas por canales de conductores denominados buses. 

La unidad central de procesamiento (CPU, por sus siglas del inglés: "Central Processing Unit") consta de manera básica de los siguientes tres elementos:


Los procesadores pueden constar de además de las anteriormente citadas, de otras unidades adicionales como la unidad de coma flotante.

La memoria principal, conocida como memoria de acceso aleatorio (RAM, por sus siglas del inglés: "Random-Access Memory"), es un conjunto de celdas de almacenamiento organizadas de tal forma que se pueden acceder numéricamente (dirección de memoria) de manera directa. Cada celda elemental corresponde a un bit o unidad mínima de información. Se accede por secuencias de 8 bits, lo que conforma un byte. Una instrucción es una determinada acción operativa, un comando que indica a la ALU la operación a realizar (suma, resta, operaciones lógicas, etc) . En los bytes de memoria principal se almacenan tanto los datos como los códigos de comandos que se necesitan para llevar a cabo las instrucciones. La capacidad de la memoria viene dada por el número de celdas que contiene, medido en bytes (y sus múltiplos). Las tecnologías empleadas para fabricar las memorias han cambiado bastante; desde los relés electromecánicos de las primeras computadoras, tubos con mercurio en los que se formaban los pulsos acústicos, matrices de imanes permanentes, transistores individuales hasta los actuales circuitos integrados con millones de celdas en un solo chip. Se subdividen en memorias estáticas (SRAM) con seis transistores integrados por bit y la mucho más utilizada memoria dinámica (DRAM), de un transistor y un condensador integrados por bit. La memoria RAM puede ser reescrita varios millones de veces; a diferencia de la memoria ROM, que solo puede ser grabada una única vez.

Los dispositivos de entrada permiten el ingreso de datos e información, en tanto los de salida son los encargados de exteriorizar la información procesada por la computadora. Hay periféricos que son a la vez de entrada y de salida. Como ejemplo, un dispositivo típico de entrada es el teclado, uno de salida es el monitor, uno de entrada/salida es el disco rígido. Hay una gama muy extensa de dispositivos E/S, como teclado, monitor, impresora, ratón, unidad de disco flexible, cámara web, etc.

Las tres unidades básicas en una computadora, la CPU, la memoria y el subsistema de E/S, están comunicadas entre sí por buses o canales de comunicación:

En los modernos computadores, un usuario tiene la impresión de que los computadores pueden ejecutar varios programas "al mismo tiempo", esto se conoce como multitarea. En realidad, la CPU ejecuta instrucciones de un programa y después tras un breve periodo de tiempo, cambia la ejecución a un segundo programa y ejecuta algunas de sus instrucciones. Dado que este proceso es muy rápido, crea la ilusión de que se están ejecutando varios programas simultáneamente; en realidad se está repartiendo el tiempo de la CPU entre los programas, uno a la vez. El sistema operativo es el que controla el reparto del tiempo. El procesamiento realmente simultáneo se realiza en computadoras que poseen más de un CPU, lo que da origen al multiprocesamiento.
El sistema operativo es el programa que gestiona y administra todos los recursos del computador, controla, por ejemplo, qué programas se ejecutan y cuándo, administra la memoria y los accesos a los dispositivos E/S, provee las interfases entre dispositivos, incluso entre el computador y el usuario. 

Actualmente se suele incluir en las distribuciones del sistema operativo algunos programas muy usados; como navegadores de Internet, procesadores de texto, programas de correo electrónico, interfaces de red, reproductores de películas y otros programas que antes se tenían que conseguir e instalar separadamente.

Los primeros computadores digitales, de gran tamaño y coste, se utilizaban principalmente para hacer cálculos científicos. ENIAC se creó con el propósito de resolver los problemas de balística del ejército de Estados Unidos. El CSIRAC, el primer computador australiano, permitió evaluar patrones de precipitaciones para un gran proyecto de generación hidroeléctrica.

Con la fabricación comercial de computadoras, los gobiernos y las empresas sistematizaron muchas de sus tareas de recolección y procesamiento de datos, que antes eran realizadas manualmente. En el mundo académico, los científicos de todos los campos empezaron a utilizar los computadores para hacer sus análisis y cálculos. El descenso continuo de los precios de los computadores permitió su uso por empresas cada vez más pequeñas. Las empresas, las organizaciones y los gobiernos empezaron a emplear un gran número de pequeños computadores para realizar tareas que antes eran hechas por computadores centrales grandes y costosos.

Con la invención del microprocesador en 1970, fue posible fabricar computadores cada vez más baratos. Nació el microcomputador y luego apareció el PC, estos últimos se hicieron populares para llevar a cabo tareas rutinarias como escribir e imprimir documentos, calcular probabilidades, realizar análisis y cálculo con hojas de cálculo, comunicarse mediante correo electrónico e Internet. La gran disponibilidad de computadores y su fácil adaptación a las necesidades de cada persona, han hecho que se utilicen para una variedad de tareas, que incluyen los más diversos campos de aplicación.

Al mismo tiempo, los pequeños computadores de programación fija (sistemas embebidos) empezaron a abrirse camino entre las aplicaciones para el hogar, los automóviles, los aviones y la maquinaria industrial. Estos procesadores integrados controlaban el comportamiento de los aparatos más fácilmente, permitiendo el desarrollo de funciones de control más complejas, como por ejemplo los sistemas de freno antibloqueo (ABS). A principios del siglo XXI, la mayoría de los aparatos eléctricos, casi todos los tipos de transporte eléctrico y la mayoría de las líneas de producción de las fábricas funcionan con un computador. 

Hacia finales de siglo XX y comienzos del XXI, los computadores personales son usados tanto para la investigación como para el entretenimiento (videojuegos), pero los grandes computadores se utilizan para cálculos matemáticos complejos, tecnología, modelado, astronomía, medicina, etc.

Tal vez el más interesante "descendiente" del cruce entre el concepto de la PC o computadora personal y los llamados "supercomputadores" sea la "Workstation" o estación de trabajo. Este término, originalmente utilizado para equipos y máquinas de registro, grabación y tratamiento digital de sonido, ahora hace referencia a estaciones de trabajo, que son sistemas de gran capacidad de cómputo, normalmente dedicados a labores de cálculo científico o procesos en tiempo real. Una Workstation es, en esencia, un equipo de trabajo personal con capacidad elevada de cálculo, rendimiento y almacenamiento, superior a los computadores personales convencionales.

La palabra española «ordenador» proviene del término francés "ordinateur", en referencia a Dios que pone orden en el mundo ("Dieu qui met de l'ordre dans le monde"). En parte por cuestiones de marketing, puesto que la descripción realizada por IBM para su introducción en Francia en 1954 situaba las capacidades de actuación de la máquina cerca de la omnipotencia, idea equivocada que perdura hoy en día al considerar que la máquina universal de Turing es capaz de computar absolutamente todo. En 1984, académicos franceses reconocieron, en el debate "Les jeunes, la technique et nous", que el uso de este sustantivo es incorrecto, porque la función de un computador es procesar datos, no dar órdenes. Mientras que otros, como el catedrático de filología latina Jacques Perret, conocedores del origen religioso del término, lo consideran más correcto que las alternativas.

El uso de la palabra "ordinateur" se ha exportado a los idiomas de España: el aragonés, el asturiano, el gallego, el castellano, el catalán y el euskera. En el español que se habla en América, así como los demás idiomas europeos, como el portugués, el alemán y el neerlandés, se utilizan términos derivados del latín "computare" «calcular».



</doc>
<doc id="8986" url="https://es.wikipedia.org/wiki?curid=8986" title="Córdoba">
Córdoba

Córdoba hace referencia a varios artículos:









</doc>
<doc id="8987" url="https://es.wikipedia.org/wiki?curid=8987" title="Córdoba (España)">
Córdoba (España)

Córdoba es una ciudad y municipio español en Andalucía, capital de la provincia homónima, situada en una depresión a orillas del Guadalquivir y al pie de Sierra Morena. Alberga una población de 325.916 habitantes en 2018, siendo la tercera ciudad más grande y poblada de Andalucía tras Sevilla y Málaga, y la duodécima de España. Su área metropolitana comprende ocho municipios, con una población de 363.326 habitantes, la vigésima tercera más poblada de España.

Fundada por los romanos durante el siglo II a. C., llegaría a ser capital de la Hispania Ulterior en tiempos de la República romana, además de la provincia Bética durante el Imperio romano. No obstante, su momento álgido trascurrirá durante la dominación musulmana de la península ibérica, cuando se alzará como capital del Emirato de Córdoba, mientras que durante el Califato de Córdoba se convirtió en la ciudad más habitada, culta y opulenta de Europa y un centro líder mundial de la educación. Durante la larga Edad Media europea, en Córdoba florecieron las letras y las ciencias, gestándose las bases del Renacimiento europeo. Abundaron las mezquitas, las bibliotecas, los baños y los zocos, además de contar con multitud de fuentes, iluminación pública y alcantarillado durante la época de mayor esplendor califal.

Córdoba es actualmente la ciudad que más títulos Patrimonio de la Humanidad de la Unesco alberga del mundo. En 1984, la Mezquita-catedral de Córdoba fue incluida en la reputada lista, declaración que una década más tarde se extendería a todo el casco histórico. La Fiesta de los Patios Cordobeses fue designada Patrimonio cultural inmaterial de la Humanidad en diciembre del 2012 y en julio de 2018 la ciudad palatina de Medina Azahara, en las afueras del núcleo urbano, fue declarada también Patrimonio de la Humanidad. Tiene además, uno de los cascos históricos más grandes de Europa, con 246'73 hectáreas con monumentos datados desde época romana.

El significado etimológico del nombre de la ciudad ha sido largamente discutido en la historiografía y no existe en la actualidad consenso al respecto. El primer nombre conocido para la población es el de ‘Corduba’, otorgado bajo la forma de ‘Colonia Patricia Corduba’ tras la fundación romana de la ciudad en el siglo I a. C. y que se supone anterior. Dado que la primera aparición de Córdoba en textos antiguos hace referencia al establecimiento de un puesto comercial fenicio en las inmediaciones de la ciudad, se ha dado un posible origen semítico al topónimo. De este modo ‘Qorteba’ vendría a significar "molino de aceite", para algunos autores, o bien "ciudad buena" a partir de Qart-tuba para otros. Otras etimologías hacen referencia a la existencia de un asentamiento íbero anterior a la llegada de los fenicios considerando que la terminación "uba" es ampliamente conocida en Hispania significando bien "colina" o bien "río", referido como Oba el antiguo nombre del río Guadalquivir, siendo Qart-Oba la "ciudad del Oba".

La bandera cordobesa es un rectángulo con un ancho igual a dos tercios del largo (ratio 3:2), de color rojo vino ahigadado con el escudo de la ciudad en el centro, rodeado de una orla circular roja con borde amarillo.

En 1241, el rey Fernando III mandó y otorgó que el Consejo de la ciudad tuviese su propio sello <nowiki>"conocido y comunal para todos"</nowiki>, según vemos en el Fuero de Córdoba, que además reguló el funcionamiento político y jurídico de la ciudad de Córdoba entonces.
El escudo es una vista del puente romano sobre el río Guadalquivir, con la noria de la Albolafia a la izquierda; con la muralla y la puerta del puente sobre este; y la torre de la Mezquita-Catedral flanqueada por tres palmeras y algunas edificaciones al fondo.

Entre los siglos XVI y XX se utilizó en la ciudad el actual escudo de la provincia de Córdoba, hasta que el 1983 se retomó el anterior escudo diseñado por el Consejo de Córdoba en 1241.

En la actualidad, también existe un logotipo que es usado por el Ayuntamiento que es una simplificación del escudo de la ciudad.

El término municipal de Córdoba ocupa 1245 km, aproximadamente el 9 % del total de la provincia. Siendo el núcleo principal de población la zona más poblada existen seis pedanías, El Higuerón, Alcolea, Santa Cruz, Cerro Muriano, Villarrubia y Santa María de Trassierra y una Entidad Local Menor, Encinarejo, nacidas bien como asentamientos agrarios o bien como núcleos residenciales. El núcleo principal de Córdoba se encuentra situado en los márgenes del río Guadalquivir que la atraviesa de este a oeste formando varios meandros. Al norte del término municipal se encuentra Sierra Morena y al sur una extensa campiña. De este modo la altitud del municipio varía entre los 90 y 693 metros.

Dentro del término municipal pueden delimitarse por su orografía dos zonas, la campiña y la sierra. Al norte de Córdoba se encuentran las faldas de Sierra Morena con unas fuertes pendientes que permiten ascender desde los aproximadamente 100 metros sobre el nivel del mar del núcleo principal a los 692 del Cerro Torre Árboles, máxima cota del municipio. La altitud media de estas sierras se encuentra alrededor de los 400 metros alternándose grandes valles labrados por los arroyos estacionales y los afluentes del río Guadalquivir sobre los materiales blandos.

Al sur del río y en una estrecha franja al norte de este se encuentran terrenos bajos con leves ondulaciones del terreno que forman la denominada genéricamente "campiña". Esta región nace como consecuencia de la sedimentación asociada a procesos geológicos derivados del plegamiento de las cordilleras béticas y a la sedimentación derivada de la propia acción de los grandes cursos de agua. Por ello se diferencian en esta zona la campiña propiamente dicha y las terrazas fluviales siendo la altitud media de la primera entre los 200 y los 300 metros, destacando el Cerro de las Pilillas con 362 metros sobre el nivel del mar, y la de las segundas entre 100 y 150 metros.

Todo el término municipal de Córdoba se halla dentro de la cuenca del Guadalquivir, río que lo atraviesa totalmente y actúa como receptor de todos los cauces menores del municipio. Nacen en la sierra los afluentes Guadiato y Guadalmellato, con caudal todo el año y numerosos arroyos estacionales. Todos estos cursos de agua ejercen una fuerte acción erosiva en el terreno debida a la gran pendiente que deben salvar antes de verter sus aguas al Guadalquivir. Al sur del término se encuentra el afluente Guadajoz con numerosos arroyos estacionales que forman una compleja red en la campiña.

El término municipal de Córdoba se encuentra situado sobre la cuenca de sedimentación asociada al río Guadalquivir que separa la Meseta Ibérica de origen paleozoico de las Cordilleras Béticas formadas durante el plegamiento alpino.
La cuenca sedimentaria tuvo su origen durante la era Cuaternaria al depositarse materiales procedentes de las cordilleras cercanas en el "surco bético", depresión formada tras el levantamiento de ésta, y su posterior consolidación. Los materiales presentes son de diferente naturaleza destacando las margas, calizas y conglomerados. Se diferencian dos zonas en esta cuenca de sedimentación, por una parte la campiña posee materiales sedimentarios de origen marino y con una gran potencia depositados en los primeros momentos de la orogenia alpina, por otra parte la zona de la vega del río Guadalquivir posee materiales sedimentarios de origen fluvial resultado del transporte y acumulación y más modernos y en continuo movimiento. Al norte del término afloran rocas pertenecientes a las estribaciones de Sierra Morena. Existe gran complejidad en las rocas presentes, calizas, esquistos y conglomerados y destacan especialmente las rocas metamórficas, principalmente anfibolitas correspondientes a la llamada "banda de Cizalla Badajoz-Córdoba" y que desde el noroeste del municipio se extiende 400 kilómetros hacia el norte. Estas formaciones alóctonas están relacionadas con diversas unidades de norte de la península y se formaron hacia el cámbrico por un mecanismo de subducción y rápido ascenso que provocaron una fuerte cristalización de eclogitas.

Biogeográficamente el municipio participa de dos provincias corológicas con diferentes tipos de vegetación potencial. La zona de sierra se corresponde con la provincia Luso-Extremadurense y sus bosques típicos serían los encinares y alcornocales. Debido a la complicada orogenia de la zona y al escaso valor económico del suelo que ocupan es aún posible encontrar comunidades vegetales de valor en la zona. La vega y campiña de Córdoba pertenece a la provincia Bética y su vegetación potencial serían encinares y choperas en las zonas próximas al río. Sin embargo la fuerte acción antrópica desarrollada desde hace siglos en esta región debido al gran potencial agrónomo del suelo ha hecho desaparecer totalmente cualquier rastro de vegetación natural que pudiera existir en la zona.

Tiene un clima mediterráneo. De acuerdo con la clasificación climática de Köppen, el clima de Córdoba es mediterráneo de tipo Csa. Los inviernos son suaves, aunque con algunas heladas que en ocasiones han llegado a ser fuertes, debido a su distancia del mar. Los veranos son muy calurosos, con importantes oscilaciones térmicas diarias y temperaturas máximas que, en promedio son entre las más altas de Europa, sobrepasándose todos los años los 40 °C en varias ocasiones y que han llegado a superar los 45 °C. Aunque las mínimas son más frescas, la temperatura media alcanza los 28 °C en julio y agosto. Las precipitaciones se concentran en los meses más fríos, debido a la citada influencia atlántica, ya que se producen por la entrada de borrascas desde el oeste, situación que se da más en el periodo de diciembre a febrero, presenta una fuerte sequía estival, típica de los climas mediterráneos. Las lluvias anuales alcanzan los 600 mm, aunque hay una importante irregularidad interanual. De acuerdo a la clasificación climática de Köppen el clima de la ciudad se define como "Csa".

La temperatura máxima registrada en el Observatorio del Aeropuerto de Córdoba (situado a 6 km de la ciudad) es de 46,9 °C, del 14 de julio de 2017. La mínima más baja corresponde a los –8,2 °C del 28 de enero de 2005.

Según el censo de 2018, Córdoba cuenta con una población de 325 708 habitantes y una densidad de población de 259,9 hab/km². Su área metropolitana cuenta con 361 880 habitantes. En el año 2014 hubo un total de 3129 nacimientos y 2565 defunciones. En 2015, la edad media de la población era de 41,76 años. El 21,76 % de la población tenía 19 años o menos, el 61,61 % tenía entre 20 y 64 años, mientras que el 17,63 % tenía más de 64. En total en 2015 había 170 051 mujeres (51,95 % del total de la población) y 157 311 hombres (48,05 %).


Se conoce un asentamiento del III milenio a C. a las afueras de la ciudad de Córdoba, en la Colina de los Quemados, aunque se ignora si la ciudad turdetana permaneció en el tiempo. Se tiene constancia de que los materiales más antiguos de este yacimiento provienen de la Edad del Bronce, Antiguo y Medio, por las excavaciones de Luzón y Mata. Existen evidencias de un poblado ocupado entre el III milenio y el II milenio a C. (Edad del Bronce) en el Campo de la Verdad, al otro lado del río, que pudo estar ocupado al mismo tiempo que el asentamiento de la Colina de los Quemados. Hay evidencias de otros asentamientos del III milenio a C. en el entorno del centro urbano, como uno identificado como Cañito María Ruiz.

Además, se conocen restos de inicios de la Edad del Cobre, hacia finales del IV milenio a C. El más conocido está en la barriada de Alcolea, junto al puente. El descubrimiento más reciente es el de la Arruzafa-Tablero Alto, que ha proporcionado una sepultura con cuatro personas inhumadas simultáneamente, cerca del Brillante, un poblado del que apenas se sabe nada.

Fundada en 169 a. C., Córdoba fue capital de la "Provincia Hispania Ulterior Baetica" (Bética), una época de esplendor, en la que llegó a contar con numerosos edificios lúdicos, proporcionando al mundo latino grandes filósofos como Lucio Anneo Séneca, oradores como Marco Anneo Séneca y poetas como Lucano. Más tarde pudo formar parte de la provincia de Spania del Imperio bizantino, aunque este hecho no está demostrado.

En el año 711, los ejércitos árabes y bereberes invadieron la península ibérica, y en menos de siete años casi todo el territorio llegó a estar ocupado por los invasores. Córdoba fue capital del Emirato Independiente y del Califato Omeya de occidente, época en la que alcanzó su mayor apogeo, llegando a tener entre 250.000 y 450.000 habitantes, siendo en el siglo X una de las ciudades más grandes del mundo, en Europa solo superada por Constantinopla, así como nodo cultural, político y económico. Recientes hallazgos arqueológicos en zonas urbanas que se consideraban que debían estar ocupadas por almunias y huertas, como el meandro del río Guadalquivir entre el barrio de Levante, el barrio de Fátima y el Polígono de las Quemadas, hacen suponer incierto el margen de los 300 000 al millón de habitantes del que hablan las crónicas musulmanas hacia el año 1000. Con la excepción de Constantinopla, a mediados del siglo X no había en Europa Occidental una ciudad similar en cuanto a superficie edificada, ya que por aquel entonces ninguna superaba las 30.000 personas. Leopoldo Torres Balbás cifró la población de la ciudad en torno al siglo X en más de 100 000 habitantes, mientras otras fuentes hablan de 200 000 o 300 000. José Calvo Poyato ha considerado la cifra del millón de habitantes una exageración y un error.

Durante el gobierno de Abderramán I, se empezó a erigir la gran mezquita de Córdoba (completada en el siglo X) sobre la base de la basílica de San Vicente Mártir, templo compartido por musulmanes y cristianos hasta esa fecha. Los cristianos debieron levantar a partir de entonces su iglesia en las afueras de Córdoba. Se afirmaba que en la Mezquita se conservaba el brazo de Muhammad, y llegó a ser lugar de peregrinación para los musulmanes. Una publicación dice: «Su carácter sagrado sólo lo superaba La Meca y [...] el visitarla absolvía a los fieles de la obligación de hacer el peregrinaje a Arabia». Igualmente, la ciudad contaba con una famosa universidad y una biblioteca pública que contenía unos 400 000 volúmenes. Había veintisiete escuelas gratuitas para enseñar a los niños pobres, y el nivel de alfabetización, tanto de los niños como de las niñas, era muy alto. Los jóvenes que pertenecían a la nobleza de los reinos católicos del norte de España recibían su educación en la corte mora, y las mujeres ricas de Francia encargaban en Córdoba sus trajes más elegantes. La ciudad estaba adornada con jardines, cascadas y lagos artificiales, y mediante un acueducto, se suministraba agua dulce en abundancia a las fuentes y los baños públicos, de los que, según un cronista musulmán, había setecientos. Por toda la ciudad podían verse suntuosos palacios, uno de los cuales, Al-Zahra (Medina Azahara), a las afueras de Córdoba, requirió veinticinco años y el duro trabajo de 10 000 obreros para completarse. Sus ruinas testifican aún hoy su anterior grandeza.

No obstante, la muerte de Almanzor desató la anarquía en Córdoba y una disputa abierta por el poder, que dio pie en los primeros años del milenio al saqueo y el pillaje de Córdoba y Medina Azahara. La antigua joya de la corona quedó relegada en pocos años a ciudad de importancia secundaria en el contexto peninsular, musulmán y europeo.

En 1236, Fernando III el Santo toma la ciudad. Dicho monarca ordena la edificación de las denominadas iglesias fernandinas. Alfonso X establece el convento de Santa Clara y durante el reinado de Alfonso XI se edifica la sinagoga de Córdoba. Asimismo, y para conmemorar la victoria de la batalla del Salado sobre los benimerines, se edifica la Real Colegiata de San Hipólito, donde se encuentra enterrado este rey y su padre. También durante su reinado se empieza a edificar el Alcázar de los Reyes Cristianos.


En septiembre de 1804 se detectó un foco de fiebre amarilla en la ciudad, epidemia que acabó en apenas unos meses con la vida de más de 1500 cordobeses. El foco se inició en la calle Almonas, posiblemente procedente del puerto de Málaga, ciudad que padeció un brote importante en 1803, con al menos 7000 muertes, y que en el verano de 1804 se vuelve a repetir con más de 11 400 defunciones. La infección pronto saltará a Córdoba, afectando a la capital y a varios municipios del entorno como Espejo, Montilla o La Rambla. En el municipio de Córdoba, de la zona de la Axerquía se extendió al resto de la ciudad, aunque se levantaron muros y se cortaron varias calles. Las puertas de la ciudad permanecieron cerradas, a excepción de las puertas del Rincón y Puerta Nueva, donde se colocaron alguaciles y un médico para realizar el control sanitario. A finales de noviembre de 1804 se declaró el fin de la epidemia, lo que se celebró con fiestas y alborozo. 

En la actualidad se trata de una de las ciudades mejor conservadas de España, con un centro histórico muy extenso, declarado Patrimonio de la Humanidad por la Unesco el 17 de diciembre de 1984. Así mismo, la ciudad presenta zonas referentes de la moderna Córdoba del siglo XXI, como los barrios de Zoco y Plan Renfe por su calidad urbana.

Córdoba fue candidata a la capitalidad cultural europea para el año 2016, siendo finalista para representar a España. 

La Junta de Andalucía está estudiando la creación del Área Metropolitana de Córdoba que estaría compuesta, además de por la capital, por las poblaciones de Villafranca de Córdoba, Obejo, La Carlota, Villaharta, Villaviciosa de Córdoba, Almodóvar del Río y Guadalcázar, contando así con una población aproximada de 362 000 habitantes.

El 1 de julio de 2018, Medina Azahara fue declarada Patrimonio de la Humanidad.

Córdoba, ciudad milenaria, posee el segundo casco histórico más grande de Europa, el mayor espacio urbano del mundo declarado Patrimonio de la Humanidad por la Unesco. Es precisamente en él donde se aglomera gran parte de los edificios históricos de la ciudad. En él cabe destacar el edificio más importante y símbolo de la ciudad, la mezquita de Córdoba y actual catedral que, junto al Puente Romano, forman la más conocida faceta de la ciudad. De la época romana pueden encontrarse, además del puente, el Templo romano situado en la calle Claudio Marcelo y dedicado en su tiempo al culto imperial, el teatro romano situado bajo el Museo Arqueológico y Etnológico de Córdoba; es el más grande conocido de toda Hispania, el mausoleo romano dedicado a una familia acomodada de la época, el foro colonial, el foro adiectum, el anfiteatro y los restos del palacio del emperador Maximiano Hercúleo en el yacimiento arqueológico de Cercadilla.

Cerca de la mezquita-catedral se emplaza la antigua judería formada por multitud de calles irregulares, tales como calleja de las Flores y la calleja del Pañuelo, en las que pueden visitarse la sinagoga y la casa de Sefarad. En el extremo suroeste del casco antiguo se encuentra el alcázar de los Reyes Cristianos, antiguo alojamiento de los reyes y sede de la Inquisición, y adyacente al mismo se hallan las Caballerizas Reales, lugar de crianza del caballo andaluz. Cerca de las caballerizas se encuentran, junto a la muralla, los antiguos baños califales. En el sur del casco antiguo y al este de la mezquita, situada en la plaza del Potro, se halla la Posada del Potro, mencionada en obras literarias como "Don Quijote" y "La Feria de los Discretos". Tanto la posada como la plaza reciben su nombre de la fuente situada en el centro de la plaza, la cual representa a un potrillo. No lejos de esta plaza se encuentra el arco del Portillo.

A lo largo del cauce del Guadalquivir se encuentran los molinos del Guadalquivir, edificios de la época musulmana que aprovechaban la fuerza de la corriente para moler la harina tales como el molino de la Albolafia, el molino de la Alegría, el molino de Martos, el molino de Enmedio, el molino de Salmoral, el molino de San Antonio, el molino de Hierro, el molino de Téllez, el molino San Rafael y el molino de Don Tello o Pápalo Tierno.

Rodeando el extenso casco histórico se sitúa la antigua muralla romana, de la cual se conservan algunos lienzos; la puerta de Almodóvar, la puerta de Sevilla y la puerta del Puente, que son las tres únicas puertas que se conservan de las trece que tuvo la ciudad; algunas torres como la torre de la Malmuerta, la torre de Belén y la torre de la Puerta del Rincón; y las fortalezas de la torre de la Calahorra y la torre de los Donceles.

Repartidos por todo el casco antiguo se encuentran edificios palaciegos tales como el palacio de Viana, palacio de la Merced, palacio de Orive, palacio de los Aguayos, palacio de los Luna, palacio del duque de Medina Sidonia, palacio de los marqueses del Carpio y el palacio del marqués de Benamejí, entre otros.

A las afueras de la ciudad se encuentra el conjunto arqueológico de la ciudad de Medina Azahara "(Madinat Al-Zahra)" que constituye junto con la Alhambra de Granada la cumbre de la arquitectura hispanomusulmana.

Otros monumentos son:

Las iglesias fernandinas son doce, y son aquellos templos cristianos que fueron mandados erigir en Córdoba (muchos fueron transformación de mezquitas que, a su vez, habían sido iglesias durante el período visigótico) por Fernando III "El Santo" tras la reconquista de la ciudad en 1236. La misión de cada una de estas iglesias era doble: por una parte, la de ser centros espirituales de la ciudad, funcionando como iglesias; y por otra parte, ser los centros administrativos de la ciudad de Córdoba, siendo cada una de las iglesias, cabeceras de los barrios o collaciones en los cuales se dividía la ciudad desde la Edad Media y hasta el siglo XX. Algunas de las que se conservan son:


Repartidos por toda la ciudad se encuentran diez estatuas dedicadas a san Rafael, custodio de la ciudad. La mayoría de ellas se encuentran en los accesos a la ciudad (puentes, antigua estación de trenes...), puesto que el Arcángel San Rafael es el patrono de los viajeros. Estas son denominadas triunfos de San Rafael y están situados en lugares tan emblemáticos como el puente romano, la puerta del Puente o la plaza del Potro.

En la parte oeste del casco histórico se encuentran la estatua a Séneca (junto a la Puerta de Almodóvar), la estatua de Averroes (junto a la puerta de la Luna), y la de Maimónides (en la plaza de Tiberíades) en homenaje a estos tres grandes filósofos cordobeses. Más al sur, junto a la puerta de Sevilla, se encuentran la escultura al poeta Ibn Zaydun y la escultura al escritor y poeta Ibn Hazm y, en el interior del Alcázar, el monumento los Reyes Católicos y Cristóbal Colón.

También hay varias esculturas colocadas en las numerosas plazas del casco antiguo. En la céntrica plaza de las Tendillas se encuentra la estatua ecuestre del Gran Capitán, en la plaza de Capuchinos se halla el Cristo de los Faroles, en la plaza de la Trinidad está la estatua a Luis de Góngora, en la plaza del Cardenal Salazar está el busto de Al-Gafequi, en la plaza de Capuchinas está la estatua al obispo Osio, en la plaza del Conde de Priego puede contemplarse el monumento en honor de Manolete y en el Campo Santo de los Mártires se encuentra la estatua a Alhakén II y el monumento a los amantes.

En los Jardines de la Agricultura se puede contemplar el monumento al pintor Julio Romero de Torres, el busto del escultor Mateo Inurria, el busto del poeta Martínez Rücker y la escultura dedicada al jardinero Aniceto García Roldán que fue asesinado en dicho parque. Más al sur, en los Jardines del Duque de Rivas, se encuentra la estatua al escritor y poeta Ángel de Saavedra, duque de Rivas, realizada por el célebre escultor Mariano Benlliure.

En el río Guadalquivir, cerca del puente de San Rafael, se encuentra la conocida como isla de las Esculturas. Se trata de una isla artificial de forma alargada en la cual se hallan una docena de esculturas realizadas en piedra durante el Simposio Internacional de Escultura. Aguas arriba del río, cerca del puente de Miraflores, se encontraba el Hombre Río, una original escultura que simulaba ser un bañista mirando hacia el cielo y cuya orientación variaba según la corriente del río. A día de hoy sigue existiendo una placa informativa, pero la escultura ha desaparecido, arrastrada por la corriente en noviembre del 2007. Hay planes para devolverla a su sitio.

"La regadera" ( abril 2014) , situada en La puerta del Rincón. Es la primera escultura de las tres que forman parte del grupo escultórico que se conoce como el Monumento a los cuidadores de los patios. Esta escultura en bronce muestra a una mujer con la tradicional caña con lata regando las macetas de su patio. Esta representa el presente.

"El abuelo y el nieto"( abril 2015), situada en la plaza Manuel Garrido de San Basilio. Es la segunda escultura, representa a un abuelo (el pasado) que entraga una maceta a su nieto (el futuro) para que continúe con la tradición.
"Obras por el escultor cordobés, José Manuel Belmonte (1964)."

La ciudad de Córdoba posee en la actualidad siete puentes:

También existen otros puentes como el viaducto que une la avenida Arroyo del Moro y la glorieta del poeta Ibn Zaydun o el puente romano de Alcolea, localizado entre las barriadas periféricas de Alcolea y Los Ángeles.

La ciudad dispone de más de 5,1 millones de metros cuadrados de zonas verdes públicas urbanas, lo que arroja un ratio superior a 15 m / habitante (lo recomendado por la OMS). Si consideramos el Parque Periurbano Los Villares y los Sotos de la Albolafia, la superficie total asciende 10,2 millones de metros cuadrados (31 m / habitante).

Con 12,80 % de superficie de zona verde y arbolado urbano, Córdoba se sitúa a la cabeza de Andalucía y cuarta a nivel nacional.












Actualmente el alcalde de Córdoba es José María Bellido, del Partido Popular. Bellido sucedió a la anterior alcaldesa Isabel Ambrosio del PSOE, quien ejerció la alcaldía desde 2015 hasta el 15 de junio de 2019. Bellido, tras ganar las elecciones y sin mayoría absoluta, se convirtió en alcalde con los votos favorables de su partido así como los de Ciudadanos, y la abstención de Vox. 

El Ayuntamiento de Córdoba se estructura en diferentes áreas: de Presidencia, Seguridad, Movilidad, Igualdad y Participación; de Urbanismo, Vivienda, Infraestructuras y Medio Ambiente; de Economía, Comercio, Empleo y Gestión; Social; y de Servicios Culturales y Turismo. El ayuntamiento celebra plenos ordinarios una vez al mes, aunque con frecuencia se celebran plenos extraordinarios, con el fin de debatir temas y problemas que afectan al municipio.

Desde julio de 2008 la ciudad se divide en 10 distritos administrativos, coordinados por Juntas Municipales de Distrito, que a su vez se subdividen en barrios

La industria joyera ha tenido una presencia muy marcada en Córdoba desde el siglo XVI. Es a principios de ese siglo que se documenta la tendencia de los plateros a agruparse en gremios para defender sus intereses frente al Ayuntamiento, que culmina con la fundación de la Cofradía de San Eloy en 1503, que se consolidó como única agrupación profesional hasta nuestros días. Los plateros eran considerados artistas del oro y la plata, que necesitaban conocimientos de química, matemáticas e incluso de arquitectura para desarrollar su trabajo. La profesionalidad de los plateros de Córdoba llevó al gremio a imponer férreos controles de calidad de los materiales para mantener la reputación de la industria cordobesa, imponiendo duros castigos a aquellos profesionales que se los saltaran. Los plateros tenía el estatus de nobles, y gozaban de una buena posición económica y social.

Actualmente, el sector joyero cordobés es el tercer exportador de joyería a nivel nacional, detrás de Madrid y Barcelona, y primero de Andalucía. Sus exportaciones anuales ascienden a 100 millones de euros, un 60% del total de Andalucía, reuniendo al 50% de las empresas exportadoras del la comunidad autónoma. Está formado por más de mil pequeños talleres, que dan trabajo a 15 000 personas.

Con el objetivo de potenciar y modernizar el sector, crear sinergias y crear un entorno seguro donde los joyeros pudieran desarrollar su actividad, se crea en 2005 el Parque Joyero. En este complejo se implantan 170 empresas, que dan más de 1 000 empleos directos y 2 000 indirectos, y que supone la mayor concentración de empresas del sector joyero de Europa. Además, en este centro de más de 140 000 m² se encuentra una Escuela de Joyería, que es referencia nacional en la formación en el sector.

En un país cuya principal actividad económica es el turismo, Córdoba ocupa el noveno lugar del ranking de ciudades más turísticas de España. Por primera vez, desde que se tienen registros, se superó la barrera del millón de turistas con 1 012 580 en el año 2017, lo que supone una subida del 2,46 % respecto al año anterior. Durante ese mismo año también aumentaron las pernoctaciones hasta 1 616 706, un 1,68 % más que en 2016. 


Como en la mayoría del país, el pequeño y mediano comercio es el que más representación tiene. Gracias al buen tamaño de la ciudad, dispone de una gran variedad de empresas y cadenas comerciales que favorecen la competencia y al consumidor. A pesar de ello, la densidad comercial es menor a la media en España.


El transporte de energía eléctrica de alta tensión desde las centrales que abastecen el consumo de la ciudad está operado por Red Eléctrica Española. En Córdoba posee líneas de tensión de primera (220 kV) y segunda categoría con las que transportan la energía hasta la ciudad, y de segunda categoría con las que distribuyen la energía hasta las 16 subestaciones locales, desde las que se da acceso a la empresa distribuidora. Endesa Distribución es la empresa que distribuye la energía hasta el consumidor final, a través de una red propia.

El consumo de energía eléctrica total en la ciudad en el año 2016 fue de 1 308 399 MWh, de los cuales 584 294 MWh son consumo residencial.

Córdoba se encuentra en una buena posición geográfica, lo que la sitúa como un nudo logístico de la Red de Carreteras del Estado que conectan con autovías Andalucía occidental y Málaga con el centro y norte de España; y por carretera Córdoba con el norte de su provincia, Ciudad Real, Toledo y norte de Extremadura. Sus vías se dividen en autovías y carreteras, no habiendo ninguna autopista ni peajes:

También existen vías de la Red de Carretera de Andalucía, que sólo transcurren por dicha comunidad y no se incluyen en la Red de Carreteras del Estado por estar gestionadas por la Junta de Andalucía.


Existen 211 803 automóviles matriculados, lo que supone un índice de 65 por cada 100 habitantes. Esto sitúa a la ciudad por encima de otras mayores como Madrid, Barcelona o Zaragoza, cuyos índices son 60, 55 y 52 vehículos respectivamente por cada 100 habitantes.

De éstos, el 68 % (144 674) son turismos, con una antigüedad media de menos de 10 años. Las motocicletas suponen el 11 % (23 746) mientras que los ciclomotores el 7,97 % (16 888), que arroja un índice de 5 ciclomotores por cada 100 habitantes, frente a una media nacional de 1 por cada 100. Los camiones y furgonetas, con 11 000 y 13 000 unidades respectivamente, completan la lista, siendo estas últimas las que mayor antigüedad media registran (12 años) de todo el parque móvil cordobés.

El edificio de la actual estación de autobuses de Córdoba es obra del arquitecto César Portela y fue galardonado con el Premio Nacional de Arquitectura en 1999. El emblemático edificio conserva en su interior restos arqueológicos de notable interés, principalmente de origen romano y varias esculturas de Agustín Ibarrola y Sergio Portela. Actualmente operan las compañías Carrera, Alsa, Rafael Ramírez, Secorbús, Socibús, Autotransportes López, Unionbús y Linesur con multitud de destinos tanto regionales como nacionales.

Hasta Córdoba llega la línea de ferrocarril convencional que une Madrid con el sur peninsular, teniendo en la ciudad la separación de la línea que lleva hasta Málaga y Algeciras. La otra línea continúa hacia Cádiz, donde se pasa por Sevilla y allí se bifurca a Huelva. También llega hasta Córdoba la línea de AVE, bifurcándose hacia Sevilla o hacia Málaga y Granada.Tanto la estación de pasajeros como la de mercancías son estaciones de referencia del sur peninsular por su alto tráfico y su gran conectividad con el resto del país.

Además, existe un servicio especial para el transporte de pasajeros desde la estación hasta el campus universitario de Rabanales.

Desde 2009, se cuenta con el Centro de Transportes Intermodal de El Higuerón (Parque Logístico de Córdoba), gracias al cual, el sector del transporte, así como su posición estratégica, se han visto reforzados notablemente. En un corto plazo de tiempo está prevista la conexión ferroviaria directa, permitiendo una intermodalidad plena ferrocarril - carretera.

Hasta los años 1980 la ciudad disponía de conexión ferroviaria con las localidades de la comarca del valle del Guadiato a través de la línea Córdoba-Almorchón (actualmente abandonada en parte y destinada únicamente al transporte de carbón a la central térmica de Puente Nuevo).

En septiembre de 2018 se estrenó la primera Línea de Cercanías, que discurre entre las barriadas periféricas de Villarrubia de Córdoba y Alcolea, pasando por el centro de la ciudad y con una frecuencia de 39 trenes semanales.

El aeropuerto de Córdoba (código IATA: ODB, código OACI: LEBA) es un aeropuerto español de Aena que se encuentra en la ciudad de Córdoba y está clasificado como de tercera categoría. Sus códigos son ODB y LEBA, en las nomenclaturas IATA y OACI respectivamente.

A efectos aeronáuticos, es un aeródromo abierto al tráfico nacional e internacional de países firmantes del Acuerdo de Schengen, en el que se presta el servicio AFIS ( Servicio de Información de Vuelo de Aeródromo). Su Horario Operativo, es de 09:00h a 20:00h y se divide en dos franjas, según el tipo de vuelo. Para Vuelos Comerciales, tiene el llamado Horario de Uso Público, que coincide con el Horario del Servicio AFIS, de 12:00h a 15:00h de lunes a viernes, aunque se puede pedir ampliación. Los vuelos comerciales no podrían operar en el resto del horario, llamado Horario de Uso Restringido, salvo petición de ampliación. En su mayoría, es utilizado por empresas de tratamientos agrícolas, traslados de órganos hacia y desde el centro de trasplantes al hospital Reina Sofía, vuelos militares, vuelos chárter de pasajeros, fotografías aéreas, cursos de pilotaje, escuelas de paracaidismo y otros trabajos aéreos.
El aeropuerto cuenta con dos pistas, la RWY03 de dimiensión 2.076m por 45 metros, y la RWY21 de 2.241m por 45m y una plataforma de 43.000 metros cuadrados aproximadamente. La terminal de pasajeros está situada en la planta baja del edificio principal e incluye la zona de salidas, llegadas, cafetería, servicios y oficinas de administración. El aeropuerto dispone también de un edificio de servicios y de una zona de aviación general, que cuenta con hangares, almacenes y oficinas. 

Los autobuses urbanos están gestionados por la empresa municipal AUCORSA (Autobuses Urbanos de Córdoba S. A.) desde su constitución en 1953. Posee 135 vehículos que realizan servicios en 14 líneas urbanas que conectan las diferentes zonas de la ciudad, 2 líneas del casco histórico, diversos servicios especiales (Servicios para Feria, Semana Santa, fútbol, etc.) y 6 líneas periféricas que conectan el núcleo principal con las diferentes pedanías.

Los primeros tramos del carril-bici de Córdoba comenzaron a construirse en el 1995 y 1996. A comienzos del 2007, Córdoba contaba con algo más de 35 km de carril-bici, incluidos los tramos de doble sentido multiplicados por dos.

Además, el ayuntamiento, con el servicio "Cyclocity", dispone de cuatro puntos de recogida y depósito que poseen 35 eco-bicis que pueden ser utilizadas por cualquier persona, debiendo previamente solicitar una tarjeta de acceso gratuita que permite la retirada de la misma.

El 1,33 % de la población se desplaza en bicicleta diariamente y el 17,34 % esporádicamente, en contraste con el 81,33 % que no la utiliza nunca.


El abastecimiento de agua potable a Córdoba lo realiza la Empresa Municipal de Aguas de Córdoba (EMACSA), creada en 1969.

El agua que suministra EMACSA está embalsada en varios pantanos:


La potabilización del agua se realiza en las estaciones de tratamiento de agua potable (ETAP), donde se trata el agua de manera que se vuelva apta para el consumo humano. La principal ETAP es Villa Azul, que se sirve del embalse Guadalmellato, y da servicio a más de 328.000 habitantes. Además existen dos ETAP más: Guadanuño y Trassierra, ha abastecen a 5.400 habitantes.

La depuración de aguas residuales se realiza en las estaciones de depuración de aguas residuales (EDAR), donde se elimina la contaminación del agua para su devolución al medio ambiente en condiciones adecuadas. Existen tres EDAR: La Golondrina, Cerro Muriano y Santa Cruz.

Sadeco es la Empresa Municipal de Saneamientos de Córdoba. Fue creada en 1986 con los objetivos de la recogida de residuos urbanos, tratamiento y destino final de residuos; especialmente dedicados al reciclaje y elaboración del compost, limpieza viaria, limpieza de colegios y edificios públicos municipales, sanidad y plagas, servicios técnicos y mantenimiento, servicio educativo y de apoyo (inspección, prevención y otros).

CECOSAM (Cementerios y Servicios Funerarios Municipales de Córdoba, S.A.), es la empresa propiedad del Ayuntamiento de Córdoba cuyo objetivo es prestar servicios funerarios varios.
Sus instalaciones son:

Gracias a su buen tamaño, Córdoba cuenta con una extensa oferta educativa entre los que encontramos guarderías, colegios de educación primario (C.E.I.P.), institutos de educación secundaria (I.E.S.), etc.

Además, existen diversos centros de Formación Profesional y otros de carácter especial como Zalima (centro de formación administrativa), la Escuela Superior de Arte Dramático, las de "Artes y Oficios", el Conservatorio Superior de Música, el Conservatorio Profesional de Música, el Conservatorio Profesional de Danza o el Consorcio Escuela de Joyería de Córdoba.

Cuenta con dos Universidades, la Universidad de Córdoba y la Universidad Loyola Andalucía. La UCO es la principal universidad de la ciudad por tamaño. A su oferta académica de 43 grados, 61 másteres y programas de doctorado, se encuentran matriculados 21.000 alumnos. La actividad docente, investigadora y administrativa se lleva a cabo en el Rectorado, antigua Facultad de Veterinaria, y 4 campus: dos urbanos (Campus de Humanidades y de Ciencias Jurídicas y Sociales, integrado y repartido por la ciudad; Campus de Ciencias de la Salud, próximo al Hospital Universitario Reina Sofía); Campus Rabanales, a 6 km al este de la ciudad; y Campus de Belmez, al norte de la provincia, creado en 1923. Cuenta con más de 1.200 docentes y 700 trabajadores no docentes.

La Universidad Loyola Andalucía es una Universidad privada católica perteneciente a la Compañía de Jesús. Tuvo su origen en ETEA, Facultad de Ciencias Económicas y Empresariales adscrita a la UCO, en 1963. Desde entonces sufre diversas ampliaciones, hasta que en 2011 se convierte en la primera Universidad privada de Andalucía. Cuenta con tres Campus, en Córdoba, Sevilla y Dos Hermanas.

La ciudad cuenta con un extenso sistema sanitario, tanto de titularidad pública (a través del Servicio Andaluz de Salud, SAS) como de titularidad privada. La regulación del sector corresponde a la comunidad autónoma, que a través de la Ley de Salud de Andalucía extienda la cobertura sanitaria pública y gratuita a todos los españoles y extranjeros, incluso si se encuentran en situación irregular en el país. Así mismo divide la atención sanitaria en primaria y hospitalaria. La atención primaria es aquella que prestan médicos de familia, pediatras y personal de enfermería en centros de salud y consultorios y a domicilio. La atención hospitalaria comprende la asistencia prestada por especialistas tanto en centros de especialidades como en hospitales.

La red sanitaria en Córdoba está formada por cuatro hospitales públicos y tres hospitales privados; varios centros de especialidades, 14 centros de salud y 9 consultorios en los distritos periféricos. Además, podemos encontrar más de 1 400 establecimientos sanitarios de diversos tipos, como farmacias (191), ópticas (90), clínicas dentales (198), centros de reproducción asistida (3), centros de diálisis, ortopedias, etc. La ciudad también cuenta con un Centro Regional de Transfusión Sanguínea (para sangre, plasma y médula ósea) y un Banco Sectorial de Tejidos.

La atención primaria pública en Andalucía se organiza en 17 Distritos de Atención Primaria, que dan servicio a uno o varios municipios. En Córdoba el Distrito de Atención Primaria de Córdoba es el encargado de la gestión y administración de las actividades de asistencia sanitaria, promoción de salud y prevención de enfermedades, cuidados para la recuperación de la salud, y vigilancia de riesgos ambientales y alimentarios; en el municipio.

Está integrado por 13 centros de salud, situados en el núcleo urbano de la ciudad; y por 9 consultorios de salud y 1 consultorio auxiliar, localizados en las barriadas periféricas.

El complejo hospitalario Hospital Universitario Reina Sofía, operado por el Servicio Andaluz de Salud (SAS), es el principal hospital de la ciudad. Con categoría (la más alta), este centro público cubre todas las especialidades ofertadas por el Sistema Nacional de Salud. Posee más de 1 450 camas, 204 consultas externas, 32 quirófanos, 36 salas de urgencias, 8 paritorios, y un completo equipamiento. Está integrado por diferentes centros:









El Hospital San Juan de Dios, en un centro privado benéfico fundado en 1935, propiedad de la Orden Hospitalaria San Juan de Dios, tras sendas reformas en los años 90 y en 2013 se ha modernizado hasta tomar el estatus de hospital general. Cubre más de 35 especialidades, cuenta con 133 camas, UCI, urgencias 24h (generales, pediátricas y ginecológicas), 8 quirófanos, dos paritorios, servicios de laboratorio y análisis clínicos, entre otras. En 2016 atendió a 54 000 pacientes, con un crecimiento del 12 % respecto al año anterior, y del 23,55 % respecto a 2012.

El Hospital Cruz Roja de Córdoba, es otro centro privado benéfico fundado en 1933, propiedad de Cruz Roja Española. Le ha sido reconocida la certificación SEP, lo que acredita que se trata de un centro de excelencia.

Quirónsalud Córdoba es en la actualidad el principal hospital privado de Córdoba. Su inauguración se dio lugar el 3 de septiembre de 2018, casi dos años desde que comenzase su construcción en noviembre de 2016 y tras una inversión de más de 55 millones de euros. Propiedad del grupo Quirón, está acogido en un edificio de 25.000 m de superficie de arquitectura singular, concebido desde el origen para ser energéticamente eficiente.

Cuenta con 100 habitaciones individuales, 7 quirófanos, UCI y UCI neonatal, sala de radiología vascular y hemodinámica, salas de endoscopia, paritorios, laboratorio integral, hospital de día (quirúrgico, médico y oncohematológico) y urgencias 24h (de adultos, pediátricas y tocoginacológicas).

Este hospital cuenta con una plantilla inicial de 300 personas, que está prevista que aumente a los 500 profesionales en el tercer año. La cartera de servicios del hospital recoge todas las especialidades médicas, incluyendo la medicina nuclear, que hasta el momento no estaba cubierta por la sanidad privada en la ciudad.

La función básica de los Servicios Sociales es orientar y colaborar con la población ante cualquier tipo de situación problemática en la que se pueda llegar a encontrar, por muy límite que sea. Aplicando los principios de solidaridad, inclusión, respeto a la diversidad, multiculturalidad y fomento del desarrollo humano, los Servicios Sociales Municipales (SSM) aportan diferentes recursos para la población: orientación ante problemas, información sobre recursos o, también, ayudas económicas.

Para los colectivos, los SSM apuestan por el asesoramiento para creación y funcionamiento de grupos que intervengan en la resolución de alguna problemática social o subvenciones a proyectos de interés social.

Cada intervención de los Servicios Sociales Municipales tiene varios tipos de beneficiarios. En primer lugar estarían los beneficiarios directos, que serían los destinatarios principales de nuestra intervención. Además de estos, cuando una persona supera una situación problemática, también su entorno más inmediato se ve favorecido y, por último, toda la población avanza socialmente al eliminar los efectos y, en su caso, las causas de diferentes problemáticas sociales.

La labor de inclusión social, además de beneficiosa para las personas que participan en los programas y actividades, es rentable para el conjunto de la sociedad, ya que en los sectores económicamente menos favorecidos y en los nuevos vecinos de origen multicultural anida un enorme potencial productivo que se puede rentabilizar. El desafía es movilizar estas capacidades y aplicarlas productivamente.

Los Servicios Sociales Municipales del Ayuntamiento de Córdoba comprenden una serie de bloques:

El municipio cuenta con un personal especializado, que se compone básicamente de trabajadores sociales, educadores comunitarios/as, administrativos y auxiliares administrativos, ordenanzas y auxiliares de Clínica, a los que se unen varios técnicos de Administración General y técnicos de Grado Medio, o bien geriatras, psicólogos o sociólogos. En cuanto a las instalaciones, las Zonas de Trabajo Social (ZTS) disponen de Centros de Servicios Sociales Comunitarios (CSSC), que se ubican en la red de Centros Cívicos Municipales o en edificios de uso específico. Por su parte, los Centros de Día se distribuyen en una red propia de Centros Municipales de Mayores.

La ciudad de Córdoba posee una amplia red de bibliotecas públicas. 

Dependientes directamente del ayuntamiento se encuentra la Red Municipal de Bibliotecas de Córdoba, integrada por una Biblioteca Central y una red de 11 bibliotecas sucursales repartidas por todo el término municipal que dan cobertura a un gran porcentaje de la población.

La Biblioteca Central de Córdoba, se encuentra situada en Ronda del Marrubial cuenta con una gran superficie y con las secciones de información y referencia, hemeroteca, conocimiento, fondo local, biografías, obras literarias, arte, música, cine, informática, sala cómic e infantil.

La Biblioteca Provincial de Córdoba es una Biblioteca Pública del Estado surgida de los fondos pertenecientes a los conventos, monasterios e iglesias que estaban siendo desamortizados entre los años 1835 y 1837, cuenta con un fondo de alrededor de 180 000 documentos entre libros, revistas, grabaciones sonoras, videograbaciones y demás tipos de documentos. Destaca su importante fondo antiguo, con 78 incunables y 647 manuscritos, aparte de una excelente colección de libros del siglo XVI. En total cuenta con más de 13.000 obras anteriores a 1900. Además de ésta, el Ministerio de Cultura está construyendo una segunda Biblioteca Pública del Estado en la Avenida de América, llamada popularmente <nowiki>"de los Patos"</nowiki>, por el parque que tiene alrededor. Con un presupuesto que ha ascendido hasta los 10 millones, está previsto que finalice tras un plazo de 2 años tras la recuperación de las obras a principios de 2019.

Aparte de las bibliotecas municipales y provinciales, existen en la ciudad bibliotecas universitarias en las diferentes facultades de la ciudad y diversas bibliotecas temáticas dependientes de la Diputación Provincial o de la Diócesis de Córdoba. La denominada Biblioteca Viva de Al-Ándalus, situada en el palacio del Bailío, posee el fondo bibliográfico más importante relativo al cultura andalusí. Esta biblioteca, propiedad de la Fundación Roger Garaudy, surge con el objetivo de divulgar la importancia de la cultura clásica andalusí y sus aportaciones a la cultura universal. Así mismo, la Universidad de Córdoba dispone de bibliotecas científicas en las diferentes facultades en las que dispone de un gran número de referencias especializadas como más de 170 000 libros, 4 076 revistas científicas, tesis leídas en la Universidad, recursos electrónicos, etc. 

El Archivo Histórico de Viana ubicado en el palacio de Viana es un importante archivo nobiliario que guarda más de 300 000 documentos sobre la nobleza española. Además de la información relacionada con los títulos nobiliarios, guarda 877 testamentos y mayorazgos desde el siglo XIII, 868 pergaminos que hacen referencia a la monarquía española desde la Edad Media y 39 sellos de plomo referidos a reyes de España y papas, entre otros.


Dada la situación estratégica de la ciudad de Córdoba, la gastronomía cordobesa se nutre principalmente de productos del campo y de su vega, así como de la Sierra, de donde viene su cabaña ganadera, así como de la parte sur, de su aceite de oliva. La conjunción de todos estos ingredientes, todos ellos de primera calidad, hacen de la cocina cordobesa, una cocina de guisos y estofados.

Por otra parte existe en la gastronomía cordobesa signos de influencia musulmana como el uso de las especias (orégano, hierbabuena, estragón), o la utilización de alimentos introducidos por los árabes, como el arroz, la espinaca, la berenjena, o la naranja amarga.

Como platos típicos de la gastronomía Cordobesa podemos resaltar el salmorejo, los flamenquines, el rabo de toro, el cordero a la miel, las naranjas picadas o las alcachofas a la "montillana" y como postre más típico podemos destacar el pastel cordobés, consiste en una masa de hojaldre rellena de cidra confitada llamada "cabello de ángel".

Desde antiguo Córdoba ha contado con una importante tradición orfebre, remontándose a la época romana. Actualmente, el sector joyero de Córdoba sigue siendo muy importante con más de un millar de empresas que suponen el 20 % del sector industrial de la provincia. El Parque Joyero de Córdoba cuenta con 148 fábricas y 202 locales comerciales siendo la mayor fábrica joyera del mundo.

Córdoba es famosa por sus curtidos y por todo tipo de artesanías en cuero monturas de caballo a cuadros, biombos o pequeños muebles, siendo quizá el producto más típico el cordobán. Destacan también el guadamecíes, traídos por los árabes en el siglo VIII y cuyas producciones gozaron de fama europea por lo menos desde el siglo XI. Actualmente quedan pocos artesanos que se dediquen a ello, al igual que ocurre con el resto de los productos artesanales.

La ciudad de Córdoba ha sido el escenario de numerosas novelas:

El Carnaval de Córdoba se consolida año tras año como una fiesta muy popular, que llega cada año a más gente. Comienza con la tradicional Gala del Sultán y la Sultana, que tiene lugar en el Bulevar del Gran Capitán frente al Gran Teatro. Meses antes, las comparsas practican para el Concurso de Agrupaciones que tiene lugar en el Gran Teatro, donde llevarán a cabo una batalla de coplas o chirigotas en las que se burlan y ridiculizan en forma de crítica humorística de temas sociales de actualidad. Tras la Gran Final se da inicio a la fiesta en la calle con el pregón.

Es una festividad religiosa y cultural en la que por una semana, desde el Domingo de Ramos hasta el Domingo de Resurrección las cofradías van recorriendo las calles de Córdoba recordando algunas de las escenas de la pasión, muerte y resurrección de Jesús o lo que es lo mismo rememorar sus últimos días, acompañadas por nazarenos, y penitentes. Las hermandades mayoritariamente van acompañadas de Bandas musicales, pero existen hermandades de silencio. Esta festividad se celebra en los meses de marzo y abril, La Pascua de Resurrección es el domingo inmediatamente posterior a la primera Luna llena tras el equinoccio de primavera, y se debe calcular empleando la Luna llena astronómica. Por ello puede ser tan temprano como el 22 de marzo, o tan tarde como el 25 de abril.

En este momento Córdoba tiene en total 6 pro-hermandades que procesionan en vísperas de Semana Santa entre el Jueves de Pasión y el Sábado de Pasión y 38 Hermandades que desde el Domingo de Ramos hasta el Domingo de Resurrección se dirigen hacia la Carrera Oficial ubicada en los alrededores de la Mezquita-Catedral de Córdoba, iniciándose ésta en la Puerta del Puente ubicada en la Plaza del Triunfo y continuando por la calle Torrijos, calle Cardenal Herrero, Patio de los Naranjos, interior de la Catedral, plaza de Santa Catalina y calle Magistral González Francés. Las zonas más transitadas por las cofradías son la calle San Fernando (o calle Feria), la Ribera y San Pedro.

Mayo es el mes grande de Córdoba. Durante este mes se celebran las principales fiestas de Córdoba y por las cuales es ampliamente conocida. 

La Batalla de las Flores es una cabalgata de carrozas en la que las personas que van dentro, ataviadas con trajes típicos como son los trajes de gitana o flamenco, arrojan flores, normalmente claveles, al público que este a su vez se las devuelve. Esta festividad se realiza el 1 de mayo sobre las doce del mediodía y se considera la apertura del mes Cordobés, que es mayo.

A principios de mayo se celebran las Cruces de Mayo, fiesta en la cual en las principales calles y plazas de Córdoba se colocan cruces de unos tres metros totalmente decoradas de flores y rodeada de bellas plantas en maceteros y un decorado tradicional que refleja los caracteres de la zona, normalmente en el centro de toda cruz. La visita de estas hermosas cruces suele estar acompañada de una barra en la cual se puede consumir bebida y la comida típica de la tierra.

También a partir de la segunda semana se celebra fiesta de la La Cata. Todas las bodegas cordobesas se reúnen en el Mayo de Córdoba para ofrecernos sus mejores vinos. Los vinos de la Denominación de Origen Montilla-Moriles son los protagonistas de esta fiesta cordobesa. El vino Fino, el Amontillado, el Oloroso, el Cream, el Pedro Ximénez, el Blanco Joven y el blanco Pedro Ximénez son las diferentes variedades que se pueden degustar en la cata.

Festival de los Patios Cordobeses

Durante la segunda y tercera semana de mayo se celebra el Festival de los Patios Cordobeses, declarado en 1980 Fiesta de Interés Turístico Nacional y más tarde Patrimonio Cultural Inmaterial de la Humanidad por la Unesco desde 6 de diciembre de 2012. Durante esta festividad los participantes abren, de modo gratuito, sus patios para que puedan ser visitados dentro del horario establecido para tal fin. Se dividen en dos categorías: arquitectura antigua y arquitectura moderna. Al mismo tiempo se celebra también el Concurso de Rejas y Balcones. Cabe señalar que debido a la popularidad de los patios cordobeses, estos permanecen abiertos también en épocas especiales como Navidad y de abril a junio; al mismo tiempo hay patios que permiten a turistas alojarse en su interior. Por otro lado tiene lugar una verbena en San Basilio.

El origen tiene lugar en la antigüedad, en civilizaciones de la cultura babilónica, egipcia, griega o romana. Las casas de estas civilizaciones estaban distribuidas alrededor de un patio central. Así pues, tanto el lenguaje como la cultura y esta distribución, además de la arquitectura en general, fueron traídos a occidente, llegando así a Córdoba entre otros lugares de la península. Por tanto, el patio cordobés proviene de las casas romanas.

A finales de mayo (la última semana entera de mayo) se celebra la Feria de Nuestra Señora de la Salud, siendo los mejores días el viernes del alumbrado, miércoles el día de los niños y el fin de semana.

La festividad del Corpus Christi se celebra el domingo, 63 días después de la Resurrección del Señor. Por la tarde procesiona la Custodia del siglo XVI labrada por Enrique de Arfe. A lo largo de su recorrido por los alrededores de la Catedral se montan esbeltos altares y se esparce romero por el suelo.

La Feria de la Fuensanta, también denominada Velá de la Fuensanta, son unas fiestas folclóricas celebradas en torno al 8 de septiembre en honor de la Virgen de la Fuensanta, la Co-Patrona de la Ciudad, en los alrededores del Santuario de Nuestra Señora de la Fuensanta.

El día 7 de septiembre, víspera de su festividad, procesiona la imagen de Nuestra Señora de la Fuensanta que sale de la Catedral de Córdoba (previamente trasladada hasta allí) hacia el Santuario de la Virgen situado en el barrio homónimo. 

"Véase también:" Historia de la devoción por San Rafael en Córdoba y San Rafael en Córdoba.

El día del Custodio San Rafael Arcángel se celebra el 24 de octubre con la visita a la Basílica del Juramento de San Rafael y con peroles en la cercana sierra.

San Rafael solo procesiona de forma excepcional. Las dos últimas ocasiones que procesionó la imagen del Arcángel fue una en 2012, con motivo del Año de la Fe. El día 20 de octubre partió desde su Basílica hacia la Catedral y posteriormente, el 24 de octubre, día de su festividad, a la inversa. En junio del año 2019 se dirigió hacia la Catedral, saliendo desde la Parroquia de San Andrés, junto a las imágenes de la Virgen de los Dolores Coronada y el Sagrado Corazón de Jesús de San Hipólito con motivo del Jubileo de las Cofradías por el Año Jubilar del Sagrado Corazón de Jesús en Córdoba.


La ciudad cuenta con las siguientes instalaciones deportivas:

Aquí están algunos de los acontecimientos deportivos que se han disputado en Córdoba.

La ciudad cuenta con las siguientes entidades deportivas:

La ciudad de Córdoba participa en la iniciativa de hermanamiento de ciudades promovida, entre otras instituciones, por la Unión Europea. 

Las ciudades hermanadas con Córdoba son:




</doc>
<doc id="8988" url="https://es.wikipedia.org/wiki?curid=8988" title="Provincia de Córdoba (España)">
Provincia de Córdoba (España)

Córdoba es una provincia del sur de España, en la parte norte-central de la comunidad autónoma de Andalucía. Limita con las provincias de Málaga, Sevilla, Badajoz, Ciudad Real, Jaén, y Granada. Su capital es Córdoba. El gobierno de la provincia es ejercido por la Diputación Provincial de Córdoba.

Su área es 13 771 km². Su población es de 785 240 habitantes (2019). Más del 40% vive en la capital, y su densidad demográfica es de 57,24 hab/km².

El Real Decreto de 30 de noviembre de 1833 creó la Provincia de Córdoba, que se formó uniendo las localidades del Reino de Córdoba y los siguientes lugares de Extremadura: Belalcázar, Fuente la Lancha, Hinojosa del Duque y Villanueva del Duque. Sin embargo Chillón y su aldea de Guadalmez, lugares pertenecientes al reino, pasaron a formar parte de la provincia de Ciudad Real. Asimismo la nueva provincia incorporó dos enclaves del Reino de Jaén que existían en el reino de Córdoba: Belmez (que incluía Peñarroya-Pueblonuevo, segregada en 1886) y Villafranca de Córdoba, antes "de las Agujas". Actualmente la provincia está compuesta por los municipios que pueden verse en el anexo "".

La provincia se divide principalmente en tres zonas geográficas: Sierra Morena al norte, el valle del Guadalquivir en el centro y las Sierras Subbéticas al sur.

El clima es mediterráneo continentalizado con unas temperaturas que en la capital oscilan entre los 9,2 °C de enero y los 27,2 °C de julio y agosto con máximas que a veces superan los 40 °C. Las precipitaciones en la capital son de 400 a 600 mm al año, concentrándose de octubre a abril.<br>

Según Ecologistas en Acción, alrededor de una población de 330 000 personas han respirado aire con niveles de sustancias contaminantes por encima de lo establecido. Esto afectaría a los habitantes de la zona de Córdoba capital y la central térmica de Puente Nuevo. No obstante, esta última será desabastecida el 30 de junio de 2020 en su apuesta por las energías renovables.

La provincia de Córdoba es la 11.ª de España en que existe un mayor porcentaje de habitantes concentrados en su capital (41,48 %, frente a 31,96 % del conjunto de España).

La provincia, formada por , está conformada administrativamente por 7 mancomunidades y judicialmente en 10 partidos judiciales.

La unidad administrativa básica en la que se divide la provincia son los municipios. Existen 77 en la actualidad. El municipio con más habitantes es la capital provincial. La provincia tiene un enclave dentro de la provincia de Sevilla en el cual se encuentra la localidad de Villar, perteneciente al municipio de Fuente Palmera.

Los municipios de la provincia de Córdoba ordenados por población son los siguientes (de acuerdo al padrón municipal del INE en 2018):

En la provincia existen 12 partidos judiciales:

En campo de plata un león rampante de gules (rojo o púrpura de la corona de león) ya que Córdoba fue conquistada por Fernando III con soldados de la corona de León. Bordura componada con las armas reales de las coronas de Castilla y León. Al timbre, corona real cerrada. La boca del escudo será la del español moderno, es decir: cuadrilongo y redondeado por su base, con 6 unidades de alto (del eje a la punta) por cada 5 unidades de ancho (del flanco diestro al flanco siniestro).

Paño rectangular vez y media más largo, del asta al batiente, que ancho; de color morado. Centrado el escudo de la provincia de Córdoba con su correspondiente timbre, siendo las dimensiones del escudo, desde la corona del timbre a la punta, ½ del ancho total de la bandera, y el resto de proporciones en concordancia.

El logotipo está constituido por un símbolo que representa la figura sintetizada de un león rampante de color amarillo (Pantone 117), que se alza frente a una masa de color roja (Pantone 485), que en su parte derecha, por su forma, dibuja la inicial de la palabra Córdoba, unido a la leyenda Diputación de Córdoba con la tipografía Trade Gothic Bold 2 para la palabra «Diputación» y Trade Gothic Light para «de Córdoba», ambas de color negro.

La provincia cuenta con tres parques naturales:





La Universidad de Córdoba fue fundada como tal en 1972, aunque cuenta con dos siglos de historia que avalan una trayectoria que hunde sus raíces en la Universidad Libre, la cual funcionó en la provincia a finales del siglo XIX y cuenta con estudios centenarios como los de la Facultad de Veterinaria, únicos en Andalucía.

Su juventud y sus dimensiones medias -la UCO tiene 21 000 alumnos, algo más de 1200 profesores y 700 trabajadores- la han dotado del dinamismo necesario para ir adaptándose y entrar en el siglo XXI como una universidad de alta calidad docente y probada solvencia científica.

Los estudios de la Universidad de Córdoba van desde las Humanidades y las Ciencias Jurídico-Sociales a las Ciencias de la Salud y las carreras científico-técnicas, tres áreas que se corresponden con su estructuración en tres grandes campus: el Jurídico social, integrado en el centro urbano; el de la Salud, al oeste de la capital, y el Agroalimentario, Científico y Técnico de Rabanales, en el área este. Además, la UCO cuenta con la Escuela Politécnica de Belmez, situada a sesenta kilómetros de la capital cordobesa.

Boletín Oficial de la Junta de Andalucía



</doc>
<doc id="8989" url="https://es.wikipedia.org/wiki?curid=8989" title="Richard Felton Outcault">
Richard Felton Outcault

Richard Felton Outcault (n. 14 de enero de 1863, Lancaster, Ohio - 25 de septiembre de 1928, Flushing, Nueva York) fue un guionista, dibujante de historietas y pintor estadounidense. Outcault fue el creador de la serie "The Yellow Kid" ("El chico amarillo"), a partir de la cual nació y se desarrolló la historieta tal y como la conocemos hoy en día.

Outcault comenzó su carrera como ilustrador técnico de Thomas A. Edison y como dibujante humorístico para las revistas "Judge " y "Life". 

Pronto firmó para el "New York World" de Joseph Pulitzer y en su suplemento dominical en color llamado "World" comenzó, el 5 de mayo de 1895, la serie de megaviñetas cómicas "Hogan's Alley" que presentaba a un niño de los suburbios sobre cuya amplia camisa aparecían los textos. A partir del 5 de enero de 1896, esta camisa fue coloreada, a modo de experimento, con un color particularmente dificultoso por aquel entonces: el amarillo; de este modo la expresión popular "Yellow Kid" acabó por filtrarse desde el público hasta el título general de la serie.

Cuando en octubre de 1896 pasó a trabajar para el "New York Journal" de William Randolph Hearst, rival de Pulitzer, "The Yellow Kid" comenzó a aparecer como una sucesión de viñetas en lugar de una sola. Esto, que Outcault llevó a cabo por iniciativa del propio Hearst es considerado por muchos teóricos el verdadero momento del nacimiento de la historieta.

En 1897 volvió a cambiar de periódico, creando para el "New York Herald", las series "Poor Li'l Mose" (1901) y "Buster Brown" (1902-1905). Esta última la continuaría en "New York American" desde 1906 hasta 1920.

Datos y curiosidades sobre Richard Felton Outcault


</doc>
<doc id="8991" url="https://es.wikipedia.org/wiki?curid=8991" title="Transbordador espacial Challenger">
Transbordador espacial Challenger

El transbordador espacial "Challenger" (designación NASA: OV-099) fue el segundo orbitador del programa del transbordador espacial en entrar en servicio. Su primer vuelo se realizó el 4 de abril de 1983, y completó nueve misiones antes de desintegrarse en su décima misión, el 28 de enero de 1986, causando la muerte a sus siete tripulantes a los 73 segundos de su lanzamiento. El "Challenger" fue reemplazado por el transbordador espacial "Endeavour", que voló por primera vez en 1992, seis años después del accidente.

El nombre "Challenger" proviene del HMS "Challenger", una corbeta británica que llevó a cabo una expedición de investigación marina global en el año 1870.

El "Challenger" fue construido a partir de la estructura STA-099, utilizada en principio en pruebas estructurales. El STA-099 no estaba diseñado para vuelos, pero la NASA consideró que el reciclaje sería menos caro que reequipar el transbordador de pruebas "Enterprise" (OV-101) para vuelo espacial, como estaba planeado originalmente.

El "Challenger", al igual que los orbitadores construidos después de éste, tenía menos losetas en su sistema de protección térmica que el "Columbia". La mayoría de las losetas en las puertas de carga, la superficie superior de las alas y la parte trasera del fuselaje fueron reemplazadas por un aislamiento de nomex blanco de DuPont. Esta modificación permitía al transbordador llevar 1100 kg más de carga útil que el "Columbia". El "Challenger" también fue el primer orbitador en llevar un sistema de pantallas HUD similares a los que se utilizan en aviones militares y civiles modernos. Este sistema eliminaba la necesidad de mirar al panel de instrumentos durante el descenso y permitía a la tripulación concentrarse más en el vuelo.

Tras su vuelo inicial, el "Challenger" se convirtió en la bestia de carga de la flota de transbordadores de la NASA, volando en más misiones por año que el "Columbia". En los años 1983 y 1984, el "Challenger" voló en el 85 % de las misiones del programa STS. Incluso cuando los orbitadores "Discovery" y "Atlantis" se unieron a la flota, el "Challenger" siguió siendo utilizado para trabajo pesado hasta tres veces por año desde 1983 hasta 1985.

El "Challenger", junto con el "Discovery", fue modificado en el centro espacial John F. Kennedy para poder llevar la etapa superior del cohete Centauro en su bahía de carga. Si la misión STS-51-L hubiese sido exitosa, la siguiente misión del transbordador hubiera sido el despliegue de la sonda "Ulysses" con el Centaur, para el estudio de las regiones polares del Sol.

El transbordador "Challenger" marcó varios hitos en el vuelo espacial, como la primera mujer estadounidense, el primer afroamericano y el primer paseo autónomo en el espacio, tres misiones Spacelab y el primer despegue y aterrizaje nocturnos de un transbordador espacial. Sin embargo, también fue el "Challenger" el primer transbordador en ser destruido en un accidente durante una misión.

El "Challenger" se desintegró a los 73 s del lanzamiento de la misión STS-51-L, la décima misión del orbitador, el 28 de enero de 1986, cuando una junta tórica de su cohete impulsor (SRB) derecho falló en su función de estanqueidad.

Las juntas fallaron debido principalmente a la sobrecompresión repetida durante el montaje y que las bajas temperaturas agravaron aún más. Esta anomalía fue advertida por los ingenieros de Morton Thiokol, los fabricantes de las partes del impulsor, se advirtió a la NASA, pero por presión de la misma NASA los ingenieros de Morton Thiokol cedieron y autorizaron el despegue.

El combustible para cohetes estaba enriquecido con viruta de aluminio que le proporcionaba un mayor poder de empuje; probablemente la escoria de aluminio selló momentáneamente la fisura de la junta retrasando la catástrofe. En el momento del despegue, el impulsor derecho deja escapar un humo negro nueve veces en un periodo de 2,6 s y se detiene cuando la nave se impulsa. Al momento de la ignición el transbordador cabecea 1 m de lado a lado antes de impulsarse; con cada cabeceo escapa el humo negro.

A los 58 s, el transbordador pasó a momento Q (inestabilidad) cuando cruzó por una fuerte corriente de viento; esto abrió nuevamente la junta. Así mismo, hizo que una columna de fuego se escapase del SRB y quemase el tanque de combustible externo (ET). El hidrógeno líquido del tanque externo derramado comenzó a arder, cortando las abrazaderas que mantenían al SRB. El SRB se balanceó y golpeó el ala derecha del Challenger. Esto causó que el montaje completo girase bruscamente y el transbordador quedó expuesto a fuerzas aerodinámicas incontroladas.

El transbordador entonces se vio envuelto en una gigantesca bola de fuego a los 73 s del despegue, desintegrándose casi en su totalidad, emergiendo la cabina intacta de la conflagración.

Los siete tripulantes fallecieron al impactar la cabina de la nave contra el océano, tras una larga caída de casi tres minutos. Las circunstancias finales de su muerte se desconocen; la comisión investigadora del accidente determinó como «poco probable» el hecho de que alguno de ellos estuviese consciente al momento del impacto, aunque posteriormente salieron a la luz pública evidencias de que al menos cuatro de los miembros de la tripulación pudieron activar sus sistemas auxiliares de suministro de oxígeno, y que intentaron socorrerse mutuamente.

La cabina fue la única sección de la nave que logró sobrevivir a la terrible destrucción de la explosión, pero no pudo soportar el impacto final contra el océano, desintegrándose junto con sus ocupantes. El módulo de la cabina cayó desde una altura de 15 240 metros, produciéndose así el fatal desenlace.

La NASA había estimado las probabilidades de un accidente catastrófico durante el lanzamiento (el momento más peligroso del vuelo espacial) en una proporción de 1 a 438.

Este accidente, el más impactante del Programa del Transbordador Espacial, perjudicó seriamente la reputación de la NASA como agencia espacial y la propuesta de la participación de civiles, promulgada por Ronald Reagan y concretada con la maestra de primaria Christa McAuliffe, echó por tierra todas las estructuras administrativas y de seguridad. La NASA suspendió temporalmente sus vuelos espaciales hasta 1988.

Una investigación posterior concluyó una serie de errores cometidos:


Todos estos factores se encadenaron uno a uno y fueron las causantes del desastre.

Hipótesis:

Los astronautas no disponían de paracaídas o equipo de eyección; tampoco tenían un entrenamiento específico para un caso como ese, circunstancias que originaron fuertes críticas a la NASA.





</doc>
<doc id="8993" url="https://es.wikipedia.org/wiki?curid=8993" title="Par de bases">
Par de bases

En genética un par de bases (en inglés bp) es una unidad que consta de dos nucleobases unidas entre sí por enlaces de hidrógeno. Forman los bloques de construcción de la doble hélice de ADN, y contribuyen a la estructura plegada de ADN y ARN. Dictados por patrones de enlace de hidrógeno específicos, los pares de bases de Watson-Crick (guanina-citosina y adenina-timina) permiten a la hélice del ADN mantener una estructura helicoidal regular que depende sutilmente de su secuencia de nucleótidos. La naturaleza complementaria de esta estructura basada en parejas proporciona una copia de seguridad de toda la información genética codificada en el ADN bicatenario. La estructura regular y la redundancia de datos proporcionada por la doble hélice de ADN hacen que el ADN sea muy adecuado para el almacenamiento de información genética, mientras que el acoplamiento de bases entre el ADN y los nucleótidos entrantes proporciona el mecanismo a través del cual la ADN polimerasa replica el ADN y la ARN polimerasa transcribe ADN en ARN. Muchas proteínas de unión a ADN pueden reconocer patrones específicos de apareamiento de bases que identifican regiones reguladoras particulares de genes.

Los pares de bases intramoleculares pueden ocurrir dentro de los ácidos nucleicos monocatenarios. Esto es particularmente importante en las moléculas de ARN (por ejemplo, ARN de transferencia), donde los pares de bases de Watson-Crick (guanina-citosina y adenina-uracilo) permiten la formación de hélices bicatenarias cortas y una amplia variedad de interacciones no Watson-Crick (Por ejemplo, GU o AA) permiten que los ARN se plieguen en una amplia gama de estructuras tridimensionales específicas. Además, el apareamiento de bases entre ARN de transferencia (ARNt) y ARN mensajero (ARNm) constituye la base para los eventos de reconocimiento molecular que dan como resultado que la secuencia de nucleótidos de ARNm se traduce en la secuencia de aminoácidos de proteínas a través del código genético.

El tamaño de un gen individual o del genoma entero de un organismo se mide a menudo en pares de bases porque el ADN es generalmente de doble hebra. Por lo tanto, el número de pares de bases totales es igual al número de nucleótidos en una de las hebras (con la excepción de regiones monocatenarias no codificantes de telómeros). Se calcula que el genoma humano haploide (23 cromosomas) tiene aproximadamente 3,2 mil millones de bases de largo y contiene 20 000-25 000 genes distintos que codifican las proteínas. Una kilobase (kb) es una unidad de medida en biología molecular igual a 1000 pares de bases de ADN o ARN. La cantidad total de pares de bases de ADN relacionados en la Tierra se estima en 5,0 x 10, y pesa 50 mil millones de toneladas. En comparación, se ha estimado que la masa total de la biosfera es de 4 TtC (billónes de toneladas de carbono).

Un par de bases consiste en dos nucleótidos opuestos y complementarios en las cadenas de ADN y ARN que están conectadas por puentes de hidrógeno.

En el ADN, adenina y timina así como guanina y citosina pueden formar un par de bases. En ARN, la timina es reemplazada por el uracilo, conectándose este con la adenina.

Las siguientes abreviaciones son usadas comúnmente para referirse a la longitud de una molécula de ADN/ARN:


En el caso de una molécula de ADN/ARN monocatenario se suele emplear como medida de longitud el número de nucleótidos, abreviado nt (o knt, Mnt, Gnt), puesto que en estas moléculas las bases no se organizan en pares.

Adicionalmente, se usa el centimorgan para indicar distancias en los cromosomas, aunque el número de pares de bases que abarca esta unidad varía extensamente. En el genoma de los seres humanos abarca alrededor de un millón de pares de bases.


</doc>
<doc id="8994" url="https://es.wikipedia.org/wiki?curid=8994" title="Genoma humano">
Genoma humano

El genoma humano es el genoma del "Homo sapiens", es decir, la secuencia de ADN contenida en 23 pares de cromosomas en el núcleo de cada célula humana diploide. De los 23 pares, 22 son cromosomas autosómicos y un par determinante del sexo (dos cromosomas X en mujeres, y un X y un Y en varones). El genoma haploide (es decir, una sola representación por cada par) tiene una longitud total aproximada de 3200 millones de pares de bases de ADN (3200 Mb) que contienen unos 20 000-25 000 genes. De las 3200 Mb, 2950 Mb corresponden a eucromatina y unas 250 Mb a heterocromatina. El Proyecto Genoma Humano produjo una secuencia de referencia del genoma humano eucromático, usado en todo el mundo en las ciencias biomédicas.

La secuencia de ADN que conforma el genoma humano contiene la información codificada necesaria para la expresión altamente coordinada y adaptable al ambiente del proteoma humano, es decir, del conjunto de las proteínas del ser humano. Las proteínas, y no el ADN, son las principales biomoléculas efectoras; poseen funciones estructurales, enzimáticas, metabólicas, reguladoras y señalizadoras, organizándose en enormes redes funcionales de interacciones. En definitiva, el proteoma fundamenta la particular morfología y funcionalidad de cada célula. Asimismo, la organización estructural y funcional de las distintas células conforma cada tejido y cada órgano, y, finalmente, el organismo vivo en su conjunto. Así, el genoma humano contiene la información básica necesaria para el desarrollo físico de un ser humano completo.

El genoma humano presenta una densidad de genes muy inferior a la que inicialmente se había predicho, con solo 1.5 % de su longitud compuesta por exones codificantes de proteínas. Un 70 % está compuesto por ADN extragénico y un 30% por secuencias relacionadas con genes. Del total de ADN extragénico, aproximadamente un 70 % corresponde a repeticiones dispersas, de manera que, más o menos, la mitad del genoma humano corresponde a secuencias repetitivas de ADN. Por su parte, del total de ADN relacionado con genes se estima que el 95 % corresponde a ADN no codificante: pseudogenes, fragmentos de genes, intrones o secuencias UTR, entre otros.
En el genoma humano se detectan más de 280 000 elementos reguladores, aproximadamente un total de 7Mb de secuencia, que se originaron por medio de inserciones de elementos móviles. Estas regiones reguladoras se conservan en elementos no exónicos (CNEEs), fueron nombrados como: SINE, LINE, LTR. Se sabe que al menos entre un 11 % y un 20 % de estas secuencias reguladoras de genes, que están conservadas entre especies, fue formado por elementos móviles.

El Proyecto Genoma Humano, que se inició en el año 1990, tuvo como propósito descifrar el código genético contenido en los 23 pares de cromosomas, en su totalidad. En 2005 se dio por finalizado este estudio llegando a secuenciarse aproximadamente 28 000 genes. Y, el 2 de junio de 2016, los científicos anunciaron formalmente el Proyecto Genoma Humano-Escrito (acrónimo en inglés HGP-Write) un plan para sintetizar el genoma humano.

La función de la gran mayoría de las bases del genoma humano es desconocida. El (acrónimo de "ENCyclopedia Of DNA Elements") ha trazado regiones de transcripción, asociación a factores de transcripción, estructura de la cromatina y modificación de las histonas. Estos datos han permitido asignar funciones bioquímicas para el 80 % del genoma, principalmente, fuera de los exones codificantes de proteínas. El proyecto ENCODE proporciona nuevos conocimientos sobre la organización y la regulación de los genes y el genoma, y un recurso importante para el estudio de la biología humana y las enfermedades.

El genoma humano (como el de cualquier organismo eucariota) está formado por cromosomas, que son largas secuencias continuas de ADN altamente organizadas espacialmente (con ayuda de proteínas histónicas y no histónicas) para adoptar una forma ultracondensada en metafase. Son observables con microscopía óptica convencional o de fluorescencia mediante técnicas de citogenética y se ordenan formando un cariotipo. 

El cariotipo humano normal contiene un total de 23 pares de cromosomas distintos: 22 pares de autosomas más 1 par de cromosomas sexuales que determinan el sexo del individuo. Los cromosomas 1-22 fueron numerados en orden decreciente de tamaño en base al cariotipo. Sin embargo, posteriormente pudo comprobarse que el cromosoma 22 es en realidad mayor que el 21. 

Las células somáticas de un organismo poseen en su núcleo un total de 46 cromosomas (23 pares): una dotación de 22 autosomas procedentes de cada progenitor y un par de cromosomas sexuales, un cromosoma X de la madre y un X o un Y del padre. "(Ver imagen 1)". Los gametos -óvulos y espermatozoides- poseen una dotación haploide de 23 cromosomas.

Un gen es la unidad básica de la herencia, y porta la información genética necesaria para la síntesis de una proteína (genes codificantes) o de un ARN no codificante (genes de ARN). Está formado por una secuencia promotora, que regula su expresión, y una secuencia que se transcribe, compuesta a su vez por: secuencias UTR (regiones flanqueantes no traducidas), necesarias para la traducción y la estabilidad del ARNm, exones (codificantes) e intrones, que son secuencias de ADN no traducidas situadas entre dos exones que serán eliminadas en el procesamiento del ARNm (ayuste).

Actualmente se estima que el genoma humano contiene entre 20 000 y 25 000 genes codificantes de proteínas, estimación muy inferior a las predicciones iniciales que hablaban de unos 100 000 genes o más. Esto implica que el genoma humano tiene menos del doble de genes que organismos eucariotas mucho más simples, como la mosca de la fruta o el nematodo "Caenorhabditis elegans". Sin embargo, las células humanas recurren ampliamente al "splicing" (ayuste) alternativo para producir varias proteínas distintas a partir de un mismo gen, como consecuencia de lo cual el proteoma humano es más amplio que el de otros organismos mucho más simples. En la práctica, el genoma "tan sólo" porta la información necesaria para una expresión perfectamente coordinada y regulada del conjunto de proteínas que conforman el proteoma, siendo éste el encargado de ejecutar la mayor parte de las funciones celulares.

Con base en los resultados iniciales arrojados por el proyecto ENCODE (acrónimo de ENCyclopedia Of DNA Elements), algunos autores han propuesto redefinir el concepto actual de gen. Las observaciones más recientes hacen difícilmente sostenible la visión tradicional de un gen, como una secuencia formada por las regiones UTRs, los exones y los intrones. Estudios detallados han hallado un número de secuencias de inicio de transcripción por gen muy superior a las estimaciones iniciales, y algunas de estas secuencias se sitúan en regiones muy alejadas de la traducida, por lo que los UTR 5' pueden abarcar secuencias largas dificultando la delimitación del gen. Por otro lado, un mismo transcrito puede dar lugar a ARN maduros totalmente diferentes (ausencia total de solapamiento), debido a una gran utilización del "splicing" alternativo. De este modo, un mismo transcrito primario puede dar lugar a proteínas de secuencia y funcionalidad muy dispar. En consecuencia, algunos autores han propuesto una nueva definición de gen,: la unión de secuencias genómicas que codifican un conjunto coherente de productos funcionales, potencialmente solapantes. De este modo, se identifican como genes los genes ARN y los conjuntos de secuencias traducidas parcialmente solapantes (se excluyen, así, las secuencias UTR y los intrones, que pasan a ser considerados como "regiones asociadas a genes", junto con los promotores). De acuerdo con esta definición, un mismo transcrito primario que da lugar a dos transcritos secundarios (y dos proteínas) no solapantes debe considerarse en realidad dos genes diferentes, independientemente de que estos presenten un solapamiento total o parcial de sus transcritos primarios.

Las nuevas evidencias aportadas por ENCODE, según las cuales las regiones UTR no son fácilmente delimitables y se extienden largas distancias, obligarían a reidentificar nuevamente los genes que en realidad componen el genoma humano. De acuerdo con la definición tradicional (actualmente vigente), sería necesario identificar como un mismo gen a todos aquellos que muestren un solapamiento parcial (incluyendo las regiones UTR y los intrones), con lo que a la luz de las nuevas observaciones, los genes incluirían múltiples proteínas de secuencia y funcionalidad muy diversa. Colateralmente se reduciría el número de genes que componen el genoma humano. La definición propuesta, en cambio, se fundamenta en el producto funcional del gen, por lo que se mantiene una relación más coherente entre un gen y una función biológica. Como consecuencia, con la adopción de esta nueva definición, el número de genes del genoma humano aumentará significativamente.

Además de los genes codificantes de proteínas, el genoma humano contiene varios miles de genes ARN, cuya transcripción reproduce ARN de transferencia (ARNt), ARN ribosómico (ARNr), microARN (miARN), u otros genes ARN no codificantes. Los ARN ribosómico y de transferencia son esenciales en la constitución de los ribosomas y en la traducción de las proteínas. Por su parte, los microARN tienen gran importancia en la regulación de la expresión génica, estimándose que hasta un 20-30 % de los genes del genoma humano puede estar regulado por el mecanismo de interferencia por miARN. Hasta el momento se han identificado más de 300 genes de miARN y se estima que pueden existir unos 500.

A continuación se muestran algunos valores promedio del genoma humano. Cabe advertir, sin embargo, que la enorme heterogeneidad que presentan estas variables hace poco representativos a los valores promedio, aunque tienen valor orientativo. 

La densidad media de genes es de 1 gen cada 100 kb, con un tamaño medio de 20-30 kb, y un número de exones promedio de 7-8 por cada gen, con un tamaño medio de 150 nucleótidos. El tamaño medio de un ARNm es de 1.8-2.2 kb, incluyendo las regiones UTR (regiones no traducidas flanqueantes), siendo la longitud media de la región codificante de 1.4 kb.

El genoma humano se caracteriza por presentar una gran heterogeneidad en su secuencia. En particular, la riqueza en bases de guanina (G) y citosina (C) frente a las de adenina (A) y timina (T) se distribuye heterogéneamente, con regiones muy ricas en G+C flanqueadas por regiones muy pobres, siendo el contenido medio de G+C del 41 %, menor al teóricamente esperado (50 %). Dicha heterogeneidad esta correlacionada con la riqueza en genes, de manera que los genes tienden a concentrarse en las regiones más ricas en G+C. Este hecho era conocido ya desde hace años gracias a la separación mediante centrifugación en gradiente de densidad de regiones ricas en G+C (que recibieron el nombre de isócoros H; del inglés "High") y regiones ricas en A+T (isócoros L; del inglés "Low").

El genoma tiene diversos sistemas de regulación de la expresión génica, basados en la regulación de la unión de factores de transcripción a las secuencias promotoras, en mecanismos de modificación epigenética (metilación del ADN o metilación-acetilación de histonas) o en el control de la accesibilidad a los promotores determinada por el grado de condensación de la cromatina; todos ellos muy interrelacionados. Además hay otros sistemas de regulación a nivel del procesamiento, estabilidad y traducción del ARNm, entre otros. Por lo tanto, la expresión génica está intensamente regulada, lo cual permite desarrollar los múltiples fenotipos que caracterizan los distintos tipos celulares de un organismo eucariota multicelular, al mismo tiempo que dota a la célula de la plasticidad necesaria para adaptarse a un medio cambiante. No obstante, toda la información necesaria para la regulación de la expresión génica, en función del ambiente celular, está codificada en la secuencia de ADN al igual que lo están los genes.

Las secuencias reguladoras son típicamente secuencias cortas presentes en las proximidades o en el interior (frecuentemente en intrones) de los genes. En la actualidad, el conocimiento sistemático de estas secuencias y de cómo actúan en complejas redes de regulación génica, sensibles a señales exógenas, es muy escaso y está comenzando a desarrollarse mediante estudios de genómica comparada, bioinformática y biología de sistemas. La identificación de secuencias reguladoras se basa en parte en la búsqueda de regiones no codificantes evolutivamente conservadas. Por ejemplo, la divergencia evolutiva entre el ratón y el ser humano ocurrió hace 70 a 90 millones de años. Mediante estudios de genómica comparada, alineando secuencias de ambos genomas pueden identificarse regiones con alto grado de coincidencia, muchas correspondientes a genes y otras a secuencias no codificantes de proteínas pero de gran importancia funcional, dado que han estado sometidas a presión selectiva.

Reciben este nombre regiones que han mostrado una constancia evolutiva casi total, mayor incluso que las secuencias codificantes de proteínas, mediante estudios de genómica comparada. Estas secuencias generalmente se solapan con intrones de genes implicados en la regulación de la transcripción o en el desarrollo embrionario y con exones de genes relacionados con el procesamiento del ARN. Su función es generalmente poco conocida, pero probablemente de extrema importancia dado su nivel de conservación evolutiva, tal y como se ha expuesto en el punto anterior.

En la actualidad se han encontrado unos 500 segmentos de un tamaño mayor a 200 pares de bases totalmente conservados (100 % de coincidencia) entre los genomas de humano, ratón y rata, y casi totalmente conservados en perro (99 %) y pollo (95 %).

En el genoma humano se han encontrado asimismo unos 19 000 pseudogenes, que son versiones completas o parciales de genes que han acumulado diversas mutaciones y que generalmente no se transcriben. Se clasifican en pseudogenes no procesados (~30 %) y pseudogenes procesados (~70 %)

Las regiones intergénicas o extragénicas comprenden la mayor parte de la secuencia del genoma humano, y su función es generalmente desconocida. Buena parte de estas regiones está compuesta por elementos repetitivos, clasificables como repeticiones en tándem o repeticiones dispersas, aunque el resto de la secuencia no responde a un patrón definido y clasificable. 
Gran parte del ADN intergénico puede ser un artefacto evolutivo sin una función determinada en el genoma actual, por lo que tradicionalmente estas regiones han sido denominadas ADN "basura" ("Junk DNA"), denominación que incluye también las secuencias intrónicas y pseudogenes. No obstante, esta denominación no es la más acertada dado el papel regulador conocido de muchas de estas secuencias. Además el notable grado de conservación evolutiva de algunas de estas secuencias parece indicar que poseen otras funciones esenciales aún desconocidas o poco conocidas. Por lo tanto, algunos prefieren denominarlo "ADN no codificante" (aunque el llamado "ADN basura" incluye también transposones codificantes) o "ADN repetitivo". Algunas de estas regiones constituyen en realidad genes precursores para la síntesis de microARN (reguladores de la expresión génica y del silenciamiento génico).

Estudios recientes enmarcados en el proyecto ENCODE han obtenido resultados sorprendentes, que exigen la reformulación de nuestra visión de la organización y la dinámica del genoma humano. Según estos estudios, el 15 % de la secuencia del genoma humano se transcribe a ARN maduros, y hasta el 90 % se transcribe al menos a transcritos inmaduros en algún tejido: Así, una gran parte del genoma humano codifica genes de ARN funcionales. Esto es coherente con la tendencia de la literatura científica reciente a asignar una importancia creciente al ARN en la regulación génica. Asimismo, estudios detallados han identificado un número mucho mayor de secuencias de inicio de transcripción por gen, algunas muy alejadas de la región próxima a la traducida. Como consecuencia, actualmente resulta más complicado definir una región del genoma como génica o intergénica, dado que los genes y las secuencias relacionadas con los genes se extienden en las regiones habitualmente consideradas intergénicas.

Son repeticiones que se ordenan de manera consecutiva, de modo que secuencias idénticas, o casi, se disponen unas detrás de otras.

El conjunto de repeticiones en tándem de tipo satélite comprende un total de 250 Mb del genoma humano. Son secuencias de entre 5 y varios cientos de nucleótidos que se repiten en tándem miles de veces generando regiones repetidas con tamaños que oscilan entre 100 kb (100 000 nucleótidos) hasta varias megabases.

Reciben su nombre de las observaciones iniciales de centrifugaciones en gradiente de densidad del ADN genómico fragmentado, que reportaban una banda principal correspondiente a la mayor parte del genoma y tres bandas satélite de menor densidad. Esto se debe a que las secuencias satélite tienen una riqueza en nucleótidos A+T superior a la media del genoma y en consecuencia son menos densas.

Hay principalmente 6 tipos de repeticiones de ADN satélite

Están compuestas por una unidad básica de secuencia de 6-25 nucleótidos que se repite en tándem generando secuencias de entre 100 y 20 000 pares de bases. Se estima que el genoma humano contiene unos 30 000 minisatélites. 

Diversos estudios han relacionado los minisatélites con procesos de regulación de la expresión génica, como el control del nivel de transcripción, el ayuste ("splicing") alternativo o la impronta ("imprinting"). Asimismo, se han asociado con puntos de fragilidad cromosómica dado que se sitúan próximos a lugares preferentes de rotura cromosómica, translocación genética y recombinación meiótica. Por último, algunos minisatélites humanos (~10 %) son hipermutables, presentando una tasa media de mutación entre el 0.5 % y el 20 % en las células de la línea germinal, siendo así las regiones más inestables del genoma humano conocidas hasta la fecha. 

En el genoma humano, aproximadamente el 90 % de los minisatélites se sitúan en los telómeros de los cromosomas. La secuencia básica de seis nucleótidos TTAGGG se repite miles de veces en tándem, generando regiones de 5-20 kb que conforman los telómeros.

Algunos minisatélites por su gran inestabilidad presentan una notable variabilidad entre individuos distintos. Se consideran polimorfismos multialélicos, dado que pueden presentarse en un número de repeticiones muy variable, y se denominan VNTR (acrónimo de "Variable number tandem repeat"). Son marcadores muy utilizados en genética forense, ya que permiten establecer una huella genética característica de cada individuo, y son identificables mediante Southern blot e hibridación.

Están compuestos por secuencias básicas de 2-4 nucleótidos, cuya repetición en tándem origina frecuentemente secuencias de menos de 150 nucleótidos. Algunos ejemplos importantes son el dinucleótido CA y el trinucleótido CAG. 

Los microsatélites son también polimorfismos multialélicos, denominados STR (acrónimo de "Short Tandem Repeats") y pueden identificarse mediante PCR, de modo rápido y sencillo.
Se estima que el genoma humano contiene unos 200 000 microsatélites, que se distribuyen más o menos homogéneamente, al contrario que los minisatélites, lo que los hace más informativos como marcadores.

Son secuencias de ADN que se repiten de modo disperso por todo el genoma, constituyendo el 45 % del genoma humano. Los elementos cuantitativamente más importantes son los LINEs y SINEs, que se distinguen por el tamaño de la unidad repetida.

Estas secuencias tienen la potencialidad de autopropagarse al transcribirse a una ARNm intermediario, retrotranscribirse e insertarse en otro punto del genoma. Este fenómeno se produce con una baja frecuencia, estimándose que 1 de cada 100-200 neonatos portan una inserción nueva de un Alu o un L1, que pueden resultar patogénicos por mutagénesis insercional, por desregulación de la expresión de genes próximos (por los propios promotores de los SINE y LINE) o por recombinación ilegítima entre dos copias idénticas de distinta localización cromosómica (recombinación intra o intercromosómica), especialmente entre elementos Alu.

Acrónimo del inglés "Short Interspersed Nuclear Elements" (Elementos nucleares dispersos cortos). Son secuencias cortas, generalmente de unos pocos cientos de bases, que aparecen repetidas miles de veces en el genoma humano. Suponen el 13 % del genoma humano, un 10 % debido exclusivamente a la familia de elementos Alu (característica de primates). 

Los elementos Alu son secuencias de 250-280 nucleótidos presentes en 1 500 000 de copias dispersas por todo el genoma. Estructuralmente son dímeros casi idénticos, excepto que la segunda unidad contiene un inserto de 32 nucleótidos, siendo mayor que la primera. En cuanto a su secuencia, tienen una considerable riqueza en G+C (56 %), por lo que predominan en las bandas R, y ambos monómeros presentan una cola poliA (secuencia de adeninas) vestigio de su origen de ARNm. Además poseen un promotor de la ARN polimerasa III para transcribirse. Se consideran retrotransposones no autónomos, ya que dependen para propagarse de la retrotranscripción de su ARNm por una retrotranscriptasa presente en el medio.

Acrónimo del inglés "Long Interspersed Nuclear Elements" (Elementos nucleares dispersos largos). Constituyen el 20 % del genoma humano, contiene unos 100 000-500 000 copias de retrotransposones L1 que es la familia de mayor importancia cuantitativa, es una secuencia de 6 kb repetida unas 800 000 veces de modo disperso por todo el genoma, aunque la gran mayoría de las copias es incompleta al presentar el extremo 5' truncado por una retrotranscripción incompleta. Así, se estima que hay unas 5000 copias completas de L1, sólo 90 de las cuales son activas, estando el resto inhibidas por metilación de su promotor.

Su riqueza en G+C es del 42 %, próxima a la media del genoma (41 %) y se localizan preferentemente en las bandas G de los cromosomas. Poseen además un promotor de la ARN polimerasa II.

Los elementos LINE completos son codificantes. En concreto LINE-1 codifica dos proteínas:
Estos elementos móviles están flanqueados por 2 regiones no codificantes, denominados como 5´UTR y 3´UTR.

Por lo tanto, se consideran retrotransopsones autónomos, ya que codifican las proteínas que necesitan para propagarse. La ARN polimerasa II presente en el medio transcribe el LINE, y este ARNm se traduce en ambos marcos de lectura produciendo una retrotranscriptasa que actúa sobre el ARNm generando una copia de ADN del LINE, potencialmente capaz de insertarse en el genoma. Asimismo estas proteínas pueden ser utilizadas por pseudogenes procesados o elementos SINE para su propagación. 

La transcripción se inicia en un promotor interno del extremo 5´UTR. La endonucleasa de L1 genera una mella en una única cadena del ADN genómico, en una secuencia consenso 5´TTTTT/A3´.

Diversos estudios han mostrado que las secuencias LINE pueden tener importancia en la regulación de la expresión génica, habiéndose comprobado que los genes próximos a LINE presentan un nivel de expresión inferior. Esto es especialmente relevante porque aproximadamente el 80 % de los genes del genoma humano contiene algún elemento L1 en sus intrones.

Se ha visto que la inserción aleatoria de L1 activos en el genoma humano ha dado lugar a enfermedades genéticas, ya que interfiere en la expresión normal. También se observa una predilección de L1 por regiones ricas en AT.

Acrónimo de "Human endogenous retrovirus" (retrovirus endógenos humanos). Los retrovirus son virus cuyo genoma está compuesto por ARN, capaces de retrotranscribirse e integrar su genoma en el de la célula infectada. Así, los HERV son copias parciales del genoma de retrovirus integrados en el genoma humano a lo largo de la evolución de los vertebrados, vestigios de antiguas infecciones retrovirales que afectaron a células de la línea germinal. 
Algunas estimaciones establecen que hay unas 98 000 secuencias HERV, mientras que otras afirman que son más de 400 000. En cualquier caso, se acepta que en torno al 5-8 % del genoma humano está constituido por genomas antiguamente virales. El tamaño de un genoma retroviral completo es de en torno a 6-11 kb, pero la mayoría de los HERV son copias incompletas.

A lo largo de la evolución estas secuencias sin interés para el genoma hospedador han ido acumulando mutaciones sin sentido y deleciones que los han inactivado. Aunque la mayoría de las HERV tienen millones de años de antigüedad, al menos una familia de retrovirus se integró durante la divergencia evolutiva de humanos y chimpancés, la familia HERV-K(HML2), que supone en torno al 1 % de los HERV.

Bajo la denominación de transposones a veces se incluyen los retrotransposones, tales como los pseudogenes procesados, los SINEs y los LINEs. En tal caso se habla de transposones de clase I para hacer referencia a los retrotransposones, y de clase II para referirse a transposones de ADN, a los que se dedica el presente apartado.

Los transposones de ADN completos poseen la potencialidad de autopropagarse sin un intermediario de ARNm seguido de retrotranscripción. Un transposón contiene el gen de una enzima transposasa, flanqueado por repeticiones invertidas. Su mecanismo de transposición se basa en "cortar y pegar", moviendo su secuencia a otra localización distinta del genoma. Los distintos tipos de transposasas actúan de modo diferente, habiendo algunas capaces de unirse a cualquier parte del genoma mientras que otras se unen a secuencias diana específicas. La transposasa codificada por el propio transposón lo extrae realizando dos cortes flanqueantes en la hebra de ADN, generando extremos cohesivos, y lo inserta en la secuencia diana en otro punto del genoma. Una ADN polimerasa rellena los huecos generados por los extremos cohesivos y una ADN ligasa restablece los enlaces fosfodiéster, recuperando la continuidad de la secuencia de ADN. Esto conlleva una duplicación de la secuencia diana en torno al transposón, en su nueva localización.

Se estima que el genoma humano contiene unas 300 000 copias de elementos repetidos dispersos originados por transposones de ADN, constituyendo un 3 % del genoma. Hay múltiples familias, de las que cabe destacar por su importancia patogénica por la generación de reordenaciones cromosómicas los elementos mariner, así como las familias MER1 y MER2.

Si bien dos seres humanos del mismo sexo comparten un porcentaje elevadísimo (en torno al 99.9 %) de su secuencia de ADN, lo que nos permite trabajar con una "única" secuencia de referencia, pequeñas variaciones genómicas fundamentan buena parte de la variabilidad fenotípica interindividual. Una variación en el genoma, por sustitución, deleción o inserción, se denomina polimorfismo o alelo genético. Puede localizarse tanto en regiones codificantes como no codificantes. No todo polimorfismo genético provoca una alteración en la secuencia de una proteína o de su nivel de expresión, es decir, muchos son silenciosos y carecen de expresión fenotípica.

La principal fuente de variabilidad en los genomas de dos seres humanos procede de las variaciones en un solo nucleótido, conocidas como SNP ("Single nucleotide polimorphisms"), en las cuales se han centrado la mayor parte de los estudios. Dada su importancia, en la actualidad existe un proyecto internacional ("International HapMap Project") para catalogar a gran escala los SNPs del genoma humano. En este contexto, la denominación de SNP frecuentemente se restringe a aquellos polimorfismos de un solo nucleótido en los que el alelo menos frecuente aparece en al menos el 1 % de la población.

Los SNP son marcadores tetralélicos, dado que en teoría en una posición puede haber cuatro nucleótidos distintos, cada uno de los cuales identificaría un alelo; sin embargo, en la práctica suelen presentar solo dos alelos en la población. Se estima que la frecuencia de SNP en el genoma humano es de un SNP cada 500-100 pares de bases, de los que una parte relevante son polimorfismos codificantes, que causan la sustitución de un aminoácido por otro en una proteína.

Gracias a su abundancia y a que presentan una distribución aproximadamente uniforme en el genoma, han tenido gran utilidad como marcadores para los mapas de ligamiento, herramienta fundamental del Proyecto Genoma Humano. Además son fácilmente detectables a gran escala mediante el empleo de chips de ADN (comúnmente conocidos como "microarrays").

Poco a poco su estudio por nuevas técnicas de secuenciación (NGS) está adquiriendo un mayor protagonismo en el ámbito clínico debido a que se ha demostrado en muchos de ellos asociación con enfermedades y pueden servir como marcadores de susceptibilidad.

La identificación de nuevas variantes de nucleótido único obtenidas por este método se denominan SNVs ("Single Nucleotide Variants") y carecen de limitaciones de frecuencia. A pesar de que se conoce su amplia distribución, existen regiones con un mayor grado de conservación, o lo que es lo mismo, menor tendencia a la variación, dada la estrecha asociación con una posible función y esencialidad celular. De esta manera las zonas que codifican a proteínas están más conservadas que zonas intergénicas, del mismo modo que lo están exones y sobre todo zonas donadoras y aceptoras de "splicing" (con muy baja tolerancia al cambio) respecto a los intrones en regiones intragénicas, pues cambios en estas posiciones podrían derivar en el truncamiento de la proteína en cuestión. Cabe mencionar que dentro de los exones existe un enriquecimiento diferencial del número de variantes en las diferentes posiciones que conforman los codones y que tienden a seguir un patrón caracterizado por una pérdida de intolerancia a la variación del tercer nucleótido en esa posición, como consecuencia de la degeneración del código genético. Por otro lado, en las regiones que codifican a RNAs que no dan lugar a proteínas, se encuentra una mayor variabilidad en el caso de los snoRNAs frente a los lncRNAs. Con respecto a secuencias reguladoras no transcritas la variabilidad se concentra en sitios de unión a factores de transcripción y zonas promotoras, siendo estas últimas los elementos más variables del genoma.

Este tipo de variaciones se refiere a duplicaciones, inversiones, inserciones o variantes en el número de copias de segmentos grandes del genoma (por lo general de 1000 nucléotidos o más). Estas variantes implican a una gran proporción del genoma, por lo que se piensa que son, al menos, tan importantes como los SNPs.

Variación estructural es el término general para abarcar un grupo de alteraciones genómicas que implican segmentos de ADN mayores de 1 Kb. La variación estructural puede ser cuantitativa (variante en número de copia, que comprende: deleciones, inserciones y duplicaciones), posicional (translocaciones) y orientacional (inversiones).

A pesar de que este campo de estudio es relativamente nuevo (los primeros estudios a gran escala se publicaron en los años 2004 y 2005), ha tenido un gran auge, hasta el punto de que se ha creado un nuevo proyecto para estudiar este tipo de variantes en los mismos individuos en los que se basó el Proyecto HapMap.

Aunque aún quedan dudas acerca de las causas de este tipo de variantes, cada vez existe más evidencia a favor de que es un fenómeno recurrente que todavía continua moldeando y creando nuevas variantes del genoma.

Este tipo de variaciones han potenciado la idea de que el genoma humano no es una entidad estática, sino que se encuentra en constante cambio y evolución.

La alteración de la secuencia de ADN que constituye el genoma humano puede causar la expresión anormal de uno o más genes, originando un fenotipo patológico. Las enfermedades genéticas pueden estar causadas por mutación de la secuencia de ADN, con afectación de la secuencia codificante (produciendo proteínas "incorrectas") o de secuencias reguladoras (alterando el nivel de expresión de un gen), o por alteraciones cromosómicas, numéricas o estructurales. La alteración del genoma de las células germinales de un individuo se transmite frecuentemente a su descendencia. Actualmente el número de enfermedades genéticas conocidas es aproximadamente de 4 000, siendo la más común la fibrosis quística.

El estudio de las enfermedades genéticas frecuentemente se ha englobado dentro de la genética de poblaciones. Los resultados del Proyecto Genoma Humano son de gran importancia para la identificación de nuevas enfermedades genéticas y para el desarrollo de nuevos y mejores sistemas de diagnóstico genético, así como para la investigación en nuevos tratamientos, incluida la terapia génica.

Las mutaciones génicas pueden ser:



Las mutaciones génica pueden afectar a:



Son enfermedades genéticas causadas por mutación en un solo gen, que presentan una herencia de tipo mendeliano, fácilmente predecible. En la tabla se resumen los principales patrones de herencia que pueden mostrar, sus características y algunos ejemplos.

Otras alteraciones genéticas pueden ser mucho más complejas en su asociación con un fenotipo patológico. Son las enfermedades multifactoriales o poligénicas, es decir, aquellas que están causadas por la combinación de múltiples alelos genotípicos y de factores exógenos, tales como el ambiente o el estilo de vida. En consecuencia no presentan un patrón hereditario claro, y la diversidad de factores etiológicos y de riesgo dificulta la estimación del riesgo, el diagnóstico y el tratamiento.

Algunos ejemplos de enfermedades multifactoriales con etiología parcialmente genética son:

Las alteraciones genéticas pueden producirse también a escala cromosómica (cromosomopatías), causando severos trastornos que afectan a múltiples genes y que en muchas ocasiones son letales provocando abortos prematuros. Frecuentemente están provocadas por un error durante la división celular, que sin embargo no impide su conclusión. Las alteraciones cromosómicas reflejan una anormalidad en el número o en la estructura de los cromosomas, por lo que se clasifican en numéricas y estructurales. Provocan fenotipos muy diversos, pero frecuentemente presentan unos rasgos comunes:

Es una alteración del número normal de cromosomas de un individuo, que normalmente presenta 23 pares de cromosomas (46 en total), siendo cada dotación cromosómica de un progenitor (diploidía). Si la alteración afecta a un solo par de cromosomas se habla de aneuploidía, de manera que puede haber un solo cromosoma (monosomía) o más de dos (trisomía, tetrasomía...). Un ejemplo de gran prevalencia es la trisomía 21, responsable del Síndrome de Down. Si por el contrario la alteración afecta a todos los cromosomas se habla de euploidías, de manera que en teoría el individuo tiene una sola dotación cromosómica (haploidía, 23 cromosomas en total) o más de dos dotaciones (triploidía: 69 cromosomas; tetraploidía: 92 cromosomas...). En la práctica las euploidías causan letalidad embronaria (abortos) siendo muy pocos los nacidos vivos, y fallecen muy tempranamente. Las aneuploidías son mayoritariamente letales, salvo las trisomías de los cromosomas 13, 18, 21, X e Y (XXY, XYY), y la monosomía del cromosoma X. En la tabla se muestran las frecuencias de nacidos vivos con estas alteraciones.

Se denominan así las alteraciones en la estructura de los cromosomas, tales como las grandes deleciones o inserciones, reordenaciones del material genético entre cromosomas... detectables mediante técnicas de citogenética.







Los síndromes de inestabilidad cromosómica son un grupo de trastornos caracterizados por una gran inestabilidad de los cromosomas, que sufren con gran frecuencia alteraciones estructurales. Están asociados con un aumento de la malignidad de neoplasias.

Los estudios de genómica comparada se basan en comparación de secuencias genómicas a gran escala, generalmente mediante herramientas bioinformáticas. Dichos estudios permiten ahondar en el conocimiento de aspectos evolutivos de escala temporal y espacial muy diversa, desde el estudio de la evolución de los primeros seres vivos hace miles de millones de años o las radiaciones filogenéticas en mamíferos, hasta el estudio de las migraciones de seres humanos en los últimos 100 000 años, que explican la actual distribución de las distintas razas humanas.

Los estudios de genómica comparada con genomas de mamíferos sugieren que aproximadamente el 5 % del genoma humano se ha conservado evolutivamente en los últimos 200 millones de años; lo cual incluye la gran mayoría de los genes y secuencias reguladoras. Sin embargo, los genes y las secuencias reguladoras actualmente conocidas suponen solo el 2 % del genoma, lo que sugiere que la mayor parte de la secuencia genómica con gran importancia funcional es desconocida. Un porcentaje importante de los genes humanos presenta un alto grado de conservación evolutiva. La similitud entre el genoma humano y el del chimpancé ("Pan troglodytes") es del 98.77 %. En promedio, una proteína humana se diferencia de su ortóloga de chimpancé en tan solo dos aminoácidos, y casi un tercio de los genes tiene la misma secuencia. Una diferencia importante entre los dos genomas es el cromosoma 2 humano, que es el producto de una fusión entre los cromosomas 12 y 13 del chimpancé

Otra conclusión de la comparación del genoma de distintos primates es la notable pérdida de genes de receptores olfativos que se ha producido paralelamente al desarrollo de la visión en color (tricrómica) durante la evolución de primates.

Durante décadas las únicas evidencias que permitían profundizar en el conocimiento del origen y la expansión del "Homo sapiens" han sido los escasos hallazgos arqueológicos. Sin embargo, en la actualidad, los estudios de genómica comparada a partir de genomas de individuos actuales de todo el mundo, están aportando información muy relevante. Su fundamento básico consiste en identificar un polimorfismo, una mutación, que se asume que se originó en un individuo de una población ancestral, y que ha heredado toda su descendencia hasta la actualidad. Además, dado que las mutaciones parecen producirse a un ritmo constante, puede estimarse la antigüedad de una determinada mutación en base al tamaño del haplotipo en el que se sitúa, es decir, el tamaño de la secuencia conservada que flanquea la mutación. Esta metodología se ve complicada por el fenómeno de recombinación entre los pares de cromosomas de un individuo, procedentes de sus dos progenitores. Sin embargo, hay dos regiones en las que no existe dicho inconveniente porque presentan una herencia uniparental: el genoma mitocondrial (de herencia matrilineal), y el cromosoma Y (de herencia patrilineal).

En las últimas décadas, los estudios de genómica comparada basada en el genoma mitocondrial, y en menor medida en el cromosoma Y, han reportado conclusiones de gran interés. En diversos estudios se ha trazado la filogenia de estas secuencias, estimándose que todos los seres humanos actuales comparten un antepasado femenino común que vivió en África hace unos 150 000 años. Por su parte, por razones aún poco conocidas, la mayor convergencia del ADN del cromosoma Y establece que el antepasado masculino común más reciente data de hace unos 60 000 años. Estos individuos han sido bautizados como Eva mitocondrial e Y-cromosoma Adan. 

La mayor diversidad de marcadores genéticos y en consecuencia, los haplotipos de menor longitud, se han hallado en África. Todo el resto de la población mundial presenta solo una pequeña parte de estos marcadores, de modo que la composición genómica del resto de la población humana actual es solo un subconjunto de la que puede apreciarse en África. Esto induce a afirmar que un pequeño grupo de seres humanos (quizá en torno a un millar) emigró del continente africano hacia las costas de Asia occidental, hace unos 50 000 a 70 000 años, según estudios basados en el genoma mitocondrial. Hace unos 50 000 años alcanzaron Australia y hace 40 000 a 30 000 años otras subpoblaciones colonizaron Europa occidental y el centro de Asia. Asimismo, se estima que hace 20 000 a 15 000 años alcanzaron el continente americano a través del estrecho de Bering (el nivel del mar era menor durante la última glaciación, o glaciación de Würm o Wisconsin), poblando Sudamérica hace unos 15 000-12 000 años. No obstante, estos datos solo son estimaciones, y la metodología presenta ciertas limitaciones. En la actualidad, la tendencia es combinar los estudios de genómica comparada basados en el ADN mitocondrial con análisis de la secuencia del cromosoma Y.

La caracterización de la diversidad genética en África es un paso crucial para la mayoría de los análisis y para reconstruir la historia evolutiva. Un estudio publicado por la revista Science el pasado 13 de noviembre de 2015 muestra el primer genoma antiguo encontrado en el continente africano. Hasta el momento, ningún estudio había logrado secuenciar el genoma antiguo obtenido a partir de fósiles en este continente. La razón era la inestabilidad de la propia molécula de ADN, que se veía afectada por las condiciones de temperatura y humedad. Por lo tanto este nuevo hallazgo es un gran avance.

Los restos de "Mota" fueron fechados alrededor de hace 4500 años y por lo tanto son anteriores tanto a la expansión bantú y, aún más importante, a lo que se conoce como el reflujo de Eurasia occidental. Es un evento migratorio que se produjo hace unos 3.000 años, cuando poblaciones de las regiones de Eurasia occidental, como Oriente Próximo y Anatolia, inundaron de nuevo el Cuerno de África. 

Mediante la comparación de 250.000 pares de bases del genoma de Mota con 40 poblaciones africanas y 81 poblaciones de Europa y Asia contemporáneas, se vio que Mota estaba más estrechamente relacionado con el Ari, un grupo étnico que vive cerca de las tierras altas de Etiopía. Se ve que Mota es más similar a las poblaciones Ari. También es bastante similar a la Sandawe del Sur de Tanzania, Estas similitudes son muy importantes, entre otras razones, para descifrar el antiguo paisaje demográfico de África.

Aparte de los cromosomas Y y mitocondrial, muchos datos han sido obtenidos a partir de los cromosomas autosómicos. A partir de un conjunto de estudios genómicos de diversas poblaciones humanas se han obtenido las distintas variaciones genómicas que ayudan a determinar las migraciones humanas. El más completo y complejo sería el Proyecto 1000 Genomas, aunque otros proyectos como el Proyecto de Diversidad Genómica de Simons, el Proyecto Internacional HapMap, etc. también han aportado muchos datos. Todos ellos han aportado información sobre distintos SNPs, STR, VNTR y otras que ayudan a completar los árboles genéticos de las poblaciones humanas, que siguen estando incompletos.

Es el genoma propio de las mitocondrias de células eucariotas. La mitocondria es un orgánulo subcelular esencial en el metabolismo aerobio u oxidativo de las células eucariotas. Su origen es endosimbionte, es decir, antiguamente fueron organismos procariotas independientes captados por una célula eucariota ancestral, con la que desarrollaron una relación simbiótica. Las características de su genoma, por tanto, son muy semejantes a las de un organismo procariota actual, y su código genético es ligeramente distinto al considerado "universal". Para adaptarse al nicho intracelular y aumentar su tasa de replicación, el genoma mitocondrial se ha ido reduciendo sustancialmente a lo largo de su coevolución, presentando en la actualidad un tamaño de 16 569 pares de bases. Así, la gran mayoría de las proteínas localizadas en las mitocondrias (~1500 en mamíferos) están codificadas por el genoma nuclear (al que hacen referencia todos los apartados anteriores), de modo que muchos de estos genes fueron transferidos de la mitocondria al núcleo celular durante la coevolución de la célula eucariota. En la mayoría de mamíferos, solo la hembra transmite al zigoto sus mitocondrias, por lo que presentan, como ya se ha dicho, un patrón hereditario matrilineal. En general una célula humana media contiene 100-10 000 copias del genoma mitocondrial por cada célula, a razón de unas 2-10 moléculas de ADN por mitocondria. 

El genoma mitocondrial posee 37 genes:

Al contrario de lo que sucedía con el genoma nuclear, donde solo el 1.5 % era codificante, en el genoma mitocondrial el 97 % corresponde a secuencias codificantes. Es una única molécula de ADN doble hebra circular. Una de las hemihebras recibe el nombre de cadena pesada o cadena H, y contiene 28 de los 37 genes (2 ARNr, 14 ARNt y 12 polipéptidos). La hemihebra complementaria (cadena ligera o L) codifica los 9 genes restantes. En ambas cadenas, los genes de los ARNt aparecen distribuidos entre dos genes ARNr o codificantes de proteínas, lo cual es de gran importancia para el procesamiento del ARN mitocondrial.







Artículos



</doc>
<doc id="9005" url="https://es.wikipedia.org/wiki?curid=9005" title="Espermatozoide">
Espermatozoide

Un espermatozoide (del griego "sperma", semilla, y "zóo", animal) es una célula haploide que constituye el gameto masculino. Es una de las células más diferenciadas y su función es la formación de un cigoto totipotente al fusionarse su núcleo con el del gameto femenino, fenómeno que dará lugar, posteriormente, al embrión y al feto. En la fecundación humana, los espermatozoides dan el sexo a la nueva célula diploide, pues pueden llevar cromosoma sexual X o Y, mientras que el óvulo lleva solo el cromosoma X.

El espermatozoide fue descrito por primera vez en 1677 por el científico Anton van Leeuwenhoek, reconocido como "padre de la microbiología". Sin embargo, la primera persona en visualizarlos fue un estudiante de medicina llamado Johan Ham, quien le comentó que había visto unos pequeños 'animálculos' en el semen. Ham pensaba que esos pequeños animales eran fruto de la putrefacción del líquido seminal. Leeuwenhoek, al contrario, supuso que se trataba de un componente habitual del semen y realizó la primera descripción detallada de los espermatozoides. Además, también fue la primera persona en proponer que la fecundación ocurría por la entrada del espermatozoide dentro del óvulo, ya que por aquel entonces se creía que la fecundación tenía lugar por vapores que emanaban del esperma. Aunque, lamentablemente, nunca pudo observar el proceso.

Posteriormente, en 1697, Nicolás Hartsocker propuso la teoría del homúnculo. Hartsocker fue un científico holandés que se dedicó a investigar sobre el origen de la vida. La observación de los espermatozoides al microscopio le llevó a pensar que dentro de cada uno de ellos había un homúnculo, una especie de ser humano en miniatura. En conclusión, su teoría expone que en cada uno de los espermatozoides ya se encuentra en potencia el ser humano que después va a ir desarrollándose en el vientre femenino.

Por último, cabe destacar la figura de Lazzaro Spallanzani, un fisiólogo y sacerdote italiano que investigó la incógnita que era aún la fecundación y el papel que jugaba el espermatozoide en el proceso. En uno de sus experimentos tomó huevos vírgenes y líquido seminal de ranas y los puso en contacto, logrando la fecundación de los primeros. Este trabajo se podría considerar como el primer trabajo sobre fecundación (o inseminación) artificial realizado a partir del método experimental. Posteriormente, sobre 1790, se dedicó a investigar la inseminación artificial en perros: inyectó con una jeringa espermatozoides a una perra y esta quedó preñada. Gracias a estos experimentos se demostró la importancia del espermatozoide en el proceso de la fecundación. Además, estos descubrimientos sirvieron de base para que el cirujano inglés Hunter pudiera intentar su aplicación a la especie humana.

La espermatogénesis es el proceso en el cual los espermatozoides se producen a partir de las células germinales primordiales del hombre (espermatogonias) mediante mecanismos de mitosis y meiosis. Es el mecanismo de gametogénesis en el hombre y se desarrolla en los testículos (gónadas masculinas), aunque la maduración final de los espermatozoides se lleva a cabo en el epidídimo. Los espermatozoides son células reproductoras masculinas, destinadas a la fecundación del óvulo; miden de diez a sesenta micras de longitud y están compuestas de una cabeza que contiene el material cromosómico y de una cola o flagelo que actúa como propulsor.

Los espermatozoides en el ser humano son de forma piriforme, solo sobreviven en un medio ambiente cálido, aunque entre 1 y 3ºC por debajo de la temperatura corporal, y son las únicas células humanas en poseer flagelo; esto la ayuda a ser una célula con alta movilidad, capaz de nadar libremente. 

Se componen principalmente de dos partes: una cabeza y su flagelo, pero dentro de ellas podemos distinguir varias estructuras, las cuales, en orden cefálico-caudal (de la cabeza a la cola, es decir, de arriba abajo), son: acrosoma, núcleo, membrana, cuello, pieza media, cola y pieza terminal. Viven de media 24 horas, aunque es posible que lleguen a fecundar el óvulo después de tres días. 

La "cabeza" contiene dos partes principales: el acrosoma, que cubre los dos tercios anteriores de la cabeza; y el núcleo, que contiene la carga genética del espermatozoide (23 cromosomas, en el pronúcleo, que, unidos a los 23 del óvulo dan lugar a la célula madre, al sumarse el total de 46 cromosomas, agrupados en pares). En los seres humanos la medida de la cabeza del espermatozoide es de 5µm (micrómetros) de longitud. Tanto el pronúcleo como el acrosoma están envueltos en medio de una pequeña cantidad de citoplasma y revestidos por una membrana plasmática que une la cabeza al cuerpo del espermatozoide. Es la parte más importante adjunto con el cuerpo. Esta membrana tiene altos niveles de ácidos grasos poliinsaturados que son las principales responsables de la movilidad del esperma.

El acrosoma es una capa formada por las enzimas hialuronidasa, acrosina y neuraminidasa que favorecerán la rotura de la zona pelúcida para la penetración, la cual rodea al ovocito.

El núcleo, después de que el acrosoma abra la zona pelúcida del ovocito, es la única parte que entra a su citoplasma, dejando atrás la membrana ya vacía, para luego fusionarse con el núcleo del óvulo, completarse como célula diploide y empezar la división celular (mitosis). Por lo tanto, como las mitocondrias y todo lo demás del gameto masculino no se unen al cigoto, todas las mitocondrias de la nueva célula provienen de la parte materna. La cromatina de un espermatozoide maduro está altamente condensada debido al reemplazo de las histonas con protaminas durante la espermatogénesis. 

El "cuello" es muy corto, por lo que no es visible mediante el microscopio óptico. Es ligeramente más grueso que las demás partes del flagelo y contiene residuos citoplasmáticos de la espermátida. Tras estos elementos contiene dos centriolos: el distal, que origina la pieza media, y el otro, el proximal, desaparece luego de haber dado origen al flagelo. Contiene una placa basal de material denso que lo separa de la cabeza y es donde se anclan 9 columnas proteicas, que son centriolos modificados, continuándose por toda la cola. De uno de ellos (el distal) se origina la pieza media.




Movilidades anormales se corresponden con porcentajes menores al 50 % de A+B o 25 % de A
-anotar que la movilidad de tipo A es poco común en el esperma de la población (entorno al 1%)-Estas anormalidades reciben el nombre de astenozoospermia o astenospermia; distinguiéndose entre leve, moderada y grave.

Existe una relación indirecta entre el volumen de eyaculado y la concentración de espermatozoides en las distintas especies:


En parte de los mamíferos, incluidos los seres humanos, los espermatozoides deben ser producidos a una temperatura más baja que la media del organismo (2 °C menos de lo normal en humanos), por ello las gónadas masculinas se encuentran fuera del cuerpo.

El desarrollo de las células germinales primordiales hasta espermatozoides maduros es una etapa clave para la reprogramación epigenética. La metilación del ADN y la modificación de histonas producen cambios en la gametogénesis; y alteraciones a cualquier nivel del epigenoma del espermatozoide puede afectar a la fertilidad y al correcto desarrollo del embrión.

Estudios recientes en ratones y humanos muestran que las células germinales masculinas poseen un único patrón de metilación en comparación con los tejidos somáticos. Los patrones de metilación de promotores en el esperma, como la hipometilación, permitirían la expresión de genes específicos de las células germinales involucrados en la espermatogénesis; mientras que la hipermetilación daría lugar a la represión de la pluripotencia y de genes específicos de tejidos somáticos. Muchos de estos sitios con metilación diferente en el esperma y en tejidos somáticos se encuentran fuera de regiones génicas y de islas CpG, por lo que parece que juegan otros papeles además de controlar la expresión génica. Los patrones de metilación en secuencias centroméricas e intergénicas pueden ser necesarias para que se forme la estructura cromatínica especializada que encontramos en las células germinales.

Los patrones de metilación de las células somáticas se establecen temprano durante la vida embrionaria y se mantienen en el desarrollo y en el adulto. Las células germinales, sin embargo, van a sufrir dos oleadas de desmetilación para poder establecer patrones específicos de sexo que dan lugar a los genes improntados. Al contrario que en el óvulo, los patrones epigenéticos de los espermatozoides se empiezan a adquirir prenatalmente. La adquisición inicial se relaciona con la expresión de Dnmt3a y Dnmt3L, lo cual es consistente con el papel de las enzimas DNMT3 como metiltransferasas de novo. Estos patrones se completan después del nacimiento en la fase paquinema de la meiosis.

La cromatina del esperma de los mamíferos es única, pues está altamente organizada, condensada y compactada. La remodelación cromatínica está facilitada por la hiperacetilación de las histonas y por el ADN topoisomerasa II, la cual produce mellas temporales en el ADN para aliviar el estrés torsional debido al superenrollamiento.

Las protaminas condensan las cadenas de ADN y forman una unidad de empaquetamiento básica de la cromatina llamada toroide. Confieren un nivel mayor de empaquetamiento del ADN al de las células somáticas. Todo esto protege a la cromatina durante el transporte a través del tracto reproductivo masculino y femenino. Además, las protaminas son necesarias para el silenciamiento del genoma paterno y la reprogramación del patrón de impronta del gameto. Sin embargo, un 15% de las histonas no son reemplazadas en la cromatina del esperma humano, causando que esté menos compactada. 

En la espermatogénesis, las protaminas sustituyen progresivamente las histonas de forma escalonada. Primero, las histonas somáticas se reemplazan por variantes de histonas específicas de los testículos. En la espermiogénesis las variantes de histonas específicas de tejido se cambian por proteínas de transición (TP1 y TP2) en un proceso que requiere la remodelación del ADN. Las proteínas de transición son necesarias para la normal condensación de la cromatina, para reducir el número de roturas del ADN y para prevenir la formación de defectos secundarios en los espermatozoides y la pérdida eventual de la integridad genómica. Finalmente, en la elongación de las espermátides, las proteínas de transición se sustituyen por protaminas. Este proceso secuencial facilita la remodelación molecular del genoma masculino en la diferenciación de la espermátida.

En humanos, la ratio P1/P2 es aproximadamente de 1.0 y alteraciones en este cociente se asocian con infertilidad. Las protaminas tienen aproximadamente la mitad del tamaño de las histonas. Son proteínas nucleares básicas que se caracteriza por un núcleo rico en argininas y residuos de cisteínas. Los niveles altos de arginina causan una carga neta positiva, facilitando así su unión al ADN. Asimismo, los residuos de cisteína facilitan la formación de múltiples puentes disulfuro inter e intraprotaminas, que son esenciales para el empaquetamiento en orden superior de la cromatina. Las protaminas P2 contienen menos grupos de cisteínas, lo que provoca que el ADN sea más susceptible al daño. 




</doc>
<doc id="9006" url="https://es.wikipedia.org/wiki?curid=9006" title="Ruptor (desambiguación)">
Ruptor (desambiguación)

Ruptor puede referirse a: 


</doc>
<doc id="9007" url="https://es.wikipedia.org/wiki?curid=9007" title="Evolución molecular">
Evolución molecular

La evolución molecular es el estudio de las variaciones en la secuencia del ADN a lo largo del tiempo, ya sean en la variación de la frecuencia de nucleótidos en una población o en la variación de locus entre linajes aislados reproductivamente. Hace referencia a los cambios en la secuencia de nucleótidos del ADN que han ocurrido durante la historia de las especies diferenciándolas de sus ancestros. 

Como disciplina, el campo de la evolución molecular se encarga de la evolución de genes y proteínas, preguntándose por la tasa de mutación (véase reloj molecular) y los mecanismos que rigen la evolución molecular. Este campo se entrelaza con la genética de poblaciones clásica y con la genómica comparativa; y tuvo grandes avances gracias a la viabilidad experimental de "Caenorhabditis elegans" y a la pronta publicación de la secuencia de su genoma, que contribuyó enormemente al entendimiento general de los procesos de evolución experimental. 

Una de las teorías más destacadas en este campo es la teoría neutralista de la evolución molecular; por la que muchos estudios de evolución molecular se centraron en el rol de la selección frente a la deriva genética, en relación a la fijación de sustituciones de aminoácidos; no obstante, en la actualidad se sabe que ambos factores contribuyen en este proceso. 

Se define como cualquier cambio en la secuencia de los ácidos nucleicos, o cualquier cambio en la estructura de los cromosomas; las mutaciones son permanentes,
cambios transmisibles en el material genético (usualmente ADN o ARN) de una célula, y pueden ser causadas por errores en la copia del material genético durante la división celular y por la exposición a la radiación, químicos, o virus, o puede ocurrir deliberadamente bajo control celular durante los procesos tales como la meiosis o la hipermutación. 

En organismos pluricelulares existen mutaciones germinales, en las cuales se producen cambios a nivel de los gametos, y mutaciones somáticas, en las cuales los cambios tienen lugar en el resto de células del organismo a excepción de las germinales y embrionarias. Las mutaciones pueden tener lugar a nivel de nucleótidos, como las mutaciones puntuales (inserciones, deleciones, transiciones y transversiones), o a nivel cromosómico, mismas que pueden ser numéricas (aneuploidía, disomía, trisomía, etc.) o estructurales (duplicación, deleción, translocación, inversión, etc.).

Las mutaciones se considera la fuerza motriz de la evolución, donde la menos favorable (o perjudicial) se eliminan del acervo genético por selección natural, mientras las más favorables (o beneficiaria) tienden a acumularse. Las mutaciones neutrales no afectan las posibilidades del organismo de la supervivencia en su medio natural y se pueden acumular con el tiempo, lo que podría dar lugar a lo que se conoce como equilibrio puntuado, la interpretación moderna de la teoría evolutiva clásica.

Es la producción de nuevas moléculas de ADN a partir de dos moléculas de ADN parentelas o a partir de diferentes segmentos de la misma molécula de ADN. La recombinación involucra el emparejamiento de cromosomas homólogos seguido por intercambio físico de material genético a través de un "crossing over."

Se han identificado al menos cuatro tipos de recombinación: 

En evolución molecular, la recombinación genera nuevas combinaciones de alelos de individuos heterocigotos, y puede facilitar la selección natural al reducir la interferencia entre alelos relacionados, sujetos a selección simultánea. Por tanto, la recombinación puede ser importante para la eliminación de alelos deletéreos y para la fijación de alelos benéficos. 

La teoría de la selección natural es la pieza central de "El origen de las especies" y de la teoría evolutiva; esta teoría explica las adaptaciones de los organismos, la divergencia de las especies a partir de ancestros comunes, y consecuentemente la infinita diversidad de la vida. 

Una gran fecundidad, y la competencia de los individuos por recursos escasos como: alimentos, pareja y lugares para vivir, proveen las condiciones previas para el proceso que Charles Darwin denominó selección natural; misma que requiere del cumplimiento de cuatro condiciones: 
Si estas condiciones se cumplen, la selección natural ocurre automáticamente; no obstante, aquellas entidades que se reproducen, pero cuyas características paternas no son heredadas a su descendencia no pueden evolucionar por selección natural.

Es un proceso aleatorio que se produce por fluctuaciones en la frecuencia de los alelos dentro de una población como resultado de un muestreo aleatorio de gametos. La deriva génica, es un tipo de fuerza de evolución que altera la frecuencia alélica a lo largo del tiempo, por lo que su impacto suele ser mayor en poblaciones pequeñas, y suele resultar en la pérdida de la diversidad genética de una población. 

Incluso si todos los individuos de una población tienen las mismas oportunidades de aparearse, sus contribuciones reproductivas a la siguiente generación variarán debido al azar. En cualquier población de tamaño infinito, este error de muestreo hará que las frecuencias de los genes fluctúen de generación en generación. Los cambios genéticos debidos a la deriva no son direcciones o predecibles; sin embargo, la deriva génica conduce al cambio evolutivo incluso en ausencia de mutación, selección natural o flujo genético. 

Hay cuatro procedimientos conocidos que afectan a la supervivencia de una característica, o, más específicamente, la frecuencia de un alelo (variante de un gen):



</doc>
<doc id="9008" url="https://es.wikipedia.org/wiki?curid=9008" title="Midas">
Midas

Midas (en griego Μίδας, llamado "Mita" en fuentes asirias) fue un rey de Frigia que gobernó en el período entre el 740 a. C. y el 696 a. C., aproximadamente. De acuerdo con la mitología griega, el monarca tenía la habilidad de convertir en oro todo lo que tocara. Según Aristóteles, la leyenda afirmaba que Midas murió de hambre debido a su extraño poder. Asimismo, su historia sostiene que Midas y su padre Gordias fundaron la ciudad de Gordio, capital de Frigia, y ataron el Nudo gordiano, lo cual indica que ambos vivieron en algún momento del II milenio a. C., mucho antes de la Guerra de Troya. Sin embargo, Homero no menciona a Midas ni a su padre Gordias, aunque sí a otros reyes frigios como Migdón y Otreo.

Casado con una griega, fue el primer rey extranjero que mandó un regalo al santuario de Delfos. Probablemente, fue durante su reinado cuando Frigia adoptó el alfabeto griego.

El reinado de Midas supone la mayor época de esplendor de Frigia, que se expandió al este, hasta la frontera con Urartu, ocupando una extensa zona de Asia Menor. Mantuvo relaciones comerciales con Asiria y Urartu, alcanzando el rey una riqueza extraordinaria, que llamó la atención de los griegos, quienes le dedicaron un espacio en la mitología.

Contemporáneo de Tiglath-Pileser III, Salmanasar V, y Sargón II, durante muchos años instigó levantamientos de los principados de Asia Menor contra Asiria, apoyando a Hama, Karkemish, Tabal, Gurgum, Kummukhu y Meliddu, hasta que finalmente fue atacado por Sargón II. Temeroso del poder del asirio, Midas le envió una embajada, declarándose vasallo.

Después de los conflictos con Sargón II, sufrió invasiones de los cimerios, que destruyeron la capital Gordio. Según la tradición, Midas se suicidó y así terminó el corto período hegemónico de Frigia.

En la mitología griega, Midas era rey de Frigia, e hijo de Gordias. Tenía una hija llamada Zoe.

De acuerdo con la mitología griega, por su hospitalidad con Sileno, Dioniso le otorgó el poder de convertir en oro todo cuanto tocara. Viendo que no podía comer los alimentos que a su contacto quedaban transformados en dicho metal, pidió a Dioniso que le liberara de su don. Este le dijo que se lavase en el río Pactolo. Cuando lo hizo, el río se volvió del color del oro.












</doc>
<doc id="9013" url="https://es.wikipedia.org/wiki?curid=9013" title="Sistema embebido">
Sistema embebido

Un sistema embebidoo empotrado (integrado, incrustado) es un sistema de computación diseñado para realizar una o algunas pocas funciones dedicadas, frecuentemente en un sistema de computación en tiempo real. Al contrario de lo que ocurre con los ordenadores de propósito general (como por ejemplo una computadora personal o PC) que están diseñados para cubrir una amplia gama de necesidades, los sistemas embebidos se diseñan para cubrir necesidades específicas. 
En un sistema embebido la mayoría de los componentes se encuentran incluidos en la placa base (tarjeta de vídeo, audio, módem, etc.) y muchas veces los dispositivos resultantes no tienen el aspecto de lo que se suele asociar a una computadora. Algunos ejemplos de sistemas embebidos podrían ser dispositivos como un taxímetro, un sistema de control de acceso, la electrónica que controla una máquina expendedora o el sistema de control de una fotocopiadora entre otras múltiples aplicaciones. 

Por lo general los sistemas embebidos se pueden programar directamente en el lenguaje ensamblador del microcontrolador o microprocesador incorporado sobre el mismo, o también, utilizando los compiladores específicos, pueden utilizarse lenguajes como C o C++; en algunos casos, cuando el tiempo de respuesta de la aplicación no es un factor crítico, también pueden usarse lenguajes Orientados a Objetos como JAVA.

Puesto que los sistemas embebidos se pueden fabricar por decenas de millares o por millones de unidades, una de las principales preocupaciones es reducir los costes. Los sistemas embebidos suelen usar un procesador relativamente pequeño y una memoria pequeña para ello. Los primeros equipos embebidos que se desarrollaron fueron elaborados por IBM en los años 1980.

Los programas de sistemas embebidos se enfrentan normalmente a tareas de procesamiento en tiempo real.

Existen también plataformas desarrolladas por distintos fabricantes que proporcionan herramientas para el desarrollo y diseño de aplicaciones y prototipos con sistemas embebidos desde ambientes gráficos, algunos ejemplos de estás son: Arduino, mbed, Raspberry Pi, BeagleBone, etc.

En la parte central se encuentra el microprocesador, microcontrolador, DSP, etc. Es decir, la CPU o unidad que aporta capacidad de cómputo al sistema, pudiendo incluir memoria interna o externa, un micro con arquitectura específica según requisitos.

La comunicación adquiere gran importancia en los sistemas embebidos. Lo normal es que el sistema pueda comunicarse mediante interfaces estándar de cable o inalámbricas. Así un SI normalmente incorporará puertos de comunicaciones del tipo RS-232, RS-485, SPI, I²C, CAN, USB, IP, Wi-Fi, GSM, GPRS, DSRC, etc.

El subsistema de presentación tipo suele ser una pantalla gráfica, táctil, LCD, alfanumérico, etc

Se denominan actuadores a los posibles elementos electrónicos que el sistema se encarga de controlar. Puede ser un motor eléctrico, un conmutador tipo relé etc. El más habitual puede ser una salida de señal PWM para control de la velocidad en motores de corriente continua 

El módulo de E/S analógicas y digitales suele emplearse para digitalizar señales analógicas procedentes de sensores, activar diodos ledes, reconocer el estado abierto cerrado de un conmutador o pulsador, etc

El módulo de reloj es el encargado de generar las diferentes señales de reloj a partir de un único oscilador principal. El tipo de oscilador es importante por varios aspectos: por la frecuencia necesaria, por la estabilidad necesaria y por el consumo de corriente
requerido. El oscilador con mejores características en cuanto a estabilidad y coste son los basados en resonador de cristal de cuarzo, mientras que los que requieren menor consumo son los RC. Mediante sistemas PLL se obtienen otras frecuencias con la misma estabilidad que el oscilador patrón

El módulo de energía (power) se encarga de generar las diferentes tensiones y corrientes necesarias para alimentar los diferentes circuitos del SE. Usualmente se trabaja con un rango de posibles tensiones de entrada que mediante conversores ac/dc o dc/dc se obtienen las diferentes tensiones necesarias para alimentar los diversos componentes activos del circuito

Además de los conversores ac/dc y dc/dc, otros módulos típicos, filtros, circuitos integrados supervisores de alimentación, etc

El consumo de energía puede ser determinante en el desarrollo de algunos sistemas embebidos que necesariamente se alimentan con baterías, con lo que el tiempo de uso del SE suele ser la duración de la carga de las baterías

Un microprocesador es una implementación en forma de circuito integrado (IC) de la Unidad Central de Proceso CPU de una computadora. Frecuentemente nos referimos a un microprocesador como simplemente “CPU”, y la parte de un sistema que contiene al microprocesador se denomina subsistema de CPU. Los microprocesadores varían en consumo de potencia, complejidad y coste.

Los subsistemas de entrada/salida y memoria pueden ser combinados con un subsistema de CPU para formar una computadora o sistema embebido completo. Estos subsistemas se interconectan mediante los buses de sistema (formados a su vez por el bus de control, el bus de direcciones y el bus de datos).

El subsistema de entrada acepta datos del exterior para ser procesados mientras que el subsistema de salida transfiere los resultados hacia el exterior. Lo más habitual es que haya varios subsistemas de entrada y varios de salida. A estos subsistemas se les reconoce habitualmente como periféricos de E/S.

El subsistema de memoria almacena las instrucciones que controlan el funcionamiento del sistema. Estas instrucciones comprenden el programa que ejecuta el sistema. La memoria también almacena varios tipos de datos: datos de entrada que aún no han sido procesados, resultados intermedios del procesado y resultados finales en espera de salida al exterior.

Es importante darse cuenta de que los subsistemas estructuran a un sistema según funcionalidades. La subdivisión física de un sistema, en términos de circuitos integrados o placas de circuito impreso (PCB) puede y es normalmente diferente. Un solo circuito integrado (IC) puede proporcionar múltiples funciones, tales como memoria y entrada/salida.

Un microcontrolador (MCU) es un IC que incluye una CPU, memoria y circuitos de E/S. Entre los subsistemas de E/S que incluyen los microcontroladores se encuentran los temporizadores, los convertidores analógico a digital (ADC) y digital a analógico (DAC) y los canales de comunicaciones serie. Estos subsistemas de E/S se suelen optimizar para aplicaciones específicas (por ejemplo audio, video, procesos industriales, comunicaciones, etc.).

Hay que señalar que las líneas reales de distinción entre microprocesador, microcontrolador y microcomputador en un solo chip están difusas, y se denominan en ocasiones de manera indistinta unos y otros.

En general, un SE (Sistema Electrónico) consiste en un sistema con microprocesador cuyo hardware y software están específicamente diseñados y optimizados para resolver un problema concreto eficientemente. Normalmente un SE interactúa continuamente con el entorno para vigilar o controlar algún proceso mediante una serie de sensores. Su hardware se diseña normalmente a nivel de chips, o de interconexión de PCB, buscando la mínima circuitería y el menor tamaño para una aplicación particular. Otra alternativa consiste en el diseño a nivel de PCB consistente en el ensamblado de placas con microprocesadores comerciales que responden normalmente a un estándar como el PC-104 (placas de tamaño concreto que se interconectan entre sí “apilándolas” unas sobre otras, cada una de ellas con una funcionalidad específica dentro del objetivo global que tenga el SE). Esta última solución acelera el tiempo de diseño pero no optimiza ni el tamaño del sistema ni el número de componentes utilizados ni el coste unitario. En general, un sistema embebido simple contará con un microprocesador, memoria, unos pocos periféricos de E/S y un programa dedicado a una aplicación concreta almacenado permanentemente en la memoria. El término embebido o empotrado hace referencia al hecho de que el microcomputador está encerrado o instalado dentro de un sistema mayor y su existencia como microcomputador puede no ser aparente. Un usuario no técnico de un sistema embebido puede no ser consciente de que está usando un sistema computador. En algunos hogares las personas, que no tienen por qué ser usuarias de una computadora personal estándar (PC), utilizan del orden de diez o más sistemas embebidos cada día.

Las microcomputadoras en estos sistemas controlan electrodomésticos tales como: televisores, videos, lavadoras, alarmas, teléfonos inalámbricos, etc. Incluso una PC tiene sistemas embebidos en el monitor, impresora, y periféricos en general, adicionales a la CPU de la propia PC. Un automóvil puede tener hasta un centenar de microprocesadores y microcontroladores que controlan cosas como la ignición, transmisión, dirección asistida, frenos antibloqueo (ABS), control de la tracción, etc.

Los sistemas embebidos se caracterizan normalmente por la necesidad de dispositivos de E/S especiales. Cuando se opta por diseñar el sistema embebidos partiendo de una placa con microcomputador también es necesario comprar o diseñar placas de E/S adicionales para cumplir con los requisitos de la aplicación concreta.

Muchos sistemas embebidos son sistemas de tiempo real. Un sistema de tiempo real debe responder, dentro de un intervalo restringido de tiempo, a eventos externos mediante la ejecución de la tarea asociada con cada evento. Los sistemas de tiempo real se pueden caracterizar como blandos o duros. Si un sistema de tiempo real blando no cumple con sus restricciones de tiempo, simplemente se degrada el rendimiento del sistema, pero si el sistema es de tiempo real duro y no cumple con sus restricciones de tiempo, el sistema fallará. Este fallo puede tener posiblemente consecuencias catastróficas.

Un sistema embebido complejo puede utilizar un sistema operativo como apoyo para la ejecución de sus programas, sobre todo cuando se requiere la ejecución simultánea de los mismos. Cuando se utiliza un sistema operativo lo más probable es que se tenga que tratar de un sistema operativo de tiempo real (RTOS), que es un sistema operativo diseñado y optimizado para manejar fuertes restricciones de tiempo asociadas con eventos en aplicaciones de tiempo real. En una aplicación de tiempo real compleja la utilización de un sistema operativo de tiempo real multitarea puede simplificar el desarrollo del software.

Una PC embebida posee una arquitectura semejante a la de un PC. Brevemente estos son los elementos básicos:


Existen fabricantes que integran un microprocesador y los elementos controladores de los dispositivos fundamentales de entrada y salida en un mismo chip, pensando en las necesidades de los sistemas embebidos (bajo coste, pequeño tamaño, entradas y salidas específicas, etc.). Su capacidad de proceso suele ser inferior a los procesadores de propósito general pero cumplen con su cometido ya que los sistemas donde se ubican no requieren tanta potencia. Los principales fabricantes son STMicroelectronics (familia de chips STPC), AMD (familia Geode), Motorola (familia ColdFire) e Intel.

En cuanto a los sistemas operativos necesarios para que un sistema basado en microprocesador pueda funcionar y ejecutar programas suelen ser específicos para los sistemas embebidos. Así nos encontramos con sistemas operativos de bajos requisitos de memoria, posibilidad de ejecución de aplicaciones de tiempo real, modulares (inclusión sólo de los elementos necesarios del sistema operativo para el sistema embebido concreto), etc. Los más conocidos en la actualidad son Windows CE, QNX y VxWorks de WindRiver.


Los equipos industriales de medida y control tradicionales están basados en un microprocesador con un sistema operativo privativo o específico para la aplicación correspondiente. Dicha aplicación se programa en ensamblador para el microprocesador dado o en lenguaje C, realizando llamadas a las funciones básicas de ese sistema operativo que en ciertos casos ni siquiera llega a existir. Con los modernos sistemas PC embebida basados en microprocesadores i486 o i586 se llega a integrar el mundo del PC compatible con las aplicaciones industriales. Ello implica numerosas ventajas:




</doc>
<doc id="9014" url="https://es.wikipedia.org/wiki?curid=9014" title="Tiempo real">
Tiempo real

Un sistema en tiempo real (STR) es aquel sistema digital que interactúa activamente con un entorno con dinámica conocida en relación con sus entradas, salidas y restricciones temporales, para darle un correcto funcionamiento de acuerdo con los conceptos de predictibilidad, estabilidad, controlabilidad y alcanzabilidad.

Los sistemas en tiempo real están presentes en nuestra vida diaria, prácticamente en todo lo que nos rodea: en los aviones, trenes y automóviles, en el televisor, la lavadora o el horno de microondas, en los teléfonos celulares y en las centrales telefónicas digitales. Son un elemento imprescindible para garantizar la generación, transmisión y distribución de la energía eléctrica y para asegurar la calidad y la seguridad de incontables procesos industriales.

La principal característica que distingue a los STR de otros tipos de sistemas es el tiempo de interacción. Sin embargo, antes de continuar es necesario aclarar el significado de las palabras "tiempo" y "real".

Los STR se pueden encontrar en lugares muy importantes debido a los servicios que prestan: ellos monitorizan, controlan y protegen, por ejemplo, los sistemas de transmisión y distribución que hacen llegar la energía eléctrica a las industrias y también a nuestros hogares. Los STR están presentes en las áreas de monitoreo de tráfico de trenes; su importancia es relevante debido a que diariamente se transportan millones de pasajeros. 

Un STR tiene tres condiciones básicas:


En contraste con la definición de STR, un sistema rápido produce su salida sin considerar las restricciones de tiempo del ambiente con que interactúa, para esa clase de sistemas no es importante el tiempo en el cual los datos llegan al sistema digital sino solamente el tiempo en que la salida es producida, en otras palabras únicamente interesa la rapidez de dar la respuesta dentro del intervalo de tiempo cuya medida, entre más pequeña es mejor, sin importar el costo de generar esa respuesta. De igual manera, tiende a confundirse el concepto de STR con el de sistema en línea:

Un sistema en línea es aquel que siempre debe estar encendido, disponible y generalmente conectado a una red de computadoras y depende de la capacidad del hardware para atender peticiones de servicio y en ningún momento está en sincronía con el mundo real ni tiene restricciones temporales.
En adición a esto, un sistema fuera de línea es aquel que no siempre está disponible para recibir y enviar información y que depende de una base de datos previamente establecida para ejecutar su cometido. Como ejemplos de sistemas en línea se tienen las aplicaciones de Internet como los navegadores web o la adquisición de datos a través de una tarjeta especializada en un ambiente de tiempo compartido como Windows.

El concepto de STR no queda restringido a los sistemas digitales o de cómputo, ya que puede extenderse al mundo vivo: humanos, animales y plantas. Como ejemplo, considérese una semilla fértil, la cual llega de alguna manera (ya sea por acción del viento, por medio del desecho de algún animal al final de su digestión, etc.) a la tierra. Se puede asegurar que el proceso de germinación de la semilla es un sistema de tiempo real en el ambiente y en las circunstancias en las cuales se desarrolla, ya que a estímulos del ambiente (humedad apropiada constante, temperatura adecuada constante, luz necesaria, etc.) el sistema (la semilla) responde dentro de sus restricciones de tiempo específicas. Si la semilla fuera solamente un sistema rápido (y no de Tiempo Real), tan pronto como ésta tocara la tierra comenzaría su proceso de germinación, sin importar la escasez de nutrientes del suelo o agua o estación del año, por lo que el comportamiento de la semilla no correspondería a lo que está sucediendo en el ambiente, es decir, que el intervalo de tiempo en el que la respuesta del sistema (semilla) se produce no sería muy importante, pero en la realidad ocasionaría que muriera rápidamente por la falta de su adaptabilidad, al tratar de consumir nutrientes más de lo que los puede asimilar o que se encuentren disponibles para ser absorbidos por la raíz de la planta.



</doc>
<doc id="9015" url="https://es.wikipedia.org/wiki?curid=9015" title="Sistema de tiempo real">
Sistema de tiempo real

Un sistema de tiempo real es un sistema informático que interacciona con su entorno físico y responde a los estímulos del entorno dentro de un plazo de tiempo determinado. No basta con que las acciones del sistema sean correctas, sino que, además, tienen que ejecutarse dentro de un intervalo de tiempo determinado.

Existen sistemas de tiempo real crítico ("tiempo real duro"), en los que los plazos de respuesta deben respetarse siempre estrictamente y una sola respuesta tardía a un suceso externo puede tener consecuencias fatales; y sistemas de tiempo real acrítico ("tiempo real suave"), en los que se pueden tolerar retrasos ocasionales en la respuesta a un suceso.

Un ejemplo que ilustra los puntos anteriores es el de un robot que necesita tomar una pieza de una banda sinfín. Si el robot llega tarde, la pieza ya no estará donde debía recogerla, por tanto, el trabajo se llevó a cabo incorrectamente, aunque el robot haya llegado al lugar adecuado. Si el robot llega antes de que la pieza llegue, la pieza aún no estará ahí y el robot puede bloquear su paso.

El determinismo es una cualidad clave en los sistemas de tiempo real. Es la capacidad de determinar con una alta probabilidad, cuanto es el tiempo que se toma una tarea en iniciarse. Esto es importante porque los sistemas de tiempo real necesitan que ciertas tareas se ejecuten antes de que otras puedan iniciar.

Esta característica se refiere al tiempo que tarda el sistema antes de responder a una interrupción. Este dato es importante saberlo porque casi todas las peticiones de interrupción se generan por eventos externos al sistema (i.e. por una petición de servicio), así que es importante determinar el tiempo que tardará el sistema en aceptar esta petición de servicio.

La responsividad se enfoca en el tiempo que tarda una tarea en ejecutarse una vez que la interrupción ha sido atendida. Los aspectos a los que se enfoca son:


Una vez que el resultado del cálculo de determinismo y responsividad es obtenido, se convierte en una característica del sistema y un requerimiento para las aplicaciones que correrán en él,(por ejemplo, si diseñamos una aplicación en un sistema en el cual el 95 % de las tareas deben terminar en cierto período entonces es recomendable asegurarse que las tareas ejecutadas de nuestra aplicación no caigan en el 5 % de bajo desempeño). 

En estos sistemas, el usuario (por ejemplo, los procesos que corren en el sistema) tienen un control mucho más amplio del sistema.


Esto aunque parece anárquico no lo es, debido a que los sistemas de tiempo real usan tipos de procesos que ya incluyen estas características, y usualmente estos TIPOS de procesos son mencionados como requerimientos. Un ejemplo es el siguiente:

«Los procesos de mantenimiento no deberán exceder el 3 % de la capacidad del procesador, a menos que en el momento que sean ejecutados el sistema se encuentre en la ventana de tiempo de menor uso.»

La confiabilidad en un sistema de tiempo real es otra característica clave. El sistema no debe solamente estar libre de fallas pero más aún, la calidad del servicio que presta no debe degradarse más allá de un límite determinado.

El sistema debe de seguir en funcionamiento a pesar de catástrofes, o fallas mecánicas. Usualmente una degradación en el servicio en un sistema de tiempo real lleva consecuencias catastróficas.

El sistema debe de fallar de manera que cuando ocurra una falla, el sistema preserve la mayor parte de los datos y capacidades del sistema en la mayor medida posible.

Que el sistema sea estable, es decir, que si para el sistema es imposible cumplir con todas las tareas sin exceder sus restricciones de tiempo, entonces el sistema cumplirá con las tareas más críticas y de más alta prioridad.

Las características especiales de los "sistemas en tiempo real" diferentes a los demás tipos de sistemas introducen en la definición del sistema una serie requerimientos no funcionales, que no se refieren directamente a las funciones específicas si no a propiedades emergentes como por ejemplo, requisitos de fiabilidad, eficiencia o implementación.
El diseño por análisis estructurado que emplea la descripción gráfica se enfoca en el desarrollo de especificaciones del programa que está formado por módulos independientes desde el punto de vista funcional.

La computación en tiempo real (o informática en tiempo real) está relacionada con los sistemas de hardware y software que se ven limitados por problemas de tiempo. El software de tiempo real debe necesariamente tener la característica de un tiempo de respuesta crítico.

Por ejemplo, el software encargado de controlar un respirador artificial debe ser de tiempo real, ya que un retraso en su tiempo de respuesta no es aceptable. Algunos tipos de programas como los empleados para jugar al ajedrez solo disponen del tiempo necesario para poder efectuar la siguiente jugada.

Se podría hacer una distinción, por ejemplo, un sistema de gestión del motor de un coche es un sistema en tiempo real activo porque una señal retrasada puede causar un daño o fallo en el motor. Otros ejemplos de sistemas integrados en tiempo real activos son los sistemas médicos como los marcadores de pasos artificiales y los controladores de procesos industriales.

Los sistemas de tiempo real pasivos se utilizan normalmente cuando hay un acceso compartido y se necesitan mantener actualizados un número de sistemas conectados con una situación cambiante. Un ejemplo serían los programas que mantienen y actualizan los planes de vuelo de las compañías aéreas comerciales. Estos programas pueden funcionar en cuestión de segundos. 

No sería posible ofrecer vuelos comerciales modernos si estas operaciones no se pudieran realizar de manera fiable en tiempo real. Los sistemas de audio y video en directo también son sistemas en tiempo real pasivos típicos ya que si se sobrepasan los límites de tiempo lo único que puede pasar es que se empeore la calidad pero el sistema continua trabajando.

Las necesidades de los programas de tiempo real se pueden solucionar con sistemas operativos en tiempo real, que ofrecen un marco sobre el que construir aplicaciones de programas en tiempo real.

Un sistema operativo de tiempo real (SOTR o RTOS -"Real Time Operating System" en inglés) es un sistema operativo que ha sido desarrollado para aplicaciones de tiempo real. Como tal, se le exige corrección en sus respuestas bajo ciertas restricciones de tiempo. Si no las respeta, se dirá que el sistema ha fallado. Para garantizar el comportamiento correcto en el tiempo requerido se necesita que el sistema sea predecible (determinista).




</doc>
<doc id="9017" url="https://es.wikipedia.org/wiki?curid=9017" title="Transposón">
Transposón

Un transposón o elemento genético transponible es una secuencia de ADN que puede moverse de manera autosuficiente a diferentes partes del genoma de una célula, un fenómeno conocido como transposición. En este proceso, se pueden causar mutaciones y cambio en la cantidad de ADN del genoma. Anteriormente fueron conocidos como "genes saltarines" y son ejemplos de elementos genéticos móviles.

El transposón modifica el ADN de sus inmediaciones, ya sea arrastrando un gen codificador de un cromosoma a otro, rompiéndolo por la mitad o haciendo que desaparezca del todo. En algunas especies, la mayor parte del ADN (hasta un 50% del total del genoma) corresponde a transposones.
Estos elementos móviles han acompañado a los organismos vivos durante su evolución contribuyendo decisivamente a los cambios genéticos. 

A diferencia de los provirus, los transposones se integran en el ADN celular en lugares bien determinados. Barbara McClintock propuso su existencia en el maíz, sin embargo, su presencia no se demostró hasta mucho más tarde en bacterias. Por ello recibió el Premio Nobel en 1983.

Los transposones están presentes en todos los seres vivos y también se han detectado en los virus gigantes. Son genes móviles que no tienen capacidad de replicación y dependen de la integración en la célula huésped.
En principio no son infectivos, pero se comportan como parásitos intracelulares. Fueron descubiertos por Bárbara McClintock en el maíz y los llamó “elementos controladores” porque podían modificar la expresión de los genes en los que se insertan.
Se han descrito varias familias de transposones que se agrupan en dos clases, I y II. Los de clase I se movilizan a través del ARN (retrotransposones), son los más conocidos. Los de clase II saltan directamente a través de ADN.
El nº de copias de cada familia varía con la especie y con el individuo.
No tiene especificidad en su sitio de integración en el genoma. Se movilizan de forma aleatoria y pueden entrar en intrones, en exones o en genes reguladores. Entonces causan mutaciones e incrementan la variabilidad genética. En este sentido son un motor evolutivo claro.

Existe una amplia diversidad de elementos genéticos móviles y pueden ser clasificados sobre la base de su contenido, su estrategia y mecanismo de transposición.




Se expresa la transposasa, y realiza dos cortes de doble cadena a la misma altura en el genoma donante, dejando aislado el transposón. A continuación localiza una secuencia diana (pongamos, ATGCA) en el genoma aceptor, y realiza un corte cohesivo. Tras eso une los extremos a los del transposón aislado, y la ADN Polimerasa de la célula rellena las zonas de cadena sencilla dejadas en la secuencia señal tras el corte cohesivo. Debido a esto, la secuencia señal queda duplicada. Queda, sin embargo, un hueco en el genoma donante, que puede ser letal si no se repara. Realmente, en este caso se habla más de recombinación que de transposición. 

Los transposones se encuentran en todas las formas de vida y en los virus gigantes. Por tanto la comunidad científica todavía está explorando su evolución y su efecto sobre la evolución del genoma. Los transposones parecen haberse desarrollado en LUCA (último ancestro común universal) o tal vez antes. Los transposones confieren beneficios a sus anfitriones, la mayoría son considerados ADN egoísta (parásitos). De esta manera, son similares a los virus. Los transposones, virus y plásmidos tienen características similares en su bioquímica lo que lleva a especular que tienen un ancestro en común. Algunos transposones de ADN pudieron evolucionar secundariamente de infecciones virales antiguas causadas por virus de ADN. 

Debido a que la actividad de los transposones es excesiva puede dañar los exones, muchos organismos han adquirido mecanismos para inhibir su actividad. Las bacterias y arqueas suelen experimentar altas tasas de eliminación de genes como parte de un mecanismo para eliminar plásmidos, transposones y restos de virus de sus genomas, mientras que los organismos eucariotas suelen utilizar la interferencia de ARN para inhibir la actividad de transposones. Sin embargo, algunos transposones generan familias numerosas a menudo asociadas con eventos de especiación. La evolución a menudo desactiva los transposones de ADN, dejándolos como intrones (secuencias genéticas inactivas). En las células de animales vertebrados, casi todos los más de 100,000 transposones de ADN por genoma tienen genes que codifican polipéptidos de transposasa inactivos. El primer transposón sintético diseñado para su uso en los vertebrados (incluyendo humanos) células, el durmiente sistema de transposón de belleza , es un transposón Tc1 / mariner-like. Sus versiones muertas ("fósiles") se extienden ampliamente en el genoma del salmónido y se diseñó una versión funcional comparando esas versiones. Los transposones similares a Tc1 humanos se dividen en subfamilias Hsmar1 y Hsmar2. Aunque ambos tipos están inactivos, se está seleccionando una copia de Hsmar1 que se encuentra en el gen SETMAR, ya que proporciona la unión al ADN para la proteína modificadora de histona. Muchos otros genes humanos se derivan de manera similar de los transposones. Hsmar2 ha sido reconstruido varias veces a partir de las secuencias fósiles. 

Sin embargo, grandes cantidades de transposones dentro de los genomas aún pueden presentar ventajas evolutivas. Repeticiones intercaladas dentro de los genomas se crean por eventos de transposición que se acumulan a lo largo del tiempo evolutivo. Debido a que las repeticiones intercaladas bloquean la conversión génica, protegen las secuencias genéticas novedosas de ser sobrescritas por secuencias genéticas similares y, por lo tanto, facilitan el desarrollo de nuevos genes. Los transposones pueden haber sido cooptados por el sistema inmune de vertebrados como un medio para producir diversidad de anticuerpos. El sistema de recombinación V (D) J funciona mediante un mecanismo similar al de algunos transposones.

Los transposones pueden contener muchos tipos de genes, incluidos los que confieren resistencia a los antibióticos y la capacidad de transponerse a plásmidos conjugativos. Algunos TE también contienen integrones, elementos genéticos que pueden capturar y expresar genes de otras fuentes. Estos contienen integrasa, que puede integrar casetes de genes. Hay más de 40 genes de resistencia a antibióticos identificados en casetes, así como genes de virulencia.

Los transposones no siempre extirpan sus elementos con precisión, a veces eliminan los pares de bases adyacentes; este fenómeno se llama barajar exón. Mezclar dos exones no relacionados puede crear un nuevo producto genético o más probablemente un intrón.

En 2009 se encontró el trasposón hAT en siete especies de animales taxonómicamente tan lejanas entre sí, que algunas divergieron hace 340 millones de años. Entre otras, erizo común ("Erinaceus europaeus"), tenrec común "(Tenrec ecaudatus)", oposum americano, gálago, y una especie de rana.

Lo más llamativo de este estudio es encontrar secuencias del genoma similares entre el erizo y el tenrec, animales semejantes pero no emparentados. El elefante no presenta este trasposón a pesar de ser más cercano taxonómicamente al tenrec (Afrotheria).

La posición de este trasposón en el genoma varía entre las distintas especies, lo que sugiere una transmisión horizontal (la información pasa de una especie a otra) y no vertical (la secuencia pasa de progenitores a descendientes).




</doc>
<doc id="9018" url="https://es.wikipedia.org/wiki?curid=9018" title="Hardware">
Hardware

La palabra hardware en informática se refiere a las partes físicas, tangibles, de un sistema informático, sus componentes eléctricos, electrónicos, electromecánicos y mecánicos. Los cables, así como los gabinetes o cajas, los periféricos de todo tipo, y cualquier otro elemento físico involucrado, componen el hardware o soporte físico; contrariamente, el soporte lógico e intangible es el llamado "software".

El término es propio del idioma inglés, y su traducción al español no tiene un significado acorde, por tal motivo se lo ha adoptado tal cual es y suena. La Real Academia Española lo define como «Conjunto de los componentes que integran la parte material de una computadora». El término, aunque sea lo más común, no solamente se aplica a las computadoras, también es a menudo utilizado en otras áreas de la vida diaria y la tecnología. Por ejemplo, "hardware" también se refiere a herramientas y máquinas, y en electrónica "hardware" se refiere a todos los componentes electrónicos, eléctricos, electromecánicos, mecánicos, cableados y tarjetas de circuitos impresos.

Otros ejemplos donde se aplica el término hardware son, en relación a los robots, así como en relación a los teléfonos móviles, las cámaras fotográficas, los reproductores digitales, o cualquier otro dispositivo electrónico. Cuando dichos dispositivos también procesan datos, poseen "firmware" y/o "software" además de "hardware".

La historia del "hardware" de computador se puede clasificar en cuatro generaciones, cada una caracterizada por un cambio tecnológico de importancia. Una primera delimitación podría hacerse entre "hardware" principal (véase figura), como el estrictamente necesario para el funcionamiento normal del equipo, y el «complementario», como el que realiza funciones específicas.

El hardware principal de un computador se compone de una unidad central de procesamiento(CPU), encargada de procesar los datos; una memoria rápida de trabajo para almacenamiento temporal; una unidad de almacenamiento fija para mantener software y datos así como extraerlos de ella; uno o varios periféricos de entrada, los que permiten el ingreso de la información y uno o varios periféricos de salida, que posibilitan dar salida (normalmente en forma visual, impresa o auditiva) a los datos procesados.

La clasificación evolucionista del "hardware" del computador electrónico está dividida en generaciones, donde cada una supone un cambio tecnológico notable. El origen de las primeras es sencillo de establecer, ya que en ellas el "hardware fue sufriendo cambios radicales." Los componentes esenciales que constituyen la electrónica del computador fueron totalmente reemplazados en las primeras tres generaciones, originando cambios que resultaron trascendentales. En las últimas décadas es más difícil distinguir las nuevas generaciones, ya que los cambios han sido graduales y existe cierta continuidad en las tecnologías usadas. En principio, se pueden distinguir:

La aparición del microprocesador marca un hito de relevancia, y para muchos autores constituye el inicio de la cuarta generación. A diferencia de los cambios tecnológicos anteriores, su invención no supuso la desaparición radical de los computadores que no lo utilizaban. Así, aunque el microprocesador 4004 fue lanzado al mercado en 1971, todavía a comienzo de la década de 1980 había computadores, como el PDP-11/44, con lógica carente de microprocesador que continuaban exitosamente en el mercado; es decir, en este caso el desplazamiento ha sido muy gradual.

Otro hito tecnológico usado con frecuencia para definir el inicio de la cuarta generación es la aparición de los circuitos integrados VLSI ("very large scale integration"), a principios de los ochenta. Al igual que el microprocesador, no supuso el cambio inmediato y la rápida desaparición de los computadores basados en circuitos integrados en más bajas escalas de integración. Muchos equipos implementados con tecnologías VLSI y MSI ("medium scale integration") aún coexistían exitosamente hasta bien entrados la década de 1990.

Una de las formas de clasificar el "hardware" es en dos categorías: por un lado, el hardware" principal, que abarca el conjunto de componentes indispensables necesarios para otorgar la funcionalidad mínima a una computadora; y por otro lado, el hardware" complementario, que, como su nombre indica, es el utilizado para realizar funciones específicas (más allá de las básicas), no estrictamente necesarias para el funcionamiento de la computadora.

Un medio de entrada de datos, la unidad central de procesamiento, la memoria RAM, un medio de salida de datos y un medio de almacenamiento de datos constituyen el "hardware" básico.

Los medios de entrada y salida de datos estrictamente indispensables dependen de la aplicación: desde el punto de vista de un usuario común, se debería disponer, al menos, de un teclado y un monitor para entrada y salida de información, respectivamente; pero ello no implica que no pueda haber una computadora (por ejemplo controlando un proceso) en la que no sea necesario teclado y/o monitor; bien puede ingresar información y sacar sus datos procesados, por ejemplo, a través de una placa de adquisición/salida de datos.

Las computadoras son aparatos electrónicos capaces de interpretar y ejecutar instrucciones programadas y almacenadas en su memoria; consisten básicamente en operaciones aritmético-lógicas y de entrada/salida. Se reciben las entradas (datos), se las procesa y almacena (procesamiento), y finalmente se producen las salidas (resultados del procesamiento). Por ende todo sistema informático tiene, al menos, componentes y dispositivos "hardware" dedicados a alguna de las funciones antedichas; a saber:


Desde un punto de vista básico y general, un dispositivo de entrada es el que provee el medio para permitir el ingreso de información, datos y programas (lectura); un dispositivo de salida brinda el medio para registrar la información y datos de salida (escritura); la memoria otorga la capacidad de almacenamiento, temporal o permanente (almacenamiento); y la CPU provee la capacidad de cálculo y procesamiento de la información ingresada (transformación).

Un periférico mixto es aquel que puede cumplir funciones tanto de entrada como de salida; el ejemplo más típico es el disco rígido (ya que en él se lee y se graba información y datos).

La Unidad Central de Procesamiento, conocida por las siglas en inglés CPU, es el componente fundamental de la computadora, encargado de interpretar y ejecutar instrucciones y de procesar datos. En computadores modernos, la función de la CPU la realiza uno o más microprocesadores. Se conoce como microprocesador a una CPU que es manufacturada como un único circuito integrado.

Un servidor de red o una máquina de cálculo de alto rendimiento (supercomputación), puede tener varios, incluso miles de microprocesadores trabajando simultáneamente o en paralelo (multiprocesamiento); en este caso, todo ese conjunto conforma la CPU de la máquina.

Las unidades centrales de proceso (CPU) en la forma de un único microprocesador no solo están presentes en las computadoras personales (PC), sino también en otros tipos de dispositivos que incorporan una cierta capacidad de proceso o "inteligencia electrónica", como pueden ser: controladores de procesos industriales, televisores, automóviles, calculadoras, aviones, teléfonos móviles, electrodomésticos, juguetes y muchos más. Actualmente los diseñadores y fabricantes más populares de microprocesadores de PC son Intel y AMD; y para el mercado de dispositivos móviles y de muy bajo consumo, los principales son Samsung, Qualcomm, Texas Instruments, MediaTek, NVIDIA e Intel.

En las computadoras, el microprocesador se monta en la llamada placa base, sobre un zócalo conocido como zócalo de CPU, que permite las conexiones eléctricas entre los circuitos de la placa y el procesador. Sobre el procesador ajustado a la placa base se fija un disipador térmico de un material con elevada conductividad térmica, que por lo general es de aluminio, y en algunos casos de cobre. Este es indispensable en los microprocesadores que consumen bastante energía, la cual, en gran parte, es emitida en forma de calor: en algunos casos pueden consumir tanta energía como una lámpara incandescente (de 40 a 130 vatios).

En equipos de alto rendimiento, adicionalmente, sobre el disipador se acopla uno o dos ventiladores (raramente más), destinados a forzar la circulación de aire para extraer más rápidamente el calor acumulado por el disipador y originado en el microprocesador. Complementariamente, para evitar daños por efectos térmicos, también se suelen instalar sensores de temperatura del microprocesador y sensores de revoluciones del ventilador, así como sistemas automáticos que controlan la cantidad de revoluciones por unidad de tiempo de estos últimos.

La gran mayoría de los circuitos electrónicos e integrados que componen el "hardware" del computador van montados en la "placa madre".

La placa base, también conocida como placa madre o principal o con los anglicismos "motherboard" o "mainboard", es un gran circuito impreso sobre el que se suelda el chipset, las ranuras de expansión (slots), los zócalos, conectores, diversos circuitos integrados, etc. Es el soporte fundamental que aloja y comunica a todos los demás componentes: microprocesador, módulos de memoria RAM, tarjetas gráficas, tarjetas de expansión, periféricos de entrada y salida. Para comunicar esos componentes, la placa base posee una serie de buses mediante los cuales se transmiten los datos hacia dentro y fuera del sistema.

La tendencia de integración ha hecho que la placa base se convierta en un elemento que incluye a la mayoría de las funciones básicas (vídeo, audio, red, puertos de varios tipos), funciones que antes se realizaban con tarjetas de expansión. Aunque ello no excluye la capacidad de instalar otras tarjetas adicionales específicas, tales como capturadoras de vídeo, tarjetas de adquisición de datos, etc.

También, la tendencia en los últimos años es eliminar elementos separados en la placa base e integrarlos al microprocesador. En ese sentido actualmente se encuentran sistemas denominados System on a Chip que consiste en un único circuito integrado que integra varios módulos electrónicos en su interior, tales como un procesador, un controlador de memoria, una GPU, Wi-Fi, Bluetooth, etc. La mejora más notable en esto está en la reducción de tamaño frente a igual funcionalidad con módulos electrónicos separados. Las figuras muestran aplicaciones típicas, placa principal de una computadora y la de un teléfono móvil.

Las principales funciones que presenta una placa base son:

La sigla RAM, del inglés "Random Access Memory", literalmente significa memoria de acceso aleatorio. El término tiene relación con la característica de presentar iguales tiempos de acceso a cualquiera de sus posiciones (ya sea para lectura o para escritura). Esta particularidad también se conoce como "acceso directo", en contraposición al acceso secuencial.

La RAM es la memoria utilizada en una computadora para el almacenamiento transitorio y de trabajo (no masivo). En la RAM se almacena temporalmente la información, datos y programas que la Unidad de Procesamiento (CPU) lee, procesa y ejecuta. La memoria RAM es conocida como memoria principal de la computadora, también como memoria central o de trabajo"; a diferencia de las llamadas memorias auxiliares, secundarias o de "almacenamiento masivo" (como discos duros, unidades de estado sólido, cintas magnéticas u otras memorias).

Las RAM son, comúnmente, memorias volátiles; lo cual significa que pierden rápidamente su contenido al interrumpir su alimentación eléctrica.

Las más comunes y utilizadas como memoria central son "dinámicas" (DRAM), lo cual significa que tienden a perder sus datos almacenados en breve tiempo (por descarga capacitiva, aun estando con alimentación eléctrica), por ello necesitan un circuito electrónico específico que se encarga de proveerle el llamado "refresco" (de energía) para mantener su información.

La memoria RAM de un computador se provee de fábrica e instala en lo que se conoce como “módulos”. Ellos albergan varios circuitos integrados de memoria DRAM que, conjuntamente, conforman toda la memoria principal.

Es la presentación más común en computadores modernos (computador personal, servidor); son tarjetas de circuito impreso que tienen soldados circuitos integrados de memoria por una o ambas caras, además de otros elementos, tales como resistores y condensadores. Esta tarjeta posee una serie de contactos metálicos,con recubrimiento de oro, que permite hacer la conexión eléctrica con el bus de memoria del controlador de memoria en la placa base.

Los integrados son de tipo DRAM, memoria denominada "dinámica", en la cual las celdas de memoria son muy sencillas (un transistor y un condensador), permitiendo la fabricación de memorias con gran capacidad (típicamente entre 1, 2 o 4 Gigabytes por módulo) a un costo relativamente bajo.

Las posiciones de memoria o celdas, están organizadas en matrices y almacenan cada una un bit. Para acceder a ellas se han ideado varios métodos y protocolos cada uno mejorado con el objetivo de acceder a las celdas requeridas de la manera más eficiente posible.

Entre las tecnologías recientes para integrados de memoria DRAM usados en los módulos RAM se encuentran:


Los estándares JEDEC, establecen las características eléctricas y las físicas de los módulos, incluyendo las dimensiones del circuito impreso.

Los estándares usados actualmente son:


Hay memorias RAM con características que las hacen particulares, y que normalmente no se utilizan como memoria central de la computadora; entre ellas se puede mencionar:




De las anteriores a su vez, hay otros subtipos más.

Se entiende por periférico a las unidades o dispositivos que permiten a la computadora comunicarse con el exterior, esto es, tanto ingresar como exteriorizar información y datos. Los periféricos son los que permiten realizar las operaciones conocidas como de entrada/salida (E/S).

Aunque son estrictamente considerados “accesorios” o no esenciales, muchos de ellos son fundamentales para el funcionamiento adecuado de la computadora moderna; por ejemplo, el teclado, el disco duro y el monitor son elementos actualmente imprescindibles; pero no lo son un escáner o un plóter. Para ilustrar este punto: en los años 80, muchas las computadoras personales no utilizaban disco duro ni "mouse" (o ratón), tenían sólo una o dos disqueteras, el teclado y el monitor como únicos periféricos.

De esta categoría son aquellos que permiten el ingreso de información, en general desde alguna fuente externa o por parte del usuario. Los dispositivos de entrada proveen el medio fundamental para transferir hacia la computadora (más propiamente al procesador) información desde alguna fuente, sea local o remota. También permiten cumplir la esencial tarea de leer y cargar en memoria el sistema operativo y las aplicaciones o programas informáticos, los que a su vez ponen operativa la computadora y hacen posible realizar las más diversas tareas.

Entre los periféricos de entrada se puede mencionar: teclado, "mouse" o ratón, escáner, micrófono, cámara web, lectores ópticos de código de barras, Joystick, lectora de CD, DVD o BluRay (solo lectoras), placas de adquisición/conversión de datos, etc.

Pueden considerarse como "imprescindibles" para el funcionamiento, (de manera como hoy se concibe la informática) al teclado, al "ratón" y algún dispositivo lector de discos; ya que tan sólo con ellos el "hardware" puede ponerse operativo para un usuario. Los otros son más bien accesorios, aunque en la actualidad pueden resultar de tanta necesidad que son considerados parte esencial de todo el sistema.

Son aquellos que permiten emitir o dar salida a la información resultante de las operaciones realizadas por la CPU (procesamiento).

Los dispositivos de salida aportan el medio fundamental para exteriorizar y comunicar la información y datos procesados; ya sea al usuario o bien a otra fuente externa, local o remota.

Los dispositivos más comunes de este grupo son los monitores clásicos (no de pantalla táctil), las impresoras, las consolas. y los altavoces.

Entre los periféricos de salida puede considerarse como imprescindible para el funcionamiento del sistema, al monitor, las consolas para sonido. Otros, aunque accesorios, son sumamente necesarios para un usuario que opere un computador moderno.

Son aquellos dispositivos que pueden operar de ambas formas: tanto de entrada como de salida. Típicamente, se puede mencionar como periféricos mixtos o de entrada/salida a: discos rígidos, disquetes, unidades de cinta magnética, lecto-grabadoras de CD/DVD, discos ZIP, etc. También entran en este rango, con sutil diferencia, otras unidades, tales como: Tarjetas de Memoria flash o unidad de estado sólido, tarjetas de red, módems, tarjetas de captura/salida de vídeo, etc.

Si bien se puede clasificar al "pendrive" (lápiz de memoria), memoria flash o memoria USB o a las unidades de estado sólido (SSD) en la categoría de "memorias", normalmente se los utiliza como dispositivos de almacenamiento masivo; siendo todos de categoría Entrada/Salida.

Los dispositivos de almacenamiento masivo también son conocidos como "Memorias Secundarias o Auxiliares". Entre ellos, sin duda, el disco duro ocupa un lugar especial, ya que es el de mayor importancia en la actualidad, en el que se aloja el sistema operativo, todas las aplicaciones, utilitarios, etc. que utiliza el usuario; además de tener la suficiente capacidad para albergar información y datos en grandes volúmenes por tiempo prácticamente indefinido. Los servidores Web, de correo electrónico y de redes con bases de datos, utilizan discos rígidos de grandes capacidades y con una tecnología que les permite trabajar a altas velocidades como SCSI incluyendo también, normalmente, capacidad de redundancia de datos, RAID; incluso se utilizan tecnologías híbridas: disco rígido y unidad de estado sólido, lo que incrementa notablemente su eficiencia. Las interfaces actuales más usadas en discos duros son: IDE, SATA, SCSI y SAS; y en las unidades de estado sólido son SATA y PCI-Express ya que necesitan grandes anchos de banda.

La pantalla táctil (no el monitor clásico) es un dispositivo que se considera mixto, ya que además de mostrar información y datos (salida) puede actuar como un dispositivo de entrada, reemplazando, por ejemplo, a algunas funciones del ratón o del teclado.

El "hardware" gráfico lo constituyen básicamente las tarjetas gráficas. Dichos componentes disponen de su propia memoria y unidad de procesamiento, esta última llamada unidad de procesamiento gráfico (o GPU, siglas en inglés de "Graphics Processing Unit"). El objetivo básico de la GPU es realizar los cálculos asociados a operaciones gráficas, fundamentalmente en coma flotante, liberando así al procesador principal (CPU) de esa costosa tarea (en tiempo) para que este pueda efectuar otras funciones en forma más eficiente. Antes de esas tarjetas de vídeo con aceleradores por hardware, era el procesador principal el encargado de construir la imagen mientras la sección de vídeo (sea tarjeta o de la placa base) era simplemente un traductor de las señales binarias a las señales requeridas por el monitor; y buena parte de la memoria principal (RAM) de la computadora también era utilizada para estos fines.

Dentro de esta categoría no se deben omitir los sistemas gráficos integrados ("IGP"), presentes mayoritariamente en equipos portátiles o en equipos prefabricados ("OEM"), los cuales generalmente, a diferencia de las tarjetas gráficas, no disponen de una memoria dedicada, utilizando para su función la memoria principal del sistema. La tendencia en los últimos años es integrar los sistemas gráficos dentro del propio procesador central. Los procesadores gráficos integrados ("IGP") generalmente son de un rendimiento y consumo notablemente más bajo que las GPU de las tarjetas gráficas dedicadas, no obstante, son más que suficiente para cubrir las necesidades de la mayoría de los usuarios de un PC.

Actualmente se están empezando a utilizar las tarjetas gráficas con propósitos no exclusivamente gráficos, ya que en potencia de cálculo la GPU es superior, más rápida y eficiente que el procesador para operaciones en coma flotante, por ello se está tratando de aprovecharla para propósitos generales, al concepto, relativamente reciente, se le denomina GPGPU ("General-Purpose Computing on Graphics Processing Units").

La ley de Moore establece que cada 18 a 24 meses la cantidad de transistores que puede contener un circuito integrado se logra duplicar; en el caso de los GPU esta tendencia es bastante más notable, duplicando, o aún más, lo indicado en la ley de Moore.

Desde la década de 1990, la evolución en el procesamiento gráfico ha tenido un crecimiento vertiginoso; las actuales animaciones por computadoras y videojuegos eran impensables veinte años atrás.




</doc>
<doc id="9023" url="https://es.wikipedia.org/wiki?curid=9023" title="Mitología romana">
Mitología romana

La mitología romana, es decir, las creencias mitológicas de los habitantes de la Antigua Roma, puede considerarse formada por dos partes: La primera, mayoritariamente antigua y ritualista, representaba los mitos y cultos autóctonos. La segunda, principalmente tardía y literaria, consiste en la fusión de la anterior con varios préstamos, completamente nuevos, procedentes de la mitología griega.

Los romanos no tenían relatos secuenciales, sus dioses, comparables a la Titanomaquia o la seducción de Zeus por Hera, hasta que sus poetas comenzaron a adoptar los modelos griegos a finales del periodo republicano. Sin embargo, lo que sí tenían era:


El modelo romano incluía una forma muy diferente a la de los antiguos griegos de definir y concebir a los dioses. Por ejemplo, en la mitología griega Deméter era caracterizada por una historia muy conocida sobre su dolor por el rapto de su hija Perséfone a manos de Hades. Los antiguos romanos, por el contrario, concebían a su equivalente Ceres como una deidad con un sacerdote oficial llamado "Flamen", subalterno de los "flamines" de Júpiter, Marte y Quirino, pero superior a los de Flora y Pomona. También se le consideraba agrupado en una tríada con otros dos dioses agrícolas, Liber y Libera, y se sabía la relación de dioses menores con funciones especializadas que le asistían: "Sarritor" (escardado), "Messor" (cosecha), "Convector" (transporte), "Conditor" (almacenaje), "Insitor" (siembra) y varias docenas más.

Así pues, la «mitología» romana arcaica, al menos en lo referente a los dioses, no estaba formada por relatos sino más bien el entrelazamiento y las complejas interrelaciones entre dioses y humanos.

La religión original de los primeros romanos fue modificada por la adición de numerosas y contradictorias creencias en épocas posteriores, y por la asimilación de grandes porciones de la mitología griega. Lo poco que se sabe sobre la religión romana primitiva no es gracias a relatos de la época sino a escritores posteriores que buscaron preservar las viejas tradiciones del olvido en el que estaban cayendo, como el estudioso del Marco Terencio Varrón. Otros escritores clásicos, como el poeta Ovidio en sus "Fastos" (‘calendario’), fueron fuertemente influidos por los modelos helenísticos, y en sus obras se recurre con frecuencia a las creencias griegas para rellenar los huecos de las tradiciones romanas.

Los romanos tenían una rica panoplia de leyendas sobre la fundación y primera expansión de su propia ciudad. Además de estas tradiciones de origen mayoritariamente local, a este surtido se añadió material procedente de las leyendas heroicas griegas en una época temprana, haciendo por ejemplo a Eneas antepasado de Rómulo y Remo.

La "Eneida" y los primeros libros de Livio son las mejores fuentes exhaustivas para esta mitología romana.

Las prácticas rituales romanas de los sacerdotes oficiales distinguían claramente dos clases de dioses: los "di indigetes" y los "di novensides" o "novensiles". Los "indigetes" eran los dioses originales del estado romano (véase "Di indigetes"), y su nombre y naturaleza están indicados por los títulos de los sacerdotes más antiguos y por las fiestas fijas del calendario. Los "novensides" eran divinidades posteriores cuyos cultos fueron introducidos en la ciudad en el periodo histórico, normalmente en una fecha conocida y como respuesta a una crisis específica o necesidad percibida.

Las divinidades romanas primitivas incluían, además de los "di indigetes", un montón de los llamados dioses especialistas cuyos nombres eran invocados al realizar diversas actividades, como la cosecha. Los fragmentos de los viejos rituales que acompañaban a estos actos como el arado o la siembra revelan que en cada parte del proceso se invocaba a una deidad diferente, estando el nombre de cada una de ellas derivado regularmente del verbo para la operación. Estas divinidades pueden ser agrupadas bajo el término general de dioses asistentes o auxiliares, que eran invocados junto con las deidades mayores. Los antiguos cultos romanos eran más un polidemonismo que un politeísmo: los conceptos que los adoradores tenían de los seres invocados consistían en poco más que sus nombres y funciones, y el "numen" o ‘poder’ del ser se manifestaba en formas altamente especializadas.

El carácter de los "indigetes" y sus fiestas muestran que los antiguos romanos no solo eran miembros de una comunidad agrícola sino que también estaban orgullosos de luchar y muy involucrados con la guerra. Los dioses representaban distintivamente las necesidades prácticas de la vida diaria, como las sentía la comunidad romana a la que pertenecían. Se entregaban escrupulosamente a los ritos y ofrendas que consideraban apropiados.
Así, Jano y Vesta guardaban la puerta y el hogar, los Lares protegían el campo y la casa, Pales los pastos, Saturno la siembra, Ceres el crecimiento del grano, Pomona la fruta, y Consus y Ops la cosecha. Incluso el majestuoso Júpiter, rey de los dioses, era honrado por la ayuda que sus lluvias daban a las granjas y viñedos. En su más amplio carácter era considerado, a través de su arma de rayos, el director de la actividad humana y, por su amplio dominio, el protector de los romanos en sus expediciones militares allende las fronteras de su propio país. Prominentes en la época más antigua fueron los dioses Marte y Quirino, que a menudo se identificaban entre sí. Marte era un dios de la guerra al que se honraba en marzo y octubre. Los investigadores modernos creen que Quirino fue el patrón de la comunidad militar en tiempos de paz.

A la cabeza del panteón primitivo se encontraba la tríada Júpiter, Marte y Quirino (cuyos tres sacerdotes, o "flamines", tenían el mayor rango), y Jano y Vesta. Estos dioses antiguos tenían poca individualidad, y sus historias personales carecían de matrimonios y genealogías. A diferencia de los dioses griegos, no se consideraba que funcionaban de la misma forma que los mortales, y por ello no existen muchos relatos de sus actividades. Este culto primitivo está asociado con Numa Pompilio, el segundo rey de Roma, de quien se creía que tuvo como consorte y consejera a la diosa romana de las fuentes y los partos, Egeria, a quien a menudo se identifica como una ninfa en las fuentes literarias posteriores. Sin embargo, se añadieron nuevos elementos en una época relativamente temprana. A la casa real de los Tarquinios se atribuyó en las leyendas el establecimiento de la gran Tríada Capitolina, Júpiter, Juno y Minerva, que asumió el lugar supremo en la religión romana. Otras adiciones fueron el culto a Diana en el monte Aventino y la introducción de los Libros Sibilinos, profecías de la historia del mundo que, según la leyenda, fueron compradas por Tarquinio a finales del a la Sibila de Cumas.

La absorción de deidades locales vecinas tuvo lugar a medida que el estado romano conquistaba el territorio vecino. Los romanos solían conceder a los dioses locales del territorio conquistado los mismos honores que a los dioses antiguos que habían sido considerados propios del estado romano. En muchos casos las recién adquiridas deidades eran invitadas formalmente a llevar su domicilio a nuevos santuarios en Roma. En , la figura de culto representativa de Cibeles fue retirada de Pesino (Frigia) y acogida ceremoniosamente en Roma. Además, el crecimiento de la ciudad atrajo a extranjeros, a los que se permitía continuar con la adoración a sus propios dioses. De esta forma llegó Mitra a Roma y su popularidad en las legiones extendió su culto hasta tan lejos como Bretaña. El dios Sol Invictus deriva del mitraísmo, tuvo un culto bastante extendido entre los militares a partir del siglo III, aparece representado en algunas monedas acuñadas por Constantino I el Grande. Además de Cástor y Pólux, los asentamientos conquistados en Italia parecen haber contribuido al panteón romano con Diana, Minerva, Hércules, Venus y otras deidades de menor rango, algunas de las cuales eran divinidades itálicas, procediendo otras originalmente de la cultura griega de Magna Grecia. Las deidades romanas importantes fueron finalmente identificadas con los más antropomórficos dioses y diosas griegos, y asumieron muchos de sus atributos y mitos.




</doc>
<doc id="9030" url="https://es.wikipedia.org/wiki?curid=9030" title="Magnitud física">
Magnitud física

Una magnitud física es una cantidad medible de un sistema físico, es decir, a la que se le pueden asignar distintos valores como resultado de una medición o una relación de medidas. Las magnitudes físicas se miden usando un patrón que tenga bien definida esa magnitud, y tomando como unidad la cantidad de esa propiedad que posea el objeto patrón. Por ejemplo, se considera que el patrón principal de longitud es el metro en el Sistema Internacional de Unidades.

Existen magnitudes básicas y derivadas, que constituyen ejemplos de magnitudes físicas: la masa, la longitud, el tiempo, la carga eléctrica, la densidad, la temperatura, la velocidad, la aceleración y la energía. En términos generales, es toda propiedad de los cuerpos o sistemas que puede ser medida. De lo dicho se desprende la importancia fundamental del instrumento de medición en la definición de la magnitud.

La Oficina Internacional de Pesas y Medidas, por medio del Vocabulario Internacional de Metrología (International Vocabulary of Metrology, VIM), define a la magnitud como "un atributo de un fenómeno, un cuerpo o sustancia que puede ser distinguido cualitativamente y determinado cuantitativamente".
A diferencia de las unidades empleadas para expresar su valor, las magnitudes físicas se expresan en cursiva: así, por ejemplo, la «masa» se indica con "m", y «una masa de 3 kilogramos» la expresaremos como "m" = 3 kg.

Las magnitudes físicas pueden ser clasificadas de acuerdo a varios criterios:



De acuerdo con el tipo de magnitud, debemos escoger leyes de transformación (por ej. la transformación de Lorentz) de las componentes físicas de las magnitudes medidas, para poder ver si diferentes observadores hicieron la misma medida o para saber qué medidas obtendrá un observador, conocidas las de otro cuya orientación y estado de movimiento respecto al primero sean conocidos.

Una magnitud extensiva es una magnitud que depende de la cantidad de sustancia que tiene el cuerpo o sistema. Las magnitudes extensivas son aditivas. Si consideramos un sistema físico formado por dos partes o subsistemas, el valor total de una magnitud extensiva resulta ser la suma de sus valores en cada una de las dos partes. Ejemplos: la masa y el volumen de un cuerpo o sistema, la energía de un sistema termodinámico, etc.

Una magnitud intensiva es aquella cuyo valor no depende de la cantidad de materia del sistema. Las magnitudes intensivas tienen el mismo valor para un sistema que para cada una de sus partes consideradas como subsistemas. Ejemplos: la densidad, la temperatura y la presión de un sistema termodinámico en equilibrio.

En general, el cociente entre dos magnitudes extensivas da como resultado una magnitud intensiva. Ejemplo: masa dividida por volumen representa densidad.

Las magnitudes tensoriales de orden igual o superior a uno admiten varias formas de representación tensorial según el número de índices contravariantes y covariantes. Esto no es muy importante si el espacio es euclídeo y se emplean coordenadas cartesianas, aunque si el espacio no es euclídeo o se usan coordenadas no cartesianas es importante distinguir entre diversas representaciones tensoriales que físicamente representan la misma magnitud. En relatividad general dado que en general el espacio-tiempo es curvo el uso de representaciones convariantes y cotravariantes es inevitable.

Así un vector puede ser representado mediante un tensor 1-covariante o mediante un tensor 1-contravariante. Más generalmente, una magnitud tensorial de orden "k" admite 2 representaciones tensoriales esencialmente equivalentes. Esto se debe a que en un espacio físico representable mediante una variedad riemanniana (o semiriemanninana como en el caso relativista) existe un isomorfismo entre tensores de tipo formula_1 y los de tipo formula_2 siempre y cuando formula_3. El paso de una representación a otra de otro tipo se lleva a cabo mediante la operación de "bajar y subir índices".

Una magnitud se dice objetiva si las medidas de dicha magnitud por observadores diferentes pueden relacionarse de manera sistemática. En el contexto de la mecánica newtoniana se restringe el tipo de observador, y se considera que una magnitud es objetiva si se pueden relacionar sistemáticamente las medidas de dos observadores cuyo movimiento relativo en un instante dado es un movimiento de sólido rígido. Existen buenos argumentos para sostener que una ley física adecuada debe estar formulada en términos de magnitudes físicas objetivas. En el contexto de la teoría de la relatividad la objetividad física se amplia al concepto de covariancia de Lorentz (en relatividad especial) y covariancia general (en relatividad general).

El Sistema Internacional de Unidades se basa en dos tipos de magnitudes físicas:

Las magnitudes básicas derivadas del SI son las siguientes:



Una vez definidas las magnitudes que se consideran básicas, las demás resultan derivadas y se pueden expresar como combinación de las primeras.

Las unidades derivadas se usan para las siguientes magnitudes: superficie, volumen, velocidad, aceleración, densidad, frecuencia, periodo, fuerza, presión, trabajo, calor, energía, potencia, carga eléctrica, diferencia de potencial, potencial eléctrico, resistencia eléctrica, etc.

Algunas de las unidades usadas para esas magnitudes derivadas son:



</doc>
<doc id="9031" url="https://es.wikipedia.org/wiki?curid=9031" title="Porinas">
Porinas

Las porinas son proteínas con estructura barril β formadas por láminas β.Pertenecen a las proteínas integrales de membrana, que son las que se ubican a través de una membrana celular y funcionan como poros a través de los cuales las moléculas se pueden difundir. A diferencia de otras proteínas de transporte de membranas, las porinas son lo suficientemente grandes para permitir procesos de difusión pasiva, por tanto actúan como canales que son específicos para diferentes tipos de moléculas. Están presentes en la membrana exterior de las bacterias gram-negativas y algunas bacterias gram-positivas del grupo Mycolata, las mitocondrias y cloroplastos.

Las porinas están compuestas por láminas β. Las láminas β se acomodan usualmente antiparalelas formando un tubo cilíndrico llamado barril β . Su estructura primaria es única en cuanto a que se alternan residuos polares y no polares. Esto significa que los residuos no polares apuntan hacia afuera para interactuar con la membrana lipídica apolar, mientras que los residuos polares apuntan hacia el centro del barril β para interactuar con el canal acuoso. 

El canal de la porina se encuentra parcialmente bloqueado por un loop llamado ojal o ""eyelet"" que se proyecta dentro de la cavidad. En general, se encuentra entre las láminas 5 y 6 de cada barril, y define el tamaño del soluto que puede pasar a través del canal. Dicho loop se encuentra cubierto casi exclusivamente con aminoácidos cargados, que se organizan en lados opuestos del canal, creando un campo eléctrico transversal a través del poro. El ojal ""eyelet"" tiene una carga negativa proveniente de cuatro residuos de ácido glutámico y siete residuos de ácido aspartico (en contraste con un residuo de histidina, dos residuos de lisina y tres residuos de arginina), esta carga es compensada por dos átomos de calcio.

El transporte de moléculas medianas o con carga a través de la membrana.
Las porinas típicamente controlan la difusión de pequeños metabolitos como azúcares, iones, y aminoácidos

En bacterias gram-negativas, la membrana interna es la mayor barrera permeable, mientras que la membrana externa contiene porinas que le confiere permeabilidad a las moléculas de menos de 1500 daltons.

El término "nucleoporina" se refiere a porinas que facilitan el transporte a través de poros nucleares en la envoltura nuclear. Sin embargo son consideradas diferentes de las demás porinas, (no están clasificadas como porinas en MeSH.)

El descubrimiento de las porinas le ha sido atribuido a Hiroshi Nikaido.




</doc>
<doc id="9034" url="https://es.wikipedia.org/wiki?curid=9034" title="Pseudogén">
Pseudogén

Un pseudogén se trata de un gen que deriva de otros genes ya conocidos y cuyas funciones son distintas, pueden haber perdido su funcionalidad o haberla cambiado radicalmente.

Se han propuesto varios escenarios para explicar el origen de un pseudogén:


Los pseudogenes pueden complicar los estudios de genética molecular. Por ejemplo, un investigador que quiera amplificar un gen mediante PCR puede amplificar simultáneamente un pseudogén que comparta secuencias similares. Además, en ocasiones los pseudogenes se registran como genes en la secuenciación de genomas.

Es típico en biología molecular encontrar ejemplos raros que cuestionan cualquier definición sencilla de un término, y el de "pseudogén" no es una excepción. Hay cierta división entre los genetistas sobre la naturaleza del producto final. Si el producto final "tiene" que ser una proteína, entonces algunos pseudogenes pueden funcionar como ARN. Por ejemplo, Hirotsune "et al" (2003) descubrieron una secuencia en el genoma humano que se había identificado como pseudogen pero que aparentemente tiene una función reguladora para su gen homólogo codificador. Sin embargo, esta definición no permite pseudogenes de ARNt o ARNr, tal y como usan el término otros genetistas.

En 2008, varios estudios en moscas y en ratones, proporcionan nueva información: en estos estudios, se sugiere una conexión entre RNAi y pseudogenes. En general, el proceso de RNAi implica varios tipos de pequeñas secuencias de ARN "guía" que regulan los niveles de la proteína diana al direccionar para su degradación el ARNm de ésta. En los seis estudios indicados, siRNAs procedentes de pseudogenes generan dos de las cuatro categorías de siRNAs naturales o endo-siRNAs (ir a ARN interferente para más información). Recientemente un trabajo ha mostrado cómo los transcritos de un pseudogén del supresor de tumores "PTEN" ("PTEN1") secuestra secuencias de los miARNs que reducen la experesión de este gen, aumentando por consiguiente los niveles de "PTEN". En realidad, el transcrito de "PTEN" actúa del mismo modo con el transcrito de "PTEN1", regulándose mutuamente. Se ha visto que en algunos tipos de cáncer, como el de colon, el pseudogen "PTEN1" está inactivo, lo cual hace que a su vez "PTEN" esté infrarregulado, aumentando de ese modo la probabilidad de que se produzcan tumores.



</doc>
<doc id="9035" url="https://es.wikipedia.org/wiki?curid=9035" title="Oncovirus">
Oncovirus

Un oncovirus es un término utilizado para describir a los virus oncogénicos que al infectar células tienen la capacidad de alterar el ciclo celular de estas, induciendo el desarrollo de tumores. 

Los mejor caracterizados son los retrovirus, los cuales durante la infección, integran su ADN al genoma de la célula huésped y por evento raro de recombinación son escindidos nuevamente del genoma portando consigo un segmento del ADN de la célula huésped. Si este segmento contiene secuencias reguladoras de un paso crítico de la división celular, el virus al infectar otras células afectará este proceso haciendo que ellas se dividan sin control y se generen tumores.



</doc>
<doc id="9036" url="https://es.wikipedia.org/wiki?curid=9036" title="Capacidad pulmonar">
Capacidad pulmonar

Las capacidades pulmonares se refieren a los distintos volúmenes de aire característicos en la respiración humana. Un pulmón humano puede almacenar alrededor de 4,6 litros de aire en su interior, pero una cantidad significativamente menor es la que se inhala y exhala durante la respiración.

Al describir los procesos del ciclo pulmonar, a veces es deseable considerar juntos dos o más volúmenes pulmonares, estas combinaciones de volúmenes son llamados capacidades pulmonares

Una persona en reposo realiza 12 respiraciones por minuto; si en cada entrada y salida de aire moviliza 500 ml, en un minuto movilizará 6000 ml.



</doc>
<doc id="9037" url="https://es.wikipedia.org/wiki?curid=9037" title="Moái">
Moái

Los moáis (del rapanui: "moai", 'escultura') son estatuas monolíticas humanoides ubicadas en la isla de Pascua (Chile). Los moáis son el principal atractivo turístico de la isla.

Los más de novecientos moáis conocidos esculpidos por los antiguos rapa nui están distribuidos por toda la isla. La mayoría de ellos fueron labrados en toba del cono volcánico Rano Raraku, donde quedan más de cuatrocientos moái en diferentes fases de construcción. El período histórico de todo el desarrollo de las diversas técnicas constructivas se extendió entre el 700 d.C y el 1600 d.C Todo indica que la cantera fue abandonada repentinamente y quedaron estatuas a medio labrar en la roca. Prácticamente todos los moái terminados, originalmente situados sobre una plataforma o altar ceremonial, llamada "ahu" en idioma rapanui, fueron posteriormente derribados por los isleños nativos en el período siguiente al cese de la construcción, en el siglo XV. Desde 1956 unos pocos de ellos han sido restaurados.

En un principio, estas estatuas gigantes llevaban también unos copetes o moños de piedra roja de más de diez toneladas llamados "pukao", que se extraían del cráter de Puna Pau. Una vez tallado el mismo,debía ser elevado hasta la debida altura para colocarlos sobre las cabezas.

Con la restauración del "ahu" Nau-Nau en la playa de Anakena en 1978, se descubrió que, en las cavidades oculares, solían colocarse placas de coral a modo de ojos. Estas fueron retiradas, destruidas, enterradas o arrojadas al mar, en donde también se han encontrado. Esto concuerda con la teoría de que los mismos pobladores los derribaron, quizás durante guerras tribales.

Los primeros navegantes europeos que, a comienzos del siglo XVIII, llegaron a la Isla de Pascua no podían creer lo que estaban viendo. En esa pequeña área de tierra, descubrieron cientos de estatuas enormes por la superficie de toda la isla.

El significado de los moáis es aún incierto, y hay varias teorías en torno a estas estatuas. La más común de ellas es que las estatuas fueron talladas por los habitantes polinesios de la isla, entre los siglos IX y XVI, como representaciones de antepasados difuntos, de manera que proyectaran su "mana" (poder sobrenatural) sobre sus descendientes.

Debían situarse sobre los "ahu" (plataformas ceremoniales) con sus rostros hacia el interior de la isla (excepto los siete situados en el "Ahu Akivi" y un moái de cuatro manos señalizando el solsticio de invierno en el "Ahu Huri A Urenga") y, tras colocarles ojos de coral con pupila de obsidiana o roca volcánica roja, se convertían en "aringa ora" (‘rostro viviente’) de un ancestro —el nombre completo de las estatuas en el idioma local es "aringa ora o te tupuna" (‘rostros vivientes de los antepasados’)—.

Un estudio de Lipo, Matt Becker y Tanya Bronson, de la Universidad Estatal de California, Long Beach, sugiere que estas estatuas ceremoniales fueron colocadas para señalar los lugares donde se encontraba el agua potable, un recurso difícil de hallar en esa isla volcánica. Descubrieron que en los lugares donde no hay moáis tampoco hay agua dulce, y en los lugares donde hay moáis, incluso en el interior de la isla, hay fuentes cercanas de agua potable.

La roca volcánica se podía cortar con relativa facilidad con herramientas de basalto y obsidiana, dándoles su forma básica en la propia cantera. Posteriormente eran extraídas y semienterradas en las cercanías para esculpir los detalles.

No se sabe exactamente cómo eran trasladados, pero es casi seguro que dicho proceso exigió el uso de trineos o rodillos de madera. Una segunda teoría de 1982 del ingeniero checo Pavel Pavel, propone la solución más simple y práctica al traslado hasta el momento, balanceando su peso erguido y haciéndolo «caminar» (según la tradición, los moái «caminaban»), teoría puesta en práctica con un modelo de hormigón en la ciudad checa de Strakonice, y posteriormente experimentada en 1985 en la isla junto a Thor Heyerdahl y a Sergio Rapu, con un moái real, y usando materiales de la isla, posteriormente el arqueólogo Carl Lipo y el antropólogo Terry Hunt prueban nuevamente esta teoría con un modelo de hormigón rudimentario.

Durante el verano del año 2000, un equipo arqueológico norteamericano descubrió datos que sugieren la utilización de máquinas complejas en la isla hace siglos. El geólogo Charles M. Love y un equipo de diecisiete estudiantes excavaron secciones de las tres principales carreteras que sirvieron para transportar las estatuas gigantes. Parte de estas carreteras fue excavada originalmente en el lecho de roca de la isla, formado principalmente de roca volcánica de un tipo conocido como pahoehoe.

Curiosamente, las carreteras no son planas sino que su sección muestra una forma característica en «V» o «U». Su anchura media es de 3,5 metros y se requiere un alto nivel de conocimiento ingenieril. En algunos tramos, las carreteras están flanqueadas por líneas de rocas.

Pero quizá lo más sorprendente es que estas rocas no están simplemente colocadas allí, sino encajadas en agujeros tallados en el lecho de roca que forma el suelo de la isla. Un detalle relevante es que este tipo de agujeros se da en los tramos en los que la carretera discurre cuesta arriba. El Dr. Love especula con la posibilidad de que estos agujeros fueran colocados allí para acomodar algún tipo de mecanismo ideado para ayudar a mover las gigantescas cabezas de piedra y salvar desniveles que, de otra manera, requerirían un notable esfuerzo.

Estos agujeros, así como la curiosa forma en «V» de las carreteras nos indican que aún existen importantes incógnitas sobre el sistema que emplearon los nativos de la isla de Pascua para erigir sus misteriosos moáis.


De los aproximadamente novecientos moái en la Isla de Pascua, unos cuatrocientos se encuentran en la cantera de Rano Raraku, 288 vinculados a los "ahu", y el resto dispersos en distintos puntos de la isla, probablemente abandonados en la ruta hacia algún ahu.

Del total, más de ochocientos fueron tallados en la toba lapilli del Rano Raraku, veintidós en traquita blanca, dieciocho en escoria roja y diez en basalto.

La altura media de los moái es de unos 4,5 metros, pero los antiguos Rapa Nui fueron capaces de trabajar y trasladar dos estatuas de diez metros de altura.

El peso estándar ronda las cinco toneladas y no más de treinta a cuarenta estatuas pesan más de diez toneladas. Estas corresponden a la época de pleno desarrollo de la cultura rapanui llamada Período Ahu Moai, situado entre los años 1500 y 

Existe una variada tipología de moái, respondiendo sin duda a una evolución en diseño —que se fue haciendo más estilizado y adornado a lo largo del tiempo—, tamaño, técnicas y materiales. Se pueden clasificar por altura de la siguiente forma:

En la cantera principal de Rano Raraku quedó inacabado aún en su nicho, un moái de 21,65 m, conocido como Te Tokanga (El Gigante), que habría llegado a pesar más de doscientas setenta toneladas, algo impensable aún para la tecnología moderna. La tradición isleña sostiene que este Moai estaba destinado al "Ahu" Vinapu.

Las estatuas de mayor tamaño se encuentran abandonadas en la ladera de la cantera, lo que demuestra que la sociedad rapanui estaba embarcada en una competencia que finalmente se resuelve abandonando estas construcciones monumentales.

El moái Tukuturi, el más antiguo, fue descubierto por Thor Heyerdahl en 1955, se trata de una figura femenina en posición sentada o arrodillada y con la cabeza ligeramente elevada hacia el cielo, las manos se encuentran en posición de orar. Único en su forma, fue datado aproximadamente en el siglo VI ()

Se trata de una figura masculina tallada en madera, originalmente de toromiro, esquelética con vientre hundido y prominentes costillas, que es precisamente lo que significa la palabra rapanui «Kava Kava» (costillas). El tronco es largo y las extremidades cortas con pies pequeños. El rostro es afilado, de mejillas finas y perfil aguileño y suele acabar en una pequeña barba. Tiene orejas largas y puntiagudas y los ojos aparecen muy abiertos con expresión de espanto y están hechos de hueso y obsidiana. Algunas tienen altorrelieves en el cráneo, otras presentan una especie de casco o sombrero y a veces aparecen adornadas con cabellos humanos. También es uno de los suvenires más reconocidos de la isla después de los colosales moáis.

Según la mitología de la isla esta figura representaría el aspecto de los espíritus o Aku-Aku avistados en Puna Pau por el ariki Tu’u Koihu, hijo mayor de Hotu Matu’a, por los cuales era vigilado y no podía contar sobre ellos, por lo que talló a las descarnadas figuras en madera.

Versión femenina del moái Kava-Kava, muy similar en forma pero con leves variaciones, como costillas poco prominentes de aspecto más plano, senos colgantes, y carecen de curvas femeninas, tienen un aspecto bastante masculino porque son enjutas, calvas y hasta con pequeñas barbas.

Figura masculina, de proporciones y rasgos prácticamente humanos, pero totalmente opuesta a la figura del moái Kava-Kava, posee vientre prominente y la cabeza aumentada.

Figura masculina que posee las mismas características estéticas del moái Kava-Kava aunque con una cabeza aviforme con un prominente pico.









</doc>
<doc id="9043" url="https://es.wikipedia.org/wiki?curid=9043" title="Adán cromosómico">
Adán cromosómico

Según la genética poblacional humana del cromosoma Y, el Adán cromosómico o Adán cromosómico-Y habría sido un hombre africano (homólogo de la Eva mitocondrial) que en la evolución humana correspondería al ancestro común más reciente humano masculino que poseía el cromosoma Y del cual descienden todos los «cromosomas Y» de la población humana actual.

Por ello, el Adán cromosómico-Y correspondería a un único antepasado masculino del cual convergería el ADN del cromosoma Y de toda la población actual de "Homo sapiens" (seres humanos).

Se han realizado varias estimaciones sobre la antigüedad del Adán cromosómico-Y que van de los 60 000 años a los 140 000. Sin embargo un reciente análisis (2012) extiende la presencia de un linaje de cromosoma Y actual desde hace unos 340 000 años aproximadamente. "(véase: Haplogrupos del cromosoma Y humano)"

El Adán cromosómico-Y recibe su nombre del personaje bíblico Adán que se relata en el libro del "Génesis" (en la "Biblia")".
Esto ha llevado a algunos malentendidos entre el público general. Una opinión común es creer que este Adán habría sido el único hombre que vivía en su tiempo. Sin embargo otros creen que hombres anteriores a Adán pertenecientes igualmente a aquella época, probablemente también habrían tenido descendencia hasta hoy en día. Sin embargo, solo el Adán cromosómico-Y fue quien produjo una línea «completa» de hijos varones hasta el día de hoy; y es el ancestro del cual converge toda la población actual.

También se lo denomina ACMR-Y (en inglés "Y-MRCA"), siglas del «ancestro común más reciente según el cromosoma Y».

El Adán cromosómico-Y sería el varón del cual descienden todos los cromosomas Y, que determinan el sexo masculino. 

Un estudio biológico de la Universidad de Stanford sobre 93 polimorfismos genéticos humanos hallados en este cromosoma, en 1000 individuos de 21 regiones del mundo, calculó que un antepasado o grupo de antepasados masculinos comunes a todos los humanos actuales vivió en África hace unos 40 000 a 50 000 años, lo que coincide con un estudio de 1996. Para el 2003 se calculaba una antigüedad de 60 000 años y se sostenía que el antepasado masculino común fue bastante posterior a la antepasada común, por razones que se desconocen y se consideraba que la aparición del Adán cromosómico-Y estaría relacionada con la Teoría de la catástrofe de Toba.

Sin embargo, los estudios en general no incluyen el genoma completo del cromosoma Y de todos los individuos testeados, por lo que era de esperar que estudios más profundos encontrasen mutaciones más antiguas. Es así que un equipo genetista italiano encontró en poblaciones aisladas del África Occidental, África del Norte y en pigmeos bakola del Camerún, los linajes relictos A1a y A1b, que aumentan la edad de Adán al menos al doble de lo previamente calculado, estimándose recientemente (2011) unos 142 000 años de antigüedad. Este mismo estudio sugiere que el origen del Adán cromosómico estaría en algún lugar de la región central-noroccidental de África; sin embargo se afirma también que esta presunción es aún muy tentativa debido a que el muestreo de los hombres africanos es aún incompleto, como también es incompleto el conocimiento sobre los acontecimientos demográficos del pasado. Efectivamente, el descubrimiento de un linaje relicto muy antiguo en una familia afroamericana de Carolina del sur extendería la antigüedad del Adán cromosómico hasta los 340 000 años.

En el año 2013 se detectó una muestra de ADN proveniente del National Geographic Genographic Project, cuyo análisis del cromosoma Y, resultó pertenecer a un linaje de ramificación aún más temprana de un cromosoma Y (cromosoma A00), de hace 338 mil años; mucho más antiguo que el más antiguos "Homo sapiens" conocido en el registro fósil, (de 200 mil años aprox.). Los investigadores descubrieron que este cromosoma era similar a un tipo de cromosoma Y presentes en baja frecuencia en los Mbo (una pequeña población que vive en el oeste de Camerún, en la África subsahariana). Este antiguo linaje del cromosoma Y, pertenecería a un homínido anterior a nosotros, seguramente algún "Homo heidelbergensis". Se postula que este cromosoma estaría presente en algunos humanos modernos producto de un proceso de introgresión producido en África entre un ancestro "Homo sapiens moderno" con un "Homo sapiens arcaico".

Así como los cromosomas-Y se heredan por vía paterna, las mitocondrias se heredan por vía materna. 
Por lo tanto es válido aplicar los mismos principios con estos.
El ancestro común más cercano por vía materna ha sido apodado Eva mitocondrial.

Sin embargo es muy importante aclarar que, de acuerdo con lo que el conocimiento actual es capaz de explicar, los Adán y Eva científicos no habrían vivido ni en la misma época ni en la misma región dentro de África. Por el contrario, según la diversidad genética, se estima que mientras la existencia del Adán cromosómico habría tenido lugar en el África centro-occidental, Eva habría vivido en el África sudoriental.

Por otra parte, un equipo de investigación de la Universidad de Stanford secuenció los cromosomas Y de 69 hombres de todo el mundo y descubrieron cerca de 9000 hasta ahora desconocidas variaciones de la secuencia de ADN. Utilizaron estas variaciones para crear un reloj molecular más confiable y encontraron que Adán vivió hace entre 120 000 y 156 000 años. Un análisis comparativo de secuencias de ADN mitocondrial de los mismos hombres sugirió que Eva vivió hace entre 99 000 y 148 000 años. Lo que indica que el Adán Cromosómico existió antes que la Eva mitocondrial.

El árbol filogenético del Adán cromosómico se organiza en grupos de haplotipos (haplogrupos) del siguiente modo:





</doc>
<doc id="9046" url="https://es.wikipedia.org/wiki?curid=9046" title="Retroviridae">
Retroviridae

Retroviridae es una familia de virus que comprende los retrovirus. Son virus con genoma de ARN monocatenario de polaridad positiva y se replican de manera inusual a través de una forma intermedia de ADN bicatenario. Este proceso se lleva a cabo mediante una enzima: la retrotranscriptasa o transcriptasa inversa, que dirige la síntesis de ADN a través de ARN y posee una importancia extraordinaria en la manipulación genética. Una vez que se ha pasado de ARN monocatenario a ADN, se inserta dentro del ADN propio de la célula infectada donde se comporta como un gen más (véase Ciclo reproductivo de los virus). Por tanto, se incluyen en el Grupo VI de la Clasificación de Baltimore.

Los retrovirus son responsables de muchas enfermedades, incluyendo algunos cánceres y el sida (VIH). Existen diversos grupos de investigación que han intentado modificar genéticamente los retrovirus para usarlos en terapia génica como vectores, pero se han encontrado con diversos problemas.

La familia incluye los siguientes géneros:


El genoma del virus toma la forma de un ARNm de polaridad positiva, incluida la cap 5' y la poly-A 3' dentro del virión. Una vez dentro de la célula del huésped, la cadena de ARN se somete a la transcripción inversa en el citosol y es integrado en el genoma del huésped, momento en que el ADN retroviral se denomina provirus.

En el caso del VIH, el genoma consta de dos moléculas de ARN de cadena simple y polaridad positiva. Las moléculas de ARN están físicamente unidas mediante puentes de hidrógeno en sus extremos 5', lo que hace que sea difícil la encapsidación de más de 2 moléculas en un provirus. 

La organización genómica es siempre la misma, 5'-Gag-Pol-env-3', y además dependiendo del tipo de retrovirus, hay genes accesorios que se solapan con los genes principales.

A pesar de la inmensa variabilidad entre los distintos tipos de retrovirus, podemos decir que la partícula viral se compone de: 


El ciclo de replicación comprende varias etapas comunes a todos los retrovirus. En una fase inicial o temprana, el virus se une a receptores específicos de la célula gracias a la glicoproteína de superficie. Las membranas vírica y celular se fusionan y la cápside viral entra en la célula. 

Las enzimas víricas permanecen asociadas al ARN genómico formando una complejo nucleoproteico. La síntesis de ADN vírico, incluyendo las LTR (Long Terminal Repeat;Repetición Terminal Larga) , se produce en el citoplasma a través de la Transcriptasa reversa RT. La actividad ARNasa H de la RT degrada la hebra de ARN y casi simultáneamente emplea la del ADNc como molde para sintetizar una segunda hebra de ADN, convirtiéndolo en bicatenario. Este permanece unido al complejo nucleoproteico, con el que pasa al núcleo celular y, mediante la Integrasa (IN), el ADN viral se integra (provirus) en el genoma celular, donde puede permanecer por un tiempo indefinido (en ocasiones luego de esta fase, puede pasar a transformarse en un retrovirus endógeno si logra infectar una célula germinal). 

Luego viene la fase de trascripción de los genes, originando ARN que sirven como genoma del nuevo virión y ARN mensajeros para las poliproteínas. Las proteínas de gag y pol se asocian con el ARN viral formando un “core” intracelular, mientras que las proteínas de env se insertan en la membrana plasmática de la célula.

Tras el ensamblaje, se produce la salida de la célula por gemación, durante la cual el virus adquiere la doble capa lipídica de su envoltura. Estudios recientes sugieren que la salida de la partícula viral se produce en lugares determinados de la célula. Parece ser que Gag migraría hacia zonas de la membrana ricas en determinados lípidos; son las llamadas balsas o raft. Las proteínas asociadas a estos raft que se incorporan a la
envuelta del virus desempeñarían también un papel importante en la replicación viral.

Finalmente se produce la maduración de las partículas gemadas mediante el procesamiento de las poliproteínas por la Proteasa (PR) viral que corta las poliproteínas precursoras. Los principales productos son las proteínas de la matriz (p17), cápside (p24), nucleocápside (p7) y la p6 (proteína). La proteasa es solamente activa dentro del virión. La maduración del VIH constituye un mecanismo importante para el diseño de
antirretrovirales.

Existen 5 retrovirus humanos identificados: el virus de la inmunodeficiencia humana de tipo 1 (VIH-1), el de tipo 2 (VIH-2) y los virus linfotrópicos de células T humanos de tipo I y II (HTLV-I y HTLV-II). Todos se hospedan en los linfocitos T. Los virus de la inmunodeficiencia humana producen la lisis de las células que infectan provocando una severa inmunodepresión. Los virus HTLVI/
II producen la inmortalización de los linfocitos infectados, generando una replicación descontrolada de los mismos, y por lo tanto una linfoproliferación.

El síndrome de inmunodeficiencia adquirida o sida, es la expresión final de la infección
por el VIH. La infección por este virus ocasiona la destrucción del sistema inmunitario
además de manifestaciones neurológicas y tumorales. Estas manifestaciones clínicas se
deben al tropismo tanto macrofágico como linfocitario del virus. Presenta una
preferencia para infectar a linfocitos TCD4+, en los que la replicación es activa y muy
agresiva, lo que provoca como característica de la infección una profunda
inmunosupresión.
La fisiopatología del sida es un proceso complejo, donde existen implicados
mecanismos patogénicos tan diferentes que algunos hasta hoy no han sido
completamente comprendidos. Los principales mecanismos de transmisión de la
infección por VIH son sexual, parenteral y sanguínea.

Al igual que el VIH-1, el VIH-2 es un lentivirus. Su genoma está compuesto por 2
cadenas simples de ARN de polaridad positiva, y también contiene la enzima RT, que
permite la integración del material genético del virus, como forma de provirus en el
genoma de la célula que infecta, que es generalmente los linfocitos T CD4+. Comparte
con el VIH-1 un 40-50% de homología genética, lo que hace necesario disponer de
técnicas de biología molecular específicas para diagnosticar la infección por VIH-2.
La distribución geográfica del VIH-2 está restringida prácticamente al continente
africano y parece ser que la patología producida es mucho más leve y lenta que la
causada por el VIH-1. El mecanismo de transmisión es igual que el VIH-1.

El virus linfotrópico de células T humano es un retrovirus que pertenece a la subfamilia Oncovirinae. 

El HTLV-I fue el primer retrovirus oncógeno humano conocido. Puede provocar una hemopatía maligna denominada leucemia/linfoma de células T del adulto (ATL) y también una mielopatía subaguda denominada Paraparesia Espástica Tropical (PET) o mielopatía asociada al HTLV-I (HAM).

El HTLV-II es un virus que no tiene una patología claramente definida, aunque se lo ha asociado con diversos síndromes neurológicos y mielopatías subagudas.

El material genético está formado por 2 moléculas de ARN de cadena simple y polaridad positiva. El HTLV-I tiene como diana principal los linfocitos TCD4+ y el HTLV-II los linfocitos TCD8+.

El HTLV, una vez que ha infectado a la célula, puede permanecer latente integrado en forma de provirus o comenzar a replicarse. Se cree que el principal mecanismo de trasmisión de la infección por HTLV es a partir de mitosis de las células que infecta. Esta expansión clonal da lugar a lo que se denomina carga proviral. 
El HTLV necesita el contacto célula-célula para producir la infección;los principales mecanismos de transmisión de la infección por virus HTLV son por vía sexual, vía parenteral y vía vertical.

El XMRV o Xenotropic Murine Retrovirus es un virus del tipo gamma-retrovirus. Se cree que es causante de cáncer de próstata y Síndrome de fatiga crónica.

La terapia génica consiste en insertar copias funcionales de un gen defectuoso en el genoma de un individuo. Se han considerado los retrovirus como vectores génicos pero existen muchos problemas en su utilización. 




</doc>
<doc id="9049" url="https://es.wikipedia.org/wiki?curid=9049" title="Huatusco">
Huatusco

Huatusco es uno de los 212 municipios que conforman el estado mexicano de Veracruz de Ignacio de la Llave.
El municipio de Huatusco se localiza en la región montañosa central del estado de Veracruz.La distancia aproximada por carretera desde la ciudad de Huatusco a las ciudades de Xalapa y de Veracruz es de 88 km y 120 km respectivamente.

Durante la Independencia de México, en el municipio y principalmente en la ciudad de Huatusco, se desarrollaron numerosos levantamientos y rebeliones en contra del ejército realista, apostado tiempo atrás en la zona y principalmente en la ciudad. Estas rebeliones fueron punto clave para que el movimiento insurgente en el estado de Veracruz comenzara a desarrollarse. Por esta razón, el congreso de Chilpancingo, nombró a Huatusco en 1813, ""Capital de la insurgencia en el estado de Veracruz"".

Por estas tierras pasaron personajes históricos como Hernán Cortés, el virrey de la Nueva España José de Iturrigaray, el general y primer presidente del México independiente Guadalupe Victoria, el brigadier Nicolás Bravo, el benemérito de las Américas Benito Juárez y el emperador Maximiliano de Habsburgo, entre otros más.

La región de Huatusco es la mayor productora de café en todo el estado de Veracruz. Debido a su privilegiada situación geográfica y los factores como el suelo, tipo de clima y la altitud, hacen que el café huatusqueño presente las características exactas de un buen café de altura. Huatusco pertenece a la denominada "Ruta Veracruzana del Café"

Actualmente, la ciudad de Huatusco representa un polo de desarrollo importante para la región, ya que brinda diferentes servicios a municipios como Coscomatepec, Totutla, Sochiapa, Tlaltetela, Tlacotepec, Calcahualco, Alpatlahuac, Tepatlaxco, Tenampa, Comapa, Zentla, Ixhuatlán del Café, Puente Nacional, e inclusive municipios del estado de puebla como Chichiquila.

En diciembre de 2013, se inauguró el primer museo de la ciudad, localizado en los bajos del palacio municipal, este espacio da albergue a piezas arqueológicas, documentos y cuadros del siglo XIX y XX, mismos que dan a conocer parte de la historia de este municipio.

Huatusco del náhuatl (Cuauhtochco):"De cuahuitl y tochtli; "Cuauh-toch-co", en el conejo de palo, y también en el bosque de los conejos," según Orozco y Berra. El jeroglífico consta del signo árbol, y en su tronco un tochtli, que no tiene la apariencia de un conejo, sino de un cuadrúpedo carnicero, el ocotochtli, gato montés.

Los signos de la escritura son fonéticos; la significación de la palabra se deduce de que el conejo de árbol es el cuadrúpedo que trepa a sus ramas, el ocotochtli.

Por lo tanto el significado más certero para la voz nahuatl "Cuauhtochco" es "En el lugar de los gatos monteses", (Aguirre, 1940)

El escudo tiene su origen en el código mendocino, el follaje hace notar la abundancia de la vegetación de la región, en el centro, con nubes blancas, representan la abundancia de lluvia, el azul significa el tiempo esplendoroso, un conejo que significa fertilidad, sobre un montículo de pasto verde representa el teocalli del lugar, en la parte superior del conejo está situado el árbol de la vida con tres ramas que indican la religión, las artes y las ciencias, las otras ramas más pequeñas indican las virtudes.

El territorio que actualmente ocupa el municipio de Huatusco data del año 1327, cuando un grupo de Tlaxcaltecas fundaron el “Gran Señorío de Cuauhtochco” (del náhuatl “En el lugar de los conejos) y con él varios poblados entre ellos Otlaquiquistla (lengua náhuatl que significa “Lugar de las trompetas de Bambú”) que siglos después sería la actual ciudad de Huatusco de Chicuellar (Aguirre, 1940). La región fue ocupada sucesivamente por dos grandes ramas de la familia nahuatlaca. De los primeros no se tiene un registro certero, pero se cree que fueron los toltecas, de los segundos, se sabe que fueron los teochichimecas, los mismos que fundaron la república de Tlaxcallan mejor conocidos como tlaxcaltecas. La población con la que contaba el mencionado señorío no superaba los cinco mil habitantes. Para el año de 1457, el mexica Moctezuma Ilhuicamina conquistó la región, incluido el pueblo de Otlaquiquistla y Tototlán.

Algunos autores refieren, que la cabecera del “Señorío de Cuauhtochco” se encontraba en el territorio que actualmente ocupa el municipio de Carrillo Puerto mismo que a la llegada y conquista de los españoles, el señorío se convierte en corregimiento y se denomina “Santiago Cuauhtochco”. Con el paso de los años, dicho corregimiento pierde importancia y la cabecera del mismo se pasa al pueblo de Otlaquiquistla que con la llegada de los españoles el pueblo ya gozaba de gran importancia, esto debido a que se localizaba en la importante ruta comercial entre el golfo y el recién descubierto Tenochtitlán. En ese tiempo se le conocía como “San Antonio Otlaquiquistla del corregimiento de Cuauhtochco”. Otros autores como el importante antropólogo Gonzalo Aguirre Beltrán en su trabajo titulado: "El señorío de Cuauhtochco; Luchas Agrarias Durante el Virreinato" de 1940, no hace mención de tal suceso. Por su parte refiere que el señorío de Cuauhtochco se encontraba estructurado en dos grandes e importantes centro de población: Cuauhtochco (lugar situado a dos kilómetros al sur del cerro de Acatepec (cerro de los carrizos) un volcán de cráter-lago extinto y del pueblo de Otlaquiquistla, hoy la actual ciudad de Huatusco. El otro centro de población era Tototlán (hoy Totutla). Existían en el señorío además pueblos de importancia menor tal es el caso de Comapan (Comapa), Ohuapan, Acolcuautla y Cuitlatepec (Tenampa).

A partir de 1670 la cabecera del corregimiento se le conoce simplemente como “San Antonio Cuauhtochco o San Antonio Huatusco” perdiendo así el nombre original de la población. El corregimiento, fue durante todo el siglo XVI el principal centro de población de la zona, y abarcaba gran parte de los actuales municipios de la zona centro del estado desde la región de Córdoba-Orizaba hasta la zona del Puerto de Veracruz. En 1778, al cambiar el sistema administrativo del país se constituyen las intendencias divididas en subdelegaciones, Huatusco quedó formando parte de la provincia de Córdoba, perteneciente a la intendencia de Veracruz (Aguirre, 1940; Ramos, 1997).
El General Guadalupe Victoria aquí formó el célebre batallón de la "República".
Cuando Guadalupe Victoria llegó a ser Presidente de la República y sabedor de que el H. Ayuntamiento de Huatusco quería darle a la población de su nombre, el Presidente les envió la siguiente carta:

En épocas del Presidente Benito Juárez, el entonces caudillo Porfirio Díaz se había revelado contra Juárez y con la bandera de la no reelección incentivaba el alzamiento en diversos puntos del país. También los conservadores y el clero estaban en contra de Juárez y veían positivos los alzamientos. En los poblados veracruzanos de Tierra Quemada, Huatusco y Perote hubo varios levantamientos contra el gobierno de Juárez durante los años de 1868 y 1869.

El decreto del 12 de diciembre de 1830, número 187, concedió el título de Villa al pueblo San Antonio Huatusco y el decreto número 25 del 21 de junio de 1880 le dio a la Villa de Huatusco el título de Ciudad.

A finales del siglo XIX, llegaron oleadas de inmigrantes Italianos a este poblado procedentes de la región de Trento y del Veneto, en especial al municipio de Zentla donde se asentaron para empezar a trabajar las labores del campo y se fundó la colonia “Manuel González” con familias de inmigrantes italianos

Hoy en día, muchos italo-mexicanos siguen residiendo en las ciudades fundadas por sus antepasados. Estas ciudades se encuentran en los estados de Veracruz, Puebla y San Luis Potosí. El italo-mexicano se ha originado en la experiencia común de la migración desde Italia a finales de 1800, un período caracterizado por una más general diáspora italiana en las Américas (en virtud de las presiones de la transformación económica y el proceso de unificación italiana en un Estado-nación en 1861), y el establecimiento de comunidades, principalmente en el centro y al occidente de México.

También se asentaron un pequeño grupo de Alemanes entre finales del siglo XIX y principios del siglo XX atraídos por la bonanza del café y sus beneficios cafetaleros.

El 31 de mayo de 1847, debido a la Intervención estadounidense en México y su avance en el estado de Veracruz y a la ciudad capital Xalapa, la Legislatura Local mediante el decreto número 17, solicitó que el Gobierno Estatal, en ese entonces encabezado por el gobernador Juan Soto Ramos debía trasladarse a la ciudad de Huatusco. Los norteamericanos permanecieron en Xalapa hasta junio de 1847, fecha en que el Ayuntamiento solicitó el retorno de los poderes del Estado a esta ciudad, para organizar la administración pública.

La sociedad Huatusqueña ha estado ligada durante más de un siglo a la cultura del café, uno de los principales motores económicos, desde el siglo XIX cuando existía el "Cantón de Huatusco" que abarcaba ocho municipios, entre ellos el de Huatusco, que fungía como cabecera del mismo; también estaban Axocuapan, Comapa, Sochiapa, Tenampa, Tlacotepec de Mejía, Totutla y Zentla. Hacia 1879 los productores importantes de café en el cantón de Huatusco eran los siguientes: "La cuchilla de Prudencio Solleiro, Xocotla y Actopan de Anastasio Pesado, la Mesa del Señor de Manuel M. Sousa, Cinco de Mayo (Chicoasén) de Joaquín Rincón, El Mirador de Florentino Sartorius", dando una producción anual entre todos de 4352 quintales de café . En 1882 se reportaron tan sólo 33 fincas de café en el cantón, y en 1890 los censos registraron 2270 productores de café, como “propietarios de buenas costumbres en general, trabajadores y progresistas”, según los calificaba el jefe del cantón, Pascual Villarauz. Esto explica el inusitado crecimiento de 3% en la tasa de población, en la primera década del siglo XX.

A fines de siglo, la cafeticultura estaba introducida hasta en los más recónditos lugares del cantón de Huatusco. El café había dado un giro completo a la sociedad huatusqueña que, a partir de las riquezas que este aromático generó, el municipio se convirtió en el eje de la economía del lugar y hasta la fecha sigue moviendo económicamente la región exportando café de alta calidad principalmente a Europa, Estados Unidos y Canadá. Aunque en los últimos años la producción ha bajado considerablemente debido a los precios del mercado actual, recientemente algunos agricultores están optando por otros cultivos como la caña de azúcar, el aguacate, la macadamia o el Bambú.

Huatusco se localiza en la zona montañosa central del estado de Veracruz, entre los paralelos 19° 04’ y 19° 13’ de latitud norte y los meridianos 96° 41’ y 97° 04’ de longitud oeste, a una altitud que varía de entre los 400 msnm hasta los 2,000 msnm.

Huatusco tiene un clima semicálido húmedo con lluvias en verano.

El municipio pertenece a la región hidrológica del Papaloapan y este a su vez a la cuenca del Río Jamapa y del río La Antigua.
El municipio se encuentra regado por una red de pequeños ríos, tributarios del río Jamapa, además de un afluente que pasa por la localidad de Elotepec que aguas abajo forma parte del río de Los pescados.

Dentro de los ríos que por su caudal corren a lo largo del territorio podemos mencionar los siguientes: 




municipal para el servicio del consumo agua potable. 



Huatusco se localiza en las estribaciones de la Sierra madre oriental, debido a esto el municipio presenta una topografía bastante accidentada, observándose elevaciones al sur y al oeste que superan los 1800 msnm y barrancas considerablemente profundas al este.
Geomorfológicamente, la ciudad de Huatusco se localiza sobre una extensa ladera tendida que se prolonga de poniente a oriente delimitada al sur por un sistema de lomeríos (serranía de Ixpila), al norte por la microcuenca del río Citlalapa y al oeste por el sistema montañoso pertenecientes a la sierra madre oriental llamada también ""Sierra alta de Huatusco"".
Dentro del territorio municipal existen cerros aislados importantes entre los cuales destacan los siguientes:

Por otro lado, en el municipio existen barrancas profundas como la de Chavaxtla al este del municipio y al oeste la barranca de Elotepec que sirve además como frontera natural entre Huatusco (Veracruz) y Chichiquila (Puebla).

El municipio de Huatusco se caracteriza por poseer "Bosque Mesófilo de Montaña" en su parte media y alta, este tipo de Bosque, ("también denominado "Bosque de Niebla"") es un tipo de bosque único a nivel mundial por su megadiversidad, su composición florística es 
una mezcla de especies de bosques templados, donde pueden convivir pinos y encinos con especies de bosques tropicales húmedos de tierras bajas. Este bosque es reconocido como uno de los principales centros de endemismo de México, donde se encuentran varios tipos de Encinos, Ixpepes ("Trema micranthum"), ocozotes, hayas ("platanus mexicana"), olmos, nogales, orquídeas, helechos y musgos. 
El Bosque Mesófilo de Montaña se destaca por los servicios ambientales que suministra como son la captación de agua, por la presencia de nubes y Neblina, su contribución al ciclo hidrológico, por proporcionar oxígeno y por la abundancia de las aves, además de que la vegetación protege de la erosión al suelo. En México este tipo de Bosque, representa a penas el 1% de la superficie del país.
La parte baja del municipio, se caracteriza por poseer vegetación secundaria y selva.

En la actualidad existe una fauna compuesta por poblaciones de mamíferos silvestres como: conejos, ardillas, armadillos, mapaches, tlacuaches, zorros tuzas y tejones. Aves como: codorniz, tordos, gavilán, golondrinas; y reptiles como coralillos, nauyacas o palancas (palancas o palancacoatl = víbora que pudre la carne - debido a su veneno necrosante que destruye los tejidos su nombre científico es "Bothrops y perros y gatos asper").

El municipio de Huatusco cuenta con una población según el último censo de INEGI 2010, de 54 561 personas, las cuales 26 216 son hombres y 28 345 son mujeres. Tiene una densidad de población de 270.1 hab/km².

Principales Localidades

Fue compuesto por el profesor de origen michoacano Adalberto Moreno, el 24 de febrero de 1975.

Huatusco es una ciudad rica en cultura, tradiciones y fiestas. Su fiesta patronal es el 13 de junio en honor a San Antonio de Padua. También son importantes las fiestas en honor a la Santa Cruz, a Santa Cecilia y en el mes de diciembre las fiestas y peregrinaciones en honor a la Virgen de Guadalupe, el cual cada 12 de diciembre la gente se reúne y visita su santuario a las afueras de la ciudad, localizado en lo alto del cerro que lleva su nombre, en donde existe una pequeña iglesia, y una efigie monumental de la virgen de Guadalupe de 30 metros, la más alta de México.

También en el mes de noviembre se exhiben productos a la ciudadanía en la plaza de día de muertos o todos santos.Y otras de sus tradiciones que aunque esta no es originaria de aquí y apenas hace unos 9 o 10 años que se celebra Xantolo es ya una tradición entre los huatusqueños.

En el mes de mayo se celebra el carnaval de Huatusco, una fiesta alegre en la que participa la ciudadanía.

Rica en hierbas y flores de la región, la gastronomía huatusqueña es una de las más representativas en el estado de Veracruz. Rica, variada y exótica, esta comida prehispánica, ha formado parte del gran acervo cultural e histórico de los huatusqueños generación, tras generación. 

El tlatonile es un tipo de mole hecho a base de ajonjolí o pipían, chile ancho, chile comapeño y pollo, forma parte del guiso más famoso de Huatusco. Por su sabor y su antigüedad (se tiene registro que se comía desde hace 600 años) es considerado como ""patrimonio intangible de los huatusqueños"". 
El tlatonile de los vocablos náhuatl ""tlatoani" y ""molli"". 
Tlatoani significa rey, príncipe, gobernante o emperador, mientras que molli o mulli, se traducen como salsa o guiso. La historia de este emblemático platillo huatusqueño, tiene sus orígenes en los ritos y costumbres aztecas y es el siguiente:

"Habiendo llegado la novia a la casa del novio, luego ponían a los dos juntos al hogar, la mujer a la mano izquierda del varón... la suegra salía para dar dones a su nuera; la vestía un huipilli y ponía a los pies un cueitl...la suegra del novio daba a su yerno una manta anudada sobre el hombro y poníale un maxtle junto a sus pies. Después de eso, las casamenteras ataban la manta del novio con el huipilli de la novia, la suegra de la novia iba y lavaba la boca de su nuera y ponía tamales en un plato de madera junto a ella y también un plato de molli que se llamaba "tlatonilli", luego le daba de comer a la novia cuatro bocados y luego al novio que era lo primero que comían". En palabras resumidas de Fray Bernardino de Sahagún el tlatonile es "el guiso que la suegra de la recién casada ofrecía durante el ceremonial del matrimonio".

Otro platillo representativo de los huatusqueños y que se acompaña con el tlatonile son los tamales de cozamalo, los cuales son tamales redondos, pequeños y delgados hechos con masa de maíz, manteca y sal, envueltos en una hoja de forma alargada (hojas de cozamalo) y que le transfiere al alimento su delicioso sabor y aroma.

Las chicatanas son al igual que el tlatonile, el platillo típico de los huatusqueños. Las chicatanas son hormigas, que tienen un extraordinario poder nutritivo por su alto contenido en proteínas, donde su preparación básica es en salsa. Esta es una de las recetas más comunes de este suculento platillo. Para su elaboración primero se limpia la chicatana quitándole las patitas, las alas y la cabeza, dejándole el tronco del cuerpo; se lavan y posteriormente se ponen en el comal a dorar a fuego lento y constante movimiento ya que están doradas se muelen las chicatanas en molcajete, consecutivamente se asan los chiles una vez que estén listos los juntas en el molcajete con las chicatanas y un diente de ajo pelado y sal al gusto. Otra forma de comer son tostadas y con las chicatanas se pueden hacer desde tamales, carne de cerdo en salsa de chicatanas, hasta nieves elaboradas con este singular insecto. Las chicatanas son consideradas un alimento exótico y afrodisíaco, su costo a mediados de año en los mercados huatusqueños llega a sobre pasar los $400 pesos por kilogramo. Las chicatanas son el platillo preferido por turistas extranjeros que visitan Huatusco y es tal la fama que tiene este insecto en el municipio, que los huatusqueños también se les conoce como "chicataneros". 

Otros platillo huatusqueño es la sopa de flor de calabaza la cual consiste en una mezcla de flor de calabaza, chile poblano, queso, epazote y tortilla frita, entre otros ingredientes. Los tepejilotes capeados también forman parte de la mesa huatusqueña, los cuales se preparan capeados y con ingredientes como jitomates, ajo, cebolla, orégano, consomé en polvo, huevo y harina, siendo uno de los platillos menos sofisticados pero no por ello menos importantes de la gastronomía propia de la ciudad. 
Para finalizar, Huatusco es una de las regiones con mayor tradición cafetalera del país, esto debido a la calidad y exquisito sabor que logran sus condiciones climáticas y de altitud, es por eso que en su visita, por ningún motivo debe olvidar tomar el delicioso café huatusqueño, uno de los más famosos y sabrosos de Veracruz y del país.

La parroquia de San Antonio de Padua se localiza en el centro de la ciudad de Huatusco, tiene una altura aproximada de 35 metros de alto por 25 metros de ancho y 70 metros de largo, posee dos torres, un atrio, estacionamiento, un sótano en donde existen salones para eventos religiosos, cuartos y la oficina parroquial. El templo cuenta con 2 niveles y el piso interior está recubierto de mármol importado de Italia, tiene una entrada principal, dos entradas laterales y un cupo aproximado para 3500 personas. La superficie total del templo es de aproximadamente 5000m². La parroquia de San Antonio de Padua, es el templo católico más alto de Veracruz y uno de los más grandes del país.

La Construcción del Templo

El cura Enrique S. Trejo y Domínguez convocó a los vecinos distinguidos de la localidad, así como a las asociaciones católicas, con el fin de informarles de la necesidades de construir un nuevo templo. Ingenieros de la época habían revisado las paredes del templo y determinaron que había que derrumbarlas en su totalidad y que, con el permiso de la mitra, se lanzara una convocatoria para que concursaran varios arquitectos.

Entre quienes presentaron proyectos para el templo se encontraba el arquitecto José Villagrán García que había sido invitado por la familia de la señorita Salustia Ruíz Bello. El plano presentado era, para su época modernista, lo que causó un enorme revuelo entre los feligreses de la parroquia. La maqueta de esté fue aprobada por las autoridades de la iglesia. Se procedió a demoler el viejo templo. El peso de este proyecto recayó en el cura Enrique Trejo y Domínguez.

En los primeros años de la construcción del templo no existía una quebradora de piedra, siendo necesaria grava de 7 cm; la cual se obtuvo con auxilio de un marro, durante 7 años, lapso en el que la mano de obra se pagó por metro cúbico o por faena, a fin de realizar esta hazaña. La varilla y el cemento se trajeron de la ciudad de México por ferrocarril a la estación de Camarón de Veracruz y desde este lugar se acarreó el material hasta la carretera Fortín–Huatusco.

Durante los años de la construcción siempre estuvo supervisada por el arquitecto Villagran García. El iniciador e impulsor de la construcción del templo, el señor cura don Enrique S Trejo y Domínguez no llegó a verla totalmente terminada, pues falleció el 21 de noviembre de 1973. Continuando con los trabajos el señor cura don Luis Palomo, cuando el templo aún estaba inconcluso, faltaba el segundo piso y el coro.

La imagen de San Antonio elevándose es obra del escultor de origen alemán Herbert Hofmann Ysenburg. El artista realizó con maestría y devoción la escultura de nueve metros de altura, hoy por hoy son reconocidas en el mundo entero como una joya del arte moderno las piezas de Hofmann.

Los retablos son del escultor Luis Ortiz Monasterio, estos son: Virgen de Guadalupe (65 cm), Espíritu Santo (66 cm) y Sagrado Corazón (70 cm). Los doce vitrales son obra de la diseñadora Kitzia Hofmann. 

Teatro del "Porfiriato". Su construcción comenzó en el año de 1882 por órdenes del alcalde municipal Prudencio Solleiro, avecindado huatusqueño de origen español, el cual siempre había anhelado la construcción de un teatro idéntico al que se encontraba en su ciudad natal, el Puerto de Vigo, España. Su inauguración fue el 1 de enero de 1890 y el arquitecto encargado de su edificación fue el Arq. José Apolonio Téllez Girón.

El Teatro Solleiro posee una superficie de 875.76 m² y por sus pasillos desfilaron figuras de la talla de Esperanza Iris, Virginia Fábregas, Agustín Lara, los cómicos Jesús Martínez «Palillo» y Joaquín Pardavé, María Conesa, Fernando Soler y sus hermanos, conocidos como los hermanos Soler, Lupe Inclán, Chequelo Vázquez, Eugenio Luna y Dagoberto Gullaumin.


El templo de Santa Cecilia se localiza en el centro de la ciudad de Huatusco, es un templo construido por los españoles que al llegar a estas tierras terminaron por evangelizar a los antiguos pobladores inculcándoles la religión católica. El templo - el cual por diferentes causas no se terminó en su totalidad - fue edificado sobre un antiguo Teocalli azteca. Sus muros de aproximadamente 15 metros de alto por 50 cm de ancho están hechos a base de rocas, lajas basálticas y en algunas partes de ladrillos. Al frente en su costado izquierdo, se erige una monumental torre de 30 metros de alto, la construcción de dicha torre comenzó en el año de 1880 a cargo del ingeniero de descendencia italiana Felipe Spota. La torre de estilo europeo y diseñada a partir de modelos de torres de la ciudad de Venecia en Italia se terminó de construir en 1898, en su parte superior posee un reloj manual traído de Suiza el cual comenzó a funcionar el 1 de enero de 1900. La Torre de Santa Cecilia forma parte del patrimonio histórico-cultural e icono de identidad para todos los huatusqueños.

Como un dato complementario, en el interior del templo de Santa Cecilia se encuentra resguardado un "Teponaxtli" (instrumento musical antiguo hecho a base de madera), cuenta la leyenda, que fue entregado por Santa Cecilia a una indígena azteca quien le pidió que en su honor se edificara un templo el cual defendería y cuidaría de los maleficios del demonio.


La construcción de lo que ahora es el edificio que alberga el cabildo huatusqueño, comenzó en el año de 1828 por oórdenes del señor Alberto Pesado, era una casa particular que años después fue adquirida por el ayuntamiento de Huatusco. El edificio ha tenido varias modificaciones como lo es la destrucción de varias columnas y una fuente ubicadas en el interior, para la creación del reclusorio municipal en el año de 1973, así como la remodelación y ampliación del inmueble anexándole un segundo piso. 

El palacio municipal sufrió severos daños cuando un terremoto en 1937 semidestruyó parte del edificio, y en el año de 1939 se incendió. Actualmente se encuentra en funcionamiento y aún no se han terminado las obras de ampliación y remodelación en la parte trasera.
Otros monumentos históricos importantes son:

Dentro de la Ciudad

Es un paseo público, cuya construcción finalizó el 16 de septiembre de 1904, se encuentra al oeste de la ciudad y posee una superficie de 40,408 m². En su interior se encuentra una cancha de fútbol y una pista de atletismo, canchas de basquetbol y fútbol rápido, juegos infantiles, y extensas jardineras. Es un área natural importante para el municipio ya que cuenta con un sin número de especies arbóreas. Es idóneo para dar un paseo o simplemente sentarse en una de sus muchas bancas y disfrutar de la tranquilidad del lugar.


Ubicado en el centro de la ciudad, su construcción comenzó en el año de 1880 por órdenes del jefe político del cantón Fernando Merino y el alcalde municipal, en ese entonces Jesús Paéz Vela e inaugurado 18 años después en septiembre de 1898. Cuenta con un kiosco, bancas y jardineras con árboles como palmeras, ficus y variadas plantas de ornato. Posee una superficie de aproximadamente 3,690 m². Es el parque principal y plaza de armas de la ciudad de Huatusco. Por su estilo francés, fue considerado uno de los parque más bellos del estado de Veracruz. Como dato importante, existe una loza de basalto con la siguiente leyenda: "Los abnegados hijos de Huatusco y los amantes de su cultura dieron cima a esta importante mejora. 1898."

Fuera de la Ciudad
Localizado al norte de la ciudad, este cerro de aproximadamente 300 metros de altura posee en su cima una capilla en honor a la virgen de Guadalupe, y una efigie monumental de 30 metros de alto de la virgen, la más alta de México. Desde la cima se puede apreciar toda la ciudad y sus alrededores, es el lugar idóneo para salir del ruido de la ciudad y entrar en contacto con la naturaleza. Para llegar a la cima, se sube a pie o en automóvil. Se localiza a 30 minutos del centro de la ciudad.


Antiguo volcán monogenético extinto, localizado a 30 minutos del centro de la ciudad y al oeste de la misma. Tiene una altura de aproximadamente 350 metros y es considerado un símbolo para la ciudad, debido a las numerosas leyendas y mitos en el que se encuentra envuelto. En sus laderas existen pequeñas grutas aún sin explorar. Desde la cima, se aprecia todo la ciudad, y por las noches despejadas se logra observar el puerto de Veracruz. Para llegar a la cima, es a pie o en camionetas 4x4 debido a lo escabroso e inclinado del camino. Es un lugar idóneo para descansar y relajarse observando la naturaleza o disfrutar de una larga caminata. 

Las Cañadas es un Centro de Agroecología y Permacultura privada, en donde se encuentra una de las últimas islas del bosque mesófilo de montaña o bosque de niebla de la zona central de Veracruz. Construido en los años 90's, actualmente se realizan en el actividades de ecoturismo y educación ambiental tanto para escuelas como para habitantes locales y turistas nacionales y extranjeros. En lo que se refiere al "ecoturismo" se pueden realizar diferentes actividades como las caminatas, observación de aves, ciclismo de montaña, rutas a caballo, baño de temazcal, talleres de elaboración de quesos, herbolaria, alfarería etc. Todo esto en un área aproximada de 306 hectáreas. La reserva se localiza a tan solo 15 minutos de la ciudad y se llega en automóvil.

Localizados en la localidad de Capulapa estas bellezas naturales se localizan a 30 min. del centro de la ciudad. El Boquerón es una cavidad de roca caliza en donde las aguas del río Jamapa se sumergen y desaparecen para luego salir varios kilómetros al sur, en el municipio de Atoyac. El Sótano, por su parte, es una enorme cavidad geológica constituida mayoritariamente de roca calcárea, la entrada posee aproximadamente 20 metros de alto. Ambos se encuentran inmersos entre la vegetación de bosque mesófilo y son lugares idóneos para practicar el ecoturismo y deportes como la caminata, la espeleología, escalada y rapel. El primer viernes de marzo, en el Sótano, se realiza el "Xochitlalli", un ritual indígena que se realiza en cuevas como una forma de agradecer a la madre naturaleza todo lo que ha brindado a los seres vivos. 
A partir de diciembre del año 2013, mediante un concurso por parte de la televisora mexicana Tv Azteca y la secretaría de turismo del estado de Veracruz, El Boquerón y el Sótano de Capulapa, fueron elegidas como unas, de las "20 Bellezas de Veracruz con orgullo jarocho", por representar mejor la belleza natural y paisajista del municipio y la región de las Altas Montañas en el centro de Veracruz.
Fundado en 1978 luego de que cinco pobladores dijeron haber tenido "visiones marianas", en las que se les solicitaba abrir una casa de oración. El Jardín de María se localiza en la localidad de Tenejapa, a tan solo 15 minutos del centro de la ciudad de Huatusco, a dos horas aproximadamente de las ciudades de Veracruz y de Jalapa, en el km. 3 de la carretera Huatusco - Fortín;.En el lugar se encuentra una capilla en honor a la virgen y extensas áreas ajardinadas, propias para el esparcimiento y recreación familiar.
Otros sitios naturales de interés son:

El 13 de junio del año del 2010, en el marco de las festividades en honor a san Antonio de Padua, en la "Feria del Tlatonile y la Chicatana" Huatusco rompió el récord Guiness con la cazuela de tlatonile más grande del mundo. Cabe destacar que para la elaboración de este, el platillo típico de los huatusqueños, se necesitaron varias toneladas de la pasta de tlatonile, 5 mil piezas de pollo, un "tortimovil" que produjo mil 200 kilos de tortilla, así como 150 garrafones de agua purificada para elaborar agua de horchata y de Jamaica. Es así como el municipio se encuentra inscrito en el libro de récord más importante del mundo.

Actualmente la cazuela se encuentra exhibida en el teatro Solleiro.




</doc>
<doc id="9050" url="https://es.wikipedia.org/wiki?curid=9050" title="Cartagena">
Cartagena

Cartagena hace referencia a varios artículos:













</doc>
<doc id="9052" url="https://es.wikipedia.org/wiki?curid=9052" title="Cost, insurance and freight">
Cost, insurance and freight

Las siglas CIF (acrónimo del término en inglés "Cost, Insurance and Freight", «Coste, seguro y flete, puerto de destino convenido») se refieren a un incoterm o término de comercio internacional que se utiliza en las operaciones de compraventa, en que el transporte de la mercancía se realiza por barco (mar o vías de navegación interior). Se debe utilizar siempre seguido de un puerto de destino.

Los riesgos de la mercancía los asume el comprador en el país del mismo cuando la mercancía ha llegado al puerto.

El incoterm «CIF, puerto de destino convenido» ha sido uno de los más usados tradicionalmente. Sin embargo, su correcto uso se debe limitar al transporte por barco, ya sea marítimo o fluvial, de carga general; en el caso de los Ro-Ro o movimientos de contenedores internacionales, la carga está contenerizada, se trata más bien de transporte multimodal y el incoterm que debe usarse es CIP. 

Cuando un artículo se tasa CIF significa que el precio de venta incluye el coste de la mercancía, el del transporte así como el seguro marítimo; coincide con el valor en la aduana de importación de la mercancía.

De acuerdo con la Cámara de Comercio Internacional, CCI, el vendedor sólo tiene obligación de contratar una cobertura mínima, equivalente a las cláusulas "C" del "«Institute of London UnderWriters»". Los compradores deberán normalmente insistir en una póliza "a todo riesgo" como las incluidas en las cláusulas "A" del mencionado Instituto. La póliza debe cubrir el precio CIF más un 10% en la divisa de transacción del contrato.





</doc>
<doc id="9055" url="https://es.wikipedia.org/wiki?curid=9055" title="Apus apus">
Apus apus

El vencejo común (Apus apus) es una especie de ave apodiforme de la familia Apodidae propia de Eurasia y África. 

El vencejo común es un ave especialmente adaptada para el vuelo, con alas falciformes, cola corta de horquilla poco profunda, boca muy ancha y grande rematada con un pico pequeño, patas muy cortas sin pulgar oponible y garras pequeñas pero de presa extraordinariamente fuerte que le permiten agarrarse a superficies verticales. Su plumaje es negruzco con una pequeña mancha blanquecina o gris clara en la garganta, solo visible a corta distancia. El vencejo común tiene una longitud corporal de 16–17 cm, mientras que su envergadura alar es de 42–48 cm, lo que en vuelo proporciona a sus alas su característica silueta de amplia media luna. 

El vencejo común fue descrito científicamente por Carlos Linneo en 1758 en la décima edición de su obra "Systema naturae", con el nombre de "Hirundo apus", que significa «golondrina sin pies». En 1777 fue trasladado como especie tipo al género "Apus" por Giovanni Antonio Scopoli. Se reconocen dos subespecies:
La etimología de su nombre científico, "Apus apus", proviene del griego antiguo, donde "apous" (άπους) significa «sin pies», en referencia a sus costumbres aéreas. Su nombre en español «vencejo» procede de la corrupción de su antiguo nombre "oncejo", por confusión con la palabra «vencejo» que significa «ligadura, lazo». A su vez oncejo provenía de hoz, en alusión a la forma de su silueta en vuelo. 

Desde los mismos orígenes de la zoología se sospechaba lo que a finales de la década de 1960 se constató: que los vencejos pasan la mayor parte de su vida en el aire: comen, duermen y copulan volando. Únicamente se posan para poner los huevos, incubarlos y criar a sus polluelos. Permanecen en vuelo ininterrumpido durante nueve meses al año. Las crías abandonan el nido una mañana volando súbitamente, sin necesidad de aprendizaje previo, y no retornan a él jamás. De noche, estas aves se elevan hasta los 2.000 m de altura y allí duermen, volando. Durante el sueño el aleteo se reduce de los habituales diez movimientos por segundo a tan sólo siete. Debido a sus extraños hábitos aéreos, aún se desconocen muchísimas cosas de la vida de estas aves. Anidan en riscos elevados y paredes verticales desde los que reemprenden el vuelo. A causa de su especial morfología alar y sus cortas patas, si caen al suelo experimentan gran dificultad para remontar el vuelo, y necesitan hacerlo desde un sitio elevado.

Es un ave migratoria que a mediados de la primavera boreal (otoño austral) aparece por casi toda Europa, norte de África y Asia Central, mientras que en el invierno boreal (verano austral) se le encuentra en el sur de África. En el campo, anida gregariamente en taludes pero está especialmente adaptado a los asentamientos humanos. Forma sus nidos bajo cornisas y aleros de edificios y casas. Suele ser fiel a su lugar de anidamiento; vuelve a él y lo reconstruyen cuando hace falta.

El vencejo común se alimenta de minúsculos insectos voladores que atrapa con su amplio pico que mantiene constantemente abierto al volar. También recoge al vuelo los materiales con los que construye el nido.

En cuanto a su reproducción, son de hábito monogámico y presentan un solo periodo de reproducción al año, en las áreas de migración estival. Durante el periodo de nidificación, cada pareja de reproductores hace una sola puesta de 2 a 3 huevos que oscilan entre los 3,2 y 4,2 gramos. El tiempo de incubación es de 19 a 21 días. Las crías abandonan el nido hacia los 35 a 59 días de la eclosión. Los juveniles abandonan el nido volando y de manera definitiva. La madurez reproductiva se alcanza a los dos años de edad.

El desarrollo de los jóvenes nidícolas es diferencial. Los órganos internos (hígado, riñones e intestinos) son los primeros en alcanzar sus pesos definitivos. El sistema esquelético y muscular le siguen en el proceso, y el plumaje de vuelo (remeras y rectrices) es lo que más tarda y marca el final del periodo nidícola. Bajo buenas condiciones alimentarias y de desarrollo, los jóvenes vencejos abandonan el nido con un ligero sobrepeso de 6-7 gramos con respecto a los adultos. Esta reserva les permite afrontar las primeras dificultades de la vida aérea, puesto que el abandono del nido es definitivo.
Es interesante constatar también que el desarrollo de los juveniles en el nido está relacionado en gran medida con la temperatura ambiente. La entrada de frentes fríos o de mal tiempo en las áreas de nidificación disminuye considerablemente la presencia de insectos voladores. Esto conlleva a un alejamiento temporal de los vencejos hacia zonas de mayor oferta o específicamente a los bordes de la zona de baja presión. Este movimiento evasivo se da sobre todo en los individuos de un año, ya que todavía no han nidificado y, por tanto, no están ligados a un emplazamiento fijo; pero incluye también individuos en nidación. Estos movimientos pueden ser de cientos de kilómetros. Los juveniles nidícolas en condiciones normales pueden sobrevivir a la ausencia parental durante cuatro días o más, entrando en un letargo que reduce el ritmo cardíaco de 90 a 20 latidos por minuto y la temperatura corporal de 36-39 °C a cerca de 20 °C.



</doc>
<doc id="9057" url="https://es.wikipedia.org/wiki?curid=9057" title="Demografía de la India">
Demografía de la India

La India es el segundo país más poblado del mundo, después de China. Posee una natalidad anual aproximada de 15 millones. Hay una población de 1.095.351.995 (julio de 2006 est.) y 1.049.700.118 (julio de 2003).

Los diversos orígenes poblacionales y culturales de la población de la India están ligados a aquellos de otros pueblos del subcontinente indio, que incluye a los habitantes de Pakistán, Bangladés, Nepal, Bután y Sri Lanka, así como otros más lejanos. Los orígenes exactos de la mayor parte de los pueblos indios son difíciles de determinar a causa de la gran variedad de poblaciones y culturas que han invadido y han sido asimiladas en el subcontinente. No obstante, según la antropología tradicional, los elementos de tres grandes grupos poblacionales (los caucásicos, los australoides y los asiáticos del este) se pueden encontrar en la India actual. A veces, la geografía y el medio ambiente han animado a mezclarse a olas sucesivas de emigrantes con los pueblos indígenas. Sin embargo, los factores medioambientales e históricos también han favorecido la coexistencia en la India de muchos pueblos con características físicas y culturales distintas; esto también se refleja en la diversidad lingüística de la India; el país tiene 15 grandes idiomas y más de 1.000 dialectos.

Más o menos el 7% del total de la población pertenece a las más de 300 tribus certificadas

En la India se hablan más de (redondeando) 2000 idiomas o dialectos, comprendidos en 15 grandes grupos. La constitución estipula que el hindi (hablado por el 30% de la población) es el idioma oficial del país, mientras el inglés es un idioma asociado a los asuntos administrativos. No obstante, el dominio oficial del hindi es inaceptable para estados como Tamil Nadu en el sur (véase también y Lenguas indoarias).

Distribución:

La constitución también reconoce 18 idiomas regionales oficiales, de los cuales los más extendidos son el bengalí, el támil, el télugu, el marathi, urdu y guyaratí.


Nota: hay 24 idiomas hablados por millones de personas, aparte de otros muchos dialectos ininteligibles para personas de otras zonas de India.

Los grandes grupos religiosos de la India son el hinduismo (83%), el islamismo (11%), el cristianismo (2%) y los sijs (2%). Otras importantes minorías religiosas son budismo, jainismo y parsis. El crecimiento del nacionalismo religioso y del fundamentalismo en la India durante la década de 1980 y 1990 ha hecho crecer las tensiones políticas y sociales en algunas áreas, como por ejemplo las revueltas de 1992 y 1993 en Panyab. Según otras fuentes:


Lista de ciudades de India con más de un millón de habitantes en el censo de 2001.




La India se caracteriza por su diversidad étnica, algunos de los principales grupos étnicos son:

Los drávidas provienen de la vertiente mediterránea y se cree que fueron uno de los primeros
visitantes de la India. Se le atribuye la creación de la civilización del valle del Indo. Con el tiempo se fueron desplazando hacia al sur del país y se establecieron allí permanentemente.

Los mongoloides se establecieron en la región nordeste del país, en las altas cordilleras. Se les puede atribuir el mérito de haber allanado el camino para la población actual de lugares como Sikkim, Ladakh, Assam, Nagaland, Mizoram, Meghalaya, Arunachal Pradesh, Manipur y Tripura. Los mongoloides se caracterizan por una tez amarillenta, piel pálida, ojos pequeños y oblicuos, pómulos altos, estatura media y pelo fino.

Se cree que proceden de África también fueron uno de los primeros pueblos que colonizaron la India.
Aunque no irrumpieron en las zonas más profundas del país, sí se asentaron en lugares como las
islas de Andamán, Nicobar y en algunas partes del sur de la India. Sobrevivieron en ese hábitat original y todavía conservan su modo de vida tradicional.

Los arios eran al parecer los últimos en llegar a la India. Ellos adquirieron la mayoría de
las regiones del norte del país después de alejar a los drávidas hacia el sur. Se
caracterizaban por su aspecto robusto y la piel blanca. Se pueden atribuir a la mayoría de la
población de la India, en el centro y norte del país.

El australoides Proto se acredita haber sentado las bases reales de la civilización de la
India. Llegaron al país inmediatamente después de los negros. Se caracterizan por su piel
morena, cabeza larga, pelo negro y abundante, la frente baja, ojos prominentes, nariz
chata, las mandíbulas anchas, etc. Se establecieron en el centro y la parte
oriental del país.

El Brachycephals occidentales incluye grupos étnicos como Alpinoids, Dinarics, Armenoids,
Parsis y Kodavas. El pueblo se caracteriza por rasgos como la frente amplia, piel morena,
rasgos afilados, etc. ocuparon el lado occidental del país y se puede llamar las bases para el
día de hoy la gente en los estados de Gujarat, Maharashtra, Karnataka y Tamil Nadu.

Leen y escriben con más de 15 años:

La antigua India era una sociedad con un considerable desarrollo educativo. Sus centros educativos atraían a numerosos estudiantes de otros lugares de Asia, sobre todo chinos, que venían a estudiar las enseñanzas de Buddha en algunas de las primeras escuelas como Nalanda, que se fundó en el siglo VI a. C. La India también extendió su influencia educativa al enviar a sus maestros a enseñar a otros lugares de Asia.

Sin embargo, desde el siglo XIII en adelante, primero bajo el control musulmán y después bajo el gobierno británico, la contribución original de los indios a la educación se redujo y con ella la aplicación de métodos educativos más novedosos.

En el siglo XX Gopal Krishna Gokhale, Mahatma Gandhi y Rabindranath Tagore recibieron reconocimiento internacional por las contribuciones educativas a su país.

Gokhale fue uno de los primeros dirigentes nacionalistas y en 1911 introdujo un proyecto de ley en el Parlamento cuya meta era la educación primaria gratuita y obligatoria.

Gandhi, influido por Gokhale, puso en práctica programas básicos de alfabetización y de mejora de las comunidades. En 1901 Tagore, uno de los más grandes poetas de la India moderna, fundó una escuela experimental en Santinikétan, 160 km al norte de Calcuta, que tomaba como modelo el antiguo tapovana indio (‘anacoreta de la selva’); pretendía combinar lo mejor de las culturas occidental e india. En 1921 la escuela se convirtió en la Universidad Visva-Bharati y atrajo a estudiantes de todo el mundo.

La India es un país secular (no clerical), que siempre ha tenido muchas religiones y grupos religiosos. No obstante, la mayoría de los indios actuales son hindúes y esto se refleja en numerosos aspectos de la cultura compartida a lo largo del país. El hinduismo, a lo largo de los siglos, ha absorbido y desarrollado un gran número de filosofías diferentes; desde el filosófico Adweita de Shánkar hasta la devoción del movimiento Bhakti.

La coexistencia de creencias minoritarias con la fe mayoritaria del hinduismo no ha sido siempre pacífica; las tensiones entre los hindúes y los musulmanes, y entre los hindúes y los sijs (a menudo animadas por motivos diferentes a los religiosos) han dado lugar a numerosos y cruentos conflictos. Las demandas del movimiento Rāma-Janma-Bhûmi (lugar de nacimiento del dios Râma) para la construcción de un templo hindú en lo que declaraban que era el lugar de nacimiento de Rāma en Ayodhya acabaron en un conflicto en 1992, con la destrucción por parte de la muchedumbre del Babri Masjid (una mezquita musulmana que según ellos había sido construida después de la destrucción del templo anterior) y han dado lugar a un importante apoyo popular. Este tipo de hechos suponen una gran amenaza para el futuro del secularismo en la India. El fundamentalismo hindú reciente (una contradicción en los términos, pues el hinduismo no tiene fundamentos definidos) es un esfuerzo por fraguar una cultura nacional singular sobre líneas religiosas desde unas tradiciones diversas. Los medios de comunicación y en concreto el amplio acceso a la televisión y a sus poderosos mensajes culturales han facilitado la extensión e inculcación de tales ideas.

La India tiene una economía mixta en la cual tanto el gobierno central como los del estado desempeñan un importante papel como reguladores y planificadores a través de la propiedad de empresas públicas. El compromiso a gran escala del gobierno en la economía comenzó en la década de 1950 como un reflejo del nacionalismo y del socialismo del primer gobierno tras la independencia, dirigido por Sri Pandit Jawaharlal Nehru, con el fin de acelerar el desarrollo económico y el crecimiento para alcanzar así las necesidades de la población de la India que crecía con rapidez. El primero de los planes quinquenales de la India se inició en 1951. Durante las siguientes décadas el estado se ocupó de ciertos sectores clave e hizo grandes inversiones en otros, mientras que el sector privado estaba sujeto a una amplia variedad de controles estatales. Se crearon aranceles y otras barreras para proteger las industrias nacionales y se iniciaron diferentes programas de reforma agraria.

En general los resultados fueron positivos, en especial cuando se comparan con los de otros muchos países en vías de desarrollo. El crecimiento económico, excepto en momentos de fuerte sequía como en 1979 y en 1987, fue constante; entre 1965 y 1980 tuvo una media del 3,6% anual en términos reales (es decir, después de tener en cuenta el crecimiento de la población) y más del 5% anual durante la década de 1980. Por lo general se pudieron mantener bajas la inflación y la deuda nacional. La producción agrícola creció de una manera significativa y el fantasma de hambruna masiva desapareció. Se pusieron las bases de un estado industrial moderno; la India es el noveno mayor productor mundial de acero. En 1997 el producto interior bruto de la India fue de 381.566 millones de dólares (según estimaciones del Banco Mundial), lo cual suponía unos ingresos per cápita de tan sólo 400 dólares. Sin embargo, los niveles de crecimiento eran aún demasiado bajos para tener más que un impacto marginal en los ingresos de la mayoría de los indios. Además, todavía el 21% de la población sufría malnutrición en el periodo 1990-1992, y el acceso a agua limpia y a instalaciones sanitarias aún estaba limitado a una minoría insignificante de la población.

La República de la India está gobernada de acuerdo con lo establecido en la Constitución adoptada en 1949 y enmendada varias veces desde entonces. Incorpora distintas características de los sistemas constitucionales del Reino Unido, Estados Unidos y otras democracias occidentales.

De acuerdo con la Constitución, la India es una república democrática soberana de la Commonwealth. El gobierno tiene una estructura federal y la India es una unión de estados y territorios unidos y administrados de manera centralizada. En la actualidad existen 25 estados y 7 territorios unidos.




</doc>
<doc id="9060" url="https://es.wikipedia.org/wiki?curid=9060" title="Octavio Paz">
Octavio Paz

Octavio Irineo Paz Lozano (Ciudad de México, 31 de marzo de 1914-ib., 19 de abril de 1998) fue un poeta, ensayista, dramaturgo y diplomático mexicano. Obtuvo el en 1990 y el premio Cervantes en 1981. Se le considera uno de los más influyentes escritores del siglo XX y uno de los grandes poetas de todos los tiempos.

Octavio Paz nació el 31 de marzo de 1914, durante la Revolución mexicana. Apenas unos meses después, al unirse su padre al ejercito zapatista junto con Antonio Díaz Soto y Gama, su madre lo llevó a vivir a la casa del abuelo paterno, Ireneo Paz, en Mixcoac, entonces un poblado cercano a la Ciudad de México. Ahí radicaron hasta que Octavio Paz Lozano tuvo que asilarse en Los Ángeles con la representación de Emiliano Zapata ante los Estados Unidos, cargo que mantuvo hasta 1919, año del asesinato de Zapata.

En ese tiempo lo cuidaron su madre Josefina Lozano, su tía Amalia Paz Solórzano y su abuelo paterno, Ireneo Paz (1836-1924), un soldado retirado de las fuerzas de Porfirio Díaz, intelectual liberal y novelista. Su padre, Octavio Paz Solórzano (1883-1935), el menor de siete hermanos, trabajó como escribano y abogado para Emiliano Zapata; estuvo involucrado en la reforma agraria que siguió a la Revolución, fue diputado y colaboró activamente en el movimiento vasconcelista. Todas estas actividades provocaron que el padre se ausentara de casa durante largos periodos.

Su educación se inició en los Estados Unidos, en donde su padre Paz Solórzano, llegó en octubre de 1916 como representante de Zapata.

La estancia en los Estados Unidos, de casi dos años, significó para Octavio Paz el enfrentamiento con la imposibilidad de comunicarse; según recuerda Paz, en Los Ángeles sus padres lo llevaron a un colegio, «y como no hablaba ni una sola palabra de inglés le costó mucho trabajo comunicarse con sus compañeros. El primer día hubo burlas y, claro, una pelea. Regresó a su casa con el traje desgarrado, un ojo semicerrado y la boca rota. A los dos años volví a México y sufrí lo mismo entre mis compatriotas: otra vez burlas y puñetazos».

En 1929 José Vasconcelos se lanza a la gran aventura de buscar la presidencia, apoyado por aspiraciones legítimas de un sector social identificado con la autonomía universitaria. Arrebatado por la huelga estudiantil, Octavio Paz, pese a no haber participado en el movimiento vasconcelista, comulgó con el ideal que lo guiaba, se vio envuelto «en la gran fe vasconcelista, en ese fervor que posteriormente produjo muchas cosas y, entre ellas, una organización de estudiantes pro obrero y campesino de la que a su vez surgieron muchas gentes que con los años se convirtieron al marxismo o al sinarquismo».

Octavio Paz se adhirió al anarquismo sostenido por José Bosch, un joven catalán a quien conocería entonces y que lo introduciría al «pensamiento libertario». Momento también de elecciones, Paz se enfrentaría a la que sería la disyuntiva de su generación: política o violencia, «de ahí la predisposición de algunos a las soluciones extremas: las tendencias al fascismo o al marxismo. Yo me identifiqué con la gente de izquierda».

Asumiendo esta elección, y siendo consecuente con ella, es como a los quince años Octavio Paz se convierte en activista de la fugaz Unión de Estudiantes Pro Obreros y Campesinos, y se inicia en la lectura de Kropotkin, Eliseo Réclus, José Ferrer y Proudhon, antecedentes con los que ingresa a la Escuela Nacional Preparatoria de San Ildefonso, donde habría de encontrar a un viejo conocido de su padre, Antonio Díaz Soto y Gamaliel Santana Banda quien, como profesor y amigo, le haría compartir la idea de que el movimiento zapatista encarnaba el verdadero espíritu de la Revolución.

Deslumbrado, literalmente, por la lectura de "The Waste Land" de T. S. Eliot, traducido por Enrique Munguía como "El páramo", y publicado en la revista "Contemporáneos" en 1930. Por eso, aunque mantuviese en sus actividades un prioritario interés en la poesía, atendía desde la prosa un panorama inevitable: "Literalmente, esta práctica dual fue para mí un juego de reflejos entre poesía y prosa".

Preocupado por confirmarse la existencia de vínculos entre la moral y la poesía, escribió en 1931, a los dieciséis años, el que sería su primer artículo publicado, «Ética del artista», donde, antes de plantearse la pregunta sobre el deber del artista entre lo que denomina arte de tesis o arte puro, descalifica al segundo en razón de la enseñanza de la tradición. Asimilando un lenguaje que recuerda al estilo religioso y, paradójicamente, marxista, encuentra el verdadero valor del arte en su intención, en su sentido, por lo que, los seguidores del arte puro, al carecer de él, se encuentran en una posición aislada y favorecen la idea kantiana del «hombre que pierde toda relación con el mundo».    

La revista "Barandal" apareció en agosto de 1931, dirigida por Rafael López Malo, Salvador Toscano, Arnulfo Martínez Lavalle y Octavio Paz, jóvenes antecedidos, excepto por Salvador Toscano, por cierta celebridad literaria debida a sus padres. Rafael López participó en la revista "Moderna" y, al igual que Miguel D. Martínez Rendón, en el movimiento de los agoristas, aunque era más comentado y conocido por los estudiantes preparatorianos, sobre todo por su poema ""La bestia de oro"". A Octavio Paz Solórzano se le conocía en este círculo como el autor ocasional de narraciones literarias aparecidas en el suplemento dominical del periódico El Universal, además de que Ireneo Paz era el nombre que le daba ya identidad a una calle de Mixcoac.

En medio de encuentros, verdaderas confrontaciones, entre representantes de la generación del Ateneo, especialmente quienes se agruparon en el Ateneo de la Juventud Mexicana, después denominado Ateneo de México, y de los Siete Sabios, sobre las ruinas de un positivismo sobreviviente en crónicas periodísticas, donde se debatían las posibilidades del materialismo histórico, el realismo socialista crecía como la única doctrina viable, a la que debían apegarse todos, o casi todos, los que simpatizaran con las promesas del comunismo. Octavio Paz, cercano a estas ideas, fundó, después de la desaparición de la revista "Barandal", y ya estando inscrito en la Escuela de Derecho de la Universidad Nacional Autónoma de México (UNAM), unos "Cuadernos del Valle de México" que sólo lograrían aparecer por dos números, pero que sirvió para, además de publicar algunos poemas, constatar que el grupo original no tendría la solidez para la continuación de una empresa en común.

En 1933, Octavio Paz publicó el poemario "Luna Silvestre", editado por Miguel N. Lira, que revelaba ya cierta asimilación de temas románticos; como expresa Carlos H. Magis, «los poemas de Luna Silvestre tocan aspectos del espíritu romántico vigentes aún en la poesía moderna: el desprendimiento de la realidad puramente sensible, el misterio de la poesía, la verdad del sueño».

Los siete poemas de "Luna silvestre" no tendrían cabida en la revisión que Paz hiciera posteriormente de su obra, pero revelan a pesar de ello un rigor en la palabra mecida en la sensualidad de sí misma, seducida por la presencia inasible de la mujer, de la naturaleza. El deseo y la pasión andan por los poemas como desprendidos del silencio y de la memoria, se recrean y se recuerdan, se fijan y se desvanecen en el pronunciamiento.

En este momento, prendido a una escritura de tipo intimista, Paz tendrá oportunidad de mostrar sus poemas a Rafael Alberti, quien le señalará una contradicción entre su ideal revolucionario de la poesía y de la política. Llegado a México en 1934, Rafael Alberti representaba la encarnación del poeta de los nuevos tiempos, el advenimiento de un lenguaje socialista congruente con la poesía: su presencia fue un acontecimiento que fascinó sobre todo a los más jóvenes, teniendo en ellos a sus mejores lectores. «Abanderado con el poema "La toma del poder" de Louis Aragón», según recuerda Efraín Huerta, Alberti venía como afiliado del Partido Comunista Español para dictar una serie de conferencias, después de las cuales se reunía con los jóvenes poetas, entre ellos Octavio Paz, quien recuerda que «Una noche, todos los que lo rodeábamos le leímos nuestros poemas... Todos éramos de izquierda pero ya desde entonces sentía cierta desconfianza ante la poesía política y la literatura que después se llamó 'comprometida'. En aquella época, en 1934, Alberti escribía una poesía política –es la época de Consignas–, aquel librito en el que había afirmado que la poesía debía estar al servicio del partido comunista, una posición muy semejante a la de Louis Aragón en Francia. Y cuando yo le enseñé mis poemas a Alberti, él me dijo: 'Bueno, esto no es poesía social' (al contrario, era una poesía intimista –una palabra horrible ésta, intimista, pero eso era: intimista–), 'no es una poesía revolucionaria en el sentido político', dijo Alberti, 'pero Octavio es el único poeta revolucionario entre todos ustedes, porque es el único en el cual hay una tentativa por transformar el lenguaje'».

La confrontación con la fatalidad provoca rebeldías: Octavio Paz, recogido en sí mismo, se enfrenta a sí mismo. La calidad de sus expresiones románticas empieza a cobrar verdadero sentido y empieza a realizar una lectura más atenta de San Juan de la Cruz, de Novalis, de Rilke y de D. H. Lawrence, en quienes encuentra el mismo interés por tender puentes entre la vida y la poesía, entre la realidad y el mito: develamiento de aquel punto de intersección que llamará «comunión». La redacción del diario íntimo que comenzará a expresar, sólo conocerá la publicación hasta cuatro años después, en 1938, bajo el título de "Vigilias: diario de un soñador", en la revista "Taller", cuando hayan sucedido dos hechos trascendentales en su vida, su estancia en Yucatán y la Guerra Civil Española.

A fines de 1936, Octavio Paz escribiría la primera versión del libro de poemas "Raíz del Hombre", que fue publicada en enero del siguiente año. El libro fue saludado por dos reseñas: una crítica y aguda, de Jorge Cuesta, la otra, despiadada e intranquila, de Bernardo Ortiz de Montellano; ambas, publicadas en el número dos de la revista "Letras de México", reflejan la visión de un grupo forjado en los ataques y la incomprensión.

Jorge Cuesta, conocido de Paz desde 1935, le destaca una voluntad para dejarse consumir por su objeto, le reconoce en posesión de un destino y le advierte una filiación con la voces de Ramón López Velarde, Carlos Pellicer, Xavier Villaurrutia y Pablo Neruda. "Raíz del Hombre", en gran medida, despejará el silencio que entornara a "Luna silvestre" y a "¡No pasarán!"; en su relación con los Contemporáneos modificará la visión que había provocado su último poema –considerado por Bernardo Ortiz de Montellano como un texto que no era poesía; será también el poemario que lo dará a conocer frente a Pablo Neruda y que le permitirá en 1937 ser invitado al "II Congreso Internacional de Escritores para la Defensa de la Cultura", celebrado en España–.

Aunque Paz conocía a algunos de los Contemporáneos desde su época de "Barandal", el libro y su recepción le valieron conocerlos a todos ellos juntos. Frente a Xavier Villaurrutia y Jorge Cuesta, Ortiz de Montellano, José y Celestino Gorostiza, Samuel Ramos, Octavio G. Barreda (director de "Letras de México"), Jaime Torres Bodet, Enrique González Rojo y el abate Mendoza, Paz fue, nuevamente cuestionado: «Me interrogaron largamente sobre la contradicción que les parecía advertir entre mis opiniones políticas y mis gustos poéticos».

Plegándose, entonces, sobre su propia angustia, Octavio Paz entendió que sólo con la renuncia podría obtener. Renunciar a los estudios de Derecho, renunciar a la familia, renunciar a la ciudad: acción de desprendimiento que intentaba, o que era símil, de la instauración de una congruencia entre la política y la poética, congruencia vista, pero no sentida. Parte en 1937 hacia Mérida, Yucatán, por un periodo de cuatro meses en los que, junto con Octavio Novaro y Ricardo Cortés Tamayo, participa en la fundación de una escuela secundaria para hijos de trabajadores, en los que escribe para "El Diario del Sureste" -mismo periódico en el que un año antes colaborara Efraín Huerta-, en los que ayuda a organizar un Comité Pro-Democracia Española, en los que escribe el poema «Entre la piedra y la flor».

Hora de palpar la realidad, Octavio Paz se encuentra con una tierra entrañable y extraña, acogedor espacio que se ata por la memoria y se desvanece en el filo del descubrimiento; otra vida, otra presencia late y se respira en medio del calor: la de lo indígena, imagen que en la luz se erige como un signo para ser descifrado o comprendido, que exige una acción, como dice Octavio Paz: «De este encuentro parte, en realidad, todo intento de comprensión, todo esfuerzo por acercarse a lo que verdaderamente mueve a la Península. Aquí lo indígena no significa el caso de una cultura capaz de subvivir, precaria y angustiosamente, frente a lo occidental, sino el de los rasgos perdurables y extraordinariamente vitales de una raza que tiñe e invade con su espíritu la superficial fisonomía blanca de una sociedad».

En junio de 1937, las actividades de Octavio Paz en Yucatán se vieron de pronto interrumpidas por una «carta de invitación al Congreso. La carta, me parece, la firmaban Pablo Neruda y Rafael Alberti». Se trataba del "II Congreso Internacional de Escritores para la Defensa de la Cultura" que había de celebrarse del 4 al 17 de julio de 1937 en Madrid, Barcelona y Valencia, ciudad esta última donde se encontraba la sede del gobierno republicano presidido por Juan Negrín.

Al evento, mecido entre una tímida distancia crítica y una coronación del dogmatismo, asistiría más de un centenar de escritores, entre los que se encontraban André Malraux, Tristan Tzara y Julien Benda, de Francia; M. Koltszov y A. Tolstoi, de Rusia; W. H. Auden y Stephen Spender, de Inglaterra; Malcom Cowley, John Dos Passos y Ernest Hemingway, de Estados Unidos; Alejo Carpentier, Nicolás Guillén y Juan Marinello, de Cuba; César Vallejo, de Perú; González Tuñón, de Argentina; Vicente Huidobro y Pablo Neruda, de Chile; José Bergamín, Antonio Machado y Rafael Alberti, de España; y de parte de México la delegación de la LEAR: José Chávez Morado y Fernando Gamboa –quienes montarían la exposición "Cien años de Grabados Políticos Mexicanos"–, José Mancisidor, Juan de la Cabada, Silvestre Revueltas –quien a su regreso a México realizaría el Homenaje a García Lorca– y la pedagoga Ma. Luisa Vera. Invitado por Neruda y Alberti, asistiría también Carlos Pellicer, conocido por su catolicismo y franco antifascismo; él, al igual que Paz, eran los únicos mexicanos que no pertenecían a la Liga de Escritores y Artistas Revolucionarios, aunque, a diferencia de este, no era mirado con tanta suspicacia y menos con la desaprobación de algunos grupos por su reticencia frente a la doctrina del realismo socialista; Paz viajaba así con la velada acusación de ser trotskista, sin serlo.

El viaje de Octavio Paz a España estaba antecedido por una admiración a los poetas de la generación del 27, conocidos en México sobre todo por la "Antología poética en honor a Góngora" que dirigiera Gerardo Diego con motivo de la celebración y recuperación del poeta barroco a trescientos años de su muerte, y en la que la propuesta de Diego era la de crear objetos verbales que en su ensalmo rebasaran al verso. En esta antología se daban a conocer poesías de García Lorca, Pedro Salinas, Jorge Guillén y Manuel Altolaguirre. Con esta procedencia, Octavio Paz iba al encuentro de una generación debatida en la búsqueda de una alternativa para la poesía que se enfrentaba a una realidad insumisa a la proclama de un hombre nuevo.

Junto con Carlos Pellicer, Octavio Paz llegó a París el 1o. de julio de 1937. Ahí conoció a Neruda y a Vallejo, al «mito nacido del océano» y al «vagabundo de la ciudad», como les llamó. De París fueron a Barcelona y de ahí a Valencia, donde sería la inauguración.

Su padre se retiró de la política en 1928, y murió el 10 de marzo de 1935, en la colonia Santa Marta Acatitla, al ser arrollado por un tren, en un accidente ocasionado por su embriaguez. Después de la muerte de su padre, se trasladó a España para combatir en el bando republicano en la guerra civil, y participó en la Alianza de Intelectuales Antifascistas. Al regresar a México fue uno de los fundadores de "Taller" (1938) y "El Hijo Pródigo".

En 1937 viajó a Yucatán como miembro de las misiones educativas del general Lázaro Cárdenas en una escuela para hijos de obreros y campesinos de Mérida. Ahí comenzó a escribir "Entre la piedra y la flor" (1941, revisado en 1976), poema sobre la dramática explotación del campo y el campesino yucateco. Estuvo casado con la dramaturga, escritora y poeta Elena Garro a quien conoció en la UNAM (1938-1959), con quien tuvo una hija, Laura Helena Paz Garro, divorciándose en 1950. En 1959 se unió a Bona Tibertelli de Pisis, con quien convivió hasta 1965, mientras era embajador de México en la India. Al año siguiente contrajo matrimonio con la francesa Marie José Tramini, su compañera hasta el final.

En 1937, Paz fue invitado a España durante la guerra civil como miembro de la delegación mexicana al Congreso Antifascista, donde mostró su solidaridad con los republicanos, y donde conoció y trató a los poetas de la revista "Hora de España", cuya ideología política y literaria influyó en su obra juvenil. Sin embargo, como confesó años después en la serie televisiva "Conversaciones con Octavio Paz", ese sentimiento de solidaridad con la causa republicana se vio afectado por la represión contra los militantes del Partido Obrero de Unificación Marxista de Cataluña entre quienes tenía camaradas. Este prolongado proceso de desilusión lo llevaría a denunciar los campos de concentración soviéticos y los crímenes de Stalin en marzo de 1951.

A su regreso de España, participó en 1938 como cofundador en la revista literaria "Taller", en la que escribió hasta 1941.

En 1943 recibió la Beca Guggenheim e inició sus estudios en la Universidad de California, Berkeley en los Estados Unidos. Dos años después comenzó a servir como diplomático mexicano, y fue destinado a Francia donde permaneció hasta 1951 y donde conoció a los surrealistas, que le influyeron, y colaboró en la revista "Esprit". Durante esa estancia, en 1950, publicó "El laberinto de la soledad", un innovador ensayo antropológico sobre los pensamientos y la identidad mexicanos.

De enero a marzo de 1952 trabaja en la embajada mexicana en la India y después, hasta enero de 1953, en Japón. Regresa a la Ciudad de México a dirigir la oficina de Organismos Internacionales de la Secretaría de Relaciones Exteriores.

En 1954, Paz tuvo «una participación muy estrecha en la fundación de la "Revista Mexicana de Literatura", influenciada «políticamente con la idea de la llamada 'tercera vía', que significaba ni con la izquierda, ni con la derecha. Esta idea venía de París, con León Blum». El primer número fue el de septiembre-octubre de 1955, y contó con el apoyo de Paz hasta que 4 años más tarde este regresó a Europa.

En 1955 contribuyó en la "Revista Mexicana de Literatura" y en "El Corno Emplumado". En 1956, participó en "Poesía en voz alta".

En 1959 regresó a París y tres años más tarde fue designado embajador en la India. En 1964 conocería a la francesa Marie-José Tramini, que se convertiría en su última esposa.

En 1968, estaba en Nueva Delhi cuando tuvo lugar la masacre de Tlatelolco como parte del Movimiento de 1968 en México el 2 de octubre. En señal de protesta contra estos lamentables sucesos, que empañaron la celebración de los Juegos Olímpicos, renunció a su cargo de embajador, dejando patentes sus diferencias con el gobierno de Gustavo Díaz Ordaz. Fue el único que se atrevió a hacerlo. Trabajará los próximos años enseñando en diversas universidades estadounidenses, como las de Texas, Austin, Pittsburgh, Pensilvania, Harvard.

Tres años más tarde, en octubre de 1971, ya bajo la presidencia de Luis Echeverría, «un poco con esa idea de redescubrir los valores liberales y democráticos
en la sociedad mexicana», fundaría la revista "Plural", «elegante fusión de literatura y política», y que dirigiría hasta su desaparición en 1976 el mismo Paz. A diferencia de otros escritores e intelectuales mexicanos, Paz no tardó en retirarle su apoyo al presidente Echeverría, una vez que este demostró su escasa voluntad de aclarar las matanzas de Tlatelolco, en 1968, y en San Cosme, el llamado Jueves de Corpus, en 1971, en donde hubo una represión brutal contra una protesta estudiantil.

Tanto en esa revista como en "Vuelta" —fundada ese mismo año de 1976 y donde «la influencia del liberalismo sería fundamental», reflejando la «reconciliación» del escritor con esta corriente de pensamiento— Paz denunció las violaciones a los derechos humanos de los regímenes comunistas. Esto le trajo mucha animosidad de parte de la izquierda latinoamericana y algunos estudiantes universitarios. En el prólogo del tomo IX de sus obras completas, publicado en 1993, Paz declaró:

En México, antes había sido visto con sospecha y recelo; desde entonces, la desconfianza empezó a transformarse en enemistad más y más abierta e intensa. Pero en aquellos días [década de los 1950] yo no me imaginaba que los vituperios iban a acompañarme años y años, hasta ahora. (Página 44).
El 19 de abril de 1998 Octavio Paz a sus 84 años de edad, murió en la Casa de Alvarado, ubicada en la calle Francisco Sosa del barrio de Santa Catarina, Coyoacán, Ciudad de México. El escritor había sido trasladado por la presidencia de la República en enero de 1997, ya enfermo, luego de que un incendio destruyó su departamento (en Río Guadalquivir 109 esquina con Paseo de la Reforma, a una cuadra del "Ángel" de la Independencia) y parte de su biblioteca, el domingo 22 de diciembre de 1996. Durante un tiempo, la Casa Alvarado fue sede de la Fundación Octavio Paz y ahora lo es de la Fonoteca Nacional.

Experimentación e inconformismo pueden ser dos de las palabras que mejor definen su labor poética. Con todo, Paz es un poeta difícil de encasillar. Ninguna de las etiquetas adjudicadas por los críticos encaja con su poesía: poeta neomodernista en sus comienzos; más tarde, poeta existencial; y, en ocasiones, poeta con tintes de surrealismo. Ninguna etiqueta le cuadra y ninguna le sobra, aunque el mismo Paz reconoció que en su formación «fundamentales fueron los surrealistas, con quienes hice amistad en el año 46 o 47, que en esa época estaban más cerca de los libertarios».

En realidad, se trata de un poeta que no echó raíces en ningún movimiento porque siempre estuvo alerta ante los cambios que se iban produciendo en el campo de la poesía y siempre estuvo experimentando, de modo que su poesía, como toda poesía profunda, acabó por convertirse en una manifestación muy personal y original. Además, se trata de un poeta de gran lirismo cuyos versos contienen imágenes de gran belleza. Después de la preocupación social, presente en sus primeros libros, comenzó a tratar temas de raíz existencial, como la soledad y la incomunicación. Una de las obsesiones más frecuentes en sus poemas es el deseo de huir del tiempo, lo que lo llevó a la creación de una poesía espacial cuyos poemas fueron bautizados por el propio autor con el nombre de "topoemas" (de topos + poema). Esto es lo que significa poesía espacial: poesía opuesta a la típica poesía temporal y discursiva. Se trata de una poesía intelectual y minoritaria, casi metafísica, en la que además de signos lingüísticos se incluyen signos visuales. En los "topoemas", igual que ocurría en la poesía de los movimientos de vanguardia, se le da importancia al poder sugerente y expresivo de las imágenes plásticas. No cabe duda de que en la última poesía de Paz hay bastante esoterismo, pero, al margen de ello, toda su poesía anterior destaca por su lirismo y por el sentido de transubstanciación que el autor da a las palabras.









Publicada en España, entre 1999 y 2005, por Galaxia Gutenberg/Círculo de lectores; y en México, en 2014, por el Fondo de Cultura Económica. Edición preparada por el autor.



<br>


</doc>
<doc id="9063" url="https://es.wikipedia.org/wiki?curid=9063" title="Factor de conversión">
Factor de conversión

El factor de conversión o factor unidad es un método de conversión que se basa en multiplicar por una o varias fracciones en las que el numerador y el denominador son cantidades iguales expresadas en unidades de medida distintas, de tal manera, que cada fracción equivale a la unidad. Es un método muy efectivo para cambio de unidades y resolución de ejercicios sencillos dejando de utilizar la regla de tres.

Cada factor de conversión se construye con una equivalencia (igualdad entre dos cantidades).








</doc>
<doc id="9064" url="https://es.wikipedia.org/wiki?curid=9064" title="Hiperión">
Hiperión

En la mitología griega, es un Titán, hijo de Urano (el Cielo) y Gea (la Tierra).

En la "Ilíada" de Homero, el dios sol se llamaba "Helios Hyperion" (‘Sol en lo más alto’), pero en la "Odisea", la "Teogonía" de Hesíodo y el himno homérico a Deméter el sol recibe el nombre de "Hyperonides" (‘hijo de Hiperión’), y ciertamente Hesíodo imaginaba a Hiperión como un ser separado de Helios en otras obras. De hecho, algunos traducen «Hiperión» como ‘el que aparece antes que el Sol’. En la literatura griega posterior Hiperión siempre se distingue de Helios.

Hiperión es considerado a menudo el dios de la observación, y su hermana Tea la diosa de la vista.

Según Hesíodo, se casó con Tea (llamada Eurifaesa en el "Himno homérico a Helios"), su hermana, con la que tuvo tres hijos: Helios (el Sol), Selene (la Luna) y Eos (la Aurora):

Hiperión desempeña un papel virtualmente nulo en los cultos griegos y muy pequeño en la mitología, con la excepción de aparecer en la lista de los doce Titanes. Autores griegos posteriores intelectualizaron sus mitos.

Como padre de Helios, Hiperión fue considerado como el “primer principio” por el emperador Juliano, aunque su relevancia en la teúrgia es desconocida.

El poeta y escritor romántico Hölderlin llamó "Hiperión" al protagonista de su novela homónima, en la que narra la lucha interna de este personaje ante el dilema de si debe permanecer junto a las personas que ama o unirse a las tropas helenas para lograr la independencia griega frente a la dominación otomana, devolviendo así la belleza y el antiguo brillo que habían desaparecido de Grecia siglos atrás.



</doc>
<doc id="9065" url="https://es.wikipedia.org/wiki?curid=9065" title="Helios">
Helios

En la mitología griega, Helio o Helios (en griego antiguo Ἥλιος "Hếlios", ‘sol’) es la personificación del Sol. Es el Titán hijo de los titanes Hiperión y Tea (de acuerdo con Hesíodo) también conocida como Eurifaesa (en el himno homérico 31) y hermano de las diosas Selene, la luna, Eos, la aurora y el dios Titán. Sin embargo, Homero lo llama a menudo simplemente Titán o Hiperión.

Helios era imaginado como un hermoso dios coronado con la brillante aureola del sol, que conducía un carro por el cielo cada día hasta el Océano que circundaba la tierra y regresaba por este hacia el este por la noche. Homero describe el carro de Helios como tirado por toros solares; más tarde Píndaro lo escribió que por «corceles que arrojaban fuego». Posteriormente, los caballos recibieron fogosos nombres: Flegonte (‘ardiente’), Aetón (‘resplandeciente’), Pirois (‘ígneo’) y Éoo (‘amanecer’).

A medida que pasó el tiempo, Helios fue cada vez más identificado con el dios de la luz, Apolo. Su equivalente en la mitología romana era el Sol, y específicamente Sol Invictus.

La historia más conocida sobre Helios es la de su hijo Faetón, que intentó conducir el carro de su padre por el cielo pero perdió el control e incendió la Tierra.

A veces se aludía a Helios con el epíteto homérico Panoptes (‘el que ve todo’). En la historia narrada en la mansión de Alcínoo en la "Odisea", Afrodita, la esposa de Hefesto, se acostaba en secreto con Ares, pero Helios, el señor del sol que todo lo ve, los espió y se lo dijo a Hefesto, quien para castigarlos atrapó a los dos amantes en unas redes tan finas que resultaban invisibles.

En la "Odisea", Odiseo y su tripulación superviviente desembarcan en una isla, Trinacia, consagrada al dios sol, al que Circe llama Hiperión en vez de Helios. Allí se guardaba el sagrado ganado rojo del sol:

Aunque Odiseo advirtió a sus hombres para que no lo hicieran, estos mataron y comieron impíamente algunas cabezas del ganado. Las guardianas de la isla, hijas de Helios, se lo dijeron a su padre. Helios, sin embargo, apeló a Zeus, quien destruyó el barco y mató a todos los hombres salvo a Odiseo.

En una vasija griega pintada, Helios aparece cruzando el mar en la copa del trípode délfico, lo que parece ser una referencia solar. En los "Deipnosofistas", Ateneo contaba que, al ponerse el sol, Helios subía a una gran copa dorada en la que pasaba desde las Hespérides en el extremo occidental hasta la tierra de los etíopes, con quienes permanecía las horas de oscuridad. Cuando Heracles viajó a Eritea para cobrarse el ganado de Gerión, cruzó el desierto libio y quedó tan frustrado por el calor que disparó una flecha a Helios, el sol. Helios le rogó que parase y Heracles pidió a cambio la copa dorada que Helios usaba para cruzar el mar cada noche, de oeste a este. Heracles usó esta copa dorada para llegar a Eritea.

Con la oceánide Perseis, Helios fue el padre de Perses, Eetes, Circe y Pasífae. También fue padre de las Helíades.

Helios es identificado a veces con Apolo: «Nombres diferentes pueden aludir al mismo ser» observa Walter Burkert, «o bien pueden ser conscientemente igualados, como en el caso de Apolo y Helios.» En la obra de Homero, Apolo es identificado claramente como un dios diferente, relacionado con las plagas, con un arco plateado (no dorado) y sin características solares.

La primera referencia segura a Apolo identificado con Helios aparece en los fragmentos conservados de la obra de Eurípides "Faetón", en un discurso cerca del final, cuando Clímene, la madre de Faetón, lamenta que Helios haya destruido a su hijo, el Helios al que los hombres llaman justamente Apolo (entendiéndose aquí que el nombre significa "Apolón", ‘destructor’).

Para la época helenística Apolo había pasado a estar estrechamente relacionado con el sol en los cultos. Su epíteto Febo (‘brillante’), tomado prestado de Helios, sería más tarde aplicado también por los poetas latinos al dios Sol.

La identificación se hizo común en textos filosóficos y aparece en las obras de Parménides, Empédocles, Plutarco y Crates de Tebas entre otros, así como en algunos textos órficos. Eratóstenes escribe sobre Orfeo en sus "Catasterismos":

Los poetas latinos clásicos también usaron "Febo" como sobrenombre para el dios-sol, de donde proceden las referencias comunes en la poesía europea posterior a Febo y su carro como metáfora para el sol. Pero en las apariciones concretas en los mitos, Apolo y Helios están separados. El dios-sol, hijo de Hiperión, con su carro solar, aunque llamado a menudo "Febo", nunca es llamado "Apolo" salvo en identificaciones expresas no tradicionales. Los poetas romanos se referían a veces al dios sol como "Titán".

Apolión aparece en el Nuevo Testamento liderando la plaga de langostas que será lanzada sobre los enemigos de Dios al Final de los Tiempos:

El nombre significa en griego ‘Destructor’ (Απολλυων, de απολλυειν, ‘destruir’). También recuerda al término hebreo sin relación Abadón (literalmente ‘lugar de destrucción’, pero aquí personalizado) y al nombre del dios griego Apolo, también un ‘destructor’ en su aspecto de controlar las plagas, si bien la atrocidad compuesta que es Apolión es de inspiración claramente babilónica y persa, no helénica. Apolión parece equipararse en el "Apocalipsis" con la Bestia. El término «Apolión» era relacionado a menudo por los primeros cristianos con el Diablo, y extravagantemente descrito, usándose aún como nombre alternativo para este.

El simbolismo de deja abierta a interpretación la identificación exacta de Abadón/Apolión. Algunos investigadores bíblicos creen que es el anticristo o Satán.

L. R. Farnell asumió «que el culto solar había sido una vez prevalente y poderoso entre los pueblos de la cultura prehelénica, pero que muy pocas de las comunidades del periodo histórico posterior lo conservaron como un factor potente de la religión estatal.» Nuestras fuentes literarias, principalmente áticas, tienden a darnos un inevitable sesgo ateniense cuando se examina la antigua religión griega, y «no podía esperarse que ningún ateniense adorase a Helios o Selene,» observa J. Burnet, «pero podríamos pensar que eran dioses, dado que Helios era el gran dios de Rodas y Selene era adorada en Elis y otras partes». James A. Notopoulos considera que la distinción de Burnet es artificial: «Creer en la existencia de los dioses implica su reconocimiento en los cultos, como muestra "Leyes" 87 D, E.». En "La paz", Aristófanes contrasta la adoración de Helios y Selene con la de los más esencialmente griegos dioses olímpicos, como deidades representativas de los persas aqueménidas. Todas las evidencias demuestran que Helios y Selene fueron dioses menores para los griegos.

«La isla de Rodas es casi el único lugar donde Helios goza de un culto importante», afirma Burkert, describiendo un espectacular rito en el que una cuadriga era despeñada por un precipicio al mar, destacando sus matices del drama de Faetón. Allí se celebraban torneos gimnásticos anuales en su honor. El Coloso de Rodas estaba dedicado a él. Helios tenía también un culto importante en la acrópolis de Corinto en el continente griego.

La tensión entre la veneración religiosa tradicional dominante de Helios, que se había enriquecido con los valores éticos y el simbolismo poético en Píndaro, Esquilo y Sófocles, y el examen jónico protocientífico de Helios el Sol, un fenómeno que los estudios griegos calificaban de "meteora", chocaron en el juicio de Anaxágoras "circa" 450 a. C., un anticipo del culturalmente traumático juicio de Sócrates por irreligiosidad, en el 399.

En "La República" de Platón Helios, el Sol, es la descendencia simbólica de la idea del Bien.

En la Antigüedad Tardía un culto de Helios Megisto (‘Gran Helios’) añadió a la imagen de Helios varios elementos sincréticos, que han sido analizados con detalle por Wilhelm Fauth mediante una serie de textos griegos tardíos, en concreto: un "Himno a Helios" órfico; la llamada Liturgia Mitraica, donde Helios gobierna los elementos; hechizos y encantamientos invocando a Helios entre los papiros mágicos griegos; un "Himno a Helios" de Proclo; la "Oración a Helios" de Juliano, el último puesto del paganismo oficial; y un episodio de las "Dionisíacas" de Nono.

Según Hesíodo en su Teogonía estos son sus familiares:

Algunos mitos dicen que él es el padre de Circe y Eetes con Hécate. Además fue esposo de Rodo, con la que tuvo siete hijos y una hija. Se dice que vivían en la Isla de Rodas donde seis de sus siete hijos construyeron el Coloso de Rodas en su honor.










</doc>
<doc id="9066" url="https://es.wikipedia.org/wiki?curid=9066" title="Falciforme">
Falciforme

En biología y otras disciplinas se llama falciforme a toda aquella estructura con forma de hoz o de media luna. El vocablo proviene del genitivo del término latino "falx" que significa "hoz" más un derivado de "forma".



</doc>
<doc id="9074" url="https://es.wikipedia.org/wiki?curid=9074" title="Campo petrolífero">
Campo petrolífero

Un campo petrolero es una zona con abundancia de pozos de los que se extrae hidrocarburos del subsuelo. Debido a que las formaciones subterráneas que contienen petróleo (yacimientos petrolíferos) pueden extenderse sobre grandes zonas, a veces de varios cientos de kilómetros cuadrados, una explotación completa conlleva varios pozos o plataformas diseminados por toda un área. Además, puede haber pozos exploratorios que investigan los límites, tuberías para transportar el petróleo a cualquier lugar y locales de apoyo.

A menudo se pueden ver pozos ubicados muy próximos entre sí (apenas unos cuantos metros). En este caso se trata de pozos perforados a distintas profundidades, debido a la disposición de los yacimientos en capas o cubetas paralelas entre sí y separadas por estratos impermeables.

Ya que un campo petrolífero puede estar bastante alejado de la civilización, establecerlo puede ser un ejercicio la mayoría de las veces extremadamente complicado, por lo que respecta a su logística. Por ejemplo, los trabajadores tienen que realizar su labor allí durante meses o años, y requieren hospedaje. Asimismo, el hospedaje y el equipamiento requiere electricidad y agua. Las tuberías en las zonas frías pueden necesitar ser calentadas. Un exceso de gas natural hace necesario quemarlo si no hay forma de hacer uso del mismo, lo que requiere un horno, almacenes, y tuberías para transportarlo del pozo al horno.

Así, el típico campo petrolífero parece una pequeña ciudad autosuficiente en medio de un paisaje punteado con torres de perforación ("oil derricks") o los gatos de las bombas, conocidos como "burros cabeceros" ("nodding donkeys"), debido a su brazo en movimiento, que también se conocen como "balancines" en algunos países. Varias empresas, como Bechtel y Halliburton, tienen organizaciones que se especializan en la construcción a gran escala de la infraestructura requerida para operar un campo de forma rentable. En muchos casos, las torres de perforación se desmontan para utilizar sus piezas en una perforación nueva. En otros, como sucede en el Lago de Maracaibo, se dejan en el lugar, no solo por el mayor costo de su reutilización, sino porque siguen sirviendo para hacer algunos trabajos de mantenimiento.

Existen más de 40.000 campos petrolíferos extendidos a lo largo del globo, tanto en tierra como mar adentro. El mayor es el Campo Ghawar en Arabia Saudita y el Campo Burgan en Kuwait, con más de 60 mil millones de barriles estimados en cada uno. La mayoría de los pozos petrolíferos son mucho menores. En la edad moderna, la localización y las reservas conocidas de campos de petróleo son un factor clave en muchos conflictos geopolíticos.


</doc>
<doc id="9075" url="https://es.wikipedia.org/wiki?curid=9075" title="Pensilvania">
Pensilvania

Pensilvania (), oficialmente Mancomunidad de Pensilvania ("Commonwealth of Pennsylvania"), es uno de los cincuenta estados que forman los Estados Unidos de América. Su capital es Harrisburg y su ciudad más poblada, Filadelfia, famosa por ser el lugar donde se elaboró la Declaración de Independencia y la Constitución.

Está ubicado en la región Noreste del país, división Atlántico Medio, limitando al norte con Nueva York, al noreste y este con el río Delaware que lo separa de Nueva York y Nueva Jersey respectivamente, al sur con Maryland, al suroeste con Virginia Occidental, al oeste con Ohio y al noroeste con el lago Erie. Con 12 702 379 habs. en 2010 es el sexto estado más poblado —por detrás de California, Texas, Nueva York, Florida e Illinois— y con 106,49 hab/km², el noveno más densamente poblado, por detrás de Nueva Jersey, Rhode Island, Connecticut, Massachusetts, Maryland, Delaware, Nueva York y Florida. Fue el segundo estado en ser admitido en la Unión, el 12 de diciembre de 1787.

Las dos ciudades más importantes del estado son Filadelfia, lugar de eventos importantes durante la Revolución y una zona metropolitana próspera en la época moderna, y Pittsburgh, un puerto interior ubicado en las orillas de tres ríos. Pensilvania es uno de los estados históricos de la nación. 

Las montañas de Pocono y el río Delaware proporcionan actividades recreativas populares. La región de los llamados «"Pennsylvania Dutch"» ('neerlandeses de Pensilvania'), en el centro-sur del estado, es otro lugar favorito de los turistas. En realidad, no son neerlandeses, sino de origen alemán. Formados por varios grupos, incluso religiosos como los amish y los menonitas, se los conoce como «la gente llana», que viven sin la tecnología ni las comodidades modernas. Se les llama "Dutch" por la confusión entre la palabra alemana "Deutsch", que significa 'alemán', con la palabra inglesa "Dutch", que significa 'neerlandés'.

Los buques USS "Pennsylvania" fueron nombrados en honor a este estado. Ha dado su nombre al período Pensilvánico en geología. Se le conoce también como «"the Keystone State"» («el estado piedra angular»).

Aunque los suecos y los neerlandeses fueron los primeros colonos europeos, el 28 de febrero de 1681 el rey Carlos II de Inglaterra le cedió un terreno al cuáquero inglés William Penn para el pago de una deuda de 16 000 libras esterlinas (equivalentes a aproximadamente 1 960 000 en 2013, con el ajuste de la inflación) que se le debían al padre de William Penn, el almirante William Penn. Fue esta una de las concesiones de tierra más grandes que se han hecho a un individuo en la historia. Fue llamada "Pennsylvania" (por el apellido Penn; y "sylvania" se deriva del latín medieval "silva", 'selva, bosque', debido a la frondosidad de sus bosques). A William Penn, quien deseaba que se llamara "New Wales" o "Sylvania", le preocupaba que la gente pensara que él mismo había bautizado el lugar en su honor, pero el rey le pidió llamarlo Pensilvania en honor a su padre, Sir William Penn. Penn estableció un gobierno con dos innovaciones que siguieron reproduciéndose en el Nuevo Mundo: la comisión del condado y la libertad de creencia religiosa.

Según algunas otras versiones, el nombre de la región proviene de una palabra de origen galés, "Pen", que significa "cabeza".

Pensilvania tiene 257 km de largo de norte a sur y 455 km de este a oeste. Del total de 119 282 km² de superficie del estado, 116 075 km² son tierra, 1.269 km² son aguas interiores y 1940 km² corresponden a las aguas del lago Erie. Pensilvania es el en los Estados Unidos.

Las fronteras del estado son la llamada Línea Mason-Dixon (39° 43' N) en el sur, el río Delaware en el este, 80° 31' W en el oeste, y el paralelo 42° N en el norte, a excepción de un pequeño segmento al final de la parte oeste, donde un triángulo se extiende hacia el norte hasta el lago Erie. Pensilvania tiene frontera con otros seis estados: Nueva York al norte, Nueva Jersey al este, Delaware y Maryland al sudeste, Virginia Occidental al sudoeste y finalmente Ohio al oeste.

La ciudad de Filadelfia se encuentra al sudeste, Pittsburgh en el suroeste, Scranton y Wilkes-Barre en el noreste y Erie en el noroeste, con la capital estatal, Harrisburg, en el río Susquehanna en la región central de la Commonwealth.

La diversidad geográfica de Pensilvania también tiene como resultado una amplia variedad climática. Entre las dos principales zonas climáticas, la esquina sudeste del estado tiene el clima más cálido. Gran Filadelfia se encuentra en la punta meridional de la zona de clima continental húmedo, con algunas características del clima subtropical húmedo que se encuentra en Delaware y Maryland hacia el sur. Moviéndose hacia el interior montañoso del estado, el clima se hace marcadamente más frío, el número de días nublados se incrementa, y las cantidades de nevadas de invierno son mayores.


La mayor parte de las zonas bajas del interior tienen un clima continental húmedo moderado (clasificación climática de Köppen "Dfa"), con veranos cálidos y húmedos e inviernos fríos o muy fríos. Las áreas montañosas de los Apalaches tienen un clima continental húmedo más severo (Köppen "Dfb"), con inviernos más fríos y nevados y veranos algo más fríos. El área del sudeste tiene un clima subtropical húmedo (Köppen "Cfa") con inviernos algo más suaves.

Las áreas occidentales del estado, en particular las ciudades cerca del lago Erie, pueden registrar más de 254 cm de nieve anualmente y en todo el estado se recoge un promedio de 1.041 mm de precipitaciones de lluvia a lo largo del año. Las inundaciones son más comunes en marzo y abril que durante otros meses del año.

Los ciclones tropicales amenazan el estado durante el verano y otoño con su principal impacto: las fuertes precipitaciones. Aunque el huracán Agnes fuera un huracán que recaló en Florida, su impacto principal fue sobre la región del Atlántico Medio, donde el Agnes se combinó con una borrasca no tropical para producir lluvias extendidas de 150 a 300 mm con cantidades que en algunos puntos al oeste del condado de Schuylkill alcanzaron los 480 mm. Estas lluvias produjeron la gran inundación que se extendió desde Virginia hacia el norte hasta Nueva York, junto con otra inundación sobre la parte occidental de las Carolinas.

Filadelfia ha recibido vientos sostenidos próximos a fuerza huracanada de ciclones tropicales en el pasado.

Antes del establecimiento de la Commonwealth, el área era el hogar de los delaware, susquehannock, iroqueses, erie (Nación del Gato), shawnee y otras tribus indígenas.
En 1681 Carlos II concedió una carta de derechos sobre estas tierras a William Penn, para reembolsar una deuda de 20.000£ (aproximadamente 30.000.000$ en 2007) que adeudaba al padre de William, el almirante Penn. Esta entrega fue una de las concesiones de tierras más grande hecha a un individuo en la historia. El lugar fue llamado Pennsylvania (Pensilvania en español), que significa «los Bosques de Penn», en honor al almirante Penn. William Penn, que quería que su provincia se llamara simplemente «Sylvania», estaba avergonzado por el cambio, temiendo que la gente pensara que él la había nombrado en honor a sí mismo, pero Carlos II no cambió el nombre de las tierras concedidas.

Penn estableció un gobierno con dos innovaciones que sirvieron como referentes posteriores en el Nuevo Mundo: la creación de las Comisiones de Condado (cuerpo de oficiales electos para el mantenimiento de la ley y el orden) y el establecimiento de la libertad de culto.

Entre 1730 y 1764, momento en que fue abolido por el Parlamento con la Ley Monetaria de 1764, la Colonia de Pensilvania tuvo su propio papel moneda, el llamado «Vale Colonial» ("Colonial Scrip"), a causa de la escasez de oro y plata en aquellos momentos. La Colonia emitió billetes de crédito que eran tan válidos como monedas de oro o de plata debido a su estatus de dinero de curso legal. El ser emitidos por el gobierno y no por una institución bancaria, era una proposición sin interés, que sufragaba en gran parte los gastos del gobierno y por lo tanto los impuestos de la gente. Esto promovió el empleo general y la prosperidad ya que el Gobierno fue discreto y no emitió demasiado para evitar la inflación. Benjamin Franklin participó en la creación de este dinero, del cual dijo que su utilidad nunca debía ser discutida y también contó con la «aprobación cautelosa» de Adam Smith.

Después del Congreso de la Ley del Timbre de 1765, el Delegado John Dickinson (de Filadelfia) escribió la "Declaration of Rights and Grievances" (Declaración de Derechos y Quejas) donde se afirmaba que los colonos americanos eran iguales a los demás ciudadanos británicos, protestando por la aplicación de impuestos sin la correspondiente representación colonial. El Congreso fue la primera reunión de las Trece Colonias, convocadas a petición de la Asamblea de Massachusetts, aunque sólo nueve colonias enviaron delegados. Ante la negativa británica, Dickinson escribió "Letters from a Farmer in Pennsylvania: To the Inhabitants of the British Colonies" (Cartas de un agricultor en Pensilvania, a los habitantes de las Colonias británicas), que fueron publicadas en el periódico "Pennsylvania Chronicle" entre el 2 de diciembre de 1767 y el 15 de febrero de 1768 donde Dickinson intentaba persuadir a sus lectores (a ambos lados del Atlántico) tanto del error económico como de la inconstitucionalidad de no tener en cuenta los derechos de los ingleses que vivían en las Colonias americanas.

Cuando los llamados «padres fundadores» de los Estados Unidos decidieron reunirse en Filadelfia en 1774, 12 colonias enviaron representantes al Primer Congreso Continental. El Primer Congreso Continental preparó y firmó en Filadelfia la Declaración de la Independencia, pero cuando la ciudad fue capturada por los británicos, el Congreso Continental se trasladó hacia el oeste, reuniéndose en el juzgado de Lancaster el sábado 27 de septiembre de 1777 y posteriormente en York. Allí prepararon los Artículos de la Confederación que unió a las 13 colonias y el Congreso actuó "de facto" como Gobierno de lo que se convertiría en los Estados Unidos. Más tarde, se redactó la Constitución y Filadelfia fue elegida de nuevo para ser la cuna de la nueva Nación.

Pensilvania fue el segundo estado en ratificar la Constitución estadounidense, el 12 de diciembre de 1787, 5 días después de Delaware, que fue el primero.
El Dickinson College de Carlisle, que recibió su nombre en honor a John Dickinson, fue el primer "college" fundado en el país. Establecido originalmente en 1773 como un «Grammar School», este centro educativo fue oficialmente fundado como universidad el 9 de septiembre de 1783, cinco días después de que se firmara el Tratado de París, haciendo de la universidad la primera fundada en los recién reconocidos Estados Unidos de América.

Durante medio siglo, la legislatura de la Commonwealth celebró sus reuniones en diversos lugares del área de Filadelfia antes de reunirse con regularidad en el "Independence Hall" de Filadelfia durante 63 años. Pero la Asamblea necesitaba una posición más céntrica y así, en 1799, la legislatura se movió al Juzgado de Lancaster y finalmente en 1812 a Harrisburg. La Asamblea celebró sus sesiones en el viejo Juzgado del condado de Dauphin hasta diciembre de 1821, cuando se terminó la construcción del "Redbrick Capitol". Este sufrió un incendió en 1897, probablemente debido a una chimenea defectuosa. La legislatura se reunió entonces en la Iglesia Metodista de Grace en la calle State (todavía en pie en la actualidad), hasta que el actual edificio del Congreso fue terminado en 1907.

El nuevo Capitolio estatal se inspiró en las cúpulas de la Basílica de San Pedro en Roma y en el Congreso de los Estados Unidos. El presidente Theodore Roosevelt lo calificó como «el Congreso estatal más hermoso de la nación» durante su inauguración. En 1989 el "New York Times" lo elogió como «Magnífico, incluso imponente por momentos, pero también es un edificio funcional, accesible a los ciudadanos... un edificio que conecta con la realidad de la vida diaria».

Pensilvania cuenta con el nueve por ciento de todas las áreas boscosas de los Estados Unidos. En 1923 el presidente Calvin Coolidge estableció el Bosque Nacional Allegheny bajo la autoridad de la llamada "Weeks Act" de 1911, en la parte noroeste del estado, en los condados de Elk, Forest, McKean y Warren con el objetivo de producción de madera y protección de la cuenca del río Allegheny. El Allegheny es el único bosque nacional del estado.

James Buchanan, del condado de Franklin, fue el único presidente de los Estados Unidos soltero, y el único nacido en Pensilvania. La Batalla de Gettysburg (la batalla que tuvo más bajas en los Estados Unidos y generalmente considerada crucial en la Guerra Civil estadounidense) tuvo lugar cerca de Gettysburg. Unos 350.000 ciudadanos de Pensilvania sirvieron en el Ejército de la Unión junto con 8.600 voluntarios afroamericanos.

En 1859 Edwin Drake perforó el primer pozo petrolífero comercial estadounidense cerca de Titusville, que se convirtió en el comienzo del gran "boom" del negocio petrolero en los Estados Unidos.

El centro de población de Pensilvania se encuentra localizado en el condado de Perry, en el borough de Duncannon.

Según el censo 2010, la población de Pensilvania era 79,5 % Blanco, 10,8 % Negro, 5,7 % Latino, 2,8 % Asiático, 1,1 % otra raza. Hay un grande cantidad de Americanos Negros en Filadelfia, Harrisburg, Pittsburgh, entre otra áreas. En la población latino, la mayoría son puertorriqueños, también menor cantidades de dominicanos y mexicanos. La mayoría de los latinos viven en ciudades como Filadelfia, Allentown, Reading, Lancaster, Hazelton.

En el año 2006 Pensilvania tenía una población estimada de 12.440.621 personas, que supone un aumento de 35.273 personas con respecto al año anterior y un aumento de 159.567 personas desde el año 2000. La migración neta de otros estados resultó en una disminución de 27.718 personas y la inmigración de otros países supuso un aumento de 126.007 personas. La migración neta a la Commonwealth fue de 98.289 personas. La migración de personas nativas de Pensilvania supuso una disminución de 100.000 personas. En 2006, el 5,00 % de los pensilvanos eran nacidos en el extranjero (621.480 personas). El estado tuvo en el 2005 una tasa de pobreza estimada del 11,9 %. Pensilvania tenía la 3ª proporción más alta de ciudadanos mayores de 65 años en 2005.

Los ciudadanos de Pensilvania no nacidos en el estado provienen principalmente de Asia (36,0 %), Europa (35,9 %), América Latina (30,6 %); el 5 % proviene de África, el 3,1 % de Norteamérica y el 0,4 % de Oceanía.

La población registrada de hispanos de Pensilvania, sobre todo entre las razas asiáticas, hawaianas y blancas, ha aumentado de forma significativa en los últimos años. No está claro en que medida este cambio refleja un cambio en la población o refleja un incremento en la voluntad de autoidentificar su estatus de minoría.

Un 5,9 % de la población del estado tiene menos de 5 años, el 23,8 % menos de 18 y un 15,6 % tiene 65 años o más. Las mujeres representan el 51,7 % de la población.

Los cinco grupos de ascendencia autorreconocidos más numerosos en Pensilvania son: alemanes (27,66 %), irlandeses (17,66 %), italianos (12,82 %), ingleses (8,89 %) y polacos (7,23 %).

La educación en Pensilvania, se puede dividir en universidades, educación secundaria y primaria. Algunas de sus mejores universidades son:

Pensilvania no tiene idioma oficial, pero el idioma oficial de facto es inglés. 

Aparte del año 2010, 90,15 % de la población con 5 años o más habla inglés como su idioma primario, mientras 4,09 % habla español, 0,87 % habla alemán u holandés de Pensilvania, 0,47 % habla chino. En total, el 9,85 % de la población con edad mayor que 5 años habla idioma aparte de inglés como su lengua materna.


Desde la época colonial, Pensilvania (junto con Rhode Island) se caracterizó por su diversidad religiosa y fue ejemplo de convivencia de múltiples religiones y esta diversidad religiosa todavía perdura en la actualidad.

La población de Pensilvania en 2000 era de 12.281.054 personas. De éstas, se estimó que 8.448.193 pertenecían a algún tipo de religión organizada. Según la "Association of Religion Data Archives (ARDA)" de la Universidad Estatal de Pensilvania, existen datos fidedignos de 7,116,348 personas pertenecientes a grupos religiosos en Pensilvania en 2000, que siguen 115 doctrinas diferentes. Su afiliación religiosa era:

En el año 2000 Pensilvania tenía la mayor concentración de población amish de los Estados Unidos (44.000), seguida por Ohio (43.000) e Indiana (33.000).

A pesar de que Pensilvania debe su existencia a los cuáqueros y muchos de los antiguos atavíos de la Commonwealth tienen sus raíces en la Sociedad Religiosa de los Amigos (como se les conoce oficialmente), los cuáqueros practicantes son una pequeña minoría en la actualidad.

Los municipios de Pensilvania están incorporadas como ciudades de distintas clases, bien como "«borough»", como "«township»" de distintas clases o bajo estatutos locales. Una "«village»" a menudo identificada por una señal al borde del camino, no está incorporada y es simplemente un lugar sin fronteras claras. En el estado existen 2567 municipalidades.

Hay cierta confusión sobre el número de pueblos ("«towns»") en Pensilvania. En 1870, Bloomsburg, la sede del condado de Columbia se incorporó como pueblo y está reconocida en publicaciones del gobierno estatal como «el único pueblo incorporado» de Pensilvania. Sin embargo, en 1975, el municipio de McCandless, en el condado de Allegheny adoptó un estatuto local bajo el nombre de «Town of McCandless».

Las diez mayores ciudades de Pensilvania, ordenadas por población, son:

El Producto Interno Bruto de Pensilvania en 2007 fue de 531.110 millones de dólares. Por renta "per cápita" Pensilvania, con 35.153 dólares, se sitúa en el puesto 42 entre los 50 estados estadounidenses.

Filadelfia en el sudeste, Pittsburgh en el sudoeste, Erie a la orilla del lago Erie en el noroeste del estado, la región del valle del Wyoming al nordeste y la región metropolitana de Allentown-Bethlehem-Easton al centro son centros urbanos de manufactura, con el resto de la Commonwealth que se conserva mucho más rural; esta dicotomía afecta a la política y a la economía estatal. Filadelfia es sede de diez compañías del Fortune 500 en el año 2007, la mayoría situadas en suburbios como King of Prussia. Pensilvania es líder en el sector financiero y la industria de seguros. Pittsburgh se sede de siete empresas del Fortune 500, incluyendo U.S. Steel, PPG Industries, H. J. Heinz y Alcoa. En total, Pensilvania es sede de cincuenta empresas del Fortune 500.

Como en el conjunto de los Estados Unidos y en la mayor parte de sus estados, la mayor empresa privada por número de empleados en la Commonwealth es Wal-Mart, seguida de la Universidad de Pensilvania, United Parcel Service y Giant Food. La mayor empresa de manufactura por número de empleados del estado es Merck.

En 2002 Pensilvania ocupaba la decimonovena posición del país en producción agrícola, pero se sitúa primero en fungicultura, tercero en la producción de árboles de Navidad y huevos, cuarto en viveros, leche, maíz para ensilado y viticultura. Se sitúa octavo en la nación por producción vinícola.

Pensilvania es sede de muchos equipos que participan en las ligas nacionales del deporte profesional: los Philadelphia Phillies y los Pittsburgh Pirates en las Grandes Ligas de Béisbol, los Philadelphia Eagles y los Pittsburgh Steelers en la NFL; los Philadelphia 76ers en la National Basketball Association; Philadelphia Union de la Major League Soccer; los Philadelphia Flyers y los Pittsburgh Penguins en la NHL; los Erie Bayhawks en la NBA Development League; y Philadelphia Soul en la Arena Football League. Estos equipos han acumulado 7 Series Mundiales (Pirates 5, Phillies 2), 14 Ligas Nacionales, 3 campeonatos de la NFL pre-Super Bowl (Eagles), 6 Super Bowl (Steelers), 1 campeonato Arena Bowl (Soul), 2 campeonatos NBA (76ers) y 4 ganadores de la Stanley Cup (Flyers 2, Penguins 2).

El fútbol americano universitario es muy popular en el estado. Los Pittsburgh Panthers ganaron nueve campeonatos nacionales (1915, 1916, 1918, 1929, 1931, 1934, 1936, 1937 y 1976) y permanecieron invictos en 8 temporadas (1904, 1910, 1915, 1916, 1917, 1920, 1937 y 1976). Los Penn State Nittany Lions con su entrenador Joe Paterno conquistaron dos campeonatos nacionales (1982 y 1986) y permanecieron invictos en cinco temporadas (1968, 1969, 1973, 1986 y 1994). Penn State juega sus partidos en el mayor estadio de los Estados Unidos, el Beaver Stadium, con capacidad para 107.282 espectadores. Otros equipos universitarios del estado consiguieron títulos nacionales de fútbol americano: el Lafayette College (1896) y la Universidad de Pensilvania (1895, 1897, 1904 y 1908).

El baloncesto universitario también es muy popular en Pensilvania, especialmente en el área de Filadelfia, donde cinco universidades (conocidas como las "Big Five") tienen una gran tradición en la División I de la NCAA de baloncesto. Las siguientes universidades del estado han conseguido títulos nacionales de baloncesto universitario: Universidad de La Salle (1954), Temple University (1938), Universidad de Pensilvania (1920 y 1921), Universidad de Pittsburgh (1928 y 1930) y Universidad Villanova (1985).

Los óvalos de carreras Nazareth Speedway y Pocono Raceway han albergado carreras de la Copa NASCAR, CART e IndyCar Series.

Arnold Palmer, uno de los principales golfistas profesionales del siglo XX, es originario de Latrobe, y Jim Furyk, uno de los principales golfistas profesionales del siglo XXI, creció cerca Lancaster. Los campos de golf de Oakmont y Merion han sido sede de numerosas ediciones del Abierto de los Estados Unidos.

Filadelfia fue sede de los X Games de 2001 y 2002.




</doc>
<doc id="9076" url="https://es.wikipedia.org/wiki?curid=9076" title="Edwin Drake">
Edwin Drake

Edwin Laurentine "Coronel" Drake (Greenville, Nueva York; 29 de marzo de 1819-Bethlehem, Pensilvania; 9 de noviembre de 1880) fue un perforador de petróleo de los Estados Unidos a quien se le atribuye popularmente el haber "descubierto" el petróleo.

En 1859, un hombre le dijo a Drake que su máquina para extraer el petróleo nunca funcionaría. Así, el 27 de agosto de 1859, en un pozo que fue construido por Drake en Oil Creek, cerca de Titusville, condado de Crawford, Pensilvania, se encontró petróleo. Desde entonces, dicho día se conoce como el Día de Drake. Aunque el petróleo era conocido con anterioridad a este hecho, no estaba disponible en grandes cantidades suficientes para ser útil.

De acuerdo con el libro de Ida Tarbell (1904) "The History of Standard Oil" ("La Historia de Standard Oil"), el pozo de petróleo no fue una idea de Drake, sino de su empleador, George Bissell.

Bissell envió a Drake al lugar en la primavera de 1858. Drake, un nativo de condado de Greene, Nueva York, había pasado con anterioridad su vida trabajando como oficinista, agente de correos y conductor de ferrocarril. Tras las dificultades iniciales para localizar las partes necesarias para construir el pozo ocasionó que su pozo fuera denominado "el disparate de Drake", pero por el contrario tuvo éxito. 

Hemos de destacar que lo más importante no es que el pozo de Drake fuera o no el primero, sino que dicho pozo en Titusville comenzó la industria en su espectacular carrera. En un mismo día, otros hombres construyeron su propios pozos petrolíferos en las cercanías.

El pozo de Drake llegó a una profundidad de 20 m, usándose perforación a percusión y una producción aproximada de 30 bbl por día.

Irónicamente, Edwin Drake murió pobre y el estado de Pensilvania tuvo que hacerse cargo de los gastos de su entierro.



</doc>
<doc id="9077" url="https://es.wikipedia.org/wiki?curid=9077" title="Sistema operativo de tiempo real">
Sistema operativo de tiempo real

Un sistema operativo de tiempo real es aquel que ha sido desarrollado para aplicaciones de tiempo real. Como tal, se le exige corrección en sus respuestas bajo ciertas restricciones de tiempo. Si no las respeta, se dirá que el sistema ha fallado. Para garantizar el comportamiento correcto en el tiempo requerido se necesita que el sistema sea predecible...

Usado típicamente para aplicaciones integradas, normalmente tiene las siguientes características:

Se caracterizan por presentar requisitos especiales en cinco áreas generales:

En la actualidad hay un debate sobre qué es tiempo real. Muchos sistemas operativos de tiempo real tienen un planificador (en inglés conocido como "scheduler"), diseños de controladores que minimizan los periodos en los que las interrupciones están deshabilitadas, un tiempo finito conocido (casi siempre calculado para el peor de los casos, término que en inglés se conoce como worst case) de la duración de interrupción. Muchos incluyen también formas especiales de gestión de memoria que limitan la posibilidad de fragmentación de la memoria y aseguran un límite superior mínimo para los tiempos de asignación y retiro de la memoria asignada.

Un ejemplo temprano de sistema operativo en tiempo real a gran escala fue el denominado «programa de control» desarrollado por American Airlines e IBM para el sistema de reservas Sabre.

Este tipo de sistemas operativos no es necesariamente eficiente en el sentido de tener una capacidad de procesamiento alta. El algoritmo de programación especializado, y a veces una tasa de interrupción del reloj alta pueden interferir en la capacidad de procesamiento.

Aunque para propósito general un procesador moderno suele ser más rápido, para programación en tiempo real deben utilizarse procesadores lo más predecibles posible, sin paginación. Todos estos factores en un procesador añade una aleatoriedad que hace que sea difícil demostrar que el sistema es viable, es decir, que cumpla con los plazos de tiempo para la ejecución de las tareas y la atención de los servicios o interrupciones.

Un sistema operativo de tiempo real puede ser implementado en microcontroladores o procesadores digitales de señal "DSP's", así, se pueden desarrollar aplicaciones embebidas en diferentes áreas de la electrónica.

Hay dos diseños básicos:


El diseño de compartición de tiempo gasta más tiempo de la CPU en cambios de tarea innecesarios. Sin embargo, da una mejor ilusión de multitarea. Normalmente se utiliza un sistema de prioridades fijas.

Uno de los algoritmos que suelen usarse para la asignación de prioridades es el Rate-Monotonic Schedule. Si el conjunto de tareas que tenemos es viable con alguna asignación de prioridades fijas, también es viable con el Rate-Monotonic Schedule, donde la tarea más prioritaria es la de menor periodo. Esto no quiere decir que si no es viable con Rate-Monotonic Schedule no sea viable con asignaciones de prioridad variable. Puede darse el caso de encontrarnos con un sistema viable con prioridades variables y que no sea viable con prioridades fijas.

En los diseños típicos, una tarea tiene tres estados: ejecución, preparada y bloqueada. La mayoría de las tareas están bloqueadas casi todo el tiempo. Solamente se ejecuta una tarea por UCP. La lista de tareas preparadas suele ser corta, de dos o tres tareas como mucho.

El problema principal es diseñar el programador. Usualmente, la estructura de los datos de la lista de tareas preparadas en el programador está diseñada para que cada búsqueda, inserción y eliminación necesiten interrupciones de cierre solamente durante un período muy pequeño, cuando se buscan partes de la lista muy definidas.

Esto significa que otras tareas pueden operar en la lista asincrónicamente, mientras que se busca. Una buena programación típica es una lista conectada bidireccional de tareas preparadas, ordenadas por orden de prioridad. Hay que tener en cuenta que no es rápido de buscar sino determinista. La mayoría de las listas de tareas preparadas sólo tienen dos o tres entradas, por lo que una búsqueda secuencial es usualmente la más rápida, porque requiere muy poco tiempo de instalación.

El tiempo de respuesta crítico es el tiempo que necesita para poner en la cola una nueva tarea preparada y restaurar el estado de la tarea de más alta prioridad.

En un sistema operativo en tiempo real bien diseñado, preparar una nueva tarea necesita de 3 a 20 instrucciones por cada entrada en la cola y la restauración de la tarea preparada de máxima prioridad de 5 a 30 instrucciones.
En un procesador 68000 20MHz, los tiempos de cambio de tarea son de 20 microsegundos con dos tareas preparadas.

Cientos de UCP MIP ARM pueden cambiar en unos pocos microsegundos.

Las diferentes tareas de un sistema no pueden utilizar los mismos datos o componentes físicos al mismo tiempo. Hay dos métodos para tratar este problema.

Uno de los métodos utiliza semáforos. En general, el semáforo binario puede estar cerrado o abierto. Cuando está cerrado hay una cola de tareas esperando la apertura del semáforo.

Los problemas con los diseños de semáforos son bien conocidos: inversión de prioridades y Bloqueo mutuo (deadlocks).

En la inversión de prioridades, una tarea de mucha prioridad espera porque otra tarea de baja prioridad tiene un semáforo. Si una tarea de prioridad intermedia impide la ejecución de la tarea de menor prioridad, la de más alta prioridad nunca llega a ejecutarse. Una solución típica sería otorgar a la tarea que tiene el semáforo la prioridad de la tarea más prioritaria de las que están esperando dicho semáforo. Esto se denomina algoritmo de herencia básica de prioridad.

En un punto muerto, dos tareas (T1,T2) pretenden adquirir dos semáforos (semA, semB) en orden inverso. En este caso si T1 adquiere semA y T2 adquiere semB cuando intenten adquirir el segundo semáforo no podrán hacerlo ya que lo tiene la otra tarea. De esta forma entran en un punto muerto del que ninguna de las dos tareas puede salir sin intervención externa. Esto se resuelve normalmente mediante un diseño por ej. obligando a adquirir los semáforos en un orden concreto.

La otra solución es que las tareas se manden mensajes entre ellas. Esto tiene los mismos problemas: La inversión de prioridades tiene lugar cuando una tarea está tratando un mensaje de baja prioridad, e ignora un mensaje de más alta prioridad en su correo. Los puntos muertos ocurren cuando dos tareas realizan envíos bloqueantes (se quedan en la función de envío esperando a que el receptor reciba el mensaje). Si T1 manda un mensaje de forma bloqueante a T2 y T2 manda un mensaje de igual forma a T1 ninguna de las dos tareas saldrá de la función de envío quedando ambas bloqueadas ya que no podrán llegar a la función de recepción. Puede resolverse reordenando envíos y recepciones o empleando envíos no bloqueantes o temporizados.

Aunque su comportamiento en tiempo real es algo más difícil de analizar que los sistemas de semáforos, los sistemas basados en mensajes normalmente son más sencillos de desarrollar que los sistemas de semáforo.

Las interrupciones son la forma más común de pasar información desde el mundo exterior al programa y son, por naturaleza, impredecibles. En un sistema de tiempo real estas interrupciones pueden informar diferentes eventos como la presencia de nueva información en un puerto de comunicaciones, de una nueva muestra de audio en un equipo de sonido o de un nuevo cuadro de imagen en una videograbadora digital.

Para que el programa cumpla con su cometido de ser tiempo real es necesario que el sistema atienda la interrupción y procese la información obtenida antes de que se presente la siguiente interrupción. Como el microprocesador normalmente solo puede atender una interrupción a la vez, es necesario que los controladores de tiempo real se ejecuten en el menor tiempo posible. Esto se logra no procesando la señal dentro de la interrupción, sino enviando un mensaje a una tarea o solucionando un semáforo que está siendo esperado por una tarea. El programador se encarga de activar la tarea y esta se encarga de adquirir la información y completar el procesamiento de la misma.

El tiempo que transcurre entre la generación de la interrupción y el momento en el cual esta es atendida se llama latencia de interrupción. El inverso de esta latencia es una frecuencia llamada frecuencia de saturación, si las señales que están siendo procesadas tienen una frecuencia mayor a la de saturación, el sistema será físicamente incapaz de procesarlas. En todo caso la mayor frecuencia que puede procesarse es mucho menor que la frecuencia de saturación y depende de las operaciones que deban realizarse sobre la información recibida.

Hay dos problemas con el reparto de la memoria en SOTR (sistemas operativos en tiempo real).

El primero, la velocidad del reparto es importante. Un esquema de reparto de memoria estándar recorre una lista conectada de longitud indeterminada para encontrar un bloque de memoria libre; sin embargo, esto no es aceptable ya que el reparto de la memoria debe ocurrir en un tiempo fijo en el SOTR.

En segundo lugar, la memoria puede fragmentarse cuando las regiones libres se pueden separar por regiones que están en uso. Esto puede provocar que se pare un programa, sin posibilidad de obtener memoria, aunque en teoría exista suficiente memoria. Una solución es tener una lista vinculada LIFO de bloques de memoria de tamaño fijo. Esto funciona asombrosamente bien en un sistema simple.

La paginación suele desactivarse en los sistemas en tiempo real, ya que es un factor bastante aleatorio e impredecible, que varía el tiempo de respuesta y no nos permite asegurar que se cumplirán los plazos, debido al trasiego de páginas de memoria con un dispositivo de almacenamiento (thrashing)

Para las comunicaciones se suelen usar conexiones o redes deterministas CAN bus o puertos serie, ya que las redes más usuales, como Ethernet son indeterministas y no pueden garantizarnos el tiempo de respuesta.
El sistema CAN bus es utilizado para la interconexión de dispositivos electrónicos de control (ECU) en los vehículos.


Principales fabricantes de sistemas RTOS en 2009



</doc>
<doc id="9079" url="https://es.wikipedia.org/wiki?curid=9079" title="Falsacionismo">
Falsacionismo

El falsacionismo o racionalismo crítico es una corriente epistemológica fundada por el filósofo austriaco Karl Popper (1902-1994). Para Popper, contrastar una teoría significa intentar refutarla mediante un contraejemplo. Si no es posible refutarla, dicha teoría queda «corroborada», pudiendo ser aceptada provisionalmente, pero no verificada; es decir, ninguna teoría es absolutamente verdadera, sino a lo sumo «no refutada». El falsacionismo es uno de los pilares del método científico.

El filósofo Karl Popper entendió que los filósofos del Círculo de Viena (al cual él mismo estuvo muy vinculado, aunque no como miembro) habían mezclado dos problemas diferentes para los que habían resuelto dar una única solución: el verificacionismo. En contraposición a este punto de vista, Popper remarcó que una teoría podría perfectamente tener significado sin ser científica, y que, como tal, un «criterio de significación» podría no necesariamente coincidir con un «criterio de demarcación». Así pues, ideó su propio sistema, al que se denomina falsacionismo (cabe señalar que Popper no llama a su metodología falsacionismo, sino racionalismo crítico). Este no solo es interpretable como una alternativa al verificacionismo; supone también un acuerdo acerca de la distinción conceptual que habían ignorado las teorías previas.

Para Popper —y a diferencia del Círculo de Viena— la ciencia "no es capaz de verificar" si una hipótesis es cierta, pero sí puede demostrar si ésta es falsa. Por eso no sirve la inducción, porque por mucho que se experimente nunca se podrá examinar "todos" los casos posibles, y basta con un solo contraejemplo para echar por tierra una teoría. Así pues, frente a la postura verificacionista preponderante hasta ese momento en filosofía de la ciencia, Popper propone el falsacionismo. Aunque Popper era realista no aceptaba la certeza, es decir, nunca se puede saber cuándo nuestro conocimiento es cierto. Popper comenzó "describiendo" la ciencia, pero en su evolución filosófica acabó siendo "prescriptivo" (aunque sin llegar al rigor normativo del Círculo), recomendando a la ciencia el método hipotético deductivo. Es decir, la ciencia no elabora enunciados ciertos a partir de datos, sino que propone hipótesis (que aunque se basen en la experiencia suelen ir más allá de ésta y predecir experiencias nuevas) que luego somete al filtro experimental para detectar los errores.

Popper vio la demarcación como un problema central en la filosofía de la ciencia. Propuso el falsacionismo como una forma de determinar si una teoría es científica o no. Simplificando, se podría decir que si una teoría es falsable, entonces es científica; si no es falsable, entonces no es ciencia.

Para Popper, afirmar que una teoría es científica quiere decir que añade conocimiento racional acerca del mundo empírico, por lo tanto, no puede ser:


La falsabilidad fue uno de los criterios utilizados por el Juez William Overton para determinar que el creacionismo no era científico y que no debería enseñarse en los colegios de Arkansas.

La falsabilidad es una propiedad de los enunciados y de las teorías, y, en sí misma, es neutral. Como criterio de demarcación, Popper busca tomar esta propiedad como base para afirmar la superioridad de teorías falsables sobre las no falsables, como parte de la ciencia, estableciendo así una posición que podría ser llamada "falsacionismo" con implicaciones políticas. Sin embargo, muchas cosas de las que pueden ser consideradas como dotadas de significado y utilidad no son falsables. Con toda certeza, los enunciados no falsables desempeñan una función en las propias teorías científicas. Lo que el criterio Popperiano permite ser llamado científico está abierto a interpretación. Una interpretación estricta concedería muy poco, puesto que no existen teorías científicas de interés que se encuentren completamente libres de anomalías. Del mismo modo, si solo consideramos la falsabilidad de una teoría y no la voluntad de un individuo o de un grupo para obtener o aceptar instancias falsables, entonces permitiríamos casi cualquier teoría.

En cualquier caso, es muy útil conocer si un enunciado de una teoría es falsable, aunque solo sea por el hecho de que nos proporciona un conocimiento acerca de las formas con las que alguien podría evaluar una teoría.

La tesis de Duhem-Quine argumenta que no es posible "probar" que un enunciado ha sido falsado; en su lugar, la falsación ocurre cuando la comunidad científica se pone de acuerdo en que ha sido falsado (véase consenso científico). Esta es una crítica importante al falsacionismo, pues cualquier enunciado observacional, por inocente que parezca, presupone ciertas concepciones acerca del mundo, y resulta imposible dejar de preguntarse si esas concepciones son científicas o no.

Dentro del falsacionismo metodológico, se pueden diferenciar el falsacionismo ingenuo inicial de Popper, el falsacionismo sofisticado de la obra tardía de Popper y la metodología de los programas de investigación de Imre Lakatos.

El problema de la inducción nace del hecho de que no se puede afirmar algo universal a partir de los datos particulares que ofrece la experiencia. Por muchos millones de cuervos negros que se vean, no será posible afirmar que «todos los cuervos son negros». En cambio, basta encontrar un solo cuervo que no sea negro para poder afirmar: «No todos los cuervos son negros». Por esa razón Popper introduce el falsacionismo como criterio de demarcación científica.

Popper en realidad rechaza el verificacionismo como método de validación de teorías. Su tesis central es que no puede haber enunciados científicos últimos, es decir, enunciados que no puedan ser contrastados o refutados a partir de la experiencia. La experiencia sigue siendo el método distintivo que caracteriza a la ciencia empírica y la distingue de otros sistemas teóricos.

Para Popper ni existen puntos de partida incuestionables "ni la racionalidad científica los requiere". El asunto de la verdad es, pues, cuestión del método de buscarla y del método de reconocer la falsedad. Aunque la ciencia es inductiva en primera instancia, el aspecto más importante es la parte deductiva. La ciencia se caracteriza por ser racional, y la racionalidad reside en el proceso por el cual sometemos a crítica y reemplazamos, o no, nuestras creencias. Frente al problema de la inducción Popper propone una serie de reglas metodológicas que nos permiten decidir cuándo debemos rechazar una hipótesis.

Popper propone un método científico de conjetura por el cual se deducen las consecuencias observables y se ponen a prueba. Si falla la consecuencia, la hipótesis queda refutada y debe entonces rechazarse. En caso contrario, si todo es comprobado, se repite el proceso considerando otras consecuencias deducibles. Cuando una hipótesis ha sobrevivido a diversos intentos de refutación se dice que está corroborada, pero esto no nos permite afirmar que ha quedado confirmada definitivamente, sino sólo provisionalmente, por la evidencia empírica.

Para los falsacionistas el científico es un artista en tanto que debe proponer audazmente una teoría que luego será sometida a rigurosos experimentos y observaciones. El avance en la ciencia está en falsar sucesivas teorías para así, sabiendo lo que no es, poder acercarse cada vez más a lo que es.

Las hipótesis que proponen los falsacionistas deben ser falsables, es decir, pueden ponerse a prueba y ser desmentidas por los hechos o por un experimento adverso. Para cumplir con esta condición, las hipótesis deben ser lo más generales posible y lo más claras y precisas posible. Una hipótesis falsable no sería «mañana tal vez llueva», ya que en ningún caso se puede falsar («mañana tal vez no llueva»).

Una hipótesis falsable sería «el planeta Mercurio gira en una órbita». Una hipótesis más general (y por lo tanto más falsable) sería «todos los planetas giran en una órbita». Y una hipótesis más precisa (y por lo tanto también más falsable) sería «todos los planetas giran en una órbita elíptica».

Los falsacionistas siempre prefieren las hipótesis o teorías que sean más falsables, es decir más susceptibles de ser demostrada su falsedad, mientras que no hayan sido ya falsadas. Así la ciencia progresaría a base de ensayo y error.

Una teoría será considerada falsable cuando se pueda dividir de manera precisa sus enunciados de base —referidos a acontecimientos observables— en dos subclases no vacías: la de todos los enunciados de base con los cuales está en contradicción —que enuncian lo que ella excluye o prohíbe—, sus falsadores potenciales, y la de todos los enunciados con los cuales no está en contradicción —los que enuncian lo que ella permite—.

El filósofo de la ciencia William Herbert Newton-Smith, expresa así su crítica:




</doc>
<doc id="9080" url="https://es.wikipedia.org/wiki?curid=9080" title="Titusville (Pensilvania)">
Titusville (Pensilvania)

Titusville es una ciudad ubicada en el Condado de Crawford, Pensilvania, Estados Unidos. En 2010, la ciudad tenía una población de 5.601 habitantes. En 1859, se extrajo por primera vez petróleo siendo Titusville la pionera, dando inicio a la industria petrolera moderna.

Titusville se encuentra ubicada en las coordenadas ..

De acuerdo con la Oficina del Censo de los Estados Unidos, la ciudad tiene una superficie total de 7,5 km². 7.5 km² de los cuales son tierra y no posee ninguna superficie cubierta por agua.

Según el censo de 2010, la ciudad cuenta con 5.601 habitantes. La densidad de población es de 746 hab/km² (1,931.2,2 hab/mi²). Hay 2.742 unidades habitacionales con una densidad promedio de 363,8 u.a./km² (943,7 u.a./mi²). La composición racial de la población de la ciudad es 97,58% Blanca, 1,20% Afroamericana o Negra, 0,29% Nativa americana, 0,28% Asiática, 0,00% De las islas del Pacífico, 0,08% de Otros orígenes y 0,57% de dos o más razas. El 0,85% de la población es de origen hispano o Latino cualquiera sea su raza de origen.

De los 2.523 hogares, en el 29,0% de ellos viven menores de edad, 42,8% están formados por parejas casadas que viven juntas, 14,2% son llevados por una mujer sin esposo presente y 38,9% no son familias. El 35,2% de todos los hogares están formados por una sola persona y 18,8% de ellos incluyen a una persona de más de 65 años. El promedio de habitantes por hogar es de 2,29 y el tamaño promedio de las familias es de 2,94 personas.

El 24,2% de la población de la ciudad tiene menos de 18 años, el 11,3% tiene entre 18 y 24 años, el 23,7% tiene entre 25 y 44 años, el 20,6% tiene entre 45 y 64 años y el 20,1% tiene más de 65 años de edad. La mediana de la edad es de 38 años. Por cada 100 mujeres hay 82,9 hombres y por cada 100 mujeres de más de 18 años hay 78,2 hombres.

La renta media de un hogar de la ciudad es de $25.945, y la renta media de una familia es de $36.679. Los hombres ganan en promedio $27.283 contra $20.458 para las mujeres. La renta per cápita en la ciudad es de $16.915. 15,9% de la población y 13,0% de las familias tienen entradas por debajo del nivel de pobreza. De la población total bajo el nivel de pobreza, el 21,3% son menores de 18 y el 9,8% son mayores de 65 años.



</doc>
<doc id="9081" url="https://es.wikipedia.org/wiki?curid=9081" title="Computadora doméstica">
Computadora doméstica

Se denomina computadora doméstica, computador doméstico u ordenador doméstico a la segunda generación de computadoras, que entraron en el mercado con el nacimiento del Altair 8800 y se extiende hasta principios de la década de 1990. Esto engloba a todas las computadoras de 8 bits (principalmente con CPU Zilog Z80, MOS Technology 6502 o Motorola 6800) y a la primera ola de equipos con CPU de 16 bits (principalmente Motorola 68000 e Intel 8086 y 8088). El término proviene de que llevaron la computadora de la industria al hogar. Aunque se suele excluir de ese grupo a los compatibles IBM PC, lo cierto es que hasta el triunfo definitivo y la adopción del término "computadora personal", tuvieron que competir con las líneas patrocinadas por Atari, Commodore y Apple Computer, por lo que algunos optan por incluir en la categoría de doméstico a los modelos más significativos de 16 bits, o al menos a los compatibles PC orientados al mismo mercado como la gama Tandy.

En cierta manera, guardando cierta similitud con las nuevas formas animales aparecidas en el periodo cámbrico, una gran cantidad de máquinas de todas las clases, incluyendo rarezas como el ordenador Jupiter Ace en lenguaje Forth aparecían en el mercado y desaparecían de nuevo. Algunos tipos de computadoras permanecieron durante más tiempo, otros evolucionaron tratando de mantener la compatibilidad (existen, por ejemplo, tarjetas de emulación Apple II para los primeros Mac). Sin embargo, al final de la década la mayoría fueron eliminados por la computadora personal compatible con IBM y las generaciones más nuevas de videoconsolas porque ambas utilizaban sus propios formatos incompatibles. La revolución IBM fue provocada en 1981 por la salida de la computadora personal de IBM 5150, el IBM PC.
Pese a ello, siguen existiendo grupos de usuarios que no renuncian a usar y mejorar sus viejos equipos dotándoles de las posibilidades modernas como disco duro o conexión a Internet. Aunque todas son muy activas (teniendo en cuenta la cada vez menor base de usuarios), destacan por mérito propio la de usuarios de MSX en los 8 bits y la de Commodore Amiga en los 16 bits (calificados por un redactor de MacByte como las "aldeas de irreductibles galos que resisten el asedio de las legiones Wintel"). Asimismo han dado nacimiento a una serie de aficiones que se suelen englobar bajo el término RetroInformática.

Una de las más conocidas es la emulación, normamente por software, pero también por hardware, de estas viejas computadoras y consolas en todo tipo de dispositivos: modernas computadoras personales, consolas, PDAs, teléfonos móviles, reproductores de DVD decodificadores de TDT, cámaras fotográficas digitales, etc.

Muchas de estas computadoras eran superficialmente similares y tenían usualmente un teclado de fabricación barata integrado en la carcasa que albergaba debajo la placa base con la CPU, una fuente de alimentación externa y como unidad de visualización más común un televisor. Muchas utilizaban casetes de audio compactos como mecanismo (notoriamente poco fiable) de almacenamiento de datos ya que las unidades de disco flexible eran muy caras en aquella época. Su bajo precio era común a la mayoría de las computadoras.

Aparte de casos como CP/M y OS-9, la mayoría tienen en ROM las rutinas básicas (que podrían considerarse su sistema operativo) junto con el lenguaje BASIC. Es lo que hoy suele conocerse como el "firmware" de los periféricos (una unidad de disco o lectora de DVD puede llevar integrada en su circuitería microcontroladores precisamente basados en las CPUs de estos equipos).



</doc>
<doc id="9084" url="https://es.wikipedia.org/wiki?curid=9084" title="Subportátil">
Subportátil

Una subportátil (del inglés "subnotebook"), es una computadora portátil con un tamaño menor, manteniendo las características. La denominación suele aplicarse a equipos que operan versiones completas de sistemas operativos de escritorio como Windows o GNU/Linux, en vez de sistemas específicos como Windows CE o Palm OS.

La Compaq LTE, lanzada en 1989, fue la primera en ser reconocida dentro de esta categoría, debido a sus reducidas dimensiones; 4,8x22x28 cm; similar a una hoja A4. En octubre de 1992 fue lanzada la IBM Thinkpad, la primera en incluir una pantalla de 10,4 pulgadas. También se puede incluir entre las pioneras a la NEC UltraLite, que data de 1988, ya que sus dimensiones eran similares a la Compaq LTE.

Computadoras más pequeñas como la Pocket PC y la Atari Portfolio, ambas lanzadas en 1989, fueron llamadas "pocket PC" o "handheld".

Otra de las primeras subportátiles fue la PowerBook 100, lanzada en 1991 por Apple, sus medidas eran 4,6x21,6x27,9 cm y pesaba 2,3 kg. Luego salió al mercado la Gateway Handbook, originalmente en 1992 y actualizada para utilizar un procesador 486 a fines de 1993, medía 246x150x41 y pesaba 1.4 kg. Apple a continuación lanzó la serie PowerBook Duo en octubre de 1992, que redujo el tamaño de la línea a 21,6x27,68x3,55 cm (8,5x10,9x1,4") y fue un ejemplo de portátil con pocas características onboard, pero que podían insertarse en un docking station para tener la funcionalidad completa de una portátil convencional

Otra de las primeras en su clase fue la Hewlett-Packard OmniBook 300, que fue promocionada como una "superportátil" en 1993. Medía 3,55x16,25x28,21 cm (1,4×6,4×11,1"), y estaba disponible con un disco flash opcional en lugar del disco rígido, para reducir el peso.

Toshiba, cuyo fuerte eran las portátiles en los 80, también entró al mercado ese año con la Portege T3400, afirmando que "Es la primera subportátil con toda la funcionalidad de una computadora más grande". La versión con pantalla monocromática de 21,33 cm (8,4") medía 4,31x20x24,89 cm (1,7 × 7,9 × 9,8") y pesaba 1,8 kg. Toshiba también introdujo la T3400CT que, en su momento, fue la primera subportátil con pantalla color. Luego lanzaron la Libretto 20, que tenía una pantalla de 15,5 cm (6,1") y un disco rígido de 270 mb. CNet afirmó sobre la Libretto 50t que "es la primera portátil con Windows 95 en los Estados Unidos que pesa menos de dos libras".

Compaq introdujo su propia subportátil en 1994, la Contura Aero, que tenía dos modelos. Uno con pantalla monocromática y otro con pantalla color, cuya cualidad principal era utilizar una batería que apuntaba a ser estándar (y no solo para productos Compaq). Tuvieron una corta vida en el mercado.

En 1997, Apple lanzó una relativamente liviana (995 gramos) PowerBook 2400c, de corta vida en el mercado. Fue diseñada junto con IBM y fabricada para Apple por la división japonesa de IBM, para reemplazar la antigua Powerbook Duo. Sin embargo, medía 4,82x21,6x26,7 cm (1,9 × 8,5 × 10,5"), por lo que era en realidad más grande que la Compaq LTE.

Las subportátiles son más pequeñas que las portátiles tradicionales pero más grandes que las UMPC. Generalmente poseen pantallas de menor tamaño, de entre 18 y 30 centímetros, y un peso variable. Debido al ahorro en peso y tamaño el precio suele ser mayor. Los sistemas operativos usados son Windows XP y Windows 7 y otros sistemas operativos basados en GNU/Linux

Suele confundirse a las subportátiles con las "netbooks", que son una categoría diferente de computadoras portátiles, aunque con productos superpuestos. Tienen en común el tamaño pequeño respecto de sus hermanas mayores. Pero lo que define a una netbook no es su tamaño, sino la reducción de componentes internos (fundamentalmente, carecen de unidad óptica) y consecuentemente la reducción de peso. Por otra parte, lo que define a una subportátil sí es su tamaño, pudiendo incluir unidad óptica y tener mayor peso. Por ejemplo, una netbook de 35,56 cm (14") no es una subportátil, mientras que una subportátil de 30 cm con unidad óptica y 3 kg de peso tampoco sería una netbook. Las netbooks por lo general son más baratas, ya que están optimizadas para usos más básicos, como funciones multimedia o navegación por internet. Por consiguiente, poseen procesadores mucho menos potentes pero con un consumo menor.



</doc>
<doc id="9086" url="https://es.wikipedia.org/wiki?curid=9086" title="Computadora portátil">
Computadora portátil

Se denomina computadora portátil, computador portátil u ordenador portátil, a un determinado dispositivo informático que se puede mover o transportar con relativa facilidad. Los ordenadores portátiles son capaces de realizar la mayor parte de las tareas que realizan los ordenadores de escritorio, también llamados «de torre» o simplemente PC, con similares capacidades y con la ventaja de su peso y tamaño reducidos. Además, también tienen la capacidad de operar por un período determinado por medio de baterías recargables, sin estar conectadas a una red eléctrica.

En algunos países también se les conoce por su término en inglés: "laptop", o en menor medida: "notebook"

La primera computadora portátil fue la Epson HX-20 desarrollada en 1981. Demostró sus grandes beneficios para el trabajo de científicos, militares, empresarios, y otros profesionales, que vieron la ventaja de poder llevar con ellos su computadora con toda la información que necesitaban de un lugar a otro.

La Osborne 1 salió al mercado comercial en abril de 1981. Tuvo éxito para el comercio mayorista con el formato actual, aunque entonces eran sumamente limitadas, incluso para la tecnología de la época.

En 1985 el Departamento I&D de CMET desarrolló Microtor I, un computador portátil basado en la CPU 6502. Fue el primero en incorporar un módem acústico, display de cristal líquido e impresora térmica. El desarrollo fue lanzado en la Feria Internacional de Santiago FISA de ese año.

En 1995, con la llegada de Windows 95, la venta de las portátiles se incrementó notablemente. En la actualidad rebasa las ventas de los equipos de escritorio. En el tercer trimestre de 2008, las ventas de los portátiles superaron por primera vez las de los equipos de escritorio, según la firma de investigación iSuppli Corp.

En 2005, miembros universitarios del MIT Media Lab, entre ellos Nicholas Negroponte y Lewis Stiward, introdujeron el portátil de 100 dólares y el proyecto "Un portátil por niño". Su objetivo era diseñar, fabricar y distribuir portátiles suficientemente baratos para proveer con uno a cada niño en el mundo, y que así pudieran tener acceso a conocimientos y métodos educativos modernos. Los ordenadores portátiles se venderian a los gobiernos y se repartirían a los niños en las escuelas estadounidenses y otros países, incluso en América Latina. El ordenador portátil se consideró el aparato más útil del mundo, porque era pequeño, era muy fácil de manejar, y era más ligero que los primeros diseños. Esta idea se adoptó en algunos países, entre ellos Uruguay (véase Plan Ceibal y OLPC (One Laptop Per Child)) y Argentina (véase Conectar Igualdad).

Una computadora portátil de escritorio o "desknote" es un híbrido entre una computadora de escritorio y una portátil tradicional. 

A finales de 2002, ECS introdujo la computadora portátil de sobremesa al mundo de las computadoras .

Una computadora portátil de sobremesa es una computadora portátil con la tecnología y especificaciones (incluyendo potencia y velocidad) más recientes de computadoras de escritorio; combina la unidad principal de computadora (p. ej. placa madre, CPU, disco duro, puertos externos, etc.) con una pantalla de cristal líquido (LCD). Por tanto, una computadora portátil de escritorio generalmente tiene un tamaño similar a un portátil grande, aunque a diferencia de estos, los "desknotes" requieren un teclado y un ratón externo.

Es un PC 2-en-1, también conocido como un Tablet 2-en-1, Laptop 2-en1, desmontables 2-en-1, laplet, o, simplemente, 2-in-1, es una computadora portátil que comparte características tanto de las tabletas y los ordenadores portátiles. Antes de la aparición de los 2-en-1, los términos convertibles e híbridos ya eran utilizados por los periodistas en tecnología. El término convertible se refiere típicamente a los PC 2-en-1 que presentaban algún tipo de mecanismo de ocultación de teclado que permite que el teclado pueda deslizarse o girar detrás de la parte posterior del chasis de la computadora, mientras que el término de híbrido normalmente se refiere a dispositivos que ofrecían la disponibilidad de acoplar un teclado.

«Lo crucial es que el crecer en una sociedad moderna ha sufrido tres cambios fundamentales: la modificación de las relaciones familiares, la restructuración de las fases de la niñez y de la juventud, y un crecimiento de los aparatos tecnológicos día a día.» La comunicación es fundamental especialmente para los jóvenes que viven en esta época de modernización.

Los aparatos tecnológicos como las computadoras portátiles, han facilitado esta comunicación de persona a persona,ya que a través de esta tecnología uno se puede comunicar sin necesidad de estar de frente a la otra persona.

El poder comunicarse a través de estos medios le han facilitado a muchos sus trabajos ya que tienen «mayor libertad y comodidad». Pero también existe una desventaja de estos avances en la tecnología como la computadora portátil, nos han hecho ser personas más individualistas y, en una sociedad tan competitiva como ésta, las personas deben desarrollarse tanto en el ámbito social como en el tecnológico, y conseguir un equilibrio entre ambos para poder progresar. 

El impacto social de la tecnología y la ciencia han sido soporte de la mejora en el bienestar de una población y su calidad de vida, sin descuidar los aspectos materiales relacionados con ellos, tales como la alimentación, la vivienda, el transporte, las comunicaciones y toda la actividad de infraestructura económica que resulta imprescindible para el desarrollo de un país y de sus personas.

Muchos de los componentes de un ordenador portátil son similares a los componentes de los ordenadores de escritorio, pero habitualmente son de menor tamaño, con componentes similares, algunos de los cuales se citan a continuación:

Common Building Block es el estándar de Intel y los principales fabricantes de portátiles para los componentes.


(Actualmente, los nuevos modelos suelen ser difíciles de reparar por usar componentes de la tecnología BGA, siendo difíciles de conseguir. Generalmente las fallas comunes son el chip de vídeo, transistores de regulación de voltaje o el procesador).

Son una nueva clase de portátiles que eliminan la unidad óptica, y reducen la potencia de otros componentes como la tarjeta gráfica, con el fin de disminuir el tamaño físico de las máquinas (y en ocasiones el coste), capaces de entrar en el bolsillo de un pantalón, como en el caso de los VAIO serie P.

Su capacidad de procesamiento es notablemente menor que los portátiles normales, por eso necesitan sistemas operativos diseñados específicamente, además del uso de almacenamiento remoto.

Muchas marcas, incluidas las más importantes, no diseñan y no fabrican sus ordenadores portátiles. En su lugar, un pequeño número de fabricantes de diseños originales (ODM) diseñan los nuevos modelos de ordenadores portátiles, y las marcas eligen los modelos que se incluirán en su alineación. En 2006, siete ODM principales fabricaron 7 de cada 10 ordenadores portátiles en el mundo, con el más grande (Quanta Computer) que tiene el 30 % de cuota del mercado mundial. Por lo tanto, a menudo son modelos idénticos a disposición tanto de una multinacional y de una empresa de bajo perfil ODM de marca local. La gran mayoría de ordenadores portátiles en el mercado son fabricados por un puñado de fabricantes de diseños originales (ODM).

Los más importantes son:

Entre los fabricantes de "notebooks", se incluyen:

Es usualmente la primera peculiaridad mencionada al comparar las computadoras portátiles con las de escritorio. La portabilidad física permite que la computadora portátil pueda ser usada en muchos lugares — no solo en el hogar y el trabajo, pero también durante el transporte o viaje, en cafeterías, auditorios, librerías, en el lugar donde se encuentra el cliente, etc. La portabilidad ofrece muchas ventajas.




Para solventar estos problemas se han fabricado varias soluciones, especialmente la primera. Existen en el mercado soportes para ordenadores portátiles regulables en altura, con lo cual se logra colocar el borde superior de la pantalla en la línea de los ojos. Desgraciadamente una solución así dificulta mucho utilizar el teclado, al estar mucho más alto el aparato y exigir tener los brazos en vilo permanentemente. Los portátiles equipados con varios puertos USB admiten el mismo tipo de teclado y de ratón que se utilizan para las máquinas de escritorio. Sin embargo, en muchos equipos del siglo XX y primera década del siglo XXI, solo podía conectarse un periférico por este sistema, posteriormente se ha ido incrementando el número. Dicho incremento ha permitido también conectar discos duros externos, pese a que estos dispositivos sufren una demora en el acceso a los datos por necesitar arrancar la primera vez que se les hace trabajar. También se han diseñado bases enfriadoras para portátiles donde se coloca encima la computadora, estas suelen llevar uno o más ventiladores que extraen o ingresan aire al interior de la base del portátil, estas funcionan por USB. Sin embargo queda desaprovechado un puerto USB por lo tanto esta solo utiliza los 5 voltios del mismo, además queda más alto el aparato y dificulta usar el teclado.




</doc>
