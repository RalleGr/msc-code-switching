<doc id="3276" url="https://es.wikipedia.org/wiki?curid=3276" title="Evolución humana">
Evolución humana

La evolución humana u hominización es el proceso de evolución biológica de la especie humana desde sus ancestros hasta la actualidad. El estudio de dicho proceso requiere de un análisis interdisciplinario en el que se complementen conocimientos desde ciencias como la genética, la antropología física, la paleontología, la estratigrafía, la geocronología, la arqueología y la lingüística.

El término "humano", en este contexto, se refiere a los individuos de la especie "Homo sapiens". Evidencia morfológica, genética y molecular han determinado que la especie viva más cercana a "Homo sapiens" es el chimpancé ("Pan troglodytes"). De esta manera, el estudio específico de la evolución humana es el estudio del linaje, o clado, que incorpora a todas las especies más cercanas a los humanos modernos que a los chimpancé. Evidencia molecular y paleontológica han estimado que el ancestro común entre "Homo sapiens" y "Pan troglodytes", vivió en África entre 5 a 7 millones de años (Ma). A partir de esta divergencia, dentro del linaje hominino continuaron emergiendo nuevas especies, todas ellas extintas actualmente a excepción de "Homo sapiens".

Al analizar el genoma humano actual se ha descubierto que en su proceso evolutivo hay varios hechos que destacar. Así, se observa por ejemplo que el "Homo sapiens" comparte casi el 99 % de los genes con el chimpancé y con el bonobo. Para mayor precisión, el genoma de cualquier individuo de nuestra especie tiene una diferencia de solo el 1,24% respecto al genoma de "Pan troglodytes" (chimpancés) y de 1,62% respecto al genoma de los gorilas.

El análisis genómico ha establecido el siguiente parentesco:

A partir del análisis genético, se ha postulado que la historia evolutiva humana, dentro de la genealogía humana se habría producido introgresión en varias ocasiones. Ejemplo de ello es el cromosoma Y actual más antiguo (cromosoma-Y A00), que se remontaría hasta el "Homo sapiens" arcaicos (hace unos 340 000 años aprox.).

También destaca el descubrimiento de la existencia de hibridación con otras especies homínidas más antiguas, tales como el "Homo neanderthalensis" (de un 1 % a un 4 % de genes neandertales por persona, principalmente en Europa), y con el homínido de Denisova (la población local que vive actualmente en Papúa Nueva Guinea, en el Sudeste Asiático, le debe al menos el 3 % de su genoma por persona a los homínidos de Denisova). Sin embargo, destaca que al analizar el porcentaje total de ADN del "Homo neanderthalensis" dentro de la población humana actual no africana, este porcentaje aumenta significativamente a un 20%; estando este genoma neandertal relacionado con genes que produjeron una "heterosis" a adaptaciones ambientales (como fenotipos de la piel), pero también implicado en enfermedades como la diabetes tipo 2, la enfermedad de Crohn, el lupus y la cirrosis biliar.

Igualmente destaca que los retrovirus endógenos humanos (HERV) ("Secuencia de ADN derivado de virus pertenecientes al grupo de los retrovirus") comprenden una parte significativa del genoma humano. Con aproximadamente 98000 fragmentos y elementos ERV, estos componen casi el 8% del genoma actual del ser humano, los cuales ha adquirido el ser humano en diferentes periodos temporales de su evolución.

Los primeros posibles homínidos bípedos (homininos) son "Sahelanthropus tchadiensis" (con una antigüedad de 7 millones de años y encontrado en el Chad, pero que genera dudas acerca de su adscripción a nuestra línea evolutiva), "Orrorin tugenensis" (con unos 6 millones de años y hallado en África Oriental) y "Ardipithecus" (entre 5,5-4,5 millones de años y encontrado en la misma región). Los fósiles de estos homínidos son escasos y fragmentarios y no hay acuerdo general sobre si eran totalmente bípedos. No obstante, tras el descubrimiento del esqueleto casi completo de un individuo de "Ardipithecus ramidus" apodado Ardi, se han podido resolver algunas dudas al respecto; así, la forma de la parte superior de la pelvis indica que era bípedo y que caminaba con la espalda recta, pero la forma del pie, con el dedo gordo dirigido hacia adentro (como en las manos) en vez de ser paralelo a los demás, indica que debía caminar apoyándose sobre la parte externa de los pies y que no podía recorrer grandes distancias.

Los primeros homínidos de los que se tiene la seguridad de que fueron completamente bípedos son los miembros del género "Australopithecus", de los que se han conservado esqueletos muy completos (como el de la famosa Lucy).

Este tipo de homininos prosperó en las sabanas arboladas del este de África entre 4 y 2,5 millones de años atrás con notable éxito ecológico, como lo demuestra la radiación que experimentó, con al menos cinco especies diferentes esparcidas desde Etiopía y el Chad hasta Sudáfrica.

Su desaparición se ha atribuido a la crisis climática que se inició hace unos 2,8 millones de años y que condujo a una desertificación de la sabana con la consiguiente expansión de los ecosistemas abiertos, esteparios. Como resultado de esta presión evolutiva, algunos "Australopithecus" se especializaron en la explotación de productos vegetales duros y de escaso valor nutritivo, desarrollando un impresionante aparato masticador, originando al "Paranthropus"; otros "Australopithecus" se hicieron paulatinamente más carnívoros, originando a los primeros "Homo".

No se sabe con certeza de qué especie proceden los primeros miembros del género "Homo"; se han propuesto "Australopithecus africanus", "A. afarensis" y "A. garhi", pero no hay un acuerdo general. También se ha sugerido que "Kenyanthropus platyops" pudo ser el antepasado de los primeros "Homo".

Clásicamente se consideran como pertenecientes al género "Homo" los homínidos capaces de elaborar herramientas de piedra. No obstante, esta visión ha sido puesta en duda; por ejemplo, se ha sugerido que "Australopithecus ghari" fue capaz de fabricar herramientas hace 2,5 millones de años. Las primeras herramientas eran muy simples y se encuadran en la industria lítica conocida como Olduvayense o Modo 1. Las más antiguas proceden de la región de Afar (Etiopía) y su antigüedad se estima en unos 2,6 millones de años, pero no existen fósiles de homínidos asociados a ellas.

De esta fase se han descrito dos especies, "Homo rudolfensis" y "Homo habilis", que habitaron África Oriental entre 2,5 y 1,8 millones de años atrás, que a veces se reúnen en una sola. El volumen craneal de estas especies oscila entre 650 y 800 cm³.

Esta es sin duda la etapa más confusa y compleja de la evolución humana. El sucesor cronológico de los citados "Homo rudolfensis" y "Homo habilis" es "Homo ergaster", cuyos fósiles más antiguos datan de hace aproximadamente 1,8 millones de años, y su volumen craneal oscila entre 850 y 880 cm³. Morfológicamente es muy similar a "Homo erectus" y en ocasiones se alude a él como «"Homo erectus" africano». Se supone que fue el primero de nuestros antepasados en abandonar África; se han hallado fósiles asimilables a "H. ergaster" (o tal vez a "Homo habilis") en Dmanisi (Georgia), datados en 1,8 millones de años de antigüedad y que se han denominado "Homo georgicus" que prueban la temprana salida de África de nuestros antepasados remotos.

Esta primera migración humana condujo a la diferenciación de dos linajes descendientes de "Homo ergaster": "Homo erectus" en Extremo Oriente (China, Java) y "Homo antecessor/Homo cepranensis" en Europa (España, Italia). Por su parte, los miembros de "H. ergaster" que permanecieron en África inventaron un modo nuevo de tallar la piedra, más elaborado, denominado Achelense o Modo 2 (hace 1,6 o 1,7 millones de años). Se ha especulado que los clanes poseedores de la nueva tecnología habrían ocupado los entornos más favorables desplazando a los tecnológicamente menos avanzados, que se vieron obligados a emigrar. Ciertamente sorprende el hecho que "H. antecessor" y "H. erectus" siguieran utilizando el primitivo Modo 1 (Olduvayense), cientos de miles de años después del descubrimiento del Achelense. Una explicación alternativa es que la migración se produjera antes de la aparición del Achelense.

Parece que el flujo genético entre las poblaciones africanas, asiáticas y europeas de esta época fue escaso o nulo. Parece que "Homo erectus" pobló Asia Oriental hasta hace solo unos 50 000 años (yacimientos del río Solo en Java) y que pudo diferenciar especies independientes en condiciones de aislamiento, como el caso del "Homo floresiensis" de la Isla de Flores (Indonesia), especie desaparecida hace 12 000 años, o el Hombre del ciervo rojo de China, desaparecido hace 11 000 años. Por su parte, en Europa se tiene constancia de la presencia humana desde hace casi 1 millón de años ("Homo antecessor"), pero se han hallado herramientas de piedra más antiguas no asociadas a restos fósiles en diversos lugares. La posición central de "H. antecessor" como antepasado común de "Homo neanderthalensis" y "Homo sapiens" ha sido descartada por los propios descubridores de los restos (Eudald Carbonell y Juan Luis Arsuaga).

Los últimos representantes de esta fase de nuestra evolución son "Homo heidelbergensis" en Europa, que supuestamente está en la línea evolutiva de los neandertales, y "Homo rhodesiensis" en África que sería el antepasado del hombre moderno.

Una visión más conservadora de esta etapa de la evolución humana reduce todas las especies mencionadas a una, "Homo erectus", que es considerada como una especie politípica de amplia dispersión con numerosas subespecies y poblaciones interfértiles genéticamente interconectadas.

La fase final de la evolución de la especie humana está presidida por tres especies humanas inteligentes, que durante un largo período convivieron y compitieron por los mismos recursos. Se trata del Hombre de Neanderthal ("Homo neanderthalensis"), la especie del homínido de Denisova y el hombre moderno ("Homo sapiens"). Son en realidad historias paralelas que, en un momento determinado, se cruzan.

El Hombre de Neanderthal surgió y evolucionó en Europa y Oriente Medio hace unos 230 000 años, presentando claras adaptaciones al clima frío de la época (complexión baja y fuerte, nariz ancha).

El homínido de Denisova vivió hace 40 000 años en los montes Altái y probablemente en otras áreas en las cuales también vivieron neandertales y sapiens. El análisis del ADN mitocondrial indica un ancestro femenino común con las otras dos especies hace aproximadamente un millón de años. La secuencia de su genoma ha revelado que habría compartido con los neandertales un ancestro hace unos 650 000 años y con los humanos modernos hace 800 000 años. Un molar descubierto presenta características morfológicas claramente diferentes a las de los neandertales y los humanos modernos.

Los fósiles más antiguos de "Homo sapiens" datan de hace unos 200 000 años (Etiopía). Hace unos 90 000 años llegó al Próximo Oriente donde se encontró con el Hombre de Neanderthal que huía hacia el sur de la glaciación que se abatía sobre Europa. "Homo sapiens" siguió su expansión y hace unos 45 000 llegó a Europa Occidental (Francia); paralelamente, el Hombre de Neanderthal se fue retirando, empujado por "H. sapiens", a la periferia de su área de distribución (Península ibérica, mesetas altas de Croacia), donde desapareció hace unos 28 000 años.

Aunque "H. neanderthalensis" ha sido considerado con frecuencia como subespecie de "Homo sapiens" ("H. sapiens neanderthalensis"), el análisis del genoma mitocondrial completo de fósiles de "H. neanderthalensis" sugieren que la diferencia existente es suficiente para considerarlos como dos especies diferentes, separadas desde hace 660 000 (± 140 000) años. (ver el apartado "Clasificación" en "Homo neanderthalensis").

Se tiene la casi plena certeza de que el Hombre de Neandertal no es ancestro directo del ser humano actual, sino una especie de línea evolutiva paralela derivada también del "Homo erectus/Homo ergaster" a través del eslabón conocido como "Homo heidelbergensis". El neandertal coexistió con el "Homo sapiens" y quizá terminó extinguido por la competencia con nuestra especie. Sin embargo, el análisis del genoma nuclear apunta a un aporte neandertal al acervo genético de los humanos modernos. Los euroasiáticos poseen entre el 1 y el 5 % de genes arcaicos por persona que se pueden atribuir a hibridación de Homo sapiens con Homo neandertales.

En cuanto al llamado Hombre de Cro-Magnon corresponde a las poblaciones de Europa Occidental de la actual especie "Homo sapiens".

Los parientes vivos más cercanos a nuestra especie son los grandes simios: el gorila, el chimpancé, el bonobo y el orangután.

Los fósiles más antiguos de "Homo sapiens" tienen una antigüedad de casi 200 000 años y proceden del sur de Etiopía (formación Kibish del río Omo), considerada como la cuna de la humanidad (véase Hombres de Kibish). A estos restos fósiles siguen en antigüedad los de "Homo sapiens idaltu", con unos 160 000 años.

Algunos datos de genética molecular concordantes con hallazgos paleontológicos, sostienen que todos los seres humanos descienden de una misma Eva mitocondrial o E.M., esto quiere decir que, según los rastreos del ADNmt - que solo se transmite a través de las madres-, toda la humanidad actual tiene una antecesora común que habría vivido en el noreste de África, probablemente en Tanzania (dada la mayor diversidad genética allí) hace entre 150 000 y 230 000 años (ver haplogrupos de ADN mitocondrial humano).

Estudios de los haplogrupos del cromosoma Y humano, concluyen que por línea paterna hay una ascendencia que llega hasta el Adán cromosómico, el cual habría vivido en el África subsahariana entre hace 60 000 y 90 000 años.

Otros indicios derivados de muy recientes investigaciones sugieren que la de por sí exigua población de "Homo sapiens" hace unos 74 000 años se redujo al borde de la extinción al producirse el estallido del volcán Toba, según la Teoría de la catástrofe de Toba, volcán ubicado en la isla de Sumatra, cuyo estallido ha dejado como rastro el lago Toba. Tal erupción-estallido tuvo una fuerza 3000 veces superior a la erupción del Monte Santa Helena en 1980. Esto significó que gran parte del planeta se vio cubierto por nubes de ceniza volcánica que afectaron negativamente a las poblaciones de diversas especies incluidas la humana. Según esta hipótesis llamada entre la comunidad científica Catástrofe de Toba, la población de "Homo sapiens" (entonces toda en África; la primera migración fuera de África fue en torno al año 70 000 a. C.) se habría reducido a solo alrededor de 1000 individuos. Si esto es cierto, significaría que el 'pool' genético de la especie se habría restringido de tal modo que se habría potenciado la unidad genética de la especie humana.

No todos están de acuerdo con esa datación. Después de analizar el ADN de personas de todas las regiones del mundo, el genetista Spencer Wells sostiene que todos los humanos que viven hoy descienden de un solo individuo que vivió en África hace unos 60 000 años.

Por todo lo antedicho queda demostrado el monogenismo de la especie humana y, consecuentemente, descartado el poligenismo, que servía de "argumento" a teorías racistas.

Junto a los hallazgos arqueológicos, los principales indicadores de la expansión del ser humano por el planeta son el ADN mitocondrial y el cromosoma Y, que son característicos de la descendencia por línea materna y paterna respectivamente.

Los humanos ya habrían comenzado a salir de África unos 90 000 años antes del presente; colonizando para esas fechas el Levante mediterráneo (Estos restos fósiles han sido atribuibles a tempranos "Homo sapiens", pero su relación real con los humanos modernos es muy discutible).

Australia y Nueva Guinea: la "Línea de Wallace" no significó para los "Homo sapiens" un límite insuperable para acceder a esta región. La llegada de humanos a Australia se data hace unos 50 000 años cuando pudieron fabricar rústicas almadías o balsas de juncos para atravesar el estrecho que separaba a Sahul de la región de la Sonda.

Europa: comenzó a ser colonizada hace solo unos 40 000 años, se supone que durante milenios el desierto de Siria resultaba una barrera infranqueable desde África hacia Europa, por lo que habría resultado más practicable una migración costera desde las costas de Eritrea a las costas yemeníes y de allí al subcontinente indio. La expansión por Europa coincide con la extinción de su coetáneo de entonces, el hombre de Neandertal.

Oceanía: la colonización de estas islas más próximas a Eurasia se habría iniciado hace unos 50 000 años, pero la expansión por esta MUG (macro-unidad geográfica) fue muy lenta y gradual, y hace unos 5000 años pueblos austronesios comenzaron una efectiva expansión por Oceanía, aunque archipiélagos como el de Hawái y Nueva Zelanda no estaban aún poblados por seres humanos hace 2000 o 1500 años (esto requirió el desarrollo de una apropiada técnica naval y conocimientos suficientes de náutica).

América: la llegada del hombre a América, se habría iniciado hace unos 20 000 o, al menos, 15 000 años, aunque no hay consenso al respecto. Durante las glaciaciones el nivel de los océanos desciende al grado que el "Viejo Mundo" y el "Nuevo Mundo" forman un megacontinente unido por el Puente de Beringia.

Cuando los ancestros del "Homo sapiens" y otros muchos primates vivían en selvas comiendo frutos, bayas y hojas, abundantes en vitamina C, pudieron perder la capacidad genética, que tiene la mayoría de los animales, de sintetizar en su propio organismo tal vitamina. Tales pérdidas durante la evolución han implicado sutiles pero importantes determinaciones: cuando las selvas originales se redujeron o, por crecimiento demográfico, resultaron superpobladas, los primitivos homininos (y luego los humanos) se vieron forzados a recorrer importantes distancias, migrar, para obtener nuevas fuentes de nutrientes (por ejemplo de la citada vitamina C).

Todos los cambios reseñados han sucedido en un periodo relativamente breve (aunque se mida en millones de años), esto explica la susceptibilidad de nuestra especie a afecciones en la columna vertebral y en la circulación sanguínea y linfática.

La cerebración y la corticalización son temas que requieren, por sí solos, artículos propios, dado el alcance y la importancia de dichos procesos. Aquí importa comentar de lo mínimo indispensable para comprender la evolución humana.

La cerebración tanto como la corticalización son fenómenos biológicos muy anteriores a la aparición de los homínidos, sin embargo en estos, y en especial en "Homo sapiens", la cerebración y la corticalización adquieren un grado superlativo (hasta el punto que Teilhard de Chardin enunció una curiosa teoría, la de la noósfera y noogénesis, esto es: teoría del pensar inteligente, que se basa en la evolución del cerebro).

El cerebro de "Homo sapiens", en relación a la masa corporal, es uno de los más grandes. Más llamativo es el consumo de energía metabólica (por ejemplo, la producida por la "combustión" de la glucosa) que requiere el cerebro: un 20% de toda la energía corporal, y aun cuando la longitud de los intestinos humanos evidencian los problemas que se le presentan.

En "Homo sapiens" el volumen oscila entre los 1200 a 1400 cm, el promedio global actual es de 1350 cm; sin embargo no basta un incremento del volumen, sino cómo se dispone; esto es: cómo está dispuesta la "estructura" del sistema nervioso central y del cerebro en particular. Por término medio, los "Homo neanderthalensis" pudieron haber tenido un cerebro de mayor tamaño que el de nuestra especie, pero la morfología de su cráneo demuestra que la estructura cerebral era muy diferente: con escasa frente, los neandertalenses tenían poco desarrollados los lóbulos frontales y, en especial, muy poco desarrollada la corteza prefrontal. El cráneo de "Homo sapiens" no solo tiene una frente prominente sino que es también más alto en el occipucio (cráneo muy abovedado), esto permite el desarrollo de los lóbulos frontales. De todos los mamíferos, "Homo sapiens" es el único que tiene la faz ubicada bajo los lóbulos frontales.

Sin embargo, aún más importante para la evolución del encéfalo parecen haber sido las mutaciones en el posicionamiento del esfenoides.

Se ha hecho mención en el apartado dedicado a la aparición del lenguaje articulado de la importancia del gen FOXP2; dicho gen es el encargado del desarrollo de las áreas del lenguaje y de las áreas de síntesis (las áreas de síntesis se encuentran en la corteza cerebral de los lóbulos frontales). El aumento del cerebro y su especialización permitió la aparición de la llamada lateralización, o sea, una diferencia muy importante entre el hemisferio izquierdo y el hemisferio derecho del cerebro. El hemisferio izquierdo tiene desarrollado en su corteza áreas específicas que posibilitan el lenguaje simbólico basado en significantes acústicos: el área de Wernicke y el área de Broca.

Es casi seguro que ya hace 200 000 años los sujetos de la especie "Homo sapiens" tenían un potencial intelectual equivalente al de la actualidad, pero para que se activara tal potencial tardaron milenios: el primer registro de conducta artística conocido se data hace solo unos 75 000 años, los primeros grafismos y expresiones netamente simbólicas fuera del lenguaje hablado se datan hace solo entre 40 000 y 35 000 años. Las primeras escrituras (" memoria segunda" como bien les llamara Roland Barthes) datan de hace entre 5500 o 5000 años, en el Valle del Nilo o en la Mesopotamia asiática.

Se ha dicho, también líneas antes, que "Homo sapiens" mantiene características de estructura craneal "primitivas" ya que recuerdan a las de un chimpancé infantil;, en efecto, tal morfología es la que permite tener la frente sobre el rostro y los lóbulos frontales desarrollados.

La cabeza de "Homo sapiens", para contener tal cerebro, es muy grande; aún en el feto y en el neonato, razón principal por la cual los partos son difíciles, sumada a la disposición de la pelvis.

Una solución parcial a esto es la heterocronía: el neonato humano está muy incompletamente desarrollado en el momento del parto; puede decirse (con algo de metáfora) que la "gestación en el ser humano no se restringe a los ya de por sí prolongados nueve meses intrauterinos, sino que se prolonga extrauterinamente hasta, al menos, los cuatro primeros años"; en efecto, el infante está completamente desvalido durante años, tan es así que, que entre los 2 a 4 años es cuando tiene lo suficientemente desarrolladas las áreas visuales del cerebro como para tener una percepción visual de su propio ser (Estadio del espejo descubierto por Jacques Lacan en la década de 1930). Ahora bien, si "Homo sapiens" tarda mucho en poder tener una percepción plena de su imagen corporal es interesante saber que es uno de los pocos animales que se percibe al ver su imagen reflejada (solo se nota esta capacidad en bonobos, chimpancés, y si acaso en gorilas, orangutanes, delfines y elefantes).

Tal es la prematuración de "Homo sapiens", que mientras un chimpancé neonato tiene una capacidad cerebral de un 65% de la de un chimpancé adulto, o la capacidad de "Australopithecus afarensis" era en el parto de un 50% respecto a la de su edad adulta, en "Homo sapiens" 'bebé' tal capacidad no supera al 25% de la capacidad que tendrá a los 45 años (a los 45 años aproximadamente es cuando se desarrolla totalmente el cerebro humano).

Pero no basta el desarrollo cronológico. Para que el cerebro humano se "despliegue" -por así decirlo- o desarrolle requiere de estimulación y afecto; de otro modo la organización de algunas de las áreas del cerebro puede quedar atrofiada.

Los Homininos, primates bípedos, habrían surgido hace unos 6 o 7 millones de años en África, cuando dicho continente se encontró afectado por una progresiva desecación que redujo las áreas de bosques y selvas. A partir de ello la primera teoría y más aceptada, es que como adaptación al bioma de sabana aparecieron primates capaces de caminar fácilmente de modo bípedo y mantenerse erguidos (East Side Story;). Más aún, en un medio cálido y con fuerte radiación ultravioleta e infrarroja algunas de las mejores soluciones adaptativas son la marcha bípeda y la progresiva reducción de la capa pilosa, lo que evita el excesivo recalentamiento del cuerpo. Hace 150 000 años el norte de África volvió a sufrir una intensa desertización lo cual significó otra gran presión evolutiva como para que se fijaran los rasgos principales de la especie "Homo sapiens".

Sin embargo, existen actualmente discrepancia respecto a la teoría de la aparición del bipedismo producto de la adaptación a la vida en la Sabana. La existencia de restos fósiles tales como los del género "Ardipithecus", con una forma de los dedos de los pies y una estructura pélvica que sugieren que andaban erguidos, y el posterior descubrimiento de los restos fósiles de "Danuvius guggenmosi", plantea un problema con esta teoría; y lleva a plantear la teoría de que el bipedismo podría haberse originado en los antepasados del ser humano mientras se movían aun sobre los árboles.

Para lograr la postura y la marcha erecta han tenido que aparecer importantes modificaciones:






Es evidente que la gran cantidad de modificaciones anatómicas que condujeron del cuadrupedismo al bipedismo requirió una fuerte presión selectiva. Se ha discutido mucho sobre la eficacia e ineficacia de la marcha bípeda comparada con la cuadrúpeda. También se ha notado que ningún otro animal de los que se adaptaron a la sabana al final de Mioceno desarrolló una marcha bípeda. 

Si tomamos en cuenta la teoría de que partimos de homínidos con un tipo de desplazamiento cuadrúpedo poco eficaz para largos desplazamientos en terreno abierto, como el que presentan los chimpancés. El modo en que se desplazan los chimpancés, apoyando la segunda falange de los dedos de las manos, no puede compararse a la marcha cuadrúpeda de ningún otro mamífero; en cambio la existencia de un desplazamiento bípedo anterior a habitar la sábana habría sido una ventaja para lograr habitar en este medio ambiente. 

Como los primeros homínidos de sabana probablemente se vieron obligados a desplazarse distancias considerables en campo abierto para alcanzar grupos de árboles situados a distancia; la marcha bípeda pudo ser una ventaja muy eficaz en estas condiciones ya que:

Hace años se argumentó que la liberación de las manos por parte de los primeros homínidos bípedos les permitió elaborar armas de piedra para cazar; lo cual habría sido el principal motor de nuestra evolución. Hoy está claro que la liberación de las manos (que se produjo hace más de 4 millones de años) no está ligada a la fabricación de herramientas, que aconteció unos 2 millones de años después, y que los primeros homininos no eran cazadores y que a lo sumo comían carroña esporádicamente.

Pero la bipedestación trajo una desventaja en la reproducción, ya que el hecho de pasar del cuadrupedismo al bipedismo conllevó un cambio anatómico de las caderas, con gran reducción del canal del parto que hacia más difícil y doloroso el alumbramiento, tal como se demuestra cuando se compara la cadera de un chimpancé promedio con la de un "Australopithecus" como "Lucy", quienes además presentan un tamaño de cerebro similar.

La postura bípeda dejó libres los miembros superiores que ya no tienen que cumplir la función de patas (excepto en los niños muy pequeños) ni la de braquiación, es decir, el desplazamiento de rama en rama con los brazos, aun cuando la actual especie humana, de la cintura hacia arriba mantenga una complexión de tipo arborícola.

Esta liberación de los miembros superiores fue, en su inicio, una adaptación óptima al bioma de sabana; al marchar bípedamente y con los brazos libres, los ancestros del hombre podían recoger más fácilmente su comida; raíces, frutos, hojas, insectos, huevos, reptiles pequeños, roedores y carroña; en efecto, muchos indicios hacen suponer como probable que nuestros ancestros fueran en gran medida carroñeros y, dentro del carroñeo, practicaran la modalidad llamada cleptoparasitismo, esto es, robaban las presas recién cazadas por especies netamente carnívoras; para tal práctica, nuestros ancestros debían haber actuado en bandas, organizadamente.

Los miembros superiores, siempre en relación con otras especies, se han acortado. Estos miembros superiores al quedar liberados de funciones locomotoras, se han podido especializar en funciones netamente humanas. El pulgar oponible es una característica heredada de los primates más antiguos, pero si en éstos la función principal ha sido la de aferrarse a las ramas y en segundo lugar aprehender las frutas o insectos que servían de alimento, en la línea evolutiva que desemboca en nuestra especie la motilidad de la mano, y en particular de los dedos de ésta, se ha hecho gradualmente más precisa y delicada lo que ha facilitado la elaboración de artefactos; aún (junio de 2005) no se tiene conocimiento respecto al momento en que la línea evolutiva comenzó a crear artefactos, es seguro que hace ya más de 2 millones de años "Homo habilis/Homo rudolfensis "realizaba toscos instrumentos que utilizaba asiduamente (en todo caso, los chimpancés, en estado silvestre, confeccionan "herramientas" de piedra, madera y hueso muy rudimentarias). El desarrollo de la capacidad de pronación en la articulación de la muñeca también ha sido importantísimo para la capacidad de elaborar artefactos.

El humano hereda de los prosimios la visión estereoscópica y pancromática (la capacidad de ver una amplia tonalidad de los colores del espectro visible); los ojos en la parte delantera de la cabeza posibilitan la visión estereoscópica (en tres dimensiones), pero si esa característica surge en los prosimios como una adaptación para moverse mejor durante la noche o en ambientes umbríos como los de las junglas, en "Homo sapiens" tal función cobra otro valor; facilita la mirada a lontananza, el otear horizontes, en este aspecto la visión es bastante más aguda en los humanos que en los otros primates y en los prosimios. Esto facilitará el hecho por el cual "Homo sapiens" sea un ser altamente visual (por ejemplo las comunicaciones mediante la mímica), y facilitará asimismo "lo imaginario".

Pese al conjunto de modificaciones morfológicas antes reseñadas, desde el punto de vista de la anatomía comparada, llama la atención una cuestión: "Homo sapiens" es un animal relativamente poco especializado. En efecto, gran parte de las especies animales ha logrado algún tipo de especialización anatómica (por ejemplo los artiodáctilos poseen pezuñas que les permiten correr en las llanuras despejadas), pero las especializaciones, si suelen ser una óptima adaptación a un determinado bioma, conllevan el riesgo de la desaparición de la especie especializada y asociada a tal bioma si éste se modifica.

La ausencia de tales especializaciones anatómicas ha facilitado a los humanos una adaptabilidad inusitada entre las demás especies de vertebrados para adecuarse a muy diversas condiciones ambientales.

Más aún, aunque parezca paradójico, "Homo sapiens" tiene características neoténicas. En efecto, la estructura craneal de un "Homo sapiens" adulto se aproxima más a la de la cría de un chimpancé que a la de un chimpancé adulto: el rostro es achatado ("ortognato" o de "bajo índice facial") y es casi inexistente el "torus" supraorbitario (en la humanidad actual apenas se encuentran vestigios de "torus" en las poblaciones llamadas australoides). De otro modo se puede decir que los arcos superciliares de "Homo sapiens" son "infantiles", delicados, el rostro aplanado o ligeramente prognato.

"Homo sapiens" es, por su anatomía, un animal muy vulnerable si se encuentra en condiciones naturales.

Asociado al hecho por el cual morfológicamente el ser humano tenga características que le aproximan a las de un chimpancé "niño" se encuentra el 'ortognatismo' y esto quiere decir, entre otras cuestiones, que los dientes de "Homo sapiens" son relativamente pequeños y poco especializados, las mandíbulas, por esto, se ha abreviado y hecho más delicadas, falta además el diastema o espacio en donde encajan los colmillos. La debilidad de las mandíbulas humanas las hace casi totalmente inútiles para la defensa a mordiscos ante un predador y, asimismo, son muy deficientes para poder consumir gran parte del alimento en su estado natural, lo que es uno de los muchos déficits corporales que llevan al humano a vivir en una sociedad organizada.

Hablar de la aparición del lenguaje humano, lenguaje simbólico, por lógica parecería implicar que hay que hablar previamente de la "cerebración", y eso es bastante cierto, pero el lenguaje humano simbólico tiene sus antecedentes en momentos y cambios morfológicos que son previos a cambios importantes en la estructura del sistema nervioso central. Por ejemplo, los chimpancés pueden realizar un esbozo primario de lenguaje simbólico basándose en la mímica (de un modo semejante a un sistema muy simple de comunicación para mudos).

Ahora bien, el lenguaje simbólico por excelencia es el basado en los significantes acústicos, y para que una especie tenga la capacidad de articular sonidos discretos, se requieren más innovaciones morfológicas, algunas de ellas muy probablemente anteriores al desarrollo de un cerebro lo suficientemente complejo como para pensar de modo simbólico. En efecto, observemos la orofaringe y la laringe: en los mamíferos, a excepción del humano, la laringe se encuentra en la parte alta de la garganta, de modo que la epiglotis cierra la tráquea de un modo estanco al beber e ingerir comida. En cambio, en "Homo sapiens", la laringe se ubica más abajo, lo que permite a las cuerdas vocales la producción de sonidos más claramente diferenciados y variados, pero al no poder ocluir completamente la epiglotis, la respiración y la ingesta deben alternarse para que el sujeto no se ahogue. El acortamiento del prognatismo que se compensa con una elevación de la bóveda palatina facilitan el lenguaje oral. Otro elemento de relevante importancia es la posición y estructura del hioides, su gracilidad y motilidad permitirán un lenguaje oral lo suficientemente articulado.

Estudios realizados en la Sierra de Atapuerca (España) evidencian que "Homo antecessor", hace unos 800 000 años, ya tenía la capacidad, al menos en su aparato fonador, para emitir un lenguaje oral lo suficientemente articulado como para ser considerado simbólico, aunque la consuetudinaria fabricación de utensilios (por toscos que fueran) por parte del "Homo habilis" hace unos dos millones de años, sugiere que en éstos ya existía un lenguaje oral articulado muy rudimentario pero lo suficientemente eficaz como para transmitir la suficiente información o enseñanza para la confección de los toscos artefactos.

Además de todas las condiciones recién mencionadas, imprescindibles para la aparición de un lenguaje simbólico, se debe hacer mención de la aparición del gen FOXP2 que resulta básico para la posibilidad de tal lenguaje y del pensamiento simbólico, como se verá a continuación.

Se han hipotetizado diferentes posibilidades respecto a la evolución futura del ser humano, entre ellos:

Una línea del pensamiento que asegura que la especie humana ha dejado de evolucionar de la misma forma que el resto de los seres vivos, por diferentes motivos.
Sin embargo, existen también otras posturas que consideran que son precisamente los adelantos tecnológicos los que impulsan actualmente la evolución humana, aunque de manera artificial no darwiniana. Por una parte, se ha propuesto que el entorno actual favorece la reproducción de las personas inteligentes, independientemente de su fuerza física o su estado de salud. Además, es posible que la ingeniería genética humana permita seleccionar las características genéticas de la descendencia.

Por otra parte, también se ha propuesto que en el futuro la tecnología posibilite a las personas vivir como cyborg o incluso como seres digitales dentro de cuerpos o estructuras completamente artificiales.

La taxonomía se encarga de la clasificación de los organismos. Por ende, la definición de especie es un aspecto fundamental para clasificar especímenes como pertenecientes a distintas o mismas especies. En organismos vivos es posible definir especies bajo el criterio de la capacidad que tienen distintos individuos de reproducirse y tener descendencia fértil (definición de especie biológica). Sin embargo, el registro fósil plantea más problemas, ya que es imposible ver el potencial reproductivo entre organismos extintos. Esto hace que el definir especies en paleontología sea extremadamente complejo. Un supuesto para definir y nombrar especies basadas en el registro fósil es a partir de la morfología; bajo esta premisa se espera que exista mayor variación morfológica entre especies que entre individuos de la misma especie.

En el estudio de la evolución humana, definir y nombrar especies es, como en toda disciplina paleontológica, no solo un fenómeno científico sino también psicológico. En ese sentido, podemos dividir a los paleontólogos en dos extremos de acuerdo a la forma de distinguir y definir especies en el registro fósil: los agrupadores ("lumpers") tratarán de definir unas pocas especies, con mayor variación inter-específica (es decir, dentro de la especie), mientras que los divisores ("splitters") definirán nuevas especies cuando existe una pequeña diferencia morfológica entre especímenes. Obviamente estos son dos extremos de un fenómeno y la mayoría de los paleontólogos se ubicarán en algún punto medio.




</doc>
<doc id="3277" url="https://es.wikipedia.org/wiki?curid=3277" title="Homo">
Homo

Homo (del latín "homo", 'hombre', 'humano') es un género de primates homínidos de la tribu Hominini. Se caracteriza por ser bípedo y plantígrado, con pies no prensiles con primer dedo alineado con los restantes, hipercefalización y una verticalización completa del cráneo. Agrupa a las especies consideradas humanas o que llevan el apelativo de "hombre", por lo que incluye al ser humano moderno ("Homo sapiens") y a sus más cercanos parientes. La antigüedad del género se estima en 2,5 millones de años ("Homo habilis"/"Homo rudolfensis"). Todas las especies, a excepción de "Homo sapiens", están extintas. Los supervivientes más recientes han sido "Homo neanderthalensis" en Europa —que se extinguió hace menos de 30 000 años—, "Homo floresiensis" en Indonesia —que, según las excavaciones realizadas por el Australian Research Council entre 2007 y 2014 y publicadas en marzo de 2016 en la revista "Nature", sobrevivió hasta hace poco más de 50 000 años atrás— y el llamado hombre de la cueva del ciervo en China —desaparecido hace cerca de 11 000 años—.

Entre las características que llevaron a separar "Homo habilis" del género "Australopithecus" destacan el tamaño del cráneo y, más importante aún, la capacidad de crear herramientas y conservarlas para un futuro uso.

La aparición del género "Homo" está sometida a varias interpretaciones. Las teorías ofrecidas por los expertos colocan a las diferentes especies "Homo" en una misma época, lo que hace difícil concretar la línea evolutiva. Por otro lado, estas interpretaciones son temporales y dependen de las investigaciones sobre los hallazgos fósiles encontrados hasta ahora, por lo cual, los nuevos descubrimientos producirán, inevitablemente, cambios en las teorías sobre la evolución humana desarrolladas sucesivamente. Aun así, se expone a continuación una versión más o menos aceptada por la mayoría de la comunidad académica.

Hasta ahora, las herramientas olduvayenses más antiguas atribuidas a "Homo", fueron encontradas en Gona (Etiopía) y datan de 2,6 a 2,5 millones de años. En Hadar (Etiopía) fue encontrado en 1994 un maxilar (AL 666-1), cuyas características indican que podría haber pertenecido a un "Homo" y que data de hace 2,33 millones de años.

Una de las teorías que explicarían el desarrollo del cerebro de "Homo", unido a la utilización de herramientas, es la teoría del tejido costoso, que sostiene que debido a una disminución de superficie de selva, los primeros "Homo" serían carroñeros y comerían el tuétano de los huesos, ayudándose de herramientas de piedra. Hay una relación inversa entre el grado de encefalización y el tamaño del aparato digestivo, por lo que este al acortarse, por una dieta más carnívora, pudo facilitar el aumento del tamaño del cerebro.

Cronológicamente, "Homo habilis" podría ser el primero de nuestros antepasados "Homo". Los restos más antiguos que con certeza pertenecen a esta especie, datan de hace unos 1,8 millones de años y su nombre, "hombre hábil", se debe a que se le adjudica cierto manejo en la elaboración de útiles de piedra. Se cree que convivió con los diferentes tipos de "Australopithecus" y que fue precisamente la presión ejercida por el género "Homo" lo que hizo desaparecer a los australopithecinos. Sin embargo, a pesar de la aparente superioridad tecnológica del "Homo habilis" sobre sus antecesores, las diferencias anatómicas eran relativamente escasas, aunque poseían un cerebro ligeramente más grande que los homininos anteriores.

Según las hipótesis tradicionales, el "H. habilis" evolucionó hace unos 1,5 millones de años hacia el "Homo erectus", especie que llegó a habitar gran parte del Viejo Mundo, desde África hasta China e Indonesia. Sin embargo se cree que tres especies de "Homo" habrían convivido entre hace 1,78 y 2,03 millones de años, Homo erectus, H. habilis y H. rudolfensis.

"H. erectus" comenzó a ser remplazado por formas arcaicas de "Homo sapiens" entre hace 400 y 250 mil años en distintas zonas geográficas. Este "Homo sapiens" arcaico, poseía un cerebro más grande aunque todavía mantenía similitudes físicas con el "Homo erectus".

Debido a los descubrimientos en la Sima de los Huesos en Atapuerca, en 1994, aparecen dos líneas evolutivas y a los estudios genéticos, hoy se tiene la perspectiva de dos líneas evolutivas. La primera desarrollada en el occidente de Asia y Europa desembocó en el "Homo heidelbergensis" y después de éste en el "Homo neanderthalensis"; y la segunda, desarrollada originalmente en el interior de África, devino en el "Homo rhodesiensis" o el "Homo helmei" y posteriormente en el "Homo sapiens". Esta teoría deja abierto el debate sobre el lugar de origen del "Homo erectus" y su relación con las especies "Homo ergaster" y "Homo georgicus".

La hipótesis de si las dos líneas son dos evolutivas es coherente con los estudios genéticos que sustentan la teoría del origen único en África del "Homo sapiens", y en cambio se opone a la hipótesis del origen multirregional, que supone la aparición simultánea del "H. sapiens" en Asia y África.

Tras el estudio del ADN mitocondrial y del genoma del homínido de Denisova, ha quedado claro además, que una tercera especie, diferente de "H. neandertalis" y de "H. sapiens", sobrevivió en Asia hasta hace 40 000 años. No se sabe aún cuál fue la relación de esta especie con "H. erectus" ni con fósiles de China y Java que antes se consideraban "sapiens" arcaicos. En cuanto a la especie "Homo floresiensis", que sobrevivió hasta hace unos 50.000 años, se consideraba inicialmente resultado de una adaptación especializada de "H. erectus" en un hábitat limitado, pero el estudio de sus extremidades indujo a pensar que no procedía de "Homo erectus", sino de algún otro homínido anterior, cuya dispersión en el sudeste de Asia aún no está documentada, o directamente de "Homo georgicus" o de una rama de "Homo habilis". También se ha documentado en Asia la presencia de los llamados Hombres de la cueva de los ciervos.


No todas las especies están plenamente aceptadas por la comunidad científica. Así "H. ergaster" es considerado por algunos autores como "H. erectus"; los escasos restos de "H. cepranensis" plantean numerosos interrogantes; "H. tsaichangensis" es el taxón asignado a una mandíbula fósil encontrada en Taiwán; se cuestiona si "H. georgicus" es una especie diferente o debe clasificarse como "H. erectus"; varios expertos consideran que "H. rhodesiensis" es una variante africana de "heidelbergensis"; "H. helmei" es una especie hipotética, a la que pertenecería un grupo de fósiles de transición hacia "H. sapiens"; se discute si "H. floresiensis" es una rama insular tardía de "H. habilis" o formas patológicas de "H. sapiens".
"Homo heildelbergensis" y "H. neanderthalensis" están muy emparentados y han sido considerados con frecuencia como subespecies de "H. sapiens", pero análisis de ADN mitocondrial de los fósiles del "H. neanderthalensis" sugieren que la diferencia existente es suficiente para denominarlos como dos especies diferentes. Y sobre el homínido de Denísova y los hombres de la cueva de los ciervos, es posible que se traten de híbridos o subespecies de "H. sapiens".




</doc>
<doc id="3278" url="https://es.wikipedia.org/wiki?curid=3278" title="Homo erectus">
Homo erectus

Homo erectus es un homínido extinto, que vivió entre 2 millones de años y 117 000 años antes del presente (Pleistoceno inferior y medio). Los "Homo erectus" clásicos habitaron en Asia oriental (China, Indonesia). En África se han hallado restos de fósiles afines que con frecuencia se incluyen en otra especie, "Homo ergaster"; también en Europa, diversos restos fósiles han sido clasificados como "Homo erectus", aunque la tendencia actual es la de reservar el nombre "Homo erectus" para los fósiles asiáticos.

Se considera que las poblaciones africanas (incluidas las referidas a "H. ergaster") son los antepasados ​​directos de varias especies humanas, como "H. heidelbergensis" y "H. antecessor", y el primero generalmente se considera que fue el antepasado directo de los neandertales, los denisovanos, y finalmente humanos modernos.

Las poblaciones asiáticas de "Homo erectus" son consideradas como posible ancestrales de "Homo floresiensis" y de Homo luzonensis . Como cronoespecie, el momento de su desaparición es, por lo tanto, una cuestión de controversia o incluso de convención. También hay varias subespecies propuestas con diferentes niveles de reconocimiento. El último registro conocido de "Homo erectus" reconocible morfológicamente son los especímenes del "Homo erectus soloensis" de Java, de alrededor de 117 000 a 108 000 años atrás. 

Una característica principal de "Homo erectus" es la «forma de la bóveda craneal, [...] relativamente baja y angular», con un marcado toro supraorbitario, «una frente marcadamente huidiza, [...] y la anchura mayor en una posición muy baja». El volumen craneal, muy variable, fue aumentando a lo largo de su dilatada historia. Tenía una capacidad mayor que la del "Homo habilis" y que la del "Homo georgicus" encontrado en Dmanisi. Los primeros restos que se encontraron del Hombre de Java muestran una capacidad craneal de 850 cm, mientras que los que se encontraron posteriormente llegan a los 1100 cm. Poseía una fuerte mandíbula sin mentón, pero de dientes relativamente pequeños. Presentaba un mayor dimorfismo sexual que en el hombre moderno.

Era muy robusto y tenía una talla elevada, hasta 1,80 m de medida.

Producía industria lítica, principalmente Achelense, y probablemente dominaba el fuego.

Entre 1891 y 1892 el médico anatomista holandés Eugène Dubois creyó encontrar el «eslabón perdido», hipotetizado por Ernst Haeckel, al descubrir algunos dientes sueltos, una calota craneal y un fémur —muy similar al del hombre moderno— en las excavaciones paleontológicas que realizaba en el río Solo cerca de Trinil, en el interior de la isla de Java (Indonesia). Dubois publicó estos hallazgos con el nombre de Pithecanthropus erectus (hombre-mono erguido) en 1894, pero más conocido popularmente como "El Hombre de Java" u "Hombre de Trinil". En la década de 1930 el paleontólogo alemán Ralpf von Koenigswald obtuvo nuevos fósiles, tanto de Trinil como de nuevas localidades como Sangiran (a unos 75 km), en total doce especímenes y, en 1938 von Koenigswald identificó claramente un magnífico cráneo de Sangiran, como ""Pithecanthropus"". No será hasta 1940 cuando Mayr atribuye todos estos restos al género "Homo" (Homo erectus erectus).

En China se encontraron otros yacimientos importantes de fósiles de esta especie como, por ejemplo, Lantian, Yuanmou, Yunxian y Hexian. Los investigadores también han encontrado gran número de utensilios fabricados por "H. erectus" en yacimientos como Nihewan y Bose, en China, y en otros lugares de antigüedad similar (al menos entre 1 millón y 250 000 años de antigüedad).

Luego se descubrieron, en Kenia el "Homo ergaster", que se puede considerar el "erectus" africano y probablemente la especie original. También se consideran relacionados con "H. erectus" en África, el cráneo KNM-ER 42700, de 1,55 millones años; y el Cráneo de Yaho (Chad) o "Tchadanthropus uxoris", de hace un millón de años.

En Dmanisi, República de Georgia, en el Cáucaso, fue descubierto el "Homo georgicus", que data de hace 1,8 millones de años, camino hacia el "erectus" de Extremo Oriente, pero relacionado descendiente del "Homo habilis", con lo cual se dibujó la ruta que siguieron los homínidos que dejaron África hasta dispersarse por Asia. Un diente encontrado en 2003 en la cueva Mohui (Guangxi, sur de China) que puede tener hasta 2 millones de años, así como los fósiles de Yuanmou (Yunnan, China, descubiertos en 1965) que datan de 1,7 millones de años, y el cráneo de Mojokerto (Java), que data de entre 1,8 y 1,49 millones de años, posiblemente estén relacionados con esta llegada temprana de "Homo" a Asia.
El conjunto de estos y otros hallazgos es clasificado actualmente dentro del género "Homo" y es designada la especie de los hombres de Java (hombre de Trinil) y Pekín, como "Homo erectus", que parece haber evolucionado en África como "Homo ergaster", a partir de las poblaciones anteriores de "Homo habilis", para a continuación dispersarse por gran parte de Asia desde hace unos 1,7 millones de años.

Fragmentos de un cráneo, identificado como perteneciente a un "H. erectus", fue encontrado en 2002 en Kocabaş, provincia de Denizli, Turquía. Este fósil data de hace 1,1 a 1,3 millones de años.

Fósiles de las épocas de los hombres de Java y Pekín fueron hallados entre 1936 y 1963 en Lantian, Shaanxi, China: los de Gongwangling datan de hace 800 000 - 750 000 años, aún con capacidad endocraneana de 780 cm³ y los de Chenjiawo, de una antigüedad de 530 000 años. También en 1994 en Tangshan, (Nanjing, Jiangsu), se encontró un cráneo de mujer de "Homo erectus" que data de 580 000-620 000 años antes del presente. 

En 2018 se publicaron los hallazgos en el yacimiento de Kalinga, al norte de la isla de Luzón (Filipinas), de industria lítica y restos de un rinoceronte con claras marcas de descarnación, datados en 709 000 años y atribuibles a "H. erectus". Este descubrimiento plantea el interrogante de si "H. erectus" adquirió la habilidad de navegar o la colonización de esta isla fue casual. 

Fósiles más recientes, clasificados como "H. erectus", han sido encontrados en Dali (Shaanxi, 1978), de hace 260 000-300 000 años, con capacidad endocraneana de 1120 cm³; en Jinniushan (Yingkou, Liaoning, 1974) de hasta 280 000 años y alta capacidad craneana; en Maba (Qujiang, Cantón, 1958) de 130 000 años, y en Dingcun (Xiangfen, Shaanxi) de 120 000-100 000 de antigüedad.

Los fósiles más recientes conocidos, atribuidos a la especie "H. erectus", proceden de la cuenca del río Solo, en Java, y fueron encontrados desde 1934 en Ngandong y en Sambungmacan (Sm-I con capacidad endocraneana de 1200 cm³), han sido datados entre 27 000 y 53 300 años antes del presente. Aunque la datación ha sido discutida y se afirma que los fósiles de Ngandong pueden tener más de 120 000 años, el hallazgo del homínido de Denisova y el estudio de su genoma concuerdan con la existencia simultánea con "H. sapiens", de otra especie de hominino en Asia, que la datación de los fósiles de Solo sugiere. De esta forma "H. erectus" habría sido una especie de gran éxito: se dispersó ampliamente y gozó de larga vida.

Desde el descubrimiento de "Homo erectus", los científicos se preguntan si esta especie era un antepasado directo de "Homo sapiens", debido a que las investigaciones hechas no eran suficientes para demostrarlo. Las últimas poblaciones de "H. erectus" —tales como las del río Solo en Java— pueden haber vivido hace solamente 50 000 años, simultáneamente con poblaciones de "H. sapiens", y se descarta que a partir de estas "últimas" poblaciones de "Homo erectus" haya evolucionado "H. sapiens".

Aunque poblaciones anteriores de "H. erectus" asiáticos podrían haber dado lugar a "H. sapiens", hoy se considera más probable que "Homo sapiens" hubiera evolucionado en África probablemente de poblaciones africanas de "H. erecto", luego los primeros "H. sapiens" habrían migrado desde el noreste de África hace menos de 100 000 años al Asia, donde tal vez se encontró con los últimos "H. erecto".

Una especie que posiblemente descienda tardíamente de "Homo erecto", es el pequeño "Homo floresiensis", aunque por el estudio de los huesos de la muñeca, los brazos y el hombro se considera más probable que descienda directamente de "Homo georgicus" o de "H. habilis".

En cuanto a la filogenia posible "Homo habilis" > "Homo erectus" aunque ésta aún es considerada posible, no parece que haya ocurrido de un modo "directo", sino, con más probabilidad, a través de un nexo de estas especies con "Homo rudolfensis". Lo concreto es que los hallazgos realizados en 2007 en Ileret, en la zona del lago Turkana, por Richard Leakey y Meave Leakey podrían indicar que los "H. habilis" vivieron en África hasta hace 1 440 000 años (cráneo KNM-ER 42703), lo cual confirmaría que ambas especies coexistieron por un lapso de por lo menos 500 000 años. Aunque hay autores como Erik Trinkaus que opinan que la convivencia no descarta que los "H. habilis" sean ancestros directos de los "H. erectus".

Un debate por resolver es si debe considerarse a "Homo ergaster" una especie diferente o si los fósiles clasificados como tal deben incluirse en "H. erectus". Su capacidad craneal oscila entre los 804 cm³ de KNM-ER 3883 y los 880 cm³ del niño de Nariokotome que data de 1,6 millones de años. Hay que tener en cuenta que un fósil africano posterior, que data de 1,55 millones de años, el cráneo KNM-ER 42700, por su morfología ha sido asignado a "H. erectus", a pesar registrar la baja capacidad craneal de 691 cm³, menor que la de "H. rudolfensis", que data de al menos 1,9 millones de años.

Se discute si la presencia de herramientas tecnología achelense o Modo 2 en África a partir de hace 1,65 millones de años y la ausencia del modo achelense por cientos de miles de años en los yacimientos en que se ha encontrado el "H. erectus" en Asia oriental y Java, es compatible con la identidad entre "H. ergaster" y "H. erectus" o si debe reconocerse una especie africana y otra asiática o euroasíatica.

En toda Eurasia los únicos yacimientos achelenses con una antigüedad cercana a la de los primeros yacimientos africanos, son el de Ubeidiya (Israel), que data de 1,3 a 1,4 millones de años y el de Attiramapakkam (Tamil Nadu, India), de hace más de un millón de años. En Asia oriental, la más antigua industria lítica similar a la achelense, fue encontrada en el sur de China, en Bose (Guangxi) y data de hace 803.000 ± 3000 años, mientras que en otros lugares de China y Java se mantiene exclusivamente el modo 1 olduvayense, con un relativo y sorprendente estancamiento tecnológico por muchísimo tiempo. 

Los fósiles asignados a "H. erectus" en África, sin embargo, se han multiplicado. Entre ellos se destacan: el Hombre de Buia (Eritrea), semejante a "H. ergaster", con antigüedad de más de un millón de años y capacidad craneal de cerca de 800 cm³; el cráneo de Daka (Etiopía) o BU-VP-2/66 de un millón de años de antigüedad y capacidad encefálica de 995 cm³; el cráneo descubierto en 1961 en Yaho (Angamma, Chad), de una edad estimada en un millón de años y que fue designado inicialmente como "Tchadanthropus uxoris"; tres mandíbulas y un parietal, descubiertos en Tighennif (Argelia), que datan de hace 800 000 años e inicialmente nombradas como "Atlathropus mauritanius"; y el cráneo OH 9 de Olduvai u Hombre de Chellean, de hace 1,15 millones de años y capacidad craneal de 1065 cm³, propuesto como prototipo de la especie "Homo louisleakeyi". Todos estos fósiles están asociados con herramientas achelenses.

Aunque los fósiles africanos de este conjunto comparten con "H. erectus" de Asia oriental un grueso toro supraorbital, cráneo alargado, así como la capacidad cerebral, presentan algunas características que los diferencian y apuntan hacia "H. sapiens": en el cráneo de Buia, por ejemplo, el tamaño y la colocación de los huesos parietales, con una posición alta de su parte más externa, lados curvados y más amplios en la parte superior. Algunos piensan que los cráneos de Daka y Buia podrían ser antepasados o relacionarse con el europeo "Homo antecessor", cuyo carácter de especie diferente de "H. erectus", también es discutida.


Hay menor consenso en la denominación de las siguientes subespecies:







</doc>
<doc id="3279" url="https://es.wikipedia.org/wiki?curid=3279" title="Homo antecessor">
Homo antecessor

Homo antecessor (del latín, "homo" ‘hombre’ y "antecessor" ‘explorador’) es una especie extinta perteneciente al género "Homo", considerada la especie homínida más antigua de Europa y probable ancestro de la línea "Homo heidelbergensis"- "Homo neanderthalensis". Vivió hace unos 900 000 años (Calabriense, Pleistoceno temprano). Eran individuos altos, fuertes, con rostro de rasgos arcaicos y cerebro más pequeño que el del ser humano actual.

Existe un intenso debate entre arqueólogos y antropólogos en torno a la clasificación taxonómica de "H. antecessor" y las relaciones de este con el resto de especies del género "Homo", debido a la falta de un cráneo adulto completo y que la mayoría de los especímenes conocidos de "H. antecessor" representan etapas juveniles. Sus descubridores, junto con el respaldo de otros expertos, consideran a "H. antecessor" como una especie diferente, sugiriendo que fue un vínculo evolutivo entre "H. ergaster" y "H. heidelbergensis", y que por tanto es también antepasado de "Homo neanderthalensis"; siendo a su vez el último ancestro común entre los humanos modernos y los neandertales. Por otra parte algunos científicos consideran que los restos de "H. antecessor" pertenecen a individuos de "H. heidelbergensis", que habitó Europa entre los 600 000 y los 250 000 años en el Pleistoceno.

La definición de esta especie es fruto de los más de ochenta restos hallados desde 1994 en el nivel TD6 del yacimiento de Gran Dolina en la sierra de Atapuerca, y que datan de hace al menos 900 000 años, según mediciones paleomagnéticas.

De acuerdo con sus descubridores, entre los caracteres anatómicos de estos homínidos cabe destacar un conjunto de rasgos muy primitivos en el aparato dental, que llevaron a establecer una relación entre estos y los homínidos africanos del Pleistoceno Inferior. Una mandíbula muy bien conservada de una mujer "H. antecessor", de entre 15 y 16 años, recuperada del yacimiento de la Gran Dolina tiene similitudes muy claras con las del Hombre de Pekín ("Homo erectus"), lo que sugiere un origen asiático de "H. antecessor". Sin embargo, el patrón de desarrollo y erupción de los dientes es prácticamente idéntico al de las poblaciones modernas. 
La morfología facial es similar a la de "Homo sapiens", con orientación coronal y ligera inclinación hacia atrás de la placa infraorbital que determina la presencia de una fosa canina muy conspicua. El borde inferior de esta placa es horizontal y ligeramente arqueado. El arco superciliar es en doble arco y la capacidad encefálica, estimada a partir de un fragmento incompleto de hueso frontal, indica una cifra superior a los 1000 cm³. Mientras que "H. erectus" tiene un patrón de crecimiento facial que es similar al observado en los primeros "Homo" y los "Australopithecus", tanto en "H. antecessor" como "H. sapiens" predomina la resorción ósea durante el crecimiento facial. Las similitudes entre la anatomía subnasal de "H. antecessor" y "H. sapiens" sugieren que la "modernización" de la cara estaba ya claramente en marcha en "H. antecessor".

La morfología de la mandíbula recuerda a la de ciertos homínidos muy posteriores, del Pleistoceno Medio, de la especie "Homo heidelbergensis", como los de la Sima de los Huesos, también de Atapuerca. El esqueleto postcraneal indica una cierta gracilidad en comparación con la mayor robustez del Hombre de Neanderthal de la segunda mitad del Pleistoceno Medio.

La mayoría de individuos alcanzarían una altura de entre 160 y 185 centímetros, con un peso de entre 60 y 90 kilogramos.

En la actualidad, la validez de esta denominación como especie diferente es defendida por sus descubridores y otros expertos, que consideran que "H. antecessor" precede a "H. heidelbergensis" y por tanto es también antepasado de "H. neanderthalensis"; sin embargo, parte de la comunidad científica la considera una simple denominación, no específica, para referirse a restos encontrados en Atapuerca, que ellos asignan a la especie "H. heidelbergensis" o bien, la consideran una variedad de "Homo erectus/Homo ergaster".

De un molar del nivel TD6 del yacimiento Gran Dolina se han podido extraer proteínas del esmalte. Un estudio, publicado en 2020, compara el proteoma del esmalte de este diente con el obtenido de restos de "Homo erectus" de Dmanisi (Georgia), y concluye que "Homo antecessor" pertenencía a un grupo afín al grupo al que pertenecen los neandertales, denisovanos y "Homo sapiens".

En 1994 se descubrió en Ceprano (Italia) la parte superior del cráneo de un "Homo" de caracteres primitivos, datada entre 800 000 y 900 000 años de antigüedad, y para el que se propuso en 2003 la especie "Homo cepranensis". Sin embargo, las características filogenéticas, cronológicas, arqueológicas y geográficas de los restos hallados en Ceprano (Italia) han llevado a algunos autores a atribuirlos a "Homo antecessor". La comparación directa de ambos conjuntos de fósiles no es posible, pues pertenecen a distintas partes anatómicas o corresponden a individuos de edades diferentes, pero ambos conjuntos poseen en común el presentar rasgos intermedios entre las poblaciones de "Homo" primitivas de África y las más recientes de "Homo heidelbergensis" de Europa. Por otro lado, tanto la datación absoluta como la industria lítica de Ceprano son coherentes con las obtenidas del nivel TD6 de la Gran Dolina. Si realmente estos restos corresponden a la misma especie de "Homo", la denominación "H. antecessor" tiene prioridad nomenclatorial.

En 2010 se informó sobre el descubrimiento de herramientas en Happisburgh, Norfolk, Inglaterra, las cuales se cree fueron usadas por "H. antecessor" y datan de hace 780 000 años antes del presente.

En la playa de Happisburgh, en la misma formación geológica en la que han aparecido los restos anteriores, se han encontrado numerosas huellas de pisadas dejadas por al menos cinco individuos, un adulto y varios juveniles, sobre los sedimentos fangosos de un estuario. Las icnitas, datadas entre 1 000 000 y 780 000 años, se atribuyen tentativamente a "Homo antecessor" atendiendo a la edad similar de los restos de Atapuerca.

El arqueólogo Eudald Carbonell de la Universidad Rovira i Virgili en Tarragona, y el paleoantropólogo Juan Luis Arsuaga de la Universidad Complutense de Madrid descubrieron restos de "Homo antecessor" en el sitio fosilífero de la Gran Dolina, en la Sierra de Atapuerca, en lo que ahora es el este de Burgos. Los restos de" H. antecessor" se han encontrado en el nivel 6 (TD6) del sitio Gran Dolina.

En 1994 y 1995 se descubrieron más de 80 fragmentos de hueso de seis individuos. El sitio también incluyó aproximadamente 200 herramientas de piedra y 300 huesos de animales. Se encontraron herramientas de piedra incluyendo un cuchillo tallado en piedra junto con los antiguos restos de homínidos. Todos estos restos databan de al menos 900 000 años de antigüedad. Los restos mejor conservados son un maxilar superior y un hueso frontal de un individuo que murió a la edad de 10-11 años.

En marzo de 2008 se dieron a conocer nuevos restos que se atribuyeron inicialmente a "Homo antecessor", concretamente parte de una mandíbula de un individuo de unos 20 años y 32 herramientas de sílex de tipo olduvayense (modo 1), datados en 1,2 millones de años de antigüedad, haciendo retroceder considerablemente la presencia de homínidos en Europa. Los restos fueron hallados en 2007 en la Sima del Elefante, yacimiento situado a unos 200 metros de la Gran Dolina. Sin embargo el estudio detallado de la mandíbula, que presenta características en la dentición y la sínfisis que la aproximan a los "Homo" más antiguos de África y de Dmanisi (Georgia) pero con algún carácter derivado (parte interna de la sínfisis), han llevado posteriormente a identificar estos restos como "Homo" sp. (pertenecientes al género "Homo" pero sin precisar la especie), ya que probablemente pertenezcan a una nueva especie aún por definir, y cuyas relaciones filogenéticas, sin más datos, son aún imprecisas.

En el municipio de Atapuerca se ha tenido un crecimiento significativo a nivel económico, demográfico y social con el impacto generado con el yacimiento arqueológico y sus servicios asociados. El 15% de la población activa ya se dedica al turismo, esta "terciarización" de su economía ha revertido el despoblamiento y ha hecho que vuelva a crecer el número de habitantes, rejuveneciendo la población y situando la media de edad en 42 años. Directamente relacionado, la creación de empleo que se ha derivado de este tipo de acciones, ha tenido un impacto social positivo en la sociedad.




</doc>
<doc id="3282" url="https://es.wikipedia.org/wiki?curid=3282" title="Homínido">
Homínido

El término homínido está relacionado con la evolución humana y tiene los siguientes usos:



</doc>
<doc id="3285" url="https://es.wikipedia.org/wiki?curid=3285" title="Idioma catalán">
Idioma catalán

El idioma catalán o valenciano es una lengua romance hablada por unos diez millones de personas, incluyendo hablantes no nativos, en Cataluña, parte de la Comunidad Valenciana, las Islas Baleares, Andorra, la Franja de Aragón, la comarca murciana de El Carche, el Rosellón, la ciudad sarda de Alguer, y en pequeñas comunidades de todo el mundo (entre las cuales destaca la de Argentina, con 198 000 hablantes). Tiene unos diez millones de hablantes, de los cuales alrededor de la mitad son nativos; su dominio lingüístico, con una superficie de 68 730 km² y 13 529 127 habitantes (2009), incluye 1687 términos municipales. Como las otras lenguas romances, el catalán desciende del latín vulgar que hablaban los romanos que se establecieron en Hispania durante la Edad Antigua.

"Català" es el autoglotónimo y la denominación oficial en las comunidades autónomas de Cataluña y las Islas Baleares, en Andorra, en la ciudad italiana de Alguer, en la Franja oriental de Aragón, y es la denominación tradicional en la región francesa del Rosellón. "Valencià" es el autoglotónimo y la denominación oficial en la Comunidad Valenciana y la tradicional en la comarca murciana de El Carche

El grado de uso y de oficialidad del catalán varía mucho según el territorio, que va desde la nula oficialidad en Francia a ser el único idioma oficial en Andorra, pasando por la cooficialidad en tres comunidades autónomas españolas. Según un estudio del Instituto de Estadística de Cataluña en 2008, el idioma catalán es el segundo más usado habitualmente en Cataluña, tras el idioma castellano, que supera al catalán no solo como lengua habitual, sino también como lengua materna y de identificación, aunque el catalán es el más usado en cinco de las siete áreas funcionales de Cataluña y el 80 % de la población lo sabe hablar. Cada aspecto y contexto social del uso del idioma en Cataluña es estudiado por la Generalidad de Cataluña con el fin de fomentar su uso, donde es la lengua principal en la educación, en las administraciones públicas y en los medios de comunicación públicos; además, esta invierte anualmente en la promoción del catalán tanto en Cataluña como en otros territorios.
El catalán recibe diversos nombres según donde se habla, tales como "mallorquí" o "fragatí". De todos estos, el nombre "valenciano" ("valencià") es, aparte de "catalán", el único con uso oficial (en la Comunidad Valenciana). Para evitar las connotaciones que el nombre "catalán" tiene para algunos, se han buscado diferentes propuestas, nombres de compromiso como "catalán-valenciano-balear" o el neologismo "bacavés", o singularizar la denominación del catalán de Cataluña con el neologismo "catalunyès". Otro nombre que se usó antiguamente es "llemosí" (lemosín).

La lengua catalana se habla en cuatro países diferentes:
Una denominación que intenta englobar a toda esa área lingüística, no exenta de discusiones por el carácter ideológico que ha ido adquiriendo, es la de Países Catalanes, acuñada a finales del siglo XIX y popularizada por Joan Fuster en su obra "Nosaltres els valencians" («Nosotros los valencianos», 1962).

La característica sociolingüística más destacada del catalán es que en todos los territorios en los que se habla se encuentra en situación de bilingüismo social: con el francés en el Rosellón, con el italiano (más que con el sardo) en Alguer, y con el castellano en el resto de su ámbito lingüístico, incluyendo Andorra, donde es la única lengua oficial según la Constitución andorrana pero donde también se habla el castellano y el francés.

(% de la población de 15 y más años).

(% de la población de 15 y más años).

En Cataluña se hablan varias lenguas, siendo las principales el catalán y el castellano o español. De acuerdo con el Estatuto de Autonomía, ambos idiomas, junto con el occitano (en su variante aranesa), son oficiales. Además, se considera al catalán lengua propia de Cataluña, en tanto que el occitano se considera lengua propia del Valle de Arán. Generalmente los ciudadanos de Cataluña son bilingües y conocen las dos lenguas principales aunque difieren respecto al idioma que tienen por lengua materna. Según los datos de 2018, el 99,8 % de los ciudadanos de Cataluña sabe hablar castellano en tanto que el 81,2 % sabe hablar catalán. Además, el uso por cada hablante de uno u otro idioma depende con frecuencia del ámbito social en el que se exprese. Según los datos del Instituto de Estadística de Cataluña, en 2018 el 36 % de los ciudadanos de Cataluña mayoritariamente utiliza el catalán como lengua habitual, el 49 % mayoritariamente el castellano, el 7 % ambas y el 0,03 % el aranés. El 6 % de la población residente de Cataluña utiliza habitualmente otras lenguas. El aranés es la lengua materna del 22,4 % de la población del Valle de Arán, la propia del 27,1 % y la habitual del 23,4 %.

En Cataluña se hablan los dos bloques principales de la lengua catalana. El oriental tiene como máximo exponente el dialecto central, que se habla en las comarcas del norte de Tarragona, Barcelona, y Gerona, en cuya región pirenaica se atisban rasgos de catalán septentrional. El occidental es el propio de las comarcas occidentales de Cataluña (provincia de Lérida y sur de las tarraconenses) y muestra rasgos similares al valenciano, con el que forma un continuo y en cuya intersección se encuentra el tortosino. El catalán es especialmente preponderante fuera del área metropolitana de Barcelona y del Campo de Tarragona. La Generalidad ha venido desarrollando legislación que promueve y protege el uso social del catalán. En 2018, la catalana era considerada la lengua materna del 34,3 % de los ciudadanos de Cataluña (31,5 % exclusivamente catalán y 2,8 % bilingüe en castellano), la propia del 43,2 (36,3 % exclusivamente catalán y 6,9 % bilingüe en castellano) y la de uso habitual del 43,5 (36,1 % exclusivamente catalán y 7,4 % bilingüe en castellano). Los porcentajes de los hablantes bilingües incluyen aquellos que consideran conjuntamente al castellano y al catalán como lengua materna, propia o de uso habitual..

El castellano que se habla en Cataluña tiene rasgos dispares, sin mostrar un dialecto específico. Algunos hablantes del castellano que son originarios de otras regiones de España muestran rasgos fonéticos y dialectales propios de su tierra de origen, mientras que otros neutralizaron esos rasgos, ya sea a voluntad, por contacto con catalanohablantes, por la influencia de los medios de comunicación, etc. Los catalanohablantes que hablan castellano muestran algunas influencias de su lengua materna y sus rasgos son, a veces, estereotipados como los propios de todos los ciudadanos de Cataluña al hablar en lengua castellana. El castellano no se considera lengua propia de Cataluña por su origen, puesto que proviene del centro de la península. No obstante, su uso fue aumentando desde el siglo XVI en ciertos sectores de la población, fundamentalmente en entornos urbanos, vinculado al mundo editorial y al mercado lector. A pesar de todo, la situación fue en la práctica de monolingüismo hasta finales del siglo XIX, cuando se inicia la escolarización masiva de la población (en castellano), según un estudio del "Centre de Recerca en Sociolingüística i Comunicació" de la universidad de Barcelona. Los inmigrantes castellanoparlantes llegados a Cataluña generalmente adoptaban el catalán, por ser "la lengua de la calle", hasta al menos los años 1930. Esta situación cambió radicalmente con la gran ola migratoria que tuvo lugar entre 1950 y 1975, cuando en poco tiempo los 2 millones y medio de catalanes "nativos" se vieron incrementados con un millón y medio de inmigrantes sin formación, provenientes de toda España y en especial de las áreas más pobres. 

En 2018, el castellano era la lengua materna del 55,5 % de los ciudadanos de Cataluña (52,7 % exclusivamente castellano y 2,8 % bilingüe en catalán), la propia del 53,5 (46,6 % exclusivamente castellano y 6,9 % bilingüe en catalán), y la habitual del 56 % (48,6 % exclusivamente castellano y 7,4 % bilingüe en catalán). Los porcentajes de los hablantes bilingües incluyen aquellos que consideran conjuntamente al castellano y al catalán como lengua materna, propia o de uso habitual.

La comunidad inmigrante o foránea instalada en Cataluña a menudo mantiene su lengua materna para comunicarse con sus familiares o hablantes de su mismo idioma que residan también en el territorio. Aparte del castellano hablado por los inmigrantes procedentes del resto de España y de Hispanoamérica, destacan sobre todo el árabe y el rumano, si bien su número se extiende considerablemente en ciudades que, como Barcelona, con habitantes de hasta 131 nacionalidades, muestra un amplio repertorio lingüístico, de los que además de los citados, destaca el bereber, el francés, el portugués, el alemán, el ruso, y el inglés. La encuesta estadística de usos lingüísticos de la "Generalidad" realizada en 2003 revelaba también la presencia importante de hablantes de gallego.

En Cataluña el factor más importante del bilingüismo social es la inmigración desde el resto de España en el siglo XX. Se ha calculado que, sin migraciones, la población de Cataluña habría pasado de unos 2 millones de personas en 1900 a 2,4 en 1980, en vez de los más de 6,1 millones censados en esa fecha (y superando los 7,4 millones en 2009); es decir, la población sin migración habría sido solamente el 39 % en 1980.

Actualmente, según el Instituto de Estadística de la Generalidad, el idioma catalán es el segundo más usado habitualmente en Cataluña, tras el idioma castellano, que supera al catalán no solo como lengua habitual, sino también como lengua materna y de identificación, aunque el catalán es el más usado en 5 de las 7 áreas funcionales de Cataluña. Según los datos del Instituto de Estadística de Cataluña (Idescat) para el año 2018, el catalán es la lengua habitual del 43 % de la población de Cataluña (un 36 % como lengua habitual y un 7 % bilingüe con el castellano). En términos absolutos, 2 779 300 personas tienen al catalán como idioma habitual ( como lengua habitual y 474 200 bilingüe con el castellano), frente a 3 566 700 (57,90 %) que tienen al castellano. Respecto a la encuesta anterior de Idescat se observa un aumento en cifras absolutas del uso habitual del catalán ( frente a 2 850 000 de 2003) pero un retroceso en valores relativos (47,6 % frente a 50,7 %). Se observa también un crecimiento, tanto en valores absolutos como relativos, de los habitantes de Cataluña que usan habitualmente tanto el castellano como el catalán (en valores absolutos se produce casi una triplicación, pasando de 265 400 a 736 700; en valores relativos, el crecimiento es del 4,7 % al 12 %), lo que se traduce en la disminución de las personas que utilizan habitualmente solo el catalán.

Los habitantes de Cataluña que tienen lengua materna al catalán son menos que aquellos que la usan de forma habitual. Según Idescat, en 2008, 2 186 000 personas (34,60 %) tenían al catalán como lengua materna (frente a 3 625 500, 58 %, que tienen al castellano). Estas cifras incluyen a 236 500 que también tienen como lengua materna al castellano. Se muestran fenómenos similares a los descritos con la lengua habitual en relación con los datos de 2003: estabilización de hablantes que tienen al catalán como lengua materna (2 177 800 en 2003 frente a los citados 2 186 000 de 2008), con retroceso en términos relativos (38,70 frente a 34,6 %); aumento de los hablantes que tienen como lenguas maternas al castellano y al catalán (se pasa de 141 600 a 236 500 hablantes; aumento del 2,5 al 3 % en términos relativos), con la consiguiente disminución del número de personas que consideran exclusivamente al catalán como lengua materna.

En sentido similar, los ciudadanos de Cataluña que consideran al catalán como lengua de identificación son menos (pero no de forma tan acusada) que los que la usan de forma habitual. Según los datos de Idescat, en 2008, 2 770 500 personas (49,3 %) tenían al catalán como lengua de identificación (por 3 410 300, 55,30 % que lo hacían con el castellano). Estas cifras incluyen a 542 800 personas que también se identifican con el castellano. Se muestran los mismos fenómenos que los relativos a lengua habitual y materna con respecto a los datos de 2003: ligero aumento del número de hablantes que se identifican con el catalán (2 770 500 en 2003 frente a los citados 2 770 500 de 2008), con retroceso en términos relativos (49,3 frente a 46 %); aumento de los hablantes que se identifican con el catalán y el castellano (se pasa de 278 600 a 542 800 hablantes; aumento del 5 al 8,8 % en términos relativos), con la simétrica disminución del número de personas que se identifican exclusivamente con el catalán.

En cuanto al conocimiento escrito, según datos oficiales de 2007, el 56,3 % de la población catalana sabía escribir en catalán.

Se observa que el catalán se mantiene como lengua habitual en términos absolutos entre 1980 y 2008, aunque de manera lenta, en vez de retroceder como en la Comunidad Valenciana o Rosellón. El retroceso en términos relativos que se ha producido en el periodo 2003-2008 se debe a la importante llegada de inmigrantes a Cataluña, más de medio millón en dicho periodo, un 36 % de los cuales tiene al castellano como lengua materna. Otros estudios, como "La segunda generación en Barcelona: un estudio longitudinal" (marzo de 2009), aplicado al área metropolitana de Barcelona, señalan que aproximadamente el 80 % de los inmigrantes de la zona de estudio considerada prefiere utilizar el castellano, un porcentaje superior al de los que lo hablan por su origen. Los autores creen que es así por haberse instalado los inmigrantes en barrios donde el castellano es más usual.

Con respecto a la distribución territorial (datos de 2008), el uso del catalán (exclusivo, sin contar a quienes hablan también habitualmente en castellano) es predominante en las áreas funcionales de las Comarcas gerundenses (50,9 %), Tierras del Ebro (72,8 %), Poniente (64,4& %), Cataluña Central (56,7 %) y Alto Pirineo y Arán (60,1 %), donde el catalán como lengua habitual (exclusiva) es usado por más del 50 % de la población. Los grados menores de uso se dan en el Campo de Tarragona (33,1 %) y el Área Metropolitana de Barcelona (27,8 %). Respecto a los datos de 2003, se observa un retroceso porcentual de los hablantes habituales exclusivos de catalán en todas las áreas, que va del 8,8 % en Poniente al 16,5 del Campo de Tarragona.

La Generalidad de Cataluña ha llevado a cabo una labor de fomento y potenciación del uso del catalán como la lengua prioritaria en Cataluña. Tanto el estatuto de autonomía de 1979 como el de 2006 definen al catalán como lengua propia de Cataluña. El estatuto de 2006 indica además que:
La parte catalanoparlante de la Comunidad Valenciana tiene una realidad sociolingüística compleja y plural, debido por un lado a la inmigración desde zonas castellanohablantes de España, y por otro lado a la sustitución lingüística, principalmente en las ciudades de Valencia y Alicante. Actualmente predomina el castellano en las zonas urbanas y el valenciano en las zonas rurales; la provincia de Castellón y el sur de la provincia de Valencia son las zonas donde más se habla el valenciano, y la provincia de Alicante y el área metropolitana de Valencia son las zonas donde menos se habla. 

En la Comunidad Valenciana existen dos lenguas de amplio uso y conocimiento entre la población autóctona: el valenciano y el castellano, declaradas como idiomas oficiales según el Estatuto de Autonomía. El valenciano está considerado como lengua propia, si bien el castellano es la lengua empleada por la mayor parte de la población y los medios de comunicación, pero ambas cuentan con una amplia tradición literaria y cultural. Asimismo, en la Comunidad Valenciana existen dos predominios lingüísticos oficiales territorialmente para el castellano y el valenciano, definidas por la Ley de uso y enseñanza del valenciano, basándose en la distribución lingüística del siglo XIX.

El predominio castellano se concentra básicamente en una franja interior central y occidental, y un exclave (Aspe y Monforte del Cid) en el extremo sur, comprendiendo en ella el 25 % del territorio y en la que reside el 13 % de la población. En dicho territorio se emplean unas variantes dialectales que son la churra y la murciana, si bien esta última no está consensuada por todos los lingüistas debido a las diferencias dialectales de la Vega Baja del Segura y Villena con la zona oriental de Murcia. El valenciano tiene en esta zona un grado de conocimiento limitado.

El predominio del valenciano se concentra en la costa y comarcas contiguas, abarca un 75 % del territorio y en ella reside el 87 % de la población. En esta área, el 36,4 % de la población afirma utilizarlo preferentemente en el hogar, según un sondeo del 2005, frente a un 54,5 % que usa preferentemente el castellano. Por zonas, el uso del valenciano en el hogar es predominante en las zonas de concentración urbana media o baja del área, mientras que el castellano lo es en las grandes concentraciones urbanas. El castellano que se habla en esta área es a grandes rasgos un estándar con algunos rasgos fonéticos y léxicos propios o influenciados por el valenciano.
En la parte de la Comunidad Valenciana donde es lengua propia, existe un proceso de sustitución lingüística del valenciano (o catalán) por el castellano. Este se ha completado casi del todo en la ciudad de Alicante y está muy avanzado en la de Valencia, aunque aún no es importante en áreas rurales. Hasta época reciente, muchos hablantes estaban en situación próxima a la diglosia, lo que significa que usaban el catalán solo en situaciones informales, mientras que en las situaciones institucionalizadas se usaba en exclusiva el castellano. Pero desde que se enseña en las escuelas ha aumentado mucho su conocimiento escrito, aunque en las últimas décadas ha retrocedido mucho su uso social. Además ha habido una importante inmigración desde otras partes de España, lo que ha contribuido al predominio estadístico del castellano en la comunidad.

El catalán es la lengua propia de las Islas Baleares (así definida en su Estatuto de Autonomía) y cooficial, junto al castellano, por serlo esta en todo el Estado. El caso balear es parecido al de Cataluña, ya que aquí el factor principal en la expansión del castellano ha sido la inmigración, en mucha mayor medida que la sustitución lingüística. 
La situación sociolingüística del catalán en las Islas Baleares es diferente según la isla y la zona, en Menorca y en la mayor parte de Mallorca, en la Parte Foránea, es donde más se habla el catalán, y en Palma y en Ibiza es donde menos se habla. Además, en las zonas turísticas, se hablan el inglés y el alemán. Aunque con menos impacto, el italiano es también un idioma frecuente, sobre todo en Formentera, que cuenta con un alto índice de turismo de esa nacionalidad.

De acuerdo con los datos del censo del Instituto de Estadística de les Illes Balears de 2001 y los datos sociolingüísticos del IEC de 2002, con respecto al catalán la población se distribuiría de la siguiente manera: sabe hablarlo el 74,6 %, lo entiende el 93,1 %, sabe leerlo el 79,6 %, sabe escribirlo el 46,9 %. Por su parte, según una encuesta realizada en 2003 por la Secretaría de Política Lingüística, de los 1 113 114 habitantes de Baleares lo entienden 749 100 (el 93,1 %), lo saben hablar 600 500 (el 74,6 %), y es la lengua habitual para 404 800 personas (el 45,7 %).

El catalán es la lengua propia y tradicional de este territorio llamado Franja Oriental de Aragón. Es hablado por un sector significativo de la población. siendo el territori donde el conocimiento oral del catalán es el más universalizado. Lo saben hablar el 80,2% de los adultos, que representan 33.743 hablantes de catalán en la Franja (datos de la última encuesta, de 2014). El año 2004 eran 42.000 personas, el 88,8% de la población adulta. La reducción se debe sobre todo a causas demográficas, más que a causas sociolingüísticas. En todo Aragón hay 55.513 hablantes de catalán, según datos censales. A pesar de todo ello, no es una lengua oficial ni en la Franja ni en Aragón, y tiene una presencia casi nula en las instituciones públicas, muy limitada en la enseñanza, donde solo es posible estudiarla como optativa, en la administración y en actos públicos en general.

El caso andorrano es parecido al de Cataluña, ya que aquí el factor principal en la expansión del español y del francés ha sido la inmigración en mucha mayor medida que la sustitución lingüística. Además es el único territorio donde el catalán es el único idioma oficial y es el único estado del mundo que lo tiene como idioma oficial.

Que un estado independiente lo tenga por idioma oficial le permite al catalán tener una cierta presencia en el ámbito internacional. El ingreso de Andorra en la ONU, el 28 de julio de 1993, permitió por primera vez en la historia el uso del catalán en una asamblea de esta organización. También Andorra llevó por primera vez la lengua catalana al Festival de la Canción de Eurovisión en 2004 con Marta Roure y la canción «Jugarem a estimar-nos».
El idioma oficial de Andorra es el catalán, aunque la realidad lingüística es el resultado de la gran transformación demográfica que ha vivido el país desde la segunda mitad del siglo XX: en 1940 las personas extranjeras residentes en el país representaban solo el 17 %; en 1989 representaban el 75,7 % —máximo histórico— y en 2007 son alrededor del 65 %. También suele oírse el francés, por la situación fronteriza del Principado. Recientemente ha habido un incremento significativo de la población de habla portuguesa.

De acuerdo con el Servicio de Política Lingüística del gobierno andorrano, el catalán es la lengua materna del 49,4 % de la población de nacionalidad andorrana, pero solo el 29,9 % de la población total lo utiliza. Por el contrario, el castellano es la lengua materna más extendida entre la población del Principado. A pesar del crecimiento de la población de nacionalidades andorrana y portuguesa, el 43,4 % declaró que el castellano es su lengua materna. El estudio muestra que en los últimos años se ha producido un deterioro de la posición de la lengua catalana en favor del castellano.

En cuanto a la alfabetización, el 100 % de los ciudadanos saben leer y escribir. El castellano es la lengua que ocupa el primer lugar respecto a la proporción de la población que aprendió a leer y escribir, seguida del francés, y en tercer lugar el catalán.

Según el Observatorio Social de Andorra del Instituto de Estudios Andorranos, los usos lingüísticos en Andorra son los siguientes:

En el Rosellón, como en la mayor parte de Francia, el proceso de sustitución lingüística del idioma local por el francés está muy avanzado, con el clásico patrón por el cual el idioma cambia primero en las ciudades y solo más tarde en el campo. Actualmente cerca de la mitad de la población entiende el catalán y entre un 20 % y un 30 % es capaz de hablarlo, pero su conocimiento escrito y su uso social es inferior al 10 %.

El real decreto francés de Luis XIV del 2 de abril de 1700, con fecha de aplicación de 1 de mayo del mismo año, prohibió drásticamente el uso de la lengua catalana en documentos oficiales, notariales y de otro tipo, bajo pena de invalidar el contenido. Desde entonces, el francés continúa siendo la única lengua oficial, y la única que se utiliza en la enseñanza pública.

Los últimos datos sociolingüísticos de los que dispone la Generalidad de Cataluña (2004) reflejan que el francés es la lengua mayoritaria en el Rosellón, con una presencia minoritaria del catalán. Habitualmente, a pesar del extendido ambiente de catalanidad, el 92 % de la población habla francés, el 3,5 % catalán, ambos idiomas un 1 % y el 3,5 % habla otras lenguas.

En cuanto a usos lingüísticos en diversos ámbitos cabe señalar que el 80.5 % de los nacidos en el Rosellón hablan únicamente francés en el ámbito familiar en contraposición con un 17,3 % en el que el catalán está presente. Además, el ámbito del uso del catalán se reduce cada vez más en las nuevas generaciones y en los inmigrantes. solo un 6.3 % de los estudiantes del Rosellón hablan en catalán entre ellos y un 0.5 % lo hace cuando va al médico. Sin embargo, la conciencia lingüística no ha disminuido y un 62.9 % de los habitantes del Rosellón cree que los niños deberían aprender catalán.

Asimismo, según las estadísticas oficiales, el 65,3 % de la población entiende el catalán, un 31,7 % lo sabe leer y un 10,6 % lo sabe escribir. Estos resultados se deben evaluar paralelamente con los deseos en relación a la lengua catalana. Así, un 57,9 % de la población querrían hablar catalán y un 62,9 % desearía que sus hijos aprendan catalán.

La ciudad de Alguer (Cerdeña, Italia) tiene una población de 43 831 habitantes (2009). La población de la ciudad fue sustituida por colonos catalanes de las comarcas del Penedès y del Camp de Tarragona tras un levantamiento popular contra el rey Pedro el Ceremonioso. A finales de 1354, la población quedó muy reducida por el hambre, después de medio año de asedio, y los resistentes alguereses fueron expulsados o esclavizados.

Por eso hasta hace relativamente poco la lengua mayoritaria de la ciudad era el catalán, en su variedad algueresa. Desde el fin de la Segunda Guerra Mundial, sin embargo, la inmigración de gente que habla sardo y la escuela, la televisión y los periódicos de habla italiana han hecho que menos familias lo hayan transmitido a los hijos. En 2004 los usos lingüísticos de la población de Alguer eran los siguientes:

Hasta hace relativamente poco, la mayoría de los habitantes de la zona hablaban alguerés, una variedad dialectal del catalán con influencias del sardo y el italiano. El catalán fue reemplazado por el castellano como lengua oficial durante el siglo XVII, y, en el siglo XVIII, por el italiano.

En 1990 un 60 % de la población local aún entendía el alguerés hablado aunque, desde hace un tiempo, pocas familias lo han transmitido a los hijos. Aun así, la mayoría de los alguereses de más de 30 años lo saben hablar y diferentes entidades promueven la lengua y la cultura, como por ejemplo Òmnium Cultural, el Centre María Montessori y la Obra Cultural de l'Alguer.

Los últimos datos sociolingüísticos de la Generalidad de Cataluña (2004) reflejan que para el 80.7 % de la población de Alguer la lengua vernácula es el italiano, la primera lengua del 59.8 % de la población y la habitual del 83.1 %. El catalán es la primera lengua para el 22.4 % de la población pero es menos de un 15 % quien la tiene como lengua habitual o la considera propia. La tercera lengua, el sardo, muestra un uso más bajo.

El Estado italiano, en virtud de la Norma en materia de tutela de las minorías lingüísticas históricas, prevé el uso de lenguas como el catalán en la administración pública, en el sistema educativo así como la puesta en marcha de trasmisiones radiotelevisivas por parte de la RAI siempre que el estatuto de lengua sujeta a tutela sea solicitado al consejo provincial por municipios en los que lo solicite el quince por ciento de la población. Anteriormente, el Consejo Regional de Cerdeña había reconocido la igualdad en dignidad de la lengua sarda con la italiana en toda la isla, así como con otras lenguas de ámbito más reducido, entre las que cita al catalán, en la ciudad de Alguer. La ciudad, por su parte, promulga su tutela y normalización en sus estatutos.

Debido a su origen catalán, los alguereses denominan a su ciudad Barceloneta, y existen vínculos culturales, fomentados por la Generalidad de Cataluña dentro de su programa de inversión en la extensión de las lengua y cultura catalanas por el mundo, entre Cataluña y Alguer. Entre sus tradiciones vivas destaca el "Cant de Sibil·la", que se canta en Nochebuena (como sucede en Mallorca).

En los últimos años ha habido un resurgimiento de la música cantada en la lengua local. Entre los más renombrados protagonistas de esta nueva ola destacan artistas como la cantante Franca Masu.

Al igual que las demás lenguas románicas de la Península, el catalán es notable por su uniformidad y las variantes dialectales no son demasiado divergentes ni comprometen la comprensión mutua. La división dialectal usada actualmente es la que Manuel Milá y Fontanals propuso ya en el año 1861: el bloque dialectal oriental (que incluye los dialectos central, insulares y de Francia) y el bloque dialectal occidental (que incluye el valenciano y el noroccidental). Pero incluso entre estos grandes grupos la diferencia es pequeña, y las discrepancias afectan más bien a la fonética (las vocales no acentuadas), que por tanto no se reflejan en la escritura, y a pequeñas variantes morfológicas y léxicas.

Los bloques dialectales no se pueden delimitar con exactitud porque entre uno y otro siempre hay una franja de transición, más o menos amplia (excepto en los insulares, obviamente). Además, ningún bloque es del todo uniforme: cualquiera de los que hay se pueden dividir en varios dialectos. Ateniéndose a ello, la lengua catalana se puede dividir en dos bloques dialectales y en subdialectos:

Existen dos estándares principales para la lengua catalana; el regulado por el Institut d'Estudis Catalans, el estándar general, que tiene como fundamento la ortografía establecida por Pompeu Fabra pero con las características del catalán central más aproximado al de Barcelona, no influenciado por el castellano, y el regulado por la Acadèmia Valenciana de la Llengua, estándar utilizado en la Comunidad Valenciana y el Carche, que parte de la tradición lexicográfica, literaria, y la realidad lingüística genuina valenciana, así como la normativización consolidada, a partir de las llamadas Normas de Castellón. 

El estándar del IEC, aparte de tener como base las características del catalán central, toma también características de otros dialectos considerándolos como estándar. Aun así, la diferencia más notable de ambos estándares es la acentuación de muchas "e" tónicas, por ejemplo: "francès" o "anglès" (IEC) / "francés" o "anglés" (AVL), "cafè" (IEC) / "café" (AVL), "conèixer" (IEC) / "conéixer", "comprèn" (IEC) / "comprén" (AVL) (inglés, francés, café, conocer, comprende). Eso es debido a la diferente pronunciación de algunas "e" tónicas, especialmente las Ē ("e" largas) y las Ǐ ("i" breves) tónicas del latín, en ambos bloques del catalán (en el bloque oriental se pronuncian y en el occidental se pronuncian [e]). A pesar de esto, el estándar de la AVL mantiene el acento abierto "è", sin pronunciarse abierto en el bloque occidental, en algunas palabras como son: "què", "València", "sèsam", "plèiade", "bèstia", "sèrie" y "època" (sin embargo, ya muchas palabras se admiten con el acento agudo; como "café", "francés", "interés", "estrés", "véncer", "paréixer", "església", "sépia", "cérvol", "éter", "mercé", "féiem", "créiem", etc.).

También hay otras divergencias como el uso de tl en algunas palabras por la AVL en vez de tll como en "ametla" / "ametlla" (almendra), "espatla" / "espatlla" (hombro) o "butla" / "butlla" (bula), el uso de los determinantes demostrativos elididos ("este", "eixe") al igual que los reforzados ("aquest", "aqueix") o el uso de muchas formas verbales comunes en el valenciano, y muchas de ellas extendidas por el bloque occidental, como las formas del subjuntivo o la escritura de los incoativos tanto con -ix- como con -eix- o el uso preferente del morfema -e de la 1ª persona singular del presente de indicativo en la 1ª conjugación (-ar), ya que las otras conjugaciones el morfema es -ø: ""jo compre"", ""jo tem"", ""jo dorm"".

En las Islas Baleares se usa el estándar del IEC adaptado al marco dialectal por la sección filológica de la Universidad de las Islas Baleares, el órgano consultivo del Gobierno balear. De esta manera, por ejemplo, el IEC indica que tanto es correcto escribir ""cantam"" como ""cantem"" (cantamos) y la Universidad determina que la forma preferente en las Islas tiene que ser ""cantam"" incluso en los ámbitos formales. Otra característica del estándar balear es la escritura de la 1ª persona del singular del presente de indicativo, donde no hay desinencia: ""jo cant"" (yo canto), ""jo tem"" (yo temo), ""jo dorm"" (yo duermo).

En Alguer, el IEC ha adaptado el estándar a la variedad algueresa. En este estándar se puede encontrar, entre otras características, el artículo "lo" de uso general, posesivos especiales "la mia", "lo sou / la sua", "lo tou / la tua", etc., uso de la "-v-" en el pretérito imperfecto en todas las conjugaciones: "cantava" (cantaba), "creixiva" (crecía), "llegiva" (leía); uso de muchas palabras de carácter arcaico de uso muy corriente en el alguerés: "manco" por "menys" (menos), "calqui u" por "algú" (alguien), "qual / quala" por "quin / quina" (cual), etc. y adaptaciones de los pronombres clíticos.

Es necesario diferenciar entre los certificados según el territorio en el que se obtienen, puesto que no son exactamente iguales, aunque son equivalentes.



Como en todas las lenguas romances, el cambio del latín vulgar al catalán fue gradual y no es posible determinar en qué momento se inicia su historia. Según Coromines, los cambios más radicales debieron producirse en los siglos VII y VIII, pero es difícil saberlo con precisión porque los textos se escribían exclusivamente en un latín artificioso, ajeno a la lengua de uso. Ya en el siglo IX y sobre todo en los siglos X y XI, aparecen palabras e incluso frases enteras intercaladas en algo que ya se puede denominar catalán, y documentos breves como el juramento feudal de 1028 o los "Greuges de Caboet" de 1080-1090, totalmente en catalán. De la primera mitad del siglo XII data la traducción del "Forum Iudicum", un fragmento de la cual se conserva en la biblioteca de la Abadía de Montserrat y que ya presenta características lingüísticas más modernas. Desde 1150 hay ya numerosos documentos escritos y hacia finales del siglo XII aparece el primer texto conocido de carácter literario, las "Homilías de Organyà", una colección de sermones. 

El catalán medieval de esta etapa, presenta muchas similitudes con la lengua occitana, con la que forma un continuo dialectal que se irá diferenciando con el tiempo hasta formar lenguas claramente diferenciadas, ya en el siglo XIII. El primer texto impreso en catalán, las "Obres e trobes en lahors de la Verge Maria", se publicó en 1474 en Valencia.

El catalán se formó en comunidades que poblaban ambos lados de los Pirineos (condados del Rosellón, Ampurias, Besalú, la Cerdaña, Urgell, Pallars y Ribagorza) y se extendió hacia el sur durante la Reconquista en varias fases: Barcelona y Tarragona, Lérida y Tortosa, el antiguo Reino de Valencia, las Islas Baleares y Alguer.

En cuanto al catalán como lengua extranjera, aunque no es una lengua muy difundida, cuenta con una larga tradición que se remonta a la Edad Media, a causa de la expansión medieval de la Corona aragonesa, y en su momento dejó huella especialmente en la Península itálica y en el vocabulario náutico mediterráneo. Actualmente, se enseña en varias universidades tanto en Europa como en los EE.UU. e Hispanoamérica, así como en numerosos centros catalanes de todo el mundo.

En todo el Mediterráneo, particularmente en el sur de Italia y las islas de mar Tirreno existen lenguas y dialectos que han sido influidos por la lengua catalana entre ellos están:

Asimismo, la influencia del catalán se dejó sentir en el suroeste andaluz debido a la emigración catalana a esas tierras, con registros aún conservados en la lengua viva del siglo XXI.

Además, la siguiente tabla muestra la población que tiene el catalán como lengua materna, en los dialectos orientales y occidentales, con datos de 2004:

Con la democracia se recuperó la lengua catalana en el ámbito educativo. Sin embargo, la introducción del catalán en las aulas fue muy desigual según el territorio. Así, mientras que en Cataluña y en Baleares se ha adoptado un modelo lingüístico según el cual el catalán es la lengua vehicular principal, en la Comunidad Valenciana se ha seguido un modelo en el cual coexisten como vehiculares tanto el castellano como el valenciano.

En los territorios de lengua catalana existen diferentes medios de comunicación en catalán, los cuales conforman el llamado espacio catalán de comunicación. En el ámbito de la prensa hay que destacar la edición en catalán de La Vanguardia y El Periódico de Cataluña, los diarios editados solo en catalán El Punt Avui, Ara y L'Esportiu; la numerosa prensa comarcal en catalán (Segre, Regió7, Diari de Girona, El 9 Nou, etc.), las revistas en catalán (El Temps, Sàpiens, etc.) y los numerosos diarios digitales en catalán (VilaWeb, Racó Català, Nació Digital, Ara.cat, 324.cat, Diari de Balears, etc.). En cuanto a la radio destacan Catalunya Ràdio, IB3 Ràdio, À Punt FM y RAC 1 como emisoras generalistas, Catalunya Informació como emisora de información 24 horas, y Catalunya Música, Ràdio Flaixbac, Flaix FM y RAC 105 como emisoras musicales. Finalmente, por lo que respecta a la televisión, hay que hacer mención de TV3, IB3 Televisió, À Punt, Andorra Televisió, 8tv y Canal 4 como canales generalistas y El 33, 3/24, Canal Super3, Esport3, Barça TV y Fibracat TV como canales temáticos.

El sistema de escritura también presenta ciertas características particulares. El catalán presenta una característica única, la escritura de la "-l-" geminada: "-l·l-" (como en "intel·ligent" –inteligente–). La otra característica es la "ny" [ɲ] (en castellano es equivalente a la "ñ") que se encuentra también en afaan oromo, aragonés, húngaro, quenya, valón, ladino, malayo, indonesio, ewe, gã, ganda, lingala, seSoto, swahili, zhuang y zulú. También cabe comentar la grafía "-ig" (pronunciada [t͡ʃ] si antes hay vocal y [it͡ʃ] si antes hay una consonante) representada en pocas palabras (como "faig" –hago–, "maig" –mayo–, "mig" –medio–, "desig" -deseo-, "puig" –monte–, "raig" –rayo–, "roig" –rojo–, "vaig" –voy–, "veig" –veo–) o la "t+"consonante"" para la representación de consonantes dobles con: "tm", "tn", "tl", "tll", o africación: "tg" y "tj" ("setmana", "cotna", "atles", "bitllet", "jutge", "platja").

El catalán tiene unas características lingüísticas específicas que lo diferencian de las lenguas románicas vecinas y se hicieron propias con la evolución local y peculiar del latín vulgar hasta lo que se conoce como lengua catalana. La lengua más cercana al catalán es el occitano, junto con el que forma el grupo occitanorromance. Se ha discutido si el catalán y el occitano deben considerarse una lengua galorromance o iberorromance, sin que haya podido establecerse consenso. De un modo más conservador, se puede afirmar el catalán y el occitano son elaboraciones distintas de un mismo idioma, de un grupo románico central, el occitanorománico, la cuestión de si este grupo occitano-romance es parte del grupo iberorromance o galorromance o independiente permanece abierta.

Tipológicamente, el catalán, al igual que las otras lenguas romances occidentales, es una lengua flexiva fusionante con orden básico SVO y preferencia por la posición de núcleo sintáctico inicial ("regens ante rectum" o "núcleo-complemento").

Las siguientes características son algunas de las mutaciones del latín que se han ido haciendo durante la consolidación del catalán, aunque también se muestran otras características generales.

El catalán es una lengua flexiva fusionante, con una morfología similar a la de las otras lenguas románicas occidentales. Los nombres, adjetivos y muchos determinantes tienen formas diferentes según su número y género gramaticales. Los pronombres personales además tienen formas distintas según el caso gramatical, aunque la distinción de género se reduce a los pronombres sujeto de tercera persona. El verbo tienen un sistema de flexión relativamente complejo, donde cada verbo pertenece a un tipo de conjugación (en catalán los verbos se agrupan usualmente en tres conjugaciones caracterizadas por la terminación del infinitivo). Todas características son compartidas por las lenguas románicas occidentales.

Algunas peculiaridades del catalán son:

El sistema vocálico está formado por 8 sonidos vocálicos o alófonos vocálicos diferentes:

Existen diferencias menores en cómo estos 8 alófonos se agrupan en fonemas. El catalán oriental estándar tiene 7 fonemas vocálicos en oposición , aunque en algunas variedades de Baleares los ocho alófonos anteriores están en oposición fonémica. También existen diferencias de realización alofónica entre los dialectos orientales y occidentales.

En catalán, cualquiera de los sonidos puede aparecer en sílaba tónica.
Sin embargo, en sílaba átona ocurren un buen número de neutralizaciones. En catalán oriental (central, balear, septentrional, alguerés), se dan las siguientes:
Mientras que en catalán occidental (noroccidental, valenciano), el sistema átono presenta menos reducciones presentando un sistema con 5 vocales átonas , en lugar de las 7 que pueden aparecer en sílaba tónica. Nótese que estas neutralizaciones tienen análogos en otras lenguas occitanorromances como el occitano.

A diferencia del castellano, suelen ser descendentes. Las pronunciaciones son según el dialecto central (Barcelona y alrededores). Ejemplos:


Los únicos diptongos ascendentes son aquellos del tipo "gu(a/o), gü(e/i)" y "qu(a/o), qü(e/i)":

Los triptongos se forman a partir de aquellos:

Para formar los hiatos, se añade diéresis sobre la "i" o la "u":


Diferencias en interdialectales:

La evolución típica del sistema vocálico tónico puede apreciarse en esta tabla:

El inventario de fonemas consonánticos del catalán, especificados mediante rasgos fonéticos binarios, se resume en el cuadro siguiente:

Como puede verse, usualmente se requieren diez rasgos fonéticos binarios para definir el inventario consonántico anterior de manera unívoca: [+/- dorsal], [+,- coronal], [+,- velar], [+/- sonorante], [+/- africada], [+/- nasal], [+/- lateral], [+/- vibrante múltiple] y [+/- sonora] (el resto de rasgos usados [+/- palatal] y [+/- labial] es redundante y puede deducirse de las combinaciones de los anteriores). Esta cantidad de rasgos es elevada si se compara con el mínimo número teórico de rasgos "abstractos" necesarios que es 5, ya que 2 = 32 > 23 (fonemas).

Las oclusivas devienen en sordas en posición final.

Las africadas devienen en sordas en posición final, pero la /ts/ y la // finales seguidas de vocal son sonoras (/dz/ y //.


Las fricativas en posición final se pronuncian sordas, pero al final de la palabra las /s/ y seguidas de vocal son sonoras.



Hay dos sonidos vibrantes en catalán.








La mayoría de palabras del catalán proceden del latín, aunque existen también una fracción apreciable de préstamos históricos de otras lenguas como: las lenguas germánicas como el gótico ("Ramon" 'Ramón', "espia" 'espía', "ganivet" 'cuchillo'... y los topónimos acabados en "-reny", como "Gisclareny") y más recientemente el inglés ("bar, web, revòlver..."); otras lenguas románicas como el francés ("brioix, garatge, fitxa..."), el italiano ("piano, macarró, pantà, pilot..."), el occitano ("espasa" 'espada', "beutat", "daurar", "aimia", el sufijo "-aire"...) y el del castellano ("bolero", "lloro", "burro"...")"; el árabe "(alcohol", "sucre", "alcova"...y muchos topónimos como" Benicàssim", "Albocàsser"...")", también el euskera ("esquerra" 'izquierda', "isard" 'gamuza, rebeco, sarrio', "estalviar"... 'ahorrar', y muchos topónimos como "Aran" y "Benavarri"...).

Signografía braille de los caracteres del catalán.




</doc>
<doc id="3302" url="https://es.wikipedia.org/wiki?curid=3302" title="20 de octubre">
20 de octubre

El 20 de octubre es el 293.º (ducentésimo nonagésimo tercer) día del año en el calendario gregoriano y el 294.º en los años bisiestos. Quedan 72 días para finalizar el año.








</doc>
<doc id="3303" url="https://es.wikipedia.org/wiki?curid=3303" title="19 de octubre">
19 de octubre

El 19 de octubre es el 292.º (ducentésimo nonagésimo segundo) día del año en el calendario gregoriano y el 293.º en los años bisiestos. Quedan 73 días para finalizar el año.





























</doc>
<doc id="3304" url="https://es.wikipedia.org/wiki?curid=3304" title="18 de octubre">
18 de octubre

El 18 de octubre es el 291.º (ducentésimo nonagésimo primer) día del año en el calendario gregoriano y el 292.º en los años bisiestos. Quedan 74 días para finalizar el año.








</doc>
<doc id="3305" url="https://es.wikipedia.org/wiki?curid=3305" title="17 de octubre">
17 de octubre

El 17 de octubre es el 290.º (ducentésimo nonagésimo) día del año en el calendario gregoriano y el 291.º en los años bisiestos. Quedan 75 días para finalizar el año.

































</doc>
<doc id="3306" url="https://es.wikipedia.org/wiki?curid=3306" title="16 de octubre">
16 de octubre

El 16 de octubre es el 289.º (ducentésimo octogésimo noveno) día del año en el calendario gregoriano y el 290.º en los años bisiestos. Quedan 76 días para finalizar el año.










</doc>
<doc id="3307" url="https://es.wikipedia.org/wiki?curid=3307" title="15 de octubre">
15 de octubre

El 15 de octubre es el 288.º (ducentésimo octogésimo octavo) día del año en el calendario gregoriano y el 289.º en los años bisiestos. Quedan 77 días para finalizar el año.









</doc>
<doc id="3308" url="https://es.wikipedia.org/wiki?curid=3308" title="Academia Ecuatoriana de la Lengua">
Academia Ecuatoriana de la Lengua

La Academia Ecuatoriana de la Lengua es una institución cultural ecuatoriana. Fue establecida en Quito el 15 de octubre de 1874 y pertenece a la Asociación de Academias de la Lengua Española. Su fin es científico y literario, en pro de la defensa del idioma, su espíritu y su unidad. Entre sus principales objetivos se encuentran la investigación lingüística, la evolución del idioma, la aparición y aceptación de neologismos y la atención a términos en desuso.

La corporación ecuatoriana procura mantener la comunicación institucional con las academias y entidades similares; fomenta y propaga el estudio de la lengua mediante sesiones privadas, conferencias, congresos y publicaciones especializadas. Asimismo, responde a solicitudes sobre dudas idiomáticas y asesora a autoridades nacionales, seccionales o locales, y a los medios de comunicación. Ejecuta planes y proyectos encaminados al conocimiento, estudio y difusión del español en el Ecuador, y al engrandecimiento de las letras del país. Además, mantiene convenios con algunas instituciones políticas y culturales para corregir el estilo de las leyes, códigos y textos que le fueren entregados. Dicta conferencias, cursos de corrección idiomática y de actualización de conocimientos sobre temas atinentes a la lengua.

La historia se remonta de alguna forma, a la de la Real Academia Española, la cual fue fundada en 1713, por don Juan Manuel Fernández Pacheco con la finalidad de limpiar, fijar y dar esplendor a la lengua española.
A finales de 1870, la Academia Española concedió las respectivas autorizaciones para poder establecer instituciones correspondientes a ella en diferentes países que poseían el habla castellana, y así, el 4 de marzo de 1875, se logró instalar en Quito la Academia Ecuatoriana de la Lengua, que entre sus principales propósitos, se encontraba el de albergar en ella a los grupos intelectuales y literarios de todas las regiones del país.

La Academia Ecuatoriana de la Lengua (AEL), la segunda fundada en América, se estableció en Quito el 15 de octubre de 1874, aunque su reconocimiento jurídico data del 4 de mayo de 1875. Los miembros fundadores fueron Pedro Fermín Cevallos, Julio Zaldumbide, Belisario Peña, Francisco Javier Salazar, Pablo Herrera y José Modesto Espinosa, quienes ya eran con anterioridad miembros correspondientes de la Real Academia Española.

A lo largo de su historia, la Academia ha publicado la revista "Memorias" y la colección «Horizonte Cultural», que reedita antiguas obras de sus miembros, con un estudio preliminar actual, y ensayos relativos a estudios idiomáticos o de crítica literaria.

En 2013, tras el fallecimiento de Renán Flores Jaramillo, Susana Cordero fue nombrada presidente de la institución, la primera mujer en ostentar el cargo.

En abril de 2016, la Academia en colaboración con la Universidad Técnica Particular de Loja (UTPL) organizó unas Jornadas Cervantinas, en conmemoración de los cuatrocientos años del fallecimiento del escritor.





</doc>
<doc id="3309" url="https://es.wikipedia.org/wiki?curid=3309" title="Quito">
Quito

Quito, oficialmente San Francisco de Quito, es la capital de la República del Ecuador, de la Provincia de Pichincha y la capital más antigua de Sudamérica. Es la ciudad más poblada del Ecuador, con 2 millones de habitantes en el área urbana, y aproximadamente 3 millones en todo el Área metropolitana. Además, es la cabecera cantonal del Distrito Metropolitano de Quito. Está ubicada sobre la hoya de Guayllabamba, en las laderas occidentales del estratovolcán activo Pichincha, en la parte oriental de los Andes y su altitud es de 2850 m s. n. m.. La ciudad está dividida en 32 parroquias urbanas, las cuales se subdividen en barrios. Quito es el centro político y cultural de Ecuador. Alberga los principales organismos gubernamentales, administrativos y culturales. Además, la mayoría de empresas transnacionales que trabajan en Ecuador tienen su matriz en la urbe.

La fecha de su primera fundación es incierta; los registros más antiguos se hallan en la hacienda del Inga alrededor del año 1030 a.C. El Inca Huayna Capac convirtió a Quito en una ciudad importante del norte del Tahuantinsuyo, territorio del imperio Inca, y durante varios lapsos de tiempo se movilizó entre esta y Tomebamba, esta última capital norteña del imperio. Sin embargo, se utiliza la conquista española de la ciudad, el 6 de diciembre de 1534, como su nacimiento y fecha de fundación. La Escuela Quiteña es como se ha llamado al conjunto de manifestaciones artísticas y de artistas que se desarrolló en el territorio de la Real Audiencia de Quito. La Escuela Quiteña alcanzó su época de mayor esplendor entre los siglos XVII y XVIII, llegando a adquirir gran prestigio entre las otras colonias americanas e incluso en la corte española de Madrid. El 24 de mayo de 1822 el ejército independentista comandado por el mariscal Antonio José de Sucre venció a las fuerzas realistas leales a España quienes estaban bajo las órdenes de Melchor de Aymerich, en la denominada Batalla de Pichincha. Gracias a la victoria de las tropas grancolombinas, se consiguió la liberación de Quito y la independencia de las provincias pertenecientes a la Real Audiencia de Quito. El 13 de mayo de 1830 se crea la República del Ecuador, con Quito como capital tras separarse de la Gran Colombia.

Es la primera ciudad declarada, junto a Cracovia en Polonia, como Patrimonio de la Humanidad por la Unesco, el 8 de septiembre de 1978. En 2008, Quito fue nombrada sede de la Unión de Naciones Suramericanas (Unasur), siendo así el centro de reuniones oficiales de los países de América del Sur. En 2018, Quito ha sido evaluada dentro del concepto de ciudades mundiales o globales como una ciudad beta, según el estudio de GaWC, siendo la ciudad mundial más globalizada del Ecuador, a la par de ciudades latinoamericanas como la Ciudad de Panamá y San José.

No se conoce con certeza la etimología del topónimo. Según la historiadora Anne Collin, este proviene de una mitológica tórtola de una antigua leyenda aborigen conocida como ""quitus"". La ciudad no cambiaría su nombre con la llegada de los incas y sufriría una pequeña modificación de la letra "u", por la "o" con los conquistadores españoles. Sin embargo esta afirmación del nombre quitu no es adecuada según el investigador Mauricio Quiroz..

Existe un mito de un Cacique llamado Quitumbe el cual provino de la costa ecuatoriana y fundo la ciudad de Quito.

Otra hipótesis defiende que su denominación proviene de las lenguas tsafiqui y cha’fiki, ""Qui"" -de quitsa-, que quiere decir mitad y ""To"" o ""Tu"", cuyo significado es tierra. Así el vocablo, se traduce como: “Tierra en la mitad del Mundo”.

Las investigaciones arqueológicas señalan que en el sector del Inga, una hacienda ubicada cerca del monte Ilaló, alrededor del año 1030 a.C. vivieron pueblos nómadas que se dedicaban a la caza, la pesca y la recolección de alimentos. Robert Bell, quien fue el primer científico que estudió esa zona, determinó que la roca obsidiana usada para la creación de herramientas databa del 7080 a.C., posteriormente se estableció su edad en más de 12 milenios de antigüedad, como se lo reconoce actualmente. Debido a ello, por ahora, es considerado como el lugar con el asentamiento humano más antiguo del país. Este primer emplazamiento pertenece al período paleolítico ecuatoriano, el cual se caracterizó por el amplio uso de los habitantes de aquel material ígneo del que se han encontrado cerca de 80 mil piezas.

En el año 900 a.C.,durante el período de desarrollo regional, la civilización de los cotocollaos (descubiertos por el Padre Porras en 1973) se estableció entre las montañas Casitagua y Pichincha. Esa sociedad fue sedentaria, basó su desarrollo tanto en la agricultura por el cultivo del maíz, la quinua, el chocho, la calabaza; por la cacería, siendo muy importante la presencia del venado, el conejo, y los camélidos; así como también por el comercio, el cual llegó a lugares distantes para la época como la región costera del país. Debido a ello la cerámica de los cotocollao compartió semejanzas, tanto en su decoración como en estilo, con las culturas Chorrera y Machalilla. Aproximadamente en el año 500 a.C. este pueblo desapareció a causa de las erupciones del volcán Pululahua.

El yacimiento arqueológico de Rumipamba (1500 a.C. hasta el 900 d.C.), una aldea y necrópolis ubicada en la parroquia homónima de la ciudad la cual fue abandonada en varias ocasiones debido a las erupciones de los volcanes Pululahua y Guagua Pichincha, es uno de los pocos vestigios remanentes que pertenecen a la cultura Quitu. Durante este período ( Integración) se da uno de los capítulos más controversiales de la historia de la urbe, El Reino de Quito, mencionado por el Padre jesuita Juan de Velasco en su «"Historia del Reyno de Quito"» publicada en el siglo XVIII.

En ella se habla de un supuesto Reino (palabra que se utilizaba en aquella época para definir al país de Quito por los españoles) conformado por las etnias Quitu y Caras, las cuales integraron un extenso territorio en la sierra central y norteña ecuatoriana. La historia fue negada aproximadamente un siglo atrás por el historiador Gonzales Suárez. Pese a las controversias, se sabe que una importante confederación como los Quitu, se asentaron en las laderas del Pichincha y habitaron la zona antes de la llegada de los incas.

La conquista Inca de esta región fue iniciada en el siglo XV por Túpac Inca Yupanqui, hijo de Pachacútec el fundador del Imperio incaico. Su hijo, Huayna Cápac, fue el primer soberano nacido en el actual territorio ecuatoriano y el que estableció su residencia en tierras cañaris en Tomebamba, la actual ciudad de Cuenca. Conquistaría el territorio de los Quitus, y luego mediante cruentas guerras libradas en los territorios caranguis (actualmente Pichincha e Imbabura) lograría su victoria definitiva, la batalla definitiva se dio en la laguna de Yaguarcocha («lago de sangre», en quichua). La importancia de la ciudad fue estratégica tanto en lo militar como en lo económico, así pues desde Quito el emperador Huayna Capac conquistó a los Caranquis. Algunos estudiosos apuntan a que Atahualpa nació en Caranqui aproximadamente en el año 1500 d.C.

Cuando llegaron los españoles al Tahuantinsuyo, el imperio inca estaba sumergido en una guerra civil provocada por la pugna de poder entre Atahualpa y su hermano Huáscar. El primero defendía su hegemonía desde Quito, el segundo desde Cuzco. Atahualpa y su ejército vencieron a Huáscar en las cercanías del río Apurimac. Le dio a elegir a Huáscar: vivir y quedarse con el imperio de Cuzco, o morir. Huáscar, indignado por la derrota, aceptó la muerte. Pero en el año de 1533, luego de pacificar el imperio, Atahualpa aceptó una reunión con Francisco Pizarro, en la cual fue capturado y días después asesinado por orden del español.

La conquista de los Andes septentrionales fue motivada principalmente por el rumor de que en Quito se encontraba el tesoro de Atahualpa. Se formaron dos expediciones, la de Pedro de Alvarado, desde Guatemala, y la de Sebastián de Belalcázar procedente del sur. Fue este último el que consiguió llegar primero y quien, el 6 de diciembre de 1534, fundó la ciudad de San Francisco de Quito en las faldas orientales del volcán Pichincha. La ciudad se encontraba sobre cenizas, ya que días antes había sido incendiada por el general inca Rumiñahui con el objeto de que los españoles no encontraran nada al llegar. En agosto, la villa había sido fundada por Diego de Almagro cerca de la ciudad de Riobamba con el nombre de Santiago de Quito.

La urbe fue establecida con aproximadamente doscientos habitantes. Inmediatamente se señalaron los límites, se estableció el cabildo, se repartieron solares y se delimitaron áreas comunales. La fundación de la ciudad en este sitio parece haber respondido más que nada a razones estratégicas. A pesar de su topografía accidentada, su ubicación en una meseta presentaba ventajas sobre los valles aledaños, más propicios para el desarrollo urbano. Este último factor fue también el que primó en la determinación del lugar por parte de los pueblos originarios. En el ámbito arquitectónico, empezaron a construirse los primeros monumentos de la villa, destacándose el inicio de la construcción de la iglesia de San Francisco, en 1536.

Aproximadamente, siete años después de la fundación de Quito, Francisco de Orellana partiendo desde esa ciudad junto a numerosos indígenas, en busca del tesoro de Atahualpa, descubrió el río Amazonas el 12 de febrero de 1542. Debido a este suceso histórico, se creó la célebre frase: "Es Gloria de Quito el Descubrimiento del Río Amazonas". El 8 de enero de 1545, el Papa Alejandro Farnesio (Paulo III) fundó la Diócesis de San Francisco de Quito con la finalidad de mejorar el proceso de evangelización a los indígenas, que era difícil por la extensión del territorio.
Debido a los problemas de comunicación y transporte, así como también por la explosión demográfica, el cabildo de la ciudad solicitó al rey Felipe II la creación de la Audiencia y Presidencia de Quito. El 29 de agosto de 1563, él firmó la cédula real que dio nacimiento a esta. La jurisdicción estableció sus límites geográficos, que abarcaban una superficie cinco veces mayor que la de la actual República del Ecuador.
El Virrey Pedro Mesía de la Cerda otorgó el título de Presidente interino de Quito con fecha de 17 de mayo de 1766 a Juan Antonio Zelaya y Vergara, que durante este período ejerció sus responsabilidades en calidad de Duque de Quito como comandante general militar y político de dicha provincia

Enriquecida por la explotación minera y la producción textil, pudo construir templos barrocos y neomudéjares adaptados con originalidad al ambiente local y los ornamentó con gran profusión de pinturas y tallas, de innegable valor didáctico religioso. Fue la época de la afamada Escuela Quiteña, obra del mestizaje indio y español.

Los geodésicos franceses del sistema decimal introdujeron en Quito el espíritu racionalista moderno y usaron la magnífica biblioteca de la Universidad Jesuita de San Gregorio. Quito alimentó la extraordinaria empresa de las misiones de Jaén y Mainas. En Quito nació y vivió Mariana de Jesús, santa y patriota. De esta ciudad salió el más ilustre de los precursores de la independencia americana, el mestizo Xavier Chusig quien cambió su nombre a Eugenio de Santa Cruz y Espejo para evitar la discriminación. Espejo fue el fundador del primer periódico de Quito. También hay otras historias como la de Manuela Sáenz, la primera mujer enrolada al ejército bolivariano quien se convirtió en la fiel compañera y novia del libertador Simón Bolívar.

Algunos de los sucesos internacionales como la Declaración de Independencia de los Estados Unidos en 1776 de Gran Bretaña y la Revolución francesa de 1789, sirvieron de ejemplo a los criollos al mostrarles que un sistema de gobierno autónomo o incluso independiente era posible. Las influencias de varios acontecimientos locales tales como la visita de los geodésicos franceses quienes impulsaron las ideas de la ilustración en la urbe, el alto índice de empobrecimiento de la Audiencia y los crecientes sentimientos nacionalistas, estimulados por el interés de los criollos de todo el continente por obtener el poder, fueron también algunas de las causas principales, que motivaron el inicio del proceso revolucionario que dio fin al colonialismo español en la ciudad.

Durante la cena de Navidad, el 25 de diciembre de 1808 en la hacienda Chillo Compañía, propiedad de Juan Pío Montúfar y Larrea II Marqués de Selva Alegre, se celebró una reunión conocida como «La conspiración de Chillo» o «La Conjura Navideña» que discutió el establecimiento de una Junta Autónoma que se encargaría de gobernar la Presidencia de Quito. A ella asistieron Juan de Dios Morales, José Riofrío, Juan Pablo Arenas, Manuel Quiroga, Nicolás de La Peña, Francisco Javier de Ascázubi y el capitán Juan de Salinas y Zenitagoya.

Meses después el complot fue descubierto por el entonces presidente de la Real Audiencia de Quito Manuel Urríez, conde Ruiz de Castilla, debido a que Salinas comentó a Andrés Torresano, sacerdote del convento de La Mercéd, el tema de la reunión. Fue apresado el primero de marzo al igual que sus compañeros Juan Pío Montufar, el día cinco y Juan de Dios Morales el seis. Pocos días después todos fueron liberados debido a que las pruebas indagatorias fueron sustraídas.

El 8 de agosto se reunieron en el hogar del Dr. Francisco Javier de Ascázubi, donde se tomó la decisión de integrar la junta el día 10. El 9 de agosto, este grupo de ilustrados criollos, se reunió nuevamente en la residencia de Manuela Cañizares. El 10 de agosto de 1809, se firmó el acta que cesó en sus funciones al entonces presidente de la Real Audiencia de Quito, conde Ruiz de Castilla, e instauraron en la ciudad la Primera Junta Autónoma de Gobierno, con autoridades quiteñas y con la estrategia de las máscaras de Fernando VII que consistían en fingir lealtad al depuesto rey Fernando VII con el objetivo de mantener la autonomía.

El rechazo de adhesión a la junta de Guayaquil, Cuenca, Popayán, Pasto, Barbacoa y Panamá así como el débil interés que poseían algunos de sus miembros, entre ellos el presidente, Juan Pío Montufar, ocasionó que el 5 de octubre se declarase una contrarrevolución y que el 24 del mismo mes se firmara la capitulación. Después de estos hechos, la mayoría de los miembros de la junta fueron perseguidos y encarceladas; en el Cuartel de Lima en Quito, lugar en el que entre el 2 y el 10 de agosto de 1810 fueron asesinados; alrededor de 300 ciudadanos de Quito acudieron a rescatar a los próceres, enfrentándose a las tropas reales con el objetivo de lograr este fin sin embargo en el enfrentamiento fueron derrotados y asesinados. Esta matanza de 300 personas significó en aquel momento la muerte del 1 por ciento de la población de la urbe. Una masacre de iguales características hoy, representaría cerca de 17 mil víctimas. El poder vuelve a manos del Conde Ruiz de Castilla. Los virreyes de Lima y Bogotá envían tropas para sitiar la ciudad. En 1812 llega como Comisionado Regio de España Carlos Montúfar, hijo del Marqués de Selva Alegre para pacificar a los sublevados, pero lo que ocurrió fue que éste se unió a la lucha que se estaba disputando en América, esto ocasionó que en 1816 pierda la vida; defendiendo los ideales de libertad.
El 9 de octubre de 1820, Guayaquil declaró su independencia de España, marcando el inicio del proceso de emancipación. El 24 de mayo de 1822 en la denominada Batalla de Pichincha, la División Protectora de Quito creado en Guayaquil y bajo el mando del Mariscal Antonio José de Sucre ingresó a la ciudad desde Chillogallo, al sur. Luego en la madrugada tuvieron que subir las laderas del Volcán PIchincha, la compañía estuvo formada de 1701 hombres en enero de 1822, luego de los cuales las personas reclutadas eran de Guayaquil y región de la Sierra terminó el ejército en la madrugada del 24 de mayo siendo 2.900, con 200 colombianos de la Alta Magdalena en la vanguardia y británicos de Albión en la protección del tren de municiones. En la madrugada hubo una fuerte llovizna en la subida por las laderas del volcán y a pesar de los esfuerzos de los soldados fue difícil la subida por la razón que los senderos se convirtieron en ciénagas, luego bordearon el flanco occidental de las faldas del Pichincha, para rodear a las fuerzas realistas a cargo de Melchor de Aymerich. Se dio un disputado combate, en el cual el ejército grancolombino consiguió la liberación de la ciudad y la independencia de las provincias pertenecientes a la Real Audiencia. El 25 de mayo Melchor de Aymerich capituló y la antigua Presidencia de Quito pasó a formar parte de la Gran Colombia.

Luego de la Batalla de Pichincha, en junio de 1822 llega el Libertador Simón Bolívar para anexionar los territorios de la antigua audiencia a la República de la Gran Colombia, conformada entonces por los actuales Colombia, Ecuador, Panamá y Venezuela, con capital en la ciudad de Santa Fe de Bogotá. La Real Audiencia se transforma en Departamento del Sur o Presidencia de Quito. El 25 de junio de 1824 se funda la provincia de Pichincha, teniendo a Quito como capital. El 18 de marzo de 1826 se inaugura la Universidad Central del Ecuador.

El 13 de mayo de 1830 se crea la República del Ecuador con Quito como capital, tras separarse de la Gran Colombia. El general venezolano Juan José Flores asume el cargo de primer Presidente. El 27 de agosto de 1869 se funda la Escuela Politécnica Nacional otra de las grandes universidades del Ecuador.
Otros hechos históricos que describen a Quito en la historia de este país son: el asesinato del Presidente del Ecuador Gabriel García Moreno el 6 de agosto de 1875; el asesinato del Presidente Eloy Alfaro cuyo cuerpo inerte fue arrastrado por las calles quiteñas e incinerado posteriormente en el Parque El Ejido el 28 de enero de 1912; la Revolución Juliana de 1925 para rescatar el estado de manos de la plutocracia bancaria; entre otros.

El 25 de junio de 1908 llegó por primera vez el tren de vapor a la Estación de Chimbacalle ubicada al sur de Quito. La obra de construcción del Ferrocarril Transandino entre Guayaquil y Quito había sido iniciada por el Presidente Gabriel García Moreno, y fue terminada en época del Presidente Eloy Alfaro. Un clavo de oro colocado en el último riel de la mencionada estación por América Alfaro, la hija del Presidente Alfaro, selló la obra de infraestructura más grande del Ecuador en aquella época. La llegada del ferrocarril de vapor a la ciudad, produjo la necesidad de crear un medio de transporte urbano que operase entre la Estación de Chimbacalle encima del cerro al lado sur de la ciudad, y el centro comercial en el otro lado del Río Machángara. La Quito Tramways Company fue organizada en 1910 en Wilmington (Delaware) - Estados Unidos, y fue controlada por la Ecuadorian Corporation Ltda. de Londres. La QTC empezó la construcción de una línea de tranvías eléctricos en 1911 y ordenó cuatro carros de dos ejes a la J. G. Brill en Philadelphia el 17 de febrero de 1914. La nueva línea, entre la estación del ferrocarril y el centro de la ciudad, fue inaugurada el 8 de octubre de 1914. La QTC mandó hacer a la Brill dos carros de cuatro ejes en 1915 y dos más de dos ejes en el año siguiente. La trocha de las líneas de tranvía de Quito, como del ferrocarril de vapor, era de 1.067 mm (42 pulgadas).

Durante 34 años la QTC operó los ocho mismos tranvías en dos servicios: de la estación Chimbacalle al Cementerio San Diego, y de Chimbacalle hasta la Avenida Colón, cerca al Palacio de La Circasiana. El depósito de tranvías se ubicó en la Avenida 18 de Septiembre y Jorge Washington. En 1921 una empresa ecuatoriana, Compañía Nacional de Tranvías, construyó une línea de tranvías en las Avenidas 10 de Agosto y la Prensa entre la Avenida Colón y la aldea de Cotocollao. Ya que la QTC poseía los derechos exclusivos de tracción eléctrica en la ciudad, los vehículos de la CNT tenían que ser accionados por motores de gasolina. La CNT importó los chasis y la parte mecánica de sus carros de la Allgemeine Elektricitäts-Gesellschaft (AEG) en Alemania, pero montó las carrocerías en Ecuador. La línea de Cotocollao, con trocha también de 1.067 mm (42 pulgadas), abrió el 22 de junio de 1923. Alrededor del 1926 los inversores ecuatorianos reorganizaron la CNT y adquirió la QTC. Los nuevos dueños cerraron la línea de gasolina de Cotocollao en 1928 hasta 1935 y las dos líneas de tranvía eléctrico aproximadamente en 1948.
En la década de 1930, las clases altas del centro de la ciudad se desplazaron al norte. Surgieron barrios residenciales dentro del esquema de "ciudad jardín". Los espacios del centro fueron ocupados por inmigrantes de las provincias vecinas. La parte antigua de la ciudad pudo por consiguiente conservar su traza original y su arquitectura colonial enriquecida con los nuevos aportes de los siglos XIX y XX. Hacia la mitad del siglo XX, el espacio urbano estaba ya socialmente estratificado.

El 5 de julio de 1941, estalla un conflicto con Perú lo que produce la Guerra peruano-ecuatoriana, durante los días de guerra con el Perú los ferrocarriles se dirigían al sur del país llevando jóvenes soldados voluntarios para hacer frente al enemigo peruano.

El 6 de agosto de 1960 se inaugura el Aeropuerto Internacional Mariscal Sucre. El 28 de marzo de 1976 en la loma de El Panecillo, el décimo primer arzobispo de Quito Pablo Muñoz Vega, inauguró la Virgen del Panecillo. Esta es una estatua hecha de aluminio, copiada a partir de un original de Bernardo de Legarda. El 8 de septiembre de 1978, Quito fue declarada como el Primer Patrimonio Cultural de la Humanidad por la UNESCO, con el objetivo de conservar sus conventos coloniales, iglesias y el centro histórico en general. Desde hace algunos años, el Municipio de Quito ha emprendido en un plan de salvamento arquitectónico y social del Centro Histórico.

A partir de la década de 1970, Quito se modernizó gracias al boom petrolero en Ecuador. Se convirtió en la capital petrolera y en el segundo centro bancario y financiero del país. Su modernidad se aprecia en la arquitectura del sector norte de la ciudad. Uno de los exponentes de este desarrollo es la Torre CFN, que con sus 23 pisos es el edificio más alto de la ciudad.

La extensión de la ciudad hacia el norte y el sur comenzó durante los años 1980, cuando la principal área turística ubicada en el centro norte de la ciudad (Quito moderno) comenzó a crecer. Actualmente es la ciudad más desarrollada del Ecuador.

El 5 de marzo de 1987, se produjo un terremoto de, aproximadamente, magnitud 7 en la escala sismológica de Richter y cuyo epicentro se localizó a 80 km de Quito. El temblor causó daños en varias edificaciones de la ciudad. El 27 de diciembre de 1993, se promulga la Ley de Régimen para el Distrito Metropolitano de Quito. El 17 de diciembre de 1995, el Municipio de Quito inaugura la primera línea de trolebuses en la ciudad y en el Ecuador con el nombre de Trolebús de Quito o Trole.

Al finalizar el siglo XX, el país entró en un periodo de inestabilidad política, siendo Quito el principal escenario de los acontecimientos que sacudieron a la nación. En 1997 se dieron varias manifestaciones populares en la ciudad que pedían la salida del presidente de ese entonces Abdalá Bucaram. La manifestación más importante se dio el 5 de febrero, y tuvo como consecuencia la denominada "Noche de los tres presidentes", suscitada durante la noche del 6 de febrero y la madrugada del 7 de febrero de 1997 en Ecuador, donde Bucaram, Rosalía Arteaga y Fabián Alarcón reclamaban la presidencia de la república. Finalmente Abdalá Bucaram fue destituido por el Congreso Nacional. Dos años más tarde, Ecuador sufrió una severa crisis financiera, teniendo como principal problema el ""feriado bancario"", esta situación volvió a crear grandes manifestaciones sociales en la capital que desembocaron en el golpe de estado del 2000, cuando el 21 de enero las Fuerzas Armadas del Ecuador le retiraron su apoyo luego de que los miembros de la Confederación de Nacionalidades Indígenas del Ecuador, (CONAIE), se tomaran las calles de Quito y avanzaran al Congreso Nacional, apoyados por un grupo de coroneles de las Fuerzas Armadas quienes actuaban de manera independiente a la institución militar.. Así, Mahuad también fue derrocado.

Para el año 2001, la ciudad había alcanzado 1´399.814 habitantes, mas su zona metropolitana se aproximaba a los 2 millones. Desde el año 2002 se inició la recuperación del centro histórico y del casco colonial. En 2002 el alcalde Paco Moncayo suscribió un contrato con el Gobierno de Canadá, por medio de la Canadian Comercial Corporation, para la construcción de un nuevo aeropuerto, en el sector que se venía planteando desde mediados del siglo XX. La licitación fue otorgada a la Corporación Quiport y su construcción empezó en enero de 2006, durante el segundo periodo de Moncayo frente al Municipio.

Los días 29 y 30 de noviembre de 2002, se llevaron a cabo los actos de inauguración de La Capilla del Hombre, un museo que contiene las mejores obras del maestro Oswaldo Guayasamín quien fue un destacado artista. Se encuentra junto al "Museo Casa-Taller Guayasamín" la residencia donde el Maestro vivió sus últimos años. El proyecto fue concebido en 1985 por el arquitecto colombiano Luis Felipe Suárez Williams. Fue tributo a los pueblos originarios precolombinos de América, quienes por más de 500 años sufrieron la represión colonial y aún luchan por revivir sus valores. La construcción del edificio empezó en 1995 y se terminó en 2002, después de la muerte del artista. Está conformada por una estructura de dos pisos de ladrillo, inspirada en un templo Inca con su parte superior rematada con un domo de cobre laminado. Se entra a la capilla por un túnel que juega con efectos de la luz.En su interior se encuentra «La llama eterna» por los Derechos Humanos y la Paz. Fue declarada por la UNESCO como «Proyecto prioritario para la cultura», y por el Gobierno Nacional del como «Patrimonio cultural del estado ecuatoriano».

Entre 2003 y 2004 se construyó la línea de buses ecológicos MetrobusQ que atraviesa la ciudad de norte a sur, también se ampliaron las avenidas y se construyeron pasos deprimidos y reformas geométricas con la finalidad de dar mayor fluidez al tránsito. Para 2005 se terminó la recuperación del sector de La Mariscal, antiguamente una zona roja, creando una gran cantidad de restaurantes, calles peatonales, piletas, cafés, bares y lugares especialmente adaptados para la presentación de conciertos musicales. El 2 de julio de 2005, se inauguró una moderna línea de teleféricos turísticos en Cruz Loma con el nombre de "TelefériQo".

Entre febrero y abril de 2005, los quiteños protagonizaron la "Rebelión de los forajidos", que derrocó al presidente Lucio Gutiérrez;las protestas se realizaron en las noches, utilizando símbolos como los cacerolazos. Posteriormente, y sólo bajo la presión ciudadana y ante la magnitud que las protestas iban tomando, los demás medios privados comenzaron a transmitir los hechos; ya que el grito cada vez más fuerte de “Que se vayan todos”, producía temor en los dueños de los medios y en los partidos políticos hegemónicos. Finalmente el 20 de abril, Gutiérrez huyó del Palacio de Carondelet en helicóptero.

En las elecciones generales 2009 el tema del transporte masivo fue un punto trascendental de la campaña para la Alcaldía Metropolitana, Augusto Barrera propuso la construcción de un Metro subterráneo. La primera fase de construcción del Metro de Quito se licitó el 8 de julio de 2010 después de que el 8 de mayo Metro de Madrid entregará los estudios definitivos de ingeniería de dicha fase; el plazo de ejecución es de 18 meses y su presupuesto es de 64,8 millones de dólares. El 1 de noviembre de 2012 la Empresa Metro de Quito adjudicó la construcción de la primera fase del Metro a la empresa española Acciona Infraestructuras de entre 5 empresas que ofrecieron construir esta fase.
El 30 de septiembre de 2010 se dio un intento de golpe de estado, conocido como "30S", por una parte de varios miembros Policía Nacional del Ecuador y la Fuerza Aérea Ecuatoriana. La crisis fue superada al final del mismo día, con la salida del presidente Correa del Hospital de la Policía Nacional rescatado por el Ejército de Ecuador y el GOE (Grupo de Operaciones Especiales de la Policía), frustrando las intenciones de los amotinados. El incidente resultó con un saldo de 5 muertos y 274 heridos.

La construcción del nuevo aeropuerto fue entregada el 11 de octubre de 2012, y el 19 de febrero de 2013 se cerraron las operaciones en el antiguo aeropuerto de Quito; esa misma noche fue inaugurada la nueva terminal con una ceremonia presidida por el presidente Correa. Al día siguiente un avión de TAME inauguró oficialmente las operaciones en el nuevo aeropuerto.

La segunda fase de la construcción del metro arrancó el 19 de enero de 2016. La obra física demorará será entregada en 2020.

El 2 de octubre de 2019 iniciaron una ola de protestas, tras la adopción de nuevas medidas económicas por parte del gobierno de Lenín Moreno las cuales fueron dictadas por el Fondo Monetario Internacional. La situación se fue tornando más crítica con el pasar de los días, por lo que el gobierno decretó el estado de excepción, e incluso llegando a ordenar el 8 de octubre un toque de queda y el traslado de la sede de gobierno a Guayaquil. Los principales enfrentamientos se dieron en las calles de Quito, entre la Confederación de Nacionalidades Indígenas (CONAIE) y la policía, la cual llegó a cometer crímenes de lesa humanidad lo que ocasionó al menos 11 fallecidos, 1340 heridos y 1192 detenidos, ocasionando una grave conmoción social; no obstante, el 13 de octubre se llevó a cabo un foro mediado por el representante de la ONU en Ecuador y la Iglesia Católica, donde los dirigentes de la CONAIE y el partido de gobierno llegaron a un acuerdo que finalizó con el conflicto.

La ciudad y el distrito se encuentran ubicados, principalmente sobre el valle de Quito, que forma parte de la hoya del río Guayllabamba, la cual está emplazada en las faldas orientales del estratovolcán activo Pichincha, en la Cordillera Occidental de los Andes septentrionales de Ecuador, a una altitud promedio de 2800 m s. n. m. La urbe está delimitada por el volcán Casitagua por el norte, la falla geológica EC-31 (conocida como "Falla de Quito-Ilumbisi" o "Falla de Quito") por el este, las faldas orientales del Pichincha por el oeste y por el Volcán Atacazo por el sur. Sus dimensiones aproximadas son de 50 km de longitud en sentido sur-norte y 8 km de ancho en sentido este-oeste.

Centenas de millones de años atrás, durante el período Paleozoico, se desarrollaron las bases de lo que serían Los Andes cuando empezó la subducción de la Placa de Nazca bajo la Placa Sudamericana, la que pertenecía al antiguo continente de Gondwana. La mayor actividad telúrica se registró durante el período cuaternario en la época del pleistoceno, la cual formó el paisaje accidentado de la ciudad. En el plioceno, se presentaron varios eventos de considerable importancia en el país y el continente, sin embargo, la ciudad no fue influenciada mayoritariamente por estos. Pese a ello, existieron algunos acontecimientos durante ese período como se evidencia en algunas zonas del Este de la urbe.

Más adelante la morfología del distrito continuó transformándose, los períodos glaciares cubrieron la región con hielo, el cual se derritió progresivamente debido al cambio climático natural que experimentó el planeta después de la última glaciación, así como también a una serie de erupciones volcánicas que provocaron el derretimiento glaciar. Esto formó varias ciénagas y lagunas en todo el territorio, algunas de las cuales eran alimentadas por ríos formados por el deshielo de los picos nevados. Se ha registrado que los habitantes de aquel período establecieron sus hogares cerca de uno de los lagos más grandes del distrito llamado iñaquito, que desapareció debido al drenado que realizaron los españoles a su llegada, con el fin de ocupar los terrenos para usarlos como zonas comunes.

Quito se ubica en varios valles cuyo terreno irregular tienen una altitud que oscila entre los 1800 m s. n. m., en los lugares llanos; como en el sector de Guayllabamba, pasando por alturas de entre 2200 m s. n. m.; como en Cumbayá, 2550 m s. n. m.; en Conocoto (Valle de los Chillos), 2650 m s. n. m.; en el sector de El Condado (en la zona urbana de la ciudad (en el valle principal), 2820 m s. n. m. en la Plaza Grande (centro histórico) y los 3600 m s. n. m.; en los barrios más elevados, como el barrio de la Libertad en el centro de la ciudad. Algunas estribaciones desprendidas de la cordillera de los Andes; han formado en el valle principal sobre el que se asienta la ciudad, un paisaje enclaustrado, dividido en su parte central por el cerro de El Panecillo (3035 m s. n. m.). Al este por las lomas de Puengasí, Guanguiltagua e Itchimbía. Así como también, la principal cadena montañosa perteneciente al volcán Pichincha, el que se encuentra emplazado en la Cordillera de los Andes, encierra a la urbe en el alargado valle; hacia el oeste con sus tres diferentes elevaciones, Guagua Pichincha (4794 m s. n. m.), Rucu Pichincha (4698 m s. n. m.) y Cóndor Guachana. Debido a ello la ciudad posee una forma alargada, cuyo ancho no supera los 8 km, mientras que el distrito ocupa el valle de 12 000 km². El punto más bajo de la zona urbana en el valle principal se encuentra a 2650 m s. n. m. en el secror de El Condado; mientras que el más alto es la cima de La Libertad a 3400 m s. n. m.. Mientras que el punto más bajo del Distrito Metropolitano se ubica en la zona de Mashpi a menos de 1000 m s. n. m..

El clima de la ciudad corresponde al clima subtropical de tierras altas, con muchas características continentalizadas que van desde climas áridos y templados hasta húmedos y fríos; Quito se divide en 3 zonas; sur, centro, y norte; donde el sur es el lugar más frío de la ciudad porque es la zona más alta, el norte es caliente; donde se dan siempre las temperaturas más altas, y el centro es templado. El clima de Quito se divide en 2 estaciones o etapas; el invierno con un período de lluvias prolongado con mucha prevalencia de fenómenos atmosféricos y climáticos como el granizo, las temperaturas suelen bajar drásticamente hasta ubicarse incluso en los 0 °C, incluso, existe cada año al menos un fenómeno nevoso a las afueras de la ciudad en páramos situados a 4000 metros sobre el nivel del mar, el último de ellos registrado en agosto de 2019. La estación seca de cuatro meses es la temporada donde se presentan las temperaturas más altas. 

Debido a que está a 2800 metros de altitud y a que está ubicada en un valle cerca de la línea ecuatorial, Quito mantiene condiciones primaverales todo el año. De junio a septiembre las temperaturas suelen ser más cálidas, sobre todo durante la tarde, mientras que el resto del año la temperatura suele ser templada con temperaturas que van desde los 10 °C hasta los 27 °C.

Debido a su posición geográfica, la ciudad de Quito recibe niveles extremos de radiación solar todo el año, siendo uno de los lugares de la tierra que más la recibe, llegando a recibir hasta 24 UVI (Índice Ultra Violeta) 

En su calidad de principal urbe del país, Quito es la sede oficial del gobierno central y la administración pública, a excepción del período comprendido desde el 7 de octubre de 2019 y durante 30 días debido al estado de excepción en rigor, y acoge las principales representaciones diplomáticas extranjeras. Después de su fundación, el 6 de diciembre de 1534 la ciudad se convirtió en la capital del actual Ecuador, al ser designada como sede de la Real Audiencia de Quito. En 1717 es suprimida debido a una guerra que enfrentó España con la Cuádruple Alianza y se restableció el 5 de noviembre de 1723. Durante esta época colonial, el Presidente de la Audiencia de Quito residió en el Palacio de Carondelet. Así como también desde aquel período, el cabildo ha utilizado el mismo edificio como su sede principal.

Luego de la independencia del país en 1822, la ciudad perdió su capitalidad al integrarse en la Gran Colombia hasta el año de 1830, cuando esta se separa junto con Cuenca y Guayaquil con los cuales formó el Ecuador. Ese año, se establecieron las instituciones políticas. Los organismos que representan a las cinco funciones del Estado fueron circunscritos en Quito desde esa época hasta la actualidad.

Quito es la cabecera del Distrito Metropolitano de Quito. La administración de la ciudad se ejerce a través del Municipio del Distrito Metropolitano de Quito conformado por un Concejo Metropolitano el cual está integrado por 21 concejales y es presidido por el Alcalde Metropolitano, todos estos componentes son elegidos para ejercer estas funciones durante un período de cinco años, mediante sufragio universal. En las últimas elecciones metropolitanas llevadas a cabo en 2019, fue elegido Jorge Yunda, auspiciado por el movimiento Unión Ecuatoriana, como Alcalde del Distrito Metropolitano de Quito, cuya posesión se realizó el 14 de mayo del mismo año.

En 1993 donde se promulgó la Ley de Régimen para el Distrito Metropolitano de Quito, según la cual el Municipio capitalino asumió más competencias, como la del transporte que en esa época era responsabilidad del Gobierno Nacional. Hoy con la Constitución 2008 a Quito como Distrito Metropolitano le corresponde asumir las competencias de los gobiernos cantonales, provinciales y regionales, convirtiéndolo así en una "ciudad-distrito-región" Algunas de las principales competencias que tiene a cargo el Gobierno Metropolitano de Quito son: las del orden urbanístico de la ciudad, promoción cultural, prestación de servicios públicos, las disposiciones tributarias competentes de la urbe, la reglamentación del transporte público y privado, del uso de los bienes públicos, la aprobación del presupuesto general de la ciudad, la fijación de los límites urbanos, distritales y parroquiales.

Quito está dividido en Zonas Metropolitanas conocidas como Administraciones Zonales cuyas funciones son el descentralizar los organismos institucionales, así como también mejorar el sistema de gestión participativa. Cada una es dirigida por un administrador zonal designado por el alcalde, el cual es responsable de ejecutar las competencias de la urbe en su Zona. Actualmente existen nueve Zonas Metropolitanas, las se fraccionan en parroquias, 32 urbanas (ciudad), 33 rurales y suburbanas.
Territorialmente, la ciudad de Quito está organizada en 32 parroquias urbanas, mientras que existen 33 parroquias rurales con las que complementa el aérea total del Distrito Metropolitano de Quito. El término "parroquia" es usado en el Ecuador para referirse a territorios dentro de la división administrativa municipal.

La estructura moderna se mezcla con la colonial donde los residentes nacionales y los visitantes extranjeros encuentran siempre un lugar para trabajar, gozar y recordar. Además, la ciudad está rodeada por los volcanes Pichincha, Antisana, Cotopaxi, y Cayambe que conforman el contorno andino. Quito está colmada de significados que la identifican y definen, ocupa laderas o baja a los valles, serpentea a través de callejones y se abre en amplias avenidas; zigzaguea, sorteando colinas y quebradas. Por esta belleza física, sus tradiciones, rincones de misticismo y leyendas vigentes, es considerada "Relicario del Arte en América". Estas fueron las características principales para que, en septiembre de 1978, Quito fuera declarada por la UNESCO "Patrimonio Cultural de la Humanidad".

En los últimos años el turismo ha crecido en la ciudad y ha significado un nuevo rubro en los ingresos de capital. La mayoría de extranjeros que visitan Quito proceden de los Estados Unidos y pertenecen a una población generalmente joven, cuyo promedio de edad no alcanza los 28 años. También visitan la ciudad europeos provenientes de Italia, Francia o Alemania. Hace unos años la mayoría de turistas que se quedaban en la ciudad unos días lo hacían porque su destino principal era el archipiélago de Galápagos pero en la actualidad eso ha cambiado. Dentro de las principales anotaciones que destacan los turistas están la amabilidad de la gente quiteña, la majestuosidad de un paisaje lleno de volcanes nevados, y el frío húmedo de montaña, que extraña mucho a los extranjeros pues se acompaña de una sensación térmica muy baja sobre todo por las noches.

En el año 2008 Quito, se inició en la sección de viajes del New York Times. Este diario publicó su tradicional lista de las 53 ciudades que los norteamericanos podrían visitar y por primera vez Quito fue incluida. El sitio web de ese conocido diario presentó a Quito como uno de los mejores y más recomendables destinos turísticos del mundo. “El crecimiento turístico fue del 12% anual, cuando el promedio antes del 2002 era de tan solo del 8%”, sostiene una publicación del Cabildo. La capital vive sus fiestas y, heredera de una rica historia y cultura, muestra su legado a quienes la recorren.

Por otra parte, en el turno de Quito en los medios de comunicación, “Pasaporte hacia América Latina con Samantha Brown” que se estrenó en el verano y continúa en el aire fue tal vez el mayor logro en televisión para la ciudad. Samantha Brown dijo del Ecuador que era “el lugar perfecto para visitar cuando se quiere descubrir América del Sur”. La ciudad también recibió a un equipo de las cadenas estadounidenses CBS y NBC recientemente. Además ha recibido reseñas tremendamente entusiastas de publicaciones como Condé Nast Traveller, Frommer’s ‘Top Destinations for 2008’ (Los mejores destinos para 2008 de Frommer’s), la revista Nexos de American Airlines, la revista Geographical, el diario San Francisco Chronicle, y la revista alemana Merian entre otros. Quito se ubicó en el top 14 en la selección de las nuevas siete ciudades maravillas del mundo, entre más de 1200 ciudades alrededor del planeta.

La ciudad de Quito, al ser además de la capital de Ecuador es un importante centro turístico y cuenta con una variada lista de establecimientos hoteleros de todo tipo, desde casas rústicas hasta grandes y modernos hoteles. La mayoría de hoteles se encuentran concentrados en el centro norte y centro histórico de la ciudad, zonas que concentran los principales atractivos turísticos. Los hoteles en el centro histórico de la ciudad, tienen en su mayoría un ambiente más familiar y rústico, esto debido a que se encuentran en el cásco histórico de la ciudad, zona donde predominan las actividades culturales. Los hoteles del norte y centro norte de la ciudad, suelen ser más modernos y grandes, promocionándose como destino para ejecutivos y empresarios principalmente.

Pero Quito ofrece una variedad de alojamientos en la ciudad para todos los gustos y bolsillos, como los famosos "hostales bed & breakfast" de la zona de La Mariscal al igual que en el centro histórico de la urbe, en donde los turistas pueden alquilar una habitación a precios asequibles y a la vez estar cerca de los centros nocturnos más frecuentados de la ciudad.

Entre de los atractivos turísticos de la ciudad, y a una altura de 3.016m sobre el nivel del mar está la loma de El Panecillo, en la que
se encuentra la estatua de la "Virgen de Quito", inaugurada el 28 de marzo de 1976, siendo uno de los miradores más visitados por su visión de toda la ciudad

Novedoso es el teleférico, o como es conocido localmente, el "TelefériQo" (con "q" de "Quito"), que permite acceder a Cruz Loma (a 4.200 m s. n. m.), una pendiente al este del Pichincha, poseedor de un ecosistema de páramo andino. Fuera de la ciudad, al norte, en la parroquia San Antonio del cantón Quito, se encuentra el monumento de la línea ecuatorial, en medio de una verdadera zona comercial conocida como la Ciudad Mitad del Mundo y administrada por el Consejo Provincial de Pichincha.

En la zona de Guayllabamba, se encuentra el Zoológico de Quito, que alberga pumas, jaguares, osos, monos, leones, venados, cóndores, guacamayos, caimanes y canguros, entre otros. Los valles de Los Chillos y Tumbaco tienen también atractivos, tanto para los habitantes de Quito como para los visitantes. En los mencionados valles, se puede encontrar un clima cálido muy bondadoso para la salud, además de platos de comida típica como hornado, yaguarlocro, fritada, etc.

Para los que busquen actividades de aventura, deportes extremos y adrenalina en Quito está el parque extremo más grande del Ecuador, Nayón Xtreme Valley ubicado en la parroquia de Nayón en San Pedro del Valle, a tan solo 10 minutos del norte de Quito y del Valle de Cumbayá. Es un parque eco-turístico de 5 hectáreas donde se practican deportes como el Canopy, el Paintball y Rapel. También se encuentran caminatas por la naturaleza, pistas de bicicletas y juegos para todas las edades. Un paraíso de la naturaleza con paisajes únicos y un clima privilegiado dentro de la ciudad.

Quito posee el centro histórico más grande, menos alterado y mejor preservado de América. Fue, junto al centro histórico de Cracovia en Polonia, los primeros en ser declarados Patrimonio Cultural de la Humanidad por la Unesco, el 8 de septiembre de 1978. El Centro Histórico de Quito se encuentra ubicado sobre una superficie de 320 hectáreas, construido en forma de dámero y es considerado uno de los más importantes conjuntos históricos de América Latina. Tiene alrededor de 130 edificaciones monumentales (donde se aloja una gran diversidad de arte pictórico y escultórico, principalmente de carácter religioso inspirado en un multifacética gama de escuelas y estilos) y cinco mil inmuebles registrados en el inventario municipal de bienes patrimoniales. 

Caminar por el Centro Histórico de Quito, es una experiencia muy placentera. La Empresa de Desarrollo del Centro Histórico, es la encargada de la restauración y conservación de iglesias, calles y plazas de este lugar. Se han implementado varios sitios turísticos que invitan a propios y extraños a visitar el Centro Histórico en un viaje al pasado, que no solo es turístico sino didáctico. Para este efecto, los miembros de la Policía Municipal han sido capacitados para servir de guías en los mencionados recorridos. Por la noche, cuando las luces encienden la ciudad, es posible dar un paseo por el centro en autobuses turísticos. En esta parte de la ciudad se encuentra el Palacio de Carondelet, que es la sede de la Presidencia de la República.

Esta monumental basílica es la obra más importante de la arquitectura neogótica ecuatoriana y una de las más representativas del continente americano, siendo a su vez la más grande en América Latina. Se ubica en el sector de San Juan; en el centro histórico de la ciudad, en las calles Carchi y Venezuela junto al Convento de los padres Oblatos. Este templo religioso fue edificado para rememorar la consagración del Estado Ecuatoriano al Sagrado Corazón de Jesús, celebrada durante la presidencia de Gabriel García Moreno en 1873. Tiene 115 m de altura lo que convierte a esta iglesia en el edificio más alto de la ciudad con una relación de 40 pisos y está conformada por 24 capillas internas que representan a las provincias del Ecuador. Este santuario fue inaugurado y bendecido por el Papa Juan Pablo II en su visita al Ecuador el 18 de enero de 1985. La Basílica, tanto por su estructura como estilo, es comparada con dos de las grandes catedrales de todo el mundo: la Catedral de San Patricio ubicada en Nueva York y la Catedral de Notre Dame de París.

Un detalle que distingue a la obra es la sustitución de las clásicas gárgolas por reptiles y anfibios propios de la fauna ecuatoriana; además están dispuestos rosetones pétreos que representan a la flora del Ecuador. En el punto más alto de la torre principal se puede observar la ciudad y las montañas que la rodean. La nave central del templo tiene 140 m de largo, 35 de ancho y 30 de alto donde están dispuestas 14 imágenes de bronce que representan 11 apóstoles y 3 evangelistas. A lo largo de la historia de su construcción fueron varios los aportes realizados para que esta obra se lleve a cabo. Los padres Oblatos donaron el terreno donde se erige la Basílica; para proseguir con la construcción se aceptaron donaciones de creyentes quienes proporcionaron piedras a cambio de grabar sus nombres en las mismas. En 1985, el Estado implantó un impuesto por las compras de la sal para continuar con la edificación y se logró terminar la construcción luego que varias generaciones de pica pedreros dedicaran sus vidas para edificar cada pared del recinto. Otro de los atractivos de la Basílica del Voto Nacional es el panteón de jefes de estado del Ecuador.

La Catedral Metropolitana y Primada del Ecuador, por su ubicación en el corazón de la ciudad histórica y su condición de templo mayor de la urbe, es uno de los símbolos religiosos de mayor valor espiritual para la comunidad católica de la ciudad. Este templo inició su edificación en 1562, diecisiete años después de que el obispado de Quito fuera creado (1545). La construcción de la iglesia culminó en 1806, por obra del Presidente de la Audiencia el Barón Héctor de Carondelet. Posteriormente uno de los acontecimientos que se suscitó en este templo fue la muerte del Obispo de Quito de la época, José Ignacio Checa y Barba, quien en la misa del Viernes Santo del 30 de marzo de 1877 fue envenenado con estricnina disuelta en el vino de consagrar. En esta iglesia se encuentran sepultados los restos del Mariscal Antonio José de Sucre. Además los de varios expresidentes de la República, así como también los de obispos y sacerdotes. La Catedral está ubicada en la calle Espejo, en el costado sur de la Plaza de la Independencia.

La Iglesia de La Compañía inició su construcción en 1605, demoró 160 años en ser edificada. Para 1765 se finalizó la obra con la construcción de la fachada del templo. Esta fue hecha por indígenas quienes cuidadosamente plasmaron el estilo barroco en uno de los ejemplos más completos del arte en América. Para 1767 la iglesia fue cerrada a causa de la expulsión de los jesuitas del Ecuador. Cuarenta años más tarde en 1807, fue reabierta por el fraile chileno Camilo Henríquez, de la orden de la Buenamuerte, quien posteriormente formó parte en las luchas de independencia de su país.

Esta iglesia está inspirada en la Iglesia del Gesù de Roma, Italia. Las columnas son una copia de las hechas por Bernini en la basílica de San Pedro de Ciudad del Vaticano. En el interior, cuenta con bellísimos retablos y púlpitos cubiertos con pan de oro. En el retablo del altar mayor, obra de Legarda, se ha retomado como principal motivo de composición las columnas salomónicas de la fachada y las cornisas que se estiran al centro en arco, y se ha hecho culminar el conjunto, abigarrado y deslumbrante, en una corona sostenida por ángeles. La iglesia está ubicada entre las calles García Moreno y Antonio José de Sucre. 140 años después del terremoto que destruyó la torre-campanario de la Iglesia de la Compañía de Jesús, el Municipio de Quito empezó la re construcción de la torre campanario de 45 metros, la cual tendrá las mismas características con las que contaba antes.

El templo está localizado en la intersección de las calles Benalcázar, Bolívar, Sucre y Cuenca. Se encuentra una cuadra más adelante de la iglesia de La Compañía; es el más grande de los conjuntos arquitectónicos existentes en los centros históricos de las ciudades de América Latina. La construcción de la iglesia se inició en 1550, en terrenos aledaños a la plaza donde los indígenas realizaban los trueques de productos. La obra estuvo a cargo del franciscano flamenco Jodoco Rique. La iglesia, concluida definitivamente hacia 1680 es el resultado armonioso de influencias mudéjares, manieristas y barrocas. Los frailes franciscanos fueron los primeros que se establecieron en Quito. Atractivos como el altar mayor del templo, las capillas laterales y el púlpito son de excepcional belleza. En el altar mayor se encuentra la imagen de la Virgen de Quito, tallada por Bernardo de Legarda, maestro de la escuela quiteña.

Cuenta la leyenda que un indio apellidado Cantuña se comprometió a construir el atrio de este templo; el tiempo de entrega era corto y Cantuña no iba a finalizar la obra en el plazo acordado, de tal manera que al verse perdido hizo un pacto con el Diablo. Este a cambio, le pidió su alma y Cantuña aceptó. Los diablillos comenzaron la construcción que demoró una noche. Cumplida la obra, Cantuña rezó a la Virgen para que le salvara de ser llevado por el demonio, y cuando Lucifer vino a buscar el alma de Cantuña, descubrió que faltaba una piedra por colocar y por tal motivo el pacto quedó anulado. De esta forma salvó su alma.
Hay rumores de queCantuña era el familia de Atahualpa, el heredo la riqueza de él y con eso se construyó la iglesia, ya que él se encuentra enterrado en la iglesia pero la iglesia predominante en ese tiempo oculto la historia y la convirtió en leyenda para atraer a la gente, es una teoría que falta de peobar pero si hay fundamentos.

En tiempos de la Colonia, la iglesia de El Sagrario constituyó uno de los mayores baluartes arquitectónicos de Quito. La construcción, de estilo renacentista italiano y edificada a finales del siglo XVII, cuenta con una mampara que posee acabados, esculturas y decoraciones que la caracterizan por su enorme belleza. Esta estructura fue construida por Bernardo de Legarda. Su bóveda central desemboca en una soberbia cúpula decorada con pinturas al fresco de escenas de la Biblia protagonizadas por arcángeles, obra de Francisco Albán. El retablo del altar mayor fue dorado por Legarda. Está ubicada sobre la calle García Moreno, junto a la Catedral.

Aunque llegaron a Quito en 1541, recién en el año 1580 los dominicos comenzaron a construir su templo, con planos y dirección de Francisco Becerra. La obra total concluyó en la primera mitad del siglo XVII. En el interior del templo se encuentran valiosas estructuras, como el altar mayor neogótico que fue colocado a finales del siglo XIX por dominicos italianos. El techo de la iglesia de estilo mudéjar, cuenta con pinturas de mártires de la Orden de Santo Domingo. La cubierta de la nave central está compuesta por una armadura apeinazada de par y nudillo, recubierta en el interior por piezas de lacería. En el museo situado al lado norte del claustro bajo, se encuentran estupendas piezas de los grandes escultores quiteños tales como: el "Santo Domingo de Guzmán" del Padre Carlos, el "San Juan de Dios" de Caspicara, y el "Santo Tomás de Aquino" de Legarda. Una de las joyas barrocas del siglo XVIII que se cuida celosamente es la Capilla de Nuestra Señora del Rosario, la cual constituye un baluarte de la arquitectura de Quito. Esta capilla fue construida junto a la iglesia, del lado del evangelio. En ella se fundó la más importante cofradía de la ciudad de Quito.

El Panecillo es una elevación natural de 3.000 metros sobre el nivel del mar, enclavada en el corazón mismo de la ciudad. Por su ubicación se ha convertido en el más importante mirador natural de la ciudad, desde el que se puede apreciar la disposición urbana de la capital ecuatoriana, desde su centro histórico y hacia los extremos norte y sur. El Panecillo está coronado por una escultura gigante de aluminio de la «Virgen de Quito», creada por el español Agustín de la Herrán Matorras, el cual se basó en la obra compuesta por Bernardo de Legarda, uno de los más importantes representantes de la Escuela quiteña. Compuesta por siete mil piezas diferentes, esta es la mayor representación de aluminio en todo el mundo. La obra, inaugurada el 28 de marzo de 1975, es una réplica de la escultura de 30 centímetros realizada en el siglo XVIII por el escultor quiteño Bernardo de Legarda, la misma que reposa en el altar mayor de la iglesia de San Francisco, y que está considerada como la obra cumbre de la escultura de la escuela quiteña colonial.

Debido a encontrarse a 2800 metros sobre el nivel del mar y por encontrarse en la zona ecuatorial, Quito es una ciudad de varios contrastes, es una ciudad de altura pero con varios pisos climáticos dentro de la misma y a sus alrededores; en la mañana hasta cerca del atardecer el clima va de tibio-muy caliente-tibio esto debido a encontrarse en la zona tropical, hasta llegar a la noche en que se pone el clima frío y en ocasiones muy frío, esto debido a que el clima se ve modificado por la cadena montañosa llamada "los Andes" y cuyos habitantes visten de acuerdo al clima que se presente, desde forma muy ligera hasta de forma abrigada. Hasta principios de los años sesenta el sombrero era pieza fundamental del guardarropa quiteño de antaño. En ocasiones el clima en la ciudad se comporta en forma desconcertante, el mismo día puede presentarse muy caluroso y a las pocas horas llover muy fuerte para luego tornarse nuevamente soleado o aún más extraño y risible para el extranjero recién llegado, en ciertas partes de la ciudad llueve mientras que en otra se observa totalmente iluminada por el sol. La ropa de abrigo aún es hoy de uso generalizado, sobre todo por las noches, pero en el día se puede ver a los habitantes de la ciudad que llegan a sudar por el calor y llevar ropa de veraneo, pero de cualquier forma el vestuario es acorde a los tiempos actuales de forma occidental moderna; la Tº promedio tanto en el día con 25 °C-26 °C, como en la noche que transcurren a 15 °C-17 °C y a la madrugada transcurren entre 10 °C-12 °C.
La vida nocturna de la ciudad gira alrededor de la Plaza El Quinde más conocida como "Plaza Foch", en el sector de La Mariscal. Son numerosas las terrazas al aire libre, restaurantes, cafés, bares, discotecas y karaokes que abren sus puertas cuando las galerías de arte, librerías y tiendas de artesanías del sector las cierran. Otra nueva opción para la tertulia, sobre todo para el público adulto y de mediana edad, se encuentra en el centro histórico de la ciudad, específicamente en el remozado barrio de La Ronda.

La zona de farra según la jerga de los la capitalinos es conocida como La Mariscal. En ella se concentran alojamientos para mochileros y extranjeros de todo el mundo, restaurantes de varios tipos para igual variedad de presupuestos que los se que encuentran por el resto de la ciudad; los bares, cafeterías, cybers, tiendas de libros y souvenirs y algunas discotecas que cierran sus puertas a altas horas de la mañana. Debido a su variada oferta, se puede andar toda la noche en la Mariscal. Los restaurantes de la zona ofrecen comida italiana, peruana, mongola, ecuatoriana, argentina, francesa, tapas españolas, o de cualquier rincón del mundo. Por precios convenientes se puede comer muy bien en varios de ellos, también los hay de mayor presupuesto para quien quiera proporcionarse un lujo o vaya con compañía a quien quiera impresionar. También se puede encontrar locales pequeños que sirven comida rápida barata junto con cervezas de precio muy cómodo.

El centro neurálgico de la zona es la Plaza Foch donde se concentran varios restaurantes y bares con terrazas que se llenan desde las horas de la tarde. En cuanto a la pura farra, se encuentra fundamentalmente en el sector de "La Mariscal" en cuanto a localización y oferta lúdico-festiva. Se puede observar multitud de bares con terrazas en el primer piso, discotecas, etc, con diversidad de ofertas para atraer a las personas que lo deseen, mucha gente joven local y extranjera yendo y viniendo o tomando en la calle medio a escondidas y mucho control policial para evitar potenciales riesgos. En sus discotecas se pueden ver diestros y algunos principiantes locales y extranjeros intentando bailar cualquier ritmo o pasos latinos, ganándose a opinión de los turistas un lugar privilegiado entre las mejores ciudades farreras en Sudamérica. La farra comienza a ser muy movida desde el jueves.

Quito ha sido sede de conciertos importantes a nivel nacional, esto se debe a las bajas tasas que se pagan por realizar eventos de gran envergadura, y además que la ciudad ofrece todo lo necesario para la realización de los mismos.

Entre los principales artistas presentados en la ciudad son: Guns N' Roses, Aerosmith, Iron Maiden, Justin Bieber, Miley Cyrus, Jonas Brothers, Metallica, Simple Plan, Bon Jovi, Korn, Cypress Hill, Enrique Bunbury, Ozzy Osbourne, Maná, Shakira, Ricky Martin, Marc Anthony, Chayanne, Elton John, Big Time Rush, David Guetta, Paul McCartney, Lacrimosa, 30 Seconds To Mars, Dimitri Vegas & Like Mike, Therion, Joaquín Sabina, Kiss, Miguel Bosé, Roxette, Maroon 5, entre otros. Además de la realización del certamen de Miss Universo en el año 2004.

Las fiestas de Quito son unas de las fiestas ciudadanas y populares, más importantes a nivel nacional. Esta se caracteriza por la presencia de: las bandas de pueblo, tarimas para todo tipo de expresión artística en muchos puntos de la ciudad. Se celebra desde fines del mes de noviembre donde se empieza a sentir en el ambiente un aíre festivo y culminan el 6 de diciembre, día de la fundación española de la ciudad. A esta vienen visitantes de todo el país y muchos extranjeros;
el 5 de diciembre la ciudad se paraliza producto del despliegue de algarabía, color, alegría incontenible y fiesta total. También se efectúan en la ciudad conciertos de diverso tipo de música, con muchos artistas locales e internacionales, con bailes generales callejeros, desfiles de varias expresiones culturales locales e invitados de todo el mundo, y ferias gastronómicas.

Destacan también la presencia de chivas (vehículos representativos de la cultura costeña, desprovisto de ventanas y puertas) que sirven para realizar city-tours, las cuales transportan a gente que baila al son de una banda de pueblo. Dichos vehículos son autorizados por el Ayuntamiento a circular por la ciudad en forma temporal previa revisión mecánica y de seguridad.

Su población es de 2'011.388 habitantes en el área urbana y más de 3 millones en todo su área metropolitana.

Quito es la Ciudad más poblada del Ecuador 

Aunque para datos reales de población de la ciudad que se obtiene con la aglomeración urbana o la conurbación de Quito, notablemente visible por las localidades suburbanas de la ciudad, esto es considerando, incluso, las parroquias de Conocoto, Amaguaña, Cumbayá, Nayón, Zámbiza, Llano Chico, Calderón, Pomasqui, San Antonio, Tumbaco, Guangopolo, Puembo, Alangasi, La Merced,y Sangolquí dan una población real de la ciudad de Quito en 2 495 043 habitantes.

La población étnica de la ciudad es marcado por un aspecto diverso, en el hecho de las diversas etnias que conviven en la misma ciudad, mayormente conviven personas de raza mestiza junto a la blanca, esta última ha significado un enorme incremento junto a la asiática y la árabe desde 2003 aunque disminuyó en 2009, se ha mantenido como una de las más crecientes.

Según las cifras presentadas por el Instituto Nacional de Estadística y Censos Ecuador INEC en el censo realizado en 2010, la composición etnográfica del cantón Quito es:

Los barrios de Quito son la división política, y en ocasiones administrativa, más pequeña de la ciudad y el Distrito Metropolitano de Quito. Los habitantes de la urbe tradicionalmente la han dividido en cuatro grandes segmentos, los cuales abarcan en gran medida su territorio, estos son: «"el norte"», conformado en su límite septentrional por las parroquias de Cárcelen y el Condado y en el austral por Belisario Quevedo y Mariscal Sucre; «"el centro"», compuesto por Centro histórico, San Juan e Itchimbia; «"el sur"», en el cual en su extremo norte están halladas Magdalena, Chilibulo y Puengasí y en su borde meridional Guamaní y Turubamba; y los «"valles"», que pese a no formar parte de la ciudad de San Francisco de Quito, componen el Distrito Metropolitano.

Las parroquias urbanas que conforman esta división no oficial, suelen subdividirse en barrios. A su vez, estos -por encontrarse a diferente altitud- pueden adquirir el término de bajo o alto según corresponda, sin ser por ello renombrados; así, un barrio como el Batán, dependiendo el lugar al que se refiera el interlocutor, puede ser calificado como «alto» o «bajo» (esta práctica solo se utiliza en los lugares cuyas construcciones estén sobre laderas. Generalmente los habitantes suelen guiarse por esta práctica y por ello los límites geográficos de un determinado barrio están sujetos a la opinión que cada ciudadano tiene sobre él, ya que actualmente no existe una demarcación específica determinada por el Municipio de la ciudad.

El sur de la ciudad se caracteriza por ser una zona de alta inmigración nacional en continuo crecimiento, renovación y alta actividad comercial. En la parroquia de Chillogallo, una de las más grandes y densas de la capital, los negocios proliferan en todas partes, especialmente los de todo tipo de servicios. Uno puede degustar desde el tradicional pollo asado, hasta platos típicos de todo el Ecuador como: la guatita, el ceviche, seco de chivo o la fritada. Aquí se ubica uno de los parques industriales más grandes de la ciudad, la Estación de Trenes de Chimbacalle, el nuevo terminal terrestre de la ciudad y el centro comercial más grande del Ecuador entre otras cosas destacables. Es importante resaltar que en el sur de la ciudad se encuentran las zonas verdes y los parques urbanos más grandes de la ciudad y del país, como el parque "Las Cuadras" de 24 hectáreas, "El Parque Metropolitano del Sur" de 672 hectáreas, y muchos otros.

En el centro de la ciudad, las calles son estrechas por tratarse el sitio donde nació la ciudad en sus albores, razón por lo cual se restringe el acceso a los vehículos durante los fines de semana, pero es atendida diariamente por el servicio de transporte público de trolebús. Este hermoso espacio urbano de la época colonial es muy llamativa y considerada como "la joya de la corona", sobre todo sus grandes iglesias, conventos, museos. Aquí también se encuentra ubicada la casa presidencial conocida como el Palacio de Carondelet. El municipio de Quito ha desarrollado un importante plan de restauración de la parte colonial de la ciudad también llamado "El Centro Histórico" especialmente por tratarse de una zona turística por excelencia con una variada riqueza social-cultural-arquitectónica y manteniendo esa atmósfera de antaño que sus habitantes han sabido conservar.
La zona limítrofe del norte con el centro histórico ha desarrollado una serie de edificaciones y torres elevadas, la más alta de las cuales es la Basílica del Voto Nacional de estilo gótico, con una altura de 36 plantas, y con un mirador excepcional de Quito. Los edificios como la Torre CFN, la Torre Corpei, la Torre Diez de Agosto, el Edificio Benalcázar Mil o la Torre Consejo Provincial de Pichincha son algunas de las construcciones quiteñas que sobrepasan las veinte plantas esto especialmente por las "ordenanzas municipales que limitan la altura de construcción en toda la ciudad", esto por que el aeropuerto internacional se encontraba enclavado dentro de la ciudad hasta febrero de 2013.

Muchos de los barrios del norte de la ciudad son de carácter residencial donde las nuevas generaciones, los más pudientes y hombres de negocios escogieron para vivir, es también donde se encuentra ubicado el centro financiero, bancario, sede de muchas empresas multinacionales y embajadas. Las casas matrices de muchos de los principales bancos que operan en Ecuador se encuentran ubicadas en esta parte de la ciudad, así como otras entidades de trascendental importancia como la Bolsa de Valores de Quito, el Banco Central del Ecuador, el Servicio de Rentas Internas, la Superintendencia de Bancos, entre otras.

Aquí es donde se puede apreciar lo más representativo de la arquitectura ecuatoriana actual, representada en muchas edificaciones levantadas para el funcionamiento de la banca, el comercio, la diversión, compras, etc. La mayoría de estas se encuentran ubicadas alrededor de un parque urbano muy conocido como "La Carolina" de 67 hectáreas. En este sector de la ciudad, es donde se puede apreciar claramente la fuerza del turismo con el cual se ve beneficiada, muchos de los transeúntes son de origen extranjero, pudiendo apreciarse los más disimiles puntos de origen de los ilustres visitantes.

Y es precisamente esta parte de Quito, donde se ha consolidado por la fuerza del propio turismo y vigor de la vida joven hace mucho tiempo, una gran "zona rosa" en la que se concentran múltiples bares, cafés, discotecas, casinos, karaokes, restaurantes, hoteles, etc, etc, conocida comúnmente como "La Mariscal", en la cual sus habitantes nativos, extranjeros residentes y turistas de todo el mundo disfrutan de múltiples terrazas al aire libre, para disfrutar de la ciudad, sus muchas y variadas manifestaciones artísticas, mientras se degusta de la gastronomía o simplemente se toma una copa.

La última y más notable zona de expansión de la ciudad se sitúa en las regiones suburbanas que en su casi totalidad se hallan ya fusionadas con el área administrativa urbana de la ciudad pero forman parte real y física de la ciudad de Quito, esta se desarrolla mayormente alrededor de los valles de los Chillos, Tumbaco, la meseta de Calderón; así como el valle de Pomasqui (mitad del mundo), las comunidades de Amaguaña, Puembo, entre otros y hasta el área urbana de Sangolqui que es un área administrativa separada, pero fuertemente ligada y dependiente de Quito. En todas estas regiones, se caracterizan por ser zonas residenciales de las personas que trabajan en otras zonas de la ciudad principalmente, pero albergan también centros comerciales, universidades, parques, instituciones, industrias, entre otros. <br>

En toda la urbe Capitalina se utiliza, el término "Veci" ,"Vecina" o "Vecino" . Esta terminología se utilizaba en los barrios populares Quiteños desde antes de los años 50 ; y forma parte del comportamiento cultural de la sociedad quiteña,
por lo que personas de diferentes estratos se relacionan a través de esta palabra.

El Aeropuerto Internacional Mariscal Sucre que sirve a la ciudad de Quito fue inaugurado el 20 de febrero de 2013 tras 7 años de construcción como remplazo del Antiguo Aeropuerto Internacional Mariscal Sucre, inaugurado en los años 60 y que por estar ubicado en medio de la ciudad tenía un alto riesgo para la misma. El Aeropuerto de Quito tiene una pista de 4.100 metros, la torre de control más alta de Latinoamérica, de 41 metros de alto y un terminal de carga con varios cuartos fríos para la exportación de flores y otros productos perecederos. Está ubicado en la parroquia Tababela al oriente del Distrito Metropolitano.

El acceso al Aeropuerto es a través del Conector Alpachaca, que se conecta con la vía E28C también conocida como Interoceánica, otro acceso es la E35 vía que es usada para el transporte de carga. Al momento se construyeron 2 vías de acceso más, la Ruta de Integración a los Valles o "Ruta VIVA" a cargo del Municipio de Quito que fue inaugurada el 12 de diciembre de 2014 y la Ruta Collas-Nuevo Aeropuerto a cargo del Gobierno Nacional que fue inaugurada el 1 de agosto de 2014.

La movilidad con vehículo privado dentro de Quito, como en cualquier gran urbe latinoamericana, es complicada. Debido a la geografía de la ciudad, la cual se extiende de norte a sur teniendo aproximadamente 50 km de largo y solo 8 km de ancho, la gran mayoría de avenidas importantes de Quito se extienden de norte a sur. La avenida más larga que cruza la ciudad de norte a sur es el Eje Longitudinal Avenida 10 de Agosto (que se transforma en la Avenida Galo Plaza al norte y Avenida Vicente Maldonado al sur). La avenida que cruza la ciudad de norte a sur del lado occidental, es la Avenida Occidental Mariscal Sucre y la autopista que cruza la ciudad de norte a sur del lado oriental es la Autopista Corredor Periférico Oriental Simón Bolívar.

Se encuentran en proyección y ejecución, algunas avenidas transversales, tanto en el centro, sur y norte de la ciudad con conectividad a los dos valles orientales de Quito, con las que la ciudad crecerá efectivamente hacia sus conurbaciones, y entonces el área de la ciudad, triplicará la actual. En estos mismos valles, ya se encuentran en construcción, avenidas, autovías, autopistas, intercambiadores, etc., que preparan a la ciudad, al "Gran Quito", al reto de crecer en el próximo lustro hacia esas latitudes, estimulado por el Nuevo aeropuerto de Quito, que está ubicado en el valle de Tumbaco.

La ciudad cuenta con varios intercambiadores de tráfico que facilitan el tránsito. Entre los más importantes se encuentra el intercambiador de El Trébol, ubicado en el centro de la ciudad, que conecta al centro con el sur y el valle de los Chillos. También el intercambiador de Miravalle, por el que cruzan las avenidas Nueva Oriental e Interoceánica, conectando al centro norte de la ciudad con el valle de Tumbaco, el norte y sur de la ciudad. También el Intercambiador de Carcelén, que conecta a la ciudad con la carretera Panamericana Norte. El intercambiador de las avenidas 10 de Agosto, Eloy Alfaro y Francisco de Orellana en el centro-norte de la ciudad, conectando los sectores de La Mariscal e Iñaquito. Uno de los intercambiadores más importantes de la zona urbana es La Y, en el que se conectan las avenidas América, 10 de agosto, La Prensa, Brasil y Gaspar de Villarroel.

El transporte de la ciudad es administrado por el Sistema Integrado de Transporte Metropolitano, más conocido por sus siglas SITM-Q, el mismo que opera la totalidad de los sistemas masivos de transporte de la ciudad, tanto públicos como privados. El SITM-Q está conformado por el Metro de Quito, el sistema Metrobus-Q y la red de autobuses.

Para el año 2020, se prevé la inauguración de la primera línea de Metro de Quito, la misma que operará en integración con la Red Integrada de Transporte Público de Quito. En un principio, contará con una sola línea de 15 estaciones, que recorrerá la ciudad desde Quitumbe (sur de la ciudad) hasta El Labrador, en la cabecera sur del actual parque Bicentenario (norte de la ciudad), tendrá una duración de viaje de 34 minutos. El Metro se planea como el sistema principal del servicio de transporte público de Quito, el cual tendrá un gran salto cualitativo.

El Sistema Metrobus-Q está constituido por 5 líneas (llamadas corredores) con buses brt de gran capacidad tanto en el área urbana de la ciudad. La red se complementa con un sistema de paradas prestablecidas, estaciones de transferencia y terminales. Se está planificando un nuevo corredor brt para el norte de la ciudad, Corredores transversales para conectar las zonas del oeste con las del este y Corredores para las Zonas Metropolitanas (por ejemplo: Cumbayá, Tumbaco, el Valle de los Chillos, Calacalí, Mitad del Mundo) Los nuevos corredores deberán estar listos antes del 2019 año en el que se inaugura el Metro de Quito. El Sistema Metrobus-Q tiene también rutas alimentadoras con buses convencionales.



La Red Convencional de Transporte de Quito está conformada por 135 líneas de transporte público operadas por 2.624 buses urbanos, que de acuerdo a las ordenanzas del municipio no pueden tener más de 10 años de servicio. Estas líneas y flotas actuales se encuentran en proceso de reestructuración, en la medida del avance de la Red Integrada de Transporte.

En la ciudad de Quito existen tres clases de transporte urbano: los buses tipo, que constituyen la mayoría; los buses especiales, que sirven a los sistemas integrados de transporte; y los buses interparroquiales, que unen el área urbana con sus distritos rurales. Es fácil diferenciarlos de acuerdo al color que ostentan: azul para los buses tipo, rojo para los especiales y verde para los interparroquiales.

Quito cuenta con dos terminales terrestres que comunican a la ciudad con el resto del país, uno de gran envergadura en el sur de la ciudad llamado "Quitumbe", por el sector en el que se ubica, y que recibe y embarca pasajeros con destino a las provincias del centro y sur del país. Y el Terminal Terrestre de Carcelén más pequeño, que hace lo mismo con los buses que se desplazan desde y hacia las provincias del norte, con mayor concurrencia desde y hacia la Villa de Ibarra.

Quito cuenta con más de 60 kilómetros de Ciclovías, la primera ciclovía implementada en 2004 (llamada Ciclo-Q) recorre los parques lineales del Sur de Quito, el Centro Histórico y la Avenida Amazonas hasta la estación La Y del Trole también ese año se implementó la Ciclovía "Interuniversitaria" que a través de la Avenida Carrión conecta de este a oesta la Universidad Central del Ecuador con las Universidades Católica, Salesiana y la Escuela Politécnica Nacional. En 2012 con el inicio de operaciones del sistema BiciQ se dio una gran ampliación de la red de ciclovías; en las siguientes avenidas: Av. de la Prensa, Av. Gerónimo Carrión, Av. Diego de Almagro, Av. Luis Cordero, Av. Antonio de Ulloa, Av. Veracruz, Av. Atahualpa, Blvd. Naciones Unidas y Av. Mariana de Jesús.

Quito también cuenta con ciclovías recreativas en los Parques Lineales del Sur, Parque El Ejido, Parque La Alameda, Parque La Carolina, Parque Itchimbía, Parque Metropolitano Guanguiltagüa, y el Chaquiñan de Cumbayá-Tumbaco que es un sendero ecológico de 28 kilómetros creado sobre una línea férrea abandonada. Además la Universidad Católica cuenta con una red interna de ciclovías, denominada CicloPuce.

En 2012 la Alcaldía de Quito implementó un sistema de alquiler de bicicletas públicas denominado BiciQuito (antes conocido como BiciQ. l sistema consta de casi dos mil bicicletas con un diseño único, distribuidas en 30 estaciones ubicadas en lugares cercanos a los puntos de mayor afluencia o interés comercial, bancario, turístico y estudiantil. Para acceder al sistema los usuarios deben registrarse en el sitio web www.biciq.gob.ec y tras pagar 25$ por año y firmar un contrato de buen uso se le otorga un carné de usuario, que sirve para hacer uso de las bicicletas de 7 de la mañana a 7 de la noche los 365 días de cada año. El perímetro de aplicación del sistema es en el denominado "Hipercentro" entre el Centro Histórico y el sector de "La Y" y se estudia expandir el sistema hacia el Sur y el Norte de la ciudad. Cada Bicicleta se puede usar por 45 minutos y debe ser entregada en cualquier estación, si el usuario ya cumplió ese tiempo y no ha llegado a su destino debe esperar 10 minutos antes de poder acceder nuevamente al sistema.

De acuerdo a recomendaciones de la Organización Mundial de la Salud (OMS), cada territorio debe contar con al menos 9 metros cuadrados de espacios verdes por habitante. En el país solamente 10 cantones superan esa recomendación, Mera (Pastaza), Huamboya (Morona), el Distrito Metropolitano de Quito, Mocha (Tungurahua), El Pan (Azuay), Pablo Sexto (Morona Santiago), Sigchos (Cotopaxi), Paute (Azuay), Quero (Tungurahua), Saquisilí (Cotopaxi).

De acuerdo al estudio del Instituto Nacional de Estadísticas y Censos de 2010, Quito tiene 20,4 metros cuadrados de áreas verdes por habitante, la tercera más alta del país. Los parques que se encuentran en el Distrito Metropolitano de Quito además de ofrecer amplios espacios destinados a la práctica deportiva y a la recreación, ayudan a mejorar la calidad de vida de todos los habitantes en medio del sistema urbano, y con el objetivo principal convertirse en una ciudad verde.

El parque se encuentra enmarcado por las avenidas Shyris, Eloy Alfaro, Amazonas y Naciones Unidas. Con 67 hectáreas de terreno es uno de los parques urbanos en medio de la ciudad, más grandes del Distrito Metropolitano, del país y de América del Sur. Durante la semana acoge a deportistas habituales que disfrutan del ambiente tranquilo que ofrece el lugar por las mañanas, mientras que los fines de semana recibe aproximadamente a 50.000 personas. Posee una amplia infraestructura que incluye: canchas de fútbol, baloncesto, tenis, voleibol, trayecto atlético, circuito de bicicrós, pista de patinaje, pista para acrobacias en bicicleta, perímetro de juegos infantiles, áreas de ejercitación deportiva, centro de exposiciones, restaurantes, centros de socialización, cinco lotes para estacionamiento de vehículos, etc.

El parque La Carolina ubicado en el sector de Iñaquito, nació como producto de la expropiación municipal a la hacienda La Carolina en 1939. El diseño moderno fue realizado por la Dirección de Planificación del Municipio en 1976. El Papa Juan Pablo II llevó a cabo en este parque una multitudinaria misa durante su visita a Ecuador en 1985. Para conmemorar este evento, se construyó una cruz gigante en el sitio donde se efectuó la ceremonia.

Otros sitios de atracción ubicados dentro del parque son:









Está localizado en la zona norte del Distrito, cercado por las calles Guanguiltagua, Arroyo Delgado y Analuisa. Con una extensión de 557 ha, es el principal pulmón de la ciudad de Quito. El parque se encuentra ubicado a 2.890 a una máxima de 2.980 m s. n. m. y registra una temperatura media de 11 °C. Rodeado de árboles y obras de arte gigantescas, los visitantes pueden disfrutar de la naturaleza respirando aire puro a pocos metros de la ciudad. En la quebrada Ashintaco ubicada en el sector nororiental del parque, se puede observar las más de diez especies de colibríes y setenta especies de aves que anidan en el lugar, algunas de las cuales están en peligro de extinción. El parque ofrece también un camino de piedra y diferentes senderos para los amantes del ciclismo de montaña y también el Downhill. Cada fin de semana llegan al Parque Metropolitano de Quito aproximadamente entre 20 y 30 mil personas para acampar, hacer pícnic, y muchas otras actividades. El parque resulta un mirador natural por su vista excepcional hacia el oeste (la ciudad propiamente dicha) y al Este (su prolongación en los valles, especialmente a Cumbaya).

Es un parque ubicado donde se asentó el antiguo Aeropuerto Mariscal Sucre y fue abierto al público el sábado 27 de abril de 2013, es el segundo parque más grande de la ciudad (después del Parque Metropolitano).

En los próximos meses se iniciará la fase de arborización que incluye la siembra de 2.800 especies nativas de árboles, en lo que corresponde a la primera etapa. El 89% del área del parque será verde y será un pulmón para la ciudad. Además de crear bosques, se conformarán humedales que alberguen variadas especies de fauna y flora silvestre. Este medio natural se complementará con varias fuentes de agua para la recreación de los visitantes. En el parque se establecerán viveros temporales, arborización permanente, jardines ornamentales, caminerías, canchas, pista atlética, juegos infantiles, grafismo temporal, accesos y estacionamientos.

El parque está delimitado por las avenidas Patria, 6 de diciembre, Tarqui y la calle Guayaquil. El Ejido marca la división entre el Quito antiguo y el Quito moderno, en él habitan alrededor de 1.470 especies de plantas nativas como el cholán, el aliso, el chamburo, las palmeras y los guabos.

En este parque todos los días se juegan partidos de ecuavoley que atraen la atención del público. Además las personas se congregan para disfrutar de los tradicionales juegos populares de los cocos cuyo objetivo consiste en sacar a éstos (bolas grandes de metal) del interior de un círculo trazado en la tierra y se debe eliminar de un pepo (golpe) a los adversarios.

También los fines de semana, y cobijados por la imponente Puerta de La Circasiana (un arco de piedra de 8 metros de alto con grabados renacentistas que en tiempos pasados fue la puerta de entrada al Palacio del mismo nombre), se realizan exposiciones culturales en las que se puede adquirir obras de arte, joyas en plata, ponchos, sacos, chalecos, entre otras novedades.

Está ubicado en el centro de la ciudad, dentro de un triángulo comprendido por la avenida Gran Colombia y las Calles Sodirio y Guayaquil. La Alameda es el parque más antiguo de Quito, era conocido antes por los indios como chuquihuada (que en quichua significa punta de lanza). Se encuentra en la parroquia San Blas y ocupa 6 ha. Este es un sitio que guarda muchas nostalgias y recuerdos, allí funcionó hasta inicios del siglo pasado la Escuela de Bellas Artes de Quito en medio de un ambiente casi místico que marcó el arte de la época. También se encuentra el Observatorio Astronómico de Quito construido en 1864 durante la presidencia de Gabriel García Moreno. En su época fue el mejor equipado de Sudamérica y utiliza aún los instrumentos de observación de ese entonces.

Actualmente los visitantes acuden al parque a descansar en el lugar, o utilizan pequeños botes para navegar en el pequeño lago. En La Alameda todavía se pueden encontrar a fotógrafos que retratan a los visitantes utilizando cámaras de antigua tecnología. En el acceso sur del parque, se inauguró el 24 de julio de 1935 el monumento al Libertador de América Simón Bolívar, acto que constituyó un verdadero acontecimiento político, social y cultural del país. Aquí se puede encontrar árboles de dimensiones importantes que han resistido al tiempo y a la invasión del cemento. Investigadores botánicos registraron una variedad importante de especies nativas y extranjeras como la acacia, la palmera, el cedro, el fresno, el pumamaqui, el yaloman, el arrayán, el eucalipto y la magnolia.

Este parque se encuentra ubicado al sur de la ciudad de Quito, en la Avenida Cardenal de la Torre en el sector de Solanda. Tiene una extensión aproximada de 20 ha, y ofrece varias distracciones para sus visitantes tales como ciclovía, ruta peatonal, áreas recreativas y de deporte, laberintos, esculturas, pileta, espejo de agua, mobiliario, baterías sanitarias, etc.

Se ubica en la cima y en las laderas de la loma Itchimbía la cual se encuentra en el límite oriental del Centro Histórico de Quito, rodeada por el río Machángara y los barrios de El Dorado, La Tola y San Blas. El parque está a 2900 m s. n. m., y es considerado como un mirador natural por la amplia visibilidad que se tiene de la ciudad desde sus cuatro puntos cardinales.

El espacio recreacional cuenta con una ciclovía, una ruta peatonal, una plazoleta y un parqueadero para 150 vehículos. Cada año entre los meses de julio y agosto se celebrá el Festival de Cometas (papalotes), en el que se divisan estos milenarios juguetes chinos surcar los cielos, favorecidos por los fuertes vientes del verano capitalino.

En el interior del parque, y como punto principal del mismo, se encuentra el Centro Cultural Itchimbía, popularmente conocido como Palacio de Cristal; una enorme construcción de hierro y cristal que en tiempos republicanos (finales del siglo XIX y comienzos del XX) sirvió para albergar un popular mercado del Centro Histórico. El diseño original le pertenece al ingeniero francés Gustave Eiffel (el mismo que diseñó la Torre Eiffel - París); el edificio fue desmantelado de su ubicación original, recuperado en talleres especializados del municipio y llevado pieza por pieza para rearmarlo en la cima de milenaria colina del Itchimbía para albergar exposiciones de todo tipo a lo largo del año.

También se desarrolla el festival más grande de música al aire libre denominado Quito Fest, en el mes de agosto de cada año.

Está ubicado en la esquina suroccidental de las avenidas 10 de Agosto y Colón, y forma parte del Palacio de La Circasiana, un palacete levantado por Manuel Jijón y Larrea a finales del siglo XIX, que fue la primera obra civil neorenacentista de proporciones monumentales erigida en Quito. En las inmediaciones del palacete, Jijón edificó dos inmuebles más: una librería dotada con 40 000 títulos y un edificio museo, los que continúan sirviendo a la ciudad.

En la década de 1930, se podía apreciar a la estructura en todo su esplendor. Toda la casa estaba rodeada de jardines bien cuidados, un gran muro exterior y una portada en arco, que la aislaban de toda la ciudad.

La Puerta de La Circasiana ("La despedida de los centauros") fue una de las puertas de ingreso a la hacienda Chillo Jijón y fue cedida por la familia a la ciudad. Hoy, este arco constituye uno de los atractivos del Parque El Ejido. La propiedad es administrada ahora por el Fondo de Salvamento (Fonsal), el cual convirtió al ingreso de la residencia en un hermoso parque adornado con las figuras fantásticas de unicornios.

Está delimitado por las avenidas 12 de Octubre, 6 de diciembre y Tarqui, además se encuentra junto a la Casa de la Cultura Ecuatoriana. Muchos consideran este pequeño espacio verde como parte de su vecino: El Ejido, aunque en realidad se trata de un lugar diseñado bastante tiempo después.

En los terrenos del actual parque de El Arbolito se encontraba hasta mediados del siglo XX el único estadio de fútbol de la capital ecuatoriana. Al construirse el Estadio Olímpico Atahualpa en Iñaquito el predio fue cedido al municipio y este lo convirtió en el espacio recreacional que admiramos en la actualidad.

Este parque es también conocido por ser el punto de encuentro de las comunidades indígenas ecuatorianas en sus marchas hasta el Palacio de Carondelet. En las fiestas celebradas por la Fundación de Quito cada diciembre, el lugar se transforma en un gran patio de "Comidas típicas del Ecuador".
Tiene una extensión de 48 hectáreas, dentro del parque podemos encontrar una variedad considerada de especies de flora y fauna. Se puede evidenciar las diferentes especies de aves como colibríes y palomas. Es el lugar perfecto para compartir con mascotas, realizar deporte, senderismo, yoga, camping y además en un lugar de recreación natural. Los principales usuarios son los habitantes del Valle de Los Chillos: Conocoto, Alangasí, Guangopolo, Pintag, Amaguaña y La Merced. El Parque cuenta con senderos y ciclovías.

Tiene una extensión 12 hectáreas. 250.000 habitantes de la parroquia de Puengasí, de los barrios de San José de Monjas, Jardín del Valle, Alma Lojana son sus usuarios. El beneficio más importante para la comunidad es la recuperación del relleno de la quebrada Cuscungo como un espacio verde reforestado, con áreas deportivas, caminerías, ciclovía y mobiliario para el disfrute de los habitantes de Quito. El parque cuenta con senderos y ciclovías con una extensión de 1,6 kilómetros aproximadamente.

Cuenta con 320 hectáreas y está ubicado en la parroquia de Chillogallo. Atraviesa varios sectores del sur de la ciudad desde La Magdalena, cruzando por la Mena Dos hasta Lloa. Se ubica a 4,5 kilómetros aproximadamente desde la Mariscal Sucre, ingresando por la Mena Dos por la Vía a Lloa.

Posee una superficie de 620 hectáreas y está ubicado al sur de la ciudad en la parroquia Quitumbe, entre la avenida Simón Bolívar, sector el Troje, a 7 kilómetros aproximadamente del intercambiador de la Simón Bolívar y autopista General Rumiñahui. A este parque acuden los vecinos de Santa Rosa, Quitumbe, Chillogallo, Amaguaña, el Valle de Los Chillos, Guamaní, Guajaló, Músculos y Rieles, Buenaventura, San Juan de Turubamba, Cuapichu, Cataguango, entre otros.

Posee 24 hectáreas y se ubica al sur de la ciudad, en la avenida Rumichaca y calle Matilde Álvarez, sector de Quitumbe. A este sitio acuden vecinos de los barrios del sur de la ciudad como Solanda, Quitumbe, Guajaló, Oriente Quiteño, Registro Civil, Santa Rita, Las Cuadras y Chillogallo.

También cabe mencionar los nombres de otros parques urbanos importantes que forman parte de la ciudad de Quito: Parque Inglés, Parque de la Mujer y el Niño, Parque Julio Andrade, Parque Lineal del Machángara que va paralelo al río de su mismo nombre, YAKU - Museo-Parque del Agua, Parque Monteserrín, La Moya, Parque de La Magdalena, entre otros.

La ciudad cuenta con excelente infraestructura para la educación, tanto pública como privada. La educación pública en la ciudad, al igual que en el resto del país, es gratuita hasta la universidad (tercer nivel) de acuerdo a lo estipulado en el artículo 348 y ratificado en los artículos 356 y 357 de la Constitución Nacional. Varios de los centros educativos de la ciudad cuentan con un gran prestigio. La ciudad está dentro del régimen Sierra por lo que sus clases inician los primeros días de septiembre y luego de 200 días de clases se terminan en el mes de julio.

La ciudad y sus alrededores cuentan con varias universidades de pregrado y postgrado. Desde su fundación Quito ha sido la capital académica y universitaria del país, la fundación de universidades de congregaciones católicas como la Santo Tomás de Aquino y la San Gregorio definieron el rumbo de lo que hoy son la Pontifica Universidad Católica del Ecuador y la Universidad Central del Ecuador. Así mismo durante la presidencia del ilustre conservador Gabriel García Moreno se fundó la Escuela Politécnica Nacional, mientras que otras instituciones de educación superior se fundaron posteriormente, tal es el caso de la Escuela Politécnica del Ejército, la Universidad de las Américas (parte de la Red Laureate), la Universidad San Francisco, la Politécnica Salesiana y la Universidad de Los Hemisferios (UDH).

Quito posee a tres de las cinco universidades con la mayor red de investigación y desarrollo del país, la Pontificia Universidad Católica del Ecuador, la Universidad San Francisco de Quito y la Escuela Politécnica Nacional; ubicándose en el primero, tercero y cuarto puesto respectivamente. El mayor campus universitario se encuentra ubicado dentro de la Universidad Central del Ecuador. Únicamente tres universidades de la ciudad son públicas (UCE, EPN y ESPE), el resto de universidades son privadas y dependiendo de cada una sus matrículas y pensiones suelen ser holgadas o elevadas como es el caso de la USFQ.

Quito posee casi un 78% de la oferta curricular nacional, pues en conjunto entre todas las universidades posee estudios en Medicina, Arquitectura, Ingenierías, Comercio, Economía, Jurisprudencia, Ciencias Físicas y Matemáticas, Tecnologías, Ciencias Químicas, Arte, Educación, Filosofía, Teología, Sociología, Deportes, Cine, Música, Electrónica, Robótica, Antropología, Veterinaria, Ciencias Ambientales, Ciencias Agrícolas, Minas y Petróleos, Odontología, Diseño, Comunicación, Idiomas, Gastronomía, Hotelería, Historia, Psicología, Geología, Política, Artes Liberales entre muchas otras. El resto de la oferta curricular nacional resulta estar relativamente cerca de la ciudad pues se concentra mayormente en la Ciudad del Conocimiento en la provincia limítrofe de Imbabura, donde se ofertan carreras como Nanociencias, Polímeros, Energías Renovables, Biomedicina, Ciencias Digitales, Sostenibilidad, entre otras; carreras como las de Aeronáutica, Ingeniería Naval y Ciencias Forestales se encuentran algo lejanas, en Guayaquil y Tena respectivamente; y Biología e Investigación evolutiva en Galápagos.

La infraestructura universitaria en Quito es por lo general óptima, con ciertas excepciones, tal es el caso de la Universidad Central del Ecuador la cual se halla severamente en deterioro por falta de inversión en la misma, algunas facultades de esta universidad datan desde hace más de cincuenta años en los cuales no ha existido una remodelación o intervención oportuna. Sin embargo el total del resto de universidades de la ciudad cuentan con campus bastante bien conservados, algunos de ellos muy modernos y vanguardistas, eclécticos y clásicos. La universidad que cuenta con mayor cantidad de campus es la Universidad de las Américas con cuatro (Granados, Queri, Colón y Udlapark). A lo largo de la Avenida 12 de Octubre e Isabel la Católica se hallan los campus de cuatro universidades (EPN, PUCE, UASB y UPS-Q).

La ciudad de Quito cuenta con la mayor cantidad de museos del país: superan los 60, convirtiéndola en el eje fundamental de la cultura de Ecuador estos atraen a muchos turistas y trae igualmente una economía a Quito, los de arte y cultura que abundan en el Centro Histórico los museos interactivos al Sur y Centro de la ciudad, de pintura como el Museo Capilla del Hombre en el norte. Entre los más representativos, tenemos:








En Quito es posible encontrar una amplia gama de alimentos. Existen zonas especializadas en la oferta de comida preparada, como restaurantes y cafeterías. En otros sitios de la ciudad es posible encontrar restaurantes internacionales y de alta cocina, que representan las tradiciones culinarias de países tan diversos como Francia, Italia, Portugal, Polonia, España Japón, China, Perú, Argentina y Brasil. Desde luego, también existen importantes establecimientos dedicados a la gastronomía ecuatoriana de todas las provincias del país. En lo que respecta a la gastronomía local, la urbe es sede de eventos gastronómicos de envergadura nacional.

La ciudad de Quito cuenta con más de 450 establecimientos gastronómicos (entre restaurantes, bares y cafeterías), los que ofrecen una gran diversidad de estilos culinarios. Desde los establecimientos reconocidos por su comida típica ecuatoriana hasta los sabores de las altas cocinas francesa, italiana o argentina. Para los turistas que llegan a la ciudad, existe una gran herramienta que los puede ayudar a encontrar el lugar ideal donde ir a comer, tomar un trago o un café; ahí podrán encontrar establecimientos gastronómicos por tipo de comida, precio promedio, ubicación y/o ambiente.

Tal como cualquier capital, la ciudad de Quito tiene una variada oferta en cuanto a gustos y temas se refiere. En esta ciudad se puede encontrar desde tiendas de diseñadores internacionales hasta tiendas artesanales locales. En la capital podrémos encontrar una gran variedad de personas de diferentes partes del país que por varios motivos han emigrado de sus ciudades natales, también una significativa cantidad de extranjeros provenientes de todos los lugares del mundo. Quito se ha convertido en una ciudad que acoge a miles de inmigrantes provenientes principalmente de Colombia, Cuba, Perú, recientemente Venezuela, europeos y norteamericanos. 

La urbe cuenta con varios lugares llenos de cultura donde se aglomeran ciudadanos de todo el país y del mundo. Entre las principales tenemos a la Mariscal, donde se asientan varios lugares de ocio y diversión que atienden de lunes a domingo. En esta zona se encuentran varios negocios de comida internacional, bares, discotecas, karaokes, restaurantes y un sin número de ofertas de encuentro y diversión nocturna. Otra zona muy concurrida por Quiteños y extranjeros es el centro histórico, ya que aquí se encuentran museos e iglesias de interés lo que lo convierte en un lugar muy transitado y un punto referencial del turismo y de la cultura Quiteña.

Existen algunos paseos urbanos en la ciudad. Los más importantes son el bulevar de la Avenida Amazonas, de la avenida Naciones Unidas y el área de la Mariscal. Existen ciertos proyectos planteados por el Municipio del Distrito Metropolitano de Quito a fin de regenerar la avenida Colón para darle el mismo estilo urbanístico de las ya mencionadas Amazonas y Naciones Unidas.

La ciudad actualmente presenta una concentración de empresas y oficinas de negocios en cinco ubicaciones principales: El Ejido, La Whymper, La Coruña, La Carolina y 12 de Octubre. Que vendrían a ser los cuatro centros financieros de la ciudad. Estos se encuentran todos en la zona norte.

La ciudad es sede de las más importantes compañías nacionales y de la casi totalidad de compañías multinacionales asentadas en el país; también es la sede de las oficinas centrales de las más importantes industrias que funcionan en el país. La actividad financiera y bancaria se concentra en el centro norte de la ciudad en los alrededores del parque de "La Carolina". Quito también es una ciudad muy agitada en el ámbito comercial, destacando grandes y modernos centros comerciales, malls, tiendas de textiles, artesanías y souvenirs, cadenas de grandes supermercados, ferreterías, farmacias, etc. La actividad económica es muy variada, aquí que es donde se concentra la mayor parte del accionar de la industria automotriz especialmente en el ensamblado para consumo nacional y exportación, la mayor actividad de construcción de todo el país, es la primera exportadora nacional de flores, madera, productos no tradicionales como el palmito y espárragos y varios más provenientes de sus valles y del mismo distrito; la actividad comercial es muy variada y la ciudad concentra la mayor cantidad de empresas dedicadas a esta actividad a nivel nacional. Es la segunda ciudad que más remesas recibe según estadísticas del Banco Central del Ecuador en el 2008. Y por último y lo más importante, la actividad turística que es la que más atrae a la ciudad y en la cual pretende concentrar y dedicar sus mayores esfuerzos. A partir de una gran inversión destinada a la regeneración urbana del Centro Histórico y otros lugares turísticos que empezó desde el 2001 por parte del Municipio del Distrito Metropolitano de Quito, el rubro turismo viene a ser una importantísima fuente de ingresos para la ciudad.

Quito, capital de la provincia de Pichincha y del Ecuador, es la ciudad que más aporta al PIB Nacional y la mayor por Renta per cápita. Quito es la de mayor grado de recaudación de impuestos en el Ecuador por concepto de gravámenes según el Servicio de Rentas Internas (S.R.I.), superando el 57% nacional al año 2009, siendo en la actualidad la región económica más importante del país, según el último "estudio" realizado por el Banco Central del Ecuador, en el año 2006, el aporte fue del 18,6% al PIB, generando 4106 millones de dólares, sin embargo su valor de adjudicación permite que este PIB sea aún mayor llegando a adquirir en términos reales el 27% del Pib país gracias a las aportaciones de la producción petrolera y predial. Actualizado: al 2009 el PIB de Quito fue de 57,650 millones de dólares aproximadamente por concepto de producción (19% de aportación), 4112 millones de dólares por concepto de adjudicación (8% de adjudicación) y 14762 millones de dólares por concepto total de PIB (27% procedente del 8% adjudicado, 19% producido).
Tabla: Datos económicos de la ciudad de Quito para el año 2009

La "Concentración Deportiva de Pichincha" es el organismo rector del deporte en toda la Provincia de Pichincha y por ende en Quito se ejerce su autoridad de control. En esta ciudad se alojan algunas de las instituciones deportivas más importantes del país. La ciudad está dotada de una red de completos polideportivos públicos que, sumados a los centros privados, facilitan la práctica del ejercicio físico, esto junto con el clima, hace de Quito una de las ciudades ecuatorianas con más practicantes de deporte. 

El principal escenario deportivo de la ciudad es el estadio Olímpico Atahualpa, sede de la selección macional masculina de fútbol, la selección femenina y varios clubes de fútbol quiteños. Otros escenarios deportivos destacados son: el estadio Rodrigo Paz Delgado (sede de Liga Deportiva Universitaria de Quito), el coliseo General Rumiñahui y el coliseo Julio César Hidalgo, que también son sedes de distintos deportes como el baloncesto, el voleibol y el fútsal, etc. Al tener una intraestructura deportiva diversa, de las más completas del país, ha sido sede de varias competiciones deportivas nacionales e internacionales, entre las que se destacan:


La competencia atlética de carácter popular más grande del país es la Quito Últimas Noticias 15K y se organiza en esta ciudad desde 1960. En su edición 2013 contó con un total de 22000 atletas inscritos.

El deporte más popular en la ciudad, al igual que en todo el país, es el fútbol, siendo el deporte con mayor convocatoria. La ciudad es sede de dos de los equipos más populares del país: Liga de Quito y el Club Deportivo El Nacional, ambos equipos participan en la Primera División del Fútbol Nacional Ecuatoriano.

Liga de Quito, conocido como el "Rey de Copas", es el equipo que posee más títulos internacionales, teniendo en su haber 12 campeonatos nacionales y 4 títulos internacionales, se ha consagrado campeón de la Copa Libertadores (edición de 2008), la Copa Sudamericana (edición de 2009), dos Recopa Sudamericana (edición de 2009, y edición de 2010), y en el 2008 fue subcampeón del mundial de clubes, desde el año 2008 es el equipo ecuatoriano mejor ubicado según el Ranking de la CONMEBOL. El Club Deportivo El Nacional es el tercer club con más títulos de campeón nacional, teniendo 13. Se caracteriza porque su plantilla de jugadores la componen únicamente futbolistas ecuatorianos. Ha sido tricampeón y lo ha hecho dos ocasiones, en 1976, 1977, 1978 y en 1982, 1983, 1984. El Independiente del Valle, originario del valle de Sangolquí, es otro de los equipos quiteños que se ha destacado en torneos continentales, siendo campeón de la Copa Sudamericana (edición 2019). Otros equipos tradicionales quiteños son Sociedad Deportiva Aucas, Club Deportivo América, y Sociedad Deportivo Quito, el primero tiene su sede en el Estadio Gonzalo Pozo Ripalda y los otros dos en el Estadio Olímpico Atahualpa.

Los equipos profesionales de fútbol de la primera división ecuatoriana que tienen a Quito como sede son:



__FORZAR_TDC__


</doc>
<doc id="3311" url="https://es.wikipedia.org/wiki?curid=3311" title="Ecuador">
Ecuador

Ecuador, oficialmente la República del Ecuador, es un país soberano ubicado en la región noroccidental de América del Sur, compuesto por veinticuatro provincias. Limita al norte con Colombia, al sur y al este con Perú y al oeste con el océano Pacífico, el cual lo separa de las islas Galápagos por miles de kilómetros entre la península de Santa Elena y la isla San Cristóbal. Por medio de su mar territorial correspondiente a las Islas Galápagos, también posee límites marítimos con Costa Rica. Una sección volcánica de la cordillera de los Andes divide el territorio de norte a sur, dejando a su flanco occidental el golfo de Guayaquil y una llanura boscosa, y al oriente, la Amazonia. El Ecuador ocupa un área de 256 370 km² . Es el de América, con algo más de diecisiete millones de habitantes, el más densamente poblado de América del Sur y el quinto más densamente poblado en toda América.

El Ecuador es una reciente potencia energética basada en energías ecosustentables. Además, se trata del país con una de las más altas concentraciones de ríos por km en el mundo, uno de los países de mayor diversidad por km por ende, uno de los países con mayor biodiversidad del mundo. Es el primer país del planeta en tener los Derechos de la Naturaleza garantizados en su Constitución.

La capital y ciudad más poblada del país es Quito. La lengua oficial es el español, hablado por un 99% de la población, junto a otras trece lenguas indígenas reconocidas, incluyendo kichwa y shuar. Para 2018, el IDH del Ecuador es catalogado como "alto", ubicándose en el puesto 81 a nivel mundial (junto a China) y decimosegundo a nivel de América. Con un PBI PPA de 205 457 millones de dólares, la economía ecuatoriana ocupa el puesto número 64 a nivel mundial y la sexta de Sudamérica. El país a nivel mundial es uno de los principales exportadores de petróleo, consta como el principal exportador de banano a nivel mundial y uno de los principales exportadores de flores, camarones y cacao. Ecuador recibió en 2019 poco más de , lo cual posiciona al país como uno de los referentes regionales en recepción de turismo internacional.

La primera referencia que se tiene de este país con relación a la línea ecuatorial está registrada en "Noticias secretas de América" en 1826, una publicación donde se recopila estudios realizados durante el transcurso del siglo XVIII, incluyendo los de la misión geodésica francesa; en esa obra se menciona por primera vez a "las tierras de Ecuador" como jurisdicción de la Real Audiencia de Quito. El nombre de la República de Ecuador hace alusión a la línea ecuatorial de la Tierra que pasa sobre territorio ecuatoriano, de este a oeste.

Los nombres fueron adoptados por el primer gobierno de los territorios que comprendían el distrito del Sur de la Gran Colombia, tras consumarse su separación el 13 de mayo de 1830. Antes de la Gran Colombia, durante el gobierno colonial, este territorio estaba conformado por los departamentos de Guayaquil, Azuay y el de Ecuador, que junto a otros territorios perdidos comprendían la Real Audiencia de Quito. Debido a la ubicación de la nueva república y como una referencia simbólica de esta ubicación, se decidió denominarla con el nombre de la línea que divide el planeta en dos partes iguales. Durante el primer gobierno comandado en ese momento por el general Juan José Flores en la primera Constitución de la nueva república, se creó oficialmente la «República de Ecuador», que, en ese entonces, estaba formada por los territorios correspondientes a los departamentos de Guayaquil, Azuay y Quito (antes Departamento de Ecuador, que más tarde serían subdivididos en provincias).

Los primeros asentamientos registrados en el actual territorio ecuatoriano datan 13 500 años, en El Inga, Cultura Las Vegas, Chobshi, Cubilán y pinturas rupestres amazónicas del Paleoindio. La época precolombina comprende cuatro períodos: Paleoindio, Formativo, de Desarrollo Regional y de Integración o Periodo Incaico.

Durante el periodo formativo se descubrió la cerámica, con la posibilidad de que la cultura Valdivia se una de las alfareras más antiguas de América. También se domesticaron especies vegetales, gracias a la diversidad biológica y climática de la región; entre ellas, cabe mencionar: piña, papaya, zapallo, maní, tomate, tomate de árbol, naranjilla, ají y cacao. La agricultura con un alto nivel de desarrollo en las zonas secas muestra restos de obras para recolección e infiltración de agua, y un sistema social conocidas como albarradas, que alteran el paisaje; las laderas de montañas en muchas regiones del país tienen restos de andenerías; mientras que en las zonas bajas y húmedas, en las vegas de ríos y orillas de lagos, se encuentran restos de camellones o grandes camas de cultivo con riego por inundación. Este último sistema es especialmente interesante por su dimensión en las cuencas de los ríos del litoral, como el río Guayas, con miles de hectáreas dedicadas al cultivo en camellones de gran tamaño.

La cultura Manteña, ubicada en la parte central del litoral ecuatoriano, controló una amplia ruta de comercio marítimo, que se extendió desde el actual Chile hasta México, basada en la navegación de cabotaje con grandes balsas impulsadas por velas. En el litoral norte, la cultura La Tolita produjo una metalurgia ornamental de alto nivel, principalmente en oro, plata y aleación de platino. La alfarería de las culturas Bahía y Jama-Coaque es recargada de detalles, y recuerda un tanto a la asiática, dando lugar a teorías de intercambio cultural transoceánico que no han podido probarse. Los pueblos de la sierra norte construyeron complejos funerarios y astronómicos como el de Cochasquí.

El territorio de Ecuador formó parte del Imperio Inca del Norte hasta la conquista española en 1533. A la llegada de los incas, se estima que habitaban en el territorio del actual Ecuador más de 36 nacionalidades, entre las cuales algunas de las más numerosas eran: Pastos, Caranquis, Imbayas, Paltas, Puruháes, Panzaleos, Cañaris, Hambatus. La influencia incaica se hizo sentir especialmente en el callejón interandino del sur y centro del país, que formaron parte del Tahuantinsuyo; la región norte se mantuvo parcialmente independiente hasta la llegada de los españoles, y tiene una de las mayores presencias de fortalezas o en el imperio Inca; mientras que las regiones de la costa y la Amazonía mantuvieron su independencia. Durante el imperio Icario, se construyeron algunos asentamientos con evidente influencia cuzqueña, siendo de los más importantes Ingapirca (aún se conserva buena parte de los restos arqueológicos) y Tumipampa (Tomebamba) (la ciudad de Cuenca fue fundada sobre la última aunque se conservan sus ruinas en algunos sectores).

En 1534, el capitán español Sebastián de Benalcázar conquistó las tierras ecuatorianas. Este, una vez tomada Quito, la refundó como ciudad española el 6 de diciembre de 1534, bautizándola como San Francisco de Quito en honor a Francisco Pizarro. Quito fue capital de la Presidencia de Quito y de la Real Audiencia de Quito, que formaba parte del Virreinato del Perú. Los españoles utilizaron los asentamientos urbanos indígenas y varios elementos de la estructura social autóctona como base de las nuevas ciudades mestizas, para colonizar los territorios que ocuparon.

Los indígenas los superaban en número, pero los españoles tenían una mayor fuerza militar, gracias a lo cual sometieron a las poblaciones indígenas, obligándoles a abandonar los valles templados de la Sierra y ubicarse en los páramos altos. Los incas, además de estar enfrentados entre sí en guerras internas, desconocían las armas de fuego. Se dice que muchos indígenas pensaron que los españoles que montaban sus caballos eran seres de cuatro patas y comparaban el sonido de los cañones con el de los truenos. Sin embargo, no pasó mucho tiempo hasta que los indígenas empezaran a defenderse, a pesar de su desventaja.

Quito fue el principal asiento español en la zona, y de ella partieron las expediciones que permitieron el descubrimiento del río Amazonas, y la fundación del resto de ciudades ecuatorianas. En 1739, Ecuador se integró en el Virreinato de Nueva Granada junto con Caracas, Panamá y Santa Fe de Bogotá. Las relaciones entre la población autóctona y los recién llegados se rigieron por instituciones jurídicas como la Mita y la Encomienda, esta última aprobada por las Leyes de Burgos en 1512 para la defensa de los indios. Enfermedades como el sarampión diezmaron la población indígena.

Esto hizo que para el trabajo forzado se trajera población africana negra, en calidad de esclavos, lo que contribuyó al mestizaje del Ecuador. Gran parte de la población negra en el país se encuentra en la actual Esmeraldas. Se dice que un barco de esclavos naufragó frente a las costas esmeraldeñas y una gran cantidad de esclavos quedaron ahí con dos españoles supervivientes que murieron al poco tiempo. Durante la época colonial se desarrollaron las artes, especialmente la arquitectura, pintura y escultura. En la Colonia se destaca la Escuela Quiteña, como un espacio de alta producción artística, famosa hasta la actualidad, por artistas como Miguel de Santiago, Caspicara y Bernardo de Legarda, entre otros.

Los primeros movimientos empezaron en 1809 con la rebelión de los Criollos contra el dominio español conocida como Primer Grito de Independencia Americana. Aunque hay otros precursores como Eugenio Espejo, sabio criollo de origen mestizo que lanzó las primeras proclamas por escrito en la publicación «"El Nuevo Luciano de Quito"». Los sublevados formaron una Junta de Gobierno provisional el 10 de agosto de 1809 en Quito, sublevando el poder Español y gobernándose autónomamente; sin embargo los participantes acabaron siendo encarcelados y asesinados en la Matanza del 2 de agosto de 1810. Este capítulo de la historia ecuatoriana fue tomado como ejemplo en el inicio de varios procesos libertarios en América Latina. En esa fecha los sublevados propugnaban el liderazgo de una junta autónoma de gobierno, cambiar las autoridades en Quito, manteniendo su autonomía utilizando la estrategia de las máscaras de Fernando VII; que consistía en jurar una falsa lealtad al cautivo rey Fernando VII con el fin de mantener la autonomía. La historiografía del Ecuador considera este suceso como el Primer Grito de Independencia Hispanoamericana y el inicio del proceso de emancipación de la región.Terminado el dominio francés y con la negativa del rey de España, Fernando VII, de acatar la Constitución de Cádiz, se desencadenaron una oleada de movimientos independentistas en la América Española.
Guayaquil inició su proceso independentista que tuvo lugar el 9 de octubre de 1820, con el propósito de romper los lazos coloniales que existían entre el territorio de la Provincia de Guayaquil y el Imperio español, y que dio paso al surgimiento de la Provincia Libre de Guayaquil. La independencia de Guayaquil marcó el comienzo de la guerra de independencia en la Real Audiencia de Quito como parte de las guerras emancipadoras de Hispanoamérica. Entre los factores más influyentes para su desencadenamiento se puede determinar la voluntad de los criollos, los cuales ya poseían un alto estatus social y económico, de obtener el poder político.

Es así como la antigua Presidencia y Audiencia de Quito consigue escindirse de la metrópoli en la batalla de Pichincha del 24 de mayo de 1822, gracias al triunfo del mariscal Antonio José de Sucre, lugarteniente de Simón Bolívar, con ayuda del Ejército Protector de Quito, formado por las tropas independentistas guayaquileñas ideadas por el poeta José Joaquín de Olmedo. El territorio de Guayaquil (que se había separado de España el 9 de octubre de 1820 y mantenía un gobierno propio) pasó a formar parte de la Gran Colombia bajo el nombre de Distrito del Sur junto a los territorios de Quito y Cuenca. El colapso de la nueva república dio lugar a la formación de los estados soberanos de Nueva Granada (actuales Colombia y Panamá), Venezuela y Ecuador en 1830. Cuando en 1822 el ejército independentista, comandado por Antonio José de Sucre, venció a las fuerzas realistas en la Batalla de Pichincha, los territorios formaron parte de la Gran Colombia, pero la gran rivalidad entre su presidente, Simón Bolívar, y su vicepresidente, Francisco de Paula Santander, ocasionó la disgregación de la Gran Colombia. Desde 1830, año del fin de la Gran Colombia, las naciones de: Ecuador, Colombia, Venezuela y Panamá mantuvieron su nexo político y económico ya que continuaron como estados federados durante cinco años más.

La República de Ecuador vio la luz el 13 de mayo de 1830 cuando se separó de la Gran Colombia. Juan José Flores fue quien tomó las riendas del nuevo estado convirtiéndose en su primer presidente. Flores inició la organización del país tomándole cinco años de mandato interrumpido de 1834 hasta 1839 (periodo durante el cual el país acumuló una cuantiosa deuda externa principalmente debido a la adquisición de material bélico); año en que Vicente Rocafuerte asumió la curul presidencial. Cuando el venezolano fue elegido para un tercer período presidencial, los grupos de poder de la costa iniciaron un levantamiento popular con el fin de abatir el militarismo extranjero el 6 de marzo de 1845, la llamada "revolución marcista".

El presidente al cargo fue José Joaquín de Olmedo, gran pensador guayaquileño que se mantuvo en el poder solo hasta que el legislativo llamó a elecciones y se nombró a Vicente Ramón Roca como tercer presidente constitucional del Ecuador. De 1845 a 1859 se vivió un período de gobiernos liberales hasta que una nueva revuelta llevó a nuevas elecciones en donde la figura de Gabriel García Moreno llegó a la política nacional en 1859, tras la re-unificación del país luego de la batalla de Guayaquil, hasta 1875, año en que es asesinado. Los gobiernos de García Moreno son criticados desde la esquina liberal anti-Católica como un régimen autoritario, represivo y dictatorial vinculado al clero católico. Gabriel García Moreno fomentó una política de construcción de obras públicas como carreteras, el ferrocarril, escuelas, colegios, universidades y hospitales. Todo este período fue de represión contra los liberales ecuatorianos. García Moreno, cuando se encontraba en su tercera presidencia, fue asesinado en el balcón del palacio de Carondelet en Quito por manos de un grupo de liberales radicales, en el que destaca Faustino Lemus Rayo. Según la versión conservadora su asesinato fue por intereses políticos de los liberales que querían llegar al poder, mientras que para los liberales se trató de un acto "patriótico". Además se conoce que su muerte fue una conspiración liderada por la Francmasonería en rechazo a su piedad Católica y protagonismo en la consagración del Ecuador al Sagrado Corazón de Jesús. Tras la muerte de Don Gabriel García Moreno, los dos partidos políticos del Ecuador trataron de unificar su pensamiento hacia lo que se denominaría el progresismo a una suerte de conservadurismo liberal. Antonio Borrero Cortázar fue el primer presidente fruto de esta unificación, pero apenas duró un año en el poder y sería seguido por Ignacio de Veintemilla y la posterior aparición del revolucionario progresista General Eloy Alfaro Delgado.

El momento más importante de la historia del progresismo sería cuando en el período del presidente Luis Cordero se realizó la conocida Venta de la Bandera lo que desencadenó en la revuelta militar que dio origen al liberalismo ecuatoriano con Eloy Alfaro como presidente constitucional. El periodo liberal dura desde el 5 de junio de 1895 hasta el 9 de julio de 1925 con los gobiernos del General Eloy Alfaro, General Leonidas Plaza Gutiérrez, Dr. Alfredo Baquerizo Moreno y Dr. José Luis Tamayo. Es de destacar el asesinato del General Eloy Alfaro y sus acompañantes el 28 de enero de 1912 cometido en la ciudad de Quito, después de ser sacados de la cárcel donde habían sido conducidos tras las derrotas en los combates de Huigra, Naranjito y Yaguachi. En la constitución de 1897 se estableció la libertad ante la ley, la libertad de pensamiento, la abolición de la pena de muerte para los delitos políticos y la garantía absoluta a la vida. Además se suprimió la participación de un eclesiástico en el consejo de Estado y se aceptó la libertad de cultos. 
En la constitución de 1906, se suprimió a la religión católica como religión oficial, se estableció la educación laica, se separó la Iglesia del Estado, se reconoció la libertad de conciencia en todas sus expresiones y se prohibió que los religiosos sean legisladores, además se aprobó el divorcio. Cabe destacar la cruel matanza del 15 de noviembre de 1922 cometida contra el pueblo guayaquileño que pedía pan durante el gobierno del Dr. José Luis Tamayo y que quedó impune, narrada magistralmente por el escritor Joaquín Gallegos Lara en su obra "Las cruces sobre el agua". Tras el golpe de estado del 9 de julio de 1925 hubo un Gobierno Plural Civil-Militar en el cual cada integrante de la junta gobernaba por una semana, gobernando así hasta marzo de 1926 cuando la junta cesó su actividad y se nombró como presidente a Isidro Ayora quien expidió una nueva constitución, la número trece. En su gobierno se creó el Banco Central del Ecuador, Banco de Fomento, Superintendencia de Bancos, Caja de Pensiones, Dirección Nacional de Aduanas, entre otras dependencias estatales; en 1930 se produjo una deflación general y Ayora se vio presionado a renunciar, dejándole el poder al ministro de Gobierno, Luis Larrea Alba, quien asumió el poder el 24 de agosto de 1931 y ante la negativa del congreso para adquirir poderes plenos, decidió disolverlo y el pueblo reaccionó frente a la dictadura y así él entregó el poder al presidente del Senado, Alfredo Baquerizo Moreno, quien convocó comicios presidenciales para octubre, y tras una serie de problemas de gobiernos, Velasco Ibarra se posesiona en 1934. Sería hacia 1941 justo y en curso de la segunda guerra mundial que el Perú invade la provincia de El Oro lo que desencadenaría la Guerra peruano-ecuatoriana durante el gobierno del liberal Dr. Carlos Arroyo del Río y que concluyó con el Protocolo de Río de Janeiro firmado el 29 de enero de 1942 cuando el sur del Ecuador se encontraba invadido. La gran figura de la política ecuatoriana desde mediados de los años 30 hasta inicios de los 70 fue el Dr. José María Velasco Ibarra quien fue Presidente del Ecuador en 5 ocasiones pero solo pudo culminar su tercer mandato. Fue Presidente en 1934-1935, 1944-1947, 1952-1956, 1960-1961 y 1968-1972.

Hacia comienzos de 1972, el Ecuador era un país sumido en el caos, con un presidente convertido en dictador civil, elecciones generales próximas a celebrarse y actores políticos cuyas futuras acciones eran impredecibles. Finalmente las fuerzas armadas decidieron intervenir, tomarse el poder e interrumpir el incipiente sistema constitucional en el que el país estaba inserto desde 1968. En febrero de 1972, hubo un golpe de estado que tomó por sorpresa a la opinión pública y a la comunidad internacional. El derrocamiento de Velasco Ibarra sucedió en Guayaquil y fue ejecutado materialmente, y sin que se disparara ni una sola bala, por un oficial de la Armada llamado Jorge Queirolo Gómez, pero llevó al general Guillermo Rodríguez Lara al poder, quien se proclamó «nacionalista» y «revolucionario», lo que devino en una serie de nacionalizaciones, las que pueden ser evaluadas más o menos críticamente pero que, para el momento en cuestión, resolvían los temas básicos del sistema productivo y social del Ecuador. Así, el gobierno creó en 1972 la Corporación Estatal Petrolera Ecuatoriana (CEPE, actualmente Petroecuador) y emprendió el camino hacia la adquisición, paso a paso, de las acciones mayoritarias del Consorcio Texaco - Gulf (Gulf Oil Corporation) - CEPE.

Ecuador da signos de querer adquirir autonomía nacional en el manejo del petróleo: en 1973 ingresa en el organismo más importante a nivel mundial de los países oferentes de petróleo, la Organización de Países Exportadores de Petróleo (OPEP). En 1974 adquiere el 25 % de las acciones del consorcio que operaba en Ecuador. En 1976 asciende al 62 %, hasta que finalmente, adquiere la totalidad de las acciones. Con este tránsito, el estado ecuatoriano pasa a ser el propietario del petróleo.

El gobierno de Rodríguez Lara también creó el Instituto Ecuatoriano de Electrificación (INECEL) (actualmente Corporación Eléctrica del Ecuador o CELEC) y un sistema para asegurar el aprovisionamiento de víveres básicos para los sectores populares: Empresa Nacional de Productos Vitales (Emprovit), que expedía esos productos a precios accesibles. También creó el Instituto Ecuatoriano de Telecomunicaciones (IETEL).

El 1 de septiembre de 1975, se produjo un intento de golpe de estado dirigido por el general Raúl González Alvear, que dejó un saldo de 22 muertos y más de 80 heridos y que produjo serios daños a la fachada del palacio de gobierno. Luego de estos hechos, el general González se fue exiliado a Chile y el general Rodríguez promulgó un decreto-mordaza para proteger el «prestigio» de las fuerzas armadas de los comentarios de los medios de comunicación. A pesar de todo esto, la situación del general Rodríguez se hizo insostenible y el Consejo de Generales de las Fuerzas Armadas le pidió su renuncia, acto que se concretó en enero de 1976. El gobierno quedó en manos de un triunvirato militar, presidido por el almirante Alfredo Poveda Burbano (Armada) e integrado por los generales Guillermo Durán Arcentales (Ejército) y Luis Leoro Franco (Fuerza Aérea). Durante este gobierno tuvo lugar la matanza del ingenio de Aztra en La Troncal el 18 de octubre de 1977 contra los trabajadores que habían iniciado una huelga, que quedó impune. Su ministro de gobierno, el entonces coronel Richelieu Levoyer, estructuró el Plan de Retorno a la Democracia que, en sus partes sustantivas, consistía en la formación de una nueva asamblea constituyente convocada por la junta militar, la que redactó una nueva constitución y organizó un referéndum que tuvo efecto en enero de 1978, con el que el pueblo ecuatoriano eligió por simple mayoría entre la constitución de 1945 reformada y la nueva. La última parte del plan de la junta militar fue convocar a elecciones generales, en las cuales participaron diecisiete partidos políticos aprobados por ese régimen.

Tras una segunda vuelta, que se realizó con mucha diferencia de tiempo de la primera, resultó elegido Jaime Roldós Aguilera, candidato del partido populista Concentración de Fuerzas Populares (CFP). Jaime Roldós gobernó de manera independiente y en abierta pugna con Assad Bucaram, que durante el primer año de su mandato ostentó el cargo de presidente del congreso. Roldós tuvo que afrontar otro conflicto fronterizo con Perú, la cual se denominó el Conflicto del Falso Paquisha en 1981, que amenazaba con convertirse en una guerra abierta que, al final, no aconteció. Al estrellarse en misteriosas circunstancias el avión en que viajaba el 24 de mayo de 1981 (todavía se investiga el accidente), el poder pasó al vicepresidente constitucional en funciones el Dr. Osvaldo Hurtado Larrea, de tendencia Social-demócrata, al que sucedió en 1984 el socialcristiano conservador León Febres-Cordero. Sus medidas de austeridad por problemas con el petróleo y sus políticas represivas, que aunque eliminaron incipientes grupos guerrilleros como Alfaro Vive Carajo, provocaron un descontento social, que dio la victoria en 1988 al socialdemócrata Rodrigo Borja Cevallos, en cuyo mandato tuvo lugar un movimiento indígena que logró la distribución de 1 700 000 hectáreas a las comunidades autóctonas. Borja también impulsó la alfabetización y la educación bilingüe.El conservador Sixto Durán Ballén propició desde 1992 una política neoliberal con privatizaciones y ajustes cuestionados por la mayoría del Congreso, y provocó el abandono de la OPEP, mientras el país aumentaba la producción petrolera. Otro conflicto con Perú conocido a nivel mundial como la Guerra del Cenepa que terminó ese mismo año en 1995 con el Acuerdo de Itamaraty y, en 1998, bajo la Presidencia del democristiano Jamil Mahuad, con la firma definitiva de la paz en Brasilia que le dio a Ecuador acceso al Amazonas y derechos de libre navegación fluvial. Asimismo el documento reconocía la soberanía peruana en Tiwinza, concediéndose al Ecuador 1 km² como propiedad privada bajo la legislación del Perú, a todo aquel que nazca en Tiwinza se le considerará peruano.

De acuerdo con este protocolo, Ecuador renunció a sus pretensiones históricas de anexar Tumbes, Jaén y Maynas; y los reconoció como territorios peruanos. Quedó así zanjada la disputa limítrofe que desde 1960 había sido enunciada por José María Velasco Ibarra. Este acuerdo tuvo provisiones para la colocación definitiva de los hitos fronterizos en cooperación con la misión de observadores de la OEA (MOMEP).

La normalidad institucional se vio resquebrajada en 1997 cuando el Congreso, en medio de manifestaciones populares en contra del Ejecutivo, destituyó por «incapacidad mental» al presidente populista Abdalá Bucaram, quien se había posesionado en agosto de 1996. En su reemplazo, el Congreso designó como Presidente Interino a Fabián Alarcón, hasta ese momento Presidente del Congreso Nacional (pese a que constitucionalmente le correspondía asumir la presidencia a la vicepresidente Rosalía Arteaga, quien se posesionó simbólicamente por unas horas). Tras una Asamblea Nacional Constituyente en 1998, la cual tuvo el mandato de revisar y modificar la Constitución de 1979, se realizaron elecciones generales en las que fue elegido presidente Jamil Mahuad Witt, de Democracia Popular. Mahuad fue depuesto en enero de 2000, en medio de una grave crisis económica ocasionada por la quiebra masiva del sistema financiero ecuatoriano, la caída de los precios internacionales del petróleo y la vinculación del gobierno de Mahuad con la banca corrupta cuya cabeza más visible fue Fernando Aspiazu, quien el 26 de agosto de 2002 fue condenado a ocho años de prisión por el delito de peculado. Todo ello provocó una huelga general, movilizaciones indígenas y un intento de golpe de estado que duró cuatro horas. Como resultado de la crisis económica durante este gobierno más de dos millones de ecuatorianos tuvieron que migrar hacia otros países teniendo como resultado la separación de innumerables familias.

El vicepresidente Gustavo Noboa, a quien correspondía la sucesión conforme a la Constitución, asumió la Presidencia y estableció en abril un acuerdo con el FMI (Fondo Monetario Internacional) para acceder a créditos por un valor cercano a los 800 millones de dólares para continuar y fortalecer la dolarización, aplicando medidas de ajuste en diversos sectores de la economía. Además, centró sus esfuerzos en la construcción de un gran oleoducto de crudos pesados (OCP) desde la Amazonia hasta la costa del Océano Pacífico, para que la exportación de crudo se duplique a partir de 2003. El coronel retirado Lucio Gutiérrez ganó las elecciones de noviembre de 2002 al frente del Partido Sociedad Patriótica (PSP), una agrupación populista de centro-izquierda, que actuó en alianza con movimientos indígenas y de extrema izquierda. Gutiérrez obtuvo el 55 % de los votos en la segunda vuelta electoral. Fue destituido por el Congreso en abril de 2005, en medio de revueltas en Quito (a cuyos participantes Gutiérrez denostó como «forajidos», en la llamada «rebelión de los forajidos»), sucediéndose en el cargo el vicepresidente Alfredo Palacio, quien hasta entonces tenía poca figuración en el plano político.
En noviembre de 2006, Rafael Correa fue elegido presidente para el período 2007-2011. El margen electoral fue el tercero más alto en el actual período constitucional y democrático (1979-2007), superado únicamente por las elecciones de Jaime Roldós (1979) y Sixto Durán Ballén (1992). El 15 de abril de 2007 se eligió a la Asamblea Constituyente, la que redactó una nueva Carta Magna, vigente desde octubre de 2008. Debido a la vigencia de una nueva Constitución, se tuvo que llamar a elecciones generales para designar a las autoridades, siendo así como el presidente Correa en 2009 fue reelegido en su cargo. El 30 de septiembre de 2010 se realizó una paralización de actividades por una parte de varios miembros Policía Nacional del Ecuador y la Fuerza Aérea Ecuatoriana. La crisis fue declarada por el gobierno como intento de golpe de Estado y fue superada al final del mismo día, con la salida del presidente Correa del Hospital de la Policía Nacional rescatado por el Ejército de Ecuador y el GOE (Grupo de Operaciones Especiales de la Policía), frustrando las intenciones de los amotinados. El incidente resultó con un saldo de 5 muertos y 274 heridos. La elección presidencial del 17 de febrero de 2013 dio como resultado la reelección de Rafael Correa con su binomio Jorge Glas con el 57,17 % de votos válidos, frente a Guillermo Lasso con el 22,66 % de votos válidos, Los demás candidatos no obtuvieron una votación que supere al 7 % en ningún caso. El binomio se posesionó el 24 de mayo de 2013.

El 16 de abril de 2016, se sucitó un devastador terremoto, con epicentro en el cantón Pedernales, Provincia de Manabí, con una magnitud de 7,8 M. Según la Oficina de la ONU para la Coordinación de Asuntos Humanitarios, más de un millón de personas fueron afectadas por el terremoto.

El 19 de febrero de 2017 se llevó a cabo la última elección presidencial, los candidatos Lenin Moreno con 39.36 % y Guillermo Lasso con 28.09 % pasaron a balotaje. El balotaje se realizó el 2 de abril y Moreno resultó presidente electo ganando la contienda con el 51.16 % de los votos contra Lasso, candidato de la alianza del Movimiento CREO y el Movimiento SUMA, quien obtuvo el 48.84 %. El presidente electo tomó posesión de sus funciones el 24 de mayo de 2017.

El 2 de octubre de 2019 iniciaron una ola de protestas, tras la adopción de nuevas medidas económicas por parte del gobierno de Lenín Moreno las cuales fueron dictadas por el Fondo Monetario Internacional. La situación se fue tornando más crítica con el pasar de los días, por lo que el gobierno decretó el estado de excepción, e incluso llegando a ordenar el 8 de octubre un toque de queda y el traslado de la sede de gobierno a Guayaquil. Los principales enfrentamientos se dieron entre la Confederación de Nacionalidades Indígenas (CONAIE) y la policía, la cual llegó a cometer crímenes de lesa humanidad lo que ocasionó al menos 11 fallecidos, 1340 heridos y 1192 detenidos, ocasionando una grave conmoción social; no obstante, el 13 de octubre se llevó a cabo un foro mediado por el representante de la ONU en Ecuador y la Iglesia Católica, donde los dirigentes de la CONAIE y el partido de gobierno llegaron a un acuerdo que finalizó con el conflicto.
El lunes 27 de enero del 2020 surgió la primera sospecha del ingreso del COVID-19 al país por parte de un ciudadano procedente de España, y el 29 de febrero se confirmó el primer paciente detectado mediante pruebas en el país. Tras el aumento de los casos, el 12 de marzo el Gobierno Nacional decidió aplicar una serie de medidas contra la pandemia. El epicentro de los contagios fue en Guayas, más específicamente en Guayaquil, donde se estima que hubo alrededor de 15 000 contagios.
Durante la pandemia de enfermedad por coronavirus de 2020 en Ecuador surgieron varias denuncias por corrupción dentro de la emergencia sanitaria. Los hechos han provocado la salida de miembros del gobierno de Lenín Moreno y la investigación o detención contra autoridades cantonales y provinciales, un expresidente de la República, sus hijos y su círculo cercano.

El actual Estado ecuatoriano está conformado por cinco funciones estatales: Función Ejecutiva, Función Legislativa, Función Judicial, Función Electoral y Función de Transparencia y Control Social.

La Función Legislativa se ejerce por la Asamblea Nacional únicamente, que tiene su sede en la ciudad de Quito en el "Palacio Legislativo", y está conformada por 137 asambleístas, repartidos en diez comisiones, elegidos para un período de cuatro años. Quince asambleístas elegidos en circunscripción nacional, dos asambleístas elegidos por cada provincia, y uno más por cada doscientos mil habitantes o fracción que supere los ciento cincuenta mil, de acuerdo al último censo nacional de la población. Ajeno a lo anterior, la ley determinará la elección de asambleístas de regiones, de distritos metropolitanos, y además de la circunscripción del exterior.

La Función Ejecutiva está delegada al Presidente de la República, actualmente ejercida por Lenín Moreno, acompañado de su vicepresidenta, María Alejandra Muñoz. El presidente es el Jefe de Estado y de Gobierno, máximo responsable de la administración pública.

El Presidente nombra a Secretarios Nacionales, Ministros coordinadores, Ministros de Estado y Servidores públicos. Define la política exterior, designa al Canciller de la República, así como también embajadores y cónsules. Ejerce la máxima autoridad sobre las Fuerzas Armadas de Ecuador y la Policía Nacional de Ecuador, nombrando a sus autoridades, Según la Constitución de la república. Son atribuciones y deberes de la Presidenta o Presidente de la República, además de los que determine la ley: Ejercer la máxima autoridad de las Fuerzas Armadas y de la Policía Nacional y designar a los integrantes del alto mando militar y policial.

La Función Judicial del País está conformada por el Consejo de la Judicatura como su ente principal y por Corte Nacional de Justicia, las Cortes Provinciales, los juzgados y tribunales, y los juzgados de paz. La representación jurídica la hace el Consejo de la Judicatura, sin perjuicio de la representación institucional que tiene la Corte Nacional de Justicia.

La Corte Nacional de Justicia está integrada por 21 jueces elegidos para un término de nueve años. Serán renovados por tercios cada tres años, conforme lo estipulado en el Código Orgánico de la Función Judicial. Estos son elegidos por el Consejo de la Judicatura conforme a un procedimiento de oposición y méritos. No son susceptibles de reelección.

Como organismos independientes de la Función Judicial están la Fiscalía General del Estado y la Defensoría Pública. Como organismos auxiliares están: el servicio notarial, los martilladores judiciales y los depositarios judiciales. Igualmente hay un régimen especial de justicia indígena.

La Constitución de 2008 elevó al rango de función del Estado a la institucionalidad electoral, cuyo mandato es garantizar el ejercicio de los derechos políticos de la ciudadanía y promover el fortalecimiento de la democracia, mediante la organización de procesos electorales y el apoyo a las organizaciones políticas y sociales; asegurando una participación equitativa, igualitaria, paritaria, intercultural, libre, democrática y justa para elegir y ser elegidos.

Constitucionalmente, la Función Electoral está conformada por dos órganos separados, bajo principios de autonomía e independencia: el Consejo Nacional Electoral, que administra y ejecuta los procesos electorales; y, el Tribunal Contencioso Electoral, encargado de la administración de la justicia electoral.

La Función Electoral, tiene su sede en la ciudad de Quito, el Consejo Nacional Electoral está conformado por 5 consejeros y el Tribunal Contencioso Electoral por 5 jueces, seleccionados en ambos casos a través de concurso público de oposición y méritos, ejerciendo sus funciones por 6 años con renovaciones parciales cada 3 años.

La Función de Transparencia y Control Social está conformada por: Consejo de Participación Ciudadana y Control Social, Defensoría del Pueblo, Contraloría General del Estado, y las Superintendencias. Sus autoridades ejercerán sus puestos durante cinco años. Este poder se encarga de promover planes de transparencia y control público, así como también planes para diseñar mecanismos para combatir la corrupción, como también designar a ciertas autoridades del país, y ser el mecanismo regulador de rendición de cuentas del país.

El territorio de Ecuador se divide en: Parroquias (urbanas o rurales), las cuales conforman los Cantones, estos las Provincias, y estas a su vez las Regiones Administrativas. Cada una de estas entidades y los Distritos Metropolitanos tienen un Gobierno Autónomo Descentralizado, encargado de ejecutar políticas dentro de su ámbito.

Las parroquias son las divisiones de cuarto nivel en Ecuador, siendo más de un millar en total. Son entidades similares a los municipios o comunas en otros países, diferenciadas a su vez en urbanas y rurales. Las parroquias están en manos de un Gobierno o Junta Parroquial de cinco vocales elegidos por sufragio universal, que es presidida por el vocal que alcanza la votación más alta, llamado Presidente de la Junta Parroquial.

Los cantones son las unidades territoriales de tercer nivel en Ecuador, siendo 221 en total. Al frente de estos existe un Gobierno Municipal, compuesto por un Alcalde y un Concejo integrado por concejales urbanos y rurales, electos todos por sufragio universal.

La República de Ecuador se divide en 24 provincias que son las unidades territoriales de segundo nivel. Las provincias eligen un Prefecto y Viceprefecto Provincial, quienes ejercen el gobierno local junto con un Gobierno Provincial integrado por todos los alcaldes de los cantones que componen la provincia.

La SENPLADES (Secretaría Nacional de Planificación y Desarrollo del Ecuador) conformó distintos niveles administrativos de planificación: zonas, distritos y circuitos a nivel nacional; que permitirán una mejor identificación de necesidades y soluciones efectivas para la prestación de servicios públicos en el territorio. Esta conformación no implica eliminar las provincias, cantones o parroquias.

Las zonas están conformadas por provincias, de acuerdo a una proximidad geográfica, cultural y económica. Hay siete zonas de planificación, dos distritos metropolitanos y el Régimen especial de Galápagos. Cada zona está constituida por distritos y estos a su vez por circuitos. Desde este nivel se coordina estratégicamente las entidades del sector público, a través de la gestión de la planificación para el diseño de políticas en el área de su jurisdicción.

El distrito es la unidad básica de planificación y prestación de servicios públicos. Coincide con el cantón o unión de cantones. Se han conformado 140 distritos en el país. Cada distrito tiene un promedio de 90 000 habitantes. Sin embargo, para cantones cuya población es muy alta como Quito, Guayaquil, Cuenca, Ambato y Santo Domingo se establecen distritos dentro de ellos.

El circuito es la localidad donde el conjunto de servicios públicos de calidad están al alcance de la ciudadanía, está conformada por la presencia de varios establecimientos en un territorio dentro de un distrito. Corresponde a una parroquia o conjunto de parroquias, existen 1134 circuitos con un promedio de 11 000 habitantes.

Las zonas, distritos y circuitos son niveles desconcentrados para la administración y planificación de los servicios públicos de algunos ministerios de la Función Ejecutiva. Fueron conformados respetando la división política administrativa, es decir corresponde a una nueva forma de planificación en el territorio más no a nuevos niveles de gobierno. Por lo tanto, los niveles de gobierno conservan autonomía y gobernabilidad a nivel de las provincias, cantones y parroquias.

Los niveles de planificación buscan contar con una oferta ideal de servicios en el territorio sustentado en un Estado planificado, desconcentrado, articulado, equitativo, con mayor cobertura y calidad de servicios públicos., compuestas por dos o más provincias contiguas, con el fin de descentralizar las funciones administrativas de la capital, Quito.
A la misma vez que por constitución se intenta llevar a las mismas a un sistema de autonomías mediante la elección por sufragio universal de Gobernadores Regionales y un cuerpo de Consejeros, con el objeto de atender políticas de desarrollo complementario entre provincias, enfocado a áreas turísticas, de inversión, comercio, etc. En Ecuador existen siete zonas, conformadas cada una por las siguientes provincias:

Además:
8: Distrito Metropolitano de Guayaquil: 
9: Distrito Metropolitano de Quito: 
10: Régimen Especial de Galápagos

La defensa del país está a cargo de las Fuerzas Armadas del Ecuador que son parte de la fuerza pública y responsable de la integridad y la soberanía del territorio nacional cuentan con un número de 167 910 efectivos activos y 185 000 en reservas dentro de las Fuerzas Terrestres, la Armada y a la Fuerza Aérea. Poseen armamento comprado a Reino Unido, Chile, Francia, Estados Unidos, Sudáfrica y Brasil.

Se trata de la participación en el sector social y el desarrollo económico del país y la prestación de asistencia en el mantenimiento del orden interno. Sus tareas incluyen la lucha contra el crimen organizado, tráfico de drogas, y operaciones de lucha contra la migración ilegal. Aplica programas de desarrollo social como la disposición de los profesores para las escuelas rurales a través de un acuerdo con el Ministerio de Educación.

La protección ambiental es también una prioridad, se implementaron varios programas como: Nacional de Forestación y Ornato, Lonely Tree, Vigilancia Verde, Fire Plan Ecuador Forestal y Reserva Militar Arenillas. Las Fuerzas Armadas son una parte esencial de la infraestructura de los países y por lo tanto muy apreciada por la sociedad. El territorio ecuatoriano está dividido en cinco regiones militares denominadas «Fuerzas de Tarea Conjunta», cuatro en el territorio continental, y el quinto es la Base Naval de la zona insular (incluida las Islas Galápagos). Los Territorios de ultramar incluyen también la Base Pedro Vicente Maldonado en la Antártica.

Además el Ecuador es el segundo país que más gasta en defensa en relación a su PIB, de entre los países de Sudamérica que destinó el 2.38 % de su PIB a las Fuerzas Armadas en el año 2018. El exministro de Defensa Fernando Cordero, en 2012, justificó esto diciendo que el gasto se dirige al mantenimiento del personal.

Ecuador se encuentra sobre la línea ecuatorial terrestre por lo cual su territorio se encuentra en ambos hemisferios. Comprende dos espacios distantes entre sí: el territorio continental al noroeste de América del Sur con algunas islas adyacentes a la costa y, el archipiélago o provincia insular de Galápagos, que se encuentra a 1000 kilómetros de distancia del litoral ecuatoriano en el Océano Pacífico.

Las principales unidades del relieve ecuatoriano son la llanura costera al norte del Golfo de Guayaquil, la sección de la Cordillera de los Andes en el centro del país y un extenso sector de la llanura amazónica ubicado al oriente del país.

Hacia el suroeste se ubica el golfo de Guayaquil, donde desemboca el río Guayas en el Océano Pacífico. Muy cerca de Quito, la capital, sobre la Cordillera de los Andes, se alza el Cotopaxi, uno de los volcanes activos más altos del mundo.

El punto más alto del Ecuador es el volcán Chimborazo, con 6 315 msnm y cuya cima es el lugar más lejano al núcleo de la tierra debido a la silueta elíptica del planeta. En la zona estatal de Ecuador hay 22 de más de 3500 metros de altitud que están todas en los Andes. Se extienden sobre las tres cadenas montañosas Cordillera Central, Cordillera Occidental e Interandino y 9 de ellas son volcanes activos o potencialmente activos.

Ecuador es el país con más ríos por metro cuadrado del mundo. La cordillera andina es el divortium aquarum entre la cuenca hidrográfica del río Amazonas, que discurre hacia el este, y del Pacífico, que incluye de norte a sur los ríos: Mataje, Santiago, Esmeraldas, Chone, Guayas, Jubones y Puyango-Tumbes.

El país posee una variedad climática amplia, pues su ubicación geográfica (zona ecuatorial), su orografía (la presencia de los Andes), la influencia de la selva amazónica, y del océano Pacífico le confieren muchas modificaciones y pisos altitudinales con características propias. Según la clasificación climática de Köppen en Ecuador se han identificado once tipos de climas ostensiblemente diferentes.

La región litoral del país posee zonas climáticas amplias, favorecidas por sus llanuras costeras, y sus cordilleras pre montanas, aquí es muy claro el patrón, el norte es muy húmedo, y mientras se desciende se torna cada vez más seco; así, se identifican cinco climas claros (todos cálidos). La región interandina posee zonas climáticas más bien esporádicas, irregulares y muy diversas, donde son los Andes su principal influencia, aquí es más difuso hablar de una zonificación climática, aunque por lo general mientras más al norte y al este más húmedo, y mientras más al sur y al oeste más seco, con salvas excepciones, aquí se identifican siete climas (dos cálidos, tres templados y dos fríos). La región oriental del país posee una consistencia mucho más basta que la costa, con un predominio zonal extenso de sus climas, todos húmedos, así se identifican tres (dos cálidos y uno templado). Por último, la región insular, es la más proclive a desarrollar microclimas, y es mucho más irregular y diversa que la Sierra, pues en su reducida extensión se identifican cinco climas esparcidos por sus islas y sus altitudes, en general mientras más baja es esta, el clima es más seco, y mientras se asciende se torna más húmedo.











A causa de su ubicación ecuatorial, cada zona climática presenta solo dos estaciones definidas: la húmeda y la seca, llamadas erróneamente «invierno» y «verano» respectivamente, al igual que ocurre en otras regiones del globo donde por sus emplazamientos próximos a la línea ecuatorial, no ocurren verdaderos inviernos y veranos.

Estas estaciones húmedas y secas causan en cada región del país diferentes estaciones climáticas. Son muy variables las temperaturas por la altura de la sierra, la región amazónica, la costa del país y la región insular.

Así, de enero a mayo es principalmente estación húmeda, con la mayor temporada de playa en toda la región litoral o costa ecuatoriana. En esos mismos meses en la sierra también es temporada húmeda, con la mayoría de días nublados y frescos.

Del modo contrario, de junio a diciembre en la región costa o litoral, es temporada seca, si bien algunas playas de clima más moderado siguen siendo disfrutadas (mayormente en la provincia de Esmeraldas) por los turistas, otras son algo más frescas (como Salinas) en comparación con otras épocas del año, y también reciben turistas de la sierra y países vecinos. En la sierra, en esos mismos meses el país tiene una estación seca, con días cálidos y soleados, y noches frías, más aún cuando el cielo está despejado.

La capital del país, Quito posee temperaturas primaverales casi todo el año, aunque durante la estación seca, las temperaturas pueden bajar incluso hasta los 3°C por la noche, y pueden alcanzar los 26°C durante el día. Cabe destacar los altos niveles de radiación que se presentan en días soleados en la Sierra, por lo que es necesario utilizar protección para los rayos UV. Un extremo opuesto es la populosa ciudad de Guayaquil donde las temperaturas suelen incluso llegar a los 40°C durante los primeros meses del año, dependiendo de cómo se presenten las lluvias.

Debido al calentamiento global fenómenos poco usuales en estas latitudes se han presentado con mayor frecuencia, por ejemplo han existido nevadas recurrentes en Papallacta, provincia del Napo ubicada a cerca de 3.000 m.s.n.m.; Aunque son algo usuales las olas de frío en la Sierra. Durante los últimos años se han exprimentado temperaturas bajo cero en ciudades como Quito, Ambato, Cuenca, Riobamba y Latacunga.

Ecuador es uno de los países mejor preservados ambientalmente en el mundo, además su situación geográfica y latitudinal, así como su condición climática y orográfica moldearon al Ecuador como el país con más especies animales de la región, y uno de los 10 países con mayor endemismo a nivel mundial. Para comprender su fauna debemos recalcar que Ecuador alberga cuatro regiones que marcan mucho la faunística, así por ejemplo la región amazónica concentra especies únicas de anfibios, mamíferos como monos capuchinos, mono aulladores, monos araña, monos barbudos, pantera negra, oso de andino, guantas, jaguares, tigrillos, ocelotes, pumas, capibaras, tapir, y otros; así como una infinidad de reptiles como caimanes, cocodrilos, boas, anacondas, serpientes venenosas, entre otras, así como aves tales como el tucán, ibis, pericos, guacamayos, aves canoras, águila, cóndor.

Ecuador posee una rica fauna y flora por lo que se encuentra dentro de la lista de países megadiversos. En efecto, el bioma de selva o bosque tropical se extiende por la mayor parte de su territorio, mientras que en el occidente, adyacente a la costa, se encuentra también el bioma del bosque seco y de los manglares. La fauna del Ecuador es muy extensa con una gran variación de especies e innumerables tipos de especies tropicales como los guacamayos, tucanes, tortugas, ranas, etc. Cabe destacar que la Planta Nacional de Ecuador es la cascarilla o quina, decretada como tal bajo la presidencia de Federico Páez, en 1936.

Al norte de la provincia de Esmeraldas en un lugar conocido como Majagual, se encuentran los manglares más altos del mundo.

En las alturas cordilleranas, se hallan dispersos además los bosques y los páramos andinos. El occidente forma parte del Chocó biogeográfico y el Oriente, de la Amazonia.

Las islas Galápagos poseen una gran variedad de especies endémicas, las cuales en su momento fueron estudiadas por el célebre naturalista inglés Charles Darwin, lo cual le permitió desarrollar su teoría de la evolución por selección natural. Las islas han ganado fama a nivel mundial debido a la particularidad de su fauna, especialmente de las tortugas conocidas como «Galápagos».

En 1986 el mar que rodea a las islas fue declarado reserva marina. Unesco incluyó a Galápagos en la lista de Patrimonio de la Humanidad en 1978, y en diciembre de 2001 se amplió esta declaración para la reserva marina.

Está además, el parque nacional Yasuní; el término Yasuní, sin conocer su origen lingüístico, significa «tierra sagrada» como es interpretado de manera general por comunidades de la zona; el parque se extiende sobre un área de 9820 kilómetros cuadrados en las provincias de Pastaza y Orellana entre el río Napo y el río Curaray en plena cuenca amazónica a unos 250 kilómetros al sureste de Quito.

El parque, fundamentalmente selvático, fue designado por la Unesco en 1989 como una reserva de la biosfera y es parte del territorio donde se encuentra ubicado el pueblo Huaorani y los tagaeri y taromenane, grupos no contactados.

Según un reciente estudio, el parque nacional Yasuní y la zona ampliada subyacente se considera la zona más biodiversa del planeta por su riqueza en anfibios, aves, mamíferos y plantas. Este parque cuenta con más especies de animales por hectárea que toda Europa junta.

El Chimborazo es el volcán y montaña más alta de Ecuador y el punto más alejado del centro de la Tierra, es decir el punto más cercano al espacio exterior, razón por la cual es llamado como «el punto más cercano al Sol», debido a que el diámetro terrestre en la latitud ecuatorial es mayor que en la latitud del Everest (aproximadamente 28º al norte). Su última erupción conocida se cree que se produjo alrededor del 550 dC. Está situado en los Andes centrales, 150 km al sudoeste de Quito y 20 km al noreste de Riobamba.

Los datos generados por el INEC informan que para agosto de (2019) habitan 17 304 827 personas en Ecuador (lo que lo convierte en el , por detrás de Siria, y por encima de los Países Bajos), de los cuales el 50,04% (8 659 335 personas) son mujeres, y el 49,96% (8 645 492 personas) son varones, lo que quiere decir un índice de masculinidad del 100,16%. Siendo la densidad poblacional de 61,03 hab/km², con lo que lo convierte en el país más densamente poblado de Sudamérica, similar a la de Irlanda, Croacia o México, estando distribuida de forma un tanto irregular, pues de sus cuatro regiones, las más pobladas, la Costa con 8 974 147 habitantes y la Sierra con 7 283 031 habitantes se concentran en menos de la mitad del territorio, alcanzando una densidad poblacional de 127,03 hab/km² y 121,77 hab/km², respectivamente, similar a la densidad de Jordania, Francia y Polonia. En contraposición el Oriente (región amazónica) que constituye más del 50% de la superficie alberga apenas a 937 406 habitantes, es decir, le corresponde una densidad poblacional de 7,81 hab/km², similar a la de Rusia o Canadá, finalmente las islas Galápagos poseen 32 320 habitantes. Con esto se puede constatar que hay relativamente la misma cantidad de personas viviendo cerca a la costa que al interior.

La esperanza de vida en Ecuador es de 76.55 años, siendo de 73,87 años para los varones y de 79,32 años para las mujeres, en 2017, lo que lo ubica como el 49° país del mundo, justo por detrás de México, y por encima de China, Turquía y Brasil. Ecuador fue el cuarto país con más longevos de América Latina tras Chile, Costa Rica y Canadá, y por delante de Estados Unidos y Argentina, siendo especial lo ocurrido en el Valle de Vilcabamba ubicado en la provincia de Loja a tan solo 30 km de la ciudad de Loja, donde la mayor parte de sus habitantes sobrepasa los 100 años.

La tasa de crecimiento de Ecuador es del 2,04% anual, viéndose mayormente estimulada por la migración, en especial la de refugiados, y en segunda instancia por la fertilidad vernácula del país, la misma que es de 2,15 hijos por mujer; siendo mayor en provincias del Oriente, como Morona Santiago donde llega a los 4,7 hijos por mujer, y siendo menor en provincias de la Sierra, con Pichincha la que menos, que llega a los 2,2 hijos por mujer. 

El 63 % de la población reside en zonas urbanas y el 37 %, en rurales. Cabe destacar que, dada la alta densidad poblacional y su extensión territorial reducida, la concentración de poblados es alta, por lo que las ciudades y poblados rurales se encuentran muy cerca unos de otros (en especial en la Sierra). La población rural ecuatoriana es la tercera población rural con mejor calidad de vida de Latinoamérica tras Uruguay y Cuba, se evidencia que la población ecuatoriana esta mayormente radicada en la zonas urbanas del Ecuador, el 44 % de la población urbana está radicada en las 15 ciudades más grandes del país de las cuales Guayaquil y Quito bordean el 60 % de la población urbana, eso resulta por factores como la expansión urbana a centros poblados rurales y el mejoramiento de estándares de vida en el sector rural en la última década, donde se ha dotado de infraestructura de calidad en centros de salud, educación, vialidad, mejoramiento de producción agro industrial, servicios básicos, bajo costo de vida, etc. La migración de población rural a las zonas urbanas es un fenómeno retroactivo de Ecuador a la tendencia sufrida en el resto de América Latina.

Ecuador posee un comportamiento demográfico que va en tono a lo que ocurre con la región, donde es de esperar un proceso lento, pero constante rumbo al envejecimiento poblacional. Así, si bien Ecuador aún posee una edad promedio relativamente joven, esta va cada año aumentando, sin síntoma alguno de haber algún cambio esperable. Para el INEC, el año 2030 será un punto clave en la transición demográfica del país, pues ese año Ecuador habrá conseguido un bono demográfico, es decir, que si los cálculos se aproximan, de los casi 20 millones de ecuatorianos que habrán para ese entonces, la mayoría total se encontrarán en el rango etario adulto (mayores de 18 años), pero no solo eso, para ese año la tasa de fertilidad se ubicará bordeando los 2,00 hijos por mujer (similar a la actual tasa de fertilidad de Francia o Países Bajos), lo que significará que sin variables contextuales imprevistas, Ecuador empezará un proceso real hacia el envejecimiento al no poder garantizar el reemplazo generacional, y estancará su crecimiento poblacional muy cerca del 1% anual. 

Con todo esto, las proyecciones observables para 2050, hablan de casi 23,4 millones de ecuatorianos, de los cuales más del 70% de la población estará por encima de los 18 años, y más aún, pues casi 1,4 millones estarán en senecitud, en contraste con los 1,3 millones de niños que se estiman. Se verá un claro ejemplo de eficiencia reproductiva, pues la tasa de fertilidad se habrá reducido a 1,8 hijos por mujer (similar a la actual tasa de fertilidad de España o Italia), con lo que Ecuador habrá entrado en una nueva fase al borde del fin del bono demográfico, la de envejecimiento general de su población, con una población en equilibrio y con una pirámide poblacional pentagonal algo cuadrada. 

Así también se espera que para ese entonces (año 2050) la esperanza de vida de los ecuatorianos al nacer sea de 80,55 años, siendo mayor para las mujeres, quienes vivirán en promedio 83,5 años, y los varones 77,6 años. La población que será urbana para ese entonces será superior al 70%, lo que demandará un esfuerzo gigante en los distritos metropolitanos de Quito, Guayaquil, y en especial en ciudades medias de importancia estratégica como Cuenca, Loja, Ambato, Machala, Ibarra, Manta, Portoviejo y Riobamba, también se espera la consolidación de ciudades espontáneas que surgieron el siglo pasado, como Milagro, Daule y Santo Domingo, así como la explosión habitacional de ciudades satélite en distritos y zonas metropolitanas, como será el caso del Valle de los Chillos, al oriente de Quito, con poblaciones como Cumbayá, Tumbaco, Puembo, Nayón y Sangolquí; y el caso de Samborondón y Durán en las periferias de Guayaquil. Para ese año también se espera un aumento considerable en la población irreligiosa, así como también en el aumento de población célibe y divorciada.

Quito, ciudad capital que cuenta con 2.011.388 de habitantes en el área urbana (sus 32 parroquias urbanas); posee 2.781.641 de habitantes en su distrito metropolitano, y 3.156.182 en su conurbación, siendo así la ciudad más poblada del país. Es sede de gobierno, donde se concentran los poderes del Estado, es la primera ciudad en ser declarada patrimonio de la Humanidad por la Unesco, porque también es el centro cultural e histórico del país, y desde 2015, es también la capital económica del país. 

Guayaquil, la segunda ciudad más poblada del Ecuador, cuenta con 2.654.433 habitantes en su zona urbana (sus 16 parroquias urbanas) y 3.453.621 millones en su área metropolitana. Polo de concentración de las ciudades del centro del país, por lo que es también la más extensa en área, es además, el puerto principal de Ecuador por donde se comercializan aproximadamente el 70 % de las importaciones y exportaciones del país, además de un importante destino de negocios en la costa del Pacífico. 

Cuenca, es la tercera ciudad en población y extensión, con 329.928 habitantes, constituyéndose como la urbe más importante del austro ecuatoriano, además de poseer un baluarte en su centro histórico, por ende, declarado Patrimonio de la Humanidad por la UNESCO. 

Según los proyecciones entregadas por el INEC para 2013, uno de cada tres ecuatorianos vive en Quito o Guayaquil; ambas ciudades sumadas engloban una población de más de 5,8 millones de habitantes. Otras ciudades importantes son Machala, Santo Domingo, Ambato, Portoviejo, Manta, Ibarra, Riobamba y Loja.

El Censo del 2010 cuestionó a los ecuatorianos mayores de 15 años sobre su autoidentificación, dando como resultado un 71.9 % de personas que se identificaron como mestizas, 7.4 % montubias, 7.8 % afroecuatorianas, 7.1 % indígenas, 7 % blancas y un 0.4 % en otras. Esto presentó un cambio frente a lo visto en 2001, en el cual se dieron los datos que siguen: mestizo 77.4 %, indígena 6.8 %, afrodescendiente 5 %, blanco 10.5 % y otros 0.3 %.,

Los estudios genéticos que se han realizado por la Universidad de las Américas y conducida por un grupo de expertos genetistas graduados en la Universidad Autónoma de Madrid sobre la población ecuatoriana han mostrado una composición algo típica para el norte de Sudamérica, dividiendo a la población en tres grandes grupos: mestizos, afrodescendientes y nativos, se determinó la presencia de los tres componentes en los tres grupos antes mencionados, variando únicamente la proporción de la misma. El estudio también reveló las posibles anomalías, ventajas y predisposiciones que existen en los genes de la mayor parte de los ecuatorianos, entre ellas un alto grado de inmunidad a la malaria en la población afro, e índices de alta tolerancia a la lactosa en mestizos. La población eurodescendiente no entró en el análisis suponiendo que su genética es netamente caucásica.

El idioma oficial es el español con sus características y modismos propios de cada zona o región, junto con el kichwa y el shuar son idiomas oficiales de relación intercultural según lo afirma la constitución de Ecuador en el artículo 2º del capítulo primero de los elementos constitutivos del estado. El wao terero, tsáfiqui y «demás idiomas ancestrales son de uso oficial para los pueblos indígenas, en los términos que fija la ley».

Según el censo de 2001, el 94 % de la población habla español, el 4.8 % habla alguna lengua nativa conjuntamente con el español y el 1.1 % hablan solo una lengua nativa. De las 13 lenguas nativas que fueron contabilizadas por el mencionado Censo, el quichua, hablado por el 4.1 % de la población, es la más difundida. La segunda lengua nativa es el shuar, hablado por el 0.55 % de la población. En la mayoría de los institutos públicos del país se imparte el inglés de una manera neutral.
El español es el idioma que se habla mayoritariamente en el Ecuador, como español ecuatoriano, a pesar de que este tiene diferencias dependiendo de muchos factores, el más importante de estos factores es la región. En el país hay tres variantes principales que son:

A pesar de que estas sean las principales variantes del idioma español en el país, hay muchos otros factores que influyen en el habla de una persona, siendo estos la etnia, la clase social o si se habita en el campo o la ciudad. Debido a que la costa y la región andina son las dos regiones más habitadas del país, sus dialectos son los más relevantes del país, ambos muy diferentes el uno del otro. Existen muchos modismos propios de cada provincia o región, así como otros que son entendidos y utilizados en todo el país.

Esta variante del español se encuentra clasificada dentro del dialecto del español ecuatorial, el cual se extiende desde las zonas costeras del Pacífico sur colombiano hasta la norte costa del Perú, cruzando el litoral ecuatoriano. El centro lingüístico influyente de esta región dialectal es la ciudad portuaria de Guayaquil.

La característica más destacable en este, es la aspiración de la letra "s" al final de las palabras o cuando va precedida de otra consonante, siendo muchas veces pronunciada como "j "suave o como una "h" inglesa [h].

Esta variante del español hablada a lo largo de la región costeña del país, así como las planicies colindantes al oeste de la cordillera de los Andes, representa una zona de transición entre las variedades caribeñas del norte de Colombia y Venezuela, y los dialectos ribereños del centro y sur del Perú; por lo que existen ciertas características compartidas con ambos que, a oídos de un foráneo, hacen difícil la identificación del dialecto costeño ecuatoriano.

Así, los dialectos de la Costa rigen y fijan el "foco fonemático de transición tonal-acentual" del español americano que se expande geográficamente desde la "entonación semigrave caribeña" y "mesoamericana" al norte –ya que la gravedad concreta la poseen las variantes del castellano peninsular o europeo– hacia la "intensa agudeza" localizadas al sur, propia de las tonalidades peruanas, chilenas y bolivianas.

Además, la variante ha incorporado dentro de su léxico, una serie de palabras compartidas con el resto de variedades dialectales del Ecuador, y que son entendidas solo dentro del país. En su mayoría son palabras provenientes del Español andino del Ecuador, con influencias del quichua, a pesar de que el idioma quechua no tuvo presencia histórica en el litoral ecuatoriano. Este es el caso de la palabra "ñaño" (hermano), muy extendida en todo el país y con origen quichua.

Uno de los acentos más relevantes de la región es el de la ciudad de Guayaquil, la segunda ciudad más grande del país. Por ser una urbe grande y que a lo largo de su historia ha ido creciendo demográficamente gracias a la migración, en la misma urbe existen muchas diferencias dialectuales, principalmente asociadas a la clase social, etnia y el nivel de escolarización. Entre las clases más escolarizadas, se tiende a corregir el acento hacia un castellano más estándar, así como incorporan a su léxico palabras foráneas, especialmente provenientes del inglés. En las clases menos escolarizadas, existen otras variedades dialectuales. Existe un grupo de personas que tienden a tener una entonación más parecida a la de un campesino costeño, pronunciando una "s" sorda alargada, mientras que otro grupo de personas tienden a tener una entonación más fuerte, que se suele conocer como "callejera", en la que la letra "s" es pronunciada numerosas veces como /"sh"/, además de contar con una serie de palabras propias, que no siempre son entendías por el resto de hablantes de la región.

Fuera de la ciudad de Guayaquil, se tiende a hablar un mismo dialecto en todas las provincias de la costa (con excepción de la provincia de Esmeraldas) con ligeras variaciones locales. Una variante importante es el dialecto hablado por los "montubios" (campesinos de la costa ecuatoriana), los cuales tienden a acentuar la primera sílaba de la mayoría de las palabras.

En la provincia de Manabí, existen una serie de dialectos también diferenciados que lo hacen fácilmente identificable como una variedad aparte del acento costeño ecuatoriano.

La provincia de Esmeraldas, por otro lado, presenta una notoria variante muy diferente a la del resto de la región, con un fuerte componente africano, y que se asemeja mucho al de región costeña fronteriza con Colombia. Debido a que esta región tiene una mayoría de habitantes afrodescendientes, este dialecto tiende a ser un poco más fuerte y con características propias tanto de léxico, como de entonación.

En el altiplano ecuatoriano suele hablarse una variante del español que muchos extranjeros suelen comparar con la del español chilango mexicano. En esta región encontramos cuatro principales variantes, siendo estas el español pastuso, el español andino central, el morlaco y el lojano. El español hablado en los Andes ecuatorianos tiende a tener muchos modismos tomados del idioma kichwa, idioma de los indígenas nativos de esta región. Palabras como "ñaño/a" o "taita" son utilizadas por personas de cualquier etnia o clase social en esta área -taita es un préstamo lingüístico bien acogido en los Andes; Corominas dice de Taita:"(...) es imposible suponer origen americano a esta palabra castellana heredada del latín..."-
El voseo es relativamente usado en esta parte del país, usado solo para conversaciones informales entre amigos o familiares.

En el Carchi, provincia andina fronteriza con Colombia se habla una variación muy especial, semejante a la del departamento colombiano de Nariño. Desde las provincias de Imbabura Pichincha y el Distrito Metropolitano de Quito suele hablarse un español con muchos préstamos colombianos, ibéricos, así como también muchos anglicismos; la forma de hablar aquí difiere mucho dependiendo al estrato social al que se pertenezca. Desde Cotopaxi hasta los límites de la Sierra Central (provincia de Chimborazo) se habla un español con aportes más bien procedentes del quichua y con ciertas influencias de la Costa. En la región norte, una gran variante es encontrada dentro del valle del Chota en la provincia de Imbabura, valle habitado por afrodecendientes, este dialecto es diferente al común dialecto andino, pero a su vez diferente del dialecto hablado por los afrodecendientes en la costa.

En las provincias de Cañar y Azuay se habla una variante bastante peculiar, el dialecto "morlaco". Este se caracteriza por su "cantado" o entonación característica de la zona, siendo muchas sílabas acentuadas en otras no correspondientes. Así mismo, la letra "r" tiende a ser muy arrastrada, sonando más como una "sh". Así mismo, esta zona tiene muchos modismos propios solo utilizados en esta zona, como la palabra "gara".
La provincia de Loja también tiene su variante, con una entonación bastante neutral.

Las ciudades de Quito e Ibarra tienen un caso especial, ya que el habla de estas ciudades no es compatible con los dialectos de la sierra y del resto del país, siendo enormemente similar al español del norte y centro de Chile, y con rasgos traídos por los españoles, que suman una importante parte de la población en ambas ciudades, los rasgos del español rioplatense, el voseo, ciertas donaciones quichuas y una infinidad de anglicismos es lo que representa al español de estas dos ciudades del norte del Ecuador.

En la región amazónica del país se habla una variante del español parecida al dialecto andino ecuatoriano. En el norte tiene influencia de los quijos, ellos tutean y conjugan el verbo en usted. En la provincia de Morona Santiago, se halla una entremezcla entre el español serrano indígena quichua de la sierra centro y el español morlaco (referente al de Azuay y Cañar). En la provincia de Zamora Chinchipe tiene una influencia considerable del acento lojano con una ligera variación de los dialectos nativos locales.

En las islas Galápagos se habla un dialecto sumamente parecido al de la costa del Ecuador continental. No se presentan grandes variaciones de importancia, al ser esta una región poco habitada en comparación al resto de regiones del país. También se conocen los dialectos de los grupos que existen en ese lugar.

En 2012 el Instituto Ecuatoriano de Estadística y Censos (INEC) realizó un censo (solo en las principales ciudades) en el que el 91.95 % de los encuestados respondió que tiene una religión, el 7.94 % se autodefinió como ateo y el 0.11 % se identificó como agnóstico. Dentro del grupo que profesa una religión el 80.40 % se autodefinió como católico y un 11.30 % como evangélica. Otras religiones mencionadas por los encuestados fueron: Mormonismo (1.42 %), Testigos de Jehová (1.29 %), Budismo (0.29 %) y Espiritismo (0.12 %).

Del total de personas que respondieron pertenecer a una religión el 39.7 % a su vez se consideraron como personas que creen en Dios pero no asisten a las celebraciones eucarísticas y el restante 60.3 % respondieron si ir regularmente a estas, siendo así uno de los países clave para el catolicismo en América.

Según el centro de estadística Pewforum (Estados Unidos) la afiliación religiosa en Ecuador es la siguiente:

En el año 2011 la investigación realizada por el Latinobarómetro, ha entrevistado en Ecuador 1189 personas. (El número hace referencia a las personas que han contestado a la entrevista).
De las cuales, 1005 se han declarado católicos, 149 afiliados a otras religiones, 33 no afiliados a una religión, 2 ateos y agnósticos.
De 1189, 1060 son practicantes. De los cuales, 624 son practicantes o muy practicantes.
Por lo tanto, en porcentaje, la afiliación religiosa en Ecuador es la siguiente:


Entre 1998 y 2002, Ecuador atravesó una grave crisis económica, política y financiera, la misma que fue acentuada por el fenómeno de la dolarización, que provocó que el Sucre se devaluara a niveles nunca antes vistos, provocando su desaparición y que el país adoptara como moneda válida al dólar de Estados Unidos.

Esta medida afectó directamente a los sectores más vulnerables de la sociedad, provocando el crecimiento de los niveles de pobreza e indigencia en el país, disminuyendo a niveles mínimos su poder adquisitivo, presentándose además una serie de fenómenos económicos que contrajeron la economía a nivel nacional, incrementando el desempleo en el país, ocasionando que el ingreso familiar no pueda cubrir ni la canasta básica.

Bajo este panorama desalentador, gran parte de los ecuatorianos al no tener un ingreso fijo que les permita satisfacer sus necesidades básicas, optaron por ofertar su fuerza laboral en el extranjero, puesto que en ciertos países se alcanzaban ofertas de trabajo y niveles de remuneración sensiblemente más elevados que los que se podría obtener en Ecuador, por esta causa varios países de Europa (principalmente España) y los Estados Unidos, comenzaron a captar personal para realizar trabajos pesados, pero que representaban una esperanza para quienes atravesaban problemas económicos, por lo tanto pese a representar muchos esfuerzos e inclusive ingresar como ilegales a otros países, se endeudaron para viajar y arriesgaron lo poco que tenían con la finalidad de alcanzar mejores ingresos económicos que les permita cubrir las necesidades básicas de su familia, pero sobre todo con el afán de obtener una remuneración más digna, para mejorar su nivel de vida y el de sus hijos; pero en muchos casos el costo de esto fue la descomposición de familias.

Durante 2007 hasta la actualidad, el repunte de la economía a niveles de hasta el 8 % anual, los factores de la crisis económica mundial del primer mundo, el conflicto militar colombiano, reformas migratorias y políticas internacionales de integración en Ecuador, modificaron completamente el panorama migratorio, atrayendo ahora una gran cantidad de inmigrantes.

A principios del siglo XX, eran muy pocos los ecuatorianos que dejaban el país para asentarse en otras latitudes.

Entre las décadas de 1910 y 1920, debido al "boom" de la cascarilla y el caucho en el Oriente, así como por el del cacao en la Costa, los antiguos terratenientes y la clase emergente de nuevos potentados comenzaron a enviar a sus hijos a estudiar en el exterior, particularmente en Francia. Esa emigración era selectiva, eventual y dio como resultado que quienes iban a Europa o a Estados Unidos no representarán mucho en las estadísticas de esos países.

Fue a partir de la década de 1950 en que se comenzó a registrar la salida de personas de bajos ingresos económicos que viajaban principalmente a Estados Unidos para «mejorar su situación económica», por necesidad, y algunas de ingresos medios que iban «a probar suerte», más por aventura que por necesidad.

En la década de los 70, en los destinos de los emigrantes se incluyeron países como Australia y Canadá. Personas de la Sierra centro, así como de Azuay y Cañar, veían reducidos sus grupos familiares. Esta migración aún no afectaba las estadísticas locales.

Ya entrados los años 80, la movilización humana hacia el exterior comenzó a adquirir dimensiones que ya incidían en lo económico y en los social. La influencia de modas, modos y costumbres comenzó a calar sobre todo en los hogares de clases media baja y baja.

Luego de la aguda crisis económica y financiera de 1999, se estima que más de tres millones de ecuatorianos (20 % del total de la población proyectada a 2005) abandonaron el país con rumbo a diferentes destinos, dirigiéndose la mayoría hacia Estados Unidos, España e Italia (a estos tres destinos fueron como mano de obra principalmente). También hubo emigración a otros países como Venezuela (en la década de los 80 y 90), Chile (con una buena cantidad de profesionales médicos o ligados a esta área), Canadá (profesionales técnicos) y, en menor grado y por diversos motivos, hacia Israel, Bélgica, México y el Reino Unido. La emigración ha continuado a lo largo de los primeros años del siglo XXI. No se conoce con exactitud cuántos ecuatorianos han emigrado ni tampoco existen estadísticas exactas sobre el número de ellos que reside en cada país, aunque extraoficialmente se calcula que solo en España viven casi 600 000 ecuatorianos.

En enero de 2007, el Gobierno nacional creó la Secretaría Nacional del Migrante (Senami), encargada de definir la política pública sobre movilidad humana (migración, emigración, inmigración, refugio, etc.), cuyas líneas se registraron en la Constitución de 2008. Ecuador desarrolló entonces importantes temas de movilidad humana, entre los que se incluyeron varios principios de la Declaración de Derechos Humanos que habían estado soslayados. Asimismo, se creó la Red Social Virtual de las Personas Migrantes y en enero de 2017 se emitió oficialmente la Ley de Movilidad Humana que vela por los Derechos de los migrantes dentro y fuera del país. Esta última fue reconocida por el ACNUR como ejemplo de protección integral para todas las personas en movimiento, posicionado al Ecuador como referente en la promulgación de Derechos Humanos de los migrantes.
Luego de 2002 la emigración se ha ido reduciendo año tras año con la estabilidad económica y se redujo más fuertemente con el inicio de la crisis del primer mundo en 2007 y el desarrollo significativo de la economía nacional y que desde 2010 se ha limitado prácticamente a migración por razones de becas estudiantiles y meramente turísticas o comerciales.

Con la larga migración desde Ecuador, también ha recibido inmigrantes durante su historia y actualmente recibe a decenas de miles de personas de diferentes países que tuvieron que abandonar sus países de origen por diferentes causas.

En un inicio, durante la colonia, la inmigración se centró en ciudadanos europeos atraídos por la explotación agrícola, luego en los inicios de la era republicana, se reflejó en árabes cristianos y europeos (principalmente españoles, belgas, neerlandeses, italianos y franceses) por crisis económicas y escapando de guerras.

En el siglo XX la inmigración fue mayormente de países latinoamericanos que por guerras civiles, crisis económicas, y dictaduras, agruparon argentinos, chilenos y uruguayos.

Del año 2002, el incremento inmigrante ha sido acelerado e importante, centrado en colombianos y venezolanos. Se argumenta que cerca de 350 000 colombianos viven en Ecuador.

Algunas personalidades extranjeras de televisión y deportes se establecieron firmemente debido al alto reconocimiento que han recibido por parte de los ecuatorianos en sus áreas de desenvolvimiento que ha merecido en muchos de los casos a solicitar naturalizaciones, conformar familias y crear inversiones de trabajo en el país. A partir del 2007 se aplicó una política para el retorno de migrantes ecuatorianos que establecía facilidades para que estos regresen al país con todo su mobiliario, materiales y accesorios de trabajo y hasta un vehículo familiar con exoneraciones totales de impuestos y aduanas. Así como accesibilidad a vivienda y créditos financieros, llevados de la mano con un auge económico y de posibilidades de empleo, lo que produjo un importante flujo de retorno de nacionales que cada vez ha aumentado considerablemente y que se acentuó aún más a raíz de la crisis económica que soporta Europa y Norteamérica.
De la misma manera varias políticas importantes en inmigración enfocada, resulta de atraer profesionales de calidad, con fines de investigación para universidades y escuelas politécnicas, cobertura en escasez y mejora en docencia universitaria, especialidades médicas para hospitales públicos, etc. que ha representado un fuerte flujo con atractivos incentivos que ha ayudado no solo al retorno de profesionales ecuatorianos de calidad, sino a la llegada de extranjeros para los mismos.

El último grupo de inmigrantes que denota en ser un reflejo de un alto valor de Ecuador a nivel internacional recae en los europeos, norteamericanos y demás latinoamericanos que han llegado al país por motivos de inversión y comodidad, atraídos por los paisajes naturales y la variedad de climas y especies, así estos han establecido sus residencias en sitios turísticos como localidades amazónicas; la ciudad de Baños de Agua Santa en la provincia de Tungurahua; ciudades como Cuenca, Guayaquil, Quito, Loja, Bahía de Caráquez, Salinas, etc; las ciudades, pueblos y villas en Imbabura (Otavalo, Cotacachi e Ibarra); balnearios de la Ruta del Sol y de la Ruta del Spondyllus, las islas Galápagos, parques nacionales entre otras.

Muchos también lo han elegido como destino de retiro para personas de la tercera edad y tratamientos médicos debido a las ventajas económicas y el coste que representa vivir en Ecuador frente al que tocarían afrontar en sus países de origen, además de la tranquilidad, accesibilidad y comodidad que este país ofrece, calificado en su excelencia por varios análisis, estudios y reportajes internacionales que lo ubican en primer lugar en esta categoría, por ello solo en Cotacachi, para 2007 más de 13 000 jubilados y veteranos alemanes y suizos llegaron a quedarse definitivamente.

La economía de Ecuador es la de América Latina y experimentó un crecimiento promedio del 4,6 % entre 2000 y 2006. En enero de 2009, el Banco Central del Ecuador (BCE) situó la previsión de crecimiento de 2010 en un 6,88 %. El PIB aumentó más de 40.000 millones de dólares entre 2007 y 2018, alcanzando los 108.398 millones de dólares según el Banco Mundial. La inflación al consumidor hasta enero de 2020 se situó en 0,23 %
En 1998, el 10 % de la población más rica tenía el 42,5 % de la renta, mientras que el 10 % de la población más pobre solamente contaba con el 0,6 % de la renta. Durante el mismo año, el 7,6 % del gasto en salud pública fue a parar al 20 % de la población pobre, mientras que el 20 % de la población rica recibió el 38,1 % de este mismo gasto. La tasa de pobreza extrema ha disminuido entre 2010 y 2018.
En 2010 se estimó en un 16,5 % de la población, mientras que para 2018 la cifra bajó a 8,4 % del total de la población. Esto se explica en gran parte por la emigración, así como la estabilidad económica lograda tras la dolarización. Las tasas de pobreza eran más elevadas para las poblaciones indígenas, afro-descendientes y rurales, alcanzando al 44 % de la población nativa

La baja del precio de petróleo desde 2015 ha hecho que este represente el 29 % de las exportaciones, manteniendo una balanza comercial marginalmente negativa a 2017. Desde finales de los años 60, la explotación del petróleo elevó la producción y sus reservas se calculan en 4.036 millones de barriles
La balanza comercial total para finales de 2019 se mantuvo en cero, lejos de lo alcanzado en 2010, cuando alcanzó un superávit de casi 5000 millones de dólares. Esta circunstancia se dio por la baja del precio del petróleo y al crecimiento de las importaciones, que crecieron más rápido que las exportaciones. La balanza comercial petrolera generó una cifra positiva de 4.443,98 millones de dólares en 2018; mientras que la no petrolera fue negativa por un monto de 4958,49 millones de dólares. Esto ha permitido que el país dependa del petróleo para evitar un mayor déficit comercial, la cual su economía demanda mayor cantidad de vehículos y maquinaria que no son producidos en el país. La balanza comercial con Estados Unidos, Chile, la Unión Europea y los países europeos que son socios de Ecuador, Bolivia, Perú, Brasil, es positiva México, Argentina, Colombia, Asia, es negativa.

En el sector agrícola, Ecuador es un importante exportador de bananas (primer lugar a nivel mundial en su producción y exportación), de flores, y el octavo productor mundial de cacao. Es significativa también su producción de camarón, caña de azúcar, arroz, algodón, maíz, palmitos y café. Su riqueza maderera comprende grandes extensiones de eucalipto en todo el país, así como manglar. Pinos y cedros son plantados en la región de la Sierra; nogales y romerillo; y madera de balsa, en la cuenca del río Guayas. Por otra parte, la industria se concentra principalmente en Guayaquil, el mayor centro industrial del país, y en Quito donde en los últimos años la industria ha crecido considerablemente, es también el mayor centro empresarial de país. La producción industrial está dirigida principalmente al mercado interno. Pese a lo anterior, existe una limitada exportación de productos elaborados o procesados industrialmente. Entre estos destacan los alimentos enlatados, licores, joyas, muebles y más.

Ecuador ha negociado acuerdos comerciales con otros países, entre los más importantes destacan el Acuerdo Comercial con la Unión Europea y el Acuerdo Comercial con la Asociación Europea de Libre Comercio. Por otra parte, tiene con Chile y Cuba acuerdos comerciales complementarios y forma parte del Acuerdo Complementario CAN- MERCOSUR. Además ha firmado acuerdos de alcance parcial con países centroamericanos, entre ellos El Salvador, Nicaragua, además de México. Ecuador pertenece a la Comunidad Andina de Naciones, y ser miembro asociado de Mercosur. También es miembro de la Organización Mundial del Comercio (OMC), además del Banco Interamericano de Desarrollo (BID), Banco Mundial, Fondo Monetario Internacional (FMI), Corporación Andina de Fomento (CAF), y otros organismos multilaterales. En abril de 2007, Ecuador pagó por completo su deuda con el FMI, aunque volvió a pedirle préstamos en 2018 con el gobierno de Lenín Moreno. En 2007, se creó la Unión de Naciones Suramericanas (UNASUR), con sede permanente en Quito, aunque Ecuador dicidió retirarse de forma definitiva en 2018 alegando falta de acciones concretas. Es miembro fundador del Foro para el progreso del Sur ProSur. También ha sido parte de la creación del Banco del Sur, junto con seis otras naciones suramericanas. Ecuador realizó negociaciones para la firma de un Tratado de Libre Comercio con Estados Unidos, pero con la elección del Presidente Correa estas negociaciones fueron suspendidas.

El sistema público financiero del Ecuador está conformado por el Banco Central del Ecuador (BCE), el BanEcuador B.P., el Banco del Estado, la Corporación Financiera Nacional y el Banco del Pacífico.

Actualmente Ecuador no permite el uso de Bitcoin para el pago de bienes o servicios. El Banco Central del Ecuador emitió el siguiente mensaje:

""El Banco Central del Ecuador informa a la ciudadanía que el bitcoin no es un medio de pago autorizado para su uso en el país. El bitcoin es una criptomoneda que no tiene respaldo, pues sustenta su valor en la especulación. Las transacciones financieras realizadas a través del bitcoin no están controladas, supervisadas ni reguladas por ninguna entidad del Ecuador, razón por la que su uso representa un riesgo financiero para quienes lo utilizan."

"Es importante señalar que no está prohibida la compra y venta de criptomonedas -como el bitcoin- a través de Internet; sin embargo, se recalca que bitcoin no es una moneda de curso legal y no está autorizada como un medio de pago de bienes y servicios en el Ecuador, conforme lo establece el artículo 94 del Código Orgánico Monetario y Financiero.""

La prohibición viene dada por la intención de que el Estado cree su propia moneda virtual.

A partir de 2007, con una economía superada por la crisis económica, una serie de reformas políticas económicas han ayudado a encaminar a la economía ecuatoriana a un desarrollo sostenido, considerable y enfocado a lograr una estabilidad financiera, política y social; basada en la tendencia tomada por la región latinoamericana que ayudó a no verse afectada por la crisis mundial del primer mundo en 2010.

Impuestos enfocados a cambiar hábitos de consumo; desarrollo de sectores estratégicos y prioritarios; construcción, mejoramiento de sectores claves; desarrollo de la industria interna; políticas claras de comercio, competitividad, inversión estratégica, mejoramiento laboral, etc., han ayudado a lograr un crecimiento económico destacado que alcanza por encima del 8 % anual en 2011, esto reflejado en una clara disminución de la mendicidad y pobreza extrema, estabilidad de la clase media, disminución de la brecha de las clases sociales, creación de puestos de trabajos, aumento del comercio interno, entre otros.

Todo esto acompañado con nuevos mercados internacionales y de cooperación, principalmente con países asiáticos y latinoamericanos. Un ejemplo es la solicitud formal de convertirse en miembro pleno del Mercosur, creación de nuevas embajadas en Asia, fortalecimiento de organismos como la CELAC, CAN, etc; e implementación de proyectos como el eje Manta-Manaos como alternativa al Canal de Panamá y el proyecto alterno de navegabilidad por los ríos Morona - Amazonas.
En las zonas rurales, en las que vive más o menos el 37 % de la población del país, se estima que el 45 % de los habitantes de dicha fracción subsiste en condiciones de necesidad. Gran parte de los casos fueron producto de no haber sido considerados por décadas al momento de hacer inversión en educación y de obras de infraestructura, como carecer de tierras adecuadas, regadíos suficientes y falta de vías de acceso en buen estado. Aunque se ha notado un progreso notable en revertir esta situación en los últimos años, desarrollando significativas inversiones en educación e infraestructura, junto con créditos de los de las cuales ha mejorado la vida del campesinado ecuatoriano y ha logrado detener la migración hacia las ciudades lo cual se reflejó en el censo de 2010 donde la población rural llegó al 37.23 % frente al 62.77 % . En el período 2018-2020 el empleo pleno en Ecuador se ha visto afectado por políticas neo liberales que ha dejado como resultado la disminución del empleo efectivo en 1,80 %, pasando de 40,6 % en diciembre de 2018 a 38,8 % en diciembre de 2019. Así mismo el desempleo llegó a situarse en el 3.80 % al cierre del año 2019, al mismo tiempo el empleo inadecuado se situó en 17,8 %, según datos del INEC, con el resto de la población en empleo no pleno y no remunerados; esto tomando en cuenta que la mayoría de la población ecuatoriana está en edad. La pobreza por ingresos ha subido sobre todo en el gobierno de Moreno, afectando al 24 % de la población en comparación al 2016, que estaba en 22,9 %. El crecimiento económico ha sido fluctuante en los últimos 7 años (2012-2019), la cual pasó de una estimación de crecimiento económico por encima de la media latinoamérica del 5.4 % para 2012, a un nulo crecimiento en 2019 cuando la economía creció apenas el 0,1%.

En el “Índice de Competitividad Global 2019” el Ecuador se encuentra en el puesto 90 con una diferencia de 7 puntos en contraste a la edición pasada. Según el mismo, uno de los factores problemáticos que resaltan en el país es la inestabilidad política que resulta ser un factor importante puesto que influye a las decisiones de posibles inversores extranjeros, este en el reporte anterior se establecía en quinto lugar, ahora se halla liderando la lista con un cambio de mandatario cuyos primeros actos han generado en una parte del pueblo ecuatoriano incertidumbre y en la otra gran parte, un respiro a la administración previa. El país enfrenta aún desafíos como la falta de capacitación en la administración de sus recursos pero su progreso sigue siendo notable.

Algunos de sus fuertes en la actualidad son la infraestructura, el tamaño del mercado, la educación superior y como índice principal, la salud y educación primaria que ha mejorado unos catorce puntos en comparación con el “Índice de Competitividad Global 2016-2017” el cual se ubica en el puesto 68.
Si bien, el alza que mostró el Ecuador con su PIB desde el 2000 al 2014 ha revelado una baja desde ese año en adelante, para el 2018 el FMI previó una mejora del PIB del país que lo ubicaría en una cifra ligeramente positiva, con 0.6 % y con una tenue mejora reduciendo la inflación del 2017 de 0.8 % a 0.7 % en 2018.

Según el Banco Mundial, la tasa de pobreza en Ecuador aumentó del 21,5% en 2017 al 25% en 2019. Además, el índice de Coeficiente de Gini, que mide la desigualdad de los ingresos, pasó del 0,45% en 2014 al 0,447% (la igualdad perfecta es 0%).

El país cuenta con una amplia gama de áreas de explotación, minera, petrolífera, agropecuaria, acuicultura y avícola, que se ha aprovechado a lo largo de su historia, y en la actualidad se ha desarrollado en un ámbito más objetivo, focalizado, ordenado y orientado a un conservacionismo ambiental (p. ej: La reserva del Yasuní) y rentabilidad a corto y largo plazo. Independizándose cada vez más de la dependencia internacional o de proveedores privados para sustentar las necesidades generales o prioritarias en el desarrollo nacional.

El país cuenta con potencial para la industria en una gran variedad de sectores, como por ejemplo el petróleo. La producción interna de materias primas textiles y manufacturadas; la minería; la industria química, petroquímica; así como la petrolera y gasífera, por disponer de la principal entrada de esta industria; generación eléctrica debido a su altísimo potencial hidráulico, solar y eólico en varios sectores del país; la elaboración de productos a base de la fundición de materiales o cristales; producción agroindustrial y de alimentos procesados; producción farmacéutica, entre otros.
Un proyecto que fue de mayor relevancia para el desarrollo fue la refinería del Pacífico, ubicada en Manta, la cual iba a ser una de las mayores en la región. Después que el proyecto fracasó, el gobierno de Moreno empezó a licitar la construcción de una refinería en la costa ecuatoriana, con total capital privado. Con esto permitirá a Ecuador pasar de ser importador de derivados del petróleo a exportador de los mismos, aunque no ha habido ofertantes a mayo 2020. Además, diversos proyectos hidroeléctricos entre el más destacable la hidroeléctrica Coca Codo Sinclair que genera cerca del 40 % de la demanda del país que junto a los demás proyectos permite a Ecuador ser unos de los principales exportadores de energía eléctrica en el continente, comprobándose la venta de energía eléctrica a Colombia por unos 80 millones de dólares a 2019; la minería a gran escala que firmó su primer contrato en la historia en marzo de 2012, y que a 2020 se analiza ser una de las industrias de explotación de mayores ingresos junto con el petrolífero, pues se han encontrado varios yacimientos de diversos metales en Azuay, Morona Santiago, El Oro y Zamora Chinchipe, en este último, ya en marcha el mencionado contrato con más de 5.000 millones de dólares de ingresos netos al Estado ecuatoriano, fuera de otros.

La rehabilitación y reapertura después de décadas de abandono del ferrocarril ecuatoriano en toda su trayectoria, considerado uno de los más aventureros hermosos y diversos; iniciado en el gobierno de Gabriel García Moreno y considerado uno de los más complicados de elaborarse en la época por las condiciones de los terrenos donde fue construido.

La vialidad del Ecuador en los últimos 10 años (2010-2020) ha sufrido un enorme desarrollo, con vías y autopistas desarrolladas bastante considerables, incluyendo la de implementación de vías rápidas o autopistas a lo largo de su territorio. Actualmente cuenta con casi su completa red vial asfaltada y con señalética y seguridades modernas para los mismos, todos los proyectos enfocados a vías de 6 carriles. Las vías de mayor importancia son la Panamericana (actualmente en ampliación de 4 a 6 carriles desde Rumichaca hasta Ambato, la conclusión de 4 carriles en todo el tramo de Ambato a Riobamba y la laguna de Colta, ampliación en la provincia de Cañar, mejoramiento entre Azogues y Cuenca y la ya en funcionamiento hasta Loja). La Ruta del Espondilus y/o Ruta del Sol (orientada a viajar por toda la línea costera ecuatoriana); la troncal amazónica (que cruza de norte a sur toda la Amazonía ecuatoriana, enlazando la mayoría y más importantes ciudades de la misma); Otro proyecto de gran importancia en desarrollo es la carretera Manta - Tena; la autopista Guayaquil - Salinas; la carretera Aloag - Santo Domingo; Riobamba - Macas (olvidada durante más de 40 años, y que atraviesa por el parque nacional Sangay), el complejo de puentes Unidad Nacional en Guayaquil, el puente sobre el río Napo en Francisco de Orellana; el puente sobre el río Esmeraldas en dicha ciudad del mismo nombre; y quizá la más rescatable de todas, el puente Bahía - San Vicente, siendo el mayor en la costa del Pacífico latinoamericano. Al igual que una amplia red vial de caminos vecinales asfaltados y de hormigón que han ayudado a mejorar sustancialmente el comercio y el desarrollo.
Los aeropuertos internacionales de Quito y Guayaquil han sufrido un alto aumento que ha requerido su modernización, que en el caso de Guayaquil y su Aeropuerto Internacional José Joaquín de Olmedo - Guayaquil implicó la construcción de una nueva terminal aérea de 53 mil metros cuadrados, considerada de las mejores de Latinoamérica y una de las mejores del mundo, terminal guayaquileña que es usada por más de cuatro millones de pasajeros anuales (aunque está en marcha la construcción del nuevo aeropuerto intercontinental en Daular en la vía a la costa, que contará con 3 pistas para vuelos simultáneos en un área de 2020 hectáreas); y en Quito se inauguró el 20 de febrero de 2013 el Aeropuerto Internacional Mariscal Sucre, ubicado en la localidad de Tababela, distante 25 kilómetros del Centro Histórico de la capital, su pista tiene 4.098 metros de longitud y su torre de control posee 41 metros de alto. Tiene espacio suficiente para una segunda pista, que será construida en el futuro. En una segunda etapa, prevista para 2023, se ampliará la terminal en 20 mil metros cuadrados más. Por el largo de su pista, puede recibir a los aviones más grandes de la actualidad, como el Boeing 747 o el Airbus A380. La superficie del Aeropuerto es de 1500 hectáreas, el área construida es de 70 hectáreas, la terminal de pasajeros tiene 38 mil metros cuadrados de superficie y se estima que cinco millones de personas lo usarán al año.

También entró a funcionar el restaurado y mejorado aeropuerto de Cotopaxi en Latacunga, orientado principalmente como aeropuerto de carga internacional, pero que funciona también para transporte interno de pasajeros. Actualmente se halla en desarrollo la implementación del aeropuerto internacional en Manta. Todos estos con proyecto de aeropuertos intercontinentales. Pero Ecuador también cuenta con varios aeropuertos de transporte interno. Entre los más destacables: Francisco de Orellana, Nueva Loja, Tulcán, Esmeraldas, Loja (Ciudad de Catamayo), Santa Rosa, Shell, Salinas, Tena, entre otros. En la Amazonía donde sirve la Fuerza Aérea para habitantes de sitios inaccesibles sus pistas y flotas han sido modernizados en los últimos años. Uno de los desarrollos destacables en aeronavegación ecuatoriana es la implementación de aeropuertos binacionales como el de Tulcán y Esmeraldas con vuelos a Colombia, y en desarrollo aún el de Santa Rosa y Loja con vuelos a Perú.

Los puertos marítimos son un notable punto en el comercio y turismo. El más importante, por donde pasa el 70 % de la exportación e importación del país, es el puerto de Guayaquil, ubicado al sur de la ciudad costera, al que también llegan cruceros con pasajeros de distintos países; además, la modernización en los últimos años ha permitido que puertos como el de Manta lleguen cruceros de gran calado. Otro puerto de gran importancia es el de Posorja en el Golfo de Guayaquil, mayormente de carga. Puerto Bolívar en Machala es principalmente para la exportación agrícola como banano, camarón, cacao, etc. El puerto de Esmeraldas principalmente para la exportación industrial de petróleo, gas y sus derivados.

En 2010 Ecuador tenía una tasa de alfabetización de 99.78 % (el quinto más alto de Latinoamérica), después de intensas campañas públicas para eliminar el analfabetismo. Con la implementación de la educación gratuita, las instituciones educativas públicas tanto escolares como secundarias no precisan cobro de aranceles ni pensiones de educación, incluyendo sectores rurales y urbanos marginales. En el caso de la educación gratuita en las universidades e institutos superiores, se aplica bajo responsabilidad académica que exonera únicamente los créditos que no reprueba cada estudiante, así como servicios académicos como internet.

La educación superior ha llevado a cabo una fuerte evaluación de calidad, que categorizó en cinco niveles en un ámbito general de evaluación a las 68 universidades dedicadas a carreras de tercer nivel principalmente, que determinó los reconocimientos y deficiencias de cada institución. Solo 11 lograron la categoría A y 26 quedaron colocadas en categoría E. Bajo una política educativa, leyes y normas que ha dado a todas plazos cortos de reestructuración académica. En abril de 2012 se procedió con la primera etapa de depuración universitaria que suspendió a 14 universidades por no contar con exigencias mínimas y básicas para continuar funcionando después de tres años de su notificación. Ocho quedaron con fuertes limitaciones y sanciones. Tres quedaron elevadas a una categoría D y una quedó pendiente debido a que se trataba de una educación diferente y de mayor estudio. En la actualidad existen 57 universidades y escuelas politécnicas entre públicas y privadas, y tres universidades exclusivas de educación de cuarto nivel.

Otro aspecto clave ha sido el aumento por miles de becas y créditos, así como la amplitud a nuevos créditos a áreas claves de desarrollo, tanto para carreras de pregrado como posgrado, dentro y fuera del país, así como la convalidación de títulos extranjeros de manera ágil, con bajos costos de tramitación ayudado por el reconocimiento en ciertos casos directo de los títulos emitidos por varias centenas de universidades extranjeras.

Cabe destacar la creación de las llamadas Universidaddes Emblemáticas:


Estas nuevas universidades nacen como parte del cambio de matriz productiva que fomentaba el gobierno para suplir la demanda de investigación e innovación en el país.

Actualmente el método de asignación de cupos en las diferentes carreras por institución educativa superior establece un examen general de aptitud a todos los postulantes sin límite de edad ni intentos, donde seleccionan en una jerarquía de 10 preferencias, la carrera y la universidad en la que desearían estudiar, después de obtener un puntaje, la lista se depura a fin de mostrar si el estudiante obtuvo el mínimo requerido exigido por cada universidad para la carrera que eligió, los mayores puntajes calificados como sobresalientes obtienen becas ilimitadas por el Estado ecuatoriano para cualquier carrera en cualquier universidad (siempre que esta universidad también lo apruebe) a nivel mundial con la condición y garantía de regresar al país una vez terminada su carrera. Los demás que alcanzan los puntajes de su listado se les asigna el cupo previa consulta al estudiante, y quienes no alcanzaron a ninguno de los cupos de su listado, se les asigna cupos de acuerdo a las aptitudes que la evaluación psicológica arrojó pues obviamente no concordaban con las aspiraciones que inicialmente pensó seguir, preguntándole, dándole la opción al estudiante de aceptar alguno de dichos cupos o esperar a la siguiente evaluación para con mejor preparación aprobar su aspiración. Finalmente si los postulantes exceden el número de cupos que posee cada carrera a nivel nacional, los interesados deberán esperar un periodo académico para poder obtener un cupo, si esta vez el número de postulantes excediera, nuevamente tendrán que postergar su ingreso a la universidad.

La esperanza de vida es de 93,8 años. 

La mortalidad infantil, de 24,4 por 1.000 en 2005, disminuyó a 18,3 en 2015.

Entre 2008 y 2016, el Gobierno invirtió más de 15.000 millones de dólares, multiplicando por cinco el gasto medio anual en sanidad del periodo 2000-2006. En cuanto a los profesionales que trabajan para el Ministerio, entre 2008 y 2015 su número pasó de 11.201 a más de 33.000, incremento que fue acompañado también de aumentos salariales.

En 2015, la corrupción sigue siendo un problema a pesar de las múltiples inversiones. La sobrefacturación se registra en el 20% de los establecimientos públicos y en el 80% de los privados.

La tasa de homicidio de Ecuador disminuyó de 18 a 5.8 por 100 mil habitantes entre 2011 y 2017, haciendo de Ecuador uno de los países más seguros de América. Esto se ha logrado, en particular, tras una importante reforma de la policía, que era conocida por su corrupción y falta de eficiencia. Se ha aumentado considerablemente la duración de la formación y la remuneración de los policías y se han realizado inversiones para la modernización del equipo policial. Además, desde 2007 se ha adoptado un nuevo enfoque, menos represivo y que presta mayor atención a la prevención y la reintegración. Se ha facilitado el acceso a los programas sociales a los antiguos delincuentes.

La literatura ecuatoriana se ha caracterizado por ser esencialmente costumbrista y, en general, muy ligada a los sucesos exclusivamente nacionales, con narraciones que permiten vislumbrar cómo es y se desenvuelve la vida del ciudadano común y corriente. Ecuador no ha dado literatos cuyos libros se vendan masivamente a nivel mundial. Pese a lo anterior, algunos escritores ecuatorianos han logrado ser medianamente conocidos en el contexto internacional, especialmente en los países hispanohablantes o iberoamericanos. Entre estos tenemos a Jorge Icaza, Alejandro Carrión Aguirre, Juan Montalvo, José de la Cuadra, Pedro Jorge Vera, Pablo Palacio, Demetrio Aguilera Malta, Alfredo Pareja Díez Canseco, Numa Pompilio Llona, Adalberto Ortiz, Medardo Ángel Silva, César Dávila Andrade, Luis Costales, Alfonso Rumazo González, José Martínez Queirolo,Pablo Palacios, Jorge Enrique Adoum, Carlos Carrera Barreto, Hugo Mayo, Jorge Carrera Andrade, Arturo Borja, Ernesto Noboa y Caamaño, Juan León Mera. Uno de los aspectos más interesantes de las letras ecuatorianas, es que estas han producido una cantidad notable de buena narrativa, con autores que lograron fotografiar la idiosincrasia criolla y plasmarla en sus relatos. Nadie podría decir, pese a la crudeza de su contenido, que por ejemplo las novelas de Jorge Icaza no son un retrato muy hábilmente fabricado de las horribles penurias del indígena de la sierra ecuatoriana. Icaza traslada al lector al escenario que describe e incluso utiliza el mismo lenguaje que tienen los protagonistas en la vida real en su novela Huasipungo.

Pero la literatura ecuatoriana no se limita únicamente a Icaza y el indigenismo. También existen otros grandes expositores de la misma, como Alfredo Pareja Díez Canseco, quien destacó más que nada como novelista. Este, en contraposición a Jorge Icaza, creó novelas esencialmente urbanas, en las que aflora la denuncia social. También fue un gran historiador. Si seguimos en la senda de los novela dedicada a la denuncia social, es imprescindible nombrar a Joaquín Gallegos Lara, cuya obra, aunque breve, es magistral al aludir a los problemas que agobian a la clase obrera y la brutal explotación que esta sufre a manos de empresarios inescrupulosos. En "Las cruces sobre el agua" narra la peor masacre obrera ocurrida en la historia del Ecuador (1922). Demetrio Aguilera Malta, en cambio, fue más que nada un novelista costumbrista aunque también muy multifacético. En sus escritos describió al "montubio", el típico campesino mestizo de la costa ecuatoriana. Entre las mujeres que escriben está Alicia Yánez Cossío, dueña de una considerable producción narrativa, en la que se incluye la novela ""Sé que vienen a matarme"", una polémica novela acerca del dictador Gabriel García Moreno y sus excesos mientras era presidente de Ecuador. La poetisa Karina Gálvez, nominada en 2011 a la 1.ª Medalla Internacional a la Paz y a la Cultura "Presidente Salvador Allende", autora del poema "La Batalla del Pichincha" , ha dado a conocer la poesía ecuatoriana a nivel internacional.

En la literatura contemporánea podemos encontrar varios ensayistas importantes como Agustín Cueva y Bolívar Echeverría; narradores como Javier Vásconez, Eliécer Cárdenas, Huilo Ruales, Santiago Páez, Abdón Ubidia, Marco Antonio Rodríguez, Humberto Salvador, Pablo Palacio, Leonardo Valencia, Gabriela Alemán, Iván Egüez, Jorge Luis Cáceres, Eduardo Varas, Miguel Antonio Chávez; o poetas como Ernesto Carrión, Alexis Naranjo, Hugo Mayo, Iván Carvajal Aguirre, Kléber Franco Cruz, Iván Oñate, Julio Pazos Barrera, Humberto Vinueza, Javier Ponce, Fernando Nieto Cadena, Jorge Martillo, Edwin Madrid, Cristian Avecillas, Carla Badillo Coronado, Marcos Rivadeneira Silva, Pedro Gil, Julia Erazo D. entre otros.

La práctica artística contemporánea en el Ecuador cuenta con talentosos exponentes que han madurado proyectos y propuestas. Las políticas culturales institucionales del país han provocado que la escena artística contemporánea no logre operar con la contundencia ni la eficiencia que se requiere para ocupar un espacio competitivo a nivel internacional. Sin embargo entre los artistas contemporáneos del Ecuador encontramos figuras que han logrado autónomamente estabilidad y competencia en la creación y exhibición de obras, así como el reconocimiento local e internacional: Oswaldo Guayasamín, Gonzalo Endara, Eduardo Kingman, Camilo Egas, Oswaldo Viteri, Teddy Cobeña, Carlos Rosero, entre otros. La extinta aerolínea Ecuatoriana de Aviación, llevaba en sus aviones las ilutraciones de los maestros Oswaldo Guayasamín y Eduardo Kingman como parte de su distintivo colorido.

Es un país con atributos en las artesanías. Esto se da, por una parte debido a su legendaria tradición de productos de uso cotidiano, pasando por la cerámica y los usos que se le dieron, además de los metales y la cestería, y por la otra gracias a una enorme cultura productora de textiles e instrumentos musicales.

En el campo de la dramaturgia casi no ha habido exponentes relevantes o que hayan alcanzado un alto grado de difusión, especialmente a nivel internacional. Sin duda el mejor, más prolífico y conocido es el guayaquileño José Martínez Queirolo, cuyas obras se han representado en Estados Unidos y Europa, a la vez que han sido traducidas a otros idiomas. También se lo conoce como autor de numerosos cuentos, entre los que también hay algunos creados para niños. Además es un destacado actor y dirige su propia compañía de teatro. Ganó el Premio Nacional de Cultura "Eugenio Espejo" en 2001.

El costumbrismo en el teatro ecuatoriano es un subgénero iniciado a principios del siglo XX cuyos principales exponentes fueron Ernesto Albán Mosquera con su personaje Evaristo Corral y Wigberto Dueñas Peña con el Indio Mariano. Es un género eminentemente satírico, con una alta carga de crítica socio-política.

La producción cinematográfica de Ecuador incluye cortos y documentales hechos a lo largo del siglo XX. Pese a la calidad o el valor histórico de algunas de esas aportaciones culturales, el cine ecuatoriano solo ha comenzado a tener repercusión internacional en el siglo XXI. La producción de largometrajes fue en el siglo pasado una limitante en lo referente a cantidad, en gran parte debido a los costos que conllevaba producir una película. Pese a eso en la última década la producción cinematográfica ha aumentado en una escalada importante, debido a las facilidades y garantías que ofrece el país bajo leyes e iniciativas para su producción y difusión, así como exoneraciones de impuestos.

Varias cintas han llegado a proyectarse comercialmente y con éxito, tal es el caso de "Crónicas", "Que tan lejos", "Con mi corazón en Yambo", "A tus espaldas", "Mono con gallinas", "Mejor no hablar de ciertas cosas" o "En el nombre de la hija", algunos de ellos con reconocimientos en festivales internacionales de cine.

Ecuador posee una gran diversidad de estilos musicales producto de la riqueza cultural, herencia y mestizaje de las culturas europeas, afro, e indígenas. El pasillo es considerado el género de música nacional, y su día nacional se celebra el 1 de octubre, natalicio de Julio Jaramillo, máximo exponente del género. Entre los géneros musicales locales se destacan ritmos mestizos como el pasacalle, el pasillo, el yaraví, el albazo, el bolero, el requinto; ritmos afros como la bomba, la marimba, guaracha, mambo; ritmos indígenas como el sanjuanito música Folclórica andina. De influencia extranjera géneros como el pop, el rock, el merengue, la salsa, la cumbia, el vallenato, la bachata, el ska, la música electrónica, el dance, el reggae, el heavy metal, el punk, el hip hop o el reguetón.

El más destacado músico, Luis Humberto Salgado quien fue un compositor de música clásica de Ecuador. Compuso nueve sinfonías, cuatro óperas, una ópera-ballet, siete conciertos, operetas y varias piezas de música popular ecuatoriana, sobre todo sanjuanitos y pasacalles, algunos con innovaciones notables como la Segunda sinfonía (Sintética no. 1), la Sexta sinfonía (para cuerdas y timbales), las sonatas 2 y 3 para piano y el sanjuanito futurista. Otro compositor de renombre, fue el lojano Carlos Miguel Agustín Vaca, cuyas aportaciones musicales fueron destacadas, ya que compuso el himno de numerosas ciudades y cantones dentro de Loja. De la música tradicional ecuatoriana extrajo motivos para sus óperas así como también ritmos y armonías para sus sinfonías. Pese a los esfuerzos de la Orquesta Sinfónica de Ecuador y del director Álvaro Manzano, buena parte de su música está aún por interpretarse y casi la totalidad por grabarse.

En el ámbito operístico, tenemos a Marlon Valverde, tenor ecuatoriano, quién encarnó al Libertador Simón Bolívar, en la ópera Manuela y Simón; Andrés Córdova, Sofía Rosado, Viviana Rodríguez, Roy Espinoza, quienes participaron en La Traviata. Destaca la soprano Beatriz Parra, por sus condecoraciones y logros otorgados en su larga trayectoria dentro de este campo.

También destacan artistas como Carlota Jaramillo, Julio Jaramillo, Las Hermanas Mendoza Suasti, Polibio Mayorga, Dúo Benítez-Valencia, Hermanos Miño Naranjo, Paulina Tamayo, Fresia Saavedra, Segundo Rosero, Anita Lucía Proaño, Soledad Morales, Segundo Bautista, Juan Fernando Velasco, entre otros. En el subgénero de música popular como la technocumbia, se destacan también muchos cantantes como: Sharon la Hechicera, Gerardo Morán, María de los ángeles, Jaime Enrique Aymara, Delfín Quishpe, entre otros.

Gerardo Mejía, conocido en los 90 simplemente como Gerardo, también es uno de los artistas ecuatorianos con mayor repercusión internacional, siendo uno de los primeros latinos en triunfar en el mercado anglo, donde llegó a ubicar su single debut en el Top 10 de Billboard Hot 100. Paulina Aguirre, cantante de música cristiana ha sido galardonada con una Gaviota en el Festival de Viña del Mar y Juan Fernando Velasco, son los únicos en ganar un Grammy Latino. Otra artista reconocida es Mirella Cesa, quien ha conseguido sonar internacionalmente, ganó una gaviota de plata en el festival Viña del Mar 2018 y aparece en los listados latinos de Billboard en Estados Unidos.

A partir de finales de la década de los 60, bajo la influencia de grupos anglosajones como The Beatles, o bandas pioneras del rock en español como Almendra, empiezan a gestarse en el país las primeras agrupaciones de rock que darían nacimiento a los primeros episodios del Rock Ecuatoriano como Boddega, Los Corvets, o Los Hippies.

Es durante la década de los 80 que el rock nacional realmente empieza a jugar un importante rol dentro de la escena de música ecuatoriana, con grupos como Tranzas, Clip, o Contravía. A su vez también son recordados artistas de gran trayectoria como Pancho Jaime, Hugo Idrovo, o Héctor Napolitano. Sin embargo, los nacientes temas del rock nacional sufrieron en gran medida numerosos intentos de censura por parte del gobierno de León Febres-Cordero Ribadeneyra llegando incluso a sabotear los shows y festivales donde se tocaran los temas de estos artistas. Durante los años 90 también aparece Verde 70, considerado uno de los mejores grupos de rock ecuatoriano.

Es gracias a la realización de festivales de música independiente como Al Sur del Cielo, Quito Fest, o Pululahua, Rock desde el Volcán que en las décadas del 2000 y 2010 se le empieza a dar gran acogida a nuevas agrupaciones ecuatorianas como Guardarraya, Esto es Eso, Sal y Mileto, Rocola Bacalao, Mamá Vudú, Sudakaya, y entre otros, que buscan renovar la escena al combinar e incorporar elementos de estilos de música autóctonos del Ecuador y de Latinoamérica como el pasillo o la Bomba del Chota, con géneros como el Rock, Techno, Hip Hop, o Reggae. En la actualidad, se destacan grupos como Da Pawn, La Máquina Camaleón, Les Petit Bâtards, o Lolabúm.

En la actualidad 8,1 de cada 10 ecuatorianos usan internet regularmente para diversos fines: desde correo electrónico, comercio electrónico, prácticas laborales, ocio, educación, información, entre otros. Se calcula que con la reducción y ampliación de accesibilidad que se ha dado en los últimos años, el 79 % de los hogares posee internet, donde el uso es mayor en el sector urbano con el 64,4%, frente al 37,9%, en el área rural. Por provincias, Galápagos es la que tiene más porcentaje de personas que han utilizado Internet con el 78,7%, seguido de Pichincha con el 67,1%, Azuay con el 61,1%, El Oro con el 59,7% y Guayas con el 59,3%. La fibra óptica cubre las 24 provincias del país y el auge de la conexión Wi-Fi mayormente por entidades públicas.

Si bien la penetración de internet es comparable con la mayoría de los países latinoamericanos, Ecuador cuenta con un número interesante de usuarios de redes sociales, y páginas web especializadas. Según Facebook, a noviembre de 2012, Ecuador posee 5 077 060 de usuarios, lo cual la ubica en la posición 35 a nivel mundial de países con más usuarios de esta red social.

Si bien la telefonía fija se mantiene aún en el país con un crecimiento periódico, esta ha sido desplazada muy notablemente por la telefonía celular, tanto por la enorme cobertura que ofrece y la fácil accesibilidad.

Actualmente se determinó que existen más líneas de telefonía celular que habitantes en Ecuador, fenómeno que se aclara por los usuarios que optaron por tener dos líneas en su poder de diferentes operadoras para reducción de costes en llamadas y mensajes, por la descontrolada venta de líneas que en su mayoría dejaron de ser utilizadas y no han sido deshabilitadas, así como otras; en la actualidad tratan mediante políticas de estado, restructurarlas, registrarlas por usuario entre otras. Fuera de ello se determina que cada ecuatoriano por lo general posee un celular a partir de los 14 años más allá muchas veces de su estatus económico, tomando en cuenta ciertas excepciones y la menor presencia que obviamente se da en el sector rural frente al urbano.

En el país, existen cuatro operadoras de telefonía fija, CNT, ETAPA (públicas), TVCABLE y Claro (privadas) y cuatro operadoras de telefonía celular, Movistar, Claro y Tuenti (privadas) y CNT (pública).

En Ecuador existen dos bandas para la recepción de imágenes, la UHF y la VHF, y según donde se ubique la recepción de imagen, se sintoniza el canal, ya que en un principio el Estado no dispuso de un control de franquicia para comprar el derecho que otorgue la privacidad y que otro tipo de recepción no ocupe su espacio en cualquier otra ciudad o provincia del país.

Actualmente existen varios medios para la dotación del servicio televisivo, sea cobertura por antenas analógicas de aire de carácter abierto y gratuito, así como también las de pago mediante proveedores de televisión por cable o por antenas satelitales, tanto públicas como privadas.

La mayoría de ciudades y poblaciones disponen de al menos un canal comunitario que puede ser de frecuencia VHF (canales primarios) o UHF (canales secundarios). Algunos de sintonía nacional, entre los canales que poseen señal de sintonía internacional están: Ecuavisa - "Posee señal internacional", RedTeleSistema (RTS), Teleamazonas, que posee señal en Estados Unidos a través de los canales 839 y 842 del Dish Pack Latino, Ecuador TV Televisión Pública, TC Televisión, Gama TV, Canal Uno -, "Posee señal internacional", Telerama, RTU, Radio y Televisión Unidas, Latele, Televicentro, CiudadColorada - "Portal internacional" Teleamazonas HD, Ecuavisa HD, Oromar HD, TC HD y Canal Uno HD son canales nacionales de Ecuador que están en full HD.

El Ecuador, debido a la modernización de la época, estableció para la implementación de la televisión y radio digitales, la norma ISDB-Tb, con fecha 26 de marzo de 2010. El anuncio lo hizo el Superintendente de Telecomunicaciones, Fabián Jaramillo. Así, Ecuador se convierte en el sexto país en adoptar el standart ISDB-Tb. Siendo el estándar más optado en la región latinoamericana.

El apagón analógico se estableció para el 31 de diciembre de 2018 a nivel nacional, el cual se llevará en tres etapas, el primer apagón para las ciudades con más de 500 mil habitantes para el 31 de diciembre de 2016, el segundo para las ciudades con más de 200 mil habitantes para el 31 de diciembre de 2017 y el tercero a nivel nacional para el 31 de diciembre de 2018.

Los diarios de mayor circulación nacional son: Diario Extra (Guayaquil), El Telégrafo (Guayaquil), El Universo (Guayaquil), El Comercio (Quito), La Hora (resto del país). Muchas de las capitales de provincias disponen de 2 a 3 periódicos locales.

A continuación una lista de medios escritos:

El Ministerio del Deporte de Ecuador es el organismo rector de la actividad física y el deporte en el país.

La chaza es el deporte nacional de Ecuador, sin embargo, el deporte más popular en Ecuador es el fútbol. El campeonato nacional de este deporte consta de 2 series profesionales la serie A y la serie B, en la serie A participan 16 equipos en un torneo cuya modalidad se modifica año a año.

El mayor representante del atletismo ecuatoriano es Jefferson Pérez, quien ha obtenido tres medallas de oro a nivel mundial en la modalidad de marcha atlética (París en 2003, Helsinki en 2005 y Osaka en 2007), además de una medalla de oro en los Juegos Olímpicos de Atlanta 1996 y una de plata en los Juegos Olímpicos de Pekín 2008. Por otra parte, Rolando Vera Rodas es el único fondista no brasileño que ha ganado cuatro veces consecutivas (1986-1989) la Carrera Internacional de San Silvestre en Brasil. Glenda Morejón se constituye como una promesa del deporte de marcha atlética ya que obtuvo la medalla de oro en el Mundial Sub 18 de Atletismo que se desarrolló en Nairobi, Kenia en el año 2017.

Liga de Quito, el equipo con mayores logros internacionalmente, tiene 13 títulos nacionales (11 de Serie A, una Copa Ecuador, edición de 2019 y una Supercopa Ecuador, edición de 2020) y 4 títulos internacionales, ha conseguido Copa Libertadores (edición de 2008), dos Recopa Sudamericana (edición de 2009, y edición de 2010) y una Copa Sudamericana (edición de 2009), en el 2008 fue subcampeón del mundial de clubes, y desde el año 2008 es el equipo ecuatoriano mejor ubicado según el Ranking de la CONMEBOL.

Barcelona Sporting Club es el club con más títulos nacionales, todos de Serie A, teniendo 15, hasta 2016. Fue el líder de Ecuador en el Ranking de equipos Conmebol por país, también ha sido subcampeón de la Copa Libertadores en dos ocasiones (edición 1990 y edición 1998), siendo el primer equipo ecuatoriano en haber llegado a una final de la Copa Libertadores, además nunca ha descendido. Barcelona posee un estadio, originalmente llamado "Estadio Monumental Isidro Romero Carbo". Reglamentariamente de más de 60 000 espectadores, siendo uno de los estadios más grandes de América y del mundo.

El Club Sport Emelec, apodado "Ballet Azul", "Eléctricos" o "Millonarios", fue el primer campeón nacional y además es el único equipo del país campeón en todas las décadas que se han disputado campeonatos de Serie A, siendo campeón 14 ocasiones hasta 2017, consiguiendo su último título en este mismo año (2017) y además es el primer tricampeón en representación del Guayas (en 2013, 2014 y 2015, respectivamente), convirtiéndolo en el segundo club con más títulos nacionales. En 2001 fue finalista de la Copa Merconorte y fue elegido del mundo según la IFFHS en junio de 2010.
El Club Deportivo El Nacional es el tercer club con más títulos de campeón nacional de Serie A, teniendo 13. Se caracteriza porqué su plantilla de jugadores la componen únicamente pertenecen deportistas de nacionalidad ecuatoriana. Ha sido tricampeón y lo ha hecho dos ocasiones, en 1976, 1977, 1978 y en 1982, 1983, 1984.

Hay otros clubes tradicionales, como América de Quito que fue el primer club de Ecuador campeón de un torneo internacional al ganar la Copa Ganadores de Copa en 1971. Deportivo Quito ha sido campeón de Ecuador 5 veces, Deportivo Cuenca, Everest, Olmedo y Delfín lo han sido una vez.

En cuanto a la selección nacional, esta se clasificó para tres Mundiales, el de Japón-Corea 2002, Alemania 2006 llegando a octavos de final en este donde cayó eliminada por por 1 a 0, quedando en el puesto 12 entre 32 selecciones; y la Copa de Brasil 2014 siendo la última a la que se clasificó hasta la actualidad. Un ecuatoriano destacado en este deporte es Alberto Spencer, máximo goleador de todos los tiempos de la Copa Libertadores con 54 goles en su cuenta. Por otro lado Iván Hurtado es el jugador con más partidos disputados en la Selección de fútbol en 167 ocasiones y Agustín Delgado es el máximo goleador con 31 tantos y en Mundiales con 3 anotaciones (1 a México en 2002, 1 a Polonia y Costa Rica en 2006). Cabe también acotar que por esta selección se han destacado otros jugadores como Édison Méndez, Álex Aguinaga, Ulises de la Cruz, Antonio Valencia, Christian Benítez y Jaime Iván Kaviedes quien en el año de 1998 fue máximo goleador del mundo, marcando 43 goles en 39 partidos, siendo además el jugador con más cantidad de goles en una temporada de la denominada Serie A de Ecuador que responde al 'Campeonato Nacional de Primera División'. En abril de 2013 la selección ecuatoriana de fútbol se ubicó en el top 10 del ranking mundial de selecciones de fútbol del mundo, ubicada en el puesto 10.

Lo más destacado de los últimos años de la selección ecuatoriana (a nivel de categorías inferiores, Sub-20) ha sido el título del Campeonato Sudamericano Sub-20 en Chile, y el tercer lugar en el Mundial de la misma categoría en Polonia, ambos torneos llevados a cabo en 2019.

En cuanto al tenis, Pancho Segura fue considerado en 1952 como el Jugador número Uno del mundo por la Professional Lawn Tennis Association. También en esta disciplina, Andrés Gómez ganó el Torneo de Roland Garros en 1990 y el Masters de Roma en y ; el tenista Nicolás Lapentti fue sexto en el mundo.

El ciclista Richard Carapaz se convirtió en el primer ecuatoriano en ganar una Gran Vuelta en el Giro de Italia 2019.

El Ministerio de información y Turismo fue creado el 10 de agosto de 1992, al inicio del gobierno de Sixto Durán Ballén, quien visualizó al turismo como una actividad fundamental para el desarrollo económico y social de los pueblos. Frente al crecimiento del sector turístico, en junio de 1994, se tomó la decisión de separar al turismo de la información, para que se dedique exclusivamente a impulsar y fortalecer esta actividad.

Ecuador es un país con una vasta riqueza natural. La diversidad de sus cuatro regiones ha dado lugar a miles de especies de flora y fauna. Cuenta con alrededor de 1 640 clases de pájaros. Las especies de mariposas bordean las 4 500, los reptiles 345, los anfibios 358 y los mamíferos 258, entre otras. No en vano el Ecuador está considerado como uno de los 17 países donde está concentrada la mayor biodiversidad del planeta, siendo además el mayor país con diversidad por kilómetro cuadrado del mundo. La mayor parte de su fauna y flora vive en 26 áreas protegidas por el Estado. Asimismo, posee una amplia gama de culturas. El gobierno de Rafael Correa impulsó la marca turística "Ecuador Ama la Vida" con la que se vendería la promoción turística de la nación. Enfocada en considerarla como un país amable y respetuoso con la naturaleza, la biodiversidad natural y la diversidad cultural de los pueblos. Y para ello se desarrollan medios de explotarlas junto con la economía privada.

El país posee dos ciudades patrimonio cultural de la humanidad: Quito y Cuenca, así como dos patrimonios naturales de la humanidad: las islas Galápagos y el parque nacional Sangay además de una reserva mundial de la biosfera, como es el macizo del Cajas. En lo cultural, es reconocido el sombrero de paja toquilla y la cultura del pueblo indígena zapara. Los sitios más concurridos por turistas nacionales y extranjeros tienen distintos matices debido a las diversas actividades turísticas que ofrece el país.

Entre los principales destinos turísticos se destacan:

A pesar de ser un país pequeño, la gastronomía de Ecuador es bastante variada, debido a la existencia en el país de cuatro regiones naturales diferenciadas –costa, sierra, oriente y región insular– con costumbres y tradiciones diferentes. Los distintos platos típicos y los ingredientes principales varían en función de estas condiciones naturales.

El pescado que suele comerse en la costa ecuatoriana es conseguido de las aguas del Océano Pacífico o de los innumerables ríos navegables de la zona. Entre los principales pescados se encuentran el picudo, albacora, dorado, camotillo, chame, corvina y la trucha. Algunos de los platos populares con pescado son: sopa marinera, ceviches de pescado, corviches, bollos, cazuelas, estofado de pescado con maní, encocado, etc.
Un plato típico de la costa se llama Encebollado de pescado, su consumo se ha extendido a lo largo de todo el país, convirtiéndolo en uno de los platillos más populares del Ecuador.

Ecuador es un principal país exportador de plátano, por lo que este representa un importante elemento en la gastronomía, en especial en la costa ecuatoriana. Existen tres principales variedades de plátano, siendo las tres más importantes: el plátano verde, el plátano maduro y el guineo. Los plátanos verdes y maduros deben cocinarse antes de ser ingeridos. El plátano verde (simplemente llamado 'verde') suele comerse frito, asado o hervido. El plátano maduro (o simplemente, 'maduro') suele comerse frito, asado o hervido de igual manera, y tiene un sabor más dulce y una consistencia más suave. El "guineo", es el nombre típico de la banana ecuatoriana; suele comerse crudo como una fruta cualquiera, aunque también hay una plétora de postres preparados a base del guineo. Existe, además, un tipo de guineo en miniatura, que se conoce con el nombre de 'orito'.

Las verduras están presentes en diferentes formas, el arroz, el plátano verde o maduro, la yuca, o la salsa de maní (cacahuate) tostado y molido. El maíz se suele comer en las muy populares tortillas de maíz conocidas como bonitísimas, cocinan los choclos (elotes) en agua y sal, las mazamorras y los comen con queso fresco. Igual los frijoles, que acompañan a muchos de sus platos. El Puré de papas o lo sirven de base para platos como los llapingachos que son tortillas de papa o los locros.

Se suele comer carne de vaca, cerdo, cordero, cabra, pato, pavo y pollo. Algunos platos se combinan con verduras como el seco de chivo o el seco de gallina que son trozos de carne, tipo estofado, servida con arroz. Dentro de los platos exóticos se tiene el cuy, que suele comerse asado en las celebraciones de ciertas partes andinas del país. La carne de chancho ("cerdo, lechón, cachorro"), que se come en varios lugares del Ecuador, participa en la elaboración de diversos platos, algunos de ellos como fritada, hornado, chugchucaras.

Es de destacar de la cocina de Ecuador, los caldos (conocidos como "sopas" o "locros") que suelen prepararse con verduras muy diversas y carne de gallina, son frecuentemente servidos en los mercados callejeros como desayuno. Algunos de ellos son muy populares como el yaguarlocro que es una sopa de papas que lleva como ingrediente borrego y una salsa.

Aunque la mayoría de las veces se ingiere fruta como postre la cocina ecuatoriana tiene postres como:
Los aguardientes de caña son muy populares en las provincias de Loja, Manabí y Tungurahua, donde se elaboran diversas bebidas alcohólicas como el llamado currincho, el tardón, el guarapo, el quemado, el norteño, o canelazo que es a base de canela y la famosa Caña. Ciertas bebidas de estas provincias llevan aderezos de frutos y productos nativos como la miel y la grosella y muchos cítricos como la naranja, mandarina, limón e injertos de estas frutas con otras, incluso se mezclan con agua de flores nativas y hojas de otros frutos medicinales y exóticos como la hierba luisa o la manzanilla. Otra bebida es el pájaro azul, que es aguardiente de caña de azúcar mezclado con las cáscara de la mandarina, que al hervir le da ese color azulado. En especial la chicha que es un preparado a base de yuca, mote o maíz.




</doc>
<doc id="3313" url="https://es.wikipedia.org/wiki?curid=3313" title="Islas Galápagos">
Islas Galápagos

Las islas Galápagos (también islas de los Galápagos y oficialmente archipiélago de Colón o archipiélago de Galápagos) constituyen un archipiélago del océano Pacífico ubicado a 1000  km de la costa de Ecuador. Está conformado por trece islas grandes con una superficie mayor a 10 km², nueve islas medianas con una superficie de 1 km² a 10 km² y otros 107 islotes de tamaño pequeño, además de promontorios rocosos de pocos metros cuadrados, distribuidos alrededor de la línea ecuatorial, que conjuntamente con el Archipiélago Malayo, son los únicos archipiélagos del planeta, junto a las Islas Maldivas, que tienen tierras tanto en el hemisferio norte como en el hemisferio sur.

Las islas Galápagos es la segunda reserva marina más grande del planeta fueron declaradas Patrimonio de la Humanidad en 1978 por la Unesco. El archipiélago tiene como mayor fuente de ingresos el turismo y recibe turistas al año. También se ha desarrollado el turismo ecológico con el fin de preservar las especies. La región fue el hábitat del Solitario George, el último espécimen de la especie tortuga gigante de Pinta, extinta el 24 de junio del 2012. Las islas también son hábitat de especies como tortugas marinas, delfines, tiburones, tiburones martillo, ballenas, arrecifes de coral, fragatas, iguanas, lagartos, cormoranes, albatros, leones marinos y pingüinos. Al igual que la masa continental de Ecuador, el archipiélago es atravesado por la línea ecuatorial, en su mayor parte por el norte de la isla Isabela. Galápagos es el segundo archipiélago con mayor actividad volcánica del planeta, superado únicamente por Hawái. Entra en la categoría de los puntos calientes; los volcanes más activos son Cerro Azul, Sierra Negra, Marchena y volcán La Cumbre en la Isla Fernandina, que es el más activo del archipiélago y uno de los más activos del mundo.

Las Galápagos son conocidas por sus numerosas especies endémicas y por los estudios de Charles Darwin que le llevaron a establecer su teoría de la evolución por la selección natural. Son llamadas, turísticamente, las Islas Encantadas, denominación que se ganó el archipiélago en el siglo XVI por su grandiosa biodiversidad de flora y fauna, heredando el nombre por generaciones.

Se estima que la formación de la primera isla tuvo lugar hace más de cinco millones de años, como resultado de la actividad tectónica. Las islas más recientes, llamadas Isabela y Fernandina, están todavía en proceso de formación, habiéndose registrado la erupción volcánica más reciente en 2009.

Administrativamente, Galápagos constituye una de las provincias de Ecuador, conformada por tres cantones que llevan los nombres de sus islas más pobladas, a saber: San Cristóbal, Santa Cruz e Isabela. El 12 de febrero de 1832, bajo la presidencia de Juan José Flores, las islas Galápagos fueron anexadas a Ecuador. Desde el 18 de febrero de 1973 constituyen una provincia de este país.

Las islas se formaron hace 5 millones de años como resultado de actividad tectónica en el fondo marino. Esta isla es muy joven. La actividad volcánica actual aún sigue expandiendo el archipiélago. El archipiélago es uno de los grupos volcánicos más activos del mundo. Muchas de las islas son solamente las puntas de algunos volcanes y muestran un avanzado estado de erosión. Islas como Baltra y North Seymour emergieron del océano por una gran actividad tectónica.

Un estudio realizado en el año 1952 por los historiadores Thor Heyerdahl y Arne Skjolsvold reveló que se encontraron cerámicas de algunos pueblos (posiblemente incas) de antes de la llegada de los españoles. Sin embargo, no se han encontrado tumbas, vasijas ni ninguna construcción antigua que revele asentamientos antes de la colonización.

Las islas Galápagos fueron descubiertas por casualidad el 10 de marzo de 1535, cuando el barco del obispo de Panamá fray Tomás de Berlanga se desvió de su destino a Perú, donde cumpliría un encargo del rey español Carlos V para arbitrar en una disputa entre Francisco Pizarro y sus subordinados tras la conquista del imperio incaico.

Los primeros mapas en incluir las islas fueron realizados por los cartógrafos Abraham Ortelius y Mercator alrededor de 1570. Las islas estaban descritas como "Insulae de los Galopegos" (Islas de las Tortugas).

Las Galápagos fueron utilizadas por piratas ingleses como escondite en sus viajes de pillaje a los galeones españoles que llevaban oro y plata de América hacia España. El primer pirata registrado que visitó las islas fue el inglés Richard Hawkins, en 1593. Desde entonces y hasta 1816 muchos piratas llegaron al archipiélago.

Recién descubiertas las islas se encontraban deshabitadas y los barcos que pasaban junto a su ubicación coincidían cuando el archipiélago era tapado por la niebla. Diversos acontecimientos las llevaron a ser conocidas como las islas Encantadas e incluso algunos navegantes españoles afirmaban que no existían y solo eran espejismos.

La primera misión científica que visitó las Islas Galápagos fue la expedición Malaspina, una expedición española dirigida por Alejandro Malaspina que llegó en 1790. Sin embargo, los registros de la expedición nunca fueron publicados.

En el siglo XVII se empieza a poblar la zona cuando el navegante James Colnett describe al lugar como unas islas ricas en flora y fauna. Esto atrajo a los primeros colonos, en su mayoría ingleses, con interés por las ballenas, cachalotes, leones marinos y principalmente por los galápagos. El descubrimiento de la grasa de los cachalotes también atrajo a muchos balleneros lo que condujo a que se creara una oficina de correos improvisada, donde los barcos dejaban y recogían cartas. Colnett también dibujó las primeras cartas de navegación de las Galápagos.

En octubre de 1831 José de Villamil envió una comisión exploradora al archipiélago de las Galápagos con el fin de averiguar sobre la existencia de orchilla, planta utilizada en tinturar los tejidos y que se exportaba a México. El 14 de noviembre se constituyó la "Sociedad Colonizadora del Archipiélago de las Galápagos" y denunció como terrenos baldíos a la isla Charles, después denominada Floreana.

El 20 de enero de 1832 salió una expedición a las Galápagos al mando del Coronel Ignacio Hernández y Ecuador las anexó el 12 de febrero de 1832 bajo el gobierno del General Juan José Flores, bautizándolas como archipiélago de Colón.

A bordo de la nave Beagle la expedición británica al mando del capitán Robert FitzRoy llegó a Galápagos el 15 de septiembre de 1835 para realizar trabajos de sondeos y cartografía, dentro de una lista de lugares aislados de Europa, como Valparaíso (Chile), Callao, islas Galápagos, Tahití, Nueva Zelanda, Australia, Cabo Buena Esperanza. La nave regresó a Falmouth el 2 de octubre de 1836. El capitán y otros a bordo, incluyendo el joven naturalista Charles Darwin, realizaron un estudio científico de la geología y biología en cuatro de las islas, antes de continuar su expedición alrededor del mundo. El barco recorrió el archipiélago durante cinco semanas, pero Darwin estuvo en tierra solo dos semanas. Investigó a los animales y plantas propios de la región. Los estudios de este viaje permitieron a Darwin formular la teoría del origen de las especies.

La Unesco declaró a las islas Galápagos como Patrimonio Natural de la Humanidad en 1979 y, seis años más tarde como Reserva de la Biosfera (1985). En el 2007 la Unesco declaró a las islas Galápagos como Patrimonio de la Humanidad en riesgo medioambiental y estuvo incluida en la Lista del Patrimonio de la Humanidad en peligro hasta 2010.

El archipiélago se conoce por una variedad de nombres; en Ecuador comúnmente se conocen por sus nombres en español, que además son los oficiales, usando los antiguos nombres en inglés solo con fines históricos. El nombre oficial de las islas es "Archipiélago de Colón", mientras que administrativamente se conoce al territorio como "Provincia de Galápagos". La denominación más conocida y común es "Islas Galápagos". La primera carta de navegación de las islas, aunque rústica, fue realizada por el bucanero Ambrose Cowley en 1684, y en dicha carta bautizó las islas con los nombres de algunos de sus amigos piratas y de algunos nobles ingleses que apoyaban la causa de los corsarios.

Las siguientes son las islas de más de un kilómetro cuadrado de superficie:

Los siguientes son los islotes con entre una y cien hectáreas de superficie. Existen multitud de otros islotes, rocas y promontorios aún más pequeños:

Llamada así en honor a la Reina Isabel I de Castilla que patrocinó el viaje de Colón (su nombre en inglés honra al Duque de Albemarle). Es la mayor isla del archipiélago, con una superficie de km² y ocupa el 58% de la zona terrestre de las islas. La forma de la isla se debe a la fusión de cinco grandes volcanes (Cerro Azul, Sierra Negra, Salcedo, Darwin y Wolf) en una sola masa. Tiene una población de aproximadamente de habitantes. El punto más alto es el volcán Wolf, que alcanza metros de altitud. En esta isla se pueden observar pingüinos, iguanas marinas, cormoranes no voladores, piqueros de patas azules, pelícanos, así como abundantes zayapas y tintoreras. En las faldas y calderas de los seis volcanes de Isabela, se pueden observar tortugas gigantes e iguanas terrestres, así como pinzones, palomas, halcones, murciélagos(Lasiurus blossevillii y Aeorestes cinereus) 
y una interesante vegetación. El tercer mayor asentamiento humano del archipiélago y su mayor puerto, conocido como Puerto Villamil o Albemarle, está ubicado en el extremo sur de la isla.
Esta isla es muy joven. La actividad volcánica actual aún sigue expandiendo el archipiélago. El archipiélago es uno de los grupos volcánicos más activos del mundo. Muchas de las islas son solamente las puntas de algunos volcanes y muestran un avanzado estado de erosión. Islas como Baltra y North Seymour emergieron del océano por una gran actividad tectónica.

Llamada así en honor a la Cruz de Cristo (su nombre en inglés se debe al barco militar HMS Indefatigable). Tiene una superficie de 986 km² y una altitud máxima de 864 metros. En Santa Cruz está localizado el mayor asentamiento humano del archipiélago, en el poblado de Puerto Ayora. La Estación Científica Charles Darwin y las oficinas centrales del Servicio del parque nacional Galápagos están ubicadas aquí. En el SPNG opera un centro de crianza de tortugas donde estos quelonios son preparados para su reintroducción en su hábitat natural. La "parte alta" de Santa Cruz tiene una exuberante vegetación y es conocida por los tubos lávicos. Una gran población de tortugas habita esta región. Caleta Tortuga Negra es un área rodeada de manglar donde tortugas marinas, rayas y pequeños tiburones la utilizan como lugar de apareamiento. Cerro Dragón, conocido por su laguna de flamencos y sus iguanas terrestres, también se encuentra en esta isla. Tiene zonas para practicar el buceo y el surf.

Bahía Tortuga está situada en la isla de isla Santa Cruz. Puerto Ayora esta alrededor de 20 minutos a pie. Hay un pequeño camino de metros de largo y se debe iniciar y cerrar sesión en la oficina del parque nacional Galápagos.

Llamada así en honor al Rey Fernando el Católico, quien patrocinó el viaje de Colón (su nombre en inglés homenajea a Sir John Narborough). Fernandina tiene una superficie de 642 km² y una altura máxima de  metros. Es la más reciente y más occidental de las islas del archipiélago. El 11 de abril de 2009 se inició un nuevo proceso eruptivo que formó una nube de ceniza y vapor de agua con flujos piroclásticos que descendieron por las laderas del volcán, hasta llegar al mar. Punta Espinoza es una estrecha franja de tierra donde se reúnen centenares de iguanas marinas en grandes grupos. En esta isla habita el cormorán no volador, así como pingüinos y lobos peleteros. También se encuentran áreas de manglar. Diversos tipos de flujos de lava pueden observarse aquí.

Llamada así en honor al santo patrón de España, es conocida también como San Salvador en honor a la primera isla del Caribe descubierta por Colón. Tiene una superficie de 585 km² y una altura máxima de 907 metros. Aquí se encuentran iguanas marinas, lobos peleteros, leones marinos, tortugas terrestres y marinas, delfines y tiburones. Una gran cantidad de animales domésticos, que fueron introducidos con la llegada de pobladores a la isla, han causado un gran daño a la flora y fauna endémicas. En esta isla se observan con frecuencia pinzones de Darwin y halcones de Galápagos. En la bahía Sullivan existe un flujo reciente de lava pahoehoe.

Llamada así en honor a Cristóbal mártir (su nombre en inglés es en memoria del Conde de Chatham). Es la capital de la provincia y tiene una superficie de 558 km² y una altura máxima de 730 metros. En la mitad sur de la isla, dentro de un cráter en la sierra de San Cristóbal, se encuentra la laguna El Junco, que es el mayor lago de agua dulce del archipiélago. Alberga una gran población de aves y cerca de allí está La Galapaguera, una estación de refugio y cría de tortugas gigantes. Cerca de la ciudad de Puerto Baquerizo Moreno, está el Cerro Tijeretas, una colonia de anidación para fragatas, y a unos diez minutos en autobús, se encuentra La Lobería, una colonia de lobos marinos. En la parte alta de la isla se encuentra la Estación Biológica San Cristóbal, dedicada a la conservación de los bosques del Ecuador. También hay excursiones en barco a sitios cercanos de buceo. "León Dormido" representa los restos de un cono de lava, ahora dividida en dos. "Isla Lobos" es un lugar de anidación de piqueros de patas azules.

Llamada así en honor al primer presidente del Ecuador, Juan José Flores, en cuya administración se tomó posesión del archipiélago (su nombre en inglés es el del rey Carlos II de Inglaterra). También se la conoce como Santa María en honor a una de las carabelas de Colón. Tiene una superficie de 173 km² y una altitud máxima de 640 metros. Entre diciembre y mayo, flamencos rosados y tortugas marinas anidan en esta isla. Aquí se puede encontrar una pequeña población de pingüinos de Galápagos y el endémico sinsonte de Floreana. Se pueden observar interesantes formaciones de coral en la denominada "Corona del Diablo", que es un cono volcánico sumergido.

Llamada así en honor de fray Antonio de Marchena. Tiene una superficie de 130 km² y una altitud máxima de  metros. Aunque no hay sitios para visitar en esta isla, es posible bucear en las aguas alrededor. Posee gran variedad de flora y fauna como los flamencos y leones marinos. También se pueden observar las tortugas gigantes. Marchena tiene una caldera volcánica con forma elíptica de aproximadamente 7 km de largo por 6 km de ancho, clasificada como grande dentro de la gama de tamaños de las calderas.

Llamada así por la primera ciudad de América (su nombre en inglés honra al vizconde Samuel Hood). Con sus 60 km² es una de las islas menores que conforman el archipiélago de las Galápagos, y es la más antigua de todas, ya que cuenta con alrededor de  millones de años de existencia. Aunque está deshabitada, en ella viven varias especies animales de interés, como el endémico sinsonte de Española, el piquero de patas azules, la tórtola de las Galápagos, la gaviota de cola bifurcada, la iguana marina y la lagartija de lava. Entre los visitantes, son especialmente populares la Bahía Gardner, que tiene una playa reconocida por su belleza, y Punta Suárez, de interés por el avistamiento de aves.

Llamada así en honor a una de las carabelas de Colón (su nombre en inglés está dedicado al Conde de Abingdon). Es la isla más septentrional de las Galápagos y la novena más grande del archipiélago. Tiene una superficie de 60 km² y una altitud máxima de 780 metros. Aquí se pueden observar gaviotas de cola bifurcada, iguanas marinas, lobos peleteros y gavilanes de las Galápagos. De esta isla era originaria la famosa tortuga, el «Solitario George», último ejemplar conocido de la especie "Chelonoidis abingdonii". También se encuentra aquí uno de los volcanes más activos.

Se desconoce el origen de su nombre, en inglés debe su nombre al marino británico lord Hugh Seymour. Tiene una superficie de 27 km² y una altitud máxima de 100 metros. Alberga el principal aeropuerto del archipiélago, que fue construido durante la Segunda Guerra Mundial por la Marina de los Estados Unidos para «patrullar» el Canal de Panamá. En esta isla se reintrodujeron iguanas terrestres después de que esta especie nativa fuera totalmente eliminada por los soldados de Estados Unidos aquí acantonados. A lo largo de la isla aún se encuentran los vestigios de los cuarteles de los soldados. Algunos de ellos después de haberse retirado regresan en calidad de turistas. En la isla hay mucha flora silvestre desértica, mayormente poblada de cactus. Desde el aeropuerto al cual llegan los aviones desde el Ecuador continental, cada 10 minutos salen buses con el costo de 5 dólares hacia el canal y el puerto. Existen decenas de puntos para practicar surf, snorkel o buceo, con el permiso previo de la Armada del Ecuador. Existe un segundo aeropuerto fuera de servicio que también data de la Segunda Guerra Mundial. Entre la Isla Baltra y la Isla Santa Cruz, se encuentra el Canal de Itabaca, utilizado por taxis acuáticos que llevan a las personas entre las islas. Los barcos operan fuera de la costa para llevar a la gente a otras islas de las Galápagos.

Llamada así en honor a las Capitulaciones de Santa Fe, en las que se otorgó a Cristóbal Colón los títulos de Almirante Mayor de la Mar Océana, Virrey y Gobernador General de las tierras que descubriera (su nombre en inglés es en honor al Almirante Samuel Barrington). Tiene una superficie de 24 km² y una altitud máxima de 259 metros. Santa Fe tiene un bosque de cactus "Opuntia" que son los más grandes del archipiélago, y de palo santo. Tiene una vistosa laguna color turquesa y aguas tranquilas donde se puede realizar snorkel con leones marinos. Sus precipicios costeros son el hogar de gaviotas de cola bifurcada, petreles y otros pájaros tropicales. La iguana terrestre de Santa Fe, endémica de la isla, habita en gran número al igual que la lagartija de lava y uas poblaciones sobrevivientes de ratón costero(uno de los pocos mamíferos terrestres de la islas galápagos).

Llamada así en honor a los hermanos Pinzón, capitanes de las carabelas la Pinta y la Niña en la primera expedición de Cristóbal Colón (su nombre en inglés recuerda al Vizconde de Duncan). Tiene una superficie de 18 km² y una altitud máxima de 458 metros. No tiene sitios para visitar y se requiere de un permiso especial de las autoridades para ingresar. Las principales especies forestales están en la isla, en la zona húmeda, se encuentra una especie única del llamado árbol margarita. Pinzón es el hogar de lobos marinos, tortugas gigantes, iguanas marinas y delfines, además de otras especies endémicas.

Llamada así en honor a la ciudad de Génova (Italia), el probable lugar de nacimiento de Colón. Tiene una superficie de 14 km² y una altitud máxima de 76 metros. La isla con forma de herradura tiene una caldera volcánica cuya pared se ha derrumbado, formando la Gran Bahía Darwin, rodeada de acantilados. El lago Arcturus, lleno de agua salada, se encuentra en el centro, y los sedimentos dentro de este lago del cráter tienen menos de 6.000 años de antigüedad. Aunque no hay erupciones históricas se conocen de Genovesa, hay flujos de lava muy jóvenes en los flancos del volcán. Es conocida como "Isla de los Pájaros", a causa de las grandes y variadas colonias de aves que anidan aquí. Hay una gran cantidad de fragatas, gaviotas de cola bifurcada, gaviotas de lava, petreles, pájaros tropicales, pinzones de Darwin y sinsontes de las Galápagos. El sitio denominado "El Barranco" constituye una magnífica meseta para observación de estas aves, especialmente de piqueros enmascarados y de patas rojas. También hay un gran bosque de palo santo.

Llamado así por el Monasterio de La Rábida donde Colón dejó a su hijo durante su viaje de descubrimiento a América (su nombre en inglés se debe al Almirante Jervis). Tiene una superficie de  km² y una altitud máxima de 367 metros. El alto contenido de hierro de la lava de Rábida ocasiona que la isla tenga un característico color rojizo. El paisaje está lleno de pequeños cráteres volcánicos a lo largo de las laderas y acantilados afilados. Ocasionalmente se pueden observar flamencos y lobos marinos en una laguna de agua salada cerca de la playa, donde pelícanos y piqueros construyen sus nidos. Se han registrado nueve especies de pinzones en esta isla. La rica fauna atrae a un sinnúmero de turistas de cruceros.

Llamada así en honor del noble inglés Lord Hugh Seymour. Tiene una superficie de  km² y una altitud máxima de 28 metros. Toda la isla está cubierta de vegetación baja y tupida, y tiene una pista para visitantes de aproximadamente 2 km de longitud que cruza la vía de la isla y permite explorar la costa rocosa. En esta isla se halla una gran población de piqueros de patas azules y gaviotas de cola bifurcada. También hay un sinnúmero de iguanas terrestres, que fueron introducidas de la isla Baltra, y que sirvieron para repoblar con esta especie la isla. Asimismo es posible observar una gran cantidad de fragatas y leones marinos con sus crías.

Llamada así en honor al geólogo alemán Theodor Wolf. Tiene una superficie de  km² y una altura máxima de 100 metros. Se encuentra alejada del grupo principal de islas y no tiene población permanente, por lo que no es accesible para visitar en tierra, no obstante, es un lugar popular para bucear. Anteriormente era conocida como isla Wenman. Aquí habitan focas peleteras, iguanas marinas y tortugas verdes. Entre las aves que se encuentran en esta isla están la fragata, el piquero de patas rojas y el pinzón vampiro. La vida marina de la isla incluye tiburones martillo, tiburones de Galápagos y ocasionalmente tiburones ballena, así como también delfines, mantarrayas y otros peces pelágicos.

Isla Tortuga se encuentra a 2 km al sur de la Isla Isabela. Tiene una superficie de  km² y una altura máxima de 100 metros. Esta isla es una antigua caldera volcánica, de la cual sólo la mitad permanece fuera del agua. El sitio de buceo está en el lado noreste de la isla, y desciende gradualmente fuera de la vista en las profundidades del Pacífico. Entre los 20 y 30 metros, se pueden observar a tiburones martillo, tiburones de Galápagos y rayas águila. También tiburones punta negra de arrecife frecuentan la zona, dando vueltas atrás y adelante, solos o en pareja. Es uno de los principales sitios de anidación de aves marinas de Galápagos. La posibilidad de avistamientos de tortugas marinas, mantarrayas y leones marinos, es también un incentivo para hacer turismo en esta isla.

Llamada así en honor a Sir Bartholomew Sulivan de la Marina Británica. Tiene una superficie de  km² y una altitud máxima de 114 metros. Esta isla ofrece algunos de los paisajes más bellos del archipiélago. La isla se compone de un volcán extinto y una variedad de formaciones volcánicas negras rojas, anaranjadas, verdes y brillantes. Los cactus de lava de Galápagos colonizan los nuevos campos de lava. En esta isla se encuentra el afamado Pináculo, que es uno de los lugares más representativos del archipiélago. Aquí se puede bucear y hacer snorkel con los pingüinos, lobos marinos, tiburones punta blanca de arrecife y otros peces tropicales. Estacionalmente, Bartolomé es el sitio de apareamiento y anidación de la tortuga verde. La bahía es también un excelente lugar para ir a nadar. Las bahías gemelas están separadas por un estrecho istmo.

Llamada así en honor a Charles Darwin, quien hiciera famosas las islas a nivel mundial. Tiene una superficie de  km² y una altura máxima de 168 metros. Esta isla no está abierta para visitas en tierra, los únicos visitantes son los que vienen a bucear. La vida marina en Darwin es diversa, las aguas de la isla atraen tiburones ballena de junio a noviembre, así como también a tiburones martillo, tiburones de Galápagos, tiburones sedosos y tiburones punta negra. Además se pueden encontrar focas peleteras, lobos marinos, delfines y ballenas. En la isla existe una gran población de aves, que incluye fragatas, piqueros de patas rojas, gaviotas de cola bifurcada y el pinzón vampiro.

El archipiélago forma la Provincia de Galápagos, cuya capital es Puerto Baquerizo Moreno. Se conforma por tres cantones:


Las Galápagos fueron declaradas parque nacional en 1959, protegiendo así el   de la superficie terrestre del archipiélago. El área restante es ocupada por asentamientos humanos que ya existían al tiempo de la declaratoria. Para entonces, aproximadamente 1000 a 2000 personas vivían en cuatro islas. En 1972 un censo determinó que 3488 personas vivían en Galápagos, pero en la década de 1980 este número se había incrementado notablemente a más de 20 000 habitantes.

En 1986 el mar que rodea a las islas fue declarado reserva marina. Unesco incluyó a Galápagos en la lista de Patrimonio de la Humanidad en 1978, y en diciembre de 2001 se amplió esta declaración para la reserva marina.

En el año 2007, fueron incluidas en la Lista del Patrimonio de la Humanidad en peligro, debido al turismo masivo y las especies invasoras. El 29 de julio de 2010, las Islas Galápagos fueron retiradas de la lista de patrimonios en peligro de extinción por el Comité de Patrimonios de la Unesco.

El archipiélago tiene diferentes figuras internacionales que se han aplicado para tratar de garantizar la conservación de Galápagos; entre ellas: Reserva de Patrimonio Natural de la Humanidad, Sitio Ramsar, Santuario de Ballenas, reserva de Biósfera, etc.
La Estrategia Mundial para la Conservación de la Naturaleza identifica a Galápagos como una provinciaI Biogeográfica prioritaria para el establecimiento de áreas protegidas.
A nivel nacional las figuras de parque nacional y Reserva Marina, reflejan el compromiso asumido por el Gobierno Ecuatoriano de conservar este importante legado para las futuras generaciones de galapagueños, ecuatorianos y para la humanidad en general.

Las especies endémicas de singular importancia que habitan las islas incluyen:





Galápagos se ha convertido en uno de los ecosistemas marinos más populares del mundo y miles de turistas acuden año tras año a disfrutar de un paraíso natural que va aumentando su popularidad. En 2019, Galápagos registró 271.238 visitantes o turistas de los cuales el 67% eran extranjeros. Esto supuso una reducción de un 1,7% con respecto al 2018, aunque los datos de los últimos años arrojan una tasa de crecimiento compuesto entre 2010 y 2019 del 5,10%.

Los viajes de buceo se han convertido en uno de los reclamos de las islas y muchos medios lo catalogan como el mejor lugar para bucear del mundo. El turismo submarino ha crecido durante los últimos años y son muchos los buceadores que eligen las islas ecuatorianas para disfrutar de las decenas de especies marinas que albergan sus aguas.

Durante el 2020, el gobierno ecuatoriano denunciaba la presencia de barcos pesqueros de origen chino cerca de las aguas que bañan las islas. Una flota de 260 barcos rondan la frontera para capturar todo tipo de especies marinas, entre las que destacan los tiburones por su codiciada aleta.

Las Islas Galápagos han sido el escenario de varias obras en distintos géneros literarios, tanto de escritores ecuatorianos como extranjeros. Uno de los ejemplos más notorios es "Las encantadas", del estadounidense Herman Melville, libro que fue escrito tomando como base los viajes de Melville en las Galápagos y que describe en una serie de relatos los personajes históricos, la flora y la fauna de las islas. Otras obras de autores extranjeros cuyas tramas se sitúan en el archipiélago incluyen las novelas "La sed" (1938), del belga Georges Simenon, en que una familia se muda se muda a la Isla Floreana pero cuyas vidas cambian con la construcción de un hotel; y "Galápagos" (1985), del estadounidense Kurt Vonnegut, una obra de ciencia ficción situada un millón de años en el futuro en que un grupo de personas que llegan a las islas se convierten en los únicos humanos que escapan de un virus que produce infertilidad.

Entre autores ecuatorianos, una de las obras más destacadas es "Más allá de las islas" (1980), de la quiteña Alicia Yánez Cossío. En la novela, que mezcla ficción con hechos reales a través del realismo mágico, ocho personas viajan a las islas Galápagos escapando de la Muerte. Del lado de la poesía, es notorio el "Sollozo por Pedro Jara", de Efraín Jara Idrovo, mientras que en la literatura infantil, las Galápagos han sido exploradas por Edna Iturralde en su obra "Las islas donde nace la Luna". Otra novela ecuatoriana situada en el archipiélago es "Hallado en la grieta" (2012), del guayaquileño Jorge Velasco Mackenzie.




</doc>
<doc id="3323" url="https://es.wikipedia.org/wiki?curid=3323" title="Astronauta">
Astronauta

Astronauta, cosmonauta o taikonauta es el término que designa a todo el personal de un objeto espacial, a la tripulación de una nave espacial e incluso «a toda persona que se encuentre en la Luna».

A los viajeros espaciales de la Unión Soviética o entrenados allí y, actualmente, en Rusia, se les denomina normalmente cosmonautas —que proviene del término ruso "kosmonavt" (космонавт), que a su vez deriva de las palabras griegas "kosmos" (κοσμος, universo) y "nautes" (ναύτης, navegante)—. De manera similar, a aquellos de la República Popular China o entrenados ahí se les llama "taikonautas" —neologismo formado a partir del término chino 太空 ("tàikōng", espacio) y del griego ναύτης ("nautes", navegante) en semejanza con "astronauta" y "cosmonauta" que derivan del griego; la palabra oficial china que designa a un astronauta es 宇航員 (yǔhángyuán) pero el término taikonauta fue propuesto por Chiew Lee Yih en mayo de 1998 en Internet y se aceptó rápidamente en el mundo anglosajón—.

Actualmente diferentes agencias especiales buscan llevar astronautas a Marte.

La primera persona en salir al espacio en toda la Historia fue el cosmonauta Yuri Gagarin al ser lanzado el 12 de abril de 1961 a bordo de la nave Vostok 1. La primera mujer en volar al espacio fue Valentina Tereshkova, la cual salió al espacio el 16 de junio de 1963 a bordo de la Vostok 6. German Titov, cosmonauta soviético, fue el segundo hombre en órbita terrestre después de Gagarin.

En el marco del programa Intercosmos, también fueron al espacio cosmonautas del Bloque del Este y otros países aliados de la Unión Soviética, como Cuba. También Francia y la India, que no eran estados socialistas, participaron de Intercosmos.

Durante el programa Apolo, (1961-1975), los Estados Unidos enviaron un total de 30 misiones tripuladas: seis en el programa Mercury, 10 en el programa Gemini, 11 en el programa Apolo, tres en el programa Skylab y uno en el programa de pruebas Apolo-Soyuz. Estas 30 misiones proporcionaron 71 oportunidades de vuelo individual: seis en el Mercury, 20 en el Gemini, 33 en el Apolo, nueve en el Skylab, y tres en el Apolo-Soyuz. Estos puestos fueron cubiertos por 43 personas. De entre ellos, cuatro hicieron un total de cuatro vuelos, tres hicieron un total de tres vuelos, 10 un par de vuelos, y los restantes 26 volaron solo una vez. Algunos de ellos hicieron vuelos adicionales con el transbordador espacial.

De los 31 vuelos de la era Apolo, tres fueron suborbitales y nueve consistieron en misiones lunares. Los restantes 20 fueron vuelos orbitales terrestres. Los nueve vuelos lunares proporcionaron la oportunidad de realizar este tipo de vuelos a 24 personas. Sólo tres personas volaron dos veces a la Luna. Los 6 alunizajes que se produjeron con éxito llevaron a 12 personas a la Luna. Ninguno alunizó dos veces, aunque dos de ellos ya habían volado a la Luna al menos una vez, cinco de ellos habían hecho ya vuelos no lunares y cinco no tenían ningún tipo de experiencia en vuelos espaciales.

Todos los vuelos del programa Mercury y tres del programa Gemini tenían una tripulación formada por novatos, igual que uno de los vuelos del programa Skylab. Sin embargo todas las misiones del programa Apolo incluían al menos un astronauta veterano. Solo dos vuelos, las misiones lunares y las pruebas incluían una tripulación formada solamente por veteranos.

El primer grupo de astronautas estadounidenses se seleccionó en abril de 1959, para el programa Mercury de la NASA. Este grupo, que fue conocido como los «Mercury Seven» («los Siete del Mercury»), estaba compuesto por Scott Carpenter, Gordon Cooper, John Glenn, Gus Grissom, Wally Schirra, Alan Shepard y Deke Slayton. Todos eran pilotos de pruebas militares, un requisito que dictó el presidente Eisenhower para simplificar el proceso de selección.

Los siete miembros del primer grupo de astronautas fueron al espacio al final, aunque uno, Deke Slayton, no voló en una misión "Mercurio" por razones médicas. Finalmente, participaría en la misión Apolo-Soyuz. Cada uno de los otros seis viajaron al espacio en una misión Mercurio. Para dos de ellos, Scott Carpenter y John Glenn, la misión Mercurio fue su único vuelo en la Era Apolo Glenn, posteriormente, fue al espacio en la Lanzadera espacial. Tres de ellos, Gus Grissom, Gordon Cooper y Wally Schirra, también volaron en una misión durante el programa Gemini. Alan Shepard no voló en misiones Gemini debido a razones médicas, pero, más tarde, saldría al espacio en una misión Apolo. Fue el único astronauta del programa Mercurio que fue a la Luna. Wally Schirra también voló en el Apolo, además de en el Mercurio y en el Gemini, y fue el único astronauta que voló en los tres tipos de naves espaciales. Gus Grissom fue incluido en la tripulación del primer lanzamiento del Apolo, el Apolo 1, pero murió en un incendio en la plataforma de lanzamiento durante su entrenamiento.

La NASA seleccionó un segundo grupo de astronautas en septiembre de 1962. Este grupo estaba formado por Neil Armstrong, Frank Borman, Charles Conrad, Jim McDivitt, Jim Lovell, Elliott See, Tom Stafford, Ed White y John Young. Todos ellos participaron en misiones del programa Gemini excepto Elliott See, que murió en un accidente de vuelo mientras se preparaba para su viaje en el Gemini. Todos los demás volaron, también, en el Apolo, salvo Ed White, que murió en un incendio en la plataforma de lanzamiento durante su entrenamiento para el primer vuelo del Apolo. Tres de este grupo: McDivitt, Borman y Armstrong, realizaron un solo vuelo en el Gemini y en el Apolo. Cuatro de los otros: Young, Lovell, Stafford y Conrad, efectuaron dos vuelos cada uno en el Gemini y, al menos un vuelo en el Apolo. Young y Lovell volaron dos veces, cada uno, en el Apolo. Conrad y Stafford también realizaron segundos vuelos en la nave Apolo, Conrad en el Skylab y Stafford en la misión Apolo-Soyuz. Seis de este grupo: Borman, Lovell, Stafford, Young, Armstrong y Conrad, viajaron a la Luna. Lovell y Young fueron dos veces a la Luna. Armstrong, Conrad y Young caminaron por la Luna. John Young también voló, posteriormente, en la Lanzadera espacial.
Cinco miembros del tercer grupo de astronautas, que la NASA seleccionó en octubre de 1963, también realizaron misiones durante el programa Gemini. Fueron: Buzz Aldrin, Eugene A. Cernan, Michael Collins, Richard Gordon y David Scott. Cada uno hizo un solo vuelo en la misión Gemini, y al menos, otro en el programa Apolo. Scott y Cernan salieron al espacio una segunda vez en otra misión Apolo. Todos los integrantes de este grupo fueron a la Luna, de ellos, Cernan fue dos veces. Aldrin, Scott y Cernan caminaron por la Luna, en las misiones Apolo 11, Apolo 15 y Apolo 17, respectivamente.

De los 30 vuelos de la era Apolo, tres fueron suborbitales y nueve consistieron en misiones lunares. Los restantes 20 fueron vuelos orbitales terrestres. Los nueve vuelos lunares proporcionaron la oportunidad de realizar este tipo de vuelos a 24 personas. Solo tres personas volaron dos veces a la Luna. Los 6 alunizajes que se produjeron con éxito llevaron a 12 personas a la Luna. Ninguno alunizó dos veces, sin embargo dos de ellos ya habían volado a la Luna al menos una vez, cinco de ellos habían hecho ya vuelos no lunares, y cinco no tenían ningún tipo de experiencia en vuelos espaciales.

Siendo entonces, Neil Armstrong, el primer astronauta y el primer ser humano en la historia en pisar la Luna el 21 de julio de 1969, en la misión Apolo 11. Fue el mayor acontecimiento logrado por una agencia espacial.

El primer taikonauta de la historia fue Yang Liwei al salir al espacio en la nave Shenzhou 5 en octubre de 2003. Los taikonautas Fei Junlong y Haisheng fueron los siguientes en salir al espacio en la Shenzhou 6 en octubre de 2005. En 2012 China envió al espacio a la primera mujer "taikonauta", Liu Yang.

El éxito de una misión espacial implica que los astronautas cuenten con una técnica fiable, una serie de conocimientos especializados, una buena forma física y cierta estabilidad psíquica.

Entre las secuelas fisiológicas más comunes tras las estancias extraterrestres se encuentran los trastornos del sueño, la debilitación del sistema inmunitario, algunas atrofias musculares, la erosión de huesos y la carga radiactiva, que provoca que, a mayor tiempo en el espacio, más aumente la tasa de mutación de los cromosomas del ser humano y, por tanto, el riesgo de cáncer.

La ingravidez repentina es la causa de la mayor parte de los problemas físicos en el espacio: mareos, falta de apetito, náuseas y vómitos, los cuales solo empiezan a remitir de 2 a 4 días después. Con todo, a largo plazo se presentan otros problemas derivados de la falta de gravedad; el más importante es la destrucción de masa muscular, que empieza a producirse apenas dos semanas después del inicio del vuelo, debe contrarrestarse con un intensivo ejercicio físico por parte de los astronautas, es por esto que actualmente en la estación espacial internacional los astronautas realizan 2 horas de ejercicio, además que les ayuda a la parte emocional.

Otro problema frecuente es la hinchazón de la cara (en inglés, Puffy Face) en los primeros días en el espacio, debido a un exceso de sangre proveniente de los miembros superiores. Las repercusiones en la rigidez facial pueden ocasionar problemas de entendimiento con otros compañeros de misión.

En cuanto a las funciones cognitivas básicas (percepción, memoria y pensamiento lógico) se mantiene estables. Sin embargo, por lo que respecta al área psicomotora son perceptibles determinadas pérdidas funcionales: algunos movimientos voluntarios se ralentizan y se vuelven imprecisos, y la ejecución de tareas simultáneas se hacen más difíciles.

Psíquicamente, el aislamiento durante las misiones puede provocar un estado de "astenia", sobre todo a partir de la mitad de la misión: pasividad en aumento, fallos de atención, sensación de agotamiento, irritabilidad, depresión, etc. Debido a esto, en la Estación Espacial Internacional se viene desde hace tiempo aplicando el llamado "Human Behavior Performance Program" con el objeto de combatir el aburrimiento y el aislamiento social. Entre los métodos utilizados se encuentran el poner a disposición de los astronautas películas, discos, páginas personales para relacionarse con la vida en la tierra, videoconferencias familiares y una conferencia psicológica privada cada dos semanas con un psicólogo en Tierra.

En el nivel colectivo, las condiciones especiales de la vida en el espacio pueden provocar tensiones y conflictos. Además, las diferencias culturales pueden también generar problemas en el grupo.



</doc>
<doc id="3324" url="https://es.wikipedia.org/wiki?curid=3324" title="14 de octubre">
14 de octubre

El 14 de octubre es el 287.º (ducentésimo octogésimo séptimo) día del año en el calendario gregoriano y el 288.º en los años bisiestos. Quedan 78 días para finalizar el año.








</doc>
<doc id="3325" url="https://es.wikipedia.org/wiki?curid=3325" title="13 de octubre">
13 de octubre

El 13 de octubre es el 286.º (ducentésimo octogésimo sexto) día del año en el calendario gregoriano y el 287.º en los años bisiestos. Quedan 79 días para finalizar el año.










</doc>
<doc id="3326" url="https://es.wikipedia.org/wiki?curid=3326" title="12 de octubre">
12 de octubre

El 12 de octubre es el 285.º (ducentésimo octagésimo quinto) día del año en el calendario gregoriano y el 286.º en los años bisiestos. Quedan 80 días para finalizar el año.









</doc>
<doc id="3327" url="https://es.wikipedia.org/wiki?curid=3327" title="11 de octubre">
11 de octubre

El 11 de octubre es el 284.º (ducentésimo octogésimo cuarto) día del año en el calendario gregoriano y el 285.º en los años bisiestos. Quedan 81 días para finalizar el año.
















</doc>
<doc id="3328" url="https://es.wikipedia.org/wiki?curid=3328" title="10 de octubre">
10 de octubre

El 10 de octubre es el 283.º (ducentésimo octogésimo tercer) día del año en el calendario gregoriano y el 284.º en los años bisiestos. Quedan 82 días para finalizar el año.









</doc>
<doc id="3329" url="https://es.wikipedia.org/wiki?curid=3329" title="Ludwig von Mises">
Ludwig von Mises

Ludwig Heinrich Edler von Mises (Lemberg; 29 de septiembre de 1881-Nueva York, 10 de octubre de 1973) fue un economista austriaco de origen judío, historiador, filósofo y escritor liberal que tuvo una influencia significativa en el moderno movimiento liberal-libertario en pro del mercado libre y en la Escuela Austriaca.

Planteó lo perjudicial del poder e intervención gubernamentales en la economía que, según su teoría, por lo general llevan a un resultado distinto al natural y por esto muchas veces perjudicial para la sociedad, ya que generan caos en el largo plazo.

Nació en Lemberg, capital de Galitzia en el antiguo Imperio austrohúngaro (actualmente Ucrania) hijo de Arthur von Mises (ingeniero de ferrocarriles y funcionario público) y Adele Landau von Mises. Su hermano menor, Richard von Mises fue un célebre físico. La familia Mises se mudó a Viena siendo Ludwig niño. En 1892 ingresó en el "Akademisches Gymnasium", donde recibió una formación humanista. Fue compañero de Hans Kelsen. Desde temprana edad Mises se interesó por la historia y la política. Después de graduarse en 1900, comenzó estudios de derecho y administración pública en la Universidad de Viena.

Bajo la dirección de Carl Grünberg, Mises comenzó siendo un exponente de la llamada Escuela Histórica de Administración Pública, que daba mayor importancia a la búsqueda de datos que al análisis teórico. Pero en otoño de 1903 leyó la obra "Principios de Economía Política" de Carl Menger, texto fundador de la escuela económica austríaca. El libro le llevó a buscar un enfoque más teórico, y en los años sucesivos profundizó sus estudios de teoría económica, especialmente en el seminario de Eugen von Böhm-Bawerk, exministro de Hacienda y prócer de la Escuela Austríaca.

Mises se graduó en febrero de 1906. Ingresó como funcionario en el ministerio de hacienda austríaco, pero después de unos pocos meses abandonó harto de la excesiva burocracia. Durante los dos años siguientes trabajó como pasante en un bufete y dio clases de economía. En 1909, empezó a trabajar en la Cámara de Comercio e Industria de Viena, donde permaneció los siguientes veinticinco años. La Cámara era una organización cuasiestatal y a través de sus publicaciones Mises ejerció una influencia considerable en la política austríaca.

En 1912 publicó "La teoría del dinero y el crédito", obra en la que aplicaba la teoría de valor de Carl Menger al dinero y presentaba una nueva teoría de la coyuntura económica en la que las crisis eran provocadas por la distribución inadecuada de los recursos debido a la inflación. Demostró que la cantidad de dinero en la economía no era neutral y que su aumento tenía efectos redistributivos.

Durante la Primera Guerra Mundial sirvió como oficial en la artillería austrohúngara y fue asesor económico en el Ministerio de la Guerra. Sus experiencias bélicas le sirvieron para desarrollar sus teorías sobre el intervencionismo estatal. El último año de la guerra recibió el prestigioso nombramiento de profesor extraordinario en la Universidad de Viena.

Después de la guerra participó como adjunto en el gobierno austríaco ocupándose de asuntos financieros con el extranjero. Su principal logro durante esta época fue disuadir a su antiguo amigo Otto Bauer, líder del partido socialista, de intentar un golpe de estado bolchevique. También escribió un libro explicando el colapso del Imperio austrohúngaro. En "Nación, Estado y Economía" (1919) afirmaba que el imperialismo germano era consecuencia de la aplicación del poder del Estado para resolver los problemas de las comunidades multiculturales de Alemania y Austria.

Posteriormente publica "El socialismo: un análisis económico y sociológico" (1922) donde afirmó que el sistema comunista no podía ser eficiente ya que le faltaba el mecanismo de precios que hacía que la distribución de los recursos fuera la adecuada, como sucedía en el sistema capitalista. Este libro tuvo gran influencia muchos años después (debido a las dificultades que tuvo su difusión y su tardía traducción al inglés) al advertir y predecir con mucha antelación el fracaso del socialismo evitando valoraciones éticas y morales.

Durante los años veinte, desde su puesto, Mises luchó con éxito contra la inflación y utilizó su influencia para imponer las reformas monetarias y financieras que experimentó Austria en 1922. No pudo impedir, no obstante, el constante aumento de la reglamentación estatal que, en su opinión, dilapidaba la hacienda pública. Esto le llevó a postular la teoría de que el intervencionismo estatal era totalmente contraproducente. Excluyó como soluciones las posibles terceras vías y defendió el "laissez-faire" como único remedio. En 1927, publicó una concisa presentación de su política filosófica utilitaria en "El liberalismo".

A finales de los años veinte publicó una serie de artículos sobre el carácter epistemológico de la ciencia económica. Mises afirmó que la ciencia económica no podía ser refutada ni comprobada a través de los datos observables. La economía era una ciencia en la que predominaban los juicios "a priori" al igual que las matemáticas, la lógica o la geometría. No obstante, opinaba que la economía era parte de una ciencia social más amplia, la praxeología. 

En 1934 deja Austria y se traslada a Ginebra para empezar un nuevo periodo docente. Desde entonces y hasta 1940 se haría cargo de la cátedra de Relaciones económicas internacionales en la Universidad de Ginebra.

En 1940 tuvo que huir de Europa por temor a ser apresado por los nazis. Se instaló en Nueva York y se naturalizó estadounidense en 1946. A partir de 1945 fue profesor visitante en la Universidad de Nueva York hasta 1969, sin llegar nunca a adquirir una plaza interina dentro de esta institución. Fue en esta época cuando retoma la investigación económica y publica "La acción humana" (1949).
Mises fue, con su renovación del liberalismo clásico a través de la Escuela Austriaca de Economía, uno de los principales mentores espirituales del libertarismo y su obra "La acción humana" (1949) ejerció gran influencia en intelectuales de raigambre austríaca como Friedrich Hayek, Murray Rothbard, Hans Sennholz, George Reisman, Ralph Raico, Leonard Liggio, Tibor Machan, Peter Boettke, Roger Garrison, Manuel Ayau y Joseph Keckeissen. Pero también fue vital para economistas no pertenecientes a su escuela (la mayoría de ellos Premios Nobel) y pensadores de muy diferentes áreas: Max Weber, Joseph Schumpeter, Oskar Lange, Henry Simons, Lionel Robbins, Maurice Allais, Milton Friedman, John Hicks y la lista sigue hasta el actual economista experimental Vernon Smith entre tantos otros. Von Mises murió en 1973 en el hospital St. Vincent de Nueva York.

La praxeología es para Mises el método para estudiar las ciencias sociales; siendo el equivalente su estudio al de las ciencias experimentales pero sin la capacidad de realizar experimentos. La praxeología hace referencia al estudio de cómo la mente humana estructura el pensamiento de modo que, conociendo dicha estructura, podemos deducir a priori los postulados que guían las decisiones individuales de cada sujeto. Este método sería similar al usado en matemáticas y en lógica. Para Mises no es posible un estudio a través de la experiencia por la imposibilidad de que existan constantes en las relaciones entre variables. En la obra "La acción humana", Mises critica el método matemático y la observación de datos como estudio de la economía, de hecho, considera que dichos métodos pueden ser usados en el análisis de la historia económica, pero no son válidos para entender o predecir el comportamiento humano. Ello se debe a que las simplificaciones y los problemas técnicos en la recogida de datos modifican de tal modo la relación entre las variables que puede alterar la relación y causalidad que, a priori y basándonos en hipótesis deductivas, persigue el ser humano para alcanzar su fin. Dicho de otro modo, todo estudio de la economía a través de datos empíricos es un estudio de hechos pasados y por tanto no sirve para deducir una pauta de comportamiento en los individuos.

Dentro del enfoque praxeológico, las hipótesis que se realizan sobre la acción humana llevan vinculadas factores como el valor de la operación, la riqueza, los términos de intercambio, precios y costes y también su valoración subjetiva ligada a la escala valorativa del sujeto, importancia relativa, escasez, etc. De este modo puede extraerse una acción humana que corresponda de forma racional a la maximización del sujeto de su bienestar individual. La consecuencia de aplicar este sistema a las ciencias sociales es que el estudio de la “economía” no solo mide las relaciones humanas mesurables, sino aquellas que no pueden medirse en términos monetarios, pero presentan relaciones de intercambio, siendo dicho conjunto conocido como “acción humana” y representando este concepto un tramo de acción acotado, que posteriormente formando fenómenos complejos sea el que defina el comportamiento del individuo. Este conjunto nuevo de conocimiento abarca todas las disciplinas de las ciencias sociales, similar a la sociología pero sin el carácter historicista que Mises le atribuye a esta última.

Otra consecuencia es que para la Escuela Austriaca, todas las demostraciones empíricas no sirven para nada, ya que se basan en el estudio de datos, pero esta relación puede cambiar. Mises no contempla una categoría de ciencia donde sus leyes sean mutables y por ello todos los axiomas demostrados matemáticamente no tienen la consideración de economía, en todo caso podría considerarse estudio de la historia económica. El autor considera que las acciones extraídas de la praxeología son inmutables y por tanto leyes humanas y que dichas leyes humanas no dependen del tiempo ni de otros factores, por lo que pueden observarse o no dentro de la realidad y del estudio de datos dependiendo de la complejidad de los fenómenos.

La teoría austriaca del ciclo económico hace énfasis en que toda producción requiere tiempo. También transcurre tiempo entre el inicio de la producción y el consumo, por lo que se hace evidente la importancia que estos autores le daban al ahorro en relación con el tiempo en el que se invierte y la duración de dicha inversión. Esta relación será crucial en la forma en que los tipos de interés provocan cambios en la estructura de consumo de bienes de consumo frente a bienes de capital. Este ahorro procede de los ingresos no gastados de los agentes y, a través de los bancos, financia el proceso empresarial. Esta relación puede ser modificada por la autoridad monetaria con el fin de aumentar la actividad económica, produce una disminución del tipo de interés, lo que a priori consigue su objetivo. Sin embargo, esa nueva actividad económica financiada con bajos tipos de interés son realmente actividades que no se llevarían a cabo en situaciones normales; son actividades especulativas y que generan poco o ningún valor añadido.
Esta nueva situación solo puede mantenerse en el tiempo si se mantienen los tipos de interés anormalmente bajos, lo que conduciría a una situación cada vez más complicada ya que nuevos recursos irán a parar a estas actividades especulativas. Por otro lado, si cesa la política de tipos de interés bajo, se producirá una pérdida de valor debido a que no es posible retirar todos los recursos asignados a los sectores poco productivos. 
Otro efecto producido por el aumento de la oferta monetaria es el aumento de la inflación, que cambia la relación entre los bienes de inversión y de consumo y añade problemas al sostenimiento de la política de tipos de interés bajo.

La teoría austriaca del dinero se presenta en la obra "Teoría del dinero y el crédito" (1912). A partir de las aportaciones de Menger sobre la utilidad marginal, Mises aplica un modelo de oferta y demanda para explicar el origen del valor del dinero. El elemento central que compone la teoría del valor misiana es que el cambio objetivo (poder adquisitivo) del dinero es el que genera las peculiaridades que tiene, ya que sin capacidad de poder adquisitivo no se haría uso del dinero. Es este elemento el determinante de la demanda y dependerá del valor subjetivo que cada ciudadano tenga del dinero. 
Por otro lado, el valor subjetivo de cada individuo depende del valor subjetivo del resto de bienes económicos en relación con el dinero. Para los autores austríacos estos dos tipos de valores están relacionados a través del teorema de la regresión monetaria.
La demanda de dinero en el día “D” se basa en el poder adquisitivo que poseía en el día “D-1”. Este mismo poder adquisitivo surge por la intersección de la oferta del dinero en el día “D-1” y su demanda que, basándonos en el valor subjetivo de los individuos, situamos nuevamente en el valor determinado por el poder adquisitivo un día antes, en el día “D-2”. Esta regresión puede desarrollarse ad infinitum pero carecería entonces de sentido. Para el autor, esta dinámica empieza en el momento en el que el oro se usaba únicamente como bien, pero su uso como medio de cambio (en palabras misianas su valor de cambio objetivo) aún no existía. En el momento en el que en un pequeño grupo se estandariza el uso de oro como medio de cambio para evitar la ineficiencia de los intercambios del trueque, este posee ya valor de cambio objetivo y nace la dinámica de mercado que produce que este medio se extienda en el resto de regiones.
En cuanto a la oferta monetaria de este sistema, para Mises debe estar basada en el patrón oro. Dado que el valor que posee el dinero está basado en una cantidad de oro, este valor no se devalúa.
Otra característica que define a la escuela austriaca es que no aceptan la existencia de un término que resuma el sistema general de precios. Para los autores de esta escuela, los precios de todos los bienes, incluido el dinero, se puede expresar en infinidad de relaciones de intercambio respecto al resto de bienes que dependen del valor subjetivo que otorgue cada individuo a dichos bienes. De este modo, cualquier “resumen” de los precios en un solo valor o conjunto de valores únicamente hacer desaparecer los matices generados por un fenómeno complejo en la determinación de precios por un valor más comprensible pero carente de importancia.

Con la publicación de “El cálculo económico en la comunidad socialista” (1920), Mises empieza una crítica al sistema socialista que complementa su animadversión por la intervención estatal. La tesis principal presentada en esta obra es que en un sistema donde los precios muestran una limitada relación de intercambio, la información que aporta a los agentes es también limitada.
En una economía libre, el cálculo monetario permite apreciar a los agentes las potencialidades económicas. En una economía donde la única fuente de rentabilidad es satisfacer las necesidades del consumidor, los empresarios buscaran cubrir dichas necesidades de la manera más barata posible, garantizando la eficiencia y el progreso. 
Si el Estado interviene en la economía, incentiva procesos productivos ineficientes y modifica la relación entre precios relativos y la utilidad relativa que obtienen los agentes económicos.
En una economía socialista los precios no solo perjudican al mercado de bienes de consumo, donde las preferencias individuales son modificadas como hemos comentado por la intervención estatal, sino que los medios de producción y el mercado de bienes de producción óptimos para producir el bien final deseado no se realiza por el proceso de mercado y por tanto, no se conocerá la eficiencia de dichos métodos.
En resumidas cuentas, el desconocimiento de los agentes de los costes que suponen sus acciones lleva a que la producción y el intercambio se realicen sobre la base de criterios no económicos y por tantos ineficientes.
Desde el punto de vista de la política monetaria, los órganos gubernamentales tienen a incentivar la economía mediante tipos de interés bajos, lo que provoca inversiones improductivas y conduce a una situación que se desarrolla en la “Teoría austriaca del ciclo económico”.

Ludwig von Mises publicó a lo largo de su vida más de doscientos ensayos en los que trató diversos temas, como la aplicación del método positivo en economía, el estatismo y la educación. 
Una de las aportaciones más destacables es su negativa a aceptar el equilibrio general de los clásicos y los neoclásicos (tomando como referencia el modelo de Walras). En economía es imposible determinar un equilibrio general donde todas las variables puedan determinarse de forma simultánea. Para Mises el protagonista de la economía es el emprendedor, de manera que tendrá éxito siempre que los precios generados en el mercado cubran sus pérdidas, de este modo irán arruinándose los empresarios menos competitivos favoreciendo la innovación y el progreso. Todo este proceso de mercado hace que los condicionantes de la oferta y de la demanda produzcan cambios en el mercado de forma continua, por lo que un modelo estático no puede plasmar la realidad.

Selección de alguna de sus obras:




</doc>
<doc id="3330" url="https://es.wikipedia.org/wiki?curid=3330" title="29 de septiembre">
29 de septiembre

El 29 de septiembre es el 272.º (ducentésimo septuagésimo segundo) día del año en el calendario gregoriano y el 273.º en los años bisiestos. Quedan 93 días para finalizar el año.









</doc>
<doc id="3331" url="https://es.wikipedia.org/wiki?curid=3331" title="30 de septiembre">
30 de septiembre

El 30 de septiembre es el 273.º (ducentésimo septuagésimo tercer) día del año —el 274.º (ducentésimo cuadragésimo cuarto) en los años bisiestos— en el calendario gregoriano. Quedan 92 días para finalizar el año.



















</doc>
<doc id="3332" url="https://es.wikipedia.org/wiki?curid=3332" title="1 de octubre">
1 de octubre

El 1 de octubre es el 274.º (ducentésimo septuagésimo cuarto) día del año en el calendario gregoriano y el 275.º en los años bisiestos. Quedan 91 días para finalizar el año.








</doc>
<doc id="3334" url="https://es.wikipedia.org/wiki?curid=3334" title="Juan de Garay">
Juan de Garay

Juan de Garay (1528 - 1583) fue un hidalgo, explorador, conquistador y gobernante colonial español que tuvo un importante papel en la organización de la parte atlántica de Sudamérica. Se destacó por su actuación en la gobernación del Río de la Plata y del Paraguay por haber sido el fundador de la ciudad de Santa Fe en 1573 en su primera ubicación, por lo cual fue asignado al año siguiente como su teniente de gobernador, para convertirse en 1577 en el teniente de gobernador general de Asunción. Al ser ya gobernador del territorio desde el año 1578, fundó la ciudad de Buenos Aires en 1580 con el nombre de "Ciudad de la Trinidad", en el lugar donde en 1536 Pedro de Mendoza había instalado (sin ninguna formalidad correspondiente a una "fundación" para el criterio español) un fuerte, de efímera vida, con el nombre de "Real de Nuestra Señora Santa María del Buen Ayre".

Juan de Garay habría nacido en 1528 en un lugar por determinar del nordeste de la ya unificada Corona de España. Su lugar exacto de nacimiento es polémico: mientras unas fuentes señalan a la ciudad vizcaína de Orduña (actual País Vasco), otras apuntan al municipio burgalés de Junta de Villalba de Losa (actual Castilla y León). Si bien ambas localidades son vecinas, la de Losa fue y sigue siendo una zona castellana, por lo cual hay que tener en cuenta que él mismo se definía como «vizcaíno» y así lo expresaría su descendencia. No se ha encontrado la fe de bautismo de Garay ni en Losa ni en Orduña.

No hay apenas referencias a la infancia de Juan de Garay. Si en cuanto al lugar de nacimiento hay dudas, también las hay en cuanto al año. No se sabe a ciencia cierta cuándo nació realmente y podría situarse entre diciembre de 1527 y enero de 1529, y muchas veces aparece el año 1528, más aceptado por ser el promedio de ambas fechas y por contener la de su antroponimia que sería el 24 de junio, ya que su nombre no lo heredaba de su progenitor y ni del de su padrastro, como tampoco del de sus abuelos paternos ni maternos, como era costumbre dentro de los linajes nobles.

Su madre era Lucía de Mendieta y Zárate (n. Orduña, ca. 1512) y su padre fue el noble Clemente López de Ochandiano y Hunciano (n. Orduña, ca. 1491), un hijo de los hidalgos Diego López de Ochandiano y de María de Hunciano, pero sería criado por su tío materno, el licenciado Pedro Ortiz de Zárate y Mendieta (Orduña, ca. 1500 - Lima, 1547), hasta que su madre se uniera en matrimonio con Martín de Garay quien lo reconoció como su hijo, dándole su apellido, aunque Juan de Garay ostentaría el blasón de Ochandiano de su verdadero progenitor: ""grifo con bordura cargada con ocho aspas"".

La versión que apoya a Orduña como el lugar de nacimiento de Garay dice que el día 7 de octubre de 1535, debido a un fuerte incendio de esta localidad, la familia de Garay debió trasladarse al vecino pueblo de Villalba de Losa, en donde su tío Pedro y su esposa Catalina Uribe y Salazar eran propietarios de otras casas, ya que su palacio en aquella ciudad se había incendiado.

En el año 1543, cuando Garay contaba con unos 15 o 16 años de edad, acompañó a su familia materna al gran Virreinato del Perú, ya que su tío Pedro Ortiz de Zárate había sido nombrado oidor de la Real Audiencia de Lima con el nuevo virrey Blasco Núñez Vela quien portaba las famosas ordenanzas del emperador Carlos V, conocidas como Leyes Nuevas, que había sancionado en Barcelona el 20 de noviembre de 1542 con el objetivo de mejorar el trato y calidad de vida de los aborígenes sometidos en América y además mandaba quitar las encomiendas a los que habían participado en el bando pizarrista durante la guerra civil peruana.

Juan de Garay y su familia de parte materna: sus tíos Pedro y Catalina, además de sus primos —el primogénito Pedro Ortiz de Zárate, Ana de Salazar y el menor Francisco de Uribe— zarparon hacia América el 3 de noviembre de 1543 desde el puerto de Sanlúcar de Barrameda.

Hicieron escala en las islas Canarias, cruzaron el océano Atlántico y el mar Caribe para llegar el 10 de enero del siguiente año al puerto indiano de Nombre de Dios en Centroamérica, y posteriormente por tierra pasaron a la ciudad de Panamá. Por diferentes motivos, los Ortiz de Zárate retrasaron su llegada a Sudamérica que lo harían a través del océano Pacífico, y entraron en Lima el 10 de septiembre de 1544.

Además de su tío que ocuparía el cargo de oidor, compondrían la recién fundada Real Audiencia: Diego Vásquez de Cépeda, Juan Álvarez y Juan Lissón de Tejada. La futura rigidez en el gobierno del virrey Núñez Vela por disposición imperial generó enfrentamientos, que llevaron a una nueva guerra civil con los partidarios de Gonzalo Pizarro. El joven Garay fue fiel a su tío que estaba de parte del virrey y más adelante participaría activamente contra Pizarro.

En marzo de 1547 murió su tío materno Ortiz de Zárate, después de recibir la visita de su yerno Blas de Soto —un hermano uterino de Gonzalo Pizarro— que se había casado con su única hija, Ana de Salazar.

En aquellos enfrentamientos civiles Juan de Garay había conocido en su morada al vascongado Martín de Robles quien al fallecimiento del tío de aquel, se aposentó unos días en casa del difunto ya que el desamparado Garay con tan solo diecinueve años de edad no sabría qué hacer, por lo cual, lo convenció para empuñar las armas contra los insurrectos, y posteriormente se transformaría en un excelente soldado.

Juan de Garay hizo la campaña de La Gasca, en la que participaba el capitán Robles, hasta la batalla de Jaquijahuana o del valle de Sacsahuana del 9 de abril de 1548, a 25 km del Cuzco.

En 1549, Juan de Garay formó parte de la expedición de Juan Núñez de Prado que había sido nombrado gobernador del Tucumán —en la actual Argentina— enviado por Pedro de la Gasca quien era el presidente de la Real Audiencia de Lima, siendo el virrey peruano y marqués de Cañete, Antonio de Mendoza.

Núñez de Prado quien antes de gobernador fuera el alcalde de Potosí, fundó en el año 1550 la ciudad de «El Barco I» —el primer asentamiento de la actual ciudad argentina de Santiago del Estero, y en donde diez años después Zurita fundara la ciudad de Cañete— pero a finales de mayo o junio de 1551 hizo mudarla al noroeste, en los valles del río Calchaquí —probablemente cercana al actual pueblo salteño de San Carlos— y se la conocería como «El Barco II», esto ocurrió por problemas jurisdiccionales con el gobernador de Chile, Pedro de Valdivia, que había enviado a su segundo Francisco de Villagra a resolver la situación.

En junio de 1552 los oidores de la Audiencia limeña ordenaron nuevamente su traslado hacia la región de los Juríes, por lo cual fue asentada sobre la margen del río Dulce —actual provincia argentina de Santiago del Estero— y de esta forma pasó a conocerse como «El Barco III», aunque en 1553, Núñez de Prado y algunos de sus hombres fueron apresados por Francisco de Aguirre, en la citada población, ya que seguía presentando problemas de jurisdicción, entonces este último resolvería volver a trasladarla a su actual emplazamiento, rebautizándola con el nombre de «Santiago del Estero del Nuevo Maestrazgo».

Juan de Garay no fue detenido por Aguirre aunque también pasó a Chile pero como proveedor del ejército. Núñez de Prado, estando preso en esas tierras, apeló ante el virrey del Perú y por mandato de los oidores de la Audiencia fue enviado a Lima, en donde fue juzgado, y luego de darle la razón, se lo liberó y confirmó mediante real provisión del 13 de febrero de 1555, en el cargo de gobernador del Tucumán, del que Aguirre lo había destituido, pero en una nueva expedición en que Garay volvió a acompañarlo, siguió participando en la misma aún después de desaparecer aquel de la escena política y de la historia.

En 1556 Garay se mudó a Potosí, cuatro años antes de la Primera Guerra Calchaquí, y se relacionó de nuevo estrechamente con sus parientes que residían allí, especialmente con su otro tío materno Juan Ortiz de Zárate.

Desde esta última fecha, Garay centraba sus actividades en la provincia de Charcas —actual Bolivia— en donde el hijo del virrey y futuro gobernador de Chile, García Hurtado de Mendoza, le encomendó trazar un camino a la costa del océano Pacífico que permitiera un comercio más activo de la «Villa Imperial», y para tal fin, estableció el puerto de Arica, villa que había sido fundada oficialmente el 25 de abril de 1541 por Lucas Martínez Vegaso, y se desempeñó nuevamente con el cargo de proveedor del ejército.

En 1557, luego de la muerte del capitán Martín Robles, Garay se integró con el grado de capitán a la expedición de conquista de Andrés Manso para poblar los territorios más allá de la «Villa de La Plata», y de esta forma asistiría a la fundación de la villa Santo Domingo de la Nueva Rioja, sobre la orilla izquierda del río Condorillos o Parapetí y cerca de los Bañados del Izozog, la cual duraría unos siete años antes de ser destruida por los aborígenes chiriguanos. En esta oportunidad fue que Garay conocería a Ñuflo de Chaves que por problemas jurisdiccionales con Manso, retrasaría su labor de conquista.

Hacia 1558 había retornado a casarse a la ciudad de Asunción del Paraguay, luego de confesar tener un hijo homónimo de tres años con una manceba aborigen, y en donde nació su primera hija: María de Garay, en el año 1559.

En cuanto a Chaves, este con sus 158 soldados se dirigió a la comarca de los aborígenes chiquitos, para fundar el 1 de agosto de 1559 una nueva ciudad que se llamaría Nueva Asunción o «La Barranca», en la orilla derecha del río Guapay que solo duraría unos cinco años antes de ser destruida por los chiriguanos, y sus habitantes trasladados a una nueva villa a 170 km al este, en la que hoy se conoce como Santa Cruz la Vieja.

Luego de la confrontación con los hombres de Manso en donde se encontraba Garay, aquel y Chaves habían marchado hacia Lima a finales del citado año, para reclamar sus derechos respectivos ante el virrey Andrés Hurtado de Mendoza quien en 1560 nombró a su hijo García Hurtado de Mendoza como administrador de la nueva comarca incorporada a la que llamaría gobernación de Moxos —que incluía a la futura gobernación de Santa Cruz de la Sierra y la de Chiquitos— a la vez que este nombrara como su lugarteniente a Ñuflo de Chávez quien se transformaría en el gobernador interino ya que aquel estaba residiendo en Chile.

Andrés Manso no se conformó con el dictamen virreinal, por lo que resistió a dichas órdenes, siendo apresado y enviado a la villa de La Plata, aunque por poco tiempo ya que se fugaría ayudado por el alcalde y prepararía una rebelión con veinte compañeros, pero Garay no adhirió a su causa porque se mantuvo leal a la disposición del virrey Mendoza.

El 26 de febrero de 1561 Garay participó junto a Ñuflo de Chaves en la fundación de la primera Santa Cruz de la Sierra —estaba situada a 14 leguas o bien a unos 56 km oeste sudoeste de la actual San José— ubicada originalmente en los Llanos de Chiquitos, de la que fue regidor de su cabildo y tuvo asignada una encomienda de indios.

En esta nueva ciudad citada tuvo con su esposa por lo menos dos hijos cruceños: Jerónima y el homónimo Juan de Garay ""el Legítimo"". A mediados del año 1568 partió otra vez hacia la Asunción, llevando consigo a su esposa y sus tres hijos legítimos, además de su hijo natural de unos 13 años de edad, por lo que tuvieron que atravesar el Chaco Boreal, el belicoso territorio de los guaycurúes.

En 1567 el tío materno de Juan de Garay, el capitán Juan Ortiz de Zárate, fue nombrado adelantado interino por el virrey del Perú —ya que desde el 19 de octubre de 1564 le había sido asignado el cargo de sexto gobernador del Río de la Plata y del Paraguay luego de destituir a Francisco Ortiz de Vergara— y por lo cual Zárate hizo ocupar el cargo de lugarteniente asunceño a Felipe de Cáceres quien a su vez, nombró capitán a Juan de Garay, pidiéndole que "traiga a gentes a la provincia de Paraguay".

Juan de Garay había partido hacia Asunción con su familia y llegó luego de unos cuatro meses, el 11 de diciembre de 1568, y en el transcurso del viaje lo nombraron el 8 de diciembre como alguacil mayor del Río de la Plata.

Bajo este cargo, a finales del año 1569, dio la orden al capitán Ruy Díaz de Melgarejo para fundar una ciudad que se llamaría Villa Rica del Espíritu Santo con el objetivo de afianzar las posesiones españolas en las zonas deslindadas por el Tratado de Tordesillas, además de sospechar de la existencia de una importante mina de oro.

Dicho capitán partió en esa fecha desde Asunción hacia el Guayrá y una vez en Ciudad Real salió con 40 hombres para erigirla, y el 14 de mayo de 1570 a 60 leguas de la misma, la fundó en su primer emplazamiento provisional que estaría entre las nacientes de los ríos Piquiri e Ivaí y en donde mandó construir una iglesia y una fortaleza para luego trazar el poblado, y aunque las minas auríferas no fueran halladas, sí descubrieron una de hierro.

El 3 de abril de 1573 Martín Suárez de Toledo, como gobernador interino del Río de la Plata y del Paraguay, le encargó a Garay una expedición por el río Paraná que tenía como finalidad fundar una urbe que facilitara a la ciudad de Asunción la salida al mar y la comunicación con la metrópoli.De esta forma, se organizó una expedición integrada por 80 mancebos de la tierra, en un bergantín, embarcaciones menores y caballos, con 75 nativos guaraníes y 9 españoles. Se componía de dos grupos, uno por el Paraná que mandaba el propio Juan de Garay y otro por tierra a cargo de Francisco de Sierra que recorrería la margen izquierda del río, evitando así los bosques del Chaco y llevando las carretas, el ganado, los caballos y otros elementos necesarios para la fundación.

Garay salió de Asunción el 14 de abril de 1573 aunque el que iba por tierra lo hizo meses antes. Además, el gobernador Suárez de Toledo le había encargado la escolta de la carabela "San Cristóbal de la Buenaventura" capitaneada por Ruy Díaz de Melgarejo y su segundo el capitán Espinosa en donde llevaría preso a España a Felipe de Cáceres que había sido depuesto por el obispo Pedro Fernández de la Torre que también viajaba para formular oficialmente la acusación ante la Corte.

Tal como indica el poder de Suárez de Toledo, Juan de Garay llevaba: 

Los dos grupos se encontraron en un lugar llamado «La Punta del Yeso», justo enfrente de la actual Cayastá, avanzando juntos por el río San Javier, entonces llamado río de la Quiloazas.

Garay decidió desembarcar muy pronto y eligió la orilla sudoeste del río (donde hoy se encuentran las ruinas de Santa Fe la Vieja, a 5 km de Cayastá) construyendo un pequeño asentamiento allí. Desde ese lugar partió una pequeña expedición de exploración para encontrar un lugar más apropiado. Durante estas exploraciones de búsqueda coincidió con Jerónimo Luis de Cabrera que también estaba explorando el Paraná e intentando erigir una ciudad para apoyar la recién fundada Córdoba. Como resultado de este encuentro Juan de Garay decidió dar la categoría de ciudad al pequeño asentamiento, al cual regresó el 30 de septiembre.

La expresión «abrir puertas a la tierra», que hizo suya Juan de Garay, fue la máxima de toda la administración española en esa parte de América. Con ella se quería indicar la necesidad de fundar ciudades para romper el aislamiento de Asunción hacia los dos lados, uno río abajo abriéndola al mar y conectándola con la metrópoli, y hacia el Alto Perú, centro político y económico de la época.

El 15 de noviembre de 1573, Juan de Garay fundó oficialmente la ciudad de Santa Fe en su primer emplazamiento. Según recogió el escribano Pedro E. Espinosa: ""Juan de Garay, en pie, junto al «palo rollo», símbolo de la justicia y el poder real"", y realizó la fundación con las siguientes palabras:

Los miembros del cabildo de la nueva ciudad fueron designados por el propio Garay. Entre las opciones de Suárez de Toledo para la ubicación de la urbe estuvo, incluso, la de hacerla en Banda Oriental, ya sea a orilla de los ríos San Juan o San Salvador, o de lo contrario en la isla San Gabriel.

El 23 de junio de 1576 la ciudad pasaría a llamarse «Santa Fe de Luyando» por orden del gobernador interino del Río de la Plata y del Paraguay, Diego Ortiz de Zárate y Mendieta, un primo de Garay. Fue así que el nombre fue cambiando hasta retornar al original de «Santa Fe de la Vera Cruz» pero esta vez en el nuevo emplazamiento.

Respecto a esta primera ubicación de la ciudad, solo duraría unos 80 años, por lo que se la conoce como «Santa Fe la Vieja», ya que luego se la mudaría unos kilómetros hacia el sur por motivos de seguridad, a causa de los ataques de los guaycurúes. El traslado duró diez años, ya que comenzó el 5 de octubre de 1650 y terminó hacia diciembre de 1660.

Elegidos los miembros del cabildo santafecino, nombraron de común acuerdo a Juan de Garay el 12 de marzo de 1574, como teniente de gobernador de Santa Fe, cargo confirmado por el adelantado Juan Ortiz de Zárate el 7 de junio del mismo año.

En el mes de mayo de este último año, Garay acompañaría al adelantado Juan Ortiz de Zárate a la Banda Oriental y cerca de la desembocadura del río San Salvador fundaría a la ciudad Zaratina del San Salvador —en las cercanías de la actual ciudad uruguaya de Dolores— con pobladores de Santa Fe pero duraría hasta su abandono tres años después.

Una vez que Juan Ortiz de Zárate confirmara ante el rey el título de adelantado el 10 de julio de 1569 y retornara al Río de la Plata el 17 de octubre de 1572,en noviembre de 1573 arribó con una armada a la isla San Gabriel en donde levantó un fortín pero al quedar aislado por la resistencia de los aborígenes charrúas, Garay iría en su auxilio en mayo del siguiente año y luego de derrotarlos en la batalla de San Salvador, Zárate podría pasar a tierra firme para ocupar el cargo de gobernador recién en el mismo mes de 1574 hasta 1576, fecha en que fallecería.

El adelantado había designado para que lo sucediera a quien se casara con su hija Juana Ortiz de Zárate y Yupanqui. Mientras eligiese a los candidatos para desposarla, designó a su sobrino Diego Ortiz de Zárate y Mendieta para que ocupara interinamente el gobierno, hecho que se consumaría desde 1576 hasta 1577. Este, a su vez, designó como alcalde de Asunción a Luis Osorio.

En enero de este último año, luego que Garay resolviera el tema de la despoblación de la «Ciudad Zaratina» cuyos pobladores huían hacia Santa Fe, Córdoba y Tucumán, se dirigió hacia el Perú ya que había sido nombrado tutor de Juana de Zárate, por lo cual tuvo que pasar por esta primera ciudad nombrada y luego por Santiago del Estero. Al dirigirse al Tucumán, Gonzalo de Abreu y Figueroa que había salido de Córdoba, lo obligó a ayudarle con la fundación de la «Ciudad de San Clemente de la Nueva Sevilla» sobre las ruinas de la anterior Córdoba de Calchaquí —fundada en marzo de 1559 por Juan Pérez de Zurita y destruida en 1560 durante la primera guerra con aborígenes lugareños, cuya ubicación actual es la localidad de Chicoana— que duró pocos días y luego de varias luchas contra los calchaquíes, la refundó pero en otro lugar —al sudeste de la actual Rosario de Lerma— pero también sería arrasada. Por tercera y última lo intentaría aunque también terminaría destruida. En marzo del mismo año, Garay pudo desprenderse de esta obligación para poder seguir con su viaje, hasta llegar a Lima.

Cuando Mendieta por sus excesos fuera depuesto en Santa Fe y tuviera que ausentarse para viajar a Charcas, designó en el mismo día 3 de mayo de 1577 a Osorio como su lugarteniente, haciéndose cargo del gobierno interino del Río de la Plata y del Paraguay, desde esa fecha hasta la llegada del nuevo adelantado. Durante la gestión de este último se despobló definitivamente la «Ciudad Zaratina de San Salvador», el 20 de julio del mismo año y ya ocupando nuevamente su cargo de alcalde mayor de Asunción mandaría una tropa de 30 arcabuceros a sofocar la rebelión de los guaraníes del norte asunceno que estaban liderados por el cacique mesiánico Oberá pero sin mayores resultados.

Finalmente el elegido para casarse con Juana de Zárate fue Juan Torres de Vera y Aragón, consumándose en secreto a mediados de 1577 y por lo cual este sería nombrado como el cuarto adelantado, y Juan de Garay su albacea, aunque no pudiera ocupar inmediatamente el cargo de gobernador del Río de la Plata y del Paraguay, dado que fue perseguido y apresado por el virrey Francisco de Toledo que estaba contrariado por no haber logrado casar a su candidato matrimonial. Garay logró huir de Lima y se refugió en Santa Fe, y finalmente Torres de Vera asumiría en el cargo el 3 de diciembre del mismo año. Durante su gobierno la ya citada «Ciudad Zaratina de San Salvador» que fue atacada por los charrúas, Garay fue en su auxilio, valiéndole el ascenso a teniente de gobernador general de todas las provincias del Río de la Plata con sede en Asunción y también lo nombró como su lugarteniente.

La impopularidad del adelantado hizo que el procurador de Asunción, Juan Caballero Bazán, llevara las reclamaciones a la Audiencia de Charcas que fueran recibidas con favorable acogida. De esta forma, el 15 de septiembre del siguiente año sería apresado en Charcas y quedaría Osorio como su reemplazante.

Luis Osorio que había sido nombrado lugarteniente de Mendieta cuando fue a Charcas, entregó inmediatamente el gobierno a Juan de Garay el 15 de septiembre de 1578, por alegar que este tenía más derecho porque había sido nombrado lugarteniente por ambos adelantados cuando fundara y defendiera la «Ciudad Zaratina de San Salvador».

Una vez gobernador, Garay dirigió una campaña en 1579 a la región del Jejuy o Jejuí y a la zona de los ñuaras o Itatín —que desde el 13 de enero de 1596 fuera la tenencia de gobierno de Santiago de Jerez y el cual sería posteriormente el actual estado brasileño de Mato Grosso del Sur— y luego de derrotar al cacique Oberá, decidió fundar una ciudad en la región cercana a la laguna de Xarayes, Jarayes o Gran Pantanal del alto río Paraguay.

Para concretar su objetivo designó al teniente de gobernador del Guayrá, el capitán Ruy Díaz de Melgarejo, que partió de Asunción en 1580 con 60 soldados y hacia la latitud 19°S, sobre la orilla del este o diestra del río Mbotetey o Miranda que es afluente oriental del Paraguay, fundaría la primera ciudad de Santiago de Jerez aunque sería abandonada al poco tiempo por sus habitantes, ya que carecía de minas la región, además de no haber tráfico comercial y estar asediada continuamente por los guaycurúes.

En enero de 1580 Juan de Garay comenzó los preparativos de la segunda fundación de Buenos Aires. Se pretendía poblar la nueva ciudad con gente de Asunción, para lo cual se promulgó un bando ofreciendo tierras y otras mercedes. Se apuntaron 200 familias guaraníes y 76 de colonos. Se llevó todo lo necesario por el río en la carabela "Cristóbal Colón" y dos bergantines entre otras naves menores, expedición que salió el 9 de marzo del mismo año. Además de los colonos iban 39 soldados. Una parte del convoy fue por tierra y partió un mes antes.

El domingo 29 de mayo de 1580, Juan de Garay llegó a la boca del Riachuelo. Desembarcó justo en el lugar donde años antes lo había hecho el adelantado Pedro de Mendoza e instaló un campamento; la columna que viajaba por tierra llegó un mes después. Para el miércoles 11 de junio ya se había levantado un pequeño asentamiento, algo más hacia al norte de la fundación anterior, que dio base a la nueva ciudad de Buenos Aires. Ese día se celebraron las ceremonias fundacionales. Es importante recalcar una parte del acta fundacional:

El acta fundacional de la nueva urbe llama a esta «Ciudad de Trinidad», en recuerdo de su llegada que tuvo lugar el domingo de la Santísima Trinidad. El puerto de la misma recibió el nombre de «Santa María de los Buenos Aires». Ortiz de Zárate había denominado oficialmente a la región como «Nueva Vizcaya», en honor a su tierra natal.

Se plantó el «árbol de justicia» o símbolo de la ciudad, y tal como se acostumbraba y era obligatorio en tales casos, blandió la espada en las cuatro direcciones y dio un tajo a la tierra para señalar la posesión, y repartieron tierras entre los 63 pobladores que lo acompañaban, algunos presentes en la primera fundación.

Fueron nombrados alcaldes Rodrigo Ortiz de Zárate y Gonzalo Martel de Guzmán y se formó el Cabildo con seis regidores, siendo uno de ellos el general Alonso de Escobar, a la vez que se asignó el escudo de armas de la nueva ciudad, cuadrado blanco con águila negra coronada, con las alas totalmente desplegadas, sosteniendo la cruz roja de Calatrava en su pata derecha. También se asignaron encomiendas. Todo ello quedó registrado en el acta del acontecimiento redactada por el escribano Pedro de Jerez y tres testigos.

La nueva fundación fue atacada por los indígenas, mandados por su jefe Tabobá, pero Garay fue advertido del ataque por Cristóbal de Altamirano, que estaba prisionero de aquel, lo cual sirvió para organizar la defensa. En ese ataque el procurador Juan Fernández de Enciso dio muerte a Tabobá.

En octubre del citado año Garay volvió a Santa Fe y regresó a Buenos Aires en febrero del siguiente. A mediados de 1581 fue por tierra hasta cabo Corrientes —donde hoy se asienta la ciudad de Mar del Plata— en busca de la mítica «Ciudad de los Césares», regresando en enero de 1582, de donde retornó a Santa Fe y luego a Asunción, ciudad adonde comenzaría a ver que la nueva urbe podría desplazar su capitalidad.

En marzo de 1583, Juan de Garay acompañó a Sotomayor San Juan en el trayecto de Buenos Aires a Santa Fe. El convoy de botes estaba compuesto por 40 hombres, un franciscano y algunas mujeres. El 20 de marzo se desorientaron (entre las numerosas islas y lagunas del río Paraná) y entraron en una laguna desconocida, por lo cual Garay, algunos de sus hombres, el fraile franciscano y dos mujeres decidieron pasar la noche en tierra, a fin de no dormir incómodamente a bordo de la pequeña embarcación.
Su campamento fue atacado por los indios del lugar, que mataron a Garay, al franciscano, a una de las mujeres y a 12 de sus 40 soldados.

Si bien no está documentado el lugar exacto de aquellos hechos, existen varias hipótesis sobre su ubicación:








Se desconoce la etnia de los guerreros («cuarenta indios que habitaban por allí») que mataron a Garay. Los distintos historiadores mencionan a

El hidalgo Juan de Garay Ochandiano y Mendieta Zárate tuvo una relación prematrimonial con una aborigen chiriguana y solo se casó una vez con una infanzona hispano-extremeña:

1) - Se unió en matrimonio hacia 1558 en la ciudad de Asunción del Paraguay con Isabel de Becerra y Mendoza (Cáceres de la Extremadura castellana, ca. 1535 - Santa Fe la Vieja, ca. 1608), una hermana de Elvira de Becerra y Contreras Mendoza —quien se enlazó con el capitán Ruy Díaz Melgarejo, teniente de gobernador del Guayrá de 1575 a 1585 y fundador de Ciudad Real del Guayrá en 1556, de Villa Rica del Espíritu Santo en 1570 y Santiago de Jerez del Itatín en 1580— siendo ambas, las hijas del capitán Francisco de Becerra (n. Cáceres, 1511 - costa Mbiaza, 1553) y de Isabel de Contreras Mendoza (n. Medellín, ca. 1518), nietas maternas de Álvaro de Contreras y Carvajal (n. Badajoz, ca. 1480), alcaide de la fortaleza de Mérida, y de su esposa Juana Carrillo de Mendoza (n. ca. 1490).

Por lo tanto, ambas hermanas eran bisnietas maternas por la vía femenina de Álvaro de Mendoza y Luna, señor de La Torre de Esteban Hambrán desde 1502 —cuyos padres eran el segundo duque Íñigo López de Mendoza y María de Luna y Pimentel— y de su mujer Teresa Carrillo de Castilla, la cual a su vez era una hija de Alfonso Carrillo de Acuña y de su mujer Leonor Álvarez de Toledo y Guzmán, nieta paterna de María de Castilla (n. 1400), señora de Mandayona, y de su esposo Gómez Carrillo de Acuña y bisnieta paterna del infante Diego de Castilla —apresado durante 55 años en el castillo de Curiel de Duero por su tío Enrique de Trastámara, el cual fuera el incipiente rey de la nueva dinastía luego de la Primera Guerra Civil Castellana— y de su concubina Isabel de Salazar.

La citada familia de Isabel —padres y hermana— había arribado a la América del Sur en 1550 en el bergantín "La Concepción" del capitán Juan de Salazar Espinosa quien fuera el fundador de la ciudad de Asunción en 1537, y dirigido por Mencia Calderón Ocampo ""la Adelantada"", la cual acompañada por el hidalgo Fernando de Trejo y Carvajal fundarían en 1553 en la costa atlántica, el efímero poblado español de San Francisco de Mbiaza —poco más de un siglo después resurgiría como una villa portuguesa— y en donde nacería el futuro obispo tucumano Hernando de Trejo y Sanabria.

Antes del matrimonio, Juan de Garay le había confesado a su futura esposa de la existencia de su homónimo hijo natural de unos tres años de edad, quien por respuesta obtuviera que ella misma ""desearía criarlo a la usanza castellana"".

Fruto del enlace entre Juan de Garay e Isabel Becerra hubo seis hijos legítimos documentados:

2) - Fruto de una unión prematrimonial de Juan Garay con una manceba aborigen de nombre desconocido —una hija del cacique cautivo de la etnia chiriguano o avá guaraní— había tenido un hijo mestizo e ilegítimo que sería criado con costumbres europeas:




</doc>
<doc id="3337" url="https://es.wikipedia.org/wiki?curid=3337" title="Química orgánica">
Química orgánica

La química orgánica es la rama de la química que estudia una clase numerosa de moléculas que en su gran mayoría contienen carbono formando enlaces covalentes: carbono-carbono o carbono-hidrógeno y otros heteroátomos, también conocidos como compuestos orgánicos. 
Debido a la omnipresencia del carbono en los compuestos que esta rama de la química estudia, esta disciplina también es llamada química del carbono.

La química orgánica constituyó o se instituyó como disciplina en los años treinta. El desarrollo de nuevos métodos de análisis de las sustancias de origen animal y vegetal, basados en el empleo de disolventes, como el éter o el alcohol, permitió el aislamiento de un gran número de sustancias orgánicas que recibieron el nombre de ""principios inmediatos"".
La aparición de la química orgánica se asocia a menudo al descubrimiento, en 1828, por el químico alemán Friedrich Wöhler, de que la sustancia inorgánica cianato de amonio podía convertirse en urea, una sustancia orgánica que se encuentra en la orina de muchos animales. Antes de este descubrimiento, los químicos creían que para sintetizar sustancias orgánicas, era necesaria la intervención de lo que llamaban ‘la fuerza vital’, es decir, los organismos vivos. El experimento de Wöhler rompió la barrera entre sustancias orgánicas e inorgánicas. Los químicos modernos consideran compuestos orgánicos a aquellos que contienen carbono e hidrógeno, y otros elementos (que pueden ser uno o más), siendo los más comunes: oxígeno, nitrógeno, azufre y los halógenos.

En 1856, "sir" William Henry Perkin, mientras trataba de estudiar la quinina, accidentalmente fabricó el primer colorante orgánico ahora conocido como malva de Perkin.

La diferencia entre la química orgánica y la química biológica es que en la química biológica las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es sólo testigo de su presente, sin pasado y sin evolución histórica.






La tarea de presentar la química orgánica de manera sistemática y global se realizó mediante una publicación surgida en Alemania, fundada por el químico Friedrich Konrad Beilstein (1838-1906). Su "Handbuch der organischen Chemie" (Manual de la química orgánica) comenzó a publicarse en Hamburgo en 1880 y consistió en dos volúmenes que recogían información de unos quince mil compuestos orgánicos conocidos. Cuando la "Deutsche chemische Gesellschaft" (Sociedad Alemana de Química) trató de elaborar la cuarta reedición, en la segunda década del siglo XX, la cifra de compuestos orgánicos se había multiplicado por diez. Treinta y siete volúmenes fueron necesarios para la edición básica, que aparecieron entre 1916 y 1937. Un suplemento de 27 volúmenes se publicó en 1938, recogiendo información aparecida entre 1910 y 1919. En la actualidad, se está editando el "Fünftes Ergänzungswerk" (quinta serie complementaria), que recoge la documentación publicada entre 1960 y 1979. Para ofrecer con más prontitud sus últimos trabajos, el "Beilstein Institut" ha creado el servicio "Beilstein On line", que funciona desde 1988. Recientemente, se ha comenzado a editar periódicamente un CD-ROM, "Beilstein Current Facts in Chemistry", que selecciona la información química procedente de importantes revistas. Actualmente, la citada información está disponible a través de internet.

La gran cantidad de compuestos orgánicos que existen tiene su explicación en las características del átomo de carbono, que tiene cuatro electrones en su capa de valencia: según la regla del octeto necesita ocho para completarla, por lo que forma cuatro enlaces (valencia = 4) con otros átomos. Esta especial configuración electrónica da lugar a una variedad de posibilidades de hibridación orbital del átomo de carbono (hibridación química).

La molécula orgánica más sencilla que existe es el metano. En esta molécula, el carbono presenta hibridación sp3, con los átomos de hidrógeno formando un tetraedro.

El carbono forma enlaces covalentes con facilidad para alcanzar una configuración estable, estos enlaces los forma con facilidad con otros carbonos, lo que permite formar frecuentemente cadenas abiertas (lineales o ramificadas) y cerradas (anillos).

La clasificación de los compuestos orgánicos puede realizarse de diversas maneras: atendiendo a su origen (natural o sintético), a su estructura (p. ej.: alifático o aromático), a su funcionalidad (p. ejm.:alcoholes o cetonas), o a su peso molecular (p. ej.: monómeros o polímeros).

La clasificación de los compuestos orgánicos según el origen es de dos tipos: naturales o sintéticos. A menudo, los de origen natural se entiende que son los presentes en los seres vivos, pero no siempre es así, ya que algunas moléculas orgánicas también se sintetizan "ex-vivo", es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Halle-Bopp.

Los compuestos orgánicos presentes en los seres vivos o "biosintetizados" constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en medicina, farmacia, perfumería, cocina y muchos otros campos más.

Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados "azúcares" pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa).
Se suelen clasificar según su grado de polimerización en:

Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se les llama incorrectamente grasas, ya que las grasas son sólo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides).

Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así "poliamidas naturales" ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia muy importante de moléculas en los seres vivos pero en especial en el reino animal. Por otra parte, son producto de la expresión de genes contenidos en el ADN. Algunos ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña.

Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por la moléculas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato.Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN. 

Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran "pequeñas" aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina.

Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales.

El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos.

En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp. Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales, este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final.

Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras téxtiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos.

El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser:

Los radicales son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono "met"-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente...) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el "2-metilbutano". En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el "2-etil, 5-metil, 8-butil, 10-docoseno".

Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos, azo, nitro o sulfóxido, entre otros.

Son cadenas de carbonos con uno o varios átomos de oxígeno.
Pueden ser:

El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras.

Dependiendo de la cantidad de grupos -OH que forman parte del alcohol, el mismo puede ser clasificado como monohidroxilado (presencia de un hidroxilo) o polihidroxilado (dos o más grupos hidroxilos en la molécula).





Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol.

Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4"n"+2 electrones en orbitales π (n=0,1,2...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en:

Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química pero distintas estructuras y propiedades.
Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional.
El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula CH puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula CHO.

Los compuestos orgánicos pueden dividirse de manera muy general en:


Una de las principales relaciones entre la química orgánica y la biología es el estudio de la síntesis y estructura de moléculas orgánicas de importancia en los procesos moleculares realizados por los organismos vivos, es decir en el metabolismo. La bioquímica es el campo interdisciplinar científico que estudia los seres vivos, y ya que estos usan compuestos que contienen carbono, la química orgánica es imprescindible para comprender los procesos metabólicos.

En términos biológicos la química orgánica es de gran importancia sobre todo en un contexto celular y esto lo podemos ejemplificar con moléculas como los carbohidratos, presentes desde la membrana plasmática así como en la estructura química del ADN, los lípidos quienes son la base principal de la membrana plasmática, las proteínas que ayudan a dar sostén a un organismo o sus funciones como enzimas y el ADN, molécula encargada de resguardar la información genética de los organismos vivos.




</doc>
<doc id="3341" url="https://es.wikipedia.org/wiki?curid=3341" title="León (España)">
León (España)

León (pronunciado: ) (en leonés, "Llión") es un municipio y ciudad española ubicada en el noroeste de la península ibérica, capital de la provincia homónima, en la comunidad autónoma de Castilla y León. León contaba en enero de 2019 con habitantes repartidos en una superficie de 39,03 km², y un área metropolitana de 203 191 habitantes según el mapa de áreas funcionales de la Junta de Castilla y León (otros proyectos dan cifras diferentes), distribuidos en quince municipios, siendo así la segunda más poblada de la comunidad.

Nacida como campamento militar romano de la "Legio VI Victrix" hacia el 29 a. C., su carácter de ciudad campamental se consolidó con el asentamiento definitivo de la "Legio VII Gemina" a partir del año 74. Tras su parcial despoblación con motivo de la conquista musulmana de la península, León recibió un nuevo impulso como parte del Reino de Asturias. En 910 comenzó una de sus etapas históricas más destacadas al convertirse en cabeza del Reino de León, participando activamente en la Reconquista contra los musulmanes, llegando a ser uno de los reinos fundamentales en la configuración del Reino de España. La ciudad albergó las primeras Cortes de la historia de Europa en 1188, bajo el reinado de Alfonso IX, gracias a lo cual en 2011 fue proclamada por la Junta de Castilla y León como Cuna del Parlamentarismo. Desde la Baja Edad Media la ciudad dejó de tener la importancia de antaño, en parte debido a la pérdida de su independencia tras la unión del reino leonés a la Corona castellana, definitiva desde 1301.

Sumida en un período de estancamiento durante la Edad Moderna, en la guerra de la Independencia fue una de las primeras ciudades en sublevarse de toda España, y años después del fin de la misma, en 1833, adquiriría su rango de capital provincial. La llegada del siglo XX trajo consigo el Plan de Ensanche, que acrecentó la expansión urbanística que venía experimentando desde finales del siglo XIX, cuando la ciudad se convirtió en un importante nudo de comunicaciones del noroeste con motivo del auge de la minería del carbón y de la llegada del ferrocarril.

Su patrimonio histórico y monumental, así como diversas celebraciones que tienen lugar a lo largo del año, entre las que destaca la Semana Santa, y su situación como paso obligado del Camino de Santiago, considerado Patrimonio de la Humanidad por la UNESCO, la convierten en una ciudad receptora de turismo nacional e internacional. Entre sus monumentos más representativos se encuentran la Catedral de Santa María de Regla, el mejor ejemplo del gótico clásico de estilo francés en España, la Basílica de San Isidoro, una de las iglesias románicas más importantes de España, tumba de los reyes de León medievales y considerada como "La Capilla Sixtina del Arte Románico", el Monasterio de San Marcos, primer ejemplo de la arquitectura plateresca y renacentista española, el palacio de Los Guzmanes, el palacio de los Condes de Luna, la iglesia del Mercado o del Camino la Antigua, la iglesia de Palat del Rey, la Casa de las Carnicerías y la Casa Botines, de estilo modernista y realizada por el genial arquitecto catalán Antoni Gaudí; todos ellos declarados Bien de Interés Cultural. Ejemplo destacado de arquitectura moderna, y uno de los museos de la ciudad, es el MUSAC, de Mansilla + Tuñón Arquitectos.

León dispone de una red desarrollada de carreteras y ferrocarril, además de contar con un aeropuerto.

La ciudad está inmersa en el desarrollo de distintos proyectos como la reconversión del Feve en tranvía o el palacio de congresos, entre otros. El proyecto del tranvía suscitó varias críticas por parte del Partido Popular, entonces en la oposición municipal, que alegaba que su desarrollo en una ciudad como León era un proyecto faraónico y de dudosa viabilidad y anunciaron que bajo su gobierno no desarrollarían tal proyecto.

La Universidad de León, fundada en 1979 como escisión de la Universidad de Oviedo, contaba en el curso 2006-07 con 13 217 alumnos; tiene su sede en la ciudad y está catalogada, a partir de criterios como la demanda universitaria, los recursos humanos o los planes de estudio, como la 2.ª universidad de Castilla y León, tras la Universidad de Salamanca, y la 30.ª de España. Desde el 4 de mayo de 2010, la ciudad alberga la segunda sede de la Universidad de Washington en Europa, tras su sede de Roma, con capacidad para 500 alumnos interesados en el aprendizaje del español. Desde 2011 la ciudad cuenta también con una sede del Instituto Confucio.

El origen del nombre de la ciudad proviene de la palabra latina "legio" en su forma de caso acusativo "legionem", que hace referencia a la Legio VII Gemina o Legión Séptima Gemela que fundó la ciudad en su actual emplazamiento. Esta tesis, comúnmente aceptada, propicia el gentilicio culto «legionense» para referirse a los habitantes de la ciudad, que coexiste con el popular «leonés». La evolución de "Legione(m)" (con pronunciación suave de la g) a "León" pasó por etapas intermedias como 'Leyone' o 'Leyón' .

El escudo de León está compuesto por un campo de plata en el que figura un león rampante de púrpura, linguado, uñado, armado de gules y coronado de oro. Aparece timbrado con una corona abierta de oro (la forma de la antigua corona real, usada hasta el siglo XVI). En el escudo de la ciudad de León aparece representada una corona marquesal en vez de la antigua real y el león no figura coronado.

Al producirse la unión en el año 1230 de las Coronas de León y Castilla con Fernando III el Santo se dispuso que en el escudo del rey los elementos heráldicos castellanos (un castillo almenado de oro sobre un campo de gules) y leoneses formaran un escudo cuarteado. Es de destacar que en los cuartelados no había sitio para dos leones, hasta aquel momento pasantes, por lo que se les situó como rampantes para ocupar por completo los cuarteles que les correspondían. Esta es la disposición que ha llegado a la actualidad. El uso de la corona sobre la cabeza del león no apareció documentado hasta el reinado de Sancho IV de Castilla y León (1284-1295).

Actualmente el escudo de León es el símbolo de la provincia y, acompañado por adornos exteriores, de la ciudad de León.


La ciudad de León está ubicada en una terraza fluvial en la confluencia de los ríos Bernesga y Torío, a una altitud de 840 msnm. Situada aproximadamente en el centro de la provincia, se encuentra en un lugar estratégico del noroeste peninsular, ya que es paso obligado para ir a Galicia y a Asturias.

Su término municipal limita al norte con Sariegos y con Villaquilambre, al este con Valdefresno, al sur con Santovenia de la Valdoncina, Onzonilla y Villaturiel, y al oeste con San Andrés del Rabanedo y Valverde de la Virgen. El territorio del término municipal está representado en la hoja 161 del Mapa Topográfico Nacional.


Situado en la transición del Páramo Leonés a la cordillera Cantábrica, su ubicación en la confluencia de dos ríos hace que la capital leonesa se asiente en una zona predominantemente llana, si bien según se aleja del núcleo urbano el terreno se eleva, encontrándose por el norte con el Monte de San Isidro y por el este con los altos en los que se encuentra Golpejar de la Sobarriba. En el término municipal se encuentran los vértices geodésicos de Valenciano, a una altitud de 938 msnm, y de San Isidro, a una altitud de 939 metros. En centro de la ciudad se encuentra a una altitud de 837 metros, mientras que la altitud del municipio varía desde los 800 metros en el último tramo en la localidad del río Bernesga hasta los 944 metros en el norte del municipio.


León está bañada por los ríos Bernesga, que recorre la ciudad por el oeste, y el Torío, que la delimita por el este, situándose la mayor parte del núcleo urbano entre los dos cauces. A su paso por la ciudad, se encuentran canalizados y adecuados para el paseante, con jardines y paseos peatonales. La confluencia de ambos se sitúa a la altura del polígono de La Lastra, donde el Torío vierte sus aguas en el Bernesga.

Sobre el río, y en el centro de la ciudad, se encuentra el Aula de Interpretación de las Energías Renovables de León, perteneciente al Ayuntamiento de León. Es un aula destinada a enseñar a sus visitantes las soluciones complementarias y alternativas que proporcionan las energías renovables al sistema energético actual, pretendiendo ser un referente en ese aspecto en la comunidad autónoma de Castilla y León.

Se trata de un edificio situado en los márgenes del río Bernesga junto al Puente de los Leones, construido tras un acuerdo alcanzado por el EREN y el Ayuntamiento de León. El Aula posee un espacio de exposiciones sobre el medio ambiente y cuenta con una instalación solar térmica, una instalación solar fotovoltaica y una minicentral hidroeléctrica. La electricidad generada por estas tres últimas se incorpora a la red eléctrica general para su posterior utilización, siendo capaz de dar luz a 1100 familias.

El clima de León es oceánico mediterráneo de tipo Csb de acuerdo a la clasificación climática de Köppen.

Las precipitaciones están repartidas, como es habitual en los climas mediterráneos, de forma muy irregular a lo largo del año, con mínimos en la época estival y máximos durante primavera y otoño. La precipitación media anual es de 556 mm. La ciudad disfruta al año de 2624 horas de sol al año y de 78 de lluvia, además de 16 de tormenta.

Las temperaturas son frescas, con una media anual de 11,1 °C según los datos de la estación meteorológica de La Virgen del Camino, con inviernos fríos, siendo frecuentes las heladas (74 días de helada de media al año). La nieve hace acto de presencia en la capital leonesa durante 16 días de media al año, si bien las grandes nevadas no son frecuentes salvo en fechas como diciembre de 2009, cuando la ciudad y parte de la provincia se colapsaron debido a un temporal de frío y nieve durante el cual se registraron temperaturas mínimas históricas en algunos lugares y obligó a la UME a intervenir para hacer frente a las complicaciones derivadas del mismo. El verano es caluroso, suavizado por la altitud de la ciudad, con temperaturas máximas que rondan los 27 °C.

A continuación se muestran los datos del observatorio meteorológico de la AEMET situado en el Aeropuerto de León a 916 msnm, en el municipio de Valverde de la Virgen muy cerca de la ciudad de León. El periodo de referencia es 1981-2010 también para las extremas.

La ciudad de León surge hacia 29 a. C. como campamento militar romano de la Legio VI Victrix, en la terraza fluvial entre los ríos Bernesga y Torío, cerca de la ciudad astur de Lancia, con motivo de las llamadas guerras cántabras. A finales del siglo I, a partir de 74, el campamento es ocupado por la Legio VII Gemina, fundada por Galba, la cual permanecerá en León hasta aproximadamente principios del siglo V. Fue la única legión asentada en Hispania hasta la caída del Imperio Romano de Occidente (476), por lo que durante todo este tiempo León fue la capital militar de la Península. La ciudad perteneció al Convento Asturicense, con capital en Asturica Augusta, el cual formó parte de la provincia Tarraconense hasta el siglo III, cuando, con la creación de la provincia de Gallaecia, fue integrado en ésta. 

El trazado campamental romano original aún puede observarse en la actualidad, puesto que se conservan gran parte de las murallas que lo rodeaban en los siglos III y IV. Alrededor de las murallas que delimitaban el campamento fue creándose un núcleo civil paralelo, la cannaba, en la que se asentaban todas las personas que se encargaban de cubrir las necesidades de los soldados. Por los restos arqueológicos se sabe que contaba con unas termas (con ruinas aún visibles bajo la catedral) e incluso un anfiteatro con capacidad para 5000 espectadores a extramuros, actualmente enterrado bajo la calle Cascalerías.

Tras el período romano, la ciudad formó parte del Reino suevo y posteriormente, tras su conquista, del Reino visigodo. Entre los siglos VI y VIII la escasez de evidencias arqueológicas proyectan una imagen carente de vitalidad urbana, con una clara reducción del espacio habitado, aunque el descubrimiento de cerámicas pertenecientes al periodo omeya cordobés cerca de Puerta Obispo nos indica que la ciudad no fue abandonada completamente, sino que conservó cierta población estable. La ciudad fue conquistada, durante la invasión musulmana de la península, en el año 712, siendo recuperada en el 754 por Alfonso I aunque debido a su condición fronteriza se mantendría deshabitada durante casi un siglo.

Tendremos que esperar hasta el año 846 cuando un grupo de mozárabes intentó repoblar la ciudad, que pese haber permanecido despoblada al estar en el centro de la línea de combate entre cristianos y musulmanes aún conservaba sus murallas romanas. El intento fue frustrado por un ataque omeya que mantendría la ciudad despoblada hasta el año 853 en el que Ordoño I incorpora de forma efectiva la ciudad al Reino de Asturias, repoblándola con éxito. Sería finalmente con Ordoño II, que ocupó el trono (914 - 924) tras la muerte de su hermano García I, cuando la ciudad se convierte en capital del reino astur, iniciando el Reino de León.

La ciudad de León fue sede regia desde la fundación del reino, con García I, a principios del siglo X, hasta la unión con Castilla en 1230, momento en que la capitalidad del reino unificado fue itinerante y debido a ello León fue creciendo y evolucionando en su desarrollo. En esta cuestión jugó un destacado papel el Camino de Santiago, quizás la más importante vía de circulación de gentes, ideas, cultura y arte del Medievo. En el siglo X destacarían reyes como Ordoño II, que fijó la capitalidad y consagró la primera catedral en las antiguas termas romanas, donde hoy se encuentra la catedral gótica, y su hijo Ramiro II, que construyó el primer palacio en Palat de Rey y, al igual que su padre, llevó a cabo exitosas campañas contra los musulmanes. La segunda mitad del siglo es de luchas civiles en León, reyes débiles con problemas con la nobleza, y de ataques y contraataques musulmanes a la ciudad, incluyendo uno de Almanzor, que causó graves daños. La recuperación y reordenamiento de la capital llegó con Alfonso V a inicios del siglo XI, así como el comienzo de la victoria cristiana en la península. Avanzado el siglo, hay un cambio de dinastía, destacando a Fernando I como rey iniciador de la basílica de San Isidoro, construida con motivo del traslado de los restos de San Isidoro a la ciudad y del panteón real del reino. Su sucesor Alfonso VI que pasó a los anales de historia por el avance en la reconquista con la conquista de Toledo y, sobre todo, por su relación política con el Cid, fue cuyo reinado presenció la consagración de la nueva catedral románica iniciada por Urraca en 1073, donde presumiblemente trabajarían los mismos canteros que en la basílica.

En el siglo XII, y tras el paso de la primera reina, Urraca I, destaca su hijo Alfonso VII, que avanzó notablemente la reconquista y llegó a coronarse emperador de toda Hispania en la antigua catedral leonesa. Es en este siglo cuando el geógrafo y viajero árabe Edrisi escribió lo siguiente sobre León: ""Allí se practica un comercio muy provechoso. Sus habitantes son ahorradores y prudentes"". Tenemos también noticia de León a través de diversos códices, entre ellos el "Codex Calixtinus", manuscrito que, entre otras cosas, contiene información sobre la ruta que los peregrinos seguían hacia Santiago de Compostela. Con todo ello, la ciudad conoció el desarrollo de nuevos barrios, en ocasiones extramuros de una ciudad que ya se quedaba pequeña, y casi siempre a la vera del camino de los peregrinos, que accedían a la ciudad por la llamada "Puerta Moneda". 

Tras la muerte de Alfonso VII, este dividió los reinos de León y Castilla entre sus hijos; Fernando II reinó en León, destacando la reconquista de Extremadura. Su sucesor y último rey privativo de León fue Alfonso IX, que convocó las primeras cortes de Europa, con participación de todos los estamentos sociales, en la basílica de San Isidoro en 1188. Será en 1230, cuando tras su muerte la corona leonesa y la castellana recaen sobre la cabeza del monarca Fernando III el Santo, algo que supondría para León la pérdida de la capitalidad fija, pues esta se vuelve itinerante. No supondría no obstante el fin de la prosperidad de la ciudad, que durante todo el siglo XIII mantuvo un gran empuje comercial y crecimiento demográfico. Es en esta época cuando a mediados de siglo, Alfonso X el Sabio ordenó el derribo de la vieja catedral y la construcción de la actual, de estilo gótico.

Tras la unión del reino leonés con Castilla pese a la pérdida de relevancia política la ciudad se mantuvo prospera y es durante el siglo XIII cuando la catedral de León se construiría, reiniciando su construcción por orden de Alfonso X en 1255 finalizando en 1302 la totalidad del templo. Durante el siglo XIV, León experimentó una crisis económica que vino acentuada por una serie de acontecimientos climáticos en toda Europa que mermaron las cosechas, produciendo hambrunas y endeudamiento de los campesinos. Estas circunstancias fueron agravadas con la llegada de la peste a León entre los años 1349 y 1350, la cual provocó una gran mortandad en la zona, despoblando pueblos y mermando, según fuentes de la época, en más de un cuarto la población de la zona. A esta serie de fatalidades se le unieron una inestabilidad política en toda la Corona castellana que produjo continuas tensiones que a menudo desembocaron en conflictos armados.

Con la llegada del Siglo XV, las cosas comenzaron a mejorar, observándose un incremento notable en la población en la edificación de nuevas casas, reconstrucción de las anteriores y ensanche de los arrabales. Se hablaba en estos años de hacer una cerca que comprendiese el arrabal de la parte oriental de la ciudad, abarcando las iglesias de San Lorenzo, San Pedro de los Huertos y San Salvador del Nido de la Cigüeña. Así, la ciudad de León, a finales de siglo, contaba con una población entre los cuatro y cinco mil habitantes mientras que ciudades vecinas como Salamanca y Burgos tienen quince mil y diez mil habitantes.


En el siglo XVI, la Guerra de las Comunidades contra Carlos I en León destacó por un insólito fervor comunero en el cabildo catedralicio y en los barrios extramuros. En la órbita local, las dos familias dominantes de aquella época, los Guzmanes, por parte de los comuneros, y los Quiñones, por parte del rey, hicieron de la guerra la excusa perfecta para resolver sus diferencias.

En estos siglos, León vive un estancamiento de su población, algo normal en las ciudades del interior. El leve incremento poblacional en la ciudad no se debe a un incremento de la actividad industrial o comercial, sino al empuje de la agricultura de las zonas rurales que rodean la ciudad. Prueba de la decadencia comercial e industrial de la ciudad es lo acontecido con las fábricas de hilados. En 1749, bajo los auspicios del secretario de Estado, José de Carvajal y Lancaster, se levanta un edificio en el llamado Campo de San Francisco para ampliar la fábrica de hilaturas que ya funcionaba en la calle de la Rúa, pero en 1769 esta fábrica ya había dejado de funcionar. El empeño puesto por el secretario de Estado contó con la oposición de las autoridades locales. Según Real Orden de 24 de enero de 1786, a instancias del obispo Cuadrillero, se crea en este edificio un hospicio, cuya obra se completa en 1793. También hubo intentos, con la ilustración, de modernizar la ciudad y sanearla con la construcción de nuevas fuentes y equipamiento público, así como con la creación de una de las Sociedades Económicas de Amigos del País en la ciudad.

La ciudad de León, con 5500 habitantes (aunque algunos viajeros, como el reverendo J. Townsend, aumenten la cifra a 6170 almas) era, junto con Zamora, una de las ciudades menos pobladas de la región y de la meseta. Las malas condiciones higiénicas y el hacinamiento contrarrestaban el avituallamiento regular y asegurado por los municipios en épocas de crisis. Además, en épocas de malas cosechas, atraían a mendigos, vagabundos y marginados de los amplios alrededores que, agrupados en las puertas de conventos y obispados, esperaban unas relativas garantías de no morir de hambre, introduciendo en la ciudad epidemias que aumentaban la tasa de mortalidad.

En los días previos al estallido de la Guerra de la Independencia, en concreto el 24 de abril de 1808, tuvo lugar en León, al mismo tiempo que una serie de incidentes acaecidos en otras ciudades españolas como Burgos, Toledo o Madrid, una manifestación popular en favor de Fernando VII ante el miedo de que Carlos IV, el cual contaba con el favor de los franceses, volviera a reinar, suponiendo, por tanto, un rechazo a Napoleón. El 26 de julio de ese mismo año la ciudad caería ante el general galo Jean-Baptiste Bessières. Retomado su dominio en junio de 1812, sólo volvió a manos francesas durante un breve período en 1813, pero acto seguido los franceses se replegaron totalmente, volviendo la ciudad a la normalidad.

En 1833 la ciudad adquirió el rango de capital de su provincia, la cual formaría parte, junto a Zamora y Salamanca, de la Región de León. Entre finales de siglo y principios del XX, el desarrollo de la minería del carbón la convirtió en nudo comercial y de comunicaciones fundamental en todo el noroeste, con el desarrollo de diversas infraestructuras, entre las que destacan la construcción de su estación de ferrocarril (luego propiedad de Renfe y hoy, de Adif) para vías de Ancho Ibérico, y el trazado de una línea de Ferrocarril de vía estrecha, conocida como "El hullero", que, desde León, conectaba las principales zonas de extracción carbonífera con el núcleo industrial de Bilbao.

En León, antes de la desamortización promovida por Juan Álvarez Mendizábal, gran parte de las tierras circundantes pertenecían a la iglesia, por lo que la ciudad tenía constreñido su crecimiento y carecía de una red vial adecuada, ya que las infraestructuras existentes se encontraban anticuadas, insuficientes para satisfacer las necesidades del nuevo y creciente tráfico rodado. Con la desamortización, las nuevas tierras desamortizadas quedaban libres para el desarrollo urbanístico, marcando un antes y un después en el desarrollo urbano de la ciudad que comenzó a superar su casco medieval. El ensanche, que es una forma de ordenar el espacio entre la ciudad y la nueva estación de tren, así como otros barrios de extrarradio, surgieron en las tierras eclesiásticas ahora libres. De todas las propiedades expropiadas, la que más destaca, es San Marcos, el cual fue vendido a la diputación por 985 700 reales, precio en el que había sido tasado.

En 1863 llega el ferrocarril a la ciudad, ubicándose la nueva estación en la margen derecha del río. Son las instalaciones de la estación las que se convertirán en el principal factor dinamizador del crecimiento urbano leonés durante la segunda mitad del siglo XIX y primeras décadas del siglo XX. La estación constituía un núcleo de fijación y expansión constituida por sus propias instalaciones, conectadas mediante otras obras públicas y accesos a la ciudad. La situación de la estación, en la margen derecha del río Bernesga, y al oeste de la ciudad vieja, fue determinante para el desarrollo urbano de los siguientes años, ya que la ciudad se expandió prioritariamente hacia esa zona. En los alrededores de la propia estación comenzaron a instalarse industrias interesadas, empezando a aparecer las primeras agrupaciones de población obrera que trabajaba en el ferrocarril, formando así el barrio de la Estación en torno a la misma.

A medida que avanzaba el siglo XIX, la vieja ciudad medieval se revelaba como un marco vital cada vez más inadecuado para satisfacer las necesidades de la población. La ocupación del espacio era más densa y en la misma proporción crecían las necesidades de vivienda, a la vez que la movilidad comercial aparecía colapsada por una estructura vial construida siglos atrás. La ausencia o ineficacia de sistemas de evacuación de todo tipo de residuos, junto a la no existencia de una reglamentación estricta en materia higiénico-sanitaria, propiciaba un medio ambiente insano, responsable de las grandes plagas que atacaron a la población española a lo largo del siglo XX.


La situación del nuevo foco de desarrollo en torno a la estación potencia la unión de la plaza de Santo Domingo y del recinto amurallado con esta zona; tomando desde entonces ya cierta importancia Ordoño II, entonces "Paseo de las Negrillas", importancia que se confirma con la construcción de un nuevo puente de hierro sobre el Bernesga en 1871. Este cambio en la situación urbana de la ciudad induce al ayuntamiento a dar contenido teórico al ensanche, siendo el jefe municipal de obras públicas, José Manuel Ruiz de Salazar, quién define los elementos urbanísticos que ha de contener el nuevo barrio de la ciudad. Este primer estudio ya define algunos elementos característicos que se han conservado en la actualidad, tales como el carácter vertebrador de Ordoño II en la nueva trama urbana. Así mismo, también recoge la creación de un pulmón verde, el "Paseo de Invierno", que conecta el jardín de San Francisco con el Bernesga, que tiene su equivalencia actualmente en la avenida Lancia de la capital leonesa.

Dada la escasa viabilidad del estudio de 1889, el ayuntamiento convoca un concurso siete años más tarde que englobaba los terrenos del anterior estudio menos los pertenecientes al monasterio de San Claudio, quedándose la actuación en 55 hectáreas. Las expectativas de crecimiento que justificaban el ensanche se basaban en el crecimiento ferroviario, la centralidad adoptada por la ciudad en los importantes negocios mineros leoneses y en las expectativas militares de la ciudad, donde se sopesaba la creación de una capitanía general. El único trabajo que se presenta recoge todos los elementos esenciales en la trama viaria que el ayuntamiento de León exigía en el concurso. Así, el proyecto recoge como centros de referencia las plazas de Santo Domingo, centro geométrico tras la expansión de la ciudad con el ensanche, la plaza de Guzmán el bueno, centro de distribución del tráfico entre la ciudad nueva y la vieja y la plaza de San Marcos, donde se buscaba conectar la ciudad vieja con el antiguo convento, ya por entonces declarado monumento nacional. Por todo ello, el proyecto original planteaba la Gran Vía de San Marcos como principal eje viario, siguiendo los preceptos del ensanche de Barcelona. El trazado de manzanas cuadriculadas, de una importante extensión, marcó el trazado de esta vía y una vía "diagonal", Ordoño II, se ordenó con un sistema de manzanas triangulares. El proyecto se vino abajo de inmediato, con una serie de modificaciones que no permitieron su aprobación hasta 1935.

La falta de recursos impidió al ayuntamiento a acudir a la expropiación como vía para adquirir los terrenos, por lo que únicamente tuvo que negociar directamente con los propietarios, que forzaran la inclusión de modificaciones en el proyecto. Estas modificaciones suponen la apertura de nuevas calles secundarias perpendiculares a Ordoño II, que en sustitución de Gran via se convertiría en la principal vía del nuevo barrio, así como la apertura de nuevas calles principales no contempladas en el proyecto inicial como Burgo Nuevo y Fajeros. El desarrollo del ensanche fue lento por la normativa municipal que prohibía la implantación de industrias y viviendas obreras, que se verían también ahuyentadas del sector por un coste del terreno que hacía inasumible la inversión por lo que el desarrollo se realizaría en función de la demanda de la clase acomodada de la ciudad.

Tras la sublevación de julio de 1936, que dio lugar al inicio de la Guerra Civil, la mayor parte de la provincia quedó en manos de los sublevados. En León, la sublevación de la guarnición tuvo lugar el 20 de julio, una vez que la columna minera, que desde Asturias se dirigía a Madrid, hubo dejado la ciudad. Las tropas sublevadas, con el general Carlos Bosch y Bosch como gobernador militar, el coronel Julián Rubio López en el aeródromo de la Virgen del Camino, y los guardias civiles y de asalto que había en la provincia, controlaban la zona, contando pronto con la ayuda de tropas venidas de Galicia, al mando del comandante López Pita.

La resistencia fue escasa y los cargos públicos del Frente Popular, entre ellos el alcalde Miguel Castaño, fueron arrestados, condenados a muerte y ejecutados por fusilamiento.

Los republicanos, por su parte, establecieron en León cuatro comandancias: las de Belmonte, Puerto Ventana, Pola de Gordón y Cangas de Onís. El Comité provincial de milicias antifranquistas, al igual que el resto de comités del Consejo Provincial del Frente Popular, se diluyó en Consejo Provincial del Frente Popular y luego en el Consejo Interprovincial de Asturias y León. En 1937, los republicanos intentaron sus últimos ataques, pero desde el mes de septiembre, la ofensiva franquista se generalizó, recuperando los puertos de montaña y dando fin a la guerra en el norte el 21 de octubre de 1937.

Tras el impás de la guerra, la ciudad continúo creciendo con normalidad, recibiendo oleadas de inmigrantes, en su mayoría obreros o empleados de baja cualificación en busca de empleo en la industria y los servicios. El problema fue inmediato ya que, si bien la ciudad ofrecía puestos de trabajo para atraer a inmigrantes, la falta de vivienda distaba de satisfacer las necesidades de estos nuevos inquilinos. Se daban además dos características: el Casco Antiguo estaba saturado y aún con un uso intensivo de las viviendas, donde era imposible alojar a tanta gente, y por otra parte, el Ensanche, donde debido a las normas municipales, estaba prohibido edificar casas de obreros. La solución a estos problemas fue la de iniciar la construcción de barrios obreros a las afueras, iniciando así la expansión suburbial de la ciudad. Comienzan así las llamadas parcelaciones particulares, en las que el propietario de una finca la parcelaba, vendiéndola después con gran beneficio económico. El Ayuntamiento, por tolerancia o por incapacidad, fue dejando que se urbanizaran estos nuevos barrios sin los requisitos mínimos de infraestructuras como agua, electricidad o el acceso a la sanidad y la educación, creándose así urbanizaciones de ínfima calidad, que sólo pudieron ser mejoradas con el concurso municipal para dotarlas de servicios año más tarde. De esa manera, al norte surgen barrios como San Esteban, San Mamés, Mariano Andrés, Las Ventas y La Inmaculada. 


A mediados de los años 50 se inician en León los proyectos para elaborar un Plan General de Ordenación Urbana, aprobándose definitivamente en 1960. Todas las actuaciones urbanísticas en la ciudad y en el municipio quedaban, por tanto, sujetas a los criterios, métodos y disciplina sancionados legalmente.

En los años 60, las ciudades españolas, y León no era una excepción, experimentan importantes crecimientos. La localización de la industria y los servicios, las demandas del éxodo rural, la construcción de viviendas y la especulación sin límites otorgaban al crecimiento urbano las características de gran negocio. En estas condiciones, y ante perspectivas inmobiliarias tan prometedoras, la Ley del Suelo queda convertida en un estrecho marco de legalidad urbanística que no hace otra cosa que entorpecer el libre juego de las fuerzas económicas de la ciudad. Consecuentemente, los Planes de Ordenación no se cumplen, siendo más grave aún la imposibilidad de que movimientos socio-políticos organizados puedan denunciarlo y reivindicarlo. Particularmente, se incumplen aquellas partes del Plan dedicadas a la previsión y provisión de espacios para equipamientos y servicios sociales, culturales y recreativos; por supuesto, las zonas verdes no constituyen ningún tipo de prioridad. 

Es gracias a este plan que se terminan los grandes barrios periféricos de la ciudad, algunos de ellos iniciados en los años 20. La incontenible especulación de estos años certificó su presencia en la ciudad con el proyecto de la gran avenida que atravesaría el continuo urbano, engullendo el caserío existente desde Santa Ana, al sur, hasta la carretera de Asturias, al norte. Los tramos que se lograron construir (Avenida Reino de León) muestran la naturaleza de la operación; aparentemente el objetivo era crear una vía de tráfico fluido pero a mayor anchura de la vía, mayor altura de los edificios, mayor número de plantas y, por tanto, más metros cuadrados a la venta.

En 1979 se celebraron de nuevo elecciones democráticas en la ciudad de León, en las que se hizo con el triunfo el PSOE por un error de conteo de los votos, siendo finalmente el verdadero ganador el UCD, con Juan Morano a la cabeza, que gobernó hasta 1987. En este año se produjo el "Pacto Cívico", impulsado por José Luis Díez Villarig, por el cual sacó del gobierno a Juan Morano durante dos años, tras los cuales volvería al gobierno municipal por el PP, gobernando hasta 1995. Le sucedió en ese año Mario Amilivia, que gobernó ocho años, hasta 2003, logrando en su primer mandato, el del 1995, la primera y única mayoría absoluta que ha existido en el Ayuntamiento de León hasta las elecciones municipales de mayo de 2011.

Paralelo al desarrollo de estos actos, renació el leonesismo, movimiento cultural del siglo XIX recuperado para la reivindicación política, produciéndose la aprobación de mociones en favor de una autonomía leonesa por parte de municipios y la Diputación Provincial de León en 1983, así como manifestaciones en favor de la autonomía leonesa, con 20 000 personas en 1983 y 90 000 en 1984.

En 1979 se crea la Universidad de León a partir de la escuela universitaria de Veterinaria perteneciente a Oviedo en el paraje de Vegazana. Es de esta época también cuando se construyen multitud de parques como el Quevedo, la Granja o el parque de los Reyes y se realiza la ampliación del caserío con la edificación de nuevos barrios como Eras de Renueva, Pinilla, el Poligono X o La Torre que permitían alojar a la entonces creciente población leonesa. Paralelamente y a finales de los 70 también la construcción del polígono de Onzonilla, supuso el primer intento de la ciudad de dotar de un espacio acotado para las actividades industriales que hasta ese momento se desarrollaban sin orden siguiendo las vías de comunicación. Las infraestructuras de la ciudad también fueron actualizadas, con la inauguración del aeropuerto y de la ronda este. Antes del cambio de siglo, en 1997, la calle ancha fue peatonalizada, iniciando el proceso de peatonalización del casco histórico. Parejo a estos desarrollos, los pueblos pertenecientes al Área metropolitana de León comenzaron a crecer, quienes con nuevos planes urbanísticos determinaron un gran área de desarrollo urbano entre Villadangos del Páramo y Mansilla de las Mulas.


Con el cambio de siglo, León creció de forma acelerada con nuevos barrios que ampliaron aún más el espacio urbano. La Lastra por el sur colmató el espacio que había entre los ríos Bernesga y Torío mientras que al norte nuevos barrios y ampliación de otros como Universidad y Palomera permitieron ir rellenando el espacio entre el casco urbano y la ronda este. Los pueblos del alfoz, que ya habían comenzado a desarrollarse en el siglo XX crecen con más ímpetu alcanzando San Andrés los 30 000 habitantes, Villaquilambre los 15 000 y Valverde y Sariegos los 5000 habitantes. El proceso constructivo se para con la llegada de la crisis inmobiliaria frenando la integración efectiva de estos nuevos barrios a la ciudad y dejando abandonadas múltiples promociones en el alfoz. Recientemente no obstante, en 2019, se ha iniciado la construcción por primera vez desde dicha crisis de un nuevo barrio en la ciudad, en las inmediaciones del parque de la Granja, retomando de nuevo la expansión urbana de la ciudad. 

Si bien León no es una ciudad centrada en la industria, el espacio dedicado a la misma se ha ido ampliando, primero como un esfuerzo de ordenar las actividades industriales ya existentes en espacios preparados para ellas y después para permitir la expansión de las mismas. Así surge la ampliación del polígono industrial de Onzonilla y nace el parque tecnológico mientras que en el alfoz paralelamente se desarrollan los polígonos de Villaquilambre, San Andrés y Villadangos con proyectos para desarrollar nuevos espacios también en la localidad de Torneros. La expansión de la actividad industrial se centra en los sectores farmaceútico y logístico principalmente, con múltiples empresas biotecnológicas que siguen la tradición empresarial de Antibióticos y grandes grupos de distribución como Inditex, Mercadona o Decathlon.

El desarrollo de nuevas infraestructuras que vive el país con el cambio de siglo no es ajeno a León, donde se construyen autovías hacia Astorga, Benavente y Valladolid mientras que se construyen la ronda sur y el Acceso Sur a León. El aeropuerto también vive un proceso continuado de ampliación que culmina en una nueva terminal de pasajeros inaugurada en 2010. Por su parte, el ferrocarril tampoco es ajeno a este desarrollo y se construye la línea de alta velocidad que comunica la ciudad con Valladolid y Madrid desde 2015 mientras se continua en su ampliación hacia Asturias. El ferrocarril de vía estrecha por su parte vive como se cierra el tramo entre el apeadero de la Asunción y la estación de Matallana, para ser reformado y convertirlo en un tren-tram, algo que en 2019 continúa pendiente. Las administraciones públicas también trasladan a León la sede de varias instituciones de corte regional y nacional como el Musac, el Eren y el Incibe, siendo este último el que catalizaría el desarrollo de actividades relacionadas con las nuevas tecnologías y la ciberseguridad.

En las elecciones de 2003, el Partido Popular no logró la mayoría y, a diferencia de lo ocurrido en 1999, año en el que pactó con Unión del Pueblo Leonés, este partido decidió dar su apoyo a Francisco Fernández, del PSOE. El PSOE duraría un año en el gobierno municipal, pues una moción de censura y la ruptura del grupo municipal leonesista haría que Amilivia recuperase la alcaldía hasta 2007. En las elecciones de dicho año, el PSOE consiguió por primera vez en la historia de la democracia el mayor número de votos en las elecciones, no llegando aun así a la mayoría absoluta, teniendo que pactar con UPL nuevamente. Tras cambiar de nuevo de signo político hacia el PP, este gobierna la ciudad hasta 2019 cuando el PSOE consigue de nuevo recuperar la alcaldía.

El municipio de León cuenta con 124 772 habitantes según el censo de población de 2018 del INE, de los que 56 536 son varones y 68 236 son mujeres. En cuanto a su distribución, 121 393 viven en León, 5076 en Armunia, 757 en Trobajo del Cerecedo y 327 en Oteruelo de la Valdoncina. En 1995, la población residente alcanzó la cifra más alta, 147 780 habitantes censados, y a partir de esa fecha se ha ido produciendo un descenso continuado del censo como consecuencia del envejecimiento de la población, la escasez de nacimientos y de la emigración de la población hacia los municipios del alfoz.


Esta estructura de la población es típica en el régimen demográfico moderno, con una evolución hacia un envejecimiento de la población y una disminución de la natalidad anual.


En 1860, la ciudad contaba con una población de 9866 habitantes, población que se incrementó rápidamente gracias a la mejora de las comunicaciones, en las que jugó un papel clave la llegada del ferrocarril a León en 1863. Así, la población creció un 58 % en apenas cuarenta años, hasta los 15 580 habitantes. Este crecimiento no se debió a un aumento de la natalidad o a una disminución del número de defunciones, sino al éxodo rural, que hizo que la mitad de las personas residentes en la ciudad hubiesen nacido en otro lugar.

Con el cambio de siglo, la ciudad comenzó un leve crecimiento, aumentando un 37 % en veinte años, hasta llegar a los 21 399 censados en 1920. Es a partir de este momento cuando se produjo el mayor crecimiento de la ciudad, duplicando el número de habitantes en el mismo período de veinte años hasta los 44 755. Como en décadas pasadas, este crecimiento se debió casi en exclusiva al éxodo rural. En el período entre 1940 y 1960, el crecimiento poblacional se moderó, debido principalmente a la continencia de la avalancha migratoria desde el medio rural, que redujo su aportación al crecimiento de la ciudad de un 97 % a un 25 %. La ciudad, con 73 483 habitantes representaba ya el 12 % del total provincial. En la década de 1960, acabada la época autárquica, el éxodo rural se intensificó, incrementando la población de la ciudad en un 62 % hasta 1975, fecha en la que la ciudad contaba con 115 176 habitantes.

A partir de 1975, la ciudad cambió la dinámica y aminoró su crecimiento a favor de un alfoz creciente donde comenzaron a despuntar pueblos como Trobajo del Camino. Este cambio de tendencia se confirmó a partir del año 1995, año en que la ciudad alcanzó su máximo histórico de 147 625 habitantes. A partir de este año, la población de la capital leonesa se fue reduciento de forma prácticamente ininterrumpida, con algunos años de leve recuperación, hasta los 135 119 habitantes del año 2008. Durante esos años, por el contrario, el área metropolitana de la ciudad experimentó un rápido crecimiento desde los apenas 31 974 habitantes con los que contaba en el año 1975 hasta los 69 256 habitantes con que contaba en 2008. Las razones hay que buscarlas en la falta de vivienda o un precio de ésta más elevado en la capital que en el área metropolitana.
</small>

Las entidades de población que componen el término municipal de León son las siguientes:

El colectivo inmigrante durante el año 2008 en la ciudad de León se cifró en 8280 personas, entre los que destacan los procedentes de América, con 3417 personas del total. Por países, los más numerosos son los de nacionalidad marroquí, integrando este colectivo 1418 personas, rumana con 1038 censados y los procedentes de Colombia con 1006, el resto de inmigrantes se reparte entre varias nacionalidades de todos los continentes.

En 1970, el área metropolitana de León contaba con una población total de 153 526 habitantes, población que disminuyó años después hasta los 150 104 de 1975. A partir de esta última fecha, el área urbana comenzó un rápido crecimiento que se prolongó hasta el año 1996, año en el que alcanzó los 190 648 habitantes. A partir de ese momento, hubo una pequeña caída en el número de habitantes del área debido a los efectos de la crisis del carbón, que atenazó las comarcas circundantes a León y a la propia ciudad. La población bajó hasta 183 611 habitantes en 2001. Es a partir de este año cuando comenzó un rápido crecimiento, que absorbió en su totalidad los municipios aledaños a León, crecimiento que todavía continúa, y que en 2008 supuso que el área tuviese 201 987 habitantes.

Una vez fue establecido el campamento romano en torno a los años 74-75, este se encargó del control, gestión y explotación de las minas de oro, de las cuales la más importante era la de Las Médulas. Su actividad atrajo a población civil que se asentó alrededor del campamento para satisfacer las necesidades de los soldados, asentándose en el recinto civil "canabae", que desarrollaba actividades como la artesanía o el comercio, que evolucionaron para no dar servicio tan solo a la legión sino también a la creciente población civil, crecimiento que atestigua la presencia de unas termas, de uso militar y civil.

Con la caída del Imperio romano, León entró en decadencia, el comercio y la artesanía pasaron a ser testimoniales y la población se redujo en gran medida, razón por la cual la ciudad pasó a ser un centro agrícola de poca importancia y un lugar de paso para los ganaderos de la zona. Con la llegada de los árabes, la ciudad se despobló definitivamente, sirviendo sus murallas como majada para los ganaderos de la zona.

No fue hasta el año 856 en el que Ordoño I repuebla la ciudad y reconstruye sus murallas, reactivando el comercio y la artesanía en la ciudad. Sin embargo el verdadero impulso lo dio Ordoño II al convertir a León en capital de su reino, haciendo que esta se convirtiera en uno de los principales centros urbanos de la España cristiana. Los avatares políticos fueron quitando protagonismo a León a lo largo de la historia, culminando esta pérdida de protagonismo en la unión definitiva con Castilla en el año 1230. Pese a ello, muchas de las instituciones del reino tuvieron continuidad después de esta unión.

Las malas comunicaciones con el resto del país hicieron que la ciudad mantuviese un aspecto rural y una población estable hasta comienzos del siglo XX. Es en ese siglo, cuando la ciudad inició una recuperación económica. Su condición de capital de provincia, y por ende, de centro urbano de referencia de la zona, así como la llegada del ferrocarril hizo que la ciudad se expandiera en todas direcciones con el ensanche y los barrios periféricos. La industria se asentó en un primer momento en los alrededores de la estación de ferrocarril desplazándose más tarde hacia el extrarradio y luego hacia los polígonos industriales habilitados en torno a la ciudad; no obstante la importancia de este sector nunca llegó a ser relevante en la estructura económica de la ciudad, en la que pesa más el sector servicios. 

Durante la primera década del siglo XXI, la ciudad está viviendo una reactivación del sector industrial, motivado por su promoción como centro de transportes del noroeste con el aeropuerto y con las nuevas vías de alta capacidad, reactivación que se ve acompañada por el crecimiento de sectores económicos relacionados con el I+D. La ciudad no obstante, mantiene la lacra de la emigración de los jóvenes por la falta de trabajo en ciertos sectores, que sin embargo en términos globales está siendo amortizada por la inmigración y la reducción progresiva de esta emigración. El 30 de septiembre de 2015 entró en servicio comercial la línea de alta velocidad Valladolid-Palencia-León, que recortó el tiempo de viaje a Madrid a unas 2 horas.

El sector primario en León se encuentra en vías de desaparición por la presión urbanizadora que la ciudad ejerce sobre los terrenos agrícolas todavía disponibles. No obstante, aún quedan remanentes de este antaño importante sector económico para la ciudad, en las vegas de los ríos Torío y Bernesga y en el alfoz, consistentes sobre todo en una modesta cabaña ganadera que hace uso de los pastos que rodean la ciudad y en pequeñas plantaciones de cultivos cerealistas, como la cebada y el trigo.

Es importante también citar la silvicultura, que se centra en las riberas de los ríos y utiliza el chopo, por su condición de especie de rápido crecimiento y aceptable calidad maderera. Por el contrario, la presencia de la acuicultura y la pesca es despreciable, en cuanto que de la primera apenas existen empresas y la actividad pesquera se centra solamente en la pesca deportiva en los ríos cercanos.

El sector secundario leonés se caracteriza por su debilidad y por inexistencia de grandes empresas que generen un entramado empresarial a su alrededor, basándose pues en pequeñas y medianas empresas. Los sectores en los que tradicionalmente se ha basado el entramado industrial de la ciudad son la metalúrgica de transformados metálicos, la industria química, de maquinaria, alimentaria, cerámica, del vidrio, del papel y artes gráficas y el textil. Es reseñable que la mayoría de las industrias de la ciudad se encuentran ubicadas fuera del término municipal de la ciudad, ubicadas en polígonos industriales que en su mayoría se encuentran conurbados con la ciudad.

Desde comienzos del siglo XXI y a consecuencia de la apertura de las grandes infraestructuras leonesas, tales como la A-66, la AP-71, la A-231 y el aeropuerto de León, inaugurado en 1999, la ciudad está experimentado cierto auge industrial, palpable en un aumento del suelo industrial disponible en el área metropolitana y en menor medida en el propio término municipal de León. La reactivación ha afectado también a las actividades relacionadas con la innovación y el desarrollo tecnológico, que tras el apoyo de las administraciones públicas con la implantación en la ciudad de varios centros tecnológicos como el Inteco y el superordenador Caléndula, perteneciente a la fundación de supercomputación de Castilla y León, además de la colaboración de la universidad con el impulso del sector, ha experimentado un desarrollo, con la llegada de varias empresas importantes del sector, como Hewlett-Packard, SAP, Telvent o Indra, entre otras.

El sector servicios leonés se encuentra diversificado, como corresponde a un centro urbano de cierta entidad, de este modo, la ciudad es el centro de referencia comercial de la provincia. Así, la ciudad cuenta con un sector comercial basado en su mayoría en un comercio tradicional, complementado en los últimos años con la apertura de grandes y medianas superficies en la ciudad, tales como Carrefour, El Corte Inglés, el E.Leclerc, Mercadona, entre otras, así como de centros comerciales, como Espacio León y León Plaza.

El turismo es también un factor clave en el sector servicios de la ciudad, pues la ciudad es visitada anualmente por más de 600 000 personas, animados por la presencia en la capital de un gran patrimonio monumental y de bellos espacios naturales en las inmediaciones de esta, así como de varias fiestas de gran afluencia y reconocido prestigio, entre las que sobresale la Semana Santa.

La Cámara Oficial de Comercio e Industria de León está presente en la ciudad desde el año 1907 por iniciativa de un grupo de comerciantes e industriales de la ciudad. Con sede en un edificio modernista de la avenida Padre Isla de la ciudad, la cámara se encarga de representar y defender los intereses generales del comercio y la industria de la provincia de León.

La ciudad de León está gobernada por el Ayuntamiento de León, cuyos representantes se eligen cada cuatro años por sufragio universal de todos los ciudadanos mayores de 18 años de edad. El órgano está presidido por el , José Antonio Díez Díaz desde 2019.

A continuación se recoge una lista de los alcaldes de la ciudad desde las elecciones democráticas de 1979:

Por otra parte, en el municipio, además de la cabecera, se encuentran las localidades de Armunia, Oteruelo de la Valdoncina y Trobajo del Cerecedo.

El desarrollo urbanístico de la capital leonesa ha estado condicionado por su situación entre los ríos Bernesga y Torío. Entre ambos se situó el núcleo romano de la Legio VII y durante la Edad Media se expandió por el lado sur, estando rodeado todo él por una muralla. A principios del siglo XIX la ciudad seguía siendo ese pequeño núcleo urbano, articulado en torno a la Catedral y de marcado carácter rural.

La llegada del ferrocarril en 1863 se convirtió en el factor que provocó el crecimiento de la ciudad a partir de ese momento. La situación de la estación, en la margen derecha del Bernesga, y al oeste del casco antiguo, fue decisiva para el posterior desarrollo urbano que vivió su área circundante, ya que la ciudad se expandió principalmente hacia esa zona. A medida que avanzó el siglo, la ciudad vieja se reveló como un marco inadecuado para satisfacer las necesidades de la creciente población.

En 1904 se inició el ensanche de la ciudad en torno a su eje principal, la calle Ordoño II. Durante medio siglo supuso el lugar de asentamiento de la burguesía leonesa debido a que la legislación prohibía casas obreras e industrias en la zona. Entre 1910 y 1950 la llegada de inmigrantes a la ciudad fue continua, lo que provocó un problema pues la falta de vivienda distaba de satisfacer las necesidades de estos nuevos inquilinos; la solución fue la de iniciar la construcción de barrios obreros a las afueras, comenzando así la expansión suburbial de la ciudad.

A mediados de los años 1950 se iniciaron los proyectos para elaborar un Plan General de Ordenación Urbana (PGOU), aprobándose definitivamente en 1960. Gracias al mismo, se concluyen los barrios periféricos de la ciudad, algunos de ellos iniciados en los años 1920. En los años 1970 dicho PGOU había quedado superado, por lo que se hacía necesaria la implantación de un nuevo Plan: en 1975 se iniciaron los trámites, aunque finalmente sólo será una adaptación del Plan de 1960.

Tras el desarrollo a finales del siglo XX del barrio Eras de Renueva, dos son los nuevos espacios residenciales con los que contará la ciudad una vez acabados: La Lastra, junto a la confluencia de los dos ríos, y La Torre, junto a la universidad. Asimismo, desde finales del mismo siglo, la ciudad ha trasladado la mayor parte de su crecimiento fuera de los límites municipales, beneficiando a su área metropolitana, con municipios como San Andrés del Rabanedo, Villaquilambre o Valverde de la Virgen.

Si bien la llegada del ferrocarril fue un revulsivo para la ciudad, el soterramiento del mismo en la segunda década del siglo XXI supondrá otro tanto, pues a la desaparición de la barrera urbanística que significaba el tren hay que añadir el espacio liberado que se dedicará a diversos usos como zonas verdes, viviendas, equipamientos, etc.


Templo gótico dedicado a Santa María, fue comenzada a construir en el reinado de Alfonso X el Sabio a mediados del siglo XIII sobre la antigua catedral románica, que a su vez ocupaba los terrenos del Palacio Real que cediera Ordoño II para ello y que, a su vez, se asentaba sobre las termas romanas. De planta similar a la catedral francesa de Reims, tiene reducida su planta en 1/3 con respecto a esta. Una característica peculiar es que las torres aparecen separadas de la nave central mediante arbotantes. Su planta es de tres naves, con bóveda de crucería. Trabajaron en ella distintos arquitectos como el Maestro Simón, el Maestro Enrique y Juan Pérez (estos dos empleados por entonces también en la Catedral de Burgos) y el Maestro Jusquín. El cuerpo principal del edificio fue terminado a principios del siglo XIV junto al claustro y la torre norte, mientras que la torre sur fue terminada en el siglo XV, en estilo gótico flamígero. En los siglos XVII y XVIII sufrió modificaciones estéticas por parte de Juan de Náveda y Joaquín de Churriguera, elementos que provocaron daños al edificio y fueron retirados en el siglo XIX por Matías Laviña, Juan Madrazo, Demetrio de los Ríos, Juan Bautista Lázaro y Juan Crisóstomo Torbado, muchos de los cuales llevaron a cabo la intensa restauración decimonónica que salvó el templo de la ruina, además de devolverle su esencia gótica original. Lo más impresionante es su interior, destacando los más de 1800 metros cuadrados de vidrieras de los siglos XIII al XVI, incluyendo tres grandes rosetones y vidrieras en la parte baja y el triforio, algunas de ellas añadidas en las restauraciones del siglo XIX.

Constituye uno de los ejemplos de arte románico más importantes de España y, sin duda, uno de los conjuntos más completos en este estilo, por cuanto que en él confluyen arquitectura, escultura y pintura, albergando en esta última técnica el Panteón Real, llamado por los expertos "Capilla Sixtina del Arte Románico". Impulsada su construcción por los reyes Fernando I y su esposa Doña Sancha en el siglo XI, originariamente fue un monasterio dedicado a San Juan Bautista, y se supone que anteriormente se asentaba en sus cimientos un templo romano. Con la muerte de San Isidoro, obispo de Sevilla, y con el traslado de sus restos a León, se cambió la titularidad del edificio. Albergó las primeras Cortes de la historia, las Cortes de León, celebradas en 1188.

Impulsada su construcción por los Reyes Católicos como sede de la Orden de Caballería de Santiago, puesto que de hecho fue erigido a orillas del río Bernesga y literalmente junto al puente medieval de San Marcos, por el que los peregrinos continuaban el Camino de Santiago, es hoy uno de los monumentos más importantes de León. De estilo plateresco, en su construcción participaron Juan de Orozco, que firmó los planos de la iglesia, Martín de Villarreal, autor de la fachada, y Juan de Badajoz el Mozo, a quien se debe el claustro y la sacristía. En el siglo XVIII se construyó el ala izquierda del edificio, respetando su arquitectura plateresca. Su historia ha estado llena de avatares y su uso original no duró mucho: tras ello ha sido cárcel (en ella encerró el Conde-Duque de Olivares a Francisco de Quevedo), cuartel, sede de los estudios veterinarios, origen de la Facultad de Veterinaria de la Universidad de León y del Instituto General y Técnico (uno de los tres primeros Institutos de Enseñanza Media creados en España por ley de 1845), fundado en 1846 (hoy IES Padre Isla), e incluso fue campo de concentración durante la Guerra Civil. Actualmente es un parador de cinco estrellas.

En el casco histórico tenemos que destacar las iglesias del Palat del Rey, nuestra Señora del mercado y la iglesia de San Marcelo. La Iglesia de San Salvador de Palat del Rey trata del templo más antiguo de la ciudad de León, fue fundada en el siglo X por Ramiro II de León y, como su nombre indica, se trata del templo del "Palat" (el Palacio) del rey. De su pasado como oratorio regio de la monarquía leonesa da buena cuenta la propia elección de la dedicación, San Salvador, recordando al templo mayor ovetense, o su uso como panteón de la monarquía, antes de la construcción del que sería el gran mausoleo isidoriano. Del templo original, prerrománico, pueden apreciarse hoy pocos restos, aunque ha sido recientemente restaurada y musealizada. La Iglesia de Nuestra Señora del Mercado es una iglesia con planta basilical en forma de sepulcro, siendo más estrecha a los pies. Presenta una portada románica de arco ciego, dos ábsides también románicos decorados con bóveda de horno, capiteles y líneas de imposta con taqueado jaqués. Los pies del edificio se cierran con bóveda de crucería. La torre es obra de Felipe de Cajiga (1598), habiendo sido rematada por Fernando de Compostiza. De lo que fuera la iglesia dedicada al centurión romano Marcelo no queda más que una portada gótica. El templo actual es de estilo herreriano, terminado a principios del siglo XVII. Del exterior destaca la torre cuadrada de la iglesia, cubierta de característicos ladrillos, que se asoma a la plaza de Santo Domingo.

Otros conjuntos eclesiásticos de relevancia podrían ser la iglesia de San Francisco, la iglesia de San Juan y San Pedro y Renueva y el convento de las concepcionistas. La iglesia de San Francisco, cuya actual construcción se remonta a la segunda mitad del siglo XVIII, construida con formas clásicas y dedicada a San Francisco. Está asociada al convento de los Capuchinos y se sitúa frente al parque al que da nombre. El Convento de las Concepcionistas, fundado en 1512 por Leonor de Quiñones, presenta una portada románica del antiguo edificio y corredores con pinturas mudéjares. Su iglesia es de una sola nave, con cabecero del siglo XVI, obra de Juan del Ribero. Se conservan en su interior mobiliario artístico, retablos barrocos, pintura y orfebrería. Mención aparte merece la capilla del cristo de la victoria, de estilo neorrománico, fue realizada a finales del siglo XIX por Demetrio de los Ríos, uno de los restauradores principales de la Catedral de León. La portada imita la puerta del Perdón de la Basílica de San Isidoro de León. En su interior se conserva una escultura gótica del crucificado.

Mandado construir por Juan Quiñones y Guzmán, obispo de Calahorra, se comenzó la obra en 1560 bajo la dirección de Rodrigo Gil de Hontañón. El edificio fue adquirido por la Diputación Provincial de León en 1882, teniendo ampliaciones en los años 1973 a 1976 por parte del arquitecto Felipe Moreno. De forma trapezoidal, los dos primeros cuerpos tienen vanos protegidos por rejería, siendo los balcones del superior adintelados, y el tercer cuerpo presenta una galería o paseador con arquillos entre pilastras corintias y gárgolas de grandes dimensiones. Tiene dos puertas del siglo XVI, una de ellas con una estructura de dos columnas jónicas, flanqueadas por dos soldados con los escudos de armas de la familia.

Antigua residencia de la familia Quiñones, condes de Luna, fue realizado en el siglo siglo XIV, época de la que se conserva el cuerpo central de la fachada. Está construido de piedra sillería y tiene cerca de once metros de ancho. La portada es gótica con dintel sobre modillones, un gran arco apuntado cobija el tímpano, y se encuadra en ancho molduraje. Se conserva también uno de los grandes torreones, de finales del siglo XVI, cuando se reformó el palacio al estilo renacentista. El edificio ha tenido diversos usos a través de la historia, además de su función original de residencia de los condes de Luna, como el ser sede del Tribunal de la Inquisición de la ciudad y vivienda particular, entre otros. Cedido al Ayuntamiento por la Fundación Octavio Álvarez Carballo, alberga la sede española de la Universidad de Washington así como la sede de la Fundación León Real.


Además de los dos palacios que representan a las dos casas principales de la ciudad, el caserío del casco antiguo tiene a su vez otra serie de edificaciones palaciegas, entre las que podríamos destacar el palacio episcopal, donde reside el obispo, el palacio de los marqueses de Prado, el del marqués de Torreblanca, de Don Gutierre y la Casa de las Carnicerías. Por último, en el patio del colegio de las Teresianas se encuentran los restos del único palacio del siglo XII que se conserva.

El palacio episcopal es un edificio cuya construcción comenzó en el siglo XVIII pero no se vio totalmente terminado hasta 1936. Durante años fue la residencia oficial del . Posee una estructura cuadrada con un patio en el centro. El palacio de los marqueses de Prado, de estilo barroco del siglo XVII, albergaba la residencia de los marqueses de Prado, señores de Valdetuéjar. Actualmente es el Hospital de Regla. Presenta una fachada barroca en la que se repiten los blasones de los Prado. Por su parte, el palacio del marqués de Torreblanca Construido en el siglo XVII, consta de grandes dimensiones, planta cuadrada y patio interior, y su fachada está recubierta de ladrillo ornamental. Actualmente es la sede del Recreo Industrial. El palacio de Don Gutierre, cuyo edificio actual data del siglo XVII aún conserva el blasón de esta familia, descendientes del emperador Alfonso VII y de Guzmán el Bueno.

El palacio medieval del colegio de las teresianas es el único edificio civil del siglo XII de la ciudad. De planta cuadrada en ruinas, en una cara conserva puerta y ventanas románicas y una escalera interior de caracol. Se desconoce el servicio que se le dio en la Edad Media. Por último, cabe mencionar la casa de las carnicerías, situada en pleno corazón del Barrio Húmedo, se inició en estilo renacentista a finales del siglo XVI. La fachada, de corte clásico, consta de dos pisos. Actualmente es la sede de la capital gastronómica.

La muralla romana de León tiene su origen en una primera fortificación militar de época augustea, en torno al siglo I a. C., y consistía en dos muros paralelos de madera rematados por un parapeto que estaban unidos por un entarimado. Pronto fueron sustituidos por una construida en piedra por la Legio VII en torno al siglo I, cuyos restos aún son visibles en la zona de San Isidoro. En torno a los siglos III y IV se construyó la que puede contemplarse hoy en día. Declarada Monumento Histórico Artístico en junio de 1931, aún quedan en pie muestras que encerraban el recinto de la ciudad en un cuadrilátero que fue rodeado de construcciones y más tarde deformado en las restauraciones de Alfonso V y Alfonso IX, con apertura de nuevas entradas a la ciudad. Está regularmente conservada desde la torre llamada de los Ponces (de origen también romano) hasta Puerta Castillo, y desde aquí hasta la torre de San Isidoro, en total casi la mitad del recinto, aunque con desigual estado de conservación. Se está procediendo actualmente a su restauración.

En cuanto a las cercas medievales, su origen data del siglo X, y fueron construidas para proteger la expansión de la ciudad extramuros de la muralla romana, que en la época romana se denominaba cannaba y que daría lugar al actual Barrio Húmedo. Las actuales murallas datan del siglo XIV y se conservan dos trazados importantes. En medio de su trazado se conserva parcialmente Puerta Moneda, antigua entrada al barrio judío de la ciudad.

En la muralla, podemos destacar la entrada de Puerta Castillo, que desde tiempos romanos era una de las puertas de entrada a la ciudad. Se situaba junto a una fortaleza que se conservó en la Edad Media. Actualmente, la fortaleza o castillo -sede del AHP de León- aún es visible, mientras que el arco de entrada fue reconstruido en el siglo XVIII. Está presidido por una estatua dedicada a Don Pelayo y es la única puerta de entrada a la ciudad que se conserva.


La plaza mayor de León, ubicada en el corazón del casco antiguo, fue finalizada en 1677 según planos de Francisco del Piñal siguiendo el ejemplo de otras plazas mayores españolas, en particular la de Madrid. El edificio del Consistorio que preside la plaza es de estilo barroco y fue diseñado por el propio Francisco del Piñal como balcón para que la corporación municipal presidiera los eventos de la plaza. La plaza era el gran centro comercial de la ciudad durante la Edad Media y hasta el siglo XIX, dedicándose al mercado dos veces por semana y con la existencia de todo tipo de comercios en sus soportales, con supremacía de productos alimenticios, pero también boticas, ferreterías, platerías, artesanía, etc. Hoy en día el mercado en la plaza sigue existiendo, celebrándose cada miércoles y sábado.

Ha servido para las corridas de toros, se han realizado en ella ejecuciones públicas y celebraciones de la Corte Isabelina, también fue en esta plaza donde los leoneses se reunieron para dar el grito de guerra contra la ocupación francesa de 1810. Hoy en día es una zona muy frecuentada durante las fiestas patronales de San Juan y San Froilán, también durante carnavales y Semana Santa, ya que la mayor parte de procesiones pasan por esta popular plaza leonesa.

En el entorno de la plaza mayor se encuentra el barrio húmedo, que es como se conoce al distrito que abarca los alrededores de esta plaza y la de San Martín. Esta plaza fue el lugar en torno al cual se concentraban los artesanos, mercaderes y peregrinos de la ciudad de León y que hoy, desaparecidas esas actividades o desplazadas a otros lugares de la ciudad, han sido sustituidas por actividades hoteleras y de esparcimiento. En el flanco sur del barrio encontramos la plaza del Grano. El barrio, desde la calle La Rúa hasta la calle Caño Badillo, se encuentra jalonado de bares, cafés y mesones que convierten la zona en la mayor ruta del "tapeo" y en escaparate de las especialidades gastronómicas de la ciudad y de la provincia. Esta circunstancia, unida a la estrechez de sus calles y sus plazas, forman el espacio más típico de la ciudad, caracterizado por su trazado medieval con irregularidades urbanísticas y que es destino obligado para los turistas que visitan la ciudad. El 22 de mayo de 1995 se terminó su peatonalización.


Obra de Antonio Gaudí, de estilo neogótico. Es de planta trapezoidal, flanqueada por cuatro torres rematadas en pináculos. Las ventanas tienen su inspiración en las ventanas del triforio de la catedral leonesa. En la portada hay una talla de San Jorge matando al dragón. El edificio fue concebido para el negocio de tejido en su planta baja y semisótano, destinándose las cuatro plantas restantes a viviendas de renta.

La construcción del edificio se debió a la iniciativa de unos comerciantes de tejidos de León, Simón Fernández Fernández y Mariano Andrés Luna, que estaban relacionados con industriales textiles catalanes, uno de los cuales, Eusebi Güell, recomendó a Gaudí como arquitecto para diseñar la nueva sede del negocio en la capital leonesa, ya que por entonces estaba construyendo cerca de León el Palacio Episcopal de Astorga. Gaudí delegó la dirección de las obras en el constructor Claudi Alsina i Bonafont, uno de sus ayudantes en varias obras en Barcelona, y contó con la colaboración de varios albañiles y artesanos catalanes.


Otros ejemplos de arquitectura destacada los encontramos en el antiguo consistorio, ubicado en la Plaza de San Marcelo, el cual fue construido a finales del siglo XVI por Juan de Rivero para ser la sede del gobierno municipal, es de estilo renacentista y consta de tres alas y una escalera interior. Actualmente solo conserva algunas de las concejalías del ayuntamiento. Subiendo la calle ancha y ya junto a la catedral tenemos el antiguo edificio de correos, edificado en la primera década del siglo XX por el arquitecto leonés Manuel de Cárdenas, su estilo arquitéctónico trata de no romper la armonía gótica catedralicia. Se observan también influencias de Gaudí en un edificio de gusto neogótico.

Destacable es también el "Castrum Iudeorum". Los primeros testimonios de presencia judía en la ciudad de León se remontan al siglo X; entonces se documenta la existencia de una próspera comunidad hebraica asentada en el cerro de la Mota, cercana a la actual pedanía de Puente Castro, sobre el curso del río Torío y a la vera del Camino de Santiago, circunstancia esta que favoreció su tradicional dedicación a actividades vinculadas al comercio y la banca. Sin embargo, tras el ataque que la aljama sufrió en el siglo XII a manos del rey de Castilla, sus moradores fueron obligados a abandonarla y asentarse en la ciudad de León, donde crearon una nueva aljama. En la actualidad, se están llevando a cabo una serie de investigaciones y estudios arqueológicos en torno a este yacimiento, dirigidas desde los departamentos de Historia y Patrimonio de la Universidad de León por Jorge Sánchez-Lafuente Pérez y José Luis Avello Álvarez.

La escultura en León está protagonizada por obras que representan a ilustres personajes, a eventos y a la propia esencia de la ciudad a los que se les ha recordado de esta manera. Así, en el año 1789 se instaló la escultura de Neptuno, inicialmente en la plaza de la catedral, trasladándose a la Plaza mayor y más tarde al jardín de San Francisco, donde permanece actualmente. En la misma fecha se inaugura la fuente del mercado, en la Plaza del Grano. Guzmán el Bueno cuenta también con su propia escultura, presidiendo la plaza homónima, instalada en 1900, recibió numerosas críticas en su día, llegando a ser con el paso del tiempo una de las esculturas más emblemáticas de la ciudad. El escultor Julio del Campo, oriundo de la provincia, posee su propia escultura en el ensanche, inaugurada en 1917, también en el ensanche, se encuentra la escultura de la Inmaculada, inaugurada en los años 50.

Otra escultura de gran relevancia es La vieja negrilla, llegó por primera vez a la plaza de Santo Domingo en diciembre de 1997, donde permaneció diez años, hasta que un conductor ebrio se empotró contra ella en 2007. Amancio González modeló entonces la figura de nuevo y, gracias al patrocinio de Renfe, la nueva 'negrilla' se volvió a colocar en el mismo lugar en 2009. Con una diferencia: la segunda vez se hizo en bronce –la anterior era de hormigón–, "para que los niños pudieran subirse a ella y jugar, como hacía yo de pequeño con la vieja negrilla de mi pueblo", recuerda Amancio.

Son destacables también los cuatro leones, que adornan el puente de los leones, anteriormente del ferrocarril, instaladas en 1967, obra del autor Víctor de los Ríos y lejos de allí el Don Quijote en Sierra Morena, en la universidad, instalada en 1964 inicialmente en el Alto del Portillo por un encargo de Caja León y trasladada más tarde al campus universitario.

Entre las nuevas esculturas, instaladas desde finales del siglo XX en nuevos barrios en zonas remodeladas, como la avenida Ordoño II, caben destacar a escala humana el "monumento a la lucha leonesa", de Ángel Muñoz Alique y situado junto al Estadio Reino de León, el "Peregrino sentado en el Crucero" (1998), de Martín Vázquez de Acuña e instalado en la plaza de San Marcos tras su remodelación, el "Homenaje al Maestro Odón Alonso", de Ángel Muñiz Alique, junto al Auditorio Ciudad de León, "Antoni Gaudí sentado en un banco" (1998), de José Luis Fernández y enfrente de la obra de Antoní Gaudí la Casa Botines. Otros ejemplos son "Padre e Hijo" (1997), en la plaza de la Regla y obra de Jesús Trapote Medina y "Las Cabezadas", obra de José Luis Fernández.
La ciudad de León es una ciudad reconocida por su gran cantidad de zonas verdes, tanto es así que León es la ciudad española que más zonas verdes pone al servicio de sus ciudadanos. León cuenta con 2 196 542 m² de zonas verdes distribuidos por toda la ciudad. Este espacio se encuentra dividido entre numerosos parques, entre los que destacan por tamaño; el Parque del Chantre, Parque de Quevedo, el Jardín del Cid, el Jardín de San Francisco y el Parque de La Granja.

Es el pulmón verde más notable de la ciudad, paralelo al río, se extiende desde el convento de San Marcos hasta las inmediaciones de la plaza de toros, interrumpido por la plaza de Guzmán el bueno, que marca la línea divisoria entre el Paseo de la Condesa, aguas arriba, y el Paseo de Papalaguinda, aguas abajo. Ambos se encuentran comunicados por el parque de la ribera del Bernesga, construido aprovechando la canalización del río.

Los orígenes de esta gran zona verde hay que buscarlos a principios del siglo XIX, cuando se planteó el ensanche para unir el casco histórico con el Bernesga, cuya unión definitiva se realizó a través de esta extensa zona verde. Hoy se encuentra jalonado de esculturas modernas y de quioscos de música, así como de escaleras para descender al río. El parque está poblado por un buen número de diferentes especies de árboles, arbustos y aves. Por su gran presencia, destacan los ciruelos, arces blancos, cipreses, enebros, castaños de indias, olmos, encinas, hayas y glicinia, entre otras especies. Entre las aves, es común la presencia en el parque de currucas capirotadas, de golondrinas zapadoras, de verderones comunes y aguzanieves.


La ciudad de León cuenta con numerosos centros de enseñanzas no universitarias. De carácter público, cuenta con 17 centros de educación infantil y primaria, uno de educación especial, 9 de educación secundaria y un centro específico de formación profesional. De carácter privado, la ciudad cuenta con 20 centros, dos de los cuales son de educación especial y uno de formación profesional.

En cuanto a las enseñanzas de régimen especial, León cuenta con una Escuela Oficial de Idiomas (en la que se imparten alemán, francés, inglés, italiano, portugués y español para extranjeros), una Escuela de Arte y de Conservación y Restauración de Bienes Culturales, dos Conservatorios de Música (uno de ellos de carácter privado) y un Centro de Educación de Personas Adultas (CEPA).

La ciudad también cuenta con la Universidad de León. Fue fundada en 1979, desgajándola de la Universidad de Oviedo, a partir de las diversas escuelas y facultades que, dependientes de aquella, existían en la ciudad de León, y sobre unos terrenos llamados Vegazana (de donde toma nombre el campus universitario) donados por la entonces Caja de Ahorros y Monte de Piedad de León. La universidad cuenta con dos campus, el ya citado de Vegazana, situado en la parte noreste de la ciudad, que está siendo objeto de una intensa ampliación para adaptarse a la normativa europea, y el de Ponferrada.

La Universidad de León cuenta con 8 facultades y 7 escuelas (3 de ellas adscritas), imparte más de 50 titulaciones, y posee 26 departamentos, 7 institutos universitarios y 4 centros tecnológicos. Además tiene un Centro de Idiomas donde se imparten 9 idiomas. En el curso 2011-12 contaba con 12 643 alumnos. Su rector es Juan Francisco García Marín.


El sistema sanitario de la ciudad de León se divide entre las prestaciones del sistema público de salud, gestionado por Sacyl (Sanidad Castilla y León), y las que realiza la medicina privada. La Ley 1/1993, de 6 de abril, de Ordenación del Sistema Sanitario, divide la atención sanitaria en tres niveles de atención: primaria, especializada y continuada.

La atención primaria en la provincia se divide en dos Áreas de Salud, El Bierzo y León. Esta última engloba 28 Zonas Básicas de Salud, correspondiendo a la capital 7 de las mismas. Para desarrollar esa atención primaria, León cuenta con 7 centros de salud, los de Eras de Renueva, La Palomera, El Crucero, Armunia, La Condesa y los dos de José Aguado.

Para la atención especializada, la ciudad cuenta con el Hospital de León, el cual lo conforman varios centros:

En cuanto a la sanidad privada, además de numerosas consultas particulares, existen 4 centros hospitalarios: la Clínica San Francisco, que cuenta con 94 camas y aglutina gran número de especialidades, la Clínica López Otazú, el Hospital de San Juan de Dios, perteneciente a la Orden Hospitalaria San Juan de Dios, que se encuentra en el límite municipal entre San Andrés del Rabanedo y León y cuenta con 234 camas y el Hospital de Nuestra Señora de Regla, con 120 camas y administrado por la Obra Hospitalaria Nuestra Señora de Regla, perteneciente al Obispado de León.


Los servicios sociales en la ciudad de León son gestionados por la Concejalía de Bienestar Social. Esta cuenta con una serie de programas sociales como Ayudas de Emergencia Social, Servicio de Apoyo a las Familias, Servicio de Ayuda a Domicilio, Servicio de Información y Orientación, Servicio de Teleasistencia y Minorías Étnicas.

Entre los medios que ofrece están el Hogar Municipal de Transeúntes, el Centro Municipal de Atención a Inmigrantes (CEMAI), el Centro Municipal de Acción Voluntaria y Cooperación (CAV) y 8 CEAS, los cuales dan cobertura a los distintos barrios y prestan los llamados Servicios Sociales Básicos, como por ejemplo Servicio de Apoyo a la Familia y Convivencia, Servicio de Atención a la Mujer o Servicio de Animación Comunitaria.


Del transporte de la energía eléctrica por todo el territorio nacional se ocupa la empresa Red Eléctrica de España. La distribución de la electricidad en León la realiza "Endesa-Distribución", del grupo Endesa. El consumo total de energía eléctrica durante el segundo trimestre de 2008 fue de 615 149 MWh, de los que 203 427 MWh correspondieron al consumo doméstico.


León y su provincia se abastecen de combustibles derivados del petróleo (gasolina y gasóleo) desde las instalaciones de almacenamiento que la Compañía Logística de Hidrocarburos (CLH) posee en la localidad de Vega de Infanzones, cercana a León.


El gas natural que se consume en León proviene, como en la mayor parte de España, principalmente de Argelia. Es transportado por una red básica en alta presión responsabilidad de Enagás, desde donde se distribuye a viviendas e industrias por las instalaciones de Gas Natural Castilla y León.


El Ente Regional de la Energía es un organismo público dependiente de la consejería de Economía y Empleo de la Junta de Castilla y León, creado el 3 de diciembre de 1996, sus funciones son las de asesorar en materia energética a las empresas de la comunidad autónoma de Castilla y León, promoviendo subvenciones a fin de mejorar la eficiencia energética en el sector empresarial y en las administraciones públicas.

Su sede se encuentra en un edificio vanguardista situado en el barrio de Eras de Renueva, enfrente del MUSAC y al lado del Tanatorio de SERFUNLE. Es la sede del organismo regional que se dedica a la planificación de la energía en la autonomía y servir de apoyo para las decisiones en el campo de la energía con la realización de estudios sobre la viabilidad e incidencia económica de dichas decisiones. En el EREN funcionan 21 metros cuadrados de paneles térmicos y una instalación fotovoltaica de 5 kWh, para auto abastecimiento energético del edificio.


El abastecimiento de agua a León lo realiza la entidad Aguas de León. Antiguamente, el agua se tomaba de los ríos Luna y Torío y de cinco perforaciones hechas en el área Bernesga-Torío, pero debido a las frecuentes restricciones estivales, se tomó la decisión de tomar agua del río Porma, corriente abajo del embalse Juan Benet. Este se sitúa en la zona norte de la provincia, en el municipio de Boñar, y cuenta con una capacidad de 317 hm³.

A su llegada a León, la conducción de agua termina en la Estación de Tratamiento de Agua Potable (ETAP), que se encuentra en la localidad de Villavante. Por su parte, la depuración de las aguas residuales se lleva a cabo en la estación de depuración de aguas residuales (EDAR), situada junto al río Bernesga, en la localidad de Trobajo del Cerecedo. Esta estación sirve a la Mancomunidad Municipal para el Saneamiento Integral de León y su Alfoz (SALEAL) (integrada por los municipios de León, San Andrés del Rabanedo, Villaquilambre, Santovenia de la Valdoncina y Sariegos), la cual es titular del Servicio Público de Tratamiento y Depuración de Aguas Residuales.


Urbaser es la empresa responsable de la gestión de los residuos sólidos urbanos y la limpieza de las vías públicas de León. Entre otros servicios, la ciudad cuenta con recogida selectiva de residuos, dos puntos limpios fijos, un punto limpio móvil y servicio de recogida puerta a puerta.

León pertenece al Consorcio Provincial de Residuos (GERSUL), el cual gestiona los residuos urbanos generados en toda la provincia mediante su tratamiento en tres plantas de clasificación y un Centro de Tratamiento de Residuos (CTR), ubicado en San Román de la Vega, en el municipio de San Justo de la Vega.


La encargada del abastecimiento de la ciudad es la entidad Mercados Centrales de Abastecimiento de León (Mercaleón). Se creó el 29 de diciembre de 1989 como resultado de la colaboración del Ayuntamiento de León y la Empresa Nacional de Mercados Centrales de Abastecimiento S.A. (Mercasa) para la distribución al por mayor de productos perecederos en la ciudad de León y su área de influencia. Inició su actividad en abril de 1993, siendo una de las 23 unidades alimentarias de Mercasa, que a su vez depende de la Sociedad Estatal de Participaciones Industriales (SEPI) y del Ministerio de Medio Ambiente, Medio Rural y Marino.

Sus instalaciones, que cubren una superficie de 41 185 m², albergan a 32 empresas, de las cuales 17 son mayoristas (frutas, hortalizas y pescados) y el resto se dedica a tareas de distribución, logística o servicios a usuarios. Cuenta con un mercado de frutas y hortalizas, un mercado de pescados, un pabellón polivalente y servicios complementarios para facilitar el desarrollo de la actividad en el centro.

Su área de influencia no solo se limita a León y su provincia, sino que se extiende incluso a otras provincias limítrofes como Lugo, Orense, Asturias, Zamora y Palencia, facilitado por su situación estratégica y por la mejora de las vías de comunicación en el noroeste peninsular.


El transporte urbano en León es gestionado por la empresa Alesa, filial del grupo ALSA. Presta servicio mediante una red de 14 líneas operadas con 52 autobuses, si bien las previsiones apuntan a la ampliación de las mismas con el objeto de ofrecer servicio al polígono industrial de Onzonilla y al aeropuerto.


La empresa FEVE mantiene en funcionamiento un servicio de Cercanías entre las localidades de León y Guardo, en la provincia de Palencia, aprovechando la línea de ferrocarril que discurre entre León y Bilbao. Este servicio atraviesa en su recorrido los municipios de León, Villaquilambre, Garrafe de Torío, Matallana de Torío, La Vecilla, Boñar, La Ercina Cistierna y Valderrueda.


Tras grandes cambios de un proyecto inicial que planteaba la creación de seis líneas de tranvía, el proyecto final planteaba dos en forma de Y. La primera de ellas discurriría entre el Área 17 y Puente Castro y la segunda, aprovechando la traza de Feve, discurriría entre la plaza de Santo Domingo y el límite municipal con Villaquilambre, con un ramal al complejo Hospitalario y la posibilidad de construcción de un segundo ramal al campus de Vegazana de la Universidad de León.

Presupuestado en 150 millones de euros, el tranvía leonés en su máximo desarrollo tendría un longitud de 9 kilómetros, sirviendo a una población de 130 000 personas a menos de 500 metros de cada parada, con una frecuencia de paso de 8 minutos en hora punta, y un uso en torno a los 9 millones de usuarios anuales. Sin embargo, el Partido Popular anunció que bajo su gobierno no desarrollaría tal proyecto por considerarlo innecesario.

La ciudad de León cuenta con una red de carril-bici en la que tradicionalmente los mayores itinerarios se reducían a las riberas del río Bernesga y del río Torío como elementos de esparcimiento, nunca de transporte de masas. Sin embargo, en los últimos años se ha mejorado la red con la construcción de nuevos itinerarios aprovechando los tramos inconexos anteriores.

Así, en 2007 se inició la construcción de un carril bici de 2,5 kilómetros, hoy inaugurado, paralelo a la ronda este y que recorre la periferia del campus universitario, viéndose prolongado poco después en 900 metros en el PAU de la Universidad. Se han construido otros itinerarios que conectan la universidad con distintos barrios de la ciudad y aprovechando la reforma de Fernández Ladreda un tramo de 800 metros. En construcción o proyectados se encuentran los itinerarios de conexión entre los paseos del Bernesga y el Torío en primer lugar y conexión de Eras de Renueva con el casco antiguo y San Andrés del Rabanedo en segundo.

Con el fin de expandir el uso de la bici por la ciudad. De este modo, se potenciará el préstamo de bicicletas, aumentando el número de puestos municipales destinados a tal fin de 4 a 16 y la creación de un 1500 puestos de aparca-bicis, que han comenzado a instalarse en el campus universitario y que se expandirán a lo largo de 2009 por toda la ciudad, incidiendo con especial interés en los principales focos atractores de viajeros de la ciudad.


La ciudad de León es cruce de comunicaciones del noroeste de España, siendo lugar de paso hacia Asturias desde la meseta y hacia Galicia desde el noreste de España.Dentro de la red principal de comunicaciones, una nutrida red de autovías, autopistas y carreteras tiene origen en León o simplemente pasan por la ciudad. Cuenta con las siguientes vías de gran capacidad:


La estación de autobuses de León se encuentra en la Avenida Ingeniero Sáenz de Miera y enlaza la ciudad no solo con diferentes puntos de la provincia y de la Comunidad, sino también con destinos nacionales e internacionales.

Entre las distintas compañías, el Grupo ALSA es uno de los que más servicios ofrece, enlazando León con múltiples destinos nacionales como por ejemplo La Coruña, Alicante, Barcelona, Bilbao, Gijón, Madrid, Valladolid, Málaga o Sevilla.


La ciudad de León es un centro de primer orden en el transporte ferroviario, con vías que en su mayor parte son una herencia del pasado minero de la provincia. Así la ciudad cuenta con dos estaciones de ferrocarril, la estación de León (ubicada en el barrio del Crucero) en las líneas de ancho ibérico Venta de Baños-Gijón y León-La Coruña y desde 2015 en la línea de alta velocidad Valladolid-León, sustituida de manera provisional por una nueva hasta el soterramiento del ferrocarril en León, y la estación de Matallana en la línea de ancho métrico del Ferrocarril de La Robla.


El aeropuerto de León, que entró en servicio en 1999, es el único aeropuerto ubicado en la provincia y el más cercano al municipio, encontrándose entre Valverde de la Virgen y San Andrés del Rabanedo. En octubre de 2010 se inauguraron las obras de ampliación, centradas en la construcción de una nueva terminal y en la duplicación de la superficie de la plataforma.


En la ciudad pueden adquirirse los periódicos nacionales, regionales e internacionales de mayor difusión, algunos de los cuales incorporan una sección de información local o regional.

En cuanto a los periódicos locales, se editan, "Diario de León" y "La Nueva Crónica", el de mayor difusión es el "Diario de León", que en 2009 tenía una difusión media de 14 102 ejemplares, mientras que "La Crónica de León", tenía una difusión media de 7058 ejemplares según la información que aporta la OJD. De manera gratuita se reparte el semanario Gente León, que se edita todos los viernes.


En la ciudad se pueden sintonizar todas las cadenas principales de radio que operan a nivel estatal y regional y en la ciudad disponen de emisoras locales que emiten espacios dedicados a la actualidad local en sus desconexiones en diferentes tramos horarios: Radio Nacional de España, Cadena SER, Onda Cero, COPE y Castilla y León esRadio. En FM se pueden sintonizar las emisoras eminentemente musicales y otras específicas dedicadas a la información deportiva, local, económica o religiosa.


Con la entrada en funcionamiento de la Televisión Digital Terrestre (TDT) se ha multiplicado el número de canales de televisión, tanto generalistas como temáticos y tanto gratis como plataformas de pago a los que pueden acceder los leoneses. A nivel autonómico funcionan en 2011 con desconexiones locales las emisoras CYLTV y La 8.


El uso creciente de dispositivos tecnológicos, desde los cuales se puede acceder a Internet, las zonas wifi libre que se van creando en la ciudad y la posibilidad que ofrece Internet de acceder a todo tipo de medios tanto prensa, radio y televisión han revolucionado el modo que tienen hoy día las personas de acceder a la información general y especializada. A nivel local cabe señalar la página web del Ayuntamiento donde se ofrece a los ciudadanos la información institucional más significativa que afecta a los leoneses, así como las versiones digitales de los periódicos locales.

El Ayuntamiento de León ha impulsado el conocimiento y uso de la lengua leonesa en la ciudad de León, tanto en enseñanza para adultos como con la creación de la asignatura "Llingua y Cultura Llïonesa" que se ofrece de manera optativa y extraescolar en los centros escolares.

En el curso 2008-2009 comenzó a impartirse la asignatura en 16 centros públicos y concertados de la ciudad de León, para niños de quinto y sexto curso de Educación primaria, con ochenta niños matriculados. El ayuntamiento, en colaboración con la Universidad de León, también ofrece cursos para adultos, habiéndose superado los cien matriculados.Los cursos de adultos se estructuran en seis niveles, llegándose en 2009 al quinto nivel.

El Ayuntamiento de León también realiza campañas de promoción del leonés y en leonés, ofreciendo algunas de sus concejalías información en leonés y castellano en los formularios públicos, y publicando las noticias en su página web en ambos idiomas.

El Museo de Arte Contemporáneo de Castilla y León fue inaugurado por los entonces Príncipes de Asturias, Felipe de Borbón y Letizia Ortiz, el 1 de abril de 2005, con un firme propósito: ser un Museo de Presente y convertirse en pieza fundamental en el desarrollo del Arte Contemporáneo, a nivel internacional. Este museo nace con un amplio sentido experimental a la hora de concebir y desarrollar proyectos y exposiciones a todos los niveles. El MUSAC se encuentra trabajando exclusivamente en el área temporal del presente, marcado por la memoria más cercana: el museo se inicia con la idea de desarrollar un nuevo comportamiento a la hora de abordar el arte del siglo XXI.

El Museo, se ha convertido en uno de los referentes internacionales en Arte Contemporáneo, superando su número de visitantes los 500 000, cifra muy superior a la de la ciudad de León, de los cuales, 51% son locales, un 28% del ámbito nacional, un 9% de Castilla y León, un 8% de la provincia y el 4% restante del extranjero.

Está localizado en Eras de Renueva, junto al edificio del EREN (Ente Regional de la Energía de Castilla y León) y es un edificio de nueva planta, obra del estudio madrileño Mansilla + Tuñón Arquitectos.

El MUSAC se une en la provincia al Museo de la Siderurgia y la Minería de Castilla y León para formar la Red de Museos Regionales de Castilla y León, en la que también se integran el Museo Etnográfico de Castilla y León, situado en Zamora, y el Museo de la Evolución Humana situado en Burgos.


El Museo de León es el más antiguo de la provincia y está dedicado a narrar su historia a través de la Arqueología, el Arte y la Etnografía. Inaugurado en 1869, aunque fundado a partir de la actividad de la Comisión Provincial de Monumentos de León en el contexto de la Desamortización decimonónica, desde 2007 se encuentra ubicado en el conocido como Edificio Pallarés, en el centro de la Ciudad. Asimismo, cuenta con dos anexos: la Villa romana de Navatejera, en el vecino municipio de Villaquilambre, y el antiguo convento de San Marcos, en la misma capital, que es asimismo la "sede histórica" del Museo.

La exposición permanente del Museo ofrece un itinerario por la historia del territorio provincial a través de algunas de sus realizaciones culturales más significativas y cualificadas. Está articulada en siete áreas de conocimiento en las que el desarrollo cronológico permite ofrecer otras reflexiones paralelas y recorridos alternativos. De este modo, el visitante puede recorrer la historia de León desde la Prehistoria hasta el mundo contemporáneo, pasando por la romanización, el final del mundo antiguo, la Edad Media y la Edad Moderna. Hay además otra sala que ofrece una panorámica sobre la ciudad de León, que incluye uno de los miradores más completos que existen sobre su perfil urbano histórico.


El Museo de la Real Colegiata de San Isidoro se destaca por el Panteón de los Reyes, el cual es denominado Capilla Sixtina del Románico por sus elaborados frescos. Otras piezas relevantes son la arqueta de San Isidoro, el cáliz de doña Urraca, del siglo XI, la Arqueta de los Marfiles y el Portapaz del Pantocrator, del mismo siglo, y la Arqueta de Limoges, entre otros.

El Museo Fundación Vela Zanetti, recoge una muestra muy significativa de la obra de este autor burgalés, aunque leonés de adopción.


Fue inaugurado el año 1981 y es el resultado de la fusión del antiguo museo catedralicio con el diocesano. Este último había sido creado por el obispo Almarcha el año 1945, aunque el mayor incremento de sus fondos se realizó a partir de la década de 1960.

En la actualidad constituye un conjunto único en su género, albergando piezas de todas las etapas de la historia del arte, desde la prehistoria hasta el siglo XX, todas ellas repartidas en diecisiete salas, en el entorno del claustro catedralicio. Se accede a él por una hermosa puerta de nogal, que según el profesor Merino Rubio, había sido hecha para la librería por Juan de Quirós, antes del año 1513; en su tímpano se narra la escena de la Anunciación, plenamente flamenca, sobre un espacio con arquerías góticas.

En la primera estancia se nos muestra la escalera plateresca de Juan de Badajoz el Mozo, que facilitaba la subida a la sala capitular. El soporte de sus tres cuerpos está profusamente decorado con labores menudas de bueráneos, "candelieri", medallones y otros temas del mejor Renacimiento. Se buscó como pretexto para colocar el escudo del obispo mecenas, Pedro Manuel, la pequeña tribuna que resalta sobre la balaustrada.


El Museo Sierra-Pambley muestra el retrato de la vida doméstica de una familia ilustrada del siglo XIX y el recorrido por la labor pedagógica de la Fundación Sierra-Pambley, fundada en 1885 por Francisco Fernández-Blanco y Segundo Sierra-Pambley en una reunión con los más notables miembros de la Institución Libre de Enseñanza en su casa de Villablino. El museo se encuentra dividido en dos partes claramente diferenciadas:




Ubicado en la Basílica de San Isidoro, cuenta con una colección de arqueología próximo-oriental que consta de cerca de mil piezas y una biblioteca de más de 10 000 volúmenes. Fue inaugurado por la reina Doña Sofía el día 11 de marzo de 2009, (la Reina, en la inauguración, pudo abrir una carta sumeria de 3000 años de antigüedad que se había mantenido inédita) abriendo sus puertas al público el día 19 del mismo mes.


El Centro Leonés del Arte se encuentra en la Avenida de Independencia, n.º 18. El complejo de edificios fue realizado por uno de los arquitectos más destacados de la primera mitad del siglo XX en León, Juan Crisóstomo Torbado (Galleguillos del Campo 1867- 1947). Es una obra de tipo neohistoricista que se construyó en 1927 para albergar el Instituto Provincial de Higiene, hasta que en el año 2006 se rehabilitó para albergar el Legado Caneja y convertirse en un centro expositivo y cultural de la Diputación de León.

La inauguración oficial del centro fue el 2 de febrero de 2007, con la presentación de dos muestras: “Legado Caneja” y “El paisaje en el coleccionismo leonés”.

La Casona de la Fundación Carriegos

La Casona es la sede de las actividades culturales de la Fundación Carriegos, ubicada en la avenida Suero de Quiñones. La casa-museo fue la vivienda particular del industrial Miguel Pérez Vázquez, importante ebanista leonés. Proyectada en 1925 por el arquitecto Manuel de Cárdenas, fue decorada en estilo modernista y art decó y amueblada según diseño y manufactura propia. Las dos plantas históricas de la casa conservan completo su ajuar original. Hoy el inmueble es un centro cultural que acoge el Aula Literaria "El fulgor de la memoria" dedicada a Victoriano Crémer que repasa la figura del poeta y periodista a través de una exposición permanente y una sala de exposiciones temporales que muestra el quehacer artístico más actual.

• "Casa-museo": ejemplo de vivienda burguesa anterior a la guerra civil española de 1936 cuyo estilo combina el historicismo neorrenacentista con otras estancias de estilo modernista y art déco (Galería, Vestíbulo, Sala de Música).

• "Exposición permanente": un conjunto de obras de Vela Zanetti y la Sala de Pintura con las obras ganadoras del Premio Carriegos de Pintura y Artes Visuales.

• "Aula Literaria El fulgor de la memoria de Victoriano Crémer": exposición sobre la vida y obra del poeta a través de su fondo documental, artículos y correspondencia, biblioteca y colección de arte.

• "Sala de exposiciones temporales": dedicada a la muestra del arte más actual.


El Auditorio Ciudad de León está situado en el barrio Eras de Renueva, junto al histórico convento de San Marcos y la Delegación del gobierno autonómico. El edificio es obra de Emilio Tuñón Álvarez y Luis Moreno Mansilla (Mansilla + Tuñón Arquitectos) y tiene una superficie construida de 9000 metros cuadrados. Cuenta con tres salas, siendo la mayor para 1128 personas, y las otras dos, más pequeñas, de 388 y 100 personas. Además, el auditorio cuenta con dos salas de exposiciones, retroproyectores, equipo de proyección y posibilidad de incorporar equipo multiconferencia.

El edificio supone un hito en la arquitectura de la ciudad, por ser de los primeros de arquitectura moderna. Su uso está en las artes escénicas y representaciones, aunque también acoge congresos de diverso tipo.


El Teatro Emperador, abierto al público en 1951 y obra del arquitecto Manuel de Cárdenas, fue uno de los teatros más destacados y bellos de León durante décadas. A finales del siglo XX, sin embargo, su uso para representaciones teatrales había quedado muy disminuido y su principal función era la de sala de cine. Finalmente, la empresa propietaria del inmueble clausuró el edificio en 2006 con una fuerte oposición de la ciudadanía leonesa. Tras un acuerdo inicial de compra por parte del Ayuntamiento de la ciudad, el edificio pasó a manos municipales, que tras las elecciones de 2007 transfirió el inmueble al Ministerio de Cultura, que proyectaba usarlo como sede del Centro Nacional de las Artes Escénicas y de las Músicas Históricas de España, organismo adscrito al INAEM. El proyecto nunca se puso en marcha y el edificio fue puesto a subasta en 2014.


El Teatro Trianón, situado en la avenida Ramón y Cajal es uno de los dos teatros que se conservan en la ciudad y el único con declaración de Bien de Interés Cultural. Esta medida de protección no ha evitado su deterioro hasta la situación actual. Además de su uso como escenario teatral, el pequeño edificio fue en tiempos sala de fiestas y de cine, e incluso, en su último uso, parque infantil. La maestría en el aprovechamiento del espacio a través de su estructura en chaflán o su decoración interior son algunos de sus aspectos más destacados.


La Plaza de Toros del Parque, actualmente también conocida como León Arena, fue construida en 1948 en el lugar de una plaza anterior de madera, edificada en 1912. Tiene dos pisos, 50 metros de diámetro, y un aforo de 11 300 localidades. Desde 2003, tras su conversión en plaza cubierta (cuando adquiere el nuevo nombre), acoge no solo espectáculos taurinos durante las fiestas patronales, sino también conciertos nacionales e internacionales, eventos deportivos, ferias y congresos, exposiciones y grandes espectáculos.

Debido a su importancia como núcleo histórico y monumental, la ciudad de León forma parte de redes turísticas o culturales como la Red de Ciudades Catedralicias o la Red de Juderías de España.

Asimismo, por León pasa el Camino de Santiago, en concreto el Camino Francés, siendo el final de una de sus etapas. La ciudad cuenta con dos albergues, ambos abiertos todo el año, uno municipal y otro el de las Carbajalas (M.M. Benedictinas). La Asociación de Amigos del Camino de Santiago en León "Pulchra Leonina" se encarga de informar a peregrinos, defender y conservar el patrimonio cultural relacionado con el Camino, así como promocionar todo tipo de actividades culturales.

La Semana Santa en León es una fiesta declarada de Interés Turístico Internacional, señalada en el calendario festivo leonés como la más importante del año.

Durante los diez días que transcurren desde el Viernes de Dolores al Domingo de Pascua, un total de 16 cofradías y hermandades, integradas por decenas de miles de "papones" (término único y de gran personalidad que en León reciben los hermanos cofrades) a las que se unen la Junta Mayor de la Semana Santa de León y la iglesia parroquial de Nuestra Señora del Mercado y del Camino "La Antigua", recorren las calles de una ciudad atestada de gente como en ningún otro momento del año.

Entre sus acontecimientos más significativos está La Ronda, la cual es un acto singular y único, reflejo de la gran tradición que envuelve la Semana Santa leonesa. Parte a las 24:00 de la Plaza de San Marcelo, en pleno centro de la ciudad, donde lleva a cabo ante el antiguo Ayuntamiento el primero de sus "toques" oficiales, con el que llama al pueblo de León a la Procesión de los Pasos, auténtica recreación del Calvario, la cual arranca a las 7:30 y no acaba hasta las 16:00 horas.

A lo largo del año son numerosos los eventos culturales y festivos que tienen lugar en León. Cronológicamente, en el mes de enero tiene lugar el CiLe (Festival de Cine Digital de León). En febrero se celebran los carnavales, en la que tienen lugar multitud de actividades como la Gala de Elección de la Reina de Carnaval, el Festival Infantil, el Desfile del Martes de Carnaval o el Entierro de la Sardina.

En el mes de marzo tiene lugar el FIMA (Festival Internacional de Música Avanzada), cuya última edición se celebró en 2007. También en este año se celebró la Feria Leer León (Feria Internacional del Libro Infantil y Juvenil). Durante la Semana Santa se celebra el Entierro de Genarín, fiesta conmemorativa en honor de Genarín, pellejero muy conocido en León, atropellado por el primer camión de basura de la ciudad mientras hacía sus necesidades en la base del tercer cubo de la muralla, lugar donde se celebra todos los años el homenaje. A finales de abril se celebran Las Cabezadas, en las cuales la ciudad, representada por la Corporación Municipal, ofrece un cirio y dos hachas de cera en la Basílica de San Isidoro, enfrentándose dialécticamente con el Cabildo.

En junio (en 2001, 2002, 2009 y 2010, mientras que en 2011 tuvo lugar en octubre, durante las fiestas de San Froilán) se celebra Festival Celta Internacional Reino de León, en el que participan diversos grupos musicales representativos de la música celta. A finales del mismo mes tienen lugar las fiestas de San Juan y San Pedro, fiestas patronales de la capital leonesa; se trata de la fiesta grande de la ciudad y referente en el resto de la provincia. Parte de las mismas son la Calle Ancha Muestra de Teatro de Calle y el Festival Flamenco de León. También en época estival tiene lugar el Festival de Música Española.

En otoño se celebran dos acontecimientos musicales, el Festival Internacional de Órgano Catedral de León y el Campeonato de Bandas de Gaitas del País LLionés. El domingo previo al día de San Froilán (5 de octubre) tiene lugar una de las fiestas más tradicionales de cuantas tiene la ciudad de León, pues viene celebrándose desde la Edad Media. Son fechas en las que se puede asistir a la romería de la Virgen del Camino, disfrutar de los Carros Engalanados, contemplar una de las mayores concentraciones de Pendones que se da en la provincia y asistir a la lucha dialéctica que provoca el Tributo de las 100 Doncellas así como el Foro u Oferta de Las Cantaderas.

Por último, en el mes de diciembre, tiene lugar el Purple Weekend y el Festival Internacional Tiempo de Magia, durante las fiestas de Navidad.

La gastronomía de la ciudad es una composición de los diferentes platos típicos de la gastronomía provincial, adaptada al frío clima provincial mediante platos energéticamente ricos que permitían afrontar las tareas cotidianas durante los fríos inviernos leoneses.

El embutido es pieza clave en este aspecto, por lo que en la ciudad de León se pueden saborear productos como la cecina de León, la morcilla de León, el chorizo de León y el Botillo del Bierzo, entre otros. Platos de mayor consistencia como el cocido Maragato, la sopa de trucha, la trucha frita y fría y el lechazo asado también son muy relevantes en la gastronomía de la ciudad, compendio de la presente en el resto de la provincia. Todo ello sin olvidar los platos de legumbres y hortalizas provenientes de las huertas leonesas, tales como las alubias de La Bañeza, pimientos de El Bierzo y de Fresno de la Vega y puerros de Sahagún.

En la bebida, destacan los vinos, avalados por dos denominaciones de origen, Bierzo y Tierras de León. Acompañando a estos, la limonada es un producto muy típico que se bebe en Semana Santa, en la tradición de "matar judíos".

Sin embargo, las tapas son sin lugar a duda el mayor exponente de la gastronomía de la ciudad. Las tapas pueden ser de todo tipo, desde guisos y platos calientes pasando por fritos, arroces hasta la más ligera de platos fríos y sencillos. Una peculiaridad de la tapa leonesa es que se sirve gratuitamente junto a la bebida en cualquier bar de la ciudad, aunque sin duda el lugar donde el tapeo alcanza su máximo esplendor es en el Barrio Húmedo, donde la concentración de bares y el esmero de los propietarios de los mismos a la hora de preparar las tapas ha propiciado el ambiente idóneo para el tapeo.

En cuanto a la repostería, bebiendo en este caso también de la provincia, sobresalen las Mantecadas de Astorga, los Lazos de San Guillermo de Cistierna, los Imperiales de La Bañeza, los Nicanores de Boñar y las Rosquillas de San Froilán. Destaca también el arroz con leche y la leche frita.

El deporte en la ciudad de León es regulado por la Concejalía de Deportes, mostrando su apoyo tanto a los equipos profesionales como a los que empiezan. Así, el Ayuntamiento ofrece una serie de Escuelas Deportivas para formar a todos aquellos niños que quieran iniciarse en algunos de los deportes ofertados.


Además de los deportes que se practican en las instalaciones municipales y de los equipos escolares, en la ciudad destacan una serie de entidades, aunque la que más logros ha obtenido en su historia es el Club Balonmano Ademar León, ganador de una Liga ASOBAL, dos copas ASOBAL, una Copa del Rey y dos Recopas de Europa. El otro club de balonmano de la ciudad es el Club León Balonmano (Cleba), miliante de la Liga femenina.

En fútbol, la ciudad cuenta con la Cultural y Deportiva Leonesa que juega en segunda división, cuyo máximo logro ha sido la estancia en Primera División en la temporada 1955/56, el C. F. Atlético Pinilla, el Club Deportivo Ejido de León, equipo de Regional y que jugó varias temporadas en Tercera División de España, CD Bosco y el Club Ruta Leonesa Fútbol Sala (O.E. Ram León), militante de la División de Plata de la LNFS.

En cuanto a baloncesto, León está representada, en categoría masculina, por el Baloncesto León S.A.D., durante muchos años equipo de la Liga ACB, llegando incluso a jugar la Copa Korać en la temporada 1996/97, En categoría femenina la ciudad contó con el Club Baloncesto San José, cuyo máximo logro fue ser subcampeón de la Copa de la Reina en 2008, pero debido a una difícil situación derivada de problemas de diversa índole, el club desapareció como tal en julio de 2009, cerrándose así la trayectoria del club de baloncesto femenino más laureado de la capital leonesa.

Otras disciplinas tienen representación en la ciudad en clubes como Rugby León (Pasgon Play), Sprint Atletismo León, Club Voleibol León, C. D. León Curling, Club Natación León, Club Ajedrez Ciudad de León, Equipo Ciclista Diputación de León (Diputación de León/Deyser), el Grupo de Montaña Yordas y el Club Ritmo de gimnasia rítmica.

León cuenta con numerosos centros deportivos dependientes de la Concejalía de Deportes, en los cuales se pueden practicar multitud de actividades. Entre ellos dos polideportivos (La Palomera y Sáenz de Miera), cuatro pabellones (La Torre, San Esteban, Margarita Ramos y Gumersindo Azcárate), los estadios Reino de León e Hispánico, el Palacio de Deportes, el Campo Hípico "El Parque" y el Área Deportiva de Puente Castro. En agosto de 2010 se inauguró el Centro Especializado de Alto Rendimiento, perteneciente al Consejo Superior de Deportes y situado junto al campus de la Universidad de León. Dedicado casi exclusivamente al atletismo, está especializado en los lanzamientos y consta de dos zonas de lanzamiento, una exterior y otra interior, y un edificio central de oficinas, vestuarios, etc. También es el lugar de entrenamiento del Club Ritmo de gimnasia rítmica.


El estadio municipal Reino de León comenzó su construcción en 1999 y fue inaugurado el 20 de mayo de 2001, con el nombre de Nuevo Antonio Amilivia, con un partido entre el equipo de fútbol de la ciudad, la Cultural, contra el Xerez, que se saldó con una victoria a favor de los locales.

El estadio, ubicado junto a la avenida Sáenz de Miera, paralela al río Bernesga, tiene un aforo de 13 451 espectadores, con las dimensiones exigidas por la FIFA para albergar partidos de carácter internacional, 105 x 68 metros. El estadio, ligado al club de fútbol de la ciudad, se encuentra en manos del ayuntamiento y ha vivido, aparte de las celebraciones propias del equipo, otros eventos deportivos de cierta entidad como un partido clasificatorio para la Eurocopa 2004 entre España y Armenia.

En septiembre de 2008 el nombre Nuevo Antonio Amilivia fue sustituido por el de Reino de León por decisión del ayuntamiento de la ciudad.


El Palacio de los Deportes de León es un recinto deportivo inaugurado en 1970. Ubicado en la Avenida Ingeniero Sáenz de Miera, junto al río Bernesga, tiene un aforo de 6500 espectadores. En él disputan sus partidos diversos equipos de la ciudad, entre los que destacan el Baloncesto León, el Club Balonmano Ademar y el Ruta Leonesa Fútbol Sala.
El área deportiva de Puente Castro es una instalación deportiva municipal del Ayuntamiento de León construida en 1998. Con un aforo de 4600 espectadores, es lugar de juego del equipo filial de la Cultural y Deportiva Leonesa. Las instalaciones se completan con un campo de hockey, otro de rugby y otro de entrenamiento.


Entre los acontecimientos deportivos que tienen lugar en León a lo largo del año, destaca el Magistral de Ajedrez Ciudad de León, en el cual participan algunos de los mejores jugadores del mundo de ajedrez, y que se celebra desde 1988, siendo en 2009 su XXII edición. En 2009 se celebró la I Media Maratón "Ciudad de León", en la que participaron 1200 atletas.

Por otra parte, la ciudad ha acogido distintos eventos deportivos de carácter nacional e internacional. En los últimos 25 años, desde 1984, León ha sido en 14 ocasiones meta o salida de etapas de la Vuelta ciclista a España, siendo 27 el total de veces desde el inicio de la prueba. León fue sede de la Copa del Rey de Baloncesto en las temporadas 1969-70 (por entonces llamada Copa del Generalísimo) y 1996-97. Asimismo, en 2003, la Selección Española de Fútbol jugó un partido oficial clasificatorio para la Eurocopa 2004 contra Armenia. En 2001 la ciudad acogió el Campeonato de Europa de ajedrez, resultando vencedor Países Bajos, y en 2008 León fue sede del XXIX Campeonato del Mundo de Lucha de Brazos, en el cual la delegación leonesa fue la triunfadora con casi 20 medallas de oro, y dándose la peculiaridad de que la Federación Internacional reconocía al País Leonés como miembro de pleno derecho.


Entre los deportistas profesionales originarios de León destaca Manuel Martínez, lanzador de peso y campeón del mundo en Birmingham 2003, campeón de Europa en Viena 2002 y campeón en los Juegos Mediterráneos de Túnez 2001, así como campeón en la Universiada de Pekín 2001, campeón de Europa Sub-23 en Ostrava 1994 y campeón de Europa junior en San Sebastián 1993, además de campeón de España en 16 ocasiones entre 1993 y 2008.

También es de León la gimnasta rítmica Carolina Rodríguez, ganadora de la competición individual en los Juegos Mediterráneos de 2013 y olímpica tanto en conjuntos (Atenas 2004, donde obtuvo la 7ª plaza) como en individual (Londres 2012, donde acabó 14ª). Además, es la gimnasta individual que más veces ha sido campeona de España del concurso general contando todas las categorías, con 10 títulos (1 en alevín, 1 en infantil, 1 en primera categoría y 7 en categoría de honor).

La ciudad de León participa en la iniciativa de hermanamiento de ciudades promovida, entre otras instituciones, por la Unión Europea. A partir de esta iniciativa se han establecido lazos con las siguientes localidades:




</doc>
<doc id="3342" url="https://es.wikipedia.org/wiki?curid=3342" title="Excentricidad (matemática)">
Excentricidad (matemática)

En matemática y geometría la excentricidad (ε) es un parámetro que determina el grado de desviación de una sección cónica con respecto a una circunferencia.

Este es un parámetro importante en la definición de elipse, hipérbola y parábola:

Para cualquier punto perteneciente a una sección cónica, la razón de su distancia a un punto fijo F (foco) y a una recta fija l (directriz) es siempre igual a una constante positiva llamada excentricidad (ε).

La designación tradicional de la excentricidad es la letra griega "ε" (llamada épsilon) y es preferible no usar la letra "e" para designar la misma porque e se reserva para la base de los logaritmos naturales o neperianos (véase número e).


Donde "a" es la longitud del semieje mayor en el caso de la elipse o semieje real en el caso de la hipérbola y "b" es la longitud del semieje menor en la elipse o semieje imaginario en la hipérbola.

Los cuerpos ligados gravitacionalmente entre sí describen órbitas en forma de elipse. La excentricidad de la órbita de un objeto se calcula de acuerdo con la fórmula anterior y expresa el grado de desviación con respecto a una órbita circular.

En el globo ocular, se llama excentricidad a la distancia desde cualquier punto de la retina a su centro. La resolución en la retina varía con la excentricidad ya que los conos se ubican principalmente en la zona de excentricidad 0°, que es el punto considerado como centro retiniano (llamado fóvea; zona de mayor poder resolutivo), y su densidad decrece con la excentricidad.


</doc>
<doc id="3343" url="https://es.wikipedia.org/wiki?curid=3343" title="Química inorgánica">
Química inorgánica

La química inorgánica se encarga del estudio integrado de la formación, composición, estructura y reacciones químicas de los elementos y compuestos inorgánicos (por ejemplo, ácido sulfúrico o carbonato de calcio); es decir, los que no poseen enlaces carbono-hidrógeno, porque estos pertenecen al campo de la química orgánica. Dicha separación no es siempre clara, como por ejemplo en la química organometálica que es una superposición de ambas.

Antiguamente se definía como la química de la materia inorgánica, pero quedó obsoleta al desecharse la hipótesis de la "fuerza vital," característica que se suponía propia de la materia viva que no podía ser creada y permitía la creación de las moléculas orgánicas. 

Tiene aplicaciones en todos los campos de la industria química, incluyendo catálisis, ciencia de materiales, pigmentos, surfactantes, recubrimientos, fármacos, combustibles y agricultura.

Muchos compuestos inorgánicos son compuestos iónicos, que consisten en cationes y aniones unidos por enlaces iónicos. Ejemplos de sales (que son compuestos iónicos) son el cloruro de magnesio MgCl que consiste en magnesio (cationes Mg) y cloruro (aniones Cl ) o el óxido de sodio, NaO, que consiste en cationes de sodio, Na, y aniones de oxígeno, O . En cualquier sal, las proporciones de los iones son tales que las cargas eléctricas se anulan, de modo que el compuesto es eléctricamente neutro. Los iones se describen por su estado de oxidación y su facilidad de formación se puede inferir a partir del potencial de ionización (para los cationes) o de la afinidad electrónica (para los aniones) de los elementos originales.

Clases importantes de compuestos inorgánicos son los óxidos, los carbonatos, los sulfatos y los haluros. Muchos compuestos inorgánicos se caracterizan por sus altos puntos de fusión. Las sales inorgánicas normalmente son malos conductores en estado sólido. Otra característica importante es su facilidad de cristalización. Mientras algunas sales (por ejemplo, el NaCl ) son muy solubles en agua, otras (por ejemplo, el AgCl) no lo son.

La reacción inorgánica más simple es el doble desplazamiento cuando, al mezclar dos sales, los iones se intercambian sin cambiar el estado de oxidación. En las reacciones rédox, sin embargo, un reactivo, el "oxidante", disminuye su estado de oxidación y otro reactivo, el "reductor", ve su estado de oxidación aumentado. El resultado neto es un intercambio de electrones. El intercambio de electrones también puede ocurrir indirectamente, por ejemplo, en las baterías, un concepto clave en la electroquímica .

Cuando un reactivo contiene átomos de hidrógeno, puede producirse una reacción al intercambiar protones en la química ácido-base . En una definición más general, cualquier especie química capaz de unirse a pares de electrones se llama un ácido de Lewis; a la inversa, cualquier molécula que tiende a donar un par de electrones se denomina base de Lewis. Como refinamiento de las interacciones ácido-base, la teoría ABDB toma en cuenta también la polarizabilidad y el tamaño de los iones.

Los compuestos inorgánicos se encuentran en la naturaleza como minerales. Por ejemplo, el suelo puede contener sulfuro de hierro como pirita o sulfato de calcio como yeso. Los compuestos inorgánicos también se encuentran con diversas funciones como biomoléculas: como electrolitos (cloruro de sodio), en el almacenamiento de energía (ATP) o en la construcción (el esqueleto de polifosfato en el ADN ).

El primer compuesto inorgánico importante hecho por el hombre fue el nitrato de amonio para la fertilización del suelo a través del proceso de Haber. Algunos compuestos inorgánicos se sintetizan para su uso como catalizadores como el óxido de vanadio (V) y el cloruro de titanio (III), o como reactivos en química orgánica, como el hidruro de litio y aluminio .

Las subdivisiones de la química inorgánica son la química organometálica, la química de clústers y la química bioinorgánica. Estos campos son áreas activas de investigación en química inorgánica, dirigidas hacia nuevos catalizadores, superconductores y terapias . 
La química inorgánica es un área altamente práctica de la ciencia. Tradicionalmente, la escala de la economía de una nación podía evaluarse por su producción de ácido sulfúrico. Los 20 productos químicos inorgánicos principales fabricados en Canadá, China, Europa, India, Japón y los Estados Unidos (datos de 2005) son: sulfato de aluminio, amoníaco, nitrato de amonio, sulfato de amonio , negro de carbón, cloro, ácido clorhídrico, hidrógeno, peróxido de hidrógeno, ácido nítrico, nitrógeno, oxígeno, ácido fosfórico, carbonato de sodio, clorato de sodio, hidróxido de sodio, silicato de sodio, sulfato de sodio, ácido sulfúrico y dióxido de titanio.

La fabricación de fertilizantes es otra aplicación práctica de la química industrial inorgánica.

La química inorgánica descriptiva se centra en la clasificación de los compuestos en función de sus propiedades. En parte, la clasificación se centra en la posición en la tabla periódica del elemento más pesado (el elemento con el mayor peso atómico) del compuesto, en parte agrupando los compuestos por sus similitudes estructurales. Al estudiar compuestos inorgánicos, a menudo se clasifican dentro de las diferentes partes de la química inorgánica (un compuesto organometálico se caracteriza por su química de coordinación y a su vez puede mostrar interesantes propiedades en estado sólido).

Las diferentes clasificaciones son:

Los compuestos de coordinación clásicos incluyen metales ligados a "pares solitarios" de electrones pertenecientes a los átomos del grupo principal de ligandos como HO, NH, Cl y CN. En los compuestos de coordinación "modernos" casi todos los compuestos orgánicos e inorgánicos pueden ser utilizados como ligandos. El "metal" normalmente corresponde a los grupos 3-13, así como a los trans-lantánidos y trans-actínidos, teniendo en cuenta que desde cierta perspectiva, todo los compuestos químicos pueden ser descritos como complejos de coordinación.

La estereoquímica de complejos de coordinación puede ser muy variada, como se desprende de la separación de Werner de dos enantiómeros del [Co((OH)Co(NH))], una manifestación temprana de que la quiralidad no es inherente solo a los compuestos orgánicos. Un tema incluido dentro de esta especialización es la química de coordinación supramolecular.


Estos compuestos contienen elementos de los grupos I, II, III, IV, V, VI, VII, 0 (excluyendo hidrógeno) de la tabla periódica. Debido a su reactividad a menudo similar, también pueden incluir elementos del grupo 3 (Sc, Y y La) y del grupo 12 (Zn, Cd y Hg), así como lantánidos y actínidos.

Los compuestos de grupo principal se conocen desde los inicios de la química, por ejemplo, el azufre elemental y el fósforo blanco destilable. Los experimentos con oxígeno, O , de Lavoisier y Priestley no solo identificaron un importante gas diatómico, sino que también abrieron el camino para describir compuestos y reacciones según las relaciones estequiométricas. El descubrimiento de una síntesis práctica de amoníaco con catalizadores de hierro por Carl Bosch y Fritz Haber a principios de la década de 1900 impactó profundamente a la humanidad, demostrando la importancia de la síntesis química inorgánica. Los compuestos de grupo principal típicos son SiO, SnCl y NO. Muchos compuestos del grupo principal también pueden clasificarse como organometálicos, ya que contienen grupos orgánicos, por ejemplo, B(CH). Los compuestos de grupo principal también se encuentran en la naturaleza, por ejemplo, el fosfato en el ADN, y por lo tanto pueden clasificarse como bioinorgánicos. A la inversa, los compuestos orgánicos que carecen de (muchos) hidrógenos como ligando pueden clasificarse como inorgánico, como los fullerenos, nanotubos y óxidos de carbono binarios.


Los compuestos que contienen metales del grupo 4 al 11 se consideran compuestos de metales de transición. Los compuestos con un metal del grupo 3 o 12 a veces también se incorporan a este grupo, aunque también pueden clasificarse como compuestos de grupo principal.

Los compuestos de metales de transición muestran una química de coordinación variada, yendo desde la tetraédrica para el titanio (p. ej., TiCl) hasta la planar cuadrada para algunos complejos de níquel o la octaédrica para los complejos de coordinación del cobalto. Pueden encontrarse algunos metales de transición en compuestos biológicamente importantes, como el hierro en la hemoglobina.


Normalmente, se considera que los compuestos organometálicos contienen el grupo M–C–H. El metal (M) en estas especies puede ser un elemento de grupo principal o un metal de transición. Operativamente, la definición de compuesto organometálico es más flexible, e incluye también complejos altamente lipófilos, tales como carbonilos metálicos e incluso alcóxidos metálicos.

Los compuestos organometálicos se consideran una categoría especial principalmente porque los ligandos orgánicos suelen ser sensibles a la hidrólisis u oxidación, lo que requiere que la química organometálica emplee métodos de preparación más especializados que los tradicionales en los complejos de tipo Werner. Los métodos de síntesis, especialmente la capacidad de manipular complejos en disolventes de bajo poder de coordinación, permiten ligandos muy débilmente coordinantes tales como hidrocarburos, H y N Dado que estos son ligandos están vinculados a la petroquímica en cierto sentido, la química organometálica se ha beneficiado enormemente de su relevancia para la industria.


Los clúster se pueden encontrar en todas las clases de compuestos químicos. De acuerdo con la definición comúnmente aceptada, un clúster consiste en un conjunto (como mínimo, triangular) de átomos que están directamente unidos entre sí. Pero los complejos dimetálicos con enlace metal-metal son especialmente relevantes en esta área. Los clúster se aparecen en sistemas inorgánicos "puros", en química organometálica, química de grupo principal y química bioinorgánica. La distinción entre clúster muy grandes y sólidos "brutos" es cada vez más borrosa. Esta interfaz es la base química de la nanociencia o la nanotecnología y surge específicamente del estudio de los efectos del tamaño cuántico en los clúster de seleniuro de cadmio. Por lo tanto, los grandes clústers pueden describirse como una estructura de átomos unidos con carácter intermedio entre una molécula y un sólido.


Por definición, estos compuestos aparecen en la naturaleza, pero el subcampo incluye especies antropogénicas, como algunos contaminantes (por ejemplo, metilmercurio) y fármacos (por ejemplo, cisplatino). El campo, que abarca también muchos aspectos de la bioquímica, incluye muchos tipos de compuestos, por ejemplo, los fosfatos en el ADN, y también complejos metálicos que contienen ligandos que van desde macromoléculas biológicas, normalmentepéptidos, hasta especies poco definidas, como el ácido húmico, o el agua (por ejemplo, cuando está coordinada en los complejos de gadolinio empleados para la RMN). Tradicionalmente, la química bioinorgánica se centraba en la transferencia de electrones y energía en proteínas relevantes para la respiración. La química inorgánica medicinal incluye el estudio de elementos no esenciales y esenciales con aplicaciones para diagnóstico y terapias.


Esta importante área se centra en la estructura, los enlaces y las propiedades físicas de los materiales. En la práctica, la química inorgánica de estado sólido utiliza técnicas como la cristalografía para la comprensión de las propiedades resultantes de las interacciones colectivas entre las subunidades del sólido. Dentro de la química del estado sólido se encuentran los metales y sus aleaciones o derivados intermetálicos. Los campos relacionados son la física de la materia condensada, la mineralogía y la ciencia de los materiales.


Una perspectiva alternativa en el área de la química inorgánica comienza con el modelo del átomo de Bohr y, utilizando las herramientas y modelos de la química teórica y la química computacional, se expande hacia la formación de enlaces en moléculas simples y luego más complejas. Las descripciones precisas de la mecánica cuántica para las especies multielectrónicas, que constituyen el ámbito de la química inorgánica, son difíciles. Este desafío ha generado muchos enfoques semicuantitativos o semi-empíricos que incluyen la teoría de orbitales moleculares y la teoría del campo del ligando. Paralelamente a estas descripciones teóricas, se emplean metodologías aproximadas, incluida la teoría del funcional de la densidad .

Las excepciones a las teorías, cualitativas y cuantitativas, son extremadamente importantes en el desarrollo de este campo. Por ejemplo, el Cu(OAc)(HO) es casi diamagnético por debajo de la temperatura ambiente, mientras que la teoría del campo cristalino predice que la molécula tendría que tener dos electrones no pareados. El desacuerdo entre la teoría cualitativa (paramagnética) y la observación (diamagnética) condujo al desarrollo de modelos para el "acoplamiento magnético". Estos modelos mejorados llevaron al desarrollo de nuevos materiales magnéticos y nuevas tecnologías.

La química inorgánica se ha beneficiado enormemente de las teorías cualitativas. Tales teorías son más fáciles de aprender, ya que requieren poca formación en teoría cuántica. Dentro de los compuestos del grupo principal, la teoría TRePEV predice, o al menos racionaliza, las estructuras de los compuestos del grupo principal, tales como la explicación de por qué el NH es piramidal, mientras que el ClF tiene forma de T. Para los metales de transición, la teoría del campo cristalino permite comprender el magnetismo de muchos complejos simples, por ejemplo, por qué el [Fe(CN)<nowiki>]</nowiki> tiene solo un electrón desapareado, mientras que el [Fe(HO)] tiene cinco. El enfoque cualitativo, especialmente potente para evaluar la estructura y la reactividad, comienza con la clasificación de las moléculas según el número de electrones, enfocándose en el número de electrones de valencia en el átomo central de una molécula, por lo general.

Un constructo central de la química inorgánica es la teoría de la simetría molecular. La teoría de grupos proporciona el lenguaje para describir las formas de las moléculas según su simetría de grupos de puntos. La teoría de grupos también permite factorizar y simplificar los cálculos teóricos.

Las características espectroscópicas se analizan y describen con respecto a las propiedades de simetría de, entre otros, los estados vibracionales o electrónicos. El conocimiento de las propiedades de simetría de los estados fundamentales y excitados permite predecir el número y la intensidad de absorciones en los espectros vibracionales y electrónicos. Una aplicación clásica de la teoría de grupos es la predicción del número de vibraciones C-O en complejos sustituidos carbonilo metal. Las aplicaciones más comunes de la simetría en la espectroscopia involucran espectros vibracionales y electrónicos.

Como herramienta didáctica, la teoría de grupos resalta los puntos en común y las diferencias entre los enlace de especies dispares, como WF y Mo(CO) o CO y NO .

Un enfoque cuantitativo alternativo a la química inorgánica se centra en las energías de reacción. Aunque este enfoque es altamente tradicional y empírico, resulta de gran utilidad. Los conceptos generales expresados en términos termodinámicos incluyen el potencial redox, la acidez y los cambios de fase . Un concepto clásico en termodinámica inorgánica es el ciclo de Born-Haber, que se utiliza para evaluar la energía de procesos elementales como la afinidad electrónica, algunos de los cuales no pueden observarse directamente.

Un aspecto importante y cada vez más popular de la química inorgánica se centra en las vías de reacción. Los mecanismos de reacción se discuten de manera diferente para diferentes clases de compuestos.

Los mecanismos de los compuestos del grupo principal de los grupos 13-18 se discuten generalmente en el contexto de la química orgánica (los compuestos orgánicos son compuestos del grupo principal, después de todo). Los elementos más pesados que C, N, O y F a menudo forman compuestos con más electrones de los que predice la regla del octeto, como se explica en el artículo sobre moléculas hipervalentes. Los mecanismos de sus reacciones difieren de los compuestos orgánicos por esta razón. Los elementos más ligeros que el carbono (B, Be, Li) así como el Al y el Mg frecuentemente forman estructuras deficientes en electrones que son electrónicamente similares a los carbocationes. Estas especies deficientes en electrones tienden a reaccionar a través de vías asociativas. La química de los lantánidos refleja muchos aspectos de la química vistos para el aluminio.

Los mecanismos para las reacciones de los metales de transición se discuten de manera diferente a los compuestos del grupo principal. El importante papel de los orbitales d en el enlace influye fuertemente en las vía y el grado de sustitución y disociación de ligandos. Estos temas se tratan en artículos sobre química de coordinación y ligandos. Se observan vías asociativas y disociativas.

Un aspecto general de la química mecanicista de los metales de transición es la labilidad cinética del complejo ilustrada por el intercambio de agua libre y enlazada en los complejos prototípicos [M(HO) ]:

Las tasas de intercambio de agua varían en 20 órdenes de magnitud en la tabla periódica, con los complejos de lantánidos en un extremo y las especies de Ir (III), las más lentas, en el otro.

Las reacciones rédox son prevalentes en los elementos de transición. Se consideran dos clases de reacciones rédox: reacciones de transferencia de átomos, como la adición oxidativa / eliminación reductiva y las de transferencia de electrones. Una reacción rédox fundamental es el "autointercambio", que implica la reacción degenerada entre un oxidante y un reductor. Por ejemplo, el permanganato y manganato, su derivado por reducción en un electrón, intercambian un electrón:

Los ligandos coordinados muestran una reactividad distinta a la de los ligandos libres. Por ejemplo, la acidez de los ligandos amoniaco en el [Co(NH3)] es elevada en relación con el NH en sí. Los alquenos enlazados a cationes metálicos son reactivos frente a los nucleófilos, mientras que los alquenos normalmente no lo son. El área de catálisis, extensa e industrialmente importante, se basa en la capacidad de los metales para modificar la reactividad de los ligandos orgánicos. La catálisis homogénea ocurre en solución y la catálisis heterogénea ocurre cuando los sustratos gaseosos o disueltos interactúan con las superficies de los sólidos. Se considera que la catálisis tradicionalmente homogénea es parte de la química organometálica y la catálisis heterogénea se discute en el contexto de la ciencia de las superficies, un subcampo de la química del estado sólido. Pero los principios químicos inorgánicos básicos son los mismos. Los metales de transición, casi exclusivamente, reaccionan con moléculas pequeñas como CO, H , O y CH. La importancia industrial de estas materias primas impulsa la ya de por sí activa área de la catálisis. Los ligandos también pueden sufrir reacciones de transferencia, como la transmetalación .

Debido a la amplia gama de elementos y las correspondientes propiedades de sus derivados, la química inorgánica está estrechamente asociada con muchos métodos de análisis. Los métodos más antiguos tendían a examinar las propiedades generales, como la conductividad eléctrica de las soluciones, los puntos de fusión, la solubilidad o la acidez . Con la llegada de la teoría cuántica y la expansión correspondiente de los equipos electrónicos, se han introducido nuevas herramientas para ensayar las propiedades electrónicas de las moléculas inorgánicas y los sólidos. Con frecuencia, estas mediciones proporcionan información relevante para los modelos teóricos. Por ejemplo, las mediciones en el espectro fotoelectrónico del metano demostraron que la descripción del enlace mediante los enlaces de dos centros y dos electrones predecible entre el carbono y el hidrógeno utilizando la teoría del enlace de valencia no es adecuada para describir los procesos de ionización de manera sencilla. Tales aportaciones llevaron a la popularización de la teoría de los orbitales moleculares como orbitales completamente deslocalizados y son una descripción simple más apropiada de la pérdida o la excitación de los electrones.

Las técnicas más comunes son:


Aunque algunas especies inorgánicas se pueden obtener en forma pura de la naturaleza, la mayoría se sintetizan en plantas químicas y en el laboratorio.

Los métodos de síntesis inorgánica se pueden clasificar según la volatilidad o solubilidad de los reactivos componentes. Los compuestos inorgánicos solubles se preparan utilizando métodos de síntesis orgánica. Para los compuestos que contienen metales reactivos con el aire, se siguen las técnicas de la línea de Schlenk y la caja de guantes. Los compuestos volátiles y los gases se manipulan en "colectores de vacío", que consisten en tuberías de vidrio interconectadas a través de válvulas, la totalidad de los cuales se pueden llevar a un vacío de 0,001 mm Hg o menos. Los compuestos se condensan utilizando nitrógeno líquido (t. eb.   78 K) u otros criógenos. Los sólidos se preparan típicamente usando hornos de tubo, con reactivos y productos sellados en contenedores, a menudo de sílice fundida SiO amorfo), o a veces materiales más especializados, tales como tubos de Ta soldados o navecillas de Pt. Productos y reactivos se transportan entre las zonas de temperatura para conducir a las reacciones. 

Los compuestos inorgánicos se dividen según su estructura en:

<references>



Apartados de interés de la química inorgánica incluyen:


Áreas de solapamiento con otros campos del conocimiento incluyen:


Hay muchos compuestos y sustancias inorgánicas de gran importancia, comercial y biológica. Entre ellos:





</doc>
<doc id="3346" url="https://es.wikipedia.org/wiki?curid=3346" title="Siglo XVIII">
Siglo XVIII

El (siglo dieciocho después de Cristo) o e.c. (siglo dieciocho de la era común) fue el octavo siglo del milenio en el calendario gregoriano. Comenzó el 1 de enero de 1701 y terminó el 31 de diciembre de 1800.

En la historia occidental, el también es llamado el «siglo de las luces», debido al nacimiento del movimiento intelectual conocido como Ilustración. En ese marco, el es fundamental para comprender el mundo moderno, pues muchos de los acontecimientos políticos, sociales, económicos, culturales e intelectuales de esos años han extendido su influencia hasta la actualidad. 

De hecho, para la historia occidental es el último de los siglos de la Edad Moderna y el primero de la Edad Contemporánea, tomándose convencionalmente como momento de división entre ambas los años 1705 (máquina de vapor), 1751 ("L'Encyclopédie"), 1776 (Independencia de Estados Unidos) o, más comúnmente, el 1789 (Revolución francesa).

Tras el caos político y militar vivido en el , el , no carente de conflictos, verá un notable desarrollo en las artes y las ciencias europeas de la mano de la Ilustración, un movimiento cultural caracterizado por la reafirmación del poder de la razón humana frente a la fe y la superstición. Las antiguas estructuras sociales, basadas en el feudalismo y el vasallaje, serán cuestionadas y acabarán por colapsar, al tiempo que, sobre todo en Inglaterra, se inicia la Revolución Industrial y el despegue económico de Europa. Durante dicho siglo, la civilización europea occidental afianzará su predominio en el mundo y extenderá su influencia por todo el orbe.












El absolutismo monárquico alcanza en toda Europa su mayor fuerza y esplendor. La burguesía se opone ya a la monarquía absoluta, pues aquella, que ya tenía el poder económico, aspira a alcanzar el poder político monopolizado por la nobleza y el clero.

Es la burguesía la que se enfrenta al sistema político-social establecido desde la Edad Media, fundamentado en el feudalismo y el vasallaje y aspiran a destruir al que llaman ""Antiguo Régimen"" es por eso que con la ayuda de las ideas de pensadores como Voltaire, Rousseau o Montesquieu desarrollan una nueva cultura: la Ilustración. Principios basados en la razón, la igualdad y la libertad.

Dicho movimiento empezaría motivar al pueblo, especialmente al francés, el cual habían sido las más duras víctimas del absolutismo impuesto por reyes como Luis XIV o Luis XV. Inspirados en el modelo que habían seguido los ingleses para conseguir el parlamentarismo deciden exigir que convoquen los Estados Generales, ya que los reyes franceses llevaban más de 70 años sin convocarlos. Tras la denegación el pueblo se alza en armas y toma la cárcel de la Bastilla ejecutando al alcaide y exhibiendo su cabeza linchada por todo París. Esto, junto a la Marcha de Versalles lo que hace que los reyes, Luis XVI y María Antonieta de Austria a huir a Austria de incógnito, sin embargo poco antes de cruzar la frontera son apresados y posteriormente condenados a muerte lo que hace que empiece la revolución francesa, acabando con el Antiguo Régimen en Francia. Desde entonces los ciudadanos empezaron a gozar de participación en la política, lo que llevaría siglos más tarde al famoso sufragio universal.

Siguiendo estos modelos, el ilustrado Thomas Jefferson junto al general Washington, John Adams y demás iniciaron la Guerra de Independencia de las trece colonias de la corona británica. El nuevo estado se basaría en la igualdad y la democracia, lo que más tarde llevaría a la creación de los Estados Unidos de América, la mayor superpotencia de la actualidad.

En el nuevo orden ilustrado europeo desaparecieron por completo las influencias religiosas que tanta importancia habían ejercido hasta mediados del siglo XVII, creando así un mayor patriotismo.

Las principales guerras del siglo incluyen: 














</doc>
<doc id="3348" url="https://es.wikipedia.org/wiki?curid=3348" title="Catálogo Köchel">
Catálogo Köchel

El catálogo Köchel ("Köchel Verzeichnis", "Köchelverzeichnis" en alemán) fue creado por Ludwig von Köchel en 1862 y enumera las obras musicales compuestas por Wolfgang Amadeus Mozart (1756–1791). Cada una de las obras de Mozart está designada por un número precedido de la abreviatura "K." o "KV". El orden cronológico que Köchel pretendió dar al catálogo es realmente válido para la mayoría de las obras. Sin embargo, en la primera edición del catálogo aparecen algunas obras de otros autores atribuidas erróneamente a Mozart, y omite otras auténticas que aún no habían sido descubiertas.

El catálogo tuvo varias revisiones; en particular la 3.ª edición, de 1936, llevada a cabo por Alfred Einstein, reubicó una gran cantidad de obras en el lugar que se estimó correcto, con el expediente de agregar una letra al número original, para no alterar el número propio de Köchel.

En las décadas posteriores a la muerte de Mozart hubo varios intentos de catalogar sus composiciones, pero no se realizaron con éxito hasta que Ludwig von Köchel las listó en su catálogo en 1862. La página 551 del catálogo Köchel fue titulada «"Chronologisch-thematisches Verzeichnis sämtlicher Tonwerke Wolfgang Amadé Mozarts"» («Catálogo cronológico y temático completo de la obra musical de Wolfgang Amadé Mozart»). El catálogo incluyó los primeros pentagramas de cada obra a la manera de "íncipit".

Köchel intentó ordenar las obras por orden cronológico, pero las composiciones anteriores a 1784 sólo tienen dataciones estimadas. Desde que Köchel elaboró el listado se han encontrado numerosas piezas que se han atribuido a otros autores o se les ha asignado una fecha diferente, por lo que han sido necesarias tres revisiones posteriores. Estas revisiones, especialmente la tercera llevada a cabo por Alfred Einstein en 1937, y la sexta de Franz Giegling, Gerd Sievers y Alexander Weinmann en 1964, incorporan numerosas correcciones.

Aun así, los números dados por Köchel son una forma rápida de estimar cuándo compuso Mozart una obra en particular. Según la fórmula creada por Neal Zaslaw, para un número KV mayor que 100, se puede dividir por 25 y sumar 10, para estimar la edad de Mozart cuando compuso la obra; si se suma 1756 (el año de su nacimiento), se obtiene el año aproximado de la composición. Las letras fueron añadidas como nuevos números para mantener la numeración original del listado de Köchel mientras se reordenaba y revisaba la secuencia cronológica. Otros apéndices y suplementos al catálogo son marcados como "KV A(nhang)".




</doc>
<doc id="3349" url="https://es.wikipedia.org/wiki?curid=3349" title="Ludwig von Köchel">
Ludwig von Köchel

Ludwig Alois Friedrich Ritter von Köchel (Stein, Baja Austria,14 de enero de 1800 - Viena, Austria, 3 de junio de 1877) fue un escritor, compositor, botánico, editor y admirador de la obra musical de Wolfgang Amadeus Mozart, cuya obra catalogó a mediados del siglo XIX. 

Estudió Derecho en Viena y fue durante 15 años tutor de los cuatro hijos del archiduque Carlos de Austria-Teschen. Esta actividad le ganó el título de "Ritter" (caballero) y le permitió vivir de rentas el resto de sus días. Se dedicó en forma privada a la investigación; realizó campañas de recolección de especímenes botánicos en el norte de África, la península ibérica, Gran Bretaña y Rusia, muy apreciadas por los científicos contemporáneos. Además se interesó por la geología y la mineralogía.

Su pasión por la música ("*1")—era miembro del Mozarteum de Salzburgo— lo llevó a sistematizar la lista de las composiciones de Mozart, que publicó en 1862 con el título de "Chronologisch-thematisches Verzeichnis sämtlicher Tonwerke Wolfgang Amadé Mozarts"
(«"Catálogo cronológico y temático de todas las obras musicales de Wolfgang Amadeus Mozart"»). Su apellido (y la inicial del mismo) quedaron desde entonces vinculados al célebre compositor; por ejemplo, el "Concierto para piano y orquesta n.º 23 en la mayor" de 1786 está identificado con el número "Köchel Verzeichnis" (abreviado KV o K.) 488. 

Von Köchel también financió en parte la primera edición completa de las obras de Mozart que publicó Breitkopf & Härtel desde 1877 hasta 1905. En ella se utilizó la clasificación en 24 categorías propuesta por Köchel.










</doc>
<doc id="3351" url="https://es.wikipedia.org/wiki?curid=3351" title="Resonancia">
Resonancia

En física, la resonancia describe el fenómeno de incremento de amplitud que ocurre cuándo la frecuencia de una fuerza periódicamente aplicada (o un componente de Fourier de esta) es igual o cercano a una frecuencia natural del sistema en el cuál actúa. Cuando una fuerza oscilatoria es aplicada en una frecuencia resonante de un sistema dinámico, el sistema oscilará en una amplitud más alta que cuándo la misma fuerza es aplicada en otra frecuencia no resonante. 

Las frecuencias en las que la amplitud de respuesta es un máximo relativo son también conocidas como frecuencias de resonancia o frecuencias resonantes del sistema. Pequeñas fuerzas periódicas que están cerca de una frecuencia resonante del sistema tiene la capacidad de producir oscilaciones de grandes amplitudes en el sistema debido al almacenamiento de energía vibratoria.

Los fenómenos de resonancia ocurren con todos los tipos de vibraciones u ondas: están la resonancia mecánica, resonancia acústica, resonancia electromagnética, resonancia magnética nuclear (NMR), resonancia de giro del electrón (ESR) y resonancia de funciones ondulatorias cuánticas. Los sistemas resonantes pueden ser usados para generar vibraciones de una frecuencia concreta (por ejemplo, instrumentos musicales), o escoger frecuencias concretas de una vibración compleja que contiene muchas frecuencias (por ejemplo, filtros).

El término resonancia (del latín resonantia, 'eco', de "resonare", "resonar") se origina del campo de la acústica, particularmente la ""resonancia" simpática" observada en instrumentos musicales, por ejemplo, cuando una cuerda empieza a vibrar y produce un sonido después de que una cuerda distinta fue golpeada. Otro ejemplo puede se la resonancia eléctrica, ocurre en un circuito con capacitores e inductores porque el campo magnético colapsante del inductor genera una corriente eléctrica en sus devanados que carga el capacitor, y entonces la descarga del capacitor proporciona una corriente eléctrica que genera un campo magnético en el inductor. Una vez que el circuito está cargado, la oscilación se auto-sostiene, y no hay acción de conducción periódica externa. Esto es análogo a un péndulo mecánico, donde la energía mecánica se convierte una y otra vez de cinética a potencial, y ambos sistemas son formas de osciladores armónicos simples.

La resonancia ocurre cuándo un sistema es capaz de almacenar y transferir fácilmente energía entre dos o más formas de almacenamiento diferentes (como energía cinética y energía potencial en el caso de un péndulo simple).Sin embargo hay algunas perdidas de ciclo a ciclo, llamado amortiguamiento. Cuándo el amortiguamiento es pequeño, la frecuencia de resonancia es aproximadamente igual a la frecuencia natural del sistema, el cual es una frecuencia de vibraciones no forzadas. Algunos sistemas tienen múltiples frecuencias resonantes.

Un ejemplo común es un columpio de parque, el cual actúa como un péndulo. Al empujar a una persona en un columpio en sincronía con el intervalo natural del columpio (su frecuencia de resonancia) hace el columpio suba cada vez más (amplitud máxima), mientras los intentos de empujar el columpio con un "tempo" más rápido o más lento produce que los arcos sean más pequeños. Esto se debe a que la energía que absorbe el columpio es maximizada cuándo los empujones se emparejan con las oscilaciones naturales del mismo.

La resonancia ocurre extensamente en la naturaleza, y es explotada en muchos dispositivos hechos por el hombre. Es el mecanismo por el cuál virtualmente todas las ondas sinusoidales y las vibraciones son generadas. Muchos sonidos que escuchamos, como cuando objetos duros de metal, vidrio, o la madera son golpeados, está causado por vibraciones resonantes breves en el objeto. La luz y otras radiaciones electromagnéticas de corta longitud de onda son producida por la resonancia a escala atómica, tales como los electrones en los átomos. Otros ejemplos de resonancia:


"Artículo principal: Puente de Tacoma (1940)"La torsión dramáticamente visible y rítmica que resultó en el colapso del puente de Tacoma Narrows en 1940, es caracterizada erróneamente como un ejemplo del fenómeno de resonancia en ciertos textos. Las vibraciones catastróficas que destruyeron el puente no fueron debidas a la resonancia mecánica simple, sino a la interacción entre el puente y el viento que lo atravesaba, produciendo un fenómeno conocido como "" o "," que empujaba periódicamente al puente, provocando el movimiento como una "". , padre de la aerodinámica de puentes, escribió un artículo acerca de este malentendido.

Los para le Estación Espacial Internacional (ISS) son controlados por piloto automático. Normalmente, los parámetros instalados para controlar el sistema de control del motor para el módulo Zvezda hacen que los motores de los cohete impulsen la Estación Espacial Internacional a una órbita superior. Los motores de los cohetes son montados en bisagras, generalmente la tripulación no se da cuenta de la operación. Sin embargo el 14 de enero de 2009, los parámetros cargados hicieron que el piloto automático balanceara los motores de los cohetes en oscilaciones cada vez más amplias, a una frecuencia de 0.5 Hz. Estas oscilaciones fueron captadas en vídeo y duraron 142 segundos.

La resonancia se manifiesta en varios sistemas lineales y no lineales como oscilaciones al rededor de un punto de equilibrio. Cuando el sistema es impulsado por una entrada sinusoidal externa, una salida del sistema puede oscilar en respuesta. La razón entre la amplitud de las oscilaciones estables de la salida y las oscilaciones de entrada es llamada ganancia, y la ganancia puede ser una función de la frecuencia de la entrada sinusoidal externa. Los picos en la ganancia a ciertas frecuencias corresponden a resonancias, donde la amplitud de las oscilaciones de salida son desproporcionalmente largas.

Dado que la mayoría de sistemas lineales y no lineales son modelados como cerca de su equilibrio, esta sección comienza con la derivada de la frecuencia resonante de un oscilador armónico amortiguado. La sección continúa con un el uso de un circuito RLC para ilustrar las conexiones entre resonancia y la función de transferencia de un sistema, respuesta de frecuencia, polos y ceros. Partiendo del ejemplo del circuito RLC, la sección generaliza esta relación para sistemas lineales de orden superior con múltiples entradas y salidas.

Considerando una masa anclada a un resorte el cual es impulsado por una fuerza sinusoidal externa aplicada en el que el sistema se amortigua. Leyes de Newton o La segunda ley de Newton toma la forma de :

Donde "m" es la masa, "x" el desplazamiento de la masa desde el punto de equilibrio, "F" la amplitud , "ω" es la frecuencia angular, "k" es la constante del resorte , y "c" es el coeficiente de viscocidad. Esto puede tomar la forma de :
Donde

En otras partes también se la conoce a "ω" como "frecuencia de resonancia". Sin embargo , como se muestra a continuación , cuando analizamos las oscilaciones del desplazamiento "x"("t"), La frecuencia de resonancia es cercana pero no igual a "ω". El ejemplo de un circuito RLC en la siguiente sección proporciona ejemplos de diferentes frecuencias resonantes para un mismo sistema.

La solución general de la ecuación () es la suma de una solución transitoria que depende de las condiciones iniciales y una Estado estacionario solución en estado estable que es independiente de las condiciones iniciales y depende solo de la amplitud "F", frecuencia angular externa "ω", la frecuencia natural "ω", y el factor de amortiguamiento "ζ". El Estado estacionario decae en un periodo de tiempo relativamente corto por lo que para estudiar la resonancia basta con considerar la solución en estado estacionario
Es posible escribir la solución de estado estacionario para "x"("t") como una función proporcional a la fuerza impulsadora con un cambio de Fase (onda) "φ",

Donde

El valor de la fase esta normalmente entre -180° y 0° por lo que representa un retraso o desface para los valores positivos y negativos del argumento arctan.

La resonancia ocurre cuando , a ciertas frecuencias de la conducción , la amplitud de estado estable de "x"("t") es grande en comparación con su amplitud en otras frecuencias . Para la masa en un resorte ,la resonancia corresponde físicamente a las oscilaciones de la masa que tienen grandes desplazamientos desde la posición de equilibrio del resorte a ciertas frecuencias de activación. Observando la amplitud de "x"("t") en función de la frecuencia "ω", la amplitud es máxima en la frecuencia natural.

formula_4

"ω" es la frecuencia de resonancia para este sistema.Nuevamente, tenga en cuenta que la frecuencia de resonancia no es igual a la frecuencia angular natural "ω" de el oscilador. Son proporcionales, y si el factor de amortiguamiento llega a cero son iguales ,pero si no es cero no son la misma frecuencia. Como se muestra en la figura , la resonancia también puede ocurrir en otras frecuencias cercanas a la frecuencia de resonancia, incluyendo "ω", pero la máxima respuesta es la frecuencia de resonancia.

Tome en cuenta que "ω" es real y distinta de cero si formula_5, por lo tanto, este sistema solo puede resonar cuando el oscilador armónico está significativamente amortiguado. Para sistemas con una relación de amortiguamiento muy pequeña y una frecuencia de conducción cercana a la frecuencia de resonancia, las oscilaciones de estado estable pueden llegar a ser muy grandes.

Para otros osciladores armónicos controlados y amortiguados cuyas ecuaciones de movimiento no se ven exactamente como en de la masa en un ejemplo de resorte, la frecuencia resonante permanece

Pero las definiciones de "ω" y "ζ" cambian según la física del sistema. Para un péndulo de longitud "l" y un pequeño desplazamiento angular "θ" <15° , La ecuación () se convierte en:

y por lo tanto

Ver también: 

Considere un circuito que consiste en un resistor con una resistencia "R" , un inductor con inductancia "L" y un condensador con capacitancia "C" conectado en serie con la corriente "i" ( "t" ) y accionado por un voltaje de fuente con el voltaje "v " ( "t" ). La caída de voltaje alrededor del circuito es
En lugar de analizar una solución propuesta a esta ecuación como en la masa del resorte del ejemplo anterior, esta sección analizará la respuesta de frecuencia de este circuito. Tomando la ecuación transformada de Laplace ( 4 ).

donde "I" ( "s" ) y "V " ( "s" ) son la transformada de Laplace de la corriente y el voltaje de entrada, respectivamente, y "s" es un parámetro de frecuencia en el dominio de Laplace. Reordenando los términos,

Resonancia de voltaje a través de un condensador

Un circuito RLC en serie presenta varias opciones para ubicar un lugar dónde medir un voltaje de salida. Supongamos que el voltaje de salida de interés es la caída de voltaje a través del condensador. Como se muestra arriba, en el dominio de Laplace este voltaje es

ó

Define para este circuito una frecuencia natural y una relación de amortiguamiento,

La relación entre el voltaje de salida y el voltaje de entrada se convierte en

Donde "H" ( "s" ) es la función de transferencia entre el voltaje de entrada y el voltaje de salida. Tenga en cuenta que esta función de transferencia tiene dos –raíces del polinomio en el denominador de la función de transferencia

y sin raíces no nulas del polinomio en el numerador de la función de transferencia. Además, tenga en cuenta que para "ζ" ≤ 1 , la magnitud de estos polos es la frecuencia natural "ω" y que para "ζ" <1 /, nuestra condición para la resonancia en el ejemplo del oscilador armónico, los polos están más cerca del eje imaginario que del eje real.

Al evaluar "H" ( "s" ) a lo largo del eje imaginario "s" = "iω" , la función de transferencia describe la respuesta de frecuencia de este circuito. De manera equivalente, la respuesta de frecuencia puede analizarse tomando la transformada de Fourier de la ecuación ( 4 ) en lugar de la transformada de Laplace. La función de transferencia, que también es compleja, puede ser escrita como ganancia y fase,

Un voltaje de entrada sinusoidal a la frecuencia "ω" da como resultado un voltaje de salida a la misma frecuencia que ha sido escalado por "G" ( "ω" ) y tiene un cambio de fase "Φ" ( "ω" ). La ganancia y la fase se pueden trazar frente a la frecuencia en un diagrama de Bode. Para el voltaje del condensador del circuito RLC, la ganancia de la función de transferencia "H" ( "iω" ) es
Observe la similitud entre la ganancia y la amplitud en la ecuación ( 3 ). Una vez más, la ganancia se maximiza a la frecuencia de resonancia.

Aquí, la resonancia corresponde físicamente a tener una amplitud relativamente grande para las oscilaciones en estado estacionario del voltaje a través del condensador en comparación con su amplitud en otras frecuencias de activación.

La frecuencia de resonancia no siempre tiene que tomar la forma dada en los ejemplos anteriores. Supongamos que para el circuito circuito RLC, el voltaje de salida de interés es el voltaje a través del inductor. Como se muestra arriba, en el dominio de Laplace, el voltaje a través del inductor es

usando las mismas definiciones para "ω" y "ζ" que en el ejemplo anterior. La función de transferencia entre "V " ( "s" ) y esta nueva "V " ( "s" ) a través del inductor es

Nótese que esta función de transferencia tiene los mismos polos que la función de transferencia en el ejemplo anterior, pero también tiene dos ceros en el numerador en "s" = 0 . Al evaluar "H" ( "s" ) a lo largo del eje imaginario, su ganancia se convierte en

En comparación con la ganancia en la ecuación ( 6 ) que usa el voltaje del condensador como salida, esta ganancia tiene un factor de "ω" en el numerador y, por lo tanto, tendrá una frecuencia de resonancia diferente que maximiza la ganancia. Esa frecuencia es

Entonces, para el mismo circuito RLC pero con el voltaje a través del inductor como salida, la frecuencia de resonancia ahora es "mayor" que la frecuencia natural, aunque todavía tiende hacia la frecuencia natural a medida que la relación de amortiguación llega a cero. Que el mismo circuito pueda tener diferentes frecuencias resonantes para diferentes opciones de salida no es contradictorio. Como se muestra en la ecuación ( 4), la caída de voltaje en el circuito se divide entre los tres elementos del circuito, y cada elemento tiene una dinámica diferente. El voltaje del condensador crece lentamente al integrar la corriente a lo largo del tiempo y, por lo tanto, es más sensible a las frecuencias más bajas, mientras que el voltaje del inductor crece cuando la corriente cambia rápidamente y, por lo tanto, es más sensible a las frecuencias más altas. Si bien el circuito en su conjunto tiene una frecuencia natural en la que tiende a oscilar, las diferentes dinámicas de cada elemento del circuito hacen que cada elemento resuene a una frecuencia ligeramente diferente.

Suponga que el voltaje de salida de interés es el voltaje a través de la resistencia. En el dominio de Laplace, el voltaje a través de la resistencia es

y usando la misma frecuencia natural y relación de amortiguamiento que en el ejemplo del capacitor, la función de transferencia es

Fíjese que esta función de transferencia también tiene los mismos polos que los ejemplos de circuitos RLC anteriores, pero solo tiene un cero en el numerador en "s" = 0. Para esta función de transferencia, su ganancia es

La frecuencia de resonancia que maximiza esta ganancia es

y la ganancia es una a esta frecuencia, por lo que el voltaje a través de la resistencia resuena "a" la frecuencia natural del circuito y a esta frecuencia la amplitud del voltaje a través de la resistencia es igual a la amplitud del voltaje de entrada.
<br>

Resonancia mecánica es la tendencia de un sistema mecánico a absorber más energía, cuando la frecuencia de sus oscilaciones coincide con la frecuencia natural de vibración del sistema que lo hace en otras frecuencias. Puede causar movimientos de balanceo violentos e incluso fallas catastróficas en estructuras construidas incorrectamente, incluidos puentes, edificios, trenes y aviones. Al diseñar objetos,los ingenieros deben asegurarse de que las frecuencias de resonancia mecánica de las partes componentes no coincidan con las frecuencias vibratorias de los motores u otras partes oscilantes, un fenómeno conocido como desastre de resonancia.

Evitar desastres de resonancia es una preocupación importante en cada proyecto de construcción, torre y puente construcción. Como contramedida, montaje flotante puede instalarse para absorber frecuencias resonantes y así disipar la energía absorbida. El edificio Taipei 101 se basa en un - un amortiguador de masa - para cancelar la resonancia. Además, la estructura está diseñada para resonar a una frecuencia que normalmente no ocurre. Los edificios en las zonas sísmicas a menudo se construyen para tener en cuenta las frecuencias oscilantes del movimiento del suelo esperado. Además, los objetos de diseño de ingeniero que tienen motores deben garantizar que las frecuencias resonantes mecánicas de las partes componentes no coincidan con las frecuencias vibratorias de los motores u otras partes fuertemente oscilantes.

Reloj mantiene el tiempo por resonancia mecánica en un volante, péndulo o cristal de cuarzo.

Se ha hipotetizado que la cadencia de los corredores es energéticamente favorable debido a la resonancia entre la energía elástica almacenada en la extremidad inferior y la masa del corredor.

La resonancia acústica es una rama de resonancia mecánica que se ocupa de las vibraciones mecánicas a través del rango de frecuencia del oído humano, en otras palabras sonido. Para los humanos, la audición normalmente se limita a frecuencias entre aproximadamente 20 Hz and 20,000 Hz (20 kHz), Muchos objetos y materiales actúan como resonadores con frecuencias resonantes dentro de este rango, y cuando se golpean vibran mecánicamente, empujando el aire circundante para crear sonido. olas. Esta es la fuente de muchos sonidos de percusión que escuchamos.

La resonancia acústica es una consideración importante para los constructores de instrumentos, ya que la mayoría de los instrumentos utilizan resonador, como cuerdas y el cuerpo de un violín, la longitud del tubo en una flauta, y la forma y tensión de una membrana de tambor.

Al igual que la resonancia mecánica, la resonancia acústica puede provocar una falla catastrófica del objeto en resonancia. El ejemplo clásico de esto es romper una copa de vino con sonido a la frecuencia resonante precisa de la copa, aunque esto es difícil en la práctica.

La resonancia eléctrica se produce en un circuito eléctrico a una " frecuencia de resonancia " particular cuando la impedancia del circuito es mínima en un circuito en serie o en un circuito en paralelo (generalmente cuando la función de transferencia alcanza su valor máximo absoluto). La resonancia en los circuitos se utiliza para transmitir y recibir comunicaciones inalámbricas, como televisión, teléfonos celulares y radio.

Una cavidad óptica, también llamada "resonador óptico", es una disposición de espejo que forma una onda estacionaria resonador para luz. Las cavidades ópticas son un componente principal de medio activo, rodean el medio de ganancia y proporcionan realimentación de la luz láser. También se usan en oscilador paramétrico óptico y algunos interferómetro. La luz confinada en la cavidad se refleja varias veces produciendo ondas estacionarias para ciertas frecuencias resonantes. Los patrones de onda estacionaria producidos se denominan "modos". Modos longitudinales difieren solo en frecuencia mientras que el modo transversal difieren para diferentes frecuencias y tienen diferentes patrones de intensidad en la sección transversal del haz. Los Resonadores de anillo y Galería susurrante son ​​ejemplos de resonadores ópticos que no forman ondas estacionarias.

Los diferentes tipos de resonadores se distinguen por las distancias focales de los dos espejos y la distancia entre ellos; Los espejos planos no se usan con frecuencia debido a la dificultad de alinearlos con precisión. La geometría (tipo de resonador) debe elegirse para que el haz permanezca estable, es decir, el tamaño del haz no continúa creciendo con cada reflexión. Los tipos de resonador también están diseñados para cumplir con otros criterios, como la cintura mínima del haz o no tener un punto focal (y, por lo tanto, luz intensa en ese punto) dentro de la cavidad.

Las cavidades ópticas están diseñadas para tener un " factor Q " . Un haz refleja un gran número de veces con poca atenuación - por lo tanto, el la frecuencia ancho de línea del haz es pequeña en comparación con la frecuencia del láser.

Resonancias ópticas adicionales son resonancias de modo guiado y resonancias de plasmón superficial, que dan como resultado una reflexión anómala y campos evanescentes altos en la resonancia. En este caso, los modos resonantes son modos guiados de una guía de onda o modos de plasmón superficial de una interfaz dieléctrico-metálica. Estos modos generalmente están excitados por una rejilla de longitud de onda inferior.

En mecánica celeste, se produce una resonancia orbital cuando dos cuerpos en órbita ejercen una influencia gravitacional periódica entre sí, generalmente debido a que su período orbital está relacionado por una relación de dos enteros pequeños. Las resonancias orbitales mejoran en gran medida la influencia gravitacional mutua de los cuerpos. En la mayoría de los casos, esto resulta en una interacción "inestable", en la cual los cuerpos intercambian ímpetu y cambian de órbita hasta que la resonancia ya no existe. En algunas circunstancias, un sistema resonante puede ser estable y autocorregible, de modo que los cuerpos permanezcan en resonancia. Los ejemplos son la resonancia 1: 2: 4 de las lunas de Júpiter Ganímedes, Europa y Io, y la resonancia 2: 3 entre Plutón y Neptuno. Las resonancias inestables con las lunas internas de Saturno dan lugar a huecos en los anillos de Saturno. El caso especial de resonancia 1: 1 (entre cuerpos con radios orbitales similares) hace que grandes cuerpos del Sistema Solar despejar el vecindario alrededor de sus órbitas expulsando casi todo lo demás a su alrededor; Este efecto se utiliza en la actual definición de planeta.

Resonancia magnética nuclear (RMN) es el nombre dado a un fenómeno de resonancia física que implica la observación de mecánica cuántica magnético propiedades de un átomo ic núcleo en presencia de un campo magnético externo aplicado. Muchas técnicas científicas explotan los fenómenos de RMN para estudiar física molecular, cristal y materiales no cristalinos a través de espectroscopia de resonancia magnética nuclear. La NMR también se usa habitualmente en técnicas avanzadas de imágenes médicas, como imagen por resonancia magnética (IRM).

Todos los núcleos que contienen números impares de nucleón s tienen un momento magnético y [[momento angular] intrínseco. Una característica clave de la NMR es que la frecuencia de resonancia de una sustancia en particular es directamente proporcional a la intensidad del campo magnético aplicado. Es esta característica la que se explota en las técnicas de imagen; Si una muestra se coloca en un campo magnético no uniforme, las frecuencias resonantes de los núcleos de la muestra dependen de en qué parte del campo se encuentren. Por lo tanto, la partícula puede ubicarse con bastante precisión por su frecuencia de resonancia.

La [[resonancia paramagnética electrónica]], también conocida como "resonancia de espín electrónico" (ESR), es una técnica espectroscópica similar a la RMN, pero utiliza electrones no apareados. Los materiales para los que se puede aplicar son mucho más limitados ya que el material debe tener un giro no apareado y ser [[paramagnético]].

El [[efecto Mössbauer]] es la emisión resonante y [[retroceso]] - libre y absorción de fotones [[rayos gamma]] por átomos unidos en forma sólida.

La [[Resonancia (física de partículas) | Resonancia en física de partículas]] aparece en circunstancias similares a [[física clásica]] a nivel de [[mecánica cuántica]] y [[teoría cuántica de campos]]. Sin embargo, también pueden considerarse partículas inestables, con la fórmula anterior válida si " Γ " es [[Desintegración de partículas | tasa de decaimiento]] y " Ω " reemplazado por la masa de la partícula " METRO". En ese caso, la fórmula proviene del propagador ] de la partícula, con su masa reemplazada por el número complejo "M" + "iΓ". La fórmula está más relacionada con la [[desintegración de partículas]] por el [[teorema óptico]].

Un sistema físico puede tener tantas frecuencias resonantes como grados de libertad; cada grado de libertad puede vibrar como un oscilador armónico. Los sistemas con un grado de libertad, como una masa en un resorte, ruedas de equilibrio, péndulo y circuitos sintonizados LC tienen una frecuencia resonante. Los sistemas con dos grados de libertad, como péndulos acoplados y transformador resonantes pueden tener dos frecuencias resonantes. A medida que crece el número de osciladores armónicos acoplados, el tiempo que lleva transferir energía de uno a otro se vuelve significativo. Las vibraciones en ellos comienzan a viajar a través de los osciladores armónicos acoplados en ondas, de un oscilador al siguiente
Los objetos extendidos que pueden experimentar resonancia debido a las vibraciones dentro de ellos se denominan resonador, como tubo de órgano, cuerda vibrante, cuarzo, microondas y láser. Como se puede ver que están hechas de muchas partes móviles acopladas (como átomos), pueden tener muchas frecuencias resonantes. Las vibraciones dentro de ellas viajan como ondas, a una velocidad aproximadamente constante, rebotando de un lado a otro entre los lados del resonador. Si la distancia entre los lados es " d ", la longitud de un viaje de ida y vuelta es 2"d ". Para causar resonancia, la fase de una onda sinusoidal después de un viaje de ida y vuelta debe ser igual a la fase inicial, por lo que las ondas refuerzan la oscilación. Entonces, la condición para la resonancia en un resonador es que la distancia de ida y vuelta, 2"d ", sea igual a un número entero de longitudes de onda " λ " de la onda:

formula_30

Si la velocidad de una onda es " v ", la frecuencia es: "f" =  entonces las frecuencias resonantes son:

formula_31
Entonces, las frecuencias resonantes de los resonadores, 

llamadas modos normales, son múltiplos igualmente espaciados de una frecuencia más baja llamada frecuencia fundamental. Los múltiplos a menudo se llaman armónicos. Puede haber varias series de frecuencias resonantes, correspondientes a diferentes modos de oscilación.

El [[factor de calidad]] es una cantidad adimensional (ver [[magnitud adimensional]]), un parámetro que describe un movimiento [[damping|amortiguado]] como un [[Oscilación|oscilador]] o [[resonador]].
Un mayor "Q" indica una menor tasa de pérdida de energía en relación con la energía almacenada del oscilador,por lo que las oscilaciones se detendrán lentamente. Un péndulo de un rodamiento de alta calidad suspendido , oscilando en el aire tiene un mayor "Q", mientras que un péndulo sumergido en aceite tendría un menor "Q". Para mantener un sistema en resonancia con amplitud constante se le debe proporcionar energía externamente, la energía provista en cada ciclo debe ser menor que la energía almacenada en el sistema (es decir, la suma del potencial y la cinética) por un factor de .Los osciladores con factores de alta calidad tienen bajo [[damping|amortiguación]], lo que tiende a hacerlos resonar más tiempo

Los [[Resonador|resonadores]] con factores Q más altos resuenan con amplitudes mayores (a la frecuencia resonante) pero tienen un rango más pequeño de frecuencias alrededor de la frecuencia a la que resuenan. El rango de frecuencias en el que resuena el oscilador se llama ancho de banda (bandwidth en inglés). Por lo tanto, un alto "Q" [[Circuito RLC|circuito sintonizado]] en un radio recibidor sería más difícil de sintonizar, pero tendría mayor selectividad,haría un mejor trabajo al filtrar las señales de otras estaciones que se encuentran cerca del espectro.Los osciladores de alta "Q" operan en un rango más pequeño de frecuencias y son más estables. (Ver [[ruido de fase]].)

El factor de calidad de los osciladores varía sustancialmente de un sistema a otro. En los sistemas para los cuales la amortiguación es importante (como amortiguadores que evitan que una puerta se cierre de golpe) tienen "Q" = , por otro lador los relojes, láseres y otros sistemas que necesitan resonancia fuerte o estabilidad de alta frecuencia necesitan factores de alta calidad, por ejemplo los [[Diapasón|diapasones]] 
tienen un factor de calidad de alrededor de "Q" = 1000, mientras que el factor de calidad de un [[reloj atómico]] y un alto [[Cavidad óptica|laser]] pueden alcanzar a Q =10 e incluso pueden llegar a ser mayores. Los físicos e ingenieros utilizan muchas cantidades alternativas para describir cuán amortiguado está un oscilador que están estrechamente relacionados con su factor de calidad.

[[File:Universal Resonance Curve.svg|thumb|upright=1.3|"Curva de resonancia universal", es una aproximación simétrica a la respuesta normalizada de un circuito resonante; La [[abscisa]] son desviaciones de la frecuencia central, en unidades de frecuencia central divididas por 2Q; [[ordenada]] es la amplitud relativa, y la fase en ciclos; las curvas discontinuas comparan el rango de respuestas de circuitos bipolares para un valor "Q" de 5; para "Q" más altos, hay menos desviación de la curva universal. las cruces marcan los bordes del ancho de banda de 3 dB(ganancia 0.707, cambio de fase 45° o 0.125 ciclo).]] 
La respuesta exacta de una resonancia, especialmente para frecuencias alejadas de la frecuencia resonante, depende de los detalles del sistema físico y, por lo general, no es exactamente simétrica con respecto a la frecuencia resonante, como se ilustra para el [[oscilador armónico]]. Para un [[oscilador]] lineal ligeramente amortiguado con una frecuencia de resonancia "Ω", la "intensidad" de las oscilaciones "I" cuando el sistema funciona con una frecuencia de activación "ω" se aproxima típicamente mediante una formula que es simétrica con respecto a la frecuencia de resonancia:

Donde la susceptibilidad formula_33 vincula la amplitud del oscilador a la fuerza impulsora en el espacio de frecuencias:

formula_34
La intensidad se define como el cuadro de la amplitud de las oscilaciones. Esta es una [[función lorentziana]], o [[distribución de Cauchy]], y esta respuesta se encuentra en muchas situaciones físicas que involucran sistemas resonantes. "Γ" es un parámetro que depende de la [[amortiguación]] del oscilador, y se conoce como el "ancho de línea" de la resonancia. Los osciladores muy amortiguados tienden a tener anchos de línea amplios, y responden a un rango más amplio de frecuencias de conducción alrededor de la frecuencia de resonancia. El ancho de línea es inversamente proporcional al [[factor de calidad]], que es una medida de la nitidez de la resonancia.

En ingeniería de difusión e [[ingeniería electrónica]], esta respuesta simétrica aproximada se conoce como la "curva de resonancia universal", un concepto introducido por Frederick Terman en 1932 para simplificar el análisis aproximado de circuitos de radio con un rango de frecuencias centrales y valores "Q".

[[Categoría:Teoría de control]]
[[Categoría:Ondas]]

</doc>
<doc id="3352" url="https://es.wikipedia.org/wiki?curid=3352" title="Solubilidad">
Solubilidad

La solubilidad es la capacidad de una sustancia de disolverse en otra llamada disolvente. También hace referencia a la masa de soluto que se puede disolver en determinada masa de disolvente, en ciertas condiciones de temperatura, e incluso presión (en caso de un soluto gaseoso). La solubilidad la podemos encontrar en diferentes mezclas como por ejemplo en el ion común es muy difícil encontrar ya que el ion común es principal en la solubilidad . Si en una disolución no se puede disolver más soluto se dice que la disolución está saturada. Bajo ciertas condiciones la solubilidad puede sobrepasar ese máximo y pasa a denominarse solución sobresaturada.Por el contrario, si la disolución admite aún más soluto, se dice que se encuentra insaturada.

No todas las sustancias se disuelven en un mismo solvente. Por ejemplo, en el agua, se disuelve el alcohol y la sal, en tanto que el aceite y la gasolina no se disuelven en agua. En la solubilidad, el carácter polar o apolar de la sustancia influye mucho, ya que, debido a este carácter, la sustancia será más o menos soluble; por ejemplo, los compuestos con más de un grupo funcional presentan gran polaridad por lo que no son solubles en éter etílico. Los compuestos poco reactivos, como las parafinas, compuestos aromáticos y los derivados halogenados tienen menor solubilidad.

El término solubilidad se utiliza tanto para designar al fenómeno cualitativo del proceso de disolución como para expresar cuantitativamente la concentración de las soluciones. La solubilidad de una sustancia depende de la naturaleza del solvente y del soluto, así como de la temperatura y la presión del sistema, es decir, de la tendencia del sistema a alcanzar el valor máximo de entropía. Al proceso de interacción entre las moléculas del solvente y las partículas del soluto para formar agregados se le llama solvatación y si el solvente es agua, hidratación.

La solubilidad se define para fases específicas. Por ejemplo, la solubilidad de aragonito y calcita en el agua se espera que difieran, si bien ambos son polimorfos de carbonato de calcio y tienen la misma fórmula molecular.

La solubilidad de una sustancia en otra está determinada por el equilibrio de fuerzas intermoleculares entre el solvente y el soluto, y la variación de entropía que acompaña a la solvatación. Factores como la temperatura y la presión influyen en este equilibrio, cambiando así la solubilidad.

La solubilidad también depende en gran medida de la presencia de otras sustancias disueltas en el solvente como por ejemplo la existencia de complejos metálicos en los líquidos. La solubilidad dependerá también del exceso o defecto de algún ion común, con el soluto, en la solución; tal fenómeno es conocido como el efecto del ion común. En menor medida, la solubilidad dependerá de la fuerza iónica de las soluciones. Los dos últimos efectos mencionados pueden cuantificarse utilizando la ecuación de equilibrio de solubilidad.

Para un sólido que se disuelve en una reacción redox, la solubilidad se espera que dependa de las posibilidades (dentro del alcance de los potenciales en las que el sólido se mantiene la fase termodinámicamente estable). Por ejemplo, la solubilidad del oro en el agua a alta temperatura se observa que es casi de un orden de magnitud más alta cuando el potencial redox se controla mediante un tampón altamente oxidante redox FeO-FeO que con un tampón moderadamente oxidante Ni-NiO.

La solubilidad (metaestable) también depende del tamaño físico del grano de cristal o más estrictamente hablando, de la superficie específica (o molar) del soluto. Para evaluar la cuantificación, se debe ver la ecuación en el artículo sobre el equilibrio de solubilidad. Para cristales altamente defectuosos en su estructura, la solubilidad puede aumentar con el aumento del grado de desorden. Ambos efectos se producen debido a la dependencia de la solubilidad constante frente a la denominada energía libre de Gibbs asociada con el cristal. Los dos últimos efectos, aunque a menudo difíciles de medir, son de relevante importancia en la práctica pues proporcionan la fuerza motriz para determinar su grado de precipitación, ya que el tamaño de cristal crece de forma espontánea con el tiempo.

La solubilidad de un soluto en un determinado solvente principalmente depende de la temperatura. Para muchos sólidos disueltos en el agua líquida, la solubilidad aumenta con la temperatura hasta 100 °C, aunque existen casos que presentan un comportamiento inverso. En la mayoría de los casos en el agua líquida a altas temperaturas la solubilidad de los solutos iónicos tiende a aumentar debido al cambio de las propiedades y la estructura del agua líquida, que reduce la constante dieléctrica de un disolvente menos polar.

Los solutos gaseosos muestran un comportamiento más complejo con la temperatura. Al elevarse la temperatura, los gases generalmente se vuelven menos solubles en agua (el mínimo que está por debajo de 120 °C para la mayoría de gases), pero más solubles en solventes orgánicos.

El gráfico muestra las curvas de solubilidad de algunas sales sólidas inorgánicas típicas. Muchas sales se comportan como el nitrato de bario y el arseniato ácido disódico, y muestran un gran aumento de la solubilidad con la temperatura. Algunos solutos (por ejemplo, cloruro de sodio (NaCl) en agua) exhiben una solubilidad bastante independiente de la temperatura. Unos pocos, como el sulfato de cerio (III) y el carbonato de litio, se vuelven menos solubles en agua a medida que aumenta la temperatura. Esta dependencia de la temperatura se refiere a veces como «retrógrada» o «solubilidad inversa». En ocasiones, se observa un patrón más complejo, como con sulfato de sodio, donde el cristal decahidrato menos soluble pierde agua de cristalización a 32 °C para formar una fase anhidra menos soluble. 

La solubilidad de los compuestos orgánicos casi siempre aumenta con la temperatura. La técnica de la recristalización, utilizado para la purificación de sólidos, depende de un soluto de diferentes solubilidades en un solvente caliente y fría. Existen algunas excepciones, tales como determinadas ciclodextrinas.

La solubilidad de los gases varía no solo con la temperatura sino además con la presión ejercida sobre el mismo. De esta manera, la cantidad de un soluto gaseoso que puede disolverse en un determinado solvente, aumenta al someterse a una presión parcial mayor (véase Ley de Henry). A nivel industrial, esto se puede observar en el envasado de bebidas gaseosas por ejemplo, donde se aumenta la solubilidad del dióxido de carbono ejerciendo una presión de alrededor de 4 atm. 

Una forma muy común de encontrar los valores que describen cuantitativamente la solubilidad de un soluto es encontrar el máximo número de gramos de soluto que pueden disolverse en una cantidad dada de solvente, teniendo esto en cuenta podemos expresar la solubilidad en moles por litro ( a esto se le conoce como solubilidad molar) solo si se sabe la masa molar de la sustancia. En el caso particular de las sales iónicas que son sólo ligeramente solubles, se suele cuantificar su solubilidad mediante el estudio del siguiente equilibrio: 

<chem>MX(s) <=>M+(ac) + X-(ac)</chem> 

cuando en un equilibrio participa alguna sustancia sólida, la concentración de ésta no aparece en la expresión de la constante de equilibrio, ya que permanece constante. Esto ocurre con la concentración de MX, por lo que la expresión queda : 

<chem>K[MX] = Kps = [M+][X-]</chem>

El producto de solubilidad de un compuesto iónico, es el producto de las concentraciones molares de los iones constituyentes, cada uno elevado a la potencia de su coeficiente estequiométrico en la ecuación de equilibrio.


</doc>
<doc id="3353" url="https://es.wikipedia.org/wiki?curid=3353" title="Mozart (desambiguación)">
Mozart (desambiguación)

El apellido Mozart lo comparten varias personas pertenecientes a la familia Mozart:

Mozart también puede referirse a:

</doc>
<doc id="3354" url="https://es.wikipedia.org/wiki?curid=3354" title="Química">
Química

La química es la ciencia que estudia la composición, estructura y propiedades de la materia, así como los cambios que esta experimenta durante las reacciones químicas y su relación con la energía. Linus Pauling la define como la ciencia que estudia las sustancias, su estructura (tipos y formas de acomodo de los átomos), sus propiedades y las reacciones que las transforman en otras sustancias en referencia con el tiempo. La química se ocupa principalmente de las agrupaciones supratómicas, como son los gases, las moléculas, los cristales y los metales, estudiando su composición, propiedades estadísticas, transformaciones y reacciones. La química también incluye la comprensión de las propiedades e interacciones de la materia a escala atómica. 

La mayoría de los procesos químicos se pueden estudiar directamente en el laboratorio, usando una serie de técnicas a menudo bien establecidas, tanto de manipulación de materiales como de comprensión de los procesos subyacentes. Una aproximación alternativa es la proporcionada por las técnicas de modelado molecular, que extraen conclusiones de modelos computacionales. La química es llamada a menudo «ciencia central», por su papel de conexión con las otras ciencias naturales.

La química moderna se desarrolló a partir de la alquimia, una práctica protocientífica de carácter esotérico, pero también experimental, que combinaba elementos de química, metalurgia, física, medicina, biología, entre otras ciencias y artes. Esta fase termina con la revolución química, con el descubrimiento de los gases por Robert Boyle, la ley de conservación de la materia y la teoría de la combustión por oxígeno postuladas por el científico francés Antoine Lavoisier. La sistematización se hizo patente con la creación de la tabla periódica de los elementos y la introducción de la teoría atómica, cuando los investigadores desarrollaron una comprensión fundamental de los estados de la materia, los iones, los enlaces químicos y las reacciones químicas. Desde la primera mitad del siglo XIX, el desarrollo de la química lleva aparejado la aparición y expansión de una industria química de gran relevancia en la economía y la calidad de vida actuales.

Las disciplinas de la química se agrupan según la clase de materia bajo estudio o el tipo de estudio realizado. Entre estas se encuentran la química inorgánica, que estudia la materia inorgánica; la química orgánica, que estudia la materia orgánica; la bioquímica, que estudia las sustancias existentes en organismos biológicos; la fisicoquímica que comprende los aspectos estructurales y energéticos de sistemas químicos a escalas macroscópica, molecular y atómica, y la química analítica, que analiza muestras de materia y trata de entender su composición y estructura mediante diversos estudios y reacciones.
La palabra "química" procede de la palabra «alquimia», el nombre de un antiguo conjunto de prácticas protocientíficas que abarcaba diversos elementos de la actual ciencia, además de otras disciplinas muy variadas como la metalurgia, la astronomía, la filosofía, el misticismo o la medicina. La alquimia, practicada al menos desde alrededor del año 330, además de buscar la fabricación de oro, estudiaba la composición de las aguas, la naturaleza del movimiento, del crecimiento, de la formación de los cuerpos y su descomposición, la conexión espiritual entre los cuerpos y los espíritus. Un alquimista solía ser llamado en lenguaje cotidiano «químico», y posteriormente (oficialmente, a partir de la publicación, en 1661, del libro "El químico escéptico", del químico irlandés Robert Boyle) se denominaría "química" al arte que practicaba.

A su vez, "alquimia" deriva de la palabra árabe "al-kīmīā" (الکیمیاء). En su origen, el término fue un préstamo tomado del griego, de las palabras χημία o χημεία ("khemia" y "khemeia", respectivamente). La primera podría tener origen egipcio. Muchos creen que "al-kīmīā" deriva de χημία, que a su vez deriva de la palabra "Chemi" o "Kimi" o "Kham", que es el nombre antiguo de Egipto en egipcio. Según esa hipótesis, "khemeia" podría ser "el arte egipcio". La otra alternativa es que "al-kīmīā" derivara de χημεία, que significa «fusionar». Una tercera hipótesis, con más adeptos en la actualidad, dice que "khemeia" deriva del griego "khumos", el jugo de una planta, y que vendría a significar "el arte de extraer jugos", y en este caso "jugo" podría ser un metal, y por tanto podría ser "el arte de la metalurgia"

La definición de "química" ha cambiado a través del tiempo, a medida que nuevos descubrimientos se han añadido a la funcionalidad de esta ciencia. El término "química", a vista del reconocido científico Robert Boyle, en 1661, se trataba del área que estudiaba los principios de los cuerpos mezclados.

En 1662, la química se definía como un arte científico por el cual se aprende a disolver cuerpos, obtener de ellos las diferentes sustancias de su composición y cómo unirlos después para alcanzar un nivel mayor de perfección. Esto según el químico Christopher Glaser.

La definición de 1730 para la palabra "química", usada por Georg Stahl, era el arte de entender el funcionamiento de las mezclas, compuestos o cuerpos hasta sus principios básicos, y luego volver a componer esos cuerpos a partir de esos mismos principios. 

En 1837, Jean-Baptiste Dumas consideró la palabra "química" para referirse a la ciencia que se preocupaba de las leyes y efectos de las fuerzas moleculares. Esta definición luego evolucionaría hasta que, en 1947, se le definió como la ciencia que se preocupaba de las sustancias: su estructura, sus propiedades y las reacciones que las transforman en otras sustancias (caracterización dada por Linus Pauling).

Más recientemente, en 1988, la definición de química se amplió, para ser «el estudio de la materia y los cambios que implica», según palabras del profesor Raymond Chang.

La ubicuidad de la química en las ciencias naturales hace que sea considerada una de las ciencias básicas. La química es de gran importancia en muchos campos del conocimiento, como la ciencia de materiales, la biología, la farmacia, la medicina, la geología, la ingeniería y la astronomía, entre otros.

Los procesos naturales estudiados por la química involucran partículas fundamentales (electrones, protones y neutrones), partículas compuestas (núcleos atómicos, átomos y moléculas) o estructuras microscópicas como cristales y superficies. 

Desde el punto de vista microscópico, las partículas involucradas en una reacción química pueden considerarse un sistema cerrado que intercambia energía con su entorno. En procesos exotérmicos, el sistema libera energía a su entorno, mientras que un proceso endotérmico solamente puede ocurrir cuando el entorno aporta energía al sistema que reacciona. En la mayor parte de las reacciones químicas hay flujo de energía entre el sistema y su campo de influencia, por lo cual puede extenderse la definición de reacción química e involucrar la energía cinética (calor) como un reactivo o producto.

Aunque hay una gran variedad de ramas de la química, las principales divisiones son:


Si hay una partícula importante y representativa en la química, es el electrón. Uno de los mayores logros de la química es haber llegado al entendimiento de la relación entre reactividad química y distribución electrónica de átomos, moléculas o sólidos. Los químicos han tomado los principios de la mecánica cuántica y sus soluciones fundamentales para sistemas de pocos electrones y han hecho aproximaciones matemáticas para sistemas más complejos. La idea de orbital atómico y molecular es una forma sistemática en la cual la formación de enlaces es comprensible y es la sofisticación de los modelos iniciales de puntos de Lewis. La naturaleza cuántica del electrón hace que la formación de enlaces sea entendible físicamente y no se recurra a creencias como las que los químicos utilizaron antes de la aparición de la mecánica cuántica. Aun así, se obtuvo gran entendimiento a partir de la idea de puntos de Lewis.

Bajo la influencia de los nuevos métodos empíricos propuestos por "sir" Francis Bacon, Robert Boyle, Robert Hooke, John Mayow, entre otros, comenzaron a remodelarse las viejas tradiciones acientíficas en una disciplina científica. Boyle, en particular, es considerado como el padre fundador de la química debido a su trabajo más importante, «El Químico Escéptico» donde se hace la diferenciación entre las pretensiones subjetivas de la alquimia y los descubrimientos científicos empíricos de la nueva química. Él formuló la ley de Boyle, rechazó los «cuatro elementos» y propuso una alternativa mecánica de los átomos y las reacciones químicas las cuales podrían ser objeto de experimentación rigurosa, demostrándose o siendo rebatidas de manera científica.

La teoría del flogisto (una sustancia que, suponían, producía toda combustión) fue propuesta por el alemán Georg Ernst Stahl en el siglo XVIII y solo fue rebatida hacia finales de siglo por el químico francés Antoine Lavoisier, quien dilucidó el principio de conservación de la masa y desarrolló un nuevo sistema de nomenclatura química utilizada para el día de hoy.

Antes del trabajo de Lavoisier, sin embargo, se han hecho muchos descubrimientos importantes, particularmente en lo que se refiere a lo relacionado con la naturaleza del "aire", que se descubrió, que se compone de muchos gases diferentes. El químico escocés Joseph Black (el primer químico experimental) y el holandés J. B. van Helmont descubrieron dióxido de carbono, o lo que Black llamaba "aire fijo" en 1754; Henry Cavendish descubre el hidrógeno y dilucida sus propiedades. Finalmente, Joseph Priestley e, independientemente, Carl Wilhelm Scheele aíslan oxígeno puro.

El científico inglés John Dalton propone en 1803 la teoría moderna de los átomos en su libro, "La teoría atómica", donde postula que todas las sustancias están compuestas de "átomos" indivisibles de la materia y que los diferentes átomos tienen diferentes pesos atómicos.

El desarrollo de la teoría electroquímica de combinaciones químicas se produjo a principios del siglo XIX como el resultado del trabajo de dos científicos en particular, J. J. Berzelius y Humphry Davy, gracias a la invención, no hace mucho, de la pila voltaica por Alessandro Volta. Davy descubrió nueve elementos nuevos, incluyendo los metales alcalinos mediante la extracción de ellos a partir de sus óxidos con corriente eléctrica.

El británico William Prout propuso el ordenar a todos los elementos por su peso atómico, ya que todos los átomos tenían un peso que era un múltiplo exacto del peso atómico del hidrógeno. Newlands ideó una primitiva tabla de los elementos, que luego se convirtió en la tabla periódica moderna creada por el alemán Julius Lothar Meyer y el ruso Dmitri Mendeleev en 1860. Los gases inertes, más tarde llamados gases nobles, fueron descubiertos por William Ramsay en colaboración con lord Rayleigh al final del siglo, llenando por lo tanto la estructura básica de la tabla.

La química orgánica ha sido desarrollada por Justus von Liebig y otros luego de que Friedrich Wohler sintetizara urea, demostrando que los organismos vivos eran, en teoría, reducibles a terminología química Otros avances cruciales del siglo XIX fueron: la comprensión de los enlaces de valencia (Edward Frankland,1852) y la aplicación de la termodinámica a la química (J. W. Gibbs y Svante Arrhenius, 1870).

Llegado el siglo XX los fundamentos teóricos de la química fueron finalmente entendidos debido a una serie de descubrimientos que tuvieron éxito en comprobar la naturaleza de la estructura interna de los átomos. En 1897, J.J. Thomson, de la Universidad de Cambridge, descubrió el electrón y poco después el científico francés Becquerel, así como la pareja de Pierre y Marie Curie investigó el fenómeno de la radiactividad. En una serie de experimentos de dispersión, Ernest Rutherford, en la Universidad de Mánchester, descubrió la estructura interna del átomo y la existencia del protón, clasificando y explicando los diferentes tipos de radiactividad, y con éxito, transmuta el primer elemento mediante el bombardeo de nitrógeno con partículas alfa.

El trabajo de Rutherford en la estructura atómica fue mejorado por sus estudiantes, Niels Bohr y Henry Mosley. La teoría electrónica de los enlaces químicos y orbitales moleculares fue desarrollada por los científicos americanos Linus Pauling y Gilbert N. Lewis.

El año 2011 fue declarado por las Naciones Unidas como el Año Internacional de la Química. Esta iniciativa fue impulsada por la Unión Internacional de Química Pura y Aplicada, en conjunto con la Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura. Se celebró por medio de las distintas sociedades de químicos, académicos e instituciones de todo el mundo y se basó en iniciativas individuales para organizar actividades locales y regionales.

El actual modelo de la estructura atómica es el modelo mecánico cuántico. La química tradicional comenzó con el estudio de las partículas elementales: átomos, moléculas, sustancias, metales, cristales y otros agregados de la materia. La materia podía ser estudiada en estados líquido, de gas o sólidos, ya sea de manera aislada o en combinación. Las interacciones, reacciones y transformaciones que se estudian en química son generalmente el resultado de las interacciones entre átomos, dando lugar a direccionamientos de los enlaces químicos que los mantienen unidos a otros átomos. Tales comportamientos son estudiados en un laboratorio de química.

En el laboratorio de química se suelen utilizar diversos materiales de cristalería. Sin embargo, la cristalería no es fundamental en la experimentación química ya que gran cantidad de experimentación científica (así sea en química aplicada o industrial) se realiza sin ella.

Una reacción química es la transformación de algunas sustancias en una o más sustancias diferentes. La base de tal transformación química es la reordenación de los electrones en los enlaces químicos entre los átomos. Se puede representar simbólicamente como una ecuación química, que por lo general implica átomos como la partícula central. El número de átomos a la izquierda y la derecha en la ecuación para una transformación química debe ser igual (cuando es desigual, la transformación, por definición, no es química, sino más bien una reacción nuclear o la desintegración radiactiva). El tipo de reacciones químicas que una sustancia puede experimentar y los cambios de energía que pueden acompañarla, son determinados por ciertas reglas básicas, conocidas como leyes químicas.

Las consideraciones energéticas y de entropía son variables importantes en casi todos los estudios químicos. Las sustancias químicas se clasifican sobre la base de su estructura, estado y composiciones químicas. Estas pueden ser analizadas usando herramientas del análisis químico, como por ejemplo, la espectroscopia y cromatografía. Los científicos dedicados a la investigación química se les suele llamar «químicos». La mayoría de los químicos se especializan en una o más áreas o subdisciplinas. Varios conceptos son esenciales para el estudio de la química, y algunos de ellos son: 

En química, la materia se define como cualquier cosa que tenga masa en reposo, volumen y se componga de partículas. Las partículas que componen la materia también poseen masa en reposo, sin embargo, no todas las partículas tienen masa en reposo, un ejemplo es el fotón. La materia puede ser una sustancia química pura o una mezcla de sustancias.

El átomo es la unidad básica de la química. Se compone de un núcleo denso llamado núcleo atómico, el cual es rodeado por un espacio denominado «nube de electrones». El núcleo se compone de protones cargados positivamente y neutrones sin carga (ambos denominados nucleones). La nube de electrones son electrones que giran alrededor del núcleo cargados negativamente. 

En un átomo neutro, los electrones cargados negativamente equilibran la carga positiva de los protones. El núcleo es denso; La masa de un nucleón es 1836 veces mayor que la de un electrón, sin embargo, el radio de un átomo es aproximadamente 10000 veces mayor que el de su núcleo

El átomo es la entidad más pequeña que se debe considerar para conservar las propiedades químicas del elemento, tales como la electronegatividad, el potencial de ionización, los estados de oxidación preferidos, los números de coordinación y los tipos de enlaces que un átomo prefiere formar (metálicos, iónicos, covalentes, etc.).

Un elemento químico es una sustancia pura que se compone de un solo tipo de átomo, caracterizado por su número particular de protones en los núcleos de sus átomos, número conocido como «número atómico» y que es representado por el símbolo Z. El número másico es la suma del número de protones y neutrones en el núcleo. Aunque todos los núcleos de todos los átomos que pertenecen a un elemento tengan el mismo número atómico, no necesariamente deben tener el mismo número másico; átomos de un elemento que tienen diferentes números de masa se conocen como isótopos. Por ejemplo, todos los átomos con 6 protones en sus núcleos son átomos de carbono, pero los átomos de carbono pueden tener números másicos de 12 o 13.

La presentación estándar de los elementos químicos está en la tabla periódica, la cual ordena los elementos por número atómico. La tabla periódica se organiza en grupos (también llamados columnas) y períodos (o filas). La tabla periódica es útil para identificar tendencias periódicas.

Un compuesto químico es una sustancia química pura compuesta de más de un elemento. Las propiedades de un compuesto tienen poca similitud con las de sus elementos. La nomenclatura estándar de los compuestos es fijada por la Unión Internacional de Química Pura y Aplicada (IUPAC). Los compuestos orgánicos se nombran según el sistema de nomenclatura orgánica. Los compuestos inorgánicos se nombran según el sistema de nomenclatura inorgánica. Además, el Servicio de Resúmenes Químicos ha ideado un método para nombrar sustancias químicas. En este esquema cada sustancia química es identificable por un número conocido como número de registro CAS.

La química cubre un campo de estudios bastante amplio, por lo que en la práctica se estudia cada tema de manera particular. Las seis principales y más estudiadas ramas de la química son:


La diferencia entre la química orgánica y la química biológica es que en la química biológica las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es solo testigo de su presente, sin pasado y sin evolución histórica.

Además existen múltiples subdisciplinas que, por ser demasiado específicas o bien multidisciplinares, se estudian individualmente:


Hace aproximadamente 455 años solo se conocían doce elementos. A medida que fueron descubriendo más elementos, los científicos se dieron cuenta de que todos guardaban un orden preciso. Cuando los colocaron en una tabla ordenada en filas y columnas, vieron que los elementos de una misma columna tenían propiedades similares. Pero también aparecían espacios vacíos en la tabla para los elementos aún desconocidos. Estos espacios huecos llevaron al científico ruso Dmitri Mendeléyev a pronosticar la existencia del germanio, de número atómico 32, así como su color, su peso, su densidad y su punto de fusión. Su “predicción sobre otros elementos como —el galio y el escandio— también resultó muy atinada”, señala la obra Chemistry, libro de texto de química editado en 1995.

Muchos científicos han contribuido al crecimiento de la Química a través de importantes descubrimientos que los han hecho merecedores del Premio Nobel en Química. A modo de ejemplo, entre mucho de ellos, podemos citar a Emil Fischer que descubrió la síntesis de la glucosa y otros azúcares, Maria Curie por sus estudios en el campo de la radiactividad descubriendo el radio y el polonio. Theodor Svedberg, por el invento y la aplicación de la ultracentrífuga; Irene Curie, hija de Maria Curie, por construir el primer reactor nuclear que utilizaba la fisión nuclear controlada. Otto Hanh por su descubrimiento de la fisión nuclear, Linus Pauling por su estudio de la estructura atómica de las proteínas y la anemia falciforme causada por defecto genético en la producción de hemoglobina. Luis Federico Leloir por el descubrimiento de los procesos químicos que dan lugar a la formación de azúcares en las plantas, Paul Crutzen compartió el Nobel con Mario Molina y Sherwood Rowlands por el descubrimiento del papel de los óxidos de nitrógeno y de los fluorocarbonos en la destrucción de la capa de ozono, Roger David Kornberg por el descubrimiento del modo en que las células copian la información genética. Los últimos científicos que han obtenido el Premio Nobel de Química han sido Stanley Whittingham y Akira Yoshino por el desarrollo de las baterías de iones de litio.
Véase también: Actualidad en el uso pacífico de la radioactividad 

El origen de la teoría atómica se remonta a la escuela filosófica de los atomistas, en la Grecia antigua. Los fundamentos empíricos de la teoría atómica, de acuerdo con el método científico, se debe a un conjunto de trabajos hechos por Antoine Lavoisier, Louis Proust, Jeremias Benjamin Richter, John Dalton, Gay-Lussac, Berzelius y Amadeo Avogadro, hacia principios del siglo XIX. 

Los átomos son la fracción más pequeña de materia estudiados por la química, están constituidos por diferentes partículas, cargadas eléctricamente, los electrones, de carga negativa; los protones, de carga positiva; los neutrones, que, como su nombre indica, son neutros (sin carga); todos ellos aportan masa para contribuir al peso.

Los tipos de átomos que forman las células son relativamente pocos:

Cada átomo tiene en su parte central un núcleo denso con carga positiva rodeado a cierta distancia por una nube de electrones con carga negativa que se mantienen en órbita alrededor del núcleo por atracción electrostática. El núcleo está formado por dos tipos de partículas subatómicas: los protones, que tienen carga positiva, y los neutrones, que son eléctricamente neutros. El número de protones presentes en el núcleo del átomo determina su número atómico. Un átomo de hidrógeno tiene un solo protón en el núcleo; por consiguiente el hidrógeno, cuyo número atómico es 1, es el elemento más liviano. La carga eléctrica de un protón es exactamente igual y opuesta a la carga de un electrón. El átomo es eléctricamente neutro; el número de electrones con carga negativa que se encuentra alrededor del núcleo es igual al número de protones con carga positiva que se encuentran dentro del núcleo; por ende, el número de electrones de un átomo también es igual al número atómico. Todos los átomos de un elemento tienen el mismo número atómico.

Los átomos son las partes más pequeñas de un elemento (como el carbono, el hierro o el oxígeno). Todos los átomos de un mismo elemento tienen la misma estructura electrónica (responsable ésta de la mayor parte de las características químicas), y pueden diferir en la cantidad de neutrones (isótopos). Las moléculas son las partes más pequeñas de una sustancia (como el azúcar), y se componen de átomos enlazados entre sí. Si tienen carga eléctrica, tanto átomos como moléculas se llaman iones: cationes si son positivos, aniones si son negativos.

El mol se usa como contador de unidades, como la docena (12) o el millar (1000), y equivale a formula_1. Se dice que 12 gramos de carbono o un gramo de hidrógeno o 56 gramos de hierro contienen aproximadamente un mol de átomos (la masa molar de un elemento está basada en la masa de un mol de dicho elemento). Se dice entonces que el mol es una unidad de cambio. El mol tiene relación directa con el número de Avogadro. El número de Avogadro fue estimado para el átomo de carbono por el químico y físico italiano Carlo Amedeo Avogadro, conde de Quarequa e di Cerreto. Este valor, expuesto anteriormente, equivale al número de partículas presentes en 1 mol de dicha sustancia:

1 mol de glucosa equivale a formula_1 moléculas de glucosa.
1 mol de uranio equivale a formula_1 átomos de uranio.

Dentro de los átomos puede existir un núcleo atómico y uno o más electrones. Los electrones son muy importantes para las propiedades y las reacciones químicas. Dentro del núcleo se encuentran los neutrones y los protones. Los electrones se encuentran alrededor del núcleo.
También se dice que el átomo es la unidad básica de la materia con características propias. Está formado por un núcleo, donde se encuentran los protones.

Los enlaces son las uniones entre átomos para formar moléculas. Siempre que existe una molécula es porque ésta es más estable que los átomos que la forman por separado. A la diferencia de energía entre estos dos estados se le denomina energía de enlace.

Los átomos se combinan en proporciones fijas para generar moléculas concretas. Por ejemplo, dos átomos de hidrógeno se combinan con uno de oxígeno para dar una molécula de agua. Esta proporción fija se conoce como estequiometría. Sin embargo, el mismo número y tipo de átomos puede combinarse de diferente forma dando lugar a sustancias isómeras.

Para una descripción y comprensión detalladas de las reacciones químicas y de las propiedades físicas de las diferentes sustancias, es muy útil su descripción a través de orbitales, con ayuda de la química cuántica.

Un orbital atómico es una función matemática que describe la disposición de uno o dos electrones en un átomo. Un orbital molecular es el análogo en las moléculas.

En la teoría del orbital molecular la formación del enlace covalente se debe a una combinación matemática de orbitales atómicos (funciones de onda) que forman orbitales moleculares, llamados así porque pertenecen a toda la molécula y no a un átomo individual. Así como un orbital atómico (sea híbrido o no) describe una región del espacio que rodea a un átomo donde es probable que se encuentre un electrón, un orbital molecular describe también una región del espacio en una molécula donde es más factible que se hallen los electrones.

Al igual que un orbital atómico, un orbital molecular tiene un tamaño, una forma y una energía específicos. Por ejemplo, en la molécula de hidrógeno molecular se combinan dos orbitales atómicos, ocupado cada uno por un electrón. Hay dos formas en que puede presentarse la combinación de orbitales: aditiva y substractiva. La combinación aditiva produce la formación de un orbital molecular que tiene menor energía y que presenta una forma casi ovalada, mientras que la combinación substractiva conduce a la formación de un orbital molecular con mayor energía y que genera un nodo entre los núcleos.

Los orbitales son funciones matemáticas para describir procesos físicos: un orbital únicamente existe en el sentido matemático, como pueden existir una suma, una parábola o una raíz cuadrada. Los átomos y las moléculas son también idealizaciones y simplificaciones: un átomo y una molécula solo existen en el vacío, y en sentido estricto una molécula solo se descompone en átomos si se rompen todos sus enlaces.

En el "mundo real" únicamente existen los materiales y las sustancias. Si se confunden los objetos reales con los modelos teóricos que se usan para describirlos, es fácil caer en falacias lógicas.

En agua, y en otros disolventes (como la acetona o el alcohol), es posible disolver sustancias, de forma que quedan disgregadas en las moléculas o en los iones que las componen (las disoluciones son transparentes). Cuando se supera cierto límite, llamado solubilidad, la sustancia ya no se disuelve, y queda, bien como precipitado en el fondo del recipiente, bien como suspensión, flotando en pequeñas partículas (las suspensiones son opacas o traslúcidas).

Se denomina concentración a la medida de la cantidad de soluto por unidad de cantidad de disolvente.

La concentración de una disolución se puede expresar de diferentes formas, en función de la unidad empleada para determinar las cantidades de soluto y disolvente. Las más usuales son:


El pH es una escala logarítmica para describir la acidez de una disolución acuosa. Los ácidos, como por ejemplo el zumo de limón y el vinagre, tienen un pH bajo (inferior a 7). Las bases, como la sosa o el bicarbonato de sodio, tienen un pH alto (superior a 7).

El pH se calcula mediante la siguiente ecuación:

donde formula_5 es la actividad de iones hidrógeno en la solución, la que en soluciones diluidas es numéricamente igual a la molaridad de iones hidrógeno formula_6 que cede el ácido a la solución.


La IUPAC, un organismo internacional, mantiene unas reglas para la formulación y nomenclatura química. De esta forma, es posible referirse a los compuestos químicos de forma sistemática y sin equívocos.

Mediante el uso de fórmulas químicas es posible también expresar de forma sistemática las reacciones químicas, en forma de ecuación química.

Por ejemplo:






</doc>
<doc id="3356" url="https://es.wikipedia.org/wiki?curid=3356" title="Louis Braille">
Louis Braille

Louis Braille (; Coupvray, Sena y Marne, Francia, 4 de enero de 1809-París, Francia, 6 de enero de 1852) fue un sacerdote católico y un pedagogo francés que diseñó un sistema de lectura y escritura para personas con discapacidad visual. Su sistema es conocido internacionalmente como sistema Braille y es usado tanto en la escritura como en la lectura y la notación musical.

Pese a su prematura discapacidad, Louis Braille destacó en sus estudios y recibió una beca para el . Todavía siendo un estudiante allí e inspirado por la criptografía militar de Charles Barbier, elaboró un código táctil específicamente diseñado para facilitar la lectura y la escritura de los alumnos con discapacidad visual de una forma mucho más rápida y eficaz en comparación con los métodos existentes en aquel momento.

A finales del siglo XVIII Francia vivió profundos cambios políticos, sociales y culturales. Durante la Revolución Francesa de 1789 empezaron a producirse una serie de transformaciones que a su vez iban a ser decisivas para las personas con discapacidad. El régimen antiguo empezó a estremecerse y a resquebrajarse de tal manera que se sucedieron nuevos cambios y aparecieron unas nuevas condiciones que resultaron favorables para que grupos que hasta entonces habían estado marginados de la sociedad, tuvieran acceso a la educación y a los derechos básicos de todos los ciudadanos. Hasta ese momento, la única atención que se había dado a las personas con discapacidad visual eran hospicios creados especialmente para ellos. A pesar de que a lo largo de la historia hubiera casos de personas con esta discapacidad que destacaran en el campo artístico, científico o incluso político, la mayoría fueron casos aislados de los que poco se conoce en la actualidad.

Valentin Haüy, un personaje erudito en el mundo de las letras que poseía cargos importantes en el ayuntamiento de París, en 1786 se interesó mucho por tratar de mejorar la situación de estas personas motivado a partir de una experiencia que él mismo describió. Haüy observó la penosa situación de un grupo de ciegos que, acogidos en el asilo Quinze-Vingt (fundado en 1269 por Luis IX), tocaban música en la calle para ganarse, entre burlas y desprecios, alguna que otra limosna:

Haüy dedicó gran parte de su vida a la educación de estas personas. El encuentro en 1784 con la compositora y pianista Maria Theresia von Paradis, ciega desde los dos años de edad y que había aprendido por sí misma a leer textos y música palpando unos alfileres clavados en almohadones, reforzó en gran parte esta gran labor.

Haüy fundó en 1786 el Instituto de los Niños Ciegos, una de las primeras escuelas dedicadas a la educación de personas ciegas. Asimismo, empezó a diseñar un método de escritura en relieve que facilitaba el acceso a la lectura y escritura mediante la percepción táctil. Durante la Revolución Francesa, Haüy fue destituido como director de su Instituto y éste pasó a manos del Estado y se llamó Instituto de los Trabajadores Ciegos hasta que finalmente pasó a ser la sede del Instituto Nacional de Jóvenes Ciegos.

Braille nació en Coupvray, un pequeño pueblo situado a unos 30 kilómetros al este de París. Él y sus tres hermanos mayores –Monique Catherine Josephine Braille (n. 1793), Louis-Simon Braille (n. 1795), y Marie Céline Braille (n. 1797)– vivían con su madre, Monique, y su padre, Simon-René, en tres hectáreas de tierra y unos viñedos en el campo. La familia Braille fue una familia humilde, tradicionalmente dedicada a la talabartería. Louis fue el hijo menor de una familia formada por padres ya mayores y hermanos también mayores. Todo ello determinó un marco familiar muy especial, sobre todo al tratarse de un niño que perdió la vista a muy temprana edad. Es posible, pues, que ese carácter afable, cálido, persistente, atento y observador se debiera en gran parte a ese marco familiar que siempre estuvo tan presente en los primeros años de su infancia. A pesar de ser una familia con poca formación cultural y de escasos recursos demostraron tener una gran tenacidad y destreza para que Louis se desarrollara tal y como lo hacían los otros niños a su edad. No deja de ser notable el hecho de que, dadas las circunstancias, no tomara una actitud sobre-protectora ante su hijo debido a la discapacidad.

Simón-René, el padre de Louis, le enseñó a leer mediante tachas de tapicero con las que formaba las letras sobre una madera o sobre un trozo de cuero. Louis recorría esas marcas con sus dedos hasta aprender letras y palabras enteras. En 1818 los Braille enviaron a su hijo a la escuela de la villa con la misma naturalidad que lo hicieron con sus otros tres hijos. A pesar de que inicialmente su aprendizaje fuera mediante transmisión oral, el maestro de la escuela Antonie Bécheret se sorprendió al observar que Louis pudiera poseer una actitud tan predispuesta hacia el aprendizaje. En sus primeros años de estudiante logró una beca para ingresar en el Instituto Nacional para Jóvenes Ciegos de París que le permitió emprender sus estudios, puesto que su familia no tenía recursos para hacerse cargo de los gastos. A partir de entonces, empezaría un largo camino tanto de alumno como de profesor en aquel instituto.

En el año 1812, Louis contaba con tan solo tres años de edad, y mientras jugaba en el taller de su padre tratando de imitarle, cogió un tranchete que este utilizaba para su trabajo y trató de cortar una correa con tal mala suerte que un pequeño accidente del cual no se tiene conocimiento exacto (un pedacito de cuero que le pudo saltar al ojo o bien la punta de la herramienta) le hirió el ojo derecho. La inflamación acabó por dañar también el ojo izquierdo, provocando una ceguera irreversible debida a una oftalmía simpática. Si dicha inflamación no es tratada a tiempo, la reacción autoinmune que se provoca en el ojo dañado acaba afectando al contralateral pudiendo causar una ceguera irreversible en ambos ojos. Teniendo presente el contexto y también la situación económica, era prácticamente imposible que no acabara afectando a ambos ojos y por eso Louis quedó con esta discapacidad a los cinco años de edad. 

Hay que tener presente que cuando la ceguera se produce antes de los cinco o seis años de edad, el niño no conserva prácticamente ninguna imagen visual clara, ni siquiera el recuerdo del rostro de sus familiares o el lugar en donde transcurrió su infancia. Además, el propio rostro pierde parte de su movilidad expresiva que surge como un efecto natural de la imitación espontánea de los niños en edades tempranas. A consecuencia de esto, existen algunas descripciones que todavía se conservan en las cuales algunos profesores de la Institución de Ciegos de París lo describen como una persona poco expresiva. Evidentemente, bajo esta apariencia exterior debido a la precocidad de su ceguera, existía una persona con unas grandes cualidades que poco a poco se irían descubriendo durante su estancia en el Instituto.

Inicialmente, la Institución Real de Jóvenes Ciegos de París estaba constituida por distintos edificios que en su mayor parte estaban viejos y realmente poco acondicionados para recibir estudiantes. En esos edificios un centenar de jóvenes estudiantes con discapacidad visual, además del personal de servicio, tenían que vivir y trabajar en una casa en la cual había una capilla, una biblioteca, una imprenta, unas aulas para las clases de instrumento y un salón para los ejercicios públicos; además de las habitaciones de los alumnos que ahí residían como internados. El comedor de los alumnos era una galería con una escalera a cada extremo y el taller principal (el telar) era un patio cubierto, con lo cual se privaba de luz a los pisos bajos contiguos. Los talleres restantes se separaban con una simple balaustrada y las habitaciones daban unas con otras. También había un cuarto de baño que dejaba entrever unas pésimas condiciones higiénicas, además que solían bañarse solamente una vez al mes. De hecho, los propios informes de los médicos de aquella época nos ilustran una realidad de apariencia lejana pero cierta. En una de las intervenciones de Pierre Henri menciona uno de los informes realizados por un médico el 14 de mayo de 1838, y dice así: «Ayer fui a visitar el establecimiento de Jóvenes Ciegos y puedo aseguraros que no hay la menor exageración en la descripción de aquel lugar [...] ya que, ciertamente, no hay descripción que pueda daros idea de aquel local estrecho, infecto y tenebroso; de aquellos pasillos partidos en dos para hacer verdaderos cuchitriles que allí llaman talleres o clases; de aquellas innumerables escaleras tortuosas y carcomidas que, lejos de estar preparadas para desgraciados que sólo pueden guiarse por el tacto, parecen un reto lanzado a la ceguera de aquellos niños [...].»

Y otras consideraciones de los propios alumnos que llegan a completar este testimonio:

En resumen, son éstas las condiciones en las que vivieron el joven estudiante y sus compañeros durante más de 15 años. La instalación de la escuela, sin embargo, se trasladó en el año 1843 a un nuevo edificio de París y las condiciones mejoraron, pero posiblemente el estado de salud de Louis Braille y la enfermedad de tuberculosis que le acompañó desde temprana edad ya se habían originado en las viejas instalaciones.

Braille empezó a destacar primero como alumno y después como maestro ideando asimismo su sistema de lectura y escritura conocido actualmente como sistema braille. Inicialmente los estudiantes del Instituto aprendían a leer y a escribir mediante el sistema de Valentin Haüy que consistía en predisponer las letras en relieve, a pesar de que a la práctica fueran poco agradecidas para el tacto. A Louis, sin embargo, este sistema le permitió leer muchos de los libros impresos que estaban en la biblioteca del Instituto. También aprendió a escribir a lápiz con la intención de poderse comunicar con los videntes. Para ello, empleaba moldes que contenían las letras vacías por cuyos bordes había que deslizar el lápiz. Aprendió de ese mismo modo matemática y geografía. En el caso de la notación musical, durante muchos años se prescindió del relieve para incorporar la enseñanza musical mediante la transmisión oral y su memorización.

Desde su ingreso demostró su capacidad para desarrollarse en distintas áreas como: gramática, retórica, historia, geometría, álgebra y sobre todo música, tanto en teoría como en práctica (aprendió a tocar el órgano, violonchelo y el piano). En el instituto, enseñó más de una materia y realizó algunos manuales de historia y aritmética para sus alumnos. De hecho, no solamente enseñó a ciegos sino también a niños videntes puesto que en ese momento el instituto admitía un determinado número de videntes a los que se les enseñaba gratuitamente a cambio de una cierta cooperación que se prestaba a los jóvenes ciegos, como por ejemplo ayudarlos a leer, a redactar o guiarlos al andar.

Braille poseía una gran capacidad reflexiva y metódica, era cercano con sus alumnos y conseguía despertar su interés, comprenderlos y aconsejarlos en los momentos más difíciles. Posiblemente esta capacidad de síntesis se deriva de los complicados procedimientos de escritura e impresión, puesto que tal y como él mismo decía «hemos de procurar expresar el pensamiento con el menor número posible de palabras»."

A partir de 1835 y debido a los primeros síntomas de tuberculosis se fue retirando progresivamente de sus enseñanzas hasta quedar encargado únicamente de las clases de música. De hecho, la profesión mencionada que aparece en su testamento es la de "profesor de música". En el año 1840 recibió clases de los mejores maestros: Mme. Van der Burch en el piano; Bénazet para el violonchelo y Mangues para el órgano. Braille fue organista durante muchos años en la iglesia de San Nicolás de los Campos de París. En el órgano, dice Cotalt, «su ejecución era exacta, brillante y desenvuelta, y presentaba bastante bien el aire de toda su persona».

Louis Braille murió a la edad de 43 años de tuberculosis, enfermedad que le había acompañado durante mucho tiempo y que probablemente se iniciara debido a las pésimas condiciones higiénicas existentes en el primer Instituto de Jóvenes Ciegos. El funeral se celebró en la capilla de la Institución Nacional y su cuerpo fue trasladado a su pueblo natal para ser enterrado en el pequeño cementerio de Coupray, al lado de su padre y su hermana que habían muerto años antes. Su ataúd se depositó allí el 10 de enero de 1852. En 1952 sus restos fueron trasladados al Panteón de París. Solo sus manos permanecieron enterrados en Coupray como un símbolo al sistema de lectura táctil que él inventó.

En 1825 Luis Braille ideó su sistema de puntos en relieve que aportaba a las personas ciegas una herramienta válida y eficaz para leer, escribir y facilitar el acceso a la educación, la cultura y la información.

Durante los primeros años del siglo XIX existía una gran preocupación por encontrar un sistema de lectura que se adecuara a las necesidades de las personas con discapacidad visuales. De hecho, años antes el sacerdote italiano Francesco Lana de Terzi en su libro Prodromo introdujo un alfabeto nuevo de invención propia para la gente ciega, basado en signos (guiones) que podían ser reconocidos por el tacto.

También Haüy ya había tratado de solucionar este problema reproduciendo las letras en altorrelieve, no obstante eso suponía una lenta y complicada tarea. En abril de 1821 Braille conoció el sistema de Barbier. Su creador, Charles Barbier, siempre había mostrado un especial interés y una gran dedicación al estudio y la experimentación de los sistemas de lectura y uno de sus objetivos primordiales era mejorar las comunicaciones del ejército Francés durante aquellos años. De esa manera generó un código cifrado que llamó «escritura nocturna» y que serviría para que los oficiales en campaña pudiesen redactar mensajes encriptados en la oscuridad y además poder descifrarlos con los dedos. 
Dicha escritura presenta una serie de virtudes que posteriormente serían tomadas y desarrolladas en el sistema de Louis Braille. La primera es el empleo del punto como el elemento clave para generar el código de lectura táctil (a diferencia del altorrelieve hasta ahora empleado) y la segunda, es el hecho de que no emplea la letra común sino que genera otras representaciones. A pesar de estas ventajas el sistema también tenía ciertos inconvenientes: en el sistema de Barbier no se representa el alfabeto sino grupos de sonidos de la lengua francesa. Además, la base constaba de un elevado número de puntos que dificultaba una rápida lectura mediante el tacto. La sonografía de Barbier se empleó en el instituto durante escasos años hasta quedar prácticamente desplazada por el sistema de Braille, a pesar de que inicialmente este último no fuera aceptado en el Instituto.

Posiblemente durante el año 1825, los alumnos más avanzados del Instituto que emplearon entusiasmados el sistema ideado por Barbier, comenzaron a reflexionar y discutir acerca de posibles mejorías de ese nuevo sistema. Es probable, pues, que entre todos tratasen de perfeccionar la sonografía dado el indiscutible interés que tenían. Este interés no consistía solamente en aprender un sistema que les permitiera agilizar su capacidad de lectura y escritura, sino también consistía en mostrar lo que eran capaces de realizar a una sociedad con tantísimos prejuicios frente a la comunidad invidente. Estos alumnos debían tener su alfabeto y posiblemente Braille fue el joven estudiante que acabó por encontrar la fórmula que le permitió perfeccionar ingeniosamente el sistema ideado por Charles Barbier. A diferencia de Barbier, fue posiblemente el hecho de que Braille fuera ciego el que dotaría a éste de una mayor «intuición psicológica» y trató de generar un signo hecho de puntos que pudiese formar una imagen bajo el dedo, convirtiendo en sintética la lectura táctil.

Antes de los 30 años, Louis había ideado un sistema que se adecuaba perfectamente a las características de la percepción táctil a nivel psicológico, estructural y fisiológico. El signo braille, compuesto por un máximo de seis puntos, se adapta perfectamente a la yema del dedo y esto produce que la persona lo pueda aprender en su totalidad, transmitido como una imagen al cerebro. Este sistema consta de 63 caracteres formados de uno a seis puntos y que al ser impresos en relieve en papel permiten la lectura mediante el tacto. Así mismo, los caracteres que integran el sistema, que Braille publicó en 1829 y 1837, están adaptados a la notación musical, facilitando así su comprensión. 

Inicialmente el sistema encontró una fuerte oposición e incluso se llegó a prohibirlo durante muchos años en aquel Instituto. Muchos maestros consideraron que dicho sistema, al ser distinto al empleado por los videntes, generaba aislamiento y segregación de cara al alumnado discapacitado. Esta argumentación no deja de parecer en muchas ocasiones una excusa para justificar que personas videntes (sobre todo profesores del Instituto) no emplearan su tiempo en aprender un código totalmente distinto de la escritura convencional. De hecho, fueron las personas ciegas las que defendieron e impulsaron el sistema, sin lugar a dudas los más indicados para decidir sobre esta cuestión.

Hay muchas anécdotas que han sobrevivido a aquellos años en los cuales se prohibió el sistema. Por ejemplo muchos alumnos y algunos profesores ciegos del Instituto lo emplearon de forma clandestina escribiendo cartas y copiando textos que luego serían mostrados a los demás y así sucesivamente. De ese modo en 1844, gracias a la presión ejercida por parte de esos grupos y coincidiendo con la inauguración del nuevo edificio del Instituto en el Boulevard des Invalides de París, el director reivindicó el sistema realizando un homenaje a su inventor. En 1853, un año después de la muerte de Braille, el sistema fue aceptado oficialmente por las Instituciones y por tanto su autor nunca llegó a ser reconocido oficialmente mientras vivía.

El sistema Braille, originado en Francia, utilizó muchos símbolos correspondientes a las 64 combinaciones de los seis puntos para representar acentos especiales correspondientes al francés. Al emplearse en otros idiomas, las combinaciones de puntos braille cambian de significado. Por ejemplo, el punto final y el signo de mayúscula cambian del español al inglés. Asimismo, Braille y su amigo Pierre Foucault llegaron a desarrollar una máquina de escribir para que fuera aún más fácil la producción de textos en Braille: el ratígrafo.

La música tuvo un especial lugar en la vida de Louis Braille. Durante toda su vida se dedicó a dar clases de música y también fue un instrumentista notable. Asimismo, ideó un sistema de notación musical en braille (Signografía Musical Braille, actualmente conocido como musicografía) para personas con discapacidad visual y estudió también las posibles formas de comunicación entre la escritura musical en tinta y en relieve con la intención de que pudieran ser empleados e intercambiados de forma recíproca entre personas videntes y discapacitadas. Debemos tener presente que durante la década del siglo XVIII e incluso a principios del XIX era habitual identificar la ceguera con la mendicidad, cosa que provocó que durante muchos años se eliminara la enseñanza musical de los programas escolares para evitar su parentesco.

En 1952, un siglo después de su muerte, sus restos fueron trasladados a la capital francesa y enterrados en el Panteón de París.

Bélgica e Italia emitieron sendas monedas conmemorativas de 2 euros en 2009 para celebrar el 200º aniversario de su nacimiento.



</doc>
<doc id="3359" url="https://es.wikipedia.org/wiki?curid=3359" title="Energía eólica">
Energía eólica

La energía eólica es aquella energía que se obtiene del viento o, dicho de otro modo, es el aprovechamiento de la energía cinética de las masas de aires.
El término «eólico» proviene del latín "aeolicus", o «perteneciente o relativo a Eolo», dios de los vientos en la mitología griega.

En la actualidad, la energía eólica se utiliza principalmente para producir electricidad, lo que se consigue mediante aerogeneradores conectados a las grandes redes de distribución de energía eléctrica, entre otras. Los parques eólicos construidos en tierra suponen una fuente de energía cada vez más barata y competitiva, esa incluso más barata en muchas regiones que otras fuentes de energía convencionales. Además se puede proporcionar electricidad en regiones aisladas que no tienen acceso a la red eléctrica mediante instalaciones eólicas de reducido tamaño, o también con energía solar fotovoltaica. Las compañías eléctricas distribuidoras adquieren cada vez en mayor medida el excedente de electricidad producido por pequeñas instalaciones eólicas domésticas. El auge de la energía eólica ha provocado también la planificación y construcción de parques eólicos marinos —a menudo conocidos como parques eólicos "offshore" por su nombre en inglés—, situados cerca de las costas. La energía del viento es más estable y fuerte en el mar que en tierra, y los parques eólicos marinos tienen un impacto visual menor, aunque los costos de construcción y mantenimiento son considerablemente mayores.

A finales de 2014, la capacidad mundial instalada de energía eólica ascendía a 370 GW, generando alrededor del 5 % del consumo de electricidad mundial. Dinamarca genera más de un 25 % de su electricidad mediante energía eólica, y más de 80 países en todo el mundo la utilizan de forma creciente para proporcionar energía eléctrica en sus redes de distribución, aumentando su capacidad anualmente con tasas por encima del 20 %. En España la energía eólica produjo un 20,3 % del consumo eléctrico de la península en 2014, convirtiéndose en la segunda tecnología con mayor contribución a la cobertura de la demanda, muy cerca de la energía nuclear con un 22,0 %.

La energía eólica es un recurso abundante, renovable y limpio que ayuda a disminuir las emisiones de gases de efecto invernadero al reemplazar fuentes de energía a base de combustibles fósiles. El impacto ambiental de este tipo de energía es además, generalmente, menos problemático que el de otras fuentes de energía.

La energía del viento es bastante estable y predecible a escala anual, aunque presenta variaciones significativas a escalas de tiempo menores. Al incrementarse la proporción de energía eólica producida en una determinada región o país, se hace imprescindible establecer una serie de mejoras en la red eléctrica local. Diversas técnicas de control energético, como una mayor capacidad de almacenamiento de energía, una distribución geográfica amplia de los aerogeneradores, la disponibilidad de fuentes de energía de respaldo, la posibilidad de exportar o importar energía a regiones vecinas o la reducción de la demanda cuando la producción eólica es menor, pueden ayudar a mitigar en gran medida estos problemas. Además, son de extrema importancia las previsiones de producción eólica que permiten a los gestores de la red eléctrica estar preparados y anticiparse frente a las previsibles variaciones en la producción eólica que puedan tener lugar a corto plazo.

La energía del viento está relacionada con el movimiento de las masas de aire que se desplazan desde zonas de alta presión atmosférica hacia zonas adyacentes de menor presión, con velocidades proporcionales al gradiente de presión y así poder generar energía.

Los vientos se generan a causa del calentamiento no uniforme de la superficie terrestre debido a la radiación solar; entre el 1 y el 2% de la energía proveniente del Sol se convierte en viento. Durante el día, los continentes transfieren una mayor cantidad de energía solar al aire que las masas de agua, haciendo que este se caliente y se expanda, por lo que se vuelve menos denso y se eleva. El aire más frío y pesado que proviene de los mares, océanos y grandes lagos se pone en movimiento para ocupar el lugar dejado por el aire caliente.
Para poder aprovechar la energía eólica es importante conocer las variaciones diurnas, nocturnas y estacionales de los vientos, la variación de la velocidad del viento con la altura sobre el suelo, la entidad de las ráfagas en espacios de tiempo breves, y los valores máximos ocurridos en series históricas de datos con una duración mínima de 20 años. Para poder utilizar la energía del viento, es necesario que este alcance una velocidad mínima que depende del Aerogenerador que se vaya a utilizar pero que suele empezar entre los 3 m/s (10 km/h) y los 4 m/s (14,4 km/h), velocidad llamada ""cut-in speed"", y que no supere los 25 m/s (90 km/h), velocidad llamada "cut-out speed".

La energía del viento se aprovecha mediante el uso de máquinas eólicas o aeromotores capaces de transformar la energía eólica en energía mecánica de rotación utilizable, ya sea para accionar directamente las máquinas operatrices o para la producción de energía eléctrica. En este último caso, el más ampliamente utilizado en la actualidad, el sistema de conversión —que comprende un generador eléctrico con sus sistemas de control y de conexión a la red— es conocido como aerogenerador. En estos la energía eólica mueve una hélice y mediante un sistema mecánico se hace girar el rotor de un generador, normalmente un alternador, que produce energía eléctrica. Para que su instalación resulte rentable, suelen agruparse en concentraciones denominadas parques eólicos.

Una turbina eólica es una máquina que transforma la energía del viento en energía mecánica mediante unas aspas oblicuas unidas a un eje común. El eje giratorio puede conectarse a varios tipos de maquinaria, sea para moler grano (molinos), bombear agua o generar electricidad. Cuando se usa para producir electricidad se le denomina generador de turbina de viento. Las máquinas movidas por el viento tienen un origen remoto, siendo las más antiguas las que funcionaban como molinos.
La energía eólica es la energía que se obtiene del viento o, dicho de otro modo, es el aprovechamiento de la energía cinética de las masas de aire que puede convertirse en energía mecánica y a partir de ella en electricidad u otras formas útiles de Energía en cotidianas actividades humanas.

La energía eólica no es algo nuevo, es una de las energías más antiguas junto a la energía térmica. El viento como fuerza motriz se ha utilizado desde la antigüedad. Así, ha movido a barcos mediante el uso de velas o ha hecho funcionar la maquinaria de los molinos al mover sus aspas. Sin embargo, tras una época en la que se fue abandonando, a partir de los años ochenta del siglo este tipo de energía limpia experimentó un renacimiento.

La energía eólica crece de forma imparable ya en el siglo , en algunos países más que en otros, pero sin duda alguna en España existe un gran crecimiento, siendo uno de los primeros países, por debajo de Alemania a nivel europeo o de Estados Unidos a escala mundial. El auge del aumento de parques eólicos se debe a las condiciones favorables de viento, sobre todo en Andalucía que ocupa un puesto principal, entre los que se puede destacar el golfo de Cádiz, ya que el recurso de viento es excepcional.

La referencia más antigua que se tiene es un molino de viento que fue usado para hacer funcionar un órgano en el siglo . Los primeros molinos de uso práctico fueron construidos en Sistán, Afganistán, en el siglo . Estos eran molinos de eje vertical con hojas rectangulares. Se usaron artefactos para moler trigo o extraer agua hechos con 6 a 8 aspas de molino cubiertas con telas.

Los primeros molinos aparecieron en Europa en el siglo en Francia e Inglaterra y fueron extendiéndose por el continente. Eran unas estructuras de madera, conocidas como torres de molino, que se hacían girar a mano alrededor de un poste central para extender sus aspas al viento. El molino de torre se desarrolló en Francia a lo largo del siglo . Consistía en una torre de piedra coronada por una estructura rotativa de madera que soportaba el eje del molino y la maquinaria superior del mismo.

Estos primeros ejemplares tenían una serie de características comunes. De la parte superior del molino sobresalía un eje horizontal. De este eje partían de cuatro a ocho aspas, con una longitud entre 3 y 9 metros. Las vigas de madera se cubrían con telas o planchas de madera. La energía generada por el giro del eje se transmitía, mediante un sistema de engranajes, a la maquinaria del molino emplazada en la base de la estructura.

Los molinos de eje horizontal fueron usados extensamente en Europa Occidental para moler trigo desde la década de 1180 en adelante. Basta recordar los famosos molinos de viento en las andanzas de Don Quijote. Todavía existen máquinas de este tipo, por ejemplo, en Países Bajos para sacar agua.

En Estados Unidos, el desarrollo de bombas eólicas, reconocibles por sus múltiples aspas metálicas, fue el factor principal que permitió la agricultura y la ganadería en vastas áreas de Norteamérica, de otra manera imposible sin acceso fácil al agua. Estas bombas contribuyeron a la expansión del ferrocarril alrededor del mundo, cubriendo las necesidades de agua de las locomotoras a vapor.

Las turbinas eólicas modernas fueron desarrolladas a comienzos de la década de 1980, si bien continúan evolucionando los diseños.

La industria de la energía eólica en tiempos modernos comenzó en 1979 con la producción en serie de turbinas de viento por los fabricantes Kuriant, Vestas, Nordtank, Nily Baltazar y Bonus. Aquellas turbinas eran pequeñas para los estándares actuales, con capacidades de 20 a 30 kW cada una. Desde entonces, la talla de las turbinas ha crecido enormemente, y la producción se ha expandido a muchos sitios.

La energía eólica alcanzó la paridad de red (el punto en el que el costo de esta energía es igual o inferior al de otras fuentes de energía tradicionales) en algunas áreas de Europa y de Estados Unidos a mediados de la década de 2000. La caída de los costos continúa impulsando a la baja el costo normalizado de esta fuente de energía renovable: se estima que alcanzó la paridad de red de forma general en todo el continente europeo en torno al año 2010, y que alcanzará el mismo punto en todo Estados Unidos en 2016, debido a una reducción adicional de sus costos del 12 %.

La instalación de energía eólica requiere de una considerable inversión inicial, pero posteriormente no presenta gastos de combustible. El precio de la energía eólica es por ello mucho más estable que los precios de otras fuentes de energía fósil, mucho más volátiles. El costo marginal de la energía eólica, una vez que la planta ha sido construida y está en marcha, es generalmente inferior a 1 céntimo de dólar por kWh. Incluso, este costo se ha visto reducido con la mejora tecnológica de las turbinas más recientes. Existen en el mercado palas para aerogeneradores cada vez más largas y ligeras, a la vez que se realizan constantemente mejoras en el funcionamiento de la maquinaria de los propios aerogeneradores, incrementando la eficiencia de los mismos. Igualmente, se han reducido los costos de inversión inicial y de mantenimiento de los parques eólicos.

En 2004, el costo de la energía eólica se había reducido a una quinta parte del que tenía en los años 1980, y los expertos consideran que la tendencia a la baja continuará en el futuro próximo, con la introducción en el mercado de nuevos aerogeneradores "multi-megavatio" cada vez más grandes y producidos en masa, capaces de producir hasta 8 megavatios de potencia por cada unidad. En 2012, los costos de capital de la energía eólica eran sustancialmente inferiores a los de 2008-2010, aunque todavía estaban por encima de los niveles de 2002, cuando alcanzaron un mínimo histórico. La bajada del resto de costos ha contribuido a alcanzar precios cada vez más competitivos. Un informe de 2011 de la Asociación Americana de la Energía Eólica ("American Wind Energy Association") afirmaba:

Otro informe de la Asociación Británica de la Energía Eólica estima un costo de generación medio para la eólica terrestre de 5-6 céntimos de dólar por kWh (2005). El costo por unidad de energía producida se estimaba en 2006 como comparable al costo de la energía producida en nuevas plantas de generación en Estados Unidos procedente del carbón y gas natural: el costo de la eólica se cifraba en $55,80 por MWh, el del carbón en $53,10/MWh y el del gas natural en $52,50. Otro informe gubernamental obtuvo resultados similares en comparación con el gas natural, en 2011 en Reino Unido. En agosto de 2011 licitaciones en Brasil y Uruguay para compra a 20 años presentaron costos inferiores a los $65 por MWh.

En febrero de 2013 "Bloomberg New Energy Finance" informó de que el costo de la generación de energía procedente de nuevos parques eólicos en Australia es menor que el procedente de nuevas plantas de gas o carbón. Al incluir en los cálculos el esquema de precios actual para los combustibles fósiles, sus estimaciones indicaban unos costos (en dólares australianos) de $80/MWh para nuevos parques eólicos, $143/MWh para nuevas plantas de carbón y $116/MWh para nuevas plantas de gas. Este modelo muestra además que «incluso sin una tasa sobre las emisiones de carbono (la manera más eficiente de reducir emisiones a gran escala) la energía eólica es un 14 % más barata que las nuevas plantas de carbón, y un 18 % más que las nuevas plantas de gas.»<ref name="bnef.com/2013/02/07/renewable-cheaper"></ref>

La industria eólica en Estados Unidos es actualmente capaz de producir mayor potencia a un costo menor gracias al uso de aerogeneradores cada vez más altos y con palas de mayor longitud, capturando de esta manera vientos mayores a alturas más elevadas. Esto ha abierto nuevas oportunidades, y en estados como Indiana, Míchigan y Ohio, el costo de la eólica procedente de aerogeneradores de entre 90 y 120 metros de altura puede competir con fuentes de energía convencionales como el carbón. Los precios han caído hasta incluso 4 céntimos por kWh en algunos casos, y las compañías distribuidoras están incrementando la cantidad de energía eólica en su modelo energético, al darse cuenta progresivamente de su competitividad.

El costo de la unidad de energía producida en instalaciones eólicas se deduce de un cálculo bastante complejo. Para su evaluación se deben tener en cuenta diversos factores, entre los cuales cabe destacar:

Existe una gran cantidad de aerogeneradores operando, con una capacidad total de 369597MW a finales de 2014, de los que Europa cuenta con el 36,3 %. China y los Estados Unidos representan juntos casi el 50 % de la capacidad eólica global, mientras que los primeros cinco países (China, EE. UU., Alemania, España e India) representaron el 71,7 % de la capacidad eólica mundial en 2014.

Alemania, España, Estados Unidos, India y Dinamarca han realizado las mayores inversiones en generación de energía eólica. Dinamarca es, en términos relativos, la más destacada en cuanto a fabricación y utilización de turbinas eólicas, con el compromiso realizado en los años 1970 de llegar a obtener la mitad de la producción de energía del país mediante el viento. En 2014 generó el 39,1 % de su electricidad mediante aerogeneradores, mayor porcentaje que cualquier otro país, y el año anterior la energía eólica se consolidó como la fuente de energía más barata del país.

La siguiente tabla muestra la capacidad total de energía eólica instalada al final de cada año (en megavatios) en todo el mundo, detallado por países. Datos publicados por el "Global Wind Energy Council" (GWEC).

A finales de 2018, España tenía instalada una capacidad de energía eólica de 23 507 MW, lo que supone el 22,6 % de la capacidad del sistema eléctrico nacional, la segunda fuente de energía del país por detrás del ciclo combinado con 26 284 MW. Se sitúa así en cuarto lugar en el mundo en cuanto a potencia instalada, detrás de China, EE. UU. y Alemania. Ese mismo año la energía eólica produjo 49 570 GWh, el 18,4 % de la demanda eléctrica.

El 29 de enero de 2015, la energía eólica alcanzó un máximo de potencia instantánea con 17 553 MW, cubriendo un 45 % de la demanda.

Asimismo, está creciendo bastante el sector de la minieólica. Existe una normativa de fabricación de pequeños aerogeneradores, del Comité Electrotécnico Internacional CEI (Norma IEC-61400-2 Ed2) la cual define un aerogenerador de pequeña potencia como aquel cuya área barrida por su rotor es menor de 200 m². La potencia que corresponde a dicha área dependerá de la calidad del diseño del aerogenerador, existiendo de hasta 65 kW como máximo.

El Reino Unido cerró 2008 con 4015 MW eólicos instalados, lo que supone una presencia testimonial en su producción eléctrica. Sin embargo es uno de los países del mundo que más capacidad eólica tiene planificada, y ya ha otorgado concesiones para alcanzar los 32 000 MW eólicos marinos en sus costas:
Según la administración británica “la industria eólica marina es una de las claves de la ruta del Reino Unido hacia una economía baja en emisiones de CO y debería suponer un valor de unos 75 000 millones de libras (84 000 millones de euros) y sostener unos 70 000 empleos hasta 2020”.

Suecia cerró 2009 con 1021 MW eólicos instalados y tiene planes para alcanzar los 14 000 MW en el año 2020, de los cuales entre 2500 y 3000 MW serán marinos.


El desarrollo de la energía eólica en los países de Centroamérica y Sudamérica está en sus inicios, y la capacidad conjunta instalada en ellos, hasta finales de 2013, llega a los 4709 MW. El desglose de potencia instalada por países es el siguiente: 


A finales de 2013, la potencia instalada acumulada por países del continente es la siguiente:

Debido a la variabilidad natural y la impredecibilidad del viento, para que la energía eólica pueda ser usada como única fuente de energía eléctrica es necesario almacenar la energía que se produce cuando hay viento para poder luego utilizarla cuando no lo hay. Pero hasta el momento no existen sistemas lo suficientemente grandes como para almacenar cantidades considerables de energía de forma eficiente. Por lo tanto, para hacer frente a los valles en la curva de producción de energía eólica y evitar apagones generalizados, es indispensable un respaldo de las energías convencionales como centrales termoeléctricas de carbón, gas natural, petróleo o ciclo combinado o centrales hidroeléctricas reversibles, por ejemplo. Esto supone un inconveniente, puesto que cuando respaldan a la eólica, las centrales de carbón no pueden funcionar a su rendimiento óptimo, que se sitúa cerca del 90% de su potencia. Tienen que quedarse muy por debajo de este porcentaje para poder subir sustancialmente su producción en el momento en que amaine el viento. Es por ello que, cuando funcionan en este modo, las centrales térmicas consumen más combustible por kWh producido. Además, al aumentar y disminuir su producción cada vez que cambia la velocidad del viento se produce un desgaste mayor de la maquinaría. Este problema del respaldo en España se va a tratar de solucionar mediante una interconexión con Francia que permita emplear el sistema europeo como colchón de la variabilidad eólica. Además, la variabilidad en la producción de energía eólica tiene otras importantes consecuencias:

Aunque estos problemas parecen únicos a la energía eólica, son comunes a todas las energías de origen natural:
Una de las formas de paliar la falta de control sobre los recursos renovables (viento, radiación solar), son los llamados sistemas híbridos, donde se combinan fuentes de energía junto con almacenamiento. Hay una tendencia a la creación de centrales renovables en las que participan generadores eólicos, solares y almacenamiento por baterías (por lo general de ion litio). En países como Australia o Estados Unidos se está regulando su uso e incluso se definen tarifas específicas para la inyección de energía desde estas centrales que comienzan a competir de igual a igual con las centrales basadas en combustibles fósiles, dado que comienzan a tener una previsión de generación a un día vista o más.



La microgeneración de energía eólica consiste en pequeños sistemas de generación de hasta 50 kW de potencia. En comunidades remotas y aislada, que tradicionalmente han utilizado generadores diésel, su uso supone una buena alternativa. También es empleada cada vez con más frecuencia por hogares que instalan estos sistemas para reducir o eliminar su dependencia de la red eléctrica por razones económicas, así como para reducir su impacto medioambiental y su huella de carbono. Este tipo de pequeñas turbinas se han venido usando desde hace varias décadas en áreas remotas junto a sistemas de almacenamiento mediante baterías.

Las pequeñas turbinas aerogeneradoras conectadas a la red eléctrica pueden utilizar también lo que se conoce como almacenamiento en la propia red, reemplazando la energía comprada de la red por energía producida localmente, cuando esto es posible. La energía sobrante producidad por los microgeneradores domésticos puede, en algunos países, ser vertida a la red para su venta a la compañía eléctrica, generando ingresos al propietario de la instalación que amortice la instalación.

Los sistemas desconectados de la red pueden adaptarse a la intermitencia del viento, utilizar baterías, sistemas fotovoltaicos o generadores diésel que complementen la energía producida por la turbina. Otros equipos, como pueden ser parquímetros, señales de tráfico iluminadas, alumbrado público, o sistemas de telecomunicaciones pueden ser también alimentados mediante un pequeño aerogenerador, generalmente junto a un sistema fotovoltaico que cargue unas pequeñas baterías, eliminando la necesidad de la conexión a la red.

La minieólica podría generar electricidad más barata que la de la red en algunas zonas rurales de Reino Unido, según un estudio de la organización Carbon Trust, publicado en 2010. Según ese informe, los mini aerogeneradores podrían llegar a generar 1,5  TWh de electricidad al año en Reino Unido, un 0,4 % del consumo total del país, evitando la emisión de 0,6 millones de toneladas de CO. Esta conclusión se basa en el supuesto de que el 10 % de las viviendas instalara miniturbinas eólicas a precios competitivos con aquellos de la red eléctrica, en torno a 12 peniques (unos 0,17 €) por kWh. Otro informe preparado en 2006 por "Energy Saving Trust", una organización dependiente del Gobierno de Reino Unido, dictaminó que la microgeneración (de diferente tipo: eólica, solar, etc.) podría proporcionar hasta el 30 % o 40 % de la demanda de electricidad en torno al año 2050.

La generación distribuida procedente de energías renovables se ha incrementado en los últimos años, como consecuencia de la mayor concienciación acerca de la influencia del ser humano en el cambio climático. Los equipos electrónicos requeridos para permitir la conexión de sistemas de generación renovable a la red eléctrica pueden además incluir otros sistemas de estabilidad de la red para asegurar y garantizar la calidad del suministro eléctrico.






</doc>
<doc id="3361" url="https://es.wikipedia.org/wiki?curid=3361" title="Mercurio (planeta)">
Mercurio (planeta)

Mercurio es el planeta del sistema solar más próximo al Sol y el más pequeño. Forma parte de los denominados planetas interiores o terrestres y carece de satélites naturales al igual que Venus. Se conocía muy poco sobre su superficie hasta que fue enviada la sonda planetaria "Mariner 10" y se hicieron observaciones con radar y radiotelescopios. Posteriormente fue estudiado por la sonda MESSENGER de la NASA y actualmente la astronave de la Agencia Europea del Espacio (ESA) denominada BepiColombo, lanzada en octubre de 2018, se halla en vuelo rumbo a Mercurio a donde llegará en 2025 y se espera que aporte nuevos conocimientos sobre el origen y composición del planeta, así como de su geología y campo magnético.

Antiguamente se pensaba que Mercurio siempre presentaba la misma cara al Sol, (rotación capturada) situación similar al caso de la Luna con la Tierra; es decir, que su periodo de rotación era igual a su periodo de traslación, ambos de 88 días. Sin embargo, en 1965 se mandaron impulsos de radar hacia Mercurio, con lo cual quedó definitivamente demostrado que su periodo de rotación era de 58,7 días, lo cual es 2/3 de su periodo de traslación. Esto no es coincidencia, y es una situación denominada resonancia orbital.

Al ser un planeta cuya órbita es interior a la de la Tierra, lo observamos pasar periódicamente delante del Sol, fenómeno que se denomina tránsito astronómico. Observaciones de su órbita a través de muchos años demostraron que el perihelio gira 43" de arco más por siglo de lo predicho por la mecánica clásica de Newton. Esta discrepancia llevó a un astrónomo francés, Urbain Le Verrier, a pensar que existía un planeta aún más cerca del Sol, al cual llamaron Vulcano, que perturbaba la órbita de Mercurio. Ahora se sabe que Vulcano no existe; la explicación correcta del comportamiento del perihelio de Mercurio se encuentra en la teoría general de la relatividad de Einstein.

Mercurio es uno de los cuatro planetas rocosos o sólidos; es decir, tiene un cuerpo rocoso, como la Tierra. Este planeta es el más pequeño de los cuatro, con un diámetro de 4879km en el ecuador. Mercurio está formado aproximadamente por un 70 % de elementos metálicos y un 30 % de silicatos. La densidad de este planeta es la segunda más grande de todo el sistema solar, siendo su valor de 5430kg/m³, solo un poco menor que la densidad de la Tierra. La densidad de Mercurio se puede usar para deducir los detalles de su estructura interna. Mientras la alta densidad de la Tierra se explica considerablemente por la compresión gravitacional, particularmente en el núcleo, Mercurio es mucho más pequeño y sus regiones interiores no están tan comprimidas. Por tanto, para explicar esta gran densidad, el núcleo debe ocupar gran parte del planeta y además ser rico en hierro, material con una alta densidad. Los geólogos estiman que el núcleo de Mercurio ocupa un 42 % de su volumen total (el núcleo de la Tierra apenas ocupa un 17 %). Este núcleo estaría parcialmente fundido, lo que explicaría el campo magnético del planeta.

Rodeando el núcleo existe un manto de unos 600 km de grosor. La creencia generalizada entre los expertos es que en los principios de Mercurio un cuerpo de varios kilómetros de diámetro (un planetesimal) impactó contra él deshaciendo la mayor parte del manto original, dando como resultado un manto relativamente delgado comparado con el gran núcleo. (Otras teorías alternativas se discuten en la sección "Formación de Mercurio").
La corteza mercuriana mide en torno a los 100-200 km de espesor. Un hecho distintivo de la corteza de Mercurio son las visibles y numerosas líneas escarpadas o escarpes que se extienden varios miles de kilómetros a lo largo del planeta. Presumiblemente se formaron cuando el núcleo y el manto se enfriaron y contrajeron al tiempo que la corteza se estaba solidificando.

La superficie de Mercurio, como la de la Luna, presenta numerosos impactos de meteoritos que oscilan entre unos metros hasta miles de kilómetros. Algunos de los cráteres son relativamente recientes, de algunos millones de años de edad, y se caracterizan por la presencia de un pico central. Parece ser que los cráteres más antiguos han tenido una erosión muy fuerte, posiblemente debida a los grandes cambios de temperatura que en un día normal oscilan entre 623K (350 °C) por el día y 103K (–170 °C) por la noche.

Al igual que la Luna, Mercurio parece haber sufrido un período de intenso bombardeo de meteoritos de grandes dimensiones, hace unos 4000 millones de años. Durante este periodo de formación de cráteres, Mercurio recibió impactos en toda su superficie, facilitados por la práctica ausencia de atmósfera que pudiera desintegrar o frenar multitud de estas rocas. Durante este tiempo, Mercurio fue volcánicamente activo, formándose cuencas o depresiones con lava del interior del planeta y produciendo planicies lisas similares a los "mares" o "marías" de la Luna; una prueba de ello es el descubrimiento por parte de la sonda MESSENGER de posibles volcanes.

Las planicies o llanuras de Mercurio tienen dos distintas edades; las jóvenes llanuras están menos craterizadas y probablemente se formaron cuando los flujos de lava enterraron el terreno anterior. Un rasgo característico de la superficie de este planeta son los numerosos pliegues de compresión que entrecruzan las llanuras. Se piensa que, como el interior del planeta se enfrió, se contrajo y la superficie comenzó a deformarse. Estos pliegues se pueden apreciar por encima de cráteres y planicies, lo que indica que son mucho más recientes. La superficie mercuriana está significativamente flexada a causa de la fuerza de marea ejercida por el Sol. Las fuerzas de marea en Mercurio son un 17 % más fuertes que las ejercidas por la Luna en la Tierra.

Destacable en la geología de Mercurio es la cuenca de Caloris, un cráter de impacto que constituye una de las mayores depresiones meteóricas de todo el sistema solar; esta formación geológica tiene un diámetro aproximado de 1550 km (antes del sobrevuelo de la sonda Messenger se creía que su tamaño era de 1300 km). Contiene, además, una formación de origen desconocido no antes vista ni en el propio Mercurio ni en la Luna, y que consiste en aproximadamente un centenar de grietas estrechas y de suelo liso conocida como "La Araña"; en el centro de esta se encuentra un cráter, desconociéndose si dicho cráter está relacionado con su formación o no. Interesantemente, también el albedo de la cuenca de Caloris es superior al de los terrenos circundantes (al revés de lo que ocurre en la Luna). La razón de ello se está investigando.

Justo en el lado opuesto de esta inmensa formación geológica se encuentran unas colinas o cordilleras conocidas como Terreno Extraño, o "Weird Terrain". Una hipótesis sobre el origen de este complejo geomorfológico es que las ondas de choque generadas por el impacto que formó la cuenca de Caloris atravesaron toda la esfera planetaria convergiendo en las antípodas de dicha formación (180°), fracturando la superficie y formando esta cordillera.

Al igual que otros astros de nuestro sistema solar, como el más semejante en aspecto, la Luna, la superficie de Mercurio probablemente ha incurrido en los efectos de procesos de desgaste espaciales, o erosión espacial. El viento solar e impactos de micrometeoritos pueden oscurecer la superficie, cambiando las propiedades reflectantes de ésta y el albedo general de todo el planeta.

A pesar de las temperaturas extremadamente altas que hay generalmente en su superficie, observaciones más detalladas sugieren la existencia de hielo en Mercurio. El fondo de varios cráteres muy profundos y oscuros cercanos a los polos que nunca han quedado expuestos directamente a la luz solar tienen una temperatura muy inferior a la media global. El hielo (de agua) es extremadamente reflectante al radar, y recientes observaciones revelan imágenes muy reflectantes en el radar cerca de los polos; el hielo no es la única causa posible de dichas regiones altamente reflectantes, pero sí la más probable. Se especula que el hielo tiene solo unos metros de profundidad en estos cráteres, conteniendo alrededor de una tonelada de esta sustancia. El origen del agua helada en Mercurio no es conocido a ciencia cierta, pero se especula que o bien se congeló de agua del interior del planeta o vino de cometas que impactaron contra el suelo.

El estudio de la interacción de Mercurio con el viento solar ha puesto en evidencia la existencia de una magnetosfera en torno al planeta. El origen de este campo magnético no es conocido. En 2007 observaciones muy precisas realizadas desde la Tierra mediante radar, demostraron un bamboleo del eje de rotación compatible solo con un núcleo del planeta parcialmente fundido. Un núcleo parcialmente fundido con materiales ferromagnéticos podría ser la causa de su campo magnético.

La intensidad del campo magnético es de 220nT.

La órbita de Mercurio es la más excéntrica entre todos los planetas que orbitan el Sol, (antes de ser reclasificado como planeta enano, ese característica le correspondía al entonces planeta Plutón). La distancia de Mercurio al Sol varía en un rango entre 46 y 70 millones de kilómetros. Tarda 88 días terrestres en dar una traslación completa. La inclinación de su plano orbital con respecto al plano de la eclíptica de 7°.

En la imagen anexa se ilustran los efectos de la excentricidad, mostrando la órbita de Mercurio sobre una órbita circular que tiene el mismo semieje. La elevada velocidad del planeta cuando está cerca del perihelio hace que cubra esta mayor distancia en un intervalo de solo cinco días. El tamaño de las esferas, inversamente proporcional a la distancia al Sol, es usado para ilustrar la distancia variable heliocéntrica. Esta distancia variable al Sol, combinada con la rotación planetaria de Mercurio de 3:2 alrededor de su eje, resulta en complejas variaciones de la temperatura de su superficie, pasando de los –185°C durante las noches hasta los 430 °C durante el día.

La inclinación de su eje de rotación respecto del eje perpendicular a su plano orbital es de tan solo 0,01° (grados sexagesimales), unas 300 veces menos que la de Júpiter, que es el segundo planeta en esta estadística con 3,1° (en la Tierra la inclinación es de 23,5°). De esta forma, un observador en el ecuador de Mercurio durante el mediodía local nunca vería el Sol más que 0,01° al norte o al sur del cenit. Análogamente, en los polos el centro del Sol nunca pasa más de 0,01° por encima del horizonte.

En Mercurio existe el fenómeno de los amaneceres dobles, cuando el Sol sale aproximadamente dos tercios de su tamaño, se detiene, se esconde nuevamente casi exactamente por donde salió y luego vuelve a salir para continuar su recorrido por el cielo; esto solo ocurre en algunos puntos de la superficie, a 180º de longitud de estos, lo que se observa es un doble anochecer.

Debido al mismo mecanismo, en el resto del planeta se observa que el Sol aparentemente se detiene en el cielo y realiza un movimiento de retroceso . Esto se debe a que aproximadamente cuatro días terrestres antes del perihelio, la velocidad angular orbital de Mercurio iguala a su velocidad angular de rotación, lo que hace que el movimiento aparente del Sol cese, se invierta el movimiento durante los ocho días seguidos en los que la velocidad angular orbital es superior a la de rotación, y finalmente cuatro días después del perihelio el Sol vuelva a detenerse y recuperar su sentido de movimiento inicial.

Justo en el perihelio es cuando la velocidad angular orbital de Mercurio excede en mayor magnitud a la velocidad angular de rotación, y es entonces cuando la velocidad aparente de retroceso del Sol es la máxima.

El avance del perihelio de Mercurio fue notado por primera vez en el siglo XIX al observar la lenta precesión de la línea de los ápsides de la órbita del planeta alrededor del Sol, la cual no conseguía ser explicada completamente por las leyes de Newton ni por perturbaciones de planetas conocidos (trabajo muy notable del matemático francés Urbain Le Verrier). Se conjeturó entonces que otro planeta desconocido en una órbita más interior al Sol era el causante de estas perturbaciones (se consideraron otras teorías como un leve achatamiento de los polos solares). El éxito de la búsqueda de Neptuno a consecuencia de las perturbaciones orbitales de Urano hicieron poner mucha fe a los astrónomos para esta hipótesis. A este hipotético planeta desconocido se le denominaría planeta Vulcano. Sin embargo, a comienzos del siglo XX, la Teoría General de la Relatividad de Albert Einstein explicó completamente la precesión observada, descartando al inexistente planeta (véase órbita planetaria relativista). El efecto en el avance del perihelio mercuriano es muy pequeño: apenas de 42,98 segundos de arco por siglo, por lo que necesita más de 12,5 millones de órbitas para exceder una vuelta completa.
La expresión que proporciona la Relatividad General para calcular la precesión del perihelio de un planeta, en radianes por revolución es :
G = Constante de gravitación universal
M = Masa del Sol
a = Semieje mayor de la órbita
e = Excentricidad de la órbita
c = Velocidad de la luz

Esta expresión proporciona 42,98" de arco por siglo para Mercurio y valores mucho menores para el resto de planetas, dando 8,52 arcosegundos por siglo para Venus, 3,84 para la Tierra, 1,35 para Marte, y 10,05 para el asteroide de tipo Apolo (1566) Ícaro.

Durante muchos años se pensó que la misma cara de Mercurio miraba siempre hacia el Sol, de forma sincrónica, similar a como lo hace la Luna respecto a la Tierra. No fue hasta 1965 cuando observaciones por radio (ver Observación con Grandes Telescopios) descubrieron una resonancia orbital de 2:3, rotando tres veces cada dos años mercurianos; la excentricidad de la órbita de Mercurio hace esta resonancia estable en el perihelio, cuando la marea solar es más fuerte, el Sol está todavía en el cielo de Mercurio. La razón por la que los astrónomos pensaban que Mercurio giraba de manera sincrónica era que siempre que el planeta estaba en mejor posición para su observación, mostraba la misma cara. Ya que Mercurio gira en un 3:2 de resonancia orbital, un día solar (la duración entre dos tránsitos meridianos del Sol) son unos 176 días terrestres. Un día sideral es de unos 58,6 días terrestres.

Simulaciones orbitales indican que la excentricidad de la órbita de Mercurio varía caóticamente desde 0 (circular) a 0,47 a lo largo de millones de años. Esto da una idea para explicar la resonancia orbital mercuriana de 2:3, cuando lo más usual es 1:1, ya que esto es más razonable para un periodo con una excentricidad tan alta.

La magnitud aparente de Mercurio varía entre –2,0 (brillante como la estrella Sirio) y 5,5. La observación de Mercurio es complicada por su proximidad al Sol, perdido en el resplandor de la estrella madre durante un período muy grande. Mercurio solo se puede observar por un corto período durante el crepúsculo de la mañana o de la noche. El telescopio espacial Hubble no puede observar Mercurio, ya que por procedimientos de seguridad se evita un enfoque tan cercano al Sol.

Como la Luna, Mercurio exhibe fases vistas desde la Tierra, siendo "nueva" en conjunción inferior y "llena" en conjunción superior. El planeta deja de ser invisible en ambas ocasiones por la virtud de este ascenso y ubicación acuerdo con el Sol en cada caso. La primera y última fase ocurre en máxima elongación este y oeste, respectivamente, cuando la separación de Mercurio del rango del Sol es de 18,5° en el periastro y 28,3 en el apoastro. En máxima elongación oeste, Mercurio se eleva antes que el Sol y en la este después que el Sol.

Mercurio alcanza una conjunción inferior cada 116 días de media, pero este intervalo puede cambiar de 111 a 121 días por la excentricidad de la órbita del planeta. Este periodo de movimiento retrógrado visto desde la Tierra puede variar de 8 a 15 días en cualquier lado de la conjunción inferior. Esta larga variación de tiempo es consecuencia también de la elevada excentricidad orbital.

Mercurio es más fácil de ver desde el hemisferio sur de la Tierra que desde el hemisferio norte; esto se debe a que la máxima elongación del oeste posible de Mercurio siempre ocurre cuando es otoño en el hemisferio sur, mientras que la máxima elongación del este ocurre cuando es invierno en el hemisferio norte. En ambos casos, el ángulo de Mercurio incide de manera máxima con la eclíptica, permitiendo elevarse varias horas antes que el Sol y no se pone hasta varias horas después del ocaso en los países situados en latitudes templadas del hemisferio sur, como Argentina y Nueva Zelanda. Por contraste, en las latitudes templadas del hemisferio norte, Mercurio nunca está por encima del horizonte en más o menos a medianoche. Como muchos otros planetas y estrellas brillantes, Mercurio puede ser visto durante un eclipse solar.
Además, Mercurio es más brillante visto desde la Tierra cuando se encuentra entre la fase creciente o la menguante y la llena. Aunque el planeta está más lejos en ese momento que cuando está creciente, el área iluminada visible mayor compensa esa mayor distancia. Justo al contrario que Venus, que aparece más brillante cuando está en cuarto creciente, porque está mucho más cerca de la Tierra.

El tránsito de Mercurio es el paso, observado desde la Tierra, de este planeta por delante del Sol. La alineación de estos tres astros (Sol, Mercurio y la Tierra) produce este particular efecto, solo comparable con el tránsito de Venus. El hecho de que Mercurio esté en un plano diferente en la eclíptica que nuestro planeta (7° de diferencia) hace que solo una vez cada varios años ocurra este fenómeno. Para que el tránsito se produzca, es necesario que la Tierra esté cerca de los nodos de la órbita. La Tierra atraviesa cada año la línea de los nodos de la órbita de Mercurio el 8-9 de mayo y el 10-11 de noviembre; si para esa fecha coincide una conjunción inferior habrá paso. Existe una cierta periodicidad en estos fenómenos aunque obedece a reglas complejas. Es claro que tiene que ser múltiplo del periodo sinódico. Mercurio suele transitar el disco solar un promedio de unas 13 veces al siglo en intervalos de 3, 7, 10 y 13 años.

Las primeras menciones conocidas de Mercurio, hechas por los sumerios, datan del tercer milenio a. C. Los babilonios (2000-500 a. C.) hicieron igualmente nuevas observaciones sobre el planeta, denominándolo como Nabu o Nebu, el mensajero de los dioses en su mitología.

Los observadores de la Antigua Grecia llamaron al planeta de dos maneras: "Apolo" cuando era visible en el cielo de la mañana y "Hermes" cuando lo era al anochecer. Sin embargo, los astrónomos griegos se dieron cuenta que se referían al mismo cuerpo celeste, siendo Pitágoras el primero en proponer la idea.

Las primeras observaciones con telescopio de Mercurio datan de Galileo en el siglo XVII. Aunque él observara las fases planetarias cuando miraba a Venus, su telescopio no era lo suficientemente potente para distinguir las fases de Mercurio. En 1631 Pierre Gassendi realizó las primeras observaciones del tránsito de Mercurio cruzando el Sol cuando vio el tránsito de Mercurio predicho por Johannes Kepler. En 1639 Giovanni Zupi usó un telescopio para descubrir que el planeta tenía una fase orbital similar a la de Venus y la Luna. La observación demostró de manera concluyente que Mercurio orbitaba alrededor del Sol.

Un hecho extraño en la astronomía es que un planeta pase delante de otro (ocultación), visto desde la Tierra. Mercurio y Venus se ocultan cada varios siglos, y el 28 de mayo de 1737 ocurrió el único e histórico registrado. El astrónomo que lo observó fue John Bevis en el Real Observatorio de Greenwich. La próxima ocultación ocurrirá en 2133.

En 1800 Johann Schröter pudo hacer algunas observaciones de la superficie, pero erróneamente estimó que el planeta tenía un periodo de rotación similar a la terrestre, de unas 24 horas. En la década de 1880 Giovanni Schiaparelli realizó un mapa de Mercurio más correcto, y sugirió que su rotación era de 88 días, igual que su período de traslación (Rotación síncrona).

La teoría por la cual la rotación de Mercurio era sincrónica se hizo extensamente establecida, y fue un giro de 180° cuando los astrónomos mediante observaciones de radio en los años 1960 cuestionaron la teoría. Si la misma cara de Mercurio estuviera dirigida siempre hacia el Sol, la parte en sombra estaría extremadamente fría, pero las mediciones de radio revelaron que estaba mucho más caliente de lo esperado. En 1965 se constató que definitivamente el periodo de rotación era de 59 días. El astrónomo italiano Giuseppe Colombo notó que este valor era sobre dos terceras partes del período orbital de Mercurio, y propuso una forma diferente de la fuerza de marea que hizo que los períodos orbitales y rotatorios del planeta se quedasen en 3:2 más bien que en 1:1 (resonancia orbital). Más tarde la "Mariner 10" lo confirmó.

Las observaciones por grandes telescopios en tierra no arrojaron mucha luz sobre este mundo difícil de ver, y no fue hasta la llegada de sondas espaciales que visitaron Mercurio cuando se descubrieron y confirmaron grandes e importantes propiedades del planeta. No obstante, recientes avances tecnológicos han llevado a observaciones mejoradas: en 2000, el telescopio de alta resolución del Observatorio Monte Wilson de 1500 mm proporcionó las primeras imágenes que resolvieron algunos rasgos superficiales sobre las regiones de Mercurio que no fueron fotografiadas durante las misiones del Mariner. Imágenes recientes apuntan al descubrimiento de una cuenca de impacto de doble anillo más largo que la "Cuenca de Caloris", en el hemisferio no fotografiado por la Mariner. Es informalmente conocido como "Cuenca de Shinakas".

Llegar hasta Mercurio desde la Tierra supone un significativo reto tecnológico, ya que la órbita del planeta está mucho más cerca que la terrestre del Sol. Una nave espacial con destino a Mercurio lanzada desde nuestro planeta deberá de recorrer unos 91 millones de kilómetros por los puntos de potencial gravitatorio del Sol. Comenzando desde la órbita terrestre a unos 30 km/s, el cambio de velocidad que la nave debe realizar para entrar en una órbita de transferencia, conocida como órbita de transferencia de Hohmann (en la que se usan dos impulsos del motor cohete) para pasar cerca de Mercurio es muy grande comparado con otras misiones planetarias.

Además, para conseguir entrar en una órbita estable el vehículo espacial debe confiar plenamente en sus motores de propulsión, puesto que el aerofrenado está descartado por la falta de atmósfera significativa en Mercurio. Un viaje a este planeta en realidad es más costoso en lo que a combustible se refiere por este hecho que hacia cualquier otro planeta del sistema solar.

La sonda "Mariner 10" (1974-1975), o "Mariner X", fue la primera nave en estudiar en profundidad el planeta Mercurio. Había visitado también Venus, utilizando la asistencia de trayectoria gravitacional de Venus para acelerar hacia el planeta.

Realizó tres sobrevuelos a Mercurio; el primero a una distancia de 703 km del planeta, el segundo a 48.069 km, y el tercero a 327 km. Mariner tomó en total diez mil imágenes de gran parte de la superficie del planeta. La misión finalizó el 24 de marzo de 1975 cuando se quedó sin combustible y no podía mantener control de orientación.

"MErcury Surface, Space ENvironment, GEochemistry and Ranging" (Superficie de Mercurio, Entorno Espacial, Geoquímica y Extensión) fue una sonda lanzada en agosto de 2004 para ponerse en órbita alrededor de Mercurio en marzo de 2011. Se esperaba que esta nave aumentara considerablemente el conocimiento científico sobre este planeta. Para ello, la nave había de orbitar Mercurio y hacer tres sobrevuelos –los días 14 de enero de 2008, 6 de octubre de 2008, y 29 de septiembre de 2009–. La misión estaba previsto que durase un año. El 18 de marzo de 2011 se produjo con éxito la inserción orbital de la sonda. Finalmente el fin de esta exitosa misión se produjo el 30 de abril de 2015, cuando la sonda se precipitó sobre la superficie del planeta produciéndose un impacto controlado.

Es una misión conjunta de la Agencia Espacial Europea (ESA) y de la Agencia Japonesa de Exploración Espacial (JAXA), que consiste en dos módulos orbitantes u orbitadores que realizarán una completa exploración de Mercurio. El primero de los orbitadores será el encargado de fotografiar y analizar el planeta y el segundo investigará la magnetosfera. Su lanzamiento está previsto en julio de 2018, la llegada al planeta en enero de 2024, y el final de la misión para un año más tarde, con una posible extensión de un año más.





</doc>
<doc id="3362" url="https://es.wikipedia.org/wiki?curid=3362" title="Mercurio (elemento)">
Mercurio (elemento)

El mercurio es un elemento químico con el símbolo Hg y número atómico 80. En la literatura antigua era designado comúnmente como plata líquida y también como azogue o hidrargiro. Elemento de aspecto plateado, metal pesado perteneciente al bloque D de la tabla periódica, el mercurio es el único elemento metálico líquido en condiciones estándar de laboratorio; el único otro elemento que es líquido bajo estas condiciones es el bromo (un no metal), aunque otros metales como el cesio, el galio, y el rubidio se funden a temperaturas ligeramente superiores.

El mercurio aparece en depósitos en todo el mundo, principalmente como cinabrio (sulfuro de mercurio). El pigmento rojo denominado bermellón se obtiene triturando cinabrio natural o sulfuro de mercurio obtenido por síntesis.

El mercurio se usa en termómetros, barómetros, manómetros, esfigmomanómetros, algunos tipos de válvulas como las bombas de vacío, los interruptores de mercurio, las lámparas fluorescentes y otros dispositivos, a pesar de que la preocupación sobre la toxicidad del elemento ha llevado a los termómetros y tensiómetros de mercurio a ser eliminados en gran medida en entornos clínicos en favor de otras alternativas, como los termómetros de vidrio que utilizan alcohol o galinstano, los termistores o los instrumentos electrónicos basados en la medición de la radiación infrarroja. Del mismo modo, manómetros mecánicos y sensores de calibradores de tensión electrónicos han sustituido a los esfigmomanómetros de mercurio. El mercurio se mantiene en uso en aplicaciones de investigación científica y en amalgamas odontológicas, todavía utilizadas en algunos países. También se utiliza en las luces fluorescentes, en las que la electricidad que atraviesa una lámpara conteniendo vapor de mercurio a baja presión produce radiación ultravioleta de onda corta, que a su vez provoca la fluorescencia del fósforo que recubre el tubo, produciendo luz visible.

El envenenamiento por mercurio puede resultar de la exposición a las formas solubles en agua del mercurio (como el cloruro mercúrico o el metilmercurio), por la inhalación de vapor de mercurio, o por la ingestión de cualquiera de sus formas.
El mercurio es un metal pesado plateado que a temperatura ambiente es un líquido inodoro. No es buen conductor del calor comparado con otros metales, aunque es buen conductor de la electricidad. Se alea fácilmente con muchos otros metales como el oro o la plata produciendo amalgamas, pero no con el hierro. Es insoluble en agua y soluble en ácido nítrico. Cuando aumenta su temperatura -por encima de los 40 °C-, produce vapores tóxicos y corrosivos, más pesados que el aire por lo que se evapora creando miles de partículas en el vapor que al enfriarse se depositan de nuevo. Es dañino por inhalación, ingestión y contacto: se trata de un producto muy irritante para la piel, ojos y vías respiratorias. Es incompatible con el ácido nítrico concentrado, el acetileno, el amoníaco, el cloro y los metales.

El mercurio es un elemento anómalo en varias de sus propiedades. Es un metal noble, ya que su potencial rédox Hg/Hg es positivo (+0,85 V), frente al negativo del cadmio Cd (-0,40 V), su vecino inmediato de grupo. Es un metal singular con algo de parecido al cadmio, pero más semejante al oro y al talio. Es el único metal de transición con una densidad tan elevada, 13,53 g/cm³; una columna de 76 cm define una atmósfera, mientras que con agua se necesita una columna de 10 m de altura. Su estado líquido en condiciones estándar indica que su enlace metálico es débil y se justifica por la poca participación de los electrones 6s² a la deslocalización electrónica en el sistema metálico (efectos relativistas).

Tiene la primera energía de ionización más alta de todos los metales (10,4375 eV) por la misma razón anterior. Además el Hg tiene muy baja entalpía de hidratación comparada con la del cinc Zn y la del cadmio Cd, con preferencia por la coordinación dos en los complejos de Hg (II), como el oro Au (I) isoelectrónico. Esto trae como consecuencia que los potenciales rédox de aquellos sean negativos y el del mercurio sea noble (positivo).

La poca reactividad del mercurio en procesos oxidativos se justifica por los efectos relativistas sobre los electrones 6s² muy contraídos hacia el núcleo y por la fortaleza de su estructura electrónica de pseudogas noble.

También es el único elemento del grupo que presenta el estado +1, en forma de especie dinuclear Hg, aunque la tendencia general a estabilizar los estados de oxidación bajos sea la contraria en los grupos de transición: formación de compuestos de Hg (I) con racimos de pares Hg-Hg. Esta rica covalencia también se puede apreciar en compuestos de Hg (II), donde se aprecia que muchos de estos compuestos de Hg (II) son volátiles como el HgCl, sólido molecular con entidades Cl-Hg-Cl en sólido, vapor e incluso en disolución acuosa. También es destacable la resistencia de amidas, imidas y organometálicos de mercurio a la hidrólisis y al oxígeno del ambiente, lo que indica la gran fortaleza del enlace con el carbono Hg-C. También el azufre S y el fósforo P son átomos dadores adecuados: ligandos blandos efectivos para ácidos blandos como el Hg en estados de oxidación cero, I y II.

El estado de oxidación más alto del mercurio es el II debido a su configuración electrónica externa ds², y a que la suma de sus tres primeras energías de ionización es demasiado alta para que en condiciones estándar se generen estados de oxidación III o superiores. Sin embargo, en 2007 se ha descubierto que a bajísimas temperaturas, del orden de -260 °C (esto es, la temperatura media del espacio), existe en estado de oxidación IV, pudiendo asociarse con cuatro átomos de flúor y obteniéndose de tal modo ese grado de oxidación adicional. A esta forma se la denomina tetrafluoruro de mercurio (HgF); la estructura es plano-cuadrada, la de mayor estabilidad para una especie d procedente de un metal "5d". Este comportamiento es esperable, dado que el mercurio tiene mayor expansión relativista de sus orbitales 5d con relación a sus homólogos del grupo 12, con lo que frente al flúor, el elemento más oxidante de la tabla periódica, puede en condiciones extremas generar enlaces covalentes. La posibilidad de sintetizar este fluoruro de mercurio, HgF, fue predicha teóricamente en 1994 de acuerdo a modelos antes indicados. Por la misma razón se puede considerar la posibilidad del estado de oxidación III para este metal, y efectivamente se ha aislado una especie compleja, en un medio especial y por oxidación electroquímica, donde aparece el catión complejo,[Hg cyclam]; el cyclam es un ligando quelato que estabiliza al mercurio en este estado de oxidación raro (1,4,8,11-Tetraazaciclotetradecane= cyclam). Con todo esto, se debe concluir que el mercurio debe ser reconsiderado para ser incluido como un metal de transición, ya que genera especies con orbitales d internos que están vacíos, por lo que se tiene una energía favorable de "estabilización por el campo de los ligandos" (EECL).

El mercurio es un metal blanco plateado y pesado. En comparación con otros metales, es un mal conductor del calor, pero un buen conductor de la electricidad. Presenta un punto de solidificación de -38,83 °C y un punto de ebullición de 356,73 °C, ambos excepcionalmente bajos para un metal. Además, el punto de ebullición del mercurio de es el más bajo de cualquier metal. Una explicación completa de este hecho se adentra profundamente en el reino de la física cuántica, pero se puede resumir de la siguiente manera: el mercurio tiene una configuración electrónica única, en la que los electrones recubren todos los niveles disponibles 1s, 2s, 2p, 3s, 3p, 3d, 4s, 4p, 4d, 4f, 5s, 5p, 5d y 6s. Debido a que esta configuración resiste considerablemente a la liberación de un electrón, el mercurio se comporta de manera similar a los gases nobles, que forman enlaces débiles y por lo tanto se funden a bajas temperaturas. Tras la congelación, el volumen del mercurio disminuye en un 3,59% y su densidad cambia de 13,69 g/cm en estado líquido a 14,184 g/cm cuando se solidifica. El coeficiente de expansión volumétrico es de 181,59x10 a 0 °C, 181,71x10 a 20 °C y de 182,50x10 a 100 °C (por cada °C).

La estabilidad del orbital 6s es debida a la presencia del nivel 4f repleto. La capa f apantalla débilmente la carga nuclear efectiva, lo que aumenta la atracción debida a la fuerza de Coulomb entre el nivel 6s y el núcleo (ver "contracción lantánida"). La ausencia de un nivel interior "f" repleto es la razón de la temperatura de fusión algo más alta del cadmio y del zinc, aunque estos dos metales también funden fácilmente y, además, presentan puntos de ebullición inusualmente bajos.

Por otro lado, el oro, que ocupa un espacio a la izquierda del mercurio en la tabla periódica, tiene átomos con un electrón menos en la capa 6s que el mercurio. Esos electrones se liberan con mayor facilidad y son compartidos entre los átomos de oro, que forman un relativamente fuerte enlace metálico.

El mercurio no reacciona con la mayoría de los ácidos, tales como el ácido sulfúrico diluido, aunque los ácidos oxidantes como el ácido sulfúrico concentrado y el ácido nítrico o el agua regia lo disuelven para dar sulfato, nitrato, y cloruro. Como la plata, el mercurio reacciona con el ácido sulfhídrico atmosférico. Así mismo, reacciona con copos de azufre sólido, que se utilizan en los equipos para absorber el mercurio en caso de derrame (también se utilizan con este mismo propósito carbón activado y zinc en polvo).

El mercurio disuelve muchos otros metales como el oro y la plata para formar amalgamas. El hierro es una excepción, por lo que recipientes de hierro se han utilizado tradicionalmente para el comercio de mercurio. Varios otros elementos de la primera fila de los metales de transición (con la excepción del manganeso, el cobre y el zinc) son reacios a formar amalgamas. Otros elementos que no forman fácilmente amalgamas con el mercurio incluyen al platino. La "amalgama de sodio" es un agente reductor común en síntesis orgánica, y también se utiliza en las lámparas de lámparas de vapor de sodio de alta presión.

El mercurio se combina fácilmente con el aluminio para formar una "amalgama de aluminio" cuando los dos metales puros entran en contacto. Esta amalgama destruye la capa de óxido de aluminio que protege al aluminio metálico de oxidarse en profundidad (como le sucede al hierro ante el agua). Incluso pequeñas cantidades de mercurio pueden corroer gravemente el aluminio. Por esta razón, el mercurio no se permite a bordo de una aeronave bajo la mayoría de las circunstancias, debido al riesgo de la formación de una amalgama con partes de aluminio expuestas en la aeronave.

El ataque del mercurio sobre el aluminio es uno de los tipos más comunes de "fragilización por metal líquido".

Hay siete isotopos estables del mercurio, con siendo el más abundante (29,86%). Los radioisótopos más longevos son con un período de semidesintregración de 444 años, y con una vida media de 46,612 días. La mayor parte de los radioisótopos restantes tienen vidas medias que son de menos de un día. y son los núcleos activos más a menudo estudiados mediante resonancia magnética nuclear, teniendo espines de y respectivamente.

Hg es el símbolo químico moderno para representar abreviadamente al mercurio. Proviene de "hydrargyrum", una forma latinizada del término griego ὑδράργυρος ("hydrargyros"), que es una palabra compuesta que significa "agua-plata" (de ὑδρ- " hydr- ", la raíz de ὕδωρ, "agua", y ἄργυρος "argyros" "plata"), ya que es líquido como el agua y brillante como la plata. Comparte el nombre con el dios romano Mercurio, conocido por su velocidad y movilidad. Por el mismo motivo, también se asocia con el planeta Mercurio. El símbolo astrológico del planeta es asimismo el símbolo alquímico del metal; la palabra sánscrita para la alquimia es "Rasavātam", que significa literalmente "el camino de mercurio". El mercurio es el único metal para el que su nombre planetario alquímico se convirtió en su nombre común.

El mercurio se encuentra en tumbas del Antiguo Egipto que datan del 1500 a. C.

En China y el Tíbet, el uso del mercurio era recomendado para prolongar la vida, curar fracturas y conservar la buena salud en general, aunque ahora se sabe que la exposición a los vapores de mercurio conduce a graves efectos adversos sobre la salud. El primer emperador de China, Qin Shi Huang (supuestamente enterrado en el denominado "Mausoleo de Qin Shi Huang", que contenía ríos de mercurio que fluyen reproduciendo un modelo de la tierra gobernada en el que se representaban los ríos de China) murió por beber una mezcla de mercurio y de jade en polvo recetado por los alquimistas de la Dinastía Qin (causándole fallo hepático, envenenamiento por mercurio y muerte cerebral) que pretendía darle vida eterna. Khumarawayh ibn Ahmad ibn Tulun, el segundo gobernante de Egipto tuluní (r. 884-896), conocido por su extravagancia y despilfarro según las crónicas de la época, construyó un recipiente lleno de mercurio, en el que se tendía sobre la parte superior de cojines llenos de aire y se balanceaba para dormir.

En noviembre de 2014, se descubrieron "grandes cantidades" de mercurio en una cámara de 18,2 m debajo del templo de 1800 años de antigüedad conocido como la "Pirámide de la Serpiente Emplumada", "la tercera pirámide más grande de Teotihuacán", México, junto con "estatuas de jade, jaguares vigilantes, una caja llena de conchas talladas y pelotas de goma".

En la Antigua Grecia se usaba el mercurio en ungüentos; las egipcias y romanas lo utilizaban en cosméticos. En Lamanai, una ciudad importante de la civilización maya, se encontró una balsa de mercurio bajo un marcador en una pista de juego de pelota. Hacia el año 500 el mercurio se utilizaba para hacer amalgamas (del latín medieval "amalgama", "aleación de mercurio") con otros metales.

Los alquimistas pensaron en el mercurio como la materia prima, a partir de la cual se formaron todos los metales. Creían que diferentes metales podrían ser producidos haciendo variar la calidad y cantidad de azufre contenido dentro del mercurio. El más puro de estos era el oro, y el mercurio se usaba en los intentos de transmutación de los metales de base (o impuros) en oro, que era el objetivo de muchos alquimistas.

Las minas de Almadén (España), Monte Amiata (Italia) e Idrija (ahora Eslovenia) dominaron la producción de mercurio a partir de la apertura de la mina de Almadén hace 2500 años, hasta que aparecieron nuevos depósitos al final del siglo XIX.

El mercurio es un elemento extremadamente raro en la corteza terrestre, que tiene una abundancia media en peso de tan solo 0,08 partes por millón. Debido a que no se mezcla geoquímicamente con aquellos elementos que constituyen la mayoría de la masa de la corteza terrestre, los minerales de mercurio se encuentran extraordinariamente concentrados teniendo en cuenta la abundancia del elemento en la roca ordinaria. Los minerales más ricos de mercurio contienen hasta un 2,5% de mercurio en peso, e incluso los depósitos de concentrados más pobres contienen al menos el 0,1% de mercurio (12.000 veces la abundancia media en la corteza terrestre). Se encuentra ya sea como metal nativo (raro) o en forma de cinabrio, corderoíta, livingstonita y otros minerales, con el cinabrio (HgS) siendo la mena más abundante. Los yacimientos de mercurio se hallan por lo general en zonas de orogénesis reciente, donde las rocas de alta densidad se ven obligadas a surgir a la corteza de la Tierra, impulsadas por aguas termales o por la actividad de determinadas regiones volcánicas.
A partir de 1558, con la invención del proceso de patio para extraer la plata a partir de sus menas usando mercurio, este metal se convirtió en un recurso esencial en la economía de España y de sus colonias americanas. El mercurio se utilizó para extraer plata en las lucrativas minas de Nueva España y Perú. Inicialmente, las minas de la Corona Española en Almadén (localizadas en el sur del centro de España) suministraban todo el mercurio necesario en las colonias, hasta que fueron descubiertos nuevos yacimientos en el Nuevo Mundo, y más de 100.000 toneladas de mercurio fueron extraídos de la región de Huancavelica, Perú (especialmente de la mina Santa Bárbara), a lo largo de los tres siglos posteriores al descubrimiento de sus yacimientos en 1563. El proceso de patio primero y después el de pan amalgamación crearon una gran demanda de mercurio para el tratamiento de minerales de plata hasta finales del siglo XIX.

Antiguas minas en Italia, Estados Unidos y México, que una vez produjeron una gran proporción de la oferta mundial, han sido completamente agotadas o, en el caso de Eslovenia (Idrija) y de España (Almadén), debieron cerrar debido a la caída del precio del mercurio. La mina de McDermitt en el estado de Nevada, la última explotación de mercurio en los Estados Unidos, se cerró en 1992. El precio del mercurio ha sido muy volátil en los últimos años y en 2006 era de $650 por cada vasija de 76 libras (34,46 kg).

El mercurio se extrae por calentamiento del cinabrio en una corriente de aire y condensando el vapor. La ecuación para esta extracción es:

En 2005, China fue el principal productor de mercurio con casi dos tercios de la cuota mundial, seguida del Kirguistán. Se cree que otros países mantienen una producción no registrada de mercurio derivada de procesos de electrodeposición del cobre y por la recuperación de los efluentes.

Debido a la alta toxicidad del mercurio, tanto la extracción del cinabrio como el refinado del mercurio son causas peligrosas e históricas de envenenamiento. En China, el trabajo penitenciario fue utilizado por una empresa minera privada, en épocas tan recientes como la década de 1950, para explotar nuevas minas de cinabrio. Miles de prisioneros fueron utilizados por la empresa minera Luo Xi para excavar nuevas galerías. La salud de los mineros de las minas en explotación corre un alto riesgo.

La directiva de la Unión Europea disponiendo el uso obligatorio de lámparas fluorescentes compactas a partir del año 2012 ha alentado a China a reabrir sus minas de cinabrio para obtener el mercurio necesario para la fabricación de este tipo de bombillas. Los peligros ambientales han sido una preocupación, en particular en las ciudades sureñas de Foshan y Cantón, y en la provincia Guizhou, situada en el sudoeste del país.

Las plantas de procesamiento de las minas de mercurio abandonadas a menudo contienen acumulaciones de desechos muy peligrosas de cinabrio calcinado. El agua de escorrentía en estos lugares es una fuente reconocida de daños ecológicos. Antiguas minas de mercurio pueden ser adecuadas para la reutilización constructiva. Por ejemplo, en 1976 el Condado de Santa Clara (California) compró la histórica Mina "Almaden Quicksilver" y creó un parque del Condado, después de realizar un exhaustivo estudio de seguridad y un análisis ambiental de la propiedad.

La producción mundial de mercurio experimentó históricamente un crecimiento continuado (primero ligado principalmente a la minería del oro y de la plata en el Nuevo Mundo, y a partir de comienzos del siglo XX también relacionado con la producción industrial de cloro), con un progresivo descenso a partir de la década de 1980, cuando se empezaron a hacer patentes los riesgos ambientales que entraña su utilización indiscriminada. Por ejemplo, en la década de 1970 se estimaba que las minas de Almadén habían producido unas 200.000 tm de mercurio a lo largo de toda su vida útil, y que todavía albergaban otras 200.000 tm en su interior. Desde el final de la Segunda Guerra Mundial hasta la década de 1970, la producción mundial anual pasó de las 3.200 tm en 1948 a las 8.650 tm en 1965, estabilizándose durante una década en las 9.000-10.000 tm anuales.

A partir de la década de 1990, tanto por motivos económicos (el descenso del precio del metal obligó a cerrar muchas de las principales minas de los países occidentales, que ya no eran rentables), como ambientales (la producción se concentró en los países con menos restricciones legales en relación con el medio ambiente), China ha copado el mercado mundial, con más del 80% de la producción total en los primeros años del siglo XXI.

La producción mundial en el año 2013 fue del orden de 1.900 toneladas (prácticamente la quinta parte de su máximo histórico, registrado como ya se ha señalado en la década de 1970), con China en un destacado primer lugar (véase la tabla adjunta), siendo Kirguistán y Chile segundo y tercer productores, con porcentajes mucho menores.

El mercurio existe en dos estados de oxidación principales, I y II. Los estados de oxidación más altos son poco frecuentes (por ejemplo, el fluoruro de mercurio (IV), ), y solo se han detectado bajo condiciones extraordinarias.

A diferencia de sus vecinos más ligeros, cadmio y zinc, el mercurio suele formar compuestos estables con simples enlaces metal-metal. La mayoría de los compuestos de mercurio (I) son diamagnéticos y cuentan con el catión dimérico, Hg. Los derivados estables incluyen el cloruro y el nitrato. El tratamiento de los compuestos complejos de Hg(I) con ligandos fuertes tales como sulfuro o cianuro, induce una desproporción a y la formación de mercurio elemental. El cloruro de mercurio (I), un sólido incoloro también conocido como "calomel", es realmente el compuesto con la fórmula HgCl, con la estructura Cl-Hg-Hg-Cl, un estándar en electroquímica que reacciona con el cloro para dar cloruro mercúrico HgCl, que se opone a la oxidación adicional. El hidruro de mercurio (I), un gas incoloro, tiene la fórmula HgH, que no contiene ningún enlace Hg-Hg.

Indicativas de su tendencia a adherirse a sí mismas son las formas de policationes de mercurio, que consisten en cadenas lineales con centros de mercurio, rematadas con una carga positiva. Un ejemplo es el .

El mercurio (II) es el estado de oxidación más común y por lo tanto el más frecuente en la naturaleza. Se conocen los cuatro haluros de mercurio, que forman complejos tetraédricos con otros ligandos, pero los haluros adoptan geometría de coordinación lineal, algo así como sucede con Ag. El más conocido es el cloruro de mercurio (II), una sustancia sólida de color blanco, fácilmente sublimable. HgCl forma complejos que son típicamente tetraédricos, por ejemplo el .

El óxido de mercurio (II), el óxido principal del mercurio, se forma cuando el metal está expuesto al aire durante largos períodos de tiempo a temperaturas elevadas. Los elementos se separan de nuevo si el óxido se calienta a cerca de 400 °C, como demostró Joseph Priestley en una de las primeras síntesis de oxígeno puro. Los hidróxidos de mercurio están mal caracterizados, como sucede con sus elementos vecinos oro y plata.

Siendo un metal "blando" a efectos de pH, los derivados de mercurio forman combinaciones muy estables con los calcógenos más pesados. La forma más abundante es el sulfuro de mercurio (II), HgS, que se produce en la naturaleza como el mineral cinabrio, utilizado como pigmento rojo brillante con el nombre de bermellón. Como el ZnS, el HgS cristaliza en dos formas, la forma rojiza cúbica y la negra con una configuración similar a la de la blenda. El seleniuro de mercurio (II) (HgSe) y el telururo de mercurio (II) (HgTe) son también conocidos, así como diversos derivados, como por ejemplo el telururo de mercurio y cadmio y el telururo de mercurio y zinc, que son semiconductores útiles como materiales detectores de infrarrojos.

Las sales de mercurio (II) forman una variedad de compuestos derivados del amoníaco. Estos incluyen la "base de Millon" (HgN), un polímero unidimensional (sales de ()), y un "precipitado blanco fusible" (el [Hg(NH)]Cl). Conocido como reactivo de Nessler, el tetraiodomercurato (II) de potasio () sigue siendo en ocasiones utilizado para la prueba del amoníaco, debido a su tendencia a formar la sal yoduro de la "base de Millon", de intenso color.

El fulminato de mercurio (II) es un detonator ampliamente utilizado en explosivos.

Estados de oxidación superiores a 2 en una especie no cargada son extremadamente raros, aunque un catión cíclico de mercurio (IV), con tres sustituyentes, puede ser un intermediario en reacciones de oximercuración. En 2007 se publicó un informe en el que se daba cuenta de la síntesis de un compuesto de mercurio (IV), el fluoruro de mercurio (IV). En la década de 1970 hubo una reclamación sobre la síntesis de un compuesto de mercurio (III), pero ahora se cree que es falsa.

Los compuestos orgánicos de mercurio son históricamente importantes en el desarrollo de la química, pero en el mundo occidental son de poco valor industrial. Las sales de mercurio (II) son un raro ejemplo de complejos metálicos simples que reaccionan directamente con los anillos aromáticos. Los compuestos organomercúricos son siempre divalentes y por lo general bidimensionales y de geometría lineal. A diferencia de los compuestos organocádmicos y los organozincados, los compuestos organomercuriales no reaccionan con agua. Por lo general tienen la fórmula HgR, que son a menudo volátiles, o HgRX, que a menudo son sólidos, donde R es arilo o alquilo y X es generalmente haluro o acetato. El metilmercurio, un término genérico para los compuestos con la fórmula CHHgX, forma una familia de compuestos peligrosos que se encuentran a menudo en el agua contaminada. Surgen por un proceso conocido como biometilación.

El mercurio (II) forma complejos con ligandos dadores de nitrógeno, fósforo y azufre, pero se resiste a formar complejos con los dadores de oxígeno; también genera complejos muy estables con bromo, iodo y cloro como corresponde a un catión blando. La estabilidad de los complejos de mercurio (II) es mayor que la de los otros dos elementos de su grupo, cinc y cadmio, porque además de enlaces σ con hibridaciones adecuadas del metal intervendrán enlaces π por la mayor expansión de los 5d del mercurio (efectos relativistas), que inyectan carga a los orbitales d vacíos de los ligandos: se creará un sistema resonante que es compatible con la asociación cuántica del subnivel lleno 5d, reforzando a la vez los enlaces M-L por retrodonación. Esto es inusual, puesto que los iones más pequeños forman normalmente los mejores complejos. No se conocen complejos con ligandos π, como CO, NO o alquenos.
Los complejos de cinc son incoloros, pero los de mercurio y en menor extensión los de cadmio, son coloreados debido a la transferencia de carga del metal al ligando (absorciones de transferencia de carga), y del ligando al metal que es más patente en el mercurio de acuerdo a lo indicado antes (expansión 5d>4d).

La mayoría de los complejos de Hg (II) son octaédricos distorsionados, con dos enlaces cortos y cuatro enlaces largos. El caso extremo de esta distorsión es la formación de solo 2 enlaces, ejemplo de esto son los compuestos Hg(CN) y Hg(SCN), y el complejo [Hg(NH)]Cl; este último contiene el ion lineal [HN-Hg-NH]. El mercurio (II) también forma complejos tetraédricos como [Hg(SCN)] y el K[HgI]. Este último es el denominado reactivo de Nessler para la determinación de amoníaco en disolución; se detectan concentraciones tan bajas como 1ppm y se forma un precipitado amarillo o marrón, [HgNI.HO] (unidades {HgN} que dan entorno tetraédrico de Hg para el N y lineal para el Hg (II), catión polimérico con estructura 3D de tipo cuprita, CuO, o bien anti-β-cristobalita.

Otros ejemplos de complejos de mercurio (II) donde se pueden apreciar diferentes entornos de coordinación, son:

El mercurio se utiliza principalmente para la fabricación de productos químicos industriales o para aplicaciones eléctricas y electrónicas. Se emplea en algunos termómetros, especialmente los que se usan para medir temperaturas elevadas. Una cantidad cada vez mayor se usa como mercurio gaseoso en lámparas fluorescentes, mientras que la mayoría de las otras aplicaciones se están eliminando lentamente debido a las regulaciones de salud y seguridad, siendo reemplazado en algunas aplicaciones por materiales menos tóxicos, pero considerablemente más caros, como la aleación Galinstano.

El mercurio y sus compuestos se han utilizado en medicina, aunque son mucho menos comunes en la actualidad de lo que lo eran antes, debido a que los efectos tóxicos del mercurio y de sus compuestos son mejor conocidos. La primera edición del "Manual de Merck", en 1899, incluía muchos de los siguientes compuestos de mercurio como medicamentos:

El mercurio es un ingrediente en amalgamas dentales. El "Thiomersal" (denominado "tiomersal", en los Estados Unidos) es un compuesto orgánico utilizado como conservante en vacunas, aunque este uso está en declive. El tiomersal se metaboliza en etilmercurio. Aunque se ha discutido ampliamente acerca de la seguridad del tiomersal ("véase: Controversia del tiomersal") y se sugiere que este conservante a base de mercurio podría provocar o desencadenar autismo en los niños, los estudios científicos no han mostrado evidencias que apoyen estas afirmaciones. Sin embargo, el tiomersal ha sido retirado o reducido a pequeñas cantidades en todas las vacunas recomendadas para los niños de Estados Unidos hasta los 6 años de edad, con la excepción de la vacuna inactivada de la gripe.

Otro compuesto de mercurio, la merbromina (mercurocromo), es un antiséptico tópico utilizado para pequeños cortes y raspaduras que todavía está en uso en algunos países.

El mercurio en la forma de uno de sus minerales más comunes, el cinabrio, se utiliza en diversas medicinas antiguas y tradicionales, especialmente en la medicina china tradicional (véase también ). Las revisiones realizadas acerca de su seguridad han encontrado que el cinabrio puede conducir al envenenamiento por mercurio significativo cuando se calienta, se consume en sobredosis, o tomado a largo plazo, y puede tener efectos adversos en dosis terapéuticas, aunque los efectos de las dosis terapéuticas suelen ser reversibles. Aunque esta forma de mercurio parece ser menos tóxica que otras formas, su uso en la medicina tradicional china aún no ha sido justificado, y la base terapéutica para su uso no está clara.

Hoy en día, el uso de mercurio en medicina ha disminuido considerablemente en todos los aspectos, especialmente en los países desarrollados. Los termómetros y los esfigmomanómetros que contienen mercurio se inventaron a principios del siglo XVIII y finales del XIX respectivamente. A principios del siglo XXI, su uso está disminuyendo y ha sido prohibido en algunos países por los propios estados y sus instituciones médicas. En 2002, el Senado de los Estados Unidos aprobó una legislación para eliminar gradualmente la venta de termómetros de mercurio. En 2003, los estados de Washington y de Maine se convirtieron en los primeros en prohibir los aparatos medidores de la presión arterial que utilizasen mercurio. Compuestos de mercurio todavía se pueden encontrar en algunos medicamentos de venta libre, incluyendo antisépticos tópicos, laxantes, pomadas para la dermatitis por pañal, colirios, y en aerosoles nasales. La FDA (Food and Drug Administration) señala que ""sus datos son insuficientes para establecer el reconocimiento general de la seguridad y la eficacia"" de los ingredientes de mercurio en estos productos. El mercurio se sigue utilizando en algunos diuréticos, aunque ya existen sustitutos para la mayoría de usos terapéuticos.

El cloro se produce a partir del cloruro sódico (sal común, NaCl) utilizando electrólisis para separar el sodio metálico del gas cloro. Por lo general, la sal se disuelve en agua para producir una salmuera. Los subproductos de dicho proceso cloro-álcali son hidrógeno (H) e hidróxido de sodio (NaOH), lo que comúnmente se conoce como sosa cáustica. Con mucho, el mayor uso de mercurio a finales del siglo XX era en el proceso de celdas de mercurio (también denominado proceso Castner-Kellner), en el que se forma el sodio metálico como una amalgama en un cátodo hecho de mercurio. Este sodio se hace reaccionar con el agua para producir hidróxido de sodio. Muchas de las emisiones de mercurio industriales del siglo XX proceden de este proceso, aunque las plantas modernas afirmaban ser seguras en este aspecto. Después de alrededor de 1985, todas las nuevas instalaciones de producción de cloro-álcali que fueron construidas en los Estados Unidos utilizan tecnologías de ósmosis para producir cloro.

Algunos termómetros, especialmente los de altas temperaturas, contienen mercurio; aunque están desapareciendo gradualmente. En los Estados Unidos, la venta sin receta de los termómetros de mercurio está prohibida desde el año 2003.

El mercurio también se utiliza en los telescopios de espejo líquido.

Algunos telescopios de tránsito utilizan un recipiente con mercurio para formar un espejo plano y absolutamente horizontal, útil en la determinación de una referencia vertical o perpendicular absoluta. Espejos parabólicos horizontales cóncavos pueden formarse mediante la rotación de mercurio líquido en un recipiente cilíndrico: el líquido adopta de este modo forma parabólica, permitiendo la reflexión y el enfoque de la luz incidente. Estos telescopios son más baratos que los grandes telescopios de espejos convencionales hasta en un factor de 100, pero el espejo de mercurio líquido no se puede inclinar y siempre debe señalar hacia la vertical del lugar.

El mercurio líquido es una parte del popular electrodo de referencia secundaria (denominado electrodo de calomel) en electroquímica, como una alternativa al electrodo estándar de hidrógeno. El electrodo de calomelanos se utiliza para calcular el potencial del electrodo de las semiceldas. Por último, pero no menos importante, el punto triple del mercurio, -38.8344 °C, es un punto fijo utilizado como un estándar de temperatura para la Escala Internacional de Temperatura (ITS-90).

Los electrodos empleados en polarografía utilizan mercurio elemental. Este uso permite que un nuevo electrodo no contaminado esté disponible para cada medición o para cada nuevo experimento.

El mercurio gaseoso se utiliza en lámparas de vapor de mercurio, y lámparas fluorescentes y en algunos reclamos publicitarios del tipo "letrero de neón". Estas lámparas de baja presión emiten luz con líneas espectralmente muy estrechas, que se utilizan tradicionalmente en espectroscopia para la calibración de las posiciones espectrales. Se venden lámparas comerciales de calibración para este fin; analizar la luz de un fluorescente de techo en un espectrómetro es una práctica de calibración frecuente. El mercurio gaseoso también se encuentra en algunos tubos electrónicos, incluyendo ignitrones, tiratrones, y rectificadores de arco de mercurio. También se utiliza en las lámparas de atención médica especializada para el bronceado de la piel y desinfección. Se añade mercurio gaseoso a las lámparas de cátodo frío que contienen argón para aumentar la ionización y la conductividad eléctrica. Una lámpara rellenada de argón sin mercurio presentará manchas mates y dejará de iluminar correctamente. Los sistemas de iluminación que contienen mercurio pueden ser tratados térmicamente una sola vez. Cuando se añade vapor de mercurio a tubos llenos de neon, la luz producida presentará manchas rojas/azules inconsistentes, hasta que se complete el proceso térmico inicial; finalmente, se encenderá un solo color, mostrando finalmente un color azul apagado coherente.

El mercurio, en forma de tiomersal, es ampliamente utilizado en la fabricación de rímel. En 2008, Minnesota se convirtió en el primer estado en los Estados Unidos en prohibir el mercurio añadido intencionadamente en los cosméticos, lo que supone una norma más dura que la del gobierno federal.

Un estudio de la media geométrica de la concentración de mercurio en la orina, identificó una fuente no reconocida previamente de la exposición al mercurio inorgánico entre los residentes de Nueva York: los productos para el cuidado de la piel. Estudios basados en la biomonitorización de la población también mostraron que los niveles de concentración de mercurio son más altos entre los consumidores de pescado y marisco.

Un compuesto de mercurio llamado "fulminato de mercurio" se utilizaba principalmente en las cápsulas fulminantes como detonador de la carga de pólvora de los cartuchos que sirven de munición a las armas de fuego.

Muchas aplicaciones históricas hacen uso de las peculiares propiedades físicas del mercurio, sobre todo como un líquido denso y como un metal líquido:



Otras aplicaciones hacen uso de las propiedades químicas del mercurio:

El cloruro de mercurio (I) (también conocido como calomel o cloruro de mercurio) se ha utilizado en la medicina tradicional como diurético, desinfectante tópico, y laxante. El cloruro de mercurio (II) (también conocido como cloruro de mercurio o sublimado corrosivo) en tiempos se utilizó para tratar la sífilis (junto con otros compuestos de mercurio), aunque es tan tóxico que a veces los síntomas de su toxicidad se confunden con los de la sífilis que se creía tratar. También se utiliza como desinfectante. El ""Blue mass"", una pastilla o jarabe en el que el mercurio es el ingrediente principal, se recetó a lo largo del siglo XIX para numerosas enfermedades, como el estreñimiento, la depresión, la infertilidad y los dolores de cabeza. A principios del siglo XX, el mercurio se administró a niños pequeños como laxante y vermífugo, y se utilizó en polvo dental para lactantes. La merbromina, un organohaluro que contiene mercurio (a veces se vende como mercurocromo) sigue siendo ampliamente utilizado, pero ha sido prohibido en algunos países como los Estados Unidos.

El mercurio y la mayoría de sus compuestos son extremadamente tóxicos y deben ser manejados con cuidado; en los casos de derrames relacionados con el mercurio (por ejemplo, en el caso de rotura de termómetros o de tubos fluorescentes que contengan el metal o sus vapores), existen procedimientos de limpieza específicos para evitar la exposición y evitar su dispersión. Protocolos para fusionar físicamente las gotas más pequeñas depositadas sobre superficies duras para poder recogerlas con un cuentagotas, o bien para empujar suavemente el derrame hacia un recipiente desechable. Aspiradoras y escobas causan una mayor dispersión del mercurio y no deben utilizarse. Posteriormente se esparcen sobre el área afectada por el derrame escamas de azufre, zinc, o algún otro material en polvo que forme fácilmente una amalgama (aleación) con el mercurio a temperaturas ordinarias, antes de ser recogidos y depositados adecuadamente. La limpieza de superficies porosas y prendas de vestir no es eficaz para eliminar todos los rastros de mercurio y, por lo tanto, se aconseja a desechar este tipo de artículos cuando han estado expuestos a un derrame de mercurio.

El mercurio puede ser absorbido por la piel y las membranas mucosas y los vapores de mercurio puede ser inhalados accidentalmente, por lo que los contenedores de mercurio deben estar bien sellados para evitar derrames o evaporación. El calentamiento del mercurio o de sus compuestos, que pueden liberarlo cuando se calientan, debe llevarse a cabo con una ventilación adecuada a fin de minimizar la exposición al vapor de mercurio. Las formas más tóxicas de mercurio son sus compuestos orgánicos, como el dimetilmercurio y el metilmercurio. El mercurio puede causar tanto intoxicaciones crónicas como agudas, incluyendo el envenenamiento por mercurio.

La exposición crónica afecta principalmente al sistema nervioso central y a los riñones. La nefrotoxicidad se debe a la alta afinidad entre los iones mercúricos y los grupos sulfhidrilos (-SH) reducidos, los conjugados mercúricos con albúmina, L-cisteína, homocisteína y glutatión son las formas biológicamente importantes de Hg en circulación.

Tanto las formas orgánicas como inorgánicas del mercurio se captan, acumulan en la corteza renal, en el exterior de la médula externa, principalmente a lo largo de los tres segmentos del túbulo proximal, expresando así su toxicidad a nivel renal. Siendo las especies inorgánicas, las que poseen mayor relevancia nefrotóxica, por el contrario en el caso de las especies orgánicas se necesitan elevadas dosis y múltiples exposiciones para producir insuficiencia renal.

La parte más sensible de la nefrona a los efectos tóxicos ocasionados por estos compuestos es el túbulo proximal, en concreto el segmento S3.

La nefrotoxicidad originada por dicho metal depende del tiempo de exposición, si la exposición es breve se produce una necrosis tubular aguda, sin embargo, si la exposición es a largo plazo, se produce glomerulonefritis.

Los índices de depósito pre-industriales de mercurio de la atmósfera pueden ser de aproximadamente 4 ng/(1 l de depósito de hielo). A pesar de que puede ser considerado un nivel natural de la exposición, las fuentes regionales o globales tienen efectos significativos. Las erupciones volcánicas pueden aumentar el nivel atmosférico entre 4 y 6 veces.

Las fuentes naturales, tales como los volcanes, son responsables de aproximadamente la mitad de las emisiones de mercurio a la atmósfera. La contaminación provocada por la actividad humana se puede dividir en los siguientes porcentajes estimados:
Los porcentajes anteriores son estimaciones de las emisiones de mercurio de origen humano a nivel mundial en el año 2000, con exclusión de la quema de biomasa, una fuente importante en algunas regiones.

La contaminación atmosférica reciente por mercurio en ambientes urbanos al aire libre se midió con valores de entre 0,01-0,02 mg/m. En 2001 se midieron y estudiaron los niveles de mercurio en 12 lugares del interior de viviendas elegidos para representar una sección transversal de las clases de construcción, la ubicación y las edades de los edificios en el área de Nueva York. Este estudio encontró concentraciones elevadas de mercurio en el interior de las viviendas significativamente más elevados que los registrados al aire libre, en un rango de entre 0,0065 y 0,523 mg/m. El promedio fue de 0,069 g/m.

El mercurio también entra en el medio ambiente a través de su eliminación inadecuada (por ejemplo, en los vertederos y en las incineradoras) de determinados productos que contienen mercurio, como: piezas de automóviles, baterías y pilas, bombillas fluorescentes, productos médicos, termómetros y termostatos. Debido a problemas de salud (véase más adelante), se está reduciendo progresivamente o eliminando el mercurio en estos productos. Por ejemplo, la cantidad de mercurio contenido en los termostatos vendidos en los Estados Unidos se redujo de 14,5 toneladas en 2004 a 3,9 toneladas en 2007.

La mayoría de los termómetros utilizan ahora alcohol tintado en lugar de mercurio, y los termómetros de la aleación galinstano son también una opción disponible. Los termómetros de mercurio se utilizan todavía de vez en cuando en el campo de la medicina, ya que son más precisos que los termómetros de alcohol, aunque frecuentemente, ambos están siendo reemplazados por los termómetros electrónicos y menos comúnmente por los ya citados termómetros de galinstano. Los termómetros de mercurio siguen siendo ampliamente utilizados para ciertas aplicaciones científicas debido a su mayor precisión y rango de trabajo.

Históricamente, una de las mayores emisiones se produjo en la planta industrial de Colex, una instalación dedicada a la separación de isótopos de litio situada en Oak Ridge, Tennessee. La planta operó en las décadas de 1950 y 1960. Los registros son incompletos y poco claro, pero las comisiones gubernamentales han estimado que se desconoce el paradero de unas novecientas toneladas de mercurio.

Un desastre industrial grave fue el vertido de compuestos de mercurio a la bahía de Minamata, en Japón. Se estima que más de 3.000 personas sufrieron varias deformidades severas, síntomas de intoxicación por mercurio o la muerte, en lo que se conoce como enfermedad de Minamata debido al envenenamiento por mercurio.

Más recientemente, en varias comunidades del estado de Querétaro, México, se ha descubierto la presencia de mercurio en alimentos de origen animal, vegetal y en el agua, y los niveles de contaminación por este elemento "exceden hasta en mil por ciento el máximo permitido, lo que implica graves riesgos para la salud".


Las emisiones de mercurio a la atmósfera se distribuyen globalmente y contaminan todos los ecosistemas. Como ya se ha señalado, el mercurio procede de actividades humanas (combustión del carbón, minería directa de mercurio, plata y oro) y actividades naturales (vulcanismo, por ejemplo). Las emisiones producen mayoritariamente Hg, con menor cantidad de Hg. El mercurio depositado puede ser re-emitido a la atmósfera mediante su intercambio entre el océano y el aire o la combustión de biomasa.

El mercurio almacenado en el hielo del monte Logan (5340 metros sobre el nivel del mar; Yukon, Canadá) desde el año 1400 hasta 1998 ha sido medido con precisión. La mayoría de la acumulación de mercurio de origen antropogénico durante 600 años se produjo en el Monte Logan durante el siglo XX y especialmente entre 1940 y 1975. El incremento entre 1993 y 1998 (final del muestreo) puede reflejar el aumento de emisiones a la atmósfera por la combustión de carbón en Asia y la minería a pequeña escala de los países en desarrollo, que se ha estimado que continúa hasta la actualidad. La recolecta y estudio de nuevas muestras de hielo es urgente debido a la desaparición acelerada de los glaciares.

Debido a los efectos sobre la salud de la exposición al mercurio, sus usos industriales y comerciales son regulados en muchos países. La Organización Mundial de la Salud, la OSHA, y la NIOSH tratan al mercurio como un riesgo laboral, y se han establecido límites de exposición laboral específicos. Las emisiones y la eliminación del mercurio ambiental están regulados en los EE. UU. principalmente por la Agencia de Protección Ambiental.

Estudios epidemiológicos han constatado numerosos efectos nocivos del mercurio, como temblores, deterioro de habilidades cognitivas, y alteraciones del sueño en trabajadores con exposición crónica al vapor de mercurio, incluso a bajas concentraciones (en el rango de 0,7 a 42 mg/m. Un estudio ha demostrado que la exposición puntual (4-8 horas) a niveles de mercurio elemental calculados entre 1,1 y 44 mg/m dio lugar a dolor en el pecho, disnea, tos, hemoptisis, deterioro de la función pulmonar, y la evidencia de neumonitis. La exposición aguda intersticial al vapor de mercurio se ha demostrado que produce profundos efectos sobre el sistema nervioso central, incluyendo reacciones psicóticas caracterizadas por el delirio, alucinaciones y tendencia suicida. La exposición ocupacional se ha plasmado en un amplio alcance de perturbaciones funcionales, incluyendo eretismo, irritabilidad, nerviosismo, timidez excesiva, e insomnio. Con la exposición continuada, se desarrolla un ligero temblor, que puede transformarse en espasmos musculares violentos. El temblor inicialmente involucra a las manos, y luego se extiende a los párpados, los labios y la lengua. A largo plazo, la exposición de bajo nivel se ha asociado con síntomas más sutiles de eretismo, incluyendo fatiga, irritabilidad, pérdida de memoria, sueños vívidos y depresión.

Los efectos nocivos del mercurio pueden ser transmitidos de la madre al feto, e incluyen daño cerebral, retraso mental, falta de coordinación, ceguera, convulsiones e incapacidad para hablar. Los niños con envenenamiento por mercurio pueden desarrollar problemas en sus sistemas nervioso y digestivo y daños renales.

La investigación sobre el tratamiento de la intoxicación y el envenenamiento por mercurio es limitada. En la actualidad los fármacos disponibles para tratar la intoxicación mercurial aguda incluyen quelantes de N-acetil-D, L-penicilamina (PAN), Dimercaprol, ácido 2,3-dimercapto-1-propanosulfónico (DMPS), y ácido dimercaptosuccínico (DMSA). En un pequeño estudio incluyendo a 11 trabajadores de la construcción expuestos al mercurio elemental, los pacientes fueron tratados con DMSA y NAP. La terapia de quelación con ambos fármacos tuvo como resultado la movilización de una pequeña fracción del mercurio total corporal estimado. El DMSA fue capaz de aumentar la excreción de mercurio en un grado mayor que el NAP.

El pescado y el marisco tienen una tendencia natural a concentrar mercurio en sus cuerpos, a menudo en forma de metilmercurio, un compuesto orgánico altamente tóxico. Las especies de peces que forman parte de los niveles superiores de la cadena alimentaria, como tiburones, peces espada, caballas, atunes o albacoras contienen mayores concentraciones de mercurio que otros. Como el mercurio y el metilmercurio son solubles en grasa, se acumulan principalmente en las vísceras, aunque también se depositan en todo el tejido muscular. Cuando un pez es consumido por un depredador, el nivel de mercurio se acumula. Dado que los peces son poco eficientes en la depuración de la acumulación de metilmercurio, las concentraciones en sus tejidos aumentan con el tiempo. Por lo tanto, las especies que están más altas en la cadena trófica acumulan una carga corporal de mercurio que puede ser diez veces más alta que la de las especies que consumen. Este proceso se llama biomagnificación. Este tipo de envenenamiento por mercurio se produjo de esta manera en Minamata, Japón, dando lugar a la denominada enfermedad de Minamata.

Se transporta en estado líquido, código europeo del Acuerdo ADR: [2809-80-8-8,Â66° c)]. Los contenedores deben cerrarse herméticamente. Se pueden emplear contenedores de acero, acero inoxidable, hierro, plásticos, vidrio o porcelana. Deben evitarse los contenedores de plomo, aluminio, cobre, estaño y cinc.

Almacenar en áreas frías, secas, bien ventiladas, alejadas de la radiación solar y de fuentes de calor y/o ignición, ya que a temperaturas mayores de 40 °C produce vapor. Debe estar alejado de ácido nítrico concentrado, acetileno y cloro. Debe almacenarse en recipientes irrompibles de materiales resistentes a la corrosión y que sean compatibles.

El mercurio puede amalgamarse accidentalmente con metales nobles como el oro, produciendo manchas sobre su superficie. Dado que el mercurio se evapora a unos 360 °C (de hecho, debe ser almacenado a una temperatura que no sobrepase los 40 °C para evitar la emanación de vapores), es posible eliminar una mancha (por ejemplo, de alguna joya) colocándola en la llama de un mechero y después puliéndola. Si la mancha es muy grande puede introducirse la joya en ácido nítrico concentrado o ácido sulfúrico concentrado (la joya debe ser de oro o platino, de lo contrario se disolverá). Los ácidos reaccionan con el mercurio, por lo que debe tenerse en cuenta que estas reacciones son exotérmicas y liberan vapores tóxicos.

De acuerdo con la legislación de la Unión Europea en el etiquetado deben incorporarse las : R 23 ("Tóxico por inhalación") y R 33 ("Peligro de efectos acumulativos"). También deben incorporarse las : S 1/2 ("Consérvese bajo llave y manténgase fuera del alcance de los niños"), S 7 ("Manténgase el recipiente bien cerrado") y S 45 ("En caso de accidente o malestar, acuda inmediatamente al médico (si es posible, muéstrele la etiqueta)").

Un total de 140 países acordaron en la el Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) con el objeto de evitar emisiones peligrosas.

En la Unión Europea, la directiva sobre la restricción del uso de ciertas sustancias peligrosas en aparatos eléctricos y electrónicos (véase RoHS) prohíbe el mercurio de ciertos productos eléctricos y electrónicos, y limita la cantidad de mercurio en otros productos a menos de 1000 ppm. También se han impuesto restricciones para la concentración de mercurio en los envases (el límite es de 100 ppm para suma de mercurio, plomo, cromo hexavalente y cadmio) y en las baterías (el límite es de 5 ppm). En julio de 2007, la Unión Europea prohibió también el mercurio en dispositivos de medición no eléctricos, tales como termómetros y barómetros. La prohibición sólo se aplica a nuevos dispositivos, y contiene excepciones para el sector de la atención sanitaria y un período de gracia de dos años para los fabricantes de barómetros.
Noruega promulgó una prohibición total del uso de mercurio en la fabricación e importación/exportación de productos de mercurio, el 1 de enero de 2008. En 2002, se constató que varios lagos en Noruega presentaban un mal estado debido a la contaminación por mercurio, con un exceso de 1 µg/g de mercurio en sus sedimentos.
En 2008, el Ministro de Desarrollo para el Medio Ambiente de Noruega, Erik Solheim, manifestó que: ""El mercurio es una de las toxinas ambientales más peligrosas. Alternativas satisfactorias al mercurio en los productos ya están disponibles, por lo que es apropiado introducir una prohibición"".

Los productos que contienen mercurio fueron prohibidos en Suecia en 2009.

En 2008, Dinamarca también prohibió la amalgama de mercurio dental, excepto para el relleno de la superficie de masticación de dientes permanentes, como los molares de adultos.

En los Estados Unidos, la Agencia de Protección Ambiental (EPA) se encarga de regular y gestionar la contaminación por mercurio. Varias leyes confieren a la EPA esta autoridad. Además, en la normativa recogida en el ""Mercury-Containing and Rechargeable Battery Management Act"", aprobada en 1996, se retira paulatinamente el uso del mercurio en las pilas, y se prevé la eliminación eficiente y rentable de los muchos tipos de baterías usadas. Los países de América del Norte contribuyeron aproximadamente con el 11% del total de las emisiones globales antropogénicas de mercurio en 1995.

La ""Clean Air Act"" (1990), aprobada en 1990, puso al mercurio en una lista de contaminantes tóxicos que necesitan ser controlados en la mayor medida posible. Por lo tanto, las industrias que liberan altas concentraciones de mercurio al medio ambiente han acordado en instalar el máximo alcanzable de las tecnologías de control (MACT). En marzo de 2005, la EPA promulgó una regulación que añadió las centrales eléctricas a la lista de fuentes que deben ser controladas e instituyó un sistema de "Comercio de derechos de emisión" nacional. Se dio de plazo hasta noviembre de 2006 para imponer controles más estrictos, pero después del desafío legal de varios estados, las regulaciones fueron derogadas por un tribunal federal de apelaciones el 8 de febrero de 2008. La norma no se considera suficiente para proteger la salud de las personas que viven cerca de las plantas de energía que queman carbón, dados los efectos negativos documentados en el "Informe al Congreso del Estudio de la EPA" de 1998. Sin embargo, nuevos datos publicados en 2015 mostraron que después de la introducción de controles más estrictos sobre el mercurio, éste se redujo drásticamente, lo que indica que la Ley de Aire Limpio surtió el efecto deseado.

La EPA anunció nuevas reglas para las plantas eléctricas de carbón el 22 de diciembre de 2011. Los hornos de cemento que queman residuos peligrosos se mantienen a un nivel de control menos estricto que las incineradoras estándar de residuos peligrosos, por lo que constituyen una fuente desproporcionada de la contaminación por mercurio.





</doc>
<doc id="3363" url="https://es.wikipedia.org/wiki?curid=3363" title="Químico">
Químico

Un químico es un científico especializado en la química. Los químicos estudian la composición de la materia y las propiedades que participan en su interacción, los productos resultantes, y la aplicación de estas propiedades en la vida del hombre como tal.

Son de especial interés para los químicos las propiedades de los compuestos, su reactividad, y su uso en campos como bioquímica, farmacología, industria cosmética, industria alimentaria, química de materiales, petroquímica, ingeniería, entre otras.

Los químicos utilizan sus conocimientos para aprender la composición y las propiedades de sustancias desconocidas; también para reproducir y sintetizar productos naturales en grandes cantidades y para crear nuevas sustancias artificiales mediante procesos rentables.

Los químicos pueden especializarse en diversas subdisciplinas de la química. Los metalúrgicos y los científicos de materiales deben compartir mucho de la educación y preparación seguidas por los químicos.

Los ingenieros químicos se relacionan con los procesos físicos necesarios para la producción industrial (calentamiento, refrigeración, mezcla, difusión, etc.) así como la separación y purificación de los productos, y trabajan con los químicos industriales y otros químicos en el desarrollo de nuevos procesos para estos productos.

La formación universitaria consiste, además de la formación en química, en una integración de ciencias auxiliares, donde se imparten asignaturas como física, matemáticas, química física, administración, legislación, bioquímica, entre otras.

Las licenciaturas que se relacionan con el campo de formación química son:

Los campos de especialización incluyen bioquímica, química orgánica, química inorgánica, química analítica, química teórica, química cuántica, química ambiental, química física, citoquímica, Fitoquímica y en algunos casos en microbiología y farmacia.

El campo laboral de un profesional de la química varía de acuerdo con el tipo de licenciatura cursada, su especialización, y los campos de aplicación de su rama. En general, el químico puede emplearse en cualquier proceso industrial donde se lleven a cabo reacciones de síntesis de compuestos de interés humano tales como: industria farmacéutica, industria petroquímica, industria cosmética, industria alimentaria, etc.

Los químicos especializados en ciencias de la salud humana, tales como Químicos clínicos, Químicos Bacteriólogos Parasitólogos, Químicos Farmacéuticos Biólogos, farmacéuticos, bioquímicos, bioquímicos diagnósticos, pueden trabajar en la síntesis, producción, venta y regulación en materia legal de medicamentos, análisis clínicos, desarrollo, investigación, docencia y capacitación continua de personal.


A veces en español se hace un uso inadecuado del término químico como producto químico o sustancia química. Esto se debe a una mala traducción del inglés "chemical" como "químico", en vez de "producto químico" o "sustancia química". 
Está recogido en Wikipedia como un ejemplo típico de , aunque este error es frecuente incluso en los medios de comunicación.


</doc>
<doc id="3364" url="https://es.wikipedia.org/wiki?curid=3364" title="Estrella">
Estrella

Una estrella (del latín: "stella") es un esferoide luminoso de plasma que mantiene su forma gracias a su propia gravedad. La estrella más cercana a la Tierra es el Sol. Otras estrellas son visibles a simple vista desde la Tierra durante la noche, apareciendo como una diversidad de puntos luminosos fijos en el cielo debido a su inmensa distancia de la misma. Históricamente, las estrellas más prominentes fueron agrupadas en constelaciones y asterismos, y las más brillantes fueron denominadas con nombres propios. Los astrónomos han recopilado un extenso catálogo, proporcionando a las estrellas designaciones estandarizadas. Sin embargo, la mayoría de las estrellas en el Universo, incluyendo todas las que están fuera de nuestra galaxia, la Vía Láctea, son invisibles a simple vista desde la Tierra. De hecho, la mayoría son invisibles desde nuestro planeta incluso a través de los telescopios de gran potencia.

Durante al menos una parte de su vida, una estrella brilla debido a la fusión termonuclear del hidrógeno en helio en su núcleo, que libera energía la cual atraviesa el interior de la estrella y, después, se irradia hacia el espacio exterior. Casi todos los elementos naturales más pesados ​​que el helio se crean por nucleosíntesis estelar durante la vida de la estrella y, en algunas de ellas, por nucleosíntesis de supernova cuando explotan. Cerca del final de su vida una estrella también puede contener materia degenerada. Los astrónomos pueden determinar la masa, edad, metalicidad (composición química) y muchas otras propiedades de las estrellas mediante la observación de su movimiento a través del espacio, su luminosidad y espectro, respectivamente. La masa total de una estrella es el principal determinante de su evolución y destino final. Otras características de las estrellas, incluyendo el diámetro y la temperatura, cambian a lo largo de su vida, mientras que el entorno de una estrella afecta a su rotación y movimiento. Una gráfica de dispersión de muchas estrellas que hace referencia a su luminosidad, magnitud absoluta, temperatura superficial y tipo espectral, conocido como el diagrama de Hertzsprung-Russell (Diagrama H-R), permite determinar la edad y el estado evolutivo de una estrella.

La vida de una estrella comienza con el colapso gravitacional de una nebulosa gaseosa de material compuesto principalmente de hidrógeno, junto con helio y trazas de elementos más pesados. Cuando el núcleo estelar es lo suficientemente denso, el hidrógeno comienza a convertirse en helio a través de la fusión nuclear, liberando energía durante el proceso. Los restos del interior de la estrella portan la energía fuera del núcleo a través de una combinación de procesos de transferencia de calor por radiación y convección. La presión interna de la estrella evita que se colapse aún más bajo su propia gravedad. Cuando se agota el combustible de hidrógeno en el núcleo, una estrella con al menos 0,4 veces la masa del Sol se expandirá hasta convertirse en una gigante roja, en algunos casos fusionando elementos más pesados en el núcleo o en sus capas alrededor del núcleo (como el carbono o el oxígeno). Entonces la estrella evoluciona hasta una forma degenerada, expulsando una porción de su materia en el medio interestelar, donde contribuirá a la formación de una nueva generación de estrellas. Mientras tanto, el núcleo se convierte en un remanente estelar: una enana blanca, una estrella de neutrones, o (si es lo suficientemente masiva) un agujero negro.

Los sistema binarios y multiestelares constan de dos o más estrellas que están unidas gravitacionalmente entre sí, y por lo general se mueven en torno a otra en órbitas estables. Cuando dos estrellas poseen una órbita relativamente cercana, su interacción gravitatoria puede tener un impacto significativo en su evolución. Las estrellas unidas gravitacionalmente entre sí pueden formar parte de estructuras mucho más grandes, como cúmulos estelares o galaxias.

Históricamente, las estrellas han sido importantes para las civilizaciones en todo el mundo, han sido parte de las prácticas religiosas y se utilizaron para la navegación celeste y la orientación. Muchos astrónomos antiguos creían que las estrellas estaban fijadas permanentemente a una esfera celeste y eran inmutables. Por convención los astrónomos agrupaban las estrellas en constelaciones y las usaban para rastrear los movimientos de los planetas y la posición inferida del Sol. El movimiento del Sol contra las estrellas de fondo (y el horizonte) fue utilizado para crear calendarios, que podrían ser utilizados para regular las prácticas agrícolas. El calendario gregoriano, utilizado actualmente casi en todo el mundo, es un calendario solar basado en el ángulo del eje de rotación de la Tierra con respecto a su estrella local, el Sol.

La carta estelar más antigua con fecha precisa fue un logro de la antigua astronomía egipcia en Los primeros catálogos de estrellas conocidos fueron compilados por los antiguos astrónomos babilónicos de Mesopotamia a finales del segundo milenio antes de Cristo, durante el período casita ("ca". 1531-1155 aC).

El primer catálogo de estrellas en la astronomía griega fue creado por Aristilo aproximadamente en 300 AC, con la ayuda de Timocharis. El catálogo de estrellas de Hiparco (siglo II aC) incluía 1020 estrellas, y se utilizó para ensamblar el catálogo de estrellas de Ptolomeo. Hiparco es conocido por el descubrimiento de la primera "nova" (nueva estrella) registrada. Muchas de las constelaciones y nombres de estrellas en uso hoy en día derivan de la astronomía griega.

A pesar de la aparente inmutabilidad de los cielos, los astrónomos chinos fueron conscientes de que podrían aparecer nuevas estrellas. En 185 d.C., fueron los primeros en observar y escribir sobre una supernova, ahora conocida como SN 185. El evento estelar más brillante registrado de la historia fue la supernova SN 1006, que fue observada en 1006 y descrita por el astrónomo egipcio Ali ibn Ridwan y varios astrónomos chinos. La supernova SN 1054, que dio origen a la Nebulosa del Cangrejo, también fue observada por astrónomos chinos e islámicos.

Los astrónomos islámicos medievales dieron nombres árabes a muchas estrellas que todavía se usan hoy e inventaron numerosos instrumentos astronómicos con los que poder calcular las posiciones de las estrellas. También construyeron los primeros grandes institutos de investigación y observatorios, principalmente con el propósito de producir catálogos "Zij" de estrellas. Entre ellos, el astrónomo persa Abd Al-Rahman Al Sufi escribió el "Libro de las Estrellas Fijas" (964), que observó varias estrellas, conglomerados de estrellas (incluidas los Omicron Velorum y los cúmulos de Brocchi) y galaxias (incluida la Galaxia de Andrómeda). Según A. Zahoor, en el siglo XI, el erudito polímata persa Abu Rayhan Biruni describió la galaxia de la Vía Láctea como una multitud de fragmentos que tenían las propiedades de estrellas nebulosas y en 1019 también dio las latitudes de varias estrellas durante un eclipse lunar.

Según Josep Puig, el astrónomo andalusí Ibn Bajjah propuso que la Vía Láctea estaba formada por muchas estrellas que casi se tocaban entre sí y parecía ser una imagen continua debido al efecto de la refracción del material sublunar, citando su observación de la conjunción de Júpiter y Marte en 500AH (1106/1107d.C.) como evidencia. 
Los primeros astrónomos europeos, como Tycho Brahe, identificaron nuevas estrellas en el cielo nocturno (más adelante denominadas "novas"), sugiriendo que los cielos no eran inmutables. En 1584, Giordano Bruno sugirió que las estrellas eran como el Sol y podrían tener otros planetas, posiblemente parecidos a la Tierra, en órbita alrededor de ellas, una idea que ya había sido sugerida anteriormente por los antiguos filósofos griegos, Demócrito y Epicuro, y por los cosmólogos islámicos medievales como Fakhr al-Din al-Razi. En el siglo siguiente la idea de que las estrellas eran iguales al Sol estaba llegando a un consenso entre los astrónomos. Para explicar por qué estas estrellas no ejercía ninguna fuerza gravitatoria neta sobre el sistema solar, Isaac Newton sugirió que las estrellas estaban igualmente distribuidas en todas las direcciones, una idea impulsada por el teólogo Richard Bentley.

En 1667 el astrónomo italiano Geminiano Montanari registró variaciones observadas en la luminosidad de la estrella Algol. Edmond Halley publicó las primeras mediciones del movimiento propio de un par de estrellas "fijas" cercanas, demostrando que estas habían cambiado sus posiciones desde el tiempo de los antiguos astrónomos griego Ptolomeo e Hiparco.

William Herschel fue el primer astrónomo que intentó determinar la distribución de las estrellas en el cielo. Durante la década de 1780 estableció una serie de indicadores en 600 direcciones y contó las estrellas observadas a lo largo de cada línea de visión. De esto dedujo que el número de estrellas se elevaba constantemente hacia un lado del cielo, en dirección al núcleo de la Vía Láctea. Su hijo John Herschel repitió este estudio en el hemisferio sur y encontró un aumento correspondiente en la misma dirección. Además de sus otros logros, William Herschel también destaca por su descubrimiento de que algunas estrellas no se encuentran simplemente a lo largo de la misma línea de visión, 

La ciencia de la espectroscopia astronómica fue iniciada por Joseph von Fraunhofer y Angelo Secchi. Comparando los espectros de estrellas como Sirio con el Sol, encontraron diferencias en la fuerza y el número de sus líneas de absorción —las líneas oscuras en un espectro estelar causadas por la absorción de la atmósfera de frecuencias específicas—. En 1865 Secchi comenzó a clasificar las estrellas por tipos espectrales. Sin embargo, la versión moderna del esquema de clasificación estelar fue desarrollado por Annie J. Cannon durante la década de 1900.
La primera medición directa de la distancia a una estrella (61 Cygni a 11,4 años luz) fue realizada en 1838 por Friedrich Bessel usando la técnica de paralaje. Las mediciones de paralaje demostraron la gran separación de las estrellas en los cielos. La observación de las estrellas dobles ganó importancia creciente durante el siglo XIX. En 1834 Friedrich Bessel observó cambios en el movimiento propio de la estrella Sirio e infirió un compañero oculto. En 1899, Edward Pickering descubrió la primera binaria espectroscópica cuando observó la división periódica de las líneas espectrales de la estrella Mizar en un período de 104 días. Las observaciones detalladas de muchos sistemas estelares binarios fueron recogidas por astrónomos como Friedrich Georg, Wilhelm von Struve y S. W. Burnham, lo que permitió que las masas de las estrellas se determinaran a partir de la computación de los elementos orbitales. En 1827 Felix Savary dio la primera solución al problema de derivar una órbita de estrellas binarias a partir de observaciones telescópicas.
El siglo XX vio avances cada vez más rápidos en el estudio científico de las estrellas. La fotografía se convirtió en una valiosa herramienta astronómica. Karl Schwarzschild descubrió que el color de una estrella, y por tanto su temperatura, podía determinarse comparando la magnitud visual con la magnitud fotográfica. El desarrollo del fotómetro fotoeléctrico permitió mediciones precisas de la magnitud en múltiples intervalos de longitud de onda. En 1921 Albert A. Michelson hizo las primeras mediciones de un diámetro estelar utilizando un interferómetro en el telescopio Hooker del Observatorio de Monte Wilson.

Durante las primeras décadas del siglo XX se produjeron importantes trabajos teóricos sobre la estructura física de las estrellas. En 1913, se desarrolló el diagrama Hertzsprung-Russell, que impulsó el estudio astrofísico de las estrellas. Se desarrollaron modelos exitosos para explicar los interiores de las estrellas y la evolución estelar. En 1925 Cecilia Payne-Gaposchkin propuso por primera vez en su tesis doctoral que las estrellas están hechas principalmente de hidrógeno y helio. Los espectros de las estrellas fueron entendidos más a fondo a través de los avances en la física cuántica. Esto permitió determinar la composición química de la atmósfera estelar.

Con la excepción de las supernovas, las estrellas individuales han sido observadas principalmente en el Grupo Local, y especialmente en la parte visible de la Vía Láctea (como lo demuestran los detallados catálogos de estrellas disponibles para nuestra galaxia). Pero se han observado algunas estrellas en la galaxia M100 del cúmulo de Virgo, a unos 100 millones de años luz de la Tierra. En el Supercúmulo Local es posible ver cúmulos de estrellas, y los telescopios actuales podrían, en principio, observar estrellas individuales débiles en el Grupo Local (ver Cefeidas). Sin embargo, fuera del Supercúmulo local de galaxias, no se han observado ni estrellas ni cúmulos de estrellas. La única excepción es una débil imagen de un gran cúmulo estelar que contiene cientos de miles de estrellas situadas a una distancia de un billón de años luz, diez veces más lejos del grupo de estrellas más distante observado anteriormente.

El concepto de constelación ya era conocido durante el período babilónico. Los antiguos observadores del cielo imaginaron que la disposición de las estrellas destacadas formaba dibujos, y los asociaron con aspectos particulares de la naturaleza o de sus mitos. Doce de estas formaciones estaban situadas a lo largo del plano de la eclíptica y se convirtieron en la base de la astrología. Muchas de las estrellas individuales más prominentes también recibieron nombres, particularmente con designaciones árabes o latinas.

Así como ciertas constelaciones y el Sol mismo, las estrellas individuales tienen sus propios mitos. Para los antiguos griegos, algunas «estrellas», conocidas como planetas (griego πλανήτης (planētēs, que significa «vagabundo»), representaban varias deidades importantes, de las cuales se tomaron los nombres de los planetas Mercurio, Venus, Marte, Júpiter y Saturno. (Urano y Neptuno también eran dioses griegos y romanos, pero ninguno de los dos fue conocido en la antigüedad debido a su bajo brillo y sus nombres fueron asignados por astrónomos posteriores).

Hacia 1600 los nombres de las constelaciones se usaron para nombrar las estrellas en las regiones correspondientes del cielo. El astrónomo alemán Johann Bayer creó una serie de mapas estelares y aplicó letras griegas como designaciones de las estrellas en cada constelación. Más tarde fue inventado un sistema de numeración basado en la ascensión recta de la estrella y se agregó al catálogo de estrellas de John Flamsteed en su libro "Historia coelestis Britannica" (la edición de 1712), por lo que este sistema de numeración llegó a llamarse "denominación de Flamsteed" o "numeración de Flamsteed".

La única autoridad internacionalmente reconocida para designar los cuerpos celestes es la Unión Astronómica Internacional (IAU). Esta asociación mantiene el Grupo de Trabajo sobre Nombres de Estrellas (WGSN) que cataloga y normaliza los nombres propios de las estrellas. Diversas compañías privadas venden nombres de estrellas, lo que la Biblioteca Británica llama una empresa comercial no regulada. La AIU se ha desvinculado de esta práctica comercial y estos nombres no son reconocidos ni por la IAU, ni por los astrónomos profesionales ni por la comunidad de astrónomos aficionados. Una de esas firmas es "International Star Registry" (Registro Internacional de Estrellas), que durante la década de 1980 fue acusada de prácticas engañosas por hacer parecer que el nombre asignado era oficial. Esta práctica de ISR, ahora interrumpida, fue informalmente etiquetada como una estafa y un fraude, y el Departamento de Asuntos del Consumidor de la Ciudad de Nueva York emitió una advertencia contra ISR por involucrarse en una práctica comercial engañosa.

Aunque los parámetros estelares puedan expresarse en unidades SI o unidades CGS, muchas veces es más conveniente expresar la masa, la luminosidad y el radio en unidades solares, sobre la base de las características del Sol. En el año 2015 la UAI definió un conjunto de valores nominales solares (definidos como constantes SI, sin incertidumbres) que pueden ser utilizados para citar parámetros estelares:

La masa solar M no fue definida explícitamente por la UAI debido a la gran incertidumbre relativa (10) de la constante gravitatoria newtoniana G. Sin embargo, dado que el producto de la constante gravitatoria newtoniana y la masa solar conjunta (GM) ha sido determinado con una precisión mucho mayor, la IAU definió el parámetro de masa solar "nominal" como:

Sin embargo se puede combinar el parámetro de masa solar nominal con la estimación CODATA más reciente (2014) de la constante gravitatoria newtoniana G para obtener una masa solar de aproximadamente 1.9885 × 10kg. Aunque los valores exactos de la luminosidad, el radio, el parámetro de masa y la masa pueden variar ligeramente en el futuro debido a las incertidumbres observacionales, las constantes nominales de IAU de 2015 seguirán siendo los mismos valores SI, ya que siguen siendo útiles para citar parámetros estelares.

Las longitudes grandes, como el radio de una estrella gigante o el eje semi-mayor de un sistema estelar binario, se expresan muchas veces en términos de la unidad astronómica —aproximadamente igual a la distancia media entre la Tierra y el Sol (150 millones de km o aproximadamente 93 millones de millas)—. En 2012 la AIU definió la como una longitud exacta en metros: 149597870700m.

Las estrellas se condensan en las regiones del espacio de mayor densidad, aunque esas regiones son menos densas que el interior de una cámara de vacío. Dichas regiones, conocidas como nubes moleculares, consisten principalmente en hidrógeno, con alrededor de 23 a 28 por ciento de helio y algunos elementos más pesados. Un ejemplo de estas regiones de formación de estrellas es la Nebulosa de Orión. La mayoría de las estrellas se forman en grupos de decenas a cientos de miles de estrellas.

Las estrellas masivas de estos grupos pueden iluminar poderosamente esas nubes, ionizar el hidrógeno y crear regiones H II. Tales efectos de retroalimentación, a partir de la formación estelar, pueden finalmente interrumpir la nube e impedir la formación de estrellas adicionales.

Todas las estrellas pasan la mayor parte de su existencia como estrellas de la "secuencia principal", alimentadas sobre todo por la fusión nuclear del hidrógeno en el helio dentro de sus núcleos. Sin embargo las estrellas de diferentes masas tienen propiedades marcadamente diferentes en varias etapas de su desarrollo. El destino final de las estrellas más masivas difiere del de las estrellas menos masivas, al igual que sus luminosidades y el impacto que tienen en su entorno, por lo que los astrónomos suelen agrupar las estrellas por su masa:





La formación de una estrella comienza con la inestabilidad gravitacional dentro de una nube molecular causada por regiones de mayor densidad —muchas veces desencadenada por la compresión de las nubes por radiación de estrellas masivas, por la expansión de burbujas en el medio interestelar, por la colisión de diferentes nubes moleculares o por la colisión de galaxias (como en una galaxia con brote estelar)—. Cuando una región alcanza una densidad suficiente de materia como para satisfacer los criterios de la inestabilidad de Jeans, comienza a colapsarse bajo su propia fuerza gravitatoria.

A medida que la nube colapsa, los conglomerados individuales de polvo denso y gas forman un "glóbulo de Bok". Cuando este colapsa y aumenta la densidad, la energía gravitacional se convierte en calor y aumenta la temperatura. Cuando la nube protoestelar ha alcanzado aproximadamente la condición estable del equilibrio hidrostático, se forma una protoestrella en el núcleo.

Generalmente estas estrellas de la secuencia pre-principal están rodeadas por un disco protoplanetario y alimentadas principalmente por la conversión de energía gravitacional. Su período de contracción gravitacional dura alrededor de 10 a 15 millones de años.
Las estrellas tempranas de menos de 2M se llaman estrellas T Tauri, mientras que aquellas con mayor masa son las estrellas Herbig Ae/Be. Estas estrellas recién formadas emiten chorros de gas a lo largo de su eje de rotación, lo que puede reducir el momento angular de la estrella colapsante y dar lugar a pequeñas manchas de nebulosidad conocidas como objetos Herbig-Haro.
Estos chorros, en combinación con la radiación de estrellas masivas cercanas, pueden ayudar a alejar la nube circundante de la cual se formó la estrella.

Al principio de su desarrollo las estrellas T Tauri siguen la trayectoria de Hayashi: se contraen y disminuyen en luminosidad mientras permanecen aproximadamente a la misma temperatura.

Se observa que la mayoría de las estrellas forman parte de sistemas estelares binarios y las propiedades de estos sistemas son el resultado de las condiciones en las que se formaron.

Una nube de gas debe perder su momento angular para colapsar y formar una estrella. La fragmentación de la nube en múltiples estrellas distribuye parte de ese momento angular. Estas interacciones tienden a dividir más los sistemas binarios separados (blandos), mientras también causan que los sistemas duros pasen a estar vinculados más estrechamente. Esto produce la separación de los sistemas binarios en sus dos distribuciones de poblaciones observadas.

Las estrellas consumen alrededor del 90% de su existencia fusionando hidrógeno en helio a altas temperaturas y en reacciones de alta presión cerca del núcleo. Se afirma que dichas estrellas están en la secuencia principal, y se llaman estrellas enanas. A partir de la secuencia principal de la edad cero, la proporción de helio en el núcleo de una estrella aumentará constantemente, así como también la tasa de fusión nuclear en el núcleo también aumentará lentamente, al igual que la temperatura y luminosidad de la estrella. El Sol, por ejemplo, se estima que ha aumentado en luminosidad en un 40% desde que alcanzó la secuencia principal hace 4600 millones (4.6 × 10) de años atrás.

Cada estrella genera un viento estelar de partículas que causa un flujo continuo de gas hacia el espacio. Para la mayoría de las estrellas, la masa perdida es insignificante. El Sol pierde 10M cada año, o alrededor de 0.01% de su masa total durante toda su vida. Sin embargo las estrellas muy masivas pueden perder 10-7 a 10-5M☉ cada año, lo que afecta significativamente a su evolución.
Las estrellas que comienzan con más de 50M pueden perder más de la mitad de su masa total mientras están en la secuencia principal.

El tiempo que una estrella consume en la secuencia principal depende principalmente de la cantidad de combustible que tiene y de la velocidad a la que lo fusiona. Se espera que el Sol viva 10 mil millones (10) años. Las estrellas masivas consumen su combustible muy rápidamente y son de corta vida. Las estrellas de baja masa consumen su combustible muy lentamente. Las estrellas de menos de 0,25M, llamadas enanas rojas, son capaces de fusionar casi toda su masa, mientras que las estrellas de alrededor de 1M solo pueden fusionar alrededor del 10% de su masa. La combinación de su lento consumo de combustible y su suministro relativamente grande de combustible utilizable permite que las estrellas de baja masa duren alrededor de un billón (10) años; las de más de 0,08M durarán alrededor de 12 billones de años. 
Las enanas rojas se vuelven más calientes y luminosas cuando acumulan helio. Cuando finalmente se quedan sin hidrógeno, se contraen en una enana blanca y disminuye su temperatura.
Sin embargo, dado que la vida útil de estas estrellas es mayor que la edad actual del universo (13,8 mil millones de años), no se espera que las estrellas menores de aproximadamente 0,85M se hayan movido de la secuencia principal.

Además de la masa, los elementos más pesados que el helio pueden desempeñar un papel significativo en la evolución de las estrellas. Los astrónomos etiquetan todos los elementos más pesados que los "metales" de helio, y llaman metalicidad a la concentración química de estos elementos en una estrella. La metalicidad de una estrella puede influir en el tiempo que tarda la estrella en quemar su combustible y controla la formación de sus campos magnéticos, lo que afecta a la fuerza de su viento estelar. Las estrellas más viejas de la población II tienen sustancialmente menos metalicidad que las estrellas más jóvenes de la población I debido a la composición de las nubes moleculares de las que se formaron. Con el tiempo, tales nubes se enriquecen cada vez más en elementos más pesados a medida que las estrellas más viejas mueren y desprenden porciones de sus atmósferas.

A medida que las estrellas de al menos 0,4M agotan su suministro de hidrógeno en su núcleo, comienzan a fusionar hidrógeno en una zona fuera del núcleo de helio. Sus capas externas se expanden y se refrescan enormemente a medida que forman una gigante roja. En unos 5000 millones de años, cuando el Sol entre en la fase de quema de helio, se expandirá hasta un radio máximo de aproximadamente 1 unidad astronómica (150 millones de kilómetros), 250 veces su tamaño actual y perderá el 30% de su masa actual.

A medida que la combustión de la capa de hidrógeno produce más helio, el núcleo aumenta en masa y temperatura. En una gigante roja de hasta 2,25"M", la masa del núcleo de helio se degenera antes de la fusión de helio. Finalmente, cuando la temperatura aumenta lo suficiente, comienza de manera explosiva la fusión de helio en lo que se llama un flash de helio, y la estrella se contrae rápidamente en radio, aumenta su temperatura superficial y se mueve a la rama horizontal del diagrama HR. Para las estrellas más masivas, la fusión del núcleo de helio comienza antes de que el núcleo se degenere, y la estrella pasa algún tiempo en el apelotonamiento rojo, quemando helio lentamente antes de que la envoltura convectiva externa se colapse y la estrella se mueva a la rama horizontal.

Después de que la estrella haya fusionado el helio de su núcleo, se fusiona el producto de carbono produciendo un núcleo caliente con una envoltura externa de helio de fusión. Entonces la estrella sigue una trayectoria evolutiva llamada rama asintótica gigante (AGB) que es paralela a la otra fase gigante roja descrita, pero con una luminosidad más alta. Las estrellas de AGB más masivas pueden experimentar un breve período de fusión de carbono antes de que el núcleo se degenere.

Durante su fase de quema de helio, una estrella de más de nueve masas solares se expande para formar primero una supergigante azul y luego una roja. Las estrellas particularmente masivas pueden evolucionar a una estrella de Wolf-Rayet, caracterizada por espectros dominados por líneas de emisión de elementos más pesados que el hidrógeno que han alcanzado la superficie debido a la fuerte convección y a la intensa pérdida de masa.

Cuando el helio se agota en el núcleo de una estrella masiva, el núcleo se contrae y la temperatura y presión se elevan lo suficiente como para fusionar el carbono (véase proceso de combustión del carbono). Este proceso continúa, con las etapas sucesivas alimentadas por neón (ver proceso de combustión del neón), oxígeno (véase proceso de combustión del oxígeno) y silicio (véase proceso de combustión de silicio). Cerca del final de la vida de la estrella, la fusión continúa a lo largo de una serie de capas consecutivas dentro de una estrella masiva. Cada capa fusiona un elemento diferente; la capa más externa fusiona el hidrógeno, la siguiente fusiona el helio, y así sucesivamente.

La etapa final se produce cuando una estrella masiva comienza a producir hierro. Dado que los núcleos de hierro están más estrechamente unidos que cualquier núcleo más pesado, cualquier fusión más allá del hierro no produce una liberación neta de energía. Tal proceso continúa en un grado muy limitado, pero consume energía. Del mismo modo, puesto que los núcleos están más estrechamente unidos que todos los núcleos más ligeros, dicha energía no puede ser liberada por fisión.

A medida que el núcleo de una estrella se contrae, aumenta la intensidad de la radiación de esa superficie, creando una presión de radiación tal en la capa externa del gas que empujará a esas capas, formando una nebulosa planetaria. Si lo que queda después de que la atmósfera exterior se haya desprendido sea inferior a 1,4"M", se reduce a un objeto relativamente pequeño. del tamaño de la Tierra, conocido como enana blanca. Las enanas blancas carecen de masa suficiente como para que se produzca una compresión gravitacional adicional. La materia degenerada de electrones dentro de una enana blanca ya no es un plasma, a pesar de que las estrellas son generalmente conocidas como esferoides de plasma. Finalmente las enanas blancas se desvanecen en enanas negras durante un período de tiempo muy largo.

En las estrellas más grandes la fusión continúa hasta que el núcleo de hierro haya crecido tanto (más de 1,4"M") que ya no pueda soportar su propia masa. Este núcleo se colapsará de repente a medida que sus electrones sean impulsados a sus protones, formando neutrones, neutrinos y rayos gamma en una explosión de captura de electrones y desintegración beta inversa. La onda de choque formada por este repentino colapso hace que el resto de la estrella explote en una supernova. Estas se vuelven tan brillantes que pueden eclipsar brevemente a toda la galaxia natal de la estrella. Cuando ocurren dentro de la Vía Láctea, las supernovas han sido históricamente descritas por observadores a simple vista como "nuevas estrellas" donde aparentemente antes no existía ninguna .

Una explosión de supernova expulsa las capas exteriores de la estrella dejando un remanente tal como la Nebulosa del Cangrejo. El núcleo se comprime en una estrella de neutrones que a veces se manifiesta como púlsar o erupción de rayos X. En el caso de las estrellas más grandes el remanente es un agujero negro mayor de 4"M". En una estrella de neutrones la materia está en un estado conocido como materia degenerada de neutrones, con una forma más exótica de materia degenerada, la materia QCD, presente posiblemente en el núcleo. Dentro de un agujero negro la materia se encuentra en un estado que no es posible entender actualmente.

En las capas externas desprendidas de estrellas moribundas se incluyen elementos pesados que pueden ser reciclados durante la formación de nuevas estrellas. Estos elementos pesados permiten la formación de planetas rocosos. El flujo de salida de las supernovas y el viento estelar de las grandes estrellas desempeñan un papel importante en la formación del medio interestelar.

La evolución posterior a la secuencia principal de las estrellas binarias puede ser significativamente diferente de la evolución de las estrellas individuales de la misma masa. Si las estrellas en un sistema binario son suficientemente cercanas, cuando una de las estrellas se expande para convertirse en una gigante roja puede desbordar su lóbulo de Roche, la región alrededor de una estrella donde el material está gravitacionalmente ligado a esa estrella, lo que lleva a la transferencia de material de una a otra. Cuando se traspasa el lóbulo de Roche puede producirse una variedad de fenómenos como estrellas binarias de contacto, binarias de envoltura común, variables cataclísmicas y supernovas del tipo Ia.

Las estrellas no se distribuyen uniformemente a través del universo sino que se agrupan normalmente en galaxias junto con el gas y el polvo interestelar. Una galaxia típica contiene cientos de miles de millones de estrellas, y hay más de 100 mil millones (10) de galaxias en el universo observable. En 2010, una estimación del número de estrellas en el universo observable fue de casi un tercio de cuatrillón ().
Aunque a menudo se cree que las estrellas solo existen dentro de las galaxias, se han descubierto estrellas intergalácticas.

Un sistema multiestelar consiste en dos o más estrellas ligadas gravitacionalmente que orbitan entre sí. El sistema multiestelar más simple y más común es una estrella binaria, pero también se encuentran sistemas de tres o más estrellas. Por razones de estabilidad orbital, tales sistemas de múltiples estrellas se organizan muchas veces en conjuntos jerárquicos de estrellas binarias. También existen grupos más grandes, llamados cúmulos estelares, que van desde asociaciones estelares sueltas con solo unas cuantas estrellas hasta enormes cúmulos globulares con cientos de miles de estrellas. Tales sistemas orbitan su galaxia de acogida.

Desde hace mucho tiempo se ha asumido que la mayoría de las estrellas se encuentran en los sistemas de múltiples estrellas ligadas gravitacionalmente. Esto es particularmente cierto para estrellas de clase O y B muy masivas, donde se cree que el 80% de las estrellas son parte de sistemas de múltiples estrellas. 
La proporción de sistemas de una sola estrella aumenta con la disminución de la masa estelar, de modo que se sabe que solo el 25% de las enanas rojas tienen compañeras estelares. Debido a que el 85% de todas las estrellas son enanas rojas, la mayoría de las estrellas en la Vía Láctea son posiblemente únicas desde su nacimiento.

La estrella más cercana a la Tierra, aparte del Sol, es Proxima Centauri, que está a 39,9 billones de kilómetros, o 4,2 años luz. Viajando a la velocidad orbital del transbordador espacial (8 kilómetros por segundo, casi 30000 kilómetros por hora), se tardaría unos 150000 años en llegar. Esto es típico de separaciones estelares en discos galácticos. Las estrellas pueden estar mucho más cercanas entre sí en los centros de las galaxias y en los cúmulos globulares, o mucho más lejos en los halos galácticos.

Debido a las distancias relativamente grandes entre las estrellas fuera del núcleo galáctico, se cree que las colisiones entre estrellas son raras. En regiones más densas como el núcleo de los cúmulos globulares o el centro galáctico, las colisiones pueden ser más comunes. Tales colisiones pueden producir lo que se conoce como rezagadas azules. Estas estrellas anómalas tienen una temperatura superficial más alta que las otras estrellas de la secuencia principal con la misma luminosidad del cúmulo al que pertenecen.

Las estrellas pueden estar ligadas gravitacionalmente unas con otras formando sistemas estelares binarios, ternarios o agrupaciones aún mayores. Una fracción alta de las estrellas del disco de la Vía Láctea pertenecen a sistemas binarios; el porcentaje es cercano al 90 % para estrellas masivas y desciende hasta el 50% para estrellas de masa baja. Otras veces, las estrellas se agrupan en grandes concentraciones que van desde las decenas hasta los centenares de miles o incluso millones de estrellas, formando los denominados cúmulos estelares. Estos cúmulos pueden deberse a variaciones en el campo gravitacional galáctico o bien pueden ser fruto de brotes de formación estelar (se sabe que la mayoría de las estrellas se forman en grupos). Tradicionalmente, en la Vía Láctea se distinguían dos tipos: (1) los cúmulos globulares, que son viejos, se encuentran en el halo y contienen de centenares de miles a millones de estrellas y (2) los cúmulos abiertos, que son de formación reciente, se encuentran en el disco y contienen un número menor de estrellas. Desde finales del siglo XX esa clasificación ha sido cuestionada al descubrirse en el disco de la Vía Láctea cúmulos estelares jóvenes como Westerlund 1 o NGC 3603 con un número de estrellas similar al de un cúmulo globular. Esos cúmulos masivos y jóvenes se encuentran también en otras galaxias; algunos ejemplos son 30 Doradus en la Gran Nube de Magallanes y NGC 4214-I-A en NGC 4214.

No todas las estrellas mantienen uniones gravitatorias estables; algunas, igual que el Sol, viajan solitarias, separándose mucho de la agrupación estelar en la que se formaron.
Estas estrellas aisladas responden tan solo al campo gravitatorio global constituido por la superposición de los campos del total de objetos de la galaxia: agujeros negros, estrellas, objetos compactos y gas interestelar.

Normalmente las estrellas no están distribuidas uniformemente en el universo, a pesar de lo que pueda parecer a simple vista, sino agrupadas en galaxias. Una galaxia espiral típica, como la Vía Láctea, contiene cientos de miles de millones de estrellas agrupadas, la mayoría, en el estrecho plano galáctico. El cielo nocturno terrestre aparece homogéneo a simple vista porque solo es posible observar una región muy localizada del plano galáctico. Extrapolando de lo observado en la vecindad del sistema solar, se puede decir que la mayor parte de las estrellas se concentran en el disco galáctico y dentro de este en una región central, el bulbo galáctico, que se sitúa en la constelación de Sagitario.

A pesar de las enormes distancias que separan a las estrellas, desde la perspectiva terrestre sus posiciones relativas parecen fijas en el firmamento. Gracias a la precisión de sus posiciones, «son de gran utilidad para la navegación, para la orientación de los astronautas en las naves espaciales y para identificar otros astros» ("The American Encyclopedia"). Las estrellas fueron la única forma que tuvieron los marinos para situarse en alta mar hasta el advenimiento de los sistemas electrónicos de posicionamiento hacia mediados del siglo XX. Véase Estrella (náutica).

Casi todo lo relacionado con una estrella está determinado por su masa inicial, incluyendo características tales como su luminosidad, tamaño, evolución, vida útil y destino final.

La mayor parte de las estrellas tienen entre 1000 y 11000 millones de años de antigüedad. Algunas estrellas pueden incluso estar cerca de los 13800 millones de años, la edad observada del universo. La estrella más antigua descubierta hasta ahora, HD 140283, apodada estrella de Matusalén, tiene una edad estimada de 14,46 ± 0,8 billones de años. (Debido a la incertidumbre en el valor, esta edad para la estrella no entra en conflicto con la edad del Universo, determinada por el satélite Planck como 13799 ± 0.021).

Cuanto más masiva es la estrella, más corta es su vida, principalmente porque las estrellas masivas tienen una mayor presión sobre sus núcleos, lo que hace que quemen el hidrógeno más rápidamente. Las estrellas más masivas duran un promedio de unos pocos millones de años, mientras que las estrellas de masa mínima (enanas rojas) queman su combustible muy lentamente y pueden durar de decenas a cientos de miles de millones de años.

Cuando se forman estrellas en la actual galaxia de la Vía Láctea, están compuestas por un 71% de hidrógeno y un 27% de helio, medido en masa, con una pequeña fracción de elementos más pesados. Típicamente, la porción de elementos pesados se mide en términos del contenido de hierro de la atmósfera estelar, ya que el hierro es un elemento común y sus líneas de absorción son relativamente fáciles de medir. La porción de elementos más pesados puede ser un indicador de la probabilidad de que la estrella tenga un sistema planetario.

La estrella con el contenido de hierro más bajo jamás medido es la enana HE1327-2326, con solo 1/200000º del contenido de hierro del Sol. Por el contrario, la estrella rica en el super-metal μ Leonis tiene casi el doble de abundancia de hierro que el Sol, mientras que la estrella planetaria 14 Herculis tiene casi el triple del hierro. 

También existen estrellas químicamente peculiares que muestran abundancias inusuales de ciertos elementos en su espectro, especialmente cromo y tierras raras. Las estrellas con atmósferas exteriores más frías, incluido el Sol, pueden formar varias moléculas diatómicas y poliatómicas.

Debido a su gran distancia de la Tierra, todas las estrellas excepto el Sol aparecen a simple vista como puntos brillantes en el cielo nocturno que titilan debido al efecto de la atmósfera de la Tierra. El Sol es también una estrella, pero está lo suficientemente cerca de la Tierra como para aparecer como un disco y proporcionar la luz natural. Aparte del Sol, la estrella con el mayor tamaño aparente es R Doradus, con un diámetro angular de solo 0,057 segundos de arco.

Los discos de la mayoría de las estrellas son demasiado pequeños en tamaño angular como para ser observados con los actuales telescopios ópticos terrestres, por lo que se requieren telescopios interferómetricos para obtener imágenes de estos objetos. Otra técnica para medir el tamaño angular de las estrellas es a través de la ocultación. Mediante la medición exacta de la caída del brillo de una estrella que va siendo ocultada por la Luna (o el aumento de brillo cuando reaparece), se puede calcular su diámetro angular.

El tamaño de las estrellas varía desde de las estrellas de neutrones, que tienen de 20 a 40km de diámetro, hasta las supergigantes como Betelgeuse en la constelación de Orión, con un diámetro aproximadamente 1070 veces el del Sol —alrededor de 1490171880km (925949878mi)— aunque con una densidad mucho más baja que el Sol.

El movimiento de una estrella en relación con el Sol puede proporcionar información útil sobre el origen y la edad de una estrella, así como sobre la estructura y evolución de la galaxia circundante. Los componentes del movimiento de una estrella consisten en la velocidad radial hacia o desde el Sol, y el movimiento angular transversal, que se denomina movimiento propio.

La velocidad radial se mide por el desplazamiento Doppler de las líneas espectrales de la estrella y se da en unidades de km/s. El movimiento propio de una estrella, su paralaje, está determinado por mediciones astrométricas precisas en unidades de mili-segundos de arco ("mas", por sus siglas en inglés) por año. Conociendo el paralaje de la estrella y su distancia, se puede calcular la velocidad de movimiento propio. Junto con la velocidad radial se puede calcular la velocidad total. Es probable que las estrellas con altas tasas de movimiento propio estén relativamente cerca del Sol, lo que las convierte en buenas candidatas para las mediciones de paralaje.

Cuando se conocen ambas velocidades de movimiento, se puede calcular la velocidad espacial de la estrella en relación con el Sol o la galaxia. Entre las estrellas cercanas, se ha encontrado que por lo general las estrellas más jóvenes de la población I tienen velocidades más bajas que las estrellas más viejas de la población II. La comparación de la cinemática de las estrellas cercanas permitió a los astrónomos trazar su origen a puntos comunes en nubes moleculares gigantes, y se denominan asociaciones estelares.

El campo magnético de una estrella se genera dentro de las regiones del interior donde ocurre la circulación convectiva. Este movimiento del plasma conductor funciona como una dinamo, donde el movimiento de las cargas eléctricas induce campos magnéticos, al igual que una dinamo mecánica. Esos campos magnéticos tienen un gran alcance que se extiende a través y más allá de la estrella. La intensidad del campo magnético varía con la masa y composición de la estrella, y la cantidad de actividad superficial magnética depende de la velocidad de rotación de la estrella. Esta actividad superficial produce manchas estelares, que son regiones de campos magnéticos fuertes con temperaturas superficiales inferiores a las normales. Los bucles coronales arquean las líneas de flujo del campo magnético que se elevan de la superficie de una estrella a la atmósfera exterior de la misma, su corona. Los bucles coronales se pueden ver debido al plasma que conducen por toda su longitud. Las erupciones estelares son explosiones de partículas de alta energía que se emiten debido a la misma actividad magnética.

Las estrellas jóvenes que giran rápidamente tienden a tener altos niveles de actividad superficial debido a su campo magnético. El campo magnético puede actuar sobre el viento estelar de una estrella, funcionando como un freno que disminuye gradualmente y con el tiempo la velocidad de rotación. Así, las estrellas más viejas como el Sol tienen una velocidad de rotación mucho más lenta y un nivel más bajo de actividad superficial. Los niveles de actividad de las estrellas que giran lentamente tienden a variar de una manera cíclica y pueden interrumpirse por completo por periodos de tiempo. Por ejemplo, durante el Mínimo de Maunder, el Sol sufrió un período de 70 años sin casi ninguna actividad de manchas solares.

Una de las estrellas más masivas conocidas es Eta Carinae, que, con 100-150 veces más masa que el Sol, tendrá una vida de solo varios millones de años. Los estudios de los cúmulos abiertos más masivos sugieren 150M como límite superior para las estrellas en la era actual del universo. Esto representa un valor empírico para el límite teórico sobre la masa de estrellas en formación debido a la creciente presión de radiación sobre la nube de gas de acreción. Se han medido varias estrellas en el cúmulo R136 en la Gran Nube de Magallanes con masas más grandes,
pero se ha determinado que podrían haber sido creadas a través de la colisión y fusión de estrellas masivas en sistemas binarios cercanos, evitando el límite de 150M en la formación de estrellas masivas.
Las primeras estrellas que se formaron después del Big Bang pudieron haber sido más grandes, hasta 300M, debido a la ausencia completa de elementos más pesados que el litio en su composición. Es probable que esta generación de estrellas supermasivas de la población III haya existido en el universo muy temprano (es decir, se observa que tienen un alto desplazamiento al rojo) y puede haber comenzado la producción de elementos químicos más pesados que el hidrógeno que son necesarios para la posterior formación de planetas y vida. En junio de 2015, los astrónomos informaron de la evidencia de estrellas de la población III en la galaxia Cosmos Redshift 7 en "z"=6,60.

Con una masa solo 80 veces mayor que la de Júpiter ("M"), 2MASS J0523-1403 es la estrella más pequeña conocida que experimenta fusión nuclear en su núcleo. Para las estrellas con metalicidad similar a la del Sol, la masa mínima teórica que la estrella puede tener y todavía sufrir fusión en el núcleo, se estima que es de unos 75"M". Sin embargo, cuando la metalicidad es muy baja, el tamaño mínimo de las estrellas parece ser alrededor del 8,3% de la masa solar, o alrededor de 87"M". Los cuerpos más pequeños llamados enanas marrones, ocupan un área gris mal definida entre las estrellas y los gigantes gaseosos.

La combinación del radio y la masa de una estrella determina su gravedad superficial. Las estrellas gigantes tienen una gravedad superficial mucho menor que las estrellas de la secuencia principal, mientras que lo contrario es el caso de las estrellas degeneradas y compactas como las enanas blancas. La gravedad superficial puede influir en la aparición del espectro de una estrella, con mayor gravedad causando un ensanchamiento de las líneas de absorción.

La velocidad de rotación de las estrellas se puede determinar a través de la medición espectroscópica, o más exactamente mediante el seguimiento de sus manchas estelares. Las estrellas jóvenes pueden tener una rotación de más de 100 km/s en el ecuador. Por ejemplo, la estrella de la clase B Achernar tiene una velocidad ecuatorial de unos 225km/s o más, lo que hace que su ecuador sobresalga hacia fuera y le da un diámetro ecuatorial que es más del 50% mayor que entre los polos. Esta velocidad de rotación está justo por debajo de la velocidad crítica de 300km/s, velocidad a la que la estrella se rompería. Por el contrario, el Sol gira una vez cada 25-35 días, dependiendo de la latitud, con una velocidad ecuatorial de 1994km/s. El campo magnético de una estrella de secuencia principal y el viento estelar sirven para ralentizar su rotación en una cantidad significativa a medida que evoluciona en la secuencia principal.

Las estrellas degeneradas se han contraído en una masa compacta, dando como resultado una velocidad de rotación rápida. Sin embargo, tienen tasas de rotación relativamente bajas en comparación con lo que cabría esperar por la conservación del momento angular: la tendencia de un cuerpo giratorio a compensar una contracción del tamaño aumentando su velocidad de giro. Una gran parte del momento angular de la estrella se disipa como resultado de la pérdida de masa mediante el viento estelar. A pesar de esto, la velocidad de rotación de un púlsar puede ser muy rápida. Por ejemplo, el púlsar en el corazón de la nebulosa del Cangrejo gira 30 veces por segundo. La velocidad de rotación del púlsar disminuirá gradualmente debido a la emisión de radiación.

La temperatura superficial de una estrella de la secuencia principal está determinada por la velocidad de producción de energía de su núcleo y por su radio, y por lo general se calcula a partir del índice de color de la estrella. La temperatura se da normalmente en términos de una temperatura efectiva, que es la temperatura de un cuerpo negro idealizado que irradia su energía a la misma luminosidad por área de superficie que la estrella. La temperatura en la región central de una estrella es de varios millones de grados kelvin.

La temperatura estelar determinará la velocidad de ionización de diversos elementos, dando lugar a líneas de absorción características en el espectro. La temperatura superficial de una estrella, junto con su magnitud absoluta visual y las características de absorción, se utilizan para clasificar una estrella (véase clasificación abajo).

Las estrellas más grandes de la secuencia principal pueden tener temperaturas superficiales de 50000K. Las estrellas más pequeñas tales como el Sol tienen temperaturas superficiales de algunos miles de K. Los gigantes rojos tienen temperaturas superficiales relativamente bajas, de cerca de 3600K; pero también tienen una alta luminosidad debido a su gran superficie exterior.

La energía producida por las estrellas, producto de la fusión nuclear, se irradia al espacio tanto como radiación electromagnética como radiación de partículas. Esta última, emitida por una estrella, se manifiesta como el viento estelar, que fluye desde las capas externas en forma de protones cargados eléctricamente y partículas alfa y beta. Aunque casi sin masa, también existe un flujo constante de neutrinos que emanan del núcleo de la estrella.

La producción de energía en el núcleo es la razón por la cual las estrellas brillan tan intensamente: cada vez que dos o más núcleos atómicos se fusionan para formar un único núcleo atómico de un nuevo elemento más pesado, se liberan fotones de rayos gamma, producto de la fusión nuclear. Esta energía se convierte en otras formas de energía electromagnética de menor frecuencia, como la luz visible cuando alcanza las capas exteriores de la estrella.

El color de una estrella, determinado por la frecuencia más intensa de la luz visible, depende de la temperatura de las capas exteriores de la estrella, incluida su fotosfera. Además de la luz visible, las estrellas también emiten formas de radiación electromagnética que son invisibles para el ojo humano. De hecho, la radiación electromagnética estelar abarca todo el espectro electromagnético, desde las longitudes de onda más largas de las ondas de radio pasando por el infrarrojo, la luz visible y la ultravioleta, hasta las más cortas de los rayos X y los rayos gamma. Desde el punto de vista de la energía total emitida por una estrella, no todos los componentes de la radiación electromagnética estelar son significativos, pero todas las frecuencias proporcionan una visión de la física de la estrella.

Usando el espectro estelar, los astrónomos pueden también determinar la temperatura superficial, la gravedad superficial, la metalicidad y la velocidad de rotación de una estrella. Si se encuentra la distancia de la estrella, tal como midiendo el paralaje, entonces se puede derivar la luminosidad de la estrella. La masa, el radio, la gravedad de la superficie y el período de rotación pueden estimarse a partir de modelos estelares. (La masa se puede calcular para las estrellas en sistemas binarios midiendo sus velocidades orbitales y las distancias. Se ha utilizado la microlente gravitacional para medir la masa individual de una estrella.) Con estos parámetros, los astrónomos también pueden estimar la edad de la estrella.

La luminosidad de una estrella es la cantidad de luz y otras formas de energía radiante irradiada por unidad de tiempo. Cuenta con unidades de potencia. La luminosidad de una estrella está determinada por su radio y temperatura superficial. Muchas estrellas no irradian uniformemente en toda su superficie. Por ejemplo, la estrella de rotación rápida Vega tiene un flujo de energía más alto (potencia por unidad de área) en sus polos que a lo largo de su ecuador.

Las manchas superficiales de una estrella con temperatura más baja y luminosidad que el promedio se conocen como manchas estelares. Por lo general, las estrellas pequeñas y "enanas", como nuestro Sol, tienen manchas esencialmente sin rasgos con solo pequeñas manchas. Por el contrario, las estrellas "gigantes" presentan manchas estelares mucho más grandes y más evidentes, y también exhiben una fuerte oscurecimiento del limbo estelar. Es decir, el brillo disminuye hacia el borde del disco estelar. Las estrellas fulgurantes enanas rojas tales como UV Ceti pueden también poseer prominentes manchas características.

El brillo aparente de una estrella se expresa en términos de su magnitud aparente. Es una función de la luminosidad de la estrella, su distancia de la Tierra, y la alteración de la luz de la estrella mientras que pasa a través de la atmósfera de la Tierra. La magnitud intrínseca o absoluta está directamente relacionada con la luminosidad de la estrella, y es la magnitud aparente de una estrella si la distancia entre la Tierra y la estrella fuera de 10 parsecs (32,6 años luz).

Tanto las escalas de magnitud aparente como absoluta son unidades logarítmicas: una diferencia de número entero en magnitud es igual a una variación de brillo de aproximadamente 2,5 veces (la raíz quinta de 100 o aproximadamente 2,512). Esto significa que una estrella de primera magnitud (+1.00) es aproximadamente 2,5 veces más brillante que una estrella de segunda magnitud (+2.00), y unas 100 veces más brillante que una estrella de sexta magnitud (+6.00). Las estrellas más débiles visibles a simple vista bajo condiciones visuales idóneas son de magnitud +6.

En las escalas tanto de magnitud aparente como absoluta, cuanto menor es el número de magnitud, más brillante es la estrella; por el contrario, cuanto mayor sea el número de magnitud, más débil será la estrella. Las estrellas más brillantes, en cualquier escala, tienen números de magnitudes negativas. La variación de brillo (Δ"L") entre dos estrellas se calcula restando el número de magnitud de la estrella más brillante ("m") del número de magnitud de la estrella más débil ("m"), utilizando la diferencia como exponente para el número de base 2,512; es decir:

En relación con la luminosidad y la distancia de la Tierra, la magnitud absoluta de una estrella ("M") y la magnitud aparente ("m") no son equivalentes; Por ejemplo, la estrella brillante Sirio tiene una magnitud aparente de –1,44, pero tiene una magnitud absoluta de +1,41.

El Sol tiene una magnitud aparente de —26,7, pero su magnitud absoluta es solo +4,83. Sirio, la estrella más brillante del cielo nocturno vista desde la Tierra, es aproximadamente 23 veces más luminosa que el Sol, mientras que Canopus, la segunda estrella más brillante del cielo nocturno con una magnitud absoluta de –5,53, es aproximadamente 14000 veces más luminosa que el Sol. Sin embargo, aunque Canopus es mucho más luminosa que Sirio, esta aparece más brillante que Canopus. Esto se debe a que Sirio está a solo 8,6 años luz de la Tierra, mientras que Canopus está mucho más lejos, a una distancia de 310 años luz.

A partir de 2006 la estrella con la magnitud absoluta más alta conocida es LBV 1806-20, con una magnitud de –14,2. Esta estrella es al menos 5000000 de veces más luminosa que el Sol. Las estrellas menos luminosas que se conocen a 2017 se encuentran en el cúmulo NGC 6397. Las enanas rojas más débiles en el cúmulo eran de magnitud 26, mientras que también fue descubierta una enana blanca de magnitud 28. Estas estrellas débiles son tan oscuras que su luz sería tan poco brillante como una vela de cumpleaños en la Luna vista desde la Tierra.

La primera clasificación estelar fue realizada por Hiparco de Nicea y preservada en la cultura occidental a través de Ptolomeo, en una obra llamada "Almagesto". Este sistema clasificaba las estrellas por la intensidad de su brillo aparente visto desde la Tierra. Hiparco definió una escala decreciente de magnitudes, donde las estrellas más brillantes son de primera magnitud y las menos brillantes, casi invisibles a simple vista, son de sexta magnitud. Aunque ya no se emplea, constituyó la base para la clasificación actual.

El sistema de clasificación estelar actual se originó a principios del siglo XX, cuando las estrellas fueron clasificadas de la "A" hasta la "Q" con base en la fuerza de la línea de hidrógeno. Se pensó que la resistencia de la línea de hidrógeno era una simple función lineal de la temperatura.Si bien era más complicado, se fortalecìa con el aumento de la temperatura, llegando a su máximo cerca de 9000K, y luego disminuyendo a mayores temperaturas. Cuando se reordenaron las clasificaciones basándose en la temperatura, se asemejó más al esquema moderno.

Además, las estrellas pueden clasificarse por los efectos de luminosidad que se encuentran en sus líneas espectrales, que corresponden a su tamaño espacial y están determinadas por su gravedad superficial. Estos van desde "0" (hipergigantes) a "III" (gigantes), a "V" (enanas de la secuencia principal); asimismo algunos autores agregan "VII" (enanas blancas). La mayoría de las estrellas pertenecen a la secuencia principal, que está constituida por estrellas ordinarias que queman hidrógeno.

Estos se dividen a lo largo de una banda estrecha, diagonal cuando representa gráficamente en función de su magnitud y espectral absoluta tipo.

El Sol es una enana amarilla del tipo G2V de secuencia principal de temperatura intermedia y tamaño ordinario.

Existe una nomenclatura adicional, en forma de letras minúsculas añadidas al final del tipo espectral, con el propósito de indicar características peculiares del espectro. Por ejemplo, una ""e"" puede indicar la presencia de líneas de emisión; ""m"" representa niveles inusualmente altos de metales, y ""var"" puede significar variaciones en el tipo espectral.

Las estrellas enanas blancas tienen su propia clase que comienza con la letra "D". Esta se subdivide en las clases "DA", "DB", "DC", "DO", "DZ" y "DQ", dependiendo de los tipos de líneas prominentes que se encuentran en el espectro. A esto le sigue un valor numérico que indica la temperatura.

La clasificación de Harvard de tipos espectrales no determina unívocamente las características de una estrella. Estrellas con la misma temperatura pueden tener tamaños muy diferentes, lo que implica luminosidades también muy diferentes. Para distinguirlas se definieron, en Yerkes, las clases de luminosidad. En este sistema de clasificación se examina nuevamente el espectro estelar y se buscan líneas espectrales sensibles a la gravedad de la estrella. De este modo es posible estimar su tamaño.

Ambos sistemas de clasificación son complementarios.

Aproximadamente un 10 % de todas las estrellas son enanas blancas, un 70% son estrellas de tipo M, un 10% son estrellas de tipo K y un 4% son estrellas tipo G como el Sol. Tan solo un 1% de las estrellas son de mayor masa y tipos A y F. Las estrellas de Wolf-Rayet son extremadamente infrecuentes. Las enanas marrones, proyectos de estrellas que se quedaron a medias a causa de su pequeña masa, podrían ser muy abundantes pero su débil luminosidad impide realizar un censo apropiado.

Las estrellas pueden clasificarse de acuerdo a cuatro criterios gravitacionales instaurados por la Unión Astronómica Internacional en 2006. Esta clasificación estelar de la UAI es la más aceptada y comúnmente usada.

El primer criterio es la presencia o ausencia de un centro gravitacional estelar, es decir si forman parte de un sistema estelar. Las estrellas que forman parte de un sistema estelar (presencia de centro gravitacional estelar) se denominan "estrellas sistémicas". Las estrellas que no forman parte de un sistema estelar (ausencia de centro gravitacional estelar) se denominan "estrellas solitarias".

Las estrellas sistémicas (que forman parte de un sistema estelar) pueden ser a su vez de dos tipos. Las "estrellas centrales" son aquellas estrellas sistémicas que actúan como centro gravitacional de otras estrellas. Esto quiere decir que otras estrellas las orbitan. Las estrellas sistémicas que orbitan a una estrella central se denominan "estrellas satélites".

Esta clasificación de las estrellas se basa en distinguir dos tipos de estrellas dependiendo de si estas se agrupan con otras estrellas mediante fuerzas de atracción gravitacional. Esta clasificación refiere a dos tipos de estrellas (cumulares e independientes) de acuerdo a si se encuentran o no unidas a otras estrellas y, además, esta unión no se debe a la presencia de un centro gravitacional estelar; es decir, ninguna estrella gira alrededor de otra y sin embargo se encuentran unidas gravitacionalmente.

Las "estrellas cumulares" son aquellas que forman cúmulos estelares. Si el cúmulo es globular, las estrellas se atraen por gravedad (las estrellas se atraen mutuamente). Si el cúmulo es abierto, las estrellas se atraen por gravitación en donde el centro gravitacional es el centro de masa del cúmulo (las estrellas orbitan un centro gravitacional en común que las mantiene unidas). Las "estrellas independientes" son aquellas que no forman cúmulos estelares con ninguna otra estrella. Sin embargo hay estrellas independientes que sí forman parte de un sistema estelar pues orbitan estrellas o son centro de otras. Este sería el caso de estrellas sistémicas-independientes.

Las estrellas que forman parte de un sistema planetario se denominan "estrellas planetarias", entendiéndose por sistema planetario al conjunto de la estrella o sistema estelar central y los distintos cuerpos celestes (planetas, asteroides, cometas) que orbitan a su alrededor. Por contra, se denomina "estrellas únicas" a las que no poseen otros cuerpos que las orbiten.

Las estrellas variables tienen cambios periódicos o aleatorios en la luminosidad debido a propiedades intrínsecas o extrínsecas. De las estrellas intrínsecamente variables, los tipos primarios pueden subdividirse en tres grupos principales.

Durante su evolución estelar, algunas estrellas pasan por fases donde pueden convertirse en variables pulsantes. Las estrellas variables pulsantes varían en radio y luminosidad a lo largo del tiempo, expandiéndose y contrayéndose con períodos que van desde minutos a años, dependiendo del tamaño de la estrella. Esta categoría incluye a estrellas como las variables Cefeidas y similares a las Cefeidas, y variables de largo período, como Mira.

Las variables eruptivas son estrellas que experimentan aumentos repentinos de luminosidad debido a erupciones o eventos de eyección de masa. Este grupo incluye protoestrellas, estrellas de Wolf-Rayet y estrellas fulgurantes, así como también estrellas gigantes y supergigantes.

Las estrellas variables cataclísmicas o explosivas son aquellas que experimentan un cambio dramático en sus propiedades. Este grupo incluye las novas y las supernovas. Un sistema de estrellas binarias que incluya una enana blanca cercana puede producir ciertos tipos de estas espectaculares explosiones estelares, incluyendo la nova y una supernova tipo 1a. La explosión se crea cuando la enana blanca acumula el hidrógeno de la estrella compañera, adquiriendo masa hasta que el hidrógeno experimenta fusión. Algunas novas también son recurrentes, presentando brotes periódicos de amplitud moderada.

Las estrellas también pueden variar en luminosidad debido a factores extrínsecos, tales como las binarias eclipsantes, así como estrellas giratorias que producen manchas extremas. Un ejemplo notable de binaria eclipsante es Algol, que regularmente varía en magnitud de 2,3 a 3,5 durante un período de 2,87 días.

El interior de una estrella estable está en un estado de equilibrio hidrostático: las fuerzas sobre cualquier pequeño volumen se contrapesan casi exactamente entre sí. Las fuerzas equilibradas son la fuerza gravitacional hacia adentro y una fuerza hacia fuera debido al gradiente de presión dentro de la estrella. El gradiente de presión se establece mediante el gradiente de temperatura del plasma; la parte exterior de la estrella es más fría que el núcleo. La temperatura en el núcleo de una estrella de secuencia principal o estrella gigante es al menos del orden de 10K. La temperatura y la presión resultantes en el núcleo de combustión de hidrógeno de una estrella de secuencia principal son suficientes para que se produzca la fusión nuclear y para que se produzca suficiente energía para evitar un colapso adicional de la estrella.

A medida que los núcleos atómicos se fusionan en el núcleo, emiten energía en forma de rayos gamma. Estos fotones interactúan con el plasma circundante, agregando a la energía térmica en el núcleo. Las estrellas de la secuencia principal convierten el hidrógeno en helio, creando una proporción lenta pero constante de helio en el núcleo. Finalmente el contenido de helio se vuelve predominante, y cesa la producción de energía en el núcleo. En cambio, para las estrellas de más de 0,4"M", la fusión se produce en una capa de expansión lenta alrededor del núcleo de helio degenerado.

Además del equilibrio hidrostático, el interior de una estrella estable también mantendrá un balance energético de equilibrio térmico. Hay un gradiente de temperatura radial a través del interior que da lugar a un flujo de la energía que fluye hacia el exterior. El flujo saliente de energía que deja cualquier capa dentro de la estrella coincidirá exactamente con el flujo entrante desde abajo.

La zona de radiación es la región del interior estelar donde el flujo de energía hacia el exterior depende de la transferencia radiante de calor, ya que la transferencia de calor conectiva es ineficiente en esa zona. En esta región el plasma no será perturbado, y cualquier movimiento de masa se extinguirá. Sin embargo, si este no es el caso, entonces el plasma se vuelve inestable y se produce la convección, formando una zona convectiva . Esto puede ocurrir, por ejemplo, en regiones donde se producen flujos de energía muy elevados, como cerca del núcleo o en áreas con alta opacidad (haciendo ineficiente la transferencia radiativa de calor) como en el envolvente exterior.

La ocurrecia de convección en la envoltura externa de una estrella de secuencia principal depende de la masa de la estrella. Las estrellas con varias veces la masa del Sol tienen una zona de convección profunda en el interior y una zona radiativa en las capas externas. Las estrellas enanas rojas con menos de 0,4M son convectivas en todas partes, lo que previene la acumulación de un núcleo de helio. Para la mayoría de las estrellas, las zonas convectivas también varían con el tiempo, a medida que se modifican la edad y la constitución de las estrellas.
La fotosfera es la porción de una estrella que es visible para un observador. Esta es la capa en la que el plasma de la estrella se vuelve transparente a los fotones de luz. A partir de aquí, se libera la energía generada en el núcleo, para propagarse al espacio. Es dentro de la fotosfera donde aparecen manchas solares, regiones de temperatura inferior a la media. 

Por encima del nivel de la fotosfera está la atmósfera estelar. En una estrella de secuencia principal como el Sol, el nivel más bajo de la atmósfera, justo por encima de la fotosfera, es la región delgada de la cromosfera, donde aparecen espículas y también donde comienzan las fulguraciones estelares.

Por encima de ella está la región de transición, donde aumenta rápidamente la temperatura a una distancia de solo . Más allá está la corona, un volumen de plasma sobrecalentado que puede extenderse hacia afuera hasta varios millones de kilómetros. A pesar de su alta temperatura, la corona emite muy poca luz, debido a su baja densidad de gas. Normalmente, la región de la corona del Sol solo es visible durante un eclipse solar.

Desde la corona, se expande un viento estelar de partículas de plasma hacia fuera desde la estrella, hasta que interactúa con el medio interestelar. Para el Sol, la influencia de su viento solar se extiende a lo largo de una región en forma de burbuja llamada heliosfera.

En los núcleos de las estrellas tienen lugar una variedad de reacciones de fusión nuclear que dependen de su masa y composición. Cuando se fusionan los núcleos, la masa del producto fusionado es menor que la masa de las partes originales. Esta masa perdida se convierte en energía electromagnética, de acuerdo con la relación de equivalencia entre masa y energía "E"="mc".

El proceso de fusión de hidrógeno es sensible a la temperatura, por lo que un aumento moderado en la temperatura del núcleo dará lugar a un aumento significativo en la tasa de fusión. Como resultado, la temperatura central de las estrellas de secuencia principal solo varía de 4 millones de grados kelvin para una estrella de clase M pequeña a 40 millones de grados kelvin para una estrella masiva de clase O.

En el núcleo del Sol, con un núcleo de 10 millones de grados kelvin, el hidrógeno se fusiona para formar helio mediante la cadena protón-protón:

Estas reacciones quedan reducidas en la reacción global:

Donde e + es un positrón, γ es un fotón de rayos gamma, νe es un neutrino, y H y He son isótopos de hidrógeno y helio, respectivamente. La energía liberada por esta reacción está en millones de electronvoltios, que en realidad solo es una pequeña cantidad de energía. Sin embargo, ocurren constantemente un número enorme de estas reacciones, produciendo toda la energía necesaria para sostener la salida de radiación de la estrella. En comparación, la combustión de dos moléculas de gas hidrógeno con una molécula de gas oxígeno solo libera 5,7eV.
En estrellas más masivas el helio se produce en un ciclo de reacciones catalizadas por el carbono, es el ciclo CNO o ciclo de Bethe. 

En estrellas cuyos núcleos se encuentran a 100 millones de grados K y cuyas masas van desde 0,5 a las 10"M", el helio resultante de las primeras reacciones puede transformarse en carbono mediante del proceso triple-alfa:

La reacción global es:

En las estrellas masivas, los elementos más pesados también se pueden producir combustión en un núcleo de contracción mediante los procesos de combustión de neón y de combustión de oxígeno. La fase final del proceso de nucleosíntesis estelar es el proceso de combustión del silicio que da como resultado la producción del hierro isotópico estable-56, un proceso endotérmico que consume energía, por lo que solo se puede producir energía adicional a través del colapso gravitacional.

El ejemplo siguiente muestra la cantidad de tiempo requerida para que una estrella de 20"M" consuma todo su combustible nuclear. Como estrella de la secuencia principal de clase O, sería 8 veces el radio solar y 62000 veces la luminosidad del Sol.






</doc>
<doc id="3366" url="https://es.wikipedia.org/wiki?curid=3366" title="Premio Nobel de Química">
Premio Nobel de Química

El Premio Nobel de Química ha sido entregado desde 1901 por la Real Academia de las Ciencias de Suecia. 160 científicos han sido laureados con este premio hasta 2018. En la actualidad (2019) está dotado con 10 millones de coronas suecas (1.5 millones de dólares). Frederick Sanger lo recibió en dos ocasiones: en 1958 y en 1980.




</doc>
<doc id="3367" url="https://es.wikipedia.org/wiki?curid=3367" title="Disolución">
Disolución

Una disolución es una mezcla homogénea a nivel molecular o iónico de dos o más sustancias puras que no reaccionan entre sí, cuyos componentes se encuentran en proporciones variables. También se puede definir como una mezcla homogénea formada por un disolvente y por uno o varios solutos.

Un ejemplo común podría ser un sólido disuelto en un líquido, como la sal o el azúcar disueltos en agua; o incluso el oro en mercurio, formando una amalgama. También otros ejemplos de disoluciones son el vapor de agua en el aire, el hidrógeno en paladio o cualquiera de las aleaciones existentes.

El término también es usado para hacer referencia al proceso de disolución. 

Una disolución es una mezcla homogénea de sustancias puras. Frecuentemente formada por un "solvente", "disolvente", "dispersante" o "medio de dispersión", medio en el que se disuelven uno o más solutos. Los criterios para decidir cuál es el disolvente y cuáles los solutos son más o menos arbitrarios; no hay una razón científica para hacer tal distinción.

Wilhelm Ostwald distingue tres tipos de mezclas según el tamaño de las partículas de soluto en la disolución:

Estas últimas se clasifican en:





A continuación se presenta un cuadro con ejemplos de disoluciones clasificadas por su estado de agregación donde se muestran todas las combinaciones posibles:

Por su concentración, la disolución puede ser analizada en términos cuantitativos o cualitativos dependiendo de su estado.

También llamadas disoluciones cualitativas, esta clasificación no toma en cuenta la cantidad numérica de soluto y disolvente presentes, y dependiendo de la proporción entre ellos se clasifican de la siguiente manera:


A diferencia de las empíricas, las disoluciones valoradas cuantitativamente, sí toman en cuenta las cantidades numéricas exactas de soluto y solvente que se utilizan en una disolución. Este tipo de clasificación es muy utilizada en el campo de la ciencia y la tecnología, pues en ellas es muy importante una alta precisión.

Existen varios tipos de disoluciones valoradas:

En función de la naturaleza de solutos y solventes, las leyes que rigen las disoluciones son distintas.




</doc>
<doc id="3368" url="https://es.wikipedia.org/wiki?curid=3368" title="Química analítica">
Química analítica

La química analítica estudia y utiliza instrumentos y métodos para separar, identificar y cuantificar la materia. En la práctica, la separación, identificación o cuantificación puede constituir el análisis completo o combinarse con otro método. La separación aísla los analitos. El análisis cualitativo identifica los analitos, mientras que el análisis cuantitativo determina la cantidad o concentración numérica. 

La química analítica consiste en métodos químicos clásicos, húmedos y métodos instrumentales modernos. Los métodos cualitativos clásicos usan separaciones como la precipitación, extracción y destilación. La identificación puede basarse en las diferencias de color, olor, punto de fusión, punto de ebullición, radioactividad o reactividad. El análisis cuantitativo clásico utiliza cambios de masa o volumen para cuantificar la cantidad. Se pueden utilizar métodos instrumentales para separar muestras mediante cromatografía, electroforesis o fraccionamiento de flujo de campo. Luego, se puede realizar un análisis cualitativo y cuantitativo, a menudo con el mismo instrumento y puede usar interacción de luz, interacción de calor , campos eléctricos o campos magnéticos. A menudo, el mismo instrumento puede separar, identificar y cuantificar un analito. 

La química analítica también se centra en las mejoras en el diseño experimental, la quimiometría y la creación de nuevas herramientas de medición. La química analítica tiene amplias aplicaciones para la medicina forense, la medicina, la ciencia y la ingeniería.

La química analítica ha sido importante desde los primeros días de la química, ya que proporciona métodos para determinar qué elementos y sustancias químicas están presentes en la muestra en cuestión. Durante este período, las contribuciones significativas a la química analítica incluyen el desarrollo del análisis elemental sistemático por Justus von Liebig y el análisis orgánico sistematizado basado en las reacciones específicas de los grupos funcionales. 

El primer análisis instrumental fue la espectrometría de emisión de llama desarrollada por Robert Bunsen y Gustav Kirchhoff, quien descubrió el rubidio (Rb) y el cesio (Cs) en 1860. 

La mayoría de los principales desarrollos en química analítica tuvieron lugar después de 1900. Durante este período, el análisis instrumental se vuelve progresivamente dominante en el campo. En particular, muchas de las técnicas espectroscópicas y espectrométricas básicas se descubrieron a principios del siglo XX y se refinaron a finales del siglo XX. 

Las ciencias de la separación siguen una línea temporal similar de desarrollo y también se transforman cada vez más en instrumentos de alto rendimiento. En la década de 1970, muchas de estas técnicas comenzaron a usarse juntas como técnicas híbridas para lograr una caracterización completa de las muestras. 

Comenzando aproximadamente en la década de los 70 hasta la actualidad, la química analítica se ha ido haciendo cada vez más inclusiva de las cuestiones biológicas (química bioanalítica), mientras que anteriormente se había centrado en gran medida en moléculas orgánicas inorgánicas o pequeñas. Los láseres se han utilizado cada vez más en la química como sondas e incluso para iniciar e influir en una amplia variedad de reacciones. A finales del siglo XX también se observó una expansión de la aplicación de la química analítica de las cuestiones químicas académicas a las cuestiones forenses, ambientales, industriales y médicas, como en la histología. 

La química analítica moderna está dominada por el análisis instrumental. Muchos químicos analíticos se centran en un solo tipo de instrumento. Los académicos tienden a centrarse en nuevas aplicaciones y descubrimientos o en nuevos métodos de análisis. El descubrimiento de un químico presente en la sangre que aumenta el riesgo de cáncer sería un descubrimiento en el que podría estar involucrado un químico analítico. Un esfuerzo por desarrollar un nuevo método podría implicar el uso de un láser sintonizable para aumentar la especificidad y la sensibilidad de un método espectrométrico. Muchos métodos, una vez desarrollados, se mantienen deliberadamente estáticos para que los datos puedan compararse durante largos períodos de tiempo. Esto es particularmente cierto en el aseguramiento de la calidad industrial (QA), aplicaciones forenses y ambientales. La química analítica desempeña un papel cada vez más importante en la industria farmacéutica donde, además del control de calidad, se utiliza en el descubrimiento de nuevos candidatos a fármacos y en aplicaciones clínicas donde la comprensión de las interacciones entre el fármaco y el paciente es fundamental.

Aunque la química analítica moderna está dominada por la instrumentación sofisticada, las raíces de la química analítica y algunos de los principios utilizados en los instrumentos modernos provienen de técnicas tradicionales, muchas de las cuales aún se utilizan en la actualidad. Estas técnicas también tienden a formar la columna vertebral de la mayoría de los laboratorios educativos de química analítica de pregrado. 

Un análisis cualitativo determina la presencia o ausencia de un compuesto en particular, pero no la masa o la concentración. Por definición, los análisis cualitativos no miden la cantidad. 

Existen numerosas pruebas químicas cualitativas, por ejemplo, la prueba de ácido para el oro y la prueba de Kastle-Meyer para detectar la presencia de sangre . 

El análisis cualitativo inorgánico generalmente se refiere a un esquema sistemático para confirmar la presencia de ciertos iones o elementos, generalmente acuosos, al realizar una serie de reacciones que eliminan los rangos de posibilidades y luego confirman los iones sospechosos con una prueba de confirmación. A veces se incluyen pequeños iones que contienen carbono en tales esquemas. Con la instrumentación moderna, estas pruebas rara vez se usan, pero pueden ser útiles para fines educativos y en trabajos de campo u otras situaciones en las que el acceso a instrumentos de última generación no está disponible o no es conveniente. 

El análisis cuantitativo es la medida de las cantidades de constituyentes químicos particulares presentes en una sustancia. 

El análisis gravimétrico implica determinar la cantidad de material presente pesando la muestra antes y / o después de alguna transformación. Un ejemplo común utilizado en la educación de pregrado es la determinación de la cantidad de agua en un hidrato calentando la muestra para eliminar el agua de tal manera que la diferencia de peso se deba a la pérdida de agua. 

La titulación implica la adición de un reactivo a una solución que se está analizando hasta que se alcanza algún punto de equivalencia. A menudo se puede determinar la cantidad de material en la solución que se está analizando. Lo más familiar para aquellos que han tomado química durante la educación secundaria es la titulación ácido-base que implica un indicador de cambio de color. Hay muchos otros tipos de titulaciones, por ejemplo titulaciones potenciométricas. Estas titulaciones pueden usar diferentes tipos de indicadores para alcanzar algún punto de equivalencia. 

La espectroscopia mide la interacción de las moléculas con la radiación electromagnética. La espectroscopia consta de muchas aplicaciones diferentes, como la espectroscopia de absorción atómica, la espectroscopia de emisión atómica, la espectroscopia ultravioleta-visible, la espectroscopia de fluorescencia de rayos X, la espectroscopia infrarroja , la espectroscopia Raman, la interferometría de polarización dual, la espectroscopia de polarización magnética , la espectroscopia de resonancia magnética nuclear, la espectroscopia de fotoemisión, la espectroscopia de fotodisposición, espectroscopía de microscopía de Motsensis. 

La espectrometría de masas mide la relación masa-carga de las moléculas mediante campos eléctricos y magnéticos. Existen varios métodos de ionización: impacto de electrones, ionización química, electropulverización, bombardeo con átomos rápidos, ionización por desorción láser asistida por matriz, y otros. Además, la espectrometría de masas se clasifica según los enfoques de los analizadores de masas: sector magnético, analizador de masas cuadrupolo, trampa de iones cuadrupolo, tiempo de vuelo, resonancia de ciclotrón de ión de transformada de Fourier, etc.

Los métodos electroanalíticos miden el potencial ( voltios ) y / o la corriente (amperios) en una celda electroquímica que contiene el analito. Estos métodos se pueden clasificar de acuerdo con los aspectos de la celda que se controlan y los que se miden. Las cuatro categorías principales son potenciometría (se mide la diferencia en los potenciales de los electrodos), coulometría (la carga transferida se mide con el tiempo), amperimetría (la corriente de la celda se mide con el tiempo) y voltametría (la corriente de la celda se mide mientras se modifica activamente potencial de la célula).

La calorimetría y el análisis termogravimétrico miden la interacción de un material y el calor. 

Los procesos de separación se utilizan para disminuir la complejidad de las mezclas de materiales. La cromatografía, la electroforesis y el fraccionamiento de flujo de campo son representativos de este campo. 

Las combinaciones de las técnicas anteriores producen una técnica "híbrida" o "con guion" Varios ejemplos son de uso popular hoy en día y se están desarrollando nuevas técnicas híbridas. Por ejemplo, la , la cromatografía de gases-espectroscopia infrarroja, la , la cromatografía líquida-espectroscopía de RMN, la cromatografía líquida, la espectroscopía infrarroja, la electroforesis capilar y la espectrometría de masas.

Las técnicas de separación con guion se refieren a una combinación de dos (o más) técnicas para detectar y separar los productos químicos de las soluciones. Muy a menudo, la otra técnica es alguna forma de cromatografía. Las técnicas con guiones se utilizan ampliamente en química y bioquímica. A veces se usa una barra oblicua en lugar de un guion, especialmente si el nombre de uno de los métodos contiene un guion.

La visualización de moléculas individuales, células individuales, tejidos biológicos y nanomateriales es un enfoque importante y atractivo en la ciencia analítica. Además, la hibridación con otras herramientas analíticas tradicionales está revolucionando la ciencia analítica. La microscopía se puede clasificar en tres campos diferentes: microscopía óptica, microscopía electrónica y microscopía con sonda de barrido. Recientemente, este campo está progresando rápidamente debido al rápido desarrollo de las industrias de computadoras y cámaras. 

Dispositivos que integran (múltiples) funciones de laboratorio en un solo chip de solo milímetros a unos pocos centímetros cuadrados de tamaño y que son capaces de manejar volúmenes de fluido extremadamente pequeños hasta menos que picolitros. 

El error se puede definir como una diferencia numérica entre el valor observado y el valor verdadero. 

Por error, el valor verdadero y el valor observado en el análisis químico se pueden relacionar entre sí mediante la ecuación 

donde 


El error de una medición es una medida inversa de una medición precisa, es decir, cuanto menor sea el error, mayor será la precisión de la medición. 

Los errores se pueden expresar relativamente. Dado el error relativo ( formula_5): 

El porcentaje de error también se puede calcular: 

Si queremos usar estos valores en una función, también podemos querer calcular el error de la función. Dejar formula_8ser una función con formula_9variables Por lo tanto, la propagación de la incertidumbre se debe calcular para conocer el error en formula_8: 

Un método general para el análisis de la concentración implica la creación de una curva de calibración. Esto permite determinar la cantidad de un producto químico en un material al comparar los resultados de una muestra desconocida con los de una serie de estándares conocidos. Si la concentración de elemento o compuesto en una muestra es demasiado alta para el rango de detección de la técnica, simplemente se puede diluir en un disolvente puro. Si la cantidad en la muestra está por debajo del rango de medición de un instrumento, se puede utilizar el método de adición. En este método, se agrega una cantidad conocida del elemento o compuesto en estudio, y la diferencia entre la concentración agregada y la concentración observada es la cantidad realmente en la muestra. 

A veces, se agrega un estándar interno en una concentración conocida directamente a una muestra analítica para ayudar en la cuantificación. La cantidad de analito presente se determina entonces en relación con el estándar interno como calibrante. Un estándar interno ideal es el analito enriquecido con isótopos que da lugar al método de dilución de isótopos. 

El método de adición estándar se usa en el análisis instrumental para determinar la concentración de una sustancia (analito) en una muestra desconocida en comparación con un conjunto de muestras de concentración conocida, similar al uso de una curva de calibración. La adición estándar se puede aplicar a la mayoría de las técnicas analíticas y se usa en lugar de una curva de calibración para resolver el problema del efecto de matriz. 

Uno de los componentes más importantes de la química analítica es maximizar la señal deseada y minimizar el ruido asociado. La figura analítica de mérito se conoce como la relación señal / ruido (S/N o SNR). 

El ruido puede surgir de factores ambientales, así como de procesos físicos fundamentales. 

El ruido térmico es el resultado del movimiento de los portadores de carga (generalmente electrones) en un circuito eléctrico generado por su movimiento térmico. El ruido térmico es ruido blanco, lo que significa que la densidad espectral de potencia es constante en todo el espectro de frecuencias . 

El valor cuadrático medio del ruido térmico en una resistencia viene dado por 

donde "k" es la constante de Boltzmann, "T" es la temperatura, "R" es la resistencia y formula_13 es el ancho de banda de la frecuencia formula_14. 

El ruido de disparo es un tipo de ruido electrónico que se produce cuando el número finito de partículas (como electrones en un circuito electrónico o fotones en un dispositivo óptico) es lo suficientemente pequeño como para dar lugar a fluctuaciones estadísticas en una señal. 

El ruido de disparo es un proceso de Poisson y los portadores de carga que forman la corriente siguen una distribución de Poisson. La fluctuación de la corriente cuadrática media está dada por 

donde "e" es la carga elemental y "I" es la corriente promedio. El ruido de disparo es el ruido blanco. 

El ruido de parpadeo es un ruido electrónico con un espectro de frecuencia de 1/"ƒ"; A medida que aumenta "f" , el ruido disminuye. El ruido de parpadeo surge de una variedad de fuentes, como las impurezas en un canal conductor, la generación y el ruido de recombinación en un transistor debido a la corriente de base, y así sucesivamente. Este ruido se puede evitar mediante la modulación de la señal a una frecuencia más alta, por ejemplo, mediante el uso de un amplificador de bloqueo . 

El ruido ambiental surge del entorno del instrumento analítico. Las fuentes de ruido electromagnético son líneas eléctricas, estaciones de radio y televisión, dispositivos inalámbricos, lámparas fluorescentes compactas y motores eléctricos. Muchas de estas fuentes de ruido tienen un ancho de banda limitado y, por lo tanto, pueden evitarse. Es posible que se requiera aislamiento de temperatura y vibración para algunos instrumentos. 

La reducción de ruido se puede lograr en hardware o software de computadora. Ejemplos de reducción de ruido de hardware son el uso de cable blindado , filtrado analógico y modulación de señal. Los ejemplos de reducción de ruido del software son el filtrado digital, el promedio de conjunto, el promedio de vagones y los métodos de correlación. 

La química analítica tiene aplicaciones que incluyen ciencias forenses, bioanálisis, análisis clínico, análisis ambiental y análisis de materiales. La investigación en química analítica está impulsada en gran medida por el rendimiento (sensibilidad, límite de detección, selectividad, robustez, rango dinámico, rango lineal , precisión, precisión y velocidad) y costo (compra, operación, entrenamiento, tiempo y espacio). Entre las principales ramas de la espectrometría atómica analítica contemporánea, las más difundidas y universales son la espectrometría de masas y óptica. En el análisis elemental directo de muestras sólidas, los nuevos líderes son la degradación inducida por láser y la espectrometría de masas de ablación con láser , y las técnicas relacionadas con la transferencia de los productos de ablación con láser al plasma acoplado inductivamente . Los avances en el diseño de láseres de diodo y osciladores paramétricos ópticos promueven desarrollos en la espectrometría de fluorescencia e ionización y también en técnicas de absorción donde se espera que se expandan los usos de las cavidades ópticas para aumentar la longitud de la ruta de absorción efectiva. El uso de métodos basados en plasma y láser está aumentando. El interés por el análisis absoluto (sin normas) ha revivido, particularmente en espectrometría de emisiones. Se está haciendo un gran esfuerzo para reducir las técnicas de análisis al tamaño del chip. Aunque hay pocos ejemplos de sistemas de este tipo que compitan con las técnicas de análisis tradicionales, las ventajas potenciales incluyen tamaño/portabilidad, velocidad y costo (sistema de análisis micro total (µTAS) o laboratorio en un chip). La química a microescala reduce las cantidades de productos químicos utilizados. 

Muchos desarrollos mejoran el análisis de los sistemas biológicos. Algunos ejemplos de campos en rápida expansión en esta área son la genómica, la secuenciación del ADN y la investigación relacionada con la identificación genética y la micromatriz de ADN; proteómica , el análisis de las concentraciones y modificaciones de proteínas, especialmente en respuesta a diversos factores estresantes, en diversas etapas de desarrollo, o en diversas partes del cuerpo, metabolómica, que se ocupa de los metabolitos; transcriptómica, incluyendo ARNm y campos asociados; lipidomica - lípidos y sus campos asociados; peptidómica - péptidos y sus campos asociados; y metalómica , que trata de las concentraciones de metales y, especialmente, de su unión a proteínas y otras moléculas. La química analítica ha jugado un papel fundamental en la comprensión de la ciencia básica para una variedad de aplicaciones prácticas, como aplicaciones biomédicas, monitoreo ambiental, control de calidad de la fabricación industrial, ciencia forense, etc. 

Los desarrollos recientes de la automatización informática y las tecnologías de la información han extendido la química analítica a varios campos biológicos nuevos. Por ejemplo, las máquinas de secuenciación de ADN automatizadas fueron la base para completar los proyectos del genoma humano que llevaron al nacimiento de la genómica . La identificación de proteínas y la secuenciación de péptidos por espectrometría de masas abrió un nuevo campo de la proteómica . 

La química analítica ha sido un área indispensable en el desarrollo de la nanotecnología . Los instrumentos de caracterización de superficie, los microscopios electrónicos y los microscopios de sonda de barrido permiten a los científicos visualizar estructuras atómicas con caracterizaciones químicas.




</doc>
<doc id="3369" url="https://es.wikipedia.org/wiki?curid=3369" title="Dmitri Mendeléyev">
Dmitri Mendeléyev

Dmitri Ivánovich Mendeléyev (en ruso: ; Tobolsk, -San Petersburgo, ) fue un químico ruso, conocido por haber descubierto el patrón subyacente en lo que ahora se conoce como la tabla periódica de los elementos.

Sobre las bases del análisis espectral establecido por los alemanes Robert Bunsen y Gustav Kirchhoff, se ocupó de problemas químico-físicos relacionados con el espectro de emisión de los elementos. Realizó las determinaciones de volúmenes específicos y analizó las condiciones de licuefacción de los gases, así como también el origen de los petróleos.

Su investigación principal fue la que dio origen a la enunciación de la ley periódica de los elementos, base del sistema periódico que lleva su nombre. Tuvo influencia sobre él artículo de 1858 "Sunto di un corso di Filosofia Chimica" de Stanislao Cannizzaro. En 1869 publicó su libro "Principios de la química", en el que desarrollaba la teoría de la tabla periódica.

Dmitri Ivánovich Mendeléyev era el menor de al menos 17 hermanos de la familia formada por Iván Pávlovich Mendeléyev y María Dmítrievna Mendeléyeva. En el mismo año en que nació, su padre quedó ciego perdiendo así su trabajo, era el director del colegio del pueblo. Uno de los mayores rasgos físicos era su enorme barba la cual según dicen los historiadores solo se afeitaba una vez al año.

Recibían una pensión insuficiente, por lo que la madre tuvo que tomar las riendas de la familia y dirigir la fábrica de cristal que había fundado su abuelo. Desde joven destacó en ciencias en la escuela, no así en ortografía. Un cuñado suyo, exiliado por motivos políticos, y un químico de la fábrica le inculcaron el amor por las ciencias.

La familia sufrió, ya que Dmitri solo terminó el bachillerato, su padre murió y la fábrica de cristal que su madre dirigía, se quemó. Ésta apostó por invertir los ahorros en la educación de Dmitri, en vez de reconstruir la fábrica. En esa época la mayoría de los hermanos, excepto una hermana, ya se habían independizado, y la madre se los llevó a Moscú para que Dmitri pudiese ingresar a la Universidad. Sin embargo, Mendeléyev, no fue admitido; su origen siberiano le cerró las puertas de las universidades de Moscú y San Petersburgo, por lo que se formó en el Instituto Pedagógico de esta última ciudad.

En 1862 se casó, obligado por su hermana, con Feozva Nikítichna Leschiova con la que tuvo tres hijos, uno de los cuales falleció. Este fue un matrimonio infeliz y desde 1871 vivieron separados. Fue acusado de bígamo, pues una vez divorciado de su esposa volvió a contraer matrimonio con ella, sin esperar los siete años que exigía la legislación rusa, aunque tuvo la suerte de que la pena recayó sobre el párroco que los había casado.

Encontró la felicidad casándose con Anna Ivánovna Popova, 26 años menor que él. Para lograrlo, Mendeléyev estuvo cuatro años desesperado, incluso llegó a caer en una depresión, debido a que su mujer se negaba a concederle el divorcio y la familia de Anna se oponía tajantemente. A punto de darse por vencido, consiguió el divorcio de su esposa y fue en busca de Anna que se encontraba en Roma. En 1882 contrajeron matrimonio. Tuvieron cuatro hijos, la mayor de los cuales, Liubov, se casaría con el poeta ruso Aleksandr Blok.

Aunque es más conocido en Occidente por haber creado la Tabla periódica de los elementos, la contribución de Dmitri Mendeléyev al desarrollo de Rusia fue muy vasta y por ello es reconocido como una verdadera personalidad del Renacimiento ruso. Sus campos de estudio variaron desde la química hasta la aeronáutica.

Su amplio conocimiento lo llevó a convertirse en una figura influyente entre sus contemporáneos, fue asesor del ministro de Hacienda de Rusia, Serguéi Witte, y escribió más de 70 artículos sobre el desarrollo económico y social del país.

Mendeléyev fue uno de los más grandes maestros de su tiempo y se le atribuye el mérito de haber educado a miles de estudiantes.
Falleció el 2 de febrero de 1907, casi ciego. Se considera a Mendeléyev un genio, no solo por el ingenio que mostró para aplicar todo lo conocido y predecir lo no conocido sobre los elementos químicos y plasmarlo en la tabla periódica, sino por los numerosos trabajos realizados a lo largo de toda su vida en diversos campos de la ciencia, agricultura, ganadería, industria y petróleo.

Presentó la tesis "Sobre volúmenes específicos" para conseguir la plaza de maestro de escuela, y la tesis "Sobre la estructura de las combinaciones silíceas" para alcanzar la plaza de cátedra de química en la Universidad de San Petersburgo. A los 23 años era ya encargado de un curso de dicha universidad.

Gracias a una beca, pudo ir a Heidelberg, donde realizó diferentes investigaciones junto a Gustav Kirchhoff y Robert Bunsen, y publicó un artículo sobre "La cohesión de algunos líquidos y sobre el papel de la cohesión molecular en las reacciones químicas de los cuerpos”. Este trabajo lo pudo realizar gracias a unos aparatos de precisión encargados en París con los cuales encontró la temperatura absoluta de ebullición, y descubrió por qué algunos gases no se podían licuar (porque se encontraban por encima de la temperatura de ebullición).

Participó en el congreso de Karlsruhe donde quedó impresionado por las ideas sobre el peso de los elementos que planteó Cannizzaro. Al volver a San Petersburgo se encontró sin trabajo fijo, lo que le dio tiempo para escribir diferentes obras. Entre las cuales destaca su libro "Química orgánica", que escribió influido por lo que había escuchado en Karlsruhe.

Sobre la personalidad de Mendeléyev se puede decir que era un adicto al trabajo y su fama de mal carácter estaba basada en que mientras trabajaba, gritaba, gruñía y refunfuñaba. Se dice que alguien le preguntó sobre su mal genio, a lo que contestó que era una manera de mantenerse sano y no contraer úlcera.

En 1864 fue nombrado profesor de tecnología y química del Instituto Técnico de San Petersburgo. En 1867, ocupó la cátedra de química en la Universidad de San Petersburgo, donde estudió el isomorfismo, la compresión de los gases y las propiedades del aire enrarecido.

Permanecería en esta cátedra 23 años. Mendeléyev estaba a favor de la introducción de reformas en el sistema educativo ruso. No consiguió ser elegido presidente de la academia imperial de ciencias, debido a su liberalismo.

En 1890, terminó su estancia en la universidad, debido a que intercedió por los estudiantes, y entregó a Iván Deliánov, ministro de Instrucción Pública, una carta dirigida al zar Alejandro III de Rusia. El ministro se la devolvió con una nota adjunta que decía:

Indignado, Dmitri dejó las aulas de la universidad. Quizá por esto, se mantuvo desde entonces al margen de la política y del Estado, aunque manifestaba sus ideas liberales y su oposición a la opresión.

En 1865, tras la liberación de los siervos producida en 1861, decidió comprar una granja en la que puso en práctica métodos científicos para la mejora de la cosecha y tuvo una relación humanitaria con los campesinos. Obtuvo un rendimiento muy por encima de lo que se producía antes, por lo que muchos campesinos de granjas cercanas fueron a pedir su consejo.

En 1869, publicó la más importante de sus obras, "Principios de química", donde formulaba su famosa tabla periódica, traducida a multitud de lenguas y que fue libro de texto durante muchos años. Fue defensor de la ciencia aplicada y de los estudios para mejorar las técnicas de producción industrial en numerosos ámbitos. Contribuyó a la construcción de la primera refinería petrolera de Rusia, planteó las primeras teorías sobre el origen del petróleo y llegó a pronosticar que este recurso se convertirá en un componente clave de la economía mundial. En 1863, fue el primero en sugerir la idea de utilizar tuberías para el transporte de combustible. Impulsó la importancia del petróleo como materia prima para la petroquímica. Se le atribuye la afirmación de que la quema de petróleo como combustible "sería similar a prender una estufa de cocina con los billetes de banco".

En 1876, fue enviado a Estados Unidos, para informarse sobre la extracción del petróleo y ponerla luego en práctica en el Cáucaso. El estudio del refino del petróleo lo llevó a investigar el fenómeno de la atracción de las moléculas de cuerpos homogéneos o diferentes, materia que estudió hasta el día de su muerte. En 1887, publicó "Estudio de las disoluciones acuosas según el peso específico", donde concluye que las soluciones contienen asociaciones de moléculas hidratadas en un estado de equilibrio móvil, que se disocian de diferentes maneras siguiendo el tanto por ciento de concentración.

En 1887, emprendió un viaje en globo en solitario para estudiar un eclipse solar. El aparato estaba destinado a levantarse a suficiente altura para ofrecer una visión sin obstrucciones de un eclipse solar al científico y a un piloto. Una oportunidad única para estudiar la corona solar. Sin embargo, el día del evento llovió, echando a perder todos los planes de observación. A pesar de esto, se dice que Mendeléyev sacó al piloto y demás cosas de la canasta para poder realizar el vuelo.

Científicamente, este viaje no tuvo importancia alguna. El aerostato no logró sobrevolar las nubes pero fue un éxito como reclamo publicitario. La dramática historia de un famoso científico que arriesga su vida y se ve a obligado a realizar reparaciones para realizar el primer vuelo en globo aerostático fue tan audaz que la Academia de meteorología francesa le otorgó una medalla. Cabe mencionar que también fue este ámbito uno de los muchos en los que mostró interés el genio ruso.

Además incentivó el uso de fertilizantes en la agricultura y experimentó con varios de ellos en su propia finca. Mejoró e inventó varios instrumentos, entre ellos un aparato para medir la densidad de los líquidos. Fue director de la Oficina de Pesos y Medidas de Rusia, e influyó en la transición del país al sistema métrico. En 1889, fue nombrado miembro honorario del Consejo de Comercio y Manufacturas.

En 1890, por un encargo del Ministerio de Guerra y Marina, preparó una pólvora sin humo al pirocolodión. A petición de la Armada rusa, realizó estudios sobre la experiencia europea en la producción de pólvora sin humo y desarrolló su propia fórmula, denominada “pyrocollodion”, así como también ayudó al desarrollo de la industria del país. No se sabe por qué la fórmula no se adoptó en Rusia y en su lugar se exportó técnica francesa. Hay quienes sostienen que fue debido a la competencia entre funcionarios militares; otros citan los estrictos requisitos para el proceso propuesto por Mendeléyev. Sin embargo, se produjo y se importó una especie de pólvora muy parecida a la suya a gran escala en los Estados Unidos durante la Primera Guerra Mundial, incluso al posible país de origen.

En 1892, fue nombrado conservador científico de la Oficina de Pesas y Medidas, en compensación por lo ocurrido en la universidad. Después de un año, tras haberlo reorganizado, fue nombrado director, lo que lo comprometió a realizar diversos viajes, entre los que se encuentra el realizado a Londres, donde recibió los doctorados "honoris causa" de las universidades de Cambridge y Oxford.

En 1902, viajó a París y visitó a Marie y Pierre Curie, en su laboratorio. Observó el experimento de la fosforescencia del sulfuro de zinc, debida a los rayos X, y concluyó que “en los cuerpos radiactivos existe un gas etéreo que provoca vibraciones luminosas y que entra y sale de los cuerpos como un cometa entra y sale del sistema solar”.

No lo terminó de convencer la teoría de la radiactividad y la estructura del átomo. Consideraba la radiactividad como una propiedad o un estado de las sustancias, mientras que los átomos y moléculas no existían realmente, aunque sí lo hacía la energía. Viajó por toda Europa visitando a diversos científicos.

Hizo aportes a la investigación y construcción naval y a los viajes marítimos al Ártico. Participó en la elaboración de la navegación por el Ártico y en la creación de nuevos tipos de buques rusos. Asimismo, participó en el diseño del Yermak, el primer rompehielos del Ártico. La idea de hacer que los territorios del norte de Rusia fueran accesibles por mar era muy atractiva para el científico ilustrado.

En Rusia, nunca se lo reconoció, debido a sus ideas liberales, por lo que nunca fue admitido en la Academia Rusa de las Ciencias. Sin embargo, en 1955 se nombró mendelevio (Md) al elemento químico de número atómico 101, en su honor.

Dmitri Mendeléyev estuvo a punto de conseguir un Premio Nobel, circunstancia que finalmente se le resistió. El Comité Nobel de Química recomendó a la Academia Sueca de Ciencias que el máximo galardón se le concediera al creador de la tabla periódica y, a pesar de que son muy pocas las ocasiones en las que la Academia ignora las recomendaciones del Comité, desafortunadamente esta fue una de ellas. En 1906, la casi totalidad de los miembros de la Real Academia de las Ciencias de Suecia estaban de acuerdo en que el justo merecedor del Premio Nobel de Química de ese año debía ser el ruso Dmitri Mendeléyev, al que habían hecho miembro de la Academia un año antes y a quien consideraban como una de las mentes más brillantes, por lo que debía de ser recompensando (entre otras muchas cosas) por poner los cimientos de la tabla periódica de elementos.

Inesperadamente, poco antes de ser anunciado el nombre de Mendeléyev, como ganador del Premio Nobel, la Academia cambió de opinión y se lo concedió al químico francés Henri Moissan. El motivo de esa repentina y sorprendente decisión estuvo causada por la intervención de Peter Klason, académico que discrepó de la conveniencia de otorgar el Nobel al químico ruso por algo que había realizado cuatro décadas atrás (en 1869), por lo que propuso el nombre de Moissan por sus "investigaciones sobre el aislamiento del flúor". Pero quien realmente estaba detrás del empeño de que a Dmitri Mendeléyev no se le concediera el Premio Nobel y había convencido a Klason para que fuese discordante con el resto de académicos fue Svante August Arrhenius, ganador del Premio Nobel en 1903 por la teoría de la disociación electrolítica.

Arrhenius, a pesar de no ser miembro de la Academia, tenía mucha influencia entre varios de sus colegas, y poco a poco logró ir convenciéndolos gracias a la inestimable colaboración que le prestó Peter Klason. Y es que todo ese empeño para que a Mendeléyev no se le concediera el premio venía originado por un conflicto personal que Arrhenius tenía desde hacía tres años atrás, cuando le concedieron a él el Nobel y su colega ruso criticó duramente y en público su teoría de la disociación electrolítica. Un año después de que se truncara ese reconocimiento a su labor, el científico murió.

Dmitri Mendeléyev nació y creció en la tradicional e inmovilista Rusia de los zares, y siempre estuvo señalado dentro de su país, por entonces todo un imperio, como una persona liberal, algo que le perjudicó dentro de sus fronteras. Creció en la fe ortodoxa, aunque su madre le animó desde pequeño a "buscar la verdad divina paciencia y científica". Más adelante, abandonó esta fe y abrazó el deísmo, que acepta el conocimiento de Dios a través de la razón y la experiencia personal, en lugar a través de la revelación directa, la fe o la tradición.

La economía y la política social fueron algunos de sus temas favoritos y fue un gran defensor del proteccionismo y del desarrollo de las industrias nacionales.

La ordenación de los elementos químicos en una tabla periódica fue el gran aporte de Mendeléyev a la ciencia, pues esta agrupación por pesos atómicos y valencias permite observar una regularidad en las propiedades de los elementos. Además, intuyó que aún faltaban elementos por descubrirse, y por este motivo había huecos en la tabla, y señaló las propiedades que éstos debían poseer.

En 1860, inició sus estudios sobre la confección de un manual de química. Para ello, elaboró unas tarjetas donde iba enumerando las propiedades más significativas de los elementos conocidos hasta entonces. Al ordenar estas tarjetas, pudo comprobar que sesenta aparecían en fila y la mayoría de los elementos estaban ordenados en orden creciente respecto a su masa atómica relativa. De esta manera, los elementos con propiedades químicas análogas, quedaban ubicados en grupos verticales.

Con anterioridad, en 1817, J. W. Döbereiner, cuando aún se conocían muy pocos elementos químicos, intuyó la existencia de las tríadas o grupos de elementos con propiedades parecidas, con la característica de que el peso atómico del elemento central era la media aritmética aproximada de los pesos atómicos de los elementos extremos; éste era el caso, por ejemplo, del litio, sodio y potasio o del cloro, bromo y yodo o del azufre, selenio y telurio.

También, A. E. de Chancourtois, en 1862, estableció una hélice telúrica o tornillo telúrico, situando los elementos químicos en orden de pesos atómicos crecientes sobre una hélice, con 16 elementos por vuelta. De esta manera, observó que muchos de los elementos de propiedades análogas quedaban ubicados en la generatriz del cilindro, unos encima de otros; enunció de esta manera una ley que decía que las propiedades de los elementos son las propiedades de los números.

En 1868, J. A. Newlands había ordenado los elementos en agrupaciones lineales, enunciando su ley de las octavas, en la que afirmaba que, si se situaban todos los elementos en un orden creciente de pesos atómicos después de cada siete elementos, aparecía un octavo cuyas propiedades son similares a las del primero, pero Dimitri desconocía este trabajo y, por otra parte, el suyo le superó con creces.

La elaboración de la tabla como tal se realizó a lo largo de los años 1868-1869. Una primera versión se presentó a la Sociedad Química Rusa, donde aparecía de forma explícita la idea de que las propiedades de los elementos pueden representarse a través de funciones periódicas de sus pesos atómicos.

Simultáneamente a Mendeléyev, pero de forma independiente, J. L. Meyer llegó a una clasificación prácticamente igual, pero se basó en las propiedades físicas de los elementos y no en las químicas, como hizo Mendeléyev.

El gran mérito de Mendeléyev estriba en la importancia que dio a la semejanza de grupo, llegando a las siguientes conclusiones:


Partiendo de este carácter periódico de la tabla, predijo las propiedades de algunos elementos desconocidos, y en concreto los que debían ocupar las posiciones inmediatamente inferiores del boro, aluminio y silicio, y a los que él denominó: eka-boro, eka-aluminio y eka-silicio, respectivamente. Poco tiempo después, el descubrimiento del eka-aluminio, designado como galio (de número atómico 31, descubierto en 1875 por L. de Boisbaudran), el eka-boro, denominado escandio (de número atómico 21, descubierto en 1879 por L. F. Nilson), y el eka-silicio, designado como germanio (el número 32, descubierto por Winkler, en 1886), le dieron la razón.

Posteriormente, se añadieron a la tabla los gases nobles y los transuránidos y, si bien cuando comenzaron a descubrirse los primeros gases inertes pareció que la teoría de la periodicidad se derrumbaba, se observó que, al intercalar en la relación de los elementos por orden de pesos atómicos crecientes, era suficiente con invertir el argón y el potasio para que todos encajaran en una columna, ubicada entre la de los halógenos y la de los metales alcalinos. Después, Moseley y Bohr dieron una explicación a esta ordenación, usando el concepto de estructura atómica. La periodicidad de las propiedades observadas por Mendeléyev se debe al número de electrones en los orbitales de sus últimos niveles.

El sistema periódico es la clasificación de todos los elementos químicos, naturales o creados artificialmente. A medida que se perfeccionaron los métodos de búsqueda, el número de elementos químicos conocidos fue creciendo sin cesar y surgió la necesidad de ordenarlos de alguna manera. Se realizaron varios intentos, pero el intento decisivo lo realizó Mendeléyev, que creó lo que hoy se denomina "sistema periódico".

Mendeléyev ordenó los elementos según su masa atómica, situando en una misma columna los que tuvieran algo en común. Al ordenarlos, se dejó llevar por dos grandes intuiciones; alteró el orden de masas cuando era necesario para ordenarlos según sus propiedades y se atrevió a dejar huecos, postulando la existencia de elementos desconocidos hasta ese momento.

Dmitri Mendeléyev publicó su tabla periódica con todos los elementos conocidos y predijo varios de los nuevos elementos para completar la tabla. Solo unos meses después, Meyer publicó una tabla prácticamente idéntica. Algunos consideran a Meyer y Dmitri Mendeléyev los cocreadores de la tabla periódica. Este último logró predecir con precisión las cualidades de lo que llamó eka-silicio, eka-aluminio y eka-boro (germanio, galio y escandio, respectivamente).

Con todo, su principal logro investigador fue el establecimiento del llamado sistema periódico de los elementos químicos, o tabla periódica, gracias al cual culminó una clasificación definitiva de los citados elementos (1869) y abrió el paso a los grandes avances experimentados por la química en el siglo XX.

Aunque su sistema de clasificación no era el primero que se basaba en propiedades de los elementos químicos, como su valencia, sí incorporaba notables mejoras, como la combinación de los pesos atómicos y las semejanzas entre elementos, o el hecho de reservar espacios en blanco correspondientes a elementos aún no descubiertos como el eka-aluminio o galio (descubierto por Boisbaudran, en 1875), el eka-boro o escandio (Nilson, 1879) y el eka-silicio o germanio (Winkler, 1886). Actualmente se emplea el formato de la tabla elaborada por Werner, con los lantánidos y actínidos en columnas propias. 

Los últimos años de la carrera los pasó en la enfermería debido a un diagnóstico de tuberculosis. Falleció el 2 de febrero de 1907, casi ciego. Se considera a Mendeléyev un genio, no solo por el ingenio que mostró para aplicar todo lo conocido y predecir lo no conocido sobre los elementos químicos y plasmarlo en la tabla periódica, sino por los numerosos trabajos realizados a lo largo de toda su vida en diversos campos de la ciencia, agricultura, ganadería, industria, petróleo, etc.







</doc>
<doc id="3370" url="https://es.wikipedia.org/wiki?curid=3370" title="Modelo estándar de la física de partículas">
Modelo estándar de la física de partículas

El modelo estándar de la física de partículas es una teoría relativista de campos cuánticos desarrollada entre 1970 y 1973 basada en las ideas de la unificación y simetrías que describe la estructura fundamental de la materia y el vacío considerando las partículas elementales como entes irreducibles cuya cinemática está regida por las cuatro interacciones fundamentales conocidas (exceptuando la gravedad, cuya principal teoría, la relatividad general, no encaja con los modelos matemáticos del mundo cuántico). La palabra "modelo" en el nombre viene de la década de 1970 cuando no había suficiente evidencia experimental que confirmara el modelo. Hasta la fecha, casi todas las pruebas experimentales de las tres fuerzas descritas por el modelo estándar están de acuerdo con sus predicciones. Sin embargo el modelo estándar no alcanza a ser una teoría completa de las interacciones fundamentales debido a varias cuestiones sin resolver.

A principios del siglo XXI, el problema de reducir las leyes que gobiernan el comportamiento y la interacción de todas las interacciones fundamentales de la materia seguía siendo un problema no resuelto. El trabajo teórico durante el siglo XX, llevó a una teoría que reducía a un esquema común el electromagnetismo y la fuerza débil, y se poseía un modelo adecuado de la fuerza fuerte. Sin embargo, a pesar de diversas propuestas prometedoras existían tres teorías diferentes para explicar las diferentes interacciones fundamentales, a saber:
Frente a este panorama, el Modelo Estándar agrupa, pero no unifica, las dos primeras teorías –el modelo electrodébil y la cromodinámica cuántica– lo que proporciona una teoría internamente consistente que describe las interacciones entre todas las partículas observadas experimentalmente.

Como antecedentes del modelo estándar se pueden citar a la teoría de campos y la teoría atómica. 

La teoría atómica supone que la materia está constituida por entes indivisibles. Los descubrimientos de J. J. Thomson sobre el electrón y de E. Rutherford sobre el núcleo atómico dieron un mejor entendimiento de la estructura interna del átomo dando lugar a la física electrónica y la física nuclear respectivamente. 

La primera, iniciada por M. Faraday, es la mejor explicación a la acción a distancia. En un entendimiento clásico de la naturaleza hay tres fenómenos que presentan una acción a distancia: electricidad, magnetismo y gravedad. Las primeras dos se consideraron fuerzas independientes hasta que H. C. Ørsted descubrió que la corriente eléctrica y el magnetismo estaban relacionados. J. C. Maxwell describe matemáticamente la relación mutua entre los campos eléctricos y magnéticos dando un marco teórico completo para la teoría electromagnética. Finalmente A. Einstein unificó ambos campos motivado por la aparente asimetría al aplicar las ecuaciones de Maxwell a cuerpos en movimiento. Un esfuerzo posterior lo llevó a generalizar esta teoría para cuerpos acelerados y el campo gravitatorio en la teoría general de la Relatividad.

En la teoría clásica de campos se modela la acción a distancia entre cuerpos puntuales mediante un campo continuo que toma, transporta y cede energía de y a los cuerpos. Actualmente en física de partículas, la dinámica de la materia y de la energía en la naturaleza se entiende mejor en términos de cinemática e interacciones de partículas fundamentales. Técnicamente, la teoría cuántica de campos proporciona el marco matemático para el modelo estándar. El modelo estándar describe cada tipo de partícula en términos de un campo matemático. Sin embargo, este marco no hace una distinción esencial entre campo y partícula: ambos pueden ser descritos por una función continua en el espacio o bien como partículas puntuales. Ninguno de los anteriores ofrece una explicación satisfactoria. Para una descripción técnica de los campos y de sus interacciones, ver la Teoría cuántica de campos.
La teoría cuántica del electrón ideada por Paul Dirac describe al electrón a velocidades relativistas. De esta se desprende la idea del spin en forma natural como parte de la solución a la formulación relativista de ecuación de Schrödinger. Este esfuerzo excedió las expectativas, no sólo explicando el espectro de ciertos átomos sino la predicción confirmada en 1932 de electrones con carga positiva: los positrones. Sin embargo, estas ecuaciones describen al electrón como un único electrón o un gas ideal de electrones, y también que el campo eléctrico del electrón es despreciable con respecto al que está inmerso. La investigación teórica sobre la interacción del electrón con el campo electromagnético y entre electrones da lugar a la electrodinámica cuántica. Esta última se la considera sumamente exitosa por el grado de precisión de sus predicciones.

Los métodos y conceptos utilizados en la electrodinámica cuántica dieron lugar a la teoría cuántica de campos y sentó las bases sobre la que se apoya el modelo estándar.

Las simetrías son invarianzas ante transformaciones. El teorema de Noether establece una correspondencia entre una simetría y una ley de conservación, es decir establece una razón fundamental por la cual se observa la conservación de ciertas magnitudes.
Wolfgang Pauli y Julian Schwinger independientemente, demostraron que la invariancia bajo las transformaciones de Lorentz, implica una invariancia CPT. Esto es, los campos cuánticos relativistas son invariantes ante el cambio de partícula por su antipartícula y viceversa (Simetría C), invariantes ante la inversión especular (Simetría P) e invariantes ante la inversión temporal (Simetría T).
Sin embargo, se verificó experimentalmente que la interacción nuclear débil viola la simetría P: se comporta diferente a su imagen especular. Esto supuso que otra simetría es violada para restaurar la simetría CPT. De esta manera la simetría CP y la simetría T se supusieron fundamentales. Experimentos sobre el kaón demostraron que el sector cuark viola la simetría CP, consecuentemente la simetría T, aunque esta última no pudo ser verificada experimentalmente debido a su dificultad.

Las simetrías internas tienen un rol importante en el modelo estándar ya que ellas se desprende la conservación de carga y define inequívocamente la interacción entre partículas.

La intensidad de la interacción queda determinada por el acoplamiento del fermión al campo gauge. Este acoplamiento coincide con la carga eléctrica en la electrodinámica cuántica y por extensión se las cargas de los fermiones cargados. Debido al teorema de Noether a la simetría introducida le corresponde una conservación de carga. La ecuación de Yang-Mills generaliza la electrodinámica cuántica introduciendo nuevas simetrías gauge. Estas simetrías introducen un nuevo bosón, que media la fuerza correspondiente.
Si bien el modelo es perfectamente simétrico, la evidencia experimental demuestra que la realidad no es así, principalmente porque la inclusión de masa en el modelo rompe estas simetrías, pero existe la evidencia empírica que demuestra que las partículas son masivas. Esto puso en evidencia una ruptura espontánea de simetría para el modelo electrodébil.

Para facilitar la descripción, los términos del lagrangiano del modelo estándar se pueden agrupar como se indica en la tabla:

El modelo estándar incluye tres campos bosónicos B, W y G correspondientes a las simetrías U(1), SU(2) y SU(3) respectivamente. Adicionalmente un bosón formula_1 añadido para preservar la simetría en el sector electrodébil. Luego de la ruptura espontánea de simetría electrodébil los bosones B y W se mezclan resultando en el campo electromagnético formula_2y el bosón neutro de la interacción nuclear débil formula_3.

Los fermiones en el modelo estándar se dividen en leptones y cuarks de acuerdo con su acoplamiento al campo color. Sin embargo, no existe razón fundamental para que esto sea así y se han formulado extensiones al modelo para afrontar esta particularidad. Los leptones son formula_4, formula_5 y formula_6 y los cuarks son formula_7, formula_8, formula_9 y formula_10. El neutrino dextrógiro no ha sido observado y puede ser por dos razones: o bien el neutrino dextrógiro es muy masivo o bien el neutrino es un fermión de Majorana y consecuentemente el antineutrino dextrógiro observado es idéntico al neutrino dextrógiro. Los fermiones reales resultan de la composición de la componente levógira y la dextrógira. Bajo la interacción electrodébil forman dobletes levógiros (subíndice L) o singletes dextrógiros (subíndice R). Implícitamente cada fermión tiene un componente por generación. Los fermiones de Dirac están compuestos por un fermión levógiro y otro dextrógiro.

Los fermiones cumplen el rol de partículas de materia ya que, debido a su estadística, no pueden existir dos de estas partículas en el mismo estado cuántico por lo cual necesariamente forman estructuras, como un átomo, una molécula o una estructura cristalina. El prototipo de los fermiones es el electrón, cuya descripción cuántica y relativista está dada por la ecuación de Dirac. Sin embargo la violación de las simetría C y P de la desintegración beta pone en duda que el neutrino responda a esta ecuación. Weyl y Ettore Majorana propusieron sendas ecuaciones para describir al neutrino. 

Se denomina así a la partícula regida por la ecuación de Dirac. Si bien esta ecuación fue en primera instancia postulada por P. M. Dirac para describir al electrón a velocidades relativistas, es generalizable a otros fermiones como protones y neutrones y por supuesto quarks.

Aunque la ecuación resulta en resultados consistentes con los experimentos, la solución admite infinitos niveles de energía negativos que no son observados: ningún electrón decae infinitamente. La interpretación a esta aparente contradicción es admitir la existencia de electrones cargados positivamente. Hipótesis luego verificada experimentalmente por C. D. Anderson. La violación de la simetría-C de la interacción nuclear débil requirió modificar la ecuación de Dirac para ajustarla a los resultados experimentales.

Ettore Majorana propuso una modificación a la ecuación de Dirac para incluir explícitamente la antipartícula y forzar una asimetría. De esta manera un fermión de Majorana es su propia antipartícula. La hipótesis del neutrino como fermión de Majorana se confirmaría si se observaran desintegraciones dobles beta sin neutrinos.

Los fermiones elementales se los puede dividir en dos grandes categorías de acuerdo a cómo interaccionan entre sí: leptones y cuarks. A diferencia de los primeros los últimos no se observan en forma aisladas sino que interaccionan fuertemente quedando confinados en hadrones: mesones, bariones y los hipotéticos tetracuarks, pentacuarks y moléculas hadrónicas. Las partículas de ambas categorías interaccionan según el modelo electrodébil.
Los seis leptones y seis cuarks se los puede agrupar en, hasta el momento, tres generaciones o familias de dos partículas cada una. Cada generación difiere solamente en la masa, el resto de las propiedades, cargas, son idénticas entre generaciones, aunque las investigaciones sobre el momento anómalo del muon podrían refutar esto. Hay que notar que esta división no es explicada por el modelo estándar como tampoco si es coincidencia que existan la misma cantidad de generaciones tanto para cuarks como para leptones.

Esta tabla se basa en parte de datos tomados por el Grupo de Datos de Partículas (cuarks).

Las cargas de las partículas elementales surgen como consecuencia necesaria de imponer simetrías "internas" o de "gauge" . 

Estas cargas las hacen susceptibles a las fuerzas fundamentales según lo descrito en la sección siguiente.

Un primer modelo de leptones fue propuesto por Steven Weinberg en 1967 basado en la simetría gauge SU(2)×U(1) y trabajos previos de Glashow, Salam y Ward y el mecanismo Brout-Englert-Higgs. Si bien el modelo incluye solamente al electrón y al neutrino electrónico, el principio de la universalidad leptónica establece que todos los leptones se acoplan de igual manera a los bosones vectoriales y permite aplicar el modelo de Weinberg igualmente a los muones y tauones. 

El modelo introduce las masas de los leptones mediante la interacción de un campo escalar. Para esto divide a cada uno de los leptones en sus dos partes quirales (dextrógira y levógira) resultando en dos fermiones de Weyl levógiros : un doblete formula_11 y un singlete formula_12. Cada componente del doblete se lo identifica con un leptón cargado y su correspondiente neutrino electrónico. El singlete es un leptón cargado dextrógiro. 

El modelo estándar de leptones se lo puede resumir en la siguiente manera.
Los leptones interaccionan emitiendo y absorbiendo bosones W, Z y fotones. La emisión o absorción de un bosón W implica un cambio de isospin débil y carga del leptón.
Los bosones Z responsables de la dispersión elástica de neutrinos y la única interacción que tienen los neutrinos con la materia.

Los leptones cargados, naturalamente, interactúan electromagnéticamente, independientemente de su quiralidad.

Los leptones cargados adquieren su masa observada luego de la ruptura espontánea de la simetría electrodébil interactuando con el bosón de Higgs.

Los leptones neutros —neutrinos— permanecen sin masa. Esto no se condice con los resultados experimentales, por lo que el mecanismo por el cual los neutrinos adquieren masa escapa al modelo estándar. En primer lugar la no observación de neutrinos dextrógiros implica que no pueden adquirir masa de Dirac. La carga eléctrica nula de los neutrinos no excluye que adquieran masa de Majorana, aunque esto violaría la conservación del número leptónico.

La masa no nula de los neutrinos implica una oscilación entre los diferentes tipos de neutrinos
formula_13

A su vez, esta oscilación permite el decaimiento de leptones cargados de una familia a otra emitiendo un fotón, por ejemplo

formula_14

Sin embargo la probabilidad de este proceso es despreciable.

El modelo de cuarks originalmente tenía tres cuarks, up down y strange. Cada uno portador de los números cuánticos isospin arriba, isospin abajo y extrañeza. El mecanismo Glashow-Iliopolous-Maiani predijo un cuarto cuark (charm o encanto). El mecanismo Cabbibo-Kobayashi-Maskawa predice una tercera generación de cuarks, top y bottom (truth y beauty).

El cuark top tiene cierta relevancia en el modelo estándar ya que su corta vida media no le permite hadronizar y su masa puede determinarse con mayor precisión que la de los otros cuarks.

Las fuerzas en la física son la forma en que las partículas interactúan recíprocamente y se influyen mutuamente. A nivel macroscópico, por ejemplo, la fuerza de Lorentz permite que las partículas cargadas eléctricamente interactúen con campo electromagnético. Otro ejemplo, la fuerza de gravitación permite que dos partículas con masa se atraigan una a otra de acuerdo con la Teoría de gravitación de Newton. El modelo estándar explica la primera de estas fuerzas como el resultado del intercambio de otras partículas por parte de las partículas de materia, conocidas como partículas mediadoras de la fuerza. Cuando se intercambia una partícula mediadora de la fuerza, a nivel macroscópico el efecto es equivalente a una fuerza que influencia a las dos, y se dice que la partícula ha mediado (es decir, ha sido el agente de) esa fuerza. Se cree que las partículas mediadoras de fuerza son la razón por la que existen las fuerzas y las interacciones entre las partículas observadas en el laboratorio y en el universo.

Las partículas mediadoras de fuerza descritas por el modelo estándar también tienen spin (al igual que las partículas de materia), pero en su caso, el valor del spin es "necesariamente" entero, particularmente unitario, significando que todas las partículas mediadoras de fuerza son bosones. Consecuentemente, no siguen el principio de exclusión de Pauli. Los diversos tipos de partículas mediadoras de fuerza son descritas a continuación.


El modelo estándar de las interacciones electrodébiles está basado en el grupo gauge SU(2)×U(1), con cuatro bosones gauge formula_18 para SU(2) y B para U(1), y las correspondientes constantes de acoplamiento g y g'. Los fermiones levógiros de la generación iésima son dobletes. Los fermiones dextrógiros son singletes en SU(2). El modelo mínimo contiene tres generaciones o familias. La interacción débil se acopla a la quiralidad del fermión de la forma más asimétrica posible: se acopla a fermiones levógiros pero no a los dextrógiros. De esta manera la interacción electrodébil se acopla solamente a los fermiones levógiros, cargados o no. Esto supone una violación a la simetría P por lo que se hace necesaria la violación de otra simetría, en este caso la conjugación de carga, para que la simetría se restaure.

El lagrangiano del fermión en la interacción electrodébil queda definido luego de la ruptura espontánea de simetría como:formula_19Donde cada uno de los términos representan:
donde formula_26es el ángulo de mezcla electrodébil.

La interacción electrodébil entre cuarks se las puede resumir de la siguiente manera:

Decaimiento beta:

formula_27
En todos los casos la carga se conserva en ambos lados de la interacción, como así el número leptónico, es decir, la diferencia entre leptones y antileptones. Además la interacción sucede siempre entre fermiones de la misma generación.

Por ejemplo el decaimiento mu:

formula_28

La partícula de Higgs es una partícula elemental (con masa) predicha en el modelo estándar. Tiene spin S=0, por lo que es un bosón.

El bosón de Higgs desempeña un papel único en el modelo estándar, y un papel dominante en explicar los orígenes de la masa de los bosones W y Z, los leptones cargados, los cuarks y su propia masa . Las masas de las partículas elementales, y las diferencias entre el electromagnetismo (causada por el fotón) y la fuerza débil (causada por los bosones W y Z), son críticas en muchos aspectos de la estructura de la materia microscópica (y por lo tanto macroscópica).

Hasta el año 2012, ningún experimento había detectado directamente la existencia del bosón de Higgs, aunque había una cierta evidencia indirecta de él. Todas las esperanzas estaban puestas en las investigaciones realizadas mediante el Gran colisionador de Hadrones (LHC del CERN por sus siglas en inglés) es el mayor acelerador de partículas del mundo. Este centro hizo el histórico anuncio del hallazgo de una partícula compatible con las propiedades del bosón de Higgs el 4 de julio de 2012, confirmado por los experimentos ATLAS y CMS. Pero aún falta ver si esta nueva partícula cumple las características predichas del bosón de Higgs dadas por el modelo estándar.

El Modelo Estándar predecía la existencia de los bosones W y Z, el gluón. Sus propiedades predichas fueron experimentalmente confirmadas con buena precisión.

El "Large Electron-Positron collider" (LEP) en el CERN probó varias predicciones entre los decaimientos de los bosones Z, y las confirmó.

La tabla siguiente muestra una comparación entre los valores medidos experimentalmente y los predichos por el Modelo Estándar:

Una de las principales dificultades a superar para el modelo estándar ha sido la falta de evidencias científicas . No obstante el 4 de julio de 2012 los físicos anunciaron el hallazgo de un bosón compatible con las características descritas, entre otros, por Peter Higgs; en cuyo honor se bautizó la partícula. El hecho de ser localizado en dos detectores distintos así como su fiabilidad (grado de certeza o sigma) hace que muy probablemente este escollo del modelo estándar haya sido superado. 

Incluso cuando el Modelo Estándar ha tenido gran éxito en explicar los resultados experimentales, tiene ciertas cuestiones importantes sin resolver:

El modelo estándar tiene 19 parámetros que deben establecerse de forma arbitraria para ser consistente con los resultados expermientales. Estos son tres constantes de acoplamiento, las nueve masas de los fermiones cargados y los cuatro ángulos y fase de la matriz CMK. Adicionalmente las masas de los neutrinos y los seis ángulos de mezcla.

El modelo estándar tiene tres constantes de acoplamiento por cada grupo de simetría SU(3), SU(2), U(1): g3, g' y g respectivamente. Alternativamente a g y g' se pueden definir a partir del ángulo de mezcla electrodébil y la carga elemental:

formula_26

formula_24

O bien a partir de la constante de estructura fina.

Las masas de los leptones cargados electrón, muon y tauón se pueden medir con relativa facilidad. 

En cambio, como los cuarks no se observan libremente, su masa tiene que inferirse.

La matrix CMK queda definida por tres ángulos y una fase, único mecanismo conocido responsable de la violación CP.

Uno de ellos es la esperanza en vacío, el cual fue determindado en 2012 en el LHC del CERN.

Otro parámetro es el acoplamiento de los fermiones al bosón de Higgs.

Ver Teoría de Peccei-Quinn

Ver Matriz de Pontecorvo–Maki–Nakagawa–Sakata

Una meta importante de la física es encontrar la base común que uniría a todas éstas en una teoría del todo, en la cual todas las otras leyes que conocemos serían casos especiales, y de la cual puede derivarse el comportamiento de toda la materia y energía (idealmente a partir de primeros principios).

Existen alternativas al Modelo Estándar que intentan dar respuesta a estas "deficiencias", como por ejemplo la teoría de cuerdas y la Gravedad cuántica de bucles.



</doc>
<doc id="3373" url="https://es.wikipedia.org/wiki?curid=3373" title="Paul Dirac">
Paul Dirac

Paul Adrien Maurice Dirac (Brístol, -Tallahassee, ) fue un ingeniero eléctrico, matemático y físico teórico británico que contribuyó de forma fundamental al desarrollo de la mecánica cuántica y la electrodinámica cuántica.

Ocupó la Cátedra Lucasiana de matemáticas de la Universidad de Cambridge, si bien pasó los últimos diez años de su vida en la Universidad Estatal de Florida. Entre otros descubrimientos formuló la ecuación de Dirac que describe el comportamiento de los fermiones y con la cual predijo la existencia de la antimateria. Dirac compartió el de 1933 con Erwin Schrödinger, «por el descubrimiento de nuevas formas productivas de la teoría atómica».

Paul Dirac nació en Brístol (Inglaterra). Su padre, Charles, fue un inmigrante del cantón suizo de Valais que enseñaba francés. Su madre, originaria de Cornualles, era hija de marineros. Paul tenía una hermana pequeña (Beatrice Isabelle Marguerite) y un hermano mayor (Reginald Charles Felix), que se suicidó a los veintiséis años, en 1924. Dirac describió su infancia como infeliz, por la severidad y autoritarismo de su padre. Una reciente biografía ha matizado tal carácter, haciendo referencia al propio carácter difícil y taciturno de Paul.

Estudió en la Bishop Primary School y en el Merchant Venturers Technical College, una institución de la Universidad de Brístol, que enfatizaba las ciencias modernas (algo inusual en la época, y a lo que Dirac estaría siempre agradecido).

Se graduó en ingeniería eléctrica en la Universidad de Brístol en 1921. Tras trabajar poco tiempo como ingeniero, Dirac decidió que su verdadera vocación eran las matemáticas. Completó otra carrera en matemáticas en Brístol en 1923 y fue entonces admitido en la Universidad de Cambridge, donde desarrollaría la mayor parte de su carrera. Empezó a interesarse por la teoría de la relatividad aunque Cunningham especialista en Cambridge en ese campo no le aceptó como estudiante y entonces trabajó bajo la supervisión de Ralph Fowler que trabajaba en el naciente campo de la física cuántica.

En 1926 desarrolló una versión de la mecánica cuántica en la que unía el trabajo previo de Werner Heisenberg y el de Erwin Schrödinger en un único modelo matemático que asocia cantidades medibles con operadores que actúan en el espacio vectorial de Hilbert y describe el estado físico del sistema. Por este trabajo recibió un doctorado en física por Cambridge.

En 1928, trabajando en los spines no relativistas de Pauli, halló la ecuación de Dirac, una ecuación relativista que describe al electrón. Este trabajo permitió a Dirac predecir la existencia del positrón, la antipartícula del electrón, que interpretó para formular el mar de Dirac. El positrón fue observado por primera vez por Carl Anderson en 1932. Dirac contribuyó también a explicar el spin como un fenómeno relativista.

El libro "Principios de la Mecánica Cuántica" de Dirac, publicada en 1930, se convirtió en uno de los libros de texto más comunes en la materia y aún hoy es utilizado. Introdujo la notación bra-ket y la función delta de Dirac.

En 1931 Dirac mostró que la existencia de un único monopolo magnético en el universo sería suficiente para explicar la cuantificación de la carga eléctrica.

El 29 de enero de 2014, el profesor David S. Hall del Amherst College Physics y de la Academia Research Fellow Mikko Möttönen de la Universidad Aalto reportan que han logrado crear, identificar y fotografiar monopolos magnéticos sintéticos en el laboratorio.

Paul Dirac compartió en 1933 el Premio Nobel de Física con Erwin Schrödinger «por el descubrimiento de nuevas teorías atómicas productivas». Dirac obtuvo la cátedra Lucasiana de matemáticas de la Universidad de Cambridge donde ejerció como profesor de 1932 a 1969.

Dirac pasó los últimos años de su vida en la Universidad Estatal de Florida en Tallahassee, Florida. Allí murió en 1984, y en 1995 se colocó una placa en su honor en la Abadía de Westminster en Londres.

Dirac era conocido entre sus colegas por su naturaleza precisa, al mismo tiempo que taciturna. Cuando Niels Bohr se quejaba de que no sabía cómo acabar una determinada frase en un artículo científico, Dirac le replicó: «A mí me enseñaron en la escuela que nunca se debe empezar una frase sin saber el final de la misma». Las anécdotas sobre su tendencia al silencio se hicieron famosas, y se acuñó una unidad, el "dirac", para la unidad mínima de palabras que se podían decir en una conversación. Una reciente biografía "The strangest man", de Graham Farmelo, ha sugerido que tenía Síndrome de Asperger, ya que su lenguaje era muy literal y no hablaba mucho con las personas.

También eran conocidas sus dificultades de relación social, su falta de empatía y su desinterés por las mujeres. No obstante esto último, en 1937 se casó con la hermana del también físico Eugene Paul Wigner, Margit Wigner (conocida familiarmente como "Manci"), con la que tuvo dos hijas, además de otros dos hijos que Manci aportó de un matrimonio anterior, que adoptaron el apellido Dirac, y a los que él consideró siempre como propios.

Dirac era también reconocido por su modestia. Llamó a la ecuación de la evolución temporal de un operador mecano-cuántico la «ecuación de movimiento de Heisenberg», cuando fue él el primero en escribirla. Para referirse a la estadística de Fermi-Dirac el siempre insistió en decir estadística de Fermi.

Cuando en una ocasión le preguntaron sobre poesía, contestó: «En ciencia uno intenta decir a la gente, en una manera en que todos lo puedan entender, algo que nunca nadie supo antes. La poesía es exactamente lo contrario».

Cuando visitó la Unión Soviética, fue invitado a una conferencia en filosofía de la física. Él simplemente se puso de pie y escribió en la pizarra: «Las leyes físicas deben tener la simplicidad y belleza de las matemáticas». Este concepto de belleza matemática, incluso antes de disponer de pruebas experimentales, guio prácticamente toda su carrera científica. Por sus frecuentes viajes a la Unión Soviética se le impidió entrar en Estados Unidos durante algún tiempo.

Dirac era un ateo reconocido. Tras hablar con Dirac, Pauli dijo en sus crónicas: «Si entiendo correctamente a Dirac, él dice: no hay Dios, y Dirac es su profeta».

Dirac aunque durante varios años se mostró como un ateo, con el paso del tiempo en 1963 declaró para un artículo de "Scientific American" que considera a Dios como un gran matemático que empleó ciencia avanzada para crear el universo. En una conferencia en 1971 se mostró escéptico de que la vida haya resultado por casualidad y dijo que «se debe asumir que Dios existe» en relación a las leyes de la física cuántica.

Mantuvo posiciones políticas relativamente escoradas a la izquierda, aunque no militantes. Visitó a menudo la Unión Soviética y mantuvo una íntima amistad con el físico soviético Piotr Kapitsa. Aunque participó en el desarrollo teórico de la energía nuclear y en desarrollos de ingeniería para el enriquecimiento de uranio, durante la Segunda Guerra Mundial se mantuvo prácticamente al margen de las investigaciones para el desarrollo de armas nucleares.

Dirac es ampliamente considerado como uno de los físicos más importantes de todos los tiempos. Fue uno de los fundadores de la mecánica cuántica y la electrodinámica cuántica, siendo considerado por algunos físicos como el físico más relevante del siglo XX.

Sus primeras aportaciones incluyen el cálculo moderno de operadores para la mecánica cuántica, que él llamó Teoría de Transformaciones, así como una versión temprana de la formulación de integrales de camino. También creó un formalismo de muchos cuerpos para la mecánica cuántica que permitía que cada partícula tuviera su propio tiempo.

Su ecuación de ondas relativista para el electrón fue el primer planteamiento exitoso de una mecánica cuántica relativista. Dirac fundó la teoría cuántica de campos con su interpretación de la ecuación de Dirac como una ecuación de muchos cuerpos, con la cual predijo la existencia de la antimateria así como los procesos de aniquilación de materia y antimateria. Asimismo, fue el primero en formular la electrodinámica cuántica, si bien no pudo calcular cantidades arbitrarias debido al límite de distancias cortas que requiere de la renormalización.




</doc>
<doc id="3374" url="https://es.wikipedia.org/wiki?curid=3374" title="Núcleo atómico">
Núcleo atómico

El núcleo atómico es la parte central de un átomo, tiene carga positiva, y concentra más del 99,9% de la masa total del átomo.

Está formado por protones y neutrones (denominados nucleones) que se mantienen unidos por medio de la interacción nuclear fuerte, la cual permite que el núcleo sea estable, a pesar de que los protones se repelen entre sí (como los polos iguales de dos imanes). La cantidad de protones en el núcleo (número atómico), determina el elemento químico al que pertenece. Los núcleos atómicos no necesariamente tienen el mismo número de neutrones, ya que átomos de un mismo elemento pueden tener masas diferentes, es decir son isótopos del elemento.

La existencia del núcleo atómico fue deducida del experimento de Rutherford, donde se bombardeó una lámina fina de oro con partículas alfa, que son núcleos atómicos de helio emitidos por rocas radiactivas. La mayoría de esas partículas traspasaban la lámina, pero algunas rebotaban, lo cual demostró la existencia de un minúsculo núcleo atómico.

El descubrimiento de los electrones fue la primera indicación de la estructura interna de los átomos. A comienzos del siglo XX el modelo aceptado del átomo era el de J.J Thomson, 'pudín de pasas', modelo en el cual el átomo era una gran bola de carga positiva con los pequeños electrones cargados negativamente incrustado dentro de la misma. Por aquel entonces, los físicos habían descubierto también tres tipos de radiaciones procedentes de los átomos: alfa, beta y radiación gamma. Los experimentos de 1911 realizados por Lise Meitner y Otto Hahn, y por James Chadwick en 1914 mostraron que el espectro de decaimiento beta es continuo y no discreto. Es decir, los electrones son expulsados del átomo con una gama de energías, en vez de las cantidades discretas de energía que se observa en rayos gamma y decaimiento alfa. Esto parecía indicar que la energía no se conservaba en estos decaimientos. Posteriormente se descubrió que la energía sí se conserva, con el descubrimiento de los neutrinos.

En 1906 Ernest Rutherford publicó "El retraso de la partícula alfa del radio cuando atraviesa la materia", en Philosophical Magazine (12, p. 134-46). Hans Geiger amplió este trabajo en una comunicación a la Royal Society (Proc. Roy. Soc. 17 de julio de 1908) con experimentos y Rutherford se había hecho pasar aire a través de las partículas α, papel de aluminio y papel de aluminio dorado. Geiger y Marsden publicaron trabajos adicionales en 1909 (Proc. Roy. Soc. A82 p. 495-500) y ampliaron aún más el trabajo en la publicación de 1910 por Geiger (Proc. Roy. Soc. 1 de febrero de 1910). En 1911-2 Rutherford explicó ante la Royal Society los experimentos y propuso la nueva teoría del núcleo atómico. Por lo que se considera que Rutherford demostró en 1911 la existencia del núcleo atómico.

Por esas mismas fechas (1909) Ernest Rutherford realizó un experimento en el que Hans Geiger y Ernest Marsden, bajo su supervisión dispararon partículas alfa (núcleos de helio) en una delgada lámina de oro. El modelo atómico de Thomson predecía que la de las partículas alfa debían salir de la lámina con pequeñas desviaciones de sus trayectorias. Sin embargo, descubrió que algunas partículas se dispersan a grandes ángulos, e incluso completamente hacia atrás en algunos casos. Este descubrimiento en 1911, llevó al modelo atómico de Rutherford, en que el átomo está constituido por protones y electrones. Así, el átomo del nitrógeno-14 estaría constituido por 14 protones y 7 electrones.

El modelo de Rutherford funcionó bastante bien durante muchos años. Se pensaba que la repulsión de las cargas positivas entre protones era solventada por los electrones (con carga negativa) interpuestos ordenadamente en medio, por lo que el electrón era considerado como un "cemento nuclear". Esto fue hasta que los estudios llevados a cabo por Franco Rasetti, en el Institute of Technology de California en 1929. En 1925 se sabía que los protones y electrones tiene un espín de , y en el modelo de Rutherford nitrógeno - 14 los 14 protones y seis de los electrones deberían cancelar sus contribuciones al espín total, estimándose un espín total de . Rasetti descubierto, sin embargo, que el tiene un espín total unidad.

En 1930 Wolfgang Pauli no pudo asistir a una reunión en Tubinga, y en su lugar envió una carta famoso con la clásica introducción "Queridos Señoras y señores radiactivos ". En su carta Pauli sugirió que tal vez existía una tercera partícula en el núcleo, que la bautizó con el nombre de "neutrones". Sugirió que era más ligero que un electrón y sin carga eléctrica, y que no interactuaba fácilmente con la materia (y por eso todavía no se le había detectado). Esta hipótesis permitía resolver tanto el problema de la conservación de la energía en la desintegración beta y el espín de nitrógeno - 14, la primera porque los neutrones llevaban la energía no detectada y el segundo porque un electrón extra se acoplaba con el electrón sobrante en el núcleo de nitrógeno - 14 para proporcionar un espín de 1. Enrico Fermi redenominó en 1931 los neutrones de Pauli como neutrinos (en italiano pequeño neutral) y unos treinta años después se demostró finalmente que un neutrinos realmente se emiten en el decaimiento beta. 

En 1932 James Chadwick se dio cuenta que la radiación que había sido observada por Walther Bothe, Herbert L. Becker, Irène y Jean Frédéric Joliot-Curie era en realidad debido a una partícula que él llamó el neutrón. En el mismo año Dimitri Ivanenko sugirió que los neutrones eran, de hecho partículas de espín 1 / 2, que existían en el núcleo y que no existen electrones en el mismo, y Francis Perrin sugirió que los neutrinos son partículas nucleares, que se crean durante el decaimiento beta. Fermi publicó 1934 una teoría de los neutrinos con una sólida base teórica. En el mismo año Hideki Yukawa propuso la primera teoría importante de la fuerza para explicar la forma en que el núcleo mantiene junto.

Luego del descubrimiento del neutrón, por James Chadwick, Werner Heisenberg (que enunció años antes el principio de incertidumbre), indicó que los neutrones pueden ser parte del núcleo, y no así los electrones. Con esta teoría se resolvía totalmente el problema del spin que no coincidía, además de explicar todos los aspectos del comportamiento nuclear.

Sin embargo, la nueva teoría traía consigo otro severo problema: con el modelo anterior, que incluía electrones como "cemento nuclear", se explicaba que los protones, todos con la misma carga positiva, permanecieran totalmente juntos, sin que saliesen disparados por la repulsión de cargas iguales. Sin embargo, con el modelo que incluye el neutrón, no había explicación alguna respecto a la forma en que en núcleo se mantiene unido y no explota de inmediato (es decir, ningún elemento debería existir, con la única excepción del hidrógeno). Para ejemplificar, la fuerza con la que se repelen dos protones a la distancia que están (una diezbillonésima de centímetro), es de aproximadamente 240 newtons, fuerza suficiente para elevar en el aire un objeto de algo más de 24 kilogramos (nótese la enormidad inimaginable de esa fuerza dado que estamos hablando de dos protones, cuya masa es de algo más de 10 kilogramos)

La enorme dificultad que sufría la teoría se fue resolviendo gradualmente. En 1927, Heisenberg propuso el principio de incertidumbre, que indica que mientras mayor sea la precisión con que conozcamos la velocidad de una partícula, con menor precisión podremos conocer su posición.

En 1930 Einstein dedujo a partir de este principio, por medios matemáticos, que si el principio es correcto, también es correcto otro tipo de indeterminación sobre la medición de la energía existente en un sistema cerrado. Mientras menor sea el lapso de tiempo en el cual se quiere saber la cantidad de energía del sistema, con menor precisión se la podrá medir.

Al momento de sugerir el modelo de núcleo protón-neutrón, en 1932, Heisenberg sugirió también la existencia de un campo de fuerza que unía los protones, por medio de la existencia efímera de una partícula. La existencia de esta partícula sería posible solo por el principio de incertidumbre, en la versión enunciada por Einstein.

El físico japonés, Hideki Yukawa, entonces se puso a analizar las propiedades de la partícula propuesta por Heisenberg, y en 1935 describió esas propiedades con precisión. La partícula solo podría existir un instante de unos segundos, tiempo suficiente para que pueda ir de un protón a otro, pero no más allá del núcleo del átomo. La energía necesaria para la existencia de esta partícula en ese breve periodo se ajusta al principio de incertidumbre en la versión de Einstein. Utilizando esas ecuaciones, la energía disponible en ese periodo sería de 20 pJ (pico julios, o ), lo que equivale a una partícula con una masa de 250 veces la del electrón.

Desde entonces hubo varios intentos de detectar esa partícula experimentalmente. Por supuesto que siendo una partícula que solo existe un breve instante, y utilizando energía no disponible, solo gracias al principio de incertidumbre, sería imposible de detectar, excepto si esa energía fuese proporcionada. Los rayos cósmicos —partículas que llegan del espacio a enormes velocidades— pueden proporcionar esa energía. En 1948, experimentando con rayos cósmicos en Bolivia, la partícula fue detectada por Cecil Frank Powell. La partícula fue llamada Pion.

Los núcleos atómicos son mucho más pequeños que el tamaño típico de un átomo (entre 10000 y 100000 veces más pequeños). Además contienen más del 99% de la masa con lo cual la densidad másica del núcleo es muy elevada. Los núcleos atómicos tienen algún tipo de estructura interna, por ejemplo los neutrones y protones parecen estar orbitando unos alrededor de los otros, hecho que se manifiesta en la existencia del momento magnético nuclear. Sin embargo, los experimentos revelan que el núcleo se parece mucho a una esfera o elipsoide compacto de 10 m (= 1 fm), en el que la densidad parece prácticamente constante. Naturalmente el radio varía según el número de protones y neutrones, siendo los núcleos más pesados y con más partículas algo más grandes. La siguiente fórmula da el radio del núcleo en función del número de nucleones "A":

Donde formula_1

La densidad de carga eléctrica del núcleo es aproximadamente constante hasta la distancia formula_2 y luego decae rápidamente hasta prácticamente 0 en una distancia formula_3 de acuerdo con la fórmula:

Donde "r" es la distancia radial al centro del núcleo atómico.

Las aproximaciones anteriores son mejores para núcleos esféricos, aunque la mayoría de núcleos no parecen ser esféricos como revela que posean momento cuadrupolar diferente de cero. Este momento cuadrupolar se manifiesta en la estructura hiperfina de los espectros atómicos y hace que el campo eléctrico del núcleo no sea un campo coulombiano con simetría esférica.

Los núcleos atómicos se comportan como partículas compuestas a energías suficientemente bajas. Además, la mayoría de núcleos atómicos por debajo de un cierto peso atómico y que además presentan un equilibrio entre el número de neutrones y el número de protones (número atómico) son estables. Sin embargo, sabemos que los neutrones aislados y los núcleos con demasiados neutrones (o demasiados protones) son inestables o radiactivos.

La explicación de esta estabilidad de los núcleos reside en la existencia de los piones. Aisladamente los neutrones pueden sufrir vía interacción débil la siguiente desintegración:

Sin embargo, dentro del núcleo atómico la cercanía entre neutrones y protones hace que sean mucho más rápidas, vía interacción fuerte las reacciones:

Esto hace que continuamente los neutrones del núcleo se transformen en protones, y algunos protones en neutrones, esto hace que la reacción apenas tenga tiempo de acontecer, lo que explica que los neutrones de los núcleos atómicos sean mucho más estable que los neutrones aislados. Si el número de protones y neutrones se desequilibra, se abre la posibilidad de que en cada momento haya más neutrones y sea más fácil la ocurrencia de la reacción .

En 1808 el químico inglés John Dalton propone una nueva teoría sobre la constitución de la materia. Según Dalton toda la materia está formada por átomos indivisibles e invisibles, estos a su vez se unen para formar compuestos en proporciones enteras fijas y constantes. De hecho Dalton propuso la existencia de los átomos como una hipótesis para explicar porqué los átomos sólo se combinaban en ciertas combinaciones concretas. El estudio de esas combinaciones le llevó a poder calcular los pesos atómicos. Para Dalton la existencia del núcleo atómico era desconocida y se consideraba que no existían partes más pequeñas.

En 1897 Joseph John Thomson fue el primero en proponer un modelo estructural interno del átomo. Thomson fue el primero en identificar el electrón como partícula subatómica de carga negativa y concluyó que «si los átomos contienen partículas negativas y la materia se presenta con neutralidad de carga, entonces deben existir partículas positivas». Es así como Thomson postuló que el átomo 
debe ser una esfera compacta positiva en la cual se encontrarían incrustados los electrones en distintos lugares, de manera que la cantidad de carga negativa sea igual a la carga positiva.

Así ni el modelo atómico de Dalton ni el de Thomson incluían ninguna descripción del núcleo atómico. La noción de núcleo atómico surgió en 1911 cuando Ernest Rutherford y sus colaboradores Hans Geiger y Ernest Marsden, utilizando un haz de radiación alfa, bombardearon hojas laminadas metálicas muy delgadas, colocando una pantalla de sulfuro de zinc a su alrededor, sustancia que tenía la cualidad de producir destellos con el choque de las partículas alfa incidentes. La hoja metálica fue atravesada por la mayoría de las partículas alfa incidentes; algunas de ellas siguieron en línea recta, otras fueron desviadas de su camino, y lo más sorprendente, muy pocas rebotaron contra la lámina.

A la luz de la fórmula dispersión usada por Rutherford:
Donde:
Los resultados del experimento requerían parámetros de impacto muy pequeños, y por tanto que el núcleo estuviera concentrado en la parte central, el núcleo de carga positiva, donde estaría concentrada la masa del átomo. con ello explicaba la desviación de las partículas alfa (partículas de carga positiva). Los electrones se encontrarían en una estructura externa girando en órbitas circulares muy alejadas del núcleo, lo que explicaría el paso mayoritario de las partículas alfa a través de la lámina de oro. 

En 1913 Niels Bohr postula que los electrones giran a grandes velocidades alrededor del núcleo atómico. Los electrones se disponen en diversas órbitas circulares, las cuales determinan diferentes niveles de energía. El electrón puede acceder a un nivel de energía superior, para lo cual necesita "absorber" energía. Para volver a su nivel de energía original es necesario que el electrón emita la energía absorbida (por ejemplo en forma de radiación).

Comúnmente existen dos modelos diferentes describir el núcleo atómico:
Aunque dichos modelos son mutuamente excluyentes en sus hipótesis básicas tal como fueron formulados originalmente, A. Bohr y Mottelson construyeron un modelo mixto que combinaba fenomenológicamente características de ambos modelos.

Este modelo no pretende describir la compleja estructura interna del núcleo sino solo las energías de enlace entre neutrones y protones así como algunos aspectos de los estados excitados de un núcleo atómico que se reflejan en los espectros nucleares. Fue inicialmente propuesto por Bohr (1935) y el núcleo en analogía con una masa de fluido clásico compuesto por neutrones y protones y una fuerza central coulombiana repulsiva proporcional al número de protones Z y con origen en el centro de la "gota". 

Desde el punto de vista cuantitativo se observa que la masa de un núcleo atómico es inferior a la masa de los componentes individuales (protones y neutrones) que lo forman. Esta no conservación de la masa está conectada con la ecuación formula_9 de Einstein, por la cual parte de la masa está en forma de energía de ligazón entre dichos componentes. Cuantitativamente se tiene la siguiente ecuación:

Donde:
El modelo de la gota de agua pretende describir la energía de enlace "B" a partir de consideraciones geométricas e interpreta la energía de los estados excitados de los núcleos como rotaciones o vibraciones semiclásicas de la "gota de agua" que representa el núcleo. En concreto en este modelo la energía de enlace se representa como "B":

Donde:

Este es un modelo que trata de capturar parte de la estructura interna reflejada tanto en el momento angular del núcleo, como en su momento angular. Además el modelo pretende explicar porqué los núcleos con un "número mágico" de nucleones (neutrones y protones) resultan más estables (los números mágicos son 2, 8, 20, 28, 50, 82 y 126). 

La explicación del modelo es que los nucleones se agrupan en "capas". Cada capa está formada por un conjunto de estados cuánticos con energías similares, la diferencia de energía entre dos capa es grande comparada con las variaciones de energía dentro de cada capa. Así dado que los nucleones son fermiones un núcleo atómico tendrá las capas de menor energía llena por lo que los nucleones no pueden caer a capas inferiores ya llenas. Las capas aquí deben entenderse en un sentido abstracto y no como capas físicas como las capas de una cebolla, de hecho la forma geométrica del espacio ocupado por un nucleón en un determinado estado de una capa se interpenetra con el espacio ocupado por nucleones de otras capas, de manera análoga a como las capas electrónicas se interpenetran en un átomo.




</doc>
<doc id="3375" url="https://es.wikipedia.org/wiki?curid=3375" title="Unidad de longitud">
Unidad de longitud

Una unidad de longitud es una cantidad estandarizada de longitud definida por convención. La longitud es una magnitud fundamental creada para medir la distancia entre dos puntos. Existen diversos sistemas de unidades para esta magnitud física; los más comúnmente usados son el Sistema Internacional de Unidades y el sistema anglosajón de unidades.

Tradicionalmente, las sociedades antiguas usaban como sistema de referencia para medir la longitud las dimensiones del cuerpo humano. Como ejemplos de esto se encontraban la pulgada, definida como el ancho de un pulgar; el pie, definido como la longitud de un pie humano; la yarda, que equivalía a la distancia desde la punta de la nariz hasta la punta del dedo medio con el brazo extendido; la braza, que correspondía a la distancia de punta a punta entre los dedos medios con los brazos extendidos; el palmo, que era la longitud de la palma de la mano; y el codo, aproximadamente el largo del antebrazo. ==
En la Antigua Roma se definieron unidades de longitud para distancias mayores. Se definió la milla como la distancia recorrida por una legión romana al dar 2000 pasos. Una milla equivalía a ocho estadio y tres millas correspondían aproximadamente a una legua.

Durante siglos m i c kk c k, cada nación definió sus propias unidades de longitud; en la mayoría de los casos, dos unidades llamadas de la misma manera en diferentes países representaban longitudes diferentes. Esto indujo la necesidad de definir un patrón de longitud universal, es decir, basado en fenómenos físicos accesibles en cualquier lugar del mundo. En 1670, el astrónomo y religioso Gabriel Mouton propuso como patrón de medida la longitud de un minuto de arco de un meridiano de la Tierra. A partir de esta idea, en 1790, durante la Revolución Francesa, la Asamblea Nacional decidió definir una unidad de longitud como la diezmillonésima parte de la distancia del Polo Norte hasta el ecuador, a lo largo del meridiano que pasa por Dunkerque y Barcelona. Esta unidad vino a conocerse como «metro» y estaría subdividida en partes de diez; de esta manera surgiría el sistema métrico decimal. En 1960, las definiciones de las unidades del sistema métrico fueron revisadas y se adoptó el nombre de Sistema Internacional de Unidades para la versión moderna del mismo.

En el Sistema Internacional de Unidades la unidad fundamental de longitud es el metro, definido como la distancia que recorre la luz en el vacío durante un intervalo de 1/299 792 458 de segundo. El símbolo del metro es «m», sin admitir nunca plural, mayúscula o punto, al no ser una abreviatura.

Utilizando los prefijos del Sistema Internacional es posible definir unidades de longitud que son múltiplos o submúltiplos del metro. A continuación se enlistan los múltiplos y submúltiplos del metro, aceptados dentro del SI, junto con su símbolo y su equivalencia en metros, en notación científica y decimal.

Múltiplos del metro: 

Submúltiplos del metro:

Existen algunos múltiplos y submúltiplos del metro que no forman parte oficialmente del Sistema Internacional de Unidades. Estos son:






</doc>
<doc id="3376" url="https://es.wikipedia.org/wiki?curid=3376" title="Unidades de superficie">
Unidades de superficie

Las unidades de superficie son las medidas utilizadas que miden superficies con una determinada área, en el caso de esta unidad se usa el m².

La medición es la técnica mediante la cual asignamos un número a una propiedad física, como resultado de comparar dicha propiedad con otra similar tomada como patrón, la cual se adopta como unidad. La medida de una superficie da lugar a dos cantidades diferentes si se emplean distintas unidades de medida. Así, surgió la necesidad de establecer una unidad de medida única para cada magnitud, de modo que la información fuese fácilmente comprendida.

Unidad básica:

Múltiplos:

Submúltiplos:

Otros:


</doc>
<doc id="3378" url="https://es.wikipedia.org/wiki?curid=3378" title="Unidades de masa">
Unidades de masa

La masa es una magnitud física que mide la cantidad de materia contenida en un cuerpo. En el Sistema Internacional de Unidades la unidad estándar es el kilogramo. En el sistema cgs es el gramo.

+

En el sistema inglés de medidas la unidad estándar de masa es la libra







</doc>
<doc id="3379" url="https://es.wikipedia.org/wiki?curid=3379" title="Yarda">
Yarda

La yarda (símbolo: yd) es la unidad de longitud básica en los sistemas de medida utilizados en Estados Unidos, Panamá y Reino Unido. En casi todos los países se usa oficialmente el metro como medida de longitud, y en los países mencionados se está en etapa de transición. 

Equivalencias:

En consonancia con otras medidas basadas en las proporciones del cuerpo humano definidas por Vitrubio, una yarda corresponde a la mitad de la longitud de los brazos extendidos, lo que equivale a tres pies. Por este motivo, es conceptualmente equivalente a una vara española (también equivalente a tres pies castellanos). Entonces, no hay que confundir el rod anglosajón, cuya traducción al español sería vara o caña, con la antigua medida española llamada "vara".

En el sistema anglosajón existen cuatro yardas, a saber:

Dado que la unidad más empleada en el ámbito industrial y técnico es la pulgada (=1/36 yardas), para evitar los inconvenientes debidos a la discrepancia entre las yardas inglesa y americana se ha convenido que "1 pulgada = 25,4 mm a 20 °C", quedando el metro y la yarda relacionados por la ecuación mostrada al inicio.




</doc>
<doc id="3381" url="https://es.wikipedia.org/wiki?curid=3381" title="Física computacional">
Física computacional

Se denomina física computacional a una rama de la física que se centra en la elaboración de modelos por ordenador de sistemas con muchos grados de libertad para los cuales ya existe una teoría computacional
. En general, se efectúan modelos microscópicos en los cuales las "partículas" obedecen a una dinámica simplificada, y se estudia el que puedan reproducirse las propiedades macroscópicas a partir de este modelo muy simple de las partes constituyentes. Las simulaciones se hacen resolviendo ecuaciones que gobiernan el sistema. Por lo general, son grandes sistemas de ecuaciones diferenciales ordinarias, ecuaciones diferenciales a derivadas parciales y ecuaciones diferenciales estocásticas, que no pueden ser resueltos explícitamente de manera analítica.

A menudo, la dinámica simplificada de las "partículas" tiene cierto grado de aleatoriedad. En general, esta vertiente se denomina método de Montecarlo, nombre que le viene por los casinos de Montecarlo como forma jocosa de recordar que el método usa la aleatoriedad.

Otras simulaciones se basan en que la evolución de una "partícula" en el sistema depende, exclusivamente, del estado de las partículas vecinas, y se rige mediante reglas muy simples y, en principio, determinadas. A esto se le llama simulaciones con autómatas celulares. Un ejemplo clásico, aunque más matemático que físico, es el famoso juego de la vida, ideado por John Horton Conway.

La física computacional tiene sus aplicaciones más relevantes en física del estado sólido (magnetismo, estructura electrónica, dinámica molecular, cambios de fase, etc.), física no-lineal, dinámica de fluidos, astrofísica (simulaciones del Sistema Solar, por ejemplo), física de partículas (teoría de campos/teoría gauge en un retículo espacio-temporal, especialmente para la Cromodinámica Cuántica (QCD) ).

Las simulaciones que se realizan en física computacional requieren gran capacidad de cálculo, por lo que en muchos casos es necesario utilizar supercomputadores o clusters de computadores en paralelo.



</doc>
<doc id="3389" url="https://es.wikipedia.org/wiki?curid=3389" title="Bellas artes">
Bellas artes

El término bellas artes se popularizó en el siglo XVIII para referirse a las principales artes y buen uso de la técnica. El primer libro que se conoce que las clasifica es "Les Beaux-Arts réduits à un même principe" ("Las bellas artes", 1746) de Charles Batteux, quien pretendió unificar las numerosas teorías sobre belleza y gusto que existían en esa época. Batteux incluyó en las bellas artes originalmente a la danza, la escultura, la música, la pintura y la poesía; se añadió posteriormente la elocuencia. 

Con el tiempo, la lista sufriría cambios según los distintos autores que añadirían o quitarían artes a esta lista (se eliminó la elocuencia). En 1911, Ricciotto Canudo es el primer teórico del cine en calificar a este como el séptimo arte, en su ensayo "Manifiesto de las Siete Artes", que se publicó en 1914. 

También por la evolución histórica del término, es habitual que el uso de "bellas artes" se asocie, en instituciones educativas y en museos de "bellas artes", casi exclusivamente a las "artes plásticas o artes visuales". En este sentido, la palabra "arte" también es muchas veces sinónimo de "artes visuales", al emplearse en términos como "galería de arte". 

Las artes son un fenómeno social, un medio de comunicación, una necesidad del ser humano de expresarse y comunicarse mediante formas, colores, sonidos y movimientos; el arte es un producto o acto creativo. Los griegos antiguos dividían las artes en artes superiores y artes menores. Las artes superiores eran aquellas que permitían gozar las obras por medio de los sentidos superiores (vista y oído), con los que no hace falta entrar en contacto físico con el objeto observado. Las bellas artes eran seis: arquitectura, escultura, pintura, música, declamación y danza. La declamación incluye la poesía, y con la música se incluye el teatro (actualmente parte de la Literatura). Esa es la razón por la que el cine se considera en la actualidad el séptimo arte. Las artes menores, en cambio, son aquellas que impresionan a los sentidos menores (gusto, olfato y tacto), con los que es necesario entrar en contacto con el objeto: gastronomía, perfumería y artesanía.
También se considera al cómic como una expresión artística, siendo denominado como "el Noveno Arte". Su lenguaje propio ya establecido y su mezcla de narrativa literaria y visual lo convierten, sin duda en una manifestación artística.




</doc>
<doc id="3391" url="https://es.wikipedia.org/wiki?curid=3391" title="Monty Python">
Monty Python

Monty Python (a veces conocidos como Los Pythons) fue un grupo británico de seis humoristas que sintetizó en clave de humor la idiosincrasia británica de los años 1960 y 1970, compuesto por Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones y Michael Palin. Del grupo únicamente Terry Gilliam no era británico sino estadounidense.

Lograron la fama gracias a su programa de televisión "Monty Python's Flying Circus" ("El Circo Volador de Monty Python"), estrenado el 5 de octubre de 1969 en la BBC y formada por 45 episodios repartidos en cuatro temporadas. El fenómeno Python se desarrolló más allá del programa de televisión adquiriendo un gran impacto: obras de teatros, películas, discos, libros y un musical. La influencia del grupo en la comedia se ha comparado con la de los Beatles en la música.

Emitido en la BBC entre 1969 y 1974, "Flying Circus" fue creado, escrito e interpretado por los seis miembros del grupo. Estaba estructurado como un programa de sketches, pero con una técnica narrativa innovadora (ayudada por las animaciones de Gilliam) que iba más allá de lo aceptable en estilo y contenido. Al ser los responsables tanto de los guiones y la interpretación, los Python tenían un control creativo que les permitía experimentar formas y contenidos, deshaciéndose de las reglas de la comedia televisiva. La influencia del grupo en la comedia británica ha sido notoria durante años, y en Norteamérica ha influido desde a los intérpretes de las primeras ediciones de "Saturday Night Live" hasta las más recientes tendencias de humor absurdo en la comedia televisiva. La palabra "Pythonesque" ha entrado en el léxico inglés como sinónimo de "absurdo" o "surrealista".

En una encuesta realizada en 2005 en el Reino Unido para encontrar al "Cómico de cómicos", tres de los seis miembros de Monty Python fueron votados por otros cómicos y aficionados como tres de los 50 mayores cómicos de la historia: Cleese en el puesto 2, Idle en el 21, y Palin en el 30.

En 2009 el grupo recibió el premio BAFTA honorífico por su contribución al mundo de la comedia. El premio se lo entregaron en el preestreno del documental dedicado al fallecido Graham Chapman, "".

Michael Palin y Terry Jones se conocieron en la Universidad de Oxford, donde ambos actuaban en el grupo de teatro estudiantil "The Oxford Revue". John Cleese y Graham Chapman se conocieron en la Universidad de Cambridge. Eric Idle también estaba en Cambridge pero empezó un año después. Cleese conoció a Terry Gilliam en Nueva York mientras estaban de gira con su grupo de teatro estudiantil, "Cambridge University Footlights". Chapman, Cleese e Idle eran miembros de los "Footlights", que en ese tiempo incluía también a Tim Brooke-Taylor, Bill Oddie y Graeme Garden (quienes más tarde formarían el grupo "Goodies"), y al actor y director Jonathan Lynn, coguionista de la serie "Sí, Ministro" y su secuela "Sí, Primer Ministro". Mientras Idle fue presidente de los "Footlights" fueron también miembros del grupo la escritora feminista Germaine Greer y el periodista Clive James. Todavía se conservan grabaciones de algunas actuaciones de este grupo de teatro en el Pembroke College de Cambridge. 

Antes de "Monty Python's Flying Circus", los Python escribieron o actuaron en diversas obras y espectáculos:

En muchas de estas actuaciones coincidieron con otros importantes cómicos y guionistas británicos del futuro: Marty Feldman, Jonathan Lynn, David Jason y David Frost, así como miembros de otros grupos de cómicos: Ronnie Corbett y Ronnie Barker (The Two Ronnies), y Tim Brooke-Taylor, Graeme Garden y Bill Oddie (The Goodies).

El éxito de "Do Not Adjust Your Set" hizo que la ITV ofreciera a Palin, Jones, Idle y Gilliam su propio programa juntos. Al mismo tiempo a Cleese y Chapman la BBC, impresionados por su trabajo en "The Frost Report" y en "At Last The 1948 Show" les ofreció un show propio. Cleese era reacio a un dúo cómico por varias razones, incluyendo la supuesta personalidad difícil de Chapman. Cleese tenía buenos recuerdos de su trabajo con Palin y le invitó a unirse al equipo. Con la serie de la ITV todavía en preproducción, Palin aceptó, y sugirió que se unieran al grupo Idle y Jones; quienes propusieron que Gilliam se encargara de proveer de animaciones a la serie. El grupo Monty Python es en buena parte resultado del deseo de Cleese de trabajar con Palin y las circunstancias que añadieron a los otros cuatro miembros al grupo.

Los Python tenían claro qué querían hacer con el programa. Eran admiradores del trabajo de Peter Cook, Alan Bennett, Jonathan Miller y Dudley Moore en el show teatral satírico "Beyond the Fringe", y habían trabajado en "The Frost Report", de estilo similar. También eran fanes del programa de Cook y Moore "Not Only... But Also". Un problema que los Python observaban en estos programas era que a pesar de que el sketch era potente, los guionistas intentaban a menudo encontrar una frase lo bastante divertida para terminarlo y esto disminuía la calidad del sketch. Decidieron entonces que no se molestarían en rematar sus sketches de la manera tradicional, y algunos de los primeros episodios del Flying Circus hacían gala de este abandono de la frase final. En Cleese se dirige a Idle y dice: "Este es el sketch más absurdo que hemos hecho", y los personajes terminan el sketch simplemente saliendo de escena. (Ver vídeo) Sin embargo, cuando empezaron a reunir material para el programa, los Python vieron a uno de sus ídolos, Spike Milligan, grabar su innovador programa "Q5" (1969). No solo era el programa más irreverente y anárquico que cualquier otro, sino que a menudo Milligan abandonaba el sketch a la mitad y salía de escena (a menudo murmurando "¿He escrito yo esto?"). Quedaba claro que su programa ahora sería menos original, y Jones particularmente decidió que los Python debían innovar.

Tras mucho debate, Jones recordó una animación creada por Gilliam en "Do Not Adjust Your Set" llamada "Beware of the Elephants", que le había intrigado por su peculiar estilo. Jones pensó que era un buen concepto para aplicar al programa: permitir que los sketches se combinaran unos con otros. A Palin también le había llamado la atención otro de los trabajos de Gilliam, titulado "Christmas Cards", y estaba de acuerdo que representaba "una forma diferente de hacer las cosas". Como Cleese, Chapman e Idle estaban menos concienciados con el desarrollo general del programa, fueron, Jones, Palin y Gilliam los responsables principales del estilo de presentación del "Flying Circus", en el que los distintos sketches estaban unidos para dar a cada episodio un estilo particular (usando a menudo una animación de Gilliam para pasar de la imagen final de un sketch a la de apertura del siguiente).

La escritura de los guiones comenzaba a las nueve de la mañana y finalizaba a las cinco de la tarde. Normalmente, Cleese y Chapman formaban una pareja aislada del resto, al igual que Jones y Palin, mientras que Idle escribía solo. Unos días después, se reunían con Gilliam, criticaban los guiones e intercambiaban ideas y opiniones. La forma de escribir era democrática. Si la mayoría encontraba graciosa una idea, se incluía en el show. También era democrático el casting para los sketches, ya que cada miembro se veía principalmente como escritor más que como un actor deseoso de aparecer en pantalla. Cuando se elegían los temas de los sketches, Gilliam tenía libertad para unirlos con sus animaciones, usando una cámara, tijeras y aerógrafo.

Al ser el programa un proceso colaborativo, las diferentes facciones dentro del grupo eran responsables de los elementos del humor del grupo. En general, el trabajo de los alumnos de Oxford (Jones y Palin) era más visual e imaginativo conceptualmente, como la llegada de la Inquisición española a la casa de un barrio suburbano (Ver vídeo) mientras que los sketches de los alumnos de Cambridge eran más verbales y agresivos (como los sketches de "confrontación" de Cleese y Chapman, donde un personaje intimida o abusa verbalmente de otro, o los personajes de Idle con extrañas peculiaridades verbales, como el hombre que habla en anagramas). Cleese confirmó que "la mayoría de sketches agresivos eran de Graham y míos, cualquier cosa que empezaba con una panorámica del campo y música impactante era de Mike y Terry, y cualquier cosa relacionada totalmente con palabras era de Eric". Mientras, las animaciones de Gilliam iban de lo extravagante a lo salvaje, ya que el formato animado le permitía crear escenas increíblemente violentas sin miedo a la censura.

Se pensaron varios nombres para el programa antes que se optara por "Monty Python's Flying Circus". Algunos fueron "Owl Stretching Time"; "The Toad Elevating Moment"; "A Horse, a Spoon and a Bucket"; "Vaseline Review"; and "Bun, Wackett, Buzzard, Stubble and Boot". "Flying Circus" apareció cuando la BBC dijo que había impreso ese nombre en su programación y no estaba preparada para modificarlo. Se empezaron entonces a sugerir variaciones en torno a este nombre (se dice que la BBC consideraba "Monty Python's Flying Circus" un nombre ridículo, y el grupo decidió cambiar de nombre cada semana hasta que la cadena se rindiera). El nombre "Gwen Dibley's Flying Circus" surgió por una mujer sobre la que Palin había leído en el periódico, pensando que sería gracioso que ella descubriera que tenía su propio programa de televisión. "Baron Von Took's Flying Circus" fue considerado como un afectuoso homenaje a Barry Took, el hombre que los había unido. Después se sugirió "Arthur Megapode's Flying Circus" y más tarde se desechó. "Baron Von Took's Flying Circus" recordaba al Circo volador Jasta 11 del Barón Manfred von Richthofen, que cobró fama en la Primera Guerra Mundial, y el grupo se formó en una época en la que la canción "Snoopy vs. the Red Baron" estaba de moda. El término "flying circus" era otro nombre de un espectáculo popular en la década de 1920 llamado "barnstorming", en el que diversos pilotos hacían acrobacias dando lugar a un espectáculo.

Existen diferentes y algo confusos orígenes del nombre Python, aunque los miembros del grupo están de acuerdo en que su único "significado" era que consideraban que sonaba gracioso. En el documental de 1998 "Live At Aspen", en el que el grupo recibió el galardón del American Film Institute, explicaron que "Monty" fue elegido a propuesta de Eric Idle como un guiño humorístico al Mariscal de Campo Montgomery, un legendario general británico de la Segunda Guerra Mundial; y añadiendo después una palabra de pronunciación suave, optaron por "Python". En otras ocasiones, Idle ha afirmado que el nombre "Monty" era un cliente habitual de su pub local; y la gente entraba a menudo preguntando al camarero: "¿Ya ha llegado Monty?", haciendo que el nombre quedara marcado en su cabeza. El nombre "Monty Python" fue descrito por la BBC como "concebido por el grupo como el nombre perfecto para un agente de talentos corrupto".

El fenómeno de los Monty Python transcendió el mundo televisivo siendo sus protagonistas también responsables de la producción de varios largometrajes:

Películas concierto:
Otras películas donde coincidieron parte de los miembros del grupo, sin ser producciones propiamente dichas del mismo (aunque heredan buena parte del humor y el surrealismo que les caracterizó), fueron:




Nació en South Shields, en el condado de Durham. Su padre falleció en un accidente cuando él era pequeño, por lo que su madre le llevó a la Royal Wolverhampton School para ser educado en dicho centro. Y realizó estudios de literatura inglesa en la Universidad de Cambridge.

Idle, el músico del grupo, no es el autor de la canción de la serie de televisión Monty Python's Flying Circus, la cual es una marcha popular llamada «La campana de la Libertad» así como de la mayoría de las canciones de las películas del grupo. En "La vida de Brian" canta la canción más reconocida de los Python, «Always Look On The Bright Side Of Life». Idle es conocido por el uso de pelucas ridículas (una de las raras ocasiones en que no usa peluca es en la escena final de La vida de Brian), y por sus exasperantes papeles, como el hombre invisible, el hombre de las fotos, el hombre que quería una hormiga, y otros. Idle interpretó al "valiente" Sir Robin en "Los caballeros de la mesa cuadrada".


El "Python agradable", es, siguiendo a John Cleese y Eric Idle, el Python más conocido por su trabajo como actor y el más popular del grupo entre las fanáticas femeninas. Participó con John Cleese en uno de los mejores sketches de "Monty Python's Flying Circus": los franceses de la oveja volante, o "Centro de discusión". Realizó los papeles de Bevis, el barbero medio psicópata travestido que quería ser leñador en el sketch «La Canción del Leñador» y de Sir Galahad en Los caballeros de la mesa cuadrada. Aparecía al principio de cada episodio de "Monty Python's Flying Circus" como el náufrago que decía "It's..."


Su padre, un vendedor de seguros, cambió el apellido de la familia de «Cheese» a «Cleese» debido al significado de la palabra cheese, que en inglés es "queso". Cleese estudió derecho en la universidad de Cambridge. Llegó a ser famoso como el presentador de la BBC que aparecía sentado frente a un escritorio en lugares tan extraños como una calle, una playa o un camión, y que decía la frase «And now for something completely different» («y ahora algo totalmente diferente»), que convirtió en eslogan de los Monty Python. Fue, junto con Graham Chapman uno de los gérmenes del grupo. Desde el fin de los Python se ha acostumbrado a trabajar en comedias de éxito de Hollywood


Gilliam nació en Medicine Lake, Minesota, en los Estados Unidos, y estudió Ciencias Políticas en el Colegio Occidental de California. Siempre le llamó más la atención la dirección que la actuación, por lo que sus papeles en la serie fueron muy esporádicos y secundarios. Es conocido por las animaciones , en las que cortaba fotografías y las volvía surrealistas. Después de la disolución del grupo, ha ganado fama como director de cintas serias y fantásticas siendo, junto con John Cleese, el Python que más reconocimiento ha logrado.


Nació el 1 de febrero de 1942 en la Bahía de Colwyn, en el norte del Gales. Junto con Gilliam, Jones es el otro Python no inglés. Recordado principalmente por sus papeles de mujer acompañado de la voz chillona que hacía. Fue la divertidísima madre de Brian en "La vida de Brian", film que dirigió él mismo. Llevó a cabo también Los caballeros de la mesa cuadrada, este último trabajando en cooperación con Terry Gilliam y la última película con todos los Python, El sentido de la vida. Después de la disolución del grupo se dedicó, principalmente, a la televisión, como guionista y presentador. Dirigió también "Erik el vikingo", y escribió el guion de "Labyrinth". Tras varios años padeciendo una afasia progresiva primaria, murió el 21 de enero de 2020.


Conocido por protagonizar a personajes autoritarios, como el coronel famoso que interrumpía los sketches. También realizó, varias veces, los papeles de doctor, para el que su formación habrá contribuido mucho, entre otros tantos otros papeles. Realizó los papeles principales en "La vida de Brian", protagonizando el papel de Brian Cohen, y en "Los caballeros de la mesa cuadrada", como el Rey Arturo. Con el tiempo, el alcoholismo perturbó su desempeño como actor. Mantuvo su homosexualidad en secreto hasta que lo confesó en un programa de entrevistas presentado por el músico de jazz George Melly. Tras la separación del grupo, tendría alguna que otra aparición en cine y televisión.

Una de sus últimas apariciones fue en un videoclip de la banda inglesa de heavy metal Iron Maiden, concretamente el vídeo de la canción «Can I Play with Madness», de su disco "Seventh Son of a Seventh Son", publicado en 1988. A mediados de ese mismo año, es diagnosticado con cáncer de esófago. 

Graham Chapman murió el 4 de octubre de 1989. Como parte de la elegía de su funeral, Eric Idle cantó un fragmento de "Always Look On The Bright Side Of Life", canción compuesta por él mismo, con la que termina La vida de Brian y John Cleese realizó un monologo en donde tendría que decir "mierda" a petición del propio Chapman.

Los Monty Python han ejercido una gran influencia en el humor contemporáneo. Su sentido del humor absurdo y políticamente incorrecto fue algo totalmente novedoso en su momento y el impacto de éste es comparado al de los Beatles en la música.

Los Monty Python dieron nombre, al parecer sin su conocimiento, al lenguaje de programación Python. Muchos de los ejemplos de uso de Python y nombres de sus componentes se basan en obras de este grupo. 

Su influencia en el mundo de la informática también puede encontrarse en la palabra "spam", derivada de uno de sus "sketchs".

El lemúrido Avahi cleesei, de Madagascar, es una especie de primate apodada así en honor a John Cleese, quien posee una gran afición por los lémures, y colabora en organizaciones para salvar las especies en peligro de estos.

Entre los artistas más influenciados por los Python están Mike Myers, Trey Parker, Matt Stone, Eddie Izzard, Seth MacFarlane, Matt Groening, Douglas Adams, el conjunto cómico-musical argentino Les Luthiers, entre otros.

También en Argentina, el actor y humorista Diego Capusotto, junto con el escritor Pedro Saborido, han realizado 12 temporadas de sketches con distintos personajes en un ciclo sin precedentes llamado "Peter Capusotto y sus videos". Capusotto ha manifestado en varias ocasiones que una de sus grandes influencias fue Monty Python.




</doc>
<doc id="3392" url="https://es.wikipedia.org/wiki?curid=3392" title="Zaida">
Zaida

Zaida (c.1063 o 1071- c.1101 o 1107) fue una princesa musulmana de al-Ándalus, nuera de Muhámmad ibn 'Abbad al-Mu'támid y concubina o esposa de Alfonso VI de León, con quien tuvo a Sancho Alfónsez, muerto en la batalla de Uclés en 1108.

Zaida nació en al-Ándalus, y su nacimiento debió de producirse entre 1063 y 1071. El "Cronicón de Cardeña" dice que era "sobrina d'Auenalfage", personaje al que Menéndez Pidal, en "La España del Cid", identifica con Alháyib, rey de Lérida y Denia (1081-1090). Las informaciones más verídicas sobre la vida de Zaida las proporciona la crónica árabe "Al-bayan al-mugrib" de Ibn Idari, traducida en el siglo XX por E. Lévi-Provençal. Esta crónica, escrita en el año 1306, y hallada en los inicios del siglo XX en la mezquita al-Karawiyin de Fez, dice que se casó con Abu Nasr al-Fath al-Ma'mun, rey de la taifa de Córdoba, hijo del rey sevillano Muhámmad ibn 'Abbad al-Mu'támid (1040-1095).

Su origen y sus relaciones amorosas con el rey Alfonso VI de León han sido objeto de diversas interpretaciones contradictorias entre sí, comenzando por calificarla de hija del rey Muhámmad ibn 'Abbad al-Mu'támid de Sevilla en vez de como su nuera, y continuando con la supuesta dote que trajo consigo para su matrimonio con el Alfonso VI, todo lo cual la historiografía ha demostrado posteriormente que es falso.

Alfonso VI conquistó Toledo en 1085. Alarmados los andalusíes, que ven peligrar sus reinos, tomaron la decisión, no sin grandes reparos, de llamar en su auxilio a unos curtidos guerreros, nómadas bereberes —sobre todo lamtunas— del otro lado del estrecho llamados almorávides.
El rey sevillano al-Mu'támid le pide ayuda en estos términos: «Él [Alfonso VI] ha venido pidiéndonos púlpitos, minaretes, mihrabs y mezquitas para levantar en ellas cruces y que sean regidos por sus monjes [...] Dios os ha concedido un reino en premio a vuestra Guerra Santa y a la defensa de Sus derechos, por vuestra labor [...] y ahora contáis con muchos soldados de Dios que, luchando, ganarán en vida el paraíso». 

Yúsuf cruzó cinco veces el estrecho. La primera vez derrotó a Alfonso VI en la batalla de Sagrajas librada el 23 de octubre de 1086. La segunda vez tuvo lugar el cerco del castillo de Aledo en 1088 que tuvo que levantar sin conseguir su toma. En la tercera incursión (1090) traía la firme decisión de destituir a todos los reyes de taifas y proclamarse emir de todo el al-Ándalus. Cayeron Málaga, Granada y viendo el giro que habían tomado los acontecimientos, el rey al-Mu'támid le pidió a su hijo al-Ma'mún, que había dejado al cargo de Córdoba, que mantuviese a todo trance la posición de la ciudad, pues sería impensable que tras la caída de esta fortaleza se pudiera mantener la de Sevilla. Los almorávides se acercaron a Córdoba y al-Ma'mún, previendo un fatal desenlace, puso a salvo a su esposa, Zaida, y a sus hijos enviándolos con setenta caballeros, familiares incluidos, al castillo de Almodóvar del Río que anteriormente había fortificado y abastecido.

La dispersión de los barrios cordobeses y la connivencia de sus moradores influyeron decisivamente para que el 26 de marzo de 1091 cayera la capital. Dice Abbad en sus cartas: "Fath al-Ma'mún intentó abrirse camino con su espada a través de los enemigos y de los traidores pero sucumbió al número. Se le cortó la cabeza, que la pusieron en la punta de una pica y pasearon en triunfo."

En verano de 1091 Alfonso VI, que recibía las parias de la taifa de Sevilla, intentó cumplir con sus obligaciones de protector enviando, al mando de Álvar Fáñez, un ejército de socorro a Almodóvar del Río. Tras una dura batalla a campo abierto contra los almorávides, en la que ambas partes sufrieron numerosas bajas, Álvar Fáñez se retiró hacia Castilla. Zaida llegó a la corte de Toledo (probablemente con las tropas de Álvar Fáñez), donde fue acogida por Alfonso VI, con quien con el tiempo iniciaría una relación.

Mucho se ha debatido sobre el nacimiento de Sancho, pues las crónicas son contradictorias, lo más probable es que naciera en el segundo semestre de 1093 o en el primero de 1094.

En esa época Alfonso VI era ya de edad madura, y tras tres matrimonios (Inés, Constanza y Berta) y un concubinato (Jimena), no tenía ningún hijo varón que le sucediera. Desde el mismo momento que nació Sancho Alfónsez, el rey lo reconoció como su directo descendiente, siendo llamado a gobernar León, Reino de Castilla, Galicia con Portugal y el resto de condados. En "El quirógrafo de la moneda" se da la noticia de que su padre lo había nombrado en 1107 gobernador de Toledo.

No queda claro en las fuentes si Zaida fue concubina, esposa o ambas cosas, primero concubina y después esposa. En la crónica "De rebus Hispaniae", del arzobispo de Toledo Rodrigo Jiménez de Rada, se cuenta entre las esposas de Alfonso VI. Pero la "Crónica najerense" y el "Chronicon mundi" indican que Zaida fue concubina y no esposa de Alfonso VI. La hipótesis de que Alfonso VI se había casado con Zaida ya ha sido también rechazada por Menéndez Pidal y por Lévi-Provençal.

Otras fuentes dicen que Zaida se acomodó en la corte leonesa, renunció al islam, y se bautizó en Burgos con el nombre de Isabel. Sin embargo, no solo conservó todas sus costumbres sino que las difundió e introdujo nuevos y frescos aires culturales de la sociedad musulmana. El arabista Ángel González Palencia escribe que la corte de Alfonso VI, casado con Zaida (sic), parecía una corte musulmana: «sabios y literatos muslimes andaban al lado del rey, la moneda se acuñaba en tipos semejantes a los árabes, los cristianos vestían a usanza mora y hasta los clérigos mozárabes de Toledo hablaban familiarmente el árabe y conocían muy poco el latín, a juzgar por las anotaciones marginales de muchos de sus breviarios».

Según Jaime de Salazar y Acha, seguido por otros autores, entre ellos, Gonzalo Martínez Díez, contrajeron matrimonio en 1100 tras enviudar Alfonso de la reina Berta, quedando legitimado el hijo de ambos, que se convirtió en príncipe heredero del reino cristiano. Para Salazar y Acha, Zaida y la cuarta esposa del rey, Isabel, son la misma persona, «Pese a los ímprobos esfuerzos de los historiadores posteriores por intentarnos demostrar que no era la mora Zaida», y también sería la madre de Elvira y de Sancha Alfónsez. Otra razón que esgrime el autor es el hecho que poco después de la boda del rey con Isabel, el infante Sancho comienza a confirmar diplomas regios y de no ser la nueva reina Zaida, no hubiera consentido el nuevo protagonismo de Sancho en detrimento de sus posibles futuros hijos. También cita un diploma en la catedral de Astorga del 14 de abril de 1107 donde el rey concede unos fueros y actúa "cum uxore mea Elisabet et filio nostro Sancio". Este es el único diploma donde se cita como «nuestro hijo», ya que en otros solamente figura como hijo del rey aunque también aparece la reina Isabel. De la misma opinión es Martínez Díez, quien resume así su planteamiento: «Años después, el 1100, esta mora Zaida, habiendo abrazado el cristianismo y siendo bautizada con el nombre de Isabel, contraería matrimonio con el rey Alfonso, convirtiéndose así en la reina Isabel. Su hijo Sancho, legitimado por este matrimonio, pasó a ser príncipe heredero»

Reilly acepta sin embargo que fueron dos Isabel, la mora Zaida —bautizada Isabel— y la otra Isabel, seguramente francesa, pero argumenta que para reforzar la posición de Sancho Alfónsez, el rey Alfonso anuló el matrimonio con Isabel en marzo de 1106 y se casó con Zaida. El 27 de marzo de 1106, el rey Alfonso confirmó una donación al monasterio de Lorenzana: "(...) eiusdemque Helisabeth regina sub maritali copula legaliter aderente", una fórmula inusual que confirma un legítimo matrimonio. Salazar y Acha y Reilly interpretan esta cita como prueba de que el rey había casado con Zaida, legitimando así al hijo de ambos y la relación de concubinato. Gambra, sin embargo, se opone y dice que es «una argumentación extremadamente endeble, empezando por la referencia documental, escasamente significativa. Su carácter es más bien ornamental y literario». Montaner Frutos también dice que la hipótesis es «poco verosímil y problemática» ya que no era necesario que el rey casase con Zaida para legitimar a su hijo Alfonso y que, además, Isabel la francesa falleció en 1107 según reza en su epitafio. También menciona Montaner Frutos una donación de la reina Urraca años después, en 1115, cuando donó unas propiedades a la catedral de Toledo y solamente menciona a una Isabel como la esposa del rey.

Fruto de su relación con el rey Alfonso VI nació: 

Si Zaida e Isabel, la cuarta esposa del rey Alfonso VI, son la misma persona, como sostienen algunos historiadores, también seria la madre de:


El rey Alfonso VI quiso que los restos mortales de Zaida descansaran en el mismo lugar que había destinado para él mismo, sus reinas e hijos, y por ello, ciertas fuentes señalan que fue sepultada en el Monasterio de San Benito de Sahagún, exactamente en el coro bajo, antes de llegar al atril. Quadrado, en sus "Recuerdos y bellezas de España", dice que "en Sahagún descansa en túmulo alto el rey y debajo de una sencilla lápida Isabel y el joven Sancho, su hijo." 

Según Elías Gago, en la lápida del monasterio de Sahagún que cubría los restos de Zaida aparecía esculpida la siguiente inscripción:

Pero en el Panteón de Reyes de San Isidoro de León se conserva otra lápida, cuyo epitafio, redactado en términos latinos, dice así:

Dado que no se puede estar enterrado en dos sitios a la vez, Henrique Flórez sugiere que probablemente primero se la enterrara en el lugar donde murió y después fuera trasladada a Sahagún.

El sepulcro que contenía los restos de Alfonso VI fue destruido en 1810, durante el incendio que sufrió el Monasterio de San Benito. Los restos mortales del rey y los de varias de sus esposas, entre ellos los de Zaida, fueron recogidos y conservados en la cámara abacial hasta el año 1821, en que fueron expulsados los religiosos del monasterio, siendo entonces depositados por el abad Ramón Alegrías en una caja, que fue colocada en el muro meridional de la capilla del Crucifijo, hasta que, en enero de 1835, los restos fueron recogidos de nuevo e introducidos en otra caja, siendo llevados al archivo, donde se hallaban en esos momentos los despojos de las esposas del soberano. El propósito era colocar todos los restos reales en un nuevo santuario que se estaba construyendo entonces. No obstante, cuando el monasterio de San Benito fue desamortizado en 1835, los religiosos entregaron las dos cajas con los restos reales a un pariente de un religioso, que las ocultó, hasta que en el año 1902 fueron halladas por el catedrático del Instituto de Zamora Rodrigo Fernández Núñez.

En la actualidad, los restos mortales de Alfonso VI reposan en el Monasterio de Benedictinas de Sahagún, a los pies del templo, en un arca de piedra lisa y con cubierta de mármol moderna, y en un sepulcro cercano, igualmente liso, yacen los restos de varias de las esposas del rey, entre ellos los que se atribuyen a Zaida. 

Los restos que se conservan de Zaida (la bóveda craneal, la clavícula derecha, el húmero izquierdo y la mitad del distal del radio de ese mismo lado) dictaminan que tenía una estatura de 152,6 cm. Los especialistas que estudiaron sus restos llegaron a la conclusión de que en el momento de su muerte debía tener unos 30 años de lo que se puede deducir su fecha de nacimiento dependiendo del año en que falleciera.

Según indica una de las lápidas que se le atribuyen, murió de postparto, el jueves 12 de septiembre (no se lee el año), pero debió ser de una hija o de un hijo que moriría de corta edad, pues nada se sabe de esta descendencia.

Cuenca ha querido reconocer a la que de una u otra forma ha influido en su historia y así, en el pleno del Ayuntamiento del 16 de febrero de 1959, siendo alcalde Bernardino Moreno Cañadas, se adoptó el acuerdo de otorgar una calle en el Polígono de Los Moralejos, en el Cerro Pinillos, a la Princesa Zaida. Actualmente es una de las principales calles céntricas de Cuenca.

En Madrid, Zaida también dispone de su calle, desde el 14 de julio de 1950, siendo alcalde el Conde Santamarta de Babio. Discurre desde la de Carlos Daban a la de la Oca en el distrito de Carabanchel.

También hay una calle llamada Zaida en Arboleas (Almería), en el barrio de La Perla.

En la ciudad de León existe también una calle con el nombre de Reina Zaida.



</doc>
<doc id="3399" url="https://es.wikipedia.org/wiki?curid=3399" title="Monoide">
Monoide

En álgebra abstracta, un monoide es una estructura algebraica con una operación binaria, que es asociativa y tiene elemento neutro, es decir, es un semigrupo con elemento neutro.

Un monoide formula_1 es una estructura algebraica en la que formula_2 es un conjunto y formula_3 es una operación binaria interna en formula_2:

Que cumple las siguientes tres propiedades (la primera es redundante con la definición):
Es fácil demostrar que el elemento neutro es necesariamente único por lo que es redundante exigir su unicidad en este axioma o propiedad. En esencia, un monoide es un semigrupo con elemento neutro.

Si además se cumple la propiedad conmutativa:
Se dice que es un monoide conmutativo o abeliano.

Dado un conjunto A de caracteres alfanuméricos, que llamaremos alfabeto, una cadena alfanumerica del alfabeto A es una secuencia de elementos de A en cualquier orden y de cualquier longitud, si tomas el conjunto como:

Cadenas del alfabeto A, que representamos C(A) pueden ser:

La cadena vacía, la que no tiene ningún carácter, sería:

Definimos la operación formula_12 de concatenación de cadenas del alfabeto A como:

que podemos representar, de las siguientes formas:


podemos ver que formula_16 tiene estructura algebraica de monoide:

1.- Es una operación interna: para cualquiera dos cadenas del alfabeto A su concatenación es una cadena de A:

2.- Es asociativa:

3.- Tiene elemento neutro: para todo elemento a cadena de caracteres de A, existe la cadena vacía formula_11 de A, de modo que:

La concatenación de cadenas de caracteres no es conmutativa:

Siendo a, b de C(A) la concatenación de a con b no es igual a la concatenación de b con a.

Luego la concatenación de cadenas alfanuméricas es un monoide no conmutativo.

Partiendo del conjunto de los números naturales:

y la operación multiplicación, podemos ver que: formula_23 es un monoide

1.- Es una operación interna: para cualquiera dos números naturales su multiplicación es un número natural:

2.- Es asociativa:

3.- Tiene elemento neutro: el 1 en N, es neutro para todos los números naturales ya que cumple:

4.- La multiplicación de números naturales es conmutativa:

El conjunto de los números naturales, bajo la operación multiplicación: formula_23, tiene estructura algebraica de monoide conmutativo o abeliano.

Una categoría monoidal, es una categoría con una operación binaria que convierte a la categoría en un monoide.
Dos ejemplos:



</doc>
<doc id="3400" url="https://es.wikipedia.org/wiki?curid=3400" title="Criptografía">
Criptografía

La criptografía (del griego κρύπτos (kryptós), «secreto», y γραφή (graphé), «grafo» o «escritura», literalmente «escritura secreta») se ha definido, tradicionalmente, como el ámbito de la criptología que se ocupa de las técnicas de cifrado o codificado destinadas a alterar las representaciones lingüísticas de ciertos mensajes con el fin de hacerlos ininteligibles a receptores no autorizados. Estas técnicas se utilizan tanto en el arte como en la ciencia y en la tecnología. Por tanto, el único objetivo de la criptografía era conseguir la confidencialidad de los mensajes, para lo cual se diseñaban sistemas de cifrado y códigos, y la única criptografía existente era la llamada criptografía clásica, donde se ocultaba tanto el algoritmo como la clave criptográfica.

La aparición de la informática y el uso masivo de las comunicaciones digitales, han producido un número creciente de problemas de seguridad. Las transacciones que se realizan a través de la red pueden ser interceptadas, y por tanto, la seguridad de esta información debe garantizarse. Este desafío ha generalizado los objetivos de la criptografía para ser la parte de la criptología que se encarga del estudio de los algoritmos, protocolos (se les llama protocolos criptográficos), y sistemas que se utilizan para proteger la información y dotar de seguridad a las comunicaciones y a las entidades que se comunican.

Para ello los criptógrafos investigan, desarrollan y aprovechan técnicas matemáticas que les sirven como herramientas para conseguir sus objetivos. Los grandes avances producidos en el mundo de la criptografía han sido posibles gracias a la evolución que se ha producido en el campo de la matemática y la informática.

La criptografía actualmente se encarga del estudio de los algoritmos, protocolos y sistemas que se utilizan para dotar de seguridad a las comunicaciones, a la información y a las entidades que se comunican. El objetivo de la criptografía es diseñar, implementar, implantar, y hacer uso de sistemas criptográficos para dotar de alguna forma de seguridad. Por tanto el tipo de propiedades de las que se ocupa la criptografía son, por ejemplo:

Un sistema criptográfico es seguro respecto a una tarea si un adversario con capacidades especiales no puede romper esa seguridad, es decir, el atacante no puede realizar esa tarea específica.

En el campo de la criptografía muchas veces se agrupan conjuntos de funcionalidades que tienen alguna característica común y a ese conjunto lo denominan 'Criptografía de' la característica que comparten. Veamos algunos ejemplos:

El objetivo de un sistema criptográfico es dotar de seguridad. Por tanto para calibrar la calidad de un sistema criptográfico es necesario evaluar la seguridad que aporta dicho sistema. 

Para poder evaluar mejor la seguridad de un sistema criptográfico, además de las verificaciones internas de seguridad que la organización haga, se puede considerar hacer público a todo el mundo los entresijos del sistema. Sin embargo, al hacer pública esa información se facilita el que alguien pueda descubrir alguna debilidad y la aproveche o incluso la haga pública para que otros la puedan utilizar. Cuanta más información se publique más fácil será encontrar debilidades tanto para buenos objetivos (mejorar el producto) como para malos (realizar ataques). En resumen cuanta más información se publique más personas podrán evaluar la seguridad y se podrán corregir las debilidades que se encuentren, pero también aumenta la exposición a ataques. En función de las decisiones que se tomen se establecerá una política de revelación.

Se considera que la seguridad de un sistema criptográfico debe descansar sobre el tamaño de las claves utilizadas y no sobre el secreto del algoritmo. Esta consideración se formaliza en el llamado principio de Kerckhoffs. Esto no quiere decir que cuando usemos criptografía tengamos que revelar los algoritmos, lo que quiere decir es que el algoritmo tiene que ser seguro aunque este sea difundido. Evidentemente si un sistema criptográfico es seguro aun revelando su algoritmo, entonces será aún más seguro si no lo revelamos. 

A la política de revelación de no publicar ninguna información para que ningún atacante encuentre debilidades se le llama de no revelación y sigue una estrategia de seguridad por oscuridad. A la política de revelación de revelar toda la información se le llama revelación total. Entre ambos tipos de política de revelación hay estrategias intermedias, llamadas "de revelación parcial".

Hay básicamente tres formas de romper la seguridad de un sistema criptográfico


Las personas o entidades interesadas en romper la seguridad de este tipo de sistemas tienen en cuenta todos estos frentes. Por ejemplo las informaciones de Edward Snowden revelan que el programa Bullrun adopta estos tres tipos de estrategias.

Cuando se evalúa la seguridad de un sistema criptográfico se puede calibrar la seguridad que aporta en función de si este es seguro de forma incondicional o si es seguro sólo si se cumplen ciertas condiciones.

Se dice que un sistema criptográfico tiene una seguridad incondicional sobre cierta tarea si un atacante no puede resolver la tarea aunque tenga infinito poder computacional. En función de la tarea sobre la que se dice que el sistema criptográfico es incondicionalmente seguro, podemos hablar por ejemplo de:


Es habitual que los sistemas incondicionalmente seguros tengan inconvenientes importantes como por ejemplo en la longitud de las claves (libreta de un solo uso).

Para certificar una seguridad incondicional los criptólogos se suelen basar en la teoría de la información y, por tanto, en la teoría de la probabilidad.

El que un sistema tenga seguridad incondicional no quiere decir que su seguridad sea inviolable. Veamos dos consideraciones:

Ejemplos de ataques que se aprovechan de vulnerabilidades producidas por una mala elicitación o análisis de requisitos, diseño, desarrollo, implementación o pruebas del producto software o hardware: desbordamiento de buffer, Inyección SQL, Cross Site Scripting, ataque informático basado en deficiencias del hardware.

Se dice que un sistema criptográfico tiene una seguridad condicional sobre cierta tarea si un atacante puede teóricamente resolver la tarea, pero no es computacionalmente factible para él (debido a sus recursos, capacidades y acceso a información).

Hay un tipo especial de seguridad condicional, llamada Seguridad Demostrable . La idea es mostrar que romper un sistema criptográfico es computacionalmente equivalente a resolver un problema matemático considerado como difícil. Esto es, que se cumplen las dos siguientes sentencias:


La seguridad demostrable es difícil de lograr para sistemas criptográficos complejos. Se ha desarrollado una metodología (modelo de oráculo aleatorio) para diseñar sistemas que no tienen realmente una seguridad demostrable, pero que dan unas "buenas sensaciones" respecto a su seguridad. La idea básica es diseñar un sistema ideal que usa una o varias funciones aleatorias -también conocidas como oráculos aleatorios- y probar la seguridad de este sistema matemático. A continuación el sistema ideal es implementado en un sistema real reemplazando cada oráculo aleatorio con una "buena" y "adecuada" función pseudoaleatoria conocida -generalmente un código de detección de manipulaciones como SHA-1 o MD5. Si las funciones pseudoaleatorias utilizadas tiene buenas propiedades, entonces uno puede esperar que la seguridad probada del sistema ideal sea heredada por el sistema real. Observar que esto ya no es una prueba, sino una evidencia sobre la seguridad del sistema real. Se ha demostrado que esta evidencia no siempre es cierta y que es posible romper sistemas criptográficos cuya seguridad se apoya en el modelo de oráculo aleatorio.

Para evaluar la seguridad de un esquema criptográfico se suelen usar tres enfoques principales. Cada enfoque difiere de las suposiciones acerca de las capacidades de los oponentes criptoanalistas. El primer método está basado en la teoría de la información, y ofrece una seguridad incondicional y por tanto una seguridad independiente del poder de computación de los adversarios. El enfoque basado en la teoría de la complejidad comienza a partir de un modelo abstracto para la computación, y asume que el oponente tienen un poder limitado de computación. El tercer enfoque intenta producir soluciones prácticas. Para ello estima la seguridad basándose en el mejor algoritmo conocido para romper el sistema y estima de forma realista el poder necesario de computación o de hardware para romper el algoritmo. A este enfoque se le suele llamar enfoque basado en la práctica.

En este enfoque se evalúa la seguridad del sistema utilizando las herramientas que proporciona la teoría de la información. Permite declarar sistemas incondicionalmente seguros, es decir, sistemas seguros independientemente del poder de computación del atacante. 

La teoría de la información proporciona valiosas herramientas para analizar la seguridad de los sistemas criptográficos. Por ejemplo está la entropía, distancia de unicidad, el concepto de secreto perfecto, etcétera.

En este enfoque se evalúa la seguridad de los sistemas criptográficos en función de la cantidad de trabajo computacional requerido para romperlo. Para estimar esa cantidad de trabajo se estudia la complejidad computacional de los mejores métodos conocidos hasta ahora para realizar esa tarea. En función de los resultados de este estudio y del poder computacional límite estimado para el atacante, se decide si esa cantidad de trabajo es realizable por un atacante. Si ese trabajo no es realizable se dice que el sistema es seguro desde un punto de vista computacional (seguridad computacional; en inglés, "").

Este tipo de enfoque para evaluar la seguridad es muy usado en la criptografía asimétrica. En concreto, la seguridad de muchos de los algoritmos de la criptografía asimétrica están basados en el análisis de complejidad de los métodos conocidos para el cálculo de factorización de enteros y del logaritmo discreto.

Por definición, el tipo de seguridad que aporta este tipo de enfoque es una seguridad condicional basada en los métodos de resolución de problemas evaluados. En este punto hay que tener en cuenta dos consideraciones:


El objetivo de este enfoque es producir soluciones prácticas a partir del estudio de sistemas concretos y de la experiencia acumulada. Es un enfoque de prueba-error donde se proponen soluciones basándose en la experiencia y luego se somete esa solución a un proceso intensivo en el que se intenta romper su seguridad. A partir de este enfoque se han hecho importantes avances en conseguir sistemas robustos ya que los criptógrafos diseñan ataques y posteriormente adaptan los sistemas para anular dichos ataques. Por ejemplo, de esta forma se han conseguido importantes avances en la seguridad frente a ataques basados en estudios estadísticos y ataques meet in the middle.

Es frecuente, en este tipo de enfoque, diseñar bloques con ciertas propiedades demostradas estableciendo una "biblioteca" de bloques disponibles. Ejemplos de propiedades buenas para este tipo de bloques pueden ser: buenas propiedades estadísticas, buenas propiedades para la confusión y difusión, o de no linealidad. Posteriormente estos bloques se ensamblan para la construcción de sistemas criptográficos que aprovechan sus propiedades para dotar de mayor seguridad.

Este enfoque permite llegar a establecer sistemas que tienen seguridad condicional. Este tipo de sistemas tienen una seguridad computacional.

La historia de la criptografía es larga y abunda en anécdotas. Ya las primeras civilizaciones desarrollaron técnicas para enviar mensajes durante las campañas militares, de forma que si el mensajero era interceptado la información que portaba no corriera el peligro de caer en manos del enemigo.

El primer método de criptografía fue en el siglo V a. C., era conocido como "Escítala", un método de trasposición basado en un cilindro que servía como clave en el que se enrollaba el mensaje para poder cifrar y descifrar. El segundo criptosistema que se conoce fue documentado por el historiador griego Polibio: un sistema de sustitución basado en la posición de las letras en una tabla. También los romanos utilizaron sistemas de sustitución, siendo el método actualmente conocido como César, porque supuestamente Julio César lo empleó en sus campañas, uno de los más conocidos en la literatura (según algunos autores, en realidad Julio César no usaba este sistema de sustitución, pero la atribución tiene tanto arraigo que el nombre de este método de sustitución ha quedado para los anales de la historia).

En 1465 el italiano Leon Battista Alberti inventó un nuevo sistema de sustitución polialfabética que supuso un gran avance de la época. Otro de los criptógrafos más importantes del siglo XVI fue el francés Blaise de Vigenère que escribió un importante tratado sobre "la escritura secreta" y que diseñó una cifra que ha llegado a nuestros días asociada a su nombre. A Selenus se le debe la obra criptográfica "Cryptomenytices et Cryptographiae" (Luneburgo, 1624). En el siglo XVI María Estuardo, reina de Escocia, fue ejecutada por su prima Isabel I, reina de Inglaterra, al descubrirse un complot de aquella tras un criptoanálisis exitoso por parte de los matemáticos de Isabel. Durante los siglos XVII, XVIII y XIX, el interés de los monarcas por la criptografía fue notable. Las tropas de Felipe II emplearon durante mucho tiempo una cifra con un alfabeto de más de 500 símbolos que los matemáticos del rey consideraban inexpugnable. Cuando el matemático francés François Viète consiguió criptoanalizar aquel sistema para el rey de Francia, a la sazón Enrique IV, el conocimiento mostrado por el rey francés impulsó una queja de la corte española ante del papa Pío V acusando a Enrique IV de utilizar magia negra para vencer a sus ejércitos.

Durante la Primera Guerra Mundial, los Alemanes usaron el cifrado ADFGVX. Este método de cifrado es similar a la del tablero de ajedrez Polibio. Consistía en una matriz de 6 x 6 utilizado para sustituir cualquier letra del alfabeto y los números 0 a 9 con un par de letras que consiste de A, D, F, G, V o X.
Desde el siglo XIX y hasta la Segunda Guerra Mundial, las figuras más importantes fueron la del holandés Auguste Kerckhoffs y la del prusiano Friedrich Kasiski. Pero es en el siglo XX cuando la historia de la criptografía vuelve a experimentar importantes avances. En especial durante las dos contiendas bélicas que marcaron al siglo: la Gran Guerra y la Segunda Guerra Mundial. A partir del siglo XX, la criptografía usa una nueva herramienta que permitirá conseguir mejores y más seguras cifras: las máquinas de cálculo. La más conocida de las máquinas de cifrado posiblemente sea la máquina alemana Enigma: una máquina de rotores que automatizaba considerablemente los cálculos que era necesario realizar para las operaciones de cifrado y descifrado de mensajes. Para vencer al ingenio alemán, fue necesario el concurso de los mejores matemáticos de la época y un gran esfuerzo computacional. No en vano, los mayores avances tanto en el campo de la criptografía como en el del criptoanálisis no empezaron hasta entonces.

Tras la conclusión de la Segunda Guerra Mundial, la criptografía tiene un desarrollo teórico importante, siendo Claude Shannon y sus investigaciones sobre teoría de la información esenciales hitos en dicho desarrollo. Además, los avances en computación automática suponen tanto una amenaza para los sistemas existentes como una oportunidad para el desarrollo de nuevos sistemas. A mediados de los años 70, el Departamento de Normas y Estándares norteamericano publica el primer diseño lógico de un cifrador que estaría llamado a ser el principal sistema criptográfico de finales de siglo: el Estándar de Cifrado de Datos o DES. En esas mismas fechas ya se empezaba a gestar lo que sería la, hasta ahora, última revolución de la criptografía teórica y práctica: los sistemas asimétricos. Estos sistemas supusieron un salto cualitativo importante, ya que permitieron introducir la criptografía en otros campos que hoy día son esenciales, como el de la firma digital.

La mayor parte de los mensajes de correo electrónico que se transmiten por Internet no incorporan seguridad alguna, por lo que la información que contienen es fácilmente accesible a terceros. Para evitarlo, la criptografía también se aplica al correo electrónico. Entre las diversas ventajas que tiene usar un certificado al enviar un correo electrónico, podríamos destacar la seguridad que nos aporta ya que así evita que terceras personas (o "hackers") puedan leer su contenido, o bien que tengamos la certeza de que el remitente de este correo electrónico es realmente quien dice ser.





</doc>
