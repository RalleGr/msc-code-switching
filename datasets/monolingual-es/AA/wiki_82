<doc id="17216" url="https://es.wikipedia.org/wiki?curid=17216" title="Esvástica">
Esvástica

La esvástica o suástica y la sauvástica son una cruz cuyos brazos están doblados en ángulo recto:

Geométricamente, sus 20 lados hacen de ella un icoságono irregular.

El término español «esvástica» proviene del idioma sánscrito "suastíka" (en devanagari: ), que literalmente significa ‘muy auspicioso’, pero también puede significar:

Desde hace unos siete mil años, los pastores preiranios de Djowi y localizaciones cercanas a Susa representaban la esvástica girando hacia la izquierda, presumiblemente como un número, siendo la cruz (+) el 10, la X el 11, la X empezando a girar el 12 y finalmente la esvástica girando el 13 (5000 a. C.). El objeto arqueológico más antiguo con una esvástica data del V milenio a. C. Fue encontrado en Samarra y se trata de un plato de arcilla (barro) con figuras femeninas que forman una cruz gamada y escorpiones (Parrot, 1963).

Según sir Alexander Cunningham (citado por sir Monier William), se trata de un monograma generado por la conjunción de las letras "su astí" en caracteres ashoka (anteriores a la escritura devanagari, que son las que desde hace varios siglos se utilizan en la escritura del sánscrito). Según algunos autores esto demuestra que el símbolo no fue creado en esta era, sino aproximadamente en el siglo V a. C., e incluso pudo haber sido anterior. Aunque los visnuistas dicen que la esvástica está eternamente dibujada en una de las cuatro manos del dios Visnú.

Este símbolo, que ha aparecido repetidamente en la iconografía, el arte y el diseño producidos a lo largo de toda la historia de la humanidad, ha representado conceptos muy diversos. Entre estos se encuentran la suerte, el Brahman, el concepto hindú de "samsara" (reencarnación) o a Suria (dios del Sol), por citar solamente los más representativos. En principio la esvástica fue usada como símbolo entre los hindúes. Se menciona por primera vez en los "Vedas" (las escrituras sagradas del hinduismo más primitivo), pero su uso se traslada a otras religiones de la India, como el budismo y el jainismo.



Además hay otros símbolos que tienen cierto parecido con la esvástica, como el "triskel" o trinacria (del griego "triskelion") usado como emblema de la isla de Man o de Sicilia y recurrente motivo celta. Posteriormente el "lauburu" vasco, de brazos curvos, reanimación moderna del lábaro cántabro, será también visualmente reminiscente de la esvástica.

La esvástica es un motivo bastante común en la cultura romana y el arte de la India de nuestros días, así como en la arquitectura del pasado, habiendo sido representada en mosaicos, frisos y otras obras del mundo antiguo.

Símbolos similares en la arquitectura occidental clásica incluyen la cruz, el triskel (tres piernas dobladas unidas por su parte superior) y el lábaro cántabro del norte de España. La esvástica también recibe en este contexto el nombre de "gammadion" (que proviene de "gamma", la tercera letra del alfabeto griego).

En el arte de China, Corea y del Japón, a menudo puede hallarse la esvástica como motivo que se repite continuamente (algo que lejanamente puede recordar un patrón en greca). Uno de estos motivos, llamado "sayagata" en el arte textil japonés, incluye esvásticas dextrógiras y levógiras que se unen mediante líneas.
Símbolos esvasticoides se han encontrado abundantemente en las ruinas de Troya.

Ya en Occidente, en el arte romano, románico y gótico la esvástica como figura aislada se encuentra raramente, pero se halla con más frecuencia como patrón que se repite en la decoración de bordes o superficies. Algunos de los teselados del suelo de la catedral de Amiens presentan motivos de esvásticas entrelazadas o unidas. Los bordes adornados con grecas de esvásticas unidas eran comunes en la arquitectura romana y pueden verse también en edificios de estilo neoclásico.

En ciertas tradiciones paganas europeas, la esvástica, en sus dos modalidades, dextrógira y levógira, es el símbolo que se utiliza para representar, respectivamente, las puertas del nacimiento y de la muerte.

En estas tradiciones, la esvástica también está ubicada en la rueda zodiacal, en los signos de Piscis y de Virgo, representando el primero la puerta del nacimiento y el segundo la puerta de la muerte. 
El eje que trazan ambos signos divide la rueda zodiacal en dos semicírculos. El semicírculo que va desde Piscis hasta Virgo (siguiendo el orden de los signos) representa una vida o encarnación, y el que transcurre desde Virgo hasta Piscis representa el tránsito o tiempo entre dos vidas. La rueda zodiacal entera representa el eterno transitar de una encarnación a la siguiente o metempsicosis.

De ahí que en ese transitar cíclico existan dos puertas para pasar de un estado a otro: la puerta del nacimiento y la puerta de la muerte, representadas por la esvástica dextrógira y levógira, respectivamente. Son puertas giratorias y el sentido de giro es vital porque determina si se dará paso a una vida o al tránsito.

Obsérvese que la esvástica dextrógira se convierte en levógira si se mira desde atrás. Lo que viene a significar que tanto el nacimiento como la muerte son relativos y dependen de la perspectiva desde la que se mira. Un nacimiento en la vida es una muerte en el tránsito, y una muerte en la vida es un nacimiento en el tránsito. Cuando un ser muere en la vida, nace en el más allá, y viceversa. Y el símbolo de la esvástica es sencillamente perfecto para representar esta idea de relatividad entre la vida y la muerte.

Podría decirse que la evidencia arqueológica más antigua de la esvástica documentado, se encuentra ampliamente utilizado en el contexto ritualista simbólico en múltiples artefactos de arcilla de múltiples sitios de la Europa Neolítica de la : incluyendo -Turdas- , y .

En el budismo la esvástica se usa en posición horizontal (a diferencia de la esvástica nazi, que aparece rotada 45 grados en la bandera del Reich). Al menos desde la Dinastía Liao forma parte de la escritura china (en pinyin: wan4), simbolizando el carácter 萬 (wan4), que quiere decir ‘todo’ y ‘eternidad’, y que apenas se usa. Las esvásticas (girando a derecha o a izquierda) aparecen sobre el pecho de algunas estatuas de Buda. Debido a la asociación de la esvástica dextrógira con el nazismo, las esvásticas budistas son casi todas levógiras desde mediados del siglo XX. Este tipo de esvástica puede hallarse a menudo en los envases o envoltorios de comida china para indicar que tales productos son vegetarianos y pueden ser consumidos por budistas estrictos. Esta misma marca se encuentra en los cuellos de ropa usada por niños chinos para protegerlos de malos espíritus. La esvástica también simboliza los 4 elementos que son: fuego, tierra, agua y aire.

Algunas iglesias cristianas románicas y góticas contienen cierta decoración de esvásticas, reminiscencias de motivos romanos anteriores, ya que los cristianos la usaban para disimular una cruz y así evitar la persecución.

Según investigadores, como John Cooper, señalan que en tiempos medievales se podía encontrar en diversos templos en Europa, la figura de gammadión, una cruz formada por cuatro letras gamma (Γ) imitando la forma de la esvástica, representando a los cuatro redactores de los evangelios canónicos, libros principales del cristianismo, siendo el centro, el símbolo de Jesús de Nazaret.

Otros autores, como Guillermo Alfredo Terrera, señalan que hay evidencias de que ciertos grupos cristianos utilizaron esvásticas dextrógiras, que colocaron en sus tumbas y monumentos, al menos durante los primeros siglos de expansión cristiana en el continente europeo, pudiendo encontrarse en lugares tan diversos como la Basílica de Santa Eulalia (Mérida) o la catedral de Notre Dame (de París).

La esvástica se encuentra por todas partes en los templos de la religión hindú, así como en símbolos, altares, escenas e iconografía en la India y en el Nepal, tanto en el pasado como en nuestros días. En el hinduismo, los dos símbolos representan las dos formas del Brahman (el concepto impersonal de Dios). En sentido de las agujas del reloj representa la evolución del universo "(pravritti)", representada por el dios creador Brahmá, mientras que en sentido antihorario representa la involución del universo "(nivritti)", representada por el dios destructor Shivá. También se puede ver de qué manera apunta hacia los cuatro puntos cardinales, simbolizando así estabilidad. Su empleo como símbolo solar puede verse en la representación de Suria, dios del Sol para los hindúes. Viene usándose como señal de buena suerte. También se concibe como un símbolo de poder y son populares las versiones que asemejan la esvástica a la figura de un hombre. Hasta hoy se utiliza en los "iantras" y en motivos religiosos hindúes. Puede apreciarse en los muros de los templos por todo el subcontinente indio. También puede encontrarse en las notas que acompañan los regalos personales y en el encabezado de las cartas. La esvástica se tiene por un símbolo sagrado y de buen auspicio entre los hindúes. Se usa normalmente en la decoración de todo tipo de elementos relacionados con la cultura hindú. El dios Ganesha está asociado con el símbolo de la esvástica. Su uso está ampliamente extendido en la India y Nepal.

En el jainismo el motivo de la esvástica se combina con el de una mano.

En el Japón la esvástica es un antiguo símbolo religioso que recibe el nombre de "manji". Como tal, aparece con cierta frecuencia en productos japoneses exportados a Occidente, como historietas y productos derivados de ellas.

Debido a su parecido con la esvástica nazi, los mapas tuvieron que alterarse en sus traducciones para países occidentales. En los mapas callejeros de las ciudades japonesas, el ícono de una esvástica dextrógira indica un templo budista.

En la religión ancestral protoindoeuropea, la esvástica o "rueda solar" a menudo representaba al sol y su poder. Se ha relacionado con la cruz solar (un símbolo parecido a la cruz celta pero con los brazos de igual longitud) y asimismo existieron combinaciones de ambos. En la Hispania prerromana aparece entre los arévacos. En la mitología germánica, la esvástica también representa poder e iluminación, de ahí que se asociase a los dioses del trueno como Thor (la esvástica era el símbolo de Mjolnir, el martillo de Thor), en la mitología nórdica, y Taranis, en la mitología celta. En Irlanda, una "rueda solar" semejante se conoce como cruz de Brigit y se usa para alejar el mal.

El autor británico Rudyard Kipling, sumamente influenciado por la cultura de la India, tenía una esvástica en la cubierta de todos sus libros hasta que el surgimiento del nazismo lo hizo poco conveniente.La esvástica fue también el símbolo elegido inicialmente en el Reino Unido por el o Movimiento Nacional de Ahorro creado en 1916 para financiar gastos estatales, especialmente de guerra.

También los boy scouts lo usaron como símbolo. Según Johnny Walker, los scouts empezaron a usarlo en el primer "Thanks Badge" que data de 1911. El motivo de la Medalla al Mérito, diseñado por Robert Baden-Powell en 1922, como signo de buena suerte al que la recibiera, añade una esvástica a la flor de lis de los Scouts. Al igual que Kipling, Baden-Powell probablemente conoció este símbolo en la India. En 1934 muchos scouts solicitaron que se cambiase la insignia debido a que el partido nazi ya usaba la esvástica. En 1935 se lanzó una nueva medalla al mérito sin la esvástica.

Entre 1918 y 1945 la Fuerza Aérea Finlandesa empleó la esvástica ("hakaristi", en finés) como distintivo nacional oficial. La organización Lotta Svärd también la usó. Una esvástica azul fue marca de buena suerte empleada por el conde sueco Erich von Rosen que durante la guerra civil finlandesa donó el primer avión al "Ejército Blanco" Finlandés. No existe conexión con el uso nazi de la esvástica. Todavía aparece en muchas medallas finlandesas, disimulada visualmente.

La compañía sueca ASEA, hoy parte de Asea Brown Boveri, desde el siglo XIX hasta 1933 tuvo en su logotipo una esvástica que fue finalmente eliminada.

En Letonia una esvástica conocida como "Cruz del trueno" o "Cruz de fuego" fue el distintivo de la Fuerza Aérea Letona. También fue usado por el movimiento fascista letón Perkonkrusts (Cruz del Trueno), así como por organizaciones no políticas.

Swastika es el nombre de una pequeña comunidad del norte de Ontario, Canadá. Está situada a unos 580 km de Toronto y a 5 km del lago Kirkland. El pueblo se fundó en 1906 y, a raíz del descubrimiento de yacimientos auríferos, se forma la Swastika Mining Company en 1908. Durante al Segunda Guerra Mundial el gobierno de Ontario quiso cambiar el nombre del pueblo pero la población se opuso.

En Windsor (Nueva Escocia), entre 1905 y 1916, hubo un equipo de hockey sobre hielo llamado Los Swastikas y sus uniformes mostraban ese símbolo. Equipos con el mismo nombre los hubo también en Edmonton (Alberta), en torno a 1916, y en Fernie (Columbia Británica), alrededor de 1922.

Por otra parte, en Rusia también se hacía uso de la cruz gamada. Se le atribuía la buena suerte. En 1917, tras la abdicación del zar Nicolás II, el gobierno provisional ruso diseñó billetes de rublos que portaban la esvástica. Cabe a destacar que este símbolo llegó a ser utilizado por los soldados zaristas en la Primera Guerra Mundial y por algunos soviéticos en la Guerra Civil Rusa.

La estación ferroviaria de Retiro en la ciudad de Buenos Aires fue, al momento de su construcción, una de las estaciones más grandes del mundo, por esta razón se puso especial esmero en su construcción en la primera década del siglo XX. Las columnas de su fachada están decoradas con esvásticas entrelazadas por las puntas.

Para muchas de las tribus nativas norteamericanas, especialmente para los hopi de Arizona, la esvástica simboliza la migración, realizada en oportunidad de la llegada de los hombres al cuarto mundo a través del "sipapu" o la ‘vagina de la tierra’. El dios Massaw (el creador) indicó a los recién llegados a migrar en las cuatro direcciones o rincones de la tierra para aprender de la vida. Se dice también que fueron las cuatro razas quienes migraron: los blancos al norte, custodios de fuego; los rojos al este, custodios de la tierra; los negros al oeste, custodios del agua y los amarillos al sur, custodios del aire. La migración o diáspora es simbolizada por la esvástica dextrógira (gira en el sentido de las agujas del reloj). Por el contrario, la reunificación de las razas es simbolizada por la rotación levógira (en contra).

En línea con esto, durante un breve tiempo, y en homenaje a estas tribus indígenas del suroeste norteamericano, la División 45º de Infantería adoptó una esvástica en su estandarte. Con el auge del nazismo, en 1934, se dejó de usar.

Igualmente, poco después de estallar la Segunda Guerra Mundial, varias de estas naciones indígenas norteamericanas, como los navajos, apaches, tohono o'odham y hopi, difundieron un decreto conjunto proclamando que no volvieran a usar las esvásticas en su artesanía. La razón se debió al uso de la esvástica con un significado maligno durante la contienda. El decreto lo firmaron representantes de estas tribus. He aquí el texto del decreto:

Al igual que en el Reino Unido, la esvástica fue símbolo de los Scouts estadounidenses, y dio nombre a la revista de la sección femenina del movimiento escultista. Pero además este símbolo de la buena suerte lució en aviones norteamericanos de la Primera Guerra Mundial, y fue imagen de publicidad de marcas como Coca Cola o Carlsberg, o de la marca de naranjas californianas Swastika.

La esvástica forma parte de los motivos ornamentales tradicionales de algunas nacionalidades indígenas del Panamá actual, como por ejemplo los Guna Yala. Por ejemplo en el archipiélago de San Blas, territorio de la nacionalidad Guna, se puede observar una bandera color naranja con una cruz esvástica en el centro de color negro en sentido antihorario, figurando en el escudo de la extinta bandera de la República de Tule.

La cultura mochica o moche (entre los siglos II a. C. y VII d. C.), que se desarrolló en gran parte del norte de Perú, es conocida como «vencedor del desierto». Estos antiguos pobladores fueron hábiles constructores de pirámides y conductos hidráulicos, además de conocedores de la metalurgia y grandes ceramistas. En el museo de sitio Huaca Rajada de Sipán, ubicado en el distrito de Zaña, en el departamento costero de Lambayeque, 900 kilómetros al norte de Lima, se exhiben una serie de cerámicas, entre los que destaca una vasija con la figura de una esvástica o cruz gamada (una cruz con los brazos doblados).

Fue encontrada por los arqueólogos cerca de la base de la pirámide mayor. La esvástica está dibujada con pintura de carbón en la cerámica. Para la cultura andina representó un «elemento geométrico del movimiento permanente de los ciclos de la vida», según Walter Alva, descubridor del Señor de Sipán (un antiguo monarca peruano).
«La esvástica que identificó al nazismo de Adolf Hitler en Alemania fue mal usada en Europa, pero para la cultura andina es un símbolo del dios del viento y el agua. No tiene ninguna connotación política», añadió Alva. Para Luis Chero ―director del museo Huaca Rajada―, la esvástica estuvo relacionada con el vuelo de las aves. «Es un paralelismo cultural, pero eso no significa que haya habido algún contacto con culturas de otras partes del mundo».

Para muchos occidentales, la esvástica se asocia primordialmente con el nazismo en particular.

Los nazis adoptaron la esvástica en 1920 pero ya estaba en pleno uso como símbolo entre los movimientos nacionalistas alemanes "völkisch", los cuales poseían ciertas veleidades místico-esotéricas. Por este motivo, lo vieron apropiado para adoptarlo como símbolo de la «raza aria». El uso de la esvástica como símbolo de la «raza aria» se remonta de nuevo a escritos de Émile-Louis_Burnouf. Tras muchos otros escritores, el poeta nacionalista alemán Guido von List lo consideró para ser un símbolo únicamente ario. Adolf Hitler se refirió a la esvástica como el símbolo de la «lucha por la victoria del hombre ario» (en el libro "Mein Kampf").

Algunos autores sostienen que lo que inspiró a Hitler para usar la esvástica como símbolo del NSDAP fue su uso por la Sociedad Thule "(Gr. Thule-Gesellschaft)" ya que había conexiones entre ellos y el Partido Obrero Alemán (DAP). Desde 1919 hasta el verano de 1921 Hitler utilizó la biblioteca especial del Dr. Friedich Krohn, un activo miembro de la Sociedad Thule. El Dr. Krohn fue además el dentista de Sternberg, quien fue nombrado en "Mein Kampf" por Hitler como el diseñador de una bandera muy similar a una que Hitler diseñó en 1920. Durante el verano de 1920, la primera bandera del partido ―hecha de forma casera― fue mostrada en el lago de Tegernsee.

Wolfgang G. Jilek dice:

Los teóricos nazis asociaron el uso de la esvástica con sus tesis que afirmaban la ascendencia cultural del pueblo alemán de la llamada raza aria. Los nazis creían que los primeros arios de la India, de cuyas tradiciones védicas surge la esvástica, fueron el prototipo de invasores de raza blanca, cooptando así el símbolo como un emblema de la supremacía blanca.

Los nazis utilizaron la esvástica negra (o "Hakenkreuz") dentro de un círculo blanco sobre fondo rojo, siendo el negro, el blanco y el rojo los colores de la antigua bandera del Imperio alemán. Los nazis también usaron la esvástica desprovista de tales círculos y fondo. Adolf Hitler escribió también en su libro que el diseño final le fue sugerido por un gran número de correligionarios nazis.

Se encuentran con frecuencia dos versiones de la esvástica nazi. Una de ellas es levógira. La otra es su imagen especular dextrógira. Aunque los nazis no parecen haber atribuido distinciones simbólicas a ambas variedades, la última es de uso más común. En ambas la cruz aparece girada 45°.

En nuestros días, el simbolismo de la esvástica ha sido adoptado por los neonazis. Consecuentemente, el empleo de la esvástica fuera de un contexto histórico se considera tabú en casi todo el mundo. En la actualidad, la legislación alemana prohíbe y sanciona el uso en público de la cruz gamada y otros símbolos nazis.

Para centenares de millones de personas, la esvástica se asocia con conceptos y prácticas que nada tienen que ver con el nazismo y por eso es de uso corriente principalmente en países no occidentales.

Una esvástica es básicamente una cruz que está en movimiento giratorio. La importancia de esta cruz es que se encuentra en movimiento y, por tanto, el sentido de giro es vital para interpretar correctamente el significado del símbolo.

Según algunos, que un brazo apunte a la derecha no significa que gire a la derecha, sino todo lo contrario.

Las dos descripciones más comunes son:

Estos términos pueden ser utilizados de manera incoherente (a veces incluso por el mismo autor), lo cual puede confundir un punto importante, ya que la rotación de la esvástica puede tener una relevancia simbólica, aunque ―debido a la incoherencia entre la información que aportan los distintos autores― se sabe poco acerca de esta relevancia simbólica.

En 1894, el abogado y profesor belga Eugène Goblet d’Alviella sostuvo (aunque sin aportar pruebas), que la esvástica levógira se llama sauvástika. lo cual desde el punto de vista del idioma sánscrito es un adefesio.
Un mito occidental sostiene que solo la esvástica con brazos doblados hacia la derecha es una marca de buena suerte, mientras que la esvástica de brazos doblados hacia la izquierda representa un augurio nefasto.
No existen pruebas de esta distinción en la historia del hinduismo (de la que proviene el símbolo).
Los hinduistas de la India y de Nepal siguen usando el símbolo en sus dos variantes, aunque la versión más corriente es la dextrógira.
Los budistas casi siempre emplean la forma levógira.
A principios del siglo XX el nazismo adoptó la cruz esvástica como emblema y ―a raíz de la Segunda Guerra Mundial― en Occidente se identifica mayoritariamente como un símbolo exclusivamente del Tercer Reich, desconociéndose prácticamente su uso previo al nazismo.

La frecuencia con que es usada la esvástica se explica por el hecho de que es un símbolo sencillo y atractivo que puede aparecer sin dificultad en cualquier civilización que haya desarrollado la cestería (aunque no necesariamente, puede aparecer de otros modos), y de ahí expandirse con facilidad, debido a los contactos entre unos pueblos y otros. La esvástica sería un diseño muy repetido, creado por los bordes de las cañas o juncos usados para fabricar una cesta de base cuadrangular. Otras teorías intentan demostrar que se produjo una transmisión del símbolo de unas culturas a otras. También existe quien se ha servido de las teorías de Carl Jung, más concretamente del inconsciente colectivo, para explicar la presencia de la esvástica en lugares tan distantes.

Mientras que la existencia de la esvástica en América puede ser explicada por la teoría de la cestería, este hecho debilita enormemente a la teoría de la difusión cultural. Aunque ha habido quien ha tratado de explicar esto mediante la suposición de que fue transmitida por alguna temprana civilización marina de Eurasia, el desarrollo separado pero paralelo del símbolo es la explicación más aceptada por los antropólogos.

El nacimiento de la esvástica se ha considerado a menudo relacionado con el de otros símbolos con forma de cruz, así como con la «cruz solar» de las religiones primitivas de la Edad del Bronce.

Aparte de las hipótesis antropológicas clásicas existe también una hipótesis astronómica del origen de la esvástica. Esta fue formulada por el astrónomo estadounidense Carl Sagan. Según Sagan, es inexplicable que este símbolo haya sido usado a lo largo de la historia por muchas civilizaciones distantes entre sí y que no habrían tendido ningún vínculo de unión, a menos que se considere la posibilidad de que se trate de un símbolo resultante de una experiencia común que tuvieron todos estos pueblos. Para Sagan, esta experiencia únicamente podría provenir del cielo: podría ser la visión de algún astro peculiar o bien la visión de alguna anomalía atmosférica.

Sagan opinaba que el origen probable de la esvástica era el acercamiento de un cometa con movimiento de rotación de tal manera que su eje quedara orientado hacia el observador terrestre.
Varios científicos han señalado al cometa Encke como el más probable candidato para ello.

Sin embargo, los críticos de esta hipótesis señalan que es altamente improbable que tal suceso se produjera, y creen que se puede explicar la utilización de la esvástica desde un punto de vista meramente terrestre.




</doc>
<doc id="17224" url="https://es.wikipedia.org/wiki?curid=17224" title="Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers">
Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers

La Encyclopédie, ou Dictionnaire raisonné des sciences, des arts et des métiers (Enciclopedia, o Diccionario razonado de las ciencias, las artes y los oficios) es una enciclopedia editada entre los años 1751 y 1772 en Francia bajo la dirección de Denis Diderot y Jean le Rond d'Alembert. Se la conoce coloquialmente como la "Enciclopedia de Diderot y d'Alembert". Su propósito fue reunir y difundir en un texto claro y accesible, los frutos del conocimiento y del saber acumulados hasta entonces a la luz de la razón. De esta manera, expone la ideología laicista, pragmática, materialista y burguesa de la Ilustración. Contiene 72.000 artículos de más de 140 colaboradores, entre ellos Voltaire, Rousseau, el barón de Holbach o Turgot, por citar algunos.

Se la considera una de las más grandes obras del siglo XVIII, no solo por ser la primera enciclopedia francesa, sino también por contener la síntesis de los principales conocimientos de la época, en un esfuerzo editorial considerable para su tiempo.

Por el saber que contiene, el esfuerzo que representa y por las intenciones que sus autores le asignaron, se convirtió en un símbolo de la plasmación de la Ilustración, un arma política y en el objeto de numerosos enfrentamientos entre los editores, los redactores y representantes de los poderes establecidos, tanto secular como eclesiástico.
La Enciclopedia o diccionario razonado de las ciencias, artes y oficios, constituyó una obra central donde los filosófos ilustrados intentaron compilar los conocimientos de la época.

La idea de publicar una enciclopedia en francés nació de la influencia y del éxito editorial en Inglaterra de la "Cyclopaedia" ("Diccionario Universal de las Ciencias y las Artes") (1728) de Ephraim Chambers, así como de la publicación del "Diccionario de Trévoux" (1704-1771) compuesto por los jesuitas. Aunque también es de destacar la existencia de una amplia demanda de los lectores de este tipo de obras. Asimismo, tomaron como base los trabajos de Pierre Bayle con su publicación del "Dictionnaire historique et critique" en 1697, y de John Harris, con su "Lexicon technologicum" o "Universal Dictionary of the Arts and Sciences" en 1704 como libros de consulta anteriores y prefiguraciones de la enciclopedia. 

El editor francés André Le Breton, librero de éxito y especialista en la traducción de obras inglesas, obtuvo en 1745 una licencia para efectuar una traducción al francés de la Cyclopaedia de Ephraim Chambers. Para dirigir el proyecto, Le Breton eligió en principio a John Mills, un inglés residente en Francia, y al abate Jean-Paul de Gua de Malves, pero por diferentes razones, abandonaron el proyecto. En 1747 Le Breton encomendó a Diderot y D’Alembert la elaboración editorial de la "Encyclopédie".

La incorporación de Diderot y D’Alembert transformó por completo el proyecto, pasando de ser una simple traducción a la creación de una obra de mayor magnitud, de sesgo laico, pragmático, burgués y crítico, destinada a la divulgación de los conocimientos de la época, con trabajos inéditos y numerosas ilustraciones.

Para la ordenación temática y estructural de la enciclopedia, Diderot y D’Alembert se inspiraron en "el árbol de los conocimientos humanos" de Francis Bacon, expuesta en su obra "Novum organum" (1620) y también tuvieron en cuenta a Descartes y su "Discurso del Método" (1637). Los términos identificados en el árbol de conocimientos se ordenan por orden alfabético, de ahí el nombre de diccionario. Además, se añaden reenvíos en cada entrada hacia los términos citados o relacionados.

Entre 1747 y 1750 se llevó a cabo la preparación de la obra. En 1750 se publicó el denominado "Prospecto de la Enciclopedia", redactado por Diderot, que ya suscitó polémica con los jesuitas y por fin en 1751 se presentó el primer volumen. Esta publicación provocó una fuerte oposición en algunos sectores de la sociedad francesa y la obra continuó en medio de grandes polémicas. Uno de estos conflictos fue el denominado "Affaire Prades". El abate Jean-Martin de Prades presentó su tesis doctoral en La Sorbona en 1752, una referencia directa al "Discurso Preliminar" de Diderot. Esto llevó a una serie de problemas tanto para el autor como para el proyecto. En primera instancia se denunció un ateísmo por parte del clérigo, como consecuencia del proyecto editorial. Diderot fue apresado y llevado a la prisión de Vincennes donde pasó cerca de cuatro meses, mientras que Prades se exilió en Berlín. Esto provocaría que el proyecto fuese sancionado en sus dos primeros volúmenes por considerarlos elementos de sedición. Este incidente fue aprovechado por polemistas como Charles Palissot y Jacob-Nicolas Moreau —contrarios a las ideas ilustradas— para generar discrepancia y desprestigiar el proyecto. 

El enorme revuelo causado en el Antiguo Régimen fue debido principalmente al tono de tolerancia religiosa de la obra, ya que la "Encyclopédie" incluía a pensadores protestantes, clasificaba la religión como una rama de la filosofía y no como el último recurso del conocimiento y de la moral. 

Desde el comienzo de su publicación, se formaron dos bandos claramente definidos en la sociedad francesa. De un lado, el poder religioso con los jesuitas al frente y con el apoyo del Delfín del Rey y sus allegados y también una parte de la intelectualidad, envidiosa del éxito alcanzado por los enciclopedistas. En el otro lado, se encontraban parte de la corte, con Madame de Pompadour —amante del rey— a la cabeza, el director de la Biblioteca Nacional, Guillaume Malesherbes y una buena parte de los mejores escritores de la época.

La obra fue puesta por la Iglesia católica en el "Índice de libros prohibidos" en 1759, cuando se habían publicado los primero siete volúmenes. En ese mismo año se les retiraron a los impresores los permisos del Estado para seguir publicándola y D'Alembert decidió abandonar el proyecto. Ya sin D'Alembert, pero consagrando el pacto propugnado por él, que consideraba la "Encyclopédie" un proyecto de interés nacional, esta pudo proseguir de forma tácita y muy vigilada hasta completar los diecisiete volúmenes de la obra en 1772.

Debido a los problemas con la censura de la "Encyclopédie" y de otros escritos independientes de sus colaboradores, se produjo un conflicto entre Diderot y D’Alembert, por un lado, y Le Breton, por el otro.

En 1775 a la conclusión de la empresa de Le Breton y Diderot, Charles-Joseph Panckoucke, editor y empresario de éxito, tomó como propio el espíritu enciclopedista y durante los siguientes veinte años reeditó la Enciclopedia en folio, le añadió índices, publicó suplementos y editó en tamaño cuarto. Con su edición en cuarto, la obra lograría una difusión masiva. 

De 1782 a 1832, Panckoucke y sus sucesores publicaron una versión ampliada de la obra con 166 volúmenes denominada la "Encyclopédie méthodique", ordenada por materias y no alfabéticamente. Este enorme trabajo para la época, ocupó un millar de empleados y 2250 colaboradores.

Para la elaboración de los primeros volúmenes se contó con la participación de Le Breton, así como de diversos socios: Antoine-Claude Briasson, Michel-Antoine David y Laurent Durand. Así mismo, también se contó con apoyo de adeptos al proyecto. Por ejemplo Madame D’Epinay, esposa de un arrendador de impuestos o Helvetius, quien tuvo una participación no solo como patrocinador financiero sino como uno de los miembros involucrados en el movimiento enciclopedista. 

Se había calculado como tirada de la primera edición una cifra de 1625 ejemplares, pero las suscripciones sobrepasaron las copias previstas, y tuvo una demanda por más de 4000 ejemplares, casi el triple de lo planeado. 

El primer volumen se publicó en 1751. El tomo 2 un año más tarde. Aproximadamente cada año se fue publicando un volumen adicional, hasta llegar a 7 en 1757. En 1762 aparecieron los dos primeros volúmenes de grabados, pero sin publicar volúmenes de texto desde 1757. El formato de los primeros 7 volúmenes era en un texto en formato de folio y 2 de láminas, con un costo aproximado de 280 libras por la suscripción. En 1769, Briasson y Diderot tuvieron que enfrentar un proceso judicial iniciado por un suscriptor descontento, Pierre-Joseph Luneau de Boisjermain, quien argumentó que el precio de la obra había excedido con creces lo anunciado por el prospecto de 1750. En efecto, el proyecto inicial era de solo 10 volúmenes, pero llegaron a ser 26 gracias al entusiasmo editor de los enciclopedistas. A consecuencia de ello, los libreros tuvieron que aumentar el precio a 850 libras en lugar de las 280 libras del precio de suscripción original. El largo proceso se extiende hasta 1778 con un fallo del juez a favor de los libreros, tres años después de la muerte de Briasson. 

Las imprentas que realizaron las ediciones de la "Encyclopédie" se encontraban en París; posteriormente pasaron a Ginebra, Lucca y Leghorn. En todas estas ciudades se imprimieron en formato de folio, mientras que en Neuchâtel se comenzaron con la elaboración en formato de cuarto; y en Lausana y Berna, la producción de hizo en formato de octavo. 

Esto generó una competencia entre las ciudades para obtener mano de obra especializada, por ello se dio una serie de deserciones y reclutamientos, hechos que quedaron registrados en la Société typographique de París y Neuchâtel, ciudades donde se registró la mayor demanda de trabajadores.

El trabajo original consta de 28 volúmenes, con 71 818 artículos y 3129 ilustraciones. Los primeros diecisiete volúmenes fueron publicados entre 1751 y 1765 y los once volúmenes de láminas se terminaron en 1772. Debido a su contenido radical para la época, la Enciclopedia causó mucha controversia en los círculos conservadores, y por iniciativa del Parlamento de Paris, el Gobierno francés suspendió los privilegios de la "Enciclopedia" en 1759. A pesar de la suspensión, el trabajo continuó "en secreto", con el apoyo de algunos miembros de la clase alta como Malesherbes y Madame de Pompadour. Las autoridades cerraron los ojos ante la continuidad del trabajo, pensando que su prohibición oficial era suficiente para apaciguar a la iglesia y a otros enemigos del proyecto.

Durante el denominado periodo "secreto", Diderot llevó a cabo una buena obra de subterfugios en la publicación. Así los volúmenes 1 al 7, publicados entre 1751 y 1757, antes de la prohibición, establecen como lugar de publicación París, las ediciones de los volúmenes 8 al 17, que fueron publicados en 1765, mostraban como lugar de publicación la localidad suiza Neuchâtel, donde la edición oficial de la Enciclopedia estaba a salvo de las interferencias de los agentes del estado francés. En particular, los opositores a la Enciclopedia no podían apoderarse en París de las placas de impresión de la Encyclopedia porque esas existían solo en Suiza. Mientras tanto, la producción real de los volúmenes de 8 a 17 fue trasladada a la periferia francesa, especialmente a Suiza.

Leyenda:
Los enlaces internos aquí llevan a la versión digitalizada en Wikisource, mientras que los enlaces externos llevan a la versión digitalizada en Gallica. Existen también varias versiones digitalizadas de la Encyclopédie 

En 1775, Charles Joseph Panckoucke obtuvo los derechos para reeditar la obra. Este editor publicó un total de siete volúmenes de materiales complementarios: en 1776 dos volúmenes de texto, en 1777 otros dos de texto y un tercero de ilustraciones y finalmente un índice completo de la obra que abarcó otros dos volúmenes y que fue elaborado por Pierre Mouchon ("Table analytique et raisonnée de l’Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers"). Jean-Baptiste-René Robinet fue el editor de estos "Suppléments".

Algunos estudiosos incluyen estos siete volúmenes "extra" como parte de la primera edición completa de la "Enciclopedia", para hacer un total de 35 volúmenes, a pesar de que no fueron escritos o editados por los autores originales. 

Entre 1782 y 1832, Panckoucke y sus sucesores publicaron una edición expandida del trabajo que llegó a tener 206 volúmenes organizados por materias, la "Encyclopédie méthodique". Dicho trabajo, enorme para su tiempo, ocupó mil trabajadores en producción y 2250 contribuidores.

A esta producción se le ha vinculado con una versión apócrifa, Agasse, que se terminó de publicar en 1832.

Debido al alto precio que costaba cada volumen, su recepción se vio limitada a la aristocracia, la burguesía, el ejército y el clero. Pero eso no impidió que el resto de la población consultara la obra; los formatos en cuartos y octavos permitieron la existencia de un ejemplar en la mayoría de las bibliotecas francesas, principalmente aquellas denominadas "cabinet littéraire," clubes de lectura donde los usuarios podían consultar los materiales por una libra y media al mes.

Wieder afirmó que la "Encyclopédie" proclamaba "un nuevo concepto de la vida, fundado en la naturaleza y la razón, bajo el signo de la libertad política, de la tolerancia religiosa y de la liberación de las trabas de la metafísica". Sería así, como anunciaba su Prospecto, "un cuadro general de los esfuerzos del espíritu humano en todos los órdenes y durante los siglos".

En ese "siglo de las luces", la evolución del pensamiento está ligada con la de las costumbres. Los relatos de viajes incitan a la comparación entre diferentes civilizaciones: la moral y las costumbres aparecen ligadas a un lugar y a un tiempo. Los burgueses llaman a la puerta de la nobleza, se convierten en la nobleza de vestimenta en oposición a la nobleza de la espada. Pero se oponen la lógica del determinismo hereditario y la del libre albedrío. 

Se imponen nuevos valores: "la naturaleza", que determina la actividad del hombre, "la felicidad terrena", que se convierte en un objetivo, "el progreso", mediante el cual cada época se esfuerza en alcanzar mejor la felicidad colectiva. El nuevo espíritu filosófico que se constituye, se basa en el amor por la ciencia, la tolerancia y la felicidad material. Se opone a restricciones como las de la monarquía absoluta o la religión. Lo esencial es ser útil a la colectividad difundiendo un pensamiento crítico, donde la aplicación concreta se impone sobre la teoría, y la actualidad sobre lo eterno.

El ateísmo, que comienza oficialmente en esta sociedad, es denunciado, e incluso castigado con la muerte.

Los métodos experimentales, aplicados a cuestiones filosóficas, llevan al "empirismo", según el cual todo conocimiento deriva directa o indirectamente de la experiencia obtenida a través de los sentidos. 

Además, el espíritu científico se expresa por su carácter enciclopédico. El siglo XVIII no se especializa, toca todos los campos: ciencia, filosofía, las artes, política, religión, etc. Se explica la producción de diccionarios y de compilaciones literarias que caracterizan este siglo y de los cuales la "Encyclopédie" es la obra más representativa.

Se pueden citar como obra relacionadas en cierta manera a "El espíritu de las leyes" de Montesquieu (31 libros), la "Histoire naturelle" de Georges Louis Leclerc, conde de Buffon (36 volúmenes), el "Ensayo sobre los orígenes de los conocimientos humanos" de Condillac, el "Diccionario filosófico" de Voltaire (614 artículos).

Bernard le Bouyer de Fontenelle, en "Entretiens sur la pluralité des mondes" (1686), ya divulgaba esta manera de pensar basada en los hechos, la experiencia y la curiosidad por las innovaciones.

En cuanto al espíritu crítico, se ejerce principalmente contra las instituciones. Frente a la monarquía absoluta, se prefiere el modelo inglés (monarquía constitucional). La crítica histórica de los textos sagrados ataca la certeza de la fe, el poder del clero y las religiones reveladas. Los filósofos se orientan hacia el "deísmo" que admite la existencia de un dios sin iglesia. Critican asimismo la persecución de los hugonotes por la monarquía francesa.

La consecuencia positiva de esta crítica es el espíritu de reforma. Los enciclopedistas toman partido por el desarrollo de la instrucción, la utilidad de las letras, la lucha contra la Inquisición y el esclavismo, la valorización de las artes «mecánicas», la igualdad y el derecho natural, el desarrollo económico que aparece como fuente de riqueza y de confort.

Para defender sus ideas, los autores han oscilado entre el tono polémico (véase el artículo "Prêtres" de Paul Henri Dietrich, Barón de Holbach) y las técnicas de autocensura que consistían en apoyarse en ejemplos históricos precisos. El examen científico de las fuentes les permitía poner en cuestión las ideas legadas del pasado. La abundancia de anotaciones históricas desalentaba a una censura a la búsqueda de ideas subversivas. Algunos enciclopedistas han preferido hacer pasar visiones iconoclastas por artículos aparentemente anodinos. Así, el artículo sobre la "capucha" es la ocasión para ridiculizar a los monjes.

Incluso si la cantidad a veces ha perjudicado a la calidad, hay que subrayar la singularidad de esta aventura colectiva que fue la "Encyclopédie": por primera vez, se describen en igualdad con los saberes nobles todos los otros conocimientos: la panadería, la cuchillería, la calderería, la marroquinería. Esta importancia acordada a la experiencia humana es una de las claves del pensamiento del siglo: la "razón" se vuelve hacia el ser humano, que es a partir de entonces su fin.

Para una adecuada comprensión de los textos de la "Encyclopédie" hay que tener en cuenta que la "Encyclopédie" fue un producto básicamente de la burguesía, tanto en su origen y elaboración, ya que la mayoría de los escritores y filósofos participantes provenían de este grupo social. Los valores propuestos y que transmitían eran también los valores propios de la burguesía, como en sus destinatarios, puesto que la clientela preferida de la obra fueron los miembros de la burguesía.

El artículo «Refugiados» es un ejemplo perfecto al respecto, valora el trabajo, la riqueza y la industria, con lo que se opone a los valores tradicionales de la nobleza, que ponía por delante los «altos hechos de armas», lo que suponía un rechazo al ejercicio del comercio y la agricultura.

Aun cuando está claro que el "siglo de las luces" hizo grandes aportaciones a la civilización humana, no era gratuito. Voltaire tenía una de las más grandes fortunas del reino y fue uno de los primeros capitalistas franceses. Voltaire, por ejemplo, fue a menudo muy desdeñoso con el «pequeño pueblo», lo que se materializa en que su defensa de las minorías es la defensa de la minoría burguesa frente a la todopoderosa nobleza.

La "Encyclopédie" supuso un recurso pedagógico de primer nivel para instruir a la ciudadanía con independencia de la enseñanza tradicional que, hasta ese momento, se hallaba íntegramente en manos de la Iglesia. Esta obra constituyó el instrumento deliberado para la transmisión de una "nueva cultura" que rompía con la impuesta desde arriba por la religión y el poder establecidos. Los valores que se divulgan, ya mencionados, se sitúan dentro del ámbito de la cultura humanista.

En consecuencia con la ambición racionalista de los enciclopedistas, existe la voluntad de tratar a todos los seres humanos de la misma manera. Este universalismo se refleja en varios aspectos:

En definitiva, se trataba de luchar contra el «proteccionismo del saber»: la apropiación del conocimiento por parte de una élite o clase privilegiada amparada en diversas prohibiciones y un lenguaje esotérico. El objeto último de la empresa enciclopedista es terminar con la desigualdad de base que impide la crítica pública y permite la imposición de un seudosaber. Esta ampliación universal del conocimiento que se persigue facilitará, según sus autores, el cambio y el progreso tanto científico como social.

La obra comprende: 35 volúmenes, publicados de 1751 a 1780. Se subdividen en dos partes correspondientes a los etapas de edición dirigidos por LeBreton y Panckoucke respectivamente. 

En total, son 72 999 artículos, que llegaron a abarcar unos 20 millones de palabras, y 2885 ilustraciones. 

Asimismo, se elaboran compendios de artículos de la "Encyclopédie". Uno a cargo del abate jesuita Joseph de La Porte en seis volúmenes conteniendo artículos seleccionados por el autor fue el titulado "L'Esprit de l'Encyclopédie" en 1782. 

La "Encyclopédie" fue una obra colectiva no solo derivada del trabajo de Diderot y D'Alembert. La ambición totalizadora de estos representaba un esfuerzo de documentación y de síntesis que no podía ser llevado a cabo por solo unos cuantos individuos aislados. En ella colaboraron 160 personas de las más variadas ocupaciones, como literatos, científicos, artistas, magistrados, teólogos, nobles y artesanos, que fueron conocidos como "les encyclopédies". Diderot en el artículo "Enciclopedia" de la propia obra describe su colaboración de la siguiente forma: «ocupado cada cual de su parte y unidos solamente por el interés general del género humano y por un sentimiento de recíproca benevolencia». 






</doc>
<doc id="17225" url="https://es.wikipedia.org/wiki?curid=17225" title="Extrema derecha">
Extrema derecha

Extrema derecha, derecha radical o ultraderecha son términos utilizados en política para describir movimientos o partidos políticos que promueven y sostienen posiciones o discursos nacionalistas y ultraconservadores considerados radicales o extremistas. Estas posturas radicales corrientemente se vinculan con prácticas antidemocráticas.

Muchos partidos adquieren unas posturas en defensa exacerbada de la identidad nacional que no abogan por el mantenimiento de las instituciones y las libertades democráticas. Otros aceptan las normas democráticas y sus electores, en casos, no asocian a estos partidos posturas reaccionarias y antidemocráticas.

Esta ideología política se masificó a mediados del , tras la desaparición de movimientos radicales que se posicionaban a sí mismos en la llamada «tercera posición», como el fascismo y el nazismo, que implementaron prácticas totalitarias. En la actualidad, varios de los dirigentes de partidos de extrema derecha suelen reivindicar o tener algún tipo de relación con la ideología de tipo fascista.

Su origen ideológico reside en el pensamiento contrarrevolucionario conservador del francés Joseph de Maistre, quien, a partir de finales del siglo , reivindicaba la Edad Media como modelo, situando la ruptura en 1789, con una postura que se acerca más al involucionismo político.

Sin embargo, las expresiones «ultraderecha» o «extrema derecha» son relativamente modernas. Tienen su origen en el lugar donde se sentaban los diputados en el parlamento francés surgido tras la Revolución francesa: los monárquicos y los conservadores de la época se sentaban siempre en el lado derecho y los liberales en el izquierdo. La extrema derecha se contrapone así a la izquierda radical, y en cierto modo es un movimiento antagónico a las ideas revolucionarias de la izquierda. En ese contexto aparecieron movimientos contrarrevolucionarios de ultraderecha, especialmente en Francia e Italia.

En la primera mitad del siglo , el fascismo y nazismo protagonizaron trágicos episodios en Europa, pero acabaron siendo claramente derrotados. Las ideas que estos movimientos representaban han ido teniendo continuidad a lo largo del tiempo, como el Partido Nacionaldemócrata de Alemania (NPD), fundado en 1964 o Falange Española, partido sucesor del fundado en 1933 por José Antonio Primo de Rivera, este último con unas ideas basadas en el fascismo italiano de Benito Mussolini con ideas nacionalsindicalistas.

Se consideran partidos de extrema derecha a aquellos partidos cuyo ideario se haya vinculado ideológicamente con el fascismo a través de referencias a sus mitos y símbolos, además del seguimiento del programa fascista. También desarrollan una activa labor de deslegitimación de la democracia mediante una oposición antisistema., aunque también se incluyen los grupos neonazis, cuya inspiración es la ideología nazi (contracción de la palabra alemana "Nationalsozialistische", que significa «nacional-socialista»).

En la actualidad hay un cierto resurgimiento de estos movimientos, exacerbado por la actual crisis económica y la creciente inseguridad que los ciudadanos ven en su futuro.

En Europa ha aparecido una ultraderecha con un gran sentimiento euroescéptico, antiglobalización, y que lucha contra la inmigración de una forma nacionalista y en ocasiones, xenófoba y racista. Igualmente tiende a tener una ideología conservadora, en sus vertientes nacionalista, liberal o social.

En Europa, tienen una fuerte presencia en países como Países Bajos, Austria e Italia, donde son muy influyentes, y también en Francia, Reino Unido, Suecia, Finlandia, Bélgica o Alemania. En Grecia, Amanecer Dorado obtuvo 17 escaños. Sin embargo, en Europa existe preocupación por el recuerdo de los episodios de la primera mitad del siglo .

La expresión «extrema derecha» ha sido utilizada por diferentes estudiosos de manera un tanto contradictoria debido a las diferentes configuraciones ideológicas, al no existir un consenso sobre una ideología concreta que defina a todos los grupos enmarcados en la extrema derecha, especialmente si tenemos en cuenta las variaciones ideológicas sufridas a lo largo del tiempo. Así, en opinión del profesor mexicano Rodríguez Araujo, el término "derecha" «es también un concepto que ha variado según las tradiciones y el tipo de sociedad y de poder que se han defendido a lo largo de la historia. Muchas de las posiciones políticas que ahora consideramos de derecha fueron de izquierda en otro momento».

En consecuencia, podemos afirmar que no todos los grupos de extrema derecha comparten los mismos ideales, pero la mayoría tiene una visión del mundo conspirativa y ultranacionalista, que les permite recoger el voto de protesta contra las imperfecciones de la democracia parlamentaria representativa. Tienen en común al menos alguna de las siguientes características:







Un estudio de la extrema derecha publicado por "Washington Post" señala que los partidos ultraderechistas a menudo tienen diferencias importantes entre sí. En el caso de los europeos se señalan tres tipos de partidos principales:


Las razones de estas diferencias son históricas y geográficas. Por un lado, muchos países de Europa Oriental (como Rumania, Hungría, Croacia, Alemania Oriental) pasaron tanto por dictaduras fascistas autóctonas como por regímenes comunistas donde se daba a menudo persecuciones antijudías. En estos países además de tener partidos que pueden derivar directamente de las agrupaciones tradicionales históricas del fascismo, el discurso «clásico» racial, nacionalista e irredentista no escandaliza tanto a la sociedad, y debido a que la cantidad de inmigrantes musulmanes es relativamente menor se requiere un nuevo «chivo expiatorio» (minorías como gitanos y judíos). Por el contrario en Europa Occidental, la mayoría de países no han tenido dictaduras fascistas y su principal contacto con el fascismo se dio durante la ocupación nazi, por lo que en muchos casos se enorgullecen de las luchas antinazis del pasado. Por ello, salvo excepciones, los partidos de ultraderecha de estas naciones dejan de lado ataques contra las pequeñas comunidades judías y se enfocan en el discurso anti-islámico siendo la inmigración musulmana un problema mayor para muchos sectores de la ciudadanía e incluso comparan al islam con el nazismo, el Corán con "Mein Kampf", etc., y el discurso racista se deja de lado (al punto de que personas de etnia negra o judía pueden pertenecer a estos partidos). 

En la siguiente tabla se incluye la evolución electoral de diversos partidos considerados de ultraderecha en Europa, se incluyen todos aquellos partidos de marcado carácter anti-inmigración, populistas y nacionalistas, aunque algunos de estos partidos son de ideologías radicalmente contrarias en determinados puntos, como el apoyo a Israel del PVV, y el antisemitismo de Amanecer Dorado, o el liberalismo económico de PxC y el socialismo de MSR en España.


</doc>
<doc id="17227" url="https://es.wikipedia.org/wiki?curid=17227" title="Dilema de Tošovský">
Dilema de Tošovský

El dilema de Tošovský es un proceso económico identificado en la década de 1990 por el director del Banco Nacional checo Josef Tošovský. Desde entonces se lo conoce con su nombre.

El dilema de Tošovský puede surgir por la combinación de dos hechos relevantemente empíricos y sin validez oficial:

El choque entre el bajo punto de equilibrio de las tasa de interés derivado del hecho 1 y el alto punto de equilibrio de la tasa de interés derivado del hecho 2 conforma los argumentos del dilema de Tošovský.

NOTA: Informe basado en "The Tošovský Dilemma, capital surges in transition countries" escrito por Leslie Lipschitz, Timothy Lane y Alex Mourmouras. "Finance and Development", número 3, vol. 39, septiembre de 2002.



</doc>
<doc id="17228" url="https://es.wikipedia.org/wiki?curid=17228" title="Inversión extranjera directa">
Inversión extranjera directa

La inversión extranjera directa, en la socialización es la colocación de capitales a largo plazo en alguna parte del mundo, para la creación de empresas agrícolas, industriales y de servicios, con el propósito de internacionalizarse. En inglés se habla de "Foreign Direct Investment" o "FDI".

Existen diversas razones para que una empresa decida invertir en otro país. Casi todos los argumentos que se han ofrecido para la existencia de IED pueden agruparse bajo tres objetivos básicos: el intento de participar en nuevos mercados, aumentar la eficiencia productiva a través de reducciones de costos y el intento de explotación de ciertos activos estratégicos. A continuación explicaremos en más detalle cada uno de estos tres objetivos.

Una de las principales razones que se han ofrecido para las diversas formas de explicar la presencia de IED en una economía es la búsqueda de nuevos mercados.Tradicionalmente se suponía que una empresa que proveía a una economía grande y/o rica, podía asegurarse su participación en dicho mercado mediante una inversión directa. En ese sentido, la IED tradicionalmente se entendía como un sustituto directo del comercio. Por ende, un factor explicativo de este tipo de IED es el tamaño del mercado de destino, el cual puede medirse mediante el ingreso total de una economía o mediante
sus dos componentes: el tamaño de la población y el ingreso por capital. De hecho, una parte de la explicación en las décadas de los sesenta y setenta estaba basada en el fuerte proteccionismo que caracterizaba a algunas economías. Esto se debía a que en un mercado protegido era más atractivo el invertir en forma directa que utilizar la vía alternativa (exportar), la cual podría ser muy costosa. Además, una economía protegida ofrecía un atractivo mercado cautivo. Este tipo de IED se conoce en inglés con el nombre de tariff-jumping.

Un enfoque más moderno, sin embargo, sugiere que existe un cierto tipo de IED que busca un mercado más grande pero no en forma directa sino indirecta. En este sentido, una economía que ofrece ventajas comerciales o de localización geográfica, podría servir para atraer a la IED que busca penetrar en un mercado más amplio (por ejemplo, en un tercer país, lo cual puede ser el resultado del establecimiento de acuerdos comerciales). En este sentido, este tipo de IED podría estar asociado a un mayor volumen de comercio internacional y no a uno menor como se suponía anteriormente. 

Además, este tipo de IED sería más común en aquellos países que ofrecieran un cierto tipo de ventajas ya sea comerciales (acceso a terceros países mediante acuerdos comerciales) o de localización geográfica (vía menores costos). Un ejemplo de esto, es el caso de México que ofrece a terceros países la posibilidad de acceder al mercado norteamericano como resultado de su participación en el Tratado de Libre Comercio de América del Norte.

l es abundante y los salarios son relativamente bajos. Sin embargo, también es el caso de la IED que busca una mayor eficiencia por costo unitario de la fuerza laboral. Esto implica que hay cierto tipo de IED que no solo busca mano de obra barata sino una combinación de salarios inferiores a los domésticos y una alta productividad de los trabajadores.

Este tipo de IED puede dividirse en dos situaciones completamente extremas. Por una parte, se encuentra la IED que busca explotar la existencia de ciertos recursos naturales. Este es la forma más tradicional y antigua de IED. Sin embargo, en la actualidad este tipo de IED ha ido perdiendo importancia en el mundo, en parte porque la dotación de factores es relativamente fija pero también como resultado del surgimiento de muchos otros bienes que pueden sustituir a este tipo de recursos. 

Los factores que hemos mencionado anteriormente sugieren una serie de posibles determinantes de la IED: el tamaño del mercado, las características de la fuerza laboral, la ubicación geográfica, la dotación de factores, etc. Sin embargo, estos determinantes son válidos en un contexto puramente abstracto en donde los aspectos institucionales son irrelevantes o idénticos en todas las economías. Por supuesto, esto no es cierto y en la práctica existen una serie de factores institucionales y de política que sin duda desempeñan un papel importante en la determinación de los flujos de IED. Este otro tipo de factores pueden ser manejados mediante políticas gubernamentales y pueden convertirse en instrumentos de atracción o repulsión de la IED. Para propósitos de este estudio, distinguiremos entre dos tipos de factores: factores institucionales o estructurales y políticas de incentivos.




</doc>
<doc id="17237" url="https://es.wikipedia.org/wiki?curid=17237" title="Amaterasu">
Amaterasu

Amaterasu es considerada una diosa de carácter amable y compasiva con quienes la adoran. Esta actitud maternal viene además porque, según los preceptos del sintoísmo, es ancestro de todos los emperadores de Japón, por lo que podría decirse que es la madre del imperio y deidad suprema en el país.

Se cree que su espejo sagrado Yata no Kagami se guarda en en santuario Naiku, en Ise, como uno de los tesoros imperiales.

Según se cuenta en el "Kojiki", la Megami ("Kami" en femenino) nació del ojo izquierdo de Izanagi cuando se purificaba tras su intento fallido de rescatar a Izanami, y de similar manera nacieron sus hermanos Susanoo y Tsukuyomi. El "Kojiki" la describe como la Megami de la que emana toda la luz, y en numerosas ocasiones se hace alusión a ella como la Megami del sol por la calidez y la compasión por aquellos que la adoran.

La mayor parte de los mitos giran en torno a un incidente en el que la Megami se quedó encerrada en una cueva por culpa de las acciones de su hermano Susanō. Sumido en un fuerte estado de embriaguez, este arrasó los campos de arroz de Amaterasu, llenó todos sus canales de irrigación, y arrojó excrementos en su palacio y templos (en otra versión estas acciones se debieron a la furia del dios tras perder una competición destinada a subsanar su descontento con el reparto que su padre había hecho de cielo, noche y océanos entre los tres hermanos). La megami le rogó a su hermano que se detuviera, pero este la ignoró y llegó incluso a arrojar el cadáver del caballo "celestial" a sus doncellas, que se encontraban tejiendo. Las mujeres murieron a causa de las astillas de madera que, al romperse el telar, atravesaron sus cuerpos (la mayoría de las versiones dicen que son sus órganos reproductivos los que fueron alcanzados por la madera).

Furiosa, tras ver el cadáver del caballo, Amaterasu se encerró en la Cueva Celestial y la selló con una roca. Como resultado, el mundo quedó sumido en tinieblas, y comenzó a marchitarse y llenarse de malos espíritus. Los "Kami-gami" (plural de "Kami") se reunieron frente a la entrada buscando una manera de hacerla salir. El dios de la inteligencia, Omoikane, ideó la manera de hacerla salir, se sentaron todos en torno a la cueva y colocaron un espejo dirigido a la entrada. Ama no Uzume, la voluptuosa Megami de la danza, dio la vuelta a una bañera y se puso a bailar sobre ella, marcando el ritmo con sus pasos. Durante su danza, se levantaba la falda y mostraba los pechos. El resto de Kami-gami hacía mucho ruido gritando, riéndose y animando. Amaterasu decidió echar un vistazo a ver qué era lo que pasaba, y le preguntó al que estaba más cerca de la entrada. Este le contestó que había una nueva Megami. Cuando Amaterasu preguntó quien era, este señaló al espejo, y esta, que nunca había visto su reflejo, se quedó absorta en la imagen. Estaba tan sorprendida que exclamó "Omo-shiroi", que significa tanto "blanca tez" como "fascinante". Mientras estaba distraída, los otros Kami-gami cerraron la cueva tras ella, convenciéndola para regresar al Plano Celestial.
Amaterasu fue enviada de joven a conquistar las Altas Llanuras del Cielo, pero pronto tuvo que esconderse, ofendida ante el comportamiento de su hermano. Cuando dejó de esconderse, envió a su nieto Ninigi-no-mikoto a pacificar Japón y fue su bisnieto Jinmu quién se convirtió en el primer emperador. Esta hipotética fundación de la dinastía imperial japonesa fomentó la idea nacionalista e imperialista surgida durante la Restauración Meiji de 1868.

Amaterasu carece de iconografía. Pese a ello, se le relaciona con el "Divino Espejo" al que se refiere la leyenda de la cueva. De hecho, cuando mandó a su nieto a pacificar Japón, le dio una espada, Kusanagi, recibida como presente de su hermano Susanoo para volver al cielo, un espejo (este), y las joyas de la Familia Imperial, consistentes en la espada, Kusanagi-no-tsurugi (草薙劍), la joya o collar de joyas Yasakani no magatama (八尺瓊曲玉) y el espejo Yata no kagami (八咫鏡).

En Japón, la diosa Amaterasu es adorada como la deidad madre de la casa imperial y como la deidad suprema de la nación japonesa. El gran santuario de Ise se ha creado para Amaterasu. Si se aprecia el interior del gran santuario de Ise, cerca de la entrada se encuentran los caballos dedicados a la diosa Amaterasu. Estos caballos no son ordinarios, sino que son los caballos que la casa imperial japonesa dedicó a la diosa del Sol. Los caballos son vestidos y llevados a un lugar santo del santuario de tres veces al mes e inclinan la cabeza hacia Amaterasu.


</doc>
<doc id="17239" url="https://es.wikipedia.org/wiki?curid=17239" title="Susanoo">
Susanoo

, en el sintoísmo, es el dios del mar, de las tormentas y de las batallas. Es el hermano de Amaterasu, la diosa del Sol, y de Tsukuyomi, el dios de la Luna. Este dios se califica a veces de brutal y a veces de considerado. El Kojiki y el Nihonshoki tienen escrita su legendaria represión de un monstruo de serpiente llamada Yamata-no-Orochi, en el país de Izumo. Ambos libros lo describen como un antecesor del linaje imperial. En contraste, algunos folclores lo consideran como un dios nativo o cabeza de un pueblo de Izumo.

La mitología cuenta que este es uno de los tres dioses japoneses, que nació de la nariz de su padre, Izanagi, cuando este se dio un baño para purificarse después de ir a la tierra de los muertos, llamada Yomi, donde intentó rescatar a su amada Izanami.
Desde muy joven mostró una actitud fría y agresiva, pero con gran potencial. Su padre al repartir su reino le concedió el mar, la tierra y el rayo, pero este quería más.

Conociendo que la ira de su padre podría convertirse en un gran peligro, esperó a que Izanagi entrara en su sueño divino. Tras esto tuvo una batalla con su hermana Amaterasu, que dio como resultado grandes consecuencias, provocando que el consejo de los ochocientos dioses lo expulsasen del cielo directamente a la región de Izumo.

Cuando Susanoo quiso enfrentarse a su hermana Amaterasu, para evitar heridas innecesarias, decidió hacer con ella un concurso de poder creador: consistía en crear cuanto más divinidades menores mejor.
En la primera tanda, Amaterasu cogió la espada de su hermano y, tras romperla en tres fragmentos y masticarla, aparecieron tres hermosas diosas. Susanoo, para poder superarla, cogió las cuentas de la fertilidad de su hermana y, con ella, creó cinco dioses muy agresivos. Susanoo se proclamó vencedor, aunque su hermana, al pertenecerle a ella las cuentas con las que Susanoo había creado a sus dioses menores, dijo que ella era la vencedora. Susanoo se negó a aceptarlo y destruyó la hilandería sagrada, hogar de Amaterasu, y en adición a eso descuartizó y repartió por la hilandería el cuerpo del caballo “celestial”. Amaterasu se asustó tanto al ver al animal sagrado muerto, que huyó hasta la cueva de Yamato Iwato, donde se encerró, provocando la oscuridad eterna.

Rápidamente, Susanoo fue juzgado por el consejo de los ochocientos dioses, que se le culpó de asesinar al caballo “celestial”, asustar a su hermana (provocando así la oscuridad eterna) y de acabar con la vida de una de las doncellas de Amaterasu (ayudantes de la diosa en la hilandería sagrada), que murieron a causa de las astillas del telar cuando se rompió por el impacto con el caballo celestial.

Después de ser juzgado, el dios fue desterrado y enviado a la región de Izumo donde conoció a un hombre cuyas siete hijas habían sido asesinadas por el "Yamata-no-Orochi" una horrible serpiente de ocho cabezas y ocho colas; y solo la octava, Kushinada-hime, había sobrevivido.
El hombre le contó al dios que la bestia vendría pronto para llevarse a la última hija; Susanoo decidió ayudar al hombre e ideó un plan.
Pronto el dios trasformó a la hija en una peineta la cual colocó en su cabello.
El día que la bestia atacó, el dios había ya construido ocho puertas colosales, y tras ellas había colocado vastas cantidades de una bebida alcohólica (sake); la serpiente cayó en la trampa y bebió la bebida neutralizante; ya caída en el suelo, Susanoo, tomó su espada y cortó cada una de las colas y cabezas de la serpiente, en la cuarta cola encontró una espada de hermosa apariencia, Kusanagi. Tomando posesión de ella como presente para su hermana, el dios logró retornar a las mansiones divinas.


Aston, William George, tr. 1896. Nihongi: Chronicles of Japan from the Earliest Times to A.D. 697. 2 vols. Kegan Paul. 1972 Tuttle reprint.


</doc>
<doc id="17241" url="https://es.wikipedia.org/wiki?curid=17241" title="Mitología japonesa">
Mitología japonesa

La mitología japonesa es un sistema complejo de creencias. El panteón Shinto por sí solo se compone de una colección numerosa de "kami" ("dioses" o "espíritus" en japonés). A pesar de la influencia de la civilización china antigua, una parte muy importante de la religión y mitología japonesa son únicas. Contiene tradiciones Shinto y budistas así como creencias populares agrícolas. Por otra parte, a diferencia de la mitología griega, nórdica y egipcia, es relativamente difícil distinguir cuál es verdaderamente un "mito" para los japoneses.

Los mitos japoneses convencionales se basan en el Kojiki, en el Nihonshoki y algunos libros complementarios. El Kojiki que literalmente significa "registro de cosas antiguas" es el libro más viejo reconocido sobre mitos, leyendas, y la historia de Japón y el Nihonshoki es el segundo más antiguo. El Shintoshu explica orígenes de deidades japonesas desde una perspectiva budista mientras que el Hotsuma Tsutae registra una versión diferente sobre la mitología.

Un resultado notable de la mitología japonesa es que explica el origen de la familia imperial, y les representa como descendencia divina. La palabra japonesa para Emperador en Japón, "tennō" (天皇), significa el "Soberano celestial" (el carácter 天 significa "cielo").

Muchas deidades aparecen en los escenarios de la mitología japonesa, muchos de ellos tienen múltiples alias y además algunos de sus nombres son muy largos. Aquí se muestran los más prominentes, y en su forma abreviada.

En algunas partes de este artículo, los nombres propios están escritos de una manera histórica. "h", "y", y "w" subrayados denotan letras silenciosas; se omiten del deletreo moderno. Esta convención es peculiar a este artículo. Otras sílabas se modernizan como sigue (véase también los sistemas japoneses de romanización).

Debido a razones históricas, k, s, t, y h en algunas ocasiones se confunden con g, z, d, y b respectivamente.

Los primeros dioses convocaron dos criaturas divinas a la existencia, el macho Izanagi y la hembra Izanami, y les encargaron la creación de la primera tierra. Para ayudarles a realizar esto, se les dio a Izanagi e Izanami una lanza decorada con joyas, llamada Amenonuhoko (lanza de los cielos). Entonces, las dos deidades fueron al puente entre el Cielo y la Tierra, Amenoukihashi (puente flotante de los cielos) y agitaron el océano con la lanza. Cuando las gotas de agua salada cayeron de la punta de la lanza, formaron la isla "Inojoro" (auto-formada). 

Descendieron del puente de los cielos e hicieron su casa en la isla. Ya que deseaban unirse construyeron un pilar llamado "Amenomihashira" y alrededor de él levantaron un palacio llamado "Yahirodono" (la habitación cuya área es de 8 brazos). Izanagi e Izanami giraron alrededor del pilar en direcciones opuestas y cuando se encontraron, Izanami, la deidad femenina, habló primero con un saludo. Izanagi pensó que esta no era la manera apropiada, sin embargo se unieron de todos modos. Tuvieron dos hijos, Hiruko (infante del agua) y Awashima (isla de burbujas) pero fueron mal hechos y no se consideraron dioses.

Pusieron a los niños en un bote y los embarcaron al mar. Entonces les pidieron a los otros dioses una respuesta sobre lo que hicieron mal. Ellos respondieron que el dios masculino debió haber iniciado la conversación durante la "Ceremonia de Unión". Así que Izanagi e Izanami se dirigieron alrededor del pilar una vez más, y esta vez, cuando se encontraron, Izanagi habló primero y su matrimonio fue exitoso.

De esta unión nacieron Ōyashima (大八洲), o las ocho grandes islas de la cadena japonesa:


Nótese que Hokkaidō, Chishima, y Okinawa no fueron parte de Japón en los tiempos antiguos.

Crearon seis islas más y muchas deidades. Sin embargo, Izanami murió al dar a luz al infante Kagutsuchi (encarnación del fuego) o Ho-Masubi (causante del fuego). Fue enterrada en el “"Monte Hiba"”, en la frontera de las viejas provincias de Izumo y Hōki, cerca de Yasugi en la Prefectura de Shimane. Sumido en cólera, Izanagi mató a Kagutsuchi. Su muerte también creó docenas de deidades.

Los dioses nacidos de "Izanagi" e "Izanami" son simbólicos sobre aspectos importantes de la naturaleza y la cultura, pero ellos son muchos para ser mencionados aquí.

Izanagi se lamentó de la muerte de Izanami y emprendió un viaje a Yomi o "la tenebrosa tierra de los muertos". Izanagi encontró muy poca diferencia entre Yomi y el mundo terrenal, excepto por la oscuridad eterna. Sin embargo, esta oscuridad sofocante fue suficiente para provocarle dolor en ausencia de la luz y la vida en la tierra de arriba. Rápidamente buscó a Izanami y la encontró. En un principio Izanagi no pudo verla por completo debido a que las sombras ocultaban su apariencia. Sin embargo él le pidió a ella que regresara con él. Izanami le escupió, indicándole a Izanagi que ya era demasiado tarde. Ella ya había probado el alimento del inframundo y ahora ya era una con la tierra de los muertos. Ella no podría regresar más a la vida.

Izanagi se quedó impactado por estas noticias, pero aun así renunció a ceder antes los deseos de Izanami de quedarse en la oscuridad de Yomi. Izanami aceptó volver al mundo superior, pero antes le pidió a Izanagi que le dejara tiempo para dormir y que no entrara en su dormitorio. Mientras que Izanami dormía, él tomó el peine que sostenía su largo cabello y lo encendió como una antorcha. Bajo la repentina explosión de luz, él vio la horrible forma de la una vez hermosa y agraciada Izanami. Ahora ella era una forma de carne en descomposición con gusanos y criaturas asquerosas que se deslizaban sobre su cuerpo destrozado.

Gritando ruidosamente, Izanagi no tuvo control sobre su miedo y comenzó a correr, intentando volver a la vida y abandonando a su esposa muerta. Izanami se despertó llorando indignada y lo persiguió. Shikomes salvajes o las mujeres asquerosas también persiguieron al asustado Izanagi, guiadas por Izanami para atraparlo. Pensando rápidamente, Izanagi lanzó su gorro, el cual se convirtió en un racimo de uvas negras. Las shikome tropezaron con éstas pero continuaron su búsqueda. Después, Izanagi lanzó su peine, que se convirtió en un grupo de brotes de bambú. Ahora eran las criaturas de Yomi quienes comenzaron a perseguirlo, pero Izanagi orinó en un árbol, creando un gran río que aumentó su aplomo. Desafortunadamente, todavía persiguieron a Izanagi, forzándolo a lanzar melocotones sobre ellos. Él sabía que esto no los retrasaría por mucho tiempo, pero él ya estaba casi libre, porque los límites de Yomi ahora estaban más cerca.

Izanagi llegó rápidamente a la entrada y empujó un canto rodado en la boca de la caverna, la cual era la entrada a Yomi. Izanami gritó detrás de esta impenetrable barricada y le dijo a Izanagi que si él no la dejaba salir ella destruiría a 1.000 residentes vivos cada día. Él furiosamente le contestó que entonces el daría vida a 1.500.

Y de esta manera comenzó la existencia de la muerte, causada por las manos de la orgullosa Izanami, la esposa abandonada de Izanagi.

Izanagi se fue a purificar después de recuperarse de su descenso a Yomi. Mientras se desnudaba y se quitaba los adornos de su cuerpo, cada artículo que él dejaba caer al suelo formó una deidad. Incluso surgieron más dioses cuando él se sumergió en el agua para lavarse. Los más importantes fueron creados de su rostro una vez que éste se lo lavó:


Izanagi se dispuso a dividir el mundo entre ellos con Amaterasu heredando los cielos, Tsukuyomi tomando el control de la noche y la luna y el dios tormenta Susano'o poseyendo los mares.

Amaterasu, la poderosa diosa del sol de Japón, es la deidad más conocida de la mitología japonesa.

Sin embargo, su incontrolable hermano Susano'o, es igualmente infame y aparece en varios cuentos. Una historia dice del comportamiento imposible de Susano'o contra Izanagi. Izanagi, cansado de las quejas repetidas de Susano'o, lo desterró al Yomi. Susano'o a regañadientes lo consintió pero tenía asuntos pendientes que atender primero. Él fue a "Takamagahara" (cielo) a despedirse de su hermana, Amaterasu. Amaterasu conocía que su imprevisible hermano no tenía ninguna buena intención en mente y se preparaba para la batalla. "¿Con qué propósito has venido aquí?" preguntó Amaterasu. "Para decir adiós," contestó Susano'o.

Pero ella no creyó sus palabras y solicitó una competencia para probar su buena fe. El desafío fue fijado en cuanto a quién produciría el niño divino más noble. Amaterasu hizo a tres mujeres de la espada de Susano'o, mientras que Susano'o hizo a cinco hombres de la cadena de ornamento de Amaterasu. Amaterasu otorgó el título a los cinco hombres hechos de sus pertenencias. Por lo tanto, atribuyeron a las tres mujeres a Susano'o.

Es suficiente con decir, que ambos dioses se declararon vencedores. La insistencia de Amaterasu en su demanda condujo Susano'o a campañas violentas que alcanzaron su clímax cuando él lanzó un potro medio desollado -un animal sagrado para Amaterasu- en la sala donde Amaterasu tejía, causando la muerte de uno de sus asistentes. Amaterasu huyó y se ocultó en la cueva llamada el Iwayado. Mientras que la encarnación del sol desapareció en la cueva, la oscuridad cubrió el mundo.

Todos los dioses y diosas en turno, trataron de convencer a Amaterasu para que saliese de la cueva, pero ella los rechazó a todos. Finalmente, el kami de la festividad, Ame-no-Uzume, tramó un plan. Ella colocó un gran espejo de bronce en un árbol, frente a la cueva de Amaterasu. Luego Uzume se arropó en flores y hojas y volcó una tina de baño, y comenzó a bailar sobre ella, percusionando la tina con sus pies. Finalmente, Uzume se deshizo de las hojas y flores y bailó desnuda. Todos los dioses masculinos se hartaron de reír. Cuando ella se asomó después de su larga estancia en la oscuridad, un rayo de la luz llamado "amanecer" escapó y Amaterasu se deslumbró por su propio reflejo en el espejo. El dios Ame-no-Tajikarawo la sacó fuera de la cueva y ésta fue sellada con una cuerda sagrada [shirukume]. Rodeada por la festividad, la depresión de Amaterasu desapareció y ella accedió a devolver su luz al mundo. Desde entonces Uzume fue conocida como el kami del amanecer y también como el de la festividad.

Susano'o, que fue exiliado del cielo, llegó a la Provincia Izumo (ahora parte de la Prefectura de Shimane). Al llegar se encontró a un viejo y a su esposa sollozando al lado de su hija. La vieja pareja explicó que, originalmente, tuvieron ocho hijas quiénes fueron devoradas una a una, cada año, por el dragón llamado "Yamata-no-orochi" ("serpiente de ocho picos", de la cual se decía se originó de Kosi que es ahora la región Hokuriku). El terrible dragón tenía ocho cabezas y ocho colas. Kusinada o "Kushinada-Hime" (princesa del arroz blanco) era la última de las ocho hijas.

Susano'o, que se dio cuenta inmediatamente de la relación de la vieja pareja con la diosa del sol Amaterasu, ofreció su ayuda en pago de la mano de su hermosa hija. Los padres aceptaron y Susano'o transformó a Kushinada en un peine y la ocultó en su pelo. También ordenó construir una cerca alrededor de la casa, con ocho puertas abiertas en la cerca, ocho mesas colocadas en cada puerta, ocho barriles colocados en cada mesa y cada uno de los barriles llenados con licor de arroz elaborado ocho veces.

El dragón Orochi, al llegar al lugar, quedó fascinado por el licor, lo bebió y durante el estupor que le produjo, Susano'o lo asesinó y un río cercano se tornó rojo con la sangre del dragón. Cuando Susano'o cortó el dragón en pedazos, encontró una excelente espada en su cola, tan dura que su propia espada no la había podido cortar. Posteriormente la espada fue presentada a Amaterasu y la llamaron “"Ame no Murakumo no Tsurugi"” (más tarde llamada Kusanagi). Esta espada sería la protagonista en muchos otros cuentos posteriores.

Ōnamuji (también conocido como Ōkuninushi) era un descendiente de Susanowo. El, junto con sus muchos hermanos, compitió por la mano de la princesa Yakami de Inaba. Mientras viajaba de Izumo a Inaba para cortejarla, los hermanos se encontraron un conejo desollado yaciendo en una playa. Al ver esto le dijeron al conejo que se bañase en la playa y se secara con el viento en una alta montaña, el conejo les creyó y sufrió en agonía. Ōnamuji, quien se reía a espaldas de sus hermanos, llegó y vio al conejo dolorido y mando al conejo a bañarse en agua fresca y cubrirse con un polvo de la flor gama (cattail). El conejo sanado, quien en realidad era una deidad, le informó a Onamuji que el seria quien desposaría a la Princesa Yakami.

Las pruebas de Ōnamuji fueron muchas y murió en dos ocasiones por sus celosos hermanos. En las dos ocasiones su madre Kusanda-hime lo salvaría. Perseguido por sus enemigos, él se aventuró al reino de Susanowo donde él se encontró con la hija del vengativo dios, Suseri-hime. Susanowo probaría a Onamuji en varias ocasiones pero al final, Susanowo aprobó al joven muchacho y predijo su victoria contra sus hermanos.

Aunque la tradición Yamato atribuye la creación de las islas japonesas a Izanagi y a Izanami, la tradición Izumo dice que Onamuji, junto con un dios enano llamado Sukunabiko, contribuirían o por lo menos acabarían la creación de las islas de Japón.

"Ninigi," nieto de Amaterasu, conoció a la Princesa "Konohana-sakuya" (símbolo de las flores), la hija de Yamatumi (amo de las montañas). Ellos se enamoraron y Ninigi pidió a Yamatumi la mano de su hija. El padre estuvo tan complacido que ofreció la mano de sus dos hijas, Iwanaga (símbolo de piedra) y Sakuya (símbolo de flores). Pero Ninigi solo se casó con Sakuya y rehusó a Iwanaga.

""Iwanaga está bendecida con la eternidad y Sakuya con la prosperidad"", Yamatumi dijo en lamentación, ""al rehusar a Iwanaga, tu vida será breve de ahora en adelante"". Debido a esto, Ninigi y sus descendientes se hicieron mortales.

Sakuya concibió de noche y Ninigi dudó de ella. Para probar la legitimidad de sus niños, Sakuya juro por su suerte y se arriesgó; ella incendio su habitación mientras daba luz a sus tres hijos y debido a esto, Ninigi reconoció su castidad. Los nombres de los niños fueron "Hoderi", "Hosuseri", y "Howori".

Amaterasu ordenó a su nieto Ninigi gobernar sobre los suelos. Ella le dio tres tesoros sagrados:


Los primeros dos fueron hechos para sacar a Amaterasu de "Iwayado". El último fue encontrado en el cuerpo del dragón Yamata no Orochi. De estos tres, el espejo es el símbolo de Amaterasu. Los tres juntos constituyen los Tesoros Imperiales de Japón.

Ninigi y su compañía bajaron a la tierra y llegaron a Himuka, allí él fundó su palacio.

Hoderi vivió de la pesca en el mar mientras que su hermano "Howori" vivió de la caza en las montañas. Un día, "Howori" le pidió a su hermano intercambiar los papeles por un día. "Howori" intento pescar, pero no pudo conseguir nada y además perdió el anzuelo que su hermano le presto. Hoderi acusó implacablemente a su hermano y no aceptó sus disculpas.

Mientras que "Howori" estaba sentado en una playa, perplejamente dolorido, Shihotuti le dijo que viajara en un barco llamado el Manasikatuma y que se dirigiera a cualquier sitio que le llevará la corriente. Después de este consejo, Howori llegó a la casa de Watatumi (amo de los mares). Allí él conoció a Toyotama, hija de Watatumi y se casó con ella. Después de tres años de la unión, recordó a su hermano y su anzuelo, entonces le habló a Watatumi sobre él.

Watatumi pronto encontró el anzuelo en la garganta de una brema y se lo dio Howori. Watatumi también le dio dos bolas mágicas, "Sihomitutama", que podía causar una inundación, y Sihohirutama, que podía causar un reflujo y lo mando a la tierra, junto con su novia.

Mientras Toyotama daba a luz, ella le pidió Howori que no la mirase durante su parto. Sin embargo, Howori, lleno de curiosidad, miró furtivamente, y la vio transformada en un tiburón en el momento que nació su hijo, Ugaya. Enterada de esto, Toyotama desapareció en el mar y no volvió pero ella confió a su hermana Tamayori sobre su vivo deseo por Howori.

Ugaya se casó con su tía Tamayori y tuvieron cinco hijos, incluyendo a Ituse y Yamatobiko.

Primer Emperador: En la mitología japonesa, Yamasachi-hiko se casó con la hija del dios del mar, y nace un niño nombrado Ugaya-fukiaezu. Ugaya-fukiaezu tenía 4 hijos. Pero su segundo y tercero hijo viajaron a otros lugares. Más adelante el primero y cuarto hijo lucharon por unificar Japón e Iwarebiko se convertiría en el primer emperador y pasó a llamarse emperador Jinmu que conquista la tierra de Yamato. En esta línea está la casa imperial de Japón. 

El primer emperador legendario de Japón fue Iwarebiko. El estableció el trono en 660 a. C. 

Su casta se resume a continuación:


Véase también Yōkai


Algunos dioses japoneses: 




</doc>
<doc id="17243" url="https://es.wikipedia.org/wiki?curid=17243" title="Magnitud absoluta">
Magnitud absoluta

En astronomía, magnitud absoluta ('M') es la magnitud aparente, 'm', que tendría un objeto si estuviera a una distancia de 10 pársecs (alrededor de 32,616 años luz, o 3 × 10 km) en un espacio completamente vacío sin absorción interestelar. La ventaja de la magnitud absoluta es que tiene una relación directa con las luminosidades de los astros, siendo la misma relación para cada uno de ellos, pudiendo así, al comparar las magnitudes absolutas entre dos o más astros, también comparar las luminosidades entre ellos —ya que la distancia no influye de ninguna forma.

Magnitud absoluta de un cometa o asteroide es el brillo que tendría el astro en cuestión si estuviera situado a 1 ua tanto del Sol como de la Tierra y su ángulo de fase fuese 0º, es decir completamente iluminado por el Sol.

Para definir la magnitud absoluta es necesario especificar el tipo de radiación electromagnética que está siendo medida. La magnitud absoluta se deduce generalmente de la magnitud visual medida con un filtro V, expresándose como M. Si está definida para otras longitudes de onda, llevará diferentes subíndices, y si se considera la radiación en todas las longitudes de onda, recibe el nombre de magnitud absoluta bolométrica (M).

La magnitud absoluta se puede hallar, si se conoce la magnitud aparente (formula_1) y la distancia (formula_2) en parsec por medio de:

o bien si se conoce el paralaje (π) por

Por ejemplo, para Vega (α Lyr) es m = +0,03 y π = 0”129; teniendo entonces:
único en su clase, es el Sol; su magnitud visual es m = –26,75, pero la paralaje solar es la que corresponde a la unidad astronómica de distancia, la cual está contenida 206264,806248 veces en el parsec(1UA=1/206264,806248 pc), así pues pondremos este número de segundos, o sea, π = 206264”806248, con lo cual
o bien:

La magnitud bolométrica M, tiene en cuenta la radiación electromagnética en todas las longitudes de onda. Incluye aquellos no observados debido a la banda de paso instrumental, la absorción atmosférica de la Tierra y la extinción por el polvo interestelar. Se define en función de la luminosidad de las estrellas. En el caso de las estrellas con pocas observaciones, se debe calcular asumiendo una temperatura efectiva.

Clásicamente, la diferencia en la magnitud bolométrica está relacionada con la relación de luminosidad según:

Que hace por inversión:

dondeː
En agosto de 2015, la Unión Astronómica Internacional aprobó la Resolución B2 que define los puntos cero de las escalas absolutas y aparentes de magnitud bolométrica en unidades SI para potencia (vatios) e irradiancia (W / m²), respectivamente. Aunque las magnitudes bolométricas habían sido utilizadas por los astrónomos durante muchas décadas, había diferencias sistemáticas en las escalas de magnitud absoluta-luminosidad presentadas en varias referencias astronómicas, y ninguna normalización internacional. Esto condujo a diferencias sistemáticas en las escalas de correcciones bolométricas, que cuando se combinan con magnitudes bolométricas absolutas asumidas incorrectamente para el Sol podrían conducir a errores sistemáticos en luminosidades estelares estimadas (y las propiedades estelares calculadas que dependen de la luminosidad estelar, tales como radios, edades y así en).

La resolución B2 define una escala absoluta de magnitud bolométrica en la que corresponde a la luminosidad 3,0128 × 10 W con la luminosidad de punto cero ajustada de manera que el Sol (con luminosidad nominal 3,828 × × 10 W) corresponde a la magnitud bolométrica absoluta 4,74. Colocando una fuente de radiación (por ejemplo estrella) a la distancia estándar de 10 parsecs, se deduce que el punto cero de la escala de magnitud bolométrica aparente corresponde a la irradiación f0 = 2,518021002 × 10 W/m. Utilizando la escala UAI 2015, la irradiancia solar total nominal ("constante solar") medida en 1 unidad astronómica (1361 W / m2) corresponde a una magnitud bolométrica aparente del −26,832 .

Siguiendo la Resolución B2, la relación entre la magnitud bolométrica absoluta de una estrella y su luminosidad ya no está directamente ligada a la luminosidad (variable) del Sol:

dondeː

La nueva escala de magnitud absoluta de la UAI desconecta permanentemente la escala de la variable Sol. Sin embargo, en esta escala de potencia SI, la luminosidad solar nominal corresponde estrechamente a = 4,74, un valor que fue adoptado comúnmente por los astrónomos antes de la resolución de la UAI de 2015.

La luminosidad de la estrella en vatios puede calcularse en función de su magnitud bolométrica absoluta como:

utilizando las variables definidas anteriormente.



</doc>
<doc id="17246" url="https://es.wikipedia.org/wiki?curid=17246" title="Revelado">
Revelado

El término revelado puede referirse:


</doc>
<doc id="17247" url="https://es.wikipedia.org/wiki?curid=17247" title="Ame-no-Uzume">
Ame-no-Uzume

Ama no Uzume es la diosa (kami) de la felicidad, la fertilidad y la danza. Ella fue, en gran parte, responsable de la salida de Amaterasu fuera de su caverna.

Tras creerse vencedor en la lucha contra su hermana Amaterasu, diosa del Sol, Susanoo, el dios de los Océanos y las Tormentas, arrastrado por el orgullo del triunfo destruyó campos de cultivo y espació excrementos por los altares sagrados. Además, rompiendo el tejado de la estancia sagrada donde las hilanderas celestiales tejían las túnicas de los dioses, huyeron aterrorizadas, clavándose las lanzaderas. Esto asustó a la diosa Amaterasu que se escondió en una cueva, Ame-no-Iwa-ya-do. El mundo, sin la iluminación del sol, oscureció y los dioses, atrayendo a los espíritus malignos.

Para intentar sacarla de su escondrijo, los dioses se reunieron y realizaron distintos rituales. La diosa Ame-no-Uzume puso un cubo boca abajo frente a la cueva, se subió en él y comenzó a bailar frenéticamente hasta entrar en trance. El movimiento hizo que se le desatara el cíngulo de la túnica, mostrándose desnuda delante de otras deidades. Tan cómico les pareció la escena que los dioses se rieron con tal estruendo de carcajadas que provocó la curiosidad de Amaterasu.

Amaterasu entreabrió la roca que cerraba la cueva, mirando con curiosidad la causa de tal alboroto. Al asomarse, vio con asombro su reflejo en un espejo que los dioses Ame-no-ko-yane y Futo-dama había puesto frente a la cueva, abriendo cada vez más la roca que cerraba la cueva. 

En aquel momento, el dios Ame-no-ta-jikara, que estaba oculto tras la puerta, le agarró por la mano y la sacó de la cueva, impidiendo a la diosa volver a esconderse. El dios Futo-dama extendió una cuerda sagrada para impedir que Amaterasu volviera a esconderse en la cueva. De esta manera volvió la luz al mundo. 

La danza de Ame-no-Uzume delante de la cueva donde se escondía la diosa Amaterasu se consideran los orígenes del Kagura, danzas sagradas del sintoísmo.
Uzume todavía es adorada hoy como kami o deidades de la religión sintoísta del Japón. También la conocen como Ame-no-Uzume, la Gran Persuadora, y la Hembra Divina Alarmante. Ella es representada en la farsa kyogen como Okame, una mujer que se deleita en su sensualidad.


</doc>
<doc id="17249" url="https://es.wikipedia.org/wiki?curid=17249" title="Izanagi">
Izanagi

Izanagi (japonés: イザナギ, registrado en el Kojiki como 伊邪那岐, y en el Nihonshoki como 伊弉諾; también escrito como 伊弉諾尊) es una deidad nacida de las siete generaciones divinas en la mitología japonesa y en el sintoísmo, es también referido en el Kojiki como "el hombre que invita" o Izanagi-no-mikoto.

Él y su esposa Izanami crearon muchas islas, deidades y antepasados de Japón. Cuando Izanami murió dando a luz, Izanagi intentó (pero falló) rescatarla del Yomi (el inframundo). En el rito de limpieza posterior a su regreso, él engendró a Amaterasu (la diosa del Sol) de su ojo izquierdo, Tsukuyomi (la diosa de la Luna) de su ojo derecho, y Susanoo (el dios de las tormentas y tempestades) de su nariz. 

La historia de Izanagi e Izanami tiene parecido con el mito griego de Orfeo y Eurídice. Pero hay también grandes diferencias. Cuando Izanagi mira antes de tiempo a su esposa, él contempla su monstruoso e infernal estado y ella se avergüenza y enfurece, por lo que le persigue para matarle. En esto se asemeja en cierto modo al mito de Cupido y Psique, aunque al contrario (Psique teme que Cupido fuera un monstruo, pero descubre en su lugar a un hermoso joven). Izanami falla, pero promete matar a mil personas cada día. Izanagi replica que mil quinientas personas nacerán.



</doc>
<doc id="17261" url="https://es.wikipedia.org/wiki?curid=17261" title="Campeonato Mundial de Ciclismo en Ruta">
Campeonato Mundial de Ciclismo en Ruta

El Campeonato Mundial de Ciclismo en Ruta es la competición de ciclismo en ruta más importante a nivel internacional; se realiza anualmente desde 1927, en diferentes categorías, bajo la organización de la Unión Ciclista Internacional (UCI). A diferencia de las grandes vueltas ciclistas, cada categoría se disputa en una sola carrera; al vencedor de cada carrera se le otorga el maillot arcoíris y por tanto es el campeón del mundo durante un año.

Las distintas carreras son para tres categorías de corredores; élite masculino, sub-23 masculino y femenino, disputándose tres especialidades; carrera en línea o ruta, contrarreloj individual y contrarreloj por equipos (excepto sub-23).

A partir de 2011, dentro de este evento, se desarrolla el Campeonato Mundial de Ciclismo en Ruta Juvenil, para ciclistas menores de 19 años (anteriormente se disputaba en forma separada).

Los ciclistas que más triunfos poseen en la carrera en línea son Alfredo Binda, Eddy Merckx, Rik Van Steenbergen, Óscar Freire y Peter Sagan con tres títulos cada uno; mientras que en contrarreloj, Fabian Cancellara y Tony Martin suman cuatro títulos cada uno.

El primer campeonato del mundo en ruta se disputó en 1921 en Copenhague, pero estuvo reservado a ciclistas amateurs, donde ganó el sueco Gunnar Skoeld.

A partir de 1927 pudieron participar ciclistas profesionales, corriendo junto con los aficionados en la misma carrera. Esa primera edición se celebró en el circuito de Nürburgring en Alemania y vio el triunfo de la selección italiana a manos de Alfredo Binda para los profesionales, mientras que la 5.ª posición del belga Jean Aerts le dio el título de campeón del mundo amateur. Posteriormente el campeonato del mundo fue dividido en dos categorías: profesionales y amateur.

En 1958 se estableció el Campeonato Mundial Femenino en línea, sólo para la categoría amateur, ya que no existía actividad femenina profesional.

En la categoría amateur, en 1962 se introdujo la contrarreloj por equipos masculina. La misma se disputaba con cuatro corredores sobre una distancia de 100 km. En 1987 también se empezó a correr la versión femenina en una distancia de 50 km.

En 1994 se produjeron varios cambios en el formato de los mundiales. Para la carrera en línea masculina dejó de existir la división entre profesionales y amateurs, volviendo a correr una sola carrera todos juntos y bajo la denominación "élite" (sin límite de edad). En cuanto a la especialidad contrarreloj, comenzó a disputarse en forma individual, desapareciendo la contrarreloj por equipos. En la rama femenina sucedió lo mismo, se abrió a ciclistas profesionales y se creó la especialidad contrarreloj, suprimiendo la de equipos. En 1996 se creó la categoría sub-23 para ciclistas de 19 a 23 años, disputándose carrera en línea y contrarreloj individual.

A partir de 2012 retornó la contrarreloj por equipos, pero siendo disputada por equipos profesionales.

Los ganadores de cada prueba del campeonato tienen el derecho y el honor de llevar el maillot arcoíris por un año hasta el siguiente campeonato. Este "maillot" es una camiseta blanca con cinco bandas horizontales que representan a los cinco continentes.

Los corredores que hayan sido campeón del mundo al menos una vez durante su carrera, tienen derecho a llevar los colores arco iris en los puños y cuello de su "maillot" como recuerdo de su triunfo.

Desde la edición de 2019, los mundiales constan de siete carreras (para ver el palmarés de cada prueba, pulsar el respectivo enlace):


Los ciclistas que han ganado las pruebas de categoría absoluta en más de una ocasión son:

Los ciclistas que han ganado las pruebas de categoría absoluta en más de una ocasión son:




</doc>
<doc id="17263" url="https://es.wikipedia.org/wiki?curid=17263" title="Natalie Wood">
Natalie Wood

Natalie Wood (nacida Natalia Nikolaevna Zakharenko ; San Francisco, California; 20 de julio de 1938 – Isla Santa Catalina, California; 29 de noviembre de 1981) fue una actriz estadounidense que comenzó su carrera en el cine cuando era niña y se convirtió en una estrella de Hollywood cuando era una joven adulta. Wood recibió tres nominaciones al Oscar antes de cumplir 25 años. Comenzó a actuar en películas a los 4 años y le dieron un papel coprotagonista a los 8 años en "Miracle on 34th Street" (1947). Cuando era adolescente, obtuvo una nominación al Premio de la Academia a la Mejor Actriz de Reparto por su actuación en Rebelde sin causa (1955), seguida de un papel en "The Searchers" de John Ford (1956). También protagonizó las películas musicales "West Side Story" (1961) y "Gypsy" (1962), y recibió nominaciones al Premio de la Academia a la Mejor Actriz por sus actuaciones en "Splendor in the Grass" (1961) y "Love with the Proper Stranger" (1963). Su carrera continuó con películas como "Sex and the Single Girl" (1964), "Inside Daisy Clover" (1964) y "Bob & Carol & Ted & Alice" (1969). 

Wood nació en San Francisco hija de inmigrantes rusos. Durante la década de 1970, comenzó una pausa en su carrera, teniendo dos hijos con su esposo Robert Wagner, con quien se casó, se divorció y se volvió a casar después de divorciarse de su segundo esposo. Natalie Wood apareció en solo tres películas a lo largo de la década, pero actuó en varias producciones de televisión, incluida una nueva versión de la película "From Here to Eternity" (1979) por la que recibió un Premio Globo de Oro. Sus películas representaron una "mayoría de edad" para ella y las películas de Hollywood en general. Los críticos han sugerido que la carrera cinematográfica de Wood representa un retrato de la feminidad estadounidense moderna en transición, ya que fue una de las pocas en incluir papeles infantiles y de personajes de mediana edad.

Wood falleció ahogada frente a la isla de Santa Catalina el 29 de noviembre de 1981, a la edad de 43 años. Los eventos que rodearon su muerte se explicaron por declaraciones contradictorias de los testigos, lo que provocó que el Departamento del Sheriff del condado de Los Ángeles, bajo la instrucción de la oficina del forense, declarara la causa de su muerte como "ahogamiento y otros factores indeterminados" en 2012. En 2018, Wagner fue nombrada una persona de interés en la investigación en curso sobre su muerte.

Sus padres habían emigrado de Rusia poco antes de nacer ella. Pronto su padre sustituyó el apellido familiar por el de "Gurdín", y a los 4 años quedó inscrita en el registro como "Natasha Gurdín".

Instada y apoyada por María, su ambiciosa madre, Natasha se convirtió en una estrella de cine infantil que intervino en numerosas películas con notable éxito, siendo una de las pocas que consiguió superar la transición de estrella infantil a actriz adulta. Esta ambición de su madre por que fuera famosa se debe a que cuando estaba embarazada le leyó la mano una anciana que estaba en la calle, diciéndole: "Su hija será una gran estrella, pero deberá tener mucho cuidado con las aguas oscuras". Esto de las aguas oscuras "explicaría" más tarde su muerte. Natalie no sabía nadar, debido a que su madre le advirtió toda la vida sobre el respeto que debería tenerle al agua.

Cuando Natalie tenía 9 años, su madre y ella estaban caminando cuando se encontraron con un rodaje. Su madre le dijo que fuera a sentarse en las rodillas del director y que le cantara una canción. Este, al escuchar su hermosa voz y su gracia, le hizo su primer contrato para la película: "Miracle on 34th Street".

Natalie Wood disfrutó durante un período del estrellato, debido a su físico, a su capacidad expresiva y a la eficacia de sus actuaciones. Fue nominada al Oscar en tres ocasiones, por su interpretación en "Rebelde sin causa", "Esplendor en la hierba" y "Amores con un extraño".

Otra de sus obras fue "West Side Story", en la que interpretó el papel principal de María. Ya que tenía una bonita voz, inicialmente fue ella quien cantó todas sus canciones. Sin embargo, finalmente fue doblada por una cantante profesional, Marni Nixon.

En estos años, en el apogeo de su carrera cinematográfica, a Natalie Wood se la consideró la actriz más popular de Hollywood junto con Elizabeth Taylor. Su historial incluye otras películas importantes como "The Searchers" ("Centauros del desierto") de John Ford y "El candidato", junto a un joven Robert Redford.

Murió en 1981 cuando cayó de noche al agua desde su yate "The Splendor" (así llamado en honor a la película que la consolidó como actriz, "Esplendor en la hierba"). La nave estaba fondeada frente a la isla de Catalina, cerca de Los Ángeles, y en esos días Natalie terminaba la película "Brainstorm", con Christopher Walken, amigo del matrimonio que estaba con ellos al ocurrir la desgracia. Tenía 43 años. Sus restos se encuentran en el Cementerio Westwood Village Memorial Park de Los Ángeles (California).

En 2011, la policía reabrió el caso cuando se hizo público un libro de la escritora Marti Rully quien asegura que en la muerte de la actriz, su esposo Robert Wagner estaba plenamente involucrado. En dicho libro, la escritora exhibe un testimonio del capitán del yate, "The Splendor", Dennis Davern, quien manifestó a la escritora que Wagner había intervenido en la muerte de la actriz. En 2018 los investigadores recalificaron su muerte como "sospechosa".



</doc>
<doc id="17285" url="https://es.wikipedia.org/wiki?curid=17285" title="Airbus">
Airbus

Airbus SE es una empresa europea que diseña, fabrica y vende aviones civiles. Airbus es el principal fabricante de aeronaves del mundo. Su sede está en Blagnac, una localidad de Toulouse, Francia. Su producción y sus manufacturas están, sobre todo, en Francia, Alemania, España, China y el Reino Unido.

Airbus empezó como un consorcio de fabricantes de aeronaves llamado Airbus Industrie.

La consolidación de la defensa europea y de las compañías aeroespaciales en 1999 y en el 2000 permitieron el establecimiento de una Sociedad Anónima en 2001, adquirida por EADS (European Aeronautic Defence and Space Company, Compañía Europea Aeronáutica de Defensa y Espacial) (80 %) y por BAE Systems (20 %). Después de un periodo prolongado, BAE vendió su accionariado a EADS el 13 de octubre de 2006.

Airbus da empleo a unas 63 000 personas en 16 lugares de cuatro Estados de la Unión Europea: Francia, Alemania, España y el Reino Unido. El ensamblaje final tiene lugar en Toulouse, Francia; Hamburgo, Alemania; Sevilla, España; y, desde 2009, y conjuntamente con otra empresa, en Tianjin, China. Airbus tiene empresas subsidiarias en los Estados Unidos, Japón y la India.

La compañía produce y comercializa el primer avión de pasajeros fly-by-wire, el Airbus A320 y el avión de pasajeros más grande del mundo, el Airbus A380. Desde el año 2011 es el mayor fabricante de aviones y equipos aeroespaciales del mundo.

Airbus Industrie empezó como un consorcio de aviación europeo para competir con compañías estadounidenses como Boeing, McDonnell Douglas y Lockheed.

Aunque muchos aviones europeos eran innovadores, hasta los más exitosos eran poco producidos. En 1991, Jean Pierson, entonces CEO y director gerente de Airbus Industrie, describió varios factores que explicaban la posición dominante de los productores de aviones estadounidenses: el tamaño de los Estados Unidos hacía del transporte aéreo el modo favorito de viajar; un acuerdo anglo-estadounidense de 1942 confiaba la producción de los aviones de transporte a los Estados Unidos; y que la II Guerra Mundial había legado a Estados Unidos una "beneficiosa, vigorosa, poderosa y estructurada industria aeronáutica".

A mediados de la década de 1960, comenzaron las negociaciones preliminares para la colaboración. Cada compañía individual tenía ya este objetivo; en 1959 Hawker Siddeley había anunciado una versión "airbus" del Armstrong Whitworth AW.660 Argosy que "podría albergar unos 126 pasajeros en rutas ultra-cortas por unas 2 libras por asiento y milla".

No obstante, los fabricantes de aviones europeos estaban al tanto de los riesgos de tal desarrollo y comenzaron a aceptar, junto con sus gobiernos, que la colaboración era necesaria para el desarrollo de dicha aeronave y para competir con los más poderosos fabricantes estadounidenses. En la exhibición aérea de París de 1965 las principales compañías aéreas europeas discutieron informalmente sobre sus requisitos para el nuevo "airbus", capaz de transportar a 100 pasajeros o más en distancias cortas y medias a bajo coste. Ese mismo año, Hawker Siddley (a instancias del gobierno del Reino Unido) se asoció con las empresas Breguet y Nord para estudiar los diseños del airbus. El grupo Hawker Siddeley/Breguet/Nord HBN 100 se convirtió en la base para la continuidad del proyecto. En 1966, los socios eran Sud Aviation, posteriormente Aérospatiale (Francia), Arbeitsgemeinschaft Airbus, posteriormente Deutsche Airbus (Alemania) y Hawker Siddeley (Reino Unido). Los tres gobiernos hicieron una solicitud de crédito en 1966. El 25 de julio de 1967 los tres gobiernos acordaron seguir adelante con la propuesta.

En los dos años siguientes a este acuerdo, los gobiernos británico y francés expresaron sus dudas sobre el proyecto. El memorándum de entendimiento establecía que se deberían haber alcanzado 75 objetivos para el 31 de julio de 1968. El gobierno francés amenazó con retirarse del proyecto debido a la financiación para el desarrollo al mismo tiempo del Airbus A300, el Concorde y el Dassault Mercure, pero fue persuadida para continuar. A causa de la preocupación por la propuesta del A300B en diciembre de 1968, y temiendo no recuperar su inversión debido a la falta de ventas, el gobierno británico se retiró del proyecto el 10 de abril de 1969. Alemania aprovechó para aumentar su accionariado en el proyecto hasta el 50 %. Dada la participación de Hawker Siddeley hasta ese momento, Francia y Alemania se mostraron reacios a hacerse cargo del diseño del ala. Por ello, se permitió que esa compañía británica continuase en el proyecto como una subcontrata priviligada. Hawker Siddeley invirtió 35 millones de libras en herramientas y, como necesitaba más capital, recibió un préstamo de 35 millones de libras del gobierno alemán.

Airbus Industrie se fundó formalmente como una "groupement d'intérêt économique" (GIE), una institución similar a un consorcio, el 18 de diciembre de 1970. Se creó por una iniciativa de 1967 de los gobiernos de Francia, Alemania y Reino Unido. El 50 % del accionariado inicial era de la compañía francesa Aérospatiale y de la compañía alemana Deutsche Airbus. El nombre de Airbus era un término sin propietario usado por la industria aeronáutica en la década de 1960 para referirse a un avión comercial de cierto tamaño, por lo que fue aceptado por el idioma francés.

Las empresas Aérospatiale y Deutsche Airbus se adjudicaron un 36,5 % del trabajo, Hawker Siddeley un 20 % y la compañía neerlandesa Fokker-VFM el 7 %. Cada empresa entregaría sus secciones totalmente equipadas y listas para volar. En octubre de 1971 la empresa española Construcciones Aeronáuticas S.A. (CASA) adquirió una participación del 4,2 % de Airbus Industrie, lo que dejó a Aérospatiale y Deutsche Airbuscon el 47,9 % del accionariado. En enero de 1979 British Aerospace, que absorbió a Hawker Siddeley en 1977, adquirió un 20 % del accionariado de Airbus Industrie. Los accionistas mayoritarios redujeron su accionariado al 37,9 %, aunque CASA mantuvo su 4,2 %.

Se quiso que el Airbus A300 fuese la primera aeronave desarrollada, construida y vendida por Airbus. A comienzos de 1967 el diseño del A300 proponía 320 asientos y dos motores iguales. De acuerdo con el tratado de los tres gobiernos de 1967, Roger Béteille fue nombrado director técnico del proyecto de desarrollo del A300. Béteille desarrolló una división del trabajo que sería la base de la producción de Airbus durante los años venideros: Francia fabricaría la cabina, el control de vuelo y la sección central inferior del fuselaje; Hawker Siddeley, que usó tecnología de Hawker Siddeley Trident, produciría las alas; Alemania haría las secciones del fuselaje delantero y trasero, así como la sección central superior; Países Bajos haría los flaps y los alerones y, finalmente, España (que se hizo pleno colaborador) haría el estabilizador horizontal. El 26 de septiembre de 1967 los gobiernos alemán, francés y británico firmaron un memorándum de entendimiento que permitió continuar los estudios de desarrollo. Esto también confirmó que Sud Aviation fuese la compañía líder, que Francia y el Reino Unido tendrían un 37,5 % del trabajo y Alemania un 25 %, y que Rolls-Royce fabricaría los motores.

Como las aerolíneas apoyaron de forma tibia un Airbus A300 de más de 300 asientos, los socios propusieron el A250, que posteriormente se convertiría en el A300B, un avión de pasajeros de 250 asientos impulsado por motores pre-existentes.

Esto redujo enormemente los costes de desarrollo, porque el uso del motor de Rolls-Royce RB207, que iba a ser usado en el A300, representaba un gran porcentaje de los costes. El RB207 también sufrió dificultades y retrasos, porque Rolls-Royce concentró sus esfuerzos en desarrollar otro motor de avión, el RB211, para el avión Lockheed L-1011 y porque Rolls-Royce entró en bancarrota en 1971. El avión A300B era más pequeño pero más ligero y más económico que los rivales norteamericanos de tres motores.

En 1972, el A300 realizó su primer vuelo y el primer modelo producido, el A300B2, entró en servicio en 1974, aunque el lanzamiento del A300 se vio ensombrecido por la aeronave supersónica Concorde. Inicialmente, el éxito del consorcio fue escaso pero los pedidos para el avión recogidos se debieron, en parte, a la capacidad de comercialización del consejero delegado de Airbus, Bernard Lathière, con aerolíneas en América y Asia. En 1979 el consorcio tenía 256 encargos del A300, y Airbus puso en marcha un avión más avanzado, el A310, el año anterior. Fue la puesta a la venta del A320 en 1987 lo que garantizó la posición de Airbus como un competidor importante en el mercado de los aviones. Ese avión contaba con más de 400 encargos antes de que volase por primera vez, en comparación con los 15 encargos que tenían del A300 en 1972.

El mantenimiento de la producción y los activos de ingeniería de las compañías asociadas al efecto hicieron de Airbus Industrie una compañía con ventas y márketing. Esta distribución dio lugar a ineficiencias debido a los conflictos de intereses inherentes entre cuatro compañías enfrentadas; los dos accionistas del GIE (del consorcio) y aquellos con quien subcontrataban.
Las compañías colaboraban en el desarrollo del Airbus pero se reservaban los detalles financieros de su propia producción para poder elevar el precio que le pasaban a sus sub-conjuntos. Cada vez estaba más claro que el proyecto del Airbus no conllevaba solamente una colaboración temporal para producir un solo avión, sino que se había convertido en una marca duradera para desarrollar nuevas aeronaves. A finales de la década de 1980 se empezó a trabajar en un par de nuevos aviones de tamaño medio, los mayores producidos hasta ese momento bajo la marca Airbus, el A330 y el A340. A comienzos de la década de 1990 Jean Pierson, entonces CEO de Airbus, dijo que se debería abandonar el GIE y que Airbus debería establecerse como una empresa convencional. No obstante, las dificultades para integrar y valorar los activos de las cuatro compañías, así como elementos legales, retrasaron esta iniciativa. En diciembre de 1998, cuando se informó que la British Aerospace y la DASA estaban próximas a fusionarse, Aérospatiale paralizó las negociaciones de la conversión de Airbus; la compañía francesa temía que la unión entre BA y DASA dominase a la compañía, ya que ambas empresas suponían el 57,9 % del accionariado de Airbus. Aérospatiale propuso una división al 50% de la compañía. No obstante, el asunto se resolvió en enero de 1999 cuando BA abandonó las conversaciones con DASA para universe con Marconi Electronic System, para pasar a convertirse en BAE Systems. En el 2000 tres de las cuatro compañías asociadas (DaimlerChrysler Aerospace, sucesora de Deutsche Airbus; Aérospatiale-Matra, sucesora de Sud-Aviation; y CASA) se unieron para formar EADS, lo que simplificó el proceso. EADS es hoy propiedad de Airbus France, Airbus Deutschland y Airbus España, que eran el 80 % de Airbus Industrie. BAE Systems y EADS transfirieron sus activos de producción a la nueva compañía, Airbus S.A.S., a cambio de su accionariado correspondiente en la compañía.

A mediados de 1988 un grupo de ingenieros de Airbus, liderados por Jean Roeder, comenzaron a trabajar en secreto en el desarrollo de un avión de pasajeros de ultra-alta capacidad ("ultra-high-capacity airliner", UHCA), para completar su oferta de productos y para romper el dominio que Boeing había tenido en este segmento del mercado desde principios de 1970 con su Boeing 747. El proyecto fue anunciado en 1990, en la exhibición aérea de Farnborough, y se declaró que el objetivo era reducir un 15 % el coste del 747-400. Airbus organizó cuatro equipos de diseñadores, uno con cada uno de sus socios (Aérospatiale, DaimlerChrysler Aerospace, British Aerospace y CASA) para proponer nuevas tecnologías para sus futuros diseños de aeronaves. En junio de 1994 Airbus comenzó a desarrollar su propio gran avión de pasajeros, designado posteriormente como A3XX. Airbus estudió algunos diseños, incluyendo una extraña combinación de dos fuselajes del A340, que era el avión más grande de la compañía en ese momento. Airbus refinó su diseño y esto provocó una reducción del 15 % al 20 % del coste del 747-400 existente. En el diseño del A3XX coincidieron un diseño de dos pisos, que proporcionaba más volumen para pasajeros que el tradicional diseño de un solo piso.

Se construyeron cinco A380 para pruebas y demostraciones. El primer A380 fue presentado en una ceremonia en Toulouse el 18 de enero de 2005, y su primer vuelo tuvo lugar el 27 de abril de 2005. Después de un aterrizaje exitoso tras 3 horas y 54 minutos, el piloto de pruebas Jacques Rosay dijo que haber volado en el A380 había sido "como conducir una bicicleta". El 1 de diciembre de 2005, el A380 alcanzó su velocidad máxima de Mach 0,96. El 10 de enero de 2006 el A380 hizo su primer vuelto transatlántico hacia Medellín, en Colombia.

El Airbus A380 se aplazó hasta octubre de 2006 debido al uso de un software incompatible para ese diseño. Al principio, la planta de ensamblaje de Tolouse usaba la última versión, la 5, del CATIA (hecha por Dassault), mientras que el centro de diseño de la fábrica de Hamburgo estaba usando la versión 4, más antigua e incompatible. El resultado fue que los 530 kilómetros de cableado del avión debían ser completamente rediseñados. Aunque no se canceló ningún encargo, Airbus ha tenido que pagar millones debido a penalizaciones por la demora en las entregas.

La primera entrega de este avión fue a Singapore Airlines el 15 de octubre de 2007. Este entró en servicio el 25 de octubre de 2007 con un vuelo inaugural entre Singapur y Sídney. Dos meses después, el CEO de Singapore Airlines, Chew Choong Seng, dijo que el A380 funcionaba mejor de lo que la compañía aérea y la empresa Airbus habían previsto, ya que consumía un 20 % menos de combustible por pasajero que la flota de 747-400 que tenían. Emirates Airline fue la segunda aerolínea a la que se le hizo entrega del A380, el 28 de julio de 2008, y empezó con una ruta entre Dubai y Nueva York el 1 de agosto de 2008. La aerolínea Qantas fue la siguiente el 19 de septiembre de 2008, con una ruta entre Melbourne y Los Ángeles el 20 de octubre de 2008.

En 2003, Airbus y el grupo Kaskol crearon un centro de ingeniería de Airbus en Rusia que empezó con 30 ingenieros y que ha supuesto un modelo exitoso para la estrategia de globalización de Airbus. Fue el primer centro de ingeniería que abrió en Europa fuera de los países donde surgió el proyecto. Con un equipamiento de última generación, y manteniéndose en contacto con los centros de ingeniería de Francia y Alemania, el centro trabajaba en asuntos como la estructura del fuselaje, la fatiga de los materiales, la instalación de los sistemas y el diseño. En 2011, el centro daba empleo a 200 ingenieros que habían completado unos 30 proyectos a gran escala para los programas de desarrollo del A320, el A330/A340 y el A380. Los ingenieros rusos también realizaron más de la mitad del diseño de avión de carga A330-200F, centrando su actividad en el diseño del fuselaje, la instalación de las rejillas del suelo y las uniones. El centro actualmente está implicado en el diseño del A320neo Sharklets y en el desarrollo del A350 XWB.

El 6 de abril 2006 BAE Systems planeó vender su 20 % de participación en Airbus, entonces valorado 3,5 mil millones €. Los analistas sugirieron esa operación para asociarse con firmas estadounidenses de forma más fehaciente, tanto en términos financieros como políticos. En un primer momento BAE intentó ponerse de acuerdo en un precio con EADS a través de un procedimiento informal. Debido al alargamiento de las negociaciones y a los desacuerdos sobre el precio, BAE ejerció una opción de venta y solicitó una tasación al banco N. M. Rothshild & Sons.

En junio de 2006 Airbus se vio envuelto en una controversia internacional significativa cuando anunció nuevos retrasos en la entrega de su A380. Tras este anuncio, el valor de las acciones de la compañía se hundió un 25 % en cuestión de días, aunque se recuperó poco después. Pronto le siguieron a esto acusaciones a Noël Forgeard, CEO de EADS, el socio mayoritario, de haber abusado de información privilegiada. La pérdida de valor de las acciones supuso una grave preocupación para BAE, y la prensa describió como "roce furioso" a la relación entre BAE y EADS, ya que BAE creía que el anuncio se había hecho para bajar el valor de su participación accionarial. Un grupo de accionistas francés presentó una demanda colectiva contra EADS por no informar a los inversores de las consecuencias financieras de los retrasos del A380 y las compañías aéreas que esperaban las entregas exigieron una compensación. Como resultado de esto, el jefe de EADS, Noël Forgeard, y el CEO de Airbus, Gustav Humbert, dimitieron el 2 de julio de 2006.

El 2 de julio 2006 Rothschild valoró el accionariado de BAE en 2,75 millones de €, muy por debajo de la expectativa de BAE, de los analistas, e incluso de EADS. El 5 de julio, BAE nombró auditores independientes para investigar por qué el valor de su participación en Airbus había caído tanto en la valoración de Rothschild. Sin embargo, en septiembre de 2006, BAE acordó la venta de su participación en Airbus a EADS por 2,75 millones de € estando todavía pendiente la aprobación de los accionistas de BAE. El 4 de octubre, los accionistas votaron a favor de la venta, dejando a Airbus completamente en propiedad de EADS.

El 9 de octubre de 2006, Christian Streiff, sucesor de Humbert, dimitió por diferencias con la compañía matriz (EADS) acerca de la cantidad de independencia que se le concedería en la implementación de su plan de reorganización de Airbus. Fue sucedido por el CEO de EADS, Louis Gallois, lo que dio a Airbus un control más directo de su compañía matriz.

El 28 de febrero de 2007, el CEO Louis Gallois anunció el plan de reestructuración de la compañía. Se titulaba Power8. El plan reduciría 10 000 empleos en cuatro años; 4300 en Francia, 3700 en Alemania, 1600 en el Reino Unido y 400 en España. 5 000 de esos 10 000 pasarían a ser empleados subcontratados. Las fábricas de Saint-Nazaire, Varel y Laupheim se pusieron a la venta o se cerraron, mientras que las de Méaulte, Nordenham y Filton se "abrieron a los inversores". El 16 de septiembre de 2008, la planta de Laupheim se vendió a un consorcio de Thales-Diehl para constituir Diehl Aerospace y aunque se conservaron las actividades de diseño de Filton, la fabricación se vendió a la compañía británica GKN. Los sindicatos de Airbus en Francia y Alemania amenazaron con realizar huelgas.

Durante la crisis económica de 2007, y debido a los problemas de las aerolíneas por la caída de pasajeros, se recortó la producción del A380 a 14. Se les dieron descuentos a los primeros clientes del A380 y se recortó la producción de su modelo A330 a 5 aviones por mes.

En la exhibición aérea de París de 2011, Airbus recibió unos encargos valorados en 72,2 miles de millones de dólares por 730 aviones, lo que representaba un nuevo récord de la industria de la aviación civil. El modelo A320neo (neo por "new engine option", nueva opción de motor), anunciado en diciembre de 2010, recibió 667 encargos, que, junto con los encargos anteriores, sumaban 1 029 encargos en 6 meses, lo que también era un nuevo récord.

La línea de productos de Airbus empezó con el A300, el primer avión bimotor con dos pasillos. Una variente más pequeña del A300, con modificaciones en las alas y en los motores, fue A310. A raíz de su éxito, Airbus lanzó el 320, que se destacó por ser el primer avión comercial que utilizó un sistema de control fly-by-wire. El A320 fue, y sigue siendo, un gran éxito comercial. El A318 y el A319 son versiones más pequeñas. Algunos A319 se realizan para e mercado de los aviones de negocios corporativos, como Airbus Corporate Jets. Existe una versión alargada del A320 conocida como A321. El principal competidor del A320 es el Boeing 737.

Los productos de fuselaje ancho de mayor alcance, el A330 bimotor y el A340 de cuatro motores, tienen alas eficientes, mejoradas por winglets. El Airbus A340-500 tiene una autonomía de 16.700 kilómetros, la segunda más grande de todos los aviones comerciales, solo después del Boeing 777-200LR (17,446 kilómetros). Todos los aviones Airbus desarrollados desde entonces tienen sistemas de cabina similares al del A320, por lo que es más fácil de entrenar a la tripulación. La producción del A340 de cuatro motores finalizó en 2011 por la escasez de ventas en comparación con sus homólogos de dos motores, como el Boeing 777.

Airbus está estudiando un modelo para reempleazar el A320, llamado provisionalmente Airbus NSR, para tener una nueva "aeronave de poca autonomía" (NRS, por sus siglas en inglés). Estos estudios han indicado una eficiencia de combustible del 9 o el 10 % para la NSR. Airbus, sin embargo, prefirió optar por mejorar el diseño del A320 ya existente con nuevos winglets y trabajando en mejoras aerodinámicas. Este "A320 mejorado" debería disminuir el consumo de combustible alrededor del 4-5 %, para poder ser vendido para reemplazar el A320 en el 2017 o el 2018.

El 24 de septiembre de 2009, el director de operaciones Fabrice Brégier declaró a "Le Figaro" que la empresa necesitaría de 800 a 1000 millones de euros en seis años para desarrollar la nueva generación de aviones y preservar el liderazgo tecnológico de la empresa frente a nuevos competidores, como el COMAC C919, programado para entre 2015 y 2020.

En julio de 2007, Airbus entregó su último A300 a FedEx, lo que supuso el fin de la producción del A300/A310. Airbus intentó reubicar el ensamblaje del A320 de Toulouse a Hamburgo, y trasladar la producción del A350/A380 hacia Toulouse desde Hamburgo como parte del plan de reestructuración Power 8, que empezó con el ex-CEO Christian Streiff.

Airbus suministró piezas de repuesto y servicio para el Concorde hasta que se retiró en 2003.

Los Airbus Corporate Jets están modificados para particulares y empresas. Existe una gama de modelos que va paralela a la gama de aviones comerciales que ofrece la empresa. Estos van desde el A318 Elite al A380 Prestige, de dos o tres pisos. Tras la aparición del 737 Boeing Business Jet, Airbus se unió al mercado de los aviones de negocios con el A319 Corporate Jet en 1997. Aunque el término Airbus Corporate Jet fue usado inicialmente solo para el A319CJ, ahora se utiliza a menudo para todos los aviones corporativos y privados de esta empresa, incluyendo el modelo VIP de fuselaje ancho. En diciembre de 2008, operaban 121 aviones corporativos y privados, y se habían recibido 164 encargos, incluyendo un A380 Prestige y 107 aviones coporativos A320.

En junio de 2013, Airbus anunció que desarrollaría una gama de "maletas inteligentes", bautizadas como Bag2Go, para viajeros aéreos, junto con la empresa de equipaje Rimowa y con la empresa de tecnologías de la información T-Systems. Los objetos cuentan con una colección de elementos electrónicos que se comunican con las aplicaciones del teléfono móvil y con los sistemas de tecnologías de la información de la aerolínea, para asistir al viajero y mejorar la seguridad y la fiabilidad del manejo de su equipaje. Los gadgets incluyen un medidor de peso, un geolocalizador por GPS, RFID para la identificación y una tarjeta SIM para mensajes. Desde aquel momento se han anunciado productos similares, con más gadgets, por parte de Delsey y Bluesmart.

En la actualidad la fabricación de productos militares de Airbus no depende de Airbus SAS, sino de Airbus Defence and Space. Esto se debe a que Airbus creó Airbus Military, que posteriormente pasó a integrar Airbus Defence and Space. Tanto Airbus SAS como Airbus Defence and Space dependen de Airbus Group SE.

A finales de la década de los 90, Airbus mostró un interés creciente en el desarrollo y la venta de aviación militar. La expansión en el mercado de la aviación militar era deseable para reducir la exposición de Airbus a los altibajos de la industria de la aviación civil. Se embarcó en dos campos principales de desarrollo: de repostaje en vuelo, con el Airbus A310 MRTT (Multi-Role Tanker Transport) y el Airbus A330 MRTT, y transporte aéreo táctico con el A400M.
En enero de 1999 Airbus creó una compañía aparte, Airbus Military S.A.S., para desarrrollar y producir un avión de transporte turbopropulsado, el Airbus A400M Atlas. El A400 está siendo desarrollado por varios miembros de la OTAN, como Bélgica, Francia, Alemania, Luxemburgo, España, Turquía y el Reino Unido, como una alternativa para no depender de las aeronaves extranjeras en el transporte aéreo táctico, como el avión ucraniano Antonov An-124 y el avión estadounidense C-130 Hercules. El proyecto del A400M ha sufrido algunos retrasos y Airbus ha amenazado con cancelar su desarrollo a menos que recibiese subvenciones estatales.

Pakistán encargó un Airbus A310 MRTT en 2008, una versión de un modelo base, el A310, que ya no está en producción. El 25 de febrero de 2008 Airbus consiguió un pedido los Emiratos Árabes Unidos para tres aviones MRTT de repostaje en el aire, que eran una adaptación de los aviones de pasajeros A330. El 1 de marzo de 2008, un consorcio de Airbus y Northrop Grumman consiguió un contrato de 35000 millones de dólares para construir el nuevo avión de repostaje en vuelo KC-45A, una versión de construcción estadounidense del MRTT, para la Fuerza Aérea de los Estados Unidos. La decisión produjo una queja formal por parte de Boeing, y el contrato del KC fue cancelado para una nueva licitación.

En septiembre de 2014, Aerion se asoció con Airbus (sobre todo con Airbus Defense) para colaborar en el diseño del Aerion AS2, un avión privado supersónico de 11 asientos. Se espera que se pueda comercializar en 2021.


Información a finales de abril de 2017.

Airbus compite con Boeing cada año por encargos de aviones. Airbus ha logrado el 50 % de los encargos de la década de los 2000 desde 2003.

Airbus tuvo una gran ratio de pedidos en 2003 y 2004. En 2005 Airbus consiguió 1 111 encargos (1 055 netos), en comparación con los 1 029 (1 002 netos) de Boeing el mismo año. No obstante, Boeing ganó el 55 % en función del total de valor de los encargos y en 2006 Boeing ganó más encargos y por más valor que Airbus. En 2006 Airbus logró 824 encargos. El 2006 fue el segundo mejor año de su historia, con excepción del anterior. Airbus planeó incrementar la producción del A320 para alcanzar los 40 al mes en 2012, al tiempo que Boeing incrementaba su producción mensual del Boeing 737 de 31,5 a 35 al mes.

En cuanto a aviones operativos, hubo 7 264 aviones Airbus operativos en abril de 2013. Aunque Airbus ha recibido la mitad de los encargos de aviones desde 2003, el número de aviones Boeing que había en funcionamiento desde abril 2013 era superior a los de Airbus, habiendo 1 972 de los primeros y 1 958 de los segundos. No obstante, esa ventaja ha ido disminuyendo a medida que se han ido retirando los aviones antiguos.

Aunque ambos fabricantes tienen varios aviones de un solo pasillo y de fuselaje ancho, sus aviones no siempre compiten directamente. En vez de eso, producen modelos ligeramente más pequeños o más grandes que el otro con el fin de tapar los agujeros en la demanda. El A380, por ejemplo, está diseñado para ser más grande que el 747. El A350XWB compite por tener características superiores a las del 787 e inferiores al 777. El A320 es más grande que el 737-700, pero menor que el 737-800. El A321 es más grande que el 737-900, pero más pequeño que el anterior 757-200. Las aerolíneas ven esto como una ventaja, puesto que consiguen una gran gama de productos entre los 100 y los 500 asientos.

En los últimos años el Boeing 777 se ha vendido más que sus homólogos de Airbus, que incluyen la familia A340, así como el A330-300. El A330-200, de menor tamaño, compite con el 767, y ha vendido más que su homólogo de Boeing en los últimos años. Se prevé que el A380 reduzca más las ventas del Boeing 747. Airbus ganando una cuota de mercado en aviones de gran tamaño, a pesar de que los frecuentes retrasos en el programa A380 han provocado que varios clientes adquiriesen el 747-8. Airbus también ha propuesto al A350 para competir con el Boeing 787 Dreamliner, tras haber sido presionado por las aerolíneas para tener un producto que compitiera con ese modelo.

Airbus abrirá un centro R&D y un fondo de capital de riesgo en Silicon Valley. El CEO de Airbus, Fabrice Bregier, dijo: "¿Cuál es la debilidad de un gran grupo como Airbus cuando hablamos de innovación?. Creemos que tenemos mejores ideas que el resto del mundo. Lo creemos porque controlamos las tecnologías y las plataformas. El mundo nos ha mostrado en la industria del automóvil, la industria espacial y la industria de la alta tecnología que esto no es cierto, y que tenemos que estar abiertos a las ideas de otros y las innovaciones de los demás".

El CEO de Airbus Group, Tom Enders, declaró que: "La única manera de hacerlo para las grandes empresas, en realidad, es crear espacios fuera de la empresa principal, donde permitir e incentivar la experimentación [...] Eso es lo que hemos empezado a hacer, pero no hay un manual [...] es un poco de ensayo y error. Todos consideramos un reto lo que están haciendo las empresas de Internet."

Boeing ha protestado varias veces por las ayudas para desarrollar productos y otras formas de ayudas gubernamentales a Airbus, aunque Airbus ha respondido que Boeing recibe subvenciones ilegales a través del ejército y los contratos de investigación y que ha tenido exenciones de impuestos.

En julio de 2004, el ex CEO de Boeing, Harry Stonecipher, acusó a Airbus de abusar de un acuerdo bilateral entre la Unión Europea y los Estados Unidos de 1992 para dar apoyo gubernamental a las grandes aeronaves civiles. Los gobiernos europeos le dieron a Airbus unos fondos a modo de inversión inicial responsable (ILR, por sus siglas en inglés), conocida como "ayuda para empezar" ("launch aid") en los Estados Unidos, que debían ser devueltos con intereses y más cantidades a modos de "royalties" solamente si el avión tenía éxito comercial. Airbus sostenía que aquello era totalmente compatible con el acuerdo EE. UU.-UE de 1992 y con la normativa de la OMC. El acuerdo dice que debe devolverse la subvención con hasta un 33 % más, con intereses y royalties, y que debe ser devuelta en los 17 primeros años. Estos préstamos tienen una tasa de interés mínimo igual al coste de endeudamiento del gobierno más un 0,25 %, lo que sería inferior a los tipos de interés de los que dispondría Airbus en el los mercados sin el apoyo del gobierno. Airbus afirma que, desde la firma del acuerdo entre la UE y Estados Unidos en 1992, ha pagado a los gobiernos europeos más de 6700 millones de dólares y que esto es un 40 % más de lo que ha recibido.

Airbus argumentaba que los contratos militares con los que se premió a Boeing, que es el segundo mayor contratista de defensa de los Estados Unidos, son una forma de subvención, a propósito de la controversia que rondó el contrato militar del Boeing KC-767. El apoyo significativo del gobierno de Estados Unidos a la NASA también provee de un apoyo significativo a Boeing. También son un gran apoyo las grandes exenciones de impuestos que se ofrecen a Boeing. Por ello algunos dijeron que todo esto era una violación del tratado de 1992 y de las normas de la OMC. En sus productos recientes, como el Boeing 787 Dreamliner, también se le ha ofrecido a Boeing financiación directa de los gobiernos locales y gubernamentales.

En enero de 2005 los representantes de las áreas de comercio de la Unión Europea y de los Estados Unidos, Peter Mandelson y Robert Zoellick respectivamente, aceptaron iniciar conversaciones para resolver las tensiones en aumento. No obstante, estas conversaciones no fueron exitosas y la disputa se hizo más intensa que antes del acercamiento.

La OMC dictaminó en agosto de 2010 y en mayo de 2011 que Airbus había recibido subsidios gubernamentales impropios a través de préstamos con intereses que estaban por debajo de las ratios de varios países europeos. En un dictamen diferente de febrero de 2011, la OMC encontró que Boeing había recibido ayudas locales y federales que violaban las normas de la OMC.

Airbus tiene varias líneas de ensamblaje para sus diferentes modelos. Estas son:


No obstante, Airbus tiene muchas plantas en diferentes lugares de Europa por haberse fundado como un consorcio. Como solución original para el transporte de las partes entre las diferentes fábricas de ensemblaje se emplea el Airbus Beluga, que es capaz de transportar secciones enteras del fuselaje de un avión Airbus. Esta solución ha hecho que Boeing investigue cómo reutilizar 3 de sus aviones Boeing 747 para transportar componentes de los Boeing 787. No obstante, el fuselaje y las alas del A380 son demasiado grandes y sus partes no se pueden transportar en el Beluga. Las partes grandes del A380 son llevadas en barco a Burdeos y luego son transportadas a la planta de ensamblaje de Toulouse por el Itinéraire à Grand Gabarit, un canal de agua con una carretera.

Airbus abrió una planta de ensamblado en Tianjin, en la República Popular de China, para su serie de aeronaves A320 en 2003.

Airbus empezó construyendo una planta de fabricación de componentes de 350 millones de dólares en Harbin, China, en julio de 2009, que daba empleo a 1000 personas. Estuvo operativa a finales de 2010. La fábrica de 30 000 metros cuadrados fabricaría piezas y esamblaría algunas partes del A350 XWB, la familia del A320 y de los futuros programas de Airbus. Harbin Aircraft Industry Group Corporation, Hafei Aviation Industry Company Ltd, AviChina Industry & Technology Company y otros socios chinos controlaban el 80 % del accionariado de la fábrica mientras que Airbus controlaba el 20 % restante.

Norteamérica es una región importante para Airbus en términos de venta de aviones y de suministros. Los clientes norteamericanos han encargado 2 000 de los 5 300 aviones vendidos por Airbus en todo el mundo (desde los A318 de 107 asientos a los de 565 asientos del A380). Los contratistas estadounidenses de Airbus, que dan trabajo a unas 120 000 personas, ganaron 5 500 millones de dólares en 2003. Como ejemplo, se puede señalar que el 51 % de la carga de trabajo del A380 está en Estados Unidos.

El CEO de Airbus, Fabrice Brégier, desveló en el centro de convenciones de Mobile el 2 de julio de 2012 los planes de la compañía para crear una planta de ensamblaje en Mobile,Alabama. Este plan incluía una fábrica de 600 millones de dólares en el Aeropuerto de Mobile en Brookley para el ensamblaje del A319, el A320 y el A321. Esta fábrica podría dar trabajo a tiempo completo a 1 000 personas cuando estuviera operativa. La construcción comenzó el 8 de abril de 2013, pasó a estar operativa en 2015, y se dedicaría a producir 50 aviones al año a partir de 2017.

Airbus ha firmado el plan Flightpath 2050, un plan de la industria aeronáutica para reducir el ruido y las emisiones de CO2 y NOx de los aviones.

Airbus fue la primera empresa aeroespacial que logró el certificado ISO 14001, en enero de 2007. Este certificado se otorga tras haber evaluado a toda la empresa, no solo a los aviones que produce.

Airbus se ha unido con Honeywell y con JetBlue para reducir la polución y la dependencia del petróleo. Están intentando desarrollar un biocombustible ("biofuel") que podría usarse en torno a 2030. Las compañías se han propuesto abastecer a un tercio de los aviones del mundo sin afectar a los recursos alimenticios. Las algas se consideran como una posible fuente de energía alternativa porque absorben dióxido de carbono durante su crecimiento y porque su uso no afectaría a la producción de alimentos. Sin embargo, los combustibles realizados a base de algas o a base de otra vegetación siguen siendo experimentales, y el combustible a base de algas es caro de desarrollar. Airbus ha hecho funcionar recientemente el primer motor que usa un combustible para volar que mezcla un 60 % de queroseno con un 40 % de gas natural. Esto no acaba con las emisiones de carbono, pero carece de emisiones de azufre. El combustible alternativo puede funcionar adecuadamente en los motores de los aviones Airbus, lo que lo convierte en un combustible alternativo que no requiere nuevos motores de avión. Esto y los esfuerzos para el largo plazo de la compañía se consideran pasos hacia aviones apropiados para el medio ambiente.

De acuerdo con Patrick Crawford, del UK Export Finance:

Airbus usa un código alfanumérico, seguido de un guion y de tres dígitos.

El número del modelo del avión normalmente tiene una A, un 3, otro dígito y un cero, por ejemplo; A380. No obstante, hay excepciones, como las del A318, el A319, el A321, el A400M y el A220. Los tres dígitos subsiguientes indican el número de serie del avión, el fabricante del motor y el número de la versión del motor respectivamente. Por ejemplo, el código de un avión A320-200 con un motor de International Aero Engines (IAE) V2500-A1, contaría con un 2 por la serie 200, un 3 es por el motor IAE, y el 1 por la versión 1, de modo que el número del avión es A320-231.

Normalmente se usa una letra adicional, entre ellas: la C para una versión combinada (de pasajeros y de carga), una F para un modelo de carga, una R para un modelo de gran tamaño y una X para un modelo mejorado.




</doc>
<doc id="17286" url="https://es.wikipedia.org/wiki?curid=17286" title="Lux">
Lux

El lux (símbolo: lx) es la unidad derivada del Sistema Internacional de Unidades para la iluminancia o nivel de iluminación. Equivale a un lumen /m². Se usa en la fotometría como medida de la iluminancia, tomando en cuenta las diferentes longitudes de onda según la función de luminosidad, un modelo estándar de la sensibilidad del ojo humano a la luz.

1 lx =

El lux es una unidad derivada, basada en el lumen, que a su vez es una unidad derivada basada en la candela.

Un lux equivale a un lumen por metro cuadrado, mientras que un lumen equivale a una candela x estereorradián. El flujo luminoso total de una fuente de una candela equivale a formula_1 lúmenes (puesto que una esfera comprende formula_1 estereorradianes).

La siguiente tabla muestra unos valores orientativos del nivel de iluminación que se puede medir, en un plano horizontal a unos 0,70m del suelo, en diferentes situaciones.

La diferencia entre el lux y el lumen consiste en que el lux toma en cuenta la superficie sobre la que el flujo luminoso se distribuye. 1000 lúmenes, concentrados sobre un metro cuadrado, iluminan esa superficie con 1000 lux. Los mismos mil lúmenes, distribuidos sobre 10 metros cuadrados, producen una iluminancia de sólo 100 lux. En otras palabras, iluminar una área mayor al mismo nivel de lux requiere de un número mayor de lúmenes.

Como todas las unidades fotométricas, el lux tiene una unidad radiométrica correspondiente. La diferencia entre unidades fotométricas y radiométricas consiste en que las segundas se basan en la potencia física, con todas las longitudes de onda medidas por igual, mientras que las unidades fotométricas toman en cuenta el hecho de que el ojo (humano) es más sensible a unas longitudes de onda que a otras, por lo que cada longitud de onda tiene un peso diferente en su cálculo. El factor que determina el peso de cada longitud de onda es la función de luminosidad.

Un lux es un lumen/metro, y la unidad radiométrica correspondiente, que mide la irradiancia, es el vatio por metro cuadrado (W/m). No hay una fórmula de conversión entre lux y W/m; existe un factor de conversión diferente para cada longitud de onda, y no es posible realizar la conversión a menos que se conozca la composición espectral de la luz en cuestión.

El valor máximo de la función de luminosidad se encuentra en los 555 nm (correspondiente al color verde); el ojo es más sensible a la luz de esta longitud de onda que a ninguna otra. En el caso de luz monocromática de esta longitud de onda, la irradiancia necesaria para producir un lux es la mínima: 1,464 mW/m. Es decir, en esta longitud de onda se obtienen 683,002 lux por W/m (o lúmenes por vatio). Otras longitudes de onda de luz visible producen menos lúmenes por vatio. La función de luminosidad cae a cero para las longitudes de onda fuera del espectro visible.

Para una fuente de luz con diversas longitudes de onda, el número de lúmenes por vatio se puede calcular usando la función de luminosidad. Para que una luz sea razonablemente blanca, se requiere una mezcla de luz verde con abundancia de luz roja y azul, a las que el ojo es mucho menos sensible. Esto implica que la luz blanca (o blanquecina) produce mucho menos de los 683 lúmenes por vatio que constituyen el máximo teórico. La relación entre el número real de lúmenes por vatio y el máximo teórico se expresa como un porcentaje que recibe el nombre de eficacia luminosa. Por ejemplo, una bombilla común suele presentar una eficiencia luminosa de tan sólo el 2%.

En realidad, cada persona presenta una variación propia de función luminosa. No obstante, las unidades fotométricas se definen con gran precisión, basándose en una función de luminosidad estándar obtenida de la medición de muchos sujetos.

Las especificaciones de videocámaras suelen incluir un nivel mínimo de iluminancia en lux, a partir del cual la cámara puede grabar una imagen satisfactoria. Una videocámara con buenas características de grabación en condiciones de luz escasa tendrá un valor bajo de lux. Las cámaras fotográficas no usan esta especificación, porque en condiciones de poca luz pueden tomar fotografías simplemente usando mayores tiempos de exposición, cosa que las videocámaras no pueden hacer, puesto que el tiempo de exposición viene determinado por las imágenes por segundo que deben registrar.




</doc>
<doc id="17292" url="https://es.wikipedia.org/wiki?curid=17292" title="Generación del 98">
Generación del 98

La generación del 98 es el nombre con el que se ha reunido tradicionalmente a un grupo de escritores, ensayistas y poetas españoles que se vieron profundamente afectados por la crisis moral, política y social desencadenada en España por la derrota militar en la guerra hispano-estadounidense y la consiguiente pérdida de Puerto Rico, Guam, Cuba y las Filipinas en 1898. Todos los autores y grandes poetas englobados en esta generación nacen entre 1864 y 1876.

Se inspiraron en la corriente crítica del canovismo denominada regeneracionismo y ofrecieron una visión artística en conjunto en "La generación del 98. Clásicos y modernos".

Estos autores, a partir del denominado Grupo de los Tres (Baroja, Azorín y Maeztu), comenzaron a escribir en una vena juvenil hipercrítica e izquierdista que más tarde se orientará a una concepción "tradicional" de lo viejo y lo nuevo. Pronto, sin embargo, siguió la polémica: Pío Baroja y Ramiro de Maeztu negaron la existencia de tal generación, y más tarde Pedro Salinas la afirmó, tras minuciosos análisis, en sus cursos universitarios y en un breve artículo aparecido en "Revista de Occidente" (diciembre de 1935), siguiendo el concepto de «generación literaria» definido por el crítico literario alemán Julius Petersen; este artículo apareció luego en su "Literatura española. Siglo XX" (1949). José Ortega y Gasset distinguió dos generaciones en torno a las fechas de 1857 y 1872, una integrada por Ganivet y Unamuno y otra por los miembros más jóvenes. Su discípulo Julián Marías, utilizando el concepto de «generación histórica», y la fecha central de 1871, estableció que pertenecen a ella Miguel de Unamuno, Ángel Ganivet, Valle-Inclán, Jacinto Benavente, Carlos Arniches, Vicente Blasco Ibáñez, Gabriel y Galán, Manuel Gómez-Moreno, Miguel Asín Palacios, Serafín Álvarez Quintero, Pío Baroja, Azorín, Joaquín Álvarez Quintero, Ramiro de Maeztu, Manuel Machado, Antonio Machado y Francisco Villaespesa. No incluyó a mujeres, pero de hecho Carmen de Burgos «Colombine» (1867-1932), Consuelo Álvarez Pool «Violeta» (1867-1959) y Concha Espina (1869-1955) podrían pertenecer a ella, pues se encuentran en esa franja de fechas y sus características coinciden.

La crítica al concepto de generación fue realizada inicialmente por Juan Ramón Jiménez en un curso dictado en la década de 1950 en la Universidad de Puerto Rico (Río Piedras), y luego por un importante grupo de críticos que desde Federico de Onís, Ricardo Gullón, Allen W. Phillips, Ivan Schulman, y termina con las últimas aportaciones de José Carlos Mainer, Germán Gullón, entre otros. Todos ellos han puesto en duda la oposición del concepto de generación del 98 y de modernismo.

Formado inicialmente por el llamado Grupo de los Tres (Baroja, Azorín y Maeztu), entre los integrantes más significativos de este grupo podemos citar a Ángel Ganivet, Miguel de Unamuno, Enrique de Mesa, Ramiro de Maeztu, Azorín, Antonio Machado, los hermanos Pío y Ricardo Baroja, Ramón María del Valle-Inclán y el filólogo Ramón Menéndez Pidal. Algunos incluyen también a Vicente Blasco Ibáñez, que por su estética puede considerarse más bien un escritor del naturalismo, y también al dramaturgo Jacinto Benavente. No debe incluirse a José Ortega y Gasset, que es considerado casi unánimemente como perteneciente a la generación del 14.

Artistas de otras disciplinas pueden también considerarse dentro de esta estética, como por ejemplo los pintores Ignacio Zuloaga y Ricardo Baroja, también escritor este último. Entre los músicos destacan Isaac Albéniz y Enrique Granados.

Miembros menos destacados (o menos estudiados) de esta generación fueron Ciro Bayo (1859-1939), los periodistas, ensayistas y narradores Manuel Bueno (1874-1936), José María Salaverría (1873-1940) y Manuel Ciges Aparicio (1873-1936), Mauricio López-Roberts, Luis Ruiz Contreras (1863-1953), Rafael Urbano (1870-1924) y muchos otros.

La mayoría de los textos escritos durante esta época literaria se produjeron en los años inmediatamente posteriores a 1910 y están siempre marcados por la autojustificación de los radicalismos y rebeldías juveniles (Machado en los últimos poemas incorporados a "Campos de Castilla", Unamuno en sus artículos escritos durante la I Guerra Mundial o en la obra ensayística de Pío Baroja).

Benavente y Valle-Inclán presidían tertulias en el Café de Madrid; las frecuentaban Rubén Darío, Maeztu y Ricardo Baroja. Poco después Benavente y sus seguidores se fueron a la Cervecería Inglesa, mientras que Valle-Inclán, los hermanos Machado, Azorín y Pío Baroja tomaban el Café de Fornos. El ingenio de Valle-Inclán le llevó luego a presidir la del Café Lyon d'Or y la del nuevo Café de Levante, sin duda alguna la que congregó a mayor número de participantes.

Los autores de la generación del 98 se agruparon en torno a algunas revistas características, "Don Quijote" (1892-1902), "Germinal" (1897-1899), "Vida Nueva" (1898-1900), "Revista Nueva" (1899), "Electra" (1901), "Helios" (1903-1904) y "Alma Española" (1903-1905).

No fueron muy aficionados los autores del 98 a hablar de sus compañeros. Pío Baroja dejó bastantes recuerdos de ellos en dos libros de memorias, "Juventud, egolatría" (1917) y los siete volúmenes póstumos "Desde la última vuelta del camino". Ricardo Baroja hizo lo propio en "Gente del 98" (1952). Unamuno dejó varios textos autobiográficos sobre su juventud, pero pocos sobre su edad madura.

Los autores de la generación mantuvieron, al menos al principio, una estrecha amistad y se opusieron a la España de la Restauración; Pedro Salinas ha analizado hasta qué punto pueden considerarse verdaderamente una generación historiográficamente hablando. Lo indiscutible es que comparten una serie de puntos en común:



Por un lado, los intelectuales más modernos, secundados a veces por los propios autores criticados, sostenían que la generación del 98 se caracterizó por un aumento del egotismo, por un precoz y morboso sentimiento de frustración, por la exageración neorromántica de lo individual y por su imitación servil de las modas europeas del momento.

Por otra parte, para los escritores de la izquierda revolucionaria de los años treinta, la interpretación negativa de la rebeldía noventayochesca se une a una fundamentación ideológica: el espíritu finisecular de protesta responde al sarampión juvenil de un sector de la pequeña burguesía intelectual, condenado a refluir en una actitud espiritualista y equívoca, nacionalista y antiprogresiva. Ramón J. Sender mantenía todavía en 1971 la misma tesis (aunque con supuestos diferentes).

Los problemas a la hora de definir a la generación del 98 siempre han sido (y son) numerosos ya que no se puede abarcar la totalidad de experiencias artísticas de una extensa trayectoria temporal. La realidad del momento era muy compleja y no permite entender la generación basándose en la vivencia común de unos mismos hechos históricos (ingrediente básico de un hecho generacional). Esto se debe a un triple motivo:


Sin embargo cabe preguntarse, ¿cómo es que la generación del 98 no tomó nombre del modernismo, ya que surgen paralelamente y persiguen metas parecidas?

Los años comprendidos entre 1876 y 1898 son de hastío creativo debido al proyecto de la Restauración de Cánovas durante el reinado de Alfonso XIII. Cuando España pierde en 1898 las colonias la sociedad vuelve a poner el dedo en la llaga de la Revolución de 1868 (Revolución de la Gloriosa). La literatura del realismo se halla anquilosada y, pese a su estabilidad, la vida política se encuentra corrompida por la oligarquía, el caciquismo y el régimen de turno de partidos, que se está descomponiendo en banderías internas en el seno de los grandes partidos progresista y conservador, mientras que un tercer gran partido, el democrático, permanece marginado y ninguneado por el reparto canovista del poder. Las perspectivas profesionales de los escritores noventayochistas habían alcanzado su cima (o estaban haciéndolo). Los más viejos se acercan a la edad de Galdós y los más jóvenes a la de Unamuno. Esto significa, en contraste con la generación del 98, que se habían formado espiritualmente en los tiempos de la Revolución de septiembre.

Lo importante de considerarlos en conjunto es el hecho de que han vivido dos épocas emocional e intelectualmente distintas.


Se trata pues de hombres doblemente engañados ya que vieron fracasar dos estructuras políticas de cariz contradictorio (Revolución y Restauración). De estos dos experimentos políticos los "intelectuales" del 98 sacaron una misma conclusión: la urgencia de buscar en zonas de pensamiento y actividad ajenas a la política los medios de rescatar a España de su progresiva catalepsia [muerte aparente].

La primera repulsa intelectual tuvo lugar en los albores de la Restauración. En 1876 Francisco Giner de los Ríos funda la Institución Libre de Enseñanza. Su tarea constituye el repudio indirecto de la enseñanza oficial, probadamente ineficaz e insuficiente en aquella época, y sujeta a la agobiante tutela de los intereses políticos y religiosos.

Se planteó entonces el problema de la personalidad histórica de España (así como lo hiciesen en Francia poco antes tras la derrota de Sedán). Unamuno estudió el casticismo, Ricardo Macías Picavea la «pérdida de la personalidad», Rafael Altamira la psicología del pueblo español, Joaquín Costa la personalidad histórica de España…

Los autores noventayochescos tienen evidentes paralelos europeos:


El periodismo en tanto práctica literaria habitual y la condición "intelectual" en tanto talante personal desarrollan una nueva modalidad ensayística, ajustada a una temática en la que la evocación o lo confesional enmarcan temas de reflexión muy característicos.

La crisis de la novela o del teatro son vividas con peculiar intensidad en la nivola unamuniana, el desmoronamiento del relato en Azorín o por la peculiar teoría narrativa de Baroja.

Si importante es la generación del 98 en la literatura española, también lo es para el historiador de la lengua. En los textos de los escritores mencionados se aprecia la realidad del lenguaje, plural en circunstancias y en recursos. Estudiando la neología y los neologismos de la generación del 98, se ha podido constatar la renovación de elementos constitutivos del español, la función del léxico como recurso caracterizador de personajes y ambientes (guindilla, guinda, rosera), el ingenio del propio autor para fecundar el idioma («verde-reuma» es creación de Valle-Inclán, «piscolabis» es voz barojiana) y la capacidad de éste para captar las innovaciones léxicas que surgieron en diferentes ámbitos: abracadabrante, afiche, alopatía, cabaré, crupier, charcutería, charcutero, chic, eslogan, estor, frufrú, maquillaje, mitomanía, papillote, pose, vodevil, etc.

El corpus del léxico del 98 representa una suma de idiolectos o sistemas lingüísticos individuales que en su totalidad permiten vislumbrar la evolución del español desde el siglo hasta la primera mitad del siglo , en una época en que el léxico estándar creció por la integración de palabras procedentes de léxicos parciales (jergas, lenguaje técnico-científico, v. Haensch, 1997, 55). Numerosas voces del 98 son generacionales, las emplearon varios escritores de este grupo y posteriormente cayeron en desuso: cocota, batracio, bilbainismo, horizontal, rastacuerismo, rayadillo, dinero-esquema, intraespañolización, catedraticina, etc.
En general, se esfuerzan por aportar nuevas ideas y por elevar a la categoría de obra de arte la realidad socio- cultural en la que se prepararon para salir a otros mundos. El espíritu de los pueblos se recupera con la palabra.

El panorama musical español también se vio afectado por la crisis del 98, y se contagió del clima regeneracionista que propiciaron los intelectuales de la época.
Encomiable labor en este sentido fue la que realizó el musicólogo Felipe Pedrell. Ya en 1897 había escrito el manifiesto "Por Nuestra Música", y entre otras obras suyas, publicó el "Cancionero musical Popular Español". Además de ser el padre de la musicología y etnomusicología en España, en el terreno de la composición abrió las puertas hacia un nacionalismo musical español, como ya existía un nacionalismo musical ruso, bohemio, escandinavo... Después de introducir a Wagner (paradigma del nacionalismo alemán en la ópera) en España, trató de impulsar un nacionalismo análogo a la española. Pedrell es más conocido por su labor como teórico, musicólogo, y crítico que como compositor. No obstante, la composición musical probablemente no habría sido la misma sin él, porque marcó el camino a otros compositores de la generación del 98 y posteriores. Isaac Albéniz, fue un pianista virtuoso que escribió la "Suite Iberia", la "Suite Española", y la ópera "Pepita Jiménez". Enrique Granados, también pianista, autor de "Doce Danzas Españolas", y "Goyescas"). El virtuoso violinista Pablo Sarasate compuso todo tipo de obras exaltando el variadísimo folclore español, de norte a sur.

También se puede hablar de análogos europeos para los músicos de este periodo. Pedrell era conocido como el Wagner español, mientras que Albéniz y Sarasate eran comparados con Debussy y Paganini respectivamente.




</doc>
<doc id="17294" url="https://es.wikipedia.org/wiki?curid=17294" title="El Grullo">
El Grullo

El Grullo es un municipio del Estado de Jalisco. Su nombre procede de la abundancia de cierto zacate semiacúatico llamado "Grullo" en los terrenos pantanosos que alguna vez existieron en esta región. Dicho zacate dio nombre a la hacienda "zacate grullo" establecida en los comienzos de este lugar. Posterior a la reforma agraria es cuando desaparece el hacendado y se convierte en municipio, anteponiendo el artículo "EL" que da finalmente el nombre como lo conocemos.
Cabe destacar, que no se tienen registros de avistamientos de grullas en la zona y que, generalmente provoca confusión entre los pobladores a la hora de explicar la procedencia del nombre de nuestra tierra.
El Grullo se localiza al suroeste del Estado de Jalisco. Pertenece a la región Sierra de Amula, siendo este municipio la cabecera de la Región. Sus coordenadas extremas son de los 19° 41’ 30’’ a los 19° 53’ 50’’ de latitud norte y de los 104° 19’ 35’’ a los 104° 53’ 50’’ de longitud oeste. Con una altura promedio de 800 metros sobre el nivel del mar.

Actualmente El Grullo pertenece a la Zona Metropolitana conformada por los municipios de Autlan de Navarro y El Limon, la cual abarca un total de 1,017 km² con un total de población de 90, 263 habitantes (2015).

El Grullo cuenta con un escudo en forma francesa, con tres cuarteles en fondo azul; el primer cuartel (arriba izquierda) muestra la imagen de una partitura musical con guirnaldas de sinople unidas al centro con una cinta de gules; en el segundo cuartel cuartel(arriba derecha) se muestran varios frutos: calabaza, maíz, y caña de azúcar; el tercer cuartel(inferior) un lago con "zacate grullo" y una grulla blanca erguida sobre el agua.
El blasón está rodeado por una bordura en color oro con las palabras: SALVE, FERAZ, VALLE en negro. En la parte superior del escudo se aprecia la silueta en color negro, de la parroquia de Santa María de Guadalupe, con lambrequines, en sinople, como guías con frutas del valle que descienden hasta un tercio del 'blasón'. 

La partitura es una representación de la vocación musical presente en la población; la guirnalda hace alusión a los músicos laureados que son oriundos de la ciudad; el valle de El Grullo es una población agrícola dependiente de esta actividad y los frutos representan los cultivos tradicionales, existentes y las productos que han dejado huella en los anales del municipio.

La hierba que aparece en el escudo llamado zacate grullo es el que da origen al nombre de la ciudad. Pero a pesar de aparecer en el escudo la "grulla", es importante resaltar que esta ave está presente sólo como figura decorativa, pues no existe prueba alguna de que ésta gruiforme haya habitado la región.

La silueta de la parroquia está incluida en el escudo por ser la construcción arquitectónica más antigua y la más representativa del municipio; asimismo la imagen es una alusión a la tradición católica presente en esta ciudad.

La leyenda que se lee en la bordura del escudo: SALVE, FERAZ, VALLE, es un saludo en forma de ofrenda y rezo a la fertilidad del suelo.
Los colores azul y oro son una reproducción de los colores del escudo del estado de Jalisco.

La elaboración del escudo estuvo a cargo del Comité Pro-Festejos del 75° aniversario de municipio, acción que fue presidida por el Dr. Pedro Rubio Sánchez y realizado por Octavio Morales ; el escudo fue oficialmente aprobado 23 de septiembre de 1987.

La región que hoy ocupa El Grullo fue conquistada en el año 1524 por el capitán, explorador y militar español Francisco Cortés de San Buenaventura, sobrino del que fue conquistador de México Hernán Cortés.
El Grullo perteneció al señorío de Autlán de Navarro, que al arribo de los conquistadores gobernaba Capaya. Los primeros pobladores que hubo en el valle de origen huichol y zapoteco se asentaron cerca del arroyo El Colomo. Tiempo después, los pocos habitantes que aún vivían en el asentamiento original del poblado, en las cercanías del arroyo El Colomo, emigraron a un lugar cercano que al paso de los años sería la Hacienda de Zacate Grullo, llamada así por la [|hierba]] con ese nombre que crecía en sus pantanos. Ya por principios de 1800 arribó el primer grupo de personas Zacapala, quedando formado y asentado el núcleo de la población alrededor del año 1830.

Por decreto número 915, publicado el 29 de septiembre de 1900, la congregación de El Grullo fue elevada a comisaría Política y Judicial; perteneciendo aún al 6° cantón de Autlán.

Al paso de los años y con un crecimiento de la población, la comisaría de El Grullo fue elevada a la categoría de municipio el 13 de diciembre de 1912, según decreto número 1,528 publicado el 14 de diciembre de 1912; siendo esa la fecha oficial del nacimiento de El Grullo como un municipio más del Estado de Jalisco, anexándose las poblaciones de El Limón y El Aguacate, pero siendo El Grullo la cabecera municipal.
Fue el 1° de enero de 1913 cuando inició sus labores el primer Ayuntamiento que tuvo este municipio siendo presidente Urbano Rosales.

Posteriormente en el año de 1921, el Congreso del Estado y bajo el decreto número 2069, que fue publicado el 21 de junio de ese mismo año, la población de El Limón es elevada a la categoría de municipio, separándose así de su homólogo.

Años más adelante y bajo el decreto estatal número 7780, publicado el 27 de diciembre de 1962, se le otorga al municipio el título de ciudad, como reconocimiento al patriotismo, laboriosidad y civismo del congénere de habitantes de El Grullo.

A pesar de que El Grullo es un municipio relativamente joven, tiene identidad e historia, siendo una población afable y abierta a recibir a cualquier persona que quiera disfrutar de su hospitalidad.

El Municipio de El Grullo cuenta con un total de 17,770 hectáreas de superficie, de las cuales 372 de las mismas han sido ocupadas por la población urbana. La mayor parte de su territorio es de clima muy húmedo; la estación de primavera es seca y cálida; el estío presenta lluvias y temporadas de calor; la estación otoñal es muy parecida a la invernal, siendo esta seca y con temporadas de frío; el invierno No es del todo una estación bien definida y sólo varía al extremo sureste. La temperatura media anual es de 29.1 °C, con una precipitación pluvial media de 854 mm y por un régimen de lluvias en los meses de junio, julio, agosto y septiembre. Los vientos dominantes son en dirección suroeste y oeste, el promedio de días con heladas es de 0.7 al año.

La zona boscosa del municipio es realmente poca, cubierta de la siguiente vegetación:


Sus recursos minerales son yacimientos de cobre y manganeso.

El subsuelo de El Grullo pertenece al período cuaternario y se compone de rocas sedimentarias, arenisca toba y arenisca conglomerado.
Predominan las zonas planas con alturas de 800 a 900 msnm; pocas son las zonas semiplanas entre los 900 y 1,000 msnm. Las zonas accidentadas tienen alturas de 1,000 a 1,700 m.

La principal corriente pluvial del municipio es el río Ayuquila que le sirve de límite con el municipio de Autlán de Navarro. Existen además los arroyos El Colomo, El Saucillo, Platanar, Capirote y muchos otros que sólo tienen afluente en época de lluvias.
El municipio pertenece al sistema de riego de la presa Tacotán y Trigomil, conocido como “Sistema de riego Autlán - El Grullo”.

Los principales tipos de suelos son el feozem háplico y vertisol pélico; y como suelos asociados los de fluvisol y regosol eútrico.
Respecto a la fauna, predomina: 

Según el Conteo de Población y Vivienda del INEGI 2010, el municipio tiene 23 857 habitantes, situándolo como el municipio número 42 en tamaño poblacional en el estado. Una situación patente en el municipio, como en muchos otros, es la creciente tasa de emigrantes que salen del municipio comúnmente a los Estados Unidos, con el fin de lograr una mejor calidad de vida para el mismo emigrante como para su familia.

La vivienda en el Municipio de El Grullo es un factor clave para el desarrollo de la sociedad; la autoridad competente del municipio trabaja en programas para la regularización y escrituración de predios. Además existen programas de fomento para la vivienda.

La mayoría de viviendas particulares cuentan con:

El Grullo provee: 

El Estado de Jalisco instaló una base de Protección Civil y Bomberos, a cargo del mismo con participación en las regiones Sierra de Amula y Costa Sur.

El Grullo cuenta con: 
De la misma manera, la población de Ayuquila cuenta con un Centro de Salud Rural, de primer nivel de atención en salud y las comunidades de El Aguacate, El Paloblanco, El Cacalote y Puerta de Barro se cuentan con Casas de Salud pertenecientes a la Secretaría de Salud.

Contando con un total de 10,441 habitantes derechohabientes a los servicios de salud que ofrecen el IMSS, el ISSSTE y otras instituciones gubernamentales y un total de 1,971 familias beneficiadas con los servicios del Seguro Popular (aproximadamente).

Un factor importante del desarrollo social es la educación. El Municipio cuenta con una población total de 23 857 habitantes, de los cuales la población mayor de 6 años se encuentra en condición de alfabetización. De la población total se estima que un 92.6% saben leer y escribir y un 7.4% se encuentran en un grado de analfabetismo.

El Grullo cuenta con una población de 19 397 habitantes mayores de 5 años en condición de asistencia escolar de los cuales: 

En el municipio se cuentan con un total de 52 planteles educativos de los cuales son 
Para dar un total de 260 aulas, 15 bibliotecas escolares, 17 laboratorios escolares, 11 talleres escolares y 256 anexos.

Así como 2 bibliotecas públicas de las cuales una pertenece al Estado. También el municipio cuenta con un módulo de la Universidad Virtual perteneciente a la Universidad de Guadalajara, un Centro de Educación para los Adultos, una Plaza Comunitaria y un Centro de Computación perteneciente al Instituto Municipal de Atención a la Juventud (IMAJ.)

Medios de Comunicación:
El Municipio de El Grullo cuenta con servicios de telecomunicación por satélite, fibra óptica, redes de datos 3G, contando con los principales proveedores nacionales e internacionales de telefonía celular. Se cuenta también con los servicios de telégrafos, correo tradicional, periódico Expresión, televisión por cable, telefonía local y además llega a nuestro municipio la mayoría de las paqueterías como Estafeta, Serviporteo, Multipak, DHL y UPS.

Vías de Comunicación:
El transporte terrestre del municipio se realiza por medio de la carretera federal número 80 Guadalajara – Barra de Navidad, en el entronque del 7 kilómetros a la ciudad.

También se cuentan con carreteras estatales a Autlán de Navarro, Ciudad Guzmán, Ejutla y Tuxcacuesco.

En el Grullo el deporte principal es el fútbol, ya que el municipio cuenta con 7 canchas empastadas municipales de fútbol, se cuenta con un estadio, y la cancha de Fútbol 7 de pasto artificial que se encuentra anexa a el domo polideportivo de El Grullo. 
El segundo deporte, en cuanto a practicantes, es el béisbol del cual solo se cuenta con tres equipos y juegan en una liga de 6. Se Tiene un estadio, de los pocos que hay en Jalisco para este deporte.

También se juega el basquetbol, voleibol, tenis, frontón y una especialidad del municipio es el ciclismo de montaña.

-Mons. Luis Robles Díaz Infante. Primer Nuncio Apostólico Mexicano (1938-2007).

-Sr. Orestes López Pimienta. Músico y compositor (compuso la canción de El Grullense).

-Contra Almirante Estanislao García Espinoza.(1903-1973). Fundador de La Banda Sinfónica De La Marina Armada de México.
Autor de varias obras musicales entre ellas (Defensa Nacional, Cadetes De La Naval, y Marinos Mexicanos).

-Mrto. Juan Carbajal Beas.(1914-1990). Músico reconocido a nivel estatal. Fue Director de la Banda Municipal De El Grullo por más de 50 años.
Brindó enseñanza musical durante toda su vida gratis a este pueblo. Creador de muchas generaciones de músicos reconocidos a nivel estado, nacional e internacional.

-Mrto. Miguel Carvajal Heredia. Fue director de la banda municipal de El Grullo desde el fallecimiento de su padre Juan Carbajal Beas en 1990 hasta el día de su muerte el 25 de julio de 2015. Brindo enseñanza musical gratis a varias generaciones grullenses y varios músicos incursionando en grandes instituciones musicales como La Banda Del Estado De Jalisco, La filarmónica del estado de jalisco, La Banda de la Secretaria De Defensa Nacional Armada de México entre otras instituciones de renombre. Dedicó su vida a la docencia hasta el día de su fallecimiento y fungió como maestro por más de 30 años En la escuela preparatoria regional de El Grullo de la Universidad de Guadalajara.

-Hector Pérez Plazola (2 de julio de 1933-10 de junio de 2015) Político y Humanista.Fue presidente municipal de Guadalajara, secretario general de gobierno del Estado de Jalisco y senador por parte del Partido Acción Nacional (PAN) por Jalisco desde 2006 hasta 2012.

-Lic. Sergio Corona Blake. Hombre que colaboro activamente para potenciar el turismo de El Grullo y la región. Fue el dueño de la Plaza de toros El Relicario, del palenque de gallos el relicario ubicado en anteriormente en la calle López rayón y el mercado municipal. Enalteció la feria de El Grullo por más de 20 años trayendo artistas de primer nivel a muy bajos costos para el beneficio de los grullenses y de toda la región. Entre los artistas que vinieron durante su organización de la feria aparecen nombres como ejemplo: Juan Gabriel, Vicente Fernández, Pedro Fernández, Lorenzo de Monteclaro, Lucha Villa, Jose Alfredo Jimenez, Yuri, Rocio Banquells, Beatriz Adriana, Jorge Muñiz, Prisma, Olga Breeskin, Maribel Guardia, entre muchos más.

La economía del municipio está basada en la prestación de servicios y compra-venta de bienes y productos, seguida de la práctica de la agricultura como la segunda fuente de ingresos. El 68,5% de la población económicamente activa se dedica a actividades terciarias, el 23,7% a actividades secundarias y el 7,8% a actividades primarias.

Dentro de la actividad primaria, la principal actividad del campo es el cultivo de la caña de azúcar con 3.923 hectáreas sembradas, seguido del manejo de pastos con 633 hectáreas, el maíz de grano con 374 hectáreas, el agave con 234 hectáreas y con 405 hectáreas el resto de cultivos como sandía, pepino, chile, tomate, entre otros.

La producción de ganado en el municipio ha disminuido de manera sustancial, actividad que ha dado paso al crecimiento de la actividad comercial. No obstante aún quedan pocos productores ganaderos, de los cuales el ganado porcino es el que tiene mayor derrama económica con una producción de 440 toneladas al año, siguiendo el ganado bovino con 328, posteriormente el ganado ovino con 13 toneladas y por último el ganado caprino con 10 toneladas al año.

En El Grullo, se producen productos de origen animal que benefician la economía municipal, como son la leche de vaca en primer lugar, en segundo lugar la producción de miel de abeja y seguido de la producción de huevo para plato como tercer lugar. Además en menor número la producción de carne de ave.

Como la principal industria de la región se encuentra el Ingenio Melchor Ocampo, que aunque se encuentra ubicado en el municipio de Autlán de Navarro aun así constituye un factor determinante en la economía de nuestra ciudad. Su principal producto es el azúcar de caña, producto que ha ganado premios a nivel nacional como el azúcar con más concentración de dulzura. Esto se debe a la riqueza natural que posee el valle de El Grullo y a su sistema de riego Autlán – El Grullo, manteniendo así nuestro valle siempre verde.

En el municipio de cuenta con un mercado municipal, además de tiendas de abarrote al mayoreo, tiendas de supermercado muy grandes y por si fuera poco, el grullo se ha destacado a nivel nacional por tener la cooperativa de consumo más grande de todo México por la cantidad de socios. Tiene la Plaza Comercial Santa Fé y una plaza de usos múltiples llamada "Las Grullas" con un puente sobre la vía. 

La historia del cooperativismo ha dejado huellas muy grandes en nuestra población y ha sido un cimiento para el desarrollo comercial y económico de nuestra ciudad. Nuestro municipio cuenta con cinco cooperativas financieras, una de las cuales fue la más grande y sólida de México por los años ochenta y fue la primera Caja Popular en alcanzar el millón de pesos en ahorros, lugar que mantuvo por algunos años.

En El Grullo actualmente se encuentran Sociedades Cooperativas de Ahorro y Crédito como la Caja Popular Santa María de Guadalupe que en la actualidad es una Caja de carácter Nacional y métodos novedosos de mercadeo, la Caja Popular Agustín de Iturbide y la Caja Popular Cristóbal Colón, además de la Sociedad Cooperativa de Consumo El Grullo, la Sociedad Cooperativa Financiera SIFRA Valle de las Grullas, y otras Sociedades que aún se encuentran en proceso de constitución, como la Sociedad Cooperativa de Educación, Sociedades Cooperativas de Producción de Ladrillo, Sociedad Cooperativa de Artes y Música, entre otras. La cooperativa de Consumo y las Cajas Populares mantienen una política de cooperación entre ellas. 
El municipio tiene una larga historia de cooperativismo entre sus habitantes, siendo esta quizá la actividad que más ha favorecido al municipio.

Actualmente la Cooperativa CEDUCOOP S.C. de R.L., otorga asesoría y capacitación a un 80% de cooperativas de la región, aplicando un principio cooperativo (cooperación entre cooperativas), forma una base de desarrollo para las cooperativas y así puedan tener éxito.

La cultura es el pilar fundamental en el desarrollo social del municipio. Es por esta razón que en la ciudad se desarrollan varios talleres referentes a diversas actividades culturales.
Dentro de los eventos públicos, la Dirección de Cultura es la encargada de realizar Los Domingos Culturales, programa que acerca la cultura a nuestra población mediante eventos culturales de danza, música, teatro, etc., realizados en la explanada del jardín municipal.

La ciudad de El Grullo tiene lazos de hermandad con la ciudad de Kent, Washington, EUA; además lazos de fraternidad con los municipios de Autlán de Navarro, Zapopan, Tijuana, Baja California y Santa Ana Chiautempan, Tlaxcala.

El Municipio de El Grullo, Jalisco cuenta con poca diversidad turística, ya que es poca la extensión territorial del municipio, aunque tiene algunos sitios turísticos, actualmente se está buscando desarrollar programas turísticos que fomenten dicha actividad en áreas determinadas que se proponen como sitios de interés para los turistas tanto nacionales como extranjeros.
Dichas áreas son:

- Las Grutas de Cucuciapa
- El Cañón de la Laja
- Las Cascadas del Tigre y la Laja
- El Almud
- La zona megalítica y zona del silencio, conocida como "Guadalajarita"(Zona de perturbación electromagnética)
- Turismo Religioso, en la Parroquia central se encuentra sepultado el entonces nuncio apostólico de la Santa Sede, Monseñor Luis Robles Días.
- El traje típico de El Grullo es el de charro. 

Artesanía:
En lo que se refiere a artesanías lo más representativo es la fabricación de 
tejas, huaraches y sandalias, piezas de alfarería, sillas de montar, cerámica, 
talabartería y carpintería. Su gastronomía es variada y rica destacando las enchiladas estilo Grullo (Pequeñas, planas y coronadas con una pieza de pollo), los ya célebres internacionalmente "chacales" entomatados (langostino de río), las típicas carnitas estilo "Grullo"(resecas y con salsa aguada aparte), de la "Nouvelle Cuisine Grullense" está el "Sushi Grullense" preparado con fruta y envuelto en "marinas" y merengues del monasterio,y que susutituye el vinagre de arroz con Lechuguilla, la bebida típica es la "Lechuguilla"(Fermentado de la misma planta) y los dulces típicos son las "marinas" (palanqueta horneada de cacahuate) y los merengues de las monjas.

El Grullo, es un municipio libre, base de la división territorial, política y administrativa. Administra su Hacienda Municipal la cual se forma de los rendimientos de los bienes, así como de las contribuciones y otros ingresos que las legislaturas establezcan a su favor y, en todo caso, percibirá las participaciones del Estado, del Gobierno Federal y los derivados de la prestación de servicios públicos a su cargo.

El municipio está encabezado por el Presidente Municipal, Regidores y Síndico, los cuales son electos por elección popular. Además por designación y aprobados por cabildo municipal, el Secretario General, Tesorero y Jefe de Policía.

El Presidente Municipal tiene una duración de tres años y tiene personalidad jurídica propia. No puede ser reelecto para el periodo inmediato.

El municipio de El Grullo, por medio del Ayuntamiento, posee la Facultad Reglamentaria en materia de Policía y Gobierno así como para la administración pública municipal, los procedimientos en áreas de su competencia, y sobre todo, los servicios públicos que corren a su cargo, a saber: producción y distribución de agua potable, alumbrado público, limpieza pública, mercados, panteones, rastros, calles, parques y jardines y seguridad pública.

Para el caso de Jalisco, los gobiernos municipales realizarán a través de autoridades auxiliares, las acciones del Ayuntamiento. Esta responsabilidad recae en las figuras de los Delegados y Agentes Municipales.

En El Grullo, existen Delegados Municipales en cada una de las comunidades que integran el municipio, en específico: La Laja, El Cacalote, Ayuquila, La Puerta de Barro, Las Pilas, El Paloblanco, El Tempizque, Cucuciapa y el Aguacate.

La ciudad está hermanada con 5 ciudades.




</doc>
<doc id="17296" url="https://es.wikipedia.org/wiki?curid=17296" title="Hueso">
Hueso

Los huesos son órganos rígidos que forman el endoesqueleto de algunos animales y de los seres humanos. Poseen varias funciones: forman una estructura sólida para el sostenimiento del cuerpo, protegen órganos muy sensibles como el cerebro, hacen posible el movimiento al servir como lugar de inserción a los músculos y producen las células que forman parte de la sangre (hematopoyesis). El conjunto organizado de las piezas óseas (huesos) forma el esqueleto o sistema esquelético. Cada pieza cumple una función en particular y de conjunto en relación con las piezas próximas a las que está articulada. 

En el hueso existen diferentes variedades de tejido. El principal es el tejido óseo, un tipo especializado de tejido conectivo firme, duro y resistente que está compuesto por células (osteocitos) y componentes extracelulares calcificados que le proporciona gran dureza. Los huesos poseen una cubierta superficial de tejido conectivo fibroso llamado periostio y presentan superficies articulares que están revestidas por tejido cartilaginoso. En el interior de los huesos se encuentra la médula ósea, formada por tejidos blandos que incluyen el tejido hematopoyético que produce las células de la sangre y tejido adiposo (grasa). Cuenta además con vasos sanguíneos y nervios que irrigan e inervan su estructura. 

El hueso es un órgano vivo que contiene células y vasos sanguíneos que le aportan oxígeno y nutrientes. Se encuentra en constante proceso de remodelación, aumenta de tamaño tanto en longitud como en grosor durante la infancia y la adolescencia, y es capaz de autoregenerarse después de sufrir una fractura, proceso que se conoce como consolidación ósea. Responde a la acción de diferentes hormonas circulantes, como la calcitonina, la parathormona y la hormona del crecimiento.

La presencia de cristales de fosfato cálcico en la matriz extracelular y su disposición espacial otorgan al tejido óseo unas propiedades físicas especiales de dureza, resistencia, ligereza y cierta flexibilidad que lo hacen idóneo para cumplir su función estructural como sostén. Sin embargo el hueso no es la sustancia de mayor dureza del organismo pues es superada por el esmalte dental.

La idea de considerar al hueso como una estructura mineral inerte es errónea y está condicionada por el hecho de que después de la muerte la matriz intercelular mineralizada perdura, conservándose durante largo tiempo. Sin embargo estos restos óseos no son verdaderos huesos aunque conserven la forma, pues han perdido los vasos sanguíneos, los nervios, la médula ósea, todas las células vivas y carecen de capacidad de crecimiento y regeneración.

Los huesos que forman el esqueleto constan de varias partes:

El tejido óseo es el componente principal del hueso, está formado por células y sustancia extracelular, también llamada matriz ósea. Las células representan únicamente el 2 % del tejido, mientras que la matriz extracelular es el 98 %. 


En el tejido óseo maduro y en desarrollo, se pueden diferenciar cuatro tipos de células: osteoprogenitoras, osteoblastos, osteocitos y osteoclastos. Los tres primeros son estadios funcionales de un único tipo celular.

Los huesos poseen varias funciones, entre ellas mecánicas, metabólicas y de síntesis de las células sanguíneas.



En el periodo embrionario no existen huesos, las estructuras equivalentes están formadas por un molde de tejido mesenquimal o por cartílago hialino. A medida que se produce el crecimiento, tiene lugar de forma progresiva el proceso de osteogénesis (formación de hueso) y osificación. Este se inicia en los puntos de osificación que son en realidad cúmulos de células formadoras de hueso u osteoblastos.

Existen dos tipos de osificación:

El hueso es un tejido dinámico que se encuentra en un proceso continuo de renovación. Se ha calculado que en un año se reemplaza alrededor del 5 % del hueso cortical y un 20 % del trabecular, por lo que esqueleto se renueva totalmente cada 10 años por término medio. La renovación del hueso es necesaria entre otros motivos para la reparación del daño tisular.

El proceso se inicia por la actividad de los osteoclastos que destruyen el hueso en pequeñas áreas localizadas, a continuación los osteoblastos lo reparan creando nueva matriz intercelular y facilitando la mineralización. El balance entre la reabsorción y la formación óseas es un proceso complejo que está determinado en parte genéticamente y en el que influyen factores nutricionales y hormonales. El remodelado óseo tiene lugar durante toda la vida de un individuo, pero solo es positivo hasta los 30 años en la especie humana, edad en la que se alcanza el máximo de masa ósea, la cual se mantiene bastante estable hasta los 50, momento en que empieza a disminuir, lo que condiciona mayor tendencia a las fracturas.

Las hormonas son mensajeros químicos que actúan en un lugar del organismo diferente al sitio en que se producen. Existen varias hormonas que desempeñan importantes funciones en la fisiología ósea. Algunas de las más importantes son las siguientes:

Presentan una forma cilíndrica, predomina la longitud sobre el ancho y grosor, se dividen en tres porciones un cuerpo y dos extremos (proximal y distal), generalmente se encuentran en los miembros locomotores. Ejemplos: húmero, fémur, etc.

Presentan una forma cuboide, no predomina ninguna de sus dimensiones, su función es de amortiguamiento. Ejemplos: huesos del carpo y tarso.

Su principal característica es que son más anchos y largos que gruesos, su función es la de proteger tejidos blandos e inserción de grandes masas musculares. Ejemplos: escápula u omóplato, huesos del cráneo y coxal.

No presentan forma o división predominante para su agrupación, son impares y se localizan en la línea media, sus funciones son variables aunque la de mayor importancia es la protección del sistema nervioso central. Ejemplo: vértebras.

Son pequeñas estructuras de tejido óseo con forma más o menos ovaladas y número inconstante que se localizan en las proximidades de los tendones. Su función es la de disminuir la fricción y alteran la dirección en que se realiza la tracción muscular. Los más importantes se localizan junto a la articulación metatarsofalángica del primer dedo del pie. La rótula, aunque se estudia de forma independiente, está considerada un hueso sesamoideo de gran tamaño. Pueden sufrir fracturas y presentar procesos inflamatorios que se conocen como sesamoiditis.

Existen dos tipos de tejido óseo que se diferencian macroscópicamente y microscópicamente y se llaman hueso compacto o cortical y hueso esponjoso o trabecular. Sin embargo no hay un límite perfectamente marcado entre las dos áreas existiendo una pequeña zona de transición.

La capa exterior dura de los huesos está compuesta de hueso cortical, que también se llama hueso compacto, ya que es mucho más denso que el hueso esponjoso, su apariencia es suave, blanca y sólida, el tejido óseo compacto o hueso cortical representa el 80 % de la masa ósea del esqueleto. Se encuentra en la diáfisis o porción central de los huesos largos, en la región exterior e interior de los huesos planos y en distintas zonas del resto de los huesos. Está constituido por capas concéntricas de laminillas óseas que forman estructuras cilíndricas llamadas osteonas.

Facilita las funciones principales de los huesos: dar apoyo a todo el cuerpo, proteger los órganos, proporcionar palancas para el movimiento y almacenar y liberar elementos químicos, principalmente calcio. Consiste en múltiples columnas microscópicas, cada una de las cuales se llama osteón o sistema de Havers.

En el centro de los osteonas se encuentran los conductos de Havers por donde transitan los vasos sanguíneos. Los conductos de Volkmann transversales sirven para conectar varios conductos de Havers entre sí. La estructura resultante podría considerarse como un conjunto de columnas unidas por lo que presenta gran resistencia a las fuerzas de compresión.

En las laminillas se ubican lagunas embutidas en el matriz mineralizada que contienen osteocitos (célula principal del tejido óseo), desde cada laguna irradian pequeños canalículos ramificados que las comunican entre sí y hacen posible la nutrición de las células.

El hueso esponjoso o trabecular no contiene osteonas, sino que las láminas intersticiales se disponen de forma irregular formando unas placas llamadas trabéculas. Estas placas forman una estructura esponjosa en la que se intercalan huecos llenos de médula ósea roja. 

Dentro de las trabéculas se encuentran los osteocitos. Los vasos sanguíneos penetran directamente en el hueso esponjoso y permiten el intercambio de nutrientes y oxígeno entre la sangre y los osteocitos. 

El hueso esponjoso representa el 20 % de la masa ósea total y se encuentra en los extremos o epífisis de los huesos largos y el interior de otros huesos.

El hueso esponjoso, también llamado hueso trabecular o esponjoso,  es el tejido interno del hueso esquelético y es una red porosa de células abiertas, el hueso esponjoso tiene una relación superficie-volumen más alta que el hueso cortical y es menos denso . Esto lo hace más débil y flexible. La mayor superficie también lo hace adecuado para actividades metabólicas como el intercambio de iones calcio. 

El hueso esponjoso se encuentra típicamente en los extremos de los huesos largos, cerca de las articulaciones y en el interior de las vértebras. El hueso esponjoso es muy vascular y a menudo contiene médula ósea roja donde la hematopoyesis, se produce la producción de células sanguíneas.

La unidad anatómica y funcional primaria del hueso esponjoso es la trabécula, las trabéculas están alineadas hacia la distribución de la carga mecánica que experimenta un hueso dentro de huesos largos como el fémur . 

En cuanto a los huesos cortos, se ha estudiado la alineación trabecular en el pedículo vertebral, las formaciones delgadas de osteoblastos cubiertos de endostio crean una red irregular de espacios,  conocidos como trabéculas, dentro de estos espacios se encuentran la médula ósea y las células madre hematopoyéticas que dan lugar a plaquetas, glóbulos rojos y glóbulos blancos .  

La médula trabecular está compuesta por una red de elementos en forma de varillas y placas que hacen que el órgano en general sea más liviano y dejan espacio para los vasos sanguíneos y la médula, el hueso trabecular representa el 20% restante de la masa ósea total, pero tiene casi diez veces la superficie del hueso compacto. 

Las palabras "esponjoso" y "trabecular se" refieren a las diminutas unidades en forma de celosía (trabéculas) que forman el tejido, primero se ilustró con precisión en los grabados de Crisóstomo Martínez.

Se llama médula ósea a un tipo de tejido blando que se encuentra en el interior de los huesos. Por término medio, representa alrededor del 4 % del peso total de un adulto humano, por lo tanto una persona de 65kg cuenta con 2.6kg de este tejido. Pueden diferenciarse dos tipos: médula ósea roja y médula ósea amarilla.


El sistema esquelético está expuesto a patologías de naturaleza circulatoria, inflamatoria, neoplásica, metabólica y congénita, tal como los otros órganos del cuerpo. Los trastornos de los huesos son numerosos y variados.

Las osteocondrodisplasias, también llamadas displasias óseas, son un conjunto de enfermedades de origen congénito que producen una alteración en el tamaño forma o resistencia de los huesos, sobre todo de las extremidades y la columna vertebral, provocando con frecuencia talla baja o enanismo. Se han descrito más de 300 displasias óseas, la mayor parte de las cuales son trastornos de origen genético que se heredan según un patrón autosómico dominante, aunque lo más habitual es que se presenten casos esporádicos por mutaciones nuevas. Algunas de las entidades más conocida del grupo son la acondroplasia que provoca enanismo y la osteogénesis imperfecta, enfermedad congénita causada por la falta o insuficiencia del colágeno que se caracteriza porque los huesos de fracturan muy fácilmente, con frecuencia tras un pequeño traumatismo o a veces sin razón aparente.

Se entiende por fractura una solución de continuidad en algún punto del hueso. Generalmente está causada por un traumatismo que provoca una tensión que supera la resistencia del hueso, causando su rotura. Tienen tendencia a consolidar gracias a los mecanismos naturales de autorregeneración que se inician con la formación de un callo de fractura. En muchas ocasiones el tratamiento consiste en realizar una inmovilización del área mediante vendaje de yeso para facilitar la curación natural y evitar el desplazamiento de los fragmentos.

La osteomielitis es un proceso infeccioso que afecta a la estructura del hueso y puede provocar su destrucción (osteonecrosis). Se debe a la existencia de microorganismos patógenos, generalmente bacterias, que llegan el hueso por contigüidad a través de los tejidos próximos o heridas abiertas, pero también por vía hematógena, es decir a través de la sangre. Puede diferenciarse osteomielitis aguda y crónica.

También llamada enfermedad de Paget, es un padecimiento crónico de causa no bien conocida. Origina agrandamiento y deformación de los huesos que debilita su estructura y provoca dolor y tendencia a las fracturas. Suele afectar a un área determinada del esqueleto.

La osteoporosis es el término general para definir la porosidad del esqueleto causada por una reducción de la densidad ósea. En esta enfermedad existe una disminución de la resistencia del hueso, debido a una alteración en la remodelación ósea, por ello hay un descenso de la masa ósea, además de presentarse conductos amplios de reabsorción; en tanto que la concentración de calcio en la matriz es normal.

La osteoporosis secundaria es la más frecuente y suele aparecer asociada con la tercera edad y la menopausia. En homeostasis la unión del estrógeno con los osteoblastos a través de receptores específicos, estimula a los osteoblastos para producir y secretar matriz ósea. Con el decremento de la secreción de estrógeno por la menopausia, la actividad osteoclástica (reabsorción) se vuelve mayor que la osteoblástica (formación de tejido óseo nuevo), teniendo como consecuencia la reducción de la masa ósea, volviendo frágil al hueso, por incapacidad para el soporte de las fuerzas de tensión.

No obstante, hay un número considerable de causas secundarias de osteoporosis a cualquier edad que no suelen ser reconocidas ni valoradas, pero que se pueden identificar si se somete al paciente a una evaluación apropiada. Entre ellas destacan la deficiencia de calcio y vitamina D, la actividad física reducida, la enfermedad celíaca y la sensibilidad al gluten no celíaca no diagnosticadas ni tratadas, la diabetes mellitus, la insuficiencia renal, la acidosis tubular renal y los tratamientos con corticoides de larga duración. En las personas con enfermedad celíaca o sensibilidad al gluten no celíaca con enfermedad activa, causa la osteoporosis y no se limitan a posibles carencias nutricionales, sino también a procesos inflamatorios o autoinmunes, en los que el consumo de gluten provoca el desarrollo de autoanticuerpos que atacan a los huesos.

Diferentes enfermedades puede causar dolor de huesos, entre ellas las siguientes:


En el paleolítico superior se utilizó el hueso como materia prima para fabricar diversos utensilios o herramientas.
Entre los objetos artísticos más antiguos fabricados con hueso, se encuentra la hebilla del cinturón de san Césaire, obispo de Arlés, datada en el siglo VI, que representa a soldados romanos dormidos al lado de la tumba de Cristo. En la actualidad existen artesanos que utilizan el hueso como materia prima en diversas artes, por ejemplo los lauderos utilizan el hueso para las clavijas de distintos instrumentos de cuerda.




</doc>
<doc id="17298" url="https://es.wikipedia.org/wiki?curid=17298" title="Huesos de la cara">
Huesos de la cara

El esplacnocráneo o viscerocráneo, hace referencia a la parte del cráneo que contiene la parte anterior de los sistemas digestivo y respiratorio. Antiguamente se consideraba que los músculos que rodeaban al esplacnocráneo se formaban de la misma forma que los músculos que rodean la parte interna del aparato digestivo, es decir que eran musculatura visceral. Actualmente se ha podido seguir el desarrollo de estos músculos, los cuales provienen de la parte dorsal del cráneo y son musculatura esquelética. La boca además se forma a partir de la boca primitiva o estomodeo por lo cual sus tejidos son de origen ectodérmico. 

En los peces, el esplacnocráneo está compuesto por el arco mandibular (la boca), el arco hioideo (donde se encuentra el espiráculo y que suele participar en el sostén de la boca) y un número variable de arcos braquiales, que sostienen las branquias, y a los huesos dérmicos relacionados con estos, incluyendo los huesos del opérculo. En los tetrápodos, estos arcos contribuyen de manera variable en el desarrollo embrionario al desarrollo de la boca, el aparato hioideo, el oído medio y la laringe.

En medicina, en los seres humanos, se consideran como huesos de la cara:


Los huesos nasales y lagrimales al estar por encima de las cápsulas nasales que forman parte del neurocráneo embrionario, no se consideran parte del esplacnocráneo en la anatomía comparada de vertebrados.



</doc>
<doc id="17303" url="https://es.wikipedia.org/wiki?curid=17303" title="Tetrodo">
Tetrodo

Se denomina tetrodo a una válvula termoiónica constituida por cuatro electrodos: cátodo, dos rejillas y ánodo.

El tubo tetrodo fue desarrollado por Walter H. Schottky mientras trabajaba para Siemens & Halske GMBH en Alemania, durante la Primera Guerra Mundial.

El tetrodo se desarrolló para evitar un efecto indeseable que se produce en la válvula triodo debido a que la rejilla y la placa de la misma se comportan como un condensador. Esta capacidad parásita realimenta la señal de la placa a la rejilla, dificultando el buen funcionamiento de la válvula en frecuencias altas, por lo que se introdujo una segunda rejilla, denominada "pantalla", entre la rejilla normal y la placa. Con ello el condensador rejilla-placa queda desdoblado en dos condensadores en serie, desacoplando la señal entre la placa y la rejilla de control, consiguiéndose con ello una mejora de la amplificación de las frecuencias altas.

A la pantalla se le aplica un potencial positivo con objeto de acelerar los electrones que van del filamento-cátodo a la placa. Este potencial positivo "oculta" el potencial del ánodo, que ejerce poca influencia sobre la rejilla de control. Esta insensibilidad a la tensión de ánodo permite mucha mayor ganancia a un tetrodo que a un triodo.

Cuando se va elevando la potencia de una válvula termoiónica, los electrones alcanzan el ánodo con energía cada vez mayor, hasta que son capaces de arrancar electrones del propio ánodo. Estos, llamados "electrones secundarios", se dirigen hacia el electrodo más positivo, de modo que en triodos o diodos vuelven a caer sobre el ánodo, pero en un tetrodo son atraídos por la rejilla pantalla, disminuyendo la corriente de ánodo y la eficiencia de la válvula. El pentodo resuelve este problema mediante la adición de una nueva rejilla, pero existe otra solución: el tetrodo de haz dirigido.

En los tubos de haz dirigido, tetrodos o pentodos, unos electrodos conectados al potencial del cátodo confinan y dirigen el haz de electrones hacia el ánodo, a través de las rejillas de control y pantalla. Estas rejillas consisten en sendos hilos bobinados en hélice con el mismo paso a fin de que los hilos de la rejilla pantalla se sitúen tras los de la rejilla de control, quedando ocultos al cátodo. Así el haz de electrones se divide en hojas, que pasan entre los hilos de la rejilla de control y alcanzan el ánodo. Con esta disposición de las rejillas, la corriente a través de la rejilla supresora es extremadamente reducida. Tras la rejilla supresora existe un espacio en el cual los electrones se frenan debido al menor potencial de ánodo con respecto a la rejilla pantalla. Estos electrones lentos crean una carga espacial negativa que repele hacia el ánodo los electrones secundarios provenientes de éste.

La acción efectiva de la carga de espacio para la eliminación de los electrones secundarios y la reducida corriente de pantalla proprcionan a este tipo de tubos alta capacidad de potencia, gran sensibilidad y eficiencia.



</doc>
<doc id="17304" url="https://es.wikipedia.org/wiki?curid=17304" title="Lanzamiento de bala">
Lanzamiento de bala

El lanzamiento de bala o de peso es una prueba del atletismo moderno, que consiste en lanzar una bola sólida de acero a la máxima distancia posible.

La actual plusmarca mundial masculina es de 23.12m, lograda por el estadounidense Randy Barnes, campeón en los Juegos Olímpicos de 1996 en Atlanta y en la categoría femenina pertenece a Natalya Lisovskaya con una marca de 22.63m desde el año 1987.

La primera mención que encontramos del lanzamiento de bala es en texto griego del , concretamente en la "Ilíada", en el Canto XXIII - 826 y 836, durante los Juegos Fúnebres en honor a Patroclo:

El lanzamiento de peso moderno nació como una demostración de fuerza en las competiciones tradicionales de Irlanda y Escocia. Aparece a mediados del siglo XVIII un intento de normalizar esta prueba y se estandariza el peso con las balas de cañón inglesas que pesaban 16 libras (7.260kg) y se lanzaba desde un cuadrado de 7 pies (2.125m) de lado delimitado con un pequeño borde, este peso y medida se mantiene actualmente.

Está compuesta de acero sólido, el peso de la bala es de 7.26kg en hombres y de 4kg en mujeres. Este peso puede variar según la edad de los practicantes; en la categoría juvenil masculina se lanzan de 6kg y en la categoría juvenil femenina de 4kg, en la categoría menor masculina se lanzan de 5kg y en la categoría sénior femenina de 3kg..

El área de lanzamiento está circunscrita a un círculo de 2.137m (7 pies) de diámetro, generalmente de cemento, y en su parte delantera tiene un borde de madera que lo delimita. La zona donde cae el peso se denomina zona de caída y es un ángulo de 40º desde el área de lanzamiento.

En los Juegos Olímpicos de Atenas 1896 y Juegos Olímpicos de París 1900 se lanza desde una plataforma rectangular pintada en el suelo o delimitada por cintas. El círculo actual, se comienza a utilizar a finales de ese mismo siglo, pero no es hasta los Juegos Olímpicos de San Luis 1904 donde se utiliza por primera vez en una competición internacional, pero pintado en el suelo con cal blanca y en otra superficie distinta al cemento, se supone que hierba o tierra y luego se añade un borde de madera que delimita frontalmente el círculo de lanzamiento. Posteriormente se utiliza una superficie de ceniza, y se mantiene su uso hasta la temporada 1952-53, fecha en la que empiezan a aparecer los círculos de cemento con un borde de madera frontal y que favorecen el desplazamiento del lanzador.

El sector de caída inicialmente fue de 65º, en 1973 se redujo a 45°, a 40° en 1979 y a 34,92° en 2004 manteniéndose hasta la actualidad.

- "Actualizado a octubre de 2019."

- "Actualizado a octubre de 2019."
- "Actualizado a octubre de 2019."

"Para medallistas tan nena 
cómo sasha nair ramos Lezama a y que darles chancletasos verr ".





</doc>
<doc id="17309" url="https://es.wikipedia.org/wiki?curid=17309" title="Comisión de Promoción del Perú para la Exportación y el Turismo">
Comisión de Promoción del Perú para la Exportación y el Turismo

La Comisión de Promoción del Perú para la Exportación y el Turismo o PromPerú es una entidad dependiente del Ministerio de Comercio Exterior y Turismo del Perú. Tiene su sede en la ciudad de Lima. Es la entidad que integra a las antiguas Comisión para la Promoción de Exportaciones (PROMPEX) y a la Comisión de Promoción del Perú (PROMPERÚ) - previamente encargada de la promoción del turismo - de acuerdo al D.S. Nº 003-2007-MINCETUR.

Desarrollar estrategias para posicionar una imagen integrada y atractiva del Perú que permitan desarrollar el turismo interno y promoverlo ante el mundo como un destino privilegiado para el turismo receptivo y las inversiones. Igualmente tiene como función la promoción de las exportaciones que realiza este país.



El Consejo Directivo de PromPerú está presidido por el Ministro de Comercio Exterior y Turismo y tiene 19 miembros más: 2 viceministros (de Comercio Exterior y de Turismo), 6 representantes de Ministerios (Relaciones Exteriores, Economía y Finanzas, Agricultura y Riego, Cultura, Ambiente y Producción), un representante de la Agencia de Promoción de la Inversión Privada; los Presidentes o representantes de la Asociación de Exportadores, de la Asociación de Gremios Productores Agroexportadores, de la Sociedad Nacional de Industrias, de la Cámara de Comercio de Lima y de la Cámara Nacional de Turismo; un representante de las Asociaciones de la Micro y Pequeña Empresa y los representantes gremiales de la Zona Turística Nor Amazónica, de la Zona Turística Centro y de la Zona Turística Sur.




</doc>
<doc id="17310" url="https://es.wikipedia.org/wiki?curid=17310" title="Agencia Peruana de Cooperación Internacional">
Agencia Peruana de Cooperación Internacional

La Agencia Peruana de Cooperación Internacional (APCI) es un organismo público ejecutor adscrito al Ministerio de Relaciones Exteriores del Perú. Tiene su sede en Lima.

Entre sus funciones principales, la APCI se encarga de:



La Cooperación Técnica Internacional responde a la Política Exterior del Perú y se articula al Plan Estratégico Sectorial Multianual (PESEM) 2015-2021 del Sector Relaciones Exteriores, que establece los objetivos y las acciones estratégicas sectoriales priorizadas a mediano plazo, correspondiéndole a la APCI participar en el desarrollo de las siguientes acciones estratégicas sectoriales: 

La Política Nacional de Cooperación Técnica Internacional – PNCTI, contempla 17 temas prioritarios agrupados en 4 áreas prioritarias: 
1. Derechos humanos y diversidad cultural.

2. Acceso universal a una justicia eficiente, eficaz y transparente.

3. Empoderamiento de la mujer y atención a grupos vulnerables.

4. Acceso equitativo a una educación integral de calidad.

5. Acceso a servicios integrales de Salud y Nutrición con calidad.

6. Acceso a servicios adecuados de agua, saneamiento, energía rural y telecomunicaciones.

7. Modernización y descentralización de la Administración Pública con eficiencia, eficacia y transparencia.

8. Participación equitativa y eficiente de las y los ciudadanos. 

9. Seguridad Ciudadana y gestión de riesgos de desastres.

10. Estructura productiva y turística diversificada, competitiva y sostenible.

11. Oferta exportable y acceso a nuevos mercados.

12. Ciencia, tecnología e innovación.

13. Gestión de la migración laboral interna y externa, con énfasis en la generación de oportunidades de trabajo.

14. Actividades económicas diversificadas concordantes con las ventajas comparativas y competitivas de cada espacio geográfico regional.

15. Conservación y aprovechamiento sostenible de los recursos naturales.

16. Manejo integrado, eficiente y sostenible del recurso hídrico y de las cuencas hidrográficas.

17. Calidad ambiental y adaptación al cambio climático, incorporando la perspectiva de la Gobernanza Climática.



</doc>
<doc id="17311" url="https://es.wikipedia.org/wiki?curid=17311" title="Archivo General de la Nación del Perú">
Archivo General de la Nación del Perú

El Archivo General de la Nación, es un organismo público ejecutor adscrito al Ministerio de Cultura del Perú. Es el ente rector del Sistema Nacional de Archivos y tienen a su cargo la conservación del Patrimonio Documental de la Nación. Tiene su sede en la ciudad de Lima.

En el Archivo General de la Nación se resguardan y conservan los testimonios documentales – archivísticos de los acontecimientos de mayor relevancia que han protagonizado los peruanos desde el siglo XVI.

Fue creado el 15 de mayo de 1861, en el gobierno de Ramón Castilla y Marquesado, como Archivo Nacional, con el objetivo de custodiar documentación histórica gubernamental, que hasta ese entonces se mantenían en el Convento de San Agustín en el Centro de Lima. El Archivo Nacional estuvo ubicado en la antigua Biblioteca Nacional.

En los años 1940, el Archivo Nacional fue trasladado al Palacio de Justicia de Lima. 

En 1972, el Gobierno Revolucionario de la Fuerza Armada cambió la denominación a Archivo General de la Nación.






</doc>
<doc id="17315" url="https://es.wikipedia.org/wiki?curid=17315" title="Fútbol Club Barcelona">
Fútbol Club Barcelona

El Fútbol Club Barcelona (), conocido popularmente como Barça, es una entidad polideportiva de Barcelona, España. Fue fundado como club de fútbol el 29 de noviembre de 1899 y registrado oficialmente el 5 de enero de 1903. Es uno de los cuatro clubes profesionales de España junto a Real Madrid Club de Fútbol, Athletic Club y Club Atlético Osasuna que no es sociedad anónima, de manera que la propiedad del club recae en sus socios.

Tanto el club como sus hinchas reciben el apelativo de «culés», y también, en referencia a sus colores, como «azulgranas o blaugranas», tal como aparece en su himno, el «Cant del Barça», el cual en su segunda línea menciona «Som la gent blaugrana» (en castellano, «Somos la gente azulgrana»). A nivel institucional, el Fútbol Club Barcelona tiene a su servicio, para atender a socios, simpatizantes y público en general, la Oficina de Atención al Barcelonista, donde quien lo solicita es atendido en los idiomas oficiales del club, que son el catalán, el castellano y el inglés.

Una de las principales características del F. C. Barcelona es su carácter polideportivo. Se distingue de las demás instituciones deportivas por el hecho de que posee un extenso palmarés a nivel europeo, por el alto nivel de formación de sus jugadores, el potencial económico que maneja, por la calidad de los jugadores y también porque «posee una remarcable cultura del juego y del triunfo». Asimismo, los medallistas olímpicos que han representado a la entidad blaugrana han conquistado once oros, veintitrés platas y veintiocho bronces en las distintas disciplinas deportivas.

Otro de sus hechos distintivos es su masa social de socios y aficionados. El club ha logrado integrar de forma estratégica cuestiones políticas, religiosas, culturales y sociales, que van enmarcadas dentro del ámbito deportivo, esto permite que los socios y los aficionados respondan a todos los eventos sociales del club, también que tengan mayor participación en actividades administrativas y se fortalezcan los vínculos entre las peñas. En 2011 alcanzó los 180 000 socios, siendo hasta ese momento el segundo equipo con más asociados del mundo, seguido por el Manchester United F. C.. Existen, además, 1175 peñas barcelonistas repartidas por todo el mundo. 

Sus dos rivales históricos son el Real Club Deportivo Espanyol, contra el que disputa el derbi catalán, y el Real Madrid C. F., con quien se enfrenta en «El Clásico», siendo este uno de los encuentros de mayor rivalidad e interés del fútbol mundial.

Es uno de los equipos más populares de su país y del mundo. Según una encuesta publicada por "Personality Media", el F. C. Barcelona posee un 32,1 % de simpatía entre la población que sigue al fútbol, seguido del Real Madrid con un 32,07 %. El número de simpatizantes favorece que posea un valor en el mercado estimado en algo más de 3500 millones de euros y obtuviera en 2018-19 una cantidad de ingresos de 690 millones.

Es el , a nivel nacional domina el palmarés con setenta y cuatro campeonatos, entre veintiséis Ligas de España, treinta Copas del Rey, dos Copas de la Liga, trece Supercopas de España y tres Copas Eva Duarte. Y a nivel internacional ostenta veintidós trofeos, con tres Copas Mundiales de Clubes, cinco Ligas de Campeones, cuatro Recopas de la UEFA, cinco Supercopas de la UEFA, dos Copas Latinas y tres Copas de Ferias.

Según las estadísticas que realiza el IFFHS, el F. C. Barcelona es el mejor equipo de fútbol europeo y mundial de la primera década del siglo, y lidera el ranking global del siglo con 4935 puntos con una diferencia de 274 puntos sobre el segundo clasificado (Real Madrid C. F.). Es además el equipo de fútbol que más veces ha figurado en los podios del FIFA World Player (15) y del Balón de Oro (20).

El Fútbol Club Barcelona fue fundado el 29 de noviembre de 1899 por un grupo de doce aficionados al fútbol, convocados por el suizo Hans Gamper mediante un anuncio publicado en la revista "Los Deportes" el 22 de octubre del mismo año. Entre los doce fundadores del club había seis españoles, tres suizos, dos ingleses y un alemán. El nombre original escogido fue «"Foot-ball Club Barcelona"», en inglés, y se designó al suizo Walter Wild como primer presidente del club por ser la persona de más edad de entre las presentes.

A finales de su primera década consiguió sus primeros títulos, una Copa de España y una Copa de los Pirineos.

Durante los años 1910 el club dio un gran salto, tanto deportivo como social: ganó dos Copas de España y tres Copas de los Pirineos, y llegó a los 3000 asociados, convirtiéndose ya en una de las sociedades más populares de Cataluña. En aquellos años fue cuando se popularizó el apelativo de «culés» referente a los aficionados del club. El equipo jugaba sus partidos en un campo situado en la calle Industria de Barcelona, que se llenaba masivamente cuando jugaba el Barcelona, y desde la calle se veía cómo estaban sentados en las galerías hechas de madera, de espaldas, los aficionados situados en la parte más alta del graderío. La imagen desde la calle era la de una gran cantidad de traseros (culos), por ello, a los aficionados del Barcelona se les comenzó a llamar «culés». De esa década también cabe anotar que, en 1914, el club creó su primera sección polideportiva, la de atletismo.

El 1 de enero de 1913 el club aceptó por primera vez a una mujer como socia, Edelmira Calvetó. En 1934 la periodista y atleta Ana María Martínez Sagi se convirtió en la primera mujer que formó parte de la junta directiva.

Los años 1920 pasaron a la historia como la primera época dorada del club. Se pasó de 3000 a 12 000 socios y, en 1922, se estrenó el primer gran estadio del club, el "Camp de Les Corts", con capacidad para 30 000 espectadores. Fueron años en los que el club ganó cuatro Copas de España y, en 1929, la primera Liga española de la historia. También cabe anotar los incidentes acaecidos en 1925 cuando el gobierno de la dictadura de Primo de Rivera cerró el estadio de Les Corts durante seis meses y obligó a dimitir al presidente Hans Gamper a causa de los silbidos con los que la afición barcelonista recibió la interpretación de la Marcha Real en los prolegómenos de un encuentro. De esa década cabe destacar que el club avanzó en la línea de ampliar su carácter polideportivo, y creó las secciones de hockey hierba, baloncesto y rugby.
Los años 1930 fueron de gran crisis para el club. Se inició la década con el suicidio de Hans Gamper, probablemente debido a la catastrófica situación económica en la que se vio sumido tras el desplome de la bolsa de Wall Street en 1929. En la temporada 1933-34 el F. C. Barcelona terminó penúltimo, evitando la última plaza que hasta la temporada anterior suponía el descenso de categoría. Esa temporada quedaría por delante del Arenas de Guecho por seis puntos de diferencia, aunque el equipo vizcaíno evitó el descenso gracias a la ampliación de la Primera División a doce equipos planificada por la Federación Española antes del inicio de la temporada y que supuso que esa temporada no hubiera plaza de descenso.

Posteriormente, con el advenimiento de la Segunda República se produjo un descenso del número de socios que se agravó con el estallido de la Guerra Civil española en 1936. Ese año, además, el presidente del club Josep Suñol, que era político de Esquerra Republicana de Catalunya, fue fusilado por las tropas nacionales en la sierra de Guadarrama. El club acabó la década con tan sólo 2500 socios.

Durante los años 1940 el club fue superando poco a poco su crisis social y deportiva. El club fue tomado por las autoridades del nuevo régimen franquista que, en adelante y hasta 1953, designarían directamente al presidente del club. Los nuevos rectores castellanizaron todos los estamentos del club, eliminando cualquier connotación catalanista o anglosajona. En 1940 el club pasó a denominarse «Club de Fútbol Barcelona» en lugar de «"Football Club Barcelona"», y se modificó el escudo: se suprimieron las cuatro barras de la bandera catalana para colocar en su lugar la bandera española, aunque en 1949, con motivo de las bodas de oro del club, el gobierno autorizó la reposición de la bandera catalana. En el plano deportivo se recompuso el equipo tras la crisis de la guerra y se acabaron conquistando tres ligas españolas, una Copa de España y dos Copas Eva Duarte. Además, en los años 1940 se crearon nuevas secciones polideportivas entre las que destacaron las de balonmano y hockey sobre patines.

Los años 1950 fueron una de las mejores décadas de la historia del club, tanto en el plano deportivo como social. El fichaje de Ladislao Kubala, en 1950, fue la piedra angular sobre la que se construyó un equipo que, en esa década, consiguió 3 Ligas españolas, 5 Copas del Generalísimo, 4 Copas Eva Duarte, 3 Copa Duward, 1 Copa Latina, 2 Copa Martini&Rossi y 1 Pequeña Copa del Mundo de Clubes. La masa social creció hasta los 38 000 socios que dejaron pequeño el campo de Les Corts, de manera que se construyó un nuevo estadio, el Camp Nou, inaugurado en septiembre de 1957. Otro hecho destacado de esa década fue la celebración de las primeras elecciones democráticas a la presidencia del club en 1953, aunque sólo votaron los socios varones. Ese mismo año tuvo lugar un contencioso con el Real Madrid por el fichaje de Alfredo Di Stéfano, quien acabaría jugando para el club madrileño.

 El estadio del F. C. Barcelona se convirtió en uno de los pocos escenarios públicos donde los aficionados se expresaban libremente, y el club se convirtió en el mejor embajador de Cataluña en el exterior. Fue en aquellos años cuando se dijo que, por su simbolismo, el Barcelona era «más que un club», expresión pronunciada por el presidente Narcís de Carreras en su discurso de toma de posesión en 1968.

Tras los éxitos de los años 1950 llegó la crisis de los años 1960, en los que el equipo de fútbol ganó 2 Copas del Generalísimo y 2 Copas de Ferias. Estos títulos, sin embargo, no lograron compensar la derrota en la final de la Copa de Europa de 1961 ni la crisis social generada por las marchas de Helenio Herrera y Luis Suárez al Inter de Milán, con los que el conjunto italiano ganaría dos Copas de Europa.

Durante los años 1970 continuó el imparable aumento de socios del club: se pasó de los 55 000 a los 80 000. Fueron los años en los que el fútbol español abrió las puertas a los jugadores extranjeros y el club fichó a internacionales como Johan Cruyff, Johan Neeskens, Hugo Sotil, Hansi Krankl y Allan Simonsen. El equipo de fútbol conquistó en esa década una Liga española, 2 Copas del Rey, 1 Copa de campeones de Ferias y 1 Recopa de Europa. En 1978 llegó a la presidencia José Luis Núñez, quien dirigiría al club las siguientes dos décadas.

El equipo catalán fue invitado de Torneo de Triangular del conjunto italiano AC Milan y un conjunto argentino Altos Hornos Zapla esos equipos en dos partidos del Triangular en la Primera Fecha jugó primero contra Altos Hornos Zapla que terminó empatados por 2-2 y quedó eliminado con la Medalla de bronce en la previa final.

Los años 1980 fueron de grandes inversiones en el fichaje de grandes estrellas como Maradona, Schuster o Lineker, pero el equipo de fútbol solo pudo ganar en España una liga, tres Copas del Rey, una Supercopa y dos Copas de la Liga. A nivel europeo se ganaron dos Recopas, pero volvió a caer en una final de la Copa de Europa, la disputada en Sevilla en 1986 frente al Steaua de Bucarest. Tras una grave crisis deportiva y social, en 1988 el club contrató a Johan Cruyff como entrenador, un hecho que marcaría el destino del club durante la siguiente década. Lo más positivo de los años 1980 fue la ampliación del Camp Nou, el incremento de socios, que superó la cifra de los 100 000, la revitalización económica del club y los éxitos de las secciones de baloncesto, balonmano y hockey sobre patines, que conquistaron importantes títulos españoles y europeos.

La década de los años 1990 fue la segunda mejor década de la historia del Fútbol Club Barcelona. Fueron diez años de éxitos para el club en todos los órdenes, tanto en el terreno futbolístico como en las secciones deportivas. El equipo de fútbol, entrenado por Johan Cruyff, y con jugadores como Koeman, Guardiola, Amor, Stoichkov, Romário, Laudrup, Zubizarreta o Bakero ganó cuatro Ligas consecutivas entre 1991 y 1994, una Copa, tres Supercopas de España, una Recopa de Europa y una Supercopa de Europa; y el 20 de mayo de 1992 conquistó el título más preciado del club: la Copa de Europa, en el estadio de Wembley, ante la Sampdoria italiana con anotación de Koeman. Durante estos años, el equipo desempeñó un gran juego y fue conocido popularmente con el nombre de «Dream Team», imitando la terminología que se usó con la Selección de baloncesto de Estados Unidos en los Juegos Olímpicos de Barcelona 1992. Tras la derrota en la final de la Copa de Europa de 1994 frente al A. C. Milan por 4-0 en el Estadio Olímpico de Atenas, se dio por cerrada la era del «Dream Team».

Cruyff empezó a trabajar entonces en una nueva generación de canteranos. La llamada «Quinta del Mini» necesitaba tiempo para acoplarse, y Barcelona no obtuvo títulos en la temporada 1994-95. El equipo contrató a Luís Figo en 1995 para reemplazar a Michael Laudrup, quien se incorporó al Real Madrid. Sin embargo, la situación deportiva del equipo se deterioró hasta el punto de una profunda división social entre partidarios del entrenador, Johan Cruyff y partidarios del presidente, José Luis Núñez. Finalmente, Cruyff fue despedido a falta de dos jornadas para terminar la temporada 1995-96, y Núñez limpió la plantilla de cualquier rastro del neerlandés. Esto creó una gran crisis social en el club, que no desapareció pese a los títulos conseguidos por Bobby Robson y Louis van Gaal, y acabó desembocando en la dimisión de José Luis Núñez en el año 2000.

Los años 1990 fueron también una gran década para las demás secciones profesionales. El equipo de baloncesto se consolidó en la élite del baloncesto español y europeo, pese a que no consiguió ganar la Copa de Europa, cuya final disputó en cuatro ocasiones en esa década. El equipo de balonmano se convirtió en el mejor equipo de balonmano del mundo: ganó todos los títulos, entre los que destacan seis Copas de Europa.

Los años 2000 pueden dividirse claramente en dos etapas. Tras la dimisión de Núñez en mayo de 2000, fue elegido presidente Joan Gaspart. El club invirtió 180 millones de euros en fichajes de futbolistas tales como Marc Overmars, Javier Saviola y Juan Román Riquelme. Pese a ello, los tres años de Gaspart como presidente se saldaron sin títulos futbolísticos, y con una frecuente rotación de entrenadores. Los únicos éxitos deportivos los aportaron las secciones, especialmente el equipo de baloncesto que en 2003 consiguió ganar la Euroliga.

Tras la dimisión de Gaspart llegó a la presidencia Joan Laporta, que afrontó una profunda renovación deportiva, económica y social.

El club fichó a jugadores como Ronaldinho, Eto'o, Rafael Márquez y Deco; y Lionel Messi quien debutó en partido oficial el 16 de octubre de 2004. El nuevo entrenador pasó a ser Frank Rijkaard. El equipo cortó la racha de seis años sin ganar títulos al proclamarse campeón de la Liga 2004-05; y además, consiguió ganar dos Ligas españolas consecutivas al repetir éxito en 2006 y la segunda Liga de Campeones, y la masa social del club superó por primera vez en la historia la cifra de los 140 000 socios.

Esta etapa terminó al final de la temporada 2007-2008, tras dos años sin títulos, con la destitución del entonces entrenador Frank Rijkaard (30 de junio de 2008) y la presentación de una moción de censura contra Joan Laporta y su junta directiva (9 de mayo de 2008).

Con la llegada de Pep Guardiola al mando del equipo, el Barcelona pasó a la historia en la tras conseguir el «triplete» conquistando la Liga, la Copa y la Liga de Campeones. Fue gracias al triunfo en la final de Roma ante el Manchester United por 2-0 que el «Pep Team» consiguió el triplete, convirtiéndose en el primer y único equipo español en haber logrado tal hazaña, y pasando al selecto círculo de clubes europeos que lo han logrado antes (Celtic, Ajax, PSV y Manchester United). Además el Barcelona se proclamó campeón de Europa contando entre sus filas con siete canteranos titulares en la final. El equipo de Pep Guardiola, además de alzarse con los tres principales títulos, logró superar las hazañas del «Dream Team» en cuanto a cifras conseguidas en Liga, batiendo varios récords de goles, partidos ganados como visitante, etc.

Al inicio de la , el equipo ganó la Supercopa de España al imponerse al Athletic Club en ambos partidos. También logró la Supercopa de Europa al vencer al Shakhtar Donetsk por 1-0. El equipo dirigido por Josep Guardiola, tras conquistar la Copa Mundial de Clubes ante Estudiantes de La Plata por 2-1, pasó definitivamente a la historia del fútbol mundial, al conseguir los seis títulos oficiales en un mismo año, una hazaña que no había sido lograda nunca antes por ningún otro club. El Barcelona de Guardiola consiguió en mayo de 2010 su segunda Liga consecutiva, con un total de 99 puntos, cifra que ningún otro club había alcanzado antes.
El 11 de julio de 2010, la selección española ganó la final de la Copa Mundial alineando a siete jugadores del Barcelona en el equipo titular, seis de ellos formados en La Masía.

El 5 de febrero de 2011 el equipo logró un nuevo récord al llegar a conseguir su decimosexto triunfo consecutivo en Liga, frente al Atlético de Madrid, llegando a superar el del Real Madrid que llegó a tener quince victorias seguidas durante la temporada 1960-61. El 2 de marzo de 2011, el Barcelona llega a conseguir otro récord histórico al llegar a vencer al Valencia, esa victoria conseguida le permitiría alcanzar los veinte partidos invictos fuera de casa llegando a superar el récord del equipo de la Real Sociedad el cual había alcanzado en la temporada 1978-79, diecinueve encuentros sin perder.

La finalizaría con el Barcelona como campeón de la Liga por tercer año consecutivo y por vigésimo primera ocasión, y campeón de la Liga de Campeones de la UEFA por cuarta vez.

Tras la consecución de la Supercopa de España, Supercopa de Europa, Mundial de Clubes y Copa del Rey en la temporada 2011-12, se cerraba la era Guardiola con catorce títulos ganados de dieciocho posibles.

La campaña 2012-13 comienza con cambio en el banquillo, el de Josep Guardiola por Tito Vilanova. El equipo mantiene su nivel y logra la mejor primera vuelta de la historia (18 victorias y 1 empate), que se ve empañada por la recaída de Tito Vilanova debido a cáncer a la glándula parótida en diciembre por lo que debe operarse en Nueva York,solo hasta el mes de abril logra reincorporarse al banquillo. La temporada continua con prometedores resultados, pero no logran pasar de semifinales en la Copa del Rey y en la Champions, aunque como se preveía durante gran parte del campeonato se proclaman campeones de Liga, igualando el récord histórico de 100 puntos y 115 goles. Tras una temporada en el banquillo, el 19 de julio de 2013 Vilanova decide abandonar el club debido a su cáncer el cual le había aquejado varios meses atrás. En su lugar se contrató a Gerardo Martino.

El 23 de enero de 2014, Sandro Rosell renuncia a su cargo de presidente debido a la querella por presunta apropiación indebida a raíz del traspaso de Neymar. Le sustituye Josep Maria Bartomeu hasta acabar el mandato, en 2016.

Tras eso el 20 de febrero, el club es imputado por un posible fraude de 9,1 millones a Hacienda en el traspaso de Neymar. El 24 de febrero, la entidad azulgrana ingresa a Hacienda 13 millones de euros, pero sigue insistiendo en su inocencia. Finalmente el 3 de junio, Hacienda concluye que el Barça ha cometido un delito mayor de 9 millones de euros. La Fiscalía de la Audiencia Nacional tiene intención de llevar al Barça y a Sandro Rosell a juicio.

El 2 de abril de 2014, la FIFA prohíbe al club fichar jugadores hasta junio de 2015, por infracciones relativas a la transferencia internacional y la inscripción de jugadores menores de 18 años. Esta se debería a infracciones graves en 10 fichajes realizados entre 2009 y 2013, infringiendo el art. 19 del Reglamento sobre el Estatuto y la Transferencia de Jugadores. El Barça recurrió y la sanción quedó en suspenso, pero posteriormente acabaría siendo confirmada.

Durante la primera mitad de la temporada, el equipo dirigido por Martino mostró unos buenos números, pero al final no lograría conseguir ningún título aparte de la Supercopa de España: El Barcelona fue eliminado por el Club Atlético de Madrid en cuartos de final de la Liga de Campeones, y unos días después, cayó en la final de la Copa del Rey frente al Real Madrid. El 17 de mayo de 2014, tras perder la Liga en la última jornada, el Tata anuncia su marcha. Dos días después, Luis Enrique asume el puesto.

El Barcelona de Luis Enrique comenzó la Liga obteniendo 7 victorias y un empate (sin conceder un solo gol) en las 8 primeras jornadas, pero perdió el liderato tras dos derrotas consecutivas ante Real Madrid y Celta de Vigo, terminando la primera vuelta en segundo lugar, con un punto de desventaja sobre el Real Madrid. Los métodos de Luis Enrique, con frecuentes rotaciones para dosificar a sus jugadores, lo enfrentaron con parte de la plantilla, más notoriamente con Lionel Messi, e incluso se llegó a plantear el despido del técnico asturiano tras una derrota contra la Real Sociedad. A partir de ese momento, los números y el juego del equipo catalán mejoraron sustancialmente, clasificándose para la final de la Copa del Rey y de la Champions League y recuperando el liderato en la Liga. El 17 de mayo de 2015, el Barcelona se proclama campeón de Liga, y dos semanas después, también gana la Copa del Rey. El Barça terminó la temporada proclamándose campeón de la Liga de Campeones por quinta vez en su historia, y convirtiéndose así en el único equipo que logra dos tripletes.

La temporada 2015-16 comenzó con la conquista del cuarto título del año, la Supercopa de Europa, ganando por 5-4 al Sevilla en la final. En cambio, la Supercopa de España se escapó al caer por un resultado global de 5-1 frente al Athletic Club. Los resultados tampoco fueron los esperados en la Liga de Campeones, donde cayeron derrotados por el Atlético de Madrid por un global 3-2 en cuartos de final. Sin embargo, al final de temporada el Barça pudo obtener el título de Liga y la Copa del Rey, consiguiendo su séptimo doblete nacional.

La temporada 2016-17 fue la última de Luis Enrique en el banquillo azulgrana debido a su renuncia producto del degaste que implica el trabajo, según contó el mismo entrenador. Ganó la Supercopa de España al Sevilla F.C por un global de 5-0. Por Liga de Campeones, después de salir primero de su grupo con una sola derrota en condición de visitante ante el Manchester City, el 8 de marzo de 2017 se produjo una remontada histórica. Después de perder en París ante el París Saint-Germain por un vergonzoso 4-0, el equipo pudo dar vuelta al marcador in extremis (el 6-1 lo marcó a tan sólo 20 segundos del final del partido) en el Camp Nou con un marcador de 6-1 con goles de Suárez, Messi, Neymar y Sergi Roberto más un autogol de Kurzawa. No obstante, perdería en cuartos de final contra la Juventus por un global 3-0. Al final solo pudo revalidar el título por Copa del Rey, ya que pierde La Liga ante el Real Madrid en la última fecha.

El 29 de mayo de 2017 se confirma la llegada de Ernesto Valverde a la dirección técnica del club. Después de una promisoria pre-temporada en Estados Unidos y teniendo que sortear el fichaje de Neymar por el PSG, el inicio de temporada no fue el esperado, ya que perdió la Supercopa de España ante su "archirrival" el Real Madrid por un global de 5-1. No obstante, en La Liga Santander su comienzo fue arrasante al conseguir 7 victorias en las primeras 7 fechas y derrotar con un abultado 3-0 a la Juventus en la Liga de Campeones.El 14 de abril de 2018 supera el récord de la Real Sociedad de 38 partidos consecutivos sin perder en liga (vigente desde hacía 38 años), quedando el récord en 43 partidos tras perder contra el Levante en la penúltima jornada del Liga en que el entrenador reservó a Messi. El 21 del mismo mes después conquista su 30ª Copa del Rey, consiguiendo el doblete, el octavo de su historia. El equipo sumó una nueva decepción en Liga de Campeones al quedar eliminados con la Roma en cuartos de final, por tercer año consecutivo en esta instancia.

La temporada 2018-19 comienza con la obtención de la Supercopa de España 2018 en la que fue la primera final disputada fuera del territorio español. Con este título, Messi se convierte en el jugador más laureado de la competición con ocho títulos, y del club con treinta y tres, superando a Iniesta. En mayo de 2019 el equipo ganó su 26ta Liga, pero perdió la final de Copa y quedó eliminado en semifinales de Liga de Campeones desaprovechando una ventaja de 3-0, perdiendo 4-0 en la vuelta.

El 13 de enero de 2020, después de quedar eliminado en semifinales de la Supercopa de España 2019-20 frente al Atlético Madrid, Valverde fue destituido tras dos temporadas y media en el cargo.

El 13 de enero de 2020, Quique Setién firmó por el club azulgrana hasta junio de 2022. Sin embargo, el 17 de agosto del mismo año fue destituido tras dirigir al equipo en solo 25 encuentros. El club cerró su primera temporada sin títulos desde 2008, siendo eliminados en Copa del Rey por el Athletic Club, quedando segundos en Liga tras el Real Madrid y culminando con la humillante derrota ante el Bayern Munich por 8 a 2 en los cuartos de final de la Champions League.

El 19 de agosto, el jugador que le dio al Barça su primera Champions, el neerlandés Ronald Koeman, es anunciado como nuevo técnico del club catalán. El neerlandés llega después de haber entrenado a los 3 grandes de Países Bajos (Ajax, P.S.V. y Feyenoord), al Valencia y a la selección neerlandesa, entre otros clubes. Para destacar, el ex-jugador llega con 8 títulos como técnico.

El escudo del Fútbol Club Barcelona tiene forma de «olla», dividida en tres cuarteles. En los dos superiores se reproduce la bandera de Barcelona, esto es, la Cruz de San Jorge y la señera catalana. En el cuartel inferior aparece un balón sobre los colores azul y grana del club. En el centro del escudo, en una franja, aparecen las iniciales del club, «F.C.B.».

Tras su fundación el club empleó como escudo propio el de la Ciudad de Barcelona como una forma de expresar su vinculación con la ciudad. Dicho escudo permaneció en uso hasta el año 1910, dos años después de que Hans Gamper salvara el club de la profunda crisis, en un intento de dotar al club de un escudo propio y diferenciado. La entidad convocó un concurso abierto a todos los socios para que enviaran sus propuestas, resultando ganador el diseño de Carles Comamala, jugador del club entre 1903 y 1912.

El escudo ha variado poco desde aquel diseño de 1910 y los cambios introducidos han sido, en gran parte, de carácter estético, con pequeñas modificaciones en el trazo de su perfil. Sin embargo, hubo cambios importantes debido a condicionantes políticos, ya que, durante el franquismo, las siglas «F.C.B.» fueron sustituidas por «C.F.B.», Club de Fútbol Barcelona, en consonancia con la castellanización de la denominación del club. El club no recuperó su denominación en catalán hasta el 8 de noviembre de 1973, tras recibir la autorización pertinente de la Real Federación Española de Fútbol. Además, se tuvieron que reducir el número de barras del cuartel superior a dos y la bandera catalana fue suprimida del escudo. En 1949, aprovechando los actos del quincuagésimo aniversario de la entidad, se volvieron a incorporar las cuatro barras. A finales de 1974 se volvieron a incluir las siglas iniciales, con lo que el escudo volvió al contenido original de 1910.

El diseño moderno del escudo es obra de una adaptación del diseñador Claret Serrahima realizada en 2002, que incluye unas líneas más estilizadas, suprime los puntos que separaban las iniciales del club, abrevia el nombre y reduce el número de puntas. El escudo ha tenido diez versiones desde la fundación del club.
Existen dos versiones sobre los orígenes del escudo del club. La primera versión cuenta que en el año 1900, un año después de la fundación del club, hubo una reunión para decidir el escudo (hasta entonces, el Barcelona había utilizado el escudo de la ciudad). Parece ser que no había acuerdo sobre la forma y el contenido del escudo y en un momento de la reunión el secretario, Luis d'Ossó, visiblemente enfadado, exclamó «esto es una olla», por lo que algunos historiadores creen que este fue el punto de partida para «trazar el boceto de una olla».

El himno oficial del F. C. Barcelona, denominado oficialmente "Cant del Barça" (en castellano, "Canto del Barça") fue creado y estrenado en 1974, con motivo de los actos de celebración del 75 aniversario del club. La letra fue escrita por Jaume Picas y Josep María Espinàs, y la música fue compuesta por Manuel Valls i Gorina. Está íntegramente escrito en lengua catalana, y en la versión oficial está interpretada por la Coral Sant Jordi.

En los últimos años ha sido interpretado por reconocidos cantantes como Joan Manuel Serrat, con motivo de diferentes actos como la conmemoración del centenario del club. Además, la directiva presidida por Joan Laporta ha incentivado que variados intérpretes y conjuntos musicales lo interpretaran en el estadio del Camp Nou, en los prolegómenos de los partidos de fútbol, versionándolo y adaptándolo a los más variados estilos musicales: pop, rock, rap, samba, rumba, entre otros.

Uno de los detalles que caracterizan al himno es la referencia al carácter abierto e integrador del club, que no diferencia la procedencia geográfica de los seguidores; como dice una de las estrofas, «tanto da de dónde venimos, si del sur o del norte, una bandera nos hermana».

Los colores distintivos del F. C. Barcelona son el azul y el grana, de donde procede el sobrenombre de «azulgrana» («blaugrana» en catalán) con el que se conoce a los jugadores y aficionados del club.

Existen diversas teorías sobre las causas que llevaron a los fundadores del club a escoger estos colores, aunque no hay ninguna que esté suficientemente contrastada como para ser considerada válida si bien la más extendida y documentada señala que fue el propio Hans Gamper, fundador del club, quien decidió los colores. De hecho, está comprobado que en el primer partido de fútbol que Gamper disputó en la ciudad de Barcelona antes de la fundación del club, ya vistió estos colores. Se afirma que Gamper escogió estos colores por ser los que identificaban al F. C. Basilea, equipo suizo en el que Gamper había jugado antes de llegar a Barcelona, ya que se había hecho socio del F. C. Basilea en 1896, vistiendo de azulgrana por primera vez tres años antes de fundar el Barça. La teoría que Joan Gamper se inspiró directamente en los colores de su antiguo equipo suizo a la hora de escoger los del Barça es una de las más razonables y fundadas, pero aun así no hay ninguna prueba documental que lo avale y por otro lado debe cohabitar con muchas más. En la reunión fundacional del F. C. Barcelona del 29 de noviembre de 1899, que se efectuó en la sala de armas del Gimnasio Solé, sobre el asunto de la elección de los colores azul y grana, Narciso Masferrer cita: «Se trató extensamente del nombre y colores que adoptaría el club, quedando acordado, como título de la sociedad es de Football Club Barcelona y los colores los azul y grana, que son, sino estamos equivocados, los mismos del F. C. de Basilea, al que ha pertenecido hasta hace poco el ex campeón suizo Hans Gamper, nuestro estimado amigo».

De la combinación de los colores azul y grana proviene el término "azulgrana". Estos colores siempre han estado presentes en la camiseta titular del equipo. Sin embargo, durante los diez primeros años de historia del club los pantalones fueron de color blanco, más tarde negros, y desde la década de 1920, azules. En la temporada 2005-06 el equipo vistió pantalones color grana, algo inédito hasta el momento, debido a motivos comerciales. En la temporada 2011-12 la innovación en la primera equipación es que la camiseta tiene las rayas verticales más finas de la historia.

El equipo dispone de uniforme alternativo o segunda equipación a nivel oficial desde el año 1913, cuando se eligieron el color blanco para la camiseta y el azul para los pantalones. Esta equipación duró más de sesenta años, hasta la temporada 1975-76, en la que entró en escena una camiseta amarilla con una franja azulgrana en diagonal. Fue variando en colores como el amarillo, el azul o el rojo, en las diferentes camisetas con una franja vertical azulgrana en su lado derecho hasta que a partir de la temporada 1998-99 Nike se convirtió en el proveedor eligiendo una amplísima gama de colores desde entonces.

En noviembre de 2012, el club informa que, como parte de su acuerdo con Qatar Sports Investment, la aerolínea Qatar Airways sustituirá a Qatar Foundation como patrocinador principal en la camiseta desde la temporada 2013-14.

El estadio del F. C. Barcelona es el Camp Nou, propiedad del propio club. Inaugurado en 1957, tiene una capacidad de 99.354 espectadores, todos sentados. Es uno de los cuatro estadios de España catalogado como «Estadio Cinco Estrellas» por la UEFA, lo que lo habilita para acoger finales de la Liga de Campeones, Supercopa de Europa y Copa de la UEFA, como ha sucedido en 15 ocasiones. Se encuentra en el barrio de Les Corts de Barcelona, junto a otras instalaciones del club, como el Mini Estadi (estadio del Barcelona B) y el Palau Blaugrana, cancha del equipo de baloncesto. En las instalaciones del Camp Nou se encuentra el Museo del F. C. Barcelona, el museo más visitado de Cataluña.

Al principio de su fundación, el 29 de noviembre de 1899, el F. C. Barcelona, no disponía de estadio propio. Debido a no tener terreno de juego de su propiedad, el F. C. Barcelona tiene que jugar sus partidos como equipo local en terrenos de juego ajenos, desde el primer partido inaugural como club el 8 de diciembre de 1899 hasta que el 14 de marzo de 1909 disputa su primer partido como local en campo propio, en el Campo de la calle Industria. Esta falta de estadio propio, le obliga a jugar en diferentes estadios en los que se tiene que mudar debido a diferentes circunstancias y en los cuales permanece en algunos durante unos años y en otros solamente en algunos partidos. Se pueden contar seis estadios diferentes antes de tener estadio propio. Su primer estadio fue el ex Velódromo de la Bonanova en el año 1899, el segundo estadio fue el Campo del Hotel Casanovas en el año 1900, el tercer estadio fue La Plaza de las Armas en el año 1901, el cuarto estadio fue el de La Carretera de Horta entre los años 1901 y 1905, el quinto estadio fue el de La Calle Muntaner entre los años 1905 y 1909 y el sexto estadio fue el Campo de la Fuxarda, donde se jugaron en el año 1909 solo dos partidos antes de pasar al estadio propio del Campo de la calle Industria
Con anterioridad al Camp Nou, el F. C. Barcelona tuvo dos estadios, también de su propiedad. Entre 1909 y 1922 jugaba en el Camp del Carrer Indústria de Barcelona, vulgarmente llamado La Escopidora, con una capacidad de 6000 espectadores, aunque las cifras de la época no eran muy precisas y el primero en Barcelona, que dispuso de una tribuna de dos pisos, que despertó admiración en aquel tiempo, en la ciudad.
Una de las versiones acerca de la etimología de la palabra «culés» procede del estadio del , pues las gradas permitían ver desde fuera del recinto las posaderas de los aficionados.
Entre 1922 y 1957 disputó sus partidos en el Campo de Les Corts, inaugurado para acoger a 30 000 espectadores, y que llegó a tener una capacidad de 60 000 personas. El Campo de Les Corts, fue el campo donde el F. C. Barcelona el 24 de septiembre de 1926, empezó a jugar como local en campo de hierba, cuando lo había hecho hasta la fecha en campo de tierra.
A expensas de los permisos urbanísticos municipales, estaba prevista la remodelación del estadio que debía de haberse iniciado en el último tercio de 2008 y se preveía su finalización para el año 2012. El arquitecto asignado ha sido el británico Norman Foster, quien resultó vencedor tras un concurso en el que sólo diez proyectos llegaron a ser finalistas. "sir" Norman Foster dijo haberse inspirado en Gaudí para crear la nueva piel que envolverá al estadio. La presidencia del F. C. Barcelona y el Colegio de Arquitectos de Cataluña fueron el jurado encargado de escoger el proyecto vencedor.

La remodelación se basa en poner al día un estadio que cuenta con más de 50 años y crear una cubierta para proteger a los espectadores de las inclemencias meteorológicas. Los requisitos fundamentales eran: ocasionar las mínimas molestias a los socios, que la remodelación fuera compatible con la competición deportiva y que se ciñera a un presupuesto determinado. Así como la creación de un diseño atractivo, moderno y funcional.

En mayo de 2010 la prensa escrita informa que el proyecto de remodelación del Camp Nou por Norman Foster, que presentó en septiembre de 2007, puede quedar en el olvido y ser reconsiderado por otro proyecto de remodelación

En el último día de campaña electoral, el 11 de junio de 2010, dos días antes de ser elegido Sandro Rosell como presidente del F. C. Barcelona, dos miembros de su equipo, Jordi Moix y Jordi Cardoner, presentaron el proyecto Espai Barça, en castellano, Espacio Barça, dentro del cual entre otras obras queda englobado el nuevo modelo de reforma del Camp Nou, según aparece en una noticia en la web del club. El proyecto Espai Barça para su ejecución dependerá de ser aprobado en la asamblea de compromisarios, ya que según Rosell: «El socio es el dueño del club y por tanto la asamblea determinará si esta propuesta sigue adelante».
También la prensa escrita informa aproximadamente en las mismas fechas que no se llevará a cabo el proyecto Foster y sobre el proyecto Espai Barça se informa de una valoración de coste global de 150 millones de euros. El Camp Nou se remodelará interiormente, mejorando la accesibilidad, cubriendo las graderías y ampliándolo con más asientos. Esta reforma destinará una partida de 30 millones de euros del coste del proyecto. Las obras en caso de que se aprobase el proyecto, se prolongarían entre seis y ocho años.


A lo largo de su historia, la entidad ha visto como su denominación variaba por diversas circunstancias hasta la actual de Fútbol Club Barcelona, vigente desde 1973. El club se fundó bajo el nombre de Foot-ball Club Barcelona, y siendo oficializado formalmente en 1903.

A continuación se listan las distintas denominaciones de las que ha dispuesto el club a lo largo de su historia:


El Fútbol Club Barcelona es una de las instituciones más laureadas a nivel mundial, totalizando en sus más de ciento diez años de historia un total de ciento veintiocho títulos oficiales regionales, nacionales e internacionales, lo que le sitúa como el —«»—. Entre ellos destacan por importancia, cinco Ligas de Campeones, cinco Supercopas de la UEFA, cuatro Recopas de Europa, tres Copas de Ferias, dos Copas Latinas, y tres Copas Mundiales de Clubes, sumando veintidós títulos internacionales oficiales convirtiéndolo en el . En competiciones nacionales posee el récord de trofeos con setenta y cuatro, desglosados entre veintiséis Ligas, treinta Copas de España, trece Supercopas de España, dos Copas de la Liga y tres Copas Eva Duarte. En cuanto a títulos de su comunidad autónoma cuenta con veintitrés Campeonatos de Cataluña, siete Copas de Catalula y dos Supercopas de Cataluña.
En 2009 dirigido por Josep Guardiola, el club conquistó todas las competiciones que disputó: la Liga, la Copa, la Supercopa de España, la Liga de Campeones, la Supercopa de Europa y la Copa Mundial de Clubes, siendo el primer y único equipo español y europeo desde entonces en lograr un «sextuplete», al ganar seis títulos oficiales en un mismo año. En 2015, bajo el mando de Luis Enrique, ganó el «triplete» por segunda vez en su historia, convirtiéndose en el primer club europeo en lograr tal hazaña.

Fue considerado por la Federación Internacional de Historia y Estadística de Fútbol (IFFHS) como mejor club del mundo en los años 1997, 2009, 2011, 2012 y 2015, siendo la institución que más veces ha encabezado dicha clasificación.

Posee cinco trofeos de la Liga en propiedad por haber ganado la competición tres veces consecutivas o bien por cinco alternas, y seis trofeos de Copa del Rey, a su vez totaliza la cifra más alta de galardones en propiedad de clubes de fútbol en España con un total de once.
Torneos nacionales (74) 
Torneos internacionales (22) 
<nowiki>*</nowiki> 

Desde su fundación en hasta la , la entidad ha logrado una trayectoria de ciento dieciocho temporadas, ochenta y nueve de ellas en el ámbito profesional. Durante de ese lapso ha disputado dieciséis competiciones oficiales regionales, nacionales e internacionales organizadas por la FIFA, la UEFA, la RFEF y la FCF.

El Fútbol Club Barcelona es uno de los únicos tres clubes que ha disputado siempre la Primera División — en España y máxima categoría del sistema de ligas— desde su fundación en la temporada 1928-29 sumando un total de ochenta y nueve presencias. Ocupa el entre los sesenta y tres participantes históricos además de ser el segundo más laureado con veintiséis y el primer campeón que tuvo la competición. Su peor posición registrada ocurrió en la temporada 1941-42 cuando finalizó en duodécimo puesto. En cuanto al resto de competiciones oficiales nacionales suma un total de ciento treinta y cuatro apariciones, en la Copa del Rey —competición en la cual domina el palmarés—, cuenta con ciento tres presencias sobre ciento quince totales, y únicamente doce ausencias.

En competiciones internacionales, participa en la Copa de Europa —actual Liga de Campeones—, en la cual ha disputado un total de treinta temporadas con ausencia en treinta y cinco ediciones; es, por tanto, el quinto club con más presencias y el . En ellas sumó un total de cinco títulos que le sitúan como el cuarto equipo más exitoso de la competición entre sus 520 participantes históricos. Totaliza cincuenta y una apariciones en el resto de competiciones oficiales internacionales para doce ausencias en temporadas de competiciones de UEFA. Entre las más relevantes, once en la Copa de la UEFA / Liga Europa y trece en la Recopa de la UEFA.

Más de 1000 futbolistas han vestido la camiseta del primer equipo del Fútbol Club Barcelona a lo largo de sus 119 años de historia. Los jugadores de origen extranjero (aunque algunos, nacionalizados españoles) han tenido siempre un gran peso en la historia del club, y han marcado las épocas más brillantes del conjunto catalán. Fundado por un grupo de extranjeros afincados en Barcelona, inicialmente el equipo estuvo formado por jugadores de origen mayoritariamente inglés, suizo y alemán. La mayoría de historiadores consideran que el húngaro Ladislao Kubala fue, en los años 1950, la primera gran figura de talla internacional que militó en el conjunto barcelonista. Pero fue a partir de los años 1970, cuando el fútbol español regularizó la participación de jugadores extranjeros, cuando el club empezó a fichar a grandes figuras internacionales. El F. C. Barcelona ha contado desde entonces con diversos jugadores que, militando en el club azulgrana, han conquistado los más prestigiosos trofeos individuales del fútbol mundial.

Cinco jugadores del club fueron galardonados con el premio de Jugador Mundial de la FIFA que los acreditaba como los mejores futbolistas del mundo: Romário, Ronaldo, Rivaldo, Ronaldinho en dos ocasiones consecutivas, y Lionel Messi; y seis fueron premiados con el Balón de Oro que los acreditaba como los mejores jugadores del fútbol europeo: Luis Suárez, Johan Cruyff en tres ocasiones, Hristo Stoichkov, Rivaldo, Ronaldinho y Messi que hizo historia en el club al ser el primer canterano en conseguirlo. Hasta en veinte ocasiones, un jugador del club ha obtenido el Balón de Oro, Plata o Bronce, además de otras cuatro ocasiones en las que un jugador ha obtenido uno de estos galardones habiendo disputado la primera parte de ese año en el club azulgrana; Ronaldo en 1997 y siendo jugador del Inter de Milán, y Luís Figo en 2000 perteneciendo al Real Madrid. Además, el club ha contado con jugadores poseedores de otras grandes distinciones internacionales como Andrés Iniesta, Xavi Hernández, Michael Laudrup, Samuel Eto'o, Allan Simonsen, Hansi Krankl, Diego Maradona, Gary Lineker y Zlatan Ibrahimović.

Tras la fusión del Jugador Mundial de la FIFA con el Balón de Oro se creó el FIFA Balón de Oro, donde el club ha contado con ocho nominaciones (Andrés Iniesta dos veces, Xavi Hernández dos veces y Lionel Messi cuatro veces) y tres premiaciones (todas a Lionel Messi, que junto con su trofeo anterior, es el jugador con más balones de oro en la historia).

Los futbolistas que más partidos oficiales han disputado en el F. C. Barcelona son: Xavi Hernández, Lionel Messi, Andrés Iniesta, Carles Puyol y Migueli. El catalán Xavi Hernández con en diecisiete temporadas, es el integrante barcelonista con la mayor cantidad de años perteneciendo a la institución.

Los jugadores que más goles han marcado en competiciones oficiales son: Lionel Messi, César Rodríguez, Ladislao Kubala, José Samitier y Luis Suárez. Destaca el argentino Lionel Messi como máximo goleador histórico con seiscientas sesenta y seis anotaciones, y el futbolista que más títulos ha cosechado en su estancia en la entidad con treinta y cuatro.

El F. C. Barcelona ha sido históricamente, junto al Real Madrid, el club que ha aportado con los mejores futbolistas de España, y uno de los conjuntos que más ha contribuido a nutrir a la . El jugador del F. C. Barcelona con más partidos internacionales con la selección española fue el centrocampista Xavi Hernández, quien disputó , y junto a Lionel Messi, Andrés Iniesta, Sergio Busquets, Gerard Piqué y Carles Puyol pertenecen al selectivo de futbolistas barcelonistas del «» por haber disputado con cien o más partidos con su selección nacional absoluta.

<br>

<br>

El F. C. Barcelona ha tenido, contando a su actual técnico, un total de 57 entrenadores de fútbol a lo largo de su historia. El primer entrenador que tuvo el club fue el inglés Billy Lambe, que dirigió al equipo de enero a septiembre de 1912, ejerciendo tanto de jugador como de entrenador. Anteriormente, el club no tenía entrenador. Era práctica habitual en los equipos de fútbol que, hasta principios de los años 1910, la plantilla fuese confeccionada por el presidente y la junta directiva, que decidían los fichajes, los traspasos y, en la mayoría de los casos, las alineaciones de los partidos. Los entrenamientos, que en aquella época eran pocos pues el fútbol no era profesional, solían autogestionarlos los propios jugadores.

Como Billy Lambe, la mayoría de entrenadores que ha tenido el F. C. Barcelona han sido extranjeros. De los 57 entrenadores del club, tan sólo 26 han sido españoles (16 de ellos catalanes). En la mayoría de los casos, los entrenadores españoles han sido exjugadores del club que accedieron al cargo tras el cese del entrenador titular. Tan sólo se han dado siete casos de entrenadores españoles que no hubiesen vestido previamente la camiseta del club como jugador: Enric Rabassa, Enrique Orizaola, Vicente Sasot, Laureano Ruiz, Luis Aragonés, Lorenzo Serra Ferrer y Quique Setién.

Las nacionalidades principales de los entrenadores no españoles han sido la inglesa (10), argentina (4), neerlandesa (4) y húngara (3). El club también ha contado con dos entrenadores alemanes, dos serbios, un austriaco, un eslovaco, un francés, un irlandés, un italiano y un uruguayo. Solo ha habido cinco entrenadores no europeos en la historia del club: cuatro argentinos (Helenio Herrera, Roque Olsen, César Luis Menotti y Gerardo Martino) y un uruguayo (Enrique Fernández).

El técnico que más años seguidos se mantuvo en el cargo fue el inglés Jack Greenwell, que dirigió al equipo en dos períodos distintos, entre 1913 y 1923, y entre 1931 y 1933. El segundo técnico más longevo en el cargo fue el neerlandés Johan Cruyff, que estuvo ocho años consecutivos, entre 1988 y 1996. Cruyff es, además, el entrenador que ha dirigido al club en más partidos con cuatrocientos treinta, el que más victorias cosechó con doscientas cincuenta, y con sus once trofeos obtenidos el segundo que más títulos ha conseguido después de Josep Guardiola; destaca también como el entrenador con la mayor racha ganadora de títulos de Liga en la historia de la entidad con cuatro entre 1991 y 1994, y el primero en ganar la Copa de Europa. El tercer entrenador en número de partidos dirigidos es el neerlandés Rinus Michels, que dirigió al equipo en trescientos sesenta y un encuentros repartidos en dos etapas: entre 1971 y 1975, y entre 1976 y 1978.

Otro destacado entrenador fue el neerlandés Frank Rijkaard, quien se desempeñó al frente del cargo desde junio de 2003 a mayo de 2008. Con un estilo ofensivo desempeñó una destacada labor, conquistando dos Ligas, dos Supercopas de España y la Liga de Campeones en la temporada 2005-2006, la segunda en toda la historia del club. El 8 de mayo de 2008, la directiva del equipo anunció la destitución del entrenador Frank Rijkaard una vez concluida la temporada, el 30 de junio, un año antes de la finalización de su contrato, siendo sustituido por el entrenador del equipo filial Josep Guardiola.

El día 5 de junio de 2008, Josep Guardiola fue presentado oficialmente como entrenador del F. C. Barcelona. Después de que el 8 de mayo de 2008 la junta directiva del club catalán anunciara que Guardiola tomaría las riendas del conjunto a partir del 30 de junio, "Pep" formalizó el acuerdo en las instalaciones del Barça el 5 de junio de 2008. Guardiola hizo historia al conseguir el «sextuplete» conquistando Copa, Liga, Liga de Campeones, Supercopa de España, Supercopa de Europa y Mundial de Clubes en su año de debut en el banquillo del equipo. También consiguió otros logros destacados, como encadenar tres ligas consecutivas (igualando a Johan Cruyff) y ganar la cuarta Copa de Europa. Más aún, con catorce títulos es el entrenador más laureado de la historia del club.

El 27 de abril de 2012 se anuncia que Tito Vilanova sería el sustituto de Guardiola al frente del primer equipo la próxima temporada, este logra mantener el ritmo logrando grandes resultados especialmente en Liga, llegando a conseguir la mejor primera vuelta de la historia. Pero en diciembre sufre una recaída de su cáncer a la glándula parótida, separándolo del banquillo por 4 meses. Ya en su regreso, la temporada continua por buen camino, pero termina siendo eliminado de la Copa del Rey y la Liga de Campeones en semifinales. En cambio, se proclaman campeones de Liga con 100 puntos y 115 goles, todo un récord. El 19 de julio de 2013 se anuncia la propia dimisión de Vilanova, tras nuevamente recaer de su enfermedad. Cuatro días después, se confirmó que Gerardo Martino sería el nuevo entrenador, firmando por 2 temporadas. El 25 de abril de 2014 el exentrenador del club Tito Vilanova, fallece debido a su cancér en la ciudad de Barcelona. Tras apenas una temporada en el club, Martino abandona la entidad luego de conseguir solo un título, la Supercopa de España.

El 19 de mayo de 2014 asume como nuevo entrenador Luis Enrique con un contrato hasta el año 2016, que a su finalización se prorrogó una temporada más, habiendo ganado 9 títulos incluyendo un triplete, un quintuplete y un doblete, para luego ser relevado por Ernesto Valverde, quien hasta su destitución en enero de 2020 y su sustitución por Quique Setién había conseguido dos Ligas, una Copa del Rey y una Supercopa de España. Setién obtuvo 16 victorias, cuatro empates y cinco derrotas en los siete meses en los que dirigió al equipo, siendo sustituido el 19 de agosto de 2020 por el holandés Ronald Koeman.

El Fútbol Club Barcelona ha tenido, contando al actual, 40 presidentes a lo largo de su historia. Además, el club ha sido dirigido por un comité de empleados, que gestionó el club durante la guerra civil, y varias comisiones gestoras. En la asamblea de constitución del club se eligió como primer presidente al suizo Walter Wild, que había ayudado estrechamente a su amigo y compatriota Hans Gamper en el proceso fundacional. Gamper, sin embargo, fue posteriormente presidente hasta en cinco etapas diferentes. El presidente con un mandato de mayor duración en la historia del club fue José Luis Núñez, que ostentó el cargo durante 22 años, entre 1978 y 2000.

La presidencia del F. C. Barcelona es elegida por sus socios, mediante comicios por sufragio universal, celebrados cada cuatro temporadas, en la que tienen derecho a elegir y a ser elegidos todos los socios y socias del club mayores de 18 años, con un año de antigüedad como socios del club.

El presidente escoge a los miembros de su Junta Directiva, que son ratificados por una asamblea de socios compromisarios: 300 socios mayores de 18 años elegidos por sorteo y que, durante un período de dos años, participan en las asambleas anuales de socios, con voz y voto, en representación de todos los socios del club.

El último presidente del F. C. Barcelona fue Josep Maria Bartomeu, un empresario que fue vicepresidente de la junta directiva de Sandro Rosell y que accedió a la presidencia tras la renuncia de Rosell en enero de 2014. La junta directiva actual fue elegida tras unas elecciones celebradas el 13 de junio de 2010, a las que se presentaron otros 3 candidatos más.

El 26 de marzo de 2010, Johan Cruyff fue designado por unanimidad de la Junta Directiva del F. C. Barcelona como Presidente de Honor del Club, tratándose este título como "rango protocolario", nombramiento que está de acuerdo, según la noticia de la web del club, con el artículo 16 de los Estatutos del F. C. Barcelona. El 2 de julio de 2010, Johan Cruyff devuelve la insignia que le distinguía como presidente de honor del F. C. Barcelona. Según Sandro Rosell la figura de Presidente de Honor, es alegal, ya que no existe y se deberá esperar a la Asamblea de Socios Compromisarios para someter a debate la creación de un cambio estatutario que contemple esta figura y la decisión, si es aprobada, de quien debe ser escogido para el cargo.

El fútbol, motivo de la fundación del club, sigue siendo el deporte principal del F. C. Barcelona, y la actividad que acapara más del setenta y cinco por ciento del presupuesto del club.

El primer equipo de fútbol juega en la Primera División de España, y es uno de los tres clubes que siempre han competido en esta categoría desde la primera edición de La Liga, en 1929. Los otros dos clubes que ostentan este honor son el Athletic Club y el Real Madrid. El Barcelona ha ganado el campeonato de Liga en un total de veintiséis ocasiones, la última en la temporada 2018-19.

En la temporada 2009-10, el equipo finalizó en Liga en el primer puesto, lo que le permitió clasificarse directamente para jugar la próxima temporada en la Liga de Campeones de la UEFA. El Barcelona ha conseguido imponerse en este trofeo en cuatro ocasiones, en los años 1992, 2006, 2009 y 2011. Además, el Barça es el único club en la historia en conseguir el triplete en 2 ocasiones: ganar la Liga, Copa del Rey y Copa de Europa en un mismo año.
El Fútbol Club Barcelona ostenta el récord de ser el único equipo de fútbol europeo que ha participado de forma ininterrumpida en las competiciones continentales desde su creación en 1955. También es el equipo con más títulos en la extinta Recopa de Europa con cuatro títulos y el que posee más triunfos en la Copa de España en sus distintas denominaciones con treinta conquistas.

El club cuenta con una importante cantera de jugadores, desde la categoría de alevines. El filial del primer equipo de fútbol es el Barcelona "B", que milita en la Segunda División de España.

Además de su sección principal, la de fútbol, el club cuenta con otras cuatro secciones profesionales: las de baloncesto, balonmano, hockey sobre patines y fútbol sala. Entre las cinco secciones profesionales, el F. C. Barcelona suma treinta y nueve Ligas de Campeones (o Euroligas en el caso del baloncesto) lo cual le convierte en el segundo club polideportivo más laureado de Europa y, junto a Dinamo de Moscú y CSKA de Moscú (este con 7), el único que ha conseguido el máximo título continental en cinco deportes diferentes. 

Uno de los datos relevantes del club fue la consecución de la Liga de Campeones consecutivamente durante diecisiete años, desde la temporada 1995-96 hasta la 2011-12 con alguna de sus secciones profesionales, además ha ganado un «triplete» europeo con las secciones de Fútbol, Balonmano y Hockey Patines en la temporada 2014-15 y seis «dobletes» europeos con las secciones de Balonmano y Hockey Patines en las temporadas 1996-97, 1999-2000 y 2004-05 con las secciones de Baloncesto y Hockey Patines en la temporada 2009-10, con las secciones de Fútbol y Balonmano en la temporada 2010-11 y con las secciones de Fútbol Sala y Hockey Patines en la temporada 2013-14. Destaca también el hecho que desde la temporada 1988-89 que se inicia con la victoria del equipo de fútbol, en la final de la Recopa de Europa en Berna contra la Sampdoria hasta la 2011-12, cada temporada durante 24 años, alguno de los deportes del club levantó algún título europeo. En cuanto a temporadas se refiere, en el cómputo global de títulos conseguidos por todas las secciones sin contar los regionales, la temporada 2011-12 fue la más exitosa del club, ya que logró la conquista de diecisiete campeonatos (récord histórico para el club), seguido de la temporada 2010-11 con 14 y la 2009-10 con 13.

Además de estas cinco secciones profesionales, el club cuenta con secciones amateurs en otras disciplinas deportivas: hockey sobre hierba masculino y femenino, atletismo masculino y femenino, patinaje masculino y femenino, hockey sobre hielo, voleibol masculino y femenino, rugby, fútbol playa, fútbol indoor, fútbol femenino y baloncesto en silla de ruedas.

El Barcelona organiza, desde 1966, un torneo de fútbol amistoso anual, el Trofeo Joan Gamper, que se disputa habitualmente en el mes de agosto como parte de la pretemporada.

El Fútbol Club Barcelona es una institución polideportiva que, además de equipos de fútbol, cuenta con equipos en doce disciplinas deportivas más. Estas disciplinas se estructuran como secciones deportivas dentro del club. El F. C. Barcelona distingue, desde un punto de vista estructural, entre las secciones masculinas profesionales, las masculinas no profesionales y las secciones femeninas.
Desde Josep Samitier a Neymar son (11 de oro, 23 de plata y 28 de bronce) que han conseguido deportistas de la entidad con contrato en vigor.

Las secciones profesionales son cuatro, las de baloncesto, balonmano, hockey sobre patines y fútbol sala. Son las cuatro secciones más profesionalizadas y prestigiosas, que participan en las competiciones de mayor categoría de sus respectivas disciplinas en España. Además, los equipos masculinos de estas cuatro secciones forman parte de la élite de los mejores clubes de Europa, por la cantidad de títulos de carácter continental que han conseguido. Entre estas cuatro secciones el F. C. Barcelona suma 32 Copas de Europa. Los equipos de estas tres secciones tienen su sede y disputan sus encuentros en el Palau Blaugrana, pabellón polideportivo anexo al Camp Nou con capacidad para 7585 espectadores.

El F. C. Barcelona de baloncesto (sección creada en 1926) es el segundo club de España en baloncesto en número de títulos conseguidos, y uno de los de mayor prestigio de Europa. La sección de baloncesto vivió sus mejores años en las décadas de 1980 y 1990, en las que consiguió diversos títulos españoles y europeos. Su título más preciado, sin embargo, la Euroliga (o Copa de Europa), no lo consiguió hasta la temporada 2002-2003, cuando ganó la fase final celebrada en la propia ciudad de Barcelona. El secretario técnico actual de la sección de baloncesto es Joan Creus, y el entrenador del equipo es Sito Alonso.

La sección de balonmano fue fundada en 1942, y es la que más títulos ha dado al club. El equipo de balonmano del Barça es el conjunto que acumula más títulos de España y Europa, con 8 Copas de Europa. En 2011 volvió a coronarse como el mejor equipo europeo al conquistar su octava Copa de Europa. El secretario técnico actual de la sección de balonmano es Enric Masip, y el entrenador del equipo es Xavier Pascual.

La sección de hockey sobre Patines del F. C. Barcelona también fue creada en 1942, y está considerada uno de los mejores clubs del mundo de hockey sobre patines. Es el más laureado de Europa, acumulando 20 Copas de Europa. El secretario técnico actual de la sección de hockey es Quim Paüls.

La sección de fútbol sala es considerada profesional desde el regreso del equipo a la División de Honor de la LNFS. En la temporada 2010/2011 obtuvo su primer título profesional: la Copa de España.

Además de las secciones masculinas profesionales, el club cuenta con secciones amateurs en otras ocho disciplinas deportivas: atletismo, patinaje, hockey sobre hielo, voleibol, hockey hierba, baloncesto en silla de ruedas, rugby y fútbol playa. El director de las secciones no profesionales del club es el exjugador argentino de hockey sobre patines Gaby Cairo.

La sección de atletismo del club tiene el honor de ser, tras la de fútbol, la primera sección creada en la historia del club. Fue formada oficialmente en 1915, aunque en el año 1911 con motivo del cierre de temporada futbolística se celebra un festival atlético y las crónicas cuentan que en el club ya se practicaba el atletismo desde el año 1900. El equipo masculino de atletismo del F. C. Barcelona ha sido considerado siempre uno de los mejores de España como atestiguan los numerosos títulos conseguidos, tanto en competiciones nacionales como internacionales. En su palmarés destacan más de 30 campeonatos de España por clubs en diferentes modalidades. El equipo ha contado históricamente con algunos de los mejores atletas de España, como los medallistas olímpicos José Manuel Abascal y Javier García Chico, y campeones de España como Antonio Corgos, Javier Moracho, Colomán Trabado o Gregorio Rojo que después siguió haciendo su labor como entrenador en esta sección y llegó a ser considerado uno de los mejores entrenadores de atletismo de España.

La sección rugby del F. C. Barcelona es una de las más antiguas del club: fue creada en 1924. Tiene su campo de juego en la Ciudad deportiva del F. C. Barcelona. El equipo sénior A empezó a competir en la temporada 2006/2007 en la División de Honor española, tras la llegada a un acuerdo y fusión con el USAP Barcelona, ocupando la plaza que este equipo tenía en la máxima categoría del rugby español, volviendo así, tras muchos años a la categoría. Se trata de uno de los clubes más laureados de España gracias a los títulos conquistados entre los años 1940 y 1960. Entre los títulos ganados por la sección de rugby destacan 16 campeonatos de España, 2 Ligas españolas y 1 Supercopa de España.

Otras secciones no profesionales del club son:

El F. C. Barcelona ha estado potenciando en los últimos años sus secciones femeninas, dada la creciente participación de las mujeres en el deporte, y la profesionalización de las estructuras competitivas. Las secciones femeninas del club son las de fútbol, baloncesto, atletismo, patinaje artístico, voleibol y hockey hierba.

La sección de fútbol femenino milita en la máxima categoría, la Primera División. En su palmarés figuran cinco Ligas, cuatro Copas de la Reina y siete Copas de Cataluña.

El primer partido de fútbol femenino que jugó un equipo del F.C. Barcelona fue el día de Navidad de 1970 con motivo de un festival benéfico. El encuentro enfrentó en el Camp Nou a las jugadoras azulgranas, entrenadas por Antoni Ramallets, contra la UE Centelles. Posteriormente participó en el primer campeonato oficioso de Cataluña de fútbol femenino, celebrado en la temporada 1971-72.

Durante los años 80 y los años 90 la entidad participó bajo el nombre de Club Femení Barcelona, y utilizó los colores, distintivos e instalaciones del Club. Durante aquellos años obtuvo sus mayores éxitos en la competición de la Copa de la Reina al alcanzar un subcampeonato (1991) y un campeonato (1994).

En 2001 el FC Barcelona incorporó definitivamente el fútbol femenino como sección oficial y continuó compitiendo en la segunda categoría, la Primera Nacional de Fútbol Femenino. En la temporada 2003-04 superó la promoción de ascenso y ascendió a la Superliga, pero aquello no se tradujo en su consolidación. En la temporada 2006-07, el equipo perdió la categoría e incluso se llegaba a plantear su desaparición.

Con la llegada al banquillo de Xavi Llorens en la temporada 2007-08 el equipo retornó a la Superliga. Desde 2010 la sección ha encadenado un éxito tras otro logrando cuatro Ligas consecutivas (2011-12, 2012-13, 2013-14 y 2014-15), cuatro Copas de la Reina (2011, 2013, 2014 y 2017) y siete Copas de Cataluña (2009, 2010, 2011, 2012, 2014, 2015 y 2016).

El equipo de baloncesto femenino era denominado UB-Barça, ya que era el resultado de la asociación entre el F. C. Barcelona y el equipo de baloncesto de la Universidad de Barcelona. Competía en la Liga española de baloncesto femenino, que conquistó en dos ocasiones. El equipo desapareció en 2007, por las deudas económicas del club asociado al Barça.

Posteriormente, el club creó una sección oficial, completamente integrada en el club. Actualmente compite en 1ª Catalana.

El equipo de atletismo femenino del F. C. Barcelona compite en la División de Honor, la máxima categoría del atletismo español. Han representado al equipo las atletas internacionales españolas Montse Mas, especialista en 800 metros lisos, Rosa Morató, campeona de Europa de cross en el año 2005, o la marchadora María Vasco, medalla de bronce en los Juegos Olímpicos de Sídney 2000.

El equipo femenino de patinaje artístico fue fundado, como el masculino, el 25 de enero de 1972, coincidiendo con la inauguración del Palau de Gel, la pista de hielo del club, anexa al Palau Blaugrana, y que es sede del equipo. El equipo ha dado numerosos éxitos al club, entre los que destacan 10 títulos del Campeonato de España de clubes. Una de las máximas figuras de la sección fue Marta Andrade, considerada la mayor figura de la historia del patinaje artístico español, y que fue finalista en los Juegos Olímpicos de Invierno de Lillehammer y Nagano.

El equipo femenino de voleibol compite con el nombre CVB Barça y milita en la división de plata del voleibol español, Superliga 2.

El equipo femenino de hockey hierba compite en la división de plata nacional, conocida como Primera División Femenina.

Hasta el año 2011, los exjugadores del Barça aprovechaban las instalaciones del Miniestadi para entrenar un par de veces por semana. En dicho año, el Ministro de Deportes de Omán, con el objetivo de la promoción del turismo deportivo en el destino, intento realizar un clásico entre FC Barcelona y el Real Madrid. Ante distintas constricciones de realizar el partido con los jugadores en activo, se produjo el primer clásico español fuera de España, y el primer clásico entre veteranos leyendas de fútbol 11. Esto, gracias al apoyo del empresario tinerfeño, y exjugador de las categorías inferiores blaugranas, Rayco Garcia. Dicho partido fue la primera piedra en la creación del departamento de Leyendas del FC Barcelona, el cual se oficializó en 2016, y es gestionado directamente por el club.

A través de la organización de distintas experiencias deportivas, alrededor del mundo, y, ya sea con formatos de 3x3, 5, 6, 7 u 11 jugadores, el exjugador Albert "Chapi" Ferrer, es el encargado de configurar los equipos que participan en cada actividad.

El FC Barcelona Legends ha participado en partidos de exhibición en países como: Colombia, Marruecos, Brasil, y muchos más; exportando los valores deportivos del FC Barcelona. Desde 2016, el FC Barcelona Legends cuenta con más de 80 exjugadores profesionales del FC Barcelona, y su plantilla incluye jugadores de la talla de: Patrick Kluivert, Carles Puyol, Dani Alves, Andrés Iniesta, Hristo Stoichkov, Guillermo Amor, Rivaldo, Ronladinho, entre otros.

El 28 de marzo de 2020 se jugaría un amistoso en Anfield Road, entre FC Barcelona Legends y Liverpool FC Legends, a beneficio de la Fundación del Liverpool F.C., o L.F.C.Foundation. Dicho partido quedó aplazado debido a la pandemia provocada por la COVID-19.

Además de todas las secciones mencionadas, el Fútbol Club Barcelona ha tenido equipos en hasta ocho disciplinas deportivas más a lo largo de su historia. Son secciones que, por una u otra razón, se disolvieron. Entre 1924 y 8 el club tuvo un equipo de lucha greco-romana cuya figura era el campeón olímpico Emili Ardèvol. Otro de los grandes deportistas españoles que perteneció a la disciplina del club fue Joaquín Blume, miembro de la sección de gimnasia que el club tuvo entre 1957 y 1976.

El Barça también tuvo secciones de tenis (1926-1936), natación (1942), patinaje artístico sobre ruedas (1952-1956) y judo (1961-1976).

Otras secciones que han desaparecido han sido la de fútbol americano, que formó parte del club entre los años 2001 y 2003, tras la desaparición del Barcelona Dragons, equipo que compitió en la división europea de la NFL (National Football League), y la de ciclismo que, después de reaparecer en 2004 bajo la dirección de Melchor Mauri, volvió a extinguirse a finales de 2006 ante la falta de acuerdo en la forma de dirigir este deporte por las diversas federaciones internacionales.

En 1941 el club creó una sección de béisbol que, pese a ser una de las menos conocidas del club, nunca había dejado de existir. La sección contaba con equipos masculinos en todas las categorías, desde alevines hasta sénior, que tenían su terreno de juego en el Estadio Pérez de Rozas, situado en la montaña de Montjuïc de Barcelona. El equipo sénior ganó la Liga española de Béisbol en cuatro ocasiones en los años 1946, 1947, 1956 y 2011 competía en la máxima categoría del béisbol español, la División de Honor cuando se anunció su disolución en junio de 2011.

El F. C. Barcelona aglutina a socios y aficionados de todas las ideologías políticas, creencias religiosas y procedencias geográficas. El club ha logrado integrar de forma estratégica cuestiones políticas, religiosas, culturales y sociales, que van enmarcadas dentro del ámbito deportivo, esto permite que los socios y los aficionados respondan a todos los eventos sociales del club, también que tengan mayor participación en actividades administrativas y se fortalezcan los vínculos entre las peñas.

El club, considerado como la entidad social más conocida de Cataluña en el exterior, ha cumplido a lo largo de su historia, para muchos aficionados, una función representativa de defensa de los valores catalanistas, que el club ha defendido públicamente en múltiples ocasiones, como apunta el periodista inglés Jimmy Burns en su libro "Barça, la pasión de un pueblo". El club siempre se ha significado por actividades y gestos en defensa de la cultura y la lengua catalana, la cual ha sido la lengua oficial de todos los documentos del club, salvo en los años de dictadura franquista. Excepto en ese mismo período, el capitán del equipo siempre ha lucido igualmente la bandera catalana como brazalete distintivo. El club, además, también se ha manifestado formal y públicamente en apoyo de las reivindicaciones de mayor autonomía para Cataluña y firmó manifiestos de apoyo a los estatutos de autonomía tanto en 1932 como en 1979 y 2006.
Esa trayectoria de defensa de los valores catalanes fue reconocida el 21 de diciembre de 1992 cuando la Generalidad de Cataluña, presidida por Jordi Pujol, le otorgó el Premio Creu de Sant Jordi, la máxima distinción que otorga el gobierno de Cataluña.

Algunos historiadores y ensayistas, como Manuel Vázquez Montalbán, llegaron a apuntar que, para muchos catalanes, el F. C. Barcelona cumple en Cataluña el papel sustitutorio de la selección catalana en el concierto internacional, a pesar de la larga tradición de deportistas españoles de origen no catalán y de extranjeros que ha tenido el club. Estos ensayistas apuntan que ese es uno de los motivos por los que el club barcelonista cuenta con equipos en tantas disciplinas deportivas diferentes como el baloncesto, balonmano, hockey sobre patines, atletismo, voleibol, etcétera.

En esa línea, cabe anotar que el F. C. Barcelona se ha manifestado públicamente en favor del reconocimiento internacional de las selecciones deportivas catalanas. En los últimos años no solo ha promovido la organización de partidos amistosos entre la selección de Cataluña y otras selecciones internacionales como o , sino que ha cedido gratuitamente sus
instalaciones como sede de los encuentros y ha prestado a todos sus deportistas. Además, el club ha firmado manifiestos públicos en favor de la causa. Durante la presidencia de Joan Laporta, él mismo y algún jugador como Oleguer participaron en una campaña publicitaria de la Plataforma Pro-Selecciones Catalanas que, bajo el eslogan «una nación, una selección», ocupó espacios publicitarios en una gran cantidad de medios de comunicación escritos y audiovisuales de Cataluña.

A pesar de su vinculación con ideas catalanistas, el club ha contado siempre con gran cantidad de aficionados e incluso socios en toda España, atraídos por los valores deportivos del club. Algunos historiadores, sin embargo, han apuntado que, además de la admiración por los valores deportivos, muchos aficionados españoles simpatizaban con el Barcelona al ver en el club catalán la alternativa al «centralismo político» con el que identificaban al Real Madrid, especialmente durante los años de la dictadura franquista, fundamento equivocado, ya que repercutió en todos los clubes del país. Fue en aquellos años cuando se acuñó la frase de que el F. C. Barcelona era "més que un club" ("más que un club"), que se convirtió en el eslogan más conocido de la entidad.

Por otra parte, y como han apuntado diversos historiadores, el club también aglutinó, especialmente durante sus primeras décadas de vida, a los simpatizantes del republicanismo. Desde principios del siglo diferentes hechos apuntan la complicidad de los dirigentes del club con los ideales republicanos. 
El momento de mayor distanciamiento entre el club y la monarquía española tuvo lugar bajo el reinado de Alfonso XIII y durante la dictadura de Primo de Rivera. En el estadio de Les Corts, los aficionados del Barcelona habían manifestado críticas a la dictadura y exhibido algunas pancartas contra el régimen. Finalmente, el 14 de junio de 1925 los 14 000 aficionados del estadio abuchearon la «Marcha Real», interpretada por una banda de música. Días más tarde, el Capitán General y Gobernador Civil de Barcelona Joaquín Milans del Bosch dictó una orden que clausuró el estadio durante seis meses y obligó a dimitir a Hans Gamper como presidente del club y a exiliarse a Suiza durante una temporada. La orden de clausura del estadio justificaba la medida indicando que «en la citada sociedad hay personas que comulgan con ideas contrarias al bien de la Patria», como recoge el historiador Jaume Sobrequés en su obra "F. C. Barcelona, Cien años de historia". Fue la sanción más dura que ha recibido el club en toda su historia. Como señala el propio Sobrequés, el punto culminante del compromiso del club con los principios republicanos tuvo lugar a partir de 1931, cuando se proclamó la Segunda República Española y, sobre todo, a partir del inicio de la Guerra Civil española cuando, en 1936, el F. C. Barcelona se convirtió voluntariamente en «Entidad al servicio del gobierno legítimo de la República».

Tras el restablecimiento de la democracia en España en 1977, el club ha ido perdiendo esa connotación política. Normalizó sus relaciones con la corona española y en diversas ocasiones expediciones formadas por dirigentes y deportistas del club han ofrecido sus trofeos en el Palacio de la Zarzuela. El noviazgo y posterior matrimonio de la Infanta Cristina con el jugador de balonmano del F. C. Barcelona Iñaki Urdangarín hizo frecuente a finales de los años 1990 y principios de los años 2000 la presencia de miembros de la Familia Real Española en el Palau Blaugrana, incluido el Rey Juan Carlos I. El último gesto de complicidad entre el club y la Casa Real tuvo lugar el 17 de mayo de 2006, con motivo de la final de la Liga de Campeones de la UEFA 2005-06, cuando los Reyes acudieron a París para mostrar su apoyo al conjunto azulgrana y, concluido el encuentro, bajaron al césped a felicitar a los jugadores del equipo junto al presidente del gobierno español José Luis Rodríguez Zapatero, declarado seguidor del conjunto barcelonista, y el presidente de la Generalidad de Cataluña Pasqual Maragall.

En el terreno religioso, y pese a que el fundador del club, Hans Gamper y sus primeros dirigentes eran protestantes, el club adquirió a partir de los años 1940, tras la Guerra Civil española, un carácter marcadamente católico. Fueron habituales las ofrendas del club al Monasterio de Montserrat, e incluso el estadio del Camp Nou cuenta con una capilla junto a los vestuarios, con una reproducción de la Virgen de Montserrat. En 1982, el papa Juan Pablo II recibió el carné de socio número 108 000 del club, con motivo de una misa multitudinaria que ofició en el Camp Nou.

En los últimos tiempos el club se ha significado por sus gestos solidarios. A principio de los años 1980 ya organizó un partido amistoso en beneficio de Unicef, en el que el Barça se enfrentó en el Camp Nou al equipo "Human Stars", una selección de los mejores futbolistas del mundo. A mediados de los años 90 volvió a repetirse la experiencia. También en esa década el club se implicó en la lucha contra la droga, organizando diversos partidos en colaboración con la Fundación de Ayuda contra la Droga, cuyos beneficios se destinaron al Proyecto Hombre.

Con la llegada de Joan Laporta a la presidencia, el club manifestó su intención de incrementar su implicación en causas sociales, expresando el deseo de que el club sea reconocido mundialmente por su talante solidario. En ese sentido, a finales del año 2005, el F. C. Barcelona organizó un partido amistoso en el Camp Nou ante una selección conjunta de jugadores israelíes y palestinos, que por primera vez compartieron equipo. En los últimos años el club ha firmado diversos acuerdos de colaboración con ONG, dando ayudas económicas para el desarrollo de países en vías de desarrollo.

El club destina el 0,7 % del presupuesto anual a la Fundación Fútbol Club Barcelona, encaminada a proyectos humanitarios. A partir de 2006, el club se comprometió a realizar una donación no inferior a 1,5 millones de euros a Unicef, para que ésta lo destine a mejorar las condiciones de vida de los niños de todo el mundo. El primer proyecto conjunto de ambos organismos estuvo destinado a los niños víctimas del sida en Suazilandia. Además, puso el nombre de la organización en el espacio central de su primer equipaje, siendo la primera vez que lucía publicidad en las camisetas del equipo de fútbol.

El 13 de diciembre de 2008, coincidiendo con la disputa del clásico, el club inició la emisión de su canal de televisión Barça TV de manera gratuita en TDT para toda Cataluña fruto de un acuerdo con el Grupo Godó. El canal temático del club se creó hace casi una década y continúa sus emisiones en la modalidad de pago para el resto de España.

El club también está presente en las redes sociales como parte de su gestión social. Se encuentra en Facebook, Twitter, Instagram y Youtube, siendo el primer club de España en llegar «a un acuerdo con Youtube».

La Fundación Fútbol Club Barcelona es la entidad social, humanitaria, cultural y deportiva creada en 1994 con el objetivo de promover y brindar apoyo a todas aquellas entidades de carácter humanitario. También se enfoca en comunidades infantiles y adultas que se consideran vulnerables.

Inicialmente las actividades y obras realizadas por la fundación se materializaban mediante las donaciones por parte de socios, simpatizantes y entidades empresariales del país que eran integradas por miembros de honor, colaboradores y miembros protectores. En 2006 la Fundación tomó un nuevo impulso al recibir desde ese año, un 0,7 % de los beneficios ordinarios del club (2,2 millones de euros en 2008). Asimismo, se adhirió al plan de Objetivos de Desarrollo del Milenio de las Naciones Unidas e incorporó por primera vez publicidad de la UNICEF en la camiseta del equipo de fútbol.

Como parte de su gestión social, la fundación ha colaborado con otras entidades gubernamentales y sociales para fomentar la educación infantil, el deporte, la cultura y la salud. Entre sus principales aliados se encuentra la UNICEF, UNESCO, el Alto Comisionado de las Naciones Unidas para los Refugiados (ACNUR), el Consejo Económico y Social de las Naciones Unidas (ECOSOC), entre otros.

Una de las principales características distintivas del F. C. Barcelona es su naturaleza jurídica: es uno de los cuatro únicos clubes profesionales de España (junto a Real Madrid, Athletic Club y CA Osasuna) que no es una Sociedad Anónima Deportiva (SAD), conservando desde sus orígenes su carácter de asociación deportiva sin ánimo de lucro, cuya propiedad recae en sus socios. En este sentido, el órgano supremo de gobierno de la entidad es la Asamblea y los socios eligen al presidente por sufragio universal directo.

En septiembre de 2009 el F. C. Barcelona registraba 170 000 socios, una cifra que le sitúa como el segundo club de fútbol con más asociados del mundo, por detrás del SL Benfica —con 1000 afiliados más— y por delante del Manchester United. Así mismo, es también el club español con más abonados: 86 314 en agosto de 2009, por delante de los 70 000 del Real Madrid.

El 16 de junio de 2011, el F. C. Barcelona comenta en su web, mediante una noticia sobre declaración institucional que el club posee 180 000 socios. En esa misma temporada, el club logró ingresos cercanos a los € 51 000 000 de euros por conceptos de socios y abonados.

La afición por el club ha llegado al punto que una pareja belga le ha puesto el nombre de Barça como nombre de pila a su primer hijo, que además han hecho socio del club, el 19 de julio de 2010 vía Internet con el número 185.508. Se trata de Barça Beeckman, nacido el 26 de abril de 2010 en Sint-Truiden, ciudad de Bélgica. Según la web del F. C. Barcelona, Barça Beeckman es el primer socio que se llama como el equipo y no tienen constancia que nadie más en el mundo se llame así.

Actualmente, según un estudio alemán, el Barcelona es el club con más seguidores en Europa.

El F. C. Barcelona es uno de los clubes con mayor actividad peñística del mundo. A fecha de julio de 2014 el club contaba con 152 004 peñistas en 267 peñas oficiales, repartidas por los cinco continentes.

Las primeras agrupaciones de hinchas barcelonistas se registraron en los años 1920 y 1930. Al no figurar en ningún registro, no existe constancia de cual fue la primera de estas peñas, si bien los historiadores suelen apuntar como tal a la "Penya Esquerra", creada en 1923 en el Izquierda Bar, de la calle Aribau de Barcelona. "La Mosca", "All-i-oli", "Colón", "La Escombra", "Continental" o el "Casal Barcelonista" son otras de las peñas pioneras, junto con la "Peña Sagi-Barba" y la "Peña Els Tres" (homenaje a Piera, Sastre y Samitier), que son las primeras dedicadas a jugadores barcelonistas. Todas ellas desaparecieron durante la guerra civil española.

La primera peña del F. C. Barcelona legalmente constituida llevaba el nombre del bar "Solera", donde fue creada en 1944 bajo el impulso del exjugador Josep Samitier, junto con los entonces jóvenes Antoni Ramallets, Mariano Martín, César Rodríguez y Gustavo Biosca, que posteriormente defenderían la camiseta del primer equipo azulgrana. La Peña Solera adquirió su impulso definitivo poco después, con la llegada de Nicolau Casaus, quien abrió varias delegaciones de la misma en otras localidades catalanas. En 1972, bajo la presidencia de Agustí Montal i Costa, tuvo lugar la primera "Trobada Mundial de Penyes Barcelonistes", un encuentro que se ha celebrado anualmente desde 1977. En ese momento, el número de peñas rondaba las 150, cifra que sufrió un crecimiento exponencial durante los años 1980, superando el millar a finales de la década de 1990.

El F. C. Barcelona mantiene una fuerte rivalidad futbolística con el Real Club Deportivo Español, equipo de la ciudad de Barcelona. El derbi barcelonés es el nombre que describe los partidos entre ambos clubes, los más representativos de la ciudad. Se han enfrentado en 195 ocasiones, contando dos partidos europeos disputados en la Copa de Ferias, con un balance de 122 partidos ganados por el Barcelona, frente a 44 del Español. Xavi Hernández es el futbolista con más partidos disputados, con un total de 32, mientras que el argentino Lionel Messi es el máximo anotador, con 21 goles.

La rivalidad nació en parte por los continuos enfrentamientos de antaño que, entre otras cosas, eran aguerridos y contaban con buenos jugadores. Otro de los hechos es porque fueron pioneros del fútbol en Barcelona, los más representativos, a esto también hay que agregar que históricamente se consolidaron como los dos mejores equipos en las competiciones organizadas por la Federación Catalana de Fútbol.

El equipo "culé" también disputa el denominado clásico del fútbol español frente al Real Madrid Club de Fútbol, unos de los partidos más importantes y trascendentes del mundo. La audiencia supera los 500 millones de espectadores y se acreditan más de 750 periodistas; a menudo, es conocido por la prensa especializada como el Partido del siglo. En el balance general, Barcelona ha ganado 96 partidos frente a los 95 del Madrid, de un total de 242 encuentros oficiales. El primer duelo se dio a comienzos de la década de 1900, con un resultado favorable para el Barcelona. La popularidad e importancia también radica en la cantidad de trofeos ganados y la calidad de jugadores de ambos clubes. A nivel nacional destacan otros equipos como el Athletic Club y Atlético de Madrid. Barcelona y Athletic son los dos clubes más laureados de la Copa del Rey y han disputado siete finales, con un balance de cinco victorias y dos derrotas para el azulgrana.

A nivel internacional destaca los enfrentamientos con la Associazione Calcio Milan italiana. El citado club italiano es a nivel internacional con el cual se ha enfrentado en más oportunidades con 19 encuentros.







</doc>
<doc id="17317" url="https://es.wikipedia.org/wiki?curid=17317" title="Pisco (aguardiente)">
Pisco (aguardiente)

Pisco es la denominación de un aguardiente de uvas actualmente producido en Perú y Chile, elaborado fundamentalmente mediante la destilación del producto de la vid, como el "brandy" y el coñac, pero sin la prolongada crianza en barricas de madera. Se le suele incluir dentro de la familia o categoría de los brandy, y posee dos estándares diferentes para su producción —para el pisco peruano y para el pisco chileno—, por lo que se consideran técnicamente productos diferentes, pero dentro del grupo de bebidas espirituosas.

Esta bebida alcohólica, junto con el singani, tiene su origen en la necesidad náutica y minera, antes que campesina, de concentrar volumétricamente de forma estable y perdurable los vinos, para su adecuado almacenamiento y transporte, típica de la costa pacífica de Sudamérica, próxima a los Andes mineros. 

Ambas variedades, el pisco del Perú y el pisco chileno, tienen en común el hecho de ser aguardientes de uva, obtenidos por destilación de cepas blancas y rosadas de "Vitis vinifera", aprobadas por sus respectivas legislaciones como "uvas pisqueras". Otra situación en común es que muchos, en sus respectivos países, las consideran bebidas nacionales. Asimismo, en ambos países, está prohibida la importación del producto del país vecino si viene etiquetado como «pisco», ya que la denominación de origen es objeto de disputa y controversia; debido a la existencia de una ciudad en Perú llamada Pisco (ciudad), es por ello que, por ejemplo, en Chile se comercializa el pisco del Perú etiquetado como «aguardiente de uva» o «destilado de vino».

Aunque entre estos productos es posible encontrar características comunes, como por ejemplo algunos tipos de uvas utilizadas por ambos, no es menos cierto que entre ellos existen importantes diferencias, más allá de la zona de elaboración.
La Norma Técnica Peruana sobre el pisco (NTP211.001:2002), define al «pisco» como el «aguardiente obtenido exclusivamente por destilación de mostos frescos de uvas pisqueras recientemente fermentados, utilizando métodos que mantengan el principio tradicional de calidad establecido en las zonas de producción reconocidas». 

En el caso del pisco del Perú, las variedades usadas y reconocidas legalmente como «uvas pisqueras», son las uvas: Quebranta, Negra Criolla, Mollar, Uvina ("no aromáticas); e Italia, Moscatel, Albilla y Torontel ("aromáticas"). Estas uvas provienen de las cinco regiones pisqueras reconocidas por la denominación de origen pisco en Perú: Lima, Ica, Arequipa, Moquegua y Tacna.

En el Perú existen oficialmente cuatro clases de piscos: 

El pisco del Perú se destila sin ajustar su graduación alcohólica final con agua u otros agregados. La graduación alcohólica del pisco del Perú varía regularmente entre los 38° y 48°, teniendo habitualmente 42º.

El reglamento chileno de la Denominación de Origen Pisco (decreto 521 de 2000 del Ministerio de Agricultura), define al «pisco» como el «aguardiente producido y envasado, en unidades de consumo, en las Regiones III y IV del país, elaborado por destilación de vino genuino potable, proveniente de las variedades de vides que se determinan en este reglamento, plantadas en dichas regiones».

En el caso del pisco chileno, la principal uva usada es la Moscatel, en diversas variedades, y, en menor medida, la Pedro Jiménez y la Torontel. La normativa chilena rechaza la uva País, que comparte origen con la Quebranta, y cualquier otra variedad no expresamente considerada como «uva pisquera».

En Chile existen cuatro clases oficiales de piscos, de acuerdo a su graduación alcohólica mínima: 
El pisco chileno se destila a alrededor de 70º grados y luego se hidrata con agua desmineralizada para ajustar su graduación a la deseada, tal como se hace en la elaboración del singani, el vodka y el whisky. La graduación alcohólica del aguardiente chileno varía regularmente entre los 30° y 50°, siendo más populares y consumidos aquellos de entre 35° y 40°.

Hay una controversia sobre la legitimidad del uso de la denominación «pisco», en referencia al aguardiente de uvas producido en Perú y en Chile. Al respecto, existe una diferencia histórica entre ambos países. Perú reclama que el nombre es una denominación de origen exclusiva y Chile sostiene que ambos tienen derecho a usarla.

El Perú considera que el nombre aplicado a la bebida espirituosa tiene una relación estrecha con el espacio geográfico donde se produce en ese país, en la ciudad de Pisco y sus alrededores, de manera que debiera tener la exclusividad en su uso, además del hecho de producirse tal aguardiente en tierras del Perú desde finales del siglo XVI. Se declaró como denominación de origen peruana por resolución administrativa en 1990, estableciéndose oficialmente su territorio de producción al año siguiente.

Por su parte, Chile sostiene que el término es igualmente aplicable a la bebida destilada producida a partir de uvas en su territorio, donde existe una localidad que utiliza dicho nombre —Pisco Elqui, adoptada en 1936; anteriormente conocida como La Unión y antes La Greda, y de tradición pisquera desde inicios del siglo XVIII—, estableciéndose legalmente como denominación de origen chilena y delimitando una zona geográfica para su producción en 1931. No niega que tal producto haya podido fabricarse primero en Perú, pero argumenta que tal denominación ha sido usada para designar el aguardiente de uvas producido en ambos países, desde la época colonial, por lo que puede ser utilizada tanto por Chile como por Perú; en ese sentido, se ha argumentado que sería un término genérico, defendiéndose que la denominación de origen de su variedad sea «pisco chileno», o que se trataría de una denominación binacional.

A nivel internacional, diversos países se han pronunciado respecto a esta controversia reconociendo la denominación «pisco» (con o sin agregados) a favor de Perú, de Chile o de ambos. Así por ejemplo, la Unión Europea, conforme a un declaración conjunta con la República de Chile, integrante del Acuerdo de Asociación entre ambos de 2002, reconoció la denominación de origen «Pisco» para uso exclusivo de productos originarios de Chile, sin perjuicio de los derechos que pudiera reconocer, además, para Perú; y, en 2013, a través de la Comisión Europea –y tras un solicitud presentada por la República del Perú–, la Unión Europea reconoció al «Pisco» como indicación geográfica de Perú, sin perjuicio de la utilización de la denominación para los productos originarios de Chile en virtud del Acuerdo de Asociación entre la Unión Europea y Chile.




</doc>
<doc id="17318" url="https://es.wikipedia.org/wiki?curid=17318" title="Combate naval de Iquique">
Combate naval de Iquique

El combate naval de Iquique fue uno de los primeros enfrentamientos ocurridos durante la campaña naval de la Guerra del Pacífico. Tuvo lugar en la bahía de Iquique el miércoles 21 de mayo de 1879. En él se enfrentaron el monitor peruano "Huáscar", al mando del capitán de navío Miguel Grau Seminario, y la corbeta chilena "Esmeralda", al mando del capitán de fragata Arturo Prat Chacón. El resultado de esta acción fue el hundimiento de la corbeta chilena y el levantamiento del bloqueo del puerto de Iquique.

El 16 de mayo, la escuadra chilena dejó bloqueando el puerto de Iquique a sus naves de guerra la "Esmeralda y" la "Covadonga, además del transporte chileno Lamar," y zarpó rumbo al norte para enfrentar a la flota peruana, a la que esperaba sorprender en el puerto del Callao. Sin embargo, el mismo día los buques de la marina de guerra del Perú habían salido con rumbo al sur con la intención de defender sus puertos en el sur. Ambas flotas se cruzaron sin verse y las naves peruanas se encontraron el día del combate con las tres naves chilenas en Iquique.

Aunque comenzaron en el mismo lugar y a la misma hora, el enfrentamiento de la "Esmeralda" contra el "Huáscar" es llamado combate naval de Iquique, y el de la "Independencia" contra la "Covadonga", combate naval de Punta Gruesa (este último es el lugar en la costa frente al cual ocurrió el desenlace de la lucha).

Antes de la declaración de guerra, el gobierno chileno decidió como estrategia movilizar su escuadra para bloquear el puerto peruano del Callao, esperando así encerrar allí a la escuadra del Perú para operar libremente en el litoral peruano o bien destruirla en un combate si se presentaba la ocasión. El contraalmirante Juan Williams Rebolledo, comandante en jefe de la escuadra chilena, rechazó este plan por considerar que sus naves no estaban en condiciones de emprender un ataque inmediato a El Callao pues carecía de víveres y combustible para la travesía. En su lugar, Williams prefirió bloquear el puerto de Iquique y desde allí hostilizar los puertos peruanos del Departamento de Tarapacá. La escuadra chilena parte el 3 de abril desde Antofagasta con destino a Iquique para establecer el bloqueo.

Chile le declaró la guerra a Perú y Bolivia el 5 de abril de 1879 y ese mismo día la escuadra chilena inició el bloqueo del puerto de Iquique.

Durante el bloqueo de Iquique la escuadra chilena incursionó en los poblados peruanos de Pabellón de Pica, Huanillos (15 de abril) y Mollendo (17 de abril) bombardeando trenes y naves; luego bombardeó Pisagua (18 de abril) y destruyó Mejillones Norte (29 de abril).

Debido a la presión del gobierno chileno, Williams es convencido de atacar el puerto del Callao. Para tal efecto, la escuadra chilena zarpó desde Iquique el viernes 16 de mayo en una expedición al Callao con todos los buques disponibles dejando el bloqueo de Iquique a cargo de la corbeta "Esmeralda," al mando de Arturo Prat, sometida a urgentes reparaciones en sus calderas; y la goleta "Covadonga", al mando de Carlos Condell; construidos en 1855 y 1859, respectivamente. También se quedó el transporte "Lamar". Debido a su mayor antigüedad, Prat quedó como jefe del bloqueo.

Para defender a las localidades peruanas del ataque chileno, el plan del Perú era terminar en el Callao las reparaciones de las naves de su escuadra y trasladar tropas y pertrechos hacia Arica, Iquique y demás puertos del departamento de Tarapacá y enviar naves para traer desde Panamá armamento y municiones adquiridos en los Estados Unidos. Los comandantes peruanos García, Grau y Moore, entre otros, estuvieron en desacuerdo con este plan ya que la "Independencia" estaba recién reparada y su tripulación no había hecho ejercicios navales mientras que el "Huáscar" no contaba con proyectiles capaces de penetrar el blindaje de los buques chilenos "Cochrane" y "Blanco Encalada". A pesar de esta oposición, la escuadra peruana zarpó el mismo 16 de mayo desde el puerto del Callao hacia Arica llevando al presidente Mariano Ignacio Prado a bordo de la nave insignia "Oroya".

Ambas escuadras se cruzaron en alta mar sin avistarse. En Mollendo el presidente Prado se enteró, por medio del vapor "Ilo" de la compañía Pacific Steam Navigation Company, de que el grueso de la escuadra chilena se había retirado. En Arica se enteró de que habían dejado a las naves "Covadonga", "Esmeralda" y un transporte a cargo del bloqueo de Iquique por lo que Prado decidió que el "Huáscar" y la "Independencia" navegaran hasta Iquique a romper el sitio, capturando o destruyendo a los buques chilenos.

Las naves enfrentadas eran en extremo desiguales, barcos peruanos de acero contra barcos chilenos de madera. La "Esmeralda" casi no disponía de calderas, lo que impedía movimientos defensivos. Más aún, desde la costa era amenazada por la artillería costera.

La "Esmeralda" era una corbeta de madera con una tripulación de 201 marinos, de 850 t de desplazamiento construida en 1855. Su armamento estaba compuesto por 12 cañones de (peso de la bala disparada). Su sistema de propulsión era mixto, máquina a vapor y vela. Al momento de entrar en combate sus máquinas estaban en mal estado de mantenimiento y sólo eran capaces de propulsar el buque a una velocidad de , los que se redujeron a 2 kt (nudos) al estallar sus calderas.

El "Huáscar", es un buque tipo monitor con una tripulación de 197 marinos, acorazado, de 1745 toneladas de desplazamiento construido en 1865. Cuenta con un casco de de espesor y su armamento principal estaba constituido, en mayo de 1879, por 2 cañones de carga Armstrong de ubicados en una torre giratoria blindada, además de 2 cañones de 40 (o pdr), uno de 12 libras y una ametralladora Gatling de 0,44 pulgadas. Su sistema de propulsión era también mixto, máquina a vapor y vela siendo capaz de alcanzar una velocidad máxima, el día del combate, de 10,5 nudos.

Al comienzo de las acciones, el general Juan Buendia ordenó colocar en la playa de Iquique 4 cañones Blakely de montaña de , además de soldados que con sus fusiles debían hacer fuego a la corbeta chilena.

En la mañana del miércoles 21 de mayo, el bloqueo de Iquique era mantenido por la corbeta "Esmeralda" y la goleta "Covadonga" fondeadas ambas a 2,7 km al norte del faro del puerto. Por su parte, el transporte "Lamar" se encontraba fondeado más cerca de la costa. A las seis y media de la mañana uno de los vigías de la "Covadonga", la cual se encontraba de guardia, avistó columnas de humo acercándose desde el norte. Al reducirse la distancia, se identificó que dichas columnas de humo correspondían a los blindados peruanos "Huáscar" e "Independencia". El comandante de la "Covadonga", ordenó advertir la presencia del enemigo al comandante de la "Esmeralda", con un cañonazo. Este, al escuchar la señal dispuso levar el ancla, hacer comer a la tripulación y tocar zafarrancho de combate. Además ordenó que la "Covadonga" se pusiera al habla para conferenciar y que se arrojara al mar, en un saco, la correspondencia para la escuadra chilena.

Los buques peruanos, al avistar las naves chilenas, izaron bandera de combate. El "Huáscar" se encontraba más cerca al puerto. El comandante Grau arengó a su tripulación:

Por su parte, Prat ordenó salir a reconocer los barcos que se aproximaban. Su buque navegó en dirección oeste y al confirmar que eran enemigos regresó y ordenó a Condell seguirlo. Izando señales dio órdenes: primero "¿Almorzó la gente?", luego "seguir mis aguas" y finalmente "venir al habla" y a continuación arengó a su tripulación. La versión de la arenga que ha pasado a la historia es una estilización resumida de la original, cuyo texto más fiel, por la cercanía temporal y espacial con los hechos, es, quizá, el que reportó el guardiamarina sobreviviente de la "Esmeralda" Vicente Zégers en una carta escrita a su padre justo una semana después del combate. En ella, el mensaje de Prat a sus hombres es descrito en las siguientes palabras:

Terminada la arenga, la "Covadonga" llegó al habla y Prat le ordenó a Condell: "¡que almuerce la gente!, ¡mantener bajos fondos!, ¡reforzar las cargas!, ¡cada uno a cumplir con su deber!". Condell simplemente respondió: "¡all right!". Terminado lo anterior se sintió una explosión y una columna de agua y espuma se levantó cerca de ambos buques, el "Huáscar" había disparado su primer tiro.

En tierra, la población puerto despertó con el primer cañonazo de la "Covadonga" y se dirigió a la playa para recibir a las naves peruanas que venían a liberarlos del bloqueo de Iquique.

El transporte chileno "Lamar" izó la bandera de los Estados Unidos y abandonó la bahía rumbo al sur. Durante 30 minutos, el "Huáscar" se enfrentó solo a las dos naves chilenas, hasta la llegada de la "Independencia", que concentraron sus tiros sobre el "Huáscar" sin mayores consecuencias.

Los movimientos iniciales de la "Esmeralda" hicieron que estallaran dos de sus calderas, lo que redujo su andar de 6 kn a 2 kn, dejando al buque prácticamente inmóvil. En atención a esto, Prat ubicó su nave frente a la población, a una distancia de 200 metros de la playa. En esta situación, los cañonazos peruanos podrían afectar a la población, lo que los obligaría a disparar por elevación.

Después de una hora de combate, las cuatro naves no presentaban daños importantes. A eso de las 11:30 horas, la "Covadonga", al mando de Condell, se dirigió al sur navegando pegada a la costa. Al respecto, no existen evidencias en ninguno de los partes del combate de que Prat haya modificado la orden dada a la Covadonga de "seguirle las aguas" por lo que se presume que Condell actuó por cuenta propia al decidir abandonar el Puerto de Iquique navegando con rumbo sur. 

Grau ordenó al comandante de la "Independencia" que persiguiera a la "Covadonga". En ese instante, el combate se dividió en dos enfrentamientos, uno entre el "Huáscar" y la "Esmeralda" y el otro entre la "Independencia" y la "Covadonga".

Cuando el "Huáscar" se encontraba a unos 600 metros de la "Esmeralda", se le acercó un bote, donde iban el capitán de puerto y de corbeta, Salomé Porras, junto al práctico Guillermo Checlay y el periodista Modesto Molina, quienes le informaron a Grau que la "Esmeralda" estaba protegida por una línea de torpedos fijos. Ante esta información, Grau decidió mantener una distancia de 500 metros de la corbeta, posición desde la cual abrió fuego.

Pasada una hora y media de combate, la "Esmeralda" no había sido impactada por ningún proyectil del "Huáscar", sus tiros pasaban largos cayendo en la playa e hiriendo a la población. Cerca de las diez de la mañana, el general Juan Buendía, jefe de las tropas peruanas en Iquique, hizo llevar a la playa 4 cañones Blakely de montaña con los cuales empezó a disparar contra la "Esmeralda". Una granada mató a tres hombres y otra hirió a otros tres. En total, realizó 60 tiros y varios de fusilería. La situación se tornó insostenible para la corbeta chilena por lo que Prat decidió cambiar su ubicación 1000 metros más al norte. Cuando iniciaba el movimiento, una granada del "Huáscar" penetró por su costado de babor saliendo por estribor provocando un incendio en la cámara de oficiales que fue prontamente controlado.


Una vez en su nueva posición, la corbeta no pudo moverse y se defendió allí una hora y media hasta su hundimiento.

Al observar el cambio de posición de la "Esmeralda", Grau se dio cuenta de que la información de la defensa con torpedos era errónea, por lo que decidió atacar empleando su espolón. Enfiló su proa hacia el costado de babor de la "Esmeralda". Prat trató de esquivar el golpe dando avante y cerrando la caña a babor no logrando esquivar el golpe que recibió a la altura del palo mesana sin mayores daños. Al chocar ambos buques, el monitor "Huáscar" disparó sus cañones de diez pulgadas (300 libras) a corta distancia, produciendo la muerte de 40 o 50 marineros y soldados.

El espolonazo del "Huáscar", a su vez, fue recibido con una tremenda descarga de las baterías de la "Esmeralda" y fuego de fusilería, lo que no causó mayor daño en el monitor.

Al ver la cubierta del buque enemigo a sus pies, Prat gritó:

Prat saltó a su cubierta enemiga seguido por el sargento Juan de Dios Aldea y el marinero Arsenio Canave quien murió en la cubierta del Huáscar.

Una vez a bordo, Prat, armado con un sable y un revólver, avanzó hacia la torre de mando; en el trayecto hacia ella, ultimó al oficial de señales, el teniente segundo Jorge Velarde. Al avanzar a babor de la torre de Coles, fue alcanzado por las balas en una de sus rodillas. Un marinero salió a cubierta y lo mató. A su vez, el sargento Aldea cayó herido por una descarga de fusilería sobre la cubierta. Grau hizo un esfuerzo por salvar la vida de Prat pero ya era tarde.


El comandante Grau quiso dar tiempo para que sus adversarios se rindieran, por lo que retiró el "Huáscar" después del espolonazo. En la "Esmeralda", tomó el mando el teniente 1.º Luis Uribe Orrego, quien llamó a reunión de oficiales y decidieron no rendirse; al tiempo que un guardiamarina subía al palo de mesana para clavar las banderas.

Al ver que la tregua no daba resultado, Grau decidió espolonear nuevamente a la "Esmeralda", lanzándose a toda velocidad sobre ella, ahora por el costado de estribor. Uribe trató de maniobrar igual que Prat y logró presentar su costado en forma oblicua al espolón del monitor "Huáscar", pero esta vez se abrió una vía de agua, ingresando a raudales a la santabárbara y a las máquinas. El buque quedó sin gobierno y sin más municiones que las que había en cubierta.

Nuevamente los cañones del "Huáscar" dispararon a corta distancia matando a varios tripulantes; entre ellos, a los ingenieros y fogoneros que salían a cubierta y arrasó la cámara de oficiales, convertida en enfermería. Se efectuó un segundo intento de abordaje por otros doce tripulantes chilenos, al mando del teniente primero Ignacio Serrano, llevando rifles y machetes, que también resultó infructuoso, cayendo sobre la cubierta del monitor.


Tras 20 minutos, se efectuó el tercer impacto con espolón en el sector del palo mesana acompañado de dos cañonazos. En este momento se produce un tercer abordaje, hecho poco conocido, en la que dos marineros saltaron a la cubierta del "Huáscar". Uno de ellos ha sido identificado como el timonel Eduardo Cornelius que logró sobrevivir al combate y relatar su historia. 
La corbeta, con el tercer impacto, se inclinó de proa y empezó a hundirse. A medida que el buque se inclinaba, el guardiamarina Ernesto Riquelme, gritando vivas a Chile, disparaba el último cañonazo.

A las 12:10 de ese día, la "Esmeralda" desapareció de la superficie del mar con su Bandera de combate al tope. 

En total, el "Huáscar" disparó 47 proyectiles y fue impactado por 6 bombas y 23 balas. Los chilenos acusaron 143 muertos. Los peruanos perdieron al teniente segundo Jorge Velarde y siete marineros resultaron heridos. Antes de avanzar para reunirse con la "Independencia", Grau dispuso el salvataje de los 57 náufragos de la Esmeralda.

Los sobrevivientes de la "Esmeralda" fueron entregados a las autoridades militares del puerto de Iquique. Los marinos sobrevivientes fueron conducidos como prisioneros a la localidad peruana de Tarma y fueron intercambiados por prisioneros del "Huáscar" en relación hombre por hombre y grado por grado a fines de diciembre de 1879. Sobre la situación de los sobrevivientes de la "Esmeralda", Jorge Hunneus del Ministerio de Asuntos Exteriores de Chile escribió al vicecónsul británico en Iquique, expresando la generosidad con que el Perú trató a los marinos prisioneros y la cual espera corresponder.

Luego del combate, el almirante Grau ordenó que los objetos personales de Prat —su diario personal, su uniforme y su espada, entre otros— fueran devueltos a su viuda. Junto con ellos, Carmela Carvajal recibió una carta del almirante peruano. En esta carta, Grau recalca la calidad personal y la hidalguía de su rival. En respuesta, Carmela Carvajal le escribió una carta agradeciendo este gesto. Este hecho sumado al rescate de los sobrevivientes de la "Esmeralda" hicieron ganar a Grau el apodo de «El Caballero de los Mares».

Los cuerpos de Arturo Prat y Serrano fueron enterrados el jueves 22 de mayo en el cementerio de Iquique, contando con el donativo del ciudadano español Eduardo Llanos y de otros miembros de su colonia para cubrir los gastos del sepelio.

La noticia llegó a Valparaíso, Chile, por el cable submarino. El sábado 24 de mayo recién se conocieron en Santiago los detalles del combate en Iquique y la muerte de Prat y, además, el hundimiento de la "Esmeralda". Según historiadores chilenos modernos, fue desde ese momento que cambió el sentimiento del pueblo chileno hacia el conflicto. Afirmando que: "Una guerra poco comprendida por el pueblo, se convirtió de pronto en una ocasión para emular el heroísmo de Prat", razón por la que una gran cantidad de chilenos acudieron voluntariamente a los cuarteles para enrolarse y participar en el conflicto.

El combate de Iquique fue una victoria peruana porque culminó temporalmente con el bloqueo del puerto, un barco chileno hundido y otro puesto en fuga. Pero, tomadas las acciones de Iquique y Punta Gruesa en su conjunto, se trató de una victoria pírrica, puesto que se batieron las naves más antiguas de la Armada chilena, una goleta y una fragata de madera, contra dos blindados peruanos. El resultado final resultó con uno de ellos encallado y hundido, y con este, el cincuenta por ciento del poderío naval ofensivo del Perú en la guerra.

Por su parte, los daños causados al "Huáscar" por el fuego de la "Esmeralda" fueron mínimos, debido al blindaje del monitor.

En 1888, los restos del comandante Arturo Prat fueron trasladados a Valparaíso, donde se les dio sepultura en un monumento construido por suscripción popular. En este monumento, descansan los máximos héroes navales chilenos, y es ahí donde cada año, en el día de las glorias navales, con la presencia del presidente de la República, se honra con desfiles militares a la figura de Prat y su tripulación. El 21 de mayo ha sido feriado en Chile a partir de 1915 y es la fecha de la Cuenta anual del presidente de la República ante el Congreso pleno desde 1926 hasta 2016. A partir de 2017, la cuenta pública presidencial se realiza el 1 de junio de cada año. 

El historiador estadounidense William Sater repara en la «inutilidad material» de su acción en Iquique; sin embargo, destaca que «al trascender de lo físico a lo espiritual, creó reglas de conducta que le significaron a su nación la victoria en la guerra y que fueron internalizadas por las siguientes generaciones». Ese al menos fue el propósito de la educación nacional chilena que lo ha erigido permanentemente como un modelo a imitar. 

Por su parte, el almirante Miguel Grau Seminario es recordado tanto en Perú como en Chile por su hidalguía y caballerosidad en combate. Algunas calles en Chile llevan el nombre de "Almirante Grau". Sus acciones durante la Guerra del Pacífico lo convirtieron en el mayor héroe naval de la marina de guerra del Perú. Miguel Grau también es considerado héroe naval en Bolivia.





</doc>
<doc id="17319" url="https://es.wikipedia.org/wiki?curid=17319" title="Centro de Formación en Turismo">
Centro de Formación en Turismo

El Centro de Formación en Turismo (CENFOTUR) es una entidad ubicada en Lima. Es dependiente del Ministerio de Comercio Exterior y Turismo del Perú.

Entidad educativa oficial del sector turismo, cuya razón de ser es formar, capacitar y perfeccionar los recursos humanos que requiere el desarrollo turístico del país, de acuerdo a estándares de competencia laboral.




</doc>
<doc id="17321" url="https://es.wikipedia.org/wiki?curid=17321" title="Nuevas siete maravillas del mundo moderno">
Nuevas siete maravillas del mundo moderno

Se denominan las nuevas siete maravillas del mundo moderno a los monumentos que resultaron los ganadores en un concurso público e internacional celebrado en 2007, inspirado en la lista de las siete maravillas del mundo antiguo y realizado por una empresa privada de nombre New Open World Corporation. Más de cien millones de votaciones, a través de Internet y SMS, dieron como resultado esta nueva clasificación. La iniciativa partió del empresario suizo Bernard Weber, fundador de la empresa.

Las siete maravillas del mundo conocido por los griegos helenísticos fueron seleccionadas por el pintor neerlandés Maerten van Heemskrerck en el siglo XVI en una serie de siete cuadros, que muestran las obras arquitectónicas y escultóricas que marcaron un antes y un después en la historia. Previamente autores como Filón de Bizancio, Antípatro de Sidón, Gregorio Nacianceno o Beda el Venerable, entre otros, habían confeccionado sus respectivos listados. Sólo una de ellas se mantiene actualmente en pie. Mientras la existencia de otras es todavía un misterio para investigadores y expertos en la materia. La pregunta más frecuente es: ¿por qué escogieron solo siete puntos de referencia? La cultura helenística consideraba tal cifra como el número perfecto

La votación fue pública. Los participantes debieron registrar un correo electrónico en el sitio web de la corporación y elegir sus candidatos favoritos. También se pudo votar vía SMS y a través de un número telefónico de pago. Una de las críticas del sistema empleado fue en la práctica nada impedía que una misma persona votara más de una vez, siempre y cuando lo hiciera desde un correo electrónico o SMS distinto. Se pudo votar por una sola candidata y recibir un certificado de la votación específica mediante el pago de dos dólares estadounidenses.

En cada voto se eligieron siete candidatas de una larga lista inicial, confeccionada por la corporación, y que se incrementó a pedido de diversos países o de solicitudes masivas de votantes. En los últimos meses de la votación solo participaron los 21 candidatos que hasta entonces habían obtenido la mayor cantidad de votos.

Ante las protestas del gobierno egipcio en la etapa final, se eliminó de la lista a las Pirámides de Guiza, hecho que fue disimulado por los organizadores asignándole a esta el estatus de Candidata Honoraria, debido a que es la única de las Siete Maravillas del Mundo Antiguo que permanece en pie.

En esta selección se admitieron estructuras creadas por el hombre hasta el año 2000, con la condición de que estuviesen en pie en la actualidad. Los resultados fueron dados a conocer el 07/07/07, es decir, el 7 de julio de 2007 en el Estádio da Luz, en Lisboa (Portugal), en una gran ceremonia.


Se consideró que la Gran Pirámide de Guiza (Egipto) sería la octava maravilla honorífica. La Gran Pirámide había sido excluida de la votación, por ser la más antigua y la única que aún perdura de las siete maravillas del mundo antiguo. El hecho se dio en el marco de una gran oposición de las autoridades culturales egipcias, tales como Zahi Hawass, secretario general del Consejo Superior de Antigüedades del gobierno egipcio (Ministro de Antigüedades hasta 2011), que calificó a este concurso de «operación publicitaria».


Después de un tiempo de votación se dieron a conocer los 75 semifinalistas quedando en este orden:

El proyecto ha sido criticado por diferentes motivos:





</doc>
<doc id="17323" url="https://es.wikipedia.org/wiki?curid=17323" title="Comisión para la Promoción de Exportaciones">
Comisión para la Promoción de Exportaciones

La Comisión para la Promoción de Exportaciones (abreviada Prompex) tiene su sede en Lima. Es dependiente del Ministerio de Comercio Exterior y Turismo del Perú.

Tiene como misión facilitar y promover las exportaciones peruanas, fomentando la iniciativa privada y contribuyendo con la generación de empleo.




</doc>
<doc id="17325" url="https://es.wikipedia.org/wiki?curid=17325" title="Televisor">
Televisor

El televisor, televisión o tele es un aparato electrónico destinado a la recepción y reproducción de señales de televisión. Usualmente consta de una pantalla y mandos o controles. La palabra viene del griego τῆλε ("tēle", ‘lejos’), y el latín "visōr" (agente de "videre", ‘ver’). 

El televisor es la parte final del sistema de televisión, el cual comienza con la captación de las imágenes y sonidos en origen, y su emisión y difusión por diferentes medios. El televisor se ha convertido en un aparato electrodoméstico habitual, cotidiano y normal con amplia presencia en los hogares de todo el mundo. El primer televisor comercial fue creado el 26 de enero de 1926 por el escocés John Logie Baird.

Los primeros televisores que se pueden considerar comerciales fueron de tipo mecánico y se basaban en un disco giratorio, el disco de Nipkow (patentado por el ingeniero alemán Paul Nipkow en 1884), que contenía una serie de agujeros dispuestos en espiral y que permitían realizar una exploración "línea por línea" a una imagen fuertemente iluminada. La resolución de los primeros sistemas mecánicos era de 30 líneas a 12 cuadros pero fueron posteriormente mejoradas hasta alcanzar cientos de líneas de resolución e inclusive incluir color.

La televisión mecánica fue comercializada desde 1928 hasta 1934 en el Reino Unido, Estados Unidos y la URSS. Los primeros televisores comerciales vendidos por Baird en el Reino Unido en 1928 fueron radios que venían con un aditamento para televisión consistente en un tubo de Neón detrás de un disco de Nipkow y producían una imagen del tamaño de una estampilla, ampliada al doble por una lente. El "televisor" Baird estaba también disponible sin la radio. El televisor vendido entre 1930 y 1933 es considerado el primer televisor comercial, alcanzando varios miles de unidades vendidas.

El sistema mecánico fue pronto desplazado por el uso del CRT (tubo de rayos catódicos) como elemento generador de imágenes, que permitía alcanzar mejores resoluciones y velocidades de exploración. Además, al no tener elementos mecánicos, el tiempo de vida útil era mucho mayor.

El primer televisor totalmente electrónico (sin elementos mecánicos para generación de la imagen) con tubo de rayos catódicos fue manufacturado por Telefunken en Alemania en 1934, seguido de otros fabricantes en Francia (1936), el Reino Unido (1936) y los Estados Unidos (1938). 

Se estima que antes de la II Guerra Mundial se fabricaron en el Reino Unido unos 19 000 aparatos y en Alemania unos 1600.

Ya en las épocas tempranas del CRT se empezaron a idear sistemas de transmisión en color, pero no fue hasta el desarrollo de los tubos de rayos catódicos con tres cañones, que se empezaron a fabricar masivamente televisores en color totalmente electrónicos.

En la década de 1970, los televisores en color fueron ampliamente difundidos y empezaron a comercializarse en los países desarrollados. La premisa de compatibilidad con los sistemas monocromáticos permitió que ambos tipos de televisores convivieran de forma armoniosa hasta nuestros días.

La electrónica de los televisores ha ido evolucionando conforme avanzaba la electrónica en general. Los primeros televisores usaban tubos al vacío y luego transistores. Más recientemente se empezaron a usar circuitos integrados, desarrollándose algunos circuitos ex proceso para las funciones específicas del televisor. A finales del siglo XX comenzaron a desarrollarse pantallas de reproducción de imagen que no usaban el TRC. En la primera década del siglo XXI el tubo desapareció, dando paso a televisores con pantallas planas de diferentes tecnologías, que aún no logrando una calidad de imagen similar a la lograda por el TRC, permitían hacer unos aparatos de volumen mucho menor, casi sin fondo, y de unas líneas estéticas muy atractivas que fueron copando el mercado mientras los fabricantes dejaban de producir televisores con tubo de imagen.

El tubo de imagen fue sustituido por pantallas de tecnología de plasma, LCD, LCD retroiluminado con led y OLED, a la par que los sistemas de transmisión se cambiaban a sistemas digitales, bien mediante la distribución por cable, satélite o la distribución terrestre mediante la TDT.

A finales de la primera década del siglo XXI, con el desarrollo de Internet aparecen los televisores conectables y se comienza a hablar de la «televisión híbrida», que comparte la recepción convencional con el acceso a la red de redes para visualizar contenidos audiovisuales o de cualquier otro tipo abriendo nuevas áreas de servicio.

Se han desarrollado también sistemas de representación en 3D (tres dimensiones) y mejoras en el sonido. Los televisores llegan a poder mostrar varias imágenes o contenidos diferentes a la vez en sus pantallas y a poder realizar grabaciones de contenidos sin necesidad de elementos externos.

La representación de una imagen en cualquiera de las tecnologías usadas para la pantalla de los televisores y monitores tiene cuatro áreas críticas a salvar, donde los resultados obtenidos son relevantes para la calidad final lograda. Estas áreas son:

La dificultad de la reproducción del negro verdadero es una de las áreas más comprometidas en la representación de una imagen en una pantalla de televisión. Los bajos niveles de luminancia que generan los negros, o grises muy oscuros, son difíciles de conseguir debido la utilización de retroiluminación o niveles de cebado de los plasmas. Las áreas oscuras de las imágenes representadas carecen de rango en los negros, quedando estos anulados (convertidos en grises) entre unos niveles y otros, dando lugar a artificios y ruido.

Los TRC tienen un nivel mínimo de excitación de los luninofósforos que proporcionan un negro aceptable. No ocurre lo mismo en los plasmas y menos aún en las pantallas LCD, que precisan retroiluminación, lo que hace que no se logre nunca tener la pantalla oscura. La tecnología OLED, al ser cada píxel un emisor individual, puede reproducir una gama de negros muy reales, ya que se logra apagar totalmente el emisor.

La reproducción del color en imágenes con áreas de muy poca luminancia es uno de los puntos más difíciles para la reproducción de una imagen. En un sistema de TV el color surge de la mezcla de tres luces correspondientes a tres colores diferentes (denominados "colores primarios" : rojo, verde y azul). Los escasos niveles de luminancia hacen que esa mezcla no pueda ser correcta al caer en las zonas no lineales de los emisores de luz. 

Este fenómeno, muy relacionado con la reproducción real de los negros, precisa de sistemas de reproducción muy lineales en el extremo de baja luminancia. Las tecnologías de plasma y LCD no tienen estas características por su propia base tecnológica, que en el TRC se podían encontrar con eficiencia suficiente y que la tecnología OLED, por el mismo motivo que el expuesto anteriormente, cubre de una manera eficiente.

Para que una imagen pueda verse clara y nítida se deben poder reproducir todos los niveles de luz contenidos en la misma. Niveles de luz que en la mezcla de los tres colores básicos dan toda la gama de colores que se deben representar.

Desde el apagado absoluto que nos proporciona un negro real hasta el encendido a pleno brillo para un blanco, tenemos toda la gama de niveles a reproducir. La linealidad, muy crítica en los extremos, de los elementos que las diferentes tecnologías utilizan para la representación de la imagen es la que da cuerpo al rango dinámico. Los tubos de rayos catódicos mantienen una curva característica, denominada ganma, que se debía de compensar (se hace en la emisora) para lograr una respuesta lineal óptima. 

Los sistemas de plasma y LCD tienen una respuesta no lineal y con una relación de contraste muy pobre lo que hace que su ancho dinámico sea pequeño. La tecnología OLED logra un buen resultado.

La televisión es un sistema de transmisión de imágenes en movimiento. El tiempo de respuesta de las pantallas de reproducción de las imágenes es fundamental para la fidelidad de lo reproducido.

Los cambios rápidos en las imágenes deben ser realizados de tal forma que no supongan retardos y distorsiones o perdida de resolución. Para ello los tiempos de persistencia y de histéresis de los elementos generadores de la imagen son importantísimos. Los tiempos de activación y desactivación de los elementos lumínicos son fundamentales para una correcta representación de la imagen en movimiento. Estos tiempos no solo dependen de la tecnología de la pantalla sino también del procesado de la señal. El concepto de tiempo de cambio entre dos niveles de grises, en inglés "grey-to-grey switching speed", es el que determina este parámentro.

Gracias a los avances en la tecnología de pantallas, hay ahora varias clases en los televisores modernos:





La resolución en píxeles es la cantidad de puntos individuales llamados píxeles en una pantalla dada. Una resolución típica de 720x480 significa que la pantalla del televisor tiene 720 píxeles horizontales y 480 píxeles en el eje vertical. La resolución afecta a la nitidez de la imagen. Cuanto mayor es la resolución de una pantalla, mayor es su nitidez.
La primera resolución tenía 48 líneas y cada una de las fábricas usaba sistemas diferentes. La estandarización de estos sistemas comienza en julio de 1941 cuando se logró el sistema NTSC, válido para todos los estados de Estados Unidos, de 325 líneas. Europa logró un sistema de 625 líneas al término de la guerra, Francia poseía uno propio de 819 líneas e Inglaterra mantuvo el suyo de 405 líneas. Posteriormente el sistema NTSC fue mejorado.


Durante los años inmediatamente posteriores a la Segunda Guerra Mundial se realizaron diferentes experimentos con varios sistemas de televisión en algunos países de Europa, incluida Francia y los Países Bajos, pero fue la Unión Soviética, que comenzó sus emisiones regulares en Moscú en 1948, el primer país del continente en poner en funcionamiento este servicio público. Cerca del 98% de los hogares en la URSS (3,2 personas por receptor) y en Francia (2,5) posee televisor, siendo el porcentaje de 94 en Italia (3,9) y 93 en los hogares de Alemania actualmente parte de la reunificada República Federal de Alemania (2,7).

Las cámaras de televisión a bordo de las naves espaciales estadounidenses transmiten a la Tierra información espacial hasta el momento inaccesible. Las naves espaciales Mariner, lanzadas por Estados Unidos entre 1965 y 1972, envió miles de fotografías de Marte. Las series Ranger y Surveyor retransmitieron miles de fotografías de la superficie lunar para su análisis y elaboración científica antes del alunizaje tripulado en julio de 1969, al tiempo que millones de personas en todo el mundo pudieron contemplar la emisión en color directamente desde la superficie lunar.

Desde 1960 se han venido utilizando también ampliamente las cámaras de televisión en los satélites meteorológicos en órbita. Las cámaras vidicón preparadas en tierra registran imágenes de las nubes y condiciones meteorológicas durante el día, mientras que las cámaras de infrarrojos captan las imágenes nocturnas. Las imágenes enviadas por los satélites no solo tienen como utilidad predecir el tiempo sino para comprender los sistemas meteorológicos globales. Se han utilizado cámaras vidicón de alta resolución a bordo de los Satélites para la Tecnología de los Recursos Terrestres conocidos también como ERTS para realizar estudios de cosechas, así como de recursos minerales y marinos.



</doc>
<doc id="17327" url="https://es.wikipedia.org/wiki?curid=17327" title="Electrodo">
Electrodo

Un electrodo es un conductor eléctrico utilizado para hacer contacto con una parte "no metálica" de un circuito, por ejemplo un semiconductor, un electrolito, el vacío del grupo (en una válvula termoiónica), un gas (en una lámpara de neón, o Argón), etc. La palabra fue acuñada por el científico Michael Faraday por composición de las voces griegas "elektron", que significa "ámbar" y de la que proviene la palabra "electricidad"; y hodos, que significa "camino".

Un electrodo en una celda electroquímica. Se refiere a cualquiera de los dos conceptos, sea ánodo o cátodo, que también fueron acuñados por Faraday. El ánodo es definido como el electrodo en el cual los electrones salen de la celda y ocurre la oxidación, y el cátodo es definido como el electrodo en el cual los electrones entran a la celda y ocurre la reducción. Cada electrodo puede convertirse en ánodo o cátodo dependiendo del voltaje que se aplique a la celda. Un electrodo bipolar es un electrodo que funciona como ánodo en una celda y como cátodo en otra.

Una celda primaria es un tipo especial de celda electroquímica en la cual la reacción no puede ser revertida, y las identidades del ánodo y cátodo son, por lo tanto, fijas. El cátodo siempre es el electrodo negativo. La celda puede ser descargada pero no recargada.

Una celda secundaria, una batería recargable por ejemplo, es una celda en que la reacción es reversible. Cuando la celda está siendo cargada, el ánodo se convierte en el electrodo negativo (-) y el cátodo en el positivo (+). Esto también se aplica a la celda electrolítica. Cuando la celda está siendo descargada, se comporta como una celda primaria o voltaica, con el ánodo como electrodo positivo y el cátodo como negativo.

En un tubo de vacío o un semiconductor polarizado (diodos, condensadores electrolíticos) el ánodo es el electrodo positivo (+) y el cátodo el negativo (-). Los electrones entran al dispositivo por el cátodo y salen por el ánodo.

En una celda de tres electrodos, un electrodo auxiliar es usado solo para hacer la conexión con el electrolito para que una corriente pueda ser aplicada al electrodo en curso. El electrodo auxiliar esta usualmente hecho de un material inerte, como un metal noble o grafito.

En la soldadura por arco se emplea un electrodo como polo del circuito y en su extremo se genera el arco eléctrico. En algunos casos, también sirve como material fundente. El electrodo o varilla metálica suele ir recubierta por una combinación de materiales diferentes según el empleo del mismo. Las funciones de los recubrimientos pueden ser: eléctrica para conseguir una buena ionización, física para facilitar una buena formación del cordón de soldadura y metalúrgica para conseguir propiedades contra la oxidación y otras características.

Para sistemas eléctricos que usan corriente alterna, los electrodos son conexiones del circuito hacia el objeto que actuará bajo la corriente eléctrica, pero no se designa ánodo o cátodo debido a que la dirección del flujo de los electrones cambia periódicamente, numerosas veces por segundo. Son una excepción a esto, los sistemas en los que la corriente alterna que se aplica es de baja amplitud (por ejemplo 10 mV) de tal forma que no se alteren las propiedades como ánodo o cátodo, ya que el sistema se mantiene en un estado pseudo-estacionario.

Los electrodos también son considerados varillas de metal cubiertas con sustancias adecuadas al tipo de soldadura. La medida de electrodos más utilizada es de y . El primer número indica el diámetro del electrodo (1,5-2,5, etc.) y el segundo número la longitud total del electrodo.




</doc>
<doc id="17336" url="https://es.wikipedia.org/wiki?curid=17336" title="Izanami">
Izanami

, es la diosa de la creación y de la muerte en la mitología japonesa y en el sintoísmo, es una diosa primordial y esposa del dios Izanagi. Junto con él creó el mundo. Es conocida también como Izana-mi, Izanami-no-mikoto o Izanami-no-kami.

Los primeros dioses invocaron a dos seres divinos a existir, el varón Izanagi y la mujer Izanami, y estos crearon la primera tierra. Para realizar esto, Izanagi e Izanami tenían una lanza decorada con joyas, llamada "Ame-no-nuboko" (lanza celestial). Con esto crearon sobre el mar una isla, y residieron en un palacio construido por ellos en esa isla. En el momento de su matrimonio, Izanami dio las gracias en primer lugar; pero Izanagi no sabía si esto era correcto. Después tuvieron dos hijos: Hiruko y Awashima, pero nacieron deformes y no fueron considerados dioses.

Pusieron a los niños en un bote en dirección al mar e hicieron una petición a los otros dioses acerca de qué hicieron mal. Ellos contestaron que la deidad masculina es la que debe dar las gracias en primer lugar durante el matrimonio. Así Izanagi e Izanami nuevamente hicieron el rito de matrimonio, pero esta vez Izanagi hablaría primero y se consumaría de manera correcta.

De su unión nacieron las "ōyashima", las "ocho grandes islas" de Japón:

Ellos engendraron seis islas adicionales y muchos dioses. Izanami murió cuando engendró a Kagutsuchi (encarnación del fuego). Fue enterrada en el Monte Hiba, en la frontera de las antiguas provincias de Izumo y Hoki. Fue tal la furia que tuvo Izanagi con la muerte de su esposa, que mató al recién nacido y de este se crearon docenas de deidades.

Decidió Izanagi hacer un viaje a Yomi ("la tierra oscura de la muerte"). Rápidamente encontró a Izanami, y le pidió que regresara con él, pero ella le dijo que era demasiado tarde, ya que había comido el alimento del inframundo y que ahora estaría en la tierra de los muertos; sin embargo trataría de convencer a los dirigentes del Yomi para que la dejaran irse y pidió a Izanagi que no entrase durante ese momento. 

Izanagi esperó y esperó, pero al final se impacientó, así que encendió una mecha y se adentró en el Yomi para buscar a su esposa, quebrando de este modo una de las reglas de la tierra de los muertos. Izanagi buscó a su esposa y cuando la encontró se horrorizó al ver su cadáver putrefacto, lo que provocó la ira de Izanami la cual mandó a los ejércitos del inframundo tras su marido. Este consiguió escapar, al salir de Yomi, cerró la entrada con una piedra y rompió el matrimonio con Izanami. Debido a esto, Izanami le lanzaría una maldición diciendo que cada día mataría a mil humanos, a lo que él respondió que de hacerlo, haría nacer a mil quinientos.

La historia y nombre de este personaje de la mitología japonesa trascendió tanto dentro del imaginario de la cultura de ese Japón que se hace presente incluso en videojuegos y anime. A continuación se darán algunos ejemplos de franquicias de anime y videojuegos influenciadas por Izanami. 




</doc>
<doc id="17341" url="https://es.wikipedia.org/wiki?curid=17341" title="Hafnio">
Hafnio

El hafnio es un elemento químico de número atómico 72 que se encuentra en el grupo 6 de la tabla periódica de los elementos y se simboliza como Hf.

Es un metal de transición, brillante, gris-plateado, químicamente muy parecido al circonio, encontrándose en los mismos minerales y compuestos, y siendo difícil separarlos. Se usa en aleaciones con wolframio en filamentos y en electrodos. También se utiliza como material de barras de control de reactores nucleares debido a su capacidad de absorción de neutrones. Recientemente, se ha convertido en el material utilizado para fabricar los transistores de los procesadores de la conocida marca Intel.

Es un metal dúctil, brillante, plateado y resistente a la corrosión; químicamente muy similar al circonio. Estos dos elementos tienen el mismo número de electrones en sus capas exteriores y sus radios iónicos son muy similares debido a la contracción de los lantánidos. Por eso son muy difíciles de separar (los procesos geológicos no los han separado y en la naturaleza se encuentran juntos) y no hay otros elementos químicos que se parezcan más entre sí. Las únicas aplicaciones para las cuales es necesario separarlos es en aquellas en las que se utilizan por sus propiedades de absorción de neutrones, en reactores nucleares.

El carburo de hafnio (HfC) es el compuesto binario más refractario conocido, con un punto de fusión de 3.890 °C, y el nitruro de hafnio (HfN) es el más refractario de todos los nitruros metálicos conocidos, con un punto de fusión de 3.310 °C. El carburo mixto de hafnio y tántalo (Ta4HfC5) es el compuesto múltiple con más alto punto de fusión conocido, 4.215 °C. 

El hafnio es resistente a las bases concentradas, pero los halógenos pueden reaccionar con él para formar tetrahaluros de hafnio (HfX). A temperaturas altas puede reaccionar con oxígeno, nitrógeno, carbono, boro, azufre y silicio.

El hafnio se utiliza para fabricar barras de control empleadas en reactores nucleares, como las que se pueden encontrar en submarinos nucleares, debido a que la sección de captura de neutrones del hafnio es unas 600 veces la del circonio, con lo cual tiene una alta capacidad de absorción de neutrones, y además tiene unas propiedades mecánicas muy buenas, así como una alta resistencia a la corrosión. Otras aplicaciones:


Se llamó hafnio por el nombre de Copenhague en latín, "Hafnia", la ciudad danesa en donde fue descubierto por Dirk Coster y George Hevesy en 1923. Poco después se predijo, usando la teoría de Bohr, que estaría asociado con el circonio, y finalmente se encontró en el circón mediante unos análisis con espectroscopia de rayos X en Noruega.

Se encuentra siempre junto al circonio en sus mismos compuestos, pero no se encuentra como elemento libre en la naturaleza. Está presente, como mezclas, en los minerales de circonio, como el circón (ZrSiO) y otras variedades de este (como la albita), conteniendo entre un 1 y un 5% de hafnio.

Debido a la química casi idéntica que presentan el circonio y el hafnio, es muy difícil separarlos. Aproximadamente la mitad de todo el hafnio metálico producido se obtiene como subproducto de la purificación del circonio. Esto se hace reduciendo el tetracloruro de hafnio (HfCl) con magnesio o sodio en el proceso de Kroll.

Es necesario tener cuidado al trabajar el hafnio pues cuando se divide en partículas pequeñas es pirofórico y puede arder espontáneamente en contacto con el aire. Los compuestos que contienen este metal raramente están en contacto con la mayor parte de las personas y el metal puro no es especialmente tóxico, pero todos sus compuestos deberían manejarse como si fueran tóxicos (aunque las primeras evidencias no parecen indicar un riesgo muy alto).



</doc>
<doc id="17367" url="https://es.wikipedia.org/wiki?curid=17367" title="Hiperenlace">
Hiperenlace

Un hiperenlace o hipervínculo (del inglés hyperlink), o sencillamente enlace o vínculo (link), es un elemento de un documento electrónico que hace referencia a otro recurso, como por ejemplo un punto específico de un documento o de otro documento. Combinado con una red de datos y un protocolo de acceso, un hipervínculo permite acceder al recurso referenciado en diferentes formas, como "visitarlo" con un agente de navegación, mostrarlo como parte del documento referenciador o guardarlo localmente.

Los hipervínculos son parte fundamental de la arquitectura de la World Wide Web, pero el concepto no se limita al HTML o a la Web. Casi cualquier medio electrónico puede emplear alguna forma de hiperenlace.

Un enlace cuenta con dos extremos, denominados anclas ("nautilus"), y una dirección. 

El enlace comienza en el ancla origen y apunta al ancla destino. Sin embargo, el término "enlace" a menudo se utiliza para el ancla origen, mientras que al ancla destino se denomina enlace de destino ("link target").
El enlace de destino más utilizado en la World Wide Web. Puede invocar a un documento, por ejemplo una página web, a otro recurso, o a una posición determinada en una página web. Este último se consigue asignando a un elemento HTML el atributo "name" o "id" en esa posición del documento HTML. El URL de la posición es el url de la página con "#atributo name" añadido.

Cuando los enlaces de destino invocan, además de texto, elementos multimedia ( audio, video, imágenes, etc. ), puede decirse que estamos navegando en un espacio hipermedia, un ámbito de interacción humana que intensifica la densidad de los mensajes, dentro de la gama exhaustiva de supuestos funcionales que aporta la Red, como por ejemplo: comunicación en tiempo real y en tiempo diferido, comunicación de una persona a una persona, de varias a una, de una a varias, de varias a varias, etc.

Un navegador web normalmente muestra un hiperenlace de alguna forma distintiva, por ejemplo en un color, letra o estilo diferente. El comportamiento y estilo de los enlaces se puede especificar utilizando lenguaje CSS.

El puntero del ratón también puede cambiar a forma de mano para indicar el enlace. En muchos navegadores, los enlaces se muestran en texto azul subrayado cuando no han sido visitados, y en texto púrpura subrayado cuando han sido visitados. Cuando el usuario activa el enlace (por ejemplo pinchando sobre él con el ratón) el navegador mostrará el destino del enlace. Si el destino no es un archivo HTML, dependiendo del tipo de archivo y del navegador y sus "plugins", se puede activar otro programa para abrir el archivo.

El código HTML contiene todas las características principales de un enlace:

Utiliza el elemento "a" de HTML con el atributo "href" y opcionalmente otros atributos como “title”, “class”, “style”, “target” o “id”:
Cuando el puntero se sitúa sobre un enlace, dependiendo del navegador, se muestra un texto informativo acerca del enlace:

El término «enlace» a menudo se utiliza para el ancla origen, mientras que al ancla destino se denomina enlace de destino ("link target"). El enlace de destino más común es un URL, utilizado en la World Wide Web. Puede invocar a un documento, por ejemplo una página web, a otro recurso, o a una posición determinada en una página web. Este último se consigue asignando a un elemento HTML el atributo "name" o "id" en esa posición del documento HTML. El URL de la posición es el URL de la página con "atributo name" añadido.

Cuando los enlaces de destino invocan, además de texto, elementos multimedia ( audio, video, imágenes, etc.), puede decirse que estamos navegando en un espacio hipermedia, un ámbito de interacción humana que intensifica la densidad de los mensajes, dentro de la gama exhaustiva de supuestos funcionales que aporta la Red, como por ejemplo: comunicación en tiempo real y en tiempo diferido, comunicación de una persona a una persona, de varias a una, de una a varias, de varias a varias, etc.





</doc>
<doc id="17369" url="https://es.wikipedia.org/wiki?curid=17369" title="Country">
Country

Country (también llamado country & western o música campirana) es un género musical surgido en los años 20 en las regiones rurales del Sur de Estados Unidos y las Marítimas de Canadá y Australia. En sus orígenes, combinó la música folclórica de algunos países europeos de inmigrantes, principalmente Irlanda, con otras formas musicales, como el "blues", el "bluegrass" y la música espiritual y religiosa, como el "gospel". El término "country" comenzó a ser utilizado en los años 1950 en detrimento del término "hillbilly", que era la forma en que se le conocía hasta entonces, terminando de consolidarse su uso en los años 1970.

El "country" tradicional, se tocaba esencialmente con instrumentos de cuerda, como la guitarra, el banjo, el violín sencillo (fiddle) y el contrabajo, aunque también intervenían frecuentemente el acordeón (de influencia francesa para la música cajún), y la armónica. En el "country" moderno se utilizan sobre todo los instrumentos electrónicos, como la guitarra eléctrica, el bajo eléctrico, los teclados, el dobro, o la "steel guitar".

La familia Carter (The Carter Family) fue la primera en grabar en disco una canción "country", junto a Jimmie Rodgers, consolidando este género musical con el nombre inicial de "hillbilly", que luego dio paso al de simplemente "country". Ambos influyeron con sus respectivos estilos a numerosos cantantes que les sucedieron. En los años 1940 fueron sobre todo cantantes como Hank Williams los que contribuyeron a su creciente popularidad. En la década de 1950, la música "country" adquirió elementos del "rock and roll" (el vigoroso "rockabilly" de Johnny Cash, Elvis Presley, Jerry Lee Lewis, Bill Haley o Buddy Holly), género que por aquel entonces vivía un enorme auge, y que aportó ritmos y melodías más desarrolladas. Si bien se pueden escuchar hoy en día toda clase de variantes del "country", el que se combina con el "rock", y más recientemente con el pop, es el que más éxito tiene ante el público masivo, en la corriente del "country pop" destacan Dolly Parton y Kenny Rogers. 

Así, el término country, actualmente es un cajón de sastre en el que se incluyen diferentes géneros musicales: el sonido Nashville (más cercano al pop de los años 1960), el "bluegrass" (popularizado por Bill Monroe y Flatt and Scruggs, basado en ritmos rápidos y virtuosísimos, interpretados con mandolina, violín y banjo), la música de westerns de Hollywood, el "western swing" (una sofisticada música basada en el "jazz" y popularizada por Bob Wills), el sonido Bakersfield (popularizado por Buck Owens y Merle Haggard), el "outlaw country", cajún, zydeco, "gospel", "Old Time" (música folk anterior a 1930), "honky tonk", "rockabilly" o Country neo-tradicional. Cada estilo es único en su ejecución, en el uso de ritmos y acordes, aunque muchas canciones han sido adaptadas para los diferentes estilos. Por ejemplo, la canción «Milk cow blues», una antigua melodía blues de Kokomo Arnold ha sido interpretada en una amplia variedad de estilos del "country", desde Aerosmith a Bob Wills, pasando por Willie Nelson, George Strait, Ricky Nelson o Elvis Presley.

Vernon Dalhart fue el primer cantante de country en tener un éxito a nivel nacional (en Estados Unidos, en MAYO de 1924, con "The Wreck of Old '97"). Otros pioneros importantes fueron Riley Puckett, Don Richardson, Fiddling John Carson, Ernest Stoneman y los grupos Charlie Poole and the North Carolina Ramblers y The Skillet Lickers.

Pero como ya se dijo, los orígenes de las grabaciones del country moderno (hillbilly) se encuentran en Jimmie Rodgers y The Carter Family ("La Familia Carter"), que están considerados por ello, los fundadores de la música country, ya que sus canciones fueron las primeras en ser registradas en soportes fonográficos, en la histórica sesión del 1 de agosto de 1927, en Bristol (Tennessee), donde Ralph Peer ejerció como técnico de sonido. Es posible categorizar a muchos intérpretes de country atendiendo a si pertenecen a la "rama de Jimmie Rodgers" o la "rama de la Carter Family".

Jimmie Rodgers incorporó al country el folk (hillbilly). Rodgers escribió y cantó canciones basadas en baladas tradicionales e influencias musicales de sur. Partió de sus propias experiencias vividas en la ciudad de Meridian (Misisipi) y en la gente pobre que conoció en trenes ("hobos" o vagabundos), en los bares o las calles, para escribir las letras de sus canciones. Desde el 26 de mayo de 1953, se celebra en Meridian el festival "Jimmi Rodgers Memorial" en el aniversario de su muerte.

Personajes patéticos, forajidos, humor, mujeres, whisky, asesinatos, muerte, enfermedades y pobreza están presentes en sus letras, temas que han sido tomados y desarrollados por sus seguidores. Músicos como Hank Williams, Merle Haggard, Waylon Jennings, George Jones, Townes Van Zandt, Kris Kristofferson o Johnny Cash han sufrido y compartido el sufrimiento de interpretar sus canciones basados en estos temas. Jimmie Rodgers cantó sobre la vida y la muerte desde una perspectiva masculina, un punto de vista que ha dominado en muchas modalidades de country. Su influencia ha sido crítica en el desarrollo del honky tonk, el rockabilly y el sonido Bakersfield.

Jimmie Rodgers es una pieza clave en la música "hillbilly", pero el artista más influyente de la "rama de Jimmie Rodgers" es Hank Williams. En su corta carrera (murió a los 29 años) dominó la escena country, y sus canciones han sido interpretadas por prácticamente todos los artistas country, tanto hombres como mujeres. Hank tuvo dos personajes: Hank Williams, el cantautor y "Luke the Drifter", el cantautor moralista y religioso. La complejidad de estos personajes se refleja en las canciones más introspectivas que escribió sobre el amor, la felicidad, el amor y los corazones rotos ("I'm so lonesome I could cry") o las más optimistas sobre la comida cajún ("Jambalaya") o sobre las típicas figuras de madera de indios que aparecen en las tiendas de cigarros estadounidenses ("Kaw-Liga"). Hank Williams llevó a la música country a otro nivel y la hizo llegar a un público más amplio, inaugurando el estilo "honky tonk" ("country" de los bares: alcohol, mujeres, y peleas...).

Su hijo, Hank Williams Jr y su nieto Hank Williams III han sido también grandes innovadores en la música country. Hank Williams Jr. fusiona rock con outlaw country, mientras que Hank Williams III va más allá, rozando el psychobilly y el death metal.

The Carter Family fue el otro descubrimiento de Ralph Peer. En sus comienzos formaban el grupo A. P. Carter (voz), su mujer Sara (voz, arpa y guitarra) y su cuñada Maybelle (guitarra). Desarrollaron una larga carrera musical. A. P. contribuyó con un montón de canciones y baladas recolectadas durante las excursiones que realizaba alrededor de su casa en Maces Springs, (Virginia). Además, al ser un hombre, hizo posible el que Sara y Maybelle se dedicasen a la música sin que ello supusiese un estigma para ellas. Sara y Maybelle se dedicaban a realizar arreglos a las canciones que A. P. recolectaba, además de escribir sus propios temas. Ellas fueron las precursoras de toda una serie de cantantes femeninas de country como Kitty Wells, Patsy Cline, Loretta Lynn, Skeeter Davis, Tammy Wynette, Dolly Parton, Taylor Swift, Linda Ronstadt, Emmylou Harris o June CarterCash (hija de Maybelle y posteriormente mujer de Johnny Cash).

El bluegrass continuó con la tradición de las antiguas bandas de instrumentos de cuerda estadounidenses y fue inventado, en su forma original, por Bill Monroe. El término "bluegrass" fue tomado del nombre de la banda que acompañaba a Bill: The Blue Grass Boys. La primera grabación que realizaron fue en 1945: Bill Monroe (mandolina y voz), Lester Flatt (guitarra y voz), Earl Scruggs (banjo de cinco cuerdas), Chubby Wise (violín) y Cedric Rainwater (contrabajo). El grupo fue el referente para todas las bandas de bluegrass que les siguieron. De hecho, muchas de las primeros y más famosos músicos de bluegrass fueron o miembros, alguna vez, de The Blue Grass Boys (como Lester Flatt & Earl Scruggs, Jimmy Martin y Del McCoury) o tocaron con Monroe ocasionalmente (como Sonny Osborne, The Stanley Brothers o Don Reno). Además, Monroe fue una gran influencia para Ricky Skaggs, Alison Krauss, Emmylou Harris o Sam Bush (estos últimos del grupo de la Nash Ramblers), quienes mezclaron elementos del folky con el bluegrass.

Durante la década de 1960 la música country se convirtió en una industria, centrada en Nashville (Tennessee), que movía millones de dólares. Bajo la dirección de productores como Chet Atkins, Owen Bradley y, posteriormente, Billy Sherrill, el denominado sonido Nashville acercó el country a un público más diverso. El sonido tomó prestados muchos elementos del pop de los década de 1950: voces suaves acompañadas de una sección de cuerdas y coros vocales. Los artistas más importantes fueron Ernest Tubb, Patsy Cline, Jim Reeves y, posteriormente, Tammy Wynette, Loretta Lynn, Dolly Parton y Charlie Rich. Debido a que la música country tenía una gran variedad estilística, hubo muchas voces críticas que señalaron que el sonido Nashville estaba acotando esta diversidad.


En los comienzos de su carrera musical, Elvis Presley, el guitarrista Scotty Moore y el contrabajista Bill Black crearon el estilo "rockabilly" en la compañía Sun Records de Memphis. Durante un descanso en una sesión de grabación, Presley comenzó a cantar acompañado de su guitarra, el "blues" "That's all right Mama" pero con un ritmo acelerado, al cual se sumaron los otros dos músicos de sesión acompañándolo en ritmo de "country", de cuya mezcla surgió el "rockabilly".

Posteriormente, Presley grabó a lo largo de su carrera numerosos temas de corte "country" como "Blue Moon of Kentucky", "Milkcow boogie blues","Old Shep", "Tiger Man", "Guitar Man" y "Kentucky Rain", entre otras, así como también el álbum "Elvis Country, I'm 10.000 years old".





</doc>
<doc id="17375" url="https://es.wikipedia.org/wiki?curid=17375" title="Sociedad de las Naciones">
Sociedad de las Naciones

La Sociedad de las Naciones (SDN) o Liga de las Naciones fue un organismo internacional creado por el Tratado de Versalles, el 28 de junio de 1919. Se proponía establecer las bases para la paz y la reorganización de las relaciones internacionales una vez finalizada la Primera Guerra Mundial. Aunque no logró resolver los graves problemas que se plantearon en los años 20 y 30, es importante porque fue la primera organización de ese tipo de la historia y el antecedente de la ONU.

La Sociedad de las Naciones consiguió algunos éxitos en su labor, ayudó a solventar pacíficamente algunos conflictos en el período inmediato de posguerra y tuvo su apogeo en el período 1924-1929. Durante ese periodo se firmó el Tratado de Locarno en 1925, se produjo el ingreso de Alemania en la Sociedad en 1926, y se firmó el Pacto Briand-Kellogg en 1928. Sin embargo, cuando la situación internacional se enturbió tras la depresión de 1929, la Sociedad de las Naciones se mostró totalmente incapaz de mantener la paz.

La SDN se basó en los principios de la seguridad colectiva, el arbitraje de los conflictos y el desarme. El Pacto de la SDN (los 26 primeros artículos del Tratado de Versalles) fue redactado en las primeras sesiones de la Conferencia de París, que comenzaron el 18 de enero de 1919, por iniciativa del Presidente de los Estados Unidos, Woodrow Wilson.

El 15 de noviembre de 1920 se celebró en Ginebra la primera asamblea de la sociedad, con la participación de 42 países.

Tras el final de la Segunda Guerra Mundial a mediados del siglo XX, la SDN fue disuelta el 18 de abril de 1946, siendo sucedida por la Organización de las Naciones Unidas (ONU). En realidad no fue una sucesión de un organismo internacional por otro. La experiencia de la Sociedad de las Naciones es lo más cercano a la actual ONU, pero dicho antecedente no fue ni siquiera mencionado por los redactores de la Carta ya que, al querer reestructurar el mundo postbélico de la Segunda Guerra Mundial, los Estados vencedores optaron por hacer desaparecer la Sociedad de las Naciones y crear una organización internacional enteramente nueva.

La Sociedad de las Naciones se fundó inmediatamente después de la Primera Guerra Mundial. Constaba originalmente de 42 países, 26 de los cuales no eran europeos. Alcanzó el número máximo cuando tuvo 57 países miembros. La Sociedad se creó porque tras la contienda en Francia, el Reino Unido y los EE. UU. la corriente de la opinión pública creía que una organización mundial de naciones podría conservar la paz y prevenir una repetición de los horrores de la guerra de 1914 a 1918 en Europa, en vista que los pactos de la diplomacia tradicional habían fracasado en este empeño. Entonces se creyó que un órgano mundial eficaz parecía posible porque las comunicaciones habían mejorado y existía una mayor experiencia de trabajo en conjunto en organizaciones internacionales. La coordinación y la cooperación para el progreso económico y social adquirirían importancia.

La creación de la Sociedad de las Naciones tiene su origen en uno de los Catorce Puntos de Woodrow Wilson para superar los efectos de la guerra y conseguir una paz duradera. Fue una consecuencia a la reacción de las diferentes naciones contra los horrores que provocó la Primera Guerra Mundial. Millones de muertos, inválidos, población civil desplazada, pobreza, deuda de guerra, la caída de cuatro imperios —el alemán, el austrohúngaro, el ruso y el otomano— que a su vez provocó inestabilidad política. La Sociedad de las Naciones pretendía impedir una repetición de los hechos que dieron lugar a la guerra, como la falta de cooperación, la existencia de pactos secretos entre estados o la ignorancia de los pactos internacionales. Por eso se quiso crear un organismo a través del cual las naciones pudiesen resolver sus disputas por medios pacíficos en lugar de militares, evitando aquellas causas que llevaron a la guerra:

La propia exposición de motivos del Pacto de la Sociedad de las Naciones proclama 

La sesión de apertura de la Liga de Naciones
Ginebra, Suiza, 15 de noviembre de 1920.

La Sociedad tenía dos objetivos básicos. En primer lugar, buscaba preservar la paz por medio de una acción colectiva, para lo cual las controversias se remitirían al Consejo de la Sociedad con fines de arbitraje y conciliación. Si fuera necesario, se usarían sanciones económicas y luego militares, por lo cual los miembros se comprometían a defender a otros miembros ante una agresión. En segundo lugar, la Sociedad deseaba promover la cooperación internacional en asuntos económicos y sociales.

La delegación japonesa apoya la inclusión del principio de "igualdad racial" en el pacto de la Liga de las Naciones, pero debe hacer frente a una fuerte oposición de Australia y, en menor medida, de los Estados Unidos y del Reino Unido. A lo largo de los debates, la prensa norteamericana y británica criticó duramente a Japón, acusado de querer facilitar la emigración de sus ciudadanos.

Por el contrario, estas discusiones aumentan las esperanzas de las poblaciones que sufren de medidas de discriminación o segregación racial, en particular los afroamericanos. El intelectual afroamericano W.E.B. Du Bois veía a Japón como un actor de la revancha de los pueblos de color: “Dado que los africanos negros, los indios morenos y los japoneses amarillos luchan por Francia e Inglaterra, podrían salir de este sangriento desorden con una idea nueva de la igualdad esencial de los hombres.

Sin embargo, el historiador japonés Matsunuma Miho señala que "el objetivo de Japón no era lograr la igualdad de todas las razas. Su gobierno temía sobre todo que un estatuto inferior asignado a sus ciudadanos perjudicara su posición en el futuro orden internacional." Los nacionales japoneses son objeto de medidas discriminatorias humillantes en los Estados Unidos, Canadá y Australia. Además, el propio Japón tiene una política de discriminación y represión contra chinos y coreanos, cuyas manifestaciones independentistas de marzo de 1919 fueron aplastadas.

El fracaso de la iniciativa provocó una gran cólera y resentimiento popular en Japón hacia Occidente, especialmente hacia los anglosajones.

Durante las negociaciones en la Conferencia de Paz de París se incluye en la Parte I del Tratado de Versalles la creación de la SDN (Sociedad de las Naciones).

Los países integrantes originales eran los 32 miembros del anexo al Pacto y los 13 de los Estados invitados a participar, quedando abierto el ingreso futuro al resto de los países del mundo. Las excepciones fueron Alemania, Turquía y la URSS. Fue permitido asimismo, en el caso del Reino Unido, el ingreso de sus dominios y colonias, como India, Sudáfrica, Australia y Nueva Zelanda. Según el artículo 1, la organización también estaba abierta al ingreso de dominios o colonias, no incluidas en el anexo final del pacto, que tuvieran autogobierno, fuera aceptada por dos tercios de la Asamblea y cumplieran con los compromisos de la Sociedad.

Los países excluido en el primer momento se incorporaron con posterioridad: Austria en 1920, Alemania —por medio del Tratado de Locarno— en octubre de 1925 (que posibilitó su ingreso como miembro en septiembre de 1926); Turquía y la Unión Soviética en 1934. Estados Unidos nunca se incorporó a la Sociedad, debido a la negativa del Congreso estadounidense a participar en ella, aunque sí perteneció a sus organismos afiliados.

En el caso de retirada de la Sociedad, el pacto, en su primer artículo, indicaba que cualquier país debía anunciar su intención de retirarse dos años antes de hacerse efectiva, teniendo que estar al corriente de todas las obligaciones internacionales además de las firmadas en el pacto. Entre los miembros que se retiraron destacan el Imperio Japonés y Alemania en 1933, Italia en 1936 o la Unión Soviética que fue expulsada en 1939.

Estaba organizada en tres organismos, según el Tratado de Versalles:


Integrantes del Consejo de la Sociedad de Naciones (1920-1940)


Organismos Internacionales asociados o afiliados a la SDN:

Las lenguas oficiales eran el francés y el inglés en 1920. Pero a principios de 1920 surgió la propuesta de adoptar el esperanto como lengua de trabajo. Diez delegados aceptaron la proposición y sólo uno la rechazó, el delegado francés Gabriel Hanotaux. Hanotaux no quería que el francés perdiera su posición como lengua franca de la diplomacia y veía en el esperanto una seria amenaza. Dos años después la Sociedad recomendaba a sus Estados miembros que incluyeran el esperanto en sus programas educativos.

Países fundadores Fueron los siguientes: 


Países integrados con posterioridad 


Las principales acciones estaban enfocadas a fomentar una política mundial de desarme y seguridad colectiva. La reforma llevada a cabo por el Protocolo de Ginebra (Protocolo de resolución pacífica de conflictos internacionales 1924) hizo obligatorio el arbitraje en caso de conflicto. El rechazo del arbitraje por una de las partes en conflicto le hacía reconocer el carácter de agresor. Para ello se podrían aplicar automáticamente sanciones militares. Otra novedad del Pacto de la Sociedad era la obligación de los Estados miembros de publicar sus tratados y registrarlos en la SDN.

Logró una solución pacífica de los siguientes conflictos:







Sin embargo, éstos fueron de menor importancia frente a conflictos en los cuales las grandes potencias o grupos de países tenían intereses ya declarados, lo cual se dio en los siguientes casos decisivos:






El Pacto de la Sociedad de las Naciones se anexó a todos los demás tratados. Su gran objetivo era hacer posible una seguridad colectiva que garantizase la integridad de todos los estados, fuertes y débiles, el arbitraje de los conflictos internacionales y el desarme. Fue el elemento clave de la propuesta del presidente Wilson. Pero la negativa del aislacionista Senado norteamericano al ingreso de EE. UU. y la exclusión de Alemania y la URSS, que no ingresaron respectivamente hasta 1926 y 1934, limitaron desde un principio su potencialidad.

Los años treinta marcaron su fracaso definitivo. Las agresiones de las potencias fascistas y militaristas mostraron su ineficacia. Alemania y Japón abandonaron la Sociedad en 1933, e Italia en 1936. La URSS fue expulsada en 1939. El inicio de la segunda guerra mundial vino a certificar la muerte de la primera organización universal de naciones.

El Tratado de Versalles entregaba a la SDN la administración directa y temporal de diversos territorios agrupados según sus dos distintos modos de administración:

Ex territorios de Alemania en Europa, administrados por el Secretariado



Territorios fuera de Europa pertenecientes al Imperio Alemán e Imperio Otomano por medio de los mandatos de la SDN

Supervisados por la Comisión Permanente de los Mandatos, organismo internacional, compuesto originariamente por 45 países, creado por la Conferencia de París el 24 de abril de 1919. Estos territorios fueron divididos en tres categorías en función de su grado de desarrollo y fueron cedidos a las potencias aliadas:


La SDN tuvo una serie de problemas desde sus comienzos. El primero fue la negativa del Senado de los Estados Unidos a aprobar el Tratado, que hizo que el gobierno estadounidense nunca se adhiriera a la SDN, dejando fuera de su alcance a una potencia mundial de la época. El segundo fue la sistemática exclusión de Alemania y Turquía (parte del Imperio otomano hasta la contemporánea partición del Imperio otomano), debido a su condición de países derrotados en la Primera Guerra Mundial, además de haberse determinado oficialmente la "responsabilidad exclusiva" de Alemania por el estallido de dicha contienda, por lo cual excluir a Alemania de la SDN fue decidido como un "castigo" a imponer por los vencedores. También fue excluida por muchos años la Unión Soviética, dado el carácter comunista de su gobierno (lo que fomentó la creación de un cordón sanitario de estados anticomunistas para evitar la propagación de la revolución bolchevique) y por el tardío reconocimiento diplomático de los vencedores de la Primera Guerra Mundial hacia el gobierno de Moscú (reconocimiento que sólo llegó poco antes de 1930), con la consiguiente pérdida de prestigio de la SDN.

Ya en 1923 se evidenció la debilidad de la organización ante sus propios impulsores cuando Francia ocupó la región alemana del Ruhr para exigir reparaciones de guerra, sin que este hecho pudiera ser siquiera condenado por la SDN a pesar de haberse prohibido oficialmente cobrar deudas de guerra mediante esta clase de operaciones militares. El hecho que Francia fuese precisamente un vencedor de la Gran Guerra sirvió para legitimar y justificar esta ocupación, dañando el prestigio de la SDN.

La invasión de Manchuria por parte de Japón en 1931 fue el segundo gran golpe que recibió la SDN. Condenado por la agresión, Japón abandonó la organización en 1933 pero sin haber recibido sanciones efectivas de los miembros de la SDN y sin haber restaurado a China el territorio de Manchuria. Alemania había sido admitida en 1926 como miembro de la SDN en calidad de "estado amante de la paz", pero el régimen nazi siguió en 1933 el mismo camino de Japón y se retiró de la SDN.

La Sociedad de las Naciones nunca consiguió la autoridad suficiente para imponer a sus miembros sus resoluciones en forma obligatoria. Esto se repitió en 1934 cuando la Asamblea General dicta una resolución referente a la Guerra del Chaco, la cual se desarrolló entre 1932 y 1935, teniendo como Estados beligerantes a la República del Paraguay y a la República de Bolivia. El fallo definitivo de la Asamblea General fijó posición en la disputa territorial favoreciendo a Bolivia, pero la República del Paraguay desafió la autoridad de este organismo y se retiró de la Sociedad de las Naciones, para luego resultar vencedor de la citada contienda.

Las invasión de Abisinia por Italia Fascista en 1935 reveló que la SDN carecía de autoridad efectiva para guiar una acción internacional contra las agresiones entre Estados, siendo completamente impotente para asegurar un mínimo de respeto a la paz mundial. Pese a que la SDN aprobó sanciones económicas contra la Italia Fascista en 1935, estas no fueron apoyadas por muchos países, y tampoco privaban a Italia de obtener materias primas indispensables para su campaña militar, por lo cual las sanciones resultaron inútiles y desprestigiaron aún más a la SDN.
Finalmente, los dos principales miembros de la SDN (Francia y Gran Bretaña) prefirieron seguir por su cuenta una política de "apaciguamiento" hacia las agresivas potencias fascistas, y por ello se negaron a tomar acciones más severas tratando de evitar (en vano) que Mussolini retirara a Italia de la SDN y se aliase con Hitler. Como resultado de esta política franco-británica, cuestiones como la reocupación alemana de Renania no pudieron ser tratadas ante la SDN, quedando esta entidad cada vez más marginada de los grandes asuntos de política internacional. Un caso de especial importancia fue el de la intervención extranjera en la Guerra Civil Española, que motivó la creación del Comité de No Intervención para sustituir en cierto modo a la SDN, aunque finalmente el conflicto español acaparó las reuniones del organismo, dadas las denuncias del gobierno español y la implicación de otros gobiernos de Estados representados, unos a favor de la República Española y otros a favor del bando sublevado liderado por el general Franco. La crisis española abrió así el camino de la Segunda Guerra Mundial, consumándose el fracaso de la SDN.

Tras el fin del conflicto español y la invasión alemana a Polonia, con la consiguiente declaración de guerra por parte de Reino Unido y Francia, mostró el fracaso final de la SDN al ocurrir el principal evento que se deseaba impedir con la creación de la Sociedad: una nueva guerra a gran escala en Europa. Con ello la SDN perdió casi toda su razón de existir. El último gran acto oficial de la SDN fue expulsar de su seno a la URSS cuando tropas soviéticas invadieron Finlandia en diciembre de 1939.

La Sociedad de las Naciones sólo mantuvo algunos servicios de ayuda a refugiados y de regulaciones laborales, en forma reducida, durante los años de la Segunda Guerra Mundial; para estos fines continuó utilizando su sede en la neutral Suiza y teniendo como último secretario general al irlandés Seán Lester, con un equipo de un centenar de servidores. La SDN fue disuelta oficialmente en abril de 1946 aunque su última Asamblea fue celebrada una vez finalizada la Guerra, entre el 8 y el 18 de abril de 1946 y su disolución legal no tuvo lugar hasta el 17 de julio del año siguiente, sus archivos y bienes fueron entonces traspasados a la recién creada ONU. En años recientes la SDN ha sido objeto de diversos estudios, tras haber permanecido durante décadas lejos de la atención del historiador.




</doc>
<doc id="17376" url="https://es.wikipedia.org/wiki?curid=17376" title="Mandato de la Sociedad de las Naciones">
Mandato de la Sociedad de las Naciones

Un Mandato de la Sociedad de las Naciones se refiere a varios territorios establecidos en el artículo 22 del Tratado de Versalles, y que habían sido previamente controlados por países derrotados en la Primera Guerra Mundial: los territorios coloniales del Imperio alemán y las antiguas provincias del Imperio otomano divididas en la partición del Imperio otomano.

El mandato era la entrega a potencias aliadas (y vencedoras en la Gran Guerra) de dichos territorios para su administración y en algunos casos eventual independencia. Dichos mandatos fueron supervisados por la "Comisión Permanente de Mandatos de la Sociedad de las Naciones". Los mandatos eran diferentes de los protectorados, en el que la potencia administradora asumía obligaciones con los habitantes del territorio y con la Sociedad de las Naciones.

El nivel exacto de control de la potencia administradora sobre cada mandato era decidida en unos principios individuales por la Sociedad de las Naciones; sin embargo, de forma general, la potencia administradora tenía prohibido construir fortificaciones y crear un ejército dentro del territorio, y tenía que presentar un informe anual sobre el territorio a la Sociedad de las Naciones.

A pesar de esto, los mandatos fueron vistos como colonias "de facto" de los imperios de las naciones victoriosas en la primera guerra mundial.

La categoría de distribución de los mandatos dependía del nivel de desarrollo de cada población, y era la siguiente:




Los mandatos fueron reemplazados por la administración fiduciaria, supervisada por el Consejo de Administración Fiduciaria de las Naciones Unidas, en 1945.



</doc>
<doc id="17382" url="https://es.wikipedia.org/wiki?curid=17382" title="Concepción">
Concepción

El término Concepción puede referirse a:



























</doc>
<doc id="17388" url="https://es.wikipedia.org/wiki?curid=17388" title="Chantal Akerman">
Chantal Akerman

Chantal Akerman (Bruselas, 6 de junio de 1950 - París, 5 de octubre de 2015) fue una directora de cine belga, artista y profesora de cine en la European Graduate School. Sus películas están basadas en observaciones sobre la vida cotidiana, la necesidad de comer, la sexualidad, el aislamiento y la política de exclusión en el siglo XX. Se dio a conocer más al público con la película "Jeanne Dielman, 23 quai du Commerce, 1080 Bruxelles" (1975), donde ejemplifica su modo de evadir la narrativa convencional mediante la elipsis. En los últimos años destacó asimismo por la escritura.

Akerman nació en el seno de una familia de Bruselas, que era judía practicante. Sus abuelos y su madre fueron enviados a Auschwitz; solo se salvó su madre. La ansiedad materna y el exilio serán temas recurrentes en su filmografía. 

Akerman señaló que a los 15 años, tras ver "Pierrot le fou" (1965) de Jean-Luc Godard, decidió rodar ella misma. A los 18 entró en el Institut National Supérieur des Arts du Spectacle et des Techniques de Diffusion, centro de enseñanza de cinematografía belga. 

Enseguida, lo abandonó, y Akerman hizo un cortometraje de 35mm en B/N, con sus ahorros: "Saute ma ville" (1968), dura 13 minutos y tiene como protagonista a una joven (ella misma como actriz) y su relación conflictiva con las cosas cotidianas. En 1971, fue premiada "Saute ma ville" en el festival de Oberhausen. Ese mismo año se trasladó a Nueva York, donde estuvo hasta 1972. Vio cine independiente, especialmente de Stan Brakhage, Jonas Mekas, Andy Warhol y Michael Snow.¹ Obras de 1972, como "Hotel Monterey" y cortos como "La Chambre 1" y "La Chambre 2", revelan su nuevo aprendizaje. Inició entonces una colaboración con la cineasta Babette Mangolte.

En 1973, Akerman regresó a Bélgica. En 1974, fue reconocida por la crítica con su "Je tu il elle", película experimental sobre la agresión. Su manifiesto es un clásico moderno. "Jeanne Dielman, 23 quai du Commerce, 1080 Bruxelles" (1975), alabado por el "New York Times" como "la primera obra maestra de lo femenino en la historia del cine"; trata de una eficiente ama de casa que se ve obligada a prostituirse para poder vivir con su hijo. Muy pronto, en 1978, concluyó "Las citas de Anna", que es un film asimismo destacado por su belleza y precisión. 

En 1991, presentó su largometraje "Noche y día", donde invierte de entrada el dúo día y noche: es la historia de un trío amoroso, con dos chicos y una joven, que viven en sus relaciones pasionales separadas en dos mitades del día, casi sin dormir. La película, como dijo Akerman, no tiene nada que ver con el modo de hacer de Truffaut, pero hay un homenaje indirecto al tema de "Jules et Jim" y otro expreso con un libro que lleva un protagonista con un relato de Truffaut. En ese mismo año fue miembro del jurado del 41 Film Festival de Berlín.

Desde entonces filmó, entre otras películas: "La mudanza" (1992), "Contra el olvido" (1992), "Retrato de una joven a finales de los sesenta en Bruselas" (1993), "Un diván en Nueva York" (o "Romance en NY', 1996) o "Del otro lado" (2002), aparte de sus colaboraciones con Eric de Kuyper y otros filmes vinculados temáticamente con el exilio y la falta de comunicación. 

Akerman siempre rechazó, pese al carácter escogido de su obra, que su cine se esconda en guetos. En 2011, presentó su versión de la novela de Joseph Conrad, "La locura de Almayer". Ha sido una de las cineastas más originales de la modernidad europea (en otro registro que Agnés Varda, mayor que ella).

Además trabajó destacadamente como actriz en varios de sus filmes y escribió cuatro libros: "Hall de Nuit" (1997); "Une famille à Bruxelles (1998); "Autoportrait", (2004), excelente autorretrato como cinasta; y "Ma mère rit"(2013), donde aparece su trama familiar y sus vivencias complejas con la herencia materna. Su escritura es precisa, sortijada, clara.

La trama de los filmes de Akerman es mínima (a veces, hasta cabe decir que no existe), y sin ser agobiantes, sus elipsis son una de las marcas de identidad creadora: expresan de un modo claro su no sentirse confortablemente en el mundo. 

Se ha asociado el cine de Akerman con el caminar (es un cine muy cinético). En ese punto se le puede emparejar con Godard o Wenders, aunque su cine sea más despojado (y más cuidado en general). 

Es el suyo un modo de transitar muy vasto, pues además de los desplazamentos fílmicos de la autora, sus personajes atraviesan diferentes espacios, incluyendo el que separa los hechos de la vida cotidiana con las fantasías; lo normal en ese tránsito puede pasar a ser inquietante y desde luego lleno de imaginaciones, ya desde su primer mediometraje, en donde la protagonista juega y canta y se mueve en su casa, pero donde parece que la ficción se inclina a la catástrofe tras una brusca ruptura (se ha hablado de lo "cotidiano hiperrealista¨, en Akerman). 

Chantal Akerman vuelca en sus filmes intimidad, precisión expresiva y rigor, con una voluntad creadora que se mueve inseparablemente en los territorios de la ficción y del documental, pues funde esos dos registros con una naturalidad original, dotando al relato de realismo y de poesía.

Correspondencias, cuadernos de viaje... Su vagabundeo está relacionado con el tema del exilio, esto es, del peregrinaje, la huida o la destrucción. En España se ha publicado el álbum "Exilios" (Intermedio, 2011), con cuatro películas suyas, donde va recogiendo huellas de vidas distintas viajando por todo el mundo: por Rusia, Ucrania y Polonia, en "Del Este", 1993, donde casi sin relato alguno da cuenta de una honda monotonía; por la Norteamérica sureña, "Sur" (1999), y su racismo mostrado en las consecuencias y ecos de un asesinato; por el norte conflictivo de México, fronterizo con EUA, "Del otro lado" (2002); por Israel, "Allá" (2006), y concretemante en Tel Aviv, donde se describe la espera y el silencio: en esta ciudad la protagonista se encierra en un piso para describir la situación israelí indirectamente, a través del paso de las horas de una mujer que habla por teléfono y da datos dispersos sobre su vida, mientras la cámara ofrece vistas de los edificios de enfrente y de sus habitantes habituales. Su cuarteto es una muestra de la peregrinación y el vacío modernos.

Esta visión medio en fuga repercute en su modo de contar las relaciones amorosas: no sólo en los malos encuentros, de su film experimental "Je, tu, il, elle" (1974), sino ya en "Las citas de Anna" ("Les rendez-vous d’Anna", 1978), en "Toute une nuit" (1982) o en "Nuit et jour" ("Noche y día", 1991). Los encuentros, en muchas ocasiones, conducen al retiro. 

Y ahí está el tema de la soledad. La propia directora hace de protagonista en "L’homme a la valise" (1983), y se recluye durante semanas en un habitáculo minúsculo (como ya en "Saute ma ville") donde vive, se refugia y vigila lo que ocurre en el exterior: sus vecinos vienen a ser representantes del mundo. 

El viaje vital, azaroso y a veces esperanzado, puede con todo asociarse al periplo romántico: así sucede con Akerman, aunque parezca muy contemporáneo y poco sentimental. Entre sus referencias parecen encontrarse el pintor Edward Hopper y artistas como Warhol, además de Godard. Pero Akerman hace de sus silencios y de los planos quietos un lugar de espera de notable belleza, y claramente vivos, abiertos a cierta ironía dentro de un mundo de monotonía impuesta.

Su mundo en parte es una reflexión a partir de su vida. Akerman prefiere mostrar a personajes que están en vías de llegar a ser ellos mismos, pero que aún son informes (como sucede en "Noche y día"). Un film abiertamente autobiográfico es "Portrait d'une jeune fille de la fin des années 60 à Bruxelles" (1994), donde desarrolla con claridad esa visión de la pre-madurez. Ella constata lo que 'hay' en este momento fugaz. En un autorretrato para la TV "Chantal Akerman par Chantal Akerman" (1996), decía al final: “Mi nombre es Chantal Akerman. Vivo en Bruselas; es verdad. Es verdad”.





</doc>
<doc id="17390" url="https://es.wikipedia.org/wiki?curid=17390" title="Instituto Nacional de Cultura del Perú">
Instituto Nacional de Cultura del Perú

El Instituto Nacional de Cultura del Perú (INC) fue un organismo público descentralizado, con personalidad jurídica, de derecho público interno y con autonomía técnica, administrativa, económica y financiera de la República del Perú. Tenía la finalidad de reafirmar la identidad nacional mediante la ejecución descentralizada de acciones de protección, conservación y promoción, puesta en valor y difusión del patrimonio y las manifestaciones culturales de la Nación para contribuir al desarrollo nacional con la participación activa de la comunidad y los sectores público y privado. 

Fue fundado durante el gobierno del general Juan Velasco Alvarado, empezando en 1972 como un organismo público descentralizado del sector educación, mediante el Art. N° 49 del Decreto Ley N°18799. 

Tuvo su sede principal en Lima y su última directora fue Cecilia Bákula.

El Reglamento de la Organización y Funciones del INC estuvo amparado por el Decreto Supremo Nº 027-2001-ED del 20 de abril de 2001.

El 1 de octubre de 2010 mediante una fusión la estructura orgánica del INC pasó a convertirse en la estructura del Ministerio de Cultura, de acuerdo a lo establecido en el Decreto Supremo Nº 001-2010-MC.

Como menciona César Coloma ""Mediante el D.S. 48 del 24 de agosto de 1962 fue creada la Casa de la Cultura en cumplimiento del Decreto Supremo N° 48, al que se le dio fuerza de ley por el D.L 14479, del 10 de junio de 1963 mediante la creación de la Comisión Nacional de Cultura. Esta última norma disponía: "El director de la Casa de la Cultura del Perú se encargará de ejecutar los acuerdos del directorio de la Comisión Nacional de Cultura Y dirigirá los organismos administrativos pertinentes, siendo responsable aqemás del movimiento económico y de los bienes muebles e inmuebles de la Comisión Nacional de Cultura". La sede limeña fue inaugurada el 24 de julio de 1963, en la Casa de Pilatos, especialmente restaurada para este efecto por el arquitecto don Héctor Velarde Bergmann. La Ley 15624 del 24 de setiembre de 1965 disolvió la comisión creándose el Sistema Nacional de Fomento de la Cultura, integrado por el Consejo Superior de Fomento de la Cultura, la Casa de la Cultura del Perú y las casas de la cultura departamentales. El Consejo Superior de Fomento de la Cultura estuvo constituido por el ministro de Educación, los directores del Archivo Nacional, de la Biblioteca Nacional, de la Casa de la Cultura del Perú, del Conservatorio Nacional de Música, de la Escuela Nacional Superior Autónoma de Bellas Artes y delegados de las universidades. Dependian directamente. de la Casa de la Cultura del Perú los museos estatales, la Orquesta Sinfónica Nacional, el Patronato Nacional de Arqueología y el Teatro Nacional. La institución tuvo una profusa actividad editorial, además de la notable revista "Cultura y pueblo" (1964-1970) y la "Revista peruana de cultura" (Lima, 1963-1970), y en mimeógrafo el "Boletín informativo de la Casa de la Cultura del Perú" (1964-1969). La Casa de la Cultura del Perú fue disuelta por el D.L 18799 del 9 de marzo de 1971, al crearse el Instituto Nacional de Cultura"".







</doc>
<doc id="17394" url="https://es.wikipedia.org/wiki?curid=17394" title="Rancagua">
Rancagua

Rancagua es una comuna y ciudad de Chile, capital de la provincia de Cachapoal y de la región del Libertador General Bernardo O'Higgins. Debido a la gran expansión de la ciudad durante los últimos años, ha llegado a formar junto a Machalí y Gultro la llamada Conurbación Rancagua, que es la octava aglomeración urbana más poblada del país.

Antes de la conquista española, el valle del río Cachapoal estaba habitado por los cachapoales y es probable que el nombre de Rancagua sea una castellanización de la palabra en mapudungun "rangka" ("Lasthenia kunthii") y "we", en castellano "lugar", significando "lugar de rancas". También podría derivar de "rangkül", una especie de caña o carrizo y "we", "lugar"; es decir, significa «lugar en que hay cañas», o simplemente «cañaveral».

Con el paso del tiempo, Rancagua ha recibido una serie de denominaciones antonomásicas de origen popular, a propósito de características culturales de la ciudad y de su entorno; algunas de éstas son:

Los picunches "promaucaes" fueron los primeros habitantes conocidos del Valle de Rancagua. Los promaucaes construyeron un pucará en el Cerro La Compañía y un puente colgante de cuerda y mimbre sobre el río Cachapoal, que facilitó las expediciones incaicas hacia el sur de su imperio, quienes utilizaron y fortificaron el pucará. La presencia incaica no significó la pérdida de autoridad local, ni de sus tierras a los caciques picunches.

Durante la conquista en 1610 el pueblo indígena de Rancagua ya era administrado por el teniente español Francisco Gutierrez de Caviedes de La Torre quien había llegado a Chile en 1605 aproximadamente a instalarse en la Estancia original del Capitán español Alonso de Córdoba y le compró tierras en 1617.

Es así como el reducto de Rancagua fue conservado por los ascendientes del cacique Tomás Guaglén, el último de los picunches, que ejerció dominio hasta, que por propia voluntad, cedió territorios para la fundación de la futura ciudad de Rancagua, además de la donación de veinte cuadras de la Estancia El Carmen por don Gabriel de Soto y Córdova, quien heredó a su sobrino Francisco de Soto, más adelante llamada Fundo El Puente, cuya casa patronal existe hasta hoy.

La fundación se realizó el 5 de octubre de 1743 con el nombre de Villa Santa Cruz de Triana y estuvo a cargo del Presidente de la Audiencia y Gobernador del Reino de Chile, José Antonio Manso de Velasco, siendo aprobado por Real Cédula del 29 de julio de 1749.

Tal como había hecho en la planificación de diversas ciudades coloniales, Manso de Velasco proyectó la ciudad según el plano ortogonal (también llamado "damero"), muy utilizado en España, que consiste en un plano similar a un tablero de ajedrez; 8 cuadras por 8 cuadras. Cada cuadra se dividía en 4 partes, denominadas solares. La villa estaba rodeada en sus cuatro lados por acequias (en las llamadas cañadillas). Se destacó la plantación de algunos árboles, especialmente álamos, que dieron origen años después a la Alameda.

A partir de esos años, la villa comenzó a organizarse. El Protector de la Villa, Martín Gregorio de Jáuregui, repartió las cuadras y solares demarcados, para las diversas funciones de la época. Entre esas designaciones, donó 2 cuadras (8 solares) a la orden de los franciscanos. Dicho terreno, actualmente constituye las dos manzanas delimitadas por las calles Estado, Almarza, Millán e Ibieta. En el año 1807, los padres Franciscanos levantaron en la actual esquina suroriente de las calles Estado e Ibieta su templo, la Iglesia San Francisco.

La ciudad de Rancagua fue testigo de la batalla que marca el fin de los primeros proyectos republicanos por la Independencia de Chile. El hecho ocurrió los días 1 y 2 de octubre de 1814. El brigadier Bernardo O'Higgins, al mando de José Miguel Carrera, se encerró en la plaza de la ciudad para detener las tropas realistas de Mariano Osorio, logrando resistir durante dos días, hasta que pudieron romper el cerco y huir. Durante el fragor de la batalla, O'Higgins dice nuevamente la frase que había acuñado meses antes en la batalla de El Roble: «O vivir con honor o morir con gloria», a las tropas.

La derrota de los patriotas en esta batalla produce el fin de la Patria Vieja, la huida de los líderes emancipadores a la Argentina y el comienzo del período histórico de la llamada Reconquista.

La heroica derrota que sufrieron las fuerzas patriotas en la plaza de Rancagua, provocó la casi total destrucción de la ciudad y la persecución de los que lucharon en la resistencia contra las autoridades realistas. Para honrar y perpetuar la memoria de estos acontecimientos, el Director Supremo, Bernardo O’Higgins, le confirió el título de ciudad «muy leal y nacional» el 27 de mayo de 1818. El mismo Decreto estableció el Escudo de la ciudad con el lema «Rancagua renace de sus cenizas porque su patriotismo la inmortalizó».

La elección del Cabildo en 1825, se vio marcada por el enfrentamiento entre O'Higginistas y Freiristas. Luego de una serie de complicaciones, se realizó la jura de la Constitución que se hizo con todas las formalidades, el 27 de septiembre de 1828. Más tarde se designó un Asesor Letrado, para dirimir los conflictos que se producían en la naciente ciudad. Hacia 1831 se comenzó la construcción de la alameda en la cañada norte, que fue inaugurada con una gran fiesta en 1834.

Administrativamente, Rancagua era cabecera del Departamento de Rancagua, perteneciente de la Provincia de Santiago. En 1840 se creó el Colegio de Instrucción Superior, que serviría de continuación a los jóvenes egresados de la escuela municipal que existía. En 1842 surgió en Chile un gran movimiento intelectual, la Sociedad Literaria de 1842, que tuvo como gran protagonista al rancagüino José Victorino Lastraría. Una ley dictada el 25 de octubre de 1854 ordenó la instalación de un Juzgado de Letras en el Departamento de Rancagua.

En 1883, se crea la Provincia de O'Higgins, a partir del sector oriental del antiguo Departamento de Rancagua, del que se segregan 10 subdelegaciones correspondientes al sector poniente del Departamento, que pasan a integrar el Departamento de Melipilla. La Provincia de O'Higgins está formada por 3 departamentos: Rancagua, Maipo y Cachapoal, siendo su capital Rancagua, la que toma una nueva importancia.

Surgieron los primeros periódicos de la ciudad, como "El Porvenir" (1871), "El Fénix" (1872), "El Lautaro", "El Heraldo", "El Crepúsculo", "El Patriota" y "El Progreso y La Voz del Pueblo". "El Lautaro" sería la voz de los balmacedistas en la Revolución de 1891, y luego de la derrota del presidente Balmaceda el diario fue descontinuado y sus imprentas saqueadas. En 1892 llega a Rancagua el doctor Eduardo De Geyter, quien sería nombrado como médico de la ciudad. Su importante labor se extendió hasta 1925, año de su muerte.

Los avances que se iniciaron desde inicios del siglo XX permitieron un mayor desarrollo urbanístico de Rancagua. En 1903 se autorizó la implementación de un sistema de tranvías por calles Estado e Independencia, proyecto que sería concretado en 1919 con la instalación de tranvías eléctricos. En 1912 se instaló el "Monumento a O'Higgins" de la Plaza de Los Héroes, reemplazando los pilones que existían en el mismo lugar.

El 2 de enero de 1922, se realizó en la ciudad el III Congreso del Partido Obrero Socialista, momento a partir del cual se refunda con el nombre de Partido Comunista de Chile. El 18 de octubre de 1925 se creó la Diócesis de Santa Cruz de Rancagua por el Papa Pío XI mediante la Bula "Apostolici Muneris Ratio". El 25 de abril de ese mismo año se formó la Cámara de Comercio de Rancagua.

En 1956, se estableció que Chile sería el país sede de la Copa Mundial de Fútbol de 1962. Luego del terremoto de 1960 que arrasó con todas las ciudades al sur de Talca, se revaluaron las ciudades sedes del Mundial, descartándose Talca, Concepción, Talcahuano y Osorno, mientras que Antofagasta y Valparaíso desisten de ser sedes debido a que sus estadios no podían ser autofinanciados. Sin embargo, la "Braden Copper Company", dueña de la mina El Teniente, permite la utilización del Estadio El Teniente, que en ese entonces tenía capacidad para 8234 espectadores. Finalmente, Rancagua acogió al grupo D en el torneo mundial, compuesto por Hungría, Inglaterra, Argentina y Bulgaria. Los partidos en la ciudad se realizaron entre el 30 de mayo y el 10 de junio de 1962, donde se enfrentaron en cuartos de final Alemania Federal y Yugoslavia.

El 11 de julio de 1971 el presidente Salvador Allende dio su discurso por la nacionalización del cobre en la Plaza de Los Héroes de la ciudad, ante los trabajadores de El Teniente. Con ese proceso, la mina pasó a ser completamente estatal, siendo desde 1976 propiedad de la Corporación Nacional del Cobre de Chile (CODELCO). En 1971 se inició "Operación Valle", donde los habitantes del campamento minero Sewell fueron trasladados a Rancagua, generando un importante aumento de la población.

En 1981 se peatonizó calle Independencia, entre San Martín y Bueras. En la década de 1990 se continuaría con el tramo entre Bueras y la Plaza de Los Héroes.

En los primeros años del nuevo siglo, Rancagua sufrió la remodelación de varias de sus arterias principales, así como también de algunas zonas de su casco histórico, como es el caso de la Plaza de los Héroes, que fue totalmente renovada durante el período del alcalde Pedro Hernández Garrido, y la peatonización del tramo sur de la calle Estado (entre la Plaza de los Héroes y la Iglesia San Francisco, en la intersección con la Avenida Millán), que se concretó en el año 2006. Ese mismo año se disputó la Copa Davis en la Medialuna Monumental de la ciudad.

En 2013 se inauguró el Teatro Regional de Rancagua y en 2015 se creó la Universidad de O'Higgins, con su casa central en la ciudad, que empezó a funcionar dos años más tarde.

La ciudad de Rancagua se emplaza en la cuenca del mismo nombre, que se extiende desde la Angostura de Paine, hasta la Angostura de Pelequén, y que forma parte de la Depresión Intermedia. La característica principal de esta cuenca es que se encuentra encajonada entre la Cordillera de los Andes y la Cordillera de la Costa.

La Cordillera de Los Andes alcanza en esta cuenca su mayor altitud, entre el Aconcagua y el Pico del Barroso, donde las cumbres sobrepasan los 5.500 msnm.

Dicha cuenca tiene un origen tectónico y está compuesta de sedimentos fluvio-glaciar-volcánicos. Tiene 60 km en su eje norte-sur y 30 km en sentido este-oeste. Tiene una altitud media de 400 msnm.

La comuna limita al norte con el estero La Cadena, que la separa de la comuna de Graneros, al sur con el Río Cachapoal que la separa de la comuna de Olivar, al este con el estero de Machalí que la separa de dicha comuna y al oeste con la loma de la Bandera y el Cordón de los cerros de Lo Miranda que la separa de la comuna de Doñihue.

El clima que se observa en Rancagua es templado con estación seca prolongada, es decir, un clima mediterráneo. Por lo tanto, las estaciones del año se presentan claramente marcadas, con veranos en general sumamente calurosos y secos e inviernos lluviosos, suaves y húmedos. En años fríos, no son raras unas leves nevadas. Aunque hay precipitaciones durante todo el año, los meses donde se concentran las lluvias son en mayo, junio, julio y agosto. En Rancagua, caen en promedio entre 505 y 538 mm anuales.

La ubicación del Río Cachapoal fue preponderante en el asentamiento de los picunches en el Valle de Rancagua. El Cachapoal es afluente norte del Río Rapel. Tiene una hoya de una superficie de 6.370 km². Es de régimen pluvio-nival, tiene su nacimiento en la Cordillera de los Andes en el sector del Volcán Overo, Pico del Barroso y Nevado de los Piuquenes. El Cachapoal es depositario, en el sector cordillerano, de las aguas de los ríos Las Leñas, Cortaderal, Los Cipreses, Coya y del Pangal (que da origen a la central hidroeléctrica "Pangal" que abastece de electricidad a la mina de El Teniente), en el valle, recibe por su margen izquierda las aguas del río Claro de Rengo, su principal afluente.

Sin duda que la presencia del Cachapoal en la Cuenca de Rancagua favorece la existencia de un clima propicio para la agricultura. En el curso medio del río está la ciudad de Rancagua (en la rivera norte); del lado sur está la localidad de Gultro, (comuna de Olivar).
Está inserto en una zona subhúmeda con predominio de "vegetación mesófita". La vegetación más abundante en la zona consiste en espinos, palmas chilenas, boldos, litres y quillayes, muchos de los cuales son ocupados para carbón vegetal o para uso doméstico en chimeneas.

La comuna de Rancagua, según los datos del censo de 2017, reflejó una población de 241.774 habitantes, 27.400 habitantes más que en el censo de 2002. Según el censo, en Rancagua la distribución de la población por sexos es de 123.832 Mujeres y 117.942 hombres.

Rancagua es una de las trece comunas que integran el distrito N.º 15, vigente desde el LV periodo legislativo del Congreso Nacional de Chile (2018-2022), siendo representado por 5 diputados. Además, pertenece a la IX circunscripción senatorial, representada por 2 senadores, que en 2022 será reemplazada por la VIII circunscripción, que tendrá tres escaños en el Senado.

En el Congreso, la comuna está representada por los senadores Alejandro García-Huidobro (UDI) y Juan Pablo Letelier (PS). A su vez, es representada en la Cámara de Diputados por Issa Kort (UDI), Javier Macaya (UDI), Diego Schalper (Ind. pro RN), Juan Luis Castro (PS) y Raúl Soto (PDC).

Es la única comuna del distrito Cachapoal I del Consejo Regional de O'Higgins. Actualmente está representada por los consejeros regionales Manuel Barrera Bernal (PS), Felipe García-Huidobro Sanfuentes (UDI), Emiliano Orueta Bustos (RN) y Cecilia Villalobos Cartes (PPD).

La administración de la comuna de Rancagua corresponde a la Ilustre Municipalidad de Rancagua, que es dirigida por el alcalde Eduardo Soto Romero (Ind./ChV), que cuenta con la asesoría del Concejo Municipal, compuesto por:


En el ámbito de la agricultura es el centro donde se reúnen la mayoría de los productos agrícolas de la región antes de ser exportados vía puertos de San Antonio o Valparaíso.

En el valle del Cachapoal, los grandes contrastes climáticos entre los faldeos de los Andes y los planos más cálidos del valle central favorecen la producción de vinos tintos generosos. Las vides ocupan alrededor del 80% de la superficie plantada. La mayor cantidad de hectáreas corresponde a Cabernet Sauvignon, seguida por Merlot y Carménère. En blancos, Sauvignon Blanc y Chardonnay.

A 50 minutos de la ciudad y en la Cordillera de los Andes se encuentra la mina de cobre "El Teniente", actualmente explotada por Codelco Chile. Este yacimiento se constituye como una importante fuente laboral para los habitantes de la conurbación de Rancagua. La "Braden Copper Company", antigua empresa propietaria de este yacimiento, fue fundamental en el desarrollo de la ciudad hasta los años 1960, tanto en aspectos materiales como culturales. También hay actividad minera a pequeña escala en el distrito minero de Chancón, en el noroeste de la comuna de Rancagua.
Rancagua ha desarrollado desde finales del siglo XX de una variada actividad de comercio. Los puntos de mayor actividad comercial son el eje formado por el Paseo Independencia y la calle Brasil, y en los últimos años la Carretera Eduardo Frei Montalva, también llamada Carretera del Cobre por llegar hasta los yacimientos cupríferos de la mina El Teniente. Esta importante vía de la ciudad ha desarrollado un auge comercial increíble desde el fin de la década de 1990, donde se han construido una serie de centros comerciales, hipermercados, locales comerciales, servicios e instituciones médicas, lo que ha aumentado considerablemente la plusvalía del sector oriente de la ciudad.

Esta ciudad no se destaca mucho por ser una urbe industrial, excepto en el campo alimentario. Existen algunos parques industriales, que bordean a la antigua ruta 5 Panamericana, y en la zona norte de la ciudad, pero que no son tan importantes a nivel nacional.

El transporte mayor de la ciudad lo componen autobuses comúnmente llamados "micros". El parque de transporte mayor está compuesto en su mayoría por microbuses ligeros de mediana capacidad. Desde la implementación del plan de transporte Trans O'Higgins se hizo una división a los tipos de transporte: las "micros" de color rojo que son las de recorrido urbano y las de color verde que son las del tipo rural. Estas últimas son el pilar del transporte con las comunas que integran la Conurbación de Rancagua.

El transporte menor lo constituyen las líneas de taxis colectivos, que se dividen en los intercomunales y los netamente urbanos. Actualmente existe una restricción vehicular a los "colectivos", que forman parte importante del parque automotriz de la ciudad.

El Rodoviario es la terminal donde se concentra la locomoción hacia las comunas aledañas a la ciudad, tanto en transporte mayor como menor.

El principal terminal de la ciudad es el Terminal O'Higgins, ubicado en la intersección de la Av. Libertador Bernardo O'Higgins (también conocida como Alameda) y la Ruta 5 sur.
Desde este terminal operan empresas de buses interprovinciales con destinos a otras ciudades de Chile como Santiago, San Fernando, Curicó, Talca, Chillán, Concepción, y Osorno; y también el Terminal Rodoviario ubicado en Dr. Salinas, con recorridos rurales como Doñihue, Coltauco, San Vicente, Peumo, Las Cabras y Pichilemu; también tienen destino a la Región Metropolitana, como San Pedro de Melipilla y Alhué y a la Quinta Región como Cartagena y San Antonio. 
El ferrocarril, a través de la Estación Rancagua, se ha transformado en una importante alternativa de transporte en la ciudad, que cobra mayor importancia gracias al servicio del Metrotrén de la empresa Trenes Metropolitanos S.A., filial de EFE, que une Santiago con San Fernando. El Metrotrén es uno de los métodos preferidos de los rancagüinos para trasladarse a otras comunas de la región y a la capital nacional. Este último viaje dura aproximadamente una hora y cuarto.

Está el Aeródromo Militar "La Independencia", perteneciente a la Brigada de Aviación del Ejército de Chile, donde se encuentra asentado el Club Aéreo de Rancagua, institución que entre otras labores, realiza vuelos populares donde turistas se pueden embarcar en sus aviones y conocer desde el aire las comunas de Machalí, Graneros y Rancagua. 
También hay servicio aéreos de Helicópteros no regulares de la empresa Discovery Air S.A. y ALFA Helicopters a través del helipuerto de la empresa Discovery Air S.A. ubicado al oriente de la ciudad.

El Hospital Regional es la base de la atención médica pública de la región, que se complementa con 6 consultorios SAPU (Servicio de Atención Primario de Urgencia), además de un laboratorio perteneciente a la Municipalidad de Rancagua y una . En el ámbito privado podemos encontrar otros hospitales, como el Hospital del Trabajador (Asociación Chilena de Seguridad), el Hospital Mutual de Seguridad y el Hospital Clínico FUSAT (Fundación de Salud de El Teniente), propiedad de CODELCO. También podemos encontrar clínicas privadas, como Isamédica, y la Clínica de Salud Integral. Para 2002 en Rancagua había 2,6 camas hospitalarias por cada 1000 habitantes.

En el área de salud pública podemos nombrar los siguientes:

Todos admonistrador por CORMUN, Corporación Municipal de Servicios Públicos Traspasados de Rancagua

Existe una variada oferta educativa en la ciudad, y dirigida a distintos intereses, tanto en el área científico-humanista, como en el área técnica. En datos concretos, en el año 2002 existía una cobertura del 96% en educación básica y de 83% en educación media.

Hay colegios y liceos que están bajo la supervisión y funcionamiento de la Municipalidad de Rancagua ("Corporación Municipal de Servicios Públicos Traspasados de Rancagua", CORMUN), que son llamados «municipales» o «públicos». Algunos de los más destacados de este tipo son el Liceo Óscar Castro (ex "Liceo de Hombres") y el Colegio Moisés Mussa (ex "Superior de Hombres N°1"), considerado el colegio más antiguo del país (fundado en 1791).

Son también importantes el Liceo María Luisa Bombal, Liceo José Victorino Lastarria y el Liceo Comercial Diego Portales. También destacan los Liceos Industriales que están bajo la tutela de corporaciones ajenas a la Municipalidad; el Liceo Ernesto Pinto Lagarrigue (ex B-5) bajo la Cámara Chilena de la Construcción y el Liceo Pdte. Pedro Aguirre Cerda bajo la administración de la USACH.

En la categoría de los colegios particulares pagados, que son mantenidos sólo con el aporte de los apoderados, existen tanto colegios confesionales (principalmente enseñanza católica), como el Instituto O'Higgins, fundado en 1915 por los Maristas, y los colegios laicos como el Colegio Coya (entre Rancagua y Machalí), el Instituto Inglés, entre otros. Otros colegios administrados por privados, conocidos como particulares subvencionados, son el Colegio Don Bosco, Colegio Javiera Carrera, Instituto Regional de Educación (I.R.E.), entre otros.

Rancagua cuenta con la Universidad de O'Higgins, cuya casa central está ubicada en dicha ciudad, creada por ley en el año 2015, y que comienza sus clases el 2017.

Además de esta universidad, se pueden encontrar sedes de universidades públicas (Universidad de Los Lagos, Universidad Arturo Prat, Universidad Metropolitana de Ciencias de la Educación, Universidad Técnica Federico Santa María y la Universidad de Valparaíso) y privadas (Universidad de Aconcagua, Universidad La República y la Universidad Santo Tomás). Rancagua cuenta también con una variada oferta de institutos profesionales y centros de formación técnica como INACAP, Instituto Profesional AIEP, Instituto Profesional de Chile y el Instituto de Estudios Bancarios Guillermo Subercaseaux.

En el pasado también funcionaron en Rancagua universidades locales como Leonardo da Vinci, Educares y de Rancagua –esta última dio paso a la actual Universidad de Aconcagua–.

Podemos encontrar dos bibliotecas abiertas a la comunidad en la ciudad, la Biblioteca Pública Santiago Benadava (nº251) y la Biblioteca Pública Eduardo De Geyter (nº34). El índice de libros por habitante en las bibliotecas públicas fue de 0,7 en 2001. Además existen dos «biblio-vagones» Logroño, ubicado en la Población Vicuña Mackenna, y Estación del Saber, ubicado en la plaza Eduardo Martínez Moreno.

Si bien Rancagua no es un centro neurálgico en materia telecomunicacional a nivel nacional, posee diversos medios de comunicación local y regional.


En la búsqueda de nuevas formas de información, distintos grupos han propuesto y concretado algunas alternativas de informativos locales, aprovechando las herramientas de Internet, como el "Diario La Visión Digital", "Zona de Escape TV", "El Rancahuaso", "El Incendio", "El Cachapoal" y "VI Región".

Existen diversas emisoras radiales locales que transmiten desde esta ciudad; las más importantes son: Radio Rancagua en señales AM Y FM, que es la radio en funcionamiento más antigua del país después del fin de Radio Chilena, Radio Primordial, entre otras.

En la ciudad de Rancagua transmiten un total de 26 emisoras en la banda de frecuencia modulada y solo una emisora en la banda de amplitud modulada.



Las señales de televisión que se reciben en Rancagua son las siguientes:


En un principio a Rancagua llegaban las señales de televisión y radio procedentes directamente de Santiago o algunas repetidoras procedentes de San Fernando. Para recibirlas, los rancagüinos montaban en sus casas sendas antenas de hilo; el número de éstas llegó a ser tal, que rápidamente -años setenta- Rancagua pasó a ser conocida como "La Ciudad de las Antenas". Esto, hasta 1988, cuando se instaló en la ciudad la repetidora de Televisión Nacional de Chile y desde 1990 en adelante, de los demás canales, lo cual hizo innecesarias dichas antenas. 
Las repetidoras de los canales santiaguinos se ubican en el Cerro Orocoipo y en el Cerro 866, cercano a Graneros.
También hay 2 canales de televisión que se transmiten desde la capital de la VI Región: La emisora de señal abierta Centrovisión y el canal para televisión por cable Sextavisión, además de la repetidora local de Televisión Nacional de Chile, llamada Red O'Higgins, que interrumpe la señal nacional para emitir en la noche un noticiario de noticias regionales.

Las actividades agrícolas de Rancagua, como el cultivo del maíz, tomates, trigo, hortalizas, entre otras, han permitido un fácil acceso de la población a los alimentos que son la base de la gastronomía nacional, sobre todo por la existencia de ferias libres en distintos puntos de la ciudad. Los platos típicos que abundan en esta zona son pastel de choclo, humitas, empanadas, porotos, pastel de papa, carbonada y asado.

En la ciudad destacan algunos lugares en cuanto a su oferta gastronómica. El "Café Reina Victoria" es toda una tradición de la gastronomía local con una amplia gama de repostería y comida rápida chilena (completos, churrascos, chaparritas, etc.), "El Viejo Rancagua", en el Paseo del Estado, es un lugar donde el ambiente bohemio de la ciudad se reúne y que destaca por sus chorrillanas. En la céntrica calle Rubio se encuentra el restaurante "La picá de la Tía Julia", lugar donde se encuentran churrascos de Rancagua y salsa de ají para acompañamiento. Saliendo de Rancagua hacia el sur por la ruta 5, existen varios locales de comida tradicional en las calles de servicios de la Autopista del Maipo, como el "Mini Restaurante" (Gultro, Olivar), antes llamado "Mini Sheraton", y el restaurante "Juan y Medio" (Rosario, Rengo).

Cada 2 de octubre se conmemora la Batalla de Rancagua con un desfile en el Estadio El Teniente de Rancagua. A este desfile asisten las Fuerzas Armadas y Carabineros de Chile, principales instituciones de la ciudad, colegios y liceos. Precisamente fue en el marco de esta conmemoración cuando, el entonces presidente de Francia, Charles De Gaulle, visitó la ciudad de Rancagua, y presenció junto al presidente chileno de la época, Jorge Alessandri, el desfile del 2 de octubre de 1964 (sesquincenario de la Batalla de Rancagua).

Durante todo el mes de octubre también se realizan las «Fiestas Rancagüinas», donde se celebra el aniversario de la ciudad. A lo largo de estas semanas se realizan diversas actividades artísticas, culturales, y recreacionales, que culminan con un gran show con artistas invitados y el lanzamiento de fuegos artificiales.

Desde el año 2002 se realiza en la Plaza de los Héroes, durante el desarrollo del Campeonato Nacional de Rodeo (a principios de abril), una feria costumbrista denominada «Fiesta Huasa». En ella se realizan shows folclóricos y exposiciones gastronómicas con productos de la VI región. Para la versión 2007 de la «Fiesta Huasa», realizada en conjunto con la Federación del Rodeo Chileno, se elaboró la chupalla más grande del mundo, y fue expuesta en la feria como atractivo principal.

También se realiza, desde el año 2008, el «Festival de Jazz Internacional de Rancagua», con el apoyo de la Embajada de Estados Unidos, Codelco Chile, y la Caja de Compensación Los Andes, destacándose en su primera versión la presentación del grupo estadounidense Western Jazz Quartet de Michigan.

La letra del himno de la ciudad fue escrita por el destacado poeta chileno Óscar Castro Zúñiga. La música fue compuesta por Fernando Morales, y los arreglos musicales fueron encargados a Antonio Muñoz. Fue interpretado por primera vez por el "Coro Braden" de la ciudad. En la canción se evoca la Batalla de Rancagua, ocurrida el 1 y 2 de octubre de 1814, y que es uno de los hechos más importantes que ha transcurrido en la ciudad.

Rancagua cuenta con varias estatuas y esculturas en sus plazas y calles, pero muchas de ellas se encuentran en mal estado y a falta de mantención. Entre estos monumentos, podemos encontrar:



La actividad deportiva en la ciudad es variada. Los clubes deportivos de fútbol y los de tenis acaparan gran cantidad de miembros.

El club de fútbol más famoso de la ciudad es O'Higgins, que milita actualmente en la primera categoría del fútbol chileno, el Campeonato Nacional. El Club Deportivo O'Higgins tuvo grandes glorias pasadas, cuando con la ayuda del mineral El Teniente se convirtió en los años 1970 y 1980 en uno de los equipos con más asistencia a los estadios en el fútbol nacional, llegando a participar en la Copa Libertadores de América en varias oportunidades, siendo su mejor actuación una semifinal, además de ser campeón y supercampeón del fútbol chileno.

Desde el año 2012, el club Tomás Greig se integró a la competencia nacional, disputando actualmente la Tercera División B de Chile. Además, el Club Deportivo Rancagua Sur, fundado hace más de 57 años, actualmente compite en la Tercera División A de Chile.

Otros clubes que han participado anteriormente en los Campeonatos Nacionales de Fútbol en Chile han sido Coinca, Cultural Orocoipo, Rancagua Oriente y Enfoque de Rancagua.

El principal coliseo de la ciudad es el Estadio El Teniente, propiedad de CODELCO División El Teniente, y sede del Club O'Higgins. El Estadio El Teniente tiene capacidad para alrededor de 14 mil personas.

Este complejo fue testigo de la Copa Mundial de Fútbol de 1962, organizado por Chile. En esta ocasión Rancagua fue ciudad sede, gracias a la ayuda de la "Braden Copper Company", junto a las ciudades de Viña del Mar, Arica y Santiago. En el Estadio El Teniente se disputaron todos los partidos del Grupo D (compuesto por Inglaterra, Argentina, Bulgaria y Hungría) y un partido de los cuartos de final (Yugoslavia v/s Alemania Federal).Muy importante para el deporte local es la Medialuna Monumental, donde todos los años se realiza la mayor cita del rodeo chileno, el «"Champion" de Chile». Luego de su remodelación, la Medialuna ha sido testigo de importantes encuentros tenísticos, como la despedida del tenista nacional Marcelo "Chino" Ríos ante el jugador Goran Ivanišević el 15 de diciembre de 2004, o el encuentro de Chile y Eslovaquia por la Copa Davis durante los días 10, 11 y 12 de febrero de 2006.

Otros complejos deportivos de la ciudad son, el "Club Ansco" (CODELCO), el "complejo Patricio Mekis" (unido a la Medialuna Monumental), el "Gimnasio Hermógenes Lizana", el "Centro de Entrenamiento Regional" (CER), el "Estadio Marista" (en el límite con Machalí), el Autódromo Internacional de Codegua en el límite con Codegua y el "Complejo Deportivo San Damián".

A pesar de no ser una ciudad turística, Rancagua tiene varios atractivos, delegados en su mayoría por ser la histórica ciudad de la independencia chilena. La Plaza de Armas, llamada "Plaza de los Héroes", fue el escenario donde Bernardo O'Higgins y sus hombres sostuvieron el sitio en la Batalla de Rancagua. La plaza, se destaca porque sus calles la atraviesan por el centro dando forma a una cruz, forma que comparte con la Plaza de Armas de Combarbalá y Vallenar, está rodeada por importantes edificios como la Catedral, la Intendencia de la Región de O'Higgins, la Gobernación Provincial de Cachapoal, entre otros.

Una cuadra hacia el norte está ubicada la Iglesia de la Merced, cuya construcción data desde el siglo XVIII y es conocida por el importante papel histórico en la Batalla de Rancagua, lugar desde donde el padre de la patria, don Bernardo O'Higgins, vigilaba desde la torre esperando inútilmente la ayuda de José Miguel Carrera. Junto a la iglesia se encuentra la casa parroquial.

Al sur de la plaza está la "calle del Rey" (actual "Paseo del Estado"), calle que en la época fundacional era el centro de la actividad urbana, y en donde se ubican importantes casas coloniales.

El Museo Regional de Rancagua está ubicado en pleno casco histórico de la ciudad. Las dos casas que forman el museo datan del siglo XVIII y son los únicos vestigios de la época de la fundación de la Villa Santa Cruz de Triana, actual Rancagua. Su estructura en torno a patios, con amplios corredores, techos de teja y gruesos muros de adobe, hacen de ellas un ejemplo de la arquitectura tradicional chilena.
El museo cuenta con una colección permanente y variadas exhibiciones que se renuevan periódicamente.

Ambas casas, que ocupaban un solar cada una, forman parte de una Zona Típica de la ciudad de Rancagua junto con la "Plazuela Marcelino Champagnat" (llamada antiguamente "Plazuela Santa Cruz de Triana", en honor al nombre fundacional de la ciudad), que se ubica en el frontis del Instituto O'Higgins. Es por su importancia histórica que las casas del Museo Regional fueron declaradas Monumento Nacional en el año 1980.

Una cuadra más al sur, llegando a calle Millán se ubica la Iglesia de San Francisco. En la cuadra siguiente se encuentra la "Casa de la Cultura", antigua construcción que formara parte de las Casas Patronales del Fundo El Puente, hijuela de la Hacienda El Carmen y que en la Batalla de Rancagua sirvieron de Cuartel al Estado mayor del coronel realista Mariano Osorio. La casa es Monumento nacional. Actualmente es un recinto donde se exponen obras artísticas.

También el turismo se desarrolla en las comunas aledañas a Rancagua. Subiendo por hacia la Cordillera, en la comuna de Machalí se encuentran las Termas de Cauquenes, conocidas por las propiedades que tienen sus aguas, fueron visitadas incluso por José de San Martín y Bernardo O'Higgins, importantes próceres de la Independencia de Latinoamérica. Para acceder a estas termas se debe tomar la Ruta del Ácido. Además por esta misma ruta es posible llegar a la Reserva Nacional Río de Los Cipreses, administrada por la CONAF.

Los ríos Cachapoal y Tinguiririca son los que alimentan al mayor lago artificial del país, el Lago Rapel. Fue creado en un primer fin como embalse para la Central Hidroeléctrica Rapel, pero con el paso de los años, diversos recintos turísticos y casas de veraneo fueron ocupando su ribera, hasta el día de hoy.


La ciudad de Rancagua ha realizado diversos hermanamientos con otras ciudades del orbe, así como también ha firmado varios acuerdos de cooperación económica y cultural. Algunas de las ciudades con que Rancagua mantiene relaciones de este tipo son:
Desde el año 2002 Rancagua también forma parte de las Mercociudades, la red de municipios del Mercosur.




</doc>
<doc id="17397" url="https://es.wikipedia.org/wiki?curid=17397" title="Codelco">
Codelco

La Corporación Nacional del Cobre de Chile más conocida como Codelco es una empresa estatal chilena dedicada a la explotación minera cuprífera, rubro en el que es la mayor compañía del planeta. Codelco opera ocho centros de trabajo, ubicados entre la región de Antofagasta y la región del Libertador General Bernardo O'Higgins. La Casa Matriz se encuentra en Santiago.

Codelco es el productor de cobre más grande del mundo y la empresa que más contribuye a la economía chilena. Durante el año 2019, su producción totalizó 1.706.013 tmf de cobre fino (incluidas sus participaciones en El Abra y Anglo American Sur), la cual corresponde al 8% de la producción mundial y un 29% de la producción nacional. Actualmente, esta empresa cuenta con un total de 19444 empleados. 

Codelco concentra el 7% de las reservas globales de cobre, contenidas en yacimientos de clase mundial y, con una participación de 10%, es el segundo mayor productor de molibdeno. 

La compañía cuenta con siete divisiones mineras: Radomiro Tomic, Chuquicamata, Gabriela Mistral, Ministro Hales, Salvador, Andina y El Teniente. A estas operaciones se suma la división Ventanas, dotada de instalaciones de fundición y refinería.

Adicionalmente, la Corporación tiene un 49% de participación en la Sociedad Contractual Minera El Abra y, desde 2012, es propietaria del 20% de Anglo American Sur. Además, Codelco participa en diversas sociedades orientadas a la exploración e investigación y desarrollo tecnológico, tanto en Chile como en el extranjero.

Clave para el desarrollo de Chile, desde la nacionalización del cobre en 1971, hasta el año 2019, los yacimientos y operaciones de Codelco han aportado US$ 116 mil millones a las arcas fiscales.

La historia de Codelco se remonta al proceso de nacionalización de la Gran minería del cobre que durante gran parte del siglo XX fue realizada por capitales estadounidenses. En 1967, el Estado se convirtió en accionista mayoritario de las principales minas cupríferas del país, hasta que en 1971, durante el gobierno de Salvador Allende, las compañías fueron totalmente expropiadas y estatizadas. Administradas cada una como una sociedad separada a cargo de la Corporación del Cobre, éstas finalmente se unificarían en una única empresa, Codelco Chile, encargada de la explotación de los yacimientos y la comercialización de sus productos, con la promulgación del Decreto Ley N° 1.350 del 1 de abril de 1976.

Codelco es encabezada por un directorio con nueve integrantes designados de acuerdo a normas establecidas en la Ley N° 20.392 promulgada el 4 de noviembre de 2009. El directorio está conformado por cuatro directores designados a partir de una quina seleccionada por el Consejo de la Alta Dirección Pública; un director escogido a partir de una quina presentada por la Federación de Trabajadores del Cobre (FTC), y un director elegido de una quina presentada, en conjunto, por la Federación de Supervisores del Cobre (FESUC) y la Asociación Nacional de Supervisores del Cobre (ANSCO). El Presidente del directorio es nombrado por el Presidente de la República, de entre los directores.

El Presidente Ejecutivo es designado por el Directorio de la empresa y es el encargado de la administración de la Corporación, cargo en el que actualmente se desempeña Octavio Araneda.

Codelco tiene 8 divisiones (instalaciones y yacimientos de la gran minería del cobre).

La producción de cobre de CODELCO es la que se detalla a continuación (expresado en miles de toneladas de cobre fino) entre los años 1990 y 2011:
(*) La producción refleja la proporción propiedad de Codelco: El Abra 49% , Anglo American 20%.

La producción de molibdeno de Codelco se detalla a continuación (expresado en toneladas métricas de molibdeno fino) entre los años 1990 y 2011:

Por la Ley Nº 19.137, del 12 de mayo de 1992, CODELCO se puede asociar con terceros para realizar sus actividades mineras. La principal asociación ha sido la SCM El Abra, para el yacimiento de cobre del mismo nombre, ubicado en la II Región. Formada en 1994, CODELCO mantiene un 49% de participación, mientras el 51% restante lo controla Cyprus El Abra Corporation, filial de Freeport-McMoRan Copper.
Además, CODELCO también participa en la sociedad contractual minera Purén, donde posee el 35% de la propiedad (el resto pertenece a Compañía Minera Mantos de Oro, controlada por Goldcorp Inc. y Kinross).

Las culturas indígenas que habitaban en la época precolombina lo que hoy es Chile, utilizaron el cobre para la elaboración de distintas herramientas. Desarrollaron metalurgias elementales que les permitieron explotar y trabajar este metal, incluso para producir aleaciones. Desde entonces hasta la actualidad, el cobre se ha mantenido como un producto fundamental de la economía chilena.

La explotación del metal rojo se mantuvo como una pequeña industria durante la colonia. En 1810, año de la Independencia, Chile produjo 19.000 toneladas del mineral. Entre 1820 y 1900, el país produjo 4 millones de toneladas de cobre y fue, durante un tiempo, el primer productor y exportador mundial.

A fines del siglo XIX comenzó un período de decadencia, debido al gran impacto del salitre, que acaparaba el interés y las inversiones, y al agotamiento de los yacimientos de alta ley. En 1897 se produjeron apenas 21.000 toneladas de cobre.

A comienzos del siglo XX se iniciaron inversiones en El Teniente y Chuquicamata por parte de empresas norteamericanas. Chile tenía en esos años una participación reducida en la gran minería del cobre.

En 1951 se firmó el Convenio de Washington, que permitió a Chile disponer del 20% de la producción de cobre, posibilitando al Gobierno chileno a aumentar los ingresos provenientes de la actividad cuprífera.

En 1955, el Senado dictó una serie de leyes cuyo objetivo era garantizar un ingreso mínimo al Estado chileno y fomentar la inversión por parte de las grandes compañías del rubro minero.

El 5 de mayo del mismo año se creó el Departamento del Cobre, un organismo estatal cuyas labores consistían en fiscalizar y participar en el mercado del metal rojo a nivel internacional.

En 1966 se dictó la Ley Nº 16.425, que autorizó la creación de sociedades mixtas entre el Estado de Chile y las compañías extranjeras productoras de cobre. En dichas sociedades mixtas, el Estado chileno debía tener como mínimo un 75% de la propiedad de los yacimientos en manos de las compañías extranjeras.

En 1967, las minas El Teniente, Chuquicamata y Salvador se convirtieron en sociedades mixtas, por lo que la entonces Corporación Nacional del Cobre adquirió el 51% de la propiedad de ellas. El 49% restante permaneció en manos de las antiguas propietarias: Braden Copper Company, subsidiaria de la Kennecott Corporation, en el caso de El Teniente; y Anaconda Copper Company, en el caso de Chuquicamata y Salvador.

Con respecto a Andina y Exótica, en 1967 pasaron a propiedad de la Corporación Nacional del Cobre en un 25%, quedando el resto en poder de Cerro Corporation y de Anaconda, respectivamente.

En 1971, a través de la modificación del Artículo 10 de la Constitución, se introdujo la posibilidad de nacionalizar la gran minería del cobre. La misma reforma constitucional agregó una disposición transitoria que establecía que: “por exigirlo el interés nacional y en ejercicio del interés soberano e inalienable del Estado de disponer libremente de sus riquezas y recursos naturales, se nacionalizan y declaran por tanto incorporadas al pleno y exclusivo dominio de la Nación las empresas extranjeras, que constituyen la gran minería del cobre”.

A través de esta modificación constitucional, pasaron a dominio nacional todos los bienes de las empresas mencionadas y se crearon cinco sociedades colectivas del Estado para administrar dicha riqueza.

En 1976 se decidió disolver estas sociedades colectivas. Por ello se agruparon las sociedades existentes en una sola empresa, minera, industrial y comercial, con personalidad jurídica y patrimonio propio, de duración indefinida, que se relaciona con el Gobierno a través del Ministerio de Minería. A través del decreto ley Nº 1.350 publicado el 28 de febrero de 1976, se creó la Corporación Nacional del Cobre de Chile, que abreviadamente se puede denominar como CODELCO o CODELCO-CHILE, que asumió la administración de los yacimientos nacionalizados.

En 1997 se inauguró Radomiro Tomić, primer yacimiento puesto en operación íntegramente por Codelco. En el año 2002, las entonces divisiones Radomiro Tomić y Chuquicamata se integraron para formar la División Codelco Norte. En el año 2003, la Corporación logró la mayor producción de su historia, con 1,84 millones de toneladas métricas de cobre fino. En mayo de 2008, finalizó con éxito el Proyecto Gaby, convirtiéndose en la segunda faena minera desarrollada por Codelco desde su ingeniería y construcción hasta su puesta en marcha y producción.


<nowiki>*</nowiki> "En moneda nominal."<nowiki>**</nowiki> Se incluyen aportes a negocios o participaciones.

La investigación, el desarrollo de tecnologías y su incorporación a los procesos industriales forman parte de la estrategia de Codelco para aumentar productividad, reducir riesgos de accidentes, beneficiar la salud de sus trabajadores y contribuir a la protección del medio ambiente.

Durante 2008, Codelco invirtió USD 52,4 millones en este ámbito: USD 25 millones en estudios y programas de investigación e innovación tecnológica corporativos; USD 7,5 millones como aportes a empresas tecnológicas; 3,1 millones como contribución a otras empresas e instituciones, y USD 16,8 millones para desarrollo tecnológico a través de las filiales de Codelco.

Por otra parte, desde el punto de vista de gestión de conocimiento, se avanzó en las directrices corporativas de protección de la propiedad intelectual, que buscan resguardar el conocimiento desarrollado por Codelco en innovaciones tecnológicas. En este sentido, se realizó un catastro de todas las solicitudes de patentes presentadas al Departamento de Propiedad Industrial del Ministerio de Economía desde 1991 hasta 2007; así como también las patentes concedidas, rechazadas o abandonadas en el período.

En junio del año 2016 el Presidente del Directorio Óscar Landerretche, anunció la creación de "Codelco Tecnologías" (CodelcoTech), como nueva filial, con la misión de consolidar los esfuerzos que realizan sus filiales que hacen innovación tecnológica bajo un solo paraguas, bajo un modelo de "innovación abierta", modelo que otras grandes mineras como BHP Billiton y Anglo American ya han experimentado con resultados prometedores. Su puesta en marcha ha sido el resultado de la fusión de las grandes filiales con foco tecnológico de la Corporación, como el IM2 y Biosigma.

Estos proyectos buscan soluciones para desafíos específicos de Codelco cuando no existen alternativas en el mercado. Algunos de ellos son:

Busca aumentar la productividad de la explotación subterránea con la identificación, investigación de procesos y equipos mineros, creación de prototipos y validación industrial de tecnologías de minería continua.

Considera el desarrollo de un proceso de acondicionamiento del macizo rocoso y la extracción con manejo de materiales en forma continua. Estas investigaciones y su validación industrial se desarrollan en las minas de las divisiones Salvador, Andina y El Teniente, con la participación de diversos centros de investigación, de la industria de proveedores y servicios mineros y de profesionales de Codelco.

El programa tiene como meta lograr una minería continua totalmente automatizada, de bajo costo, alto rendimiento y ambientalmente amigable para el año 2012. En el mediano plazo busca desarrollar modelos teóricos de tronadura; ingeniería de taludes; nuevas formas de arranque; y desarrollo y evaluación industrial de iniciativas de transporte autónomo, de gran capacidad y alta eficiencia energética.

Se ha validado la aplicación de las tecnologías de biolixiviación, desarrolladas por BioSigma, en recursos sulfurados de baja ley de Codelco.

Su objetivo es definir y estandarizar la caracterización de los recursos para la aplicación de la lixiviación in situ.

2008 marcó el inicio del programa de robotización de operaciones mineras. Junto a la empresa MIRS (ligada a Codelco), se trabajó en la conceptualización de siete soluciones robóticas para la industria minera.

Codelco ha desarrollado un esquema de asociatividad, a través de alianzas con empresas y organizaciones de desarrollo e investigación, líderes en el mundo. En varios casos, estas alianzas han evolucionado a sociedades, logrando acelerar la integración de las innovaciones a los procesos mineros.

Desde 1994, Codelco ha actuado como contraparte industrial en 42 proyectos Fondef en un plan permanente de apoyo a la investigación científica que permita generar nuevos avances para la industria minera.

Codelco y la empresa japonesa JX Nippon Mining & Metals Co. Ltd. crearon BioSigma S.A. en el año 2002. BioSigma es una empresa que desarrolla, implementa y adapta soluciones biotecnológicas integrales para la industria minera a través de procesos sustentables. A la fecha BioSigma cuenta con una planta de producción de biomasa lixiviante ubicada en la División Radomiro Tomic de CODELCO, y las tecnologías de biolixiviación desarrolladas por BioSigma están siendo aplicadas a escala industrial para la recuperación de cobre desde minerales sulfurados primarios de baja ley. Actualmente BioSigma cuenta con más de 90 patentes de invención concedidas en 14 países y continua incorporando a la minería los avances de la biotecnología de forma de aumentar los beneficios económicos disponibles en los vastos recursos de baja ley y otros materiales secundarios. A partir de Diciembre del año 2016, Biosigma se fusiona con las demás filiales tecnológicas de Codelco, para dar paso a la creación de CodelcoTec.

Codelco creó en 1998 el IM2, como una filial de generación de conocimiento dedicada al desarrollo de innovación con base tecnológica en minería y metalurgia. Su gestión se focalizaba en los programas tecnológicos de minería subterránea, procesamiento de minerales, minería a cielo abierto y procesos a altas temperaturas. A partir de Diciembre del año 2016, el IM2 se fusiona con las demás filiales tecnológicas de Codelco, para dar paso a la creación de CodelcoTec.

Esta filial tiene como objetivo el tratamiento de productos con altas impurezas.
EcoMorales Limited es una empresa filial de Codelco Technologies Ltd (Bermuda), que ofrece soluciones ambientales a la minería y procesos para la recuperación de metales.La empresa nace en el año 2001 para contribuir a resolver los problemas que genera la disposición de residuos mineros, aprovechando de recuperar los componentes de valor presentes en dichos residuos.EcoMetales desarrolla procesos industriales a partir de investigación aplicada y genera soluciones innovadoras y ambientalmente sustentables.
Sus instalaciones están ubicadas a 35 kilómetros al nororiente de Calama, en el sur de Chile, donde opera una planta con altos estándares de seguridad y salud ocupacional. Cuenta con un equipo experimentado de ingeniería, procesos y proyectos.Su localización estratégica, sumada a su infraestructura y experiencia industrial en el manejo de grandes volúmenes de residuos, le permiten ofrecer adaptabilidad a los requerimientos de sus clientes, garantía de cumplimientos de todos los estándares ambientales y de seguridad y una relación costo-calidad muy competitiva. EcoMetales procesa actualmente, residuos provenientes de las Divisiones de Chuquicamata, Ventanas y El Salvador. Además de Codelco, busca activamente oportunidades para colaborar con otras compañías mineras.

Codelco y la japonesa Nippon Telegraph and Telephone Co. Ltd. (NTT) crearon en 2006 Micomo (Mining, Information, Communication and Monitoring S.A.). Su objetivo es diseñar, suministrar, instalar y mantener productos de tecnologías de comunicaciones y de la información, para lo cual aprovecha desarrollos de NTT aplicados en otros campos industriales. Desde su funcionamiento, Micomo ha puesto en marcha servicios tecnológicos que generan mayor eficiencia y seguridad para Codelco.

Codelco y Honeywell (empresa líder en sistemas de automatización y control de procesos) se unieron para incorporar automatización de última generación a las plantas concentradoras. De esta forma, pueden aumentar la productividad y sustentar el negocio de Codelco en el largo plazo.

MIRS es una sociedad anónima cerrada, formada por Industrial Support Company Ltda. (HighService), Codelco, Nippon Mining & Metals Co Ltd. y KUKA Roboter GmbH. El ámbito de acción de esta empresa va desde la investigación, diseño, creación, fabricación e instalación, hasta el suministro, mantención y comercialización de soluciones robóticas para la minería. Desde su creación, con el apoyo de Innova-Chile, MIRS se ha focalizado en el mercado chileno y peruano, realizando una intensa campaña comercial. Como resultado se obtuvieron estudios exploratorios de identificación de oportunidades y se comercializaron dos soluciones robóticas.

En el marco de la transferencia y comercialización de tecnologías, Codelco realizó la ingeniería conceptual para incorporar la tecnología Teniente de fusión de concentrados y limpieza de escoria en la fundición de Mednogorsk, Rusia. Otro proyecto fue el entrenamiento de personal de la fundición Ust-Kamenogorsk de Kazajistán, en la operación de limpieza de escoria en horno eléctrico, realizada en División Ventanas.

Adicionalmente, Codelco estableció en China, durante 2008, seis acuerdos de colaboración con empresas mineras no ferrosas, de ingeniería e institutos de investigación para explorar o validar conjuntamente tecnologías y oportunidades de negocios.




</doc>
<doc id="17399" url="https://es.wikipedia.org/wiki?curid=17399" title="Transmisión de energía eléctrica">
Transmisión de energía eléctrica

La red de transporte de energía eléctrica es la parte del sistema de suministro eléctrico constituida por los elementos necesarios para llevar hasta los puntos de consumo y a través de grandes distancias, la energía eléctrica generada en las centrales eléctricas.

Para ello, los niveles de energía eléctrica producidos deben ser transformados, elevándose su nivel de tensión. Esto se hace considerando que para un determinado nivel de potencia a transmitir, al elevar la tensión se reduce la corriente que circulará, reduciéndose las pérdidas por Efecto Joule. Con este fin se emplazan subestaciones elevadoras en las cuales dicha transformación se efectúa empleando transformadores, o bien autotransformadores. De esta manera, una red de transmisión emplea usualmente voltajes del orden de 220 kV y superiores, denominados alta tensión, de 400 o de 500 kV.

Parte de la red de transporte de energía eléctrica son las llamadas líneas de transporte.

Una línea de transporte de energía eléctrica o línea de alta tensión es básicamente el medio físico mediante el cual se realiza la transmisión de la energía eléctrica a grandes distancias. Está constituida tanto por el elemento conductor, usualmente cables de acero, cobre o aluminio, como por sus elementos de soporte, las torres de alta tensión.

Generalmente se dice que los conductores «tienen vida propia» debido a que están sujetos a tracciones causadas por la combinación de agentes como el viento, la temperatura del conductor, la temperatura del viento, etc.

Existen una gran variedad de torres de transmisión como son conocidas, entre ellas las más importantes y más usadas son las torres de amarre, la cual debe ser mucho más fuertes para soportar las grandes tracciones generadas por los elementos antes mencionados, usadas generalmente cuando es necesario dar un giro con un ángulo determinado para cruzar carreteras, evitar obstáculos, así como también cuando es necesario elevar la línea para subir un cerro o pasar por debajo/encima de una línea existente.

Existen también las llamadas torres de suspensión, las cuales no deben soportar peso alguno más que el del propio conductor. Este tipo de torres son usadas para llevar al conductor de un sitio a otro, tomando en cuenta que sea una línea recta, que no se encuentren cruces de líneas u obstáculos.

La capacidad de la línea de transmisión afecta al tamaño de estas estructuras principales. Por ejemplo, la estructura de la torre varía directamente según el voltaje requerido y la capacidad de la línea. Las torres pueden ser postes simples de madera para las líneas de transmisión pequeñas hasta 46 kilovoltios (kV). Se emplean estructuras de postes de madera en forma de H, para las líneas de 69 a 231 kV. Se utilizan estructuras de acero independientes, de circuito simple, para las líneas de 161 kV o más. Es posible tener líneas de transmisión de hasta 1.000 kV.

Al estar estas formadas por estructuras hechas de perfiles de acero, como medio de sustentación del conductor se emplean aisladores de disco o aisladores poliméricos y herrajes para soportarlos.

El impacto ambiental potencial de líneas de transmisión de energía eléctrica incluyen la red de transporte de energía eléctrica, el derecho de vía, las playas de distribución, las subestaciones y los caminos de acceso o mantenimiento. Las estructuras principales de la línea de transmisión son la línea misma, los conductores, las torres y los soportes.

Las líneas de transmisión pueden tener pocos, o cientos de kilómetros de longitud. El derecho de vía donde se construye la línea de transmisión puede variar de 20 a 500 metros de ancho, o más, dependiendo del tamaño de la línea, y el número de líneas de transmisión. Las líneas de transmisión son, principalmente, sistemas terrestres y pueden pasar sobre los humedales, arroyos, ríos y cerca de las orillas de los lagos, bahías, etc. Son técnicamente factibles, pero muy costosas, las líneas de transmisión subterráneas.

Las líneas de transmisión eléctrica son instalaciones lineales que afectan los recursos naturales y socioculturales. Los efectos de las líneas cortas son locales; sin embargo, las más largas pueden tener efectos regionales. En general, mientras más larga sea la línea, mayores serán los impactos ambientales sobre los recursos naturales, sociales y culturales. Como se tratan de instalaciones lineales, los impactos de las líneas de transmisión ocurren, principalmente, dentro o cerca del derecho de vía. Cuando es mayor el voltaje de la línea, se aumenta la magnitud e importancia de los impactos, y se necesitan estructuras de soporte y derechos de vía cada vez más grandes. Se aumentan también los impactos operacionales. Por ejemplo, los efectos del campo electromagnético (EMF) son mucho mayores para las líneas de 1.000 kV, que para las de 69 kV.

Los impactos ambientales negativos de las líneas de transmisión son causados por la construcción, operación y mantenimiento de las mismas. Las causas principales de los impactos que se relacionan con la construcción del sistema incluyen las siguientes:


La operación y mantenimiento de la línea de transmisión incluye el control químico o mecánico de la vegetación dentro del derecho de vía y, de vez en cuando, la reparación y mantenimiento de la línea. Estas actividades, más la presencia física de la línea misma, pueden causar impactos ambientales.

En el lado positivo, al manejarlos adecuadamente, los derechos de vía de las líneas de transmisión pueden ser beneficiosos para la fauna. Las áreas desbrozadas pueden proporcionar sitios de reproducción y alimentación para las aves y los mamíferos. El efecto de "margen" está bien documentado en la literatura biológica; se trata del aumento de diversidad que resulta del contacto entre el derecho de vía y la vegetación existente. Las líneas y las estructuras pueden albergar los nidos y servir como perchas para muchas aves, especialmente las de rapiña.

Las líneas eléctricas pueden dar lugar a la electrocución tanto de aves como de mamíferos, cuando éstos tocan dos conductores o un conductor y parte del apoyo. El grupo más afectado son las aves, de las que mueren miles anualmente al posarse sobre los apoyos. Entre las aves, las más afectadas son las rapaces, ya que suelen posarse sobre los apoyos para utilizarlos como oteaderos para acechar a sus presas.

El diseño de los apoyos, la orografía del terreno, la meteorología o el hábitat, son algunos de los factores que condicionan la peligrosidad de las líneas. Aumentar la separación entre los conductores o entre conductores y la cruceta son algunas de las medidas que permiten disminuir el riesgo de electrocución. También, forrar los conductores con materiales aislantes y resistentes a la intemperie en las zonas de mayor riesgo de contacto.

Además de por electrocución, las líneas de transporte de electricidad también causan accidentes a las aves por colisión con los conductores. Este problema es especialmente grave en zonas con nieblas frecuentes, ya que con la niebla las aves no ven los cables del tendido y es más fácil que choquen con ellos. Se pueden colocar distintos dispositivos para aumentar la visibilidad de los cables, tales como espirales de polipropileno, bolas de distintos materiales, lazos, piezas reflectantes, etc.

El mayor impacto de las líneas de transmisión de energía eléctrica se produce en los recursos terrestres. Se requiere un derecho de vía exclusivo para la línea de transmisión de energía eléctrica. Normalmente, no se prohíbe el pastoreo o uso agrícola en los derechos de vía, pero, en general, los otros usos son incompatibles. Si bien no son muy anchos los derechos de vía, pueden interrumpir o fragmentar el uso establecido de la tierra en toda su extensión. Las líneas de transmisión largas afectarán áreas más grandes y causarán impactos más significativos.

Las líneas de transmisión pueden abrir las tierras más remotas para las actividades humanas como colonización, agricultura, cacería, recreación, etc. La ocupación de espacio reservado al derecho de vía puede provocar la pérdida o fragmentación del hábitat, o la vegetación que encuentra en su camino. Estos efectos pueden ser importantes si se afectan las áreas naturales, como humedales o tierras silvestres, o si las tierras recién accesibles son el hogar de los pueblos indígenas.

Hay una variedad de técnicas para limpiar la vegetación del derecho de vía y controlar la cantidad y tipo de la nueva vegetación. Desde el punto de vista ambiental, el desbroce selectivo utilizando medios mecánicos o herbicidas es preferible y debe ser analizado en las evaluaciones ambientales del proyecto.

Se debe evitar el rocío aéreo de herbicidas porque no es selectivo e introduce grandes cantidades de químicos al medio ambiente, y además es una técnica de aplicación imprecisa y puede contaminar las aguas superficiales y las cadenas alimenticias terrestres, y eliminar las especies deseables y envenenar la fauna.

Al colocar líneas bajas o ubicarlas próximas a áreas con las actividades humanas (p.e., carreteras, edificios) se incrementa el riesgo de electrocución. Normalmente, las normas técnicas reducen este peligro. Las torres y las líneas de transmisión pueden interrumpir la trayectoria de vuelo de los aviones cerca de los aeropuertos y poner en peligro las naves que vuelan muy bajo, especialmente, las que se emplean para actividades agrícolas.

Las líneas de transmisión de energía eléctrica crean campos electromagnéticos. Se disminuye la potencia de los campos, tanto eléctricos, como magnéticos, con el aumento de la distancia de las Líneas de transmisión. La comunidad científica no ha llegado a ningún consenso en cuanto a las respuestas biológicas específicas a la fuerza electromagnética, pero resultados emergentes en comunidades anexas a esta influencia física, sugieren que hay antecedentes fundamentados de riesgos para la salud, asociados a algunos tipos de cáncer.

Se han promulgado normas en varios estados de los Estados Unidos que reglamentan la fuerza electromagnética que está asociada con las líneas de transmisión de alto voltaje.

Si bien, existe gente que argumenta que las líneas de alta tensión pudiesen afectar el medioambiente y a la gente que vive cerca de las líneas de transmisión, lo cierto es que dicha contaminación electromagnética se ve aplacada por los beneficios económicos de transportar la potencia a una tensión elevada. Existen países en los cuales se subsidia a la gente que vive bajo o en las inmediaciones de las líneas de alta tensión, bajo el supuesto que los tejidos orgánicos pudiesen ser perjudicados por los campos electromagnéticos provocados.

Dependiendo de su ubicación, las líneas de transmisión pueden inducir desarrollo en los derechos de vía o junto a estos, o en las tierras que se han vuelto más accesibles. En los lugares donde la vivienda sea escasa, los derechos de vía, a menudo, son sitios atractivos para construir viviendas informales, y esto, a su vez, causa otros impactos ambientales y sobrecarga la infraestructura y servicios públicos locales.

La electricidad es una de las pocas energías que no es posible almacenar a gran escala (excepto los sistemas de baterías o las presas hidráulicas que pueden ser consideradas reservas electromecánicas de energía de baja inercia). Por ello los operadores de red deben de garantizar el equilibrio entre la oferta y la demanda en permanencia. Si se produce un desequilibrio entre oferta y demanda, se pueden provocar dos fenómenos negativos:

En el caso en que el consumo supera la producción, se corre el riesgo de «apagón» por la rápida pérdida de sincronismo de los alternadores, mientras que en el caso de que la producción sea superior al consumo, también puede provocarse un “apagón” por la aceleración de los generadores que producen la electricidad.

Esta situación es típica de las redes eléctricas insulares donde la sobre-producción eólica conlleva a veces la aparición de frecuencias “altas” en las redes.

Las interconexiones entre los países pueden repartir mejor el riesgo de apagones en los territorios interconectados, al ser estos solidarios entre sí en la gestión del equilibrio entre la oferta y la demanda.

La aparición masiva de redes de generación distribuida también conduce a tener en cuenta este balance global de las redes, especialmente en cuestiones en tensión. La aparición de redes inteligentes deben contribuir al equilibrio general de la red de transporte (frecuencia y tensión), con el equilibrio las redes locales de distribución. Para ello los operadores europeos reflexionan sobre las soluciones técnicas pertinentes teniendo en cuenta la evolución de los modos de generación, hoy por hoy muy centralizados (hidroeléctrica, térmicas o nucleares), pero que podrían llegar a ser mucho más descentralizados en un futuro cercano (energía eólica o solar fotovoltaica).

Mientras qué, una distribuidora gestiona los redes de distribución, una comercializadora compra la electricidad en el mercado mayorista para venderla a consumidores que no quieren o no pueden contratar directamente con los generadores.





</doc>
<doc id="17411" url="https://es.wikipedia.org/wiki?curid=17411" title="Organización de los Estados Americanos">
Organización de los Estados Americanos

La Organización de los Estados Americanos (OEA) es una organización internacional panamericanista de ámbito regional y continental creada el 30 de abril de 1948, con el objetivo de ser un foro político para la toma de decisiones, el diálogo multilateral y la integración de América. La declaración de la organización dice que trabaja para fortalecer la paz, seguridad y consolidar la democracia, promover los derechos humanos, apoyar el desarrollo social y económico favoreciendo el crecimiento sostenible en América. En su accionar busca construir relaciones más fuertes entre las naciones y los pueblos del continente. Los idiomas oficiales de la organización son el español, el portugués, el inglés y el francés. Sus siglas en español son OEA y en inglés OAS (Organization of American States).

La OEA tiene su sede en el Distrito de Columbia, Estados Unidos. También posee oficinas regionales en los distintos países miembros. La organización está compuesta de 35 países miembros. En el trigésimo noveno período ordinario de sesiones de la Asamblea General, realizada del 1 al 3 de junio de 2009 en San Pedro Sula (Honduras), en su Resolución AG/RES. 2438 (XXXIX-O/09) señala que la Resolución VI adoptada el 31 de enero de 1962 en la Octava Reunión de Consulta de Ministros de Relaciones Exteriores, mediante la cual se excluyó al Gobierno de Cuba de su participación en el sistema interamericano, queda sin efecto en la Organización de los Estados Americanos; a partir de esa fecha quedó sin efecto dicha exclusión (pero Cuba no se ha reincorporado). La OEA es uno de los organismos regionales más antiguos y el segundo más extenso después del Diálogo de Cooperación de Asia.

En 1890, la Primera Conferencia Internacional Panamericana, efectuada en la ciudad de Washington, estableció la Unión Internacional de las Repúblicas Americanas y su secretaría permanente, la Oficina Comercial de las Repúblicas Americanas, precursora de la OEA. En 1910, esta organización se convirtió en la Unión Panamericana.

El 30 de abril de 1948, 21 naciones del hemisferio se reunieron en Bogotá (Colombia), para adoptar la "Carta de la Organización de los Estados Americanos", con la cual confirmaron su respaldo a las metas comunes y el respeto a la soberanía de cada uno de los países. La OEA tuvo una inauguración turbulenta, ya que la IX Conferencia Panamericana debió ser trasladada a los campos del Gimnasio Moderno por los disturbios del 9 de abril.

La OEA se alineó ampliamente con las posiciones del gobierno de EE.UU. durante la Guerra Fría. Sin embargo, en raras ocasiones se opuso a los Estados Unidos: durante los conflictos marítimos entre los Estados Unidos y el Ecuador y el Perú a finales del decenio de 1960, durante la guerra de las Malvinas en 1982 y en el momento de la invasión de Panamá por el ejército de los Estados Unidos en 1989.

En la década de 2000, la llegada al poder de la izquierda en varios países latinoamericanos llevó, en 2005, a la elección al frente de la OEA, por primera vez en la historia de la organización, de un Secretario General no apoyado por el gobierno americano: el chileno José Miguel Insulza. Según un diplomático que siguió de cerca las negociaciones finales, "hay muchos indicios de que, antes de darle vía libre, los Estados Unidos obtuvieron compromisos tanto de Insulza como del gobierno chileno, en particular en lo que respecta a la política de la OEA hacia Venezuela y Cuba".

En la década de 2010 se ha produjo un retorno al poder de la derecha en la mayoría de los países del hemisferio, lo que ha dado a la OEA un impulso más conservador. Esto fue celebrado por Mike Pompeo, el Secretario de Estado de Relaciones Exteriores de los Estados Unidos, como un "retorno al espíritu de la OEA de los años 50 y 60" (declaración hecha el 17 de enero de 2020). Luis Almagro, Secretario General de la organización, aboga por una "línea dura" hacia Venezuela y plantea la posibilidad de una intervención armada contra el país. En octubre de 2019, denunció el "papel de Cuba y Venezuela" en la ola de "desestabilización" en Ecuador, Colombia y Chile: "Las brisas del régimen bolivariano impulsado por el madurismo y el régimen cubano llevan la violencia, el saqueo, la destrucción y la intención política de atacar directamente el sistema democrático y forzar la interrupción de los mandatos constitucionales. "Cinco días después, felicitó al presidente ecuatoriano Lenín Moreno por la forma en que enfrentó al movimiento social. Sobre todo, la OEA contribuyó de manera muy controvertida al derrocamiento del presidente boliviano Evo Morales en noviembre de 2019, acusándolo de fraude electoral, sin llegar a fundamentar esta acusación.

Todos los Estados independientes de América son miembros de la OEA. No son miembros los territorios que son considerados dependencias de algún otro Estado, tal es el caso de Puerto Rico, la Guayana Francesa, Groenlandia, entre otros. 

La organización es particularmente respetada por las elites latinoamericanas. El embajador de América Latina o el Caribe ante la OEA es uno de los diplomáticos más importantes de su país. En cuanto al Secretario General, ejerce una influencia en los debates políticos de los países miembros. Sin embargo, la organización no desempeña ningún papel en la política interna de los Estados Unidos y es en gran medida desconocida en el país, tanto para las elites políticas como para el público en general. 

Los primeros miembros fueron las 21 repúblicas independientes americanas el 8 de mayo de 1948. Luego, la OEA, se fue expandiendo con la incorporación de Canadá y a medida que se independizaban otros territorios americanos. Actualmente hay 35 países:

Durante la 6.ª Conferencia de Cancilleres de la Organización de Estados Americanos (OEA), en Costa Rica, entre el 16 y el 20 de agosto de 1960, se acordó a unanimidad una condena contra el Estado de la República Dominicana. La sanción se motivó porque los cancilleres comprobaron la veracidad de que el régimen de Trujillo había auspiciado el atentado contra Rómulo Betancourt, en ese momento, presidente constitucional de Venezuela. A esa reunión asistieron los ministros de relaciones exteriores de 21 naciones americanas, Cuba inclusive, porque en ese momento no había sido expulsada del sistema interamericano.

Todos los países, Estados Unidos y Haití inclusive rompieron relaciones diplomáticas con la República Dominicana. Adicionalmente se aplicó un bloqueo económico que afecto la exportaciones de azúcar, que en ese momento eran el pilar de la economía dominicana.

Fue la primera aplicación del Tratado Interamericano de Asistencia Recíproca, el cual se había aprobado en la OEA el 29 de julio de 1960.

En 1962, Cuba fue expulsada de participar en la organización. Esta decisión fue tomada mediante la Resolución VI, adoptada en la octava cumbre en Punta del Este (Uruguay), el 31 de enero de 1962.
La votación se produjo con el voto en contra de Cuba. Esta resolución también contó con varias abstenciones de países iberoamericanos que no quisieron verse implicados, pero sí seguir manteniendo relaciones con Estados Unidos: Argentina, Bolivia, Brasil, Chile y Ecuador.

La parte operativa de la resolución decía literalmente que la adhesión al marxismo-leninismo es incompatible con el sistema interamericano y que el alineamiento de tal gobierno con el bloque comunista rompía la unidad y solidaridad continental; que el gobierno de Cuba, identificado con el marxismo-leninismo, es incompatible con los principios y objetivos del sistema interamericano y que esta incompatibilidad excluye al gobierno cubano de participar en el sistema interamericano.

Sin embargo, al expulsar sólo al gobierno cubano la comisión de la OEA se dedicó a redactar informes sobre derechos humanos en Cuba y atender casos de ciudadanos cubanos que fueron cuestionados por otros países americanos miembros. Como respuesta, el Gobierno cubano envió una nota oficial a la organización que decía que Cuba había sido expulsada arbitrariamente y que la OEA no tenía ninguna jurisdicción ni competencia en el país. En cambio, no se excluyó de la OEA a ninguna de las dictaduras militares latinoamericanas.

El 3 de junio de 2009 en la XIX Asamblea General de la OEA, realizada en San Pedro Sula (Honduras), con el apoyo de Bolivia, Ecuador, Honduras, Nicaragua y Venezuela, se logra un acuerdo entre los cancilleres de los países integrantes de la OEA en la llamada Comisión General, presidida por el canciller canadiense Lawrence Cannon, para la reinclusión de Cuba en la entidad. Este acuerdo no integra a Cuba automáticamente a la OEA, sino que deroga en su primer artículo la resolución de 1962 que determinó su suspensión y establece en su artículo segundo la vía para la participación de Cuba. Este habría de constituir un diálogo iniciado por este país con la organización en conformidad con las «prácticas, principios y propósitos» de la OEA.

El 4 de julio de 2009, Honduras fue suspendida como miembro del organismo, luego de que el Golpe de Estado de 2009 enviase al exilio al presidente Manuel Zelaya y de que el nuevo presidente interino rechazase el ultimátum de la OEA para restituir a Zelaya en su puesto presidencial.

La suspensión, que tuvo carácter inmediato, fue avalada de manera unánime por 33 de los 34 países representados en la OEA, pues la delegación de Honduras se abstuvo de votar.

En su asamblea extraordinaria sobre el Golpe de Estado en el país centroamericano, el organismo consideró que no existía otra alternativa después de que el régimen "de facto" hondureño rechazó el ultimátum dado por la OEA para devolver el poder a Zelaya.

Con esta resolución, a Honduras le es aplicado un artículo de la OEA referente a la ruptura del orden constitucional al amparo del artículo 20 de la Carta Democrática Interamericana que fue adoptada por la OEA en 2001.

La suspensión de Honduras en la OEA, implicaría sanciones económicas para el país y agudizaría los efectos de la crisis mundial en el país exportador de café y textiles, muy dependiente de los préstamos de organismos multilaterales.

Al conocer que el país iba a ser expulsado de la organización panamericana el gobierno de Roberto Micheletti se adelantó declarando que ellos mismos eran quienes se retiraban, sin embargo ésta retirada no tuvo ninguna validez puesto que la OEA considera ilegítimo tanto al gobierno como a sus decisiones.

El 1 de junio de 2011 Honduras se reincorporó a la OEA al aprobarse una resolución en la que Ecuador fue el único de todos los estados miembros que emitió voto en contra. Honduras vuelve al organismo continental cuatro días después de que Zelaya regresó del exilio en República Dominicana, la principal demanda de los estados miembros de la OEA para permitir el reingreso de Honduras a la organización. Zelaya regresó mediante un acuerdo de reconciliación que firmó con el entonces mandatario Porfirio Lobo el 22 de mayo de 2011, bajo la mediación de Venezuela y Colombia.

Ya en 2012 el presidente venezolano Hugo Chávez criticó a la OEA como un organismo al servicio de los intereses imperialistas de Estados Unidos en el continente americano y denunció (entiéndase como proceso de renuncia) la Convención Americana de Derechos Humanos, así como la jurisdicción de la Corte Internacional de Derechos Humanos sobre Venezuela.

Ante las continuas críticas de Luis Almagro, Secretario General de la OEA, sobre la situación en Venezuela, en abril de 2017 el presidente de Venezuela Nicolás Maduro comenzó el proceso del retiro de la Organización de Estados Americanos tras lo que calificó como una serie de ataques injerencistas por parte de un grupo de países en el seno de la OEA y de su Secretario General, a instancias de Estados Unidos. Hizo efectivo el anuncio luego de realizarse una reunión no consensuada que denunció como violatoria de la normativa de esta institución. Para concretarse la salida, una vez denunciada la carta fundamental de la OEA (así se llama al proceso) tienen que pasar dos años desde que iniciado el proceso de retiro y liquidarse toda deuda con el organismo que asciende a la cantidad de doce millones de dólares.

En tanto se cumplía el plazo de la salida formal de Venezuela de la OEA, se emitió una resolución el 5 de junio de 2018 sobre el desconocimiento de la legitimidad de las elecciones presidenciales realizadas el 20 de mayo de 2018 en Venezuela. La votación fue la siguiente:

Posteriormente, en 2019 la Asamblea Nacional desconoció el gobierno del presidente Nicolás Maduro y reconoció a Juan Guaidó como presidente encargado para un gobierno de transición, gobierno reconocido por algunos países dentro y fuera de la región de la OEA. Bajo este nuevo escenario interno venezolano, a vísperas de cumplirse los dos años para formalizar la salida de Venezuela de la OEA, la Asamblea Nacional nombró a Gustavo Torre como nuevo representante permanente ante este organismo, quien tomó asiento el 9 de abril. Torre de inmediato detuvo el proceso de retiro de Venezuela de la OEA, aseguró que se liquidará la deuda que se tiene con este organismo, anunció que se reintegrará este país a la Comisión Interamericana de Derechos Humanos y a la Corte Interamericana de Derechos Humanos y en la operación diaria del organismo calificó de dictadura al gobierno de Daniel Ortega en Nicaragua (gran aliado de Nicolás Maduro).

Por su parte, el gobierno de Nicolás Maduro continuó con el proceso de salida de la OEA y su ministro de Relaciones Exteriores, Jorge Arreaza anunció el 27 de abril que "Desde hoy la República Bolivariana de Venezuela no pertenece a la OEA. Estamos fuera de la OEA por voluntad del pueblo, habiendo cumplido con todo lo que establece la carta fundacional". A pesar de este anuncio, Venezuela sigue siendo miembro de pleno derecho en la OEA.

El 12 de noviembre de 2010 el Consejo Permanente de la OEA rompió una tradición de 15 años de tomar todas sus decisiones bajo consenso. Aunque los reglamentos y estatutos del organismo proveen los mecanismos necesarios para la votación de las decisiones bajo mayoría simple o calificada dependiendo del asunto, la organización, bajo el espíritu de ser un organismo de diálogo multilateral no acostumbra a utilizar este mecanismo para la toma de decisiones. Sin embargo, el 12 de noviembre de 2010 el Consejo, bajo votación de 21 votos a favor, 3 abstenciones y 2 en contra aprobó una resolución conteniendo las recomendaciones del secretario general sobre el conflicto Costa Rica-Nicaragua por las actividades nicaragüenses denunciadas por el Gobierno de Costa Rica en el sector de la isla Calero.

En el ámbito de los países latinoamericanos se discute si la participación como grupo se debe dar en el ámbito de la OEA, o de la mucho más joven CELAC (Comunidad de Estados Latinoamericanos y Caribeños) que no incluye a los Estados Unidos ni a Canadá. Se declaraba en la CELAC: Los mandatarios presentes en la cumbre manifestaron esperanza que la consolidación de la CELAC pueda suponer la liberación de los países latinoamericanos de la tutela tradicional de EE. UU. y Europa posibilitando el avance en la integración de los pueblos, la resolución de sus conflictos así como la promoción del desarrollo económico. En mayo del 2015 el presidente de la República de Ecuador Rafael Correa afirmaba que la OEA está "totalmente influenciada por el poder de los países hegemónicos".

Desde un principio se ha puesto en duda la independencia del organismo frente a la política exterior de Estados Unidos, esta hipótesis se mostró en 1962, cuando la OEA apoyó todas las medidas de «cuarentena» declarada por el presidente Kennedy y expulsó de la OEA al Gobierno cubano, alegando la incompatibilidad de los principios de la organización con el «comunismo».

Nuevamente fue cuestionado su papel por la renunciao golpe cívico-militar del expresidente Evo Morales tras las intensas protestas en Bolivia de 2019, y un llamado a nuevas elecciones motivado por un informe de la OEA según el cual el 20 de octubre hubo fraude a favor de Evo Morales . Algunas ONGs internacionales sacaron reportes y argumentos sobre cómo posiblemente no hubo fraude en las elecciones . Además Almagro se negó a calificar como golpe de estado al derrocamiento de Morales, afirmando en su lugar que fue Evo quien se dio un "autogolpe" al realizar el presunto fraude .


Según la Carta de la OEA (Título VIII), las instancias consultivas y políticas son:




</doc>
<doc id="17414" url="https://es.wikipedia.org/wiki?curid=17414" title="Charles Sanders Peirce">
Charles Sanders Peirce

Charles Sanders Peirce ("purse" en inglés) (Cambridge, Massachusetts, 10 de septiembre de 1839 - Milford, Pensilvania, 19 de abril de 1914) fue un filósofo, lógico y científico estadounidense. Es considerado el fundador del pragmatismo y el padre de la semiótica moderna o teoría de los signos, junto a Ferdinand de Saussure.

Hijo de Sarah y Benjamin Peirce, fue profesor de astronomía y matemáticas en la Universidad Harvard. Aunque se graduó en química en la Universidad Harvard, nunca logró tener una posición académica permanente a causa de su difícil personalidad (tal vez maniaco-depresiva), y del escándalo que rodeó a su segundo matrimonio después de divorciarse de su primera mujer, Melusina Fay. Desarrolló su carrera profesional como científico en la United States Coast Survey (1859-1891), trabajando especialmente en astronomía, en geodesia y en medidas pendulares. Desde 1879 hasta 1884 fue profesor de lógica a tiempo parcial en la Universidad Johns Hopkins. Tras retirarse en 1887, se estableció con su segunda mujer, Juliette Froissy, en Milford (Pensilvania) donde murió de cáncer después de 26 años de escritura intensa y prolífica. No tuvo hijos.

Para los hispanoparlantes no deja de ser sorprendente que desde 1890 Peirce añadiera un "Santiago" a su nombre y utilizara con alguna frecuencia para su firma la de "Charles Santiago Sanders Peirce". Puede encontrarse una detallada explicación al respecto (en inglés).

Peirce publicó dos libros, "Photometric Researches" (1878) y "Studies in Logic" (1883), y un gran número de artículos en revistas de diferentes áreas. Sus manuscritos, una gran parte de ellos sin publicar, ocupan cerca de 80.000 páginas. Entre 1931 y 1958 se ordenó temáticamente una selección de sus escritos y se publicó en ocho volúmenes con el nombre de "Collected Papers of Charles Sanders Peirce" (generalmente citado por volumen [punto] párrafo, en la forma ""CP" x.y"). Desde 1982, se han publicado además algunos volúmenes de "Writings of Charles S. Peirce: A Chronological Edition" (volumen [dos puntos] página: ""W" x:y"), que aspira a alcanzar treinta volúmenes.

William James reconoció a Charles Peirce como fundador del pragmatismo. El pragmatismo, como Peirce lo describía, puede entenderse como un método de resolver confusiones conceptuales relacionando el significado de concepto alguno con un concepto de las concebibles consecuencias prácticas de los efectos de la cosa concebida ("CP" 8.208) — las implicaciones imaginables para la práctica informada. El significado de un concepto es general y consiste no en los resultados individuales fácticos mismos sino en el concepto general de los resultados que ocurrirían. Sin ninguna duda, esta teoría no guarda ninguna semejanza con la noción vulgar de pragmatismo, que connota una burda búsqueda del beneficio así como la conveniencia política. En cambio, el pragmatismo de Peirce es un método de experimentación conceptual, hospitalario para la formación de hipótesis explicativas, y propicio para el uso y la mejora de la verificación. Típico de Peirce es su interés en la formación de hipótesis explicativas como fuera de la alternativa fundacional habitual entre el racionalismo deductivista y el empirismo inductivista, aunque Peirce fue un lógico matemático y un fundador de la estadística.

Peirce es también considerado como el padre de la semiótica moderna: la ciencia de los signos. Más aún, su trabajo —a menudo pionero— fue relevante para muchas áreas del conocimiento, tales como astronomía, metrología, geodesia, matemáticas, lógica, filosofía, teoría e historia de la ciencia, semiótica, lingüística, econometría y psicología. Cada vez más, ha llegado a ser objeto de abundantes elogios. Popper lo ve como “uno de los filósofos más grandes de todos los tiempos”. Por lo tanto, no es sorprendente que su trabajo y sus ideas acerca de muchas cuestiones hayan sido objeto de renovado interés, no solo por sus inteligentes anticipaciones a los desarrollos científicos, sino sobre todo porque muestra efectivamente cómo volver a asumir la responsabilidad filosófica de la que abdicó gran parte de la filosofía del siglo XX.

La filosofía de Peirce incluye:

En la obra de Peirce, el falibilismo y el pragmatismo pueden parecer que funcionan algo así como el escepticismo y el positivismo, respectivamente, en las obras de otros. Sin embargo, para Peirce, el falibilismo se equilibra con un anti-escepticismo y es una base para creer en la realidad del azar absoluto y de la continuidad ("CP" 1.141–75), y el pragmatismo somete a uno a la creencia anti-nominalista en la realidad de lo general ("CP" 5.429–35).

Sin embargo, Charles S. Peirce no debería ser considerado principalmente como filósofo o como lógico, sino como científico, tanto por su formación como por su carrera profesional. Sus informes a la "Coast Survey" son un testimonio notable de su experiencia personal en el duro trabajo de medir y obtener evidencias empíricas. Una mirada a esos informes oficiales o a sus "Photometric Researches" producidos en los años 1872-1875 proporciona una vívida impresión de trabajo científico sólido. Como escribió Max Fisch, «Peirce no era meramente un filósofo o un lógico que ha estudiado cuestiones científicas. Era un científico profesional con todo derecho, que llevó a su trabajo las preocupaciones del filósofo y del lógico».

Aunque Peirce fue un filósofo sistemático en el sentido tradicional de la palabra, su obra aborda los problemas modernos de la ciencia, la verdad y el conocimiento a partir de su propia experiencia como lógico y científico experimental que trabajaba en el seno de una comunidad internacional de científicos y pensadores. Aunque realizó importantes contribuciones a la lógica deductiva, Peirce estaba principalmente interesado en la lógica de la ciencia y, más especialmente, en lo que llamó abducción (como complemento a los procesos de deducción e inducción), que es el proceso por el que se genera una hipótesis, de forma que puedan explicarse hechos sorprendentes. Peirce consideró que la abducción estaba en el corazón no solo de la investigación científica sino de todas las actividades humanas ordinarias.

Una dificultad en el estudio de Peirce es que la interpretación del pensamiento de Peirce ha provocado durante años un amplio desacuerdo entre los estudiosos peirceanos, debido en parte a la presentación fragmentaria y caótica de su obra en los "Collected Papers" y en parte a su ir contracorriente. El hecho es que Peirce no es un filósofo fácil de clasificar: algunos lo consideraron un pensador sistemático, pero con cuatro sistemas sucesivos, otros lo vieron como un pensador contradictorio, o como un metafísico especulativo de tipo idealista. Sin embargo, en años más recientes ha comenzado a ganar aceptación general una comprensión más profunda de la naturaleza arquitectónica de su pensamiento y de su evolución desde sus primeros escritos en 1865 hasta su muerte en 1914. En la última década todos los estudiosos peirceanos han reconocido claramente la coherencia básica y la sistematización del pensamiento de Peirce.

Las afirmaciones de Peirce sobre la naturaleza de la actividad científica tienen una sorprendente continuidad con las discusiones contemporáneas en epistemología, metodología y filosofía de la ciencia, sobre todo por el énfasis que puso en el carácter social y comunitario de la ciencia. Sin duda, algunas de las manifestaciones de su absoluta confianza en el progreso científico resultan hoy en día anacrónicas. Peirce era un hombre del siglo XIX y, en consonancia con el espíritu de su época, tenía una fe casi religiosa en la capacidad de la ciencia para descubrir la verdad. En este sentido, Peirce era un firme defensor de una aproximación científica a la filosofía. Es más, en cierto modo Peirce quería transformar la filosofía en una ciencia estricta, hacer de la filosofía una “filosofía científica”, no solo en los ámbitos de la lógica y la epistemología, sino de manera más urgente y necesaria en metafísica y cosmología.

Hoy en día esa pretensión puede parecer anticuada, e incluso ridícula, propia de los filósofos del pasado o del positivismo más crudo e intransigente. Esta actitud científica ha motivado que Peirce, a diferencia de otros pragmatistas como William James o F. C. S. Schiller, fuera visto con simpatía e incluso admiración por parte de muchos pensadores de la tradición de la filosofía analítica. Sin embargo, aunque en alguna ocasión denominara al pragmatismo como una filosofía proto-positivista ("EP" 2:339, 1905), sería más que inexacto decir que Peirce fue un filósofo positivista en sentido estricto. En primer lugar, una de las lecciones que más vivamente aprendió del devoto espíritu unitario de Harvard, —del que su padre, Benjamin Peirce, fue incansable promotor— era la idea de reconciliar ciencia y religión. Este es, efectivamente, un impulso central en toda la obra de Peirce que a menudo ha pasado desapercibido por los autores que sostienen una lectura naturalista de la máxima pragmática y del método científico. De hecho, para Peirce la investigación científica es la actividad religiosa por excelencia, puesto que su objeto es, sencillamente, la búsqueda apasionada y desinteresada de la verdad ("CP" 1.234, 1901). 

Peirce adoptó un concepto muy amplio de ciencia que no quedaba restringido a las ciencias entendidas como ciencias de laboratorio. Para él la ciencia no consiste ni única ni principalmente en una colección de hechos o métodos, ni siquiera en un conjunto sistemático de conocimientos; se trata de una actividad social. Esto es, la ciencia es una investigación auto-controlada, responsable y auto-correctiva llevada a cabo por hombres y mujeres reales bajo un mismo principio de cooperación con vistas a un fin muy particular: la consecución de la verdad ("CP" 7.87, 1902; cfr. "EP" 2:459, 1911). En otras palabras, la ciencia es un “proceso vivo” encarnado en un grupo de investigadores y animado por un intenso deseo de averiguar cómo son las cosas realmente ("CP" 1.14, c.1897), por “un gran deseo de aprender la verdad” ("CP" 1.235, 1902). De hecho, dirá Peirce, “el deseo de aprender” es el más importante requisito de la ciencia y la primera regla de la razón ("CP" 1.135, c. 1899). Este requisito viene de la mano de otro precepto que, según Peirce, debería escribirse en todas las paredes de la ciudad de la filosofía: “no bloquear el camino de la investigación” ("CP" 1.135, c. 1899). De acuerdo con su experiencia como científico entrenado en las salas de laboratorio, Peirce quería hacer de la filosofía una ciencia alejada tanto del diletantismo literario como de la filosofía académica tradicional, a la que consideraba animada por un espíritu dogmático y racionalista. 

Pero esto no suponía reducir, como hacía el positivismo, todos los modos de conocimiento al conocimiento científico, sino que indicaba simplemente la necesidad de abordar los problemas filosóficos con una actitud experimental. Es decir, con un talante comunicativo y abierto a la revisión continua, a la necesaria corrección que implican tanto la discusión pública con los colegas como el contraste con la experiencia en el proceso de investigación científica. Esta actitud, que Peirce denominó falibilismo, era una consecuencia necesaria de su rechazo radical del fundacionalismo característico de la filosofía moderna, que consideraba encarnada de modo prototípico en la figura de Descartes. En concreto, Peirce criticó muy duramente el repliegue de la filosofía moderna hacia el interior de la conciencia, el recurso a la introspección como garantía del conocimiento y la idea de intuición, entendida como aquella cognición no determinada por cogniciones previas. En su rechazo del espíritu escolástico, el cartesianismo había hecho del cogito la fuente última de la certeza, así como el eslabón fundante de todo el edificio del conocimiento, entendido como una cadena de razonamientos que se deducen de ese fundamento o principio necesario. Como consecuencia, el individuo y su conciencia constituían, en última instancia, la única garantía de la ciencia y el conocimiento racional. Para Peirce esto era una "filosofía de sillón", meramente especulativa y alejada del modo en que realmente trabajan los científicos. Para Peirce la ciencia era, en gran medida, el trabajo cooperativo y comunitario de hombres y mujeres trabajando en intercomunicación, corrigiéndose unos a otros en un proceso continuo de revisión de hipótesis, que conduciría a una opinión final encarnada en una comunidad ideal de investigadores. 

De igual modo, la duda metódica era para Peirce un modo insincero de acercarse a los problemas del conocimiento, pues no tenía en cuenta que los seres humanos estamos siempre enmarcados en un proceso activo y dinámico de corrección y adquisición de nuevas creencias. Este proceso es descrito y detallado en lo que denominó como los métodos para la fijación de las creencias. En este proceso, la duda es una irritación, una insatisfacción real producida por la resistencia que la realidad impone sobre determinadas creencias previas debido a una situación nueva que desafía el conjunto de hábitos acumulado por la experiencia. La duda es, por tanto, un catalizador para la puesta en marcha de nuevas creencias que permitan controlar esa situación inestable y, por tanto, proporcionan al agente de disposiciones firmes para actuar. Como dice Peirce, no se puede dudar a placer. La duda cartesiana es una duda artificial, una “duda de papel” ("CP" 5.445, 1905). En definitiva, no podemos pretender dudar en la filosofía de aquello de lo que no dudamos en nuestros corazones ("CP" 5.265, 1868).

Frente a la concepción dualista que tiene su origen moderno en el lingüista Ferdinand de Saussure, para Peirce las palabras, los signos, no son sólo lo que está en nuestro discurso en lugar de las cosas, sino que, sobre todo, signo es «lo que al conocerlo nos hace conocer algo más» ("CP", 8.332, 1904). Según Peirce, "no tenemos ningún poder de pensamiento sin signos". En principio el proceso de indagación puede caracterizarse como un proceso que opera en virtud de la manipulación de signos (o "pensamientos–signos"). De acuerdo a Peirce, el pensamiento es continuo, es decir, en la continuidad del pensamiento los pensamientos–signos están en permanente flujo. Un pensamiento lleva a otro y éste a su vez a otro y así sucesivamente. Pero en el proceso de indagación, gatillado por la obstaculización del flujo de la experiencia, ejercemos un control sobre la continuidad del pensamiento. Dicho control hace posible constreñir las asociaciones de pensamiento.

Esto supone un contraste con los filósofos de la Edad Moderna, pues tanto racionalistas como empiristas sostuvieron que tenemos un conocimiento directo e infalible de nuestros propios pensamientos, y en ese conocimiento fundaron tanto la ciencia como la autonomía moral del individuo.
Desde sus primeros escritos Peirce rechazó tajantemente tanto el dualismo cartesiano como la tesis de Locke de que todo pensamiento era percepción interna de ideas. Esta concepción de los efectos del conocimiento determinará su original modo de considerar también el operar de los signos. Pierce estudió en profundidad el fenómeno del signo y elaboró su propia noción, diferente del concepto estructuralista de inspiración saussureana. Para Pierce el signo es algo que, bajo cierto aspecto, representa alguna cosa para alguien. Esto significa -en el fondo- que el signo posee una composición tríadica, y en ese cuerpo emergen y se hacen presente en él, sus tres elementos formales.

El ariete de toda su reflexión es la comprensión de la estructura triádica que conforma la relación lógica de nuestro conocimiento como un proceso de significación. Peirce caracteriza la noción de signo como:

Un signo o representamen es algo que está para alguien por algo en algún respecto o capacidad. Apela a alguien, esto es, crea en la mente de esa persona un signo equivalente o quizás un signo más desarrollado. Ese signo que crea lo llamó el interpretante del primer signo. El signo está por algo, su objeto. Está por ese objeto no en todos los respectos, sino por referencia a un tipo de idea a la que él llamó algunas veces "la base" del representamen.

La función representativa del signo no estriba en su conexión material con el objeto ni en que sea una imagen del objeto, sino en que sea considerado como tal signo por un pensamiento. En esencia, el argumento es que toda síntesis proposicional implica una relación significativa, una semiosis (la acción del signo), en la que se articulan tres elementos:


Todo signo es un "representamen". Representar es la operación más propia del signo, es estar en lugar del objeto «como el embajador toma el lugar de su país, lo representa en un país extranjero». Representar es «estar en una relación tal con otro que para un cierto propósito es tratado por una mente como si fuera ese otro. Así, un portavoz, un diputado, un agente, un vicario, un diagrama, un síntoma, una descripción, un concepto, un testimonio, todos ellos representan, en sus distintas maneras, algo más a las mentes que los consideran» ("CP" 2.273, 1901). Pensar es el principal modo de representar, e interpretar un signo es desentrañar su significado. El representamen no es la mera imagen de la cosa, la reproducción sensorial del objeto, sino que toma el lugar de la cosa en nuestro pensamiento. El signo no es solo algo que está en lugar de la cosa (que la sustituye, con la que está en relación de «equivalencia»), sino que es algo mediante cuyo conocimiento conocemos algo más. Al conocer el signo inferimos lo que significa. El representamen amplía así nuestra comprensión, de forma que el proceso de significación o semiosis llega a convertirse en el tiempo en un proceso ilimitado de inferencias. Por ello los signos no se definen sólo porque sustituyan a las cosas, sino porque funcionan realmente como instrumentos que ponen el universo al alcance de los intérpretes, pues hacen posible que pensemos también lo que no vemos ni tocamos o ni siquiera nos imaginamos.

Las personas o intérpretes son portadores de interpretantes, de interpretaciones. El signo crea algo en la mente del intérprete, y ese algo creado por el signo, ha sido creado también de una manera indirecta y relativa por el objeto del signo. En este sentido, puede decirse que la aportación capital de Peirce consiste en poner de manifiesto que, si se acepta que los procesos de significación son procesos de inferencia, ha de aceptarse también que la mayor parte de las veces, esa inferencia es de naturaleza hipotética («abductiva» en terminología de Peirce), esto es, que implica siempre una interpretación y tiene un cierto carácter de conjetura. Nuestra interpretación es siempre falible, esto es, puede ser siempre mejorada, corregida, enriquecida o rectificada.





</doc>
<doc id="17417" url="https://es.wikipedia.org/wiki?curid=17417" title="Medellín">
Medellín

Medellín es un municipio colombiano, capital del departamento de Antioquia. Es la ciudad más poblada del departamento y la segunda más poblada del país después de Bogotá. Se asienta en la parte más ancha de la región natural conocida como Valle de Aburrá, en la cordillera central de los Andes. Se extiende a ambas orillas del río Medellín, que la atraviesa de sur a norte, y es el núcleo principal del área metropolitana del Valle de Aburrá. La ciudad tiene una población proyectada de 2 508 452 habitantes (2018), mientras que dicha cifra, incluyendo el área metropolitana, asciende a 3 821 797 personas (2016).

En 1826 fue designada capital de Antioquia, título que ostentaba Santa Fe de Antioquia desde la época colonial. Durante el siglo XIX, Medellín se desarrolló como un centro dinámico de comercio, primero exportando oro, y posteriormente mercancías provenientes de la industrialización de la ciudad.

Como capital departamental, Medellín alberga las sedes de la Gobernación de Antioquia, la Asamblea Departamental, el Tribunal Superior del Distrito Judicial de Medellín, el Área metropolitana del Valle de Aburrá y la Fiscalía General, así como diferentes empresas públicas, instituciones y organismos del estado colombiano; es sede de empresas nacionales e internacionales en sectores como: textil, confecciones, metalmecánico, energético, financiero, salud, telecomunicaciones, construcción, automotriz, y alimentos, entre otros.

En 75 a.C., Quintus Caecilius Metellus Pius fundó una población en Hispania la cual nombró "Metellinum", actual Medellín de Extremadura, en la provincia de Badajoz, España. El nombre de la ciudad fue dado en 1675 en honor a Pedro Portocarrero y Luna, conde de Medellín en Extremadura y, por aquel entonces, presidente del Consejo de Indias. Esto fue realizado por el interés que tomó en la erección en villa del poblado de "Nuestra Señora de la Candelaria de Aná". En vista de aquello hubo una fuerte oposición de la ciudad de Santa Fe de Antioquia, que en dicha época era la capital de la Provincia de Antioquia. Esto se dio porque con dicha erección verían disminuida su jurisdicción y su control político sobre la provincia.

Ese mismo año, finalmente llegó la real cédula firmada por la reina regente, Mariana de Austria, en representación de Carlos II, menor de edad a la fecha. Fechada el 22 de noviembre de 1674, concede la fundación en Villa de la "Candelaria de Aná". El 2 de noviembre de 1675 le correspondió al gobernador y capitán general de la Provincia de Antioquia, Miguel de Aguinaga y Mendigoitia, proclamar la erección de la Villa de Nuestra Señora de la Candelaria de Medellín.

Hace 1500 años aproximadamente el Valle de Aburrá era recorrido por tribus de cazadores y recolectores. Cuando llegaron los conquistadores españoles encontraron asentada una numerosa población nativa, que opuso poca resistencia. Eran aburraes, yamesíes, peques, ebéjicos, noriscos y maníes que estaban allí desde el siglo V a.C., según estimaciones. Tenían grandes cultivos de maíz y fríjol, criaban curíes y perros mudos, tejían mantas de algodón, comerciaban con sal, y conocían la orfebrería. Bajo el dominio español fueron repartidos en encomiendas y desplazados de sus tierras. La deserción, el maltrato, las enfermedades y el duro trabajo intensivo en la tierra y las minas, en pocos años los diezmaron.

El valle en donde hoy se asienta Medellín fue descubierto el 24 de agosto de 1541, día de San Bartolomé, por Jerónimo Luis Tejelo, un capitán a órdenes del mariscal Jorge Robledo, quien fundó la ciudad de Santa Fe de Antioquia ese mismo año y es considerado el conquistador de Antioquia. El valle era llamado Valle de Aburrá por los indígenas que lo habitaban y fue llamado por los españoles Valle de San Bartolomé o de Los Alcázares. Los indígenas respondieron con belicosidad según algunos cronistas, resistencia que obligó a Tejelo a atrincherarse para la defensiva y a despachar un expreso al mariscal Robledo pidiéndole auxilio, con el cual vencieron fácilmente a los aborígenes. Dicha resistencia la hicieron propiamente los indígenas que habitaban el caserío de Guayabal, pues los demás que ocupaban el valle prefirieron huir o quitarse la vida.
El 2 de marzo de 1616 el licenciado Francisco de Herrera Campuzano, del Consejo del Rey, Oidor de la Real Audiencia del Nuevo Reino de Granada y Visitador General de la Provincia de Antioquia, fundó una población a la que llamó San Lorenzo de Aburrá, en donde hoy se sitúa El Poblado. El caserío, que estaba compuesto por trecientos indígenas y algunos pocos españoles, finalmente no prosperó y en 1646 fue trasladado al ángulo que forman el río Medellín (antes río Aburrá) y el arroyo Santa Elena, sitio que los indígenas llamaban Aná y los españoles Aguasal. Pocos años después se levantó la primera iglesia de tapias y tejas, consagrada a la Virgen de la Candelaria, y desde entonces la población se llamó Nuestra Señora de la Candelaria de Aná, que a los 54 años de vida contaba apenas con 700 habitantes de los 3000 que poblaban el territorio comprendido entre el Ancón de la Valeria (hoy Caldas) hasta los potreros de Barbosa.

Desde 1670 los habitantes pidieron a la Real Audiencia la elección en villa de su población, encontrando resistencia por parte de la ciudad de Santa Fe de Antioquia. Finalmente Mariana de Austria, viuda de Felipe IV, en nombre de su hijo Carlos II, otorgó la elección en villa de la pequeña población, que ahora pasó a llamarse Villa de Nuestra Señora de la Candelaria de Medellín. Un siglo después, en 1783, se abrieron nuevas calles por orden de Francisco Silvestre y Sánchez, quien fue gobernador de la Provincia de Antioquia. En 1786, el Oidor Juan Antonio Mon y Velarde hizo numerar las casas, que eran 242 de un piso y 29 de balcón, y marcar las calles con los nombres de San Francisco, San Lorenzo, La Amargura (hoy calle Ayacucho), El Prado, entre otros. También dictó medidas sobre saneamiento, instrucción pública, mejora del comercio y sistemas administrativos; igualmente dota a la villa de agua corriente, crea colonias agrícolas y estimula la minería. Estas medidas progresistas levantan el ánimo de los habitantes y permiten entrever tiempos mejores para la población y para la provincia entera.

Ya en el siglo XIX y en plena época de la Independencia, el 21 de agosto de 1813, Juan del Corral erige en ciudad a Medellín, privilegio que hasta entonces, y en lo que respecta a la provincia, solo tenían Santa Fe de Antioquia y Rionegro. En 1826 se le nombra capital de Antioquia, contando en ese año con 6050 habitantes.

En los primeros años del siglo XIX la ciudad experimentó un lento desarrollo debido, entre otras cosas, a las precarias vías de comunicación con el resto del país y el exterior. Desde el punto de vista intelectual, material y social se seguían conservando las características de pueblo de incipiente civilización. No fue sino hasta el periodo comprendido entre 1830 y 1850 cuando la ciudad comenzó su desarrollo paulatino. La educación dio en este periodo un salto trascendental. Durante la época colonial y aún en los inicios de la República se contaba con pocas escuelas y colegios, situación que cambió a partir de la mitad del siglo, siendo notable durante el gobierno de Pedro Justo Berrío.

El río Medellín carecía en aquel entonces de puentes que lo atravesaran y fue haciéndose necesaria la construcción de alguno, ya que sus aguas eran abundantes, sobre todo en invierno, y se requerían balsas para pasarlo. El puente de Colombia fue el primero dentro del territorio de la ciudad y fue levantado con auxilio nacional ofrecido por el entonces presidente Tomás Cipriano de Mosquera en 1846. El segundo puente sobre el río fue el de Guayaquil. 

En 1868 se decretó el traslado de la sede de la diócesis de Santa Fe de Antioquia a Medellín, lo que le permitió a esta fortalecer las instituciones religiosas que existían en aquel entonces. La construcción de la Catedral Metropolitana marcó un hito no solo en el ámbito religioso sino también desde el punto de vista arquitectónico. Igualmente el comercio se fue fortaleciendo hasta consolidarse como actividad financiera. Fue así como en Medellín surgió el Banco de Antioquia en 1871, el Banco de Medellín en 1881, el Banco Popular en 1882 y el Banco del Comercio en 1896.

El despegue de la ciudad hacia la modernidad coincidió con un acelerado crecimiento de su población, de 20 000 habitantes en 1870 a 140 000 en 1938. La ciudad se consolidó como un centro de comercio de oro, café, finca raíz, mecánica, fundición, especulación e importación de mercancías. Esta vocación comercial se complementó al comienzo del siglo XX con una industrial (textil, gaseosas, cigarrillos, calzado, entre otras), al aprovechar la presencia de abundantes fuentes hídricas, avances en movilidad y mercados cercanos.

Al tiempo que entraron los primeros automóviles importados de Estados Unidos y de Francia, se crearon fábricas importantes, en particular empresas textileras, como la Compañía Colombiana de Tejidos Coltejer (1907), o la fábrica de Hilados y Tejidos del Hato (Fabricato), la cual empezó a funcionar en los años veinte y en menos de dos décadas se consolidaría como la segunda textilera más importante, después de Coltejer. Otras empresas importantes fueron creadas en estos años como la Compañía Colombiana de Tabaco (1919), la fábrica de Gaseosas Lux (1925) y en lo referente a la industria cafetera se destaca la fundación de Café La Bastilla en 1922.

A partir de 1910 estas industrias se convirtieron en el motor principal del crecimiento urbano, y crearon una primera generación de empresarios industriales y de obreros asalariados. La ciudad atrajo inmigrantes del campo con aspiración de trabajar en las fábricas y almacenes. También llegaron inmigrantes más prósperos, como empresarios de la minería, comerciantes, ganaderos y jóvenes de familias pudientes, con la idea de educarse.

A mediados de siglo, la ciudad comenzó también a desarrollar una arquitectura moderna de la mano de arquitectos como el austriaco Federico Blodek, quien diseñó obras como los edificios de oficinas Suramericana,​ de Fabricato (que fue el más alto de la ciudad)​ y Banco de Colombia. El Plan Piloto de Medellín, elaborado en 1950 por los urbanistas Paul Wiener y José Luis Sert, recomendaba: canalizar el río, controlar los asentamientos en las laderas, montar una zona industrial en Guayabal, articular la ciudad en torno al río, construir el estadio Atanasio Girardot y el centro administrativo “La Alpujarra”. Pronto, el Plan Piloto se vio desbordado por la realidad de una población que se triplicó en 20 años, pasando de 358 189 habitantes, en 1951, a 1 071 252, en 1973.

En este periodo la construcción tuvo un gran dinamismo y los campesinos, que no tenían acceso a los créditos de vivienda, empezaron a construir en las laderas. Muchas de las edificaciones antiguas del centro, y aún las de principios del siglo XX, fueron demolidas para dar paso a los edificios que fueron destinados para oficinas y vivienda, entre ellos el de Coltejer, símbolo de la ciudad. El sector textil se modernizó bastante en este periodo y se consolidó de forma definitiva la vocación industrial de la ciudad.

Por primera vez, después de tener Antioquia una economía en ascenso durante 150 años, se presentan en la década de 1970 los síntomas iniciales de lo que sería la más grande crisis económica y social en su historia. Aparecen indicadores de aumento del desempleo, y con él la criminalidad y la inseguridad general. Aunque el país en su conjunto afrontó entre 1970 y 1980 un periodo crítico en su economía, esta crisis tuvo un especial impacto en Medellín, que llegó a tener la tasa de desempleo más alta de la nación. 

El sector manufacturero no solo había perdido dinámica, sino que se mostraba incapaz para afrontar la situación creada con los altos índices de desempleo, la recesión económica y la imposición desde el gobierno central de un nuevo modelo de desarrollo fundamentado en las actividades financieras y de la construcción. Es entonces cuando el contrabando, primero, y luego el narcotráfico, aparecen como alternativa para miles de personas que no tenían en el mercado legal ninguna o muy poca posibilidad de encontrar empleo o de ejercer una actividad económica rentable.

El Cartel de Medellín fue creado en 1976 y contó, hasta mediados de los años 1980, de una relativa libertad y tolerancia como resultado de su directa penetración en todos los sectores de la sociedad. Con la aprobación de las medidas que permitían la extradición de colombianos a los Estados Unidos, tomadas por el presidente Belisario Betancur luego del asesinato de su ministro de Justicia, el cartel de la droga inició un gran movimiento para desestabilizar al Estado. La ciudad sufrió todo el peso de la lucha entre el narcotráfico y el gobierno central en los últimos años de la década de los ochenta y a principios de los noventa. Aparecieron el narcoterrorismo, el sicariato, las bandas delincuenciales en los barrios populares, los secuestros, y los asesinatos de jueces y políticos.

La muerte de Pablo Escobar, en 1993, supuso el fin del llamado Cartel de Medellín, pero dejó profundos conflictos sociales en la región. La guerrilla y el paramilitarismo continuaron con su activismo armado que han creado duros impactos no solo en la ciudad sino en el país como el aumento de desplazados por la violencia y el endurecimiento de las políticas de seguridad del Estado como la Operación Orión en San Javier (octubre de 2002).

Hasta 2008 en la Región Paisa, de la que Medellín hace parte, operaban por lo menos seis de las principales bandas emergentes provenientes de los restos de los grupos paramilitares que se desmovilizaron durante las conversaciones de paz con el gobierno Colombiano en el periodo 2004-2006. Entre ellas están: Autodefensas Gaitanistas de Colombia, Águilas Negras, la Oficina de Envigado, Los Urabeños, Los Rastrojos o el grupo de Los Paisas; los cuatro últimos grupos son los que están activos en el país a 2015. 

Estos grupos criminales concentran sus operaciones en las ciudades y pequeños pueblos a lo largo y ancho del país, tratando de controlar los flujos de drogas hacia la costa Caribe y el control de la minería ilegal. Las rutas del narcotráfico se mueven especialmente por los departamentos de Córdoba y Sucre donde venden la droga a las organizaciones que cuentan con infraestructuras más grandes y pueden mover las drogas a nivel internacional.

La preocupación por detener los flujos de violencia urbana ha hecho que se presenten proyectos de inclusión social que incluyen grandes infraestructuras como los parques-bibliotecas en áreas urbanas conflictivas, los sistemas de transporte masivo como el Metro, el Metroplús, y el Tranvía y la participación del sector privado, oficial y las instituciones para unificar un proyecto de ciudad, la proliferación de eventos culturales y artísticos, la construcción de bibliotecas, parques y centros educativos y la creación y renovación del espacio público.

Medellín se encuentra ubicada en el centro geográfico del Valle de Aburrá, sobre la cordillera central de los Andes en las coordenadas . La ciudad cuenta con un área total de 328 km² de los cuales 110 km² son suelo urbano y 218 km² son suelo rural.

El valle de Aburrá posee una extensión de 1152 km² que forman parte de la cuenca del río Medellín, principal arteria fluvial que cruza la región de sur a norte. La conformación del Valle de Aburrá es el resultado de la unidad geográfica determinada por la cuenca del río Medellín y por una serie de afluentes que caen a lo largo de su recorrido. El Valle tiene una longitud aproximada de 60 kilómetros y una amplitud variable. Está enmarcado por una topografía irregular y pendiente, que oscila entre 1300 y 2800 metros sobre el nivel del mar. 

Las cordilleras que lo encierran, dan lugar a la formación de diversos microclimas, saltos de agua, bosques y sitios de diverso valor paisajístico y ecológico. El valle tiene una forma alargada y presenta un ensanchamiento en su parte media, el cual mide 10 kilómetros y es donde se localiza Medellín. El Valle de Aburrá está totalmente urbanizado en su parte plana, y muy ocupado en sus laderas.

Topográficamente la ciudad es un plano inclinado que desciende desde 1800 a 1500 metros de altura sobre el nivel del mar, sin embargo, la altura oficial de la ciudad es de 1479 msnm en la confluencia de las quebradas La Iguaná, Santa Elena y el río Medellín, y se eleva a 3200 msnm en los altos El Romeral, Padre Amaya y cuchilla Las Baldías. 

Dentro del paisaje urbano se destacan los cerros Nutibara y El Volador, que se levantan como manchas verdes en medio de la ciudad. Los altiplanos y montañas que circundan el valle sobrepasan los 2500 metros. Las principales alturas en el territorio de Medellín son: Alto Padre Amaya (3100 msnm), Alto Patio Bonito (2750 msnm), Alto Boquerón (2600 msnm), Alto Venteadero (2500 msnm) y el Alto Las Cruces (2400 msnm), entre otros.

El río Medellín es la corriente hidrográfica más importante de la ciudad, la divide en dos partes y es su drenaje natural. Nace en el alto de San Miguel, en el municipio de Caldas, a una altura de 3000 msnm; tiene una extensión aproximada de 100 km desde su nacimiento hasta su desembocadura (donde confluye con el río Grande y le dan nacimiento al Porce) y recibe las aguas de aproximadamente 196 afluentes a lo largo de todo su recorrido. 

En lo que respecta al territorio de la ciudad, recibe 57 afluentes directos y más de 700 corrientes de segundo y tercer orden, con 23 corrientes mayores, constituyendo una red hidrográfica de una densidad considerable. Las quebradas Santa Elena y La Iguaná , por su caudal y longitud de recorrido, son las de mayor importancia en el territorio municipal. La quebrada La Iguaná nace en la serranía de Las Baldías y la quebrada Santa Elena nace en el cerro Espíritu Santo Verde. La primera atraviesa la zona centro-occidental, mientras que la segunda atraviesa la zona centro-oriental y está cubierta en su paso por el centro de la ciudad.

Las corrientes mayores de la ciudad además de estas son de sur a norte: Doña María, La Aguacatala, La Jabalcona, La Volcana, La Presidenta, La Poblada, La Guayabala, Altavista, La Picacha, Ana Díaz, La Hueso, Malpaso, El Ahorcado, El Molino, La Quintana, La Bermejala, La Rosa, La Herrera, Cañada Negra y La Madera.

La latitud y altitud de la ciudad dan como resultado un clima subtropical monzónico. El clima es templado y húmedo, con una temperatura promedio de 21,6°C. El apelativo «"ciudad de la eterna primavera"» proviene de la fama de un clima bastante uniforme durante todo el año, con unas pocas variaciones de temperatura entre diciembre y enero y entre junio y julio, las temporadas más secas y cálidas del año. Sin embargo hay muchas diferencias en cuanto al clima de los diferentes barrios de la ciudad. 

En los días soleados a mediodía las temperaturas pueden llegar hasta los 30 °C. Sin embargo, en Medellín los días completamente despejados son poco comunes, un día normal en Medellín es parcialmente nublado con intervalos de sol y de sombra, lo que genera que la tasa de insolación en Medellín sea relativamente baja (unas 5 o 6 horas de sol al día en promedio) frente a la de ciudades como Barranquilla (que tiene entre 7 y 8 horas de sol al día en promedio). En un día parcialmente nublado las temperaturas suben a los 27 °C al mediodía y en los lluviosos alcanza apenas los 24 °C.

La temperatura de Medellín está determinada por los pisos térmicos que van del páramo (que equivale a 3 km² del territorio), pasando por el frío (192 km²) hasta llegar al medio (185 km²), en donde está la zona urbana, la cual tiene una temperatura que oscila entre 12 °C y 30 °C. Las temperaturas más altas oscilan entre 27 °C y 31 °C, con máxima absoluta de 33,2 °C, la cual fue registrada en 1993 en el barrio San Javier, en el centroccidente de la ciudad. Las más bajas oscilan alrededor de 13 °C y 15 °C, con mínima absoluta de 10 °C. El comienzo y la mitad del año son estaciones secas, de resto el clima es variable, lluvioso en algunas épocas. La precipitación media anual es moderada: 1656 mm, y no es igual en todo el valle: llueve más al sur que al norte.

Las temperaturas son constantes durante el año, en verano las temperaturas pueden subir arriba de los 30 °C, llueve más en otoño, raras veces hay bajas temperaturas en invierno.

Por su ubicación entre montañas, Medellín es una ciudad de vientos suaves y constantes. El régimen de vientos lo determinan los alisios dominantes del nordeste y las masas de aire cálido que suben desde los valles bajos de los ríos Cauca y Magdalena, con predominio de movimiento en la zona norte del valle, lo que origina que el viento sople en dirección norte-sur. Es de advertir que todas estas condiciones varían de acuerdo con los cambios climáticos originados en el océano Pacífico, llamados fenómeno del Niño y de la Niña. Entonces hay más lluvia o más sequía.

En consecuencia al crecimiento urbano y demográfico de la ciudad se ha presentado una notable alteración de la fauna y flora dentro del valle de Aburrá. Con la contaminación de las aguas desaparecieron casi toda la fauna y flora acuática del río que la atraviesa y sus afluentes. Sin embargo, existen reservas naturales notables dentro del área de la ciudad que se complementan a su vez con todo el sistema ecológico del Valle de Aburrá. Medellín se encuentra entre el grupo de las veinte ciudades más contaminadas de América Latina.

En cuanto a minerales, en los corregimientos de San Cristóbal y Altavista, oeste del área urbana, hay más de 30 minas a cielo abierto que extraen materiales de construcción de tipo arcilloso. Adicionalmente, en la zona conocida como Marmato-Titiribí hay potencial de explotación de pórfidos y vetas con metales como cobre, oro y molibdeno. Estas zonas están dispersas en el área de los corregimientos al oeste del casco urbano.

Medellín no escapa a la tendencia colombiana de crecimiento de las áreas urbanas en detrimento de la población rural, este proceso de urbanización acelerado no se debe exclusivamente a la industrialización, ya que existen unas complejas razones políticas y sociales como la pobreza y la violencia: principalmente causadas por el conflicto armado que ha vivido Colombia, las cuales han motivado la migración del campo a la ciudad a lo largo del siglo XX, generando un crecimiento exponencial de la población en las zonas urbanas. Hoy en día el 58 por ciento de la población de Antioquia habita en el área metropolitana. El 67 por ciento de los habitantes de dicha área, corresponden a Medellín, de los cuales el 61,3 por ciento nacieron en la ciudad, el 38,4 por ciento en otro municipio y el 0,3 por ciento son de otro país.
De acuerdo con las cifras del último censo nacional (2005) realizado por el Departamento Administrativo Nacional de Estadística -DANE-, dio como resultado una población de 2 223 078 habitantes para Medellín y 3 312 165 personas para el área metropolitana conformada por otros 9 municipios, con proyecciones al 2014 de 2 541 123 y 3 731 447 respectivamente,siendo esta la segunda aglomeración urbana de Colombia. Además, según el censo, la ciudad cuenta con una densidad poblacional de aprox. 5820 habitantes por kilómetro cuadrado. Solo 130 031 habitantes se ubican en la zona rural de Medellín. El 46,7 por ciento de la población son varones y el 53,3 por ciento mujeres y el promedio de personas por hogar es de 4.

La ciudad cuenta con una tasa de analfabetismo del 6,8 por ciento en la población mayor de 5 años de edad. Los servicios públicos tienen una cobertura del 98,8 por ciento de viviendas con servicio de energía eléctrica, mientras que un 97,3 por ciento tiene servicio de acueducto y un 91,0 por ciento de comunicación telefónica.

Actualmente la ciudad enfrenta una ola de migración de extranjeros derivada de su proyección internacional. Estadounidenses, alemanes, suecos y hasta coreanos han encontrado en Medellín un nuevo hogar. Además, se destaca la migración de venezolanos, debido a la crisis interna que vive el país vecino, la cual se estima a 2017 en 57 932 venezolanos viviendo en la entidad.

En 2015 se registraron 95 335 nacimientos en Medellín (48 858 varones y 56 497 niñas). La longevidad en la ciudad es de 75 años, siendo esta mayor en mujeres que en hombres.

Las defunciones en 2015 fueron 15,430 (8,191 varones y 7,236 mujeres).

En 2015 más de 99 mujeres fueron asesinadas, de las cuales 88 murieron en crímenes relacionados con conflictos entre pandillas. En 2010 la Alcaldía de Medellín registró 182 homicidios de niños y adolescentes (entre 0 y 17 años), mientras que en cada 100 víctimas de muertes violentas, 9 eran niños o adolescentes.

Sin embargo, la Personería de Medellín en su informe sobre derechos humanos en la ciudad concluyó que durante el 2012 se presentó una reducción del 28 por ciento de homicidios, el 8 por ciento de violencia intrafamiliar y el 7 por ciento de violencia sexual.

Para el 2017 se logró una tasa de mortalidad de 19 homicidios por cada 100 000 habitantes.

Según las cifras presentadas por el DANE del censo 2005, la composición etnográfica de la ciudad es:


El 4 por ciento de los hogares medellinenses tiene experiencia migratoria internacional siendo los Estados Unidos el primer país de preferencia (55,5 por ciento), seguido por España (17 por ciento), y otros países (12,1 por ciento). Pero hay preferencias de destino significativas hacia Venezuela (5,5 por ciento), Perú, Panamá, México, Ecuador, Costa Rica, Canadá, Bolivia y Australia. La demanda de fuerza de trabajo poco calificada convierte la búsqueda de oportunidades laborales en uno de los principales motivadores para esta migración, de igual forma, el anhelo de una mejor calidad de vida, la búsqueda de oferta de estudios superiores o la reunificación familiar son también motivos principales.

El 39 por ciento de la población residente en la ciudad nació en otra región del país, siendo en su mayoría desplazados por el conflicto armado interno en Colombia, convirtiendo a Medellín en ciudad de inmigrantes, provenientes principalmente del Chocó e internamente de otras regiones de Antioquia; mientras que el 0,4 por ciento provienen de otra nación.

La crisis política y social en Venezuela provocó la llegada masiva de ciudadanos de ese país que, desde 2017, tienen a la ciudad de Medellín como quinto destino preferido para asentarse. Según cifras de Migración Colombia, en agosto de 2018, 41 128 venezolanos residían en la capital de Antioquia, de los cuales 23 mil lo hicieron de forma legal. Muchos han encontrado oportunidades con empleos informales.

Medellín está regido por un sistema democrático basado en los procesos de descentralización administrativa generados a partir de la proclamación de la Constitución Política de Colombia de 1991. A la ciudad la gobierna un alcalde (poder ejecutivo) y un Concejo Municipal (poder legislativo).

El alcalde de Medellín es el jefe de gobierno y de la administración municipal, representando legal, judicial y extrajudicialmente al municipio. Es un cargo elegido por voto popular para un periodo de cuatro años. Entre sus funciones principales está la administración de los recursos propios de la municipalidad, velar por el bienestar y los intereses de sus conciudadanos y representarlos ante el Gobierno Nacional, además de impulsar políticas locales para mejorar su calidad de vida, tales como programas de salud, vivienda, educación e infraestructura vial y mantener el orden público.

El Concejo de Medellín es una Corporación Administrativa de elección popular, compuesta por 21 ediles de diferentes tendencias políticas, elegidos democráticamente para un período de cuatro años, y cuyo funcionamiento tiene como eje rector la participación democrática de la comunidad. El concejo es la entidad legislativa de la ciudad, emitiendo acuerdos de obligatorio cumplimiento en su jurisdicción territorial. Entre sus funciones está aprobar los proyectos de los alcaldes, elegir personero y contralor municipal y posesionarlos, dictar las normas orgánicas del presupuesto y expedir anualmente el presupuesto de rentas y gastos.

Administrativamente la Alcaldía de Medellín se divide en dos grandes grupos: La administración central y las entidades descentralizadas. Se entiende por Administración Central, el conjunto de entidades que dependen directamente del Alcalde. Estas entidades son denominadas Secretarías o Departamentos Administrativos. Las secretarías son unidades administrativas cuyo principal objetivo es la prestación de servicios a la Comunidad o a la Administración Central. Los Departamentos Administrativos son unidades de carácter técnico. Para lo cual, la Alcaldía cuenta con 21 Secretarías, 2 Departamentos Administrativos y 23 entidades descentralizadas.

Los sectores urbanos de la ciudad se dividen en 6 "zonas", y estas a su vez se dividen en "comunas", sumando un total de 16. Las zonas en realidad carecen de valor territorial, y solo son utilizadas para agrupar a las comunas según su ubicación dentro de la ciudad. Las comunas se dividen en "barrios" y en áreas institucionales. La ciudad tiene 249 barrios oficiales y 20 áreas institucionales. Las áreas institucionales son grandes sectores con algunas características de barrio, pero su población no es permanente y carece de viviendas, ejemplo los campus universitarios. La zona rural se divide en 5 corregimientos, estos a su vez se dividen en veredas. Los corregimientos "San Antonio de Prado" y "San Cristóbal", son los corregimientos más poblados de Colombia, con más de treinta mil habitantes cada uno. Como se ve en el diagrama, Medellín está estructurada siguiendo el caudal del río que lo cruza, el río Medellín, el cual la recorre de sur a norte.

Cada comuna y corregimiento cuenta con una Junta Administradora Local —JAL—, integrada por no menos de cinco ni más de nueve miembros, elegidos por votación popular para un período de cuatro años que deberán coincidir con el período del Concejo Municipal. Una JAL cumple funciones concernientes con los planes y programas municipales de desarrollo económico y social de obras públicas, vigilancia y control a la prestación de los servicios municipales en su comuna o corregimiento y las inversiones que se realicen con los recursos públicos, además de lo concerniente a la distribución de las partidas globales que les asigne el presupuesto municipal y, en general, velar por el cumplimiento de sus decisiones, recomendar la adopción de determinadas medidas por las autoridades municipales, y promover la participación ciudadana. En Medellín existe una zonificación por estratos en toda la ciudad. Las 16 comunas de Medellín, en su respectivo orden, son:



Los 5 corregimientos de Medellín, sin orden establecido, son:


El área metropolitana del Valle de Aburrá es una entidad político-administrativa que se asienta a todo lo largo del Valle de Aburrá a una altitud promedio de 1538 msnm. El Área está compuesta por los 10 municipios que se asientan en el valle. Envigado ingresó al área metropolitana luego de haberse realizado una consulta popular el día 10 de julio del año 2016.

Fue la primera área metropolitana creada en Colombia en 1980, y es la segunda área en población en el país después del Distrito Capital de Bogotá. La población total, que suma la población urbana y rural de los diez municipios es de
3 821 797 habitantes. La principal zona urbana del área metropolitana se encuentra en el centro del valle y está conformada por las cuatro ciudades más grandes por número de habitantes, Medellín, Bello, Itagüí y Envigado.

El Hospital Universitario San Vicente de Paúl, el Hospital Pablo Tobón Uribe y la Clínica Cardiovascular Santa María son pioneras en trasplantes de órganos, méritos que han tenido reconocimiento nacional e internacional. En Medellín se han marcado hitos en la historia de la medicina en Colombia como la creación del primer laboratorio de válvulas y banco de tejidos, los primeros trasplantes de corazón, pulmón, médula ósea, riñón, células madres e intestino. Se realizó el primer trasplante de hígado de Latinoamérica y a nivel mundial, el primero de tráquea y de esófago.
Así mismo, la ciudad ha ganado reconocimiento como destino en el turismo médico, por lo cual ha hecho que la salud se comporte como un sector industrial, buscando oportunidades de crecimiento en utilidades; lo que implica tener en cuenta las exportaciones de servicios médicos como estrategia para aumentar su número de clientes y para obtener mayores márgenes operativos. El distrito ofrece a los pacientes ventajas frente a otros países con desarrollos similares: en cuanto costo-utilidad del tratamiento, tiempos de espera del mismo y hoteles de primera categoría. De esta forma se consolida cada vez más el turismo médico; en los últimos cinco años, más de 4000 extranjeros han visitado a Medellín en busca de alivio.
En cuanto a infraestructura, la ciudad cuenta con 12 hospitales, 43 clínicas, 39 centros de salud y 5 puestos de salud. Además del servicio privado de salud, el servicio público de salud está a cargo de dos instituciones locales, la "Secretaría de Salud" y "Metrosalud". En cada zona y comuna de la ciudad existe un centro médico oficial. No obstante, la demanda de servicios de urgencias en los hospitales públicos casi copa la oferta, por lo cual, si se presentase alguna calamidad masiva, habría que acudir a los servicios privados, situación que está por debajo de los estándares internacionales, que recomiendan mantener un 20 por ciento de extra-oferta de camas de urgencias sobre el funcionamiento normal del sistema hospitalario público para atender posibles casos de calamidades masivas. Algunos de los principales centros hospitalarios de la ciudad son: Hospital Universitario San Vicente de Paúl, Hospital Pablo Tobón Uribe, Hospital General de Medellín, Clínica Cardiovascular Santa María, Clínica Las Américas, Clínica El Rosario, Clínica Universitaria Bolivariana, Clínica Medellín, Clínica León XIII, Clínica Las Vegas, Clínica Soma, Fundación Instituto Neurológico de Colombia entre varios más.

La red de escuelas y colegios públicos de educación básica y bachillerato depende de la Secretaría de Educación. El 78 por ciento de los alumnos estudian en escuelas y colegios públicos, mientras el 22 por ciento lo realizan en el sector privado.
Entre las instituciones de educación pública más destacadas en los exámenes de Estado (ICFES) se encuentran el Instituto Técnico Industrial Pascual Bravo, Institución Educativa INEM José Félix De Restrepo, Institución Educativa Santo Ángel, Institución Educativa San Juan Bosco, Institución Educativa Centro Formativo de Antioquia "CEFA", Colegio la Salle de Campoamor, Liceo Municipal Concejo de Medellín, Institución Educativa Cristo Rey y la Institución Educativa Ana de Castrillón, entre otros. 

Hay numerosos centros educativos privados con nivel certificado como el Colegio San José de La Salle, la Comunidad Colegio Jesús María, el Colegio de la Compañía de María La Enseñanza, el Colegio Salesiano el Sufragio, el Colegio Parroquial Emaús, Colegio San Ignacio de Loyola, Colegio Gimnasio los Pinares, Instituto Musical Diego Echavarría, Colegio Fontán, Colegio Militar José María Córdoba, Colegio Calasanz, Colegio de la Presentación, Instituto San Carlos de Lasalle, Colegio San José de las Vegas, Colegio Padre Manyanet, Colegio Corazonista, Colegio Sagrada Familia Aldea Pablo VI, Colegio de la UPB, el Colegio Liceo Salazar y Herrera y el Instituto Educativo Salesiano Pedro Justo Berrío. Muchos de estos centros educativos cuentan con la titulación de Bachiller-Técnico.
Medellín tiene 130 000 estudiantes en alrededor de 35 instituciones de educación superior, entre públicas y privadas. Algunas de las universidades públicas más destacadas son las de Antioquia y la Nacional. Entre las privadas están la EAFIT, la EIA, el CES, la Bolivariana.
"Véase también: "

En Antioquia existen 511 grupos de investigación registrados, 95 por ciento de los cuales se encuentran en Medellín, que se ubica como la segunda ciudad de Colombia más representativa en materia de investigación y desarrollo en cuanto se refiere a la cantidad de trabajos producidos.

Durante las décadas de 1980 y 1990, Medellín fue notoria debido a las altas tasas de violencia, entre ellas el alto índice de homicidios. En 2002, la tasa de muertes violentas por cada 100 000 habitantes fue de 229; pero, gracias a los programas sociales y culturales en contra de la violencia, en 2005 esta cifra fue de 66,1 por cada 100 000 habitantes, una de las cifras más bajas de los últimos años. También para 2002, la tasa de homicidios era muy alta: 183,3 por cada 100 000 habitantes; este dato también se redujo notoriamente, pues en 2005 pasó de aquel 183,3 a sólo 33,2 por cada 100 000 habitantes. 

En 2010, la guerra entre pandillas aumentó de nuevo la tasa de homicidios, llevándola hasta 87,2 por cada 100 000 habitantes; en el transcurso de ese año se presentaron serios problemas de orden público que motivaron al gobierno nacional a intervenir en varias ocasiones por medio de consejos de seguridad y aumento en la fuerza pública; esta situación fue originada por bandas delincuenciales que se disputan el control de los centros de expendio de drogas.

Los datos presentados por el gobierno Municipal contrastan con los estudios internacionales. Según el ranking de Seguridad, Justicia y Paz presentado por el Consejo Ciudadano para la Seguridad Pública y Justicia Penal A.C, la ciudad de Medellín presentó una tasa de 38,06 homicidios por cada cien mil habitantes en 2013, colocándose entre las 50 ciudades más violentas del mundo, según el mismo estudio.

Para 2014, se hizo entrega de 150 motos, 160 patrullas y 10 CAI móviles a la policía de la ciudad por un valor de 16 490 millones; políticas que incidieron para que la ciudad presentara la tasa de homicidios más baja en 30 años, con 26,7 homicidios por cada 100 000 habitantes (inferior a la media de Colombia). En 2015, según el escalafón anual del Consejo Ciudadano para la Seguridad Pública y la Justicia Penal de México, el número se redujo a 19 homicidios por cada 100 000 habitantes.

Están a cargo de Empresas Públicas de Medellín (EPM), la cual fue creada el 6 de agosto de 1955. El consejo administrativo de Medellín, mediante el acuerdo No. 58, fusionó las cuatro entidades independientes que hasta ese momento prestaban los servicios públicos en la ciudad (energía, acueducto, alcantarillado y teléfonos), en un solo establecimiento autónomo. El 18 de noviembre de 1955 la alcaldía reglamentó la existencia de EPM; una semana después, el 25 de noviembre, el gobernador sancionó el decreto en que se expedían los estatutos, y a partir de enero de 1956 se inició su vida administrativa.

En 1989 se incluyó el manejo y mejoramiento del medio ambiente como parte de sus estatutos y se cambió el nombre de servicio telefónico por el de telecomunicaciones. Este servicio fue escindido en 2007 para crear la filial UNE. En 1998 EPM fue transformada en Empresa Industrial y Comercial del Estado y por eso hoy se encuentra sometida a las disposiciones de la ley comercial para el ejercicio de sus actividades. Fue elegida como la mejor empresa del siglo XX en Colombia tanto por sus ejecutorias en el campo de los servicios públicos, así como por su sólida proyección nacional e internacional.

La principal puerta de acceso a Medellín para viajeros internacionales y nacionales es el Aeropuerto Internacional José María Córdova, ubicado en jurisdicción del municipio de Rionegro, a 35 kilómetros de la ciudad en dirección oriente. Fue inaugurado en 1985 y posteriormente amplió su muelle nacional con la construcción de 4200 metros cuadrados nuevos de área. Dentro del perímetro urbano del municipio de Medellín, al suroccidente, está ubicado el Aeropuerto Olaya Herrera, que presta servicios de vuelos nacionales y regionales (departamentales).

Es el primer sistema de transporte masivo que se construyó en Colombia. Inició operaciones el 30 de noviembre de 1995 y desde entonces ha movilizado a más de mil millones de pasajeros. El metro atraviesa el área metropolitana de sur a norte, entre los municipios de Bello y La Estrella; también se extiende desde el centro de la ciudad hacia el oeste. El Metro combina un sistema férreo con un sistema de cable aéreo denominado metrocable (no confundir con el sistema teleférico, aunque son similares), el cual ha sido usado por primera vez en el mundo en Medellín como transporte masivo permanente. El Metro cuenta con varios tipos de niveles (nivel de tierra, viaductos elevados y cables aéreos), y no tiene tramos subterráneos. La Red del Metro posee una longitud de 33 km y comprende 5 líneas: Línea A (férrea) con 19 estaciones, Línea B (férrea) con 7 estaciones, la línea C (férrea) con 11 estaciones, la línea L (cable) que comunica el área metropolitana con el parque natural de Arví desde la estación Santo Domingo Sabio hasta el corregimiento de Santa Elena, la Línea K y la Línea J cuenta cada una con 3 estaciones (estas tres últimas son del sistema cable aéreo). 

Es una línea de tren ligero o tranvía, compuesta por seis paradas y tres estaciones: San Antonio, Miraflores y Oriente. Junto a dos nuevas líneas del Metrocable, H y M, conectan los barrios centro-orientales con el centro de la ciudad. El tranvía va por la calle 49 (Ayacucho), tiene 4,3 kilómetros de largo y su entrada en operación total ocurrió en noviembre de 2015.Entretanto, también se tiene proyectada la entrada de un "Monorriel", o 'Metro pequeño' que atraviese las comunas 1, 3, 8, 9 y 14 en la zona nororiental de la ciudad.

Es un sistema de buses articulados para transporte masivo. Está integrado físicamente con el Metro de Medellín en las estaciones Hospital, Industriales y Cisneros; además cuenta con una segunda línea pretroncal, Aranjuez-Universidad de Medellín, que atraviesa el centro de la ciudad por la avenida Oriental. Tiene estaciones cada 500 metros y los vehículos están unidos por una articulación que les confiere movilidad, con una capacidad de 160 personas cada uno; vienen equipados con tres puertas de acceso, caja automática y suspensión neumática. Actualmente está en construcción la pretroncal Envigado a Itagüí. Aunque Metroplús está integrado física y tarifariamente con el Metro de Medellín, es en realidad una empresa aparte, que cuenta entre sus accionistas con el mismo Metro de Medellín (25,64 por ciento de partición accionaria).

Medellín cuenta con un sistema de transporte público mediante cables aéreos denominado Metrocable, el primero de su tipo en el mundo. El sistema, ideado completamente en esta ciudad, consta actualmente de varias líneas, tales como la línea J y la línea K, que se complementan y se enlazan con las líneas férreas A y B. Es así que los Metrocables sirven también como fuente alimentadora del Metro. Actualmente varias ciudades de Colombia quieren implementarlo, como Ibagué, Bucaramanga y Pereira.

Los actuales y futuros proyectos y sus inversiones tienen y tendrán un importante carácter social y de beneficio común, ya que están dirigidos al mejoramiento de las condiciones de vida de las poblaciones de menores ingresos, usuarias de los sistemas de transporte público.

La Línea K se ejecutó con recursos propios de la Alcaldía de Medellín (55 por ciento) y de la empresa Metro de Medellín Ltda. (45 por ciento), bajo la premisa de aportar al desarrollo social de los habitantes de una de las zonas más deprimidas de la ciudad. La Línea J se construyó con aportes de la Alcaldía de Medellín (73 por ciento) y de la empresa Metro de Medellín Ltda. (27 por ciento).

Existe en la ciudad un sistema privado de buses urbanos que atiende todos los distritos o zonas de la urbe, el cual se está estructurando en 2007 en el llamado SIT, Sistema Integrado de Transporte, un proyecto ya en marcha que integrará el servicio de buses urbanos con el Metro y el nuevo sistema Metroplús.

De igual manera, hay numerosas empresas de taxis que cubren toda el área metropolitana, y entre ellas hay algunas con servicios bilingües en inglés-español. El servicio de pedido de taxi por teléfono es el más usual y seguro. Algunas empresas prestan servicios intermunicipales. Es usual además el servicio de taxi colectivo; algunos de estos colectivos pueden ser cómodos y rápidos, aunque suelen estar supeditados al cupo completo. El uso del GPS se implementó en todos los taxis de la ciudad el 31 de marzo de 2012.

En Medellín hay dos terminales de transporte intermunicipal: Terminal de Transporte Intermunicipal del Norte y Terminal de Transporte Intermunicipal del Sur. Las terminales de transporte son además centros comerciales con servicios bancarios, de comercio y de telecomunicaciones. Debido a sus proyectos en transporte sostenible, la ciudad obtuvo, junto con San Francisco (California), el premio Transporte Sostenible 2012, otorgado por el Instituto de Políticas de Transporte y Deasarrollo.

Medellín es el segundo centro económico más importante de Colombia, después de Bogotá. La ciudad representa más del 8 por ciento del PIB Nacional y en conjunto con el Valle de Aburrá aporta cerca del 11 por ciento del mismo, siendo una de las regiones más productivas del país. 

Tiene un PIB per cápita para el 2014 (con PPA) de US$ 11 466 y una densidad empresarial de 25 empresas por cada 1000 habitantes, lo que la posiciona como la segunda más alta de Colombia tras la misma. La industria representa el 43,6 por ciento del producto interno bruto del Valle de Aburrá, los servicios el 39,7 por ciento y el comercio el 7 por ciento. Los sectores industriales con mayor participación en el valor agregado generado en el Área Metropolitana son las empresas textiles, con 20 por ciento; sustancias y productos químicos, con el 14,5 por ciento; alimentos, con el 10 por ciento, y bebidas con el 11 por ciento. 

El 10 por ciento restante comprende sectores como el metalmecánico, eléctrico y electrónico, entre otros. La Industria textil y de confecciones es hoy una de las grandes exportadoras de productos hacia los mercados internacionales; el desarrollo en estos sectores ha convertido a la ciudad en un centro de la moda latinoamericana. En las últimas tres décadas se ha venido registrando una diversificación de la estructura económica de la ciudad, con el desarrollo de otros subsectores, como el de bienes intermedios y bienes de capital.

En el sector del turismo, Medellín ha avanzado hasta convertirse en el tercer destino turístico para los visitantes extranjeros que visitan Colombia. Entre 2005 y 2006, el número de extranjeros que tuvo como destino final Medellín creció un 33,4 por ciento, al pasar de 71 213 a 95 026 visitantes. A julio de 2007, ese número fue de 62 003, lo que representa un incremento de 20,7 por ciento en relación con lo registrado en igual periodo de 2006. Estos avances son principalmente generados por el turismo de negocios, ferias y convenciones, y por el turismo médico, gracias al excelente nivel de la medicina con que cuenta la ciudad, en particular en el ámbito de los trasplantes de órganos. La ciudad hace parte del sistema integral económico del departamento de Antioquia, el cual aporta el 15 por ciento del PIB nacional.

En la actualidad, Medellín es la principal ciudad exportadora de Colombia en tejido plano y punto, con un 53 por ciento del total de las exportaciones en prendas terminadas a países como Estados Unidos, Venezuela, Ecuador, México, Costa Rica y la Comunidad Europea. La industria textil genera para la ciudad un 30 por ciento del total del empleo, lo que equivale a 45 000 empleos directos y 135 000 indirectos.

En Medellín están ubicadas las sedes de las compañías discográficas Discos Fuentes y Codiscos, cada una con estudios de grabación.

Con el crecimiento de la economía y de las exportaciones, varios retos surgieron para la industria de Antioquia y Medellín: diversificar la base exportadora, desarrollar un recurso humano avanzado, mejorar las condiciones internas para inversión extranjera . Antioquia fue el departamento más exportador de Colombia en 2007, por lo cual se incluyeron cerca de 500 nuevas posiciones arancelarias en el portafolio exportador y se pasó de 990 a 1750 empresas exportadoras en el último quinquenio . Una buena proporción de estas empresas pertenece a la primera Comunidad Cluster de Colombia, creada con el apoyo de la Cámara de Comercio de Medellín para Antioquia y la Alcaldía de Medellín, y a la que pertenecen cerca de 21 000 empresas con una participación del 40 por ciento de las exportaciones totales, el 25 por ciento del PIB regional y el 40 por ciento del empleo del Área Metropolitana.

Los clusters son entendidos como una concentración geográfica de empresas e instituciones que interactúan entre sí y que al hacerlo crean un clima de negocios para mejorar su desempeño, competitividad y rentabilidad. Los clusters que ya están constituidos son Energía eléctrica, Textil/Confección, Diseño y Moda, Construcción, Turismo de Negocios, Ferias y Convenciones.

Según los datos publicados por la Misión para el Empalme de las Series de Empleo, Pobreza y Desigualdad -MESEP- de noviembre de 2009, en Medellín y su área metropolitana el índice de pobreza en el periodo 2002-2008 se redujo en un 22,5 por ciento, pasando de 49,7 por ciento al 38,5 por ciento. Igualmente, el índice de indigencia disminuyó en un 25,2 por ciento pasando del 12,3 por ciento al 9,2 por ciento. Estos resultados están en sintonía con la mayor cobertura de servicios básicos como la salud, la educación y los servicios públicos en la ciudad. Sin embargo, la pobreza y la indigencia en Medellín y su área metropolitana continúa estando por encima del promedio de las 13 principales áreas metropolitanas de Colombia. En 2008 dicho promedio fue del 30,7 por ciento para la pobreza y del 5,5 por ciento para la indigencia.

Por otro lado, la tasa de desempleo en Medellín ha presentado una tendencia decreciente. En 2000 el desempleo en la ciudad estaba situado en el 17,7 por ciento, y según datos del DANE, en Medellín y su área metropolitana el desempleo en el trimestre junio-agosto de 2010 fue del 14,3 por ciento, aunque todavía ubicándose por encima de la media nacional, que para agosto de 2010 era del 11,2 por ciento. El DANE situó en sus resultados de 2012 a Medellín como la ciudad más desigual de Colombia, al revelar que su coeficiente de Gini es de 0.54 por factores como el índice de pobreza, en relación con el total de la población, situado en el 22 por ciento.

Entre los principales destinos se destacan el Museo de Antioquia, la Plaza de Botero, el Pueblito Paisa, el Centro Internacional de Convenciones y Exposiciones Plaza Mayor, el Pasaje Peatonal Carabobo, el Parque de los Pies Descalzos, la Catedral Metropolitana, la Basílica Nuestra Señora de la Candelaria, el Teatro Pablo Tobón Uribe, el Teatro Metropolitano, el Centro Comercial Oviedo, el Parque Explora, el Jardín Botánico, el Parque Lleras y más recientemente, la Comuna 13. 

Con respecto a sitios naturales, los más concurridos son el Cerro El Volador y el Cerro Nutibara. Un nuevo espacio natural inaugurado hace pocos años es el Parque Regional Arví, el cual cuenta con un área cercana a las 20 000 hectáreas, comprende prácticamente todo el territorio del corregimiento de Santa Elena y se extiende entre los municipios de Bello, Copacabana, Guarne y Envigado. Por su parte, en diciembre, la ciudad se cubre de miles de bombillas de colores, creando el famoso alumbrado navideño, considerado por la National Geographic como uno de los diez más bellos del mundo, y que puede apreciarse principalmente en el Parque Norte, cerca del Jardín Botánico, en la avenida la Playa y en el Parque del río Medellín, entre la Calle Avenida 33 y la Avenida San Juan.
En la ciudad existen diferentes sectores donde se concentra la oferta hotelera. Los hoteles de El Poblado están en capacidad de alojar 8200 personas. En la zona de Laureles–Estadio existen 73 hoteles con capacidad de alojamiento para 2100 personas y en el centro de la ciudad, 34 hoteles que tienen a su disposición 1400 camas.

Medellín ofrece a todos sus visitantes diferentes opciones de alojamiento en fincas tradicionales, casas campestres y casas urbanas tipo familiar, Hoteles Boutique y Hoteles de ciudad.

En los inicios del Siglo XXI la ciudad ha vivido un proceso de intensa transformación urbanística que le ha conferido importantes reconocimientos nacionales e internacionales. Dicha transformación se basa en el "urbanismo social", una política pública consistente en otorgarle prioridad a los pobladores y territorios más pobres, así como a las víctimas de la violencia, mediante obras y programas que buscan reparar el herido tejido físico y social. Un ejemplo de estas políticas son unas novedosas escaleras eléctricas instaladas en un barrio de la comuna 13, zona que se caracteriza no solo por los problemas mencionados anteriormente, sino también por su ubicación en las laderas de montaña, lo que hace un tanto difícil la comunicación y la calidad de vida de esa parte de la población.

El Concejo municipal de la ciudad expidió en 1890 un acuerdo mediante el cual se ordenaba trazar el plano para el ensanchamiento futuro de la ciudad, en el cual se reglamentaban además aspectos como la construcción de edificios, la apertura y pavimentación de vías, el acueducto, el alcantarillado, y hasta la forma de las ventanas para que estas no obstruyeran el paso de los transeúntes. En una reforma posterior se contempló la rectificación y canalización del río Medellín, que recorría en forma sinuosa todo el Valle de Aburrá, con el fin de ganar terreno para la construcción y el crecimiento de la ciudad. Dicho plano, llamado Medellín Futuro, solo pudo cumplirse parcialmente, pero sirvió para guiar el avance de la ciudad en la primera mitad del siglo. 

Algunas fechas cronológicas importantes en esta etapa fueron las siguientes: en 1900 la quebrada Santa Elena se consolida como el centro de la ciudad y se comienza a trazar como Paseo Urbano. En 1905 se inaugura un tranvía tirado por mulas; en 1914 llega el Ferrocarril de Antioquia a la ciudad; en 1920 se inician los trazados de las vías. En 1925 comienzan a funcionar los tranvías eléctricos; en 1928 tiene lugar la cobertura de la quebrada Santa Elena; en 1931 se construye el aeropuerto Olaya Herrera; en 1940 comienzan las obras de canalización y rectificación del río Medellín; en 1941 el arquitecto Pedro Nel Gómez es el encargado de diseñar urbanísticamente el sector de Laureles. Y en 1945 se construye el hotel Nutibara.<ref name="Medellín futuro/Plan piloto"></ref>

Una vez se llevaron a cabo las obras en el río, y debido a la expansión urbana hacia el occidente (la Otrabanda), a finales de la década del cuarenta se vio la necesidad de trazar un nuevo plano para organizar la ciudad. Fue así como los urbanistas Paul L. Wiener y José L. Sert se encargaron de proyectar entre 1948 y 1950 el Plan Piloto, que sugería, entre otras cosas, la construcción de diversas avenidas y el diseño de un nuevo centro de gobierno. Debido a esto, obras tan representativas en Medellín como la avenida Oriental, construida en los años setenta, y el Centro Administrativo La Alpujarra, en los ochenta, si bien no estuvieron directamente contempladas en el Plan de Wiener y Sert, sí se puede considerar que estuvieron basadas en este.

Entre los años 1950 y 1980 se agudiza el fenómeno de invasión territorial dificultando el cumplimiento de los planes que trataban de ordenar el crecimiento de la ciudad. El Plan Piloto se vio desbordado por la realidad de una población que se triplicó en 20 años, pasando de 358 189 habitantes, en 1951, a 1 071 252, en 1973. La construcción tuvo gran dinamismo en ese periodo y buena parte de las laderas de la ciudad empezaron a ser ocupadas por los habitantes que, llegados del campo, no tenían la posibilidad de acceder a créditos para vivienda. Muchas de las edificaciones antiguas del centro, y aún las de principios del siglo XX, fueron demolidas para dar paso a edificios altos que fueron destinados a oficinas y vivienda. 

El sector textil se modernizó en este periodo y se consolidó de forma definitiva la vocación industrial de la ciudad. Algunas fechas importantes durante este periodo fueron las siguientes: en 1962 se empieza a construir la Unidad Deportiva Atanasio Girardot en los alrededores del estadio; desde finales de los años 1960 hasta principios de los setenta se construye el edificio Coltejer, un complejo de edificaciones que aún hoy es el símbolo urbano más representativo de Medellín. En 1980 se construye el plan vial del río; en 1987 se inaugura La Alpujarra. A partir de 1995 comienza a funcionar el Metro, una obra que desde el punto de vista urbanístico ha tenido detractores debido a su paso elevado por el centro de la ciudad.

En 2013, Medellín ganó el Premio Verde Verónica Rudge en diseño urbano, otorgado por la Universidad de Harvard, debido al proyecto Urbano Integral PUI de la zona Nororiental, diseñada y ejecutada por la Empresa de Desarrollo Urbano.Medellín conserva muy poca memoria urbanística colonial y del siglo XIX. Aunque el Valle de Aburrá fue una zona activa en agricultura y ganadería a lo largo del periodo colonial, su relativa riqueza no se expresó en una arquitectura civil y religiosa sobresaliente como en Cartagena, Tunja, Popayán o Bogotá. Esto puede explicarse por el hecho de que la Villa de Medellín no fue un centro político-administrativo y sí un lugar aislado geográficamente cuya élite invirtió poco en el desarrollo de una arquitectura monumental. De los finales de la colonia quedan, pero con muchas transformaciones, la Iglesia de la Candelaria y la Iglesia de la Veracruz.

Se denomina "republicana" a la arquitectura producida en Colombia entre 1850 y 1930. El uso del ladrillo y la aplicación de estilos históricos europeos fueron la principal novedad. El alemán Enrique Haeusler fue el autor del puente de Guayaquil (1879). Pero fue el arquitecto francés Carlos Carré la principal figura de la arquitectura republicana del siglo XIX en Medellín; Carré llegó a la ciudad en 1889, habiendo sido contratado para diseñar y edificar la nueva catedral episcopal y varios edificios comerciales y residenciales que se tenían proyectados para diferentes lugares de la ciudad, sobre todo en el nuevo barrio de Guayaquil. 

La Catedral Metropolitana fue terminada en 1931; igualmente son de su autoría los edificios Vásquez y Carré, que se encuentran ubicados junto a la Plaza de Cisneros. La Estación Medellín del Ferrocarril de Antioquia fue obra de Enrique Olarte, una obra que permitió la consolidación urbana definitiva del sector de Guayaquil.

En los años 1920 la arquitectura republicana llegó a su fase culminante. De este periodo sobresalen el antiguo Palacio Municipal (hoy Museo de Antioquia) en 1928, y los edificios del Palacio Nacional y el Palacio de Gobierno Departamental (hoy Palacio de la Cultura) entre 1925 y 1928. Estas dos últimas obras fueron diseñadas por el belga Agustín Goovaerts, ambas inspiradas en la corriente modernista belga, en las cuales aplicó los estilos románico y neogótico respectivamente. Otras obras de Goovaerts fueron la Iglesia del Sagrado Corazón (sector de Guayaquil), y la Iglesia de San Ignacio, entre otras. De los años 1930 se destacan algunas construcciones del barrio Prado como la casa egipcia y el actual Teatro Prado.

La expansión económica del Estado, la industria, la banca y la población enmarcaron la aparición de los rascacielos. Al ubicarse en el centro histórico-cívico de la ciudad, la construcción de rascacielos para oficinas, comercio y vivienda trajo consigo la destrucción de una buena parte del ya escaso patrimonio urbanístico antiguo de Medellín. Vivir en grandes edificios en el centro fue por entonces un signo análogo de prestigio y estatus social. Los edificios Furatena (1966) con sus treinta pisos y Coltabaco (1967), este último ubicado en el Parque de Berrío, inauguraron esta tendencia. 

En la década siguiente vendría la ya mencionada torre Coltejer (1968-1972), la cual sigue siendo el edificio más alto de la ciudad; fue diseñada por los mismos proyectistas del edificio Avianca en Bogotá. Para su construcción se demolió el Teatro Junín, uno de los hitos de la ingeniería y de la arquitectura colombianas y el único exponente del art nouveau en la ciudad. De 1974 a 1978 tuvo lugar la construcción de la Torre del Café, la segunda más alta. De esta época se destacan edificios de mediana altura como el de Camacol (1972-1974) cerca del puente de Colombia, y el del Banco de la República (1969-1974) en el Parque de Berrío.
El primer edificio inteligente del país se inauguró en 1997. Se trata del edificio de EPM, cuyo diseño se constituyó en una innovación arquitectónica al proyectar luces de 36 metros de altura y disponer de mayor amplitud en sus áreas de oficinas. Una de sus ventajas es la flexibilidad de su interior ya que permite ajustarse cuando se requiera, sin necesidad de romper paredes o destrozar los pisos.

El cambio de siglo trajo consigo una nueva arquitectura expresada en obras de gran impacto urbanístico entre las que se destacan la plaza de Cisneros (2002-2006), el Parque de Los Deseos (2003), el Centro Internacional de Convenciones y Exposiciones Plaza Mayor (2003-2005), la Biblioteca de las Empresas Públicas de Medellín (2004), el Orquideorama del Jardín Botánico (2005-2006), los Parques Biblioteca (2005-2012), el Parque Explora (2005-2008) y la Plaza de La Libertad (2009-2011).

Uno de los parques más importantes con que cuenta la ciudad es el "Central Park", el cual, si bien está ubicado en territorio municipal de la ciudad de Bello, es propiedad del Municipio de Medellín.

Con una extensión de un millón de metros cuadrados, el “Central Park”, en el costado norte de Medellín, es un amplio terreno destinado a actividades recreativas, lúdicas y deportivas, además de conciertos, con una capacidad para 100.000 asistentes, y que se puede utilizar también para concentraciones de otra índole, por ejemplo políticas o religiosas. En su interior se localiza el Autódromo de Medellín, una pista para competencias de motor de 300.000 metros cuadrados de extensión. El autódromo está diseñado para practicar carreras de carros, motos y karts, pero además ciclismo, patinaje y atletismo, entre otros deportes.

Otros parques tradicionales principales de la urbe son: el Parque de Berrío, ubicado en el corazón de la ciudad; el Parque de Bolívar, ubicado un poco más al norte del anterior y enmarcado por la imponente Catedral Metropolitana, el edificio más grande del mundo construido en ladrillo cocido; otros parques están situados en zonas más residenciales como el Parque de Belén, el Parque de El Poblado o los Parques de Laureles. Los parques construídos más recientemente son más interactivos, y han tenido gran acogida por parte de los habitantes ya que algunos no solo son lugares de esparcimiento sino que también permiten el aprendizaje por medio de experiencias directas y personales de los visitantes; entre ellos se destacan el Parque de los Pies Descalzos, el Parque de Los Deseos, el Parque Explora y el Parque Bicentenario, este último inaugurado con motivo del Bicentenario de Colombia.

De igual manera se pueden encontrar parques recreativos que llevan tiempo abiertos al público y se han convertido en referentes para la ciudad; entre ellos se destacan: el Parque Norte, el Parque Juan Pablo II, el Jardín Botánico, el Cerro El Volador, el Cerro Nutibara (en cuya cima se encuentra el Pueblito Paisa), y el Parque Arví ofrece una temática singular para caminantes, observadores de aves y excursionistas. Es un parque exótico digno de explorar para propios y visitantes. El Zoológico Santa Fe fue fundado en 1960 y alberga a fecha de 2020 cerca de 1000 animales procedentes de Asia, África y otros lugares de América.




La vía principal cruza el Valle de Aburrá de sur a norte por los costados del Río Medellín. Se conoce como la "Autopista" en el costado occidental del río y la "Avenida Regional" en el costado oriental. Hacia el oriente por el Túnel de Oriente como el más largo de Suramérica (8,2 km) que comunica a la ciudad con el Aeropuerto Internacional José María Córdova y el Túnel de Occidente (5 km) que la comunica con la ciudad colonial de Santa fe de Antioquia. Adicionalmente están la "Autopista Medellín-Villeta-Bogotá" que va directo a la Calle 80 al noroccidente de Bogotá, "Las Palmas" y la antigua "Carretera al Mar".

El parque de Berrío, de gran significado histórico, es el centro fundacional de Medellín, por lo cual es el punto de partida de la trama vial y sitio de referencia de la nomenclatura de la ciudad. En el costado sur-oriental del Parque, se cruza la calle 50 (Colombia) con la carrera 50 (Palacé), nombradas así para honrar la Batalla de Palacé, primera contienda por la Independencia de Colombia.

La numeración de las vías es alfanumérica y está compuesta por un número, opcionalmente de un apéndice alfabético de máximo dos literales, y de los apéndices “Sur” para las calles y “Este” para las carreras. Ejemplos de calles: Calle 43, Calle 44A, Calle 56 FE y Calle 5 Sur. Ejemplo de carreras: Carrera 76, Carrera 70B, Carrera 22AA y Carrera 2 Este.

Las vías de la ciudad de Medellín están divididas en:


Las vías también difieren en nomenclatura de acuerdo a su ubicación, ya que estando en la zona urbana de la ciudad, todas se rigen por la misma denominación vial. Pero en los corregimientos, al tener su cabecera urbana por aparte, su nomenclatura era propia, debido a su lejanía de la zona urbana de Medellín. Con el nuevo Plan de Ordenamiento Territorial (POT), la idea es integrarlos a la nomenclatura vial de Medellín y el Área Metropolitana del Valle de Aburrá (excepto los municipios de Bello e Itagüí).





Con esto, el único corregimiento con nomenclatura propia hasta la fecha es el de San Sebastián de Palmitas.

En las veredas de cada corregimiento no se utiliza nomenclatura vial, debido a la escasez de concentración urbana.

Las telecomunicaciones de la ciudad están representadas desde los teléfonos públicos, pasando por redes de telefonía móvil, redes inalámbricas de banda ancha, centros de navegación o cibercafés, entre otras. La principal empresa en este sector es Tigo, filial de Millicom; también están presentes "Claro" (de América Móvil) y "Movistar" (de Telefónica).

En la ciudad funcionan los seis operadores de telefonía móvil de cobertura nacional, de los cuales tres son operadores móviles con red: Claro, Movistar y Tigo; los otros tres son operadores móviles virtuales: Uff Móvil, UNE y ETB, que usan la red de Tigo. En el caso de UNE, actualmente está desplegando su propia red por el país, y ya cubre las áreas metropolitanas de Medellín y Bogotá con su propia red 4G LTE. También funciona en la ciudad la empresa Avantel, ofreciendo el servicio de trunking, el cual se hace por medio de un dispositivo híbrido entre celular y radio.

En la ciudad se sintonizan varios canales de televisión de señal abierta terrestre, los 4 canales locales (Telemedellín, Canal U Televida y Cosmovisión, uno regional (Teleantioquia), y los cinco canales nacionales: los 3 privados Caracol Televisión, RCN Televisión y Canal 1, y los 2 públicos Canal Institucional y Señal Colombia.

En la ciudad están establecidas en todo el espectro emisoras en AM y FM, tanto de cobertura local como nacional, de las cuales la mayoría son manejadas por Caracol Radio o RCN Radio, aunque hay otras emisoras independientes de gran sintonía, como Todelar y Super. 
En Medellín y Antioquia circulan diarios de cobertura regional como "El Mundo", e igualmente los de tiraje nacional: "El Colombiano (propio de Medellín)," "El Tiempo" y "El Espectador."

Uno de los lugares más apetecidos de Medellín es su Zona Rosa conocida como la "milla de oro", un sector ubicado en El Poblado cuyo punto de referencia es el "Parque Lleras". Este parque es muy concurrido pues alberga, tanto en su perímetro como en sus alrededores, numerosos bares, cafés y restaurantes para todos los públicos. La Zona Rosa abarca también el "Parque del Poblado" y una parte de la célebre calle 10.

El barrio Colombia también cuenta con bares y discotecas muy populares. Igualmente, la avenida Las Palmas se ha consolidado con los años como un sector dedicado a la vida nocturna, en especial los fines de semana. De igual manera en el occidente de la ciudad, en la calle 33, se han asentado numerosos establecimientos. El epicentro de la llamada Zona Fucsia (en contraposición a la Zona Rosa) es el "Parque del Periodista", ubicado en el centro y en donde confluyen numerosas 'tribus urbanas'.

Entre los principales artistas medellinenses figuran Fernando Botero, Rodrigo Arenas Betancur y Débora Arango. En música se destacan Juanes, J Balvin , Maluma, Karol G, Piso 21, Sebastián Yatra y más atrás en el tiempo "Jaime R. Echavarría". También han surgido bandas como lo son: Estados Alterados, Bajo Tierra, Ekhymosis y Kraken.

Los principales centros culturales de la urbe son el "Museo de Antioquia" y la "Plaza Botero". En la ciudad anualmente tiene lugar el Festival Internacional de Poesía, un evento de carácter cultural que se realiza desde 1991; es también destacada la "Orquesta Infantil y Juvenil de Medellín". Además es la ciudad colombiana con mayor cantidad de esculturas en pie, y la gastronomía antioqueña es la más representativa de la región. Adicionalmente, el reggaeton es una tendencia fuerte en la ciudad: existen más de 300 grupos conformados, se hacen más de 200 conciertos por año y hay varias discotecas dedicadas exclusivamente al género.

Medellín y el Área Metropolitana cuentan con una "Red de Bibliotecas", un conjunto de bibliotecas comunicadas entre sí que comparten recursos, esfuerzos, conocimientos y experiencias con el fin de mejorar las condiciones educativas y culturales de las comunidades que atienden. La red está conformada por 36 bibliotecas, de las cuales 24 corresponden a Medellín.







Bibliotecas públicas y universitarias no adscritas a la Red de Bibliotecas




También son de destacar las bibliotecas centrales de las universidades privadas Pontificia Bolivariana y EAFIT, las cuales atesoran material bibliográfico sobre una gran variedad de disciplinas.











Otros museos destacados son: Centro Cultural Banco de la República, Museo Entomológico Francisco Luis Gallego, Casa Museo Santa Fe, Museo de Ciencias Naturales, Museo Etnográfico Miguel Ángel Builes, y Museo de la Madre Laura.

Medellín cuenta con más de 17 salas de artes escénicas, en las que se presentan alrededor de 50 grupos, algunos de amplia trayectoria y reconocimiento local y nacional. Están distribuidos en más de una veintena de escuelas.

Algunas de las principales instalaciones teatrales de la ciudad son:






Ejemplo típico del teatro paisa - en este caso del teatro humorístico -, es el grupo El Águila Descalza.

Otras organizaciones e instalaciones teatrales de la ciudad son: Teatro Porfirio Barba Jacob, Teatro El Triángulo, El Firulete, Asociación Pequeño Teatro de Medellín, Teatro de Muñecas La Fanfarria, Teatro Popular de Medellín,Teatro Matacandelas, La Casa del Teatro, Café Concierto Los Inquietos, Teatro Manicomio de Muñecos, Corporación Cultural Teatro de Seda, Teatro Barra del Silencio, Manicomio de Vargasvil, Sala Beethoven, Instituto de Bellas Artes, Planetario Jesús Emilio Ramírez.

La gastronomía de la ciudad corresponde a la antioqueña. Entre los platos típicos se destacan la bandeja paisa, plato fuerte representativo de la región, y la arepa paisa, la cual se come usualmente con acompañamientos. El desayuno es común acompañarlo con chocolate, calentao (sobras calentadas del día anterior) y parva, la cual es una componente tradicional de la gastronomía antioqueña, conformada por una amplia variedad de piezas de panadería, entre las que se destacan el pandequeso, el bizcochuelo, el pandero, el buñuelo, el pandebono y el pan.

Los silleteros han sido proclamados como Patrimonio Cultural de Colombia.

Durante la colonia los pasos de cordillera eran tales que dificultaban la utilización de animales de carga voluminosos (como bueyes, mulas o caballos) por los tortuosos y estrechos caminos, y entonces así muchas veces se hacía necesario transportar los arrumes (y hasta los hijos) en las espaldas de los arrieros, en aparatajes de madera cargados en la espalda llamados 'silletas', y también motivo por el cual quienes los utilizaban eran llamados 'silleteros'. Gracias a ellos fue posible el intercambio de productos y la movilización de viajeros entre lugares distantes. Su habilidad consistía en soportar grandes pesos a sus espaldas durante largas jornadas. Algunas crónicas de viaje de finales del siglo XIX describen caravanas de silleteros avanzando por los caminos de montaña.

La silleta y el silletero se adaptaron a los tiempos modernos del departamento y del país; de este modo, en muchas viviendas campesinas la silleta persistió como un instrumento útil para transportar personas desvalidas o enfermas, o para movilizar productos, y para el campesino de Santa Elena en especial, fue un recurso del que se sirvió con ingenio para la tarea de comercializar sus productos en Medellín. La ciudad se familiarizó con el silletero vendedor de flores y hortalizas, que recorría las calles céntricas y los barrios como proveedor por encargo de ciertas familias. Fue común verlos en las plazas de mercado más reconocidas, como la de Cisneros o la de Flores, y en los atrios de las iglesias, hasta que se convirtieron en vistosos personajes incorporados al paisaje cotidiano de la ciudad.

En 1957 se organizó un desfile, y desde ese momento, su figura fue creciendo hasta consolidarse hoy por hoy como uno de los símbolos culturales de Medellín.

La ciudad realiza dos ferias de moda a lo largo del año, siendo la más importante "Colombiamoda", que tiene lugar durante tres días en el mes de julio. Es considerada una de las ferias más importantes del país y de América Latina y cuenta con 24 años de trayectoria. La primera feria que se realizó en Medellín se llevó a cabo en 1987 con el apoyo de empresas textiles tradicionales en la ciudad como Coltejer, Fabricato y Tejicóndor. A pesar de la gran acogida obtenida ese año y en el siguiente, la feria solo tuvo dos versiones. "Inexmoda" tomó las riendas de la moda en Medellín y en 1989 se realiza la primera versión de "Colombiamoda". En años posteriores la feria contó con la presencia de reconocidos diseñadores como Óscar de la Renta, Carolina Herrera, Badgley & Mischka, entre otros.

Altavoz Fest: El festival nació de la idea de crear un espacio para los jóvenes y los músicos de la ciudad expusieran su música, ya que en ocasiones anteriores se hicieron festivales de música que no duraron mucho tiempo; como lo fue el Festival de Ancón (1971 y 2004) y Rock a Lo Paisa (2000). El festival ha sido un gran eje de apoyo para los nuevos artistas de la ciudad, para que expongan su trabajo ante un gran público y con las buenas condiciones de sonido.

Otros eventos destacados de la ciudad: Expofinca, Feria del Hogar y la Integración Cooperativa, Feria de la Construcción, Feria Metalmecánica, Feria de la Antioqueñidad, Expocasa, Colombiamoda, Superventas, Feria internacional del Transporte, Café de Colombia, Saludexpo, Expoempresa, Agroferia, Hecho a Mano, entre otras.

El fútbol es uno de los deportes más populares en la ciudad. Atlético Nacional e Independiente Medellín son los dos equipos profesionales de la ciudad que participan en la Categoría Primera A del fútbol colombiano.

Las principales ligas de la ciudad son atletismo, BMX, baloncesto, balonmano, béisbol, ciclismo, esgrima, fútbol, gimnasia, judo, karate, microfútbol, monopatín, motociclismo, natación, patinaje, halterofilia, softbol, taekwondo, tejo, tenis, voleibol, ajedrez, tenis de mesa y voleibol de playa.

Un deporte muy popular en Medellín, y en general en toda Antioquia, es la equitación, por lo tanto se ha incentivado el comercio y producción de aperos y aparejos para esta actividad, como sillas y herraduras para exportación. Durante la Feria de las Flores las cabalgatas de Medellín lograron un Récord Guinness 1996 y 1999.

En 1978, Medellín fue sede de los XIII Juegos Centroamericanos y del Caribe y, entre el 19 y el 30 de marzo de 2010 se desarrollaron los IX Juegos Suramericanos "Medellín 2010", para lo cual se realizó una fuerte inversión en infraestructura deportiva, renovando y construyendo nuevos escenarios.

El Fútbol Americano, también se ha convertido en un deporte creciente en la ciudad. En Antioquia encontramos varios equipos que practican este deporte. Hunters y Lobos son dos de los equipos más representativos de la ciudad de Medellín.

Además, en 2011 con la organización la Copa Mundial de Fútbol Sub-20 de la FIFA de 2011 en Colombia, Medellín fue la sede de 10 partidos mundialistas. Este fue el evento futbolístico más importante del año, contando con 24 países participantes.

Sobre la infraestructura, Medellín cuenta con varios escenarios deportivos ubicados en los diferentes barrios de la ciudad, en los cuales la comunidad puede ingresar a ellas gratuitamente.

Unidad Deportiva Atanasio Girardot: Es la principal área deportiva de la ciudad. Se extiende en un área de 280 000 m² y aglutina canchas y estadios para la práctica y competencia de 34 deportes. Allí se encuentran la sede de la mayoría de las ligas deportivas. Tiene capacidad para capacidad para 44 500 espectadores lo mismo que con escenarios profesionales para varios miles de espectadores de baloncesto, voleibol, combate, gimnasia, atletismo, béisbol, natación, tenis, entre otros. 

El escudo, la bandera y el himno de la ciudad tienen el reconocimiento de símbolos oficiales del municipio de Medellín según el Decreto 151 del 20 de febrero de 2002, y como emblemas de la ciudad forman parte de la imagen institucional de la administración municipal, por lo cual están presentes en los actos, eventos y medios oficiales en los que deban figurar por su carácter representativo.

El escudo de armas de Medellín es el emblema más antiguo de la ciudad; tiene su origen en la concesión de su uso por el rey Carlos II de España por medio de la Real Cédula dada en Madrid el 31 de marzo de 1678, y cuyo documento dice:

Sin embargo, una descripción más refinada y estructurada en el lenguaje heráldico, aunque no es oficial, sería:

El blasón se ha mantenido con el tiempo desde que fue otorgado, sin más variaciones que las estéticas, pues es de destacar que existen diferentes versiones estilísticas entre la Alcaldía y el Concejo Municipal, además ninguna cumple estéticamente con las normas heráldicas.

El municipio adoptó la bandera de Antioquia, a la cual se le agregó el escudo de la ciudad para diferenciarlas. La bandera está compuesta por dos franjas horizontales de iguales proporciones, la superior blanca y la inferior verde, y en el centro entre ambas franjas se ubica el escudo. El color blanco simboliza pureza, integridad, obediencia, firmeza y elocuencia. El verde representa la esperanza, la abundancia, la libertad y la fe.

Igualmente, Medellín adoptó el Himno Antioqueño, de acuerdo con el decreto 151 del 20 de febrero de 2002, Artículo 10:

Adicionalmente están en trámite los Convenios con las siguientes ciudades: Río de Janeiro (Brasil), Florencia (Italia), Mendoza (Argentina), Valencia (España), Andalucía (España), Monterrey (México), Guayaquil (Ecuador), Cuenca (Ecuador), Makati (Filipinas), Ekurhuleni (Sudáfrica), Lima (Perú) y Miami (USA).




</doc>
<doc id="17421" url="https://es.wikipedia.org/wiki?curid=17421" title="Sievert">
Sievert

El sievert es la unidad de equivalencia de dosis de radiación ionizante del Sistema Internacional de Unidades (SI), igual al "joule" o "julio" por kilogramo (símbolo: Sv). Es una medida del efecto sobre la salud de bajos niveles de radiación ionizante en el cuerpo humano. El sievert es de importancia en dosimetría y protección radiológica, y lleva el nombre de Rolf Maximilian Sievert, un físico médico sueco reconocido por su trabajo en la medición de la dosis de radiación y la investigación de los efectos biológicos de la radiación. 

1 Sv es equivalente a un julio por cada kilogramo (J kg). Esta unidad da un valor numérico con el que se pueden cuantificar los efectos no estocásticos o determinísticos por las radiaciones ionizantes.

El sievert se utiliza para cantidades de dosis de radiación tales como dosis equivalente y dosis efectiva, que representan el riesgo de radiación externa de fuentes externas al cuerpo, y dosis comprometida, que representa el riesgo de irradiación interna debido a sustancias radiactivas inhaladas o ingeridas. El sievert pretende representar el riesgo estocástico para la salud, que para la evaluación de la dosis de radiación se define como la probabilidad de cáncer inducido por la radiación y el daño genético. Un sievert lleva consigo una probabilidad del 5,5% de desarrollar cáncer basado en el modelo lineal sin umbral.

Para permitir la consideración del riesgo estocástico para la salud, se realizan cálculos para convertir la cantidad física dosis absorbida en dosis equivalente y dosis efectiva, cuyos detalles dependen del tipo de radiación y del contexto biológico. Para las aplicaciones en la evaluación de la protección contra las radiaciones y la dosimetría, la Comisión Internacional de Protección Radiológica (CIPR) y Comisión Internacional de Unidades y Medidas de Radiación (ICRU) han publicado recomendaciones y datos que se utilizan para calcularlas. Estos son objeto de una revisión continua, y se aconsejan cambios en los "Informes" formales de esos órganos.

Convencionalmente, el sievert no se utiliza para altas tasas de dosis de radiación que producen efectos determinísticos, que es la gravedad del daño tisular agudo que es seguro que ocurra, como el síndrome de irradiación aguda; estos efectos se comparan con la cantidad física dosis absorbida medida por la unidad gray (Gy).

Su diferencia con el gray (unidad de la dosis absorbida) es que el Sievert está corregido por el daño biológico que producen las radiaciones, mientras que el gray mide la energía absorbida por un material.

La definición de SI dada por el Comité Internacional de Pesos y Medidas (CIPM) dice:

El valor de "Q"' no está definido por CIPM, pero requiere el uso de las recomendaciones pertinentes de la ICRU para proporcionar este valor.

El CIPM también dice que:

En resumen:

El gray - cantidad "D"

El sievert - cantidad "H"

La definición de la Comisión Internacional de Protección Radiológica (ICRP) (también conocida como CIPR) del sievert es:

El sievert se utiliza para una serie de cantidades de dosis que se describen en el presente artículo y que forman parte del sistema internacional de protección radiológica concebido y definido por la ICRP y la ICRU.

El sievert se utiliza para representar los efectos estocásticos de la radiación ionizante externa en el tejido humano. Las dosis de radiación recibidas se miden en la práctica con instrumentos radiométricos y dosímetros y se denominan cantidades operacionales. Para relacionar estas dosis reales recibidas con los probables efectos sobre la salud, se han desarrollado cantidades de protección para predecir los probables efectos sobre la salud utilizando los resultados de grandes estudios epidemiológicos. En consecuencia, esto ha requerido la creación de una serie de cantidades de dosis diferentes dentro de un sistema coherente desarrollado por la ICRU en colaboración con la ICRP.

Las cantidades de dosis externas y sus relaciones se muestran en el diagrama adjunto. La ICRU es la principal responsable de las cantidades de dosis operativas, basadas en la aplicación de la metrología de las radiaciones ionizantes, y la ICRP es la principal responsable de las cantidades de protección, basadas en la modelización de la absorción de dosis y la sensibilidad biológica del cuerpo humano.

Las cantidades de dosis de ICRU/ICRP tienen propósitos y significados específicos, pero algunos usan palabras comunes en un orden diferente. Puede haber confusión entre, por ejemplo, "dosis equivalente" y "equivalente de dosis"

Aunque la definición del CIPM establece que la función de transferencia lineal de energía (Q) del ICRU se utiliza para calcular el efecto biológico, la ICRP en 1990 desarrolló las cantidades de dosis de "protección" dosis "efectiva"' y dosis "equivalente" que se calculan a partir de modelos computacionales más complejos y se distinguen por no tener la frase "dosis equivalente" en su nombre. Solo las cantidades de dosis operativas que todavía utilizan Q para el cálculo conservan la frase "equivalente de dosis". Sin embargo, existen propuestas conjuntas ICRU/ICRP para simplificar este sistema mediante cambios en las definiciones de dosis operativas para armonizarlas con las de las cantidades de protección. Estos se esbozaron en el tercer Simposio Internacional sobre Protección Radiológica en octubre de 2015, y si se implementan harían más lógicos los nombres de las cantidades operativas introduciendo "dosis a la lente del ojo" y "dosis a la piel local" como "dosis equivalentes".

En los EE.UU. hay cantidades de dosis con diferentes nombres que no forman parte de la nomenclatura de la ICRP.

Se trata de cantidades físicas directamente medibles en las que no se han tenido en cuenta los efectos biológicos. Radiación fluencia es el número de partículas de radiación que inciden por unidad de área por unidad de tiempo, kerma es el efecto ionizante en el aire de los rayos gamma y rayos X y se utiliza para la calibración de instrumentos, y la dosis absorbida es la cantidad de energía de radiación depositada por unidad de masa en la materia o tejido bajo consideración.

Las cantidades operacionales se miden en la práctica, y son el medio de medir directamente la absorción de dosis debido a la exposición, o predecir la absorción de dosis en un entorno medido. De este modo, se utilizan para el control práctico de la dosis, proporcionando una estimación o límite superior para el valor de las cantidades de protección relacionadas con una exposición. También se utilizan en la normativa práctica y en las directrices.

La calibración de dosímetros individuales y de área en campos de fotones se realiza midiendo la colisión "kerma del aire libre en el aire" en condiciones de equilibrio de electrones secundarios. A continuación, se obtiene la cantidad operativa adecuada aplicando un coeficiente de conversión que relaciona el kerma del aire con la cantidad operativa adecuada. Los coeficientes de conversión para la radiación de fotones son publicados por el ICRU.

Se utilizan "fantomas" simples (no antropomórficos) para relacionar las cantidades operativas con la irradiación de aire libre medida. El fantasma de esfera ICRU se basa en la definición de un material ICRU equivalente a 4 elementos tisulares que realmente no existe y no puede ser fabricado. La esfera ICRU es una esfera "equivalente de tejido" teórico de 30 cm de diámetro compuesta por un material con una densidad de 1 g·cm y una composición de masa de 76,2% de oxígeno, 11,1% de carbono, 10,1% de hidrógeno y 2,6% de nitrógeno. Este material está especificado para aproximarse lo más posible al tejido humano en sus propiedades de absorción. Según la ICRP, la "esfera fantasma" de la ICRU en la mayoría de los casos se aproxima adecuadamente al cuerpo humano en lo que se refiere a la dispersión y atenuación de los campos de radiación penetrantes bajo consideración. Por lo tanto, la radiación de una determinada fluencia de energía tendrá aproximadamente la misma deposición de energía dentro de la esfera que la que tendría en la masa equivalente de tejido humano.

Para permitir la retrodispersión y la absorción del cuerpo humano, la "placa fantasma" se utiliza para representar el torso humano para la calibración práctica de los dosímetros de todo el cuerpo. La losa fantasma es de profundidad para representar el torso humano.

Las propuestas conjuntas ICRU/ICRP esbozadas en el tercer Simposio Internacional sobre Protección Radiológica en octubre de 2015 para cambiar la definición de las cantidades operativas no cambiarían el uso actual de los fantasmas de calibración o los campos de radiación de referencia.

Las cantidades de protección son modelos calculados y se utilizan como "cantidades límite" para especificar los límites de exposición a fin de garantizar, en palabras de la ICRP, "que la aparición de efectos estocásticos sobre la salud se mantenga por debajo de niveles inaceptables y que se eviten las reacciones de los tejidos". Estas cantidades no pueden medirse en la práctica, pero sus valores se derivan de modelos de dosis externas a órganos internos del cuerpo humano, utilizando fantasmas antropomórficos. Se trata de modelos computacionales 3D del cuerpo que tienen en cuenta una serie de efectos complejos, como la autoprotección del cuerpo y la dispersión interna de la radiación. El cálculo comienza con la dosis absorbida por el órgano y luego se aplican factores de radiación y de ponderación tisular.

Dado que las magnitudes de protección no pueden medirse en la práctica, deben utilizarse las magnitudes operativas para relacionarlas con las respuestas prácticas de los instrumentos de radiación y los dosímetros.

Esta es una lectura real obtenida de un monitor de dosis ambiental gama, o de un dosímetro personal. Estos instrumentos se calibran utilizando técnicas de metrología de radiaciones que los relacionan con una norma nacional de radiación y, por lo tanto, con una cantidad operativa. Las lecturas de los instrumentos y los dosímetros se utilizan para prevenir la captación de dosis excesivas y para proporcionar registros de la captación de dosis a fin de cumplir la legislación sobre seguridad radiológica, como en la UK, la Ionising Radiations Regulations 1999.

El sievert se utiliza en la protección radiológica externa para la dosis equivalente (la fuente externa, los efectos de la exposición de todo el cuerpo, en un campo uniforme), y la dosis efectiva (que depende de las partes del cuerpo irradiadas).

Estas cantidades de dosis son promedios ponderados de dosis absorbidas diseñados para ser representativos de los efectos estocásticos de la radiación sobre la salud, y el uso del sievert implica que se ha aplicado los factores de ponderación apropiados a la dosis absorbida (expresado en grays).

El cálculo de la ICRP proporciona dos factores de ponderación para permitir el cálculo de las magnitudes de protección.

Cuando se irradia uniformemente todo el cuerpo, solo se utiliza el factor de ponderación de la radiación "W", y la dosis efectiva es igual a la dosis equivalente para todo el cuerpo. Pero si la irradiación de un cuerpo es parcial o no uniforme, el factor tisular "W" se utiliza para calcular la dosis a cada órgano o tejido. Luego se suman para obtener la dosis efectiva. En el caso de la irradiación uniforme del cuerpo humano, estos se suman a 1, pero en el caso de la irradiación parcial o no uniforme, se suman a un valor más bajo en función de los órganos afectados, lo que refleja el menor efecto general sobre la salud. El proceso de cálculo se muestra en el diagrama adjunto. Este enfoque calcula la contribución del riesgo biológico a todo el cuerpo, teniendo en cuenta la irradiación total o parcial, y el tipo o tipos de radiación.
Los valores de estos factores de ponderación se eligen de forma conservadora para que sean mayores que la mayor parte de los valores experimentales observados para los tipos de células más sensibles, sobre la base de promedios de los obtenidos para la población humana.

Dado que los diferentes tipos de radiación tienen diferentes efectos biológicos para la misma energía depositada, un factor de ponderación de la radiación correctivo "W", que depende del tipo de radiación y del tejido diana, se aplica para convertir la dosis absorbida medida en la unidad gray para determinar la dosis equivalente. El resultado es el sievert de la unidad.

La dosis equivalente se calcula multiplicando la energía absorbida, promediada por la masa sobre un órgano o tejido de interés, por un factor de ponderación de radiación apropiado al tipo y energía de la radiación. Para obtener la dosis equivalente para una mezcla de tipos de radiación y energías, se toma una suma sobre todos los tipos de dosis de energía de radiación.

dónde
Así, por ejemplo, una dosis absorbida de 1 Gy por partículas alfa dará lugar a una dosis equivalente de 20 Sv.

Esto puede parecer una paradoja. Implica que la energía del campo de radiación incidente en julios ha aumentado en un factor de 20, violando así las leyes de conservación de la energía. Sin embargo, este no es el caso. El sievert se utiliza solo para transmitir el hecho de que un gray de partículas alfa absorbidas causaría veinte veces el efecto biológico de un gray de rayos X absorbidos. Es este componente biológico el que se expresa cuando se utilizan los tamices en lugar de la energía real suministrada por la radiación absorbida por el incidente.

El segundo factor de ponderación es el factor tisular "W", pero solo se utiliza si ha habido irradiación no uniforme de un cuerpo. Si el cuerpo ha sido sometido a una irradiación uniforme, la dosis efectiva es igual a la dosis equivalente para todo el cuerpo, y sólo se utiliza el factor de ponderación de la radiación "W"'. Pero si hay irradiación corporal parcial o no uniforme, el cálculo debe tener en cuenta las dosis individuales de cada órgano recibidas, porque la sensibilidad de cada órgano a la irradiación depende de su tipo de tejido. Esta dosis sumada de sólo los órganos afectados da la dosis efectiva para todo el cuerpo. El factor de ponderación de los tejidos se utiliza para calcular las contribuciones individuales de las dosis de los órganos. 

Los valores de la ICRP para "W" se indican en el cuadro que se muestra aquí.

El artículo sobre dosis efectiva da el método de cálculo. La dosis absorbida se corrige primero por el tipo de radiación para dar la dosis equivalente, y luego se corrige para el tejido que recibe la radiación. Algunos tejidos como la médula ósea son particularmente sensibles a la radiación, por lo que se les da un factor de ponderación que es desproporcionadamente grande en relación con la fracción de la masa corporal que representan. Otros tejidos como la superficie ósea dura son particularmente insensibles a la radiación y se les asigna un factor de ponderación desproporcionadamente bajo.

En resumen, la suma de las dosis ponderadas por tejido de cada órgano o tejido irradiado del cuerpo se suma a la dosis efectiva para el cuerpo. El uso de la dosis efectiva permite comparar la dosis total recibida independientemente del grado de irradiación corporal.

Las cantidades operativas se utilizan en aplicaciones prácticas para supervisar e investigar situaciones de exposición externa. Se definen para mediciones operacionales prácticas y evaluación de dosis en el cuerpo. Se diseñaron tres cantidades de dosis operacionales externas para relacionar las mediciones operacionales del dosímetro y del instrumento con las cantidades de protección calculadas. También se diseñaron dos fantasmas, los fantasmas ICRU "losa" y "esfera", que relacionan estas cantidades con las cantidades de radiación incidente utilizando el cálculo Q(L).

Se utiliza para la vigilancia de la superficie de las radiaciones penetrantes y suele expresarse como la cantidad "H"*(10). Esto significa que la radiación es equivalente a la que se encuentra 10 mm dentro de la esfera ICRU en la dirección de origen del campo. Un ejemplo de radiación penetrante son los rayos gamma.

Se utiliza para monitorizar la radiación de baja penetración y suele expresarse como la cantidad "H"'(0.07). Esto significa que la radiación es equivalente a la que se encuentra a una profundidad de 0,07 mm en la esfera ICRU. Ejemplos de radiación de baja penetración son las partículas alfa, partículas beta y fotones de baja energía. Esta cantidad de dosis se utiliza para la determinación de dosis equivalentes a, por ejemplo, la piel, la lente del ojo. En la práctica de la protección radiológica, el valor de omega no suele especificarse, ya que, por lo general, la dosis se encuentra como máximo en el punto de interés.

Esto se utiliza para el monitoreo de dosis individuales, como por ejemplo con un dosímetro personal que se usa en el cuerpo. La profundidad recomendada para la evaluación es de 10 mm, lo que da la cantidad "H"(10).

Con el fin de simplificar los medios de cálculo de las cantidades operativas y ayudar a la comprensión de las cantidades de protección radiológica, el Comité 2 de la ICRP y el Comité 26 del Informe de la ICRU iniciaron en 2010 un examen de los diferentes medios para conseguirlo mediante coeficientes de dosis relacionados con la dosis efectiva o la dosis absorbida.

Específicamente;

La causa de ello es que "H"(10) no es una estimación razonable de la dosis efectiva debida a los fotones de alta energía, como resultado de la ampliación de los tipos de partículas y los rangos de energía que se considerarán en el informe 116 de la ICRP. Este cambio eliminaría la necesidad de la esfera ICRU e introduciría una nueva cantidad llamada "E".

El motor para ello es la necesidad de medir el efecto determinístico, que se sugiere, es más apropiado que el efecto estocástico. Esto calcularía las cantidades de dosis equivalentes "H" y "H".

Esto eliminaría la necesidad de la Esfera ICRU y la función Q-L. Cualquier cambio reemplazaría al informe 51 de la ICRU y a parte del informe 57.

En julio de 2017, el ICRU/ICRP emitió un proyecto de informe final para consulta. 

El sievert se utiliza para las cantidades de dosis internas humanas en el cálculo de la dosis comprometida. Se trata de una dosis de radionúclidos que han sido ingeridos o inhalados en el cuerpo humano, y por lo tanto "comprometidos" a irradiar el cuerpo durante un período de tiempo. Los conceptos de cálculo de las magnitudes de protección descritos para las radiaciones externas son aplicables, pero como la fuente de radiación se encuentra dentro del tejido del cuerpo, el cálculo de la dosis de órganos absorbida utiliza diferentes coeficientes y mecanismos de irradiación.

La ICRP define la dosis efectiva comprometida, E("t") como la suma de los productos de las dosis comprometidas de órgano o tejido equivalente y los factores de ponderación tisular apropiados "W", donde "t" es el tiempo de integración en los años siguientes a la ingesta. Se considera que el período de compromiso es de 50 años para los adultos y de 70 años para los niños.

La ICRP afirma además: "Para la exposición interna, las dosis efectivas comprometidas se determinan generalmente a partir de una evaluación de la ingesta de radionucleidos a partir de mediciones de bioensayos u otras cantidades (por ejemplo, actividad retenida en el cuerpo o en la excreta diaria). La dosis de radiación se determina a partir de la ingesta utilizando los coeficientes de dosis recomendados".

Una dosis comprometida de una fuente interna está destinada a conllevar el mismo riesgo efectivo que la misma cantidad de dosis equivalente aplicada uniformemente a todo el cuerpo desde una fuente externa, o la misma cantidad de dosis efectiva aplicada a una parte del cuerpo.

Síntomas en los humanos a causa de la radiación acumulada durante un mismo día (los efectos se reducen si el mismo número de Sieverts se acumula en un periodo más largo):

Síntomas en humanos por radiación acumulada durante un año, en milisieverts (1 Sv = 1000 mSv = 1000000 μSv):

En los viajes espaciales, y debido a que en el espacio existe radiación a causa del viento solar y de los rayos cósmicos, la NASA tiene la norma por la cual en 10 años de servicio, un astronauta no debería recibir mayor radiación que la que incrementaría en un 3% la probabilidad de sufrir a futuro un cáncer mortal.

Usando esta norma, la NASA calcula la cantidad de radiación máxima que un astronauta debería recibir en 10 años de servicio (basados en cálculos aproximados, sin mucha estadística disponible):

Hombres de 25 años: 0,7Sv; Mujeres de 25 años: 0,4Sv<br>
Hombres de 35 años: 0,9Sv; Mujeres de 35 años: 0,6Sv<br>
Hombres de 45 años: 1,5Sv; Mujeres de 45 años: 0,9Sv<br>
Hombres de 55 años: 2,9Sv; Mujeres de 55 años: 1,6Sv

El sievert tiene su origen en el röntgen equivalent man (rem) que deriva de las unidades CGS. La Comisión Internacional de Unidades y Medidas de Radiación (ICRU) promovió un cambio a unidades coherentes de SI en la década de 1970, y anunció en 1976 que tenía previsto formular una unidad adecuada para la dosis equivalente. La ICRP se adelantó a la ICRU introduciendo el sievert en 1977.

El sievert fue adoptado por el Comité Internacional de Pesos y Medidas (CIPM) en 1980, cinco años después de adoptar el gray. El CIPM emitió entonces una explicación en 1984, recomendando cuándo se debe usar el sievert en lugar del gray. Esta explicación se actualizó en 2002 para acercarla a la definición de dosis equivalente de la ICRP, que había cambiado en 1990. Específicamente, la ICRP había introducido dosis equivalentes, renombro el factor de calidad (Q) como factor de ponderación de la radiación (W), y redujo otro factor de ponderación 'N' en 1990. En 2002, el CIPM también eliminó el factor de ponderación 'N' de su explicación, pero mantuvo otros términos y símbolos antiguos. Esta explicación solo aparece en el apéndice del folleto del SI y no forma parte de la definición del sievert.

La radiación ionizante tiene efectos determinista y estocástico sobre la salud humana. Los eventos deterministas (efecto tisular agudo) ocurren con certeza, con las condiciones de salud resultantes en cada individuo que recibió la misma dosis alta. Los Estocásticos (inducción del cáncer y eventos genéticos) son intrínsecamente aleatorios, con la mayoría de los individuos en un grupo que no exhiben ningún causal efecto negativo para la salud después de la exposición, mientras que una minoría aleatoria indeterminada lo hace, a menudo con los sutiles efectos negativos para la salud que resultan siendo observables solo después de grandes estudios detallados epidemiológicos.

El uso del sievert implica que sólo se consideran los efectos estocásticos, y para evitar confusiones se comparan convencionalmente los efectos determinísticos con los valores de dosis absorbida expresados por la unidad SI gray (Gy).

Los efectos estocásticos son aquellos que ocurren al azar, como cáncer inducido por la radiación. El consenso de los reguladores nucleares, los gobiernos y el UNSCEAR es que la incidencia de cánceres debido a la radiación ionizante puede ser modelada de manera lineal con dosis efectiva a una tasa de 5.5% por sievert. Esto se conoce como el modelo lineal sin umbral (modelo LNT). Algunos comentaristas como la Academia Francesa de Ciencias argumentan que este modelo de LNT está obsoleto y debería ser reemplazado por un umbral por debajo del cual los procesos celulares naturales del cuerpo reparan el daño y/o reemplazan las células dañadas. Existe un acuerdo general de que el riesgo es mucho mayor para los bebés y los fetos que para los adultos, más alto para los de mediana edad que para los mayores, y más alto para las mujeres que para los hombres, aunque no existe un consenso cuantitativo al respecto.

Los efectos determinísticos (daño tisular agudo) que pueden conducir a síndrome de irradiación agudo solo se producen en el caso de dosis altas agudas (≳ 0.1 Gy) y tasas de dosis altas (≳ 0.1 Gy/h) y, convencionalmente, no se miden utilizando el sievert unitario, sino que se utiliza la unidad gray (Gy).
Un modelo de riesgo determinístico requeriría diferentes factores de ponderación (aún no establecidos) que los que se utilizan en el cálculo de la dosis equivalente y efectiva.

La ICRP recomienda una serie de límites para la asimilación de dosis en la tabla 8 del informe 103. Estos límites son "situacionales", para situaciones planificadas, de emergencia y existentes. Dentro de estas situaciones, se establecen límites para los siguientes grupos;


Para la exposición ocupacional, el límite es de 50 mSv en un solo año con un máximo de 100 mSv en un periodo consecutivo de cinco años, y para el público a un promedio de 1 mSv (0.001 Sv) de dosis efectiva por año, sin incluir las exposiciones médicas y ocupacionales.

A modo de comparación, los niveles naturales de radiación en el interior del Capitolio de los Estados Unidos son tales que un cuerpo humano recibiría una tasa de dosis adicional de 0.85 mSv/a, cercana al límite reglamentario, debido al contenido de uranio de la estructura de granito. Según el modelo conservador de la ICRP, alguien que pasó 20 años dentro del edificio del Capitolio tendría una posibilidad extra de contraer cáncer, por encima de cualquier otro riesgo existente (calculado como: 20 a-20 a·0.85 mSv/a·0.001 Sv/mSv·5.5%/Sv ≈ 0.1%). Sin embargo, ese "riesgo existente" es mucho mayor; un estadounidense promedio tendría un 10% de probabilidad de contraer cáncer durante este mismo período de 20 años, incluso sin ninguna exposición a la radiación artificial (ver epidemiología del cáncer natural y tasas de cáncer). Sin embargo, estas estimaciones no tienen en cuenta los mecanismos naturales de reparación de todas las células vivas, evolucionaron durante unos pocos miles de millones de años de exposición a las amenazas químicas y de radiación del medio ambiente que eran mayores en el pasado, y fueron exageradas por la evolución de el metabolismo del oxígeno.

No es frecuente encontrar dosis significativas de radiación en la vida diaria. Los siguientes ejemplos pueden ayudar a ilustrar las magnitudes relativas; estos son sólo ejemplos, no una lista completa de posibles dosis de radiación. Una "dosis aguda" es aquella que se produce en un período de tiempo corto y finito, mientras que una "dosis crónica" es una dosis que continúa durante un período de tiempo prolongado, de modo que se describe mejor mediante una tasa de dosis.

Todas las conversiones entre horas y años han asumido la presencia continua en un campo constante, sin tener en cuenta las fluctuaciones conocidas, la exposición intermitente y la radiactividad. Los valores convertidos se muestran entre paréntesis.
Notas en los ejemplos:
Los prefijos del SI se utilizan con frecuencia como el milisievert (1 mSv = 0.001 Sv) y el microsievert (1 μSv = 0.000001 Sv) y las unidades de uso común para las indicaciones derivadas del tiempo o "tasa de dosis" en los instrumentos y las advertencias para la protección radiológica son μSv/h y mSv/h. Los límites reglamentarios y las dosis crónicas se dan a menudo en unidades de mSv/a o Sv/a, donde se entiende que representan un promedio a lo largo de todo el año. En muchos escenarios ocupacionales, la tasa de dosis por hora puede fluctuar a niveles miles de veces más altos durante un breve período de tiempo, sin infringir los límites anuales. La conversión de horas a años varía debido a los años bisiestos y los programas de exposición, pero las conversiones aproximadas sí lo son:

La conversión de las tarifas por hora a las tarifas anuales se complica aún más por las fluctuaciones estacionales de la radiación natural, la desintegración de las fuentes artificiales y la proximidad intermitente entre los seres humanos y las fuentes. Una vez que la ICRP adoptó la conversión fija para la exposición ocupacional, aunque éstas no han aparecido en documentos recientes:

Por lo tanto, para las exposiciones de ocupación de ese período de tiempo,

1 Sv = 100 rem

En las aplicaciones que pueden encontrarse comúnmente suelen ser utilizados sus submúltiplos mSv y μSv. A partir de 1 Sv los efectos más importantes son los deterministas, por lo que se utiliza la dosis absorbida (por tanto los gray).

La siguiente tabla muestra las cantidades de radiación en unidades SI y no SI:
Aunque la Comisión Reguladora Nuclear de los Estados Unidos permite el uso de las unidades curie, rad y rem, junto con las unidades SI, Las Directivas europeas de unidades de medida de la Unión Europea exigían que su uso para "fines de salud pública..." se eliminara gradualmente para el 31 de diciembre de 1985.

Una unidad más antigua para la dosis equivalente es el rem, todavía se utiliza con frecuencia en los Estados Unidos. Un sievert es igual a 100 rem:


</doc>
<doc id="17425" url="https://es.wikipedia.org/wiki?curid=17425" title="Curio">
Curio

El curio es un elemento sintético de la tabla periódica cuyo símbolo es Cm y su número atómico es 96. Se produce bombardeando plutonio con partículas alfa (iones de helio). Es un actínido. El curio no existe en el ambiente terrestre, pero puede producirse en forma artificial. Sus propiedades químicas se parecen tanto a las de las tierras raras típicas que, si no fuera por su radiactividad, podría confundirse fácilmente con uno de estos elementos. Entre los isótopos conocidos del curio figuran los de número de masa 238 a 250. El isótopo Cm es de particular interés a causa de su uso potencial como una fuente compacta de fuerza termoeléctrica, al utilizarse el calor generado por decaimiento nuclear para generar fuerza eléctrica.

El curio metálico puede producirse por reducción del trifluoruro de curio, con vapor de bario. El metal tiene un lustre plateado, el cual se pierde al contacto con el aire, y una densidad relativa de 13.5. El punto de fusión es de 1340 (+/-) 40 °C (2444 +/- 72 °F). El metal se disuelve con facilidad en ácidos minerales comunes, con formación de ion tripositivo.

Se han preparado varios compuestos sólidos del curio y sus estructuras se han determinado por difracción de rayos X. Estos incluyen CmF, CmF, CmCl, CmBr, CmI, CmO, CmO. En los lantánidos hay análogos isoestructurales de los compuestos de curio.

El curio fue sintetizado por primera vez en la Universidad de California, Berkeley, también por Glenn T. Seaborg, Ralph A. James y Albert Ghiorso en 1944. Se eligió el nombre curio en honor a Marie Curie y su marido Pierre, famosos por descubrir el radio y por otros importantes trabajos sobre radiactividad.


</doc>
<doc id="17434" url="https://es.wikipedia.org/wiki?curid=17434" title="Aisladores de disco">
Aisladores de disco

Los aisladores de disco son un tipo de aislador empleado en líneas eléctricas de transmisión y distribución. Los hay principalmente de vidrio y de cerámica y sus características están normalizadas según el peso o fuerza soportable, nivel de contaminación admisible y diámetro.

Los aisladores en conjunto con los herrajes asociados tienen la misión de soportar al conductor de la línea, para soportarlo desde las torres o postes que la sostienen, proporcionando al mismo tiempo la aislación eléctrica requerida.

Dado un nivel de tensión aplicado, un cierto nivel de contaminación ambiental (según las categorías definidas en la Norma IEC 60815) y altitud de instalación respecto del nivel del mar, se requiere que los aisladores en su conjunto posean una cierta longitud mínima para asegurar que la línea sea adecuadamente aislada para evitar descargas a tierra a través de la estructura torre o poste. Esta se logra agrupando varios de estos aisladores en lo que se denomina una "cadena de aisladores".



Dada la aparición y continuo mejoramiento de nuevos materiales poliméricos, que poseen ventajas comparativas en cuento a resistencia mecánica frente a golpes y mejor comportamiento ante la contaminación, este tipo de aisladores ha ido progresivamente cayendo en el desuso. En este tipo de aplicaciones, una cadena de aisladores de disco es ahora reemplazada por un único aislador polimérico, lo que además simplifica su instalación o reemplazo.


</doc>
<doc id="17436" url="https://es.wikipedia.org/wiki?curid=17436" title="Idioma oficial">
Idioma oficial

Idioma oficial es el idioma establecido como de uso corriente en los actos de gobierno de un Estado. Por lo general su empleo está indicado taxativamente en la Constitución o en las leyes fundamentales del Estado. Las leyes, los documentos públicos o administrativos, los servicios de la administración pública y los procesos en el sistema judicial son conducidos en dicho idioma. El Estado, legalmente, presupone su conocimiento general; por lo cual la educación es dictada en el idioma oficial. 

En algunos casos, aunque no exista una norma legal de rango consititucional, se considera idioma oficial a aquel en el cual se redactan las leyes.

El idioma oficial está ligado fuertemente a la definición de Estado-nación. En esta concepción, a cada nación, entendida como comunidad cultural y social, le corresponde un Estado. Tal postulado implica que el Estado tiene una base lingüística exclusiva. 

En Europa, sobre todo, la ideología del Estado-nación dio origen, entre finales del siglo XIX y la primera mitad del siglo XX, a la aparición de nuevos estados nacionales basados en diferentes lenguas, como en el caso del Imperio Austrohúngaro, cuya desintegración se basó en las nacionalidades que lo componían, o bien a la integración en un mismo Estado de todos los hablantes de la misma lengua; es el caso de la unificación alemana y la base de la política nacionalista del Tercer Reich. 

En otros casos, una lengua regional fue tomada como oficial e impuesta a todo el territorio del Estado, eliminando en el proceso las lenguas no oficiales o los dialectos como sucedió en Italia desde el Resurgimiento y en España.

En algunos casos, el desconocimiento del idioma oficial por parte de un grupo de habitantes ha significado para este, la pérdida de sus derechos constitucionales.

En la actualidad se presentan casi todas las posibilidades lógicas al respecto del reconocimiento de una o más lenguas como oficiales:

Como consecuencia del colonialismo o del neocolonialismo, en algunos países de África, en las Filipinas y Belice, las lenguas oficiales y de la enseñanza (francés o inglés) no son las lenguas nacionales habladas por la mayoría de la población. Se pueden dar algunos casos como resultado del nacionalismo, como en la República de Irlanda donde la lengua oficial (el irlandés) es hablada solo por una pequeña porción de la población, mientras que la lengua secundaria que goza de un estatus legal inferior, el inglés, es la lengua de la mayoría de la población.

Técnicamente, solo son oficiales las lenguas cuyo uso establece explícitamente una ley. Sin embargo, muchas lenguas son consideradas "de facto" lenguas oficiales, lo cual significa que aunque ninguna regulación jurídica les atribuya un papel especial, son lenguas utilizadas en la comunicación cotidiana. Un ejemplo destacado de esto es el estatus del inglés en los Estados Unidos. En este país, ninguna ley declara que el inglés sea o deba ser la lengua oficial a nivel federal, aunque en la actualidad 30 estados lo reconocen como lengua oficial. El hecho de que en ese país, en todos los niveles, el inglés es "de facto" el único idioma usado en todos los asuntos oficiales, hace que pueda ser considerado como la lengua oficial de los Estados Unidos, aunque técnicamente no esté reconocido como tal.

Las consecuencias prácticas del carácter "oficial" de una lengua varían, y frecuentemente dependen de cuán extendido esté su uso hablado. En algunos casos, solo la lengua oficial es la única que se puede usar ante tribunales de justicia, en el sistema educativo u otros ámbitos, mientras que en otros casos el estatus de oficial simplemente autoriza a que dicha lengua pueda ser usada. Por ejemplo, en Nueva Zelanda, la "Māori Language Act" permite que el maorí sea usado en asuntos legales, aunque la inmensa mayoría de ellos se llevan a cabo en inglés. En otros lugares, como Gales o Irlanda, las leyes establecen que las publicaciones oficiales deben estar tanto en la lengua minoritaria como en la lengua predominante. El reconocimiento oficial, por otra parte, está correlacionado con el que dicha lengua sea ampliamente enseñada en la educación infantil o que su conocimiento tenga carácter obligatorio para ciertos funcionarios del gobierno.

Un punto importante es que lengua oficial no debe confundirse con las lenguas nacionales que frecuentemente gozan de cierto reconocimiento por parte del gobierno.

Un idioma oficial está frecuentemente relacionado con cuestiones políticas, sociales y económicas, implicando tomas de posición acerca de la soberanía, la supremacía cultural y étnica, nacionalismo cultural o los derechos de las minorías étnicas, por lo que es un recurso utilizado para la construcción de los que algunos historiadores llaman "comunidades imaginadas". Por ejemplo, la campaña "English-only movement" para lograr que el inglés sea considerado legalmente como idioma oficial de los Estados Unidos de América, es vista como un intento para marginar a las comunidades de origen extranjero, particularmente la latinoamericana. En el caso de la República de Irlanda, la decisión de hacer oficial al irlandés se correspondía con un amplio programa de revitalización de dicha lengua, conectado con el nacionalismo gaélico.

En la actualidad solo unos 80 idiomas son idiomas generales de uso en un estado, aunque un número importante de lenguas tienen el estatus de cooficiales en algunas áreas o regiones de países. La lista de lenguas que son oficiales en más de un estado es bastante más limitada e incluye solo 21 lenguas:


</doc>
<doc id="17437" url="https://es.wikipedia.org/wiki?curid=17437" title="Amida">
Amida

Una amida es un compuesto que se forma conceptual o químicamente por el reemplazo del hidroxilo de un oxácido por un sustituyente amino. 
En química orgánica, se le denomina por antonomasia como "amida" a las amidas de los ácidos carboxílicos (estrictamente, carboxamida). Se puede considerar como un derivado de un ácido carboxílico por sustitución del grupo —OH del ácido por un grupo —NH, —NHR o —NRR' (llamado grupo amino). Por esto su grupo funcional es del tipo RCONR'R<nowiki>"</nowiki>, siendo CO un carbonilo, N un átomo de nitrógeno, y R, R' y R<nowiki>"</nowiki> radicales orgánicos o átomos de hidrógeno:

Cuando el grupo amida no es el principal, se nombra usando el prefijo carbamoil:

CH-CH-CH(CONH)-CH-CH-COOH → ácido 4-carbamoilhexanoico.

Todas las amidas, excepto la primera de la serie, son sólidas a temperatura ambiente y sus puntos de ebullición son elevados, más altos que los de los ácidos correspondientes. Presentan excelentes propiedades disolventes y son bases muy débiles. Uno de los principales métodos de obtención de estos compuestos consiste en hacer reaccionar el amoníaco (o aminas primarias o secundarias) con ésteres. Las amidas son comunes en la naturaleza, y una de las más conocidas es la urea, una diamida que no contiene hidrocarburos. Las proteínas y los péptidos están formados por amidas. Un ejemplo de poliamida de cadena larga es el nailon. Las amidas también se utilizan mucho en la industria farmacéutica.

La amida del ácido carbónico es denominada urea y sus derivados son el grupo funcional ureido.

Las diacilaminas son denominadas como grupo funcional imida y son análogas a los anhídridos carboxílicos:
Las poliamidas son compuestos que contienen grupos amida. Algunos son sintéticas, como el nailon, pero también se encuentran en la naturaleza, en las proteínas, formadas a partir de los aminoácidos, por reacción de un grupo carboxilo de un aminoácido con un grupo amino de otro. En las proteínas al grupo amida se le llama enlace peptídico.
El nailon es una poliamida debido a los característicos grupos amida en la cadena principal de su formulación. Por ejemplo, el nailon 6 se obtiene por polimerización de la ε-caprolactama.

Ciertas poliamidas del tipo nailon son la poliamida-6, la poliamida-11, la poliamida-12, la poliamida-9,6, la poliamida-6,9, la poliamida-6,10 y la poliamida-6,12. Se pueden citar como ejemplo de poliamidas no lineales los productos de condensación de ácidos dimerizados de aceites vegetales con aminas.

Los péptidos, incluyendo las proteínas como la seda, a la que el nailon reemplazó, también son poliamidas. Estos grupos amida son muy polares y pueden unirse entre sí mediante enlaces por puente de hidrógeno. Debido a esto y a que la cadena del nailon es tan regular y simétrica, los nailones son a menudo cristalinos, y forman excelentes fibras.
La síntesis de amidas se puede llevar a cabo por diversos métodos. 
El método más simple es la condensación de un ácido carboxílico con una amina. Esta reacción es termodinámicamente favorable en general, pero tiene una elevada energía de activación, debido principalmente a la primera desprotonación del ácido carboxílico y la protonación de la amina, lo cual forma un producto estable, el carboxilato de aminio. Esto reduce la reactividad. Además, se requieren altas temperaturas.

Se conocen muchos métodos para conducir el equilibrio hacia la derecha. En su mayor parte, estas reacciones implican "activar" el ácido carboxílico convirtiéndolo primero en un mejor electrófilo; tales como ésteres, cloruros de ácido (reacción de Schotten-Baumann) o anhídridos (método de Lumière-Barbier). Los métodos convencionales en la síntesis de péptidos usan agentes de acoplamiento tales como HATU, hidroxibenzotiazol (HOBt) o PyBOP. Recientemente, han surgido reactivos novedosos a base de boro para la formación del enlace CO-N, incluyendo el uso de catalizadores de ácido 2-yodofenilborónico, MIBA y borato de tris(2,2,2-trifluoroetilo).

Las principales reacciones de las amidas son:


Las amidas son comunes en la naturaleza y se encuentran en sustancias como los aminoácidos, las proteínas, el ADN y el ARN, hormonas y vitaminas.

La urea es utilizada para la excreción del amoníaco (NH) en el ser humano y mamíferos. También es muy utilizada en la industria farmacéutica y en la industria del nailon.


</doc>
<doc id="17443" url="https://es.wikipedia.org/wiki?curid=17443" title="Política internacional">
Política internacional

La política internacional es una relación sociocultural.
Para cada Estado, sus principales prioridades en política exterior están al mismo nivel en los Estados geográficamente colindantes, en las relaciones con aquellos países que tienen voto en las Naciones Unidas, en los organismos internacionales con sede principalmente en Nueva York y Ginebra y con aquellos países con los que mantiene relaciones económicas privilegiadas.

La política exterior moderna debe obedecer a criterios de Estado; a una percepción de la síntesis histórica de la ubicación de un país en el mundo, a una lectura adecuada de los desafíos de la globalización y de su impacto en la vida de cada uno de los individuos de una nación.

La política externa es, también, una variable de la política interna. Los procesos internacionales, políticos, estratégicos, comerciales, financieros, sociales, demográficos, científico-tecnológicos, culturales y de comunicación, pueden influir negativamente o positivamente en los esfuerzos de un gobierno para consolidar la democracia y el estado de derecho, avanzando en la transformación.

La política internacional interpreta la realidad nacional y la relaciona con las tendencias positivas y eventualmente negativas de la globalización, en función de las relaciones limítrofes, regionales y mundiales. De los resultados de la ecuación entre las demandas del proceso político, económico y social interno, y los límites y posibilidades que ofrece el entorno mundial globalizado, surgen las bases conceptuales, los atributos, los intereses nacionales, los principios, la agenda, las prioridades y el modelo de gestión institucional de la política exterior de un país.

En el estado moderno, las instituciones de gobierno constituyen los instrumentos políticos generalmente aceptados para mantener un marco de orden en la sociedad.

El objetivo de la política exterior es el de generar y preservar un ambiente de paz, distensión, estabilidad y respeto del derecho internacional, en los ámbitos limítrofe, subregional, regional y mundial, con la finalidad de obtener el escenario más idóneo que permita aplicar una diplomacia adecuada a sus intereses. Una diplomacia para el desarrollo económico y social con equidad. 

Se aspira normalmente a un mundo basado en el equilibrio, respetuoso de los principios del derecho internacional, en el que el multilateralismo lejos de debilitarse se fortalezca. Un mundo donde se entienda que la globalización requiere de una gobernabilidad basada en los valores de los derechos humanos.

Una estructura internacional donde haya menos desigualdad entre naciones y al interior de estas. Un mundo que haga del desarrollo sustentable no solo un programa, sino una realidad en la que el eje de la sostenibilidad sean los seres humanos.

En este contexto, la política exterior se sustenta en algunos principios históricos y en otros que se derivan de la modernidad: 


Esta agenda básica se aplica utilizando todos los instrumentos de la política exterior, bilaterales y multilaterales siendo la política internacional multidimensional. 

En el mundo actual, la globalización ha reducido los espacios y ha ampliado las comunicaciones, por ello la diplomacia directa del jefe de Estado o de gobierno también es un instrumento esencial de las relaciones internacionales contemporáneas. También ordenada civilmente en cada estado retrospectivo.




</doc>
<doc id="17446" url="https://es.wikipedia.org/wiki?curid=17446" title="Láser">
Láser

Un láser (del acrónimo inglés LASER, "light amplification by stimulated emission of radiation"; amplificación de luz por emisión estimulada de radiación) es un dispositivo que utiliza un efecto de la mecánica cuántica, la emisión inducida o estimulada, para generar un haz de luz coherente tanto espacial como temporalmente. La coherencia espacial se corresponde con la capacidad de un haz para permanecer con un pequeño tamaño al transmitirse por el vacío en largas distancias y la coherencia temporal se relaciona con la capacidad para concentrar la emisión en un rango espectral muy estrecho.

En 1915, Albert Einstein estableció los fundamentos para el desarrollo de los láseres y de sus predecesores, los máseres (que emiten microondas), utilizando la ley de radiación de Max Planck basada en los conceptos de emisión espontánea e inducida de radiación.

En 1928, Rudolf Ladenburg informó haber obtenido la primera evidencia del fenómeno de emisión estimulada de radiación, aunque no pasó de ser una curiosidad de laboratorio, por lo que la teoría fue olvidada hasta después de la Segunda Guerra Mundial, cuando fue demostrada definitivamente por Willis Eugene Lamb y R. C. Rutherford.

En 1953, Charles H. Townes y los estudiantes de postgrado James P. Gordon y Herbert J. Zeiger construyeron el primer máser: un dispositivo que funcionaba con los mismos principios físicos que el láser pero que produce un haz coherente de microondas. El máser de Townes era incapaz de funcionar en continuo. Nikolái Básov y Aleksandr Prójorov de la Unión Soviética trabajaron independientemente en el oscilador cuántico y resolvieron el problema de obtener un máser de salida de luz continua, utilizando sistemas con más de dos niveles de energía.

Townes, Básov y Prójorov compartieron el Premio Nobel de Física en 1964 por «los trabajos fundamentales en el campo de la electrónica cuántica», los cuales condujeron a la construcción de osciladores y amplificadores basados en los principios de los máser-láser.

El primer láser fue uno de rubí y funcionó por primera vez el 16 de mayo de 1960. Fue construido por Theodore Maiman. El hecho de que sus resultados se publicaran con algún retraso en "Nature", dio tiempo a la puesta en marcha de otros desarrollos paralelos. Por este motivo, Townes y Arthur Leonard Schawlow también son considerados inventores del láser, el cual patentaron en 1960. Dos años después, Robert Hall inventa el láser generado por semiconductor. En 1969 se encuentra la primera aplicación industrial del láser al ser utilizado en las soldaduras de los elementos de chapa en la fabricación de vehículos y, al año siguiente Gordon Gould patenta otras muchas aplicaciones prácticas para el láser.

El 16 de mayo de 1980, un grupo de físicos de la Universidad de Hull liderados por Geoffrey Pert registran la primera emisión láser en el rango de los rayos X. Pocos meses después se comienza a comercializar el disco compacto, donde un haz láser de baja potencia «lee» los datos codificados en forma de pequeños orificios (puntos y rayas) sobre un disco óptico con una cara reflectante. Posteriormente esa secuencia de datos digitales se transforma en una señal analógica permitiendo la escucha de los archivos musicales. En 1984, la tecnología desarrollada comienza a usarse en el campo del almacenamiento masivo de datos. En 1994, en el Reino Unido, se utiliza por primera vez la tecnología láser en cinemómetros para detectar conductores con exceso de velocidad. Posteriormente se extiende su uso por todo el mundo.

Ya en el siglo XXI, científicos de la Universidad de St. Andrews crean un láser que puede manipular objetos muy pequeños. Al mismo tiempo, científicos japoneses crean objetos del tamaño de un glóbulo rojo utilizando el láser. En 2002, científicos australianos «teletransportan» con éxito un haz de luz láser de un lugar a otro. Dos años después el escáner láser permite al Museo Británico efectuar exhibiciones virtuales. En 2006, científicos de Intel descubren la forma de trabajar con un chip láser hecho con silicio abriendo las puertas para el desarrollo de redes de comunicaciones mucho más rápidas y eficientes.

Un láser típico consta de tres elementos básicos de operación. Una cavidad óptica resonante, en la que la luz puede circular, que consta habitualmente de un par de espejos de los cuales uno es de alta reflectancia (cercana al 100 %) y otro conocido como acoplador, que tiene una reflectancia menor y que permite la salida de la radiación láser de la cavidad.

Dentro de esta cavidad resonante se sitúa un medio activo con ganancia óptica, que puede ser sólido, líquido o gaseoso (habitualmente el gas se encontrará en estado de plasma parcialmente ionizado) que es el encargado de amplificar la luz. Para poder amplificar la luz, este medio activo necesita un cierto aporte de energía, llamada comúnmente bombeo. Este bombeo es generalmente un haz de luz (bombeo óptico) o una corriente eléctrica (bombeo eléctrico).
La cavidad óptica resonante, conocida también como cavidad láser, existe en la gran mayoría de los dispositivos láser y sirve para mantener la luz circulando a través del medio activo el mayor número de veces posible. Generalmente está compuesta de dos espejos dieléctricos que permiten reflectividades controladas que pueden ser muy altas para determinadas longitudes de onda.

El espejo de alta reflectividad refleja cerca del 100 % de la luz que recibe y el espejo acoplador o de salida, un porcentaje ligeramente menor. Estos espejos pueden ser planos o con determinada curvatura, que cambia su régimen de estabilidad.

Según el tipo de láser, estos espejos se pueden construir en soportes de vidrio o cristales independientes o en el caso de algunos láseres de estado sólido pueden construirse directamente en las caras del medio activo, disminuyendo las necesidades de alineación posterior y las pérdidas por reflexión en las caras del medio activo.

Algunos láseres de excímero o la mayoría de los láser de nitrógeno, no utilizan una cavidad propiamente dicha, en lugar de ello un solo espejo reflector se utiliza para dirigir la luz hacia la apertura de salida. Otros láser como los construidos en microcavidades ópticas emplean fenómenos como la reflexión total interna para confinar la luz sin utilizar espejos.
El medio activo es el medio material donde se produce la amplificación óptica. Puede ser de muy diversos materiales y es el que determina en mayor medida las propiedades de la luz láser, longitud de onda, emisión continua o pulsada, potencia, etc.

El "medio activo" es donde ocurren los procesos de excitación (electrónica o de estados vibracionales) mediante bombeo de energía, emisión espontánea y emisión estimulada de radiación. Para que se dé la condición láser, es necesario que la ganancia óptica del medio activo sea superior a las pérdidas de la cavidad más las pérdidas del medio.

Dado que la "ganancia óptica" es el factor limitante en la eficiencia del láser, se tiende a buscar medios materiales que la maximicen, minimizando las pérdidas, es por esto que si bien casi cualquier material puede utilizarse como medio activo, solo algunas decenas de materiales son utilizados eficientemente para producir láseres.

Con mucha diferencia, los láseres más abundantes en el mundo son los de semiconductor. Pero también son muy comunes los láseres de estado sólido y en menos medida los de gas. Otros medios son utilizados principalmente en investigación o en aplicaciones industriales o médicas muy concretas.

Para que el medio activo pueda amplificar la radiación, es necesario excitar sus niveles electrónicos o vibracionales de alguna manera. Comúnmente un haz de luz (bombeo óptico) de una lámpara de descarga u otro láser o una corriente eléctrica (bombeo eléctrico) son empleados para alimentar al medio activo con la energía necesaria.

El "bombeo óptico" se utiliza habitualmente en láseres de estado sólido (cristales y vidrios) y láseres de colorante (líquidos y algunos polímeros) y el bombeo eléctrico es el preferido en láseres de semiconductor y de gas. En algunas raras ocasiones se utilizan otros esquemas de bombeo que le dan su nombre, por ejemplo a los láseres químicos o láseres de bombeo nuclear que utilizan la energía de la fisión nuclear.

Debido a las múltiples pérdidas de energía en todos los procesos involucrados, la potencia de bombeo siempre es mayor a la potencia de emisión láser.
Si bien existen varios mecanismos que producen emisión láser, se describe el ejemplo sencillo de un láser de cuatro niveles con bombeo óptico continuo, como puede ser el láser de neodimio.
En el estado inicial, la mayoría de los electrones se encuentran en el Estado fundamental y son excitados mediante un haz de luz de bombeo que contiene energía en las bandas de absorción del neodimio. Los electrones excitados en varios niveles se desexcitan rápidamente de forma no radiativa hacia un nivel metaestable, que en el caso del neodimio es el F donde permanece un tiempo relativamente largo, decayendo lentamente al nivel fundamental y al nivel I. Si se cumplen ciertas condiciones en el material y la potencia de bombeo, es posible que se produzca la inversión de población, esto es, que existan más átomos excitados en el nivel F que los que están en el nivel inferior I.
Desde el nivel metaestable F pueden desexcitarse espontáneamente algunos electrones que producen una emisión de luz a 1 064 nm. Algunos de estos se emiten en el ángulo correcto para reflejarse por los espejos de la cavidad un número elevado de veces. Estos fotones que se reflejan con el ángulo correcto pasan varias veces cerca de átomos excitados de neodimio y producen la emisión estimulada de radiación.

Si el medio activo se encuentra en la condición de inversión de población y las pérdidas de la cavidad son inferiores a la ganancia del medio activo, ocurre que al reflejarse en las paredes de la cavidad se produce una amplificación del primer fotón que se emitió espontáneamente. Tras un número determinado de reflexiones, la intensidad dentro de la cavidad es muy elevada, y las pequeñas perdidas del espejo acoplador son la radiación láser que emite el dispositivo.

Según la peligrosidad de los láseres y en función del Límite de Emisión Accesible (LEA) se pueden clasificar los láseres en las siguientes categorías de riesgo:
Cuando se inventaron, en 1960, los láseres se calificaron como «una solución a la espera de un problema». Desde entonces, se han vuelto omnipresentes y actualmente pueden encontrarse en miles de aplicaciones, en campos muy variados, como la electrónica de consumo, la tecnología de la información, la investigación científica, la medicina, la industria y el sector militar.

En muchas aplicaciones, los beneficios de los láseres se deben a sus propiedades físicas, como la coherencia, la monocromaticidad y la capacidad de alcanzar potencias extremadamente altas. A modo de ejemplo, un haz láser muy coherente puede enfocarse por debajo de su límite de difracción que, a longitudes de onda visibles, corresponde solamente a unos pocos nanómetros. Cuando se enfoca un haz de láser potente en un punto, este recibe una enorme densidad de energía. Esta propiedad permite al láser grabar gigabytes de información en las microscópicas cavidades de un CD, DVD o Blu-ray. También permite a un láser de media o baja potencia alcanzar intensidades muy altas y usarlo para cortar, quemar o incluso sublimar materiales. El rayo láser se emplea en el proceso de fabricación de grabar o marcar metales, plásticos y vidrio.
Atendiendo a la naturaleza de su medio activo, podemos clasificar los dispositivos láser en:
Estos láseres emplean típicamente vidrios, cristales o fibras dopadas como medio activo. Aunque los semiconductores son también de estado sólido, se suelen tomar en una categoría diferente. Algunos láseres de estado sólido son:






</doc>
<doc id="17447" url="https://es.wikipedia.org/wiki?curid=17447" title="Radiación cósmica">
Radiación cósmica

Los rayos cósmicos, también llamados radiación cósmica, son partículas subatómicas procedentes del espacio exterior cuya energía es muy elevada debido a su gran velocidad. Se descubrieron cuando se comprobó que la conductividad eléctrica de la atmósfera terrestre se debe a la ionización causada por radiaciones de alta energía.

En 1911, Victor Franz Hess, físico austríaco, demostró que la ionización atmosférica aumenta proporcionalmente a la altitud. Concluyó que la radiación debía proceder del espacio exterior.

El descubrimiento de que la intensidad de radiación depende de la altitud indica que las partículas integrantes de la radiación están eléctricamente cargadas y que las desvía el campo magnético terrestre.

Ernest Rutherford y sus colaboradores, contraria y anteriormente a las experiencias de Hess, supusieron que la ionización observada por el espectroscopio se debía a la radiactividad terrestre, ya que, medidas realizadas en 1910 en la base y la cúspide de la Torre Eiffel, así lo detectaban.

Robert Andrews Millikan acuñó la expresión "rayos cósmicos" tras sus propias mediciones que concluyeron en que, efectivamente, eran de origen muy lejano, incluso exterior al sistema solar.

Tras el descubrimiento de la radiactividad por Henri Becquerel en 1896, se aceptaba que la electricidad atmosférica —ionización del aire— era provocada exclusivamente por la radiación generada a su vez por elementos radiactivos en el suelo y por los gases radiactivos o isótopos de radón que aquellos producen. La posterior medición, durante la década de 1900 a 1910, de la tasa de ionización (ritmo de ionización del aire) respecto a la altitud demostró un descenso que podía explicarse por la absorción de la radiación ionizante por el aire interpuesto.

En 1909, Theodor Wulf desarrolló el primer electrómetro. Este era un instrumento diseñado para medir la tasa de producción de iones dentro de un contenedor sellado herméticamente. Wulf usó este instrumento para demostrar que los niveles de radiación ionizante en la cúspide de la Torre Eiffel eran mayores que en su base. Sin embargo, su artículo, publicado en "Physikalische Zeitschrift", no encontró amplia aceptación. En 1911, Domenico Pacini observó variaciones simultáneas de la tasa de ionización sobre un lago, sobre el mar y a una profundidad de 3 metros bajo la superficie. Del descenso observado bajo el agua, Pacini concluyó que una parte de la ionización se debe a fuentes distintas de la radiactividad terrestre. 

Más tarde, en 1912, Victor Hess elevó tres electrómetros Wulf de precisión mejorada a una altitud de 5300 metros usando un globo aerostático y encontró que la tasa de ionización se multiplicaba aproximadamente por cuatro en comparación con la que podía medirse a nivel del suelo. Hess también descartó al Sol como la fuente de radiación responsable mediante un nuevo ascenso en globo durante un eclipse de sol casi total. Cuando la Luna estaba bloqueando la mayor parte de la radiación solar visible, Hess todavía pudo medir una tasa de ionización en aumento con la altura, y concluyó: "La mejor explicación al resultado de mis observaciones viene dada por la suposición de que una radiación de un enorme poder de penetración entra en nuestra atmósfera desde arriba". En 1913-1914, Werner Kolhörster confirmó las primeras observaciones de Hess al medir el incremento de la tasa de ionización a 9 km de altitud.

Hess recibió el Premio Nobel de física en 1936, por su descubrimiento.

El vuelo del globo de Hess tuvo lugar el 7 de agosto de 1912. Exactamente 100 años después, el 7 de agosto de 2012, el vehículo Mars Science Laboratory midió los niveles de radiación ionizante por vez primera en otro planeta por medio de su RAD (Detector de evaluación de radiación, por las siglas en inglés de Radiation Assessment Detector).

Aún no está claro el origen de los rayos cósmicos. Se sabe que, en los períodos en que se emiten grandes erupciones solares, el Sol emite rayos cósmicos de baja energía, pero estos fenómenos estelares no son frecuentes. Por lo tanto, no son motivo de explicación del origen de esta radiación. Tampoco lo son las erupciones de otras estrellas semejantes al Sol. Las grandes explosiones de supernovas son, al menos, responsables de la aceleración inicial de gran parte de los rayos cósmicos, ya que los restos de dichas explosiones son potentes fuentes de radio, que implican presencia de electrones de alta energía.

En 2007, un grupo de científicos argentinos del Observatorio Pierre Auger realizó un espectacular descubrimiento que inauguró una nueva rama de la astronomía. Este grupo encontró evidencias de que la mayor parte de las partículas de rayos cósmicos proviene de una constelación cercana: Centaurus. Esta constelación contiene una galaxia de núcleo activo, cuyo núcleo se debe a existencia de un agujero negro (probablemente supermasivo), al caer la materia a la ergosfera del agujero negro y rotar velozmente.

A enormes velocidades, centrífugamente, se fuga parte de esa materia, constituida por protones y neutrones. Al alcanzar la Tierra (u otros planetas con atmósferas suficientemente densas) sólo llegan los protones, los cuales, tras chocar contra las capas superiores atmosféricas, caen en cascadas de rayos cósmicos. El descubrimiento observado en Centaurus parece ser extrapolable a todas las galaxias de núcleos activados por agujeros negros.

También se cree que, como resultado de las ondas de choque procedentes de las supernovas que se propagan hasta el espacio interestelar, en este se genera aceleración adicional. No existen pruebas directas de que las supernovas contribuyan de manera significativa a los rayos cósmicos. Sin embargo, se sugiere que las estrellas binarias de rayos X pueden ser fuentes de rayos cósmicos. En esos sistemas, una estrella normal cede masa a su complementaria, a una estrella de neutrones o bien a un agujero negro.

Los estudios radioastronómicos de otras galaxias muestran que estas también contienen electrones de alta energía. Los centros de algunas galaxias emiten ondas de radio de mucha mayor intensidad que la Vía Láctea. Esto indica que contienen fuentes de partículas de alta energía.

Los rayos cósmicos que alcanzan la atmósfera en su capa superior son principalmente (98%) protones y partículas alfa de alta energía. El resto está constituido por electrones y partículas pesadas ionizadas. A estas se les denomina "partículas primarias".

Estas partículas cargadas interaccionan con la atmósfera y el campo magnético terrestre, se convierten en "partículas secundarias" (son producto de la interacción de las partículas primarias con la atmósfera) y se distribuyen de tal modo que, debido al campo magnético, la mayor intensidad de las partículas que alcanzan el suelo ocurre en los polos.

Por tanto, la componente de partículas que alcanzan el suelo varía según la altitud (a mayor altura, menos atmósfera con la cual interaccionar) y por la latitud (a mayor latitud, mayor cantidad de partículas desviadas por el campo magnético), y propician cierta variación con el ciclo solar (de 11 años).

A nivel del mar y a una latitud de unos 45º N, los componentes importantes de estas partículas son:


Las dosis recibidas debido a los rayos cósmicos varían entre 300 μSv (microsieverts) y 2 000 μSv al año. Promediada por la población, datos de ocupación y otros factores, se encuentra un valor promedio de 380 μSv/año.


Las lluvias o cascadas de partículas subatómicas se originan por acción de rayos cósmicos primarios, cuya energía puede ser superior a 10 eV (electronvoltios): cien millones de veces superior a la que se puede impartir a una partícula subatómica en los más potentes aceleradores de partículas.

Cuando un rayo cósmico de alta energía llega a la atmósfera terrestre interactúa con átomos de ésta, choca contra los gases y libera electrones. Este proceso excita los átomos y genera nuevas partículas. Estas, a su vez, colisionan contra otras y provocan una serie de reacciones nucleares, que originan nuevas partículas que repiten el proceso en cascada. Así, puede formarse una cascada de más de 10 nuevas partículas. Los corpúsculos integrantes de las cascadas se pueden medir con distintos tipos de detectores de partículas, generalmente basados en la ionización de la materia o en el efecto Cherenkov.




</doc>
<doc id="17448" url="https://es.wikipedia.org/wiki?curid=17448" title="Ollantaytambo">
Ollantaytambo

Ollantaytambo (quechua: "Ollantay Tampu") es un poblado y sitio arqueológico incaico, capital del distrito de Ollantaytambo (provincia de Urubamba), situado al sur del Perú, a unos 90 km al noroeste de la ciudad del Cuzco.

Durante el incanato, Pachacuti conquistó la región y construyó el pueblo y un centro ceremonial. En la época de la conquista sirvió como fuerte de Manco Inca Yupanqui, líder de la resistencia inca. En Ollantaytambo hay andenes de resistencia (para evitar deslizamientos), no agrícolas como en los demás sitios arqueológicos del Cuzco. En la actualidad es una importante atracción turística debido a sus construcciones incas y por ser uno de los puntos de partida más comunes del camino inca hacia Machu Picchu.

Ollantaytambo trata de un típico ejemplo de la extraordinaria planificación urbana de los incas.

Sus callejuelas empedradas y serpenteantes, las ruinas diseminadas por doquier y sus terrazas agrícolas son atractivos que destacan por sí mismos y el visitante lo puede apreciar en todo su esplendor. Entre las ruinas, es recomendable la visita a la antigua fortaleza y al templo, donde podemos apreciar magníficas vistas del Valle Sagrado de los Incas.

Ollantaytambo está ubicado al margen del río Patakancha, cerca del punto donde confluye con el río Urubamba. Se encuentra en el distrito del mismo nombre, provincia de Urubamba, aproximadamente a 60 km al noroeste de la ciudad del Cuzco y tiene una altura de 2.792 metros sobre el nivel del mar.

El clima de Ollantaytambo es seco de abril a diciembre y lluvioso en los meses de enero a marzo. Debido a su ubicación entre dos vertientes por las noches corre un viento moderado. La temperatura mínima es de 5°C a 11°C y máximas de 18°C a 23°C durante todo el año.

Según el lingüista Rodolfo Cerrón-Palomino, "Ollantay" tiene un origen aimara. Según el mismo, devendría de "Ullantawi": La raíz verbal "ulla-" ('ver') deverberado por el morfema "-nta" (acción hacia abajo o hacia adentro) de por conjunto "ullanta-" (ver hacia abajo, observar), que con el sufijo "-wi" es donominalizado a "lugar de observación desde lo alto", es decir, atalaya o mirador.

Con posterioridad, el quechua comenzó a desplazar al aimara de la zona del Cuzco, alterando el nombre por apocopación del nombre sin símil en el nuevo idioma ("Ullantawi → "Ullantaw") para después trocar el final /w/ en/y/ ("Ullantaw → "Ullantay"), fenómeno constantemente repetido en este proceso de cambio lingüístico.

Posteriormente, con la dominación inca, Viracocha Inca manda fundar un tambo en el nueva plaza conquistada al parangón de la administración cuzqueña: el tambo de Ollantay o "Ullantay Tampu". A la postre, "Ullantay" quedó relegado a modificador de la raíz "tampu" (pronunciada como en la época de la conquista).

Algunos autores, como el historiador cuzqueño Víctor Angles, aseguran sin mayor argumentación que el origen del nombre de Ollantaytambo se da a fines del siglo XVIII, cuando se puso en escena un drama de argumento inca cuyo protagonista era el General Ollantay, y el lugar donde se desarrollaron las acciones —según la obra literaria— fue el tambo abajo de Yucay, que desde ese entonces comenzó a generalizarse como Ollantaytambo, sin embargo, el nombre se halla registrado en documentos de mayor antigüedad, como en los escritos del Inca Garcilaso de la Vega, quien después de elogiar la grandeza y magnificencia de las antiguas fortificaciones de Tanpu, cuenta que fueron mandadas a construir por el inca Wiraqucha, al igual que los grandes y antiguos edificios que existen en ese lugar.

Según Pedro Sarmiento de Gamboa, un cronista español del siglo XVI, el emperador inca Pachacútec conquistó y destruyó Ollantaytambo para luego incorporarlo en su imperio. Bajo el gobierno de los incas, el pueblo fue reconstruido con espléndidos edificios y el valle del río Urubamba fue irrigado y provisto de andenes; el pueblo sirvió de albergue para la nobleza inca mientras que los andenes eran trabajados por "yanaconas", sirvientes del emperador. Después de la muerte de Pachacútec la región pasó a la custodia de su "panaqa", su grupo familiar.

Durante la conquista, Ollantaytambo funcionó como capital temporal para Manco Inca Yupanqui, líder de la resistencia inca contra los conquistadores españoles. Bajo su mandato, el pueblo y sus alrededores fueron severamente fortificados en dirección a la antigua capital inca de Cuzco, la cual había caído bajo dominio español. En el llano de Mascabamba, cerca de Ollantaytambo, Manco Inca derrotó una expedición española bloqueando su avance desde un conjunto de andenes e inundando el llano. Sin embargo, a pesar de su victoria, Manco Inca no consideró viable el permanecer en Ollantaytambo así que se retiró al espeso bosque de la zona de Vilcabamba. En 1540, la población nativa de Ollantaytambo fue asignada en encomienda a Hernando Pizarro.
Se trata de uno de los complejos arquitectónicos más monumentales del antiguo Imperio inca, comúnmente llamado «Fortaleza», debido a sus descomunales muros, fue en realidad un Tambo o ciudad-alojamiento, ubicado estratégicamente para dominar el Valle Sagrado de los Incas.

El tipo arquitectónico empleado, así como la calidad de cada piedra, trabajada individualmente (ver ), hacen de Ollantaytambo una de las obras de arte más peculiar y sorprendente que realizaron los antiguos peruanos, especialmente el Templo del Sol y sus gigantescos monolitos.

Las calles rectas, estrechas y pintorescas hoy forman quince manzanas de casas ubicadas al norte de la plaza principal de la ciudad, que constituyen en sí un verdadero legado histórico. Algunas casas de tipo colonial están construidas sobre hermosos muros incaicos pulidos con finura. Los tonos de la piedra son alegres, de un color de flor petrificada, rosa oscuro. En la plaza principal un gran bloque de perfectas aristas encaja en una doble hilera sus quince ángulos de estrella terrestre.

Fue declarado Parque Arqueológico con la Resolución Directoral Nacional Nº 395 del año 2002. Tiene una extensión de 34,800 hectáreas.




</doc>
<doc id="17449" url="https://es.wikipedia.org/wiki?curid=17449" title="Tambo (arquitectura)">
Tambo (arquitectura)

En el Imperio inca un tambo (del quechua "tanpu", que significa alojamiento temporal) era un recinto situado al lado de un camino importante usado por personal estatal itinerante como albergue y como centro de acopio para fines administrativos y militares. Su importancia está en que los tambos son las edificaciones de mayor presencia a lo largo del Imperio inca. El camino del Inca ("Qhapaq Ñan") tenía tambos distantes 20 o 30 km (una jornada de camino a pie) entre sí. Su principal función era la de albergar a los chasquis (emisarios) y a los funcionarios incas que transitaban estos caminos. No se tiene información si albergaban a hombres comunes y corrientes. Personas de las comunidades cercanas eran reclutadas para servir en los tambos como parte del sistema de trabajo denominado mita. 

Además de servir de refugio, se sabe que los tambos eran centros de acopio de alimentos, lana, leña u otros materiales básicos para la alimentación. De este modo, en épocas de penurias climáticas o desastres naturales los tambos alimentaban y proveían de algunos materiales para la población de las aldeas más cercanas. Como la agricultura era la principal fuente de alimentación de los habitantes del Imperio inca, la administración estableció estos lugares como una bodega donde se podía guardar alimento en caso de emergencia, asegurando así el buen vivir de la población.

Los incas construyeron muchos tambos cuando comenzaron a ampliar el "Qhapaq Ñan" durante el reinado de Túpac Yupanqui de 1471 a 1493. Los investigadores estiman que hubo 2000 o más tambos. Dada esta cantidad, la gran variedad de tamaños y funciones de los tambos es difícil de describir completamente. Como mínimo, los tambos tenían alojamiento, instalaciones para cocinar y silos de almacenamiento llamados collcas ("qullqas"). Más allá de esto, existe una considerable cantidad de variación entre diferentes tambos. Algunos tambos eran poco más que simples posadas, mientras que otros eran esencialmente ciudades que proporcionaban alojamiento temporal para los viajeros. Además, no hay marcadores claros que distingan grandes tambos de poblaciones o de centros administrativos pequeños. La arquitectura y la evidencia documental sugieren que los tamaños funcionales de los asentamientos probablemente correspondían a su capacidad para albergar a una población.
Las funciones de los tambos dependían de su tamaño y de las instalaciones que contenían. Cada tambo tenía la capacidad de albergar a varios funcionarios estatales. Por ejemplo, los tambos más pequeños sirvieron como estaciones de relevo para los chasquis. Tambos más grandes también podrían proveer otras funciones. Por ejemplo, los tambos más grandes tenían almacenes más grandes que podían proporcionar suministros y algunos alojamientos para los ejércitos en movimiento. Esta función, sin embargo, no debe confundirse con la de las collcas, que eran solo almacenes de los que se reabastecían los ejércitos cuando pasaban. Los tambos más grandes y lujosos generalmente se usaban para alojar al inca viajante y a su séquito (típicamente esposas y funcionarios estatales). 

Más allá de atender a varios tipos de viajeros, los tambos más grandes también contenían instalaciones donde diversos especialistas, como alfareros y tejedores, elaboraban sus productos. También podían servir como centros administrativos desde los cuales los señores locales supervisaban la región. Además, los tambos más grandes contenían espacios ceremoniales que servían como lugares para las prácticas religiosas. Adicionalmente, los historiadores también encontraron evidencia de actividad de caza, actividad minera y producción/explotación de coca en los sitios de algunos tambos.

Pedro Cieza de León hizo numerosas referencias a los tambos en sus "Crónicas de Perú". En el siguiente pasaje, Cieza de León describió los usos generales de los tambos que aprendió de los pueblos nativos: 

El Imperio inca estuvo comunicado por muchos caminos principales y secundarios que unieron de manera eficaz los pueblos del antiguo Perú. El diseño de estos caminos (de más de 30 000 km) fue de gran calidad y profesionalismo, a pesar de las grandes dificultades geográficas (cordillera de los Andes). El Cuzco fue el centro de esta red vial y en él confluyeron la mayor parte de los caminos, pues la capital de los incas era para ellos el "ombligo del mundo" y todo debía partir y culminar en ella.

Muchos historiadores o eruditos afirman que los tambos generalmente se colocaban a un día caminando unos de otros. Sin embargo, como señala Hyslop, hay muchos factores, tanto individuales como externos, que pueden afectar la cantidad de tiempo que se puede caminar en un día, lo que hace que esta descripción sea problemática. En la práctica, las distancias entre tambos varían enormemente, desde menos de 10 km hasta casi 45 km. Muchos factores diferentes afectaron la colocación de estos tambos. En general, los incas trataban de construirlos cerca del agua y en un terreno favorable, mientras que trataban de evitar un terreno desfavorable (como marismas o pendientes pronunciadas). En algunos casos, los incas intentaban construir lejos de los centros de población local (por razones desconocidas), pero otras veces intentaban construir cerca de fuentes de mano de obra locales. Además, la ubicación de los tambos puede haber sido influenciada por la velocidad promedio de las caravanas de llamas, que se movían más lentamente que un individuo. Otro viajero importante que se movea más lentamente que un individuo típico era el Inca o soberano del imperio ("Sapa Inka"). Dado que el "Sapa Inka" viajaba con una gran procesión, el viaje era más lento que si viajara solo, lo que requeriría una colocación más cercana de los tambos.

El tambo descubierto más al sur es el tambo Pirque, ubicado en el borde del río Maipo con un antiguo puente colgante que unía las dos extremos del río. Fue descubierto, con la expansión de Santiago de Chile hacia el sur. 

Felipe Guamán Poma de Ayala escribió que existían cinco categorías de tambos: ciudad y mesón real, villa y tambo real, pueblo y tambo real, tambo real (o "tambo del Inga"), tambillo. También señaló que cada diez tambos había uno de mayor jerarquía. Los tres primeros son asentamientos con población residente, mientras que los dos últimos solo son de prestación de servicios al camino. En los tambillos solo había un recinto sin vituallas ni gente.

Los restos de tambos se encuentran dispersos en el Perú moderno, Argentina, Bolivia, Chile, Colombia y Ecuador. Los restos de los tambos muestran una gran variedad de estilos arquitectónicos. Aunque esta variación es difícil de capturar en detalle completo, se pueden definir algunas categorías aproximadas. Por ejemplo, algunos tambos fueron construidos antes de que existiera el Imperio inca, y los incas simplemente tomaron el control de ellos. La arquitectura de los tambos preincas se puede dividir en dos categorías básicas. Algunos tambos no se modificaron de ninguna manera y, por lo tanto, presentan un estilo arquitectónico que es claramente preinca. Sin embargo, algunos de estos sitios fueron renovados por los incas, por lo que algunos sitios preincas sí presentan alguna arquitectura inca.

Para los sitios construidos en el período inca, los estilos de arquitectura se pueden dividir en tres categorías básicas. Algunos tambos fueron definitivamente locales en su estilo arquitectónico. Esto sucedió típicamente en lugares donde la cultura local era fuerte y se le permitió continuar. Otros sitios tenían en su mayoría arquitectura inca, pero tenían al menos cierta influencia sutil de las tradiciones locales. Finalmente, algunos tambos solo contienen arquitectura de estilo inca. Debido a la fuerte influencia que la cultura local tiende a tener en estas estructuras, los tambos de estilo inca tienden a existir más en áreas aisladas que en áreas con grandes poblaciones.

La "kancha" fue una característica arquitectónica que se encuentra en muchos tambos en todo el Imperio inca. La "kancha" consiste en una gran estructura con paredes rectangulares, que alberga una serie de estructuras más pequeñas de una habitación en su interior. La decisión de construir estructuras más pequeñas en el interior parece estar relacionada con el ambiente frío y lluvioso de las tierras altas andinas. Hyslop señala que la "kancha" estaba presente en estructuras incas que iban desde el gran Coricancha en Cuzco hasta el tambo más pequeño y remoto a lo largo del sistema de carreteras incas. Por lo tanto, las "kanchas" no solo estaban presentes en los tambos, sino que estaban presentes en una variedad de edificios incas. Los historiadores piensan que las "kanchas" se usaban típicamente como instalaciones para vivir, lo que refleja el propósito de los tambos de alojar a personas o grupos que viajan.

La piedra era el material más importante para construir las estructuras de los incas, pero también tenía otro gran significado. La piedra fue muy importante en la historia de la creación de los incas. Dentro de la piedra vivía el espíritu o poder que tenía la capacidad de convertirse en humano o viceversa. Por esta razón los incas adoraban las piedras y apreciaban la sustancia actual en vez de lo que se podría construir con piedras. Por ejemplo, "huacas" o piedras sagradas aparecen en la historia de la creación. Cuando todos los hermanos de Manco Cápac se convirtieron en piedras, los restos eran considerados como huacas. Aya Auca, el tercer hermano de Capac fue renombrado Cuzco Huaca y fue él, el que cuidaba el campo de Cuzco. También, durante la guerra contra los enemigos de los incas, conocidos como "chanca", uno de los gobernadores más poderosos del imperio, Pachacútec, rezó a los dioses, y las piedras se transformaron en una fuerza de soldados y que derrotaron a los chanca.

Este respeto por la piedra y sus poderes dio lugar a su dominio y pericia con la albañilería. Usaban piedras de tamaños inusuales y las pegaban sin ninguna argamasa para hacer paredes. Las piedras estaban tan bien situadas que una hoja de papel no se podía poner entre estas. La superficie era tallada lisa y sin ángulos rectos para que parecieran que estaban vivas. Los tambos eran construidos aprovechando las abertura de las montañas, los cuales se contenían y edificaban con piedra y tenían techo para proteger de las inclemencias climáticas, debido a su ubicación generalmente alrededor de la cordillera de los Andes. 

Las dos técnicas más conocidas usadas para los tambos son:

El Imperio inca tenía unos 10 a 12 millones de habitantes en el siglo XV, ocupó gran parte andina de la costa occidental y para poder controlar todo el territorio, hicieron una red de caminos a los pies de los cerros para que los chasquis pudieran llevar noticias, mensajes, encomiendas, pescado y frutas a todas las poblaciones del Imperio inca. Esta red de caminos se llamaba "Qhapaq ñan" o "Capac ñan" en quechua que significa gran camino y también lo llamaban "Inka ñan" o camino inca. 

Cuando llegaron los conquistadores españoles ya había 16 km aproximadamente de caminos empedrados. Este sendero llegó hasta el valle del Mapocho y una parte es hoy la avenida Independencia en Santiago de Chile. La arteria principal es de 5200 km y la red secundaria penetraba por varios caminos transversales que incluso llegaban hasta las selvas y el Gran Chaco la cual llegaba hasta los 5 km de altura en la cordillera de los Andes.

Los incas, que se basaron en los mensajeros mochicas y chimúes (culturas del antiguo Perú surgida en la costa norte huari entre los años 1000 y 1200) para así crear a los chasquis que significa "“el que recibe”" o "“dar, recibir algo”". 

Cada pueblo contaba con chasquis de entre 18 y 25 años, sirviendo los turnos diarios de 6 a 12 h en las postas que les eran asignadas. El peruano Luis Millones Santa Gadea, en su obra sobre los chasquis los describe con una túnica o camisa y con ojotas. Solo llevaban un sonoro caracol, un penacho de plumas blancas en la cabeza para ser visto de lejos y un bastón labrado. 

Había dos técnicas para llevar un mensaje. Una eran los quipus, una serie de cuerdas de colores y anudadas que servían para la administración. Recientemente algunos investigadores dicen que el color y la ubicación de los nudos pueden significar frases no solo cifras. La otra técnica era la palabra, donde el chasqui se pasaba el mensaje repitiéndolo varias veces, en voz alta cuando estaba llegando o corrían juntos un tramo hasta que el otro chasqui lo recordaba.

Los estudiosos como Craig Morris señalan que, después del colapso del Imperio inca, las personas que vivían en el territorio del antiguo imperio dejaron de usar los tambos. A partir de esto, Morris sugiere que el sistema de los tambos era parte de un "urbanismo artificial" creado por el Imperio inca. Por lo tanto, los tambos habían sido menos útiles para las personas que vivían a lo largo del Imperio inca que para los incas mismos. Morris apoya este argumento con declaraciones de que los tambos se posicionaban con frecuencia para el contacto y los viajes interregionales, en lugar de estar ubicados cerca de grandes aldeas locales.

Aunque los indígenas andinos pueden haber dejado de usar tambos después de la caída del Imperio inca, los tambos no quedaron totalmente fuera de uso: los colonizadores españoles comenzaron a utilizar el sistema de tambos. A veces, los españoles utilizaban las estructuras originales de los tambos incas, pero también construían nuevas estructuras a lo largo de las carreteras. A veces los españoles construían un tambo nuevo a lo largo de una carretera que todavía tenía un tambo sobrante del Imperio inca. Los historiadores saben que los españoles extendieron el sistema de tambos más allá de lo que existía en todo el Imperio inca, aumentando la cantidad de territorios cubiertos. El uso por parte de los españoles para el avance de sus tropas y por la economía colonial hizo que los restos actuales de los tambos sean difíciles de distinguir si durante el Incanato eran solo tambos o eran tambos con funciones adicionales administrativas, religiosas o políticas. Esta distinción puede lograrse mediante excavaciones arqueológicas.



</doc>
<doc id="17450" url="https://es.wikipedia.org/wiki?curid=17450" title="Pista de aterrizaje">
Pista de aterrizaje

La pista de aterrizaje o pista de despegue es la superficie de un campo de aviación o de un aeropuerto, así como también de un portaaviones, sobre la cual los aviones toman tierra y frenan o en la que los aviones aceleran hasta alcanzar la velocidad que les permite despegar. En español es más habitual hablar de pista de aterrizaje que de pista de despegue. En inglés existe una única palabra para ambos términos, que es "runway". El piloto y el controlador aéreo utilizan simplemente la expresión «pista» cuando se comunican entre ellos.

La pista de aterrizaje y despegue se dimensiona en función de la aeronave característica. Esta, a su vez, viene determinada por estudios previos sobre el tráfico que va a soportar el aeropuerto, rangos de los orígenes de esos tráficos y otras varias consideraciones que confluyen en la determinación de cual es la aeronave que por su capacidad, autonomía y otras características técnicas resulta ser la ideal o característica. A partir de este dato fundamental, se conoce la "longitud básica de pista" dato ideal determinado por el fabricante y que viene a ser la longitud ideal y segura que necesita esa aeronave en cuestión para operar al nivel del mar y con una temperatura ambiente de 15° C.
Es con estos datos con los que esa longitud básica de pista se corrige en función de la altitud de la pista en cuestión y de la temperatura media del lugar. Cuanto más altitud y/o mayor temperatura, más debe incrementarse la longitud básica que a la postre será la longitud de proyecto.

Los grandes aeropuertos, donde la demanda es muy elevada, disponen de varias pistas. Los grandes aviones, con plena carga de combustible y de pasajeros, como el Boeing 747 o el Airbus 340 requieren de pistas de al menos 2,5 km para despegar y para aterrizar de forma segura. Por el contrario, aviones de pasajeros pequeños necesitan pistas que no superan un kilómetro. En el caso de las bases aéreas militares sucede lo mismo.

Excepcionalmente, en el caso de los portaviones la pista de aterrizaje es distinta de la pista de despegue. El motivo es que deben poder utilizarse ambas pistas simultáneamente. Su pista de despegue es muy corta, de unos 100 metros, de forma que los aviones deben ser acelerados en pocos segundos de 0 a 200 km/h mediante catapultas para poder despegar. La pista de aterrizaje es algo más larga, de unos 200 metros, longitud que obliga a utilizar cables de frenado para que los aviones puedan aterrizar. Sin embargo, debe observarse que en el caso de un portaviones, las operaciones se realizan con el barco navegando a máxima velocidad en contra del viento, si lo hay, por lo cual el avión se ve beneficiado con un viento frontal virtual que puede ser por lo menos de 25 nudos, por lo que los requerimientos de longitud de pista se ven disminuidos. Si hay un viento de veinte nudos, este se sumará a la velocidad del navío, o sea, que el avión, aparcado antes de ser catapultado para despegar, puede ya estar gozando de 45 nudos de viento en cara. Si se permite el símil, un portaviones es un aeropuerto con viento de proa incorporado.

La pista de aterrizaje y despegue puede tener solamente unos pocos grados de inclinación, ya que una pendiente mayor afectaría a la velocidad de los aviones al despegar y aterrizar.

Los aviones requieren cierta velocidad para poder sustentarse en el aire. Al tener vientos frontales, los aviones requieren menos velocidad relativa para poder volar. Por otra parte, los aviones tienen grandes dificultades para despegar y aterrizar con vientos laterales (conocidos técnicamente como vientos cruzados). Con base en esto, las pistas de un aeropuerto por lo general se construyen de tal manera que, durante un año, los vientos cruzados no superen el 5 % del tiempo los valores admisibles para la aeronave de diseño. En la medida en que los vientos varían sustancialmente de año a año, se requieren series históricas de al menos 10 años para establecer si la orientación es adecuada.

En aeropuertos importantes, las pistas están hechas generalmente en un pavimento de asfalto u hormigón. El grosor de la base de la pista depende del tipo y tamaño de los aviones que la utilizarán y de la composición de la demanda. Así por ejemplo, las pistas destinadas a los grandes aviones requieren una base extremadamente gruesa (entre 15 y 51 cm aproximadamente) resistente para soportar el peso elevado de tales aparatos. Sin embargo, los campos de aterrizaje de poca envergadura, de ciudades pequeñas, a menudo son de tierra, césped, afirmado, hierba o gravilla.

El paquete estructural de un pavimento rígido para pistas de aeródromos se compone de una base de hormigón simple (o armado, en las intersecciones con otras pistas y salidas) que distribuye las tensiones al suelo natural compactado (o subrasante). Si la masa de la aeronave de diseño superare las 100 000 libras, se incluye una capa intermedia de material estabilizado que distribuye uniformemente la carga de la losa y reduce los daños por congelamiento y deshielo.

Para aquellos aeropuertos pequeños de campos de aterrizaje de poca longitud y situados en pequeñas ciudades, fincas o pueblos, estas pueden ser de tierra, césped, grava, hierba entre otros.

En estas pistas pueden aterrizar aeronaves que poseen un tren de aterrizaje triciclo de alta resistencia como:


Cada pista es denominada con dos números, uno para cada una de las dos direcciones y eventualmente una letra. Esto permite que los pilotos puedan identificar fácilmente la pista y el lado de esta que deben utilizar. El número significa la dirección en grados (redondeado a la decena más cercana y recortado en el último dígito) con respecto al norte magnético a la que se encuentra dirigida la pista (en una cabecera) y respectivamente la cabecera opuesta, estará denominada con el ángulo de complemento (dirección contraria, es decir 180° de diferencia).

Si, por ejemplo, una pista tiene en una dirección la denominación 04, su identificación en la dirección opuesta será 22. Estos números están pintados en caracteres muy grandes, en blanco, sobre la superficie de la pista en sus dos extremos, de forma que puedan ser reconocidos por los pilotos desde el aire a cierta distancia. Si un aeropuerto dispone de dos pistas que transcurren paralelamente, y que por ello están identificadas con el mismo número, se añade a continuación del número una R (del inglés "Right") en la pista derecha, y una L (de "Left") en la pista izquierda. En tal caso, las dos pistas podrían tener, por ejemplo, los identificativos 07R y 07L. Si el aeropuerto dispone incluso de una tercera pista paralela a las otras dos, la denominación de la pista del centro será en este ejemplo 07C (de "Center"). La dirección de la pista es indicada en grados magnéticos, eliminando la última cifra. Una pista cuya dirección es, por ejemplo, hacia el este, o sea 90 grados, tendrá por lo tanto como denominación 09, y una pista cuya dirección es hacia el suroeste, o sea 225 grados, se identificará como 22.

Un caso especial como es el del Aeropuerto Internacional de Dallas-Fort Worth, ya que de sus siete pistas de aterrizaje y despegue (uno de los aeropuertos con más pistas del mundo), cinco son paralelas, necesita utilizar un procedimiento un poco distinto. A las dos pistas situadas al oeste se les da la denominación de 36L y 36R, y a las tres restantes se les resta un número quedando en 35L, 35C y 35R. No es del todo correcto, pero es la mejor manera de determinarlas.


La letra hace referencia a la posición de la pista con respecto hacia las demás que mantengan un mismo sentido. Existen 3 letras: L ("Left", izquierda), R ("Right", derecha) y C (central). En el caso de que existan más de tres pistas paralelas, la W se aplica a la que esté a la izquierda de la L. De igual manera, si un aeropuerto no tiene pistas paralelas, basta con poner solo la numeración, sin añadir letras.

Ejemplo: En el mismo aeropuerto del ejemplo anterior, hay dos pistas que se encuentran hacia el mismo sentido, y son la 05L ("Left", 05 izquierda), y la 05R ("Right", 05 derecha), análogamente las del otro lado son la 23L (el otro extremo de la 05R) y la 23R (el otro extremo de la 05L).

Las pistas de aterrizaje y despegue disponen de una señalización blanca pintada sobre la superficie de la pista, cuyo objetivo es informar a los pilotos al despegar, y sobre todo al aterrizar, sobre los diversos tramos y distancias de la pista, así como sobre su eje longitudinal central, para facilitarles las maniobras.

Para los despegues y aterrizajes nocturnos y en condiciones de visibilidad reducida, como en el caso de niebla, las pistas están iluminadas mediante luces que señalizan sus lados, el eje longitudinal central, los diversos tramos de la pista, así como su comienzo y su final. Para los aterrizajes en dichas condiciones las pistas de cierta importancia disponen de balizas de aterrizaje que se instalan en una longitud de varios centenares de metros por delante de la pista, y que constan de focos montados en un orden determinado.

Indicadores de Pendiente de Aproximación Visual (VASI)

Una importante ayuda visual a la aproximación final hacia la cabecera de la pista son los Indicadores de Pendiente de Aproximación Visual, que aporta una mayor certeza en la aproximación en conjunto con los sistemas de ayuda de aproximaciones visuales e instrumentales. Los VASI están instalados normalmente en lugares donde una o más de las siguientes condiciones existen:






Luces indicadoras del fin de la pista (REIL)

Algunas veces las luces están ubicadas al final de la pista para ayudar a la rápida y efectiva identificación del acercamiento del fin de la pista. Cuando se está en la segunda mitad de la pista, las luces blancas de eje y de borde se convierten en una hilera que alterna una bombilla blanca y una roja. En el último tramo de pista solo hay bombillas rojas. De esta manera el piloto puede identificar adecuadamente el final de pista sin posibilidad de confusión. También se suele incorporar un sistema que consiste en dos series de luces que sincronizadamente emiten flash (llamadas luces estroboscópicas), una de cada lado del último tramo de la pista. Sin embargo, estas no se suelen instalar, pues el sistema de luces estroboscópicas están incorporadas al Sistema de Luces de Aproximación. El sistema REIL es usado para distinguir la cabecera de la pista en lugares caracterizados por numerosas luces de suelo, como señales de neón u otras luces que pueden distraer la atención del piloto.

Sistemas de Luces de Aproximación (ALS)

Los Sistemas de Luces de Aproximación son usados en las cercanías de la cabecera de la pista como parte a las ayudas electrónicas de navegación para la parte final de aproximaciones precisas y no precisas de un vuelo IFR; y también como una guía visual en vuelos VFR nocturnos. El sistema de luces de aproximación suministra al piloto con entradas visuales respecto a la alineación de la aeronave, el equilibrio, el horizonte, el ancho y la posición con respecto a la cabecera de la pista. Desde que los sistemas de iluminación aeroportuarios relevaron a las necesariamente rápidas acciones mentales sobre la información visual que encabezaban las decisiones, un sistema visual es ideal para una guía durante los últimos segundos críticos del movimiento descendente sobre el patrón de planeo.
El sistema de luces de aproximación se creó con base en el ángulo del patrón de planeo, el rango visual, el ángulo de visibilidad cortada en la cabina y de las velocidades de aterrizaje. Esto es esencial para que los pilotos estén propensos a utilizar e identificar ALS y de interpretar el sistema sin confusión.
Entre los principales sistemas de ALS se encuentran:


Sistema PAPI

El sistema PAPI consiste en una barra transversal de cuatro luces rojas o blancas situadas, normalmente, en el lado izquierdo de la pista. Si el avión va muy alto sobre la senda de planeo verá todas las luces blancas; si va un poco alto verá tres luces blancas y una roja; si va muy bajo, las verá todas rojas; si solo va un poco bajo verá tres rojas y una blanca, y si va en la senda correcta, verá dos blancas y dos rojas.

Iluminación del acotamiento central y de la zona de contacto de la pista

Estos sistemas de iluminación facilitan los aterrizajes, los giros y los despegues. Las luces de la zona de contacto son más que nada utilizadas para los aterrizajes, y las luces de centro de la pista ayudan después del contacto y brindan la guía primaria durante la carrera de despegue. Ambos sistemas son utilizados como complemento a las ayudas de aproximación electrónicas y ALS bajo condiciones de visibilidad limitada.

Las Luces del Centro de la Pista están unidas casi a la misma altura del pavimento y sobrepuestas sobre un máximo de dos pies para dejar libre la pintura del acotamiento. Las luces del centro son blancas a excepción de los últimos 3000 pies. De los 3000 a los 1000 pies de la pista, las luces se alternan en rojo y blanco.




</doc>
<doc id="17451" url="https://es.wikipedia.org/wiki?curid=17451" title="Hildegarda de Bingen">
Hildegarda de Bingen

Santa Hildegarda de Bingen O.S.B. (en alemán: "Hildegard von Bingen"; Bermersheim vor der Höhe, Sacro Imperio Romano Germánico, 16 de septiembre de 1098-Monasterio de Rupertsberg, 17 de septiembre de 1179) fue una santa, compositora, escritora, filósofa, científica, naturalista, médica, polímata, abadesa, mística, líder monacal y profetisa alemana. Conocida también como la sibila del Rin y la profetisa teutónica.

Considerada una de las personalidades más influyentes, polifacéticas y fascinantes de la Baja Edad Media y de la historia de Occidente, es también de las figuras más ilustres del monacato femenino y quizás quien mejor ejemplificó el ideal benedictino, al estar dotada de una inteligencia y cultura fuera de lo común, comprometida con la reforma gregoriana y al ser una de las escritoras de mayor producción de su tiempo. Además es considerada por muchos expertos como la madre de la historia natural.

Aunque la historia de su canonización es compleja, diversas ramas de la Iglesia la han reconocido como santa durante siglos; el 7 de octubre de 2012, durante la misa de apertura de la Asamblea general ordinaria del sínodo de los obispos, el papa Benedicto le otorgó el título de doctora de la Iglesia junto a san Juan de Ávila. En palabras de la filóloga Victoria Cirlot:

Hildegarda nació en Bermersheim, en el valle del Rin (actualmente Renania-Palatinado, Alemania), durante el verano del año 1098, en el seno de una familia noble alemana acomodada. Fue la menor de los diez hijos de Hildeberto de Bermersheim, caballero al servicio de Meginhard, conde de Spanheim, y de su esposa, Matilde de Merxheim-Nahet, y por eso fue considerada como el diezmo para Dios, entregada como oblata y consagrada desde su nacimiento a la actividad religiosa, según la mentalidad medieval. De esta manera, fue dedicada por sus padres a la vida religiosa y entregada para su educación a la condesa Judith de Spanheim ("Jutta"), hija del conde Esteban II de Spanheim y, por tanto, noble como ella, quien la instruyó en el rezo del salterio, en la lectura del latín —aunque no le enseñó a escribirlo o, cuando menos, no con pericia—, en la lectura de la Sagrada Escritura y en el canto gregoriano.

Durante algunos años maestra y discípula vivieron en el castillo de Spanheim. Cuando Hildegarda cumplió catorce años, ambas se enclaustraron en el monasterio de Disibodenberg. Este monasterio era masculino, pero acogió a un pequeño grupo de enclaustradas en una celda anexa, bajo la dirección de Judith. La ceremonia de clausura solemne fue celebrada el 1 de noviembre de 1112 y en ella participaron Hildegarda, Judith y otra enclaustrada más, también infante. En 1114, la celda se transformó en un pequeño monasterio, a fin de poder albergar el creciente número de vocaciones. En ese mismo año, Hildegarda emitió la profesión religiosa bajo la regla benedictina, recibiendo el velo de manos del obispo Otón de Bamberg. De esta manera continuó su educación monástica rudimentaria dirigida por Judith. 

Judith murió en 1136, con fama de santidad tras haber llevado una vida de mucha austeridad y ascesis, que incluía largos ayunos y penitencias corporales. Hildegarda, a pesar de su juventud, fue elegida abadesa ("magistra") de manera unánime por la comunidad de monjas.

Desde niña, Hildegarda tuvo débil constitución física, sufría de constantes enfermedades y experimentaba visiones. En una hagiografía posterior escrita por el monje Teoderico de Echternach se consignó el testimonio de la propia Hildegarda, donde dejó constancia que desde los tres años tuvo la visión de «"una luz tal que mi alma temblaba"». Estos hechos continuaron aún durante los años en que estuvo bajo la instrucción de Judith quien, al parecer, tuvo conocimiento de ellos. Vivía estos episodios conscientemente, es decir, sin perder los sentidos ni sufrir éxtasis. Ella los describió como una gran luz en la que se presentaban imágenes, formas y colores; además iban acompañados de una voz que le explicaba lo que veía y, en algunos casos, de música.

En 1141, a la edad de cuarenta y dos años, sobrevino un episodio de visiones más fuerte, durante el cual recibió la orden sobrenatural de escribir las visiones que en adelante tuviese. A partir de entonces, Hildegarda escribió sus experiencias, que dieron como resultado el primer libro, llamado "Scivias" ("Conoce los caminos"), que no concluyó hasta 1151. Para tal fin, tomó como secretario y amanuense a uno de los monjes de Disibodenberg llamado Volmar y, como colaboradora, a una de sus monjas, llamada Ricardis de Stade.

No obstante, siguió teniendo reticencias para hacer públicas sus revelaciones y los textos resultantes de ellos, por lo que para disipar sus dudas recurrió a uno de los hombres más prominentes y con mayor reputación espiritual de su tiempo: Bernardo de Claraval, a quien dirigió una sentida carta pidiéndole consejo sobre la naturaleza de sus visiones y la pertinencia de hacerlas de conocimiento general. En dicha misiva, enviada hacia 1146, confesaba al ilustre monje cisterciense que lo había visto en una visión «"como un hombre que veía directo al sol audaz y sin miedo"», y, al mismo tiempo que se atribuía a sí misma «debilidad», solicitaba su consejo:

La respuesta de Bernardo no fue ni muy extensa ni tan elocuente como la carta enviada por Hildegarda, pero en ella la invitaba a «"reconocer este don como una gracia y a responder a él ansiosamente con humildad y devoción [...]"». Además, parece que el abad de Claraval posteriormente intervino ante el papa Eugenio III en favor de Hildegarda, ya que tenía trato personal con el obispo de Roma porque éste era también cisterciense y antiguo discípulo suyo.

Precisamente, el arzobispo Enrique de Maguncia, bajo cuya jurisdicción se encontraba el monasterio de Disibodenberg, y que estaba enterado de las visiones y profecías de Hildegarda, mandó una comisión al papa Eugenio para informarse de lo sucedido y lograr que se declarara sobre la naturaleza de tales dones. El papa se encontraba por aquellos días en Tréveris para presidir el sínodo que se celebró en aquella ciudad entre 1147 y 1148.

En 1148, un comité de teólogos, encabezado por Albero de Chiny-Namur, obispo de Verdún, estudió y aprobó, a petición del papa, parte del "Scivias". El mismo papa leyó públicamente algunos textos durante el sínodo de Tréveris y declaró que tales visiones eran fruto de la intervención del Espíritu Santo. Tras la aprobación, envió una carta a Hildegarda, pidiéndole que continuase escribiendo sus visiones. Con ello dio comienzo no solo la actividad literaria aprobada canónicamente, sino también la relación epistolar con múltiples personalidades de la época, tanto políticas como eclesiásticas, tales como el ya mencionado Bernardo de Claraval, Federico I Barbarroja, Enrique II de Inglaterra o Leonor de Aquitania, que pedían sus consejos y orientaciones. Tal fue su reconocimiento, que llegó a ser conocida como la "Sibila del Rin".

También en 1148 y sin haber concluido la redacción del "Scivias", una visión la hizo concebir la idea de partir de Disibodenberg y marchar a un lugar «donde no había agua y donde nada era placentero» inspirándola así para la fundación de un monasterio en la colina de san Ruperto ("Rupertsberg"), cerca de Bingen al oeste del río Rin en la desembocadura del Nahe, para trasladar a la crecida comunidad y emanciparla de los monjes de Disibodenberg. 

Sin embargo, Kuno, entonces abad de Disibodenberg, se opuso a su salida, lo que contrarió a la monja en gran medida, al punto de ocasionarle trastornos físicos, que fueron atribuidos a causas divinas: 

Ante esta situación intervino la marquesa Ricardis de Stade ("Richardis von Stade"), madre de la monja que servía de secretaria a Hildegarda, quien logró convencer a Enrique I, arzobispo de Maguncia (1142—1153), de que diera la autorización para la salida de las religiosas y la fundación del nuevo monasterio. Hacia 1150, se trasladó a Rupertsberg con cerca de veinte de sus monjas, obtuvo el permiso del conde Bernardo de Hildesheim, propietario del terreno elegido y fundó el monasterio de Rupertsberg, del cual se convirtió en abadesa.

Por esa época, su asistente y secretaria Ricardis la abandonó para ser abadesa del convento de Bassum en Sajonia. Ello causó la tristeza y oposición de Hildegarda, que luego reflejaría en serias cartas de protesta al arzobispo Hartwig de Bremen, hermano de Ricardis, quien había influido para conseguir el cargo abacial; llegó a apelar hasta al papa, sin conseguir que la monja volviera. Ricardis murió al año de la separación.

Un año después del traslado concluyó el "Scivias" y de esa misma época datan sus dos libros de contenidos sobre ciencias naturales ("Physica") y medicina ("Cause et cure"), en los cuales expuso gran cantidad de conocimientos sobre el funcionamiento del cuerpo humano, de herbología y otros tratamientos médicos de su época basados en las propiedades de piedras y animales. Asimismo, comenzó la colección de cantos que tituló "Symphonia armonie celestium revelationum", que compuso para atender a las necesidades litúrgicas de su comunidad. Según algunas cronologías, también de 1150 dataría el inicio del "Liber vite meritorum".

Hacia 1163, como fruto de sus constantes visiones, empezó a redactar el "Liber divinorum operum", la tercera de sus tres obras más importantes y que tardaría alrededor de diez años en concluir. Sin embargo, la abadesa alternó la vida contemplativa y de escritora con la de predicación y fundación, ya que en 1165 fundó un segundo monasterio en Eibingen, que visitaba regularmente dos veces a la semana.

La fama de santa y profetisa que llegó a tener la abadesa fue tal que, en 1150, el propio emperador Federico I Barbarroja la invitó a entrevistarse con él en su palacio en Ingelheim. El aprecio mutuo que generó esta entrevista, manifestado en las subsecuentes cartas, llegó a tal grado que, trece años más tarde, el soberano otorgó un edicto de protección imperial a perpetuidad al monasterio de Rupertsberg.

La labor de escritora de Hildegarda se vio interrumpida muchas veces por los viajes de predicación. Si bien la clausura en sus tiempos no era tan rígida como lo sería a partir de Bonifacio VIII, no dejó de sorprender y admirar a sus contemporáneos que una abadesa abandonara su monasterio para predicar.

El contenido de su predicación giraba en torno a la redención, la conversión y la reforma del clero, criticando duramente la corrupción eclesiástica, además de oponerse firmemente a los cátaros; al condenar las doctrinas de estos, proponía el combate de sus errores mediante la predicación y la edificación del clero.

En total fueron cuatro los viajes de predicación que realizó: el primero entre 1158 y 1159, en el que viajó a Maguncia y a Wurzburgo. En 1160 realizó el segundo a Tréveris y a Metz. En su tercera predicación, entre 1161 y 1163, viajó por el Rin hasta Colonia. En el último de sus viajes, comprendido entre 1170 y 1171, predicó en la región de Suabia.

Además de estos viajes de predicación, Hildegarda usó las cartas para hacer sentir su opinión ante personajes notables. Con motivo del cisma provocado por la elección del antipapa Víctor IV con el apoyo del emperador Barbarroja, frente al papa romano Alejandro III, alargado a la muerte de Víctor IV con la elección de los también antipapas Pascual III y Calixto III, Hildegarda hizo graves amonestaciones proféticas al primero de estos, así como al emperador mismo.

En el año 1173, poco antes de concluir el "Liber divinorum operum", murió el monje Volmar, su más cercano colaborador y secretario, lo que la orilló a ayudarse de los monjes de la abadía de san Eucario de Tréveris para terminar dicha obra. Durante algún tiempo el monje Godofredo de Disibodenberg le sirvió como amanuense, a la vez que comenzó la redacción de una biografía de la profetisa, pero también él murió poco tiempo después, en 1176. El último de sus secretarios lo encontró en Guiberto de Gembloux, un monje flamenco, con el que había sostenido conversación epistolar iniciada por el interés de éste sobre la manera en que Hildegarda tenía sus visiones.

La última situación crítica a la que tuvo que enfrentarse Hildegarda aconteció en 1178, cuando su comunidad dio sepultura en el cementerio conventual a un noble supuestamente excomulgado. Por la imposición de esta pena eclesiástica, el derecho canónico prohibía su entierro en suelo sagrado. Se pidió a Hildegarda que exhumara el cadáver. Ella se negó e incluso hizo desaparecer cualquier rastro del enterramiento para que nadie pudiera buscarlo. Sostuvo que había sido reconciliado con la Iglesia antes de morir. Los prelados de Maguncia, en ausencia del arzobispo Christian, que estaba en Roma, pusieron en entredicho al monasterio. Por él se prohibió el uso de las campanas, los instrumentos y los cantos en la vida y liturgia de Rupertsberg. Hildegarda se defendió escribiendo una carta de rico contenido doctrinal, donde recogía el significado teológico de la música. Cuando regresó el arzobispo en marzo de 1179, se presentaron testigos que apoyaban la versión de Hildegarda y fue levantado el entredicho.

A los pocos meses de ser levantado el entredicho, el 17 de septiembre de 1179, a los 81 años de edad murió Hildegarda. Las crónicas hagiográficas cuentan que a la hora de su muerte aparecieron dos arcos muy brillantes y de diferentes colores que formaban una cruz en el cielo. 

Entre 1180 y 1190 el monje Teoderico de Echternach escribió la "Vita" ("Vida") de Hildegarda, recogiendo pasajes autobiográficos que la monja había dejado y contado. Gregorio IX abrió el proceso de canonización en 1227, aunque no se concluyó. Fue reabierto por Inocencio IV en 1244, sin que tampoco en esta ocasión se llegase a concluir. Sin embargo, debido a la difusión de su culto se la inscribió en el Martirologio romano, incluyéndose además su nombre en algunas letanías; se extrajeron reliquias de su sepulcro; se celebró su fiesta litúrgica; se le atribuyeron milagros y sus representaciones pictóricas y escultóricas comenzaron a ser objeto de veneración.

Sus reliquias fueron conservadas en el convento de Rupertsberg hasta la destrucción de éste en 1632 durante la Guerra de los Treinta Años. Entonces fueron llevadas a Colonia y después a Ebingen, donde se depositaron en la iglesia parroquial donde aún reposan.

En 1940 se aprobó oficialmente su celebración para las iglesias locales. Con motivo del 800 aniversario de su muerte, Juan Pablo II se refirió a ella como profetisa y santa. De la misma manera, en 2006, el papa Benedicto XVI también se refirió a Hildegarda como santa y la encomió como una de las grandes mujeres de la cristiandad junto con Catalina de Siena, Teresa de Ávila y la madre Teresa de Calcuta.

En el año 2010 el papa Benedicto XVI dedicó a Hildegarda las Audiencias Generales del 1 y 8 de septiembre, dentro del marco de una serie de catequesis sobre escritores cristianos, siendo la primera mujer presentada en estas catequesis; recordó, entre otras cosas, que los contemporáneos de Hildegarda la consideraron con el título de ""profetisa teutónica"" y puntualizó el valor teológico de sus escritos y enseñanzas.

En diciembre de 2011, el papa Benedicto XVI anunció su decisión de otorgar a santa Hildegarda el título de "Doctora de la Iglesia". El 10 de mayo de 2012 procedió a inscribirla en el catálogo de los santos y extender su culto litúrgico a la Iglesia universal, en una "canonización equivalente". El 27 de mayo de 2012, durante el rezo del Regina Caeli del día de Pentecostés, el papa determinó la fecha para la proclamación como Doctora. El 7 de octubre de 2012, durante la misa de apertura del Sínodo de los obispos en la Basílica de San Pedro en Roma, se realizó la proclamación oficial por el cual se le concedió el título de Doctora para la Iglesia Universal junto con san Juan de Ávila por el papa Benedicto XVI.

Hildegarda también es venerada por algunas de las Iglesias que conforman la Comunión anglicana, entre ellas la Iglesia de Inglaterra y la Iglesia episcopal escocesa. Tanto en la Iglesia católica como en la Comunión anglicana se la celebra el 17 de septiembre.

La iconografía religiosa de Hildegarda es escasa, probablemente porque su culto fue local por bastante tiempo. Se la retrata con los atributos propios de una abadesa de la orden de san Benito: báculo abacial y hábito benedictino con velo negro y blanco; sus representaciones más antiguas reproducen la manera en que aparece en las miniaturas de sus escritos: sentada con un estilo en la mano en actitud de escribir sobre un par de tablillas o dictando a un monje, con cinco flamas alrededor de la cabeza representando la visión divina. Más tarde se cambia el estilo por una pluma de ave, con algún pergamino o libro en la mano — comúnmente el "Scivias" — y algún instrumento musical.

Las obras de esta religiosa del siglo XII fueron escritas —como la mayor parte de los escritos de su tiempo— en latín medieval, salvo ciertas anotaciones y palabras que podemos encontrar en algunas de sus cartas y principalmente en sus obras relativas a la "Lingua ignota", que figuran en alemán medieval propio de la región media de Franconia–Renania/Mosela. En su obra, ella misma acusó en variadas ocasiones su poca preparación en latín, pero por sus propias confesiones y sus hagiógrafos se conoce que su método de escritura comenzaba al escribir sus visiones y luego pasarlas a un secretario que corregía los errores y pulía la escritura. Dos de ellos — Volmar y Gottfried — fueron monjes de Rupertsberg y el tercero, de origen flamenco — Guibert de Gembloux — era monje de la abadía de Gembloux, de ahí que todos ellos estuvieran bien preparados en el latín eclesiástico.

Empleó varios estilos de escritura: el tratado teológico, el epistolar, el hagiográfico y el tratado médico; pero destacan sus obras visionarias, en las que hace un uso constante y fecundo de la alegoría ética-religiosa, que aunque era bastante común en su tiempo, llegaba a usar símbolos poco frecuentes.

En lo referente a las influencias recibidas y a su manera de escribir, indudablemente se destacan las Sagradas Escrituras a través de la "Vulgata", con especial atención hacia los profetas y el Nuevo Testamento; en este último se destacan la importancia que el Evangelio de san Juan y el Apocalipsis tuvieron en ella, ya que incluso en algunas narraciones autobiográficas consignadas en la "Vita" llegó a comparar sus dones espirituales con las inspiraciones del evangelista Juan sumado al tono apocalíptico de las partes finales del "Scivias". 

Igualmente se le atribuyen conocimientos de algunas obras de la patrística latina, entre las cuales se ha detectado la influencia de san Agustín y san Isidoro de Sevilla; se ha señalado especialmente la influencia y similitud con el "Pastor de Hermas" y Boecio como fuentes de la identificación alegórica como mujeres que Hildegarda hace de la Iglesia y de algunas virtudes en el "Scivias". Además, pese a que la abadesa se calificara a sí misma de «indocta», se ha detectado en sus obras un gran bagaje cultural clásico proveniente de Cicerón, Lucano y Séneca; con Galeno coincide en algunas teorías médicas sobre los humores; en el "Scivias" y el "Ordo virtutum" representa la lucha constante de las virtudes contra los vicios a través de su personificación como mujeres ataviadas con los atributos correspondientes a la actitud moral que encarnan, combatiendo cada virtud contra el vicio opuesto a ella. Esta tradición alegórica es común a otros escritores del medioevo y puede rastrearse hasta la Psychomachia de Prudencio en el siglo IV.

Sus obras fueron legadas a la posterioridad gracias al interés de los monjes que la admiraron y la ayudaron a escribirlas, encabezados por Guibert de Gembloux, quienes tras su muerte terminaron de transcribir las obras de la abadesa, las compilaron e ilustraron con miniaturas. Entre los manuscritos medievales más importantes que se han conservado, en donde se contienen las obras escritas y musicales de la profetisa teutónica, se encuentran:

El códice de Wiesbaden, conocido en alemán como "«Riesencodex»" ("Códice gigante") por su gran tamaño (46 x 30 cm) y peso (15 kg), es un manuscrito medieval de 481 folios, cuya datación oscila entre los últimos años de vida de Hildegarda y algunos posteriores a su muerte, siendo la fecha más tardía el año 1200. Originalmente, se custodiaba en Rupertsberg, pero su riqueza artística ha llevado a algunos investigadores a dudar de que haya sido creado ahí o en Eibingen.

Cuando el convento de Rupertsberg fue destruido en el siglo XVII, el manuscrito fue trasladado al monasterio de Eibingen junto con las reliquias de la santa. En 1814, fue llevado a la biblioteca de Wiesbaden (actualmente Universidad y Biblioteca Estatal de Rhein-Main). Durante la Segunda Guerra Mundial, el manuscrito original fue casi destruido, pero su contenido se conservó gracias a fotocopias y facsímiles extraídos durante las primeras décadas del siglo XX.

Contiene una versión de sus tres principales obras místicas: "Scivias", "Liber vite meritorum" y "Liber divinorum operum". También es la fuente de todas sus composiciones musicales, sus obras acerca de la "Lengua ignota", trabajos hagiográficos ("Vita sancti Ruperti"), algunas cartas, homilías y la "Vita" escrita por el monje Theoderic, por lo que es la fuente más numerosa e importante del trabajo de la monja medieval. Contiene las ilustraciones de las visiones descritas por la abadesa, inspiradas en las que ilustraban los manuscritos originales.


De las obras religiosas que escribió Hildegarda, destacan tres de carácter teológico: "Scivias", sobre teología dogmática; "Liber vite meritorum", sobre teología moral; y "Liber divinorum operum", sobre cosmología, antropología y teodicea. Esta trilogía forma el mayor corpus de las obras y pensamiento de la visionaria del Rin.

El nombre "Scivias" es una forma abreviada del latín "«Scito vias Domini»" que significa «"Conoce los caminos del Señor"». Esta obra fue inspirada tras una visión tenida por Hildegarda a la edad de cuarenta y dos años, esto es, hacia 1141, en la cual aseguraba haber asistido a una teofanía que le ordenaba escribir lo que percibiera:

Dividida en tres libros, en esta obra describe las veintiséis visiones que tuvo, las cuales se encuentran ilustradas en los manuscritos conservados, sirviendo de alegoría y medio de explicación de los principales dogmas del catolicismo y la Iglesia de una manera más o menos sistemática. Tras la descripción de cada visión cargada de un complicado simbolismo, la voz celestial pasa a explicar su significado. De esta manera recorre los temas de «"la majestad divina, la Trinidad, la Creación, la caída de Lucifer y Adán, las etapas de la historia de la salvación, la Iglesia y los sacramentos, el Juicio Final y el mundo futuro"».

El "Libro de los méritos de la vida", cuyo título completo es "Liber vite meritorum, per simplicem hominem a vivente lucem revelatorum", fue escrito entre 1158 y 1163. Es una obra de carácter moral en la que, partiendo de la visión de Dios como un hombre cósmico que sustenta y vivifica al universo, Hildegarda llega a una exposición de los principales vicios espirituales y sus virtudes opuestas. Esta sistematización hace corresponder aspectos naturales del mundo y del hombre con las pasiones del alma humana. Dicha visión está explicada a lo largo de cinco libros y se complementa con un sexto que detalla la descripción de las penas que en la otra vida corresponderán a cada vicio. De esta manera el "Liber vite meritorum" deviene en un catálogo de treinta y cinco vicios, descritos bajo la figura simbólica de seres alegóricos conformados de partes de bestias y humanos.

El "Liber divinorum operum" o "Libro de las obras divinas" fue creado entre 1163 y 1173, siendo Hildegarda ya sexagenaria. Es la descripción de diez visiones, en donde realiza una cosmología que estructura al universo en correspondencia con la fisiología humana, y que convierte los actos del hombre en paralelos a los actos de Dios, mediante su cooperación activa en la construcción y orden del cosmos.

Así, desarrolla también una explicación del quehacer creador de Dios, centro del universo, que se desenvuelve en el tiempo humano teniendo su manifestación en la naturaleza del mundo y en la historia, con su máxima expresión en la encarnación de Cristo, Verbo divino.

Otra de sus principales obras es la creación de su "Lingua ignota", primera lengua artificial de la historia, por la que fue nombrada patrona de los esperantistas.

Dicha lengua fue expuesta en su escrito "Ignota Lingua per simplicem hominem Hildegardem prolata", que ha llegado a nosotros integrada con otras obras en el "Riesencodex", en sus folios 461–464, así como en el de Berlín, folios 57–62. La obra es un glosario de 109 palabras escritas en dicha lengua con su significado en alemán, incluyendo el de algunas plantas y términos usados en sus obras médicas.

En ambos manuscritos también se encuentra una pequeña obra conocida como "Littere ignote" ("Letras desconocidas") en la que presenta 23 nuevas letras constituyendo un alfabeto hasta entonces desconocido, que si bien tienen cierta semejanza con los rasgos del alfabeto griego y hebreo, no se considera que Hildegarda haya intentado emularlos.

Se ha propuesto que su creación fue de carácter místico, tal vez una especie de glosolalia. No obstante, muchas de las palabras de dicho lenguaje parecen tender hacia un interés científico. Pero no hay un motivo claro del porqué de su creación.

Además escribió obras de carácter científico: "Liber simplicis medicine" o "Physica", es una obra sobre medicina, dividido en nueve libros sobre las correspondientes propiedades curativas de plantas, elementos, árboles, piedras, peces, aves, animales, reptiles y metales. El más amplio de tales capítulos es el primero dedicado a las plantas, lo que indica que Hildegard tenía amplio conocimiento en su aplicación terapéutica desde una perspectiva holística. En este libro aplica la difundida teoría médica medieval de los humores que relaciona con la idea de que la constitución de los seres a partir del plan divino se realiza a través de cuatro elementos constitutivos, cuyo equilibrio determina la salud o enfermedad del individuo. Así, a cada planta le otorga el correspondiente calificativo de su cualidad: "robustus", "siccus", "calidus", "aridus", "humidus", etcétera.

El "Liber composite medicine" o "Cause et cure", sobre el origen de las enfermedades y su tratamiento.

Se ha comprobado la autoría de alrededor de 300 cartas, donde toca temas de lo más variado: teología, espiritualidad, política, remedios curativos, consejos sobre la vida monástica y clerical, entre otros temas que le consultaban. El estilo en sus cartas es, en ocasiones, igual de simbólico que en sus escritos visionarios, ya que llega a proporcionar consejos con la misma autoridad y en nombre de la voz divina que dictaba sus visiones.

En lo que se refiere a sus escritos hagiográficos, se encuentra la "Vita sancti Disibodi" ("Vida de san Disibodo") escrita hacia 1170 a petición de Helenger, abad del monasterio de Disibodenberg, donde trata la vida y obra del eremita irlandés Disibodo que terminó su vida en las cercanías del monasterio que aquel presidía. Por las mismas fechas escribe la "Vita sancti Ruperti" para documentar la vida del santo patrón del monasterio fundado en la colina donde supuestamente descansaban las reliquias de Ruperto de Bingen.

Escribió, además, una explicación de la regla de san Benito ("Explanatio regule s. Benedicti") y otra del Símbolo atanasiano ("Explanatio symboli s. Athanasii").

Lo prolífico de la obra musical de Hildegarda permite establecer la importancia que para la sibila del Rin tuvieron la música y el canto. Tal importancia se puso de manifiesto en la carta escrita a la curia de Maguncia, dictada tras el entredicho interpuesto con ocasión del conflicto derivado de que la abadesa diera sepultura a un hombre supuestamente excomulgado y por el cual se prohibió a su comunidad cantar el salterio y tener misa.

En dicha misiva, tras declararse dispuesta a obedecer las medidas impuestas y partiendo de una cita del salmo 150, Hildegarda explica que el canto es una manifestación del espíritu divino en el hombre, que con ello recuerda vagamente la bienaventuranza de Adán en el paraíso, quien participaba de la voz y el canto de los ángeles en alabanza a Dios. Los profetas, a quienes Dios les otorgaba una gracia extraordinaria habían compuesto cantos y creado instrumentos entreviendo el pasado beatífico de la humanidad. De hecho, los instrumentos musicales, al ser tocados con los dedos recordaban a Adán mismo creado por el «dedo de Dios».
La alabanza a Dios dentro de la Iglesia tiene su origen en el Espíritu Santo y es conforme a la armonía celeste:

Si bien emplea la técnica monofónica, el melisma y la notación propias de su época, la música hildegardiana se diferencia por el uso de amplios rangos tonales, que exigen a la cantante o al coro subir a agudos intensos estando en una nota intermedia o baja. Contrae frases melódicas que impulsan a la voz a ser más rápida para luego ralentizarse. Usa igualmente intervalos de cuarta y quinta, cuando el canto de su época rara vez pasaba de terceras.

La totalidad de las obras musicales de la profetisa teutona fueron creadas para las necesidades litúrgicas de su propia comunidad, así como para la didáctica teológico-moral en el caso del "Ordo Virtutum".

Hildegarda compuso setenta y ocho obras musicales, agrupadas en "Symphonia armonie celestium revelationum" ("Sinfonía de la armonía de las revelaciones celestes"): 43 antífonas, 18 responsorios, 4 himnos, 7 secuencias, 2 sinfonías (con el significado propio del siglo XII), 1 aleluya, 1 kyrie, 1 pieza libre y 1 oratorio (fascinante, pues el oratorio se inventó en el siglo XVII). Además, compuso un auto sacramental musicalizado llamado "Ordo Virtutum" ("Orden de las virtudes", en latín), sobre las virtudes.

Todo el bagaje simbólico y originalidad de las obras de Hildegarda encuentra su origen en la inspiración sobrenatural de sus experiencias visionarias, de ahí que la explicación de dicha enigmática fuente de conocimiento haya sido causa de interés e investigación incluso durante la vida de la abadesa.

Precisamente, una de las fuentes más importantes sobre el origen y descripción de sus visiones se encuentra en la carta con la que Hildegarda respondía a los cuestionamientos epistolares hechos en 1175 por el flamenco Guibert de Gembloux en nombre de los monjes de la abadía de Villers, acerca de la manera en que tenía sus visiones. Por estas respuestas se sabe que las visiones comenzaron desde su muy temprana infancia y que en ellas no mediaba el sueño, ni el éxtasis, ni la pérdida de los sentidos:

Igualmente, explica que este conocimiento sobrenatural que adquiere se da al mismo tiempo de tener la experiencia, tal como ella misma escribe: «"simultáneamente veo y oigo y sé, y casi en el mismo momento aprendo lo que sé."». 

Tales visiones siempre se acompañaban de manifestaciones lumínicas, de hecho, los mandatos divinos que recibía provenían de una teofanía luminosa a la que nombra «sombra de la luz viviente» ("umbra viventis lucis") y es esta luz a la que nombra en la introducción del "Scivias" y de "Liber divinorum operum" como la que toma voz para ordenarle poner por escrito cuanto experimenta.

Esta luz divina le mostraba las visiones que describe en sus obras y que posteriormente fueron ilustradas, las cuales han llegado hasta nosotros gracias a los manuscritos sobrevivientes, que muestran un simbolismo cuya interpretación no resulta tan obvia. Luego pasa a explicar su significado profundo y las enseñanzas derivadas de tales visiones. Ordinariamente estas visiones venían acompañadas de trastornos físicos para la abadesa como debilidad, dolor y, en algunos casos, rigidez muscular.

Lo anterior ha llevado a algunos estudiosos a buscar causas neurológicas, fisiológicas e incluso psicológicas para las visiones de esta mujer medieval, siendo una de las respuestas médicas más difundida que sufría un cuadro crónico de migraña, teoría esta última propuesta por el historiador de la medicina Charles Singer y popularizada por Oliver Sacks.

El valor teológico de las enseñanzas de Hildegarda ha sido reconocido desde antiguo por la Iglesia católica en una tradición continuada hasta nuestros días. Muestra de ello fue la inclusión de su vida y obras en el famoso compilado histórico de teólogos publicado en 1885 por Jacques Paul Migne, la "Patrologia Latina", que dedica su tomo CXCVII a esta escritora. A ello se aúna su estudio y consideración modernas, de lo que es prueba su mención en declaraciones públicas y homilías de Benedicto XVI, así como su reconocimiento como Doctora de la Iglesia.

Interpretaciones modernas de sus escritos, como las que hacen Barbara Newmann o Sabina Flanagan, han puesto el énfasis en el carácter femenino de la teología hildegardiana, reivindicando un carácter de género a sus enseñanzas.

La concepción hildegardiana de Dios no es diferente de las concepciones teológicas católicas medievales, matizadas por las peculiaridades propias de sus visiones. La Trinidad, en el libro del "Scivias", aparece como una luz en la que, a su vez, se diferencian una «"luz serenísima"» ("splendidissimam lucem"), que figura al Padre, una figura humana color zafiro ("spphirini coloris speciem hominis"), que simbolizaba al Hijo, y un «"suavísimo fuego rutilante"» ("suavissimo rutilantem igne"), como manifestación del Espíritu Santo, imágenes que conservan su diferenciación compartiendo la misma naturaleza única: «"de tal modo que era una única luz en una única fuerza"», «"inseparable en su Divina Majestad"» e «"inviolable sin cambio"».

Dios también se presenta como la fuente de toda fuerza, vida y fecundidad. En el "Liber vite meritorum" es representado como un varón ("vir") precisamente porque en él radica el vigor que comunica a lo existente, no sólo a través del acto de la creación sino incluso a través de la inmanencia de su poder que sostiene al mundo, otorgando fecundidad ("viriditas") a la naturaleza y al espíritu.

Como en la restante cultura teológica medieval, Hildegarda considera al hombre como el centro del mundo creado por Dios y partícipe de la obra redentora. Según el "Liber divinorum operum", el hombre, hecho a semejanza de Dios, posee parecido con otra de las grandes obras del omnipotente: el cosmos. Esta semejanza se refleja incluso a nivel corporal, pues en el cuerpo se pueden distinguir partes aéreas, acuosas, invernales, nubosas, cálidas, etcétera. Hombre y cosmos interactúan y están ordenados conforme al plan divino. Es por ello que el cosmos puede ser leído como una lección para enseñar al hombre a amar a su creador y guardar la debida moral. Tanto uno como otro están destinados a su reintegración final a Dios, pero el hombre con su libre albedrío puede optar por rebelarse. 

La calidad moral del hombre se encuentra herida desde la caída de Adán y Eva a causa del pecado, no obstante, Dios elige esa misma debilidad para otorgar la salvación por medio de su hijo Jesucristo, quien toma carne para rescatar al hombre, quien a su vez debe tender hacia Dios con sus pensamientos y actos, eligiendo las virtudes antes que los vicios.

El Verbo de Dios, hecho carne en la figura de Jesucristo, posee así la doble naturaleza divina y humana, de la misma manera que la Iglesia, los sacramentos y las virtudes poseen las realidades sobrenatural y mundana.

La abadesa del Rin comparte la visión patrística de la Iglesia como nueva Eva salida de la costilla de Cristo, custodia de la salvación en el mundo y prefigurada en la virgen María. Se opone a la Sinagoga, que representa a los enemigos de la fe y de Dios. En las visiones descritas en el "Scivias", la Iglesia es figurada como una «"mujer inmensa como una ciudad"», coronada y vestida con resplandor, con el vientre perforado por donde entran una multitud de hombres con piel obscura que son purificados al salir por su boca. 

Una imagen común en la teología cristiana no es ajena a la eclesiología de Hildegarda, la de los «esponsales de la Iglesia». La Iglesia como esposa mística contrae matrimonio con Cristo a través de su pasión: «"Inundada por la sangre que manaba de su costado, fue unida a él en felices esponsales por la voluntad superior del Padre, y notablemente dotada por su carne y por su sangre"» haciéndose así mediadora de los sacramentos que actualizan la vida de Cristo en el tiempo.

La figura de Hildegarda de Bingen y su obra dejaron sentir su influencia aún fuera de Alemania y llegaron hasta nuestros días con una vigencia indiscutible, que ha llevado al mundo de la cultura a realizar diversos homenajes a la santa alemana.

La iglesia parroquial de Eibingen, donde reposan las reliquias de esta santa, fue reconstruida en gran parte en 1932 tras un incendio, tras lo cual fue adaptada a un estilo más contemporáneo por los hermanos Rummel. El altar principal se encuentra adornado por un mosaico que reproduce la visión de Hildegarda sobre la Trinidad que se encuentra en "Scivias" II, 2, dicha obra fue diseñada en 1965 por el expresionista alemán Ludwig Baur, quien también diseñó los vitrales de las ventanas de la iglesia, los cuales representan igualmente algunas visiones de la abadesa. 

La abadía de santa Hildegarda en Rüdesheim am Rhein es una abadía benedictina reconstruida entre 1900 y 1908 sobre las ruinas originales de una de las fundaciones de Hildegarda. La reconstrucción fue ordenada por el príncipe Carlos Enrique de Löwenstein-Wertheim-Rosenberg bajo un estilo neorrománico. La nave principal de la iglesia abacial se encuentra adornada con frescos que representan las visiones de la abadesa y en sus arcos se encuentran otros más que muestran escenas de la vida de Hildegarda pintadas bajo el estilo de la escuela Beuron de arte de Desiderius Lenz bajo la dirección de Paulus Krebs. Dicha abadía forma parte del Paisaje cultural del Valle Superior del Medio Rin declarado patrimonio cultural de la humanidad por la UNESCO en 2002.

En la población de Bingen am Rhein se ha dedicado un museo a la vida y obra de esta santa, donde se exponen documentos contemporáneos suyos así como algunos restos de las construcciones lideradas por la abadesa. Igualmente se expone una primera impresión de 1533 de su obra "Physica", contando, además, con un jardín adjunto donde se encuentra las plantas descritas en las obras naturalistas.

En la cinematografía, la película "A Beautiful Mind", ganadora del en el año 2001, utilizó una de las canciones de Hildegarda titulada "Columba aspexit" dentro de la banda sonora, por la cual también obtuvo una nominación a dicho galardón. En el año 2009, la directora alemana Margarethe von Trotta filmó la película ""Visión: La historia de Hildegard von Bingen"" ("Vision. Aus dem Leben der Hildegard von Bingen"), basada en la vida de esta santa, quien fuera caracterizada por la actriz alemana Barbara Sukowa. Fue estrenada en español el 27 de agosto de 2010. En la película italiana del año 2009 «Barbarossa» (traducida al inglés como "«Sword of War»"), basada en la vida del emperador Federico Barbarroja, Hildegarda de Bingen tiene una aparición en la cual es interpretada por la actriz española Ángela Molina.

También en televisión la figura de Hildegarda ha tenido cierta presencia: en 1994 la BBC de Londres produjo el documental «"Hildegard of Bingen"» para la televisión inglesa; asimismo la televisión alemana produjo el documental "«Hildegard von Bingen - Eine Frau des 12. Jahrhunderts»" ("Hildegarda de Binben. Una mujer del siglo XII") y dedicó un capítulo de la serie «"Die Deutschen"» ("Los alemanes") a esta monja benedictina.

La discografía generada a partir de la música de Hildegarda es abundante. Desde 1979 se produjeron alrededor de 35 discos con ejecuciones de las canciones religiosas compuestas por ella, destacando las interpretaciones realizadas por Gothic Voices, Emma Kirkby, la Oxford Camerata bajo la dirección de Jeremy Summerly, Garmarna y Anonymous 4.

El 14 de abril de 1998, el gobierno alemán puso en circulación una moneda conmemorativa del 900 aniversario de Hildegarda de Bingen. La edición constó de un total de 4,5 millones de monedas de 10 marcos, hechas de plata de ley de 925 milésimas, donde se aprecia la efigie de la santa escribiendo los mensajes divinos junto a una banda que dice "Liber Scivias Domini" y los años de su nacimiento y muerte.

En la astronomía, el asteroide (898) Hildegard, descubierto por el astrónomo alemán Max Wolf el 3 de agosto de 1918, lleva su nombre en honor a esta mística alemana.

Igualmente, la consideración moderna sobre la relevancia de la figura de Hildegarda en la Edad Media así como para la historia de la Iglesia, ha llevado a grupos feministas eclesiásticos y seculares a tomarla como un ejemplo relevante de reivindicación del papel de la mujer en la historia y de su importancia en la apertura de roles tradicionalmente masculinos al género femenino.

También, el músico Devendra Banhart homenajeó a esta Santa en su vídeo "Für Hildegard von Bingen" que fue lanzado en octubre de 2013, mostrando el lado artístico de Hildegarda.

La escritora cubana Daína Chaviano dedicó su novela "El hombre, la hembra y el hambre" (Premio Azorín de Novela 1998) a esta monja, cuya figura juega un papel fundamental en la trama. Aunque la novela gira en torno a una jinetera o prostituta cubana, la interacción de este personaje con una monja amiga sirve de base para comentar la vida mística de Hildegarda y sus aportes musicales.

El cráter lunar Hildegard lleva este nombre en su memoria desde febrero de 2016.







</doc>
<doc id="17461" url="https://es.wikipedia.org/wiki?curid=17461" title="Neón">
Neón

El neón es un elemento químico de número atómico 10 y símbolo Ne. Es un gas noble, incoloro, prácticamente inerte, presente en trazas en el aire, pero muy abundante en el universo, que proporciona un tono rojizo característico a la luz de las lámparas fluorescentes en las que se emplea. Tiene 10 protones

Es el segundo gas noble más ligero, y presenta un poder de refrigeración, por unidad de volumen, 40 veces mayor que el del helio líquido y tres veces mayor que el del hidrógeno líquido. En la mayoría de las aplicaciones el uso de neón líquido es más costoso que el del helio, ya que es mucho más raro y difícil de conseguir.

El tono rojo-anaranjado de la luz emitida por los tubos de neón se usa abundantemente para los indicadores publicitarios, también reciben la denominación de tubos de neón otros de color distinto que en realidad contienen gases diferentes. Otros usos del neón que pueden citarse son:

El neón (del griego νέος "neos", nuevo) fue descubierto por William Ramsay y Morris Travers en Londres, Inglaterra, en el año 1898 por la destilación fraccionada del aire líquido, pero sin la misma cantidad de calor.

El neón se encuentra usualmente en forma de gas. La atmósfera terrestre contiene 65,8 ppm y se obtiene por subcalentamiento del aire y cristalización del líquido biocompuson resultante del gas.
El neón es el quinto elemento más abundante en el universo por masa, luego del hidrógeno, helio, oxígeno y carbono. Se encuentra en pequeñas cantidades en la atmósfera y en la corteza terrestre se halla en una proporción de 0,005 ppm.

Se sabe que el neón se sintetiza en estrellas masivas durante las últimas etapas de estas como gigantes o supergigantes rojas (durante la fase de fusión de carbono y oxígeno en neón y magnesio), o a veces como variables azules luminosas o estrellas Wolf-Rayet.

Aún cuando el neón es inerte a efectos prácticos, se ha obtenido un compuesto con flúor en el laboratorio. No se sabe con certeza si este o algún otro compuesto de neón distinto existe en la naturaleza, pero algunas evidencias sugieren que puede ser así. Los iones Ne, (NeAr), (NeH) y (HeNe) han sido observados en investigaciones espectrométricas de masa y ópticas. Además, se sabe que el neón forma un hidrato inestable. De todas maneras, si son posibles sus compuestos, su electronegatividad (según la escala de Pauling) debería ser de 4,5, siguiendo con la norma aplicada al segundo período, y actuaría como oxidante en compuestos con, incluso, el flúor, dando lugar al heptaneonuro (nombre debatido) FNe.
De forma similar al xenón, el neón de las muestras de gases volcánicos presenta un enriquecimiento de Ne así como Ne cosmogénico. Igualmente se han encontrado cantidades elevadas de Ne en diamantes lo que induce a pensar en la existencia de reservas de neón solar en la Tierra.




</doc>
<doc id="17467" url="https://es.wikipedia.org/wiki?curid=17467" title="Catherine Breillat">
Catherine Breillat

Catherine Breillat (13 de julio de 1948, Bressuire, Francia) directora de cine radicada en París, reconocida por su trabajo documental basado en las problemáticas de la sexualidad, los problemas del género y la competencia de la hermandad, también se destaca por sus novelas.

Catherine Breillat es controvertida por la manera en como presenta la sexualidad. En 1999 trabajó con el actor porno Rocco Siffredi, en el film titulado "Romance", considerado uno de los primeros filmes con escenas de sexo real que se destinaron al circuito del cine comercial. 

Su trabajo se ha asociado al cine del género corporal. En una entrevista con "Senses of Cinema", describió a David Cronenberg como otro cineasta que considera que tiene un enfoque similar a la sexualidad en la película.

Breillat nació en Bressuire, Deux-Sèvres, pero creció en Niort. Decidió convertirse en escritora y directora a la edad de doce años después de ver Noche de Circo de Ingmar Bergman, creyendo que había encontrado su "cuerpo ficticio" en el personaje de Harriet Andersson.

Comenzó su carrera en 1967 después de estudiar actuación en el "Studio d'Entraînement de l'Acteur" de Yves Furet en París junto con su hermana, la actriz Marie-Hélène Breillat. En 1968 se publicó su novela "L'Homme facile". El gobierno francés lo prohibió para lectores menores de 18 años. Una película basada en la novela se realizó poco después de la publicación del libro, pero el productor se declaró en quiebra y el distribuidor bloqueó cualquier lanzamiento comercial de la película durante veinte años.

Aunque Breillat pasa la mayor parte de su tiempo detrás de la cámara, ha actuado en un puñado de películas. Debutó en el cine en "El último tango en París" de Bernardo Bertolucci (1972) como Mouchette, modista, junto a su hermana Marie-Hélène Breillat.

En 2004, Breillat sufrió una hemorragia cerebral, que le provocó un derrame que le paralizó el lado izquierdo. Después de cinco meses de hospitalización y una lenta rehabilitación, volvió gradualmente al trabajo, produciendo Une vieille maîtresse (La última amante) en 2007. Esta película fue una de las tres películas francesas seleccionadas oficialmente para el Festival de Cine de Cannes de ese año.

En 2007, Breillat conoció al conocido estafador Christophe Rocancourt, y le ofreció un papel principal en una película que estaba planeando hacer, basada en su propia novela Bad Love, y protagonizada por Naomi Campbell. Poco después, ella le dio dinero para escribir un guión titulado La vie amoureuse de Christophe Rocancourt (La vida amorosa de Christophe Rocancourt), y durante el próximo año y medio, le otorgó préstamos de cantidades aún mayores. En 2009, se publicó un libro escrito por Breillat, en el que alegaba que Rocancourt se había aprovechado de su capacidad mental disminuida, ya que todavía se estaba recuperando de su accidente cerebrovascular. El libro se titula "Abus de faiblesse", un término legal francés que generalmente se traduce como "abuso de debilidad". En 2012, Rocancourt fue condenado por abuso de hecho por tomar el dinero de Breillat y sentenciado a prisión .

En septiembre de 2010, la segunda película basada en un cuento de hadas de Breillat, La belle endormie (La bella durmiente), se estrenó en el 67 ° Festival de Cine de Venecia. 

A partir de 2011, aunque Breillat se había trasladado a otros proyectos, todavía esperaba filmar "Bad Love", pero aún no había podido encontrar financiación para hacerlo. Sin embargo, una adaptación cinematográfica de su libro "Abus de faiblesse", dirigida por Breillat y protagonizada por Isabelle Huppert, comenzó la producción en 2012 y se proyectó en el Festival Internacional de Cine de Toronto 2013.

Se ha observado que "Breillat sigue comprometida con las tomas largas, particularmente durante las escenas de negociación sexual, una técnica que muestra el virtuosismo de sus artistas y enfatiza los elementos políticos y filosóficos del sexo".




</doc>
<doc id="17472" url="https://es.wikipedia.org/wiki?curid=17472" title="Charles Messier">
Charles Messier

Charles Messier (Badonviller, Lorena; 26 de junio de 1730-París, 12 de abril de 1817) fue un astrónomo y cazacometas francés, conocido por ser el creador del catálogo de 110 objetos del espacio profundo (nebulosas, galaxias y cúmulos de estrellas) que constituyen el catálogo de objetos Messier. Este catálogo se publicó por primera vez en 1774. Los objetos Messier se numeran del M1 al M110, y aún hoy en día los aficionados los conocen por ese nombre.

Messier había trabajado muchos años como asistente en el Observatorio Marino, instalado en el Hôtel de Cluny, en pleno París, desde donde había realizado todos sus descubrimientos.

Cuenta la leyenda que Messier, gran aficionado a la caza de cometas, inauguró su catálogo con M1 (la nebulosa del Cangrejo) la noche del 28 de agosto de 1758, cuando buscaba en el cielo el cometa 1P/Halley en su primera visita predicha por el astrónomo inglés.

Messier no descubrió todos los objetos de su catálogo, ya que muchos fueron observados por el también francés Pierre Méchain y, años antes, por otros astrónomos como Edmond Halley. El primer verdadero descubrimiento de Messier fue el Cúmulo globular M3 en Canes Venaciti en 1764. Curiosamente, Messier es más famoso por su catálogo de objetos estelares que por los cometas que descubrió.

El interés de Messier en catalogar aquellos objetos fijos estaba en poder distinguirlos de los errantes, lo que le facilitaría la tarea de buscar cometas. Gracias a la publicación de su catálogo, William Herschel se vio estimulado para iniciar (en 1783) un ambicioso proyecto que, a lo largo de 20 años de investigación, le permitió catalogar un gran número de nebulosas y cúmulos en el hemisferio norte.





</doc>
<doc id="17475" url="https://es.wikipedia.org/wiki?curid=17475" title="Saponificación">
Saponificación

La saponificación, también conocida como una hidrólisis de éster en medio básico, es un proceso químico por el cual un cuerpo graso, unido a una base y agua, da como resultado jabón y glicerina. Se llaman jabones a las sales sódicas y potásicas derivadas de los ácidos grasos. Son susceptibles de saponificación todas aquellas sustancias que en su estructura molecular contienen restos de ácidos grasos, y son sustancias naturales a las que llamamos lípidos saponificables. Los lípidos saponificables más abundantes en la naturaleza son las grasas neutras o glicéridos. La saponificación de un triglicérido se resume así:

Este proceso químico es utilizado como un parámetro de medición de la composición y calidad de los ácidos grasos presentes en los aceites y grasas de origen animal o vegetal, denominándose este análisis como Índice de saponificación; el cual es un método de medida para calcular el peso molecular promedio de todos los ácidos grasos presentes. Igualmente este parámetro es utilizado para determinar el porcentaje en los cuerpos grasos de materias insaponificables, es decir, sustancias que no contienen ácidos grasos.

Un método de saponificación común en el aspecto industrial consiste en hervir la grasa en grandes calderas, añadir lentamente hidróxido de sodio (NaOH) y agitarlo continuamente hasta que la mezcla comienza a ponerse pastosa.
Inicialmente, esta reacción era conocida por transformar la mezcla de un éster de glicerol y de una base fuerte en una mezcla de jabones (o sales de ácidos grasos) y glicerol, de ahí su nombre. Fue identificada en 1823 por el químico francés Michel-Eugène Chevreul (1786-1889), que en "Recherches chimiques sur les corps gras d'origine animale" explicó la reacción química de la saponificación y la composición de la estearina. Demostró que los triglicéridos pueden considerarse combinaciones químicas entre el glicerol y los ácidos grasos (en otras palabras, un triglicérido es un compuesto cuya molécula contiene un residuo de glicerol y tres residuos de ácidos grasos).

El mecanismo de la reacción se descompone en tres etapas (más una acidificación del medio si se desea volver a obtener un ácido carboxílico y ya no un ion carboxilato).



Como puede verse, esta reacción es el único mecanismo irreversible y (casi) total (10 < K < 10). Por lo tanto, desplaza los equilibrios de las reacciones anteriores (al consumir sus productos por completo), haciéndolos también totales (o casi).

En el contexto de la síntesis de un jabón, se puede parar en esta última etapa. Por otro lado, si se quiere obtener un ácido carboxílico, hay que volver a acidificar el medio:

Un lípido saponificable sería todo aquel que esté compuesto por un alcohol unido a uno o varios ácidos grasos (iguales o distintos). Esta unión se realiza mediante un enlace éster, muy difícil de hidrolizar. Pero puede romperse fácilmente si el lípido se encuentra en un medio básico. En este caso se produce la saponificación alcalina. En los casos en los que para la obtención del jabón se utiliza un glicérido o grasa neutra, se obtiene como subproducto el alcohol llamado glicerina, que puede dar mayor beneficio económico que el producto principal.

En el ejemplo de arriba una molécula de un lípido es tratada con dos de hidróxido de potasio; se obtienen dos moléculas de palmitato de potasio (un jabón) y una de glicerina.

La acción limpiadora del jabón se debe a su poder emulsionante, esto es, su habilidad para suspender en agua sustancias que normalmente no se disuelven en agua pura. La cadena hidrocarbonada (parte hidrofóbica) de la sal (el jabón), tiene afinidad por sustancias no polares, tales como las grasas de los alimentos. El grupo carboxilato (parte hidrofílica) de la molécula tiene afinidad por el agua.

En la solución de jabón, los iones carboxilato rodean a las gotas de grasa: sus partes no polares se ubican (disuelven) hacia adentro, mientras que los grupos carboxilato se ordenan sobre la superficie externa. Así, reducidas a volúmenes muy pequeños, las gotas pueden asociarse con las moléculas de agua y se facilita la dispersión de la grasa. Estas pequeñas gotas que contienen las partículas no polares rodeadas de aniones carboxilato se denominan micelas. Es la presencia de estos aniones carboxilato la que hace que las superficies de las micelas estén cargadas negativamente y se repelan entre sí, impidiendo la coalescencia y manteniendo la emulsión, es decir la dispersión en gotas muy finas.

La transparencia del jabón depende del contenido de ácidos grasos. Un exceso de éste hace que sea opaco y de consistencia lechosa.

Cuando se hace un jabón mediante un procedimiento en frío, el jabón saldrá opaco, aun así las medidas de álcalis y aceites hayan sido muy precisas, ya que este proceso rara vez produce el calor suficiente para neutralizar por completo los ácidos grasos.

El proceso en caliente incorpora el calor de la cocina al calor químico producido por la saponificación. Este calor añadido une todos los ácidos grasos con el álcali y como resultado se obtiene un jabón transparente y neutro.

La saponificación es una reacción química que produce calor, y cuanto más calor produzca más completa será la saponificación.



</doc>
<doc id="17476" url="https://es.wikipedia.org/wiki?curid=17476" title="Neopaganismo">
Neopaganismo

El neopaganismo es el conjunto de movimientos espirituales modernos inspirados en diversas formas de religiosidad politeísta anteriores al cristianismo, a menudo emparejado con una interpretación religiosa de la ecología moderna. Este movimiento puede dividirse en cuatro grandes ámbitos: la brujería tradicional, la wicca y tradiciones derivadas, los sincretismos y, finalmente, diversos tipos de reconstruccionismo neopagano.

Se estima que actualmente en el mundo hay aproximadamente un millón de neopaganos.

La Wicca fue fundada por el autor y ocultista inglés Gerald Gardner durante los años cincuenta (del siglo XX). En sus libros, Gardner aseguraba haber sido iniciado en un conventículo secreto por brujas británicas, que supuestamente mantenían el culto heredado de una "antigua religión" (véase hipótesis del culto de las brujas) tras siglos de persecución por parte de algunas iglesias cristianas, especialmente la Iglesia católica y las protestantes.

La teología de la Wicca gardneriana puede definirse como un biteísmo, que integra dos divinidades principales arquetípicas de la brujería europea. La Diosa o la Señora (expresión divinizada del principio femenino, y diosa de las brujas) y el Dios o el Señor (dios astado, inspirado en los antiguos dioses de la caza, particularmente el Cernunnos céltico y demonizado por la Iglesia católica). No obstante, hay tradiciones monoteístas de diosa femenina, como la Wicca Diánica.

Su símbolo principal es la estrella de cinco puntas dentro de un círculo llamado pentáculo.

Algunas tradiciones de brujería a menudo se autodenominan "brujería tradicional" para indicar que difieren de la Wicca y no comparten orígenes históricos con la misma.

La brujería tradicional, cuando no se refiere a las tradiciones específicas (Clan de Tubal Caín, Cultus Sabbati, Anderson Feri) es un término que incluye diversas tradiciones de la brujería - algunas basadas en la "cultura" (spaecrafte, seidr, brujería latinoamericana, "streghoneria") y otras basadas en la práctica ("hedgewitchery", "greenwitchery", "kitchenwitchery"). Por último, otras son tradiciones únicas y personales para el individuo.

Hay diversas similitudes entre tradiciones de brujería tradicional; entre ellas podemos citar: tratar con espíritus y elementos de la naturaleza, rendir culto a los antepasados, creencia en el animismo, y un uso de la magia popular (la magia baja) en vez de la alta magia. Los brujos tradicionales basan sus prácticas en cantos, conjuros, baladas, supersticiones, colecciones de tradición oral y prácticas de brujería y rituales documentados.

Estas tradiciones prescinden de algunos elementos característicos de Wicca, como la Rede y la ley de la triplicidad. En el ámbito de la Ética, reconocen que puede ser ambigua dependiendo de cada situación, y hacen hincapié en que el individuo debe asumir la responsabilidad de sus acciones.

Se denominan «reconstruccionismos» aquellas formas de neopaganismo que aspiran a una recuperación de religiones antiguas de la humanidad, particularmente las de Europa, Oriente Medio y Egipto. Destacan principalmente Asatrú (reconstruccionismo nórdico o germánico), el politeísmo helénico, la religión romana, el druidismo (celta), la mitología guanche en las Islas Canarias (España), las religiones precristianas de los países bálticos como la "Romuva" (Lituania) o "Dievturība" (Letonia), el tengrismo (monoteísmo húngaro-altaico) y distintas formas de neochamanismo, así como, en menor medida, los cultos a Mitra y a deidades egipcias de la época faraónica.

Los seguidores de cada uno de los distintos reconstruccionismos suelen reunirse (por lo general de manera separada) en grandes festivales anuales donde se visten de acuerdo a la época histórica que intentan revivir y realizan distintos rituales inspirados en aquellas tradiciones, aunque suelen evitar los aspectos más crueles y sangrientos de las mismas, como los sacrificios. Desde 1998 existe un Congreso Mundial de Religiones Étnicas, cuya sede central se encuentra en Lituania.

La cosmología es uno de los puntos de encuentro entre varias religiones neopaganas.

Hablando de la creación en el ámbito pagano, emerge la diferencia en relación a los cultos judíos y cristianos. La creación no tiene un inicio preciso, como para poder parar una vuelta completamente, sino que en realidad no está conclusa, porque la creación es un acto, un hecho constante y perenne en el universo.
La creación pagana, por tanto, corresponde a un proceso de desarrollo natural, cambio, mutación y evolución de la existencia. Este proceso es causado por un dios, pero no originado, porque es un mecanismo derivado de la emancipación misma de la divinidad en el mundo, y de su manifestación.

El motor que causa el nacimiento, el crecimiento y la muerte de las cosas, o bien los eternos ciclos de la vida, es el espíritu divino mismo, permanente en el cosmos. Son las divinidades que están en el universo, las que lo plasman, modelan y modifican, otorgando la vida. Los dioses son conceptos, junto a las fuerzas creadas, que hacen que la materia se agregue y forme todas las cosas que existen en la naturaleza. Ellos son perceptibles en el mundo que el hombre habita.

La fuerza creativa se identifica, en esta óptica, con la naturaleza misma, el vehículo a través del cual se cumple el misterio divino de la vida, caracterizada por el eterno movimiento cíclico, en el cual las fuerzas místicas se reforman, renuevan y reencarnan continuamente.

La visión de la wicca es muy similar a lo descrito, aunque hunde sus propias raíces en el dualismo: el principio que emana del cosmos y anima la creación no es único, sino dual. El Dios y la Diosa, que representan el principio masculino y el femenino, personifican las dos fuerzas cósmicas cuya alternancia —en eterno intercambio— da origen a la existencia y es base de todas las cosas. Según esta idea de unión mística, las relaciones sexuales entre hombre y mujer son sagradas porque respetan el proceso infinito de manifestación de la divinidad en el mundo.

Sin embargo, la cosmología del neopaganismo intenta dar una explicación a lo que existe antes del origen de todo: antes de la creación estaba el caos, llamado de diferentes formas según las religiones, y en el caos estaba presente una identidad primordial inactiva y eterna: la divinidad.

La creación tuvo inicio cuando la divinidad pasó de un estado de inactividad a otro, que se manifestó como una luz en la oscuridad infinita, una energía cósmica.

Esta energía no creó, en el sentido literal del término, pero comenzó a poner en orden al interior del caos, comenzó a determinar su espíritu, dar forma a la materia inanimada, dándole armonía, ordenándola.

Los sistemas rituales neopaganos se diferencian de una tradición a otra. Existe todavía un hilo conductor que pasa a través del contacto con la naturaleza. La mayor parte de los ritos envuelven la presencia de elementos y símbolos naturales. Otros se relacionan con el pentáculo. En los rituales se utilizan piedras, cristales, agua, sales, flores y símbolos. Los elementos naturales son considerados catalizadores del contacto entre el mundo divino y el mundo humano. Los neopaganos creen que el mejor modo de estar en contacto con los dioses es vivir y meditar en el universo que ellos llenan.

Los lugares naturales son, en la situación actual que ve una escasa presencia de templos estables, las mejores zonas en las cuales se puedan celebrar los ritos, prácticas y misterios divinos.

En la wicca, en particular, existe una liturgia bastante codificada que prevé la utilización de una serie de objetos litúrgicos precisos en un ritual aunque es practicado por todas las diversas "tradiciones wiccanas" y en los "covens". El ritual prevé la presencia de elementos como, por ejemplo, el "boline", el "athame", el cáliz y el caldero.

Cada tradición prevé la celebración de la unión matrimonial religiosa enfrente de un sacerdote. Los rituales, además, esta vez se diversifican en la corriente neopagana: en la wicca hay un ritual llamado «atadura de manos», que prevé, como se deduce a través del nombre, que las manos de los esposos están unidas en un lazo para formar un nudo. Esta práctica matrimonial es en realidad muy antigua, y representa a otros grupos tanto neopaganos como wiccanos.

Generalmente no hay prejuicios sexuales, hay matrimonios heterosexuales y homosexuales. En algunas tradiciones o grupos el legado espiritual se renueva cada año y se puede elegir si seguir con la misma pareja (renovar el matrimonio) o divorciarse.

Otros rituales comunes, que tienden a abrazarse con el paso del tiempo, deviniendo en una liturgia codificada como la misa cristiana, existen una serie de rituales no codificados o simples expresiones de fe que cada pagano desarrolla tras los muros domésticos. La devoción personal prevé la utilización de un altar sobre el cual se ponen iconos de la divinidad y se la ofrece incienso, agua, fruta. Cerca del altar, que puede ser de cualquier forma y dimensión, el celebrante reza, medita y recita oraciones como símbolo de devoción a los dioses.

Todas las religiones neopaganas tienen en común un sentido ético similar, el cual pone el acento sobre el respeto a la naturaleza.

La naturaleza en lo referente a lo sagrado, es respetada hoy en su forma y expresión. Respeto por la naturaleza es un respeto ecológico ya que en muchas tradiciones la Diosa Madre es identificada con la propia naturaleza.

El neopaganismo en cada una de sus formas reconoce el rol central de la naturaleza en el proceso que ha portado el ser humano a envolverse, a conocer el mundo, a desarrollar sus peculiaridades, a descubrir cuál es la belleza de la existencia.

Además del respeto al ecologismo, respeto a la naturaleza significa respeto de los "seres naturales" de cada humano y de cada criatura: cada uno es respetado y valorizado por eso que es, por su "yo"; cada uno es divino en su particularidad. Esto está bajo la línea de la naturaleza múltiple y multicolor de la vida.

El neopaganismo ofrece, por tanto, una significativa ética social, que permite al ser humano vivir respetando totalmente al prójimo, este respeto se traduce en respeto a cualquier diferencia. La enseñanza pagana se fundamenta, por tanto, en preceptos que pueden ser fácilmente traducidos como reglas de vida cotidiana en particular en el campo ecológico y en el campo social; simples reglas éticas de aproximación a la cotidianidad que permite la realización de una armonía que subraya el legado del ser humano con el mundo, con el prójimo y con la tierra.

Como por la ética, aunque la doctrina de varias formas de neopaganismo son muy similares. Esencialmente todas las religiones neopaganas se fundamentan en una serie de principios.

El más relevante es la ciclicidad: a diferencia de las religiones abrahámicas y de las religiones iraníes, en las que el tiempo es concebido como una línea recta en la que se hace la voluntad de Dios que conducirá a la persona hacia el juicio final, en el neopaganismo (como en el hinduismo) el tiempo es concebido como un proceso circular.

Esta concepción cíclica es perceptible por las más breves expresiones de tiempo de las grandes eras. La misma vida de hoy, por los creyentes neopaganos, es circular: atraviesa tres fases, el nacimiento, el crecimiento y la vejez. La muerte es vista como un paso de un círculo a otro, cosa con la que termina la edad senil y pone fin a la vida biológica, dando lugar a un nuevo periodo de vida, análogo al precedente.

La vida después de la muerte es un concepto de renacimiento, en realidad, un paso de una vida a la otra. En el neopaganismo es vista como un futuro natural, necesario al verificarse de la regeneración de la vida, al nuevo de la existencia.

Es a partir de esta concepción cíclica que, a fin de cuentas, es común en todas las religiones indoeuropeas, que las corrientes neopaganas han asimilado el concepto de reencarnación, mientras que no estuviera presente ya en la forma antica de la religión. La reencarnación es común en prácticamente todas las religiones neopaganas (aunque porque es un trato distintivo del sistema interno de tradiciones indoeuropeas; sin embargo, en religiones como el druidismo el concepto era ya individual en la forma arcaica, en las otras religiones neopaganas no estaba presente, o por lo menos era una creencia difundida únicamente entre los órdenes sacerdotales, iniciados en los más altos misterios. Efectivamente, el mito kemético de la muerte y la resurrección del dios Osiris opera en un contexto que podría ser considerado afín a ese de la reencarnación propiamente dicha.

La tolerancia es el tercer elemento clave de las enseñanzas neopaganas: es expresada firmemente la multiplicidad de vidas que podrían conducir a la comprensión de lo divino y, por tanto, cada religión es considerada válida y justificada.

Cada cual puede seguir la carrera espiritual que siente más cercana a sus exigencias, sea de raíces indoeuropeas o abrahámicas. La intolerancia es vista como una degeneración de la moral, una incapacidad de percibir la existencia como eso que es, caracterizada por unos múltiples puntos de vista, todos válidos y respetables, dado que ninguno conoce la verdad absoluta.

En esta óptica, el neopaganismo se opone a la intolerancia de las religiones abrahámicas las cuales se consideran detentorias de la verdad única o, al menos, en su parte extremista. En el universo neopagano hay muchas concepciones y cada una tiene una variedad de la propia verdad: cada uno puede creer en la propia verdad. Lo importante es no hacer mal a nadie y no imponer las propias ideas a los otros. En la doctrina pagana no existe la contraposición bien-mal, puesto que son conceptos de la mente humana.

El bien y el mal, en realidad, no existen, porque es la misma persona quien tiende a etiquetar las cosas creadas como positivas o negativas. De esto no nace un códice de comportamiento "a priori", basado sobre una moral que establece lo que es bueno y lo que es malo, antes bien, una ética colectiva y cooperativa, garantía de la buena sociedad, basada fundamentalmente sobre los principios morales de la aceptación de las diferencias y respeto a la naturaleza.

Aunque la ética neopaganista es "natural" no condena lo que bajo ese criterio condena las religiones abrahámicas. El sexo libre, la homosexualidad y el progreso científico (aunque las religiones abrahámicas no se oponen a este último) no son vistos como una impiedad o violaciones de la naturaleza, la ciencia es vista como un medio a través del cual se pueden conocer las leyes que gobiernan el cosmos.

El neopaganismo aparece como una religión llena de significados ocultos y misteriosos. El simbolismo es un componente esencial. Detrás de una fachada que pueda parecer simple y fácilmente interpelable, se oculta, para sus seguidores, un significado místico y profundo.

Es esta tendencia de tipo esotérico, que distingue las religiones neopaganas del cristianismo y del mundo abrahámico en general. Estas últimas religiones, de hecho, son esotéricas, tendiendo a no enfatizar los significados profundo y filosóficos de la teología.

El neopaganismo es mayoritariamente esotérico porque propone a sus fieles un encuentro directo con la dimensión oculta de la naturaleza, enfatizando el significado estático y subrayando la emanación del poder divino que destaca la transcendencia. De esta idea de interacción directa entre la persona y lo divino, el neopaganismo basa todos sus rituales en la divinidad de la naturaleza, ricos en devociones votivas y elementos prácticos, sin olvidar los elementos meditativos.

Algunas corrientes neopaganas, pero en particular la wicca, adoptan la magia como elemento de la doctrina. Las prácticas mágicas no son todavía mayoritarias, pero se utilizan como elemento ritual que canaliza la energía cósmica para favorecer el contacto con las fuerzas divinas. La práctica mágica puede utilizarse para guarecerse, como en el chamanismo. En la wicca la magia está sujeta a la "ley de tres", por la cual los practicantes deben abstenerse de hacer mal con la magia porque recibirán el mal multiplicado por tres. En la brujería tradicional ese sentido ético no está presente.

En otras religiones en las cuales está incluido el concepto de magia, como el druidismo, es considerada únicamente como algo de los órdenes sacerdotales de los druidas; paralelamente, la mayor parte de las religiones neopaganas, en particular el kemetismo, dodecateísmo romanismo y Ásatrú no consideran la magia como parte central de su propia doctrina y, por lo tanto, sus fieles la practican de forma personal al margen de los rituales colectivos.

Los días considerados sagrados por las religiones paganas son muchos, todavía hay fiestas que todos los paganos celebran en el mundo, indiferentemente de la tradición a la que pertenecen: son el sabbat y el esbat.

Estos últimos, sobre todo, que verdaderamente son propios de las fiestas de los rituales, teniendo una vuelta al mes por los wiccanos. Celebrados al final de cada mes lunar, hay trece tipos distintos. Como están basados en el mes lunar, nunca caen en el mismo día. El sistema de los sabbat se basa en el mecanismo de rotación del Sol alrededor de la Tierra, son fiestas que celebran la sacralidad de los solsticios y de los equinoccios, considerados eventos astronómicos con una mística propia particular.

En la wicca, las fiestas sabáticas adquieren un significado teológico importante: celebran, de hecho, la unión entre el Dios y la Diosa, mito que recalca la unión divina de los dos principios y brota de las fuerzas de la naturaleza. Los sabbats son ocho:

Las religiones neopaganas siempre han sido ricas en símbolos de mucha variedad y de orígenes pasados. Hoy es predominante un símbolo, el cual proviene de la religión grecorromana, el pentáculo que puede ser utilizado por todas las variedades del neopaganismo porque tiene mucha simbología.

El pentáculo formado por un pentagrama metido en un círculo es considerado un símbolo de fuerte significado místico; esto representa, de hecho, una suerte de reproducciones esquematizadas de los procesos vitales que rigen el universo y, por tanto, el cosmos. Los cinco vértices de los ángulos constituidos de la estrella simbolizan los cinco elementos base con los que se organiza la vida: aire, agua, tierra, fuego y espíritu.

Este último es la energía emanada de la divinidad, sobre la cual está fundado todo el orden del cosmos: ella, mediante las fuerzas ocultas creadas, se condensa formando los átomos de la materia y, por consiguiente, la materia misma, la cual sería otra cosa que la manifestación física del Dios. Los otros elementos representan, generalizando, las fuerzas divinas que hacen perennemente el universo, forjándolo y dando origen a la vida. Son las divinidades, emancipaciones del Uno, permanentes en el cosmos en cada uno de sus aspectos.

El pentáculo es muy utilizado en la liturgia de muchas de las corrientes paganas. Generalmente es puesto en los altares, siendo considerado un símbolo en grado de evocar las fuerzas misteriosas del cosmos, pero aunque generalmente es utilizado como amuleto para colgar del cuello, en particular por el clero (como la cruz de los cristianos, que se ponen los sacerdotes, monjes y fieles).

Cada tradición neopagana tiende a tener sus propios símbolos, que, en el caso de las religiones reconstruccionistas, son herederas del patrimonio cultural de las antiguas religiones paganas de donde están radicadas.

La wicca tiende a tener como símbolo propio el pentáculo, significa el equilibrio entre los cuatro elementos del mundo (aire, tierra, agua y fuego) con el espíritu. Las 3 puntas superiores representan los tres aspectos de la Diosa: doncella, madre y anciana, mientras que las dos puntas inferiores representan al Dios en su aspecto de Dios de luz y Dios de la oscuridad

El kemetismo tiende a tener como símbolo propio el Anj, que representa el misterio de la vida y la manifestación de lo divino. También tienen el Ojo de Horus (o "udjat") y el disco solar del dios Atón, en el cual la divinidad tiende a manifestarse en el cosmos.

En ásatrú tiende a tener como símbolo propio el Mjolnir, que representa protección, la consagración, la justicia.
también se usa el valknut simboliza el viaje de Odín por los Nueve Mundos de Yggdrasil, que culmina con su momentánea muerte y regeneración, en el que obtiene el saber rúnico, no se recomienda el uso del valknut

En el druidismo es de particular importancia la Triskel y el awen. Entre ambos representan la triple naturaleza de la divinidad: la triquetra, como todos los símbolos paganos, es el más difundido pero no se conoce su origen.
El Triskel representa los 3 caminos evolutivos del ser humano: Cuerpo, mente y alma.
El Awen es el espíritu inspirado: la repentina llama de lucidez que inflama los pensamientos de los hombres y les da sabiduría, facilidad de palabra y energía en medio de la batalla.



</doc>
<doc id="17479" url="https://es.wikipedia.org/wiki?curid=17479" title="Peter Greenaway">
Peter Greenaway

Peter Greenaway, CBE (Newport, 5 de abril de 1942), es un director de cine británico, cuya formación se dio en las artes plásticas, específicamente en la pintura.

A una muy temprana edad, Greenaway decidió que quería ser un pintor y desarrolló un interés por el cine europeo, particularmente por las cintas de Antonioni, Bergman, Godard, Pasolini y Resnais. 

En 1962 inició estudios en el Walthamstow College of Art, donde compartió cursos con el músico Ian Dury con quien posteriormente trabajaría en "El cocinero, el ladrón, su mujer y su amante". En el Walthamstow College realizó su primer cortometraje titulado "Death of Sentiment" y que se desarrollaba alrededor de objetos del patio de una iglesia: cruces, ángeles volando, tipografía esculpida en la roca. La película fue filmada en cuatro cementerios londinenses. 

En 1965 se unió a la Oficina Central de Información (COI), donde trabajó durante 15 años como editor y director. En 1966 dirigió "Train", con fragmentos de la filmación del último tren de vapor que llegó a la estación de Waterloo, que estaba ubicada justo detrás de su lugar de trabajo en el COI. Una cinta de estilo abstracto influenciada por Fernand Léger y su "Ballet mécanique", todo montado por cortes sobre una banda sonora de música concreta. En 1966 también dirigió "Tree", siendo el protagonista un árbol del Royal Festival Hall de Londres que se encontraba completamente rodeado de cemento. 

La década de 1970 verá un Greenaway más serio que desarrollará en 1978 "Vertical Features Remake" y "A Walk Through H". La primera, un estudio sobre formas con estructuras aritméticas, y la segunda, un viaje a través de varios mapas. 

En 1980 Greenaway producirá su más ambicioso trabajo hasta ese momento, titulado "The Falls": un monstruo fantástico, una enciclopedia de lo absurdo de material asociable con el vuelo, con la ley de la gravedad, 92 víctimas de algo que denominó (VUE) "Violent Unknown Event" o Evento Violento Desconocido. Los años 80 vieron las mejores películas de Greenaway: "El contrato del dibujante" en 1982, "A Zed & Two Noughts" en 1985, "El vientre del arquitecto" en 1987, "Drowning by Numbers" (también traducida como "Conspiración de mujeres") en 1988 y "El cocinero, el ladrón, su mujer y su amante" en 1989, su película más conocida por el público. 

Los noventa nos dieron las más atractivas a nivel visual: "Los libros de Próspero" en 1991, la controvertida "El niño de Mâcon" en 1993, "The Pillow Book" en 1996 y "8 1/2Women" en 1999.

" (1.ª parte)" de 2003, " (2.ª parte)" de 2004, y "Nightwatching" de 2007 (sobre el cuadro "La ronda de noche" de Rembrandt) son sus últimos filmes para la pantalla grande, unas "extravaganzas" multimedia que incluyen las más innovadoras técnicas. 

En la mente de Peter Greenaway está la actitud de que aún no hemos visto lo que puede ser el cine, como se dijo anteriormente. Su ambición es intentar reinventarlo.

Expuso vídeos de su creación en la edición de la Exposición Universal Shanghái 2010, en el Árbol del Aire del pabellón de Madrid, en una iniciativa llevada a cabo por el grupo Open This End.





</doc>
<doc id="17486" url="https://es.wikipedia.org/wiki?curid=17486" title="Vela">
Vela

Vela puede referirse a:







</doc>
<doc id="17487" url="https://es.wikipedia.org/wiki?curid=17487" title="Jean-François Lyotard">
Jean-François Lyotard

Jean-François Lyotard (Versalles; 10 de agosto de 1924-París, 21 de abril de 1998) fue un filósofo, sociólogo y teórico literario francés. Su discurso interdisciplinario incluye temas que abarcan la epistemología, la comunicación, el cuerpo humano, el arte moderno y posmoderno, la literatura y la crítica teórica, la música, el cine, el tiempo y la memoria, el espacio, la ciudad y el paisaje, lo sublime, y la relación entre estética y política. Es conocido por su formulación del posmodernismo después de la década de 1970 y el análisis del impacto de la posmodernidad en la condición humana. Fue cofundador del Colegio Internacional de Filosofía ("Collège International de Philosophie)" junto con Jacques Derrida, François Châtelet y Giles Deleuze.

Entre las muchas influencias que tuvo a lo largo de su carrera se encuentran: Immanuel Kant, Karl Marx, Georg Wilhelm Friedrich Hegel, Martin Heidegger, Maurice Merleau-Ponty, Sigmund Freud, Jacques Lacan, Ludwig Wittgenstein y Giles Deleuze, cuyos trabajos no solo dieron marco a la labor de Lyotard, sino que en muchas ocasiones sirvieron como guía de su pensamiento crítico.

Era hijo de Jean-Pierre Lyotard, un representante de ventas, y Madeleine Cavalli. Asistió a la escuela primaria Lycée Buffon y posteriormente al Lycée Louis le Grand, ambos en París. De niño tuvo muchas aspiraciones, como convertirse en artista, historiador, fraile dominico y escritor. Posteriormente, se dio por vencido del sueño de ser escritor, cuando terminó de escribir una novela de ficción poco exitosa a la edad de 15 años. Lyotard describió cómo se dio cuenta de que ninguna de estas ocupaciones se había convertido en su “destino” en su autobiografía llamada "Peregrinations" (1986).

Estudió filosofía en la Sorbona al final de los años 1940. Al estallar la Segunda Guerra Mundial, Lyotard interrumpió sus estudios. Sirvió como voluntario de primeros auxilios para el ejército francés y participó en la lucha para liberar París en agosto de 1944. Sin duda influido por la destrucción y la devastación que había presenciado durante la guerra y atraído por las primeras promesas del socialismo, se convirtió en un devoto marxista en los años posteriores a la Segunda Guerra Mundial. Por ello culminó sus estudios en 1947 con una tesis DES ("diplôme d´études supérieures") titulada "La indiferencia como un concepto ético" ("L""'indifférence comme notion éthique"), donde analizó formas de indiferencia y desapego en el budismo zen, estoicismo, taoísmo y epicureísmo. Después de su graduación obtuvo un puesto en el Centro Nacional para la Investigación Científica de Francia.

Durante la primera etapa de su vida militó en grupos de izquierda, y su pensamiento se desarrolló dentro de lo que se podría llamar el marxismo crítico. Como alumno de Maurice Merleau-Ponty, se interesó también por la fenomenología y publicó su primer libro sobre este tema (esencialmente divulgativo) en la colección "Que sais-je", proporcionando una visión clara y global del papel de dicha corriente filosófica en el siglo XX.

Posteriormente, sin embargo, se alejó del marxismo e inició durante los años 1960 una evolución hacia el postmodernismo en la que se aprecia ya el desarrollo de un pensamiento original. Se centró durante esta época en el tema del deseo como búsqueda de lo imposible, en términos muy cercanos a los defendidos por el psicoanálisis, especialmente dentro de la corriente psicoanalítica representada por Jacques–Marie Émile Lacan. Con ello, el papel de la crítica y análisis del lenguaje se hace sumamente importante en su filosofía. Por otra parte, y durante esta misma época, realiza importantes incursiones en el ámbito de la estética, concretamente en el análisis de la obra pictórica, a la que ve como un campo determinante en la posición del deseo.

Destacan especialmente sus estudios de la obra de Paul Cézanne en relación con la concepción freudiana del arte. Para Lyotard, la obra de Cézanne ejemplifica una suerte de reinversión del sentido de dicha concepción, al producirse su pintura desde el fluir de los impulsos inconscientes de la libido. Dicho fluir se plasma en la capacidad de creación del pintor de espacios análogos a los del inconsciente, que producen en el que contempla su obra estados de inquietud y de perturbación.

En 1950, Lyotard aceptó un puesto para enseñar filosofía en el Lycée de Constantine, en Constantina, Argelia. En 1971 obtuvo un doctorado estatal con su disertación "Discurso, figura" bajo la tutoría de Mikel Dufrenne, trabajo que se publicó ese mismo año. Dedicó un periodo de su vida, tras el final de la Segunda Guerra Mundial, a las revoluciones socialistas, cuestión que quedó de manifiesto en sus escritos, ya que se centraron en gran medida en la política de izquierda. En este periodo, Lyotard se interesó particularmente por la guerra de Independencia de Argelia, que vivió en persona mientras enseñaba allí.

Se casó con Andree May en 1948, con quien tuvo dos hijos, Corinne y Laurence, y después se casó en segundas nupcias en 1993 con Dolores Djidzek, madre de su hijo David, nacido en 1986.

Lyotard expuso en "Le Différend" que el discurso humano ocurre en un variado pero discreto número de dominios inconmesurables, ninguno de los cuales tiene el privilegio de pasar o emitir juicios de valor sobre los otros. Siendo así, en "Economía libidinal" (1974), "La condición postmoderna" (1979) y "Au juste: Conversations" (1979) criticó teorías literarias contemporáneas e incitó a un discurso experimental desprovisto del interés por la verdad. En este sentido, consideró que ya había pasado la época de los grandes relatos o "metarrelatos" que intentaban dar un sentido a la marcha de la historia.

Lyotard criticó la sociedad actual postmoderna por el realismo del dinero, puesto que esta se acomoda a todas las tendencias y necesidades siempre y cuando tenga poder adquisitivo. Igualmente, criticó los metadiscursos: el cristiano, el ilustrado, el marxista, el capitalista. Según Lyotard, estos son incapaces de conducir a la liberación. La cultura postmoderna se caracteriza por la incredulidad con respecto a los metarrelatos, invalidados por sus efectos prácticos. Frente a ellos no se trata de proponer un sistema alternativo al vigente, sino de actuar en espacios muy diversos para producir cambios concretos.

En 1954 entró en el grupo "Socialismo o Barbarie", una organización política francesa formada en 1948 en torno a la inadecuación del análisis trotskista para explicar las nuevas formas de dominación en la Unión Soviética. La organización tenía como objetivo realizar una crítica del marxismo desde dentro durante la guerra argelina de liberación. Sus escritos en este periodo se refieren principalmente a la política de extrema izquierda, especialmente en relación con la situación argelina, que presenció de primera mano mientras enseñaba filosofía en Constantina. También escribió ensayos llenos de optimismo, esperanza y aliento hacia los argelinos, los cuales fueron más tarde reproducidos en escritos políticos. Lyotard esperaba alentar una lucha argelina por la independencia de Francia y una revolución social. 

Después de disputas con Cornelius Castoriadis en 1964, Lyotard abandonó "Socialismo o Barbarie" e ingresó en el recién formado grupo "Poder Obrero" ("Pouvoir Ouvrier"), el cual abandonó en 1966. Aunque Lyotard participó activamente en la revolución de mayo de 1968, se distanció del marxismo revolucionario con su "Economía Libidinal" (1974). Se distanció más tarde del marxismo porque sentía que este tenía un enfoque estructuralista rígido e imponía la «sistematización de los deseos» mediante un fuerte énfasis en la producción industrial como cultura fundamental.

Enseñó en el Lycée de Constantine (Argelia) de 1950 a 1952. En 1972 comenzó a dar clases en la Universidad de París VIII, donde enseñó hasta 1987, cuando se convirtió en profesor emérito. Durante las dos décadas siguientes impartió clases fuera de Francia, especialmente como profesor de teoría crítica en la Universidad de California en Irvine y como profesor visitante en universidades de todo el mundo. Algunas de estas universidades fueron la Universidad de Johns Hopkins, la Universidad de California Berkeley, la Universidad de Yale, la Universidad Stony Brook, la Universidad de California, San Diego en Estados Unidos, la Université de Montréal en Quebec (Canadá) y la Universidad de São Paulo en Brasil. También fue director fundador y miembro del consejo del Colegio Internacional de Filosofía ("Collège International de Philosophie)" en París. Antes de su muerte, dividió su tiempo entre París y Atlanta, donde enseñó en la Universidad de Emory, con el nombramiento de Profesor Woodruff de filosofía y francés.

Las obras filosóficas de Jean François Lyotard se desarrollan entre 1954 y 1998. Su primera obra "La Fenomenología" (1954) establece una serie de criterios para analizar la aportación de la fenomenología al pensamiento del siglo XX, y su última obra, "La confesión de Agustín" (1998), refleja una de sus más hondas preocupaciones religiosas de su juventud.

El trabajo de Lyotard se caracteriza por una oposición persistente a las universalidades, metanarrativas y la generalidad. Es ferozmente crítico con muchas de las afirmaciones "universalistas" de la Ilustración, y varias de sus obras sirven para socavar los principios fundamentales que generan estas reivindicaciones amplias.

En sus escritos de principios de los 70 rechaza lo que considera como fundamentos teológicos de Karl Marx y Sigmund Freud: ""En Freud, es judaico, sombrío crítico (olvidadizo de lo político); en Marx es católico. Hegeliano, reconciliador (...) en el uno y en el otro la relación de lo económico con el sentido está bloqueada en la categoría de representación (...) Aquí una política, hay una terapéutica, en ambos casos una teología laica, en la cima de la arbitrariedad y la itinerancia de fuerzas"." En consecuencia, rechazó la dialéctica negativa de Theodor W. Adorno porque la consideraba como la búsqueda de una "solución terapéutica en el marco de una religión", en este caso, la religión de la historia. En ""Economía libidinal"," Lyotard incitaba a ""descubrir y describir diferentes modos sociales de inversión de intensidades libidinales"."

La condición posmoderna

Lyotard fue un escéptico respecto al pensamiento cultural moderno. En este sentido, su obra ""La condición posmoderna"" provocó el escepticismo acerca de las teorías universalizadoras. Lyotard argumenta que hemos superado nuestras necesidades de grandes narrativas debido al avance de las técnicas y tecnologías desde la Segunda Guerra Mundial: argumenta contra la posibilidad de justificar las narrativas que reúnen disciplinas y prácticas sociales, tales como la ciencia y la cultura: ""las narraciones que decimos para justificar un solo conjunto de leyes y apuestas son intrínsecamente injustas"". Su pérdida de fe en las metanarrativas afectó la manera en que vemos la ciencia, el arte y la literatura. Las pequeñas narrativas se han convertido en la manera adecuada de explicar las transformaciones sociales y los problemas políticos. Lyotard sostiene que ésta es la fuerza impulsora detrás de la ciencia posmoderna. A medida que las metanarrativas se desvanecen, la ciencia sufre una pérdida de fe en su búsqueda de la verdad y por tanto debe encontrar otras formas de legitimar sus esfuerzos. Conectada a esta legitimidad científica está el creciente dominio de las máquinas de información. Lyotard sostiene que un día, para que el conocimiento sea considerado útil, tendrá que ser convertido en datos computarizados. Esto le llevó, años más tarde, a escribir "The Inhuman", publicado en 1988, en el que ilustra un mundo en el que la tecnología domina el mundo.

Lyotard denomina "legitimación" al ""proceso por el cual el legislador se encuentra autorizado para promulgar una ley como norma"". Un enunciado debe presentar un conjunto de condiciones para ser aceptado como científico. En este caso la legitimación es el proceso por el cual un legislador, que se ocupa del discurso científico, está acreditado para prescribir las condiciones convenidas, generalmente de consistencia interna y de verificación experimental, para que un enunciado forme parte de ese discurso y pueda ser considerado como válido por la comunidad científica.

De tal manera, señala que, desde Platón, la cuestión de la legitimación de las ciencias se halla fuertemente relacionada con la de la legitimación del legislador. Asimismo, el derecho a decidir lo que es verdadero se encuentra entreverado con el derecho a decidir lo que es justo. Hay un lazo de similitud entre el tipo de lenguaje que llamamos ciencia y ese otro que llamamos ética y política, ambos proceden de la misma tradición occidental.

Por otro lado, Lyotard permite ver cómo la ciencia se ha convertido en la forma de legitimación de los relatos y metarrelatos en la sociedad posmoderna, ya que pone en duda la producción de los saberes científicos. Estos se han llegado a establecer como una especie de discurso de legitimación por parte de quien promueve las ciencias: "“el Estado puede gastar mucho para que la ciencia pueda presentarse como epopeya, a través de ella se hace creíble, crea el asentimiento público del que sus propios decididores tienen necesidad”", señalando que incluso los saberes se han transformado en objeto de uso y objeto de cambio con la finalidad de ser consumidos y valorados de una forma especifica. Así pues, dentro del vigente estatuto del saber científico, Lyotard asegura que la cuestión de la doble legitimación, lejos de diluirse, se plantea con mayor vigor. De esta forma, saber y poder son las dos caras de una misma moneda: ""¿Quién decide lo que es saber, y quién sabe lo que conviene decidir? La cuestión del saber en la edad de la informática es más que nunca la cuestión del gobierno"."

Según Lyotard, no interesa tanto la verdad como la eficacia de la información. Por eso la ciencia y el derecho en las sociedades postindustriales se legitiman por su eficiencia, y todo sistema queda regulado por la optimización de sus actuaciones. Así, se pierden los saberes unificantes y aparece un tipo de saber fragmentario, una explosión de pequeños sistemas que nadan en el eclecticismo y que caracterizan lo que él mismo llamó ""una época helada de apabullante postmodernismo"", en la que se diluyen todas las utopías del siglo XX. Como medio de conseguir que en el seno de esta heterogeneidad de saberes se produzca una conciliación y se establezcan lazos de comunicación entre las diferencias, Lyotard repara en la relevancia del papel que los juegos de lenguaje desempeñan a la hora de garantizar la existencia de una sociedad consolidada.

El colapso de la “gran narrativa” y el valor de los “juegos del lenguaje” 

En "“La condición posmoderna: informe sobre el saber”" "(La Condition postmoderne: Rapport sur le savoir)" (1979), propone lo que él llama una simplificación extrema del pensamiento "posmoderno" como una ""incredulidad hacia las metanarrativas"". Estas metanarrativas, consideradas como “grandes narraciones”, son grandes teorías sobre el mundo: el progreso de la historia, la cognoscibilidad de todo por la ciencia y la posibilidad de una libertad absoluta. Lyotard sostiene que hemos dejado de creer que narrativas de este tipo son adecuadas para representar y contener a todos. Él señala que nadie parece estar de acuerdo en lo que es real (si es que algo lo es) y cada uno tiene su propia perspectiva y la historia. Nos hemos puesto hecho sensibles a la diferencia, a la diversidad, a la incompatibilidad de nuestras aspiraciones, creencias y deseos, y por eso la posmodernidad se caracteriza por una abundancia de micronarrativas. Para este concepto, Lyotard se basa en la noción de "juegos de lenguaje" que se encuentra en la obra de Ludwig Wittgenstein.

En las obras de Lyotard, el término "juegos de lenguaje", a veces también llamado "regímenes de frase", denota la multiplicidad de comunidades de significado, los innumerables e inconmensurables sistemas separados en los que se producen significados y se crean reglas para su circulación. Esto implica, por ejemplo, una incredulidad hacia la metanarrativa de la emancipación humana. Es decir, la historia de cómo la raza humana se ha liberado, la cual reúne el juego del lenguaje científico, del lenguaje de los conflictos históricos humanos y de las cualidades humanas en la justificación general del desarrollo constante de los seres humanos en términos de riqueza y bienestar moral. Según esta metanarrativa, la justificación de la ciencia está relacionada con la riqueza y la educación. El desarrollo de la historia es visto como un progreso constante hacia la civilización o el bienestar moral.

El juego de lenguaje de las pasiones humanas, cualidades y culpas es visto como un cambio constante en favor de nuestras cualidades, mientras que la ciencia y los desarrollos históricos nos ayudan a conquistar nuestras culpas en favor de nuestras cualidades. El punto es que cualquier acontecimiento debe ser capaz de ser entendido en términos de las justificaciones de esta metanarrativa; cualquier cosa que suceda puede entenderse y juzgarse según el discurso de la emancipación humana. Por ejemplo, para cualquier nueva revolución social, política o científica podríamos hacer la pregunta: ""¿Es esta revolución un paso hacia el mayor bienestar del conjunto de seres humanos?"" Para Lyotard debería ser posible siempre responder a esta pregunta en términos de las reglas de justificación de la metanarrativa de la emancipación humana.

Esto se vuelve más crucial en "Au juste: Conversations" (1979) y "Le Différend (La Diferencia)" (1983), que desarrollan una teoría posmoderna de la justicia. Podría parecer que la atomización de los seres humanos que implica la noción de la micronarración y el juego del lenguaje sugieren un colapso de la ética. Se ha pensado a menudo que la universalidad es una condición para que algo sea una declaración ética apropiada: ""no robarás"" es una declaración ética en una manera que ""no robarás a Margaret"" no lo es. Este último es demasiado particular para ser una declaración ética, puesto que cabe preguntarse ¿qué tiene de especial Margaret?. Sólo es ético si se basa en una declaración universal: ""no robarás a nadie"". Sin embargo, los universales son inadmisibles en un mundo que ha perdido la fe en las metanarrativas, por lo cual parecería que la ética es imposible. La justicia y la injusticia sólo pueden ser términos dentro de los juegos de lenguaje, y la universalidad de la ética quedaría fuera.

Lyotard sostiene que las nociones de justicia e injusticia permanecen de hecho en el posmodernismo. La nueva definición de injusticia es usar las reglas del lenguaje de un "régimen de frases" y aplicarlas a otro. El comportamiento ético consiste en permanecer alerta precisamente ante la amenaza de esta injusticia, en prestar atención a las cosas en su particularidad y no encerrarlas dentro de la conceptualidad abstracta. Uno debe dar testimonio de lo ""differend"". En otro caso, hay un conflicto entre dos partes que no se puede resolver de manera justa. Sin embargo, el acto de ser capaz de unir las dos y comprender las reclamaciones de ambas partes, es el primer paso hacia la búsqueda de una solución.

La Différend

En ""Le Différend"", basándose en los puntos de vista de Immanuel Kant sobre la separación de la comprensión, el juicio y la razón, Lyotard identifica el momento en que el lenguaje falla explicado en términos de la diferencia, y lo explica así: ""[...] el estado inestable y el instante del lenguaje en el que algo que debe ser capaz de poner en frases no puede ser todavía. [...] Los seres humanos que pensaban que podían usar el lenguaje como instrumento de comunicación, aprenden a través del sentimiento de dolor que acompaña al silencio (y del placer que acompaña a la invención de un nuevo idioma)"". Lyotard socava la visión común de que los significados de las frases pueden ser determinados de acuerdo a lo que refieren (el referente). El significado de una frase -un evento (algo que sucede)- no puede ser fijado apelando a la realidad (lo que realmente sucedió). Lyotard desarrolla esta visión del lenguaje definiendo la "realidad" de una manera original, como un complejo de sentidos posibles unidos a un referente a través de un nombre.

El sentido correcto de una frase no puede ser determinado por una referencia a la realidad, ya que el referente en sí mismo no fija el sentido, y la realidad misma se define como el complejo de los sentidos competidores unidos a un referente. Por lo tanto, la frase, en tanto evento, permanece indeterminada.

Lyotard utiliza el ejemplo de Auschwitz y las demandas del historiador revisionista de demostrar el Holocausto, para mostrar cómo ""le différend"" opera como un doble vínculo (un dilema o circunstancia de la cual no hay escapatoria debido a condiciones mutuamente conflictivas o dependientes). Faurisson sólo aceptará la prueba de la existencia de cámaras de gas de testigos oculares que fueron víctimas de las cámaras de gas. Sin embargo, estos testigos están muertos y no pueden testificar. O no había cámaras de gas, en cuyo caso no habría testigos oculares para producir evidencia, o había cámaras de gas, en cuyo caso todavía no habría testigos oculares para producir evidencia, porque estarían muertos. Puesto que Faurisson no aceptará ninguna evidencia para la existencia de cámaras de gas, excepto el testimonio de víctimas reales, concluirá, en relación a ambas posibilidades (las cámaras de gas existían y las cámaras de gas no existían), que las cámaras de gas no existían. La dos alternativas conducen a la misma conclusión: no había cámaras de gas. El caso es un considerado como un ejemplo de ""différend"" porque el daño hecho a las víctimas no puede ser presentado en el criterio de juicio sostenido por Faurisson.

Lo Sublime

Lyotard fue un escritor frecuente en materia de estética. Él era, a pesar de su reputación como posmodernista, un gran promotor del arte modernista. Lyotard veía el posmodernismo como una tendencia latente dentro del pensamiento a lo largo del tiempo y no como un período histórico claramente delimitado. Favoreció las obras desconcertantes de las vanguardias modernistas. En ellos encontró una demostración de los límites de nuestra conceptualización, una lección valiosa para cualquier persona plenamente convencida de la confianza transmitida por la Ilustración. Lyotard escribió extensivamente también sobre algunos artistas contemporáneos: Valerio Adami, Daniel Buren, Marcel Duchamp, Bracha Ettinger y Barnett Newman, así como también sobre Paul Cézanne y Wassily Kandinsky.

Desarrolló estos temas desde el punto de vista de lo "sublime". Este es un término de la estética cuyas referido a la experiencia de ansiedad placentera que se experimenta al enfrentarse a paisajes salvajes y amenazadores como, por ejemplo, una enorme montaña escarpada, negra contra el cielo, que se cierne ante la visión. Lo sublime es, pues, la conjunción de dos sentimientos opuestos, lo cual hace más difícil que se la vea como injusticia o como solución.

Lyotard encontró particularmente interesante la explicación del sublime ofrecido por Immanuel Kant en su Crítica del Juicio (a veces Crítica del Poder del Juicio). En este libro, Kant explica esta mezcla de ansiedad y placer en los siguientes términos: hay dos tipos de experiencia "sublime". En lo sublime "matemáticamente", un objeto golpea la mente de tal manera que nos encontramos incapaces de tomarlo como un todo. Más precisamente, experimentamos un choque entre nuestra razón (que nos dice que todos los objetos son finitos) y la imaginación (el aspecto de la mente que organiza lo que vemos y que ve un objeto incalculablemente más grande que nosotros mismos y se siente infinito). En lo sublime "dinámico", la mente retrocede ante un objeto tan inconmensurablemente más poderoso que nosotros, cuyo peso, fuerza y escala podrían aplastarnos sin la más remota esperanza de poder resistirla. Kant subraya que si estamos en peligro real, nuestra sensación de ansiedad es muy diferente de la de un sentimiento sublime, el cual es una experiencia estética, no un sentimiento práctico de peligro físico. Esto explica el sentimiento de ansiedad.

Lo que es profundamente inquietante acerca de lo matemáticamente sublime es que las facultades mentales que presentan percepciones visuales son inadecuadas para el concepto que le corresponde. En otras palabras, lo que somos capaces nosotros mismos de hacernos ver no puede coincidir plenamente con lo que sabemos que esta ahí. Sabemos que es una montaña, pero no podemos tomar el “todo” completo en nuestra percepción. Nuestra sensibilidad es incapaz de afrontar tales visiones, pero nuestra razón puede afirmar la finitud de la representación. Con lo sublime dinámico, nuestro sentido de peligro físico debe despertar la conciencia de que no sólo somos seres materiales, sino también morales y por tanto, en términos de Kant, noumenales. El cuerpo puede ser empequeñecido por su poder, pero nuestra razón no. Esto explica, en ambos casos, por qué lo sublime es una experiencia tanto de placer como de ansiedad.

Lyotard está fascinado por esta admisión, de uno de los arquitectos filosóficos de la Ilustración, de que la mente no siempre puede organizar el mundo racionalmente. Algunos objetos son simplemente incapaces de ser asociados cuidadosamente a conceptos. Para Lyotard, en "Lecciones sobre la analítica de lo sublime (Lessons on the Analytic of the Sublime)", pero basándose en su argumento de lo ""différend"", esto es algo bueno. Las generalidades, en tanto que conceptos, no prestan la debida atención a la particularidad de las cosas. Lo que sucede en lo sublime es una crisis donde nos damos cuenta de la insuficiencia de la imaginación y la razón entre sí. Lo que estamos presenciando, dice Lyotard, es en realidad lo ""différend"", la tensión de la mente al límite de sí misma y al límite de su capacidad de conceptualización.

Economía Libidinal

En uno de los libros más famosos de Lyotard, "Economía Libidinal (Économie libidinale)", ofrece una crítica de la "falsa conciencia" de Marx y afirma que la clase obrera del siglo XIX disfrutaba de ser parte del proceso de industrialización. Lyotard afirma que esto se debe a la energía libidinal. El término "libidinal" viene de libido, concepto referido a los deseos inconscientes no accesibles desde la conciencia, en la teoría psicoanalítica. Los escritos de Lyotard en Economía Libidinal, muestran un logro en nuestros intentos de vivir con el rechazo de todos los principios religiosos y morales a través de un socavamiento de las estructuras asociadas con él. Las estructuras ocultan las intensidades libidinales, mientras que los intensos sentimientos y deseos nos obligan a alejarnos de las estructuras establecidas. Sin embargo, tampoco puede haber intensidades o deseos sin estructuras, porque no habría ningún sueño de escapar de las estructuras represivas si no existen. ""La energía libidinal viene de esta intervención disruptiva de los acontecimientos externos dentro de las estructuras que buscan el orden y la auto-contención."" Este era el primer de los escritos de Lyotard que había criticado realmente el punto de vista marxista. Fue este tema en particular donde realmente se opuso a las opiniones de Karl Marx.

Algunos de los últimos trabajos en los que Lyotard trabajó antes de morir versaron sobre el escritor, activista y político francés, André Malraux. Una de ellas es una biografía, "Firmado, Malraux (Signé Malraux)". Lyotard estaba interesado en las opiniones estéticas de la sociedad que compartía Malraux. Otra de las obras tardías de Lyotard fue "La Confesión de Agustín (La Confession d’Augustin)", un estudio sobre la fenomenología del tiempo. Este trabajo inconcluso, fue publicado póstumamente el mismo año de su muerte.

Lyotard volvió repetidamente a la noción de lo posmoderno en los ensayos reunidos en inglés como ""La posmodernidad explicada a los niños", "Hacia el posmoderno", y "Fábulas posmodernas"". En 1998, mientras se preparaba para una conferencia sobre "Posmodernismo" y "Teoría de los Medios", murió inesperadamente de una leucemia que había avanzado rápidamente. Fue enterrado en el "Cementerio del Père Lachaise" en París.

Tras su muerte, se organizó un homenaje colectivo por el Colegio Internacional de Filosofía (Collège International de Philosophie), presidido por Dolores Lyotard y Jean-Claude Milner, director del "Collège" en aquella época. Las actas fueron publicadas por la Prensa Universitaria de Francia (PUF) en 2001 bajo el título "Jean-François Lyotard," "l'exercice du différend."

El trabajo de Lyotard sigue siendo importante en la política, la filosofía, la sociología, la literatura, el arte y los estudios culturales. Para conmemorar el décimo aniversario de la muerte de Lyotard, se celebró en París, del 25 al 27 de enero de 2007, un simposio internacional sobre Jean-François Lyotard organizado por el "Collège International de Philosophie," bajo la dirección de Dolores Lyotard, Jean-Claude Milner y Gerald Sfez.




</doc>
<doc id="17492" url="https://es.wikipedia.org/wiki?curid=17492" title="Curva del dragón">
Curva del dragón

La curva del dragón es un fractal que se construye siguiendo los siguientes pasos:
La siguiente figura muestra los trece primeros pasos:

Agrandando la imagen y después de una veintena de iteraciones, se obtiene la "curva del dragón":

Se suele citar a Martin Gardner como su autor.

Esta curva llega a rellenar completamente una parte del plano, por lo que su dimensión fractal debe ser 2. El cálculo de su dimensión se hace como en el copo de nieve de Koch, pues las construcciones de ambas curvas son similares.

En el primer paso de la construcción, se observa que a partir del segmento inicial se obtienen los otros catetos del primer triángulo mediante dos semejanzas (una es indirecta) de razón formula_1, de centros los extremos del segmento, y de ángulos formula_2 y formula_3 radianes (o sea, 45°). Llamemos formula_4 y formula_5 estas dos similitudes. Por construcción misma, la formula_6-ésima figura obtenida en el proceso, formula_7, es la reunión de las imágenes por formula_4 y formula_5 de la figura anterior formula_10:

Tomando el límite de esta relación ("n" tiende hacia +∞), y llamando formula_12 la curva del dragón, obtenemos:

Es decir, formula_14 es la reunión de dos copias de sí misma, a escala formula_15, como se puede ver en la figura siguiente:
Por tanto, si agrandamos "D" con una homotecia de razón formula_16, obtenemos dos veces "D", a la misma escala.

Si "D" es de dimensión "d", su "volumen" es multiplicado por formula_17 por esta homotecia. Aquí tenemos, pues, formula_18, y, por tanto, "d" = 2. 

La curva del dragón tiene además la propiedad de que se puede pavimentar el plano con ella, es decir rellenarlo sin dejar huecos y sin que se sobrepongan dos o más piezas: 


</doc>
<doc id="17494" url="https://es.wikipedia.org/wiki?curid=17494" title="Gao Xingjian">
Gao Xingjian

Gao Xingjian (; Ganzhou, provincia de Jiangxi, China, ) es un escritor en lengua china.

Es dramaturgo y novelista, y su obra más importante es la novela "La montaña del alma". En la actualidad reside en Francia y es ciudadano francés. En 2000 obtuvo el .

Estudió francés en la Universidad de Estudios Extranjeros de Pekín. Tras licenciarse, trabajó como traductor de francés en China.

Su obra refleja influencias del modernismo y el teatro del absurdo. Fue contratado como guionista del Teatro Popular de las Artes de Pekín, donde en 1982 se representó su primera obra, "La señal de alarma", escrita en colaboración con Liu Huiyuan. Su segunda obra, "La estación de autobuses" se estrenó en 1983, dirigida, al igual que "La señal de alarma" por Lin Zhaohua. En la representación, esta obra seguía a una pieza corta de Lu Xun, cuyo protagonista acababa poniéndose en la cola de una parada de autobuses. Así, se enlazaban el final de la obra de Lu Xun con el comienzo de "La estación de autobuses". Esta obra, la más famosa de sus composiciones para teatro, tiene ciertas similitudes con "Esperando a Godot", con un toque de darwinismo social. En la obra se utilizan numerosas expresiones locales, con un lenguaje muy pequinés, al estilo de la famosa obra de teatro "El salón de té" de Lao She.

La «Campaña contra la Contaminación Intelectual», emprendida por el gobierno chino a mediados de los años 1980, le causó problemas con la censura, y el estreno de su tercera obra, "El hombre salvaje" se pospuso hasta 1985. A partir de 1986 se prohibió la representación de sus obras nuevas.

En 1987 viajó a París y se quedó viviendo allí. En Francia acabó su obra maestra, la novela "La montaña del alma" (1990).

En 2000 se le concedió el Premio Nobel de Literatura. La noticia fue recibida con indignación por parte de las autoridades chinas, y los medios de comunicación de la China continental no informaron sobre la concesión del premio. La concesión del premio Nobel le dio fama mundial. Sus obras empezaron a traducirse al español y otros muchos idiomas a partir de ese momento.

A continuación se listan algunas de sus obras más representativas.







</doc>
<doc id="17495" url="https://es.wikipedia.org/wiki?curid=17495" title="Lastras de Cuéllar">
Lastras de Cuéllar

Lastras de Cuéllar es un municipio situado en el norte de la provincia de Segovia, y está situado a medio camino de los principales centros económicos de la misma: Cantalejo, Cuéllar y Segovia; además, dentro de su término se localizan los despoblados de San Esteban, La Serreta y Santa María de Sacedón.

Atraviesa su término el río Cega creando a lo largo de sus riberas un paraje de alisos, abedules, vergueras y pinos albares. Posee un conjunto lacustre de especial interés formado por las lagunas del Carrizal y la Tenca.

El escudo heráldico que representa al municipio se blasona de la siguiente manera: 

Tiene su origen a partir del siglo XII, al igual que el resto de núcleos poblados pertenecientes a la Comunidad de Villa y Tierra de Cuéllar surgidos tras la repoblación de Cuéllar, y fue denominado hasta el siglo XIX únicamente como La Lastra, pasando después a adoptar su actual denominación.

A mediados del siglo XIX, Pascual Madoz en su diccionario dijo que disponía de ayuntamiento, constaba de unas 150 casas distribuidas en varias calles y articuladas respecto a una plaza. Además, poseía una escuela, dos ermitas y una iglesia parroquial. Su población era de 135 vecinos, que producían cereales y legumbres, además de ganado vacuno, lanar y caballar.

La localidad de "Lastras de Cuéllar" se encuentra situada en la zona central de la península ibérica, en el extremo norte de la provincia de Segovia, tiene una superficie de 65,42 km², y sus coordenadas son .

El clima de "Lastras de Cuéllar" es mediterráneo continentalizado, como consecuencia de la elevada altitud y su alejamiento de la costa, sus principales características son:



En la Clasificación climática de Köppen se corresponde con un clima "Csb" ("oceánico mediterráneo"), una transición entre el "Csa" ("mediterráneo") y el "Cfb" ("oceánico") producto de la altitud. A diferencia del "mediterráneo" presenta un verano más suave, pero al contrario que en el "oceánico" hay una estación seca en los meses más cálidos.

La localidad de "Lastas de Cuéllar" tiene una población de 410 habitantes (INE 2014), muy lejos de los 1.456 que llegó a alcanzar a mediados del siglo XX.Debido al éxodo rural, muchos de sus habitantes emigraron a otras regiones de España, siendo éstas preferentemente Cataluña, País Vasco y las ciudades de Madrid y Valladolid.

En el centro del municipio se localiza la iglesia de Santa María Magdalena, edificio de una sola nave con yeserías barrocas, del mismo estilo que su retablo mayor. Además, retiradas del núcleo urbano se localizan dos ermitas, la del Humilladero y la de Nuestra Señora de Sacedón, esta última perteneciente al despoblado de su nombre, y que alberga en su interior un retablo renacentista del conocido "maestro de Sacedón", que debe su nombre a esta obra.

Cerca del pueblo se localiza La Serreta, un bosque dedicado a la caza durante siglos, donde Beltrán de la Cueva primer duque de Alburquerque construyó un palacio en el siglo XV que todavía se conserva; en la actualidad sigue existiendo actividad en la finca, frecuentada por el rey Juan Carlos I y considerada una de las fincas de caza más antiguas de España. Cercana al palacio, se conserva otra pequeña ermita, que perteneció en su tiempo al despoblado de su nombre.

Sus fiestas mayores son el día 8 de septiembre en honor de la Natividad de Nuestra Señora.
Celebra también otras fiestas menores: 
El lunes de la Pascua de Pentecostés, 50 días después de la Semana Santa, se celebra la Romería de la Virgen de Sacedón. Cada año se nombran 2 parejas de mayordomos que son los encargados de cuidar la ermita durante todo el año y de organizar la romería. La Virgen sobre su carroza de ruedas empujada por los mayordomos y vestida con sus mejores galas sale de la iglesia de la Magdalena donde la han llevado una semana antes los mayordomos y todos los vecinos del pueblo que han querido acompañarles y donde llevan rezándola novenas cada día por la tarde; entre campanadas y a la puerta de la iglesia, presencia el primero de los paloteos que los danzantes ataviados con los trajes tradicionales bailan al son de las dulzainas; antiguamente sólo danzaban hombres, ya que a las mujeres ni siquiera se las permitía bailar una jota delante de la Virgen, pero hoy en día el grupo de paloteos está formado por hombres y mujeres que danzan juntos y dan más vistosidad a la fiesta. Así comienza una larga procesión en la que la Virgen avanza lentamente detenida por la gente que baila jotas delante de ella y por los paloteos que se van bailando hasta llegar a la puerta de la ermita situada a 1 km. del pueblo donde se baila la última danza, la contradanza, y donde los danzantes forman un arco por el que pasa la Virgen para entrar a su ermita donde esperará hasta el próximo año que vuelvan a buscarla.

En el campo artesano ha destacado a lo largo de su historia la actividad alfarera.

El origen de su nombre no conocemos exactamente en qué contexto aparece, y no tenemos fuentes directas que hablen de cuando fue realizada dicha talla, sabemos que el despoblado de Salcedón (que veneraba esta imagen y se estableció en torno a su ermita) aparece mencionado en el fuero de Sepúlveda (un texto realizado en 1076 por Alfonso VI de León y ratificado en 1305 por el rey Fernando IV de Castilla)

Podría derivar del vocablo latino salictum. Esta hipótesis de defiende sobre dos pilares; por un lado, la propia palabra mediante las leyes de evolución fonética del latín pudo derivar desde este término culto hasta el término coloquial Salcedón que conocemos hoy:

Salictum> término latino
Salictun > la “m” final cae y cambia a “n”
Salicton> la “u” final cierra en “o”
Salciton> la vocal “i” tónica breve cambia de posición y se une el grupo consonántico “lc”
Salceton> la vocal “i” tónica breve pasa a “e”
Salcedon> la oclusiva sorda “t” tiende a convertirse en “d”

Por otro lado, dentro de un análisis más propio de las ciencias sociales, el vocablo latino al que nos referimos significa “sacedal”, “lugar de sauces” e incluso un “lugar húmedo”. Esta descripción teóricamente vendría a denominar el lugar en que la imagen de la virgen se apareció, lo cual coincide con la descripción real del lugar donde se levanta la ermita, puesto que próximo a la misma existe un gran prado con algunas charcas, álamos blancos (chopos) y arroyos; una zona que en el momento en que se levantó la ermita y apareció la imagen pudo denominarse con este término.

La leyenda de la Virgen de Salcedón cuenta que mientras unos pastores andaban en el campo se les apareció la imagen de la virgen. Esta, se sentó en una roca y se convirtió en la talla que conocemos hoy. Más tarde, los vecinos se la intentaron llevar al cercano pueblo de Aguilafuente para lo cual se cargó la imagen sobre un carro tirado por bueyes, sin embargo, los bueyes no pudieron tirar del carro, se quedaban atascados, se les hundían las patas a los animales y las ruedas del carro. Este hecho fue interpretado como un mensaje de la virgen, que no quería que la trasladasen del lugar donde se había aparecido por lo que la imagen se quedó en ese lugar, sobre el cual se construyó una ermita, la que es en la actualidad la ermita de Nuestra Señora de Salcedón.

Se trata de una talla de bulto redondo, difícil de datar (en torno al s. XII). Está realizada en madera policromada y se conserva en la ermita de Nuestra Señora de Salcedón, ermita a la que presta su nombre.

La imagen representa a la Virgen con el niño, este tipo de representaciones son muy comunes en la Edad Media y aunque al principio pretenden relegar la imagen de María como madre de Jesús (al que representan con rasgos adultos) con el tiempo le van a ir concediendo una mayor importancia a la Virgen. 

En este caso, la virgen aparece sentada en un trono vestida con un traje dorado y un manto azul que sale desde su cabeza y cae cubriendo la parte derecha de su cuerpo hasta llegar a la parte frontal, creando una serie de pliegues sobre las rodillas que hacen un cierto contraste con la luz y aportan dinamismo a la imagen, contrastando con las líneas regulares y rectas del resto de la talla. 

Asimismo, la Virgen tiene en la mano izquierda una flor. Normalmente las esculturas de estos siglos se suelen representar con una serie de objetos muy característicos y simbólicos. En este caso, las primeras vírgenes suelen llevar en su mano una manzana que con el tiempo pasa a ser una flor, ambas representaciones aparte de ser decorativas están cargadas de un fuerte simbolismo y aluden al Paraíso.

El niño aparece sobre la rodilla izquierda de su madre, de pie y con el rostro girado, mirándola. Tiene rasgos infantiles, aunque se trata de un niño pequeño sin llegar a ser un bebé. El hecho de que no sea representado como un niño muy pequeño se hace para remarcar la importancia de Jesús, incluso en este caso en que se le representa como a un niño, como hombre. En su mano izquierda lleva una bola del mundo mientas que la derecha aparece levantada en aptitud de bendecir, una forma muy común en las tallas de esta época. Viste un traje dorado similar al de la virgen, para no crear un contraste entre la imagen de la virgen y el niño. Este hecho de que los colores de sus ropajes sean similares podría relacionarse con un intento del autor de la talla por establecer una conexión entre la virgen y el niño, algo que se remarca también en la mirada entre la madre y su hijo o cómo el brazo de la virgen rodea el cuerpo del niño. Durante la edad Media las tallas de las vírgenes pasarán desde una imagen de María como mero trono en los que se sienta cristo, hasta concederle cada vez una mayor importancia a la virgen, que rodea con la mano o que establece una conexión con su hijo.

Por otra parte, las proporciones son tibiamente seguidas, de este modo nos encontramos con unas mano y cabeza, en el caso de la virgen, de un tamaño mayor. Este hecho no hay que relacionarlo con una falta de técnica del autor, sino que intenta realzar partes del cuerpo sobre las demás huyendo del realismo que caracterizará épocas estilísticas anteriores (arte Clásico) y posteriores (Renacimiento), aportando una importancia simbólica a las mismas. Las líneas más características son las verticales, que predominan en la talla (el brazo y mano de la virgen, el propio cuerpo del niño, las piernas de la virgen e incluso se vislumbran en los pliegues del vestido); no obstante, también tiene una marcada importancia la línea que se realiza entre la madre y el niño. Una línea inclinada que se genera con la mirada del niño hacia la madre y que se realza con la posición del brazo de Jesús.

En lo que se refiere al análisis del color, las dos figuras utilizan colores realistas en sus rasgos físicos (colores marrones para el pelo, carne rosada…) aunque cabe destacar los colores de sus vestidos puesto que utilizan para ellos el dorado y el azul. Esta elección de colores para los ropajes y manto de la virgen son muy comunes en la Edad Media y, sobre todo, en el Renacimiento. En ambos casos se trata de colores con una gran carga simbólica, el primero está relacionado con la riqueza por su relación cromática con el oro. En el segundo caso, el azul era un color difícil de conseguir, que a menudo se obtenía con minerales procedentes de Asia Menor por lo que era un tono muy caro y exclusivo que solo se podía emplear en superficies y tallas muy especiales, como en este caso, para el manto de la Virgen. Además, cabe destacar que los colores dorado-naranja y azul son opuestos, se realzan más si se ponen juntos, esto, unido al hecho de que con la pintura se consigan detalles (como el remate que se hace en dorado en el manto de la virgen) nos empujan a pensar que el autor de la talla conocía las artes plásticas, que el tratamiento de la pintura de la talla no fue casual.

Debido a las características anteriormente citadas, esta escultura es difícil de inscribir en un estilo artístico concreto puesto que mezcla características románicas (como son la frontalidad de la composición, la talla en madera, las proporciones y el simbolismo o el propio tema de la composición); junto con otras que son propias del gótico (la simetría de la composición se rompe al estar la imagen del niño sobre la rodilla izquierda, se huye de la representación de María como trono al rodear el cuerpo de su hijo con el brazo así como el niño se representa mirando a su madre)

En la actualidad, la imagen suele ser vestida con mantos de diferentes colores, bordados, que se posan sobre su cabeza, coronas de metales preciosos y vestidos; fruto de las diferentes donaciones que los fieles han hecho durante años a esta imagen.





</doc>
<doc id="17501" url="https://es.wikipedia.org/wiki?curid=17501" title="Juan Carlos Onetti">
Juan Carlos Onetti

Juan Carlos Onetti Borges (Montevideo, Uruguay; 1 de julio de 1909-Madrid, España; 30 de mayo de 1994) fue un escritor uruguayo, autor de novelas, cuentos y artículos periodísticos. 

La escritora uruguaya Cristina Peri Rossi considera que Onetti es «uno de los pocos existencialistas en lengua castellana». Mario Vargas Llosa, quien publicó un ensayo sobre Onetti, dijo en una entrevista a la agencia AFP en mayo de 2008 que Onetti «es uno de los grandes escritores modernos, y no sólo de América Latina». «No ha obtenido el reconocimiento que merece como uno de los autores más originales y personales, que introdujo sobre todo la modernidad en el mundo de la literatura narrativa». «Su mundo es un mundo más bien pesimista, cargado de negatividad; eso hace que no llegue a un público muy vasto».

Juan Carlos Onetti Borges nació en Montevideo, Uruguay, el 1 de julio de 1909 a las 6:00. Hijo de Carlos Onetti —un funcionario de aduanas— y de Honoria Borges —una descendiente de una familia aristocrática brasileña, de Río Grande do Sul—. Tuvo dos hermanos: uno mayor que él, Raúl, y una hermana menor, Raquel. Onetti recordó su infancia como una época feliz, describiendo a sus padres como una pareja muy unida y amorosa con sus hijos. El apellido original de su familia era O'Nety —de origen irlandés o escocés—; el propio escritor comentó acerca de esto tiempo después: «El primero que vino acá, o sea mi tatarabuelo, ese hombre era inglés nacido en Gibraltar. Fue mi abuelo el que italianizó el nombre».

En 1930, con apenas veintiún años, se casó con su prima María Amalia Onetti. En marzo de ese mismo año la pareja viajó a la ciudad de Buenos Aires, su nueva residencia. El 16 de junio de 1931 nació su primer hijo: Jorge Onetti Onetti. En 1933 apareció su primer cuento publicado: «Avenida de Mayo-Diagonal-Avenida de Mayo», en "La Prensa", después de ganar un concurso convocado por el diario. Poco después se divorció de su mujer y un año más tarde, de regreso en Montevideo, volvió a contraer matrimonio, esta vez con María Julia Onetti, la hermana de María Amalia. Por esa época escribió la novela "Tiempo de abrazar", la cual publicaría décadas después, en 1974.

Continuó ejerciendo diferentes oficios y escribiendo cuentos y artículos los cuales fueron publicados en diversos medios de Buenos Aires y Montevideo hasta 1939, año en donde publicó su primera novela, "El pozo" —considerada como la primera en abrir la novela de creación o nueva novela en América Latina— y fue nombrado secretario de redacción del semanario "Marcha —"para el que escribió columnas bajo los seudónimos «Grucho Marx» y «Periquito el Aguador»—. Por ese entonces se separó de su segunda esposa. También por aquella época desarrolló interés por las artes plásticas —como se refleja en su correspondencia con su amigo Julio E. Payró y en su relación estrecha con Joaquín Torres García—. Desempeñó el cargo de secretario de redacción hasta 1941, cuando abandonó el semanario por diferencias con Carlos Quijano y comenzó a trabajar en la agencia de noticias Reuters. Ese año obtuvo el segundo lugar, con su novela "Tierra de nadie", en un concurso convocado por la editorial Losada. El jurado estuvo compuesto por Guillermo de Torre, Norah Lange y Jorge Luis Borges. Se otorgó el primer lugar a la novela "Es difícil empezar a vivir," de Bernardo Verbitsky. Poco tiempo después, Onetti fue enviado como corresponsal a Buenos Aires, donde permaneció hasta 1955.

Trabajó como secretario de redacción de las revistas "Vea y Lea" e "Ímpetu". En 1943 publicó "Para esta noche". En 1945 se casó con una compañera de trabajo en Reuters: la neerlandesa Elizabeth María Pekelharing. El 26 de julio de 1949 nació su primera hija: Isabel «Litti» María Onetti.

En 1950 publicó "La vida breve", una novela central en su obra. A pesar de que en sus primeras ediciones la novela no tuvo mucho éxito, no tardó en ser reconocida como una de las novelas más innovadoras de su tiempo, y aún hoy en día es considerada una de las obras más importantes de la lengua española. Poco después publicó la novela corta "Los adioses." A fines de 1955 regresó a Montevideo y comenzó a trabajar en el diario "Acción", y contrajo matrimonio por cuarta vez, esta vez con la joven argentina de ascendencia alemana Dorothea «Dolly» Muhr, a quien había conocido en 1945 y quien se convertiría en su compañera definitiva. En 1959 publicó la novela corta "Para una tumba sin nombre", y en 1961 "El astillero", otra de sus novelas más celebradas. En 1964 publicó "Juntacadáveres", novela la cual fue finalista del Premio Rómulo Gallegos en 1967, pero perdió ante "La casa verde" de Mario Vargas Llosa.

En 1967, Onetti grabó un disco para la serie "Voz Viva" de América Latina, la cual contiene la lectura de fragmentos de la obra del autor en su propia voz. En ese mismo año apareció en Buenos Aires la primera edición de sus "Cuentos completos" por el Centro Editor de América Latina, y en 1970 se publicó en México una primera edición de sus "Obras completas" —si bien se omitieron algunos relatos de su juventud—. En 1973 publicó la novela corta "La muerte y la niña". En 1974 publicó una segunda edición de sus "Cuentos completos" y la novela corta "Tiempo de abrazar", junto con todos sus cuentos escritos y publicados entre 1933 y 1950, además de ser jurado del Premio Anual de Narrativa organizado por "Marcha", el cual se otorgó a Nelson Marra por su cuento «El guardaespaldas». Debido a que tanto el relato como su autor fueron censurados por el dictador uruguayo Juan María Bordaberry, Onetti fue detenido y encerrado en un hospital psiquiátrico, de donde logró salir al cabo de tres meses gracias a la intervención del poeta español Félix Grande —entonces director de "Cuadernos Hispanoamericanos", quien recogió firmas para lograr la liberación del escritor uruguayo, y la del diplomático español Juan Ignacio Tena Ybarra (director del Instituto de Cultura Hispánica)—. Después de otra breve estadía en Buenos Aires, fue invitado nuevamente a Madrid por el Instituto Internacional de Literatura Iberoamericana para participar en un congreso sobre el barroco. Onetti decidió entonces instalarse definitivamente en la capital española, donde residió durante casi veinte años.

Los años de Onetti en España se caracterizaron por una menor producción literaria pero de muchos premios y participaciones en congresos —participaciones que muchas veces se vieron afectadas por la timidez de Onetti, quien llegó a permanecer encerrado en una habitación de hotel durante la celebración del Primer Congreso Internacional de Escritores de Lengua Española en la ciudad de Las Palmas, en Gran Canaria (evento del cual había sido designado presidente, negándose entonces a participar en ninguna de las actividades previstas)—.

En 1979 publicó "Dejemos hablar al viento" —novela con la que concluyó la «Saga de Santa María» (trilogía conformada por "La vida breve", "El astillero" y "Juntacadaveres"), y que está dedicada a su amigo Juan Ignacio Tena Ybarra, en agradecimiento a las gestiones que emprendió para permitir su liberación—. Además de esta novela, continuó escribiendo artículos, muchas veces tratando la problemática de los exiliados latinoamericanos. En 1981 fue anunciado como el ganador del Premio Cervantes de 1980, recibiendo así el galardón más importante de su carrera, el mismo año que fue propuesto por el Pen Club Español como candidato al Premio Nobel de Literatura —el cual no recibió—. Cuando en 1985 la democracia regresó a Uruguay, el presidente electo, Julio María Sanguinetti, lo invitó a la ceremonia de instalación del nuevo Gobierno; el escritor agradeció la invitación pero decide reclinar la oferta y permanecer en Madrid.

En 1987 publicó "Cuando entonces", su primera novela después de ocho años. En ese tiempo, Onetti llevaba una vida cada vez más ermitaña: pasó sus últimos doce años encerrado en su departamento sobre la avenida América, donde recibía la visita de lectores y periodistas. Prácticamente no salía de su cama, en la que se la pasaba leyendo, fumando y tomando whisky. En 1993 publicó su último libro, la novela "Cuando ya no importe."

Falleció el 30 de mayo de 1994 a los 84 años de edad, en una clínica madrileña a causa de problemas hepáticos. Siguiendo su última voluntad, sus restos fueron cremados en el Cementerio de La Almudena (Madrid). Tras su muerte, en Uruguay, la Intendencia de Montevideo rebautizó al Concurso Literario Municipal como Concurso Literario Juan Carlos Onetti.

La obra literaria de Onetti fue influenciada por dos raíces distintas. La primera de ellas fue la admiración del escritor por la obra de William Faulkner. Como él, Onetti creó un mundo ficcional autónomo —cuyo centro es la inexistente ciudad de «Santa María»—. La segunda de esas raíces fue la admiración del escritor por el existencialismo.



















</doc>
<doc id="17504" url="https://es.wikipedia.org/wiki?curid=17504" title="Tuxedomoon">
Tuxedomoon

Tuxedomoon es una banda estadounidense de rock y de música de vanguardia, de estilo inclasificable, capaces de instrumentar temas "new wave", "jazz fusión" o puramente experimentales, utilizando instrumentos tradicionales y sintetizadores. Creado en San Francisco, Estados Unidos, en 1977 por dos estudiantes de música electrónica, Blaine Reininger y Steven Brown. Otros músicos importantes que participaron en algún momento en la formación son Peter Principle y Winston Tong. Durante parte de su historia han vivido en Bélgica. 

En 1980 publican su primer LP, "Half Mute". En 1982, el coreógrafo Maurice Béjart les encarga la música para el ballet "Divine", tributo a Greta Garbo. En 1983 Reininger deja el grupo para seguir su carrera en solitario. En 1985 publican "Holy Wars", que se convierte en su mayor éxito comercial. Wim Wenders utiliza el tema "Some Guys" en las escenas iniciales de su película El cielo sobre Berlín.

Su último disco de estudio fue "You", publicado en 1987.

Han sido teloneros de Devo, Cabaret Voltaire, Pere Ubu y The Residents y actualmente Tuxedomoon, a pesar de nunca haber obtenido popularidad, es considerado en la actualidad como un grupo de culto.



</doc>
<doc id="17505" url="https://es.wikipedia.org/wiki?curid=17505" title="Feudo">
Feudo

Feudo
("feodum" o "feudum" en latín medieval, con distintos nombres locales en lenguas vulgares, como "fief" -en francés medieval-, "Lehn", "Lehen" o "leen" -en las lenguas germánicas-) es el término con el que en el feudalismo se designaba a la tierra que el señor otorga al vasallo en el contrato de vasallaje, como parte del "beneficium" (‘beneficio’) que el señor debe al siervo por el cumplimiento de sus obligaciones de "auxilium et consilium" (‘auxilio’ —apoyo militar—, y ‘consejo’ —apoyo político—). Las relaciones económicas y de producción que se establecían en el feudo se daban entre ese "vasallo", ahora en funciones de "señor", y los campesinos de su jurisdicción según su distinta situación: siervos (el de mayor sujeción, habitualmente sometidos a prestaciones obligatorias de trabajo) o campesinos libres.

En el feudo se encontraban establecimientos por una utilización, el señor cobraba contra prestaciones en metálico o en especie. Entre estos establecimientos se hallaban la panadería, la herrería, la taberna y el molino. También podían devengarse por la explotación de un bosque, el uso de un río, y eventualmente su vadeo a través de un puente, cuya utilización devengaba el "peaje" o pontazgo. Todas estas rentas constituían el monopolio del señor y se complementaban además con las que obtenía de su propio campo de cultivo o dominio, en el que trabajaban los siervos. También la Iglesia tenía su propio impuesto llamado diezmo, consistente en el cobro del 10% de la cosecha. El Rey era el que mandaba y organizaba todo.

En Castilla, puede considerarse como equivalente al señorío. No obstante, hay un debate historiográfico sobre las diferencias entre el régimen señorial en Castilla y el modelo europeo, ligado a la descomposición del Imperio carolingio.

En la Corona de Aragón el término feudo era tan común como el de "señorío". A diferencia de los modelos de Castilla, en Aragón convive, junto al feudo tradicional, el llamado feudo honrado u honorato, que se diferencia del resto en que no se produce contraprestación económica alguna, ya que la concesión feudal es "absque tamen aliquius prestacione servicii" o "nulli servitio obnoxium", y se recibe bajo la fórmula de juramento, fidelidad y homenaje.

Aunque en origen el señor (por ejemplo, el rey) retenía la capacidad de retirar el feudo a su vasallo (por ejemplo, un conde), el feudo en la práctica se fue haciendo vitalicio y hereditario, pasando a convertirse en el patrimonio de una familia noble. No conviene utilizar el término propiedad para esta relación, más propiamente vinculación. Los derechos plenos de propiedad no son propios de la Edad Media (ni siquiera del Antiguo Régimen), sino del derecho romano o del Estado Liberal. El señor que lo da, y su vasallo noble que lo recibe, comparten de alguna manera algún tipo de derecho de dominio sobre el feudo (que podría llegar hasta la teórica reversión al señor en caso de felonía o incumplimiento de la fidelidad debida, o de la liberación de toda obligación para el vasallo en felonía por parte del señor), del mismo modo que el señor y su siervo campesino también comparten el dominio sobre la tierra (dominio útil y dominio eminente).

La manera de explotar económicamente el feudo, en su manera "clásica", en los siglos de la Alta Edad Media, en que no había casi circulación monetaria y muy escasa comercialización de los excedentes, consistía en repartir la tierra en dos porciones: la reserva señorial y el manso. Cada manso era entregado a un campesino, que se encomendaba ("commendatio") al señor (bien libremente o bien forzosamente), pasando a convertirse en su siervo. En latín "servus" ("ancilla") podría traducirse por esclavo, pero en realidad en la época feudal la utilización de mano de obra esclava en la agricultura no era dominante. La condición jurídica del siervo tampoco era de libertad, puesto que estaba ligado a la tierra que trabajaba. Eso sí, disponía del producto de su manso, que cultivaba a su criterio y del que obtenía lo necesario para su subsistencia (en términos del materialismo histórico, la reproducción de su fuerza de trabajo). Los días que fijara la costumbre (corvea en Francia, serna en Castilla) debía trabajar obligatoriamente en la reserva señorial. Ese trabajo excedente es la forma de obtener el excedente por parte del señor, que se beneficiará del producto de esa reserva (apropiación del excedente por coerción extraeconómica, en esos mismos términos, que define el modo de producción feudal).

La activación de la economía a lo largo de los siglos, sobre todo después del año 1000, que permite que haya circulación monetaria y el surgimiento de mercados, comarcales, urbanos y luego a larga distancia, harán que el modelo se altere, y se conviertan los pagos en trabajo en pagos en especie (fijos o porcentajes, como en la aparcería) o en dinero (renta feudal). Para el señor también eran multitud de derechos feudales que garantizaban que todo tipo de excedente le sea entregado (portazgos, peajes, derecho de molino, de taberna, de tienda, de explotación de bosques, caza y ríos...) incluyendo los pagos más polémicos ("ius primae noctis" o derecho de pernada, habitualmente redimible con un pago). La apropiación de impuestos teóricamente del rey (como la alcabala en Castilla) era también muy común de los señores, en la Baja Edad Media.


</doc>
<doc id="17507" url="https://es.wikipedia.org/wiki?curid=17507" title="Gray">
Gray

Gray puede referirse a las siguientes personalidades:


Asimismo, puede hacer referencia a los siguientes autores de nombre científicos:

También, puede referirse a los siguientes lugares o divisiones administrativas:

Además, puede hacer referencia a:


</doc>
<doc id="17508" url="https://es.wikipedia.org/wiki?curid=17508" title="David Lynch">
David Lynch

David Keith Lynch (Missoula, Montana; 20 de enero de 1946), más conocido como David Lynch, es un director de cine, actor, productor de música electrónica y guionista estadounidense. Su actividad artística se extiende asimismo al terreno de la pintura, la música, la publicidad, la fotografía, e incluso el diseño de mobiliario.

Reconocido admirador de Stanley Kubrick, Jacques Tati, Ingmar Bergman y Werner Herzog, su amor por el dadaísmo y el surrealismo queda patente en algunas de sus películas, cuya misteriosa atmósfera mezcla lo cotidiano con lo soñado, escapando a veces a la comprensión exhaustiva del espectador. Estos rasgos están presentes desde su primer largometraje, "Eraserhead" (1977). Su segunda película, "El hombre elefante" (1980), fue un gran éxito crítico y comercial, que recibió ocho nominaciones a los Premios Óscar. Su tercer filme, "Dune" (1984), no contó con el respaldo de la crítica y supuso un fracaso comercial. Seguidamente dirigió "Blue Velvet" (1986), con la que volvió a recibir la aclamación crítica y una nueva nominación al Óscar en la categoría de mejor director.

Posteriormente se unió a Mark Frost para crear la serie de televisión "Twin Peaks" (1990–1991; 2017), que gozó de gran popularidad y apoyo unánime por parte de la crítica, y fue considerada una serie de culto. Con "Corazón salvaje" (1990) recibió la Palma de Oro en el Festival de Cine de Cannes. Su siguiente largometraje fue "" (1992), una precuela de la serie. Regresó con "Lost Highway" (1997), un thriller psicológico que, aunque recibió críticas mixtas, en la actualidad es considerada una película de culto. Posteriormente, dirigió el que se considera su filme más accesible, "The Straight Story" (1999), que contó con gran aclamación de la crítica. Ya en el siglo XXI, realizó "Mulholland Drive" (2001), un nuevo thriller psicológico de estructura no lineal por el que recibió el y su tercera nominación al Óscar como mejor director. Su décimo y último largometraje, que le ocupó varios años de rodaje usando exclusivamente técnicas digitales, ha sido "Inland Empire" (2006). En la actualidad, algunos de sus proyectos cinematográficos y de animación son solo accesibles a través de su sitio web.

Un elemento recurrente en su cine es describir los entresijos de pequeñas comunidades de Estados Unidos, como es el caso de "Blue Velvet" o "Twin Peaks", sintiendo también predilección por los secretos ocultos de los barrios periféricos de Los Ángeles, retratados en "Lost Highway", "Mulholland Drive" e "Inland Empire". El sonido en sus películas es de gran importancia, y por ello cada banda sonora es trabajada con esmero. El responsable de ese sonido es el compositor Angelo Badalamenti, colaborador habitual del director y creador, entre otras, de la reconocida banda sonora de la serie "Twin Peaks", o de la de "The Straight Story" y "Mulholland Drive", ambas nominadas al . Lynch ha conseguido destacarse según la crítica como uno de los pocos directores actuales con un estilo auténticamente personal y un referente ineludible en el cine contemporáneo.

Lynch podría considerarse el arquetipo del muchacho estadounidense de clase media. Según Thierry Jousse, redactor de la revista "Cahiers du cinéma" y autor de un libro sobre Lynch, «cuando habla de su infancia Lynch la describe invariablemente como una etapa idílica, una especie de permanente sueño despierto, cuyo único aspecto problemático fue, sin duda, una forzada vida nómada». Su padre, Donald, fue un científico adscrito al Ministerio de Agricultura norteamericano, y su madre Sunny era profesora de lengua. David nació con un pie deforme, zambo o equinovaro. La familia vivió en distintos lugares, entre el noroeste del país y Carolina del Norte. Lynch fue boy scout y a los 15 años participó como acomodador en la toma de posesión del presidente John F. Kennedy.

Pronto experimentó impulsos artísticos y asistió al "Corcoran School of Art" en Washington, D.C. mientras terminaba sus estudios secundarios en Alexandria, Virginia. Después se apuntó al "School of the Museum of Fine Arts" de Boston durante un año, antes de partir rumbo a Europa en compañía de su amigo y colega artístico Jack Fisk. Sus planes eran estudiar con el pintor del expresionismo austríaco Oskar Kokoschka (quien resultaría uno de sus principales referentes artísticos) durante tres años. Sin embargo, Lynch regresó a los Estados Unidos al cabo de solo 15 días.

En 1966, Lynch se instala en la ciudad Filadelfia, Pensilvania, y asiste al "Pennsylvania Academy of Fine Arts" (PAFA). Allí se dedicó en principio a la confección de complejos mosaicos a base de figuras geométricas, a los que él llamó "Industrial Symphonies". Por aquel tiempo, tuvo sus primeros devaneos cinematográficos. Su primer corto recibió el título de "Six Men Getting Sick" ("Seis hombres enfermos") (1966). Él lo describió como "57 segundos de desarrollo y pasión, y tres segundos de vómito". Con esta pieza ganó el certamen anual de la Academia. Este pequeño éxito le permitió abordar su segundo cortometraje: "The Alphabet".

A partir de 1970, Lynch se centró exclusivamente en el arte cinematográfico. Consiguió un premio de 5.000 dólares del "American Film Institute" por "The Grandmother", ("La abuela"), que trata sobre un pobre chico de la calle que se las ingenia para conseguir una abuela a partir de una semilla. Esta película de 30 minutos de duración muestra ya muchos de los patrones característicos en su cine de madurez, incluyendo un sonido perturbador y envolvente y una potente imaginería enfocada a los deseos y al inconsciente reprimido, todo ello lejos de los métodos tradicionales de narrar.

En 1971, Lynch se trasladó a Los Ángeles para asistir a las clases del American Film Institute Conservatory. Fue allí donde empezó a trabajar en su primer largometraje, "Eraserhead", aprovechando una ayuda de 10.000 dólares concedida por dicha institución. Este dinero no alcanzó para terminar la película, y por este motivo no se remataría hasta el año 1977. Lynch tuvo que pedir dinero a amigos y familiares, incluyendo a su amigo de la infancia Jack Fisk, diseñador de producción y marido de la actriz Sissy Spacek, e incluso se dedicó a vender periódicos para financiarla.

"Eraserhead" es una película enigmática y sombría, llena de guiños surrealistas y elementos desasosegantes. Por tal motivo fue rodada apropiadamente en blanco y negro. Cuenta la historia de un joven tranquilo (papel que interpreta Jack Nance) que vive en una especie de área industrial y cuya novia da a luz a una rara bestezuela que no para de gemir. Lynch se refiere a la película como “mi historia de Filadelfia”, aludiendo al hecho de que refleja muy bien todas las crudas experiencias que vivió en esa ciudad en su etapa de estudiante, experiencias que le marcaron profundamente.

Sobre la película, la crítica ha afirmado que sugiere o intenta sugerir los miedos y ansiedades del propio cineasta acerca de la paternidad, personificados en el grotesco aspecto del bebé, que se ha convertido en uno de los íconos del cine fantástico de todas las épocas. El director ha rehuido en más de una ocasión explicar cómo fue elaborada la criatura, pero la leyenda cuenta que fue construida a partir de un feto de vaca embalsamado.

Debido a sus extravagantes contenidos, al principio se pensó que "Eraserhead" no podría ser exhibida comercialmente. Sin embargo, gracias al esfuerzo del distribuidor Ben Barenholtz, se convirtió pronto en un clásico, típico en salas especializadas en proyecciones de medianoche, fuera de las grandes audiencias. La crítica más avanzada la alabó inmediatamente como obra maestra, lo que colocó al director a la cabeza de la vanguardia cinematográfica. El gran director Stanley Kubrick afirmó con admiración que era una de sus películas favoritas de toda la historia del cine. El éxito provocó que el equipo de actores y técnicos (entre ellos, el cámara Frederick Elmes, el técnico de sonido Alan Splet, y el actor Jack Nance) siguieran trabajando con Lynch en años posteriores.

"Eraserhead" atrajo la atención del productor Mel Brooks, quien contrató a Lynch para dirigir la película de 1980 "The Elephant Man" ("El hombre elefante"). Escrita por Chris de Vore y Eric Bergren, se trata de un biopic inspirado en la figura de Joseph Merrick, un hombre de clase baja con tremendas malformaciones físicas. El film fue protagonizado por John Hurt como John Merrick (su nombre real fue cambiado) y Anthony Hopkins como Frederick Treves. El rodaje tuvo lugar en Londres, y Lynch le dio su propio enfoque surrealista a la película, y la filmó del mismo modo que la anterior, en blanco y negro. No obstante, ha sido descrita como "una de las más convencionales" de sus películas. "El hombre elefante" fue un gran éxito comercial y obtuvo ocho nominaciones a los Óscar, incluyendo la de mejor director y mejor guion adaptado para Lynch. De este modo, quedó probada la viabilidad comercial de sus propuestas.

Posteriormente, el cineasta aceptó dirigir una superproducción que adaptaba la novela de ciencia ficción "Dune", del escritor Frank Herbert, para el productor italiano Dino De Laurentiis, con la condición de que la productora se comprometiera a financiar un segundo proyecto sobre el cual Lynch mantendría control creativo total. Aunque el productor esperaba que "Dune" (1984) supondría algo así como una nueva "Guerra de las galaxias", la película resultó un gran fiasco comercial, y, además, fue vapuleada por la crítica. Se calculó que ingresarían 45 millones de dólares de los que, al final, quedaron solo 27,4. Para compensar pérdidas, el estudio elaboró una versión alargada para la televisión que desvirtuaba el montaje del director y que Lynch desautorizó inmediatamente.

La segunda película de Lynch producida por De Laurentiis fue "Blue Velvet" ("Terciopelo azul", 1986), la historia de un joven universitario (representado por el actor que protagonizara "Dune", Kyle MacLachlan) que descubre el lado oscuro de una pequeña ciudad, al investigar la procedencia de una oreja cortada que había encontrado casualmente en el transcurso de un paseo campestre. La película muestra actuaciones memorables de Isabella Rossellini, en el papel de una cantante atormentada, y de Dennis Hopper en el de un criminal psicópata, líder de una banda de matones de medio pelo.

"Blue Velvet" obtuvo un gran éxito de crítica y proporcionó a Lynch su segunda nominación al Óscar al mejor director. La película presenta algunos lugares comunes en su cine: una cuidadísima puesta en escena, ciertos episodios y conductas inexplicables, mujeres ultrajadas, los malsanos entresijos de una pequeña comunidad, y la utilización poco convencional de canciones antiguas. "Blue velvet", de Bobby Vinton e "In dreams" de Roy Orbison suenan en este filme extrañas y perturbadoras. Esta fue la primera ocasión en que Lynch trabajaba con el compositor Angelo Badalamenti, quien, a partir de entonces, contribuiría en todas sus películas posteriores.

El director Woody Allen, cuya cinta "Hannah y sus hermanas" fue nominada como mejor película, afirmó que "Blue Velvet" era el mejor filme del año. La película, que es comúnmente considerada como una de las obras maestras del cine contemporáneo, ha llegado a convertirse en un ícono de la cultura popular.

Al no obtener financiación para posteriores guiones, a finales de los 80 Lynch optó por colaborar con el productor televisivo Mark Frost en la serie televisiva "Twin Peaks", acerca de una pequeña localidad de Washington donde ocurren extraños sucesos. La historia se centraba en las investigaciones realizadas por el agente especial del FBI Dale Cooper (de nuevo Kyle MacLachlan) en torno a la muerte de una conocida estudiante de secundaria llamada Laura Palmer, una investigación que iba revelando los escabrosos secretos de muchos ciudadanos aparentemente respetables. El cineasta dirigió seis episodios en total, incluyendo los dos primeros, y escribió o co-escribió algunos más, e incluso apareció como actor en algunos de ellos.

La serie se estrenó en la cadena ABC el 8 de abril de 1990 y poco a poco fue revelándose como todo un fenómeno cultural. Ningún otro proyecto de Lynch ha obtenido semejante aceptación. La serie fue vendida a infinidad de países, y algunos de sus latiguillos ingresaron en la cultura popular. Se hicieron parodias de la misma en el show Saturday Night Live y en la serie de animación Los Simpson. Lynch apareció en la portada de la revista Time en gran medida debido al gran éxito cosechado con "Twin Peaks". El director encarnó el papel del vociferante y medio sordo jefe del agente Cooper, Gordon Cole. Pese a todo, Lynch chocó con los responsables de la cadena por distintos motivos, en especial por la posibilidad de revelar o no la identidad del asesino de Laura Palmer. La cadena insistía en desenmascararlo ya en la segunda temporada, pero Lynch quería guardarlo en secreto hasta el final. Lynch pronto se desencantó de la serie y como resultado muchos miembros del reparto declararon sentirse “abandonados”.

Fue en aquel tiempo cuando Lynch empezó a colaborar con la editora, productora y su compañera en la vida real, Mary Sweeney, que había trabajado como asistente para él en "Blue Velvet". Esta colaboración se prolongaría a lo largo de once proyectos. De su relación nació un hijo.

Su siguiente largometraje fue una adaptación de la novela de Barry Gifford, "Wild at Heart" ("Corazón salvaje"), una "road movie" protagonizada por los actores Nicolas Cage y Laura Dern. La producción obtuvo la Palma de Oro en el Festival de Cannes de 1990, pero no contó con la aprobación de la crítica ni el respaldo del gran público.

"Twin Peaks" acabó sufriendo serios reveses de audiencia y fue retirada en 1991. Mientras tanto Lynch escribió una precuela sobre los últimos siete días en la vida del personaje de Laura Palmer, que dio lugar al largometraje "" (1992), que fracasó en taquilla y acarreó al director las peores críticas de su carrera.

El eslabón perdido entre "Twin Peaks" y "Wild at Heart", es el espectáculo musical "", una nueva colaboración con Angelo Badalamenti en la que canta Julee Cruise y actúan varios actores de "Twin Peaks", así como Nicolas Cage y Laura Dern. Lynch confesó que la obra reflejaba de alguna forma una relación sentimental rota. El director produjo en 1990 un vídeo de 50 minutos sobre la obra.

Durante este periodo Lynch volvió a colaborar con Mark Frost la serie documental "American Chronicles" (1990) y en la serie de humor "On the Air" (1992) para la ABC, sobre los orígenes de la televisión. En EE.UU. solo se emitieron tres episodios. Su siguiente proyecto fue la miniserie para la cadena HBO titulada "Hotel Room" (1993), la cual narraba los acontecimientos que se enmarcaban en una misma habitación de hotel a lo largo de varias décadas.

En 1997 Lynch volvió a la palestra con el complejo filme de argumento no lineal "Lost Highway" ("Carretera perdida"), el cual poseía muchos elementos de cine negro. Fue coescrito con Barry Gifford y protagonizado por los actores Bill Pullman y Patricia Arquette. La película fracasó comercialmente, pero recibió críticas contrapuestas. No obstante, gracias en parte a la banda sonora en que aparecían cantantes y grupos como Marilyn Manson, Rammstein, Nine Inch Nails y The Smashing Pumpkins, Lynch obtuvo una nueva audiencia por parte de espectadores de la llamada Generación X.
En 1999, sorprendió muy positivamente a sus fanes y a la crítica con una película producida por la compañía Disney: "The Straight Story" ("Una historia verdadera/Una historia sencilla"), que era, al menos aparentemente, una sencilla película sin pretensión alguna contando una historia real acerca de un hombre de pueblo (interpretado por el veterano actor Richard Farnsworth) que emprende un largo viaje de estado en estado, a bordo de un cortacésped, con el único fin de hacer las paces con su hermano enfermo. La película recibió críticas muy elogiosas y proporcionó a su autor nuevas audiencias. Fue nominada la Palma de Oro, recibió dos nominaciones a los Globos de Oro (banda sonora y actor dramático), y Richard Farnsworth fue nominado a los Óscar en la categoría de mejor actor.

Ese mismo año Lynch tentó una vez más a la cadena ABC con la idea de un drama para la televisión. La cadena dio el visto bueno y se grabó el episodio piloto, de dos horas de duración. Pero controversias sobre el contenido y la duración de la serie la aparcaron definitivamente. Con la aportación de 7 millones de dólares por parte de la productora francesa Studio Canal, el director convirtió ese episodio piloto en el largometraje "Mulholland Drive". Estrenada finalmente en 2001, es una historia que trata de ahondar en la vertiente oscura de Hollywood, la “fábrica de sueños”. Está protagonizada por Naomi Watts, Laura Harring y el actor Justin Theroux. En lo comercial, la película funcionó relativamente bien en todo el mundo, mereciendo además reseñas positivas, y Lynch obtuvo por ella el premio al mejor director en el Festival de Cannes del año 2001 (este premio lo compartió con Joel Coen por "El hombre que nunca estuvo allí") y otro premio al mejor director otorgado por la "New York Film Critics Association". La película recibió además cuatro nominaciones a los Globos de Oro y supuso además la tercera nominación al Óscar como mejor director de David Lynch.

En 2002, Lynch desarrolló una serie de cortos para Internet titulada "DumbLand". Los ocho episodios de que constaba, intencionadamente muy duros de contenido e interpretación, aparecieron posteriormente en formato DVD.

Lynch dedicó a sus incondicionales ese mismo año una comedia de situación a través de su página web. La serie se tituló "Rabbits" ("Conejos") y constó de ocho episodios plenos de surrealismo que se desarrollaban en un cuarto habitado por extrañas personas con cabeza de roedor. Posteriormente, el director rodó en vídeo digital el corto "Darkened Room", a imitación del exitoso cine de terror japonés de los últimos años.
En el Festival de Cannes correspondiente a 2005, el cineasta anunció que durante un año había estado rodando en Polonia, por medio de técnicas digitales, su último filme. La película, titulada "Inland Empire", es interpretada como un compendio del cine de Lynch. La historia, compleja y con tintes pesadillescos, desarrolla distintos niveles argumentales entremezclados, sin aclarar nunca los nexos lógicos entre ellos. Abunda en primeros planos expresionistas (especialmente de Laura Dern), el sonido es distorsionado y envolvente, y los efectos especiales unidos a las numerosas escenas cómico-grotescas que contiene producen un gran impacto visual. El reparto de la película incluye actores con los que Lynch había trabajado anteriormente, como la ya mencionada Laura Dern, Justin Theroux, Harry Dean Stanton, Grace Zabriskie o Diane Ladd, así como Jeremy Irons, Karolina Gruszka, Peter J. Lucas, Krzysztof Majchrzak o Julia Ormond. Además, Naomi Watts y Laura Harring prestan su voz a dos de los conejos de la película. Lynch describió la película como "un misterio acerca de una mujer metida en grandes dificultades”. Se estrenó en diciembre de 2006, suscitando entre los críticos, al igual que los últimos largometrajes del director, multitud de comentarios enfrentados, aunque la opinión fue mayoritariamente muy positiva.

Lynch ha declarado a menudo admirar profundamente a los cineastas Stanley Kubrick y Federico Fellini, al escritor Franz Kafka y al pintor Francis Bacon. Sostiene que muchas de las películas de Kubrick se encuentran entre sus preferidas, y que las obras de Kafka y Bacon lo subyugan por su fuerza visual y su conmovedora sensibilidad. Igualmente ha citado al pintor expresionista austriaco Oskar Kokoschka como fuente de inspiración. Lynch se siente desde siempre hechizado por la película "The Wizard of Oz" ("El mago de Oz"), aunque no con el culto típico norteamericano y a menudo ha hecho clara referencia a la misma en cintas como "Wild at Heart", donde crea una gran polémica al parodiarla.

Una influencia temprana en él fue el libro "The Art Spirit" del artista y profesor norteamericano Robert Henri, del cual afirmó que le ayudó a decidir el curso que tomaría su trabajo plástico. Como Henri, Lynch se trasladó del campo a un entorno urbano para desarrollar su carrera artística. Henri era un pintor realista urbano, que adoptaba la vida en la ciudad como materia principal de su trabajo, lo que imitó Lynch en sus orígenes. Y si la obra de Henri sirvió de puente entre la Norteamérica agrícola del siglo XIX y la urbana del XX, las cintas de Lynch entremezclan la nostalgia feliz de los años 50 con la extrañeza existencial de los 80 y 90.

Tienen gran peso sobre su obra igualmente los cineastas Luis Buñuel, Ingmar Bergman, Jacques Tati, Werner Herzog y Roman Polański, alguno de los cuales ha reconocido también al propio Lynch como referente.

Lynch emplea a menudo a los mismos actores y al mismo equipo técnico y artístico en sus producciones:

Angelo Badalamenti: música de los largometrajes "Blue Velvet, Twin Peaks, Wild at Heart, On the Air, Lost Highway" y "Mulholland Drive". También escribió la música de "Industrial Symphony No. 1, Twin Peaks: Fire Walk With Me, Hotel Room, The Straight Story, Darkened Room" y "Rabbits".

Mary Sweeney, su habitual editora y productora. Escribió el guion de "The Straight Story". Trabajó igualmente en: "Blue Velvet" (1986), "Wild at Heart" (1990), "Twin Peaks" serie de TV, "Twin Peaks: Fire Walk with Me" (1992), "Hotel Room", serie de TV (1993), "Lost Highway" (1997), "Mulholland Drive" (2001), "Inland Empire" (2006) y co-produjo "Nadja" (1994) con Lynch.


Jack Nance: en "Eraserhead, Dune, Blue Velvet, The Cowboy and the Frenchman, Twin Peaks, Wild at Heart" y "Lost Highway" 


Harry Dean Stanton: "The Cowboy and the Frenchman, Wild at Heart, Twin Peaks: Fire Walk With Me, Hotel Room, The Straight Story" e "Inland Empire". 


Scott Coffey: "Wild at Heart, Lost Highway, Mulholland Drive, Rabbits" e "Inland Empire".

Freddie Jones: "The Elephant Man, Dune, Wild at Heart, Hotel Room" y "On the Air".

Michael J. Anderson: "Twin Peaks, Industrial Symphony No. 1, Twin Peaks: Fire Walk With Me" y "Mulholland Drive".

Eric DaRe: "Twin Peaks", "Wild at Heart" (casting), "Twin Peaks: Fire Walk With Me" y "Lost Highway" (departamento artístico).
Laura Dern: "Blue Velvet, Wild at Heart, Industrial Symphony No. 1" e "Inland Empire".
Bellina Logan: "Wild at Heart, Twin Peaks, On the Air" e "Inland Empire".

Kyle MacLachlan: "Dune, Blue Velvet, Twin Peaks" y "Twin Peaks: Fire Walk With Me".
Grace Zabriskie: "Twin Peaks, Wild at Heart, Twin Peaks: Fire Walk With Me" e "Inland Empire".

Frances Bay: "Blue Velvet, Twin Peaks" y "Wild at Heart".
Catherine E. Coulson: "The Amputee, Twin Peaks" y "Twin Peaks: Fire Walk With Me".
Miguel Ferrer: "Twin Peaks, Twin Peaks: Fire Walk With Me" y "On the Air".
Laura Harring: "Mulholland Drive, Rabbits" e "Inland Empire".
Sheryl Lee: "Twin Peaks, Wild at Heart" y "Twin Peaks: Fire Walk With Me".
Everett McGill: "Dune, Twin Peaks" y "The Straight Story".
Frank Silva: "Twin Peaks" y "Twin Peaks: Fire Walk With Me", y trabajó como encargado de vestuario en "Wild at Heart".
Charlotte Stewart: "Eraserhead, Twin Peaks" y "Twin Peaks: Fire Walk With Me". 

Naomi Watts: "Mulholland Drive, Rabbits" e "Inland Empire".

Alicia Witt: "Dune, Twin Peaks" y "Hotel Room".


Jeanne Bates: "Eraserhead" y "Mulholland Drive".
Nicolas Cage: Wild at Heart e "Industrial Symphony No. 1". 

Brad Dourif: "Dune" y "Blue Velvet". 

Sherilyn Fenn: en "Twin Peaks" y "Wild at Heart". 

Crispin Glover: "Wild at Heart" y "Hotel Room". Lynch estaba interesado en producir el debut como director de Glover, "What is it?".

Diane Ladd: "Wild at Heart" e "Inland Empire". 

Dean Stockwell: "Dune" y "Blue Velvet".

Justin Theroux: "Mulholland Drive" e "Inland Empire".

Músicos que han aparecido en sus films: Sting en "Dune", Chris Isaak en "Twin Peaks: Fire Walk With Me", David Bowie en "Twin Peaks: Fire Walk With Me", Julee Cruise en "Twin Peaks" y "Twin Peaks: Fire Walk With Me", John Lurie en "Wild at Heart", Marilyn Manson, Twiggy Ramirez y Henry Rollins en "Lost Highway" y Billy Ray Cyrus en "Mulholland Drive".

El propio Lynch se reservó pequeños papeles en "The Amputee, Dune, Twin Peaks" y "Twin Peaks: Fire Walk With Me". También hizo cameos de voz en "INLAND EMPIRE" y "Nadja", y aparecía en una escena suprimida de "Lost Highway".
También interpreta al cantinero "Gus" en la serie animada de tv The Cleveland Show tanto en la voz como en la apariencia.

Durante su trayectoria también dedica su trabajo al formato publicitario, llevando a cabo multitud de spots televisivos desde 1988, entre los que se encuentran distintos fines comerciales; perfumes, artículos de deporte, productos alimenticios, automóviles o dispositivos de entretenimiento. 

Aparte del cine, David Lynch ha desarrollado su creatividad en el campo de la pintura. En España su obra pictórica pudo verse con la exposición Action-reaction, que recorrió en 2009 ciudades como Zaragoza y Granada.

Lynch también ha colaborado en el mundo de la música con la creación de videoclips, entre los que destacan "Dangerous" de Michael Jackson (1992), "Longing" de Yoshiki (1995), "Rammstein" de Rammstein (1996), "Shot in the Black of the Head" de Moby (2009) y "Came back Haunted" de Nine Inch Nails (2013), entre otros.

También es destacable su labor en la divulgación de la Meditación Trascendental (MT), desde que se iniciara en ella hacia 1973. En julio de 2005 fundó la David Lynch Foundation For Consciousness-Based Education and World Peace (también conocida simplemente como David Lynch Foundation), para ayudar económicamente a los estudiantes en las escuelas intermedias y secundarias que estén interesados en la Meditación Trascendental y para financiar la investigación sobre la técnica y sus efectos en el aprendizaje. Con los años, la Fundación ha ampliado su enfoque para incluir otras poblaciones "en riesgo", tales como las personas sin hogar, los veteranos de guerra estadounidenses, los refugiados de las guerras africanas y los presos.

En lo personal, Lynch ha mantenido relaciones sentimentales con la actriz Isabella Rossellini (a raíz del rodaje de "Terciopelo azul"), y ha estado casado tres veces: con Peggy Lentz (1967-1974; una hija, Jennifer Chambers Lynch, 1968, ahora directora cinematográfica). Con Mary Fisk (1977-1987) (un hijo, nacido en 1982, Austin Jack Lynch). Y con la editora y productora de sus películas Mary Sweeney (2006; un hijo nacido en 1992, Riley Lynch).








Lynch ha ganado dos veces el Premio César francés a la mejor película extranjera (por "El hombre elefante" y "Mulholland Drive"). En 2002 fue presidente del jurado del Festival de Cannes. En 2002 fue asimismo galardonado por el gobierno francés con la Legión de Honor. El 6 de septiembre de 2006 recibió el León de Oro en el Festival de Venecia por sus contribuciones al Séptimo Arte. En este mismo festival presentó su último filme, "Inland Empire".




</doc>
<doc id="17510" url="https://es.wikipedia.org/wiki?curid=17510" title="Historia de la aviación">
Historia de la aviación

La historia de la aviación se remonta al día en el que el hombre prehistórico se paró a observar el vuelo de las aves y de otros animales voladores. El deseo de volar está presente en la humanidad desde hace siglos, y a lo largo de la historia del ser humano hay constancia de intentos de volar que han acabado mal. Algunos intentaron volar imitando a los pájaros, usando un par de alas elaboradas con un esqueleto de madera y plumas, que colocaban en los brazos y las balanceaban sin llegar a lograr el resultado esperado.

Muchas personas decían que volar era algo imposible para las capacidades de un ser humano. Pero aun así, el deseo existía y varias civilizaciones contaban historias de personas dotadas de poderes divinos que podían volar. El ejemplo más conocido es la leyenda de Ícaro y Dédalo, que encontrándose prisioneros en la isla de Minos se construyeron unas alas con plumas y cera para poder escapar. Ícaro se aproximó demasiado al Sol y la cera de las alas comenzó a derretirse, haciendo que se precipitara en el mar y muriera. Esta leyenda era un aviso sobre los intentos de alcanzar el cielo, semejante a la historia de la Torre de Babel en la Biblia, y ejemplifica el deseo milenario del hombre de volar.

La historia moderna de la aviación es compleja. Durante siglos se dieron tímidos intentos por alzar el vuelo, fracasando la mayor parte de ellos, pero ya desde el siglo XVIII el ser humano comenzó a experimentar con globos aerostáticos que lograban elevarse en el aire, pero tenían el inconveniente de no poder ser controlados. Ese problema se superó ya en el siglo XIX con la construcción de los primeros dirigibles, que sí permitían su control. A principios de ese mismo siglo, muchos investigaron el vuelo con planeadores, máquinas capaces de sustentar el vuelo controlado durante algún tiempo, y también se comenzaron a construir los primeros aeroplanos equipados con motor, pero que, incluso siendo impulsados por ayudas externas, apenas lograban despegar y recorrer unos metros. No fue hasta principios del siglo XX cuando se produjeron los primeros vuelos con éxito. El 17 de diciembre de 1903 los hermanos Wright se convirtieron en los primeros en realizar un vuelo en un avión controlado, no obstante algunos afirman que ese honor le corresponde a Alberto Santos Dumont, que realizó su vuelo el 13 de septiembre de 1906.

A partir de entonces, las mejoras se fueron sucediendo, y cada vez se lograban mejoras sustanciales que ayudaron a desarrollar la aviación hasta tal y como la conocemos en la actualidad. Los diseñadores de aviones se siguen esforzando en mejorar continuamente las capacidades y características de estos, tales como su autonomía, velocidad, capacidad de carga, facilidad de maniobra o la seguridad, entre otros detalles. Las aeronaves han pasado a ser construidas de materiales cada vez menos densos y más resistentes. Anteriormente se hacían de madera, en la actualidad la gran mayoría de aeronaves emplea aluminio y materiales compuestos como principales materias primas en su producción. Recientemente, los ordenadores han contribuido mucho en el desarrollo de nuevas aeronaves . 

Se sabe que alrededor del año , Arquitas de Tarento, un estudioso de la Antigua Grecia, construyó un artefacto de madera que él mismo bautizó con el nombre de "Peristera" (en griego: "Περιστέρα", "Paloma"), que tenía forma de ave y era capaz de volar a unos 180 metros de altura. Utilizaba un chorro de aire para alzar el vuelo, pero no se tiene constancia de qué era lo que producía ese chorro. El objeto volador se amarraba mediante unas cuerdas que permitían realizar un vuelo controlado hasta que el chorro de aire terminaba. Este artefacto de madera probablemente fue la primera máquina voladora capaz de moverse por medios propios.

La linterna de Kong Ming, precursora del globo aerostático, era conocida en China desde la antigüedad. Su invención se atribuye al general Zhuge Liang, y fueron usadas para asustar a las tropas enemigas. Sobre el año los chinos inventaron la cometa, que se considera un tipo de planeador, y desarrollaron técnicas para hacerla volar en el aire. Siglos después, en el año 559 hay documentados vuelos de seres humanos usando cometas. El emperador Gao Yang experimentó con prisioneros, entre los que se encontraba Yuan Huangtou, hijo del anterior emperador, Yuan Lang. Les ordenó lanzarse desde lo alto de una torre, y Yuan Huangtou planeó hasta sobrepasar las barreras de la ciudad, aunque poco después moriría ejecutado.

En el año 852, el andalusí Abbás Ibn Firnás, se lanzó desde el minarete de la mezquita de Córdoba con una enorme lona para amortiguar la caída, sufriendo heridas leves, pero pasando a la historia como el precursor de los modernos paracaídas. En el 875, contando con 65 años de edad, Ibn Firnás se hizo confeccionar unas alas de madera recubiertas de tela de seda que había adornado con plumas de rapaces. Con ellas se lanzó desde lo alto de una colina, y logró permanecer en el aire durante un breve espacio de tiempo, aunque hay relatos que afirman que voló durante más de diez minutos. El aterrizaje resultó muy violento y Abbás Ibn Firnás se fracturó las dos piernas, pero consideró que la experiencia había sido un éxito, al igual que la gran multitud de personas que lo observaron.

Este vuelo sirvió de inspiración para Eilmer de Malmesbury, un monje benedictino, que más de un siglo después, hacia el año 1010, recorrió más de 200 metros en el aire, sobre un aparato similar al de Abbás Ibn Firnás.

En el 1290, Roger Bacon, un monje inglés, escribió que el aire, al igual que el agua, tenía algunas características propias de los sólidos. Bacon estudió las ideas de Arquímedes relacionadas con la densidad de los elementos, y llegó a la conclusión de que si las personas pudieran construir una máquina que tuviese las características adecuadas, el aire podría soportar esa máquina, al igual que el mar soporta un navío.

Muy probablemente fue el artista e inventor italiano Leonardo da Vinci la primera persona que se dedicó seriamente a proyectar una máquina capaz de volar. Leonardo diseñó planeadores y ornitópteros, que usaban los mismos mecanismos usados por los pájaros para volar, a través de un movimiento constante de las alas para arriba y para abajo. Sin embargo, nunca llegó a construir tales máquinas, pero sus diseños se conservaron, y posteriormente, ya en el siglo XIX y siglo XX, uno de los planeadores diseñados por Leonardo da Vinci fue considerado digno de atención. En un estudio reciente, se creó un prototipo basado en el diseño de ese mismo planeador, y de hecho, el aparato era capaz de volar. No obstante, al interpretar el diseño del planeador, se aplicaron algunas ideas modernas relacionadas con la aerodinámica. Aun así, este diseño es considerado como el primer esbozo serio de una aeronave.

Según crónicas de la época, el primer vuelo realizado con éxito de un globo de aire caliente, fue gracias al padre Bartolomeu Lourenço de Gusmão, un portugués nacido en Brasil en la época colonial, que logró alzar el vuelo de un aerostato, al que denominaría passarola, el 8 de agosto de 1709 en la corte de Juan V de Portugal, en Lisboa. En la demostración, la passarola se elevó unos tres metros por encima del suelo, dejando impresionados a los observadores, y ganándose el apodo de "Padre Volador". No se conservaron descripciones detalladas del acontecimiento, probablemente debido a que fueron destruidas por la inquisición, pero algunos diseños fantasiosos de la excéntrica aeronave salieron en el periódico vienés "Wienerische Diarium" de 1709. Según una crónica de ese periódico, el aparato consistía en un globo de papel grueso, que dentro contenía un cuenco con fuego, y que consiguió elevarse más de veinte palmos. No obstante, la passarola no influyó en los desarrollos de la aviación que ocurrirían posteriormente.

El primer estudio de aviación publicado fue "Sketch of a Machine for Flying in the Air" ("Esbozo de una máquina para volar por el aire"), de Emanuel Swedenborg, publicado en 1716. Este esbozo de máquina voladora consistía en un fuselaje y dos grandes alas que se moverían a lo largo del eje horizontal de la aeronave, generando el empuje necesario para su sustentación en el aire. Swedenborg sabía que su máquina jamás volaría, pero decía que los problemas que existían en su diseño serían resueltos en el futuro. Sus palabras fueron:

La "fuerte barra en espiral" descrita por Swedenborg es lo que actualmente se conoce como hélice. Él sabía que la sustentación y la manera de generar esa sustentación serían indispensables para la creación de un aparato capaz de volar por medios propios.

El primer vuelo humano del que se tiene noticia fue realizado en París el 15 de octubre de 1783, en un globo cautivo. Dos meses más tarde, el doctor Jean-François Pilâtre de Rozier y el noble François Laurent d'Arlandes, realizaron el primer vuelo libre en una máquina creada por el hombre. Consiguieron volar durante 25 minutos, recorriendo ocho kilómetros en un globo de aire caliente, inventado por los hermanos Montgolfier, dos fabricantes de papel. El aire dentro de la cámara de aire del globo se calentaba por una hoguera de madera. El globo tenía el inconveniente de que era incontrolable, volaba donde el viento le llevase. Este globo, por ser bastante pesado, alcanzó una altura máxima de apenas 26 metros. Los hermanos Montgolfier continuaron fabricando otros globos, logrando varios vuelos con éxito, lo que hizo que la experimentación de vuelos con globos se extendiera por Europa a lo largo del siglo XVIII. Los globos permitían la profundización en los conocimientos acerca de la relación entre altitud y atmósfera. Incluso Napoleón Bonaparte planeó usar globos en una posible invasión francesa a Inglaterra.

En noviembre de 1792, los ensayos realizados por un grupo de artilleros en el Real Colegio de Artillería de Segovia y después ante el rey Carlos IV de España del vuelo de un globo aerostático, todos ellos dirigidos por Louis Proust; fueron los primeros realizados en el mundo en el aspecto militar.

También en España, Diego Marín Aguilera fue el primer hombre, que se tiene noticia que voló con un aparato que pesaba más que el aire. En la noche de 15 de mayo de 1793, Diego Marín Aguilera realizó en Coruña del Conde, provincia de Burgos, un vuelo de 360 metros con un artefacto de hierro y plumas de ave, controlado por el propio piloto que logró alcanzar «cinco a seis varas» de altura sobre el punto de partida hasta tomar tierra al otro lado del río después de haber hecho un recorrido de «431 varas castellanas» (unos 360 metros). El motivo del rápido aterrizaje fue la rotura de uno de los pernos que movían las alas. A la mañana, al enterarse los vecinos de lo acontecido en aquella noche emotiva de mayo, se mofaron de su convecino Marín, creyéndole loco, e incendiaron el plumífero aparato como cosa diabólica.

Otros inventores, como el francés Jacques Charles, sustituyeron el aire caliente por hidrógeno, que es un gas más ligero que el aire. Pero de igual forma, los globos seguían sin poder ser dirigidos, y solamente la altitud era controlable por los aviadores.

En el siglo XIX, en 1852, el ingeniero francés Henri Giffard inventó el dirigible, que es una máquina más ligera que el aire, y se diferencia del globo en que su dirección sí podía ser controlada a través del uso de timones y motores. El primer vuelo controlado de un dirigible se realizó el 24 de septiembre de ese mismo año en Francia, controlado por el propio Giffard, logrando recorrer 24 kilómetros, a una velocidad de 8km/h usando un pequeño motor a vapor. A lo largo de finales del siglo XIX y en las primeras décadas del siglo XX, el dirigible fue un método de transporte de confianza.

Con la invención del globo y del dirigible, los inventores pasaron a intentar crear una máquina más pesada que el aire, que fuese capaz de volar por medios propios.

En primer lugar, aparecieron los planeadores, máquinas capaces de sustentar el vuelo controlado durante algún tiempo. En 1799, George Cayley, un inventor inglés, diseñó un planeador relativamente moderno, que contaba con una cola para controlarlo, y un lugar donde el piloto se podía colocar, por debajo del centro de gravedad del aparato, dando así estabilidad a la aeronave. Cayley construyó un prototipo, que realizó sus primeros vuelos no tripulados en 1804. Durante las cinco décadas siguientes, trabajó en su prototipo, tiempo durante el cual Cayley dedujo muchas de las leyes básicas de la aerodinámica. En 1853, un ayudante de Cayley realizó un vuelo de corta duración subido al planeador, en Brompton (Inglaterra). George Cayley es considerado el fundador de la ciencia física de la aerodinámica, habiendo sido la primera persona que describió un aeronave de ala fija propulsada por motores.

En 1856 el francés Jean-Marie Le Bris realizó el primer vuelo que planeó más alto que su punto de despegue, gracias a su planeador, el "L'Albatros artificiel", el cual, para despegar, fue arrastrado por caballos en la playa. Según afirmó, alcanzó una altura de 100 metros y recorrió una distancia de 200.
En 1866, un campesino y carpintero polaco llamado Jan Wnęk construyó y voló un planeador controlable. Wnęk era analfabeto y autodidacta, y todos los conocimientos y deducciones sobre los planeadores los obtuvo mediante la observación del vuelo de los pájaros y gracias a sus habilidades. Jan Wnęk estaba atado con firmeza a su planeador por el pecho y las caderas y lo controlaba mediante giros de las alas. Para probarlo, se lanzó desde la torre de la iglesia de Odporyszów, a 45 metros de altura, y esta a su vez situada sobre una colina de 50 metros, haciendo que la altura relativa fuera de 95 metros hasta el valle. Realizó varios vuelos con público entre 1866 y 1869, especialmente durante festivales religiosos, carnavales y celebraciones de año nuevo, pero apenas hubo constancia de los hechos de Jan Wnęk, y estos no tuvieron impacto en el progreso de la aviación.

En esa época, Frank Wenham intentó construir una serie de planeadores, pero no tuvieron éxito. En sus esfuerzos, descubre que la mayor parte de la sustentación de un pájaro parecía ser generada en la parte frontal, y Wenham dedujo que unas alas finas, largas y fijas, semejantes a las alas de los aviones actuales, serían más eficientes que las alas similares a las de pájaros o murciélagos. Su trabajo fue presentado en la recién creada Royal Aeronautical Society de Gran Bretaña en 1866, y Wenham decidió probar sus ideas construyendo el primer túnel de viento del mundo, en 1871. Los miembros de la sociedad hicieron uso del túnel y quedaron sorprendidos y encantados con el resultado: las alas fijas generaban sensiblemente más sustentación que lo que los científicos habían previsto. Este experimento claramente demostró que la construcción de máquinas más pesadas que el aire era posible, el problema era como generar el empuje necesario para mover el aparato hacia delante, ya que habían comprobado que las aeronaves de ala fija precisaban de un flujo de aire constante pasando por las alas, y aún hacía falta poder tener el control de la aeronave en vuelo.

En 1874, Félix du Temple construyó un planeador realizado con aluminio, en Brest (Francia), al que denominó "Monoplane". Contaba con una envergadura de 13 metros y un peso de 80 kilogramos sin contar al piloto, además de ser autopropulsado. Realizó varias pruebas, y al parecer consiguió despegar gracias a una rampa, y lograr después un aterrizaje seguro, realizando el primer vuelo autopropulsado de la historia, aunque fuera durante un breve espacio de tiempo y la distancia recorrida fuera escasa.

La década de 1880 fue un tiempo de estudios intensos, caracterizados por los "gentleman scientists", científicos que disponían de recursos necesarios para investigar de manera independiente sin tener que depender de financiación ajena, que hicieron la mayor parte de las investigaciones en el campo de la aeronáutica hasta la llegada del siglo XX. Se realizaron un gran número de avances que harían posible disponer de los primeros planeadores prácticos. Tres nombres en particular aportaron grandes conocimientos: Otto Lilienthal, Percy Pilcher y Octave Chanute.

Uno de los primeros planeadores modernos fue construido en Estados Unidos por John Joseph Montgomery, que voló en su máquina el 28 de agosto de 1883, en un vuelo controlado. Pero tuvo que pasar mucho tiempo para que los trabajos de Montgomery fueran conocidos. Otro planeador fue construido por Wilhelm Kress en 1877 en Viena.

El alemán Otto Lilienthal continuó el trabajo de Frank Wenham, publicando sus investigaciones en 1889. Lilienthal también fabricó una serie de planeadores, y en 1891 fue capaz de hacer vuelos sustentados logrando recorrer más de 25 metros, mejorando intentos anteriores que presentaban resultados inestables. El alemán documentó rigurosamente su trabajo, incluso con fotografías, y por esa razón, es uno de los pioneros de la aviación más conocidos. También promovió la idea de "salta antes de que alces el vuelo", sugiriendo que los investigadores deberían comenzar con planeadores y después intentar trabajar en proyectos para desarrollar un avión, en vez de diseñar tal avión directamente en un papel y esperar a que ese diseño funcione.

Lilienthal realizó con éxito varios vuelos hasta 1896, año en el que falleció en un accidente aéreo el 9 de octubre, causado por un viento lateral repentino, que rompió un ala de su aeronave en pleno vuelo, haciendo que se precipitara desde una altura de 17 metros. Por todo eso, Lilienthal es considerado la primera persona que realizó un vuelo planeado controlado, en el cual era el piloto el que controlaba a la aeronave. Sus últimas palabras antes de morir, al día siguiente, fueron: «Deben hacerse sacrificios».

En esos momentos, Lilienthal estaba trabajando en busca de pequeños motores adecuados para equipar a sus aeronaves, con la idea de crear un prototipo más pesado que el aire y capaz de alzar el vuelo por medios propios.

Octave Chanute continuó el trabajo de Lilienthal en el área de los planeadores. Creó varios prototipos e incluyó mejoras en sus aeronaves. En el verano de 1896, realizó varios vuelos sobre sus planeadores en Miller Beach (Indiana, Estados Unidos), y decidió que el mejor de todos ellos era un biplano. Al igual que Otto Lilienthal, Chanute documentó detalladamente su trabajo, y también fotografió sus máquinas y experimentos. Durante sus investigaciones, dedicó parte de su tiempo a comunicarse mediante correspondencia con personas que tenían sus mismos intereses, entre ellas Percy Pilcher. Chanute estaba particularmente interesado en solucionar el problema de cómo proporcionar estabilidad a la aeronave cuando esta estuviese en vuelo. Esa estabilidad se conseguía de manera natural en pájaros, pero tenía que ser realizada manualmente en el caso de humanos. Dentro de los problemas relacionados con la estabilidad del biplano en vuelo, el más desconcertante era la estabilidad longitudinal, ya que el ángulo de ataque del ala, hacía que el centro de presión de la aeronave se incrementara e hiciese que el ángulo del biplano aumentase todavía más, y entrara en pérdida.

En el siglo XIX se realizaron algunos intentos de producir un avión que despegase por medios propios. Pero la mayoría de ellos eran de pésima calidad, construidos por personas interesadas en la aviación pero que no tenían los conocimientos de los problemas que trataron Lilienthal y Chanute.

En 1843, William Henson, un inventor inglés, registró la primera patente de una aeronave equipada con motores, hélices, y provista de un ala fija, lo que en la actualidad se conoce como avión. Pero el prototipo construido basándose en los diseños de Henson no tuvo buenos resultados, y desistió en su proyecto. En 1848, su amigo John Stringfellow construyó una pequeña aeronave basada en los diseños de Henson, que tuvo éxito en ciertos aspectos, pudiendo despegar por medios propios, pero lo hacía sin piloto, y podía volar apenas dos o tres segundos.

En 1890, Clément Ader, un ingeniero francés, construyó un avión al que llamó "Éole", equipado con un motor a vapor. Ader consiguió despegar en el Éole, pero no consiguió controlar el aparato, y solo pudo recorrer unos 50 metros en el aire. Aun así, consideró los resultados satisfactorios, y se planteó construir una aeronave mayor, cuya construcción le llevó cinco años de su vida. Pero por desgracia, su nuevo avión, denominado "Avión III" era demasiado pesado y nunca fue capaz de despegar.

En 1884 el ruso Aleksandr Mozhaiski diseñó y creó un monoplano con el que logró despegar gracias a un motor a vapor y recorrer una distancia de entre 20 y 30 metros.

En esa época, Hiram Stevens Maxim, un estadounidense nacionalizado británico, estudió una serie de diseños en Inglaterra, y construyó un avión de dimensiones monstruosas para los patrones de la época. Era un biplano de 3175 kg y con una envergadura de 32 metros, equipado con dos motores a vapor, cada uno capaz de generar 180CV. Maxim construyó la aeronave para estudiar los problemas básicos de la aerodinámica y la potencia. Observó que el aparato, sin equipamientos que ayudasen a obtener su control, sería insegura y peligrosa a cualquier altitud, entonces construyó una pista especial, de 550 metros de longitud, donde colocó unos raíles en los que se situaba el avión para realizar pruebas. Las primeras pruebas las realizó en busca de problemas, y a partir del 31 de julio de 1894 comenzó a incrementar la potencia de los motores en cada prueba, alineando el aparato en la pista. Las dos primeras tuvieron un éxito razonable, el aparato consiguió "saltar" sobre los raíles durante unos segundos, pero no llegó a volar. En la tercera prueba, la tripulación aplicó potencia máxima a los motores del avión, hasta alcanzar 68 km/h, y después de recorrer 180 metros se produjo tanta sustentación que el avión se salió de los raíles, consiguiendo despegar y volar recorriendo 60 metros, momento en el cual el aparato chocó contra el suelo. Maxim solamente volvió a hacer nuevas pruebas en la década de 1900, usando motores a gasolina y aeronaves menores.

Otro pionero de la aviación fue Samuel Pierpont Langley, un científico estadounidense, que después de una exitosa carrera relacionada con la astronomía, comenzó a estudiar seriamente la aerodinámica en lo que actualmente es la Universidad de Pittsburgh (Estados Unidos). En 1891, Langley publicó "Experiments in Aerodynamics" ("Experimentos en aerodinámica"), donde detallaba sus investigaciones, y es a partir de ahí cuando se dedicó a diseñar y construir aeronaves basadas en sus ideas. El 6 de mayo de 1896, un prototipo construido por él, realizó su primer vuelo con éxito. El nombre de la aeronave era "Aerodrome No.5". El avión recorrió aproximadamente mil metros a una velocidad de 40 km/h. El 28 de noviembre del mismo año, realizó otro vuelo con éxito, con el aparato "Aerodrome No.6", que consiguió recorrer con éxito 1460 metros, pero despegaba sin tripulantes.

Después de los éxitos de estas pruebas de vuelo, Langley decidió construir un avión que fuese capaz de volar pilotado por una persona, por lo que comenzó a buscar personas dispuestas a invertir en su nueva máquina. Es entonces cuando el gobierno estadounidense le subvencionó con cincuenta mil dólares, gracias al interés que despertaba la idea de disponer de un aparato que sirviera como observador militar aéreo, ya que en ese momento se iniciaba la guerra hispano-estadounidense. Langley construyó entonces su "Aerodrome A", y pasó a realizar pruebas en una versión idéntica pero con un cuarto de tamaño con respecto al modelo original, y sin tripulantes. El prototipo voló dos veces el 18 de julio de 1901, realizando con éxito hasta 1903 algunos despegues más.

Con el diseño básico de la aeronave aparentemente aprobado en las pruebas realizadas, Langley acreditaba que el "Aerodrome A" estaba en condiciones de ser probado con un tripulante a bordo. Entonces comenzó a buscar un motor adecuado, y contrató a Stephen Balzser para la construcción de este. Langley quedó decepcionado al ver que el motor generaba apenas 8CV de fuerza, en vez de los 12CV que él esperaba. Un asistente de Langley, Charles M. Manly, rediseñó el motor, transformándolo en uno con cinco cilindros y refrigerado por agua, capaz de generar 52CV y 950 revoluciones por minuto, con un peso de 57kg.

El 7 de octubre y el 8 de diciembre de 1903, Langley, a los mandos del "Aerodrome A", intentó hacer que su avión despegara. Realizó sus intentos en un navío sobre el río Potomac, y utilizó una catapulta para proporcionar el empuje necesario para el despegue. Pero por desgracia, el avión era muy frágil, y en ambos intentos el avión terminó chocándose con el agua justo después de despegar. Además de eso, el avión no disponía de control longitudinal ni tampoco de tren de aterrizaje, y por eso tenía que realizar los intentos de despegue sobre el río. Otro problema era que los fondos monetarios de los que disponía se agotaban, por lo que intentó conseguir más, pero sus esfuerzos fracasaron.

Por toda la labor realizada dentro del mundo de la aviación, Langley fue reconocido por el Instituto Smithsoniano, una institución educacional ubicada en Washington D. C., como el inventor del avión, gracias a que Glenn Hammond Curtiss posteriormente haría varias modificaciones en el "Aerodrome A" de Langley en la década de 1910, y conseguiría alzar el vuelo.

Mientras, en el Reino Unido, Percy Pilcher estuvo a punto de convertirse en la primera persona que alza el vuelo en un avión. Pilcher construyó varios planeadores: "The Bat" ("El Murciélago"), "The Beetle" ("El Escarabajo"), "The Gull" ("La Gaviota") y "The Hawk" ("El Halcón"). Logró alzar el vuelo en todos ellos, teniendo éxito en sus intentos. En 1899 construyó un prototipo de avión con motor a vapor, pero por desgracia Pilcher falleció en un accidente aéreo con uno de sus planeadores, no habiendo probado su prototipo. Sus trabajos permanecieron escondidos durante años, y solo mucho tiempo después, despertaron interés en la comunidad científica. Estudios más recientes indicaron que su prototipo hubiera sido capaz de alzar el vuelo por sus propios medios con un tripulante a bordo.

Otro nombre digno de destacar es el de Gustave Whitehead, del que se tiene documentado un primer vuelo ocurrido el 14 de agosto de 1901 en Connecticut (Estados Unidos), día en el que logró volar con su modelo "Número 21" en tres ocasiones. La información salió reflejada en los periódicos "Bridgeport Herald", "New York Herald" y el "Boston Transcript", y en ellos se dice que el vuelo más largo logró recorrer más de 2500 metros a una altura de 60 metros, siendo mayor que la marca alcanzada por los hermanos Wright dos años más tarde.

Meses después, en enero de 1902 logró volar 10 kilómetros sobre Long Island en su modelo "Número 22". Pero antes de eso, algunos testigos confirman un vuelo de 1 km hacia el año 1899. Tanto el modelo "Número 21" como el "Número 22" eran monoplazas, el primero impulsado con un motor de 15CV y el segundo con un motor de 30 CV. El motor aceleraba las ruedas delanteras para adquirir la velocidad de despegue y el piloto cambiaba la fuerza hacia las hélices. De esta forma se evitaba el mecanismo de catapulta necesario en el modelo de los hermanos Wright.

Los planos de los modelos de Whitehead han sido conservados y en 1937, Stella Randolph recopiló su labor en la obra "Los vuelos perdidos de Gustave Whitehead". El reconocimiento a Gustave Whitehead solo vendría a partir de esa época.

Durante la década de 1890, los hermanos Wilbur y Orville Wright empezaron a interesarse por el mundo de la aviación, especialmente con la idea de fabricar y hacer volar una aeronave más pesada que el aire, que pudiese despegar por medios propios. En esa época, ambos administraban una fábrica de bicicletas en Dayton (Ohio, Estados Unidos), y comenzaron a leer y estudiar con gran interés, libros y documentos relacionados con la aviación. Siguiendo el consejo de Lilienthal, en el año 1899 empezaron a fabricar planeadores. A finales de siglo, comenzaron a realizar sus primeros vuelos con éxito con sus prototipos, en Kitty Hawk (Carolina del Norte), lugar elegido debido a que en esa zona podían encontrar vientos constantes, que soplaban también en una misma dirección, facilitando así los vuelos con planeadores. Además de eso, la zona disponía de un suelo plano, que hacía más fáciles los aterrizajes.

Después de la realización de varias pruebas y vuelos con planeadores, los Wright decidieron en 1902 ponerse a fabricar un avión más pesado que el aire. Se convirtieron en el primer equipo de diseñadores que realizaron pruebas serias para intentar solucionar problemas aerodinámicos, de control y de potencia, que afectaban a todos los aviones fabricados en esa época. Para la realización de un vuelo con éxito, la potencia del motor y el control del aparato serían esenciales, y al mismo tiempo el aparato precisaba ser bien controlado. Las pruebas fueron difíciles, pero los Wright fueron perseverantes. Al mismo tiempo, fabricaron un motor con la potencia deseada y solucionaron los problemas de control de vuelo a través de una técnica denominada alabeo, poco usada en la historia de la aviación pero que funcionaba en las bajas velocidades a las que el avión volaría.

El avión que fabricaron los hermanos Wright era un biplano al que denominaron "Flyer" ("Volador"). El piloto permanecía echado sobre el ala inferior del avión, mientras que el motor se situaba a la derecha de este, y hacía girar dos hélices localizadas entre las alas. La técnica del alabeo consistía en cuerdas atadas a las puntas de las alas, de las que el piloto podía tirar o soltar, permitiendo al avión girar a través del eje longitudinal y vertical, lo que permitía que el piloto tuviera el control del avión. El "Flyer" fue el primer avión registrado en la historia de la aviación, dotado de maniobrabilidad longitudinal y vertical, excluyendo a los planeadores de Lilienthal, donde el control era realizado a través de la fuerza del propio tripulante.
El 17 de diciembre de 1903, apenas unos meses después de las pruebas sin éxito de Langley, Orville Wright se convirtió en la primera persona en volar sobre una aeronave más pesada que el aire, propulsada por medios propios, aunque no sin controversias. El vuelo sucedió en Kitty Hawk. Los hermanos utilizaron rieles para mantener el aparato en su trayecto, y una catapulta para impulsarlo. El avión ganó altitud al acabar el recorrido sobre los raíles, recorriendo 37 metros a una velocidad media de 48 km/h durante los 12 segundos que duró el vuelo. Ese mismo día realizaron tres vuelos, que fueron presenciados por cuatro socorristas y un niño de la zona, haciendo que estos fueran los primeros vuelos públicos y documentados. En un cuarto vuelo realizado el mismo día, Wilbur Wright consiguió recorrer 260 metros en 59 segundos. Algunos periódicos del estado de Ohio, entre ellos el "Cincinnati Enquirer" y el "Dayton Daily News" publicaron el día siguiente la noticia del acontecimiento.

Los hermanos Wright realizaron diversos vuelos públicos (más de 105) entre 1904 y 1905, esta vez en Dayton, Ohio, invitando a amigos y vecinos. En 1904, una multitud de periodistas se reunió para presenciar uno de los vuelos de los Wright, pero a causa de problemas técnicos en su avión, que no pudieron corregir en dos días, Orville y Wilbur fueron ridiculizados por los medios, pasando a recibir poca atención, con la excepción de la prensa de Ohio. Varios periodistas de ese estado, presenciaron diversos vuelos suyos, incluyendo el primer vuelo circular del mundo, y un nuevo récord de distancia, ya que durante un intento el 5 de octubre de 1905 recorrieron 39 kilómetros en 40 minutos. A partir de 1908, los aviones de los hermanos Wright ya no necesitaron más la catapulta para alzar el vuelo.

El 7 de noviembre de 1910, realizaron el primer vuelo comercial del mundo. Este vuelo, realizado entre Dayton y Columbus (Ohio), duró una hora y dos minutos, recorriendo 100 kilómetros y rompiendo un nuevo récord de velocidad, alcanzando los 97 km/h.

El brasileño Alberto Santos Dumont estaba fascinado por las máquinas. En 1891 se mudó con su padre a París, donde quedó maravillado por el mundo de la aviación. Realizó sus primeros vuelos como pasajero en globo y posteriormente creó su propio globo, el "Brésil" ("Brasil" en francés). Santos Dumont también creó una serie de modelos de dirigibles, de los que algunos lograron volar con éxito pero otros no. Los hechos realizados por Santos Dumont en París lo convirtieron en una persona famosa en esa ciudad.
El 13 de septiembre de 1906, Santos Dumont realizó un vuelo público en París, en su famoso avión, el "14-bis". Este aparato usaba el mismo sistema de alabeo empleado en las aeronaves de los hermanos Wright y logró recorrer una distancia de 221 metros. El "14-bis", al contrario que el "Flyer" de los Wright, no necesitaba raíles, catapultas o viento para alzar el vuelo y, como tuvo mucha repercusión mediática en aquel momento, el vuelo es considerado por algunas personas como el primero realizado con éxito de un avión. Cuando se realizó este vuelo, poco o nada se sabía de los hermanos Wright, por lo que la prensa internacional consideró al "14-bis" de Santos Dumont como el primer avión capaz de despegar por medios propios.

Santos Dumont, después del "14-bis", inventaría el primer ultraligero, el "Demoiselle", que fue el último aparato que desarrollaría. También realizó importantes avances relacionados con el control del avión en vuelo y de los alerones de sus aeronaves.

Existe gran controversia en lo relativo a la realización del primer vuelo. Generalmente hay dos opiniones, los que consideran como autor de esta hazaña a los hermanos Wright (concretamente a Orville Wright) y los que consideran a Alberto Santos Dumont. Este último realizó en París el vuelo del "14-bis", el primero de un avión en la historia de la aviación que se logra sin artificios externos y que queda registrado y publicado. Los especialistas alegan el uso de raíles y catapultas en las pruebas de despegue de los hermanos Wright, y el testimonio de vuelo del "14-bis" en París por aviadores y autoridades de aviación.

En cuanto a esto, los hermanos Wright no realizaron muchos vuelos públicos, ya que pretendieron realizar sus vuelos solos o con la presencia de pocos testigos, aunque habían intentado realizar demostraciones para las fuerzas armadas de los Estados Unidos, de Francia, del Reino Unido y de Alemania, todas sin éxito, con la intención de evitar el robo de informaciones por parte de otros aviadores, y en busca de perfeccionar el aparato lo suficiente como para obtener la patente de su avión (irónicamente, Santos Dumont ponía todas sus invenciones en el dominio público).

Algunos especialistas en aviación acreditan que los hermanos Wright fueron los primeros en volar en un avión más pesado que el aire. A pesar de la falta de informes de aviadores y de organizaciones de aviación, los mismos especialistas también apuntan en el hecho de que, a través de las noticias publicadas en periódicos de Ohio, el testimonio de habitantes de la región donde estos vuelos se realizaron y las fotos de estos vuelos, demuestran que estos vuelos ocurrieron, pero las aeronaves no despegaban por sí solas, sino que utilizaban artefactos que las catapultaban, haciendo que el vuelo de Santos Dumont sea considerado como el primero en la historia de la aviación, a pesar de haber ocurrido algunos años después de los primeros vuelos de los hermanos Wright.

De hecho, los Wright son acreditados en los Estados Unidos, como los primeros en volar en un avión. Sus primeros vuelos públicos, realizados en presencia de un gran número de testigos, fueron realizados en 1908 en Le Mans (Francia).

Santos Dumont es considerado el inventor del avión en la mayor parte del mundo, donde es llamado "el padre de la aviación". Varias personas, sin embargo, critican ese título, alegando que otros aviadores hicieron sus contribuciones en el mundo de la aviación mucho tiempo antes de Santos Dumont o de los Wright, y que ese título no debería emplearse con ningún aviador en particular.

Varios aviadores afirmaron haber volado en un avión con anterioridad a los vuelos de los hermanos Wright y de Santos Dumont, volviendo todavía más controvertido el primer vuelo de la historia en un avión. Esta controversia fue alimentada por los hermanos Wright, que permanecieron distanciados de los medios de comunicación mientras preparaban la patente de su avión, por lo que fueron poco conocidos en su momento por la comunidad de la aviación mundial, y también por el gran número de posibles primeros vuelos en un avión, por las diferentes categorías y cualificaciones de las aeronaves y de los medios utilizados para lograr tales vuelos, por la falta de testigos creíbles y por el orgullo y patriotismo de las naciones de estos aviadores.

En España, Diego Marín Aguilera, en la noche de 15 de mayo de 1793, realizó un vuelo de 360 metros con un artefacto de hierro y plumas de ave, controlado por el propio piloto. Tan solo quedan algunos testimonios y un tardío reconocimiento.

Gustave Whitehead afirmó haber volado en una aeronave más pesada que el aire, por medios propios, el 14 de agosto de 1901. Cometió el error de no documentar su supuesto vuelo, pero posteriormente, una réplica de su avión denominado "Número 21" consiguió alzar el vuelo con éxito. El estadounidense Lyman Gilmore también dijo haber volado el 15 de mayo de 1902.

En Nueva Zelanda, el granjero e inventor Richard Pearse construyó un monoplano que alzó el vuelo el 31 de marzo de 1903. Hay grandes evidencias que dicen que eso ocurrió realmente, entre relatos y fotografías. Pero el propio Pearse admitiría tiempo después que ese vuelo no fue controlado y que terminó al chocarse en un monte después de haber volado a una altura de unos 3 metros. El alemán Karl Jatho voló en una aeronave más pesada que el aire el 18 de agosto de 1903. Su vuelo fue de corta duración, pero con la velocidad y el diseño de las alas que poseía, hacían que el avión no fuera controlable por el piloto. Todavía en 1903, hubo testigos que afirmaban haber visto al escocés Preston Watson realizar vuelos en Errol, al este de Escocia. Pero a falta de evidencias fotográficas o documentadas, hacen que sea imposible su verificación.

El ingeniero rumano Traian Vuia también afirmó haber volado en un avión, y que logró despegar y mantenerse en el aire durante un tiempo razonable, y sin ayuda de ningún elemento. Vuia pilotó el avión que él mismo diseñó y construyó, el 18 de marzo de 1906 en Montesson, cerca de París. Ninguno de sus vuelos superó los 30 metros de distancia. En comparación, a finales de 1905 los hermanos Wright ya habían realizado vuelos de 39 kilómetros de distancia y de 40 minutos de duración.Sin embargo, los Wright tuvieron que emplear una catapulta para lograr el despegue.

Muchas reivindicaciones de vuelos son complicadas de demostrar por el hecho de que alcanzaron tan poca altura que los aviones se confundían con el suelo. Además de eso, forma parte del debate también los medios utilizados para alzar el vuelo. Algunos alzaron vuelo completamente por medios propios, pero hubo otros que inicialmente eran catapultados en el despegue, y en el aire se sustentaban por medios propios. Por todo esto, Alberto Santos Dumont y los hermanos Wright son considerados en el mundo entero como los primeros en volar en un avión, ya que hay abundancia de pruebas de sus vuelos.

Durante estos años, dos inventores, el francés Henri Farman y el inglés John William Dunne, también estaban trabajando por su cuenta en sus propios prototipos de aviones.

En enero de 1908, Farman ganó el "Grand Prix" de la aviación, con un avión que recorrió un kilómetro, aunque antes ya se habían realizado vuelos que habían recorrido más distancia, como el de los hermanos Wright en 1905, recorriendo un total de 39 kilómetros. Más tarde, el 30 de octubre de 1908, Farman se convirtió en el primero en realizar un vuelo de ciudad a ciudad, realizado desde el pequeño pueblo de Bouy y Reims, ambas en Francia (27 kilómetros en 20 minutos). El 27 de agosto de 1909 volvió a batir otro récord, llegando a recorrer 180 kilómetros en poco más de tres horas en su avión, el "Farman III", y más tarde 232 kilómetros en cuatro horas, 17 minutos y 53 segundos en ese mismo aparato.
Los trabajos iniciales de Dunne fueron patrocinados por las Fuerzas Armadas del Reino Unido, y probados en Glen Tilt (Tierras Altas de Escocia). Su mejor diseño fue el "D4", que voló en diciembre de 1908, cerca de Blair Atholl, en Pertshire. Sus principales contribuciones a la historia de la aviación fueron en lo relativo a la estabilidad de las máquinas, que era uno de los principales problemas a los que se enfrentaron inicialmente los pioneros de la aviación.

El 14 de mayo de 1908, Wilbur Wright realizó el primer vuelo de un avión cargado con dos personas, portando a Charles Furnas como pasajero.

El 17 de septiembre de 1908, el estadounidense Thomas Etholen Selfridge se convirtió en la primera persona en morir en un avión en vuelo, cuando Wilbur Wright estrelló su avión de dos pasajeros en una de las pruebas militares que realizó en Fort Myer (Virginia, Estados Unidos). También en 1908, Hart O. Berg se convirtió en la primera mujer en volar, haciéndolo como pasajera junto a Wilbur Wright en Le Mans (Francia).

El 25 de julio de 1909, el ingeniero francés Louis Blériot se convirtió en la primera persona que a bordo de un aeroplano atravesó el canal de la Mancha. Pilotando su avión "Blériot XI", y partiendo desde la localidad francesa de Calais, tras 37 minutos en el aire logró aterrizar cerca de Dover, ya en territorio británico. Gracias a su hazaña, Blériot ganó el premio de 1000 libras esterlinas que ofreció el periódico inglés "Daily Mail" a la primera persona que lo lograra.

El 8 de marzo de 1910, la baronesa de Laroche fue la primera mujer en conseguir la licencia de piloto. Había realizado su primer vuelo el 22 de octubre de 1909.

El 23 de septiembre de 1910, el aviador peruano-francés Jorge Chávez Dartnell junto a su avión "Blériot XI" logró superar por primera vez los Alpes desde Brig (Suiza) hasta Domodossola (Italia) donde a 20 metros de altura el avión cayó en picado después de que las alas se quebraran debido al fuerte viento. Herido de gravedad, Chávez murió cuatro días después.

En 1911, Calbraith Perry Rodgers se convirtió en la primera persona en hacer un viaje transcontinental con un avión, viajando desde Sheepshead Bay (Nueva York), a orillas del océano Atlántico, hasta Long Beach (California), a orillas del océano Pacífico, en una serie de vuelos cortos que le llevarían un total de 84 días.

Al mismo tiempo que se desarrollaban los aviones de ala fija, los dirigibles se volvían cada vez más avanzados. Durante las primeras décadas del siglo XX, los dirigibles eran capaces de transportar mucha más carga y pasajeros que los aviones. Muchos de los avances relacionados con los dirigibles fueron obra del conde alemán Ferdinand von Zeppelin.

La construcción del primer dirigible Zeppelin comenzó en 1899 en Alemania. El prototipo inicial, denominado "LZ1" (siglas en alemán de "Luftschiff Zeppelin 1"), tenía 128 metros de longitud y era propulsado por dos motores Daimler de 14.2CV cada uno. El primer vuelo del "LZ1" ocurrió el 2 de julio de 1900, durando apenas 18 minutos, debido a que se vio obligado a descender debido a que el mecanismo de control había sufrido un fallo mecánico. Después de repararlo, el Zeppelin pudo mostrar todo su potencial en los siguientes vuelos, sobrepasando el récord de 6m/s del dirigible "La France" por un margen de 3m/s, pero aun así, no logró atraer a posibles inversores. Tuvieron que pasar unos años hasta que Ferdinand von Zeppelin reuniera fondos suficientes para seguir sus pruebas.

En 1902, el ingeniero español Leonardo Torres Quevedo desarrolló un nuevo tipo de dirigible que solucionaba el grave problema de suspensión de la barquilla al incluir un armazón interior de cables flexibles que dotaban de rigidez al dirigible por efecto de la presión interior, combinando las propiedades de los dirigibles rígidos y flexibles. Tres años después, junto a Alfredo Kindelán, Torres Quevedo construye el primer dirigible español, denominado "España", que se caracterizaba por disponer de un globo separado en tres compartimentos (trilobulado), lo que aumentaba la seguridad. A raíz de este hecho empezó la colaboración entre Torres Quevedo y la empresa francesa Astra, que llegó a comprarle la patente con una cesión de derechos extendida a todos los países, excepto a España, para posibilitar la construcción del dirigible en el país. Así, en 1911, se inicia la fabricación de los dirigibles conocidos como Astra-Torres. Algunos ejemplares fueron adquiridos por los ejércitos francés e inglés a partir de 1913, y utilizados durante la Primera Guerra Mundial en muy diversas tareas, fundamentalmente de protección e inspección naval.

En 1918, Torres Quevedo diseñó, en colaboración con el ingeniero español Emilio Herrera Linares, un dirigible trasatlántico, al que llamaron "Hispania", que llegó a alcanzar el estado de patente, con objeto de realizar desde España la primera travesía aérea del océano Atlántico. Por problemas de financiación el proyecto se fue retrasando y fueron los británicos John William Alcock y Arthur Whitten Brown los que lograron esa hazaña por primera vez, en el año 1919.

En 1877 el italiano Enrico Forlanini desarrolló un prototipo no tripulado de helicóptero, de unos 13 metros de altura y alimentado con un motor a vapor. Fue el primero de su tipo. Logró un despegue vertical y permaneció en el aire unos 20 segundos, aunque el primer vuelo realizado con éxito y registrado de un helicóptero ocurrió en 1907, realizado por Paul Cornu en Francia, pero hasta 1936 con el Focke-Wulf Fw 61 de fabricación alemana, no se dispuso de un helicóptero funcional.

El autogiro fue inventado por el ingeniero español Juan de la Cierva, quien desarrolló el rotor articulado que más tarde usaría Igor Sikorsky en sus helicópteros, pagando incluso la patente y los derechos de utilización al inventor español. En su primer vuelo en 1923, el autogiro logró recorrer 200 metros, y más tarde, realizó el primer viaje entre aeródromos desde Getafe a Cuatro Vientos en 1924.

En cuanto a hidroaviones, el primero de la historia fue obra del ingeniero francés Henri Fabre. Lo denominó "Le canard" (en francés, ‘el pato’), y el 28 de marzo de 1910 despegó del agua y logró recorrer 800 metros. Sus experimentos fueron seguidos de cerca por Charles y Gabriel Voisin, que adquirieron varios de sus prototipos para desarrollar el suyo propio, al que denominaron "Canard voisin". En octubre de 1910, el "Canard voisin" se convirtió en el primer hidroavión que voló sobre el río Sena, en París, y en marzo de 1912 se convirtió también en el primer hidroavión que fue usado militarmente desde el portaaviones francés "La Foundre" (en francés, ‘el relámpago’).

No mucho después de haber sido inventado, el avión pasó a ser usado en servicios militares. El primer país que usó aviones con ese propósito fue Bulgaria, en ataques sobre posiciones otomanas durante la Primera Guerra de los Balcanes.

Pero la primera guerra en la que se usaron aviones en misiones de ataque, defensa y de reconocimiento fue en la Primera Guerra Mundial. Los Aliados y las Potencias Centrales hicieron un uso extensivo de los aviones. Irónicamente, la idea del uso de aviones como arma de guerra antes de la Primera Guerra Mundial fue motivo de risas y mofas por parte de muchos comandantes militares, durante los tiempos que precedieron a la guerra.

La tecnología relacionada con la aviación avanzó rápidamente debido a la guerra. Al principio de esta, los aviones apenas podían cargar con el piloto, pero después de muchas mejoras, se pudo añadir a un pasajero adicional. Los ingenieros crearon motores más potentes, y se fabricaron aeronaves cuya aerodinámica era sensiblemente mejor que el de las de antes de la guerra. Como comparación, al inicio de la guerra los aviones no superaban los 110 km/h, sin embargo al finalizar la contienda, muchos ya alcanzaban los 230 km/h o incluso más.

Después del comienzo de la guerra, los comandantes militares descubrieron la importancia que tenía el avión como arma de espionaje y reconocimiento, pudiendo fácilmente localizar fuerzas y bases enemigas sin mucho peligro, hasta que se empezó a desarrollar el armamento antiaéreo según iba avanzando la guerra.

Pero el uso de los aviones que realizaban patrullas de reconocimiento generó un problema: estas frecuentemente se encontraban con aviones enemigos. Así que no se tardó mucho en equipar a esas aeronaves con armas de fuego a bordo, para que así pudieran defenderse, pero a la vez el piloto tenía que controlar el aparato, lo que complicaba la situación, por lo que algunos aviones contaban con un observador que podía apuntar y disparar una ametralladora que portaba en sus brazos, lo que resultaba poco eficaz hasta que se creó la ametralladora sincronizada.

Los franceses se esforzarían seriamente en resolver ese problema, y a finales de 1914, Roland Garros colocó una ametralladora fija al frente de su aeronave, permitiéndole disparar a la vez que controlaba el aparato, gracias a que cubría las hélices con una placa metálica que las blindaba. El 19 de abril de 1915 Garros fue derribado y hecho prisionero por los alemanes, y debido a que su avión no quedó destruido, el ingeniero Anthony Fokker estudió y mejoró el sistema, gracias a un mecanismo que sincronizaba el giro de la hélice con los disparos de la ametralladora, y que acabaría siendo equipado en todos los aviones, por lo que las batallas aéreas entre cazas pasaron a ser muy comunes. También se extendió el uso de hidroaviones, usándolos para misiones de reconocimiento en el mar, para poder captar fotografías de las fuerzas navales enemigas y para bombardear submarinos enemigos.
En esta época apareció la denominación as de la aviación, considerándose así a los pilotos que conseguían derribar en combate a cinco aeronaves enemigas o más. Muchos de ellos se convertirían en personajes famosos durante y después de la guerra. El más famoso fue el alemán Manfred von Richthofen, más conocido como "Barón Rojo", que logró abatir 80 aeronaves enemigas con diferentes aviones, aunque el más famoso fue el Fokker Dr.I que empleaba pintado de rojo. Fue abatido por un canadiense en 1918, poco antes de acabar la guerra.

El avión más famoso de la guerra fue el Sopwith Camel, que contaba con más victorias aéreas que cualquier otro avión aliado, pero también era conocido por su difícil manejo, responsable de la muerte de varios pilotos novatos. También de este periodo es el Junkers J 1, avión de fabricación alemana que se convirtió en el primer avión fabricado completamente de metal en 1915.

En el período de entreguerras se desarrolló toda la tecnología relacionada con la aviación, realizándose importantes avances en el diseño de aviones, y siendo el momento en el que comenzaron a operar las primeras líneas aéreas. También fue una época en la que los aviadores comenzaron a impresionar al mundo con sus hazañas y habilidades. Los aviones empezaron a sustituir la madera por el metal de manera generalizada. También los motores experimentaron un gran incremento de potencia. Esta serie de avances tecnológicos, junto con el creciente impacto socioeconómico que los aviones pasaron a tener, hicieron que el periodo entreguerras sea considerado como "la era de oro de la aviación". Todo esto fue posible en parte, gracias a la gran cantidad de aviones y pilotos que quedaban después de la Primera Guerra Mundial.

Una de las principales razones para explicar estos desarrollos fue la entrega de una serie de premios que se otorgaban a los aviadores que conseguían establecer récords de distancia recorrida y de velocidades alcanzadas. Un ejemplo de premio de estas características era el Premio Orteig, que premiaba con 25&nbsp000 dólares a la primera persona que realizara el trayecto Nueva York-París o viceversa, sin realizar escalas de ningún tipo. Este premio lo ganó Charles Lindbergh, que en su monoplano de un solo motor Ryan NYP (un Ryan M-2 modificado), bautizado como "Spirit of Saint Louis", despegó del aeródromo Roosevelt (Long Island, Ciudad de Nueva York) el 20 de mayo de 1927 y tras un vuelo de 33 horas y 32 minutos, aterrizó en el aeropuerto de Le Bourget, cercano a París. Pero Lindbergh no fue el primer aviador en realizar un vuelo trasatlántico sin escalas. John William Alcock y Arthur Whitten Brown, dos aviadores británicos, lograron volar años antes desde Lesters Field, cerca de Saint Johns, Nueva Escocia (Canadá), a Clifden (Irlanda), del 14 al 15 de junio de 1919 en su avión Vickers Vimy IV (un bombardero modificado). Por haber logrado esta hazaña, Alcock y Brown ganaron el premio de 10 000 libras esterlinas del periódico londinense "Daily Mail", recibiendo el premio de manos de Winston Churchill.
Además un año antes al vuelo de Lindbergh, el hidroavión español Plus Ultra, tripulado por Ramón Franco, Julio Ruiz de Alda, Juan Manuel Durán y Pablo Rada, cruzó el Atlántico Sur desde la localidad de Palos de la Frontera (Huelva, España) hasta Buenos Aires (Argentina), para emular el viaje de Colón, pero por aire, en vez de por agua.

En 1914, el estadounidense Tony Jannus se convirtió en el primer piloto de la historia que realizaba un vuelo comercial. Jannus pilotó un hidroavión para transportar carga y pasajeros entre San Petersburgo y Tampa, en Florida (Estados Unidos). Su hidroavión tenía espacio para un pasajero, que pagaba cinco dólares por un vuelo de 35 kilómetros. Este taxi aéreo, la Aerolínea St. Petersburg-Tampa Airboat considerado la primera línea aérea del mundo, en poco tiempo se encontró con dificultades financieras, por lo que duró apenas unos meses. En 1919 y durante los años 20, varias líneas aéreas se establecieron por Europa y los Estados Unidos. Estas compañías comenzaron usando aviones que previamente habían tenido un uso militar en la Primera Guerra Mundial, pero que habían reconvertido para poder transportar carga y pasajeros, y a los cuales se decoraba de una manera elegante por dentro. Aun así, estos aparatos resultaban muy ruidosos y no estaban apropiadamente presurizados ni acondicionados.

Después de la guerra, los gobiernos estadounidense y canadiense ofrecieron a precios bajos el exceso de aviones del que disponían, a aviadores. A pesar de que estas aeronaves eran más fuertes que las fabricadas antes de la guerra, aún no podían ser consideras seguras, ya que estaban realizadas la mayoría de las veces con madera y tejidos, y no disponían de equipamientos de navegación básicos. Aun así, muchos pilotos que antes habían luchado en la guerra, compraron esos aviones y los emplearon para ganar dinero, realizando exhibiciones acrobáticas y peligrosas en ferias, lo que hacía que los accidentes fueran frecuentes, y muchos de estos aviadores murieran.

La agencia de correos de Estados Unidos también empleó antiguos aviones militares para transportar correo entre algunas ciudades estadounidenses, hasta el año 1927, en el que dejaron de operar estos vuelos, prefiriendo contratar a líneas aéreas para que realizaran ese servicio. Los correos aéreos tuvieron mucha importancia en el desarrollo de la aviación comercial.

En 1929, la tecnología relacionada con los dirigibles avanzó de manera notable, llegando a realizar un Zeppelin el primer viaje alrededor del mundo, a los mandos de Ferdinand von Zeppelin. En esos años, los dirigibles eran usados por numerosas líneas aéreas de Europa, y en los años 30 se iniciaron las primeras rutas trasatlánticas, que tuvieron gran éxito. La era de los dirigibles terminó en 1937 cuando el dirigible Hindenburg sufrió un accidente en Lakehurst (Nueva Jersey, Estados Unidos), en el que murieron 35 personas. El suceso ocurrió debido a que el dirigible estaba lleno de hidrógeno, un gas altamente inflamable. Después de este acontecimiento, la gente dejó de usar los dirigibles, a pesar de que tal accidente fue el único sucedido en este tipo de aeronaves.

En la década de 1930, muchas líneas aéreas utilizaron hidroaviones que empleaban principalmente en vuelos transoceánicos. Uno de los mayores hidroaviones de la época fue el Dornier Do X, tan grande que necesitaba doce motores para despegar, seis en cada ala. Voló por primera vez en 1929, pero no fue demasiado popular. Otro hidroavión, el Boeing 314 Clipper, capaz de transportar 74 pasajeros, sí que resultó popular en esos años. En 1938 realizaron sus primeros vuelos comerciales sobre el océano Atlántico, pero el desarrollo de aviones cada vez más potentes y de aeropuertos con pistas cada vez más largas, hicieron que el uso de hidroaviones terminase a lo largo de los años 40.

En lo que respecta a otro tipo de aeronaves, en los años 20 el ingeniero español Juan de la Cierva y Codorníu comenzó a desarrollar una aeronave de ala rotativa que puede ser considerada un híbrido entre un avión y un helicóptero, y que recibió el nombre de autogiro. De la Cierva realizó su primer vuelo en un autogiro en 1923, recorriendo 200 metros, y un año después en otra prueba logró alcanzar los 100 km/h. El español siguió evolucionando su aparato en Inglaterra y Estados Unidos con apoyo de inversores particulares, y llegó a tener gran éxito con sus modelos en los primeros años 30. Pero con la llegada de la guerra civil española, de la Cierva muere y las investigaciones relativas al autogiro quedan prácticamente paralizadas, centrándose todos los esfuerzos en el desarrollo del helicóptero aprovechando las investigaciones y avances conseguidos por Juan de la Cierva con el autogiro, aparato que hoy en día es considerado como el precursor del helicóptero. Heinrich Focke en Alemania e Igor Sikorsky en Estados Unidos desarrollaron los primeros modelos operativos de helicópteros a finales de los años 30 y principios de los años 40, llegando a tener que comprar varias de las patentes del autogiro para desarrollar sus aparatos.

Años antes, otros pioneros realizaron avances en lo relativo a los helicópteros, como en eslovaco Ján Bahýľ a principios del siglo XX, el argentino Raúl Pateras Pescara, que realizó el primer vuelo de un helicóptero medianamente controlable en 1916, o el español Federico Cantero Villamil, que desarrolló uno de los primeros helicópteros eficaces, la Libélula Viblandi, pero la Guerra Civil Española paralizó sus proyectos.

Durante este periodo, y especialmente en la década de 1930, hubo varias mejoras técnicas que facilitaron la construcción de aviones más grandes, capaces de recorrer distancias mayores y de volar más rápido y a mayor altitud, lo que hizo que se pudiera transportar más carga y a más pasajeros. Los avances en la ciencia de la aerodinámica permitieron a los ingenieros desarrollar aeronaves cuyo diseño interfiriera lo menos posible en el vuelo del avión. Los equipamientos de control y las cabinas de los aviones también mejorarían de una manera considerable. Además de eso, las mejoras en la tecnología de las radiocomunicaciones permitían el uso de equipamientos de este tipo en los aviones, así los pilotos podían recibir instrucciones de vuelo desde equipos en tierra, y también se podrían comunicar pilotos de distintas aeronaves entre sí. Todo esto generó técnicas más precisas de navegación aérea. El piloto automático también comenzó a usarse en los años 30, lo que permitió a los pilotos tomarse cortos periodos de descanso en vuelos de larga duración.
El avión más característico de esta etapa fue el Douglas DC-3, un monoplano bimotor que realizó sus primeros vuelos en 1936. Tenía una capacidad para 21 pasajeros y era capaz de alcanzar una velocidad de crucero de 320 km/h. Rápidamente se convirtió en el avión comercial más usado de la época, y es considerado uno de los aviones más importantes que se ha producido en la historia de la aviación.

El motor a reacción comenzó a ser desarrollado en Inglaterra y Alemania en estos años. El británico Frank Whittle patentó un diseño de una turbina a reacción en 1930, y desarrolló un motor que podía ser usado para fines prácticos al final de la década. El alemán Hans von Ohain patentó su versión de motor a reacción en 1936, y comenzó a desarrollar una máquina semejante. Ninguno de ellos sabía del trabajo que desarrollaba el otro, por eso mismo, a ambos se les considera como sus inventores. A punto de terminar la Segunda Guerra Mundial, Alemania empleaba los primeros aviones de reacción y fabricaba una serie de Messerschmitt Me 262, el primer caza a reacción de la historia.

El hecho de que los aviones volasen a altitudes cada vez mayores, donde las turbulencias y otros factores climáticos no deseables son más raros, generó un problema: en altitudes mayores, el aire es menos denso, y por tanto, posee menores cantidades de oxígeno para la respiración. A medida que los aviones pasaban a volar más alto, los pilotos, tripulantes y pasajeros tenían cada vez más dificultades para respirar. Los especialistas, para resolver este problema, crearían la cabina presurizada, que lograba mantener constante la presión atmosférica con independencia de la altura de vuelo. Estas se empezaron a hacer populares a finales de los años 40, aunque el primer avión comercial con cabina presurizada fue el Boeing 307, que realizó su primer vuelo en 1938. Hoy en día, prácticamente todas las cabinas de aviones comerciales de pasajeros son cabinas presurizadas.


Los años de la Segunda Guerra Mundial se caracterizaron por un drástico crecimiento en la producción de aviones, y por el gran desarrollo de la tecnología relacionada con la aviación. En la siguiente tabla se puede comprobar el crecimiento exponencial en la producción de aviones en este periodo:
Durante el conflicto se desarrollaron los primeros bombarderos de larga distancia, el primer avión de reacción de uso práctico y el primer caza con reactores. Al inicio de la guerra, los cazas podían alcanzar velocidades máximas de 480 km/h y volar a una altura de 9000 metros. Al finalizar la guerra, después de todas las investigaciones y desarrollos realizados por ambos bandos, los cazas estaban volando a 640 km/h y muchos alcanzaban los 12 000 metros de altura.

Los cazas a reacción desarrollados a lo largo del conflicto podían desplazarse todavía más rápido, pero no se usaron hasta el final de la guerra. El primer reactor funcional fue el alemán Heinkel He 178, que realizó su primer vuelo en 1939, poco antes de empezar la guerra. Años después, en 1944, el Messerschmitt Me 262 se convirtió en el primer caza a reacción que operó en la guerra, y podía alcanzar una velocidad máxima de 900 km/h. Un prototipo alemán, el Messerschmitt Me 163 era capaz de alcanzar 970 km/h en vuelos cortos, y sirvió de base para el Messerschmitt Me 163 Komet, el caza más rápido de la guerra, que se empleó en algunas misiones al final de la guerra, en 1945. Los alemanes también crearon los primeros misiles balísticos de larga distancia, el V-1 y el V-2.

Los bombarderos de la Segunda Guerra Mundial eran capaces de cargar el doble de carga y recorrer el doble de distancia que los existentes antes de la guerra. Los bombarderos de larga distancia fueron los que causaron más impacto en el transcurso de la guerra, ya que los cazas a reacción comenzaron a operar al final de la guerra, y la derrota alemana era cuestión de tiempo. Los misiles V-1 eran ineficientes y los V-2 no fueron producidos en grandes cantidades. El caza estadounidense North American P-51 Mustang resultó clave junto a los bombarderos pesados, ya que les servían de protección frente a los cazas enemigos. Otros aviones famosos de la guerra fueron el caza británico Supermarine Spitfire, considerado como "el salvador del Reino Unido", el caza japonés Mitsubishi A6M Zero y el bombardero estadounidense Boeing B-29 Superfortress.

Después del fin de la Segunda Guerra Mundial, la aviación comercial pasó a desarrollarse de manera independiente a la aviación militar. Las empresas fabricantes de aviones pasaron a crear modelos especialmente diseñados para el transporte de pasajeros y, durante los primeros años después de la guerra, las líneas aéreas usaron aviones militares modificados para uso civil, o versiones derivadas de los mismos, entre los que cabría destacar el Boeing 377 Stratocruiser, que derivaba del Boeing C-97 Stratofreighter, y que se convirtió en el primer avión de dos pisos de la historia de la aviación, ya que su fuselaje denominado "de doble burbuja" permitía que en la parte superior albergara una cubierta con asientos, y en la inferior llevara una pequeña sala VIP a la que se accedía mediante una escalera de caracol, y que a la vez fue el mayor avión comercial hasta la llegada del Boeing 707 en 1958.

De las aeronaves comerciales que se desarrollaron en este periodo, destacan los cuatrimotores Douglas DC-4 y el Lockheed Constellation, que fueron usados para vuelos locales de pasajeros o de media distancia. También realizaron rutas transoceánicas, pero para estas necesitaban hacer escalas para reabastecerse de combustible. Los vuelos transoceánicos necesitaban de motores más potentes, que ya existían en 1945 en forma de turbinas a reacción, pero estos, en ese momento todavía consumían demasiado combustible y con ellas un avión solo podría recorrer pequeñas distancias.

Para resolver este problema, aunque fuera de manera temporal, se desarrollaron motores turbohélices, que eran propulsores capaces de generar más de tres mil caballos de fuerza. Estos motores comenzarían a ser empleados en los Vickers Viscount, Lockheed L-188 Electra o Ilyushin Il-18, aviones capaces de transportar entre 75 y 110 pasajeros entre las ciudades de Nueva York y París sin escalas y a una velocidad de crucero de más de 500km/h.

A finales de los años 40, los instructores comenzaro 
las turbinas usadas en los cazas a reacción producidos durante la Segunda Guerra Mundial. En un principio, los Estados Unidos y la Unión Soviética querían turbinas a reacción para producir bombarderos y cazas cada vez mejores, y así mejorar todavía más su arsenal militar. Cuando comenzó la guerra de Corea en 1950, tanto los Estados Unidos como la Unión Soviética disponían de cazas a reacción, entre los que destacaban el norteamericano North American F-86 Sabre y el soviético MiG-15.

En cuanto al primer avión de reacción de carácter comercial de la historia de la aviación, fue el De Havilland Comet de fabricación británica. El Comet comenzó su uso como avión de pasajeros en 1952, siendo capaz de volar a 850km/h, y con una cabina presurizada y relativamente silenciosa. Este avión comenzó siendo un éxito comercial, y muchas líneas aéreas hicieron pedidos. Pero dos accidentes ocurridos en 1954 en medio del mar, hicieron que surgieran grandes dudas en lo relativo a la seguridad del avión. La causa principal de los accidentes fue debida a la fatiga del metal alrededor de la ventanilla donde se alojaba la unidad de radio, y de las ventanillas que ambas eran cuadradas, que terminaron por sucumbir por las aristas a la presurizacion de la cabina. De ahí que desde entonces las ventanillas de los aviones son ovaladas, para disipar la energía alrededor de ellas. La compañía De Havilland intentó salvar su avión, cuyas ventas habían caído drásticamente, a través de algunas modificaciones estructurales, pero un tercer accidente ocurrido en 1956 hizo que de nuevo las ventas cayeran, y al final la producción cesó en 1964.

La norteamericana Boeing lanzó el Boeing 707 en 1958, el cual se convirtió en el primer avión de pasajeros a reacción que tuvo éxito. Los ingenieros que desarrollaron el modelo, dedicaron especial empeño en que los errores que se habían cometido en el De Havilland Comet no se dieran en el 707. Los modelos a reacción Douglas DC-8 y Convair 880 fueron lanzados algunos años después, aunque el éxito comercial que ambos modelos tuvieron fue más modesto que el que alcanzó el 707, del que se produjeron un total de 1010 unidades, convirtiendo a la Boeing desde entonces, en el mayor fabricante de aviones del mundo.

Los modelos 727, 737 y 747 son derivados directos del 707. El Boeing 737, cuya producción fue iniciada en 1964 es el avión para transporte de pasajeros más producido y popular de la historia, con más de seis mil aviones producidos, y ya entrado el siglo XXI, el modelo continúa en producción, gracias a todas las mejoras y variantes producidas.

Los aviones de fuselaje ancho son aviones comerciales que poseen tres filas de asientos separadas por dos pasillos. Se crearon para proporcionar más comodidad a los pasajeros, y facilitar su movilidad y la de los tripulantes por el avión.

El primer avión que poseía un fuselaje ancho fue el Boeing 747, apodado "Jumbo", capaz de transportar a más de 500 pasajeros en un único vuelo. Fue presentado en 1968, y en ese momento muchos pensaban que no tendría éxito comercial, por lo que Boeing pasó por problemas económicos durante el proceso de desarrollo del avión. Sin embargo, el "Jumbo" se convirtió en todo un logro comercial, rompiendo todas las expectativas, y pasando a servir rutas con mucha densidad de pasajeros. Desde su lanzamiento fue el avión comercial más grande del mundo hasta la aparición del Airbus A380, ya en el siglo XXI.

En la década de 1970, aparecieron los primeros trirreactores comerciales, el McDonnell Douglas DC-10 y el Lockheed L-1011 TriStar, capaces de realizar rutas intercontinentales, también el nacimiento del F-14 Tomcat el 21 de diciembre de ese año y que tuvieron un gran éxito en su momento. Años después, también se produciría un derivado del DC-10, el McDonnell Douglas MD-11.

El primer birreactor de fuselaje ancho fue el Airbus A300, un avión comercial de medio alcance, fabricado por el consorcio europeo Airbus. La norteamericana Boeing contraatacó con el Boeing 767, similar al A300 pero que podía operar rutas más largas, y con el Boeing 757 para las rutas de medio alcance, pero que no disponía de fuselaje ancho. El Boeing 767 revolucionó la aviación comercial, ya que su largo alcance, sus bajos costes de operaciones y su capacidad de transporte (podía transportar más de 200 pasajeros) permitían vuelos regulares usando el menor número de aviones posible en rutas transatlánticas y en rutas anteriormente impracticables debido a los altos costes operacionales y al bajo número de pasajeros. Gracias a este avión, se popularizaron los viajes transatlánticos, y a finales de los años 80 y principios de los años 90, había más Boeing 767 cruzando el océano Atlántico diariamente, que todos los demás aviones comerciales sumados que operaban esas rutas, y durante los primeros años del siglo XXI, continúa siendo el avión que más veces es usado para cruzar el Atlántico diariamente, a pesar de la creciente competencia de aviones más modernos y recientes.

Después del fin de la Segunda Guerra Mundial, la tecnología necesaria para la realización de vuelos supersónicos controlados todavía no estaba disponible. Además de eso, los aviones aún no eran lo suficientemente resistentes para soportar las fuertes ondas de choque generadas por las velocidades supersónicas. Al nivel del mar, la velocidad del sonido es de aproximadamente 1225 km/h, pero a 15 000 metros de altura, esta es de apenas 1050 km/h. De hecho, algunos aviadores en la Segunda Guerra Mundial, lograron pasar la barrera del sonido, pero con resultados catastróficos: las fuertes ondas de choque generadas por la velocidad, destruían los aviones, que no habían sido proyectados para alcanzar esas velocidades.

Llegado el año 1947, ingenieros estadounidenses pasaron a trabajar en pequeños prototipos de aviones no controlados. La mayor preocupación de los especialistas en aviación era que estos aviones resistiesen las ondas de choque que se crean a altas velocidades. Los buenos resultados obtenidos en estas pruebas llevarían a la producción de una serie de aviones que denominaron Aviones X ("X-planes" en inglés). El estadounidense Charles Yeager se convirtió en la primera persona en sobrepasar la velocidad del sonido, el 4 de octubre de 1947, pilotando un Bell X-1 bautizado como "Glamorous Glennis".
En 1962, el avión cohete North American X-15 se convirtió en el primer avión en llegar a la termosfera, pilotado por el estadounidense Robert White. Logró permanecer a una altura de 95 936 metros durante dieciséis segundos, recorriendo en ese periodo aproximadamente 80  kilómetros. Este fue el primer vuelo de un avión por el espacio. Posteriormente, el X-15 llegaría a los 107 960 metros de altitud, y también se convirtió en el primer avión hipersónico (5 veces la velocidad del sonido), rompiendo diversos récords de velocidad, y superando Mach 6 (seis veces la velocidad del sonido) en diversos vuelos.

Los primeros aviones supersónicos para uso civil fueron creados a finales de los años 60. El primer avión supersónico comercial del mundo fue el soviético Tupolev Tu-144, que realizó su primer vuelo el 31 de diciembre de 1968.
El Concorde, fabricado por un consorcio franco-británico, hizo su primer vuelo dos meses después. El Tu-144 comenzó sus primeros vuelos de pasajeros en 1977, pero por causa de problemas operacionales, dejó de ser utilizado como avión para el transporte de personas al año siguiente. En cuanto al Concorde, realizó sus primeros vuelos comerciales en 1976, sirviendo en rutas transatlánticas. Ambas aeronaves han sido, hasta el momento, las únicas aeronaves supersónicas comerciales que se han desarrollado. En el 2004, se suspendieron sus vuelos comerciales quedando los Concorde como ejemplares en museos de aviación alrededor del mundo.

Con la carrera espacial siendo uno de los puntos clave de la Guerra Fría entre Estados Unidos y la Unión Soviética, el cielo dejó, literalmente, de ser el límite, al menos para los vuelos controlados. En 1957 el satélite soviético Sputnik se convirtió en el primer satélite en orbitar la tierra, y en 1961, el cosmonauta soviético Yuri Gagarin se convirtió en la primera persona en viajar al espacio, orbitando una vez alrededor del planeta, y permaneciendo allí durante 108 minutos. Los Estados Unidos reaccionaron meses más tarde lanzando al astronauta Alan Shepard al espacio, y años después, lanzando la primera misión a la Luna dentro del Programa Apolo. El 20 de julio de 1969 Neil Armstrong, comandante de la misión Apollo 11 se convertiría en la primera persona en pisar la luna.

El 12 de junio de 1994 el Boeing 777 realizó su primer vuelo, convirtiéndose en el primer avión diseñado y planeado completamente con ordenadores, y en la actualidad es el mayor avión birreactor del mundo. Junto al cuatrirreactor Airbus A340, son los aviones con mayor alcance operacional del planeta, pudiendo recorrer más de 16 000 kilómetros en un único vuelo.

Desde los años 1970, los aeropuertos y aviones comerciales pasaron a ser uno de los objetivos preferidos de ataques terroristas. El peor de estos ataques ocurrió en 2001, cuando dos aviones de American Airlines y dos de United Airlines fueron utilizados en los atentados del 11 de septiembre. Como consecuencia directa de este acontecimiento, el número de viajeros de avión disminuyó en la mayoría de líneas aéreas, y muchas de ellas se enfrentaron a grandes dificultades financieras en los años siguientes. Los efectos del ataque, aunque minimizados, todavía persisten en varias compañías. El resultado de la amenaza terrorista es el incremento de medidas de seguridad que se toman en los aeropuertos desde entonces.

Desde el inicio del siglo XXI, la aviación subsónica pretende sustituir al piloto por aeronaves controladas a distancia o por ordenadores. En abril de 2001, el avión no tripulado denominado Northrop Grumman RQ-4 Global Hawk voló desde la Base de la Fuerza Aérea Edwards (California, Estados Unidos) hasta Australia, sin escalas y sin reabastecerse de combustible, tardando 23 horas y 23 minutos, siendo el vuelo más largo realizado por un avión no tripulado.

Uno de los Concorde de Air France sufrió un accidente el 25 de julio de 2000, cuando una turbina del avión comenzó a arder, haciendo que se estrellara en Gonesse (Francia) poco después de despegar. Hasta entonces, el Concorde era considerado el avión comercial más seguro del mundo. Pasó por un proceso de modernización hasta el 2003, pero por causa del bajo número de pasajeros y de los altos costes operacionales, todos los aparatos dejaron de volar en 2003, cuando British Airways retiró el último en servicio, y desde entonces ningún avión supersónico realiza vuelos comerciales.

El 27 de abril de 2005, el Airbus A380 voló por primera vez, y el 25 de octubre de 2007, con la realización de su primer vuelo comercial entre Singapur y Sídney, se convirtió en el mayor avión comercial de pasajeros del mundo, superando al Boeing 747, que había ostentado ese récord desde que realizó su primer vuelo en 1969. Pero aun así, el A380 es superado en tamaño por el Antonov An-225, que realizó su primer vuelo el 21 de diciembre de 1988, y desde entonces es el mayor avión de la historia.

El 15 de diciembre de 2009, después de dos años de retraso, el Boeing 787 realiza su primer vuelo en las instalaciones que la compañía tiene en el aeropuerto de Paine Field (Everett, Washington, Estados Unidos), convirtiéndose en el primer avión comercial fabricado principalmente con materiales compuestos.

Desde el comienzo de la década de 1990, la aviación comercial pasó a desarrollar tecnologías que en el futuro convertirán al avión en un aparato cada vez más automatizado, reduciendo gradualmente la importancia del piloto en las operaciones de la aeronave, con la intención de reducir los accidentes aéreos causados por fallos humanos. Los fabricantes de aviones comerciales continúan investigando posibles maneras de mejorarlos, convirtiéndolos en aparatos cada vez más seguros, eficientes y silenciosos. Al mismo tiempo, los pilotos, controladores aéreos y mecánicos cada vez estarán mejor preparados y las aeronaves pasarán unas revisiones más rigurosas con el fin de evitar accidentes por fallos humanos o mecánicos.

El sistema de lanzamiento reutilizable, también conocido por sus siglas en inglés "RLV" ("Reusable Launch Vehicle") es un vehículo de lanzamiento que es capaz de ser lanzado al espacio más de una vez, gracias a sus cohetes reutilizables, que generarían el empuje suficiente para alcanzar el espacio y una vez allí, orbitar alrededor del planeta. Estas aeronaves podrán despegar y aterrizar de la misma manera que los aviones, en pistas de aterrizaje largas. Aunque todavía no están disponibles, hay varios modelos que se encuentran en fase de pruebas, como el SpaceShipOne, que se convirtió en el primer vehículo espacial tripulado de capital privado. Con el tiempo podrían usarse para la realización de viajes espaciales, de bajo coste y alta seguridad. No obstante, para que puedan emplearse en múltiples ocasiones, es necesario que posean una estructura más resistente para soportar el uso continuado, lo que aumentaría el peso del aparato, y dada la falta de experiencia con estos vehículos, aún se tienen que considerar los costes que implicaría su realización.

También se están investigando nuevas fuentes de energía más limpias, como el etanol, electricidad, o incluso empleando energía solar fotovoltaica. Con esta última, la NASA creó el "Helios", un avión alimentado gracias a la energía que le proporciona el sol y sus células fotovoltaicas instaladas en toda su superficie alar. El "Helios" batió el récord de altura en ese tipo de aparatos, y también es capaz de mantenerse durante días en vuelo, lo que hace que en un futuro, aviones similares puedan ser empleados como satélites más económicos. Otras iniciativas privadas, como el avión "Solar Impulse" se han venido desarrollando en los últimos años, augurando el próximo despegue de la aviación solar.

A pesar de los crecientes problemas a los que se ha enfrentado la aviación en general, se cree que el siglo XXI será un siglo de avances dentro del mundo de la aviación. Aviones y cohetes ofrecerán capacidades únicas en términos de velocidad y capacidad de pasajeros y de carga que no deben ser subestimados. Mientras las personas tengan necesidades de transporte de un punto a otro del planeta a gran velocidad, la aviación siempre será necesaria.







</doc>
<doc id="17513" url="https://es.wikipedia.org/wiki?curid=17513" title="Mira Sorvino">
Mira Sorvino

Mira Sorvino (Tenafly, Nueva Jersey; 28 de septiembre de 1967) es una actriz estadounidense, ganadora de un Óscar y un Globo de Oro.

Nació en Tenafly como Lorraine Ruth Davis, en el estado de Nueva Jersey. Es hija del también conocido actor Paul Sorvino, de ascendencia italiana. Creció en Nueva Jersey y dedicó su infancia y adolescencia a los estudios académicos y no en prepararse para una carrera de actriz; ya que su padre no quería que sus hijos se convirtiesen en estrellas de cine siendo aún muy jóvenes.

Aun así, durante los últimos cursos Sorvino intervino en producciones teatrales del colegio, donde adquirió sus primeras experiencias de interpretación. Más tarde continuó actuando en los montajes teatrales de su universidad. Estudió en la Universidad Harvard, donde se licenció "magna cum laude" en estudios de Lengua y Civilización del Este Asiático. Vivió un año en China, durante el cual aprendió a hablar con fluidez el chino mandarín. También habla el francés.

Después de terminar sus estudios universitarios, Sorvino pasó tres años en Nueva York intentando hacerse un nombre como actriz sin ayuda de nadie. Como tantos otros jóvenes actores y actrices, también ella hizo diversos trabajos mientras esperaba su oportunidad. Uno de sus trabajos fue el de ayudante de producción en una compañía cinematográfica. Cuando en 1993 se inició la preproducción de la película "Amongst Friends", Sorvino fue contratada como tercera ayudante de dirección. Sin embargo, pronto fue promocionada a los puestos de directora de casting y de productora asistente, y finalmente se le ofreció el papel principal femenino de la película. Las críticas de su actuación fueron positivas, lo que le abrió las puertas para actuar en otras películas.

Una interpretación que en 1995 la convirtió en una actriz de reconocido prestigio, fue la de prostituta parlanchina y de voz aguda en "Poderosa Afrodita", de Woody Allen, por la que Sorvino obtuvo el Óscar en 1996 a la mejor actriz de reparto. Desde entonces ha trabajado de forma continuada en papeles principales y secundarios, en películas de las que algunas tuvieron un considerable éxito comercial, como "Romy y Michelle", coprotagonizada por Lisa Kudrow, y "A primera vista", con Val Kilmer.

Sorvino mantuvo durante varios años una relación con Quentin Tarantino, y luego con el actor francés Olivier Martínez. Actualmente está casada con el actor Christopher Backus, con el que tiene 4 hijos: Mattea Angel, Johnny Christopher King, Holden Paul Terry y Lucia.

Sorvino es participante vocal del movimiento #MeToo y denunció haber sufrido acoso sexual por parte de Harvey Weinstein como parte de las acusaciones de abuso sexual contra él. El director Peter Jackson reconoció que Weinstein la había vetado junto a Ashley Judd (otra víctima de Weistein) de participar en la trilogía cinematográfica de "El Señor de los Anillos". En 2019 también confesó haber sufrido una violación durante una cita. Sorvino es activista por los derechos humanos y ha participado en campañas contra el tráfico humano en Darfur.

2018 Start up Rebecca Stroud 


</doc>
<doc id="17515" url="https://es.wikipedia.org/wiki?curid=17515" title="Olorun">
Olorun

Olorun es una de las manifestaciones del Dios supremo de la religión yoruba Olodumare. Olorun etimológicamente significa òlò, Señor, y Òrún, cielo; es decir, Señor del cielo o del Más Allá.

De la energía de este Dios supremo surgieron los Ìrúnmalé, subdivididos en Òrìsà (energías masculinas) y Èborás (energías femeninas). Dichos Ìrúnmalé son emanaciones de este Dios supremo y su misión es mantener el equilibrio universal. 

Olorun no posee culto ni templo propios. La mención de su nombre está restricta al ser humano, siendo los Ìrunmalé los intermediarios entre él y los hombres. A través del sol, se le ofrenda en el ñangareo, dando cuenta de que en la tierra se va a hacer un itá o cuando nace un Iyawó.



</doc>
<doc id="17518" url="https://es.wikipedia.org/wiki?curid=17518" title="Eshu">
Eshu

Eshu, Exu o Esu es un orisha en la mitología yoruba. A medida que la religión se ha extendido por el mundo, el nombre de este orisha ha variado en diferentes lugares, pero las creencias siguen siendo similares.

Eshu. Orisha, del grupo de los Orisha Oddé, comúnmente denominados "Los Guerreros". Rige las manifestaciones de lo malévolo.

Èsù es considerado en la religión yorubá la primera partícula diferenciada de vida creada por Òlòórún. La palabra Èsù significa "esfera", y representa la infinitud, el movimiento permanente. Èsù es el mensajero de los Òrìsà; es el intermediario entre los hombres y los Òrìsà. Es el permanente comunicador entre Aiyé (algunas veces mal traducido como mundo en que vivimos) y Òrún (más allá, morada de los Òrìsà, algunas veces mal traducido como cielo).

Eshu Alawana o Alaguana está en todas partes. Vive solitario en los parajes oscuros e inhóspitos del monte o en la sabana. Es el jefe de los eggun, con los que tiene un gran comercio. Representa la desesperanza y el infortunio.

Eshu Alawana es el más pequeño de los Elegguá y acompaña mucho a Oggún. Es considerado el dueño de todo tipo de cadenas, además del garabato de guayaba, de un muñeco de cedro que vive y come con él y del Ariku Bambaya.

Es un gran hechicero, tiene la capacidad para liberar de la prisión, se le invoca con el garabato de guayaba. Tiene su propio Ossain que se prepara en un tarro de toro y tiene su propia sopera. Eshu Alawana para que trabaje se pone en suelo sobre el carapacho de una jicotea.

Entre sus herramientas se encuentran, un Ariku Bambaya, unas cadenas, un garabato de guayaba, su muñeco de cedro y su carapacho de jicotea.

En el panteón yoruba Èsù rige sobre la comunicación, la palabra, las encrucijadas de los caminos (simbolizando las diferentes opciones de la vida), el comercio, el trabajo, etc.

Es hijo de Obatala y de Yenbo o Orchanla, amigo de Ogun y de Ochosi y Ozun son inseparables.
Creado por Olorun.

Eshu es la primera partícula de vida creada por Olorun.

Se le inmolan chivos, cazal de gallas de Guinéa, gallos o pollos, pollones, Cazal de palomas (generalmente Oscuras para Lodé y Laná; pero más claras para Adàgué y Aselú).

Las Ofrendas (también llamados Frentes) pueden variar según cada tradición dentro del mismo culto yoruba.
Bara Lodé: maíz tostado, 7 papines (papas chicas) asadas con cáscara y untadas en aceite de dendé (palmera).

Bará Laná: maíz tostado, 7 papines (papas chicas) asadas con cáscara y untadas en aceite de dendé (palmera)

Bará Adàgué: maíz tostado claro, 7 papines (papas chicas) asadas con cáscara y untadas en aceite de dendé (palmera): en el centro, se coloca un buen puñado de pipoca (pororó).

Bará Aselú: Axoxó (maíz hervido), 7 papines (papas chicas) hervidas con cáscaras (una vez frías se pelan para ser presentadas; las mismas van intercaladas con 7 caramelos de miel. Eventualmente, puede reemplazarse los caramelos por tiras de coco fresco pelado.

Como parte de la transculturación y del peligro que vieron los esclavos traídos a Cuba de perder sus raíces, cada santo adoptó el nombre de un santo católico. También está el hecho de que los esclavos venían de diferentes partes de África y en cada uno se le llamaba diferente



</doc>
<doc id="17522" url="https://es.wikipedia.org/wiki?curid=17522" title="Donna Haraway">
Donna Haraway

Donna Haraway (Denver, Colorado, 1944) profesora emérita distinguida del programa de Historia de la Conciencia en la Universidad de California, es la autora de «Cyborg Manifesto» (1985) «Primate Visions: Gender, Race, and Nature in the World of Modern Science» (1989), «Simians, Cyborgs, and Women: The Reinvention of Nature» (1991) y "When species meet" (2008).

Haraway se graduó en Zoología y Filosofía el año 1966 en el «Colorado College» obteniendo la beca de la fundación Boettcher. Vivió en París un año estudiando filosofía de la evolución con una beca Fulbright antes de completar su doctorado en el Departamento de Biología de Yale en 1972. Escribió su tesis sobre las funciones de la metáfora en la configuración de la investigación en biología del desarrollo en el siglo XX.

Haraway ha enseñado estudios de la mujer y «Ciencia General» en la universidad de Hawái y en la universidad Johns Hopkins. Pero la mayor contribución de Haraway vendrá durante sus años de docente de posgrado en el reconocido Departamento de Historia de la Conciencia en la University of California - Santa Cruz integrando el personal junto a Hayden White, Teresa de Lauretis, Angela Davis y James Clifford.

En septiembre de 2000, Haraway fue premiada con altos honores por la «Society for Social Studies of Science», con el premio J.D. Bernal, por una vida de contribuciones en el campo. Haraway es la principal pensadora acerca de la relación amor/odio entre personas y máquinas. Sus ideas han detonado una explosión de debates en áreas tan diversas como en primatología, filosofía y biología del desarrollo (Kunzru, 1). Actualmente ha encontrado un lugar destacado en los debates configurados en torno al antropoceno donde argumenta a favor de una "política multiespecies".

Al leer los libros de Haraway, se hace claro que sus escritos se basan predominantemente en su conocimiento de la historia de la ciencia y la biología (Carubia, 4). En su libro, "Visiones de Primate: Género, Raza, y Naturaleza en el Mundo de la Ciencia Moderna", Haraway explica las metáforas y narrativas que dirigen la ciencia de la primatología. Demuestra que hay una tendencia a masculinizar las historias acerca de la "competencia reproductiva y el sexo entre machos agresivos y hembras receptivas que facilitan algunos y excluyen otros tipos de conclusiones" (Carubia, 4). Alega que las primatólogas se enfocan en observaciones diferentes que requieren más actividades de comunicación y supervivencia básica, ofreciendo perspectivas de los orígenes de la naturaleza y la cultura muy diferentes de las actualmente aceptadas. Recurriendo a estos ejemplos de narrativas e ideologías de género, raza y clase social occidentales, Haraway cuestiona las construcciones más fundamentales de las historias de la naturaleza humana basadas en los primates. En "Visiones de Primate", escribe:

Mi esperanza ha sido que el enfoque siempre oblicuo y a veces perverso facilitara revisiones de narrativas occidentales fundamentales y persistentes acerca de la diferencia, especialmente la diferencia sexual y racial; acerca de la reproducción, especialmente en términos de las multiplicidades de generadores y crías; y acerca de la supervivencia, especialmente acerca de la supervivencia imaginada en las condiciones límite tanto de los orígenes como del fin de la historia, tal y como se cuenta en las tradiciones occidentales de ese complejo género (p.377).

La mira de Haraway es que la ciencia “revele los límites e imposibilidades de su 'objetividad' y que considere algunas revisiones recientes ofrecidas por primatólogas feministas” (Russon, 10). Una experta en su campo, Haraway propuso una perspectiva alternativa de las ideologías aceptadas que continúan moldeando la manera en que se crean historias científicas sobre la naturaleza humana. Lo más importante es que Haraway ofrece analogías inventivas que revelan nuevos horizontes y posibilidades para la investigación (Elkins).

Haraway ha sido descrita como una «feminista, más laxamente una neomarxista y una postmodernista» (Young, 172). Jugando con las palabras del famoso "Manifiesto Comunista" de Marx, Haraway publicó el ensayo «Un manifiesto cíborg: ciencia, tecnología, y feminismo socialista a finales del siglo XX» en la revista "Socialist Review" en 1985. Aunque la mayoría del trabajo anterior de Haraway se enfoca en el énfasis de la inclinación masculina en la cultura científica, ella también ha contribuido enormemente a las narrativas feministas del siglo XX.

Haraway toma de su bagaje científico y se convierte en la observadora y testigo de una tendencia en la sociedad actual y no puede silenciar lo que ve. En "Un manifiesto cíborg", Haraway usa la metáfora del cíborg para ofrecer una estrategia política para los intereses aparentemente disparatados del socialismo y el feminismo. Primeramente, introduce y define el cíborg en cuatro partes. Un cíborg es:


En este ensayo, Haraway también trata un par de formas de feminismo populares durante la década de 1980. Como feminista postmoderna, argumenta en contra del esencialismo, que es «cualquier teoría que declare identificar una causa o constitución de identidad de género o patriarcado universal, transhistórica y necesaria» ("Epistemología feminista", 2006). Tales teorías, argumenta Haraway, excluyen a las mujeres que no se conforman a la teoría y las segregan de las «mujeres reales» o las representan como inferiores. Otra forma de feminismo que Haraway disputa es «un modelo jurisprudencial de feminismo popularizado por la estudiosa legal y marxista Catharine MacKinnon» (Burow-Flak, 2000) que luchó para hacer ilegal la pornografía en la década de 1980, a la cual ella consideró una forma de discurso del odio. Haraway argumenta que el feminismo radical de MacKinnon asimila todas las experiencias de las mujeres en una identidad particular que incorpora las ideologías occidentales que contribuyen a la opresión de las mujeres. Escribe: «Es factual y políticamente erróneo asimilar todos los “momentos” o “conversaciones” diversos en la política femenina reciente nombrada como feminismo radical a la versión de MacKinnon» (158).

De acuerdo con Haraway en su "Manifiesto", «No hay nada acerca de ser hembra que una naturalmente a las mujeres. Ni siquiera existe tal estado como el de “ser” hembra, que de por sí es una categoría altamente compleja construida en discursos científicos sexuales debatidos y otras prácticas sociales» (155). Un cíborg, por otro lado, no requiere una identidad estable y esencialista, argumenta Haraway, y las mujeres deberían considerar crear coaliciones basadas en «afinidad» en vez de identidad. Para dar base a su argumento, Haraway analiza la frase «mujeres de color», sugiriéndola como una categoría posible de política de afinidades (Senft, 2001). Usando un término acuñado por la teórica Chela Sandoval, Haraway escribe que «la “consciencia oposicional” es comparable con la política de cíborg, ya que en vez de la identidad enfatiza cómo la afinidad resulta de la otredad, diferencia y especificidad» (156).

La idea es modificar el propio pensamiento de individuos aislados al pensamiento de la gente como vértices en una red. En este sentido, se puede desarrollar un nexo que no tiene nada que ver con ideales occidentales patriarcales. El «mundo cíborg» ideal de Haraway consiste en gente viviendo junta, sin miedo de su nexo comunal con los animales y las máquinas. «La lucha política es ver desde ambas perspectivas al mismo tiempo, ya que cada una revela tanto dominaciones como posibilidades inimaginables desde el otro punto de vista. La visión sencilla produce peores ilusiones que la visión doble o los monstruos de muchas cabezas» (155).

En los 90 se inició la era ciborg y Haraway es una fiel colaboradora de la cibercultura actual. Aunque en sus textos Haraway utiliza la tecnología a través de la metáfora del ciborg, a la vez es crítica con las consecuencias de lo tecnológico. La idea de que las máquinas pueden contribuir a la liberación es algo que las feministas y mujeres deberían considerar. Haraway escribe: “Hasta ahora (había una vez), la personificación femenina parecía ser algo dado, orgánico, necesario; y la expresión de lo femenino parecía significar el disponer de habilidades maternales o en sentido estricto o metafórico. Solo estando fuera de lugar lograremos un placer intenso con las máquinas y, entonces, con la excusa de que al fin y al cabo se trata de una actividad orgánica después de todo, podremos apropiárnoslas para las mujeres(180).

La siguiente tabla está tomada de "Simios, ciborg y mujeres" e ilustra el cambio que estamos viviendo desde una sociedad orgánica e industrial (columna de la izquierda) hasta un sistema de información polimorfo (columna de la derecha) creado por la política de ciencia y tecnología. Haraway escribe: «Simultáneamente materiales e ideológicas, las dicotomías pueden ser expresadas en la siguiente lista de transiciones desde unas dominaciones jerárquicas confortablemente viejas hasta las aterradoras nuevas redes que he llamado las informáticas de la dominación». (275):






</doc>
<doc id="17526" url="https://es.wikipedia.org/wiki?curid=17526" title="Volker Schlöndorff">
Volker Schlöndorff

Volker Schlöndorf (Wiesbaden, Hesse, Alemania, 31 de marzo de 1939) es un director de cine, guionista, productor, documentalista y actor alemán. Es considerado uno de los directores clave del denominado nuevo cine alemán, movimiento en el que introdujo muchas de las características aprendidas de la Nouvelle Vague.

Dejó la casa de sus padres para ir a un internado jesuita en la Bretaña francesa con intención de que aprendiera francés durante un par de meses. Se quedó tres años allí. Iniciaría su carrera cinematográfica en París, donde se mudó con su familia cuando tenía diecisiete años.

Estudió ciencia política y económica en La Sorbona y dirección en IDHEC, la famosa escuela de cine, y comenzó su carrera profesional durante 1960 como asistente de dirección en varias películas de la Nouvelle Vague: con Alain Resnais en su conocida película "El año pasado en Marienbad" (1961), Louis Malle en "Una vida privada" (1962), "El fuego fatuo" o "¡Viva María!", o Jean-Pierre Melville en "El confidente" (1962).

Su primera película fue un cortometraje rodado en 1965, "Wen Kummert's", que fue prohibido en Francia porque trataba sobre la guerra de Argelia. Dirigió en 1966 "El joven Törless", adaptación de la novela que 60 años antes realizara Robert Musil. Esta película recibió el premio FIPRESCI en el Festival de Cannes así como los Premios del Cine Alemán de mejor película, guion y director y convirtiéndose en uno de los puntales, junto a Fassbinder o Herzog, del nuevo cine alemán.

Posteriormente filmó un episodio de la película "Der Paunkenspieler" (1967), "Mord und Totschlag" (1967), película que contaba con el protagonismo de una de las musas de los Rolling Stones, Anita Pallenberg, quien también aparecía en "El rebelde" (1969), un film ambientado en la Edad Media.

Estuvo casado con la actriz, guionista y directora Margarethe von Trotta.

Entre los numerosos premios que ha recibido su obra destacan el Óscar a la mejor película de habla no inglesa y la Palma de Oro en el Festival de Cine de Cannes logrados por "El tambor de hojalata" (1979), basada en la novela original de Günter Grass.

Schlöndorff adopta diferentes estilos de novela para sus películas que generalmente se comprometen con el tema de la las políticas de la posguerra en Alemania. Ha servido también como Director Ejecutivo del estudio UFA en Babelsberg.

Como autor de documentales destaca especialmente el ciclo de entrevistas que realizó a Billy Wilder en 1992: "Billy Wilder, ¿cómo lo hiciste?" y "Billy Wilder habla" (1996).




</doc>
<doc id="17529" url="https://es.wikipedia.org/wiki?curid=17529" title="Frederick Winslow Taylor">
Frederick Winslow Taylor

Frederick Winslow Taylor fue un promotor de la organización científica del trabajo y es considerado el padre de la Administración Científica. En 1878 efectuó sus primeras observaciones sobre la industria del trabajo en la industria del acero. A ellas les siguieron una serie de estudios analíticos sobre tiempos de ejecución y remuneración del trabajo. Sus principales puntos, fueron clave para determinar científicamente el concepto de trabajo estándar, crear una revolución mental y un trabajador funcional a través de diversos conceptos que se instruyen a partir de un trabajo suyo publicado en 1911 llamado Shop Management.

Según Antonio Siera Monra, Taylor desde su adolescencia comenzó a perder la vista, además, su cuerpo era de complexión débil y no podía participar de los juegos que los otros organizaban como el béisbol y el tenis. «Obligado al degradante, para un muchacho, papel de espectador, dedicó su vida a concebir cómo mejorar el rendimiento del esfuerzo físico derrochado por los jugadores mediante un diseño más adecuado de los instrumentos por ellos utilizados».
Esta actitud lo marcaría de por vida, para él lo importante era medir el esfuerzo, el lugar y los movimientos para obtener una vasta información y de ahí, sacar provecho de manera que se diera la mayor eficiencia posible tanto en el deporte como en la producción. Sus biógrafos también lo califican como una persona de actitud inflexible frente a las reglas del juego «incluso un juego de críquet representaba para él una fuente de estudio y de análisis».

Antes de las propuestas de Taylor , los trabajadores eran responsables de planear y ejecutar sus labores. A ellos se les encomendaba la producción y se les daba la "libertad" de realizar sus tareas de la forma que ellos creían era la correcta sin tener conocimientos técnicos. El autor lo describe de esta manera: “encargados y jefes de taller saben mejor que nadie que sus propios conocimientos y destreza personal están muy por debajo de los conocimientos y destreza combinados de todos los hombres que están bajo su mando. Por consiguiente, incluso los gerentes con más experiencia dejan a cargo de sus obreros el problema de seleccionar la mejor forma y la más económica de realizar el trabajo”. De ahí que sus principios “vistos en su perspectiva histórica, representaron un gran adelanto y un enfoque nuevo, una tremenda innovación frente al sistema”. Se debe reconocer aquí que Taylor representa el sueño de una época, como lo es Estados Unidos de los primeros años del siglo XX donde era imperativo alcanzar la mayor eficiencia posible, cuidando el medio ambiente aunado a una explosión demográfica acelerada en las ciudades, una demanda creciente de productos. taylor.

Existe una diferencia muy particular entre la teoría de Taylor y Henry Fayol que resultó adyacente hacia la conyugal del sistema de Estados Unidos, en el uso del tiempo, ya que Fayol se enfoca más en la estructura general de la organización, mientras que Taylor se enfocaba más en el método y herramientas del trabajo para una mejor eficacia. Otra diferencia entre Taylor y Fayol es el área de la pirámide de la organización que estudiaban, una es el nivel operario que es el área de estudio de Taylor mientras que Fayol se dedicó al estudio del área superior de la organización.

En su libro "The Principles of Scientific Management" publicado en 1911 mencionaba los principios que sustentaban la perspectiva científica de la administración y le daban un nuevo giro a la manera de cómo se hacía el trabajo en aquella época, es así como las personas que administran la producción deben adquirir nuevas responsabilidades como se verá a continuación. Según Taylor, la gerencia:

El deseo de Taylor en aplicar su venerado “scientific management”, iba en la noble dirección de conseguir la máxima prosperidad del empresario, así como la máxima prosperidad para el trabajador (Taylor, pág. 21), aun así, después contradice esta afirmación diciendo que ha visto como los trabajadores que empiezan a tener aumentos en su sueldo en más de un 60 % se convierten en "tomadores de trago" y empiezan a disminuir su producción y, así, su calidad de vida; de ahí que el 60 % en el aumento de sueldo sea para él, el tope máximo a pagarle a quien califique como un trabajador tipo buey.

Se deben citar algunos de los argumentos de Taylor para la aplicación de sus propuestas. Para él, el hombre es por naturaleza perezoso, e intenta escudarse en ello para realizar lentamente su trabajo haciendo creer al empresario que está dando lo mejor de sí. De ahí que se deben medir los tiempos y los movimientos de estos trabajadores para estudiarlos y encontrar la mejor combinación de movimientos musculares para elevar la producción y, también, dar uniformidad a los procesos, lo que no ocurría en el antiguo sistema. Para ello era necesario dividir entre quienes piensan las mejores maneras de hacer el trabajo y quienes tienen las fortalezas físicas para ejecutarlo, a los primeros se les daba la responsabilidad de adiestrar a los segundos hasta obtener de ellos el mayor rendimiento que su cuerpo pudiera dar. También habla de la especialización de tareas, pues de esta manera, el trabajador gana más tiempo y destreza haciendo lo mismo todos los días.
"La organización científica del trabajo según Taylor".

El Principio de Excepción planteado por Taylor, en el campo de la administración, representa un filtro de información en el que, según sus palabras: «... el administrador debería recibir informes condensados, resumidos e invariablemente comparativos, cubriendo, sin embargo, todos los elementos que son de interés para la administración. Esos resúmenes deberían ser cuidadosamente revisados por un ayudante antes de que lleguen al administrador, y poseer, además, todas las excepciones buenas como de las excepciones malas; se obtiene en pocos minutos, una visión global de los progresos realizados de los reveses y deja al administrador la posibilidad de considerar las líneas de la política, y estudiar el carácter y el ajuste de los hombres importantes bajo su mando».

Lo que sugiere Taylor, es una autonomía relativa a los diferentes departamentos operativos de la empresa y una red de comunicación "Algedónica" que indique el momento en que la jerarquía debe entrar a funcionar. Este es el "Principio de Excepción" en el campo de la Administración de Empresas.

De esta forma la Algedonía (o el Principio de Excepción) representa el mecanismo que une e integra los conceptos de autonomía de un subsistema con los de jerarquía entre subsistemas. A través de dicho mecanismo la libertad de los subsistemas se hace efectiva y real. El subsistema poseerá total autonomía hasta los límites de su capacidad para controlar su conducta. Pasado ese límite, la Algedonía lo hace dependiente de su subsistema jerárquicamente superior a éste, dentro de su propia autonomía, resuelva el problema y restituya la autonomía perdida del subsistema subordinado. Así éste continuará gozando de "libertad".

Evidentemente, si el sistema jerárquico inmediatamente superior se ve incapacitado de resolver el problema de ese subsistema, entonces deberá acudir, a su vez, a su nivel superior, perdiendo así también su autonomía, en relación con ese problema en particular.



</doc>
<doc id="17530" url="https://es.wikipedia.org/wiki?curid=17530" title="Henri Fayol">
Henri Fayol

Jules Henri Fayol Le Maire (29 de julio de 1841 - 19 de noviembre de 1925) fue un ingeniero de minas francés, ejecutivo de minas, autor y director de minas que desarrolló la teoría general de la administración de empresas que a menudo se llama Fayolismo. Él y sus colegas desarrollaron esta teoría independientemente de la gestión científica, pero más o menos de forma contemporánea. Como su contemporáneo, es ampliamente reconocido como fundador del método moderno de gestión.

Fayol nació en 1841 en un suburbio de Constantinopla. (actual Estambul). Nació en el seno de una familia francesa de la clase media. Su padre (un ingeniero) estaba en el ejército en ese momento y fue nombrado superintendente de obras para construir el Puente de Galata, que tendió un puente sobre el Cuerno de Oro. La familia regresó a Francia en 1847. Asistió al Liceo de Lyon de 1856 a 1858, y de 1858 a 1860 se graduó en la academia de minería "École Nationale Supérieure des Mines" en Saint-Étienne en 1860.

En 1860, a la edad de diecinueve años, Fayol comenzó a trabajar en la compañía minera llamada "Compagnie de Commentry-Fourchambault-Decazeville" en Commentry como ingeniero de minas. Fue contratado por Stéphane Mony, que había decidido contratar a los mejores ingenieros de la Escuela de Minas de Saint-Étienne. Fayol se unió a la empresa como ingeniero y aprendiz de gerente. Mony hizo de Fayol su protegido, y Fayol le sucedió como gerente de la Mina Commentry y finalmente como director gerente de Commentry-Fourchambault y Decazeville. En 1872 fue administrador general del Grupo de Minas de Commentry, Montirco y Berry. Durante su estancia en la mina, estudió las causas de los incendios subterráneos, cómo prevenirlos, cómo combatirlos, cómo recuperar las zonas mineras que se habían quemado, y desarrolló un conocimiento de la estructura de la cuenca. En 1888 fue ascendido a director general de la Compañía Commentry-Fourchambault. Durante su tiempo como director, hizo cambios para mejorar las situaciones de trabajo en las minas, como permitir a los empleados trabajar en equipo, y cambiar la división del trabajo. Más tarde, se añadieron más minas a sus tareas.
Su éxito fue tal que logró adquirir para la compañía Commentry-Fourchambautl, las minas de Bressac, las de Decazeville, así como las de Joudreville, en los campos carboníferos del este de Francia.

En 1900 Fayol se convirtió en miembro del Comité Central des Houillères de France, miembro del consejo del Comité des forges y administrador de la Société de Commentry, Fourchambault et Decazeville. Finalmente, la junta decidió abandonar su negocio de hierro y acero y las minas de carbón. Eligieron a Henri Fayol para supervisar el negocio como el nuevo director general. Al recibir el puesto, Fayol presentó al consejo un plan para restaurar la empresa. La junta aceptó la propuesta. Cuando se retiró en 1918, la compañía era financieramente fuerte y una de las mayores cosechadoras industriales de Europa.

Basándose en gran medida en su propia experiencia de gestión, desarrolló su concepto de administración. En 1916 publicó estas experiencias en el libro "Administration Industrielle et Générale", más o menos al mismo tiempo que Frederick Winslow Taylor publicaba su libro Los Principios de la Administración Científica.

En 1918 se retiró como jefe ejecutivo de la Commentry, pero siguió siendo su director.

Murió a los 84 años, habiendo escrito solamente dos obras principales: la más notable, editada en 1916, fue Administración Industrial y General, la que desafortunadamente no fue traducida al inglés, y por ello poco conocida en Estados Unidos sino 30 años más tarde. En 1921 escribió, el libro La Incapacidad Industrial del Estado y el Despertar del Espíritu Público.

La obra de Fayol se dio a conocer más ampliamente con la publicación en 1949 de "Administración industrial y general", la traducción al español del artículo del 1916 "Administration industrielle et générale". En este trabajo Fayol presentó su teoría de la administración, conocida como Fayolismo. Antes de que Fayol escribiera varios artículos sobre la ingeniería de la minería, comenzando en la década de 1870, y algunos artículos preliminares en administración.

A partir de la década de 1870, Fayol escribió una serie de artículos sobre temas mineros, como el calentamiento espontáneo del carbón (1879), la formación de los yacimientos de carbón (1887), la sedimentación de la Commentry, y sobre fósiles vegetales (1890).

Sus primeros artículos se publicaron en un "Bulletin de la Société de l'Industrie minérale' francés, y a partir de principios de los años 1880 en los Comptes rendus de l'Académie des sciences, las actas de la Academia Francesa de Ciencias.

El trabajo de Fayol fue una de las primeras declaraciones completas de una teoría general de la administración. Propone que hay cinco funciones primarias de gestión y catorce principios de gestión.

Antes de hacer un esquema de las ideas básicas o, más bien, de la estructura de la teoría de Fayol sobre la administración, es conveniente señalar lo que a nuestro juicio es lo más más importante: aquellos principios que, aunque no todos inventados por él, recibieron de este autor francés -considerado por algunos como el verdadero padre de la administración- su plena consagración y fuerza. En primer lugar, señala Fayol, con toda precisión que los principios de la administración son aplicables no solo a empresas, sino a cualquier actividad o institución en que exista coordinación de esfuerzos humanos para alcanzar un fin con eficacia. Puede decirse que establece el principio de la universalidad: por ello se ha dicho que muchos consideran esta escuela con el nombre de "universalista". Fayol establece que la administración se basa en las técnicas y principios de otras disciplinas, pero que es algo distinto de ellas, así como de las actividades sobre las que se aplica. Así pues, afirma su "especificidad". Fayol sostiene también que la administración deduce sus regalas de la experiencia de los más grandes administradores. No es, pues meramente teórico, sino que surge de la experiencia en la forma que se ha señalado en el principio anterior. Señala Fayol que la teoría de la administración es un medio o un esquema para organizar la experiencia que se ha adquirido. No le da, por consiguiente, ni carácter puramente teórico ni puramente empírico; a la teoría le da principalmente el papel y el valor de servir para organizar y aplicar esa experiencia que se adquirido. Establece también que la administración es algo capaz de ser enseñado, y que no surge meramente de la capacidad personal que tenga un determinado jefe de empresa o de otra institución. Esto es un enorme avance que conduce hacia su definitiva incursión en las escuelas y en las universidades como una carrera especial. Los siguientes principios o ideas más bien insinuadas, son quizá lo más propiamente personal de Fayol, y expresan explícitamente que la administración descansa en la organización formal, esto es, en la estructura de la autoridad y responsabilidades de cada uno. Por ello cuida de precisar las funciones que pertenecen a cada unidad de trabajo, por lo que trata de identificar las que corresponden a cada ejecutivo. Podría decirse que esto es un principio de "impersonalidad de las funciones administrativas".

Fayol comienza por establecer que la administración de una empresa comprende las operaciones fundamentales: 


En su trabajo original, "Administration industrielle et générale; prévoyance, organisation, commandement, coordination, controle", se identificaron cinco funciones primarias:


La función de control, del francés "contrôler", se utiliza en el sentido de que un gerente debe recibir información sobre un proceso para hacer los ajustes necesarios y debe analizar las desviaciones. Últimamente los estudiosos de la gestión combinaron la función de mando y coordinación en una función de dirección.

Por cuanto a estas últimas, que son las que se analizan en su libro Administración General e Industrial, señala que la administración consiste en "prever, organizar, mandar, coordinar y controlar". La sección más importante de su libro establece que entiende por cada una de estas actividades, sus normas y por qué son importantes.
Al respecto cabe hacer dos aclaraciones:
a) En la primera de estas operaciones, la previsión -pre, antes, y visión, lo que se ve-, Fayol señala que consiste en "escrutar el futuro y hacer los planes de acción". Con aquellos que separan-como lo hacemos nosotros-la previsión del la planeamiento, hay que tomar en cuenta que en la época de Fayol ya estaban señaladas estas dos divisiones en la misma definición, como ya se explico.
b) Igualmente, es necesario hacer énfasis en que la cuarta función que el pone (coordinación) es en realidad la esencia de la administración; la previsión es coordinación de los hechos que se logran investigar con los medios con que cuentan. Así, la planificación, es una coordinación de las distintas actividades que se van a realizar, por lo que se ha preferido cambiarla por dirección, que comprende ya en la práctica la coordinación final y concreta que tiene que realizarse siempre al ajustar los planes a los casos concretos.


Mientras que Fayol ideó sus teorías hace casi un siglo, muchos de sus principios todavía están representados en las teorías de gestión contemporáneas. 



Fue galardonado con el premio Delesse de la Academia de Ciencias con la medalla de Oro de la Sociedad de Estímulos para la Industria Nacional y con las medallas de Oro y la de Honor de la Sociedad de la Industria Minera.
Fue también designado caballero de la Legión de Honor de 1888, oficial de la misma en 1913 y alcanzó el grado de comendador de la orden de la Corona de Rumanía en 1925.

Lo anterior no significa que Fayol desconozca el papel de la organización informal, sino que simplemente que Fayol pone como cimiento del manejo de las empresas y organizaciones ante todo una estructura adecuada. En cuanto a su teoría en sí misma, se advierte que Fayol más que acudir a definiciones tiende a establecer enumeraciones, con diversos ejemplos que pueden dar una idea más completa de las funciones o actividades que Fayol trata de describir.

Si se comparan las teorías de Taylor con las de Fayol, aunque este último en algún tiempo se opuso a las del primero, se entenderá por qué al final comprendió que eran puntos de vista complementarios de sus propias ideas. Podría decirse que Taylor mira más bien los principios microadministrativos-sin que ello deje de ver los macroadministrativos, las grandes estructuras de la empresa-en tanto que Fayol mira principalmente las estructuras-también contempladas en los principios fundamentales de Taylor, aunque con menos menos precisión y detalle, y que todavía existen y subsistirán-, sin que por ello deje de ver aspectos de tipo administrativos. Como ya se ha dicho, la tardía aparición del libro de Fayol (30 años después)-inclusive en la misma Francia, dos años después de que lo que había terminado en 1914, por razones de la Segunda Guerra Mundial-hizo que aun en los ejércitos se aplicara más bien la doctrina de Taylor, y así quedara oscurecida la labor de Fayol, pero el tiempo se encargó de revelar que ambos son los pioneros de la administración.



</doc>
<doc id="17531" url="https://es.wikipedia.org/wiki?curid=17531" title="Harrington Emerson">
Harrington Emerson

Harrington Emerson (2 de agosto de 1853 – 2 de septiembre de 1931) fue una de las figuras más importantes y relevantes que revolucionaron la Ingeniería Industrial.
Es conocido por sus contribuciones a la administración científica, donde desarrolló un enfoque que contrasta la eficiencia.

Sus padres fueron Edwin y Mary Louisa Emerson.
Emerson se casó dos veces: en la década de 1870 a Florencia, Brooks y en 1895 a Mary Crawford Supple. Su hijo Raffe nació en 1880. Emerson y Mary Supple tuvo tres hijas: Louise, Isabel y Margarita. Desde sus inicios, Emerson tuvo influencias políticas importantes ya que gran parte de su familia dedicaba sus actividades ello. Su abuelo materno, Samuel Delucenna Ingham, es un claro ejemplo de ello ya que desempeño cargos importantes con el Gobierno estadounidense, llegando a ser Secretario de Tesorería del Gobierno Norteamericano. De esta forma, Samuel logró ser fundador y propietario del Hazleton Coal and Railroad Company, asegurando así la fortuna para la familia Emerson.

Desde muy pequeño Harrignton estuvo a la disposición de tutores y escuelas privadas en Inglaterra, Francia y Grecia. Fue también estudiante de Ingeniería en el Royal Bavarian Polytechnique. Al finalizar sus estudios, regresó a los Estados Unidos y se convirtió en un profesor de lenguajes modernos en la Universidad de Nebraska. Sin embargo, fue despedido en 1882 debido a confrontaciones de índole religiosas. Emerson dejó de lado sus intereses académicos para empezar una carrera como banquero, especulador de propiedades, agente de impuestos, agente fiscal y solucionador de problemas para el Union Pacific y los ferrocarriles de Burlington y Missouri. 

Emerson estableció su propia compañía de préstamos privados en 1883, y en colaboración con su hermano Samuel formó una compañía que invirtió en la construcción de pueblos futuros en el oeste de Nebraska. Emerson invirtió $70.000 en proyectos para el Lincoln Land Company, pero a causa de la sequía y malas cosechas de los cultivos, no pudo afrontar sus pagos hipotecarios y perdió toda su fortuna. En el año 1896 Emerson fue encargado de la campaña política para las elecciones presidenciales y fue el representante del sindicato de inversión inglés.

Para el desarrollo de las industrias americanas, Emerson se dedicó a buscar fondos mediante una investigación en la manufactura, y al negocio de la minería. Gracias al tiempo de investigación dedicado, Emerson tuvo el conocimiento industrial necesario para crear una fundación de consultoría en eficiencia industrial, lo que le llevó de nuevo ser la mano derecha de la campaña del presidente William Jennings Bryan. Después de esto Emerson empezó a trabajar en la ingeniería mecánica para aplicarlo en la energía eléctrica y diésel para la navegación marítima. Después, en 1897, trabajó para General Electric Storage Battery Company en Nueva York para ser el encargado de la línea de investigación. A pedido de la empresa, Emerson se mudó a Seattle, Washington y experimentó con los buques de potencia de navegación eléctrica. Atraídos por el “Gold Rush” en Alaska en 1897, Emerson y varios asociados llevaron a cabo una variedad de proyectos especulativos. Uno de los proyectos más ambiciosos de Emerson, fue la propuesta de la construcción de un cable-telegráfico transpacífico desde Seattle hasta la Filipinas a través de Alaska. El proyecto fracasó por complicaciones financieras y jurídicas, por ello decidió tomar el trabajo de consultoría en eficiencia industrial para poder cubrir la deuda.

Después de su éxito como director general de una pequeña fábrica de vidrio en Pensilvania, en 1900, Emerson decidió tomar la ingeniería de la eficiencia como profesión. A través de reuniones de la Sociedad Americana de Ingenieros Mecánicos, se convirtió en socio laboral de Frederick W. Taylor, el fundador de la administración científica. Entre 1921-28 fue consejero de líderes de gobierno y ministros de transportes de China, Japón, Perú, Polonia y Unión Soviética.

A partir de este periodo, Harrigton Emerson se dedicó a trabajar dentro del campo industrial, desarrollando proyectos, técnicas y principios que innovarían el mundo de la industria.

Su libro, "The Twelve Principles of Efficiency" (1911), presentaba las bases para obtener operaciones eficientes, y sus 12 principios que de alguna forma fueron paralelos a las enseñanzas de Frederick Winslow Taylor, eran los siguientes:


No cabe duda de que los 12 principios expuestos por Harrington Emerson en 1911 son tan válidos hoy como lo fueron entonces.


</doc>
<doc id="17532" url="https://es.wikipedia.org/wiki?curid=17532" title="Revolución Industrial">
Revolución Industrial

La Revolución Industrial o Primera Revolución Industrial es el proceso de transformación económica, social y tecnológica que se inició en la segunda mitad del siglo XVIII en el Reino de Gran Bretaña, que se extendió unas décadas después a gran parte de Europa occidental y América Anglosajona, y que concluyó entre 1820 y 1840. Durante este periodo se vivió el mayor conjunto de transformaciones económicas, tecnológicas y sociales de la historia de la humanidad desde el Neolítico, que vio el paso desde una economía rural basada fundamentalmente en la agricultura y el comercio a una economía de carácter urbano, industrializada y mecanizada.

La Revolución Industrial marca un punto de inflexión en la historia, modificando e influenciando todos los aspectos de la vida cotidiana de una u otra manera. La producción tanto agrícola como de la naciente industria se multiplicó a la vez que disminuía el tiempo de producción. A partir de 1800 la riqueza y la renta per cápita se multiplicó como no lo había hecho nunca en la historia, pues hasta entonces el PIB per cápita se había mantenido prácticamente estancado durante siglos. En palabras del premio Nobel Robert Lucas:
A partir de este momento se inició una transición que acabaría con siglos de una mano de obra basada en el trabajo manual y el uso de la tracción animal, siendo estos sustituidos por maquinaria para la fabricación industrial y para el transporte de mercancías y pasajeros. Esta transición se inició hacia finales del siglo XVIII en la industria textil, así como en lo relacionado con la extracción y utilización de carbón. La expansión del comercio fue posible gracias al desarrollo de las comunicaciones, con la construcción de vías férreas, canales, y carreteras. El paso de una economía fundamentalmente agrícola a una economía industrial influyó sobremanera en la población, que experimentó un rápido crecimiento sobre todo en el ámbito urbano. La introducción de la máquina de vapor de James Watt (patentada en 1769) en las distintas industrias, fue el paso definitivo en el éxito de esta revolución, pues su uso significó un aumento espectacular de la capacidad de producción. Más tarde, el desarrollo de los barcos y de los ferrocarriles a vapor, así como el desarrollo en la segunda mitad del XIX del motor de combustión interna y la energía eléctrica, supusieron un progreso tecnológico sin precedentes. 

Como consecuencia del desarrollo industrial nacieron nuevos grupos o clases sociales encabezadas por el proletariado —los trabajadores industriales y campesinos pobres— y la burguesía, dueña de los medios de producción y poseedora de la mayor parte de la renta y el capital. Esta nueva división social dio pie al desarrollo de problemas sociales y laborales, protestas populares y nuevas ideologías que propugnaban y demandaban una mejora de las condiciones de vida de las clases más desfavorecidas, por la vía del sindicalismo, el socialismo, el anarquismo, o el comunismo.

Aún sigue habiendo discusión entre historiadores y economistas sobre las fechas de los grandes cambios provocados por la Revolución Industrial. El comienzo más aceptado de lo que podríamos llamar Primera Revolución Industrial, se podría situar a finales del siglo XVIII, mientras su conclusión se podría situar a mediados del siglo XIX, con un período de transición ubicado entre 1840 y 1870. Por su parte, lo que podríamos llamar Segunda Revolución Industrial, partiría desde mediados del siglo XIX a principios del siglo XX, destacando como fecha más aceptada de finalización a 1914, año del comienzo de la Primera Guerra Mundial. El historiador marxista Eric Hobsbawm, considerado "pensador clave de la historia del siglo XX" sostenía que el comienzo de la revolución industrial debía situarse en la década de 1780, pero que sus efectos no se sentirían claramente hasta 1830 o 1840. En cambio, el historiador económico inglés T.S. Ashton declaraba por su parte, que la revolución industrial tuvo sus inicios entre 1760 y 1830.
Algunos historiadores del siglo XX, como John Clapham y Nicholas Crafts, argumentan que el proceso de cambio económico y social fue muy gradual, por lo que el término «revolución» resultaría inapropiado. Estas cuestiones siguen siendo tema de debate entre historiadores y economistas.

Los inicios de la industrialización europea hay que buscarlos en la Edad Moderna. A partir del siglo XVI se vislumbra un avance en el comercio, métodos financieros, banca y un cierto progreso técnico en la navegación, impresión o relojería. Sin embargo, estos avances siempre se veían lastrados por epidemias, constantes y largas guerras y hambrunas que no permitían la dispersión de los nuevos conocimientos ni un gran crecimiento demográfico. Según el historiador Angus Maddison, Europa Occidental experimentó un crecimiento demográfico prácticamente nulo entre 1500 y 1800. 

El Renacimiento marcó otro punto de inflexión con la aparición de las primeras sociedades capitalistas en Holanda y el norte de Italia. Es a partir de mediados del siglo XVIII cuando Europa comenzó a distanciarse del resto del mundo y a asentar las bases de la futura sociedad industrial debido al desarrollo, aún primitivo, de la industria pesada y la minería. La "alianza" de los comerciantes con los agricultores hizo aumentar la productividad, lo que a su vez provocó una explosión demográfica, acentuada a partir del XIX. La Revolución Industrial se caracterizó por la transición de una economía agrícola y manual a una comercial e industrial cuya ideología se basaba en el racionalismo la razón y la innovación científica.

Otro de los principales desencadenantes de la Revolución nace de la necesidad. Aunque en algunos lugares de Europa como Gran Bretaña ya existía una base industrial, las Guerras Napoleónicas consolidaron la industria europea. Debido a la guerra, que se extendía por la mayor parte de Europa, las importaciones de muchos productos y materias primas se suspendieron. Esto obligó a los gobiernos a presionar a sus industrias y a la nación en general para producir más y mejor que antes, desarrollándose industrias antes inexistentes.
La industrialización tuvo lugar en diferentes oleadas en los distintos países. Las primeras áreas industriales aparecieron en Gran Bretaña a finales del siglo XVIII, extendiéndose a Bélgica y Francia a principios del siglo XIX y a Alemania y a Estados Unidos a mediados de siglo, a Japón a partir de 1868 y a Rusia, Italia y España a finales de siglo. Entre las razones se encontraron algunas tan dispares como la notable ausencia de grandes guerras entre 1815 y 1914, la aceptación de la economía de mercado y el consecuente nacimiento del capitalismo, la ruptura con el pasado, un cierto equilibrio monetario y la ausencia de inflación.

Otras interpretaciones sugieren que este nuevo cambio de mentalidad y la posterior evolución del sistema económico fue por causas morales y religiosas. La Reforma protestante de Martín Lutero y Juan Calvino trajo consigo un cambio de mentalidad en el trato y visión respecto del trabajo. Según Max Weber el protestantismo considera al trabajo y al esfuerzo como un bien y un valor fundamental, al contrario que la ética católica que lo considera un castigo a raíz del pecado original. Esto explicaría en parte las diferencias a la hora de desarrollarse de las distintas naciones europeas, teniendo como "pioneros" a países protestantes como Gran Bretaña, Alemania u Holanda y como países atrasados a España, Portugal e Italia, todos ellos católicos. Esta interpretación sigue siendo muy discutida.

La Revolución Industrial se originó en Inglaterra a causa de diversos factores, cuya elucidación es uno de los temas historiográficos más trascendentes.
Como factores técnicos, era uno de los países con mayor disponibilidad de las materias primas esenciales, sobre todo el carbón, mineral indispensable para alimentar la máquina de vapor que fue el gran motor de la Revolución Industrial temprana, así como los altos hornos de la siderurgia, sector principal desde mediados del siglo XIX. Su ventaja frente a la madera, el combustible tradicional, no es tanto su poder calorífico como la mera posibilidad en la continuidad de suministro (la madera, a pesar de ser fuente renovable, está limitada por la deforestación; mientras que el carbón, combustible fósil y por tanto no renovable, solo lo está por el agotamiento de las reservas, cuya extensión se amplía con el precio y las posibilidades técnicas de extracción).

Como factores ideológicos, políticos y sociales, la sociedad inglesa había atravesado la llamada crisis del siglo XVII de una manera particular: mientras la Europa meridional y oriental se refeudalizaba y establecía monarquías absolutas, la guerra civil inglesa (1642-1651) y la posterior revolución gloriosa (1688) determinaron el establecimiento de una monarquía parlamentaria (definida ideológicamente por el liberalismo de John Locke) basada en la división de poderes, la libertad individual y un nivel de seguridad jurídica que proporcionaba suficientes garantías para el empresario privado; muchos de ellos surgidos de entre activas minorías de disidentes religiosos que en otras naciones no se hubieran consentido (la tesis de Max Weber vincula explícitamente "La ética protestante y el espíritu del capitalismo"). Síntoma importante fue el espectacular desarrollo del sistema de patentes industriales.

Como factor geoestratégico, durante el siglo XVIII Inglaterra (que tras las firmas del Acta de Unión con Escocia en 1707 y del Acta de Unión con Irlanda en 1800, después de la derrota de la rebelión irlandesa de 1798, consiguieron la unión con Escocia e Irlanda, formando el Reino Unido de Gran Bretaña e Irlanda) construyó una flota naval que la convirtió (desde el tratado de Utrecht, 1714, y de forma indiscutible desde la batalla de Trafalgar, 1805) en una verdadera talasocracia dueña de los mares y de un extensísimo imperio colonial. A pesar de la pérdida de las Trece Colonias, emancipadas en la guerra de Independencia de Estados Unidos (1776-1781), controlaba, entre otros, los territorios del subcontinente Indio, fuente importante de materias primas para su industria, destacadamente el algodón que alimentaba la industria textil, así como mercado cautivo para los productos de la metrópolis. La canción patriótica "Rule Britannia" (1740) explícitamente indicaba: "rule the waves" (gobierna las olas).

Durante la revolución industrial se vivió un incremento espectacular de la población, debido fundamentalmente a la caída de la tasa de mortalidad provocada por la mejora de las condiciones higiénicas, sanitarias y alimenticias que se plasmó en gran medida en la reducción de la mortandad infantil. En este periodo nacen las primeras vacunaciones y se mejoran los sistemas de alcantarillado y de depuración de aguas residuales. Una alimentación más abundante y regular, no sometida a las fluctuaciones de las cosechas, bajó la incidencia de las epidemias e hizo posible la casi desaparición de la mortalidad catastrófica, sobre todo la infantil.

La población de Inglaterra y Gales, que había permanecido constante alrededor de 6 millones desde 1700 a 1740, se incrementó bruscamente a partir de esta fecha y alcanzó 8,3 millones en 1801, para doblarse en cincuenta años y llegar a los 16,8 millones en 1850 y en 1901 casi se había doblado de nuevo con 30,5 millones. En Europa, la población pasó de 100 millones en 1700 hasta alcanzar 400 millones en 1900. La revolución industrial fue así el primer periodo histórico durante el que hubo simultáneamente un incremento de la población y un incremento de la renta per cápita. El aumento de la población fue un estímulo para el crecimiento industrial, ya que proporcionó a la vez mano de obra abundante para las nuevas industrias y de otro lado supuso un incremento de la demanda interna para los nuevos productos.

El aumento de la población urbana en ciudades con trazado medieval supuso el hacinamiento, la insalubridad y la aparición de las primeras patologías sociales (alcoholismo, prostitución y delincuencia).

Entre finales del siglo XVII y principios del XVIII el gobierno británico aprobó una serie de leyes con el fin de proteger a la industria de la lana británica de la creciente cantidad de tela de algodón que se importaba desde India Oriental.
También empezó a darse una mayor demanda de tejidos gruesos, los cuales eran fabricados por la industria británica en la localidad de Lancashire, donde destacaba la producción de pana, fabricada a partir de fibras entrecruzadas de lino y algodón. El lino era utilizado para dotar de más resistencia al tejido, cuyo material principal, el algodón, no tenía una resistencia suficiente, aunque esta mezcla resultante no era tan suave como los tejidos 100% algodón y era más difícil de coser.

Hasta el nacimiento de la industria textil, los tejidos y el hilado en general se realizaba en los hogares, en la mayor parte de los casos para consumo propio. Este método productivo, basado en que la producción estaba dispersa y se desarrollaba en los domicilios de los trabajadores, es a menudo denominado en inglés como sistema Putting-out ("Putting-out system") en contraposición al posterior sistema industrial o "factory system". Solo en ocasiones puntuales los trabajos se realizaban en el taller de un maestro tejedor. Bajo el sistema "putting-out" los trabajadores, antes de fabricar su producto, pactaban contratos con comerciantes y vendedores, quienes les suministraban a menudo las materias primas necesarias. Fuera de temporada, por la general, las esposas de los agricultores hacían los hilados mientras que los hombres producían los tejidos. Utilizando la máquina de hilar o rueca, en cualquier momento entre cuatro y ocho hilanderas podían echar una mano al tejedor.

Uno de los grandes inventos de la industria textil fue la lanzadera volante, patentada en 1733 por John Kay, que permitió una cierta automatización del proceso de tejido. Posteriores mejoras, destacando las de 1747, permitieron duplicar la capacidad de producción de los tejedores, lo que también agravó el desequilibrio que existía entre el hilado y el tejido. Este invento empezó a ser ampliamente utilizado en todo Lancashire en la década de 1760, cuando Robert Kay, hijo de John Kay, inventó la caja ascendente ("drop box").

Lewis Paul patentó en Birmingham, con la ayuda de John Wyatt, la máquina de hilar mediante rodillos y el sistema "flyer-and-bobbin", que conseguían un espesor más uniforme en el proceso de elaboración de la lana. Paul y Wyatt abrieron una fábrica en Birmingham que utilizaba una nueva máquina de laminado impulsada por un burro. En 1743 se abrió una fábrica en Northampton que empleaba cinco máquinas como la de Paul con cincuenta husos cada una. Estuvo en funcionamiento hasta 1764. Una fábrica similar fue construida por Daniel Bourn en Leominster, pero un incendio la destruyó. Tanto Paul como Bourn habían patentado el cardador de lana en 1748. El uso de dos conjuntos de rodillos que giraban a diferentes velocidades fue utilizado posteriormente en la primera fábrica de hilados de algodón. La invención de Lewis fue posteriormente mejorada por Richard Arkwright con su Water frame y por Samuel Crompton con su Spinning mule.

En 1764 en el pueblo de Stanhill, Lancashire, James Hargreaves inventó la hiladora Jenny, que patentó en 1770. Fue la primera máquina que empleaba varios husos de una manera eficaz. La hiladora Jenny trabajaba de una manera similar a la rueca. Era una máquina simple, construida con madera y que solo costaba alrededor de 6 libras (un modelo de 40 husos) en 1792. Era utilizada principalmente en los hogares o por pequeños artesanos. La hiladora Jenny producía un hilo ligeramente torcido solo adecuado para la trama, que se torcía.

La máquina de hilar ("Water frame") inventada por Richard Arkwright, fue patentada por este junto con dos socios en 1769. El diseño se basaba en parte en una máquina de hilado construida por Thomas High, quien fue contratado por Arkwright.

Sin embargo, y a pesar de todos los factores anteriores, la Revolución industrial no hubiese podido prosperar sin el concurso y el desarrollo de los transportes, que llevarán las mercancías producidas en la fábrica hasta los mercados donde se consumían.

Estos nuevos transportes se hacen necesarios no solo en el comercio interior, sino también en el comercio internacional, ya que en esta época se crean los grandes mercados nacionales e internacionales. El comercio internacional se liberaliza, sobre todo tras el Tratado de Utrecht (1713) que liberaliza las relaciones comerciales de Inglaterra, y otros países europeos, con la América española. Se termina con las compañías privilegiadas y con el proteccionismo económico; y se aboga por una política imperialista y la eliminación de los privilegios gremiales. Además, se desamortizan las tierras eclesiásticas, señoriales y comunales, para poner en el mercado nuevas tierras y crear un nuevo concepto de propiedad. La Revolución industrial generó también un ensanchamiento de los mercados extranjeros y una nueva "división internacional del trabajo" (DIT). Los nuevos mercados se conquistaron mediante el abaratamiento de los productos hechos con la máquina, por los nuevos sistemas de transporte y la apertura de vías de comunicación, así como también, mediante una política expansionista.

El Reino Unido fue el primero que llevó a cabo toda una serie de transformaciones que la colocaron a la cabeza de todos los países del mundo. Los cambios en la agricultura, en la población, en los transportes, en la tecnología y en las industrias, favorecieron un desarrollo industrial. La industria textil algodonera fue el sector líder de la industrialización y la base de la acumulación de capital que abrirá paso, en una segunda fase, a la siderurgia y al ferrocarril.

A mediados del siglo XVIII, la industria británica tenía sólidas bases y con una doble expansión: las industrias de bienes de producción y de bienes de consumo. Incluso se estimuló el crecimiento de la minería del carbón y de la siderurgia con la construcción del ferrocarril. Así, en Gran Bretaña se desarrolló de pleno el capitalismo industrial, lo que explica su supremacía industrial hasta 1870 aproximadamente, como también financiera y comercial desde mediados de siglo XVIII hasta la Primera Guerra Mundial (1914). En el resto de Europa y en otras regiones como América del Norte o Japón, la industrialización fue muy posterior y siguió pautas diferentes a la británica.

Unos países tuvieron la industrialización entre 1850 y 1914: Francia, Alemania y Bélgica. En 1850 apenas existe la fábrica moderna en Europa continental, solo en Bélgica hay un proceso de revolución seguido al del Reino Unido. En la segunda mitad del siglo XIX se fortalece en Turingia y Sajonia la industrialización de Alemania.

Otros países siguieron un modelo de industrialización diferente y muy tardía: Italia, Imperio austrohúngaro, España o Rusia. La industrialización de éstos se inició tímidamente en las últimas décadas del siglo XIX, para terminar mucho después de 1914.

El ferrocarril, nacido en el siglo XVIII, es uno de los grandes protagonistas de la Revolución Industrial.
En sus comienzos se empleaba la fuerza animal como medio de locomoción, los raíles eran de madera y su empleo se limitaba a las minas para el transporte de carbón. En un libro publicado en 1797, Carz aseguraba haber sido el primero que pensó en sustituir la madera por hierro. La primera concesión del Parlamento de Inglaterra para la construcción de un ferrocarril —movido por caballos— se remonta a 1801; se trataba de una línea entre Wandsworth y Croydon con unos 13 kilómetros de longitud y con un coste de 60 000 libras. La gran revolución del ferrocarril comenzó en 1814, cuando George Stephenson utilizó la máquina de vapor como medio de locomoción. Su "invento" fue un éxito y comenzó a usarse de inmediato en las minas, pudiendo transportar ocho vagones de 30 toneladas a una velocidad de 7 km/h. Estos resultados eran suficientes para expandir el uso de la máquina a otros servicios. Fue un 1821 cuando el Parlamento autorizó la construcción de la primera línea de ferrocarril con tracción de vapor entre Stockton y Darlington. La línea fue inaugurada en 1825 con una máquina maniobrada por el propio Stephenson tirando de 34 vagones a una velocidad de entre 10 y 12 millas por hora —16-19 km/h— ; El periódico "The Times" describió esta hazaña de la siguiente manera:
En los 5 años posteriores el Parlamento autorizó la construcción de 23 nuevas líneas de ferrocarril entre las que se encontraba la célebre línea entre Mánchester y Liverpool, siendo sus constructores los primeros en ofrecer en el ferrocarril el servicio de transporte de pasajeros. En aquel momento se desconfiaba de la seguridad que podían ofrecer las locomotoras, pero la acogida fue muy buena, mejorando en un 10% los beneficios derivados de este servicio, aunque los ingresos por el transporte de algodón, tejidos, carbón y ganado aún seguían siendo mayoritarios. Este éxito también fue tratado por George Porter, quien en su libro "El progreso de la nación" dice :

Fue en esta ocasión el propio Stephenson el que ganó la puja en esta línea convirtiéndose su "Cohete" en el encargado de remolcar un tren de 12 toneladas a 22 km/h. El primer correo por ferrocarril se envió el 11 de noviembre de 1830. Los tiempos de llegada se redujeron considerablemente, llegando el correo entre Londres y Manchester en aproximadamente 18 horas. En Inglaterra, siguiendo la consigna "laissez faire", el Estado no intervenía en la construcción o subvención del ferrocarril sino que se limitaba a otorgar las licencias y permisos de construcción y explotación; de esta manera se gastaron enormes fortunas con el objetivo de obtener los distintos permisos; por ejemplo el "Great Western" costó en gastos preliminares 89 000 libras y otros como el "London and Birmingham" 62 000.
Los ferrocarriles eran al principio de vía estrecha y solo admitían velocidades comprendidas entre los 15 y los 20 kilómetros por hora, pero en 1840 se habían ensanchado las vías y se podían conseguir unas velocidades de casi 40 km/h.

El primer país continental en seguir el ejemplo inglés fue Bélgica con dos líneas Bruselas-Malinas y Malinas-Amberes en 1835. El primer año transportaron 70 000 pasajeros. El coste fue bajísimo y el billete Bruselas-Amberes costaba solo un franco. El invento entró en Francia con algo de retraso, pues mientras jóvenes, ingenieros y adeptos al saintsimonismo reclamaban su construcción, tropezaban con el rechazo y la desconfianza de muchos, además de la carencia de hierro. El gobierno francés, que veía el potencial del aparato, ordenó un estudio para un plan nacional de los ferrocarriles. El estudio quedó finalizado en 1837 y los capitalistas, impacientes, presionaban al gobierno para la ejecución del proyecto con el fin de especular con las obras y los terrenos. El plan consistía en siete líneas con centro en París, que unirían el Atlántico, el Mediterráneo y el Rin. Al contrario que en Inglaterra y Bélgica, el estado se hizo cargo, al menos en parte, de su construcción y explotación, aportando 150 000 francos por kilómetro de vía y construyendo las infraestructuras necesarias. Mientras, las compañías privadas aportaron 100 000 francos para edificios y material. Tras 40 años de administración y explotación privada, el sistema pasaría al Estado. Socialistas románticos y conservadores se oponían al proyecto, los primeros reclamaban que el sistema fuera del estado desde el primer día y los segundos lo consideraban demasiado caro. Finalmente el plan fue aprobado, pero algunos acuerdos se revisaron y en la práctica la construcción y explotación corrió a cuenta casi exclusiva del sector privado. En 1857 la red estaba consolidada siendo propiedad de 6 grandes compañías. Debido a la obligación de ceder la propiedad al Estado a los 40 años de explotación se descuidó sobremanera su cuidado y mantenimiento por lo que el gobierno francés se vio en la obligación de ampliar el plazo en 99 años más, comprometiéndose incluso a pagar las obligaciones a su vencimiento.

En Alemania la primera línea se construyó en 1835 con una extensión de siete kilómetros entre Núremberg y Fürth, pero fue en 1839 cuando se construyó la primera línea de importancia entre Dresde y Leipzig, promovida por el profesor de economía política List, uno de los principales promotores de la línea Núremberg-Fürth. Pronto se vio al ferrocarril como una poderosa arma política; en el momento de la aparición del ferrocarril, Alemania se encontraba dividida en más de 300 pequeños estados y ciudades autónomas. Desde la construcción de la línea Dresde-Leipzig todas las ciudades alemanas quisieron unirse con su vecina lo que además de un gran impulso económico hizo un gran servicio para el triunfo del "Zollverein". Al contrario que en el resto de países, en Alemania fue la administración la encargada de vigilar o administrar todos los ferrocarriles. En 1850 el "Zollverein" ya poseía 5800 kilómetros casi el doble que toda Francia. Hannover, Bremen, Hamburgo, Berlín, Fráncfort formaban una gran línea que transcurría sobre los principales focos industriales y unía Alemania con Suiza a través de Basilea y a Austria a través de Moravia y Silesia.

A partir de la década de 1820 el ferrocarril y el vapor saltaron a los Estados Unidos y pronto conquistaron a la opinión pública. Stevens realizó en Hoboken una primera prueba que causó un gran interés entre los hombre de negocios de Pensilvania, quienes compraron una locomotora a Inglaterra. Al igual que en Gran Bretaña, la acumulación de capital hizo posible solo un año después el comienzo de la construcción de una primera línea entre Washington y Winchester. En 1830 una locomotora llamada "Best Friend" explotó cuando marchaba por la línea Charleston-Hambourg debido a que el maquinista se había sentado sobre la válvula de escape por las molestias que sentía debido al silbido del vapor al salir. Pero lejos de echarse atrás, el país progresó a un ritmo frenético y a mediados de 1830 ya producía sus propias locomotoras en la fundición de West Point asegurando una industria nacional sólida. Desde entonces Estados Unidos colocó raíles a través de su vasto territorio a una velocidad mucho mayor que Europa. Si en 1830 poseía tan solo 65 kilómetros de trazado —contra 316 europeos, 276 de ellos en Gran Bretaña—, 10 años después ya superaba a Europa con 4509 kilómetros contra 3543 europeos. En 1850 las vías férreas ya sumaban 14 400 kilómetros. Uno de los problemas que planteaban los ferrocarriles era el ancho de vía, que variaba en anchura en los distintos países, lo que obligaba a numerosos transbordos para deleite de los hosteleros. Pero problemas aparte el tiempo de viaje no hizo sino disminuir; así, en apenas unos años no se tardaban más de 20 horas en viajar de Boston a Nueva York en ferrocarril cuando antes se tardaban unas 80.

En Italia los augurios de d´Azeglio de que "los ferrocarriles coserían la bota" no pasaron de simples promesas, pues hasta 1845 solo se encontraban pequeñas líneas aisladas como la línea Milán-Monza, Padua-Venecia, Liorna-Pisa o la línea de Campania que Fernando de Nápoles construyó para su recreo y uso privado. En Hungría solo existía una pequeña vía alrededor de Budapest y en Rusia el zarismo tuvo que imponer la construcción de la línea Moscú-San Petersburgo debido a los numerosos detractores. En España, el gran tirón y entusiasmo que de manera muy temprana había producido el invento se apaga en la guerra civil de 1833, que paraliza todas las obras de construcción ante la desconfianza de los capitalistas. Hubo que esperar hasta 1843 cuando se concedió a Juan Manuel Roca y Miguel Biada la construcción y explotación del ferrocarril Barcelona-Mataró, que estuvo construido en solo cinco años bajo la dirección del ingeniero inglés Locke, su inauguración fue el 28 de octubre de 1848, un trayecto de 28 km y 600 m que se completaba en 35 minutos. En 1851 realizó su primer viaje el segundo ferrocarril español que cubría la línea Madrid-Aranjuez, cuya concesión había sido otorgada en 1844 con prolongación hasta Cádiz. En 1850 se inició la construcción de la primera locomotora española, finalizada en 1852.

Excepciones aparte, en el periodo entre 1820 y 1840, Gran Bretaña conservaba un adelanto manifiesto sobre el resto del mundo. Era la única que poseía una buena red de transporte entre sus principales ciudades. Trabajó con verdadero frenesí entre 1840 y 1847 a pesar de la rivalidad latente entre la oposición, los grupos financieros, los "Turnpike trusts" y la población, cuyo medio de subsistencia continuaban siendo las carreteras. Similar situación se dio en Bélgica, que en 1843 tenía incluso más kilómetros que Francia y una opinión pública muy favorable al ferrocarril.
No fueron pocos los que vieron en el ferrocarril un gran peligro, incluso mortal. Desde el siglo XVIII, cuando se pusieron en marcha en Inglaterra hubo voces, incluso procedentes de la Real Academia de Ciencias británica, que sugerían que a unas velocidades superiores a los 40 km/h los pasajeros se asfixiarían, se volverían ciegos y el ganado enloquecería. Se temía también la destrucción de las tierras de cultivo o que la gente y mercancías salieran despedidas del aparato por sus "endiabladas" velocidades.
Pasada la primera mitad de siglo, el medio siglo siguiente entre 1851 y 1901, conocido con el nombre de "Railway Age" vive el apogeo y reinado definitivo del ferrocarril. Pero la tracción mecánica sobre raíles es sobre todo, obra de Occidente. En 1860 Europa y EE. UU. se reparten más o menos 198 000 en igualdad mientras que el resto del mundo no cuenta con más de 15 000 kilómetros, la mayoría ubicados en colonias europeas. En 1910 ya se han construido más de un millón de kilómetros de los que 380 000 están en EE. UU. y 330 000 en Europa. Su construcción necesitó de un esfuerzo enorme, movilizando grandes cantidades de capital, trabajadores y estimulando la industria metalúrgica y la construcción de gigantescos talleres de trabajo, además de dar su máximo esplendor a la máquina de vapor.
Además de los vagones y locomotoras, también evolucionaron los raíles sobre los que circulaban. El raíl de acero sustituye al de hierro y a la madera de las traviesas se le empezó a inyectar cloruro de cinc para evitar que se pudriera. El ferrocarril también necesitó de una gran infraestructura que fue necesario desarrollar, como túneles, que se excavaban a costa del sufrimiento obrero a altísimas temperaturas con el uso de perforadoras de aire comprimido y el revestimiento de las galerías con fundición, en sustitución de la madera; La ventilación se lograba con sopladoras. Hay que destacar algunos éxitos entre los que se encuentran el túnel que atraviesa el Mont Cenis, construido a lo largo de 15 años y con una extensión de 13 600 m a 1300 metros de altura. Otros como el San Gotardo de más de 15 000 metros se terminaron en menos de 10 años usando la perforadora automática siendo las condiciones de trabajo nefastas: los obreros llegaron a trabajar a una temperatura de 86 grados. Fuera de Europa los estadounidenses construyeron un túnel bajo el río Hudson. Escandinavia queda unida a Alemania a través del "ferry-boats" entre Rügen y Malmoe.
Mientras que en la primera mitad de siglo la locomotora apenas había ganado en velocidad sin sobrepasar nunca los 40 km/h, hace progresos decisivos a partir de la idea del ingeniero inglés Crampton de colocar las ruedas motrices detrás de la caldera (y no debajo), ruedas que están acopladas, transfiriéndose el movimiento de rotación. En 1850 la velocidad media que se situaba en 27 km/h se eleva en 1880 a 74 km/h en Inglaterra y a 59 km/h en Estados Unidos. En 1890 el "Empire-State-Express" rebasó por primera vez en la historia los 100 km/h entre Nueva York y Búfalo. Para cruzar Francia de un extremo en ferrocarril solo se precisaban 14 horas. En esta segunda parte del siglo el coste del billete disminuyó entre un 50 y un 70 %.

Las prestaciones de la locomotora aumentaron sin cesar. El freno de mano se sustituyó por un nuevo freno hidráulico de aire comprimido. Los vagones de pasajeros fueron dotados de alumbrado de gas a base de aceite de esquisto o iluminación eléctrica a finales de siglo, siendo la línea Londres-Brighton la primera en incorporarla. La máquina de vapor, el corazón de la máquina, también procura calefacción en los vagones. El llamado "Boggie" o bastidor de varios ejes permitió al convoy dar curvas mucho más acentuadas disminuyendo los riesgos, pues se adaptaba a la curvatura de la vía. También se crearon los llamados "palace-cars" en las líneas más largas para las familias ricas en las que disfrutaban de todo tipo de comodidades y sin tener que mezclarse con el resto de pasajeros. En 1880 se instaló en la línea del Pacífico un vagón imprenta en el que se editaba un periódico diario con las noticias recibidas telegráficamente en las estaciones.

Exceptuando Gran Bretaña, Bélgica y algunas partes de España y Alemania, las vías férreas no dibujaban redes en ninguna parte antes de 1860. En Francia por fin se realizó un esfuerzo serio a partir del Segundo Imperio y en los albores de la Tercera República. En esta segunda mitad de siglo se empezaba a vislumbrar la columna vertebral de ferrocarriles europeos. Sus límites se extendían desde el norte de Francia hasta la Alta Silesia de este a oeste y de Alemania al norte de Italia de norte a sur; en el centro, Suiza reparte el tráfico por el continente. En cambio la mayor parte de Italia, la península ibérica y los países del este quedaban fuera. En Estados Unidos se siguen consiguiendo grandes logros. En 1869 se finalizó el primer transcontinental que conectó el país de este a oeste. La construcción fue dirigida por el implacable general Grenville M. Dodge como si se tratará de una campaña militar. Usó como mano de obra a los soldados desmovilizados, inmigrantes irlandeses y hasta chinos en California. Pero este triunfo no se logró con facilidad; indios, el relieve irregular y sobre todo la competencia entre Union Pacific y Central Pacific dificultaron sobremanera la situación. Pero el entusiasmo predomina y en 1893 ya había en funcionamiento otras 5 líneas transcontinentales, usándose como medio de colonización en el oeste americano o en la Columbia británica como medio de presión para conseguir su adhesión a la Unión.

Aunque tardío, se presenta el esfuerzo ruso, logrado gracias a los préstamos de Occidente. En primer lugar se construyó el transcaspiano al que a partir de 1905 complementó el transaraliano. En Siberia las dificultades eran mayúsculas: hielo, infiltraciones de agua, ríos inmensos, débil densidad humana, distancias enormes, sin olvidar el irregular relieve. Pero las viejas rutas y caminos ya no eran suficientes y el ferrocarril más largo del mundo se empezó en 1891 y alcanzó su destino, Vladivostok, gracias a un acuerdo con China, en 1902.

Así pues, el ferrocarril no solo sirvió para revolucionar el mundo del transporte tanto material como humano sino que fue empleado como un excelente instrumento de unión. Sirvió bien en la reconciliación y la anexión de nuevos territorios a Estados Unidos y el Imperio alemán sabía lo mucho que le debía al ferrocarril como para dejarlo en manos privadas. En Italia facilitó la hegemonía de la Casa de Saboya. No ocurrió igual en Francia o en Gran Bretaña, donde se encontraban mayoritariamente en manos privadas, aunque en Inglaterra prestaron un servicio inigualable, encumbrando al naciente Imperio británico a la hegemonía mundial. Hacia 1850 el ferrocarril había conducido a entre 400 y 500 millones de viajeros y entre 200 y 300 millones de toneladas de mercancías desde su nacimiento. Cinco décadas después, solo en 1905 transportó a entre 4000 y 5000 millones de viajeros.

Antes del siglo XIX la larga tradición naval europea se había sustentado sobre el control de los vientos como medio de propulsión y la seguridad más que por la velocidad en el mar. A principios de siglo no se empleaban menos de dos o tres semanas en cruzar el Atlántico de este a oeste, necesitándose entre 30 y 40 días de oeste a este. Con la formación de los imperios coloniales europeos se hizo necesario desarrollar una tecnología que asegurase el viaje sobre las aguas; en el siglo XVIII se generalizó el uso del sextante, mapas con las notaciones de los vientos y el cronómetro. La invención de la nueva embarcación partió de los trabajos de Jouffroy d´Abbens sobre el Sena y los de Fulton con su máquina "Clermont". Fue en Estados Unidos donde tuvieron lugar las primeras pruebas del navío de ruedas sobre el río Hudson. En 1815 ya circulaban un centenar de estos navíos de ruedas que obtenían su energía de la leña, material barato y abundante. El "Savannah" consiguió cruzar en 29 días el Atlántico Norte en 1819 y la "Sphink", que llevó a Francia las noticias de la toma de Argel, desarrollaba una velocidad de 6 nudos. Pero los problemas eran numerosos: las paletas utilizadas provocaban un gran desperdicio de energía, existía el riesgo de incendio o explosión a bordo, su velocidad era aún menor a la desarrollado por los veleros y el poder militar aún se oponía a su utilización como navío de guerra.

Pero a pesar de las dificultades los avances prosiguieron y en 1838, con una combinación de vapor y velas, los navíos "Sirius" y "Great Western" cruzaron el Atlántico entre Liverpool y Nueva York en 16 y 13 días respectivamente. Los grandes avances llegaron entre 1840 y 1860 con la invención de la hélice, basándose los primeros modelos en el tornillo de Arquímedes, el condensador de superficie y la máquina "Compound", que logró ahorrar grandes cantidades de combustible y la introducción de calderas cilíndricas que posibilitaron la producción de vapor a alta presión.

Lo que sí es indudable es la supremacía del velero sobre el vapor durante la mayor parte del siglo; la seguridad y prestigio de la que aún gozaba, sobre todo en Estados Unidos, donde también tenía lugar la mayoría de los avances del barco de vapor era indiscutible. En 1850 el barco de vapor había transportado ya 750 000 toneladas, aunque el vapor aún estaba muy lejos de ganar la partida.

El esfuerzo en la construcción y mejora de carreteras (o caminos) comenzó en muchas partes de Europa antes de la Revolución Industrial. Desde el fin de las guerras napoleónicas a principios del siglo XVIII y en ausencia de otros medios de comunicación más eficaces, las carreteras fueron extensamente mejoradas. A principios del siglo XIX el país más adelantado en esta materia era Francia con una red de 33 000 kilómetros de gran calidad que se extendían hasta Alemania, Suiza e Italia. Los Países Bajos, el Reino de Prusia o Suiza también habían vivido una gran mejora en las comunicaciones. En el otro extremo se encontraban lugares como Sicilia, que no empezó su construcción hasta bien entrado el XIX, la Rusia zarista, que no tendría su primera calzada entre Moscú y San Petersburgo —sus principales ciudades— hasta 1834 o España, que cuenta antes de la mitad del siglo XIX con solo 6000 kilómetros de vías, siendo además estrechas y llenas de irregularidades y deficiencias. En Gran Bretaña el rápido desarrollo de ferrocarriles y canales quita importancia a su construcción, pero aun así se suceden las ampliaciones y modernizaciones de la maltrecha red británica contando en 1850 con más de 50 000 kilómetros de trazado, 18 000 más que veinte años atrás.

La técnica en la construcción de estas vías de comunicación también mejora. En cada país se construyen de manera distinta, pero los problemas "clásicos" derivados de estas construcciones como filtraciones de agua, mantenimiento o infraestructura se solucionan en las décadas de 1820 y 1830 a partir de las mejoras introducidas por Mac Adam o Telford. El uso de la diligencia y los servicios públicos de transporte se desarrollan y generalizan con unas velocidades que oscilan entre los 10 y 15 km/h, usándose en el transporte de pasajeros, mercancías y correo. No es hasta principios del siglo XX cuando gracias al motor de explosión y el desarrollo del automóvil se de un uso masivo a estos trazados.

Los primeros canales empezaron a ser construidos en Gran Bretaña en el siglo XVIII con el objeto de comunicar los centros industriales del norte británico con los puertos marítimos del sur y Londres. Los canales fueron la primera tecnología que permitió un fácil y relativamente rápido transporte de mercancías por todo el país, pudiéndose transportar varias docenas de veces más de tonelaje por viaje que con un transporte terrestre. A esto se unía el relieve del país, completamente llano, lo que permitía que los canales fueran construidos rápidamente y a un bajo precio. A principios de la década de 1820, ya existía una red nacional consolidada. El ejemplo inglés fue copiado en Francia que con un relieve similar al británico pudo desarrollar su propio sistema, que a mediados del siglo XIX contaba con 8500 kilómetros de vías. En Alemania gracias a sus grandes ríos como el Rín y el Elba, la navegación se vio muy favorecida, así como el comercio que vivió un gran desarrollo. En otros países como España la construcción de canales no pasó de un proyecto por el difícil relieve y la falta de capitales. Fuera del continente, los estadounidenses con su ímpetu emprendedor y sus numerosos lagos y grandes ríos consiguieron desarrollar con velocidad su propio sistema, que al igual que el ferrocarril, ayudó en la colonización y explotación de las vastas tierras del país. A principios de 1835 EE. UU. ya contaba con 7000 kilómetros de canales que allanaron el camino a la introducción del barco de vapor en el país con una rapidez incluso mayor a la siempre innovadora Gran Bretaña.

El uso de los canales en Gran Bretaña empezó a decaer a partir de 1840, cuando el ferrocarril se impuso en el transporte de mercancías y pasajeros. El irregular y más tardío desarrollo a gran escala del ferrocarril en el resto de países, con la siempre notable excepción de los Estados Unidos, alargó en ocasiones el uso pleno de los canales hasta los albores del siglo XX. Hoy en día la red de canales británicos y la infraestructura ligada a esta es una de las características más perdurables y destacables de la Revolución Industrial en el país.

La existencia de controles fronterizos más intensos evitaron la propagación de enfermedades y disminuyó la propagación de epidemias como las ocurridas en tiempos anteriores. La revolución agrícola británica hizo además más eficiente la producción de alimentos con una menor aportación del factor trabajo, alentando a la población que no podía encontrar trabajos agrícolas a buscar empleos relacionados con la industria y, por ende, originando un movimiento migratorio desde el campo a las ciudades así como un nuevo desarrollo en las fábricas. La expansión colonial del siglo XVII acompañada del desarrollo del comercio internacional, la creación de mercados financieros y la acumulación de capital son considerados factores influyentes, como también lo fue la revolución científica del siglo XVII. Se puede decir que se produjo en Inglaterra por su desarrollo económico.

La presencia de un mayor mercado doméstico debería también ser considerada como un catalizador de la Revolución Industrial, explicando particularmente por qué ocurrió en el Reino Unido.

La invención de la máquina de vapor fue una de las más importantes innovaciones de la Revolución industrial. Hizo posible mejoramientos en el trabajo del metal basado en el uso de coque en vez de carbón vegetal. En el siglo XVIII la industria textil aprovechó el poder del agua para el funcionamiento de algunas máquinas. Estas industrias se convirtieron en el modelo de organización del trabajo humano en las fábricas.

Además de la innovación de la maquinaria, la cadena de montaje (fordismo) contribuyó mucho en la eficiencia de las fábricas.

La Revolución Industrial estuvo dividida en dos etapas: la primera del año 1750 hasta 1840, y la segunda de 1880 hasta 1914. Todos estos cambios trajeron consigo consecuencias tales como:

A mediados del siglo XIX, en Inglaterra se realizaron una serie de transformaciones que hoy conocemos como Revolución Industrial dentro de las cuales las más relevantes fueron:

La industrialización que se originó en Inglaterra y luego se extendió por toda Europa no solo tuvo un gran impacto económico, sino que además generó enormes transformaciones sociales.

Proletariado urbano. Como consecuencia de la revolución agrícola y demográfica, se produjo un éxodo masivo de campesinos hacia las ciudades; el antiguo agricultor se convirtió en obrero industrial. La ciudad industrial aumentó su población como consecuencia del crecimiento natural de sus habitantes y por el arribo de este nuevo contingente humano. La carencia de habitaciones fue el primer problema que sufrió esta población socialmente marginada; debía vivir en espacios reducidos sin comodidades mínimas y carentes de higiene. A ello se sumaban jornadas de trabajo, que llegaban a más de catorce horas diarias, en las que participaban hombres, mujeres y niños con salarios miserables, y carentes de protección legal frente a la arbitrariedad de los dueños de las fábricas o centros de producción. Este conjunto de males que afectaba al proletariado urbano se llamó la "Cuestión social", haciendo alusión a las insuficiencias materiales y espirituales que les afectaban.

Burguesía industrial. Como contraste al proletariado industrial, se fortaleció el poder económico y social de los grandes empresarios, afianzando de este modo el sistema económico capitalista, caracterizado por la propiedad privada de los medios de producción y la regulación de los precios por el mercado, de acuerdo con la oferta y la demanda.

En este escenario, la burguesía desplaza definitivamente a la aristocracia terrateniente y su situación de privilegio social se basó fundamentalmente en la fortuna y no en el origen o la sangre. Avalados por una doctrina que defendía la libertad económica, los empresarios obtenían grandes riquezas, no solo vendiendo y compitiendo, sino que además pagando bajos salarios por la fuerza de trabajo aportada por los obreros.

Las propuestas para solucionar el problema social. Frente a la situación de pobreza y precariedad de los obreros, surgieron críticas y fórmulas para tratar de darles solución; por ejemplo, los socialistas utópicos, que aspiraban a crear una sociedad ideal, justa y libre de todo tipo de problemas sociales (para algunos, el comunismo). Otra propuesta fue el socialismo científico de Karl Marx, que proponía la revolución proletaria y la abolición de la propiedad privada (marxismo); también la Iglesia católica, a través del papa León XIII, dio a conocer la Encíclica "Rerum Novarum" (1891), primera encíclica social de la historia, la cual condenaba los abusos y exigía a los estados la obligación de proteger a lo más débiles. A continuación, un fragmento de dicha encíclica: Estos elementos fueron decisivos para el surgimiento de los movimientos reivindicativos de los derechos de los trabajadores. Durante el siglo XX en medio de los procesos de democratización, el movimiento obrero lograba que se reconocieran los derechos de los trabajadores y su integración a la participación social. Otros ejemplos de tendencias que buscaron soluciones fueron los nacionalismos, así como también los fascismos en los cuales se consideraban a los obreros y trabajadores como una parte fundamental en el desarrollo productivo de la nación, por lo que debían ser protegidos por el Estado.

Uno de los principios fundamentales de la industria moderna es que nunca considera a los procesos de producción como definitivos o acabados. Su base técnico-científica es revolucionaria, generando así el problema de la obsolescencia tecnológica en períodos cada vez más breves. Desde esta perspectiva puede afirmarse que todas las formas de producción anteriores a la industria moderna (artesanía y manufactura) fueron esencialmente conservadoras, al trasmitirse los conocimientos de generación en generación sin apenas cambios. Sin embargo, esta característica de obsolescencia e innovación no se circunscribe a la ciencia y la tecnología, sino debe ampliarse a toda la estructura económica de las sociedades modernas. En este contexto la innovación es, por definición, negación, destrucción, cambio, la transformación es la esencia permanente de la modernidad.

El desarrollo de nuevas tecnologías, como ciencias aplicadas, en un receptivo clima social, es el momento y el sitio para una revolución industrial de innovaciones en cadena, como un proceso acumulativo de tecnología, que crea bienes y servicios, mejorando el nivel y la calidad de vida. Son básicos un capitalismo incipiente, un sistema educativo y espíritu emprendedor. La no adecuación o correspondencia entre unos y otros crea desequilibrios o injusticias. Parece ser que este desequilibrio en los procesos de industrialización, siempre socialmente muy inestables, es en la práctica inevitable, pero mensurable para poder construir modelos mejorados.





</doc>
