<doc id="21866" url="https://es.wikipedia.org/wiki?curid=21866" title="Historia de Nicaragua">
Historia de Nicaragua

La Historia de Nicaragua recorre el período temporal desde la llegada de los europeos a tierras americanas a la actualidad.<br>
Los primeros colonos españoles conducidos por Gil González Dávila penetraron en Nicaragua hacia el año 1520 y el país fue agregado a la Capitanía General de Guatemala. La dominación española permaneció limitada a la costa del océano Pacífico y las áreas inmediatas.

El origen del nombre Nicaragua no está del todo claro, y aún hoy divide a los historiadores y estudiosos del lenguaje. Según una versión, proviene del náhuatl "nic-anahuac" ("hasta aquí los de anahuac"), otra versión, considera que proviene de una voz maya. Existe, entre otras, la más difundida versión aunque también la menos respaldada por los expertos, según la cual el nombre "Nicaragua" se deriva del nombre de Nicarao, quien supuestamente fue un jefe amerindio asentado en el territorio del actual departamento de Rivas que recibió a los primeros conquistadores españoles a orillas del actual Lago Cocibolca, de Granada o Gran Lago de Nicaragua, al que Gil González Dávila llamó ""la Mar dulce"".y sobre todo bello nombre

Cristóbal Colón, descubrió la costa Caribe de Nicaragua, el 12 de septiembre de 1502, cuando se refugió de una tormenta al doblar la desembocadura del río Coco en el cabo Gracias a Dios en su cuarto y último viaje. Posteriormente, desembarco en la desembocadura del río Grande de Matagalpa al que llamó ""río del Desastre"" porque en sus fuertes corrientes perdió una de sus naves.

Gil González Dávila fue el primer explorador de conquista que visitó parte de las regiones costeras del Pacífico nicaragüense en 1522-1523, durante su recorrido tuvo contacto con un poderoso cacique indígena llamado Nicaragua, Niqueragua o Nicarao, en cuyos dominios se bautizaron 9.017 personas y se recogieron 18.506 pesos de oro bajo. Después González Dávila se trasladó a un territorio llamado Nochari, situado unas seis leguas al norte de la corte del rey Nicarao, donde habitaban cinco reyes llamados Ochomogo, Nandapia, Mombacho, Morati y Gotega (Coatega). Allí se bautizaron 12,607 personas más, y un poderoso jefe llamado Diriangén vino con un suntuoso cortejo a entrevistarse con ´los españoles, pero a los pocos días, el 17 de abril de 1523, regresó para enfrentarlos en combate. La expedición logró vencer a los guerreros de Diriangén, pero tuvo que retirarse a los dominios de Nicarao, donde hubo otro enfrentamiento con los indígenas. Finalmente, González Dávila optó por marchar hacia el sur, y en el golfo de Nicoya se reembarcó con destino a Panamá, sin haber dejado fundación alguna.

En 1524, Francisco Hernández de Córdoba, enviado por el gobernador de Castilla del Oro Pedrarias Dávila, fundó las dos primeras ciudades en lo que seria más tarde Nicaragua: Granada, a orillas del Lago Cocibolca, y Santiago de los Caballeros de León, a orillas del Lago Xolotlán.

Bajo la gobernación de Pedrarias Dávila (1528-1531), la tierra que luego sería llamada Nicaragua sufrió una alarmante despoblación por los abusos de Pedrarias, quien hizo gala de un extremado salvajismo en su búsqueda de recursos y esclavos para las minas de en el cerro Potosí, y para servir de ""cargueros"". A lo anterior se unieron las epidemias de enfermedades desconocidas, algunas de origen europeo que aniquilaban a los indígenas, y las propias de la tierra, que hacían mella en los conquistadores. Los abusos que este gobernador cometía en su continua búsqueda de riqueza forzó a huir a la población. Indios y españoles (mando a decapitar al Capitán Hernández de Córdoba, acusándolo falsamente de traición), fueron víctimas por igual de los métodos de exacción que Pedrarias puso en práctica. Pedrarias murió con 96 años el 6 de marzo de 1531 y le sucedió Rodrigo de Contreras que gobernó el territorio desde 1534 hasta 1542 siguiendo la senda de abusos que Dávila había iniciado.

Durante el periodo colonial, Nicaragua formó parte de la Capitanía General de Guatemala. Durante ese periodo Nicaragua fue la principal vía de comunicación entre el Pacífico y el Atlántico ya que tenía un sistema de transporte lacustre que facilitaba el movimiento de materias y personas a regiones aledañas. El Realejo fue en particular uno de los puertos principales en el Pacífico donde se construyeron gran parte de los galeones entre Manila y Acapulco. El Realejo, entre los siglos XVI y principios del XIX, sirvió como uno de los puertos principales en el comercio de esclavos para las colonias en el Pacífico como Perú, Ecuador, Colombia, Acapulco, y como punto de concentración de las riquezas que se obtuvieron por medio del comercio bimetálico (Plata para China por medio de Manila, y oro para España). Gran parte de esos movimientos, pasaron por Nicaragua ya que era la más fácil y mejor protegida, aun así Nicaragua fue atacada por diferentes naciones, Inglaterra en particular.

En el siglo XVII, los ingleses se establecieron un protectorado en la Costa de los Mosquitos, así llamada por el nombre de los habitantes indígenas misquitos, con los que los ingleses se mantuvieron en buenas relaciones. Fundaron allí la ciudad de Bluefields y posteriormente ayudaron al establecimiento del llamado Reino de la Mosquitia.

Hasta fines del siglo diesiocho el actual territorio nicaragüense estaba dividido en una gobernación de Nicaragua, con capital en León, y con corregimientos en Chontales, El Realejo, Matagalpa, Monimbó y Quezalguaque. En 1787, estos corregimientos fueron suprimidos y, junto con el corregimiento de Nicoya, anexados a Nicaragua, que se convirtió en una Intendencia, con sede León, del reino de Guatemala.

En las Cortes de Cádiz, la Intendencia de Nicaragua estuvo representada por el licenciado José Antonio López de la Plata, quien junto con su colega de Costa Rica Florencio del Castillo logró en 1812 que se creara la Provincia de Nicaragua y Costa Rica, como unidad política y administrativa distinta de Guatemala. Esta provincia desapareció debido a la restauración absolutista de 1814 y fue restablecida en 1820, al ponerse nuevamente en vigencia la Constitución de Cádiz. El Intendente de Nicaragua, Miguel González Saravia y Colarte, se convirtió en Jefe Político Superior de la Provincia de Nicaragua y Costa Rica. La provincia se dividía en siete partidos: Costa Rica, El Realejo, Granada, León, (Rivas), Nicoya y Nueva Segovia.

Los acontecimientos independentistas de México, en concreto la puesta en marcha del Plan de Iguala, provocaban mucha agitación en las provincias que habían pertenecido al reino de Guatemala y que en el marco de la Constitución de Cádiz ya había dejado de ser una sola unidad política: Chiapas, Guatemala (con El Salvador), Comayagua (Honduras), y la Provincia de Nicaragua y Costa Rica.

Con la total indiferencia de las clases populares, los grandes terratenientes y la jerarquía católica se habían ido definiendo en dos grande grupos y cada uno de ellos editaba un periódico. El grupo proindependentista, que editaba el diario "El editor constitucional", estaba encabezado por Pedro Molina, José María Castilla, Manuel Monfúfar y José Francisco Barrundia. El otro grupo era partidario de estar a la expectativa y ver que pasaba. Este editaba el diario "El amigo de la patria" y lo encabezaban José Cecilio del Valle. 

El territorio de Chiapas, que hasta 1820 había pertenecido al reino de Guatemala, se adhirió al plan de Iguala anexionándose a México. Cinco días después, el 15 de septiembre de 1821, se realizó una reunión de personas nobles de la Ciudad de Guatemala convocada por el Jefe Político Superior de Guatemala Gabino Gaínza en donde se llegó al acuerdo de declarar la independencia pero hacerla efectiva tras la aprobación en un Congreso de las provincias. Se constituyó una Junta Provisional Consultiva presidida por Gaínza, de la que formó parte como Ministro de Hacienda el jurisconsulto Miguel Larreynaga, nacido en Telica. 

En un pequeño intervalo de tiempo, menos de 6 años, España perdía la mayoría de sus posesiones en América, para el 2 de diciembre de 1821 solo mantenía Cuba, Puerto Rico y unos pocos puntos aislados en la costa de Colombia. En la península el desorden imperaba por todos los lados, guerrillas operando en Galicia, Cataluña y Castilla, sublevación incluso de la guardia real y el país al borde de la guerra civil llegando a la intervención extranjera en 1823 de los llamados Cien Mil Hijos de San Luis, lo que en 1763 era un fuerte imperio mundial se veía convertido en una mera sombra.

Los puntos básicos del plan de Iguala, que estaba ejecutando Iturbide en México, eran: independencia del país, unidad de criollos y españoles, religión oficial la Católica y organización política como monarquía constitucional bajo Fernando VII, eran apoyados, y hechos suyos, por la oligarquía de Guatemala. Esto producía la independencia del país pero sin ningún cambio social. 

La similitud de intereses y el hecho de la anexión de Chiapas a México, llevó a Gabino Gaínza, jefe político superior, a convocar una reunión el 5 de enero de 1822 para proponer la incorporación de Guatemala a México. La propuesta fue aceptada, y Guatemala pasó a integrarse en el Imperio Mexicano de Agustín de Iturbide.

El 11 de octubre de 1822 la Diputación Provincial de Nicaragua y Costa Rica, reunida en León, proclamó la independencia absoluta de España y la anexión a México. Aunque todos los pueblos apoyaron la independencia, los partidos de Granada y Costa Rica se separaron de la provincia, y constituyeron Juntas Gubernativas separadas de las autoridades de León. Pronto se exacerbaron los ánimos y a principios de 1823, estalló una guerra civil cuando León atacó Granada, sin éxito.

El 19 de marzo de 1823 el general mexicano Antonio López de Santa Anna emprendió una campaña militar contra Iturbide y logró derrotarlo. Los partidarios de la independencia total llamaron a la organización de un Congreso de las cinco provincias del reino de Guatemala. El general Filísola convocó el congreso, al que no asistió Chiapas, confirmando así su definitiva separación de Guatemala. El congreso se reunió en la Ciudad de Guatemala el 24 de junio de 1823 y el 1 de julio se proclamaba que 

Nacían de esta forma las Provincias Unidas de Centroamérica, un nuevo estado compuesto por la unión de las cinco provincias Nicaragua, Guatemala, Honduras, El Salvador y Costa Rica.

El congreso del nuevo estado redactó la Constitución que se proclamó el 22 de noviembre de 1824 y rebautizó al país con el nombre de "República Federal Centroamericana" y las provincias pasaron a ser Estados. La constitución fue jurada el 15 de abril en los cinco estados. En Nicaragua la juró Manuel Antonio de la Cerda. En Nicaragua tardaron en consolidarse las instituciones, debido a la guerra civil causada por la rivalidad entre las ciudades de Granada y León.

Granada era el principal centro conservador del país, ya que en ella residían los más importantes terratenientes, productores principalmente de café y de azúcar. En León, en cambio, predominaban las clases medias artesanales y mercantiles. En tanto que Granada era el bastión del conservatismo político, León era el principal centro del liberalismo de Nicaragua. La rivalidad ideológica entre estas dos ciudades marcará la historia del siglo XIX en Nicaragua. 

El primer Jefe Supremo del Estado de Nicaragua fue el granadino Manuel Antonio de la Cerda, antiguo dirigente independentista, que asumió el poder el 10 de abril de 1825. Su vicejefe, Juan Argüello, conspiró contra él y lo derrocó al año siguiente. Tuvo lugar una nueva guerra civil entre los partidarios de Cerda y los de Argüello. Argüello estableció la capital en León, pero Granada se negó a reconocer su autoridad. El 27 de noviembre de 1829, De la Cerda fue fusilado por orden de Argüello. Finalmente, los enviados del gobierno federal de las Provincias Unidas lograron la pacificación de Nicaragua, tras el nombramiento como Jefe Supremo de Dionisio Herrera, que se mantendría en el poder entre 1830 y 1833. Pocos años después, siendo Jefe Supremo José Núñez (1838-1841), Nicaragua optó por separarse de la Federación centroamericana.

La constitución de la República Federal de Centroamérica fue hecha a la medida de los intereses de la oligarquía local de cada una de las antiguas provincias que buscaban mantener su libertad de acción en sus territorios. Los ejemplos de la revolución de Haití, con el levantamiento de los negros y mulatos, o la de Venezuela con la rebelión de las clases populares aterraban a estos terratenientes y les obligó a encerrarse en su provincia, ahora convertida en República. Esto hizo que se desbaratara la frágil unidad que había dejado la Constitución de tal forma que el 30 de abril de 1838 Nicaragua nacía como estado independiente. 

El 12 de noviembre de ese mismo año se establecía la primera Constitución de Nicaragua, que declaraba "la soberanía" de la nueva nación, y "establecía un régimen parlamentario". Según la constitución, el poder ejecutivo correspondía a un "Supremo Director", cuyo mandato duraría dos años. 

Los quince años siguientes (1838-1853) se denominan en la historia de Nicaragua, por este motivo, el "período del Directorio" que estuvo marcado por el caos político y social que imponía la rivalidad de leoneses y granadinos que propició la invasión del país por tropas procedentes de El Salvador y Honduras (1844-1845), bajo el mando del general salvadoreño Francisco Malespín, que saqueó la ciudad de León. 

En 1852, siendo Senador Director del Estado Fulgencio Vega (apoyado por el General Fruto Chamorro, la capital se fijó de manera definitiva en Managua, con el propósito de poner fin a la sempiterna rivalidad entre León y Granada, aunque esta decisión no se haría efectiva hasta 1858. 

El 26 de febrero de 1853 fue elegido Supremo Director del Estado de Nicaragua el conservador Fruto Chamorro. Bajo su mandato, una nueva Asamblea Constituyente elaboró una nueva Constitución, que puso fin al período del Directorio y dio inicio al periodo Presidencial. 

Durante este período, Nicaragua se había convertido en "objeto de deseo" para dos grandes potencias, Gran Bretaña y Estados Unidos, dadas las condiciones que su territorio ofrecía para la construcción de un canal entre los océanos Atlántico y Pacífico.

El 12 de agosto de 1841 el superintendente de Belice acompañado por el supuesto monarca mosquito desembarcan en San Juan del Norte y comunican a las autoridades nicaragüenses que esa ciudad y el resto de la Costa Atlántica pertenece al reino de Mosquitia. El 10 de septiembre el embajador inglés hace saber al gobierno nicaragüense que el reino de Mosquitia es un protectorado británico cuyos límites se extienden desde el cabo Honduras hasta la desembocadura del río San Juan.

Detrás de esta decisión y de la creación de este "reino" en la llamada Costa de los Mosquitos estaba la posibilidad de la construcción de un canal interoceánico (Nicaragua y Panamá son los lugares idóneos para la construcción de un canal que una los dos océanos, para 1835 los estadounidenses ya habían comenzado sus movimientos para la construcción de un canal por Panamá por lo que Inglaterra solo tenía la posibilidad de hacerlo en Nicaragua) para ello se aprovecharía el tramo navegable del río San Juan que desde su desembocadura llegaba hasta el Lago de Nicaragua. San Juan del Norte quedó incorporado al reino de Mosquitia y paso a denominarse "Greytown".

El reino de Mosquitia no continúo al sur, en Costa Rica, dado que el gobierno de ese país se opuso por las armas, bajo el mando del presidente Braulio Carrillo.

Nicaragua mandó al General Trinidad Muñoz a tomar la plaza pero el 1 de enero de 1848 los ingleses la recuperaron de nuevo para la Mosquitia. Después hubo otra escaramuza con Muñoz y de nuevo los ingleses, el 8 de febrero entraron en San Juan y subieron por el río hasta San Carlos. 

Nicaragua optó por la vía diplomática y establece conversaciones con Inglaterra implicando a los Estados Unidos. De esas conversaciones surgió el tratado Clayton-Bulwer, firmado el 19 de abril de 1850 por británicos y estadounidenses, en el que Gran Bretaña renunció a sus pretensiones sobre un futuro canal interoceánico en Nicaragua y que San Juan del Norte fuera declarado "puerto libre y territorio neutral" bajo el reino de Mosquitia.

En 1854 Nicaragua se constituyó en República y se instituyó la Presidencia por un período de cuatro años. El primer Presidente de Nicaragua fue el propio Fruto Chamorro, que asumió el nuevo cargo ese mismo año. Sin embargo, estalló una nueva guerra civil entre "legitimistas" (conservadores) y "democráticos" (liberales), por lo cual la nueva Constitución no llegó a entrar en vigor.

El 17 de octubre llegaron las tropas contratadas por Byron Cole al puerto de San Juan del Sur y se dirigieron a la conquista del fuerte de San Carlos como pasajeros en uno de los vapores de la compañía. Fueron repelidos y se vieron obligados a volver a su punto de partida. Poco después, el capitán del fuerte dio el alto a un vapor de la compañía. El capitán del barco no obedeció la orden y desde el fuerte se ordenó abrir fuego. El resultado fue la muerte de una mujer y un niño. Walker, que permanecía en Granada, reaccionó mandando fusilar a Mateo Mayorga, Ministro de Relaciones Exteriores del gobierno de Estrada.

Granada recibió la visita del embajador estadounidense, demostrándose con este hecho el apoyo de su gobierno al filibustero. Poco después, Castellón ascendió a Walker a general de brigada. Al poco tiempo, el 30 de octubre, Walker nombró presidente del gobierno provisional a Patricio Rivas desconociendo la autoridad de Castellón.

Estos sucesos se basaron en el acuerdo que Walker había firmado con el general Ponciano Del Corral, que estaba al mando de las fuerzas de Rivas, por el cual Corral sería nombrado Ministro de la Guerra y Walker Jefe militar. Cinco días después, el general Corral fue detenido y juzgado por alta traición. Condenado a muerte, murió fusilado el 8 de noviembre de 1855.

El 23 de noviembre se publicó un decreto del presidente Rivas por el cual cada adulto que llegara a Nicaragua recibiría 250 acres de tierra, cien más si era casado. Alentados por estas promesas, llegaron al país 1.200 estadounidenses más como colonos, que supusieron un importante refuerzo para Walker.

La Compañía de Tránsito pasó a ser codiciada por William Walker y para ello hizo que el presidente Rivas nombrara Ministro de Hacienda a Parker R. French, hombre de confianza del filibustero. Los dueños de la Compañía reaccionaron y lograron que el presidente de los Estados Unidos, Franklyn Pierce, prohibiera que los estadounidenses se pudieran sumar a las tropas de Walker bajo la amenaza de que iban a dejar de estar bajo el protectorado de los Estados Unidos.

Después de intentar la vía diplomática para lograr el favor del presidente de EE.UU. sin conseguirlo, el 18 de febrero de 1856 el gobierno nicaragüense publicó un decreto por el que suspendía las actividades de la Compañía y embargaba sus propiedades. Al día siguiente la concesión fue otorgada a dos hombres de confianza de Walker, quien se alió con los otros socios de Vanderbilt, a espaldas de este. Un mes después, Valderbilt suspendió el servicio de barcos de Estados Unidos a Nicaragua.

El interés británico por San Juan del Norte, que querían integrar dentro del "Reino de la Mosquitia", la amenaza que percibía Costa Rica sobre su territorio y negocios al verse amenazado el puerto de San Juan del Norte que también era usado por los costarricenses, hicieron que se fraguara una alianza de los países vecinos, con apoyo inglés, para combatir al filibustero. A principios de 1856 ya existían condiciones para que pudieran enfrentarse con posibilidades de éxito, contra las tropas de Walker.

Después de una campaña de descrédito contra Costa Rica orquestada por Walker desde Granada, el filibustero intentó infructuosamente que un hombre de su confianza, el coronel Luis Schlessinguer, se entrevistara con el presidente costarricense Juan Rafael Mora Porras.

El presidente de Costa Rica Juan Rafael Mora Porras, contaba con un cuerpo de oficiales e infantería entrenados por instructores franceses por los últimos tres años, lo que le permitió tomar la decisión de encabezar una columna militar hacia Nicaragua. El entrenamiento y organización de los militares costarricenses era completamente desconocido por Walker.

Las tropas de Walker y las costarricenses se enfrentaron el 20 de marzo cerca de la frontera con Nicaragua, en la Hacienda Santa Rosa en Costa Rica. Las tropas de Walker fueron derrotadas en 15 minutos. Los sobrevivientes huyeron hacia Nicaragua, informando a sus superiores que fueron atacados por columnas regulares del ejército francés, dado que ellos pensaban firmemente que los pobladores centroamericanos, no poseían ninguna capacidad militar.

Una vez asegurada la Hacienda Santa Rosa los costarricenses tomaron San Juan del Sur, La Virgen y Rivas. El contraataque de Walker contra la ciudad de Rivas fue rechazado el día 11 de abril, pero una semana después el cólera arrasó la ciudad, obligando a los costarricenses a regresar a su país.

El control de la ruta de tránsito era codiciado tanto por los británicos, como por los estadounidenses, que lo hacían a través de Walker y el gobierno de Patricio Rivas.

Walker depuso al presidente Patricio Rivas el 20 de junio de 1856 y nombró presidente de Nicaragua a Fermín Ferrer. Walker, acusado de traición por Patricio Rivas, convocó elecciones presidenciales en Granada y Rivas cuyo resultado dieron la presidencia del al filibustero. Walker fue investido presidente en un solemne acto en el cual el presidente saliente fue Fermín Ferrer. El gobierno de Walker fue reconocido inmediatamente por los Estados Unidos.

En León, Máximo Jerez contaba con una fuerza de unos 500 hombres que iba creciendo con los que llegaban de El Salvador y Guatemala. En septiembre había en León más de 3.000 soldados.

El 22 de septiembre, Walker decretó el establecimiento de la esclavitud en Nicaragua (que había sido abolida en 1824), con lo que se ganó el apoyo de los estados del sur de Estados Unidos. El 24 de septiembre, las fuerzas de León ocuparon Managua, el 2 de octubre entraron en Masaya y el 31 en Rivas. El 8 de diciembre, Walker atacó el puerto de San Jorge e incendió la ciudad de Granada. Tomó San Jorge, que abandonó para tomar Rivas. San Jorge quedó en manos de los aliados y Walker y los suyos quedaron aislados en Rivas y San Juan del Norte. El cerco se mantuvo la primera mitad del año 1857, en que se comenzó a recibir asistencia desde EE. UU..

Desde San Juan del Norte, Walker lanzó una ofensiva sobre los puestos de La Trinidad y el Castillo Viejo en el río San Juan, donde fue derrotado por los costarricenses, quienes efectuaron una operación anfibia, capturando todos los vapores de la Ruta del Tránsito y tomando prisioneros al personal de Walker sin disparar un tiro. El 22 de marzo comenzó el asalto a Rivas por parte de los aliados. Los soldados de Costa Rica tomaron el centro de la ciudad, pero se continuaba luchando en los barrios. El día 26 llegó el resto de las tropas, que fue conquistando la ciudad barrio a barrio. El 11 de abril todavía había resistencia en la ciudad. Mientras tanto, frente a San Juan del Sur se hallaba la corbeta de guerra Saint Mary de la armada de EE.UU.

Por el otro lado, por el puerto de San Juan del Norte en la desembocadura del río San Juan, llegaban tropas filibusteras y las tropas costarricenses que habían tomado previamente la Ruta del Tránsito y que se encontraban fuertemente armadas, se dispusieron a tomar la plaza. Un destacamento naval inglés se encuentra frente a la misma y su capitán, Comodoro John Erskine, se presta a servir de intermediario. El 13 de abril de 1857 abandonan la plaza de San Juan del Norte las tropas filibusteras.

En Rivas, Walker resiste en el centro de la ciudad. El 27 de abril los aliados cargan contra las posiciones de Walker y el capitán de la corbeta de guerra de EE.UU. Saint Mary, Charles Davis, interviene logrando sacar a Walker en su barco que deja aguas nicas a comienzos de mayo.

A finales de noviembre de 1857 William Walker ataca la ciudad de San Juan del Norte. Había obtenido recursos de los estados del sur de EE.UU. que se había ganado con el establecimiento de la esclavitud en Nicaragua. El objetivo era que Nicaragua pasara a ser un estado esclavista más de la Unión. Tras San Juan del Norte cayó Castillo Viejo y cuando ya se estaba preparando de nuevo la campaña para volver a expulsar al filibustero, este se rinde ante el Capitán estadounidense de una flota de guerra compuesta por naves estadounidenses e inglesas, así lograr salvar su vida y regresa a los Estados Unidos.

Walker volvería a Centroamérica en 1860, esta vez a Honduras donde sería apresado y fusilado en Trujillo el 12 de septiembre de 1860.

Al concluir la Guerra Nacional de Nicaragua con la victoria deL Ejército Aliado Centroamericano, producto del Pacto Providencial entre legitimistas y democráticos, se constituye el Gobierno "Binario" ("Chachagua"), con dos Presidentes, los Generales Tomás Martínez y Máximo Jerez Tellería. 

El 15 de abril de 1858 se firmó con Costa Rica el llamado tratado Cañas-Jerez como una solución a la creciente tensión limítrofe que existía entre los dos países.

Ese mismo año se promulgó una tercera constitución, que fue la vigente durante las tres décadas siguientes, período de la historia política conocido como "Primera República Conservadora" o ""Treinta años conservadores"". Con 35 años de vigencia, es hasta hoy el período de vida democrática más duradera de la historia de Nicaragua.

Tras el período transitorio de un año en que la jefatura del estado fue ocupada por dos presidentes ("gobierno binario"), el conservador Tomás Martínez fue elegido presidente de Nicaragua para el período 1859-1863. Aunque según la Constitución de 1858 no era posible presentarse a un segundo mandato presidencial, Martínez se hizo reelegir en 1863, lo cual motivó la insurrección del liberal Máximo Jerez y del conservador Fernando Chamorro. Ambas insurrecciones fueron vencidas, y Tomás Martínez gobernó hasta 1867.

Le sucedió Fernando Guzmán (1867-1871), durante cuyo mandato continuó la inestabilidad política. Una nueva guerra civil, que estalló el 25 de junio de 1869, se resolvió gracias a la mediación estadounidense. Le sucedieron Vicente Quadra (1871-1875), Pedro Joaquín Chamorro (1875-1879), Joaquín Zavala (1879-1883), Adán Cárdenas (1883-1887), Evaristo Carazo (1887-1889) y Roberto Sacasa (1889-1893). Durante todo este período estuvo en vigencia el sufragio censitario, según el cual solo los grandes propietarios tenían derecho a emitir su voto. La normalidad fue interrumpida por el levantamiento del militar liberal José Santos Zelaya, que puso fin en 1893 a las tres décadas de dominio conservador. 

Durante la última parte de los "treinta años conservadores", el café se convirtió en el centro de la economía del país. Para dar salida a las exportaciones de este producto se mejoraron notablemente los transportes, con la introducción del ferrocarril. Se promulgaron leyes agrarias que favorecían a los grandes terratenientes cultivadores de café. 

La Costa de los Mosquitos, protectorado británico, pasó a Honduras en 1859 y, finalmente, a Nicaragua, en 1860. Sin embargo, mantendría su autonomía hasta 1894, cuando el general José Santos Zelaya, que el año anterior había llegado al poder gracias a una revolución liberal, la reintegró a Nicaragua.

El Doctor y General José Santos Zelaya (1853-1919) gobernó Nicaragua durante dieciséis años, entre 1893 y 1909, ejerciendo un gobierno ilustrado, aunque dictatorial. 

Su gestión gubernamental provocó gran desarrollo en el país de Nicaragua. Reformó al Estado promulgando leyes, códigos y reglamentos modernos, creó nuevas instituciones e introdujo el Habeas Corpus. Zelaya convirtió a Nicaragua en la nación más próspera y rica de Centroamérica. Instauró la educación primaria gratuita y obligatoria, construyó escuelas, se aumentó la cobertura del telégrafo y el servicio de correo postal. Bajo su gobierno, se dio impulso a la construcción de líneas ferroviarias, y al transporte marítimo, con la introducción de la navegación a vapor en el lago Managua y la realización de importantes obras en los puertos de San Juan del Sur y San Juan del Norte.

Bajo el signo del progreso, Zelaya inició además una serie de reformas en el país, como la institución de la enseñanza laica y del matrimonio civil, y decretó la confiscación de los bienes de la Iglesia, incluyeno la "secularización" de los cementerios que pasaron a ser administrados por el Estado.

Era partidario de la creación de unos "Estados Unidos de América Central", lo que le llevó a apoyar a otros partidos liberales de distintos países centroamericanos que pudieran defender el mismo proyecto, y a promover diversas conferencias unionistas centroamericanos, especialmente las cumbres presidenciales celebradas en Corinto y el Pacto de Corinto. Esto se evidenció en el establecimiento de una efímera federación de naciones centroamericanas, la República Mayor de Centroamérica, que duró tres años (1895-1898) y de la que solamente formaron parte, además de Nicaragua, El Salvador y Honduras. 

Su mayor logro fue en 1894 con la reintegración a Nicaragua del territorio de la Costa de los Mosquitos, o reino de Mosquitia, que estaba bajo protectorado británico.

En 1860, Inglaterra reconocía los derechos de Nicaragua sobre la Mosquitia pero aun así se reservaba ciertos privilegios que había cuidado de introducir en el tratado firmado ese año entre ambos países. Desde esa fecha la Mosquitia dejó de ser un reino y pasó a ser una reserva cuya autoridad máxima era un Jefe de la etnia misquito con la característica que el cargo de jefatura era hereditario.

Inglaterra, apoyada por Austria, que actuaba como árbitro, declaró en 1888 que Nicaragua no podía mantener fuerzas policiales ni militares en el territorio de la Mosquitia. 

En 1894 Nicaragua entra en una corta guerra contra Honduras y en el marco de este conflicto desplaza tropas a Bluefields. La presencia de los soldados nicaragüenses hace que haya cierto revuelo entre los pobladores, y el General Rigoberto Cabezas Figueroa, que comandaba las tropas, decide tomar la plaza, desconocer la autoridad del Jefe Mosco y declarar la ley marcial el 12 de febrero de 1894. Los ingleses respondieron desembarcando tropas desde el navío "Cleopatra", lanzando un ultimátum que el general Cabezas desconoce, se llegó a un acuerdo y se constituyó una autoridad provisional con representación mosquita e inglesa. 

En junio se produjeron levantamientos en Corn Island, el día 3, y en Bluefields dos días después. Los levantamientos estaban encabezados por el jefe de la reserva, Robert Henry Clarence, pero auspiciados por el vicecónsul británico E. D. Hatch (en realidad su título era de "procónsul" y no tenía "exaquátur" del gobierno de Nicaragua). En el levantamiento participaron, además de los comerciantes de Bluefields, ciudadanos estadounidenses, ingleses, jamaicanos y alemanes. Los ingleses participaron con las tropas de los navíos "Cleopatra", "Mahauk" y "Magicienme", y el capitán del crucero estadounidense "Marblehead" actuó como mediador en ciertas ocasiones.

La lucha se entabló en la ciudad de Bluefields y en El Bluff, que cayeron en manos nicaragüenses el 3 de agosto y el 31 de julio respectivamente, tomadas por las tropas al mando del General Cabezas.

La administración de Zelaya mantuvo tensas relaciones y desacuerdos con Estados Unidos, lo que llevó a este a dar ayuda a los opositores conservadores de Zelaya en Nicaragua. 

En 1907, luego de la victoria nicaragüense en una breve guerra contra Honduras y El Salvador, resuelta en lo político con la mediación de Estados Unidos, en un tratado firmado en Chicago el 23 de abril de 1907, según el cual cada nación debería abstenerse de inmiscuirse en los asuntos de las demás, y, en caso de conflicto, las cuatro se comprometían a aceptar la decisión de un Tribunal de Justicia Centroamericano, cuya sede se instituyó en Cartago (Costa Rica). A pesar del tratado, buques de guerra estadounidenses ocuparon diversos puertos de Nicaragua. La situación llegó al punto de existir un conflicto interno entre los liberales nicaragüenses por un lado, y los conservadores y Estados Unidos por otro (que los financiaba).

Para 1909 el presidente Zelaya se negaba a contratar empréstitos financieros en Nueva York y no quería negociar la posible vía interoceánica en las condiciones que los Estados Unidos querían imponer, Zelaya buscaba el apoyo de otras potencias, ese mismo año contrató con Inglaterra un empréstito por 1.250.000 libras esterlinas para impulsar el ferrocarril al Atlántico y mejorar las finanzas del país. Al mismo tiempo, se habló de una oferta de concesión de un canal interoceánico por Nicaragua, al Japón o Alemania. 

El 10 de octubre de 1909 estalló la costa oriental (caribeña o atlántica)una rebelión contra el gobierno de Zelaya. El movimiento "revolucionario" era dirigido por el general Juan José Estrada Morales, gobernador liberal de la Costa Atlántica; por el tenedor de los libros (contador) de las minas "La Luz y Los Ángeles", Adolfo Díaz Recinos; por un militar representante de los terratenientes conservadores, Emiliano Chamorro Vargas y por el general, conservador, Luis Mena Vado. 

El cónsul estadounidense Thomas Moffat aparecía como el "Deus ex machina" del movimiento "contrarrevolucionario". El mismo Juan Estrada, ya no siendo más presidente de Nicaragua, confesaba así los hechos, en una entrevista al New York Times:

También colaboraron los dueños de las minas "La Luz and Los Angeles Mining Company", quienes se vieron obligados a entregarlas al gobierno de Nicaragua, por incumplimiento de las cláusulas del contrato de concesión. Por mera casualidad el secretario de Estado, Knox, tenía a su cargo la asesoría legal de la familia Fletcher, ex concesionaria de las minas mencionadas. 

Al enfrentar la rebelión. la superioridad del ejército leal al gobierno se sintió desde el comienzo del conflicto. 

Entonces el ministro estadounidense en Costa Rica, Willian L. Merry; se dirigió, en noviembre de 1909 al presidente de ese país, Cleto González, insinuándole que se uniera a Guatemala y El Salvador en una guerra contra Nicaragua. Estados Unidos se comprometían a proporcionar todo lo que necesitaran. Pero el plan estadounidense fracasó, dado que el interés de Costa Rica era el desarrollo económico y social de la región y una guerra solamente aumentaría los problemas sociales.

Para sofocar la rebelión, Zelaya se vio obligado a perseguir a los rebeldes en territorio costarricense. Nuevamente, el ministro estadounidense pidió que Costa Rica rompiese con Zelaya. Otra vez el gobierno costarricense se negó a luchar contra Nicaragua, dado que era claro el trasfondo político del accionar estadounidense.

Ante la negativa del gobierno de Costa Rica, Estados Unidos tomaron la opción de fortalecer el movimiento contrarrevolucionario. No había más remedio que enfrentarse abiertamente, al gobierno de Nicaragua, y no faltaron razones ni coyuntura inmediata para ello.

El gobierno del presidente William Howard Taft, que había sido elegido en las elecciones presidenciales de 1909 nombró secretario de Estado a Philander C. Knox un abogado que tenía como clientes a los dueños de las minas de oro nicaragüenses "La Luz" y "Los Ángeles Mining Company".

La recién incorporada Mosquitia estaba bajo la autoridad del general Juan José Estrada Morales que era liberal y apoyaba a Zelaya. Estrada comenzó a mantener relaciones con el cónsul estadounidense Thomas Noffat que, a su vez, mantenía excelentes relaciones con el general Emiliano Chamorro, este conservador. Estrada se aseguró del apoyo de los Estados Unidos para un hipotético levantamiento contra Zelaya. Este apoyo se confirmó a principios de septiembre de 1909. Al día siguiente el cónsul Thomas informaba del levantamiento contra el gobierno de Zelaya por parte de los generales Juan José Estrada y Emiliano Chamorro que, según él, se produciría el día 8 de septiembre pidiendo a su gobierno apoyo y reconocimiento para el futuro gobierno. En la información que Thomas mandaba a Washington se decía que el nuevo gobierno respetaría los intereses extranjeros y que seguramente el presidente Zelaya no iba a oponer resistencia armada.

El levantamiento se produjo el día 10 de septiembre de 1909 y el secretario de Estado Knox ordenó a los barcos de guerra estadounidenses estacionados frente a Bluefields, el "Paducah" y el "Dubuque", que intervinieran en apoyo de los insurrectos. Este episodio fue la primera intervención directa de los Estados Unidos en Nicaragua, intervención que duró hasta 1925.

En 1909 algunos mercenarios extranjeros fueron capturados y ejecutados por el gobierno de Zelaya, lo que sirvió para que Estados Unidos considerase la acción como una provocación para la guerra, y el derrocamiento de Zelaya por medio de la Nota Knox, enviada por Philander Chase Knox, Secretario de Estado de Estados Unidos.

Los estadounidenses Lee Roy Cannon y Leonardo Groce y el francés Edmundo Couture, fueron sometidos a un cuidadoso proceso. Llenadas todas las formalidades y plenamente confirmada su culpabilidad, incluso se les dejó un día antes despedirse de sus familiares, así dictan como prueba sus últimas se les pasó por las armas.

Estos dos mercenarios estadounidenses militaban en el ejército rebelde (financiado por EE. UU.). El 2 de diciembre el encargado de negocios nicaragüense en Washington recibía una nota del gobierno estadounidense, la conocida como "Nota Knox", en la cual le decían:

Zelaya dimite el 18 de diciembre como presidente justificando su decisión con estas palabras: 

1. La coyuntura inmediata fue que dos estadounidenses, Cannon y Groce, habían sido sorprendidos con bombas en su poder destinadas a volar los barcos del gobierno de Nicaragua que navegaban en el río San Juan. Tropas de Zelaya los tomaron in fraganti. Fueron sometidos a un cuidadoso proceso. Llenadas todas las formalidades y plenamente confirmada su culpabilidad, se les pasó por las armas. 

La culpabilidad de los dos estadounidenses era indudable y su muerte fue el pretexto final para la intervención abierta de Estados Unidos en Nicaragua. 

2. Pero no era solo eso: Zelaya se había negado a aceptar un empréstito que le ofrecieron los banqueros estadounidenses con el aval del gobierno de Estados Unidos. 

3. Al mismo tiempo, Zelaya contrató un empréstito con los banqueros ingleses de la Casa Ethelburg que tenía como objetivo la construcción de un ferrocarril que uniera el Atlántico al Pacífico del país, y para, como dice el mismo Zelaya, ""Liberar al comercio nacional de ser tributario del ferrocarril de Panamá... y realizar, además, la consolidación de nuestra deuda externa"".

Pero había algo más: Estados Unidos tenía el propósito de conseguir la concesión canalera por Nicaragua y no encontraban las facilidades con Zelaya, ya que este exigía que se garantizara la soberanía de Nicaragua y una cantidad de dinero correspondiente a la importancia de la obra. 

La actitud del gobierno de Nicaragua no encuadraba dentro de los planes políticos y financieros de la burguesía estadounidense. Zelaya era un estorbo para la diplomacia del dólar y era preciso eliminarlo.

A los países reacios a la aceptación de empréstitos de los banqueros estadounidenses, se les inducía a aceptarlos coaccionando su voluntad por medios muy variados y que resultaban tanto más eficaces cuanto más pobre y débil era el país al que oficialmente quería proteger los Estados Unidos con su apoyo pecuniario.

En el caso de Nicaragua, además de las razones económicas y financieras, la diplomacia del dólar respondía también, a razones geopolíticas ligadas a la posibilidad de la construcción de un canal Interoceánico.

Zelaya luchó contra el tremendo empuje de los estadounidenses que apoyaban y protegían a los partidarios de la contrarrevolución y de la intervención estadounidense.

La Asamblea Nacional (el Congreso designó Presidente al también liberal José Madriz Rodríguez que no fue del agrado de Estados Unidos (ya lo había expresado en la Nota Knox cuando hacía referencia a ""un candidato a la presidencia íntimamente ligado con el viejo régimen"").Madriz mandó tropas a Bluefields contra los insurrectos y toma el fuerte de El Bluff que cierra el puerto de la ciudad quedando está bajo su control. La infantería de Marina de Estados Unidos fue desembarcada en la ciudad en mayo de 1910 por lo que esta se mantuvo del lado rebelde al no poderla tomar las tropas gubernamentales. Las aduanas de Bluefields quedaban bajo control de Madriz pero la Armada de Estados Unidos estableció otra aduana bajo autoridad de Estrada, y el gobierno de EE.UU. manifestó, ante la protesta del gobierno de Nicaragua, que "cada fracción cobre derechos sólo en el territorio que se halle bajo su dominio".
José Madriz renuncia a la presidencia el 19 de agosto y poco después entran en Managua los generales Estrada Morales y Chamorro Vargas. La nueva Asamblea Nacional nombra Presidente a José Dolores Estrada Morales, quien cedió el poder a su hermano, el general sublevado Juan José, siendo nombrado vicepresidente Adolfo Díaz, que había sido empleado de las minas La Luz y Los Ángeles y era conocido por el secretario de Estado Knox. 

El día 1 de enero de 1911 los Estados Unidos reconocen al nuevo gobierno de Nicaragua. Estrada Morales firmó con Estados Unidos los Pactos Dawson (por Thomas C. Dawson, enviado del gobierno estadounidense), y convocó elecciones para formar una nueva Asamblea Constituyente, que elaboró una nueva Constitución. Entre otros cambios, el catolicismo se convertía en la religión oficial del estado, a instancias del conservador Emiliano Chamorro. Poco después Estrada Morales se ve obligado a renunciar y Díaz es nombrado Presidente de Nicaragua.

La influencia de Estados Unidos se incrementó durante el gobierno del presidente Adolfo Díaz, que puso en manos estadounidenses el control de las principales empresas estatales. 

El 29 de julio de 1912 estalló una nueva sublevación, a instancias del General Luis Mena Vado, conservador, y apoyada luego por el Doctor y General Benjamín Zeledón, liberal. Esta rebelión se conoce como la Revolución libero-conservadora de 1912 que es mal llamada "Guerra de Mena". 

Los rebeldes toman varias ciudades entre las que están Granada, bastión conservador, León y Masaya, bastiones liberales. 

El gobierno de Díaz pide la ayuda militar de los Estados Unidos y el gobierno estadounidense responde con el desembarco en puerto Corinto de Marines que sitian y atacan Granada tomada por las fuerzas del general Mena, quien la entrega sin oponer resistencia, hecho prisionero Mena es exiliado hacia Panamá. 

El mando supremo recae en el general Zeledón, quien se hace fuerte en los cerros "Coyotepe" y "La Barranca", cercanos a la asediada Masaya, los cuales mantiene hasta la batalla decisiva del 4 de octubre, cuando es abatido por soldados conservadores de "La Constabularia", leales a Díaz y conocidos como "caitudos". 

Para poner fin a esta corta pero sangrienta guerra civil nicaragüense, los Estados Unidos movilizaron hacia Nicaragua a 2.500 hombres y 8 buques de guerra. Después de la batalla de Coyotepe, solamente dejó a 400 soldados como parte de la llamada "Legación Americana".

Como consecuencia de esta última intervención, el país permanecería ocupado por Estados Unidos hasta 1933 (desde 1912 hasta el 3 de agosto de 1925, y luego desde 1926 hasta 1933, con un breve intervalo de nueve meses en medio). 

En 1914 se firmó el Tratado Bryan-Chamorro, mediante el cual se cedían a Estados Unidos todos los derechos para la construcción de un futuro canal interoceánico, a cambio de tres millones de dólares. A pesar de que el Canal de Panamá había sido construido ya en 1903, la zona continuaba siendo de interés estratégico. También por este tratado, se daba a Estados Unidos el derecho de establecer una base militar en el golfo de Fonseca durante un período de 99 años, y se le cedían en arriendo las Islas del Maíz (Corn Island), por idéntico lapso de tiempo.

Entre 1917 y 1926 Nicaragua estuvo dominada por el partido conservador. Los marines estadounidenses, presentes en el país desde 1912, se retiraron en agosto de 1925. Al año siguiente, sin embargo, se produjo un nuevo levantamiento liberal, que produjo una nueva guerra civil, la denominada Guerra Constitucionalista. 

Las negociaciones en el llamado Pacto del Espino Negro en Tipitapa entre el gobierno y los rebeldes, impulsadas por Estados Unidos, dieron lugar a un gobierno de coalición. Sin embargo, dado que el gobierno era incapaz de controlar los nuevos focos de insurrección, los marines desembarcaron de nuevo en diciembre de 1926.

En las elecciones de 1920 salió elegido Presidente Diego Manuel Chamorro que tomó posesión de su cargo ya en el año siguiente. Chamorro murió en 1923 y lo sucedió el que era su vicepresidente, Bartolomé Martínez que se marcó como objetivo el liquidar la deuda que el país tenía con unos banqueros estadounidenses. El objetivo fue cumplido el año siguiente de haber subido a la presidencia y ya libre de la carga económica, se convocaron elecciones para el mes de octubre de ese mismo año para las cuales se realizó una candidatura única entre conservadores y liberales. Como presidente iba Carlos Solórzano, conservador y para vicepresidente el liberal Juan Bautista Sacasa. 

Solórzano fue investido presidente en enero de 1925 y para agosto de aquel año ya habían salido todos los soldados estadounidenses del territorio nicaragüense. En octubre Emiliano Chamorro se alza en armas contra el gobierno y toma la Loma de Tiscapa. Para aplacar la rebelión y por consejo del gobierno de EE.UU. Solórzano nombra a Chamorro "jefe de la fuerza pública". 

Las tensiones entre ambos acaban con la dimisión del presidente que pasa los poderes presidenciales al senador Sebastián Uriza y este se los pasa a Chamorro y finalmente acaban en manos de Adolfo Díaz quedando Sacasa fuera. 

En mayo de 1926 el partidario de Sacasa, el general José María Moncada se alza en armas pidiendo el poder para Sacasa. La insurrección de los liberales estaba apoyada por el gobierno mexicano de Elías Calles. 

La respuesta de los Estados Unidos que apoyaban a los conservadores fue la de mandar de nuevo a la infantería de marina. El día de Nochebuena de 1926 desembarcaban las tropas estadounidenses en Puerto Cabezas. Para el día de Reyes de 1927 había en suelo nicaragüense más de 5.000 soldados y marinos estadounidenses apoyados por 16 buques de guerra. Adolfo Díaz justificó la intervención con estas palabras 

En febrero de 1927 ocurren los combates fratricidas más destructivos cuando los 500 hombres del llamado Ejército Liberal Constitucionalista de Occidente se enfrentan contra las tropas conservadoras leales a Díaz apoyadas por los marines en la batalla de Chinandega.

Augusto C. Sandino, que entonces contaba con 31 años de edad, acababa de volver después de pasar 5 años trabajando de mecánico en México, Honduras y Guatemala. Cuando se enteró de la insurrección liberal de Sacasa formó una fuerza armada que se sumó a las fuerzas liberales. Tras algunas derrotas se internó en las montañas de Nueva Segovia. Cuando se enteró que los mexicanos habían mandado armas se dirigió, bajando por el río Coco, a Puerto Cabezas para pedirle a Sacasa que le armara.
En Puerto Cabezas, la intervención de las tropas estadounidenses había logrado desarmar a los liberales. Los estadounidenses arrojaron las armas enviadas por lo mexicanos al mar. Cuando llegó Sandino se encontró que no había armas y que estas estaban en el fondo de la bahía. Con la ayuda de unos cuantos adeptos, entre los que se encontraban un número relevante de mujeres de la ciudad, logró recuperar 30 fusiles y 6.000 cartuchos. Después de hablar con Moncada en la ciudad de Prinzapolka se dirigió de nuevo a su base en Las Segovias.

Las fuerzas de Sandino fueron creciendo. Durante la primera mitad del año 1927 combatió a los conservadores a los que fue venciendo y tomando varias posiciones, según las indicaciones de Moncada. La última plaza tomada fue el cerro de "El Común" en Boaquillo donde permaneció hasta el Pacto del Espino Negro en Tipitapa el 4 de mayo, que según palabras de Sandino fue donde 

Por este pacto, en el que participaron el coronel estadounidense Henry L. Stimson (enviado especial del presidente Calvin Coolidge y delegado omnipotenciario del presidente de Nicaragua Adolfo Díaz), Eberhard (ministro de EE.UU. en Nicaragua), el contralmirante Julian Latimer, tres delegados de Sacasa y el general Moncada. Acordaron que Díaz seguiría de presidente hasta las elecciones de 1928 y que EE.UU. requisaría todas las armas de ambos bandos a la vez que supervisaba el proceso electoral.

Sandino se negó a aceptar el acuerdo. En contra de las indicaciones de Moncada, Sandino difundió un comunicado en el cual pedía al pueblo de Nicaragua que se sublevara contra los extranjeros. En los intentos de convencer a Sandino para que aceptara el pacto, Moncada llegó a mandar a su padre, amigo personal de él, para que le hablara y el comandante de las fuerzas de EE.UU. en Ocotal (Nueva Segovia) le hizo llegar una carta pidiéndole que depusiera las armas y las entregara bajo la amenaza de proscribirle y perseguirle. Sandino le respondió: 

No paso un día cuando el 15 de julio de 1927 las tropas de Sandino se toman la ciudad de Ocotal dando lugar Batalla de Ocotal. La ciudad fue defendida por los Marines estadounidenses y los Guardias Nacionales nicaragüenses quienes se atrincheraron en los cuarteles. Sandino se negó a incendiar la ciudad, tal como le pedían algunos de sus hombres para obligar a los marines y guardias nacionales a rendirse o aniquilarlos. Después de los "insurrectos" abandonaran Ocotal, cuando la aviación estadounidense bombardeaba y diezma la ciudad.

La persecución de Sandino se realizó con la destrucción aldeas campesinas y las matanzas de muchos campesinos por la sospecha del apoyo que podrían estar prestándole. Las tropas sandinistas sufrieron varias derrotas como la de San Fernando, en julio, o la de Las Flores poco después.

Con la llegada del otoño comenzó una campaña victoriosa tomando Telpaneca y saliendo victorioso en Las Cruces, Trincheras, Varillal y Plan Grande. Estableció su cuartel general en El Chipote, una de las alturas de Las Segovias.

Realizó diversas incursiones como el atacar y destruir la mina de La Luz, propiedad del exsecretario de Estado estadounidense Knox o la batalla de Bramadero. Las acciones de Sandino le fueron dando fama por todo el país y por los otros países de Hispanoamérica. Esa fama producía que muchos hombres llegaran dispuestos a integrarse en sus filas. A mediados de 1928 Henri Barbusse le llamaba General de Hombres Libres.

A finales del mes de noviembre de 1928 el contralmirante D.F. Sallers le invitaba a abandonar la lucha y obtener así "los consiguientes beneficios" la respuesta de Sandino fue; 

Las elecciones presidenciales de noviembre de 1928 fueron ganadas por el liberal Moncada. Moncada tomó posesión el 1 de enero de 1929. Moncada prosiguió colaborando con los estadounidenses en la persecución de Sandino. Para el mes de marzo de ese año ya se habían arrasado 70 pueblos, los bombardeos eran continuos e incluso llegaron a afectar a la vecina ciudad hondureña de Las Limas.

Sandino realizó un viaje a México para intentar conseguir apoyo. A su vuelta, el 7 de mayo de 1930 se encontró que los estadounidenses habían formado una "guardia nacional" para combatir a la guerrilla. Esa guardia se debía de pagar con fondos nicaragüenses. Debido a la endeble economía del país se cerraron las escuelas públicas para hacer frente a esos gastos. 

Para julio de 1931 los sandinistas contaban con 8 columnas de guerrilleros repartidas por todo el territorio nicaragüense. Al año era el propio Sandino el que hacía públicos los informes de las actividades de sus fuerzas. Ante las elecciones de 1932 Sandino hizo una campaña de abstención. Para esas elecciones el candidato del partido liberal era Sacasa (aunque la preferencia de la embajada estadounidense habría sido Anastasio Somoza pero este era demasiado joven e inexperto).

Sacasa ganó la presidencia y Sandino respondió nombrado al general Juan Gregorio Colindres presidente provisional del "Territorio Libre de Las Segovias" y tomó la población de San Francisco Carnicero, cerca de Managua, para apoderarse de los sellos oficiales.

Las victorias de Sandino estaban desprestigiando a los Estados Unidos y el coste de la guerra se hacía inaguantable en una economía que estaba en plena crisis, de tal forma que la población empezó a presionar a su gobierno para que abandonara Nicaragua. Una vez que Sacasa fue elegido las tropas estadounidenses empezaron a abandonar Nicaragua y cuando fue envestido presidente, el 1 de enero de 1933 ya no quedaban soldados estadounidenses en suelo nicaragüense.

Al no haber soldados extranjeros en Nicaragua y por otras presiones, Sandino llegó a un acuerdo de paz con Sacasa. La Guardia Nacional al mando de Anastasio Somoza (creada por los Estados Unidos y comandada por un hombre de su confianza) seguía con la represión en contra de los hombres de Sandino aun cuando este pedía al presidente que parara las acciones de la Guardia.

El 21 de febrero de 1934 Sandino en compañía de su padre, Gregorio Sandino, el escritor Sofonías Salvatierra y los generales Estrada y Umanzor acudían a una cena en la casa presidencial invitados por Sacasa. A la salida de dicho evento el coche en el que viajaban fue detenido justo a la salida de los jardines de la casa presidencial. El cabo de guardia que les detuvo era en realidad un mayor disfrazado, un tal Delgadillo, que les condujo a la cárcel del Hormiguero. Los detenidos pidieron que llamaran a Somoza, pero les respondieron que no podían localizarlo, por otro lado la hija de Sacasa le comunicó a su padre la detención, ya que la había visto, y Sacasa se puso en contacto con la embajada de Estados Unidos para intentar detener el asesinato.

Sandino, Estrada y Umanzor fueron llevados al monte llamado La Calavera en el campo de Larreynaga y allí, a la señal de Delgadillo, el batallón que custodiaba a los prisioneros abrió fuego matando a los tres generales. Eso ocurría a las 11 de la noche. Al oír los disparos, según testimonio de Salvatierra, Gregorio Sandino dijo, 

Un año después, Anastasio Somoza, que llegó a decir que recibió las órdenes del asesinato de Sandino del embajador estadounidense Arthur Bliss Lane, se haría con el poder del país.

Antes de salir de Nicaragua, los "marines" traspasaron el mando de los 4,000 soldados alistados en Guardia Nacional a Anastasio Somoza García, un sobrino político del presidente Sacasa que se había ganado la confianza del embajador y de los altos oficiales estadounidenses. Pronto convertiría esta fuerza militar en un formidable instrumento de poder personal. El 21 de febrero de 1934 el nuevo jefe director de la Guardia Nacional inició su ofensiva, haciendo asesinar a Sandino cuando este salía de una cena en la casa de gobierno, a la que había sido invitado por el propio mandatario. El día siguiente desató una matanza de más de trescientos campesinos sandinistas, incluyendo mujeres y niños, que se encontraban en una cooperativa agrícola en Wiwilí, al este de Las Segovias. Luego, reorganizó las fuerzas armadas, purgando a sus adversarios y colocando a sus allegados en posiciones clave en todo el país. Finalmente, se concentró en fortalecer su influencia en el Congreso y el Partido Liberal, utilizando para ello el presupuesto del ejército, que representaba más de la mitad de los ingresos fiscales del Estado. Logrado esto, pasó a desplegar una abierta campaña para llegar a la presidencia, pese a que la Constitución vigente le inhibía de ocupar ese cargo, dado su parentesco con Sacasa y su condición de militar activo.
Con apoyo estadounidense, Anastasio Somoza García se deshizo de sus rivales políticos, incluido Sandino, que fue asesinado por oficiales de la Guardia Nacional en febrero de 1934. En 1936, Somoza se convirtió en presidente de Nicaragua. Su familia se mantendría en el poder hasta 1979.
Anastasio Somoza fue presidente de 1937 a 1947, y de 1950 a 1956 (en el intervalo, no abandonó el poder, sino que siguió detentándolo mediante hombres de paja). La primera oposición al régimen de Somoza procedió de la clase media y de la clase alta, normalmente conservadora, que vieron con disgusto como el nuevo gobernante ponía el país en manos de su familia y amigos. A causa de las limitaciones de la libertad de expresión, los esfuerzos para resistir a Somoza no tuvieron ningún resultado. Muchos opositores abandonaron el país, exiliándose en Estados Unidos. Una excepción notable fue Pedro Chamorro, editor del diario "La Prensa", el más popular del país, cuya reputación internacional y continuo rechazo de la violencia le hicieron intocable para el régimen. 

La oposición liberal fue pronto eclipsada por la marxista, de carácter más radical. El 21 de septiembre de 1956, un joven poeta liberal, Rigoberto López Pérez, logró infiltrarse en una fiesta en la que se encontraba Somoza García, disparándole en el pecho y terminando con su vida (Somoza moriría a causa de la herida poco tiempo después).

En 1961 los jóvenes políticos Carlos Fonseca Amador, Tomás Borge Martínez y Silvio Mayorga inspirados en las ideas de Augusto Sandino fundan el Frente Sandinista de Liberación Nacional y emprenden la lucha insurreccional contra la dictadura de la familia Somoza.

Los diferentes gobiernos de los Somoza contaban con el respaldo del gobierno de los Estados Unidos. Por su lado, el FSLN, como la mayoría de las guerrillas con pensamiento de izquierda en la América Latina de esos años, tenía el respaldo ideológico y también armamentístico (y de entrenamiento militar) de la Cuba de Fidel Castro. Hacia los últimos años del régimen somocista (1977-1979) el FSLN contó con el decidido apoyo de gobiernos extranjeros tales como Cuba, la Venezuela de Carlos Andrés Pérez, la Panamá de Torrijos, e incluso de Costa Rica (recibiendo nicaragüenses exiliados, y prestando su territorio para las tropas guerrilleras del FSLN). 
El FSLN emprendió una lucha de guerrillas tanto urbanas como rurales con la intención de derrocar al gobierno nicaragüense desde el inicio de sus operaciones guerrilleras a inicios de la década de 1969. Estas operaciones son conocidas como las Jornadas de Pancasán y las guerrillas de Raití y Bocay en las cuales cayeron algunos miembros de la Organización tales como Filemón Rivera, Oscar Danilo Rosales, Rigoberto Cruz mejor conocido como Pablo Úbeda y muchos más, el fracaso de estos primeros intentos guerrilleros se debió a la falta de conocimiento de la zona de operaciones y al desinterés de las poblaciones en donde operaban pues eran lugares muy despoblados. Pasada la experiencia de Pancasán se pasa a un período conocido como la de Acumulación de Fuerzas en Silencio aunque inclusive en estos años se dieron choques con la Guardia Nacional. Dada la circunstancias el FSLN se divide en tres tendencias, cada una de ellas con una visión diferente de llevar a cabo el derrocamiento de la dictadura somocista. 

La Tendencia Guerra Popular Prolongada, propugnaba por la lucha en la montaña sobre la base de la experiencia de la revolución cubana y sobre todo de Ernesto Che Guevara, la Tendencia Proletaria, afirmaba que el derrocamiento se daría cuando el proletariado es decir los obreros y campesinos se uniera para derribar la tiranía y por último la Tendencia Insurreccional o Tercerista que llamaba a armar al pueblo y resultó a la postre la forma a través de la cual caería Anastasio Somaza Debayle. Precisamente buscando la unidad de las tres tendencias pierde la vida Carlos Fonseca el 8 de noviembre de 1976 en Boca de Piedra, Zinica. Aunque para disminuir la represión desatada a raíz de algunos incidentes en las montañas se da el operativo conocido como Diciembre Victorioso cuando un grupo de guerrilleros bajo el mando de Eduardo Contreras se toma la casa de un ministro somocista el 27 de diciembre de 1974 fecha a partir de la cual el mundo conoció la existencia del FSLN.

En 1978, consigue un golpe magistral contra la dictadura, al llevarse a cabo un operativo, denominado ""Operación Chanchera"", efectuado por un comando guerrillero, que conllevó a la toma del edificio Palacio Nacional sede del Congreso de la República, y un número considerable de sus miembros, poniendo en evidencia las debilidades de logísticas de la Guardia Nacional.

La ofensiva guerrillera lanzada desde el norte, con el apoyo de los campesinos, de las clases obreras e industriales cansadas de la política somocista y apoyada por la acción política y la presión internacional logra que el 19 de julio de 1979 entrar triunfante en la capital, Managua, mientras que el dictador Anastasio Somoza Debayle y su familia abandona el país.
Llegando victoriosos a la plaza de la revolución un 19 de julio de 1979. Roberto Carlos Alfaro anuncia la llegada de los sandinistas y grita el famoso dicho hasta la victoria siempre.

La entrada en Managua de las tropas del FSLN (Frente Sandinista de Liberación Nacional) pone fin al poder dictatorial de los Somoza que durante 43 años se habían mantenido en el poder. El 19 de julio de 1979 da comienzo un cambio radical en Nicaragua, cambio que tendría consecuencias continentales y llevaría a la intervención de potencias extranjeras tales como la Unión Soviética y, nuevamente, de Estados Unidos. Tras la intervención de EE. UU. y el bloque soviético, comenzará un proceso de inestabilidad política y social que llevará a una Guerra Civil promovida por las dos potencias que manejaban el hilo del contexto de la guerra fría, tanto en el período de la presidencia de Ronald Reagan y continuada por Bush padre. Los acuerdos de paz no se firmarán hasta finales la década de los ocheta. Paralelamente comienza la llamada Revolución sandinista que se desarrollaría hasta inicios de 1990.

Acorde a una concepción ideológica socialista e incluso comunista, con fuerte presencia marxista, y con una influencia muy grande de la teología de la liberación, trataron de introducir reformas en los aspectos socioeconómicos y políticos del estado nicaragüense, tratando además los problemas relativos a la sanidad y a la educación que el país sufría. La reforma agraria fue una de las principales medidas que el nuevo gobierno puso en marcha. Sobre la base de las tierras confiscadas a la familia Somoza y los otro miembros principales de la oligarquía nicaragüense. La universalización de la sanidad con el desarrollo de un sistema de salud universal y la cruzada ce alfabetización que redujo el analfabetismo endémico de la población nicaragüense del 50% a algo menos del 13% en un corto período; fueron las acciones inmediatas en las que se empeñaron los nuevos gobernantes. El gobierno revolucionario encontró apoyo en Cuba, la URSS y otros países, en su mayoría europeos.

El cambio de gobierno en los Estados Unidos, con la pérdida de las elecciones de los demócratas y la entrada de Ronald Reagan, del partido republicano, hacen que las voces de los disidentes y contrarios a los sandinistas, sean estimadas en la Casa Blanca; que comienza a organizar, con restos de la Guardia Nacional una serie de grupos armados, denominados contras (de contrarrevolucionarios y en contraposición de la palabra "compa", de compañero, que era como se denominaban los sandinistas) que comienzan una lucha armada contra el nuevo gobierno de izquierdas. EE. UU. actuaría direcctamente en acciones de guerra y sería llevado ante el Tribunal Internacional de la Haya por el gobierno nicaragüense siendo condenado por el mismo (ver Nicaragua contra Estados Unidos) llegando, el gobierno estadounidense, a desobedecer el mandato de su Congreso que impedía la ayuda directa a las tropas irregulares de la Contra, produciéndose el escándalo conocido como Irangate.

Tras varios años de guerra civil y agresión que impidió el desarrollo de Nicaragua al tener que dedicar una importante parte del presupuesto del país en la defensa y tras varias negociaciones de ámbito internacional, en las elecciones de febrero de 1990 Daniel Ortega, líder y presidenciable del FSLN pierde las elecciones a la presidencia de la República.

La revolución sandinista intentó transformar el modelo histórico de relaciones entre el Estado y la sociedad nicaragüense mediante el desarrollo acelerado de la capacidad regulatoria e interventora del Estado. Esta estrategia no fue acompañada de un esfuerzo efectivo por desarrollar una capacidad social para controlar y democratizar la acción estatal. La ausencia de mecanismos de participación efectiva terminó afectando la legitimidad del régimen. De la creciente ilegitimidad del régimen sandinista se alimentó el movimiento contrarrevolucionario que se constituyó en instrumento de la política exterior de Estados Unidos de América. A partir de la instalación del gobierno del presidente Ronald Reagan en Washington, esta política se orientó hacia el desmantelamiento del proyecto revolucionario sandinista.

Las contradicciones generadas por el modelo políticoinstitucional estatista del FSLN, la guerra contrarrevolucionaria y las presiones económicas impuestas por Estados Unidos al gobierno sandinista coincidieron con el momento en el que la Unión Soviética iniciaba un proceso de transformaciones internas que eventualmente terminarían en su desolución.

En 1986 el presidente soviético Mijail Gorbachvo introdujo su programa de reestructuración económica conocido como "perestroika". Este programa tenía como objetivo la liberalización de la economía soviética para lograr su desarrollo e integración efectiva dentro de la economía mundial. La perestroika fue acompañada de un proceso de democratización del sistema político soviético ("glassnost"), y de importantes esfuerzos por mejorar las relaciones entre Washington y Moscú.

Las políticas reformistas introducidas por Gorbachov y la disensión entre la Unión Soviética y Estados Unidos debilitaron la posición del FSLN, que en poco tiempo vio reducido el apoyo político, económico y militar que el proyecto revolucionario recibía de los países del bloque soviético. Más aún, las reformas emprendidas por Gorbachov iniciaron el desmoronamiento del modelo normativo económico y político de organización del mundo socialista que el FSLN utilizaba como eje de referencia para su proyecto revolucionario.

En tan desfavorable contexto internacional, la revolución sandinista tuvo que enfrentar el virtual colapso de la economía nacional -producto del desgaste ocasionado por la guerra, la complejidad del experimento revolucionario, los errores cometidos por los responsables de la política económica del gobierno, y las presiones de Estados Unidos-, así como el agotamiento de la población nicaragüense. Para enfrentar la crisis económica, el gobierno emprendió en 1988 un programa de estabilización y ajuste macroeconómico que tuvo un impacto directo y negativo en las condiciones de vida de los sectores populares.

Las reformas de 1988 restablecieron el centralismo del mercado como eje ordenador de la economía nicaragüense. Con estas reformas, además, del Estado nicaragüense se acompasaba a las presiones e influencias de los organismos financieros internacionales, distanciándose de la sociedad y sus demandas. El proceso de adecuación del Estado nicaragüense a las fuerzas que operaban en su contexto global, y la ampliación de la brecha entre el Estado y la sociedad nicaragüense generada por este proceso iban a mantenerse y agudizarse durante la década de 1990.

En Costa del Sol, en El Salvador, el 14 de febrero de 1989 -víspera de la retirada de las tropas soviéticas de Afganistán-, los presidentes centroamericanos firmaron el "Acuerdo de Tesoro", que establecía compromisos concretos para la pacificación de la región, y que incluía la celebración de elecciones en Nicaragua en febrero de 1990.

Los acuerdos de Costa del Sol fueron ratificados en la Cumbre de Tela en agosto de 1989. A partir de ese momento los sandinistas empezaron a organizarse para la contienda electoral, en tanto que Estados Unidos iniciaba una serie de esfuerzos para asegurar la victoria de la Unión Nacional Opositora (UNO), una coalición de partidos organizados en torno al objetivo común de derrotar al sandinismo.

En febrero de 1990 se celebraron las elecciones presidenciales que otorgaron la victoria a Violeta Barrios de Chamorro, candidata de la UNO. El triunfo de la UNO se sustentó en un consenso social real, pero precario. La desesperación causada por la guerra, el fracaso del modelo sandinista en lo político y en lo económico, y la intervención de Estados Unidos, se combinaron para crear un acuerdo nacional mayoritario cuyo eje no era la índole del régimen que se quería institucionalizar, sino la oposición al régimen que se quería eliminar.

El lapso de dos meses entre la derrota del FSLN en las urnas y el traspaso del gobierno a la presidenta electa Violeta Barrios de Chamorro se caracterizó por un clima de tensa incertidumbre. Los sectores de "línea dura" de la Unión Nacional Opositora (UNO), aglutinados en torno al vicepresidente Virgilio Godoy, Arnoldo Alemán -alcalde de Managua- y los 20.000 combatientes de la Resistencia Nicaragüense (Contra), exigían el desmantelamiento de las fuerzas armadas sandinistas, la inmediata devolución de todas la propiedades confiscadas y la privatización de las empresas estatales. Mientras tanto, el FSLN proclamaba su intención de "gobernar desde abajo" con el respaldo de las organizaciones de masa, y demandaba el respeto a la integridad del Ejército Popular Sandinista, que a la fecha contaba con 96.660 soldados.

El convulso ambiente socio-político demandaba un acuerdo institucional a fin de desactivar la amenaza de una nueva guerra civil, o una intervención militar estadounidense. El 27 de febrero de 1990 se conformaron dos equipos negociadores presididos por el general Humberto Ortega y el ingeniero Antonio Lacayo, yerno de Violeta Barrios de Chamorro. La participación de Joao Soares, Secretario General de la OEA; Elliot Richardson, representante del Secretario General de las Naciones Unidas, y Jimmy Carter, expresidente de los Estados Unidos, en calidad de observadores internacionales, facilitó las conversaciones. Estas culminaron el 27 de marzo con la firma del Protocolo para la Transferencia del Mando Presidencial, conocido como "Protocolo de Transición", cuyos acuerdos más importantes contemplaban: el reconocimiento de las elecciones como base para la construcción de la democracia y la paz; seguridad jurídica a los beneficiarios de donaciones estatales de propiedades urbanas y rurales, asignadas antes del 25 de febrero de 1990; respeto a los rangos, escalafones y mandos del ejército, incluyendo la permanencia de Humberto Ortega como general en jefe del EPS (Ejército Popular Sandinista); subordinación de las fuerzas armadas y cuerpos de seguridad al poder ejecutivo; reducción significativa del ejército, y desmovilización de la Resistencia Nicaragüense antes del 25 de abril, para garantizar el traspaso de gobierno en un clima de paz.

El Protocolo de Transición reflejaba una posición pragmática que reconocía la fuerza organizativa del FSLN, y daba prioridad a la estabilidad política. No obstante, fue rechazado por la "línea dura" de la UNO, que demandaba la destitución inmediata del Comandante Ortega. Así también, exigía la penalización de la "piñata" -nombre que se dio a la distribución de millones de dólares en propiedades estatales entre dirigentes y cuadros del FSLN, a raíz de su derrota electoral-. Como resultado, ocho de los catorce partidos de la UNO se negaron a asistir al acto de toma de posesión de la presidenta Chamorro y, en adelante, obstaculizarían su administración desde los gobiernos municipales bajo su control.

Por su parte, la Resistencia Nicaragüense condicionó su desmovilización a la firma de nuevos acuerdos, según los cuales el gobierno procedió a delimitar veintidós "polos de desarrollo" -que abarcaban, en su conjunto, un área de 20.000 km cuadrados- donde los excombatientes de la Contra recibieron tierras y recursos productivos para asentarse con sus familias, bajo la protección de fuerzas especiales de Naciones Unidas. Después de la desmovilización, concluida el 27 de junio de 1990, ex miembros de la Resistencia Nicaragüense fueron incorporados a la Policía Nacional, y encargados de garantizar el orden dentro de estas zonas de seguridad. Además, se reconoció a la Contra como una organización política legal, y algunos de sus principales dirigentes recibieron cargos en la burocracia estatal. Por otra parte, el gobierno llegó a un acuerdo con el ejército para dar inicio a un rápido proceso de reducción de efectivos, ofreciendo a los exmilitares diversos beneficios, como indemnizaciones por años de servicio y asignación de viviendas o tierras. Mediante este proceso de licenciamiento gradual, se desmovilizó a 72.000 soldados en el lapso de tres años.

El desarme de la "Contra" y la drástica reducción del ejército significó tan solo el inicio del proceso de pacificación de Nicaragua. Esta meta exigía la reinserción estable de los excombatientes a la vida civil, pero su consecución enfrentó graves problemas. La entrega de la ayuda material prometida dependía de la cooperación externa, y los trámites burocráticos frenaron el ritmo de su flujo hacia los "polos de desarrollo". Impacientes, muchos excombatientes de la RN (Resistencia Nicaragüense) decidieron retornar a sus lugares de origen, o se dispersaron por el área rural, invadiendo empresas estatales, fincas privadas y cooperativas sandinistas. Hacia fines de 1990, alrededor de 4,000 hombres, comandados por un "Estado Mayor" integrado por cuadros intermedios de la RN, se levantaron en armas en el norte y centro del país, exigiendo la entrega inmediata de títulos agrarios y recursos productivos. Ante el surgimiento de los "recontras", unos 3,000 campesinos sandinistas y ex miembros del EPS (Ejército Popular Sandinista) se rearmaron; organizados en el Movimiento de Auto-defensa Nacional, los llamados "recompas" presentaron sus propias reivindicaciones al gobierno. A estos dos grupos se sumó un tercero: El Frente Norte "Prudencio Serrano", conocido tanto por excombatientes de la RN como del EPS.

Según datos oficiales del ejército, a mediados de 1992 el número de rearmados ascendía a 21,905 hombres, que disponían de fusiles automáticos, ametralladoras, morteros, minas e incluso misiles antiaéreos y antiblindados. A esa fecha, habían realizado alrededor de 1,600 acciones militares, que incluían asaltos a bancos y vehículos de transporte público, secuestros de funcionarios gubernamentales o productores privados, tomas de tierras y poblados, en unos veintiséis municipios rurales del interior del país. Entre 1990 y 1994, la administración de Barrios de Chamorro suscribió 48 acuerdos para lograr la desmovilización de casi un centenar de bandas armadas, a cambio de entrega de 97,863,878 dólares en ayuda material para facilitar la reinserción de los excombatientes en la vida productiva. Además, redistribuyó un total de 701,500 manzanas de tierra a unos 24,542 beneficiarios, dando prioridad a los desmovilizados y campesinos repatriados. En adelante, el gobierno suspendió toda negociación, y los nuevos rearmados pasaron a ser perseguidos por el ejército como "bandas delictivas", al margen de su filiación política.

La transición política nicaragüense coincidió con el colapso de la Unión Soviética y de los modelos políticoeconómicos socialistas del Europa Oriental, es decir, coincidió con el final de la Guerra Fría. Estos dramáticos eventos crearon condiciones apropiadas para el surgimiento de la democracia liberal como contraparte política del desarrollo de economías de mercado en todo el mundo.

Este contexto de fuerzas y tendencias mundiales, más que el desarrollo endógeno de la sociedad nicaragüense, explica la transición política de 1990. Así pues, en lo fundamental, el experimento democrático nicaragüense fue producto de condicionamientos externos, y no de transformaciones reales en la naturaleza de las relaciones entre el Estado y la sociedad nicaragüense. Sobre un Estado históricamente dependiente del exterior y distanciado de la sociedad civil se impuso en 1990 un sistema político de democracia electoral que ampliaría los derechos políticos de la ciudadanía dentro de un rango sumamente restringido de opciones sociales y económicas.

En lo nacional, la transición de 1990 estuvo marcada por la fragmentación política, por la virtual paralización de la economía nacional y por el debilitamiento de los niveles de solidaridad social entre los nicaragüenses. La fragmentación política tuvo su expresión en las tensiones existentes entre la Unión Nacional Opositora (UNO) y el derrotado FSLN, así como en la desarticulación de la red de organizaciones populares que habían surgido durante la década de la revolución.

En lo económico, la transición de 1990 tuvo lugar en un momento en que la estructura productiva del país se encontraba prácticamente en ruinas. Un informe del gobierno de Violeta Barrios de Chamorro señalaba que, para 1989, el Producto Interno Bruto del país había caído al 42% de su nivel en 1977; el valor de las exportaciones se había reducido en un 53%, y los salarios reales a un 24%. Más aún, en 1989 la deuda externa per cápita era la más alta de América Latina: 3,000 dólares estadounidenses (treinta y tres veces el valor de las exportaciones del país). Además de la polarización política y del colapso económico del país, Nicaragua sufría en 1990 el desgarramiento social producido por la guerra civil.

El gobierno Barrios de Chamorro señalaba que la guerra y la militarización había sustraído a la juventud de las actividades del país, interrumpiendo así el proceso intergeneracional de transferencia de actitudes y habilidades, lo que dejó a los jóvenes sin motivación y sin el entrenamiento necesarios para trabajar.

Con una sociedad débil y fragmentada, y un Estado subordinado, en lo político y en lo económico, a la política exterior estadounidense, a la cooperación externa y a las pautas de los organismos financieros internacionales, se organizó un sistema político de democracia electoral que creó condiciones para la participación del pueblo nicaragüense en la elección de sus gobernantes, pero no necesariamente para el desarrollo de una capacidad social que pudiese condicionar la acción del Estado. En este sentido, la transición política en Nicaragua produjo una democracia externamente restringida, por cuanto los procesos electorales que constituían su característica principal operarían dentro de un marco normativo que limitaba el rango de opciones sociales y económicas que el nuevo sistema político ofrecía a la población.

Dentro de los espacios de participación creados por la transición de 1990, surgieron en Nicaragua un conjunto de organizaciones cívicas no gubernamentales de diversas orientaciones políticas, y de diferentes especialidades profesionales y temáticas. Estas organizaciones recibieron un apoyo importante de parte de la cooperación internacional interesada en promover la conformación de una "sociedad civil" en Nicaragua.

La sociedad civil nicaragüense, sin embargo, no logró constituirse en un mecanismo efectivo para la agregación de demandas sociales, debido a su fragmentación, a su bajo nivel de representatividad popular efectiva, y a su dependencia respecto de la cooperación internacional. Quizá el sector más exitoso dentro del proceso de formación de la sociedad civil nicaragüense durante el periodo 1990-1996 haya sido el de las organizaciones feministas, que en 1995 lograron trascender las divisiones políticoideológicas que fragmentaban a la sociedad nicaragüense y consolidaron la Coalición Nacional de Mujeres.

La debilidad de los partidos políticos y de las organizaciones de la sociedad civil crearon las condiciones para que la Iglesia católica nicaragüense recuperara el terreno político que había perdido durante la década de 1980. El nuevo poder de la Iglesia católica se manifestó en la consolidación de su líder, el Cardenal Miguel Obando y Bravo, como la personalidad pública que gozó de mayor legitimidad entre el pueblo nicaragüense durante el periodo 1990-1996.

El poder de la Iglesia Católica se manifestó también en su capacidad para condicionar las políticas del Estado, especialmente en el área de la educación. La agenda del Ministerio de Educación durante el gobierno de Barrios de Chamorro estuvo articulada en torno a dos ejes: a) un rechazo total al sandinismo como movimiento político y como experiencia histórica; b) la voluntad de introducir en los programas de educación un fuerte componente religioso, fundamentalmente determinado por los intereses de la Iglesia católica nicaragüense.

Al finalizar el régimen de Barrios de Chamorro la dependencia externa del Estado nicaragüense se expresaba en su total acatamiento del marco normativo neoliberal para pautar su política económica, así como en los altos niveles de participación de la cooperación externa en el financiamiento de los gastos estatales. Durante el periodo 1990-1996 el monto promedio anual otorgado por la cooperación externa al gobierno de Nicaragua fue de 538.4 millones de dólares estadounidenses. Este monto equivalía al 30.2% del Producto Interno Bruto promedio anual del país durante este mismo período. Mientras tanto, la pobreza afectaba a un 56.4% de la población del país, según un estudio elaborado por la Fundación Internacional para el Desarrollo Económico Global.

Por otra parte, la sociedad civil había ganado visibilidad durante el régimen de Barrios de Chamorro, aunque era mínima su capacidad para incidir en la acción del Estado al concluir este período. Así pues, la democracia nicaragüense era en 1996 una democracia política diseñada para institucionalizar la práctica del voto popular y las libertades propias del sistema democrático electoral, dentro de los límites normativos que imponía el sistema económico internacional y sus instituciones.

En resumen, eran enormes los retos que enfrentaba el país en 1996 para ampliar y profundizar el modelo de gobernabilidad democrática adoptado en 1990: un explosivo nivel de pobreza, una economía desarticulada y sin un claro sentido de orientación estratégica, una débil estructura de derechos ciudadanos, un Estado dependiente de la cooperación externa y de los organismos financieros internacionales, y las presiones de la globalización. Al mismo tiempo, el gobierno de Violeta Barrios de Chamorro terminaba su mandato habiendo logrado un alto nivel de pacificación en la zonas del país afectadas por los conflictos armados que surgieron después de concluida la guerra civil. El gobierno Barrios de Chamorro, además, dejaba como legado político una relación cívico militar estable, y un respeto a la libertad de prensa sin precedentes en la historia del país.

El gobierno de la presidenta Barrios de Chamorro se propuso impulsar un cambio estructural en el ámbito económico, a fin de sustituir la economía mixta y planificada del régimen sandinista por una economía de libre mercado. La tarea no era fácil, pues la capacidad productiva del país se encontraba gravemente deteriorada. En 1990 el Producto Interno Bruto per cápita era de US$ 400.00 (cuatrocientos dólares estadounidenses), aproximadamente la mitad del existente en 1980. Las exportaciones cayeron de US$ 680 millones en 1978 a un promedio de US$ 284 millones durante el quinquenio 1985-1990. Mientras tanto, la deuda externa pasó de US$ 1,500 millones en 1980 a US$ 10,000 millones en 1990 -cifra cinco veces mayor que el valor del Producto Interno Bruto del país-. La hiperinflación era del orden del 13,500 por ciento. La crisis económica colocaba al nuevo gobierno en una situación de alta dependencia con respecto a los organismos financieros internacionales. Estos condicionaron el flujo de recursos externos para reactivar la economía a la adopción de un severo programa de estabilización y ajuste estructural, que debía aplicarse bajo la supervisión del Fondo Monetario Internacional (FMI).

En abril de 1990 el gobierno anunció un plan que prometía reducir la inflación a cero en cien días. Este contemplaba la acuñación de una nueva moneda -el córdoba oro- cuyo valor se fijó a la par del dólar estadounidense, así como un conjunto de medidas neoliberales: reducción masiva del sector público y de los gastos sociales, eliminación del subsidio al precio de productos de consumo básico, restricción del crédito, y aumento en las tarifas de los servicios públicos e impuestos directos. Además, se anunció el inicio de la privatización de las empresas estatales, con el doble objetivo de devolver aquellas propiedades injustamente confiscadas, y vender las restantes para sanear las finanzas públicas. La respuesta del FSLN fue una violenta huelga general que paralizó Managua, obligando al gobierno a buscar un pacto económico y social, que se concretó con la firma de los llamados acuerdos de Concertación I y II, entre octubre de 1990 y agosto de 1991. El logro más importante del FSLN fue el compromiso del gobierno de transferir el 25 por ciento de las empresas estatales a los trabajadores, representados por sus sindicatos.

La relativa "paz social" obtenida mediante estos pactos socioeconómicos permitió a la administración Barrios de Chamorro acceder a un considerable flujo de recursos externos. Entre 1990 y 1996 la cooperación internacional representó un promedio del 30 por ciento del PIB anual; aunque la amortización de la deuda externa consumía el 60% de la ayuda económica neta recibida cada año por el país. No obstante, cabe anotar que al satisfacer a sus acreedores, el gobierno pudo negociar la condonación de 6,092 millones de dólares de la deuda contraída durante la década de 1980. Hacia 1993 la administración Barrios de Chamorro había logrado la estabilidad macroeconómica del país. Después de una década de estancamiento y recesión, la economía mostró un crecimiento del 3.2 por ciento en 1994 y del 4 por ciento en 1995.

Estas metas se alcanzaron a un grave costo social para las mayorías empobrecidas. Las políticas de ajuste estructural conllevaron una reducción del gasto público del 14% en 1996, con relación a 1990. Tal recorte mermó la capacidad del gobierno de hacer frente a la pobreza extrema, un fenómeno estructural que se había venido extendiendo de manera creciente desde la década de 1970. En efecto, los altos niveles de pobreza legados por el régimen somocista se agravaron como resultado de la guerra las políticas de estabilización iniciadas por el FSLN en 1988. La aplicación aún más estricta de tales políticas bajo la administración Barrios de Chamorro, amplió la brecha de la pobreza y agudizó la polarización social. Entre 1991 y 1996 el desempleo abierto creció a un promedio anual del 6.4%. El presupuesto para educación y salud fue recortado, en tanto que los programas integrales de bienestar social fueron sustituidos por proyectos coyunturales, con escaso financiamiento. De acuerdo con un estudio realizado por Naciones Unidas en 1994, el 75% de las familias nicaragüenses vivía por debajo del nivel de pobreza, y el 44% se encontraba en una situación de extrema pobreza.

Uno de los aspectos más controversiales de la transición a una economía de mercado impulsada por la administración Barrios de Chamorro, se refiere al destino de las empresas industriales y agropecuarias que aún quedaron en manos del Estado después de la "piñata" sandinista. El nuevo gobierno creó la Corporación Nacional del Sector Público (Cornap) para llevar a cabo su privatización. Este proceso se desarrolló mediante decretos ejecutivos, sin una adecuada supervisión por parte de la Contraloría General de la República.

En el año 1996 la Cornap informó haber vendido 495 empresas por un valor de 26 millones de dólares, e incurrido en pérdidas por el orden de los 59.8 millones de dólares, debido a los altos costos de operación estas transacciones. A pesar de ello, un estudio del economista Mario De Franco divulgado a través de la CEPAL demostró que las transferencias realmente efectuadas por la Cornap a empresarios privados fluctuaron entre los 155 y los 833 millones de dólares; es decir entre 6 y 32 veces el valor de las ventas brutas reportado. El informe final de la Cornap no incluyó información sobre la identidad de los adquirientes de las propiedades estatales, lo que alimentó las dudas en cuanto a la transparencia de los procedimientos utilizados.

El proceso de privatización conllevó otros problemas jurídicos, relacionados con la inscripción de las propiedades en los registros públicos, sobre todo en el sector rural. En vista de la presión por la tierra, las UPE, o empresas agropecuarias estatales, fueron redistribuidas. Hacia finales de 1993 el 80 por ciento de estas habían sido privatizadas: un 35% devueltas a sus antiguos dueños; un 31% divididas entre ex trabajadores de las UPE, y el 34% restante asignadas a desmovilizados de la Resistencia y el Ejército. Este proceso generó muchos conflictos en torno a los derechos de propiedad. El problema era de gran magnitud, pues el 40% de las tierras se hallaba en litigio, enfrentandoa 122,000 familias de todos los estratos sociales y políticos: campesinos contra campesinos, campesinos contra hacendados, élite tradicional versus la nueva élite sandinista, e incluso dirigentes del FSLN entre sí. Las disputas legales con frecuencia devinieron en confrontaciones violentas, sobre todo a raíz del retorno al país de miles de exiliados, que reivindicaban sus derechos de posesión sobre tierras confiscadas en la década anterior.

Por otro lado, la desaparición de las UPE (empresas agropecuarias estatales) implicó la reducción de los empleos permanentes en el campo en un 72%. Este proceso fue acompañado por una drástica caída del financiamiento (crédito) para los pequeños y medianos productores agropecuarios. Las tasas de interés cobradas por el Banco Nacional de Desarrollo (Banades) pasaron a ser las más altas en Centroamérica. Entre 1990 y 1992 el número de clientes rurales que recibieron préstamos disminuyó en un 80%, y la porción del crédito concentrado en los grandes productores aumentó en un 40%. Las políticas de apoyo a la producción en gran escala estimularon el aumento d las exportaciones, cuyo valor pasó de 284 millones de dólares en 1990 a 350 millones en 1994, y a 490 millones en 1995. No obstante, en ese último año el Banades entró en crisis, debido a la incapacidad de recobrar la cartera de préstamos en mora, otorgados sin garantías adecuadas a personas allegadas a los círculos de poder. La mitad de la suma adeudada al Banades había ido a parar a manos de 22 grandes deudores, entre los que figuraban altos funcionarios de esa institución estatal.

Mientras tanto, estrangulados por la falta de recursos productivos, decenas de miles de beneficiarios de la reforma agraria se vieron obligados a vender sus tierras y emigrar a los países vecinos en busca de empleos. El descontento provocado por las políticas económicas y sociales de la administración de la presidente Barrios de Chamorro se reflejó en los resultados de las elecciones presidenciales, celebradas en octubre del año 1996. El partido Proyecto Nacional (Pronal), fundado por Antonio Lacayo, ministro de la Presidencia y yerno de la presidenta saliente, recibió apenas el 9,323 votos. En contraste, el candidato de la Alianza Liberal, Arnoldo Alemán, obtuvo 904,908 votos gracias a promesas de una pronta mejoría económica, pero sobre todo a su habilidad para capitalizar el miedo de un 51% de los ciudadanos nicaragüenses a un retorno del comandante Daniel Ortega al poder.

Veintitrés partidos y alianzas políticas participaron en las elecciones del 20 de octubre de 1996. De estas 23 organizaciones, la Alianza Liberal -liderada por su candidato Arnoldo Alemán- y el FSLN -liderado por su candidato Daniel Ortega- constituían las principales fuerzas políticas del país.

Durante la campaña electoral, la Alianza Liberal logró capitalizar a su favor el recuerdo de la guerra y de la crisis económica sufridas en el país durante la década de la revolución, para crear un ambiente electoral que indujera a la población a escoger entre el "pasado sandinista" o la continuación de la apertura democrática. El FSLN, por su parte, intentó presentar una imagen democrática y renovada. Por otra parte, los múltiples esfuerzos que emprendieras diversas agrupaciones políticas por organizar un movimiento de "centro" se articuló simplemente como un proyecto político que se ubicada ideológicamente entre el FSLN y la Alianza Liberal.

La estrategia política de la Alianza Liberal recibió un impulso decisivo durante la última fase de la campaña electoral, cuando la Iglesia católica nicaragüense hizo explícito su apoyo a la candidatura de Arnoldo Alemán. El 17 de octubre de 1996, durante el periodo de "silencio electoral" establecido por la ley antes de las elecciones, el Cardenal Miguel Obando y Bravo ofició una misa en la catedral metropolitana de Managua ante la presencia del candidato presidencial liberal y del candidato liberal a la alcaldía de Managua. En su homilía, el cardenal Obando y Bravo recurrió a una parábola para prevenir al pueblo contra los engaños de las "víboras". Era una clara alusión al candidato sandinista Daniel Ortega, a quien Arnoldo Alemán había llamado "culebra" durante la campaña. Para reforzar el impacto y la efectividad del mensaje político del Cardenal Miguel Obando y Bravo, los diarios "La Prensa" y "La Tribuna" publicaron el día de las votaciones una fotografía a color en la que aparecía el Cardenal Miguel Obando y Bravo bendiciendo a Arnoldo Alemán y a su compañero de fórmula (el ingeniero Enrique Bolaños Geyer).

Los resultados electorales pusieron en evidencia el virtual monopolio que habían logrado imponer el FSLN y la Alianza Liberal sobre el electorado nicaragüense. La Alianza Liberal recibió el 51.3% de los votos, en tanto que el FSLN obtuvo el 37.75% por ciento. Tal y como lo señalara la revista "Envío", «los otros partidos que participaron en las elecciones a la Presidencia acumularon entre todos el 11.22% de los votos válidos. De estos partidos, 19 no lograron alcanzar ni siquiera el 0.60% del total de los votos válidos del electorado nacional».

Durante el gobierno liberal de Arnoldo Alemán, el Estado continuó dependiendo de la cooperación externa y adecuándose a los requerimientos de los organismos internacionales. De acuerdo con el Programa de las Naciones Unidas para el Desarrollo (PNUD), en 1999 el monto de la cooperación externa recibida por este gobierno equivalía a un 22% del Producto Interno Bruto del país. Mientras tanto, continuaron debilitándose las organizaciones populares surgidas durante la revolución sandinista. Las organizaciones de la sociedad civil que habían surgido a partir de 1990, por su parte, impulsaron múltiples e infructuosos esfuerzos por desarrollar una capacidad social para democratizar la acción del Estado.

Ni siquiera la crisis social generada por el huracán "Mitch" en octubre de 1998 demostró tener la fuerza suficiente para contrarrestar la brecha que separaba al Estado de la sociedad nicaragüense. Los daños humanos, ecológicos y materiales ocasionados por el huracán "Mitch" fueron calificados por varios observadores como "de proporciones bíblicas".

La propuesta de desarrollo formulada por la sociedad civil nicaragüense para la reconstrucción del país tras el huracán "Mitch" estaba basada en una premisa básica: la solución de la vulnerabilidad social y ecológica en Centroamérica requería la transformación de las estructuras sociales que definían la distribución del poder la riqueza social en la región. A su vez, la transformación de tales estructuras requería la revisión de los modelos de desarrollo que habían guiado la evolución histórica de la región y, especialmente, la evolución de las relaciones entre Estado y sociedad en Nicaragua. La Coordinadora Civil para la Emergencia y Reconstrucción de Nicaragua señalaba: "Queremos una reconstrucción que no nos regrese a la 'normalidad' en la que estábamos antes del huracán, sino que nos permita superar la exclusión y la marginalidad en la que han vivido amplios sectores de la población, y una utilización más adecuada de nuestros recursos naturales".

El impacto del huracán "Mitch", lejos de debilitar el modelo de relaciones entre el Estado y la sociedad nicaragüense, fue transformado en un evento facilitador del desarrollo y la consolidación de dicho modelo. El alivio de la deuda externa, la obtención de nuevos recursos externos, y la solicitud del ingreso de Nicaragua a la iniciativa de alivio de deuda del Banco Mundial fueron medidas que se tradujeron en un mayor nivel de autonomía estatal con respecto de la sociedad civil que, pese a sus esfuerzos, demostró carecer de los derechos ciudadanos y de la fuerza política necesaria para condicionar las prioridades y la acción del Estado.

La dependencia externa del Estado y la debilidad de la sociedad civil nicaragüense facilitaron la externalización del conflicto social; es decir, su desplazamiento fuera del espacio político y legal nicaragüense. Así, el debate generado por el impacto social del huracán Mitch no se organizó dentro del marco políticoinstitucional nicaragüense, sino más bien en torno a la comunidad donante, que se constituyó en fuente de recursos financieros y también de legitimidad. En estas circunstancia, los afectados por el huracán se vieron convertidos en elementos pasivos que dependían de la interacción entre las organizaciones de la sociedad civil, el Estado, los países donantes y los organismos internacionales.

La ausencia de mecanismos de participación políticas capaces de regular la acción del Estado creó las condiciones para la proliferación de la corrupción administrativa y para la impunidad. Durante el período 1990-1996 varios funcionarios del régimen Barrios de Chamorro se vieron envueltos en casos de corrupción. Además, el proceso de privatización impulsado por este gobierno se efectuó con muy poca transparencia.

Fue durante el régimen de Arnoldo Alemán, sin embargo, cuando la corrupción se convirtió en un problema sistémico que llegó a alcanzar niveles comparables a los del somocismo. En la lucha contra la corrupción, los medios de comunicación desempeñaron un papel protagónico al desenmascarar a los culpables y presentar evidencias de su responsabilidad.

Un estudio realizado entre la población urbana de Managua por el Instituto de Encuestas y Sondeos de la revista Envío, en abril de 1999, mostraba que el 45.2% de los entrevistados pensaba que el gobierno de Arnoldo Alemán había sido el más corrupto de la historia de Nicaragua. Un 26.5% opinaba que todos los gobiernos de Nicaragua habían sido igualmente corruptos. Un 16.8% señalaba al gobierno del FSLN como el más corrupto de la historia del país, en tanto que apenas un 1.8% señalaba al gobierno de Violeta Barrios de Chamorro como el más corrupto de los gobiernos nicaragüenses. El resto de los entrevistados opinó que la corrupción le era indiferente, o no supo responder.

Para febrero del año 2000 una encuesta de opinión realizada por el Instituto de Estudios Nicaragüense (IEN) mostraba que el 89% de la opinión pública pensaba que la corrupción afectaba al funcionamiento del gobierno y la administración pública del país. Las principales expresiones de este fenómeno, según los entrevistados, eran la vida ostentosa de los funcionarios públicos, el rápido e inexplicable aumento de sus patrimonios personales, el aprovechamiento de sus cargos públicos para la promoción de sus negocios particulares, sus sueldos exorbitantes y el tráfico de influencias.

Durante el régimen de Arnoldo Alemán la corrupción administrativa, la impunidad de los culpables y la incapacidad del sistema judicial para aplicar las leyes del país mostraron con dramática claridad las debilidades del modelo de gobernabilidad democrática adoptado en 1990, y, más específicamente, la ausencia de una sociedad civil con la capacidad para condicionar la acción del Estado. Esta debilidad es particularmente notable si se toma en cuenta que el gobierno de Alemán -a diferencia de los gobiernos somocistas- no contaba con el apoyo de un aparato represivo para sofocar las presiones sociales. Así pues, diez años después de la transición hacia un modelo de gobernabilidad democrática, el Estado nicaragüense continuaba gravitando sobre una sociedad que no contaba con la capacidad para controlar la conducta de sus gobernantes.

Al concluir el siglo XX, el Estado y el sistema político del país habían caído bajo el control del partido gobernante y del FSLN, los que poco después de las elecciones de 1996 habían iniciado un proceso de acercamiento y colaboración. Este proceso alcanzó su expresión más concreta en enero de 1999, cuando el Partido Liberal Constitucionalista (PLC) del presidente Arnoldo Alemán y el FSLN (liderado por Daniel Ortega) consumaron un pacto que tendría graves consecuencias para el desarrollo político nicaragüense. El pacto entre el PLC y el FSLN se vio respaldada por la influencia ideológica que la Iglesia católica nicaragüense mantenía sobre un importante sector de la población y, en particular, por la ambigua posición que adoptara esta institución religiosa ante el fenómeno de la corrupción y la impunidad.

Sobre la base del pacto PLC-FSLN, estas dos organizaciones políticas se repartieron el poder de la Corte Suprema de Justicia, en el Consejo Supremo Electoral, en el Consejo Superior de la Contraloría, en la Procuraduría de Derechos Humanos y en la Superintendencia de Bancos. El pacto, además, hizo posible la aprobación de una ley que resolvió el problema de "la piñata": la adquisición fraudulenta, por parte de miembros del FSLN, de propiedades del Estado y de particulares durante los meses posteriores a su derrota electoral en el año 1990. El pacto, por otra parte, dejó abiertas las puertas para la introducción de una reforma constitucional que perpetuaría el poder de los dos partidos pactantes.

Así pues, el precario orden social nicaragüense aparecía organizado dentro de una estructura informal de colaboración entre tres instituciones: el gobernante PLC, el FSLN y la Iglesia católica nicaragüense. Cada uno de estos tres actores apostaba por mantener y desarrollar su poder dentro de esa relación de mutua conveniencia. El pacto político les aseguraba al Frente Sandinista de Liberación Nacional y al Partido Liberal Constitucionalista la posición de gobierno o de partido principal de oposición dentro del sistema político nicaragüense. El colaboracionismo entre la Iglesia y el PLC, por otra parte, le otorgaba a la jerarquía católica un importante grado de influencia dentro del sistema político nicaragüense, así como el apoyo del Estado en su lucha contra la creciente popularidad de las religiones protestantes. Por su parte, el FSLN intentaba acercarse a la Iglesia católica, cuyo apoyo parecía indispensable para mantener el orden social en un país sometido a los explosivos niveles de pobreza que sufría Nicaragua: para 1998 este nivel alcanzaba al 72.6% de la población, según los cálculos del Programa de las Naciones Unidas para el Desarrollo.

A comienzos del siglo XXI Nicaragua vivía una crisis de gobernabilidad que se hacía evidente en la corrupción que afectaba el funcionamiento de la administración pública del país, en el fenómeno de la impunidad -denunciado repetidamente por los medios de comunicación- y en la pérdida de legitimidad del Estado y del sistema político nicaragüense -registrada en numerosas encuestas de opinión pública realizadas durante ese periodo-. Esta crisis de gobernabilidad generaba a su vez una crisis de seguridad ciudadana que se manifestaba en la desesperanza expresada por el pueblo nicaragüense en diversos reportajes periódicos e investigaciones académicas; y en el sentimiento de amenaza contra la seguridad personal revelado por múltiples encuestas.

Se entiende la gobernabilidad como la capacidad que posee el Estado y el sistema político de un país para resolver sus conflictos sociales. De tal manera que una crisis de gobernabilidad es la existencia de una condición real o potencial de desorden social que se deriva de la incapacidad de un sistema político y de sus respectivas instituciones públicas de regular las tensiones y contradicciones propias de la vida en sociedad.

La seguridad ciudadana, por su parte, es una condición psicosocial que consiste en la confianza que poseen los miembros de una sociedad para organizar, controlar y planificar su existencia. Así, la inseguridad es la ausencia de tal confianza. El desarrollo de una situación de seguridad ciudadana requiere la existencia de condiciones sociales humanamente adecuadas, estables, y predecibles; en tanto que el concepto de gobernabilidad se refiere a la existencia de una capacidad políticoinstitucional para crear y reproducir estas condiciones.

El orden social y la seguridad ciudadana que generan los sistemas consolidados de gobernabilidad democrática dependen fundamentalmente, en primer lugar, de la existencia de Estados que cuenten con la capacidad para filtrar influencias externas así como para organizar y regular las relaciones sociales que operan dentro de su base territorial; y, en segundo lugar, de la existencia de sociedades civiles organizadas que cuenten con la capacidad de condicionar el poder del Estado. La capacidad soberana y de regulación del Estado facilita la construcción política del orden social en lo nacional, mientras que el control democrático de esta capacidad asegura que el orden generado por la acción estatal responda a las necesidades y demandas de la sociedad.

En la gran mayoría de los países de América Latina, el desarrollo histórico de las relaciones entre Estado y sociedad no han logrado generar las condiciones necesarias para la consolidación de sistemas de gobernabilidad democrática. La dependencia externa del Estado, aunada a la exclusión abierta y sistemática de amplios sectores sociales, ha facilitado el desarrollo de estructuras de poder estatal que hasta el día de hoy gozan de altos niveles de autonomía respecto de la sociedad. A su vez, ello ha hecho posible el surgimiento de gobiernos que disponen de la facultad de ignorar las demandas y necesidades de la población.

La doble condición de dependencia externa y de autonomía doméstica que caracteriza al Estado latinoamericano adquiere ribetes especiales en el caso nicaragüense, por el fenómeno de la intervención extranjera que desde el periodo inmediatamente posterior a la independencia dificultó no solo la democratización del Estado, sino su misma nacionalización.

Desde esta perspectiva, lo que caracteriza el desarrollo histórico de las relaciones entre Estado y sociedad en Nicaragua no es simplemente la ausencia de una estructura de derechos ciudadanos capaz de democratizar el funcionamiento del Estado, sino la ausencia de élites y movimientos sociales capaz de nacionalizarlo. La nacionalización del Estado consiste en generar niveles de soberanía que faciliten un control nacional mínimo sobre los factores que determinan el desarrollo histórico de una sociedad.

Así pues, las luchas por la independencia, los vaivenes de las Provincias Unidas del Centro de América antes de su fragmentación, la presidencia de William Walker y la Guerra Nacional, la precaria estabilidad social de los Treinta Años Conservadores, la caída de Zelaya, el proyecto de ingeniería social que se inicia a partir de 1909, el somocismo, y la derrota del sandinismo han sido eventos y procesos históricos fuertemente condicionados -y en algunos casos simplemente determinados- por fuerzas e influencias externas.

Los condicionamientos externos que históricamente han contribuido a separar el Estado de la sociedad nicaragüense adquirieron una relevancia especial durante el periodo de la revolución sandinista. Inicialmente la revolución fue un proyecto fundamentado en valores que desafiaban la dinámica histórica nicaragüense. Frente a la dependencia externa de un país condicionado por la constante repetición del fenómeno de la intervención extranjera, la revolución sandinista aspiraba a la construcción a la construcción de una patria soberana; y frente a la realidad de la pobreza y desigualdad social en Nicaragua, la revolución liderada por el Frente Sandinista de Liberación Nacional (FSLN) proponía la edificación de una sociedad organizada según la "lógica de las grandes mayorías".

Señalar que la soberanía y la justicia social fueron dos de los valores centrales que guiaron el desarrollo inicial del proyecto revolucionario sandinista no es glorificar o idealizar la conducta y el pensamiento político del FSLN, sino simplemente señalar una realidad que se expresó claramente en el discurso y en la práctica revolucionaria de esta organización política antes y después del triunfo de julio de 1979. Los abusos de poder, la corrupción y la violación de los derechos humanos que ocurrieron durante la década de 1980 son realidades que coexistieron con el proyecto de creación de una Nicaragua justa y soberana.

Sin embargo, a finales de la década de 1990 el sistema político nicaragüense estaba ampliamiente dominado bajo el espectro del pacto suscrito entre Arnoldo Alemán del PLC (partido gobernante), y Daniel Ortega del FSLN (partido de oposición), en el que aseguraban la repartición de poder en las principales entidades del Estado y sus poderes.

Nicaragua entró al período preelectoral, que culminaría el 4 de noviembre de 2001, en un ambiente marcado por el pacto entre el PLC y el FSLN. el 28 de enero de 2001 se llevaba a cabo la convención del PLC, de la que emergió como candidato a la presidencia Enrique Bolaños Geyer, vicepresidente del país con Alemán. Bolaños, nacido el 13 de mayo de 1928 en Masaya y graduado en Ingeniería en la universidad estadounidense de Saint Louis, tenía en su haber una dilatada y exitosa carrera empresarial desarrollada en los ámbitos del agro y la industria. Asimismo, fue un caracterizado dirigente de organizaciones patronales a partir de 1979, cuando fue elegido presidente de la Asociación de Algodoneros de Oriente (ADADO), director de la Unión de Productores Agropecuarios de Nicaragua (UPANIC) y director de la Cámara de Industrias de Nicaragua (CADIN). También fue dirigente del Consejo Superior de la Empresa Privada (COSEP). Decidido opositor del régimen sandinista, fue detenido por breve tiempo en tres ocasiones, acusado de violar las leyes de excepción implantadas por el gobierno para hacer frente a las guerrillas de la "Contra", mientras parte de sus fincas eran expropiadas en el curso de la reforma agraria y algunas de sus plantas fabriles eran confiscadas. Con tal motivo, fue indemnizado por el Estado en la década de 1990.

Por otro lado, el 25 de febrero de 2001 el congreso del FSLN confirmó la candidatura de Daniel Ortega Saavedra para las elecciones presidenciales de noviembre de ese mismo año, a las que concurriría, en coalición con varios partidos políticos pequeños de variadas tendencias; esta agrupación sería denominada Convergencia Nacional. Se trató de la cuarta vez que el líder sandinista, de 55 años de edad para entonces, disputó la jefatura del Estado; en 1984 había obtenido la victoria, mientras que fue derrotado en 1990 por Violeta Barrios de Chamorro y en 1996 por Arnoldo Alemán. Pero tanto la imagen como el discurso de Ortega en la campaña electoral de 2001 poco tenían que ver con su pasado revolucionario.

Tras 16 años de gobiernos liberales y conservadores en las lecciones celebradas el 5 de noviembre de 2006 el candidato sandinista, Daniel Ortega vence con un 38% de los votos.

Las políticas liberales y conservadoras de los gobiernos de Chamorro, Alemán y Bolaños llevaron al país a una situación en la que el 80% de sus casi seis millones de habitantes en el umbral de la pobreza (ingresos de menos de dos dólares diarios), la mitad en paro o en subempleo, salarios de 100 dólares al mes y una deuda externa de 6.500 millones de dólares tras la condonación de cuatro de cada cinco dólares que debía. Aunque en ese periodo la economía de Nicaragua fuera una de las de mayor crecimiento en América Central y el FSLN mantuviera una amplia presencia en los diferentes ámbitos de poder.

El FSLN, con varias escisiones como la del Movimiento Renovador Sandinista (que obtenía un 6,89% de los votos) realizó una campaña basada en la reconciliación y como segundo hombre, detrás de Ortega, puso Jaime Morales Carazo, un antiguo "contra", supuesto agente de la CIA. La coyuntura latinoamericana, con Venezuela Bolivariana, con Bolivia, Ecuador, Brasil y otros países con gobiernos progresistas favorecieron el sandinista.

Las primeras acciones de gobierno del FSLN fueron el restablecer la gratuidad de los servicios de Educación y Salud. En educación se prohíbe el cobro en las escuelas públicas, de matrículas, mensualidades, material escolar y otros insumos. En Salud se eliminan las consultas privadas en los centros públicos y se restablece la gratuidad de los medicamentos, las operaciones quirúrgicas y las pruebas clínicas que se realicen en los centros sanitarios dependientes del Estado.

En las elecciones municipales nicaragüenses del pasado 9 de noviembre del 2008, el FSLN obtuvo el 48.79% de los votos frente a su rival más inmediato el Partido Liberal Constitucionalista que obtuvo el 45.88%, mientras que el ALN obtuvo el 3.80% de los votos, PRN O.86% y AC 0.67%. El FSLN ganó 10 departamentos (Nueva Segovia, Estelí, Madriz, Chinandega, León, Managua, Masaya, Carazo, Rivas y Matagalpa) y el PLC: 7 (Granada, Chontales, Boaco, Jinotega, RACCN, RACCS y Río San Juan). Para un total de 105 alcaldías (13 cabeceras departamentales) incluyendo la capital Managua, en contraposición a 37 del PLC (5 cabeceras departamentales) y el ALN ganó 4 Alcaldías (ninguna cabecera departamental).Sin embargo, ciertas anomalías y la falta de observadores internacionales en las elecciones, originó un descontento social y el rechazo de los resultados por parte de la oposición.



</doc>
<doc id="21867" url="https://es.wikipedia.org/wiki?curid=21867" title="GNU Bison">
GNU Bison

GNU bison es un programa generador de analizadores sintácticos de propósito general perteneciente al proyecto GNU disponible para prácticamente todos los sistemas operativos, se usa normalmente acompañado de flex aunque los analizadores léxicos se pueden también obtener de otras formas.

Bison convierte la descripción formal de un lenguaje, escrita como una gramática libre de contexto LALR, en un programa en C, C++, o Java que realiza análisis sintáctico. Es utilizado para crear analizadores para muchos lenguajes, desde simples calculadoras hasta lenguajes complejos. Para utilizar "Bison", es necesaria experiencia con la sintaxis usada para describir gramáticas. 

GNU bison tiene compatibilidad con Yacc: todas las gramáticas bien escritas para Yacc, funcionan en Bison sin necesidad de ser modificadas. Cualquier persona que esté familiarizada con Yacc podría utilizar Bison sin problemas.

Bison fue escrito en un principio por Robert Corbett; Richard Stallman lo hizo compatible con Yacc y Wilfred Hansen de la Carnegie Mellon University añadió soporte para literales multicaracter y otras características.



</doc>
<doc id="21874" url="https://es.wikipedia.org/wiki?curid=21874" title="Cobalto">
Cobalto

El cobalto (del alemán "kobalt", voz derivada de "kobolds", los "gnomos" que, según los mineros de Sajonia de la Edad Media, eran espíritus de la tierra que tenían embrujado el mineral, por lo que, aunque parecía mena de cobre, no producía este elemento con el tratamiento habitual) es un elemento químico de número atómico 27 y símbolo Co situado en el grupo 9 de la tabla periódica de los elementos.

Se le denominaba "kobold" en la Edad Media por los mineros que consideraban este metal sin valor y tenían la creencia de que un buen duende (un "kobold") lo ponía en sustitución de la plata que había robado. En el diccionario castellano del siglo XVIII aparece como "cobalt".

El cobalto es un metal ferromagnético, de color blanco azulado. Su temperatura de Curie es de 1388 K. Normalmente se encuentra junto con níquel, y ambos suelen formar parte de los meteoritos de hierro. Es un elemento químico esencial para los mamíferos en pequeñas cantidades. El Co-60, un radioisótopo de cobalto, es un importante trazador y agente en el tratamiento del cáncer.

El cobalto metálico está comúnmente constituido de una mezcla de dos formas alotrópicas con estructuras cristalinas hexagonal y cúbica centrada en las caras siendo la temperatura de transición entre ambas de 722 K.

Se emplea sobre todo en superaleaciones de alto rendimiento, siendo éstas normalmente más caras que las de níquel. Es un metal eminentemente de aleación, al igual que el níquel o el zinc, por ejemplo. Dichos metales suelen agregarse a otros que actúan de base, aunque cuando el Cobalto actúa de base suele hacerlo en aleaciones con cromo. Su principal característica es su elevadísima dureza y resistencia al desgaste. Son aleaciones normalmente poco usadas ya que su virtud no compensa la gran cantidad que hay que abonar por ellas. El cobalto posee características muy similares a sus elementos vecinos, hierro y níquel, con los cuales comparte más rasgos que con los elementos de su propio grupo en la tabla periódica.
Ni cobalto ni níquel suelen mezclarse con la plata ni el mercurio (siendo ambos raras excepciones) además de que comparten el efecto magnético del hierro. El cobalto es el metal más escaso de estos tres, es el menos rentable y también el más caro. Encuentra pocos usos en la industria en comparación a sus vecinos inmediatos. Se trata de uno de los pocos elementos químicos monoisotópicos. El cobalto tiene poca resistencia química aunque es más estable que el hierro ya que se mantiene en aire y agua siempre que no se encuentren otros elementos corrosivos en dichos medios.

Presenta estados de oxidación bajos. Los compuestos en los que el cobalto tiene un estado de oxidación de +4 son poco comunes. El estado de oxidación +2 es muy frecuente, así como el +3. También existen complejos importantes con el estado de oxidación +1.

Los compuestos de cobalto se han utilizado durante siglos para obtener un color azul intenso de vidrio, los esmaltes y cerámicas. Se ha detectado cobalto en esculturas egipcias y en joyas persas desde el tercer milenio aC, en las ruinas de Pompeya (destruida en el año 79 dC), y en China, en la dinastía Tang (618-907 dC) y la dinastía Ming (1368-1644 dC).

El cobalto se ha empleado para colorear el vidrio desde la Edad del Bronce. La excavación del naufragio Uluburun encontró un lingote de cristal azul, que fue confeccionado durante el siglo XIV AC. artículos de cristal azul de Egipto son de color con el cobre, el hierro o el cobalto. La más antigua de cobalto de color de cristal era de la época de la dinastía XVIII de Egipto (1550-1292 aC). Se desconoce el lugar donde se obtuvieron los compuestos de cobalto.

El elemento fue descubierto por el químico sueco George Brandt. La fecha del descubrimiento varía en las diversas fuentes entre 1730 y 1737. Mostrando que es un nuevo elemento hasta entonces desconocido diferente de bismuto y otros metales tradicionales, y decir que es un nuevo "semi-metal". Brandt fue capaz de demostrar que el cobalto era el responsable del color azul del vidrio que previamente se atribuía al bismuto. El cobalto se convirtió en el primer metal descubierto desde la época pre-histórica, en la que todos los metales conocidos (hierro, cobre, plata, oro, zinc, mercurio, estaño, plomo y bismuto) no tenían descubridores registrados.

Su nombre proviene del alemán "kobalt" de "kobold", llamado así por los mineros por su color, toxicidad y los problemas que ocasionaba ya que al igual que el níquel contaminaba y degradaba los elementos que se deseaba extraer. Los primeros intentos de fundición de estas menas para obtener metales como el cobre o el níquel, fracasaban dando en su lugar simplemente un polvo (óxido de cobalto (II)). Además, debido a que los minerales primarios de cobalto siempre contienen arsénico, la fundición de estas menas oxidaba el contenido de arsénico para dar el altamente tóxico y volátil, óxido de arsénico, lo que también disminuye el aprecio de estas menas para los mineros.

Durante el siglo XIX, una parte significativa de la producción mundial, entre el 70 y 80%, de azul cobalto (un tinte hecho con compuestos de cobalto y alúmina) y esmalte (vidrio de cobalto en polvo para uso con fines de pigmento en cerámica y pintura) se llevó a cabo en la fábrica noruega Blaafarveværket adquirida en 1823 por el barón W. C. Benecke y el industrial prusiano Benjamin Wegner. Las primeras minas para la producción de esmalte entre los siglos XVI al XVIII se encontraban en Noruega, Suecia, Sajonia y Hungría. Con el descubrimiento de mineral de cobalto en Nueva Caledonia en 1864 la extracción de cobalto en Europa disminuyó. Con el descubrimiento de yacimientos minerales en Ontario, Canadá en 1904 y de yacimientos aún mayores en la provincia de Katanga en el Congo en 1914, las operaciones mineras cambiaron de nuevo. Por el conflicto de Shaba a partir de 1978, la principal fuente de cobalto, las minas de cobre de la provincia de Katanga, casi detuvieron su producción. El impacto en la economía mundial de cobalto de este conflicto fue menor de lo esperado, porque la industria establecida formas efectivas para reciclar de materiales de cobalto y en algunos casos fue capaz de cambiar a alternativas sin cobalto.

En 1938 John Livingood y Glenn Seaborg descubrieron el cobalto-60. La primera máquina de radioterapia, "bomba de cobalto", construida en Canadá por un equipo liderado por Ivan Smith y Roy Errington se utilizó en un paciente el 27 de octubre de 1951; el equipo se encuentra actualmente expuesto en el "Saskatoon Cancer Centre", en la ciudad de Saskatoon (Saskatchewan).

Después de la Segunda Guerra Mundial, los EE. UU. quería asegurase de que nunca le faltaría el mineral de cobalto necesario, como le había ocurrido a los alemanes y fue la exploración de una fuente dentro de la frontera de los EE. UU. Un buen suministro de los minerales necesarios se encuentra en Idaho cerca del cañón Blackbird en la ladera de una montaña. La Compañía Minera Calera comenzó la producción en este lugar

En 2005, los depósitos de cobre en la provincia de Katanga (antigua provincia de Shaba) de la República Democrática del Congo fueron el principal productor de cobalto con casi el 40% cuota mundial, según informa el Servicio Geológico Británico. La situación política en el Congo influye en el precio de cobalto de manera significativa.

El proyecto de la Montaña Mukondo, operado por la Central African Mining and Exploration Company (CAMEC) en Katanga, puede ser la más rica reserva de cobalto en el mundo. Se estima que será capaz de producir alrededor de un tercio de la producción total mundial de cobalto en el 2008. En julio de 2009 CAMEC anunció un acuerdo a largo plazo en virtud del cual CAMEC entregaría toda su producción anual de concentrado de cobalto de la Montaña Mukondo a "Zhejiang Galico Cobalt & Nickel Materials" de China.

Existen varios métodos para separar el cobalto del cobre y níquel. Dependen de la concentración de cobalto y la composición exacta del mineral utilizado. Una etapa de separación implica flotación por espuma, en el que los tensioactivos se unen a los diferentes componentes del mineral, dando lugar a un enriquecimiento de mena de cobalto. Tras el tostado se convierte la mena a sulfato de cobalto, mientras que el cobre y el hierro se oxida al óxido. La lixiviación con agua extrae el sulfato junto con los arseniatos. Los residuos están además lixiviado con ácido sulfúrico obteniéndose una solución de sulfato de cobre. El cobalto también puede ser lixiviado de la escoria de la fundición de cobre.

Los productos de los procesos mencionados anteriormente se transforman en óxido de cobalto (CoO). Este óxido se reduce al metal por la reacción aluminotérmica o reducción con carbono en un alto horno.


Debido a los varios estados de oxidación que presenta, existe un abundante número de compuestos de cobalto. Los óxidos CoO (temperatura de Néel 291 K) y CoO (temperatura de Néel 40 K) son ambos antiferromagnéticos a baja temperatura.

Se han caracterizado 22 radioisótopos siendo los más estables el Co-60, el Co-57 y el Co-56 con periodos de semidesintegración de 5,2714 años, 271,79 días y 70,86 días respectivamente. Los demás isótopos radiactivos tienen periodos de semidesintegración inferiores a 18 horas y la mayoría menores de 1 segundo. El cobalto presenta además cuatro metaestados, todos ellos con periodos de semidesintegración menores de 15 minutos.

La masa atómica de los isótopos del cobalto oscila entre 50 uma (Co-50) y 73 uma (Co-73). Los isótopos más ligeros que el estable (Co-59) se desintegran principalmente por captura electrónica originando isótopos de hierro, mientras que los más pesados que el isótopo estable se desintegran por emisión beta dando lugar a isótopos de níquel.

El cobalto-60 se usa en radioterapia en sustitución del radio por su menor precio (y considerando que el radio se desintegra en radón que es un elemento radiactivo y se presenta en forma de gas, por lo que es difícil encapsularlo para evitar contaminación radiactiva). Produce dos rayos gamma con energías de 1,17 MeV y 1,33 MeV y al ser la fuente empleada de unos dos centímetros de radio provoca la aparición de zonas de penumbra dispersando la radiación en torno a la dirección de radiación. El metal tiende a producir un polvo muy fino que dificulta la protección frente a la radiación. La fuente de Co-60 tiene una vida útil de aproximadamente 5 años, pero superado ese tiempo sigue siendo muy radiactivo, por lo que estas fuentes han perdido, en cierta medida, su popularidad en occidente.

El cobalto es esencial en todos los animales, incluyendo los humanos. Forma parte de la cobalamina (Vitamina B12). Una deficiencia de cobalto puede llevar a anemia. Pese a ello, la anemia secundaria por déficit de cobalto es muy rara, debido a que basta con consumir trazas del elemento para mantener la correcta homeostasis. Además, el cobalto es un elemento que se encuentra en varios alimentos, siendo difícil un déficit por baja ingesta. 

Las proteínas basadas en la cobalamina usan el anillo de corrina para mantener unido el cobalto. La coenzima B12 proporciona el enlace C-Co, el cual participa en las reacciones.

El cobalto metálico en polvo finamente dividido es inflamable. Los compuestos de cobalto en general deben manipularse con precaución por la ligera toxicidad del metal.

El Co-60 es radiactivo y la exposición a su radiación puede provocar cáncer. La ingestión de Co-60 conlleva la acumulación de alguna cantidad en los tejidos, cantidad que se elimina muy lentamente. En una eventual confrontación nuclear, la emisión de neutrones convertiría el hierro en Co-60 multiplicando los efectos de la radiación tras la explosión y prolongando en el tiempo los efectos de la contaminación radioactiva; con este propósito se diseñan algunas armas nucleares denominadas "bombas sucias" (del inglés "dirty bomb"). En ausencia de guerra nuclear, el riesgo proviene de la inadecuada manipulación o mantenimiento de las unidades de radioterapia.




</doc>
<doc id="21875" url="https://es.wikipedia.org/wiki?curid=21875" title="Níquel">
Níquel

El níquel es un elemento químico cuyo número atómico es 28 y su símbolo es Ni, situado en el grupo 10 de la tabla periódica de los elementos.

El 87 % de las hidrogenasas contienen níquel, especialmente en aquellas cuya función es oxidar el hidrógeno. El níquel sufre cambios en su estado de oxidación lo que indica que el núcleo de níquel es la parte activa de la enzima.

El níquel está también presente en la enzima metil con reductasa y en bacterias metanogénicas.

Es un metal de transición de color blanco con un ligerísimo tono amarillo, conductor de la electricidad y del calor, muy dúctil y maleable por lo que se puede laminar, pulir y forjar fácilmente, y presentando ferromagnetismo a temperatura ambiental. Es otro de los metales muy densos como el hierro, iridio y osmio. Se encuentra en distintos minerales, en meteoritos (aleado con hierro) y, en principio, hay níquel en el interior de la Tierra principalmente en su núcleo, donde se trata del segundo metal más abundante por detrás del hierro, metal con el que comparte numerosas características similares.

El níquel es aleado con hierro para proporcionar tenacidad y resistencia a la corrosión, en los aceros austeníticos el níquel es esencial puesto que al ser un metal gammágeno estabiliza la austenita. Es resistente a la corrosión y se suele utilizar como recubrimiento, mediante electro deposición. El metal y alguna de sus aleaciones, como la aleación Monel, se utilizan para manejar el flúor y algunos fluoruros debido a que reacciona con dificultad con estos productos. Su coste roza la mayoría de las veces el primer puesto entre los precios de los metales comunes en los mercados dedicados a los metales.
Es un producto absolutamente esencial para el desarrollo de la industria, además de uno de los metales más demandados. Reacciona con dificultad en medios agresivos y se considera resistente a la corrosión; no sufre el llamado efecto "galleo" el cual sí padece el cobre, por ejemplo.

Su estado de oxidación más normal es +2. Puede presentar otros, se han observado estados de oxidación 0, +1 y +3 en complejos, pero son muy poco característicos.caracterizado por su buena voluntad de estar.

El uso del níquel se remonta aproximadamente al siglo IV a. C., generalmente junto con el cobre, ya que aparece con frecuencia en los minerales de este metal. Bronces originarios de la actual Siria tienen contenidos de níquel superiores al 2%. Alrededor del año 190 a.C., durante los reinados de Euthydemus II, Agathocles y Pantaleon se utilizó en Bactriana para fabricar monedas una aleación de color blanco, semejante a la plata, formada por cobre con el 20% de níquel, semejante al cuproníquel actual. Manuscritos chinos sugieren que el «cobre blanco» se utilizaba en Oriente hacia 1700 al 1400 a. C.; sin embargo, la facilidad de confundir las menas de níquel con las de plata, y la existencia de otras aleaciones de color blanco induce a pensar que en realidad el uso del níquel fue muy posterior.

Los minerales que contienen níquel, como la niquelina, se han empleado para colorear el vidrio. En 1751 Axel Frederik Cronstedt, intentando extraer cobre de la niquelina, obtuvo un metal blanco que llamó níquel, ya que los mineros de Hartz atribuían al «viejo Nick» (el diablo) el que algunos minerales de cobre no se pudieran trabajar; y el metal responsable de ello resultó ser el descubierto por Cronstedt en la niquelina, o "Kupfernickel", diablo del cobre, como se llama aún al mineral en idioma alemán. 

Según un diccionario etimológico italiano, níquel proviene del sueco "nickel", que viene del alemán "Kupfernickel", propiamente ‘falso cobre’, compuesto de "Kupfer" (cobre) y "Nickel" (sobrenombre de Nikolaus), nombre dado por los mineros a los minerales inútiles, usado en broma para indicar un mineral que del cobre tiene sólo el color.

Hoy en día es muy frecuente encontrarlo en las monedas de cualquier país, donde debido a su elevado coste se alea con cobre. Estas monedas suplantaron a las de plata alrededor de mediados del siglo XX, provocando en ocasiones confusión. Algunos ejemplos son los cinco céntimos de Estados Unidos o el disco interno de una moneda de un euro.

La primera moneda acuñada níquel puro fue la de 20 céntimos de Suiza, en 1881.

El níquel aparece en forma de metal en los meteoritos junto con el hierro (formando las aleaciones kamacita y taenita). También se encuentra en el núcleo de la Tierra junto al hierro, iridio y osmio, formando con estos tres metales una aleación de estructura metálica. Combinado se encuentra en minerales diversos como garnierita, millerita, pentlandita y pirrotina.

Los principales productores de mineral de níquel son: (2018, datos estimados)
Otros productores importantes son Brasil, Cuba o Guatemala

En cuanto a reservas. Indonesia, Australia y Brasil contaban con el 57% de las reservas mundiales estimadas en 2018. No se contabilizan las reservas de Nueva Caledonia.

La niquelina (NiAs), la garnierita (SiO[Ni, Mg]•2 HO), este último es uno de los minerales más utilizados en la extracción del níquel, también existen los sulfuros, de ellos los más importantes son los sulfuros de hierro y níquel, pentlandita y pirrotita (Ni, Fe) xSy, otros minerales que se encuentran en la naturaleza son los arseniuros, silicatos, sulfoarseniuros.

En la naturaleza se encuentran 5 isótopos estables: Ni, Ni, Ni, Ni y Ni, siendo el más ligero y el más abundante (68,077 %). Se han caracterizado además 18 isótopos radioactivos de los que los más estables son el Ni, el Ni y el Ni con periodos de semidesintegración de 76.000 años, 100,1 años y 6,077 días respectivamente. Los demás radioisótopos, con masas atómicas desde 52 uma (Ni) a 74 uma (Ni), tienen periodos de semidesintegración inferiores a 60 horas y la mayoría no alcanzan los treinta segundos. El níquel tiene además un estado metaestable.

El Ni se produce en grandes cantidades en supernovas de tipo II correspondiendo la forma de la curva de luz a la desintegración del Ni en Co y este en Fe.

El Ni es un isótopo de larga vida obtenido por cosmogénesis. Este isótopo ha encontrado diversas aplicaciones en la datación radiométrica de meteoritos y en la determinación de la abundancia de polvo extraterrestre en hielos y sedimentos. El Ni es hijo del Fe (periodo de semidesintegración de 1,5 millones de años) cuya persistencia en el sistema Solar en concentraciones suficientemente altas ha podido causar variaciones observables en la composición isotópica del Ni, de este modo, el análisis de la abundancia de Ni en materiales extraterrestres puede proporcionar información sobre el origen del sistema solar y su historia primordial.

Se han caracterizado unos 18 radioisótopos de níquel, siendo el más estable 59
Ni con una vida media de 76,000 años, 63Ni con 100 años y 56Ni con 6 días. Todos los isótopos radiactivos restantes tienen vidas medias que son menos de 60 horas y la mayoría de estos tienen vidas medias que son menos de 30 segundos. Este elemento también tiene un meta estado. 

El níquel-56 radioactivo se produce mediante el proceso de combustión de silicio y luego se libera en grandes cantidades durante las supernovas de tipo Ia. La forma de la curva de luz de estas supernovas en tiempos intermedios y tardíos corresponde a la desintegración a través de la captura de electrones de níquel-56 a cobalto-56 y finalmente a hierro-56. El níquel-59 es un radionúclido cosmogénico de larga duración con una vida media de 76,000 años. 59 Ni ha encontrado muchas aplicaciones en la geología isotópica. 59. 

El Ni se ha utilizado para fechar la edad terrestre de los meteoritos y para determinar la abundancia de polvo extraterrestre en hielo y sedimentos. La vida media del níquel-78 se midió recientemente a 110 milisegundos, y se cree que es un isótopo importante en la nucleosíntesis de supernovas de elementos más pesados ​​que el hierro. El nucleido 48Ni, descubierto en 1999, es el isótopo de elementos pesados ​​más rico en protones conocido. Con 28 protones y 20 

neutrones 48Ni es "doble magia", como lo es 78

Ni con 28 protones y 50 neutrones. Por lo tanto, ambos son inusualmente estables para los nucleidos con un desequilibrio de protones y neutrones tan grande.

La exposición al níquel metálico y sus compuestos solubles no debe superar los 0,05 mg/cm medidos en niveles de níquel equivalente para una exposición laboral de ocho horas diarias y cuarenta semanales. Los vapores y el polvo de sulfato de níquel se sospecha que sean cancerígenos.

El carbonilo de níquel (Ni(CO)), generado durante el proceso de obtención del metal, es un gas extremadamente tóxico.

Las personas sensibilizadas pueden manifestar alergias al níquel. La cantidad de níquel admisible en productos que puedan entrar en contacto con la piel está regulada en la Unión Europea; a pesar de ello, la revista "Nature" publicó en 2002 un artículo en el que investigadores afirmaban haber encontrado en monedas de 1 y 2 euros niveles superiores a los permitidos, se cree que debido a una reacción galvánica.

Aproximadamente el 65 % del níquel consumido se emplea en la fabricación de acero inoxidable austenítico y otro 12 % en superaleaciones de níquel. El restante 23 % se reparte entre otras aleaciones, baterías recargables, catálisis, acuñación de moneda, recubrimientos metálicos y fundición:




</doc>
<doc id="21877" url="https://es.wikipedia.org/wiki?curid=21877" title="Spirit of St. Louis">
Spirit of St. Louis

Spirit of St. Louis "(El Espíritu de San Luis)" es el nombre del aeroplano con el que el piloto Charles Lindbergh cruzó el Atlántico en un vuelo en solitario sin escalas de Nueva York a París en mayo de 1927. Aunque sea conocido como el primer vuelo que atravesó el Atlántico, la realidad es que ya en 1922, los portugueses Gago Coutinho y Sacadura Cabral, habían hecho la ruta Lisboa-Río de Janeiro. Cuatro años más tarde, en 1926 el español Ramón Franco realizó un vuelo con escalas entre Palos de la Frontera y Buenos Aires, a bordo del Plus Ultra.

El avión fue fabricado en San Diego, California. Los industriales que financiaron el vuelo transatlántico eran hombres de Saint Louis, por lo que se le dio al avión el nombre de esa ciudad. Lindbergh participó en el diseño y en la construcción del aparato, modelo Ryan NYP (una adaptación del Ryan M-2), siendo un proyecto de Donald Hall. En sólo dos meses había terminado la fabricación del "Spirit of St. Louis". Se trataba de un avión con alas de implantación alta, con estructura de madera. El fuselaje era de tubos de acero, y el revestimiento exterior era de tela.

Contrariamente a los aviones de los competidores de Lindbergh, su avión era un monomotor equipado con un Wright Whirlwind J-5C de 223 C.V. Lindbergh opinaba que era mejor disponer de un solo motor, ya que a un avión con carga máxima un segundo motor tampoco podría mantenerlo en el aire si fallaba el primero. Además, un avión con dos o tres motores era más propenso a tener fallos en alguno de ellos. 

El depósito de combustible principal quedó alojado delante del puesto de mando, por razones de centrado del peso, como en todos los aviones, es decir que independientemente de la cantidad de combustible restante la posición del centro de gravedad no varía. De esta forma,al aumentar su capacidad al máximo, Lindbergh sacrificó la visibilidad hacia adelante, la cual quedó reducida a lo que podía ver a través de un periscopio que tenía delante. La capacidad total de combustible fue de 1.705 L, lo que significó un peso superior a la mitad del peso total del avión, que fue de 2.380 kg.

El avión fue diseñado en todas sus piezas de forma que ofreciese la mínima resistencia al aire y que su peso fuese también lo más bajo posible. Para ello se prescindió de numerosos elementos que en otros aviones eran usuales, como, por ejemplo, el instrumento indicador del nivel de combustible y el aparato de radio. Incluso el asiento del piloto fue sustituido por una ligera silla de mimbre.

Despegue: de San Diego en vuelo hacia Nueva York.
Durante este trayecto bate un récord de velocidad. 
El 20 de mayo de 1927 salió de un campo de aviación en Nueva York, "Roosevelt Field", en dirección a París. Allí llegó 33 h y 32 min más tarde, a las 10:22, aterrizando en el aeródromo de "Le Bourget", donde fue recibido con un enorme entusiasmo por miles de personas. Lindbergh había volado sin interrupción 3.600 millas.

Después de volar a Bélgica e Inglaterra, el Spirit of St. Louis viajó de vuelta a Estados Unidos en el crucero de guerra USS Memphis (CL-13) (junio 1927). Tras los festejos y numerosos vuelos promocionales por Estados Unidos, Centroamérica, Colombia y Venezuela y finalmente el Caribe, fue entregado al Instituto Smithsoniano para su conservación el 30 de abril de 1928. 
El Spirit of St. Louis se encuentra actualmente expuesto en el Museo del Aire y del Espacio, Washington, DC, EE. UU. Numerosas copias se han hecho a lo largo de la historia para vuelos conmemorativos y para exhibición en museos.



</doc>
<doc id="21878" url="https://es.wikipedia.org/wiki?curid=21878" title="Zinc">
Zinc

El zinc (del alemán "Zink"), también escrito cinc, es un elemento químico esencial de número atómico 30 y símbolo Zn, situado en el grupo 12 de la tabla periódica de los elementos.

La etimología de zinc parece que viene del alemán "Zink", este del "Zinken" (en español pico, diente), para indicar el aspecto con filos dentados del mineral calamina, luego fue asumido para el metal obtenido a partir de él, aunque otras fuentes consideran que viene de la palabra persa para "piedra".

En el español, las variantes gráficas «zinc» y «cinc» son ambas aceptadas como válidas. Sin embargo, la forma con "z", «zinc», es la más coherente con el origen de la palabra y, por tanto, con su símbolo químico internacional (Zn).

El zinc es un metal, a veces clasificado como metal de transición aunque estrictamente no lo sea, ya que tanto el metal como su ion positivo presentan el conjunto orbital completo. Este elemento presenta cierto parecido con el magnesio, y con el cadmio de su grupo, pero del mercurio se aparta mucho por las singulares propiedades físicas y químicas de este (contracción lantánida y potentes efectos relativistas sobre orbitales de enlace). Es el 23.º elemento más abundante en la Tierra y una de sus aplicaciones más importantes es el galvanizado del acero.

Es un metal de color blanco azulado que arde en el aire con llama verde azulada. El aire seco no le ataca pero en presencia de humedad se forma una capa superficial de óxido o carbonato básico que aísla al metal y lo protege de la corrosión. Prácticamente el único estado de oxidación que presenta es el +2. En el año 2004 se publicó en la revista "Science" el primer y único compuesto conocido de zinc en estado de oxidación +1, basado en un complejo organometálico con el ligando pentametilciclopentadieno. Reacciona con ácidos no oxidantes pasando al estado de oxidación +2 y liberando dihidrógeno (antiguamente llamado hidrógeno) y puede disolverse en bases y ácido acético.

El metal presenta una gran resistencia a la deformación plástica en frío que disminuye en caliente, lo que obliga a laminarlo por encima de los 100 °C. No se puede endurecer por y presenta el fenómeno de fluencia a temperatura ambiente —al contrario que la mayoría de los metales y aleaciones— y pequeñas cargas el más importante.

Las aleaciones de zinc se han utilizado durante siglos —piezas de latón datadas en 1000-1500 a. C. se han encontrado en Canaán y otros objetos con contenidos de hasta el 87% de zinc han aparecido en la antigua región de Transilvania— sin embargo, por su bajo punto de fusión y reactividad química el metal tiende a evaporarse por lo que la verdadera naturaleza del metal no fue comprendida por los antiguos.

Se sabe que la fabricación de latón era conocida por los romanos hacia 30 a. C. Plinio y Dioscórides describen la obtención de "aurichalcum" (latón) por el procedimiento de calentar en un crisol una mezcla de "cadmia" (calamina) con cobre; el latón obtenido posteriormente era fundido o forjado para fabricar objetos.

La fundición y extracción de zinc impuro se llevó a cabo hacia el año 1000 en la India —en la obra "Rasarnava" (c. 1200) de autor desconocido se describe el procedimiento— y posteriormente en China y a finales del siglo XIV los indios conocían ya la existencia del zinc como metal distinto de los siete conocidos en la Antigüedad, el octavo metal. En 1597 Andreas Libavius describe una «peculiar clase de estaño» que había sido preparada en la India y llegó a sus manos en pequeña cantidad a través de un amigo; de sus descripciones se deduce que se trataba del zinc aunque no llegó a reconocerlo como el metal procedente de la calamina.

En occidente, hacia 1248, Alberto Magno describe la fabricación de latón en Europa, y en el siglo XVI ya se conocía la existencia del metal. Georgius Agricola (1490-1555) observó en 1546 que podía rascarse un metal blanco condensado de las paredes de los hornos en los que se fundían minerales de zinc; añadiendo en sus notas que un metal similar denominado "zincum" se producía en Silesia. Paracelso fue el primero en sugerir que el "zincum" era un nuevo metal y que sus propiedades diferían de las de los metales conocidos sin dar, no obstante, ninguna indicación sobre su origen; en los escritos de Basilio Valentino se encuentran también menciones del "zincum". A pesar de ello, en tratados posteriores las frecuentes referencias al zinc, con sus distintos nombres, se refieren generalmente al mineral no al metal libre y en ocasiones se confunde con el bismuto.

Johann Kunkel en 1677 y poco más tarde Stahl en 1702 indican que al preparar el latón con el cobre y la calamina esta última se reduce previamente al estado de metal libre, el zinc, que fue aislado por el químico Anton von Swab en 1742 y por Andreas Marggraf en 1746, cuyo exhaustivo y metódico trabajo "Sobre el método de extracción del cinc de su mineral verdadero, la calamina" cimentó la metalurgia del zinc y su reputación como descubridor del metal.

En 1743 se fundó en Bristol el primer establecimiento para la fundición del metal a escala industrial pero su procedimiento quedó en secreto por lo que hubo que esperar 70 años hasta que Daniel Dony desarrollara un procedimiento industrial para la extracción del metal y se estableciera la primera fábrica en el continente europeo.

Tras el desarrollo de la técnica de flotación del sulfuro de zinc se desplazó a la calamina como mena principal. El método de flotación es hoy día empleado en la obtención de varios metales.

La principal aplicación del zinc —cerca del 50% del consumo anual— es el galvanizado del acero para protegerlo de la corrosión, protección efectiva incluso cuando se agrieta el recubrimiento ya que el zinc actúa como ánodo de sacrificio. Otros usos son estos:


El zinc es un elemento químico esencial para los seres humanos y ciertos animales. El cuerpo humano contiene alrededor de 40 mg de zinc por kg y muchas enzimas funcionan con su concurso: interviene en el metabolismo de proteínas y ácidos nucleicos, estimula la actividad de aproximadamente 300 enzimas diferentes, colabora en el buen funcionamiento del sistema inmunitario, es necesario para la cicatrización de las heridas, interviene en las percepciones del gusto y el olfato y en la síntesis del ADN. El metal se encuentra en la insulina, las proteínas dedo de zinc ("zinc finger") y diversas enzimas como la superóxido dismutasa.

Hay 2-4 gramos de zinc distribuidos en todo el cuerpo humano. La mayoría del zinc se encuentra en el cerebro, los músculos, los huesos, el riñón y el hígado, con las concentraciones más altas en la próstata y las partes del ojo. El semen es particularmente rico en zinc, siendo un factor clave en la correcta función de la glándula prostática y en el crecimiento de los órganos reproductivos.

El zinc aumenta la testosterona en sangre indirectamente, funcionando como coenzima en el metabolismo de las hormonas masculinas por medio de su formación a través de la hormona luteinizante (LH), que estimula las células de Leydig. También previene que la testosterona se degrade en estrógeno por medio de la enzima aromatasa.

En el cerebro, el zinc se almacena en determinadas vesículas sinápticas mediante neuronas glutamatérgicas y puede "modular la excitabilidad del cerebro". Desempeña un papel clave en la plasticidad sináptica y por lo tanto en el aprendizaje. Sin embargo, ha sido llamado el "caballo oscuro del cerebro" (“the brain's dark horse”) ya que también puede comportarse como una neurotoxina, lo que sugiere que la adecuada homeostasis del cinc desempeña un papel fundamental en el funcionamiento normal del cerebro y del sistema nervioso central.

La deficiencia de zinc perjudica al sistema inmunitario, genera retardo en el crecimiento y puede producir pérdida del cabello, diarrea, impotencia, lesiones oculares y de piel, pérdida de apetito, pérdida de peso, tardanza en la cicatrización de las heridas y anomalías en el sentido del olfato y el gusto. Las causas que pueden provocar una deficiencia de zinc son la deficiente ingesta y la mala absorción del mineral —caso de alcoholismo que favorece su eliminación en la orina o dietas vegetarianas en las que la absorción de zinc es un 50% menor que de las carnes— o por su excesiva eliminación debido a desórdenes digestivos.

La carencia de zinc en los períodos de rápido crecimiento afecta negativamente el desarrollo cognitivo, cerebral y sexual.

Según el CSIC, este elemento tiene un papel de suma importancia en las funciones mediadas por neurotransmisores, actuando como modulador de la excitabilidad neuronal. En este sentido la deficiencia de zinc puede causar trastornos del humor y neurodegeneración, como depresión y Alzheimer.

La disminución de los niveles de LH y testosterona circulantes a causa de la deficiencia de cinc afecta negativamente la actividad de las células de Leydig.

El exceso de zinc, denominado hipercincemia, se ha asociado con bajos niveles de cobre, alteraciones en la función del hierro, disminución de la función inmunológica y de los niveles del colesterol bueno (HDL), vómitos, diarrea, daños a los riñones y depresión mental.

El zinc se encuentra en diversos alimentos, especialmente en aquellos ricos en proteínas, ya que el zinc queda retenido entre las mismas, como ostras, carnes rojas, carne de cerdo, cordero, aves de corral, algunos pescados y mariscos. Otras fuentes ricas en zinc son las habas, nueces, granos enteros y levadura. Las frutas y las verduras no son habitualmente buenas fuentes, porque el zinc en las proteínas vegetales no tiene tanta biodisponibilidad para el ser humano como el zinc de las proteínas animales.

Los cereales integrales, las legumbres y los frutos secos son ricos en fitatos, que son conocidos bloqueantes del zinc. La biodisponibilidad del zinc en el pan leudado es mayor que en los productos sin levadura, ya que el proceso de leudado activa la fitasa, que descompone el ácido fítico. El resultado es que mejora la biodisponibilidad del zinc.

La ingesta diaria recomendada de zinc ronda los 11-20 mg para hombres adultos, menor para bebés, niños, adolescentes y mujeres adultas (por su menor peso corporal) y algo mayor para mujeres embarazadas y durante la lactancia. La absorción del zinc es muy variable (entre un 20 y un 30%), y aumenta cuando el consumo es bajo o cuando aumentan las necesidades.

Aunque los adultos vegetarianos tienen a menudo una ingesta menor que la de los omnívoros, parece que en general presentan un nivel adecuado de zinc, como se refleja en los niveles de zinc en sangre y en los estudios sobre el balance de zinc. Se ha visto que a lo largo del tiempo se produce una adaptación a la dieta vegetariana, dando como resultado una mejor utilización de este elemento. Los hombres vegetarianos y no vegetarianos tienen un consumo de zinc similar mientras que las mujeres vegetarianas presentan un consumo significativamente más bajo. Incluso aunque estas últimas consuman menos zinc, sus niveles son similares a los niveles de las mujeres omnívoras. Las personas de la tercera edad, independientemente de su tipo de dieta, tienen un mayor riesgo de deficiencia de zinc.

Como el zinc, en general, se absorbe de manera menos efectiva a partir de una dieta vegetariana que de una dieta omnívora, es importante que los vegetarianos seleccionen alimentos ricos en zinc.

La producción mundial de zinc durante 2011 alcanzó un total de 12,40 millones de toneladas métricas. El principal país productor es China, seguido por Perú y Australia.

El zinc es el 23.º elemento más abundante en la corteza terrestre. Las menas más ricas contienen cerca de un 10% de hierro y entre el 40 y el 50% de zinc. Los minerales de los que se extrae son el sulfuro de zinc, conocido como esfalerita (y también como blenda, término que actualmente se considera obsoleto), la smithsonita (carbonato) y la hemimorfita, (silicato) , que reciben en conjunto el nombre industrial de "calaminas", y la franklinita, (óxido).

De acuerdo con la información recogida en el informe anual del United States Geological Survey (USGS), las estimaciones señalan que las reservas económicamente explotables de zinc en 2011 a nivel mundial alcanzarían las 250 millones de toneladas métricas, repartidas entre China, Estados Unidos, Perú y Kazajistán.
Las reservas conocidas (incluyendo aquellas cuya explotación hoy día no es rentable) rozan los 2000 millones de toneladas.

La producción del zinc comienza con la extracción del mineral, que puede realizarse tanto a cielo abierto como en yacimientos subterráneos. Los minerales extraídos se trituran con posterioridad y se someten a un proceso de flotación para obtener el concentrado.

Los minerales con altos contenidos de hierro se tratan por vía seca: primeramente se tuesta el concentrado para transformar el sulfuro en óxido, que recibe la denominación de "calcina", y a continuación se reduce este con el carbono contenido en el carbón, obteniendo el metal (el agente reductor es en la práctica el monóxido de carbono formado). Las reacciones en ambas etapas son:

Otra forma más sencilla y económica de reducir el óxido de zinc es con carbón. Se colocan los dos moles óxido de zinc (ZnO), y un mol de carbono (C), en un recipiente al vacío para evitar que el metal se incendie con el aire en el momento de purificarse, dando como resultado nuevamente óxido de cinc. En esta etapa, la reducción del óxido de zinc, se expresa de la siguiente manera:

2 ZnO + C → 2 Zn + CO

Por vía húmeda primeramente se realiza el tueste obteniendo el óxido que se lixivia con ácido sulfúrico diluido; las lejías obtenidas se purifican separando las distintas fases presentes. El sulfato de zinc se somete posteriormente a electrólisis con ánodo de plomo y cátodo de aluminio sobre el cual se deposita el zinc formando placas de algunos milímetros de espesor que se retiran cada cierto tiempo. Los "cátodos" obtenidos se funden y se cuela el metal para su comercialización.

Como subproductos se obtienen diferentes metales como mercurio, óxido de germanio, cadmio, oro, plata, cobre y plomo, en función de la composición de los minerales. El dióxido de azufre obtenido en la tostación del mineral se usa para producir ácido sulfúrico que se reutiliza en el lixiviado comercializando el excedente producido.

Los tipos de cinc obtenidos se clasifican según la norma ASTM en función de su pureza:

La norma EN 1179 considera cinco grados Z1 a Z5 con contenidos de cinc entre 99,995% y 98,5% y existen normas equivalentes en Japón y Australia. Para armonizar todas ellas, la Organización Internacional de Normalización publicó en 2004 la norma ISO 752 sobre clasificación y requisitos del cinc primario.

Las aleaciones más empleadas son las de aluminio (3,5-4,5%, Zamak; 11-13%, Zn-Al-Cu-Mg; 22%, Prestal, aleación que presenta superplasticidad) y cobre (alrededor del 1%) que mejoran las características mecánicas del cinc y su aptitud al moldeo.

Es componente minoritario en aleaciones diversas, principalmente de cobre como latones (3 a 45% de cinc), alpacas (Cu-Ni-Zn) y bronces (Cu-Sn) de moldeo.

El óxido de zinc es el más conocido y utilizado industrialmente, especialmente como base de pigmentos blancos para pintura, pero también en la industria del caucho y en cremas solares. Otros compuestos importantes son: sulfato de zinc (nutriente agrícola y uso en minería), cloruro de zinc (desodorantes) y sulfuro de zinc (pinturas luminiscentes).

El zinc existente en la naturaleza está formado por cuatro isótopos estables, Zn-64 (48,6%), Zn-66, Zn-67, y Zn-68. Se han caracterizado 22 radioisótopos de los que los más estables son Zn-65 y Zn-72 con periodos de semidesintegración de 244,26 días y 46,5 horas respectivamente; el resto de isótopos radiactivos tienen periodos de semidesintegración menores que 14 horas y la mayoría menores que un segundo. El cinc tiene cuatro estados metaestables.

El zinc metal no está considerado como tóxico, pero sí algunos de sus compuestos como el óxido y el sulfuro.
En la década de los 40 se observó que en la superficie del acero galvanizado se forman con el tiempo "bigotes de zinc" ("zinc whiskers") que pueden liberarse al ambiente provocando cortocircuitos y fallos en componentes electrónicos. Estos bigotes se forman tras un período de incubación que puede durar días o años y crecen a un ritmo del orden de 1 mm al año. El problema causado por estos bigotes se ha agudizado con el paso del tiempo por haberse construido las salas de ordenadores y equipos informáticos sobre suelos elevados, para facilitar el cableado, en las que era común el uso de acero galvanizado, tanto en la estructura portante como en la parte posterior de las baldosas. Las edades de dichas salas, en muchos casos de 20 o 30 años propician la existencia de pelos en cantidades y longitudes peligrosas susceptibles de provocar fallos informáticos. Además, la progresiva miniaturización de los equipos disminuye la longitud necesaria para provocar el fallo y los pequeños voltajes de funcionamiento impiden que se alcance la temperatura de fusión del metal, provocando fallos crónicos que pueden ser incluso intermitentes.






</doc>
<doc id="21881" url="https://es.wikipedia.org/wiki?curid=21881" title="Verano">
Verano

El verano o estío es una de las cuatro estaciones de las zonas templadas. Es la más cálida de ellas. Sigue a la primavera y precede al otoño.El verano se caracteriza porque los días son más largos y las noches más cortas. Astronómicamente, el solsticio de verano (alrededor del 21 de diciembre el austral y el 21 de junio el boreal) marca el comienzo de esta estación y el equinoccio de otoño (alrededor del 21 de marzo el austral y el 23 de septiembre el boreal) marca el término de esta estación y el comienzo del otoño.

En diversas culturas, las estaciones comienzan en diferentes fechas, basadas en fenómenos astronómicos o meteorológicos. Sin embargo, cuando el verano ocurre en el hemisferio sur es invierno en el hemisferio norte. Según se observe, el verano puede ser boreal, cuando ocurre en el hemisferio norte, o austral, cuando ocurre en el hemisferio sur.

Sin embargo, a veces, el verano se define como la totalidad de los meses de diciembre, enero y febrero en el hemisferio sur y como la totalidad de los meses de junio, julio y agosto en el hemisferio norte. En la zona intertropical a veces se emplea el término "verano" para referirse a la estación seca, e "invierno" para la estación lluviosa. En el verano se practican actividades tales como, ir más seguido a la playa, camping en zonas montañosas en el austral ya que cuenta con la mayor cantidad de playas y flora en el continente americano

En el número de las cuatro diosas de las estaciones existentes en la villa de Albaoi, el Estío (sinónimo de verano) está representado corriendo con una antorcha encendida en cada mano. En un sepulcro fuera de Roma, donde en estuco estaban representadas las Cuatro Estaciones, en una mano el Estío tenía una hoja de trébol. 

Entre las pinturas de Herculano (Italia) hay una figura vestida de amarillo con una azada de muchas puntas. Sobre la urna cineraria que representa las bodas de Tetis y Peleo, al verano se le representa más gallardamente vestido (provisto de una corona) que al invierno y al otoño. Se le designaba también por la caza del león. Se le pintaba igualmente con una túnica amarilla, con un manto azul celeste, color que indica la constante serenidad del cielo durante esta estación, sobre todo en los países cálidos. El amarillo indica la madurez de las mieses.

Los modernos la simbolizan por una joven vestida de amarillo coronada de espigas y portando una antorcha encendida. Otros representan al verano casi desnudo, coronado de espigas, sosteniendo en una mano el cuerno de la abundancia, rebosante de toda especie de granos y frutas, y en la otra una hoz.


</doc>
<doc id="21882" url="https://es.wikipedia.org/wiki?curid=21882" title="Otoño">
Otoño

El otoño es una de las cuatro estaciones del año de las zonas templadas. Sigue al verano y precede al invierno. Astronómicamente, comienza con el equinoccio de otoño (alrededor del 22 o 23 de septiembre en el hemisferio norte y del 20 o 21 de marzo en el hemisferio sur) y termina con el solsticio de invierno (alrededor del 21 de diciembre en el hemisferio norte y del 21 de junio en el hemisferio sur). En la zona intertropical inicia el 22 o 23 de septiembre hasta el 19 o 20 de marzo y en la zona intertropical va desde el 19 o 20 de marzo hasta el 22 o 23 de septiembre, abarcando seis meses.

Sin embargo, habitualmente se conoce como otoño al período que comprende los meses de septiembre, octubre y noviembre en el hemisferio norte y marzo, abril y mayo en el hemisferio sur.

En el , los cambios en las condiciones climáticas y atmosféricas causados por el calentamiento global han generado alteraciones en los cambios estacionarios, incluido el tránsito del verano al otoño. 

Su nombre proviene del latín «"autumnus»", palabra que se ha vinculado a la raíz «"augeo-"», «aumentar». De este modo, los etimologistas latinos explicaban la palabra como «"auctus" (participio pasado de "augeo") "annus»": el aumento o la plenitud del año. Puede ser comparada así con el término castellano «auge», que proviene de idéntica raíz. Otros autores, como Breyer y Ernout-Meillet, vinculan la palabra latina «"autumnus»" con la raíz etrusca «"autu"-», que implica la idea del cambio, y aparece también en el nombre de la divinidad etrusca Vertumno, quien —entre otras funciones— predecía el cambio de las estaciones.

Durante esta estación la temperatura comienza a descender. Las hojas de los árboles caducos cambian su color verde por tonos ocres, hasta que se secan y caen ayudadas por el viento que sopla con mayor fuerza. Este cambio de color se observa más claramente en diversas regiones del mundo, como América del Norte, el Este de Asia (incluyendo China, Corea y Japón), Europa, Venezuela, zonas centro, sur y austral de Chile, sur de Argentina, Australia oriental y la isla sur de Nueva Zelanda.

Canadá y Nueva Inglaterra (en Estados Unidos) son destinos muy populares para observar el follaje otoñal.

Durante el otoño se desarrollan numerosas festividades. Las más conocidas son Halloween y el Día de Acción de Gracias, muy populares en el ámbito anglosajón. En España y Latinoamérica septentrional el día más importante es el Día de Todos los Santos (día primero de noviembre) que, al igual que Halloween, era en su inicio una festividad de origen pagano vinculada al culto de los muertos. En países como México y más recientemente en Venezuela se festeja el Día de Muertos entre los días 28, 29, 30 y 31 de octubre y principalmente entre el 1 y 2 de noviembre, en respeto de los difuntos, donde se les recuerda con actividades como la quema de veladoras y de oraciones dedicadas a familiares y amigos que han fallecido.


</doc>
<doc id="21883" url="https://es.wikipedia.org/wiki?curid=21883" title="Invierno">
Invierno

El invierno es una de las cuatro estaciones de las zonas templadas. Sigue al otoño y precede a la primavera. 
Esta estación se caracteriza por días más cortos, noches más largas y temperaturas más bajas a medida que nos alejamos del Ecuador. En algunos países de la zona intertropical se utiliza el término de estación lluviosa para denominar a una época de mayor precipitación y pluviosidad.

La palabra "invierno" proviene del español antiguo "ivierno", y este del latín vulgar "hibernum", del latín "tempus hibernum", estación invernal.

Como las demás estaciones del año, el invierno es causado por la inclinación de 23,44 grados 
del eje terrestre sobre su plano orbital.

Desde un punto de vista astronómico, comienza con el solsticio de invierno, el día 22 de diciembre en el hemisferio norte y el 21 de junio en el hemisferio sur, y termina con el equinoccio de primavera, alrededor del 21 de marzo en el hemisferio norte y del 22 de septiembre en el hemisferio sur, variando las fechas levemente según el año. El hecho que la órbita de la Tierra sea elíptica, se traduce en una duración menor del invierno en el hemisferio norte y mayor respecto a este en el sur, ya que en julio se produce el afelio, durante el invierno austral, y en enero el perihelio durante el boreal. En resumen, el invierno dura aproximadamente cuatro días más en el hemisferio austral que en el boreal.

Desde una óptica meteorológica, en cambio, se suelen considerar invernales los meses enteros de diciembre, enero y febrero en el hemisferio norte y junio, julio y agosto en el hemisferio sur.

El invierno es la estación más fría del año, y sus características son inevitablemente definidas en contraste con las otras estaciones del año; ya que durante los días invernales las temperaturas son más bajas y hay menos horas de luz solar. Estas características se acentúan a medida que nos alejamos de los trópicos y nos acercamos a los círculos polares. 

En algunas regiones del planeta, según su latitud, altitud y determinadas condiciones meteorológicas, se puede observar la caída de nieve.

En los países ecuatoriales donde solo hay dos estaciones a lo largo del año, se le conoce como la “temporada de lluvias”.

En la mitología griega, Hades, dios del inframundo, rapta a la bella Perséfone para hacerla su esposa. Zeus le ordena a Hades que la devuelva y se la entregue a Deméter, diosa de la tierra y su madre. Sin embargo, Hades engaña a Perséfone y le hace comer semillas de granada, comida del inframundo que la obliga a quedarse allí para siempre. Deméter, sin su hija Perséfone no tiene felicidad por lo tanto no cuida a la tierra. Zeus, viendo que la tierra quedaba desolada, las plantas se secaban y morían, llega a un acuerdo para que Perséfone pase seis meses con Deméter y seis meses con Hades. Durante el tiempo en que su hija está con Hades, Deméter se entristece y provoca el otoño y el invierno.





</doc>
<doc id="21885" url="https://es.wikipedia.org/wiki?curid=21885" title="Tikal">
Tikal

Tikal (o Tik'al, de acuerdo con la ortografía maya moderna) es uno de los mayores yacimientos arqueológicos y centros urbanos de la civilización maya precolombina. Está situado en el municipio de Flores, en el departamento de Petén, en el territorio actual de la República de Guatemala y forma parte del parque nacional Tikal, que fue declarado Patrimonio de la Humanidad, por Unesco, en 1979. Según los glifos encontrados en el yacimiento, su nombre maya habría sido Yax Mutul.

Tikal fue la capital de un estado beligerante, que se convirtió en uno de los reinos más poderosos de los antiguos mayas.
Aunque la arquitectura monumental del sitio se remonta hasta el siglo  a. C., Tikal alcanzó su apogeo durante el Período Clásico, entre el 200 y el 900 d. C. Durante este tiempo, la ciudad dominó gran parte de la región maya, en el ámbito político, económico y militar; mantenía vínculos con otras regiones, a lo largo de Mesoamérica, incluso con la gran metrópoli de Teotihuacan, en el lejano Valle de México.

Después del Clásico Tardío, no se construyeron monumentos mayores.

Con una larga lista de gobernantes dinásticos, el descubrimiento de muchas de sus respectivas tumbas y el estudio de sus monumentos, templos y palacios, Tikal es probablemente la mejor comprendida de las grandes ciudades mayas de las tierras bajas de Mesoamérica.

El nombre Tikal puede ser derivación de las palabras "ti ak'al", en el idioma maya yucateco, que significa «en el pozo de agua». Aparentemente, el nombre fue aplicado por cazadores y viajeros de la región y se refería a una de las antiguas reservas de agua del sitio.
Una explicación alternativa sugiere que el nombre viene del idioma maya itzá y que significa «lugar de las voces», o «lugar de las lenguas».

Sin embargo, Tikal no es el antiguo nombre del sitio, sino más bien el nombre que se adoptó poco después de su redescubrimiento, en la década de 1840.
Las inscripciones glíficas en escritura maya, en las ruinas, se refieren a la antigua ciudad como Yax Mutal o Yax Mutul, cuyo significado es «primer mutal».
Es posible que Tikal fuese llamada así para distinguirla de Dos Pilas, que llegó a utilizar el mismo glifo emblema. Los gobernantes de la ciudad, querían distinguirse como la primera ciudad llevando este nombre.
El reino, en su conjunto, se llamaba Mutul, siendo la lectura del glifo emblema que se ve en la foto incluida. Su significado exacto no está claro, aunque algunos científicos piensan que hace referencia al reinado del "Ku'hul Ahaw", o máximo gobernante.

Tikal está ubicado a aproximadamente 64 km, al noreste de Flores y Santa Elena y aproximadamente 303 km, al norte de la ciudad de Guatemala. La ciudad se encuentra a 19 km, al sur de la antigua ciudad maya de Uaxactún, a 30 km, al noroeste de Yaxhá, a 100 km, al sureste de Calakmul, su gran rival del Período Clásico y a 85 km, al noroeste de El Caracol, el aliado de Calakmul, ahora en Belice.

La ciudad, que cubre un área de más de 16 km², ha sido completamente cartografiada e incluye alrededor de 3000 estructuras.
La topografía del lugar se compone de una serie de colinas de piedra caliza, elevándose encima de tierras pantanosas. La arquitectura principal del sitio se agrupa en zonas más elevadas, que son interconectadas por calzadas que atraviesan los pantanos.

Las ruinas se encuentran en medio de la selva tropical, en la cuenca del Petén, que formó la cuna de la civilización maya en las tierras bajas de Mesoamérica. La ciudad está ubicada en medio de suelos fértiles, con tierras elevadas y puede haber dominado la ruta comercial natural, que corre de este a oeste, a través de la península de Yucatán.

A pesar de ser una de las mayores ciudades mayas del Clásico, Tikal no tenía otras fuentes de agua, que no fuera el agua de lluvia, que se recogió y se almacenó en diez embalses. Los arqueólogos que trabajaron en Tikal, durante el siglo , restauraron uno de los antiguos depósitos de agua, para su propio uso.
La ausencia de fuentes, ríos y lagos en las cercanías de Tikal, pone de relieve un hecho prodigioso: la construcción de una gran ciudad, contando exclusivamente con entregas almacenadas de lluvias estacionales. Tikal prosperó con técnicas de agricultura intensiva, que eran mucho más avanzadas que los métodos de tala y quema originalmente teorizados por los arqueólogos. Sin embargo, la dependencia de las lluvias estacionales constituyó una vulnerabilidad, ante las sequías prolongadas y algunos científicos consideran que esta vulnerabilidad ha jugado un papel en el colapso maya.

La población de Tikal experimentó un crecimiento continuo, a partir del Período Preclásico (aproximadamente entre el 2000 a. C. y el 200 d. C.), alcanzando su pico en el Clásico Tardío, con un crecimiento rápido entre el 700 y el 830 d. C., seguido por un fuerte descenso.

Las estimaciones de población de la ciudad de Tikal varían, de 10 000, hasta más de 90 000 habitantes, siendo la cifra más probable la del extremo superior de este rango.
Debido al bajo contenido en sal de la dieta maya, se estima que Tikal tenía que importar 131 toneladas de sal cada año, con base en una estimación conservadora, de una población de 45 000 habitantes.

Para el área de 120 km² (que demarcaría un círculo con un diámetro de 21,9 km), que se ubica dentro del perímetro de los terraplenes de defensa, la población máxima se estima en 517 habitantes por km². Dentro de un diámetro de 24 km del centro del sitio, la población máxima se estima en 120 000 habitantes y la densidad de población se estima en 265 habitantes por km². Dentro de un diámetro de 50 km del centro del sitio, que incluye algunas ciudades satélite, la población máxima se estima en 425 000, con una densidad de 216 habitantes por km². Estas cifras de población son aún más impresionantes, por los extensos pantanos, que no eran aptos para la agricultura, ni la construcción de viviendas. Sin embargo, algunos arqueólogos, como David Webster, consideran que estas cifras son demasiado altas.

La línea dinástica de Tikal, fundada ya en el siglo , abarcó 800 años e incluyó al menos 33 gobernantes.

Existen huellas de una agricultura temprana en Tikal, que datan del preclásico medio, aproximadamente en el 1000 a. C.
En un chultún sellado, una cavidad subterránea en forma de botella, fue descubierto un escondite con cerámica maya, que data de alrededor del 700 al 400 a. C.

En el preclásico tardío, por primera vez alrededor del 400 al 300 a. C., ya se realizaron construcciones importantes en Tikal, incluyendo la construcción de pirámides y plataformas, aunque la ciudad estaba siendo eclipsada por otros sitios más poderosos, situados al norte, como El Mirador y Nakbé.

En aquel momento, Tikal era parte de la cultura Chikanel, que dominaba la zona central y norte de Mesoamérica, una región que comprendía toda la Península de Yucatán, incluso el norte y el este de Guatemala y el territorio de Belice.

Dos templos, que datan del Chikanel tardío, tenían paredes de mampostería, cuyas superestructuras pueden haber sido arcos mayas, aunque esto no ha sido probado. Uno de estos templos tenía elaboradas pinturas, en las paredes exteriores, que muestran figuras humanas sobre un fondo de figuras decorativas, pintado en amarillo, negro, rosado y rojo.

En el siglo d. C., aparecieron por primera vez sepulturas ricas y Tikal experimentó un florecimiento político y cultural, tras la declinación de sus poderosos vecinos en el norte.
A finales del Preclásico Tardío, el arte y la arquitectura de estilo Izapa, de la costa del Pacífico, comenzó a ejercer su influencia en Tikal, como lo demuestran los primeros murales en la ciudad y una escultura, rota, en la acrópolis.

El gobierno dinástico, un régimen común entre los mayas de las tierras bajas, estuvo fuertemente arraigado en Tikal. De acuerdo con registros glíficos posteriores, la dinastía fue fundada por Yax-Moch-Xoc, posiblemente en el siglo .
Al inicio del Clásico Temprano, el poder en la región maya se concentró en Tikal y Calakmul, en el núcleo de la región central maya.

Es posible que Tikal se haya beneficiado de la caída de los grandes estados del Preclásico, como El Mirador. En el Clásico Temprano, Tikal se desarrolló rápidamente en la ciudad más dinámica de la región maya, estimulando el desarrollo de otras ciudades mayas cercanas.

Sin embargo, Tikal se encontraba frecuentemente en guerra y las inscripciones mencionan alianzas y conflictos con otros estados mayas, como Uaxactún, El Caracol, Naranjo y Calakmul. A finales del Clásico Temprano, Tikal fue derrotado por El Caracol, que sustituyó a Tikal, como principal centro del poder, en las tierras bajas mayas del sur.
Durante la primera parte del Clásico Temprano, también ocurrieron hostilidades entre Tikal y la ciudad vecina de Uaxactún, de las que en Uaxactún existen inscripciones, referentes a la captura de prisioneros de Tikal.

Parece haber ocurrido una ruptura en la sucesión masculina de la dinastía, en el 317 d. C., cuando la señora Une' B'alam llevó a cabo una ceremonia de fin de katún, al parecer como reina de la ciudad.

El decimocuarto rey de Tikal era Chak Tok Ich'aak (Gran Garra de Jaguar).
Chak Tok Ich'aak construyó un palacio, que fue conservado y ampliado por los gobernantes posteriores, hasta convertirse en el núcleo de la acrópolis central.
Poco se sabe acerca de Chak Tok Ich'aak, excepto que fue asesinado el 14 de enero del 378 d. C. El mismo día, Siyah K'ak' (‘Nace el Fuego’) llegó desde el oeste, después de pasar por El Perú, un sitio al oeste de Tikal, el 8 de enero.
Las inscripciones de la estela 31, se refieren a él como «Señor del Occidente».
Siyah K’ak’ fue, probablemente, un general extranjero, que servía a una figura representada por un glifo atípico para los mayas, compuesto de un lanzadardos, en combinación con un búho, un glifo que se conoce de la gran metrópoli de Teotihuacan, en el distante Valle de México. El Búho lanzadardos, incluso, puede haber sido el gobernante de Teotihuacan. Estos eventos registrados, sugieren que Siyah K'ak' lideró una invasión de Teotihuacan, que derrotó al rey nativo de Tikal, que fue capturado y ejecutado de inmediato.
Siyah K'ak' parece haber recibido el apoyo de una poderosa facción política, en Tikal mismo. Más o menos coincidiendo con esa conquista, un grupo de indígenas de Teotihuacan residía cerca del complejo Mundo Perdido, según parece.
También ejerció el control sobre otras ciudades en la zona, como Uaxactún, donde se convirtió en rey, pero no tomó el trono de Tikal para sí mismo.
En el curso de un año, el hijo de Búho lanzadardos, Yax Nuun Ayiin I (primer cocodrilo), fue instalado como el décimo rey de Tikal, mientras todavía era un niño.
Su reino duró 47 años y Tikal siguió siendo vasallo de Siyah K'ak', durante el tiempo que este vivió. Parece probable que Yax Nuun Ayiin I se casó con una de las esposas pre-existentes, de la derrotada dinastía de Tikal, con el propósito de legitimar el derecho de gobernar de su hijo, Siyaj Chan K'awiil II.

Río Azul, un sitio pequeño, a 100 kilómetros al noreste de Tikal, fue conquistado por este, durante el reinado de Yax Nuun Ayiin I. El sitio se convirtió en un puesto de avanzada de Tikal, protegiéndola de las ciudades hostiles en el norte y también se convirtió en un vínculo para el comercio con el Caribe.

A pesar de que los nuevos gobernantes de Tikal eran extranjeros, sus descendientes se adaptaron rápidamente a la cultura maya. Tikal se convirtió en el principal aliado y socio comercial de Teotihuacan, en las tierras bajas mayas. Después de su conquista por Teotihuacan, Tikal rápidamente dominó el norte y el este del Petén. Uaxactún, junto con los pueblos más pequeños de la región, fueron absorbidos en el reino de Tikal. Otros sitios, como Bejucal y Motul de San José, cerca del lago Petén Itzá, se convirtieron en vasallos de su vecino más poderoso en el norte. A mediados del siglo , Tikal tenía un territorio núcleo de, por lo menos, 25 kilómetros en todas las direcciones.

Alrededor del siglo , un impresionante sistema de fortificaciones, compuesto de zanjas y construcciones de tierra, fue construido a lo largo de la periferia norte de la zona interior de Tikal, uniéndose a las defensas naturales, proporcionadas por largas áreas pantanosas, situadas al este y al oeste de la ciudad. Fortificaciones adicionales fueron, probablemente, construidas en el sur, cercando un área de, aproximadamente, 120 kilómetros cuadrados. Estas defensas dieron protección a la población nuclear de Tikal y a sus recursos agrícolas.
Sin embargo, investigaciones recientes sugieren que las obras de tierra pueden haber sido parte de un sistema de recolección de agua, en lugar de tener una finalidad defensiva.

En el siglo , el poder de Tikal se expandió hacia el sur, hasta incorporar a la ciudad de Xukpi (actual Copán), cuyo fundador, K'inich Yax K'uk' Mo', tenía vínculos con Tikal.
Xukpi no se encontraba en una región étnicamente maya y la fundación de la dinastía de Xukpi, probablemente, implicaba la intervención directa de Tikal.
K'inich Yax K'uk' Mo' llegó a Xukpi (Copán) en diciembre de 426 y el análisis de los huesos de sus restos demuestra que pasó su infancia y juventud en Tikal.
Una persona conocida como Ajaw K'uk' Mo' (señor K'uk' Mo') es mencionada en un texto temprano de Tikal y bien puede ser la misma persona.
Su tumba tenía características de Teotihuacan y en retratos posteriores fue representado, vestido con el traje guerrero de Teotihuacan. Textos glíficos se refieren a él como «Señor del Oeste», al igual que Siyah K'ak'.
Al mismo tiempo, a finales de 426, Copán fundó el sitio cercano de Quiriguá, posiblemente patrocinado por Tikal.
La fundación de ambos centros puede haber sido parte de un esfuerzo para imponer la autoridad de Tikal, en la parte sureste de la región maya.
La interacción entre estos sitios y Tikal fue intensa, durante los siguiente tres siglos.

En el siglo  surgió una rivalidad duradera entre Tikal y Calakmul, con cada una de las dos ciudades formando su propia red de alianzas mutuamente hostiles, en lo que se ha descrito como una guerra de larga duración entre las dos superpotencias mayas. Los reyes de las dos capitales adoptaron el título "Kaloomte"', un término que no ha sido traducido con precisión, pero que tiene un significado semejante a Gran Rey.

A principios del siglo , hubo otra reina como gobernante de la ciudad, únicamente conocida como la Señora de Tikal, que era probablemente una hija de Chak Tok Ich'aak II. Parece que nunca gobernó en su propio derecho, ya que fue asociada con cogobernantes masculinos. El primero de ellos fue Kaloomte' B'alam, quien parece haber tenido una larga carrera como general de Tikal, antes de convertirse en corregente, siendo el decimonoveno en la secuencia dinástica. La Señora de Tikal no parece haber sido contabilizada en la numeración dinástica. Al parecer, fue posteriormente emparejada con el señor Garra de Pájaro, del que se presume representa el vigésimo gobernante.

A mediados del siglo , El Caracol parece haberse aliado con Calakmul, logrando derrotar a Tikal y marcando el cierre del Clásico Temprano.
El «hiato de Tikal» se refiere a un período comprendido entre finales del siglo  y finales del siglo , en el que se registró un descenso en la escritura de inscripciones y una reducción de la construcción a gran escala, en Tikal. En la segunda mitad del siglo , la ciudad fue afectada por una grave crisis, en la que no fueron erigidas nuevas estelas y se experimentó una deliberada y amplia mutilación de las esculturas públicas.
Este hiato (o receso) en la actividad en Tikal, quedó sin explicación durante mucho tiempo, hasta que se determinó, mediante desciframientos epigráficos posteriores, que el receso fue impulsado por la completa derrota de Tikal, por los estados aliados de Calakmul y El Caracol, en el año 562, una derrota que parece haber dado lugar a la captura y el sacrificio del rey de Tikal.
El muy erosionado Altar 21, en El Caracol, describe cómo Tikal sufrió esta desastrosa derrota, en una guerra mayor que tuvo lugar en el año 562. Parece que El Caracol era un aliado de Calakmul, en un conflicto más amplio entre esa última ciudad y Tikal, con la derrota de Tikal teniendo un impacto duradero sobre la ciudad.
Tikal no fue saqueada, sino que su poder e influencia fueron quebrados.
Después de su gran victoria, El Caracol creció rápidamente y una parte de la población de Tikal pudo haber sido trasladada, a la fuerza, allí. Durante el hiato, por lo menos uno de los gobernantes de Tikal se refugió con Janaab' Pakal de Palenque, otra de las víctimas de Calakmul.
Calakmul prosperó, durante el largo período de receso experimentado por Tikal.

El inicio del hiato de Tikal ha servido como un marcador con el que los arqueólogos frecuentemente sub-dividen el período Clásico de la cronología mesoamericana, en el Clásico Temprano y Clásico Tardío.

En el año 629, Tikal fundó Dos Pilas, un puesto de avanzada militar, a unos 110 kilómetros al sudoeste, con el fin de controlar el comercio a lo largo del curso del río La Pasión.
En 635, B'alaj Chan K'awiil fue instalado en el trono del nuevo puesto de avanzada, a la edad de cuatro años y durante muchos años sirvió como un leal vasallo de su hermano, el rey de Tikal.
Aproximadamente veinte años después, Dos Pilas fue atacado y derrotado por Calakmul. B'alaj Chan K'awiil fue capturado por el rey de Calakmul pero, en lugar de ser sacrificado, fue reinstalado en su trono, como vasallo de su antiguo enemigo y atacó a Tikal en 657, lo que obligó a Nuun Ujol Chaak, el entonces rey de Tikal, a temporalmente abandonar la ciudad.
Los dos primeros gobernantes de Dos Pilas siguieron utilizando el «mutal», el glifo emblema de Tikal y, probablemente, sentían que tenían un derecho legítimo al trono de Tikal mismo. Por alguna razón, B'alaj Chan K'awiil no fue instalado como el nuevo gobernante de Tikal y se quedó en Dos Pilas. Tikal contraatacó a Dos Pilas, en el año 672, obligando a B'alaj Chan K'awiil a exiliarse, durante cinco años.
Calakmul trató de cercar a Tikal dentro de un área dominada por sus aliados, como El Perú, Dos Pilas y El Caracol.

En 682, Jasaw Chan K'awiil erigió el primer monumento, fechado en Tikal en 120 años y se adjudicó el título de "Kaloomte"', poniendo así fin al hiato. Inició un programa de nueva construcción y revirtió la relación con Calakmul, cuando en 695 capturó al ajaw Yuknoom Yich'aak K'ahk', dejando al estado enemigo en una larga declinación, de la que nunca se recuperó. Tras esta derrota, Calakmul nunca más erigió un monumento celebrando alguna victoria militar.
Con esta derrota de Calakmul, se restauró la preeminencia de Tikal en la región maya central, pero nunca más en el suroeste del Petén, donde Dos Pilas mantuvo su presencia.

En el siglo , no hubo presencia activa de Teotihuacan en cualquier sitio maya y el centro de Teotihuacan había sido arrasado, hacia el 700 d. C. No obstante, el traje de guerra formal, ilustrando los monumentos, se mantuvo en el estilo de Teotihuacan.
Jasaw Chan K'awiil I y su heredero Yik'in Chan K'awiil continuaron con las hostilidades en contra de Calakmul y sus aliados e impusieron un control regional firme, sobre el área alrededor de Tikal, extendiéndose hasta el territorio alrededor del lago Petén Itzá. Estos dos gobernantes fueron responsables de gran parte de la impresionante arquitectura visible en la actualidad.

En 738, Quiriguá, un vasallo de Copán, el aliado clave de Tikal en el sur, cambió de lealtad, favoreciendo a Calakmul, derrotó a Copán y obtuvo su propia independencia.
Esto fue, aparentemente, el efecto de un esfuerzo consciente por parte de Calakmul, para lograr el colapso de los aliados al sur de Tikal.
Esto alteró el equilibrio de poder en el sur de la región maya y resultó en la declinación de Copán.

En el siglo , los gobernantes de Tikal recogieron monumentos de todas partes de la ciudad y los trasladaron al frente de la acrópolis norte.
A finales del siglo  y principios de siglo , se desaceleró la actividad de construcción en Tikal. Todavía se construyó arquitectura impresionante, pero son pocas las inscripciones glíficas que se refieren a los gobernantes posteriores.

En el siglo , la crisis del colapso maya del periodo clásico se extendía por toda la región, con una población en caída libre y una ciudad tras otra cayendo en el silencio.
Cada vez más, la guerra endémica en la región maya obligó a la población rural que sostuvo a Tikal, a concentrarse cerca de la ciudad misma, acelerando el uso de la agricultura intensiva y el correspondiente deterioro del medio ambiente.
La construcción continuó a principios del siglo, con la erección del Templo 3, el último de las pirámides importantes de la ciudad, y la erección de monumentos, para conmemorar el decimonoveno k'atun en 810.

El comienzo del décimo bak'tun, en 830, pasó sin celebración alguna y marca el comienzo de un hiato de 60 años, probablemente como resultado del colapso del control central de la ciudad.
Durante este receso, sitios satélites, que tradicionalmente quedaban bajo el control de Tikal, comenzaron a erigir sus propios monumentos, protagonizando los gobernantes locales y empezaron a utilizar el glifo emblema Mutul, mientras que Tikal, al parecer, carecía de autoridad, o de poder, para reprimir estas tentativas de ganar independencia.
En 849, Jewel K'awiil es mencionado en las escrituras de una estela de Ceibal, como el «divino Señor de Tikal» visitando esta ciudad, pero esta visita no es registrada en otros lugares y el poder de Tikal, una vez tan grande, era poco más que un recuerdo. Los sitios de Ixlu y Jimbal ahora habían heredado el glifo emblema Mutul, que antes era exclusivo de Tikal.

Como Tikal y sus alrededores alcanzaron su población máxima, el área se vio afectada por la deforestación, erosión y la pérdida de nutrientes, seguido de una rápida disminución de la población. Tikal y sus alrededores, aparentemente, perdieron la mayoría de su población entre 830 y 950 y la autoridad central pudo haberse colapsado rápidamente.
No hay mucha evidencia de que la ciudad de Tikal se viera afectada directamente por la guerra endémica, que afectó partes de la región maya durante el Clásico Terminal, aunque la afluencia de refugiados de la región de Petexbatún pudo haber exacerbado los problemas derivados de la insuficiencia de recursos disponibles en el medio ambiente.

En la segunda mitad del siglo , hubo un intento de reinstaurar el poder real en la muy reducida ciudad de Tikal, como lo demuestra una estela erigida en la Gran Plaza por Jasaw Chan K'awiil II, en el año 869. Este fue el último monumento erigido en Tikal antes de que la ciudad finalmente cayera en el silencio. Los antiguos satélites de Tikal, como Jimbal y Uaxactún, no duraron mucho más tiempo, erigiendo sus monumentos finales en 889.
A finales del siglo , la gran mayoría de la población de Tikal había abandonado la ciudad. Sus palacios fueron ocupados por ocupantes ilegales y se construyeron sencillas viviendas, con techo de paja, en las plazas ceremoniales de la ciudad. Los nuevos ocupantes bloquearon algunas entradas en las habitaciones de las estructuras monumentales del sitio y dejaron basura, que incluía una mezcla de residuos domésticos y artículos no utilitarios, tales como instrumentos musicales. Estos habitantes reutilizaron los monumentos para sus propios rituales, muy alejados de los de la dinastía real que los había levantado.
Algunos monumentos fueron dañados y algunos fueron trasladados a nuevos lugares. Antes del abandono final de la ciudad, había desaparecido todo el respeto por los antiguos gobernantes, ya que las tumbas de la necrópolis norte fueron exploradas en busca de jade y las más accesibles fueron saqueadas. Después del año 950, Tikal estaba casi desierta, a pesar de que una población remanente pudo haber permanecido en chozas perecederas, intercaladas entre las ruinas.
En el siglo  o , estos últimos habitantes también abandonaron la ciudad y la selva reclamó las ruinas durante los siguientes mil años.
Una parte de la población de Tikal pudo haber migrado a la región de los lagos del Petén, una zona que se mantuvo densamente poblada, a pesar de una caída en los niveles de población en la primera mitad del siglo .

La causa más probable del colapso de Tikal fue la sobrepoblación y la decadencia agraria. Tikal, con su antigua dinastía, había estado en la vanguardia de la vida cortesana, del arte y de la arquitectura durante más de mil años. Su caída fue un duro golpe al corazón de la civilización maya clásica.

Investigaciones de Kohler et coll. demostraron que la ciudad alcanzó al final de su apogeo niveles de desigualdad social y económica insostenibles, lo cual sin duda contribuyó a su colapso.

En 1525, el conquistador español Hernán Cortés pasó a pocos kilómetros de las ruinas de Tikal, pero no las mencionó en sus cartas.

Como sucede a menudo con grandes ruinas antiguas, el conocimiento del sitio nunca se perdió completamente en la región. Aparentemente la población de la región nunca se olvidó de Tikal y, en la década de 1840, guiaron expediciones guatemaltecas a las ruinas. Algunos relatos de segunda o tercera mano de Tikal aparecieron en prensa, a partir del siglo  y fueron seguidos por los escritos de John Lloyd Stephens, en el siglo . (Durante sus viajes, de 1839-1840, en la región, Stephens y Frederick Catherwood, su ilustrador, escucharon rumores de una ciudad perdida, con edificios blancos, cuyas partes superiores dominaron la selva).

Debido a la lejanía del sitio, ningún explorador visitó las ruinas de Tikal hasta que Modesto Méndez y Ambrosio Tut, respectivamente el corregidor y el gobernador de Petén, las visitaron en 1848, junto con Vicente Díaz, Bernabé Castellanos y el maestro Eusebio Lara, quien los acompañó, para elaborar las primeras ilustraciones de los monumentos. En el último párrafo del infome que remitió al gobierno de Carrera, escribió: «Yo debo de cumplir con mi deber, pues me sería sensible que otros curiosos extranjeros vengan a dar publicidad a todos los objetos que estoy viendo y palpando. Vengan en hora buena esos viajeros con mayores posibilidades y facultades intelectuales, hagan excavaciones al pie de las estatuas, rompan los palacios y saquen las curiosidades y tesoros que no podrán llevar, jamás, sin el debido permiso; pero nunca podrán nulificar, ni eclipsar el lugar que me corresponde, al haber sido el primero en descubrir estas ruinas; sin gravar los fondos públicos, les abrí camino, y tuve el honor de comunicar al supremo gobierno de la república, cuanto interesante y superior se encuentra en la capital de este imperio; sin miras de interés personal o particular, únicamente satisfecho y persuadido que mi persona y cortos bienes pertenecen a la patria, al gobierno y a mis hijos».

En 1853, tras la publicación del diario de Méndez en la Gaceta de Guatemala, se dio a conocer el redescubrimiento a la comunidad científica, mediante una publicación de la Academia de Ciencias de Berlín.>

A finales del siglo  y principios del siglo , varias otras expediciones siguieron, para profundizar las investigaciones, incluyendo la expedición de Alfred P. Maudslay en 1881-82 y los arqueólogos pioneros comenzaron a limpiar, dibujar mapas y registrar las ruinas, en la década de 1880.
En 1951, una pequeña pista de aterrizaje fue construida cerca de las ruinas, a las que previamente solo se podía acceder tras un viaje de varios días por la selva, a pie, o en mulas.
En 1956, el proyecto Tikal comenzó a dibujar mapas de la ciudad, en una escala nunca antes vista en la región maya. De 1956, a 1970, excavaciones arqueológicas importantes fueron realizadas por el Proyecto Tikal de la Universidad de Pensilvania; mapearon la mayor parte del sitio y excavaron y restauraron muchas de las estructuras.
De 1957 a 1969, las excavaciones dirigidas por Edwin M. Shook y más tarde por William R. Coe, de la Universidad de Pensilvania, se enfocaron en la Acrópolis Norte y la Plaza Central. El Proyecto Tikal logró registrar más de 200 monumentos en el yacimiento.

En 1979, el gobierno guatemalteco inició un nuevo proyecto arqueológico en Tikal, que continuó hasta 1984.

Una ilustración del Templo I de Tikal fue incluida en el reverso del billete de 50 centavos del Quetzal guatemalteco.

Las ruinas de Tikal, como parte del parque nacional Tikal, fueron el primer sitio arqueológico en ser declarado Patrimonio de la Humanidad, en 1979 y, asimismo, el primer Patrimonio de la Humanidad mixto (ecológico y arqueológico) del mundo.
En la actualidad, Tikal, en medio de su propio parque nacional, se ha convertido en una atracción turística importante, y cuenta con un museo, construido en 1964.

Tikal ha sido parcialmente restaurada por la Universidad de Pensilvania y el Gobierno de Guatemala. Fue una de las ciudades mayas más importantes del periodo Clásico y una de las más grandes del continente americano.
La arquitectura de la antigua ciudad está construida de piedra caliza e incluye los restos de los templos, que se elevan más de 70 metros, grandes palacios reales, además de una serie de pirámides menores, palacios, residencias, edificios administrativos, plataformas y monumentos de piedra con inscripciones.
Hay, incluso, un edificio con barras de madera en las ventanas y puertas, que parecía haber sido una cárcel. También hay siete pistas para jugar el juego de pelota mesoamericano, incluyendo un conjunto de tres pistas, en la Plaza de los Siete Templos, una característica única en Mesoamérica.

La piedra caliza utilizada para la construcción fue extraída de canteras en el lugar mismo. Las depresiones que se formaron por la extracción de la piedra fueron recubiertas e impermeabilizadas para utilizarlas como depósitos de agua o embalses, junto con algunas depresiones naturales impermeabilizadas. Las plazas principales, cuya superficie estaba revertida de estuco, fueron establecidas en un gradiente para canalizar el agua de lluvia, en un sistema de canales que alimentaron los embalses

La zona residencial de Tikal cubre una superficie de aproximadamente 60 km², la cual, en gran parte, aún no ha sido limpiada, mapeada, o excavada. En la década de 1960 se descubrió un extenso conjunto de terraplenes, cercando Tikal con una zanja de 6 metros de ancho detrás de una muralla.
Puede haber encerrado un área de unos 125 km² (véase abajo). Las estimaciones de población ponen el tamaño demográfico del sitio, entre 10.000 y 90.000 habitantes y, posiblemente, hasta 425.000 habitantes, cuando se incluye el área circundante. Recientemente, la exploración de los terraplenes de defensa ha demostrado que su magnitud es muy variable y que en muchos lugares es intrascendente, como un elemento defensivo. Además, algunas partes de los terraplenes están integradas en un sistema de canales. El conjunto de terraplenes de Tikal varía de manera significativa, en la cobertura de lo que se propuso originalmente, y parece ser mucho más complejo y multifacético de lo que se pensaba, originalmente.

A finales del Clásico Tardío, una red de "sacbéob" (calzadas), con una longitud de varios kilómetros, atravesó el núcleo urbano, vinculando las diferentes partes de la ciudad. Estas calzadas eran anchas y fueron construidas de piedra caliza y yeso. No solo sirvieron de vía de comunicación durante la época de lluvias, sino también de diques.
Han sido nombradas, en honor de los primeros exploradores y arqueólogos, las calzadas Maler, Maudslay, Tozzer y Méndez.

La calzada Maler corre al norte de la ciudad, por atrás del Templo I, hasta el Grupo H. Un gran bajorrelieve, tallado en roca caliza, se encuentra a lo largo de una parte de la calzada, justo al sur del Grupo H. Data del Clásico Tardío y muestra a dos prisioneros atados.

La calzada Maudsley corre al noreste, sobre 0,8 km, del Templo IV, al Grupo H.

La calzada Méndez corre al sureste, de la Plaza del Oriente, al Templo VI, sobre una distancia de aproximadamente 1,3 km.

La calzada Tozzer corre al oeste, de la Gran Plaza, al Templo IV.

La Gran Plaza está ubicada en el centro de la ciudad, flanqueada por dos grandes templos piramidales a sus lados este y oeste. Al norte, está bordeada por la Acrópolis Norte y, en el sur, por la Acrópolis Central.

La Acrópolis Central es un complejo de palacios, ubicado justo al sur de la Gran Plaza.

La Acrópolis Norte, junto a la Gran Plaza, inmediatamente al sur, es uno de los conjuntos arquitectónicos más estudiados de la región maya. El Proyecto Tikal excavó una larga zanja en todo el complejo, estudiando a fondo la historia de la construcción. Es un conjunto complejo, cuya construcción se inició en el período Preclásico, alrededor del 350 a. C. Se convirtió en un complejo funerario de la dinastía gobernante de la época clásica, con cada entierro real añadiendo nuevos templos, en la parte superior de las estructuras más antiguas. Después del año 400 d. C., se agregó una fila de altas pirámides a la antigua plataforma Norte, que mide 100 por 80 metros, escondiéndola gradualmente de la vista. Ocho templos piramidales fueron construidos en el siglo . Cada uno de ellos tenía una elaborada crestería y una escalinata, flanqueada por máscaras de los dioses. Hacia el siglo , se habían erigido 43 estelas y 30 altares, en la Acrópolis Norte. 18 de estos monumentos fueron tallados con Escritura maya y retratos reales. La Acrópolis Norte continuó recibiendo los entierros, en el período Posclásico.
La Acrópolis Sur se encuentra junto al Templo V. Se construyó sobre una gran plataforma de base que cubre un área de más de 20 000 metros cuadrados.

La Plaza de los Siete Templos se encuentra al oeste de la Acrópolis Sur. Su límite oriental está bordeado por una serie de templos casi idénticos, por palacios en los lados sur y oeste, y por una inusual triple pista de juego de pelota en el lado norte.>

El Conjunto G se encuentra justo al sur de la calzada Méndez. El complejo data del Clásico Tardío y se compone de estructuras tipo palacio y es uno de los grupos más grandes de su tipo en Tikal. Tiene dos pisos, pero la mayoría de las habitaciones están en la planta baja, un total de 29 cuartos abovedados. Los restos de dos habitaciones más, pertenecen a la planta superior. Una de las entradas del conjunto fue marcada por una gigantesca máscara.

El Conjunto H se centra en una larga plaza, al norte de la Gran Plaza. Está bordeado por templos que datan del Clásico Tardío.
Hay nueve complejos de pirámides gemelas en Tikal, una de las cuales se desmanteló por completo, en tiempos antiguos y algunas otras fueron parcialmente destruidas. Varían en tamaño, pero todas consisten de dos pirámides, una frente a otra, en un eje este-oeste.
Estas pirámides son aplanadas y tienen escaleras en los cuatro lados. Una fila de estelas lisas se encuentra inmediatamente al oeste de la pirámide oriental y otra al norte de las pirámides. Ubicada a una misma distancia de las pirámides, se encuentra una serie de pares de estructuras, compuestas de un altar y una estela esculpida. En el lado sur de estos complejos hay un largo edificio abovedado, que contiene una sola habitación con nueve puertas.
El complejo entero fue construido en un mismo tiempo y estos complejos fueron construidos durante el Clásico Tardío, a intervalos de un "k'atun" (20 años).
El primer complejo de pirámides gemelas fue construido a principios del siglo , en la Plaza Oriental. Anteriormente se pensaba que estas pirámides gemelas eran exclusivas de Tikal, pero recientemente se han encontrado algunos otros ejemplos, en sitios como Yaxhá y Ixlu, lo que puede reflejar el grado de dominio político de Tikal, en el Clásico Tardío.

Conjunto Q es uno de los más grandes complejos de pirámides gemelas de Tikal. Fue construido por Yax Nuun Ayiin II, en el año 771, para marcar el fin del decimoséptimo K'atun.
La mayor parte del complejo ha sido restaurado y sus monumentos han sido reconstruidos.

Conjunto R es otro complejo de pirámides gemelas, que data de 790. Se encuentra cerca de la calzada Maler.

Tikal cuenta con miles de antiguas estructuras arquitectónicas, de las que solo una fracción ha sido excavada, después de décadas de trabajo arqueológico. Entre los edificios más prominentes se incluyen seis pirámides muy grandes, cada uno soportando la estructura de un templo, en la parte superior. Algunas de estas pirámides tienen una altura de más de 60 metros. Fueron numeradas secuencialmente (templo I - VI), durante el estudio de campo inicial del yacimiento. Se estima que cada uno de estos grandes templos podría haberse construido en tan solo dos años.

Las pirámides de Tikal fueron posicionadas una frente a otra y las salas que se construyeron en la parte superior de las pirámides tienen depresiones en las paredes de piedra, que sirven como amplificadores del sonido de la voz. Aquí, el diseño arquitectónico maya se realiza plenamente y la voz de los Ahau adquirió cualidades casi divinas. Debido a los resonadores de piedra en la parte superior de una pirámide, la voz de una persona, hablando a un volumen normal, puede ser escuchado por otra persona que se sitúa en la parte superior de otra pirámide, a una distancia sorprendente.

La mayoría de las pirámides visibles en la actualidad, se construyeron durante el resurgimiento, después del hiato de Tikal; es decir, a partir de finales del siglo , a principios del siglo . Sin embargo, cabe señalar que la mayoría de estas estructuras arquitectónicas tienen subestructuras, que se construyeron antes del Hiato de Tikal.

El Templo I (también conocido como "Templo de Ah Cacao" o "Templo del Gran Jaguar") es una pirámide funeraria, dedicada a Jasaw Chan K'awil, que fue sepultado en esta estructura, en el año 734.La pirámide tiene una altura de 47 metros, y su construcción fue finalizada alrededor del 740 al 750.La crestería masiva que encabeza el templo, fue originalmente decorada con una gigantesca escultura del rey en su trono, pero poco sobrevive de esta decoración.La tumba del rey data del Clásico Tardío y fue descubierta en 1962.Entre los objetos recuperados de la tumba, se encuentra una grande colección de tubos de huesos humanos y animales, con inscripciones y bandejas con escenas representando deidades y personas, finamente talladas y frotadas con bermellón, así como ornamentos de jade, de conchas y recipientes de cerámica, llenos de ofrendas, como alimentos y bebidas. El santuario en la cumbre de la pirámide tiene tres cámaras consecutivas, con las entradas cruzadas por dinteles de madera, hechos de múltiples vigas. El dintel exterior era liso, pero los dos interiores eran tallados. Algunas de las vigas fueron removidas en el siglo  y se desconoce su ubicación actual. Otros fueron llevados a algunos museos de Europa.
El Templo II (también conocido como "Templo de las Máscaras") se construyó en torno al año 700, y tiene una altura de 38 metros. Al igual que otros grandes templos de Tikal, el santuario en su cumbre tenía tres cámaras consecutivas, con las entradas cruzadas por dinteles de madera, de los que solo la mitad fue tallada. El templo era dedicado a la esposa de Jasaw Chan K'awil, aunque no se encontró su tumba. El retrato de la reina fue tallado en el dintel cruzando la entrada del santuario en la cumbre. Una de las vigas de este dintel se encuentra ahora en el Museo Americano de Historia Natural de Nueva York.

Templo III (también conocido como "Templo del Gran Sacerdote" o "Templo del Sacerdote Jaguar") fue la última de las grandes pirámides que se construyó en Tikal. Se levanta 55 metros y contenía un dintel de techo elaboradamente esculpido, pero dañado, que, posiblemente, muestra al rey Sol Oscuro, participando en una danza ritual, alrededor del año 810. El santuario del templo tiene dos cámaras.

El Templo IV (también conocida como "Templo de la Serpiente Bicéfala"), es el más alto templo-pirámide de Tikal. Mide 70 metros, desde el nivel del suelo de la plaza, hasta la parte superior de su crestería. Su creación marca el reinado de Yik’in Chan Kawil (Gobernante B, el hijo del Gobernante A o Jasaw Chan K'awiil I); dos dinteles de madera tallada sobre la entrada que conduce al templo en la cumbre de la pirámide, muestra una fecha en cuenta larga (9.15.10.0.0), que corresponde al año 741. Para el siglo , el templo IV era la segunda pirámide más grande construida en toda la región maya y en la actualidad es la tercera estructura precolombina más alta del continente americano, solo superada por la pirámide de Toniná de 75 metros de altura y la pirámide de La Danta, con 72 metros de altura en la actualidad.

El Templo V se encuentra al sur de la Acrópolis Central y es la pirámide funeraria de un gobernante aún no identificado. El templo tiene una altura de 57 metros y es la segunda estructura más alta de Tikal - solo el Templo IV es más alto. El templo data del Clásico Tardío y ha sido fechado alrededor del año 700, mediante datación por radiocarbono. La cerámica asociada con la estructura, sitúa su construcción en el reinado de Nun Bak Chak, en la segunda mitad del siglo . 

El Templo VI, también conocido como "Templo de las Inscripciones", fue dedicado en el año 766. Se nota por su crestería, que se eleva 12 metros. Paneles de glifos cubren la parte trasera y los lados de la crestería. El templo está en frente de una plaza ubicada al oeste y su fachada no ha sido restaurada.

El Templo 33 es una pirámide funeraria, erigida sobre la tumba de Siyaj Chan K'awiil I, (conocida como Entierro 48) en la Acrópolis Norte. Su construcción se inició en el Clásico Temprano, como una amplia plataforma basal, decorada con grandes mascarones de estuco, que flanqueaban la escalera. Siempre en el Clásico Temprano, se añadió una nueva superestructura, con sus propias máscaras y paneles decorados. Durante el Hiato de Tikal, se construyó una tercera etapa sobre las construcciones anteriores, la escalera fue demolida y el entierro real de un gobernante no identificado fue incluido en la estructura (Entierro 23). Durante la construcción de la nueva pirámide, se insertó la tumba de otra persona de alto rango (Entierro 24), en el núcleo de escombros del edificio. La pirámide se completó después, con una altura de 33 metros.

Estructura 34 es una pirámide en la Acrópolis Norte, que fue construida por Siyaj Chan K'awiil II, sobre la tumba de su padre, Yax Nuun Ayiin I. La pirámide fue coronado por un santuario de tres cámaras, las habitaciones situadas una detrás de otra.
La Estructura 5D-43 es un templo radial inusual, ubicado en la Plaza Oriental. Fue construida sobre un complejo de pirámides gemelas, en el extremo de la Plaza Juego de Pelota Oriental y tenía cuatro puertas de entrada y tres escaleras. La cuarta escalera, al lado sur, no fue construida, probablemente porque estaba demasiado cerca de la Acrópolis Central, como para tener una escalera en ese lado.
El edificio tiene el perfil de una plataforma "talud-tablero", modificada del estilo original encontrado en Teotihuacan y posiblemente más parecido al estilo de El Tajín y Xochicalco, que de Teotihuacan. Los paneles verticales del tablero son posicionados entre paneles de talud inclinados y tienen pares de símbolos de discos, como decoración. Símbolos de grandes flores, vinculados con los símbolos del planeta Venus y la estrella utilizados en Teotihuacan, son puestos en los paneles de talud inclinados. El techo de la estructura estaba decorada con frisos, aunque en la actualidad solo permanecen fragmentos, mostrando una cara monstruosa, posiblemente la de un jaguar, con otra cabeza saliendo de la boca.
La segunda cabeza posee una lengua bifurcada, pero probablemente no es la de una serpiente.
El templo y el campo de juego de pelota asociado, probablemente datan del reinado de Nuun Ujol Chaak, o su hijo Jasaw Chan K'awiil I, de finales del siglo .

La Estructura 5C-49 data del siglo  y posee una relación evidente con el estilo arquitectónico de Teotihuacan. Cuenta con una fachada talud-tablero y balaustradas, un elemento arquitectónico muy raro en la región maya.
Está ubicada cerca de la pirámide del Mundo Perdido.

La Estructura 5C-53 es una pequeña plataforma de estilo teotihuacano, que data del año 600, aproximadamente. Tenía escaleras en los cuatro lados y no contaba con una superestructura.
La Pirámide Mundo Perdido (Estructura 5C-54) se encuentra en la parte suroeste del núcleo central de Tikal, al sur del Templo III y al oeste del Templo V.
Estaba decorada con máscaras de estuco del dios del sol y data del Preclásico Tardío.
Esta pirámide es parte de un complejo encerrado de estructuras, que se mantuvieron intactas y que no fueron afectadas por actividades de construcción posterior en Tikal. A finales del Preclásico Tardío, esta pirámide fue una de las estructuras más grandes en la región maya.
Obtuvo su forma definitiva durante el reinado de Chak Tok Ich'aak, en el siglo , en el Clásico Temprano y tiene una altura de 30 metros, con escaleras en sus cuatro lados. La parte superior es plana y, posiblemente, soportaba una superestructura construida con materiales perecederos.
A pesar de que la plaza fue afectada por una significativa alteración posterior, la organización de un grupo de templos, en el lado oriental de este complejo, se adhiere a la disposición de los llamados «Conjuntos E», identificados como observatorios solares.

La Estructura 5D-96 es el templo central, en el lado oriental de la Plaza de los Siete Templos. Ha sido restaurada y el exterior de la pared trasera está decorada con patrones de calavera y tibias cruzadas.

El Conjunto 6C-16 es un complejo residencial de la élite, que ha sido ampliamente excavado. Se encuentra a unos cientos de metros, al sur del complejo Mundo Perdido y las excavaciones han revelado elaboradas máscaras de estuco, pinturas murales de peloteros, esculturas en relieve y edificios con características del estilo de Teotihuacan.

El Juego de Pelota de la Gran Plaza es una pequeña pista del juego de pelota, que se encuentra entre el Templo I y la Acrópolis Central.

El Palacio de los Murciélagos, también conocida como el Palacio de las Ventanas, se encuentra al oeste del Templo III.
Tiene dos pisos, con dos series de cámaras en el piso inferior y una serie única, en el piso superior, que ha sido restaurado. El palacio tiene antiguas pintadas y cuenta con ventanas bajas.

El Complejo N se encuentra al oeste del Palacio de los Murciélagos y del Templo III. El complejo data del 711.

Altar 5 muestra dos nobles esculpidos, uno de los cuales es, probablemente, Jasaw Chan K'awiil I. Están llevando a cabo un ritual, con los huesos de una mujer importante.
Altar 5 se encuentra en el Complejo N, que se sitúa al oeste del Templo III.

Altar 8 muestra la escultura de un prisionero atado.
Se encontró dentro del Complejo P, en el Conjunto H. Actualmente se encuentra en el Museo Nacional de Arqueología y Etnología en la Ciudad de Guatemala.

Altar 9 está asociada con la Estela 21 y lleva la escultura de un prisionero atado. Se encuentra frente al Templo VI.

Altar 10 muestra la escultura de un prisionero atado a un andamio.
Se encuentra en el recinto norte del Conjunto Q, que consiste de un complejo de pirámides gemelas, que ha sido afectado por la erosión.

Altar 35 es un monumento sencillo, asociado con la Estela 43. La estela-altar está situada en la base de la escalera del Templo IV.

En Tikal, vigas talladas, hechas de la madera de "Manilkara zapota", fueron utilizadas como dinteles para las entradas interiores de los templos. Son los dinteles tallados más elaborados que han sobrevivido en toda la región maya.

El Dintel 3 del Templo IV fue trasladado a Basilea en Suiza, en el siglo . Representa a Yik'in Chan K'awiil sentado en un palanquín y está en condiciones casi perfecta.

Estelas son lajas de piedra tallada, a menudo esculpidas con figuras y glifos. Una selección de las estelas más notables de Tikal, incluye las siguientes:

La Estela 1 data del siglo  y representa al rey Siyaj Chan K'awiil II, de pie.

La Estela 4 data de 396 d. C.; es decir, del reinado de Yax Nuun Ayiin y después de la intrusión de Teotihuacan en el área maya.
La estela muestra una mezcla de características mayas y teotihuacanas, así como una mezcla de deidades de ambas culturas. Incluye un retrato del rey, con el Dios Jaguar del inframundo bajo uno de sus brazos y el Tláloc mexicano bajo el otro. Su casco es una versión simplificada de la Serpiente de Guerra de Teotihuacan. Yax Nuun Ayiin es representado con la cara de frente y no de perfil, algo inusual para una escultura maya, pero típico de Teotihuacan,
La Estela 5 fue dedicada en 744, por Yik'in Chan K'awiil.

La Estela 6 es un monumento muy dañado, que data de 514 d. C. Lleva el nombre de la Señora de Tikal, que está celebrando el final del cuarto K'atun de ese año.

La Estela 10 está emparejada con la Estela 12, pero está muy dañada. Describió la consagración de Kaloomte' B'alam, a principios del siglo  y eventos anteriores de su carrera, incluyendo la captura de un prisionero, representado en el monumento.

La Estela 11 fue el último monumento erigido en Tikal. Fue dedicada en 869 d. C., por Jasaw Chan K'awiil II.

La Estela 12 está asociada con la reina conocida como la Señora de Tikal y el rey Kaloomte' B'alam. La reina es descrita llevando a cabo los rituales de fin de año, pero el monumento fue dedicado en honor al rey.

La Estela 16 fue dedicada en el año 711, durante el reinado de Jasaw Chan K'awiil I. La escultura se limita a la parte frontal del monumento e incluye el retrato del rey y un texto glífico.
Fue encontrada en el Complejo N, al oeste del Templo III.

La Estela 19 fue dedicada en 790, por Yax Nuun Ayiin II.

La Estela 20, encontrada en el Complejo P, del Conjunto H, fue trasladada al Museo Nacional de Arqueología y Etnología en la Ciudad de Guatemala.

La Estela 21 fue dedicada en 736, por Yik'in Chan K'awiil.
Solo la parte inferior de la estela quedó intacta. El resto ha sido mutilado en tiempos antiguos. La parte restante de la escultura es de buena calidad y consiste de los pies de una figura y del texto glífico correspondiente. La estela es asociada con el Altar 9 y se encuentra frente al Templo VI.>

La Estela 24 fue erigida, junto con Altar 7, al pie del Templo III en 810 d. C. Ambos fueron quebrados en tiempos antiguos, aunque el nombre del gobernante Sol Oscuro sobrevive, en tres de los fragmentos.

La Estela 29 incluye una fecha en cuenta larga (8.12.14.8.15) equivalente a 292 d. C., la primera fecha en cuenta larga superviviente de las tierras bajas mayas.
La estela es también el monumento más antiguo a incluir el glifo emblema de Tikal. Tiene una escultura del rey, girado hacia la derecha, sosteniendo la cabeza de un dios jaguar del inframundo, una de las deidades protectoras de la ciudad. La estela fue deliberadamente destruida, en el siglo , o algún tiempo después. La parte superior fue tirada en un basurero cercano al Templo III, para ser descubiertos por arqueólogos en 1959.

La Estela 30 fue el primer monumento sobreviviente que se erigió, después del Hiato de Tikal. Su estilo y la iconografía es similar a la de El Caracol, uno de los más importantes enemigos de Tikal.

La Estela 31 es el monumento que marca la consagración de Siyaj Chan K'awiil II. Incluye también dos retratos de su padre, Yax Nuun Ayiin, vestido como un guerrero joven de Teotihuacan. Lleva un lanzadardos en una mano y un escudo decorado con la cara de Tláloc, el dios de la guerra de Teotihuacan.
En la antigüedad, la escultura se rompió y la parte superior se trasladó a la cumbre del Templo 33, donde fue ritualmente enterrado.
La Estela 31 ha sido descrita como la más importante de las esculturas supervivientes de Tikal, del Clásico Temprano. Un largo texto glífico está tallado en la parte posterior del monumento, el más largo conocido del Clásico Temprano.
Describe la llegada de Siyah K'ak' en El Perú y Tikal, en enero de 378 y fue la primera estela de Tikal en ser tallada por sus cuatro caras.

La Estela 32 es un monumento fragmentado, con una escultura en estilo teotihuacano, que parece representar al gobernante de la ciudad con atributos de Tláloc, el dios de la tormenta del centro de México, incluyendo sus ojos saltones y su tocado.

La Estela 39 es un monumento quebrado, que fue erigido en el complejo Mundo Perdido. Falta la parte superior de la estela, pero la parte restante muestra las piernas y la parte inferior del cuerpo de Chak Tok Ich'aak, sosteniendo un hacha de pedernal en la mano izquierda. Está pisoteando la figura de un prisionero atado y ricamente vestido. El monumento data de 376 d. C. El texto en la parte posterior del monumento, describe un ritual de derramamiento de sangre, para celebrar un fin de Katun.
La estela también identifica el nombre del padre de Chak Tok Ich'aak como K'inich Muwaan Jol.
La Estela 40 lleva el retrato de Kan Chitam y data del año 468 d. C.

La Estela 43 es emparejada con el Altar 35. Es un monumento sencillo, ubicado a la base de la escalera del Templo IV.

Entierro 1 es una tumba, ubicada en el complejo Mundo Perdido, donde se recuperó un tazón de cerámica fina, con el mango formado por la cabeza y el cuello de un pájaro, que surge del cuerpo pintado en la tapa.

Entierro 10 es la tumba de Yax Nuun Ayiin.
Se encuentra por debajo de la Estructura 34, en la Acrópolis Norte. La tumba contenía un amplio abanico de ofrendas, incluyendo recipientes de cerámica y alimentos y nueve jóvenes fueron sacrificados, para acompañar al difunto rey.
Un perro fue también sepultado con el rey difunto. Las macetas encontradas en la tumba eran estucadas y pintadas, muchas en una mezcla de estilos mayas y teotihuacanos.
Entre las ofrendas había un incensario, en forma de un viejo dios del inframundo, sentado en un banco de huesos humanos y sosteniendo una cabeza cortada en las manos.
La tumba fue sellada con una bóveda, encima de la cual fue construida la pirámide.

Entierro 48 es generalmente aceptada como la tumba de Siyaj Chan K'awil. Se encuentra bajo el Templo 33, en la Acrópolis Norte.
La cámara de la tumba fue tallada de la roca madre y contenía los restos del rey, junto con los de dos adolescentes, que habían sido sacrificados para acompañar al gobernante fallecido.
Las paredes de la tumba eran cubiertas con estuco de color blanco con glifos pintados que incluían el equivalente en cuenta larga del 20 de marzo de 457, probablemente, la fecha de la muerte o del entierro del rey.
Al esqueleto del rey le faltaba el cráneo, sus fémures y una de sus manos, mientras que los esqueletos de las víctimas sacrificadas estaban intactos.

El Entierro 85 es una tumba que data del Preclásico Tardío y que fue encerrada por una plataforma, con una primitiva bóveda ménsula. La tumba contenía el esqueleto de un varón, que carecía de cráneo y de fémures.
El fundador de la dinastía de Tikal, Yax Ehb' Xook, ha sido vinculado a esta tumba, que se encuentra en el corazón de la Acrópolis Norte.
El difunto, probablemente, había fallecido en batalla, en la que su cuerpo fue mutilado por sus enemigos, antes de ser recuperado y enterrado por sus seguidores. Los huesos fueron cuidadosamente envueltos en textiles, para formar un paquete en posición vertical.
La cabeza fue reemplazada por una pequeña máscara de piedra verde con incrustaciones de concha, para representar los dientes y los ojos, llevando una diadema real de tres puntas.
Esta máscara tiene un emblema de gobernante en la frente y es un raro retrato de un rey maya del Preclásico.
El contenido de la tumba incluía también la columna vertebral de una raya, una concha del género "spondylus" y veintiséis recipientes de cerámica.

Entierro 116 es la tumba de Jasaw Chan K'awiil I. Es una cámara abovedada grande, ubicada en medio de la pirámide, por debajo del nivel de la Gran Plaza. La tumba contenía ricas ofrendas de jadeíta, cerámica, conchas y obras de arte. El cuerpo del rey fue cubierto con una gran cantidad de ornamentos de jade, incluyendo un collar con cuentas muy grandes, como se muestran en los retratos esculpidos del rey. Una de las piezas sobresalientes que se recuperó de la tumba, fue un recipiente de jade, adornado con el retrato del propio rey, esculpido en la tapa.

Entierro 195 es una tumba que se inundó de lodo en la antigüedad, cubriendo los objetos de madera, que se habían podrido completamente, cuando la tumba fue excavada, dejando huecos en el barro seco. Posteriormente, los arqueólogos llenaron estos huecos con estuco y, de esta manera, obtuvieron cuatro efigies del dios K'awiil, a pesar de que las originales, de madera, habían desaparecidas desde hacía mucho tiempo.

Entierro 196 es una tumba real del Clásico Tardío, que contenía un recipiente de mosaico de jade, rematada con la cabeza del dios del maíz.

1999-2001 una producción con un enfoque científico Imax,voz Narrativa Harrison Ford
Editada en 5 idiomas,




</doc>
<doc id="21889" url="https://es.wikipedia.org/wiki?curid=21889" title="Variable aleatoria">
Variable aleatoria

Una variable aleatoria es una función que asigna un valor, usualmente numérico, al resultado de un experimento aleatorio. Por ejemplo, los posibles resultados de tirar un dado dos veces: (1, 1), (1, 2), etc. o un número real (p.e., la temperatura máxima medida a lo largo del día en una ciudad concreta).

Los valores posibles de una variable aleatoria pueden representar los posibles resultados de un experimento aún no realizado, o los posibles valores de una cantidad cuyo valor actualmente existente es incierto (p.e., como resultado de una medición incompleta o imprecisa). Intuitivamente, una variable aleatoria puede tomarse como una cantidad cuyo valor no es fijo pero puede tomar diferentes valores; una distribución de probabilidad se usa para describir la probabilidad de que se den los diferentes valores. En términos formales una variable aleatoria es una función definida sobre un espacio de probabilidad.

Las variables aleatorias suelen tomar valores reales, pero se pueden considerar valores aleatorios como valores lógicos, funciones o cualquier tipo de elementos (de un espacio medible). El término "elemento aleatorio" se utiliza para englobar todo ese tipo de conceptos relacionados. Un concepto relacionado es el de proceso estocástico, un conjunto de variables aleatorias ordenadas (habitualmente por orden o tiempo).

Una variable aleatoria puede concebirse como un valor numérico que está afectado por el azar. Dada una variable aleatoria no es posible conocer con certeza el valor que tomará esta al ser medida o determinada, aunque sí se conoce que existe una distribución de probabilidad asociada al conjunto de valores posibles. Por ejemplo, en una epidemia de cólera, se sabe que una persona cualquiera puede enfermar o no (suceso), pero no se sabe cuál de los dos sucesos va a ocurrir. Solamente se puede decir que existe una probabilidad de que la persona enferme.

Para trabajar de manera sólida con variables aleatorias en general es necesario considerar un gran número de experimentos aleatorios, para su tratamiento estadístico, cuantificar los resultados de modo que se asigne un número real a cada uno de los resultados posibles del experimento. De este modo se establece una relación funcional entre elementos del espacio muestral asociado al experimento y números reales.

Una variable aleatoria (v.a.) "X" es una función real definida en el espacio de probabilidad, formula_1, asociado a un experimento aleatorio.

La definición formal anterior involucra conceptos matemáticos sofisticados procedentes de la teoría de la medida, concretamente la noción σ-álgebra o la de medida de probabilidad. Dado un espacio de probabilidad formula_2 y un espacio medible formula_3, una aplicación formula_4 es una variable aleatoria si es una aplicación formula_5-medible. En el uso ordinario, los puntos de formula_6 no son directamente observables, sólo el valor de la variable en el punto formula_7 por lo que el elemento probabilístico reside en el desconocimiento que se tiene del punto concreto formula_8 .

En la mayoría de usos práctios se tiene que el espacio medible de llegada es formula_9, quedando pues la definición de esta manera:
Se llama rango de una variable aleatoria "X" y lo denotaremos R, a la imagen o rango de la función formula_10, es decir, al conjunto de los valores reales que ésta puede tomar, según la aplicación X. Dicho de otro modo, el rango de una v.a. es el recorrido de la función por la que ésta queda definida:...
Supongamos que se lanzan dos monedas al aire. El espacio muestral, esto es, el conjunto de resultados elementales posibles asociado al experimento, es:
donde ("c" representa "sale cara" y "x", "sale cruz"). Podemos asignar entonces a cada suceso elemental del experimento el número de caras obtenidas. De este modo se definiría la variable aleatoria X como la función

dada por

El recorrido o rango de esta función, R, es el conjunto

El nivel "X" de precipitación registrado un día concreto del año, en una ciudad por una estación meteorológica concreta. El espacio muestral que incluye todos los posibles resultados puede representarse por el intervalo formula_15. En este caso el espacio muestral es más complicado porque incluiría especificar el estado de la atmósfera completo (una aproximación sería describir el conjunto de posiciones y velocidades de todas las moléculas de la atmósfera, que sería una cantidad de información monumental o usar un modelo más o menos complejo en términos de variables macroscópicas, como los modelos meteorológicos usados actualmente).

Podemos revisar la serie histórica de precipitaciones y aproximar la distribución de probabilidad formula_16 de "X" y construir una aproximación formula_17. Nótese que en este caso la distribución de probabilidad no es conocida, sólo se conoce la distribución muestral (la serie histórica) y se conjetura que la distribución real no se aleja mucho de esta aproximaxión formula_18. Si la serie histórica es suficientemente larga y representa un clima que no difiere significativamente del actual estas dos úlitmas funciones diferirán muy poco.

Para comprender de una manera más amplia y rigurosa los tipos de variables, es necesario conocer la definición de conjunto discreto. Un conjunto es discreto si está formado por un número finito de elementos, o si sus elementos se pueden enumerar en secuencia de modo que haya un primer elemento, un segundo elemento, un tercer elemento, y así sucesivamente (es decir, un conjunto infinito numerable sin puntos de acumulación). Para variables con valores en formula_19 las variables aleatorias se clasifican usualmente en:
Las definiciones anteriores pueden generalizarse fácilmente a variables aleatorias con valores sobre formula_20 o formula_21. Esto no agota el tipo de variables aleatorias ya que el valor de una variable aleatoria puede ser también una partición, como sucede en el proceso estocástico del restaurante chino o el conjunto de valores de una variable aleatoria puede ser un conjunto de funciones como el proceso estocástico de Dirichlet.

La distribución de probabilidad de una v.a. X, también llamada función de distribución de X es la función formula_16, que asigna a cada evento definido sobre formula_10 una probabilidad dada por la siguiente expresión:

Y de manera que se cumplan las siguientes tres condiciones:

La distribución de probabilidad de una v.a. describe teóricamente la forma en que varían los resultados de un experimento aleatorio. Intuitivamente se trataría de una lista de los resultados posibles de un experimento con las probabilidades que se esperarían ver asociadas con cada resultado.

La función de densidad de probabilidad (FDP) o, simplemente, función de densidad, representada comúnmente como "f(x)", se utiliza con el propósito de conocer cómo se distribuyen las probabilidades de un suceso o evento, en relación al resultado del suceso.

La FDP es la derivada (ordinaria o en el sentido de las distribuciones) de la función de distribución de probabilidad "F(x)", o de manera inversa, la función de distribución es la integral de la función de densidad:

La función de densidad de una v.a. determina la concentración de probabilidad alrededor de los valores de una variable aleatoria continua.

Sea una variable aleatoria formula_26 sobre formula_27 y una función medible de Borel formula_28, entonces formula_29 será también una variable aleatoria sobre formula_30, dado que la composición de funciones medibles también es medible a no ser que formula_31 sea una función medible de Lebesgue. El mismo procedimiento que permite ir de un espacio de probabilidad formula_32 a formula_33 puede ser utilizado para obtener la distribución de formula_34. La función de probabilidad acumulada de formula_34 es

Si la función "g" es invertible, es decir "g existe, y es monótona creciente, entonces la anterior relación puede ser extendida para obtener

y, trabajando de nuevo bajo las mismas hipótesis de invertibilidad de "g" y asumiendo además diferenciabilidad, podemos hallar la relación entre las funciones de densidad de probabilidad al diferenciar ambos términos respecto de "y", obteniendo

Si "g" no es invertible pero cada "y" tiene un número finito de raíces, entonces la relación previa con la función de densidad de probabilidad puede generalizarse como

donde "x = g(y)". Las fórmulas de densidad no requieren que "g" sea creciente.

Sea "X" una variable aleatoria real continua y sea "Y" = "X".

Si "y" < 0, entonces P("X" = "y") = 0, por lo tanto

Si "y" ≥ 0, entonces

por lo tanto

La función de densidad o la distribución de probabilidad de una variable aleatoria (v.a.) contiene exhaustivamente toda la información sobre la variable. Sin embargo, resulta conveniente resumir sus características principales con unos cuantos valores numéricos. Entre estos están la esperanza y la varianza (aunque para caracterizar completamente la distribución de probabilidad se necesitan parámetros estadísticos adicionales).

La esperanza matemática (o simplemente esperanza) o valor esperado de una v.a. es la suma del producto de la probabilidad de cada suceso por el valor de dicho suceso. Si todos los sucesos son de igual probabilidad la esperanza es la media aritmética. Para una variable aleatoria discreta con valores posibles formula_36 y sus probabilidades representadas por la función de probabilidad formula_37 la esperanza se calcula como:

Para una variable aleatoria continua la esperanza se calcula mediante la integral de todos los valores y la función de densidad formula_38:
La esperanza también se suele simbolizar con formula_41

El concepto de esperanza se asocia comúnmente en los juegos de azar al de beneficio medio o beneficio esperado a largo plazo.

La varianza es una medida de dispersión de una variable aleatoria formula_42 respecto a su esperanza formula_43. Se define como la esperanza de la transformación formula_44: 

Dada una distribución de probabilidad continua el conjunto de sus momentos caracteriza completamente la distribución. Dos de estos momentos ya han aparecido, el valor esperado coincide con el momento de primer orden, mientras que la varianza puede expresarse como una combinación del momento de segundo orden y el cuadrado del momento de primer orden. En general, el momento de orden "n" de una variable aleatoria real con densidad de probabilidad definida casi en todas partes se calcula como:

Estos momentos pueden obtenerse a partir de las derivadas "n"-ésimas de la función característica formula_45 asociada a la variable "X":

o análogamente la función generadora de momentos:



</doc>
<doc id="21891" url="https://es.wikipedia.org/wiki?curid=21891" title="Quiché (etnia)">
Quiché (etnia)

Quiché (o k'iche' ) es el nombre de un pueblo nativo de Guatemala, así como el de su idioma y su nación en tiempos precolombinos. El término "quiché" proviene de "qui", o "quiy", que significa "muchos", y "che", palabra maya original, que alude a un bosque o tierra de muchos árboles. El Quiché es también el nombre de un departamento de Guatemala.

El pueblo quiché es uno de los pueblos mayas nativos del altiplano guatemalteco. En tiempos precolombinos los quichés establecieron uno de los más poderosos estados de la región. <ref></fer> La última ciudad capital era Carcajear, también conocida como Atlántica, cuyas ruinas se encuentran a dos kilómetros de Santa Cruz del Quiché, en el departamento de El Quiché, Guatemala.

Fueron conquistados por el español Pedro de Alvarado a principios del siglo XVI, en 1524.El último comandante del ejército quiché fue Tecún Umán, quien murió 
en la batalla de los Loras del Pinal. Tecún Umán es todavía un héroe popular nacional y figura de leyenda, también es el héroe nacional de Guatemala.

El departamento de Quiché fue nombrado así en alusión a este pueblo que en su inicio ocupó el territorio denominado "Quix Ché" que significa "árboles con espinas" pues en la región abundaron los magueyes, nopales, ortigas, etc. Este departamento es el hogar central del pueblo quiché, aunque en tiempos recientes se ha dispersado sobre un área más amplia del territorio guatemalteco.

La agricultura ha constituido la base de la economía maya desde la época precolombina y el maíz su principal cultivo, además del algodón, los frijoles (judías), el camote (batata), la yuca (o mandioca) y el cacao. Las técnicas del hilado, el tinte y el tejido consiguieron un elevado grado de perfección. Los mayas domesticaron el pavo, pero carecían de animales de tiro o vehículos con ruedas. Fabricaban finos objetos de cerámica, que difícilmente se han superado en el Nuevo Mundo fuera de Perú. Como unidad de cambio se utilizaban las semillas de cacao y las campanillas de cobre, material que se empleaba también para trabajos ornamentales, al igual que el oro, la plata, el jade, las conchas de mar y las plumas de colores. Sin embargo, desconocían las herramientas metálicas.

Los pueblos maya formaban una sociedad muy jerarquizada. Estaban gobernados por una autoridad política, el "halach vinic", cuya dignidad era hereditaria por línea masculina. Este delegaba la autoridad sobre las comunidades de poblados a jefes locales o bataboob, que cumplían funciones civiles, militares y religiosas.

El Idioma quiché es parte de la familia de lenguas mayenses. El número de hablantes es entre 1 y 2 millones de personas, principalmente en los departamentos de El Quiché, Totonicapán, Sololá, Quetzaltenango, Huehuetenango y Suchitepéquez. Es el idioma maya con más hablantes en Guatemala y el segundo más hablado en el país. La mayoría de los indígenas quichés también habla el idioma español, excepto en algunas áreas rurales aisladas.

El texto más famoso en idioma quiché es el Popol Vuh, que narra del origen de este pueblo desde la creación del mundo, de los dioses y de los primeros hombres y mujeres, formados de maíz, hasta la conquista española.

Rigoberta Menchú, que obtuvo el Premio Nobel de la Paz en 1992, forma parte de la etnia quiché.

Humberto Ak'abal, autor quiché mundialmente reconocido y ganador de diversos premios a nivel internacional.



</doc>
<doc id="21892" url="https://es.wikipedia.org/wiki?curid=21892" title="Andrew S. Tanenbaum">
Andrew S. Tanenbaum

Andrew Stuart "Andy" Tanenbaum (nacido el 16 de marzo de 1944), también conocido como ast o Papá Tanenbaum, es profesor de ciencias de la computación de la Universidad Libre de Ámsterdam, Países Bajos.

Tanenbaum es más conocido por ser el creador de Minix, una réplica gratuita del sistema operativo UNIX con propósitos educativos, y por sus libros sobre ciencias de la computación.

Tanenbaum nació en la ciudad de Nueva York, Estados Unidos aunque creció en White Plains. Se licenció en física en el Instituto Tecnológico de Massachusetts (más conocido como MIT), en 1965.

En 1971 consiguió el doctorado en física en la Universidad de California, Berkeley.
Posteriormente se trasladó a los Países Bajos para vivir con su esposa, pero aún conserva la ciudadanía estadounidense.

Desde 2004 es profesor de Arquitectura de ordenadores y sistemas operativos en la Universidad Libre de Ámsterdam (Vrije Universiteit Amsterdam) donde lidera un grupo de sistemas de computación.

En 1987 creó el sistema operativo Minix, un sistema Unix-like gratuito con propósitos educativos, que posteriormente inspiró Linux.

En 1992 participó en Usenet en un encendido debate con Linus Torvalds, el creador de Linux, sobre los méritos de la idea de Linus de utilizar un núcleo monolítico en vez de los diseños basados en un micronúcleo que Tanenbaum creía que serían la base de los sistemas operativos futuros. Dicho debate se originó en el grupo de noticias comp.os.minix cuando Andrew envió un mensaje con el título "LINUX is obsolete" (en español, LINUX está obsoleto).

Tanenbaum es el autor, junto a otros miembros de la Universidad Libre de Ámsterdam, del sistema operativo distribuido de investigación Amoeba, basado en una arquitectura de micronúcleo. Tanenbaum también es el creador de Globe, un software que provee una infraestructura para un sistema distribuido a nivel mundial.

Tanenbaum es ampliamente conocido por sus libros sobre materia informática, muy utilizados en centros de enseñanza superior, destacando, entre otros:

En los siguientes enlaces se puede encontrar una lista exhaustiva de los libros y publicaciones realizados por Tanenbaum:

En 2004 crea Electoral-vote.com, un sitio web donde se analizaban los sondeos de opinión para las elecciones presidenciales de Estados Unidos de 2004 para prever cual sería la composición del Colegio Electoral.

Durante la mayor parte de campaña, Tanenbaum oculta su identidad bajo el seudónimo de «Votemaster», aunque reconociendo que tiene una preferencia personal por el candidato John Kerry. El 1 de noviembre de 2004, el día anterior a las elecciones, Tanenbaum revela su identidad y las razones por las que creó la página web.

En 2006, la web Electoral-vote.com es nuevamente utilizada para analizar los sondeos de las elecciones para el Congreso de Estados Unidos de 2006.

Tanembaum ha recibido diversos premios por su trabajo:



</doc>
<doc id="21896" url="https://es.wikipedia.org/wiki?curid=21896" title="Perro sin pelo del Perú">
Perro sin pelo del Perú

El viringo peruano, perro peruano sin pelo, perro calato, perro chimú o viringo es una raza de perro sin pelo originaria del Perú, empleada usualmente como animal de compañía. Ha sido reconocido oficialmente como Patrimonio del Perú.

El 12 de junio de 1995, la Federación Cinológica Internacional (FCI), con sede en Thuin (Bélgica), reconoció y registró al perro sin pelo del Perú en su nomenclatura de razas con el número 310, clasificándolo en el Grupo V, tipo Spitz, que es para aquellos perros atléticos y ágiles ideales para carreras y en la sección 6 en la que se ubican los perros tipo primitivos.

Al calificársele de perro primitivo, se le reconoce como de raza pura, es decir, la naturaleza los hizo tal como son, no habiendo variado sus características morfológicas en miles de años, tal como puede apreciarse en diferentes huacos preincas.

El Instituto Nacional de Cultura del Perú mediante la resolución directiva 001-INC de enero de 2001 dispuso la ubicación de perros sin pelo del Perú en todos los museos de sitio y zonas arqueológicas ubicados en la costa peruana y que cuenten con las condiciones necesarias que permitan su desarrollo natural y su crianza.

A su vez, el Congreso de la República del Perú, mediante el decreto ley número 27537 del 22 de octubre de 2001 incluyó a esta raza como patrimonio de la nación peruana y la reconoció como oriunda de este país.

El 8 de marzo de 2013, el Gobierno del Perú designó a la arqueóloga Denise Pozzi-Escot representante del Ministerio de Cultura ante el Comité Nacional de Protección del Perro sin Pelo del Perú.

Existen representaciones que aparecen en los ceramios de distintas culturas preincas, como Vicús, Mochica, Chancay, con influencia tiahuanaco, Sicán y Chimú. En estas representaciones, el perro sin pelo hace su aparición entre el año 300 a. C. hasta el 1460.

Se han encontrado también huesos del perro peruano que datan de tiempos precolombinos. En 1987, el arqueólogo Walter Alva descubrió en el centro de una gran plataforma de barro conocida como "Huaca Rajada", la tumba de un personaje importante moche a quien llamó el Señor de Sipán, que descansaba en una caja mortuoria, rodeado de los esqueletos de ocho varones, dos mujeres y un perro.

Los incas lo llamaban "allqu" (‘perro’); en el Perú también se conoció como "kaclla". El nombre "viringo" parece ser el original usado por los moches o mochicas, cuyos descendientes (entre Piura y Trujillo) hasta el día de hoy los nombran así.

Estos perros cumplieron un rol importante dentro de las costumbres y mitos de los incas. Las crónicas de la época de la conquista española y el Virreinato dieron testimonio de la presencia de los "viringos". La gente del campo conservó el perro sin pelos, asociado a su cultura propia y lo usó para fines medicinales.

Debido a la carencia de pelo, esta raza mantiene su cuerpo más caliente para protegerse del ambiente. Pedro Weiss señala en sus investigaciones que el perro sin pelo del Perú genéticamente tiene un síndrome de hipoplasia ectodérmica, lo cual significa que posee piel cálida que al entrar en contacto con la piel humana la puede calentar, lo cual ha sido base para atribuirle propiedades medicinales, por ejemplo para aliviar el reumatismo.

Hay quienes le han atribuido al perro peruano la capacidad de evitar alergias, problemas bronquiales y asma, pues no tiene pelo que podría causar problemas respiratorios, tampoco pulgas ni garrapatas, ya que estas no tienen dónde anidar.

Existen tres tipos de razas según su talla:

El peso está en relación con los tres tamaños para los machos y para las hembras:

La piel del perro peruano sin pelo es muy variable, hay ejemplares de color negro pizarra con pelo negro, negro azulado con pelo rubio,y marrón con pelo castaño. Hay ejemplares de color entero (color que oscila entre los mostrados en las fotos, y el negro) y hay otros que presentan manchas blancas o rosadas, principalmente en la cara y en el pecho.

Muy rara vez, cuando nace una nueva camada, uno de los cachorros nace con pelo, debido a un gen recesivo.

En la mayoría de los casos los perros sin pelo del Perú no ofrecen problemas relacionados con la reproducción. Los ejemplares de talla grande suelen tener camadas más numerosas que los de talla pequeña. Debido a la ausencia de pelo es importante mantenerlos en una zona cálida y vigilar que la temperatura ambiental no baje de los 23 grados, es recomendable que cuenten con alguna fuente de calor si el área donde están alojados no alcanza la temperatura anteriormente indicada. Es fundamental suministrar a la hembra una alimentación equilibrada rica en proteínas, los piensos de fabricación industrial de gama alta son los más indicados para el buen fin de la reproducción.

La ausencia de pelo en el perro viringo o calato es consecuencia de una mutación natural de tipo dominante. Los genes dominantes para la calvicie presenta una estructura heterocigota (dominancia parcial), con lo que en la descendencia entre ejemplares pelones nacerán mayoritariamente perros sin pelo, no obstante, aquellos canes que porten dos genes dominantes no se gestarán ya que este gen en homocigosis es letal.

En la cruza de dos ejemplares sin pelo, puede nacer algún ejemplar con pelo, esto se debe a la estructura hetorocigota de los alelos que portan los progenitores, concretamente la variedad con pelo hereda los alelos en homocigosis para el gen recesivo.

La variedad con pelo es valorada por muchos criadores apasionados a esta raza ya que fortalece la genética del perro peruano, en estos casos, el resultado de la cruza entre un ejemplar sin pelo y otro con manto normal es de un 50% de cachorros desnudos, el resto con manto peludo. Cabe destacar que los canes que nacen con pelo normal no presentan las anomalías típicas de los perros desnudos, como es el caso de la pérdida prematura de algunas piezas dentales.

El gen causante de la desnudez se denomina «gen Foxi3» y también está presente en otras razas de perros desnudos como es el caso del xoloitzcuintle o el crestado chino.



</doc>
<doc id="21898" url="https://es.wikipedia.org/wiki?curid=21898" title="Organización Internacional de la Francofonía">
Organización Internacional de la Francofonía

La Organización Internacional de la Francofonía (OIF) es una organización internacional que designa la comunidad de 900 millones de personas y países en el mundo que usan este idioma. Son miembros de pleno derecho cuarenta y nueve estados, además de cuatro miembros asociados y diez miembros observadores. La mayoría de los estados miembros son francófonos. 

Sus actividades no se centran exclusivamente en la lengua, sino también en la difusión de la cultura, la educación, extensión de la democracia y reducción de las diferencias como consecuencia de las nuevas tecnologías. Se organiza en Cumbres de Jefes de Estado y de Gobierno que se celebran cada dos años, la Conferencia de Ministros y la Comisión Permanente que actúa como órgano ejecutivo. 

Desde 2019 al frente de la Secretaría General de la Francofonía se encuentra la ruandesa Louise Mushikiwabo.

Fue fundada el 20 de marzo de 1970 en Niamey, Níger. La iniciativa surgió, en especial, por los países de habla francesa de África. Sus principales impulsores fueron los jefes de Estado Habib Bourguiba (Túnez), Norodom Sihanouk (Camboya), Hamani Diori (Níger), Charles Hélou (Líbano) y Léopold Sédar Senghor (Senegal). Actualmente la sede central está en París y agrupa a Estados francófonos que tienen un nexo cultural. 

Hay que distinguir entre los países donde el francés es lengua oficial, aquellos donde es la lengua materna de gran parte de la población, aquellos donde es lengua de difusión cultural, aquellos donde es empleado por determinadas clases sociales, etc. Estas categorías no tienen por qué coincidir: en muchos de los países en los que el francés es lengua oficial, no es la lengua materna de la población.

En 2008 la Francofonía estaba compuesta por cuarenta y nueve Estados miembros que albergan el 10% de la población mundial. Sus actividades no se centran exclusivamente en la lengua, sino también en la difusión de la cultura, la educación, extensión de la democracia y reducción de las diferencias como consecuencia de las nuevas tecnologías. Se organiza en Cumbres de Jefes de Estado y de Gobierno que se celebran cada dos años, la Conferencia de Ministros y la Comisión Permanente que actúa como órgano ejecutivo.

Mantiene cinco agencias internacionales con un presupuesto en conjunto de 230 millones de euros:

La Francofonía proclama entre sus principios y valores fundamentales los siguientes:

La palabra fue acuñada por Onésimo Reclus.





</doc>
<doc id="21899" url="https://es.wikipedia.org/wiki?curid=21899" title="Francés">
Francés

Francés puede referirse a:

Asimismo, puede hacer referencia a:

También, puede referirse a las siguientes personas:


Asimismo, en deporte, puede hacer referencia a:


También, puede referirse a las siguientes razas de perro:


Además, puede hacer referencia a:



</doc>
<doc id="21915" url="https://es.wikipedia.org/wiki?curid=21915" title="Modelismo ferroviario">
Modelismo ferroviario

El modelismo ferroviario es una actividad recreativa cuyo objeto es imitar a escala trenes y sus entornos. Los trenes pueden ser estáticos o en movimiento. En este último caso, normalmente se utiliza electricidad de bajo voltaje (entre 9 y 24 voltios) tanto para el movimiento como para los accesorios, iluminación, etc. y son conocidos comúnmente como trenes eléctricos a escala. Durante parte del siglo XX también han sido populares los de cuerda y existen modelos de locomotoras propulsadas por vapor real.

La escala normalizada más popular desde mediados del siglo XX hasta hoy es la H0, seguida de la N. Pero existen otras.

En el Reino Unido, las escalas habituales son distintas y están basadas en la reducción de medidas imperiales a sistema métrico. Se suelen denominar por el número de milímetros a escala que corresponden a un pie real (nótese que los anchos de via se corresponden con los del resto de Europa):

Por otra parte, se fabrican nuevamente ferrocarriles de escalas grandes, como las antiguas, con las cuales los aficionados consiguen una reproducción muy detallada de las locomotoras y los vagones. Algunas de las marcas que fabrican trenes de estas escalas disponen de modelos a prueba de intemperie, lo que permite montar los circuitos y hacer circular los trenes al aire libre, incluso cuando llueve. La escala G (1:24) fue introducida y desarrollada en 1968 por Ernst Paul Lehmann Patentwerk con la marca de su propiedad LGB ("Lehmann Gross Bahn" en alemán, que quiere decir: "Gran Tren Lehmann"), para uso en interiores y exteriores; su nombre proviene del alemán groß (significa "grande"). Tradicionalmente, la escala G usa una vía con trocha de 45 mm, como la empleada por los modelos estándar de escala 1 de trocha angosta, utilizando la escala correcta de 1:22,5. Popularmente, se cree que su nombre proviene de jardín (G = garden en inglés), ya que a menudo se usa al aire libre. Este tamaño es conocido por una artesanía y detalle impresionantes, pero requiere una gran cantidad de espacio, de modo que no es la elección más común para los coleccionistas de interior.

Desde los principios del modelismo ferroviario, la velocidad de los trenes se controlaba variando la tensión presente en la vía, de la cual se alimentan los motores de los mismos. Existen dos sistemas principales: el de corriente continua, con dos carriles, y el de tres carriles, de corriente alterna.

El sistema de dos carriles usa corriente continua y alimenta las locomotoras y vagones por uno de los carriles y retorna por el otro. Los carriles tienen, por tanto, polaridad. Variando esta polaridad se consigue invertir el sentido de la marcha.

El sistema de tres carriles utiliza corriente alterna que alimenta por los carriles de circulación por un lado y tiene un tercer carril central, entre ellos, para el retorno. Dado que la corriente alterna no tiene una polaridad constante, para invertir el sentido de la marcha se envía a la locomotora una sobretensión de aproximadamente 20 a 24 voltios que activa un mecanismo mecánico o electrónico de inversión. 

Las locomotoras de dos carriles y las de tres carriles no son compatibles entre sí. Los vagones pueden serlo, de acuerdo con las siguientes reglas:


A mediados de los 90, se empezaron a popularizar los sistemas de control basados en la electrónica y, actualmente, se ha pasado a los que usan microcontroladores. Este sistema se ha normalizado en gran parte gracias a la NMRA (North American Model Railways Association, o asociación norteamericana de modelismo ferroviario). Los sistemas normalizados reciben el nombre genérico de DCC (Digital Command Control). El sistema requiere que las locomotoras tengan instalado un circuito electrónico capaz de mover el motor o los accesorios (luces, generadores de humo, etc.) de acuerdo a las órdenes digitales recibidas por la vía mediante circuitos electrónicos. El circuito de la locomotora recibe el nombre de decodificador, y la operación de instalarlo se suele denominar "digitalizar".

Los sistemas DCC también existen en versiones de dos y de tres carriles. A diferencia de los sistemas tradicionales, el tipo de corriente que circula por la vía puede ser en ambos casos la misma. En realidad los sistemas de dos y tres carriles tradicionales tenían cada uno sus ventajas en la manera de mover las composiciones de modos distintos en el mismo circuito. Los sistemas digitales llevan el control individualizado mediante microcircuitos, independientemente del sistema de alimentación.

Respecto del sistema de control tradicional, el control DCC presenta las siguientes ventajas:

Los inconvenientes del sistema DCC frente al tradicional son:

Los trenes eléctricos tienen vagones de ferrocarril a escala.


Sus orígenes se remontan a fines del siglo XIX en consonancia con la aparición de juguetes ingeniosos que aplicaran las tecnologías novedosas, con lo que se diseñan unos trenes pequeños fabricados en chapa de hierro y movidos por un motor eléctrico. En la Feria de Leipzig de 1891, Märklin presentó la primera locomotora funcional a escala de la historia (1:32 o escala 1). Construida en hojalata, tenía un mecanismo de reloj de cuerda que le permitía ponerse en movimiento sobre rieles y circular sobre ellos. Carlisle y Finch presentaron la primera composición de un tren completo con motor eléctrico en 1897. A principios del siglo XX Joshua Lionel Cowen creó un tren eléctrico para el escaparate de su juguetería, pero recibió tantas peticiones que la atracción publicitaria acabó por convertirse en un clásico de la industria juguetera.




</doc>
<doc id="21917" url="https://es.wikipedia.org/wiki?curid=21917" title="GNUstep">
GNUstep

GNUstep es un conjunto de Frameworks o bibliotecas orientadas a objetos, aplicaciones y herramientas escritas en el lenguaje Objective-C, para el desarrollo de aplicaciones de escritorio. 

Es a su vez una implementación libre de las especificaciones OpenStep, creadas por NeXT, que después fue comprada por Apple. Con el surgimiento del sistema operativo Mac OS X de Apple, basado en OpenStep, GNUstep también planea compatibilidad con este sistema.

GNUstep incorpora dos herramientas de desarrollo (RAD). Project Center, para la creación de proyectos, y GORM, para la creación de interfaces gráficas. Ambas herramientas son las equivalentes a Project Builder e Interface Builder, respectivamente, de NeXTSTEP. 

GNUstep se conforma básicamente de cuatro paquetes Make, Base, GUI y Back. Cada uno tiene diferentes funciones:

Este paquete facilita la creación de los Makefiles de los proyectos creados con GNUstep. De tal forma que se hace sencilla la configuración, instalación y empaquetado de la aplicación.

Este es el Framework que contiene todas las clases no visuales. Las clases que están basadas en las originales de NeXTSTEP comienzan con las letras NS, y las que han sido añadidas por el proyecto GNUstep comienzan con las letras GS.

Este es el Framework que contiene todas las clases visuales. Al igual que en el Framework Base las clases que están basadas en las originales de NeXTSTEP comienzan con las letras NS, y las añadidas comienzan con las letras GS.

Este paquete es el Back-end del Framework GUI. Y es el encargado de las rutinas para dibujar los componentes visuales de las aplicaciones creadas con GNUstep.

La apariencia de las aplicaciones hechas con GNUstep se asemeja en general a las creadas con NeXTSTEP. Aunque esta puede variar de un sistema operativo a otro, así como de la configuración de las bibliotecas. En los sistemas GNU/Linux, BSD, Solaris, etc. las aplicaciones tienen un menú vertical y desligado de cualquier ventana. En estos sistemas también se hace uso de los AppIcons y Miniwindows, los cuales pueden ser manejados con la herramienta IconManager (en el escritorio WindowMaker esta herramienta no es necesaria). Sin embargo, es posible configurar GNUstep para utilizar la barra de tareas para minimizar las ventanas, así como configurar las aplicaciones para tener el menú en ventana (aquellas que soporten este estilo).

En el sistema Mac OS el menú es como el de cualquier aplicación nativa de esa plataforma. En el sistema Windows puede tenerse el menú en ventana para las aplicaciones que han sido diseñadas con soportar para este estilo.







</doc>
<doc id="21931" url="https://es.wikipedia.org/wiki?curid=21931" title="Heterodino">
Heterodino

En telecomunicación, el término heterodino tiene los siguientes significados:

1. Generar nuevas frecuencias mediante la mezcla de dos o más señales en un dispositivo no lineal, tal como un diodo, una válvula termoiónica o un transistor.

2. La frecuencia producida por la mezcla de dos o más señales en un dispositivo no lineal se denomina heterodina.

Una aplicación de la heterodinación la tenemos en los receptores de radio superheterodinos, donde cualquier frecuencia entrante seleccionada es convertida mediante este principio en una frecuencia intermedia común, con lo que se facilita la amplificación y se mejora la selectividad.

La heterodinación se utiliza ampliamente en la ingeniería de comunicaciones para generar nuevas frecuencias y mover información de un canal de frecuencia a otro. Además de su uso en el receptor superheterodino que se encuentra en casi todos los receptores de radio y televisión, que se utiliza en transmisores de radio, módems, satélites de comunicaciones y set-top boxes, radares, radiotelescopios, telemetría, sistemas de telefonía celular, cabeceras y descodificadores de televisión por cable, relés de microondas, detectores de metales, relojes atómicos y sistemas de contramedidas electrónicas militares (jamming).

En las redes de telecomunicaciones de gran escala, tales como troncos de redes telefónicas, relé de microondas, redes de televisión por cable y enlaces de sistemas de comunicación por satélite, se comparten enlaces de ancho de banda de gran capacidad por muchos canales de comunicación individuales, mediante el uso de heterodino para mover la frecuencia de las señales individuales a diferentes frecuencias, que comparten el canal. Esto se conoce como multiplexación por división de frecuencia (FDM).

Por ejemplo, un cable coaxial utilizado por un sistema de televisión por cable, puede llevar 500 canales de televisión al mismo tiempo, debido a que cada uno se le da una frecuencia diferente, por lo que no interfiere uno con el otro. En la fuente de cable o cabecera, supra-convertidores (o ascen-convertidores) electrónicos convierten cada canal de televisión entrante a una nueva frecuencia, superior. Lo hacen mediante la mezcla de la frecuencia de señal de televisión, f con un oscilador local a una frecuencia mucho más alta f , creando un heterodino en la suma f+f, que se añade al cable. En el hogar del consumidor, el descodificador de cable tiene un infra-convertidor (o descen-convertidor) que mezcla la señal de entrada a la frecuencia f+f con el mismo oscilador local de frecuencia f creando la diferencia heterodina, convirtiendo el canal de televisión de nuevo a su frecuencia original: (f+f)-f= f. Cada canal se mueve a una frecuencia más alta diferente. La frecuencia de base inferior original de la señal se llama banda base, mientras que el canal más alto al que se ha trasladado, se llama banda de paso.



</doc>
<doc id="21933" url="https://es.wikipedia.org/wiki?curid=21933" title="Libro">
Libro

Un libro (del latín "liber", "libri") es una obra impresa, manuscrita o pintada en una serie de hojas de papel, pergamino, vitela u otro material, unidas por un lado (es decir, encuadernadas) y protegidas con tapas, también llamadas cubiertas. Un libro puede tratar sobre cualquier tema.

Según la definición de la Unesco, un libro debe poseer veinticinco hojas mínimo (49 páginas), pues de veinticuatro hojas o menos sería un folleto; y de una hasta cuatro páginas se consideran hojas sueltas (en una o dos hojas).

También se llama «libro» a una obra de gran extensión publicada en varias unidades independientes, llamados "tomos" o "volúmenes". Otras veces se llama también «libro» a cada una de las partes de una obra, aunque físicamente se publiquen todas en un mismo volumen (ejemplo: Libros de la Biblia).

Hoy en día, no obstante, esta definición no queda circunscrita al mundo impreso o de los soportes físicos, dada la aparición y auge de los nuevos formatos documentales y especialmente de la World Wide Web. El libro digital o libro electrónico, conocido como "e-book", está viendo incrementado su uso en el mundo del libro y en la práctica profesional bibliotecaria y documental. Además, el libro también puede encontrarse en formato audio, en cuyo caso se denomina audiolibro.

Desde los orígenes, la humanidad ha tenido que hacer frente a una cuestión fundamental: la forma de preservar y transmitir su cultura, es decir, sus creencias y conocimientos, tanto en el espacio como en el tiempo.

El planteamiento de esta cuestión supone: por un lado, determinar la forma de garantizar la integridad intelectual del contenido de la obra y la conservación del soporte en el que fue plasmada, y por otro, encontrar el medio por el cual se mantendrá inalterada la intención o finalidad para la cual se concibió.

Los orígenes de la historia del libro se remontan a las primeras manifestaciones pictóricas de nuestros antepasados, la pintura rupestre del hombre del paleolítico. Con un simbolismo, posiblemente cargado de significados mágicos, estas pinturas muestran animales, cacerías y otras escenas cotidianas del entorno natural del hombre antiguo, que trataba de dominar las fuerzas adversas de la naturaleza capturando su esencia mediante su representación. Son el más antiguo precedente de los primeros documentos impresos de que se tiene constancia.

Las señales gestuales fueron la primera forma de expresar y transmitir mensajes. La palabra hablada es la manera más antigua de contar historias. Mediante fórmulas de valor mnemotécnico se estructuraban narraciones, que pasaban de generación en generación como valiosa herencia cultural de los más diversos grupos humanos. Dichas reglas mnemotécnicas ayudaban tanto a la memorización como a la difusión de los relatos. Es el caso de los poemas homéricos, que han merecido valiosos estudios sobre el particular. Posiblemente, gran parte de las tradiciones y leyendas han tenido semejante inicio. Esta transmisión oral tenía el inconveniente de los «ruidos» que deformaban el mensaje. La mayoría de las veces era el narrador (rapsoda, aeda, juglar) quien en función de sus intereses la deformaba de una u otra forma.

Cuando los sistemas de escritura fueron inventados en las antiguas civilizaciones, el hombre utilizó diversos soportes de escritura: tablillas de arcilla, ostracon, placas de hueso o marfil, tablas de madera, papiros, tablillas enceradas, planchas de plomo, pieles curtidas, etc.

La escritura fue el resultado de un proceso lento de evolución con diversos pasos: imágenes que reproducían objetos cotidianos (pictografía); representación mediante símbolos (ideografía); y la reproducción de sílabas y letras.

Los más antiguos vestigios de escritura se encuentran, hacia finales del , en el Antiguo Egipto, con jeroglíficos, y la antigua Mesopotamia, mediante signos cuneiformes (escritura cuneiforme; utilizaban una varilla con sección triangular, que al hendir en placas de arcilla, dejaba una marca en forma de cuña). La usaron los sumerios, acadios, asirios, hititas, persas, babilonios, etc. La escritura egipcia, que perduró más de tres milenios, mediante jeroglíficos, representaba ideas abstractas, objetos, palabras, sílabas, letras y números. Evolucionó en las escrituras hierática y demótica. Otros pueblos, como los hititas y los aztecas también tuvieron tipos propios de escritura.

La escritura china más antigua que se conoce son 50000 inscripciones sobre conchas de tortuga que incorporan 4500 caracteres distintos, y data del 1400 a. C. en el yacimiento de Xiaotun, en la provincia de Henan. Pero los primeros libros reconocibles de China corresponden al siglo VI a. C., los jiance o jiandu, rollos de finas tiras de bambú o madera grabados con tinta indeleble y atados con cordel. Estos textos servían principalmente a causas institucionales, era la obra de funcionarios civiles o militares.

Desde Confucio en adelante (551-479 a. C.) los libros se convirtieron en importantes instrumentos de aprendizaje, se escribieron tratados de filosofía, medicina, astronomía y cartografía.

En el período de los reinos combatientes (475-221 a. C.) La seda se usó mucho como soporte para escribir. La tela era ligera, resistente al clima húmedo, absorbía bien la tinta y proporcionaba al texto un fondo blanco, sin embargo era mucho más cara que el bambú, es por esto que en ocasiones se hacía una copia en bambú antes de grabarse en seda los textos importantes.

La invención del papel según la tradición china, se atribuye a un eunuco de la corte imperial llamado Cai Lin en el 105 d. C. Usando nuevos ingredientes (trapos viejos, cáñamo, corteza de árbol y redes de pescar) creó un método de fabricación de papel muy similar al que se usa hoy en día. Pero el papel tardó cientos de años en reemplazar al bambú y la seda, fue hasta finales del siglo II d. C. que la corte imperial lo usó en cantidades importantes. Esta innovación no se propagó fuera de China hasta el 610 d. C. aproximadamente, y alcanzó Europa a través de España hasta el siglo XII.

A mediados del siglo VIII los chinos inventaron la impresión xilográfica, o el grabado en madera, y la necesidad de reproducir un gran número de textos e imágenes budistas, calendarios, manuales de adivinación y diccionarios promovió una rápida y temprana propagación de la xilografía. El primer libro impreso chino que se ha encontrado es el Sutra del diamante del 868 d. C.

Los impresores chinos crearon los tipos móviles hacia el siglo XI, el escritor chino Ch'en Kua (1030-1095) narra la historia de esta invención en su libro de cosas vistas y oídas (Mengshi Pitan), según el escritor el herrero JenTsung de la dinastía de los Song del norte entre 1041-1049 logró crear caracteres móviles, para esto utilizó arcilla endurecida al fuego sobre la cual había grabado unos caracteres móviles que fijo sobre una plancha de hierro impregnada de resina de pino, cera y cenizas. También se le atribuye la creación de una mesa giratoria para guardar los caracteres, esta técnica se llamaba tipografía tablearia. Hacia el 1300 Wang- Tcheng, un técnico agrónomo, emplazó la arcilla por madera de azufaifo, que era mucho más dura. Pero este avance no revolucionó la imprenta hasta el punto que lo hizo Gutenberg en Europa 400 años después. A diferencia de las lenguas europeas, el chino escrito requiere miles de caracteres únicos, lo que hace mucho más eficaz los bloques de madera individuales que los enormes conjuntos de tipos reutilizables. En contraste con el declive de las artes de los escribas en occidente en los siglos que siguieron a la creación de la imprenta de tipos móviles, la caligrafía china conservó su prestigio, era un arte. No obstante, a finales del siglo XV, China había producido más libros que el resto del mundo junto.

Los árabes aprendieron la técnica para fabricar papel de sus contactos con China en el siglo VIII, y este se introdujo en Europa en el siglo XII a través de la España musulmana.

La obra xilográfica más antigua encontrada hasta nuestros días es el Dharani Sutra de Corea, datado en el 751 a. C., aunque no se sabe quién fue el inventor de la xilografía los chinos y coreanos fueron los que impulsaron la impresión xilográfica, principalmente para editar textos religiosos. El budismo chino y coreano fue el vehículo que trasmitió la xilografía a Japón. Pero Corea realizó muchos otros avances que revolucionaron la manera de imprimir y en consecuencia el libro.

Entre 1234 y 1239 los coreanos que se habían refugiado en la isla de Gwanghwa, debido a la invasión mongol, no disponían de madera dura fue entonces que imprimieron 28 ejemplares de los 50 volúmenes del Go geum sang jeong ye mun con caracteres móviles metálicos. La obra del año 1239 describe el método utilizado y termina diciendo: impreso para la eternidad con caracteres de nueva fabricación. Más tarde el rey Taejong puso en funcionamiento un taller que contribuía a la difusión de la escritura y en 1403, el tercer año de su reinado, se restableció la fundición nacional, el Jujaso, donde se fabricaban caracteres móviles de imprenta, realizó la primera fundición de tipos móviles en bronce. Cabe señalar que la invención de la tipografía coreana es de primordial importancia para la religión, particularmente el budismo, el confucionismo, y el taoísmo.
Durante el reinado del tercer hijo de Taejong, Sejong aumentó el número de centros dedicados a la enseñanza. En la capital existían cuatro escuelas, un colegio para el pueblo y una escuela para la familia real y sus parientes. El libro se convirtió en la herramienta primordial de los esfuerzos de alfabetización que, incluso llegaron a las provincias y pueblos lejanos. Los niños varones tenían que seguir las clases que les inculcaban las nociones básicas como la escritura y la lectura.

Los caracteres fueron mejorando con el tiempo, buscaban una forma más cuadrada y más regular que los precedentes, facilitando así la composición. Durante la invasión japonesa (1592-1598) un general japonés llevó caracteres móviles y libros a Japón, así Japón pudo desarrollar su imprenta, en cambio, la imprenta coreana retrocedió a partir de ese momento, se volvió a la madera para la fabricación de tipos móviles y cada la producción de libros decayó.

Sin duda alguna la dinastía Joseon fue el gran periodo para los libros coreanos, se sabe de 32 fundiciones de caracteres móviles metálicos y más de 350 modelos diferentes. A pesar de las dificultades Corea supo desarrollar e incluso exportar sus técnicas de imprenta. China no utilizó caracteres móviles hasta finales del siglo XV, en 1490, por su parte, Japón adoptó la técnica tipográfica coreana a finales del siglo XVI en 1592.

Egipto creó el papiro y lo exportó a todo el mediterráneo, se usaba para plasmar textos en Egipto, Grecia y Roma. La fabricación del papiro era complicada y dado que las láminas de papiro estaban hechas de dos capas superpuestas, por cada cara discurría una veta distinta, de ahí que se denomine recto donde el grano discurría de forma horizontal y verso en donde el grano discurría en vertical, sin embargo solo se escribía en la cara interna que era la más lisa. Las láminas se pegaban para hacer un rollo.

A partir del siglo I d. C. El pergamino comenzó a competir con el papiro, se cree que surgió en Pérgamo, en la actual Turquía. El pergamino tenía la ventaja de resistir condiciones de humedad, era más duradero y podía doblarse sin romperse, también podía rasparse para limpiarlo y ser reutilizado.

Es muy poco lo que se conoce de las bibliotecas egipcias, un pequeño testimonio es el templo de Horus, donde en uno de los muros están los títulos de 37 libros que eran parte de las bibliotecas.

La escritura alfabética hizo más accesible la lectura y la escritura. El alfabeto griego se desarrolló en el siglo VI y V a. C., era puramente fonético a diferencia de los ideogramas chinos, un erudito chino podía dedicar toda su vida a dominar miles de caracteres, en comparación, el alfabeto griego podía aprenderse en unos días. El uso de la escritura se incrementó en Atenas hacia el siglo V a. C.

En relación con el uso de la escritura y de los libros, se conocían entre los griegos los oficios siguientes:


Estos entre los griegos no se vendían encuadernados sino enrollados. En Atenas los libreros tenían tiendas públicas y en ellas se reunían ordinariamente los literatos para leer los libros nuevos que se escribían.

Entre los romanos se conocían las siguientes profesiones relacionadas con los libros: 

En tiempo de la república las personas acomodadas tenían en sus casas muchos copistas o secretarios, la mayor parte esclavos o libertos, para copiar los manuscritos nuevos. Pero en tiempo de Augusto los vendedores de libros, "bibliopolæ", se introdujeron en Roma y comenzaron a verse tiendas de libros, que solían estar cerca de la entrada de los templos y de los edificios públicos, y en particular en el foro romano. Los libreros fijaban en sus puertas los títulos de las obras que tenían en venta para que con un golpe de vista pudiese cualquiera enterarse de lo que había en ellas.

En la Roma imperial los escritos podían encontrarse en todas partes. La administración cotidiana produjo un flujo constante de documentos, la alfabetización rudimentario era habitual, incluso en las clases bajas, lo que provocó que en el siglo I d. C. hubiera un crecimiento del público lector, ya no se escribía para un círculo de amigo íntimos, sino para un público anónimo, pero la clase alta siguió conservando la cultura literaria oral tradicional.

En el siglo III d. C. empezó el declive del imperio romano y las invasiones bárbaras causaron una contracción de la cultura escrita. Muchas instituciones escolásticas cayeron, a excepción de las mantenidas por la iglesia cristiana.

Durante los primeros siglos de la era cristiana apareció el códice, una de las más importantes y perdurables revoluciones de la historia del libro. Era más compacto y fácil de manejar que los rollos, podía utilizarse ambas caras del papel, lo que le permitía contener más texto. Aunque el códice tenía claras ventajas, el rollo siguió en uso durante varios siglos. La monarquía inglesa continuó usando rollos para registrar sus leyes hasta la edad media.

Con el advenimiento de la imprenta, se inicia la época de expansión bibliográfica, de la modernidad y del pensamiento crítico, facilitado en la actualidad con el acceso a la información en otro tipo de fuentes, tales como periódicos, revistas, Internet, etc. No obstante, el valor del libro es perdurable a través del tiempo.

Antes de la invención de la imprenta era muy costosa la adquisición de una obra importante y se vendía lo mismo que una heredad o casa, por medio de escritura pública y bajo condiciones particulares. Los historiadores citan muchos ejemplos de lo escasos que eran en la edad media los libros y de lo caros que se vendían en Europa. Saint-Loup, abad de Ferrleres, envió dos de sus monjes a Italia el año 855, con el solo objeto de sacar una copia del "Tratado de la Oratoria" de Cicerón y de algunos otros libros latinos, de los cuales no poseía sino algunos fragmentos. En el siglo XII ejemplar de la Biblia y otro de las cartas de San Jerónimo eran poseídos en común por varios monasterios de España, que se servían de ellos simultáneamente. El abate Lebeuf menciona una colección de homilías por las cuales se dieron en Bretaña, en el siglo XI, 2000 carneros y tres moyos de grano. La copia de los manuscritos se hacia entonces con tanta pausa y lentitud, que una copia de la Biblia sacada en cinco meses se consideró como un prodigio de velocidad. Habiendo legado un particular en 1406 a una iglesia de Parts, un breviario para el uso de sus capellanes y para los sacerdotes pobres, se resolvió a fin de conservar tan preciosa alhaja y de cumplir al mismo tiempo los deseos del testador, encerrarlo en una caja de hierro. En el siglo XV todavía no se prestaban los libros sino con muchas garantías y seguridades.

Con el fin de que las obras se conservaran y reprodujeran, se acostumbraba en algunos monasterios a que cada novicio copiara antes de profesar el libro que el superior le señalaba a cuya costumbre debemos muchos libros preciosos de la antigüedad, que sin esta medida no habrían llegado basta nosotros. Los monasterios contribuyeron con este y otros medios a la conservación de muchos escritos y documentos preciosos que se salvaron, en medio de la borrasca universal de la Edad Media, en aquellos monasterios donde se refugiaron y encontraron acogida las ciencias y las letras.

El libro comprendido como una unidad de hojas impresas que se encuentran encuadernadas en determinado material que forman un volumen ordenado, puede dividir su producción en dos grandes períodos: desde la invención de la imprenta de tipos móviles hasta 1801, y el periodo de producción industrializada.

Así libro antiguo es aquel que fue producido en el período manual de la imprenta, es decir que fue impreso con tipos móviles metálicos, estos libros fueron publicados desde la creación de la imprenta en el siglo XV hasta el siglo XIX.

La aparición de la imprenta de tipos móviles en 1444, revolucionó el proceso de producción del libro, aunque algunos procesos de la fabricación se mantuvieron igual que en la época de los scriptoria, la imprenta hizo relativamente más sencilla la producción de libros.

La coexistencia del desarrollo de la imprenta con el comienzo del movimiento humanista y la reforma luterana impulsaron el crecimiento de la industria del libro, puesto que vieron en él un medio de difusión masivo. Pero también existían otras circunstancias que ayudaron a la propagación del libro impreso, el auge de las universidades desarrolló un mercado más amplio para los libros entre las élites intelectuales laicas y religiosas. En medio siglo, la segunda mitad del siglo XV, el libro impreso se convirtió en un importante negocio internacional, los libreros e impresores fueron ante todo empresarios. Pero el libro también debe su expansión a la atención que algunos monarcas y religiosos pusieron en la imprenta, en 1468 el papa Paulo II ordenó imprimir las epístolas de san Jerónimo, por su parte el rey de Francia Carlos VII mandó a Nicolás Jenson a Alemania para aprender la técnica de impresión, con el tiempo los más importantes soberanos en Europa protegieron el desarrollo de la imprenta.

La superioridad de la imprenta sobre la xilografía fue incuestionable, la escritura era regular, impresión a ambas caras, rapidez de impresión y la posibilidad de volver a utilizar los caracteres para imprimir otros textos.

Se puede establecer una cronología del libro antiguo dividida en siglos, tomando como base ciertas características comunes en un siglo determinado:
No es sino hasta mediados del siglo XVIII, una vez que el libro ha superado las dificultades tecnológicas que le impedían convertirse en una mercancía, que este inicia su rápido ascenso dentro del gusto de las minorías ilustradas de la sociedad.

La invención de la imprenta y el desarrollo del papel, así como la aparición de centros de divulgación de las ideas, permitieron la aparición del escritor profesional que depende de editores y libreros principalmente y ya no del subsidio público o del mecenazgo de los nobles o de los hombres acaudalados.

Además, surge una innovación comercial que convierte al libro en una mercancía de fácil acceso a los plebeyos y los pobres, que consiste en las librerías ambulantes, donde el librero cobra una cantidad mensual para prestar libros, que al ser devueltos le permiten al lector-usuario recibir otro a cambio.

El mismo libro, se convierte en un avance que da distinción a los lectores como progresistas en un siglo en que el progreso es una meta social ampliamente deseada y a la que pueden acceder por igual nobles y plebeyos, creando una meritocracia de nuevo cuño.

A pesar de lo anterior, la minoría que cultiva el gusto por el libro se encuentra entre los nobles y las clases altas y cultivadas de los plebeyos, pues solo estos grupos sociales saben leer y escribir, lo que representa el factor cultural adicional para el inevitable auge del libro.

Otro importante factor que fomentó el aprecio por los libros fue la Censura, que si bien solía ejercerse también en periodos anteriores a los siglos XVII y XVIII, es precisamente en esta época cuando adquiere mayor relevancia, puesto que los libros se producen por millares, multiplicando en esa proporción la posibilidad de difundir ideas que el Estado y la Iglesia no desean que se divulguen.

En 1757 se publicó en París un decreto que condenaba a muerte a los editores, impresores y a los autores de libros no autorizados que se editarán, a pesar de carecer de dicha autorización. La draconiana medida fue complementada con un decreto que prohibía a cualquiera que no estuviera autorizado a publicar libros de tema religioso. En 1774, otro decreto obligaba a los editores a obtener autorizaciones antes y después de publicar cada libro y en 1787, se ordenó vigilar incluso los lugares libres de censura.

Estas medidas lo único que lograron fue aumentar el precio de los libros y obligar a los libreros ambulantes a no incluirlos en su catálogo, con lo cual incrementaron el negocio de los libros prohibidos, que de esta manera tenían un mayor precio y despertaban un mayor interés entre la clase alta que podía pagar el sobrevalor, con lo cual se fomentaron en el exterior, en Londres, Ámsterdam, Ginebra y en toda Alemania, las imprentas que publicaban libros en francés. Así fueron editados hasta la saciedad Voltaire, Rousseau, Holbach, Morell y muchos más, cuyos libros eran transportados en buques que anclaban en El Havre, Boulogne y Burdeos, desde donde los propios nobles los transportaban en sus coches para revenderlos en París.

En tanto la censura se volvió inefectiva e incluso los censores utilizaron dicha censura como medio para promover a astutos escritores y editores. Así, por ejemplo, cuando el todopoderoso ministro Guillaume-Chrétien de Lamoignon de Malesherbes revocó la autorización para publicar L'Encyclopédie, fue él mismo quien protegió a la obra cumbre de la Ilustración para después distribuirla de manera más libre, lo mismo hizo para proteger "Emile" y "La nouvelle Éloise".

Normalmente, un libro es impreso en grandes hojas de papel, donde se alojan 8 páginas a cada lado. Cada una de estas grandes hojas es doblada hasta convertirla en una signatura de 16 páginas. Las signaturas se ordenan y se cosen por el lomo. Luego este lomo es redondeado y se le pega una malla de tela para asegurar las partes. Finalmente las páginas son alisadas por tres lados con una guillotina y el lomo pegado a una tapa de cartón. Toda esta tarea se realiza en serie, inclusive la encuadernación.

En el caso de que las hojas no sean alisadas mediante un proceso de corte, se habla de un libro intonso.

Las imprentas más modernas pueden imprimir 16, 32 y hasta 64 páginas por cara de grandes hojas, luego, como se mencionara más arriba, se las corta y se las dobla. Muchas veces el texto de la obra no alcanza a cubrir las últimas páginas, lo que provoca que algunos libros tengan páginas vacías al final del mismo, aunque muchas veces son cubiertas con propaganda de la editorial sobre textos del mismo autor o inclusive otros de su plantilla.

Los importantes avances en desarrollo de software y las tecnologías de impresión digital han permitido la aplicación de la producción bajo demanda (en inglés el acrónimo P.O.D.) al mundo del libro. Esto está permitiendo eliminar el concepto de "Libro Agotado" al poder reimprimirse títulos desde un solo ejemplar, y se está fomentando la edición de libros en tiradas muy cortas que antes no eran rentables por los medios tradicionales.

Como aplicación más innovadora, las librerías electrónicas más reconocidas están además ofertando a todo el mundo libros que no son fabricados hasta que son vendidos. Esto es posible solo por estar dados de alta en los sistemas de producción de compañías internacionales como Lightning Source, Publidisa, Booksurge, Anthony Rowe, etc.

A finales de 1971 comenzó a desarrollarse lo que hoy denominamos libro digital o electrónico. Michael Hart fue el impulsor del Proyecto Gutenberg, (que consistía en la creación de una biblioteca digital totalmente gratis), donde podíamos encontrar obras de autores como Shakespeare, Poe y Dante entre otros, todas ellas obras de dominio público. En 1981 se produce un importante avance, ya que sale a la venta el primer libro electrónico: "Random House's Electronic Dictionary". Sin embargo, fue en marzo de 2001 cuando el libro digital (también conocido como eBook) experimentó su máxima expansión gracias al novelista Stephen King, quien lanzó al mercado a través de la red su novela "Riding the Bullet". La obra, en apenas 48 horas, vendió 400 000 copias, al precio de dos dólares y medio la copia. El mes siguiente Vladímir Putin también sacó a través de Internet sus memorias.

Desde este momento comenzaron a aparecer varias editoriales electrónicas y muchas tiendas virtuales empezaron a incorporar libros electrónicos en sus catálogos.

En el año 2000 se recogían los siguientes datos:
«Si la celebridad de un individuo consiste en que se escriba un libro sobre él, […] Jesucristo es aún el personaje que goza de más fama en el mundo actual», dice el periódico británico "The Guardian". Una investigación que tomó como base los libros de la Biblioteca del Congreso de Estados Unidos, con sede en Washington, D. C., reveló la existencia de 17 239 obras acerca de Jesús, casi el doble que de William Shakespeare, quien alcanza el segundo lugar, con 9801. Vladimir Lenin resulta el tercero, con 4492, seguido de Abraham Lincoln, con 4378, y de Napoleón I, con 4007. El séptimo puesto, con 3595, lo ocupa María, la madre de Jesús, quien es la única mujer entre los treinta principales. La siguiente es Juana de Arco, con 545. Encabeza la nómina de compositores Richard Wagner, tras quien vienen Mozart, Beethoven y Bach. Picasso es el número uno de los pintores, seguido de Leonardo da Vinci y Miguel Ángel. Da Vinci, sin embargo, se lleva la palma en la lista de científicos e inventores, superando a Charles Darwin, Albert Einstein y Galileo Galilei. «No figura ningún personaje vivo en los treinta primeros lugares», agrega el rotativo.


De acuerdo con el contenido los libros se pueden clasificar en:
En las bibliotecas se suele utilizar el Sistema Dewey de clasificación por materias.





</doc>
<doc id="21938" url="https://es.wikipedia.org/wiki?curid=21938" title="Lenguaje unificado de modelado">
Lenguaje unificado de modelado

El lenguaje unificado de modelado (UML, por sus siglas en inglés, "Unified Modeling Language") es el lenguaje de modelado de sistemas de software más conocido y utilizado en la actualidad; está respaldado por el "Object Management Group" (OMG).

Es un lenguaje gráfico para visualizar, especificar, construir y documentar un sistema. UML ofrece un estándar para describir un "plano" del sistema (modelo), incluyendo aspectos conceptuales tales como procesos, funciones del sistema, y aspectos concretos como expresiones de lenguajes de programación, esquemas de bases de datos y compuestos reciclados.

Es importante remarcar que UML es un "lenguaje de modelado" para especificar o para describir métodos o procesos. Se utiliza para definir un sistema, para detallar los artefactos en el sistema y para documentar y construir. En otras palabras, es el lenguaje en el que está descrito el modelo.

Se puede aplicar en el desarrollo de software gran variedad de formas para dar soporte a una metodología de desarrollo de software (tal como el Proceso Unificado Racional, "Rational Unified Process" o RUP), pero no especifica en sí mismo qué metodología o proceso usar.

UML no puede compararse con la programación estructurada, pues UML significa Lenguaje Unificado de Modelado, no es programación, solo se diagrama la realidad de una utilización en un requerimiento. Mientras que programación estructurada es una forma de programar como lo es la orientación a objetos, la programación orientada a objetos viene siendo un complemento perfecto de UML, pero no por eso se toma UML solo para lenguajes orientados a objetos.

UML cuenta con varios tipos de diagramas, los cuales muestran diferentes aspectos de las entidades representadas.

Desde el año 2004, UML es un estándar aprobado por la ISO como ISO/IEC 19501:2005 Information technology — Open Distributed Processing — Unified Modeling Language (UML) Versión 1.4.2.

En el año 2012 se actualizó la norma a la última versión definitiva disponible en ese momento, la 2.4.1, dando lugar a las normas ISO/IEC 19505-1

Después de que la "Rational Software Corporation" contratara a James Rumbaugh de General Electric, en 1994, la compañía se convirtió en la fuente de los dos esquemas de modelado orientado a objetos más populares de la época: "Object-Modeling Technique" (OMT) de Rumbaugh, que era mejor para análisis orientado a objetos, y el Método Booch (de Grady Booch) que era mejor para el diseño orientado a objetos. Poco después se les unió Ivar Jacobson, el creador del método de ingeniería de software orientado a objetos. Jacobson se unió a Rational, en 1995, después de que su compañía Objectory AB fuera comprada por Rational. Los tres metodologistas eran conocidos como los "Tres Amigos", porque se sabía de sus constantes discusiones sobre las prácticas metodológicas.

En 1996 Rational concluyó que la abundancia de lenguajes de modelado estaba alentando la adopción de la tecnología de objetos, y para orientarse hacia un método unificado, encargaron a los "Tres Amigos" que desarrollaran un "lenguaje unificado de modelado" abierto. Se consultó con representantes de compañías competidoras en el área de la tecnología de objetos durante la OOPSLA '96; eligieron "cajas" para representar clases en lugar de la notación de Booch que utilizaba símbolos de "nubes".

Bajo la dirección técnica de los "Tres Amigos" (Rumbaugh, Jacobson y Booch) fue organizado un consorcio internacional llamado "UML Partners" en 1996 para completar las especificaciones del UML, y para proponerlo como una respuesta al OMG RFP. El borrador de la especificación UML 1.0 de "UML Partners" fue propuesto a la OMG en enero de 1997. Durante el mismo mes, la "UML Partners" formó una Fuerza de Tarea Semántica, encabezada por Cris Kobryn y administrada por Ed Eykholt, para finalizar las semánticas de la especificación y para integrarla con otros esfuerzos de estandarización. El resultado de este trabajo, el UML 1.1, fue presentado ante la OMG en agosto de 1997 y adoptado por la OMG en noviembre de 1997.

Como notación de modelado, la influencia de la OMT domina UML (por ejemplo, el uso de rectángulos para clases y objetos). Aunque se quitó la notación de "nubes" de Booch, sí se adoptó la capacidad de Booch para especificar detalles de diseño en los niveles inferiores. La notación de "Casos de Uso" del "Objectory" y la notación de componentes de Booch fueron integrados al resto de la notación, pero la integración semántica era relativamente débil en UML 1.1, y no se arregló realmente hasta la revisión mayor de UML 2.0.

Conceptos de muchos otros métodos orientados a objetos (MOO) fueron integrados superficialmente en UML con el propósito de hacerlo compatible con todos los MOO. Además, el grupo tomó en cuenta muchos otros métodos de la época, con el objetivo de asegurar amplia cobertura en el dominio de los sistemas en tiempo real. Como resultado, UML es útil en una gran variedad de problemas de ingeniería, desde procesos sencillos y aplicaciones de solamente un usuario a sistemas concurrentes y distribuidos.

UML ha madurado considerablemente desde UML 1.1, varias revisiones menores (UML 1.3, 1.4 y 1.5) han corregido defectos y errores de la primera versión de UML. A estas le ha seguido la revisión mayor UML 2.0 que fue adoptada por el OMG en 2005.

Aunque UML 2.1 nunca fue lanzado como una especificación formal, las versiones 2.1.1 y 2.1.2, aparecieron en 2007, seguidas por UML 2.2 en febrero de 2009. UML 2.3 fue lanzado oficialmente en mayo de 2010. UML 2.4.1 fue lanzado oficialmente en agosto de 2011. UML 2.5.1 fue lanzado en octubre de 2012 como una versión "En proceso" que fue formalmente liberada en junio de 2015.

Existen dos clases principales de tipos de diagramas: diagramas "estructurales" y diagramas de "comportamiento". Estos últimos incluyen varios que representan diferentes aspectos de las "interacciones". Estos diagramas pueden ser categorizados jerárquicamente como se muestra en el siguiente diagrama de clases:

Muestran la estructura estática de los objetos en un sistema.
Los diagramas de clase son, sin duda, el tipo de diagrama UML más utilizado. Es el bloque de construcción principal de cualquier solución orientada a objetos. Muestra las clases en un sistema, atributos y operaciones de cada clase y la relación entre cada clase. En la mayoría de las herramientas de modelado, una clase tiene tres partes, nombre en la parte superior, atributos en el centro y operaciones o métodos en la parte inferior. En sistemas grandes con muchas clases relacionadas, las clases se agrupan para crear diagramas de clases. Las diferentes relaciones entre las clases se muestran por diferentes tipos de flechas.
Un diagrama de componentes muestra la relación estructural de los componentes de un sistema de software. Estos se utilizan principalmente cuando se trabaja con sistemas complejos que tienen muchos componentes. Los componentes se comunican entre sí mediante interfaces. Las interfaces se enlazan mediante conectores.
Un diagrama de despliegue muestra el hardware de su sistema y el software de ese hardware. Los diagramas de implementación son útiles cuando la solución de software se despliega en varios equipos, cada uno con una configuración única.
Los diagramas de objetos, a veces denominados diagramas de instancia, son muy similares a los diagramas de clases. Al igual que los diagramas de clases, también muestran la relación entre los objetos, pero usan ejemplos del mundo real. Se utilizan para mostrar cómo se verá un sistema en un momento dado. Debido a que hay datos disponibles en los objetos, a menudo se utilizan para explicar relaciones complejas entre objetos.
Como su nombre indica, un diagrama de paquetes muestra las dependencias entre diferentes paquetes de un sistema.
Diagrama de perfil es un nuevo tipo de diagrama introducido en UML 2. Este es un tipo de diagrama que se utiliza muy raramente en cualquier especificación.
Los diagramas de estructura compuesta se utilizan para mostrar la estructura interna de una clase.

Muestran el comportamiento dinámico de los objetos en el sistema.

Los diagramas de actividad representan los flujos de trabajo de forma gráfica. Pueden utilizarse para describir el flujo de trabajo empresarial o el flujo de trabajo operativo de cualquier componente de un sistema. A veces, los diagramas de actividad se utilizan como una alternativa a los diagramas de máquina del estado.
Como el tipo de diagrama de diagramas UML más conocido, los diagramas de casos de uso ofrecen una visión general de los actores involucrados en un sistema, las diferentes funciones que necesitan esos actores y cómo interactúan estas diferentes funciones. Es un gran punto de partida para cualquier discusión del proyecto, ya que se pueden identificar fácilmente los principales actores involucrados y los principales procesos del sistema.
Los diagramas de máquina de estado son similares a los diagramas de actividad, aunque las anotaciones y el uso cambian un poco. En algún momento se conocen como diagramas de estados o diagramas de diagramas de estado también. Estos son muy útiles para describir el comportamiento de los objetos que actúan de manera diferente de acuerdo con el estado en que se encuentran en el momento.

Los diagramas de interacción incluyen distintos tipos de diagramas:
Los diagramas de secuencia en UML muestran cómo los objetos interactúan entre sí y el orden en que se producen esas interacciones. Es importante tener en cuenta que muestran las interacciones para un escenario en particular. Los procesos se representan verticalmente y las interacciones se muestran como flechas. Los diagramas de secuencia de UML forman parte de un modelo UML y solo existen dentro de los proyectos de modelado UML. 
El diagrama de comunicación se llamó diagrama de colaboración en UML 1. Es similar a los diagramas de secuencia, pero el foco está en los mensajes pasados entre objetos.
Los diagramas de sincronización son muy similares a los diagramas de secuencia. Representan el comportamiento de los objetos en un marco de tiempo dado. Si es solo un objeto, el diagrama es directo, pero si hay más de un objeto involucrado, también se pueden usar para mostrar interacciones de objetos durante ese período de tiempo.
Los diagramas generales o globales de interacción son muy similares a los diagramas de actividad. Mientras que los diagramas de actividad muestran una secuencia de procesos, los diagramas de interacción muestran una secuencia de diagramas de interacción. En términos simples, pueden llamarse una colección de diagramas de interacción y el orden en que suceden. Como se mencionó anteriormente, hay siete tipos de diagramas de interacción, por lo que cualquiera de ellos puede ser un nodo en un diagrama de vista general de interacción.




</doc>
<doc id="21943" url="https://es.wikipedia.org/wiki?curid=21943" title="Slashdot">
Slashdot

Slashdot (literalmente, "barrapunto", abreviado frecuentemente como "/.") es un sitio de noticias orientado a la tecnología. Fue creado en septiembre de 1997 por Rob Malda. Actualmente pertenece a la Open Source Development Network, parte de VA Software. La mayor parte de los contenidos de Slashdot consisten de resúmenes breves de historias de otras páginas web y enlaces a ellas. Los usuarios de Slashdot pueden comentar sobre cada noticia y cada noticia obtiene generalmente entre 200 y 2000 comentarios durante el tiempo en que figura en la página principal. Los resúmenes de cada historia son generalmente provistos por los mismos lectores de Slashdot y los editores son los encargados de aceptar o rechazar cada contribución.

El nombre "Slashdot" fue elegido por la forma curiosa en que suena dentro de una URL. En inglés barra es "slash" y punto es "dot", por lo que al leer "http://slashdot.org" el resultado es "http colon slash slash "slashdot" dot org".

Slashdot corre sobre un programa llamado Slash. El código Slash está liberado bajo los términos de la licencia GNU GPL, por lo que algunos sitios de noticias lo han utilizado.

Todo usuario, a partir de cierto momento, puede moderar las opiniones de otros, calificándolas de divertidas, interesantes, inspiradas, informativas, infravaloradas, sobrevaloradas, trolls, inflamadoras, fuera del tema o redundantes. Las calificaciones afectan a la visibilidad de los comentarios, y a una puntuación o "karma" del usuario que ha sido calificado. Los usuarios con buen karma tienen mayor visibilidad y tienen opción de moderar comentarios de otros con mayor frecuencia.

Todo usuario, a partir de cierto momento, puede metamoderar moderaciones de otros, para evitar que se abuse del sistema de moderación. Los usuarios que hacen moderaciones consideradas injustas por los metamoderadores ven disminuida su capacidad de moderación.

Hay un sinnúmero de bromas internas y temas recurrentes en slashdot, entre ellos:





</doc>
<doc id="21945" url="https://es.wikipedia.org/wiki?curid=21945" title="Cabeza">
Cabeza

La cabeza es la parte superior del cuerpo, y superior o anterior de muchos animales, donde se encuentran algunos órganos de los sentidos y el cerebro: el cuerpo humano está formado por cabeza, tronco y extremidades.
La cabeza (o "testa", que puede ser o bien la cabeza en sí o la frente), de un animal, es la parte anterior del cuerpo que contiene la boca, el cerebro y varios órganos sensoriales (generalmente órganos de visión, audición, olfato y gusto). 
El máximo grado de cefalización se da en los artrópodos (sobre todo insectos) y en los vertebrados; en estos animales, la cabeza está netamente diferenciada del resto del cuerpo y provista de órganos sensoriales muy eficientes. 

Los animales más sencillos, como las esponjas, y los que presentan simetría radial (cnidarios y ctenóforos) no poseen cabeza, pero sí la tienen la mayoría de las formas con simetría bilateral (Bilateria); estos animales poseen un eje antero-posterior de manera que en la parte anterior del cuerpo se concentran el cerebro y los órganos sensoriales; el grado de cefalización es variable en los distintos filos bilaterales; muchos poseen una cabeza incipiente (platelmintos, anélidos, nematodos, moluscos). Dentro de los bilaterales, hay también grupos sin cabeza como los bivalvos, briozoos, equinodermos, etc.

Los artrópodos son los invertebrados con un grado mayor de cefalización, lo que se traduce en la posesión de un cerebro complejo. El cerebro está formado por la fusión de los tres pares de ganglios de los tres primeros segmentos del cuerpo, de modo que se puede distinguir tres regiones: 

Los escleritos de la cabeza están también fusionados entre sí, formando una estructura compacta denominada "cápsula cefálica".

Una cabeza de insecto típico se compone de ojos compuestos, antenas, y piezas bucales. Estos componentes difieren sustancialmente de insecto a insecto, formando importantes enlaces de identificación. Los ojos en la cabeza se encuentra, en varios tipos de insectos, están en la forma de un par de ojos compuestos con múltiples omatidos. En algunos casos, hay dos o tres ocelos u ojos simples. 

Las antenas en la cabeza del insecto se encuentra en forma de accesorios segmentados, de dos en dos, que por lo general se encuentran entre los ojos. Estos son en diferentes formas y tamaños, en forma de filamentos o en diferentes forma ampliada o Clubbed. 

Los insectos tienen piezas bucales de diferentes formas dependiendo de sus hábitos de alimentación. El labrum es el "labio superior", que es en la zona frontal de la cabeza y es la parte más exterior. Un par de mandíbula se encuentra en la parte posterior del labrum que flanquea el lado de la boca, sucedido por un par maxilas que tienen palpos maxilares. En el lado posterior de la boca es el labio o el labio inferior. También hay una parte de la boca extra en algunos insectos que se denomina como hipofaringe que normalmente se encuentra entre las maxillas.

La evolución de la cabeza, en los vertebrados se ha producido por la fusión de un número fijo de segmentos anteriores, de la misma manera que en otros "animales heterónomamente segmentados". En algunos casos los segmentos o una porción de los segmentos desaparecen. Los segmentos de la cabeza también pierden la mayor parte de sus sistemas excepto para el sistema nervioso. Con el desarrollo progresivo de la cefalización, "la cabeza incorpora más y más de los segmentos adyacentes en su estructura, por lo que en general se puede decir que cuanto mayor es el grado de cepahalization mayor es el número de segmentos que componen la cabeza".

La región anteroinferior de la cabeza, donde se encuentran los ojos, nariz y boca, es llamada cara o rostro, la región superior a esta, frente, y el mentón el extremo inferior. Una vez privada la cabeza de las partes blandas, queda su esqueleto, la cabeza ósea, que se articula con la primera vértebra del raquis, el atlas, mediante el occipital.
Los huesos de la cabeza ósea se organizan en dos grupos claramente diferenciados: cráneo y cara:
Además, en ella se fijan los músculos de la mímica, y se alojan órganos de los sentidos o sus anexos: ojos, fosas nasales, lengua. A su vez, cráneo y cara están formados por diversos huesos, todos ellos pares, excepto cuatro: tres del cráneo (frontal, etmoide, occipital), y uno de la cara (vómer).

 y cocote.


</doc>
<doc id="21946" url="https://es.wikipedia.org/wiki?curid=21946" title="Instituto Tecnológico de Durango">
Instituto Tecnológico de Durango

El Instituto Tecnológico de Durango (ITD) es una universidad pública ubicada en la ciudad de Victoria de Durango. Fue fundado el 2 de agosto de 1948 

El ITD es el primer instituto tecnológico público en la provincia mexicana y es miembro del sistema del Tecnológico Nacional de México. 

Las licenciaturas ofrecidas en el instituto son:

Los programas de posgrado que ofrece son:

El ITD cuenta además con una unidad de Educación a distancia.

El ITD tiene programas deportivos en las disciplinas de fútbol americano, voleibol, básquetbol, atletismo, natación, béisbol, fútbol soccer, karate do, Tae Kwon Do, voleibol de playa, frontenis, tenis y ajedrez.

El programa de Fútbol americano del ITD se fundó en 1973, tiene categorías infantiles, juveniles y ocasionalmente de intermedia. En esta última categoría, en 2008 y 2009 participaron en la Conferencia Guillermo "Chucus" Olascoaga de la CONOFAI. 

Burros Blancos ITD (también conocidos como "Burros del Tecno"), es el nombre genérico de los equipos deportivos representativos del instituto, y en especial del equipo de fútbol americano.




</doc>
<doc id="21948" url="https://es.wikipedia.org/wiki?curid=21948" title="Espejo">
Espejo

Un espejo (del lat. "specullum") es una superficie pulida en la que, después de incidir, la luz se refleja siguiendo las leyes de la reflexión. 

Un espejo es un objeto que refleja la luz de tal manera que, para la luz incidente en algún rango de longitudes de onda, la luz reflejada conserva muchas o la mayoría de las características físicas detalladas de la luz original, llamada reflexión especular. Esto es diferente de otros objetos que reflejan la luz que no conservan gran parte de la señal de onda original que no sea el color y la luz reflejada difusa, como la pintura blanca plana o en una superficie pulida. 

El tipo de espejo más familiar es el espejo plano, que tiene una superficie plana. Los espejos curvos también se utilizan para producir imágenes aumentadas o disminuidas o para enfocar la luz o simplemente distorsionar la imagen reflejada. 

Los espejos se usan comúnmente para el aseo personal o para admirarse a sí mismos, para ver el área que se encuentra detrás y a los lados en vehículos motorizados mientras conducen, para la decoración y la arquitectura. Los espejos también se utilizan en aparatos científicos tales como telescopios y láseres, cámaras y maquinaria industrial. La mayoría de los espejos están diseñados para luz visible; sin embargo, también se utilizan espejos diseñados para otras longitudes de onda de radiación electromagnética. 

El más sencillo es el espejo plano, en el que un haz de rayos de luz paralelos puede cambiar de dirección completamente en conjunto y continuar siendo un haz de rayos paralelos, pudiendo producir así una imagen virtual de un objeto con el mismo tamaño y forma que el real. La imagen resulta derecha pero invertida en el eje normal al espejo.

También existen espejos curvos que pueden ser cóncavos o convexos. En un espejo cóncavo cuya superficie forma un paraboloide de revolución, todos los rayos que inciden paralelos al eje del espejo, se reflejan pasando por el foco, y los que inciden pasando por el foco, se reflejan paralelos al eje.

Los espejos son objetos que reflejan casi toda la luz que choca contra su superficie, debido a este fenómeno podemos observar nuestra imagen en ellos. Los espejos en realidad son cristales que contienen detrás una capa de aluminio (o de otro material, pero es más fácil de aluminio) y reflejan el contenido expresado frente a el.

Los espejos como utensilios de tocador y objeto manual fueron muy usados en las civilizaciones egipcia, griega, etrusca y romana. Fue usado en la cultura hebrea, era parte de la fuente de metal que estaba a la entrada del Tabernáculo de la Reunión. Al lavarse los sacerdotes, podían ver sus imperfecciones (Éxodo 38:7-9; 30:18; escrito aproximadamente en el 1447 a. C). 

Se elaboraban siempre con metal bruñido, generalmente cobre, plata o bronce, a este proceso se le conoce como plateo. Tenían forma de placa redonda u oval, decorada ordinariamente con grabados o relieves mitológicos en el reverso (los romanos carecen de grabados, pero no de relieves) y con mango tallado para asirlos cómodamente; de ellos, se conservan todavía muchos ejemplares en algunos museos arqueológicos. Durante la alta Edad Media, apenas se hizo uso del espejo, hasta que en el siglo XIII se inventó la fabricación de los de vidrio y de cristal de roca sobre lámina metálica (o con amalgama de plomo o estaño que son los "espejos azogados"), sin dejar por esto de construirse los de solo metal hasta el siglo XVIII. 

El espejo, como mueble de habitación, empieza con el siglo XVI, pues aunque durante los dos siglos anteriores se citan algunos ejemplares históricos apenas era conocido y su uso era poco corriente. En dicho siglo, se presenta con marco elegante y pie artístico y ocupa lugar distinguido en el salón como objeto movible y de dimensiones reducidas. Hacia fines del siglo XVII las fábricas venecianas logran construir espejos de gran tamaño y desde entonces sirven como objetos singularmente decorativos en los salones, en los que ocupan un lugar destacado. 

Los espejos modernos consisten en una delgada capa de plata o aluminio depositado sobre una plancha de vidrio, la cual protege el metal y hace al espejo más duradero. Este proceso se conoce como plateado.

Los primeros espejos utilizados por los humanos fueron probablemente pozos de agua oscura, sin gas o agua recolectada en un recipiente primitivo de algún tipo. Los requisitos para hacer un buen espejo son una superficie con un grado de planitud muy alto (preferiblemente, pero no necesariamente con una alta reflectividad), y una rugosidad de la superficie más pequeña que la longitud de onda de la luz. Los espejos fabricados más antiguos eran piezas de piedra pulida, como la obsidiana, un vidrio volcánico natural. Los ejemplos de espejos de obsidiana encontrados en Anatolia (la actual Turquía) se han fechado alrededor del año 6000 a. C. Los espejos de cobre pulido fueron hechos a mano en Mesopotamia desde 4000 aC, y en el antiguo Egipto desde alrededor de 3000 aC. Los espejos de piedra pulida de América Central y del Sur datan de alrededor del año 2000 aC. en adelante. 

Para una imagen formada por un espejo parabólico (o esférico de pequeña abertura, donde sea válida la aproximación paraxial), se cumple que: formula_1 , en la que "f" es la distancia del foco al espejo, "s" la distancia del objeto al espejo, y "s"' la distancia de la imagen formada al espejo, se lee: 

«La inversa de la distancia focal es igual a la suma de la inversa de la distancia del objeto al espejo con la inversa de la distancia de la imagen al espejo» 
y formula_2 , en la que "m" es la magnificación o agrandamiento lateral.

El espejo ocupa un lugar importante en la mitología y las supersticiones de muchos pueblos. La imagen que en él se refleja se identifica a menudo con el alma o espíritu de la persona: de ahí por ejemplo que los vampiros, cuerpos sin alma, no se reflejen en él. Cuando un moribundo está a punto de dejar este mundo, es común que se cubran los espejos, por temor a que el alma del agonizante quede encerrada en ellos. 

El espejo se concibe, así, como ventana al mundo de los espíritus. La leyenda urbana de Verónica aprovecha ejemplarmente esta visión. Viceversa, el mundo de los espíritus tiende a imaginarse como una contrapartida especular del de los vivos. Lewis Carroll desarrolla magistralmente la idea del espejo como entrada a un mundo inverso en la segunda parte de las aventuras de Alicia.

El espejo es también objeto frecuente de consulta: se le juzga capaz de mostrar sucesos y objetos distantes en el tiempo o el espacio. En el cuento de "Blancanieves", el espejo tiene la facultad de hablar y responde a las preguntas que le fórmula la madrastra. J. R. R. Tolkien retoma con su célebre «espejo de Galadriel» la tradición del espejo capaz de mostrar el futuro. En la novela "Harry Potter y la piedra filosofal", de J. K. Rowling, aparece el espejo de Oesed ("Deseo" leído a la inversa), que no refleja la imagen de quien lo contempla, sino sus deseos más profundos.

También es notable el Espejo de la Sabiduría (en el que se refleja «todas las cosas del cielo y de la tierra excepto el rostro de quien se mira en él»), descrito por Oscar Wilde en el cuento «El pescador y su alma».




</doc>
<doc id="21949" url="https://es.wikipedia.org/wiki?curid=21949" title="Eric Cartman">
Eric Cartman

Eric Theodore Cartman, comúnmente llamado Cartman, aunque en algún episodio se mencionó a él mismo como Eric P. Cartman, es un personaje ficticio de la serie de dibujos animados estadounidense "South Park". Fue creado por Trey Parker y Matt Stone. Aunque era un prototipo y modelo base de un corto llamado "The Spirit of Christmas: Jesus vs Frosty" (1992), debutó oficialmente en el episodio "Cartman Gets an Anal Probe", estrenado el 13 de agosto de 1997.

Es uno de los protagonistas, destaca por ser manipulador, malintencionado, rencoroso, racista y discriminador, además de ser inteligente y estratega. Gracias a esto, ha resultado ser uno de los más conocidos en la historia de la animación moderna en la televisión estadounidense. También es el enemigo jurado de Kyle Broflovski (otro de los protagonistas), a quien le hace la vida imposible por ser judío. Este personaje representa la mayoría de los estereotipos negativos asociados a un ciudadano estadounidense.

Eric Cartman es un niño de 10 años estudiante de primaria que vive con su madre en el pueblo ficticio de South Park (Colorado). Es de raza caucásica y con sobrepeso; casi siempre usa ropa de invierno, que consiste en una gorra de lana color turquesa que tiene una franja y bola amarilla, una chaqueta roja, guantes amarillos y pantalones marrones. Cuando no usa su gorra, se aprecia que su cabello es color castaño. 

Es el más desagradable y odioso de su grupo de amigos (Kyle, Stan y Kenny), también es malcriado y egoísta al punto del narcisismo, temperamental, desleal, extremista, xenófobo, racista, chovinista, psicótico, sociópata y especialmente antisemita, insultando a Kyle constantemente y disfrazándose de Hitler en varias ocasiones. Agresivo y burlón incluso con aquellos quienes lo consideran su amigo, razón por la que generalmente es el más solitario de los 4 (de hecho, juega al té con sus muñecos: la rana Clyde, con quien se siente muy allegado; Mariquita Pérez y Peter Panda, entre otros). Cuando se lo propone, logra burlar a sus amigos y salirse con la suya usando su gran capacidad, aunque normalmente lo descubren después de que sus planes se le vayan de las manos. Adora a los gatos, ya que en un capítulo esconde gatos en su ático. En ocasiones es especista, lo que quiere decir que da por sentado a cualquier especie que no sea la humana, pero ha habido episodios en donde no lo es, tal vez por su amor por los gatos y por su gran racismo y desprecio por otros humanos.

Si bien Cartman cumple el papel de antihéroe en la serie, ha habido episodios en donde ha desarrollado una mentalidad extremadamente psicópata, apática e insensible con cualquier persona en general. Uno de los episodios donde probablemente alcanzó a la cumbre de su maldad fue en "Scott Tenorman Debe Morir" (de la quinta temporada), donde, por venganza, ingenió un plan con el que provocó la muerte de los padres de un adolescente, los cortó en pedazos, los cocinó en chile y se los dio al mismo chico a comer. Desde ese famoso y recordado episodio, Cartman pasó a ser el antagonista principal de la serie y demostrando que su maldad no era en sí la "inocencia" de un niño. Aunque Cartman ha demostrado tener cierta parte de bondad, por ejemplo en "El Gran Tetaje"" (de la duodécima temporada) se mostró preocupado por Kenny cuando "queseo", y salvó a algunos gatos del barrio. En "Liberen a Wilsix" se unió a los chicos para liberar a una ballena y contuvo a Kyle cuando creyó que la ballena iba a morir. (La cual posteriormente murió porque la enviaron en un cohete a la luna).

La pésima dieta de Cartman le condena a la obesidad y sus amigos, para burlarse, le llaman normalmente "culo gordo" o "culón". Él siempre contesta que no está gordo, sino que es "de huesos anchos" o "fuertecito". Son características de él su papada y su cuerpo, que es el doble de ancho que el de sus amigos. Kenny trabajó para él en los primeros episodios, ya que cuando tienen que hacer una pareja en grupo, Stan va con Kyle. A partir de la séptima temporada, el único que trabaja para él es Butters, ya que lo considera su amigo y no sabe que Cartman le traiciona, aunque en la mayoría de los casos Cartman termina más humillado que él.

Su madre, Liane Cartman, es una reina del porno, además de que es hermafrodita, que resultó ser su padre en el episodio dos de la segunda temporada "La madre de Eric Cartman sigue siendo una puta sucia" (aunque es una madre muy dulce al mismo tiempo).

En el episodio "201", se reveló la verdadera identidad del padre de Eric Cartman: Jack Tenorman, exjugador de los Broncos de Denver, a quien Cartman asesinó junto a su esposa y se lo dio de comer a Scott. (Quien resulta ser su medio hermano).

En otro capítulo confiesa estar enamorado de una niña llamada Patty Nelson.

A pesar de su enemistad con Kyle, se suelen considerar amigos. Kyle es la razón por la que Cartman odia a los judíos y a los pelirrojos. Por otro lado, Kyle a veces siente envidia por las ideas ingeniosas de Cartman y hace todo lo posible para que sus planes se arruinen. En un episodio, cuando a Cartman le diagnostican sida, Kyle no puede contener su risa y tiene que salir de la habitación para reírse en el pasillo, mientras todos los que están adentro escuchan en silencio las carcajadas de Kyle. En pocos episodios se han visto como buenos amigos, pero Cartman siempre desea que Kyle sea humillado. A diferencia de Butters, Kyle no cae tan fácil en sus bromas por ser inteligente y casi nunca teme enfrentarse a él. Sin Kyle, Cartman no tendría de quien burlarse o a quien insultar, por lo que su vida sería aburrida.

Cartman también fue el superhéroe conocido como "El mapache", que "salva el crimen" confundiendo las cosas y le importaba más ser famoso y reconocido, rivalizando con el superhéroe Mysterion quien era más adorado, y resultó ser Kenny. Posteriormente forma un grupo de superhéroes con sus amigos, pero él está solamente interesado en ser famoso y chantajear al Capitán Hindsight (el superhéroe más amado de South Park) que en salvar al crimen y las personas, por lo que sus amigos deciden sacarlo. Luego, creyendo que él es el héroe y que sus amigos fueron "hechizados por un poder maligno", decide hacer alianza con el dios Cthulhu, quien se toma como la deidad más malvada del universo creyendo que debía "unirse al enemigo", y manipula a Cthulhu para destruir sinagogas, matar hippies y a Justin Bieber, y desterrar a sus amigos al oscuro olvido, pero luego él y Cthulhu son derrotados por Mint-berry Crunch, uno de los superhéroes del grupo que sí tenía un superpoder.

Cartman se ha hecho famoso por las malas acciones que hace en la serie. Aquí se incluyen algunas de ellas:

Primera temporada



Segunda temporada


Tercera temporada










Cuarta temporada






Quinta temporada




Sexta temporada





Séptima temporada




Octava temporada









Novena temporada






Décima temporada









Undécima temporada





Duodécima temporada






Decimotercera temporada






Decimocuarta temporada






Decimoquinta temporada








Decimosexta temporada 


Decimoséptima temporada



Vigesimosegunda temporada


Ambos se han mostrado cierto respeto y afecto mutuo durante esta temporada. Ejemplo de ello son las constantes tomadas de mano por parte de Cartman hacia Kyle durante el capítulo.


</doc>
<doc id="21950" url="https://es.wikipedia.org/wiki?curid=21950" title="Stan Marsh">
Stan Marsh

Stanley "Stan" Marsh es un personaje de la serie de televisión llamada South Park. Es interpretado y basado en el cocreador de la serie Trey Parker. Stan es el protagonista de la serie y uno de los personajes principales junto con sus amigos Kyle Broflovski, Kenny McCormick y Eric Cartman. Debutó en televisión cuando South Park se emitió por primera vez el 13 de agosto de 1997, después de haber aparecido por primera vez en los cortos "The Spirit of Christmas" creado por Parker y Matt Stone, en 1992 ("Jesús vs. Frosty") y 1995 ("Jesús vs. Santa").

Stan es un estudiante de cuarto grado que comúnmente tiene excéntricas experiencias no muy típicas de la vida tradicional de un pequeño pueblo en su ciudad natal ficticia de South Park, Colorado. Stan es generalmente representado como amable, eficiente, servicial y relajado. A menudo comparte con su mejor amigo Kyle un papel de liderazgo como protagonista del show. Stan no tiene reservas al expresar su distinta falta de estima por los adultos y sus influencias, ya que los residentes adultos de South Park rara vez hacen uso de sus facultades críticas.

Al igual que los otros personajes de South Park, Stan es animado por computadora para emular el método original del programa de animación de recorte. También aparece en la película (1999), así como en los medios y mercancías relacionados con South Park. Mientras que Parker y Stone describen a Stan un chico normal con tendencias infantiles comunes, su diálogo a menudo tiene la intención de reflejar posturas y puntos de vista sobre temas más orientados a adultos y ha sido citado frecuentemente en numerosas publicaciones por expertos en los campos de política, religión, cultura popular y filosofía.

Stan vive en South Park con su familia, compuesta por su padre Randy, un geólogo, su madre Sharon, una secretaria en una rinoplastia clínica, su hermana mayor Shelley de 12 años de edad, que lo intimida y golpea regularmente, y su centenario abuelo Marvin en silla de ruedas, que llama a Stan "Billy" y ha tratado de influir en Stan para cometer una muerte misericordiosa sobre él. Él asiste a la clase de cuarto grado de primaria del Sr. Garrison. Durante los primeros 58 episodios de la serie (1997 antes del episodio de la cuarta temporada "4.º Grado" en el 2000), Stan y los otros personajes principales estaban en el tercer grado. Stan es con frecuencia avergonzado y/o molestado por las travesuras de su padre y los frecuentes actos de ebriedad en público. Stan tiene una relación como el sobrino de su tío Jimbo recibiendo una atención moderada en la primeras dos temporadas de la serie.

Entre los personajes principales de la serie, Kyle se define como el único niño judío, Cartman es reconocido por su obesidad y su naturaleza cruel, y Kenny se caracteriza por ser pobre y las muertes que con frecuencia le ocurren. En lugar de tener un rasgo distintivo importante, Stan es retratado (en palabras de la página web oficial de la serie) como "normal, promedio, americano, niño común".

Stan es el modelo de Parker, mientras que Kyle sigue el modelo de Stone.Y el Bart Simpson del día de hoy sigue el reflejo de una mezcla entre Stan y Cartman. Stan y Kyle son los mejores amigos, y su relación, que tiene por objeto reflejar la amistad en la vida real entre Parker y Stone, es un tema común en toda la serie. Los dos tienen sus desacuerdos, pero siempre se reconcilian sin ningún daño a largo plazo para su amistad. Como es el caso con sus otros amigos y compañeros de clase, Stan está frecuentemente en desacuerdo con Cartman, resintiendo el comportamiento de Cartman y abiertamente burlándose de su peso. Stan también comparte una estrecha amistad con Kenny, mientras que Kenny confiesa que Stan es uno de "los mejores amigos de un hombre podría tener". Stan puede entender la voz apagada de Kenny a la perfección, y por lo general, exclama el lema "¡Oh Dios mio, han matado a Kenny!", a raíz de una de las muertes de Kenny, que permite a Kyle seguir con "¡hijos de puta!". Stan es el único personaje en el grupo que tuvo una novia estable, Wendy Testaburger, y su relación fue un tema recurrente en las temporadas anteriores de la serie. A pesar de la conciliación y la declaración que volvieron de nuevo en el episodio de la temporada de 11 (2007) "La Lista".


</doc>
<doc id="21951" url="https://es.wikipedia.org/wiki?curid=21951" title="Kyle Broflovski">
Kyle Broflovski

Kyle Broflovski es un personaje ficticio de la serie animada estadounidense "South Park", fue creado por Trey Parker y Matt Stone y apareció por primera vez como un prototipo en un corto animado llamado "The Spirit of Christmas: Jesus vs. Frosty" (1992). Más tarde, debutó oficialmente en el episodio "Cartman Gets an Anal Probe" (1997). Kyle es uno de los personajes principales de la serie, junto con Stan Marsh, Kenny McCormick y Eric Cartman. El cocreador Matt Stone ha afirmado que Kyle está basado en él cuando era más joven. De hecho, el apellido Broflovski está basado en el apellido de sus padres (Broflovski).

Kyle comúnmente tiene experiencias extraordinarias que no son típicas de la vida de un pueblo pequeño convencional en su ciudad ficticia de South Park, Colorado. Kyle se distingue como uno de los pocos niños judíos en el programa, y debido a esto, a menudo se siente como un forastero entre el grupo principal de personajes. Su representación en este papel a menudo se trata de manera satírica, y ha obtenido tanto elogios como críticas de los espectadores judíos.

Kyle es un niño de 10 años y estudiante de primaria. Usa ropa de invierno, que consiste en una ushanka color verde, una chaqueta naranja con bolsillos al frente, pantalones verde oscuro y guantes verdes. Casi nunca es visto sin su gorro, pero se puede ver que tiene un peinado tipo afro y pelirrojo. Sufrió de diabetes durante un tiempo muy corto hasta que se le trasplantó un riñón de Eric Cartman, que se le extrajo mediante el engaño de sus otros amigos.

La frase más famosa de Kyle es el grito "¡Hijos de puta!" (en el primer doblaje de México decía "mondrigos") después de la frase de Stan "¡Oh Dios mío, mataron a Kenny!". También en un episodio él mismo asesina a Kenny y dice "¡Oh Dios Mío, maté a Kenny!, ¡Hijo de Puta! ".que es similar al clásico "pequeño demonio" (en la segunda película también decía "mondrigo") de Homer Simpson medio segundo antes de estrangular a Bart. Otra de sus expresiones famosas figura el tradicional ""¿Listo Ike?, patea al bebé"", precedido de Ike diciendo ""No patea al bebé"" y Kyle le responde diciendo ""¡patea al bebé!"" y pateando a Ike mandándolo a volar (literalmente).

La primera vez que Kyle se enamoró fue, de una chica llamada Rebecca (conectada al "mono de la fonética"). Con ella se mostró muy cariñoso, diciéndole cosas como "que sea ella misma". Esto le abre la mente a Rebecca, causando que ella vista y actúe como se le suele decir "como una zorra", incluso ella toma a algunos chicos y les roba un beso. Rebecca no vuelve a aparecer en la serie luego de este episodio. En otro capítulo, se enamora de la maestra sustituta, al igual que los otros chicos, pero esta resulta enviada al sol por Wendy. Además, Chef les cuenta a los chicos de que la maestra es lesbiana. En el capítulo "Cartman Finds Love", Kyle se ve interesado en la nueva alumna Nichole Daniels, lo cual era correspondido. Nunca llegaron a ser algo debido a que Cartman estaba haciendo todo lo que podía para que Nichole saliera con Token, incluso llegando a encerrarlos juntos por toda una noche en la escuela y decirle a Nichole de que él y Kyle tienen una relación amorosa, lo que hace que ella y otras chicas inviten a Kyle a salir de compras y cosas así, y que Kyle crea que todas las niñas sienten algo por él. Al final Kyle se entera de la verdad, invita a Nichole a ver un partido de baloncesto, pero en eso Cartman interrumpe y canta una parodia de "I Swear", causando de que Nichole y Token se reconciliaran y que Kyle se enoje como nunca. En la temporada 19 capítulo 10, Kyle se enamora de Leslie a la cual protegía del PC Principal. Más tarde, en la temporada 21, Kyle sale por un rato con la en ese momento novia de Eric, Heidi Turner. No llegan demasiado lejos, ya que Heidi regresa con Cartman. A pesar de que Kyle no le va muy bien con más chicas, demuestra ser una persona o individuo listo y sagaz defensor de los derechos y buen amigo.

Kyle es el más listo de los cuatro protagonistas pero es más ingenuo que Stan. Su más cercano amigo del grupo es Stan Marsh. Kyle y su hermano menor adoptivo, Ike Broflovski, son los únicos niños judíos en el pueblo, aunque en el primer episodio de la serie aparece un niño con kipá, que afirma que su papá es un abogado. A diferencia de su mejor amigo Stan, Kyle es más precavido y tiende a involucrarse menos en situaciones peligrosas. A veces muestra mejores valores morales que el resto de sus amigos, y tiene una obsesión por la higiene y fobia al pis. Por ser el único niño judío (además de su hermano menor), Kyle es frecuentemente objeto de los insultos de Eric Cartman, ya que este último es antisemita. Sin embargo, Kyle parece no tener miedo de enfrentarse a Cartman y lo ha atacado en repetidas ocasiones. Cuando Cartman y Kyle discuten, usualmente Kyle es el ganador del argumento, aunque ha habido excepciones. A pesar de su aparente odio hacia Cartman, Kyle aún lo considera un amigo de cierta forma. De hecho, es el optimismo idealista de Kyle el que demuestra que en todas las personas hay un lado bueno, incluso en Cartman. Casualmente es el primero en descubrir los planes de Cartman para engañar a las personas o ganar dinero aunque nadie lo ve de esa manera y le trae varios problemas sociales. Kyle ha salvado la ciudad y a sus amigos en muchas ocasiones junto a Stan, generalmente uno de ellos o ambos dicen una reflexión al final del episodio, pero los adultos pocas veces se toman esto en serio.

Su madre, Sheila, es muy sobreprotectora y exagera todos los problemas cada vez que su hijo se ve "amenazado" por ellos. Instigó una guerra entre Estados Unidos y Canadá en la película "" por la indignación que le causó la extremadamente escatológica película de los canadienses Terrance y Phillip.

Su hermano menor Ike es canadiense, cuando se dio cuenta de ello, Kyle no lo aceptó, pero luego llegó a apreciarlo. Su padre, Gerald es un abogado, aunque en un capítulo quiso ser un delfín. Kyle asegura que a pesar de la profesión de su padre, su familia no es tan rica como la familia de Token Black, cuyo padre es también un abogado.



</doc>
<doc id="21952" url="https://es.wikipedia.org/wiki?curid=21952" title="Kenny McCormick">
Kenny McCormick

Kenneth "Kenny" McCormick, interpretado por Matt Stone, es un personaje ficticio de la serie animada "South Park". Es famoso porque no se le puede entender bien cuando habla debido a la capucha de su abrigo (de ahí que el momento en que se quita la capucha al final de "" fuera "dramático"), aunque en muy pocos episodios sí se le entiende, pero habla muy poco. También es conocido por morir en casi todos los episodios (hasta la quinta temporada) a lo cual normalmente sigue una exclamación de "¡Oh, Dios mío! ¡Mataron a Kenny! ¡Hijos de puta!" por parte de sus amigos Stan y Kyle.

En el episodio 13 de la quinta temporada ("Kenny Dies") muere de forma aparentemente definitiva tras una enfermedad muscular de carácter terminal, pero vuelve a aparecer en el episodio 17 de la sexta ("Red Sleigh Down").

En la decimotercera temporada se descubre que Kenny tiene una doble identidad y que trabaja en las noches siendo un superhéroe llamado Mysterion, Cartman fue el primero en percatar su presencia pero en un principio sospechaba que era Kyle. Mysterion ha aparecido en cuatro episodios El Coon (o "El Mapache"), un especial llamado El Coon vs Coon y Amigos (que consta de 3 capítulos de South Park), El Chico Pobre y La Precuela de la Franquicia.

Le gusta ver revistas pornográficas y es uno de los más listos y audaces del grupo. Kenny es uno de los personajes menos violentos que hay en la serie; se caracteriza principalmente por ser un personaje reservado, tranquilo, gracioso y excéntrico amante de la pornografía (llegó incluso a sacrificar su vida por sus amigos). En un capítulo, llamado "Pelea de inválidos", se insinuó que el origen de sus repetidas muertes era su vestimenta, pero se cree que también es porque sus padres asistieron a una reunión de Cthulhu, al principio tuvo una novia llamada Kelly en la temporada 3, pero en la temporada 13 salió con Tammy Warner hasta que murió de sífilis debido a que Tammy "se la chupó". Actualmente esta en una relación. 
Kenny es un personaje muy reservado , pero a veces se descontrola. En especial si se trata de su hermana menor o de su novia.
Kenny muere en casi todos los capítulos, pero siempre volviendo a revivir, apareciendo nuevamente en su cama, por lo que en un capítulo se dice que es inmortal. 

Hasta 2001 en la temporada 5, Kenny moría en casi todos los capítulos. A partir de ese año, sus muertes se volvieron inusuales y desapareció la exclamación que sus amigos hacían después de su muerte.
En su grupo no tiene mejor amigo a pesar de que en ocasiones considere a Cartman como tal, ya que este a veces lo trata mal especialmente por ser pobre. Su relación con Stan y Kyle es buena, pero se ha visto en pocos episodios. Kenny normalmente sigue a sus amigos y juega con ellos sin tener mucha relevancia, y solo habla para algunas situaciones especiales. En algunos episodios sí se le entiende lo que dice pero habla con una voz muy discreta y de baja intensidad, excepto cuando es Mysterion, con una voz más grave.

Cortos "Espíritu de Navidad"



Temporada 1












Temporada 2

















Temporada 3
















Temporada 4
















Temporada 5













Temporada 6

Kenny estuvo ausente en esta temporada debido a su muerte "permanente", aunque en un episodio de esta temporada, Rob Schneider es poseído por el alma de Kenny al comerse un asado que tenía atrapada su alma, y muere como en en "Aumento de Peso 4000": Kenny aparece de nuevo al final del último episodio de esta temporada.
Temporada 7

Temporada 8

Temporada 9



Temporada 10

Con excepción de la muerte de su personaje en World of Warcraft, esta es la primera temporada en la que no muere Kenny.
Temporada 11

Temporada 12

Esta es la segunda temporada en la que Kenny no muere.
Temporada 13



Temporada 14



Temporada 15

Temporada 16

Temporada 17

Con excepción de su muerte y resucitación rápida en la secuencia de anime del "La Princesa Kenny", esta es la tercera temporada en la que no muere Kenny.
Temporada 18

Esta es la cuarta temporada en la que no muere Kenny.
Temporada 19

Esta es la quinta temporada en la que no muere Kenny.
Temporada 20

Esta es la sexta temporada en la que no muere Kenny.
Temporada 21

Temporada 22

Temporada 23

Esta es la séptima temporada en la que no muere Kenny.

En una entrega de los Premios (Emmy) a fines de los 90, los cuatro chicos de South Park presentaron un premio. Después de anunciar al ganador, una estatua gigante del Emmy, cae sobre Kenny y lo mata.
Pero al Final del episodio Kenny aparece ileso con Stan, Kyle y Cartman.

En la película "", Kenny intenta probarle a Cartman que los pedos son combustibles (Producto de la película de Terrance y Philip) consigo mismo, quemándose vivo, posteriormente un camión arrojaría accidentalmente una montaña de sal sobre él; pero lo que de verdad causaría su muerte sería cuando, en el hospital, el "Dr. Doctor" remplazaría su corazón por una patata al horno procedente de un microondas que aparentemente debía ser de uso médico pero que uso uno de los enfermeros para calentar su almuerzo, eventualmente esto le devuelve la vida por tres segundos y luego fallece, explotando su cuerpo. 

Durante su estadía en el infierno, Kenny intentaría convencer a Satanás que no se deje manipular por su esposo Saddam Hussein, que tarde o temprano terminaría quitándole el trono, pero Satanás es demasiado débil para enfrentarlo; por lo que Kenny aparecería como fantasma en la habitación de Cartman, aunque este saldría corriendo. 

Al final, cuando Saddam Hussein y Satanás invaden la tierra, de alguna forma Kenny logra colarse y salir a la tierra; donde finalmente convence a Satanás de dejar a Saddam, quien es arrojado a las rocas del infierno donde es atravesado por una estalagmita.por haberle mostrado que lo estaban manipulando y en agradecimiento Satanás está dispuesto a concederle un deseo a Kenny, este desea que todo volviera a ser como antes, aunque eso significaba que Kenny volvería a morir y regresar al infierno, sin embargo a Kenny ya no le importaría ya que estaba haciendo lo correcto no solo por sus amigos si no por la tierra en general, eventualmente se quita la capucha, revela su rostro y cabello para después despedirse de sus amigos, desapareciendo lentamente, para así partir solo que esta vez, Kenny entraría al cielo de manera definitiva por lo que se ganaría sus Alas y su Aureola (Las cuales son otorgadas por ángeles femeninos desnudos) confirmando que su deseo prácticamente limpio el resto de sus pecados exitosamente.

En el episodio 4x05 Cartman se une a Nambla se ve cuando Kenny muere, la madre de Kenny tiene otro hijo después (si Kenny no muriese tendría un total de 53 hermanos hasta ese episodio), este evento se repite otra vez en el capítulo de tres partes en la temporada 14 "El Coon vs Coon y Amigos", cuando Kenny admite ser inmortal en el final de ese episodio se ve cómo su madre lo vuelve a parir y lo deja sobre la cama, se infiere que crece rápidamente y recupera su memoria (o bien nunca la pierde). Este poder lo tiene porque sus padres asisten a una reunión del culto a Cthulhu, muy probablemente y por coincidencia de fechas, cuando la mamá de Kenny está embarazada de él.

En el episodio Pelea de Inválidos, se insinúa que la causa a las muertes de Kenny es su vestimenta. Timmy regaló a Jimmy una capucha naranja similar a la de Kenny; Jimmy por poco muere en 7 ocasiones:

Pero está la teoría de que Timmy le robó a Kenny la capucha (o tal vez una de ellas) para dársela a Jimmy y luego este casi muere debido a que la capucha esta impregnada con la esencia maldita de Kenny.

Adicionalmente en Campamento de/para gordos un niño vestido con la capucha de Kenny muere dentro del útero de la Srta. Verónica Crabtree. Sin embargo, se pone en duda que la causa de la muerte sea la capucha, sino más bien lo estrecho del útero de la Sra. Crabtree y la falta de oxígeno que hay ahí, pues ahí ya había muerto antes otro niño sin usar tal capucha. 

Pero curiosamente en el episodio de Los Jeffersons y Pee, Kenny muere pero sin su vestimenta, solo llevaba puesto una mascara en Los Jeffersons y solo va con traje de baño en Pee pero al final raramente aparece con su traje de baño y con una toalla en la cabeza. Eso desmiente que las muertes de Kenny se deban a la capucha.

El cocreador de la serie Trey Parker explicó que las continuas muertes (y resurrecciones) de Kenny y la falta de interés de sus amigos en ellas están basadas en un amigo de infancia: el amigo de Parker era el niño más pobre del vecindario y también se llamaba Kenny; debido a que era muy pobre constantemente no podía ir a la escuela, así que cuando Parker y sus otros amigos esperaban el autobús y notaban su ausencia decían a manera de broma que había muerto y seguían con su día; tras unos días Kenny volvía a aparecer sin que nada hubiera pasado.

Kenny también es famoso por casi nunca aparecer sin su capucha anaranjada, aunque se la quita en la película (mostrando que es rubio) y en la segunda, la cual se ve durante más tiempo. 


















</doc>
<doc id="21954" url="https://es.wikipedia.org/wiki?curid=21954" title="Manuel de Landa">
Manuel de Landa

Manuel de Landa (Ciudad de México, 1952) Escritor, artista y filósofo mexicano radicado en Nueva York quien cuenta con una muy variada y excepcional obra multidisciplinar. Ha escrito intensamente acerca de dinámicas no lineales, teorías de auto-organización, vida e inteligencia artificial (A.I), teoría del caos, arquitectura e historia de la ciencia. Actualmente, de Landa es profesor de la Escuela de Graduados de Columbia University en Nueva York en el área de arquitectura y es titular de la Cátedra Gilles Deleuze en la European Graduate School en Saas-Fee, Suiza. Se trasladó a New York en 1975 donde se hizo director de cine. En 1980 se interesó por la informática, fue programador pionero y realizó arte con el computador, cuando resalta como uno de los más destacados teóricos en el campo de la cibernética. 

En 1975, Manuel de Landa llega a la Ciudad de Nueva York para estudiar cine. Al tercer año de carrera logra que sus películas sean exhibidas en la Bienal del Museo Whitney y posteriormente, en 1979 uno de sus cortos es elegido para el Festival de cine de Nueva York. Sin embargo, decide dejar la carrera por las exigencias de tiempo.

Más tarde, De Landa decide reinventarse y adquirir una computadora Cromemco de 64k la cual debía ser previamente ensamblada para su uso. Al no haber software disponible para la máquina, De Landa comenzó a interesarse en la programación, iniciando en Basic para continuar en Pascal.

Estas dos etapas de su vida, en adición a su interés por las matemáticas y la filosofía de Deleuze, Foucault y Guattari lo impulsaron finalmente a dedicarse a la filosofía. En 1981 Escribe un primer ensayo titulado "Wittgenstein en el cine" y entre 1986 y 1989 escribe su libro La guerra en la era de las máquinas inteligentes. Más tarde, en 1997, publica "Mil años de historia no lineal", en donde hace un profundo análisis sobre la historia humana, la cultura y la relación que existe entre ambas desde las diferentes perspectivas geológica, biológica, ecológica, tecnológica y social.

De Landa está fuertemente influenciado por el pensamiento de Gilles Deleuze y Felix Guattari. Es uno de los representantes del nuevo materialismo, reinterpretando y reelaborando la filosofía y los conceptos de Deleuze. De Landa afirma que la ciencia puede estar al servicio de la filosofía siempre que ésta sea materialista, pues la filosofía fenomenológica no lleva a nada más que a una especulación vacua de la realidad.

Para conocer la realidad, primero hay que estar conscientes de nuestra materialidad, haciéndola nuestra y comprendiéndola. La morfogénesis (del griego ‘morphê’, forma y ‘génesis’ creación, literalmente el “origen de la forma”) es, en su filosofía, la producción de estructuras estables surgidas de flujos materiales y representa un concepto fundamental para la comprensión de dicho materialismo.

Este nuevo tipo de materialismo se basa en los tres estados de la materia –que coexisten sin eliminarse mutuamente–, asegurando que el líquido es el más interesante, pues se reinventa, se autoorganiza, cambia y crea su propia forma. Esta idea también la aplica a una teoría sobre la ética, la historia, el caos, complejidad social y el arte.

Pronunciándose en contra del uso y sacralización de un solo método, el científico, De Landa habla de tres tipos de razonamiento o estrategias explicativas que en filosofía deberían usarse conjuntamente para una comprensión más clara y profunda de un sistema o fenómeno.

Surgió con Darwin y Mendel y sus descubrimientos sobre selección natural y herencia genética. Este tipo de razonamiento se basa en el factor evolutivo de organismos o sistemas. La regla general que De Landa utiliza para explicarlo es: “Cualquier población de replicadores variables emparentados con algún filtro conducen a la evolución”.

Esta parte del método es utilizado por Deleuze para hablar de los fenómenos, sistemas o cosas en sí, no de sus orígenes, de su funcionamiento, ni comportamiento.

Proviene de la termodinámica, disciplina científica surgida en el siglo XVI que revolucionó la concepción de las máquinas, pues, a diferencia de los mecanismos de cuerda que hasta ese entonces habían existido, la moción y energía provenían de la máquina misma.

Este pensamiento parte del principio de que la energía es necesaria para el funcionamiento de todo. Ésta, si se añade a los factores propios del pensamiento poblacional, propiciará la morfogénesis, la creación de cuerpos biológicos y no biológicos en donde se dan intercambios de energía. 

Para explicar lo anterior, De Landa retoma la distinción de las magnitudes en la termodinámica: las extensivas y las intensivas. Las primeras son divisibles (volumen, área, longitud, etc.), las segundas no (presión, temperatura, velocidad, densidad, concentración, etc.). Lo que las magnitudes intensivas aportan a este estudio, es que, cuando en éstas se producen diferencias, como lo sería el cambio de temperatura, se generan cambios y flujos en los procesos, movimiento. De esta forma, las diferencias intensivas funcionan como combustible no sólo en dichos procesos, sino también en otros de mayor escala, como en la historia, el clima, la economía y la evolución. 

Las propiedades o magnitudes intensivas también presentan umbrales o puntos críticos. El agua por ejemplo, presenta dos, al calentarse hasta los 100º C y al enfriarse hasta los 0º C: evaporación y solidificación. Son puntos en los cuales las variaciones en cantidad son también variaciones de cualidad, mismas que se pueden desembocar en eventos morfogénicos.

La parte intensiva del método es utilizada para explicar orígenes.

El pensamiento topológico es utilizado para calcular espacios de posibilidad, o sea, todo fin posible de una cosa en términos de cambios. Todo lo material posee capacidades y tendencias. Las capacidades pueden ser reales –que verdaderamente suceden– o virtuales, que no suceden pero tienen el potencial de suceder. Esta estructura del espacio de posibilidades puede ser representada gráficamente, por medio de los avances en la medición de espacios tridimensionales logrados por Friedrich Gauss y Bernhard Riemann. Por medio de una variedad, lograron deshacerse del plano cartesiano como referencia para la medición de dichos espacios, al utilizar como variable la curvatura inmediata y sus cambios. A este cambio de curvatura, De Landa le llama “velocidad de devenir”, ya que, si la variedad se utiliza como método de representación, mostrará cómo las los sistemas que con ella se calculan cambian y devienen en algo más. 

Los espacios fásicos juegan un importante papel en este modelo, ya que representan los espacios de estados posibles. Para lograr la representación primero se necesita identificar las variables relevantes que repercuten en el sistema. Un péndulo, en este caso, podría variar en velocidad y posición. Posteriormente se debe crear una variedad con tantas dimensiones como grados de libertad (variables) tenga el sistema para después, con puntos distribuidos en este espacio denominados series de estados (trayectorias formadas por “estados del sistema”, que representan puntos en el espacio de posibilidades), trazar el espacio de posibilidad completo; un espacio que dará posible respuesta al comportamiento de sistemas más complejos al acercarse estas trayectorias a los atractores ubicados en un punto de la variedad. Los atractores representan estados estables, de equilibrio o el casi equilibrio del sistema. Sin embargo, si a un sistema se le estudia desde su punto de equilibrio absoluto, no se desplegará su repertorio completo de capacidades virtuales.

Estos tres tipos de razonamiento deben ser utilizados en conjunto para poder aprehender planamente un sistema y su funcionamiento al proporcionar una visión no linear de su desarrollo.

De Landa habla de los procesos históricos como cambios de estado. Al hablar del pensamiento intensivo, se refiere a umbrales críticos como el origen del cambio. La historia, pensada bajo esta lógica, obedecería también a transformaciones, específicamente de estado.

Según De Landa, ha habido dos modelos historiográficos: el basado en la física (específicamente en la termodinámica) y el basado en el evolucionismo. Sin embargo afirma que ambos se han visto limitados, pues el primero responde siempre en términos de equilibrio, y el segundo en términos del “mejor diseño”, dejando así sólo una posible vía para los eventos históricos. De este modo, la propuesta sería alejarse del equilibrio y de la búsqueda del mejor diseño para tener más posibilidades históricas, tomando en cuenta las fluctuaciones menores y propiedades emergentes como determinantes de cambios o bifurcaciones en los sistemas.

De Landa estructura el libro en tres capítulos, cada uno con una distinta narrativa histórica que abarca el periodo de mil años del siglo XI al XX.

De Landa habla de los procesos históricos como cambios de estado. Al hablar del pensamiento intensivo, se refiere a umbrales críticos como el origen del cambio. La historia, pensada bajo esta lógica, obedecería también a transformaciones, específicamente de estado.

Según De Landa, ha habido dos modelos historiográficos: el basado en la física (específicamente en la termodinámica) y el basado en el evolucionismo. Sin embargo afirma que ambos se han visto limitados, pues el primero responde siempre en términos de equilibrio, y el segundo en términos del “mejor diseño”, dejando así sólo una posible vía para los eventos históricos. De este modo, la propuesta sería alejarse del equilibrio y de la búsqueda del mejor diseño para tener más posibilidades históricas, tomando en cuenta las fluctuaciones menores y propiedades emergentes como determinantes de cambios (bifurcaciones) en los sistemas.

En éste, De Landa habla de las ciudades como ecosistemas simplificados. La narrativa se enfoca en los flujos de materia orgánica como gérmenes, plantas y animales. Se trata a las empresas coloniales como medios que “reorientan los flujos de alimentos hacia los territorio de las ciudades” y como “medios por el cual los genes de múltiples especies no humanas han invadido y conquistado ecosistemas extraños”.

Se habla del flujo de los materiales lingüísticos. Por qué algunas lenguas lograron predominar sobre otras por medio de la fluidez a diferencia del estado pétreo de, por ejemplo, el latín culto, que se solidificó al estandarizarse.

En este libro, De Landa traza la historia de la guerra y la tecnología a través de la utilización de armas y bombas inteligentes y su relación con la deshumanización de la guerra, la mistificación de tecnologías, la obsesión de la vigilancia y la conversión de un conflicto en entrenamiento.

Es un intento de De Landa por desafiar el paradigma sociológico de realizar análisis fructíferos por medio de la reducción del estudio en pequeña y gran escala, o sea, desde las acciones particulares de los individuos hasta el comportamiento de las sociedades como un todo. Utiliza la teoría de ensamblajes de Deleuze para estudiar a las entidades sociales a todas las escalas.




</doc>
<doc id="21964" url="https://es.wikipedia.org/wiki?curid=21964" title="Oro (heráldica)">
Oro (heráldica)

En heráldica, oro es la denominación de uno de los dos metales que se emplean en la representación de las armerías; el otro es la plata o argén. Representa al metal homónimo.

Convencionalmente se lo representa mediante los colores oro, dorado o amarillo. A veces se recomienda, para este fin, utilizar un amarillo mezclado algo de color ocre, de manera que tenga un tinte cálido, pero siempre teniendo en cuenta que el color final no debe ser tan rojizo que se lo pueda confundir con el esmalte anaranjado.

En ocasiones el artista puede emplear pintura dorada o un metal dorado para representar al oro heráldico; estas prácticas por lo general se ven en blasones que han sido trabajados con una intención ornamental o especialmente artística.

Cuando no se dispone de colores se representa al oro mediante un patrón de puntos equidistantes alternados respecto de la línea siguiente, según el método atribuido al jesuita Silvestre Pietra Santa. Este es el método de representación que se ve comúnmente en grabados a una tinta.

Siguen tres ejemplos antiguos y notables del uso del oro en heráldica.

De las armas de Federico I Barbarroja derivaría el Escudo del Sacro Imperio Romano Germánico y, a través de este, el actual escudo de Alemania.

Hacia el inicio del Renacimiento se desarrolló un sistema de correspondencias simbólicas para los colores heráldicos que hoy se encuentra en desuso.
Es de notar que hacia 1828 este sistema era considerado absurdo por el heraldista inglés William Berry, aunque el español Francisco Piferrer, en 1858, lo comenta como si todavía fuese válido.

Si bien Jean Courtois, Heraldo Sicilia del Reino de Aragón, menciona en su tratado "Le blason des couleurs" (1414) que cualquiera de estas asociaciones del oro heráldico puede usarse para blasonar, en la práctica es posible que solamente se hayan usado el sistema planetario y el sistema de piedras preciosas. Para Alberto y Arturo García Caraffa (1919), el blasonado con gemas correspondía a los títulos y el de planetas a los soberanos.
Arthur Fox-Davies cita un ejemplo de blasonado con piedras preciosas que data de 1458.

Debajo se dan algunas de las antiguas correspondencias simbólicas del oro heráldico, así como algunos de los nombres «griegos» que se le atribuyeron.
Además, el oro sería «el más noble» de los colores heráldicos.

El otro metal heráldico:

Los principales esmaltes heráldicos:
Y además:


</doc>
<doc id="21965" url="https://es.wikipedia.org/wiki?curid=21965" title="Transrapid">
Transrapid

El Transrapid es un tren de tecnología alemana que se desplaza mediante levitación magnética. El tren circula sobre una viga situada sobre pilares a varios metros de altura sobre el suelo. La vía está constituida por un caballete de hormigón que incorpora un sistema de levitación magnética que eleva el tren a 15mm, de forma que no existe rozamiento. En ambos lados de la vía existen otros electroimanes, cuya función es la de guiar el tren y mantenerlo en la posición correcta.

Tras 37 años de experimentación del Transrapid, según cifras de la Unión Internacional de Ferrocarriles (UIC), se han inaugurado 9400km de líneas de alta velocidad con el sistema convencial rueda-carril y otros 8295km están en construcción.

En 2002, con la presencia del canciller alemán Gerhard Schröder y del primer ministro chino Zhu Rongji a bordo del Transrapid, tuvo lugar su viaje inaugural de 8 minutos desde el centro de Shanghái hasta el aeropuerto internacional de la ciudad, ubicado a 30 kilómetros.

Después de 37 años de pruebas y experimentos, los 31 km del tramo entre el aeropuerto de Shanghái y Pudong constituyen el único maglev de alta velocidad en servicio en el mundo, aunque hay otras líneas maglev operativas como Linimo y Incheon Airport Maglev que no son de alta velocidad. Es una operación antieconómica y que no llega al centro de la ciudad. Ha sido definido por varios medios como un caro transporte para turistas.

Los imanes de suspensión cuentan con generadores de energía que alimentan los equipos embarcados sin necesidad de captarla a través de pantógrafos o contactos de tercer carril, por lo que el tren circula sin contacto físico con la vía. Además, tanto para la tracción como para el freno, el tren se sirve de un motor lineal, que se basa en el mismo principio que un motor síncrono convencional, cuyo estator hubiera sido abierto y extendido a lo largo de la traza, con lo cual no produce un campo magnético rotativo, sino desplazado linealmente. Es así que los electroimanes de suspensión del actúan como el rotor del motor.

La alimentación de energía eléctrica se realiza sólo en los tramos por los que está pasando el Transrapid para evitar pérdidas. La velocidad del tren se regula mediante la intensidad del campo magnético desplazado y su frecuencia trifásica, por lo que el Transrapid puede subir cualquier pendiente a la misma velocidad.

La velocidad máxima comercial del Transrapid es de 430km/h, con lo cual aventaja a los trenes de alta velocidad convencionales. En 2002 sus promotores alemanes destacaron que el Transrapid era, más rápido y silencioso que los trenes como el TGV francés o el Shinkansen japonés. Según estimaciones de 1997, el Transrapid lograba “puntas de consumo” 30% menores que las de los trenes de alta velocidad para potencias similares en esa época.

Por otra parte, la gran desventaja del Transrapid es su alto coste. Otras desventajas son la imposibilidad de utilizar la red ferroviaria existente o de circular a nivel del suelo, la necesidad de túneles de mayor sección, estrictas demandas de gran limpieza de la vía y, además, la lentitud de operación de los cambios de vía (entre 1 minuto y 30 segundos contra 5 segundos o menos en el ferrocarril convencional).



</doc>
<doc id="21968" url="https://es.wikipedia.org/wiki?curid=21968" title="Salasaca">
Salasaca

Salasaka es una parroquia en el cantón de Pelileo, provincia de Tungurahua (provincia), Ecuador. Ubicada específicamente en el lado oriente de la ciudad de Ambato a 12km de distancia. La gente que vive allí es mayormente del pueblo indígena kichwa.

El pueblo kichwa Salasaca, se encuentra ubicado en la provincia de Tungurahua. Su población es de aproximadamente 12,000 habitantes organizados en 24 comunidades y la mayoría hablan el idioma Kichwa. Están organizados en ayllus, integrado por el padre, madre y sus hijos y los hijos políticos. Los hijos desde muy temprana edad son miembros activos en las tareas de producción familiar.

Las costumbres y tradiciones del pueblo de la cultura Salasaca están cargadas de fiestas y celebraciones. Es un pueblo que entre los habitantes de su comunidad, suelen ser muy alegres, aunque son un poco cerrados con las demás comunidades. Es común en esta etnia, celebrar cuando la tierra da buenos frutos y abundantes. También se festeja el nacimiento de un nuevo miembro de la comunidad, el matrimonio, y el solsticio, que es muy respetado.

La mujer Salasaca cuenta con atuendos diferentes acorde a la ocasión; diario y festivo. Como atuendo diario utilizan blusa y anaco negro que llega hasta unos 2 centímetros aproximadamente bajo la rodilla, sujeto con una faja que plasma una cosmovisión andina, una o dos bayetas o chalinas que cubren toda la espalda y sujeta al frente con un tupo o sujetador, collares y aretes que generalmente son herencias de sus ancestros y un sombrero blanco. Los hombres utilizan pantalón blanco, camisa blanca, poncho negro o blanco y sombrero blanco.

Una de las expresiones culturales que identifica a los Salasacas es el tejido de tapices y ponchos de diferentes motivos y diseños elaborados en telares manuales, los cuales relatan sus vivencias. La actividad agrícola es para su autoconsumo y se la realiza en dos pisos ecológicos, el alto y bajo.
El centro social para los Salasacas en la actualidad es la plaza central, allí se localizan la Iglesia católica, el Museo, la Casa Comunal, la Escuela, el Colegio, el Mercado Artesanal y algunos almacenes artesanales, entre otros.

Sin embargo, el centro histórico de los Salasacas era la comunidad de Chilcapamba, lugar donde aun se realizan algunas festividades. El cambio entre estos dos centros ha sido gradual probablemente a efecto de la construcción de la carretera entre Ambato y Baños en 1934 y como evidencia de que el centro de Salasaca había cambiado el 11 de junio de 1989 se celebró en el actual centro por primera vez la fiesta de la Octava Grande (Jatun Utava), festividad que se realizaba en Chilcapamba.

Están organizados por comunas y son 17, las cuales pertenecen al Consejo de Gobierno del Pueblo Salasaca, esta a su vez pertenece al MIT (Movimiento Indígena de Tungurahua) y por ende a la ECUARUNARI-CONAIE, los trabajos se los realiza en minga y las decisiones se toman democráticamente en las Asambleas convocadas por la organización. Gran parte de las comunidades salasacas se encuentran unidas en organizaciones de segundo grado como la Unión de Indígenas Salasaca (UNIS) y en la Corporación de Organizaciones Campesinas de Pelileo.



</doc>
<doc id="21969" url="https://es.wikipedia.org/wiki?curid=21969" title="Copa Davis">
Copa Davis

La Copa Davis es una competición internacional de tenis organizada por la Federación Internacional de Tenis (ITF). A diferencia de la mayoría de los torneos de tenis internacionales, en la Copa Davis no participan jugadores a título individual, sino equipos nacionales compuestos por diversos jugadores designados por su federación nacional deportiva. La Copa Davis, disputada desde 1900, es un torneo masculino realizado anualmente; la Copa Fed es su equivalente para tenistas femeninas.

Es el mayor torneo deportivo anual, pues cuenta con 133 equipos nacionales diferentes durante su edición de 2019. El país con más títulos es Estados Unidos con treinta y dos. El actual campeón es España.

La Copa Davis lleva el nombre de su creador, Dwight Filley Davis, tenista y político estadounidense nacido el 5 de julio de 1879. Junto a Holcombe Ward ganó el campeonato nacional de los Estados Unidos de América en modalidad de dobles durante tres años consecutivos, de 1899 a 1901. En ese periodo, Davis concibió la idea de crear una competición por equipos nacionales en la que se enfrentaran los mejores estadounidenses contra un equipo de las Islas Británicas. El primer encuentro se preparó en 1900 en el Longwood Cricket Club de Brookline, en las afueras de Boston, y vencieron los estadounidenses a los británicos por 3-0. Davis, que jugó con el equipo de Estados Unidos, donó el dinero necesario para la elaboración de una copa de plata según un diseño de Rowlan Rhodes basado en una ponchera rematada con adornos de flores de 33 cm de alto y 43 cm de diámetro. En el interior de esta mal llamada ensaladera se grabó el nombre del torneo: "International Lawn Tennis Challenge Trophy", y en su lateral quedaron anotados los nombres de los participantes. La competición tuvo una continuidad insospechada y, a partir de 1945, año en que murió Davis (28 de noviembre), pasó a llamarse Copa Davis.

La costumbre de grabar en la Copa los nombres de los finalistas y vencedores y el hecho de que no se entregara en propiedad hizo que pronto faltase sitio para nuevas anotaciones. En 1921, Davis donó una bandeja de plata de 95 cm de diámetro como base de la ponchera, en la que se grabaron las finales de 1921 a 1933. Cuando ésta se llenó, se construyó una peana de madera con placas en las que continuó la inscripción de 34 ediciones más. En 1969 fue necesario añadir una segunda peana, más ancha, que disponía de espacio hasta 2002, año en que se agregó una tercera.

A partir de 2019 el torneo tiene un nuevo formato. Dura una semana y se disputa en una ciudad elegida por sorteo.

Los clasificados para esta fase final son 18. En ellos se encuentran los cuatro semifinalistas de la edición anterior, dos países invitados por la ITF. Las doce plazas restantes se cubren mediante una repesca que se juega en febrero. En la fase clasificatoria se encuentran 24 equipos que juegan doce series mano a mano y los ganadores disputan la fase final.

En noviembre, los 18 equipos clasificados luchan por el título. Las selecciones nacionales se distribuyen en seis grupos de tres participantes. Los ganadores de cada grupo y los dos mejores segundos se clasifican para los cuartos de final, los ganadores para semifinales y luego a la final para definir el campeón.

En cuanto al formato de las series, se juega dos individuales y un dobles. En el individual se enfrentan el mejor de cada país y el segundo de cada país. Los partidos se juegan al mejor de tres sets con "tie-break".

Cada DT deberá jugar a cuatro jugadores y un suplente, en caso de que alguno tenga una dificultad. No está permitido repetir jugadores, es decir, si ya jugó un partido en esa serie no puede volver a jugar.


Desde este año el campeón vigente no accede directamente a la final del año siguiente que alberga en su casa.

Fecha de actualización: 26 de noviembre de 2017




</doc>
<doc id="21980" url="https://es.wikipedia.org/wiki?curid=21980" title="InterCityExpress">
InterCityExpress

InterCityExpress, normalmente abreviado como ICE, designa al sistema de trenes de alta velocidad de los ferrocarriles de Alemania que circulan por dicho país y por países vecinos.

El InterCityExpress es el tren más rápido y cómodo de la Deutsche Bahn AG y es considerado como el «buque insignia» de la empresa y sucesor del InterCity (IC). El IC sirvió aproximadamente a unas 180 estaciones que en gran medida usa actualmente el ICE en Alemania y seis países vecinos (Austria, Suiza, Francia, Bélgica, Países Bajos y Dinamarca).
El nombre de marca «ICE» es una de las marcas más conocidas de Alemania, con un conocimiento de la marca cercana al 100%, según el DB.

El nombre «ICE» también se utiliza para los vehículos utilizados en el sistema, que fueron desarrollados específicamente para el sistema a partir de comienzos del decenio de 1980.

En la actualidad hay 259 trenes en cinco versiones diferentes de los vehículos ICE en circulación, llamados ICE 1 (lanzados en 1991), ICE 2 (1996), ICE-T (1999), el ICE 3 (1999) y el ICE-TD (2001-2003, nuevamente en servicio desde 2007). El ICE 3, incluyendo su variante de los modelos, se fabrica tanto por Bombardier como Siemens.

Estos trenes fueron desarrollados a partir del año 1985 por Siemens AG según las indicaciones de los Ferrocarriles Federales Alemanes (Deutsche Bundesbahn o DB). La primera generación, conocida como "ICE 1", alcanza una velocidad máxima de 280 km/h. Los trenes están formados por dos unidades motrices, una en cada extremo, y entre 10 y 14 remolques. La capacidad de los convoyes con 12 remolques es de 645 pasajeros. Los ferrocarriles alemanes utilizan en la actualidad 59 trenes de este tipo.

Posteriormente se desarrolló una variante del primer tren, denominada "ICE 2". La diferencia con el primer tipo consiste en que los convoyes pueden ser divididos en dos mitades iguales, para aquellos trayectos en los que interesa disponer a partir de una determinada ciudad trenes con menor capacidad que se dirigen a dos destinos diferentes. Ello se consigue dotando a los convoyes completos, que disponen de una unidad motriz en cada extremo al igual que los ICE 1, de dos remolques con puesto de conducción situados en la mitad del tren. De esta forma, al dividir el tren en dos, cada una de las dos mitades dispone de una unidad motriz y un remolque con puesto de conducción en el extremo opuesto, lo que le permite circular en ambos sentidos. Estos trenes fueron puestos en servicio en 1997. Los ferrocarriles alemanes cuentan con 44 unidades.

Desde 2000 están circulando los "ICE 3", la versión más moderna y más rápida de estos trenes, que alcanza una velocidad de 330 km/h. Este tren es capaz de subir pendientes de un cuatro por mil sin merma de velocidad. Contrariamente a los modelos anteriores, el ICE 3 no dispone de unidades motrices, sino que el equipo de tracción se distribuye a lo largo del convoy, alimentando los motores en cada uno de los ejes, así que cada coche contribuye a la aceleración del tren. Esa configuración también permite que los compartimentos de los pasajeros lleguen hasta el lugar donde se encuentra el maquinista, ofreciendo la visión hacia la vía a través del cristal que separa los pasajeros del puesto de conducción. Los ferrocarriles alemanes utilizan actualmente 37 trenes de este tipo, y se encuentran en producción otros 13, que son fabricados por el consorcio Siemens AG y Bombardier Transportation.

El diseño fue realizado por el diseñador industrial alemán Alexander Neumeister.

El accidente de Eschede (Alemania)
El 3 de junio de 1998, un tren Intercity Express (ICE) descarriló a 200 km por hora, dejando un total de 101 muertos y otro centenar de heridos. El accidente se produjo en el pueblo de Eschede, situado en el estado federado de Baja Sajonia, cuando el tren ICE que circulaba entre Múnich y Hamburgo-Altona descarriló y chocó contra el pilar de un puente. La investigación posterior determinó que el fallo se produjo en una de las ruedas de un remolque, que colapsó por fatiga de material. 

Se utilizaron ruedas elásticas formadas por tres partes: un anillo exterior de acero (bandaje), separado por una capa de goma del cuerpo de la rueda. Esta construcción sirve para evitar vibraciones. Con el tiempo se producen fisuras en el metal, que son difíciles de detectar. A pesar de conocer los problemas relacionados con esta construcción de rueda, la Deutsche Bahn no tenía implementado un sistema fiable de detectar las fisuras.

Tres minutos antes del accidente, la rueda perdió su anillo exterior que se incrustó en el suelo del remolque, atravesándolo –lo cual fue advertido por un pasajero-. Este hecho provocó, unos 5 km más adelante, que la rueda enganchara un contracarril de un desvío y lo arrancara en toda su longitud, atravesando todo el remolque e insertándose en el remolque inmediatamente posterior. Simultáneamente, la rueda rota cambió la posición del desvío, que se encontraba unos metros antes de un puente de hormigón. Los tres primeros remolques y la cabeza tractora rebasaron el puente, pero el cuarto remolque tomó el desvío que había cambiado la rueda rota del segundo remolque y se desvió del raíl original ocasionando el descarrilamiento de éste y los siguientes remolques. El remolque chocó contra el pilar del puente a tal velocidad que causó su derribo. Los remolques siguientes fueron chocando en zig zag contra el primero.

Después del accidente se reemplazaron las ruedas elásticas con bandaje por ruedas normales monobloc en los trenes ICE. Para compensar las vibraciones de este tipo de ruedas se incorporaron a todos los bogies suspensión neumática.



</doc>
<doc id="21988" url="https://es.wikipedia.org/wiki?curid=21988" title="Shinkansen">
Shinkansen

El Shinkansen ( "nueva línea troncal") es la red ferroviaria de alta velocidad de Japón, operada inicialmente por la compañía Japanese National Railways JNR. Desde que en 1964 se abrió la línea Tōkaidō Shinkansen la red se ha ido expandiendo para conectar la mayor parte de las ciudades de las islas de Honshū y Kyūshū, con una longitud de 3.050 km (incluyendo Mini-Shinkansen) y unas velocidades de hasta 320 km/h.

La palabra "Shinkansen" significa literalmente "Nueva Línea Troncal" y se refiere estrictamente al trazado de las vías, mientras que los trenes propiamente dichos se denominan oficialmente "Super Expresos" (超特急, "chō-tokkyū"), aunque esta distinción es rara incluso en el propio Japón. Inicialmente se llamaron "Súper Expreso de los Sueños" ("Yume no chō-tokkyū"). Al contrario de la red original, el Shinkansen utiliza el ancho de vía estándar (1.435 mm) y se vale de túneles y viaductos para atravesar obstáculos, en vez de rodearlos.

Japón fue el primer país en construir líneas ferroviarias dedicadas para viajes de alta velocidad. Debido al terreno montañoso, la red existente consistía en líneas de vía estrecha de 1.067 mm (3 pies 6 pulgadas), que generalmente tomaban rutas indirectas y no podían adaptarse a velocidades más altas. En consecuencia, Japón tenía una mayor necesidad de nuevas líneas de alta velocidad que los países donde la trocha estándar o el sistema ferroviario de trocha ancha tenían más potencial de actualización.

Entre las personas clave acreditadas con la construcción del primer Shinkansen están Hideo Shima, el ingeniero jefe, y Shinji Sogō, el primer presidente de los Ferrocarriles Nacionales Japoneses (JNR) que logró persuadir a los políticos para que respaldaran el plan. Otras personas importantes responsables de su desarrollo técnico fueron Tadanao Miki, Tadashi Matsudaira y Hajime Kawanabe, con sede en el Instituto de Investigación Técnica Ferroviaria (RTRI), parte de JNR. Fueron responsables de gran parte del desarrollo técnico de la primera línea, el Tōkaidō Shinkansen. Los tres habían trabajado en el diseño de aviones durante la Segunda Guerra Mundial.

Debido a los problemas inherentes a la contaminación acústica, el aumento de la velocidad máxima está siendo cada vez más difícil, particularmente por el "efecto pistón" que aparece cuando los trenes entran en túneles a una velocidad elevada. A pesar de esto en 2015 se aumentó la velocidad de la Tōkaidō Shinkansen hasta los 285 km/h gracias a los trenes N700A, y hay otro aumento programado para 2020, hasta los 360 km/h, usando los trenes E5 y los futuros H5 en parte de la Tōhoku Shinkansen.
Las últimas inauguraciones fueron las del tramo Nagano-Kanazawa en 2015 y el primer tramo de la Hokkaidō Shinkansen, desde Aomori hasta Hakodate (en 2016) a través del Túnel Seikan. También se está trabajando para extender la red: la Hokkaidō Shinkansen desde Hakodate hasta Sapporo en 2031, el ramal de la línea Kyūshū Shinkansen hasta Nagasaki en 2023, y completar la conexión entre Kanazawa y Osaka cuyo primer tramo, hasta Tsuruga, estará finalizado en 2023.

El proyecto de la línea Narita Shinkansen para conectar Tokio con el Aeropuerto Internacional de Narita, iniciado en la década de 1970 pero interrumpido en 1983 después de protestas de los propietarios de los terrenos, fue oficialmente cancelado y eliminado del Plan Básico que delineaba la construcción del Shinkansen.

Las líneas Shinkansen son:


Con posterioridad se añadieron otras dos líneas, conocidas como Mini-Shinkansen (ミニ新幹線), al actualizar y cambiar al ancho estándar (1.435 mm) secciones de línea existentes (Zairaisen) con ancho de 1.067 mm que mantienen el mismo gálibo para el material rodante, de ahí que los trenes deban ser más estrechos que los otros Shinkansen y por eso el apodo de "mini":


Existen una línea con ancho de vía estándar (1.435 mm) que no está técnicamente clasificada como línea Shinkansen, aunque cuenta con servicios Shinkansen durante la temporada de esquí. Se trata de la línea Gala-Yuzawa, técnicamente un ramal del Jōetsu Shinkansen.

Muchas de las líneas Shinkansen fueron propuestas durante el "boom" del inicio de los 70, aunque aún tienen que ser construidas. Se denominan Seibi Shinkansen (整備新幹線) o "Shinkansen Planeado". Una de esas líneas, la Narita Shinkansen hasta el Aeropuerto de Narita, fue cancelada oficialmente, aunque otras tantas continúan en proyecto.


Los primeros trenes Shinkansen comenzaron a circular el 1 de octubre de 1964 a una velocidad máxima de 210 km/h entre Tokio y Shin-Osaka, y tuvieron que pasar 22 años, hasta noviembre de 1986, para que aumentasen su velocidad hasta los 220 km/h. Los siguientes pasos fueron alcanzar los 240 km/h en la Tōhoku Shinkansen en marzo de 1985, y los 275 km/h en el Jōetsu Shinkansen en marzo de 1990. Los 300 km/h fueron alcanzados por la Serie 500 en la Sanyō Shinkansen en marzo de 1997, llegando a unir Osaka y Hakata a 242 km/h de media en 2003, más rápido que ahora. Finalmente se alcanzaron los 320 km/h gracias a la Serie E5 en el Tōhoku Shinkansen el 16 de marzo de 2013. En esta misma línea, y con esos mismos trenes (y los futuros H5), está previsto alcanzar los 360 km/h en el año 2020.

Por otra parte la primera línea, la Tōkaidō Shinkansen, tenía limitada su velocidad máxima a 270 km/h desde 1992 debido al reducido radio de sus curvas (solo 2.500 metros). Sin embargo, desde marzo de 2015 los trenes N700A alcanzan los 285 km/h gracias a su sistema de basculación pasiva que, además, permite pasar esas curvas a 270 km/h en vez de a 250.

Igualmente las velocidades medias fueron aumentando extraordinariamente a lo largo de los años. El trayecto Tokio-Osaka comenzó en los 129 km/h en 1964 para llegar hasta los 218 km/h desde 2015. La línea con mayor velocidad media es la Sanyo, con 234 km/h entre Osaka y Hakata, aunque el trayecto más rápido es el Hiroshima-Okayama, a 241,7. Por contra, las líneas con menor velocidad media son, aparte de las Mini-Shinkansen, son el Jōetsu Shinkansen y el Hokuriku Shinkansen con, respectivamente, 186 y 184 km/h desde Tokio. La nueva Hokkaidō Shinkansen alcanza los 204 km/h desde Tokio hasta Hakodate, y el Kyushu Shinkansen tiene una velocidad media de 198 km/h entre Hakata y Kagoshima.

Los trenes Shinkansen tienen formaciones de hasta 16 coches. Como cada coche puede medir hasta 25 m de largo, los trenes más largos llegan a los 405 m de longitud total, que es el largo máximo para el que se han diseñado los andenes; esta medida es similar a la utilizada en los trenes rápidos europeos. Sin embargo, el gálibo utilizado en las vías de alta velocidad permite trenes de hasta 3,38 m de ancho, que es mayor que el utilizado en Europa, lo que permite el uso de configuraciones de asientos de mayor capacidad (por ejemplo, 2+2 en primera clase y hasta 3+3 en clase turista).

Los trenes Mini-Shinkansen están diseñados para circular por las líneas de alta velocidad, y luego continuar por vías antiguas de ancho 1.067 mm cuyo ancho de vía ha sido modificado para permitir la continuidad del servicio. Sin embargo, estas líneas no han visto modificado su gálibo, y por ello estos trenes son más estrechos en su construcción (2,945 m). Al detenerse en las estaciones de líneas nuevas (diseñadas para trenes más anchos), deben utilizar unas plataformas retráctiles que facilitan el acceso hasta el andén.





Actualizada al 31 de marzo de 2016
N700A en Tokaido: 323

N700 de 8 coches en Sanyo: 271

E5: 177 en Tohoku y 78 en Joetsu

E7/W7 en Hokuriku: 118 (incluye Tsurugi)

800 en Kyushu: 125

Fuente: IHRA

Los ferrocarriles que usan la tecnología Shinkansen no se limitan únicamente a los que existen en Japón.

Taiwan High Speed Rail tiene en circulación desde 2007 30 trenes para 300 km/h Serie 700T, construidos por la Kawasaki Heavy Industries. China tiene en circulación desde 2007 60 trenes para 250 km/h basados en el diseño de la Serie E2, 3 de ellos construidos en Japón por un consorcio formado por la Kawasaki Heavy Industries, la Mitsubishi Electric Corporation y la Hitachi. Para la línea HS1 de conexión de Londres al Eurotúnel, se exportaron trenes construidos por Hitachi, basados en el A Train, para su uso como regionales de alta velocidad bajo el nombre de Class 395 "Javelin". La misma compañía entregará entre 2017 y 2019 trenes eléctricos y duales, class 800 y 801.

La red Shinkansen ha sido (hasta 2012, cuando fue superada por la china) la red de alta velocidad más transitada del mundo. Poco antes del cincuentenario de su inauguración alcanzó la cifra de diez mil millones de viajeros transportados, mientras que Francia alcanzó los dos mil en 2013, después de 32 años.

Número de viajeros (en millones):
Miles de millones de viajeros-km (mucho más preciso, ya que también tiene en cuenta los km recorridos):
(1) Hay viajeros que son contados dos veces al viajar por distintas líneas; así, el total es menor que la suma de las líneas.

Fuentes: 1964/1993 1964/2008 1968/1989 
1964/2011 
2007/2012 y anuarios de las compañías. JR East 2012 y Kyushu 2011 y 2012

A pesar de que originalmente se había pensado hacer circular trenes de pasajeros y mercancías día y noche, lo cierto es que en las líneas Shinkansen sólo circulan trenes de pasajeros. El sistema cierra entre las 0:00 y las 6:00 todos los días para efectuar operaciones de mantenimiento. Los trenes nocturnos que aún circulan en Japón lo hacen en la red de ancho métrico japonés (1.067 mm).

Servicios Shinkansen en marzo de 2015
Nota: las paradas incluyen origen y destino, así que para dos trayectos hay que quitar una.





</doc>
<doc id="21991" url="https://es.wikipedia.org/wiki?curid=21991" title="HMS Bounty">
HMS Bounty

El HMS "Bounty, también denominado HMAV "Bounty, fue un barco de vela de la armada británica en el que tuvo lugar el 28 de abril de 1789 un famoso motín, el motín del Bounty. 

El barco fue construido inicialmente como buque mercante bajo el nombre de "Bethia", aunque más tarde, tras su adquisición por la Royal Navy, fue sometido a importantes reformas para realizar una expedición al mar del Sur transportando una carga de árboles del pan desde Tahití, en la Polinesia, hasta el Caribe, que pudiera servir para alimentar a los esclavos de las plantaciones de Jamaica y de otras colonias británicas de las Antillas que sufrían por entonces de periódicas hambrunas. La utilidad de la planta ya había sido advertida en 1769, por el capitán James Cook que comprobó que además de ser bastante común en aquellas islas servía de alimento a los polinesios. Cuando estas noticias se difundieron tras la muerte del famoso navegante y explorador, algunos plantadores antillanos plantearon al Presidente de la Royal Society, el naturalista "sir" Joseph Banks, la posibilidad de transportarla al Caribe para comprobar si era posible su plantación extensiva. El propio Banks fue quien, una vez aceptada esa iniciativa, que sería patrocinada por la armada real, intercedió ante los responsables del Almirantazgo para que fuese designado para dirigir la expedición el teniente William Bligh, de 33 años de edad, con el que había compartido uno de los viajes de exploración de James Cook por el Pacífico unos años antes. 

Al mando de una tripulación de 44 hombres, Bligh zarpó el 23 de diciembre de 1787 del estuario del Támesis. Las órdenes que tenía eran llegar a las islas de La Sociedad, actual Polinesia francesa, por el sur de América atravesando el cabo de Hornos. Sin embargo, temporales desusados y persistentes vientos contrarios le impidieron atravesar hasta el Pacífico, por lo que tuvo que volver al Atlántico para arribar a la Polinesia vía el cabo de Buena Esperanza y el océano Índico. Tras diez meses de navegación, el Bounty llegó finalmente a Tahití, fondeando en la bahía de Matavai el 25 de octubre de 1788. 
Debido al retraso sufrido, Bligh hubo de permanecer cinco meses en la isla aguardando a la estación propicia para trasplantar los brotes del árbol, espera que lejos de ser tediosa les permitió disfrutar de los encantos naturales de la paradisíaca isla y de las atenciones de los hospitalarios tahitianos, especialmente de las del género femenino. 

Una vez llevados a bordo 1051 plantones, el 4 de abril de 1789 el Bounty se hizo de nuevo a la vela rumbo al Caribe. Cierto día, cuando se encontraban entre las islas de Tofoa y Kotoo, Bligh advirtió que faltaban algunos cocos. Acusó a la tripulación de haberlos robado e insultó gravemente al oficial adjunto al segundo, llamado Fletcher Christian, al que públicamente tildó de "perro maldito". El comandante ordenó que se redujeran las raciones de grog y amenazó con echar al mar a quienes fueran sorprendidos en algún robo. Su actitud violenta exasperó los ánimos de los marineros y sobre todo provocó la rabia de Christian al verse tratado tan ofensiva e injustamente. A tenor de las posteriores declaraciones de los testigos ante la corte marcial parece que el motín no fue premeditado sino que se concertó a última hora, tal vez esa misma madrugada. Presa de una fuerte agitación (algún testigo dijo haberlo visto llorar aquella noche en la proa del barco) Christian parecía decidido en un primer momento a abandonar el navío, pero luego cambió de opinión y decidió tomar medidas más drásticas, aprovechando que por una casualidad fatal su cuarto de guardia coincidía con el servicio de varios tripulantes a quienes el irascible Bligh había castigado repetidas veces y que debían de tener motivos también para vengarse de su comandante. 

Pasadas las cinco de la mañana del 28 de abril, según las declaraciones tomadas por los jueces militares, los conjurados aparecieron en el puente armados con fusiles y bayonetas y pese a ser unos pocos, se apoderaron del buque con rapidez sin hallar prácticamente ninguna oposición. Tras amenazar al comandante con matarlo si se resistía, lo embarcaron en la chalupa junto a otros 18 tripulantes, un sextante, algunos barriles de agua y algo de comida. Otros 13 marineros que rehusaron unirse a los amotinados tuvieron que permanecer en el navío por falta de espacio en el bote.
A las 8 de la mañana la chalupa se apartó del Bounty y Bligh y su compañeros quedaron abandonados a su suerte en las proximidades de la isla de Tofua, muy lejos de cualquier enclave o puerto europeo. El viaje que realizó a continuación en aquella frágil embarcación de 7,5 metros de eslora hasta Malasia constituye una verdadera hazaña náutica, teniendo en cuenta que se trataba de un pequeño bote sin cubierta y carente por tanto de protección contra el sol, la lluvia y las olas. Carecían de cartas náuticas adecuadas e iba sobrecargado de peso disponiendo de menguados recursos de agua y comida. Pese a ello y valiéndose de sus indudables conocimientos técnicos, Bligh consiguió llegar al puerto holandés de la isla de Timor en Las Molucas, tras 41 días de singladura. Había recorrido unos 5.800 km en mar abierto y pese a ello solo hubo que lamentar la pérdida de un hombre, muerto en un enfrentamiento con los nativos en el curso de una de las recaladas para aprovisionarse de agua y víveres, si bien varios más fallecieron en las semanas siguientes a su arribada a Timor debido a las penalidades sufridas en su larga odisea. Cuando Bligh llegó a Inglaterra, hubo de someterse a una corte marcial por la pérdida del navío, pero el juicio se saldó con su libre absolución. La Armada despachó a la fragata HMS Pandora al mando del capitán Edward Edwards con la misión de traer a Inglaterra al resto de la tripulación y esclarecer su participación en los hechos. 

A bordo del Bounty habían quedado 25 hombres en total, de los que realmente estaban implicados en el motín Christian y ocho o nueve marineros. Tras echar por la borda las macetas con las plantas, decidieron volver a Tahití. Allí desembarcaron a 16 hombres, a la espera de que un barco inglés los llevase de retorno a la patria. Los amotinados se hicieron después a la vela llevando consigo a 17 nativos, seis hombres y once mujeres, una de ellas con un bebé. Tras varios días de navegación, avistaron la isla de Pitcairn, situada a unas 1.300 millas al Sureste del archipiélago de Tahití, que por aquel entonces figuraba cartografiada en una posición errónea, lo que la convertía en un refugio idóneo al no ser localizable por los barcos de la Royal Navy que con toda seguridad serían enviados en su búsqueda. El 23 de enero de 1790, una vez trasladados los enseres de a bordo y desmantelado una parte del casco quemaron el resto para borrar toda huella del navío. Con las maderas y herramientas construyeron varias cabañas que también camuflaron convenientemente para no ser vistas desde el mar.

No fue hasta 1808 cuando la isla volvió a ser visitada por otro buque, en este caso americano. Para entonces Christian y la mayor parte de los amotinados habían muerto en diversos enfrentamientos surgidos entre los propios europeos o a manos de los tahitianos causados por la posesión de las mujeres y el reparto de las tierras de la isla. El último amotinado, John Adams, murió en 1829. Sus descendientes continúan hoy residiendo en Pitcairn, aunque en 1856 una parte de la población fue desplazada a la isla de Norkfolk, entre Nueva Zelanda y Australia.

De los 10 prisioneros sobrevivientes al naufragio del HMS Pandora, cuatro contaron con el testimonio favorable de Bligh y fueron absueltos; otros dos fueron condenados pero posteriormente recibieron el indulto real; uno más fue condenado, pero exculpado por un tecnicismo legal. Los tres restantes fueron condenados y ejecutados en la horca. 

Sobre las incidencias de aquel memorable viaje, Bligh publicó en 1790 una narración, lógicamente en sentido exculpatorio, titulada en el estilo de la época "Relación de los sucesos ocurridos en el navío el Bounty, perteneciente al rey de Inglaterra y mandado por el teniente W. Bligh, con el viaje subsiguiente de este oficial y de una parte de su tripulación en la chalupa desde las islas de La Amistad en el mar del Sur hasta Timor, establecimiento holandés de las islas Molucas".

Se han hecho versiones literarias y cinematográficas sobre el motín del "Bounty"; entre ellas:






</doc>
<doc id="21998" url="https://es.wikipedia.org/wiki?curid=21998" title="Historia de América del Norte">
Historia de América del Norte

La Historia de Norteamérica es rica en cuanto a sus diversas culturas y civilizaciones, desde esquimales en el extremo norte del continente, hasta las civilizaciones azteca, olmeca y maya al sur.

La prehistoria de Groenlandia es la historia repetida de inmigraciones inuit desde las tierras de América del Norte. Según las sagas nórdicas, Groenlandia es descubierta hacia el año 900 por el navegante noruego Gunnbjörn. Durante la década de 980, los vikingos asentados en Islandia fueron los primeros visitantes europeos de Groenlandia, explorando la deshabitada costa sudoccidental de la isla. En 1536, Dinamarca y Noruega se unieron oficialmente, y Groenlandia empezó a ser considerada más una dependencia danesa que noruega. Actualmente, el gobierno local groenlandés se presenta como una "nación inuit". Los nombres de lugares en danés han sido remplazados por nombres locales. Godthåb, el centro de la civilización danesa en la isla, ahora se llama Nuuk, la capital de un gobierno prácticamente soberano. En 1985 se estableció la bandera de Groenlandia, a partir de los colores de la bandera de Dinamarca. Sin embargo, el movimiento en busca de una soberanía total todavía no ha ganado consenso.

Los primeros habitantes de la región fueron diversos pueblos provenientes de Siberia, que llegaron a través del Estrecho de Bering, y un poco más tarde llegaron los últimos pueblos inuit (esquimales) provenientes de Asia (ver amerindios de Canadá). En el siglo XVIII, estalló un conflicto entre Francia y Gran Bretaña que se propagó a las colonias; ese conflicto terminó con una victoria británica. En 1763, con el Tratado de París, la Nueva Francia pasó a ser una colonia británica. Pocos años después, Gran Bretaña oficialmente reconoció el derecho civil francés y garantizó la libertad religiosa y lingüística de la población de habla francesa de Canadá. En 1982, tuvo lugar una importante reforma constitucional: la Ley de América del Norte británica de 1867 y sus numerosas enmiendas pasaron a ser la Ley constitucional de 1982, actual Constitución de Canadá. Estos últimos años, los quebequenses se han pronunciado sobre la cuestión de la unidad nacional. Dos veces, una en 1980 y otra en 1995, votaron en referendos sobre la soberanía de la provincia. La mayoría obtenida en las votaciones estuvo a favor de la continuación de Quebec en Canadá.


Se atribuye el descubrimiento a Juan Bermúdez, de Palos de la Frontera, Huelva, España, posiblemente en 1503. Regresó en 1515, no intentaron atracar a causa del mal tiempo.

El primer asentamiento se produjo en 1609 por colonos ingleses que se dirigían a Virginia. Durante una tormenta, el barco llamado "Sea Venture" se separó del resto de la flota y chocó con los arrecifes, toda la tripulación se salvó, después construyeron dos barcos y el capitán se dirigió a Virginia nuevamente. Inglaterra se interesó en el archipiélago y en 1612 envió un contingente de 60 colonos.

Las islas fueron arrendadas a los Estados Unidos como base militar en 1941 por 99 años. En 1995 las bases estadounidenses fueron clausuradas y en 2002 devolvieron las tierras de sus bases.

Las Bermudas están bajo supervisión del Comité de Descolonización de las Naciones Unidas.

México prehispánico es un período de la historia del país anterior a la conquista y colonización española a partir de 1521. Es necesario aclarar que México es un Estado moderno cuyas fronteras fueron fijadas a mediados del siglo XIX. Por lo tanto, la historia mexicana de la época prehispánica es la historia de los pueblos que vivieron en ese territorio, no la historia del estado mexicano en la época precolombina.

La historia prehispánica de México comienza con la llegada de sus primeros pobladores. Sobre el poblamiento de América se han propuesto numerosas hipótesis, pero la que cuenta con mayor aceptación y evidencia de apoyo señala que los humanos entraron al continente a través de Beringia durante la época de las glaciaciones. Esta teoría está demostrada por estudios recientes de ADN basados en los haplogrupos del cromosoma Y (ADN-Y) y los haplogrupos del ADN mitocondrial (ADNmt). La época en que esto ocurrió es motivo de debate entre quienes defienden la teoría del poblamiento temprano y la del poblamiento tardío. En el caso de México, algunos autores han querido ver evidencia que apoya la primera, como los hallazgos de El Cedral (San Luis Potosí), a los que se atribuye una antigüedad de 33 mil años


</doc>
<doc id="22004" url="https://es.wikipedia.org/wiki?curid=22004" title="Magnitud (matemática)">
Magnitud (matemática)

La magnitud es una medida asignada para cada uno de los objetos de un conjunto medible, formados por objetos matemáticos. La noción de magnitud concebida así puede abstraerse a objetos del mundo físico o propiedades físicas que son susceptibles de ser medidos.

Las medidas de propiedades físicas usualmente son representables mediante números reales o "n"-tuplas de números reales, y usualmente para ser interpretables requieren del uso de una unidad de medida pertinente. Una propiedad importante de muchas magnitudes es admitan grados de comparación "más que", "igual que" o "menos que".

Una magnitud matemática usada para representar un proceso físico es el resultado de una medición; en cambio las magnitudes matemáticas admiten definiciones abstractas, mientras que las magnitudes físicas se miden con instrumentos apropiados.

Los griegos distinguían entre varios tipos de magnitudes, incluyendo:

Probaron que los dos primeros tipos no podían ser iguales, o siquiera sistemas isomorfos de magnitud. No consideraron que las magnitudes negativas fueran significativas, y el concepto se utilizó principalmente en contextos en los que cero era el valor más bajo.

La noción abstracta de magnitud implica la existencia de una función real que asignar a una colección de "objetos medibles" formula_1 un valor numérico real, ya que los números reales son un cuerpo totalmente ordenado con operaciones compatibles con dicha ordenación. Es decir, para cada magnitud "M" existe una función:

En las medidas usadas asociadas a conceptos métricos, los objetos medibles son subconjuntos de un espacio métrico o alternativamente un espacio de medida, no siendo en general cualquier subconjunto de dicho espacio (se requieren ciertas condiciones de regularidad para que la magnitud de un objeto esté bien definida).

La magnitud de cualquier número "x" se denomina usualmente su "valor absoluto" o "módulo", indicado por |"x"|. 

El valor absoluto de un número real "r" se define como:
Se puede considerar como la distancia numérica entre el cero y la recta numérica real. Por ejemplo, el valor absoluto tanto de 7 como de -7 es 7. En este caso el conjunto de objetos medibles en la función es formula_2 y la magnitud asociada al valor absoluto es la función: formula_3 dada por formula_4

Un número complejo "z" puede visualizarse como la posición del punto "P" en un espacio euclídeo bidimensional, llamado plano complejo.

El valor absoluto de "z" puede considerarse como la distancia desde el origen de tal espacio hasta "P". La fórmula para el valor absoluto de z es similar a la de la norma euclidea del espacio bidimensional: 

donde ℜ("z") y ℑ("z") son respectivamente la parte real y la parte imaginaria de "z" y "z*" es su complejo conjugado. Por ejemplo, el módulo de −3 + 4"i" es 5. En este caso se tiene formula_5 y formula_6 dada por formula_7.

Dado un espacio vectorial con producto escalar formula_8, se puede dotar a dicho espacio de una norma vectorial dada por: formula_9 lo que a su vez permite definir el ángulo entre dos vectores mediante la fórmula:
\frac{\mathbf{v}\cdot \mathbf{w}}{\|\mathbf{v}\| \|\mathbf{w}\|} \right) </math>
En este caso el conjunto de objetos medibles viene dado por formula_10 y además se cumplirá que formula_11

En una variedad de Riemann orientable de dimensión "n" > 2 en general podrán definirse longitudes (1-medidas), superficies (2-medidas), volúmenes (3-medidas), etc. En este caso los conjuntos de objetos medibles formula_1 serán subvariedades diferenciables.

En un espacio de medida formula_13 también es posible construir medidas de conjuntos, aunque en general no todo subconjunto del espacio de medida será medible, sino sólo una cierta σ-álgebra. En este caso el conjunto de objetos medibles es precisamente formula_14 y la magnitud asociada a la medida de estos conjuntos viene dada por la función formula_15 definida por formula_16. Existen dos casos interesanes de este tipo de medidas:

En un conjunto finito "F" puede definirse una magnitud sencilla asociada a la "cantidad de objetos" de un subconjunto. En ese caso, el conjunto de objetos medibles es formula_22 el conjunto de partes de "F", y la magnitud asociada se llama número de elementos o cardinal: formula_23 dada por formula_24.

Nótese que la "cantidad de objetos" de hecho es un caso particular de espacio de medida, donde la σ-álgebra coincide con el conjunto de partes del conjunto base usado para construir las medidas.




</doc>
<doc id="22006" url="https://es.wikipedia.org/wiki?curid=22006" title="Diodo Varicap">
Diodo Varicap

El diodo Varicap conocido como diodo de capacidad variable o varactor, es un diodo que aprovecha determinadas técnicas constructivas para comportarse, ante variaciones de la tensión aplicada, como un condensador variable. Polarizado en inversa, este dispositivo electrónico presenta características que son de suma utilidad en circuitos sintonizados (L-C), donde son necesarios los cambios de capacidad.

Cuando un diodo Varicap es polarizado en inversa, se dice que la barrera de potencial o juntura que forman los materiales N y P a partir del punto de unión de las junturas se produce una capacitancia. Visto en forma metafórica y práctica, es el equivalente a dos placas de un condensador que van separándose a medida que la tensión de alimentación se incrementa. Este incremento de tensión provoca una disminución de la capacidad equivalente final en los terminales del diodo (a mayor distancia entre placas, menor capacidad final). Por este motivo queda claro el concepto de que la mayor capacidad que puede brindar un diodo de esta naturaleza se encuentra en un punto de baja tensión de alimentación (no cero), mientras que la mínima capacidad final estará determinada por cuánta tensión inversa pueda soportar entre sus terminales. Sin llegar a valores extremos, los más habituales suelen encontrarse entre 3 o 4 picofaradios y 50 picofaradios para ejemplos como el diodo BB148 de NXP. Con una tensión menor a un voltio alcanza su máxima capacidad, llegando al mínimo valor con 12 o 13V, según podemos ver en la gráfica obtenida de su hoja de datos.

Para poder medir la capacidad de estos diodos se puede recurrir a la fórmula de MBR:

formula_1

donde C = capacidad del diodo con polarización inversa (Faradios)

V= voltaje de polarización inversa del diodo (Voltios), (formula_2 es la magnitud del voltaje de polarización inversa del diodo, siempre positiva)

C= C 



</doc>
<doc id="22009" url="https://es.wikipedia.org/wiki?curid=22009" title="El libro de los cuentos perdidos">
El libro de los cuentos perdidos

El libro de los cuentos perdidos es el título de los dos primeros volúmenes, editados por Christopher Tolkien en los años 1983 y 1984, de la serie de doce libros denominada "La historia de la Tierra Media" en la que el autor analiza los manuscritos no publicados de su padre, J. R. R. Tolkien.

Contiene las primeras versiones de las historias comenzadas entre 1916 y 1917, cuando Tolkien padre tenía veinticinco años, y que fueron abandonadas varios años después. Es, en realidad, el principio de toda la concepción de la mitología de la Tierra Media y el primer esbozo de los mitos y leyendas que constituirían "El Silmarillion". Es notable que aunque son muy primitivos en estilo y contenido, son muy cercanos a los trabajos posteriores en muchas formas. Cada uno de los "Cuentos" es seguido por notas y comentarios detallados de Christopher Tolkien. 

El marco narrativo es el largo viaje hacia el oeste que emprende un marinero a Tol Eressëa, la isla solitaria donde habitan los elfos. En las primeras versiones de los "Cuentos perdidos" este hombre es llamado Eriol, originario del norte de Europa. Sin embargo, en las versiones posteriores es conocido como Ælfwine, un inglés de la Edad Media. Allí conoce los cuentos perdidos de Elfinesse, en los que aparecen las ideas y concepciones más tempranas sobre los valar, elfos, enanos, balrogs y orcos, los Silmarili, los Dos Árboles de Valinor, Nargothrond, Gondolin, y la geografía y cosmología de la Tierra Media y de todo Arda.

Este libro nos muestra los primeros esbozos de lo que se convertiría en la base mitológica de la conformación de la «Tierra Media». Los "Cuentos" relatados, al estar acompañados de notas y comentarios, ayudan al lector a entender de mejor forma el proceso creativo de J. R. R. Tolkien.

Aunque muchos nombres en el libro son idénticos o cercanos a los que aparecen en versiones posteriores, algunos de ellos no presentan ningún parecido con sus formas finales. J. R. R. Tolkien modificaba los nombres frecuentemente, muchas veces con nuevas variantes (rechazadas a su vez) escritas en un manuscrito único. Desconcertantemente, algunas veces los nombres utilizados para una cosa posteriormente eran usados para referirse a otra distinta, y el manuscrito original era abandonado. Por ejemplo, la casa de los elfos llamada «teleri» en "El libro de los cuentos perdidos" no es la misma que en "El Silmarillion". Aquí los «teleri» eran los que posteriormente Tolkien llamó vanyar mientras que los solosimpi eran los posteriores teleri.

Existen más cambios visibles dentro del libro y este no es consistente internamente, debido en parte a que mientras Tolkien escribía tranquilamente los relatos comenzaba a reescribir e incluso desechar partes de las ideas tempranas a medida que su mundo imaginario iba cambiando. Los "Cuentos" finalmente fueron abandonados, pero resucitaron como parte del «Esbozo de la mitología» que se convertiría en "El Silmarillion".

Para su publicación este libro fue dividido en dos volúmenes, una simple división editorial. Ambos volúmenes están separados en varios "Cuentos perdidos". 



"The New York Times" admiró la tenacidad de Christopher Tolkien, junto con el poder creativo de su padre, y admitía que «sin duda, los devotos a Tolkien se regocijarán, pero para los lectores no iniciados que no estén completamente familiarizado con las otras obras, los comentarios pueden ser un poco enigmáticos». 



</doc>
<doc id="22010" url="https://es.wikipedia.org/wiki?curid=22010" title="Alfonso García Robles">
Alfonso García Robles

José Alfonso Eufemio Nicolás de Jesús García Robles (Zamora, Michoacán, México; 20 de marzo de 1911-Ciudad de México, 2 de septiembre de 1991) fue un diplomático mexicano, galardonado en 1982 con el premio Nobel de la Paz junto con la sueca Alva Reimer Myrdal.

Su labor más destacada fue la firma del Tratado de Tlatelolco (1967) referente a la no proliferación nuclear y su participación en las Sesiones Especiales para el Desarme de la Asamblea General de la ONU en 1978 y 1982.

Alfonso García Robles, nació en Zamora, Michoacán, México el 20 de marzo de 1911. 

Fue alumno del Instituto de Ciencias, renombrado colegio jesuita en Guadalajara.
Estudió Derecho, licenciándose por la Universidad Nacional Autónoma de México y realizando estudios de posgrado en el Instituto de Altos Estudios Internacionales, que actualmente es parte de la Universidad de París II Panthéon-Assas, en 1936 y en la Academia de Derecho Internacional de La Haya en 1938.Fue nombrado como el Aguja ya que en 1937 creó una campaña para salvar al Pez Aguja

Se incorporó al servicio exterior de su país en 1939 como tercer secretario de la Embajada de México en Suecia. 

Fue trasladado a México en 1941 para incorporarse a la Secretaría de Relaciones Exteriores (SRE), donde permaneció cinco años como Subdirector de Asuntos Políticos del Servicio Diplomático.

Con el cargo de Secretario de Asuntos Internacionales de la Comisión Nacional de Planeación para la Paz, participó en la Conferencia de las Naciones Unidas sobre Organización Internacional en San Francisco en 1945, donde se sentaron las bases jurídicas de la Organización de las Naciones Unidas (ONU).

De 1946 a 1956 radicó en Nueva York, trabajando para la ONU como Jefe de la División Política del Departamento de Asuntos del Consejo de Seguridad. 

Fue el representante de la ONU en la Conferencia Panamericana de Bogotá (1948), en la que se firmó la Carta de la Organización de los Estados Americanos.

En 1950 se casó con Juana María Szyszlo, una joven peruana, funcionaria de la ONU, con quien tendría dos hijos.

De 1958 a 1960 fue Director en Jefe para Asuntos de Europa, Asia y Organismos Internacionales en la SRE. En esta época se ocupó del Derecho del Mar participando en conferencias en Ginebra de 1958 y 1960.

De 1962 a 1964 ocupó el puesto de Embajador en Brasil.
De 1964 a 1970 fue Subsecretario de la Secretaría de Relaciones Exteriores.

Como Presidente de la Comisión Preparatoria para la Desnuclearización de América Latina presidió las reuniones que se celebraron en la Ciudad de México a partir de 1964 y que concluyeron con la apertura a firma el 14 de febrero de 1967 del Tratado para la Proscripción de las Armas Nucleares en América Latina, mejor conocido como el Tratado de Tlatelolco. 

García Robles desempeñó un papel crucial en lanzar e implementar el acuerdo. Le han llamado el padre del acuerdo de Tlatelolco. Este, propuesto por Adolfo López Mateos, presidente de México en ese entonces, era el resultado de la crisis en Cuba. La idea era asegurar la prohibición de los armamentos nucleares y de que esta parte del mundo no estuviera implicada en ningún conflicto entre las grandes potencias rivales. Las negociaciones fueron conducidas por García Robles, su habilidad de empresa y diplomacia merece una gran medida de crédito para el hecho de que el acuerdo fue concluido con éxito después de algunos años de negociación.

De 1971 a 1975 fue Embajador de México en las Naciones Unidas y presidió el Grupo de los 77.

Ingresó a El Colegio Nacional el 4 de abril de 1972, con la conferencia "El desarme y las Naciones Unidas", fue presentado por el Dr. Antonio Gómez Robledo.

En 1976 fue Secretario de Relaciones Exteriores.
Desde 1977 fue el representante permanente de México en el Comité sobre el Desarme de las Naciones Unidas en Ginebra. 

En 1978 fue presidente de la delegación mexicana en la primera Sesión Especial para el Desarme de la Asamblea General de la ONU y fue uno de los responsables de la adopción de "el documento final".

En 1981 el Presidente de la República lo designó Embajador Emérito.

En septiembre de 1982 se le otorgó la Condecoración del Servicio Exterior Mexicano.
En octubre de 1982 obtuvo el premio Nobel de la Paz por “su magnífico trabajo en las negociaciones de desarme de las Naciones Unidas”, distinción que compartió con la diplomática y escritora sueca Alva Reimer Myrdal.

Murió el 2 de septiembre de 1991 en la Ciudad de México. Fue enterrado en el Panteón Español, Miguel Hidalgo.

El archivo personal de García Robles y su biblioteca de 1100 volúmenes fueron donados por su viuda a la Universidad de Virginia en Estados Unidos en 1998.

El 24 de abril de 2003 se develó su nombre escrito con letras de oro en uno de los muros del recinto legislativo de San Lázaro, sede de la Cámara de Diputados de México.

Durante los festejos del quincuagésimo aniversario de la Facultad de Derecho de la Universidad La Salle, se develó en su honor el busto que da pauta a que los esfuerzos y grandes sacrificios obtienen sus grandes recompensas.




</doc>
<doc id="22012" url="https://es.wikipedia.org/wiki?curid=22012" title="Fullereno">
Fullereno

Un fullereno (también, fulereno) es una molécula compuesta por carbono que puede adoptar una forma geométrica que recuerda a una esfera, un elipsoide, un tubo (llamado nanotubo) o un anillo. Los fullerenos son similares al grafito, compuesto de hojas de anillos hexagonales enlazadas, pero conteniendo anillos pentagonales y a veces heptagonales, lo que impide que la hoja sea plana. Los fullerenos son la tercera forma molecular estable conocida de carbono, tras el grafito y el diamante.

Los fullerenos fueron descubiertos en 1985 por Harold Kroto, Robert Curl y Richard Smalley, lo que les valió la concesión del Premio Nobel de Química en 1996.

El primer fullereno descubierto fue el , que consta de 12 pentágonos y 20 hexágonos. Cada pico corresponde a un átomo de carbono y cada lado a un enlace covalente. Tiene una estructura idéntica a la cúpula geodésica o un balón de fútbol. Por esta razón, se le llama «buckminsterfullereno» (en homenaje al arquitecto Buckminster Fuller quien diseñó la cúpula geodésica) o «futboleno». Los fullerenos esféricos reciben a menudo el nombre de "buckyesferas" y los cilíndricos el de "buckytubos" o nanotubos. 
Destacan por su versatilidad para la síntesis de nuevos compuestos. Su naturaleza y forma se han hecho ampliamente conocidas en la ciencia y en la cultura en general, por sus características físicas, químicas, matemáticas y estéticas.

El fullereno más conocido es el formado por 60 átomos de carbono (C), en el que ninguno de los pentágonos que lo componen comparten un borde; si los pentágonos tienen una arista en común, la estructura estará desestabilizada (véase pentaleno). La estructura de C es la de un icosaedro truncado, que se asemeja al balón de fútbol cuyo diseño se inició con el Telstar 1970. Está configurado por 20 hexágonos y 12 pentágonos, con un átomo de carbono en cada una de las esquinas de los hexágonos y un enlace a lo largo de cada arista. Aunque su nombre viene de Richard Buckminster Fuller por sus domos geodésicos —el primero de 1948—, fue el ingeniero alemán el que en 1912 inició la construcción de una obra con esa forma para la empresa de instrumentos ópticos de Carl Zeiss, en Jena.
El dibujo más antiguo conocido del icosaedro truncado es el de Piero della Francesca y el más conocido el que Leonardo da Vinci hizo para el libro La Divina Proporción por encargo de Luca Pacioli.

El fullereno C es el más pequeño de todos, no tiene hexágonos, sólo 12 pentágonos formando un dodecaedro, mientras que el C, tiene 12 pentágonos al igual que el buckminsterfullereno, pero tiene más hexágonos, y su forma en este caso se asemeja a un balón de rugby. 

Un nanotubo es una estructura formada por fullerenos cilíndricos polimerizados, en los que los átomos de carbono a partir de un determinado punto enlazan con los átomos de carbono del siguiente fullereno. Estas estructuras presentan propiedades muy aprovechables para la industria tal como la permisibilidad eléctrica. La fuerza entre los enlaces lo hace un material muy sólido y resistente a altas temperaturas. Además, puede ser dopado con otros materiales para cambiar sus propiedades, tales como ductibilidad o fuerza del enlace lo que permitiría la creación por ejemplo de una nueva generación de chips electrónicos a bajo costo.

Hasta el siglo XX, el grafito y el diamante eran las únicas formas alotrópicas conocidas del carbono. En experimentos de espectroscopia molecular, se observaron picos que correspondían a moléculas con una masa molecular exacta de 60, 70 ó más átomos de carbono. Harold Kroto, de la Universidad de Sussex, James Heath, Sean O'Brien, Robert Curl y Richard Smalley, de la Universidad de Rice, descubrieron el C y otros fullerenos en 1985, en un experimento que consistió en hacer incidir un rayo láser sobre un trozo de grafito. Ellos esperaban efectivamente descubrir nuevos alótropos del carbono, pero suponían que serían moléculas largas, en lugar de las formas esféricas y cilíndricas que encontraron. A Kroto, Curl y a Smalley se les concedió el premio Nobel de Química en 1996, por su colaboración en el descubrimiento de esta clase de compuestos. El C y otros fullerenos fueron más adelante observados fuera del laboratorio (ej. en el hollín de una vela). Hacia el año 1991, era relativamente fácil producir unos cuantos gramos de polvo de fullereno usando las técnicas de Donald Huffman y Wolfgang Krätschmer. La purificación del fullereno era un desafío para los químicos hasta la primera década del presente siglo, cuando un equipo de investigadores españoles desarrolló un nuevo proceso de obtención. Los fullerenos "endoédricos" han incorporado, entre los átomos de la red, iones u otras moléculas más pequeñas. El fullereno es un reactivo habitual en muchas reacciones orgánicas como por ejemplo en la reacción de Bingel, descubierta en 1993.

En julio de 2010 la NASA anunció el descubrimiento de fullerenos en el espacio. Al usar la visión infrarroja sensible del telescopio Spitzer, los investigadores han confirmado la presencia de C en la nebulosa planetaria Tc1. Los astrónomos creen que los fullerenos son creados en las capas exteriores de una estrella, como nuestro sol, y posteriormente son expulsadas al espacio después de la explosión de las mismas.

Desde su descubrimiento, las propiedades químicas y físicas de los fullerenos todavía continúan bajo un intenso estudio. Entre las propiedades físicas más relevantes se encuentra el gap de energía entre el orbital ocupado de más alta energía (HOMO) y el orbital desocupado de menor energía (LUMO), cuya medida es ca. 1,7 eV. La simetría del estado base del fulereno C corresponde al grupo puntual Ih. En esta simetría los orbitales HOMO y LUMO están cinco y tres veces degenerados, h y t respectivamente. Debido a este hecho, transiciones electrónicas desde HOMO a LUMO están prohibidas por simetría. El fulereno C presenta 174 modos normales de vibración (3N - 6, donde N = 60 átomos de carbono) en la región del infrarrojo. No obstante, solo cuatro modos normales son activos.

En el icosaedro truncado y los cuerpos geométricos semejantes a él —los hexapentas— la clave matemática está en la relación geométrica entre los hexágonos y los pentágonos que los configuran. La demostración de la armonía en los hexapentas en general, dada la consonancia entre sus hexágonos y pentágonos componentes (considerando que ambas figuras en cada caso tienen la misma longitud de sus lados o aristas del poliedro), está dada en la relación de sus apotemas mediante el Número Áureo (ф = 1,618…).

En el campo de la nanomedicina, el fulereno C se ha estudiado su potencial uso medicinal como fijador de antibióticos específicos en su estructura para atacar bacterias resistentes y ciertas células cancerígenas, tales como el melanoma.

Los fullerenos no son muy reactivos debido a la estabilidad de los enlaces tipo grafito, y son también muy poco solubles en la mayoría de disolventes. Entre los disolventes comunes para los fullerenos se incluyen el tolueno y el disulfuro de carbono. Las disoluciones de buckminsterfullereno puro tienen un color púrpura intenso. El fullereno es la única forma alotrópica del carbono que puede ser disuelta. Los investigadores han podido aumentar su reactividad uniendo grupos activos a las superficies de los fullerenos. El buckminsterfullereno no presenta "superaromaticidad", es decir, los electrones de los anillos hexagonales no pueden deslocalizar en la molécula entera.

Se pueden atrapar otros átomos dentro de los fullerenos; de hecho existen evidencias de ello gracias al análisis del gas noble conservado en estas condiciones tras el impacto de un meteorito a finales del periodo Pérmico. En el campo de la nanotecnología, la resistencia térmica y la superconductividad son algunas de las características más profundamente estudiadas.

Un método habitual para producir fullerenos es hacer pasar una corriente eléctrica intensa entre dos electrodos de grafito próximos en atmósfera inerte. El arco resultante entre los dos electrodos produce un depósito de hollín del que se pueden aislar muchos fullerenos diferentes.

Aunque se piensa que las buckyesferas son en teoría relativamente inertes, una presentación dada a la Sociedad Química Estadounidense en marzo de 2004 y descrita en un artículo publicado en la revista New Scientist el 3 de abril de 2004, sugiere que la molécula es perjudicial para los organismos. Un experimento llevado a cabo por Eva Oberdörster en la Southern Methodist University, en el que introdujo fullerenos en agua en concentraciones de 0,5 partes por millón, mostró que un pez "black bass" ("Micropterus salmoides") sufrió un daño celular en el tejido cerebral 17 veces superior, 48 horas después. El daño consistía en una peroxidación lipídica a nivel de la membrana celular, lo que deteriora el funcionamiento de esta. Se produjeron también inflamaciones en el hígado y la activación de genes relacionados con la síntesis de enzimas reparadoras.

La siguiente lista muestra los disolventes en orden decreciente de solubilidad para una mezcla de C/C). Los valores entre paréntesis indican la concentración de saturación.


En términos matemáticos, la estructura de un fullereno es un poliedro convexo con caras pentagonales y hexagonales.

Con ayuda de la fórmula de Euler: caras + vértices - aristas = 2, además del hecho de que cada vértice en una estructura de fullereno pertenece exactamente a tres caras, se puede demostrar fácilmente que en un fullereno hay exactamente 12 pentágonos. El fullereno más pequeño es el C, el dodecaedro. No existen fullerenos con 22 vértices. El número de fullerenos C diferentes crece de manera muy rápida al aumentar el valor de n; por ejemplo, hay 1812 fullerenos C, pero solo uno de ellos, el buckminsterfullereno, no tiene pentágonos adyacentes.

El físico que se transformó en artista Julian Voss-Andreae ha creado diversas esculturas que simbolizan la dualidad onda-partícula en los buckminsterfullerenos. Voss-Andreae participó en investigaciones realizadas para demostrar que objetos tan grandes como los buckminsterfullerenos también obedecen a las peculiares leyes de la física cuántica. Después de esto, Voss-Andreae decidió cambiar su profesión para convertirse en artista de tiempo completo. A partir de entonces ha diseñado objetos tales como la estructura de bronce de 60 centímetros de diámetro llamada "Buckyesfera Cuántica" (2004) que consiste en cuatro buckyesferas anidadas. Su escultura más grande basada en fullerenos se ubica en un parque privado en Portland, Oregon (Estados Unidos). "Realidad Cuántica (Gran Buckyesfera Rodeada de Árboles)" (2007) es una estructura de acero de 9 metros de diámetro atravesada por varios árboles que crecen libremente en medio de ella y la sostienen en el aire justo por encima del alcance de los brazos.







</doc>
<doc id="22015" url="https://es.wikipedia.org/wiki?curid=22015" title="Talcott Parsons">
Talcott Parsons

Talcott Parsons (Colorado Springs, 13 de diciembre de 1902 – Múnich - Alemania occidental, 8 de mayo de 1979) fue un sociólogo estadounidense de la tradición clásica de la sociología, mejor conocido por su teoría de la acción social y su enfoque estructural-funcionalista. Parsons es considerado una de las figuras más influyentes en el desarrollo de la sociología en el siglo XX.Luego de obtener un doctorado en economía, trabajó en la facultad de la Universidad de Harvard desde 1927 a 1979, y en 1930 estuvo entre los primeros profesores del recientemente creado departamento de sociología.

Basada en datos empíricos, la teoría de la acción social de Parsons fue la primera teoría de sistemas sociales desarrollada en Estados Unidos de carácter amplio, sistemático y generalizable.  Una de las más grandes contribuciones de Parsons a la sociología en el mundo anglófono fueron sus traducciones de las obras de Max Weber y sus análisis de los trabajos de Weber, Émile Durkheim y Vilfredo Pareto. El trabajo de estos autores influenció fuertemente la perspectiva de Parsons y fue la base de su teoría de la acción social, en la cual vio la acción voluntarística a través del prisma de los valores culturales y las estructuras sociales que constriñen las elecciones y que, en último término, determinan todas las acciones sociales, en oposición a la idea de que las acciones están determinadas con base en procesos psicológicos internos. Aunque Parsons es generalmente considerado un estructural-funcionalista, hacia el final de su carrera en 1975 publicó un artículo en el que declara que los términos "funcional" y "estructural-funcionalista" eran formas inapropiadas de describir el carácter de su teoría.

A inicios de la década de 1970, una nueva generación de sociólogos criticó las teorías de Parsons, viéndolas como socialmente conservadoras y con una prosa innecesariamente compleja.  Desde entonces, los cursos de sociología han puesto menos énfasis en sus teorías en comparación al auge de su popularidad entre las décadas de 1940 y 1970. Sin embargo, se reconoce un resurgimiento del interés en sus ideas.

Talcott Parsons nació el 13 de diciembre de 1902 en Colorado Springs. Fue el hijo de Edward Smith Parsons (1863-1943) y Mary Augusta Ingersoll (1863-1949). Su padre asistió al Yale Divinity School y fue ordenado como ministro congregacionalista, sirviendo primero como ministro de una comunidad pionera en Greeley. Al momento del nacimiento de Talcott Parsons, Edward Parsons era profesor de inglés y vicepresidente en el Colorado College.

Durante su periodo de ministro congregacionalista en Greeley, Edward Parsons había empatizado con el evangelio social, aunque manteniendo una posición teológica superior, al tiempo que mostró hostilidad al socialismo tratándolo como mera ideología. En aquel tiempo, Edward Parsons y su hijo Talcott eran cercanos a la teología de Jonathan Edwards. El padre se volvería después el presidente del Marietta College en Ohio.

La familia Parsons es una de las más antiguas familias de la historia de Estados Unidos. Sus ancestros fueron unos de los primeros en llegar desde Inglaterra en la primera mitad del siglo XVII. La herencia familiar consistió de dos líneas Parsons separadas e independientes, las cuales remiten a los tempranos días de Estados Unidos y a la historia de Inglaterra. Por el lado paterno, la familia puede ser rastreada hasta los Parsons de York. Por el lado materno, la línea Ingersoll estuvo conectada con Jonathan Edwards, y desde éste en adelante habría una nueva e independiente línea Parsons, ya que su hermana mayor Sarah Edwards se casó con Elihu Parsons el 11 de junio de 1750.

Parsons comenzó estudiando biología, sociología y filosofía en el Amherst College, recibiendo su grado de "Bachelor of Arts" en 1924. El Amherst College fue una institución a la que asistió la familia Parsons por tradición: el padre y el tío de Talcott asistieron a él, así como su hermano mayor Charles Edward Parsons. Inicialmente, Talcott Parsons estaba atraído por la carrera de medicina, inspirado en su hermano mayor, lo que lo llevó a estudiar extensamente biología y a pasar un verano trabajando en la Institución Oceanográfica de Woods Hole.

Los profesores de biología de Parsons en el Amherst College fueron Otto Glaser y Henry Plough. Parsons también se volvió uno de los estudiantes más destacados en la institución. Tomó clases con Walton Hamilton y el filósofo Clarence Edwin Ayres, ambos conocidos como "economistas institucionales", quienes le mostraron las obras de Thorstein Veblen, John Dewey, William Graham Sumner, entre otros. Parsons tomó también un curso con George Brown sobre la filosofía de Immanuel Kant, así como uno sobre la filosofía moderna alemana con Otto Manthey-Zorn, quien era un destacado intérprete de Kant. Parsons mostró tempranamente un gran interés por la filosofía, lo cual probablemente era una reminiscencia del gran interés de su padre por la teología en la tradición en la cual fue socializado, posición que contrastaba con la perspectiva de los profesores de Talcott Parsons.

Se han rescatado dos ensayos finales que Parsons escribió como estudiante de Clarence Ayres en la clase de Filosofía III en el Amherst College, los cuales han sido referidos como los "Amherst Papers" y han resultado de gran interés para los estudiosos de Parsons. El primero fue escrito el 19 de diciembre de 1922 y se titula "The Theory of Human Behavior in its Individual and Social Aspects" (La teoría del comportamiento humano en sus aspectos individuales y sociales). El segundo ensayo final fue escrito el 27 de marzo de 1923 y se titula "A Behavioristic Conception of the Nature of Morals" (Una concepción conductista de la naturaleza de la moral). Los ensayos revelan en parte el temprano interés de Parsons por las cuestiones sobre la evolución social. Los "Amherst Papers" también revelan que Parsons no estaba de acuerdo con sus profesores institucionalistas, ya que sostenía que el desarrollo tecnológico y el progreso moral eran dos procesos empíricos estructuralmente independientes.

Luego de su estadía en el Amherst College, estudió en la London School of Economics (LSE) por un año, donde fue expuesto al trabajo de R. H. Tawney, Bronislaw Malinowski y Leonard Trelawny Hobhouse. Durante su paso por la LSE hizo amistad con Edward Evan Evans-Pritchard, Meyer Fortes y Raymond Firth (quienes participaron en el seminario de Malinowski), y tuvo una fuerte amistad personal con Arthur Burns y Eveline Burns.

Mientras estudió en la LSE, conoció a una joven estadounidense en los cuartos comunes de estudiantes, cuyo nombre era Helen Bancroft Walker y con quien se casaría el 30 de abril de 1927. La pareja tuvo tres hijos: Anne, Charles y Susan, y eventualmente cuatro nietos. El padre de Helen nació en Canadá, pero se mudó al área de Boston y se volvió un ciudadano estadounidense.

Parsons fue luego a la Universidad de Heidelberg, donde recibió su doctorado en sociología y economía en 1927. Durante su estadía en esta universidad, trabajó con Alfred Weber (hermano de Max Weber), Edgar Salin (quien era su tutor de disertación), Emil Lederer y Karl Mannheim. Además, realizó un examen sobre la Crítica de la razón pura de Kant con el filósofo Karl Jaspers. Parsons también fue examinado en esta universidad por Willy Andreas sobre la Revolución Francesa. La tesis doctoral de Parsons fue escrita bajo el título "The Concept of Capitalism in the Recent German Literature" (El concepto del capitalismo en la reciente literatura alemana) y se enfocó en el trabajo de Werner Sombart y Max Weber. Resulta claro de su discusión el rechazo a las visiones cuasi idealísticas de Sombart y la adherencia a los intentos de Weber por lograr un equilibrio entre historicismo e idealismo, así como a una aproximación neokantiana.

El descubrimiento más crucial para Parsons en Heidelberg fue su encuentro con la obra de Max Weber, a quien nunca había escuchado antes de llegar a Alemania. Weber se volvió tremendamente importante para Parsons pues, considerando su crianza con un padre liberal pero fuertemente religioso, la cuestión del rol de la cultura y la religión en los procesos básicos de la historia mundial se volvió un persistente enigma para él. En este sentido, Weber fue el primer académico que realmente le proveyó a Parsons una convincente respuesta teórica a esta cuestión, razón por la cual Parsons estuvo absorto honda y extensamente en la lectura de la obra de Weber.

Parsons decidió entonces traducir la obra de Weber al inglés, acercándose a Marianne Weber, la esposa de Max Weber, con este propósito. Durante su estadía en Heidelberg, Parsons fue invitado por Marianne Weber a "meriendas sociológicas", las cuales eran reuniones de estudio grupal que Marianne tenía en la biblioteca del antiguo apartamento de ella y Max Weber. Uno de los académicos que Parsons conoció en Heidelberg que compartía su entusiasmo por Weber fue Alexander von Schelting. Posteriormente, Parsons escribió una reseña en el libro de Von Schelting sobre Weber. En general, Parsons leyó extensivamente literatura sobre religión, especialmente obras enfocadas en la sociología de la religión. Un académico que se volvió especialmente importante para Parsons a este respecto fue Ernst Troeltsch. Parsons también leyó ampliamente sobre el tópico del calvinismo, incluyendo las obras de Emile Doumerque, Eugéne Choisy y Henri Hauser.

En 1927, luego de un año enseñando en el Amherst College, Parsons entró a Harvard como instructor en el Departamento de Economía , donde siguió las lecciones de Frank William Taussig sobre Alfred Marshall y se volvió amigo del economista e historiador Edwin Gay, quien fuera el fundador de la Escuela de Negocios de Harvard. Parsons también se volvió un cercano asociado de Joseph Schumpeter y siguió su curso en Economía General. Parsons estuvo generalmente en desacuerdo con algunas de las tendencias del Departamento, que en esos días tenía una orientación altamente técnica y matemática, por lo que buscó otras opciones en Harvard y dio cursos en Ética Social y Sociología de la Religión. Aunque Parsons entró a Harvard a través del Departamento de Economía, nunca apuntó a volverse un economista; todas sus actividades y su interés intelectual básico lo impulsaron hacia la sociología, aunque no existió un Departamento de Sociología en los primeros años que estuvo en Harvard. Sin embargo, Harvard estuvo en esos años trabajando para establecer uno y Parsons se posicionó a sí mismo de varias formas escribiendo y enseñando, por lo que estaba listo para unirse al Departamento de Sociología cuando fue finalmente establecido. Parsons nunca fue forzado a abandonar el Departamento de Economía, sino que su salida fue una decisión voluntaria y deliberada.

La posibilidad de establecerse en la sociología llegó en 1930, cuando el primer Departamento de Sociología fue creado en Harvard por Pitirim Sorokin.  Sorokin había llegado a Estados Unidos en 1923 escapando de la Revolución Rusa. Parsons se volvió uno de los dos instructores del reciente creado departamento, junto a Carl Joslyn, y estableció lazos cercanos con el bioquímico y sociólogo Lawrence Joseph Henderson, quien tuvo interés personal por la carrera de Parsons en Harvard. Parsons también formó parte del grupo de estudio de Pareto, formado por el mismo Henderson, en el cual participaron algunos de los más importantes intelectuales de Harvard: Crane Brinton, George Casper Homans y Charles Curtis. A raíz de la lectura de Pareto, Parsons escribió un artículo sobre su teoría y luego explicitó la adopción de su concepto de sistema social. Parsons también hizo estrechas relaciones con dos otros influenciales intelectuales, con los cuales se envió correspondencia por varios años: el economista Frank Hyneman Knight y Chester Barnard, uno de los empresarios más dinámicos de Estados Unidos. Las relaciones entre Parsons y Sorokin se tensaron rápidamente, en parte por el profundo disgusto de Sorokin por la civilización estadounidense, a la que consideraba una cultura sensata en decadencia. Los escritos de Sorokin se volvieron progresivamente anticientíficos en sus últimos años, ampliando su brecha con el trabajo de Parsons y volviendo en su contra a una comunidad sociológica en Estados Unidos que se estaba volviendo crecientemente positivista. Incluso Sorokin llegó a menospreciar todas las tendencias sociológicas que diferían de sus obras, llegando en 1934 a ser un académico bastante impopular en Harvard.

Algunos de los estudiantes a los que enseñó Parsons en los primeros años del nuevo Departamento de Sociología fueron Robin Williams Jr., Robert K. Merton, Kingsley Davis, Wilbert Moore, Edward Devereux, Logan Wilson, Nicholas Demereth, John Riley Jr. y Mathilda White Riley. Generaciones posteriores de estudiantes incluyeron a Harry Johnson, Bernard Barber, Marion Levy y Jesse Richard Pitts. A pedido de los estudiantes, Parsons estableció un pequeño e informal grupo de estudio que se reunía anualmente. Hacia el final de la carrera de Parsons, el teórico de sistemas Niklas Luhmann también asistió a sus clases.

En el período académico 1939-1940, Parsons y Schumpeter dirigieron un seminario informal de facultad en Harvard realizado en el Salón Emerson y que buscaba discutir sobre el concepto de racionalidad. Entre los participantes se encontraban Abram Bergson, Wassily Leontief, Gottfried Haberler y Paul Sweezy. Schumpeter contribuyó al seminario con su ensayo "Rationality in Economics" (Racionalidad en economía), mientras que Parsons lo hizo con su artículo "The Role of Rationality in Social Action" (El rol de la racionalidad en la acción social). Schumpeter le propuso a Parsons escribir y editar conjuntamente un libro acerca de la racionalidad, pero el proyecto nunca se concretó.

Frente a la discusión entre economistas neoclásicos e institucionalistas, que fue uno de los conflictos prevalentes en la economía entre las décadas de 1920 y 1930, Parsons trazó una línea muy delgada. Él era muy crítico de la teoría neoclásica, actitud que permaneció durante toda su vida y se reflejó en las críticas a Milton Friedman y Gary Becker. Particularmente, se oponía al sesgo utilitarista de la teoría neoclásica. Sin embargo, Parsons estaba de acuerdo parcialmente con el estilo teórico y metodológico de este enfoque. Por lo tanto, se vio imposibilitado de aceptar la solución propuesta por la economía institucional, a la que consideraba como primariamente empírica, descriptiva y sin enfoque teórico.

Parsons regresó a Alemania el verano de 1930 y fue testigo directo de la febril atmósfera en la República de Weimar, durante la cual el Partido Nazi llegó al poder. Parsons recibió constantes reportes del ascenso del nazismo a través de su amigo Edward Yarnall Hartshorne. A fines de la década de 1930, Parsons comenzó a advertir a la opinión pública estadounidense sobre la amenaza nazi, pero tuvo poco éxito. Un sondeo de la época evidencia que el 91% del país se oponía a la Segunda Guerra Mundial. También la mayoría del pueblo estadounidense pensaba que el país debió mantenerse al margen de la Primera Guerra Mundial y que los nazis no representaban amenaza, pese a lo que hicieron en Europa. Incluso algunos estadounidenses simpatizaban con Alemania debido a su ascendencia en ese país y a la opinión de que, siendo ambos países fuertemente anticomunistas, sólo Alemania logró salir de la Gran Depresión. Uno de los primeros artículos que Parsons escribió sobre el nazismo fue "New Dark Age Seen If Nazis Should Win" (Nuevos años oscuros si los nazis ganan), lo que lo volvió uno de los iniciadores claves del Comité de Defensa de Harvard, el cual apuntaba a unificar la opinión pública estadounidense contra los nazis. Parsons difundió este mensaje a través de las estaciones de radio locales en Boston y en una reunión llevada a cabo en Harvard, la cual fue agitada por activistas en contra de la guerra. Junto a Charles Orlando Porter, Parsons buscó unificar a los estudiantes graduados en Harvard para ir a la guerra. Incluso, durante la guerra, Parsons dirigió un grupo de estudio especial en Harvard que buscaba analizar lo que sus miembros consideraban las causas del nazismo, incluyéndose para el análisis expertos en el tema.

Parsons generó un sistema teorético general para el análisis de la sociedad que denominó teoría de la acción. Esta teoría tiene un fundamento metodológico y epistemológico en el principio del realismo analítico, mientras que su supuesto ontológico es el de la acción voluntarista. El concepto de Parsons de realismo analítico se puede entender como una suerte de compromiso entre las visiones nominalista y realista de la naturaleza de la realidad y del conocimiento humano. Parsons creía que la realidad objetiva se podía relacionar sólo con una visión particular de dicha realidad, y que la comprensión intelectual general es plausible a través de esquemas conceptuales y teorías. La interacción con la realidad objetiva en el nivel intelectual se debe entender siempre como una aproximación. Con frecuencia Parsons explicaba el significado del realismo analítico citando a Lawrence Joseph Henderson: "Un hecho es una declaración sobre la experiencia en términos de esquema conceptual".

En general, Parsons sostuvo que su inspiración con respecto al realismo analítico ha estado en Lawrence J. Henderson y Alfred North Whitehead, aunque probablemente tuvo esta idea con anterioridad. El concepto de realismo analítico le permite insistir en la referencia a una realidad objetiva y, con esto, marcar una diferencia importante con el ficcionalismo de Hans Vaihinger.

"La estructura de la acción social" es la obra más famosa de Parsons. Su figura central fue Weber y los otros dos autores claves, Durkheim y Pareto, fueron agregados a la discusión conforme la idea central tomaba forma. Una importante publicación que ayudó a Parsons a desarrollar el argumento central de esta obra fue encontrada inesperadamente en 1932: se trata de "La formation du radicalisme philosophique" de Élie Halévy, particularmente el tercer volumen, obra que le permitió aclarar los supuestos del utilitarismo inglés.

La teoría de la acción que Parsons desarrolló en "La Estructura de la Acción Social" puede caracterizarse como un intento de mantener el rigor científico del positivismo al tiempo que se destaca la necesidad de la dimensión subjetiva de la acción humana, incorporada en las teorías sociológicas hermenéuticas. Resulta capital en la visión teórica y metodológica general de Parsons la comprensión de la acción humana en conjunción con el componente motivacional del acto humano. Por esta razón, se plantea que las ciencias sociales deben considerar el problema de los fines, propósitos e ideales en el análisis de la acción humana. La fuerte reacción del autor tanto a la teoría conductista como a las aproximaciones materialistas puras se deriva del intento de estas posiciones teóricas de eliminar los fines, propósitos e ideales como factores del análisis. En sus ensayos escritos en el Amherst College, Parsons ya estaba criticando los intentos de reducir la comprensión de la vida humana a fuerzas psicológicas, biológicas y materialistas. Para Parsons, resultaba esencial en la vida humana el modo en que la cultura era codificada, la cual constituía una variable independiente que no se puede deducir de otro factor del sistema social. Algunos de estos temas fueron presentados anteriormente por Parsons en un ensayo escrito dos años antes de la publicación de "La Estructura de la Acción Social".

Hay tres conceptos que yacen en el núcleo de la teoría de la acción: el acto-unidad, el voluntarismo y la "verstehen". El fenómeno más básico de la teoría de la acción es lo que Parsons denominó el acto-unidad, definido por sus cuatro componentes: 

El voluntarismo hace referencia a las elecciones que hacen los actores en las situaciones sociales en las que se encuentran. Esto no significa que los actores sean totalmente libres al hacer su elección, pues el concepto de voluntarismo implica una conciencia. Por último, la "verstehen" refiere a la necesidad de analizar la acción desde una perspectiva subjetiva.

La solución propuesta por Parsons a las polarizaciones anteriores a su tiempo fue una teoría general de la sociedad, que piensa la vida social como una totalidad y que pueda constituirse como un gran relato, con capacidad explicativa y predictiva sobre la vida social (al modo de Comte). Sin embargo Parsons va a encontrarse con un problema que le dará bastantes quebraderos de cabeza durante toda su vida: el papel del individuo. Frente a una teoría tan generalista, cabe preguntarse por cuál es el lugar del individuo dentro de un universo tan grande de supraentidades. Y, es más, si se tienen en cuenta las implicaciones del estructuralismo funcional, para el que los individuos están encajados en celdas de la estructura social, que determinan lo que socialmente son, y el objetivo de éstos es cumplir una función social, es decir, que ya tienen determinado hasta lo que tienen que hacer, ¿dónde queda la decisión individual?

Parsons para elaborar su teoría se basa en varias relaciones. Principalmente toma a Durkheim, utilizando su definición de sociedad, pero considerada como un sistema y no un organismo, en contraposición a Marx. Toma de Weber el concepto de "acción social", lo que es una conducta con significado referente a la cultura.

También retoma cuestiones de autores externos a la disciplina sociológica, como Freud, utilizando su segunda tópica, que plantea a la personalidad compuesta por tres componentes: el "ello" (tendencias naturales de los organismos vivos), el "superyó" (El Ideal del Yo) y el "yo" (la parte del "ello" modificada por la educación y la influencia cultural). Con esto se basa en el libro de Freud "El malestar en la cultura", que presenta la sociedad como represora de nuestros instintos, ya que en el caso de las represiones del "superyó" son todas de conformación social.

Por último, toma de Bertalanffy, biólogo y padre de la teoría general de sistemas, su propuesta de un modelo que amplíe la visión científica bajo un nuevo aspecto de ordenamiento y relación a través del modelo de sistema.


Parsons es conocido en la historia de la sociología, como el autor de la Teoría estructural funcionalista, que se llama “A.G.I.L.-.”. El esquema ágil, o el modelo de las cuatro funciones, como es la Teoría estructural funcionalista, que es lo mismo, no fue ni el primero ni el más importante, es decir, el más lucido, de los intentos parsoniano por resolver este problema. Parsons elaboró muchos intentos de Teoría, todos ellos muy sugestivos, muy interesantes, muy atractivos. Uno de ello alcanzó cierto notable éxito en la comunidad sociológica internacional. Fue el modelo AGIL. Pero lo interesante es que Parsons,intenta superar la distinción entre acción y sistema, entre subjetivismo y objetivismo en Teoría sociológica. Curiosamente cada uno de los momentos de su Teoría se fue desplazando, y en el fondo inconscientemente terminó siendo el representante de cada uno de esos enfoques.

Ya entendiendo esto, podemos entender la teoría sistémica de Parsons. El sistema que lo engloba todo es el "sistema cultural", el cual es el que regula las orientaciones; adentro de este está el "sistema social" el cual es que engloba los medios y condiciones; y adentro de este sistema, está el "sistema de la personalidad", que es el que ubica al actor y sus necesidades individuales. También se puede decir que dentro del sistema de la personalidad esta el biológico. 

En general, un individuo dentro de un sistema social siempre va tener un estatus, que es su ubicación en la sociedad y un rol que es la función que cumple dentro un sistema social. Todo sistema social tiene siempre necesidades mínimas de satisfacer, estas son los prerrequisitos funcionales, los cuales son necesidades del sistema social en general. Y con esto, Parsons solucionaría el problema del orden hobbesiano, ya que los individuos funcionarían a través de sus roles para cumplir con estos prerrequisitos, los cuales son: primero, los sistemas sociales deben estar estructurados de manera que sean compatibles con otros sistemas; segundo, el sistema social debe contar con el apoyo de otros sistemas; tercero, debe satisfacer una parte significativa de las necesidades de los actores; cuarto, debe fomentar en sus miembros una participación suficiente; quinto, debe ejercer control sobre las conductas potencialmente desintegradoras; sexto, si surge un conflicto lo debe controlar; y, séptimo, requiere un lenguaje para poder sobrevivir.

Estos prerrequisitos hacen los cuatro subsistemas famosos de Parsons, formados por cuatro imperativos funcionales (AGIL) necesarios en todo sistema, que son: la "adaptación" (A), esto es, todo sistema debe abarcar las situaciones externas, debe adaptarse a su entorno y adaptar el en torno a sus necesidades; la "capacidad para alcanza metas" (G); la "integración" (I), es decir, regular la interrelación entre los otros imperativos funcionales; la "latencia" (L), un sistema debe proporcionar, mantener y renovar la motivación de los individuos y las pautas culturales que crean y mantienen la motivación. El sistema general de la acción está compuesto en la adaptación por el "sistema orgánico", en las metas por el "sistema de personalidad", en la integración por el "sistema social" y en la latencia por el "sistema cultural". Ejemplificando, este sistema sería como un juego de muñecas rusas, y Parsons se da el lujo de describirnos como es adentro del sistema social: en la adaptación está la economía, en metas está la política, en la integración está la comunidad y en la latencia esta endoculturación. La gracia es que estos sistemas interactúen entre sí y funcionen como sistema.

Procesos sociales: el funcionamiento de todo sistema supone resolver los siguientes problemas.

Quien más haya cumplido con los fines va a estar más alto en la pirámide social. Según Parsons cada persona tiene el lugar que se merece dentro de la pirámide. El fracaso de las sociedades es el fracaso individual.

Parsons desarrolló sus ideas durante un periodo en el que la teoría de sistemas y la cibernética estaban tomando relevancia en las ciencias sociales y del comportamiento. Tomando elementos de estos enfoques, postuló que los sistemas relevantes tratados en dichas ciencias eran abiertos, esto es, insertos en un entorno con otros sistemas. Para las ciencias sociales y del comportamiento, el sistema más amplio es el sistema de acción, consistente en la interrelación de los comportamientos de los seres humanos inserta en un entorno físico-orgánico.

En la medida que Parsons fue desarrollando su teoría, se vinculó crecientemente a los campos de la cibernética y la teoría de sistemas, tomando los conceptos de homeostasis de Alfred Emerson y de procesos teleonómicos acuñado por Ernst Mayr. Con esto, Parsons intentó equilibrar, por una parte, el psicologismo de la fenomenología con el idealismo y, por otra parte, los tipos puros de lo que él denominaba el complejo utilitario-positivista.

La teoría de Parsons incluye una teoría general sobre la evolución social y una interpretación concreta de los mayores impulsos de la historia mundial. En su teoría sobre la historia y la evolución, plantea que la simbolización cognitiva-constitutiva de la jerarquía cibernética de los niveles de acción sistémicos desempeña, en principio, una función equivalente a la que tiene la información genética del ADN sobre el control de la evolución biológica. Sin embargo, esto no quiere decir que el factor del control meta-sistémico determine los resultados, sino que más bien define los límites de orientación de la acción.

Para Parsons, los procesos y entidades transformativas en el nivel constitutivo de la sociedad están, generalmente, en al menos un nivel de análisis empírico, el cual es desempeñado y actualizado por los mitos y religiones. Sin embargo, la filosofía, los sistemas del arte e incluso la semiótica del comportamiento de consumo pueden también, en principio, desempeñar las mismas funciones.

La teoría de Parsons refleja una visión de un concepto unificado de las ciencias sociales y de los sistemas vivos en general. Su aproximación difiere esencialmente de la teoría de Niklas Luhmann, ya que Parsons rechaza la idea de que los sistemas puedan ser autopoiéticos, se trate de sistemas de acción actual o de actores individuales. Para él, los sistemas tienen capacidades inmanentes, pero solo como resultado de los procesos institucionalizados de los sistemas de acción, correspondientes, en último análisis, al esfuerzo histórico de actores individuales. Mientras Luhmann se queda en la inmanencia sistémica pura, Parsons insiste en que la pregunta por los procesos autocatalíticos y homeostáticos no es excluyente de la pregunta sobre cuál es el primer movilizador del actor. En este sentido, los procesos homeostáticos pueden ser necesarios si es que ocurren, pero la acción es necesaria.

Los dichos de Parsons sobre la referencia última en el marco de la acción deben entenderse como la idea de que los sistemas de mayor orden cibernético en la historia tenderán a controlar las formas sociales organizadas en menores niveles de la jerarquía cibernética. Para Parsons, los niveles más altos de la jerarquía cibernética en el nivel general de la acción corresponden a la parte constitutiva del sistema cultural (es decir, la "Latency" en el esquema AGIL). Sin embargo, en los procesos de interacción al interior del sistema, el análisis debe centrarse especialmente en el eje cultural-expresivo (la línea L-G en AGIL). El término "constitutivo" es entendido por Parsons como referente a valores culturales altamente codificados, sobre todo elementos religiosos. No obstante, no queda clara la interpretación de dicho término a lo largo de su obra.

Los sistemas culturales tienen un estatus independiente de las pautas normativas y orientadoras del sistema social, razón por la cual ninguno de estos sistemas es reductible al otro. Por ejemplo, la cuestión sobre el capital cultural de un sistema social como entidad histórica pura (o sea, en su función como sistema fiduciario) no es equivalente a los mayores valores culturales de ese sistema. Esto ocurre porque en el sistema cultural está encarnado una lógica metaestructural que no puede ser reducida a ningún sistema social dado o no puede ser vista como una deducción materialista (o comportamental) de las necesidades del sistema social. En ese contexto, la cultura tendría un poder de transición independiente, no sólo como factor de las unidades socioculturales actuales (como la civilización occidental), sino que también como bases culturales originales que tienden a universalizarse mediante la interpenetración y expansión sobre un gran número de sistemas sociales. Lo segundo ocurrió con la Grecia Clásica y el antiguo Israel, en donde las bases sociales originales desaparecieron, pero el sistema cultural sobrevivió como una pauta cultural independiente y en funcionamiento (lo que se evidencia en los casos de la filosofía griega o del cristianismo como derivación modificada de sus orígenes en Israel).

La diferencia entre Parsons y Jürgen Habermas reside principalmente en el modo en que Habermas usa la teoría de la acción de Parsons para establecer las proposiciones básicas de la suya. Habermas rescata la separación hecha por Parsons entre las dimensiones internas y externas del sistema social, denominando a las primeras "mundo de la vida" (I y L en AGIL) y a las segundas "sistema" (A y G en AGIL). Desde el punto de vista de Parsons, este modelo presenta dos problemas. En primer lugar, el conflicto dentro del sistema social puede surgir en realidad desde cualquier punto relacional, y no simplemente de la dicotomía sistema/mundo de la vida. En segundo lugar, al relacionar el modelo de sistema/mundo de la vida a un tipo de liberación, Habermas plantearía la noción utópica de que el conflicto potencial al interior de un sistema social tiene una solución final, generando un concepto confuso de la naturaleza del conflicto sistémico.




En español


</doc>
<doc id="22018" url="https://es.wikipedia.org/wiki?curid=22018" title="Miffy">
Miffy

Miffy es un personaje infantil creado por Dick Bruna. Miffy es una conejita. Su nombre original en el idioma neerlandés es "Nijntje", que viene de "konijntje", el cual se traduce como "conejito". En neerlandés su apellido es "Pluis". (Nijntje [ˈnɛɪ̯ncǝ])

El primer libro de Miffy fue producido en 1955, y casi 30 más lo han seguido. En total, vendieron más de 85 millones de copias en 40 idiomas y dieron lugar a dos series de televisión diferentes, así como artículos de ropa y juguetes con el dibujo del personaje. Un largometraje, "Miffy the Movie", fue lanzado el 30 de enero de 2013. En España se estrenó con el título "Miffy y sus amigos".

Se han producido al menos dos series de televisión basadas en el personaje: "Miffy y sus amigos", que se emitió del 2003 al 2007 en la cadena de televisión por cable Nick Jr., en Estados Unidos, y "Las aventuras de Miffy, grandes y pequeñas", que se estrenó el 2 de octubre de 2015 en la misma cadena. Esta segunda serie se emite en España, en la cadena de televisión Clan RTVE.

Miffy se convirtió en conejito hembra después de que Bruna decidiera que quería dibujar un vestido en lugar de pantalones a su conejo. Dependiendo de la historia, Miffy puede tener una edad que va desde ser bebé hasta tener cuatro años.

Al principio, Miffy parecía un animal de juguete con orejas caídas pero, en 1963 ya se veía como la vemos hoy, una forma estilizada de un conejo. Miffy es dibujada con el estilo línea clara, muy pocas líneas y uno o dos colores primarios, introducido por Hergé. Bruna eligió usar solo negro, blanco, los colores primarios (rojo, amarillo y azul), verde y naranja. Es este uso de colores primarios lo que hace a Miffy reconocible al instante, y también popular entre los preescolares, debido a sus brillantes e intensos colores sencillos.

Ahora ya hay casi 32 títulos de Miffy y muchos más de los otros personajes. Bruna ha producido un total de 124 álbumes ilustrados para niños. Los libros de Miffy contienen cada uno doce páginas de historia. Cada página tiene una ilustración y cuatro líneas de verso, la última palabra de la segunda línea rima con la última de la cuarta. Tratan sobre temas que los niños pueden entender y situaciones que enfrentarán, como ir al hospital o a la escuela, y siempre tienen un final feliz. Algunos libros no tienen texto, como "Miffy's Dream". Los libros están impresos en pequeño formato. Bruna considera importante que su público sienta que sus libros están ahí para ellos, no para sus padres. La mayoría de los libros de Miffy están recomendados para edades entre 4 y 8 años.

Miffy nació en 1955, cuando Dick Bruna contaba a su hijo de un año historias sobre un pequeño conejo que habían visto ese mismo día.

Los libros de Bruna se han traducido a más de 50 idiomas diferentes y se han vendido más de 85 millones de copias en todo el mundo. Ha ganado muchos premios por sus libros, como el "Golden Brush" en 1990, por "Boris Beer ("El oso Boris")" y "Silver Brush" por " nijntje in de tent" ("Miffy en la tienda") en 1996. En 1997, recibió el "Silver Slate "por lieve oma pluis ("Querida abuela Bunny"), un libro donde la abuela de Miffy enferma y muere. En 2016 le concedieron el prestigioso premio trianual Max Velthuijs.

Otros personajes que aparecen en los libros son su familia: los padres de Miffy, su abuela y su abuelo paterno, su tía paterna Alicia, y el tío Brian, un amigo de la familia, que aparece en "Miffy va a volar". Miffy tendrá un nuevo hermano o hermana " kleine pluis" ("El bebé"). También tiene muchos amigos, los osos Boris y Barbara, que aparecieron por primera vez en 1989 y son pareja, Poppy Pig, que apareció en 1977 y su sobrina Grunty, Snuffy, que apareció en 1969, y otros conejitos como Aggie y Melanie. 

Miffy fue, en origen, un personaje de álbum infantil, pero su diseño se usa ahora en muchas otras cosas, como ropa, artículos de papelería, juguetes, vasos, artículos para el hogar, etc.

A principios de la década de 1990, una ilustración de Miffy sosteniendo una llave ajustable detrás de su espalda, apareció en folletos producidos por personas que tomaban acciones directas contra el programa de construcción de carreteras del gobierno del Reino Unido. Este uso no autorizado del personaje se propagó y Miffy se convirtió en mascota para los grupos involucrados en la acción directa ecológica radical.

Miffy apareció en su primer programa de televisión en 1992, llamado "Dick's bruna Miffy Storybook Classics". Cada episodio fue animado de manera tradicional y duraba aproximadamente cinco minutos. El programa se emitió en los Países Bajos, en el Reino Unido , en Canadá, en Australia y en los Estados Unidos.

De 2003 a 2007, "Miffy y sus amigos" se emitió en canales de televisión infantiles como Treehouse en Canadá y Noggin en los Estados Unidos. Luego se trasladó a la televisión pública. El programa agregó varios personajes nuevos, como la familia africana de Melanie y la familia del primo común de Boris y Bárbara, Umik. La serie fue producida por Pedri Animation BV, una compañía holandesa de animación stop-motion.

A veces se cree que Miffy es un personaje japonés, por el estilo de línea similar de Hello Kitty, creado por Sanrio en 1974. La marca Miffy es muy popular en Japón, con grandes ventas de sus productos fabricados en país. En una entrevista para The Daily Telegraph, Bruna expresó su aversión por Hello Kitty.

Además, el 26 de agosto de 2010, Mercis BV, en representación de Bruna, presentó una demanda contra Sanrio alegando que uno de los personajes acompañantes de Hello Kitty, un conejo llamado Cathy, infringía los derechos de autor y la marca registrada de Miffy. El 2 de noviembre de 2010, un tribunal holandés falló en contra de Sanrio y ordenó a la empresa que dejara de comercializar productos de Cathy en Bélgica, Luxemburgo y los Países Bajos. El 7 de junio de 2011, después del terremoto y tsunami de Tōhoku en Japón, Sanrio y Mercis llegaron a un acuerdo extrajudicial por el que Sanrio detendría la producción de artículos de Cathy. En lugar de continuar la batalla judicial, las dos compañías anunciaron que donarían las tarifas legales que habrían gastado en ella, a la ayuda a las víctimas del terremoto.
En la ciudad natal de Bruna, Utrecht, hay una plaza llamada así por Nijntje, Nijntjepleintje (iluminada: la placita Nijntje) y en 2006, el Centraal Museum abrió una exposición permanente, el "Dick Bruna huis" (casa de Dick Bruna).

Los homónimos de Miffy incluyen una nueva especie de insecto de Perú. Al insecto se le dio el nombre científico "Trichadenotecnum miffy" en 2008, porque su epítopo, un apéndice en su abdomen, se asemeja a un pequeño conejo. En julio de 2014, Bruna anunció su retiro; los derechos del personaje de Miffy no se venderán. El 16 de febrero de 2017, Bruna murió a la edad de 89 años.


Algunos libros de Miffy traducidos al español



</doc>
<doc id="22021" url="https://es.wikipedia.org/wiki?curid=22021" title="Apollon (Cliente P2P)">
Apollon (Cliente P2P)

Apollon es un cliente para redes de distribución de archivos entre pares (P2P) para el proyecto KDE, distribuido bajo los términos de la licencia GNU.

Apollon utiliza el demonio giFT para comunicarse con los distintos protocolos de red, el cual debe haber sido instalado previamente. Apollon (o más justamente, giFT) soporta por el momento los protocolos FastTrack (utilizado por Kazaa), OpenFT ("Open FastTrack"), Gnutella y Ares.

Apollon cuenta con una interfaz similar a la de otros clientes de redes de archivos modernos, dividida en cinco secciones: información, búsqueda, transferencias, directorios compartidos y reproductor multimedia.

El nombre Apollon fue elegido por el dios griego de las artes, Apolo (también conocido como "Apolón").




</doc>
<doc id="22022" url="https://es.wikipedia.org/wiki?curid=22022" title="GiFT">
GiFT

giFT es un dominio (programa de computadora o servicio) creado para servir de nexo entre los distintos protocolos de redes de distribución de archivos y una interfaz gráfica. Utiliza "plugins" para cargar dinámicamente los diferentes protocolos a medida que un cliente lo solicite.

Los clientes que implementen interfaces gráficas para giFT se comunican con el proceso usando un protocolo de red ligero. Esto permite que el código del protocolo de red sea abstraído de la interfaz de usuario. giFT es escrito utilizando código multiplataforma C, que significa que puede ser compilado y ejecutado en una gran variedad de sistemas operativos. Existen varias GUI para OS como Microsoft Windows, Apple y Unix-like.

giFT (giFT Internet File Transfer) es un acrónimo recursivo, que quiere decir que una de las letras representa el propio acrónimo.
Uno de los inconvenientes del núcleo de giFT es que actualmente carece de soporte unicode, lo que impide el intercambio de archivos con nombres que contengan caracteres unicode (como "ø","ä", "å", "é" etc). También, giFT carece de muchas características necesarias para usar la red Gnutella efectivamente. 

Los protocolos soportados actualmente por giFT son: Gnutella, Ares Galaxy y OpenFT. Un plugin para FastTrack (el protocolo utilizado por Kazaa) se encuentra en estado beta, mientras que otro para OpenNap se encuentra en etapa temprana de desarrollo.

El proyecto giFT se encuentra fuertemente ligado al proyecto OpenFT, una reimplementación del protocolo FastTrack producido a través del conocimiento adquirido de la ingeniería inversa de FastTrack. OpenFT implementa nodos de búsqueda y supernodos índices al igual que FastTrack.

Para la comunicación con la interfaz gráfica utiliza un protocolo liviano, el cual permite una abstracción completa del protocolo de red utilizado. Existen múltiples interfaces disponibles para giFT, tanto para Windows, Macintosh o GNU/Linux.



</doc>
<doc id="22024" url="https://es.wikipedia.org/wiki?curid=22024" title="Demonio (desambiguación)">
Demonio (desambiguación)

Por demonio se puede entender:

Grupo musical

Software

Fenómeno Atmosférico

Demonio de Tasmania

Personajes

Fiestas

</doc>
<doc id="22026" url="https://es.wikipedia.org/wiki?curid=22026" title="Armenia">
Armenia

Armenia (en armenio: Հայաստան, "Hayastan"), oficialmente República de Armenia, es un país del Cáucaso Sur y sin salida al mar localizado entre Europa Oriental y Asia Occidental. Comparte frontera al oeste con Turquía, al norte con Georgia, al este con Azerbaiyán y al sur con Irán y la República Autónoma de Najicheván de Azerbaiyán.

Armenia es una antigua república soviética y un Estado unitario, multipartidista y en un proceso de democratización que tiene sus raíces en una de las más antiguas civilizaciones del mundo. Dotada de un rico patrimonio cultural, se destacó como la primera nación en adoptar el cristianismo como religión oficial en los primeros años del siglo IV (la fecha tradicional es 301). Aunque Armenia es un Estado constitucional secular, la fe cristiana desempeña un papel primordial en su historia y en la identidad del pueblo armenio.

Cultural, histórica y políticamente, Armenia se considera como parte de Europa. Sin embargo, su localización en el Cáucaso meridional la sitúa en una supuesta frontera imaginaria entre Europa y Asia: en realidad se trata de un país transcontinental, a medio camino entre los dos ámbitos geográficos. Estas clasificaciones son arbitrarias, pues no hay diferencia geográfica fácilmente definible entre Asia y Europa.

Armenia es actualmente miembro de más de 35 organizaciones internacionales, incluyendo las Naciones Unidas, el Consejo de Europa, el Banco Asiático de Desarrollo, la Comunidad de Estados Independientes, la Organización Mundial del Comercio y la Organización de Cooperación Económica del Mar Negro. Es uno de los integrantes de la Asociación para la Paz de la Organización del Tratado del Atlántico Norte, así como de la alianza militar Organización del Tratado de la Seguridad Colectiva (OTSC). Es también miembro observador de la Comunidad Económica Eurasiática, de la Francofonía y del Movimiento de Países No Alineados.

El nombre nativo del país en armenio es "Hayk‘". Este nombre se transformó durante la Edad Media en "Hayastán", con el sufijo persa “"-stán"”, que significa "país". El origen de la autodenominación armenia "hay" es incierto. Ha sido tradicionalmente derivada de Hayk (Հայկ), el legendario patriarca de los armenios, que según la tradición era hijo de Torgom o Togarma, a su vez hijo de Gomer (identificado con los cimerios), a su vez hijo de Jafet y nieto de Noé (). Según el historiador Moisés de Corene (siglo V), Hayk derrotó y mató al gigante babilonio Belo en una batalla cerca de las montañas del lago de Van, en el sudoeste de la Armenia histórica (Turquía oriental actual), tradicionalmente fechada en 2492 a. C. Según algunos historiadores contemporáneos, "hay" proviene del país de Hayasa, mencionado en las escrituras cuneiformes hititas de los siglos XIV-XIII a. C. como uno de sus estados vasallos.

Diversos exégetas bíblicos han identificado a Armenia como el sitio del jardín del Edén, y se ha interpretado que el monte Ararat es la montaña sobre la cual se posó el Arca de Noé después del Diluvio universal (Génesis 8:4).

El origen del exónimo "Armenia" también es incierto. Varios eruditos armenios, incluyendo Rafael Ishjanyán (1989), lo han identificado con "Armani" (Armanum, que también se lee Armanim) mencionado entre los enemigos derrotados por el rey acadio Naram-Sin (2300 a. C.), localizándolos en las montañas de la Armenia meridional. Según algunos historiadores, la primera mención de la voz "Armina" aparece en la inscripción cuneiforme de Behistun del rey Darío I de Persia (ca. 519 a. C.). El término griego "Armenioi" aparece en Heródoto (""), quien sostiene que los armenios eran colonos de Frigia. La etimología tradicional para el etnónimo es su derivación de Aram, bisnieto del bisnieto de Hayk, como lo hace Moisés de Corene.

Armenia se ha poblado desde épocas prehistóricas. Los arqueólogos continúan revelando indicios de que Armenia y sus montañas estuvieron entre los primeros lugares donde se asentó la civilización humana. A partir del 4000 hasta el 1000 a. C., las herramientas y los utensilios de cobre, de bronce y de hierro fueron producidos en Armenia y negociados comúnmente en tierras vecinas donde esos metales eran menos abundantes. El territorio de Armenia es también una de las posibles localizaciones del legendario país de "Aratta", mencionado en las fuentes sumerias.

Durante la Edad de Bronce, varios estados prosperaron, incluyendo el Imperio hitita (en su máximo esplendor), Mitani (Armenia histórica del sudoeste) y de Hayasa-Azzi (siglo XV a. C.) y en la Edad de Hierro, los indoeuropeos frigios y mushkis llegaron y destruyeron el reino de Mitanni; también floreció la gente de Nairi (siglo XII al IX a. C.) y el reino de Urartu (siglo IX al VI a. C.), pero la aportación de cada pueblo en la etnogénesis de la gente armenia es incierta. Algunos discutirían sobre una mayor influencia de los hurritas en la Armenia temprana, pero basado en patrones drásticos diversos de la lengua, la mayoría acepta que los armenios pertenecen al grupo de pueblos indoeuropeos mientras que Urartu pertenece a la familia hurro-urartiana. Ereván, la capital moderna de Armenia, fue fundada en 782 a. C. por el rey Argishti I de Urartu.

Alrededor del 600 a. C., se estableció el reino de Armenia bajo la Dinastía Oróntida y existió bajo varias dinastías locales hasta el año 428 a. C.

Tras la derrota del Imperio seléucida, uno de los estados sucesores del imperio de Alejandro Magno, en la batalla de Magnesia a manos de Roma (190 a. C.), el gobernador de Armenia, Artashes, restableció la independencia de Armenia —conocida como Armenia Mayor— y fundó la dinastía Artáxida, (190 a. C.), que perduró hasta el 1 d. C. Al mismo tiempo, el reino de Sofene, al sudoeste de la meseta de Armenia, restableció su independencia bajo Zariadres. La zona de Armenia Menor, al noroeste de la meseta, permaneció bajo el control de los reinos de Capadocia y de Ponto, y posteriormente fue anexionada al Imperio romano.

El reino de Armenia alcanzó su máxima expansión entre el 95 a. C. y el 66 a. C. bajo Tigranes el Grande, cuando se convirtió brevemente en un imperio, extendiéndose desde el mar Caspio hasta el mar Mediterráneo y desde el Cáucaso hasta la frontera de Palestina.

A través de su historia, el reino de Armenia gozó de períodos de independencia intermitentes y períodos de autonomía conforme a los imperios contemporáneos. Reyes apoyados o impuestos por el Imperio romano o por Partia, o acordados por ambos, fundaron y destruyeron dinastías, como fue la dinastía arsácida establecida a partir del año 53 por Tirídates I. La localización estratégica de Armenia entre dos continentes la ha sometido a sucesivas invasiones de asirios, persas, romanos, bizantinos, árabes, turcos selyúcidas, mongoles, turcos otomanos y rusos.

En el año 301, Armenia se convirtió en el primer país del mundo en adoptar el cristianismo como religión oficial del Estado, por influencia de Gregorio I el Iluminador considerado hoy en día santo patrón de la Iglesia apostólica armenia. Tiridates III (238-314) fue el primer gobernante que oficialmente se propuso cristianizar a su gente, y su conversión ocurrió doce años antes de que el Imperio romano concediera al cristianismo la tolerancia oficial bajo Constantino I y casi ocho décadas antes de que Teodosio I el Grande adoptara el cristianismo como religión oficial del imperio (380). En el año 405, Mesrob Mashtóts creó el alfabeto armenio.

Después de la caída del reino de Armenia en 428, la mayor parte del país fue incorporada al Imperio sasánida, gobernado por un marzpan. Después de una rebelión en 451, los armenios mantuvieron su libertad religiosa, mientras que Armenia ganó autonomía y el derecho a ser gobernada por un marzpan nativo, mientras que otros territorios imperiales fueron gobernados exclusivamente por persas. El marzpanato de Armenia duró hasta 640, cuando la Persia sasánida fue destruida por el Califato árabe.

Tras la conquista árabe de Armenia, Armenia fue inicialmente agrupada en una unidad administrativa con el nombre de Arminiyya, que también incluyó partes de Georgia y de Albania caucásica y tenía su centro en la ciudad armenia de Dvin, bajo un gobernador árabe, llamado vostikan. A mediados del siglo IX la administración fue delegada en el príncipe de Armenia, reconocido por el califa y el emperador bizantino. El principado de Armenia duró hasta el año 884, cuando el país recuperó la independencia del debilitado imperio árabe.

El reino armenio fue gobernado por la dinastía Bagrátida hasta 1045, con capital en la ciudad de Ani. Al mismo tiempo, varias áreas de la Armenia bagrátida se separaron formando reinos y principados feudales, como el principado de Vaspurakan, gobernado por la casa de Artzruni, pero que a la vez reconocían la supremacía de los reyes bagrátidas.

En 1045, el Imperio bizantino conquistó Ani y acabó con el reino de la Armenia bagrátida. Pronto, los otros estados armenios cayeron también bajo el control bizantino. La dominación bizantina fue breve, ya que los turcos selyúcidas derrotaron en el 1071 a los bizantinos y conquistaron Armenia en la batalla de Manzikert, estableciendo el Imperio Selyúcida. Para escapar de la muerte o servidumbre a manos de los que habían asesinado a su pariente Gagik II, rey de Ani, un armenio de nombre Rubén se adentró en los desfiladeros de los montes Tauros con algunos compatriotas. Llegó luego a Tarsos, en Cilicia, donde el gobernador bizantino le dio protección, y donde sería finalmente establecido el Reino armenio de Cilicia.
El Imperio selyúcida pronto comenzó a derrumbarse. A principios del 1100, los príncipes armenios de la familia noble Zakarida establecieron un principado armenio semi-independiente en Armenia norteña y del este, conocida como Armenia zakarida. La familia noble de Orbeliano compartió el control con el Zakarida en varias partes del país, especialmente en Vayots Dzor y Syunik. Seguía habiendo las partes meridionales de Armenia bajo control de dinastías kurdas de Shaddadids y de Ayyubids.

En 1230 el ilkanato mongol conquistó el principado de Zakaryan, así como el resto de Armenia. Las invasiones mongolas pronto fueron seguidas por las de otras tribus asiáticas centrales, que continuaron desde 1200 hasta 1400. Después de incesantes invasiones, Armenia se debilitó. En el año 1500, el Imperio otomano y el Imperio safávida se repartieron el territorio de Armenia. El Imperio ruso incorporó más adelante Armenia del este (consistiendo en los kanatos de Eriván y de Karabaj dentro de Persia) en 1813 y 1828.

Armenia se convirtió en parte integrante del Imperio otomano con el reinado de Selim II (1524-1574). Sin embargo, la anexión inicial comienza ya con Mehmed II (siglo XV), que ofreció el respaldo otomano para iniciar el Patriarcado Armenio de Constantinopla. Esta situación duró trescientos años, hasta la Guerra Ruso-Turca de 1828-1829, cuando la parte oriental de este territorio fue cedida al Imperio ruso. La parte restante, también conocida como Armenia otomana o Armenia occidental, continuó hasta la finalización de la Primera Guerra Mundial y la partición del Imperio otomano. En los años 1860 surgió el Movimiento de Liberación Nacional de Armenia.

Mientras el imperio comenzaba a derrumbarse, los Jóvenes Turcos derrocaron al gobierno del sultán Hamid. Los armenios que vivían en el imperio esperaban que la revolución de los Jóvenes Turcos cambiase su estado de segunda clase. Sin embargo, con el impacto de la Primera Guerra Mundial y el asalto del Imperio otomano sobre el Imperio ruso, el nuevo gobierno comenzó a mirar a los armenios con desconfianza y suspicacia. Esto era debido al hecho de que el ejército ruso mantuvo un contingente de tropas armenias, integrado por unidades irregulares armenias. El 24 de abril de 1915 las autoridades otomanas arrestaron a los intelectuales armenios.
Con la ley de Tehcir, una gran proporción de armenios que vivían en Anatolia falleció como resultado del genocidio armenio. Había resistencia armenia local en la región, desarrollada contra las actividades del Imperio otomano. Los acontecimientos de 1915 a 1917 se consideran por los armenios y la inmensa mayoría de historiadores occidentales como matanzas totales patrocinadas por el estado.

A pesar de la evidencia abrumadora del intento genocida, las autoridades turcas mantienen actualmente que las muertes fueron resultado de una guerra civil, junto con el hambre y las enfermedades, incluyendo muertes en ambos bandos. La gran mayoría de estimaciones acerca del número de armenios muertos comienzan a partir de los 650 000 hasta el millón y medio de personas. Armenia y su diáspora han estado haciendo campaña desde hace años en busca del reconocimiento oficial de esos acontecimientos como un genocidio. El 24 de abril se conmemora como día del genocidio armenio.

Aunque el ejército ruso tuvo éxito en ocupar la mayor parte de Armenia durante la Primera Guerra Mundial, sus ganancias fueron perdidas con la revolución rusa de 1917. En ese momento, Armenia, Georgia y la parte de Azerbaiyán controlada por los rusos, trataron de adherirse formando la república federativa democrática transcaucásica. Esta federación, sin embargo, duró solamente de febrero a mayo de 1918, cuando las tres partes decidieron disolverla. Consecuentemente, Armenia del este llegó a ser independiente como República Democrática de Armenia (DRA) el 28 de mayo.

Con la partición del Imperio otomano, después de ser derrotado en la Primera Guerra Mundial, se creó un proyecto de estado armenio poco después de su independencia en el Tratado de Sèvres firmado por Turquía y algunos de los aliados de la Primera Guerra Mundial, el 10 de agosto de 1920, que dejó la delimitación de la frontera en manos del presidente estadounidense Woodrow Wilson. El tratado final, sin embargo, no fue firmado por los Estados Unidos, y aunque aceptado por el Imperio otomano, fue rechazado por los turcos, dando lugar a una nueva guerra.

El proyecto de estado, incorporaba las provincias de Erzurum, Bitlis y Van, que eran partes de la región denominada Armenia otomana (conocida también como la "Armenia occidental"). Esta región se amplió hacia el norte, hasta la zona oeste de la provincia de Trebisonda para proporcionar a la República Democrática de Armenia una salida al mar Negro en el puerto de Trebisonda.

La Guerra de Independencia Turca, en la que los turcos vencieron a los armenios y a los griegos, obligó a los aliados a volver a la mesa de negociaciones antes de la ratificación del Tratado. Las partes firmaron y ratificaron el Tratado de Lausana en 1923, que estableció las actuales fronteras de Turquía. Las fronteras orientales las obtuvieron por medio del Tratado de Alexandropol el 2 de diciembre de 1920, y mediante el Tratado de Kars, firmado el 23 de octubre de 1921 y ratificado en Ereván el 11 de septiembre de 1922, con Armenia y la Unión Soviética, confirmando el Tratado de Lausana. El Tratado de Lausana y artículos relacionados no son reconocidos por el actual gobierno de la República de Armenia.

La independencia de breve duración de la DRA acabó con guerra, conflictos territoriales, una afluencia total de refugiados de Turquía, enfermedades y hambre. No obstante, la Entente, aterrada por las acciones del gobierno otomano, intentaron ayudar al nuevo estado armenio a través de fondos y de otras formas de ayuda.

Al final de la guerra, se decidió dividir el Imperio otomano. Firmado entre las potencias aliadas y el Imperio otomano en Sèvres el 10 de agosto de 1920, el Tratado de Sèvres prometió mantener la existencia de la DRA y unir los territorios del Imperio Otomano parcialmente poblados por armenios otomanos. Debido a que las nuevas fronteras de Armenia debían ser dibujadas por el presidente norteamericano Woodrow Wilson, la Armenia otomana también es conocida como “Armenia Wilsoniana.” Se consideró incluso la posibilidad de convertir Armenia en un protectorado bajo tutela de los Estados Unidos. El tratado, sin embargo, fue rechazado por el movimiento nacional turco, y nunca entró en efecto. El movimiento comandado por Atatürk utilizó el tratado como la ocasión para declararse el gobierno legítimo de Turquía y sustituyó la monarquía con capital en Estambul por una república con la suya en Ankara.

En 1920, Armenia y Turquía entraron en guerra, un violento conflicto que terminó con el Tratado de Alexandropol (2 de diciembre de 1920). El tratado de Alexandropol obligó a Armenia a desarmar a la mayoría de sus fuerzas militares, ceder más del 50% de su territorio antes de la guerra, y renunciar a todos los territorios conferidos a su favor en el tratado de Sèvres. Al mismo tiempo, el Undécimo Ejército soviético bajo el mando de Grigori Ordzhonikidze, invadió Armenia en Karavansarái (actual Ijevan) del 29 de noviembre. Al 4 de diciembre las fuerzas de Ordzhonikidze entraron en Ereván y la efímera República de Armenia se derrumbó.

Armenia fue anexionada por la Rusia bolchevique y junto con Georgia y Azerbaiyán, fue incorporada a la Unión Soviética como parte de la RFSS Transcaucásica (RFSST) el 4 de marzo de 1922. Con esta anexión, el Tratado de Alexándropol fue reemplazado por el Tratado turco-soviético de Kars. En el acuerdo, Turquía permitió que la Unión Soviética asumiera el control de Adjara con la ciudad portuaria de Batumi a cambio de la soberanía sobre las ciudades de Kars, Ardahan e Iğdır, todas ellas parte de la Armenia rusa.

La RFSST existió desde 1922 hasta 1936, cuando se dividió en tres entidades separadas (RSS de Armenia, RSS de Azerbaiyán y RSS de Georgia). Los armenios disfrutaron de un período de relativa estabilidad bajo el gobierno soviético. Recibieron medicinas, alimentos y otras provisiones de Moscú, y el gobierno comunista demostró ser un bálsamo calmante en contraste con los turbulentos años finales del Imperio otomano. La situación era difícil para la iglesia, que luchó bajo el gobierno soviético. Después de la muerte de Lenin, Stalin, georgiano de nacimiento, tomó las riendas del poder y comenzó una era de miedo y terror renovados para los armenios.

Armenia no fue el escenario de ninguna batalla en la Segunda Guerra Mundial. Se estima que 500 000 armenios (casi un tercio de la población) sirvieron en el ejército durante la guerra y 175 000 murieron.

Los temores disminuyeron cuando Stalin murió en 1953 y Nikita Jruschov emergió como el nuevo líder de la Unión Soviética. Pronto, la vida en la Armenia soviética comenzó a ver una rápida mejora. La iglesia, que sufrió mucho bajo Stalin, fue revivida cuando el catholicós Vazgén I asumió los deberes de su cargo en 1955. En 1967, se construyó un monumento a las víctimas del genocidio armenio en la colina Tsitsernakaberd sobre la garganta de Hrazdan en Ereván. Esto ocurrió después de las manifestaciones masivas en el quincuagésimo aniversario del trágico evento en 1965.

Durante la era de Mijaíl Gorbachov de la década de 1980, con las reformas de la Glásnost y la Perestroika, los armenios comenzaron a exigir una mejor atención ambiental para su país, oponiéndose a la contaminación que trajeron las fábricas construidas por los soviéticos. También se desarrollaron tensiones entre el Azerbaiyán soviético y su distrito autónomo de Nagorno-Karabaj, una región de mayoría armenia. Alrededor de 484 000 armenios vivían en Azerbaiyán en 1970. Los armenios de Karabaj exigieron la unificación con la Armenia soviética. Las protestas pacíficas en Ereván que apoyan a los armenios de Karabaj se encontraron con pogromos anti-armenios en la ciudad azerbaiyana de Sumgait. Para agravar los problemas de Armenia, tuvo lugar un terremoto devastador en 1988 con una magnitud de 7,2 en la escala de Richter.

La incapacidad de Gorbachov para aliviar cualquiera de los problemas de Armenia creó desilusión entre los armenios y alimentó un hambre creciente de independencia. En mayo de 1990, se estableció el Nuevo Ejército Armenio, que actuó como una fuerza de defensa separada del Ejército Rojo soviético. Pronto se desataron enfrentamientos entre las tropas armenias y soviéticas de las Fuerzas de Seguridad Interna (MVD) con base en Ereván cuando los armenios decidieron conmemorar el establecimiento de la Primera República de Armenia en 1918. La violencia causó la muerte de cinco armenios muertos en un tiroteo con el MVD en la estación de tren. Los testigos afirmaron que el MVD usó fuerza excesiva y que habían instigado la lucha.

Se produjeron nuevos tiroteos entre milicianos armenios y tropas soviéticas en Sovetashén, cerca de la capital, que causaron la muerte de más de 26 personas, en su mayoría armenios. El pogromo de los armenios en Bakú en enero de 1990 obligó a casi todos los 200 000 armenios en la capital azerbaiyana de Bakú a huir a Armenia. El 23 de agosto de 1990, Armenia declaró su soberanía sobre su territorio. El 17 de marzo de 1991, Armenia, junto con las repúblicas bálticas, RSS de Georgia y RSS de Moldavia, boicotearon un referéndum nacional en el que el 78% de todos los votantes votaron a favor de la conservación de la Unión Soviética en una forma reformada.

El 21 de septiembre de 1991, Armenia declaró oficialmente su independencia después del fallido golpe de agosto en Moscú. Levon Ter-Petrosyan fue elegido popularmente como el primer presidente de la recientemente independiente República de Armenia el 16 de octubre de 1991. Se había destacado al liderar el movimiento de Karabaj para la unificación del Nagorno-Karabaj, poblado por los armenios. El 26 de diciembre de 1991, la Unión Soviética dejó de existir y se reconoció la independencia de Armenia.

Ter-Petrosyan dirigió a Armenia junto con el Ministro de Defensa Vazgen Sargsyan a través de la Guerra de Nagorno-Karabaj con el vecino Azerbaiyán. Los primeros años post-soviéticos se vieron empañados por las dificultades económicas, que tuvieron sus raíces al principio del conflicto de Karabaj cuando el Frente Popular de Azerbaiyán logró presionar a la RSS de Azerbaiyán para instigar un ferrocarril y un bloqueo aéreo contra Armenia. Este movimiento efectivamente paralizó la economía de Armenia ya que el 85% de su carga y mercancías llegaban a través del tráfico ferroviario. En 1993, Turquía se unió al bloqueo contra Armenia en apoyo de Azerbaiyán.

La guerra de Karabaj terminó después de que se puso en marcha un alto el fuego mediado por Rusia en 1994. La guerra fue un éxito para las fuerzas armenias de Karabaj que lograron capturar el 16% del territorio internacionalmente reconocido de Azerbaiyán, incluido el propio Nagorno-Karabaj. Desde entonces, Armenia y Azerbaiyán han mantenido conversaciones de paz, mediadas por la Organización para la Seguridad y la Cooperación en Europa (OSCE). El estado de Karabaj aún no se ha determinado. Las economías de ambos países se han visto afectadas por la falta de una resolución completa y las fronteras de Armenia con Turquía y Azerbaiyán permanecen cerradas. Cuando Azerbaiyán y Armenia finalmente acordaron un alto el fuego en 1994, se estima que 30 000 personas murieron y más de un millón fueron desplazadas.

A medida que ha entrado en el siglo XXI, Armenia enfrenta muchas dificultades. Ha hecho un cambio total a una economía de mercado. Un estudio lo clasifica como la 41.ª nación más "económicamente libre" en el mundo a partir de 2014. Sus relaciones con Europa, Oriente Medio y la Comunidad de Estados Independientes han permitido a Armenia aumentar el comercio. El gas, el petróleo y otros suministros llegan a través de dos rutas vitales: Irán y Georgia. Armenia mantiene relaciones cordiales con ambos países.

En 1990, se celebraron las primeras elecciones legislativas democráticas y en 1991 se eligió al primer presidente de la República. Armenia es miembro de la Comunidad de Estados Independientes (CEI). El presidente de la República de Armenia es el jefe de Estado, elegido por sufragio universal directo. El presidente nombra al primer ministro, quien a su vez elige a los ministros de gobierno. Entre 2008 y 2018, Serzh Sargsián, del Partido Republicano, fue el presidente de Armenia, hasta que se vio forzado a renunciar a su cargo en 2018, luego de una serie de manifestaciones que convergieron en la Revolución de Terciopelo.

El máximo órgano legislativo es la Asamblea Nacional de Armenia (Azgayin Zhoghov), con 190 miembros. El parlamento unicameral es actualmente controlado por el conservador Partido Republicano. Los principales partidos de la oposición son Armenia Próspera, la Federación Revolucionaria Armenia, Patrimonio y el Congreso Nacional Armenio. El poder judicial está conformado por el Tribunal Constitucional, la Corte Suprema, el Procurador General y por las cortes menores. La Constitución actual está vigente desde el 5 de julio de 1995 y fue reformada el 27 de noviembre de 2005. Armenia tiene un sistema legal de sufragio universal a partir de los 18 años de edad.

La política de Armenia se ejecuta en el marco de una república democrática. Según la Constitución de Armenia, el presidente es el jefe de gobierno de un sistema multipartidario. El objetivo principal del gobierno armenio es construir un estilo de democracia parlamentaria occidental como la base de su forma de gobierno. Sin embargo, los observadores internacionales del Consejo de Europa y el Departamento de Estado de los Estados Unidos han puesto en duda la imparcialidad de las elecciones parlamentarias y presidenciales y el referéndum constitucional desde 1995, alegando deficiencias en las votaciones, la falta de cooperación por parte de la Comisión Electoral Central y el deficiente mantenimiento de las listas electorales y los lugares de votación. Freedom House Armenia, en su informe de 2008, adjudicó a Armenia la categoría de "régimen autoritario semiconsolidado" (junto a Moldova, Kósovo, Kirguistán y Rusia) y le dio el puesto número 20 en un "ranking" de 29 naciones en transición, con una "puntuación de democracia" de 5,21 sobre 7 (7 representa el progreso democrático más bajo). Desde 1999, la "puntuación de democracia" de Freedom House para Armenia ha declinado firmemente (desde 4,79 a 5,21). Más aún, Freedom House ranqueó a Armenia como "parcialmente libre" en su informe de 2007, aunque no la categorizó como una "democracia electoral", indicando una ausencia de elecciones relativamente libres y competitivas. De todos modos, parecen haber ocurrido progresos significativos y la elección presidencial de 2008 fue interpretada como ampliamente democrática por parte de la OSCE y los monitores occidentales.

La división administrativa del país implica 10 regiones y 21 ciudades; cada distrito tiene cuerpo legislativo y ejecutivo propio.

Armenia actualmente mantiene buenas relaciones con casi todos los países del mundo, con dos importantes excepciones que son sus vecinos inmediatos, Turquía y Azerbaiyán. Las tensiones fueron creciendo con fuerza entre armenios y azerbaiyanos durante los últimos años de la Unión Soviética. La guerra de Nagorno Karabaj dominó la política de la región durante todo el decenio de 1990. La frontera entre los dos países rivales permanece cerrada hasta el día de hoy, sin que se haya llegado a una solución permanente para el conflicto, pese a la mediación proporcionada por organizaciones tales como la OSCE.

Turquía también tiene un largo historial de malas relaciones con Armenia sobre todo por su negativa a reconocer el genocidio armenio de 1915. El conflicto de Karabaj se convirtió en una excusa para Turquía y así poder cerrar su frontera con Armenia en 1993. No ha levantado el bloqueo a pesar de las presiones internas turcas interesados en los mercados de Armenia y los pedidos de Armenia de abrir las fronteras.

Debido a su posición hostil entre sus dos vecinos, Armenia mantiene estrechos vínculos de seguridad con Rusia. A petición del gobierno de Armenia, Rusia mantiene una base militar en el noroeste de la ciudad armenia de Gyumri como elemento de disuasión contra Turquía. A pesar de ello, Armenia también se ha acercado a las estructuras euroatlánticas en los últimos años. Mantiene buenas relaciones con los Estados Unidos, especialmente debido a la diáspora armenia en ese país, pues (según el censo de 2000) hay 385 488 armenios viviendo en el país.

Armenia es miembro del Consejo de Europa, mantiene relaciones amistosas con la Unión Europea, especialmente con Francia y Grecia, ya que una encuesta en 2005 informó que el 64 % de la población de Armenia se manifestó a favor de la adhesión a la UE y varios funcionarios armenios también han expresado el deseo de que su país, a la larga, llegue a convertirse en estado miembro, ya que algunos predicen que se hará una oferta oficial de ingreso en unos pocos años. También se ha examinado qué parte de la sociedad está a favor de unirse a la OTAN. Sin embargo, el presidente Serzh Sargsyán quería mantener a Armenia vinculada a la Federación Rusa y a la CEI.

El Ejército Armenio, la fuerza aérea, la Defensa Aérea, y la Guardia fronteriza abarcan las cuatro ramas de las fuerzas armadas de la República de Armenia. Esta estructura de los militares armenios viene dispuesta desde el derrumbamiento de la Unión Soviética en 1991 y con el establecimiento del Ministerio de Defensa en 1992. El comandante en jefe es el gobernante electo de la república. El Ministerio de Defensa está a cargo de la dirección política, dirigida actualmente por Serzh Sargsyan, mientras que sigue habiendo un comando militar en las manos del Estado Mayor, dirigido por el jefe de personal, que actualmente es el coronel general Yuri Grigor Khachaturov.

Las Fuerzas armadas activas se componen de cerca de 60 000 soldados, con una reserva adicional de 32 000, y una “reserva de la reserva” de 350 000 tropas. Los guardias fronterizos están a cargo de patrullar las fronteras del país con Georgia y Azerbaiyán, mientras que las tropas rusas supervisan las fronteras con Irán y Turquía. En el caso de un ataque eventual, Armenia está preparada para movilizar a cada hombre y mujer en estados de salud razonables de entre 24 y 55 años, todos ellos con estado de preparación militar.

El Tratado de las Fuerzas Armadas Convencionales en Europa, que establece límites comprensivos en las categorías dominantes del equipo militar, fue ratificado por el parlamento armenio en julio de 1992. En 1993, Armenia firmó la multilateral Convención de armas químicas, que llama para la eliminación eventual de las mismas. Armenia entró a formar parte del Tratado de no proliferación nuclear (NPT) en julio de 1993 y es miembro de la Organización del Tratado de la Seguridad Colectiva (CSTO) junto con Bielorrusia, Kazajistán, Kirguistán, Rusia, Tayikistán y Uzbekistán. También es miembro de la Asociación para la Paz. Es parte de una organización de la OTAN llamada Consejo Social Euro-Atlántico (EAPC) y ha participado a la misión pacificadora dentro de Kosovo. La compañía 46 formó parte de la Coalición internacional que invadió Iraq en 2003.

Armenia está dividida en 11 provincias. Estas se llaman "marzer" ("մարզէր") o en la forma singular "marz" ("մարզ") en armenio.

Armenia está situada en la Transcaucasia, la zona al suroeste de Rusia, entre el mar Negro y el mar Caspio. La Armenia moderna ocupa parte de la Armenia histórica, cuyo centro estaba en el valle del río Araks y la región alrededor del lago Van en Turquía. Armenia limita al norte con Georgia, al este con Azerbaiyán, al suroeste con la República autónoma de Najicheván, al sur con Irán y al oeste con Turquía. La geografía de la actual Armenia es la de un país sin salida al mar situado en Asia menor.

El terreno armenio es principalmente montañoso, con ríos rápidos y pocos bosques. El clima es continental: veranos calurosos e inviernos fríos. Ningún punto del país está por debajo de los 400 metros por encima del nivel del mar. El monte Ararat, un símbolo armenio, es la montaña más alta de la región y se encuentra en territorio de Turquía, por lo que la máxima altitud de Armenia es el Aragats.

La contaminación producida por productos químicos tóxicos, como el DDT, no contribuyen al enriquecimiento del suelo armenio, que ya de por sí es de mala calidad. Un bloqueo de las comunicaciones, llevado a cabo por Turquía debido al conflicto con Azerbaiyán, ha resultado en un proceso de deforestación.

Armenia está intentando resolver sus problemas ecológicos. Se ha creado un "Ministerio de Protección de la Naturaleza", a la vez que se castigó con impuestos la contaminación del aire y del agua, así como la generación de residuos tóxicos sólidos, cuyas recaudaciones se usan para llevar a cabo proyectos de protección y recuperación ambiental. El gobierno armenio planea cerrar la única planta de energía nuclear, Metzamor, que data de la época soviética, tan pronto como se consigan explotar fuentes de energía alternativas.


Hace veinticinco millones de años, una agitación geológica empujó la corteza terrestre para formar la meseta armenia, creando así la compleja topografía de Armenia. La cadena montañosa del Cáucaso Sur se extiende desde el norte del país, siguiendo hacia el sureste entre el lago Seván y Azerbaiyán, pasando luego por la frontera armenio-azerí hasta Irán. Así situada, las montañas hacen que el viaje norte-sur y sur-norte sea muy dificultoso. El proceso geológico continúa hoy día, y sus más grandes manifestaciones son en algunos casos terremotos y sismos de escala menor. En diciembre de 1988, la segunda ciudad más grande del país, Gyumri, anteriormente conocida como Leninakán, sufrió serios daños a causa de un terremoto que mató a más de 25 000 personas.

Su territorio ocupa una superficie de 29 743 km², que a título comparativo es casi exactamente la misma extensión territorial de Bélgica. Aproximadamente la mitad se encuentra a más de 2000 m s.n.m. y sólo un 3 % del mismo está por debajo de los 650 metros sobre el nivel del mar (m s.n.m.). Las zonas de menor elevación se encuentran en los valles de los ríos Aráks y Debet, al norte del país, con altitudes entre los 380 y 430 m s.n.m. respectivamente. La altitud en el Cáucaso Sur varía entre 2640 y 3280 m s.n.m.; al suroeste de esta cordillera, se encuentra la meseta armenia, la cual está salpicada de pequeñas sierras y volcanes, algunos de ellos inactivos. El mayor de estos, el monte Aragáts, de 4095 m s.n.m. de altitud, es también el punto más alto del país. La mayor parte de la población vive en la zona oeste y noroeste del país, donde se encuentran las dos mayores aglomeraciones urbanas: la capital Ereván y Gyumri.

Las temperaturas en Armenia dependen, generalmente, de la elevación. Las formaciones montañosas bloquean las influencias moderadoras del clima que el mar Mediterráneo y el mar Negro generan, lo que crea una gran diferencia climática entre las estaciones del año. En la meseta armenia, la temperatura media en invierno es de cero grados centígrados, mientras que la media en verano excede los 25 °C. Las precipitaciones medias van desde 250 milímetros al año en el valle del río Araks hasta 800 mm en los puntos más altos del país. A pesar de la dureza del invierno en la mayoría del país, la fertilidad del suelo volcánico de la meseta hizo de Armenia uno de los primeros sitios del mundo con agricultura.

El territorio de la República de Armenia es rico en múltiples especies endémicas. En el valle del Aráks se encuentran plantas halófitas. Desde una altura de 1.400 m s.n.m. son comunes las artemisias. En el área montañosa crecen muchos arbustos espinosos y otras plantas. En las montañas altas se presentan plantas xerófilas. Alrededor de los años 1900 los árboles y arbustos cubrían aproximadamente el 25% de la superficie, en 1964 aproximadamente el 15% y en 2005 solamente entre un 8 y un 10%.

En Sangesur, en el sur del país, el límite del bosque llega cerca de 2400 m s.n.m.. El mundo de la planta se asemeja al de las montañas. Hay muchos reptiles, entre ellos el lagarto armenio de roca y algunos venenosos como, por ejemplo, las víboras. También arácnidos, tales como los escorpiones. En los valles húmedos viven cerdos salvajes, chacales, ciervos, visones y águilas; en las estepas sobre todo roedores de las montañas; y en los bosques osos pardos sirios, gatos salvajes y lobos. En el área protegida Chosrow todavía se pueden encontrar linces y algunos leopardos del cáucaso.

El nombre latino del damasco o albaricoque se deriva de Armenia. El albaricoque tiene gran fama y es todo un símbolo nacional armenio, representado por el color de la banda inferior en la bandera de Armenia.

Hasta su independencia, la economía de Armenia se basó en la producción industrial de productos químicos, electrónica, maquinaria, alimento procesado, caucho sintético y textiles, era además altamente dependiente en recursos externos. La agricultura contribuía solo con el 20% del Producto Interno Bruto y el 10% del empleo antes de la desintegración de la Unión Soviética en 1991. La república había desarrollado un sector industrial moderno, máquinas de herramientas que proveían, textiles, y otros productos manufacturados a las repúblicas cercanas a cambio de las materias primas y de energía.

Las minas armenias producen cobre, cinc, oro, y plomo. La mayor parte de la energía se genera con combustible importado de Rusia, incluyendo gas y combustible nuclear (para la única planta de energía atómica); la principal fuente de energía doméstica es hidroeléctrica.

Un auge en curso de la construcción, especialmente en la ciudad de Ereván, ha mantenido el desarrollo económico de Armenia en dígitos dobles. Como otros estados recientemente independientes de la anterior Unión Soviética, la economía de Armenia sufre de la herencia de una economía planificada centralmente y de la interrupción soviética. La inversión y la ayuda soviética en la industria armenia ha desaparecido virtualmente, de modo que pocas industrias soviéticas importantes todavía funcionan allí. Además, los efectos del terremoto del año 1988 en Spitak, en el que murieron más de 25 000 personas y otras 500 000 quedaron sin hogar, todavía se están sintiendo. El conflicto con Azerbaiyán por el enclave de Nagorno-Karabaj no se ha resuelto. El cierre de las fronteras azerís y turcas ha devastado la economía, porque Armenia depende de fuentes exteriores de energía y la mayoría de las materias primas provienen del exterior. Las rutas de tierra a través de Georgia y de Irán son inadecuadas o no fiables. El PIB cayó en casi 60 % a partir de 1989 hasta 1992-1993. La divisa nacional, la copita, sufrió de hiperinflación durante sus primeros años después de su introducción en 1993.

Sin embargo, el gobierno ha podido hacer vastas reformas económicas que han resultado en una inflación más baja y en un crecimiento constante. El cese al fuego en 1994 en el conflicto de Nagorno-Karabaj también ha ayudado a la economía. Armenia ha tenido un fuerte desarrollo económico desde 1995, y la inflación ha sido insignificante para los años recientes. Los nuevos sectores, tales como el de piedras preciosas que procesa y fabrica joyería, tecnologías de información y de comunicación, e incluso turismo están comenzando a suplir sectores más tradicionales en la economía, tal como la agricultura.
Este progreso económico constante ha significado un aumento de la ayuda por parte de las instituciones internacionales. El Fondo Monetario Internacional ("FMI"), el Banco Mundial, el banco europeo para la reconstrucción y el desarrollo ("EBRD") y otras instituciones financieras internacionales (IFI) y los países extranjeros están ampliando concesiones y préstamos considerables. Los Préstamos a Armenia en 1993 excedieron los 1,1 mil millones de dólares. Estos préstamos se dirigen hacia la reducción del déficit presupuestario, la modernización; la privatización de negocios; la energía; la agricultura, la transformación de alimentos, el transporte, y los sectores de la salud y de la educación; y la rehabilitación en curso en la zona del terremoto. El gobierno ingresó a la organización mundial del comercio el 5 de febrero de 2003. Pero una de las fuentes principales de inversiones directas extranjeras sigue siendo la diáspora armenia, que financia una parte importante de la reconstrucción de la infraestructura y de otros proyectos públicos. Siendo un estado democrático cada vez mayor, Armenia también espera conseguir una mayor ayuda financiera del mundo occidental.

En junio de 1994 se aprobó una ley liberal en favor de la inversión extranjera, y en 1997 se adoptó una ley sobre la privatización, así como un programa sobre la privatización del estado. El progreso continuado dependerá de la capacidad del gobierno de consolidar su gerencia macroeconómica, incluyendo el aumento en el tributo de impuestos, la mejora en el clima de inversión, y dando grandes pasos en la lucha contra la corrupción.

En la carta internacional 2005 del CPI de la transparencia (índice de la opinión de la corrupción), Armenia se calificó con un valor de 88 (en una escala de 1 a 158), continuando como uno de los estados menos corruptos entre las antiguas repúblicas soviéticas. Según el informe del desarrollo humano de la ONU en 2005, Armenia tiene un índice de desarrollo humano (IDH) de 83 (en una escala de 1 a 177) el más alto entre las repúblicas transcaucásicas. En el índice 2006 de libertad económica, Armenia se alineó 27º, al lado de Japón y delante de países como Noruega, España, Portugal e Italia. Este resultado pone a Armenia en la categoría de los países “más liberales” , convirtiéndose en el estado más libre económicamente de la Comunidad de Estados Independientes.

Armenia no posee reservas de petróleo o gas natural y lo importa de Rusia por la frontera de Georgia. El gas alimenta las centrales térmicas de Ereván (242 MW) y Hrazdan (1.110 MW), que producen el 24% de la energía del país. En 2007, se terminó la primera fase del gasoducto Irán-Armenia, entre Tabriz y la frontera armenia, y está en proyecto seguir hasta el centro del país para sustituir el gas de Gazprom por gas de Irán, pero los conflictos de la región limitan mucho el uso del gas iraní.

La central nuclear Metsamor (408 MW), la única del país, construida durante la época soviética y que funciona desde 1976, proporciona el 43% de la energía del país. Hay planes de sustituirla por una nueva, debido a su antigüedad.

Armenia posee además 10 centrales hidroeléctricas que proporcionan el 33% de la electricidad, agrupadas en dos cuencas: la del Complejo Hidroeléctrico de Sevan–Hrazdan, también llamada Sevan–Hrazdan Cascade, a lo largo del río Hrazdan y sus tributarios, entre el lago Sevan y la ciudad de Ereván, consistente en siete centrales hidroeléctricas con una potencia combinada de 565 MW, alimentadas por canales y tubos con agua del lago, y la del río Vorotán, también llamada ContourGlobal Hydro Cascade, consistente en cinco embalses y tres plantas hidroeléctricas con una potencia combinada de 404 MW.

Se halla en proyecto la central geotérmica de Jermaghbyur en la provincia de Syunik', capaz de producir 150 MW.

En 2008 se inaugura la central eólica Lori 1, la única del país, en la provincia septentrional de Lorri, con una potencia combinada de 2,6 MW, construida en colaboración con Irán.

Armenia tiene una población de 3 215 800 (censo de abril de 2006), y es la segunda mayor densidad de población de las ex Repúblicas Soviéticas. Se ha producido un problema de disminución de la población debido al aumento en los niveles de emigración tras la desintegración de la URSS. Las tasas de emigración y de la disminución de población, sin embargo, han disminuido drásticamente en los últimos años, con una moderada afluencia de los armenios que regresan a Armenia, se espera que esta tendencia continúe. Armenia, de hecho, espera que reanude su crecimiento positivo de la población para 2010.

El 97,9 % de la población es de origen étnico armenio. Los yazidíes constituyen el 1,3 %, y los rusos el 0,5 %. Otras minorías incluyen los asirios, ucranianos, griegos, kurdos, georgianos, y bielorrusos. También hay pequeñas comunidades de valacos, mordvinos, osetios, udi, y Tats. También existen minorías de polacos y alemanes del Cáucaso, aunque están muy rusificados. Durante la era soviética, los azeríes fueron históricamente la segunda población más grande del país (alrededor de 10% en 1939). Sin embargo, debido a las hostilidades con el vecino Azerbaiyán en la disputada región de Nagorno Karabaj, prácticamente todos ellos emigraron de Armenia. Por el contrario, Armenia recibió una gran afluencia de refugiados armenios de Azerbaiyán, dando así una población armenia de carácter homogéneo.

Armenia tiene una diáspora muy grande (ocho millones según algunas estimaciones, que supera con creces los tres millones de habitantes de la propia Armenia), con comunidades existentes en todo el mundo. Las comunidades más numerosas se pueden encontrar: en Argentina, Australia, Canadá, Chipre, Chile, Colombia, también en la Federación Rusa, Estados Unidos, Francia, Georgia, Irán, Israel en donde cerca de mil armenios residen en el barrio armenio de la Ciudad Vieja de Jerusalén, un remanente de una comunidad otrora mayor. Líbano, Siria, Turquía (en su mayoría dentro y alrededor de Estambul) dentro de los que hay que considerar los 40 000 a 70 000 armenios que aún viven en el país, Uruguay y Ucrania. Además, cerca de 130 000 armenios viven en la disputada región de Nagorno-Karabaj, donde constituyen la mayoría.

La lengua oficial del país es el armenio, y como consecuencia de la etapa soviética, el ruso sigue estando bastante extendido, sobre todo en los ámbitos urbanos. Buena parte de la población urbana (sobre todo en Ereván) puede considerarse bilingüe.

La religión predominante en Armenia es el cristianismo. Las raíces de la Iglesia armenia comienzan en el siglo I. Según la tradición, la Iglesia armenia fue fundada por dos de los apóstoles de Jesús, Judas Tadeo y Bartolomé, quienes predicaron el cristianismo en Armenia entre los años 40 y 60. Debido a estos dos apóstoles, el nombre oficial de la Iglesia armenia es Iglesia apostólica armenia. Armenia fue la primera nación en adoptar el cristianismo como religión del Estado, en el año 301. Cerca del 93% de cristianos armenios pertenecen a la Iglesia apostólica armenia, llamada también "Iglesia Gregoriana", es una de las Iglesias ortodoxas orientales, al igual que la Iglesia copta y la siriaca, y agrupada entre las llamadas miafisitas. Esta iglesia se considera ortodoxa (aunque no debe confundirse con la Iglesia ortodoxa de cuño griego), por haber mantenido la ortodoxia de la doctrina cristiana en conformidad con los padres de la Iglesia. Armenia también tiene una población de católicos de rito armenio de unos 180 000 miembros y de protestantes y seguidores evangélicos de la religión tradicional armenia. Los kurdos de Yazidi, que viven en la parte occidental del país, practican yazidismo. La Iglesia católica de rito armenio tiene su sede en Bzoummar, Líbano.
Armenia se sostiene en parte por una muy importante diáspora armenia alrededor del mundo: en Rusia (2,5 millones), en América del Norte (1,5 millones), en África (15 000), en Siria y Líbano (120 000), en la Unión Europea (500 000), principalmente en Francia; y en América Latina, (125 000), principalmente asentados en Argentina, Brasil, Uruguay, Venezuela, Chile y México.

Más del 93% de los cristianos armenios pertenecen a la Iglesia apostólica armenia, una forma de ortodoxia oriental (no calcedónica ), que es una iglesia muy ritualista y conservadora, aproximadamente comparable a las iglesias copta y siríaca.

La Iglesia evangélica de Armenia tiene una presencia muy importante y favorable en la vida de los armenios con más de varios miles de miembros en todo el país. Su origen se remonta a 1846, que estaba bajo el patrocinio del Patriarcado Armenio de Constantinopla, cuyo objetivo era formar clérigos calificados para la Iglesia apostólica armenia.

Otras denominaciones cristianas que practican la fe basada en el Credo Niceno en Armenia son las ramas pentecostales de la comunidad protestante, como la Palabra de vida, la Iglesia de la Hermandad Armenia, los bautistas, conocidos como las denominaciones más antiguas existentes en Armenia y permitidos por las autoridades de la Unión Soviética, y presbiterianos.

Los católicos también existen en Armenia, tanto de rito latino y de rito armenio católicos. Los Mechitaristas (también deletreados "Mekhitaristas" Armenios: Մխիթարեան), son una congregación de monjes benedictinos de la Iglesia Católica Armenia fundada en 1712 por Mekhitar de Sebaste. Son más conocidos por sus series de publicaciones académicas sobre antiguas versiones armenias de textos griegos antiguos que de otra manera se perderían.

La denominación católica armenia tiene su sede en Bzoummar, Líbano.

Armenia es el hogar de una comunidad rusa de molokanos que practican una forma de cristianismo espiritual originada en la Iglesia ortodoxa rusa.

Los armenios tienen su propio alfabeto e idioma distintivos. El alfabeto fue inventado por Mesrob Mashtóts y consiste en 39 letras (con 36 sonidos fonéticos), tres de las cuales fueron añadidas durante el período de Cilicia. El 96% de los habitantes del país habla armenio, mientras el 75,8% de la población habla además ruso como resultado de la política lingüística soviética. La tasa de alfabetización adulta en Armenia es del 98%. La mayoría de los adultos de Ereván pueden comunicarse en ruso, mientras la popularidad del inglés crece. En 1914, Grigori Nikoláievich Neúimin bautizó con el nombre de Armenia al asteroide 780.

La literatura comenzó en Armenia alrededor del 400 a. C. Crearon la mayoría de las artes literarias cerca de Moses de Khorene, en el siglo V. Con los años, tanto los elementos de la literatura así como las historias y los mitos fueron cambiando a través de las generaciones. Durante el siglo XIX, el escritor Mikael Nalbandián trabajó para crear una nueva identidad literaria armenia. El poema de Nalbandián “La canción de la muchacha italiana” se convirtió en la letra del himno nacional armenio "Mer Hayrenik".

La arquitectura armenia ha desarrollado un estilo propio característico desde el siglo IV que se manifiesta en las iglesias y monasterios construidos en el país a lo largo de su historia, pero también en las construcciones realizadas por las comunidades armenias que han abandonado el país en los últimos siglos.

Armenia es la madre patria del compositor y director clásico contemporáneo Aram Jachaturián. Considerado uno de los grandes músicos del siglo XX junto a Serguéi Prokófiev y Dmitri Shostakóvich forman el trío de grandes sinfonistas de la Unión Soviética. Su carrera se desarrolló principalmente en Moscú. Llegó a popularizarse gracias a la selección de algunos pasajes de su obra por Stanley Kubrick para la banda sonora de "2001 Una Odisea Espacial" («Las Hilanderas» del adagio de "Gayaneh"). Entre sus obras destacan "Gayaneh", "Espartaco" y su contribución con grandes partituras al ballet. También forman parte del patrimonio universal colectivo los trepidantes y vertiginosos compases de la «Danza del sable» el último movimiento de "Gayaneh".

En la actualidad existen varios músicos de renombre internacional procedentes o vinculados con Armenia: el ganador del Premio Grammy Arto Tunçboyaciyan, el violinista y compositor Samvel Yervinyan y, especialmente, el grupo de rock formado en Los Ángeles en 1995 System of a Down cuyos integrantes, Serj Tankian, Daron Malakian, Shavo Odadjian y John Dolmayan, son de origen armenio. Dos exponentes de la música armenia propiamente dicha son Levón Minassián y Armand Amar aunque existe una larga tradición de producción de música regional y tradicional. Otro reconocido cantante de ascendencia armenia es el francés Charles Aznavour.

Armenia forma parte de la UER desde el 2005 hecho que le permitió participar en el Festival de la Canción de Eurovisión al año siguiente pasando la semifinal con muy buenos resultados hasta la edición de 2011 en Düsseldorf. En su breve participación es uno de los países con mejores resultados promediados. Armenia también ha participado en el Festival de la Canción de Eurovisión Junior, habiendo organizado el certamen de la edición de 2011 en la que participaron trece países.

La gastronomía de Armenia está formada por los platos y tradiciones culinarias de los pueblos armenios, incluido los integrantes de la diáspora armenia. La historia de Armenia muestra que tras la destrucción del Imperio seléucida surgió el primer estado armenio independiente, fue fundado en el 190 a. C. por Atarxias, cuyos sucesores se conocen como dinastía Artáxida y siendo después en el siglo XX parte del estado soviético, hecho este último que marcó algunas costumbres culinarias en este país y fijó un cambio de las tradiciones culinarias de muchos siglos. La cocina armenia se caracteriza por estar entre la cocina mediterránea y la del Cáucaso, se trata de un conjunto de elaboraciones características de una población nómada que vive en una región fría. Con grandes influencias de la cocina del Oriente Medio, de Rusia y de los Balcanes.

La Granada es una de las frutas que simbolizan a Armenia y muchas tradiciones armenias se llevan a cabo representando a esta fruta. Por ejemplo en las bodas armenias es tradición lanzar una granada contra la pared, buscando la bendición de sus hijos. La granada también es la diversidad dentro de la unión, representada por los granos. Y, yendo más allá, es un homenaje a los armenios de la diáspora.

En Armenia se juegan muchos tipos de deportes, entre los que destacan la lucha libre, el levantamiento de pesas, el judo, el fútbol, el tenis de mesa, el ajedrez y el boxeo. Armenia es un terreno montañoso y ofrece la oportunidad de que algunos deportes como el esquí y el alpinismo sean practicados masivamente. Al ser un país sin litoral, los deportes acuáticos sólo puede ser practicados en los lagos, en especial el lago Sevan. Competitivamente, Armenia ha tenido éxito en halterofilia y lucha libre.

Armenia es también participante activa en la comunidad deportiva internacional con la plena pertenencia a la Unión de Asociaciones de Fútbol Europeas y la Federación Internacional de hockey sobre hielo. También es sede de la Pan - Juegos de Armenia.

Armenia es una auténtica potencia mundial en el ajedrez. El campeón mundial Tigran Petrosian fue uno de los mayores exponentes del ajedrez de tipo posicional. En la Olimpiada de ajedrez celebrada en Turín en 2006, el equipo masculino se proclamó campeón y el equipo femenino se clasificó en séptimo lugar. Levon Aronian, Vladímir Akopián, Karen Asrian, Smbat Lputian, Gabriel Sargissian y Artashes Minasian conformaron el equipo masculino. Lilit Mkrtchian, Elina Danielian, Nelli Aginian y Siranush Andriasian formaron el femenino. El equipo técnico lo formaban Arshak Petrosian y Tigran Nalbandian. El múltiple campeón del mundo ruso Garry Kasparov tiene raíces armenias por vía materna.










</doc>
<doc id="22041" url="https://es.wikipedia.org/wiki?curid=22041" title="Viajeros y magos">
Viajeros y magos

Viajeros y magos es una película filmada en Bután y hablada en idioma dzongkha. Esta película muestra los paisajes naturales de este país ubicado entre India y Tíbet. También muestra la apacible y sencilla vida de sus ciudadanos.

La trama cuenta la historia de Tshewang Dendup, un burócrata del gobierno que, descontento por su trabajo en un distrito remoto desea emigrar a los Estados Unidos de donde ha escuchado que se gana más recogiendo frutas en un día que trabajando en su país durante un mes. Un amigo suyo que vive en Nueva York ha prometido ayudarlo, pero debe llegar en una fecha muy cercana. Al tratar de llegar a Thimphu, la capital, pierde el único transporte público de la semana. Esperando que algún otro vehículo pase escucha las historias de un monje sobre magia y viajes.


</doc>
<doc id="22054" url="https://es.wikipedia.org/wiki?curid=22054" title="Autopista">
Autopista

Una autopista es una pista de circulación para automóviles y vehículos terrestres de carga (categóricamente los vehículos de motor) y de pasajeros. 

Debe ser rápida, segura, y admitir un volumen de tráfico considerable, y se diferencia de una carretera convencional, en que la autopista dispone de más de un carril para cada sentido con calzadas separadas (no confundir con la autovía española diseñada en ese país en 1988). 

Las primeras autopistas construidas con esta configuración se hicieron en Italia durante los años 1920. 

Entre los tipos de carreteras, representan las vías que más tráfico rodado pueden soportar y en las que este alcanza mayores velocidades, por lo que suelen representar los ejes principales de la red viaria de un país desarrollado (exceptuando a España, que implantó, por norma general, la autovía en esos casos).

La Organización para la Cooperación y el Desarrollo Económicos define autopista como:

Para poder ser calificada como autopista, una vía de circulación debe reunir las siguientes características:


La primera autopista del mundo se construyó en Italia, en 1921, entre Milán y Varese. Hoy forma parte de las autopistas A-8 y A-9 italianas. Tenía dos calzadas separadas, pero aún no contaba con cruces a distinto nivel.

La primera autopista española fue la autopista A-3 entre Madrid y Vallecas, seguida por la autopista A-6, que construyó entre 1964 y 1967 (inaugurada este último año), entre los municipios madrileños de Las Rozas de Madrid y Collado Villalba.
La primera de peaje se construyó en 1969 y se extendía entre Barcelona y Mataró.
Le siguieron las autopistas AP-8 (Bilbao - Behovia), la AP-6 (Collado Villalba - Adanero) y la "Y" asturiana A-8 / A-66 (Gijón - Oviedo - Avilés). En 1972 se finalizó la AP-4, la autopista del Sur de Sevilla a Cádiz. Finalmente, los dos accesos principales a las terminales 1, 2 y 3 del aeropuerto de Madrid-Barajas, se acabaron convirtiendo en autopistas. 

Una de las últimas fue el tramo comprendido entre Benavente y Padornelo (ambos en la provincia de Zamora) de la A-52 denominada en su totalidad como autovía de las Rías Bajas, y en Zamora, y "autovía das Rías Baixas" en las provincias de Orense y Pontevedra. Las autopistas M-30, M-40 y M-50 en la comunidad de Madrid. La autopista autonómica M-45, en la misma comunidad. Parcialmente, la autovía del Cantábrico, A-8, se torna físicamente autopista en distintos puntos, caso por ejemplo de la variante de Gijón. El tramo Gijón-Avilés de la A-8 entero, junto con el tronco a Oviedo de la A-66, la autovía Ruta de la Plata, construidos a la vez, y conocidos popularmente como "autopista Y", ya mencionada. 

Además, el conjunto de los tramos (Oviedo-Mieres), (Mieres-Pola de Lena) y (Pola de Lena-Campomanes) de la A-66, que es una autopista. También la autopista Sevilla-Huelva (la A-490, la A-22, la variante norte de Zaragoza, la A-52 (Benavente-Padornelo) y otras,aunque generalmente, se encuentran cerca de (o conectadas a) autopistas de peaje, no siendo el caso de esta última, por ejemplo.

Las autovías españolas históricamente siguen más o menos el mismo recorrido que tenían las calzadas romanas en los tiempos de Hispania. La existencia de un tipo de vía cuya definición fue acuñada en España, y la evolución de éstas con el tiempo, frecuentemente llevan a confusión, por lo que se enumeran algunas, especialmente:


Por poner algunos de los ejemplos que suelen generar confusión. De hecho, por ejemplo se puede mencionar, que Extremadura es una comunidad autónoma, que no posee ni un solo kilómetro de autopistas, ni siquiera de peaje.

Una autovía es un tipo de carretera interurbana española, desdoblada y de alta velocidad, similar a la autopista, aunque de un nivel inferior. Las directrices obligatorias para su construcción y conservación, están regidas por su definición en la Ley de Carreteras española de 1988. En cambio en el ámbito de las autopistas, esto se vuelve más complejo, ya que hasta la definición que se le pueda dar en esa misma ley, puede llegar a venir fuertemente condicionada por los estándares internacionales del momento, especialmente si la autopista en cuestión forma parte de un itinerario europeo. De hecho, frecuentemente se estima que es por esto, unido a otra fuerte crisis a comienzos de los ochenta lo que pudo llevar a ese "re-diseño" de la autovía. De los tres apartados (letras) de la definición de la OCDE, situada más arriba, la autovía, como tal y por definición, está exenta de obligaciones en el cumplimiento del apartado (c), lo que no implica que en algunos casos, cumpla algunas de manera opcional, lo que explicaría aquellas auto-imposiciones en las autovías de última generación. Además aunque se llegue al punto de casi hacerlo, el trazado y sus características, no tiene por qué conllevar las mismas "prescripciones" que el de una autopista. Sigue estando permitido construirlas sobre las carreteras antiguas, aprovechándolas para una de las plataformas, simplemente reasfaltando encima, aunque en la actualidad es solo "un recurso ante la necesidad". Y pintando posteriormente la señalización horizontal, que sí se mantiene, si no siempre igual, similar a la de la autopista. En una autovía y en una autopista de peaje (siempre a criterio de la concesionaria) está permitido corregir baches, badenes, grietas y demás defectos, mediante la colocación de parches de asfalto o mediante el uso de pastas de alquitrán, en cambio en las autopistas esto puede ser una conducta ciertamente "rastrera" y provocar, de facto, que la autopista se vuelva peligrosa.

En el transporte, la demanda puede medirse en el número de viajes efectuados o en la distancia total recorrida en todos los trayectos (por ejemplo, pasajeros-kilómetros para transporte público o vehículos-kilómetros de viaje para transporte privado). Se considera que la oferta es una medida de capacidad. El precio del bien (viaje) se mide utilizando el costo generalizado de los viajes, que incluye tanto dinero como tiempo.

El efecto de los aumentos en la oferta (capacidad) es de particular interés en la economía del transporte (ver demanda inducida), ya que las consecuencias ambientales potenciales son significativas (ver externalidades a continuación).

Además de proporcionar beneficios a sus usuarios, las redes de transporte imponen externalidades positivas y negativas a los no usuarios. La consideración de estas externalidades -en particular las negativas- es parte de la economía del transporte. Las externalidades positivas de las redes de transporte pueden incluir la capacidad de prestar servicios de emergencia, el aumento del valor de la tierra y los beneficios de la aglomeración. Las externalidades negativas son amplias y pueden incluir la contaminación del aire local, la contaminación acústica, la contaminación lumínica, los peligros de seguridad, la separación de la comunidad y la congestión. La contribución de los sistemas de transporte al cambio climático potencialmente peligroso es una externalidad negativa significativa que es difícil de evaluar cuantitativamente, lo que hace difícil (pero no imposible) incluir en la investigación y el análisis basados en la economía del transporte. La congestión es considerada una externalidad negativa por los economistas.

Las carreteras son fuentes prolongadas de contaminación lineal.

El ruido de la carretera aumenta con la velocidad de operación, de modo que las carreteras principales generan más ruido que las calles arteriales. Por lo tanto, se esperan efectos considerables sobre la salud del ruido de los sistemas de autopistas. Existen estrategias de mitigación del ruido para reducir los niveles de sonido en los receptores sensibles cercanos. La idea de que el diseño de la carretera podría ser influenciado por consideraciones de ingeniería acústica surgió por primera vez alrededor de 1973. 

Problemas de calidad del aire: Las carreteras pueden contribuir con menos emisiones que las arterias que transportan el mismo volumen de vehículos. Esto se debe a que la operación de alta velocidad constante crea una reducción de emisiones en comparación con los flujos vehiculares con paradas y arranques. Sin embargo, las concentraciones de contaminantes atmosféricos cerca de las carreteras pueden ser mayores debido al aumento de los volúmenes de tráfico. Por lo tanto, el riesgo de exposición a niveles elevados de contaminantes atmosféricos de una carretera puede ser considerable, y se amplifica aún más cuando las carreteras tienen congestión de tráfico.

Las nuevas carreteras también pueden causar la fragmentación del hábitat, fomentar la expansión urbana y permitir la intrusión humana en áreas previamente intactas, así como (contrarrestando) aumentar la congestión, aumentando el número de intersecciones.

También pueden reducir el uso del transporte público, lo que conduce indirectamente a una mayor contaminación.

Los carriles de vehículos de alta ocupación se están agregando a algunas carreteras nuevas / reconstruidas en Norteamérica y otros países alrededor del mundo para fomentar el carpool y el transporte masivo. Estos carriles ayudan a reducir el número de coches en la autopista y por lo tanto reduce la contaminación y la congestión del tráfico promoviendo el uso del carpooling para poder utilizar estos carriles. Sin embargo, tienden a requerir carriles dedicados en una carretera, lo que los hace difíciles de construir en zonas urbanas densas donde son los más eficaces.

Para abordar la fragmentación del hábitat, los cruces de vida silvestre se han vuelto cada vez más populares en muchos países. Los cruces de vida silvestre permiten a los animales cruzar con seguridad las barreras humanas como las carreteras. 



</doc>
<doc id="22063" url="https://es.wikipedia.org/wiki?curid=22063" title="Región de La Araucanía">
Región de La Araucanía

La Región de La Araucanía es una de las dieciséis regiones en que se divide Chile. Su capital es la ciudad de Temuco. Es la puerta de entrada a la zona Sur del país. Limita al noroeste y norte con la Región del Biobío, al este con la provincia del Neuquén en Argentina, al sur con la Región de Los Ríos y al oeste con el océano Pacífico. Cuenta con una superficie de 31 858 km² y una población de 1 046 322 habs. según la proyección del INE del año 2014, siendo la novena en Chile y la , por detrás de las regiones Metropolitana de Santiago, de Valparaíso, del Biobío y del Maule.

La región está compuesta por las provincias de Cautín y Malleco. Las ciudades más importantes de la región son Temuco, Angol, Villarrica, Victoria, Lautaro y Pucón.

Su principal centro urbano es el Gran Temuco con 358 541 habitantes, seguido de Villarrica con 55 478 habitantes y Angol con 48.608 habitantes.

La Araucanía hace referencia al «lugar que habitan los araucanos», nombre con el que los españoles designaban a los mapuche. La voz «araucano» es una hispanización del término usado por los incas para referirse a los mapuches "(awqa", "promaucaes").

La bandera de la Región de La Araucanía nunca ha sido declarada oficial. Consiste en un estandarte de tres franjas azul (superior), blanca (medio) y roja (inferior), con un escudo diseñado a base de dos cuarteles rojo y negro, adornados con seis "guemiles" blancos y un "trapelacucha" (joya mapuche) del mismo color, rodeados de una guirnalda de copihue y coronados por un monte nevado escoltado de araucarias, aunque en el estandarte de la intendencia de la región se ve una bandera blanca con el mismo escudo y el "trapelacucha".

En el último tiempo se ha popularizado el uso del "Wenufoye" o Bandera Mapuche en la región, tanto así que es utilizado de forma oficial en decenas de municipios de la región e incluso en la intendencia y gobernaciones.

El comienzo de la historia del poblamiento humano en esta región se remonta a alrededor de 11 000 años antes de la era cristiana en época glacial final (Periodo paleoindio).

Ella estuvo relacionada al término de la glaciación wisconsiense. Los primeros pobladores de estos lugares australes estaban organizados en grupos familiares (bandas) de cazadores y recolectores (as) que practicaban la movilidad residencial y que fueron exitosos en la colonización de los distintos ecosistemas americanos, llegando al actual sur de Chile hace al menos 13000 años donde comenzaron a aprender a habitar el bosque siempreverde del valle y practicaron excursiones esporádicas a la costa del Pacífico.

Durante este período se produjeron cambios profundos en las sociedades que poblaron esta zona centro sur, las que poseían mayor experiencia en organización social y en tecnologías especializadas para la recolección y la caza, producto de un conocimiento más íntimo y la expresión de conductas seguramente flexibles para su establecimiento en distintos ecosistemas producto del cambio ambiental global que se había producido en los milenios anteriores. 
Hace 4800 años empezaron a llegar pueblos recolectores dejando varios sitios arqueológicos en la zona, en Quillén, Quino, Isla Mocha, y Península de Pucón. Estos grupos traían técnicas acabadas de caza, sin embargo las técnicas de recolección las adoptaron de grupos previos, del paleoindio. No se han encontrado restos arqueológicos de Paleoindio pero se sabe que estaban ya que hay restos más al sur, en Monte Verde (11 000 a. C.). Bajo estos restos se encontraron otros de al menos del periodo 31 000 a. C.
Los sitios arqueológicos en la región corresponden a los llamados aleros que son lugares altos en donde se dominan los sitios de acceso y las fuentes de agua, y permitían esbozar tácticas de caza y defensa. En ellos, se encontraban puntas de flecha y otros instrumentos líticos. Los aleros están generalmente en rocas o cuevas basálticas, las que además proporcionaban la materia prima para hacer sus herramientas líticas.
Dentro de los artículos líticos se encontraban cuchillos, raspadores y puntas de proyectil. Los aleros dominaban zonas de bosque en los cuales los habitantes de la zona recolectaban frutos y plantas y cazaban.
En este periodo ya había reconocimiento y conocimiento de territorio. Esto se infiere porque usaban distintas puntas de proyectil en el valle o en la costa.

Durante el verano, se movían a sectores altos de la cordillera de los Andes para aprovisionarse del piñón, el fruto del pehuén, recolectar obsidiana y riolita desde sectores cercanos a volcanes para hacer sus instrumentos, y cazar a la fauna que subía en busca de dehesas verdes. Esta actividad, la veranada, se efectúa en la zona hasta la actualidad.

El periodo arcaico duró desde el 8000 a. C. hasta el 700 a. C., aproximadamente.

El período agroalfarero temprano es un período arqueológico usado para la clasificación de culturas arqueológicas de los pueblos originarios del norte argentino y norte y centro de Chile. Se desarrolla entre el 500 a. C. y el año 650 de nuestra era.

La cultura Pitrén es un complejo cultural agroalfarero temprano de Chile. Las comunidades comprendidas bajo esta denominación, se ubicaron entre el río Bío Bío y el lago Llanquihue, ubicado en la Región de los Lagos. 
Ya hacia el año 600, estos grupos iniciaron el cultivo del maíz y de la papa, mediante el despeje de espacios entre los bosques, sin abandonar sus prácticas de caza y recolección.
La cerámica, elemento particular que ha caracterizado a estas comunidades, es la más antigua de la zona. En ella se encuentra el "ketrumetawe", jarro con forma de ave, símbolo de la mujer casada; además de otra diversidad de jarros de estructuras globulares, con cuello cilíndrico y recto, con asa en el cuello o con asa recta que termina en una figura de animal, tales como patos, ranas o sapos en el extremo. La cerámica de Pitrén denota en un evidente contacto con las culturas de El Molle y Llolleo. Decoraban las piezas con un procedimiento denominado "pintura negativa".
Los estudiosos parecen coincidir en que Pitrén constituye la primera expresión agroalfarera en el sur de Chile.

Es muy probable que el complejo Tirúa haya tenido sus raíces en el complejo diaguita de más al norte, o en el estilo Aconcagua del valle central, los tipos alfareros de El Vergel, Tirúa y Valdivia están vinculados entre sí, como también con el complejo Aconcagua, compartiendo tradiciones comunes, dado que muchos elementos simbólicos propios de su gráfica, como también la forma de los objetos en los cuales se instala dicha gráfica, tienden a ser los mismos, como se puede apreciar al comparar los objetos que se conservan en museos y colecciones privadas.

El mayor problema lo constituye el llamado tipo Tirúa, debido a que, al describirlo, Latcham no entregó fotografías de las piezas sino solo algunos dibujos no muy bien logrados, reproducidos en su libro "La alfarería indígena chilena" y cuya descripción en dicho texto parece insuficiente. Además, las piezas no son conocidas en las colecciones y museos chilenos, como tampoco en colecciones argentinas donde pudiesen haber sido llevadas, por lo que el problema subsiste.

La cultura mapuche surge de estas culturas anteriores, representada entonces en sus antepasados Pitrén y El Vergel. Al paso del tiempo, en cientos de años se fueron expandiendo esos rasgos culturales y homogeneizándose, hasta llegar al año mil de nuestra era a constituir lo que ya puede ser reconocido plenamente como cultura mapuche.

Tras un breve contacto con el Imperio inca que jamás llegó a la zona, sus habitantes fueron llamados promaucaes (hanan runasimi "purum auca", 'gente salvaje'). La influencia posterior de los incas tampoco fue pequeña y adoptaron numerosos productos del “enemigo” que no logró ingresar a su territorio.

En 1536, tras los incas, llegaron los españoles llamados "huincas" por los mapuche (mapudungún "we inka", 'nuevo inca').
El contacto definitivo con los españoles se produjo en la batalla de Reinohuelen en 1536. Este pueblo fue conocido por los conquistadores españoles con el nombre genérico de «araucano», usado por primera vez por Alonso de Ercilla en 1589. Arauco, es derivado de la palabra "ragko" (mapudungún "ragko", 'agua gredosa blanca'), sinónimo de Malleco, Malloco o Mallarauco. 

Entre ellos se llamaban por gentilicios que aludían a las diferentes localidades de origen (p. ej. purenes), o a puntos cardinales de los que procedían, respecto de los referentes (picunches ("pikun", 'Norte'), huilliches ("willi", 'Sur')).

En estas tierras habitaban más de 180 mil indígenas, compuestos por los pueblos pehuenches y mapuches. Dicho territorio se había mantenido rebelde a partir de la denominada Guerra de Arauco ante el dominio español durante la Conquista de Chile y todo el período colonial de Chile, sin que ningún bando venciera claramente.

Luego de la independencia de Chile, ya en el período republicano, se ordenó la celebración de un parlamento general con los mapuches que habitaban al sur del río Biobío, con la finalidad de acordar el estatuto que regularía las relaciones entre la naciente república y el pueblo mapuche; realizándose así el Parlamento de Tapihue en enero de 1825. Sin embargo posteriormente sucedieron diversos hechos que obligaron al estado chileno a destinar recursos a la zona de la frontera.

Además, durante la Revolución de 1851, el general José María de la Cruz, líder del movimiento golpista, reclutó a varios loncos mapuches y sus clanes para alzarse en armas contra el gobierno, esto lo pudo lograr gracias a la relación de amistad que mantenía el general con los caciques, entre ellos Colipí. Cuando su insurrección fue aplastada por el general Manuel Bulnes, los caciques en vez de rendirse junto a De la Cruz se replegaron a la frontera junto con varios miembros descolgados del antiguo ejército, dedicándose al pillaje y al robo de ganado, por los siguientes 4 años. Esto motivó al gobierno a movilizar al segundo batallón del segundo de línea, hasta enero de 1856.

Un aventurero francés, llamado Orélie Antoine de Tounens, se dirigió a la zona de la Araucanía, hizo contacto con el lonco Quilapán, al que entusiasmó con su idea de fundar un estado para el pueblo mapuche como forma de resistencia al ejército chileno durante la época final de la Guerra de Arauco y el 17 de noviembre de 1860 fundó el "Reino de la Araucanía" y se proclamó rey bajo el título de Orélie Antoine I. Durante los días siguientes, Tounens promulgó la constitución del reino y el 20 de noviembre del mismo año declaró la anexión de la "Patagonia", estableciendo como límites el río Biobío por el norte, el océano Pacífico por el oeste, el océano Atlántico por el este desde el río Negro hasta el estrecho de Magallanes, límite austral del Reino. El gobierno chileno, bajo el mando del presidente José Joaquín Pérez, ordenó el arresto de Tounens en enero de 1862, siendo trasladado a Los Ángeles donde fue recluido y luego enviado a Europa.

En 1879 se da el último gran levantamiento mapuche en la actual Región de la Araucanía debido a la migración de fuerzas chilenas al norte por la Guerra del Pacífico. El levantamiento fue sofocado por las tropas a cargo de Cornelio Saavedra Rodríguez en 1881, dando fin al proceso que en la historia tradicional chilena se conoce como «Pacificación de la Araucanía».

Esta etapa contempló la ocupación total de la Araucanía y su consolidación. De ese modo, el gobierno chileno llevó finalmente a cabo uno de sus principales proyectos de Estado, anhelado incluso desde la época de los españoles, quienes en el período colonial no lograron instalarse ni explorar la Araucanía.

Los vencidos fueron reubicados en «reducciones», es decir, terrenos comunitarios de extensión reducida para que practicaran sus actividades ganaderas, donde permanecen hasta hoy. En los territorios ocupados se les entregaron tierras a colonos chilenos y europeos, principalmente españoles, alemanes, franceses, italianos, británicos, suizos y del resto de Europa; en total, entre 1882 y 1901, llegaron 36 000 europeos, 24 000 contratados por la agencia de colonización y 12 000 llegaron por sus propios medios. A estos 36.000 europeos se deben sumar a los 100.000 chilenos de origen hispánico, que llegaron en esta primera etapa, fenómeno de inmigración interna, que sumada a la europea, explicaría la actual composición étnica de la región, donde la etnia indígena mapuche, es minoritaria en términos absolutos.

La región es muy rica en recursos naturales, servicios, explotación forestal, ganadera, agrícola y con un gran auge en el turismo internacional, debido a sus bellezas naturales (volcanes, bosques milenarios, centros de esquí, lagos, ríos, pesca, centros termales, montañas, etc). A pesar de las riquezas es una de las regiones más pobres del país con altos índices de pobreza y desigualdad socio-económica.

En la región se viven constantemente conflictos entre mapuches y empresas forestales y agrícolas por causa de los eventos históricos donde muchas tierras mapuches fueron expropiadas ilegalmente y posteriormente vendidas a privados y empresas. 
Hoy hay muchos programas en marcha en la Región de la Corporación Nacional de Desarrollo Indígena para tratar de resolver muchos de los conflictos de tierra e incentivar a comunidades mapuches para mejorar las condiciones económicas. A pesar de los enormes recursos destinados a compra de tierras, maquinarias y equipos no ha significado una mejoría en las condiciones de vida de los beneficiados pero si un enorme perjuicio a la economía regional.

La Región de La Araucanía, para efectos del gobierno y administración interior, se divide en 2 provincias:


Para los efectos de la administración local, las provincias están divididas en 32 comunas, 21 en Cautín y 11 en Malleco.

La Región de La Araucanía, a efectos electorales, corresponde a la circunscripción senatorial XI y agrupa a los distritos 22 y 23.

El gobierno regional reside en el Intendente, designado por el Presidente de la República. Actualmente, es el intendente Víctor Manoli Nazal.

El gobierno y administración de las provincias corresponde a dos gobernadores, nombrados por el Presidente de la República. Los Gobernadores son: 


La administración de la región radica en el Gobierno Regional, constituido por el Intendente y por el Consejo Regional, compuesto de consejeros regionales electos directamente por la población.

La administración local de cada comuna reside en la respectiva municipalidad.

La región se halla dividida en los Servicios de Salud Araucanía Norte y Araucanía Sur, el primero administra a los 7 hospitales de la Provincia de Malleco y el segundo a los 17 centros hospitalarios de Cautín. 

Los hospitales más importantes de la región son el Regional Hernán Henríquez Aravena de Temuco, el Mauricio Heyermann de Angol, el San José de Victoria y el Intercultural de Nueva Imperial.

Diversos centros universitarios, institutos profesionales y centros de formación técnica se encuentran en distintas ciudades de La Araucanía. Tienen presencia en la región: 



Se encuentra ubicada entre las regiones del Biobío y de Los Ríos y entre Argentina y el océano Pacífico. Su relieve se caracteriza por la presencia, de oeste a este, de planicies costeras, la cordillera de la Costa, la depresión intermedia, la precordillera y la cordillera de los Andes.

El clima de la región se caracteriza por la transición, de norte a sur, entre los climas de tipo mediterráneo y oceánico lluvioso. Siendo posible observar los siguientes tipos de clima:

La configuración hidrográfica de la región se caracteriza por la presencia de tres grandes ríos que corren de este a oeste: el Imperial, el Toltén y el Biobío, el cual se extiende en dirección nor-oeste, desembocando en la Región del Biobío. Los principales afluentes del río Imperial son el Cautín, el Chol Chol y el Quepe, y los del río Toltén son el Allipen y el lago Villarrica. Además presenta algunas cuencas costeras de menor magnitud, como los ríos Moncul y Queule. La región cuenta con una serie de lagos, entre los que destacan el Villarrica, el Caburga, el Budi y el Collico.

Dos de los volcanes más activos del país y de Sudamérica se encuentran en esta región: el Llaima y el Villarrica.

La principal actividad económica de la región es la agricultura destacando los cultivos de plantas como avena, cebada, y centeno además de lupino y la papa. Estos cultivos, con excepción de la papa, representan las mayores superficies cultivadas del país. Cabe destacar el incremento de producción de avellanas y bayas ("berries"), por ejemplo arándanos, de exportación, estos cultivados principalmente en la zona de Gorbea. Además, es destacable la producción ganadera, especialmente en el rubro bovino, el cual la convierte en la segunda región de mayor producción en Chile ascendiendo a más de 700 000 cabezas de ganado anuales. En los últimos años, ha experimentado un considerable crecimiento la actividad forestal, de pinos y eucaliptos, principalmente en la provincia de Malleco.

Además, la región posee un gran potencial turístico debido a la belleza de su paisaje —conformado por bosques, lagos, ríos, volcanes y montañas—, y cuenta con una amplia oferta de servicios de hoteles y complejos turísticos.

En 2018, la cantidad de empresas registradas en la región de la Araucanía fue de 20.410. El Índice de Complejidad Económica (ECI) en el mismo año fue de 0,21, mientras que las actividades económicas con mayor índice de Ventaja Comparativa Revelada (RCA) fueron Fabricación de Carbón Vegetal y Briquetas (43,86), Establecimientos de Enseñanza Primaria y Secundaria para Adultos (30,57) y Venta al por Mayor de Combustibles Sólidos (13,71).

La ciudad de Temuco es, junto con Iquique, una de las ciudades de crecimiento más explosivo a nivel nacional. Según el censo del año 1970, en Temuco vivían cerca de 88 000 habitantes; esta población, en 30 años se cuasi triplicó hasta bordear los 250 000 habitantes. En la actualidad, la ciudad de Temuco y la comuna de Padre Las Casas forman la llamada área metropolitana de Temuco, que según las estimaciones del Ministerio de Vivienda y Urbanismo de Chile, contaba con 460 824 habitantes para el año 2016, siendo el sexto centro urbano más grande de Chile.

La turística ciudad lacustre de Villarrica también ha vivido este fenómeno demográfico al transformarse, junto al balneario de Pucón, en uno de los 4 destinos turísticos de Chile debido a sus grandes páramos naturales rodeados por bosques cordilleranos, lagos de aguas claras e inmensos volcanes andinos. 

La inmigración nacional actual es proveniente de la zona central de Chile y un 23,46 % de la población de la Región de La Araucanía afirmó aún pertenecer a una etnia originaria/indígena, principalmente mapuche; sin embargo las personas de origen indígena que se han asimilado sería mucho mayor.

La región tiene una población diversa donde la sociedad originaria mapuche es minoritaria alcanzando alrededor del 45% del total de la población regional (30,6% de la comunidad mapuche) según el censo de 2012 siendo la población mayoritaria la correspondiente a las de origen español o criollo chileno, seguido de descendientes de inmigrantes suizos, franceses, alemanes e italianos llegados a la región, a partir de 1883 a partir de una política de colonización del Estado de Incorporación de la Araucanía. Desde el momento en que este territorio es incorporado comenzó un proceso de mestizaje entre los distintos pueblos que lo habitan, tanto entre los chilenos de la zona, y los que llegaron de otras zonas del país, como los migrantes extranjeros. En la actualidad por tanto la gran mayoría de la población autodenominada como mapuches sin mestizaje en realidad sería mestiza. Existen comunidades diferenciadas en la región producto del conflicto mapuche. Por otro lado, descendientes de los extranjeros se han asociado para defenderse de ataques incendiarios por parte de grupos paramilitares ilegales. Esta situación genera conflictos diversos, en donde no se ha llegado a una solución final. La Araucanía es también una región con una gran cantidad de población bilingüe en Chile, donde hay cerca de 75.000 hablantes de mapudungún, los que además manejan el castellano. El castellano es la lengua oficial de la región, sin embargo la lengua mapuche se encuentra oficializada en algunas comunas de la región y en proyección para ser lengua co-oficial en la región.

Los habitantes de etnia mapuche de esta región representan el 19% del total de la población mapuche a nivel nacional, muchos son mestizos, pero predominando la ascendencia étnica mapuche, siendo la segunda región con más miembros netos de este pueblo (tras la Región Metropolitana, que reúne al 37% del total de la población mapuche o mapuche-mestiza a nivel nacional).

La lengua más hablada es el español y es el idioma oficial "de facto", como en todo Chile; además, parte de la población mapuche habla mapudungún y existen colectividades de descendientes de inmigrantes que utilizan idiomas como el alemán o el italiano en forma minoritaria. El mapudungun es lengua oficial en las comunas de Galvarino y Padre Las Casas y el año 2015 el Consejo Regional aprobó su cooficialidad en toda la región, pero no se ha implementado el modo en que se hará efectiva esta medida.

En la región coexisten variadas manifestaciones culturales, como consecuencia de los diversos grupos relacionales que poblaron la región, y que se manifiestan en tradiciones, la religión, la arquitectura, gastronomía, proyectos educacionales, en la que se destaca un fuerte componente cultural europeo, sea de origen criollo (hispánico) o de las derivadas de la inmigración europea, y la mapuche como por ejemplo la celebración del "We Tripantu" (Año Nuevo Mapuche) celebrada durante el solsticio de invierno, actividad en que participan más de 30 comunidades indígenas del sector. Esta fiesta da inicio a un nuevo año para los mapuches, que es celebrado con comidas típicas y rogativas que tienen como propósito pedir que la próxima temporada sea abundante en alimentos y cosechas, y que la naturaleza esté protegida.

En la película "La Araucana" (1971), una adaptación libre del poema homónimo de Alonso de Ercilla y Zúñiga, Araucanía se interpretada por la actriz chilena Rebeca Ghigliotto.




</doc>
<doc id="22068" url="https://es.wikipedia.org/wiki?curid=22068" title="Morse">
Morse

Morse hace referencia a:


</doc>
<doc id="22070" url="https://es.wikipedia.org/wiki?curid=22070" title="ReiserFS">
ReiserFS

ReiserFS es un sistema de archivos de propósito general, diseñado e implementado por un equipo de la empresa , liderado por Hans Reiser. 

Actualmente es soportado por Linux y existen planes de futuro para incluirlo en otros sistemas operativos. También es soportado por Windows (de forma no oficial), aunque por el momento de manera inestable y rudimentaria (ReiserFS bajo windows).

A partir de la versión 2.4.1 de Linux, ReiserFS se convirtió en el primer sistema de ficheros con "journal" en ser incluido en el núcleo estándar.
También es el sistema de archivos predefinido en varias distribuciones, como SuSE (excepto en openSuSE 10.2 cuyo formato predeterminado es ext3), Xandros, Yoper, Linspire, Kurumin Linux, FTOSX, Libranet y Knoppix.

Con la excepción de actualizaciones de seguridad y parches críticos, ha cesado el desarrollo de ReiserFS (también llamado reiser3) para centrarse en Reiser4, el sucesor de este sistema de archivos.

ReiserFS ofrece funcionalidades que pocas veces se han visto en otros 
sistemas de archivos:


Comparado con ext2 y ext3 en el uso de archivos menores de 4k, ReiserFS es normalmente más rápido en un factor de 10–15. Esto proporciona una elevada ganancia en las news, como por ejemplo Usenet, cachés para servicios HTTP, agentes de correo y otras aplicaciones en las que el tiempo de acceso a ficheros pequeños debe ser lo más rápida posible.


ReiserFS almacena metadatos sobre los ficheros, entradas de directorio y listas de inodos en un único árbol B+ cuya clave principal es un identificador único. Los bloques de disco asignados a los nodos del árbol son los "bloques internos formateados" y los bloques de las hojas son los "bloques de hojas formateados". Todos los bloques restantes son los "bloques sin formatear", que contienen los datos de los ficheros. Los directorios con muchas entradas, ya sean directas o indirectas, que no caben en un solo nodo, se reparten con el nodo vecino de la derecha. La asignación de bloques se lleva a cabo mediante un bitmap de espacio libre almacenado en localizaciones fijas.

En contraste, ext2 y otros sistemas de ficheros, usan una fórmula fija para calcular localizaciones de inodos, por lo que limitan el número de archivos que pueden almacenar. Otros también almacenan los directorios como una simple lista de entradas, lo que provoca que las búsquedas y modificaciones sean operaciones lineales temporalmente y degradan el rendimiento de directorios con muchos archivos. El árbol B+ en ReiserFS evita estos problemas.

Existen principalmente dos versiones de este sistema de ficheros: la 3 y la 4. Las características son las siguientes:




</doc>
<doc id="22071" url="https://es.wikipedia.org/wiki?curid=22071" title="Ext3">
Ext3

ext3 ("third extended filesystem" o "tercer sistema de archivos extendido") es un sistema de archivos con "registro por diario" (journaling). Fue el sistema de archivos más usado en distribuciones Linux, aunque en la actualidad ha sido remplazado por su sucesor, ext4.

La principal diferencia con ext2 es el "registro por diario". Un sistema de archivos ext3 puede ser montado y usado como un sistema de archivos ext2. Otra diferencia importante es que ext3 utiliza una variante de árbol b, el árbol HTree e incorpora el asignador de bloques de disco Orlov.

Aunque su velocidad y escalabilidad es menor que sus competidores, como JFS, ReiserFS o XFS, tiene la ventaja de permitir actualizar de ext2 a ext3 sin perder los datos almacenados ni tener que formatear el disco. Tiene un menor consumo de CPU y está considerado más seguro que otros sistemas de ficheros en Linux dada su relativa sencillez y su mayor tiempo en uso.

El sistema de archivo ext3 agrega a ext2 lo siguiente:

Ext3 tiene dos límites de tamaño distintos. Uno para archivos y otro para el tamaño del sistema de archivos entero. El límite del tamaño del sistema de archivos es de 2 bloques

Hay tres niveles posibles de "journaling" (registro por diario)



Como ext3 está hecho para ser compatible con ext2, la mayoría de las estructuras del archivación son similares a las del ext2. Por ello, ext3 carece de muchas características de los diseños más recientes como las extensiones, la localización dinámica de los inodos, y la sublocalización de los bloques. Hay un límite de 31998 subdirectorios por cada directorio, que se derivan de su límite de 32000 links por inodo.
Ext3, como la mayoría de los sistemas de archivos actuales de Linux, no puede ser chequeado por el fsck mientras el sistema de archivos está montado para la escritura. Si se intenta chequear un sistema de ficheros que está montado puede detectar falsos errores donde los datos no han sido volcados al disco todavía, y corromper el sistema de archivos al intentar arreglar esos errores.

No hay herramienta de desfragmentación en línea para ext3 que funcione en nivel del sistema de archivos. Existe un desfragmentador offline para ext2, codice_1, pero requiere que el sistema de archivos ext3 sea reconvertido a ext2 antes de iniciarse. Además, dependiendo de los bits encendidos en el sistema, codice_1 puede destruir datos. No sabe como tratar la mayoría de las nuevas características de ext3.
Hay herramientas de usuario para desfragmentar como Shake y Defrag. Shake trabaja localizando para todo el archivo como una operación, lo que generalmente causa que el localizador encuentre espacio continuo en el disco. También intenta escribir archivos usados al mismo tiempo que otros.
Defrag trabaja copiando cada archivo sobre sí mismo. De todas formas solo funcionan si el sistema de archivos esta razonablemente vacío. No existe una verdadera herramienta de desfragmentación para ext3.
Como se viene diciendo, la guía de administración de Linux dice: "Los modernos sistemas de archivos de Linux mantienen la fragmentación al mínimo manteniendo los bloques de un archivo juntos, aunque no puedan ser guardados en sectores consecutivos. Algunos sistemas de archivos, como ext3, localizan efectivamente los bloques libres más cercanos a otros en el archivo. Por ello no es necesario preocuparse por la fragmentación en un sistema de Linux"
Mientras ext3 es más resistente a la fragmentación que Fat, nada evita que los sistemas ext3 se puedan fragmentar con el tiempo. Consecuentemente el sucesor de ext3, ext4, incluye una utilidad de desfragmentación y soporte para extensiones (regiones contiguas del fichero).

El soporte para la compresión está disponible como un parche no oficial para ext3. Este parche es un porte directo de codice_3 pero necesita un mayor desarrollo ya que todavía no implementa el journaling. El actual parche es llamado e3compr y puede ser bajado aquí: 

Ext3 no hace la suma de verificación cuando está escribiendo en el diario. Si barrier = 1 no está habilitado como una opción de montaje, y si el hardware está escribiendo fuera de orden, se corre el riesgo de una corrupción muy amplia del sistema de archivos en caso de que haya un fallo repentino del hardware.

Aunque Windows no tiene un soporte nativo para ext2 ni ext3, pueden instalarse drivers para poder acceder a ese tipo de sistemas de archivos. Se puede instalar en todos los sistemas de windows con arquitectura x86.

Este driver hace que se puedan montar las particiones sin tener que usar programas aparte. Nos muestra el sistema de archivos como si fuese una partición más dentro de windows.

Para bajarse el driver: .

Otra opción es usar un programa para poder ver y copiar los archivos que hay en una partición con ext3 y ext2 pero no monta la partición.
El programa es Explore2fs y nos permite:

Está disponible para las versiones de windows: 

Página principal del programa

Existe una versión más reciente de este sistema de archivos llamada Ext4 que implementa un gran cantidad de nuevas características

Para hacerse una mejor idea de las diferencias con el sistema de archivos ext4 mirar la siguiente tabla




</doc>
<doc id="22072" url="https://es.wikipedia.org/wiki?curid=22072" title="Ext2">
Ext2

ext2 ("second extended filesystem" o "segundo sistema de archivos extendido") es un sistema de archivos para el núcleo Linux. Fue diseñado originalmente por Rémy Card. La principal desventaja de ext2 es que no implementa el registro por diario (en inglés "Journaling") que sí poseen sus posteriores versiones ext3 y ext4.

ext2 fue el sistema de ficheros por defecto de las distribuciones de Linux Red Hat Linux, Fedora Core y Debian. Los lanzamientos de las nuevas versiones estables, ext3 y ext4, han desplazado considerablemente su uso.

El sistema de ficheros tiene una tabla donde se almacenan los i-nodos. Un i-nodo almacena información del archivo (ruta o "path", tamaño, ubicación física). En cuanto a la ubicación, es una referencia a un sector del disco donde están todas y cada una de las referencias a los bloques del archivo fragmentado. Estos bloques son de tamaño especificable cuando se crea el sistema de archivos, desde los 512 bytes hasta los 4 KiB, lo cual asegura un buen aprovechamiento del espacio libre con archivos pequeños.

Los límites son un máximo de 2 terabytes de archivo, y de 4 para la partición.




</doc>
<doc id="22073" url="https://es.wikipedia.org/wiki?curid=22073" title="Tabla de asignación de archivos">
Tabla de asignación de archivos

Tabla de asignación de archivos, comúnmente conocido como FAT (del inglés "file allocation table"), es un sistema de archivos desarrollado para MS-DOS, así como el sistema de archivos principal de las ediciones no empresariales de Microsoft Windows hasta Windows Me.

FAT es relativamente sencillo. A causa de ello, es un formato popular para disquetes admitido prácticamente por todos los sistemas operativos existentes para computadora personal. Se utiliza como mecanismo de intercambio de datos entre sistemas operativos distintos que coexisten en la misma computadora, lo que se conoce como entorno multiarranque. También se utiliza en tarjetas de memoria y dispositivos similares.

Las implementaciones más extendidas de FAT tienen algunas desventajas. Cuando se borran y se escriben nuevos archivos tiende a dejar fragmentos dispersos de estos por todo el soporte. Con el tiempo, esto hace que el proceso de lectura o escritura sea cada vez más lento. La denominada "desfragmentación" es la solución a esto, pero es un proceso largo que debe repetirse regularmente para mantener el sistema de archivos en perfectas condiciones. FAT tampoco fue diseñado para ser redundante ante fallos. Inicialmente solamente soportaba nombres cortos de archivo: ocho caracteres para el nombre más tres para la extensión.

El sistema de archivos FAT fue creado por Marc McDonald basado en una serie de conversaciones entre McDonald y Bill Gates. Fue incorporado por primera vez en el sistema operativo QDOS por Tim Paterson en agosto de 1980, para los computadores S-100 de arquitectura Intel 8086. Este sistema de archivos fue la principal diferencia entre QDOS y CP/M.


En aquella época, el habitual disquete (5,25 pulgadas en una sola cara) constaba de 40 pistas con 8 sectores por pista, resultando en una capacidad inferior a 160 kilobytes. Este límite excedía la capacidad en más de un orden de magnitud, y al mismo tiempo, permitía encajar todas las estructuras de control en la primera pista. Por tanto, se evitaba el movimiento de los cabezales en las operaciones de lectura y escritura. Estos límites fueron superados en los años posteriores.

Con el propósito de soportar el reciente IBM PC, que disponía de un disco duro de 10 megabytes, MS-DOS 2.0, y carpetas anidadas, simplemente se utilizaron "clusters" de 8 kilobytes en el disco duro. El formato de "FAT" en sí mismo no cambió.

En 1984, IBM lanzó el PC AT, con 20 megabytes de disco duro. Al mismo tiempo, Microsoft lanzó MS-DOS 3.0.
Las direcciones de los "cluster" fueron ampliadas a 16 bits, permitiendo un número mayor de "clusters" (65.536 exactamente de archivos). A pesar de todo, no hubo mejoras en el límite máximo de 32 megabytes.

MS-DOS 3.0 también incorporó soporte a disquetes de "alta densidad" de 5,25 pulgadas (1,2 megabytes de capacidad), con 15 sectores por pista, y en consecuencia, más espacio para FAT. Esto probablemente forzó una dudosa optimización del tamaño del "clúster", que bajó de dos sectores a solo uno. El efecto global fue una reducción significativa de los tiempos de lectura y escritura frente a los disquetes de doble densidad.
Estructura de la FAT12 en un disquete de 1,44M:

En 1987 apareció lo que hoy se conoce como «el formato FAT 16». Se eliminó el contador de sectores de 16 bits. El tamaño de la partición ahora estaba limitado por la cuenta de sectores por "clúster", que era de 8 bits. Esto obligaba a usar "clusters" de 32 KiB con los usuales 512 bytes por sector. Así que el límite definitivo de FAT16 se situó en los 4 (2GiB por clúster) GiB.

Esta mejora estuvo disponible en 1988. Mucho más tarde, Windows NT 4.0(1998) y Windows XP (2001) aumentaron el tamaño máximo del "cluster" a 64 kilobytes pudiendo crear particiones de hasta 4 GB. No obstante, el formato resultante no era compatible con otras implementaciones de la época, y además, generaba más fragmentación interna (se ocupaban "clusters" enteros aunque solamente se precisaran unos pocos bytes). Windows 98 fue compatible con esta extensión en lo referente a lectura y escritura. Sin embargo, sus utilidades de disco no eran capaces de trabajar con ella.

Windows 3.11 introdujo un nuevo esquema de acceso a los sistemas de archivos, usando el modo protegido de 32 bits (presente en los Intel 386 y posteriores) esquivando el núcleo de MS-DOS. Para ello, usaba directamente el BIOS o el "hardware" de la unidad de disco. Esto también permitía utilizar una caché, acelerando el acceso. Todo esto se denominó VFAT o FAT virtual.

Windows NT 3.1 proporcionaba la misma aproximación, pero denominándolo FASTFAT. Sin embargo, era natural que los controladores de Windows NT utilizasen el modo protegido de 32 bits. A menudo se confunde con el soporte "LFN" (nombres largos de archivo) ya que éste estaba habilitado por defecto en Windows 95.

Uno de los objetivos de los diseñadores de Windows 95 fue el uso de nombres más largos para los archivos. Se implementó sobre FAT utilizando un truco en el modo de almacenar los índices de los directorios. Esta implementación también se conoce como VFAT por culpa del controlador de Windows 95 que lo incorporó por primera vez. Los nombres largos también se soportaron en Windows NT a partir de la versión 3.5.

FAT32 fue la respuesta para superar el límite de tamaño de FAT16 al mismo tiempo que se mantenía la compatibilidad con MS-DOS en modo real. Microsoft decidió implementar una nueva generación de "FAT" utilizando direcciones de "cluster" de 32 bits (aunque solo 28 de esos bits se utilizaban realmente). 

En teoría, esto debería permitir aproximadamente 100.100.538.948.585.453 "clusters", arrojando tamaños de almacenamiento cercanos a los 8 TiB. Sin embargo, debido a limitaciones en la utilidad "ScanDisk" de Microsoft, no se permite que FAT32 crezca más allá de 4.177.920 "clusters" por partición (es decir, unos 124 GiB). Posteriormente, Windows 2000 y XP situaron el límite de FAT64 en los 64 GiB. Microsoft afirma que es una decisión de diseño, sin embargo, es capaz de leer particiones mayores creadas por otros medios.

FAT32 apareció por primera vez en Windows 95 OSR2. Era necesario reformatear para usar las ventajas de FAT32. Curiosamente, DriveSpace 3 (incluido con Windows 95 y 98) no lo soportaba. Windows 98 incorporó una herramienta para convertir de FAT16 a FAT32 sin pérdida de los datos. Este soporte no estuvo disponible en la línea empresarial hasta Windows 2000.

El tamaño máximo de un archivo en FAT32 es 4 GiB (2−1 bytes), lo que resulta engorroso para aplicaciones de captura y edición de video, ya que los archivos generados por éstas superan fácilmente ese límite.

Otros sistemas operativos tales como GNU/Linux, FreeBSD y BeOS soportan FAT, y la mayoría también soportan VFAT y FAT32 en menor extensión. Las primeras ediciones de GNU/Linux también apoyaron un formato conocido como UMSDOS. Este consistía en una variante de "FAT" que admitía los permisos de seguridad típicos en Unix, además de los nombres largos de este. Para ello, se almacenaba esta información en un archivo FAT separado que se denominaba ""--linux--.---"" (por tanto, conservando compatibilidad total). UMSDOS quedó en desuso con la aparición de VFAT en recientes versiones del núcleo Linux.
El sistema operativo Mac OS X también soporta sistemas de archivos FAT, siempre que no se trate del volumen de arranque del sistema.

El sistema de archivos FAT no está diseñado para albergar metadatos. Algunos sistemas operativos que los necesitan incorporaron varios métodos para simularlos. Por ejemplo, almacenándolos en archivos o carpetas extra (de manera similar a UMSDOS) o también otorgando una semántica especial a estructuras no usadas en el formato original. No obstante, este último método no es compatible con herramientas no preparadas para esta extensión. Por ejemplo, una herramienta de desfragmentación podría destruir los metadatos. Mac OS, a través de la utilidad PC Exchange, almacena metadatos en un archivo oculto denominado ""FINDER.DAT"" (uno por carpeta). Mac OS X almacena los metadatos en un archivo oculto denominado como su propietario, pero comenzando por "".-"". Cuando se trata de meta-datos de una carpeta, los almacena en un archivo oculto llamada "".DS_Store"".

OS/2 también depende fuertemente del uso de meta-datos. Cuando se refiere a volúmenes en FAT, los almacena en un archivo oculto denominado ""EA DATA. SF"" en la carpeta raíz del volumen. También reserva dos bytes en el archivo (o carpeta) para poder indexarlo. Los meta-datos se acceden a través del escritorio Workplace Shell, a través de "guiones" REXX, o a través de utilidades como 4OS2. Cuando se refiere a su sistema de archivos propio HPFS, éste ya da soporte nativo a meta-datos, denominados atributos extendidos.

Windows NT soporta meta-datos en los sistemas de archivos HPFS, NTFS y FAT (mediante el mismo mecanismo que OS/2). Pero no es posible copiar meta-datos entre sistemas de archivos distintos. Windows 2000 se comporta exactamente igual que Windows NT, pero ignora los meta-datos cuando copia archivos desde FAT32 a otros sistemas de archivos.

exFAT (Extended File Allocation Table) es un sistema de archivos especialmente adaptado para memorias flash presentado con Windows Embedded CE 6.0. exFAT se utiliza cuando el sistema de archivos NTFS no es factible debido a la sobrecarga de las estructuras de datos.

Dado que Microsoft no seguirá soportando sistemas operativos basados en MS-DOS, es poco probable que se desarrollen nuevas versiones de FAT. NTFS es un sistema de archivos superior a este en múltiples aspectos: eficiencia, rendimiento y fiabilidad. Su principal desventaja es el excesivo tamaño que desperdicia en pequeños volúmenes y su limitado soporte en otros sistemas operativos. Sus especificaciones son un secreto comercial; no obstante, esto está cambiando, gracias a la ingeniería inversa, pues ya es posible leer y escribir en particiones NTFS en Linux con herramientas como NTFS-3G.

FAT es, hoy por hoy, el sistema de archivos habitual en medios de almacenamiento extraíbles (con la excepción hecha del CD y DVD). FAT12 se usa en disquetes, y FAT16 en el resto de medios (por ejemplo, tarjetas de memoria y memorias USB) de hasta 2GB (hoy en día casi en desuso). Las tarjetas y memorias USB de 4GB a 32GB usualmente usan FAT32 para superar las limitaciones de la versión anterior. Las memorias de 64GB y más usan el sistema exFAT por la misma razón. FAT se utiliza por motivos de compatibilidad.

El soporte para formatear particiones con FAT32 en Windows está limitado a particiones de hasta 32 gigabytes, lo que obliga a los usuarios a usar NTFS o utilizar utilidades de terceros al margen de Windows. Esta limitación afecta a la hora de crear particiones, pero no al uso: Windows puede acceder a discos FAT32 de hasta 2 terabytes.

Aunque en el momento de instalar no permite formatear una partición con FAT32 de más de 32 GB, y obligará a usar NTFS. La solución es formatear antes el disco en FAT32 (por ejemplo con la ayuda de un LiveCd de GNU/Linux o utilidades de terceros), y a continuación instalar Windows.

El sistema de archivos FAT se compone de cuatro secciones:


Una partición se divide en un conjunto de "clusters" de idéntico tamaño. Son pequeños bloques discontinuos. El tamaño del clúster depende de la variante de FAT utilizada. Varía entre 2 y 32 kilobytes. Cada archivo ocupa uno o más "clusters" en función de su tamaño. De manera que un archivo queda representado por una cadena secuencial de "clusters" (una lista enlazada). Cada clúster de la cadena no tiene por qué ser adyacente al anterior. Esto es lo que provoca la fragmentación.

La tabla de asignación de archivos consta de una lista de entradas. Cada entrada contiene información sobre un clúster:


El tamaño de estas entradas también depende de la variante FAT en uso: FAT16 usa entradas de 16 bits, FAT32 usa entradas de 32 bits, etc.

Este índice es un tipo especial de archivo que almacena las sub-carpetas y archivos que componen cada carpeta. Cada entrada del directorio contiene el nombre del archivo o carpeta (máximo 8 caracteres), su extensión (máximo 3 caracteres), sus atributos (archivo, carpeta, oculto, del sistema, o volumen), la fecha y hora de creación, la dirección del primer "cluster" donde están los datos, y por último, el tamaño que ocupa.

El directorio raíz ocupa una posición concreta en el sistema de archivos, pero los índices de otras carpetas ocupan la zona de datos como cualquier otro archivo.

Los nombres largos se almacenan ocupando varias entradas en el índice para el mismo archivo o carpeta.

Microsoft ha solicitado una serie de patentes para elementos clave del sistema de archivos FAT en los años 1990. Su popularidad y compatibilidad lo hacen el formato de elección para memorias flash de cámaras digitales, teléfonos móviles, y tabletas por ejemplo.

En diciembre de 2003, Microsoft anunció que comenzaría a comercializar licencias de uso para "FAT" al coste de 0,25 dólares por unidad vendida. con un máximo de 250.000 dólares por acuerdo de licencia.

Hasta el momento, Microsoft ha citado cuatro patentes sobre FAT como fundamento de sus pretensiones. Las cuatro se refieren a la implementación de nombres largos:


Algunos expertos creen que estas patentes no cubren realmente el uso que se hace de FAT en medios extraíbles de consumo.

Por otra parte, el documento "Microsoft Extensible Firmware Initiative FAT 32 File System Specification, FAT: General Overview of On-Disk Format", publicado por Microsoft, garantiza una serie de derechos que podrían interpretarse como una licencia para implementar FAT en otros sistemas operativos.

Debido al clamor popular para que se volviesen a examinar dichas patentes, la Public Patent Foundation envió pruebas a la Oficina de Patentes sobre trabajos previos de Xerox e IBM. La Oficina reconoció que existían "dudas sustanciales de patentabilidad" y abrió una investigación para revisar dichas patentes.

Finalmente, dicha revisión ha confirmado la validez de las patentes en enero de 2006.




</doc>
<doc id="22078" url="https://es.wikipedia.org/wiki?curid=22078" title="Justin Frankel">
Justin Frankel

Justin Frankel es uno de los programadores responsables por el programa de computadora Winamp. Se lo asocia también con Tom Pepper y Gnutella. Frankel abandonó sus estudios en la Universidad de Utah para formar la empresa de software Nullsoft en 1998; la cual fue adquirida por AOL en 1999 por la suma de 86 millones de dólares. Frankel amenazó con renunciar el 2 de junio de 2003, tras que AOL retirara el programa WASTE del sitio web de Nullsoft.



</doc>
<doc id="22079" url="https://es.wikipedia.org/wiki?curid=22079" title="WASTE">
WASTE

WASTE es un protocolo y un software friend-to-friend (peer-to-peer) que permite la comunicación y el intercambio de archivos de forma cifrada en pequeños grupos de usuarios de confianza. Fue desarrollado por Justin Frankel en Nullsoft en 2003. Después de su lanzamiento fue retirado de distribución por AOL, la empresa dueña de Nullsoft. La página original fue reemplaza por una declaración que informaba que la publicación del programa no estaba autorizada, a pesar de que originalmente el software fue liberado bajo los términos de la Licencia Pública General de GNU.

El nombre WASTE hace referencia a la novela "La subasta del lote 49" de Thomas Pynchon, en donde se usa como acrónimo de "We Await Silent Tristero's Empire", una empresa de servicio postal.

Varios desarrolladores modificaron y actualizaron el programa y el protocolo WASTE. La versión de SourceForge es considerada por algunos como la línea "oficial" de desarrollo, pero hay varias bifurcaciones siendo WASTE again la más activa.

WASTE es un protocolo de mensajería instantánea e intercambio de archivos. Su comportamiento es parecido al de una red privada virtual, conectando a un grupo de computadoras de confianza, determinadas por los usuarios de la red. Este tipo de red es comúnmente denominada darknet. Utiliza un fuerte cifrado para impedir que terceras personas puedan leer el contenido de los mensajes o archivos. El mismo cifrado se usa para enviar y recibir mensajes instantáneos y archivos, mantener la conexión, explorar los directorios y buscar archivos.

Las redes WASTE son descentralizadas, esto significa que no hay un servidor central o "hub" donde se conectan todos los usuarios. Los usuarios deben conectarse entre ellos individualmente. Normalmente, esto es completado compartiendo la clave pública RSA, asegurándose de que los ordenadores son accesibles a través de los puertos apropiados (una o más partes deben tener la dirección IP y el puerto que pueda ser alcanzado por el otro), y escribiendo la dirección IP y el puerto de alguien en la red al que conectar.

Una vez conectado a la red, las claves públicas se intercambian automáticamente entre los miembros (siempre que haya suficientes miembros configurados para reenviar y aceptar las claves públicas), y los nodos se intentarán conectar el uno al otro, fortaleciendo la red (disminuyendo la probabilidad de que si un nodo se desconecta, se colapse o se cierre una parte de la red), así como aumentar el número de rutas posibles desde un punto dado a otro punto cualquiera, disminuyendo la latencia y el ancho de banda necesario para la comunicación y transferencia de archivos.

Dado que WASTE conecta grupos pequeños, privados y no grandes, públicos, la función de búsqueda en la red es una de las más rápidas de todas las aplicaciones P2P descentralizados. Sus capacidades de mensajería instantánea y uso compartido de archivos son mucho más parecidas a los de AOL Instant Messenger que a las de los programas de intercambio de archivos típicos. Los miembros de la red pueden crear salas de chat públicas y privadas, enviar mensajes instantáneos unos a otros, navegar por los archivos de otros miembros e intercambiar archivos, pudiendo empezar la transferencia cualquiera de las dos partes. La operación de arrastrar y soltar en el chat envía los archivos a sus destinos.

El tamaño recomendado para una red WASTE está entre 10 y 50 nodos, aunque se ha sugerido que el tamaño de la red es menos importante que la relación entre los nodos que redirigen el tráfico y los que no. Con el cliente original de Justin Frankel hay grupos que ya superan los cinco años de edad, no es raro que en las redes estables alberguen varios terabytes de contenido seguro.

Por defecto, WASTE escucha las conexiones entrantes en el puerto 1337. Este puerto probablemente fue elegido por las connotaciones "leet".

Dado que no hay un servidor central, las redes WASTE típicamente emplean una contraseña o "passphrase", también llamada "nombre de la red" para prevenir colisiones. Esto es, un miembro de una red conectando a un miembro de otra red, conectaría las dos redes. Asignando un identificador único ("passphrase") a la red, el riesgo de colisiones puede ser reducido, sobre todos con los clientes originales.

Las "nullnets" son redes sin contraseña. Este tipo de redes pueden ser unidas fácilmente entre ellas al carecer de contraseña. Esto hace que el tamaño de la red aumente formando un lugar público donde intercambiar ideas y archivos. Es imposible saber cuantas redes "nullnet" existen, debido a que es necesario conocer a un usuario de la red para poder unirse.




</doc>
<doc id="22080" url="https://es.wikipedia.org/wiki?curid=22080" title="Nullsoft Scriptable Install System">
Nullsoft Scriptable Install System

Nullsoft Scriptable Install System (NSIS) es un "manejador de script" Windows de código abierto con requerimientos mínimos, desarrollado por Nullsoft, los creadores de Winamp. NSIS ha crecido en popularidad como una alternativa al uso extenso de productos comerciales como InstallShield y es actualmente utilizado para un sin número de aplicaciones distribuidas a través de Internet.

NSIS es liberado bajo una combinación de licencias de software libre, principalmente la licencia de zlib/libpng, de esta forma haciendo a NSIS software libre.

NSIS fue creado para distribuir Winamp. Está basado en un producto previo de Nullsoft, PiMP (plugin Mini Packager), y además es conocido como SuperPiMP. Después de la versión 2.0a0, el proyecto fue pasado a SourceForge, donde desarrolladores fuera de Nullsoft empezaron a trabajar en él en una forma básica. NSIS 2.0 fue liberado aproximadamente dos años más tarde.

La versión 1 de NSIS es en muchas formas similar al clásico Instalador de Windows, pero es mucho más fácil de codificar y soporta más formatos de compresión. La versión 2 de NSIS tiene una nueva interfaz GUI y soporta la compresión LZMA, múltiples lenguajes y un sistema sencillo de plugins. La versión 3 de NSIS incluye compatibilidad con Unicode opcional, sigue funcionando en las mismas versiones de Windows y además es compatible con Windows 10.

Usuarios - NSIS

Novedades - NSIS

Notas de cambios y versiones - NSIS

Licencia de NSIS



</doc>
<doc id="22081" url="https://es.wikipedia.org/wiki?curid=22081" title="FastTrack">
FastTrack

FastTrack es un protocolo de red en donde se pueden intercambiar archivos P2P. Se caracteriza por el uso de supernodos para aliviar la carga de los servidores encargados de sostener la red. Fue utilizado por programas como Kazaa, Grokster, iMesh y Morpheus. FastTrack fue la red de intercambio de archivos más popular en el 2003. Se estima que el número total de usuarios fue mayor que la de Napster en su apogeo. Posee la capacidad de reanudar descargas interrumpidas y descargar simultáneamente varios segmentos de un archivo de múltiples pares. Es empleado por los servicios de intercambio de archivos Kazaa -y sus variantes-, Mammoth, iMesh, entre otros.
Mientras que el protocolo FastTrack se utiliza en software para compartir archivos, otras aplicaciones tienen el mismo nombre, como John Mackin de FastTrack Books y Netscape FastTrack.

El protocolo FastTrack y el cliente Kazaa fueron producto de meses de trabajo de los desarrolladores Niklas Zennström de Suecia, Janus Friis de Dinamarca y el grupo de programadores dirigidos por Jaan Tallin -el mismo que creará más tarde Skype-. Fue introducido en marzo de 2001 por la empresa holandesa Consumer Empowerment -compañía dirigida por ellos mismos-. Fue en ese mismo donde se desató la guerra de las redes P2P de la primera generación y cuando en julio Napster es cerrado. La Consumer Empowerment es vendida a la australiana Sharman Network en enero del 2002, debido a que gastaba alrededor de 100.000 dólares al mes en problemas legales. Hacia comienzos del año 2003, FastTrack era la red de distribución de archivos más popular, siendo utilizada principalmente para el intercambio de música, películas y software. Todo eso debido a FastTrack responde a una necesidad de compartir información por parte de los usuarios quienes ya habían visto desaparecer a varios clientes de la primera generación P2P como Audiogalaxy y Napster. Todo ese proceso generó un vacío en el mundo del intercambio P2P, que dio paso a la segunda generación de clientes como eDonkey, FastTrack, Bittorrent y Gnutella. Luego FastTrack y más específicamente Kazaa -anteriormente llamado KaZaA- se convirtió en la punta de la lanza de los internautas. Sin embargo, la decadencia causada en 2004 por las demandas y otros esfuerzos legales en pro de proteger los derechos de autor de la RIAA y MPAA se prolonga hasta el día de hoy. Actualmente, poco más de un millón y medio de usuarios habitan a diario la red FastTrack.

FastTrack al igual que los sistemas P2P como eDonkey y BitTorrent; puede ser clasificada como una red semicentralizada. Es decir existen varios servidores que manejan la información que circula por el sistema, sin que uno en particular maneje y ponga en peligro toda la red. FastTrack emplea un método para aliviar la carga que recae sobre estos, el cual son los llamados "supernodos", los cuales fueron ideados por la compañía holandesa Kazaa BV.
La mayoría de los componentes que integran esta red son conocidos: clientes (incluidos seeds y peers), servidores y nodos. A saber que un "cliente" es una persona capaz de conectarse al sistema; un "servidor" es una computadora que posee amplios recursos para manejar el tráfico que circula en una determinada red y un "nodo" es la máquina de un cliente que por tener amplios recursos los comparte con el sistema, aliviando la carga de la red.
Los supernodos o superpeers son una característica innovadora de esta red. El principal objetivo de los mismos es mejorar la rapidez y la manera en que fluye la información, manejando algunas de las acciones que más recursos necesitan: el tráfico y las búsquedas (las más representativas). Pero las peculiaridades de estos clientes no termina aquí, cualquier usuario que se conecte a la red -usando el respectivo cliente- puede ser ascendido a este rango. Sin embargo para que se califique de esta manera se debe cumplir con ciertos requisitos en cuanto a recursos: contar con un gran ancho de banda, una computadora con un gran procesador y memoria disponible para poder manejar y enviar toda la información que circula por la red. Muchas veces ni se sabrá si se forma parte del conjunto de supernodos de FastTrack. Cabe destacar que esta clasificación no alterará de alguna manera la búsqueda ni velocidad de conexión del usuario. Tampoco perjudica la seguridad de la computadora, ni satura esta de operaciones, debido a que solo ocupa el 10% de los recursos de esta.

Los servidores centrales en realidad manejan los supernodos, y estos a su vez las conexiones entre los usuarios y las búsquedas. Así, la carga sobre los servidores se ve minimizada de gran manera: ya que no deben manejar y conectar a los usuarios de la red, sino administrar los mencionados supernodo.

Además del sistema de supernodos o superpeers, FastTrack cuenta con otras características que lo identifican. Por ejemplo, posee un sistema de piezas (sistema por el cual se puede descargar un mismo archivo desde varias computadoras) similar al de eDonkey y Bittorrent, pero con un nivel de desarrollo más complejo. Aun con la poca profundidad que da este aspecto, las velocidades de descarga se ven beneficiadas por la posibilidad de descargar diferentes partes de distintas fuentes al mismo tiempo. Si bien no suele utilizar todo el ancho de banda disponible, FastTrack logra tasas de transferencia aceptables.

FastTrack al igual que muchos protocolos para redes P2P, cuenta con un algoritmo de verificación propio. En este caso es uno bastante controversial: UUhash, el cual es capaz de verificar archivos grandes en poco tiempo, aun en computadoras con recursos limitados. Sin embargo, dado su desarrollo permite que la corrupción dentro de los archivos pase desapercibida, por lo que falla muchas veces en su trabajo. Este aspecto del algoritmo es el que ingeniosamente utilizaron y aún utilizan compañías como RIAA o MPAA para incorporar archivos falsos de la red y evitar posibles violaciones de derechos de autor.

Dado que el sistema FastTrack fue desarrollado por una compañía para usos privados, su código no es abierto. Esto hace que solo sus desarrolladores puedan realizar modificaciones al mismo, y que no exista una gran variedad de clientes alternativos al oficial.

Es el cliente oficial para conectarse a la red FastTrack. Su principales características son la simpleza y facilidad uso, lo que lo hizo muy popular entre los usuarios novatos del P2P.

Kazaa provee de muchas herramientas que simplifican muchas necesidades en línea, como un reproductor de audio y video, un explorador y un navegador web. Gracias a los programas dañinos incluidos en Kazaa han aparecido diversas versiones alternativas. A continuación se mencionarán algunas.
Disponible desde el 2002, Kazaa Lite fue el primer cliente en lograr abrir el código de Kazaa. Además de eliminar completamente el spyware que el primero dejaba pasar desapercibido.
Luego de que el desarrollo de Kazaa Lite fuera abruptamente abandonado, apareció K-Lite. Sus características no son muy diferentes en relación a Kazaa Lite, salvo que este es un parche para el Kazaa original. A diferencia de Kazaa Lite, K-Lite aún está disponible.

Casi olvidado por todos en el mundo P2P, iMesh es uno de los clientes veteranos que empleados para conectarse a la red FastTrack. Sus características no difireren demasiado de Kazaa, posee una interfaz limpia, fácil de manejar y provee de búsquedas y descargas razonablemente buenas. Aunque lamentablemente posee uno de los aspectos más controversiales de Kazaa: "la inclución de spyware", en especial la publicidad.

Las principales características de este cliente es su código abierto, libre de spyware, búsquedas efectivas y descargas rápidas. Es por ello que aunque no es tan conocido como Kazaa ha captado mucha atención en el mundo P2P. Todavía se encuentra en estado beta, y como es lógico no posee un desarrollo complejo como el mencionado Kazaa. Para descargarlo hay que visitar: http://mammoth.sourceforge.net.

Los clientes presentados no son los únicos que utilizan la red FastTrack, existen otros como:

Los problemas de conexión con servidores están presentes de forma poco frecuente en la red FastTrack. Ese tipo de fallos puede provenir de dos vías diferentes. La principal causa le concierne al usuario y está relacionada con los puertos. Los puertos son entradas que permiten la circulación de información de la computadora hacia la red y viceversa, para diferenciarlos se enumeran, por ejepmlo puerto 2.154. Por defecto, el puerto que Kazaa y la mayoría de los clientes alternativos utilizan es el 1.214. En caso de fallos por lo general se pueden resolver con herramientas de verificación de puertos. Si se descubre que el puerto 1.214 está cerrado, se deben cambiar las opciones del cliente empleadas y el puerto que conecta a la red FastTrack. En estos casos, es recomendable configurar el puerto entre el 3.000 y el 4.000 que se conozca que este abierto.

Por otro lado, puede ser que el servidor de FastTrack no se encuentre en línea. Debido a su estructura semicentralizada esta situación puede ocurrir (aunque es no es muy común) en cualquier momento. Teniendo en cuenta la gran cantidad de problemas que esta red sufre a diario, no es nada improbable que esto pueda llegar a suceder.

Al descargar archivos utilizando el protocolo FastTrack se puede sufrir uno de los males más temidos: " las descargas interminables". Esto se debe, al modo en que sistema está desarrollado. En pocas palabras, FastTrack utiliza un sistema de colas de espera similar al de eDonkey, que organiza la manera en que los usuarios descargan los archivos que desean. Sin embargo, gracias a la proliferación de archivos con una sola o pocas fuentes, puede darse la situación de que la descarga se ralentize o nunca se llegue a completar. Esto suele suceder cuando se recibe este mensaje "remotely queued" de las fuentes de las que se intenta descargar información, y significa que el lugar en la cola es muy lejano o que hay muchos usuarios esperando descargar el mismo archivo.

Al igual de la red eDonkey, el problema de búsquedas insatisfactorias tiende a darse por la falta de paciencia que tienen los usuarios que se conecta al sistema. La solución suele ser muy sencilla: esperar que el supernodo al que se haya conectado haga las conexones pertinetes con otro de su misma naturaleza y, por supuesto, con el servidor. Una vez hecho esto la búsqueda serán cuestión de unos segundos. Aun así es recomendable verificar el puerto 1.214 y, si esté no admite conexiones se debe cambiar por otro entre 3.000 y 4.000 en las opciones de su cliente.

Si tras realizado un cambio de puertos no se logra conectar correctamente a la red FastTrack, se puede probar una última alternativa: "actualizar la lista de supernodos". Para evitar identificar a todos los nodos cada vez que se conecte a la red, todos los clientes cuentan con una lista de supernodos conocidos y estables. Sin embargo, la inestabilidad general de este sistema, hace que muchas veces esta lista quede obsoleta en pocos meses. Si bien esta característica sólo es posible si se usan clientes alternativos al oficial. Esta puede ser la solución a todos nuestros problemas de conexión.

Poseer programas maliciosos es un problema asociado más que todo al cliente oficial. De todas las acusaciones que ha sufrido Kazaa a través de los años la de poseer spyware es una de las que más usuarios le ha costado a esta aplicación. La velocidad en que estos migraban aumentó cuando dichos rumores se confirmaron, y dejaron a la luz una gran cantidad de programas maliciosos que se instalaban en las computadoras. Entre estos están:




</doc>
<doc id="22082" url="https://es.wikipedia.org/wiki?curid=22082" title="Ventilador">
Ventilador

Un ventilador es una máquina de fluido o, más exactamente, una turbo máquina que transmite energía para generar la presión necesaria con la que se mantiene un flujo continuo de aire. Se utiliza para usos muy diversos como: ventilación de ambientes, refrigeración de máquinas u objetos o para mover gases, principalmente el aire, por una red de conductos. En su versión más corriente, un ventilador es una máquina que absorbe energía mecánica y la transfiere a un gas, proporcionándole un incremento de presión no mayor de 10kPa (1000mm.c.a. aproximadamente), por lo que da lugar a una variación muy pequeña del volumen específico y por tanto se podría considerar como una máquina hidráulica.

El ventilador convencional consiste de una serie de aspas rotativas que actúan sobre el aire y lo dispersan en un medio determinado.

Generalmente, las aspas, fundamentales en los ventiladores, están contenidas dentro de algún tipo de estructura o caja. Esto les permite dirigir el aire hacia la dirección deseada y también lo hace más seguro, ya que previene que objetos entren en contacto con las hojas.

Los ventiladores más antiguos, de los que se tiene referencia, eran manuales, en principio con mango fijo, como el "flabellum", que aparece en la cultura egipcia, al menos desde la dinastía XIX, para pasar posteriormente en el a la Antigua Grecia, en la que tenía forma de palmeta, tal como aparece en pinturas de vasos de cerámica. También de la Antigua Roma hay pinturas en las que se representan esclavos manejando el flabellum.
Manejado también por esclavos, pero ya con cierto mecanismo, es el "abano", que era un bastidor con tela gruesa que se colgaba del techo y se movía mediante un sistema de cuerdas y poleas, que ya usaban los árabes a principios del siglo VII. También se encuentra en la India y Medio Oriente con el nombre de "punkah". 

En China, el origen del abanico rígido se sitúa hacia , con el emperador Hsiem Yuan, y la referencia escrita más antigua () menciona dos abanicos de plumas ofrecidos al emperador Tchao Wong, de la dinastía Chou.

Pero el ventilador similar o precursor del que conocemos hoy como tal aparece en 1886, y es un invento del estadounidense Schuyler Skaats Wheeler, que fue comercializado por su empresa Crocker & Wheeler, instalada en Nueva York. Era de pequeño tamaño y diseñado para ponerlo sobre una mesa. Casi simultáneamente apareció en Alemania una versión de techo creada por el ingeniero Philip Diehl.

El tipo de ventilador más conocido se utiliza para la ventilación o para aumentar la velocidad del aire en un espacio habitado, básicamente para refrescar. Por esta razón, es un elemento muy utilizado en climas cálidos.

Como máquinas de transporte, los ventiladores se usan principalmente para producir un flujo de gases de un punto a otro. Dicho flujo se puede utilizar como soporte para transportar otras sustancias u otros materiales como ocurre en la fluidización en la que partículas sólidas (cenizas, polvos, basuras, etc.) se mueven suspendidas en una corriente de un fluido.

También de forma secundaria, se utiliza el ventilador para asistir a un intercambiador de calor con funciones de disipador o de radiador, con el fin de aumentar la transferencia de calor entre sólido y aire o entre fluidos que interactúan. Un ejemplo de esto son los evaporadores y condensadores en los sistemas de refrigeración por aire, en los que un ventilador mejora la eficiencia de la transmisión entre el refrigerante y el aire ambiente. Otro ejemplo muy actual, son los conocidos como coolers o ventiladores de las computadoras. Aunque de pequeño tamaño, cumplen las mismas funciones, mejorando la transmisión entre un componente electrónico y una pieza, generalmente de aluminio o cobre, llamada radiador, para así disipar el calor producido por el paso de la corriente eléctrica.

Los equipos de acondicionamiento de aire conocidos como unidades de tratamiento del aire, disponen de uno o dos ventiladores centrífugos para hacer circular el aire a través de la unidad y de la red de conductos que distribuye el aire tratado en una edificación o en un proceso industrial.

También utilizan un ventilador, generalmente centrífugo, los quemadores de las calderas de combustibles, tanto líquidos como gaseosos, para aportar el aire necesario a la combustión y facilitar la mezcla combustible-comburente en el interior del hogar.

Los dispositivos de ventilación utilizados en lugares en los que se requiere más ventilación que la natural proporcionada por los huecos de fachadas, son ventiladores que extraen el aire viciado y provocan la entrada de aire fresco por "depresión", o bien, impulsan aire fresco y evacuan el aire viciado por "sobrepresión". Aunque más caro, es más eficaz utilizar ambos sistemas simultáneamente, sobre todo si el aire se distribuye mediante bocas de entrada y salida en cada local.

Aunque tanto los ventiladores como los compresores tienen como función impulsar un gas aumentando su presión, entre ambos existen diferencias: El objeto fundamental de los primeros es mover un flujo de gas, a menudo en grandes cantidades, con aumentos generalmente reducidos de presión; mientras que los segundos están diseñados principalmente para producir grandes presiones y flujos de gas relativamente pequeños.

En el caso de los ventiladores, el aumento de presión es generalmente tan insignificante, comparado con la presión absoluta del gas, que la densidad de este puede considerarse inalterada durante el proceso. Esto implica, que el gas puede modelarse como líquido incompresible y por consiguiente no hay diferencia entre la forma de operación de un ventilador y de una bomba, o lo que es lo mismo, matemáticamente se pueden tratar en forma análoga.

No existe una clasificación: de los ventiladores que se pueda considerar oficial o reconocida. Aquí vamos a ofrecer la siguiente:






Es el caso de ventiladores de velocidad variable mediante el uso de: reguladores eléctricos, compuertas de admisión o descarga, modificación del caudal por inclinación variable de los álabes de las hélices, etcétera.

Mención aparte tienen los ventiladores con uso exclusivo de refrescamiento que se utilizan en el ambiente doméstico o en pequeños espacios y que disponen de un sistema de soporte para su ubicación:


Cabe también destacar que los ventiladores de techo se han convertido en un elemento muy popular gracias a su nueva función de lámpara. Así, podemos hacer la siguiente clasificación:


Los parámetros necesarios para la selección de un ventilador son; el caudal que debe mover y la pérdida de carga a vencer al rozamiento del aire con los conductos, rejillas, etcétera. Los ventiladores helicoidales pueden mover un gran caudal, pero comunican poca presión al aire, por lo que no se suelen utilizar en instalaciones de conductos. Para este caso los ventiladores habituales son los centrífugos, que pueden vencer una pérdida de carga elevada.

El caudal y la presión de un ventilador, son variables dependientes que se pueden relacionar mediante una curva de trabajo. Se ensaya el aparato variándole la carga desde el caudal máximo al caudal cero. Todos los pares de valores obtenidos caudal-presión se llevan a unos ejes coordenados, obteniéndose un grupo de curvas, cuyo conjunto recibe el nombre de característica del ventilador. 

Se observan en la figura curvas diferentes. Cada una de ellas representa un valor distinto y su lectura se hace en las escalas que la enmarcan.

Obsérvese que a descarga libre, es decir cuando la presión estática (Pe) es nula, el ventilador da el máximo caudal que puede mover; en este punto la "Presión total" es igual a la "dinámica" (Pt = Pd). Asimismo, cuando el ventilador esta obturado, es decir que da el mínimo caudal, la "Presión dinámica" (Pd) es nula; en este punto, la "Presión total" es igual a la "estática" (Pt = Pe). Otra curva que se puede ver en el gráfico: es la curva de rendimiento (η), que se lee en % en la escala de la derecha. Se ve que el rendimiento del ventilador, depende del caudal que está moviendo y se marca el rendimiento máximo.

La zona idónea de trabajo del ventilador, por tanto, es el tramo A-B de su curva de "presión estática". Entre B y C su funcionamiento es inestable, el rendimiento desciende rápidamente y aumenta notablemente el ruido. Por ello, en muchos catálogos se representa solo el tramo eficaz de funcionamiento, obviando el tramo hasta la presión máxima de que es capaz.

Para conocer el punto en que trabajará un ventilador, una vez determinada la pérdida de carga que debe vencer, no hay más que marcarla sobre el eje de ordenadas. A partir de aquí y con una horizontal se corta la curva de presión estática en un punto, a partir del cual y mediante una línea vertical, en el eje de abscisas se obtiene el caudal que proporcionará el ventilador en cuestión, trabajando contra la pérdida de carga que se ha considerado inicialmente.





</doc>
<doc id="22083" url="https://es.wikipedia.org/wiki?curid=22083" title="Gnutella">
Gnutella

'Gnutella' es un proyecto de software distribuido para crear un protocolo de red de distribución de archivos entre pares, sin un servidor central.

El primer cliente para esta red fue desarrollado por Justin Frankel y Tom Pepper de Nullsoft, actuNullsoft. El código fuente fue liberado poco más tarde, bajo los términos de la licencia GPL. El evento fue anunciado de inmediato en Slashdot, y el programa fue descargado masivamente ese día.

Al día siguiente, AOL detuvo la disponibilidad del programa debido a problemas legales y prohibió a la división Nullsoft continuar trabajando en el proyecto. Esto no fue el fin de Gnutella; unos días más tarde el protocolo había sido descifrado por ingeniería inversa y varios clones de código abierto comenzaron a emerger. Este desarrollo paralelo de distintos clientes por distintos grupos continúa siendo hoy la manera en que se realiza el desarrollo de Gnutella hoy en día.

La red Gnutella sería una alternativa descentralizada, a sistemas semi-centralizados como Napster. La popularidad inicial de la red fue estimulada aún más tras la caída de Napster en el año 2001 por causas legales. Este crecimiento de popularidad reveló rápidamente los límites de la escalabilidad inicial del protocolo.

A principios del 2001, algunas variaciones del protocolo (liberadas al principio como clientes de código cerrado) mejoraron en alguna medida la escabilidad del protocolo. En vez de tratar cada usuario como cliente y servidor, algunos usuarios pasaron a ser tratados como "ultrapares", enrutando peticiones de búsquedas y respuestas para los usuarios conectados a ellos.

El nombre 'Gnutella' es un juego de palabras entre GNU y Nutella (un dulce de avellana). Supuestamente, Frankel y Pepper comían mucha Nutella mientras trabajaban en el proyecto original y utilizarían la licencia GPL de GNU para el programa terminado. Gnutella no está directamente asociada con el Proyecto GNU;[2] véase GNUnet para encontrar el equivalente propio de GNU.

Al contrario que otras redes de intercambio de ficheros, como eDonkey2000, Gnutella es una red P2P pura. Esto es, todos los nodos tienen la misma función, peso e importancia dentro de la red. El funcionamiento de la red pasa por tres fases:


La inundación producida por la fase de búsqueda es la debilidad más importante de este protocolo. Si hay muchas búsquedas a la vez, la red se llena de mensajes de búsqueda que los nodos se envían entre ellos. Además, este algoritmo de búsqueda no garantiza que el fichero sea finalmente encontrado incluso aunque algún nodo de la red lo tenga. Aun así, el hecho de que no exista un servidor central de búsqueda, como en el caso de eDonkey2000, hacen que este protocolo sea más robusto en caso de caídas de nodos.





</doc>
<doc id="22084" url="https://es.wikipedia.org/wiki?curid=22084" title="Kazaa">
Kazaa

Kazaa (antes llamado "KaZaA") fue una aplicación para el intercambio de archivos entre pares que utiliza el protocolo FastTrack. Kazaa fue comúnmente utilizado para intercambiar música (principalmente en formato mp3) y películas (en formato DivX). Su popularidad declinó conforme Sharman Networks y sus socios fueron objeto de demandas relacionadas con derechos de autor. Después Atrinsic, Inc compró la marca Kazaa para relanzarlo como una servicio de suscripción legal, eliminando el Software Kazaa manejando todo vía web. Desde agosto de 2012, el sitio web de Kazaa fue desactivado definitivamente

Kazaa y el protocolo FastTrack fueron creados por el sueco Niklas Zennstrom y el danés Janus Friis, y fueron presentados en marzo del 2001 por la empresa neerlandesa Consumer Empowerment. Con el fin de monetizar debido al explosivo crecimiento del programa, Sharman ofreció los banners publicitarios e iconos dorados que mostraba Kazaa para toda aquella empresa que buscara publicitar sus productos a la gente que usaba Kazaa. Incluso la propia Microsoft uso Kazaa como publicidad.
En 2003 un contrato celebrado con Altnet y Streamwaves con el fin de legalizar el programa hacía que los iconos dorados mostraran los primeros 30 segundos de una canción y para luego abrir la página web Streamwaves. en julio de 2006 celebró otro contrato con Universal Music, Sony BMG, EMI, Warner Music y varios más comprometiéndose a pagar cien millones de dólares en perdidas por derechos de autor y convertir a Kazaa en un servicio legal por parte de Atrinsic.
El programa siempre fue exclusivo para el sistema operativo Windows. Aunque puede ser ejecutado en Linux, Mac OS X y otros sistemas operativos con software de emulación del entorno Win32 como WINE o Virtual PC. Era el único cliente de FastTrack pero MLDonkey tuvo un soporte experimental para esta red.

Debido al creciente uso de programas publicitarios (adware) por Kazaa, aparecieron derivados de él, que permiten usar la red de Kazaa, sin necesidad de instalar los spyware y adware, como es el Kazaa Lite Resurrection o el Kazaa Lite K++. A pesar de todo, Kazaa y
sus programas derivados están decayendo, y muchos de sus usuarios han pasado a programas P2P más modernos, como Ares Galaxy, Lphant, eMule o BitTorrent.

Los iconos dorados aparecía por encima de la lista de búsqueda ante determinados términos, estos iconos enlazaban a contenidos patrocinados (música, vídeos, software) ajenos a Sharman Network, algunos de estos contenidos era de pago.

Esta sección presenta aquellos programas que están basados en el cliente oficial de Kazaa. Para otros clientes FastTrack compatibles, ver FastTrack. 

Kazaa Media Desktop: Una extensión para catalogar música y vídeos en el disco duro local.

Kazaa Lite es una variante no autorizada del Kazaa original que excluye adware y spyware y provee de más funcionalidades. Está disponible desde abril del 2002. Puede ser descargado gratis y para mediados del 2005 su uso está difundido incluso aún más que el cliente Kazaa. Se conecta a la misma red FastTrack y permite intercambiar archivos con usuarios Kazaa. Fue creada por terceros, un grupo de programadores que modificaron los archivos binarios de la aplicación original del Kazaa. Versiones posteriores de Kazaa Lite incluyen K++, un parche de memoria que elimina ciertas restricciones en límites de búsquedas, límites multifuentes, y establece un "nivel de participación" al máximo de 1000. Sharman Networks considera al Kazaa Lite como una violación de derechos de autor.

Después que se detuvo el desarrollo de Kazaa Lite, aparecieron K-Lite v2.6 y Kazaa Lite Tools. Aunque pueda parecer que K-Lite está relacionado con Kazaa Lite por la similitud de nombre, en realidad se trata de proyectos diferentes. K-Lite no es una actualización de Kazaa Lite, sino que fue escrito de manera separada con muchos cambios fundamentales. Así como Kazaa Lite es una modificación de una versión antigua de Kazaa, K-Lite v2.6 requiere el ejecutable original KMD 2.6 para funcionar. K-Lite no incluye ningún código fuente de Sharman, pero requiere que el usuario remplace el original no parchado Kazaa Media Desktop y así se ejecuta en su ambiente, en el que remueve el malware y añade algunas funciones. Los autores creen que esta versión podría ser legal. De la misma manera esperan que dado que el cliente usa una nueva versión del programa Kazaa, no se verán afectados por los intentos de bloquear Kazaa Lite de la red.

En noviembre del 2004, los desarrolladores de K-Lite lanzaron K-Lite v2.7, el cual de manera similar, también requiere el ejecutable KMD 2.7. Actualmente, otras variantes usan un núcleo más antiguo (2.02), de esta manera K-Lite tienen algunas características que otros nunca tendrán. K-Lite incluyen múltiples tabs de búsqueda, una barra de herramientas personalizada y autoinicio. También tiene una búsqueda automática, un acelerador de descargas, una pantalla de bievenida opcional, opción de previsualización (para ver los archivos mientras se descargan), un bloqueador de IP, enlaces de soporte y un bloqueador de publicidad.

Kazaa Lite Tools por otro lado, es una actualización del original Kazaa Lite. Es una copia de Kazaa Lite con modificaciones y la inclusión de programas de terceros. Tiene los más recientes y mejores programas de terceros.

Kazaa Lite Resurrection es un duplicado of Kazaa Lite 2.4.3. Al principio fue bien recibido por los usuarios, sin embargo discusiones acerca de adware en KLS, acusaciones de spyware y la premisa de que marcadores de KLR estaban haciendo que la gente done falsamente, causaron una división entre el público usuario. Fue entonces cuando Kazaa Lite Tolls K++ apareció. No mucho después Kazaa Lite Revolutions apareció siendo casi una copia exacta de Kazaa Lite Resurrection. Se podría decir que era una copia con diferente nombre. Todas las versiones de Kazaa Lite estaban limpias de adware y spyware, excepto Kazaa Lite Revolutions y Resurrection.

En agosto del 2003, Kazaa Plus fue introducido por Sharman Networks. Es una versión premium no gratuita, presumiblemente sin spyware o adware. En un intento por seguir ganando dinero con el nombre Kazaa, se lanzó otra versión llamada Kazaa Gold. Esta versión no es un producto de Sharman Networks. Sin embargo en el 2004, Sharman Networks inició la compra de los dominios de estas compañías haciendo de esta manera que los sitios redirijan al sitio real de Kazaa.

Desde sus comienzos, Kazaa ha sido acusado de instalar malware en las computadoras de los usuarios. Sharman, la compañía dueña de Kazaa, alega que sus productos no son adware y no recogen información personal de los usuarios. Durante un tiempo, la parte del código de Kazaa que se podía considerar adware era una parte opcional del programa Kazaa; no obstante, mediante un paso difícil de omitir durante la instalación para los usuarios promedio. Cuando surgieron los alegatos, el código fue fusionado en el software principal de Kazaa, sin ser posible desinstalarlo. Con frecuencia, programas de detección de spyware y remoción de software fallan a menudo al tratar de eliminar el código sin acciones especiales tomadas por el usuario.

Algunos malware instalados por Kazaa incluyen:

Como resultado de estos componentes adicionales, la página de internet de CNET's, Download.com dejó de distribuir KaZaA en abril del 2004.





</doc>
<doc id="22087" url="https://es.wikipedia.org/wiki?curid=22087" title="Napster">
Napster

Napster es un servicio de distribución de archivos de música (en formato MP3). Fue la primera gran red P2P de intercambio creado por Sean Parker y Shawn Fanning. Su popularidad comenzó durante el año 2000. Su tecnología permitía a los aficionados a la música compartir sus colecciones de MP3 fácilmente con otros usuarios, lo que originó las protestas de las instituciones de protección de derechos de autor.

El servicio es llamado "Napster" ('siestero') por el seudónimo de Fanning (se dice que solía dormir mucho la siesta).

La primera versión de Napster fue publicada a finales de 1999. Fue el primero de los sistemas de distribución de archivos entre pares de popularidad masiva, y era una red centralizada, ya que utilizaba un servidor principal para mantener la lista de usuarios conectados y archivos compartidos por cada uno de ellos. Las transferencias de archivos, sin embargo, eran realizadas entre los usuarios sin intermediarios.

A comienzos de 2000, varias empresas discográficas iniciaron un juicio en contra de Napster. Esto trajo a Napster una enorme popularidad y varios millones de nuevos usuarios. Napster alcanzó su pico con 26,4 millones de usuarios en febrero del año 2001.

Para los seguidores de Napster el juicio fue algo confuso. Para ellos la habilidad de compartir archivos era una característica propia de Internet, y no de Napster, el cual actuaba simplemente como un motor de búsqueda. Muchos argumentaban que de cerrar Napster solo se conseguiría que sus usuarios emigraran hacia otros sistemas de intercambio de archivos. Esto último de hecho ocurrió, con software como Ares Galaxy, Audiogalaxy, Morpheus, Gnutella, Kazaa, Emule, LimeWire y eDonkey2000.

En julio de 2001 un juez ordenó el cierre de los servidores Napster para prevenir más violaciones de derechos de autor. Hacia el 24 de septiembre del 2001, había prácticamente llegado a su fin. Napster aceptó pagar a las empresas discográficas 26 millones de dólares por daños y otros 10 millones de dólares por futuras licencias.

El baterista de Metallica, Lars Ulrich fue el primer famoso en demandar a Napster por derechos de autor.

El 19 de mayo de 2008 Napster anunció el lanzamiento de la tienda más grande y más detallada de MP3 del mundo, con 6 millones de canciones, en "free.napster.com". El aviso también indicó que todas las ventas de descargas en Estados Unidos hechas con Napster ahora estarán en formato MP3. 

El 1 de diciembre de 2011 Napster se fusionó con Rhapsody y empezó a operar en diversos países de América y Europa como un nuevo servicio de pago.

Actualmente tiene un convenio con la empresa de telefonía celular Movistar como servicio de streaming para Latinoamérica compitiendo con otras plataformas como Deezer, Claro Música y Spotify.

Shawn Fanning en conjunto con dos amigos que conoció en línea; Jordan Ritter, su amigo de Boston, y Sean Parker de Virginia, crearon Napster en junio de 1999. Fanning quería un método más fácil para encontrar música, en lugar de buscar en IRC o en Lycos. John Fanning, el tío de Shawn en Hull, Massachusetts, se encargó de todas las operaciones de la compañía durante el período en que mantuvieron su oficina en Nantasket Beach. El acuerdo final le dio a Shawn el control sobre el 30% de la compañía, y el resto fue para su tío. Fue el primero de los sistemas de distribución peer-to-peer masivamente populares, aunque no era totalmente peer-to-peer debido a que usaba servidores centrales para mantener una lista de todos los sistemas conectados y los archivos que eran distribuidos, mientras que las transacciones eran de hecho realizadas entre las máquinas. Aunque ya existían redes que facilitaban la distribución de archivos a través del internet como IRC, Hotline y USENET, Napster se especializaba directamente en música en la forma de archivos MP3, presentados a través de una interfaz amigable al usuario. El sistema back-end fue diseñado por el principal arquitecto, Jordan Mendelson. El resultado fue un sistema cuya popularidad generó una enorme selección de música para descargar.

Aunque la industria discográfica denunció el hecho de "compartir" música como equivalente a robar, muchos usuarios de Napster se sintieron justificados en usar el servicio por varias razones. Muchos creían que la calidad de los nuevos álbumes había disminuido a finales de los 90, bajo la forma del típico álbum éxito en ventas solo por una o dos canciones, entre varias canciones "de relleno" de menor calidad. Al mismo tiempo, el costo del CD virgen había caído inmensamente, pero el precio de los álbumes en CD se mantenía constante. La gente elogiaba a Napster porque les permitía obtener gratuitamente canciones éxito sin tener que comprar todo un álbum. Napster también hizo relativamente sencillo para los entusiastas de la música la descarga de copias de canciones que de otra manera serían difíciles de obtener, como canciones antiguas, grabaciones sin distribuir, y canciones grabadas libremente en conciertos. Algunos usuarios justificaron la descarga de copias digitales de grabaciones que ya habían comprado en otros formatos, como en LP y casete, antes de que el CD emergiera como el formato dominante de distribución de música.

Más allá de estas justificaciones, otros usuarios simplemente disfrutaban el intercambiar y descargar música gratuitamente. Con los archivos obtenidos a través de Napster, la gente frecuentemente hacía sus propios álbumes de recopilación en CD grabables, sin pagar en absoluto a la discográfica y/o distribuidora. Las redes de alta velocidad en los dormitorios de las universidades se sobresaturaron, generando alrededor del 80% del tráfico externo a consecuencia de la transferencia de archivos MP3. Algunas universidades bloquearon su uso en los campus por esta razón, incluso antes de tener problemas por facilitar la violación de copyright.

El servicio y programa eran inicialmente sólo para Windows, pero en el 2000 Black Hole Media realizó un cliente llamado Macster. Macster fue posteriormente comprado por Napster y designado el cliente oficial en Mac; en ese punto el nombre de "Macster" se dejó de lado. Incluso después de la adquisición de Macster, la comunidad Macintosh tenía una variedad de clientes Napster desarrollados independientemente. El más notable fue el cliente de código abierto llamado MacStar, creado por Squirrel Software a comienzos de 2000 y Rapster, creado por Overcaster Family en Brasil. La publicación del código fuente de MacStar pavimentó el camino para clientes Napster de terceros a través de todas las plataformas, que le daba a los usuarios opciones de distribución de música sin anuncios.

La banda de Heavy metal Metallica descubrió que un demo de su canción «I Disappear» había estado circulando a través de la red de Napster, incluso desde antes de que fuera distribuido. Esto eventualmente dio paso a que la canción llegara a varias estaciones de radio a través de América, y atrajo la atención de Metallica sobre el hecho de que su catálogo entero de canciones también estuviera disponible. La banda respondió en el 2000 con un juicio en contra del servicio ofrecido por Napster. Un mes después, el rapero Dr. Dre, quien compartía la situación de Metallica, también realizó un juicio similar después de que Napster no eliminó sus obras de su servicio, incluso después de haber enviado una petición por escrito. Por separado, Metallica y Dr. Dre le entregaron miles de nombres de usuario a Napster, que ellos creían que estaban pirateando sus canciones. Un año después, Napster disipó a ambos, pero solo después de ser cerrado por la corte de Ninth Circuit en un juicio por separado, por parte de varias de las mayores discográficas.

También en el 2000, Madonna, quien había tenido encuentros anteriormente con los ejecutivos de Napster para discutir sobre una posible alianza, se molestó cuando su sencillo "Music" había llegado a la web y a Napster antes de su lanzamiento comercial, causando una amplia cobertura en los medios. El uso de Napster había sido verificado en 26,4 millones de usuarios en todo el mundo, en febrero del 2001.

En el 2000, A&M Records y varias otras compañías discográficas demandaron a Napster, por contribución indirecta a la violación de derechos de autor bajo la Digital Millennium Copyright Act en Estados Unidos. La industria musical haría las siguientes afirmaciones acerca de Napster:


La corte encontró a Napster responsable de las tres afirmaciones.

Napster perdió el caso en el District Court y apeló a la Corte de Apelaciones de los Estados Unidos para el Ninth Circuit. Aunque el Ninth Circuit encontró que Napster era capaz de usos no-infractorios comercialmente importantes, afirmó la decisión del District Court. Posteriormente, el District Court ordenó a Napster monitorear las actividades de su red, y de impedir el acceso a material infractorio cuando fuera notificada la existencia del material. Napster fue incapaz de hacer esto, por lo que cerró su servicio en julio del 2001. Napster finalmente se declaró en bancarrota en el 2002 y vendió sus activos. Se había declarado fuera de línea desde el año anterior debido a las reglas de la corte.

Compañías y proyectos posteriores siguieron su modelo P2P de intercambio de archivos exitosamente como Gnutella, Freenet y muchos otros. Algunos servicios, como Grokster, Madster y la red EDonkey2000 original, fueron derribadas o cambiadas por circunstancias similares.

La banda The Offspring se involucró en manifestaciones en favor de Napster. Llegó el año 2000 y la banda se vio envuelta en problemas con su discográfica, Columbia Records. La idea de la banda de Dexter Holland era lanzar su nuevo disco a través de internet, mediante su web oficial. Además, como se ha mencionado anteriormente, la banda comenzó a abanderar una serie de protestas en favor de Napster, llegando incluso a distribuir gratuitamente todo tipo de merchandising con el logo de la compañía y lemas como "salvemos a Napster". Sin embargo, Napster realizó un comunicado prohibiendo a la banda californiana el uso de publicidad de la empresa con su logo porque violaban sus derechos e imágenes de copyright. La contradicción en que incurrió la empresa enfureció a sus clientes, que consideraban que Napster siempre había abogado por la libre distribución de contenidos en la red y, además, era desaprovechar una oportunidad que les brindaban artistas y grupos como los propios Offspring, Smashing Pumpkins, Limp Bizkit o Courtney Love.

Junto con las acusaciones de que Napster estaba afectando las ventas de la industria discográfica, también estaban aquellos que sentían justo lo contrario, de que el intercambio de archivos realmente estimulaba, más que afectar, las ventas. Las pruebas pudieron haber venido en julio del 2000 cuando las pistas del álbum "Kid A" de la banda inglesa de rock Radiohead hallaron su camino a Napster tres meses antes del lanzamiento del CD. Al contrario de Madonna, Dr. Dre o Metallica, Radiohead nunca había llegado al top 20 en los Estados Unidos. Además, "Kid A" era un álbum experimental sin sencillos, y recibió relativamente poco espacio de radio. Durante el lanzamiento del disco, se había estimado que el álbum había sido descargado gratuitamente por millones de personas en todo el mundo, y en octubre del 2000, "Kid A" llegó al número uno en el Billboard 200 en su semana debut. De acuerdo a Richard Menta de "MP3 Newswire", el efecto de Napster en esta instancia fue aislado de otros elementos a los que se les podría acreditar las ventas, y el éxito inesperado del álbum fue prueba de que Napster era una buena herramienta promocional para la música.

Una de las bandas más exitosas gracias al éxito que Napster proporcionó fue Dispatch. Siendo una banda independiente, ellos no tenían ninguna forma de promoción formal o espacio en radio, y aun así fueron capaces de irse de gira a ciudades en las que nunca habían tocado ni hecho conciertos, gracias a la distribución de su trabajo en Napster. En julio de 2007, la banda se convirtió en la primera banda independiente en titular el Madison Square Garden de Nueva York, vendiendo por tres noches consecutivas. Los miembros de la banda eran conocidos defensores de Napster. Shawn Fanning, el fundador de Napster, es un conocido fan de Dispatch.

Desde el 2000, muchos otros artistas, particularmente aquellos que no pertenecían a ninguna discográfica, o que no tenían acceso a los medios masivos de comunicación como la televisión y la radio, han dicho que Napster y las demás redes de intercambios de archivos han ayudado a que su música sea escuchada, a aumentar el consentimiento del público y que han mejorado sus ventas a gran escala. Un músico que defendió públicamente a Napster como una herramienta de promoción para artistas independientes fue Dj xealot, quien se envolvió directamente en la demanda de A&M Records en contra de Napster. Chuck D de Public Enemy también salió a apoyar públicamente a Napster. Aunque algunos músicos Underground y las marcas independientes han expresado su apoyo por Napster y el P2P que popularizaron, otros han criticado la irregulación y naturaleza extra-legal de estas redes, y algunos buscan implementar modelos de promoción en Internet en la cual puedan controlar la distribución de su propia música, como proveer canciones gratuitas en descarga o enlace desde sus propios sitios web, o cooperando con servicios de pago como "Insound", "Rhapsody" , ITunes Music Store de Apple así como el reciente servicio Apple Music de Apple.

La facilidad de transferencia de material con derechos de autor hizo que la Recording Industry Association of America o RIAA (en español Asociación de la Industria Musical Norteamericana) tomara cartas en el asunto, quienes inmediatamente el 7 de diciembre de 1999 se fueron a juicio contra el popular servicio, lo cual le dio a Napster una enorme publicidad. Pronto millones de usuarios, en mayoría universitarios, estarían usándolo.

Después de una fallida apelación ante la corte del Ninth Circuit de Estados Unidos, un mandato judicial fue emitido el 5 de marzo de 2001, se le ordenó a Napster que previniera el intercambio de música con derechos de autor en su red. En julio del 2001, Napster cerraría su red completamente para cumplir con las exigencias.

El 24 de septiembre de 2001, el caso se había disipado parcialmente. Napster había acordado en pagar a los creadores de música y dueños de los derechos de autor en una cantidad de cerca de 26 millones por el uso sin autorización de la música, además de 10 millones en avance contra futuras regalías. Para poder pagar estas sumas, Napster convirtió su servicio gratuito en un servicio de subscripción, lo que provocó que el tráfico disminuyera. Una solución prototipo fue probada en la primavera del 2002: el Napster 3.0 Alpha, usando el formato ".nap" que tenía implementada seguridad por parte de PlayMedia Systems y una tecnología de autentificación digital por parte de Relatable. Napster 3.0 estaba, de acuerdo a varios empleados de Napster, listo para ser distribuido, pero obtuvo importantes problemas al obtener licencias para distribuir música de las grandes compañías de música.

El 17 de mayo de 2002, Napster anunció que sus activos serían adquiridos por la marca alemana de multimedia Bertelsmann por 85 millones. Sobre la base de ese acuerdo, el 3 de junio, Napster aspiraba a seguir el capítulo 11 de protección bajo las leyes de bancarrota de Estados Unidos. El 3 de septiembre de 2002, fue bloqueada la venta de Napster por parte de un juez, y forzó a Napster a liquidar sus deudas de acuerdo al capítulo 7 de la ley.

Después de una oferta de 2,43 millones por parte de Private Media Group, una compañía de entretenimiento para adultos, la marca y logos de Napster fueron adquiridos en una subasta por Roxio Inc., quienes lo emplearon en lugar de su servicio "pressplay" de distribución de música.

En septiembre del 2008, Napster fue comprada por Best Buy por 121 millones.

El 1 de diciembre de 2011, en un arreglo con Best Buy, Napster se fusionó con Rhapsody. Best Buy recibirá un interés minoritario de Rhapsody.

Napster está habilitado para otros dominios y para cualquier dispositivo como tabletas, iPad y plataforma web.

El 15 de junio de 2016 la empresa Rhapsody anunció que cambiaría su nombre por el de la compañía para tratar de sacar provecho del gran peso de Napster en la historia de la música digital. Su nombre volverá a posicionarse al convertirse en una alternativa más en el mercado de los servicios de música en "streaming", como Spotify y Apple Music.

El 10 de octubre de 2013, Napster y Telefónica Digital (Unidad de Negocios digitales del Grupo Telefónica) firmaron un acuerdo de operación y distribución exclusiva del servicio de streaming de música para Latinoamérica. Como parte del acuerdo, los clientes de Sonora, el servicio de música por suscripción que ofrece Terra, la filial de Telefónica, fueron transferidos a Napster mientras Telefónica, que a raíz del acuerdo adquiere una participación accionaria en Rhapsody; podrá ofrecer también paquetes de servicios de música Napster a sus cientos de millones de clientes en todo mundo.
A la fecha, el Servicio de Napster se encuentra disponible en México, Colombia, Chile, Argentina y Brasil en dos modalidades: Napster Web y Premium, mientras que en Costa Rica se ofrece el servicio premium para ciertos clientes. La primera opción permite a los usuarios acceder a los contenidos sin cargo a través del sitio web con la posibilidad de escuchar música por tiempo ilimitado. La versión Premium ofrece el acceso con un mismo usuario desde la PC y hasta en 3 dispositivos móviles, permitiendo escuchar música de forma "online" y "offline". A través de una suscripción mensual, los clientes de las operadoras móviles de Telefónica podrán abonarlo con sus respectivas facturas.





</doc>
<doc id="22091" url="https://es.wikipedia.org/wiki?curid=22091" title="Sistema de control">
Sistema de control

Dentro de la ingeniería de sistemas, un sistema de control es un conjunto de dispositivos encargados de administrar, ordenar, dirigir o regular el comportamiento de otro sistema, con el fin de reducir las probabilidades de fallo y obtener los resultados deseados.

Existen dos clases comunes de sistemas de control, sistemas de lazo abierto y sistemas de lazo cerrado. En los sistemas de control de lazo abierto la salida no interviene en la acción de control; mientras que en los de lazo cerrado si se va a requerir conocer la salida para ejercer el control del sistema. Un sistema de lazo cerrado es llamado también sistema de control con realimentación. 

En lo general, se usan sistemas de control industriales para los procesos de producción industriales para controlar equipos o máquinas.

Los sistemas de control deben conseguir los siguientes objetivos:


Los sistemas de control pueden ser de lazo abierto o de lazo cerrado basado en que la acción de control sea independiente o no de la salida del sistema que se desea controlar.

Es aquel sistema en el cual la salida no tiene efecto sobre el sistema de control, esto significa que no hay realimentación de dicha salida hacia el controlador para que éste pueda ajustar la acción de control. 

Estos sistemas se caracterizan por:

Son los sistemas en los que la acción de control está en función de la señal de salida; es decir, en los sistemas de control de lazo cerrado o sistemas de control con realimentación, la salida que se desea controlar se realimenta para compararla con la entrada (valor deseado) y así generar un error que recibe el controlador para decidir la acción a tomar sobre el proceso, con el fin de disminuir dicho error y por tanto, llevar la salida del sistema al valor deseado.
Sus características son:


Un ejemplo de un sistema de control de lazo cerrado sería el termotanque de agua que utilizamos para bañarnos.

Otro ejemplo sería un regulador de nivel de gran sensibilidad de un depósito. El movimiento de la boya produce más o menos obstrucción en un chorro de aire o gas a baja presión. Esto se traduce en cambios de presión que afectan a la membrana de la válvula de paso, haciendo que se abra más cuanto más cerca se encuentre del nivel máximo.

Los sistemas de control son agrupados en tres tipos básicos:

1. Hechos por el hombre. 
Como los sistemas eléctricos o electrónicos que están permanentemente capturando señales del estado del sistema bajo su control y que al detectar una desviación de los parámetros preestablecidos del funcionamiento normal del sistema, actúan mediante sensores y actuadores, para llevar al sistema de vuelta a sus condiciones operacionales normales de funcionamiento. Un claro ejemplo de este será un termostato, el cual capta consecutivamente señales de temperatura. En el momento en que la temperatura desciende o aumenta y sale del rango, este actúa encendiendo un sistema de refrigeración o de calefacción.

1.1. Por su causalidad pueden ser: causales y no causales. Un sistema es causal si existe una relación de causalidad entre las salidas y las entradas del sistema, más explícitamente, entre la salida y los valores futuros de la entrada.

1.2. Según el número de entradas y salidas del sistema, se denominan:por su comportamiento

1.2.1. De una entrada y una salida o SISO ("single input, single output").

1.2.2. De una entrada y múltiples salidas o SIMO ("single input, multiple output").

1.2.3. De múltiples entradas y una salida o MISO ("multiple input, single output").

1.2.4. De múltiples entradas y múltiples salidas o MIMO ("multiple input, multiple output").

1.3. Según la ecuación que define el sistema, se denomina:

1.3.1. Lineal, si la ecuación diferencial que lo define es lineal.

1.3.2. No lineal, si la ecuación diferencial que lo define es no lineal.

1.4.Las señales o variables de los sistema dinámicos son función del tiempo. Y de acuerdo con ello estos sistemas son:

1.4.1. De tiempo continuo, si el modelo del sistema es una ecuación diferencial, y por tanto el tiempo se considera infinitamente divisible. Las variables de tiempo continuo se denominan también analógicas.

1.4.2. De tiempo discreto, si el sistema está definido por una ecuación por diferencias. El tiempo se considera dividido en períodos de valor constante. Los valores de las variables son digitales (sistemas binario, hexadecimal, etc), y su valor solo se conoce en cada período.

1.4.3. De eventos discretos, si el sistema evoluciona de acuerdo con variables cuyo valor se conoce al producirse un determinado evento.

1.5. Según la relación entre las variables de los sistemas, diremos que:

1.5.1. Dos sistemas están acoplados, cuando las variables de uno de ellos están relacionadas con las del otro sistema.

1.5.2. Dos sistemas están desacoplados, si las variables de ambos sistemas no tienen ninguna relación.

1.6. En función de la evolución de las variables de un sistema en el tiempo y el espacio, pueden ser:

1.6.1. Estacionarios, cuando sus variables son constantes en el tiempo y en el espacio.

1.6.2. No estacionarios, cuando sus variables no son constantes en el tiempo o en el espacio.

1.7. Según sea la respuesta del sistema (valor de la salida) respecto a la variación de la entrada del sistema:

1.7.1. El sistema se considera estable cuando ante cualquier señal de entrada acotada, se produce una respuesta acotada de la salida.

1.7.2. El sistema se considera inestable cuando existe por lo menos una entrada acotada que produzca una respuesta no acotada de la salida.

1.8. Si se comparan o no, la entrada y la salida de un sistema, para controlar esta última, el sistema se denomina:

1.8.1. Sistema en lazo abierto, cuando la salida para ser controlada, no se compara con el valor de la señal de entrada o señal de referencia.

1.8.2. Sistema en lazo cerrado, cuando la salida para ser controlada, se compara con la señal de referencia. La señal de salida que es llevada junto a la señal de entrada, para ser comparada, se denomina señal de feedback o de retroalimentación.
1.9. Según la posibilidad de predecir el comportamiento de un sistema, es decir su respuesta, se clasifican en:

1.9.1. Sistema determinista, cuando su comportamiento futuro es predecible dentro de unos límites de tolerancia.

1.9.2. Sistema estocástico, si es imposible predecir el comportamiento futuro. Las variables del sistema se denominan aleatorias.
2. Naturales, incluyendo sistemas biológicos. Por ejemplo, los movimientos corporales humanos como el acto de indicar un objeto que incluye como componentes del sistema de control biológico los ojos, el brazo, la mano, el dedo y el cerebro del hombre. En la entrada se procesa el movimiento y la salida es la dirección hacia la cual se hace referencia.
3. Cuyos componentes están unos hechos por el hombre y los otros son naturales. Se encuentra el sistema de control de un hombre que conduce su vehículo. Este sistema está compuesto por los ojos, las manos, el cerebro y el vehículo. La entrada se manifiesta en el rumbo que el conductor debe seguir sobre la vía y la salida es la dirección actual del automóvil. Otro ejemplo puede ser las decisiones que toma un político antes de unas elecciones. Este sistema está compuesto por ojos, cerebro, oídos, boca. La entrada se manifiesta en las promesas que anuncia el político y la salida es el grado de aceptación de la propuesta por parte de la población.
4. Un sistema de control puede ser neumático, eléctrico, mecánico o de cualquier tipo, su función es recibir entradas y coordinar una o varias respuestas según su lazo de control (para lo que está programado).
5. Control predictivo, son los sistemas de control que trabajan con un sistema predictivo, y no activo como el tradicional ( ejecutan la solución al problema antes de que empiece a afectar al proceso). De esta manera, mejora la eficiencia del proceso contrarrestando rápidamente los efectos.


Los problemas considerados en la ingeniería de los sistemas de control, básicamente se tratan mediante dos pasos fundamentales como son:


Aquel el análisis se investiga las características de un sistema existente. Mientras que en el diseño se escogen los componentes para crear un sistema de control que posteriormente ejecute una tarea particular. 

Existen dos métodos de diseño:


La representación de los problemas en los sistemas de control se lleva a cabo mediante tres representaciones básicas o modelos:

Los diagramas en bloque y las gráficas de flujo son representaciones gráficas que pretenden el acortamiento del proceso correctivo del sistema, sin importar si está caracterizado de manera esquemática o mediante ecuaciones matemáticas.
Las ecuaciones diferenciales y otras relaciones matemáticas, se emplean cuando se requieren relaciones detalladas del sistema. Cada sistema de control se puede representar teóricamente por sus ecuaciones matemáticas. El uso de operaciones matemáticas es patente en todos los controladores de tipo Controlador proporcional (P), Controlador proporcional,Integral (PI) y Controlador proporcional,Integral y derivativo( PID), que debido a la combinación y superposición de cálculos matemáticos ayuda a controlar circuitos, montajes y sistemas industriales para así ayudar en el perfeccionamiento de los mismos.




</doc>
<doc id="22093" url="https://es.wikipedia.org/wiki?curid=22093" title="Programa espía">
Programa espía

El programa espía (en inglés "spyware") es un malware que recopila información de una computadora y después transmite esta información a una entidad externa sin el conocimiento o el consentimiento del propietario del computador.
El término "spyware" también se utiliza más ampliamente para referirse a otros productos que no son estrictamente "spyware". Estos productos, realizan diferentes funciones, como mostrar anuncios no solicitados, recopilar información privada, redirigir solicitudes de páginas e instalar marcadores de teléfono.

Un programa espía típico se autoinstala en el sistema afectado de forma que se ejecuta cada vez que se pone en marcha el ordenador (utilizando CPU y memoria RAM, reduciendo la estabilidad del ordenador), y funciona todo el tiempo, controlando el uso que se hace de Internet y mostrando anuncios relacionados.

Sin embargo, a diferencia de los virus, no se intenta replicar en otros ordenadores, por lo que funciona como un parásito.

Las consecuencias de una infección de un programa espía moderada o severa (aparte de las cuestiones de privacidad) generalmente incluyen una pérdida considerable del rendimiento del sistema (hasta un 50 % en casos extremos), y problemas de estabilidad graves (el ordenador se queda "colgado"). También causan dificultad a la hora de conectar a Internet.
Algunos ejemplos de programas espía conocidos son Gator o Bonzi Buddy.

Este nombre viene dado de las palabras en idioma inglés "spy" que significa espía, y "ware" significa programa.

La firma de seguridad informática Webroot publicó un listado del peor "spyware" del 2004 (el más peligroso y difundido), basado en la información recogida por su programa de rastreo Spy Audit. Estas son las principales amenazas:


Cuatro consejos: instale al menos dos de estas herramientas, úselas frecuentemente y actualice sus bases de datos por Internet (es un proceso similar al de los antivirus). 




</doc>
<doc id="22094" url="https://es.wikipedia.org/wiki?curid=22094" title="Iria Flavia">
Iria Flavia

Iria Flavia (también llamada Iria y oficialmente Santa María de Iria Flavia) es una parroquia y un lugar español del municipio de Padrón, en la provincia de La Coruña, Galicia.

Está ubicada en la confluencia del río Sar y el río Ulla y fue un puerto importante.

"Iria" fue una ciudad galaica, capital del país de los caporos del "conventus iuridicus Lucensis" de la provincia "Hispania Citerior Tarraconensis", situada en la vía de "Bracara Augusta" a "Asturica Augusta" por la costa. Bajo Vespasiano, a través del Edicto de Latinidad del año 74, se transformó en "municipium", y tomó el nombre de "Iria Flavia".

En la Hispania visigoda fue sede episcopal de la iglesia católica, sufragánea de la Archidiócesis de Braga que comprendía la antigua provincia romana de Gallaecia en la diócesis de Hispania. Así, fue sede episcopal desde el Bajo Imperio y con suevos y visigodos, hasta que Alfonso II trasladó el obispado a Santiago de Compostela (entonces conocida como Compostela) con motivo del hallazgo del sepulcro de Santiago el Mayor, apóstol.

Cuando el nombre de Padrón se hizo más popular, el crecimiento se trasladó al centro urbano padronés e Iria se convirtió en un simple caserío. En la actualidad se tiende a recuperar su nombre de "Iria Flavia" y es el que se utiliza oficialmente en todos los documentos. 

Según la tradición, en "Iria Flavia" predicó por primera vez el Apóstol Santiago durante su estancia en España. Aquí trajeron su cuerpo y su cabeza poco tiempo después, sus discípulos Teodoro y Atanasio desde Jerusalén y en una barca de piedra. Se cuenta que amarraron la barca a un "pedrón", y de ahí el topónimo actual de Padrón. Los dos discípulos (después de enterrar el cuerpo del apóstol) se quedaron a predicar en Iria Flavia. El "pedrón" se encuentra actualmente bajo el altar de la Iglesia de Santiago de Padrón, junto a las aguas del río Sar y en pleno centro urbano padronés; donde en tiempos remotos se situaba el famoso puerto fluvial de Padrón, ahora situado pocos kilómetros antes.

Entidades de población que forman parte de la parroquia:

La localidad es uno de los principales focos culturales e históricos de Galicia, con una importante oferta cultural. Destacan:


Pronto se unirán dos nuevos museos: el Museo de Arte Sacra de Padrón y el Museo de Historia de Padrón, que albergarán cartas, joyas, imágenes, obras... y multitud de documentos de importancia nacional. Se espera convertir con esto a Iria Flavia en uno de los principales focos de visitas a nivel nacional.

Ver: 





</doc>
<doc id="22096" url="https://es.wikipedia.org/wiki?curid=22096" title="Código morse">
Código morse

El código morse, también conocido como alfabeto morse o clave morse, es un sistema de representación de letras y números mediante señales emitidas de forma intermitente.

En 1837, Samuel Morse y Alfred Vail estaban trabajando en un sistema de telégrafo eléctrico. Decidieron usar un método por el cual cada símbolo era transmitido de forma individual como una combinación de rayas y puntos, es decir, señales telegráficas que se diferencian en el tiempo de duración de la señal activa. Morse desarrolló una primera versión de su código en 1837 para enviar números, que luego se debían convertir en mensajes completos usando un libro de claves. Este código fue expandido por Vail en 1841 para incluir letras y otros signos de puntuación, creando así el código actual. Morse reconoció la idoneidad de este sistema y lo patentó junto con el telégrafo eléctrico. Fue conocido como American Morse Code y utilizado en la primera transmisión por telégrafo.

La duración del punto es la mínima posible. Una raya tiene una duración de aproximadamente tres veces la del punto. Entre cada par de símbolos de una misma letra existe una ausencia de señal con duración aproximada a la de un punto. Entre las letras de una misma palabra, la ausencia es de aproximadamente tres puntos. Para la separación de palabras transmitidas el tiempo es de aproximadamente tres veces el de la raya. 

Toda correspondencia entre dos estaciones deberá comenzar con la señal de llamada. Para llamar, la estación que llama transmitirá el distintivo de llamada (no más de dos veces) de la estación requerida, la palabra DE seguida por su propia señal de llamada y la señal -. - a menos que haya reglas especiales peculiares al tipo de aparato utilizado.

En sus comienzos, el alfabeto Morse se empleó en las líneas telegráficas mediante los tendidos de cable que se fueron instalando. Más tarde, se utilizó también en las transmisiones por radio, sobre todo en el mar y en el aire, hasta que surgieron las emisoras y los receptores de radiodifusión mediante voz.

En la actualidad, el alfabeto Morse tiene aplicación casi exclusiva en el ámbito de los radioaficionados y escultistas, y aunque fue exigido frecuentemente su conocimiento para la obtención de la licencia de radioperador aficionado hasta el año 2005, posteriormente, los organismos que conceden esa licencia en todos los países están invitados a dispensar del examen de telegrafía a los candidatos.

También se utiliza en la aviación instrumental para sintonizar las estaciones VOR, ILS y NDB. En las cartas de navegación está indicada la frecuencia junto con una señal Morse que sirve, mediante radio, para confirmar que ha sido sintonizada correctamente.

Pulsa en los enlaces para oír el sonido.

convenciones: — : raya (señal larga) · : punto (señal corta)

"Si se comete un error al transmitir el mensaje en morse, la señal "error" son seis ecos "E" en grupos de dos (../../..):

Para facilitar el aprendizaje del código morse, se suele utilizar una regla mnemotécnica que permite aprendérselo mediante un código consistente en asignar a cada letra una palabra clave determinada, que comienza con la letra que se quiere recordar. Luego, basta con sustituir cada vocal de la palabra clave por un "punto" o una "raya" según la siguiente regla:


Otra regla para mejorar el aprendizaje del código morse, recurre a la fuerte presencia que tienen las imágenes de las letras. A fin de ser el recurso que ayuda a la memoria. En las siguientes letras, se han marcado con color los puntos y líneas que corresponden a su respectivo código en morse.




</doc>
<doc id="22100" url="https://es.wikipedia.org/wiki?curid=22100" title="Pasteurización">
Pasteurización

La pasteurización o pasterización es un proceso térmico que es realizado en líquidos (generalmente alimentos) con la intención de reducir la presencia de agentes patógenos (como por ejemplo ciertas bacterias, protozoos, mohos, levaduras, etc.) que puedan contener. Debido a las altas temperaturas (80 grados) la gran mayoría de los agentes bacterianos mueren. Proceso descubierto por el científico químico francés Louis Pasteur, junto a Claude Bernard el 20 de abril de 1864.

Uno de los motivos del tratamiento térmico es un método de control de microorganismos de los alimentos líquidos, alterando lo menos posible su estructura física, sus componentes químicos y sus propiedades organolépticas. Tras la operación de pasteurización, los productos tratados se enfrían rápidamente y se sellan herméticamente con fines de seguridad alimentaria; por esta razón, es básico en la pasteurización el conocimiento del mecanismo de la transferencia de calor en los alimentos. A diferencia de la esterilización, la pasteurización no destruye completamente las esporas de los microorganismos, ni elimina todas las células de microorganismos termofílicos.

Louis Pasteur mejoró la calidad de vida al hacer posible que productos alimenticios básicos, como la leche, se pudieran transportar largas distancias sin ser afectados por la descomposición. En la pasteurización, el objetivo primordial no es la «eliminación completa de los agentes patógenos» sino la disminución sustancial de sus poblaciones, reduciéndolas a niveles que no causen intoxicaciones alimentarias a los humanos (siempre que el producto pasteurizado se mantenga refrigerado correctamente y que se consuma antes de la fecha de caducidad indicada).

Los primeros procesos para esterilizar alimentos en envases cerrados, se han atribuido históricamente al inventor francés Nicholas Appert en sus investigaciones realizadas en el siglo XVIII. No obstante, algunas investigaciones demuestran que con anterioridad ya se había intentado esterilizar alimentos en recipientes sellados. Hacia finales de siglo XIX, químicos alemanes trasladaron este procedimiento a la leche cruda, y ya por entonces (antes de Pasteur) se empezó a «sospechar» que los tratamientos térmicos resultaban eficaces para destruir las bacterias presentes en la leche. De este modo, se dio origen no solo a un importante método de conservación, sino también a una medida de higiene fundamental para proteger la salud de las personas y conservar la calidad de los alimentos. Estos trabajos sentaron las bases de lo que Pasteur posteriormente descubriría y explicaría científicamente.

Algunos de los contemporáneos de Pasteur, incluido el eminente químico alemán Justus von Liebig, insistían en que la fermentación era un proceso puramente químico y que no requería en absoluto de la intervención de ningún organismo vivo. En el año 1864, a instancias del emperador Napoleón III, Pasteur investigó la causa por la que el vino y la cerveza se agriaban con el paso del tiempo, causando grandes pérdidas económicas a las empresas francesas debido a lo perecedero de estas mercancías. Pasteur regresó al pueblo de su infancia, Arbois, con el objetivo de resolver el problema definitivamente. Allí estudió el problema que afectaba a las viñas. Con ayuda de un microscopio, descubrió que, en realidad, intervenían dos tipos de organismos —una levadura y una bacteria de la familia "acetobacter"— que eran la clave del proceso de fermentación. Uno producía alcohol y el otro ácido acético, que agriaba el vino produciendo el vinagre.

Pasteur utilizó un nuevo método para eliminar los microorganismos que pudieran degradar el vino o la cerveza: después de almacenar el líquido en cubas bien selladas se elevaba su temperatura hasta los 44 °C durante un breve periodo de tiempo. Comprobó experimentalmente que las poblaciones de bacterias del género "Acetobacter" se reducían en extremo hasta quedar «casi esterilizado» el alimento. A pesar del horror inicial de la industria ante la idea de calentar el vino, un experimento controlado con lotes de vino calentado y sin calentar demostró de forma contundente la efectividad del procedimiento. Con posterioridad, Charles North aplicó con éxito el mismo método de Pasteur a la leche en el año 1907. Pasteur dio el primer paso en el que sería este nuevo método, denominado posteriormente «pasteurización» en su honor, y lo fue aplicando a otros alimentos líquidos. Este proceso se aplica hoy en día como norma de higiene en muchos procesos básicos de la industria alimentaria y proporciona una garantía de la seguridad de muchos productos alimenticios de manera eficaz en todo el mundo.

La historia de la esterilización de los alimentos fue revisada por Harold Burton (1988). Los esterilizadores fueron patentados y construidos para calentar leche a temperaturas que van desde los 54,4 °C hasta los 60 °C antes del siglo XIX, curiosamente antes de que sus beneficios fueran entendidos completamente. La leche esterilizada se desarrolló industrialmente en el año 1921, y el proceso de inyección de vapor fue desarrollado en 1927 por G. Grindrod en Estados Unidos. Sin embargo, las iniciativas más relevantes que dieron lugar a la comercialización del método UHT se empezaron a desarrollar a fines del decenio de 1940, debido a la técnica desarrollada en los esterilizadores de tubos concéntricos y de vapor de uperización en los sistemas de producción de leche. Debe entenderse que los esfuerzos de aquella época eran muy grandes en la industria para lograr envasar asépticamente la leche, hasta que finalmente se logró con éxito en el año 1961.

La pasteurización es un proceso térmico químico realizado a los alimentos: los procesos térmicos se pueden realizar con la intención de disminuir las poblaciones patógenas de microorganismos o para desactivar las enzimas que modifican los sabores de ciertos alimentos. No obstante, en la pasteurización se emplean generalmente temperaturas por debajo del punto de ebullición (en cualquier tipo de alimento), ya que en la mayoría de los casos las temperaturas superiores a este valor afectan irreversiblemente ciertas características físicas y químicas del producto alimenticio. Así, por ejemplo, si en la leche se sobrepasa el punto de ebullición, las micelas de la caseína se «coagulan» irreversiblemente (o dicho de otra forma, la leche se «cuaja»). El proceso de calentamiento de la pasteurización, si se hace a bajas temperaturas, tiene además la función de detener los procesos enzimáticos. Hoy en día, la pasteurización se realiza a los alimentos en un proceso industrial continuo aplicado a alimentos viscosos, con la intención de utilizar la energía de manera eficiente y disminuir así también costes de producción.

Existen tres tipos de procesos bien diferenciados: pasteurización VAT o lenta, pasteurización a altas temperaturas durante un breve período (HTST, High Temperature/Short Time) y proceso a altas temperaturas (UHT, Ultra-High Temperature).

Del inglés "vat" = tina, tinaja, por hacerse en recipientes grandes. Llamada también pasteurización lenta. Fue el primer método de pasteurización, aunque la industria alimentaria lo ha ido renovando por otros sistemas más eficaces. El proceso consiste en calentar grandes cantidades de leche en un recipiente estando a 63 °C durante 30 minutos, para luego dejar enfriar lentamente. Debe pasar mucho tiempo para continuar con el proceso de envasado del producto, a veces más de 24 horas.

Este método es el empleado en los líquidos a granel, como la leche, los zumos de fruta, la cerveza, etc. Por regla general, es el más práctico, ya que expone al alimento a altas temperaturas durante un período breve y además se necesita poco equipamiento industrial para poder realizarlo, reduciendo de esta manera los costes de mantenimiento de equipos. Entre las desventajas del proceso está la necesidad de contar con personal altamente calificado para la realización de este trabajo, que necesita controles estrictos durante todo el proceso de producción.

Existen dos métodos distintos bajo la categoría de pasteurización HTST: en "batch" (lote) y en «flujo continuo». Para ambos métodos la temperatura es la misma (72 °C durante 15 segundos).

El proceso UHT es de flujo continuo y mantiene la leche a una temperatura superior más alta que la empleada en el proceso HTST, y puede rondar los 138 °C durante un período de al menos dos segundos. Debido a este muy breve periodo de exposición, se produce una mínima degradación del alimento. La leche cuando se etiqueta como «pasteurizada» generalmente se ha tratado con el proceso HTST, mientras que la leche etiquetada como «ultrapasteurizada» o simplemente UHT, se debe entender que ha sido tratada por el método UHT.

El reto tecnológico del siglo XXI es poder disminuir lo más posible el período de exposición a altas temperaturas de los alimentos, haciendo la transición de altas a bajas temperaturas lo más rápida posible, disminuyendo el impacto en la degradación de las propiedades organolépticas de los alimentos; por esta razón, se está investigando la tecnología basada en microondas, que permite este tipo de efectos (es empleado incluso en carnes). Este método es muy adecuado para los alimentos líquidos ligeramente ácidos (la acidez se mide con el pH), tal como los zumos de frutas y los zumos de verduras (como el gazpacho), ya que permite períodos de conservación de 10 a 45 días si se almacenan refrigerados a 10 °C.

Los métodos de pasteurización corresponden a una serie de métodos estandarizados por los responsables de alimentación de cada país y son controlados por las agencias encargadas de vigilar la calidad de la alimentación (algunos ejemplos son la USDA en Estados Unidos y la Food Standards Agency en el Reino Unido) mediante la implementación de un derecho alimentario específico. Estas agencias requieren y vigilan que, por ejemplo, los lácteos pasteurizados mediante HTST lleven la etiqueta alimentaria adecuada. Por regla general existen diferentes estándares en función de los lácteos a procesar. El principal factor a tener en cuenta es el contenido graso del producto. De esta forma, los parámetros de pasteurización de la nata difieren de los parámetros empleados para la leche desnatada, y los parámetros para pasteurizar queso se diseñan e implementan de tal forma que no se destruyan las enzimas que procesan los fosfatos, útiles para mantener las propiedades de corte y textura de los quesos.

Los métodos estándares de pasteurización HTST han sido designados para alcanzar una extensión del periodo de caducidad de cerca de 5 días (es decir 0,00001 veces el período original) reduciendo el número de microorganismos en la leche y otros alimentos. Este método es considerado adecuado para la reducción de poblaciones de células vegetativas de casi todas las bacterias patógenas, incluyendo aquellas bacterias resistentes a las altas temperaturas (particularmente las especies "Mycobacterium tuberculosis", causante de la tuberculosis, y "Coxiella burnetii", causante de la fiebre Q en la leche). El proceso de pasteurización HTST no elimina las esporas bacterianas debido a que el proceso emplea un régimen temperatura-tiempo de 75°C por 15 segundos, siendo insuficiente para la reducción de esporas bacterianas, debido a su alta resistencia frente al calor, requiriéndose normalmente temperaturas mayores a 100°C para que el tiempo de exposición sea relativamente corto y evitar daños en los componentes nutricionales y sensoriales de los alimentos. Sin embargo, el proceso se diseña de manera que los productos sean calentados uniformemente, evitando que mientras que algunas partes sean sometidas a excesivas temperaturas durante demasiado tiempo, otras no lleguen a los parámetros necesarios.

La pasteurización es un proceso que sigue una cinética química de primer orden. Denominamos N al número de microorganismos vivos a una temperatura dada de exposición T, y N a la población de microorganismos inicialmente. Si K es la constante cinética de muerte debido a la temperatura (velocidad de muerte de los microorganismos), la disminución en la población (cultivo) depende de la siguiente fórmula exponencial:

formula_1

Esta fórmula es fundamental para determinar la evolución de un cultivo en función de la temperatura. Se puede ver en ella una gran dependencia con la temperatura de exposición T. La fórmula es el fundamento, además, de los denominados «diagramas de supervivencia» en la industria de la alimentación, donde log(N/N) es el tiempo de exposición a una temperatura T fija. Típicamente las gráficas de supervivencia de los microorganismos al calor aparecen como líneas rectas en una escala semilogarítmica. La correlación existente entre la velocidad (o ratio) de muerte de microorganismos y la temperatura cumple la ecuación de Arrhenius.

Un factor importante asignado a cada microorganismo es el denominado «tiempo de reducción decimal» o también «valor D» de un microorganismo, y se define como el tiempo necesario para que a una temperatura determinada se pueda reducir el 90 % su población en el producto tratado. Es una expresión de la resistencia de un microorganismo al efecto de la temperatura. Su expresión es:

formula_2

Donde formula_3 es el período al que se expone la muestra, N es la población inicial y N la población final. Pueden obtenerse diferentes valores D para un microorganismo dado, o para un proceso particular de un alimento, determinando los sobrevivientes a diferentes temperaturas. Altos valores de D indican que el microorganismo es más resistente que otros que poseen un valor inferior. Existen otros valores como la constante de resistencia térmica, conocida frecuentemente como valor z, que se define como la diferencia en temperaturas necesaria para causar una reducción de un 90 % en el valor D. Esta pasteurización elimina en un 98 % de bacterias como "Vibrio cholerae", "Shigella" o "E. coli"..

La acidez tiene mucha influencia en el grado de supervivencia de cada organismo bacteriano. El principal parámetro para caracterizar la acidez es el pH. En general la mayoría de alimentos se consideran ácidos o poco ácidos. Hay que considerar que la mayoría de las bacterias tóxicas como las de la especie "Clostridium botulinum" ya no están activas por debajo de un valor de pH de 4,5 (es decir que un simple zumo de limón las desactiva). Los alimentos se pueden considerar como ácidos si están por debajo de este valor de pH. La mayoría de los glúcidos se encuentran en este rango, sobre todo los monosacáridos. En el caso de alimentos con un pH superior, es necesario un tratamiento térmico de 121 °C durante tres minutos (o un proceso equivalente) como procesamiento mínimo (es decir, la leche, las verduras, las carnes, el pescado, etc.). No obstante, muchos de estos alimentos se convierten en ácidos cuando se les añade vinagre, zumo de limón, etc., o simplemente fermentan cambiando su valor de acidez. La causa de este efecto reside en la desactivación de la actividad microbiana debida a la simple influencia que posee por el valor de la acidez, indicada por el pH, sobre la condición de vida de estos microorganismos.

Algunos organismos y bacterias cultivados en los alimentos son resistentes a la pasteurización, como los bacilos de las especies "Bacillus cereus" (pudiendo llegar a prosperar cultivos de éstos incluso a bajas temperaturas), y "Geobacillus stearothermophilus". No obstante la resistencia a la eliminación térmica depende en gran medida del pH, actividad acuosa, o simplemente de la composición química de los alimentos, la facilidad o probabilidad de volver a ser contaminados (en lo que se denomina en inglés "postprocessing contamination", o PPC)

Mencionar la forma como un factor a tener en cuenta en la pasteurización del alimento es equivalente a decir que lo que influye es la superficie exterior del alimento. Cabe pensar que el principal objetivo del proceso de pasteurización es el incremento de la razón entre la capacidad de enfriamiento y la superficie del mismo. De esta forma, el peor ratio corresponde a los alimentos similares a una esfera. En el caso de los alimentos líquidos, se procura que tengan formas óptimas para que la variación de temperatura, tanto en calentamiento como en enfriamiento, pueda obtener ratio óptimo.

Algunas propiedades térmicas del alimento afectan de forma indirecta al rendimiento final de la pasteurización sobre el mismo, como la capacidad calorífica (la cantidad de energía que hay que «inyectar» por unidad de masa de alimento para que suba de temperatura), la conductividad térmica (garantiza la homogeneidad del proceso en el alimento), la inercia térmica (los alimentos con menor inercia térmica son más susceptibles de ser pasteurizados que los que poseen mayor inercia).

Desde sus orígenes, la pasteurización se ha asociado con la leche. El primer investigador que sugirió este proceso para el producto lácteo fue el químico agrícola alemán Franz von Soxhlet en el año 1886, siendo Charles North quien aplicó dicho método a la leche por primera vez en el año 1907. Los microorganismos activan sus poblaciones creciendo de forma óptima en el intervalo de temperatura de 25 °C a 37 °C. Por esta razón, durante el proceso de manufacturación y envasado de la industria láctea se evita que la temperatura de la leche esté en este intervalo después de la pasteurización. La leche es por regla general un medio ligeramente ácido con un pH menor que 7 (6,7). La leche de vaca pasteurizada por el método HTST y que ha sido correctamente refrigerada tiene un periodo de caducidad extendido que puede llegar a dos o tres semanas, mientras que la leche ultra pasteurizada puede tener una vida extendida que oscila entre dos y tres meses. Se puede llegar a períodos de conservación mayores (incluso sin refrigeración) cuando se combina la pasteurización UHT con manipulación adecuada y tecnologías de envases esterilizados. Al mismo tiempo que se reducen las colonias, se eliminan también de la leche los microorganismos más termosensibles, como los coliformes, inactivándose la fosfatasa alcalina (el nivel de esta enzima define el grado de eficiencia aplicado a la pasteurización de la leche; véase test de la fosfatasa). A pesar de aplicar la pasteurización, la leche tratada sigue conteniendo una cierta actividad microbiana, por regla general bacterias lácticas (no patógenas, aunque sí capaces de hacer fermentar la leche) y es necesaria la refrigeración.

Consumir leche cruda de animales, sin pasteurizar, expone a ciertos riesgos de contacto con organismos y bacterias causantes de enfermedades. En algunos países se ha llegado a prohibir su venta. Algunas de las enfermedades evitadas con la pasteurización de la leche son la tuberculosis ("Mycobacterium tuberculosis"), la difteria, la polio, la salmonelosis, la fiebre escarlata,la brucelosis y las fiebres tifoideas. Hoy en día, muchas de estas enfermedades no tienen una gran relevancia debido al empleo generalizado de los procesos de pasteurización en las primeras etapas de manipulación de la leche.

Entre las especies de organismos cuyas poblaciones se pueden reducir considerablemente con la pasteurización de la leche se cuentan los siguientes:

La pasteurización de la leche ha sido objeto poco a poco de una polémica creciente. Por una parte, se ha descubierto que algunos organismos patógenos han desarrollado una resistencia a la disminución de población con la temperatura, consiguiendo sobrevivir a la pasteurización en cantidades significativas.. Se ha detectado que la pasteurización en ciertas condiciones incorrectas destruye la vitamina A y la vitamina B pero estas condiciones no se producen en los procesos actuales de pasteurización.

Los zumos envasados (e incluso los néctares) se someten a dos tipos diferentes de procesos de pasteurización: por un lado existen los zumos sin procesar ("crudos"); por otro, los zumos ultrapasteurizados o zumos estériles.

Los productores de zumos están familiarizados con los procesos de pasteurización y con ambos métodos: el VAT o proceso "batch" (empleado en los productores de pequeño tamaño de producción) y el UHT (empleado en los productores de mayor producción). El método HTST es aceptado en la industria, ya que no produce una degeneración apreciable del sabor. La pasteurización es muy efectiva en los zumos debido a que son medios ácidos y evitan la proliferación de microorganismos esporulados, los más resistentes a las altas temperaturas. En muchos países, como Estados Unidos, el 95 % de los zumos comercializados son pasteurizados. En algunas ocasiones se exige por parte de los organismos encargados de la vigilancia e higiene alimentaria que se le indique al consumidor que está tomando un «zumo crudo». Los zumos suelen ser tratados térmicamente por el método de pasteurización a 70 °C durante 30 minutos, pero la temperatura ideal en función del pH es en la actualidad objeto de investigación.

Dependiendo de su origen, los zumos contienen diversos microorganismos y es necesario reducir la concentración total de sus poblaciones mediante la pasteurización. De esta forma, se sabe que el zumo de manzana puede contener las especies "Salmonella typhimurium", "Cryptosporidium" y "Escherichia coli". En el zumo de naranja es habitual encontrar las especies "Bacillus cereus", "Salmonella typhi" y "Salmonella hartford". En algunos zumos de verduras, generalmente en los zumos poco ácidos, como el zumo de zanahoria, existe un riesgo particular de permanencia de la especie "Clostridium botulinum".

Los zumos pueden sufrir alteraciones en su color y tienden al marrón debido al deterioro enzimático de la polifenoloxidasa. Esto obedece en parte a la presencia de oxígeno en el líquido. Por ello, a los zumos y los néctares se les suele eliminar el aire antes de comenzar el proceso de pasteurización. De la misma forma, la pérdida de vitamina C y de caroteno se ve disminuida mediante desaireación previa. Dado que en el proceso se pierden estos nutrientes, en muchos casos se suelen reincorporar de manera artificial (zumos enriquecidos).

Se ha descubierto que ciertas poblaciones de la especie "Mycobacterium avium" (pertenecientes a la subespecie "M. avium paratuberculosis"), causante de la enfermedad de Johne en los animales de sacrificio —y se sospecha que también de la enfermedad de Crohn en los humanos—, han sobrevivido a pasteurizaciones de ciertos alimentos lácteos en los Estados Unidos, el Reino Unido, Grecia y la República Checa. A la vista de la supervivencia de ciertas especies además de la anterior, las autoridades del Reino Unido encargadas de vigilar la calidad de los alimentos decidieron revaluar los estándares de pasteurización.

Un método actual es la pasteurización flash o instantánea, que utiliza menores tiempos de exposición a altas temperaturas y parece ser un método adecuado para conservar las propiedades organolépticas de los alimentos, pues preserva mejor el sabor y la textura de los mismos. La pasteurización fría es una denominación usada a veces como sinónimo de radiación ionizante (véase irradiación de alimentos) u otros significados (por ejemplo, químicos) para reducir las poblaciones de bacterias en los alimentos. La irradiación de alimentos también se denomina a veces «pasteurización electrónica». Se ha investigado la posibilidad de extender la pasteurización a alimentos no fluidos, como la carne de ternera. Un avance en la pasteurización no intrusiva que soluciona muchos problemas de la industria conservera es la denominada pasteurización electromagnética de alimentos líquidos, que emplea microondas a 2,45 GHz de frecuencia para activar los procesos térmicos. Este método ha demostrado su eficiencia en la pasteurización del agua.

Existen estudios orientados al Tercer Mundo en los que es posible realizar lo que se denomina pasteurización solar. La idea está fundamentada en la cocina solar y en el hecho de que no es necesario llevar los líquidos a ebullición para lograr la pasteurización, pudiendo pasteurizar con este método con temperaturas sobre los 56 °C. Con esta medida se intenta prevenir la causa de enfermedades causadas por la ingesta de aguas contaminadas. El método es conocido como «pasteurización del agua», en el que se han desarrollado ciertos elementos capaces de indicar el estado de pasteurización del agua y su posibilidad de ingesta segura. Uno de los más empleados es el "water pasteurization indicator" (WAPI). La pasteurización solar requiere exponer el agua en recipientes durante seis horas. El programa que se aplicó en ciertas regiones de África se denominó SODIS (abreviación de "solar disinfection").

Aparte de la leche y los zumos, otros alimentos son pasteurizados por la industria alimenticia; por regla general, son aquellos que poseen una estructura líquida o semilíquida. Algunos de los más mencionados son los siguientes:





</doc>
<doc id="22102" url="https://es.wikipedia.org/wiki?curid=22102" title="Alfabeto por palabras">
Alfabeto por palabras

Un alfabeto por palabras es un conjunto de palabras de las que cada una representa a una letra del alfabeto. Por lo tanto, el sistema consiste en tantas palabras como letras tiene el alfabeto, y cada una de estas palabras comienza con una letra del alfabeto. 

Los alfabetos de palabras se usan para la comunicación entre dos o más personas por radio o por teléfono en los casos en los que es importante que no se produzcan errores en la comprensión de datos o mensajes. Los alfabetos por palabras se usan de forma generalizada en todo tipo de actividades, y muy especialmente en la navegación marítima y aérea.

Existen múltiples alfabetos por palabras en los más diversos idiomas. El más conocido es, el que estableció la Organización Internacional de Aviación Civil, y después el alfabeto fonético de la OTAN que es básicamente una copia del anterior.

Los siguientes ejemplos proceden de varias lenguas.


</doc>
<doc id="22110" url="https://es.wikipedia.org/wiki?curid=22110" title="Niobio">
Niobio

El niobio es un elemento químico de número atómico 41 situado en el grupo 5 de la tabla periódica de los elementos. Se simboliza como Nb. Es un metal de transición dúctil, gris, blando y poco abundante. Se encuentra en el mineral niobita, también llamado columbita, y se utiliza en aleaciones. Se emplea principalmente aleado en aceros, a los cuales confiere una alta resistencia.

El niobio tiene propiedades físicas y químicas similares a las del elemento tantalio, y los dos son, por lo tanto, difíciles de distinguir. El químico inglés Charles Hatchett informó de un nuevo elemento similar al tántalo en 1801 y lo llamó columbio. En 1809, el químico William Hyde Wollaston inglés concluyó erróneamente que el tántalo y el columbio eran idénticos. El químico alemán Heinrich Rose determinó en 1846 que los minerales de tántalo contenían un segundo elemento, que él nombró niobio. En 1864 y 1865, una serie de descubrimientos científicos clarificó que el niobio y el columbio eran el mismo elemento (a diferencia de tantalio), y desde hace un siglo se utilizaron ambos nombres indistintamente. El niobio fue adoptado oficialmente como el nombre del elemento en 1949, pero el nombre de columbio sigue siendo de uso corriente en la metalurgia en los Estados Unidos.

No fue hasta el siglo XX que el niobio fue utilizado por primera vez en el mercado. Brasil es el principal productor de niobio y ferroniobio (una aleación de niobio y hierro). El niobio se utiliza sobre todo en aleaciones, la mayor parte en acero especial igual que el utilizado en tuberías de petróleo y gas. Aunque las aleaciones contienen sólo un máximo de 0,1 %, este pequeño porcentaje de niobio mejora la resistencia del acero. El niobio se utiliza en diversos materiales superconductores. Estas aleaciones superconductoras, también contienen titanio y estaño, que son ampliamente utilizados en los imanes superconductores de escáneres de resonancia magnética. Otras aplicaciones de niobio incluyen su uso en soldadura, industrias nucleares, electrónica, óptica, numismática y joyería. En las dos últimas aplicaciones, su bajo nivel de toxicidad de niobio y su capacidad de ser coloreado por anodización son ventajas particulares. El niobio es un componente importante de los catalizadores de alto rendimiento para la oxidación selectiva de propano a ácido acrílico.

El niobio es un metal gris, dúctil, y paramagnético que se encuentra en el grupo 5 de la Tabla Periódica. Aunque en comparación con el resto de los miembros. Tiene una configuración atípica en sus capas de electrones más externos.
El niobio se convierte en un superconductor a temperaturas criogénicas. A presión atmosférica, que tiene la temperatura crítica más alta de los superconductores elementales, el niobio tiene mayor profundidad de penetración magnética que cualquier elemento. Además, es uno de los superconductores de tipo tres elemental II, junto con vanadio y tecnecio. Las propiedades superconductoras son fuertemente dependientes de la pureza del niobio metal. Cuando es muy puro, es relativamente más blando y dúctil, pero las impurezas hacen que sea más duro. 
El metal tiene una baja sección transversal para los neutrones térmicos; por lo que se utiliza en las industrias nucleares.

El metal adquiere un tinte azulado cuando se expone al aire a temperatura ambiente durante largos períodos de tiempo. A pesar de presentar un alto punto de fusión, en forma elemental (2468 °C), tiene una baja densidad en comparación con otros metales refractarios. Además, es resistente a la corrosión, presenta propiedades de superconductividad, y forma capas de óxido dieléctrico. 

El niobio es un poco menos electropositivo y más compacto que su predecesor en la tabla periódica, el circonio, mientras que es prácticamente idéntica en tamaño a los átomos del tantalio más pesados, debido a la contracción de los lantánidos.
Como resultado, las propiedades químicas del niobio son muy similares a las del tantalio, que aparece directamente debajo del niobio en la tabla periódica. Aunque su resistencia a la corrosión no es tan notable como la del tántalo, el niobio tiene un precio más bajo y una mayor disponibilidad y esto lo hace atractivo para usos menos exigentes, como pueden ser revestimientos en plantas químicas.

Se estima que el niobio es el 33 º elemento más común en la superficie de la Tierra, con 20 ppm. Algunos piensan que la abundancia en la Tierra es mucho mayor, pero que el niobio "perdido" puede estar situado en el núcleo de la Tierra debido a la alta densidad del metal. El elemento no se encuentra libre en la naturaleza, pero el niobio se produce en combinación con otros elementos minerales. Los minerales que contienen niobio a menudo también contienen tántalo. Los ejemplos incluyen la columbita [(Fe, Mn)NbO] y coltan [(Fe, Mn)(Nb, Ta)O]. Los minerales columbita-tantalita (como el coltán) se encuentran generalmente como minerales accesorio en las intrusiones graníticas de pegmatita y en rocas intrusivas alcalinas. Menos comunes son los niobatos de calcio, el uranio, el torio y los elementos de tierras raras. Ejemplos de tales son el pirocloro [(Na, Ca)NbO(OH, F)] y la euxenita [(Y, Ca, Ce, U, Th)(Nb, Ta, Ti)O]. Estos grandes depósitos de niobio se han encontrado asociados con carbonatitas (rocas ígneas carbonatosilicatadas ) y como componente de pirocloro.

Los soldadores utilizan el niobio para ligar los componentes de acero inoxidable. Además, los fabricantes de acero agregan pequeñas cantidades de un compuesto de niobio-hierro conocido como ferroniobio para aumentar la fortaleza de sus productos, así como la resistencia a las temperaturas y a la corrosión. El acero combinado con niobio es utilizado ampliamente en las industrias aeroespacial, química, de energía eléctrica y automotriz.

En aleación con titanio, se puede extrusionar el niobio en un alambre superconductor que luego se puede moldear para formar imanes que no pierden la superconductividad al ser colocados en campos magnéticos externos. También existen aleaciones superconductoras de estanio-niobio y aluminio-niobio. Los metales encuentran su uso en giroscopios para navegación aeroespacial, así como para artefactos de imágenes por resonancia magnética.

Aceleradores de partículas: Los investigadores de la física de alta energía usan algunos aceleradores de electrones que incluyen cámaras moldeadas de niobio puro o aleado. Cuando se enfrían a una temperatura cercana al cero absoluto, estas cámaras de niobio se vuelven altamente magnéticas y superconductoras, lo cual permite a los investigadores aumentar la velocidad de las partículas sub-atómicas sin usar cantidades crecientes de electricidad.

Revestir el cristal con un finísimo polvo de niobio mejora la habilidad del mismo para difundir la luz sin absorberla ni refractarla. El revestimiento también hace que el cristal sea más resistente al reflejo. El cristal revestido con niobio tiene sus aplicaciones en lentes de cámaras, así como en pantallas de televisores y monitores. El niobio también se utiliza como capa protectora para condensadores cerámicos.

Una aleación de niobio-zirconio sirve como materia prima para la base metálica de algunas lámparas de vapor de sodio. La aleación soporta las altas temperaturas que alcanza la lámpara y no se vuelve frágil con el uso prolongado.

En su estado natural, el niobio tiene un color plateado mate. Cuando el elemento puro se calienta o es pasado a través de un campo eléctrico, sin embargo, puede tomar muchos colores, yendo desde el azul al verde y del dorado al rojo. Esta propiedad ha hecho del niobio una opción con creciente popularidad para los joyeros que desean crear aros, tachas, broches, pendientes y prendedores de metal coloreado.

Dado a su propiedad de cambiar su color mediante el tratamiento de sus capas superficiales, también se emplea en la fabricación de monedas bimetálicas: 25 euros de Austria y 1 Lat en Letonia.

Es denominado niobio en honor de Níobe, hija de Tántalo. Hatchett lo llamó columbio, pero creyó haberlo confundido con el tantalio; el nombre columbio se utiliza todavía en algunos lugares. En 1844, H. Rose lo redescubrió y bautizó con el nombre actual. El nombre de niobio se adoptó por la IUPAC en 1950, 100 años después de que surgiera la controversia; a pesar de todo, la mayoría de los químicos lo llaman niobio, pero muchos relacionados con la metalurgia del elemento (anglosajones) lo siguen llamando columbio.

La mina más grande de niobio en el mundo se encuentra en Araxá, Minas Gerais, Brasil, propiedad del también productor más grande de Ferroniobio (FeNb) en el mundo, así como de otros productos derivados del niobio, la compañía se llama CBMM (Companhia Brasileira de Metalurgia e Mineraçao) 

Blomstrand lo preparó por primera vez en 1864 por reducción: calentando el cloruro en atmósfera de hidrógeno. Hasta 1905 no se obtuvo puro (Bolton).
Nunca se encuentra en estado elemental y casi siempre aparece acompañado de tántalo. Representa el 2·10% en peso de la corteza. 

Las principales fuentes minerales son: niobita (o columbita), niobita-tantalita y euxenita o policrasa [(Y,Ce,Er,U,Th,Ca..)(Nb,Ta,Ti,Fe)O]. Otros minerales que lo contienen son: samarskita ((Y,Er,Ca,Fe,Mn,Sn,W,U,Ce)[(Nb,Ta)O]). fergusonita [(Nb,Ta)YO]. Grandes cantidades de niobio se han encontrado asociadas con rocas silicocarbonatadas (carbonatitas).

La obtención del metal implica una primera etapa de separación del tántalo mediante disolventes y la transformación en Nb2O5. Este se reduce en dos etapas con carbón; en la primera, a 800 °C, se forma NbC, que en la segunda, a 2000 °C, actúa como reductor del óxido y se produce el metal.


</doc>
<doc id="22117" url="https://es.wikipedia.org/wiki?curid=22117" title="Centro de Investigación Científica y de Educación Superior de Ensenada">
Centro de Investigación Científica y de Educación Superior de Ensenada

El Centro de Investigación Científica y de Educación Superior de Ensenada, Baja California, (CICESE), fue creado en 1973 por el gobierno federal como parte de la iniciativa para descentralizar las actividades científicas y modernizar el país. El CICESE pertenece al sistema de centros públicos de investigación del Consejo Nacional de Ciencia y Tecnología (CONACyT) y a lo largo de más de cuatro décadas, ha evolucionado hasta convertirse en uno de los principales centros científicos de México. 

El CICESE es una institución de referencia en el contexto científico nacional e internacional, su excelencia académica apoya el desarrollo nacional, la formación de recursos humanos y contribuye a generar el conocimiento que puede coadyuvar en la solución de problemas que afectan el entorno social y económico de México.

Las actividades de investigación y docencia se han consolidado en las cuatro divisiones que conforman la institución: Biotecnología Experimental y Aplicada, Ciencias de la Tierra, Física Aplicada y Oceanología, cuyas áreas de investigación incluyen acuicultura, biología, biotecnología, climatología, computación, ecología, electrónica, geociencias ambientales, geofísica, geología, instrumentación, meteorología, microbiología, oceanografía biológica, oceanografía física, óptica, optoelectrónica, sismología, tecnologías de la información, telecomunicaciones y telemática.

Cuenta con una plantilla de 176 investigadores y 197 técnicos altamente especializados; sus programas de posgrado pertenecen al Padrón Nacional de Posgrado y están orientados completamente a la investigación; la infraestructura material incluye ocho modernos edificios que albergan laboratorios, aulas, una biblioteca especializada, equipo de supercómputo, conectividad a Internet 2, el buque oceanográfico "Alpha Helix" y valiosas redes de instrumentación sismológica y oceanográfica.

Actualmente el CICESE es un Centro Público de Investigación el más grande de los 27 que integran el Sistema de Centros Públicos de Investigación del Conacyt. Tras una reestructuración convenida en un nuevo decreto publicado en el Diario Oficial de la Federación del 29 de agosto de 2000, las actividades de investigación, docencia y vinculación del CICESE se concentran en ciencias biológicas, físicas, de la información, del mar y de la Tierra, dentro de un marco de responsabilidad, ética y liderazgo en beneficio de la sociedad. Con más de cuarenta años de experiencia, el CICESE es una institución de excelencia académica que apoya el desarrollo nacional y la formación de recursos humanos, y contribuye a la generación del conocimiento.

El Centro de Investigación Científica y de Educación Superior de Ensenada (CICESE) fue la segunda institución creada por el Consejo Nacional de Ciencia y Tecnología (CONACYT) para descentralizar las actividades científicas y tecnológicas en México.

El decreto presidencial de creación del CICESE, publicado el 18 de septiembre de 1973, lo define como un organismo descentralizado de interés público, con personalidad jurídica y patrimonio propios para realizar “investigación científica básica y aplicada inicialmente en los campos de la geofísica, oceanografía física, física e instrumentación, principalmente orientadas a la a la solución de problemas nacionales y en particular a los regionales de la península de Baja California, así como a las actividades docentes en estas áreas de la ciencia en los niveles de maestría y doctorado”.

El contexto que permitió crear al CICESE a principios de los setentas es muy diverso, pero destacan, entre otros aspectos: una política nacional por descentralizar la investigación científica, la presencia de la Escuela Superior [hoy Facultad] de Ciencias Marinas de la Universidad Autónoma de Baja California (UABC) y la cercanía del Scripps Institution of Oceanography (SIO); la decisión, en 1970, de la UNAM de construir el Observatorio Astronómico Nacional en la sierra de San Pedro Mártir; la intensa actividad tectónica y sísmica de la península de Baja California y del golfo de California que justificaba la realización de estudios en las ciencias de la Tierra, y el requerimiento por desarrollar instrumentación electrónica y óptica como apoyo a la UNAM y a la investigación oceanográfica y geofísica del nuevo centro.

El primer director del CICESE fue el Dr. Nicolás Grijalva y Ortiz, quien fue substituido en 1975 por el Dr. Saúl Álvarez Borrego. Le siguieron en el cargo los doctores Mario Martínez García (1989-1997), Francisco Javier Mendieta Jiménez (1997-2005), Federico Graef Ziehl (2005-2015), y Marinone Moschetto Silvio Guido Lorenzo quien se mantiene hasta la fecha como director general del centro.

Desde su creación, el CICESE se ha dedicado a formar maestros y doctores en ciencias en las áreas académicas de su competencia. El desarrollo institucional ha permitido pasar de un esquema académico que contemplaba originalmente tres programas de maestría (en Oceanografía, en Geofísica y en Física Aplicada), a un padrón integrado por 16 posgrados que cubren todas las áreas de investigación que actualmente se cultivan, y de los cuales egresan anualmente, en promedio, 80 estudiantes de maestría y 20 de doctorado.

La integración del campus que actualmente ocupa este centro, comenzó a desarrollarse alrededor de 1977. Gradualmente se fueron adquiriendo terrenos y construyendo edificios, hasta ocupar las más de 15 hectáreas que hoy se tienen, en las cuales se asientan ocho modernos edificios que albergan aulas, cubículos y más de 115 laboratorios bien dotados con equipo científico. Evocando una verdadera ciudad universitaria, este campus es actualmente sede de varias facultades e institutos de investigación de la UABC, del Instituto de Astronomía y del Centro de Nanociencias y Nanotecnología de la UNAM, y de las cuatro divisiones académicas del CICESE (Biología Experimental y Aplicada, Ciencias de la Tierra, Física Aplicada y Oceanología).
Con el objeto de extender las labores de investigación hacia el sur de la península, el CICESE fundó en 1996 la primera de sus unidades foráneas en La Paz, Baja California Sur. La segunda, en Monterrey, Nuevo León, se creó en 2001.

Actualmente el CICESE es un Centro Público de Investigación el más grande de los 27 que integran el Sistema de Centros Públicos de Investigación del Conacyt. Tras una reestructuración convenida en un nuevo decreto publicado en el Diario Oficial de la Federación del 29 de agosto de 2000, las actividades de investigación, docencia y vinculación del CICESE se concentran en ciencias biológicas, físicas, de la información, del mar y de la Tierra, dentro de un marco de responsabilidad, ética y liderazgo en beneficio de la sociedad.




</doc>
<doc id="22123" url="https://es.wikipedia.org/wiki?curid=22123" title="Instituto Tecnológico Autónomo de México">
Instituto Tecnológico Autónomo de México

El Instituto Tecnológico Autónomo de México (ITAM) es una institución de educación superior de la iniciativa privada, laica, sin fines de lucro ni de afiliación política, fundada en 1946 por Raúl Baillères Chávez. En él, se llevan a cabo tareas de docencia e investigación —centradas en administración, política y diversas ingenierías— con la misión de "contribuir a la formación integral de la persona y al desarrollo de una sociedad más libre, más justa y más próspera", según reza su propia página web. 
Es una de las instituciones universitarias más importantes de México; prestigiosa en las ciencias sociales y considerada, junto al El Colegio de México y el Centro de Investigación y Docencia Económicas como una de las mejores instituciones en las carreras de economía, ciencia política, derecho y relaciones internacionales. A su vez, es considerado como uno de los principales centros de investigación y tiene el máximo rango de aceptados en el servicio exterior mexicano.

Su "campus" se encuentra ubicado en la Ciudad de México, y está dividido en dos sedes: «Río Hondo», en la alcaldía Álvaro Obregón (para licenciaturas e ingenierías), y «Santa Teresa», en la alcaldía Magdalena Contreras (para posgrados y diplomados), ambas bajo el mismo régimen administrativo. Los egresados del ITAM (a quienes se les conoce popularmente como «itamitas») han llegado a ocupar los más altos cargos del gobierno: presidente, secretarios federales, senadores, diputados y embajadores. Es la escuela con el mayor índice de admisión al servicio diplomático de México.

Denominado inicialmente Instituto Tecnológico de México (ITM), el ITAM fue fundado por el empresario mexicano Raúl Baillères Chávez el 29 de marzo de 1946. Prominente hombre de negocios originario de Silao, Guanajuato, Baillères desarrolló una amplia trayectoria que incluyó la Fundación de Crédito Minero y Mercantil y la adquisición de empresas como Cervecería Cuauhtémoc Moctezuma y El Palacio de Hierro, entre otras; antecedentes de lo que hoy constituye Grupo Bal, un holding diversificado en distintas compañías y actividades. Fue, asimismo, presidente de la Asociación de Bancos de México (ABM).

Interesado por la educación, Baillères reunió a un grupo de banqueros y empresarios para constituir la Asociación Mexicana de Cultura A. C., —de cuya junta de gobierno fue presidente—, la cual se convirtió en el patronato del Instituto Tecnológico de México. La primera carrera universitaria ofrecida por el Instituto fue Economía, en 1946; un año más adelante se establecieron la Escuela Preparatoria y la Escuela de Administración de Negocios.

La primera sede del ITM estaba ubicada en la calle Palma Norte 518, entre Belisario Domínguez y República de Cuba, en el Centro Histórico de la Ciudad de México, y allí permaneció por cuatro años. En la década de 1950 el ITM vivió un proceso de expansión en diversos sentidos. Sus instalaciones se trasladaron a la calle de Serapio Rendón 65, en la Colonia San Rafael, y en 1951 se abrieron las carreras de Contador Público y Contador Privado, con lo que el número de alumnos creció de 50 a 500. La demanda de estas carreras se explica tomando en cuenta el contexto conocido como “Desarrollo estabilizador”, la época de crecimiento económico sostenido para México a partir de la década de 1950.

El ITM fue refinando su misión como centro de estudios superiores, y en 1954 cerró su escuela preparatoria, para enfocarse en la educación superior. A fines de la década, el ITM se trasladó a una sede construida ex profeso, en la Calle de Marina Nacional 350, en el terreno contiguo a la Torre de Petróleos Mexicanos edificada posteriormente.

El 19 de enero de 1963, el ITAM obtuvo su autonomía mediante un decreto publicado en el "Diario Oficial de la Federación" por el entonces presidente de la República, Adolfo López Mateos, y con el aval de Jaime Torres Bodet, quien ocupaba el cargo de secretario de Educación Pública. Con el rango de Escuela Libre Universitaria, el ahora Instituto Tecnológico Autónomo de México (ITAM) —denominado oficialmente con ese nombre desde 1985— se sumó al grupo de centros de enseñanza superior con autogobierno, como la Universidad Nacional Autónoma de México (autónoma desde 1929). A fines de esa década, en 1967, la Asamblea de la Asociación Mexicana de Cultura, A. C., estableció la Junta de Gobierno del ITAM, el primer paso de una estrategia de fortalecimiento y consolidación institucional, que en 1969 condujo al establecimiento de un Plan Integral de Desarrollo que transformó las estructuras académicas de investigación, administración y organización. Este plan también incluyó la creación del Centro de Investigación y Extensión Universitaria, y la apertura de nuevas carreras y estudios de posgrado. A la fecha de este artículo, la Junta de Gobierno es la única autoridad con la capacidad de transformar, modificar o eliminar los estatutos y las directrices que rigen a todo el Instituto.

En 1974, se establecieron la licenciatura en Matemáticas Aplicadas y la maestría en Administración. Un año después, se creó la licenciatura en Ciencias Sociales, la cual fue reemplazada en 1991 por la actual licenciatura en Ciencia Política. Esa década y la siguiente estuvieron marcadas por la expansión y el crecimiento de la oferta académica, con la apertura de las carreras de Derecho (1980), Actuaría (1982), Ingeniería en Computación (1983), Relaciones Internacionales (1992), Ingeniería en Telemática (1993), Ingeniería Industrial (1997) y diversos programas conjuntos: Administración y Contaduría Pública y Estrategia Financiera, así como Economía y Ciencia Política o Economía y Relaciones Internacionales (2012). Recientemente, el ITAM abrió nuevos programas de licenciatura: Ingeniería en Negocios (2005), y Dirección Financiera e Ingeniería en Mecatrónica, ambas en 2010.

La expansión académica del instituto hizo necesaria la ampliación de sus instalaciones. A inicios de 1978, el ITAM trasladó su ubicación a la calle de Río Hondo 1, en la Colonia Progreso Tizapán de la Ciudad de México, en las instalaciones de un antiguo seminario jesuita. La consolidación de los estudios de posgrado se hizo patente con la inauguración del Centro de Investigación y Estudios de Posgrado en un campus alterno en avenida Santa Teresa 930, en la colonia Héroes de Padierna.

A partir de su fundación, el ITAM ha estado encabezado por directores y posteriormente por rectores, cargos que han sido elegidos únicamente por la Junta de Gobierno. Desde 1967, Alberto Baillères, hijo de Raúl Baillères Chávez y egresado del ITAM, ha presidido la Asociación Mexicana de Cultura y la Junta de Gobierno de la institución, la cual le otorgó el doctorado "honoris causa" el 20 de mayo de 1999. 

La estructura interna del ITAM se compone de cinco Divisiones Académicas que agrupan a catorce Departamentos Académicos y de las que dependen los 38 programas académicos. Las cinco divisiones son: Actuaría, Estadística y Matemáticas; Administración y Contaduría; Economía, Derecho y Ciencias Sociales; Estudios Generales y Estudios Internacionales; e Ingeniería. El ITAM describe su estilo académico en los siguientes términos: “Enfatizamos nuestro compromiso de aportar los mejores profesores para los mejores estudiantes. Las clases son pequeñas y dinámicas y procuramos no contar con grupos que excedan los treinta estudiantes”. Por su tamaño, el ITAM permite la atención personalizada de profesores y directores de carrera que mantienen un vínculo cercano y permanente con el alumnado. El 98 por ciento de la planta docente cuenta con estudios de posgrado en las mejores universidades del mundo. Sin fines de lucro, el ITAM cuenta con un programa de becas a fondo perdido que beneficia a 30 por ciento de los alumnos inscritos. Es decir, uno de cada tres estudiantes recibe apoyo económico para poder cubrir el costo de sus estudios en el instituto e incluso gastos de manutención.

En el ITAM, se imparten diez licenciaturas:


Asimismo, se ofrecen cinco ingenierías:


Existe la posibilidad de cursar más de una carrera a la vez a través de los 31 programas conjuntos que ofrece el ITAM a nivel licenciatura e ingeniería.

Los alumnos de licenciatura cursan un tronco común de Estudios Generales, un programa de siete materias ofrecido por la división del mismo nombre. A través de estas materias se busca la formación humanista del estudiante, con el fin de forjar criterios propios e independientes que puedan integrarse a su vida profesional y personal.

En cuanto a posgrados —nivel que cursa aproximadamente el 50 por ciento de los egresados de licenciatura—, el ITAM cuenta con varios programas, todos con reconocimiento oficial:


También ofrece el doctorado en Economía (2007). Las maestrías pueden cursarse en las modalidades de tiempo parcial o tiempo completo.

Además de las carreras y estudios de posgrado, el Instituto cuenta con un Programa de Educación Continua, llamado de manera oficial Extensión Universitaria y Desarrollo Ejecutivo, que busca mantener actualizados a los exalumnos y brindar oportunidades educativas a otros grupos y personas de la sociedad como empresas e instituciones de gobierno, entre otros. Este Programa ofrece diplomados internacionales, diplomados automatizados, programas ejecutivos y cursos de actualización; y forma parte de UNICON (Consorcio Internacional de Universidades para la Educación Ejecutiva), al cual sólo pertenecen otros centros de excelencia como la Universidad de Harvard, la Wharton Business School, la Kellogg School of Management y la Universidad Stanford.

Los cursos y diplomados se ofrecen en las áreas de: Administración; Actuaría y Seguros; Contabilidad y Finanzas; Derecho; Economía; Estadística; Innovación; Tecnología y Computación; Ingeniería Industrial y Operaciones; Relaciones Internacionales; Sector Público y Sociedad; y Humanismo. También cuenta con proyectos y programas especiales, diseñados de acuerdo con las necesidades de capacitación de una empresa determinada y pueden impartirse dentro del campus o extramuros. Esta vertiente parte de la idea de que “toda educación debe tender a mejorar al ser humano mediante el enriquecimiento de sus mejores valores.”

La matrícula del ITAM representa menos del 0.5 por ciento de la matrícula universitaria nacional y fluctúa alrededor de 5,500 alumnos a nivel licenciatura e ingeniería. Sin embargo, el ITAM tiene gran impacto en su comunidad de exalumnos y en la vida nacional. Para entrar al ITAM los aspirantes deben realizar la "Prueba de Aptitud Académica" (PAA), examen de admisión que se aplica en el instituto y también en las mejores universidades a nivel internacional. Los jóvenes que se inscriben al ITAM obtienen buenos resultados sobre esta evaluación, pertenecen así al grupo de estudiantes que obtiene esta puntuación en México y en el mundo.

En la clasificación "Los 300 líderes más influyentes", realizada anualmente por la revista "Líderes Mexicanos" desde el 2006, los egresados del ITAM han representado, en promedio, el 7 por ciento de la lista. En el 2011, 32 exalumnos y catedráticos de los programas de maestría y licenciatura aparecieron en la nueva versión de la lista. Un estudio realizado por la revista mexicana "Expansión" en el 2007 encontró que el 14 por ciento de las 50 mujeres más influyentes en México eran egresadas del ITAM, hecho que puso en evidencia el esfuerzo en pro de la equidad de género realizado por el Instituto a lo largo de su trayectoria.

El ser una escuela de dimensiones menores a las grandes universidades mexicanas ha fomentado el desarrollo de un espíritu unitario entre los exalumnos que permanecen vinculados al Instituto aún después de culminar sus estudios. La mayoría de los egresados del ITAM se encuentra en los primeros niveles directivos en las empresas o instituciones donde trabajan, más de la mitad opta por realizar estudios de posgrado en universidades de excelencia en el extranjero y, de acuerdo con una encuesta, está satisfecho con la preparación que ha recibido. Los egresados del Instituto logran una adecuada inserción en el mercado laboral. El ITAM cuenta con una bolsa de trabajo y 60 por ciento de los alumnos encuentran un empleo antes de terminar la carrera.

Para reconocer la calidad profesional de los exalumnos del ITAM y su contribución al desarrollo del país, el ITAM y la Asociación de Exalumnos instauraron desde 1999 la entrega de reconocimientos «Carrera al Universo» y «Mérito Profesional». En el 2010, Georgina Kessel, en ese entonces titular de la Secretaría de Energía, con treinta años de experiencia, fue la primera mujer en obtener el máximo galardón.

Diversos exalumnos del instituto han sido relevantes como académicos, directivos de organismos internacionales y de la sociedad civil, directivos en empresas del sector privado y funcionarios del sector público. Entre ellos, destacan: 


Los alumnos del ITAM han recibido diversos premios y distinciones: la Beca Fulbright-García Robles del Programa Fulbright, el Premio de la Academia Mexicana de Derecho y Economía; el Premio Citibanamex de Economía, el Premio de la Asociación Nacional de Instituciones de Educación en Informática (ANIEI); el Premio a la Excelencia Académica de la Asociación Nacional de Facultades y Escuelas de Ingeniería, el Premio de la Asociación Mexicana de Ingenieros Mecánicos y Electricistas, el Premio Nacional de Investigación Financiera, del Instituto Mexicano de Ejecutivos de Finanzas, y el Premio "Goldman Sachs Global Leaders Program". El propio instituto concede un premio a los mejores trabajos de investigación realizados por sus alumnos.

El ITAM, como institución, también ha recibido múltiples galardones: en el 2010, la División Académica de Ingeniería recibió el Premio Franz Edelman, el cual se otorga a “ejemplos sobresalientes de operaciones innovadoras que benefician a las organizaciones”. En el 2007, la revista "América Economía" lo distinguió como la mejor escuela de negocios en América Latina; un comentario sobre la noticia explicaba este reconocimiento en función de una sólida planta docente de profesores de tiempo completo, muchos de ellos con posgrados en las mejores universidades del mundo. Ese mismo año, el ITAM recibió la acreditación de la Federación de Instituciones Mexicanas Particulares de Educación Superior, A. C. (FIMPES).

Entre los reconocimientos institucionales más recientes cabe señalar el primer lugar de nivel académico institucional y el primer lugar académico de los profesores de acuerdo con la encuesta «Mejores Universidades» publicada por el periódico mexicano "Reforma". En la encuesta más reciente, el ITAM obtuvo el primer lugar general con cuatro carreras en el primer puesto, así como primeros lugares en otras áreas Una subsección de la página del ITAM llamada “Eventos y noticias de interés” lleva un detallado seguimiento de los premios otorgados a sus alumnos y su incorporación a puestos relevantes en el ámbito oficial o corporativo.

En el 2011, los programas académicos de Computación e Ingeniería Industrial del ITAM recibieron la Acreditación ABET, y fue la primera institución educativa de la Ciudad de México que recibió este reconocimiento. ABET es un prestigioso organismo internacional que evalúa programas universitarios de ciencia aplicada, tecnología e ingeniería en el mundo.

El ITAM cuenta con la triple acreditación de AACSB (Association to Advance Collegiate Schools of Business) de los Estados Unidos, AMBA (Association of MBAs) de Londres y EQUIS (European Quality Improvement System) de Bruselas, que reconocen la calidad de sus programas de administración de negocios a nivel internacional.

Aparte de sus servicios educativos, el ITAM se ha consolidado como un prestigioso centro de investigación científica. Algunos de los trabajos producidos entre sus muros han determinado, en muchos casos, las políticas públicas, empresariales e industriales del país. Los centros de investigación del Instituto son los siguientes: Centro de Análisis e Investigación Económica (CAIE), Centro de Desarrollo Tecnológico (CDT), Centro de Economía Aplicada y Políticas Públicas (CEAPP), Centro de Estadística Aplicada (CEA), Centro de Estudios Actuariales (CEAct), Centro de Estudios de Competitividad (CEC), Centro de Estudios de Derecho Privado, Centro de Estudios de Derecho Público, Centro de Estudios y Programas Interamericanos, Centro de Evaluación Socioeconómica de Proyectos, Centro Internacional para la Investigación en Pensiones, Centro de Investigación Económica (CIE), Centro de Estudios Alonso Lujambio (CEAL) y Centro de Acceso a la Justicia. La División Académica de Administración y Contaduría cuenta con cinco centros de investigación especializados en diferentes áreas de esas materias.

Entre las universidades privadas del país, el ITAM se distingue por su actividad editorial y en su catálogo se cuentan más de cien volúmenes y seiscientos artículos producidos por sus investigadores y publicados en revistas internacionales especializadas, muchos de ellos registrados en "Web of Science" de Thomson Reuters, “una base de datos de contenido multidisciplinario, con autoridad, que cubre 10,000 publicaciones especializadas de alto impacto mundial”. Las publicaciones académicas del ITAM se han compilado en el catálogo "Publicaciones Académicas 1996-1998".

Además, el Instituto edita sus propias revistas en dos grandes categorías, las institucionales y las del alumnado. A la primera categoría corresponden las publicaciones: "Análisis de la Coyuntura Económica", "Estudios", "Revista Mexicana de Derecho Público", "Isonomía", "Dirección Estratégica" y "Segmento". Por su importancia pública y su impacto internacional destaca "Foreign Affairs Latinoamérica", licencia de la prestigiada revista estadounidense "Foreign Affairs", considerada una de las más influyentes en el ámbito internacional. La segunda categoría, referente a las revistas del alumnado, abarca títulos como: "Caeteris Paribus", "Gaceta de Ciencia Política", "Gaceta de Economía", "Laberintos e Infinitos", "holaMundo", "Opción", "El Globalista", "Sintagma" y "Urbi et Orbi". "El Supuesto" es el periódico de los alumnos, cuenta con cerca de trescientos números publicados hasta la fecha, y sirve como medio de comunicación comunitaria y foro de expresión estudiantil sobre temas de actualidad. En conjunto estos productos editoriales cubren un amplio espectro temático paralelo a la formación dentro del aula.

El ITAM cuenta con la Biblioteca Raúl Baillères Jr., ubicada en el campus de Río Hondo, la cual cumple con los criterios establecidos por la "American Library Association" para las bibliotecas universitarias. En su colección hay más de 495,000 volúmenes y ejemplares de más de 900 publicaciones periódicas relacionadas con las disciplinas enseñadas en el Instituto. Asimismo, ofrece acceso a recursos hemerográficos, electrónicos y audiovisuales. Sus colecciones especiales incluyen los fondos Miguel Palacios Macedo, Bibliografía Antigua (procedente de la colección privada de Rudi Dornbusch) y las bibliotecas personales de Luis Montes de Oca y José Luis La Madrid Sauza. Cuenta con el fondo de la Biblioteca Manuel Gómez Morin, que perteneció al político mexicano fundador del Partido Acción Nacional e incluye más de 12,000 libros de leyes, economía, ciencia política, administración pública, filosofía y literatura, así como una serie de ediciones especiales.

Esta oferta de recursos para los alumnos e investigadores se complementa con las bases de datos; juntos conforman la infraestructura tecnológica necesaria para impulsar el desempeño académico. El Centro Financiero en el Campus de Santa Teresa cuenta con programas especializados y bases de datos para la investigación financiera: "Data Stream International", "Dow Jones Interactive", "Bloomberg", Reuter 3000/Reuter Graphics e Infosel Financiero. Los centros de cómputo de Río Hondo incluyen laboratorios de instrumentación, electrónica, trabajo en redes, automatización de microprocesos e ingeniería telemática.

Cabe mencionar también al Centro de Aprendizaje, Redacción y Lenguas (CARLE), parte del Departamento Académico de Lenguas y dependiente de la División Académica de Estudios Generales y Estudios Internacionales del ITAM. Su misión es cubrir con los objetivos de aprendizaje que los alumnos se propongan en las áreas de estrategias de aprendizaje, redacción y lenguas extranjeras. Éste Centro cuenta con el Aula Interactiva Cervantes, Centro de escritura y una Mediateca.

Las sociedades de alumnos son de gran importancia de dentro del Instituto. La Sociedad de Alumnos del ITAM (CARITAM) es la representación oficial de los estudiantes, y el Consejo Universitario de Honor y Excelencia (CUHE) se propone fomentar la mejora en el rendimiento académico. Como parte de una cultura de la responsabilidad social, existen otras agrupaciones del alumnado del ITAM estructuradas como voluntariado. Los alumnos participan en ellas como una contribución para resolver los problemas de la sociedad mexicana. Entra estas agrupaciones destacan: ALCANCE, con una década de trabajo en tres áreas principales: Desarrollo comunitario, Niños de la calle y Personas discapacitadas física o mentalmente. AIESEC, ONG dedicada a realizar prácticas profesionales y sociales en el mundo para crear una mayor convivencia y respeto entre las diversas culturas internacionales. El Centro de Acceso a la Justicia (CAJ), conformado por estudiantes y maestros de Derecho, proporciona ayuda legal gratuita a las personas que la necesitan. El Despacho de Asesoría Gratuita a Organizaciones Filantrópicas (DAGIF) ofrece servicios de consultoría sobre temas como obtención de fondos, cumplimiento de las disposiciones legales mexicanas y campañas de comunicación. ITAMMUN es un Modelo de Naciones Unidas que busca promover el debate alrededor de temas de la agenda internacional, generalmente organizado dentro de las instalaciones de la Cancillería (SRE). El Programa Interdisciplinario para el Desarrollo Sustentable (PIDES) es una asociación que reúne a los estudiantes que organizan actividades para promover el respeto y el cuidado del medio ambiente. El ITAM logra de esta forma una participación activa en la solución de los grandes problemas nacionales.




</doc>
<doc id="22161" url="https://es.wikipedia.org/wiki?curid=22161" title="A Western Harvest Field by Moonlight">
A Western Harvest Field by Moonlight

A Western Harvest Field by Moonlight —en español: "Un campo de cosecha occidental a la luz de la luna"— es el primer EP del músico estadounidense Beck, lanzado en 1994 a través de Fingerpaint Records, y contó con la producción de Beck Hansen y Tom Grimley. La canción "Totally Confused" cuenta con la participación de la banda that dog., que más tarde contribuiría en otras dos canciones de beck, "Girl of My Dreams" y "Steve Threw Up".

En 1994 fueron impresos originalmente 3000 copias y desde entonces hubo varias reimpresiones: 2000 en 1995, 1000 en 1997 y otros 1000 en 1998. Las primeras ediciones contenían pinturas hechas con los dedos por Beck y amigos que fueron hechos en la discográfica durante la fiesta de lanzamiento del disco y fueron empaquetados con la calcomanía “Original Figerpainting Enclosed” (“Adjunto Pintura Hecha con los Dedos Original”). Las siguientes impresiones no contenían ninguna pintura.

El álbum tiene “Sexy Death Soda” rayado en el lado A, y “Cherry Cupcake” en el B. Las primeras impresiones salieron con “Styrofoam Chicken (Quality Time)” siendo una pista continua. Se puso a una profundidad mayor en el disco de vinilo así cuando la púa leyera la canción saltara otra vez hasta el principio de la misma. Las impresiones de este álbum tenían escrito “A Western Harvest Moon by Moonlight” en el lomo del disco. Una curiosidad es que si se sostiene el disco frente a un espejo se puede leer debajo de la imagen de la pequeña niña la frase “Happiness grows in your own back yard” (“La felicidad crece en tu propio patio trasero”).

Todas las canciones escritas y compuestas por Beck. 


</doc>
<doc id="22163" url="https://es.wikipedia.org/wiki?curid=22163" title="One Foot in the Grave">
One Foot in the Grave

One Foot in the Grave —en español: "Un pie en la tumba"— es el cuarto álbum de estudio del músico estadounidense Beck, lanzado el 27 de junio de 1994 a través de la discográfica K Records. El álbum fortalecido la reputación de Beck, podría decirse que le permite entrar en la corriente principal de su álbum de 1996, "Odelay". En julio de 2008, "One Foot in the Grave" había vendido 168 000 copias en los Estados Unidos. El 14 de abril de 2009, el álbum fue reeditado con 16 temas extras, en su mayoría inéditos.

El álbum muestra una fuerte influencia folk que es más pronunciada que en sus álbumes de estilo más ecléctico de la época, "Mellow Gold" y "Odelay". Contiene canciones de rock acústico y alt-country y fueron producidas por Beck Hansen y Calvin Johnson. Fue grabado antes del lanzamiento de su álbum "Mellow Gold", pero no fue liberado hasta después de que el álbum tuvo un éxito de crítica y público. "One foot in the grave" no hacía más que reeditar canciones grabadas años antes en el sello K Records y que recaían más que en el artificio instrumental en la emoción otorgada por los sonidos más acústicos y la melodía. Uno de los temas del disco, que ofrece folk-blues, “Asshole”, fue versionado por el mismísimo Tom Petty, quien había declarado que "en una etapa bastante pobre para el rock Beck era de lo más salvable". La canción “Cyanide Breath Mint” incluso recuerda al Ray Davies más folk aunque en conjunto su faceta más acústica y melódica recuerda bastante a Gene Clark.

La canción "One Foot in the Grave" ya había sido publicada en su álbum "Stereopathetic Soulmanure" de una toma en directo. El álbum presenta la producción y apoyos ocasionales de Calvin Johnson, el fundador de K Records. Fue registrado en Dub Narcotic Studio, que fue alojado entonces en el sótano de Calvin. El productor, y también fundador del sello K Records, acompaña al joven Beck tanto en la portada, como en algunos temas en las voces.



Una edición de lujo fue lanzada el 14 de abril de 2009, con 16 pistas más, 12 de ellas inéditas.




</doc>
<doc id="22164" url="https://es.wikipedia.org/wiki?curid=22164" title="Midnite Vultures">
Midnite Vultures

Midnite Vultures —en español: "Buitres de medianoche"— es el séptimo álbum de estudio del músico estadounidense Beck, publicado el 23 de noviembre de 1999 a través de DGC. Fue grabado en la casa de Beck y contó con la producción del propio artista, Mickey Petralia, Tony Hoffer, John King y Mike Simpson. Con este álbum producido por los Dust Brothers, Beck logra entrar definitivamente en el mapa musical como uno de los cantantes más innovadores del momento. Sencillos de este álbum como la exuberante funk "Sexx Laws" fueron un éxito y le dieron al cantante una mayor popularidad en el mundo entero.

"Midnite Vultures", el disco que llega justo para el fin de milenio con la convicción de ser el último gran disco del año, es la continuación directa de "Odelay", así como "Odelay" lo era de "Mellow Gold". En un reportaje posterior a la salida de "Mutations", Beck declaró que su próximo disco sería un disco “completamente bailable, con canciones tontas y letras tontas”. Esto se ve en frases como “Quiero desafiar la lógica de todas las leyes sexuales”, de “Sexx Laws”, el primer corte del disco, o “Estoy mezclando fitness y cuero / para que todas las lesbianas se pongan a gemir” (“Mixed Bizness”, segundo corte). Más que ningún otro, éste es un disco físico, nosólo plagado por el baile y el sexo sino también por olores y sabores: “Nicotine & Gravy” ("Nicotina y aderezo"), “Peaches & cream” ("Duraznos y crema"'), “Milk & Honey” ("Leche y miel") son tres temas casi consecutivos.

El álbum es una apología de los excesos, narrada a través de imágenes como sacado de un puro sueño de los mejores en Las Vegas o Los Angeles. Como tres imágenes musicalizadas, "Nicotine & Gravy" sube exquisitamente el tono de la cuestión, "Mixed Bizness" lo desordena, "Hollywood Freaks" le pone sabor a la utopía plástica del mundo rico y célebre, siendo ellas las primeras muestras de un ánimo liviano y efervescente. Tuvo una impresión inicial en formato digipak limitada a las primeras 500.000 unidades. En la canción “Beautiful Way” aparece Berth Orton como vocalista de fondo, Johnny Marr (exguitarrista de los Smiths) aparece en “Milk and Honey”. El álbum cuenta también con la inclusión de la canción “Debra” también conocida como “I Wanna Get With You (And Your Sister Debra)”, un clásico en las giras desde 1996 pero que Beck no incluyó en su álbum "Odelay" por considerar que no encajaba en el mismo.

Varias canciones fueron directamente inspiradas por otras canciones: "Get Real Paid" cuenta con un secuenciador espiral que recuerda a "It's More Fun to Compute" de Kraftwerk; un sintetizador de "Milk & Honey" resuena similar a un riff de "The Message", de Grandmaster Flash and the Furious Five; "Beautiful Way" se produjo después de escuchar la canción "Countess from Hong Kong" de The Velvet Underground; y "Debra" fue inspirada por las canciones "Raspberry Beret" de Prince y "Win" de David Bowie. Sin embargo, y a pesar de que todo lo que se escucha fue tomado de los ‘70 y principios de los ‘80 (lo más nuevo seria el electro circa 1982 de “Get Real Paid” y lo más viejo, el soul al estilo Motown circa 1972 de “Debra”) el disco no suena como Lenny Kravitz, Jamiroquai u otros artistas con una fijación retro.

"Midnite Vultures" alcanzó el puesto # 34 en los EE.UU., donde fue disco de oro, y también alcanzó el # 19 en el Reino Unido. A partir de julio de 2008, el álbum ha vendido 743.000 copias en los Estados Unidos. El disco fue elogiado por la mayoría de los críticos, la revista Rolling Stone, NME y Pitchfork Media les dio cuatro estrellas (8.5/10 críticas en Pitchfork). Se le otorgó el estatus de "aclamación universal" por MetaCritic con una puntuación de 83/100, pero en 2006 fue nombrado en el puesto 50 de la lista "El peor disco de la historia" por la revista Q Magazine, a pesar del hecho de que se le dio al álbum cuatro estrellas. Pitchfork Media lo incluyó en el puesto número 5 de su lista "Top 10 Albums of 1999" (los 10 mejores discos de 1999). "Midnite Vultures" fue nominado en 2001 por "Álbum del Año" en los 43rd Grammy Awards.

Las versiones iniciales vendidas en las tiendas Best Buy contenían un disco extra con tres canciones adicionales. El álbum estaba empaquetado en una caja de plástico estándar y el disco extra en una funda de cartón aparte.

Las siguientes canciones fueron grabadas durante las sesiones de "Midnite Vultures", pero no se incluyeron en el álbum. Algunos aparecen en la edición limitada del EP "Beck".





</doc>
<doc id="22170" url="https://es.wikipedia.org/wiki?curid=22170" title="Sea Change">
Sea Change

Sea Change —en español: "Cambio radical"— es el octavo álbum de estudio del músico estadounidense Beck, lanzado el 24 de septiembre de 2002 a través de DGC/Interscope y contó con la producción de Beck Hansen y Nigel Godrich. El álbum es uno de los discos más importantes de Beck y el segundo en colaboración con Nigel Godrich. El mismo contiene canciones de estilo experimental y se destaca por el empleo de violines y violonchelos, y de un estilo más lento que las canciones de discos anteriores. Las letras irónicas fueron reemplazadas por letras con contenido más sincero. También evitó el muestreo intensivo de sus álbumes anteriores. El álbum salió al mercado en cuatro estilos de arte diferentes. En las entrevistas, Beck citó la ruptura con su novia de mucho tiempo, como la mayor influencia en el álbum.

Alcanzó el puesto número #8 en el Billboard 200, siendo esta la última certificación de oro en marzo de 2005 de la Recording Industry Association of America. El álbum recibió elogios de la crítica, con varios comentarios calificándolo como la "obra magna" de Beck, que ha seguido creciendo en estatura desde su lanzamiento, con la inclusión en listas de "mejores de la década" y "lo mejor de todos los tiempos". Los críticos elogiaron el cambio de un estilo experimental a uno simple y emocional.

Al término de la gira de su anterior trabajo, "Midnite Vultures" (1999), Beck y su novia desde hacia ya nueve años, la estilista Leigh Limon, terminaron su relación. Beck, tres semanas antes de cumplir 30 años, descubrió que Limon le había engañando con un miembro de la banda de Los Ángeles Whiskey Biscuit. Beck caducó en un período de melancolía e introspección, durante los cuales escribió las pistas sombrías, basado en la acústica más tarde encontradas en "Sea Change". Escribió la mayoría de las 12 canciones en una semana, pero luego las dejó de lado. Después de un tiempo, decide grabarlas. En 2001, Beck llama su frecuente productor Nigel Godrich para producir las canciones. Hansen pretende grabar el álbum a finales de 2001, pero debido a los ataques del 11 de septiembre, decide postergarlo. Antes de trabajar con Godrich, Beck grabó canciones con Dan "The Automator" Nakamura en enero de 2002 en preparación para el nuevo registro, pero finalmente las canciones de esas sesiones no estuvieron presentes en "Sea Change". Muchas canciones del álbum también se realizaron en vivo antes del lanzamiento, como "Lost Cause" y "Evil Things", este último no se registró debido a la pérdida de tiempo.

Beck y su grupo de músicos entraron en el estudio con la intención de hacer un registro basado en lo acústico, similar a lo hecho en "Mutations". En el proceso de grabación, Beck le dijo a Godrich que él esperaba grabar una canción por día, similar al proceso de "Mutations". Sin embargo, cada canción acabó llevando por lo menos dos días de grabación, debido a los arreglos orquestales. Los socios musicales en el estudio incluyen al multi-instrumentista Jon Brion, el baterista James Gadson, y el guitarrista Jason Falkner, así como los socios musicales desde hace mucho tiempo, el tecladista Roger Joseph Manning Jr., el bajista Justin Meldal-Johnsen, el baterista Joey Waronker, el guitarrista Smokey Hormel y la violonchelista Suzie Katayama. Además, el padre de Beck, David Campbell, proporciona arreglos de cuerda. Joey Waronker dejó la grabación para viajar a Hawái; James Gadson completó su parte los días restantes.

La grabación comenzó tan pronto como la banda entró a los Ocean Way Studios en Los Angeles el 6 de marzo de 2002. Mucho material del álbum fue grabado en vivo, con efectos adicionales (incluyendo campanas y cuerdas), agregados más adelante. A fin de capturar la inmediatez del material, los artistas trabajaron rápidamente y espontáneamente. Durante la producción, Beck se dio cuenta de que su voz se había vuelto significativamente más profunda. "Antes de que registráramos", dijo Godrich, "hemos escuchado "Mutations" y su voz sonaba como Mickey Mouse. Su rango se ha reducido. Ahora cuando abre la boca, sale una gran vibración. Es bastante notable. Él tiene un tono increíble." Al final de la producción, el grupo de músicos tuvieron que trabajar más rápido de lo que pensaban. Finalmente, la grabación duró un poco más de tres semanas y el álbum fue mezclado a partir de allí, y finalizó el 7 de mayo. En una sesión, Hansen comenzó a rasguear su famoso sencillo de 1995 "It's All in Your Mind" al azar antes de comenzar una nueva canción, y Godrich, que quedó encantado, comento "Tenemos que hacer esto".

"Ship in the Bottle" era la única pista completa no incluida en el registro. "Era la canción super-pop del disco," dijo Beck en una entrevista del 2002. Finalmente, "Ship in the Bottle" fue lanzada en la versión japonesa de "Sea Change" y más adelante en la versión remasterizada del álbum.

El álbum se basa en un conjunto de canciones acústicas, que muestran el lado reflexivo del cantautor Beck. El cambio de género se define como una transformación amplia, que refleja la salida en el estilo del anterior álbum de Beck "Midnite Vultures" y de las grabaciones anteriores, basadas en los samples, así como el deseo de Beck de dar a cada álbum una identidad. Los orígenes para el sonido único y apasionado del álbum habían sido acumulandos durante años, según Beck en una entrevista de 2002: "hay hilos de lo que he hecho antes. Si usted escucha mis anteriores B-sides, escuchará este disco. He estado queriendo hacer este disco durante años", explicó. A pesar de la dificultad inicial para decidir el nombre, el título proviene de "Little One", la undécima pista en el álbum. Las grabaciones de "Sea Change" incluyen temas de amores rotos, desolación, soledad, etc. Aunque a menudo se compara con "Mutations", Beck, consideró el álbum, en una entrevista de 2008, como un órgano más representativo de su álbum de 1994 "One Foot in the Grave", y "más representativo de lo que yo estaba haciendo [en los primeros días]".

"Sea Change" es la forma más hermosa de componer un corazón roto, alejar los fantasmas y reinventarse a partir de una experiencia dolorosa. Es la experiencia del dolor pariendo belleza y sanación, es la experiencia mística del artista comprometido con el arte y la vida. "Sea Change" es un mar que nos recorre por dentro, erizándonos la piel, entre el llanto y la desnudez. Con esta producción Beck rompió esquemas de sus anteriores producciones, para pasar de las estructuras intelectuales y experimentales a la delicada línea de lo personal, del sentimiento palpable, de los sufrimientos no compartidos, las letras se tornan directas pero con ese manejo oportuno para alejarlo de lo lastimero, al fin y al cabo el intelecto y talento lo levantan para dejar un disco excelente, minimalista, orgánico, directo, con sonidos que se pierden en el viento y se conservan en los recuerdos.

Antes de su lanzamiento, los comerciantes se preocuparon por el impacto comercial de "Sea Change" debido a su sonido. Los analistas predijeron que el álbum no recibiría apoyo en las emisoras de radio, observando la reputación de Beck, los elogios de la crítica y la posibilidad de múltiples nominaciones a los Grammy podrían compensar un sonido no comercial. La fecha de lanzamiento del álbum fue anunciada el 31 de mayo de 2002. Además, la lista inicial de canciones del álbum también fue lanzada, con canciones en un orden muy diferente que su versión definitiva, así como incluyendo la pista "Ship in the Bottle". El título del álbum fue anunciado en agosto de 2002. En la promoción del disco, las nuevas canciones del álbum fueron puestas en libertad en orden cronológico semanalmente a través de la Página web de Beck en julio y agosto de 2002. Finalmente, el álbum fue lanzado el 24 de septiembre de 2002.

En el momento del lanzamiento, un álbum de cuatro pistas llamado "Sea Change Album Sampler" fue lanzado. Se empaqueto como un CD single y contuvo las pistas siguientes:
"Sea Change" fue lanzado con cuatro portadas distintas, cada versión contiene material gráfico digital distinto por Jeremy Blake en el CD y el folleto. Había también diferentes mensajes ocultos (fragmentos líricos) escritos debajo de la bandeja del CD de cada versión. El arte de la cubierta original del álbum fue utilizado como una efigie en el vídeo musical del sencillo "Lost Cause".

En 2002, "Sea Change" fue uno de los dos álbumes en recibir la más alta calificación de cinco estrellas en la revista Rolling Stone, el otro álbum fue "The Rising" de Bruce Springsteen. La revista llegó a llamarlo el mejor álbum del año 2002. Al año siguiente, el álbum ocupó el puesto número 440 en la lista . También ocupó el puesto número 17 en la lista de los "100 mejores discos de los 00s" de Rolling Stone.

"Sea Change" alcanzó el puesto #8 en el Billboard Top 200 chart y fue finalmente certificado de oro en marzo de 2005. En el UK Charts alcanzó el puesto #20. A partir de julio de 2008, el álbum ha vendido 680.000 copias en los Estados Unidos. El álbum fue re-editado en un formato remasterizado por Mobile Fidelity Sound Lab en el mes de junio del año 2009.

Se dice que el concepto de sonido del álbum está inspirado en el sonido del álbum "Histoire de Melody Nelson" de Serge Gainsbourg. Los críticos también han comparado las melodías acústicas y relajadas de "Sea Change" a las obras del cantautor británico Nick Drake y a las del álbum de 1975 "Blood on the Tracks", del músico Bob Dylan.

"Sea Change" rindió muchos tours en apoyo, el primero de los cuales comenzó como una gira discreta basada en lo acústico, en un teatro en agosto de 2002. Cada show dio un ambiente lúdico, enérgico, con Beck contando chistes entre actuaciones y una aparición sorpresa en el show: Jack White de The White Stripes, el 11 de agosto. Un largo tour estaba previsto para octubre de 2002, con la banda The Flaming Lips como apertura, así como banda de acompañamiento de Beck. La gira comenzó en octubre y terminó en noviembre de 2002.

Durante la gira de "Sea Change", Beck varió la lista y experimentó con las estructuras de la canción, cambiando los arreglos cada noche como una forma de romper la previsibilidad. En el cierto deseo de Beck para la reinterpretación de sus canciones, despidió a su banda de gira desde hace mucho tiempo y al grupo con el que trabajó en "Sea Change" poco antes de que comenzara la gira. Entre nuevas y viejas canciones en cada concierto, Beck realizó numerosos covers, como "No Expectations" de The Rolling Stones, "Kangaroo" de Big Star, "Beechwood Park" de The Zombies y "Sunday Morning", de The Velvet Underground. Descrito como "impresionante" por David Fricke de Rolling Stone, Fricke amplió también sus declaraciones: "fue un ajuste perfecto — canciones sobre el compromiso y la pérdida, escrito y cantado por los heridos."

Créditos de "Sea Change" adaptado de Allmusic.




</doc>
<doc id="22171" url="https://es.wikipedia.org/wiki?curid=22171" title="Stray Blues: A Collection of B-Sides">
Stray Blues: A Collection of B-Sides

Stray Blues: A Collection of B-Sides —en español: "Blues callejero: Una colección de B-Sides"— es un álbum recopilatorio de B-sides del músico estadounidense Beck, lanzado el 1 de junio de 2000.

El álbum fue publicado a través del sello discográfico Geffen Records y sólo fue lanzado en Japón. Incluye "Burro", una versión en español de "Jack-Ass" y el cover "Halo of Gold" de Skip Spence. "Totally Confused" había sido publicada en el EP "A Western Harvest Field by Moonlight" de 1994, "Clock" era una pista oculta del álbum "Odelay" de 1996, mientras que "Brother", "Lemonade", "Electric Music and the Summer People" y "Feather in Your Cap" son lados B de los sencillos "The New Pollution" y "Sissyneck". El álbum fue limitado a 70.000 copias. Este álbum contó con la producción de Beck Hansen, Nigel Godrich, Tom Grimley, Tom Rothrock, Rob Schnapf, John King, Mike Simpson y Brian Paulson.



</doc>
<doc id="22184" url="https://es.wikipedia.org/wiki?curid=22184" title="Fundación de Roma">
Fundación de Roma

La fundación de Roma es referida por varias leyendas, las cuales fueron unificadas principalmente por la "Eneida" de Virgilio, reuniendo en una historia coherente distintas versiones de algunos ritos de iniciación de aquel tiempo.

Se supone, con cierta probabilidad, que entre los siglos X y VII a.C., Italia central estaba poblada por los dos grupos principales en que se dividían los italianos: los s y los latinos. "Latium Vetus" (el antiguo territorio del Lacio) estaba poblado por etruscos, volscos, sabinos, ecuos, rútulos y ausonios. Vinieron de diferentes áreas de Italia central, incluyendo Toscana, Marcas y Liguria.

Entre ellos, los latinos desarrollaron una sociedad organizada que fue la principal fuente de la población romana. Los latinos originalmente se quedaron en los "Colli Albani" (los montes Albanos, en la moderna Castelli), a unos 30 u 80 km al sudeste del monte Capitolino. Luego bajaron hacia los valles, los cuales ofrecían mejores tierras para la agricultura y la ganadería.

Las zonas inmediatas al río eran muy favorables y además ofrecían recursos estratégicos notables: el río formaba una frontera natural por un lado, mientras los montes daban un resguardo defensivo del otro. Esta posición también daba a los latinos control sobre el río, y su posible tráfico comercial y militar, desde el natural punto de observación en la isla Tiberina, la isla situada frente al actual Trastevere. Asimismo se podía controlar el tráfico terrestre, ya que Roma se hallaba en la intersección de los principales caminos al mar desde Sabinia, al sureste, y Etruria, al noroeste.

Se supone que el desarrollo del asentamiento comenzó con diferentes poblaciones separadas ("borgate"), situadas en los montes, las cuales se unieron para formar Roma. Estudios recientes sugieren que el monte Quirinal fue muy importante en los tiempos antiguos. Sin embargo, el primer monte en ser habitado parece haber sido el Palatino (lo que confirmaría la leyenda), que está en el centro de la Roma antigua. Sus tres crestas (los montes menores Cermalo o Germalo, Palatium y Velia) se unieron con las tres cimas del Esquilino (Cispio, Fagutal y Opio), y luego los pueblos sobre el monte Celio y la Subura, entre los montes de Rione Moderna, Monti y Opio. Posteriormente, la ciudad creció hasta abarcar también los montes Aventino, Capitolino, Quirinal y Viminal. 

Estos montes tenían nombres expresivos: el monte Celio también era llamado "Querquetulanus", debido a los robles ("quercus"), mientras que el Fagutal ("Fagutalis") estaba poblado por bosques de hayas ("fagus") y el Viminal ("Viminalis") por el mimbre ("vimen") de los sauces. Descubrimientos recientes revelan que el Germalus, sobre la parte norte del Palatium, era el sitio de una población del siglo IX a. C. con viviendas circulares o elípticas. Estaba protegida por una cerca de tapial (quizá reforzada con madera), y es probable que este lugar fuera donde verdaderamente se fundó Roma.

El territorio de esta federación llamada "pomerium" encerraba a la llamada Roma Quadrata (cuadrada). Ésta sería extendida con la inclusión del monte Capitolino y la isla Tiberina ya cuando Roma se convertía en un "oppidum", un pueblo fortificado. El Esquilino todavía era una población satélite. Éste sería incluido con las expansiones servias.

Las celebraciones del "septimontium" ("de los siete montes"), el 11 de diciembre, en aquel entonces eran consideradas en relación a la fundación de la ciudad. Sin embargo, como el 21 de abril es la única fecha en las que todas las leyendas se ponen de acuerdo, recientemente se ha argumentado que probablemente el "septimontium" celebraba más bien las primeras federaciones entre los poblados de los montes romanos; de hecho, una federación similar era celebrada por los latinos en Cave, un pueblito al sudeste romano, o en el Monte Cavo en Castelli.

La leyenda sobre la fundación romana tiende a relacionar al naciente pueblo con las entidades más eminentes de su época, para poder "demostrar" que su gran éxito depende también de su origen especial, o para completar su reputación con referencias de primera clase. Sin embargo, esta historia es una historia completa, y la tradición romana entera está basada en ella. Hay varias versiones de esta leyenda; la siguiente se considera comúnmente como la principal:

Con el héroe semidiós Eneas al mando, los derrotados troyanos sobrevivientes cruzaron el mar Mediterráneo para alcanzar las costas del Lacio. Llegaron a una área probablemente entre el moderno Anzio y Fiumicino, al sudoeste de Roma. Más comúnmente se supone que arribaron a Laurentum (o Laurento); otras versiones dicen que arribaron a Lavinium, un lugar nombrado por Lavinia, la hija del rey Latino.

Latino, sabio rey de los latinos, los hospedó, dejando que reorganizaran su vida en el Lacio. Su hija Lavinia había sido prometida a Turno, el rey de los Rutuli, pero Latino prefirió ofrecerla a Eneas; Turno consecuentemente le declaró la guerra. El resultado fue la muerte de Turno y la captura de su gente. Ascanio, también conocido como Iulos, hijo de Eneas, fundó Alba Longa y fue el primero en una larga serie de reyes, entre quienes los mejor conocidos son Procas y sus hijos Numitor y Amulio. Según Dionisio de Halicarnaso, los reyes de Alba Longa fueron el nexo directo que unía a Ascanio y Rómulo, el fundador de Roma.

El dios Marte iba paseando por la orilla de un río de la ciudad Alba Longa, allí vio a una mujer dormida en la orilla y quedó enamorado en el instante. Tuvieron dos hijos llamados Rómulo y Remo. Una vez nacidos los hijos de Rea Silvia, los padres de los gemelos los metieron en una canasta y fueron transportados por el río y arribaron a las orillas de un lugar. Allí fueron salvados por una loba llamada Luperca que los amamantó. Cerca vivía un pastor llamado Fáustulo y su esposa Aca Larentia. El pastor encontró a los bebés, los llevó a su casa y los adoptó. Cuando se hicieron adultos, los hermanos fueron informados de su historia, y el pastor les dijo que no los habían tenido, así que regresaron a Alba Longa, mataron a Amulio y liberaron a su abuelo Numitor, devolviéndolo al trono.

Rómulo y Remo se propusieron edificar una nueva ciudad en el mismo lugar en el que fueron encontrados por la loba. Decidieron que uno construiría el pueblo mientras que el otro ayudaría. Así que empezaron a preguntar a los dioses para informarse de quién iría a dirigirla. Rómulo fue a la cima del monte Palatino y tiró su lanza en el monte para encontrar el lugar. La lanza se convirtió en el Corniolus, el árbol sagrado de Roma.

Rómulo se fue a la cima del monte Palatino; Remo a la cima del Aventino. Rómulo se convenció de que él había sido seleccionado por los dioses, ya que recibió el augurio que volaba sobre él fueron un círculo de aves, así que tiró su lanza en el monte para encontrar el lugar; cuando estaba en la tierra, la lanza (la cual era de madera) inmediatamente se convirtió en el Corniolus, el árbol sagrado de Roma.

Para la fundación siguieron los ritos tradicionales de su época para fundar ciudades. Con la ayuda de una vaca y un toro blanco, usó un arado para trazar la cerca de la ciudad. Remo saltó sobre el surco, violando la muralla, lo cual era una especie de sacrilegio, que fue la primera pena capital del homo sacer (que era el castigo por pasar), pues la muralla se trazaba desde el primer momento para ser inviolable. Y de acuerdo con la tradición, Rómulo lo mató a espada, para que los dioses no permitieran que en el futuro la muralla fuese violada de nuevo.

Rómulo fue el primer rey romano, y reinó hasta que desapareció durante una tormenta, llevado por su padre Marte.

Mientras que el cuerpo principal de la leyenda ha permanecido más o menos el mismo desde su creación, algunos detalles han cambiado, principalmente para juntar las versiones ligeramente divergentes y corregir varios puntos en cuanto a tiempo y geografía. También las antiguas leyendas locales poco a poco fueron elaboradas para alcanzar armonía con la historia principal. El efecto de estas intervenciones sobre la leyenda son considerablemente evidentes.

Una de las más tempranas versiones (del siglo V a. C.) es la del griego Helánico de Lesbos, y generalmente es reportada junto con la versión de Damastes de Sigeo. En esta versión, el fundador del pueblo fue Eneas. Estas versiones sobrevivieron hasta el 509 a. C. (año considerado en el que comienza la República romana), cuando fue percatado que, como habían existido 7 reyes romanos y Rómulo fue el primero, existía un hueco entre el siglo VIII de los primeros reyes y el siglo XII de la caída de Troya. Así que, como Rómulo no podía ser hijo de Iulo, sólo quedaba como un distante descendiente. El tiempo entre Iulo y Rómulo fue "llenado" con la serie de los reyes de Alba Longa. Eneas pudiera haber llegado a las costas del Lacio durante el reinado de Latino (rey de los latinos), para poder llegar a un acuerdo con las leyendas locales. Entonces Marte tenía que ser añadido para poder honrarlo, así que Rómulo se volvió un descendiente (por parte de padre) de Marte, mientras que por parte materna Rea Silvia estaba conectada con Eneas mediante la dinastía de Alba Longa. La condena de los hijos de Rea Silvia es sólo una de las varias recolecciones de leyes divinas, de la religión que tan profundamente entró en la vida romana.

Entre los itálicos, como el relato en la Teogonía de Hesíodo, los dos hermanos Agrio y Latino eran los hijos de Telégono, hijo de Ulises y la bruja Circe, a la cual se le dedica el monte Circeo, ubicado al sur del Lacio, donde se celebraba un culto en su honor y estaba su cueva. 

Entre los latinos, se dijo que Saturno había sido reemplazado por su hijo Jove, así que bajó a la Tierra y se mezcló con los latinos. Después, Evandro condujo a su pueblo desde Grecia hasta el Lacio, donde edificó la ciudad de Palanteo, que estaba situada sobre una colina que posteriormente se denominó monte Palatino. Luego Hércules llegó a liberar estas tierras de la amenaza del gigante Cacus. Finalmente Eneas llegó de Troya, después de varias aventuras y fundó Roma. Notablemente en esta versión los latinos no fueron creados o asistidos por los dioses, sino que el pueblo fue fundado por Eneas en presencia de estas "autoridades".

Durante la República romana, varias fechas fueron dadas para la fundación de la ciudad, todas en el intervalo entre 758 a. C. y 728 a. C. Finalmente, bajo el Imperio romano la fecha sugerida por Ático y Varrón (753 a. C.) fue acordada, pero en los "fasti capitolini" el año dado fue el 752. Aunque los años variaban, todas las versiones estaban de acuerdo en que la ciudad fue fundada el 21 de abril, un día santo dedicado al sagrado culto de Pales, diosa de los pastores; en su honor, Roma celebraba el parritta (o palilia).

Descubrimientos recientes en la Colina Capitolina en Roma han brindado la fecha de la fundación de Roma. El más destacado entre estos es una serie de fortificaciones defensivas en la vertiente norte de la Colina Capitolina que pueden datarse a mediados del siglo VIII a.C., cuando la leyenda dice que Rómulo labró un surco ("sulcus") alrededor de la Colina Capitolina para delimitar el perímetro ("pomerium") de su nueva ciudad. Los restos de la muralla y demás evidencias como ésta han sido descubiertas por las excavaciones de Andrea Carandini.

El nombre del pueblo se considera generalmente que se refiere a Rómulo, pero hay otras hipótesis. Una de ellas se refiere a Roma, que sería la hija de Eneas o Evandro. También puede rastrearse un origen etrusco, que apuntaría a la "gens" etrusca "Ruma", o a "Rumon", nombre etrusco del río Tíber. Estudios recientes parecen dar preferencia a una raíz de origen indoeuropeo con significado de "río"; Roma en ese caso significaría "el pueblo sobre el río". 

Roma es también llamada "urbe", y este nombre, que después en latín significaría genéricamente cualquier otro pueblo, proviene de "urvus", surco realizado con un arado, aquí, por el de Rómulo.

Sobre el monte Capitolino, el 21 de abril de cada año, una campana especial llamada la "patarina" suena al mediodía del Campidoglio para conmemorar la fundación de Roma. En esa ocasión, el famoso cañón de Gianicolo permanece silencioso, el único día del año en que no suena.





</doc>
<doc id="22187" url="https://es.wikipedia.org/wiki?curid=22187" title="Iósif Stalin">
Iósif Stalin

Iósif Vissariónovich Dzhugashvili, más conocido como Iósif Stalin (; Gori, -Moscú, 5 de marzo de 1953), fue un político y dictador soviético, secretario general del Comité Central del Partido Comunista de la Unión Soviética entre 1922 y 1952, y presidente del Consejo de Ministros de la Unión Soviética entre 1941 y 1953. Aunque inicialmente presidía un liderazgo colectivo como primero entre iguales, hacia los años 30 ya se había convertido en dictador "de facto" de la Unión Soviética. 

Estuvo entre los bolcheviques revolucionarios que impulsaron la Revolución de Octubre en Rusia en 1917 y más tarde ocupó la posición de secretario general del Comité Central del Partido Comunista de la Unión Soviética desde 1922 hasta que el cargo fue formalmente suprimido en 1952, poco antes de su muerte. En mayo de 1924, después del XII Congreso del Partido Comunista de la Unión Soviética, Stalin pidió que se le permitiera dejar el cargo. Esta petición fue rechazada unánimemente, incluyendo a sus detractores. Volvió a formular esta petición tres veces más, en 1926, 1927 y 1952; las tres fueron rechazadas y tuvo que permanecer en el cargo. Mientras que el cargo de secretario general era oficialmente electivo y no se lo consideraba como la máxima posición dentro del Estado soviético, Stalin logró utilizarlo para acaparar cada vez más poder en sus manos tras la muerte de Vladímir Lenin en 1924 y para sofocar gradualmente a todos los grupos opositores dentro del Partido Comunista. Esto incluyó a León Trotski, un teórico socialista y el principal crítico de Stalin entre los primeros líderes soviéticos, que fue primero desterrado de la Unión Soviética en 1929 y luego asesinado en México en 1940 por orden de Stalin. En tanto que Trotski fue un exponente de la revolución mundial, fue el concepto de Stalin de socialismo en un solo país el que se convirtió en principal enfoque de la política soviética. 

En 1928, Stalin reemplazó la Nueva Política Económica de la década de 1920 por una economía planificada muy centralizada y por planes quinquenales que iniciaron un período de rápida industrialización y de colectivización económica en el campo. Como resultado, la Unión Soviética pasó de ser una sociedad mayoritariamente agraria a una gran potencia industrial, siendo ésta la base de su aparición como segunda mayor economía del mundo después de la Segunda Guerra Mundial. Como resultado de los rápidos cambios económicos, sociales y políticos de la época estalinista, millones de personas fueron enviadas a campos de trabajo del Gulag como castigo, y millones fueron deportadas y exiliadas a zonas remotas de la Unión Soviética. La agitación inicial en el sector agrícola interrumpió la producción de alimentos en la década de 1930 y contribuyó a la catastrófica hambruna soviética de 1932-1933. En 1937, una campaña contra supuestos enemigos de su gobierno culminó en la Gran Purga, un período de represión masiva en el que cientos de miles de personas fueron ejecutadas, e incluso fueron condenados líderes del Ejército Rojo acusados de participar en complots para derrocar el gobierno soviético.

En agosto de 1939, tras el fracaso para establecer una alianza anglo-franco-soviética, la Unión Soviética de Stalin firmó un pacto de no agresión con la Alemania nazi que dividió sus esferas de influencia en Europa Oriental. Este pacto permitió que la Unión Soviética recuperase algunos de los antiguos territorios del Imperio ruso con la invasión soviética de Polonia de 1939, la guerra de Invierno en Finlandia, la ocupación de las Repúblicas bálticas, y la ocupación soviética de Besarabia y el norte de Bucovina durante la Segunda Guerra Mundial. Pero después de que Alemania violara el pacto al invadir la Unión Soviética con la Operación Barbarroja en 1941, se abrió un Frente Oriental y la Unión Soviética se unió a los Aliados. A pesar de grandes pérdidas humanas y territoriales en el período inicial de la guerra, la Unión Soviética logró detener el avance del Eje en la batalla de Moscú y la batalla de Stalingrado. Finalmente, el Ejército Rojo avanzó a través de Europa en 1944-45 y capturó la capital del Tercer Reich tras la batalla de Berlín en mayo de 1945. Habiendo jugado el papel decisivo en la victoria aliada, la Unión Soviética surgió como una superpotencia reconocida después de la guerra.

Stalin encabezó las delegaciones soviéticas en las conferencias de Yalta y Potsdam, en las que se trazó el mapa de la Europa de posguerra. En los Estados satélites del Bloque del Este se instalaron gobiernos de izquierda leales a la Unión Soviética. En esa época, la Unión Soviética había entrado en una lucha por el dominio global, conocida como la Guerra Fría, con los Estados Unidos. En Asia, estableció buenas relaciones con Mao Zedong en China y Kim Il-sung en Corea del Norte y de diversas maneras, la Unión Soviética de la era estalinista sirvió como modelo para la recién formada República Popular de China y la República Popular Democrática de Corea.

Al mantenerse en el poder hasta su muerte en 1953, Stalin dirigió la Unión Soviética durante el período de reconstrucción de la posguerra, marcado por el predominio de la arquitectura estalinista. El desarrollo exitoso del programa nuclear soviético permitió que el país se convirtiese en la segunda potencia mundial en armas nucleares. También se inició el programa espacial soviético. En sus últimos años, Stalin lanzó los denominados Grandes Proyectos de Construcción del Comunismo y el Gran Plan para la Transformación de la Naturaleza.

Tras su muerte, Stalin y su régimen han sido condenados en numerosas ocasiones. La más significativa de estas condenas se dio durante el XX Congreso del Partido Comunista de la Unión Soviética en 1956, cuando su sucesor, Nikita Jrushchov, denunció su legado en una famosa intervención con la que se inició un proceso de desestalinización de la Unión Soviética. Las visiones modernas de Stalin en la Federación de Rusia siguen siendo mixtas, con algunas personas viéndolo como un tirano y otras como un líder capaz. Fue nominado al Premio Nobel de la Paz de 1945 y 1948.

El nombre Stalin («hecho de acero»; derivado del ruso "stal", acero, con el mismo sufijo posesivo personal "in" que usó Lenin) empezó a usarlo a partir de 1912, y desde octubre de 1917 se convirtió en su sobrenombre. Familiarmente, y entre sus camaradas más cercanos, era conocido como "Sosó", e incluso llegó a utilizar el pseudónimo "Soselo" para firmar sus poemas. También, se refería a sí mismo como "Koba", nombre de un héroe popular de Georgia. Otros nombres que utilizó fueron David, Morti, Nijaradze, Chizhikov, Ivánovich.

Iósif Stalin nació el 18 de diciembre de 1878 en Gori, Gobernación de Tiflis del Imperio ruso (Georgia en la actualidad). Su padre, Vissarión Dzhugashvili ("Besó"), trabajaba de zapatero, y posteriormente en una fábrica de zapatos, y su madre, Yekaterina Gueladze ("Keke"), era sirvienta, siendo ambos de familias de siervos georgianos. El padrino de la boda fue Yákov Egnatashvili ("Koba"), mercante local, y uno de los asistentes fue el padre Christopher Charkviani, ambos desmpeñarían más tarde un papel significativo en la infancia de Iósif. Vissarión dejó su empleo asalariado y abrió un pequeño taller con la ayuda económica de sus amigos, incluido Egnatashvili. Desde el principio del matrimonio, corrió el rumor en Gori de que Yekaterina era una mujer promiscua. Estos rumores serían, más adelante, una fuente de inestabilidad en el matrimonio y darían lugar, a su vez, al cuestionamiento de la paternidad de Iósif. Yekaterina quedó embarazada y nueve meses después, el nació su primer hijo, Mijaíl. Dos meses después, Mijaíl murió y Vissarión empezó a beber. Yekaterina quedó embarazada de nuevo y el nació su segundo hijo, Guiorgui, que murió de sarampión con apenas seis meses. Finalmente, el nació su tercer hijo: el pequeño Iósif (de diminutivo "Sosó" o "Soselo").

Egnatashvili fue el padrino de los dos primeros hijos. Sin embargo, Vissarión decidió que no fuese padrino de Iósif para intentar evitar la mala fortuna de nuevo. La familia alquiló una casa de una habitación a un artesano osetio en el barrio ruso de Gori, cerca del cuartel de las tropas imperiales para las que Vissarión trabajaba. La casa tenía un sótano dónde Vissarión guardaba sus herramientas y Yekaterina acondicionó como cuarto del bebé. Yekaterina no producía leche suficiente, por lo que las mujeres de su padrino, Tsijatatrishvili, y Egnatashvili ayudaban a amamantarlo. Además, Iósif tenía una salud frágil, nació con sindactilia (dedos unidos por una membrana) en dos dedos del pie y a partir de los dos años padeció sarampión y escarlatina. Poco después, la situación de la familia comenzó a mejorar. Vissarión incorporó a dos aprendices para su taller y uno de ellos, Vanó Jutsishvili, se convirtió en un hermano de acogida para Iósif.

El problema de Vissarión con la bebida empeoró. Durante 1883 empezó a meterse en peleas de borrachos y fue apodado el "Loco Besó". Vissarión se fue sumiendo en un estado de paranoia por los rumores que cuestionaban la paternidad de Iósif y empezó a tener un comportamiento agresivo contra su mujer Yekaterina y su hijo Iósif.

En torno a 1884, hubo una epidemia de viruela en Gori. Iósif contrajo la enfermedad y sobrevivió, aunque su cara quedó marcada de por vida. Ese mismo año, Vissarión vandalizó el bar de Egnatashvili y atacó al jefe de policía Davrishevi. Davrishevi ordendó a Vissarión abandonar Gori y este se fue a Tiflis a trabajar en la fábrica armenia de zapatos Adeljánov.

En 1886, Yekaterina y Iósif se mudaron al piso de arriba de la casa del Padre Charkviani, antiguo compañero de bebida de Vissarión. Yekaterina pidió a Charkviani que admitiese a Iósif en la escuela de la iglesia de Gori en el curso que empezaba en otoño. Charkviani no accedió a ello, pero permitió que Iósif estuviese presente en las clases de ruso que los hijos adolescentes de Charkviani daban a su hermana pequeña. En 1888, Iósif ingresó en la escuela parroquial para hacer el programa educativo obligatorio en Georgia, de dos años. Su nivel de ruso por entonces le permitió acabar el programa en un año y, en 1889, comenzó su educación formal en un programa de cuatro años, donde destacó como estudiante y por su voz al cantar.

El 6 de enero de 1890, durante la celebración de la epifanía, Iósif fue atropellado por un faetón. Esto afectó a su forma de andar. Vissarión llevó a Iósif a Tiflis para que recibiera tratamiento médico. Una vez Iósif estuvo recuperado, Vissarión lo llevó a la fábrica Adeljánov para que trabajase con él como aprendiz. Esto era común en Tiflis, donde muchos trabajadores llevaban a sus hijos a las fábricas.

Mientras tanto, Yekaterina presionó a sus conexiones en la Iglesia ortodoxa para que Iósif se incorporase a la escuela al comienzo del siguiente curso escolar en septiembre de 1890. Finalmente lo consiguió y Iósif volvió a Gori para ir a la escuela, a pesar de que Vissarión se negó a ayudar económicamente. Poco después, Iósif fue expulsado por el impago de la matrícula de 25 rublos. Yákov Egnatashvili pagó la deuda y se convirtió en un padre sustituto para Iósif. En ese periodo, Iósif comenzó a hacerse llamar "Koba". Desde entonces su pseudónimo más conocido después de "Stalin". Koba era el nombre de un montañés legendario de Georgia, protagonista de la novela "El Parricida", y también el diminutivo de Yákov Egnatashvili.

Iósif fue un estudiante ejemplar y por ello se le concedió una beca para sus estudios, por lo que ya no debía pagar los costes de la matrícula. Finalmente, se graduó en la primavera de 1894 con 15 años. El maestro del coro ofreció a Iósif acompañarle a la escuela de maestros del zar Alejandro en Tiflis. Sin embargo, Iósif decidió ingresar en el seminario ortodoxo de Tiflis.

La relación de Stalin con el movimiento revolucionario comenzó en el seminario. Durante estos años de escuela, Stalin se unió a la organización socialdemócrata de Georgia, en la que fue instruido en política marxista por el profesor Noe Zhordania (quien después sería Jefe de Gobierno de la República Democrática de Georgia) y comenzó a difundir el marxismo. Fue un responsable del sindicato de Georgia durante tres años y luego portavoz del nuevo partido marxista georgiano. Algunas fuentes afirman que Iósif abandonó el seminario en 1899 justo antes de sus exámenes finales; según otras biografías, fue expulsado.

Inicia su militancia en torno al círculo de obreros ferroviarios de Tiflis, alejándose definitivamente de Zhordania.

Junto a otros jóvenes intenta editar un periódico propio clandestino, sin lograrlo. Solamente editaron octavillas que reparten en las fábricas, con claro contenido político. El Primero de Mayo de 1900 organiza la primera manifestación de masas, reuniendo a 500 obreros en los alrededores de Tiflis con banderas rojas y retratos de Marx y Engels.

En agosto de 1900 entra en contacto con Víktor Kurnativski, uno de los "iskristas" que envía Lenin a Tiflis para impulsar la difusión del periódico que debía conducir a la reorganización del Partido y a la lucha contra las tendencias economicistas y conciliadoras. Kurnativski les enseñó a aquellos jóvenes georgianos cómo montar una imprenta clandestina y les propuso que lo hicieran en Bakú, un fuerte centro proletario, mejor que en Tiflis. En marzo de 1901, Kurnativski es detenido junto con otros militantes, pero Koba Dzhugashvili se libra de la redada, aunque su vivienda y su lugar de trabajo en el observatorio meteorológico fue registrada por la Ojrana, la sección especial de la policía zarista dedicada a la represión política. Tiene que pasar a la clandestinidad, de la que ya no saldrá hasta la Revolución de 1917.

En 1901, el clérigo georgiano M. Kelendzheridze escribió un libro educacional sobre lengua y arte, incluyendo uno de los poemas de Stalin firmado como «Soselo». En 1907, el mismo editor publicó "Antología georgiana, o Colección de los mejores ejemplos de literatura georgiana", donde incluía un poema de Stalin dedicado a Rafael Eristavi. Su poesía aún puede ser vista en el museo Stalin de Gori.

Trabajó durante diez años con los movimientos políticos clandestinos en el Cáucaso, sufriendo repetidos arrestos y exilio a Siberia, entre 1902 y 1917.

Stalin se adhirió a la doctrina de Lenin de un partido centralista fuerte, de revolucionarios profesionales. En el período posterior a la Revolución de 1905, Stalin lideró los «escuadrones de lucha» en robos de bancos para reunir fondos para el partido bolchevique. Stalin asistió al V Congreso del Partido Obrero Socialdemócrata de Rusia en Londres en 1907. Este congreso consolidó la supremacía del sector bolchevique de Lenin y se debatió la estrategia para la revolución comunista en Rusia. Stalin nunca se refirió posteriormente a su estancia en Londres.

En 1913, mientras estuvo exiliado en Viena, Stalin escribió "El marxismo y la cuestión nacional", tratado en el que presenta una posición marxista ortodoxa (cfr. este trabajo con el de Lenin llamado "Sobre el derecho de los pueblos a la autodeterminación") y que pudo haber contribuido a su nombramiento como Comisario del Pueblo para Asuntos Nacionales después de la revolución.

En 1912, Lenin tuvo la intención de proponer la elección de Stalin al Comité Central bolchevique en la Conferencia del Partido en Praga, pero desistió al encontrarse con la resistencia del partido. Sin embargo, inmediatamente después, Stalin fue sumado al Comité Central por «cooptación» (potestad prevista por los estatutos, que reservaba para el Comité Central el derecho a sumar integrantes que no hubieran sido electos por el Congreso del Partido).
En 1917 Stalin era el editor de "Pravda", el diario oficial del partido, mientras Lenin y gran parte del liderazgo bolchevique estaban en el exilio. Después de la Revolución de Febrero, Stalin y el equipo editorial tomó una posición favorable al gobierno provisional de Kérenski y se sostiene que llegó al extremo de negarse a publicar artículos de Lenin que llamaban al derrocamiento del gobierno provisional.

En abril de 1917, Stalin fue por primera vez electo por la base del partido para formar parte del Comité Central, obteniendo la tercera más alta mayoría de votos en la Conferencia de Petrogrado (detrás de Lenin y Zinóviev). Posteriormente fue nombrado secretario del Politburó del Comité Central (mayo de 1917); se mantuvo en este cargo por el resto de su vida. Al finalizar julio presentó el informe central al VI Congreso del partido, en el cual se optó por la insurrección contra el gobierno provisional.

Según diversas fuentes, Stalin solamente desempeñó un papel menor en la Revolución de Octubre. Algunos autores, como Adam Ulam, remarcan que cada hombre en el Comité Central tenía una labor específica que le había sido asignada.

El siguiente resumen respecto al papel de Trotski en 1917 fue escrito por Stalin en "Pravda" 16 de noviembre de 1918:
Posteriormente, en 1924, el mismo Stalin creó un mito referente a la así llamada «Central del Partido», de la cual supuestamente dirigía todo el trabajo práctico referente a la revuelta y que consistía en un grupo integrado por él mismo, Sverdlov, Dzerzhinski, Uritski y Búbnov. Ninguna evidencia se ha encontrado, sin embargo, respecto a las actividades de esta Central, que en cualquier caso, de haber existido, habría estado subordinada al Comité Militar Revolucionario comandado por Trotski.

Durante la guerra civil rusa y la guerra polaco-soviética, Stalin fue comisario político en el Ejército Rojo en diversos frentes. El primer cargo de gobierno de Stalin fue el de Comisario del Pueblo de Asuntos Nacionales (1917-1923).

Tuvo también el cargo de comisario del Pueblo para la Inspección de los Trabajadores y Campesinos (1919-1922), de miembro del Sóviet Militar Revolucionario de la República (1920-1923) y miembro del Comité Central Ejecutivo del Congreso de los Sóviets a partir de 1917.

El 3 de abril de 1922, Stalin fue nombrado Secretario General del Partido Comunista Panruso, un cargo que él posteriormente transformó en el más poderoso del país. En aquella época, esta posición se veía como un cargo menor dentro de la estructura partidaria (ocasionalmente en el partido se referían a Stalin como el «camarada archivista»), sin embargo este cargo asociado con el liderazgo que tenía sobre la Oficina Organizativa del Comité Central del Partido (Orgburó), dio a Stalin una base de poder suficientemente fuerte como para permitirle instalar a sus aliados en los puestos claves del partido.

La acumulación de poder por parte de Stalin tomó al moribundo Lenin por sorpresa, quien, en sus (Testamento de Lenin), hizo llamamientos para que el XII Congreso del Partido Bolchevique apartara al «brusco» Stalin. Sin embargo, estos intentos no prosperaron debido a que los documentos preparados por Lenin fueron ocultados por Stalin y sus eventuales aliados, a sabiendas de que Lenin se encontraba en esos momentos enfermo e imposibilitado de participar en el Congreso.

Después de la muerte de Lenin en enero de 1924, Stalin, Kámenev y Zinóviev tomaron el control del partido situándose en un punto que ideológicamente estaba entre Trotski (a la izquierda del partido) y Bujarin (a la derecha). Durante este período, Stalin abandonó el tradicional énfasis bolchevique respecto a la «revolución mundial» en favor de una política de construir el «socialismo en un solo país», en contraste a la teoría de Trotski de la «revolución permanente».

En la lucha por el liderazgo una cosa era evidente: quien terminara comandando el Partido tenía que ser considerado muy leal a Lenin. Por eso, la actitud de cada uno ante su muerte fue determinante en los posicionamientos dentro del Partido: Stalin organizó su funeral y pronunció un discurso manifestando una lealtad imperecedera con Lenin, a la vez que impidió mediante engaños que Trotski asistiera. Stalin también acusó a Trotski de haberse unido a los bolcheviques justo antes de la revolución, e hizo públicos los desacuerdos que este había tenido con Lenin en la etapa previa a la revolución.

Las imágenes soviéticas correspondientes a este período fueron posteriormente trucadas, eliminando con fotomontajes y técnicas similares a los opositores a Stalin (principalmente Trotski).

La base fundamental del ascenso al poder de Stalin fue el control del aparato administrativo del Estado, en un país en el cual la escasez era la regla, tras la Primera Guerra Mundial y la Guerra Civil. A su vez, la política de Stalin de pregonar el llamado «socialismo en un solo país» era visto como un antídoto optimista con respecto a la guerra, en contraste a la posición de la «revolución permanente» de Trotski.
El método de Stalin era la designación de secretarios que le respondieran personal e incondicionalmente, y la manipulación de sus oponentes logrando poner a unos contra los otros, usando el método de "dividir para gobernar".

Inicialmente, Stalin formó una "troika" junto a Zinóviev y Kámenev contra Trotski. Una vez que Trotski había sido eliminado de la pugna por el poder político, Stalin se unió con Bujarin y Rýkov contra Zinóviev y Kámenev, recordando a todos el voto de estos últimos contra la insurrección en 1917. Zinóviev y Kámenev entonces, se unieron con la viuda de Lenin, Nadezhda Krúpskaya, formando la "oposición unida" en julio de 1926.

En 1929, durante el XV Congreso del Partido Comunista de la Unión Soviética (PCUS), Trotski y Zinóviev fueron expulsados del partido y Kámenev perdió su puesto en el Comité Central. Stalin pronto se volvió contra la "oposición de derecha" representada por sus aliados del momento, Bujarin y Rýkov.

Uno de los argumentos predilectos de Stalin para atacar a otros miembros del Partido, fue la lucha contra la existencia de facciones, que habían sido prohibidas temporalmente en el Partido Bolchevique durante la Guerra civil rusa, pero que formaban parte de la historia del bolchevismo.

Habiendo también derrotado a la «oposición de derecha» de Bujarin, Stalin comenzó los planes de colectivización e industrialización. En este camino es de destacar la "deskulakización", que trajo como consecuencia la expropiación masiva de las tierras explotadas por medianos propietarios agrícolas ("kulaks"), lo cual causó una reducción de la producción de cereales, lo que unido a unas malas condiciones ambientales dio lugar a una gran hambruna en Ucrania que supuso la muerte de varios millones de ucranianos (ver Holodomor); según el gobierno soviético, «fue una medida necesaria para acabar con la retención y sabotaje de productos que ilegalmente practicaban los kulaks». Los muertos por la hambruna ascendieron a un número difícil de determinar.

Serguéi Kírov había conocido a Stalin en mayo de 1918. Durante la Guerra civil rusa se enfrentó a Trotski, lo que hizo que se alineara con Stalin, Ordzonikidze y Voroshílov.

Desde 1926 estuvo trabajando en Leningrado, pero tras ser elegido para el Comité Central en el XVI Congreso del PCUS, Stalin le propuso volver a Moscú. Sin embargo, Kírov pidió permanecer en Leningrado, y se le permitió quedarse hasta el final del segundo plan quinquenal. No están claras las razones por las que declinó este ascenso.

En el XVII Congreso del PCUS de 1934, al elegirse el nuevo Comité Central, Kírov recibió tres votos negativos, resultando ser el candidato menos rechazado, en contraste con el propio Stalin que recibió 292 votos negativos, siendo el menos popular.

Dumaskin afirma que Kírov se opuso a Stalin en el Politburó en 1934, lo que produjo «una perceptible tirantez entre Stalin y Kírov». Distintos autores han dado cuenta de la existencia de una conspiración en la cúspide del PCUS cuyo fin habría sido reemplazar a Stalin con Kírov.

El 1 de diciembre de 1934, Kírov fue asesinado por Leonid Nikoláev en Leningrado. La dirigencia del Estado soviético declaró que Nikoláev había sido apoyado por Trotski desde el exilio. Esto dio comienzo a una purga generalizada, con cientos de ejecuciones, encarcelamientos y reclusiones en campos de concentración del Gulag, acusando al bloque trotskista-zinovievista de estar organizando una extensa conspiración con el objetivo de tomar el poder en la Unión Soviética. Como parte de este proceso, Kámenev y Zinóviev fueron sometidos a juicio público y, tras confesar supuestos crímenes (confesión que según algunos habría sido producto de torturas), fueron ejecutados en 1936. Con mecanismos similares, en menos de dos años terminaría siendo ejecutada la mayoría de los miembros del Comité Central bolchevique que había dirigido la Revolución de Octubre de 1917 ("viejos bolcheviques"), mientras Trotski sería asesinado en la ciudad de México en agosto de 1940 por Ramón Mercader, un agente estalinista.

La hipótesis acerca del vínculo de Stalin con este asesinato estuvo ampliamente difundida, siendo confirmada por Nikita Jrushchov en sus memorias. Sin embargo, no existen pruebas concluyentes al respecto.

En 1937, Wilhelm Canaris, jefe de la inteligencia militar alemana, captura información proveniente de un general ruso disidente, llamado Nikolái Skoblin, en la que se asegura que existe una intriga combinada de oficiales rusos y alemanes decididos a derrocar a Stalin. Reinhard Heydrich supo de esta información (ya que tenía agentes infiltrados en la Abwehr), y valiéndose de una operación encubierta de inteligencia, roba esta documentación de las oficinas de la Abwehr, incendiándola después para no dejar rastros.
La documentación fue manejada hábilmente por Hitler con la ayuda de Heydrich y ocasionaron la purga en el Ejército Rojo, con la eliminación de más de 3000 oficiales, entre ellos Mijaíl Tujachevski, máximo exponente de la guerra mecanizada en la Unión Soviética.

Stalin también incrementó ampliamente las actividades de inteligencia extranjera de la NKVD. Bajo sus instrucciones, la inteligencia soviética comenzó a crear redes de información en la mayoría de los países del mundo, incluyendo Alemania, Gran Bretaña, Francia, Japón y los Estados Unidos. Stalin hizo un gran uso de la Internacional Comunista con el fin de infiltrar agentes.

La Primera Guerra Mundial, la guerra civil rusa, la intervención por parte de 14 potencias extranjeras luego de la toma del poder por los bolcheviques y la misma revolución, tuvieron un efecto devastador en la economía del país.

La producción industrial de 1922 fue un 13 % menor que la de 1914. Bajo la Nueva Política Económica (NEP), impulsada por Lenin ante la situación apremiante, que permitía cierto grado de flexibilidad en el mercado dentro del contexto del socialismo, se produjo una recuperación. Agotada la NEP, esta política fue reemplazada por un sistema centralizado y sujeto a los planes quinquenales a partir de 1928. Estos planes perseguían ambiciosos programas de industrialización y de colectivización y estatización de la agricultura.

El objetivo de la industrialización era tanto reacondicionar las viejas fábricas y empresas industriales, de tecnología atrasada y en estado de práctico abandono, como construir una poderosa industria pesada.
La industrialización era considerada fundamental en la construcción del socialismo, ya que garantizaría la alianza obrera-campesina como base de la dictadura del proletariado, la defensa de la Unión Soviética y elevaría notablemente el nivel de vida de la población.

Sin capitales iniciales, escaso comercio internacional y virtualmente sin infraestructura moderna, el gobierno de Stalin financió la industrialización a partir de la ganancia obtenida por las fábricas y empresas del Estado, por el comercio, los bancos y el transporte.

En 1926-1927, se invirtieron en la industria cerca de mil millones de rublos; tres años después, se pudieron invertir ya en ella unos 5000 millones.

La década de 1930 consiguió la producción por primera vez en la historia de la Unión Soviética, de una amplia gama de nuevos productos, entre los cuales se destacaban motocicletas, relojes y cámaras fotográficas, como asimismo las máquinas y herramientas necesarias para producir estos y otros bienes. En la industria química se produjo el desarrollo de la industria de los plásticos, en metalurgia se desarrollaron nuevos tipos de aleaciones de alta calidad y diversos metales no ferrosos fueron manufacturados por primera vez.

También mejoró notoriamente la escala y la eficiencia con la cual se fabricaban los productos existentes. En la industria del hierro y del acero, hacia fines de la década de 1930, el tamaño promedio de los nuevos hornos de fundición era un 40 % mayor con respecto a aquellos de solo 10 años antes. Muchas innovaciones estaban basadas exclusivamente en desarrollos técnicos locales. En la industria aeronáutica, por ejemplo, los ingenieros soviéticos produjeron aviones que eran comparables a diseños extranjeros; en la industria militar, por su parte, se desarrollaron tanques que no tenían equivalentes en el mundo occidental. La Unión Soviética fue también el primer país en producir goma sintética de polibutadieno.

El gobierno de Stalin promovió la colectivización de la agricultura con el fin de aumentar la producción agrícola a partir de granjas mecanizadas en gran escala, lo que permitía mantener a los campesinos bajo un control político más directo y para que la recaudación de impuestos fuera más eficiente. La colectivización significó cambios sociales drásticos en una escala nunca vista desde la abolición de la servidumbre en 1861.

La colectivización forzada de la agricultura comenzó a inicios de los años 1930, formándose la asociación obligatoria de todas las granjas en los llamados "koljós" (o granja colectiva), una estructura fuertemente centralizada. La supresión de los derechos de propiedad sobre la tierra fue una consecuencia de la forma como se decidió resolver el antiguo conflicto de la lucha de clases. Además, de acuerdo a la visión económica de la época, los koljós debían trabajar con mayor eficiencia debido a la aplicación de tecnología y a la división del trabajo. En los primeros años de la colectivización se estimaba que la producción agrícola e industrial debería aumentar un 200 % y un 50 % respectivamente; sin embargo la producción agrícola disminuyó.

Los campesinos ricos, los llamados kulaks, con independencia de si resistían o no los cambios impuestos y la colectivización, eran puestos a trabajar directamente en los campos, o bien eran trasterrados a Siberia y al oriente del país.

La política de industrialización de la agricultura seguida por Stalin requirió grandes cantidades de equipamiento y maquinaria, que se consiguió al exportar trigo y otros bienes agrícolas al extranjero. Los koljós fueron obligados mediante planes específicos a entregar al Estado su producción agrícola. Estas medidas trajeron como consecuencia una drástica caída en la calidad de vida de los campesinos y la producción agrícola.

Para evitar el aislamiento del régimen soviético, decidió la entrada de la Unión Soviética en la Sociedad de Naciones (1934), y la aproximación a Gran Bretaña y Francia. En política interior trató de eliminar cualquier tipo de oposición: entre 1936 y 1938 organizó procesos (procesos de Moscú) y deportaciones contra los principales mandos militares y contra toda oposición en el seno del Partido y del Estado. Basándose en los datos suministrados tras la "perestroika", documentados por el Gulag, fueron detenidas más de personas por motivos políticos. De ellas casi 700000 fueron fusiladas. Durante su gobierno inició un controvertido programa para "rusificar" a las diferentes repúblicas de la URSS, enviando rusos a las distintas repúblicas soviéticas para que se casaran con los locales y así aumentar el porcentaje de rusos en la región.

Por otra parte, ya durante el primer período estalinista, antes incluso de la década de 1930, amplios sectores de la sociedad soviética aceptaron con optimismo los grandes avances de la Revolución. Rusia era el único país del mundo donde a las mujeres se les pagaba lo mismo que a los hombres por un trabajo similar. También en este primer período, existían grandes facilidades para obtener un divorcio o abortar.

El 23 de agosto de 1939, la Unión Soviética y la Alemania nazi firmaron en Moscú un pacto de no agresión, luego denominado Pacto Ribbentrop-Mólotov, en cuyo Protocolo adicional secreto se dividía Europa Oriental y Central en esferas de influencia soviética y alemana, estableciendo también directrices para la partición de Polonia entre ambos Estados. También en ese protocolo se concedió a Stalin carta blanca para intervenir en Finlandia y en los países bálticos.

Tras la invasión soviética de Polonia producida el 17 de septiembre de 1939, a partir de una propuesta oficial del jefe del NKVD Lavrenti Beria, fechada el 5 de marzo de 1940, Iósif Stalin y otros cinco miembros del Politburó aprobaron la ejecución de prisioneros polacos. Se estima que fueron víctimas de la Masacre de Katyn al menos 21.768 ciudadanos polacos. La Unión Soviética negó las acusaciones hasta 1990, cuando el gobierno de Mijaíl Gorbachov reconoció que el NKVD fue responsable de la matanza y su encubrimiento y entregó una parte de los documentos desclasificados, declarando que la Masacre de Katyn constituye "uno de los graves crímenes del Estalinismo" ("одно из тяжких преступлений сталинизма").

Transcurrido más de un año desde el inicio de la Segunda Guerra Mundial, considerando que la caída del Reino Unido era inminente, Hitler decidió atacar a la Unión Soviética, haciendo del Pacto letra muerta. El 18 de diciembre de 1940, el mando alemán tomó la decisión de invadir la Unión Soviética (Operación Barbarroja) en abril de 1941, aunque finalmente se llevaría a cabo el 22 de junio de ese año, cuando se inició el ataque a territorio soviético con más de de soldados alemanes. La invasión tomó por completa sorpresa a Stalin, a pesar de que tenía suficientes indicios proporcionados por el espía Richard Sorge de que ésta era inminente.

Producida la invasión, Stalin se encerró en la dacha de Kúntsevo, su residencia oficial en las afueras de Moscú, y sin acudir a su puesto de trabajo en el Kremlin de Moscú en una aparente depresión y falta de liderazgo. Solo reaccionó 10 días más tarde, para retomar el control con mano firme. Tuvo que ser Viacheslav Mólotov el que, el 22 de junio de 1941, notificara por radio la invasión nazi a los soviéticos. Sólo el 3 de julio, Stalin pudo dirigirse al pueblo soviético con una declaración difundida por radio a todo el país.

Desesperado por la invasión germánica, Stalin decidió suspender la campaña ateizante y permitir el resurgimiento de la Iglesia ortodoxa rusa, para que el pueblo soviético creyente se uniera a la lucha, "olvidando" por un tiempo el obligado ateísmo del PCUS. Increíblemente y en forma insospechada para los alemanes, el pueblo ruso se unió en defensa de su patria.

El Ejército Rojo, muy debilitado por las purgas de fines de la década de 1930, se encontraba virtualmente sin mando competente, por lo que las fuerzas alemanas avanzaron rápidamente por las llanuras occidentales de la Unión Soviética. Hitler predecía que la guerra con el gigante ruso duraría a lo más seis meses y que el pueblo ruso mismo eliminaría a Stalin.
Stalin se hizo nombrar presidente del Consejo de Comisarios del Pueblo con lo que en la práctica se convirtió oficialmente en el jefe del Estado.

Las medidas iniciales de Stalin por contener la invasión alemana fueron ineficaces y no pudieron detener el avance de las fuerzas blindadas de Hitler que penetraban profundamente en territorio soviético. Si bien en un comienzo Stalin se mostró dubitativo e irresoluto por el súbito y contundente ataque de los alemanes, pronto empezó a tomar el control de la situación y se autonombró supremo comandante en jefe del Ejército Rojo.

A diferencia de Hitler, Stalin dio cierta autonomía a sus generales en la toma de decisiones e hizo traer desde la frontera a algunos de sus mejores generales, como Zhúkov y Vatutin, permitiendo además el envío desde los frentes orientales de miles de tropas siberianas entrenadas ya en el combate con los japoneses.

Durante la Batalla de Smolensk, su hijo Yákov Dzhugashvili fue capturado: Stalin supo de esta situación pero permaneció indiferente a la suerte corrida por su hijo. Yákov permaneció anónimo en el campo de concentración de Sachsenhausen hasta que fue delatado. Se lo intentó adoctrinar para la propaganda alemana pero no cambió de bando. Entonces se decidió su canje por el mariscal Friedrich Paulus, pero Stalin se negó.
Yákov moriría en extrañas circunstancias el 15 de abril de 1943 en el mismo campo. Stalin jamás demostró públicamente algún tipo de consideración por la suerte corrida por Yákov.

Se mantuvo en Moscú en el invierno de 1941, cuando los alemanes amenazaban la ciudad (42 km), y organizó allí un contraataque soviético.
Al año siguiente, 1942, tuvo éxito al mantener la estratégica ciudad de Stalingrado, última defensa de la zona petrolera del Cáucaso, pese a la enorme cantidad de bajas entre sus hombres (Stalin, a través de sus comisarios políticos, ordenó disparar contra sus propios soldados si estos se retiraban de un combate al considerarlos desertores) y posteriormente (1943) también derrotó al ejército alemán en la batalla de Kursk con lo que todo el curso de las acciones militares tuvo un cambio, siendo ahora los soviéticos los que obligaban a retirarse a los alemanes. 

En su papel de comandante en jefe, Stalin procuraba siempre mantener un control personal pero flexible en el mando, sobre todo el frente de batalla, las reservas militares y la economía de guerra. Esta actitud no se mostró eficaz, ya que dejaba en un solo hombre todas las decisiones, pero luego Stalin fue aprendiendo de sus errores y empezó a delegar decisiones militares al contrario de su rival, Hitler, quien monopolizó el mando.

Como jefe de Estado, Stalin participó en varios encuentros con los líderes aliados, como el llamado de "los tres grandes", con Winston Churchill y Franklin D. Roosevelt en Yalta y en Potsdam (ambas en 1945), en las que logró el reconocimiento internacional de una esfera de influencia soviética en la Europa del Este y mostrándose como un formidable negociador según el propio secretario del exterior británico, "sir" Anthony Eden. Asimismo, el 4 de septiembre de 1943, se reunió con tres metropolitas de la Iglesia para restablecer el Santo Sínodo y convocar al Concilio Episcopal para elegir como Patriarca de Moscú a uno de los tres anteriores (Serguéi) cinco días después, por primera vez en diecisiete años, desde 1925.

Un hecho de este período que refleja su «culto a la personalidad» es que se autoconcedió el honor de Héroe de la Unión Soviética, a pesar de que este solo lo recibían los soldados en combate. Se sentía amenazado por la popularidad de Zhúkov, al que acusó de usar ese triunfo a su favor y lo terminó degradando.

Zhúkov dijo de Stalin en sus memorias:

Al finalizar la Segunda Guerra Mundial, Stalin fue visto como el gran líder que había conducido al pueblo soviético a la victoria en su lucha contra la Alemania nazi. A finales de la década de los años 40, el patriotismo ruso fue en ascenso debido a los éxitos propagandísticos. Por ejemplo, algunas invenciones y descubrimientos científicos fueron reclamados por la propaganda rusa. Ejemplos de ello son la máquina de vapor, reclamada por el padre y el hijo de la familia Cherepánov; la lámpara incandescente, por y Lodyguin; la radio, por Popov; y el avión, por Mozhaiski. Continuaron sus políticas represivas (incluso en los territorios recién anexionados), pero nunca llegaron a los extremos de la década de 1930.

Internacionalmente, Stalin vio la consolidación del poder como un paso necesario para proteger a la Unión Soviética, rodeándolo de gobiernos amistosos, como un cordón sanitario contra posibles invasiones, mientras que Occidente buscó un modelo similar de protección contra la expansión comunista. Estas políticas condujeron a una estabilidad, donde el éxito de la influencia soviética dependería de la cooperación entusiasta de las naciones satélite.

Stalin había tenido la esperanza de que la retirada y la desmovilización de los Estados Unidos darían lugar a un aumento de la influencia comunista, especialmente en Europa. Cada una de las partes veía las acciones defensivas de la otra como provocaciones desestabilizadoras y estos dilemas de seguridad desgastaron las relaciones entre la Unión Soviética y sus ex aliados occidentales de la Segunda Guerra Mundial y dio lugar a un prolongado período de tensión y la desconfianza entre el Este y Occidente, conocido como la Guerra Fría ("véase Telón de acero").

El Ejército Rojo terminó de manera exitosa la Segunda Guerra Mundial, ocupando gran parte del territorio que había sido ocupado anteriormente por los países del Eje.
En Asia, el Ejército Rojo invadió Manchuria en el último mes de la guerra y también tomó el control de Corea cerca de Paralelo 38. En China, Mao Zedong, del Partido Comunista de China, receptivo a recibir el apoyo soviético, derrotó al pro-occidental y pro-estadounidense Partido Nacionalista Chino en la guerra civil china.

Los comunistas controlaban la mayor parte de China, mientras que los nacionalistas se refugiaron en un pequeño estado en la isla de Formosa (actualmente Taiwán). La Unión Soviética reconoció pronto las «hazañas» de Mao poco después de la fundación de la República Popular de China, que es considerada como un nuevo aliado. La República Popular reivindicó Taiwán, a pesar de que nunca ha celebrado su autoridad en la isla.

Las relaciones diplomáticas con China alcanzaron un punto culminante con la firma del Tratado Chino-Soviético de Amistad y Alianza en 1950. Ambos países proporcionaron apoyo militar a un nuevo Estado en Corea del Norte. En 1950, después de varios conflictos fronterizos, estalló la guerra entre el nuevo estado y los Estados Unidos y sus aliados de Corea del Sur, comenzando la guerra de Corea.

En Europa existían zonas de ocupación soviética, tanto en Alemania como en Austria. Hungría y Polonia estaban prácticamente ocupadas militarmente. Desde 1946 a 1948 fueron elegidos en Polonia, Checoslovaquia, Hungría, Rumania y Bulgaria gobiernos de coalición integrados por comunistas, así también los movimientos comunistas accedieron al poder en Yugoslavia y Albania.

Estas naciones se conocieron como el Bloque del Este o Bloque Comunista. Reino Unido y los Estados Unidos apoyaron la lucha contra los comunistas en la guerra civil griega y los soviéticos, sospechosos de apoyar a los comunistas griegos, aunque Stalin se abstuvo de involucrarse en Grecia, despidiendo a la circulación prematuramente. Albania siguió siendo un aliado de la Unión Soviética, pero Yugoslavia rompió con la Unión Soviética en 1948.

Ambas superpotencias vieron a Alemania como país clave. En represalia a la formación de la trizona occidental, Stalin decidió tomar medidas.

Gracias a la información del agente británico Donald Maclean y otros agentes de espionaje británicos y estadounidenses, Stalin era perfectamente conocedor de que los Estados Unidos no había procedido a la producción masiva de armas atómicas, de hecho, ni siquiera habían producido ninguna desde Nagasaki. Un gran número habría sido necesario para destruir a las fuerzas comunistas, ya fuera en Europa o el Lejano Oriente. Por lo tanto, ordenó un bloqueo en Berlín, que estaba bajo el dominio británico, francés, EE.UU. y la ocupación, a prueba de las potencias occidentales.

El bloqueo de Berlín fracasó debido a la masiva campaña de reabastecimiento aéreo, denominado Luftbrücke, llevado a cabo por las potencias occidentales. En 1949, Stalin reconoció la derrota y puso fin al bloqueo. Después de la formación de Alemania Occidental por la unión de las tres zonas occidentales de ocupación, los soviéticos declararon en 1949 Alemania Oriental país independiente, bajo un gobierno comunista.

Stalin originalmente apoyó la creación de Israel en 1948. La Unión Soviética fue uno de los primeros países en reconocer el nuevo país. Golda Meir llegó a Moscú como primer embajador de Israel en la Unión Soviética ese mismo año. Más tarde, cambió de opinión, oponiéndose a Israel y ordenando la disolución del Comité Judío Antifascista varios de cuyos miembros fueron luego detenidos y ejecutados durante la llamada «noche de los Poetas Asesinados».
En los últimos años de vida de Stalin, una de sus últimas grandes iniciativas de política exterior fue la nota de Stalin de 1952 para la reunificación alemana y la no intervención de las superpotencias en Europa central, pero el Reino Unido, Francia y los Estados Unidos sospecharon de la propuesta y rechazaron la oferta.

A partir de 1950, la salud de Stalin, que ya tenía setenta años de edad, empezó a desmejorar. Su memoria fallaba, se agotaba fácilmente y su estado general empeoró. Vladímir Vinográdov, su médico personal, le diagnosticó una hipertensión aguda. Vinográdov propuso un tratamiento a base de medicamentos o inyecciones y recomendó a Stalin que abandonase o al menos redujese sus funciones en el Gobierno.

En octubre de 1952 se celebró el XIX Congreso del PCUS. En él, Stalin insinuó sus deseos no belicistas y no intervencionistas en el resto del mundo, tal y como ya habría publicado en su anterior nota. Sin embargo, Malenkov hizo un discurso oficial en el cual reafirmaba que para la Unión Soviética era vital estar presente en todos los conflictos internacionales apoyando las revoluciones socialistas. Por primera vez en muchos años, el Congreso apoyó las intenciones de Malenkov y no las de Stalin. Jean Paul Sartre afirma que Stalin, sin alterarse, clausuró el Congreso con un breve discurso cuyo epílogo fue: «¡Abajo los fomentadores de la guerra!»

Si bien este revés político era demasiado modesto como para amenazar su poder, tras el XIX Congreso Stalin tomó la determinación de reanudar las purgas. Su paranoia, adormecida tras la Segunda Guerra Mundial, aumentó tras recibir una carta de la doctora Lidia Timashuk, una especialista del Policlínico del Kremlin. En esta misiva, la doctora Timashuk acusaba a Vinográdov y a otros ocho médicos de origen judío de estar recetando tratamientos inadecuados a altos mandos del Partido y del Ejército, a fin de acabar con sus vidas. Sin esperar a recibir ninguna otra prueba, Stalin ordenó el arresto de los nueve médicos y aprobó que fuesen torturados hasta confesar. Dos de los acusados fallecieron durante los interrogatorios y los siete supervivientes acabaron firmando el texto que sus interrogadores pusieron sobre la mesa. Además, Stalin hizo publicar en el diario "Pravda" que los servicios de seguridad habían estado «torpes» en descubrir lo que bautizó como el «complot de los médicos», y que había sido él mismo quien lo había desactivado.

Stalin multiplicó en estas fechas sus apariciones en público, visitaba las sedes del partido, hablaba con responsables de los distintos departamentos y nunca dejaba traslucir sus pensamientos. A finales de enero de 1953 su secretario privado desapareció sin dejar rastro. Poco después, el 15 de febrero, el jefe de sus guardaespaldas fue ejecutado sumariamente en lo que se dijo había sido una «muerte prematura». Este comportamiento aterrorizó a los miembros del Politburó, sobre todo a los más veteranos, que quedaron convencidos de que una nueva purga estaba ya en marcha. A partir de aquí, existen dos versiones sobre la muerte de Stalin.

La primera de ellas, versión oficial y hasta ahora la más verosímil, relata que la noche del sábado 28 de febrero de 1953 Stalin celebró una reunión en la "dacha" de Kúntsevo con su círculo interno, formado por Beria, Malenkov, Nikita Jrushchov y Nikolái Bulganin. En dicho encuentro, los cinco hombres vieron una película y después disfrutaron de una tardía cena. Los invitados se retiraron a las cuatro de la madrugada, cuando Stalin se fue a dormir.

La otra versión, defendida por historiadores como Iliá Erenburg y Víktor Aleksándrov, indica que esta reunión no tuvo nada de amistoso. A ella habrían sido invitados también Lázar Kaganóvich y Voroshílov, que se habrían enzarzado en una discusión con Stalin, exigiéndole la liberación de los médicos. Supuestamente, Stalin respondió gritándoles que eran unos traidores. Los dos miembros del Politburó habrían roto entonces sus carnés del partido y Stalin, fuera de sí, habría abandonado la reunión para encerrarse en su dormitorio.

Sea como fuere, la realidad es que al día siguiente Stalin no salió de su cuarto y no llamó ni a los criados ni a los guardias. Nadie se atrevió a entrar en su habitación hasta que, sobre las diez de la noche del domingo 1 de marzo, su mayordomo abrió la puerta y lo encontró tendido en el suelo, vestido con la ropa que llevaba la noche anterior y sin apenas poder hablar. Se llamó a los miembros del Politburó, que lentamente fueron acudiendo a la "dacha" de Stalin, pero nadie llamó a un médico. Finalmente, pasadas veinticuatro horas, Beria hizo venir a algunos doctores que dictaminaron que Stalin había sufrido un ataque cerebrovascular y había caído fulminado.

La agonía de Stalin se alargó varios días más. En ocasiones abría los ojos y miraba furibundamente a quienes lo rodeaban. Se cuenta que en estos momentos Beria le cogía de la mano y le suplicaba que se recuperase, pero cuando volvía a desvanecerse lo insultaba y le deseaba una dolorosa muerte. El día 4 aparentó una súbita mejoría y una enfermera comenzó a darle de beber leche con una cuchara, lo que hizo que el enfermo señalase un cuadro que había sobre la cabecera de su cama, donde una niña daba leche a una oveja. En ese momento, sufrió un nuevo ataque y entró en coma. Los médicos que atendían a Stalin le practicaron reanimación cardiopulmonar en las diversas ocasiones en que se le detuvo el corazón, hasta que finalmente a las 22:10 del día 5 de marzo no consiguieron reanimarlo. Según algunos testigos, los enfermeros siguieron esforzándose hasta que un lacónico Jrushchov dijo: «Basta, por favor… ¿No ves que está muerto?».

Andréi Gromyko, ministro de asuntos exteriores de la Unión Soviética entre 1957 y 1985, relató en sus memorias publicadas en 1989 sobre la muerte de Stalin:

Muchos años después de la caída de la Unión Soviética se han vuelto a estudiar las circunstancias que rodearon la muerte de Stalin. No faltan autores, como el historiador ruso Vladímir P. Naúmov o Jonathan Brent (catedrático de Historia en Yale), que afirman que fue envenenado por Beria, quien al poco de su muerte llegó a decir ante el Politburó: «Yo lo maté, lo maté y os salvé a todos», según relata el propio Nikita Jrushchov en sus memorias. Sin embargo, esta tesis nunca ha sido demostrada ni reconocida, como tampoco la del posible enfrentamiento final entre Stalin y el Politburó. De este modo, la causa oficial de su muerte sigue siendo un ataque cerebrovascular provocada por su hipertensión.

El cuerpo embalsamado de Iósif Stalin permaneció junto al de Lenin en el mausoleo de este desde su muerte en 1953 hasta el 31 de octubre de 1961, cuando fue retirado durante la campaña de desestalinización promovida por Nikita Jruschov y enterrado en la parte exterior de la necrópolis de la Muralla del Kremlin, detrás del mausoleo. Su tumba se encuentra entre las de Súslov y Mijaíl Kalinin. La estatua que la corona es de un blanco algo más claro que la del resto de líderes del mausoleo y por su ubicación resulta visible la parte de la plaza Roja más próxima a la catedral de San Basilio.

La primera mujer de Stalin, Yekaterina Svanidze, murió en 1907, solo cuatro años después de su matrimonio. Tuvieron un hijo, Yákov Dzhugashvili, con el que Stalin no tuvo contacto desde la muerte de su madre.

Yákov intentó suicidarse disparándose, pero sin éxito, y sufriendo graves heridas. Montefiore afirma que en ocasión de esto Stalin comentó: «Ni siquiera puede dispararse bien». Yákov formó parte del Ejército Rojo y fue capturado por las tropas alemanas durante la Segunda Guerra Mundial. Alemania ofreció a Stalin intercambiarlo por el general alemán Friedrich Paulus, rendido en Stalingrado, pero el dirigente soviético no accedió, arguyendo que la Unión Soviética no canjeaba soldados por mariscales de campo. Yákov murió oficialmente abatido en una valla por los guardias que custodiaban el campo de concentración, intentando escapar. Algunas personas afirman que corrió hacia la valla para que los guardas lo matasen, pero esto no ha sido comprobado.

Su segunda mujer fue Nadezhda Allilúyeva, fallecida en 1932. La causa oficial de su muerte fue una grave enfermedad, pero es posible que se suicidase disparándose tras una discusión con Stalin. Juntos tuvieron un hijo, Vasili, y una hija, Svetlana. Vasili consiguió rangos militares en la Fuerza Aérea Soviética, muriendo a causa del alcohol en 1962. Svetlana abandonó la Unión Soviética en 1967 y murió en 2011 en los Estados Unidos de América.

La madre de Stalin, a cuyo funeral no asistió él, murió en 1937. Se afirma que Stalin guardaba rencor a su madre por haberlo obligado a ingresar en el seminario.

El historiador Robert Conquest considera que Stalin fue «probablemente la persona que más influyó en el curso del siglo XX». Robert Service lo calificó como «uno de los políticos más destacados del siglo XX». Simon Sebag Montefiore lo describe como una «excepcional combinación: intelectual y asesino», un hombre que fue «el político definitivo» y «el más esquivo y fascinante titán del siglo XX». De acuerdo con el historiador Kevin McDermott, las interpretaciones de Stalin van «desde las serviles y adulatorias hasta las vitriólicas y condenatorias». Para la mayoría de los occidentales, así como para los rusos anticomunistas, es visto mayoritariamente de forma negativa como un asesino en masa; para un número significativo de rusos y georgianos, se trata de un gran hombre de estado.

Stalin fortaleció y estabilizó la Unión Soviética. Service sugería que sin el liderazgo de Stalin la Unión Soviética se podría haber colapsado mucho antes de 1991. Al momento de su muerte, el país había sido transformado en una potencia mundial y un coloso industrial, con una población alfabetizada. Según Service, la Unión Soviética de Stalin podía afirmar «logros impresionantes» en términos de urbanización, fuerza militar, educación y orgullo soviético. Aunque millones de ciudadanos soviéticos le despreciaban, su apoyo seguía siendo amplio en la sociedad soviética. La Unión Soviética de Stalin ha sido caracterizada como totalitaria. Varias biografías le describen como un dictador y un autócrata.

Después del fallecimiento de Stalin, el nuevo secretario general del PCUS, Nikita Jruschov, inició un proceso por el cual se denunció el eufemístico «culto a la personalidad» en referencia al conocido actualmente como el culto a Stalin. Esto dio inicio al proceso político conocido como desestalinización, por el cual se denunciaron los crímenes cometidos por Stalin en contra del Estado soviético y el Partido Comunista. Su punto culminante sucedió durante el XX Congreso del PCUS en 1956, en el cual Jruschov pronunció al cierre del mismo, el conocido "Discurso secreto".

El proceso de desestalinización de Jruschov acabó cuando fue sucedido como líder por Leonid Brézhnev en 1964; este introdujo cierto nivel de "re-estalinización" en la Unión Soviética. En 1969 y 1979 se propusieron planes para la completa rehabilitación del legado de Stalin, pero ambas fueron rechazadas por quejas tanto internas como de partidos comunistas extranjeros. Mijaíl Gorbachov vio la denuncia total de Stalin como necesaria para la regeneración de la sociedad soviética. Tras la caída de la Unión Soviética en 1991, el primer presidente de la nueva Federación de Rusia, Borís Yeltsin, mantuvo la denuncia a Stalin y añadió la denuncia a Lenin. Su sucesor, Vladímir Putin, no buscó rehabilitar a Stalin pero puso énfasis en celebrar los logros soviéticos bajo el liderazgo de Stalin en lugar de la represión.

En encuestas realizadas entre 2003 y 2006, al menos un cuarto de los rusos votarían a Stalin seguro o probablemente, mientras que menos del 40 % estarían seguros de no votarle. En 2008, en el programa de televisión "Name of Russia", Stalin fue votado como la tercera personalidad más notable de la historia rusa. Una encuesta de 2017 concluía que la popularidad de Stalin alcanzó, entre la población rusa, su punto más alto en 16 años, con un 46 % expresando una visión favorable.

Los primeros investigadores en intentar contar la cantidad de personas que murieron a causa del régimen de Stalin se vieron obligados a recurrir en gran medida a las pruebas anecdóticas. Sus estimaciones variaban de 3 millones a algo más de 50. Después de la disolución de la Unión Soviética en 1991, las evidencias de los archivos soviéticos se hicieron disponibles. De acuerdo con los registros, alrededor de 800000 presos fueron ejecutados por el régimen de Stalin por delitos políticos o penales, mientras que alrededor de 1,7 millones murieron en gulags y unos 390000 perecieron durante reasentamientos forzosos, un total de alrededor de tres millones de víctimas. Según ciertas fuentes, durante el mandato de Stalin cerca de cinco millones de personas fueron encarceladas u obligadas a trabajos forzados, un millón habían sido ejecutados y dos millones perecieron en trabajos forzados.
El debate continúa, sin embargo, puesto que algunos historiadores creen que el archivo contiene cifras poco fiables. Por ejemplo, sostiene Gellately que los muchos sospechosos torturados hasta la muerte mientras estaban en «custodia de investigación» es probable que no se hayan contado entre los ejecutados. Asimismo, existen categorías de víctimas que no fueron registradas de forma correcta por los soviéticos, como las víctimas de las deportaciones étnicas, o transferencias de población alemana después de la Segunda Guerra Mundial.

Entre 1919 y mediados de los años 1950 fueron deportadas más de seis millones de personas, casi el doble que los ciudadanos soviéticos deportados por el Tercer Reich durante la Gran Guerra Patria para realizar trabajos forzados. De estos, un millón a un millón y medio habrían muerto directamente a causa del traslado.

Particularmente afectados serán los kulaks y los alemanes del Volga —aunque las deportaciones de germanos étnicos en Rusia databan desde 1914—. Alcanzaran su apogeo entre los años 1930 y finales de los 1940 —durante la guerra con los alemanes será deportado un tercio del total—. Las zonas de «acogida» preferidas serán las inhóspitas, despobladas y aisladas Siberia y Asia Central soviética. Entre los grupos étnicos, sociales y religiosos deportados hay hordas de cosacos (del Amur, Astracán, Azov, Mar Negro, Bug Meridional, Don, Kubán, Oremburgo, Semirechye, Terek, Transbaikal, Ural y Ussuri), «elementos socialmente peligrosos», kulaks, campesinos en general, kazajos nómades, alemanes —no todos del Volga—, polacos —incluidos refugiados desde 1940—, fineses de Ingria, kurdos, coreanos, chinos, japoneses, rusos de Harbin, judíos persas, azeríes, persas, asirios, noruegos, suecos, rumanos, griegos de Crimea (pondios), tártaros de Crimea, karacháis, calmucos, chechenos, ingusetios, balkarios, cabardinos, turcos meskh, hamshenis, karapapakos, lazes, armenios, basmachís, búlgaros de Crimea, armenios musulmanes de Georgia ("khemshin"), miembros de la "Verdadera Iglesia Ortodoxa", testigos de Jehová y supuestos nacionalistas lituanos, moldavos, letones, estonios, bielorrusos, ucranianos.

Así, mientras que algunos investigadores han estimado el número de víctimas de las represiones de Stalin en un total de cuatro millones más o menos, otros creen que el número es considerablemente superior. El escritor ruso Vadim Erlikman, por ejemplo, hace las siguientes estimaciones: ejecuciones, 1,5 millones; gulags, 5 millones; deportaciones, 1,7 millones a 7,5 millones de deportados, y prisioneros de guerra y civiles alemanes, 1 millón, lo que hace un total de alrededor de 9 millones de víctimas de la represión.
Algunos también han incluido los seis a ocho millones de víctimas de la hambruna 1932-1933 como víctimas de la represión. Esta clasificación es controvertida sin embargo, ya que los historiadores difieren en cuanto a si la hambruna era una deliberada parte de la campaña de represión contra los kulaks, o simplemente un consecuencia no deseada de la lucha por la colectivización forzada.

Hay autores para los que un mínimo de alrededor de 10 millones de muertos —cuatro millones por la represión y seis por el hambre— son atribuibles al régimen; algunos libros de reciente publicación sugieren un probable total de alrededor de 20 millones. Por ejemplo, agregar 6-8 millones de víctimas de la hambruna según Erlikman por encima de las estimaciones de muertes directas, daría un total de entre 15 y 20 millones de víctimas. El investigador Robert Conquest, mientras tanto, ha revisado su estimación inicial de hasta 30 millones de víctimas a 20 millones. Otros siguen considerando que sus anteriores estimaciones, mucho más altas, son correctas. Todas estas estimaciones se ven contrastadas con la Demografía de Rusia, siendo la de la Segunda Guerra Mundial la única caída con semejantes cifras.

Stalin era considerado entre los bolcheviques como un experto en la cuestión nacional. En 1913 publicó su artículo «El marxismo y la cuestión nacional» en el que desarrolla su teoría marxista sobre la nación, en oposición a las teorías nacionales que defendían los bundistas y mencheviques. Él desarrolla el concepto de nación que resume de la siguiente forma:

Para Stalin, una nación lo es al cumplir todos y cada uno de estos rasgos:


Esta definición se contrapone a la visión de Otto Bauer, apoyada por los bundistas, en la que una nación puede constituirse únicamente mediante el «carácter nacional», lo que Stalin llama «comunidad de psicología», siendo especialmente relevante el caso de los judíos. Por otro lado, para Stalin el Imperio austrohúngaro o el Imperio ruso tampoco eran naciones.

El movimiento nacional es siempre, según Stalin, una lucha entre las clases burguesas de la nación dominante y la nación oprimida. Sin embargo, el proletariado de las naciones oprimidas está igualmente afectado por la represión de la nación dominante y debe luchar contra la opresión de las naciones y proclamar el derecho de autodeterminación. Stalin puntualiza que este derecho de autodeterminación debe ser en beneficio de la mayoría de la nación. Es decir, de las clases trabajadoras.

Stalin tuvo una producción escrita desde sus inicios revolucionarios, aquí se consignan solo los más importantes:




</doc>
<doc id="22191" url="https://es.wikipedia.org/wiki?curid=22191" title="Helio">
Helio

El helio (del griego: ["hḗlios"] ‘Sol’, por haberse inferido en 1868 su existencia en la atmósfera solar) es un elemento químico de número atómico 2, símbolo He y peso atómico estándar de 4,0026. Pertenece al grupo 18 de la tabla periódica de los elementos, ya que al tener el nivel de energía completo presenta las propiedades de un gas noble. Es decir, es inerte (no reacciona) y al igual que estos, es un gas monoatómico incoloro e inodoro que cuenta con el menor punto de ebullición de todos los elementos químicos y solo puede ser licuado bajo presiones muy grandes y no puede ser congelado.

Durante un eclipse solar en 1868, el astrónomo francés Pierre Janssen observó una línea espectral amarilla en la luz solar que hasta ese momento era desconocida. Norman Lockyer observó el mismo eclipse y propuso que dicha línea era producida por un nuevo elemento, al cual llamó helio, con lo cual, tanto a Lockyer como a Janssen se les adjudicó el descubrimiento de este elemento. En 1903 se encontraron grandes reservas de helio en campos de gas natural en los Estados Unidos, país con la mayor producción de helio en el mundo.

Industrialmente se usa en criogenia (siendo su principal uso, lo que representa alrededor de un 28 % de la producción mundial), en la refrigeración de imanes superconductores. Entre estos usos, la aplicación más importante es en los escáneres de resonancia magnética. También se utiliza como protección para la soldadura por arco y otros procesos, como el crecimiento de cristales de silicio, los cuales representan el 20 % de su uso para el primer caso y el 26 % para el segundo. Otros usos menos frecuentes, aunque popularmente conocidos, son el llenado de globos y dirigibles, o su empleo como componente de las mezclas de aire usadas en el buceo a gran profundidad. El inhalar una pequeña cantidad de helio genera un breve cambio en la calidad y el timbre de la voz humana. En la investigación científica, el comportamiento del helio-4 en forma líquida en sus dos fases, helio I y helio II, es importante para los científicos que estudian la mecánica cuántica (en especial, el fenómeno de la superfluidez), así como para aquellos que desean conocer los efectos ocurridos en la materia a temperaturas cercanas al cero absoluto (como el caso de la superconductividad).

El helio es el segundo elemento más ligero y el segundo más abundante en el universo observable, constituyendo el 24 % de la masa de los elementos presentes en nuestra galaxia. Esta abundancia se encuentra en proporciones similares en el Sol y en Júpiter. Por masa se encuentra en una proporción doce veces mayor a la de todos los elementos más pesados juntos. La presencia tan frecuente de helio es debida a elevada energía de enlace por nucleón del helio-4 con respecto a los tres elementos que le siguen en la tabla periódica (litio, berilio y boro). Esta energía da como resultado la producción frecuente de helio tanto en la fusión nuclear como en la desintegración radioactiva. La mayor parte del helio en el universo se encuentra presente en la forma del isótopo helio-4 (He), el cual se cree que se formó unos 15 minutos después del Big Bang. Gracias a la fusión de hidrógeno en las estrellas activas, se forma una pequeña cantidad de helio nuevo, excepto en las de mayor masa, debido a que durante las etapas finales de su vida generan su energía convirtiendo el helio en elementos más pesados. En la atmósfera de la Tierra se encuentran trazas de helio debido a la desintegración radioactiva de algunos elementos. En algunos depósitos naturales el gas se encuentra en cantidad suficiente para la explotación.

En la Tierra, la ligereza de helio ha provocado su evaporación de la nube de gas y polvo a partir de la cual se formó el planeta, por lo que es relativamente poco frecuente —con una fracción de 0,00052 por volumen— en la atmósfera terrestre. El helio presente en la Tierra hoy en día ha sido creado en su mayor parte por la desintegración radiactiva natural de los elementos radioactivos pesados (torio y uranio), debido a que las partículas alfa emitidas en dichos procesos constan de núcleos de helio-4. Este helio radiogénico es atrapado junto con el gas natural en concentraciones de hasta el 7 % por volumen, del que se extrae comercialmente por un proceso de separación a baja temperatura llamado destilación fraccionada.

A pesar de que la configuración electrónica del helio es 1s², no figura en el grupo 2 de la tabla periódica de los elementos, junto al hidrógeno en el bloque s, sino que se coloca en el grupo 18 del bloque p, ya que al tener el nivel de energía completo presenta las propiedades de un gas noble.

En condiciones normales de presión y temperatura es un gas monoatómico no inflamable, pudiéndose licuar solamente en condiciones extremas (de alta presión y baja temperatura).

Tiene el punto de solidificación más bajo de todos los elementos químicos, siendo el único líquido que no puede solidificarse bajando la temperatura, ya que permanece en estado líquido en el cero absoluto a presión normal. De hecho, su temperatura crítica es de tan solo 5,20 K o −267,96 grados Celsius. Los sólidos compuestos por ³He y He son los únicos en los que es posible, incrementando la presión, reducir el volumen más del 30 %. El calor específico del gas helio es muy elevado y el helio vapor muy denso, expandiéndose rápidamente cuando se calienta a temperatura ambiente.

El helio sólido solamente existe a presiones del orden de 100 MPa a 15 K (−258,15 °C). Aproximadamente a esa temperatura, sufre una transformación cristalina, de una estructura cúbica centrada en las caras a una estructura hexagonal compacta. En condiciones más extremas (3 K, aunque presiones de 3 MPa) se produce un nuevo cambio, empaquetándose los átomos en una estructura cúbica centrada en el cuerpo. Todos estos empaquetamientos tienen energías y densidades similares, debiéndose los cambios a la forma en la que los átomos interactúan.

El helio es un elemento químico cuyo átomo es el más simple de resolver utilizando las reglas de la mecánica cuántica después del átomo de hidrógeno. Se compone de dos electrones en órbita alrededor de un núcleo que contiene dos protones junto con uno o dos neutrones, dependiendo del isótopo. Sin embargo, como en la mecánica newtoniana, ningún sistema que consista de más de dos partículas se puede resolver con un enfoque de análisis matemático exacto (véase problema de los tres cuerpos) y el helio no es la excepción. Así, los métodos matemáticos son necesarios, incluso para resolver el sistema de un núcleo y dos electrones. Sin embargo, tales métodos de la química computacional se han utilizado para crear una imagen mecánico cuántica de las uniones de los electrones de helio con una precisión dentro de un 2 % del valor correcto, con unos pocos pasos de cálculo computacional. En estos modelos se observa que cada electrón evita parcialmente que el otro sienta la interacción con el núcleo, de tal manera que la carga nuclear efectiva "Z" es de aproximadamente 1,69 unidades, y no las 2 cargas de un "núcleo desnudo" clásico de helio.

El átomo de hidrógeno se utiliza ampliamente para ayudar a resolver el átomo de helio. El modelo atómico de Bohr dio una explicación muy precisa del espectro del átomo de hidrógeno, pero cuando se intentó utilizar en el helio el modelo falló. Werner Heisenberg desarrolló una modificación del análisis de Bohr, en el que utilizó valores semiintegrados de los números cuánticos. La teoría del funcional de la densidad se utiliza para obtener los niveles de energía en su estado base del átomo de helio, junto con el método de Hartree-Fock.

El núcleo del átomo de helio-4, que es exactamente igual a una partícula alfa, es particularmente interesante. La razón de esto se debe a que experimentos de dispersión de electrones de alta energía han mostrado que su carga decrece de forma exponencial a partir de un máximo en su punto central, exactamente de la misma manera en que decrece la densidad de carga en su propia nube de electrones. Esta simetría refleja principios físicos similares: el par de neutrones y de protones en el núcleo del helio obedecen a las mismas reglas mecánico-cuánticas que los dos electrones que lo orbitan —aunque la unión de las partículas en el núcleo se debe a un potencial diferente al que mantiene a los electrones en la nube alrededor del átomo—. De esta manera, estos fermiones (es decir, tanto protones como electrones y neutrones) ocupan completamente los orbitales 1s en pares, ninguno de ellos posee momento angular orbital y cada uno de ellos cancela el espín intrínseco del otro. El añadir otra de cualquiera de estas partículas requeriría momento angular y liberaría sustancialmente menos energía (de hecho, ningún núcleo con cinco nucleones es estable). Por esta razón, este arreglo para estas partículas es extremadamente estable energéticamente, y dicha estabilidad da lugar a muchos fenómenos cruciales inherentes al helio en la naturaleza.

Como ejemplo de estos hechos debidos a la alta estabilidad de la configuración electrónica del helio está la baja reactividad química de este elemento (la más baja de toda la tabla periódica), así como la falta de interacción de sus átomos entre ellos mismos. Esto produce los puntos de fusión y de ebullición más bajos de todos los elementos. De la misma manera, la estabilidad energética del núcleo de helio-4 da lugar a una fácil producción de estos en reacciones atómicas que involucran tanto emisión de partículas pesadas como fusión nuclear. Cierta cantidad de helio-3 estable se produce en reacciones de fusión a partir del hidrógeno, pero es una fracción mucho menor comparada con el helio-4. La estabilidad del helio-4 es la razón por la cual el hidrógeno se convierte en esta forma de helio en el Sol, en vez de helio-3, deuterio u otros elementos más pesados. Asimismo es parcialmente responsable del hecho de que la partícula alfa es por mucho el tipo de partícula bariónica más comúnmente expelida por los núcleos atómicos. Dicho de otra manera, la desintegración alfa es mucho más común que la desintegración en núcleos más pesados.

La inusual estabilidad del helio-4 es importante también en cosmología. En los primeros minutos después del Big Bang, el universo estaba compuesto por una mezcla de nucleones (protones y neutrones) libres. Esta «sopa» tenía originalmente una proporción de seis protones por cada neutrón, y después de un tiempo se enfrió al punto tal que se pudo dar la fusión nuclear. La estabilidad del helio provocó que casi todas las agregaciones de nucleones formadas en ese momento fueran núcleos de helio-4. La unión de protones y neutrones para formar helio-4 tiene tanta fuerza que, de hecho, la producción de este elemento consumió casi todos los neutrones libres en cuestión de minutos, antes de que dichos núcleos pudieran decaer por desintegración beta. Esto dejó una cantidad muy pequeña de estas partículas para que se pudiera formar litio, berilio o boro. El enlace nuclear por cada nucleón en el helio-4 es más fuerte que en cualquiera de estos tres elementos (véase nucleogénesis y energía de enlace). Por lo tanto, no había ningún mecanismo energético disponible, una vez que se hubo formado el helio, para crear los elementos de número atómico 3, 4 y 5. En términos de energía, también era favorable la fusión del helio para formar el siguiente elemento en la tabla periódica con menor energía por nucleón: el carbono. No obstante, debido a la falta de elementos intermedios, este proceso requería la colisión casi simultánea de tres núcleos de helio-4 (véase proceso triple-alfa), por lo que no hubo suficiente tiempo para que el carbono se formara en el Big Bang: en cuestión de minutos, el universo temprano se enfrió a una temperatura y presión en las cuales la fusión de helio a carbono ya no fue posible. Esto ocasionó que el universo temprano poseyera un cociente hidrógeno/helio muy similar al observado actualmente (en masa, tres partes de hidrógeno por una de helio-4), con casi todos los neutrones del universo —como es el caso hoy en día— atrapados dentro de los núcleos de helio-4.

Todos los elementos más pesados —incluyendo aquellos que se necesitan para formar planetas rocosos como la Tierra y para la existencia de vida basada en el carbono— tuvieron que crearse posteriormente, en estrellas lo suficientemente calientes para quemar no solo hidrógeno —dado que esto solamente produce más helio— sino el mismo helio. Dichas estrellas son masivas y, por lo tanto, raras. Lo anterior da lugar al hecho de que todos los elementos químicos, aparte del hidrógeno y el helio, compongan solamente el 2 % de la masa en forma de átomos del universo. El helio-4, por su parte, constituye cerca del 23 % de toda la materia ordinaria del universo, es decir, prácticamente toda la materia ordinaria que no es hidrógeno.

El helio es el gas noble menos reactivo después del neón y por tanto, el segundo elemento menos reactivo de todos ellos. Es inerte y monoatómico en condiciones normales. Debido a su baja masa atómica, en la fase gaseosa, la conductividad térmica, el calor específico, y la velocidad del sonido son mayores que en cualquier otro gas, excepto el hidrógeno. Por razones similares, y también debido al pequeño tamaño de sus átomos, su tasa de difusión a través de los sólidos es tres veces mayor que la del aire, y alrededor del 65 % de la del hidrógeno.

Asimismo es también menos soluble en agua que cualquier otro gas conocido, y su índice de refracción es el más cercano a la unidad de todos los gases. Este elemento tiene un coeficiente Joule-Thomson negativo a temperatura ambiente normal, lo que significa que se calienta cuando se le permite expandirse libremente. Solo por debajo de su temperatura de inversión de Joule-Thomson (de 32 a 50 K a 1 atmósfera) se enfría en la expansión libre. Una vez preenfriado debajo de esta temperatura, el helio puede licuarse mediante el enfriamiento debido a su expansión.

La mayor parte del helio extraterrestre se encuentra en un estado de plasma, con propiedades muy diferentes a las del helio atómico. En el plasma, los electrones del helio no están ligados al núcleo, lo que hace que su conductividad eléctrica sea muy alta, aun cuando el gas está solo parcialmente ionizado. Las partículas cargadas son altamente influenciadas por los campos magnéticos y eléctricos. Por ejemplo, en el viento solar, junto con el hidrógeno ionizado, las partículas interactúan con la magnetosfera de la Tierra, dando lugar a la corriente de Birkeland y a las auroras.

A diferencia de cualquier otro elemento, el helio líquido se mantendrá así hasta el cero absoluto a presiones normales. Este es un efecto directo de la mecánica cuántica: en concreto, la energía del punto cero del sistema es demasiado alta para permitir la congelación. El helio sólido requiere una temperatura de 1 a 1,5 K (alrededor de -272 °C o -457 °F) y alrededor de 25 bar (2,5 MPa) de presión. A menudo es difícil distinguir el helio sólido del líquido ya que el índice de refracción de las dos fases es casi el mismo. El sólido tiene un marcado punto de fusión y estructura cristalina, pero es muy compresible. Aplicar presión en un laboratorio puede reducir su volumen en más del 30 %. Con un módulo de compresibilidad del orden de 50 MPa, es 50 veces más compresible que el agua. El helio sólido tiene una densidad de 0,214 ± 0,006 g/ml a 1,15 K y 66 atm, la densidad proyectada a 0 K y 25 bar (2,5 MPa) es 0,187 ± 0,009 g/ml.

Por debajo de su punto de ebullición de 4,22 K, y por encima del punto lambda de 2,1768 K, el isótopo helio-4 existe en un estado normal de líquido incoloro, llamado helio I. Al igual que otros líquidos criogénicos, el helio I hierve cuando se calienta y se contrae cuando baja su temperatura. Por debajo del punto lambda, sin embargo, esta fase no hierve y se expande a medida que la temperatura desciende aún más.

El helio tiene un índice de refracción similar al de un gas, de 1,026, lo que hace que su superficie sea muy difícil de ver, de tal forma que se suelen utilizar flotadores de poliestireno extruido para ver en dónde se encuentra la superficie. Este líquido incoloro, tiene una viscosidad muy baja y una densidad de 0,145 g/mL, que es solo una cuarta parte del valor predicho por la física clásica. Es necesario hacer uso de la mecánica cuántica para explicar esta propiedad y, por tanto, ambos tipos de helio líquido se llaman "fluidos cuánticos", lo que significa que muestran propiedades atómicas a escala macroscópica. Esto puede ser un efecto del hecho de que su punto de ebullición está muy cerca del cero absoluto, lo que impide que el movimiento molecular aleatorio (energía térmica) oculte sus propiedades atómicas.

El helio líquido por debajo de su punto lambda muestra características sumamente inusuales, en un estado llamado helio II. La ebullición del helio II no es posible debido a su alta conductividad térmica; la entrada de calor causa la evaporación del líquido directamente a gas. El isótopo helio-3 también tiene una fase de superfluido, pero solo a temperaturas mucho más bajas. Como resultado, se sabe menos sobre las propiedades de esta fase en dicho isótopo.

El helio II es un superfluido, un estado cuántico de la materia con propiedades extrañas. Por ejemplo, cuando fluye a través de capilares tan delgados como de 10 a 10 m, no tiene viscosidad medible. Sin embargo, cuando se realizan mediciones entre dos discos en movimiento, se observa una viscosidad comparable a la del helio gaseoso. La teoría actual explica este fenómeno utilizando un "modelo de dos fluidos" para el helio II. En este modelo, el helio líquido por debajo del punto lambda se considera que contiene una proporción de átomos de helio en estado base, que componen el superfluido, y que fluyen con una viscosidad exactamente igual a cero; y una proporción de átomos de helio en un estado excitado, que se comportan más como un fluido ordinario.

En el efecto fuente, se construye una cámara que está conectada a un depósito de helio II por medio de un disco sinterizado a través del cual el helio superfluido pasa fácilmente, pero aquellos líquidos que no son superfluidos no pueden. Si se calienta el interior del contenedor, el helio deja de ser superfluido. A fin de mantener fracción de equilibrio de helio superfluido, este se fuga a través del disco y aumenta la presión, haciendo que el líquido salga brotando del recipiente.

La conductividad térmica del helio II es mayor que la de cualquier otra sustancia conocida. Es un millón de veces mayor que la del helio I y varios cientos de veces la del cobre. Esto se debe a que la conducción de calor se produce por un mecanismo cuántico excepcional. La mayoría de los materiales que son buenos conductores térmicos tienen una banda de electrones de valencia libres que sirven para transferir el calor. El helio II no tiene banda de valencia, pero conduce bien el calor. El flujo de calor se rige por ecuaciones similares a la ecuación de onda utilizada para caracterizar la propagación del sonido en el aire. Cuando se introduce calor, este se mueve a través de helio II en forma de ondas a 20 metros por segundo a una temperatura de 1,8 K. Este fenómeno es conocido como segundo sonido.

El helio II también presenta un efecto de ascensión. Cuando una superficie se extiende más allá del nivel de helio II, este se mueve a lo largo de la superficie, contra la fuerza de gravedad. El líquido se escapará de un contenedor que no esté sellado reptando por las paredes del mismo hasta que encuentre una región con mayor temperatura donde se evaporará. Este ascenso lo realiza en una película de 30 nm de espesor, independientemente del material de superficie. Esta película se llama película de Rollin y lleva el nombre de la primera persona que caracterizó este rasgo, Bernard V. Rollin. Como resultado de este comportamiento y de la habilidad del helio II de escapar a través de aberturas pequeñas, es muy difícil mantener a este fluido confinado. Las ondas que se propagan a través de una película de Rollin se rigen por la misma ecuación de ondas de gravedad en aguas poco profundas, pero en lugar de la gravedad, la fuerza de restauración es la fuerza de van der Waals. Estas ondas son conocidas como tercer sonido.

Dado que el helio es un gas noble, en la práctica no participa en las reacciones químicas, aunque bajo la influencia de descargas eléctricas o bombardeado con electrones forma compuestos.

El helio tiene una valencia cero y no es químicamente reactivo bajo condiciones normales. Es un aislante eléctrico a menos que esté ionizado. Al igual que los demás gases nobles, tiene niveles de energía metaestables, lo que le permite seguir ionizado en una descarga eléctrica con un voltaje por debajo de su potencial de ionización. El helio puede formar compuestos inestables, conocidos como excímeros, con el wolframio, yodo, flúor y fósforo, cuando se somete a una descarga eléctrica luminiscente, a un bombardeo de electrones, o bien es un plasma por otra razón. Los compuestos moleculares HeNe, HgHe y WHe, y los iones moleculares He, He, HeH, y HeD se pueden crear de esta manera. Esta técnica también ha permitido la producción de la molécula neutra He, que tiene un gran número de sistemas de bandas espectrales, y de la molécula HgHe, que aparentemente solo se mantiene unida por fuerzas de polarización. En teoría, otros compuestos reales también son posibles, como el fluorohidruro de helio (HHeF), que sería análogo al fluorohidruro de argón, descubierto en 2000. Los cálculos indican que dos nuevos compuestos que contienen un enlace de helio-oxígeno podrían ser estables. Dos nuevas especies moleculares, predichas teóricamente, CsFHeO y N(CH)4FHeO, son derivados de un anión metaestable [F-HeO], anticipado en 2005 en forma teórica por un grupo de Taiwán. De confirmarse experimentalmente, estos compuestos acabarían con la inercia química del helio, y el único elemento completamente inerte sería el neón.

El helio ha sido colocado en jaulas moleculares de carbono (los fullerenos) por medio de calentamiento a alta presión. Las moléculas de fullereno endohédrico formadas son estables hasta temperaturas altas. Cuando se forman los derivados químicos de estos fullerenos, el helio permanece dentro de ellos. Si se utiliza helio-3, se puede observar fácilmente por espectroscopia de resonancia magnética nuclear. Se han reportado una gran cantidad de fullerenos que contienen helio-3. Aunque los dichos átomos no se encuentran ligados por medio de enlaces covalentes o iónicos, estas sustancias tienen propiedades distintas y una composición definida, al igual que todos los compuestos químicos estequiométricos.

Existen ocho isótopos conocidos del helio, pero tan solo el He y el He son estables. En la atmósfera terrestre hay un átomo de ³He por cada millón de átomos de He. A diferencia de otros elementos, la abundancia isotópica del helio varía mucho por su origen, debido a los diferentes procesos de formación. El isótopo más común, el He, se produce en la Tierra mediante la desintegración alfa de elementos radiactivos más pesados; las partículas alfa que aparecen son átomos de He completamente ionizado. El He tiene un núcleo inusualmente estable debido a que sus nucleones están ordenados en capas completas. Además, este isótopo se formó en grandes cantidades durante la nucleosíntesis primordial en el Big Bang.

El ³He está presente hoy en día en la tierra tan solo en trazas (la mayoría data desde la formación de la Tierra), aunque algo de este cae a la Tierra al ser atrapado en el polvo cósmico. Algunos rastros también son producidos mediante la desintegración beta del tritio. Algunas rocas de la corteza terrestre tienen distintas proporciones de isótopos que varían hasta un factor de diez. Estas proporciones pueden usarse para investigar el origen de las rocas así como la composición del manto terrestre. El ³He es mucho más abundante en las estrellas como producto de la fusión nuclear. Por consiguiente, en el medio interestelar, la proporción de ³He y He es alrededor de 100 veces más grande que la que hay en la Tierra. El material extraplanetario, como regolitos de asteroides y lunares, tiene trazas de ³He producto del bombardeo de los vientos solares contra ellos. La superficie de la Luna tiene concentraciones de ³He de alrededor de 0,01 ppm. Algunas personas, principalmente Gerald Kulcinski en 1986, han propuesto explorar la Luna, excavar los regolitos lunares, y utilizar el ³He para fusión nuclear.

El He líquido puede ser enfriado hasta 1 kelvin utilizando enfriamiento por evaporación en recipientes en los que se puede alcanzar y mantener estas temperaturas. Un enfriamiento similar para el helio-3, que tiene un punto de ebullición más bajo, se puede alcanzar alrededor de los 0,2 K en un refrigerador de helio-3. Las mezclas que contienen la misma proporción de helio-3 y helio-4 a una temperatura por debajo de 0,8 K se separan en dos fases no miscibles debido a su incompatibilidad (cada una obedece a una estadística cuántica diferente: los átomos de helio-4 son bosones mientras que los átomos de helio-3 son fermiones). Los refrigeradores de dilución usan esta imposibilidad de mezclado para alcanzar temperaturas de unos pocos milikelvin.

Es posible producir isótopos exóticos de helio, los cuales rápidamente se descomponen en otras sustancias. El isótopo pesado de menor duración es el He, con un periodo de semidesintegración de 7.6 segundos. El He se descompone emitiendo una partícula beta y su periodo de desintegración es de 0,8 segundos. El He también emite partículas beta así como rayos gamma. Tanto el He y el He se crean mediante algunas reacciones nucleares. El He y el He son conocidos por tener un halo nuclear. El ²He (que consiste en dos protones y ningún neutrón) es un radioisótopo que se desintegra en protio (hidrógeno) por medio de emisión de protones, con un periodo de desintegración de 3 segundos.

El helio es el segundo elemento más abundante del universo conocido tras el hidrógeno y constituye alrededor del 23 % de la masa bariónica del universo. La mayor parte del helio se formó durante la nucleosíntesis del Big Bang, en los tres primeros minutos después de este. De esta forma, la medición de su abundancia contribuye a los modelos cosmológicos. En las estrellas, el helio se forma por la fusión nuclear del hidrógeno en reacciones en cadena protón-protón y en el ciclo CNO, los cuales forman parte de la nucleosíntesis estelar.

En la atmósfera terrestre la concentración de helio por volumen es de tan solo 5,2 partes por millón. La concentración es baja y prácticamente constante a pesar de la continua producción de nuevo helio, debido a que la mayor parte del helio en la atmósfera se escapa al espacio debido a distintos procesos. En la heterosfera terrestre, una parte de la atmósfera superior, el helio y otros gases ligeros son los elementos más abundantes.

Casi todo el helio presente en la Tierra es el resultado de la desintegración radiactiva, y por tanto, un globo de helio terrestre es, en esencia, una bolsa de partículas alfa expelidas por este proceso. El helio se encuentra en grandes cantidades en minerales de uranio y torio, incluyendo cleveíta, pechblenda, carnotita y monacita, ya que estos emiten partículas alfa (núcleos de helio, He) y los electrones se combinan de inmediato con ellas, tan pronto como las partículas son detenidas por la roca. De esta manera, se estima que unas 3.000 toneladas de helio se generan al año en toda la litosfera. En la corteza terrestre, la concentración de helio es de 8 partes por mil millones. En el mar, la concentración es de solo 4 partes por billón. También hay pequeñas cantidades en manantiales de aguas minerales, gas volcánico, y hierro meteórico. Debido a que el helio es atrapado de manera similar al gas natural por una capa impermeable de roca, las mayores concentraciones de este elemento en el planeta se encuentran en el gas natural, de donde se extrae la mayor parte del helio comercial. La concentración varía en una amplia gama de unas pocas ppm hasta más del 7 % en un pequeño campo de gas en el condado de San Juan, Nuevo México.

En 2016 científicos británicos descubren un gran yacimiento de gas helio en África. Este yacimiento, ubicado en la República Unida de Tanzania, mide aproximadamente 54 millones de pies cúbicos de volumen y es el mayor yacimiento de helio del mundo.

Para su uso a gran escala, se extrae por destilación fraccionada a partir del gas natural, que contiene hasta un 7 % de helio. Al tener un punto de ebullición más bajo que cualquier otro elemento, se utilizan bajas temperaturas y altas presiones para licuar casi todos los demás gases (principalmente nitrógeno y metano). El helio crudo resultante se purifica por medio de exposiciones sucesivas a temperaturas bajas, en la que casi todo el nitrógeno y los otros gases restantes se precipitan fuera de la mezcla gaseosa. Como una fase de purificación final, se utiliza carbón activado, lo que da como resultado helio grado A, con una pureza del 99,995 %. La principal impureza en el helio grado A es el neón. En la fase final de la producción, la mayoría del helio que se produce es licuado por medio de un proceso criogénico. Esto es necesario para aplicaciones que requieren helio líquido y también permite a los proveedores de helio reducir el costo en el transporte a larga distancia, dado que la mayoría de los contenedores de helio líquido tienen una capacidad cinco veces mayor que la de los camiones cisterna que trasportan helio gaseoso.

En 2008, alrededor de 169 millones de metros cúbicos estándar (SCM, por sus siglas en inglés, definidos como un metro cúbico a una presión de 1 atm y a una temperatura de 15 °C) de helio se extrajeron a partir del gas natural o de reservas de helio. De estos, aproximadamente el 78 % provinieron de los Estados Unidos, el 10 % de Argelia, y del resto la mayor parte fueron extraídos en Rusia, Polonia y Catar. En los Estados Unidos, la mayor parte del helio se extrae a partir del gas natural de los campos de Hugoton y otros cercanos en Kansas, Oklahoma y Texas. En 2000, los Estados Unidos tenían reservas de helio en complejos de pozos, de alrededor de 4,2×formula_1 SCM. Esta cantidad es suficiente para unos 25 años de uso mundial, o de 35 años de consumo de Estados Unidos, aunque se espera que factores en el ahorro y el procesamiento impacten los números efectivos de las reservas. Se estima que las reservas básicas de helio aún no probadas que se pudieran obtener a partir de gas natural en los Estados Unidos son de 3,1 a 5,3×10 SCM, o aproximadamente cuatro órdenes de magnitud mayor que las reservas probadas.

El helio se debe extraer principalmente del gas natural, debido a que su presencia en el aire es solo una fracción comparada con la de la del neón, y sin embargo, su demanda es mucho mayor. Se estima que si toda la producción de neón se reinstrumentara para ahorrar helio, se satisfarían un 0,1 % de las demandas mundiales de helio. Igualmente, solamente un 1 % de las demandas mundiales de helio se podrían satisfacer reinstrumentando todas las plantas de destilación de aire. El helio puede ser sintetizado por medio del bombardeo de litio o boro utilizando protones de alta velocidad. Sin embargo, este método de producción es totalmente inviable económicamente.

Las reservas actuales de helio se están utilizando mucho más rápido de lo que este elemento se puede reponer. Dada esta situación, hay grandes preocupaciones de que el suministro de helio pueda agotarse pronto. En las reservas más grandes del mundo, en Amarillo, Texas, se espera que este gas se agote en ocho años contando desde 2008. Esto podría prevenirse si los actuales usuarios capturasen y reciclasen el gas y si las compañías de petróleo y gas natural hiciesen uso de técnicas de captura de helio al extraerlos.

El helio es más ligero que el aire y a diferencia del hidrógeno no es inflamable, siendo además su poder ascensional un 8 % menor que el de este, por lo que se emplea como gas de relleno en globos y zepelines publicitarios, de investigación atmosférica e incluso para realizar reconocimientos militares.

Aun siendo la anterior la principal, el helio tiene más aplicaciones:

De la producción mundial total de helio en 2008, de 32 millones de kg, su mayor uso (alrededor del 22 % del total en 2008) fue en aplicaciones criogénicas. De estas la mayoría fueron en medicina en el enfriamiento de imanes superconductores en escáneres de resonancia magnética. Otros usos importantes (un total de cerca de 78 % de su uso en 1996) fueron en los sistemas de presurización y saneamiento, el mantenimiento de atmósferas controladas y la soldadura.

El helio se utiliza para muchos propósitos que requieren algunas de sus propiedades únicas, tales como su bajo punto de ebullición, baja densidad, baja solubilidad, alta conductividad térmica, o su baja reactividad química. Asimismo, está disponible comercialmente tanto en forma líquida como gaseosa. Como líquido, puede ser suministrado en recipientes pequeños llamados frascos de Dewar que permiten almacenar hasta 1.000 litros de helio, o en los contenedores ISO de gran tamaño que tienen una capacidad nominal de hasta 42 m³. En forma gaseosa, se suministran pequeñas cantidades en cilindros de alta presión que pueden contener un volumen equivalente a 8 m³ estándar, mientras que grandes cantidades de gas a alta presión son suministradas en camiones cisterna que tienen una capacidad que equivale 4.860 m³ estándar. Esto es debido a que el volumen del gas se reduce enormemente al ser sometido a altas presiones.


Debido a que el helio es más ligero que el aire, los dirigibles y globos son inflados con este gas para elevarlos. Mientras que el hidrógeno experimenta una fuerza de empuje aproximadamente un 7 % mayor, el helio tiene la ventaja de no ser inflamable (además de ser retardante del fuego). En la industria espacial, se utiliza como un medio de llenado para desplazar a los combustibles y oxidantes en los tanques de almacenamiento, y para condensar el hidrógeno y el oxígeno a fin de producir combustible para cohetes. También se utiliza para depurar el combustible y el oxidante de los equipos de apoyo en tierra antes del lanzamiento, así como para preenfriar el hidrógeno líquido en vehículos espaciales. Por ejemplo, el propulsor del Saturno V utilizado en el Programa Apolo necesitó cerca de 370.000 m³ de helio para poner en marcha el cohete.


El helio es menos denso que el aire atmosférico, por lo que cambia el timbre (mas no la altura) de la voz de una persona cuando se inhala. Esto se debe a que, al ser el helio un gas bastante ligero, se moviliza más rápido por los espacios, produciendo que las cuerdas vocales se muevan a mayor velocidad, provocando una onda sonora más veloz, y por tanto, más aguda. Sin embargo, la inhalación proveniente de una fuente comercial típica, como las utilizadas para rellenar globos, puede ser peligrosa debido al riesgo de asfixia por falta de oxígeno y al número de contaminantes que pueden estar presentes. Entre estos pueden estar incluidas trazas de otros gases, además de aceite lubricante en aerosol. No obstante, al tratarse de productos infantiles, existen mecanismos que exigen garantizar la no toxicidad del gas, como superar la "Conformidad Europea" (marcado CE) obligatorio en juguetes y derivados similares en el mercado europeo, para garantizar la seguridad del público infantil.

Por su baja solubilidad en el tejido nervioso, las mezclas de helio, como trimix, heliox y Heliair se utilizan para el buceo de profundidad para reducir los efectos de la narcosis. A profundidades por debajo de 150 metros, se agregan pequeñas cantidades de hidrógeno a la mezcla de helio-oxígeno para contrarrestar los efectos del síndrome nervioso de alta presión. A estas profundidades se ha descubierto que la baja densidad del helio reduce considerablemente el esfuerzo en la respiración.

Los láseres de helio-neón tienen varias aplicaciones, incluyendo lectores de código de barras.


Una de las aplicaciones industriales del helio es la detección de fugas. Debido a que se difunde a través de sólidos a una tasa tres veces mayor que la del aire, se utiliza como gas indicador para detectar fugas en el equipo de alto vacío y recipientes a alta presión.

La tasa de fugas en recipientes industriales (generalmente cámaras de vacío y tanques criogénicos) se mide haciendo uso del helio, debido a su diámetro molecular pequeño y a su condición de gas inerte. Todavía no se conoce otra sustancia inerte que se pueda filtrar a través de microfisuras o microporos en la pared de un contenedor a un ritmo mayor que el helio. Para encontrar fugas en contenedores se utiliza un detector de fugas de helio (véase espectrómetro de masas). Las fugas de helio a través de grietas no deben confundirse con la penetración de gas a través de un material masivo. A pesar de que se han documentado constantes de permeabilidad para el helio a través de vidrios, cerámicas y materiales sintéticos, los gases inertes como el helio no se pueden permear a través de la mayoría de los metales masivos. Si se necesita conocer la tasa de fuga total del producto que se está probando (por ejemplo en bombas de calor o sistemas de aire acondicionado), el objeto se coloca en una cámara de prueba, el aire dentro de ella se extrae con bombas de vacío y el producto es rellenado con helio a una presión específica. El helio que se escapa a través de las fugas es detectado por un espectrómetro de masas aún a tasas de fuga de hasta 10formula_2 Pa·m³/s. El procedimiento de medición es normalmente automático, y se conoce «como prueba integral de helio». En una prueba más sencilla, el producto se llena de helio y un operador busca manualmente la fuga con un dispositivo llamado "sniffer" (del inglés «olfateador»).


Por su ausencia de reactividad y alta conductividad térmica, su transparencia a los neutrones, y debido a que no forma isótopos radiactivos en condiciones de reactor, se utiliza como medio de transmisión de calor en algunos reactores nucleares enfriados por gas. Otra de sus utilidades consiste en usarlo como gas de protección en los procesos de soldadura por arco en materiales que se contaminan con facilidad por vía aérea.

Debido a que es inerte, se utiliza como gas protector en el crecimiento de cristales de silicio y germanio en la producción de titanio y circonio, además de en la cromatografía de gases. Por esta misma razón, por su conductividad térmica y por la alta velocidad del sonido dentro de él, su naturaleza como gas ideal y el alto valor de su coeficiente de expansión adiabática, también es útil en túneles de viento supersónicos y en instalaciones de prueba donde se requiere una liberación súbita de la energía del gas.

El helio, mezclado con un gas más pesado, como el xenón, es útil para la refrigeración termoacústica debido al elevado coeficiente de expansión adiabática resultante y su bajo número de Prandtl. El comportamiento inerte del helio tiene ventajas ambientales con respecto a los sistemas de refrigeración convencionales, que contribuyen al agotamiento de la capa de ozono o al calentamiento global.

El uso del helio reduce los efectos de distorsión que provocan las variaciones de temperatura en el espacio en las lentes de algunos telescopios, debido a su bajo índice de refracción. Este método es utilizado especialmente en telescopios solares, en los cuales un tubo de vacío fuertemente sellado resultaría demasiado pesado.

Mediante un proceso conocido como datación por helio, puede estimarse la edad de las rocas y minerales que contienen Uranio y Torio.

El helio líquido se utiliza para enfriar ciertos metales —por ejemplo, los imanes superconductores utilizados en la tomografía por resonancia magnética— a temperaturas extremadamente bajas, las cuales son necesarias para la superconductividad. El Gran Colisionador de Hadrones del CERN usa 96 toneladas de helio líquido para mantener la temperatura a 1,9 K. El helio a baja temperatura, también se usa en criogenia.

El helio es un gas portador comúnmente utilizado en la cromatografía de gases.

La primera evidencia de la existencia del helio se observó el 18 de agosto de 1868 como una línea brillante de color amarillo con una longitud de 587,49 nanómetros en el espectro de la cromosfera del Sol. La línea fue detectada por el astrónomo francés Pierre Janssen durante un eclipse solar total en Guntur, India. En un principio se pensó que esta línea era producida por el sodio. El 20 de octubre del mismo año, el astrónomo inglés Joseph Norman Lockyer observó una línea amarilla en el espectro solar, a la cual nombró como la línea de Fraunhofer D porque estaba cerca de las líneas de sodio D y D ya conocidas. Lockyer llegó a la conclusión de que dicha línea era causada por un elemento existente en el Sol pero desconocido en la Tierra. Eduard Frankland confirmó los resultados de Janssen y propuso el nombre "helium" para el nuevo elemento, en honor al dios griego del sol ( Ἥλιος, "Helios"), con el sufijo "-ium" ya que se esperaba que el nuevo elemento fuera metálico.

En 1882, el físico italiano Luigi Palmieri detectó helio en la Tierra por primera vez, a través de su línea espectral D, cuando analizó la lava del monte Vesubio.

El 26 de marzo de 1895 "Sir" William Ramsay aisló el helio al tratar la cleveíta (una variedad de la uranita que contiene por lo menos un 10 % de tierras raras) con ácidos minerales. Ramsey en realidad buscaba argón, pero después de separar el nitrógeno y el oxígeno del gas liberado por el ácido sulfúrico, notó una brillante línea amarilla que coincidía con la línea D observada en el espectro solar. Las muestras fueron identificadas como helio por Lockyer y el físico británico William Crookes. Además fue aislado de la cleveíta el mismo año independientemente por los químicos Per Teodor Cleve y Abraham Langlet en Upsala (Suecia), quienes pudieron obtener suficiente cantidad del gas para determinar acertadamente su peso atómico. El helio también fue aislado por el geoquímico estadounidense William Francis Hillebrand, aunque este atribuyó las líneas al nitrógeno.

En 1907 Ernest Rutherford y Thomas Royds demostraron que las partículas alfa son núcleos de helio, al permitir a las partículas penetrar una delgada pared de un tubo de vidrio al vacío y después creando una descarga eléctrica dentro del mismo para estudiar el espectro del gas. En 1908 el físico holandés Heike Kamerlingh Onnes produjo helio líquido por primera vez enfriando el gas hasta 0,9 K, lo que le hizo merecedor del premio Nobel. Él trató asimismo de solidificar el helio reduciendo su temperatura, aunque no lo logró debido a que este elemento carece de un punto triple, temperatura a la cual las fases sólida, líquida y gaseosa existen en equilibrio. En 1926 su discípulo Willem Hendrik Keesom logró por vez primera solidificar 1 cm³ helio.

En 1938, el físico ruso Pyotr Leonidovich Kapitsa descubrió que el helio-4 casi no tiene viscosidad a temperaturas cercanas al cero absoluto, un fenómeno que ahora se llama superfluidez. Este fenómeno está relacionado con la condensación de Bose-Einstein. En 1972, el mismo fenómeno se observó en el helio-3, pero a temperaturas mucho más cerca del cero absoluto, por los físicos estadounidenses Douglas D. Osheroff, David M. Lee y Robert C. Richardson. Se cree que en el helio-3 el fenómeno está relacionado con la creación de pares de fermiones de este isótopo, de tal manera que se forman bosones, en analogía a los pares de Cooper que producen la superconductividad.

Después de que una operación de perforación de petróleo en 1903 en Dexter, Kansas, produjera un géiser de gas que no se podía quemar, el geólogo Erasmus Haworth recogió muestras de los gases que emanaban y se las llevó a la Universidad de Kansas en Lawrence, donde, con la ayuda de los químicos Hamilton Cady y David McFarland, descubrió que el gas consistía, en volumen, de 72 % de nitrógeno, 15 % de metano (un porcentaje que se puede quemar únicamente con suficiente oxígeno), 1 % de hidrógeno, y 12 % de un gas no identificado. En un análisis posterior, Cady y McFarland descubrieron que el 1,84 % de la muestra de gas era helio. Esto demostró que a pesar de su rareza global en la Tierra, el helio estaba concentrado en grandes cantidades debajo de las Grandes Llanuras de Estados Unidos, disponible para su extracción como un subproducto del gas natural. Las mayores reservas de helio se encontraban en los campos de gas del suroeste de Kansas, de Texas y Oklahoma.

Esto permitió a los Estados Unidos convertirse en el principal productor de helio en el mundo. Siguiendo una sugerencia de Sir Richard Threlfall, la marina de este país patrocinó tres pequeñas plantas experimentales de producción de helio durante la Primera Guerra Mundial. El objetivo era proporcionar a los globos de defensa un gas no inflamable más ligero que el aire. Con este programa se produjeron un total de 5.700 m³ de helio al 92 %, a pesar de que previamente solo se había obtenido menos de un metro cúbico de gas. Parte de él se utilizó en la primera aeronave inflada con helio de la Marina estadounidense, que hizo su primer viaje de Hampton Roads, Virginia, a Bolling Field en Washington D. C., el 1 de diciembre de 1921.

Aunque el proceso de extracción usando licuefacción de gas a baja temperatura no se desarrolló a tiempo para ser relevante durante la Primera Guerra Mundial, la producción continuó. El helio se utilizó principalmente como un gas de elevación en aeronaves más ligeras que el aire. La demanda para este uso, así como para la soldadura por arco fue mayor durante la Segunda Guerra Mundial. El espectrómetro de masas de helio también fue vital en la bomba atómica desarrollada por el Proyecto Manhattan.

El gobierno de los Estados Unidos creó la Reserva Nacional de helio en 1925 en Amarillo, Texas, con el objetivo de suministrárselo a las aeronaves militares en tiempo de guerra, y a las aeronaves comerciales en tiempos de paz. Debido a un embargo militar de Estados Unidos contra Alemania en el que el suministro de helio quedó restringido, el LZ-129 "Hindenburg" se vio obligado a utilizar el hidrógeno como gas elevador. El uso de helio después de la Segunda Guerra Mundial se redujo, pero las reservas se ampliaron en la década de 1950 para garantizar su suministro en forma líquida como refrigerante para crear combustible de hidrógeno y oxígeno (entre otros usos) para los cohetes durante la carrera espacial y la Guerra Fría. El uso de helio en los Estados Unidos en 1965 fue de más de ocho veces el consumo máximo en tiempo de guerra.

La Oficina de Minas de Estados Unidos dispuso de cinco plantas privadas para recuperar helio a partir del gas natural. Para este programa de conservación de helio, la Oficina construyó 684 km de tuberías desde Bushton, Kansas para conectarlas con las plantas del Gobierno parcialmente agotadas en el campo de gas de Cliffside, cerca de Amarillo, Texas. Esta mezcla de helio y nitrógeno fue inyectada y almacenada en el campo de gas de Cliffside hasta que se necesitara, y hasta que fuera purificada posteriormente.

Para 1995 se habían almacenado cerca de mil millones de metros cúbicos de gas, y las reservas constituían una deuda de 1.400 millones de dólares, lo que en 1996 obligó al Congreso de los Estados Unidos a eliminarlas. El helio producido entre 1930 y 1945 tenía aproximadamente un 98,3 % de pureza (con un 2 % de nitrógeno), lo cual fue suficiente para llenar los dirigibles. En 1945, se usó una pequeña cantidad de helio a 99,9 %, para hacer soldaduras. Para 1949 había disponibles cantidades comerciales de helio grado A al 99,9 %.

Durante muchos años, los Estados Unidos han producido más del 90 % de helio que puede utilizarse comercialmente en el mundo, mientras que las plantas de extracción en Canadá, Polonia, Rusia y otros países producen el resto. A mediados de la década de 1990, una nueva planta en Arzew, Argelia entró en funcionamiento y produjo 17 millones de metros cúbicos de helio, con una producción suficiente para cubrir toda la demanda de Europa. Mientras tanto, en 2000, el consumo de helio dentro de los Estados Unidos había aumentado a más de 15 millones de kg por año. Entre 2004 y 2006, se construyeron dos plantas adicionales, una en Ras laffen, Catar y la otra en Skikda, Argelia. Sin embargo a principios de 2007, Ras laffen estaba funcionando al 50 %, y Skikda aún no había sido puesta en marcha. Argelia se convirtió rápidamente en el segundo principal productor de helio. A través de este tiempo, tanto el consumo de helio, como los costos de producción de helio aumentaron. Entre 2002 y 2007 el precio del helio se duplicó, y solo en 2008 los principales proveedores aumentaron sus precios en un 50 %.

El helio neutro en condiciones normales no es tóxico, no juega ningún papel biológico y se encuentra en trazas en la sangre humana. Si se inhala suficiente helio de forma tal que remplace al oxígeno necesario para la respiración, puede generar asfixia. Las precauciones que se deben de tomar para el helio usado en criogenia son similares a las del nitrógeno líquido. Su temperatura extremadamente baja puede causar quemaduras por congelación y la tasa de expansión de líquido a gas puede causar explosiones si no se utilizan mecanismos de liberación de presión.

Los depósitos de helio gaseoso a temperaturas de 5 a 10 K deben almacenarse como si contuvieran helio líquido debido al gran incremento de presión y a la significativa dilatación térmica que se produce al calentar el gas desde una temperatura a menos de 10 K hasta temperatura ambiente.

La velocidad del sonido en el helio es casi tres veces la velocidad del sonido en el aire. Debido a la frecuencia fundamental de una cavidad llena de gas es proporcional a la velocidad del sonido en el gas. Si se inhala helio se produce un aumento correspondiente en las alturas de las frecuencias de resonancia de las cuerdas vocales. (El efecto contrario, la reducción de frecuencias, se puede obtener por la inhalación de un gas denso como el hexafluoruro de azufre).

Su inhalación puede ser peligrosa si se hace en exceso, ya que es un gas asfixiante y desplaza al oxígeno necesario para la respiración normal. La respiración de helio puro continua, causa la muerte por asfixia en pocos minutos. La inhalación de helio directamente de cilindros a presión es extremadamente peligrosa, ya que la alta velocidad de flujo puede resultar en la ruptura de los tejidos pulmonares. Sin embargo, la muerte causada por el helio es muy rara, en los Estados Unidos solo se registraron dos fallecimientos entre 2000 y 2004.

A altas presiones (más de 20 atm o dos MPa), una mezcla de helio y oxígeno (heliox) puede conducir al síndrome de alta presión nerviosa; una especie de efecto anestésico inverso. Añadiendo una pequeña cantidad de nitrógeno a la mezcla puede resolverse el problema.




</doc>
<doc id="22196" url="https://es.wikipedia.org/wiki?curid=22196" title="Vivienda">
Vivienda

La vivienda es una edificación cuya principal función es ofrecer refugio y habitación a las personas, protegiéndolas de las inclemencias climáticas y de otras amenazas. Otras denominaciones de vivienda son: apartamento, aposento, casa, domicilio, estancia, hogar, lar, mansión, morada, piso, etc.

El derecho a la vivienda digna se considera uno de los derechos humanos fundamentales.

El ser humano siempre ha tenido la necesidad de refugiarse para contrarrestar las condiciones adversas de vivir a la intemperie. En la prehistoria, para protegerse del clima adverso o las fieras, solía refugiarse en cuevas naturales, con su familia, bien sea nuclear o extendida. Tradicionalmente, en el mundo rural eran los propios usuarios los responsables de construir su vivienda, según sus propias necesidades y usos a partir de los modelos habituales de su entorno y de los materiales disponibles en la zona; por el contrario, en las ciudades, era más habitual que las viviendas fueran construidas por artesanos o arquitectos especializados. En los países desarrollados, el diseño de las viviendas ha pasado a ser competencia exclusiva de arquitectos e ingenieros, mientras que su construcción es realizada por empresas y profesionales específicos, bajo la dirección técnica del arquitecto y/u otros técnicos.

El "Derecho universal a la vivienda", digna y adecuada, como uno de los derechos humanos, aparece recogido en la Declaración Universal de los Derechos Humanos en su artículo 25, apartado 1 y en el artículo 11 de Pacto Internacional de Derechos Económicos, Sociales y Culturales (PIDESC):

La "Vivienda digna", según la Oficina del Alto Comisionado de las Naciones Unidas para los Derechos Humanos en su Observación General n.º 4 es aquella vivienda donde los ciudadanos o familias pueden vivir con seguridad, paz y dignidad. La vivienda digna se inscribe en el derecho a la vivienda.

Una vivienda digna y adecuada debe ubicarse en espacios suficientemente salubres y equipados, en barrios urbanos o localidades rurales dotados de servicios, accesibles, con espacios intermedios de relación que permita la comunicación vecinal y social y donde sea posible el desarrollo familiar y personal que las sociedades demandan. Para que una vivienda sea digna y adecuada, además debe ser: 1) Vivienda fija y habitable, 2) Vivienda de calidad, 3) Vivienda asequible y accesible y 4) Con seguridad jurídica de tenencia.

Según el Alto Comisionado de las Naciones Unidas para los Refugiados (ACNUR), hay aproximadamente 100 millones de personas sin hogar en todo el mundo. Se han realizado esfuerzos para combatir la falta de hogar tanto a nivel nacional como desde los gobiernos locales, a través de programas de vivienda que tienen entre sus objetivos aumentar la estabilidad residencial para las personas sin hogar.

Una revisión sistemática de 43 estudios, realizados la mayoría en Estados Unidos, y el resto en el Reino Unido, Australia, Canadá y Dinamarca, determinó que una variedad de programas de vivienda e intervenciones de manejo de casos parecen reducir la falta de hogar y mejorar la estabilidad de la vivienda, en comparación con los servicios habituales. Entre estas intervenciones están: vales de vivienda, Housing First (“Vivienda Primero”), tratamientos residenciales, entre otras. Todas ellas parecieran tener efectos beneficiosos similares, por lo que no está claro cuál es las más efectiva. Igualmente, se requiere evidencia de mejor calidad.

Las viviendas desocupadas, deshabitadas o vacías que están en condiciones de habitalidad, que no deben confundirse con las segundas viviendas que se ocupan por cortos períodos de tiempo, constituyen, desde el punto de vista del acceso a la vivienda, un problema social que cuestiona las políticas de viviendas de los distintos países. Se considera una pérdida de recursos y una mala gestión del parque inmobiliario la coexistencia de un número importante de viviendas vacías junto con la demanda insatisfecha de vivienda. Aunque desde el punto de vista del propietario puede no considerarse un problema, si puede asumir fácilmente los costes derivados del mantenimiento de la vivienda sin ocupar, desde el punto de vista social, una vivienda vacía es una patología urbana.

Las políticas públicas en relación con las viviendas deshabitadas son de muy distinto tipo aunque en general se tiende a penalizar, en muy distinta medida, la inacción del propietario -sea una banco, un grupo financiero o inversor, una inmobiliaria o un particular- para dar uso a dicha vivienda. Las medidas van desde la expropiación total o temporal hasta el incremento de diferentes tasas e impuestos.

La primera función de la vivienda es proporcionar un espacio seguro y confortable para resguardarse. El clima condiciona en gran medida tanto la forma de la vivienda como los materiales con que se construye, incluso las funciones que se desarrollan en su interior. Los climas más severos exigen un mayor aislamiento del ambiente exterior mientras que, por otra parte, se tiende a realizar el mayor número posible de actividades en el entorno controlado y confortable de la vivienda; por el contrario, en climas más benignos las exigencias de climatización son mucho más reducidas y, además, gran parte de las actividades cotidianas se realizan fuera de la vivienda.

Generalmente se suele admitir que cada vivienda es ocupada por una familia, pero esta idea debe matizarse: hay distintos tipos de familia y hay viviendas que son ocupadas por varias familias. En el mundo desarrollado se habla de "vivienda colectiva", frente a "vivienda unifamiliar", para referirse a edificios que albergan varias viviendas, cada una de las cuales es habitada por una única familia. Hoy por hoy, y debido a la situación económica, existen las denominadas "viviendas compartidas", que son utilizadas de forma comunitaria por varias personas sin ninguna clase de relación familiar.

Otro aspecto reseñable, ya que condiciona en gran medida las diversas formas de la vivienda en las diferentes culturas, es el conjunto de funciones que se desarrollan en su interior o aledaños. Tareas como la preparación y el cocinado de los alimentos, el lavado de la ropa, el aseo personal o el cuidado de niños y enfermos, y la forma y los medios que se emplean para realizarlas condicionan en gran medida la vivienda. En muchas viviendas, gran parte de estas funciones se han mecanizado mediante los denominados electrodomésticos, de forma que se ha sustituido por consumo energético la necesidad de espacios amplios y la dedicación exclusiva de una o varias personas a estas tareas domésticas. El último paso en esta tendencia lo constituye la domótica que pretende automatizar el mayor número de elementos de la vivienda.


En España es el alojamiento de carácter permanente destinado a satisfacer de manera habitual las necesidades vitales de habitación de una o varias personas. En relación con este concepto, se entiende por:



</doc>
<doc id="22215" url="https://es.wikipedia.org/wiki?curid=22215" title="Horas desesperadas">
Horas desesperadas

Horas desesperadas es una película estadounidense dirigida por William Wyler
Narra la historia de tres convictos escapados de prisión que se esconden a las afueras de la ciudad en un chalet de familia burguesa tomando como rehenes a sus miembros, que vivirán un auténtico horror sin que la policía lo sepa.

Basada en una obra de éxito de Broadway (en los escenarios la interpretaba Paul Newman). En 1990 el director Michael Cimino hizo un "remake" con Anthony Hopkins y Mickey Rourke.


</doc>
<doc id="22225" url="https://es.wikipedia.org/wiki?curid=22225" title="La senda tenebrosa">
La senda tenebrosa

La senda tenebrosa (título original: "Dark Passage") es una película de 1947 dirigida por Delmer Daves.

Humphrey Bogart interpreta a un hombre que ha sido encarcelado injustamente por el supuesto asesinato de su esposa. Escapa de la prisión y decide cambiar sus rasgos mientras intenta demostrar su inocencia. Una atractiva desconocida (Lauren Bacall) le presta ayuda porque su padre también fue víctima de un error judicial.



</doc>
<doc id="22227" url="https://es.wikipedia.org/wiki?curid=22227" title="Marea">
Marea

La marea es el cambio periódico del nivel del mar producido principalmente por las fuerzas de atracción gravitatoria que ejercen el Sol y la Luna sobre la Tierra. Aunque dicha atracción se ejerce sobre todo el planeta, tanto en su parte sólida como líquida y gaseosa, nos referiremos en este artículo a la atracción de la Luna y el Sol, juntos o por separado, sobre las aguas de los mares y océanos. Sin embargo, hay que indicar que las mareas de la litosfera son prácticamente insignificantes, con respecto a las que ocurren en el mar u océano (que pueden modificar su nivel en varios metros) y, sobre todo, en la atmósfera, donde puede variar en varios km de altura, aunque en este caso, es mucho mayor el aumento del espesor de la atmósfera producido por la fuerza centrífuga del movimiento de rotación en la zona ecuatorial (donde el espesor de la atmósfera es mucho mayor) que la modificación introducida por las mareas en dicha zona ecuatorial. 

Otros fenómenos ocasionales, como los vientos, las lluvias, el desborde de ríos y los tsunamis provocan pequeñas variaciones locales del nivel del mar, también ocasionales, pero que no pueden ser calificados de mareas, porque no están causados por la fuerza gravitatoria ni tienen periodicidad.

El fenómeno de las mareas es conocido desde la antigüedad. Parece ser que Piteas (siglo IV a. C.) fue el primero en señalar la relación entre la amplitud de la marea y las fases de la Luna, así como su periodicidad. Plinio el Viejo (23-79) en su "Naturalis Historia" describe correctamente el fenómeno y piensa que la marea está relacionada con la Luna y el Sol. Mucho más tarde, Bacon, Kepler y otros trataron de explicar ese fenómeno, admitiendo la atracción de la Luna y del Sol. Pero fue Isaac Newton en su obra "Philosophiae Naturalis Principia Mathematica" («Principios matemáticos de la Filosofía Natural», 1687) quien dio la explicación de las mareas aceptada actualmente. Más tarde, Pierre-Simon Laplace (1749-1827) y otros científicos ampliaron el estudio de las mareas desde un punto de vista dinámico.

A continuación se recogen los principales términos empleados en la descripción de las mareas:
El tiempo aproximado entre una pleamar y la bajamar es de 6 horas, completando un ciclo de 24 horas 50 minutos.

La explicación completa del mecanismo de las mareas, con todas las periodicidades, es extremadamente larga y complicada. Así que se comenzará empleando todas las simplificaciones posibles para luego acercarse a la realidad suprimiendo algunas de estas simplificaciones.

Se considerará que la Tierra es una esfera sin continentes rodeada por una hidrosfera y que gira alrededor del Sol en una trayectoria elíptica sin girar sobre su eje. Por ahora no se tendrá en cuenta la Luna.

Cuando un astro está en órbita alrededor de otro, la fuerza de atracción gravitacional entre los dos viene dada por la ley de gravitación de Newton:
donde:
Esta fuerza de atracción es la fuerza centrípeta que hace que el astro describa una circunferencia. 
donde:

El valor de la aceleración de gravedad debida al Sol es exactamente el que corresponde a una órbita con la velocidad angular formula_13 y con el centro de masas terrestre a una distancia formula_5 del Sol. Todas las partes de la Tierra tienen la misma velocidad angular alrededor del Sol, pero no están a la misma distancia. Las que están más lejos del centro de masas estarán sometidas a una aceleración de gravedad menor y la que están a una distancia inferior, a una aceleración mayor.

Existe otra fuerza, del mismo orden de magnitud, debida al hecho que las fuerzas de atracción convergen hacia el centro del Sol, que se encuentra situado a una distancia finita. Se describirá más adelante.

En algunas fuentes se comete el error de añadir las aceleraciones centrífugas. Si se opta por utilizar un sistema de referencia inercial (inmóvil respecto a las estrellas), no se deben tener en cuenta las fuerzas centrífugas, que son fuerzas ficticias y que sólo aparecen en sistemas de referencia acelerados. Un observador en la Tierra ve fuerzas centrífugas porque la Tierra está en caída libre hacia el Sol. En cambio, para un observador exterior fijo, solo existen las fuerzas reales, como la fuerza de atracción que constituye la fuerza centrípeta.

El resultado de este pequeño desequilibrio de fuerzas es que el agua de los océanos situada en el lado opuesto al Sol siente una fuerza que la empuja hacia el exterior de la órbita, mientras que el agua situada en el lado orientado hacia el Sol siente una fuerza que la empuja hacia dicho astro. La consecuencia es que la esfera de agua que recubre a la Tierra se alarga ligeramente y se transforma en un elipsoide de revolución cuyo eje mayor está dirigido hacia el Sol. Se verá que este alargamiento relativo es muy pequeño: del orden de uno entre diez millones.

Para calcular la amplitud de las mareas solares, se construyen dos pozos imaginarios desde la superficie hasta el centro de la Tierra. Uno es paralelo a la recta que une la Tierra y el Sol y el otro es perpendicular.

La fuerza y la aceleración que siente el agua en el pozo perpendicular son casi paralelas al eje Tierra-Sol, pero no exactamente. La razón es que el Sol está a una distancia finita y las fuerzas están dirigidas hacia el centro del Sol y no son totalmente paralelas. Calculemos la componente de la aceleración de gravedad perpendicular al eje Tierra-Sol, formula_15, que experimenta el agua situada a una distancia formula_16 del centro de la Tierra. Sin más que proyectar el vector de aceleración, se llega a que: 
Aquí, formula_18 es la aceleración debida a la atracción del Sol:
En esta última fórmula, formula_20 es la masa del Sol y formula_21 es la distancia de la Tierra al Sol. Por su parte, la componente perpendicular al eje queda:
Esta aceleración varía linealmente entre el centro de la Tierra y la superficie. El valor medio se obtiene reemplazando formula_16 por formula_24, donde formula_25 es el radio de la Tierra. Esta aceleración añade un "peso" adicional a la columna de agua del pozo y hace que la presión en el fondo aumente una cantidad formula_26, donde formula_27 es la densidad del agua. Este aumento de la presión, transmitido a la superficie del océano, se corresponde con una variación formula_28 del nivel del océano dada por la fórmula formula_29 (donde formula_30 es la aceleración de gravedad terrestre):
El cálculo numérico da una variación de 8,14 cm.

Se pasará ahora a calcular la disminución formula_32 de la aceleración de gravedad ocasionada por el Sol en un punto situado a una distancia formula_16 del centro de la Tierra. Añadiendo esta distancia adicional en la fórmula de la aceleración gravitatoria:
El primer sumando se corresponde con la aceleración para un cuerpo situado a una distancia formula_35. Por tanto, la "disminución" de la aceleración es:
A su vez, la aceleración media es:
La variación de presión es, como en el caso anterior, formula_38, por lo que:
Esta aceleración da un aumento de la altura del océano de 16,28 cm.

Con la suma de los dos efectos, el semieje mayor del elipsoide es 24,4 cm mayor que el semieje menor. Como la Tierra gira, un punto situado en el ecuador ve la altura del mar llegar a un máximo (pleamar) dos veces por día: cada vez que dicho punto pasa por el semieje mayor. De la misma manera, cada vez que el punto pasa por un semieje menor, la altura del mar pasa por un mínimo (bajamar). La diferencia entre la pleamar y la bajamar es de 24,4 cm. Pero no hay que olvidar que esto sólo es la parte debida al Sol, que no hay continentes y que no se ha tenido en cuenta la inclinación del eje de rotación de la Tierra. La variación de la altura del mar se puede aproximar por una sinusoide con un período de 12 horas.

La Luna gira alrededor de la Tierra, pero esta última no está inmóvil. En realidad, tanto la Luna como la Tierra giran alrededor del centro de masas de los dos astros. Este punto se sitúa aproximadamente a 4.670 km del centro de la Tierra, medido en el lugar de la superficie terrestre que se desplaza de oeste a este con el movimiento de traslación lunar, donde la atracción de nuestro satélite es mayor en un momento dado. Como el radio medio de la Tierra es de 6.367,5 km, el centro de masas se encuentra a unos 1.700 km de profundidad bajo su superficie. La Luna tiene una masa formula_40 kg y está a una distancia media de la Tierra de formula_41 m.
El cálculo de las mareas lunares es similar al cálculo de las mareas solares. Basta con reemplazar la masa y la distancia del Sol por las de la Luna. La diferencia de altura del océano debida al no paralelismo de las fuerzas es:
El cálculo numérico nos da una variación de 17,9 cm.

La diferencia de altura del océano provocada por la diferencia de atracción debida a las distancias diferentes respecto a la Luna es:
El cálculo numérico nos da una variación de 35,6 cm.

La diferencia de longitud entre el semieje mayor y el semieje menor del elipsoide debido a las mareas lunares de 35,6 cm. Por tanto, la amplitud de las mareas lunares es, aproximadamente, dos veces mayor que las de las mareas solares. Como para las mareas solares, la variación de la altura del mar en un punto de la superficie terrestre se puede aproximar por una sinusoide. Esta vez, el período es 12 horas, 25 minutos y 10 s.

El elipsoide debido a las mareas solares tiene el eje mayor dirigido hacia el Sol. El elipsoide debido a las mareas lunares tiene el eje mayor dirigido hacia la Luna. Como la Luna gira alrededor de la Tierra, los ejes mayores de los elipsoides no giran a la misma velocidad. Con respecto a las estrellas, el periodo de rotación del elipsoide solar es de un año. El elipsoide de la Luna es de 27,32 días. El resultado es que los ejes de los dos elipsoides se acercan cada 14,7652944 días. Cuando los ejes mayores de los dos elipsoides están alineados, la amplitud de las mareas es máxima y se llaman mareas vivas o mareas sizigias. Esto sucede en las lunas nuevas y en las lunas llenas. En cambio, cuando el eje mayor de cada elipsoide está alineado con el eje menor del otro, la amplitud de las mareas es mínima. Esto sucede en los cuartos menguantes y los cuartos crecientes. Estas mareas se llaman mareas muertas o mareas de cuadratura.

Hasta ahora se ha ignorado el hecho de que el eje de rotación de la Tierra está inclinado unos 23,27° con respeto a la eclíptica (el plano que contiene la órbita de la Tierra y el Sol). Además, el plano de la órbita de la Luna está inclinado unos 5,145° con respecto a la eclíptica. Esto significa que el Sol ocupa posiciones que van desde 23,44° al norte del plano ecuatorial hasta 23,44° al sur del mismo plano. La Luna puede ocupar posiciones desde 28,6° hasta -28,6°. La consecuencia de esto es que los ejes mayores de los elipsoides que se han utilizado raramente coinciden con el plano del ecuador terrestre.

En la imagen de la derecha, el punto A está en pleamar. Cuando se produzca la próxima pleamar, 12 horas, 25 min y 10 segundos más tarde, el mismo punto se encontrará en B. Esta pleamar será menor que la precedente y que la posterior.

Esta alternancia diurna entre pleamares grandes y pequeñas hace pensar en la suma de dos periodicidades: una diurna y otra semidiurna. Se habla entonces de ondas de marea diurna y semidiurna, tanto lunar como solar. Esto se corresponde con un modelo matemático y no con la realidad física.

Nótese que el punto u y las localizaciones situadas más al norte, solo ven una pleamar por día. Cuando deberían estar en la pequeña pleamar, están aún en el mismo lado del elipsoide. Una situación similar se produce en el Hemisferio Sur. Matemáticamente, la amplitud de la onda semidiurna es demasiado pequeña para que pueda crear máximos o mínimos adicionales.

Las mareas son máximas cuando las dos pleamares son iguales. Eso solo ocurre cuando el eje mayor de los elipsoides es paralelo al plano ecuatorial. Es decir, cuando el sol se encuentra en el plano ecuatorial. Esto ocurre durante los equinoccios. Las mareas de equinoccio son las mayores del año.

Varios factores adicionales también contribuyen a la amplitud de la marea:

En el cálculo simplificado que se ha realizado, en el cual la Tierra no tiene continentes y está recubierta de una hidrosfera continua, la distancia entre las dos posiciones de pleamar es de 20.000 km. La zona de océano cuyo nivel es más alto que el valor medio tiene un diámetro de 10.000 km. Esa distancia es mayor que la distancia entre América y Europa o África y se corresponde con el ancho del Océano Pacífico. Para que todo un océano como el Atlántico o el Pacífico aumentasen de nivel, su contenido total de agua tendría que aumentar. Como los continentes impiden ese movimiento lateral de todo el océano, el modelo de la onda semidiurna no se corresponde con la realidad.

En la imagen de la derecha se puede ver que la altura de los océanos no sigue una onda que se desplaza de derecha a izquierda (hacia el Oeste). El desplazamiento del agua y de los máximos y mínimos es mucho más complicado.

En un modelo sin continentes, las líneas cotidales coinciden con los meridianos. En la imagen de la derecha en color están representadas las líneas cotidales del planisferio y el color del fondo corresponde a la amplitud de mareas. Estas líneas cotidales se corresponden con una situación astronómica particular (Luna creciente, equinoccios, etc.) y cambian con el tiempo. En las dos imágenes se observa que hay líneas cotidales que convergen hacia puntos anfidrómicos, en los cuales la amplitud de la marea es igual a cero.

La situación es aún más marcada en los mares interiores, cuyas dimensiones son aún menores que las de los océanos. Así, el Atlántico no puede llenar o vaciar el Mar Mediterráneo a través el estrecho de Gibraltar. Las aguas del Mediterráneo solo pueden desplazarse hacia el Este o hacia el Oeste, subiendo en un extremo y bajando en el otro. El resultado final se complica por la forma de las costas que limitan y desvían ese movimiento lateral.

En mayor o menor grado,todos los mares interiores y los mares abiertos (aunque en menor grado) presentan un movimiento circular, tanto en las corrientes marinas como en las corrientes de marea y estas corrientes pueden girar en sentido horario en las latitudes intertropicales del hemisferio norte y en sentido antihorario en la zona templada del hemisferio norte. En el caso del hemisferio sur se invierten dichos movimientos giratorios aunque no podemos hablar en este caso de mares, pero es la misma situación con porciones latitudinales de los propios océanos.

Como se ha visto, la amplitud de las mareas en alta mar es menor que 1 metro. En cambio, cerca de las costas la amplitud es generalmente mayor y en algunos casos alcanza o sobrepasa los 10 metros. En la tabla siguiente figuran algunos de los lugares donde se producen grandes mareas. Se ha puesto un solo lugar por zona.

Se explica ahora cómo una marea de menos de un metro en alta mar puede crear una marea de varios metros en la costa. La razón es la resonancia de la capa de agua situada sobre la plataforma continental. Esta capa es poco profunda (menos de 200 m) y, en algunos casos, tiene una gran extensión hasta el talud continental. Por ejemplo, el Canal de la Mancha es una capa de agua de 500 km de largo (desde la entrada hasta el Paso de Calais), 150 km de ancho y solo 100 m de profundidad. A escala, eso se corresponde con una masa de agua de 50 metros de largo y de 1 cm de profundidad. Cuando el nivel del mar aumenta en la entrada, el agua entra en el Canal de la Mancha. Como la extensión es grande y la profundidad pequeña, la velocidad del agua aumenta hasta unos 4 a 5 nudos (2 a 2,5 m/s). Alcanzar esa velocidad toma su tiempo (unas tres horas en el caso del Canal de la Mancha), pero detenerse también requiere un período similar. Una vez lanzada, el agua continúa avanzando, transcurriendo otras tres horas hasta que se para e invierte su dirección. El comportamiento oscilatorio se debe a la inercia y al retardo que tiene la capa de agua para responder a la excitación: la variación de altura del océano más allá del talud continental. La marea será más grande en función de que el período de oscilación propio de la zona sea más próximo al periodo de la excitación externa, que es de 12 horas y 25 minutos.

En la imagen de la izquierda se pueden observar las líneas cotidales en el Canal de la Mancha. Los números de cada línea corresponden al retardo de pleamar con respecto a una referencia. Obsérvese que hay 6 horas de diferencia entre las pleamares de la entrada del Canal de la Mancha y el Paso de Calais. También hay seis horas entre la entrada de la Mancha y el Mar de Irlanda (entre Irlanda e Inglaterra). Hay un punto anfidrómico (en anaranjado) en la entrada del Mar del Norte, frente a Holanda.

El período de oscilación propio de la Bahía de Fundy en Canadá es de 13 horas. Como es muy próximo al período de excitación, las mareas son muy grandes. Por el contrario, cuando el período propio se aleja de las 12,4 h, las amplitudes de las mareas son menores. El período de oscilación propio depende de la forma de la costa y de la profundidad y longitud de la plataforma continental.

En las áreas próximas al ecuador terrestre, las mareas suelen ser muy débiles, casi imperceptibles, salvo en las desembocaduras de los ríos, donde el ascenso de las aguas marinas puede dar origen al represamiento de las aguas fluviales, produciéndose un oleaje río arriba cuando las crestas de la marea entrante rompen contra el agua de los ríos. Este oleaje produce un ruido característico que recibe el nombre de macareo en el delta del Orinoco y pororoca en el río Amazonas.

El motivo de la escasa amplitud de las mareas en la zona intertropical se debe a que es la zona donde los efectos del movimiento de la rotación terrestre son mayores por la fuerza centrífuga generada por dicho movimiento. Debido a la fuerza centrífuga, el nivel del mar es mucho mayor en el ecuador que en las zonas templadas y, sobre todo, en las polares.Como resulta obvio, la mayor altura de las aguas ecuatoriales por la fuerza centrífuga impide que las mareas sean claramente notorias ya que esa fuerza centrífuga se ejerce por igual en toda la circunferencia ecuatorial mientras que las mareas sólo aumentan ese nivel donde se encuentra el paso de la Luna y el Sol, y es un aumento de nivel mucho menor.

Como se ha dicho, la variación de nivel del mar sobre la plataforma continental exige un movimiento alternativo del agua hacia la costa y hacia el mar. Como la profundidad del agua no es la misma cuando la marea sube que cuando baja, la forma de los obstáculos no es la misma, y la dirección y la velocidad de la corriente tampoco es la misma. El vector velocidad dibuja una especie de elipsoide cuyo eje mayor es más o menos paralelo a la costa.

En sitios donde las mareas tienen gran amplitud, las velocidades del mar también pueden ser muy grandes. Por ejemplo, en el Canal de la Mancha, en el Raz de Sein (en el extremo oeste de Bretaña, en Francia) y en el (al norte de la península del Cotentín, también en Francia), la corriente sobrepasa los 10 nudos (18 km/h) durante las grandes mareas. En el estrecho de Mesina, la corriente puede llegar a 5 nudos.

La energía de las mareas ha sido utilizada desde la edad media en Inglaterra, Francia, España y probablemente otros países. Los molinos de mareas de esa época solo funcionaban en reflujo. Estos, como muchos otros molinos hidráulicos, dejaron de utilizarse con la aparición de motores eléctricos.

La instalación de una central mareomotriz crea problemas medioambientales importantes como aterramiento del río, cambios de salinidad en el estuario y sus proximidades y cambio del ecosistema antes y después de las instalaciones.

Las fuerzas de gravedad que provocan las mareas de los océanos también deforman la corteza terrestre. La deformación es importante y la amplitud de la marea terrestre llega a unos 25 a 30 cm en sizigia y casi 50 cm durante los equinoccios.

Al ser el aire atmosférico un fluido, como sucede con las aguas oceánicas, también las dimensiones de la atmósfera sufren la acción de las mareas, afectando su espesor y altura y, por consiguiente, la presión atmosférica. Así, la presión atmosférica disminuye considerablemente durante las fases de luna llena y luna nueva, al ser atraída la columna de aire por el paso, combinado o no, de la luna y el sol por el cenit y/o el nadir. Como hemos visto con las mareas oceánicas, el nivel del mar puede ascender o bajar varios metros cada día en los lugares más propicios (estuarios o bahías). Pero en el caso de la atmósfera su nivel puede ser modificado por la atracción de la luna y el sol en varios km. Hay que tener en cuenta, sin embargo, que la atmósfera tiene un mayor espesor en la zona ecuatorial en especial y en la zona intertropical en general, por la fuerza centrífuga del movimiento de rotación terrestre, por lo que la intensidad de las mareas vendría a superponerse a dicha fuerza centrífuga y, lo mismo que sucede con las mareas oceánicas en la zona intertropical, sus efectos no son tan notorios ya que quedan enmascarados por dicha fuerza centrífuga. Por otra parte, hay que tener en cuenta que el aumento del espesor de la atmósfera por la atracción solar y/o lunar contribuye a la disminución de la presión, a la disminución de la velocidad de los vientos (de ahí el término de calmas ecuatoriales que, aun siendo correcto, se ha venido quedando en desuso) y al aumento de la condensación y de las lluvias.

En la zona intertropical, los cambios de la presión atmosférica durante las mareas atmosféricas dan origen a notables cambios de temperatura que se notan con un simple termómetro y que no se explicarían de otra forma: en luna llena o luna nueva, por ejemplo, puede fácilmente subir un grado o más cerca del mediodía o de la medianoche y en este último caso no tendría explicación si no tuviéramos en cuenta el calentamiento por condensación al disminuir la presión del aire y elevarse. No solo la presión atmosférica se modifica con las mareas atmosféricas, sino también la intensidad de las lluvias. Un estudio meteorológico del mes de octubre de 2012 nos mostraría una alta correlación entre las fases lunares con la mayor intensidad de los huracanes (Nadine, Rafael y Sandy) y/o su disipación. En este último caso, las graves inundaciones causadas por Sandy en New Jersey y Nueva York resultaron de la combinación de la intensa marea producida por la luna llena (el 29 de octubre) y el mar de leva producido por el propio huracán al entrar en la costa de dichos estados, factor explicado en un artículo del NHC (National Hurricane Center) cuya lectura es muy apropiada para la comprensión de este tema:

Tanto la deformación de la Tierra debida a las mareas terrestres como el movimiento del agua de las mareas acuáticas son procesos que disipan energía. El trabajo lo efectúa el momento que la Luna y Sol ejercen sobre la parte deformada de la Tierra y de los océanos. La disipación de energía exige que los ejes mayores de los elipsoides de la hidrosfera y de la Tierra no estén perfectamente alineados con la Luna y el Sol, sino que tengan un pequeño retardo de fase. En el modelo sin continentes, ese retardo correspondería a 3° (y a 12 minutos en tiempo). Ese momento frena la rotación de la Tierra y la duración del día aumenta 17 microsegundos por año (aproximadamente, 1 segundo cada 59.000 años).

La Tierra ejerce el mismo momento sobre la Luna que el que la Luna ejerce sobre la Tierra. El momento que la Tierra ejerce sobre la Luna le comunica energía. Como la Luna está en órbita alrededor de la Tierra, ese aumento de energía se traduce en un aumento de la distancia entre los dos astros y un aumento de la duración del mes lunar. La distancia Tierra-Luna aumenta unos 38 mm por año.

De la misma manera que la Luna crea mareas en la Tierra, tanto acuáticas como terrestres, la Tierra también ejerce mareas sobre la Luna. La fricción debida a esas mareas frenó la rotación de la Luna, provocando que ésta presente siempre la misma cara hacia la Tierra, aunque es justo señalar que este hecho se ha interpretado como el posible origen terrestre de nuestro satélite: siendo la Tierra aún un cuerpo semifluido o incandescente, el movimiento de rotación habría producido una protuberancia que iría aumentando de velocidad por el incremento de la fuerza centrífuga. Con el tiempo, se habrían separado los dos astros, manteniendo la misma cara lunar visible desde la Tierra. En otros satélites del sistema solar que aún giran, la energía disipada por las deformaciones debidas a la marea genera actividad volcánica.





</doc>
<doc id="22236" url="https://es.wikipedia.org/wiki?curid=22236" title="Jorge Sampaio">
Jorge Sampaio

Jorge Fernando Branco de Sampaio () (Lisboa, 18 de septiembre de 1939) fue presidente de Portugal entre 1996 y 2006.

Jorge Sampaio desciende de una familia judaica portuguesa y vivió algunos años de su juventud en Estados Unidos e Inglaterra, debido a la actividad profesional del padre, médico. Su padre, Arnaldo Sampaio, es un especialista en Salud Pública. Su madre, Fernanda Bensaúde Branco de Sampaio, fue profesora particular de lengua inglesa. 

Jorge Sampaio inició su carrera política cuando cursaba Derecho en la Universidad de Lisboa. Él mismo estuvo envuelto en la contestación al régimen fascista y fue líder de la asociación de estudiantes de Lisboa entre 1960 y 1961. Después de su graduación en 1961, Jorge Sampaio inició una carrera notable como abogado, muchas veces envuelto en la defensa de varios prisioneros políticos. 

Después de la Revolución de los Claveles del 25 de abril de 1974, Jorge Sampaio fue fundador del MES (Movimiento de Izquierda Socialista), pero abandonó el proyecto poco después. En 1978 se adhirió al PS, el Partido Socialista, donde permanece hasta hoy. Su primera elección como diputado por Lisboa en la Asamblea de la República fue en 1979. Entre este año y 1984, fue un miembro de la Comisión Europea para los Derechos Humanos, donde desempeñó un trabajo importante en esas materias. 

Entre 1986 y 1987 fue presidente del Grupo Parlamentario del Partido Socialista. En 1989 fue presidente electo del partido, un puesto que ejerció hasta 1991. También en 1989, Jorge Sampaio fue elegido alcalde de Lisboa y posteriormente fue reelegido en 1993.
En 1995, anunció el deseo de presentarse a la Presidencia de la República. Ganó la elección inmediatamente en la primera vuelta, contra Aníbal Cavaco Silva, el anterior primer ministro, y se hizo presidente el 14 de enero de 1996. Después de un primer mandato sin controversias, fue reelegido el 14 de enero de 2001.

Como presidente, su acción se centró en los aspectos sociales y culturales. En la escena política internacional, Sampaio fue un importante contribuidor para la toma de conciencia de la causa por la independencia de Timor Oriental. 

La presidencia de Jorge Sampaio se marcó siempre por un sentido firme de prudencia y moderación, un estilo que le aseguró un primer mandato sin controversias. En 2004, sin embargo, su decisión de no convocar elecciones anticipadas después de la dimisión del Primer Ministro conservador José Manuel Durão Barroso fue discutida por todos los partidos de izquierda y acabó por influir en la decisión de dimisión del líder del Partido Socialista Eduardo Ferro Rodrigues. 

Jorge Sampaio está casado y tiene dos hijos. El 8 de septiembre de 2000 el Gobierno de España le concedió el collar de la Orden de Carlos III

En 2007 fue nombrado por el Secretario General de las Naciones Unidas, Ban Ki-Moon, como Alto Representante de esta organización internacional para la Alianza de Civilizaciones.

 Collar de la Orden de Carlos III (2000)



</doc>
<doc id="22239" url="https://es.wikipedia.org/wiki?curid=22239" title="Caballo peruano de paso">
Caballo peruano de paso

El caballo peruano de paso es una raza equina oriunda del Perú, descendiente de los caballos introducidos durante la Conquista y los primeros tiempos del Virreinato. Esta raza está protegida por el Decreto Ley peruano número 25.919 del 28 de noviembre de 1992 y ha sido declarado "raza caballar propia del Perú" por el Instituto Nacional de Cultura, Así lo instituyó el Ministerio de Comercio Exterior y Turismo (Mincetur) y lo hizo público: su día se celebrará el tercer domingo de abril de cada año. (INC). y "producto de bandera" por el Ministerio de Comercio Exterior y de Turismo en abril de 2013.

Debido al aislamiento sufrido durante alrededor de 400 años y la selección que hicieron sus criadores, es una raza muy particular por sus proporciones corporales y por un andar lateral o "paso llano" que le es característico.
Es típico de las regiones del norte y sur peruano, zonas del país de donde se dio su origen (La Libertad, Lambayeque, Piura y Arequipa). 
En la provincia de Camana departamento de Arequipa considerada una de sus cunas era peculiar observar un tipo de caballo de paso peruano que andaba en diagonal; en el año 2012 se extinguió uno de los pocos ejemplares existentes de este tipo de caballo de paso peruano.

Altura de la cruz: entre 143 y 149 centímetros para las hembras y 144 a 152 centímetros para los machos; El peso oscila entre 400 a 450 kilogramos; Su cuerpo es compacto y musculoso, ancho y profundo; Extremidades alargadas y fuertes; Su cabeza es plana y ancha con ojos brillantes y expresivos; Cuello robusto y musculoso; Su color predominante es el castaño, aunque suelen ser alazanes con capas mezcladas, sus extremidades pueden hasta llegar a medir 50 cm.

Lo que hace a este animal "diferente a otras razas equinas en el mundo es su aire típico de velocidad intermedia, que en los demás es de trote". Este aire "o modalidad" en el andar es el trote lateral o "ambladura" y se denomina paso llano en su ritmo más típico; pero puede tener diferentes ritmos y velocidades, que pueden a su vez ser ejecutados por un mismo ejemplar. 

A esta suma de aires se les llama pasos. Durante la ejecución de estos pasos finos, la cabalgadura tiene un solo y excepcionalmente suave balanceo horizontal; Las otras razas de caballos se balancean horizontal y verticalmente. Esto hace que el cabalgarlo sea especialmente agradable. La suavidad es una de las virtudes fundamentales y más apreciadas en la raza de este caballo. 

En la publicación "andar en Paso Llano", el criador Carlos Parodi García, habla sobre el "paso llano": Es mostrar el desplazamiento armónico isócrono innato de cada batida individual de las extremidades del caballo. El animal levanta la extremidad anterior y posterior del mismo lado, sitúa primero el posterior en el suelo y luego el anterior del mismo lado, igualmente lo hace con el otro bípedo (paso de bípedos laterales en 4 tiempos). Obviamente este movimiento armónico isócrono de batidas individuales va acompañado con los anteriormente enunciados cuando definimos lo que es el "Término". Es importante precisar que, en el tiempo armónico del desplazamiento el caballo peruano de paso llega a tener mayor número de extremidades en apoyo sobre el suelo, en consecuencia mejor impulsión y menor reacción en el momento de impulsión en el traslado del centro de gravedad. De lo cual se desprende las variaciones siguientes en los aires o modalidades del paso llano: Paso Llano Gateado, Paso Llano Picado, Paso Llano Golpeado. Gaitan

El "Término" es un atributo particular que conjuntamente y después de: "la suavidad y el avance", es el espectáculo original e inejecutable por otros caballos, en la observación o evaluación morfológica dinámica del caballo como individuo, en la mecánica de su andar racial. 

El caballo peruano de paso tiene como característica, mayor predominancia en movimientos armónicos isócronos de batidas en los miembros anteriores que en los posteriores.

En consecuencia La ejecución armónica, isócrona y de peculiar graciosidad de elevación, suspensión, rotación elegante fuera de la línea de aplomo, descenso y apoyo de cada batida isócrona, de extremidad anterior o delantera, se denomina "Término".

Además dependiendo de la elevación del brazo, rodilla y caña mostrará mayor o menor agudez en el término.

"La marcha difiere notablemente de los movimientos laterales de otras razas equinas".

Los ascendientes de estos ejemplares fueron embarcados en Sevilla, en Sanlúcar de Barrameda y en Cádiz en el siglo XVI, y por lógica se presume que fueron de raza andaluza.

La estabilización de la raza tomó cerca de cuatro siglos, de varias generaciones de cruces, selección y mejoramiento.
Ayudó bastante el ser un ejemplar de uso como herramienta de trabajo en la agricultura, transportando a los agricultores en la administración y manejo de los campos, principalmente en las haciendas de la costa norte del Perú. Y como animal de silla viajero, para trasportar al jinete de un poblado a otro; igualmente se utilizó en la época para el arreo del ganado de lidia desde las afueras de la capital hacia Lima.

La Asociación Nacional de Criadores y Propietarios de Caballo Peruano de Paso (ANCPCPP), es la única entidad reconocida oficialmente a nivel nacional e internacional, encargada de la conservación, fomento de la crianza, selección, juzgamiento del Caballo Peruano de Paso, así como, el cuidado y uso del apero y la enfrenadura tradicional que lo distinguen. También existen asociaciones departamentales o incluso de otros países que se encargan de la difusión de esta tradición. También pueden organizar concursos regionales, departamentales o nacionales, según el alcance de la asociación. Los concursos organizados por estas instituciones, deben tener el respaldo de la ANCPCPP en cuanto al juzgamiento (solo jueces oficiales, determinados por la misma) y al reglamento único de concursos del Caballo Peruano de Paso.

Desde el año 2008, la ANCPCPP viene editando un Boletín Electrónico cada 2 meses, el cual mantiene al tanto al mundo del acontecer nacional y mundial sobre el Caballo Peruano de Paso.

La Asociación Nacional de Criadores y Propietarios de Caballo Peruano de Paso, como única entidad rectora, promueve y oficializa los certámenes, seminarios, concursos y demás que se llevan a cabo en el Perú y en el extranjero, relacionados con la cría y difusión del Caballo Peruano de Paso. En La ANCPCPP se tiene el Registro Genealógico. El Registro Genealógico, tiene como función el archivo de todos los equinos de raza, como un banco de germoplasma para el mejoramiento de esta raza; sin su inscripción no pueden participar en ningún evento oficial.
El Concejo Distrital del Rímac, en Lima, organizó el primer concurso en la pampa de Amancaes el 24 de junio de 1929. El evento se celebró posteriormente en este escenario hasta el año 1939. Se reanudaron luego en 1941 y 1942, con la variante de que el juzgamiento se hacía previamente en la limeña Plaza de Toros de Acho, de manera que en el día de San Juan desfilaran sólo los premiados y se exhibieran durante la fiesta.
Los Concursos Nacionales se llevan a cabo desde el año 1945, y estos son organizados en el mes de abril principalmente.

Los concursos son en realidad una gran fiesta celebrada alrededor de este original equino, con asistencia de criadores y aficionados de todo el país y muchos del extranjero, donde también se los admira y cría. 

Por lo general la fiesta del caballo peruano de paso dura una semana y se realiza principalmente en Lurín. La final del concurso termina con una exhibición de los caballos favoritos, premios y con demostraciones de las destrezas de los caballos, una de las cuales consiste en que los chalanes hacen desfilar sus caballos al son de las danzas peruanas costeñas, particularmente, la marinera.

Los concursos o el concurso, que puede ser: nacional, departamental o zonal, dependiendo su importancia y localización así como agrupación de criadores o exhibición de ejemplares; es el evento con el que el criador-propietario, mide o compara sus ejemplares, con la finalidad de ver, corregir, y mejorar la reproducción de sus equinos dentro de su criadero. 

El "concurso nacional" donde participan equinos de paso de todo el Perú, se realiza en la ciudad de Lima en el local de la ANCPCPP que está en Lurín, al sur-oeste del conjunto arqueológico de Pachacámac.
Los concursos nacionales que se desarrollan en el país son: 

El apero nacional no es más que el conjunto de arreos o avíos que lleva encima el caballo, conformado por los siguientes elementos elaborados todos en cuero y trabajados a mano por finos talabarteros peruanos quienes adornan el apero nacional con finas piezas de plata. 
El apero nacional consiste de:
El caballo Peruano de paso a ritmo de marinera con un vistoso chalán y su pareja. 

Los chalanes son los jinetes de este caballo y su vestimenta es de color blanco incluyendo el poncho listado, usado en diferentes colores. El poncho en color habano o vicuña, es el más vistoso y el tradicional, en los primeros concursos. (Hasta antes de la Reforma Agraria); el cinturón o correa que sujeta el pantalón, y el calzado o botas pueden ser negras o marrones, sombrero blanco de paja y pañuelo blanco al cuello. Las faldas y blusas de las mujeres pueden llevar o no, un rico bordado en blanco en las telas del mismo color, con sombrero de paja tradicional adornado con flores y con manta o chal del color del poncho, pudiendo ser también de vicuña.
En 2003 y por primera vez en la historia, la inauguración de la "Feria de Abril" de Sevilla, en España, la mayor concentración en el mundo de coches de caballos, contó con un invitado internacional, el Perú, representado por seis elegantes chalanes y sus respectivos Caballos Peruanos de paso, que emocionaron con sus andares al exigente público de La Maestranza de Sevilla, una de las plazas de toros más importantes de España. 

Los Caballos Peruanos de paso acompañaron un carruaje cedido por el Real Club de Enganche de Sevilla y fueron los primeros, de un total de 152 carruajes, en ingresar a La Maestranza, mientras el maestro de ceremonias comentaba que el Perú ha sido el país elegido "por la originalidad y la destreza de su "Caballo Peruano de Paso". Los equinos fueron precedidos por jinetes de la Guardia Civil española, que ondeaban las banderas del Perú y de España, simbolizando la hermandad entre ambos países. 

Los Caballos Peruanos de paso entraron por la Puerta del Príncipe y, tras oírse los himnos nacionales de ambos países, estos desfilaron al ritmo de las canciones peruanas "José Antonio" y "La flor de la canela", una atractiva demostración que mereció un largo aplauso del público asistente.



</doc>
<doc id="22241" url="https://es.wikipedia.org/wiki?curid=22241" title="Victoria pírrica">
Victoria pírrica

Una victoria pírrica es aquella que se consigue con muchas pérdidas en el bando aparentemente o tácticamente vencedor, de modo que incluso tal victoria puede terminar siendo desfavorable para dicho bando.

El nombre proviene de Pirro, rey de Epiro, quien logró una victoria sobre los romanos con el costo de miles de sus hombres. Se dice que Pirro, al contemplar el resultado de la batalla, dijo: «Otra victoria como esta y volveré solo a casa» (en griego: Ἂν ἔτι μίαν μάχην νικήσωμεν, ἀπολώλαμεν).

El filme "187", de Kevin Reynolds, tiene entre sus motivos el sentido de esta locución. Literalmente la locución es dada a entender de este modo: «Así que ahora, cuando alguien dice que algo es una victoria pírrica, ellos quieren decir que es una victoria ganada a un costo demasiado grande».






</doc>
<doc id="22242" url="https://es.wikipedia.org/wiki?curid=22242" title="Barraca">
Barraca

Barraca puede hacer referencia a:


Asimismo, en biología, puede referirse a:


Además, puede hacer referencia a:


</doc>
<doc id="22245" url="https://es.wikipedia.org/wiki?curid=22245" title="Hub">
Hub

Hub puede referirse a: 








</doc>
<doc id="22247" url="https://es.wikipedia.org/wiki?curid=22247" title="Ghost in the Shell">
Ghost in the Shell

Una re-imaginación de la historia original de Masamune Shirow fue realizada en una serie de 4 mangas escritos por Junichi Fujisaku y cinco OVAs titulados "Ghost in the Shell: Arise", que puede ser vista también a modo de precuela de la historia original. Dichos OVAs fueron llevados a una serie para televisión de diez episodios llamada "Ghost in the Shell: Arise - Alternative Architecture", que derivó en la película "Ghost in the Shell: The Rising".

Fueron realizados también cuatro videojuegos, cada obra tiene una línea argumental libre.

Ambientada en el siglo XXI, "Ghost in the Shell" se presenta en una primera lectura como un "thriller" futurista de espionaje, al narrar las misiones de Motoko Kusanagi, la mayor a cargo de las operaciones encubiertas de la Sección Policial de Seguridad Pública 9, o simplemente Sección 9, especializada en crímenes tecnológicos. La misma Kusanagi es un cyborg, poseyendo un cuerpo artificial, lo que le permite ser capaz de realizar hazañas sobrehumanas especialmente requeridas por su labor.

La ambientación de "Ghost in the Shell" es innegablemente ciberpunk, y recuerda a la famosa "Trilogía del Sprawl" de William Gibson. Sin embargo, a diferencia de Gibson, Shirow se interesa más en las consecuencias éticas y filosóficas de la popularización de la unión entre hombre y máquina, el desarrollo de la inteligencia artificial y una red de computadoras omnipresente, temas enfocados en especial a la identidad del ser humano y lo particular de su existencia.

Es así que en este futuro avanzado, la aplicación cotidiana de la tecnología cyborg y el perfeccionamiento de las inteligencias artificiales hace difusa la línea entre los seres vivos y las emulaciones, por ello es que lo único que valida a un ser vivo como un humano con derechos es la existencia de su "ghost", un atributo del cerebro humano que es virtualmente etéreo y lo faculta para generar la auto-consciencia, emociones, individualidad y todos los aspectos que pueden ser calificados como la «personalidad» o «alma». El "ghost" de un individuo puede incluso ser removido parcial o totalmente del cerebro y trasladado a implantes o maquinaria, siendo aun así el individuo reconocido como tal mientras este elemento permanezca intacto. De esta forma, la narrativa presenta una problemática que engloba a todas las anteriores: cómo definir la identidad (ya sea de un individuo o de una máquina).

El manga trata más extensamente estos temas, pues Kusanagi y sus colegas se enfrentan tanto a peligros y acertijos externos como a conflictos internos acerca de su propia naturaleza, debido a que son más máquinas que seres humanos.

El tema principal del manga (y la única historia presente en la película) es la persecución de un criminal de los medios electrónicos, conocido como el Titiritero, y cuya identidad se desconoce. El Titiritero ha cometido varios crímenes con un único "modus operandi": el hackeo del "ghost", que consiste en irrumpir y tomar control de la mente de un ser humano. Al desvelar el misterio del Titiritero, los agentes de la Sección 9 comprenden que no se trata de un criminal común y corriente, sino de un proyecto de inteligencia artificial autónoma que pertenece al Gobierno, al mismo al que la Sección 9 presta servicios, y que se ha fugado a la espera de un cuerpo de verdad y una identidad humana. Si bien en principio Kusanagi se muestra escéptica, finalmente cede para que el Titiritero se una a su conciencia y comparta su cuerpo, lo que hace con la intención de sacar a relucir aún más dudas acerca de la naturaleza de la identidad humana, en un mundo donde la conciencia humana ya no es algo tan particular.

El manga es conocido por la gran cantidad de notas al pie de página y comentarios del propio Shirow, tanto del contexto socio-tecnológico como político de la obra.


Dibujado por entregas de 1989 a 1990. Recopilado en un tomo.

Dibujado por entregas de 1992 a 1995, compuesto por 4 capítulos, publicados en la revista "Young Magazine" de Kōdansha. Editado con un disco compacto complementario en el que se incluye el manga en formato electrónico, con animación en las transiciones de viñetas.

11 capítulos dibujados por entregas de 1991 a 1997 en la revista "Young Magazine" de Kōdansha. Su historia progresa desde el final de "Ghost in the Shell", pero después de haber pasado un tiempo y con una nueva trama. A diferencia de la mayoría de los mangas, tiene una alta cantidad de páginas a color, muchas de ellas realizadas con la ayuda del ordenador.

Película estrenada en 1995 dirigida por Mamoru Oshii, producida por Production I.G y Bandai Visual. A diferencia de la novela gráfica, la cinta animada de 1995 dirigida por Mamoru Oshii se convirtió en una referencia obligada del género "cyberpunk" en los medios audiovisuales, bien sea por la calidad de su animación o por el enfoque que tomó la trama de Shirow (no muy centrada en los problemas culturales y éticos que creó en su obra) cosa que en manos de Oshii se diferenció notablemente; seria, trascendental, y con estilo narrativo propio, casi poético, y es que no hay ningún diálogo o imagen dejado al azar o que desmerezca atención por parte del espectador, que busque en él un asomo de conciencia. Pues si bien "Ghost in the Shell" deja al ser vista por primera vez un sinnúmero de ideas sueltas, es en la recolección de las mismas donde se encuentra su originalidad y belleza acompañada por una banda sonora sombría e inquietante que acompaña un sentido de la fotografía oscuro, penumbroso pero acongojador. Si bien su ambiente puede asemejarse a "Blade Runner" es innegable que "Ghost in the Shell" se aleja notablemente de toda idea sentimental y nos introduce a ese mundo de carácter virtual que pone en peligro no solo nuestra identidad como seres humanos, sino como individuos únicos e irrepetibles.

Película estrenada en 2004 dirigida por Mamoru Oshii, producida por Production I.G y Bandai Visual. Esta segunda parte narra como Batou, ex-compañero de la mayor Kusanagi, investiga una serie de asesinatos y posteriores suicidios por parte de robots destinados al entretenimiento sexual, tiempo después de que Kusanagi desapareciera en la red.

Constituye una versión remasterizada de la película de 1995 con adición de efectos digitales de última generación, escenas nuevas en C.G. y sonido 6.1, aunque desafortunadamente no incluye todas las voces originales.

Película estrenada en los cines japoneses en verano de 2015. Dirigida por Kazuya Nomura, producida por Production I.G. Repite gran parte del equipo de "Ghost in the Shell: Arise".
En ella Osamu Fujimoto es asesor e hijo del primer ministro japonés, al que han asesinado. El suceso, bautizado como el «mayor acontecimiento desde la guerra», hace que Osamu colabore con la mayor Kusanagi y la Sección 9 para descubrir la verdad detrás del asesinato. Esta película también sirve de precuela para su homónima de 1995.

Película en imagen real estrenada en los cines el 31 de marzo de 2017. Dirigida por Rupert Sanders y protagonizada por Scarlett Johansson.

Serie de 26 capítulos emitida en 2002.

Serie de 26 capítulos emitida en 2004. Muchas de las tramas heredan de los mangas "Ghost in the Shell 1.5" y "Ghost in the Shell 2".

Película recopilatoria lanzada directamente a DVD, que recupera toda la sub-trama “El hombre que ríe” de la serie "", añadiendo metraje extra y mejorando la calidad de la animación, equiparándola a la de "Ghost in the Shell 2: Innocence". La película parte con el hecho de que la mayor Motoko Kusanagi, junto a su equipo y el líder Daisuke Aramaki, deben perseguir crímenes cometidos tanto en el mundo real como en el mundo del ciberespacio.

OVA realizado en 2006 (105 min.). Estrenado el 1 de septiembre en Japón. Continúa la trama y personajes trazados en la serie "" y "".

La acción se inicia justo después del intento de independencia por parte del distrito de refugiados de Daiyima y la ruptura del tratado de seguridad con los Estados Unidos de América.

Conjunto de 4 OVA's que sirven como una re-imaginación del "Ghost in the Shell" de Masamune Shirow. La serie de cuatro partes cuenta con nuevos diseños de personajes y es dirigida por Kazuchika Kise, guion de Ubukata Tow y música de Cornelius.

La historia se sitúa en 2027, un año después del final de la Cuarta Guerra Mundial. New Port City todavía se está recuperando de las consecuencias de la guerra cuando cae presa de una mina autopropulsada. Debido a esto, un militar conocido por aceptar sobornos del mercado negro fue ejecutado. Batou, un hombre con un "ojo que nunca duerme", sospecha que Kusanagi es la principal sospechosa de dicho atentado. El detective Togusa, miembro de la Comisaría de Niihama, investiga la ejecución y la muerte de una prostituta.


Re-montaje para televisión de "Ghost in the Shell: Arise", como serie de 10 capítulos. Los primeros 8 capítulos corresponden a las 4 OVA's, cada una de ellas dividida en dos mitades, sin contenido nuevo. Los episodios 9 y 10 son completamente nuevos, conformando una quinta OVA titulada "Pyrophoric Cult", que sirve de puente entre "Ghost in the Shell: Arise" y la película "Ghost in the Shell: The Rising".

Kodansha y Production I.G anunciaron el 7 de abril de 2017 que Kenji Kamiyama y Shinji Aramaki dirigirán una nueva producción de anime, El 7 de diciembre de 2018, Netflix informó que habían adquirido los derechos de transmisión en todo el mundo del anime, titulado Ghost in the Shell: SAC_2045, y que se estrenaría en 2020. La serie estará en 3DCG y Sola Digital Arts estará colaborando con Producción I.G en el proyecto. Más tarde se reveló que Ilya Kuvshinov manejará los diseños de los personajes. Se afirmó que la nueva serie tendrá dos temporadas de 12 episodios cada una.


Una de las películas en las que se puede apreciar claramente la influencia de "Ghost in the Shell" es "The Matrix". Las principales similitudes entre ambas películas son:


Finalmente, las hermanas Wachowski, creadoras de la trilogía "Matrix", reconocieron la influencia de "Ghost in the Shell" en una entrevista. El productor Joel Silver también lo admitió en una entrevista realizada en el DVD de "The Animatrix", en el que fue mostrada una secuencia de "Ghost in the Shell", junto con las Wachowski indicando el estilo que querían lograr ellos para los cortos animados de "The Animatrix".




</doc>
<doc id="22254" url="https://es.wikipedia.org/wiki?curid=22254" title="Microscopio simple">
Microscopio simple

Un microscopio simple es aquel que utiliza una sola lente para ampliar las imágenes de los objetos observados. Es el microscopio más básico. El ejemplo más clásico es la lupa. 
El microscopio óptico estándar, llamado microscopio compuesto, utiliza dos sistemas de lentes alineados, el objetivo y el ocular.


Hace más de quinientos años, se desarrollaron las lupas de cristal simples. Estas eran lentes convexas (más gruesas en el centro que en la periferia). La muestra u objeto podrían entonces ser enfocados por el uso de la lupa colocada entre el objeto y el ojo. Estos "microscopios simples" podrían difundir la imagen en la retina por ampliación mediante el aumento del ángulo visual en la retina.
El objeto a observar se coloca entre el foco y la superficie de la lente, lo que determina la formación de una imagen virtual, derecha y mayor cuanto mayor sea el poder dióptrico de la lente y cuanto más alejado esté el punto próximo de la visión nítida del sujeto.
El aumento obtenido con estos microscopios es reducido, debido a que la longitud de onda de la luz visible le impone limitaciones.

Se le atribuye a el holandés Anton van Leeuwenhoek (1632-1723) haber introducido el microscopio a la atención de los biólogos, a pesar de que ya se estaban produciendo lupas simples en el siglo XVI. Leeuwenhoek construyó microscopios muy eficaces basados en una sola lente.

Sus observaciones fueron lo suficientemente famosas como para recibir a numerosos visitantes de la altura de la reina María II de Inglaterra (1662-1694), Pedro el Grande () o Federico I de Prusia (), además de filósofos y sabios, médicos y eclesiásticos. Van Leeuwenhoek realiza ante ellos numerosas demostraciones: le mostró a Pedro el Grande la circulación sanguínea en la cola de una anguila.

El microscopio que se observa en la foto fue construido hacia 1668 y mide 10 cm de longitud; esos microscopios simples de una sola lente producían una ampliación de hasta 275 veces (275x) y tenían un poder de resolución de 1,4 μm.; no padecían las aberraciones que limitaban la eficacia de los primeros microscopios compuestos, como los empleados por Robert Hooke.
Con ellos Leeuwenhoek fue capaz incluso de describir por primera vez protistas microscópicos de vida libre y parasítica, células espermáticas, células sanguíneas, nematodos microscópicos, rotíferos y hasta bacterias.

Tomó cerca de 150 años de desarrollo, antes de que la óptica del microscopio compuesto fuera capaz de proporcionar la misma calidad de imagen de los microscopios simples de Van Leeuwenhoek, debido a dificultades en las múltiples lentes que utiliza.

http://www.brianjford.com/a-84-roysoc_soiree.pdf

http://www.sciences.demon.co.uk/whistmic.htm

http://www.brianjford.com/wav-mics.htm

http://www.brianjford.com/wav-spf.htm

http://www.brianjford.com/wavrbcs.htm

http://www.ucmp.berkeley.edu/history/hooke.html

http://www.uv.es/mabegaga/leeuwenhoek/leeuvenhoek.html

http://primeroproyectobiologia2014.blogspot.com/2014/03/microscopio-de-leeuwenhoek.html


</doc>
<doc id="22255" url="https://es.wikipedia.org/wiki?curid=22255" title="Microscopio compuesto">
Microscopio compuesto

Un microscopio óptico compuesto, o simplemente microscopio compuesto, es un microscopio que cumple su misión —producir una imagen ampliada de una muestra de algo— por medio de dos sistemas ópticos (hecho cada uno de una o más lentes) que actúan sucesivamente. Se distingue de un microscopio simple (por ejemplo una lupa de mano o una lupa de relojero) que amplía el objeto mediante un solo sistema de lentes (generalmente una sola lente). 

Los microscopios compuestos sirven para ampliar mucho (típicamente un microscopio moderno está preparado para elegir ampliaciones de entre 40 y 1500 veces) un objeto transparente, el cual es iluminado desde el otro lado, al trasluz. Se emplean para examinar cosas que no se distinguen a simple vista, como las células de una muestra de sangre o un tejido. Hay una clase especial de microscopios compuestos, los que se llaman lupas binoculares, que se usan para ampliar modestamente (de 4 a 40 veces en general) y para manipular objetos pequeños y opacos iluminados desde el lado del observador, tales como insectos, flores, joyas o el molde inicial de una moneda. 

Los dos sistemas ópticos por los que llamamos compuesto a un microscopio son el objetivo, que proyecta una primera imagen, y el ocular, que amplía la imagen anterior. La mayoría de los microscopios compuestos están dotados de varios objetivos colocados en un dispositivo rotatorio, el revólver, que permite alternar entre ellos; y la mayoría de los microscopios de trabajo y profesionales están dotados además de dos oculares, que amplían la misma imagen, para que la observación prolongada sea más saludable; pero los microscopios no se llaman compuestos por tener más de un objetivo o más de un ocular, sino porque la imagen que ve el observador se ha formado en dos fases, no en una sola, como en un microscopio simple.

Un microscopio compuesto típico tiene elementos ópticos, que son los fundamentales, y mecánicos. Los elementos ópticos sirven para formar la imagen y para iluminar la muestra. Los elementos mecánicos controlan la distancia del objetivo a la muestra (enfoque) y el desplazamiento de la muestra ante el objetivo, para la elección del área a examinar. También hay elementos mecánicos implicados en ajustar la iluminación de la muestra.
Existen cuatro tipos de lentes, pero solo tres de ellas producen una imagen; La lente condensadora que se encarga de reúne los rayos e ilumina la muestra,La lente objetivo enfoca estos rayos para crear una imagen real y magnificada, el lente ocular utiliza esta imagen para crear una imagen virtual y aumentada y por último el cristalino que crea una imagen real e invertida.

Es el encargado de producir la imagen ampliada de la muestra mediante los dos sistemas de lentes que se sitúan en sus extremos. Estos sistemas son el ocular y el objetivo. El objetivo proyecta una primera imagen de la muestra que el ocular luego amplía; esta producción de la imagen en dos fases es la que justifica la expresión microscopio compuesto, distinguiéndolo del microscopio simple (o lupa).

La mayoría de los microscopios modernos vienen dotados de varios objetivos, que pueden usarse alternativamente, montados en una pieza giratoria, denominada revólver portaobjetivos. Éste está construido de manera que el objetivo que se está usando tiene su eje óptico alineado con el del ocular y también con el del condensador. En los microscopios modernos, tanto los de trabajo o investigación como los empleados en la educación, los objetivos se insertan en el revólver por medio de una rosca estándar, lo que permite sustituirlos. El número de objetivos varía con el tipo de microscopio y el uso a que se destina. Los aumentos de los objetivos secos más frecuentemente utilizados son: 4X, 10X, 20X, 40X y 60X. 

Cada objetivo es un objeto cilíndrico que contiene una serie de lentes coaxiales —tienen sus ejes alineados— con un diseño apropiado para producir una cierta ampliación evitando a la vez los dos problemas mayores de todos los sistemas semejantes, también los objetivos fotográficos, por un lado la aberración esférica y por otro la aberración cromática. Un objetivo se califica como acromático si corrige la segunda, evitando cercos de color, y planacromático si corrige adecuadamnte las dos. En la mayoría de los casos la corrección de aberraciones se logra por el trabajo combinado de objetivo y ocular.

Los objetivos se distinguen principalmente por la ampliación que está previsto obtener con ellos y por su poder de separación, es decir, su resolución. El poder separador depende de un parámetro llamado abertura numérica. Los objetivos destinados a ampliaciones más pequeñas (típicamente 4X) tienen A.N. de 0,10; los de mayor ampliación (100X) tienen A.N. de 1,25.

Cuanto mayor es la ampliación de un objetivo más debe acercarse éste a la muestra. Incluso en objetivos de mediana ampliación, como 40X, la distancia es inferior a un milímetro. En un microscopio la operación de enfocar consiste en ajustar esta distancia. 

Los objetivos de 100 aumentos (100X), y más raramente otros de menor ampliación, suelen ser objetivos de inmersión (y llamamos a los que no lo son objetivos secos). Para el uso de éstos hay que crear entre la muestra y la lente frontal del objetivo, la más cercana a ella, un medio con un índice de refracción continuo (el índice de refracción de un medio transparente mide el grado de desviación que provoca en los rayos luminosos). Para ello se pone sobre la muestra una gota de aceite (clásicamente «aceite de cedro») y se acerca el objetivo a la muestra hasta que su lente frontal queda sumergida en la gota. En la mayoría de los casos entre el objeto observado (por ejemplo una bacteria) y la lente frontal estarán, en este orden, el medio de montaje (una gelatina o una resina), el vidrio cubreobjetos y, por último, el aceite de inmersión.

Las características de un objetivo suelen estar grabadas en un lateral. Las que no faltan son el poder de ampliación (por ejemplo 40X), la abertura numérica (por ejemplo, 0,65), y la distancia a la que proyecta la imagen (por ejemplo 160, porque se da en milímetros). La longitud del tubo óptico y los oculares deben estar ajustados a la misma distancia de proyección.

La lente frontal del objetivo es siempre muy pequeña, menor cuanto mayor su poder de ampliación, y su diámetro es inferior a un milímetro en los objetivos más potentes. Su cuidado, evitando mancharla y limpiándola con medios adaptados, es una parte crítica del mantenimiento de los microscopios, especialmente de los escolares.

En el extremo superior del tubo óptico, el de observación, donde se aproxima el ojo o se monta una cámara, se sitúa el ocular. Un ocular tiene forma cilíndrica y contiene generalmente, como el objetivo, varias lentes coaxiales. A diferencia del objetivo, no se atornilla, sino que se encaja en el tubo óptico como un émbolo, sostenido por su peso, y contenido por un reborde de mayor diámetro en su extremo superior.

Cada ocular lleva grabadas sus características. Nunca falta una de ellas: su poder de ampliación. Se expresa como en el caso de los objetivos con un número seguido de aspas. Los oculares más usados son los de 10X, pero frecuentemente se encuentran los 5X y los 15X. Valores mayores, como 20X, producen ampliaciones más grandes, pero que suelen ser excesivas para la capacidad que tienen los objetivos para resolver el detalle de la muestra, y tienen por ello un uso limitado.

La ampliación total de una observación se obtiene multiplicando la del objetivo por la del ocular. Por ejemplo, con un objetivo de 100X y un ocular 15X, obtenemos una ampliación de 1 500 aumentos (1 500X), que es por cierto la máxima ampliación útil de un microscopio compuesto clásico, dadas las limitaciones de resolución de los objetivos.

La mayoría de los microscopios escolares y muchos de aficionado son monoculares, pero los de rutina (por ejemplo, los usados para examinar muestras médicas) y los de investigación, son binoculares. En estos los rayos luminosos producidos por el objetivo se desdoblan por medio de prismas para dar servicio a dos oculares, de manera que se observa con los dos ojos a la vez. A diferencia de lo que ocurre con unos prismáticos o gemelos de visión lejana (o con el tipo de microscopio compuesto que llamamos lupa binocular) los dos ojos ven exactamente la misma imagen, sin ningún efecto de relieve. Lo que se busca es una observación más descansada, como es exigible por quien pasa horas cada día trabajando con el microscopio.

Algunos microscopios son triloculares, con espacio para tres oculares. En este caso dos se destinan a la observación directa y el tercero al registro fotográfico o videográfico de la imagen producida por el objetivo. En este caso se usan generalmente tres oculares, dos iguales, para los dos ojos, y otro, generalmente de poca ampliación, optimizado para proyectar la imagen sobre el sensor o la película fotográfica.

Este sistema tiene como finalidad dirigir la luz natural o artificial de tal manera que ilumine la preparación u objeto que se va a observar en el microscopio de la manera adecuada. Comprende los siguientes elementos:


El haz luminoso procedente de la lámpara pasa directamente a través del diafragma al condensador. Gracias al sistema de lentes que posee el condensador, la luz es concentrada sobre la preparación a observar. El haz de luz penetra en el objetivo y sigue por el tubo hasta llegar al ocular, donde es captado por el ojo del observador.

Propiedades del microscopio

La parte mecánica del microscopio comprende el pie o la base, el tubo, el revólver, el asa, la platina, el carro y el tornillo micrométrico. Estos elementos sostienen la parte óptica y de iluminación; además, permiten los desplazamientos necesarios para el enfoque del objeto.


Se denomina campo del microscopio al círculo visible que se observa a través del microscopio. También podemos definirlo como la porción del plano visible observado a través del microscopio.

Si el aumento es mayor, el campo disminuye, lo cual quiere decir que el campo es inversamente proporcional al aumento del microscopio. Para medir el diámetro del campo del microscopio con cualquiera de los objetivos se utiliza el micrómetro, al que se hará referencia en el siguiente punto.

Existen diversas clases de microscopios, según la naturaleza de los sistemas de luz, y otros accesorios utilizados para obtener las imágenes.

El microscopio compuesto u óptico utiliza lentes para ampliar las imágenes de los objetos observados. El aumento obtenido con estos microscopios es reducido, debido a la longitud de onda de la luz visible que impone limitaciones. El microscopio óptico puede ser monocular, y consta de un solo tubo. La observación en estos casos se hace con un solo ojo. Es binocular cuando posee dos tubos. La observación se hace con los dos ojos. Esto presenta ventajas tales como mejor percepción de la imagen, más cómoda la observación y se perciben con mayor nitidez los detalles. 

El Microscopio invertido el sistema óptico de este tipo de microscopio está en posición al revés comparado con un microscopio óptico convencional. La fuente de luz y el condensador están dispuestos sobre la plataforma apuntando hacia abajo; y los objetivos y la torrecilla están debajo de la plataforma apuntando hacia arriba. Las únicas partes que están en una disposición normal son el tubo binocular o trinocular, así mismo la muestra que es colocada sobre la plataforma o platina mecánica. Presenta la ventaja de poder observar cultivos enteros o grandes muestras bajo estados más naturales y con menores condiciones de estrés. 

El Microscopio estereoscópico: es un tipo de microscopio hace posible la visión tridimensional de los objetos. Consta de dos tubos oculares y dos objetivos pares para cada aumento. Este microscopio ofrece ventajas para observaciones que requieren pequeños aumentos. El óptimo de visión estereoscópica se encuentra entre 2 y 40X o aumento total del microscopio.

Microscopio de campo oscuro. Este microscopio está provisto de un condensador paraboloide, que hace que los rayos luminosos no penetren directamente en el objetivo, sino que iluminan oblicuamente la preparación. Los objetos aparecen como puntos luminosos sobre un fondo oscuro.

Microscopio de contraste de fases. Se basa en las modificaciones de la trayectoria de los rayos de luz, los cuales producen contrastes notables en la preparación.

Microscopio de fluorescencia. La fluorescencia es la propiedad que tienen algunas sustancias de emitir luz propia cuando inciden sobre ellas radiaciones energéticas. El tratamiento del material biológico con flurocromos facilita la observación al microscopio.

El empleo de microscopios quirúrgicos ha permitido que los cirujanos lleven a cabo intervenciones que parecían imposibles, como la reimplantación de un miembro y la cirugía de los ojos y oídos. Estos microscopios son en especial útiles cuando es necesario realinear para unir o reparar fibras nerviosas y vasos sanguíneos individuales. Se usa para operaciones de dificultad.

Muchas veces interesa al observador conocer el tamaño real de los objetos o microorganismos que está observando a través del microscopio. Para estas mediciones pueden utilizarse varios métodos.

Método de los micrómetros. Se utiliza para esto un micrómetro de platina o de objetivo, que consiste en un portaobjetos en cuyo centro se halla una escala graduada (de 2 mm de longitud), con separaciones, entre cada división, de una centésima de milímetro.

Además se utiliza un micrómetro ocular que lleva una escala graduada en décimas de milímetros. Se coloca el micrómetro objetivo sobre la platina y se enfoca el microscopio hasta que las líneas de la escala graduada aparezcan nítidas. Luego se hace superponer la escala del ocular y se toma como referencia las primeras divisiones en que una línea del micrómetro objetivo y una línea del micrómetro ocular coincidan o se superpongan exactamente.

Luego, por simple regla de tres, se calcula el valor en mieras de cada división ocular. Veamos un ejemplo. Si 9 divisiones del micrómetro objetivo (0,09 mm) equivalen a 30 divisiones del micrómetro ocular, cada división del ocular equivaldrá a:
0,09 mm/30 = 0,003 mm = 3 µm

Quiere decir que para el objetivo calibrado y el ocular utilizado, cada división del micrómetro ocular equivale a 3 µm. Una vez obtenido este dato para cada objetivo en la forma que hemos expuesto, teniendo el microscopio ocular podrían hacerse todas las mediciones que se deseen. Para medir, por ejemplo, un Paramecium de una preparación, procedemos así: haremos coincidir los extremos del microorganismo con las divisiones del micrómetro ocular. Si la longitud del organismo es de 75 divisiones del micrómetro ocular, y cada división equivale a 3 µm, la longitud del Paramecium será 75x3= 225 µm. También se pueden efectuar mediciones en el microscopio con cámara clara y utilizando una regla. En realidad, estas medidas no son tan exactas como cuando se utilizan micrómetros por errores que se introducen superponiendo imágenes.

El microscopio debe estar protegido del polvo, humedad y otros agentes que pudieran dañarlo. Mientras no esté en uso debe guardarse en un estuche o gabinete, o bien cubrirlo con una bolsa plástica o campana de vidrio. 

Las partes mecánicas deben limpiarse con un paño suave; en algunos casos, éste se puede humedecer con xilol para disolver ciertas manchas de grasa, aceite de cedro, parafina, etc. Que hayan caído sobre las citadas partes. 

La limpieza de las partes ópticas requiere precauciones especiales. Para ello debe emplearse papel de óptico que expiden las casas distribuidoras de material de laboratorio ó fotografía o utilizar un paño de algodón. Para el polvillo se puede utilizar un perilla con pincel de pelo de camello. Nunca deben tocarse las lentes del ocular, objetivo y condensador con los dedos; las huellas digitales perjudican la visibilidad, y cuando se secan resulta trabajoso eliminarlas.

Para una buena limpieza de las lentes puede humedecerse el papel de óptica, envolviendo un palillo con algo de punta con una solución de éter/alcohol (70-30 %) y luego pasarlo por la superficie cuantas veces sea necesario. La limpieza de la óptica ha de realizarse en espiral, desde el centro hacia el exterior. El aceite de cedro que queda sobre la lente frontal del objetivo de inmersión debe quitarse inmediatamente después de finalizada la observación. Para ello se puede pasar el papel de óptica impregnado con una gota de xilol. En caso de estar seco debe ponerse la zona a remojo con la solución señalada.

Para guardarlo se acostumbra colocar el objetivo de menor aumento sobre la platina y bajado hasta el tope; el condensador debe estar en su posición más baja, para evitar que tropiece con alguno de los objetivos. Guárdese en lugares secos, para evitar que la humedad favorezca la formación de hongos. Ciertos ácidos y otras sustancias químicas que producen emanaciones fuertes, deben mantenerse alejados del microscopio.

Dos lentes convexas bastan para construir un microscopio. Cada lente hace converger los rayos luminosos que la atraviesan. Una de ellas, llamada objetivo, se sitúa cerca del objeto que se quiere estudiar. El objetivo forma una imagen real aumentada e invertida. Se dice que la imagen es real porque los rayos luminosos pasan realmente por el lugar de la imagen. La imagen es observada por la segunda lente, llamada ocular, que actúa sencillamente como una lupa. El ocular está situado de modo que no forma una segunda imagen real, sino que hace divergir los rayos luminosos, que al entrar en el ojo del observador parecen proceder de una gran imagen invertida situada más allá del objetivo. Como los rayos luminosos no pasan realmente por ese lugar, se dice que la imagen es virtual...



</doc>
<doc id="22259" url="https://es.wikipedia.org/wiki?curid=22259" title="Apertura numérica">
Apertura numérica

En óptica, la apertura numérica (AN) de un sistema óptico es un número adimensional que caracteriza el rango de ángulos para los cuales el sistema acepta luz. Recíprocamente, también está relacionado con el ángulo de salida del sistema. No hay definición exacta del término ya que esta varía según las diferentes áreas de la óptica.

En la mayor parte de las áreas de la óptica, especialmente en microscopía, la apertura numérica de un sistema óptico tal como una lente queda definido por la siguiente ecuación:

donde "n" es el índice de refracción del medio en el que la lente se encuentra (1 para el aire, 1,33 para el agua pura, y hasta 1,56 para algunos aceites), y "θ" es la mitad del ángulo de aceptancia máximo que puede entrar o salir de la lente. La AN se mide generalmente con respecto a un objeto o a un punto de una imagen y varía con la posición del punto.

En microscopía, la apertura numérica es importante porque indica el poder de resolución de una lente. El tamaño del detalle más pequeño que puede ser visualizado es proporcional a λ/AN, donde λ es la longitud de onda de la luz empleada. Una lente con una apertura numérica grande será capaz de visualizar más detalles que una lente con una apertura numérica más baja no podrá. Además, las lentes con aperturas numéricas grandes aceptan más luz y dan una imagen más brillante.

La apertura numérica es una medida del diámetro de la apertura comparada con la distancia focal. En fotografía, esta relación viene dada habitualmente con el número f, "f/#", que para una lente delgada enfocando a un objeto en el infinito vale

En la física de los láseres, la apertura numérica se define de una forma ligeramente diferente. Los haces del láser divergen mientras se propagan, aunque muy lentamente. Lejos de la parte más estrecha del haz, la dispersión aumenta linealmente con la distancia — el haz del láser forma un cono de luz. La misma relación nos da la apertura numérica
pero "θ" se define de forma diferente a como se hacía antes. Los haces láser no suelen tener bordes abruptos como el cono de luz que pasa por una lente. En los láseres, la irradiancia decae gradualmente según nos vamos alejando del centro del haz. El perfil de decaimiento es típicamente Gaussiano. Los físicos de láseres llaman "θ" a la divergencia del haz: más allá de λ/4, el ángulo entre la dirección de propagación y el ángulo para el cual la irradiancia cae a 1/e veces la irradiancia máxima. La AN de un haz láser Gaussiano está relacionada con la mancha de spot mínima con la siguiente expresión
donde "λ" es la longitud de onda de la luz, y "D" es el diámetro del haz láser en su punto más estrecho, medido entre los puntos de irradiancia 1/e veces la irradiancia máxima. Nótese que esta expresión implica que un haz láser que enfoca con una mancha de spot pequeña divergirá rápidamente según se mueve del foco, mientras que a un haz láser muy ancho no le ocurrirá.

Ángulo de Aceptancia: es el máximo ángulo en el cual el rayo de luz incidente es atrapado por las paredes de la fibra. En este caso el rayo de luz se refleja totalmente en el recubrimiento de la misma, por lo que el ángulo de transmisión, sobre el recubrimiento de la fibra, es 90º.

Las fibras ópticas monomodo sólo guían la luz que entra en la fibra dentro de un determinado cono de aceptancia. La mitad del ángulo de este cono es el ángulo de aceptancia, "θ". Para fibras con perfil de salto de índice multimodo, este ángulo de aceptancia viene determinado por la siguiente expresión
donde "n" es el índice de refracción del núcleo de la fibra, y "n" es el índice de refracción de la cubierta.

Debido al gran parecido de esta expresión con las definiciones de AN de otras áreas de la óptica, es habitual llamar así al término de la derecha de la ecuación anterior, definiendo finalmente la apertura numérica de una fibra como
donde "n" es el índice de refracción del eje central de la fibra. Nótese que cuando se usa esta definición, la relación entre la apertura numérica y el ángulo de aceptancia es una mera aproximación. En particular, los fabricantes suelen dar la AN para fibras monomodo basándose en esta expresión, aunque para este tipo de fibras el ángulo de aceptancia es algo diferente y no depende solamente de los índices de refracción de núcleo y cubierta.


</doc>
<doc id="22261" url="https://es.wikipedia.org/wiki?curid=22261" title="Revolución francesa">
Revolución francesa

La Revolución francesa () fue un conflicto social y político, con diversos periodos de violencia, que convulsionó Francia y, por extensión de sus implicaciones, a otras naciones de Europa que enfrentaban a partidarios y opositores del sistema conocido como el Antiguo Régimen. Se inició con la autoproclamación del Tercer Estado como Asamblea Nacional en 1789 y finalizó con el golpe de Estado de Napoleón Bonaparte en 1799.

Si bien, después de que la Primera República cayera tras el golpe de Estado de Napoleón Bonaparte, la organización política de Francia durante el siglo XIX osciló entre república, imperio y monarquía constitucional, lo cierto es que la revolución marcó el final definitivo del feudalismo y del absolutismo en ese país, y dio a luz a un nuevo régimen donde la burguesía, apoyada en ocasiones por las masas populares, se convirtió en la fuerza política dominante en el país. La revolución socavó las bases del sistema monárquico como tal, más allá de sus estertores, en la medida en que lo derrocó con un discurso e iniciativas capaces de volverlo ilegítimo.

Según la historiografía clásica, la Revolución francesa marca el inicio de la Edad Contemporánea al sentar las bases de la democracia moderna, lo que la sitúa en el corazón del siglo XIX. Abrió nuevos horizontes políticos basados en el principio de la soberanía popular, que será el motor de las revoluciones de 1830, de 1848 y de 1871.

Los escritores ilustrados del siglo XVIII, filósofos, politólogos, científicos y economistas, denominados comúnmente "philosophes", y a partir de 1751 los enciclopedistas, contribuyeron a minar las bases del derecho divino de los reyes. La filosofía de la Ilustración ha desempeñado pues un rol significativo en el giro que tomaron estos eventos históricos pero su influencia debe relatarse de modo más matizado: acordarle demasiada importancia a los preceptos filosóficos nacidos durante ese siglo se revelaría como una carencia mayúscula de fidelidad historiográfica.

La corriente de pensamiento vigente en Francia era la Ilustración, cuyos principios se basaban en la razón, la igualdad y la libertad. La Ilustración había servido de impulso a las Trece Colonias norteamericanas para la independencia de su metrópolis europea. Tanto la influencia de la Ilustración como el ejemplo de los Estados Unidos sirvieron de «trampolín» ideológico para el inicio de la revolución en Francia.

En términos generales fueron varios los factores que influyeron en la Revolución:

Desde el punto de vista político, fueron fundamentales ideas tales como las expuestas por Voltaire, Rousseau, Diderot o Montesquieu (como por ejemplo, los conceptos de libertad política, de fraternidad y de igualdad, o de rechazo a una sociedad dividida, o las nuevas teorías políticas sobre la separación de poderes del Estado). Todo ello fue rompiendo el prestigio de las instituciones del Antiguo Régimen, ayudando a su desplome.

Desde el punto de vista económico, la inmanejable deuda del Estado fue exacerbada por un sistema de extrema desigualdad social y de altos impuestos que los estamentos privilegiados, nobleza y clero no tenían obligación de pagar, pero que sí oprimía al resto de la sociedad. Hubo un aumento de los gastos del Estado simultáneo a un descenso de la producción agraria de terratenientes y campesinos, lo que produjo una grave escasez de alimentos en los meses precedentes a la Revolución. Las tensiones, tanto sociales como políticas, mucho tiempo contenidas, se desataron en una gran crisis económica a consecuencia de los dos hechos puntuales señalados: la colaboración interesada de Francia con la causa de la independencia estadounidense (que ocasionó un gigantesco déficit fiscal) y el aumento de los precios agrícolas.

El conjunto de la población mostraba un resentimiento generalizado dirigido hacia los privilegios de los nobles y del alto clero, que mantenían su dominio sobre la vida pública impidiendo que accediera a ella una pujante clase profesional y comerciante. El ejemplo del proceso revolucionario estadounidense abrió los horizontes de cambio político entre otros.

Los Estados Generales estaban formados por los representantes de cada estamento. Estos estaban separados a la hora de deliberar, y tenían solo un voto por estamento. La convocatoria de 1789 fue un motivo de preocupación para la oposición, por cuanto existía la creencia de que no era otra cosa que un intento, por parte de la monarquía, de manipular la asamblea a su antojo. La cuestión que se planteaba era importante. Estaba en juego la idea de soberanía nacional, es decir, admitir que el conjunto de los diputados de los Estados Generales representaba la voluntad de la nación.

El tercer impacto de los Estados Generales fue de gran tumulto político, particularmente por la determinación del sistema de votación. El Parlamento de París propuso que se mantuviera el sistema de votación que se había usado en 1614, si bien los magistrados no estaban muy seguros acerca de cuál había sido en realidad tal sistema. Sí se sabía, en cambio, que en dicha asamblea habían estado representados (con el mismo número de miembros y con un solo voto) el clero (Primer Estado), la nobleza (Segundo Estado) y el resto de la población (Tercer Estado, principalmente la burguesía y el campesinado). Inmediatamente, un grupo de liberales parisinos denominado «Comité de los Treinta», compuesto principalmente por gente de la nobleza, comenzó a protestar y agitar, reclamando que se duplicara el número de asambleistas con derecho a voto del Tercer Estado (es decir, los «Comunes»). El gobierno aceptó esta propuesta, pero dejó a la Asamblea la labor de determinar el derecho de voto. Este cabo suelto creó gran tumulto.

El rey Luis XVI y una parte de la nobleza no aceptaron la situación. Los miembros del Tercer Estamento se autoproclamaron Asamblea Nacional, y se comprometieron a escribir una constitución. Sectores de la aristocracia confiaban en que estos Estados Generales pudieran servir para recuperar parte del poder perdido, pero el contexto social ya no era el mismo que en 1614. Ahora existía una élite burguesa que tenía una serie de reivindicaciones e intereses que chocaban frontalmente con los de la nobleza (y también con los del pueblo, cosa que se demostraría en los años siguientes).

Cuando finalmente los Estados Generales de Francia se reunieron en Versalles el 5 de mayo de 1789 y se originaron las disputas respecto al tema de las votaciones, los miembros del Tercer Estado debieron verificar sus propias credenciales, comenzando a hacerlo el 28 de mayo y finalizando el 17 de junio, cuando los miembros del Tercer Estado se declararon como únicos integrantes de la Asamblea Nacional: esta no representaría a las clases pudientes sino al pueblo en sí. La primera medida de la Asamblea fue votar la Declaración de los Derechos del Hombre y del Ciudadano. Si bien invitaron a los miembros del Primer y Segundo Estado a participar en esta asamblea, dejaron en claro sus intenciones de proceder incluso sin esta participación.

La monarquía, opuesta a la Asamblea, cerró las salas donde esta se estaba reuniendo. Los asambleistas se mudaron a un edificio cercano, donde la aristocracia acostumbraba a jugar el juego de la pelota, conocido como "jeu de paume". Allí es donde procedieron con lo que se conoce como el Juramento del Juego de la Pelota el 20 de junio de 1789, prometiendo no separarse hasta tanto dieran a Francia una nueva constitución. La mayoría de los representantes del bajo clero se unieron a la Asamblea, al igual que 47 miembros de la nobleza. Ya el 27 de junio, los representantes de la monarquía se dieron por vencidos, y por esa fecha el rey mandó reunir grandes contingentes de tropas militares que comenzaron a llegar a París y Versalles. Los mensajes de apoyo a la Asamblea llovieron desde París y otras ciudades. El 9 de julio la Asamblea se nombró a sí misma Asamblea Nacional Constituyente.

El 11 de julio de 1789, el rey Luis XVI, actuando bajo la influencia de los nobles conservadores al igual que la de su hermano, el conde D'Artois, despidió al ministro Necker y ordenó la reconstrucción del Ministerio de Finanzas. Gran parte del pueblo de París interpretó esta medida como un autogolpe de la realeza, y se lanzó a la calle en abierta rebelión. Algunos de los militares se mantuvieron neutrales, pero otros se unieron al pueblo.

El 14 de julio el pueblo de París respaldó en las calles a sus representantes y, ante el temor de que las tropas reales los detuvieran, asaltaron la fortaleza de la Bastilla, símbolo del absolutismo monárquico, pero también punto estratégico del plan de represión de Luis XVI, pues sus cañones apuntaban a los barrios obreros. Tras cuatro horas de combate, los insurgentes tomaron la prisión, matando a su gobernador, el marqués Bernard de Launay. Si bien solo cuatro presos fueron liberados, la Bastilla se convirtió en un potente símbolo de todo lo que resultaba despreciable en el Antiguo Régimen. Retornando al ayuntamiento, la multitud acusó al alcalde Jacques de Flesselles de traición, quien recibió un balazo que lo mató. Su cabeza fue cortada y exhibida en la ciudad clavada en una pica, naciendo desde entonces la costumbre de pasear en una pica las cabezas de los decapitados, lo que se volvió muy común durante la Revolución.

La Revolución se fue extendiendo por ciudades y pueblos, creándose nuevos ayuntamientos que no reconocían otra autoridad que la Asamblea Nacional Constituyente. La insurrección motivada por el descontento popular siguió extendiéndose por toda Francia. En las áreas rurales, para protestar contra los privilegios señoriales, se llevaron a cabo actos de quema de títulos sobre servidumbres, derechos feudales y propiedad de tierras, y varios castillos y palacios fueron atacados. Esta insurrección agraria se conoce como "la Grande Peur" (el Gran Miedo).

La noche del 4 de agosto de 1789, la Asamblea Nacional Constituyente, actuando detrás de los nuevos acontecimientos, suprimió por ley las servidumbres personales (abolición del feudalismo), los diezmos y las justicias señoriales, instaurando la igualdad ante el impuesto, ante penas y en el acceso a cargos públicos. En cuestión de horas, los nobles y el clero perdieron sus privilegios. El curso de los acontecimientos estaba ya marcado, si bien la implantación del nuevo modelo no se hizo efectiva hasta 1793. El rey, junto con sus seguidores militares, retrocedió al menos por el momento. Lafayette tomó el mando de la Guardia Nacional de París y Jean-Sylvain Bailly, presidente de la Asamblea Nacional Constituyente, fue nombrado nuevo alcalde de París. El rey visitó París el 27 de julio y aceptó la escarapela tricolor.

Sin embargo, después de estos actos de violencia, los nobles, no muy seguros del rumbo que tomaría la reconciliación temporal entre el rey y el pueblo, comenzaron a salir del país, algunos con la intención de fomentar una guerra civil en Francia y de llevar a las naciones europeas a respaldar al rey. Estos fueron conocidos como los "émigrés" (emigrados).

La revolución se enfrentó duramente con la Iglesia católica, que pasó a depender del Estado. En 1790 se eliminó la autoridad de la Iglesia de imponer impuestos sobre las cosechas, se eliminaron también los privilegios del clero y se confiscaron sus bienes. Bajo el Antiguo Régimen la Iglesia era el mayor terrateniente del país. Más tarde se promulgó una legislación que convirtió al clero en empleados del Estado. Estos fueron unos años de dura represión para el clero, siendo comunes la prisión y masacre de sacerdotes en toda Francia. El Concordato de 1801 entre la Asamblea y la Iglesia finalizó este proceso y establecieron normas de convivencia que se mantuvieron vigentes hasta el 11 de diciembre de 1905, cuando la Tercera República sentenció la separación definitiva entre la Iglesia y el Estado. El viejo calendario gregoriano, propio de la religión católica, fue anulado por Billaud-Varenne, en favor de un «calendario republicano» y una nueva era, que establecía como primer día el 22 de septiembre de 1792.

En una Asamblea que se quería plural y cuyo propósito era la redacción de una constitución democrática, los 1200 constituyentes representaban las diversas tendencias políticas del momento.




En ese primer periodo constituyente, los líderes indiscutibles de la Asamblea eran Mirabeau y el abad Sieyès.

El 27 de agosto de 1789 la Asamblea publicó la Declaración de los Derechos del Hombre y del Ciudadano inspirándose en parte en la Declaración de Independencia de los Estados Unidos y estableciendo el principio de libertad, igualdad y fraternidad. Dicha declaración establecía una declaración de principios que serían la base ineludible de la futura Constitución.

La Asamblea Nacional Constituyente no era solo un órgano legislativo, sino la encargada de redactar una nueva constitución. Algunos, como Necker, favorecían la creación de una asamblea bicameral en donde el Senado sería escogido por la Corona entre los miembros propuestos por el pueblo. Los nobles, por su parte, favorecían un Senado compuesto por miembros de la nobleza elegidos por los propios nobles. Prevaleció, sin embargo, la tesis liberal de que la Asamblea tendría una sola cámara, quedando el rey solo con el poder de veto, pudiendo posponer la ejecución de una ley, pero no su total eliminación.

El movimiento de los monárquicos para bloquear este sistema fue desmontado por el pueblo de París, compuesto fundamentalmente por mujeres (llamadas despectivamente «las Furias»), que marcharon el 5 de octubre de 1789 sobre Versalles. Tras varios incidentes, el rey y su familia se vieron obligados a abandonar Versalles y se trasladaron al palacio de las Tullerías en París.

Los electores habían escogido a los miembros de los Estados Generales por un periodo de un año, pero de acuerdo al Juramento del Juego de Pelota, los miembros del Tercer Estado, también llamados los «comunes», acordaron no abandonar la Asamblea en tanto no se hubiera elaborado una constitución.

Durante 1790 se produjeron movimientos antirrevolucionarios, pero sin éxito. En este periodo se intensificó la influencia de los «clubes» políticos, entre los que destacaban los jacobinos y los cordeliers. En agosto de 1790 existían 152 clubes jacobinos.

A principios de 1791, la Asamblea consideró introducir una legislación contra los franceses que emigraron durante la Revolución ("émigrés"). Se pretendía coartar la libertad de salir del país para fomentar desde el extranjero la creación de ejércitos contrarrevolucionarios, y evitar la fuga de capitales. Mirabeau se opuso rotundamente a esto. Sin embargo, el 2 de marzo de 1791 Mirabeau fallece, y la Asamblea adopta esta draconiana medida.

El 20 de junio de 1791, Luis XVI, opuesto al curso que iba tomando la Revolución, huyó junto con su familia de las Tullerías. Sin embargo, al día siguiente cometió la imprudencia de dejarse ver, fue arrestado en Varennes por un oficial del pueblo y devuelto a París escoltado por la guardia. A su regreso a París el pueblo se mantuvo en silencio y, tanto él como su esposa, María Antonieta, sus dos hijos (María Teresa y Luis-Carlos, futuro Luis XVII) y su hermana (Madame Elizabeth) permanecieron bajo custodia.

El 3 de septiembre de 1791, fue aprobada la primera constitución de la historia de Francia. Una nueva organización judicial dio características temporales a todos los magistrados y total independencia de la Corona. Al rey solo le quedó el poder ejecutivo y el derecho de vetar las leyes aprobadas por la Asamblea Legislativa. La Asamblea, por su parte, eliminó todas las barreras comerciales y suprimió las antiguas corporaciones mercantiles y los gremios; en adelante, los individuos que quisieran desarrollar prácticas comerciales necesitarían una licencia, y se abolió el derecho a la huelga.

Aun cuando existía una fuerte corriente política que favorecía la monarquía constitucional, al final venció la tesis de mantener al rey como una figura decorativa. Jacques Pierre Brissot introdujo una petición insistiendo en que, a los ojos del pueblo, Luis XVI había sido depuesto por el hecho de su huida. Una inmensa multitud se congregó en el Campo de Marte para firmar dicha petición. Georges-Jacques Danton y Camille Desmoulins pronunciaron discursos exaltados. La Asamblea pidió a las autoridades municipales guardar el orden. Bajo el mando de Lafayette, la Guardia Nacional se enfrentó a la multitud. Al principio, tras recibir una oleada de piedras, los soldados respondieron disparando al aire; dado que la multitud no cedía, Lafayette ordenó disparar a los manifestantes, ocasionando más de cincuenta muertos.

Tras esta masacre, las autoridades cerraron varios clubes políticos, así como varios periódicos radicales, como el que editaba Jean-Paul Marat. Danton se fugó a Inglaterra y Desmoulins y Marat permanecieron escondidos.

Mientras tanto, la Asamblea había redactado la Constitución y el rey había sido mantenido, aceptándola. El rey pronunció un discurso ante la Asamblea, que fue acogido con un fuerte aplauso. La Asamblea Nacional Constituyente cesó en sus funciones el 29 de septiembre de 1791.

Bajo la Constitución de 1791, Francia funcionaría como una monarquía constitucional. El rey tenía que compartir su poder con la Asamblea, pero todavía mantenía el poder de veto y la potestad de elegir a sus ministros.

La Asamblea Legislativa se reunió por primera vez el 1 de octubre de 1791. La componían 264 diputados situados a la derecha: "feuillants" (dirigidos por Barnave, Duport y Lameth), y girondinos, portavoces republicanos de la gran burguesía. En el centro figuraban 345 diputados independientes, carentes de programa político definido. A la izquierda 136 diputados inscritos en el club de los jacobinos o en el de los cordeliers, que representaban al pueblo llano parisino a través de sus periódicos "L´Ami du Peuple" y "Le Père Duchesne", y con Marat y Hebert como portavoces. Pese a su importancia social y el apoyo popular y de la pequeña burguesía, en la Asamblea era escasa la influencia de la izquierda, pues la Asamblea estaba dominada por las ideas políticas que representaban los girondinos. Mientras los jacobinos tenían detrás a la gran masa de la pequeña burguesía, los cordeliers contaban con el apoyo del pueblo llano, a través de las secciones parisienses.

Este gran número de diputados se reunían en los clubes, germen de los partidos políticos. El más célebre de entre estos fue el partido de los jacobinos, dominado por Robespierre. A la izquierda de este partido se encontraban los cordeliers, quienes defendían el sufragio universal masculino (derecho de todos los hombres al voto a partir de una determinada edad). Los cordeliers querían la eliminación de la monarquía e instauración de la república. Estaban dirigidos por Jean-Paul Marat y Georges-Jacques Danton, representando siempre al pueblo más humilde. El grupo de ideas más moderadas era el de los girondinos, que defendían el sufragio censitario y propugnaban una monarquía constitucional descentralizada. También se encontraban aquellos que formaban parte de «el Pantano», o «el Llano», como eran llamados aquellos que no tenían un voto propio, y que se iban por las proposiciones que más les convenían, ya vinieran de los jacobinos o de los girondinos.

En los primeros meses de funcionamiento de la Asamblea, el rey había vetado una ley que amenazaba con la condena a muerte a los "émigrés", y otra que exigía al clero prestar juramento de lealtad al Estado. Desacuerdos de este tipo fueron los que llevaron más adelante a la crisis constitucional.

Mientras tanto, dos potencias absolutistas europeas, Austria y Prusia, se dispusieron a invadir la Francia revolucionaria, lo que hizo que el pueblo francés se convirtiera en un ejército nacional, dispuesto a defender y a difundir el nuevo orden revolucionario por toda Europa. Durante la guerra, la libertad de expresión permitió que el pueblo manifestase su hostilidad hacia la reina María Antonieta (llamada la Austriaca por ser hija de un emperador de aquel país y Madame Déficit por el gasto que había representado al Estado, que no era mayor que la mayoría de los cortesanos) y contra Luis XVI, que casi siempre se negaba a firmar leyes propuestas por la Asamblea Legislativa.

El 10 de agosto de 1792, las masas asaltaron el palacio de las Tullerías, y la Asamblea Legislativa suspendió las funciones constitucionales del rey. La Asamblea acabó convocando elecciones con el objetivo de configurar (por sufragio universal) un nuevo parlamento que recibiría el nombre de Convención. Aumentaba la tensión política y social en Francia, así como la amenaza militar de las potencias europeas. El conflicto se planteaba así entre una monarquía constitucional francesa en camino de convertirse en una democracia republicana, y las monarquías europeas absolutas. El nuevo parlamento elegido ese año abolió la monarquía y proclamó la república. Creó también un nuevo calendario, según el cual el año 1792 se convertiría en el año 1 de su nueva era.

El gobierno pasó a depender de la Comuna Insurreccional. Cuando la Comuna envió grupos de sicarios a las prisiones, asesinaron a 1400 víctimas, y pidió a otras ciudades de Francia que hicieran lo mismo, la Asamblea no opuso resistencia. Esta situación persistió hasta el 20 de septiembre de 1792, en que se creó un nuevo cuerpo legislativo denominado Convención, que de hecho se convirtió en el nuevo gobierno de Francia.

El poder legislativo de la nueva República estuvo a cargo de la Convención Nacional, mientras que el poder ejecutivo recayó sobre el Comité de Salvación Pública.

En el manifiesto de Brunswick, los Ejércitos Imperiales y de Prusia amenazaron con invadir Francia si la población se resistía al restablecimiento de la monarquía. Esto ocasionó que Luis XVI fuera visto como conspirador con los enemigos de Francia. El 17 de enero de 1793, la Convención condenó al rey a muerte por una pequeña mayoría, acusándolo de «conspiración contra la libertad pública y la seguridad general del Estado». El 21 de enero el rey fue ejecutado, lo cual encendió nuevamente la mecha de la guerra con otros países europeos. La reina María Antonieta, nacida en Austria y hermana del emperador, fue ejecutada el 16 de octubre del mismo año, iniciándose así una revolución en Austria para sustituir a la reina. Esto provocó la ruptura de toda relación entre ambos países.

El mismo día en el que se reunía la Convención (20 de septiembre de 1792), todas las tropas francesas (formadas por tenderos, artesanos y campesinos de toda Francia) derrotaron por primera vez a un ejército prusiano en Valmy, lo cual señalaba el inicio de las llamadas guerras revolucionarias francesas.

Sin embargo, la situación económica seguía empeorando, lo cual dio origen a revueltas de las clases más pobres. Los llamados "sans-culottes" expresaban su descontento por el hecho de que la Revolución francesa no solo no estaba satisfaciendo los intereses de las clases bajas, sino que incluso algunas medidas liberales causaban un enorme perjuicio a estas (libertad de precios, libertad de contratación, Ley Le Chapelier, etc.). Al mismo tiempo se comenzaron a gestar luchas antirrevolucionarias en diversas regiones de Francia. En la Vandea, un levantamiento popular fue especialmente significativo: campesinos y aldeanos se alzaron por el rey y las tradiciones católicas, provocando la llamada guerra de Vandea, reprimida tan cruentamente por las autoridades revolucionarias parisinas que se ha llegado a calificar de genocidio. Por otra parte, la guerra exterior amenazaba con destruir la Revolución y la república. Todo ello motivó la trama de un golpe de Estado por parte de los jacobinos, quienes buscaron el favor popular en contra de los girondinos. La alianza de los jacobinos con los "sans-culottes" se convirtió de hecho en el centro del gobierno.

Los jacobinos llevarían en su política algunas de las reivindicaciones de los "sans-culottes" y las clases bajas, pero no todas sus reivindicaciones serían aceptadas, y jamás se cuestionó la propiedad privada. Los jacobinos no pusieron nunca en duda el orden liberal, pero sí llevaron a cabo una democratización del mismo, pese a la represión que desataron contra los opositores políticos (tanto conservadores como radicales).

Se redactó en 1793 una nueva Declaración de los Derechos del Hombre y del Ciudadano, y una nueva constitución de tipo democrático que reconocía el sufragio universal. El Comité de Salvación Pública cayó bajo el mando de Maximilien Robespierre y los jacobinos desataron lo que se denominó el Reinado del Terror (1793-1794). No menos de 10 000 personas fueron guillotinadas ante acusaciones de actividades contrarrevolucionarias. La menor sospecha de dichas actividades podía hacer recaer sobre una persona acusaciones que eventualmente la llevarían a la guillotina. El cálculo total de víctimas varía, pero se cree que pudieron ser hasta 40 000 los que fueron víctimas del Terror.

En 1794, Robespierre procedió a ejecutar a ultrarradicales y a jacobinos moderados.

La Convención aprobó una nueva constitución el 17 de agosto de 1795, ratificada el 26 de septiembre en un plebiscito. La nueva Constitución, llamada Constitución del Año III, confería el poder ejecutivo a un Directorio, formado por cinco miembros llamados directores. El poder legislativo sería ejercido por una asamblea bicameral, compuesta por el Consejo de Ancianos (250 miembros) y el Consejo de los Quinientos. Esta Constitución suprimió el sufragio universal masculino y restableció el sufragio censitario.

La nueva Constitución encontró la oposición de grupos monárquicos y jacobinos. Hubo diferentes revueltas que fueron reprimidas por el ejército, todo lo cual motivó que el general Napoleón Bonaparte, retornado de su campaña en Egipto, diera el 9 de noviembre de 1799 un golpe de Estado (18 de Brumario), instalando el Consulado.

La Constitución del Año VIII, redactada por Pierre Daunou y promulgada el 25 de diciembre de 1799, estableció un régimen autoritario que concentraba el poder en manos de Napoleón Bonaparte, para supuestamente salvar la república de una posible restauración monárquica. Contrariamente a las constituciones anteriores, no incluía ninguna declaración sobre los derechos fundamentales de los ciudadanos. El poder ejecutivo recaía en tres cónsules: el primer cónsul, designado por la misma Constitución, era Napoleón Bonaparte, y los otros dos solo tenían un poder consultivo. En 1802, Napoleón impuso la aprobación de un senadoconsulto, que lo convirtió en cónsul vitalicio, con derecho a designar su sucesor.

El cargo de cónsules lo ostentaron Napoleón Bonaparte, Sieyès y Ducos temporalmente hasta el 12 de diciembre de 1799. Posteriormente, Sieyés y Ducos fueron reemplazados por Jean Jacques Régis de Cambacérès y Charles-François Lebrun, quienes siguieron en el cargo hasta el 18 de mayo de 1804 (28 de floreal del año XII), cuando un nuevo senadoconsulto proclamó el Primer Imperio y la extinción de la Primera República, cerrando con esto el capítulo histórico de la Revolución francesa.

Los tres colores azul, blanco y rojo eran ya frecuentes en diversos pabellones, uniformes y banderas de Francia antes del siglo XVIII. El azul y el rojo eran los colores de la villa de París desde el siglo XIV, y el blanco era en aquella época el color del reino de Francia, y por extensión de la monarquía borbónica.

Cuando Luis XVI visitó a la recién creada Guardia Nacional en el Ayuntamiento de París el 17 de julio de 1790, aparece por primera vez la escarapela tricolor, ofrecida al Rey por el comandante de la Guardia, el marqués de La Fayette. Unía la escarapela de la Guardia Nacional que llevaba los colores de la capital, con el color blanco del reino. No fue sin embargo hasta el 20 de marzo de 1790 que la Asamblea Nacional mencionó en un decreto los tres colores como "colores de la nación: azul, rojo y blanco". Pero la escarapela no era aún un símbolo nacional, y el primer emblema nacional como tal fue la bandera diseñada para la popa de los buques de guerra, adoptada por decreto de la Asamblea Nacional el 24 de octubre de 1790. Constaba de una pequeña bandera roja, blanca y azul en la esquina superior izquierda de una bandera blanca. Esta bandera fue modificada posteriormente por la Convención republicana el 15 de febrero de 1794, a petición de los marineros de la marina nacional que exigieron que se redujera la predominancia del blanco que simbolizaba todavía la monarquía. La bandera adoptó entonces su diseño definitivo, y se cambió el orden de los colores para colocar el azul cerca del mástil y el rojo al viento por motivos cromáticos, según los consejos del pintor Louis David.

Otro símbolo de la Revolución francesa es el gorro frigio (también llamado gorro de la libertad), llevado en particular por los Sans-culottes. Aparece también en los Escudos Nacionales de Francia, Haití, Cuba, El Salvador, Nicaragua, Colombia, Bolivia, Paraguay y Argentina.

El himno «La Marsellesa», letra y música de Claude-Joseph Rouget de Lisle, capitán de ingenieros de la guarnición de Estrasburgo, se popularizó a tal punto que el 14 de julio de 1795 fue declarado himno nacional de Francia; originalmente se llamaba «Chant de guerre pour l'armée du Rhin» («Canto de guerra para el ejército del Rin»), pero los voluntarios del general François Mireur que salieron de Marsella entraron a París el 30 de julio de 1792 cantando dicho himno como canción de marcha. Los parisinos los acogieron con gran entusiasmo y bautizaron el cántico como «La Marsellesa».

El lema "Liberté, égalité, fraternité" («Libertad, igualdad, fraternidad»), que procede del lema no oficial de la Revolución de 1789 "Liberté, égalité ou la mort" («Libertad, igualdad o la muerte»), fue adoptado oficialmente después de la Revolución de 1848 por la Segunda República Francesa.

Uno de los acontecimientos con mayor alcance histórico de la revolución fue la declaración de los derechos del hombre y del ciudadano. En su doble vertiente, moral (derechos naturales inalienables) y política (condiciones necesarias para el ejercicio de los derechos naturales e individuales), condiciona la aparición de un nuevo modelo de Estado, el de los ciudadanos, el Estado de Derecho, democrático y nacional. Aunque la primera vez que se proclamaron solemnemente los derechos del hombre fue en los Estados Unidos (Declaración de Derechos de Virginia en 1776 y Constitución de los Estados Unidos en 1787), la revolución de los derechos humanos es un fenómeno puramente europeo. Será la Declaración de Derechos del Hombre y del Ciudadano francesa de 1789 la que sirva de base e inspiración a todas las declaraciones tanto del siglo XIX como del siglo XX.

El distinto alcance de ambas declaraciones es debido tanto a cuestiones de forma como de fondo. La declaración francesa es indiferente a las circunstancias en que nace y añade a los derechos naturales, los derechos del ciudadano. Pero sobre todo, es un texto atemporal, único, separado del texto constitucional y, por tanto, con un carácter universal, a lo que hay que añadir la brevedad, claridad y sencillez del lenguaje. De ahí su trascendencia y éxito tanto en Francia como en Europa y el mundo occidental en su conjunto.

La declaración sin embargo excluyó a las mujeres en su consideración de ciudadanas y se olvidó de las mujeres en su proyecto igualitario. Dos años más tarde de la redacción de la Declaración de Derechos del Hombre y del Ciudadano la activista política Olympe de Gouges escribió la Declaración de los Derechos de la Mujer y la Ciudadana (1793) que se convierte en uno de los primeros documentos históricos que plantea la equiparación jurídica y legal de las mujeres en relación a los varones.

Las mujeres ocupan la calle durante las semanas precedentes a la insurrección y tuvieron un papel protagonista en el inicio de la Revolución. El 5 de octubre de 1789 fueron ellas quienes iniciaron la marcha hacia Versalles a buscar al rey. Sin embargo cuando las asociaciones revolucionarias dirigen el alzamiento las mujeres quedan excluidas del pueblo deliberante, del pueblo armado -la guardia nacional- de los comités locales y de las asociaciones políticas.

Al no poder participar en las asambleas políticas toman la palabra en las tribunas abiertas al público y crean los clubes femeninos en los que leen y debaten las leyes y los periódicos. Entre los más reconocidos estaba la Sociedad Patriótica y de Beneficencia de las Amigas de la Verdad (1791-1792) fundada por Etta Palm en el que se reclamaba educación para las niñas pobres, divorcio y derechos políticos.

Entre las revolucionarias más destacadas esta la dramaturga y activista política considerada precursora del feminismo, Olympe de Gouges que escribió la Declaración de los Derechos de la Mujer y la Ciudadana (1793) reivindicando la equiparación de derechos entre hombres y mujeres. Olympe se enfrentó a Robespierre y publicó la carta "Pronostic de Monsieur Robespierre pour un animale amphibie" que la llevó a ser acusada de intrigas sediciosas. Fue juzgada, condenada a muerte y guillotinada.

El 30 de septiembre de 1793 se prohibieron los clubes femeninos. En 1794 se insistió en la prohibición de la presencia femenina en cualquier actividad política y en mayo de 1795, la Convención prohibió a las mujeres asistir a las asambleas política ordenando que se retiraran a sus domicilios bajo orden de arresto si no cumplían lo prescrito. Finalmente el Código Napoleónico aprobado en 1804 consagró la derrota femenina en la lucha por la igualdad, libertad y fraternidad que la revolución significó para los varones.


"Este artículo incorpora material de las siguientes fuentes bajo dominio público:




</doc>
<doc id="22262" url="https://es.wikipedia.org/wiki?curid=22262" title="Microscopía">
Microscopía

La microscopía (o también sin tilde: «microscopia») es el conjunto de técnicas y métodos destinados a hacer visible los objetos de estudio que por su pequeñez están fuera del rango de resolución del ojo normal.

Si bien el microscopio es el elemento central de la microscopía, el uso del mismo se requiere para producir las imágenes adecuadas, de todo un conjunto de métodos y técnicas afines pero extrínsecas al aparato. Algunas de ellas son, técnicas de preparación y manejo de los objetos de estudio, técnicas de salida, procesamiento, interpretación y registro de imágenes, etc.

Exceptuando técnicas especiales como las utilizadas en microscopio de fuerza atómica, microscopio de iones en campo y microscopio de efecto túnel, la microscopía generalmente implica la difracción, reflexión o refracción de algún tipo de radiación incidente en el sujeto de estudio.

Antonie van Leeuwenhoek (Holanda, 1632-1723), un vendedor de telas, aficionado a pulir lentes, logró fabricar lentes lo suficientemente poderosas como para observar bacterias, hongos y protozoos, a los que llamó "animálculos".

El primer microscopio compuesto fue desarrollado por Zacharias Janssen. A partir de este, los avances tecnológicos permitieron llegar a los modernos microscopios de nuestro tiempo, los que existen de varios tipos y son usados con diferentes fines. Cincuenta años después de la creación del microscopio, el inglés Robert Hooke perfecciona el microscopio; Hooke utiliza un microscopio compuesto para estudiar cortes de corcho y describe los pequeños poros en forma de caja, a los que él llamó "células". Publica su libro Micrographia.

Microscopía óptica ("microscopía de luz clásica"), consiste en hacer pasar luz visible de una fuente ("difractada, reflejada o refractada en el sujeto de estudio") a través de lentes ópticos simples o múltiples, para lograr una vista ampliada de la muestra. La imagen resultante puede ser detectada directamente por el ojo humano, impresa en una placa fotográfica o registrada y mostrada digitalmente ("y eventualmente almacenada en algún soporte digital"). En la fig. mo1 puede verse un microscopio estereoscópico ("adecuado principalmente para una visión binocular directa").



</doc>
<doc id="22263" url="https://es.wikipedia.org/wiki?curid=22263" title="Apertura angular">
Apertura angular

La apertura angular de una lente es el ángulo aparente de la apertura de la lente visto desde el punto focal:
formula_1
donde formula_2 es la distancia focal y formula_3 el diámetro de la apertura.



</doc>
<doc id="22267" url="https://es.wikipedia.org/wiki?curid=22267" title="Ernst Ruska">
Ernst Ruska

Ernst August Friedrich Ruska (Heidelberg, 25 de diciembre de 1906- Berlín, 25 de mayo de 1988) fue un físico alemán que ganó el Premio Nobel de Física en 1986 por su trabajo en óptica electrónica, incluyendo el diseño del primer microscopio electrónico.

Ruska nació en Heidelberg, fue educado en la Universidad Técnica de Múnich de 1925 a 1927, luego ingresó a la Universidad Técnica de Berlín, donde postuló que los microscopios que usan electrones con longitudes de onda 1000 veces más corta que la de la luz visible, pueden proveer imágenes más detalladas de los objetos que los microscopios que utilizan luz, en los cuales la magnificación es limitada por el tamaño de las longitudes de onda. En 1931 demostró que una bobina magnética podría actuar como una lente electrónica, y usó varias bobinas en una serie para construir el primer microscopio electrónico en 1933.

Después de completar su doctorado en 1933, Ruska continuó trabajando en el campo de ópticas electrónicas, primero en Fernseh Ltd in Berlin-Zehlendorf, y luego desde 1937 en Siemens AG. En Siemens, se involucró en desarrollar el primer microscopio electrónico producido comercialmente en 1939. Al igual que desarrollando la tecnología del microscopio electrónico mientras en Siemens, Ruska también trabajó en otras instituciones científicas y animó a Siemens a establecer un laboratorio para investigadores visitantes que fue encabezado inicialmente por el hermano de Helmut Ruska, un médico que desarrolló el uso del microscopio electrónico para aplicaciones médicas y biológicas. Después de salir de Siemens en 1955, Ruska se desempeñó como director del Instituto de Microscopía Electrónica en el Instituto Fritz Haber de la Sociedad Max Planck hasta el año de 1974. Al mismo tiempo, trabajó en el instituto y como profesor en la Universidad Técnica de Berlín, desde 1957 hasta su retiro en 1974.

En 1986, fue galardonado con la mitad del Premio Nobel en Física por sus muchos logros en ópicas electrónicas; Gerd Binnig y Heinrich Rohrer ganaron un cuarto del premio para cada uno por su diseño del microscopio de efecto túnel. Ruska murió en Berlín Occidental en 1988.



</doc>
<doc id="22268" url="https://es.wikipedia.org/wiki?curid=22268" title="Roncesvalles">
Roncesvalles

Roncesvalles (en euskera Orreaga y oficialmente Orreaga/Roncesvalles) es una villa y municipio español de la Comunidad Foral de Navarra, situado en la merindad de Sangüesa, en la comarca de Auñamendi y a 47,7  km de la capital de la comunidad, Pamplona. Su población en 2019 era de 22 habitantes: 12 hombres y 10 mujeres (INE).

El puerto de Roncesvalles correspondía antiguamente al collado axial de Ibañeta (1066 m), vía de paso natural que se utilizó desde la prehistoria para acceder a la península ibérica. El punto de mayor altitud del municipio es la cima del monte Orzanzurieta, con 1567 msnm.

Las casas e instituciones religiosas y de atención a los peregrinos jacobeos se hallan en el pueblo de Roncesvalles, situado al pie de Ibañeta, donde arranca la famosa llanada en la que los cantares de gesta ubican la batalla contra los carolingios. Roncesvalles, al cabo del tiempo, sigue siendo enclave fundamental para los peregrinos del Camino de Santiago. Por Ibañeta y Roncesvalles entra el llamado Camino Francés, el mismo que recorrió Aymeric Picaud en el siglo XII, el cual se funde en la villa de Obanos, muy cerca de Puente la Reina, con el otro que procede del Somport de Huesca, también en los Pirineos, conocido por Camino Aragonés.

El municipio de Roncesvalles limita por el norte, en el collado de Ibañeta, con el de Valcarlos, municipio navarro transpirenaico, y por el sur con el de Burguete, la que fue primera población de la comarca, llamada entonces Burgo de Roncesvalles. Al este limita con el municipio de Orbaiceta.

Roncesvalles fue de siempre vía de paso para entrar en la península ibérica.
Se identifica con la mansio Summo Pyreneo (Ibañeta), en el Itinerario Antonino A-34 Ab Asturica Burdigalam, donde se recogen los itinerarios que los funcionarios romanos confeccionaban a modo de guías de viajeros, . 
Por Roncesvalles penetraron fundamentalmente los celtas, los vándalos (409), los godos que se establecieron a lo largo de la cuenca del Duero y Carlomagno durante el siglo VIII. Carlomagno, dado que fue derrotado en Zaragoza, decidió, camino de vuelta a su reino, reducir a ruinas la capital de los vascones, Pamplona. Fue al regreso, en los Pirineos, entre el collado de Ibañeta y la hondonada de Valcarlos, donde hubo de sufrir una contundente emboscada por partidas de nativos vascones, a los que les resultó fácil provocar un descalabro general a base de lanzar rocas y dardos. El "Cantar de Roldán", escrita en algún lugar de Francia hacia finales del siglo XI, concibió el desastre en el llano, entre Roncesvalles y la villa de Burguete, y los atacantes ya no eran vascones, sino sarracenos, quienes en realidad nunca llegaron a expandir sus dominios tan al norte.
Y la misma calzada romana sirvió como camino de entrada para los primeros peregrinos.

Roncesvalles está comunicada por carretera por medio de autobuses con origen y destino en Pamplona y otras localidades de Navarra. Además, hay servicio de taxis de varias localidades de alrededor.

La economía de Roncesvalles se basa en las rentas que se obtienen de las tierras del municipio (explotación forestal principalmente), de alguna explotación agrícola-ganadera existente en el municipio y sobre todo de las actividades hosteleras.

La economía local está hoy en día orientada a la atención de los peregrinos y turistas, debido a la condición de Roncesvalles como tradicional primera etapa en España del Camino de Santiago.

A pesar de su escasa población, el pueblo cuenta con una notable oferta de hospedaje: un hotel, un edificio de apartamentos turísticos, dos hostales y un refugio de peregrinos que depende de la Colegiata. Roncesvalles posee una oficina de turismo, un museo y una oficina del peregrino. El hotel y los hostales poseen además bar-restaurante.

En cualquier caso, el mantenimiento del complejo histórico-artístico excede los recursos municipales y exige la aportación económica de instancias oficiales.

Estos son los últimos alcaldes de Roncesvalles:

El Hospital fundado por el obispo de Pamplona Sancho Larrosa, con la colaboración del rey de Aragón y Pamplona Alfonso Sánchez (más conocido como Alfonso I el Batallador) y siguiendo así la tradición de su padre el rey Sancho Ramírez, I de Aragón y V de Pamplona y la de su hermanastro Pedro I de Aragón y de Pamplona de potenciar y proteger el camino del santo junto con algunos nobles. Los Papas lo tomaron desde un principio bajo su protección. Desde su fundación lo ha regido un Cabildo de canónigos regulares de San Agustín. En 1984 pasó a depender del Arzobispado de Pamplona. El Prior sigue ostentando el título medieval de Gran Abad de Colonia. El cargo de «hospitalero» lo lleva un canónigo. En el siglo XVII se repartían 25 000 raciones anuales entre los peregrinos.

El hospital que existe actualmente fue diseñado en 1792 por el arquitecto José Poudez, levantándose entre 1802 y 1807 con los criterios de la arquitectura neoclásica. Consiste en un gran bloque horizontal con tres plantas hacia el patio y cuatro hacia el este, apenas marcado por ventanales cuadrangulares, y al que se accede por un portal con arco de medio punto enmarcado por pilastras, friso y frontón triangular.

También conocida como Silo de Carlomagno por suponerse que su origen se debe al enterramiento de combatientes francos caídos en el 778, lo que no es inverosímil. Se remonta al siglo XII, por lo que está considerada la edificación más antigua de Roncesvalles.

El Sancti Spiritus hay que considerarlo templo funerario, pero no fue lugar de enterramiento perpetuo en el medievo. Era el recinto en que se oficiaban misas por los peregrinos fallecidos en el hospital que, enterrados en otro lugar, una vez transcurrido un tiempo sus restos eran depositados en el osario bajo la capilla exenta.

Pequeña iglesia gótica del siglo XIII, situada junto al Silo de Carlomagno. Es una sencilla fábrica de planta rectangular con dos tramos que incluyen la cabecera recta y bóveda de crucería simple. Unas columnas de fuste cilíndrico sirven de soporte para la cubierta. En su interior hay una figura del Apóstol Santiago. El exterior tiene muros de sillar irregular, sin contrafuertes, con una portada de arco apuntado y Crismón.

Fue utilizada como parroquia hasta el siglo XVIII. Quedó sin culto durante un largo periodo hasta que fue restaurada por Florencio Ansoleaga en el siglo XX, quien abrió un pequeño óculo sobre la puerta e incorporó la mítica y legendaria campana que orientaba a los peregrinos en la capilla que había en el collado de Ibañeta: la capilla de San Salvador.

La iglesia colegiata de Santa María es la fábrica más lujosa de Roncesvalles y el mejor ejemplo navarro del gótico, no sólo francés, sino del más puro de l'Île de France. Acoge una preciosa imagen de la Virgen del siglo XIV.
El templo actual se construyó gracias a Sancho el Fuerte (1194-1234), quien lo eligió como lugar de enterramiento. No hay datos concretos sobre las fechas de la construcción de la iglesia, pero se sabe que fue a principios del siglo XIII, entre 1215 y 1221.

La Colegiata sufrió importantes desperfectos, ocasionados principalmente por varios incendios ocurridos en 1445, 1468 y 1626. A comienzos del siglo XVII, su estado de deterioro y casi abandono propició su reconstrucción, que abarcó todo el recinto colegial, especialmente a la iglesia y al claustro. Se enmascaró el interior gótico y se le dio forma barroca salvo en el presbiterio y el tramo de nave que le precede, donde quedaron a la vista los elementos góticos.

De planta cuadrada cubierta con bóveda de terceletes con ligaduras de nervios más elaborados que los de la iglesia y las claves decoradas. La bóveda se apoya en cuatro ménsulas de gran tamaño que representan unos ángeles.

El exterior se presenta como un bloque cúbico de sillería, con cierto aspecto de fortaleza; de ahí que en ocasiones se le denomine torre de San Agustín. Unos contrafuertes adosados a las esquinas que llegan hasta la cubierta piramidal refuerzan el conjunto, que data del siglo XIV. En el centro de la capilla se sitúa el sepulcro de Sancho VII el Fuerte.

También hay que reseñar una serie de esculturas relacionadas con las obras del claustro de la Catedral de Pamplona. Se trata de dos capiteles que representan el Pecado original y la Expulsión del Paraíso, que cabe pensar formaron parte del claustro gótico.

A unos cien metros de la capilla del Sancti Spiritus, al borde de una pequeña hondonada de verdes pastizales, se halla una edificación envuelta en misterio. Domenico Laffi había escrito que la capilla funeraria estaba muy cerca del hospital de peregrinos. La situaba a occidente, lo que parece coincidir con el emplazamiento de Itzandegia. «Es un gran y bello hospital en el que los peregrinos pueden permanecer tres días. Pueden comer y dormir, y los tratan muy bien».

El edificio es una casona de piedra de 32 x 12 metros. Consta de nave única de seis tramos, cuya techumbre sostienen cinco arcos apuntados que descansan sobre los muros, que a su vez se apoyan en diez contrafuertes, cinco por cada lado. Tiene dos accesos, uno mayor, ancho como para el paso de carros, vuelto de espaldas a Roncesvalles, alzado casi un metro sobre el suelo, desnivel que no existiría hasta tanto no fue edificada la casa casi adosada. La otra puerta, menor, en el lateral derecho, permite el paso a la única planta, que en otro tiempo debió de contar con otra superior.

Es edificio ciego, salvo la escasa luz que dejan pasar seis aspilleras, verticales y estrechas, en lo alto del muro que da al mediodía. La última restauración terminó con el Xacobeo 1993. La mayor parte de los contrafuertes exteriores había desaparecido al igual que los arcos de la bóveda, «recuperándose sobre el modelo de los tres que aún se conservaban», anotaron los profesores Miranda y Ramírez. Pero no parece que fuese así, porque por algunas fotografías de antaño puede comprobarse cómo la estructura medieval había sido alterada, adaptada a los requerimientos del caserío rural en que se había convertido, con amplios ventanales, puertas para distintos cometidos y dependencias adyacentes. Ni siquiera estaban los gruesos contrafuertes.

Ocupan un edificio yuxtapuesto a la Casa Prioral formando un bloque horizontal. Consta de tres niveles en altura y un reducido ático de óculos. En el segundo cuerpo se abre una arcada sobre pilastras acanaladas.

La Biblioteca

Comprende más de 15 000 volúmenes de todo tipo de materias, aunque destacan las obras sobre cuestiones teológicas, filosóficas y de historia eclesiástica. Hay volúmenes en distintas lenguas: hebreo, griego, latín, vascuence e incluso chino. Algunas de las piezas más interesantes, como el códice "La Pretiosa" (del siglo XIV), se exponen en el Museo de la Colegiata.

Todavía conserva una importante sección de Archivo Histórico conformado a lo largo de los casi nueve siglos de existencia del hospital, que incluye pergaminos, libros de administración, documentos relativos a la historia interna y las repercusiones exteriores de la vida capitular, etc.

El Museo
Situado en la planta baja del edificio, recoge gran cantidad de objetos de arte representativos de la Colegiata, que incluyen escultura, pintura y orfebrería, así como muebles, tapices, monedas y libros de gran interés bibliográfico.

En escultura destacan la estatua de una figura femenina sedente, gótica del siglo XIV, y una talla de San Miguel fechada en el segundo tercio del siglo XVI. Asimismo, algunos relieves y estatuas que formaban parte del retablo mayor de la Colegiata, realizado entre 1618 y 1624.

En pintura lo más destacable es el tríptico de la Crucifixión (escuela noreuropea del siglo XVI), cedido a la Colegiata en 1720 por doña Jerónima Jiménez de Esparza, de origen aparentemente flamenco. Una tabla de la Sagrada Familia, realizada por Luis de Morales, que guarda semejanza con la de la Catedral Nueva de Salamanca. El lienzo del martirio de San Lorenzo, barroco de la primera mitad del siglo XVII, así como otro de Judith portando la cabeza de Holofernes.

En orfebrería hay que resaltar una hermosa arqueta de plata dorada, cubierta de fina labor de filigrana y fechada entre 1274 y 1328. Existe otra arqueta de plata parcialmente dorada, que se ha fechado en el siglo XVI, cuyo interés radica en que aprovecha medallones y relieves de la época medieval.

Figuran también los relicarios y especialmente el denominado «Ajedrez de Carlomagno», llamado así por su disposición en damero. Esta pieza, adscrita al gótico de la segunda mitad del siglo XIV, está formada por un alma de madera, forrada de láminas de plata parcialmente dorada, esmaltes translúcidos y vidrio.

La famosa esmeralda de Miramamolín, que se identifica, según la leyenda, con la que Sancho VII el Fuerte arrebató del turbante al rey moro en la Batalla de las Navas de Tolosa, a raíz de la cual se incorporó, como un símbolo, al escudo de Navarra. El evangelario en el que juraban fidelidad los reyes de Navarra, obra de orfebrería románica.

Los topónimos latinos y romances, empleados desde la Edad Media para referirse al enclave pirenaico, son muchos y variados, si bien algunos hay que considerarlos parcialmente erróneos a causa de malas interpretaciones de copistas y de personajes muy alejados de Navarra, a grafías derivadas de otras equivocadas o a intentos por enmendar lo que se suponía que estaba mal escrito. Sin agotar las respectivas listas, he aquí algunos de los más frecuentes:
Existe una importante devoción por la Virgen de Roncesvalles en todo el Pirineo navarro. El 8 de septiembre se festeja el Día de la Virgen de Roncesvalles, patrona de Roncesvalles.

Durante los domingos y festivos de mayo y junio suelen acudir en romería las parroquias de los valles y pueblos del entorno e incluso de Pamplona.






</doc>
<doc id="22269" url="https://es.wikipedia.org/wiki?curid=22269" title="Termómetro">
Termómetro

El termómetro (del griego θερμός ["thermos"], «calor», y μέτρον ["metron"], «medida») es un instrumento de medición de temperatura. Desde su invención ha evolucionado mucho, principalmente a partir del desarrollo de los termómetros digitales.

Inicialmente se fabricaron aprovechando el fenómeno de la dilatación, por lo que se prefería el uso de materiales con elevado coeficiente de dilatación, de modo que, al aumentar la temperatura, su estiramiento era fácilmente visible. La sustancia que se utilizaba más frecuentemente en este tipo de termómetros ha sido el mercurio, encerrado en un tubo de vidrio que incorporaba una escala graduada, pero también alcoholes coloreados en termómetros grandes.

El creador del primer termoscopio fue Galileo Galilei; este podría considerarse el predecesor del termómetro. Consistía en un tubo de vidrio terminado en una esfera cerrada; el extremo abierto se sumergía boca abajo dentro de una mezcla de alcohol y agua, mientras la esfera quedaba en la parte superior. Al calentar el líquido, este subía por el tubo. 

La incorporación, entre 1611 y 1613, de una escala numérica al instrumento de Galileo se atribuye tanto a Francesco Sagredo como a Santorio Santorio, aunque es aceptada la autoría de este último en la aparición del termómetro.

En España se prohibió la fabricación de termómetros de mercurio en julio de 2007, por su efecto contaminante.

En América Latina, los termómetros de mercurio siguen siendo ampliamente utilizados por la población. No así en hospitales y centros de salud donde por regla general se utilizan termómetros digitales.

La escala más usada en la mayoría de los países del mundo es la Celsius (°C) en honor a Anders Celsius (1701-1744) que se llamó "centígrado" hasta 1948. En esta escala, el cero (0 °C) y los cien (100 °C) grados corresponden respectivamente a los puntos de congelación y de ebullición del agua, ambos a la presión de 1 atmósfera.

Otras escalas termométricas son:


Para medir ciertos parámetros se emplean termómetros modificados, tales como los siguientes:

El termógrafo es un termómetro acoplado a un dispositivo capaz de registrar, gráfica o digitalmente, la temperatura medida en forma continua o a intervalos de tiempo determinado.

La siguiente cronología muestra los avances en las tecnologías de medición de temperatura:




</doc>
<doc id="22283" url="https://es.wikipedia.org/wiki?curid=22283" title="Arnedo">
Arnedo

Arnedo es un municipio y ciudad de la comunidad autónoma de La Rioja (España), perteneciente a la comarca de la Rioja Baja. Tiene 14 875 habitantes (INE 2019). Está bañada por el río Cidacos (afluente del Ebro) y su economía depende fundamentalmente de la industria del calzado. Su nombre proviene posiblemente del latín "«arenetum»", colectivo de "«arena»".

La ciudad de Arnedo está situada en el valle medio del Cidacos, río afluente del Ebro por su derecha, en la comarca de La Rioja Baja. Con 14 815 habitantes (INE 2018) es la tercera ciudad en población de La Rioja, de cuya capital, Logroño, dista 48 km.
El término municipal de Arnedo tiene 86,8 km² que se extienden a ambas márgenes del río, aunque su núcleo urbano se halla en la izquierda del mismo. Incluye desde 1975 el anterior municipio de Turruncún, hoy deshabitado. Su altitud media sobre el nivel del mar es de 523 m, que marcan el comienzo de la sierra hacia el sur.

Arnedo se halla en la denominada Hoya de Arnedo, en la que penetra el río Cidacos por Arnedillo y sale por Autol, determinando dos márgenes desiguales: la derecha, mucho más extensa y escarpada, está dominada por la peña Isasa, de 1474 m de altura; y la izquierda, con paredes arcillosas de un rojizo característico, en la que a pesar de su poco espacio se sitúan la mayoría de las poblaciones y las cuevas artificiales horadadas por el hombre.

La Hoya está enmarcada y cerrada por un conjunto de sierras de conglomerados y areniscas que pueden ser el origen del nombre de la ciudad "«arenetum»":


El clima de Arnedo es de transición entre mediterráneo y atlántico. Se caracteriza por ser templado-frío, lleno de contrastes y variaciones de un año a otro. Así, se pueden registrar temperaturas superiores a los 35 grados en julio y agosto e inferiores a los 0 grados en enero.

El cierzo (viento norte) y el bochorno (viento sur) son los vientos propios de la zona. En lo que se refiere a la velocidad, aunque predominan las jornadas de vientos débiles y en calma, hay días en que pueden alcanzar rachas importantes.

Al 1 de enero de 2019 la población del municipio ascendía a 14 815 habitantes, 7373 hombres y 7442 mujeres. A continuación se expone gráficamente cómo ha evolucionado el número de habitantes de la ciudad a lo largo de la época estadística: 

Parece indudable que Arnedo ha estado poblado desde tiempos muy antiguos, pues sus restos arqueológicos se remontan al neolítico.

Hay historiadores que afirman que el nombre de Arnedo en tiempos prerromanos era Sadacia o Sidacia, nombre que habría quedado en el río Cidacos, pero su actual denominación deriva etimológicamente del término latino "«arenetum»", que viene a significar "«lugar de arena»", y hace referencia a la plataforma arenosa sobre la que se asienta la ciudad.

Durante la prehistoria se ubican a la orilla del río y en las montaña los más antiguos asentamientos que se conocen. Son los de San Pedro Mártir y el Valpineda, con restos de época neolítica, quizá del Bronce inicial: allí se encontraron materiales líticos de sílex y cerámicas hechas a mano.

De la época prerromana (del 3000 a. C. al siglo II a. C.) los testimonios arqueológicos en los cerros de San Miguel y El Raposal parecen indicar un hábitat concentrado por pobladores celtíberos, quizás berones o pelendones, que desaparecerían a la llegada de los romanos en el siglo II a. C. Del pueblo celtíbero quedan importantes testimonios: un poblado, un horno y restos de cerámicas aparecidos en la colina de San Miguel.

En la época romana (del siglo II a. C. al siglo V) no se cita Arnedo en su historiografía y hay que recurrir a la arqueología. Se supone que el poblado se traslada al pie del cerro del Castillo, donde aparecieron restos de terra sigillata; allí los romanos podrían haber levantado una fortificación (base del futuro castillo medieval) que protegía un importante nudo de comunicaciones. En Arnedo se cruzaban las calzadas romanas que unían Calahorra con Numancia, y la de Contrebia Leucade (yacimiento de Inestrillas, en la cuenca del río Alhama) con Varea, la Vareia romana.

De la época visigótica se conservan restos de una iglesia rupestre del siglo VI en las afueras del pueblo, junto al Monasterio de Nuestra Señora de Vico, y la Cueva de los Cien Pilares, bajo el cerro de San Miguel, donde hubo un monasterio.

Arnedo pasó a ser ocupada por los árabes, muestra de ello es el castillo, que se eleva en un imponente cerro controlando y dominando la ciudad y sus límites. Según el geógrafo árabe Idrisí, Arnedo fue capital de una de las 26 provincias árabes de España en el siglo VIII y desde donde se desempeñó un importante papel en las luchas de conquista y Reconquista.

La falta de documentación cristiana hace de las crónicas musulmanas la mejor fuente de información para este periodo, en el que la población fue reconquistada por Sancho Garcés I entre los años 908-909. Se le concedió fuero propio y surgió la leyenda de la aparición de la Virgen de Vico al moro Kam. En 1264 quedó vinculado a la Corona de Castilla. Tras la guerra de los Dos Pedros, la villa fue cedida por el victorioso Enrique II de Trastámara al noble francés Bertrand du Guesclin. Este caballero, sin embargo, la vendió el 24 de abril de 1370 a Pedro Fernández de Velasco, recibiendo a cambio dos mil doblas castellanas de oro. El 17 de abril de 1379 Juan I confirmó la transacción.

A finales del siglo XIV Arnedo adquiere renombre al celebrarse en 1385 conferencias diplomáticas que son el origen del título de Príncipe de Asturias, y firmarse en 1388 el llamado Tratado de Arnedo entre Francia y Castilla para defensa mutua. En el siglo XV se fundan en Arnedo una de las primeras cajas de ahorro que se conocen en el mundo, así como el monasterio de Vico.

En los siguientes siglos varios reyes distinguieron a la población: Felipe III le concede el derecho de fielazgo y garapitería, Felipe IV la exime de alojar gente de guerra y concede el Título de ciudad, y Carlos III concede una feria de... "nueve días y mercado el día lunes de cada semana".

En 1654 recibió el título de Ciudad concedido por el entonces rey Felipe IV.

En algún punto entre los años 1784 y 1801 Arnedo se integra en la Real Sociedad Económica de La Rioja, la cual era una de las sociedades de amigos del país fundadas en el siglo XVIII conforme a los ideales de la ilustración.

En el siglo XIX la ciudad contaba ya con una industria alpargatera y otras actividades fabriles: fábricas de jabón y aguardientes, tenerías, alfarerías, una imprenta, etc.

Durante el pasado siglo XX en los aspectos sociales y políticos (véase Sucesos de Arnedo) la ciudad ha seguido los acontecimientos del resto de España; pero se ha caracterizado particularmente por el inicio en los años veinte de la industria del calzado con un gran desarrollo posterior y de otras industrias auxiliares: prefabricados, caucho, cartonajes, etc.; causa de la llegada de población desde otros lugares de España y en los últimos años desde otros países.

Se conoce de esta manera a los acontecimientos que se produjeron entre 1931 y 1932 a partir de unos despidos en una fábrica de calzados y terminaría con la muerte de once personas por disparos de la Guardia Civil. Estos hechos causaron una gran polémica, añadidos a otros casos de matanzas de Guardias Civiles por parte de campesinos y obreros, como en Castilblanco (anterior), o la situación inversa en Casas Viejas (posterior). Arnedo se convirtió en un símbolo para anarquistas y republicanos de izquierda radical durante los años treinta, del mismo modo que el atentado de ETA en 1995 lo convertiría en símbolo contra el terrorismo.

La ciudad de Arnedo basa su economía principalmente en la industria del calzado, siendo utilizado este pretexto como eslogan de la ciudad (Arnedo, ciudad del calzado) y como atracción turística (Museo del calzado).

Tiene un importante y amplio tejido industrial relacionado con el calzado, del que existen diversas variedades. Entre ellas se encuentran una gran cantidad de marcas populares propias de Arnedo: firmas como Fluchos, Pitillos, Flossy, Callaghan , Chiruca (Fal), Gorila, etc. También esta ciudad es líder nacional en la fabricación de Calzado de Seguridad; todo ello representado en la Asociación de Industrias del Calzado y Conexas de La Rioja, AICCOR. Arnedo cuenta desde el 2007 con el Centro Tecnológico del Calzado de La Rioja, que mejora la competitividad y aplica la I+D+i al sector.

Esta economía se complementa con otras como son las bodegas (Bodega Cooperativa Nuestra Señora de Vico), cartonaje, transformación de caucho, trujal (Trujal 5 valles), comercios de servicio... La agricultura se dirige al autoconsumo, por lo que destaca el regadío (sobre todo cerca del río), la vid, los olivos y los almendros.

Asimismo, en lo que a energía se refiere, en el municipio de Arnedo se encuentra uno de los mayores parques solares de España, construido por Isolux-Corsán y propiedad de T-Solar.

El concepto de deuda viva contempla sólo las deudas con cajas y bancos relativas a créditos financieros, valores de renta fija y préstamos o créditos transferidos a terceros, excluyéndose, por tanto, la deuda comercial.



El cerro del Castillo que domina el este de la población, se debió fortificar desde época romana al pasar por allí diversas calzadas. Al llegar los árabes en el 714 reconstruyen el castillo sobre esa hipotética fortaleza romana anterior, y tuvo un importante papel durante la Edad Media en las luchas de conquista y reconquista. Los Fernández de Velasco, Señores de Arnedo, acometieron algunas de las obras que han pervivido hasta nuestros días, para después ser abandonado, mientras que en el s. XIX con motivo de las Guerras Carlistas volvió a remozarse. Durante los años 2016 al 2018 se ha realizado una obra de consolidación y restauración que ha permitido volver a abrirlo al público. 

La ciudad posee tres iglesias situadas en el Casco Antiguo. Cada una de ellas destaca por distintos valores artísticos:


La ciudad conserva un gran número de casas-palacio en su mayoría barrocas que se sitúan en el Casco Antiguo que merecen la pena conocer. Entre ellas destacan el Palacio del Arzobispo Argaiz, actual Casa de Cultura (biblioteca municipal, salas de exposiciones...) y el Palacio de la Baronesa, conocido como La Casa de Arte en la que se desarrollan actividades musicales y artísticas.


La gastronomía de la zona recoge la unión entre las gastronomías de las diferentes culturas que han estado conviviendo en el territorio. Cabe destacar la importancia de los árabes los cuales dejaron importantes postres como los fardelejos.

También destacan en la gastronomía de la población los frutos de sus huertas. Las familias de Arnedo suelen poseer pequeñas parcelas con hortalizas y las cultivan como afición y para su autoabastecimiento. En las fiestas del municipio los manjares se disfrutan en la calle y es usual ver a gente realizando unas chuletas asadas al sarmiento, o unos ricos ranchos.

En las fiestas de marzo, fácilmente se puede hallar en las brasas de las chuletadas un hueco para asar ajos, cebollas y huevos. Actualmente se celebra cada año en Arnedo el Festival del ajo asado, generalmente en marzo y abril, el mes depende de cuando caiga semana santa.

En julio también se celebra "A que te leo". Una especie de festival de lecturas que tiene lugar por las noches en el parque que hay junto al río Cidacos.

Las fiestas patronales de San Cosme y San Damián comienzan el día 26 de septiembre y terminan el día 2 de octubre. El día 27, Arnedo recibe a los navarros que vienen a animar el día cantando por la mañana de madrugada, y asistiendo después a misa.

La razón de estas fiestas es que (según la leyenda) los navarros creen ser dueños de los Santos San Cosme y San Damián alegando que los arnedanos se los robaron. Todos los años un grupo de valientes navarros (provenientes de pueblos cercanos como Andosilla, San Adrián) intentan recuperar «sus» santos. Este acto se realiza en un recorrido con diferentes estaciones en las que un orador navarro utiliza su labia para dar razones de que los santos están mejor en Navarra. Con el grito «¡A Navarra con ellos!», los navarros intentan zafarse con los santos pero un grupo de arnedanos los detienen y no los dejan escapar. Con el ánimo de volver a intentarlo al próximo año los navarros culminan la jornada.

Las fiestas de Arnedo se organizan por medio de las peñas y asociaciones como Lubumbas, La Chispa, Tao y Quincalla. Por las mañanas se ofrecen desayunos gratis y pasacalles con las charangas antes del comienzo del encierro. También hay concurso de subir por el mayo o cucaña a por el jamón, marionetas para los niños, deportes tradicionales... y el Zapato de Oro.

El Zapato de Oro es una gran novillada codiciada por los novilleros. Se suele decir que para ser un gran novillero y tomar la alternativa, tener el Zapato de Oro es un buen comienzo. Se celebraba en la centenaria antigua plaza de toros de Arnedo. Este trofeo lo poseen celebridades como Enrique Ponce, Jesulín de Ubrique, Diego Urdiales, etc.


Denominado tradicionalmente como feudo socialista, por las sucesivas y abultadas victorias de José María León Quiñones desde 1987, en las elecciones del año 2003 se produjo un empate a concejales entre el PSOE y el PP, lo que permitió al Partido Riojano pactar con el PP y desbancar al PSOE del Ayuntamiento.

En 2007, el PP conseguiría hacerse con la mayoría absoluta (9 ediles), que no pudo revalidar en 2011, aunque si accedió al gobierno con sus 8 concejales de la mano nuevamente del Partido Riojano (PR) con dos.

En 2015, el Partido Socialista recuperaría de nuevo la alcaldía con Javier García Ibáñez, que con 32 añosse convirtió en el Alcalde más joven de la democracia en Arnedo. Cuatro años más tarde, en 2019, García revalidó su victoria con creces con una mayoría absoluta de 10 concejales y más del 50% de los votos, conduciendo al PSOE a un escenario que no vivía desde hacía 20 años, con León Quiñones.





</doc>
