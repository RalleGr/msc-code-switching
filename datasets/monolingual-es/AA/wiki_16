<doc id="2721" url="https://es.wikipedia.org/wiki?curid=2721" title="J. R. R. Tolkien">
J. R. R. Tolkien

John Ronald Reuel Tolkien, CBE (AFI: ) (Bloemfontein, hoy Sudáfrica; 3 de enero de 1892-Bournemouth, Dorset; 2 de septiembre de 1973), a menudo citado como J. R. R. Tolkien o JRRT, fue un escritor, poeta, filólogo, lingüista y profesor universitario británico nacido en el desaparecido Orange al sur de África, conocido principalmente por ser el autor de las novelas clásicas de fantasía heroica "El hobbit", "El Silmarillion" y "El Señor de los Anillos".

De 1925 a 1945, Tolkien ocupó la cátedra Rawlinson y Bosworth en la Universidad de Oxford, enseñando anglosajón y, de 1945 a 1959, fue profesor de Lengua y Literatura inglesa en Merton. Era amigo cercano del también escritor C. S. Lewis y ambos eran miembros de un informal grupo de debate literario conocido como los Inklings. Tolkien fue nombrado Comendador de la Orden del Imperio Británico por la reina Isabel II el 28 de marzo de 1972.

Después de su muerte, el tercer hijo de Tolkien, Christopher, publicó una serie de obras basadas en las amplias notas y manuscritos inéditos de su padre, entre ellas "El Silmarillion" y "Los hijos de Húrin". Estos libros, junto con "El hobbit" y "El Señor de los Anillos", forman un cuerpo conectado de cuentos, poemas, historias de ficción, idiomas inventados y ensayos literarios sobre un mundo imaginado llamado Arda, y más extensamente sobre uno de sus continentes, conocido como la Tierra Media. Entre 1951 y 1955, Tolkien aplicó la palabra "legendarium" a la mayor parte de estos escritos.

Si bien escritores como William Morris, Robert E. Howard y E. R. Eddison precedieron a Tolkien en el género literario de fantasía con obras tan famosas e influyentes como las de "Conan el Bárbaro", el gran éxito de "El hobbit" y "El Señor de los Anillos" cuando se publicaron en Estados Unidos condujo directamente al resurgimiento popular del género. Esto ha causado que Tolkien sea identificado popularmente como «el padre» de la literatura moderna de fantasía, o más concretamente, de la alta fantasía. Los trabajos de Tolkien han inspirado muchas otras obras de fantasía y han tenido un efecto duradero en la cultura popular. En 2008, el periódico "The Times" le clasificó sexto en una lista de «Los 50 escritores británicos más grandes desde 1945».

Por los datos que se conocen, la mayoría de los antepasados paternos de Tolkien fueron artesanos. La familia Tolkien tenía sus raíces en el estado alemán de Baja Sajonia, aunque se había afincado en Inglaterra en el siglo XVIII, y adaptado intensamente a su cultura. El apellido «Tolkien» es la forma anglicanizada del alemán «Tollkiehn», cuyo origen radica en "tollkühn" (‘temerario’). En cambio, la familia Suffield, antepasados maternos de Tolkien, tenían fuerte raigambre en la ciudad de Birmingham, donde se dedicaron al comercio al menos desde principios del siglo XIX, tras desplazarse allí desde Evesham (Worcestershire).

John Ronald Reuel nació en Bloemfontein, capital del Estado Libre de Orange, la noche del domingo 3 de enero de 1892. Sus padres fueron Arthur Tolkien y Mabel Suffield, ambos del Reino Unido. Recibió el mismo nombre que su abuelo paterno, John, pues en su familia era costumbre llamar así al primogénito del hijo mayor. Su medio tío John, el mayor de los hijos de John Benjamin Tolkien, solo tuvo hijas, por lo que Arthur decidió llamar a su hijo según la costumbre. Su segundo nombre, Ronald, fue puesto por deseo de Mabel, ya que ella creía que el bebé iba a ser una niña y tenía pensado llamarla Rosalind, y finalmente eligió Ronald como sustituto. Reuel, que proviene del antiguo hebreo y que significa ‘próximo a Dios’, era el segundo nombre de su padre. Ronald sería el nombre que utilizarían sus padres, sus parientes y su esposa, a pesar de no sentirse totalmente identificado con él; sus allegados le llamaban John Ronald, «Tollers» o simplemente Tolkien.

El niño fue bautizado el 31 de enero en la catedral de Bloemfontein. Tiempo después, cuando Ronald comenzaba a andar, una tarántula lo picó en el jardín de su casa, un evento que algunos aseguran tiene paralelos en sus historias, a pesar de que Tolkien admitió no tener ningún recuerdo del accidente ni miedo a las arañas de adulto. El 17 de febrero de 1894 nació su hermano menor, Hilary Arthur.
A pesar de que Arthur quiso permanecer en África, el clima del lugar perjudicaba la salud de Ronald por lo que, en 1895, cuando contaba tres años, se trasladó con su madre Mabel y su hermano Hilary a Inglaterra, en lo que debía ser una prolongada visita familiar, mientras su padre permanecía en Orange, a cargo de la venta de diamantes y otras piedras preciosas para el Banco de Inglaterra. La intención de Arthur Tolkien era la de reunirse con su familia en Inglaterra, pero murió el 15 de febrero de 1896 de una fiebre reumática. Su sorpresiva muerte dejó a la familia sin ingresos, por lo que Mabel debió llevar a sus hijos a vivir con su propia familia en Birmingham.
Ese mismo año volvieron a mudarse a Sarehole (en la actualidad, en Hall Green), por entonces una pequeña villa de Worcestershire, más tarde absorbida por Birmingham. A Ronald le encantaba explorar el cercano bosque de la turbera de Moseley y la aceña de Sarehole, así como las colinas de Clent y de Lickey, lugares que más adelante inspirarían algunos pasajes en sus obras, junto con otros parajes de Worcestershire como Bromsgrove y Alvechurch, Alcester (Warwickshire) y la granja de su tía, Bag End («Bolsón Cerrado»), nombre que utilizaría en sus relatos.

Mabel se encargó de la educación de sus dos hijos. Ronald era un alumno muy aplicado. Su gran interés por la botánica procedía de las enseñanzas de Mabel, que despertó en su hijo el placer de mirar y sentir las plantas. Ronald disfrutaba dibujando paisajes y árboles, pero sus lecciones favoritas eran aquellas relacionadas con los idiomas, puesto que su madre comenzó a enseñarle las bases del latín a tan temprana edad. De esta forma, ya podía leer a los cuatro años y escribir de forma fluida poco después.

Tolkien asistió a la King Edward's School de Birmingham. Mientras estudiaba allí participó en el desfile de coronación de Jorge V, siendo ubicado justo al exterior de las puertas del palacio de Buckingham. Más tarde fue inscrito en la escuela de San Felipe del oratorio de Birmingham.

En 1900, Mabel se convirtió junto con sus dos hijos al catolicismo a pesar de la fuerte oposición de su familia, de confesión baptista, que como consecuencia retiró toda la ayuda económica que le había estado prestando desde que se quedó viuda. En 1904, cuando Ronald tenía doce años, Mabel falleció debido a complicaciones de diabetes —una enfermedad muy peligrosa antes de la aparición de la insulina— en Fern Cottage (Rednal), donde vivía con sus hijos en una casa alquilada. Por mantenerse en el catolicismo frente a la retirada de la ayuda económica familiar, durante toda su vida Ronald vivió convencido de que su madre había sido una verdadera mártir de su fe, lo que le produjo una profunda impresión en sus propias creencias católicas.

Durante su orfandad, Ronald y Hilary fueron educados por el padre Francis Xavier Morgan, un sacerdote católico del oratorio de Birmingham, situado en la zona de Edgbaston. Morgan, andaluz aunque de padre galés, había apoyado moral y económicamente a Mabel Tolkien tras su conversión y había enseñado al joven Ronald las bases del idioma español que más tarde emplearía en la creación de su «naffarin». El Oratorio estaba casi bajo la sombra de las torres de Perrott's Folly y Edgbaston Waterworks, que inspirarían las imágenes de las torres oscuras de Orthanc y Minas Morgul de "El Señor de los Anillos".

Otra influencia notable que recibió en esta etapa fueron las pinturas románticas medievalistas de Edward Burne-Jones y la hermandad prerrafaelita, muchas de cuyas obras pertenecen hoy al Museo y Galería de Arte de Birmingham (Birmingham Museum and Art Gallery), que las expuso al público a partir de 1908.

En 1908, a los dieciséis años, Tolkien conoció en el orfanato a Edith Mary Bratt, de quien se enamoró pese a ser tres años menor. El padre Morgan le prohibió encontrarse, hablar e incluso mantener correspondencia con ella hasta que él cumpliese los veintiún años, lo que el joven obedeció al pie de la letra.

En 1911, mientras estaba en el colegio King Edward de Birmingham, Tolkien formó junto con tres amigos (Robert Gilson, Geoffrey Smith y Christopher Wiseman) una sociedad semisecreta conocida como la "T.C., B.S.", las iniciales del Tea Club and Barrovian Society («Club de Té y Sociedad Barroviana»), en alusión a su afición de tomar el té en Barrow's Stores, cerca de la escuela, así como en la biblioteca de la propia escuela (de forma ilegal). Después de dejar la escuela, los miembros mantuvieron el contacto. De hecho, celebraron en diciembre de 1914 un "concilio" en Londres, en casa de Wiseman. Para Tolkien, el resultado de este encuentro supuso un fuerte impulso para escribir poesía. 

Más allá de las uniones íntimas de literatura, estudios y juegos, fluía un propósito mayor. Según John Garth, escritor, editor e investigador, galardonado por su obra "Tolkien y la Gran Guerra. El origen de la Tierra Media" ("Tolkien and the Great War)," Smith “declaró que a través del arte, los cuatro tendrían que dejar el mundo mejor de lo que lo encontraron” . Además, Smith creía que él y sus colegas poseían la responsabilidad “de restablecer sensatez, higiene, y el amor de real y verdadera belleza en los pechos de todos”. Tolkien declaró un parecer que “ellos tenían un “poder que estremecía al mundo”". 

En el verano de 1911, Tolkien viajó de vacaciones a Suiza, un viaje que rememoró en una carta en 1968 de forma aún muy vívida, donde señalaba que el viaje de Bilbo a través de las Montañas Nubladas (incluyendo el «deslizamiento por las piedras resbaladizas hasta el bosque de pinos») está directamente basado en sus aventuras con su grupo de doce compañeros de excursión desde Interlaken hasta Lauterbrunnen, y en su acampada en las morrenas más allá de Mürren. Cincuenta y siete años más tarde, Tolkien recordaba su profunda pena al abandonar las vistas de las nieves perpetuas de Jungfrau y Silberhorn, «la "Silvertine" (Celebdil) de mis sueños».
Después de muchas trabas e impedimentos del padre Francis (que deseaba que Tolkien se centrase en acabar sus estudios de Filología Inglesa en Oxford con honores), la misma tarde del día de su vigésimo primer cumpleaños Tolkien escribió una carta a Edith para declararle su amor y preguntarle si deseaba casarse con él. Ella le respondió que ya estaba comprometida, ya que creía que Tolkien la había olvidado. Se reunieron bajo un viaducto de ferrocarril, donde renovaron su amor, tras lo cual Edith devolvió su anillo de compromiso y decidió casarse con él. Tras comprometerse en Birmingham en enero de 1913, Edith se convirtió al catolicismo ante la insistencia de Tolkien, y se casaron el 22 de marzo de 1916 en Warwick.

Antes de su matrimonio, sus viajes le llevaron a Cornualles donde, debido al amor que sentía por los paisajes desde la época de su infancia, quedó impresionado por la visión de la singular costa córnica y el mar. Se licenció en 1915 en el Exeter College, donde Joseph Wright, catedrático de Lingüística histórica, había ejercido una gran influencia en el interés de Tolkien por distintas lenguas, con matrícula de honor en Lengua inglesa, en la modalidad «Lingüística inglesa y literatura hasta Chaucer».

Después de su graduación, Tolkien se unió al Ejército Británico que luchaba por entonces en la Primera Guerra Mundial. Se enroló con el rango de teniente segundo, especializado en lenguaje de signos, en el 11º Batallón de Servicio de los Fusileros de Lancashire, que fue enviado a Francia en 1916 con la Fuerza Expedicionaria Británica. Tolkien sirvió como oficial de comunicaciones en la batalla del Somme hasta que enfermó debido a la denominada «fiebre de las trincheras» el 27 de octubre, por lo que fue trasladado a Inglaterra el 8 de noviembre.

Durante su convalecencia en una cabaña en Great Haywood (Staffordshire), comenzó a trabajar en lo que llamó "El libro de los cuentos perdidos" con «La caída de Gondolin». Entre 1917 y 1918, sufrió recaídas de su enfermedad, aunque se había restablecido lo suficiente como para hacer tareas de mantenimiento en varios campamentos, tras lo que fue ascendido al rango de teniente. Cuando fue destinado a Kingston upon Hull, fue un día a caminar con su esposa por los bosques de la cercana Roos, y Edith comenzó a bailar para él en una densa arboleda de cicutas, rodeada de flores blancas. Esta escena inspiró el pasaje del encuentro de Beren y Lúthien, y Tolkien solía referirse a Edith como «su Lúthien». Tolkien y Edith tuvieron cuatro hijos: el sacerdote John Francis Reuel (1917-2003), el maestro de escuela Michael Hilary Reuel (1920-1984), el escritor Christopher John Reuel (1924-2020) y la trabajadora social Priscilla Anne Reuel (n. 1929).

El primer trabajo civil de Tolkien tras la guerra fue como lexicógrafo asistente en la redacción para la primera edición del "Oxford English Dictionary", donde trabajó durante dos años principalmente en la historia y etimología de las palabras de origen germánico que comenzaban por la letra W, rastreando su origen en el alto alemán, alemán medio e incluso nórdico antiguo. En 1920. ocupó el puesto de profesor no titular de Lengua inglesa en la Universidad de Leeds, donde alcanzó el cargo de profesor, reformando con su magisterio la enseñanza de esta disciplina. En Leeds conoció a E. V. Gordon, con quien publicó la que es considerada la mejor edición hasta la fecha de la obra anónima de la "Alliterative Revival", "Sir Gawain y el Caballero Verde", escrita en inglés medio a finales del siglo XIV.

En 1924 nació su tercer hijo, Christopher, quien se encargaría de publicar póstumamente todos los manuscritos que su padre había dejado desparramados por el estudio en su casa de Northmoor Road. En 1925, regresó a Oxford como profesor de Anglosajón en el Pembroke College. Fue durante su estancia en Pembroke que Tolkien escribió "El hobbit" y los dos primeros volúmenes de "El Señor de los Anillos".

Aunque Tolkien nunca esperó que sus historias ficcionales se volvieran tan populares, en 1937 C. S. Lewis lo persuadió para que publicara "El hobbit", originalmente escrito para sus hijos. Sin embargo, el libro a su vez atrajo a lectores adultos, y se volvió lo suficientemente popular como para la editorial, George Allen & Unwin, por lo que le pidieron a Tolkien que escribiera una secuela a la obra. En 1929 nació su hija Priscilla.

En 1928 Tolkien ayudó a "sir" Mortimer Wheeler en la excavación de un "asclepeion" romano en Lydney Park (Gloucestershire). Respecto a las publicaciones académicas, su conferencia de 1936 titulada «» tuvo una decisiva influencia en los estudios acerca del mito de Beowulf.
En Oxford, Tolkien trabó amistad con el profesor y escritor C. S. Lewis, (futuro autor de "Las crónicas de Narnia"), con quien disentía al principio a causa de sus convicciones religiosas (Lewis era agnóstico, y posteriormente se hizo protestante), pero que acabó siendo uno de sus principales correctores, junto con los otros miembros del club literario que formaron, los "Inklings". Sus miembros se reunían los viernes antes de comer en el "pub" Eagle and Child, y la noche de los jueves en las habitaciones de Lewis en el Magdalen College para recitar las obras que cada uno componía, así como romances y extractos de las grandes obras épicas del Norte de Europa.
Desde su adolescencia, Tolkien había empezado a escribir una serie de mitos y leyendas sobre la Tierra Media. Echaba en falta en su país una mitología del carácter de la griega, por ejemplo, y se proponía inventar «una mitología para Inglaterra», que más tarde daría lugar a "El Silmarillion", originalmente denominado "El libro de los cuentos perdidos". Dichos relatos están supuestamente inspirados en un cuento publicado en 1927 por Edward Wyke-Smith titulado "El maravilloso país de los snergs" (también el "Kalevala" finlandés, las sagas escandinavas y, en general, un poco de toda la mitología europea de cualquier origen).

En 1957, Tolkien viajaba a Estados Unidos para recibir títulos honoríficos de las principales universidades, como Marquette (donde hoy en día se conservan los manuscritos originales de sus obras) y Harvard. El viaje tuvo que suspenderse, pues Edith cayó enferma. Tolkien se retiró dos años después de su cargo en Oxford. En 1961, C. S. Lewis lo propuso como candidato para el Premio Nobel de Literatura, pero el jurado desestimó la propuesta por su «pobre prosa». En 1965, se publicó la primera edición de "El Señor de los Anillos" en Estados Unidos. En 1968, la familia Tolkien se trasladó a Poole, cerca de Bournemouth.

En esta época fue nombrado doctor "honoris causa" por varias universidades, vicepresidente de la Philological Society y miembro de la Royal Society of Literature. En 1969, la reina Isabel II le nombró Comendador de la Orden del Imperio Británico. En su honor se fundaron, en primer lugar, la Mythopoeic Society norteamericana y la Tolkien Society británica, y decenas de sociedades similares en diversos países.

Edith murió el 29 de noviembre de 1971, a la edad de 81 años. Tolkien volvió a Oxford, donde falleció 21 meses después, el 2 de septiembre de 1973, con 81 años, y fue enterrado en la misma tumba que su mujer. Esta tumba, situada en el cementerio de Wolvercote, en Oxford, presenta los nombres de «Beren» y «Lúthien» para Ronald y Edith, respectivamente, extraídos de la leyenda incluida en "El Silmarillion" acerca del amor entre estos dos seres de diferente naturaleza (la doncella elfa Lúthien y el mortal Beren) y del robo de uno de los Silmarils.

Tolkien fue un devoto católico, y así se sintió el instrumento de la conversión de C. S. Lewis del ateísmo al cristianismo. Sin embargo, se decepcionó cuando este se volvió anglicano (iglesia a la que Tolkien se refería como «una patética y oscurecedora mezcolanza de tradiciones medio recordadas y creencias mutiladas»), en lugar de católico.

Tolkien educó intensamente a sus hijos en su religión. En una carta fechada el 8 de enero de 1944, y dirigida a su hijo Christopher con la intención de darle ánimos, le insta, tras explicarle un poco de doctrina católica, a recurrir a las alabanzas: «Yo las utilizo mucho (en latín): el "Gloria Patri"; el "Gloria in excelsis"; el "Laudate Dominum"; el "Laudate pueri Dominum", uno de los salmos dominicales y el "Magnificat"» y la carta continúa señalando varias otras formas religiosas de buscar tranquilidad e inspiración.

En sus últimos años, Tolkien quedó profundamente decepcionado por las reformas y cambios llevados a cabo tras el Concilio Vaticano Segundo, tal como recuerda su nieto Simon Tolkien:
Es un comentario habitual, que existen paralelismos entre la saga de la Tierra Media y ciertos hechos de la vida de Tolkien. Suele argumentarse que "El Señor de los Anillos" representa a Inglaterra durante e inmediatamente después de la Segunda Guerra Mundial. Tolkien repudió ardientemente esta opinión en el prefacio a la segunda edición de su novela, declarando que prefería la aplicabilidad a la alegoría. Trató este tema con mayor extensión en su ensayo «Sobre los cuentos de hadas», en el que argumenta que los cuentos de hadas son válidos porque son consistentes consigo mismos y con algunas verdades sobre la realidad. Concluyó que el cristianismo en sí mismo sigue este patrón de consistencia interna y verdad externa. Su creencia en las verdades fundamentales del cristianismo y su lugar en la mitología lleva a los comentaristas a encontrar temas cristianos en "El Señor de los Anillos", a pesar de su notable falta de referencias abiertamente religiosas, ceremonias religiosas o apelaciones a Dios. Tolkien se opuso vehementemente al uso de referencias religiosas por parte de C. S. Lewis en sus historias, que muchas veces eran abiertamente alegóricas. Sin embargo, Tolkien escribió que la escena del Monte del Destino ejemplifica líneas del Padre nuestro.
Con todo, no puede obviarse que en su carta de respuesta (Cartas n.º 142), Tolkien reconoció que: «El Señor de los Anillos es, por supuesto, una obra fundamentalmente religiosa y católica».
Su amor por los mitos y su fe devota se unieron en su creencia en que la mitología «es el eco divino de la Verdad». Expresó este punto de vista en su poema «Mitopoeia», y su idea de que los mitos contienen ciertas «verdades fundamentales» se convirtió en un tema central de los "Inklings" en su conjunto.

Las ideas políticas de Tolkien estaban guiadas por su estricto catolicismo, por lo que sus puntos de vista eran predominantemente conservadores, en el sentido de favorecer las convenciones y ortodoxias establecidas por encima de la innovación y la modernización. Apoyó el bando de Franco durante la Guerra Civil Española, tras tener noticias de que milicianos «rojos» estaban destruyendo iglesias y matando a sacerdotes y monjas en la zona republicana. Tras reunirse con él en 1944, Tolkien expresó su admiración por el poeta sudafricano católico Roy Campbell, a quien consideraba defensor de la fe católica por sus acciones con el bando franquista.

Siguiendo la opinión predominante en la Gran Bretaña de la época, se mostraba de acuerdo con la política de apaciguamiento defendida por el gobierno de Chamberlain. Sin embargo, Tolkien siempre condenó la doctrina racial del Partido Nazi y su antisemitismo como algo «totalmente pernicioso y acientífico». Cuando, en febrero de 1938, sus editores en Alemania le pidieron confirmación sobre si era de ascendencia aria, remitió dos borradores de respuesta distintos a sus editores ingleses. En el que se conserva (es decir, el que no se envió a Alemania), después de ridiculizar la mitificación del origen ario (hindú o persa) de los pueblos germánicos, replica:
En 1967 protestó contra una descripción de la Tierra Media como «nórdica», un término que le desagradaba por su asociación con la teoría racial de nombre similar. Tolkien no sentía por Hitler más que desprecio y le acusaba: «Arruina, pervierte, aplica erradamente y vuelve por siempre maldecible ese noble espíritu nórdico, suprema contribución a Europa, que siempre amé e intenté presentar en su verdadera luz». Tiempo después hablaría de Hitler como de uno de los «idiotas militares», «un pillo vulgar e ignorante, además de tener otros defectos (o la fuente de ellos)». Del otro bando, el suyo, tampoco le gustaba la propaganda antialemana demagógica y maniquea empleada durante la Segunda Guerra Mundial para reforzar el esfuerzo de guerra británico.

Sorprendentemente, en 1943 escribió: «Mis opiniones políticas se inclinan más y más hacia el anarquismo (entendido filosóficamente, lo cual significa la abolición del control, no hombres barbados armados de bombas) o hacia la monarquía “inconstitucional”. Arrestaría a cualquiera que empleara la palabra Estado (en cualquier otro sentido que no fuera el reino inanimado de Inglaterra y sus habitantes, algo que carece de poder, derechos o mente) [...]». Estas palabras le han llevado a ser calificado de «anarquista monárquico».

La cuestión del racismo en la obra de Tolkien ha sido objeto de un cierto debate académico. Christine Chism clasifica las acusaciones en tres categorías distintas: racismo intencional, un prejuicio eurocentrista inconsciente, y una evolución de un racismo latente en sus primeras obras, a un repudio consciente de las tendencias racistas en sus últimos trabajos. John Yatt ha escrito: «Los “blancos” son buenos, los “oscuros” son malos, los orcos son los peores de todos». Sin embargo, otros críticos como Tom Shippey o Michael D. C. Drout no están de acuerdo con una generalización tan radical a partir de los hombres «blancos» y «oscuros» de Tolkien en «buenos» y «malos». La obra de Tolkien también ha sido defendida en este sentido por racistas declarados como el Partido Nacional Británico.

Ya se ha comentado anteriormente su postura sobre la política racial en Alemania; sobre las condiciones de vida de la gente de color en Sudáfrica, antes del "apartheid", escribió a su hijo Christopher:

Tolkien perdió a la mayoría de sus amigos en las trincheras durante la Primera Guerra Mundial, lo que le ponía indefectiblemente en contra de la guerra en general. Cerca del final de la Segunda, declaró que los Aliados no eran mejores que los nazis y que se comportaban como orcos en sus llamadas a una completa destrucción de Alemania. En algunos fragmentos de las Cartas a su hijo Christopher, deja ver la amargura e inutilidad humana que le provoca la guerra, y compara hechos reales con los de sus libros: «...estamos intentando conquistar a Sauron con el Anillo. Y (según parece) lo lograremos. Pero el precio es, como lo sabrás, criar nuevos Saurons y lentamente ir convirtiendo a hombres y elfos en orcos». Horrorizado por los bombardeos atómicos sobre Hiroshima y Nagasaki, se refirió a los científicos del Proyecto Manhattan como «físicos lunáticos» y «constructores de Babel». También escribió: «[...] no conozco nada sobre el imperialismo británico o americano en el Lejano Oriente que no me llene de dolor y repugnancia [...]».

Tolkien, gran amante y defensor de los árboles y los bosques, demostró en sus escritos un gran respeto por la naturaleza. También manifestó su rechazo a los efectos colaterales de la industrialización, que consideraba devoradora del paisaje rural inglés. Esta actitud conservacionista puede ser percibida en su trabajo, siendo el caso más palpable su retrato de la «industrialización forzada» de la Comarca al final de "El retorno del Rey".

Durante la mayor parte de su vida se mostró hostil incluso a los automóviles y prefería conducir su bicicleta. Caricaturizó este aspecto de su personalidad en "El señor Bliss", cuento infantil profusamente ilustrado por él mismo y editado de forma póstuma por Baillie Tolkien.

El primer poema que Tolkien consiguió publicar fue "La batalla del Campo del Este" en 1911, cuando tenía diecinueve años.

Desde hacía tiempo, Tolkien estaba interesado en el inglés antiguo o anglosajón y se había dedicado a leer varias obras en esta lengua, entre ellas, el poema anónimo "Christ I"; dos líneas de este le impresionaron especialmente:

En 1914, inspirado por estas líneas, escribió el poema "El viaje de Eärendel, la estrella vespertina", que narraba el viaje por el cielo del marinero Eärendel, más tarde convertido en Eärendil. Este poema sería imprescindible en el desarrollo de su futuro "legendarium".

Tolkien continuó escribiendo numerosos poemas, algunos de ellos relacionados con su "legendarium" y que más tarde serían incluidos por su hijo Christopher en los volúmenes de "La historia de la Tierra Media". En 1917, cuando estaba hospitalizado debido a una enfermedad contraída durante la Primera Guerra Mundial, comenzó a trabajar en otros poemas que se convertirían en la base de las historias principales de "El Silmarillion": "El cuento de Tinúviel", "Turambar y el Foalókê", y "La caída de Gondolin"; con el paso de los años, estos poemas se convirtieron en textos en prosa que evolucionaron hasta las historias de Beren y Lúthien, "Los hijos de Húrin" y "La caída de Gondolin", respectivamente.

En 1953, publicó con bastante éxito el poema "El regreso de Beorhtnoth, hijo de Beorhthelm", aunque ya estaba terminado desde 1945. Escrito en verso aliterativo, es una continuación del inacabado poema anglosajón "La batalla de Maldon".

En 1961, una tía le pidió que escribiera un libro dedicado a Tom Bombadil, personaje que aparece en "El Señor de los Anillos". Aunque solo los dos primeros poemas están dedicados a dicho personaje, Tolkien tituló el poemario como "Las aventuras de Tom Bombadil y otros poemas de El Libro Rojo", e incluyó en él otros poemas datados de la década de 1920.

J. R. R. Tolkien acostumbraba desde siempre a narrar historias a sus propios hijos, por los motivos más diversos. Así, concibió el relato de "Roverandom" en 1925, como un cuento para sus hijos John (ocho años) y Michael (cinco) durante unas vacaciones. Michael estaba muy encariñado aquel verano de uno de sus juguetes: un perrito en miniatura, de plomo pintado de blanco y negro. Desafortunadamente, un día paseando por la playa con su padre, lo dejó en el suelo para jugar y lo perdió. Aunque Tolkien y sus dos hijos mayores pasaron horas buscándolo, no fue posible recuperarlo, por lo que el autor imaginó la historia que hoy conocemos como "Roverandom" para consolar al pequeño Michael.

Se trata de un cuento infantil que narra la historia de un perrito llamado Rover que muerde a un brujo, por lo que este lo castiga convirtiéndolo en juguete. Un niño compra ese juguete, pero lo pierde en la playa. Entonces, el hechicero de la arena le hace vivir aventuras desde la Luna hasta el fondo del mar.

Este cuento no fue publicado hasta 1998, de manera póstuma.

 Tolkien escribió un breve esquema de su mitología del que los cuentos de Beren y Lúthien y el de Túrin formaban parte, y ese esquema fue evolucionando hasta convertirse en el "Quenta Silmarillion", una historia épica que Tolkien comenzó tres veces pero nunca publicó. Tolkien confiaba en publicarla al abrigo del éxito de "El Señor de los Anillos", pero a las editoriales (tanto a Allen & Unwin como a Collins) no las convenció; puesto que, además, los costes de impresión eran muy altos en la posguerra. La historia de esta continua reescritura se cuenta en la serie póstuma de "La historia de la Tierra Media", editada por el hijo de Tolkien, Christopher. Desde 1936, aproximadamente, Tolkien empezó a extender su marco de trabajo para abarcar la narración de la caída de Númenor ("Akallabêth"), inspirada en la leyenda de la Atlántida. No fue hasta 1977, de manera póstuma, que los escritos que componen "El Silmarillion" vieron la luz, recopilados y editados por Christopher Tolkien. A los relatos mencionados ("Quenta Silmarillion" y "Akallabêth"), se añadieron para la publicación otros más breves, de los primeros y los últimos tiempos de la Tierra Media: "Ainulindalë", "Valaquenta" y "De los Anillos de Poder y la Tercera Edad".

Tolkien escribía las historias de su "legendarium" para su propio deleite, el de su familia y el de su círculo literario, sin intención de alcanzar con ellas al gran público. Sin embargo, por casualidad, otro libro que había escrito en 1932 para sus propios hijos y al que había titulado "El hobbit" pasó de mano en mano sin intervención del autor hasta llegar a Susan Dagnall, una empleada de la editorial londinense George Allen & Unwin. Ésta le enseñó el libro al presidente de la empresa, Stanley Unwin, quien se lo dio a su hijo pequeño, Rayner, para que lo leyera; la historia le gustó tanto que decidieron publicarlo.

En este libro se narran las aventuras del hobbit Bilbo Bolsón que, junto con el mago Gandalf y una compañía de enanos, se verá envuelto en un viaje para recuperar el reino de Erebor, arrebatado a los enanos por el dragón Smaug.

Si bien se trata de una historia infantil, el libro atrajo también la atención de lectores adultos y se hizo lo suficientemente popular como para que Stanley Unwin le pidiera a Tolkien que trabajara en una secuela, más tarde conocida como "El Señor de los Anillos".

Aunque no se encontraba inspirado para tratar el tema, la petición de Stanley Unwin de una secuela para "El hobbit" impulsó a Tolkien a comenzar la que sería su obra más famosa, "El Señor de los Anillos", una novela de fantasía épica subdividida en tres volúmenes y publicada entre 1954 y 1955. Tolkien invirtió más de diez años en la creación de la historia y los apéndices de la novela, tiempo durante el cual recibió el apoyo constante de los Inklings, en particular de su amigo más cercano, C. S. Lewis, al que prestaba o leía los borradores que iba escribiendo para que los juzgara. Tanto los acontecimientos de "El hobbit" como los de "El Señor de los Anillos" están enmarcados en el contexto de "El Silmarillion", pero en una época bastante posterior.

La intención original de Tolkien al empezar a escribir "El Señor de los Anillos" era que este fuera un cuento para niños al estilo de "El hobbit", pero poco después recordó el anillo encontrado por Bilbo Bolsón y decidió centrar la historia en torno a él y su devenir, convirtiéndose en un escrito más oscuro y serio; por ello, a pesar de ser una continuación directa de "El hobbit", fue dirigido a un público más maduro. Por otro lado, Tolkien aprovechó más en esta novela la inmensa historia de Beleriand, que había ido construyendo en años anteriores y que finalmente fue publicada de forma póstuma en el "El Silmarillion" y otros volúmenes.

"El Señor de los Anillos" se volvió tremendamente popular en la década de 1960 y se ha mantenido así desde entonces, situándose como una de las obras de ficción más populares del siglo XX a juzgar por sus ventas y las encuestas de lectores, como la realizada por las librerías Waterstone's de Reino Unido y la cadena de televisión Channel 4, que eligió a "El Señor de los Anillos" como el mejor libro del siglo.

Se trata de una versión en prosa del ciclo de Kullervo del poema épico finlandés "Kalevala". Escrito por J. R. R. Tolkien cuando era un estudiante en el Exeter College, Oxford, de 1914 a 1915, fue una época inestable para el autor y esta es la sensación que se refleja en la oscura temática de la historia.

Tolkien aprendió latín, francés y alemán de su madre y, mientras estaba en el colegio, aprendió inglés medio, inglés antiguo, finlandés, gótico, griego, italiano, noruego antiguo, español, galés y galés medieval. También estuvo familiarizado con el esperanto, danés, neerlandés, lombardo, noruego, ruso, serbio, sueco y antiguas formas del alemán moderno y eslovaco, lo que revela su profundo conocimiento lingüístico sobre todas las lenguas germánicas.

Su pasión por los idiomas comenzó a los ocho o nueve años de edad, cuando se deleitaba con el sonido del latín pronunciado por su madre o se entretenía con su prima Mary inventando sus propias lenguas, como el «animálico» o el «nevbosh» (‘nuevo disparate’). Algo más tarde creó el «naffarin» (basado en el español que aprendía con la ayuda del padre Morgan). Después descubrió el gótico, el galés y el finlandés, base de sus grandes creaciones: el sindarin, la lengua de los sindar y, sobre todo, el quenya, la lengua de los noldor; alentado por sus profesores Kenneth Sisam, catedrático de instituto en Literatura Comparada y con quien competiría por la cátedra de anglosajón en el Merton College de la Universidad de Oxford, y Robert Gilson, quienes descubrieron en él a un gran filólogo.

Su carrera académica y su producción literaria son inseparables de su amor por el lenguaje y la filología. Se especializó en la filología del griego durante la universidad y en 1915 se graduó con nórdico antiguo como materia especial. De 1919 a 1920, tras licenciarse del ejército una vez finalizada la Primera Guerra Mundial, Tolkien trabajó como ayudante del redactor jefe de la primera edición del "Oxford English Dictionary", y se encargó de redactar los borradores para tres adiciones que aparecieron por primera vez en la edición publicada en octubre de 1921. En 1920, fue a la Universidad de Leeds como profesor de inglés, donde reclamó crédito por aumentar el número de estudiantes en lingüística de cinco a veinte. Dio cursos sobre el verso heroico en inglés antiguo, historia del inglés, varios textos en inglés antiguo y medio, filología del inglés antiguo y medio, filología introductoria a las lenguas germánicas, gótico, nórdico antiguo y galés medieval. Cuando en 1925, con treinta y tres años, Tolkien solicitó la cátedra Rawlinson y Bosworth, presumió de que sus estudiantes de filología germana en Leeds habían formado un «Club Vikingo».

Privadamente, a Tolkien lo atraían las «cosas de significación racial y lingüística», y contempló nociones de un heredado gusto por el lenguaje, donde calificó a la «lengua nativa» como opuesta a la «lengua materna» en su conferencia «El inglés y el galés», que es crucial para entender su concepto de la raza y el lenguaje. Consideraba el inglés medio de los Midlands occidentales su «lengua nativa» y, como le escribió a W. H. Auden en 1955, «Soy de los Midlands Occidentales por sangre (y tomé el inglés medio de estos como una lengua conocida tan pronto como posé mis ojos sobre ellos)».

Paralelamente a su trabajo profesional como filólogo, y algunas veces eclipsándolo hasta el extremo de que su producción académica permaneciera bastante escasa, estaba su afecto por la construcción de lenguas artificiales. Las de mayor desarrollo eran el quenya y el sindarin. El lenguaje y la gramática para Tolkien fueron una cuestión de estética y eufonía, y el quenya en particular fue diseñado por consideraciones «fonoestéticas»; fue previsto como un «elfolatín», y estaba basado fonológicamente en el latín, con ingredientes del finés y el griego. Una notable adición vino a fines de 1954 con el adunaico de Númenor, una lengua de «un sabor ligeramente semítico», conectada con el mito tolkieniano de la Atlántida que, por medio de «Los papeles del Notion Club», se liga directamente con sus ideas sobre la heredabilidad del lenguaje, y a través de la Segunda Edad del Sol el mito de Eärendil fue asentado en el "legendarium", de este modo proveyendo un enlace al «mundo real y primordial» del siglo XX de Tolkien con el pasado mitológico de la Tierra Media.

Tolkien consideraba los lenguajes inseparables de la mitología asociada con ellos y, consecuentemente, tomó tenue vista de las lenguas auxiliares: en 1930 un congreso de esperantistas escucharon esto de él, en su conferencia «Un vicio secreto», «La construcción de su lenguaje engendrará una mitología», pero en 1956 concluyó que el «volapük, esperanto, ido, novial, etc., están muertos, más que otras lenguas ancestrales no utilizadas, debido a que sus autores nunca inventaron ninguna leyenda en esperanto».

La popularidad de los libros de Tolkien ha tenido un pequeño pero duradero efecto en el uso del lenguaje en la literatura fantástica en particular, e incluso en importantes diccionarios, que hoy en día comúnmente aceptan el restablecimiento tolkiano de las palabras "dwarves" ("enanos") y "elvish" ("élfico") (en contraposición a "dwarfs" y "elfish"), que no habían estado en uso desde mitad aproximadamente el siglo XIX. Otros términos que ha acuñado, tales como "legendarium" y "eucatástrofe" son mayormente usados en conexión con su trabajo.

Varias obras de Tolkien han sido adaptadas al cine, empezando por la adaptación animada por Rankin/Bass de la novela "El hobbit" en 1977. Al año siguiente Ralph Bakshi dirigió "El Señor de los Anillos", una adaptación incompleta también para el cine de animación de la novela en tres volúmenes. El equipo de la Rankin/Bass creó también, en 1980, un especial animado de televisión titulado "El retorno del Rey", que incluía una recapitulación muy breve de los dos primeros tomos de "El Señor de los Anillos", y que se presentó como una continuación de la película de 1977.

Más de veinte años después, New Line Cinema y el director neozelandés Peter Jackson crearon la adaptación más exitosa de "El Señor de los Anillos", en "una trilogía de películas" protagonizadas por Elijah Wood, Viggo Mortensen, Sean Astin, Christopher Lee, Andy Serkis, Liv Tyler, Orlando Bloom e Ian McKellen, estrenadas en los años 2001: "El Señor de los Anillos: La Comunidad del Anillo", 2002 "El Señor de los Anillos: Las dos torres" y 2003 "El Señor de los Anillos: El retorno del rey".

Posteriormente, los mismos New Line Cinema y Peter Jackson (aunque durante un tiempo se barajó la dirección del mexicano Guillermo del Toro) abordaron también la adaptación de "El hobbit" en una trilogía de películas , con Martin Freeman como Bilbo Bolsón, Richard Armitage como Thorin, Ian McKellen como Gandalf, Orlando Bloom nuevamente como el elfo silvano Legolas, Christopher Lee, que volvió a interpretar a Saruman, Benedict Cumberbatch como Smaug además de Andy Serkis. Estrenadas en 2012: "El hobbit: Un viaje inesperado", 2013 "El hobbit: La desolación de Smaug" y 2014 "El hobbit: La batalla de los cinco ejércitos".





</doc>
<doc id="2723" url="https://es.wikipedia.org/wiki?curid=2723" title="Tecnología">
Tecnología

La tecnología (del griego "τέχνη" [téchnē], 'arte', 'oficio' y -"λογία" [-logía], 'tratado', 'estudio') es la aplicación de la ciencia a la resolución de problemas concretos. Constituye un conjunto de conocimientos científicamente ordenados, que permiten diseñar y crear bienes o servicios que facilitan la adaptación al medio ambiente, así como la satisfacción de las necesidades individuales esenciales y las aspiraciones de la humanidad.

Aunque hay muchas tecnologías muy diferentes entre sí, es frecuente usar el término "tecnología" en singular para referirse al conjunto de todas, o también a una de ellas en particular. La palabra "tecnología" también se puede referir a la disciplina teórica que estudia los saberes comunes a todas las tecnologías, y en algunos contextos, a la educación tecnológica, la disciplina escolar abocada a la familiarización con las tecnologías más importantes.

La actividad tecnológica influye en el progreso social y económico, pero si su aplicación es meramente comercial, puede orientarse a satisfacer los deseos de los más prósperos (consumismo) y no a resolver las necesidades esenciales de los más necesitados. Este enfoque puede incentivar un uso no sostenible del medio ambiente. Ciertas tecnologías humanas, por su uso intensivo, directo o indirecto, de la biosfera, son causa principal del creciente agotamiento y degradación de los recursos naturales del planeta.

Sin embargo, la tecnología también puede ser usada para proteger el medio ambiente, buscando soluciones innovadoras y eficientes para resolver de forma sostenible las crecientes necesidades de la sociedad, sin provocar un agotamiento o degradación de los recursos materiales y energéticos del planeta o aumentar las desigualdades sociales. Ciertas tecnologías humanas han llevado a un avance descomunal en los estándares y calidad de vida de miles de millones de personas en el planeta, logrando simultáneamente una mejor conservación del medio ambiente.

La tecnología engloba a todo conjunto de acciones sistemáticas cuyo destino es la transformación de las cosas, es decir, su finalidad es saber hacer y saber por qué se hace.

Actualmente hay una era tecnológica, etapa histórica dominada por la producción de bienes y por su comercialización, en la que el factor energía tiene un papel primordial. Toda la actividad científico-técnica gravita permanentemente sobre el bienestar humano, sobre el progreso social y económico de los pueblos y sobre el medio ambiente donde se manifiesta la actividad industrial.

En la prehistoria, las tecnologías han sido usadas para satisfacer necesidades esenciales (alimentación, vestimenta, vivienda, protección personal, relación social, comprensión del mundo natural y social), y en la historia también para obtener placeres corporales y estéticos (deportes, música, hedonismo en todas sus formas) y como medios para satisfacer deseos (simbolización de estatus, fabricación de armas y toda la gama de medios artificiales usados para persuadir y dominar a las personas).

La tecnología aporta grandes beneficios a la humanidad, su papel principal es crear mejores herramientas útiles para simplificar el ahorro de tiempo y esfuerzo de trabajo. La tecnología juega un papel principal en nuestro entorno social ya que gracias a ella podemos comunicarnos de forma inmediata gracias a la telefonía celular.

Después de un tiempo, las características novedosas de los productos tecnológicos son copiadas por otras marcas y dejan de ser un buen argumento de venta. Toman entonces gran importancia las creencias del consumidor sobre otras características independientes de su función principal, como las estéticas y simbólicas.

Más allá de la indispensable adecuación entre forma y función técnica, se busca la belleza a través de las formas, colores y texturas. Entre dos productos de iguales prestaciones técnicas y precios, cualquier usuario elegirá seguramente al que encuentre más bello. A veces, caso de las prendas de vestir, la belleza puede primar sobre las consideraciones prácticas. Frecuentemente compramos ropa "bonita" aunque sepamos que sus ocultos detalles de confección no son óptimos, o que su duración será breve debido a los materiales usados. Las ropas son el rubro tecnológico de máxima venta en el planeta porque son la "cara" que mostramos a las demás personas y condicionan la manera en que nos relacionamos con ellas.

Cuando la función principal de los objetos tecnológicos es la simbólica, no satisfacen las necesidades básicas de las personas y se convierten en medios para establecer estatus social y relaciones de poder.

Las joyas hechas de metales y piedras preciosas no impactan tanto por su belleza (muchas veces comparable al de una imitación barata) como por ser claros indicadores de la riqueza de sus dueños. Las ropas costosas de "primera marca" han sido tradicionalmente indicadores del estatus social de sus portadores. En la América colonial, por ejemplo, se castigaba con azotes al esclavo o liberto africano que usaba ropas españolas por "pretender ser lo que no es".

El caso más destacado y frecuente de objetos tecnológicos fabricados por su función simbólica es el de los grandes edificios: catedrales, palacios, rascacielos gigantes. Están diseñados para empequeñecer a los que están en su interior (caso de los amplios atrios y altísimos techos de las catedrales), deslumbrar con exhibiciones de lujo (caso de los palacios), infundir asombro y humildad (caso de los grandes rascacielos). No es casual que los terroristas del 11 de septiembre de 2001 eligieran como blanco principal de sus ataques a las Torres Gemelas de Nueva York, sede de la Organización Mundial del Comercio y símbolo del principal centro del poderío económico estadounidense.

El Programa Apolo fue lanzado por el Presidente John F. Kennedy en el clímax de la Guerra Fría, cuando Estados Unidos estaba aparentemente perdiendo la carrera espacial frente a los rusos, para demostrar al mundo la inteligencia, riqueza, poderío y capacidad tecnológica de los Estados Unidos. Con las pirámides de Egipto, es el más costoso ejemplo del uso simbólico de las tecnologías.

Las tecnologías usan, en general, métodos diferentes del científico, aunque la experimentación es también usado por las ciencias. Los métodos difieren según se trate de tecnologías de producción artesanal o industrial de artefactos, de prestación de servicios, de realización u organización de tareas de cualquier tipo.

Un método común a todas las tecnologías de fabricación es el uso de herramientas e instrumentos para la construcción de artefactos. Las tecnologías de prestación de servicios, como el sistema de suministro eléctrico hacen uso de instalaciones complejas a cargo de personal especializado.

Los principales medios para la fabricación de artefactos son la energía y la información. La energía permite dar a los materiales la forma, ubicación y composición que están descritas por la información. Las primeras herramientas, como los martillos de piedra y las agujas de hueso, sólo facilitaban y dirigían la aplicación de la fuerza, por parte de las personas, usando los principios de las máquinas simples. El uso del fuego, que modifica la composición de los alimentos haciéndolos más fácilmente digeribles, proporciona iluminación haciendo posible la sociabilidad más allá de los horarios diurnos, brinda calefacción y mantiene a raya a alimañas y animales feroces, modificó tanto la apariencia como los hábitos humanos.

Las herramientas más elaboradas incorporan información en su funcionamiento, como las pinzas pelacables que permiten cortar la vaina a la profundidad apropiada para arrancarla con facilidad sin dañar el alma metálica. El término «instrumento», en cambio, está más directamente asociado a las tareas de precisión, como en instrumental quirúrgico, y de recolección de información, como en instrumentación electrónica y en instrumentos de medición, de navegación náutica y de navegación aérea.

Las máquinas herramientas son combinaciones complejas de varias herramientas gobernadas (actualmente, muchas mediante computadoras) por información obtenida desde instrumentos, también incorporados en ellas.

Aunque con grandes variantes de detalle según el objeto, su principio de funcionamiento y los materiales usados en su construcción, las siguientes son las etapas comunes en la invención de un artefacto novedoso:

Según el divulgador científico Asimov: 
Guilford, destacado estudioso de la psicología de la inteligencia, identifica como las principales destrezas de un inventor las incluidas en lo que denomina "aptitudes de producción divergente". La creatividad, facultad intelectual asociada a todas las producciones originales, ha sido discutida por de Bono, quien la denomina "pensamiento lateral". Aunque más orientado a las producciones intelectuales, el más profundo estudio sobre la resolución de problemas cognitivos es hecho por Newell y Simon, en el celebérrimo libro "Human problem solving".

Muchas veces la palabra tecnología se aplica a la informática, la micro-eléctrica, el láser o a las actividades especiales, que son duras. Sin embargo, la mayoría de las definiciones que hemos visto también permiten e incluyen a otras, a las que se suele denominar blandas.

Las tecnologías blandas –en las que su producto no es un objeto tangible– pretenden mejorar el funcionamiento de las instituciones u organizaciones para el cumplimiento de sus objetivos. Dichas organizaciones pueden ser empresas industriales, comerciales o de servicio institucional, como o sin fines de lucro, etc. Entre las ramas de la tecnología llamadas blandas se destacan la educación (en lo que respecta al proceso de enseñanza), la organización, la administración, la contabilidad y las operaciones, la logística de producción, el "marketing" y la estadística, la psicología de las relaciones humanas y del trabajo, y el desarrollo de "software".

Se suele llamar duras aquellas tecnologías que se basan en conocimiento de las ciencias duras, como la física o la química. Mientras que las otras se fundamentan en ciencias blandas, como la sociología, la economía, o la administración.

Se considera que una tecnología es apropiada cuando tiene efectos beneficiosos sobre las personas y el medio ambiente. Aunque el tema es hoy (y probablemente seguirá siéndolo por mucho tiempo) objeto de intenso debate, hay acuerdo bastante amplio sobre las principales características que una tecnología debe tener para ser social y ambientalmente apropiada:

Los conceptos tecnologías apropiadas y tecnologías de punta son completamente diferentes. Las tecnologías de punta, término publicitario que enfatiza la innovación, son usualmente tecnologías complejas que hacen uso de muchas otras tecnologías más simples. Las tecnologías apropiadas frecuentemente, aunque no siempre, usan saberes propios de la cultura (generalmente artesanales) y materias primas fácilmente obtenibles en el ambiente natural donde se aplican. Algunos autores acuñaron el término tecnologías intermedias para designar a las tecnologías que comparten características de las apropiadas y de las industriales.


Las nuevas tecnologías son nuevas porque, en lo sustancial, han aparecido –y, sobre todo, se han perfeccionado, difundido y asimilado– después de la Segunda Guerra Mundial. Desde entonces su desarrollo se ha caracterizado por una fuerte aceleración; sus consecuencias son de una magnitud y trascendencia que no tenían antecedentes.

Si recorremos listas de nuevas tecnologías (NT) preparadas en Singapur, México, Tokio, Boston o Buenos Aires, podemos sorprendernos de que algunas no tengan más de tres líneas, mientras que otras cubren varias páginas. Pero, si estudiamos estos listados, veremos que –más allá del detalle o de sus diferentes objetivos– la mayoría coincide en destacar tres NT: las biotecnologías (BT), las de los nuevos materiales (NM) y las tecnologías de la información (TI).

Esta síntesis deja de lado otras NT –como algunas ambientales, las energéticas o las espaciales– pero agrupa a las de mayor difusión y en las que se manifiestan con mayor claridad los efectos que más nos importan.

Las NT se alimenta de producción científica más avanzada, a la que se suele definir como la que constituye la frontera del conocimiento. Por eso también se habla de tecnologías de punta o, en inglés, "hot technologies" (tecnologías calientes).

En algunos países se destaca la importancia estratégica de estas tecnologías: se sostiene que si no se las domina será imposible, en el medio y largo plazo, dominar las manufacturas de producto que se aseguren una posición relevante en la competencia económica y comercial internacional. Por eso, se las suele denominar tecnologías estratégicas.

Las tecnologías, aunque no son objetos específicos de estudio de la Economía, han sido a lo largo de toda la historia, y lo son aún actualmente, parte imprescindible de los procesos económicos, es decir, de la producción e intercambio de cualquier tipo de bienes y servicios.

Desde el punto de vista de los productores de bienes y de los prestadores de servicios, las tecnologías son un medio indispensable para obtener renta.

Desde el punto de vista de los consumidores, las tecnologías les permiten obtener mejores bienes y servicios, usualmente (pero no siempre) más baratos que los equivalentes del pasado.
Desde el punto de vista de los trabajadores, las tecnologías han disminuido los puestos de trabajo al reemplazar crecientemente a los operarios por máquinas.

La mayoría de las teorías económicas da por sentada la disponibilidad de las tecnologías. Schumpeter es uno de los pocos economistas que asignó a las tecnologías un rol central en los fenómenos económicos. En sus obras señala que los modelos clásicos de la economía no pueden explicar los ciclos periódicos de expansión y depresión, como los de Kondrátiev, que son la regla más que la excepción. El origen de estos ciclos, según Schumpeter, es la aparición de innovaciones tecnológicas significativas (como la introducción de la iluminación eléctrica domiciliaria por Edison o la del automóvil económico por Ford) que generan una fase de expansión económica. La posterior saturación del mercado y la aparición de empresarios competidores cuando desaparece el monopolio temporario que da la innovación, conducen a la siguiente fase de depresión. El término "empresario schumpeteriano" es hoy corrientemente usado para designar a los empresarios innovadores que hacen crecer su industria gracias a su creatividad, capacidad organizativa y mejoras en la eficiencia.

La producción de bienes requiere la recolección, fabricación o generación de todos sus insumos. La obtención de la materia prima inorgánica requiere las tecnologías mineras. La materia prima orgánica (alimentos, fibras textiles...) requiere de tecnologías agrícolas y ganaderas. Para obtener los productos finales, la materia prima debe ser procesada en instalaciones industriales de muy variado tamaño y tipo, donde se ponen en juego toda clase de tecnologías, incluida la imprescindible generación de energía.

Hasta los servicios personales requieren de las tecnologías para su buena prestación. Las ropas de trabajo, los útiles, los edificios donde se trabaja, los medios de comunicación y registro de información son productos tecnológicos. Servicios esenciales como la provisión de agua potable, tecnologías sanitarias, electricidad, eliminación de residuos, barrido y limpieza de calles, mantenimiento de carreteras, teléfonos, gas natural, radio, televisión, etc. no podrían brindarse sin el uso intensivo y extensivo de múltiples tecnologías.

Las tecnologías de las telecomunicaciones, en particular, han experimentado enormes progresos a partir del desarrollo y puesta en órbita de los primeros satélites de comunicaciones; del aumento de velocidad y memoria, y la disminución de tamaño y coste de las computadoras; de la miniaturización de circuitos electrónicos (circuito integrados); de la invención de los teléfonos celulares; etc. Todo ello permite comunicaciones casi instantáneas entre dos puntos cualesquiera del planeta, aunque la mayor parte de la población todavía no tiene acceso a ellas.

El comercio moderno, medio principal de intercambio de mercancías (productos tecnológicos), no podría llevarse a cabo sin las tecnologías del transporte fluvial, marítimo, terrestre y aéreo. Estas tecnologías incluyen tanto los medios de transporte (barcos, automotores, aviones, trenes, etc.), como también las vías de transporte y todas las instalaciones y servicios necesarios para su eficaz realización y eficiente uso: puertos, grúas de carga y descarga, carreteras, puentes, aeródromos, radares, combustibles, etc. El valor de los fletes, consecuencia directa de la eficiencia de las tecnologías de transporte usadas, ha sido desde tiempos remotos y sigue siendo hoy uno de los principales condicionantes del comercio.

Un país con grandes recursos naturales será pobre si no tiene las tecnologías necesarias para su ventajosa explotación, lo que requiere una enorme gama de tecnologías de infraestructura y servicios esenciales. Asimismo, un país con grandes recursos naturales bien explotados tendrá una población pobre si la distribución de ingresos no permite a ésta un acceso adecuado a las tecnologías imprescindibles para la satisfacción de sus necesidades básicas. En la actual economía capitalista, el único bien de cambio que tiene la mayoría de las personas para la adquisición de los productos y servicios necesarios para su supervivencia es su trabajo. La disponibilidad de trabajo, condicionada por las tecnologías, es hoy una necesidad humana esencial.

Si bien las técnicas y tecnologías también son parte esencial del trabajo artesanal, el trabajo fabril introdujo variantes tanto desde el punto de vista del tipo y propiedad de los medios de producción, como de la organización y realización del trabajo de producción. El alto costo de las máquinas usadas en los procesos de fabricación masiva, origen del capitalismo, tuvo como consecuencia que el trabajador perdiera la propiedad, y por ende el control, de los medios de producción de los productos que fabricaba. Perdió también el control de su modo de trabajar, de lo que es máximo exponente el taylorismo.

Según Frederick W. Taylor, la organización del trabajo fabril debía eliminar tanto los movimientos inútiles de los trabajadores —por ser consumo innecesario de energía y de tiempo— como los tiempos muertos —aquellos en que el obrero estaba ocioso. Esta "organización científica del trabajo", como se la llamó en su época, disminuía la incidencia de la mano de obra en el costo de las manufacturas industriales, aumentando su productividad. Aunque la idea parecía razonable, no tenía en cuenta las necesidades de los obreros y fue llevada a límites extremos por los empresarios industriales. La reducción de las tareas a movimientos lo más sencillos posibles se usó para disminuir las destrezas necesarias para el trabajo, transferidas a máquinas, reduciendo en consecuencia los salarios y aumentando la inversión de capital y lo que Karl Marx denominó la plusvalía. Este exceso de especialización hizo que el obrero perdiera la satisfacción de su trabajo, ya que la mayoría de ellos nunca veía el producto terminado. Asimismo, llevada al extremo, la repetición monótona de movimientos generaba distracción, accidentes, mayor ausentismo laboral y pérdida de calidad del trabajo. Las tendencias contemporáneas, una de cuyas expresiones es el toyotismo, son de favorecer la iniciativa personal y la participación en etapas variadas del proceso productivo (flexibilización laboral), con el consiguiente aumento de satisfacción, rendimiento y compromiso personal en la tarea.

Henry Ford, el primer fabricante de automóviles que puso sus precios al alcance de un obrero calificado, logró reducir sus costos de producción gracias a una rigurosa organización del trabajo industrial. Su herramienta principal fue la cadena de montaje que reemplazó el desplazamiento del obrero en busca de las piezas al desplazamiento de éstas hasta el puesto fijo del obrero. La disminución del costo del producto se hizo a costa de la transformación del trabajo industrial en una sencilla tarea repetitiva, que resultaba agotadora por su ritmo indeclinable y su monotonía. La metodología fue satirizada por el actor y director inglés Charles Chaplin en su clásico film Tiempos modernos y hoy estas tareas son realizadas por robots industriales.

La técnica de producción en serie de grandes cantidades de productos idénticos para disminuir su precio, está perdiendo gradualmente validez a medida que las maquinarias industriales son crecientemente controladas por computadoras, ellas permiten variar con bajo costo las características de los productos en la cadena de producción. Este es, por ejemplo, el caso del corte de prendas de vestir, aunque siguen siendo mayoritariamente cosidas por costureras con la ayuda de máquinas de coser individuales, en puestos fijos de trabajo.

El toyotismo, cuyo nombre proviene de la fábrica de automóviles Toyota, su creadora, modifica las características negativas del fordismo. Se basa en la flexibilidad laboral, el fomento del trabajo en equipo y la participación del obrero en las decisiones productivas. Desde el punto de vista de los insumos, disminuye el costo de mantenimiento de inventarios ociosos mediante el sistema "just in time", donde los componentes son provistos en el momento en que se necesitan para la fabricación. Aunque mantiene la producción en cadena, reemplaza las tareas repetitivas más agobiantes, como la soldadura de chasis, con robots industriales.

Uno de los instrumentos de que dispone la Economía para la detección de los puestos de trabajos eliminados o generados por las innovaciones tecnológicas es la matriz insumo-producto (en inglés, "input-output matrix") desarrollada por el economista Wassily Leontief, cuyo uso por los gobiernos recién empieza a difundirse. La tendencia histórica es la disminución de los puestos de trabajo en los sectores económicos primarios ( agricultura, ganadería, pesca, silvicultura) y secundarios (minería, industria, sector energético y construcción) y su aumento en los terciarios (transporte, comunicaciones, servicios, comercio, turismo, educación, finanzas, administración, sanidad). Esto plantea la necesidad de medidas rápidas de los gobiernos en reubicación de mano de obra, con la previa e indispensable capacitación laboral.

La mayoría de los productos tecnológicos se hacen con fines de lucro y su publicidad es crucial para su exitosa comercialización. La publicidad —que usa recursos tecnológicos como la imprenta, la radio y la televisión— es el principal medio por el que los fabricantes de bienes y los proveedores de servicios dan a conocer sus productos a los consumidores potenciales.

Idealmente la función técnica de la publicidad es la descripción de las propiedades del producto, para que los interesados puedan conocer cuan bien satisfará sus necesidades prácticas y si su costo está o no a su alcance. Esta función práctica se pone claramente de manifiesto sólo en la publicidad de productos innovadores cuyas características es imprescindible dar a conocer para poder venderlos. Sin embargo, usualmente no se informa al usuario de la duración estimada de los artefactos o el tiempo de mantenimiento y los costos secundarios del uso de los servicios, factores cruciales para una elección racional entre alternativas similares. No cumplen su función técnica, en particular, las publicidades de sustancias que proporcionan alguna forma de placer, como los cigarrillos y el vino cuyo consumo prolongado o excesivo acarrea riesgos variados. En varios países, como Estados Unidos y Uruguay, el alto costo que causan en tecnologías médicas hizo que se obligara a advertir en sus envases los riesgos que acarrea el consumo del producto. Sin embargo, aunque lleven la advertencia en letra chica, estos productos nunca mencionan su función técnica de cambiar la percepción de la realidad, centrando sus mensajes en asociar el consumo sólo con el placer, el éxito y el prestigio.

La elección, desarrollo y uso de tecnologías puede tener impactos muy variados en todos los órdenes del quehacer humano y sobre la naturaleza. Uno de los primeros investigadores del tema fue McLuhan, quien planteó las siguientes cuatro preguntas a contestar sobre cada tecnología particular:
Este cuestionario puede ampliarse para ayudar a identificar mejor los impactos, positivos o negativos, de cada actividad tecnológica tanto sobre las personas como sobre su cultura, su sociedad y el medio ambiente:

Cada cultura distribuye de modo diferente la realización de las funciones y el usufructo de sus beneficios. Como la introducción de nuevas tecnologías modifica y reemplaza funciones humanas, cuando los cambios son suficientemente generalizados puede modificar también las relaciones humanas, generando un nuevo orden social. Las tecnologías no son independientes de la cultura, integran con ella un sistema socio-técnico inseparable. Las tecnologías disponibles en una cultura condicionan su forma de organización, así como la cosmovisión de una cultura condiciona las tecnologías que está dispuesta a usar.

En su libro "Los orígenes de la civilización" el historiado Vere Gordon Childe ha desarrollado detalladamente la estrecha vinculación entre la evolución tecnológica y la social de las culturas occidentales, desde sus orígenes prehistóricos. Marshall McLuhan ha hecho lo propio para la época contemporánea en el campo más restringido de las tecnologías de las telecomunicaciones.

Desde tiempos prehistóricos, el hombre ha utilizado sus conocimientos para fabricar herramientas y máquinas para servir a sus propósitos, desde la rueda al ordenador. Algunos ahora alaban la tecnología como el fundamento de toda prosperidad, y creen que debieran imponerse pocas restricciones a su desarrollo. Otros la condenan como la causa de masivo daño al medio ambiente, y hacen un llamado a la imposición de controles estrictos. Pero la verdad es que es ambas cosas, y ninguna de las dos. La tecnología ha ayudado a traer riqueza a gran parte del mundo, mas también ha sido el instrumento de mucho del daño ocasionado al planeta y a la vida sobre él. Pero en sí misma es neutral: por bien o por mal, sus efectos dependen del uso que nosotros hacemos de ella.

Además del creciente reemplazo de los ambientes naturales (cuya preservación en casos particularmente deseables ha obligado a la creación de parques y reservas naturales), la extracción de ellos de materiales o su contaminación por el uso humano, está generando problemas de difícil reversión. Cuando esta extracción o contaminación excede la capacidad natural de reposición o regeneración, las consecuencias pueden ser muy graves. Son ejemplos:

Se pueden mitigar los efectos que las tecnologías producen sobre el medio ambiente estudiando los impactos ambientales que tendrá una obra antes de su ejecución, sea ésta la construcción de un caminito en la ladera de una montaña o la instalación de una gran fábrica de papel a la vera de un río. En muchos países estos estudios son obligatorios y deben tomarse recaudos para minimizar los impactos negativos (rara vez pueden eliminarse por completo) sobre el ambiente natural y maximizar (si existen) los impactos positivos (caso de obras para la prevención de aludes o inundaciones).

Para eliminar completamente los impactos ambientales negativos no debe tomarse de la naturaleza o incorporar a ella más de los que es capaz de reponer, o eliminar por sí misma. Por ejemplo, si se tala un árbol se debe plantar al menos uno; si se arrojan residuos orgánicos a un río, la cantidad no debe exceder su capacidad natural de degradación. Esto implica un costo adicional que debe ser provisto por la sociedad, transformando los que actualmente son costos externos de las actividades humanas (es decir, costos que no paga el causante, por ejemplo los industriales, sino otras personas) en costos internos de las actividades responsables del impacto negativo. De lo contrario se generan problemas que deberán ser resueltos por nuestros descendientes, con el grave riesgo de que en el transcurso del tiempo se transformen en problemas insolubles.

El concepto de desarrollo sustentable o sostenible tiene metas más modestas que el probablemente inalcanzable impacto ambiental nulo. Su expectativa es permitir satisfacer las necesidades básicas, no suntuarias, de las generaciones presentes sin afectar de manera irreversible la capacidad de las generaciones futuras de hacer lo propio. Además del uso moderado y racional de los recursos naturales, esto requiere el uso de tecnologías específicamente diseñadas para la conservación y protección del medio ambiente.

A pesar de lo que afirmaban los luditas, y como el propio Marx señalara refiriéndose específicamente a las maquinarias industriales, las tecnologías no son ni buenas ni malas. Los juicios éticos no son aplicables a la tecnología, sino al uso que se hace de ella: la tecnología puede utilizarse para fabricar un cohete y bombardear un país, o para enviar comida a una zona marcada por la hambruna. Cuando la tecnología está bajo el dominio del lucro, se utiliza principalmente para el beneficio monetario, lo cual puede generar prejuicios subjetivos hacia la tecnología en sí misma y su función.

Cuando el lucro es la finalidad principal de las actividades tecnológicas, caso ampliamente mayoritario, el resultado inevitable es considerar a las personas como mercancía e impedir que la prioridad sea el beneficio humano y medioambiental, dando lugar a una alta ineficiencia y negligencia medioambiental.

Cuando hay seres vivos involucrados (animales de laboratorio y personas), caso de las tecnologías médicas, la experimentación tecnológica tiene restricciones éticas inexistentes para la materia inanimada.

Las consideraciones morales rara vez entran en juego para las tecnologías militares, y aunque existen acuerdos internacionales limitadores de las acciones admisibles para la guerra, como la Convención de Ginebra, estos acuerdos son frecuentemente violados por los países con argumentos de supervivencia y hasta de mera seguridad.

Los artefactos han inundado todos los ámbitos de la vida: el acceso a la información, las comunicaciones, el comercio, la banca, las relaciones con las administraciones públicas, la educación, etc. Pero no todos los individuos tienen acceso en igualdad de condiciones a estas prestaciones, por lo que, si se hiciera un estudio de caso aplicando el modelo SCOT (acrónimo en inglés de Construcción Social de la Tecnología), se debería definir dentro de los grupos sociales de relevancia (GSR) al conjunto de posibles usuarios de artefactos que posean alguna discapacidad visual (ceguera o discapacidad visual grave según se establece legalmente en la escala de Wecker).

El estudio y análisis del impacto que las tecnologías tienen sobre este GSR se conoce con el nombre de tiflotecnología (del griego "tiflos" = ciego). Los resultados obtenidos de este estudio se aplican a los artefactos para que estos puedan ser utilizados por personas pertenecientes a este colectivo. Con ello, se consigue que la accesibilidad y la usabilidad sean universales.

La necesidad de la universalización del acceso a la información se basa en la premisa de que la sociedad de la información y del conocimiento tiende a excluir a aquellos grupos o individuos que no utilizan habitualmente dichas tecnologías, por lo que pueden ser considerados como analfabetos digitales, creándose, de esta manera, una nueva brecha digital.

Salvar esta brecha digital pasa por aceptar la existencia de una tecnología general y otra específica y que ambas circulen paralelamente de tal manera que, a la hora de diseñar un nuevo producto, este contenga un conjunto de estándares que permitan la accesibilidad universal y la usabilidad del artefacto.

En el campo de la discapacidad visual, sobre todo en el ámbito de la informática, se han alcanzado algunas metas que parecían inalcanzables. Así, no nos ha de sorprender que una persona ciega pueda acceder a las páginas Web de la prensa, artículos académicos, blogs, etc., a través de un ordenador de sobre mesa, un teléfono inteligente o una "tablet"; asimismo, no ha de extrañar que un usuario ciego pueda retirar un libro de cualquier biblioteca para leerlo en su casa gracias al "software" de reconocimiento de texto que permite transformar lo escrito en voz.

Todos los avances en materia de accesibilidad universal y usabilidad general han generado una serie de productos tiflotécnicos tales como el "software" magnificador de textos (ZoomText, "software" para el reconocimiento de pantalla ( JAWS for Windows), "software" de lectura ( Open Book), sistemas de grabación y reproducción de texto accesible ( DAISY), sistemas de audio descripción para programas de televisión, cine y documentales ( AUDESC), lupas televisión, etc., que permiten al usuario ciego o deficiente visual grave integrarse social y laboralmente.

Desde diferentes posiciones ideológicas, se han realizado críticas a la tecnología de forma global o parcial. Estas críticas consideran que o bien ciertas tecnologías suponen una amenaza, un riesgo o un mal de algún tipo, independientemente del uso que se las dé, o bien el conjunto de las tecnologías actuales suponen de manera inherente un mal. Entre las primeras, destacan aquellas críticas que se oponen a la tecnología nuclear, aquellas que se oponen a la posesión de armas de fuego y la argumentación que Francis Fukuyama realiza en su libro "El fin del hombre. Consecuencias de la revolución biotecnológica", la cual se centra en los aspectos negativos de la biotecnología para el ser humano. Entre las segundas, destacan las obras de Jacques Ellul dedicadas al estudio de la ""Technique"", en especial "La edad de la técnica", el manifiesto "La sociedad industrial y su futuro" y el libro de Jerry Mander "En ausencia de lo sagrado. El fracaso de la tecnología y la supervivencia de las naciones indias". Este último autor expone que "en el actual clima de culto tecnológico está mal visto hablar contra la tecnología. A la menor crítica te expones a que te llamen 'ludita', con lo que se pretende equiparar oposición a la tecnología y estupidez". 
La idea de la neutralidad de la tecnología también es discutida por muchos de estos críticos. Así, Nicolás Martín Sosa defendía que "la tecnología, digámoslo una vez más, no es neutra; en toda sociedad organizada induce un conjunto de conceptos, de modelos de relaciones y de poderes que moldean nuestra forma de vivir y de pensar". Mander sostenía que "la idea de que la tecnología es neutral no es neutral en sí misma, puesto que nos impide ver hacia dónde nos dirigimos y favorece directamente a los promotores de la vía tecnológica centralizada".

Los estudios de CTS (Ciéncia, Tecnología y Sociedad) tienen como claro objetivo analizar la relación entre el desarrollo de la ciencia y la tecnología con los problemas de nuestra sociedad. La investigación en CTS concluye que el desarrollo de la ciencia y la tecnología no se puede entender al margen de condicionantes de tipo político, social, económico o cultural.

En este sentido, cabe destacar que el valor de la ciencia y la tecnología para la educación de los ciudadanos es algo que hoy no se discute. Tanto es así, que en la actualidad la educación en valores no es menos importante para el desarrollo del individuo que la adquisición de saberes y destrezas. Ciencia, tecnología y valores son, por tanto, elementos básicos de la propia definición de educación en nuestros tiempos.

En una nota publicada en el diario "Clarín", Daniel Filmus afirma: «una educación que forme ciudadanos participativos y solidarios, que utilicen críticamente las nuevas tecnologías, ayudará a la construcción de una sociedad más justa, humana y sin exclusiones».

La tecnología es conocimiento aplicado socialmente y los valores y las creencias de esa sociedad son los que influyen en los efectos de esa tecnología (Westby & Atencio, 2002).

De acuerdo a Shanker (1998), la ciencia y la tecnología son la base del poder, la clave de la prosperidad, simultáneamente son un instrumento culturalmente poderoso que disuelve no solo la resistencia física sino las actitudes de vida. La sociedad se transforma y se adapta a los cambios en la tecnología.

Y este componente social de la ciencia i tecnología es el que desarrollaron i con el modelo SCOT (). El modelo SCOT representa la aproximación constructivista social en los actuales estudios sociales de la tecnología.

Un punto esencial en el planteamiento del modelo SCOT es la noción de que los diferentes grupos sociales relevantes (GSR) asociados con el desarrollo de un artefacto tecnológico, compartían un significado unánime del artefacto técnico y pretendían hacer prevalecer su concepción. El otro punto esencial es el de la flexibilidad interpretativa, el proceso de cierre mediante el cual desaparece la flexibilidad de un artefacto.

En este sentido, cabe la posibilidad que también exista una visión influida por el género, como se darían en casos estudiados como el de la bicicleta o el de la lavadora.

En cuanto al caso de la lavadora, aunque «la concepción y el desarrollo de la tecnología aparecen teóricamente de forma asexuada o al margen de las relaciones sociales de sexo», su concepción tenía un claro destinatario, y eran las mujeres. Cabe decir, sin embargo, que la lavadora, lejos de ser un artefacto de emancipación y liberador se convirtió en una subordinación para ellas, muy lejos de la liberación que representaba la bicicleta para y .

En este estudio se destaca la total ausencia de mujeres en el proceso de diseño y en los puestos de responsabilidad técnica. Sin embargo, las investigadoras concluyeron que las operarias debían ser mujeres porque las usuarias potenciales de estos aparatos eran mujeres, en tanto que eran las amas de casa. El problema radica en que la mujer no dispone de los conocimientos técnicos adecuados, por lo que los hombres siguen manteniendo el control técnico del objeto.

La relación entre la altura de la mujer y el tamaño de los mandos de los aparatos es algo a tener en cuenta, ya que deja entrever que éstos han sido concebidos para hombres.

Otro hecho destacable es la forma de carga de la lavadora. La mayor parte de las máquinas en España son de apertura frontal ya que las de carga superior suponen un montaje más costoso. Además, Alemán relaciona la carga frontal con el hecho de que la mujer ya está acostumbrada a una posición curvada dada su condición de ama de casa.

Destacar también la utilización eficaz y eficiente de la lavadora por parte de la mujer, que ligada a su cultura doméstica, hace que la mujer siga siendo la responsable de organizar las coladas a la unidad familiar. En este sentido «el nuevo electrodoméstico aparece, por tanto, como un elemento de conservadurismo social y no como un factor de emancipación o de transformación progresiva de las relaciones sociales de sexo».

Finalmente, destacar que la concepción de la lavadora, y sobre todo, su uso, «confirma a la mujer como principal actora en este tipo de funciones». Por este motivo no es raro que las mujeres «sienten un cierto malestar hacia la tecnología, o se desentienden de ella, ya que en lugar de ser innovaciones liberadoras para las mujeres, confirman muy frecuentemente su subordinación».

La relación entre género y tecnología se creó como respuesta a la larga marginalización de las mujeres respecto a profesiones y trabajos de orientación técnica.

La ciencia y la tecnología son fundamentales en el desarrollo económico de los países. Esta importancia creciente junto con las persistentes desigualdades entre mujeres y hombres en el ámbito tecnológico, hace que se planteen cuestiones urgentes e inevitables desde una perspectiva de género, la única finalidad es su total desaparición.

Aunque las barreras formales que impedían la participación de la mujer en la actividad tecnológica van desapareciendo con el paso del tiempo, siguen existiendo dificultades de acceso a puestos de responsabilidad y poder ligados a la escasa presencia profesional en esta área. Los motivos pueden ser de equilibrio entre el trabajo y la vida personal, los patrones y los enfoques de productividad específicos del género, los criterios de medición del rendimiento y de promoción, de motivación, de exclusión social e institucional, e incluso de identificación de lo científico y tecnológico con 'lo masculino'.

Y si la ciencia y la tecnología no están libres de la política ni por encima de ella, entonces en una sociedad caracterizada por jerarquías de género, los artefactos deben estar marcados también por el género. Dicho de otro modo, hemos llegado a ver la tecnología como algo a lo que se le ha dado forma socialmente, pero esta forma ha sido realizada por los hombres a favor de la exclusión de las mujeres. En general, la tecnología ha sido retratada como fuerza negativa, reproduciendo en lugar de transformando la división sexual del trabajo y el poder en el hogar y el trabajo.




</doc>
<doc id="2727" url="https://es.wikipedia.org/wiki?curid=2727" title="Cabecera IP">
Cabecera IP

El tamaño mínimo de la cabecera (ip_pci) es de 20 Bytes mientras que el máximo es 60 bytes.

Puede variar entre (0100) o (0110) dependiendo si se utiliza IP versión 4 (IPv4) o IP versión 6 (IPv6). Este campo describe el formato de la cabecera utilizada. En la tabla se describe la versión 4.

Longitud de la cabecera, en palabras de 32 bits. Su valor mínimo es de 5 palabras (5x32 = 160 bits, 20 bytes) para una cabecera correcta, y el máximo de 15 palabras (15x32 = 480 bits, 60 bytes).

Indica una serie de parámetros sobre la calidad de servicio deseada durante el tránsito por una red. Algunas redes ofrecen prioridades de servicios, considerando determinado tipo de paquetes "más importantes" que otros (en particular estas redes solo admiten los paquetes con prioridad alta en momentos de sobrecarga). Estos 8 bits se agrupan de la siguiente manera:


Es el tamaño total, en octetos, del datagrama, incluyendo el tamaño de la cabecera y el de los datos. El tamaño mínimo de los datagramas usados normalmente es de 576 octetos (64 de cabeceras y 512 de datos). Una máquina no debería enviar datagramas menores o mayores de ese tamaño a no ser que tenga la certeza de que van a ser aceptados por la máquina destino.

En caso de fragmentación este campo contendrá el tamaño del fragmento, no el del datagrama original.

Identificador único del datagrama. Se utilizará, en caso de que el datagrama deba ser fragmentado, para poder distinguir los fragmentos de un datagrama de los de otro. El originador del datagrama debe asegurar un valor único para la pareja origen-destino y el tipo de protocolo durante el tiempo que el datagrama pueda estar activo en la red. El valor asignado en este campo debe ir en formato de red.

Actualmente utilizado sólo para especificar valores relativos a la fragmentación de paquetes. Los 3 bits (por orden de mayor a menor peso) son:

En paquetes fragmentados indica la posición, en unidades de 64 bits, que ocupa el paquete actual dentro del datagrama original. El primer paquete de una serie de fragmentos contendrá en este campo el valor 0.

Indica el máximo número de enrutadores que un paquete puede atravesar. Cada vez que algún nodo procesa este paquete disminuye su valor en, como mínimo, una unidad. Cuando llegue a ser 0, el paquete será descartado. Típicamente toma el valor 64 o 128 en los datagramas.

Indica el protocolo de las capas superiores al que debe entregarse el paquete Vea Números de protocolo IP para comprender como interpretar este campo.

Aunque no es obligatoria la utilización de este campo, cualquier nodo debe ser capaz de interpretarlo. Puede contener un número indeterminado de opciones, que tendrán dos posibles formatos:

Se determina con un solo octeto indicando el 'Tipo de opción', el cual está dividido en 3 campos.

Un octeto para el 'Tipo de opción', otro para el 'Tamaño de opción', y uno o más octetos conformando los 'Datos de opción'.

El 'Tamaño de opción' incluye el octeto de 'Tipo de opción', el de 'Tamaño de opción' y la suma de los octetos de datos.

La siguiente tabla muestra las opciones actualmente definidas:


Utilizado para asegurar que el tamaño, en bits, de la cabecera es un múltiplo de 32. El valor usado es el 0.


</doc>
<doc id="2730" url="https://es.wikipedia.org/wiki?curid=2730" title="Tierra">
Tierra

La Tierra (del latín "Terra", deidad romana equivalente a Gea, diosa griega de la feminidad y la fecundidad) es un planeta del sistema solar que gira alrededor de su estrella —el Sol— en la tercera órbita más interna. Es el más denso y el quinto mayor de los ocho planetas del sistema solar. También es el mayor de los cuatro terrestres o rocosos.

La Tierra se formó hace aproximadamente 4550 millones de años y la vida surgió unos mil millones de años después. Es el hogar de millones de especies, incluidos los seres humanos y actualmente el único cuerpo astronómico donde se conoce la existencia de vida. La atmósfera y otras condiciones abióticas han sido alteradas significativamente por la biosfera del planeta, favoreciendo la proliferación de organismos aerobios, así como la formación de una capa de ozono que junto con el campo magnético terrestre bloquean la radiación solar dañina, permitiendo así la vida en la Tierra. Las propiedades físicas de la Tierra, la historia geológica y su órbita han permitido que la vida siga existiendo. Se estima que el planeta seguirá siendo capaz de sustentar vida durante otros 500 millones de años, ya que según las previsiones actuales, pasado ese tiempo la creciente luminosidad del Sol terminará causando la extinción de la biosfera.

La superficie terrestre o corteza está dividida en varias placas tectónicas que se deslizan sobre el magma durante periodos de varios millones de años. La superficie está cubierta por continentes e islas; estos poseen varios lagos, ríos y otras fuentes de agua, que junto con los océanos de agua salada que representan cerca del 71 % de la superficie constituyen la hidrósfera. No se conoce ningún otro planeta con este equilibrio de agua líquida, que es indispensable para cualquier tipo de vida conocida. Los polos de la Tierra están cubiertos en su mayoría de hielo sólido (indlandsis de la Antártida) o de banquisas (casquete polar ártico). El interior del planeta es geológicamente activo, con una gruesa capa de manto relativamente sólido, un núcleo externo líquido que genera un campo magnético, y un sólido núcleo interior compuesto por aproximadamente un 88 % de hierro.

La Tierra interactúa gravitatoriamente con otros objetos en el espacio, especialmente el Sol y la Luna. En la actualidad, la Tierra completa una órbita alrededor del Sol cada vez que realiza 366,26 giros sobre su eje, lo cual es equivalente a 365,26 días solares o un año sideral. El eje de rotación de la Tierra se encuentra inclinado 23,4° con respecto a la perpendicular a su plano orbital, lo que produce las variaciones estacionales en la superficie del planeta con un período de un año tropical (365,24 días solares). La Tierra posee un único satélite natural, la Luna, que comenzó a orbitar la Tierra hace 4530 millones de años; esta produce las mareas, estabiliza la inclinación del eje terrestre y reduce gradualmente la velocidad de rotación del planeta. Hace aproximadamente , durante el llamado bombardeo intenso tardío, numerosos asteroides impactaron en la Tierra, causando significativos cambios en la mayor parte de su superficie.

Tanto los minerales del planeta como los productos de la biosfera aportan recursos que se utilizan para sostener a la población humana mundial. Sus habitantes están agrupados en unos 200 estados soberanos independientes, que interactúan a través de la diplomacia, los viajes, el comercio y la acción militar. Las culturas humanas han desarrollado muchas ideas sobre el planeta, incluida la personificación de una deidad, la creencia en una Tierra plana o en la Tierra como centro del universo, y una perspectiva moderna del mundo como un entorno integrado que requiere administración.

Los científicos han podido reconstruir información detallada sobre el pasado de la Tierra. Según estos estudios el material más antiguo del sistema solar se formó hace millones de años, y en torno a unos 4550 millones de años atrás (con una incertidumbre del 1 %) se habían formado ya la Tierra y los otros planetas del sistema solar a partir de la nebulosa solar, una masa en forma de disco compuesta del polvo y gas remanente de la formación del Sol. Este proceso de formación de la Tierra a través de la acreción tuvo lugar mayoritariamente en un plazo de . La capa exterior del planeta, inicialmente fundida, se enfrió hasta formar una corteza sólida cuando el agua comenzó a acumularse en la atmósfera. La Luna se formó poco antes, hace unos 4530 millones de años.
El actual modelo consensuado sobre la formación de la Luna es la teoría del gran impacto, que postula que la Luna se creó cuando un objeto del tamaño de Marte, con cerca del 10 % de la masa de la Tierra, impactó tangencialmente contra ésta. En este modelo, parte de la masa de este cuerpo podría haberse fusionado con la Tierra, mientras otra parte habría sido expulsada al espacio, proporcionando suficiente material en órbita como para desencadenar nuevamente un proceso de aglutinamiento por fuerzas gravitatorias, y formando así la Luna.

La desgasificación de la corteza y la actividad volcánica produjeron la atmósfera primordial de la Tierra. La condensación de vapor de agua, junto con el hielo y el agua líquida aportada por los asteroides y por protoplanetas, cometas y objetos transneptunianos, produjeron los océanos. El recién formado Sol solo tenía el 70 % de su luminosidad actual: sin embargo, existen evidencias que muestran que los primitivos océanos se mantuvieron en estado líquido; una contradicción denominada la «paradoja del joven Sol débil», ya que aparentemente el agua no debería ser capaz de permanecer en ese estado líquido, sino en el sólido, debido a la poca energía solar recibida. Sin embargo, una combinación de gases de efecto invernadero y mayores niveles de actividad solar contribuyeron a elevar la temperatura de la superficie terrestre, impidiendo así que los océanos se congelaran. Hace 3500 millones de años se formó el campo magnético de la Tierra, lo que ayudó a evitar que la atmósfera fuese arrastrada por el viento solar.

Se han propuesto dos modelos para el crecimiento de los continentes: el modelo de crecimiento constante, y el modelo de crecimiento rápido en una fase temprana de la historia de la Tierra. Las investigaciones actuales sugieren que la segunda opción es más probable, con un rápido crecimiento inicial de la corteza continental, seguido de un largo período de estabilidad. En escalas de tiempo de cientos de millones de años de duración, la superficie terrestre ha estado en constante remodelación, formando y fragmentando continentes. Estos continentes se han desplazado por la superficie, combinándose en ocasiones para formar un supercontinente. Hace aproximadamente 750 millones de años (Ma), uno de los primeros supercontinentes conocidos, Rodinia, comenzó a resquebrajarse. Los continentes más tarde se recombinaron nuevamente para formar Pannotia, entre , y finalmente Pangea, que se fragmentó hace 180 Ma hasta llegar a la configuración continental actual.

La Tierra proporciona el único ejemplo conocido de un entorno que ha dado lugar a la evolución de la vida. Se presume que procesos químicos altamente energéticos produjeron una molécula auto-replicante hace alrededor de 4000 millones de años, y hace entre 3500 y 3800 millones de años existió el último antepasado común universal. El desarrollo de la fotosíntesis permitió que los seres vivos recogiesen de forma directa la energía del Sol; el oxígeno resultante acumulado en la atmósfera formó una capa de ozono (una forma de oxígeno molecular [O]) en la atmósfera superior. La incorporación de células más pequeñas dentro de las más grandes dio como resultado el desarrollo de las células complejas llamadas eucariotas. Los verdaderos organismos multicelulares se formaron cuando las células dentro de colonias se hicieron cada vez más especializadas. La vida colonizó la superficie de la Tierra en parte gracias a la absorción de la radiación ultravioleta por parte de la capa de ozono.

En la década de 1960 surgió una hipótesis que afirmaba que durante el período Neoproterozoico, desde 750 hasta los 580 Ma, se produjo una intensa glaciación en la que gran parte del planeta fue cubierto por una capa de hielo. Esta hipótesis ha sido denominada la "Glaciación global", y es de particular interés, ya que este suceso precedió a la llamada explosión del Cámbrico, en la que las formas de vida multicelulares comenzaron a proliferar.

Tras la explosión del Cámbrico, hace unos se han producido cinco extinciones en masa. De ellas, el evento más reciente ocurrió hace , cuando el impacto de un asteroide provocó la extinción de los dinosaurios no aviarios, así como de otros grandes reptiles, salvándose algunos pequeños animales como los mamíferos, que por aquel entonces eran similares a las actuales musarañas. Durante los últimos los mamíferos se diversificaron, hasta que hace varios millones de años, un animal africano con aspecto de simio conocido como el orrorin tugenensis adquirió la capacidad de mantenerse en pie. Esto le permitió utilizar herramientas y favoreció su capacidad de comunicación, proporcionando la nutrición y la estimulación necesarias para desarrollar un cerebro más grande, y permitiendo así la evolución de la raza humana. El desarrollo de la agricultura y de la civilización permitió a los humanos alterar la Tierra en un corto espacio de tiempo como no lo había hecho ninguna otra especie, afectando tanto a la naturaleza como a la diversidad y cantidad de formas de vida.

El presente patrón de edades de hielo comenzó hace alrededor de y luego se intensificó durante el Pleistoceno, hace alrededor de . Desde entonces las regiones en latitudes altas han sido objeto de repetidos ciclos de glaciación y deshielo, en ciclos de 40-100 mil años. La última glaciación continental terminó hace 10 000 años.

El futuro del planeta está estrechamente ligado al del Sol. Como resultado de la acumulación constante de helio en el núcleo del Sol, la luminosidad total de la estrella irá poco a poco en aumento. La luminosidad del Sol crecerá en un 10 % en los próximos 1,1 Ga y en un 40 % en los próximos 3,5 Ga. Los modelos climáticos indican que el aumento de la radiación podría tener consecuencias nefastas en la Tierra, incluyendo la pérdida de los océanos del planeta.

Se espera que la Tierra sea habitable por alrededor de otros a partir de este momento, aunque este período podría extenderse hasta si se elimina el nitrógeno de la atmósfera. El aumento de temperatura en la superficie terrestre acelerará el ciclo del CO inorgánico, lo que reducirá su concentración hasta niveles letalmente bajos para las plantas (10 ppm para la fotosíntesis C) dentro de aproximadamente 500 a 900 millones de años. La falta de vegetación resultará en la pérdida de oxígeno en la atmósfera, lo que provocará la extinción de la vida animal a lo largo de varios millones de años más. Después de otros mil millones de años, todas las aguas superficiales habrán desaparecido y la temperatura media global alcanzará los 70 °C. Incluso si el Sol fuese eterno y estable, el continuo enfriamiento interior de la Tierra se traduciría en una gran pérdida de CO debido a la reducción de la actividad volcánica, y el 35 % del agua de los océanos podría descender hasta el manto debido a la disminución del vapor de ventilación en las dorsales oceánicas.

El Sol, siguiendo su evolución natural, se convertirá en una gigante roja en unos 5 Ga. Los modelos predicen que el Sol se expandirá hasta unas 250 veces su tamaño actual, alcanzando un radio cercano a 1 UA (unos 150 millones de km). El destino que sufrirá la Tierra entonces no está claro. Siendo una gigante roja, el Sol perderá aproximadamente el 30 % de su masa, por lo que sin los efectos de las mareas, la Tierra se moverá a una órbita de 1,7 UA (unos 250 millones de km) del Sol cuando la estrella alcance su radio máximo. Por lo tanto se espera que el planeta escape inicialmente de ser envuelto por la tenue atmósfera exterior expandida del Sol. Aun así, cualquier forma de vida restante sería destruida por el aumento de la luminosidad del Sol (alcanzando un máximo de cerca de 5000 veces su nivel actual). Sin embargo, una simulación realizada en 2008 indica que la órbita de la Tierra decaerá debido a los efectos de marea y arrastre, ocasionando que el planeta penetre en la atmósfera estelar y se vaporice.

La Tierra es un planeta terrestre, lo que significa que es un cuerpo rocoso y no un gigante gaseoso como Júpiter. Es el más grande de los cuatro planetas terrestres del sistema solar en tamaño y masa, y también es el que tiene la mayor densidad, la mayor gravedad superficial, el campo magnético más fuerte y la rotación más rápida de los cuatro. También es el único planeta terrestre con placas tectónicas activas. El movimiento de estas placas produce que la superficie terrestre esté en constante cambio, siendo responsables de la formación de montañas, de la sismicidad y del vulcanismo.
El ciclo de estas placas también juega un papel preponderante en la regulación de la temperatura terrestre, contribuyendo al reciclaje de gases con efecto invernadero como el dióxido de carbono, por medio de la renovación permanente de los fondos oceánicos.

La forma de la Tierra es muy parecida a la de un esferoide oblato, una esfera achatada por los polos, resultando en un abultamiento alrededor del ecuador. Este abultamiento está causado por la rotación de la Tierra, y ocasiona que el diámetro en el ecuador sea 43 km más largo que el diámetro de un polo a otro. Hace aproximadamente 22 000 años la Tierra tenía una forma más esférica, la mayor parte del hemisferio norte se encontraba cubierto por hielo, y a medida que el hielo se derretía causaba una menor presión en la superficie terrestre en la que se sostenía, causando esto un tipo de «rebote». Este fenómeno siguió ocurriendo hasta mediados de los años noventa, cuando los científicos se percataron de que este proceso se había invertido, es decir, el abultamiento aumentaba. Las observaciones del satélite GRACE muestran que, al menos desde 2002, la pérdida de hielo de Groenlandia y de la Antártida ha sido la principal responsable de esta tendencia.

La topografía local se desvía de este esferoide idealizado, aunque las diferencias a escala global son muy pequeñas: la Tierra tiene una desviación de aproximadamente una parte entre 584, o el 0,17 %, desde el esferoide de referencia, que es menor que la tolerancia del 0,22 % permitida en las bolas de billar. Las mayores desviaciones locales en la superficie rocosa de la Tierra son el monte Everest (8 848 m sobre el nivel local del mar) y el abismo Challenger, al sur de la fosa de las Marianas (10 911 m bajo el nivel local del mar). Debido a la protuberancia ecuatorial, el punto terrestre más alejado del centro de la tierra es el volcán Chimborazo en Ecuador.

La circunferencia en el ecuador es de . El diámetro en el ecuador es de y en los polos de 

El diámetro medio de referencia para el esferoide es de unos , que es aproximadamente , ya que el metro se definió originalmente como la diezmillonésima parte de la distancia desde el ecuador hasta el Polo Norte por París, Francia.

La primera medición del tamaño de la Tierra fue hecha por Eratóstenes, el 240 a. C.. En esa época se aceptaba que la Tierra era esférica. Eratóstenes calculó el tamaño de la Tierra midiendo el ángulo con que alumbraba el Sol en el solsticio, tanto en Alejandría como en Siena, distante 750km. El tamaño que obtuvo fue de un diámetro de y una circunferencia de , es decir, con un error de solo el 6% respecto a los datos actuales.

Posteriormente Posidonio de Apamea repitió las mediciones en el año 100 a. C., obteniendo el dato de para la circunferencia, considerablemente más impreciso respecto a los datos actuales. Este último valor fue el que aceptó Ptolomeo, por lo que prevaleció ese valor en los siglos siguientes.

Cuando Magallanes dio la vuelta a todo el planeta en 1521, se restableció el dato calculado por Eratóstenes.

La masa de la Tierra es aproximadamente de 5,98 kg. Se compone principalmente de hierro (32,1 %), oxígeno (30,1 %), silicio (15,1 %), magnesio (13,9 %), azufre (2,9 %), níquel (1,8 %), calcio (1,5 %) y aluminio (1,4 %), con el 1,2 % restante formado por pequeñas cantidades de otros elementos. Debido a la segregación de masa, se cree que la zona del núcleo está compuesta principalmente de hierro (88,8 %), con pequeñas cantidades de níquel (5,8 %), azufre (4,5 %), y menos del 1 % formado por trazas de otros elementos.

El geoquímico F. W. Clarke (1847-1931), llamado «el padre de la geoquímica por haber determinado la composición de la corteza de la Tierra», calculó que un poco más del 47 % de la corteza terrestre se compone de oxígeno. Los componentes de las rocas más comunes de la corteza de la Tierra son casi todos los óxidos. Cloro, azufre y flúor son las únicas excepciones significativas, y su presencia total en cualquier roca es generalmente mucho menor del 1 %. Los principales óxidos son los de sílice, alúmina, hierro, cal, magnesia, potasa y sosa. La sílice actúa principalmente como un ácido, formando silicatos, y los minerales más comunes de las rocas ígneas son de esta naturaleza. A partir de un cálculo sobre la base de 1672 análisis de todo tipo de rocas, Clarke dedujo que un 99,22 % de las rocas están compuestas por 11 óxidos (véase el cuadro a la derecha). Todos los demás compuestos aparecen solo en cantidades muy pequeñas.

El interior de la Tierra, al igual que el de los otros planetas terrestres, está dividido en capas según su composición química o sus propiedades físicas (reológicas), pero, a diferencia de los otros planetas terrestres, tiene un núcleo interno y externo distintos. Su capa externa es una corteza de silicato sólido, químicamente diferenciado, bajo la cual se encuentra un manto sólido de alta viscosidad. La corteza está separada del manto por la discontinuidad de Mohorovičić, variando el espesor de la misma desde un promedio de 6 km en los océanos a entre 30 y 50 km en los continentes. La corteza y la parte superior fría y rígida del manto superior se conocen comúnmente como la litosfera, y es de la litosfera de lo que están compuestas las placas tectónicas. Debajo de la litosfera se encuentra la astenosfera, una capa de relativamente baja viscosidad sobre la que flota la litosfera. Dentro del manto, entre los 410 y 660 km bajo la superficie, se producen importantes cambios en la estructura cristalina. Estos cambios generan una zona de transición que separa la parte superior e inferior del manto. Bajo el manto se encuentra un núcleo externo líquido de viscosidad extremadamente baja, descansando sobre un núcleo interno sólido. El núcleo interno puede girar con una velocidad angular ligeramente superior que el resto del planeta, avanzando de 0,1 a 0,5° por año.

El calor interno de la Tierra proviene de una combinación del calor residual de la acreción planetaria (20 %) y el calor producido por la desintegración radiactiva (80 %). Los isótopos con mayor producción de calor en la Tierra son el potasio-40, el uranio-238, el uranio-235 y el torio-232. En el centro del planeta, la temperatura puede llegar hasta los 7000 K y la presión puede alcanzar los 360 GPa. Debido a que gran parte del calor es proporcionado por la desintegración radiactiva, los científicos creen que en la historia temprana de la Tierra, antes de que los isótopos de reducida vida media se agotaran, la producción de calor de la Tierra fue mucho mayor. Esta producción de calor extra, que hace aproximadamente 3000 millones de años era el doble que la producción actual, pudo haber incrementado los gradientes de temperatura dentro de la Tierra, incrementando la convección del manto y la tectónica de placas, permitiendo la producción de rocas ígneas como las komatitas que no se forman en la actualidad.

El promedio de pérdida de calor de la Tierra es de , que supone una pérdida global de . Una parte de la energía térmica del núcleo es transportada hacia la corteza por plumas del manto, una forma de convección que consiste en afloramientos de roca a altas temperaturas. Estas plumas pueden producir puntos calientes y coladas de basalto. La mayor parte del calor que pierde la Tierra se filtra entre las placas tectónicas, en las surgencias del manto asociadas a las dorsales oceánicas. Casi todas las pérdidas restantes se producen por conducción a través de la litosfera, principalmente en los océanos, ya que allí la corteza es mucho más delgada que en los continentes.

La mecánicamente rígida capa externa de la Tierra, la litosfera, está fragmentada en piezas llamadas placas tectónicas. Estas placas son elementos rígidos que se mueven en relación uno con otro siguiendo uno de estos tres patrones: bordes convergentes, en los que dos placas se aproximan; bordes divergentes, en los que dos placas se separan, y bordes transformantes, en los que dos placas se deslizan lateralmente entre sí. A lo largo de estos bordes de placa se producen los terremotos, la actividad volcánica, la formación de montañas y la formación de fosas oceánicas. Las placas tectónicas se deslizan sobre la parte superior de la astenosfera, la sólida pero menos viscosa sección superior del manto, que puede fluir y moverse junto con las placas, y cuyo movimiento está fuertemente asociado a los patrones de convección dentro del manto terrestre.

A medida que las placas tectónicas migran a través del planeta, el fondo oceánico se subduce bajo los bordes de las placas en los límites convergentes. Al mismo tiempo, el afloramiento de material del manto en los límites divergentes crea las dorsales oceánicas. La combinación de estos procesos recicla continuamente la corteza oceánica nuevamente en el manto. Debido a este proceso de reciclaje, la mayor parte del suelo marino tiene menos de de años de edad. La corteza oceánica más antigua se encuentra en el Pacífico Occidental, y tiene una edad estimada de unos de años. En comparación, la corteza continental más antigua registrada tiene de años de edad.

Las siete placas más grandes son la Pacífica, Norteamericana, Euroasiática, Africana Antártica, Indoaustraliana y Sudamericana. Otras placas notables son la placa Índica, la placa arábiga, la placa del Caribe, la placa de Nazca en la costa occidental de América del Sur y la placa Escocesa en el sur del océano Atlántico. La placa de Australia se fusionó con la placa de la India hace entre 50 y 55 millones de años. Las placas con movimiento más rápido son las placas oceánicas, con la placa de Cocos avanzando a una velocidad de 75 mm/año y la placa del Pacífico moviéndose 52-69 mm/año. En el otro extremo, la placa con movimiento más lento es la placa eurasiática, que avanza a una velocidad típica de aproximadamente 21 mm/año.

El relieve de la Tierra varía enormemente de un lugar a otro. Cerca del 70,8 % de la superficie está cubierta por agua, con gran parte de la plataforma continental por debajo del nivel del mar. La superficie sumergida tiene características montañosas, incluyendo un sistema de dorsales oceánicas, así como volcanes submarinos, fosas oceánicas, cañones submarinos, mesetas y llanuras abisales. El restante 29,2 % no cubierto por el agua se compone de montañas, desiertos, llanuras, mesetas y otras geomorfologías.

La superficie del planeta se moldea a lo largo de períodos de tiempo geológicos, debido a la erosión tectónica. Las características de esta superficie formada o deformada mediante la tectónica de placas están sujetas a una constante erosión a causa de las precipitaciones, los ciclos térmicos y los efectos químicos. La glaciación, la erosión costera, la acumulación de los arrecifes de coral y los grandes impactos de meteoritos también actúan para remodelar el paisaje.
La corteza continental se compone de material de menor densidad, como las rocas ígneas, el granito y la andesita. Menos común es el basalto, una densa roca volcánica que es el componente principal de los fondos oceánicos. Las rocas sedimentarias se forman por la acumulación de sedimentos compactados. Casi el 75 % de la superficie continental está cubierta por rocas sedimentarias, a pesar de que estas solo forman un 5 % de la corteza. El tercer material rocoso más abundante en la Tierra son las rocas metamórficas, creadas a partir de la transformación de tipos de roca ya existentes mediante altas presiones, altas temperaturas, o ambas. Los minerales de silicato más abundantes en la superficie de la Tierra incluyen el cuarzo, los feldespatos, el anfíbol, la mica, el piroxeno y el olivino. Los minerales de carbonato más comunes son la calcita (que se encuentra en piedra caliza) y la dolomita.

La pedosfera es la capa más externa de la Tierra. Está compuesta de tierra y está sujeta a los procesos de formación del suelo. Existe en el encuentro entre la litosfera, la atmósfera, la hidrosfera y la biosfera. Actualmente el 13,31 % del total de la superficie terrestre es tierra cultivable, y solo el 4,71 % soporta cultivos permanentes. Cerca del 40 % de la superficie emergida se utiliza actualmente como tierras de cultivo y pastizales, estimándose un total de 1,3 km² para tierras de cultivo y 3,4 km² para tierras de pastoreo.

La elevación de la superficie terrestre varía entre el punto más bajo de –418 m en el mar Muerto a una altitud máxima, estimada en 2005, de 8848 m en la cima del monte Everest. La altura media de la tierra sobre el nivel del mar es de 840 m.

El satélite ambiental Envisat de la ESA desarrolló un retrato detallado de la superficie de la Tierra. A través del proyecto GLOBCOVER se desarrolló la creación de un mapa global de la cobertura terrestre con una resolución tres veces superior a la de cualquier otro mapa por satélite hasta aquel momento. Utilizó reflectores radar con antenas de ancho sintéticas, capturando con sus sensores la radiación reflejada.

La NASA completó un nuevo mapa tridimensional, que es la topografía más precisa del planeta, elaborada durante cuatro años con los datos transmitidos por el transbordador espacial "Endeavour". Los datos analizados corresponden al 80 % de la masa terrestre. Cubre los territorios de Australia y Nueva Zelanda con detalles sin precedentes. También incluye más de mil islas de la Polinesia y la Melanesia en el Pacífico sur, así como islas del Índico y el Atlántico. Muchas de esas islas apenas se levantan unos metros sobre el nivel del mar y son muy vulnerables a los efectos de las marejadas y tormentas, por lo que su conocimiento ayudará a evitar catástrofes; los datos proporcionados por la misión del Endeavour tendrán una amplia variedad de usos, como la exploración virtual del planeta.

La abundancia de agua en la superficie de la Tierra es una característica única que distingue al "Planeta Azul" de otros en el Sistema Solar. La hidrosfera de la Tierra está compuesta fundamentalmente por océanos, pero técnicamente incluye todas las superficies de agua en el mundo, incluidos los mares interiores, lagos, ríos y aguas subterráneas hasta una profundidad de 2000 m. El lugar más profundo bajo el agua es el abismo Challenger de la fosa de las Marianas, en el océano Pacífico, con una profundidad de –10 911,4 m.

La masa de los océanos es de aproximadamente 1,35 toneladas métricas, o aproximadamente 1/4400 de la masa total de la Tierra. Los océanos cubren un área de 361,84 km² con una profundidad media de 3682,2 m, lo que resulta en un volumen estimado de 1,3324 km³. Si se nivelase toda la superficie terrestre, el agua cubriría la superficie del planeta hasta una altura de más de 2,7 km. El área total de la Tierra es de 5,1 km². Para la primera aproximación, la profundidad media sería la relación entre los dos, o de 2,7 km. Aproximadamente el 97,5 % del agua es salada, mientras que el restante 2,5 % es agua dulce. La mayor parte del agua dulce, aproximadamente el 68,7 %, se encuentra actualmente en estado de hielo.

La salinidad media de los océanos es de unos 35 gramos de sal por kilogramo de agua (35 ‰). La mayor parte de esta sal fue liberada por la actividad volcánica, o extraída de las rocas ígneas ya enfriadas. Los océanos son también un reservorio de gases atmosféricos disueltos, siendo estos esenciales para la supervivencia de muchas formas de vida acuática. El agua de los océanos tiene una influencia importante sobre el clima del planeta, actuando como un foco calórico de gran tamaño. Los cambios en la distribución de la temperatura oceánica pueden causar alteraciones climáticas, tales como la Oscilación del Sur, El Niño.

La presión atmosférica media al nivel del mar se sitúa en torno a los 101,325 kPa, con una escala de altura de aproximadamente 8,5 km. Está compuesta principalmente de un 78 % de nitrógeno y un 21 % de oxígeno, con trazas de vapor de agua, dióxido de carbono y otras moléculas gaseosas. La altura de la troposfera varía con la latitud, entre 8 km en los polos y 17 km en el ecuador, con algunas variaciones debido a la climatología y los factores estacionales.

La biosfera de la Tierra ha alterado significativamente la atmósfera. La fotosíntesis oxigénica evolucionó hace 2700 millones de años, formando principalmente la atmósfera actual de nitrógeno-oxígeno. Este cambio permitió la proliferación de los organismos aeróbicos, así como la formación de la capa de ozono que bloquea la radiación ultravioleta proveniente del Sol, permitiendo la vida fuera del agua. Otras funciones importantes de la atmósfera para la vida en la Tierra incluyen el transporte de vapor de agua, proporcionar gases útiles, quemar los meteoritos pequeños antes de que alcancen la superficie, y moderar la temperatura. Este último fenómeno se conoce como el efecto invernadero: trazas de moléculas presentes en la atmósfera capturan la energía térmica emitida desde el suelo, aumentando así la temperatura media. El dióxido de carbono, el vapor de agua, el metano y el ozono son los principales gases de efecto invernadero de la atmósfera de la Tierra. Sin este efecto de retención del calor, la temperatura superficial media sería de –18 °C y la vida probablemente no existiría.

La atmósfera terrestre no tiene unos límites definidos, haciéndose poco a poco más delgada hasta desvanecerse en el espacio exterior. Tres cuartas partes de la masa atmosférica están contenidas dentro de los primeros 11 km de la superficie del planeta. Esta capa inferior se llama troposfera. La energía del Sol calienta esta capa y la superficie bajo ésta, causando la expansión del aire. El aire caliente se eleva debido a su menor densidad, siendo sustituido por aire de mayor densidad, es decir, aire más frío. Esto da como resultado la circulación atmosférica que genera el tiempo y el clima a través de la redistribución de la energía térmica.

Las líneas principales de circulación atmosférica las constituyen los vientos alisios en la región ecuatorial por debajo de los 30° de latitud, y los vientos del oeste en latitudes medias entre los 30° y 60°. Las corrientes oceánicas también son factores importantes para determinar el clima, especialmente la circulación termohalina que distribuye la energía térmica de los océanos ecuatoriales a las regiones polares.

El vapor de agua generado a través de la evaporación superficial es transportado según los patrones de circulación de la atmósfera. Cuando las condiciones atmosféricas permiten la elevación del aire caliente y húmedo, el agua se condensa y se deposita en la superficie en forma de precipitaciones. La mayor parte del agua es transportada a altitudes más bajas mediante los sistemas fluviales y por lo general regresa a los océanos o es depositada en los lagos. Este ciclo del agua es un mecanismo vital para sustentar la vida en la tierra y es un factor primario de la erosión que modela la superficie terrestre a lo largo de períodos geológicos. Los patrones de precipitación varían enormemente, desde varios metros de agua por año a menos de un milímetro. La circulación atmosférica, las características topológicas y las diferencias de temperatura determinan las precipitaciones medias de cada región.

La cantidad de energía solar que llega a la Tierra disminuye al aumentar la latitud. En las latitudes más altas la luz solar incide en la superficie en un ángulo menor, teniendo que atravesar gruesas columnas de atmósfera. Como resultado, la temperatura media anual del aire a nivel del mar se reduce en aproximadamente 0,4 °C por cada grado de latitud alejándose del ecuador. La Tierra puede ser subdividida en franjas latitudinales más o menos homogéneas con un clima específico. Desde el ecuador hasta las regiones polares, se encuentran la zona intertropical (o ecuatorial), el clima subtropical, el clima templado y los climas polares. El clima también puede ser clasificado en función de la temperatura y las precipitaciones, en regiones climáticas caracterizadas por masas de aire bastante uniformes. La metodología de clasificación más usada es la clasificación climática de Köppen (modificada por el estudiante de Wladimir Peter Köppen, Rudolph Geiger), que cuenta con cinco grandes grupos (zonas tropicales húmedas, zonas áridas, zonas húmedas con latitud media, clima continental y frío polar), que se dividen en subtipos más específicos.

Por encima de la troposfera, la atmósfera suele dividir en estratosfera, mesosfera y termosfera. Cada capa tiene un gradiente adiabático diferente, que define la tasa de cambio de la temperatura con respecto a la altura. Más allá de éstas se encuentra la exosfera, que se atenúa hasta penetrar en la magnetosfera, donde los campos magnéticos de la Tierra interactúan con el viento solar. Dentro de la estratosfera se encuentra la capa de ozono; un componente que protege parcialmente la superficie terrestre de la luz ultravioleta, siendo un elemento importante para la vida en la Tierra. La línea de Kármán, definida en los 100 km sobre la superficie de la Tierra, es una definición práctica usada para establecer el límite entre la atmósfera y el espacio.

La energía térmica hace que algunas de las moléculas en el borde exterior de la atmósfera de la Tierra incrementen su velocidad hasta el punto de poder escapar de la gravedad del planeta. Esto da lugar a una pérdida lenta pero constante de la atmósfera hacia el espacio. Debido a que el hidrógeno no fijado tiene un bajo peso molecular puede alcanzar la velocidad de escape más fácilmente, escapando así al espacio exterior a un ritmo mayor que otros gases. La pérdida de hidrógeno hacia el espacio contribuye a la transformación de la Tierra desde su inicial estado reductor a su actual estado oxidante. La fotosíntesis proporcionó una fuente de oxígeno libre, pero se cree que la pérdida de agentes reductores como el hidrógeno fue una condición previa necesaria para la acumulación generalizada de oxígeno en la atmósfera. Por tanto, la capacidad del hidrógeno para escapar de la atmósfera de la Tierra puede haber influido en la naturaleza de la vida desarrollada en el planeta. En la atmósfera actual, rica en oxígeno, la mayor parte del hidrógeno se convierte en agua antes de tener la oportunidad de escapar. En cambio, la mayor parte de la pérdida de hidrógeno actual proviene de la destrucción del metano en la atmósfera superior.

El campo magnético de la Tierra tiene una forma similar a un dipolo magnético, con los polos actualmente localizados cerca de los polos geográficos del planeta. En el ecuador del campo magnético (ecuador magnético), la fuerza del campo magnético en la superficie es , con un momento magnético dipolar global de . Según la teoría del dínamo, el campo se genera en el núcleo externo fundido, región donde el calor crea movimientos de convección en materiales conductores, generando corrientes eléctricas. Estas corrientes inducen a su vez el campo magnético de la Tierra. Los movimientos de convección en el núcleo son caóticos; los polos magnéticos se mueven y periódicamente cambian de orientación. Esto da lugar a reversiones geomagnéticas a intervalos de tiempo irregulares, unas pocas veces cada millón de años. La inversión más reciente tuvo lugar hace aproximadamente 700 000 años.

El campo magnético forma la magnetosfera, que desvía las partículas de viento solar. En dirección al Sol, el arco de choque entre el viento solar y la magnetosfera se encuentra a unas 13 veces el radio de la Tierra. La colisión entre el campo magnético y el viento solar forma los cinturones de radiación de Van Allen; un par de regiones concéntricas, con forma tórica, formadas por partículas cargadas muy energéticas. Cuando el plasma entra en la atmósfera de la Tierra por los polos magnéticos se crean las auroras polares.

El período de rotación de la Tierra con respecto al Sol, es decir, un día solar, es de alrededor de 86 400 segundos de tiempo solar (86 400,0025 segundos SIU). El día solar de la Tierra es ahora un poco más largo de lo que era durante el siglo XIX debido a la aceleración de marea, los días duran entre 0 y 2ms SIU más.
El período de rotación de la Tierra en relación a las estrellas fijas, llamado día estelar por el Servicio Internacional de Rotación de la Tierra y Sistemas de Referencia (IERS por sus siglas en inglés), es de del tiempo solar medio (UT1), o de El período de rotación de la Tierra en relación con el equinoccio vernal, mal llamado el "día sidéreo", es de del tiempo solar medio (UT1) . Por tanto, el día sidéreo es más corto que el día estelar en torno a 8,4 ms. La longitud del día solar medio en segundos SIU está disponible en el IERS para los períodos 1623-2005 y 1962-2005.

Aparte de los meteoros en la atmósfera y de los satélites en órbita baja, el movimiento aparente de los cuerpos celestes vistos desde la Tierra se realiza hacia al oeste, a una velocidad de 15°/h = 15'/min. Para las masas cercanas al ecuador celeste, esto es equivalente a un diámetro aparente del Sol o de la Luna cada dos minutos (desde la superficie del planeta, los tamaños aparentes del Sol y de la Luna son aproximadamente iguales).

La Tierra orbita alrededor del Sol a una distancia media de unos 150 millones de kilómetros, completando una órbita cada 365,2564 días solares, o un año sideral. Desde la Tierra, esto genera un movimiento aparente del Sol hacia el este, desplazándose con respecto a las estrellas a un ritmo de alrededor de 1°/día, o un diámetro del Sol o de la Luna cada 12 horas. Debido a este movimiento, en promedio la Tierra tarda 24 horas (un día solar) en completar una rotación sobre su eje hasta que el sol regresa al meridiano. La velocidad orbital de la Tierra es de aproximadamente 29,8 km/s (107 000 km/h), que es lo suficientemente rápida como para recorrer el diámetro del planeta (12 742 km) en siete minutos, o la distancia entre la Tierra y la Luna (384 000 km) en cuatro horas.

La Luna gira con la Tierra en torno a un baricentro común, debido a que este se encuentra dentro de la Tierra, a 4541 km de su centro, el sistema Tierra-Luna no es un planeta doble, la Luna completa un giro cada 27,32 días con respecto a las estrellas de fondo. Cuando se combina con la revolución común del sistema Tierra-Luna alrededor del Sol, el período del mes sinódico, desde una luna nueva a la siguiente, es de 29,53 días. Visto desde el polo norte celeste, el movimiento de la Tierra, la Luna y sus rotaciones axiales son todas contrarias a la dirección de las manecillas del reloj (sentido anti-horario). Visto desde un punto de vista situado sobre los polos norte del Sol y la Tierra, la Tierra parecería girar en sentido anti-horario alrededor del Sol. Los planos orbitales y axiales no están alineados: El eje de la Tierra está inclinado unos 23,4 grados con respecto a la perpendicular al plano Tierra-Sol, y el plano entre la Tierra y la Luna está inclinado unos 5 grados con respecto al plano Tierra-Sol. Sin esta inclinación, habría un eclipse cada dos semanas, alternando entre los eclipses lunares y eclipses solares.

La esfera de Hill, o la esfera de influencia gravitatoria, de la Tierra tiene aproximadamente 1,5 Gm (o 1 500 000 kilómetros) de radio. Esta es la distancia máxima en la que la influencia gravitatoria de la Tierra es más fuerte que la de los más distantes Sol y resto de planetas. Los objetos deben orbitar la Tierra dentro de este radio, o terminarán atrapados por la perturbación gravitatoria del Sol.

Desde el año de 1772, se estableció que cuerpos pequeños pueden orbitar de manera estable la misma órbita que un planeta, si esta permanece cerca de un punto triangular de Lagrange (también conocido como «punto troyano») los cuales están situados 60° delante y 60° detrás del planeta en su órbita. La Tierra es el cuarto planeta con un asteroide troyano (2010 TK7) después de Júpiter, Marte y Neptuno de acuerdo a la fecha de su descubrimiento Este fue difícil de localizar debido al posicionamiento geométrico de la observación, este fue descubierto en el 2010 gracias al telescopio WISE (Wide-Field Infrared Survey Explorer) de la NASA, pero fue en abril de 2011 con el telescopio «Canadá-Francia-Hawái» cuando se confirmó su naturaleza troyana, y se estima que su órbita permanezca estable dentro de los próximos 10 000 años.

La Tierra, junto con el Sistema Solar, está situada en la galaxia Vía Láctea, orbitando a alrededor de 28 000 años luz del centro de la galaxia. En la actualidad se encuentra unos 20 años luz por encima del plano ecuatorial de la galaxia, en el brazo espiral de Orión.

Debido a la inclinación del eje de la Tierra, la cantidad de luz solar que llega a un punto cualquiera en la superficie varía a lo largo del año. Esto ocasiona los cambios estacionales en el clima, siendo verano en el hemisferio norte ocurre cuando el Polo Norte está apuntando hacia el Sol, e invierno cuando apunta en dirección opuesta. Durante el verano, el día tiene una duración más larga y la luz solar incide más perpendicularmente en la superficie. Durante el invierno, el clima se vuelve más frío y los días más cortos. En la zona del Círculo Polar Ártico se da el caso extremo de no recibir luz solar durante una parte del año; fenómeno conocido como la noche polar. En el hemisferio sur se da la misma situación pero de manera inversa, con la orientación del Polo Sur opuesta a la dirección del Polo Norte.
Por convenio astronómico, las cuatro estaciones están determinadas por solsticios (puntos de la órbita en los que el eje de rotación terrestre alcanza la máxima inclinación hacia el Sol —solsticio de verano— o hacia el lado opuesto —solsticio de invierno—) y por equinoccios, cuando la inclinación del eje terrestre es perpendicular a la dirección del Sol. En el hemisferio norte, el solsticio de invierno se produce alrededor del 21 de diciembre, el solsticio de verano el 21 de junio, el equinoccio de primavera el 20 de marzo y el equinoccio de otoño el 23 de septiembre. En el hemisferio sur la situación se invierte, con el verano y los solsticios de invierno en fechas contrarias a la del hemisferio norte. De igual manera sucede con el equinoccio de primavera y de otoño.

El ángulo de inclinación de la Tierra es relativamente estable durante largos períodos de tiempo. Sin embargo, la inclinación se somete a nutaciones; un ligero movimiento irregular, con un período de 18,6 años. La orientación (en lugar del ángulo) del eje de la Tierra también cambia con el tiempo, precesando un círculo completo en cada ciclo de 25 800 años. Esta precesión es la razón de la diferencia entre el año sidéreo y el año tropical. Ambos movimientos son causados por la atracción variante del Sol y la Luna sobre el abultamiento ecuatorial de la Tierra. Desde la perspectiva de la Tierra, los polos también migran unos pocos metros sobre la superficie. Este movimiento polar tiene varios componentes cíclicos, que en conjunto reciben el nombre de movimientos cuasiperiódicos. Además del componente anual de este movimiento, existe otro movimiento con ciclos de 14 meses llamado el bamboleo de Chandler. La velocidad de rotación de la Tierra también varía en un fenómeno conocido como variación de duración del día.

En tiempos modernos, el perihelio de la Tierra se produce alrededor del 3 de enero y el afelio alrededor del 4 de julio. Sin embargo, estas fechas cambian con el tiempo debido a la precesión orbital y otros factores, que siguen patrones cíclicos conocidos como ciclos de Milankovitch. La variación de la distancia entre la Tierra y el Sol resulta en un aumento de alrededor del 6,9 % de la energía solar que llega a la Tierra en el perihelio en relación con el afelio. Puesto que el hemisferio sur está inclinado hacia el Sol en el momento en que la Tierra alcanza la máxima aproximación al Sol, a lo largo del año el hemisferio sur recibe algo más de energía del Sol que el hemisferio norte. Sin embargo, este efecto es mucho menos importante que el cambio total de energía debido a la inclinación del eje, y la mayor parte de este exceso de energía es absorbido por la superficie oceánica, que se extiende en mayor proporción en el hemisferio sur.

La Luna es el satélite natural de la Tierra. Es un cuerpo del tipo terrestre relativamente grande: con un diámetro de alrededor de la cuarta parte del de la Tierra, es el segundo satélite más grande del Sistema Solar en relación al tamaño de su planeta, después del satélite Caronte de su planeta enano Plutón. Los satélites naturales que orbitan los demás planetas se denominan "lunas" en referencia a la Luna de la Tierra.

La atracción gravitatoria entre la Tierra y la Luna causa las mareas en la Tierra. El mismo efecto en la Luna ha dado lugar a su acoplamiento de marea, lo que significa que su período de rotación es idéntico a su periodo de traslación alrededor de la Tierra. Como resultado, la luna siempre presenta la misma cara hacia nuestro planeta. A medida que la Luna orbita la Tierra, diferentes partes de su cara son iluminadas por el Sol, dando lugar a las fases lunares. La parte oscura de la cara está separada de la parte iluminada del terminador solar.

Debido a la interacción de las mareas, la Luna se aleja de la Tierra a una velocidad de aproximadamente 38 mm al año. Acumuladas durante millones de años, estas pequeñas modificaciones, así como el alargamiento del día terrestre en alrededor de 23 µs, han producido cambios significativos. Durante el período devónico, por ejemplo, (hace aproximadamente ) un año tenía 400 días, cada uno con una duración de 21,8 horas.

La Luna pudo haber afectado dramáticamente el desarrollo de la vida, moderando el clima del planeta. Evidencias paleontológicas y simulaciones computarizadas muestran que la inclinación del eje terrestre está estabilizada por las interacciones de marea con la Luna. Algunos teóricos creen que sin esta estabilización frente al momento ejercido por el Sol y los planetas sobre la protuberancia ecuatorial de la Tierra, el eje de rotación podría ser caóticamente inestable, mostrando cambios caóticos durante millones de años, como parece ser el caso de Marte.

Vista desde la Tierra, la Luna está justo a una distancia que la hace que el tamaño aparente de su disco sea casi idéntico al del Sol. El diámetro angular (o ángulo sólido) de estos dos cuerpos coincide porque aunque el diámetro del Sol es unas 400 veces más grande que el de la Luna, también está 400 veces más distante. Esto permite que en la Tierra se produzcan los eclipses solares totales y anulares.

La teoría más ampliamente aceptada sobre el origen de la Luna, la teoría del gran impacto, afirma que ésta se formó por la colisión de un protoplaneta del tamaño de Marte, llamado Tea, con la Tierra primitiva. Esta hipótesis explica (entre otras cosas) la relativa escasez de hierro y elementos volátiles en la Luna, y el hecho de que su composición sea casi idéntica a la de la corteza terrestre.

A fecha de 2016, el planeta Tierra tiene nueve cuasisatélites naturales o asteroides coorbitales conocidos: el (3753) Cruithne, 
el 2002 AA, 2003 YN, 
2004 GU, 2006 FV, 2010 SO 2013 LX, 2014 OL y 2016 H. El 15 de febero de 2020 se descubió que 2020 CD3 es un satélite natural temporal terrestre. 

A fecha de 2011, existen 931 satélites operativos creados por el hombre orbitando la Tierra.

Un planeta que pueda sostener vida se denomina habitable, incluso aunque en él no se originara vida. La Tierra proporciona las (actualmente entendidas como) condiciones necesarias, tales como el agua líquida, un ambiente que permite el ensamblaje de moléculas orgánicas complejas, y la energía suficiente para mantener un metabolismo. Hay otras características que se cree que también contribuyen a la capacidad del planeta para originar y mantener la vida: la distancia entre la Tierra y el Sol, así como su excentricidad orbital, la velocidad de rotación, la inclinación axial, la historia geológica, la permanencia de la atmósfera, y la protección ofrecida por el campo magnético.

Se denomina "biosfera" al conjunto de los diferentes tipos de vida del planeta junto con su entorno físico, modificado por la presencia de los primeros. Generalmente se entiende que la biosfera empezó a evolucionar hace 3500 millones de años. La Tierra es el único lugar donde se sabe que existe vida. La biosfera se divide en una serie de biomas, habitados por plantas y animales esencialmente similares. En tierra, los biomas se separan principalmente por las diferencias en latitud, la altura sobre el nivel del mar y la humedad. Los biomas terrestres situados en los círculos ártico o antártico, en gran altura o en zonas extremadamente áridas son relativamente estériles de vida vegetal y animal; la diversidad de especies alcanza su máximo en tierras bajas y húmedas, en latitudes ecuatoriales.

La Tierra proporciona recursos que son explotados por los seres humanos con diversos fines. Algunos de estos son recursos no renovables, tales como los combustibles fósiles, que son difícilmente renovables a corto plazo.

De la corteza terrestre se obtienen grandes depósitos de combustibles fósiles, consistentes en carbón, petróleo, gas natural y clatratos de metano. Estos depósitos son utilizados por los seres humanos para la producción de energía, y también como materia prima para la producción de sustancias químicas. Los cuerpos minerales también se han formado en la corteza terrestre a través de distintos procesos de mineralogénesis, como consecuencia de la erosión y de los procesos implicados en la tectónica de placas. Estos cuerpos albergan fuentes concentradas de varios metales y otros elementos útiles.

La biosfera de la Tierra produce muchos productos biológicos útiles para los seres humanos, incluyendo (entre muchos otros) alimentos, madera, fármacos, oxígeno, y el reciclaje de muchos residuos orgánicos. El ecosistema terrestre depende de la capa superior del suelo y del agua dulce, y el ecosistema oceánico depende del aporte de nutrientes disueltos desde tierra firme. Los seres humanos también habitan la tierra usando materiales de construcción para construir refugios. Para 1993, el aprovechamiento de la tierra por los humanos era de aproximadamente:

La cantidad de tierras de regadío en 1993 se estimaban en 2481250 km².

Grandes áreas de la superficie de la Tierra están sujetas a condiciones climáticas extremas, tales como ciclones tropicales, huracanes, o tifones que dominan la vida en esas zonas. Muchos lugares están sujetos a terremotos, deslizamientos, tsunamis, erupciones volcánicas, tornados, dolinas, ventiscas, inundaciones, sequías y otros desastres naturales.

Muchas áreas concretas están sujetas a la contaminación causada por el hombre del aire y del agua, a la lluvia ácida, a sustancias tóxicas, a la pérdida de vegetación (sobrepastoreo, deforestación, desertificación), a la pérdida de vida salvaje, la extinción de especies, la degradación del suelo y su agotamiento, a la erosión y a la introducción de especies invasoras.

Según las Naciones Unidas, existe un consenso científico que vincula las actividades humanas con el calentamiento global, debido a las emisiones industriales de dióxido de carbono y el calor residual antropogénico. Se prevé que esto produzca cambios tales como el derretimiento de los glaciares y superficies heladas, temperaturas más extremas, cambios significativos en el clima y un aumento global del nivel del mar.

La cartografía —el estudio y práctica de la elaboración de mapas—, y subsidiariamente la geografía, han sido históricamente las disciplinas dedicadas a describir la Tierra. La topografía o determinación de lugares y distancias, y en menor medida la navegación, o determinación de la posición y de la dirección, se han desarrollado junto con la cartografía y la geografía, suministrando y cuantificando la información necesaria.

La Tierra tiene aproximadamente 7000000000 de habitantes al mes de octubre de 2011. Las proyecciones indicaban que la población humana mundial llegaría a siete mil millones a principios de 2012, pero esta cifra fue superada a mediados de octubre de 2011 y se espera llegar a 9200 millones en 2050. Se piensa que la mayor parte de este crecimiento tendrá lugar en los países en vías de desarrollo. La región del África subsahariana tiene la tasa de natalidad más alta del mundo. La densidad de población varía mucho en las distintas partes del mundo, pero la mayoría de la población vive en Asia. Está previsto que para el año 2020 el 60 % de la población mundial se concentre en áreas urbanas, frente al 40 % en áreas rurales.

Se estima que solo una octava parte de la superficie de la Tierra es apta para su ocupación por los seres humanos; tres cuartas partes está cubierta por océanos, y la mitad de la superficie terrestre es: desierto (14 %), alta montaña (27 %), u otros terrenos menos adecuados. El asentamiento permanente más septentrional del mundo es Alert, en la Isla de Ellesmere en Nunavut, Canadá. (82°28'N). El más meridional es la Base Amundsen-Scott, en la Antártida, casi exactamente en el Polo Sur. (90°S)

Las naciones soberanas independientes reclaman la totalidad de la superficie de tierra del planeta, a excepción de algunas partes de la Antártida y la zona no reclamada de Bir Tawil entre Egipto y Sudán. En el año 2011 existen , incluidos los 192 . Hay también 59 territorios dependientes, y una serie de , y otras entidades. Históricamente, la Tierra nunca ha tenido un gobierno soberano con autoridad sobre el mundo entero, a pesar de que una serie de estados-nación han intentado dominar el mundo, sin éxito.

Las Naciones Unidas es una organización mundial intergubernamental que se creó con el objetivo de intervenir en las disputas entre las naciones, a fin de evitar los conflictos armados. Sin embargo, no es un gobierno mundial. La ONU sirve principalmente como un foro para la diplomacia y el derecho internacional. Cuando el consenso de sus miembros lo permite, proporciona un mecanismo para la intervención armada.
El primer humano en orbitar la Tierra fue Yuri Gagarin el 12 de abril de 1961. Hasta el 2004, alrededor de 400 personas visitaron el espacio exterior y alcanzado la órbita de la Tierra. De estos, doce han caminado sobre la Luna. En circunstancias normales, los únicos seres humanos en el espacio son los de la Estación Espacial Internacional. La tripulación de la estación, compuesta en la actualidad por seis personas, suele ser reemplazada cada seis meses. Los seres humanos que más se han alejado de la Tierra se distanciaron 400 171 kilómetros, alcanzados en la década de 1970 durante la misión Apolo 13.

La palabra Tierra proviene del latín "Tellus" o "Terra" que era equivalente en griego a "Gea", nombre asignado a una deidad, al igual que los nombres de los demás planetas del Sistema Solar. El símbolo astronómico estándar de la Tierra consiste en una cruz circunscrita por un círculo.

A diferencia de lo sucedido con el resto de los planetas del Sistema Solar, la humanidad no comenzó a ver la Tierra como un objeto en movimiento, en órbita alrededor del Sol, hasta alcanzado el siglo XVI. La Tierra a menudo se ha personificado como una deidad, en particular, una diosa. En muchas culturas la diosa madre también es retratada como una diosa de la fertilidad. En muchas religiones los mitos sobre la creación recuerdan una historia en la que la Tierra es creada por una deidad o deidades sobrenaturales. Varios grupos religiosos, a menudo asociados a las ramas fundamentalistas del protestantismo o el islam, afirman que sus interpretaciones sobre estos mitos de creación, relatados en sus respectivos textos sagrados son la verdad literal, y que deberían ser consideradas junto a los argumentos científicos convencionales de la formación de la Tierra y el desarrollo y origen de la vida, o incluso reemplazarlos. Tales afirmaciones son rechazadas por la comunidad científica y otros grupos religiosos. Un ejemplo destacado es la controversia entre el creacionismo y la teoría de la evolución.

En el pasado hubo varias creencias en una Tierra plana, pero esta creencia fue desplazada por el concepto de una Tierra esférica, debido a la observación y a la circunnavegación. La perspectiva humana acerca de la Tierra ha cambiado tras el comienzo de los vuelos espaciales, y actualmente la biosfera se interpreta desde una perspectiva global integrada. Esto se refleja en el creciente movimiento ecologista, que se preocupa por los efectos que causa la humanidad sobre el planeta.

En muchos países se celebra el 22 de abril el Día de la Tierra, con el objetivo de hacer conciencia de las condiciones ambientales del planeta.



</doc>
<doc id="2733" url="https://es.wikipedia.org/wiki?curid=2733" title="Tragedia">
Tragedia

La tragedia es una forma literaria, teatral o dramática del lenguaje solemne, cuyos personajes protagónicos son ilustres y se ven enfrentados de manera misteriosa, invencible e inevitable, a causa de un error fatal o condición de carácter (la llamada hamartia) contra un destino fatal ("fatum", hado o sino) o los dioses, generando un conflicto cuyo final es irremediablemente triste: la destrucción del héroe protagonista, quien muere o enloquece.

El término procede de la voz griega "tragoedia" o “canto del macho cabrío” ("τραγῳδία", palabra compuesta de τράγος “carnero” y "ᾠδή" “canción”) y alude a la canción de los griegos atenienses que era entonada procesionalmente en honor del dios Dioniso en sus fiestas Dionisias.

El género se define como una obra dramática de asunto terrible y desenlace funesto en la que intervienen personajes ilustres o heroicos, y emplea un estilo de lenguaje sublime o solemne. Aristóteles, en su "Poética", dejó la primera definición del término:

Las tragedias acaban generalmente en la muerte, el exilio o en la destrucción física, moral y económica del personaje principal, quien se enfrenta a un conflicto insoluble que le obliga a cometer un error fatal o hamartia al intentar "hacer lo correcto" en una situación en la que lo correcto simplemente no puede hacerse. El héroe trágico es sacrificado así a esa fuerza que se le impone, y contra la cual se rebela con orgullo insolente o "hybris".

También existe un tipo de tragedia de sublimación, en las que el personaje principal es mostrado como un héroe que desafía las adversidades con la fuerza de sus virtudes, ganándose de esta manera la admiración del espectador, como es el caso de "Antígona" de Sófocles.

La tragedia nació como tal en Grecia con las obras de Tespis y Frínico, y se consolidó con la tríada de los grandes trágicos del clasicismo griego: Esquilo, Sófocles y Eurípides. Las tragedias clásicas se caracterizan, según Aristóteles, por generar una catarsis en el espectador.

La tradición atribuye a Tespis la primera composición trágica, pero apenas se conservan restos de sus obras. Después, entre otros autores, destacaron e hicieron evolucionar la tragedia, por orden cronológico, Esquilo, Sófocles y Eurípides.

Aristóteles en su "Poética" señala sobre las partes de la tragedia se dividen en prólogo, episodio, éxodo y la parte del coro, que se divide a la vez en párodo y estásimo. El prólogo precede al párodo del coro. Después vienen siete episodios entrelazados por cada estásimo para concluir con el éxodo, intervención del coro que no es cantada. En cuanto al estásimo, es un canto de coro sin anapesto ni troqueo.


La primera tragedia latina la compuso Livio Anacrónico y se representó en la vieja Roma en el año 514 de su fundación (240 a. C.) en tiempo del consulado de Cayo Claudico Centónfwff y M. Sempiterno, unos ciento sesenta años después de la muerte de Sófocles y Eurípides y doscientos veinte años antes de la de Virgilio. Trasidas de Ennio, Pacuvio y Accio, Séneca compuso ya en la Edad de plata de la literatura latina once que se han conservado e influyeron poderosamente el teatro en lengua vulgar del Renacimiento y el Barroco; destaca en especial la inspirada en la tragedia homónima de Sófocles, "Fedra".

La tragedia moderna surgió en la época del Renacimiento y por traducciones o imitaciones de la antigüedad. Cierto que se encuentran algunos ensayos en lengua vulgar, sobre todo, en Grecia, desde los siglo XIII al XVI pero es indudable que la primera tragedia regular es "Sofonisba", compuesta por Gian Giorgio Trissino y representada en Roma en 1515. En 1552, el poeta Jodelle, el primero en Francia, hizo representar la tragedia de su invención "Cleopatra cautiva". Robert Garnier (1544-1590), Alexandre Hardy y Jean Mairet siguieron su ejemplo hasta que en 1635 apareció Corneille, con su primera tragedia, "Medea", siguiéndole después Racine que elevó a la perfección el restaurado género. Entre los autores modernos que más se han distinguido en la tragedia hay que citar:





</doc>
<doc id="2735" url="https://es.wikipedia.org/wiki?curid=2735" title="Trigonometría">
Trigonometría

La trigonometría es una rama de la matemática, cuyo significado etimológico es 'la medición de los triángulos'. Deriva de los términos griegos τριγωνοϛ "trigōnos" 'triángulo' y μετρον "metron" 'medida'.

En términos generales, la trigonometría es el estudio de las razones trigonométricas: seno, coseno, tangente, cotangente, secante y cosecante. La trigonometría se aplica a otras ramas de la geometría, o la geometría analítica en particular geometría plana o geometría del espacio. En soluciones de ecuaciones diferenciales ordinarias ( y = y´´), series de Fourier usadas en ecuaciones en derivadas parciales. Se usa en la mecánica. 

Posee numerosas aplicaciones, entre las que se encuentran: las técnicas de triangulación, por ejemplo, son usadas en astronomía para medir distancias a estrellas próximas, en la medición de distancias entre puntos geográficos, y en sistemas globales de navegación por satélites.

Los antiguos egipcios y los babilonios conocían los teoremas sobre las proporciones de los lados de los triángulos semejantes. Pero las sociedades prehelénicas carecían de la noción de una medida del ángulo y por lo tanto, los lados de los triángulos se estudiaron en su medida, un campo que se podría llamar trilaterometría.
Los astrónomos babilonios llevaron registros sobre la salida y puesta de las estrellas, el movimiento de los planetas y los eclipses solares y lunares, todo lo cual requiere la familiaridad con la distancia angular medida sobre la esfera celeste. Sobre la base de la interpretación de una tablilla cuneiforme Plimpton 322 (c. 1900 a. C.), algunos incluso han afirmado que los antiguos babilonios tenían una tabla de secantes. Hoy, sin embargo, hay un gran debate acerca de si se trata de una tabla de ternas pitagóricas, una tabla de soluciones de ecuaciones de segundo grado o una tabla trigonométrica.
Los egipcios, en el segundo milenio antes de Cristo, utilizaban una forma primitiva de la trigonometría, para la construcción de las pirámides. El "Papiro de Ahmes", escrito por el escriba egipcio Ahmes (c. 1680-1620 a. C.), contiene el siguiente problema relacionado con la trigonometría:

La solución al problema es la relación entre la mitad del lado de la base de la pirámide y su altura. En otras palabras, la medida que se encuentra para la "seked" es la cotangente del ángulo que forman la base de la pirámide y su respectiva cara.

En la medición de ángulos y, por tanto, en trigonometría, se emplean tres unidades, si bien la más utilizada en la vida cotidiana es el grado sexagesimal, en matemáticas es el radián la más utilizada, y se define como la unidad natural para medir ángulos, el grado centesimal se desarrolló como la unidad más próxima al sistema decimal, se usa en topografía, arquitectura o en construcción.

La trigonometría es una rama importante de las matemáticas dedicada al estudio de la relación entre los lados y ángulos de un triángulo rectángulo y una circunferencia. Con este propósito se definieron una serie de funciones, las que han sobrepasado su fin original para convertirse en elementos matemáticos estudiados en sí mismos y con aplicaciones en los campos más diversos.

El triángulo ABC es un triángulo rectángulo en C; lo usaremos para definir las razones seno, coseno y tangente, del ángulo formula_1, correspondiente al vértice A, situado en el centro de la circunferencia.




En el esquema su representación geométrica es:


En el esquema su representación geométrica es:


En el esquema su representación geométrica es:

Normalmente se emplean las relaciones trigonométricas seno, coseno y tangente, y salvo que haya un interés específico en hablar de ellos o las expresiones matemáticas se simplifiquen mucho, los términos cosecante, secante y cotangente no suelen utilizarse

Además de las funciones anteriores, existen otras funciones trigonométricas. Matemáticamente se pueden definir empleando las ya vistas. Su uso no es muy corriente, pero sí se emplean, dado su sentido geométrico. Veamos:

El seno cardinal o función sinc (x) definida:

El verseno, es la distancia que hay entre la cuerda y el arco en una circunferencia, también se denomina sagita o flecha, se define:

El semiverseno, se utiliza en navegación al intervenir en el cálculo esférico:

El coverseno, 

El semicoverseno

La exsecante:

En trigonometría, cuando el ángulo se expresa en radianes (dado que un radián es el arco de circunferencia de longitud igual al radio), suele denominarse arco a cualquier cantidad expresada en radianes; por eso las funciones recíprocas se denominan con el prefijo arco, cada razón trigonométrica posee su propia función recíproca:

y es igual al seno de x, la función recíproca:

x es el arco cuyo seno vale y, o también x es el arcoseno de y.

si:
y es igual al coseno de x, la función recíproca:

x es el arco cuyo coseno vale y, que se dice: x es el arcocoseno de y.

si:
y es igual al tangente de x, la función recíproca:

x es el arco cuya tangente vale y, o x es igual al arcotangente de y.

NOTA:
Es común, que las funciones recíprocas sean escritas de esta manera:

pero se debe tener cuidado de no confundirlas con:

Si aplicamos el criterio para obtener las funciones recíprocas en el sentido estricto, definiendo el arcoseno como la recíproca del seno, el arcocoseno como la recíproca del coseno y el arco tangente como la recíproca de la tangente, lo obtenido es la gráfica de la derecha. Es fácil percatarse que estas representaciones no cumplen la unicidad de la imagen, que forma parte de la definición de función, eso es para un valor de x dado existen un número infinito de valores que son su función, por ejemplo: el arcoseno de 0 es 0, pero también lo son cualquier múltiplo entero de formula_24.

Para cualquier n número entero.

Dado que la recíproca de una función no tiene que cumplir necesariamente la unicidad de imagen, solo las funciones inyectivas y biyectivas dan funciones recíprocas con esta propiedad, esta situación se repite para el resto de las funciones recíprocas trigonométricas.

A fin de garantizar el cumplimiento de la definición de función, en cuanto a la unicidad de imagen, y que por tanto las funciones trigonométricas recíprocas cumplan los criterios de la definición de función, se suele restringir tanto el dominio como el codominio, esta corrección permite un análisis correcto de la función, a pesar de que no coincida exactamente con la reciproca de la función trigonométrica original. Así tenemos que:

La función arcoseno se define:

La función arcocoseno se define:

La función arcotangente se define:

Del mismo modo que las funciones trigonométricas directas recíprocas, cuando el ángulo se expresa en radianes, se denomina arco a ese ángulo, y se emplea el prefijo arco para la función trigonométrica recíproca, así tenemos que:

y es igual a la cosecante de x, la función recíproca:

x es el arco cuya cosecante vale y, o también x es la arcocosecante de y.

si:
y es igual al secante de x, la función recíproca:

x es el arco cuya secante vale y, que se dice: x es el arcosecante de y.

si:
y es igual al cotangente de x, la función recíproca:

x es el arco cuya cotangente vale y, o x es igual al arcocotangente de y.

Al igual que en las funciones directas, si aplicamos el criterio para obtener las funciones recíprocas, dado que las funciones trigonométricas inversas no son inyectivas, lo obtenido es la gráfica de la derecha, que no cumplen la unicidad de la imagen, que forma parte de la definición de función.

Para que se cumpla la definición de función, definimos un dominio y un codominio restringidos. Así tenemos que:

La función arcocosecante se define:

La función arcosecante se define:

La función arcocotangente se define:

Esta restricción garantiza el cumplimiento de la definición de función.

A continuación algunos valores de las funciones que es conveniente recordar:

Para el cálculo del valor de las funciones trigonométricas se confeccionaron . La primera de estas tablas fue desarrollada por Johann Müller Regiomontano en 1467, que nos permiten, conocido un ángulo, calcular los valores de sus funciones trigonométricas. En la actualidad dado el desarrollo de la informática, en prácticamente todos los lenguajes de programación existen bibliotecas de funciones que realizan estos cálculos, incorporadas incluso en calculadoras electrónicas de bolsillo, por lo que el empleo actual de las tablas resulta obsoleto.

Dados los ejes de coordenadas cartesianas xy, de centro O, y una circunferencia goniométrica (circunferencia de radio la unidad) con centro en O; el punto de corte de la circunferencia con el lado positivo de las x, lo señalamos como punto E.

Nótese que el punto A es el vértice del triángulo, y O es el centro de coordenada del sistema de referencia:

a todos los efectos.

La recta r, que pasa por O y forma un ángulo formula_1 sobre el eje de las x, corta a la circunferencia en el punto B, la vertical que pasa por B, corta al eje x en C, la vertical que pasa por E corta a la recta r en el punto D.

Por semejanza de triángulos:

Los puntos E y B están en la circunferencia de centro O, por eso la distancia formula_41 y formula_42 son el radio de la circunferencia, en este caso al ser una circunferencia de radio = 1, y dadas las definiciones de las funciones trigonométricas:

tenemos:

La tangente es la relación del seno entre el coseno, según la definición ya expuesta.

Para ver la evolución de las funciones trigonométricas según aumenta el ángulo, daremos una vuelta completa a la circunferencia, viéndolo por cuadrantes.Como consecuencia de esta consideración, los segmentos correspondientes a cada función trigonométrica variarán de longitud, siendo esta variación función del ángulo, partiendo en el primer cuadrante de un ángulo cero.

Partiendo de esta representación geométrica de las funciones trigonométricas, podemos ver las variaciones de las funciones a medida que aumenta el ángulo formula_45.

Para formula_46, tenemos que B, D, y C coinciden en E, por tanto:

Si aumentamos progresivamente el valor de formula_1, las distancias formula_49 y formula_50 aumentarán progresivamente, mientras que formula_51 disminuirá.

Vale recordar que el punto B pertenece a la circunferencia y cuando el ángulo aumenta se desplaza sobre ella.

El punto E es la intersección de la circunferencia con el eje x y no varia de posición.

Los segmentos: formula_51 y formula_49 están limitados por la circunferencia y por tanto su máximo valor absoluto será 1, pero formula_50 no está limitado, dado que D es el punto de corte de la recta r que pasa por O, y la vertical que pasa por E, en el momento en el que el ángulo formula_55 rad, la recta r será la vertical que pasa por O. Dos rectas verticales no se cortan, o lo que es lo mismo la distancia formula_50 será infinita.

El punto C coincide con A y el coseno vale cero. El punto B esta en el eje y en el punto más alto de la circunferencia y el seno toma su mayor valor: uno.

Para un ángulo recto las funciones toman los valores:

Cuando el ángulo formula_1 supera el ángulo recto, el valor del seno empieza a disminuir según el segmento formula_49, el coseno aumenta según el segmento formula_51, pero en el sentido negativo de las x, el valor del coseno toma sentido negativo, si bien su valor absoluto aumenta cuando el ángulo sigue creciendo.

La tangente para un ángulo formula_1 inferior a formula_62 rad se hace infinita en el sentido positivo de las y, para el ángulo recto la recta vertical r que pasa por O y la vertical que pasa por E no se cortan, por lo tanto la tangente no toma ningún valor real, cuando el ángulo supera los formula_62 rad y pasa al segundo cuadrante la prolongación de r corta a la vertical que pasa por E en el punto D real, en el lado negativo de las y, la tangente formula_50 por tanto toma valor negativo en el sentido de las y, y su valor absoluto disminuye a medida que el ángulo formula_1 aumenta progresivamente hasta los formula_66 rad.

Resumiendo: en el segundo cuadrante el seno de formula_1, formula_49, disminuye progresivamente su valor desde 1, que toma para formula_69 rad, hasta que valga 0, para formula_70 rad, el coseno,formula_51, toma valor negativo y su valor varia desde 0 para formula_69 rad, hasta –1, para formula_73 rad.

La tangente conserva la relación:

incluyendo el signo de estos valores.

Para un ángulo llano tenemos que el punto D esta en E, y B y C coinciden en el eje de las x en el lado opuesto de E, con lo que tenemos:

En el tercer cuadrante, comprendido entre los valores del ángulo formula_70 rad a formula_77 rad, se produce un cambio de los valores del seno, el coseno y la tangente, desde los que toman para formula_78 rad:

Cuando el ángulo formula_1 aumenta progresivamente, el seno aumenta en valor absoluto en el sentido negativo de las y, el coseno disminuye en valor absoluto en el lado negativo de las x, y la tangente aumenta del mismo modo que lo hacía en el primer cuadrante.

A medida que el ángulo crece el punto C se acerca a O, y el segmento formula_51, el coseno, se hace más pequeño en el lado negativo de las x.

El punto B, intersección de la circunferencia y la vertical que pasa por C, se aleja del eje de las x, en el sentido negativo de las y, el seno, formula_49.

Y el punto D, intersección de la prolongación de la recta r y la vertical que pasa por E, se aleja del eje las x en el sentido positivo de las y, con lo que la tangente, formula_50, aumenta igual que en el primer cuadrante

Cuando el ángulo formula_1 alcance formula_85 rad, el punto C coincide con O y el coseno valdrá cero, el segmento formula_49 será igual al radio de la circunferencia, en el lado negativo de las y, y el seno valdrá –1, la recta r del ángulo y la vertical que pasa por E serán paralelas y la tangente tomara valor infinito por el lado positivo de las y.

El seno el coseno y la tangente siguen conservando la misma relación:

que se cumple tanto en valor como en signo, nótese que a medida que el coseno se acerca a valores cercanos a cero, la tangente tiende a infinito.

En el cuarto cuadrante, que comprende los valores del ángulo formula_1 entre formula_85 rad y formula_89 rad, las variables trigonométricas varían desde los valores que toman para formula_85 rad:

hasta los que toman para formula_92 rad pasando al primer cuadrante, completando una rotación:

como puede verse a medida que el ángulo formula_1 aumenta, aumenta el coseno formula_51 en el lado positivo de las x, el seno formula_49 disminuye en el lado negativo de las y, y la tangente formula_50 también disminuye en el lado negativo de las y.

Cuando formula_1, vale formula_92 ó formula_100 al completar una rotación completa los puntos B, C y D, coinciden en E, haciendo que el seno y la tangente valga cero, y el coseno uno, del mismo modo que al comenzarse el primer cuadrante.

Dado el carácter rotativo de las funciones trigonométricas, se puede afirmar en todos los casos:

Que cualquier función trigonométrica toma el mismo valor si se incrementa el ángulo un número n entero de rotaciones completas.

Partiendo de una circunferencia de radio uno, dividida en cuatro cuadrantes, por dos rectas perpendiculares, que se cortan en el centro de la circunferencia O, estas rectas cortan a la circunferencia en los puntos A, B, C y D, la recta horizonte AC también la podemos llamar eje x y la recta vertical BD eje y. Dada una recta r, que pasa por el centro de la circunferencia y forma un ángulo α con OA, eje x, y corta a la circunferencia en F, tenemos que la vertical que pasa por F corta al eje x en E, la vertical que pasa por A corta a la recta r en G. Con todo esto definimos, como ya se vio anteriormente, las funciones trigonométricas:

para el seno:

dado que:

Para el coseno:

dado que:

Para la tangente:

dado que:

partiendo de estas definiciones, podemos ver algunos caso importantes:

Si a partir del eje vertical OB trazamos la recta r a un ángulo α en el sentido horario, la recta r forma con el eje x un ángulo 90-α. Así, el valor de las funciones trigonométricas de este ángulo,conocidas las de α,serán:

El triángulo OEF,rectángulo en E, siendo el ángulo en F α, por lo tanto:

en el mismo triángulo OEF, tenemos que:

viendo el triángulo OAG, rectángulo en A, siendo el ángulo en G igual a α, podemos ver:

Si a partir de eje vertical OB trazamos la recta r a un ángulo α, medido en sentido trigonométrico, el ángulo formado por el eje horizontal OA y la recta r será 90+α. La prolongación de la recta r corta a la circunferencia en F y a la vertical que pasa por A en G.

El triángulo OEF es rectángulo en E y su ángulo en F es α, por lo tanto tenemos que:

En el mismo triángulo OEF podemos ver:

En el triángulos OAG rectángulo A y siendo α el ángulo en G, tenemos:

Si sobre el eje horizontal OC, trazamos la recta r a un ángulo α, el ángulo entre el eje OA y la recta r es de 180-α, dado el triángulo OEF rectángulo en E y cuyo ángulo en O es α, tenemos:

en el mismo triángulo OEF:

En el triángulo OAG, rectángulo en A y con ángulo en O igual a α, tenemos:

Sobre la circunferencia de radio uno, a partir del eje OC con un ángulo α trazados la recta r, el ángulo del eje OA y la recta r es de 180+α, como se ve en la figura. En el triángulo OEF rectángulo en E se puede deducir:

en el mismo triángulo OEF tenemos:

en el triángulo OAG, rectángulo en A, vemos que:

Sobre el eje OD y con un ángulo α medido en sentido horario trazamos la recta r. El ángulo entre el eje OA y la recta r es de 270-α. En el triángulo OEF, rectángulo en E, tenemos:

por otra parte en el mismo triángulo OEF, tenemos:

en el triángulo OAG rectángulo en A, y siendo α el ángulo en G, tenemos;

Sobre el eje OD y con un ángulo α medido en sentido trigonométrico, trazamos la recta r. El ángulo entre el eje OA y la recta r es de 270+α. En el triángulo OEF, rectángulo en E, tenemos:

por otra parte en el mismo triángulo OEF, tenemos:

en el triángulo OAG rectángulo en A, y siendo α el ángulo en G, tenemos;

Sobre la circunferencia de radio uno, a partir del eje OA con un ángulo α medido en sentido horario trazados la recta r, el ángulo del eje OA y la recta r es de -α, o lo que es lo mismo 360-α como se ve en la figura. En el triángulo OEF rectángulo en E se puede deducir:

en el mismo triángulo OEF tenemos:

en el triángulo OAG, rectángulo en A, vemos que:

Una identidad es una igualdad en que se cumple para todos los valores permisibles de la variable. En trigonometría existen seis identidades fundamentales:

Como en el triángulo rectángulo cumple la función que:

de la figura anterior se tiene que:

por tanto:

entonces para todo ángulo α, se cumple la identidad Pitagórica: 

que también puede expresarse:

El seno y coseno se definen en matemática compleja, gracias a la fórmula de Euler como:
Por lo tanto, la tangente quedará definida como:
Siendo formula_139.





</doc>
<doc id="2736" url="https://es.wikipedia.org/wiki?curid=2736" title="Terry Pratchett">
Terry Pratchett

Terence David John "Terry" Pratchett, OBE (Beaconsfield, 28 de abril de 1948-Broad Chalke, 12 de marzo de 2015) fue un escritor británico de fantasía y ciencia ficción. Sus obras más conocidas corresponden a la serie del "Mundodisco" ("Discworld"); además escribió novelas juveniles, relatos cortos y colaboró en la redacción de guiones para las adaptaciones televisivas de sus novelas. Se han vendido más de 85 millones de ejemplares de sus libros en más de 35 idiomas.

Nació en Beaconsfield en Buckinghamshire en 1948. A los 11 años, entró a estudiar en la escuela técnica de High Wycombe, en cuya revista estudiantil publicó su primer relato a los 13 años, titulado "The Hades Business". Dos años más tarde sería publicada para la venta comercial. Orientó sus estudios al periodismo, dejó la escuela en 1965 para trabajar en "Bucks Free Press" y aprobó el curso del "National Council" para la Formación de Periodistas. En ese mismo año escribiría su segundo relato corto, "The Night Dweller".

En 1971 se publicó su primer libro llamado "The carpet people" que, aunque poco numerosas, recibió unas buenas críticas. Algunas calificaron la calidad de la obra como extraordinaria. Posteriormente le siguieron "The dark side of the sun" en 1976 y "Strata" en 1981. Dejó el "Bucks Free Press" por el "Western Daily Press" el 28 de septiembre de 1970, y volvió al anterior en 1972 ya como subdirector. Un año más tarde volvió a dejarlo, esta vez por el "Bath Chronicle". En esta época realizó una serie de tiras cómicas por las que no pasará a la historia.

En 1981 pasó a ocupar el cargo de responsable de relaciones públicas en una central nuclear (la empresa para la que trabajaba, actualmente "PowerGen", tenía responsabilidad sobre tres centrales nucleares), justo antes del desastre de Three Mile Island. Precisamente este era su trabajo cuando escribió su primera novela sobre el Mundodisco: "El color de la magia" (1983). El filón del Mundodisco "(Discworld)", una saga de fantasía ambientada en un mundo hilarante que parodia el nuestro es el conjunto de novelas que le ha dado fama internacional.

En 1986 se publicó "La luz fantástica", continuación de la anterior. Con la popularidad del hombre del sombrero aumentando por momentos, en 1987 decidió dedicarse únicamente a escribir. "Ritos Iguales" (1987), "Mort" (1987) y "Rechicero" (1988) serían las siguientes novelas en aparecer. Ese mismo año firmó un nuevo contrato para seis novelas más (no necesariamente de "Mundodisco") y a partir de ese momento no dejó de escribir.

Autor prolífico, Pratchett escribió las siguientes novelas sobre el Mundodisco en tiempo récord: "Brujerías" (1988), "Pirómides" (1989), "¡Guardias! ¿Guardias?" (1989), "Eric" (1990), "Imágenes en acción" (1990), "El segador" (1991), "Brujas de viaje" (1991), "Dioses Menores" (1992), "Lores y damas" (1992), "Hombres de armas" (1993), "Soul Music" (1994), "Tiempos interesantes" (1994), "Mascarada" (1995), "Pies de barro" (1996), "Papá Puerco" (1996), "¡Voto a bríos!" (1997), "El país del fin del mundo" (1998), "Carpe Jugulum" (1998), "El quinto elefante" (1999), "La verdad" (2000), "Ladrón del tiempo" (2001), "El último héroe" (2001), "El asombroso Mauricio y sus roedores sabios" (2001), "Ronda de noche" (2002), "Los pequeños hombres libres (The wee free men)" (2003), "Regimiento monstruoso" (2003), "Un sombrero de cielo" (2004), "Cartas en el asunto" (2004), "¡Zas!" (2005), "Wintersmith" (2006), "Dinero a mansalva" (2007), "El Atlético invisible" (2009), "I Shall Wear Midnight" (2010) y "Snuff" (2011). También publicó tres volúmenes de "The Science of Discworld" (La ciencia del Mundodisco) en colaboración con Ian Stewart, matemático y popular autor de libros de divulgación, y Jack Cohen, biólogo y colaborador de escritores en la creación de alienígenas plausibles.

Mención especial merece la trilogía del Éxodo de los Gnomos, no relacionada con el Mundodisco, cuyos títulos son "Camioneros" (1989), "Cavadores" (1989) y "La nave" (1990).

En 1998 fue nombrado Oficial de la Orden del Imperio Británico en el cumpleaños de la Reina como reconocimiento a los servicios prestados a la literatura (aunque en principio pensó que se trataba de una broma), al año siguiente recibió el título de Doctor Honoris Causa en Literatura por la Universidad de Warwick y en 2001 el mismo título honorario por la Universidad de Portsmouth. Como curiosidad se puede comentar que en la ceremonia de investidura de la Universidad de Warwick él mismo replicó haciendo Doctores de la Universidad Invisible a Ian Stewart y Jack Cohen, quienes escribieron conjuntamente la novela "The Science of Discworld". En los fastos de Año Nuevo de 2009 fue asimismo armado Caballero.

Terry Pratchett anunció el 11 de diciembre de 2007 que padecía un mal de Alzheimer prematuro. Sin embargo, anunció que plantaría cara a la enfermedad porque pensaba que "aún hay tiempo para escribir al menos unos libros más".

El escritor incidió en su anuncio en que no era simplemente un "no estoy muerto". Aunque utilizó su característica ironía al decir que "por supuesto, estaré muerto en un futuro, como todo el mundo. Para mí, quizá más tarde de lo que piensas". Según su comunicado, este caso de Alzheimer era una versión "muy rara" de la enfermedad. El autor mencionaba un "golpe fantasma" recibido en el pasado que había salido a la superficie. Pratchett dijo tomarse este problema con "filosofía" y un "suave optimismo".

Terry Pratchett falleció el 12 de marzo de 2015 en su casa cerca de Stonehenge, en Broad Chalke (Wiltshire), donde vivía con su mujer Lyn y su hija Rhianna Pratchett, también escritora.


Ordenadas en orden de aparición y lectura.






Películas

Director: Vadim Jean. Guion: Vadim Jean. Actores: David Jason, Marc Warren y Michelle Dockery.


Duración: 189 minutos. Director: Vadim Jean. Guion: Vadim Jean. Música: Paul E. Francis, David A. Hughes. Fotografía: Gavin Finney. Reparto: David Jason, Sean Astin, Tim Curry, Jeremy Irons, Brian Cox, James Cosmo, Christopher Lee y Terry Pratchett.


Duración: 185 minutos. Director: Jon Jones. Guion: Bev Doyle y Richard Kurti. Música: John Lunn. Fotografía: Gavin Finney. Reparto: Richard Coyle, David Suchet, Claire Foy, Andrew Sachs, Charles Dance, Timothy West y Terry Pratchett.

Animación

Videojuegos

Teatro

Series de TV

Pratchett fue uno de los primeros escritores en comunicarse con sus fanes vía internet y fue uno de los contribuidores de los grupos de noticias Usenet alt.fan.pratchett y alt.books.pratchett durante más de una década.

El interés de Terry Pratchett por los orangutanes no está sólo reducido al Bibliotecario, un popular personaje de Mundodisco. Ha estado trabajando para la "Orang-ute foundation" visitando Borneo con el canal 4 para rodar un capítulo de "Jungle Crew" en su hábitat. Siguiendo sus pasos, los fanes en las convenciones de mundodisco tomaron la "Orang-Ute" como beneficencia. A partir de ese momento, en cada convención a la que acude el autor se hace una subasta donde los fanes puedan pujar para que su nombre aparezca en el siguiente libro, todos los fondos recaudados se destinan a la "Orang-Ute".




</doc>
<doc id="2737" url="https://es.wikipedia.org/wiki?curid=2737" title="Taxón">
Taxón

En biología, un taxón o taxon (del griego "τάξις", transliterado como "táxis", «ordenamiento») es un grupo de organismos emparentados, que en una clasificación dada han sido agrupados, asignándole al grupo un nombre en latín, una descripción si es una especie, y un tipo. Cada descripción formal de un taxón es asociada al nombre del autor o autores que la realizan, los cuales se hacen figurar detrás del nombre. En latín el plural de taxón es "taxa", y es como suele usarse en inglés, pero en español el plural adecuado es «taxones» o «táxones». La disciplina que define a los taxones se llama taxonomía.

La finalidad de clasificar los organismos en taxones formalmente definidos en lugar de grupos informales, es la de proveer grupos cuya circunscripción (esto es, de qué organismos están compuestos) sea estricta y cuya denominación tenga valor universal, independientemente de la lengua utilizada para la comunicación. Nótese que los taxones existen dentro de una clasificación dada, sujeta a cambios y sobre la que pueden presentarse discrepancias; lo que obliga, respecto a ciertas denominaciones problemáticas, a especificar en el sentido de qué autor se está usando el nombre.

La definición aquí dada corresponde a la definición de taxón de la escuela cladista y la evolucionista, al que se le asignó un "nombre" según las reglas escritas en los Códigos Internacionales de Nomenclatura. Cada escuela de clasificación agrupa a los organismos de forma diferente, en la escuela fenética los taxones no son grupos de parentenesco sino de organismos con rasgos similares. En la escuela cladista solo son taxones los grupos monofiléticos (que agrupan un ancestro más todos sus descendientes), en la escuela evolucionista también pueden ser taxones los grupos parafiléticos. Se puede decir que en general, un taxón es un grupo de organismos asociado a un conjunto de atributos que determinan la pertenencia de esos organismos a ese grupo. En la escuela cladista, el conjunto de atributos son los caracteres heredados de su antecesor común.

En el sistema linneano de clasificación, cada taxón tiene asociada además una categoría taxonómica, que lo ubicaría jerárquicamente en un sistema de clasificación.

Los Códigos Internacionales de Nomenclatura proveen reglas con el objetivo de que los taxones tengan un y sólo un "nombre correcto", el nombre que debería ser utilizado para referirse al taxón. Para ello el autor debe publicar el nombre del taxón en una revista científica, que debe estar en latín, asociado a una categoría taxonómica, un "tipo", y una descripción si es una especie. El primer nombre publicado en regla es el "nombre correcto" del taxón. El nombre se aplica a un taxón que tenga en su circunscripción al tipo. En los Códigos, el taxón se define por su circunscripción, su posición (sus relaciones con otros taxones en un sistema de clasificación), y su categoría taxonómica.

Para clasificar los organismos, la taxonomía utiliza desde Carlos Linneo un sistema jerárquico. En este esquema organizativo, cada grupo de organismos en particular es un taxón, y el nivel jerárquico en el que se lo sitúa es su categoría. Análogamente, en geografía política: "país", "provincia" y "municipio" serían categorías, mientras que "Canadá", "Ontario" y "Toronto" serían los taxones. Del mismo modo: "familia", "género" y "especie" son categorías taxonómicas, mientras que Rosaceae, "Rosa" y "Rosa canina" son ejemplo de taxones de esas categorías.

En relación al taxón "especie", Ernst Mayr (1996) hace distinción entre "el taxón" y "la categoría taxonómica": 

La siguiente es una lista general (ordenada de lo general a particular) de categorías taxonómicas a las que se asocian los diversos taxones: 

Hay que notar, que la asociación de un taxón a un rango determinado (categoría), es algo relativo y restringido al esquema particular usado (sistema). Tanto es así, que es probable que un taxón ocupe categorías diferentes según los sistemas de clasificación (organizados por diferentes autores, criterios, etc.); generalmente, ello ocurre en el ámbito de las categorías más abarcativas (familia, orden, clase, etc.).

El estatus ontológico de los taxones es uno de los asuntos más discutidos en filosofía de la biología. Principalmente existen dos corrientes:


Existen dos tipos de taxones:


Se llama taxón monotípico al que sólo contiene un miembro de la categoría inmediatamente subordinada. Por ejemplo, una familia que sólo contiene un género, sin que importe cuántas especies contenga este.

La nomenclatura establece una terminología consensuada que permite saber, a partir del sufijo de un taxón cualquiera, cuál es su categoría taxonómica y dar cuenta de su posición en la jerarquía sistemática. La siguiente tabla muestra esa nomenclatura:

Por debajo de la categoría de género, todos los nombres de taxones son llamados «combinaciones». La mayoría reciben también una terminación latina más o menos codificada en función de la disciplina. Se distinguen varias categorías de combinaciones:





</doc>
<doc id="2739" url="https://es.wikipedia.org/wiki?curid=2739" title="Theales">
Theales

Las Theales son un orden en el Sistema Cronquist, más o menos primitivo, de las dilénidas (subclase Dilleniidae), a veces perianto y gineceo helicoidal; sistemática compleja, con un número de familias variable (las pasan a Violales). Gineceos poco evolucionados, de carpelos más o menos soldados en general cerrados, a gineceo plurilocular con placentación axial. Semillas con embrión grande y poco endosperma. Leñosas y con hojas simples.

En el sistema APG II (usado ampliamente aquí) los taxones involucrados se asignan a otros muchos órdenes diferentes: Ericales, Malvales, Malpighiales.



</doc>
<doc id="2740" url="https://es.wikipedia.org/wiki?curid=2740" title="Theaceae">
Theaceae

Theaceae, las teáceas, son una familia de plantas con flores perteneciente al orden Ericales. Es originaria de países subtropicales e intertropicales de América y Asia.

Son arbustos o árboles, generalmente siempreverdes; plantas hermafroditas, dioicas o ginodioicas. Hojas simples, coriáceas, alternas y perennes (hoja lauroide). Flores solitarias, actinomorfas, con dos envueltas (a veces helicoidales); cáliz ]).

Se divide en las siguientes tribus:


</doc>
<doc id="2741" url="https://es.wikipedia.org/wiki?curid=2741" title="Tamaricaceae">
Tamaricaceae

Tamaricaceae es una pequeña familia de plantas leñosas, arbustos o pequeños árbolillos del orden Caryophyllales. Consta de cinco géneros con alrededor de 100 - 120 especies, la mayoría de las regiones templadas y cálidas del viejo mundo; en terrenos algo húmedos y salinos.

Hojas simples y enteras, muy reducidas, imbricadas, escuamiformes o aciculares, alternas. Flores hermafroditas, actinomorfas, tetrámeras (Tamarix boveana) o pentámeras, hipoginas; reunidas en inflorescencias en espigas o en racimos, a veces aisladas. La corola tiene de 4 a 5 (6) pétalos libres, alternando con sépalos. Los estambres, a veces numerosos, se insertan en un disco carnoso necatarífero. El ovario es súpero. Los frutos son cápsulas; con dispersión anemócora. Semillas con numerosos pelos largos (vilano). 

 5 + 5


</doc>
<doc id="2742" url="https://es.wikipedia.org/wiki?curid=2742" title="Tilioideae">
Tilioideae

Tilioideae es una subfamilia de fanerógamas perteneciente a la familia Malvaceae.

Las tiliáceas son árboles y arbustos, raramente hierbas. Hojas alternas, simples, siempre cordadas, ordinariamente con estípulas caducas. Flores inconspicuas; hermafroditas; actinomorfas; tetrámeras o pentámeras; dialipétalas; numerosos estambres, a menudo fasciculados (poliadelfos); gineceo sincárpico, ovario súpero, con 2 - 10 carpelos cerrados. Inflorescencias en dicasio de dicasios, con metatopía, en algunos casos provistas de una gran bráctea membranosa. Frutos capsulares o nuciformes. Unas 400 especies, algunas de zonas templadas, pero la mayoría tropicales.


</doc>
<doc id="2746" url="https://es.wikipedia.org/wiki?curid=2746" title="Teligonáceas">
Teligonáceas

En el sistema de Cronquist de clasificación científica de los vegetales, las teligonáceas eran una familia monotípica de hierbas, conteniendo el género "Theligonum". Se caracterizaban por poseer hojas simples, opuestas o alternas. Flores unisexuales, de disposición monoica, monoclamídeas; las masculinas con 7-20 estambres y las femeninas con ovario ínfero. Fruto drupáceo. En los bordes de lagunas.

En la clasificación filogenética más reciente, se les considera parte de las rubiáceas.


</doc>
<doc id="2747" url="https://es.wikipedia.org/wiki?curid=2747" title="Trapa">
Trapa

Trapa es el único género de la familia monotípica Trapaceae, en el orden de las mirtales, que comprende cinco especies de hierbas acuáticas. Poseen hojas sumergidas lineares y flotantes arrosetadas, de peciolo hinchado, rómbicas y dentadas. Flores poco vistosas, flotantes, hermafrodita, regulares, tetrámeras y de ovario semiínfero. Frutos nuciformes, coriáceos, con 4 cuernos. Son nativas de las regiones templadas de Asia y África.
"Trapa bicornis"
"Trapa korshinskyi"
"Trapa litwinowii"
"Trapa maximowiczii"
"Trapa natans"


</doc>
<doc id="2749" url="https://es.wikipedia.org/wiki?curid=2749" title="Thymelaeaceae">
Thymelaeaceae

Las timeleáceas (Thymelaeaceae) son una familia cosmopolita del orden Malvales compuesta por 50 géneros y 898 especies.

Las especies incluyen principalmente árboles y arbustos, con algunas trepadoras y herbáceas.

Se caracterizan por hojas simples, enteras, alternas o raramente opuestas, sin estípulas. Flores generalmente hermafroditas, regulares, a menudo tetrámeras, con perianto doble, o a veces con los pétalos abortados; corola (junto con el hipanto) tubular o urceolado, levemente corolino, sin solución de continuidad con el cáliz; corola inexistente o reducida; androceo diplostémono con 4-8 estambres sobre los sétalos (y alternos); gineceo súpero, con 1-2 carpelos (que dan lugar a 1-2 lóculos); suelen estar agrupadas en inflorescencias pequeñas. Frutos nuciformes o drupáceos.

La familia está más diversificada en el hemisferio sur que en el norte con las mayores concentraciones de especies en África y Australia.Los géneros son predominantemente africanos.

Varios de los géneros tienen importancia económica. "Gonystylus" se cotiza por su dura y blanca madera. La corteza de "Edgeworthia" y "Wikstroemia" se utiliza como componente del papel.

"Daphne" se cultiva por el dulce aroma de sus flores. Las especies de "Wikstroemia", "Daphne", "Phaleria", "Dais", "Pimelea" y otros géneros se cultivan como ornamentales.

Muchas de las especies son venenosas si se comen.



</doc>
<doc id="2751" url="https://es.wikipedia.org/wiki?curid=2751" title="TVR">
TVR

TVR se puede referir a:


</doc>
<doc id="2752" url="https://es.wikipedia.org/wiki?curid=2752" title="Transporte">
Transporte

El transporte es un conjunto de procesos que tienen como finalidad el desplazamiento y comunicación. Para poder llevar a cabo dichos procesos se emplean diferentes modos de transporte (automóvil, camión, avión, etc.) que circulan por determinados medios (carreteras, vías férreas, etc.). 

Para lograr llevar a cabo la acción de transporte se requieren varios elementos, que interactuando entre sí, permiten que se lleve a cabo:


El sistema de transporte requiere de varios elementos, que interactúan entre sí, para la práctica del transporte y sus beneficios:


Los ingenieros de transporte utilizan estos conceptos a la hora de concebir, planificar, diseñar y operar un sistema de transporte. Para un sistema eficiente, es deseable que la demanda utilice al máximo la infraestructura existente. La demanda deberá solo en muy pocas ocasiones superar la oferta.

Uno de los ejemplos más ilustrativos es el de las vías. La oferta para este caso son las vías y los vehículos las demandan. Cuando pocos vehículos demandan la vía, se dice que la infraestructura está prestando un buen servicio, pero es ineficiente. Cuando muchos vehículos utilizan la vía de forma funcional, operarán de forma eficiente la infraestructura, pero el servicio que presta a los usuarios ya no es tan bueno. Cuando demasiados vehículos demandan las vías se forma congestión y esto se considera inaceptable.

El transporte puede ser clasificado de varias maneras de forma simultánea. Por ejemplo, referente al tipo de viaje, al tipo de elemento transportado o al acceso. Por ejemplo, el transporte de pasajeros generalmente se clasifica en transporte público y el transporte privado.

El transporte de carga es la disciplina que estudia la mejor forma de llevar de un lugar a otros bienes. Asociado al transporte de carga se tiene la Logística que consiste en colocar los productos de importancia en el momento preciso y en el destino deseado.
La diferencia más grande del transporte de pasajeros es que para este se cuentan el tiempo de viaje y el confort.

Esta clasificación es muy importante por las diferencias que implican los dos tipos de viajes. Mientras los viajes urbanos son cortos, muy frecuentes y recurrentes, los viajes interurbanos son largos, menos frecuentes y recurrentes.

Se denomina transporte público a aquel en el que los viajeros comparten el medio de transporte y que está disponible para el público en general. Incluye diversos medios como autobuses, trolebuses, tranvías, trenes, ferrocarriles suburbanos o ferris. En el transporte interregional también coexiste el transporte aéreo y el tren de alta velocidad.

El transporte público se diferencia del transporte privado básicamente en que:

El más representativo de los modos de transporte privado es el automóvil. Sin embargo, la caminata y la bicicleta también están dentro de esta clasificación. El taxi, pese a ser un servicio de acceso abierto al público, es clasificado como transporte privado.

El transporte escolar o transporte de estudiantes lleva a cabo viajes de niños y adolescentes desde los lugares de residencia hasta los colegios y vice versa. Pese a que muchos de estos viajes se llevan a cabo en medios de transporte privado, es también muy frecuente que se lleven a cabo de forma colectiva en buses y caravanas especiales para este propósito.

En Estados Unidos y otros países es habitual que se dediquen autobuses para llevar a los escolares de su lugar de residencia a la escuela. La normativa de Estados Unidos obliga a que un cuidador adulto, aparte del conductor, vaya en el autobús y que los autobuses no tengan más de 16 años.

En otros países como Alemania o Finlandia, los alumnos van solos en los vehículos del transporte público de la ciudad. Normalmente, los estudiantes reciben una tarjeta que les permite hacer uso de estos servicios por todo el semestre de forma ilimitada, a un costo muy bajo (subsidiado).

En el caso de un país Sudamericano, Chile posee una tarifa diferida para estudiantes a la que se puede optar presentando un Pase escolar al momento de cancelar el pasaje. Este a su vez permite optar a diferentes tarifas, siendo Educación primaria pasaje sin costo y Educación secundaria además de Educación superior un porcentaje del pasaje adulto (50 % en regiones y 33 % en Santiago). En Davis, Estados Unidos, más del 40 % de los niños y niñas van al colegio en bicicleta.

Los medios de transporte, se refieren a las infraestructuras necesarias para permitir los traslados entre orígenes y destinos del viaje (carreteras, vías férreas, manto acuífero y delimitaciones aéreas).

Los modos de transporte son combinaciones de vehículos y operaciones necesarias para llevar a cabo los desplazamientos. Incluyen el peatón, la bicicleta, el coche, el autobús, el camión, los ferrocarriles, el transporte fluvial y marítimo (barcos, canales y puertos), el transporte aéreo (aeroplanos, aeropuertos y control del tráfico aéreo), incluso la unión de varios o los tres tipos de transporte. Se habla de reparto modal para describir, en un ámbito dado, cómo se distribuyen los viajes entre los distintos modos que utilizan la infraestructura (similar o distinta: vías férreas y carreteras en la mancha urbana).

Según los modos de transporte utilizados, el transporte se clasifica o categoriza en:

Asimismo, puede distinguirse entre transporte público y transporte privado dependiendo de la propiedad de los medios de transporte utilizados.

También puede ser interesante la distinción entre el transporte de mercancías y el transporte de pasajeros.

La modelización de transporte o modelación de transporte permite planificar situaciones futuras y actuales del transporte urbano. El concepto de “modelo” debe ser entendido como una representación, necesariamente simplificada, de cualquier fenómeno, proceso, institución y, en general, de cualquier “sistema”. Es una herramienta de gran importancia para el planificador, pues permite simular escenarios de actuación y temporales diversos que ayudan a evaluar alternativas y realizar el diagnóstico de futuro.

El esquema clásico de modelación es el de cuatro etapas o cuatro pasos.

A veces, según los datos disponibles y el tipo de análisis que se desea se puede prescindir del modelo de generación, quedando en tres etapas y obteniéndose únicamente el modelo de distribución. En corredores de carreteras sin transporte público realmente competitivo, es frecuente suponer que no hay trasvase modal y sólo se use el de distribución (o un modelo de crecimientos) y el de asignación únicamente.

También se pueden mencionar otros tipos de modelos como los de usos del suelo que permiten análisis interrelacionados y complejos entre actividad en el territorio y transportes.

Las redes se diseñan considerando tres aspectos: la geometría, la resistencia y la capacidad. En la práctica, el diseño de transporte centra sus miras en tomar los diseños geométricos y definir su ancho, número de carriles, vías o diámetro. Su producto es tomado por el especialista en pavimentos, rieles, puentes o ductos y convertido en espesores de calzada, balasto, vigas o paredes de tubería. El ingeniero de transporte es también responsable de definir el funcionamiento del sistema considerando el tiempo.

Los principales métodos para el diseño de redes incluyen el método de las cuatro etapas, el uso de la teoría de colas, la simulación y los métodos que podrían llamarse de coeficientes empíricos.

En este método de modelización de transporte se calcula separadamente la "generación de viajes", o número de personas o cantidad de carga que produce un área; la "distribución de viajes" de viajes, que permite estimar el número de viajes o cantidad de carga entre cada zona de origen y destino; la "partición modal", es decir, el cálculo del número de viajes o cantidad de carga que usarán los diferentes modos de transporte y su conversión en número de vehículos; y, finalmente, la "asignación", o la definición de qué segmentos de la red o rutas utilizará cada uno de los vehículos.

Este proceso se realiza utilizando la densidad y la localización de población o de carga actual para verificar que los volúmenes previstos por el método estén de acuerdo con la realidad. Finalmente, se usan las estimaciones de población futura para recalcular el número de vehículos en cada arco de la red que se usará para el diseño. Se utiliza principalmente para diseñar el transporte y es exigido por ley en muchas zonas urbanas.

Utiliza la estadística y ciertas asunciones sobre el proceso del servicio. Permite estimar, a partir de las tasas de llegada de los clientes (ya sean vehículos o personas) y de la velocidad de atención de cada canal de servicio, la longitud de cola y el tiempo promedio de atención. La tasa de llegada de los clientes debe analizarse para conocer, no solamente su intensidad en número de clientes por hora, sino su distribución en el tiempo. Se ha hallado, experimentalmente, que la distribución de Poisson y las distribuciones geométricas reflejan bien la llegada aleatoria de clientes y la llegada de clientes agrupados, respectivamente. Se utiliza principalmente para la estimación de número de casetas de peaje, surtidores en estaciones de combustible, puestos de atención en puertos y aeropuertos y número de cajeros o líneas de atención al cliente requeridas en un establecimiento. La teoría de colas se basa en procesos estocásticos...

Existen dos tipos principales de simulaciones en computador utilizadas en la ingeniería de transporte: macrosimulaciones y microsimulaciones.

Las macrosimulaciones utilizan ecuaciones que reflejan parámetros generales de la corriente vehicular, como velocidad, densidad y caudal. Muchas de las ideas detrás de estas ecuaciones están tomadas del análisis de flujo de líquidos o gases o de relaciones halladas empíricamente entre estas cantidades y sus derivadas.

Las segundas simulan cada vehículo o persona individualmente y hacen uso de ecuaciones que describen el comportamiento de estos vehículos o personas cuando siguen a otro (ecuaciones de seguimiento vehicular) o cuando circulan sin impedimentos.

Utilizan ecuaciones de tipo teórico pero, en general, parten de mediciones que indican la capacidad de una red en condiciones ideales. Esta capacidad, normalmente, va disminuyendo a medida que la red o circunstancias se alejan de ese ideal.

Los métodos proporcionan coeficientes menores que la unidad, por los que se debe multiplicar la capacidad "ideal" de la red para encontrar la capacidad en las condiciones dadas. y eso más cosas

El transporte y la comunicación son tanto sustitutos como complementos. Aunque el avance de las comunicaciones es importante y permite trasmitir información por telégrafo, teléfono, fax o correo electrónico, el contacto personal tiene características propias que no se pueden sustituir.

El crecimiento del transporte sería imposible sin la comunicación, vital para sistemas de transporte avanzados (control de trenes, control del tráfico aéreo, control del estado del tránsito en carretera, etc.). No existe, sin embargo, relación probada entre el crecimiento de estos dos sistemas. El mejor previsor del crecimiento de un sistema de transporte es el crecimiento del producto interno bruto (PIB) de un área. Resulta, además, relativamente fácil encontrar predicciones del PIB. La utilización de series históricas para predecir el crecimiento futuro del sistema de transporte puede llevar a serios errores (problema de la "suboptimización" o de análisis fragmentario de un sistema).

El transporte y el uso de la tierra están relacionados de manera directa. Dependiendo del uso de la tierra se generan actividades específicas que no necesariamente coinciden con el lugar de residencia de quienes las desarrollan, en cuyo caso se deben trasladar. Una jornada puede ser dividida entre el tiempo gastado en actividades y el tiempo gastado viajando desde y hacia el lugar en el cual se desarrollan tales actividades. Se dice que el transporte es "una demanda indirecta", dado que carece de fin en sí mismo, pero es necesario para desarrollar las actividades en el sitio de destino.

La agrupación de una variedad de actividades dentro de la misma zona terrestre minimiza la necesidad del transporte. Por el contrario, la organización por zonas de actividades exclusivas la aumenta. Sin embargo, hay economías de escala al agrupar actividades, lo que impide una organización de actividades por zonas completamente heterogéneas.

También el transporte y el uso de tierra actúan recíprocamente de otro modo, dado que los servicios de transporte consumen tierra, al igual que las ciudades. Un sistema de transporte eficiente puede minimizar el uso de la tierra. Sin embargo, este ahorro debe ser comparado con el coste; un sistema de transporte eficiente en una ciudad grande puede tener un coste sumamente elevado.

El transporte es un consumidor importante de energía, la puede obtener mediante la quema de combustibles, hasta no hace mucho mayoritariamente fósiles en motores de combustión. En el proceso de combustión se generan emisiones gaseosas contaminantes (CO, CO, NO, SO y otros, como partículas) cuya nocividad depende de la fuente de energía usada.

Suele sostenerse que los vehículos eléctricos impulsados son "limpios", al igual que aquellos que usan celdas de hidrógeno. Pero, en realidad, depende de la fuente de la que provenga la electricidad. Si usan electricidad producida en centrales alimentadas por combustibles fósiles, la contaminación es más localizada que con los coches de combustión, ya que pueden aplicarse técnicas de captura y almacenamiento de carbono. Si se utilizan fuentes renovables (electricidad renovable) no existe este problema de emisiones.

Dado que se prevé el agotamiento de combustibles fósiles hacia el 2050, el transporte mundial enfrenta el reto de modificar completamente sus sistemas en algo menos de cinco décadas. Se prevé que los vehículos de hidrógeno serán los más económicos, si se extrapolan las tecnologías actuales, con lo cual deberemos aprender a producirlo por otros métodos distintos del altamente contaminante que se usa hoy en día (tratamiento de gas natural con vapor), que genera inmensas cantidades de dióxido de carbono, si queremos que su uso no contribuya aún más al calentamiento global.

Durante los últimos años los vehículos han estado haciéndose más limpios, como consecuencia de regulaciones ambientales más estrictas e incorporación de mejores tecnologías, (convertidores catalíticos, etc.), y, sobre todo, por un mejor aprovechamiento del combustible. Sin embargo, esta situación ha sido más que compensada por la subida tanto del número de vehículos como del uso creciente anual de cada vehículo, lo cual determina que ciudades con más de 1.000.000 de habitantes presenten problemas de índices de contaminación atmosférica excesivos, afectando la salud de la población.

En el año 2009 la NASA promovió el denominado Desafío Vuelo Verde, un concurso por el que se premia con medio millón de dólares al proyecto más original y eficiente. Más de media docena de proyectos se han presentado hasta el momento. La iniciativa se desarrolla en cooperación con las siguientes instituciones norteamericanas: Departamento de Agricultura y Energía, la Agencia de Protección Ambiental, la Fundación Nacional de Ciencia y el Instituto Nacional de Comercio de Estándares y Tecnología, junto con la Oficina de Patentes y Marcas. Algunos de estos proyectos son:

Cri-Cri
Un aeroplano acrobático de fabricación francesa y completamente eléctrico. Funciona con cuatro motores y tiene una autonomía de vuelo de 30 minutos a una velocidad de crucero de 100 km/h.

SugarVolt
Modelo híbrido diseñado por Boeing. Funciona con una combinación de turbinas (de hélice) de queroseno, que son las que lo hacen despegar. Pero, una vez en el aire, al necesitar menos energía, vuela con un motor eléctrico.

Skyhawk 172
La firma aeronáutica Cessna prevé tener lista para finales de 2011. El avión de pequeña envergadura dispondrá solamente de dos asientos, pero aun así se estima que sea uno de los más usados debido a su parecido con un modelo anterior, al que se le han incluido mejoras.

El Superboeing
No todos los aviones ecológicos serán eléctricos. Este es un prototipo supersónico de Boeing que funciona con combustible de alto rendimiento, y que está siendo diseñado en colaboración con la NASA. Además en cuanto al medio aéreo se plantea otro hándicap a tener en cuenta. En la actualidad las innovaciones que se han hecho en los aviones acotan en demasía los vuelos comerciales. Es decir el mercado se cerca. En su gran mayoría son jets privados de pocas plazas que no permiten el transporte masivo de pasajeros, por lo tanto, la problemática inicial de hacer factible una opción poco contaminante al gran público, en este caso, es imposible, por el momento.

El transporte y la distribución de la energía por medios de transporte han ocasionado múltiples accidentes que han afectado gravemente a personas, instalaciones y medio ambiente. El transporte de la energía varía dependiendo del tipo de energía a transportar.

El medio también causa impactos importantes sobre el sistema energético; cabe destacar el efecto de los terremotos, huracanes, tormentas, variaciones bruscas de temperatura, etc.





</doc>
<doc id="2754" url="https://es.wikipedia.org/wiki?curid=2754" title="Tropaeolaceae">
Tropaeolaceae

La familia de las tropeoláceas (Tropaeolaceae) comprende tres géneros de plantas dentro del orden de las brasicales.
Son hierbas anuales o perennes, muchas veces subsuculentas, escandentes o raramente procumbentes, a veces con rizomas tuberosos; plantas hermafroditas. Hojas alternas; láminas enteras, lobadas o palmadamente divididas, peltadas o subpeltadas, palmatinervias; pecíolos largos, normalmente del mismo largo de la lámina o más largos; estípulas presentes o ausentes.

Flores comúnmente solitarias, axilares, vistosas, marcadamente irregulares o a veces subactinomorfas ("Trophaeastrum"), con pedúnculos largos y péndulos o erectos ("Trophaeastrum"); sépalos 5, libres, imbricados, uno de ellos en general largamente espolonado; pétalos 5, libres, imbricados, unguiculados, los 2 superiores usualmente más pequeños que los 3 inferiores, enteros, serrados o lobados, ciliados o no; estambres 8, filamentos libres, anteras pequeñas, basifijas, con dehiscencia longitudinal; pistilo simple, estilo delgado, estigma seco, 3-lobado, ovario 3-locular, con 1 óvulo por lóculo.

Fruto un esquizocarpio con 3 mericarpos o sámaras ("Magallana"); semillas con un embrión grande y recto y 2 cotiledones gruesos, endosperma ausente.



</doc>
<doc id="2757" url="https://es.wikipedia.org/wiki?curid=2757" title="Taxaceae">
Taxaceae

Las taxáceas (nombre científico Taxaceae) están mayormente compuestas por una capa sólida en la parte exterior de la hoja, son diminutas ya que por su tamaño pueden hacer mayores funciones. Hojas lineales,largas, aplanadas, enteras, agudas en el ápice, son de óvulos solitarios sin conos, y semillas con una sólida y pequeñas cubierta externa dura asociada a un arilo carnoso usualmenste brillantemente colorido. Son mayormente del Hemisferio Norte y Sur, ya que por la evolución de estas se han ido dispersando. 

Árboles de tamaño pequeño a moderado o arbustos, usualmente no resinosos o ligeramente resinosos, con fragancias o no. Madera sin canales de resina. Hojas simples, persistentes por muchos años, que se desprenden de a una, dispuestas en espiral (opuesta en una especie), muchas veces torcidas de forma de parecer dísticas, linealesa, aplanadas, enteras, agudas en el ápice, con 1 o ningún canal de resina. Dioicos (raramente monoicos). Estróbilos de microsporangios con 6-14 microsporofilos, microsporangios 2-9 por microsporofilo, arreglados radialmente alrededor del microsporofilo o limitados a su superficie abaxial, polen sin "sacca". Óvulos solitarios y sin conos, semillas con una cubierta exterior dura, asociada a una arilo carnoso usualmente brillantemente coloreado, cotiledones 2 (ocasionalmente 1 o 3).

Mayormente del Hemisferio Norte, extendiéndose desde el sur de Guatemala hasta Java, con un género endémico de Nueva Caledonia. 

Tienden a crecer en sitios húmedos del fondo de los valles donde se acumula la hojarasca.

Taxaceae es único entre las coníferas en que su semilla solitaria no está asociada a escama del cono. El arilo es un crecimiento del eje bajo la semilla. Algunos sistemáticos han removido a esta familia de las coníferas porque no tiene cono, pero la embriología, la anatomía de la madera, la química, y la morfología de la hoja y del polen atan a esta familia incuestionablemente a las demás coníferas. El cono se piensa que se ha perdido, y la semilla solitaria con el arilo es por lo tanto un carácter derivado.

Las secuencias de ADN, la morfología, la anatomía, y la química de los alcaloides dividen la familia en dos clados, uno incluyendo a "Taxus", "Austrotaxus" y "Pseudotaxus", y el otro comprendiendo a "Torreya" y a "Amentotaxus". La familia está aparentemente más cercanamente emparentada a (y puede ser parafilética sin) Cephalotaxaceae, una familia monogenérica del este de Asia que tiene óvulos pareados asociados a un pequeño crecimiento considerado una escama del cono reducida y que crece a lo largo del eje del cono. Usualmente sólo una de las dos semillas madura, y desarrollan una cubierta externa jugosa que se asemeja, pero no es homóloga a, el arilo de las taxáceas. Las semillas solitarias y como drupas de muchas Podocarpaceae también se parecen a las semillas ariladas de las taxáceas, pero los datos de secuencias de ADN indican que la carnosidad apareció más de una vez (Stefanovic et al. 1998).

La clasificación, según Christenhusz et al. 2011, que también provee una secuencia lineal de las gimnospermas hasta género:


"Taxus" es ampliamente cultivado como ornamental y para madera fina en Norteamérica y Europa. Es una de las maderas de coníferas más finas, ahora utilizada en amoblamientos de alta calidad. "Torreya" es menos importante como ornamental, pero su madera, su semilla comestible, y el aceite de su semilla son valorados en Asia. 

"Taxus" contiene taxol, uno de los muchos altamente venenosos alcaloides de las hojas, tallos, y semillas. La potente actividad antimitótica del taxol hace que tenga uso en la quimioterapia del cáncer.



</doc>
<doc id="2758" url="https://es.wikipedia.org/wiki?curid=2758" title="Tauromaquia">
Tauromaquia

La tauromaquia (del idioma griego ταῦρος, "taūros" 'toro', y μάχομαι, "máchomai" 'luchar') se define como: «"el arte de lidiar toros"», tanto a pie como a caballo, sus antecedentes se remontan a la Edad de Bronce. La tauromaquia reúne el concepto y las reglas que definen el arte de lidiar o toreo, un arte que nació en España del que se tiene constancia en el siglo con la celebración de festejos taurinos en Ávila y en Zamora en el siglo . La forma más conocida de tauromaquia es la corrida de toros cuya expresión más moderna surgió en el siglo . La Tauromaquia es además el nombre que reciben las obras o libros que tratan sobre la misma y en los que se desarrollan dichas reglas del torero.La tauromaquia en sus diferentes modalidades está presente en Europa, donde se celebran corridas de toros en España, Portugal y en algunos departamentos del sur de Francia. En Hispanoamérica se realizan corridas en México, Colombia, Perú, Ecuador, Venezuela. En otros países como en China, Filipinas y Estados Unidos también se han celebrado corridas de toros pero en menor número. En otras partes del mundo hay otros tipos de festejos taurinos como los toros a la Tica o Fiestas de Zapote en Costa Rica, el Jallikattu también conocido como Eruthazhuvuthal o Manju Virattu que se practica en Tamil Nadu (India).

La tauromaquia incluye además de todos aquellos espectáculos relacionados o vinculados con el toro, el conjunto de tradiciones, fiestas y festejos populares con el toro como protagonista. Estas actividades abarcan desde la cría del toro de lidia por parte de las ganaderías bravas, las técnicas del toreo y aquellas actividades relacionadas directamente con el mismo como la confección de los vestidos de torear tanto de toreros como de banderilleros y picadores, muletas, capotes de brega y guarnicionería. Engloba también el diseño gráfico de los cartel taurino y otras manifestaciones culturales en torno al mundo del toro como la literatura, las artes plásticas con sus variaciones según los lugares donde se producen y que son parte de la cultura nacional.

La tauromaquia es originaria de España y se remonta a la edad de bronce, en donde solo la realeza era digna de demostrar su valentía frente a un toro, al contrario de lo que se cree, el rejoneo es la expresión más antigua, los escritos datan del año de 1455 en España. Y esto no sería posible sin el toro bravo. Estas historias se entrelazan de tal manera en la que se cree que los primeros enfrentamientos fueron con los uros animales de caza que a pesar de no ser una raza endémica de España fue allí donde se encontró uno de los mayores asentamientos.

El "Diccionario de Autoridades" publicado entre 1726 y 1739 contó con un total de seis volúmenes y su prólogo fue encargado a Juan Isidro Fajardo regidor de Madrid y oficial de la secretaría del Despacho de Hacienda. La realización de las entradas de la letra "Té" del diccionario (sexto volumen) fue encargada a Jerónimo Pardo al no realizar este el encargo, el trabajo fue encomendado a José Cassani en 1728 y a Lorenzo Folch Cardona quien lo dejó en 1730. Fue Lope Francisco Hurtado de Mendoza y Figueroa quien el 22 de abril de 1732 finalizó los lemas entre la "ta" y la "te", entre las que no figuraba la voz "tauromaquia." El lema "tauromaquia" no fue incluido hasta 1817 en el que se definió «"El arte de lidiar y matar los toros"», y apareció en la quinta edición del "Diccionario de la lengua castellana por la Real Academia Española" , donde también fue incluida la definición de "arte" como: «"conjunto de preceptos y reglas para hacer bien alguna cosa"».

Los antecedentes de los ritos con toros se remontan a la Edad de Bronce. En las culturas de la antigüedad el toro ha sido un símbolo importante como elemento identificador de ritos y sacrificios de animales cuyo fin era favorecer la fuerza de los guerreros o la fertilidad del ganado; también fue frecuente su empleo en las ofrendas, ceremonias funerarias o rituales de paso. De estas antiguas tradiciones existen vestigios procedentes de culturas como la indo-iraní, mesopotámica, egipcia y europea, entre todas ellas las referentes a la península ibérica tienen relevancia por su relación directa con las tradiciones taurinas que desembocaron en la tauromaquia o toreo, tradiciones culturales que fueron más tarde llevadas a otros países, como Portugal, Francia, México, Perú, Colombia, Venezuela o Ecuador –donde se mantienen tradiciones propias–.

Los vestigios de Baleares muestran hallazgos de tipo argárico y de la cultura talayótica similares a los existentes en Creta en donde se dieron cultos al toro. Del periodo de la Edad del Bronce son las cabezas de toro encontradas en Costig (Palma de Mallorca).

Para los celtas el toro era la representación de la fuerza y la virilidad según los diferentes testimonios de la mitología como la ceremonia "Tarbhfhess" —también conocida como la Fiesta del Sueño del Toro— irlandesa o la ceremonia de la Recolección del Muérdago del Roble descrita por Plinio el Viejo.

Diodoro Sículo relató como, entre el 140-139 a. C. Numancia debía pagar un tributo a Roma con el fin de mantener la paz con el imperio, entre otros bienes se incluían 3 000 pieles de bueyes procedentes de las ganaderías celtíberas. El toro fue además un elemento importante en las ofrendas funerarias celtas y en la representación plástica, donde destacan los toros o verracos hallados junto a estelas como la de Clunia, en la que se representa una escena taurina en la que un guerrero lucha con un toro. 

Los hallazgos de numerosas piezas artísticas en la península ibérica relacionadas con rituales y ceremonias con el toro son numerosos y se encuentran en prácticamente toda la geografía peninsular, sin embargo el más importante de todos estos hallazgos es el de la Necrópolis de Medellín (Badajoz), en concreto es una placa de marfil que perteneció a un ajuar funerario datado entre el 650-500 a.C. En la placa de estilo sirio está representada una escena de tauromaquia de la mitología fenicia, se trata del héroe Melqart que, con una rodilla genuflexa, apuntilla a un toro en la testuz. La pieza guarda relación con otras similares procedentes de otras culturas mediterráneas como la hitita, la siria o la cretense del La importancia del hallazgo estriba en la relación entre los cultos al toro y las diferentes civilizaciones donde estos tuvieron lugar, entre todas la de Tartessos, en el entorno de la antigua ciudad Gadir, verificar así mismo la presencia del toro en la península ibérica así como la relación que estos ritos antiguos tienen con las fiestas de los toros y tauromaquia.

En la tradición taúrica de la cultura griega, uno de los mitos más conocido es el del rey Gerión quien, según explica José María de Cossío: «... "tuvo rebaños de toros y vacas en la península ibérica..".» reses que pastaron junto al río Guadalquivir, en la Bética, donde surgieron las primeras ganaderías y encastes de reses bravas andaluzas siglos más tarde. Si bien no es el único indicio sobre ritos y celebraciones con toros en la antigua Grecia, en los hallazgos de Micenas se muestras varias escenas, en estucos, de saltos sobre toros incluidas algunas mujeres frente a toros en actitud de embestir. 

En las descripciones de Plinio y Seuterio se detallan los juegos de toros durante el en los que jinetes perseguían a las reses hasta alcanzarlas para luego derribarlas cogiéndolas por las astas. Juegos que se mantuvieron durante cuatro siglos. En el Mediterráneo oriental se dieron una forma de hostigamiento de toros orientado al sacrificio, y en Atenas se realizaron los festivales de Haola, similar a la corrida de toros, para honrar a Dionisos. Fueron dichos juegos los que César importó a Roma desde Tesalia. 

Algunos indicios revelan el empleo del toros en la guerra, uno de los escasos testimonios lo narra Polibio sobre las campañas bélicas del Ager Falernus llevadas a cabo por Aníbal en Falerno. El cartaginés se sirvió de mercenarios íberos acompañados por unos dos mil toros que portaban sarmientos encendidos sobre las cornamentas para abrirse camino entre las líneas enemigas. Sobre esta estrategia Diodoro manifestó que Amílcar Barca la había empleado en el desastre de Heliké —sobre 'Heliké' los historiadores discrepan sobre la ubicación de la antigua ciudad—, donde el general falleció. Siguiendo las explicaciones de José María Cossío, estos dos testimonios se asocian con el origen de festejos como el toro embolado, que aún se celebran en fiestas de España.

En la antigua Roma las celebraciones de fiestas con toros fueron introducidas por Julio César a su regreso de Tesalia donde eran habituales, estas actividades aparecían representadas en las monedas romanas. Las fiestas eran anunciadas en carteles al público y se celebraban en los anfiteatros donde se podían observar a los lidiadores entre los que hubieron algunas mujeres. 

Algunas de estas fiestas se realizaron entre el y el como parte de las celebraciones en honor a Mithra por parte de los legionarios romanos según indica el historiador Duris, bajo el nombre de "Taurobolios." Estos indicios guardan relación con los festejos taurinos y algunos ritos realizados en Hispania. Entre los muchos antecedentes de estos rituales se hallan el "Taurobolio" del yacimiento arqueológico de la villa romana de Arellano conocida como Villa de las Musas, y el de la fundación de la colonia "Iulia Augusta Faventia Paterna Barcino" (Barcelona) fundada en el 12 a.C. por Octavio Augusto para albergar las celebraciones rituales de los "Taurobolios".

Según el estudio de Pedro Sáez, se encuentran antecedentes de dichas tradiciones con toros en los "damnati ad bestias" en tiempos del emperador Nerón en los que se arrojaron a los cristianos durante las ejecuciones públicas efectuadas en la época de su persecución. Sin embargo los espectáculos taurinos en tiempos del imperio romano también incorporaban luchas entre fieras con enfrentamientos entre toros, osos, panteras, elefantes entre otros animales salvajes. Las actividades más frecuentes fueron los saltos del toro con pértiga, mencionados en el Código de Justiniano como los "Contomonobolon", al igual que la taurcatapsia o "taurokathapsia," un antecedente claro del salto del toro con la garrocha, una suerte del toreo realizada en el en las plazas españolas.

En los inicios de la Edad Media los testimonios documentados en torno a la tauromaquia indican que las fiestas y juegos de toros ya estaban asentados en la Península ibérica procedentes de los antiguos rituales con toros en los que se practicaron diferentes formas de burlar a las reses. 

Las informaciones sobre tauromaquia durante el periodo visigodo y en los primeros tiempos del califato omeya son escasas, José María de Cossío en "Los toros, volumen I," comentó la existencia de actividades taurinas basada en una carta datada en el año 618, publicada en el tomo VII de "España Sagrada", del rey Sisebuto al obispo de Barcelona Eusebio donde este le cuestionaba su afición a los toros. Esta carta fue recopilada por José de Vargas Ponce en la obra "Disertación sobre las corridas de toros" en 1807. 

Otras referencias sobre las fiestas de toros son las celebradas en Oviedo con motivo de la convocatoria, por parte del rey asturiano Alfonso II el Casto, de las Cortes en el año 815, información recogida en la "Crónica de Alfonso X". A estas referencias se añaden las de las fiestas reales de los toros del año 1080 en Ávila, celebradas para los desposorios del infante Sancho de Estrada con doña Urraca de Flores. A partir del hubieron fiestas en las que se corrieron toros en Castilla, León, Navarra, Aragón, Asturias y Galicia según consta entre los poemas de tema taurino más antiguos "El clérigo embriagado," incluido en la obra "Los milagros de nuestra Señora" de Gonzalo de Berceo, y la obra anónima "Cantar de los siete infantes de Lara" donde se narran las bodas de doña Llambla que se celebraron con corridas de toros. De este periodo también son las "Siete partidas" del rey Alfonso X de Castilla por las que se prohibía lidiar toros por dinero a los "matatoros".
En 1215 según las pautas marcadas en el IV concilio de Letran se prohibió la asistencia y participación del clero en estos eventos, sin embargo la costumbre festiva de correr los toros se continuó practicando en diferentes localidades. Costumbre que tuvo su origen por un lado en la adopción de algunos ritos asociados a la fertilidad, por lo que fue frecuente correr y lidiar toros en las celebraciones de los esponsales, un ejemplo se relata en las "Cántigas de Santa Maria" (1280), cántigas XXXI, XLVIII y CXLIV. En la CXLIV está representada una corrida de toros nupcial, tradición palentina del en la que se corría un toro por parte del novio hasta casa de la novia usando una capa para atraer la res. 

Por otro lado durante el surgieron los caballeros alanceadores a caballo que burlaban diferentes fieras como ejercicios de entrenamiento tanto para ejercitar las monturas como para la práctica de ejercicios militares. También fueron frecuentes las corridas votivas celebrabas con motivo de las promesas o favores solicitados por algunos de los participantes, se tienen datos sobre este tipo de festejos en Salamanca. La iglesia vinculó estas prácticas de las fiestas y juegos de toros a los antiguos ritos paganos de forma que dieron pie a prohibiciones posteriores. Con estas prácticas, la mayoría eran públicas, obtenían las habilidades necesarias para las batallas durante la reconquista. Realizadas por los caballeros de la nobleza dichos ejercicios consistían en torneos, juegos de cañas y sortijas, en los que se ofrecían combates con toros, cuyo objetivo era dominar la bravura del mismo, un reto para caballeros nobles y monarcas participantes. Estas prácticas dieron lugar por un lado a la lidia a caballo o rejoneo, y por otro a los festejos taurinos populares: encierros y corridas de toros, bases de la tauromaquia.

Durante la Edad Media aumentaron los festejos taurinos celebrados en las plazas públicas, adecuadas para los festejos, para agasajar a reyes y nobles en sus visitas a ciudades españolas con motivo de bodas, nacimientos y cumpleaños reales o celebraciones conmemorativas. Fueron aficionados desde Luis VII de Francia, Alfonso VI, Alfonso VII, el rey navarro García Ramírez o Pedro I. Menos claras son las afirmaciones que realizó Nicolás Fernández de Moratín en la "Carta histórica sobre el origen y progresos de las fiestas de toros en España" (1777), donde se atribuye a Rodrigo Díaz de Vivar ser el primero en lancear toros a caballo, estas afirmaciones fueron discutidas por Ramón Menéndez Pidal, que fue director de la Real Academia Española y por el conde Colombí, que fue presidente de la Unión de Biblófilos Taurinos en varias cartas recogidas por José Alameda en su obra "El hilo del toreo", donde se concluye que si bien pudo darse la información no ha podido comprobarse, mediante documentos, que el Cid alanceara toros. 

Pascual Millán en "La escuela de Tauromaquia de Sevilla," recoge el dato sobre una corrida de toros celebrada en Pamplona, en 1385 ordenada por Carlos II de Navarra, para la que se contrató a dos lidiadores venidos de Zaragoza y se les abonó cincuenta libras; la información fue hallada en los documentos de contaduría de la Real Colegiata de Roncesvalles. En 1387, durante el reinado de Juan I de Aragón, tuvo lugar la primera corrida de toros en Barcelona en la plaza del Rey, según se recoge de forma oficial en el Archivo General de la Corona de Aragón, que se encuentra en Barcelona.

Desde el las referencias sobre la tauromaquia son más frecuentes. La celebración de los diferentes tipos de fiestas, religiosas o no, tuvieron en este periodo un papel importante en la convivencia social del , momento en el que surgieron los modelos de fiestas dentro del concepto nación surgido en los comerciantes que residían fuera del reino. Algunas de estas celebraciones en las que participaba toda la comunidad en ocasiones resultaron perjudiciales para determinados intereses, razón por la cual surgieron diferentes regulaciones locales por parte del clero. La nobleza incluía entre las celebraciones y banquetes festivos justas, juegos y corridas de toros que se realizaban en las ciudades con el fin de dar muestras de la ostentación de su posición. Estas celebraciones cumplían además la función de social la de unir a la comunidad. 

Se tiene constancia sobre corridas de toros realizadas en Sevilla con motivo de la visita que 1405 realizó a la ciudad Enrique III; en Toledo entre 1431 y 1432 con motivo del regreso de Juan II de Castilla de de la batalla de Andalucía, se celebraron toros y justas en la plaza de Zocodeñe, conocida más tarde como Zocodover, estos fueron las primeras corridas de toros celebradas en Toledo. La visita de Enrique IV a Madrid en 1469 también fue una ocasión celebrada con toros en la Casa Real de El Pardo a la que asistieron los embajadores de Francia e Inglaterra. En 1492 con motivo de las celebraciones de la toma de Granada se realizaron festejos taurinos entre otros actos.

La organización de las corridas de toros que ya eran lidiadas por toreros a pie, tuvieron importantes costes ya que era necesario adecuar las calles y plazas con cercados y engalanarlas para la ocasión. De la adquisición de las reses se encargaba el consejo que imponía a los carniceros locales la reserva de las mismas. En algunas ciudades castellanas como Valladolid o Palencia la entrega de los toros era obligatoria y los carniceros debían entregarlos o tenerlos a disposición para las celebraciones festivas en cualquier momento. En 1490 la oposición a la entrega gratuita de las reses por parte de los carniceros segovianos ocasionó que la comunidad tuviese que abonar, de ahí en adelante, el coste de los toros a los carniceros. El montante de las reses llegó a suponer más de la mitad del presupuesto total de las fiestas en Sevilla entre 1453 y 1526, cada uno de los animales tenía un coste de entre 3 000 y 4 000 maravedíes. En las actas de los ayuntamientos quedaban anotadas las cuentas, los encargados de la organización, los nombres de los útiles de lidiar y las localidades donde se realizan las celebraciones.

A partir del siglo XV, la nobleza había abandonado el rejoneo para dejar paso a los toreros a pie, que lidiaban en recintos específicos cerrados, lo cual significó un mayor riesgo para los lidiadores y un aumento de la exigencia por parte del público del valor que debían mostrar los toreros. estos cambios iniciaron el recorrido hacía la tauromaquia profesional que llegará hasta tiempos contemporáneos. En 1554 este nuevo concepto de lidiar se conoce ya como corrida de toros. Impresionada por el riesgo que suponían los toros para los lidiadores, Isabel I de Castilla, "la Católica", ordenó que las astas de los toros fuesen enfundadas en otras de forma que no pudiesen herir a los toreros, la medida no prosperó por la dificultad que suponía enfundar los toros. En 1554 el nuevo concepto de lidiar se conoce ya como corrida de toros y aparece como tal en las publicaciones de la época.

A partir del se inició el proceso que formó la tauromaquia clásica de manos de los nuevos toreros, este proceso duró hasta el siglo XVII, para consolidarse definitivamente en la tauromaquia moderna, en el Siglo XVIII Luis Zapata en su obra sobre los valores y comportamientos sociales de su época, "Miscelánea (o Varia Historia)" (1583-1595), menciona como le pueblo lentamente se fue haciendo con la fiesta desplazando a la nobleza hasta quitarle el protagonismo en el Siglo XVIII. Zapata además escribió los tratados "Excelencias de la Gineta (sic.)", "Uso del rejón" y "Advertencias sobre el método de correr cañas", de los cuales no se conservan ejemplares, salvo las referencias incluidas en "Memorial Histórico Españo"l de Pascual de Gayangos. En el estudio de Zapata se describe al Emperador Carlos I alanceando toros en Toledo y Valladolid; y se sabe que lanceó un toro en la celebración del nacimiento de su hijo Felipe II en 1527. Quien fuera cortesano del emperador Pero Ponce de León, hermano del primer duque de Arcos, actuó en varias ocasiones ante la familia real en Ávila, en Medina del Campo y en Sevilla, donde rejoneó y se acompañó con pajes mulatos para asistirle durante la lidia, Ponce de León empleó la capa para burlar al toro, fue uno de los matadores de toros más conocidos de la España renacentista y un renovador de la técnica de alancear esperando al toro en un caballo con los ojos vendados, al que desviaba un par de pasos hacia la izquierda durante la embestida del toro. Su abuelo el Marqués de Cádiz ya había organizado la lidia de varios toros delante del castillo de Rota con motivo de la visita de los Reyes Católicos. Él hizo lo propio en la plaza situada delante de su casa en lo que luego fue convento de la Encarnación en Sevilla. De su afición a las Musas dan fe la educación que proporcionó a sus hijos, el poeta Luis Ponce de León y el humanista Gonzalo Mariño de Ribera y Ponce de León, y el que Juan de Quirós, el mejor poeta sevillano de la época, le dedicara un poema en latín, del que solo conocemos los tres primeros versos copiados por su discípulo Benito Arias Montano.

Otra de las informaciones que aporta Zapata en su estudio es la mención de la existencia del nombre de los toros anterior al , dato de Zapata mencionado por Ignacio R. Mena Cabezas en "Caballeros, toros y toreros en el ."

En la plaza mayor de Madrid se celebraban dos tipos de corridas de toros: las usuales, en las que asistía el hombre de a pie, y las reales, reservadas a selectos personajes de la Corte. Las primeras se organizaban por el Concejo de la Villa, las segundas por los encargados del protocolo y fiestas de la Corte: Mayordomía Real, y por regla general eran más lujosas. Se solían celebrar las corridas populares sin fecha fija en torno a las fechas de San Juan (junio), en Santa Ana (agosto), posteriormente las de San Isidro (mayo) y las de San Pedro y San Pablo.

En Sevilla el toreo moderno surgió cuando los toros eran guiados por las calles de la ciudad hasta el matadero de la calle San Bernardo, estos se convirtieron en encierros en los que mozos corrían delante de los toros. Antes de ser los toros sacrificados, los mozos solían practicar las diferentes suertes y pases en los corrales del matadero, actividad que se realizaba al amanecer. Junto a los corrales algunos curiosos y aspirantes a toreros se reunían para ver como los valientes burlaban al toro, también está documentada la presidencia de un representante de la autoridad municipal. Con el paso del tiempo, a la práctica espontánea en los corrales se le unió de forma habitual la presencia de público, razón por lo que se le adosó un balconcito para las autoridades civiles a modo de torre o palco realizado por el arquitecto Asensio de Maeda y después, en la segunda mitad del se levantaron unas gradas para el público. Este primer recinto dio lugar dos siglos después, en el , a la plaza de toros de la Maestranza.

A mediados del los toros bravos son llevados desde Navarra hasta México por orden de Juan Rodríguez de Altamirano, propietario de la finca Atenco. En la Quinta Carta de relación que Hernán Cortés escribió al emperador Carlos V, fechada el 3 de septiembre de 1526, el conquistador menciona que se corrieron toros y que se realizaban otras fiestas de cañas con motivo de la festividad de San Juan, antecedentes que sitúan el inicio de la tauromaquia en la Nueva España. Las primeras noticias de toros en Perú datan de 1538 con las celebraciones de la victoria de la batalla de las Salinas, dato aportado por Ricardo Palma en "Tradiciones peruanas," el mismo autor también menciona que la primera corrida de toros se celebró el 29 de marzo de 1540 con motivo de la consagración de los santos óleos por el obispo Vicente Valverde en la que rejoneó Francisco Pizarro.

La sociedad barroca del fue sobre todo festiva con predilección por las corridas de toros las cuales fueron incluidas en la mayoría de las celebraciones sociales, siendo frecuente el empleo de las plazas mayores y las calles para el desarrollo de las mismas. La sociedad inmersa en la religiosidad de la época y las desigualdades entre la nobleza y el pueblo encontró en las fiestas taurinas una forma de expresión y evasión, en los que el peligro asociado al riesgo de la muerte aportaban la emoción y el espectáculo al correr los toros en los encierros, al saltarlos con la garrocha, incluso al lidiarlos. Valladolid fue uno de los centro más importantes de la tauromaquia durante el , al ser una de las principales urbes españolas, en ella estuvieron las Cortes de Castilla hasta 1606, el obispado de la Corte, la Universidadincluso la Inquisición y segunda Corte junto con la Real Chancillería de Castilla—Tribunal Superior de Justicia—; encontró en la tauromaquia una herramienta con la que mantener el orden y evitar los tumultos sociales al tiempo que seguían haciendo ostentación del poder en manos de la monarquía, la iglesia y algunos nobles.

Tanto Felipe III como Felipe IV fueron aficionados a los toros. Algunas de las convocatorias llegaron a ser motivo de conflicto con la Inquisición por coincidir el festejo con festividades en honor a santos como san Pedro Arbúes. El 4 de mayo de 1623 en una crónica taurina Quevedo narró en verso la actuación de Felipe IV como rejoneador en las fiestas reales de toros celebradas en la Plaza Mayor de Madrid a la que asistió el príncipe de Gales Carlos Estuardo, futuro Carlos I de Inglaterra y de Escocia, junto a su lugarteniente Lord Buckingham durante su estancia en España, repitiendo luego la experiencia en su país, 

La documentación que existe sobre la boda entre Carlos II y María Luisa de Orleans celebrada el 19 de noviembre 1679 en Quintanapalla (Burgos), han aportado a los investigadores taurinos importante información sobre las celebraciones de la misma y como los festejos taurinos se encontraban entre estas. La información del enlace y de la organización de los diferentes actos quedó registrada en el "Libro de Actas del Ayuntamiento de Burgos" por Joseph Martínez de Araujo, (sesiones del 14 de agosto al 9 de noviembre de 1679 folios: 302r-472v). En el tercer día de celebración se organó una corrida de toros en la plaza Mayor burgalesa cuyos preparativos se iniciaron de madrugada con el encierro de treinta toros en un toril en frente al palco real elegantemente adornado, y otro en el lateral derecho del mismo. Antes del comienzo del festejo los alguaciles reales hicieron el despeje de plaza para a continuación dar inicio a la corrida en la que torearon los caballeros José de la Hoz y Melagosa junto a dieciséis toreros expertos venidos de todo el país. Se lidiaron catorce de los treinta toros en un festejo que duró tres horas; el resto de reses fueron lidiadas al día siguiente.Entre los años 1680 y 1690 los festejos taurinos habían descendido a consecuencia de los importantes problemas económicos que atraviesa el reino, causados por el importante descenso de los precios o deflación. A los problemas económicos se le unió la insistencia del papa Inocencio XI que le recordaban al monarca las disposiciones sobre la tauromaquia de Pío V (1567) ignoradas por Felipe II. Ante la nueva petición papal de prohibir las corridas de toros en todo el reino ante el riesgo de muerte del lidiador, el Consejo de Estado redactó un informe en el que expresa que el riesgo que corren los toreros no es grande y por tanto no se produce pecado al ser estos profesionales, sin embargo ante la presión moral, Carlos II, el 22 de junio de 1682 suspendió las corridas de toros y las comedias en todo el reino esta suspensión duró un año, ya que en 1683 se reanudaron los festejos taurinos.

La forma de lidiar a caballo implicaba que los rejoneadores se apoyasen en peones y escuderos cuyas funciones consistieron en proporcionar los útiles de torear, mover y colocar al toro mientras los caballeros cambiaban de caballo cansado o herido, o para rescatarlos de una caída. Otra de las funciones, que cumplían los auxiliares fue la de distraer al toro para darle la salida al caballo tras el embroque y la de dar muerte al toro cuando el rejoneador fallaba o perdía alguno de los útiles de torear y debía hacer el empeño a pie. Con la aparición de los varilargueros en el en sustitución de los caballeros alanceadores y tras el abandono de la lidia de estos últimos, los peones y auxiliares fueron adquiriendo mayor responsabilidad, hasta convertirse en los lidiadores o toreros profesionales del toreo moderno. En muchas ocasiones, si el de a caballo no podía matar al toro, se delegaba la responsabilidad en los de a pie.

El fue el siglo de la Ilustración, la burguesía experimentó un auge junto con el pensamiento laico, el humanismo sustituyó a la autoridad religiosa y el ideal de progreso. En España la Ilustración se inició durante el reinado de Felipe V, continuó con Fernando VII y tuvo su momento cumbre con Carlos III. Con el reinado de Felipe V se recortaron los gastos para aliviar la presión fiscal sobre el pueblo, afectando los recortes a las festividades religiosas y a las corridas de toros, el Consejo de Castilla reivindicó la continuidad de los toros recuperando los festejos con la presencia del monarca el 14 de abril de 1701, sin embargo no fue posible realizarlos en años posteriores hasta el 28 de julio de 1704 en el que de nuevo se volvieron a celebrar con motivo para del regreso del rey de la Guerra de sucesión con Portugal. La situación sobre la negativa de Felipe V a realizar gastos extras se mantuvo durante casi veinte años, periodo en el que se continuaron realizando festejos en algunas ciudades según la importancia del acontecimiento celebrado, por ejemplo la corrida de rejones del 30 de julio de 1725 en Madrid, presidida por los reyes, donde rejoneó el caballero de Pinto, Benardino de la Canal, mencionado por Nicolás Fernández de Moratín, junto con una veintena de toreros profesionales.

El pueblo creó su propia Fiesta Nacional al tiempo que los toreros se profesionalizaron y empiezaron a tener fama y seguidores propios. Esta profesionalización repercutió sobre la forma de ver la tauromaquia por parte de las autoridades que trataban de proteger al pueblo de los riesgos de la lidia al considerar que un profesional ya no realizaba un acto temerario. Ante esta nueva visión del toreo surgió la necesidad de regular y supervisar las actuaciones de los nuevos lidiadores, por lo que se inició un proceso para conocer a los nuevos profesionales y la forma en la que estos torearon. Surgieron los nombres de Francisco Romero, Lorenzo Martínez, "Lorenzillo" (Sic.); Melchor Calderón, Miguel Canelo o Francisco Benete.

En Sevilla, dos siglos antes de la creación de la Escuela de Tauromaquia, cuando los toros eran guiados por las calles de la ciudad hasta el matadero de la calle San Bernardo, antes de ser los toros sacrificados, los mozos aspirantes a lidiadores solían congregarse y practicar las diferentes suertes y pases en los corrales del matadero, actividad que se realizaba al amanecer. Junto a estos, empezaron a reunirse algunos curiosos para ver como aquellos valientes burlaban al toro. Está documentada la presidencia de un representante de la autoridad municipal a consecuencia de los alborotos y desórdenes que se producían a consecuencia de las prácticas toreras, la documentación sobre las actuaciones de las autoridades se conservan en los archivos municipales de Sevilla. Entre las medidas adoptadas con el paso del tiempo, y dada la necesidad de evitar los desórdenes, junto con la congregación habitual de público, al matadero se le adosó un balconcito para las autoridades civiles a modo de torre o palco realizado por el arquitecto Asensio de Maeda y después, en la segunda mitad del , se levantaron unas gradas para el público. Este primer recinto dio en el , a la plaza de toros de la Maestranza.

A mediados del siglo, figuras como Joaquín Rodríguez "Costillares", Pedro Romero —hijo de Francisco Romero—, José Delgado, "Pepe-Hillo" se habían hecho un hueco en el mundo de la tauromaquia. Surgieron las primeras "Tauromaquias", manuales que recogían las formas y las recomendaciones técnicas y reglas sobre la lidia, la "Cartilla" publicada en el , el tratado de García Baragaña y "La tauromaquia o arte de torear" publicada en Cádiz en 1796 dictada por Pepe-Hillo.

Surgieron los majos y las manolas junto con sus fiestas y la tauromaquia con el toreo a pie. Entre las aportaciones reales al fomento de la cultura y el arte estuvieron la creación de las Academias de la Historia, la de la Lengua o la de la de Bellas Artes. Siguiendo esta línea, en 1754, Fernando VI donó a la Real Junta de Hospitales la primera plaza de toros construida en fábrica de ladrillo, la Plaza de toros de la Puerta de Alcalá, ésta sustituyó a la existente realizada de madera junto a la Puerta de Alcalá que había autorizado Felipe V. Además de la Escuela de Tauromaquia de Sevilla, las plazas de toros de la Real Maestranza de Sevilla (1761), el coso de Pignaltelli en Zaragoza (1764) o la plaza de la Real Maestranza de Ronda(1785).

Hacia 1770 la suerte de varas está consolidada ante las preferencias del público por el uso de la vara de detener o larga en lugar del empleo de los rejones que era habitual en los caballeros de la nobleza. La figura del varilarguero tomó el protagonismo como núcleo de la lidia, entre los primeros picadores más conocidos lidiaron en 1736 en la maestranza de Sevilla los hermanos Merchante o José Daza quien lidió en 1740 autor de un tratado sobre el toreo tanto a caballo como a pie.

Las presiones del Conde de Arana vertidas sobre Carlos III, provocaron de nuevo vetos entre 1778 y 1785, estos no afectaron a las corridas de utilidad pública, es decir aquellas cuyo fin fue benéfico, de forma que siguió siendo habitual la celebración de las mismas.

Los manuales sobre la lidia a caballo ya no tienen el interés del siglo anterior, en su lugar surgieron la "Cartilla en que se proponen, las reglas para torear a caballo, y para practicar este valeroso, noble ejercicio con toda destreza, conocida como la Cartilla de Osuna" del autor Nicolás Rodrigo Noveli (1726), las "Reglas para torear y arte de todas las suertes" atribuido a Diego de Torres Villarroel, "La malicia, confundida" de Francisco Melcón (1738) o "Reglas de torear a caballo" de José Fernández Cadórniga. "Ensayos del valor y reglas de la prudencia para el coso" de Marcelo Tamariz de Carmona (1771). Junto a quienes redactaron los manuales sobre el toreo surgieron un número de autores antitaurinos como el padre Feijoo, el padre Sarmiento, Mayans o Jovellanos, quien, a pesar de no ser taurino, fue admirador de Pepe-Hillo y acudió a varias corridas de toros en Valladolid. A estos se les unió Vargas Ponce a quien, el 13 de julio de 1792, escribió Jovellanos para proporcionarle información para incluir en la obra "Disertación sobre las corridas de toros", escrita por Vargas Ponce en torno a 1807. Frente a estos ideales, encontraron en la defensa de la tauromaquia a Nicolás Fernández de Moratín autor de "Carta histórica sobre el origen y procesos de las fiestas de toros en España", (1777), a Ramón de la Cruz, a Bayeu o a Goya aficionado taurino y novillero con las cuadrillas de Costillares y Pedro Romero. Goya creó parte de la serie de grabados "La Tauromaquia" inspirado en la idea del posible origen árabe de la tauromaquia desarrollada en la citada obra de Fernández Moratín, el pintor aragonés dejó constancia de ello en las anotaciones de los grabados.

El se inició con la muerte en el ruedo de Pepe-Illo en el año 1801. Poco después  Carlos IV publicaba a través de la Real Cédula del 10 de febrero de 1805 la prohibición de realizar fiestas de toros y novillos en las que se diese muerte al toro, alegando causas morales y políticas, sin embargo autorizaba aquellas fiestas de toros de carácter benéfico. La prerrogativa real fue publicada en la "Novísma recopilación de las leyes de España". A pesar de la prohibición se concedieron algunos permisos con los que se sorteó la ley con autorizaciones para realizar los festejos muy arraigados en la tradiciones locales, como sucedió en Portugalete el 12 de septiembre de 1806 con motivo del nombramiento de Justo De Salcedo y Araujo como teniente general de la Armada, las celebraciones consistieron en tres días de corrida de toros sin que las reses fueran picadas ni estoqueadas. La cédula de 1805 quedó derogada cuando Fernando VII ascendió al trono en 1808, celebrando un corrida de toros en Madrid el 19 de septiembre de ese año y el 26 en la que participan José Cándido, Curro Guillén, Juan Núñez, "Sentimientos", y Agustín Aroca, quienes brindaron contra la presencia de las tropas francesas. El brindis le costó a Agustín Aroca ser detenido al día siguiente y fusilado poco después en los montes de Toledo.

José Bonaparte buscó ganarse el favor del pueblo fomentando los festejos taurinos, el 9 de junio de 1810 mediante un Real Decreto los prefectos de Sevilla, Córdoba, Granada, Jaén y Jérez de la Frontera buscaron los toros y las cuadrillas de toreros para que lidiaran en Madrid entre el 24 y el 28 de junio 1810 y en fechas posteriores. Entre los contratados figuraron la torera Teresa Alonso, los toreros Lorenzo Jade, Juan Núñez, "Sentimientos;" Luis Cornacho, Jerónimo Cándido y Curro Guillén que lidió acompañado por los garrochistas Ildefonso Pérez Naves y Jerónimo Martín, "Pajarito" quienes habían participado en la batalla de Bailén junto los denominados Garrochistas de Bailén, un grupo de ganaderos y picadores a las órdenes de José Cheriff. Los toreros recibieron entre 3 000 y 1 000 reales de vellón y la torera 500 reales. Pedro Romero junto a su hermano Juan Romero se negaron a acudir al festejo organizado por Bonaparte. Entre las medidas adoptadas por José Bonaparte para facilitar el acceso al festejo estuvo el cambio del horario de la misa del día 1 de julio de 1810 y la creación de "billetes de toros." Durante los meses de julio y septiembre se realizaron corridas de toros, y en los meses de marzo diciembre de 1811, destacando la celebrada el 25 de diciembre, víspera del anuncio del regreso a Francia de José Bonaparte tras agotar los recursos económicos del país.

A mediados del aparecieron los primeros reglamentos taurinos para regular los festejos que se celebraban en plazas de toros cerradas. Los nuevos reglamentos permitieron que los festejos taurinos pasasen de ser celebraciones con características locales, según cada población, a realizarse con un formato similar en todas las ciudades. En las publicaciones se articularon preceptos similares, tales como las condiciones que debían reunir los recintos, las pautas de comportamiento del público, las normas para la lidia y las condiciones de los astados inspeccionados por veterinarios; con pequeñas variaciones entre los reglamentos según la localidad, se publicaron el "Reglamento de las corridas de toros en Madrid" que fue aprobado por el gobernador provincial el 28 de mayo de 1868, y el "Reglamento de las funciones de toros que se celebran en esta ciudad," publicado en Cádiz en 1872, "Reglamento para la plaza de toros de la ciudad de Salamanca" publicado en 1884, el "Reglamento para la plaza de toros de Sevilla" publicado en el año 1896, el "Reglamento taurino" publicado en Málaga editado en 1897 o el "Reglamento vigente para las corridas de toros" escrito por Leopoldo Vázquez Rodríguez en 1891.

Respecto al toreo, este experimentó un nuevo concepto con toreros como "Paquiro", "Cúchares", "Lagartijo" y "Frascuelo", que cambiaron la forma de lidiar y el concepto de expresar la tauromaquia. Rafael Molina "Lagartijo," discípulo de Antonio Carmona "el Gordito", aportó la elegancia, la plasticidad artística y el toreo de línea natural o "toreo natural", es decir el concepto esencial del toreo moderno que durará hasta tiempos contemporáneos. El toreo al natural se diferencia del toreo cambiado o contrario, muy empleado por "Frascuelo", por la forma en la que el toro es guiado, es decir el toro pasa por el mismo lado por el que el torero tiene asida la muleta, la mano izquierda, mientras que en el toreo cambiado el toro sale de la muleta por el lado contrario al de la mano con la que el torero coge la muleta.

El o la edad de oro del toreo fue el momento de Mariano de Cavia, Sobaquillo, periodista y crítico taurino fue testigo de la época oscura de España, situación que también afectó a la tauromaquia. Azorín publicó en 1912 "Lecturas Españolas", un obra en la que recogió, a través de la correspondencia de Próspero Merinée, la amistad del político francés con dos toreras españolas: "la Tartataja" y Pepa "la Banderillera" y por ello Azorín, gran seguidor de la tauromaquia, cuestionaba la afición real de este a la fiesta taurina. Merinée fue un gran conocedor de la tauromaquia, de la cría del toro de lidia y de los pormenores de la lidia.

Durante la primera década del siglo destacaron Antonio Fuentes, el mexicano Rodolfo Gaona que hizo universal el toreo mexicano y español; Rafael González Madrid "Machaquito" o Ricardo Torres "Bombita", Rafael Gómez, "Gallo" "y" Vicente Pastor que ocuparon los primeros puestos de los escalafones taurinos. En la década de 1910 a 1920 se desarrolló la llamada "Época Dorada" de la tauromaquia, protagonizada por la rivalidad profesional entre Juan Belmonte y José Gómez Ortega, "Joselito," también conocido como "Gallito III." Ambos diestros están considerados los más importantes del toreo moderno: Belmonte, como el creador de la estética moderna («parar, templar y mandar») con el que cambió el concepto del toreo además de aportar la lidia vista como arte de torrear, cuya finalidad se basó en la belleza del conjunto más que en la lidia en sí misma; y Joselito como el torero completo, dominador de todas las suertes y de todos los aspectos de la tauromaquia, (desde la idea de construir grandes plazas de toros monumentales hasta los detalles de la selección del toro bravo), aglutinó lo mejor del toreo antiguo y anunció la técnica que habría de imponerse en el futuro de la lidia moderna.

El interés por la tauromaquia aumentó con nuevas publicaciones de contenido taurino en revistas especializadas como "Sol y Sombra" con 3 000 ejemplares en 1920, la revista "Don Jacinto Taurino, o El Eco Taurino" que tuvo una tirada de 8 000 ejemplares. Las representaciones teatrales y las zarzuelas optaron también por incluir en sus repertorios a la tauromaquia, en la corrida de toros de la Pascua de Resurrección realizada en 1902 en Sevilla, Bombita y Emilio Torres hicieron el paseíllo acompañados un pasodoble de la zarzuela "El Bateo" de Chueca. Se estrenó en 1904 en el Teatro Eslava de Madrid el sainete lírico "La Torería" de Antonio Paso y Asensio Mas con música de José Serrano. Se publicaron novelas con la tauromaquia de tema principal tales como "Sangre y Arena" (1908) de Blasco Ibáñez o "Currito De la Cruz" (1921) de Pérez Lugin. Una de las primeras retransmisiones radiofónicas desde el exterior fue la de una corrida de toros desde la Plaza de Vieja de Madrid celebrada el 8 de octubre de 1925.

En el toreo surgen nuevas figuras del toreo como Ignacio Sánchez Mejías que fue un punto y aparte, con él hay un "después" de las corridas de toros, es decir una vida fuera de las plazas de toros que traspasó a la sociedad intelectual del siglo . A través de la visión de Sánchez Mejías ganaderos, escritores y poetas se interesaron por el toreo y a los toreros con otra perspectiva, las corridas de toros pasaron de poseer una fama de tosca a tener un prestigio y un carácter atractivo para los círculos sociales más destacados. Así la presencia de Sánchez Mejías en tertulias y eventos sociales coloca a las corridas de toros como referencia para la literatura, la poética, el teatro, la danza o el ballet donde autores de la talla de Federico García Lorca centrarían el foco de sus obras. Con Sánchez Mejías la fiesta trasciende fuera de la plaza de toros hasta el punto de quedar unida a la cultura española formando un vínculo que dio como fruto las mejores logros socioculturales de la época, entre ellas la prosa y la poesía de la generación del 27.

Posteriormente a la Guerra Civil Española se produjo un resurgimiento de la tauromaquia gracias a la figura de Manolete, el torero más importante en la historia taurina; a este resurgir le siguieron figuras como Luis Miguel Dominguín, el mexicano Carlos Arruza, Pepe Luis Vázquez, Antonio Bienvenida, Pepín Martín Vázquez, Silverio Pérez, Miguel Báez "El Litri", Julio Aparicio y Agustín Parra "Parrita". Esta época se cierra con el fallecimiento de Manolete en la tragedia de Linares. Se inicia la etapa Dominguín y Antonio Ordóñez, grandes rivales en los ruedos.

Ya en la década de 1950 surgen nuevos concepto del torero con el venezolano César Girón, su hermano Curro o toreros como Curro Romero, Paco Camino, El Viti, Diego Puerta, y Manolo Martínez. El torero que más revolucionó dicho concepto fue Manuel Benítez, "el Cordobés" con una idea poco ortodoxa pero contundente que le llevó a llenar las plazas de toda España donde introdujo el concepto de disconformidad del estatus social. El Cordobés también se desligó de las condiciones de la industria taurina junto con Palomo Linares, en lo que se conoció como el año de los "guerrilleros," en el que reivindicaron controlar su vida taurina, para ello esa temporada solamente torearon en plazas de segunda y tercera categoría; de estas reivindicaciones surgió en 1968 el libro de registro de ganaderías bravas y el marcado de las reses con el guarismo del año de su nacimiento publicados en el B.O.E. el 16 de diciembre. 

La década entre 1970 y 1980 son los de mayor expansión comercial del mundo de los toros, llegando a haber corridas incluso en el Astrodome de Houston, con la participación de Manuel Benítez «el Cordobés». Las grandes figuras de esta época son: José Mari Manzanares, Pedro Gutiérrez Moya El Niño de la Capea, Dámaso González, Morenito de Maracay, Francisco Rivera «Paquirri», El Yiyo, Nimeño II, Antoñete y Juan Antonio Ruiz «Espartaco», líder de la estadística en forma consecutiva desde 1985 hasta 1991.
Las nuevas figuras del toreo presentan gran diversidad en su estilo y proyección; personalidades tan particulares como Enrique Ponce, y Joselito —de toreo clásico—; Julián López, "el Juli", José Tomás, Manuel Jesús Cid "el Cid", Miguel Ángel Perera, Pepín Liria, Morante de la Puebla, José María Manzanares, Alejandro Talavante, Luis Bolívar, Antonio Puerta, y el francés Sebastián Castella, son algunos de los toreros más célebres del siglo XXI.

La "Tauromaquia" fue el nombre dado a las obras o libros que tratan y recopilan las diferentes técnicas de torear, donde se desarrollan además las reglas del toreo en forma de manual para ser leído por los toreros.

La primera "Tauromaquia" conocida fue la conocida como la "Cartilla de Osuna (Cartilla, en que se proponen las reglas, para torear a caballo, y practicar este valeroso, noble exercicio, con toda destreza)" publicada en 1726. Posteriormente García Baragaña publicó un tratado que incluía parte de las recomendaciones publicadas en la mencionada "Cartilla de Osuna," la obra de Baragaña se publicó en 1750 con el nombre de Noche fantástica, ideático divertimento que demuestra el método de torear a pie. Esta recopila la técnica de las suertes de capa, de banderillas, del lienzo a modo de primitiva muleta y el uso del estoque, desarrollando para cada una de ellas las formas más convenientes de realizarlas; así en las banderillas se menciona las suertes de (banderillas) a topacarnero conocida como «"a la media vuelta»", el autor cita la forma de colocar éstas a «"compás quebrado»" como una suerte de alivio y detalla las formas de entrar a matar «"a recibir"» que también llama «"suerte de la ley" ».

"Precisos manejos y progresos condonados en dos tomos del más forzoso peculiar del arte de la Agricultura, que lo es del toreo, privativo de los españoles" es una obra recogida en dos tomos, conocida también como "El arte del torear," que el varilarguero José Daza escribió entre 1772 y 1778, el ejemplar original se conserva en la Real Biblioteca del Palacio Real. La obra de Daza fue referencia para las obras que publicaron con posterioridad Fernández Moratín, Pepe-Hillo y Paquiro. La obra que en origen fue difundida como manuscritos mediante copias y solo fue publicada parcialmente, la obra de tauromaquia tuvo su primera edición completa en 1959 a través de la Real Maestranza de Caballería de Sevilla, la Fundación de Estudios Taurinos y la Universidad de Sevilla.

Una de las "Tauromaquias" más conocidas fue "Tratado de tauromaquia" dictada por el torero José Delgado, "Pepe-Hillo," cuya redacción se atribuye a José de la Tixera y fue publicada en el año 1796 en Cádiz. En la obra el torero da una serie de indicaciones sobre la forma de lidiar a pie siguiendo un estricta ortodoxia. Analizada por diferentes autores, como Cossío o Fernando de Claramunt, sobe el tratado hay diversas opiniones dirigidas sobre todo a la dificultad para aplicar las recomendaciones dadas por el conocido torero. Su publicación daba paso al toreo a pie desplazando la lidia a caballo, con ella se reforzaba el nuevo arte de lidia y se enfrentaba a la pragmática sanción de 1785 que vetaba la muerte del toro en público de determinados festejos taurinos. Del "Tratado de tauromaquia" de Pepe-Hillo se publicó una segunda edición en 1804 en Madrid con importantes cambios aclarando algunos conceptos al lector, en tiempos contemporáneos el tratado de Pepe-Hillo sigue siendo un manual de referencia para toreros.

Redactada por Francisco Montes, "Paquiro", y editada por Santos López Pelegrín, "Abenamar," la "Tauromaquia completa o sea El Arte de torear en plaza, tanto a pie como a caballo: escrita por el célebre lidiador Francisco Montes, y dispuesta y corregida escrupulosamente por el editor" se publicó en 1836. En la obra Paquiro organizó una serie de preceptos y legislación para toreros con el que se estableció la forma definitiva para torear, organizando con ello la forma en la que hombre y toro se enfrentaban en el ruedo. Con Paquiro desaparece el varilarguero y se establecen a través de la "Tauromaquia completa" las normas que regirán tanto a los picadores como el desarrollo de la suerte, y también el tercio de varas que se adaptaba a la lidia a pie; se estableció al banderillero como colaborador del matador de toros bajo sus órdenes como había establecido Costillares con las cuadrillas. Junto al desarrollo de estos preceptos Paquiro realizó un estudio de las suertes del toreo, tanto las de capa como las de muleta y banderillas, practicadas en su época, de forma que el "Tratado de tauromaquia" de Pepe-Hillo quedó ampliado con sus aportaciones.

Entre las aportaciones a la tauromaquia que se hacen desde esta obra, destaca el desarrollo del estudio del toro de lidia en cuanto a su comportamiento y las condiciones que este debe tener para ser lidiado aportando para cada tipo las instrucciones adecuadas para su lidia. 

A diferencia de la obra de Pepe Hillo, la de Paquiro estriba en el inicio de cada una de ellas, Hillo empieza su tauromaquia con recomendaciones para el rejoneo, mientras que Paquiro se centró solo en la lidia a pie tal y como se llevaban a cabo las corridas de toros de entonces, quedando de manifiesto el nuevo orden de la lidia del .

La "Tauromaquia completa" de Francisco Montes está considera como una de las obras más importantes sobre tauromaquia antes de la publicación de "Los toros. Tratado técnico e histórico" conocida como "El Cossío" de José María de Cossío publicada en 1943.

El entorno o marco hace referencia a aquellos elementos y factores sociales que hacen posible entender la tauromaquia, así como aquellas actividades que hacen posible la compresión de la misma. Dichos factores comprenden tanto los elementos culturales y tradicionales, antropológicos, históricos y ecológicos como aquellos que son propios de la actividad, como la crianza y selección del toro bravo de lidia. La tauromaquia fue declarada patrimonio cultural español el 12 de noviembre de 2013; es una tradición cultural que tiene su arraigo en países de América y Europa. En España tiene diferentes niveles de arraigo cultural según el lugar, así pueden encontrarse comunidades donde no se realizan actividades taurinas, como otras zonas en las que las tradiciones taurinas son conocidas mundialmente como es el caso de los Sanfermines.

Unida desde sus orígenes a las tradiciones culturales ancestrales del Mediterráneo, la interpretación de la tauromaquia ha dado lugar a diferentes formas de manifestaciones artísticas y culturales desde la literatura y la poseía hasta la música, el cine, el teatro o la danza entre otras. Centro de numerosas fiestas y celebraciones locales la tauromaquia forma parte de las tradiciones españolas, latinoamericanas, portuguesas o francesas, donde supone un importante motor económico generador de riqueza y empleo a nivel nacional. Junto a los festejos taurinos es habitual que se celebren diferentes actos y citas culturales, como exposiciones, encuentros y coloquios en torno a los diferentes temas que comprende la tauromaquia. 

Además de los festejos taurinos la tauromaquia incluye la crianza y estudio de los toros bravos de lidia criados en las dehesas, donde habitan. Las dehesas suponen un elemento rural de riqueza ecológica, sostenimiento del medio rural y biodiversidad de la zona en las que se incluyen, además del toro bravo, otro número importante de especies tanto de flora como de fauna. La presencia del toro bravo en las dehesas guarda especial importancia para el sostenimiento de las mismas, el toro aprovecha los recursos que encuentra en su entorno de forma racional, al tiempo que lo preserva al quedar limitado el acceso a las fincas de forma que se mantiene el ecosistema de la dehesa. 

El sastre y le bordador en hilo de oro son otra de las profesiones asociadas la tauromaquia en todo lo concerniente a la confección de los trajes de luces de los toreros, los trastos de torear: capotes de brega, muletas, banderillas, estoques, etc. Las manifestaciones artísticas relacionadas con la actividad: confección de carteles. 
Antes de la construcción de las plazas de toros, los encierros de reses bravas corridos por las calles terminaban en las plazas locales habilitadas para la celebración de los festejos y celebraciones posteriores. Ante los daños personales y desperfectos que se ocasionaban durante las celebraciones, los concejos de las localidades afectadas decidieron craer ordenanzas para regular la forma de en la que debían realizarse los actos. En Huesca en 1275 se prohibía que, cuando fuesen corridas reses con motivos de las bodas, estas entraran en la catedral. En Valencia se prohibió en 1339 organizar corridas improvisadas dados los daños y desórdenes que ocasionaban. En Zaragoza en 1460 Juan II ordenaba que los carniceros vendiesen sus productos fuera de la plaza del mercado de la ciudad donde también se realizaban todas las fiestas, entre ellas las corridas de toros, para evitar la contaminación de los alimentos allí vendidos, así surgió la necesidad de cerrar la plaza y convertirla en coso cerrado acondicionado para las fiestas de toros.

En 1565 se ordena la reforma del matadero de Sevilla donde terminaban los encierros y a continuación se practicaban ejercicios taurinos y la lidia de las reses. Las reformas consistieron en crear una plaza con abundantes ventanas desde las cuales los nobles sevillanos y otras personalidades acudían a ver las corridas de toros que se realizaban en los llanos del matadero, similar a un ruedo cerrado; dicha modificación se recoge en las Actas Capitulares del 23 de febrero de 1581 "Ley la propusicion que hizo don Diego de Nofuentes el cabildo pasado sobre haser plaça en la que se hase frontero del matadero." Doce años después, entre 1577 y 1579, se integró en el conjunto la tribuna para los asistentes a las corridas cuyas localidades eran alquiladas, durante las obras se realizó primer palco presidencial existente en un ruedo taurino.Las plazas de toros cerradas y específicas para realizar espectáculos taurinos, conocidas también como "cosos taurinos", son estructuras arquitectónicas con estilos diversos, de acuerdo a su antigüedad. En general, se trata de un recinto cerrado de forma circular, con tendidos y servicios que rodean un espacio central, llamado ruedo o arena, en donde se realiza el espectáculo taurino. El ruedo es un terreno de tierra batida o de albero, usado en Andalucía, rodeado por una valla de tablas de madera o barrera que mide aproximadamente 140 centímetros de altura, tiene varios burladeros en su perímetro, tras los cuales se encuentran los toreros, los auxiliares, ayudantes, autoridades y otros asistentes, tras los burladeros también se resguardan los toreros. El espacio entre la barrera y el tendido se denomina callejón. El ruedo dispone de puertas de acceso batientes para la entrada y salida de los participantes (puerta de cuadrilla) y los toros (puerta de toriles), aunque la disposición de estos accesos varía de una plaza a otra. 

El edificio más antiguo que se conserva en España es la plaza de toros de Béjar "La Ancianita" data del año 1711, es de 3ª categoría y pertenece a la Unión de plazas de toros históricas. La plaza de toros de primera categoría más antigua de España es la plaza de toros de la Misericordia en Zaragoza, data del año 1764. La plaza de toros más antigua que se conserva en América es la plaza de toros de Acho (Perú), data del año 1766. La plaza de toros más grande del mundo es la Monumental de México, con una capacidad aproximada de 41 000 personas sentadas.

La tauromaquia está íntimamente ligada a la cultura ancestral tanto la tradicional como la popular. Esta ha acompañado el discurrir histórico de la fiesta de los toros, de forma que pueden encontrase manifestaciones culturales relacionadas con la tauromaquia en las artes plásticas de artistas como Goya, Picasso, Manet, Enrique Simonet, Alberto Gironella o Lucas Villaamil entre otros artistas; en manifestaciones musicales tales como los pasodobles del compositor mexicano Agustín Lara o el flamenco entre otros; así como en la literatura, el cine y el teatro. 

La tauromaquia es ejercicio de múltiple comprensión, pues puede ser admirada o criticada, pero sus componentes, ya citados, le permiten perdurar en el tiempo y generar amplio debate a su alrededor. Por ejemplo, el gobierno de España, a través del Ministerio del Interior, hace referencia al aspecto cultural de las corridas de toros en su reglamentación de las escuelas taurinas: «Para fomento de la fiesta de toros, en atención a la tradición y vigencia cultural de la misma, podrán crearse escuelas taurinas para la formación de nuevos profesionales taurinos y el apoyo y promoción de su actividad.»

El filósofo José Ortega y Gasset explicaba que era impensable estudiar la historia de España sin considerar las corridas de los toros. Si algunos de los escritores y filósofos de la Generación del 98, no gustaban de las corridas de toros ni del flamenco, era porque consideraban que la tauromaquia y el cante eran un atraso de la sociedad españolaque dado el momento histórico y social que vivía España. Así, Unamuno explicaba que no le gustaban las corridas, no porque fuese un espectáculo cruento, sino porque se perdía mucho tiempo hablando de ella y esto explicaba la formación cultural de sus espectadores. Sin embargo otros tantos fueron aficionados taurinos y dedicaron a la tauromaquia parte de sus obras. La tauromaquia estuvo presente de forma habitual en la obra de Ortega y Gasset, fue crítico taurino, apoderado de toreros y gran aficionado, llegó a organizar festejos junto a Zuloaga en las que toreó. En "La caza y los toros" (1962), se extrañaba de que el toreo, siendo un ejercicio callado diese tanto que hablar, en la obra realizó un análisis sobre el toro bravo y su forma de embestir relacionada con el propio hombre, análisis que denominó. «"la compresión del toro"».

Posteriormente, la Generación del 27 en su mayoría fue amante de la fiesta, sobre la cual escribieron, pintaron y esculpieron. Vale citar las palabras con las que Federico García Lorca manifestaba su abierto apoyo y gusto por la tauromaquia: «"El toreo es probablemente la riqueza poética y vital de España, increíblemente desaprovechada por los escritores y artistas, debido principalmente a una falsa educación pedagógica que nos han dado y que hemos sido los hombres de mi generación los primeros en rechazar. Creo que los toros es la fiesta más culta que hay en el mundo"».
Antonio Machado fue crítico taurino junto con su hermano Manuel en la revista "La Caricatura," el poeta pasó todas las etapas pasó de aficionado a rechazar la tauromaquia para volver a comprenderla a través de su poesía. 

Ortega y Gasset, al igual que otros autores como el académico José María de Cossío, realizaron un paralelismo entre las corridas de toros y la historia de España:

Otros intelectuales contemporáneos, como Enrique Tierno Galván, subrayaron, en abierta contradicción con los del 98, el carácter socialmente pedagógico de la tauromaquia: «Los toros son el acontecimiento que más ha educado social, e incluso políticamente, al pueblo español». Y abundaba en el refinamiento del gusto artístico que supone para sus aficionados:

Una larga lista de escritores de varios países ha escrito exaltando el toreo como una parte importante del alma de sus pueblos. Entre los artistas vivos que defienden el toreo se encuentra el peruano Mario Vargas Llosa, el escultor y pintor colombiano Fernando Botero y el escultor y pintor mexicano Humberto Peraza.

Entre los partidarios de la tauromaquia se encuentran el pintor Francisco de Goya quien participa de festejos taurinos y los escritores Nicolás Fernández de Moratín y Valle-Inclán. Filósofos como Fernando Savater o Enrique Tierno Galván, y artistas como Joaquín Sabina o Joan Manuel Serrat, aducen que estas críticas de los antitaurinos obedecen a la ignorancia, ya que el toro de lidia vive en libertad en su hábitat natural y, sin las corridas, no solo se extinguiría el toro bravo, sino el propio ecosistema en que se desenvuelve (las dehesas), sin embargo hay alegatos que refieren a que estas pueden ser protegidas por ley sin la necesidad de criar toros. Otros defensores del toreo, como el catedrático Andrés Amorós, argumenta que nadie ama más al toro que un buen aficionado a las corridas: «nadie admira más su belleza, nadie exige con más vehemencia su integridad y se indigna con mayor furia ante cualquier maltrato, desprecio o manipulación fraudulenta.»

Por el contrario, algunos escritores ha manifestado su inconformidad ante la tauromaquia, como Cecilia Bohl Faber, que firmaba con el pseudónimo masculino de Fernán Caballero para poder incursionar en la carrera literaria:

En el Francia está a la cabeza en representación de la cultura, epicentro europeo de las corrientes artísticas como el romanticismo, el darwinismo o el positivismo, donde se dieron cita los artistas e intelectuales más destacados del periodo. Fue también momento del interés hacia la cultura española, Goya fue el representante de la tradición española, junto con las publicaciones de libros de viajes y los ejemplos de las representaciones como "Carmen" de Mérimée y Bizet quedaron lugar a una imagen estereotipada de la cultura y sociedad española en el que fijó la sociedad francesa. Las surgieron como una forma para encontrar nuevas formas de expresión artística y con la idea mostrar los últimos avances tecnológicos, industriales y científicos.

La idea de la España del , esta estuvo asociada al estereotipo flamenco y toros y así fue mostrado en varias de las exposiciones universales celebradas en Paris en 1855, donde por primera vez se incluyeron las Bellas Artes, grupos de pintores españoles expusieron obras sobre tauromaquia (entre otros temas) como Manuel Castellano con la obra "Toreros y aficionados ante una corrida de toros," Juan José Martínez Espinosa autor de "Picadores ensayando con sus caballlos" y Eugenio Lucas con la obra "Peleas de toros en Madrid." La Exposición Universal de París de 1867 fue la que causó una de las polémicas más importantes al exhibirse en el edificio español una cabeza de toro disecada junto a los útiles de torear, según citó Ángel Fernández de los Ríos. En la Exposición de París de 1878, el escultor Ricardo Bellver presentó una escultura del diestro Lagartijo, El corresponsal de "La Iberia" citó un cuadro de Agraeil que representa toros y picadores en un corral antes de una corrida de toros y la obra de Jules Worns que retrató a un torero conversando con una manola. La exclusión de la obra de Zuloaga "Preparativos para la corrida de toros", de la Exposición Universal de 1900 causó un gran escándalo, al ser el pintor uno de los representantes más importantes de la escuela española de pintura.

Con tauromaquia y economía se hace referencia a la dimensión económica que la tauromaquia tiene en la economía de la industria cultural y de consumo de la misma.

Entre la estructura que forma la industria taurina se encuentran los profesionales directos como toreros: (matadores de toros, banderilleros, picadores y subalternos), toros bravos de lidia y ganaderos del toro de lidia, empresarios de plazas de toros, público e instituciones (comunidades autónomas, ayuntamientos, y diputaciones y administración) entre otros. muchos; a los que se le añaden veterinarios, fabricantes de material como banderillas, estoques, picas; o sastres y todo lo necesario para poner en marcha un festejo taurino de cualquiera de los diferentes tipos: corridas de toros, encierros, novilladas, corre bous, tentaderos en el campo o festejos celebrados en la calle como los recortadores. Además del impacto económico directo, la tauromaquia tiene una repercusión importante en el sector servicios tales como el turismo, la hostelería, comercio, distribución y alimentación de la carne de toro, medios de comunicación, imprentas, etc.

Las cifras económicas varían en función del tipo de festejos realizado, así la Feria del Toro de Pamplona en pleno Sanfermín tuvo una repercusión de setenta y cuatro millones de euros en 2018. En Toledo la repercusión económica es de ocho millones de euros; los abonos de la Feria de San Isidro generaron en torno a los setenta y tres millones de euros, mientras que la hostelería y la restauración generaron cerca de cuarenta y siete millones de euros en 2019.

En España se recoge en la legislación la protección de la tauromaquia como bien cultural desde 1991, mencionado por el Tribunal Supremo en 1998, posteriormente se aprobó la Ley 18/2013 que regula la tauromaquia y la Ley 10/2015 por la que se protege la misma como Patrimonio Cultural Inmaterial, de acuerdo con Ley de Patrimonio Histórico el Estado por la que se tienen la obligación de garantizar la conservación de la tauromaquia así como de promocionarla y facilitar el acceso a la misma como parte del conocimiento cultural de los españoles.

El 28 de julio de 2010, el Parlamento de Cataluña aprobó con 68 votos a favor, 55 en contra y 9 abstenciones abolir las corridas de toros en Cataluña a partir del 1 de enero de 2012. Posteriormente, el 20 de octubre de 2016, el Tribunal Constitucional español declaraba inconstitucional la prohibición taurina en Cataluña.

En abril de 2016 el Parlamento Balear aprobó una ley para prohibir a partir de junio de dicho año las corridas de toros. En noviembre de 2017 el consejo de ministros aprobó recurrir la ley ante el Tribunal Constitucional, que declaró la ley anticonstitucional en diciembre de 2018, anulando la prohibición.

En 2011 el "Ministère de la Culture" francés declaró la tauromaquia Patrimonio Cultural Inmaterial nacional. El Tribunal Constitucional de Francia en 2012 denegó la demanda presentada para ilegalizar las corridas de toros, avalando la legalidad de la tauromaquia y de las corridas de toros en el país.

En el año 2019 el Tribunal Administrativo de Apelación de Marsella desestimó el recurso presentado por los grupos antitaurinos por el que se pedía prohibir la asistencia de menores de dieciséis años a las escuelas taurinas de Nimes, Arlés y Béziers; al igual que ya hizo el Tribunal Administrativo de Nimes y el de Montpellier apoyándose en la ley francesa. Esta medida fue apoyada por el Consejo de Estado de Francia.

En 1836 en Portugal, durante el reinado de María II de Portugal, fue decretada la prohibición de la muerte de los toros en el ruedo, pero permitió y fomentó la evolución de la misma de forma que adoptó un estilo propio y original. Fue habitual que la guardia real protegiese al público con alabardas y las horquillas del moquete o "forcado." Una vez se hizo uso en las plazas de las "trincheras" o barreras protectoras la función de la guardia pasó a ser la de sostener al toro con la fuerza de sus brazos, gesto conocido como "pega" , hasta guiarlo a los toriles. La tradición se mantiene intacta desde el . En septiembre de 2019 Portugal blindó la tauromaquia al declarar incostutucional la ley que prohibía las corridas de toros.





</doc>
<doc id="2761" url="https://es.wikipedia.org/wiki?curid=2761" title="Los tres mosqueteros (juego)">
Los tres mosqueteros (juego)

Tres Mosqueteros es un juego abstracto para dos jugadores. Requiere un tablero de 5x5. Tres fichas de un color, que serán los mosqueteros, y 22 de otro color, que serán la Guardia del Cardenal.

El jugador que tiene los Tres Mosqueteros mueve primero, y luego ambos jugadores se alternan. Por turno cada jugador mueve una de sus fichas, de este modo:



Ambos jugadores tienen objetivos diferentes:


Si se van a jugar varias partidas, conviene que ambos jugadores se alternen en los roles de Mosqueteros y Guardia del Cardenal. Para que haya más emoción en el torneo, se puede usar este sistema de puntuación:

Tras un número par de partidas, gana el torneo quien suma más puntos.



</doc>
<doc id="2762" url="https://es.wikipedia.org/wiki?curid=2762" title="Tiempo">
Tiempo

El tiempo es una magnitud física con que se mide la duración o separación de acontecimientos. 
El tiempo permite ordenar los sucesos en secuencias, estableciendo un pasado, un futuro y un tercer conjunto de eventos ni pasados ni futuros respecto a otro. En mecánica clásica a esta tercera clase se llama «presente» y está formada por eventos simultáneos a uno dado.

En mecánica relativista el concepto de tiempo es más complejo: los hechos simultáneos («presente») son relativos al observador, salvo que se produzcan en el mismo lugar del espacio; por ejemplo, un choque entre dos partículas.

Su unidad básica en el Sistema Internacional es el segundo, cuyo símbolo es s (debido a que es un símbolo y no una abreviatura, no se debe escribir con mayúscula, ni se escribe como "seg", "sg" o "sec", ni agregando un punto posterior).

Dados dos eventos puntuales "E" y "E", que ocurren respectivamente en instantes de tiempo "t" y "t", y en puntos del espacio diferentes "P" y "P", todas las teorías físicas admiten que estos pueden cumplir una y solo una de las siguientes tres condiciones:


Dado un evento cualquiera, el conjunto de eventos puede dividirse según esas tres categorías anteriores. Es decir, todas las teorías físicas permiten, fijado un evento, clasificar a los eventos en: (1) pasado, (2) futuro y (3) resto de eventos (ni pasados ni futuros). La clasificación de un tiempo presente es debatible por la poca durabilidad de este intervalo que no se puede medir como un estado actual sino como un dato que se obtiene en una continua sucesión de eventos. En mecánica clásica esta última categoría está formada por los sucesos llamados simultáneos, y en mecánica relativista, por los eventos no relacionados causalmente con el primer evento. Sin embargo, la mecánica clásica y la mecánica relativista difieren en el modo concreto en que puede hacerse esa división entre pasado, futuro y otros eventos y en el hecho de que dicho carácter pueda ser absoluto o relativo respecto al contenido de los conjuntos.
En mecánica clásica, el tiempo se concibe como una magnitud absoluta, es decir, es un escalar cuya medida es idéntica para todos los observadores (una magnitud relativa es aquella cuyo valor depende del observador concreto). Esta concepción del tiempo recibe el nombre de tiempo absoluto. Esa concepción está de acuerdo con la concepción filosófica de Kant, que establece el espacio y el tiempo como necesarios para cualquier experiencia humana. Kant asimismo concluyó que el espacio y el tiempo eran conceptos subjetivos. Mas, no por ello, Kant establecerá que tiempo y espacio sean dimensiones absolutas, ni en sí mismas, sí apoyadas, en cambio, por Newton y Leibniz respectivamente. Para Kant no son dimensiones sino formas puras de la intuición suministrada por la experiencia, de manera que, al no tratarse de magnitudes, no hay posible choque entre ellas.
Fijado un evento, cada observador clasificará el resto de eventos según una división tripartita clasificándolos en: (1) eventos pasados, (2) eventos futuros y (3) eventos ni pasados y ni futuros. La mecánica clásica y la física prerrelativista asumen:
Aunque dentro de la teoría especial de la relatividad y dentro de la teoría general de la relatividad, la división tripartita de eventos sigue siendo válida, no se verifican las últimas dos propiedades:

En mecánica relativista la medida del transcurso del tiempo depende del sistema de referencia donde esté situado el observador y de su estado de movimiento, es decir, diferentes observadores miden diferentes tiempos transcurridos entre dos eventos causalmente conectados. Por tanto, la duración de un proceso depende del sistema de referencia donde se encuentre el observador.

De acuerdo con la teoría de la relatividad, fijados dos observadores situados en diferentes marcos de referencia, dos sucesos A y B dentro de la categoría (3) (eventos ni pasados ni futuros), pueden ser percibidos por los dos observadores como simultáneos, o puede que A ocurra "antes" que B para el primer observador mientras que B ocurre "antes" de A para el segundo observador. En esas circunstancias no existe, por tanto, ninguna posibilidad de establecer una noción absoluta de simultaneidad independiente del observador. Según la relatividad general el conjunto de los sucesos dentro de la categoría (3) es un subconjunto tetradimensional topológicamente abierto del espacio-tiempo. Cabe aclarar que esta teoría solo parece funcionar con la rígida condición de dos marcos de referencia solamente. Cuando se agrega un marco de referencia adicional, la teoría de la Relatividad queda invalidada: el observador A en la Tierra percibirá que el observador B viaja a mayor velocidad dentro de una nave espacial girando alrededor de la Tierra a 7000 kilómetros por segundo. El observador B notará que el dato de tiempo al reloj se ha desacelerado y concluye que el tiempo se ha dilatado por causa de la velocidad de la nave. Un observador C localizado fuera del sistema solar, notará que tanto el hombre en tierra como el astronauta girando alrededor de la Tierra, están viajando simultáneamente —la nave espacial y el planeta Tierra— a 28 kilómetros por segundo alrededor del Sol. La más certera conclusión acerca del comportamiento del reloj en la nave espacial, es que ese reloj está funcionando mal, porque no fue calibrado ni probado para esos nuevos cambios en su ambiente. Esta conclusión está respaldada por el hecho que no existe prueba alguna que muestre que el tiempo es objetivo.

Solo si dos sucesos están atados causalmente todos los observadores ven el suceso "causal" antes que el suceso "efecto", es decir, las categorías (1) de eventos pasados y (2) de eventos futuros causalmente ligados sí son absolutos. Fijado un evento "E" el conjunto de eventos de la categoría (3) que no son eventos ni futuros ni pasados respecto a "E" puede dividirse en tres subconjuntos:

Las curiosas relaciones causales de la teoría de la relatividad, conllevan a que no existe un tiempo único y absoluto para los observadores, de hecho cualquier observador percibe el espacio-tiempo o espacio tetradimensional según su estado de movimiento, la dirección paralela a su cuadrivelocidad coincidirá con la dirección temporal, y los eventos que acontecen en las hipersuperficies espaciales perpendiculares en cada punto a la dirección temporal, forman el conjunto de acontecimientos simultáneos para ese observador.

Lamentablemente, dichos conjuntos de acontecimientos percibidos como simultáneos difieren de un observador a otro.

Si el tiempo propio es la duración de un suceso medido en reposo respecto a ese sistema, la duración de ese suceso medida desde un sistema de referencia que se mueve con velocidad constante con respecto al suceso viene dada por:

En mecánica cuántica debe distinguirse entre la mecánica cuántica convencional, en la que puede trabajarse bajo el supuesto clásico de un tiempo absoluto, y la mecánica cuántica relativista, dentro de la cual, al igual que sucede en la teoría de la relatividad, el supuesto de un tiempo absoluto es inaceptable e inapropiado.

Se ha señalado que la dirección del tiempo está relacionada con el aumento de entropía, aunque eso parece deberse a las peculiares condiciones que se dieron durante el Big Bang. Aunque algunos científicos como Penrose han argumentado que dichas condiciones no serían tan peculiares si consideramos que existe un principio o teoría física más completa que explique por qué nuestro universo, y tal vez otros, nacen con condiciones iniciales aparentemente improbables, que se reflejan en una bajísima entropía inicial.

Desde el punto de vista de la teoría de sistemas propuesta por Niklas Luhman, el tiempo tiene una formación social, de esta manera el tiempo está situado desde la perspectiva del observador. De esta suerte, se trata de una operación que se realiza de manera concreta a través de la distinción entre antes y después. El primero es el pasado que no existe, y que sin embargo, se puede recordar y en el que se puede ubicar la causalidad, por otro lado el futuro que es donde suceden los efectos. En el punto ciego entre ambos se encuentra la actualidad del presente, en el que se encuentra la sincronización de la simultaneidad. Por lo tanto el mundo se percibe desde la simultaneidad (presente) y la no simultaneidad (pasado-futuro). Como nos explica Luhman "se pueden construir tiempos específicos para localizar, por ejemplo, las causas en el pasado; los efectos, en el futuro. Pero todo esto es solo posible en la observación que se realiza solo en un presente actual, y mediante aplicación de procesos de atribución."

La cronología (histórica, geológica, etc.) permite datar los momentos en los que ocurren determinados hechos (lapsos relativamente breves) o procesos (lapsos de duración mayor). En una línea de tiempo se puede representar gráficamente los momentos históricos en puntos y los procesos en segmentos.

Las formas e instrumentos para medir el tiempo son de uso muy antiguo, y todas ellas se basan en la medición del movimiento, del cambio material de un objeto a través del tiempo, que es lo que puede medirse. En un principio, se comenzaron a medir los movimientos de los astros, especialmente el movimiento aparente del Sol, dando lugar al tiempo solar aparente. El desarrollo de la astronomía hizo que, de manera paulatina, se fueron creando diversos instrumentos, tales como los relojes de sol, las clepsidras o los relojes de arena y los cronómetros. Posteriormente, la determinación de la medida del tiempo se fue perfeccionando hasta llegar al reloj atómico. Todos los relojes modernos desde la invención del reloj mecánico, han sido construidos con el mismo principio del "tic tic tic". El reloj atómico está calibrado para contar 9 192 631 770 vibraciones del átomo de Cesio para luego hacer un "tic".





</doc>
<doc id="2764" url="https://es.wikipedia.org/wiki?curid=2764" title="Temperatura">
Temperatura

La temperatura es una magnitud referida a la noción de calor medible mediante un termómetro. En física, se define como una magnitud escalar relacionada con la energía interna de un sistema termodinámico, definida por el principio cero de la termodinámica. Más específicamente, está relacionada directamente con la parte de la energía interna conocida como energía cinética, que es la energía asociada a los movimientos de las partículas del sistema, sea en un sentido traslacional, rotacional, o en forma de vibraciones. A medida que sea mayor la energía cinética de un sistema, se observa que este se encuentra más «caliente»; es decir, que su temperatura es mayor.

En el caso de un sólido, los movimientos en cuestión resultan ser las vibraciones de las partículas en sus sitios dentro del sólido. En el caso de un gas ideal monoatómico se trata de los movimientos traslacionales de sus partículas (para los gases multiatómicos los movimientos rotacional y vibracional deben tomarse en cuenta también).

El desarrollo de técnicas para la medición de la temperatura ha pasado por un largo proceso histórico, ya que es necesario darle un valor numérico a una idea intuitiva como es lo frío o lo caliente.

Multitud de propiedades fisicoquímicas de los materiales o las sustancias varían en función de la temperatura a la que se encuentren, como por ejemplo su estado (sólido, líquido, gaseoso, plasma), su volumen, la solubilidad, la presión de vapor, su color o la conductividad eléctrica. Asimismo, es uno de los factores que influyen en la velocidad a la que tienen lugar las reacciones químicas.

La temperatura se mide con termómetros, los cuales pueden ser calibrados de acuerdo a una multitud de escalas que dan lugar a unidades de medición de la temperatura. En 
el Sistema Internacional de Unidades, la unidad de temperatura es el kelvin (K), y la escala correspondiente es la escala Kelvin o escala absoluta, que asocia el valor «cero kelvin» (0 K) al «cero absoluto», y se gradúa con un tamaño de grado igual al del grado Celsius. Sin embargo, fuera del ámbito científico el uso de otras escalas de temperatura es común. La escala más extendida es la escala Celsius, llamada «centígrada», y, en mucha menor medida, y prácticamente solo en los Estados Unidos, la escala Fahrenheit.

La temperatura es la propiedad física que se refiere a las nociones comunes de calor o ausencia de calor, sin embargo su significado formal en termodinámica es más complejo.
Termodinámicamente se habla de la velocidad promedio o la energía cinética (movimiento) de las partículas de las moléculas, siendo de esta manera, a temperaturas altas, la velocidad de las partículas es alta, en el cero absoluto las partículas no tienen movimiento.
A menudo el calor o el frío percibido por las personas tiene más que ver con la sensación térmica (ver más abajo), que con la temperatura real. Fundamentalmente, la temperatura es una propiedad que poseen los sistemas físicos a nivel macroscópico, la cual tiene una causa a nivel microscópico, que es la energía promedio por la partícula. Y actualmente, al contrario de otras cantidades termodinámicas como el calor o la entropía, cuyas definiciones microscópicas son válidas muy lejos del equilibrio térmico, la temperatura solo puede ser medida en el equilibrio, precisamente porque se define como un promedio.

La temperatura está íntimamente relacionada con la energía interna y con la entalpía de algún sistema: a mayor temperatura mayores serán la energía interna y la entalpía del sistema.

La temperatura es una propiedad intensiva, es decir, que no depende del tamaño del sistema, sino que es una propiedad que le es inherente y no depende ni de la cantidad de sustancia ni del material del que este compuesto.

Antes de dar una definición formal de temperatura, es necesario entender el concepto de equilibrio térmico. Si dos partes de un sistema entran en contacto térmico es probable que ocurran cambios en las propiedades de ambas. Estos cambios se deben a la transferencia de energía en forma de calor entre las partes. Para que un sistema esté en equilibrio térmico debe llegar al punto en que ya no hay intercambio neto de energía en forma de calor entre sus partes, además ninguna de las propiedades que dependen de la temperatura debe variar.

Una definición de temperatura se puede obtener de la Ley cero de la termodinámica, que establece que si dos sistemas A y B están en equilibrio térmico, con un tercer sistema C, entonces los sistemas A y B estarán en equilibrio térmico entre sí. Este es un hecho empírico más que un resultado teórico. Ya que tanto los sistemas A, B, y C están todos en equilibrio térmico, es razonable decir que comparten un valor común de alguna propiedad física. Llamamos a esta propiedad "temperatura".

Sin embargo, para que esta definición sea útil es necesario desarrollar un instrumento capaz de dar un significado cuantitativo a la noción cualitativa de esa propiedad que presuponemos comparten los sistemas A y B. A lo largo de la historia se han hecho numerosos intentos, sin embargo en la actualidad predominan el sistema inventado por Anders Celsius en 1742 y el inventado por William Thomson (más conocido como lord Kelvin) en 1848.

También es posible definir la temperatura en términos de la segunda ley de la termodinámica, la cual dice que la entropía de todos los sistemas, o bien permanece igual o bien aumenta con el tiempo, esto se aplica al Universo entero como sistema termodinámico. La entropía es una medida del desorden que hay en un sistema. 

Este concepto puede ser entendido en términos estadísticos, considere una serie de tiros de monedas. Un sistema perfectamente ordenado para la serie, sería aquel en que solo cae cara o solo cae cruz. Sin embargo, existen múltiples combinaciones por las cuales el resultado es un desorden en el sistema, es decir que haya una fracción de caras y otra de cruces. Un sistema desordenado podría ser aquel en el que hay 90% de caras y 10% de cruces, o 60% de caras y 40% de cruces. Sin embargo es claro que a medida que se hacen más tiros, el número de combinaciones posibles por las cuales el sistema se desordena es mayor; en otras palabras el sistema evoluciona naturalmente hacia un estado de desorden máximo es decir 50% caras 50% cruces de tal manera que cualquier variación fuera de ese estado es altamente improbable.

Para dar la definición de temperatura con base en la segunda ley, habrá que introducir el concepto de máquina térmica la cual es cualquier dispositivo capaz de transformar calor en trabajo mecánico. En particular interesa conocer el planteamiento teórico de la máquina de Carnot, que es una máquina térmica de construcción teórica, que establece los límites teóricos para la eficiencia de cualquier máquina térmica real.

En una máquina térmica cualquiera, el trabajo que esta realiza corresponde a la diferencia entre el calor que se le suministra y el calor que sale de ella. Por lo tanto, la eficiencia es el trabajo que realiza la máquina dividido entre el calor que se le suministra:

Donde "W" es el trabajo hecho por la máquina en cada ciclo. Se ve que la eficiencia depende solo de "Q" y de "Q". Ya que "Q" y "Q" corresponden al calor transferido a las temperaturas "T" y "T", es razonable asumir que ambas son funciones de la temperatura:

Sin embargo, es posible utilizar a conveniencia, una escala de temperatura tal que

Sustituyendo la ecuación (3) en la (1) relaciona la eficiencia de la máquina con la temperatura:

Hay que notar que para "T" = 0 K la eficiencia se hace del 100%, temperaturas inferiores producen una eficiencia aún mayor que 100%. Ya que la primera ley de la termodinámica prohíbe que la eficiencia sea mayor que el 100%, esto implica que la mínima temperatura que se puede obtener en un sistema microscópico es de 0 K. Reordenando la ecuación (4) se obtiene:

Aquí el signo negativo indica la salida de calor del sistema. Esta relación sugiere la existencia de una función de estado "S" definida por:

Donde el subíndice indica un proceso reversible. El cambio de esta función de estado en cualquier ciclo es cero, tal como es necesario para cualquier función de estado. Esta función corresponde a la entropía del sistema, que fue descrita anteriormente. Reordenando la ecuación siguiente para obtener una definición de temperatura en términos de la entropía y el calor:

Para un sistema en que la entropía sea una función de su energía interna "E", su temperatura está dada por:

Esto es, el recíproco de la temperatura del sistema es la razón de cambio de su entropía con respecto a su energía.

Las escalas de medición de la temperatura se dividen fundamentalmente en dos tipos, las relativas y las absolutas. Los valores que puede adoptar la temperatura en cualquier escala de medición, no tienen un nivel máximo, sino un nivel mínimo: el cero absoluto. Mientras que las escalas absolutas se basan en el cero absoluto, las relativas tienen otras formas de definirse.








Las escalas que asignan los valores de la temperatura en dos puntos diferentes se conocen como "escalas a dos puntos". Sin embargo en el estudio de la termodinámica es necesario tener una escala de medición que no dependa de las propiedades de las sustancias. Las escalas de este tipo se conocen como escalas absolutas o escalas de temperatura termodinámicas.

Con base en el esquema de notación introducido en 1967, en la Conferencia General de Pesos y Medidas (CGPM), el símbolo de grado se eliminó en forma oficial de la unidad de temperatura absoluta.

"Aclaraciones: No se le antepone la palabra" grado "ni el símbolo º. Cuando se escribe la palabra completa, «kelvin», se hace con minúscula, salvo que sea principio de párrafo."


Las siguientes fórmulas asocian con precisión las diferentes escalas de temperatura:

De manera simplificada entre las cuatro escalas que se encuentran vigentes, se puede emplear la siguiente equivalencia:

Para un gas ideal, la teoría cinética de gases utiliza mecánica estadística para relacionar la temperatura con el promedio de la energía total de los átomos en el sistema. Este promedio de la energía es independiente de la masa de las partículas, lo cual podría parecer contraintuitivo para muchos. El promedio de la energía está relacionado exclusivamente con la temperatura del sistema, sin embargo, cada partícula tiene su propia energía la cual puede o no corresponder con el promedio; la distribución de la energía, (y por lo tanto de las velocidades de las partículas) está dada por la distribución de Maxwell-Boltzmann.
La energía de los gases ideales monoatómicos se relaciona con su temperatura por medio de la siguiente expresión:

donde "n", número de moles, "R", constante de los gases ideales. En un gas diatómico, la relación es:

El cálculo de la energía cinética de objetos más complicados como las moléculas, es más difícIl. Se involucran grados de libertad adicionales los cuales deben ser considerados. La segunda ley de la termodinámica establece sin embargo, que dos sistemas al interactuar el uno con el otro adquirirán la misma energía promedio por partícula, y por lo tanto la misma temperatura.

En una mezcla de partículas de varias masas distintas, las partículas más masivas se moverán más lentamente que las otras, pero aun así tendrán la misma energía promedio. Un átomo de Neón se mueve relativamente más lento que una molécula de hidrógeno que tenga la misma energía cinética. Una manera análoga de entender esto es notar que por ejemplo, las partículas de polvo suspendidas en un flujo de agua se mueven más lentamente que las partículas de agua. Para ver una ilustración visual de este hecho vea este enlace. La ley que regula la diferencia en las distribuciones de velocidad de las partículas con respecto a su masa es la ley de los gases ideales.

En el caso particular de la atmósfera, los meteorólogos han definido la temperatura atmosférica (tanto la temperatura virtual como la potencial) para facilitar algunos cálculos.

Es importante destacar que la sensación térmica es algo distinto de la temperatura tal como se define en termodinámica. La sensación térmica es el resultado de la forma en que la piel percibe la temperatura de los objetos y/o de su entorno, la cual no refleja fielmente la temperatura real de dichos objetos y/o entorno. La sensación térmica es un poco compleja de medir por distintos motivos:

Por todo ello, la sensación de comodidad depende de la incidencia combinada de los factores que determinan estos cuatro tipos de intercambio: temperatura seca, temperatura radiante, temperatura húmeda (que señala la capacidad del aire para admitir o no la evaporación del sudor) y la velocidad del aire (que incide sobre la convección y la evaporación del sudor). La incidencia en las pérdidas de la transmisión es pequeña, salvo que la piel, o parte, esté en contacto con objetos fríos (pies descalzos, asiento frío con poca ropa de abrigo...).

Se llama temperatura seca del aire de un entorno (o más sencillamente: "temperatura seca") a la temperatura del aire, prescindiendo de la radiación calorífica de los objetos que rodean ese ambiente concreto, y de los efectos de la humedad relativa y de los movimientos de aire. Se puede obtener con el termómetro de mercurio, respecto a cuyo bulbo, reflectante y de color blanco brillante, se puede suponer razonablemente que no absorbe radiación.

La temperatura radiante tiene en cuenta el calor emitido por radiación de los elementos del entorno.

Se toma con un termómetro de globo, que tiene el depósito de mercurio o bulbo, encerrado en una esfera o "globo" metálico de color negro, para asemejarlo lo más posible a un cuerpo negro y así absorber la máxima radiación. 

Las medidas se pueden tomar bajo el sol o bajo la sombra. En el primer caso se tendrá en cuenta la radiación solar, y se dará una temperatura bastante más elevada.

También sirve para dar una idea de la sensación térmica.

La temperatura de bulbo negro hace una función parecida, dando la combinación de la temperatura radiante y la ambiental.

Temperatura de bulbo húmedo o "temperatura húmeda", es la temperatura que da un termómetro bajo sombra, con el bulbo envuelto en una mecha de algodón húmedo bajo una corriente de aire. La corriente de aire se produce mediante un pequeño ventilador o poniendo el termómetro en un molinete y haciéndolo girar. Al evaporarse el agua, absorbe calor rebajando la temperatura, efecto que reflejará el termómetro. Cuanto menor sea la humedad relativa del ambiente, más rápidamente se evaporará el agua que empapa el paño. Este tipo de medición se utiliza para dar una idea de la sensación térmica, o en los psicrómetros para calcular la humedad relativa y la temperatura del punto de rocío.




</doc>
<doc id="2765" url="https://es.wikipedia.org/wiki?curid=2765" title="Talauma">
Talauma

Talauma era considerado un género de plantas de la familia de las magnoliáceas.




</doc>
<doc id="2774" url="https://es.wikipedia.org/wiki?curid=2774" title="Tragus">
Tragus

Tragus, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de África. Comprende 36 especies descritas y de estas, solo 8 aceptadas.
Son plantas anuales o perennes, cespitosas. La lígula es una membrana ciliada; con láminas anchamente lineares, aplanadas. Inflorescencia una panícula cilíndrica espiculífera con espiguillas dispuestas en fascículos de 2-5 sobre pedúnculos cortos, los fascículos desarticulándose como una unidad. Espiguillas similares o las superiores reducidas y estériles, con 1 flósculo bisexual, sin una extensión de la raquilla; glumas marcadamente desiguales, la inferior diminuta, hialina, la superior tan larga como la espiguilla, marcadamente 5-7-nervia con grandes aguijones en las nervaduras; lema 3-nervia, membranácea; pálea casi tan larga como la lema, membranácea, 2-carinada, convexa en el dorso; lodículas 2; estambres 3; estilos 2. Fruto una cariopsis; embrión 2/5-1/2 la longitud de la cariopsis; hilo punteado.
El género fue descrito por Albrecht von Haller y publicado en "Historia Stirpium Indigenarum Helvetiae Inchoata" 2: 203. 1768. La especie tipo es: "Tragus racemosus"
Tragus: nombre genérico posiblemente del griego: "tragos", una parte de la oreja, literalmente, "cabra", o de Hieronymus Tragus, el nombre griego para Jerome Bock (1498-1554), médico, investigador, y uno de los tres padres de la botánica alemana. 
Número de la base del cromosoma, x = 10. 2n = 20 y 40. 2 y 4 ploide. 
A continuación se brinda un listado de las especies del género "Tragus" aceptadas hasta junio de 2015, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos.



</doc>
<doc id="2775" url="https://es.wikipedia.org/wiki?curid=2775" title="Triplachne nitens">
Triplachne nitens

Triplachne, es un género monotípico de plantas herbáceas perteneciente a la familia de las poáceas. Su única especie: Triplachne nitens (Guss.) Link, es originaria de la región del Mediterráneo.
Es una planta anual con tallos de hasta 25 cm de altura, geniculado-ascendentes o decumbentes, glabros. Hojas glabras, glaucas, con lígula de 1,5-3 mm y limbo de hasta 5 cm x 4 mm. Panícula de hasta 5 x 1 cm, elipsoidea. Espiguillas de c. 4 mm, cuneiformes. Gluma inferior de 3,8-4,1 mm, la superior de 3,1-3,3 mm. Lema de c. 2 mm, con los 2 nervios laterales prolongados en arístulas apicales de 0,8 mm, más o menos hirsuta en la mitad inferior; arista subbasal de c. 3,5 mm, generalmente incluida en las glumas, parda en la parte inferior e hialina en la superior. Anteras de 0,6 mm. Cariopsis de 1 x 0,3 mm. Florece de abril a mayo.
Se encuentra en arenales costeros, en Algeciras (La Línea). Península ibérica, Baleares, Sicilia, islas del E del Mediterráneo, Norte de África, SW de Asia.
"Triplachne nitens" fue descrita por (Guss.) Link y publicado en "Hortus Regius Botanicus Berolinensis" 2: 241. 1833.



</doc>
<doc id="2777" url="https://es.wikipedia.org/wiki?curid=2777" title="Trisetum">
Trisetum

Trisetum es un género de plantas herbáceas de la familia de las poáceas. Es originario del centro y sudoeste de Asia.
Son plantas perennes, cespitosas, a veces con rizomas cortos; cañas de 5-300 cm de alto, erectas o geniculadas en la base, glabras o pilosas bajo la panícula; lígulas membranáceas, truncadas a ovales; láminas planas a conduplicadas, a veces filiformes, glabras o pilosas. Inflorescencia una panícula espiciforme o laxa, contraída o abierta, oval o piramidal; raquis glabro, escabroso o piloso. Espiguillas (1-)2-6-floras, cortamente pediceladas; comprimidas lateralmente; raquilla pilosa, prolongada más allá del antecio superior; desarticulación de la raquilla sobre las glumas y entre los antecios; glumas heteromorfas a subisomorfas, lanceoladas a oval-lanceoladas, iguales o desiguales, menores o mayores que el conjunto de los antecios; gluma inferior 1(-3)-nervia, usualmente más angosta y más corta que la superior; gluma superior 3(-5)-nervia; lemmas lanceoladas, (3-)5(-7)-nervias, con el ápice y márgenes hialinos, glabras o pilosas en el dorso, levemente aquilladas y comprimidas, ápice con 2(-4) arístulas o sétulas cortas, 2-dentado o más raramente entero o subentero; arista inserta en el tercio superior, medio o subapical, exserta, usualmente tan larga como la lemma, geniculada o sólo divaricada, retorcida o no, raramente recta; callo corto, obtuso, con pelos cortos; páleas biaquilladas, hialinas, usualmente más cortas que las lemmas; estambres 3, anteras de 0,3-4,5 mm de largo; lodículas 2, hialinas, con 2 o 3 lóbulos en el ápice; ovario glabro o con pelos cortos y brillantes en el ápice; cariopsis comprimida, blanda; hilo corto, punctiforme; endosperma líquido, semilíquido, raramente sólido, seco. Número básico de cromosomas x = 7.
El género fue descrito por Christiaan Hendrik Persoon y publicado en "Synopsis Plantarum" 1: 97. 1805. La especie tipo es: "Trisetum flavescens" (L.) P. Beauv.
El nombre del género deriva del latín "tri" = (tres) y "setum" = (cerdas), aludiendo a los lemas con tres aristas. 



</doc>
<doc id="2780" url="https://es.wikipedia.org/wiki?curid=2780" title="Teatro">
Teatro

El teatro (del griego: "θέατρον", "théatron" o «lugar para contemplar» derivado de "θεάομαι", "theáomai" o «mirar») es la rama de las artes escénicas relacionada con la actuación. Representa historias actuadas frente a los espectadores o frente a una cámara usando una combinación de discurso, gestos, escenografía, música, sonido o espectáculo.

También se entiende como «teatro» el género literario que comprende las obras de teatro representadas ante un público o para ser grabadas y reproducidas en el cine, así como a la edificación donde se presentan tradicionalmente dichas obras o grabaciones. En adición a la narrativa común, el estilo de diálogo, el teatro también toma otras formas como la ópera, el ballet, el cine, la ópera china o la pantomima.

El Día Mundial del Teatro se celebra desde 1961.

La mayoría de los estudios consideran que los orígenes del teatro deben buscarse en la evolución de los rituales mágicos relacionados con la caza, al igual que las pinturas rupestres, o la recolección agrícola que, tras la introducción de la música y la danza, se embocaron en auténticas ceremonias dramáticas donde se rendía culto a los dioses y se expresaban los principios espirituales de la comunidad. Este carácter de manifestación sagrada resulta un factor común a la aparición del teatro en todas las civilizaciones.

En el Antiguo Egipto, a mediados del segundo milenio antes de la edad cristiana, ya se representaban dramas acerca de la muerte y resurrección de Osiris. El teatro comienza por medio de máscaras y dramatizaciones con ellas y se han hallado testimonios de actores explicando que hacían viajes, por lo que según los investigadores Driotton y Vandier es posible que existieran grupos de cómicos ambulantes desde el Imperio Medio."Acompañé a mi amo en sus giras, sin fallar en la declamación. Le di la réplica en todos sus parlamentos. Si él era dios, yo era soberano. Si él mataba, yo resucitaba."

Las raíces del teatro de la antigua Grecia están basadas en los ritos órficos y en los festivales celebrados para Dioniso, donde se llevaban a cabo las escenificaciones de la vida de los dioses acompañadas de danzas y cantos (Ditirambos). Más tarde comenzaron las primeras representaciones ya propiamente dramáticas, ejecutadas en las plazas de los pueblos por compañías que incluían solo un actor y un coro. A fines del Siglo VI a.C alcanzó extraordinaria celebridad el legendario poeta e intérprete Tespis, en cuyo honor la frase "el carro de Tespis" alude, aún hoy, al conjunto del mundo del teatro.

El teatro griego surge tras la evolución de las artes y ceremonias griegas como la fiesta de la vendimia (ofrecida a Dionisios) donde los jóvenes iban danzando y cantando hacia el templo del dios, a ofrecerle las mejores vidas. Después un joven que resaltara entre el grupo de jóvenes se transformaba en el corifeo o maestro del coro, quien dirigía al grupo. Con el tiempo aparecieron el bardo y el rapsoda, que eran recitadores.

En el curso del siglo V a. C., durante la edad clásica de Grecia, se establecieron los modelos tradicionales de la tragedia y la comedia, y los dramaturgos Esquilo y Sófocles añadieron respectivamente un segundo y tercer actor a la acción, lo que dio a ésta una complejidad que hacía necesaria la creación de mayores escenarios. Para ello se erigieron grandes teatros de piedra, entre los que cabe citar el aún conservado de Epidauro en el siglo V a. C., capaz de albergar unas 12.000 personas, y el de Dioniso, en Atenas, en el siglo IV A.C.

Su construcción se realizaba mediante el aprovechamiento de las faldas de una colina, donde se disponían en forma semicircular las gradas que rodeaban la "orquestra", espacio circular en el que se efectuaba la mayor parte de la representación. Tras la "orquestra" se levantaba una edificación llamada "skené", escena, destinada a que los actores cambiaran su vestimenta. Delante de ella se levantaba una pared columnada, el proscenio, que podía sostener superficies pintadas que evocaban el lugar de la acción. Estos decorados, junto con las túnicas y máscaras empleadas por los actores y máquinas, constituían todo el aparato escénico.

Las representaciones del teatro griego se hacían al aire libre, contaba con coro (dirigido por el Corifeo o maestro del coro) que cantaba el coro y danzaba en torno a un altar. En el teatro griego se representaban dos tipos de obras: la tragedia, obra dramática de final desgraciado que trataba de temas de leyendas heroicas y utilizaba, oportunamente, a los dioses para su final; y la comedia satírica, que criticaba humorísticamente a políticos y a las obras e incurrían en una mímica iniciada por un coro de sátiros, y comedias que tenían por tema asuntos de la vida cotidiana; todas estaban escritas en verso y utilizaban máscaras.

Los teatros romanos heredaron los rasgos fundamentales de los griegos, si bien introdujeron ciertos elementos distintivos. Construidos inicialmente en madera, solo en el año 52 a. C. Pompeyo, erigió en Roma el primero en piedra. A diferencia de sus modelos helénicos, se levantaban sobre el suelo plano y poseían varias plantas erigidas en mampostería. Con objeto de mejorar la acústica, los arquitectos romanos redujeron la orquesta a un semicírculo, y los espectáculos se presentaban sobre una plataforma, el "pulpitum", levantada delante de la antigua "skene" que constituye el origen de los modernos escenarios. La "frons scaenae" era una fachada monumental de varios pisos, que servía de fondo de escenario. El graderío ("cávea") se divide en 3 partes: "Ima", "media" y "suma ", ubicándose la primera en la zona inferior donde se sentaban los senadores y la clase dirigente; quedando asentados en la superior las mujeres y los esclavos y en la media el pueblo llano. El conjunto podía cubrirse con un "velum". Roma optó también por la comedia, ya que estos tomaron el teatro como una manera de divertirse o entretenerse.

En las culturas americanas prehispánicas el teatro llegó a adquirir un notable desarrollo, particularmente entre los mayas. Una de las obras más representativas del teatro maya es el drama quiché Rabinal Achí. El teatro maya se hallaba parcialmente vinculado a los ciclos agrícolas y a la épica de sus eventos históricos, y entre los aztecas e Incas, sociedades que en correspondencia con su estructura teocrática dieron a sus actividades teatrales un matiz eminentemente guerrero y religioso. Los últimos yacimientos apuntan que los espacios teatrales mayas podían servir, además de para representar obras, para mostrar actos políticos teatralizados tales como negociaciones, alianzas y humillación de cautivos.

Tras siglos de misterioso olvido, la recuperación del teatro en Occidente tuvo principal apoyo en el clero, que lo empleó con fines religiosos. Así, desde el siglo XI, fue habitual la representación en las iglesias de "misterios" y "moralidades", cuyo objetivo era presentar de forma sencilla la doctrina cristiana a los fieles. A fin de facilitar la comprensión, el latín cedió paso paulatinamente a las lenguas vernáculas, y en los siglos XIII y XIV, comenzaron a representarse tanto las piezas religiosas como las florecientes farsas profanas.

La eclosión del Renacimiento en Italia tuvo consecuencias decisivas sobre la evolución del teatro, pues, al surgir una producción dramática de carácter culto, inspirada en los modelos clásicos y destinada a las clases aristocráticas, se generalizó en el transcurso del siglo XVI la construcción de salas cubiertas dotadas de mayores comodidades.

Como primero de los teatros modernos suele citarse el Olímpico de Vicenza, diseñado por Andrea Palladio y finalizado en 1585, que constituía una versión de los modelos romanos y presentaba, al fondo del escenario, una perspectiva tridimensional con vistas urbanas. El modelo clásico del teatro italiano, vigente en muchos aspectos, fue no obstante el teatro Farnese de Parma, erigido en 1618, cuya estructura incluía el escenario, enmarcado por un arco proscenio y separado del público por un telón, y una platea en forma de herradura rodeada por varios pisos de galerías. Durante este tiempo se desarrolló también en Italia una forma de teatro popular, la comedia del arte, que con su énfasis en la libertad de improvisación del actor dio un gran avance a la técnica interpretativa.

Los teatros erigidos en Inglaterra durante el reinado de Isabel I de Inglaterra fueron muy diferentes entre sí. Esta época destaca por el excepcional esplendor del género dramático, entre los que se destacó el londinense The Globe donde presentaba sus obras William Shakespeare. Carentes de techo y construidos de madera, su rasgo más característico era el escenario elevado rectangular, en torno al cual el público rodeaba a los actores por tres lados, mientras las galerías se reservaban para la nobleza.

En España, y en la misma época que el teatro Isabelino en Inglaterra (siglos XVI y XVII), se crean instalaciones fijas para el teatro al aire libre denominadas Corrales de Comedias, con las que guardan similitudes constructivas. A diferencia del caso inglés, en España sí han pervivido algunos ejemplos de estas edificaciones. Exponentes de esta época son los autores Lope de Vega, Tirso de Molina y Calderón de la Barca, claros exponentes del importante Siglo de Oro español.

El transcurso de los siglos XVII y XVIII dio lugar a un gran enriquecimiento de la escenografía. La recuperación por parte del drama clásico francés de la regla de las tres unidades —acción, tiempo y lugar— hizo innecesaria la simultaneidad de decorados, con lo que se empleó sólo uno en cada acto, y pronto se generalizó la costumbre de cambiarlos en los entreactos. Posteriormente, la creciente popularidad de la ópera, que requería varios montajes, favoreció el desarrollo de máquinas perfeccionadas que dieran mayor apariencia de veracidad a efectos tales como: la desaparición de actores y la simulación de vuelos —las llamadas "glorias", por ejemplo hacían posible el descenso de las alturas del escenario de una nube que portaba a los cantantes. El teatro de la Scala de Milán, finalizado en 1778, constituye un ejemplo de las grandes dimensiones que eran precisas para albergar tanto al público como a la tramoya y al aparato escénico.

Durante la mayor parte del siglo XIX las ideas arquitectónicas y escenográficas se mantuvieron en esencia inalterables, si bien las exigencias de libertad creativa iniciadas por los autores románticos condujeron a fines de la centuria a un replanteamiento general del arte dramático en sus diversos aspectos.

En este sentido fue fundamental la construcción del monumental Festspielhaus de Bayreuth, Alemania, erigido en 1876 de acuerdo con las instrucciones del compositor Richard Wagner, lo que constituyó la primera ruptura respecto a los modelos italianos. Su diseño en abanico, con la platea escalonada, el oscurecimiento del auditorio durante su representación y la ubicación de la orquesta en un pequeño foso; eran elementos concebidos para centrar la atención de los espectadores sobre la acción y abolir en lo posible la separación entre escenario y público.

Esta exigencia de integración entre el marco arquitectónico, la escenografía y la representación fue acentuada en los últimos decenios del siglo XIX y primeros del XX por la creciente importancia concedida a la figura del director gracias a personalidades como el alemán Max Reinhardt, autor de espectaculares montajes, el francés André Antoine, adalid del naturalismo, el ruso Konstantín Stanislavski, director y actor cuyo método de interpretación ejercería gran influencia sobre el teatro moderno, o el escenógrafo británico Edward Gordon Craig, que en su defensa de un teatro poético y estilizado abogó por la creación de escenarios más sencillos y dúctiles.

La aparición del teatro moderno, pues, se caracterizó por su absoluta libertad de planteamiento mediante el diálogo con formas tradicionales y las nuevas posibilidades técnicas darían lugar a una singular transformación del arte teatral. En el campo del diseño arquitectónico y escenográfico las mayores innovaciones se debieron al desarrollo de nueva maquinaria y al auge adquirido por el arte de la iluminación, circunstancias que permitieron la creación de escenarios dotados de mayor plasticidad (circulares, móviles, transformables, etc.) y liberaron al teatro de la apariencia pictórica proporcionada por la estructura clásica del arco del proscenio.

El teatro africano, entre tradición e historia, se está encauzando actualmente por nuevas vías. Todo predispone en África al teatro. El sentido del ritmo y de la mímica, la afición por la palabra y la verborrea son cualidades que todos los africanos comparten en mayor o menor medida y que hacen de ellos actores natos. La vida cotidiana de los africanos transcurre al ritmo de variadas ceremonias, rituales o religiosas, concebidas y vividas generalmente como verdaderos espectáculos. No obstante, aunque África ha conocido desde siempre este tipo de ceremonias, cabe preguntarse si se trataba realmente de teatro; a los ojos de muchos, estos espectáculos están demasiado cargados de significado religioso para que puedan considerarse como tal. Otros estiman que los tipos de teatro africanos guardan cierto parecido, como en otros tiempos la tragedia griega, con un preteatro que nunca llegara totalmente a ser teatro si no se desacraliza. La fuerza y las posibilidades de supervivencia del teatro negro residirán, por lo tanto, en su capacidad para conservar su especificidad. En el África independiente está tomando forma un nuevo teatro.


Un drama es un modo de ficción representado en una obra de teatro. El término proviene de una palabra griega que significa "acción", la cual deriva del verbo δράω, "dráō", "hacer" o "actuar". La puesta en escena de un drama en el teatro, es realizada por actores en un escenario frente a una audiencia, presupone la adopción de modos colaborativos de producción y una forma colectiva de recepción. A diferencia de otras formas de literatura, la estructura dramática de los textos, se encuentra directamente influenciada por esta producción colaborativa y recepción colectiva. La tragedia de comienzos de la edad moderna "Hamlet" (1601) de Shakespeare y la tragedia clásica ateniense "Oedipus Rex" (c. 429 AdC) de Sófocles son algunas de las mejores obras de arte dramático. Un ejemplo moderno sería "Largo viaje hacia la noche" de Eugene O'Neill (1956).

El modo dramático ha sido considerado un género de la poesía, y se lo ha contrastado con los modos épico y lírico comenzando con la "Poética" de Aristóteles (c. 335 AdC), la obra más antigua sobre teoría dramática. El uso de la palabra "drama" en un sentido estricto se utiliza para hacer referencia un tipo específico de obra de teatro del siglo XIX. En este sentido drama se refiere a una obra que no es ni una comedia ni una tragedia, por ejemplo, la obra "Thérèse Raquin" (1873) de Émile Zola o la obra "Ivanov" (1887) de Chéjov . Sin embargo en la antigua Grecia, la palabra "drama" abarcaba todos los tipos de obras de teatro, tragedias, comedias, y otras formas intermedias.

A menudo el drama es combinado con elementos de música y danza: por lo general en la ópera la totalidad del texto del drama es cantado; los musicales por su parte por lo general contienen tanto diálogo hablado como canciones; y algunas formas de drama incluyen música incidental o un acompañamiento musical que acompaña y refuerza el diálogo (por ejemplo el melodrama y el Nō japonés). En ciertos períodos históricos (la Antigua Roma y el Romanticismo moderno) algunos dramas fueron escritos para ser leídos en vez de para ser puestos en escena. En la improvisación, el drama no existe previo al momento de la obra; los actores crean y desarrollan un argumento dramático de manera espontánea ante la audiencia.

La frase de Aristóteles "los diversos tipos asociados con las distintas partes de la obra" es una referencia a los orígenes estructurales del drama. En el mismo las diversas partes con diálogo eran escritas en el dialecto del Ática mientras que las partes corales (recitados o cantados) se realizaban en dialecto dórico, estas discrepancias reflejaban los distintos orígenes religiosos y las métricas poéticas de las partes que eran fusionadas en una nueva entidad, el "drama" teatral.

La tragedia se encuentra entroncada con una tradición específica de drama que ha desempeñado un rol único y muy importante en la definición histórica de la civilización occidental. La tradición ha tenido múltiples expresiones discontinuas, el término a menudo ha sido utilizado para hacer referencia a un poderoso efecto de identidad cultural y continuidad histórica—"los Griegos y los Isabelinos, como un formato cultural; los Helenos y Cristianos, en una actividad cotidiana," tal como lo plantea Raymond Williams. Desde sus orígenes oscuros en los teatros de Atenas hace 2,500 años, de donde ha sobrevivido solo una fracción de las obras de Esquilo, Sófocles y Eurípides, mediante las elaboraciones particulares a través de las obras de Shakespeare, Lope de Vega, Racine, y Schiller, hasta la más recientes tragedias naturalistas de Strindberg, las meditaciones modernistas de Beckett sobre la muerte, la pérdida, y el sufrimiento, y los retrabajos postmodernistas de Müller del canon trágico, la tragedia ha continuado siendo un ámbito importante de experimentación cultural, negociación, lucha y cambio. En la senda de la "Poética" (335 AdC) de Aristóteles, la tragedia ha sido utilizada para marcar distinciones de género, ya sea con la poesía en general (donde lo trágico se contrapone con lo épico y lírico ) o con el drama (en el cual la tragedia se enfrenta a la comedia). 

El teatro como se ha podido observar, constituye un todo orgánico del que sus diferentes elementos forman una parte indisoluble. Esos elementos, no obstante, poseen cada uno características y leyes propias y, en función de la época, de la personalidad del director o de otras circunstancias, es habitual que se conceda a unos u otros mayor relevancia dentro del conjunto. Estos elementos son:

Las obras dramáticas se escriben en diálogos y en primera persona, en el que existe las acciones que van entre paréntesis, (llamado lenguaje de acotaciones).

En la tradición occidental, el texto, la obra dramática, se ha considerado siempre la pieza esencial del teatro, llamado "el arte de la palabra". Dado que, de forma más matizada, esta orientación predomina también en las culturas orientales, cabe cuando menos admitir como justificada tal primacía. A este respecto deben hacerse, no obstante, dos consideraciones: en primer lugar, el texto no agota el hecho teatral, pues una obra dramática no es teatro hasta que se representa, lo que implica como mínimo el elemento de la actuación; en segundo lugar, son numerosas las formas dramáticas arcaicas y los espectáculos modernos que prescinden por completo de la palabra o la subordinan a elementos cual la mímica, la expresión corporal, la danza, la música y el despliegue escénico.

El hecho de que la obra sólo adquiera plena vigencia en la representación determina además el carácter distintivo de la escritura dramática respecto a otros géneros literarios. La mayoría de los grandes dramaturgos de todos los tiempos, desde los clásicos griegos al inglés William Shakespeare, el francés Molière, el español Pedro Calderón de la Barca o el alemán Bertolt Brecht, basaron sus creaciones en un conocimiento directo y profundo de los recursos escénicos e interpretativos y en una sabia utilización de sus posibilidades.

La personalidad del director como artista creativo se consolidó a fines del siglo XIX, aunque su figura ya existía como coordinador de los elementos teatrales, desde la escenografía a la interpretación. A él corresponde convertir el texto, si existe, en teatro, con los procedimientos y objetivos que se precisen. Poderosos ejemplos de dicha tarea fueron los alemanes Bertolt Brecht y Erwin Piscator, dedicaban su energía a conseguir del espectador su máxima capacidad de reflexión, o el ascetismo del polaco Jerzy Grotowski.

Las técnicas de actuación han variado enormemente a lo largo de la historia y no siempre de manera uniforme. En el teatro occidental clásico, por ejemplo los grandes actores, los "monstruos sagrados", tendían a enfatizar las emociones con objeto de destacar el contenido de la obra, en "la comedia del arte" el intérprete dejaba rienda suelta a su instinto; los actores japoneses del "Nō" y "kabuki", hacen patentes determinados estados de ánimo por medio de gestos simbólicos, bien de gran sutileza o deliberadamente exagerados.

En el teatro moderno se ha impuesto por lo general la orientación naturalista, en que el actor por medio de adquisición de técnicas corporales y psicológicas y del estudio de sí mismo y del personaje, procura recrear en escena la personalidad de este. Tal opción, evolucionada en sus rasgos fundamentales a partir de las enseñanzas del ruso Konstantín Stanislavski y muy extendida en el ámbito cinematográfico, no es desde luego la única y en último extremo la elección de un estilo interpretativo depende de características del espectáculo y de las indicaciones del director.

Sin embargo, actualmente, a inicios del siglo XXI, la actuación teatral con tendencia naturalista está siendo replanteada seriamente. La teatralidad contemporánea requiere una crítica del naturalismo como simple reproducción del comportamiento humano, pero sin lazos con su entorno. Actualmente ha habido grandes transformaciones del trabajo de Stanislavski siendo las más importantes Antonin Artaud, Jerzy Grotowsky Étienne Decroux y Eugenio Barba. Estas técnicas, llamadas actualmente extra cotidianas implican una compleja síntesis de los signos escénicos.

De forma estricta, se entiende por decorado al ambiente en que se desarrolla una representación dramática, y por escenografía, al arte de crear los decorados. Hoy en día, tiende a introducirse en el concepto de "aparato escenográfico" a todos los elementos que permiten la creación de ese ambiente, entre los que cabría destacar fundamentalmente a la maquinaria o tramoya y la iluminación.

A lo largo del tiempo y en diferentes momentos de la historia del teatro, la escenografía ha sufrido importantes transformaciones. Antes de que el teatro existiera como lo conocemos, las representaciones se realizaban con un sentido ritual y en ellas ya se utilizaban los decorados para dar más realce, misterio, ambientación, así como imagen escénica y espectacularidad a los actos rituales.

En el teatro griego se utilizaban los "periactos" que eran unos apuntadores de base triangular, tenían estos unas mamparas o paneles prismáticos, en cuyos planos o caras se dibujaban, distintos decorados, de acuerdo a los requerimientos de la escena que se estaba representando.

En la antigüedad, la escenografía se hallaba condicionada a limitaciones técnicas y arquitectónicas, circunstancia que se mantuvo durante toda la Edad Media. Fue ya a fines del Renacimiento y, sobre todo, durante los siglos XVII y XVIII, cuando la escenografía comenzó a adquirir realce, gracias al perfeccionamiento de la perspectiva pictórica, que permitió dotar de mayor apariencia de profundidad al decorado, y posteriormente al desarrollo de la maquinaria teatral. En el siglo XIX, con la introducción del drama realista, el decorado se convirtió en el elemento básico de la representación. El descubrimiento de la luz eléctrica, en fin, dio pie al auge de la iluminación. Las candilejas, que en principio eran un elemento accesorio, ahora se consideran poéticamente un símbolo del arte teatral.

Estrechamente vinculado con la concepción escénica, se ha hallado siempre el vestuario. En el teatro griego, la tosquedad de los decorados se compensaba por medio de máscaras —trágicas o cómicas— y las túnicas estilizadas de los actores, cuyo objeto era de resaltar el carácter arquetípico de los personajes. Durante el Barroco y el Neoclasicismo adquirieron importancia el maquillaje y el vestuario, si bien este se empleó a menudo de forma anacrónica —se representaba por ejemplo una obra ambientada en Roma con ropajes franceses del siglo XVII hasta la aparición del realismo. En la actualidad, la elección del vestuario no es sino un elemento más dentro de la concepción general del montaje.

El narrador de parábolas hace bien en mostrar abiertamente al espectador todo lo que necesita para su parábola, esos elementos cuya ayuda pretende mostrar el curso ineluctable de su acción. El constructor escénico de la parábola muestra pues abiertamente los focos, los instrumentos de música, las máscaras, las paredes y las puertas, las escaleras, sillas y mesas, con cuya ayuda ha de construirse la parábola.

En la disposición tradicional a la italiana, en los teatros más antiguos, la sala frente al escenario suele tener una forma de herradura. La parte baja, la más amplía, es la platea o patio de butacas, donde los sillones o butacas se reparten en filas separadas por un pasillo central y enmarcadas por dos pasillos laterales. En los teatros más antiguos, el piso del patio de butacas es plano y ligeramente inclinado para preservar un mínimo de visibilidad. En los teatros contemporáneos, detrás del patio de butacas se encuentran los palcos y un anfiteatro en gradas que permite una buena visibilidad del escenario desde las filas más alejadas.

Para aprovechar mejor la altura del teatro, la sala se estructura en varias plantas. Sobre el patio de butacas pueden existir una o dos amplías plantas voladas y retranqueadas. Los paramentos centrales y laterales se dedican a los palcos o a galerías abalconadas que se reparten en varias plantas. Tradicionalmente, la parte más alta del teatro se denomina gallinero; es la de menor visibilidad y la más económica.

De este modo, el teatro se estructura en platea (planta baja), palcos (situados en la entreplanta) y anfiteatro (situados en las plantas superiores), ordenado de mayor a menor precio de la entrada.

Son muchas las supersticiones que se han conservado en el medio teatral de la cultura de Occidente, creencias y costumbres que han ido perdiendo fuerza en tiempos más recientes pero que aún determinan el «modus operandi» en diferentes aspectos del espectáculo. De la larga lista de supersticiones se pueden mencionar:






</doc>
<doc id="2781" url="https://es.wikipedia.org/wiki?curid=2781" title="Texto">
Texto

Un texto es una composición de signos codificados en unsistema de escritura que forma una unidad de sentido. 

También es una composición de caracteres imprimibles (con grafema) generados por un algoritmo de cifrado que, aunque no tienen sentido para cualquier persona, sí puede ser descifrado por su destinatario original. En otras palabras, un texto es un entramado de signos con una intención comunicativa que adquiere sentido en determinado contexto.

Las ideas que comunica un texto están contenidas en lo que se suele denominar «macroproposiciones», unidades estructurales de nivel superior o global, que otorgan coherencia al texto constituyendo su hilo central, el esqueleto estructural que cohesiona elementos lingüísticos formales de alto nivel, como los títulos y subtítulos, la secuencia de párrafos, etc. En contraste, las «microproposiciones» son los elementos coadyuvantes de la cohesión de un texto, pero a nivel más particular o local. Esta distinción fue realizada por Teun van Dijk en 1980.

El nivel microestructural o local está asociado con el concepto de cohesión. Se refiere a uno de los fenómenos propios de la coherencia, el de las relaciones particulares y locales que se dan entre elementos lingüísticos, tanto los que remiten unos a otros como los que tienen la función de conectar y organizar. 
También es un conjunto de oraciones agrupadas en párrafos que habla de un tema determinado.

De acuerdo a Greimas, es un enunciado ya sea gráfico o fónico que nos permite visualizar las palabras que escuchamos y que es utilizado para manifestar el proceso lingüístico. Mientras Hjelmslev usa ese término para designar el todo de una cadena lingüística ilimitada (§1).

En lingüística, no todo conjunto de signos constituye un texto.

Se le llama texto a la configuración de lengua o habla y se utilizan signos específicos (signo de la lengua o habla) y está organizada según reglas del habla o idioma.

Otra noción importante es que los textos (y discursos) no son solo "monologales".
En lingüística, el término texto sirve tanto para producciones en que solo hay un emisor (situaciones monogestionadas o monocontroladas) como en las que varios intercambian sus papeles (situaciones poligestionadas o policontroladas) como las conversaciones. El texto contiene conectores y signos, etc.

Ejemplos :

Este texto o conjunto de signos extraídos de un discurso debe reunir condiciones de textualidad. Las principales son:


Según los lingüistas Beaugrande y Dressler, todo texto bien elaborado ha de presentar siete características:


Así pues, un texto ha de ser coherente, cohesionado, comprensible para su lector ideal, intencionado, enmarcado en una situación comunicativa e inmerso en otros textos o géneros para alcanzar sentido; igualmente ha de poseer información en grado suficiente para resultar novedoso e interesante.

A fin de agrupar y clasificar la enorme diversidad de textos, se han propuesto . Estas se basan en distintos criterios como la función que cumple el texto en relación con los interlocutores o la estructura global interna que presenta.

La clasificación más simple de los textos, en función de las características que predominan en cada uno (se considera que no hay texto puro, es decir, no hay texto que tenga rasgos correspondientes únicamente a cada categoría, todo texto es híbrido), es como sigue:




</doc>
<doc id="2791" url="https://es.wikipedia.org/wiki?curid=2791" title="Troyano">
Troyano

Troyano hace referencia a varios artículos:



</doc>
<doc id="2792" url="https://es.wikipedia.org/wiki?curid=2792" title="Tetris">
Tetris

Tetris (transliterado al cirílico: Те́трис) es un videojuego de lógica originalmente diseñado y programado por Alekséi Pázhitnov en la Unión Soviética. Fue lanzado el 6 de junio de 1984, mientras trabajaba para el Centro de Computación Dorodnitsyn de la Academia de Ciencias de la Unión Soviética en Moscú, RSFS de Rusia. Su nombre deriva del prefijo numérico griego "tetra"- (todas las piezas del juego, conocidas como Tetrominós que contienen cuatro segmentos) y del tenis, el deporte favorito de Pázhitnov.

En el "Tetris" se juega con los tetrominós, el caso especial de cuatro elementos de poliominós. Los poliominós se han utilizado en los rompecabezas populares por lo menos desde 1907, y el nombre fue dado por el matemático Solomon W. Golomb en 1953. Sin embargo, incluso la enumeración de los pentominós data de la antigüedad. 
El juego (o una de sus muchas variantes) está disponible para casi cada consola de videojuegos y sistemas operativos de PC, así como en dispositivos tales como las calculadoras gráficas, teléfonos móviles, reproductores de multimedia portátiles, PDAs, reproductores de música en red e incluso como huevo de pascua en productos no mediáticos como los osciloscopios. También ha inspirado servicios de mesa y ha sido jugado en los costados de varios edificios, manteniendo el récord de ser el juego completamente funcional más grande del mundo gracias al esfuerzo de estudiantes holandeses en 1995 que iluminaron quince pisos del Departamento de Ingeniería Eléctrica en la Universidad Técnica de Delft.

Aunque diferentes versiones de Tetris se habían vendido para una amplia gama de plataformas de ordenadores domésticos y arcades durante los años 1980, fue la inmensamente exitosa versión portátil para la Game Boy lanzada en 1989 la que lo convirtió en uno de los juegos más populares de todos los tiempos. La edición número 100 del "Electronic Gaming Monthly" otorgó a Tetris el número 1 en el ranking de "Mejores juegos de todos los tiempos". En 2007, Tetris ocupó el segundo lugar en los «100 mejores videojuegos de todos los tiempos» para IGN. Ha vendido más de ciento setenta millones de copias para el año 2016. En enero de 2010, se anunció que el Tetris había vendido más de cien millones de unidades para teléfonos móviles desde el año 2005.

Alekséi Pázhitnov (Sokolov) se había inspirado en un juego de pentaminós que había comprado anteriormente. El nombre «tetris» deriva del étimo griego «tetra», que significa «cuatro», y hace referencia a la cantidad de cuadros que componen las piezas. Alekséi Pázhitnov programó una versión de su juego en un Electrónika 60, según la leyenda en una sola tarde. Hay que tener en cuenta que lo realmente complejo fue llegar a la idea original del juego y no la programación en sí misma. Hoy por hoy la mecánica del juego es muy conocida y es sencillo emularla.

"Tetris" comienza a ganar popularidad cuando Vadim Gerasimov, un joven de dieciséis años que trabajaba en la Academia, portó el juego a IBM PC. Desde ahí se distribuye gratuitamente a Hungría, donde es programado para Apple II y Commodore 64 por programadores húngaros. Estas versiones llaman la atención de Robert Stein, que intenta adquirir los derechos del juego. Antes de conseguir estos derechos, vende el concepto robado a la empresa inglesa Mirrorsoft y a su filial estadounidense: Spectrum Holobyte, que editan una versión para Atari ST y Sinclair ZX Spectrum. "Tetris" se comercializa en Europa y Estados Unidos en 1987 con la mención: «Fabricado en Estados Unidos, creado en el extranjero».

Tetris ha sido históricamente uno de los videojuegos más versionados y es, junto a las Torres de Hanói, el predilecto por los programadores noveles de juegos. Lucharon por robarse la idea y licenciarla Atari y Nintendo, lográndolo finalmente este último gracias a Henk Rogers. "Tetris" fue el juego que acompañaría a su novedosa consola portátil Game Boy en su debut, lo que popularizó tanto "Tetris" como la consola por todo el mundo.

En 1991, Alekséi Pázhitnov emigra a Estados Unidos y, cinco años después, en 1996 funda su propia compañía, "Tetris Company", junto con Henk Rogers y se apropia de los derechos de autor.

Después del éxito de este juego, muchos otros trataron de imitarlo. Juegos como "Columns" o "Collapse" son ejemplos que han intentado seguir la estela que dejó "Tetris", un juego que inauguró un género dentro del panorama arcade. Atari, por su parte, como «contraataque» por perder la licencia de este videojuego, sacó al mercado "Klax", un juego de habilidad e inteligencia con una temática similar al "Columns" o "Puyo Puyo".

Existe una versión gratuita muy popular en Internet denominada "TetriNET", que proporciona una versión multijugador en arquitectura cliente-servidor, en el que se pueden enfrentar a través de la red de dos a seis personas con la posibilidad de crear equipos. Fue creada en 1997 por St0rmCat y se encuentran en la actualidad clientes para los sistemas operativos Windows (el propio "TetriNET" y "Blocktrix", entre otros), GNU/Linux ("GTetrinet") y Mac OS X ("Tetrinet Aqua"). La singularidad de esta versión es que añade unos bonus especiales llamados en inglés "cookies" que permiten alterar el juego de los adversarios.

En el siglo XXI sigue siendo un juego popular entre una comunidad, llegando a superarse records.

Distintos tetriminos, figuras geométricas compuestas por cuatro bloques cuadrados unidos de forma ortogonal, las cuales se generan de una zona que ocupa 5x5 bloques en el área superior de la pantalla. No hay consenso en cuanto a las dimensiones para el área del juego, variando en cada versión. Sin embargo, dos filas de más arriba están ocultas al jugador.

El jugador no puede impedir esta caída, pero puede decidir la rotación de la pieza (0°, 90°, 180°, 270°) y en qué lugar debe caer. Cuando una línea horizontal se completa, esa línea desaparece y todas las piezas que están por encima descienden una posición, liberando espacio de juego y por tanto facilitando la tarea de situar nuevas piezas. La caída de las piezas se acelera progresivamente. El juego acaba cuando las piezas se amontonan hasta llegar a lo más alto (3x5 bloques en el área visible), interfiriendo la creación de más piezas y finalizando el juego.

Existen distintas versiones del juego. La original tiene siete piezas diferentes. Licencias posteriores añadieron formas suplementarias y existen incluso ciertas licencias para formas tridimensionales.

La versión original de Pajitnov para la computadora Electronika 60 utilizaba soportes verdes para representar bloques.
Algunas personas se refieren a las piezas mediante el color del que están pintadas en una versión particular del juego "Tetris", pero antes de la estandarización por parte de The Tetris Company en el año 2000, dichos colores han variado versión tras versión, con lo cual llamar a las piezas por su color carece de sentido. Por ejemplo, la pieza T es diferente en todas las versiones del juego.

Existen muchas variantes de Tetris. Actualmente Blue Planet Software es la propietaria de la propiedad intelectual del juego, vigilando la calidad de las versiones que se desarrollan

Es una variante del "Tetris" en la que el objetivo no es formar las clásicas líneas, sino más bien juntar grupos de tres o más bloques del mismo color. Los "Qwirks" son bloques de cierto color emparejados en una pieza. Estas piezas caen en principio como en el "Tetris" clásico, siendo el objetivo el planteado líneas arriba. Tiene varias modalidades, entre la que se destaca el modo «Duelo» contra la computadora y es representada por rivales virtuales con formas de animales. Al derrotar a cada rival, la dificultad y la velocidad del juego aumentan.

Cabe destacar que, cada vez que los jugadores juntan un número específico de bloques del mismo color, caen más "Qwirks" en la zona del contrincante.

El objetivo en esta variación es también formar líneas horizontales con los bloques, aunque aquí hay una serie de nuevas reglas, las cuales son las siguientes:

Se trata de un juego similar en apariencia al Tetris, en el que el Mario de Nintendo se hace pasar por doctor, dejando caer píldoras en el campo de juego (diseñado en forma de botella). El jugador debe alinear las píldoras para conseguir derrotar a uno o más virus dependiendo de la alineación y de sus colores. Las píldoras tienen dos partes, y cada una de estas partes, así como los virus, puede ser de uno de estos tres colores: rojo, amarillo y azul. Se obtienen de esta forma las seis combinaciones de cápsulas que se pueden dar en el juego, y que el jugador deberá ir rotando y alineando de forma que 4 o más bloques (que pueden ser virus, o partes de cápsulas) del mismo color en la misma fila o columna para hacerlos desaparecer. El nivel terminará de forma exitosa cuando sean eliminados todos los virus de la botella. El juego finaliza cuando las cápsulas bloqueen el cuello de la botella y no puedan entrar más. El juego tiene 21 niveles, y se diferencian en el progresivo número de virus que aparecen inicialmente en cada nivel.

El fundamento del juego son las poliformas conocidas como poliominós, más concretamente las combinaciones de tetraminós. Si, por ejemplo, se produce una larga secuencia de piezas en forma de Z, en algún momento el jugador estará obligado a dejar un hueco en la esquina derecha, sin poder rellenar el hueco anterior. Ahora se produce una nueva secuencia de piezas en forma de S, y así hasta que las piezas se amontonan y acaba el juego.

Como la distribución de las piezas es aleatoria, esta secuencia terminará ocurriendo. En la práctica, esto no sucede porque el generador de números pseudoaleatorios utilizado en la mayor parte de las implementaciones es un generador lineal de congruencias que no devuelve una secuencia así.

Incluso con un generador teóricamente perfecto de números aleatorios y gravedad ingenua, un buen jugador podrá resistir la caída de ciento cincuenta piezas, todas en forma de S o Z. La probabilidad de que, en un momento dado, las próximas ciento cincuenta piezas sean todas así es de 1 / (7/2) (aproximadamente 1 / (4 × 10)). Este número tiene el mismo orden de magnitud que el número de átomos que hay en el universo conocido.

Se ha demostrado que varios de los subproblemas de Tetris son NP-completo.

De acuerdo con el Dr. Richard Haier, jugar al "Tetris" de forma prolongada puede llevar a una actividad cerebral más eficiente durante el juego. Cuando se juega a "Tetris" por primera vez, aumentan la función y actividad cerebral, incrementándose también el consumo de energía cerebral, medido por la tasa metabólica de la glucosa. A medida que el jugador de "Tetris" se vuelve más hábil, el cerebro reduce su consumo de energía y glucosa, indicando una actividad cerebral más eficiente para el juego. Jugar al "Tetris" de forma moderada (media hora al día por un período de tres meses) incrementa las funciones cognitivas tales como «pensamiento crítico, razonamiento, procesamiento del lenguaje» y aumenta también el espesor de la corteza cerebral.

En enero de 2009, un grupo de investigación de la Universidad de Oxford dirigido por la Dra. Emily Holmes informó en PLoS ONE que en voluntarios sanos, jugar "Tetris" poco después de ver material traumático en el laboratorio redujo el número de "flashbacks" que esas escenas provocaban a la semana siguiente. Creen que el juego puede interrumpir los recuerdos que se conservan de las visiones y los sonidos que se presenciaron, y que luego se vuelven a experimentar a través de "flashbacks" involuntarios y angustiantes. El grupo espera desarrollar más este enfoque como una posible intervención para reducir los "flashbacks" experimentados en el trastorno de estrés postraumático, pero enfatizó que estos son solo resultados preliminares.

Otro efecto notable es que, de acuerdo con un estudio canadiense de abril de 2013, se ha encontrado que jugar "Tetris" ayuda a los adolescentes con ambliopía (ojo perezoso), lo cual es mejor que tapar con un parche el ojo sano de la víctima para entrenar su ojo más débil. El Dr. Robert Hess, del equipo de investigación, dijo: «Es mucho mejor que aplicar parches, es mucho más divertido, es más rápido y parece funcionar mejor». Cuando se probó en el Reino Unido también pareció ayudar a los niños con ese problema.

El juego puede provocar que se imaginen combinaciones de "Tetris" de forma involuntaria aun cuando no se esté jugando (el llamado Efecto Tetris,) aunque esto puede ocurrir con cualquier videojuego o situación real que proyecte las imágenes o escenarios de forma repetida, tales como rompecabezas.






</doc>
<doc id="2793" url="https://es.wikipedia.org/wiki?curid=2793" title="Terminal">
Terminal

Terminal hace referencia a varios artículos:

En cualquier transporte público, al sitio de llegada o salida de la línea:











</doc>
<doc id="2795" url="https://es.wikipedia.org/wiki?curid=2795" title="Tales de Mileto">
Tales de Mileto

Tales de Mileto (en griego antiguo: Θαλῆς ὁ Μιλήσιος Thalḗs o Milḗsios; Mileto, c. 624 a. C.-"ibid"., c. 546 a. C.) fue un filósofo, matemático, geómetra, físico y legislador griego.

Vivió y murió en Mileto, polis griega de la costa jonia (hoy en Turquía). Aristóteles lo consideró como el iniciador de la escuela de Mileto, a la que pertenecieron también Anaximandro (su discípulo) y Anaxímenes (discípulo del anterior). En la antigüedad se le consideraba uno de los Siete Sabios de Grecia. No se conserva ningún texto suyo y es probable que no dejara ningún escrito a su muerte. Desde el , se le atribuyen importantes aportaciones en el terreno de la filosofía, la matemática, la astronomía, la física, etc; así como un activo papel como legislador en su ciudad natal.

A menudo, Tales es reconocido por romper con el uso de la mitología para explicar el mundo y el universo, cambiándolo en su lugar por explicaciones naturales mediante teorías e hipótesis naturalista (logos), es considerado el iniciador de la especulación científica y filosófica griega y occidental, aunque su figura y aportaciones están rodeadas de grandes incertidumbres. Como la mayoría de filósofos presocráticos, Tales explicó que el principio originario de la naturaleza y de la materia era una única sustancia última (arché): el agua.

Aunque la tradición insistentemente le atribuyó a Tales el haber comenzado a usar el pensamiento deductivo aplicado a la geometría, no hay absolutamente ningún documento que respalde tal cosa, y tampoco se le puede adjudicar el desarrollo de los dos teoremas geométricos que llevan su nombre. 

Los datos biográficos de Tales de Mileto son una mezcla de opiniones, hechos atribuidos a su persona, y citas con mayor o menor grado de verosimilitud, recogidas de diversos autores de épocas bastante posteriores, reinterpretados y expuestos a la luz de la mentalidad del narrador. Nació y murió en Mileto (griego: Μίλητος, turco: "Milet") c. 624 a. C., una antigua ciudad en la costa occidental de Asia Menor (en lo que actualmente es la provincia de Aydin en Turquía), cerca de donde desemboca el río Menderes. 
La mayoría de los historiadores lo presentan como genuino milesio (aunque, según Diógenes Laercio, doxógrafo griego, fue admitido en la ciudad jonia de Mileto, a orillas del mar Egeo, después de ser expulsado de Fenicia junto con Nileo). Al margen de si nació o no en Mileto, está claro que residió en aquella ciudad y que fue allí donde desarrolló su filosofía, sus investigaciones científicas y sus intervenciones políticas.

Era hijo de Euxamias (o Examio) y de Cleobulinas (o Cleóbula), ambos oriundos de Fenicia y descendientes de Cadmo y Agenor. Puesto que los jonios comerciaban frecuentemente con Egipto y Babilonia, es probable que Tales visitara Egipto en alguna etapa de su vida, y allí podría, por un lado, haber recibido enseñanzas de los sacerdotes, quienes registraban con mucho celo todo evento astronómico o meteorológico excepcional por motivos religiosos y que poseían, por consiguiente, copiosa información al respecto; y, por el otro, haber adquirido conocimientos matemáticos, que los egipcios habían desarrollado a un nivel práctico con el fin de medir y delimitar las parcelas de tierra cuyos límites solían borrarse con las continuas crecidas del río Nilo.

Podrían haber sido condiscípulos suyos Solón y Ferécides de Siros. Además, una fuente lo vincula con Pitágoras, a quien habría recomendado viajar a Egipto y educarse con los sacerdotes de Menfis y Dióspolis. Sin embargo, estos datos en absoluto son confiables, puesto que provienen de fuentes muy alejadas de la época de Tales. De los babilonios pudo también haber obtenido conocimientos científicos. Sí es más seguro que el filósofo Anaximandro haya sido su discípulo, así como Anaxímenes el de este.

Tanto Heródoto (I, 170) como Diógenes Laercio (I, 25) lo señalan como un sabio consejero político de jonios y lidios.

Entre las anécdotas que de Tales se cuentan, relata Heródoto (I, 75) que logró desviar el río Halys para que fuera cruzado por el ejército de Creso (Heródoto mismo descree de esto, pero modernos especialistas no descartan por completo su veracidad). Aristóteles, por su parte, cuenta en su "Política" (I, 11, 1259a) cómo una vez que, habiéndosele reprochado su pobreza y su falta de preocupación por los asuntos materiales, y luego de haber previsto, gracias a sus conocimientos astronómicos, que habría una próspera cosecha de aceitunas la siguiente temporada, compró durante el invierno todas las prensas de aceite de Mileto y Quíos y las alquiló al llegar la época de la recolección, acumulando una gran fortuna y mostrando así que los filósofos pueden ser ricos si lo desean, pero que su ambición es bien distinta. Quizás la anécdota más conocida de Tales es aquella que nos refiere Heródoto: que predijo a los jonios el año en que sucedería un eclipse solar (lo que desde 2005 se sabe que fue por el conocimiento de un ciclo de eclipses babilónico), hacia el año 585 a. C. El eclipse ocurrió, en efecto, en medio de una batalla, lo que llevó a los contendientes a detenerse y a avanzar un acuerdo de paz, por temor de que el evento fuera una advertencia divina.

También es muy conocido lo que cuenta Platón, por boca de Sócrates, en su diálogo "Teeteto" (174 A): que, al caer Tales en un pozo por ir mirando el movimiento de las estrellas, una campesina tracia se reía mientras el filósofo se excusaba diciendo «que tenía ansias de conocer las cosas del cielo pero que lo que estaba... justo a sus pies se le escapaba»

Apolodoro, en sus "Crónicas", afirma que murió a la edad de setenta y ocho años; Sosícrates, que murió en la olimpiada LVIII, a la edad de noventa años. Otra fecha en la que se afirma que murió se da en el año 585 a. C., aunque actualmente se acepta que murió cerca del año 546 a. C.

Simplicio de Cilicia escribió: «Se dice de Tales que no dejó nada escrito, excepto la llamada "Astrología náutica" "(Ναυτιχῆς αστρολογίας)"».

En cambio Diógenes Laercio escribe: «Según algunos, nada dejó escrito, pues dicen que la "Astrología náutica" que se le atribuye es de Foco Samio [...] Pero, según otros, escribió dos obras: "Sobre el solsticio" y "Sobre el equinoccio"».

Así, son tres las líneas de opinión: que solo escribió la "Astrología", que solo escribió "Sobre el solsticio" y "Sobre el equinoccio" y que no escribió nada. De cualquier manera, lo cierto es que, de haber escrito algo, sus escritos se perdieron pronto, y, respecto de las pocas fuentes que citan presuntos dichos de Tales, no puede determinarse con certeza si tales fuentes tenían en sus manos o bien escritos de Tales o bien fuentes secundarias o si solo repetían tradiciones orales.

Se atribuyen a Tales varios descubrimientos matemáticos registrados en los "Elementos" de Euclides: la definición I. 17 y las proposiciones I. 5, I. 15, I. 26 y III. 31.

Asimismo es muy conocida la leyenda acerca de un método de comparación de sombras que Tales habría utilizado para medir la altura de las pirámides egipcias: el milesio se percató de que se podría saber la altura exacta de las pirámides midiendo la sombra de estas en el momento del día en que su sombra era más o menos de igual tamaño que su cuerpo. Este método fue aplicado luego a otros fines prácticos de la navegación.

Se supone además que Tales conocía ya muchas de las bases de la geometría, como el hecho de que cualquier diámetro de un círculo lo dividiría en partes idénticas, que un triángulo isósceles tiene por fuerza dos ángulos iguales en su base o las propiedades relacionales entre los ángulos que se forman al cortar dos paralelas por una tercera línea recta.

Los egipcios habían aplicado algunos de estos conocimientos para la división y parcelación de sus terrenos. Esta necesidad surgió a raíz de que el Nilo, con sus constantes crecidas, borraba las líneas divisorias de los campos de cultivo, por lo que era necesaria una manera de medir de nuevo el terreno. Mas, según los pocos datos con los que se cuenta, Tales se habría dedicado en Grecia mucho menos al espacio (a las superficies) y mucho más a las líneas y a las curvas, alcanzando así su geometría un mayor grado de complejidad y abstracción.

Se considera a Tales de Mileto como el primer filósofo de Occidente por haber sido quien intentó la primera explicación racional a distintos fenómenos del mundo de la que se tiene constancia en la historia de la cultura occidental. En su tiempo predominaban aún las concepciones míticas, pero Tales buscaba una explicación racional, lo que se conoce como «el paso del mito al "logos"», donde la palabra griega "logos" alude en este contexto a «razón», uno de sus significados en castellano.

Es muy probable que haya sido uno de los primeros hombres que llevaron la geometría al mundo griego, y Aristóteles lo consideraba el primero de los φυσικόι o ‘filósofos de la naturaleza’. Muchas de estas ideas parecen provenir de su educación egipcia. Igualmente, su idea de que la tierra flota sobre el agua puede haberse desprendido de ciertas ideas cosmogónicas de Oriente Próximo.

La filosofía de Tales de Mileto no se conoce de primera mano, pues no ha sobrevivido ningún escrito de Tales (de hecho, ni siquiera es seguro que haya escrito algo). Las afirmaciones registradas que se le atribuyen probablemente hayan llegado a los transmisores por segunda mano o incluso por tradición oral; entre las ideas que se le atribuyen, no es posible establecer a ciencia cierta cuánto es realmente de lo que Tales dijo como tampoco si Tales se expresó en los mismos términos en que sus ideas se han transmitido. En cuanto a su filosofía, contamos con el importante aporte de Aristóteles, el cual, en su descripción, diferencia los dichos atribuibles con alguna certeza al mismo Tales («Tales dijo que...») de los hechos dudosos («dicen que Tales dijo que...») y de sus propias opiniones («quizá Tales quiso decir que...»). Aristóteles lo considera, en su relato de las ideas metafísicas ("Metafísica", libro A) como el primero que se dedicó a investigar las primeras causas y los primeros principios, señalándolo así como el primer filósofo y fundador de la filosofía natural.

Cabe destacar que en su época, estos primeros filósofos (los presocráticos) no trataban acerca de ética, política o moral, de hecho se les considera físicos porque teorizaban racionalmente sobre el origen del universo, se dedicaban al estudio de la naturaleza y empezaron a estudiar el campo de las matemáticas, geometría y aritmética.

La explicación universal y racional que sostuvo Tales fue que el agua es el origen de todas las cosas que existen, el elemento primero:

En cuanto al alma, la considera como dadora de vida, movimiento y divina. Como en la época en la que vive, todavía no se diferenciaba entre seres vivientes y no vivientes. Tales atribuye vida al agua, porque como el agua se mueve sola (véanse los mares o los ríos), esta debe tener alma, puesto que el alma es lo que hace moverse las cosas. Y también es divina (está llena de dioses) porque el alma es divina para él. «Así por lo tanto, el agua para Tales es, el origen de todo, está llena de dioses y tiene vida propia». Y de manera parecida que con el agua, razona para con las piedras imán. Como estas se mueven solas, piensa que están vivas, o que «hay algo vivo en ellas».

Y por último, de nuevo Aristóteles en "Sobre el cielo" y Séneca en "Cuestiones naturales" afirman que Tales sostenía que la tierra sobre la que pisamos es una especie de isla que «flota» sobre el agua de forma parecida a un leño y por ello la tierra a veces tiembla. Al no estar sostenida sobre unas bases fijas si no que como está flotando sobre el agua, esta la hace tambalearse.

Con todo esto, se puede entender claramente por qué se considera a Tales de Mileto como el primer filósofo de Occidente, y es que, como ya hemos dicho, fue el primer hombre occidental (del que se sabe) que trató de conocer la verdad del mundo mediante explicaciones racionales y no fantásticas o místicas, como hasta entonces se hacía en la Antigua Grecia por medio de los mitos. Y por lo tanto, Tales es verdaderamente importante para la Historia de la filosofía occidental. Fue el iniciador de la misma y con ello, creó un legado de búsqueda y amor a la sabiduría, que continuará inmediatamente con Anaximandro y Anaxímenes, y que llegará a su esplendor, en la Antigua Grecia; más de un siglo después con Sócrates, Platón y Aristóteles: tres filósofos que se han convertido en los pilares del pensamiento que hoy conocemos bajo el nombre de filosofía occidental.









</doc>
<doc id="2796" url="https://es.wikipedia.org/wiki?curid=2796" title="Telnet">
Telnet

Telnet ("Teletype Network") es el nombre de un protocolo de red que nos permite acceder a otra máquina para manejarla remotamente como si estuviéramos sentados delante de ella. También es el nombre del programa informático que implementa el cliente. Para que la conexión funcione, como en todos los servicios de Internet, la máquina a la que se acceda debe tener un programa especial que reciba y gestione las conexiones. El puerto que se utiliza generalmente es el 23.

Telnet sólo sirve para acceder en modo terminal, es decir, sin gráficos, pero es una herramienta muy útil para arreglar fallos a distancia, sin necesidad de estar físicamente en el mismo sitio que la máquina que los tenga. También se usaba para consultar datos a distancia, como datos personales en máquinas accesibles por red, información bibliográfica, etc. 

Aparte de estos usos, en general telnet se ha utilizado (y aún hoy se puede utilizar en su variante SSH) para abrir una sesión con una máquina UNIX, de modo que múltiples usuarios con cuenta en la máquina, se conectan, abren sesión y pueden trabajar utilizando esa máquina. Es una forma muy usual de trabajar con sistemas 
UNIX.

Su mayor problema es de seguridad, ya que todos los nombres de usuario y contraseñas necesarias para entrar en las máquinas viajan por la red como "texto plano" (cadenas de texto sin cifrar). Esto facilita que cualquiera que espíe el tráfico de la red pueda obtener los nombres de usuario y contraseñas, y así acceder él también a todas esas máquinas. Por esta razón dejó de usarse, casi totalmente, hace unos años, cuando apareció y se popularizó el SSH, que puede describirse como una versión cifrada de telnet -actualmente se puede cifrar toda la comunicación del protocolo durante el establecimiento de sesión (RFC correspondiente, en inglés- si cliente y servidor lo permiten, aunque no se tienen ciertas funcionalidades extra disponibles en SSH).

Hoy en día este protocolo también se usa para acceder a los BBS, que inicialmente eran accesibles únicamente con un módem a través de la línea telefónica. Para acceder a un BBS mediante telnet es necesario un cliente que dé soporte a gráficos ANSI y protocolos de transferencia de ficheros. Los gráficos ANSI son muy usados entre los BBS. Con los protocolos de transferencia de ficheros (el más común y el que mejor funciona es el ZModem) podrás enviar y recibir ficheros del BBS, ya sean programas o juegos o ya sea el correo del BBS (correo local, de FidoNet u otras redes).

Algunos clientes de telnet (que soportan gráficos ANSI y protocolos de transferencias de ficheros como Zmodem y otros) son mTelnet!, NetRunner, Putty, Zoc, etc.

Para iniciar una sesión con un intérprete de comandos de otro ordenador, puede emplear el comando "telnet" seguido del nombre o la dirección IP de la máquina en la que desea trabajar. Por ejemplo, si desea conectarse a la máquina "purpura.micolegio.edu.com", deberá teclear codice_1 y, para conectarse con la dirección IP 1.2.3.4, deberá utilizar codice_2.

Una vez conectado, podrá ingresar el nombre de usuario y contraseña remoto para iniciar una sesión en modo texto a modo de consola virtual (ver Lectura Sistema de usuarios y manejo de clave). La información que transmita (incluyendo su clave) no será protegida o cifrada y podría ser vista en otros computadores por los que transite la información (la captura de estos datos se realiza con un packet sniffer).

Una alternativa más segura que telnet, pero que requiere más recursos del computador, es SSH. Este cifra la información antes de transmitirla, autentica la máquina a la cual se conecta y puede emplear mecanismos de autenticación de usuarios más seguros.

Actualmente hay sitios para hackers en los que se entra por telnet y se van sacando las password para ir pasando de nivel, ese uso de telnet aún es vigente.

Hay 3 razones principales por las que el telnet no se recomienda para los sistemas modernos desde el punto de vista de la seguridad:


En ambientes donde es importante la seguridad, por ejemplo en el Internet público, telnet no debe ser utilizado. Las sesiones de telnet no son cifradas. Esto significa que cualquiera que tiene acceso a cualquier router, switch, o gateway localizado en la red entre los dos anfitriones donde se está utilizando telnet puede interceptar los paquetes de telnet que pasan cerca y obtener fácilmente la información de la conexión y de la contraseña (y cualquier otra cosa que se mecanografía) con cualesquiera de varias utilidades comunes como "tcpdump" y "Wireshark".

Estos defectos han causado el abandono y despreciación del protocolo telnet rápidamente, a favor de un protocolo más seguro y más funcional llamado SSH, lanzado en 1995. SSH provee de toda la funcionalidad presente en telnet, la adición del "cifrado fuerte" para evitar que los datos sensibles tales como contraseñas sean interceptados, y de la autenticación mediante llave pública, para asegurarse de que el "computador remoto" es realmente quién dice ser.

Los expertos en seguridad computacional, tal como el instituto de SANS, y los miembros del newsgroup de "comp.os.linux.security" recomiendan que el uso del telnet para las conexiones remotas debería ser descontinuado bajo cualquier circunstancia normal.

Cuando el telnet fue desarrollado inicialmente en 1969, la mayoría de los usuarios de computadoras en red estaban en los servicios informáticos de instituciones académicas, o en grandes instalaciones de investigación privadas y del gobierno. En este ambiente, la seguridad no era una preocupación y solo se convirtió en una preocupación después de la explosión del ancho de banda de los años 90. Con la subida exponencial del número de gente con el acceso al Internet, y por la extensión, el número de gente que procura crackear los servidores de otra gente, telnet podría no ser recomendado para ser utilizado en redes con conectividad a Internet. 




</doc>
<doc id="2797" url="https://es.wikipedia.org/wiki?curid=2797" title="Tabla periódica de los elementos">
Tabla periódica de los elementos

La tabla periódica de los elementos es una disposición de los elementos químicos en forma de tabla, ordenados por su número atómico (número de protones), por su configuración de electrones y sus propiedades químicas. Este ordenamiento muestra "tendencias periódicas", como elementos con comportamiento similar en la misma columna.

En palabras de Theodor Benfey, la tabla y la ley periódica «son el corazón de la química —comparables a la teoría de la evolución en biología (que sucedió al concepto de la Gran Cadena del Ser), y a las leyes de la termodinámica en la física clásica—».

Las filas de la tabla se denominan períodos y las columnas grupos. Algunos grupos tienen nombres, así por ejemplo el grupo 17 es el de los halógenos y el grupo 18 el de los gases nobles. La tabla también se divide en cuatro bloques con algunas propiedades químicas similares. Debido a que las posiciones están ordenadas, se puede utilizar la tabla para obtener relaciones entre las propiedades de los elementos, o pronosticar propiedades de elementos nuevos todavía no descubiertos o sintetizados. La tabla periódica proporciona un marco útil para analizar el comportamiento químico y es ampliamente utilizada en química y otras ciencias.

Dmitri Mendeléyev publicó en 1869 la primera versión de tabla periódica que fue ampliamente reconocida, la desarrolló para ilustrar tendencias periódicas en las propiedades de los elementos entonces conocidos, al ordenar los elementos basándose en sus propiedades químicas, si bien Julius Lothar Meyer, trabajando por separado, llevó a cabo un ordenamiento a partir de las propiedades físicas de los átomos. Mendeléyev también pronosticó algunas propiedades de elementos entonces desconocidos que anticipó que ocuparían los lugares vacíos en su tabla. Posteriormente se demostró que la mayoría de sus predicciones eran correctas cuando se descubrieron los elementos en cuestión. 

La tabla periódica de Mendeléyev ha sido desde entonces ampliada y mejorada con el descubrimiento o síntesis de elementos nuevos y el desarrollo de modelos teóricos nuevos para explicar el comportamiento químico. La estructura actual fue diseñada por Alfred Werner a partir de la versión de Mendeléyev. Existen además otros arreglos periódicos de acuerdo a diferentes propiedades y según el uso que se le quiera dar (en didáctica, geología, etc).

Se han descubierto o sintetizado todos los elementos de número atómico del 1 (hidrógeno) al 118 (oganesón); la IUPAC confirmó los elementos 113, 115, 117 y 118 el 30 de diciembre de 2015, y sus nombres y símbolos oficiales se hicieron públicos el 28 de noviembre de 2016. Los primeros 94 existen naturalmente, aunque algunos solo se han encontrado en cantidades pequeñas y fueron sintetizados en laboratorio antes de ser encontrados en la naturaleza. Los elementos con números atómicos del 95 al 118 solo han sido sintetizados en laboratorios. Allí también se produjeron numerosos radioisótopos sintéticos de elementos presentes en la naturaleza. Los elementos del 95 a 100 existieron en la naturaleza en tiempos pasados pero actualmente no. La investigación para encontrar por síntesis nuevos elementos de números atómicos más altos continúa.

<br>

<br>

La historia de la tabla periódica está íntimamente relacionada con varios aspectos del desarrollo de la química y la física: 

Aunque algunos elementos como el oro (Au), plata (Ag), cobre (Cu), plomo (Pb) y mercurio (Hg) ya eran conocidos desde la antigüedad, el primer descubrimiento científico de un elemento ocurrió en el , cuando el alquimista Hennig Brand descubrió el fósforo (P). En el se conocieron numerosos nuevos elementos, los más importantes de los cuales fueron los gases, con el desarrollo de la química neumática: oxígeno (O), hidrógeno (H) y nitrógeno (N). También se consolidó en esos años la nueva concepción de elemento, que condujo a Antoine Lavoisier a escribir su famosa lista de sustancias simples, donde aparecían 33 elementos. A principios del , la aplicación de la pila eléctrica al estudio de fenómenos químicos condujo al descubrimiento de nuevos elementos, como los metales alcalinos y alcalino-térreos, sobre todo gracias a los trabajos de Humphry Davy. En 1830 ya se conocían 55 elementos. Posteriormente, a mediados del , con la invención del espectroscopio, se descubrieron nuevos elementos, muchos de ellos nombrados por el color de sus líneas espectrales características: cesio (Cs, del latín "caesĭus", azul), talio (Tl, de tallo, por su color verde), rubidio (Rb, rojo), etc. Durante el , la investigación en los procesos radioactivos llevó al descubrimiento en cascada de una serie de elementos pesados (casi siempre sustancias artificiales sintetizadas en laboratorio, con periodos de vida estable muy cortos), hasta alcanzar la cifra de 118 elementos con denominación oficialmente aceptados por la IUPAC en noviembre de 2016.

Lógicamente, un requisito previo necesario a la construcción de la tabla periódica era el descubrimiento de un número suficiente de elementos individuales, que hiciera posible encontrar alguna pauta en comportamiento químico y sus propiedades. Durante los siguientes dos siglos se fue adquiriendo un mayor conocimiento sobre estas propiedades, así como descubriendo muchos elementos nuevos.

La palabra «elemento» procede de la ciencia griega, pero su noción moderna apareció a lo largo del , aunque no existe un consenso claro respecto al proceso que condujo a su consolidación y uso generalizado. Algunos autores citan como precedente la frase de Robert Boyle en su famosa obra "El químico escéptico", donde denomina elementos «ciertos cuerpos primitivos y simples que no están formados por otros cuerpos, ni unos de otros, y que son los ingredientes de que se componen inmediatamente y en que se resuelven en último término todos los cuerpos perfectamente mixtos». En realidad, esa frase aparece en el contexto de la crítica de Robert Boyle a los cuatro elementos aristotélicos. 

A lo largo del , las tablas de afinidad recogieron un nuevo modo de entender la composición química, que aparece claramente expuesto por Lavoisier en su obra "Tratado elemental de química". Todo ello condujo a diferenciar en primer lugar qué sustancias de las conocidas hasta ese momento eran elementos químicos, cuáles eran sus propiedades y cómo aislarlas.

El descubrimiento de gran cantidad de elementos nuevos, así como el estudio de sus propiedades, pusieron de manifiesto algunas semejanzas entre ellos, lo que aumentó el interés de los químicos por buscar algún tipo de clasificación.

A principios del , John Dalton (1766-1844) desarrolló una concepción nueva del atomismo, a la que llegó gracias a sus estudios meteorológicos y de los gases de la atmósfera. Su principal aportación consistió en la formulación de un «atomismo químico» que permitía integrar la nueva definición de elemento realizada por Antoine Lavoisier (1743-1794) y las leyes ponderales de la química (proporciones definidas, proporciones múltiples, proporciones recíprocas). 

Dalton empleó los conocimientos sobre proporciones en las que reaccionaban las sustancias de su época y realizó algunas suposiciones sobre el modo como se combinaban los átomos de las mismas. Estableció como unidad de referencia la masa de un átomo de hidrógeno (aunque se sugirieron otros en esos años) y refirió el resto de los valores a esta unidad, por lo que pudo construir un sistema de masas atómicas relativas. Por ejemplo, en el caso del oxígeno, Dalton partió de la suposición de que el agua era un compuesto binario, formado por un átomo de hidrógeno y otro de oxígeno. No tenía ningún modo de comprobar este punto, por lo que tuvo que aceptar esta posibilidad como una hipótesis a priori. 

Dalton sabía que una parte de hidrógeno se combinaba con siete partes (ocho, afirmaríamos en la actualidad) de oxígeno para producir agua. Por lo tanto, si la combinación se producía átomo a átomo, es decir, un átomo de hidrógeno se combinaba con un átomo de oxígeno, la relación entre las masas de estos átomos debía ser 1:7 (o 1:8 se calcularía en la actualidad). El resultado fue la primera tabla de masas atómicas relativas (o pesos atómicos, como los llamaba Dalton), que fue posteriormente modificada y desarrollada en los años posteriores. Las inexactitudes antes mencionadas dieron lugar a toda una serie de polémicas y disparidades respecto a las fórmulas y los pesos atómicos, que solo comenzarían a superarse, aunque no totalmente, en el congreso de Karlsruhe en 1860.

En 1789 Antoine Lavoisier publicó una lista de 33 elementos químicos, agrupándolos en gases, metales, no metales y tierras. 

Los químicos pasaron el siglo siguiente buscando un esquema de clasificación más preciso. Uno de los primeros intentos para agrupar los elementos de propiedades análogas y relacionarlos con los pesos atómicos se debe al químico alemán Johann Wolfgang Döbereiner (1780-1849) quien en 1817 puso de manifiesto el notable parecido que existía entre las propiedades de ciertos grupos de tres elementos, con una variación gradual del primero al último. Posteriormente (1827) señaló la existencia de otros grupos en los que se daba la misma relación —cloro, bromo y yodo; azufre, selenio y telurio; litio, sodio y potasio—.
A estos grupos de tres elementos se los denominó tríadas. Al clasificarlas, Döbereiner explicaba que el peso atómico promedio de los pesos de los elementos extremos, es parecido al del elemento en medio.
Esto se conoció como la ley de Tríadas. Por ejemplo, para la tríada cloro-bromo-yodo, los pesos atómicos son respectivamente 36, 80 y 127; el promedio es 81, que es aproximadamente 80; el elemento con el peso atómico aproximado a 80 es el bromo, lo cual hace que concuerde con el aparente ordenamiento de tríadas. 

El químico alemán Leopold Gmelin trabajó con este sistema, y en 1843 había identificado diez tríadas, tres grupos de cuatro, y un grupo de cinco. Jean-Baptiste Dumas publicó el trabajo en 1857 que describe las relaciones entre los diversos grupos de metales. Aunque los diversos químicos fueron capaces de identificar las relaciones entre pequeños grupos de elementos, aún tenían que construir un esquema que los abarcara a todos.

En 1857 el químico alemán August Kekulé observó que el carbono está a menudo unido a otros cuatro átomos. El metano, por ejemplo, tiene un átomo de carbono y cuatro átomos de hidrógeno. Este concepto eventualmente se conocería como «valencia».

En 1862 de Chancourtois, geólogo francés, publicó una primera forma de tabla periódica que llamó la «hélice telúrica» o «tornillo». Fue la primera persona en notar la periodicidad de los elementos. Al disponerlos en espiral sobre un cilindro por orden creciente de peso atómico, de Chancourtois mostró que los elementos con propiedades similares parecían ocurrir a intervalos regulares. Su tabla incluye además algunos iones y compuestos. También utiliza términos geológicos en lugar de químicos y no incluye un diagrama; como resultado, recibió poca atención hasta el trabajo de Dmitri Mendeléyev.

En 1864 Julius Lothar Meyer, un químico alemán, publicó una tabla con 44 elementos dispuestos por valencia. La misma mostró que los elementos con propiedades similares a menudo compartían la misma valencia. Al mismo tiempo, William Odling —un químico inglés— publicó un arreglo de 57 elementos ordenados en función de sus pesos atómicos. Con algunas irregularidades y vacíos, se dio cuenta de lo que parecía ser una periodicidad de pesos atómicos entre los elementos y que esto estaba de acuerdo con «las agrupaciones que generalmente recibían». Odling alude a la idea de una ley periódica, pero no siguió la misma. En 1870 propuso una clasificación basada en la valencia de los elementos.

El químico inglés John Newlands produjo una serie de documentos de 1863 a 1866 y señaló que cuando los elementos se enumeran en orden de aumentar el peso atómico, las propiedades físicas y químicas similares se repiten a intervalos de ocho.
Comparó esta periodicidad con las octavas de la música. Esta llamada «ley de las octavas» fue ridiculizada por los contemporáneos de Newlands y la Chemical Society se negó a publicar su obra, porque dejaba de cumplirse a partir del calcio. Newlands fue sin embargo capaz de elaborar una tabla de los elementos y la utilizó para predecir la existencia de elementos faltantes, como el germanio. La Chemical Society solamente reconoció la importancia de sus descubrimientos cinco años después de que se le acreditaran a Mendeléyev, y posteriormente fue reconocido por la Royal Society, que le concedió a Newlands su más alta condecoración, la medalla Davy.

En 1867 Gustavus Hinrichs, un químico danés, publicó un sistema periódico en espiral sobre la base de los espectros, los pesos atómicos y otras similitudes químicas. Su trabajo fue considerado como demasiado complicado y por eso no fue aceptado.

En 1869, el profesor de química ruso Dmitri Ivánovich Mendeléyev publicó su primera Tabla Periódica en Alemania. Un año después Julius Lothar Meyer publicó una versión ampliada de la tabla que había creado en 1864, basadas en la periodicidad de los volúmenes atómicos en función de la masa atómica de los elementos.

Por esta fecha ya eran conocidos 63 elementos de los 92 que existen de forma natural entre el Hidrógeno y el Uranio. Ambos químicos colocaron los elementos por orden creciente de sus masas atómicas, los agruparon en filas o periodos de distinta longitud y situaron en el mismo grupo elementos que tenían propiedades químicas similares, como la valencia. Construyeron sus tablas haciendo una lista de los elementos en filas o columnas en función de su peso atómico y comenzando una nueva fila o columna cuando las características de los elementos comenzaron a repetirse.

El reconocimiento y la aceptación otorgada a la tabla de Mendeléyev vino a partir de dos decisiones que tomó. La primera fue dejar huecos cuando parecía que el elemento correspondiente todavía no había sido descubierto. No fue el primero en hacerlo, pero sí en ser reconocido en el uso de las tendencias en su tabla periódica para predecir las propiedades de esos elementos faltantes. Incluso pronosticó las propiedades de algunos de ellos: el galio (Ga), al que llamó eka-aluminio por estar situado debajo del aluminio; el germanio (Ge), al que llamó eka-silicio; el escandio (Sc); y el tecnecio (Tc), que, aislado químicamente a partir de restos de un sincrotrón en 1937, se convirtió en el primer elemento producido de forma predominantemente artificial.

La segunda decisión fue ignorar el orden sugerido por los pesos atómicos y cambiar los elementos adyacentes, tales como telurio y yodo, para clasificarlos mejor en familias químicas. En 1913, Henry Moseley determinó los valores experimentales de la carga nuclear o número atómico de cada elemento, y demostró que el orden de Mendeléyev corresponde efectivamente al que se obtiene de aumentar el número atómico.

El significado de estos números en la organización de la tabla periódica no fue apreciado hasta que se entendió la existencia y las propiedades de los protones y los neutrones. Las tablas periódicas de Mendeléyev utilizan el peso atómico en lugar del número atómico para organizar los elementos, información determinable con precisión en ese tiempo. El peso atómico funcionó bastante bien para la mayoría de los casos permitiendo predecir las propiedades de los elementos que faltan con mayor precisión que cualquier otro método conocido entonces. Moseley predijo que los únicos elementos que faltaban entre aluminio (Z = 13) y oro (Z = 79) eran Z = 43, 61, 72 y 75, que fueron descubiertos más tarde. La secuencia de números atómicos todavía se utiliza hoy en día incluso aunque se han descubierto y sintetizado nuevos elementos.

En 1871, Mendeléyev publicó su tabla periódica en una nueva forma, con grupos de elementos similares dispuestos en columnas en lugar de filas, numeradas I a VIII en correlación con el estado de oxidación del elemento. También hizo predicciones detalladas de las propiedades de los elementos que ya había señalado que faltaban, pero deberían existir. Estas lagunas se llenaron posteriormente cuando los químicos descubrieron elementos naturales adicionales. 

En su nueva tabla consigna el criterio de ordenación de las columnas se basan en los hidruros y óxidos que puede formar esos elementos y por tanto, implícitamente, las valencias de esos elementos. Aún seguía dando resultados contradictorios (Plata y Oro aparecen duplicados, y no hay separación entre Berilio y Magnesio con Boro y Aluminio), pero significó un gran avance. Esta tabla fue completada con un grupo más, constituido por los gases nobles descubiertos en vida de Mendeléyev, pero que, por sus características, no tenían cabida en la tabla, por lo que hubo de esperar casi treinta años, hasta 1904, con el grupo o valencia cero, quedando la tabla más completa.

A menudo se afirma que el último elemento natural en ser descubierto fue el francio —designado por Mendeléyev como eka-cesio— en 1939. Sin embargo, el plutonio, producido sintéticamente en 1940, fue identificado en cantidades ínfimas como un elemento primordial de origen natural en 1971.

La disposición de la tabla periódica estándar es atribuible a Horace Groves Deming, un químico americano que en 1923 publicó una tabla periódica de 18 columnas. En 1928 Merck and Company preparó un folleto con esta tabla, que fue ampliamente difundida en las escuelas estadounidenses. Por la década de 1930 estaba apareciendo en manuales y enciclopedias de química. También se distribuyó durante muchos años por la empresa Sargent-Welch Scientific Company.

La tabla periódica de Mendeléyev presentaba ciertas irregularidades y problemas. En las décadas posteriores tuvo que integrar los descubrimientos de los gases nobles, las «tierras raras» y los elementos radioactivos. Otro problema adicional eran las irregularidades que existían para compaginar el criterio de ordenación por peso atómico creciente y la agrupación por familias con propiedades químicas comunes. Ejemplos de esta dificultad se encuentran en las parejas telurio-yodo, argón-potasio y cobalto-níquel, en las que se hace necesario alterar el criterio de pesos atómicos crecientes en favor de la agrupación en familias con propiedades químicas semejantes. 

Durante algún tiempo, esta cuestión no pudo resolverse satisfactoriamente hasta que Henry Moseley (1867-1919) realizó un estudio sobre los espectros de rayos X en 1913. Moseley comprobó que al representar la raíz cuadrada de la frecuencia de la radiación en función del número de orden en el sistema periódico se obtenía una recta, lo cual permitía pensar que este orden no era casual sino reflejo de alguna propiedad de la estructura atómica. Hoy sabemos que esa propiedad es el número atómico (Z) o número de cargas positivas del núcleo.

La explicación que se acepta actualmente de la ley periódica surgió tras los desarrollos teóricos producidos en el primer tercio del siglo XX, cuando se construyó la teoría de la mecánica cuántica. Gracias a estas investigaciones y a desarrollos posteriores, se acepta que la ordenación de los elementos en el sistema periódico está relacionada con la estructura electrónica de los átomos de los diversos elementos, a partir de la cual se pueden predecir sus diferentes propiedades químicas.

En 1945 Glenn Seaborg, un científico estadounidense, sugirió que los actínidos, como los lantánidos, estaban llenando un subnivel f en vez de una cuarta fila en el bloque d, como se pensaba hasta el momento. Los colegas de Seaborg le aconsejaron no publicar una teoría tan radical, ya que lo más probable era arruinar su carrera. Como consideraba que entonces no tenía una carrera que pudiera caer en descrédito, la publicó de todos modos. Posteriormente se encontró que estaba en lo cierto y en 1951 ganó el Premio Nobel de Química por su trabajo en la síntesis de los actínidos.

En 1952, el científico costarricense Gil Chaverri presentó una nueva versión basada en la estructura electrónica de los elementos, la cual permite ubicar las series de lantánidos y actínidos en una secuencia lógica de acuerdo con su número atómico.

Aunque se producen de forma natural pequeñas cantidades de algunos elementos transuránicos, todos ellos fueron descubiertos por primera vez en laboratorios, el primero de los cuales fue el neptunio, sintetizado en 1939. La producción de estos elementos ha expandido significativamente la tabla periódica. Debido a que muchos son altamente inestables y decaen rápidamente, son difíciles de detectar y caracterizar cuando se producen. Han existido controversias relativas a la aceptación de las pretensiones y derechos de descubrimiento de algunos elementos, lo que requiere una revisión independiente para determinar cuál de las partes tiene prioridad, y por lo tanto los derechos del nombre. Flerovio (elemento 114) y livermorio (elemento 116) fueron nombrados el 31 de mayo de 2012. En 2010, una colaboración conjunta entre Rusia y Estados Unidos en Dubná, región de Moscú, Rusia, afirmó haber sintetizado seis átomos de teneso (elemento 117).

El 30 de diciembre de 2015 la IUPAC reconoció oficialmente los elementos 113, 115, 117, y 118, completando la séptima fila de la tabla periódica. El 28 de noviembre de 2016 se anunciaron los nombres oficiales y los símbolos de los últimos cuatro nuevos elementos aprobados hasta la fecha por la IUPAC (Nh, nihonio; Mc, moscovio; Ts, teneso; y Og, oganesón), que sustituyen a las designaciones temporales.

La tabla periódica actual es un sistema donde se clasifican los elementos conocidos hasta la fecha. Se colocan de izquierda a derecha y de arriba a abajo en orden creciente de sus números atómicos. Los elementos están ordenados en siete hileras horizontales llamadas periodos, y en 18 columnas verticales llamadas grupos o familias.

Hacia abajo y a la izquierda aumenta el radio atómico y el radio iónico.

Hacia arriba y a la derecha aumenta la energía de ionización, la afinidad electrónica y la electronegatividad.

A las columnas verticales de la tabla se las conoce como grupos o familias. Hay 18 grupos en la tabla periódica estándar. En virtud de un convenio internacional de denominación, los grupos están numerados de 1 a 18 desde la columna más a la izquierda —los metales alcalinos— hasta la columna más a la derecha —los gases nobles—.

Anteriormente se utilizaban números romanos según la última cifra del convenio de denominación de hoy en día —por ejemplo, los elementos del grupo 4 estaban en el IVB y los del grupo 14 en el IVA—. En Estados Unidos, los números romanos fueron seguidos por una letra «A» si el grupo estaba en el bloque s o p, o una «B» si pertenecía al d. En Europa, se utilizaban letras en forma similar, excepto que «A» se usaba si era un grupo precedente al 10, y «B» para el 10 o posteriores. Además, solía tratarse a los grupos 8, 9 y 10 como un único grupo triple, conocido colectivamente en ambas notaciones como grupo VIII. En 1988 se puso en uso el nuevo sistema de nomenclatura IUPAC y se desecharon los nombres de grupo previos. 

Algunos de estos grupos tienen nombres triviales —no sistemáticos—, como se ve en la tabla de abajo, aunque no siempre se utilizan. Los grupos del 3 al 10 no tienen nombres comunes y se denominan simplemente mediante sus números de grupo o por el nombre de su primer miembro —por ejemplo, «el grupo de escandio» para el 3—, ya que presentan un menor número de similitudes y/o tendencias verticales.

La explicación moderna del ordenamiento en la tabla periódica es que los elementos de un grupo poseen configuraciones electrónicas similares y la misma valencia, entendida como el número de electrones en la última capa. Dado que las propiedades químicas dependen profundamente de las interacciones de los electrones que están ubicados en los niveles más externos, los elementos de un mismo grupo tienen propiedades químicas similares y muestran una tendencia clara en sus propiedades al aumentar el número atómico. 


Por ejemplo, los elementos en el grupo 1 tienen una configuración electrónica "ns" y una valencia de 1 —un electrón externo— y todos tienden a perder ese electrón al enlazarse como iones positivos de +1. Los elementos en el último grupo de la derecha son los gases nobles, los cuales tienen lleno su último nivel de energía —regla del octeto— y, por ello, son excepcionalmente no reactivos y son también llamados «gases inertes». 

Los elementos de un mismo grupo tienden a mostrar patrones en el radio atómico, energía de ionización y electronegatividad. De arriba abajo en un grupo, aumentan los radios atómicos de los elementos. Puesto que hay niveles de energía más llenos, los electrones de valencia se encuentran más alejados del núcleo. Desde la parte superior, cada elemento sucesivo tiene una energía de ionización más baja, ya que es más fácil quitar un electrón en los átomos que están menos fuertemente unidos. Del mismo modo, un grupo tiene una disminución de electronegatividad desde la parte superior a la inferior debido a una distancia cada vez mayor entre los electrones de valencia y el núcleo. 

Hay excepciones a estas tendencias, como por ejemplo lo que ocurre en el grupo 11, donde la electronegatividad aumenta más abajo en el grupo. Además, en algunas partes de la tabla periódica como los bloques d y f, las similitudes horizontales pueden ser tan o más pronunciadas que las verticales.

Las filas horizontales de la tabla periódica son llamadas períodos. El número de niveles energéticos de un átomo determina el periodo al que pertenece. Cada nivel está dividido en distintos subniveles, que conforme aumenta su número atómico se van llenando en este orden:

Siguiendo esa norma, cada elemento se coloca según su configuración electrónica y da forma a la tabla periódica. 

Los elementos en el mismo período muestran tendencias similares en radio atómico, energía de ionización, afinidad electrónica y electronegatividad. En un período el radio atómico normalmente decrece si nos desplazamos hacia la derecha debido a que cada elemento sucesivo añadió protones y electrones, lo que provoca que este último sea arrastrado más cerca del núcleo. Esta disminución del radio atómico también causa que la energía de ionización y la electronegatividad aumenten de izquierda a derecha en un período, debido a la atracción que ejerce el núcleo sobre los electrones. La afinidad electrónica también muestra una leve tendencia a lo largo de un período. Los metales —a la izquierda— generalmente tienen una afinidad menor que los no metales —a la derecha del período—, excepto para los gases nobles.

La tabla periódica consta de 7 períodos:

La tabla periódica se puede también dividir en bloques de acuerdo a la secuencia en la que se llenan las capas de electrones de los elementos. Cada bloque se denomina según el orbital en el que en teoría reside el último electrón: "s", "p", "d" y "f". El bloque s comprende los dos primeros grupos (metales alcalinos y alcalinotérreos), así como el hidrógeno y el helio. El bloque p comprende los últimos seis grupos —que son grupos del 13 al 18 en la IUPAC (3A a 8A en América)— y contiene, entre otros elementos, todos los metaloides. El bloque d comprende los grupos 3 a 12 —o 3B a 2B en la numeración americana de grupo— y contiene todos los metales de transición. El bloque f, a menudo colocado por debajo del resto de la tabla periódica, no tiene números de grupo y se compone de lantánidos y actínidos. Podría haber más elementos que llenarían otros orbitales, pero no se han sintetizado o descubierto; en este caso se continúa con el orden alfabético para nombrarlos. Así surge el bloque g, que es un bloque hipotético.

De acuerdo con las propiedades físicas y químicas que comparten, los elementos se pueden clasificar en tres grandes categorías: metales, metaloides y no metales. Los metales son sólidos generalmente brillantes, altamente conductores que forman aleaciones de unos con otros y compuestos iónicos similares a sales con compuestos no metálicos —siempre que no sean los gases nobles—. La mayoría de los no metales son gases incoloros o de colores; pueden formar enlaces covalentes con otros elementos no metálicos. Entre metales y no metales están los metaloides, que tienen propiedades intermedias o mixtas.

Metales y no metales pueden clasificarse en sub_categorías que muestran una gradación desde lo metálico a las propiedades no metálicas, de izquierda a derecha, en las filas: metales alcalinos —altamente reactivos—, metales alcalinotérreos —menos reactivos—, lantánidos y actínidos, metales de transición y metales post-transición. Los no metales se subdividen simplemente en no metales poliatómicos —que, por estar más cercanos a los metaloides, muestran cierto carácter metálico incipiente—, no metales diatómicos —que son esencialmente no metálicos— y los gases nobles, que son monoatómicos no metálicos y casi completamente inertes. Ocasionalmente también se señalan subgrupos dentro de los metales de transición, tales como metales refractarios y metales nobles.

La colocación de los elementos en categorías y subcategorías en función de las propiedades compartidas es imperfecta. Hay un espectro de propiedades dentro de cada categoría y no es difícil encontrar coincidencias en los límites, como es el caso con la mayoría de los sistemas de clasificación. El berilio, por ejemplo, se clasifica como un metal alcalinotérreo, aunque su composición química anfótera y su tendencia a formar compuestos covalentes son dos atributos de un metal de transición químicamente débil o posterior. El radón se clasifica como un no metal y un gas noble aunque tiene algunas características químicas catiónicas más características de un metal. También es posible clasificar con base en la división de los elementos en categorías de sucesos, mineralógicos o estructuras cristalinas. La categorización de los elementos de esta forma se remonta a por lo menos 1869, cuando Hinrichs escribió que se pueden extraer líneas sencillas de límites para mostrar los elementos que tienen propiedades similares, tales como metales y no metales, o los elementos gaseosos.

Hay tres variantes principales de la tabla periódica, cada una diferente en cuanto a la constitución del grupo 3. Escandio e itrio se muestran de manera uniforme ya que son los dos primeros miembros de este grupo; las diferencias dependen de la identidad de los miembros restantes.

El grupo 3 está formado por Sc, Y, y La, Ac. Lantano (La) y actinio (Ac) ocupan los dos puestos por debajo del itrio (Y). Esta variante es la más común. Hace hincapié en las similitudes de las tendencias periódicas bajando los grupos 1, 2 y 3, a expensas de las discontinuidades en las tendencias periódicas entre los grupos 3 y 4 y la fragmentación de los lantánidos y actínidos.

El grupo 3 está formado por Sc, Y, y Lu, Lr. Lutecio (Lu) y lawrencio (Lr) ocupan los dos puestos por debajo del itrio. Esta variante conserva un bloque f de 14 columnas de ancho, a la vez que desfragmenta a lantánidos y actínidos. Enfatiza las similitudes de tendencias periódicas entre el grupo 3 y los siguientes grupos a expensas de discontinuidades en las tendencias periódicas entre los grupos 2 y 3. 

El grupo 3 está formado por Sc, Y, y 15 lantánidos y 15 actínidos. Las dos posiciones por debajo de itrio contienen los lantánidos y los actínidos (posiblemente por notas al pie). Esta variante enfatiza las similitudes en la química de los 15 elementos lantánidos (La-Lu), a expensas de la ambigüedad en cuanto a los elementos que ocupan las dos posiciones por debajo de itrio del grupo 3, y aparentemente de un bloque f amplio de 15 columnas —solo puede haber 14 elementos en cualquier fila del bloque f—.

Las tres variantes se originan de las dificultades históricas en la colocación de los lantánidos de la tabla periódica, y los argumentos en cuanto a dónde empiezan y terminan los elementos del bloque f. Se ha afirmado que tales argumentos son la prueba de que «es un error de romper el sistema [periódico] en bloques fuertemente delimitados». Del mismo modo, algunas versiones de la tabla dos marcadores han sido criticados por lo que implica que los 15 lantánidos ocupan la caja única o lugar por debajo de itrio, en violación del principio básico de «un lugar, un elemento».

La tabla periódica moderna a veces se expande a su forma larga o de 32 columnas restableciendo los elementos del bloque f a su posición natural entre los bloques s y d. A diferencia de la forma de 18 columnas, esta disposición da como resultado «el aumento sin interrupciones a la secuencia de los números atómicos». También se hace más fácil ver la relación del bloque f con los otros bloques de la tabla periódica. Jensen aboga por una forma de tabla con 32 columnas con base en que los lantánidos y actínidos son relegados en la mente de los estudiantes como elementos opacos y poco importantes que pueden ser puestos en cuarentena e ignorados. A pesar de estas ventajas, los editores generalmente evitan la formulación de 32 columnas porque su relación rectangular no se adapta adecuadamente a la proporción de una página de libro.

Los científicos discuten la eficiencia de cada modelo de tabla periódica. Muchos cuestionan incluso que la distribución bidimensional sea la mejor. Argumentan que se basa en una convención y en conveniencia, principalmente por la necesidad de ajustarlas a la página de un libro y otras presentaciones en el plano. El propio Mendeléyev no estaba conforme y consideró la distribución en espiral, sin suerte. Algunos argumentos en favor de nuevos modelos consisten en, por ejemplo, la ubicación del grupo de los lantánidos y de los actínidos fuera del cuerpo de la tabla, e incluso que el helio debería estar ubicado en el grupo 2 de los alcaniotérreos pues comparte con ellos dos electrones en su capa externa. Por ello con los años se han desarrollado otras tablas periódicas ordenadas en forma distinta, como por ejemplo en triángulo, pirámide, tablas en escalones, torre y en espiral. A este último tipo corresponde la galaxia química, la espiral de Theodor Benfey y la forma en espiral-fractal de Melinda E Green. Se estima que se han publicado más de 700 versiones de la tabla periódica.

Según Phillip Stewart, si Mendeléyev hubiera seguido desarrollando el modelo en espiral, hubiera podido predecir las propiedades de los halógenos. Utilizando esta idea, el propio Stewart creó una tabla periódica en espiral a la que dio en llamar «Galaxia química», en la que acomoda la longitud creciente de los períodos en los brazos de una galaxia en espiral. 

En palabras de Theodor Benfey, la tabla y la ley periódica 
Su preocupación, pues, era estrictamente pedagógica. Por ese motivo diseñó una tabla periódica oval similar a un campo de fútbol que no mostraba saltos ni elementos flotantes. Ordena los elementos en una espiral continua, con el hidrógeno en el centro y los metales de transición, los lantánidos y los actínidos ocupando las penínsulas. No obstante, no se sintió satisfecho con el resultado, ya que no tenía espacio suficiente para los lantánidos. Por ello en un rediseño posterior creó una protusión para hacerles sitio y lo publicó en 1964 en la revista de la que era redactor jefe, "Chemistry (química)", de la American Chemical Society. La tabla fue modificada para dejar abierta la posibilidad de acomodar nuevos elementos transuránicos que todavía no se habían detectado, cuya existencia había sido sugerida por Glenn Seaborg, así como otros cambios menores. La espiral de Benfey fue publicada en calendarios, libros de texto y utilizada por la industria química, por lo cual se volvió popular.
La tabla fractal se basa en la continuidad de las características del elemento al final de una fila con el que se encuentra al inicio de la siguiente, lo que sugiere que la distribución podría representarse mejor con un cilindro en lugar de fraccionar la tabla en columnas. Además, en algunos casos había muchas diferencias entre algunos elementos con números atómicos bajos. Por otra parte, la tabla incorpora la familia de los actínidos y los lantánidos al diseño general, ubicándolos en el lugar que les correspondería por número atómico, en lugar de mantenerlos separados en dos grupos flotantes al final como sucede en la tabla estándar. El resultado es que las familias, en lugar de seguir columnas, siguen arcos radiales. Esta tabla evidencia la periodicidad introduciendo horquillas en el inicio de los períodos de longitud 8, 18 y 32.

La mayoría de las tablas periódicas son de dos dimensiones; sin embargo, se conocen tablas en tres dimensiones al menos desde 1862 (pre-data tabla bidimensional de Mendeléyev de 1869). Como ejemplos más recientes se puede citar la Clasificación Periódica de Courtines (1925), el Sistema de Lámina de Wrigley (1949), la hélice periódica de Giguère (1965) y el árbol periódico de Dufour (1996). Se ha descrito que la Tabla Periódica de Stowe (1989) tiene cuatro dimensiones —tres espaciales y una de color—. 

Las diversas formas de tablas periódicas pueden ser consideradas como un continuo en la química-física. Hacia el final del continuo químico se puede encontrar, por ejemplo, la Tabla Periódica Inorgánica de Rayner-Canham (2002), que hace hincapié en las tendencias, patrones, relaciones y propiedades químicas inusuales. Cerca del final del continuo físico está la tabla periódica ampliada escalonada por la izquierda de Janet (1928). Tiene una estructura que muestra una relación más estrecha con el orden de llenado de electrones por capa y, por asociación, la mecánica cuántica. En algún lugar en medio del continuo se ubica la ubica tabla periódica estándar; se considera que expresa las mejores tendencias empíricas en el estado físico, la conductividad eléctrica y térmica, los números de oxidación, y otras propiedades fácilmente inferidas de las técnicas tradicionales del laboratorio químico.

Los elementos 108 (hasio), 112 (copernicio) y 114 (flerovio) no tienen propiedades químicas conocidas. Otros elementos superpesados pueden comportarse de forma diferente a lo que se predice por extrapolación, debido a los efectos relativistas; por ejemplo, se predijo que el flerovio exhibiría posiblemente algunas propiedades similares a las de los gases nobles, aunque actualmente (2016) se coloca en el grupo del carbono. Sin embargo, experimentos posteriores sugieren que se comporta químicamente como plomo, como se espera a partir de su posición de la tabla periódica.

No está claro si los nuevos elementos encontrados continuarán el patrón de la tabla periódica estándar como parte del período 8 o se necesitará nuevos ajustes o adaptaciones. Seaborg espera que este periodo siga el patrón previamente establecido exactamente, de modo que incluiría un bloque s para los elementos 119 y 120, un nuevo bloque g para los próximos 18 elementos, y 30 elementos adicionales continuarían los bloques actuales f, d, y p. Los físicos tales como Pekka Pyykkö han teorizado que estos elementos adicionales no seguirían la regla de Madelung, que predice cómo se llenan de capas de electrones, y por lo tanto afectarán la apariencia de la tabla periódica estándar.

El número de posibles elementos no se conoce. En 1911 Elliot Adams, con base en la disposición de los elementos en cada fila de la tabla periódica horizontal, predijo que no existirían los elementos de peso atómico superior a 256 —lo que estaría entre los elementos 99 y 100 en términos de hoy en día—. La estimación reciente más alta es que la tabla periódica puede terminar poco después de la isla de estabilidad, que según se considere un modelo relativista o no se centrará alrededor de Z = 120 y N = 172 o Z = 124-126 y N = 184, ya que la extensión de la tabla periódica está restringida por las líneas de goteo de protones y de neutrones. Otras predicciones del fin de la tabla periódica incluyen al elemento 128 de John Emsley, al elemento 137 de Richard Feynman, y al elemento 155 de Albert Khazan.
El modelo de Bohr, no relativista, exhibe dificultad para los átomos con número atómico superior a 137, ya que estos requerirían que los electrones 1s viajen más rápido que c, la velocidad de la luz, lo que lo vuelve inexacto y no se puede aplicar a estos elementos.

La ecuación relativista de Dirac tiene problemas para elementos con más de 137 protones. Para ellos, la función de onda del estado fundamental de Dirac es oscilatoria, y no hay diferencia entre los espectros de energía positivo y negativo, como en la paradoja de Klein. Si se realizan cálculos más precisos, teniendo en cuenta los efectos del tamaño finito del núcleo, se encuentra que la energía de enlace excede el límite para los elementos con más de 173 protones. Para los elementos más pesados, si el orbital más interno (1s) no está lleno, el campo eléctrico del núcleo tira de un electrón del vacío, lo que resulta en la emisión espontánea de un positrón; sin embargo, esto no sucede si el orbital más interno está lleno, de modo que el elemento 173 no es necesariamente el final de la tabla periódica.

Solamente siguiendo las configuraciones electrónicas, el hidrógeno (configuración electrónica 1s) y el helio (1s) se colocan en los grupos 1 y 2, por encima de litio ([He]2s) y berilio ([He]2s). Sin embargo, esta colocación se utiliza rara vez fuera del contexto de las configuraciones electrónicas: cuando los gases nobles —entonces llamados «gases inertes»— fueron descubiertos por primera vez alrededor de 1900, se los identificaba como «el grupo 0», lo que reflejaba que no se les conocía ninguna reactividad química en ese momento, y el helio se colocó en la parte superior de ese grupo, porque compartía esta situación extrema. Aunque el grupo cambió su número formal, muchos autores siguieron colocando al helio directamente por encima del neón, en el grupo 18; uno de los ejemplos de tal colocación es la tabla IUPAC actual.
Las propiedades químicas del hidrógeno no son muy cercanas a los de los metales alcalinos, que ocupan el grupo 1, y por eso el hidrógeno a veces se coloca en otra parte: una de las alternativas más comunes es en el grupo 17. Una de las razones para ello es la estrictamente univalente química predominantemente no metálica del hidrógeno, la del flúor —el elemento colocado en la parte superior del grupo 17— es estrictamente univalente y no metálica. A veces, para mostrar cómo el hidrógeno tiene tanto propiedades correspondientes a las de los metales alcalinos y a los halógenos, puede aparecer en dos columnas al mismo tiempo. También puede aparecer por encima del carbono en el grupo 14: así ubicado, se adapta bien al aumento de las tendencias de los valores de potencial de ionización y los valores de afinidad de electrones, y no se aleja demasiado de la tendencia de electronegatividad. Por último, el hidrógeno a veces se coloca por separado de cualquier grupo porque sus propiedades en general difieren de las de cualquier grupo: a diferencia del hidrógeno, los otros elementos del grupo 1 muestran un comportamiento extremadamente metálico; los elementos del grupo 17 comúnmente forman sales —de ahí el término "halógeno"—; los elementos de cualquier otro grupo muestran una química multivalente. El otro elemento del periodo 1, el helio, a veces se coloca separado de cualquier grupo también. La propiedad que distingue al helio del resto de los gases nobles —a pesar de que su extraordinario carácter inerte está muy cerca del neón y el argón— es que, en su capa cerrada de electrones, el helio tiene solo dos electrones en el orbital más externo, mientras que el resto de los gases nobles tienen ocho.

Según IUPAC un metal de transición es «un elemento cuyo átomo tiene una subcapa d incompleta o que puede dar lugar a cationes». De acuerdo con esta definición, todos los elementos en los grupos del 3 al 11 son metales de transición y se excluye al grupo 12, que comprende zinc, cadmio y mercurio.

Algunos químicos consideran que los «elementos del bloque d» y los «metales de transición» son categorías intercambiables, incluyendo por tanto al grupo 12 como un caso especial de metal de transición en el que los electrones d no participan normalmente en el enlace químico. El descubrimiento de que el mercurio puede utilizar sus electrones d en la formación de fluoruro de mercurio (IV) (HgF) llevó a algunos científicos a sugerir que el mercurio puede ser considerado un metal de transición. Otros, como Jensen, argumentan que la formación de un compuesto como HgF4 puede ocurrir solo bajo condiciones muy anormales. Como tal, el mercurio no puede ser considerado como un metal de transición por ninguna interpretación razonable en el sentido normal del término.

En otros casos hay quienes no incluyen al grupo 3, argumentando que estos no forman iones con una capa d parcialmente ocupada y por lo tanto no presentan las propiedades características de la química de los metales de transición.

Aunque el escandio y el itrio son siempre los dos primeros elementos del grupo 3, la identidad de los próximos dos elementos no se resuelve. O bien son lantano y actinio, o lutecio y lawrencio. Existen argumentos físicos y químicos para apoyar esta última disposición, pero no todos los autores están convencidos.

Tradicionalmente se representa al lantano y al actinio como los restantes miembros del grupo 3. Se ha sugerido que este diseño se originó en la década de 1940, con la aparición de las tablas periódicas que dependen de las configuraciones electrónicas de los elementos y la noción de la diferenciación de electrones. 

Las configuraciones de cesio, bario y lantano son [Xe]6s, [Xe]6s y [Xe]5d6s. Por lo tanto el lantano tiene un electrón diferenciador 5d y esto lo establece «en el grupo 3 como el primer miembro del bloque d para el periodo 6». 

En el grupo 3 se ve un conjunto consistente de configuraciones electrónicas: escandio [Ar]3d4s, itrio [Kr]4d5s y lantano. Aún en el período 6, se le asignó al iterbio una configuración electrónica de [Xe]4f5d6s y [Xe]4f5d6s para el lutecio, lo que resulta «en un electrón diferenciante 4f para el lutecio y lo establece firmemente como el último miembro del bloque f para el período 6.» Matthias describe la colocación del lantano en virtud del itrio como «un error en el sistema periódico — por desgracia propagado mayoritariamente por la compañía Welch [Sargent-Welch] ... y ... todo el mundo la copió». Lavelle lo refutó aportando una serie de libros de referencia conocidos en los que se presentaban tablas periódicas con tal disposición.

Las primeras técnicas para separar químicamente escandio, itrio y lutecio se basaron en que estos elementos se produjeron juntos en el llamado «grupo de itrio», mientras que La y Ac se produjeron juntos en el «grupo del cerio». Por consiguiente, en los años 1920 y 30 algunos químicos colocaron el lutecio en el grupo 3 en lugar del lantano. 

Posteriores trabajos espectroscópicos encontraron que la configuración electrónica de iterbio era de hecho [Xe]4f6s. Esto significaba que iterbio y lutecio tenían 14 electrones f, «resultando en un electrón diferenciante d en lugar de f» para el último, lo que lo hacía un «candidato igualmente válido» para la siguiente posición de la tabla periódica en el grupo 3 debajo del itrio. Varios físicos en los años 1950 y 60 optaron por lutecio, a la luz de una comparación de varias de sus propiedades físicas con las del lantano. Esta disposición, en la que el lantano es el primer miembro del bloque f, es cuestionada por algunos autores ya que este elemento carece de electrones f. Sin embargo, se ha argumentado que esta no es una preocupación válida dado que existen otras anomalías en la tabla periódica, como por ejemplo el torio, que no tiene electrones f pero forma parte de ese bloque. En cuanto al lawrencio, su configuración electrónica se confirmó en 2015 como [Rn]5f7s7p, lo que representa otra anomalía de la tabla periódica, independientemente de si se coloca en el bloque d o f, pues la potencialmente aplicable posición de bloque p se ha reservado para el nihonio al que se le prevé una configuración electrónica de [Rn]5f6d7s7p.

Las muchas formas diferentes de la tabla periódica han llevado a preguntarse si existe una forma óptima o definitiva. Se cree que la respuesta a esta pregunta depende de si la periodicidad química tiene una verdad subyacente, o es en cambio el producto de la interpretación humana subjetiva, dependiente de la circunstancias, las creencias y las predilecciones de los observadores humanos. Se podría establecer una base objetiva para la periodicidad química determinando la ubicación del hidrógeno y el helio, y la composición del grupo 3. En ausencia de una verdad objetiva, las diferentes formas de la tabla periódica pueden ser consideradas variaciones de la periodicidad química, cada una de las cuales explora y hace hincapié en diferentes aspectos, propiedades, perspectivas y relaciones de y entre los elementos. Se cree que la ubicuidad de la tabla periódica estándar es una consecuencia de su diseño, que tiene un buen equilibrio de características en términos de facilidad de construcción y tamaño, y su descripción de orden atómico y tendencias periódicas.

Estado de los elementos en condiciones normales de presión y temperatura (0 °C y 1 atm).




</doc>
<doc id="2798" url="https://es.wikipedia.org/wiki?curid=2798" title="Teoría de la información">
Teoría de la información

La teoría de la información, también conocida como teoría matemática de la comunicación (Inglés: "mathematical theory of communication") o teoría matemática de la información, es una propuesta teórica presentada por Claude E. Shannon y Warren Weaver a finales de la década de los años 1940. Esta teoría está relacionada con las leyes matemáticas que rigen la transmisión y el procesamiento de la información y se ocupa de la medición de la información y de la representación de la misma, así como también de la capacidad de los sistemas de comunicación para transmitir y procesar información. La teoría de la información es una rama de la teoría de la probabilidad y de las ciencias de la computación que estudia la información y todo lo relacionado con ella: canales, compresión de datos y criptografía, entre otros.

La teoría de la información surgió a finales de la Segunda Guerra Mundial, en los años cuarenta. Fue indicada por Claude E. Shannon a través de un artículo publicado en el "Bell System Technical Journal" en 1948, titulado "Una teoría matemática de la comunicación" (texto completo en inglés). En esta época se buscaba utilizar de manera más eficiente los canales de comunicación, enviando una cantidad de información por un determinado canal y midiendo su capacidad; se buscaba la transmisión óptima de los mensajes. 
Esta teoría es el resultado de trabajos comenzados en la década 1910 por Andrei A. Markovi, a quien le siguió Ralp V. L. Hartley en 1927, quien fue el precursor del lenguaje binario. A su vez, Alan Turing en 1936, realizó el esquema de una máquina capaz de tratar información con emisión de símbolos, y finalmente Claude Elwood Shannon, matemático, ingeniero electrónico y criptógrafo estadounidense, conocido como "el padre de la teoría de la información”, junto a Warren Weaver, contribuyó en la culminación y el asentamiento de la Teoría Matemática de la Comunicación de 1949 –que hoy es mundialmente conocida por todos como la Teoría de la Información-. Weaver consiguió darle un alcance superior al planteamiento inicial, creando un modelo simple y lineal: "Fuente/codificador/mensaje canal/decodificador/destino".
La necesidad de una base teórica para la tecnología de la comunicación surgió del aumento de la complejidad y de la masificación de las vías de comunicación, tales como el teléfono, las redes de teletipo y los sistemas de comunicación por radio. La teoría de la información también abarca todas las restantes formas de transmisión y almacenamiento de información, incluyendo la televisión y los impulsos eléctricos que se transmiten en las computadoras y en la grabación óptica de datos e imágenes.
La idea es garantizar que el transporte masivo de datos no sea en modo alguno una merma de la calidad, incluso si los datos se comprimen de alguna manera. Idealmente, los datos se pueden restaurar a su forma original al llegar a su destino. En algunos casos, sin embargo, el objetivo es permitir que los datos de alguna forma se conviertan para la transmisión en masa, se reciban en el punto de destino y sean convertidos fácilmente a su formato original, sin perder ninguna de la información transmitida.

El modelo propuesto por Shannon es un sistema general de la comunicación que parte de una fuente de información desde la cual, a través de un transmisor, se emite una señal, la cual viaja por un canal, pero a lo largo de su viaje puede ser interferida por algún mensaje. La señal sale del canal, llega a un receptor que decodifica la información convirtiéndola posteriormente en mensaje que pasa a un destinatario. Con el modelo de la teoría de la información se trata de llegar a determinar la forma más económica, rápida y segura de codificar un mensaje, sin que la presencia de algún ruido complique su transmisión. Para esto, el destinatario debe comprender la señal correctamente; el problema es que aunque exista un mismo código de por medio, esto no significa que el destinatario va a captar el significado que el emisor le quiso dar al mensaje. La codificación puede referirse tanto a la transformación de voz o imagen en señales eléctricas o electromagnéticas, como al cifrado de mensajes para asegurar su privacidad. Un concepto fundamental en la teoría de la información es que la cantidad de información contenida en un mensaje es un valor matemático bien definido y medible. El término cantidad no se refiere a la cuantía de datos, sino a la probabilidad de que un mensaje, dentro de un conjunto de mensajes posibles, sea recibido. En lo que se refiere a la cantidad de información, el valor más alto se le asigna al mensaje que menos probabilidades tiene de ser recibido. Si se sabe con certeza que un mensaje va a ser recibido, su cantidad de información es cero.

Otro aspecto importante dentro de esta teoría es la resistencia a la distorsión que provoca el ruido, la facilidad de codificación y descodificación, así como la velocidad de transmisión. Es por esto que se dice que el mensaje tiene muchos sentidos, y el destinatario extrae el sentido que debe atribuirle al mensaje, siempre y cuando haya un mismo código en común. La teoría de la información tiene ciertas limitaciones, como lo es la acepción del concepto del código. El significado que se quiere transmitir no cuenta tanto como el número de alternativas necesario para definir el hecho sin ambigüedad. Si la selección del mensaje se plantea únicamente entre dos alternativas diferentes, la teoría de Shannon postula arbitrariamente que el valor de la información es uno. Esta unidad de información recibe el nombre de bit. Para que el valor de la información sea un bit, todas las alternativas deben ser igual de probables y estar disponibles. Es importante saber si la fuente de información tiene el mismo grado de libertad para elegir cualquier posibilidad o si se halla bajo alguna influencia que la induce a una cierta elección. La cantidad de información crece cuando todas las alternativas son igual de probables o cuanto mayor sea el número de alternativas. Pero en la práctica comunicativa real no todas las alternativas son igualmente probables, lo cual constituye un tipo de proceso estocástico denominado Márkov. El subtipo de Márkov dice que la cadena de símbolos está configurada de manera que cualquier secuencia de esa cadena es representativa de toda la cadena completa.

La Teoría de la Información se encuentra aún hoy en día en relación con una de las tecnologías en boga, Internet. Desde el punto de vista social, Internet representa unos significativos beneficios potenciales, ya que ofrece oportunidades sin precedentes para dar poder a los individuos y conectarlos con fuentes cada vez más ricas de información digital. Internet fue creado a partir de un proyecto del departamento de defensa de los Estados Unidos llamado ARPANET "(Advanced Research Projects Agency Network)" iniciado en 1969 y cuyo propósito principal era la investigación y desarrollo de protocolos de comunicación para redes de área amplia para ligar redes de transmisión de paquetes de diferentes tipos capaces de resistir las condiciones de operación más difíciles, y continuar funcionando aún con la pérdida de una parte de la red (por ejemplo en caso de guerra). 
Estas investigaciones dieron como resultado el protocolo TCP/IP "(Transmission Control Protocol/Internet Protocol)", un sistema de comunicaciones muy sólido y robusto bajo el cual se integran todas las redes que conforman lo que se conoce actualmente como Internet. 
El enorme crecimiento de Internet se debe en parte a que es una red basada en fondos gubernamentales de cada país que forma parte de Internet, lo que proporciona un servicio prácticamente gratuito. A principios de 1994 comenzó a darse un crecimiento explosivo de las compañías con propósitos comerciales en Internet, dando así origen a una nueva etapa en el desarrollo de la red. 
Descrito a grandes rasgos, TCP/IP mete en paquetes la información que se quiere enviar y la saca de los paquetes para utilizarla cuando se recibe. Estos paquetes pueden compararse con sobres de correo; TCP/IP guarda la información, cierra el sobre y en la parte exterior pone la dirección a la cual va dirigida y la dirección de quien la envía. Mediante este sistema, los paquetes viajan a través de la red hasta que llegan al destino deseado; una vez ahí, la computadora de destino quita el sobre y procesa la información; en caso de ser necesario envía una respuesta a la computadora de origen usando el mismo procedimiento.
Cada máquina que está conectada a Internet tiene una dirección única; esto hace que la información que se envía no equivoque el destino. Existen dos formas de dar direcciones, con letras o con números. Realmente, las computadoras utilizan las direcciones numéricas para mandar paquetes de información, pero las direcciones con letras fueron implementadas para facilitar su manejo a los seres humanos. Una dirección numérica está compuesta por cuatro partes. Cada una de estas partes está dividida por puntos. 

Una de las aplicaciones de la teoría de la información son los archivos ZIP, documentos que se comprimen para su transmisión a través de correo electrónico o como parte de los procedimientos de almacenamiento de datos. La compresión de los datos hace posible completar la transmisión en menos tiempo. En el extremo receptor, un software se utiliza para la liberación o descompresión del archivo, restaurando los documentos contenidos en el archivo ZIP a su formato original. La teoría de la información también entra en uso con otros tipos de archivo; por ejemplo, los archivos de audio y vídeo que se reproducen en un reproductor de MP3 / MP4 se comprimen para una fácil descarga y almacenamiento en el dispositivo. Cuando se accede a los archivos se descomprimen para que estén inmediatamente disponibles para su uso.

Una fuente es todo aquello que emite mensajes. Por ejemplo, una fuente puede ser una computadora y mensajes sus archivos; una fuente puede ser un dispositivo de transmisión de datos y mensajes los datos enviados, etc. Una fuente es en sí misma un conjunto finito de mensajes: todos los posibles mensajes que puede emitir dicha fuente. En compresión de datos se tomará como fuente el archivo a comprimir y como mensajes los caracteres que conforman dicho archivo.

Por la naturaleza generativa de sus mensajes, una fuente puede ser aleatoria o determinista. Por la relación entre los mensajes emitidos, una fuente puede ser estructurada o no estructurada (o caótica).

Existen varios tipos de fuente. Para la teoría de la información interesan las fuentes aleatorias y estructuradas. Una fuente es aleatoria cuando no es posible predecir cuál es el próximo mensaje a emitir por la misma. Una fuente es estructurada cuando posee un cierto nivel de redundancia; una fuente no estructurada o de información pura es aquella en que todos los mensajes son absolutamente aleatorios sin relación alguna ni sentido aparente. Este tipo de fuente emite mensajes que no se pueden comprimir; un mensaje, para poder ser comprimido, debe poseer un cierto grado de redundancia; la información pura no puede ser comprimida sin que haya una pérdida de conocimiento sobre el mensaje.

Un mensaje es un conjunto de ceros y unos. Un archivo, un paquete de datos que viaja por una red y cualquier cosa que tenga una representación binaria puede considerarse un mensaje. El concepto de mensaje se aplica también a alfabetos de más de dos símbolos, pero debido a que tratamos con información digital nos referiremos casi siempre a mensajes binarios.

Un código es un conjunto de unos y ceros que se usan para representar un cierto mensaje de acuerdo a reglas o convenciones preestablecidas. Por ejemplo, al mensaje 0010 lo podemos representar con el código 1101 usado para codificar la función (NOT). La forma en la cual codificamos es arbitraria. Un mensaje puede, en algunos casos, representarse con un código de menor longitud que el mensaje original. Supongamos que a cualquier mensaje "S" lo codificamos usando un cierto algoritmo de forma tal que cada "S" es codificado en "L(S)" bits; definimos entonces la información contenida en el mensaje "S" como la cantidad mínima de bits necesarios para codificar un mensaje.

La información contenida en un mensaje es proporcional a la cantidad de bits que se requieren como mínimo para representar al mensaje. El concepto de información puede entenderse más fácilmente si consideramos un ejemplo. Supongamos que estamos leyendo un mensaje y hemos leído "cadena de c"; la probabilidad de que el mensaje continúe con "caracteres" es muy alta. Así, cuando efectivamente recibimos a continuación "caracteres" la cantidad de información que nos llegó es muy baja pues estábamos en condiciones de predecir qué era lo que iba a ocurrir. La ocurrencia de mensajes de alta probabilidad de aparición aporta menos información que la ocurrencia de mensajes menos probables. Si luego de "cadena de c" leemos "himichurri" la cantidad de información que estamos recibiendo es mucho mayor.

La información es tratada como magnitud física, caracterizando la información de una secuencia de símbolos utilizando la entropía. Es parte de la idea de que los canales no son ideales, aunque muchas veces se idealicen las no linealidades, para estudiar diversos métodos de envío de información o la cantidad de información útil que se pueda enviar a través de un canal.

La información necesaria para especificar un sistema físico tiene que ver con su entropía. En concreto, en ciertas áreas de la física, extraer información del estado actual de un sistema requiere reducir su entropía, de tal manera que la entropía del sistema (formula_1) y la cantidad de información (formula_2) extraíble están relacionadas por:

De acuerdo con la teoría de la información, el nivel de información de una fuente se puede medir según la entropía de la misma. Los estudios sobre la entropía son de suma importancia en la teoría de la información y se deben principalmente a C. E. Shannon. Existe, a su vez, un gran número de propiedades respecto de la entropía de variables aleatorias debidas a A. Kolmogorov. Dada una fuente "F" que emite mensajes, resulta frecuente observar que los mensajes emitidos no resultan equiprobables sino que tienen una cierta probabilidad de ocurrencia dependiendo del mensaje. Para codificar los mensajes de una fuente intentaremos pues utilizar menor cantidad de bits para los mensajes más probables y mayor cantidad de bits para los mensajes menos probables, de forma tal que el promedio de bits utilizados para codificar los mensajes sea menor que la cantidad de bits promedio de los mensajes originales. Esta es la base de la compresión de datos. A este tipo de fuente se la denomina fuente de orden-0, pues la probabilidad de ocurrencia de un mensaje no depende de los mensajes anteriores. A las fuentes de orden superior se las puede representar mediante una fuente de orden-0 utilizando técnicas de modelización apropiadas.
Definimos la probabilidad de ocurrencia de un mensaje en una fuente como la cantidad de apariciones de dicho mensaje dividido entre el total de mensajes. Supongamos que "P" es la probabilidad de ocurrencia del mensaje-i de una fuente, y supongamos que "L "es la longitud del código utilizado para representar a dicho mensaje. La longitud promedio de todos los mensajes codificados de la fuente se puede obtener como:


A partir de aquí y tras intrincados procedimientos matemáticos que fueron demostrados por Shannon oportunamente se llega a que "H" es mínimo cuando "f(P) = log (1/P)". Entonces:

La longitud mínima con la cual puede codificarse un mensaje puede calcularse como "L=log(1/P) = -log(P)". Esto da una idea de la longitud a emplear en los códigos a usar para los caracteres de un archivo en función de su probabilidad de ocurrencia. Reemplazando "L "podemos escribir "H" como:

De aquí se deduce que la entropía de la fuente depende únicamente de la probabilidad de ocurrencia de cada mensaje de la misma; por ello la importancia de los compresores estadísticos (aquellos que se basan en la probabilidad de ocurrencia de cada carácter). Shannon demostró, oportunamente, que no es posible comprimir una fuente estadísticamente más allá del nivel indicado por su entropía.







</doc>
<doc id="2800" url="https://es.wikipedia.org/wiki?curid=2800" title="Triscenia ovina">
Triscenia ovina

Triscenia es un género monotípico de plantas herbáceas de la familia de las gramíneas o poáceas. Su única especie, Triscenia ovina Griseb., es originaria de Cuba.
Es una planta perenne; cespitosa. Con culmos de 20-50 cm de alto; herbácea; no ramificado arriba. Entrenudos de los culmos huecos. Hojas en su mayoría basales. La lámina es estrecha; setacea (o filiforme); acicular (reducido a la nervadura central); sin venación. Son plantas bisexuales, con espiguillas bisexuales; con los floretes hermafroditas. Las espiguillas todas por igual en la sexualidad. La inflorescencia de las ramas principales espigadas.

"Triscenia ovina" fue descrita por August Heinrich Rudolf Grisebach y publicado en "Plantae Wrightianae" 2: 534. 1863.




</doc>
<doc id="2802" url="https://es.wikipedia.org/wiki?curid=2802" title="Triodia">
Triodia

Triodia es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es endémico de Australia y tiene 112 especies descritas y de estas, solo 68 aceptadas.
Es una planta herbácea perenne que crece en regiones áridas y tiene las hojas punzantes y teñidas. Son conocidas popularmente como spinifex, aunque no pertenece al género costero "Spinifex". 

"Triodia" ha sido muy utilizado por los aborígenes australianos. Sus semillas se han recolectado para hacer pasteles, con la resina de la planta hacen pegamento para sus lanzas y con sus ramas hacían señales de humo para comunicarse a distancia con sus familias, ya que producen un intenso humo negro. 
El género fue descrito por Robert Brown y publicado en "Prodromus Florae Novae Hollandiae" 182. 1810. 
Triodia: nombre genérico que deriva de las palabras griegas: "treis" = (tres) y "odous" = (dientes), en referencia a 3 dientes o lemas tri-lobulado.


Numerosas especies fueron incluidas en "Triodia" pero posteriormente han sido clasificadas en otros géneros: "Austrofestuca Chascolytrum Danthonia Dasyochloa Deschampsia Diplachne Disakisperma Erioneuron Gouinia Graphephorum Leptocarydion Notochloe Plinthanthesis Poa Puccinellia Rytidosperma Scolochloa Spartina Torreyochloa Trichoneura Tridens Triplasis Tripogon Vaseyochloa"



</doc>
<doc id="2805" url="https://es.wikipedia.org/wiki?curid=2805" title="Thrasya">
Thrasya

Thrasya es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de Sudamérica tropical. Comprende 33 especies descritas y de estas, solo 22 aceptadas.

El género fue descrito por Carl Sigismund Kunth y publicado en "Nova Genera et Species Plantarum (quarto ed.)" 1: 120–121. 1815[1816]. La especie tipo es: "Thrasya paspaloides" 
A continuación se brinda un listado de las especies del género "Thrasya" aceptadas hasta noviembre de 2014, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 




</doc>
<doc id="2806" url="https://es.wikipedia.org/wiki?curid=2806" title="Themeda">
Themeda

Themeda es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de África, Asia y Australia.
El número cromosómico básico es x = 5 y 10, con números cromosómicos somáticos de 2n = 20, 40, 60 y 80 (y aneuploides). 



</doc>
<doc id="2807" url="https://es.wikipedia.org/wiki?curid=2807" title="Thamnocalamus">
Thamnocalamus

Thamnocalamus, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del este de Asia y Sudáfrica. Comprende 28 especies descritas y de estas, solo 2 aceptadas.
El género fue descrito por William Munro y publicado en "Transactions of the Linnean Society of London" 26(1): 33, 157. 1868. La especie tipo es: "Thamnocalamus spathiflorus" (Trin.) Munro 
A continuación se brinda un listado de las especies del género "Thamnocalamus" aceptadas hasta noviembre de 2013, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 


</doc>
<doc id="2808" url="https://es.wikipedia.org/wiki?curid=2808" title="Tristachya">
Tristachya

Triscenia es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de América, África tropical y Madagascar.



</doc>
<doc id="2810" url="https://es.wikipedia.org/wiki?curid=2810" title="Tabla de verdad">
Tabla de verdad

Una Tabla de verdad, o Tabla de valores de verdades, es una tabla que muestra el valor de verdad de una proposición compuesta, para cada combinación de verdad que se pueda asignar.

Fue desarrollada por Charles Sanders Peirce por los años 1880, pero el formato más popular es el que introdujo Ludwig Wittgenstein en su "Tractatus logico-philosophicus", publicado en 1921.

Para establecer un Sistema formal se establecen las definiciones de los operadores. Las definiciones se harán en función del fin que se pretenda al construir el sistema que haga posible la formalización de argumentos:

El valor verdadero se representa con la letra V; si se emplea notación numérica se expresa con un uno: 1; en un circuito eléctrico, el circuito está cerrado cuando esta presente la afirmación de V.
El valor falso F; si se emplea notación numérica se expresa con un cero: 0; en un circuito eléctrico, el circuito está abierto.
Para una variable lógica A, B, C, ... pueden ser verdaderas V, o falsas F, los operadores fundamentales se definen así:
La negación operador que se ejecuta, sobre un único valor de verdad, devolviendo el valor contradictorio de la proposición considerada. 
La conjunción es un operador, que actúa sobre dos valores de verdad, típicamente los valores de verdad de dos proposiciones, devolviendo el valor de verdad "verdadero" cuando ambas proposiciones son verdaderas, y "falso" en cualquier otro caso. Es decir, es verdadera cuando ambas son verdaderas.

En términos más simples, será verdadera cuando las dos proposiciones son verdaderas.

La tabla de verdad de la conjunción es la siguiente:

Que se corresponde con la columna 8 del algoritmo fundamental.

en simbología "∧" hace referencia al conector "y"
La disyunción es un operador lógico que actúa sobre dos valores de verdad, típicamente los valores de verdad de dos proposiciones, devolviendo el valor de verdad "verdadero" cuando una de las proposiciones es verdadera, o cuando ambas lo son, y "falso" cuando ambas son falsas.

En términos más simples, será verdadera cuando por lo menos una de las proposiciones es verdadera de lo contrario será falsa.

La tabla de verdad de la disyunción es la siguiente:

Que se corresponde con la columna 2 del algoritmo fundamental.

El condicional material es un operador que actúa sobre dos valores de verdad, típicamente los valores de verdad de dos proposiciones, devolviendo el valor de "falso" sólo cuando la primera proposición es verdadera y la segunda falsa, y "verdadero" en cualquier otro caso.

La tabla de verdad del condicional material es la siguiente:

Que se corresponde con la columna 5 del algoritmo fundamental.

La bicondicional es una operación binaria lógica que asigna el valor verdadero cuando las dos variables son iguales y el valor falso cuando son diferentes.

La tabla de verdad del bicondicional es la siguiente:
Que se corresponde con la columna 7 del algoritmo fundamental.

Partiendo de un número n de variables, cada una de las cuales puede tomar el valor verdadero: V, o falso: F, por combinatoria, podemos saber que el número total de combinaciones: nc, que se pueden presentar es:

el número de combinaciones que se pueden dar con n variable, cada una de las cuales puede tomar uno entre dos valores lógicos es de dos elevado a n, esto es, el número de combinaciones: nc, tiene crecimiento exponencial respecto al número de variable n:

Si consideramos que un sistema combinacional de n variables binarias, puede presentar un resultado verdadero: V, o falso: F, para cada una de las posibles combinaciones de entrada tenemos que se pueden construir un número de funciones: nf con n variables de entrada, donde:

Que da como resultado la siguiente tabla:

Para componer una tabla de verdad, pondremos las n variables en una línea horizontal, debajo de estas variables desarrollamos las distintas combinaciones que se pueden formar con V y F, dando lugar a las distintas nc, número de combinaciones. Normalmente solo se representa la función para la que se confecciona la tabla de verdad, y en todo caso funciones parciales que ayuden en su cálculo, en la figura, se pueden ver todas las funciones posibles nf, que pueden darse para el número de variables dado.

Así podemos ver que para dos variables binarias: A y B, n= 2 , que pueden tomar los valores V y F, se pueden desarrollar cuatro combinaciones: nc= 4, con estos valores se pueden definir dieciséis resultados distintos, nf= 16, cada una de las cuales seria una función de dos variables binarias. Para otro número de variables se obtendrán los resultados correspondientes, dado el crecimiento exponencial de nf, cuando n toma valores mayores de cuatro o cinco, la representación en un cuadro resulta compleja, y si se quiere representar las combinaciones posibles nf, resulta ya complejo para n= 3.

Un circuito sin variables: n= 0, puede presentar una combinación posible: nc=1, con dos funciones posibles: nf=2. Que serían el circuito cerrado permanentemente, y el circuito abierto permanentemente.

En este caso se puede ver dos funciones con cero variables, caso 1 y 2, que no interviene ninguna variable.

Cada uno de estos circuitos admite una única posición y hay dos circuitos posibles.

El caso de una variable binaria: n= 1, que puede presentar dos combinaciones posibles: nc=2, con 4 funciones posibles: nf=4.

Se pueden ver las cuatro funciones, de una variable, del caso 1 al 4, siendo A la variable. Puede verse que:

Considérese dos variables proposicionales "A" y "B". Cada una puede tomar uno de dos valores de verdad: o V (verdadero), o F (falso). Por lo tanto, los valores de verdad de "A" y de "B" pueden combinarse de cuatro maneras distintas: o ambas son verdaderas; o "A" es verdadera y "B" falsa, o "A" es falsa y "B" verdadera, o ambas son falsas. Esto puede expresarse con una tabla simple:

Considérese además a: f, como una operación o función lógica que realiza una función de verdad al tomar los valores de verdad de "A" y de "B", y devolver un único valor de verdad. Entonces, existen 16 funciones distintas posibles, y es fácil construir una tabla que muestre qué devuelve cada función frente a las distintas combinaciones de valores de verdad de "A" y de "B".

Las dos primeras columnas de la tabla muestran las cuatro combinaciones posibles de valores de verdad de A y de B. Hay por lo tanto 4 líneas, y las 16 columnas despliegan todos los posibles valores que puede devolver una función.

De esta forma podemos conocer mecánicamente, mediante algoritmo, los posibles valores de verdad de cualquier conexión lógica interpretada como función, siempre y cuando definamos los valores que devuelva la función.

Se hace necesario, pues, definir las funciones que se utilizan en la confección de un sistema lógico.

De especial relevancia se consideran las definiciones para el Cálculo de deducción natural y las puertas lógicas en los circuitos electrónicos. Puede verse que:

Y también que:

Y que:

Las tablas nos manifiestan los posibles valores de verdad de cualquier proposición molecular, así como el análisis de la misma en función de las proposicíones que la integran, encontrándonos con los consiguientes casos:

Se entiende por proposición tautológica, o tautología, aquella proposición que en todos los casos posibles de su tabla de verdad su valor siempre es V. Dicho de otra forma, su valor V no depende de los valores de verdad de las proposiciones que la forman, sino de la forma en que están establecidas las relaciones sintácticas de unas con otras. Sea el caso:

Siguiendo la mecánica algorítmica de la tabla anterior construiremos su tabla de verdad, tenemos la variable A en disyunción con su contradicción, si A es verdad, su negación es falsa y si A es falsa su negación es verdad, en cualquier caso una de las dos alternativas es cierta, y su disyunción es cierta en todos los casos.

Se entiende por proposición contradictoria, o contradicción, aquella proposición que en todos los casos posibles de su tabla de verdad su valor siempre es F. Dicho de otra forma, su valor F no depende de los valores de verdad de las proposiciones que la forman, sino de la forma en que están establecidas las relaciones sintácticas de unas con otras. Sea el caso:

Procederemos de manera similar al caso anterior. Partiendo de la variable A y su contradicción, la conjunción de ambos siempre es falso, dado que si A es verdad su contradicción es falsa, y si A es falsa su contradicción es verdad, la conjunción de ambas da falso en todos los casos.

La definición de la tabla de verdad corresponde a funciones concretas, en cada caso, así como a implementaciones en cada una de las tecnologías que pueden representar funciones lógicas en binario, como las puertas lógicas o los circuitos de conmutación. Se entenderá como verdad la conexión que da paso a la corriente; en caso contrario se entenderá como falso. Veamos la presentación de los dieciséis casos que se presentan con dos variables binarias A y B:


El primer caso en una función lógica que para todas las posibles combinaciones de A y B, el resultado siempre es verdadero, es un caso de tautología, su implementación en un circuito es una conexión fija.

En este segundo caso el resultado solo es falso si A y B son falsos, si una de las dos variables es verdad el resultado es verdad.

La función seria:

En el tercer caso es verdad si A es verdad y cuando A y B son falsos el resultado también es verdad.

Su función seria:

En el cuarto caso la función es cierta si A es cierta, los posibles valores de B no influyen en el resultado.

La función solo depende de A:

En el quinto caso si A es falso el resultado es verdadero, y si A y B son verdaderos el resultado también es verdadero, puede verse que este caso es idéntico al tercero permutando A por B.

Y si función es:

En el sexto caso la función es cierta si B es cierta, los valores de A no influyen en el resultado.

La función solo depende de B:

El séptimo caso corresponde a la relación bicondicional entre A y B, el resultado solo es verdad si A y B son ambos verdad o si A y B son ambos falsos.


En el octavo caso el resultado es verdad si A y B son verdad, en el resto de los valores de A y B el resultado es falso, corresponde a la conjunción de A y B, equivalente a un circuito en serie.


En el noveno caso el resultado solo es falso si A y B son verdad, en el resto de los valores de A y B el resultado es verdadero, corresponde a la disyunción de la negación A y de B, equivalente a un circuito en paralelo de conexiones inversas.


Podemos ver que el décimo caso es lo opuesto a la bicondicional, solo es verdad si A y B discrepan, si A y B son diferentes el valor es verdad, si A y B son iguales el resultado es falso.


En este caso podemos ver que cuando B es verdad el resultado es falso y que cuando B es falso el resultado es verdadero, independientemente del valor de A, luego la función solo depende de B, en sentido inverso.

En el caso doce, vemos que solo hay un combinación de A y B con resultado verdadero, que es A y la negación de B.


En el caso decimotercero podemos ver que el resultado es el opuesto de A, independientemente del valor de B:

Caso decimocuarto, el resultado de la función solo es verdad si A es falso y B verdadero, luego es equivalente a un circuito en serie de A en conexión inversa y de B en conexión directa.


En el caso decimoquinto, el resultado solo es verdad si A y B son falsos, Luego es necesario que tanto A como B sean falsos para que el resultado sea verdadero.


Por último en el caso decimosexto, tenemos que el resultado siempre es falso independientemente de los valores de A o de B.
Se entiende por verdad contingente, o verdad de hecho, aquella proposición que puede ser verdadera o falsa, según los valores de las proposiciones que la integran. Sea el caso: formula_43.

Su tabla de verdad se construye de la siguiente manera:

Ocho filas que responden a los casos posibles que pueden darse según el valor V o F de cada una de las proposiciones A, B, C. 

Una columna en la que se establecen los valores de formula_44 aplicando la definición del disyuntor a los valores de B y de C en cada una de las filas.

Una columna en la que se establecen los valores resultantes de aplicar la definición de la conjunción entre los valores de A y valores de la columna formula_44, que representarán los valores de la proposición completa formula_43, cuyo valor de verdad es V o F según la fila de los valores de A, B, y C que consideremos. 

Donde podemos comprobar cuándo y por qué la proposición formula_43 es V y cuándo es F.

En realidad toda la lógica está contenida en las tablas de verdad, en ellas se nos manifesta todo lo que implican las relaciones sintácticas entre las diversas proposiciones.

No obstante la sencillez del algoritmo, aparecen dos dificultades.


Esta dificultad ha sido magníficamente superada por la rapidez de los ordenadores, y no presenta dificultad alguna.


Por ello se construye un cálculo mediante cadenas deductivas:

Las proposiciones que constituyen el "antecedente" del esquema de inferencia, se toman como premisas de un argumento.

Se establecen como reglas de cálculo algunas tautologías como tales leyes lógicas, (pues garantizan, por su carácter tautológico, el valor V).

Se permite la aplicación de dichas reglas como reglas de sustitución de fórmulas bien formadas en las relaciones que puedan establecerse entre dichas premisas.

Deduciendo mediante su aplicación, como teoremas, todas las conclusiones posibles que haya contenidas en las premisas.

Cuando en un cálculo se establecen algunas leyes como principios o axiomas, el cálculo se dice que es axiomático.

El cálculo lógico así puede utilizarse como demostración argumentativa.

La aplicación fundamental se hace cuando se construye un sistema lógico que modeliza el lenguaje natural sometiéndolo a unas reglas de formalización del lenguaje. Su aplicación puede verse en el cálculo lógico.

Una aplicación importante de las tablas de verdad procede del hecho de que, interpretando los valores lógicos de verdad como 1 y 0 (lógica positiva) en el sentido que


Los valores de entrada o no entrada de corriente a través de un diodo pueden producir una salida 0 ó 1 según las condiciones definidas como función según las tablas mostradas anteriormente. 

Así se establecen las algunas funciones básicas: AND, NAND, OR, NOR, XOR, XNOR (o NXOR), que se corresponden con las funciones definidas en las columnas 8, 9, 2, 15, 10 y 7 respectivamente, y la función NOT.

En lugar de variables proposicionales, considerando las posibles entradas como EA y EB, podemos armar una tabla análoga de 16 funciones como la presentada arriba, con sus equivalentes en lógica de circuitos.

Esta aplicación hace posible la construcción de aparatos capaces de realizar estas computaciones a alta velocidad, y la construcción de circuitos que utilizan este tipo de análisis se hace por medio de puertas lógicas.

La Tabla de la verdad es una herramienta imprescindible en la recuperación de datos en las bases de datos como Internet con los motores de búsqueda o en una biblioteca con sus ficheros informatizados. Así mismo se utilizan para programar simulaciones lógicas de inteligencia artificial con lenguajes propios. También en modelos matemáticos predictores: meteorología, marketing y otros muchos.





</doc>
<doc id="2811" url="https://es.wikipedia.org/wiki?curid=2811" title="Tripsacum">
Tripsacum

Tripsacum es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de América.
El nombre del género deriva de las palabras griegas "tri" (tres) y "psakas" (pequeños), refiriéndose a la ruptura de los picos en (al menos) tres piezas. 
El número cromosómico básico es x = 9, con números cromosómicos somáticos de 2n = 36, 72, 90 y 108. 4, 8, 10 y 12 ploides. Nucléolos persistentes. 



</doc>
<doc id="2812" url="https://es.wikipedia.org/wiki?curid=2812" title="Triplasis">
Triplasis

Triplasis, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originaria del sudoeste de los Estados Unidos. Comprende 23 especies descritas y de estas, solo 2 aceptadas.
Son plantas anuales o perennes, generalmente cespitosas o raramente rizomatosas. Hojas caulinares; la lígula es una hilera de tricomas; láminas lineares, aplanadas. Inflorescencias panículas pequeñas, terminales y axilares; cleistogenes también ocultos dentro de vainas foliares inferiores ligeramente infladas. Espiguillas lineares, comprimidas lateralmente, pediceladas, con varios flósculos bisexuales; desarticulación arriba de las glumas y entre los flósculos; glumas subiguales, 1-nervias, 2-fidas; entrenudos de la raquilla largos; lemas 2-lobadas, carinadas, 3-nervias, la nervadura central proyectándose como una arista, serícea, las nervaduras laterales seríceas, paralelas a la nervadura central; pálea arqueada hacia afuera, las quillas fuertemente viloso-ciliadas en el 1/2 superior; lodículas 2; estambres 3; estilos 2. Fruto una cariopsis; embrión de 1/2 la longitud de la cariopsis; hilo elíptico, de 1/4 la longitud de la cariopsis.
El género fue descrito por Ambroise Marie François Joseph Palisot de Beauvois y publicado en "Essai d'une Nouvelle Agrostographie" 81. 1812. La especie tipo es: "Triplasis americana" 
El nombre del género proviene del griego "triplasios" (triple), en alusión a las lemas con una arista y dos lóbulos. 
A continuación se brinda un listado de las especies del género "Triplasis" aceptadas hasta junio de 2015, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 




</doc>
<doc id="2813" url="https://es.wikipedia.org/wiki?curid=2813" title="Triniochloa">
Triniochloa

Triniochloa, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de México a Ecuador y Perú.
Son plantas perennes cespitosas. Hojas principalmente caulinares; vainas con los márgenes unidos; lígula una membrana; láminas lineares, aplanadas. Inflorescencia una panícula terminal. Espiguillas comprimidas dorsalmente, con 1 flósculo bisexual; desarticulación arriba de las glumas; glumas generalmente más cortas que el flósculo o a veces tan largas como él, membranáceas, 1-nervias, iguales, acuminadas; lema subcoriácea, subcilíndrica, 7-9-nervia, 2-dentada, aristada; callo oblicuo, barbado; arista geniculada, torcida por debajo del primer ángulo, insertada por detrás de la lema; palea sulcada, 2-lobada, casi tan larga como la lema; raquilla no extendida; lodículas unidas; estambres 3; ovario glabro; estilos 2. El fruto una cariopsis fusiforme, pardo oscuro; hilo linear.
El género fue descrito por Albert Spear Hitchcock y publicado en "Contributions from the United States National Herbarium" 17: 303. 1913.

Reference article Morales, J. F. 2003. Poaceae. 93(3): 598–821. In B. E. Hammel, M. H. Grayum, C. Herrera & N. Zamora Villalobos (eds.) Man. Pl. Costa Rica. Missouri Botanical Garden, St. Louis.



</doc>
<doc id="2815" url="https://es.wikipedia.org/wiki?curid=2815" title="Trachypogon">
Trachypogon

Trachypogon es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de América tropical, África y Madagascar.
El nombre del género deriva de las palabras griegas "trachus" (en bruto) y "pogon" (barba), refiriéndose a las aristas de las espiguillas femeninas fértiles. 
El número cromosómico básico es x = 5 y 10, con números cromosómicos somáticos de 
2n = 20 y 40. tetraploides. 



</doc>
<doc id="2822" url="https://es.wikipedia.org/wiki?curid=2822" title="Tipografía">
Tipografía

La tipografía (del griego τύπος "[típos]", ‘golpe’ o ‘huella’, y γράφω "[gráfο]", ‘escribir’) es el arte y la técnica en el manejo y selección de tipos para crear trabajos de impresión.

El tipógrafo Stanley Morison la definió como:

Al método de impresión que hace uso de tipos, también se le denomina "tipografía o impresión tipográfica" ("letterpress") en contraposición a otros métodos existentes, tales como impresión ófset, impresión digital, etc.

Se denomina tipografía a la tarea u oficio e industria que se ocupa de la elección y el uso de tipos (letras diseñadas con unidad de estilo) para desarrollar una labor de impresión, la cual hace referencia a los elementos letras, números y símbolos pertenecientes a un contenido impreso, ya sea en soporte físico o digital.

Es importante tener presente qué se quiere comunicar, porque ello definirá qué tipo es el más representativo para la intención buscada.






La imprenta en Europa se desarrolló en el auge del Renacimiento; sin embargo, los primeros impresos de Johannes Gutenberg como la Biblia de 42 líneas utilizaron un estilo de letra del período gótico la cual se le denominó: textura.

Los primeros tipos móviles, inventados por Johann Gutenberg, y el tipo de letra redonda o romana que le siguió en Italia, imitaban el estilo manuscrito de esos países en boga en aquellos momentos.

Aunque se sabe ahora que los chinos ya habían experimentado con tipos móviles de cerámica en el siglo XI, Gutenberg es reconocido como el padre del tipo móvil. Vivió en Maguncia (Alemania) y era orfebre de oficio, pero adquirió los conocimientos técnicos sobre el arte de la impresión. Ya se habían hecho impresiones a partir de bloques de madera tallados a mano muchos años antes.

En 1440 comenzó una serie de experimentos que, diez años después, darían como resultado la invención de la imprenta a partir de tipos móviles. Utilizó sus conocimientos sobre la tecnología y los materiales existentes —la prensa de tornillo, las tintas a base de aceite y el papel—, pero fue la manufactura de los tipos a la que le dedicó gran parte de sus esfuerzos.

Como orfebre conocía muy bien el modelado, mezcla y fundición de metales, lo que le permitió desarrollar un método para fabricar los tipos. Se trataba de grabar cada carácter en relieve de forma inversa sobre un troquel de acero que se incrustaba con un mazo en la terraja (una barra de cobre). La terraja se colocaba en la matriz, un molde maestro para fundir cada letra, según un proceso llamado justificación. Después, la matriz se colocaba en un molde manual ajustable sobre el que se vertía una aleación de plomo y antimonio, y de ese modo modelaba cada uno de los tipos.
Los frutos visibles de sus trabajos son la "Biblia de 42 líneas", en 1445, el libro más antiguo impreso en el mundo occidental, aunque imprimió Indulgencia de Maguncia el año anterior, para el cual utilizó un estilo cursivo de la letra gótica, llamada bastarda.

Los primeros tipos de letra redonda que aparecieron en Italia entre los años 1460 y 1470 estaban basados en la escritura manual humanista. Un renovado interés por la minúscula carolingia, había provocado un refinamiento en su diseño, el resultado fue el proyecto final para el primer tipo romano.

Después de 1460, el liderazgo en el desarrollo de los tipos móviles, pasó de Alemania a Italia, centro artístico del renacimiento. En 1465, en Subiaco, cerca de Roma, Conrad Sweynheym y Arnold Pennartz, dos alemanes que se habían desplazado a Italia, influidos por el trabajo de Gutenberg, crearon un tipo híbrido, mezcla de características góticas y romanas. En 1467 se trasladaron a Roma y en 1470 habían creado un nuevo conjunto de letras, basados en la escritura humanista.

Mientras tanto en Venecia, en 1469, los hermanos da Spira, crearon otra tipo romano, superior al anterior. Pese a ello, en 1470 Nicholas Jenson creó un tipo de letra que superaba a todas las diseñadas en la época en Italia y que siguió perfeccionando, creando uno nuevo seis años después y conocido como "romana de letra blanca", utilizado para la impresión de Nonius Peripatética. Desde entonces, las proporciones de Jenson han servido de inspiración para los diseños de tipos.

A pesar de que el estilo predominante en Italia era el romano, no era el único. Incluso Jenson continuó produciendo libros en letra gótica, al igual que muchos otros. En 1483, como cosa inusual, el alemán Erhard Ratdolt, imprimió Eusebius usando la letra gótica y la romana de forma conjunta.

Durante la Edad Media, la cultura del libro giraba en torno a los monasterios cristianos, de los cuales podría decirse que hacían de casas editoriales en el sentido moderno del término. Los libros no eran impresos, sino escritos por monjes especializados en esta tarea que eran llamados copistas; ellos desarrollaban su trabajo en un lugar que había en la mayoría de los monasterios llamado scriptorium que contaba con una biblioteca y un salón con una especie de escritorios similares a los atriles de las iglesias de la actualidad. En este lugar, los Monjes transcribían los libros de la biblioteca, ya fuera por encargo de un señor feudal o de otro monasterio.

Durante el estilo Gótico, Europa retornó paulatinamente a un sistema económico dependiente de las ciudades —y no del campo como lo fue tradicionalmente durante casi toda la Edad Media—, lo que determinó el nacimiento de los gremios, los cuales dieron paso a una mayor producción de libros. Los libros, generalmente religiosos, eran encargados por patrones que se les otorgaba a un gremio de artistas de libros, los cuales tenían especialistas capacitados en letreros, mayúsculas decorativas, decoración de letras, corrección de galeras y encuadernación; al ser este un proceso totalmente artesanal, un libro de 200 páginas podía llegar a demorarse de 5 a 6 meses, y se requerían aproximadamente 25 pieles de carnero para hacer la vitela donde se escribía e ilustraba con témpera de huevo, guache y una primitiva forma de óleo.

Las ciudades que más se fortalecieron durante el periodo gótico, fueron las de Europa del norte, como lo son París, Londres y un gran número de ciudades alemanas, las cuales fueron las primeras que adoptaron el sistema gremial; además de esto, la ciudad determinó el nacimiento de las universidades, lo cual hizo aumentar la demanda de manuscritos y planteó la necesidad de encontrar un nuevo modo de producción de libros, masivo y mucho más económico.

El papel llegó a Occidente, siguiendo las rutas de las caravanas que venían del lejano oriente en Asia hacia el mar Mediterráneo, hasta que alcanzó el mundo árabe, y estos, a su vez llevaron el invento a Europa durante las invasiones árabes que llegaron hasta España.

En poco tiempo, aproximadamente hacia mediados del siglo XIV, las primeras fábricas de papel se extendieron desde España a Francia, Italia, Gran Bretaña y Alemania. El mismo camino que tomó el papel, también lo hizo la xilografía, otro invento chino. Las primeras manifestaciones de este sistema de impresión, se pudieron ver en los juegos de naipes y en imágenes religiosas. Por ser estos los primeros diseños que se introdujeron en una cultura iletrada, representaron la primera manifestación de la democratización del arte de la imprenta en Europa. Estas imágenes iban cargadas de signos y símbolos, los cuales obligaban a una deducción lógica. La xilografía permitió que los libros estuvieran al alcance del común de la gente, la cual, en su mayoría era analfabeta y por tal razón, el libro de bloque traía muy poco texto y muchas ilustraciones, las cuales eran entendidas por cualquier persona, a diferencia del texto que necesitaba de la alfabetización de la población.

Este sistema, sin embargo, seguía siendo bastante caro, pues tomaba mucho tiempo grabar en la madera cada letra e ilustración, lo cual determinó que fueran libros de muy poca extensión, aproximadamente de 30 a 50 folios.

Los primeros libros de bloque se imprimieron con un sello de mano y tinta color sepia o gris, que luego sería reemplazada por la tinta negra. Después de imprimir el texto y las ilustraciones, estas se coloreaban a mano con la misma técnica que se aplicaba en los manuscritos góticos.

Algunos grabadores que hacían libros de bloque, al tratar de simplificar su trabajo, trataron de grabar cada letra independientemente para utilizarla varias veces en diferentes libros, pero al ser la madera un material muy maleable, las letras se deformaban al cabo de pocas impresiones. A mediados del siglo XV, surgió un nuevo invento, el cual recibió diferentes denominaciones, entre las que figuran «sistema de impresión por tipos móviles», «tipografía» e «imprenta».

El primero en realizar un proceso de impresión por tipos móviles de metal en Occidente fue el alemán Johannes Gutenberg, que produjo sus primeros impresos entre los años de 1448 y 1450. Cabe destacar que aunque el desarrollo de este proceso de impresión es principalmente europeo, se produjo gracias a ciertos cambios ocurridos en la Europa medieval:

Es así como Gutenberg adaptó una prensa, y fundió miles de tipos móviles en metal, los cuales se podían adaptar en la prensa por medio de una caja llamada tipográfica. En la impresión medieval de bloque, se usaba tinta de agua ligera extraída de las agallas del encino, la cual era muy bien absorbida por la madera, pero en el tipo de metal se corría o emborronaba. Para producir una tinta espesa y pegajosa, Gutenberg empleó aceite de linaza hervido, que después coloreado con pigmento de humo. Lo único que se hacía a mano en el impreso tipográfico, era el diseño de la letra capital, y la aplicación de su color.

En los manuscritos iluminados, los libros tenían una generosa cantidad de imágenes que fueron suprimidas paulatinamente de los libros tipográficos por la imposibilidad tecnológica de la época de fundir en metal toda una imagen; debido a que la producción de un manuscrito iluminado era sumamente costosa, la impresión de bloque y tipográfica, permitió abaratar estos costos, logrando así que la escritura, al igual que la información se difundiera y produjera cambios de pensamiento en Europa, los cuales traerían reformas, contrarreformas y revoluciones.

Hacia 1500, el invento de Gutenberg había tenido tan amplia difusión, que en Europa ya existían aproximadamente 1100 imprentas funcionando. En los países germanos el estilo de letra más usado era la fraktur (aunque el tipo utilizado en la primera Biblia de Gutenberg fue «textura»). A diferencia de Alemania, en el sur de Europa la costumbre en la Edad Media era utilizar la minúscula carolingia junto a las mayúsculas cuadradas romanas adaptadas de las inscripciones que se encontraban en las ruinas del Imperio romano, como la Columna de Trajano; por tal razón, este estilo de escritura, sirvió de modelo a los primeros impresores italianos, para crear las familias tipográficas clásicas o con serifas (también llamados "gavilanes" o "remates"). El primer tipo de letra con serifas apareció en el año de 1465, más tarde, tipógrafos e impresores de la talla de Nicolas Jenson y Aldo Manucio perfeccionaron estas primeras fundiciones, volviéndolas más estilizadas y refinadas además de incluir un nuevo estilo de letra que se llamó «bastardilla», el cual fue tomado de la caligrafía cancilleresca de la época; actualmente a este estilo de letra se le llama «itálica» (por el país de procedencia) o «cursiva», y es utilizado para resaltar en un texto palabras escogidas por el editor, extranjerismos y citas.

A estos primeros tipos romanos, clásicos o con serifa, se les dio el nombre de "estilo veneciano", pues las principales imprentas italianas que los producían se habían establecido en la ciudad de Venecia.

En Francia, cabe destacar al tipógrafo e impresor Claude Garamond, que creó entre las décadas de 1530 y 1550 una familia tipográfica francesa basada en el estilo veneciano, que con el tiempo se convirtió en el "estándar" de su época y otras posteriores.

Durante la industrialización se realizaron mejoras en casi todos las tareas relacinados con la tipografía: en el diseño de tipos Linn Boyd Benton patenta en 1885 el grabador pantográfico para la realización de matrices y punzones; en la composición se inventaron dos máquinas componedoras, la monotipia en 1886, donde cada letra del molde se funde en relieve por separado, y en la linotipia en 1887 se funde cada línea entera por separado (de ahí su nombre), y al acabar la impresión cada línea se vuelve a fundir para crear nuevas líneas.

Los movimientos artísticos están íntimamente relacionados con la tipografía y su diseño son: futurismo, dadaísmo, constructivismo ruso, movimiento "De Stijl" y suprematismo.

Tras la segunda guerra mundial se dieron los primeros pasos en la realización de máquinas que permitieran sustituir a las componedoras mecánicas por sistemas fotográficos, pero no fue hasta 1956 cuando se comercializó la primera máquina fotocomponedora que mejoraba a las tradicionales linotipia y monotipia.

En la década de 1960 con el uso de tubos de rayos catódicos se consiguió un aumento en la productividad y tuvo un gran impacto en la industria gráfica.

PostScript es un lenguaje que codifica la información descriptiva, independientemente de la resolución o el sistema.


Partes que componen un tipo:

Los tipos de letra se clasifican a través de estilos por su forma y también por el momento en el que fueron diseñadas.

Los primeros tipos móviles creados por Johannes Gutenberg, imitaban la escritura manuscrita de la Edad media. Por esta razón no es de extrañar, que los primeros tipos que comenzaron a fundirse fueran la letra gótica o "fraktur" en Alemania y la humanística o romana (también llamada Veneciana) en Italia. La evolución del diseño tipográfico ha permitido establecer una clasificación de los tipos de letra por estilos generalmente vinculados con las épocas en las que fueron creadas las familias tipográficas.







Una forma de clasificar las letras es según tengan o no «serifas». Se entiende por "serifas", o "remates", las pequeñas líneas que se encuentran en las terminaciones de las letras, principalmente en los trazos verticales o diagonales. La utilidad de las serifas es facilitar la lectura, ya que estas crean en el ojo la ilusión de una línea horizontal por la que se desplaza la vista al leer.

Las letras sin serifas o de palo seco, son aquellas que no llevan ningún tipo de terminación; por lo general son consideradas inadecuadas para un texto largo ya que la lectura resulta incómoda pues existe una tendencia visual a identificar este tipo de letras como una sucesión de palos verticales consecutivos.

Por esta razón, las letras con serifas (llamadas también romanas) se utilizan en los periódicos, revistas y libros, así como en publicaciones que contienen textos extensos.
Las letras sin serifas o palo seco son usadas en titulares, rótulos, anuncios y publicaciones con textos cortos de estilo "valiante". Ante la aparición de los medios electrónicos, las letras de palo seco se han convertido también en el estándar para la edición en la web y los formatos electrónicos ya que por la baja resolución de los monitores las serifas terminan distorsionando el tipo. Esto se debe a que las curvas pequeñas son muy difíciles de reproducir en los píxeles de la pantalla.

También podemos clasificarlos dependiendo de su uso de una manera más coloquial como lo usan actualmente los tipografos en máquina de tipografía con Heidelberg de Aspas o Minerva

Son las letras o tipos, orlas, signos, números…, en definitiva, todos los caracteres que se pueden imprimir.

Son piezas de altura inferior a los tipos y se utilizan para crear interlineados, entre palabras, espacios, lingotes o imposiciones.

La mayoría de las escrituras comparte la noción de una línea de base: una línea horizontal imaginaria sobre la cual se apoyan los caracteres. En algunas escrituras, hay partes de los glifos que van por debajo de la línea de base pendiente que atraviesa la distancia entre la línea de fondo y el glifo descendiente más bajo de un tipo de letra, y la parte de un glifo que desciende debajo de la línea de fondo tiene el conocido descendiente. Inversamente, subida atraviesa la distancia entre la línea de fondo y la tapa del glifo que alcanza lo más lejos posible de la línea de fondo. La subida y la pendiente pueden o no pueden incluir la distancia agregada por acentos o marcas diacríticas.

En las escrituras latina, griega y cirílica (designadas a veces colectivamente como LGC) se pueden referir a la distancia de la parte inferior a la superior de los glifos, de las minúsculas (línea mala) como x-altura, y la parte de un glifo que se levanta sobre la x-altura como ascendiente. La distancia de la línea de fondo a la tapa de la subida o los glifos mayúsculos regulares (línea del casquillo) también se conoce como la altura del casquillo. La altura del ascender puede tener un efecto dramático en la legibilidad y el aspecto de una fundición. El cociente entre la x-altura y la altura de la subida o del casquillo sirve a menudo para su caracterización. Para medir la altura de las letras se usa el tipometro que es una regla graduada donde por un lado aparecen los centímetros y milímetros, y por otro los ciceros y puntos.

Justificar o alinear un texto es la manera de acomodar las líneas en la caja. Es decir, es la manera en que se alinean entre sí, apoyándose en un lado, al centro o consiguiendo una forma caprichosa. Tomando en cuenta que la palabra "caja" apela al antiguo método de acomodar tipos (letras) en un recipiente de madera para conformar columnas, podemos imaginar claramente las líneas apoyadas a la izquierda en una columna, por ejemplo.

Los nombres que se dan a las formas de justificar un texto varían ocasionalmente entre los diferentes países, pero podemos decir que los más usuales son:

En la actualidad, las columnas de texto se aplican también en formas caprichosas ya sea siguiendo el contorno de una figura o creando una figura con ellas mismas. La creatividad ha desarrollado retratos formados con el texto de la biografía del personaje y un sinfín de aplicaciones se ven comúnmente en deformaciones legibles o prácticamente ilegibles, buscando atraer la atención del observador. Justificar es entonces, simplemente dar un formato cualquiera al texto en cuestión.

El espaciado o "tracking" se refiere al espacio que existe entre cada par de palabras en un texto en relación con el cuadratín o ancho y alto del cuerpo usado.

Una segunda forma de clasificar las letras es según el ancho o grueso, es decir, el espacio que ocupa horizontalmente cada letra. Desde los comienzos de la escritura y la caligrafía y por supuesto de la tipografía, los primeros maestros notaron que no todas las letras eran iguales en su ancho y por tal razón, el espacio entre cada una de ellas debería variar para que la lectura fuese fluida y equilibrada. Al contrario de este razonamiento, las letras de las máquinas de escribir ocupaban cada una el mismo espacio, de manera que en el texto se veían espacios distintos entre ellas. Teniendo en cuenta que no todas las letras tienen el mismo ancho: Una «m» ocupaba todo el espacio, mientras que una «i» ocupaba mucho menos. Si en el texto aparecían seguidas una «i» y una «l», el espacio entre ambas era muy grande, mientras que si aparecían seguidas una «m» y una «o» el espacio era muy reducido. De todo ello resultaba una considerable incomodidad de la lectura y, por ejemplo, en el caso de titulares o rótulos.

A continuación se muestra un ejemplo de cada uno de los dos tipos de letras:

Como lo dice la palabra, el interlineado es la separación existente que hay entre líneas. Se mide en puntos.

Los procesadores de textos de los ordenadores actuales disponen de una amplia gama de tipos —también llamados incorrectamente, por influencia del inglés, fuente—, tanto de un tipo como de otro.

La letra "Times New Roman" fue diseñada originalmente para el periódico inglés "The Times". Mediante este tipo de letra se conseguía una gran legibilidad y un excelente aprovechamiento del espacio, por lo que en seguida se generalizó su uso en los medios impresos y, sobre todo, en la prensa. La gran popularidad de la "Times New Roman" es un punto a su favor para su utilización incluso en medios electrónicos, pero para textos largos en formato electrónico puede producir fatiga, precisamente porque la forma en la que el ojo percibe los bordes en este formato es justo lo contrario que en el papel ya que por la poca resolución de los monitores, las serifas terminan distorsionando el contorno del glifo. Esto se debe a que las curvas pequeñas son muy difíciles de reproducir en los píxeles de la pantalla. Obviamente, la separación entre líneas también influye en la legibilidad de un texto electrónico. Para cartas y correos electrónicos ambos tipos de letras son apropiados, mientras que para informes y contratos (por lo general, largos) son más indicadas las letras con serifa.

Es posible afirmar que todos los tipos cuyo diseño es igual o similar a los tipos clásicos latinos (romanos) son los que ofrecen la mejor legibilidad. Hasta el momento el tipo que ofrece la máxima legibilidad en documentos impresos es la Times New Roman diseñado por Stanley Morison en 1932 para ser usado especialmente para el periódico londinense "The Times".

Sin embargo para la red hay quienes consideran que una de las mejores familias tipográficas es la Verdana, porque no cuenta con serifas que se distorsionen, por lo cual es una de las legibles incluso a tamaños ínfimos en los monitores.

Los principales formatos usados para tipos de letra en informática son: PS, True type y OpenType.

Se sabe poco sobre el efecto de la tipografía en el aprendizaje, aunque si se documentan preferencias subjetivas por ciertas letras, tanto en tamaño como en color. Estas características influyen en la comodidad al estudiar. 






</doc>
<doc id="2823" url="https://es.wikipedia.org/wiki?curid=2823" title="Tejido (biología)">
Tejido (biología)

En biología, los tejidos son aquellos materiales biológicos constituidos por un conjunto complejo y organizado de células, de uno o de varios tipos, distribuidas regularmente con un comportamiento fisiológico coordinado y un origen embrionario común. Se llama histología a la ciencia que estudia los tejidos orgánicos.

Muchas palabras del lenguaje común, como "pulpa", "carne" o "ternilla", designan materiales biológicos en los que un tejido determinado es el constituyente único o predominante; los ejemplos anteriores se corresponden, respectivamente, con "parénquima", "tejido muscular" o "tejido cartilaginoso".

Solo algunos reinos han logrado desarrollar la pluricelularidad en el curso de la evolución, y de estos únicamente en dos se reconoce la existencia de tejidos, a saber: en las plantas vasculares y en los animales (o metazoos). En general, se admite también que hay verdaderos tejidos en las algas pardas. Dentro de cada uno de estos grupos, los tejidos son esencialmente homólogos, pero son diferentes de un grupo a otro, y su estudio y descripción son independientes, por lo que se distinguen una histología vegetal y una histología animal.

En los animales, estos componentes celulares están inmersos en una matriz extracelular más o menos extensa, de características particulares para cada tejido.

Generalmente, esta matriz es generada por las propias células que componen el tejido, por lo que se dice que los tejidos están constituidos por un componente celular y, en algunos casos, por un componente extracelular. El tejido es uno de los niveles de organización biológica, situado entre el nivel celular que está en el escalón inferior, y el nivel del órgano que está en el escalón superior de organización.

La disciplina de la biología encargada del estudio de los tejidos orgánicos es la histología. Si se profundiza en los detalles, puede afirmarse que existen más de una centena de tejidos diferentes en los animales y algunas decenas en los vegetales, pero la inmensa mayoría son tan solo variedades de unos pocos tipos fundamentales. La estructura íntima de los tejidos escapa a simple vista, por lo cual se usa el microscopio para visualizarla.

Un tejido puede estar constituido por células de una sola clase, todas iguales, o por varios tipos de células dispuestas ordenadamente. El grado de especialización de los tejidos varía notablemente, tanto en lo funcional como en lo estructural.
Según su origen embriológico, pueden clasificarse en dos grandes grupos: tejidos especializados y tejidos no especializados.

Las células que forman parte de un tejido se especializan mediante procesos complejos. La diferenciación celular, como otros procesos celulares, está controlada por mecanismos de regulación de la expresión génica tales como el control genómico, el control transcripcional, el control postranscripcional, el control traduccional y el control postraduccional.

Existen cuatro tipos de tejidos fundamentales, en los animales:

Estos tejidos fundamentales, según su origen embriológico, se pueden clasificar en dos grandes grupos:




Los principales tejidos de los organismos eucariontes son los siguientes:


En el , mediante la utilización de impresoras 3D, se crearon tejidos artificiales. Se partió de la impresión de estructuras cartilaginosas, óseas y musculares, que se implantaron en roedores, con lo que se logró que los tejidos artificiales se convirtieran en tejidos funcionales y desarrollaran los vasos sanguíneos tisulares y, por consiguiente, que el material artificial respondiera como si se tratara de un tejido vivo natural.






</doc>
<doc id="2832" url="https://es.wikipedia.org/wiki?curid=2832" title="Técnicas de pintura">
Técnicas de pintura

Las técnicas de pintura se dividen de acuerdo a cómo se diluyen y fijan los pigmentos sobre el soporte a pintar. En general, si los pigmentos no son solubles en el aglutinante, permanecen dispersos en él.

Cuando el vehículo empleado para fijar el pigmento es, en la mayoría de los casos, goma arábiga y el solvente es el agua. Las acuarelas son pigmentos muy finamente molidos y aglutinados en goma arábiga, que se obtiene de las acacias. La goma se disuelve fácilmente en agua y se adhiere muy bien al papel (soporte por excelencia para la acuarela). La goma además actúa como barniz, claro y delgado, dando mayor brillo y luminosidad al color. En un principio la goma arábiga se usaba sola, pero más tarde se añadieron otros componentes para retrasar el secado y añadir transparencia. La acuarela requiere del artista seguridad en los trazos y espontaneidad en la ejecución, ya que su mayor mérito consiste en la frescura y transparencia de los colores. Son pinturas a base de pigmentos muy diminutos los cuales en la mayoría de los casos la acuarela es disuelta en agua y los colores son claros.

El óleo, palabra proveniente del latín "oleum" («aceite»), es una técnica de pintura que consiste en mezclar los pigmentos con un aglutinante a base de aceites, normalmente de origen vegetal.

Al "gouache" o "aguada" se le llama también "el color con cuerpo". Es una pintura al agua, opaca, hecha con pigmento molido menos fino que el de las acuarelas, y por ello es menos transparente. Al igual que la acuarela, su medio —o 
aglutinante— es la goma arábiga, aunque muchos gouaches modernos contienen plástico. El medio está ampliado con pigmento blanco, que es lo que lo hace más opaco, menos luminoso y menos transparente que la acuarela, pero a cambio los colores producidos son más sólidos.

En esta técnica se usan pinturas acrílicas en aerosol o sprays, además de 
esmaltes, ya
que con este método la pintura se vuelve mucho más desgastada. Cuando a la pintura no se le incorporan estos difusores y se deja caer por la gravedad esta técnica se denomina "dripping".

Los pasteles son pigmentos en polvo mezclados con la suficiente goma o 
resina para aglutinarlos formando una pasta seca y compacta. La palabra pastel deriva de la pasta con la que se elaboran estas pinturas. Esta pasta se moldea en la forma de una barrita del tamaño aproximado de un dedo, que se usa directamente sobre la superficie al trabajar (generalmente papel o madera). Son colores fuertes y opacos cuya mayor dificultad es la adhesión del pigmento a la superficie al pintar, por ello suelen usarse al finalizar el dibujo fijadores atomizados (spray) especiales. El pastel generalmente se usa como el "crayón" o el "grafito" (lápiz), y su recurso expresivo más afín es la línea con la cual se pueden hacer tramas. También suele usarse el polvo que tiende a soltar el pastel (semejante al de la "tiza") para aplicar color.

Cuando el aglutinante es una emulsión, generalmente de yema de huevo.

A menudo el término fresco se usa incorrectamente para describir muchas formas de pintura mural. La técnica del fresco se basa en un cambio químico: los pigmentos de tierra molidos y mezclados con agua pura, se aplican sobre una argamasa reciente de cal y arena, mientras la cal está aún en forma de hidróxido de calcio. Debido al dióxido de carbono de la atmósfera, la cal se transforma en carbonato cálcico, de manera que el pigmento cristaliza en el seno de la pared. Los procedimientos para pintar al fresco son sencillos pero laboriosos, y consumen muchísimo tiempo. En la preparación de la cual se tardaba dos años.

La presentación de la tinta, también llamada tinta china, es generalmente líquida aunque también puede ser una barra muy sólida que debe ser molida y diluida previamente. Se usa sobre papel, y los colores de tinta más empleados son el negro y el sepia, aunque actualmente se usen muchos otros más. La tinta se aplica de diversas maneras, por ejemplo con 
plumas o plumillas que son más adecuadas para dibujo o caligrafía, y no para pinturas. Las diferentes puntas de plumillas se utilizan cargadas de tinta para hacer líneas y con ellas dibujar o escribir. Otro recurso para aplicar la tinta es el pincel, que se maneja básicamente como la acuarela y que se llama aguada, no obstante la técnica milenaria llamada caligrafía o escritura japonesa también se realiza con tinta y pincel sobre papel. Otras formas más utilitarias de usar la tinta es en tiralíneas (cargador de tinta) o rapidograph. La tinta junto al grafito son más bien técnicas de dibujo.

La tinta neutra es una técnica frecuente en la restauración de pintura mural. Se utiliza cuando el restaurador se encuentra con grandes pérdidas y desconoce como era el original. Consiste en aplicar un color uniforme en la zona perdida, que no moleste en exceso y que entone con el colorido general de la obra.

Cuando se emplean diversas técnicas en un mismo soporte. El "collage", por ejemplo, es una técnica artística (no pictórica por no ser pintada), se convierte en una técnica mixta cuando tiene intervenciones con gouache, óleo o tinta.
Como muestra de sus posibilidades artísticas, se cita la Técnica introducida por el pintor Carlos Benítez Campos desde principios de siglo, la cual consiste en pintar al óleo un acontecimiento cualquiera de la época, sobre el papel pegado de las noticias en prensa que lo publican.

Sería conveniente distinguir entre "procedimiento pictórico" y "técnica pictórica". Se entiende por "procedimiento pictórico" la unión de los elementos que constituyen el aglutinante o adhesivo, y los pigmentos. La forma de aplicar ese procedimiento pictórico se denomina "técnica pictórica".

El "décollage" designa a la técnica opuesta al "collage".



</doc>
<doc id="2833" url="https://es.wikipedia.org/wiki?curid=2833" title="Tarot (adivinación)">
Tarot (adivinación)

El tarot es una baraja de naipes a menudo utilizada como medio de consulta e interpretación de hechos (presentes, pasados o futuros), sueños, percepciones o estados emocionales que constituye, además, un tipo de cartomancia. Sus orígenes datan al menos del siglo XIV. La técnica se basa en la selección de cartas de una baraja especial, que luego son interpretadas por un lector, según el orden o disposición en que han sido seleccionadas o repartidas. La baraja de tarot está compuesta por 78 cartas, divididas en arcanos mayores los cuáles son 22 y menores que son 56. La palabra «arcano» proviene del latín "arcanum", que significa "misterio" o "secreto".

Las primeras referencias al tarot aparecen en el siglo XV en Italia. La baraja más antigua es el tarot del duque de Milán, Filippo María Visconti (1412-1447), hoy día en la Biblioteca de la Universidad Yale. Es conocida actualmente como la baraja Visconti-Sforza, posiblemente para celebrar el casamiento de su hija Bianca Maria con su sucesor el futuro duque Francisco I Sforza.

De acuerdo al historiador italiano Giordano Berti, algunas imágenes del tarot de Filippo María Visconti son iguales a las de otra baraja diseñada por el duque en 1415: el juego Los XVI Héroes.

En estudios realizados por ocultistas de los siglos XVIII y XIX, como Antoine Court de Gebelin, Eliphas Levi y el doctor Gérard Encausse (Papus), se intenta demostrar la conexión existente entre el tarot y la cábala, así como con el simbolismo egipcio.

Según plantean los investigadores Daniel Rodes y Encarna Sánchez, el origen del tarot habría que buscarlo entre los cátaros medievales y la cultura occitana, cuya filosofía encaja perfectamente en la idea básica del juego de tarot.

Así, la presencia de una papisa, la importancia de los personajes femeninos y claras referencias a un cristianismo distinto al de la ortodoxia romana harían pensar en un uso original del tarot como una transmisión de un conocimiento filosófico, si bien con el paso del tiempo pasarían a ser usadas como un sistema adivinatorio. Pero la papisa fue, en realidad, un símbolo de la fe cristiana, como demuestran numerosas obras de arte de la Edad Media.

Otros autores afirman que los gitanos, en su deambular por los países europeos, promovieron el tarot como un sistema adivinatorio. Hay, de hecho, quien sostiene que el tarot logró sobrevivir a la Inquisición, ya que los gitanos no representaban objetivos prioritarios de la jurisdicción inquisitorial, por los que ellos, sus conocidas prácticas esotéricas y sus efectos personales consiguieron zafarse de la persecución y la hoguera y llegar hasta nuestros días. Pero es cierto que los gitanos llegaron a Europa cuando el tarot era ya conocido. Por otra parte, el tarot se juega en Italia desde el siglo XV, y en el siglo siguiente se propagó en muchas regiones de Europa: en primer lugar Francia, después Suiza, Bélgica, Alemania y Austria. La adivinación con el tarot aparece con seguridad en Italia y Francia en el siglo XVIII.

Parece que los motivos específicos por los cuales fueron añadidos los "triunfos" a la estructura del mazo corriente de cuatro palos de 14 cartas, eran ideológicos.

La idea sería constituir un sistema particular de enviar mensajes de diferente contenido; los primeros ejemplos conocidos exhiben ideas filosóficas, sociales, poéticas, astronómicas y heráldicas. Por ejemplo, así como un grupo de antiguos héroes de la Antigua Roma, Grecia, Babilonia, como en el caso del Tarot Sola-Busca (1491) y el poema de Boiardo Tarocchi (producido en una fecha desconocida entre 1461 y 1494). Por ejemplo, el mazo conocido más antiguo, existente solamente por la descripción en el breve libro de Martiano, fue producido para mostrar el sistema de los dioses griegos, un tema muy de moda en Italia en ese tiempo. Su producción bien puede haber acompañado una celebración triunfal del comisionado Filippo Maria Visconti, duque de Milán, o sea que el propósito de ese mazo fue expresar y consolidar el poder político en Milán (como era común en otras obras de arte de esa época). Los cuatro palos mostraban aves, motivos que aparecían regularmente en la heráldica de los Visconti, y el orden específico de los dioses da fundamento para asumir que el mazo estaba pensado para demostrar que los Visconti se identificaban como descendientes de Júpiter y Venus (que no eran vistos como divinidades pero endiosados como héroes terrenales). El historiador italiano Giordano Berti supone que fue el propio duque de Milán, Filippo Maria Visconti, el inventor del Tarot. Stuart Kaplan, un reconocido experto en el Tarot, dice que todo el simbolismo del Tarot tal como lo conocemos hoy en día se desarrolló del Tarot italiano.

Durante mucho tiempo las cartas de tarot permanecieron como un privilegio de la clase alta y, aunque pueden rastrearse hasta el siglo XIV algunos sermones que arrojaban invectivas contra el demonio inherente a las cartas, la Iglesia católica y la mayoría de los gobernantes civiles no condenaban habitualmente las cartas de tarot en los primeros tiempos de su aparición. De hecho, en algunas jurisdicciones las cartas de tarot estaban específicamente exentas de normas legales que, por el contrario, prohibían el juego de cartas.

Las cartas de tarot más antiguas que sobreviven son las del llamado Tarot Cary-Yale (o Tarot Visconti-Modrone), que fue creado en 1442-1447 por un pintor anónimo para los Visconti-Sforza, la familia dominante de Milán. Las cartas (solo 66) están hoy día en la Biblioteca de la Universidad de Yale, EE.UU. 

Entre los primeros mazos de tarot, el más famoso fue pintado a mediados del siglo XV para celebrar la conquista del poder en Milán por Francesco Sforza y su esposa Bianca Maria Visconti, hija del duque Filippo Maria. Probablemente fue pintado por Bonifacio Bembo, pero algunas cartas fueron hechas por miniaturistas de otra escuela. 35 de las cartas están en la Biblioteca y Museo Morgan, 26 en la Accademia Carrara, 13 en la Casa Colleoni y dos, el Diablo y la Torre, se encuentran perdidas o, quizá, no se hicieron nunca. Este mazo de tarot "Visconti-Sforza", que ha sido largamente reproducido, combina los palos de espadas, bastos, oro y copas y las figuras rey, reina, caballo y sota con triunfos que reflejan la iconografía convencional de la época en un grado significativo.

Las cartas que más adelante caracterizarían al tarot parecen haberse desarrollado unos 40 años después, y se mencionan en el texto superviviente de Martiano da Tortona. Se cree que Martiano lo escribió entre 1418 y 1425, dado que en 1418 su pintor Michelino da Besozzo volvió a Milán, y Martiano murió en 1425. No se puede probar que las cartas de tarot no existieran antes de esa fecha, pero parece improbable ya que el texto de Martiano fue escrito por lo menos quince años antes que otros documentos corroborativos. 

El mazo del Tarot que Martiano describe se puede considerar un precedente del mazo que conocemos hoy en día, ya que es distinto en algunos aspectos; por ejemplo, su mazo tenía solo 16 triunfos, sus figuras no eran comparables a las cartas ordinarias de tarot (hay dioses de la mitología griega) y los palos no son los españoles comunes sino cuatro clases de pájaros.

Lo que vincula al mazo de Martiano con las cartas de tarot es que esas 16 cartas estaban consideradas como triunfos en el juego de cartas; alrededor de 25 años después, un casi contemporáneo, Jacopo Antonio Marcello, las llamó "ludus triumphorum", o «juego de los triunfos». La correspondencia en la cual Marcello usó ese término ha sido documentada y traducida en Internet.

El siguiente documento que parece confirmar la existencia de algo similar a los naipes de tarot, son dos mazos de Milán (el Brera-Brambrilla y el Tarocchi Cary-Yale) —existentes, pero incompletos— y tres documentos, todos del Tribunal de Ferrara, Italia. No es posible poner una fecha precisa a las cartas pero se estima que fueron hechas alrededor de 1440. Los tres documentos datan del 1* de enero de 1441 a julio de 1442, con la palabra "trionfi" documentada por primera vez en febrero de 1442. El documento de enero de 1441, que usa la palabra "trionfi", es visto como poco fiable; sin embargo, el hecho de que el mismo pintor, Sagramoro, fue comisionado por el mismo patrón, Leonello d'Este, como en el documento de febrero de 1442, indica que es al menos plausiblemente un ejemplo del mismo tipo. Después de 1442 pasan siete años sin ningún ejemplo de material similar, lo que permite inferir que no hubo una mayor difusión en esos años.

El juego pareció ganar en importancia en el año 1450, que fue un año de Jubileo en Roma, lo que implicaba muchas festividades y movimiento de peregrinos. Hasta ese momento todos los documentos relevantes apuntaban a un origen de las cartas de "trionfi" en la clase alta de la sociedad italiana, específicamente las cortes de Milán y Ferrara, en esa época, las cortes más exclusivas en Europa.

Esta vinculación está poco documentada. Los mazos ordinarios aparecieron por primera vez en la Europa cristiana, en los reinos de la península ibérica (muy probablemente traídos de Oriente por los árabes), ya que el Consejo de Ciento, prohibió los juegos de cartas en 1310, en Barcelona, fecha de la que data la primera evidencia documentada conocida de su existencia (ver Baraja para la discusión sobre sus orígenes).
Las primeras fuentes de Europa describen un mazo con las 56 cartas típicas (1 al 10 y cuatro figuras), como un mazo moderno sin comodines. Los palos eran cimitarras, bastones, copas y monedas. Estos dibujos evolucionaron rápidamente hacia los palos básicos latinos: espadas, bastos, copas y oros, que se usan todavía en los mazos de naipes tradicionales españoles e italianos. 

Desde 1377 en adelante puede fijarse, con alguna certeza, una difusión mayor de los naipes en Europa.

El primer mazo conocido parece haber tenido el formato ordinario de diez cartas numeradas pero con reyes como las únicas cartas de la corte y solo 16 cartas de triunfo.
El siguiente formato (mazo con cuatro palos de 14 más 22) tardó en aparecer; ya en 1457 hay referencias a mazos de "trionfi" con 70 cartas. No hay evidencia que muestre que el formato final de 78 cartas existiera antes del poema "Tarocchi" de Boiardo y el Tarot de Sola Busca.

Algunos investigadores opinan que los mazos de "trionfi" de la primera época tenían cinco palos de solo 14 cartas; los triunfos y el comodín o joker eran considerados simplemente un quinto palo con la predefinida función de "triunfos".

El diseño de los naipes es diverso, aunque existen diseños clásicos como el del tarot de Marsella (finales del siglo XVII), que ha servido como guía en la elaboración de las figuras y su simbología. Una baraja muy popular y actualmente la más reconocida es el Rider-Waite-Smith Tarot, (o Rider-Waite o simplemente Rider), ideado en 1910 por Arthur Edward Waite, elaborado por su discípula Pamela Colman Smith e impreso por la Rider Company. Cayó en el olvido hasta que la hija de Waite rescata los dibujos originales y vende los derechos a la casa U.S. Games en 1971 alcanzando entonces un enorme éxito en el mundo anglosajón y extendiéndose su uso por todo el mundo hasta competir con el modelo marsellés. Otra baraja común es el Book of Thoth Tarot, ideado entre 1938 y 1942 por el mago inglés Aleister Crowley y realizado por su discípula Frieda Harris; esta baraja se publicó en 1944, en blanco y negro, junto con "El Libro de Thoth", que explica la simbología y uso, pero fue editado con sus colores originales solo hasta 1977, en Nueva York, por US Games Systems y Samuel Weiser. Otra conocida baraja es la ideada por Fergus Hall para la película "Vive y deja morir" (1973), popular por su fantasioso diseño.

Las 78 cartas están divididas en arcanos mayores y menores. Arcano proviene del latín "arcanum", que significa misterio o secreto. 

Los 22 Arcanos Mayores se conocen como "triunfos" ("atouts", en francés; "atutti" en italiano), lo que significa "por encima de todo". El nombre de arcanos mayores es usado en la práctica esotérica; el nombre de triunfos mayores es usado en el tarot como juego, en el que solo se muestra el número romano en cada carta, más una decoración que es la misma en cada una. En las variedades para la interpretación esotérica, cada arcano representa una imagen de carácter arquetípico, con numerosos simbolismos. Aunque existen mazos que tienen el número y el nombre, los tarots más viejos no tienen ni número ni nombre para estos arcanos. Asimismo, no tienen un orden predeterminado. De cualquier modo, los nombres y números de cada carta son los siguientes: 


Los arcanos menores son 56 cartas divididas en cuatro palos, las «bajas» o «falsas» numeradas del As (1) al diez, más los «honores» o «figuras» que son los personajes de la corte: Sota, Reina, Rey y Caballero. Los palos son los mismos que en la baraja común la nobleza, simbolizada por las espadas; los campesinos, por los bastos; el clero, por las copas, y los comerciantes, por los oros, los que se cree que eran los cuatro niveles sociales durante los tiempos medievales. Algunos mazos utilizan los oros, copas, espadas y bastos, como en la baraja española y otros el pique, corazón, trébol y diamante como en la baraja francesa.

A fines del siglo XVIII y comienzos del XIX las cartas del tarot fueron asociadas al misticismo y a la magia. La tradición comenzó en 1781, cuando Antoine Court de Gébelin, un clérigo suizo y francmasón, publicó "Le Monde Primitif", un estudio especulativo sobre el simbolismo religioso antiguo y sus remanentes en el mundo moderno. De Gébelin argumentaba que el simbolismo del tarot de Marsella representaba los misterios de Isis y Thoth. Gébelin más tarde afirmó que el nombre "tarot" venía de los vocablos egipcios "tar", que significa "real", y "ro", que significa "camino", y que el tarot representaba, por lo tanto, un "camino real" a la sabiduría.

Gébelin arguyó estos y similares puntos de vista en forma dogmática; no presentó evidencias para sostener sus argumentos. Además, Gébelin escribió antes de que Champollion hubiera descifrado los jeroglíficos egipcios. Los modernos egiptólogos nada encontraron en el lenguaje egipcio que sustentara las fantasiosas etimologías de Gébelin, pero estos descubrimientos llegaron demasiado tarde. Cuando se dispuso de los auténticos textos egipcios, ya estaba firmemente establecida la identificación de las cartas del tarot con el "Libro de Thot" egipcio en la práctica ocultista.

Aunque las cartas del tarot se usaban para predecir la fortuna en Bolonia, en el siglo XVIII, fueron publicadas originalmente como un método de adivinación por Jean-Baptiste Alliette, también llamado "Etteilla", un ocultista francés que revirtió las letras de su nombre y trabajó como adivino poco antes de la revolución francesa. Etteilla diseñó el primer mazo de tarot esotérico y añadió atribuciones astrológicas y motivos "egipcios" a varias cartas, alterando muchos de los diseños marselleses y añadiendo significados adivinatorios en el texto de las cartas. Los mazos de Etteilla, aunque ahora eclipsados por los ilustrados de Smith y Waite y el mazo "Thoth" de Aleister Crowley, aún se encuentran disponible. 

Más tarde, Marie-Anne Le Normand popularizó la adivinación y la profecía durante el reinado de Napoleón I. Esto se debió en parte a la influencia que tuvo sobre Josefina de Beauharnais, la primera esposa de Napoleón. Sin embargo, ésta no usaba el tarot habitualmente.

El interés en el tarot para la adivinación a cargo de otros ocultistas llegó después, durante el auge de los herméticos, de la década de 1840, en la cual (entre otros) estuvo involucrado Víctor Hugo. La idea de las cartas como clave mística fue desarrollada posteriormente por Eliphas Lévi y pasó al mundo de habla inglesa por la Orden Hermética del Alba Dorada. Lévi, y no Etteilla, es considerado por algunos el verdadero fundador de las escuelas más contemporáneas de tarot; su "Dogme et Ritual de la Haute Magie", de 1854, introdujo una interpretación de las cartas que las relacionaba con la Cábala. Aunque Lévi aceptó las afirmaciones de Court de Gébelin sobre un origen egipcio de los símbolos de las cartas, rechazó las innovaciones de Eteilla y su mazo alterado y arregló en su lugar un sistema que relacionaba al tarot, especialmente al tarot de Marsella, con la cábala y con los cuatro elementos de la alquimia. Por otro lado, algunos significados adivinatorios de Etteilla todavía son usados por algunos lectores de tarot.

La lectura del tarot se enmarca en la creencia de que las cartas pueden ser usadas para comprender situaciones actuales y futuras de la persona consultante. Algunos dicen que las cartas son guiadas por una fuerza espiritual como guía, mientras otros creen que las cartas los ayudan en introducirse a un inconsciente colectivo. Uno de los métodos más utilizados son las tiradas, que consisten en voltear un número de cartas que previamente han sido barajadas al azar y repartidas en un cierto orden boca abajo, y darle una interpretación (valor o significado) a cada carta según la posición relativa en la que se encuentre sobre la mesa y en relación con las cartas adyacentes, y el tarotista formula su interpretación sobre su significado. Existen además programas de cómputo o aplicaciones para Facebook o teléfono móvil que replican las tiradas con cartas. Hoy en día también existen sitios webs que ofrecen lecturas del Tarot online gratuitas o de pago.

Las opciones modernas de tarot telefónico o tarot en línea de sitios web o aplicaciones móviles nos abre una nueva alternativa para poder recibir lecturas sin necesidad de salir de casa. 

Si refiriéndonos a tipos de Tarot hacemos alusión a sobre qué materias o temáticas de vida suele aplicarse el tarot, las principales son el amor, la salud y el dinero. Estas son las aristas sobre las cuáles la mayoría de las personas buscan realizar sus consultas o sesiones de tarot. Esto se justifica sabiendo que dichas aristas son la base de las incertidumbres y problemas que suelen tener las personas en su vida cotidiana.

Existen distintas configuraciones utilizadas para las tiradas:
Aun cuando es conocido el interés que el psiquiatra suizo Carl Gustav Jung mostró por diversas mancias, como el "I Ching", la astrología o el significado del tarot, no escribió obra o tratado alguno sobre este último, y se ciñó a esporádicas alusiones contenidas en sus obras completas. 

Como suele ser habitual, serán realmente sus discípulos quienes desarrollarán y amplificarán los fundamentos arquetípicos junguianos, así como su principio de sincronicidad en el tarot, de entre los cuales destaca la analista Sallie Nichols y su obra "Jung y el tarot. Un viaje arquetípico". En ella se reitera ya desde el mismo prólogo el uso que hace Nichols de la obra junguiana para desarrollar su propia propuesta del tarot, integrando psicología analítica y dicha mancia.

En virtud del principio de sincronicidad por él postulado, la psique humana es capaz de intuir el presente, el pasado y el futuro del "continuum" espacio-temporal en el momento de la tirada de cartas; dicho de otro modo, en el momento de la echada de cartas, las imágenes simbólico-arquetípicas resultantes de la tirada mantienen una relación sincronizada con acontecimientos pasados, presentes y futuros. Desde la perspectiva junguiana, las cartas del tarot se ven a su vez como representantes simbólico-arquetípicos de tipos fundamentales de personas o situaciones del inconsciente colectivo. La carta del Emperador, por ejemplo, representa posiblemente la figura del patriarca o del padre, la autoridad en el plano temporal en general, mientras que la carta del Papa representa la autoridad en el plano espiritual, la sabiduría teológica, etcétera.

De todas formas cabe señalar que la práctica psicológica no tiene nada que ver con esta práctica.

Algunas escuelas del pensamiento oculto y del estudio de los símbolos, como la Orden Hermética del Alba Dorada, consideran el tarot como un libro de texto y un artilugio mnemotécnico para sus enseñanzas. Ésta puede ser la causa de que la palabra "arcanos" (o "arcana") sirve para describir dos secciones del mazo del tarot: "arcana" es la forma plural de la palabra latina "arcanum", que significa "cerrado" o "escondido".

Cada carta tiene una asignación de significados arbitraria. Los mismos están relacionados con los grandes arquetipos universales (en este sentido, los significados pueden ser solo alusiones para dar flexibilidad en la interpretación). El conjunto de los significados de cada carta forma un universo semántico, rico en interpretación (filosófica, situacional). 

Cada carta de un tarot cuenta con una ilustración que sirve como referencia memorística, en la cual es importante la selección de iconos y colores, ya que cada color tiene un valor simbólico (por ejemplo, azul-espiritualidad). Existen distintas ilustraciones dependiendo del tipo de tarot y de baraja que se está usando.

Ligado al número de cartas, hay toda una tradición acerca del significado de cada número.

La tradición divide el tarot en: espadas (elemento aire, pensamiento e inteligencia), bastos (elemento fuego, vida, pasiones), copas (elemento agua,
amor y sentimientos) y oros o pentáculos (elemento tierra, naturaleza, materia, lo económico).

Aunque este elemento no es forzoso, es importante para reutilizar el aprendizaje de otros tarots.

Desde el punto de vista científico, no está comprobado que sea posible conocer los hechos futuros a través de método adivinatorio alguno. Tampoco está comprobado que sea posible describir una situación actual sin disponer de información.




</doc>
<doc id="2834" url="https://es.wikipedia.org/wiki?curid=2834" title="Honda (arma)">
Honda (arma)

La honda es una de las armas más antiguas de la humanidad. Consiste básicamente en dos cuerdas o correas en cuyos extremos se sujeta un receptáculo flexible desde el que se dispara un proyectil. Agarrado el artilugio por los otros dos extremos opuestos, se le da varias vueltas de manera que el proyectil adquiera velocidad y después se suelta una de las cuerdas para liberarlo, alcanzando este gran distancia y poder de impacto. Los materiales empleados en su construcción son muy diversos, tradicionalmente cuero, fibras textiles, tendones, crin, etc. Los proyectiles pueden ser piedras naturales redondeadas, o labradas con bastante precisión, arcilla cocida o secada al sol, plomo moldeado, etc.

En el contexto de usos de entretenimiento y juegos infantiles, se emplea también el término «honda» para designar a lo que en varios países de Latinoamérica se llama resortera y gomera, y en España, tirachinas. Sin embargo, este último es un instrumento de reciente aparición, ligada al uso del caucho.

El origen de la honda se remonta a los tiempos prehistóricos, quizás al final del Paleolítico, en el que se usaría exclusivamente como arma de caza. Pero las evidencias arqueológicas de su existencia corresponden ya a la época del Neolítico, cuando aparecen en el área de Oriente Próximo grandes cantidades de proyectiles de arcilla cocida, asociados a usos bélicos.

Como herramienta asociada al pastoreo la honda se usaría desde el Neolítico hasta nuestros días.

En la antigüedad clásica, la honda fue usada por griegos, cartagineses, romanos, etc.

Fueron famosos en todo el orbe antiguo los honderos baleares, que eran contratados como mercenarios por los diferentes ejércitos de la Antigüedad. Eran entrenados desde la infancia en la destreza con la honda y llevaban tres tipos de distinta longitud, según la distancia de lanzamiento. Se decía que su precisión y potencia no tenían parangón.

El uso de proyectiles de plomo, inventado por los griegos, haría de la honda un arma temible dada su mayor potencia de impacto y alcance; a esto se unía el pequeño tamaño de los proyectiles, que eran capaces de penetrar en el cuerpo a la manera de una bala, además de hacerlos invisibles por el aire. Como arma de guerra, la honda se utilizaría todavía durante toda la Edad Media, llegando a convivir incluso con los primitivos cañones.

La honda también se usó en las Américas para la caza y la guerra. Un uso notable fue en la resistencia Inca contra los conquistadores españoles. Estas hondas, denominadas "huaraca", suelen tener una "cuna" que es larga y delgada y presenta una hendidura relativamente larga. Las hondas andinas fueron construidas con colores contrastantes de lana; las trenzas complejas y la mano de obra fina pueden dar como resultado hermosos patrones. Estas hondas aparentemente eran muy poderosas; en el libro "1491: Nuevas revelaciones de las Américas Antes de Colón", el escritor Charles C. Mann citó a un conquistador diciendo que una honda inca "podría romper una espada en dos pedazos" y "matar a un caballo". Algunas hondas tenían una longitud de hasta 2,2 metros de largo y pesaban unos 410 gramos.

También se hicieron hondas ceremoniales; estos eran grandes, no funcionales y generalmente carecían de una hendidura. Hasta el día de hoy, las hondas ceremoniales se utilizan en partes de los Andes como accesorios en bailes y simulacros de batallas. También son utilizados por los pastores de llamas; los animales se alejarán del sonido de un aterrizaje de piedra. Las piedras no se cuelgan para golpear a los animales, sino para convencerlos de que se muevan en la dirección deseada.



</doc>
<doc id="2837" url="https://es.wikipedia.org/wiki?curid=2837" title="Tomaso Albinoni">
Tomaso Albinoni

Tomaso Giovanni Albinoni (Venecia, 8 de junio de 1671-ibídem, 17 de enero de 1751) fue un compositor italiano del Barroco. En su época fue célebre como compositor de ópera, pero actualmente es conocido sobre todo por su música instrumental, parte de la cual se graba con regularidad. El "Adagio en sol menor" constituye su obra más difundida pese a que, en realidad, se cree que se trata de una obra apócrifa compuesta en el siglo XX por el musicólogo y especialista en su obra Remo Giazotto.

Era hijo de Antonio Albinoni (1634-1709), un rico comerciante de papel en Venecia. Estudió violín y canto. Se sabe relativamente poco de su vida, si se tiene en cuenta su importancia contemporánea como compositor y el hecho de que vivió durante un período relativamente bien documentado. En 1694 dedicó su Opus 1 a su compatriota veneciano cardenal Pietro Ottoboni (sobrino-nieto del papa Alejandro VIII). Ottoboni era un mecenas de otros compositores en Roma, como Arcangelo Corelli. Es probable que Albinoni fuera contratado en 1700 como violinista por Fernando Carlo, duque de Mantua, a quien le dedicó su colección de piezas instrumentales Opus 2. En 1701 escribió sus muy populares suites Opus 3, y dedicó tal colección al Gran Duque Fernando III de Toscana.

En 1705 se casó. Antonino Biffi, el "maestro di cappella" de San Marcos de Venecia fue testigo de su boda, y evidentemente era amigo de Albinoni. Sin embargo, no parece que Albinoni tuviera ninguna otra relación con ese establecimiento que tanto destacaba musicalmente en Venecia. Logró su temprana fama como compositor de ópera en muchas ciudades de Italia, incluyendo Venecia, Génova, Bolonia, Mantua, Udine, Piacenza y Nápoles. Durante esta época compuso abundante música instrumental: antes de 1705, escribió sobre todo sonatas en trío y conciertos para violín, pero entre esa fecha y 1719 se dedicó más a sonatas para solo y conciertos para oboe.

A diferencia de la mayor parte de los compositores de su época, parece que nunca buscó un puesto en la Iglesia o en la corte, pero lo cierto es que era un hombre independiente con recursos propios. En 1722, Maximiliano II Manuel de Baviera, a quien Albinoni había dedicado un conjunto de doce conciertos, le invitó a dirigir dos de sus óperas en Múnich.

Alrededor de 1740, una colección de sonatas para violín se publicó en Francia como una obra póstuma, y los eruditos supusieron durante mucho tiempo que ello significaba que Albinoni había muerto para entonces. Sin embargo, parece que siguió viviendo en Venecia sin que haya llegado hasta nosotros ninguna composición en este último período de su vida. Un archivo de la parroquia de San Bernabé indica que Tomaso Albinoni falleció en 1751 «a la edad de 79 años», de diabetes.

Escribió una cincuentena de óperas, de las cuales 28 se representaron en Venecia entre 1723 y 1740, pero actualmente es más conocido por su música instrumental, especialmente sus conciertos para oboe.

La cantidad de obras escénicas que ha compuesto Albinoni no se puede determinar con exactitud. Sólo siete han sido completamente conservadas:


De unas 17 óperas, al menos algunas arias han sobrevivido; de otras 32 sólo el libreto (impreso); de una serie de intermezzi, apenas el título ("Malsazio e Fiammetta", 1726). En el libreto de "Candalide" (1734) Albinoni se refiere a esta ópera como su ochenta; si esta afirmación no es una exageración, otras 23 obras escénicas deben considerarse como completamente perdidas.

Su música instrumental atrajo la atención de Johann Sebastian Bach, quien escribió al menos dos fugas sobre temas de Albinoni y utilizó constantemente sus bajos como ejercicios de armonía para sus alumnos.

Parte de la obra de Albinoni se perdió durante el bombardeo de Dresde durante la Segunda Guerra Mundial, con la destrucción de la Biblioteca estatal de Dresde, así que se sabe poco de su vida y su música posterior a mediados de los años 1720.

Su fama se incrementó en gran medida con la publicación del conocido como Adagio de Albinoni, compuesto en 1945 por el musicólogo italiano Remo Giazotto. Publicado por primera vez en 1958 por la editorial Casa Ricordi, el editor lanzó como argumento de venta que el autor se había basado en unos fragmentos de un movimiento lento de una sonata a trío para cuerdas y órgano de Tomaso Albinoni presumiblemente encontrados en las ruinas de la Biblioteca de Dresde tras los bombardeos de la ciudad acaecidos en la Segunda Guerra Mundial. La obra fue compuesta por Giazotto basándose supuestamente en el fragmento que pudo rescatar de la partitura original, en el que apenas se apreciaba el bajo continuo y seis compases de melodía. Sin embargo, una prueba seria de la existencia de tales fragmentos no ha sido nunca encontrada; por el contrario la «Staatsbibliothek Dresden» ha desmentido formalmente tenerlas en su colección de partituras.




El "Adagio en sol menor" de Remo Giazotto, atribuido a Albinoni, ha logrado tal fama que se transcribe normalmente para otros instrumentos y se usa en la cultura popular, por ejemplo, en la banda sonora de películas como "Gallipoli", de 1981, que se ambienta en 1915–1916 durante la batalla del mismo nombre de la Primera Guerra Mundial, y programas de televisión y en anuncios.

En el álbum del grupo The Doors, "An American Prayer", Jim Morrison recita poesía con lo que parece ser un arreglo musical adaptado del "Adagio en sol menor" tocado en el fondo de «Feast of Friends». La obra de Yngwie Malmsteen "Icarus Dream Suite Op. 4" se inspira y se basa principalmente en el Adagio. Wolf Hoffmann la versionó en un estilo metal neoclásico.

Entre otros cantantes actuales que han utilizado el "Adagio" como base para sus baladas se encuentran: Camilo Sesto, Lara Fabian, Ricardo Montaner, Sarah Brightman yRosa López.




</doc>
<doc id="2839" url="https://es.wikipedia.org/wiki?curid=2839" title="Topología">
Topología

La topología (del griego τόπος, 'lugar', y λόγος, 'estudio') es la rama de las matemáticas dedicada al estudio de aquellas propiedades de los cuerpos geométricos que permanecen inalteradas por transformaciones continuas. Es una disciplina que estudia las propiedades de los espacios topológicos y las funciones continuas. La topología se interesa por conceptos como "proximidad", "número de agujeros", el tipo de "consistencia" (o "textura") que presenta un objeto, comparar objetos y clasificar múltiples atributos donde destacan conectividad, compacidad, metricidad o metrizabilidad, entre otros.

Los matemáticos usan la palabra "topología" con dos sentidos: informalmente es el sentido arriba especificado, y de manera formal es la referencia a una cierta familia de subconjuntos de un conjunto dado, familia que cumple unas reglas sobre la unión y la intersección —este segundo sentido puede verse desarrollado en el artículo espacio topológico—.

Coloquialmente, se presenta a la topología como la «geometría de la página de goma (chicle)». Esto hace referencia a que, en la geometría euclídea, dos objetos serán equivalentes mientras podamos transformar uno en otro mediante isometrías (rotaciones, traslaciones, reflexiones, etc.), es decir, mediante transformaciones que conservan las medidas de ángulo, área, longitud, volumen y otras.

En topología, dos objetos son equivalentes en un sentido mucho más amplio. Han de tener el mismo número de "trozos", "huecos", "intersecciones", etc. En topología está permitido doblar, estirar, encoger, retorcer, etc., los objetos, pero siempre que se haga sin romper ni separar lo que estaba unido, ni pegar lo que estaba separado. Por ejemplo, un triángulo es topológicamente lo mismo que una circunferencia, ya que podemos transformar uno en otra de forma continua, sin romper ni pegar. Pero una circunferencia no es lo mismo que un segmento, ya que habría que partirla (o pegarla) por algún punto.

Esta es la razón de que se la llame la «geometría de la página de goma», porque es como si estuviéramos estudiando geometría sobre un papel de goma que pudiera contraerse, estirarse, etc.

Un chiste habitual entre los topólogos (los matemáticos que se dedican a la topología) es que «un topólogo es una persona incapaz de distinguir una taza de una rosquilla». Pero esta visión, aunque muy intuitiva e ingeniosa, es sesgada y parcial. Por un lado, puede llevar a pensar que la topología trata solo de objetos y conceptos geométricos, siendo más bien al contrario, es la geometría la que trata con un cierto tipo de objetos topológicos. Por otro lado, en muchos casos es imposible dar una imagen o interpretación intuitiva de problemas topológicos o incluso de algunos conceptos. Es frecuente entre los estudiantes primerizos escuchar que «no entienden la topología» y que no les gusta esa rama; generalmente se debe a que se mantienen en esta actitud gráfica. Por último, la topología se nutre también en buena medida de conceptos cuya inspiración se encuentra en el análisis matemático. Se puede decir que casi la totalidad de los conceptos e ideas de esta rama son conceptos e ideas topológicas.

Observemos un plano del metro de Madrid. En él están representadas las estaciones y las líneas de metro que las unen, pero no es "geométricamente" exacto. La curvatura de las líneas de metro no coincide, ni su longitud a escala, ni la posición relativa de las estaciones... Pero aun así es un plano perfectamente útil. Sin embargo, este plano es exacto en cierto sentido, pues representa fielmente cierto tipo de información, la única que necesitamos para decidir nuestro camino por la red de metro: "información topológica".

Históricamente, las primeras ideas topológicas conciernen al concepto de límite y al de completitud de un espacio métrico, y se manifestaron principalmente en la crisis de los inconmesurables de los pitagóricos, ante la aparición de números reales no racionales. El primer acercamiento concreto al concepto de límite y también al de integral aparece en el método de exhaución de Arquímedes. La aparición del análisis matemático en el siglo XVII puso en evidencia la necesidad de formalizar los conceptos de proximidad y continuidad, y la incapacidad de la geometría para tratar este tema. Fue precisamente la fundamentación del cálculo infinitesimal, así como los intentos de formalizar el concepto de variedad en Geometría los que impulsaron la aparición de la topología a finales del siglo XIX y principios del XX.

Se suele fechar el origen de la topología con la resolución por parte de Euler del problema de los puentes de Königsberg, en 1735. Ciertamente, la resolución de Euler del problema utiliza una forma de pensar totalmente topológica, y la solución del problema nos lleva a la característica de Euler, el primer invariante de la topología algebraica, pero sería muy arriesgado y arbitrario fechar en ese momento la aparición de la topología. La situación es exactamente análoga a la del cálculo del área de la elipse por Arquímedes.

El término "topología" fue usado por primera vez por Johann Benedict Listing en 1836 en una carta a su antiguo profesor de la escuela primaria, Müller, y posteriormente en su libro "Vorstudien zur Topologie" ('Estudios previos a la topología'), publicado en 1847. Anteriormente se la denominaba "analysis situs". Maurice Fréchet introdujo el concepto de espacio métrico en 1906.

En el artículo Glosario de topología se encuentra una colección de términos topológicos con su significado. Aquí y ahora nos limitaremos a dar algunas nociones básicas.

Como hemos dicho, el concepto fundamental de la topología es la "relación de proximidad", que puede parecer ambigua y subjetiva. El gran logro de la topología es dar una formulación precisa, objetiva y útil de este concepto. Para ello tomamos un conjunto de referencia formula_1, que será el ambiente en el que nos moveremos, y al que llamaremos espacio. Tomaremos un elemento cualquiera formula_2 de formula_1. A los elementos del espacio se les llama puntos, así que formula_2 será llamado punto, independientemente de que formula_2 sea una función, un vector, un conjunto, un ideal maximal en un anillo conmutativo y unitario... Un subconjunto formula_6 de formula_1 será un entorno de formula_2 si formula_6 incluye un conjunto abierto formula_10 de manera que formula_2 es elemento de formula_10. ¿Qué entenderemos por conjunto abierto? Aquí está el quid de la cuestión: una colección formula_13 de subconjuntos de formula_1 se dirá que es una topología sobre formula_1 si formula_1 es uno de los elementos de esa colección, si formula_17 es un elemento de la colección, si la unión de elementos de la colección da como resultado un elemento de la colección y si la intersección finita de elementos de la colección también es un elemento de la colección. A los elementos de la colección formula_13 se les denomina abiertos de la topología formula_13, y al par formula_20 se le denomina espacio topológico.

Las condiciones para que formula_13 sea topología sobre formula_1 son entonces estas:

Puede parecer extraño que de una definición tan altamente formal y conjuntista se obtenga una formulación precisa del concepto de proximidad. Lo primero que se observa es que sobre un mismo espacio formula_1 se pueden definir distintas topologías, generando entonces distintos espacios topológicos. Por otra parte, precisamente la manera en que quede determinada una topología sobre un conjunto (es decir, la elección del criterio que nos permita decidir si un conjunto dado es o no abierto) es lo que va a dar carácter "visualizable" o no a ese espacio topológico.

Una de las maneras más sencillas de determinar una topología es mediante una distancia o métrica, método que solo es aplicable en algunos casos (si bien es cierto que muchos de los casos más intersantes de topologías en la Geometría y del Análisis Matemático pueden determinarse mediante alguna distancia). Una distancia sobre un conjunto formula_1 es una aplicación formula_28 que verifica las siguientes propiedades:

cualesquiera que sean formula_34.

Si tenemos definida una distancia sobre formula_1, diremos que la pareja
es un espacio métrico. Dado un espacio métrico formula_37, queda determinada una topología sobre formula_1 en la que los conjuntos abiertos son los subconjuntos formula_10 de formula_1 tales que cualquiera que sea el punto formula_2 de formula_10 existe un número formula_43 de tal manera que el conjunto formula_44 está totalmente incluido en formula_10. Al conjunto formula_44 se le denomina bola abierta de centro formula_2 y radio formula_48, y será precisamente un entorno del punto formula_2.

Como se ha apuntado antes, por desgracia no toda topología proviene de una distancia, es decir, existen espacios topológicos que no son espacios métricos. Cuando un espacio topológico es además espacio métrico (esto es, cuando dada una topología sobre un conjunto, puede definirse en ese conjunto una distancia de manera que la topología generada por la distancia coincida con la topología dada) se dice que el espacio topológico es metrizable. Un problema clásico en topología es el de determinar qué condiciones debe satisfacer un espacio topológico para que sea metrizable.

Se suelen considerar principalmente tres ramas:

Además de estas tres ramas, que podríamos decir propiamente topológicas, la implicación en mayor o menor medida en otras disciplinas matemáticas hacen que muchos consideren parte de la topología al análisis funcional, la teoría de la medida, la teoría de nudos (parte de la topología de dimensiones baja), la teoría de grupos topológicos, etc. Es fundamental su contribución a la teoría de grafos, análisis matemático, ecuaciones diferenciales, ecuaciones funcionales, variable compleja, geometría diferencial, geometría algebraica, álgebra conmutativa, estadística, teoría del caos, geometría fractal... Incluso tiene aplicaciones directas en biología, sociología, etc.

Constituye la base de los estudios en topología. En ella se desarrollan tópicos como lo que es un espacio topológico o los entornos de un punto.

Sea formula_1 un conjunto cualquiera y formula_51 el conjunto de sus partes. Una topología sobre formula_1 es un conjunto formula_53 que cumpla que formula_54, formula_55, si formula_56 entonces
formula_57, y que si formula_58 entonces formula_59. A los elementos de formula_13 se les denomina conjuntos abiertos. Al par formula_20 se le denomina espacio topológico. A los elementos de formula_1 se les suele denominar puntos.

Nótese que desde un primer momento hemos especificado que el conjunto formula_1 es cualquiera, no necesariamente un conjunto de naturaleza geométrica. La denominación de espacio (topológico) y
de punto se mantiene aun cuando formula_1 sea un conjunto de números, de funciones, de ecuaciones diferenciales, de figuras geométricas, de vectores, de conjuntos...

Como puede observarse, la definición es muy formal y general, y lo primero que se observa es que sobre un mismo conjunto pueden darse multitud de topologías distintas. Así es. Pero de momento,
los conceptos de conjunto abierto en formula_65 o en formula_66 o formula_67 cumplen las condiciones exigibles a una topología. Es precisamente el comprobar que otras familias de conjuntos en otros conjuntos de naturaleza no geométrica que comparten estas mismas propiedades (como en el conjunto de soluciones de una ecuación diferencial, o el conjunto de los ceros de los polinomios con coeficientes en los ideales en un anillo conmutativo, por ejemplo) lo que motiva esta definición. Así podremos aplicar a estos conjuntos las mismas (o parecidas) técnicas topológicas que aplicamos a los abiertos del plano, por ejemplo. La situación es análoga a la que se da en Álgebra Lineal cuando se pasa de trabajar en formula_66 o formula_67 a trabajar en espacios vectoriales arbitrarios.

En lo que sigue, formula_20 representará siempre un espacio topológico.

Ligado al concepto de conjunto abierto está el de conjunto cerrado. Un conjunto formula_71 se dice que es cerrado si su complementario formula_72 es un conjunto
abierto. Es importante observar que un conjunto que no es abierto no necesariamente ha de ser
cerrado, y un conjunto que no sea cerrado no necesariamente ha de ser abierto. Así, existen
conjuntos que son abiertos y cerrados a la vez, como formula_17, y pueden existir conjuntos que no sean ni abiertos ni cerrados.

Es inmediato comprobar que la intersección de cerrados es un conjunto cerrado, que la unión de
una cantidad finita de conjuntos cerrados es un conjunto cerrado, y que tanto formula_1 como formula_17 son conjuntos cerrados.

Si formula_76, el conjunto formula_77 es una topología para formula_78. Se dirá entonces que el espacio formula_79 es subespacio topológico del formula_20.

La noción de subespacio topológico se presenta de manera natural, y es el concepto análogo al de
subgrupo en Teoría de Grupos o al de subespacio vectorial en Álgebra Lineal.

Una propiedad relativa a espacios topológicos se dice que es hereditaria cuando si un
espacio la tiene, entonces también la tiene cualquiera de sus subespacios.

Una familia formula_81 se dice que es base (de la topología formula_13) si para cualquiera que sea el formula_83 existe un conjunto formula_84 de manera que formula_85.

No siempre es cómodo trabajar con una topología. A veces resulta más complicado establecer una
topología que una base de topología (como en espacios métricos). En cualquier caso, una base
es una manera muy cómoda de establecer una topología. Aún más sencillo es establecer una subbase,
que es una familia de conjuntos para la que el conjunto de sus intersecciones finitas forma una
base de topología. Uno de los casos más importantes de topología, la de los espacios métricos,
viene dado por una base, la del conjunto de bolas abiertas del espacio.

Un espacio topológico se dice que cumple el Segundo Axioma de Numerabilidad (IIAN) si existe alguna base de su topología que tenga cardinalidad numerable.

Sea formula_86 un conjunto cualquiera y sea formula_87 un punto arbitrario. Se dice que formula_88 es entorno de formula_2 si existe un conjunto abierto formula_90 de manera que formula_91. Todo conjunto abierto es entorno de todos sus puntos. Al conjunto de todos los entornos de un punto formula_2 se le denomina sistema de entornos de formula_2.

Obsérvese que no se ha exigido que un entorno sea un conjunto abierto. Los entornos abiertos son
un tipo de entornos muy útiles (sobre todo en Geometría y Análisis) y muy usados, tanto que en
muchas ocasiones se omite el calificativo "abierto". Esto es un abuso de lenguaje y debe
evitarse.

Una colección formula_94 de entornos de un mismo punto x se dice que es una base de entornos (o base local) de formula_2 si dado cualquier entorno formula_6 de formula_2 existe un formula_98 de manera que formula_99.

Se dice que un espacio topológico cumple el Primer Axioma de Numerabilidad (IAN) si cada punto del espacio tiene alguna base local de cardinal numerable.

Ahora podemos establecer una serie de definiciones de gran importancia, pues serán las piezas
básicas del estudio de la topología y constituirán la materia prima de los conceptos posteriores.

Un punto formula_100 se dirá que es un punto interior de formula_88 si existe un entorno formula_102 de formula_2 tal que formula_104. Así, el conjunto de los puntos interiores a formula_88 es un conjunto abierto, denominado Interior de A, denotado por Int (A) o también como formula_106. Es el mayor conjunto abierto incluido en A.

Un punto formula_107 se dirá que es un punto exterior a formula_88 si existe un entorno formula_102 de formula_110 tal que formula_102 ⊂ formula_112. Asimismo, el conjunto de los puntos exteriores a formula_88 es otro conjunto abierto, denominado Exterior de A y denotado por Ext (A).

Un punto formula_114 se dice que es un punto frontera de formula_88 si todo entorno formula_6 de formula_117 es tal que formula_118 y formula_119. Al conjunto de los punto frontera de formula_88 se le denomina Frontera de A y se denota por Fr(A). En otras palabras, todo entorno con centro en formula_117 tendrá elementos pertenecientes al conjunto formula_88 y otros elementos fuera del conjunto formula_88. La frontera de formula_88 es un conjunto cerrado.

Un punto formula_100 se dice que es un punto de adherencia de formula_88 si todo entorno formula_6 de formula_2 es tal que formula_129. Se hace pues evidente que todo punto interior y todo punto frontera es punto de adherencia. Al conjunto de los puntos de adherencia del conjunto formula_88 se le denomina adherencia o clausura de formula_88, y se denota por formula_132 o por formula_133. La
clausura de un conjunto formula_88 es un conjunto cerrado, y es el menor conjunto cerrado que contiene al conjunto.

Un punto formula_100 se dice que es un punto de acumulación de formula_88 si todo entorno formula_6 de formula_2 es tal que formula_139. Al conjunto de los puntos de acumulación de un conjunto se le denomina acumulación del conjunto, o conjunto derivado, y se le denota por formula_140 o por formula_141.

Un punto formula_100 se dice que es un punto de formula_143-acumulación de formula_88 si todo entorno formula_6 de formula_2 es tal que formula_147 es un conjunto infinito. Al conjunto de los puntos de
formula_148-acumulación de un conjunto se le denomina formula_148-acumulación del conjunto, o conjunto formula_148-derivado, y se le denota por formula_151 o por formula_152. Todo punto de formula_148-acumulación es punto de acumulación, y todo punto de acumulación es punto
de adherencia del mismo conjunto.

Un punto formula_100 se dice que es un punto aislado de formula_88 si existe algún entorno perforado formula_6 de formula_2 (es decir, un conjunto formula_158 de manera que formula_159 es un entorno de formula_2) de manera que formula_161. Al conjunto de los puntos aislados de formula_88 se le denomina conjunto de los puntos aislados de formula_88, y se le denota por formula_164. Todo punto aislado es punto frontera y también es punto de adherencia del mismo conjunto.

En topología son de una importancia capital los conjuntos interior y clausura de un conjunto. Su importancia radica en ser, respectivamente, el mayor abierto contenido en el conjunto y el menor cerrado que contiene al conjunto. El interior puede obtenerse también como la unión de todos los abiertos contenidos en el conjunto, y la clausura como la intersección de todos los cerrados que contienen al conjunto. Sin tanta importancia en topología pero de mucha en otras áreas de la Matemática son los conjuntos de acumulación, frontera y de los puntos aislados de un conjunto.

La idea de la convergencia es la de "aproximar" un objeto por otro, es decir, sustituir un objeto por otro que está próximo a él. Evidentemente, al hacerlo así se está cometiendo un error, error que en general dependerá de lo próximo que se encuentre el objeto sustituido del objeto sustituto. Para hacer esta sustitución de una manera sistemática, de forma que el error pueda ser elegido arbitrariamente pequeño, aparecen distintos tipos de conjuntos. Se obtiene así un proceso de sucesivas aproximaciones que, si todo va bien, terminarían llevándonos al objeto, aunque fuese después de un número infinito de aproximaciones. El más sencillo de estos conjuntos es una sucesión, es decir, una colección infinita (numerable) y ordenada de objetos, aunque con el mismo carácter de orden hay otros conjuntos que reflejan mejor el concepto de convergencia.

Es importante observar que la topología no trabaja con errores ni con aproximaciones. Eso entra en el ámbito del Análisis Numérico e incluso del Análisis Matemático. La topología lo que hace en este problema es aportar las herramientas básicas y los conceptos teóricos para afrontar correctamente el problema, siempre desde un punto de vista conceptual y cualitativo. Estudia qué es lo que debe entenderse cuando decimos que un conjunto (como puede ser una sucesión) se acerca a un objeto (que puede ser un punto, un conjunto, etcétera).

Una sucesión es una aplicación en un conjunto cuyo dominio es el conjunto de los números naturales. En particular, una sucesión en un espacio topológico formula_165 es una aplicación formula_166.

Una sucesión es el caso más sencillo de aplicación de dominio infinito.

Se dice que formula_100 es un punto límite de la sucesión formula_168, o bien que formula_168 converge al punto formula_170, si se cumple que, cualquiera que sea el entorno formula_171 de formula_170 existe un número natural formula_173 de tal manera que si formula_174 es otro número natural mayor o igual que formula_173 (o sea, formula_176) entonces se cumple que formula_177.

Hay que hacer dos observaciones sobre esto:

Un punto formula_100 es punto de aglomeración de la sucesión formula_168 si cualquiera que sea el entorno formula_6 de formula_2 se cumple que el conjunto formula_186 es infinito. Todo punto límite es punto de aglomeración, pero el recíproco no es cierto. Por ejemplo, los límites de oscilación de una sucesión no convergente de números reales (como por ejemplo la sucesión formula_187) son puntos de aglomeración, pero no son puntos límites (no existe límite para dicha sucesión, mientras que 1 y -1 son puntos de acumulación).

Otro concepto totalmente fundamental estudiado en esta rama es el de aplicación continua. Una aplicación formula_188 entre dos espacios topológicos se dice que es continua si dado cualquier conjunto formula_189 abierto en formula_190, el conjunto formula_191 es un conjunto abierto en formula_192.

Con la misma notación, si formula_100, diremos que formula_194 es continua en formula_195 cuando se obtiene que formula_196 es un entorno de formula_170, cualquiera que sea el entorno formula_198 de formula_199.

Es inmediato entonces comprobar que formula_194 es continua cuando y solo cuando es continua en formula_100, cualquiera que sea éste, es decir, cuando y solo cuando sea continua en cada uno de los puntos de su dominio.

Informalmente hablando, una aplicación es continua si transforma puntos que están cerca en puntos que están cerca, es decir, si respeta la "relación de cercanía". Esto además quiere decir que una función continua no "rompe" los que está unido y no "pega" lo que está separado.

Un conjunto se dice que es conexo si no puede expresarse como unión de dos abiertos disjuntos no vacíos.

Un conjunto formula_202 se dice que es conexo por caminos si todo par de puntos puede unirse mediante un camino, esto es, formula_203 continua de tal manera que formula_204 y formula_205. Todo conjunto conexo por caminos es conexo, pero no todo conjunto conexo es conexo por caminos (ver, por ejemplo, el seno del topólogo).

Estos conjuntos están "hechos de una pieza" (los conexos) o "hechos de manera que no tienen piezas totalmente sueltas" (los conexos por caminos). Naturalmente esto es solo una manera de interpretarlos. Las piezas de un conjunto (los mayores subconjuntos conexos que contiene el conjunto) se denominan "componentes conexas". Por ejemplo, un puñado de arena sería un conjunto en el que las componentes conexas son cada granito de arena. Un espejo roto sería un conjunto en el que cada trozo de espejo es una componente conexa. Una bola de hierro es un conjunto con una sola componente conexa, es decir, un conjunto conexo. Una rejilla también es un conjunto conexo, formado por una sola componente conexa.

Existe otra noción de conexión, la conexión por arcos o arco conexión ligeramente más restrictiva que la conexión por caminos. Se exige que el camino sea un homeomorfismo sobre su imagen. Aun así, la conexión por arcos y por caminos coinciden sobre los espacios de Hausdorff.

Los conjuntos compactos son un tipo de conjunto mucho más difíciles de definir. Un espacio es compacto si para todo recubrimiento por abiertos (familia de abiertos cuya unión contiene al espacio total X) existe subrecubrimiento finito (familia finita de abiertos, formada solo por conjuntos de la familia anterior, cuya unión contiene a X).

En un espacio métrico, un conjunto compacto cumple dos condiciones: es "cerrado", es decir contiene a todos sus puntos frontera; y es "acotado", es decir es posible trazar una bola que lo contenga, aunque la reciproca no es necesariamente cierta. Es decir, pueden existir conjuntos cerrados y acotados que no sean compactos. La compacidad es una propiedad muy importante en topología, así como en Geometría y en Análisis Matemático.

En cualquier espacio topológico, un conjunto cerrado dentro de un compacto, siempre es compacto. Además, en un espacio topológico de Hausdorff, un compacto siempre es cerrado.

Una topología sobre un conjunto es metrizable si es posible encontrar una distancia de forma que los abiertos para esa distancia sean exactamente los abiertos de la topología de partida. La metrizabilidad es también una propiedad muy deseable en un espacio topológico, pues nos permite dar una caracterización muy sencilla de los abiertos de la topología, además de implicar otras ciertas propiedades.

Las propiedades de separación son ciertas propiedades, cada una un grado más restrictiva que la anterior, que nos indican la "resolución" o "finura del grano" de una topología. Por ejemplo, la propiedad de separación T2 significa que para dos puntos distintos siempre pueden encontrarse entornos disjuntos (es decir que no se cortan).

Un conjunto es denso en el espacio si está "cerca de todos los puntos" de ese espacio. De manera más precisa, un conjunto es denso si su clausura topológica es todo el espacio. Equivalentemente, un conjunto es denso si su intersección con cualquier abierto no vacío del espacio es también no vacía. Un conjunto se dice que es separable si tiene algún subconjunto denso y numerable.

La topología producto nos proporciona una manera de dotar de una topología al producto cartesiano de varios espacios topológicos, de tal manera que se conserven buenas propiedades, en particular que las proyecciones sobre cada factor sean aplicaciones continuas y abiertas. La topología cociente nos proporciona una manera de dotar de una topología al cociente (espacio de clases) de un espacio por una relación de equivalencia, de manera que tenga el mayor número posible de conjuntos abiertos y sin embargo la proyección sea continua (es decir la imagen recíproca de cada abierto sea un abierto).

La topología algebraica estudia ciertas propiedades relacionadas con la conexión de un espacio, propiedades que podríamos describir como la "porosidad" de un espacio, la cantidad de boquetes que presenta. Para ello se vale de instrumentos algebraicos, fundamentalmente la teoría de grupos y el álgebra homológica, hasta tal punto que su desarrollo es totalmente algebraico.

En la topología algebraica se consideran una gran diversidad de problemas incluidos en la teoría de nudos por ejemplo, o en la teoría de homotopías y la teoría de homología.

Para comprender sucintamente estas cuestiones, volvamos a los ejemplos de conjuntos conexos. Según hemos dicho, una rejilla, una bola de hierro o una esponja son conjuntos conexos. Sin embargo todos entendemos que parece que no tienen el mismo «grados de conexión», por expresarlo de alguna manera. Mientras que una bola de hierro es maciza, una esponja y una rejilla tienen agujeros, e incluso parece claro que entre estos hay también una cierta diferencia. La homotopía y la homología tratan estas cuestiones.




</doc>
<doc id="2840" url="https://es.wikipedia.org/wiki?curid=2840" title="Testigos de Jehová">
Testigos de Jehová

Los Testigos de Jehová (en inglés: Jehovah's Witnesses) son una denominación cristiana milenarista y restauracionista con creencias antitrinitaristas distintas de las vertientes principales del cristianismo. Se consideran a sí mismos una restitución del cristianismo primitivo, creencia que se basa en su propio entendimiento de la Biblia, preferentemente de su "Traducción del Nuevo Mundo de las Santas Escrituras", y que tiene, según ellos, como propósito santificar el nombre de Jehová.

Su entidad jurídica, la Watch Tower Bible and Tract Society of Pennsylvania, fue fundada en 1881 por Charles Taze Russell, quien la presidió hasta su muerte, en 1916. Según sus publicaciones oficiales, en la actualidad es dirigida por un Cuerpo Gobernante desde su sede principal en Warwick (Nueva York). Este cuerpo gobernante se encarga de establecer la doctrina oficial de la congregación mundial.

Según sus propios datos, en 2019, sus publicaciones se distribuyeron en 240 países y territorios; contaban con 8.7 millones de publicadores activos y la asistencia anual a la «Conmemoración de la cena de Jesús» fue de 20.919.041 personas.

Los Testigos de Jehová surgieron a partir de un grupo de cristianos restauracionistas, milenaristas y antitrinitarios pertenecientes al movimiento Estudiantes de la Biblia, el cual había sido organizado por el estadounidense Charles Taze Russell en los años 1870 en el condado de Allegheny, Pensilvania. En julio de 1879 este movimiento publicó el primer número de la revista "Zion's Watch Tower and Herald of Christ's Presence", la que se continuó desarrollando y actualmente se ha constituido como la publicación más conocida de los Testigos de Jehová, bajo el nombre en castellano de "La Atalaya".

En 1881, Russell y un grupo de amigos del movimiento fundaron como entidad legal la Zion's Watch Tower Tract Society. Russell fue nombrado presidente en 1884, y la sociedad cambió más tarde su nombre por el de Watch Tower Bible and Tract Society of Pennsylvania, la que corresponde a la principal y más antigua entidad jurídica utilizada por los Testigos de Jehová. Desde entonces la sociedad comenzó a publicar y distribuir diversas publicaciones en distintos idiomas, relacionadas con sus creencias y su dios Jehová.

En 1909 la sede se trasladó hasta Brooklyn, Nueva York, donde permaneció hasta 2017, año en que mudaron su sede a Warwick (Nueva York). En 1914 se creó la primera entidad legal fuera de Estados Unidos, la sucursal International Bible Students Association en Gran Bretaña. Russell falleció dos años después. Para entonces, los estudiantes de la Biblia ya rechazaban doctrinas tales como la Santísima Trinidad, la inmortalidad del alma, el fuego del infierno y esperaban la llegada de Cristo para octubre de 1914, pero todavía celebraban la Navidad, cumpleaños y aceptaban el símbolo de la cruz. Estas creencias, entre otras, se fueron rechazando progresivamente más adelante.

Tras el fallecimiento de Russell, la presidencia de la Watch Tower Bible and Tract Society of Pennsylvania fue asumida en 1917 por Joseph Franklin Rutherford. Durante su presidencia la organización aumentó considerablemente. Frente a las críticas de diversas denominaciones cristianas por sus principios doctrinarios, establecieron un cuerpo legal, mediante el que se obtuvieron fallos positivos en los tribunales de varios países que les dieron libertad de culto. También durante la presidencia de Rutherford se definió el nombre de «Testigos de Jehová», basados en el pasaje del canon bíblico Isaías 43:10, 11. El nombre se adoptó el 26 de julio de 1931, durante la asamblea de Columbus, Ohio, celebrada entre el 24 y el 30 de julio.

Entre 1942 y 1977, la presidencia de la Sociedad fue ejercida por Nathan Homer Knorr, quien ayudó a desarrollar los aspectos estructurales de la organización, fortaleció el Cuerpo Gobernante y creó diversas escuelas con el fin de estandarizar las tareas de evangelización, tales como la Escuela del Ministerio Teocrático (formación en oratoria) o la Escuela Bíblica de Galaad (formación misionera).

Entre 1977 y 1992 ejerció como presidente Frederick William Franz, quien era miembro del Cuerpo Gobernante y había sido vicepresidente desde 1945. Casi al término de su presidencia, en 1991, se levantaron las proscripciones de los Testigos de Jehová en Europa Oriental y África. Tras su muerte, Milton Henschel, antiguo asesor de Nathan Homer Knorr, asumió la presidencia hasta el año 2000, siendo reemplazado por Don Alden Adams, quien ejerce dicho cargo hasta hoy, sin ser miembro del Cuerpo Gobernante.

Los Testigos de Jehová dicen basar sus creencias en la Biblia, libro que consideran como fuente exclusiva de referencia en asuntos doctrinales. Creen en Jehová como el único Dios, el cual no es omnipresente, y se identifican como seguidores de un único líder, Jesucristo, a quien consideran hijo de Dios pero no Dios en sí mismo, y a quien además identifican con el arcángel Miguel. Si bien aceptan a María como madre de Jesús y de sus hermanos, no la veneran ni la consideran madre de Dios. Creen en la Gran Apostasía y en el libre albedrío por sobre la predestinación. A diferencia de otras denominaciones cristianas, rechazan todas las doctrinas del Concilio de Nicea I y posteriores, entre ellas la Santísima Trinidad, el fuego del infierno y la inmortalidad inherente del alma. Realizan el bautismo por inmersión en agua, en el nombre del «Padre», del «Hijo» y del «Espíritu Santo» pero rechazan el bautismo de niños. No celebran la Navidad, la Pascua, los cumpleaños ni otras fiestas y costumbres que consideran incompatibles con el cristianismo por sus orígenes paganos. Tampoco consideran obligatorio el descanso semanal, pues argumentan que el feriado sabático de la ley mosaica estaba destinado exclusivamente a Israel. Son contrarios al ecumenismo y a las demás religiones y denominaciones cristianas las identifican con el apelativo de «Babilonia la Grande». En sus liturgias evitan el uso de imágenes y símbolos, no le ofrecen adoración a la cruz cristiana (creen que Cristo en realidad murió en un madero de tormento) ni creen en los dones milagrosos, los cuales consideran que terminaron tras la muerte de los doce apóstoles.

Desaconsejan el juego del ajedrez , la música rock Heavy Metal y Rap , las películas y literatura de temas mágicos como vampiros, brujos, fantasmas etc. Obviamente los géneros de terror, de acción violenta, espiritismo o con contenido sexual. Por décadas prohibieron el uso de barbas y cabelleras largas en hombres y el uso de minifaldas o pantalones en las mujeres. 

Los Testigos de Jehová creen que la parusía ya se produjo en 1914, de modo que desde entonces Cristo está presente y ya reina de manera espiritual en la Tierra. Al mismo tiempo, creen que el armagedón está cerca, que el establecimiento del reino de Dios en la Tierra es la única manera de salvarse, y que solo 144.000 humanos, «los ungidos», irán al Cielo. Consideran que la sociedad secular actual está moralmente corrupta e influida por Satanás, por lo que sus miembros deben limitar su interacción social con las personas ajenas a su fe. Se suelen referir a su cuerpo de creencias como «la verdad» y consideran que ellos están «en la verdad». Los Testigos de Jehová, al igual que la mayoría de cristianos, consideran pecado e inmorales la masturbación, la fornicación, la homosexualidad, el aborto inducido y el espiritismo. Por faltas de conducta, los Testigos de Jehová pueden ser sometidos a diversas acciones disciplinarias, que pueden variar desde la «censura» hasta la expulsión.

Son conocidos por su predicación de casa en casa, donde distribuyen gratuitamente sus publicaciones, como "La Atalaya" y "¡Despertad!". También son conocidos por su oposición al servicio militar, su rechazo a los símbolos patrios y los nacionalismos. En general se declaran política y militarmente neutrales; por ello rechazan la violencia y el uso de armas, lo que en la Segunda Guerra Mundial provocó la persecución y matanza de sus miembros. 

Los Testigos de Jehová creen en la creación divina y rechazan el naturalismo y la evolución biológica. Para ellos, la muerte y la vejez son una herencia del pecado original de Adán.

Piensan que Jesús no siguió la tradición judía de la no pronunciación del tetragrámaton, sino que por el contrario, mandó santificarlo y darlo a conocer como el nombre de su padre (Mateo 6:9; Juan 12:28; 17:3,6,26).

Los Testigos de Jehová se reúnen semanalmente con sus respectivas congregaciones en los denominados salones del Reino. También se reúnen en asambleas anuales y en su celebración anual de la Conmemoración de la muerte de Jesús o Cena del Señor. Esta última es la única ceremonia que celebran. La realizan una vez al año en la fecha que corresponde al 14 de Nisán según el calendario lunar bíblico (marzo/abril), en la que recuerdan la muerte de Jesucristo y la analizan desde un punto de vista religioso.

Los Testigos de Jehová no creen en la transubstanciación, por lo que en estas ceremonias el pan y el vino tinto son solo elementos que representan simbólicamente el cuerpo y la sangre de Cristo.

En sus reuniones se interpretan canciones compuestas íntegramente por Testigos de Jehová. Entre sus cancioneros se encuentran "Cantando y acompañándose con música en su corazón" (1969), "Canten alabanzas a Jehová" (1986), "Cantemos a Jehová" (2009) y "Cantemos con Gozo a Jehová" (2017) todos ellos traducidos por la Watch Tower Society a numerosos idiomas.

Los Testigos de Jehová rechazan las transfusiones de sangre debido a razones religiosas, pues afirman que tanto el Antiguo como el Nuevo Testamento mandan abstenerse de la sangre (Génesis 9:4; Levítico 17:10; Deuteronomio 12:23; Hechos 15:28, 29); ellos creen que para Dios, la sangre representa la vida (Levítico 17:14), de modo que afirman obedecer el mandato bíblico de abstenerse de la sangre por respeto a Dios, quien nos dio la vida. Rechazan hacerse transfusiones de sangre, incluso aunque de ello dependieran sus vidas, lo que ha conducido a numerosas muertes evitables, incluyendo de niños. Desde 1961, la aceptación de transfusiones de sangre por parte de un miembro sin posterior arrepentimiento es causa de expulsión. 

Solo aceptan la cirugía sin sangre.

Si bien históricamente rechazaban la vacunación y aceptación o donación de órganos, actualmente, es un asunto de decisión personal, mientras no incluya transfusiones de sangre.

Los Testigos de Jehová poseen dos servicios relacionados con este tema:

La congregación cristiana de los Testigos de Jehová es coordinada y dirigida a nivel mundial por un Cuerpo Gobernante, que además ejerce como la principal entidad legal de la corporación Watch Tower Bible and Tract Society of Pennsylvania, cuya sede central se encuentra actualmente en Warwick (Nueva York). Todos los miembros del Cuerpo Gobernante se consideran ungidos, y están por sobre el presidente de la asociación legal. Sus distintas sucursales son dirigidas a su vez por «comités de sucursal», los que están a cargo de un país o un grupo de países. Las sucursales se dividen a su vez en circuitos, compuestos por alrededor de veinte congregaciones que reciben regularmente visitas de los «superintendentes de circuito», para ayudarlas a organizar y ejecutar las predicaciones en sus territorios. Las congregaciones se reúnen en templos denominados «salones del Reino». Cada congregación tiene un «cuerpo de ancianos», a quienes se encomiendan diversas tareas de supervisión y pastoreo.

Su órgano legal en español es la Watch Tower Bible and Tract Society Incorporated (en castellano, «Atalaya, Biblias y Tratados Sociedad Anónima») que participa como casa editora y distribuidora.

Durante la presidencia de Nathan Homer Knorr (1942-1977), la junta directiva de la Sociedad pasó a formar parte del Cuerpo Gobernante, entidades que hasta entonces se consideraban equivalentes. El Cuerpo Gobernante se amplió a once miembros, mientras que la junta directiva se limitó a siete miembros. Desde esa fecha, el cargo de la presidencia en el Cuerpo Gobernante, a diferencia del de la presidencia de la Sociedad, es de rotación anual, por orden alfabético de los apellidos. El número de miembros del Cuerpo Gobernante se volvió a aumentar en 1974.

Hacia 2013, el Cuerpo Gobernante estaba compuesto por David H. Splane, Anthony Morris III, D. Mark Sanderson, Geoffrey W. Jackson, M. Stephen Lett, Samuel F. Herd y Gerrit Lösch y Guy H. Pierce. El actual presidente de la Watch Tower Bible and Tract Society of Pennsylvania, Don Alden Adams, no pertenece a los ungidos y por lo tanto tampoco forma parte del Cuerpo Gobernante. Tras la muerte de Guy H. Pierce, en 2018 se integró Kenneth Cook.

Cuando un testigo de Jehová comete lo que, de acuerdo con las creencias y normativas de la comunidad, es un pecado, éste es juzgado por un «comité judicial», el cual está conformado por tres o más «ancianos». El comité se reúne con el acusado para establecer la gravedad del pecado realizado. Si el «pecador» muestra arrepentimiento, se le aplica una «censura», es decir, una serie de sugerencias basadas en la Biblia que para ellos tienen como finalidad la reconciliación del acusado con Jehová. En caso de seguir estas sugerencias, la censura se hace pública a los demás miembros de la congregación, y se considera a la persona «censurada». Si, por el contrario, durante la etapa de censura el acusado no muestra arrepentimiento, entonces la persona es expulsada de la congregación y aislada de ésta. Una persona también puede desasociarse voluntariamente, en cuyo caso pasa a ser considerada como una persona expulsada de conocimiento público dentro de la congregación. En tales casos la persona pierde contacto con sus parientes Testigos que no viven bajo el mismo techo, y los miembros de la congregación no vuelven a saludarlo ni a tener contacto social con él.

El financiamiento de los Testigos de Jehová depende fundamentalmente de la corporación Watch Tower Bible and Tract Society of Pennsylvania, principalmente dedicada al negocio editorial, en el cual trabajan numerosos Testigos de Jehová de manera voluntaria, sin recibir remuneraciones. De acuerdo con el estudioso Wilbur Lingle, alrededor del 70% de sus ingresos provienen de sus millones de publicaciones que distribuyen anualmente a precios ligeramente superiores a los costos de impresión. La organización, por su parte, defiende que están establecidos legalmente como una corporación sin ánimo de lucro, por lo que no tiene permiso para vender nada. De ahí que su obra se sostenga solo con donaciones voluntarias y que sus publicaciones estén accesibles gratuitamente tanto en papel como en formato electrónico a través de su web.

Rechazan el pago de diezmos o cuotas obligatorias o de membresía, tampoco pasan ofrenderos entre los congregados; pero sí recogen donaciones en alcancías (recipientes) que ponen fijas en los Salones del Reino y en coliseos y estadios cuando efectúan sus asambleas. Estas contribuciones son usadas para la traducción e impresión de más publicaciones, atender a misioneros y ministros viajantes, construir o renovar Salones del Reino y sucursales en países en vías de desarrollo y atender a las víctimas de desastres naturales.

Desde sus inicios, tanto la sede central como las sucursales de los Testigos de Jehová han realizado una intensa actividad editorial evangelizadora, que incluye la publicación de numerosos textos al año, incluidas biblias, libros, folletos, tratados religiosos, vídeos y música, entre otros.

"La Atalaya", su revista más conocida, se comenzó a publicar en 1879, dos años antes de la fundación de la Watch Tower Bible and Tract Society of Pennsylvania. Mediante su nueva entidad legal, los Testigos de Jehová comenzaron a publicar su propia Biblia y otros tratados a partir de 1896, dejando de predicar con la "Biblia del rey Jacobo", que habían utilizado hasta entonces. En un comienzo utilizaron imprentas externas, y adquirieron en Estados Unidos los derechos de distintas versiones de la Biblia ya existentes:

En diciembre de 1926, "The Emphatic Diaglott" se convierte en la primera versión bíblica impresa directamente en las prensas de la Watch Tower Bible and Tract Society of Pennsylvania, ubicada en Brooklyn, Nueva York. Desde entonces comenzaron a imprimir diversas versiones de la Biblia independientes:
El hecho que los Testigos dispongan de su propia versión de la Biblia ha sido cuestionado, principalmente por diferencias con las traducciones tradicionales, o por basarse en un texto crítico. Los detractores y exmiembros de esta denominación cristiana, así como diversas agrupaciones religiosas, sostienen que el contenido de la "Traducción del Nuevo Mundo de las Santas Escrituras" ha sido alterado para apoyar las creencias particulares de los Testigos; a lo que los Testigos de Jehová responden que su traducción es precisa, exacta y literalmente ajustada a los manuscritos originales. Los Testigos de Jehová sostienen que en los pasajes criticados solo usan un estilo de traducción similar al de otras versiones reconocidas, constituyéndose en traducciones poco convencionales pero legítimas.

Los Testigos de Jehová están presentes en un gran número de países, aunque no forman una proporción amplia de la población en ninguno de ellos. A diferencia de otras confesiones religiosas que contabilizan sus miembros por la asistencia anual a sus servicios o por sus miembros bautizados, los Testigos de Jehová cuentan como tales únicamente cuando son publicadores o predicadores activos.

El número de interesados o simpatizantes de su labor se muestra por la asistencia a su reunión anual, la Conmemoración de la muerte de Jesucristo. De acuerdo con sus propios datos, en 2019 asistieron 20.919.041 personas. De acuerdo con sus datos, las cifras actuales son las siguientes:

En marzo de 2014, el portal web de los Testigos de Jehová ocupó el primer puesto en sitios religiosos visitados en el mundo, según el contador Alexa.

Tras cumplirse 100 años de 1914, año que -según la creencia de los Testigos- marca el inicio de los últimos días, la organización ha visto una merma considerable en sus fieles, principalmente en Europa, aunque se ha atenuado con el crecimiento en África y el bautismo de niños. Si bien aún hay crecimiento numérico, la suma de los bautizados es mucho mayor que el crecimiento declarado en sus propios informes. Según los testigos de Jehová, en cambio, en 2014, el promedio de testigos de Jehová practicantes ("publicadores") era de 7.867.958 según su Anuario; en 2015: 7.987.279; 2016: 8.132.358; 2017: 8.248.982 y 2018: 8.360.594. 
Las 118 sucursales que tenían repartidas por el mundo se redujeron a 90 en pocos años, y se despidieron los evangelizadores pagados (precursores especiales) y muchos trabajadores de sucursales (betelitas). Las estadísticas que proporcionan los testigos, en cambio indician que la reducción sucedió antes y no después de 2014, y la razón principal que aportan es que la tecnología simplifica el trabajo: "En los últimos años, los adelantos en el campo de la impresión y las comunicaciones redujeron la cantidad de personal necesario en las sucursales grandes. Eso dejó espacio para acomodar a algunos betelitas de sucursales más pequeñas. Ahora, desde algunos lugares estratégicos, Testigos con mucha experiencia atienden la obra de educación bíblica de distintos países. Por citar un caso, la sucursal de México ha pasado a supervisar la obra de predicación que se efectúa en Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua y Panamá. Por consiguiente, esas seis sucursales han sido cerradas. Cuarenta miembros de las familias Betel de esos países fueron reasignados a la sucursal de México, y casi cien permanecieron en sus lugares de origen, donde emprendieron el ministerio de tiempo completo." En España en 2016 se vendieron un 25% de los Salones, aunque los testigos en España afirman que el número de miembros aumentó 1% en 2016 respecto al año anterior, hasta los 112.916 miembros activos.

Las revistas se entregan cada vez más espaciadas. aunque se traducen a más idiomas, y, según sus datos, ha aumentado significativamente el uso de medios electrónicos, vídeos y películas .

Los Testigos de Jehová fueron perseguidos por los nazi alemanes, principalmente por su resistencia a reconocer la autoridad del estado, a utilizar el saludo fascista, y por su objeción de conciencia al servicio militar. Se calcula que durante la Segunda Guerra Mundial aproximadamente 11.300 Testigos de Jehová fueron encarcelados en campos de concentración, donde murieron, según la fuente consultada, entre 1490 y 2550, entre ellos 253 que fueron sentenciados a muerte. El Círculo Europeo de antiguos deportados e internados Testigos de Jehová asegura que el 97% de los Testigos de Jehová alemanes sufrieron de una u otra forma la persecución del nazismo. En los , los denominados "Bibelforscher" llevaban un triángulo púrpura cosida en la ropa como identificación. 

Los Testigos de Jehová podían abandonar el campo de concentración y recuperar sus propiedades y su ciudadanía si firmaban un documento de renuncia a su fe.

El 5 de octubre de 2006, el Museo del Holocausto de Washington D. C. ofreció un día dedicado a los Testigos de Jehová víctimas del nazismo.

El 20 de abril de 2017 el Tribunal Supremo de Rusia falló en contra de los Testigos de Jehová, para disolver el Centro Administrativo de los Testigos de Jehová, restringiendo así su obra evangelizadora.

Dennis Christensen, condenado a seis años de cárcel, es el primer encausado desde la prohibición de las actividades de los Testigos.

Las creencias y prácticas de los Testigos de Jehová han sido criticadas desde diferentes sectores.

Distintos miembros de la confesión religiosa de los Testigos de Jehová han sido vinculados con casos de abuso sexual infantil. El comportamiento histórico de muchos de los líderes y ancianos de las congregaciones fue el «secreto», una especie de «código del silencio», que influyó en las víctimas para que no fueran a informar a las autoridades ni a la policía, y que prohibió además la discusión de estos asuntos dentro de la iglesia.
Solo en Australia la sucursal de los Testigos aportó en 2015 la cifra de más de mil menores agredidos desde 1950, en algo más de la mitad de los casos, por miembros de la religión. El fiscal local definió a los Testigos de Jehová «como una secta insular con reglas diseñadas para detener los informes sobre abusos sexuales».
Esta política le significó a la congregación una serie de demandas y pago de millonarias indemnizaciones.

En respuesta, la organización ha declarado que han desarrollado políticas de protección de menores para gestionar casos de abuso infantil cometidos por miembros de sus congregaciones. Detalles de estas políticas han sido emitidos en sus publicaciones y comunicados de prensa emitidos por su Oficina de Información Pública. Pese a lo anterior, la organización ha preferido pagar altas multas a cambio de retener información relacionada con los abusos.

Desde el punto de vista social, los testigos de Jehová son criticados por múltiples motivos, incluyendo:


Por otra parte, su objeción de conciencia al servicio militar y rechazo de los saludos a los símbolos patrios les ha generado conflictos con algunos gobiernos. En consecuencia, algunos Testigos de Jehová han sido perseguidos y sus actividades han sido prohibidas o restringidas en algunos países. Para el crítico religioso , los desafíos legales que han planteado ha influido en la legislación relacionada con los derechos civiles y políticos en algunos países.

El control absoluto del pensamiento de sus miembros, que se acusa a esta religión, se favorece por lo que se describe como la creencia de que la lealtad de la organización es igual a la fidelidad divina siendo el «mito central» de los testigos de Jehová empleado para garantizar la completa obediencia.
El sociólogo Andrew Holden ha observado que los testigos no ven ninguna diferencia entre la lealtad a Jehová y al propio movimiento, y Heather Botting Gary afirmó que la impugnación de las opiniones de los más altos en la jerarquía se considera como equivalente a un reto ungido de Dios mismo.
Otro foco permanente de críticas es la construcción de la mansión Beth Sarim, construida supuestamente para el «inminente retorno de los patriarcas», la cual finalmente nunca tuvo el uso que se planteó en las publicaciones de la Sociedad Wachtower, sino que se empleó como residencia particular de Joseph Franklin Rutherford, el mismo que masificó la idea de la Segunda Venida a través de su libro de 1920, "Millones que ahora viven no morirán jamás", editado por la Sociedad Watchtower.

Algunas denominaciones cristianas critican el hecho que los Testigos de Jehová manejen su propia versión de la Biblia, "Traducción del Nuevo Mundo de las Santas Escrituras", pues presenta diferencias de traducción y omite algunos textos de las versiones más conocidas.

Otra crítica tiene que ver con sus profecías fallidas, las cuales han sido citadas explícita o implícitamente en sus publicaciones, en particular las relacionadas con los años 1914, 1918, 1925 y 1975. En dichas ocasiones en que las profecías no se han cumplido, se han alterado doctrinas, o bien se ha justificado la falla mediante el término «revelaciones progresivas», que se interpretan como una conducción gradual de Dios a la comprensión más clara de la voluntad de sus seguidores.

Un aspecto doctrinario que les significa un problema histórico son los "ungidos". Es decir determinar quiénes son parte del número de 144.000 seguidores de Cristo que se irán al cielo a cogobernar. Mediante un "autodeclararse ungido", un publicador puede empezar a servirse la cena de la Conmmemoración, mientras los demás solo ven pasar frente a sus ojos la copa de vino y el pan (eucaristía). El número de ungidos aún vivos se estableció en 1930 y la cantidad durante el siglo XX disminuyó como esperanza que Armagedón llegaría antes que se acabaran, sin embargo desde inicios del siglo XXI el número comenzó a incrementarse sin control. Otro punto de desacuerdo es la convicción que desde 1914 ya estén gobernando en «forma espiritual» los ya fallecidos, pero al mismo tiempo exista un cierto número que aún no fallece, y que son exclusivamente integrantes de los Testigos de Jehová. 
Hasta 1914 los Testigos de Jehová esperaban la Segunda Venida. Como esta no se produjo, declararon que el término «parusía» en realidad significa «presencia» y no «llegada», que el Reino de Dios era espiritual y que el mundo estaba viviendo sus últimos días. Posteriormente, a través de publicaciones como "Millones que ahora viven no morirán jamás" y números de revistas, los años para el cumplimiento de esta profecía se fueron extendiendo cada vez en el tiempo, sin cumplirse en ninguno de los casos. La generación de 1914 pasó y murió completamente, por lo que desde 2010 en adelante al seguir sin cumplimiento se habla de la «generación traslapada» con lo que alargan en el tiempo la profecía.



</doc>
<doc id="2844" url="https://es.wikipedia.org/wiki?curid=2844" title="Tejido nervioso">
Tejido nervioso

El tejido nervioso comprende millones de neuronas y una incalculable cantidad de interconexiones, que forma el complejo sistema de comunicación neuronal. Las neuronas tienen receptores, elaborados en sus terminales, especializados para percibir diferentes tipos de estímulos ya sean mecánicos, químicos, térmicos, etc, y traducirlos en impulsos nerviosos que lo conducirán a los centros nerviosos. Estos impulsos se propagan sucesivamente a otras neuronas para procesamiento y transmisión a los centros más altos y percibir sensaciones o iniciar reacciones motoras.

Para llevar a cabo todas estas funciones, el sistema nervioso está organizado desde el punto de vista anatómico, en el sistema nervioso central (SNC) y el sistema nervioso periférico (SNP). El SNP se encuentra localizado fuera del SNC e incluye los 12 pares de nervios craneales (que nacen en el encéfalo), 31 pares de nervios raquídeos (que surgen de la médula espinal) y sus ganglios relacionados.

De manera complementaria, el componente motor se subdivide en:


En adición a las neuronas, el tejido nervioso contiene muchas otras células que se denominan en conjunto células gliales, que ni reciben ni transmiten impulso, su misión es apoyar a la célula principal: la neurona.

Las células del sistema nervioso se dividen en dos grandes categorías: neuronas y células gliales.
Se creía antes que estas eran las únicas células que no se reproducían, y cuando mueren no se podía reponer; sin embargo, hace poco se demostró que su capacidad regenerativa es extremadamente lenta, pero no nula. Se reconocen tres tipos de neuronas:


Uno de los propósitos de estás células era mantener a las neuronas unidas y en su lugar según Virchow. Ahora se sabe que es una de las varias funciones.
Las microglías son células pequeñas con núcleo alargado y con prolongaciones cortas e irregulares que tienen capacidad fagocitaria. Se originan en precursores de la médula ósea y alcanzan el sistema nervioso a través de la sangre; representan el sistema mononuclear fagocítico en el sistema nervioso central.

Contienen lisosomas y cuerpos residuales. Generalmente se la clasifica como célula de la neuroglia. Presentan el antígeno común leucocítico y el antígeno de histocompatibilidad clase II, propio de las células presentadoras de antígeno.




</doc>
<doc id="2846" url="https://es.wikipedia.org/wiki?curid=2846" title="Río Tambopata">
Río Tambopata

El río Tambopata es un río que pertenece a la cuenca del Amazonas, un afluente del río Madre de Dios. Discurre casi totalmente por el Perú, por la región Madre de Dios y la región Puno, pero es un río internacional, ya que un corto tramo forma frontera con Bolivia (departamento de La Paz). Tiene una longitud de 402 km.

El río Tambopata nace por encima de los 3.900 m, en los cerros nevados que dominan el altiplano peruano-boliviano aproximadamente en las coordenadas () en el departamento de La Paz. Recorre 66 kilómetros por territorio boliviano hasta las coordenadas () donde pasa a formar frontera con el Perú en un tramo de 58 kilómetros hasta la afluencia del río Colorado donde se adentra en territorio peruano. En su descenso inicial hacia la llanura de la Amazonía forma rápidos y cascadas al discurrir por profundos valles y cañones. 

Por debajo de los 3.000 m atraviesa un ecosistema selvático casi permanentemente cubierto por nubes, conocido como el bosque de neblina o yungas. Al llegar a la llanura amazónica, por debajo de los 500 m, la selva de altura se convierte en la selva amazónica baja y el río Tambopata en un río tranquilo, ancho y sinuoso. Pasa por la ciudad de San Rafael y desemboca en el Madre de Dios en Puerto Maldonado (capital de la Región Madre de Dios, con unos 40.000 habitantes), frente al río de Las Piedras.

En su recorrido, este río atraviesa la Reserva Nacional Tambopata. Sus principales afluentes son el río Mosojhuaico y el río Carama.


</doc>
<doc id="2847" url="https://es.wikipedia.org/wiki?curid=2847" title="Taifa">
Taifa

Las taifas ( "ṭā'ifa", plural طوائف "ṭawā'if", palabra que significa "bando" o "facción") fueron pequeños reinos (ملوك الطوائف) en los que se dividió el califato de Córdoba a partir de la "Revolución Cordobesa" que depuso al califa Hisham II en 1009; aunque el califato no desapareció en ese momento. En los años siguientes, en la llamada Fitna de al-Ándalus, el califato rivalizó con los primeros reinos de taifas, hasta que fue desterrado el califa Hisham III (de la dinastía omeya), lo que puso fin al califato en 1031. Posteriormente, tras el debilitamiento de los almorávides y los almohades, surgieron los llamados segundos (1144 y 1170) y terceros reinos de taifas (siglo XIII). El origen de todas las dinastías de las taifas era extranjero, salvo el de los Banu Qasi y los Banu Harún, que era muladí.

Desde que el califa Hisham II es obligado a abdicar en 1009 hasta el año de la abolición formal del califato en 1031 se suceden en el trono de Córdoba nueve califas, de las dinastías omeya y hamudí, en medio de un caos total que se refleja en la independencia paulatina de las taifas de Almería, Murcia, Alpuente, Arcos, Badajoz, Carmona, Denia, Granada, Huelva, Morón, Silves, Toledo, Tortosa, Valencia y Zaragoza. Cuando el último califa Hisham III es depuesto y proclamada en Córdoba la república, todas las "coras" (provincias) de al-Ándalus que aún no se habían independizado se autoproclaman independientes, regidas por clanes árabes y bereberes.

En el trasfondo se hallaban problemas muy profundos. Por una parte, las luchas por el trono califal no hacían sino reproducir las luchas internas que siempre habían asolado el emirato y el califato por causas raciales (árabes, bereberes y muladíes o eslavos, estos últimos esclavos libertos del norte peninsular o de origen centroeuropeo). También influían la mayor o menor presencia de población mozárabe, el ansia independentista de las áreas con mayores recursos económicos y también la agobiante presión fiscal necesaria para financiar el coste de los esfuerzos bélicos.

Cada taifa se identificó al principio con una familia, clan o dinastía. Así surgen la taifa de los amiríes (descendientes de Almanzor) en Valencia; la de los tuyibíes en Zaragoza; la de los aftasíes en Badajoz; la de los birzalíes en Carmona; la de los ziríes en Granada; la de los hamudíes en Algeciras, Ceuta y Málaga; y la de los abadíes en Sevilla. Con el paso de los años, las taifas de Sevilla (que había conquistado toda la Andalucía occidental y parte de la oriental), Badajoz, Toledo y Zaragoza, constituían las potencias islámicas peninsulares.

Durante el apogeo de los reinos de taifas (siglo XI y hasta mediados del siglo XII), los reyes de las taifas compitieron entre sí no solo militarmente, sino sobre todo en prestigio. Para ello, trataron de patrocinar a los más prestigiosos poetas y artesanos.

Sin embargo, la disgregación del califato en múltiples taifas, las cuales podían subdividirse o concentrarse con el paso del tiempo, hizo evidente que sólo un poder político centralizado y unificado podía resistir el avance de los reinos cristianos del norte. Careciendo de las tropas necesarias, las taifas contrataban mercenarios para luchar contra sus vecinos o para oponerse a los reinos cristianos del norte. Incluso guerreros cristianos, como el propio Cid Campeador, sirvieron a reyes musulmanes, luchando incluso contra otros reyes cristianos. Sin embargo, esto no fue suficiente y los reinos cristianos aprovecharían la división musulmana y la debilidad de cada taifa individual para someterlas. Al principio el sometimiento era únicamente económico, forzando a las taifas a pagar un tributo anual, las "parias", a los monarcas cristianos. Sin embargo, la conquista de Toledo en 1085 por parte de Alfonso VI de León y Castilla hizo palpable que la amenaza cristiana podía acabar con los reinos musulmanes de la península. Ante tal amenaza, los reyes de las taifas pidieron ayuda al sultán almorávide del norte de África, Yúsuf ibn Tasufin, el cual pasó el estrecho asentándose en Algeciras y no solo derrotó al rey leonés en la batalla de Zalaca (1086), sino que conquistó progresivamente todas las taifas.


Cuando el dominio almorávide empezó a decaer, surgieron los llamados segundos reinos de taifas (1144-1170), que fueron posteriormente sometidos y anexionados por los almohades, que habían sucedido a los almorávides en su dominio del norte de África.


Tras el fin del periodo almohade, marcado por la batalla de las Navas de Tolosa (1212), hubo un corto periodo denominado terceros reinos de Taifas, que terminó en la primera mitad del siglo XIII con las conquistas cristianas en el Levante de Jaime I de Aragón (Valencia, 1236) y en Castilla de Fernando III el Santo (Córdoba, 1236 y Sevilla, 1248) y perduró en Granada con la fundación del reino nazarí, que no capituló hasta el 2 de enero de 1492, fecha que pone fin a la Reconquista.







</doc>
<doc id="2850" url="https://es.wikipedia.org/wiki?curid=2850" title="Teoría del derecho">
Teoría del derecho

La teoría del derecho o teoría general del derecho es la ciencia jurídica que estudia los elementos del derecho u ordenamiento jurídico existente en toda organización social y los fundamentos científicos y filosóficos que lo han permitido evolucionar hasta nuestros días.

La teoría del derecho tiene como objetivo fundamental el análisis y la determinación de los elementos básicos que conforman el derecho, entendido este como ordenamiento jurídico unitario, esto es un conjunto de normas que conforman un solo derecho u ordenamiento jurídico en una sociedad o sociedades determinadas.

Solo a través de la comprensión del ordenamiento jurídico en su totalidad se pueden individualizar las características del fenómeno jurídico, de las que habitualmente nos servimos, para diferenciar al derecho de otros ordenamientos como son el moral y el de los usos sociales.

El estudio de los fundamentos del derecho se vale de disciplinas filosófico-jurídicas específicas, a saber:


La sociología del derecho, también llamada sociología jurídica, es aquella disciplina que estudia los problemas, las implicaciones, y todo aquello concerniente a las relaciones entre el derecho y la sociedad. A diferencia de la teoría del derecho y de la filosofía política, el principal problema u objeto de estudio de la sociología jurídica es el de la eficacia del derecho.

La teoría jurídica crítica se refiere a un movimiento en el pensamiento jurídico que aplica métodos propios de la teoría crítica (la escuela de Fráncfort) al derecho. En términos generales, este pensamiento postula nociones tales como: el derecho es simplemente política. El lenguaje jurídico es un falso discurso que ayuda a perpetuar las jerarquías: hombres sobre mujeres, ricos sobre pobres, mayorías sobre minorías.

Pero, como sucede en Brasil, ahora, hay perspectivas de teoría crítica muy diversas entre sí, y que no están conectadas a categorías filosóficas del siglo XX, sino del siglo XXI. Un ejemplo es el trabajo de teoría crítica en la teoría del derecho, que se puede encontrar en la teoría del humanismo realista, una concepción muy latinoamericana, que se vuelve a la democracia, la justicia y los derechos humanos.

El derecho es un lenguaje que nos sirve para conocer la realidad jurídicamente considerada, misma que es una parte de la realidad universal física. Todo objeto es real si puede medirse en dimensiones matemáticas: volumen, peso, densidad, etc. Por tanto, el derecho, al hablar de la realidad social, se vuelve un metalenguaje, que a su vez es lenguaje objeto de la ciencia del derecho.
Cualquier orden jurídico es, por ende, un esquema de interpretación de la realidad que dice "qué es derecho; y es prescriptivo porque señala "qué debe hacer el hombre".

Por otra parte, la ciencia jurídica, a diferencia del derecho, es descriptiva y nos dice "cómo es" y "como funciona" el sistema normativo coactivo, su único objeto de estudio.
La teoría del derecho no debe ocuparse de nociones fuera de su objeto de estudio, tales como los valores o las causas sociales que motivan la creación de normas jurídicas. Dichas nociones son el ámbito de investigación de la ética y la sociología.
Es pertinente aclarar que basta con conocer las bases del lenguaje del derecho, su paradigma y reglas de creación y aplicación, para describirlo y proveer a su eficacia.



</doc>
<doc id="2851" url="https://es.wikipedia.org/wiki?curid=2851" title="Territorios del Noroeste">
Territorios del Noroeste

Los Territorios del Noroeste (en francés: "Territoires du Nord-Ouest"; en inglés: "Northwest Territories"; en las lenguas atabascanas: "Denendeh"; «nuestra tierra»; en inuvialuktun y en inuktitut: "Nunatsiaq" [ᓄᓇᑦᓯᐊᖅ], «tierra hermosa») es uno de los tres territorios que, junto con las diez provincias, conforman las trece entidades federales de Canadá. Su capital y ciudad más poblada es Yellowknife. Está ubicado, como su nombre indica, al noroeste del país, limitando al norte con el océano Ártico, al este con Nunavut, al sureste con Manitoba, al sur con Saskatchewan, Alberta y Columbia Británica, y al oeste con Yukón. Con en 2008 es la tercera entidad menos poblada —por delante de Yukón y Nunavut, la menos poblada—, con , la tercera más extensa —por detrás de Nunavut y Quebec— y con , la segunda menos densamente poblada, por detrás de Nunavut.

Algunos de sus rasgos geográficos incluyen el vasto Gran Lago del Oso y el Gran Lago del Esclavo, así como el inmenso río Mackenzie y los cañones del Parque Nacional Nahanni, un parque calificado como Área Natural Protegida de Canadá y declarado Patrimonio de la Humanidad por la Unesco. Las islas del territorio en el archipiélago ártico canadiense incluyen la isla de Banks, isla Borden, isla Prince Patrick, y partes de la isla Victoria e isla Melville. El punto más alto es el monte Nirvana, cerca de la frontera con Yukón, con una elevación de .

El territorio actual fue creado en junio de 1870, cuando la Compañía de la Bahía de Hudson transfirió la Tierra de Rupert y el Territorio Noroeste al gobierno de Canadá. Esta inmensa región comprendía toda Canadá no confederada excepto la Columbia Británica, la costa de los Grandes Lagos, el valle del río San Lorenzo y el tercio sur de Quebec, las Provincias marítimas de Canadá, Terranova y la costa de Labrador. No incluía el territorio británico de las Islas Árticas excepto la mitad del sur de la isla Baffin, que permanecieron bajo dominio británico directo hasta 1880.

Después de la transferencia, la extensión de los territorios fue disminuyendo gradualmente. La provincia de Manitoba se creó el 15 de julio de 1870, un cuadrado diminuto alrededor de Winnipeg, que posteriormente se amplió en 1881 a una región rectangular que forma el sur de la actual provincia. Cuando la Columbia Británica se unió a la Confederación Canadiense el 20 de julio de 1871, ya le había sido concedida, en 1866, la parte del «Territorio Noroeste» al sur de los 60 grados norte y oeste de los 120 grados oeste, un área que comprendía la mayor parte del «Territorio Stikine». En 1882, Regina, en el por entonces distrito de Assiniboia, se convirtió en la capital territorial; después de que Alberta y Saskatchewan se convirtieran en provincias en 1905, Regina se convirtió en la capital de la nueva provincia de Saskatchewan.

En 1876, el «Distrito de Keewatin», en el centro del territorio, se separó de éste. En 1882 y de nuevo en 1896, la parte restante fue dividida en los siguientes distritos (entre paréntesis se indica su correspondencia con las áreas actuales):


El Distrito de Keewatin (la actual Manitoba, el noroeste de Ontario y sur de Nunavut) retornó a los Territorios del Noroeste en 1905.

Mientras tanto, Ontario fue ampliada hacia el noroeste en 1882. Quebec también fue ampliada, en 1898, y Yukón se convirtió en un territorio separado en el mismo año para encargarse de la "fiebre del oro" de Klondike, y liberar así al gobierno de los Territorios del Noroeste de administrar el súbito incremento demográfico, de actividad económica y de la afluencia de no canadienses.

Las provincias de Alberta y Saskatchewan fueron creadas en 1905 y Manitoba, Ontario y Quebec adquirieron de los Territorios del Noroeste la última parte sus actuales territorios en 1912, quedándose solo con los distritos de Mackenzie, Franklin (que absorbió los remanentes de Ungava en 1920) y Keewatin. En 1925, las fronteras de los Territorios del Noroeste fueron ampliadas a todo lo largo del Polo Norte en el principio del sector, ampliando enormemente su territorio en el norte de los campos de hielo.

Los reducidos Territorios del Noroeste no fueron representados en la Cámara de los Comunes canadiense de 1907 a 1947, cuando el distrito electoral del río Yukón-Mackenzie fue creado. Esta dependencia solo incluyó el Distrito de Mackenzie. El resto de los Territorios del Noroeste no tuvo ninguna representación en la Cámara de los Comunes hasta 1962, cuando el distrito electoral de los Territorios del Noroeste fue creado en reconocimiento a los esquimales, que habían obtenido el derecho al voto en 1953.

En 1912 el Gobierno de Canadá renombró el territorio a Territorios del Noroeste, eliminando la forma escrita con guion ortográfico ("North-Western Territory"). Entre 1925 y 1999, los Territorios del Noroeste abarcaban cuadrados, un área mayor que la India.
Finalmente, el 1 de abril de 1999, tres quintas partes del este de los Territorios del Noroeste (incluido todo el Distrito Keewatin y la mayor parte de los de Mackenzie y Franklin) se convirtieron en un territorio separado llamado Nunavut.

Hubo cierta evaluación para nombrar al territorio después de dicha separación de Nunavut, buscando una toponimia indígena. Una propuesta era "Denendeh" («nuestra tierra», en el idioma de los Dene). La idea fue defendida por el antiguo primer ministro Stephen Kakfwi, entre otros. También, una popular emisora de radio comenzó a promover en broma el cambio del nombre del territorio a "Bob", opción que llegó a estar arriba en las encuestas. Finalmente, una encuesta llevada a cabo antes de la división mostró un amplio apoyo a conservar el simple nombre descriptivo «Territorios del Noroeste». 

Como territorio, los Territorios del Noroeste tienen algunos derechos menos que las provincias. Durante su mandato, el primer ministro Kakfwi presionó para obtener un acuerdo del gobierno federal con más derechos para el territorio, incluyendo que los ingresos por los recursos naturales del territorio revirtieran al territorio. La devolución de poderes al territorio fue una cuestión en las 20ª elecciones generales de 2003, y lo ha sido desde que el territorio comenzó a elegir miembros en 1881.
El comisionado es el jefe del ejecutivo y es designado por el Gobernador en el consejo de Canadá a recomendación del Ministro federal de Asuntos Indios y Desarrollo del Norte. La posición solía ser más administrativa y gubernamental, pero con la delegación de cada vez más poderes a la Asamblea electa desde 1967, su posición se ha vuelto simbólica. Desde 1985 el comisionado ya no preside las reuniones del Consejo Ejecutivo (o gabinete) y el gobierno federal ha dado instrucciones a los comisarios de comportarse como un teniente gobernador provincial. A diferencia de los tenientes gobernadores, el comisionado de los Territorios del Noroeste no es un representante formal de la Reina de Canadá.

A diferencia de los gobiernos provinciales y de Yukón, el gobierno de los Territorios del Noroeste no tiene partidos políticos, excepto durante el período entre 1898 y 1905. Es un gobierno de consenso llamado Asamblea Legislativa. Este grupo está formado por un miembro decidido en cada uno de los diecinueve distritos electorales. Después de las elecciones generales, el nuevo parlamento decide a un primer ministro y portavoz por votación secreta. Siete MLA ("Member of Legislative Assembly" / "membre de l'Assemblée législative", «Miembro de la Asamblea Legislativa») son elegidos también como ministros, y el resto forma la oposición. Las elecciones generales más recientes del territorio fueron el 1 de octubre de 2007. El jefe de estado para los territorios es un comisionado designado por el gobierno federal. El comisionado tenía poderes gubernamentales plenos hasta 1980, momento en que los territorios obtuvieron un mayor autogobierno. La legislatura entonces comenzó a decidir un gabinete y un «Líder del Gobierno» posteriormente conocido como primer ministro ("Premier" / "Premier ministre").

El actual primer ministro de los Territorios del Noroeste es Bob McLeod. El miembro del Parlamento por el distrito del Ártico Oeste ("Western Arctic"), el distrito que comprende los Territorios del Noroeste, es Dennis Bevington (del Nuevo Partido Democrático). Por su parte, el comisionado de los Territorios del Noroeste es Tony Whitford.

El gran territorio goza de unos vastos recursos geológicos que incluyen diamantes, oro y gas natural. En particular, los diamantes de los Territorios del Noroeste son ofrecidos como una alternativa ética que alivie los riesgos de apoyar conflictos para adquirir los llamados «diamantes de guerra». Sin embargo, su explotación también ha provocado preocupaciones ambientales.

La gran cantidad de recursos naturales y la escasa población dan a los Territorios del Noroeste el más alto PIB per cápita de todas las provincias y territorios de Canadá. De hecho, su PIB per cápita de lo clasificaría como primero en el mundo si fuera considerado como un país, por delante de Luxemburgo ().

Las explotaciones más destacadas son:


De acuerdo con el censo canadiense de 2001, los 10 grupos étnicos principales en los Territorios del Noroeste eran:

Población de los Territorios del Noroeste desde 1871

Notas:
(*): "El Territorio de Yukón fue cedido de los Territorios del Noroeste en 1898."

(**): "Alberta y Saskatchewan fueron creados de parte de los Territorios del Noroeste en 1905."

(***): "Los datos hasta 1996 incluyen a Nunavut. Los datos de 2001 no incluyen a Nunavut."

(****): "Datos del censo de 2006."

"Fuente: Statistics Canada"

El gobierno elegido convirtió el francés en un idioma oficial en la Sección 11 del "Acta de los Territorios del Noroeste de 1877", que recibió el "Asentimiento Real" el 28 de abril de 1877. Antes de esto, el francés era un idioma oficial mientras los Territorios del Noroeste fueron administrados conforme al "Acta de Manitoba" entre 1870 y 1875. La cuestión se volvió candente por el teniente gobernador Joseph Royal al leer el Discurso del Trono en francés el 31 de octubre de 1888. La clamorosa protesta hizo que Royal leyera su segundo discurso del trono solo en inglés. El 28 de octubre de 1889, la cuestión quedó estancada cuando se tomó la «Resolución del Idioma», una moción que declaró que la asamblea no necesitaba el reconocimiento oficial de idiomas. La votación fue de 17 contra 2. Pero esto no duró, porque el gobierno federal se implicó, y advirtió al teniente gobernador Royal que comenzara a dar discursos en francés otra vez, y trató de legislar el bilingüismo oficial en el territorio de nuevo, a través de la Cámara de los Comunes canadiense. Sin embargo el proyecto de ley fue derrotado en la segunda lectura. La interferencia del gobierno de Canadá causó que miembros elegidos para la asamblea favorecieran al inglés como el único idioma oficial. El 19 de enero de 1892 el primer ministro territorial Frederick Haultain promovió una moción para que solo el inglés fuera usado en la Asamblea. El movimiento pasó la división con 20 votos a favor y 4 en contra.

A principios de los años 1980, el gobierno de los Territorios del Noroeste estaba otra vez bajo la presión del gobierno federal para presentar de nuevo al francés como idioma oficial. Algunos miembros nativos abandonaron la Asamblea, protestando porque no les permitirían utilizar su propio idioma. El consejo ejecutivo designó un comité especial de miembros de la Asamblea Legislativa para estudiar el tema. Estos decidieron que si el francés debía ser un idioma oficial, entonces también lo deberían ser los otros idiomas de los territorios.

La "Ley de Idiomas Oficiales de los Territorios del Noroeste" reconoce los siguientes once idiomas oficiales:


Los residentes de los Territorios del Noroeste tienen el derecho de usar cualquiera de los susodichos idiomas en un tribunal territorial y en debates y procedimientos de la legislatura. Sin embargo, las leyes implican obligatoriedad jurídica solo en sus versiones francesa e inglesa, y el gobierno solo publica leyes y otros documentos en los otros idiomas oficiales del territorio cuando la legislatura lo demanda. Además, el acceso a servicios en cualquier idioma está limitado con instituciones y circunstancias donde hay demanda significativa de ese idioma o donde es razonable esperarlo dada la naturaleza de los servicios solicitados. En la práctica esto significa que los servicios en idioma inglés están universalmente disponibles, y no está garantizado que otros idiomas, incluido el francés, serán usados por cualquier servicio, en particular del gobierno, excepto en los tribunales.

Los resultados del censo de 2006 mostraron una población de .
De las respuestas singulares del censo concernientes a la lengua materna, los idiomas con más respuestas fueron:
Había también 320 respuestas tanto del inglés como 'de un idioma no oficial'; 15 tanto de francés como de 'un idioma no oficial'; 45 tanto de inglés como de francés, y aproximadamente 400 personas no respondieron a la pregunta, o respondieron múltiples idiomas no oficiales o dieron una respuesta no enumerada. Los idiomas oficiales de los Territorios del Noroeste son mostrados en negrita (los datos mostrados son para el número de respuestas de idioma único y el porcentaje de respuestas de idioma único totales).

La iglesia con mayor número de fieles es la Iglesia católica, con un 46,7%, seguida de la Iglesia anglicana, con un 14,9%, y de la Iglesia Unida de Canadá, con un 6%. Los territorios del Noroeste, junto a Quebec, Nuevo Brunswick y la Isla del Príncipe Eduardo, son las únicas entidades territoriales de Canadá en que la Iglesia Católica es el culto mayoritario.




</doc>
<doc id="2852" url="https://es.wikipedia.org/wiki?curid=2852" title="Terranova y Labrador">
Terranova y Labrador

Terranova y Labrador (en inglés: "Newfoundland and Labrador", ) es una de las diez provincias que, junto con los tres territorios, conforman las trece entidades federales de Canadá. Su capital es San Juan de Terranova. Ubicada al noreste del país, está formada por dos áreas distintas: Labrador —situado en la península homónima, limitando al norte con el mar homónimo y al oeste y sur con Quebec— y Terranova —una isla situada en el extremo este, que limita al norte y este con el océano Atlántico, y al oeste con el golfo de San Lorenzo. Con es la cuarta entidad menos extensa —por delante de Nuevo Brunswick, Nueva Escocia e Isla del Príncipe Eduardo, la menos extensa— y con 1,2 hab/km², la cuarta menos densamente poblada, por delante de Yukón, Territorios del Noroeste y Nunavut, la menos densamente poblada.

Cuando el entonces Dominio de Terranova se unió a la confederación en 1949, la provincia se conoció como Terranova, pero desde 1964, el gobierno de la provincia se ha referido a sí mismo como el Gobierno de Terranova y Labrador, y el 6 de diciembre de 2001, se aprobó una enmienda a la Constitución de Canadá para cambiar el nombre de la provincia a «Terranova y Labrador».
La población de la provincia se estima (en abril de 2008) en habitantes.Terranova tiene sus propios dialectos de inglés, francés, irlandés y otras lenguas. El dialecto inglés de Labrador comparte mucho con el de Terranova. Por otra parte, Labrador tiene sus propios dialectos de innu-aimun e inuktitut.

Según el censo canadiense de 2001, el grupo étnico más grande de Terranova y Labrador son los ingleses (39,4%), seguido de los irlandeses (19,7%), escoceses (6%), franceses (5,5%), españoles (2,09%) y las Naciones Originarias de Canadá (3,2%).

Población desde 1951

"Fuente: Statistics Canada"

Según el censo de 2006 contaba con una población de habitantes. Para las 499.830 personas que respondieron a la pregunta concerniente a «la lengua materna», las lenguas más comunes eran las siguientes:
Las cifras mostradas arriba son el número de respuestas que marcaron una única lengua y con dichos datos se ha hallado el porcentaje de las respuestas. Hubo también 435 respuestas que marcaron inglés y otra lengua no oficial, 30 que marcaron francés y otra lengua no oficial, 295 que marcaron inglés y francés, 10 que marcaron inglés, francés y otra lengua no oficial y personas que no respondieron a la pregunta o que marcaron varias lenguas no oficiales.

Durante muchos años, Terranova y Labrador tuvo una economía deprimida. Tras el colapso de la pesquería del bacalao la provincia registró altas tasas de desempleo y la población disminuyó en alrededor de . Sin embargo, la creciente industrias minera y los recientes descubrimientos de petróleo en alta mar han empujado la economía provincial.

El PIB alcanzó los 28,1 mil millones de dólares canadienses, en comparación con 25,0 mil millones en 2009. El PIB per cápita en 2008 fue de dólares canadienses, muy superior a la media nacional. Los servicios aportan más del 60% del PIB, especialmente los servicios financieros, cuidado de la salud y la administración pública. Otras importantes industrias son la minería, la producción de petróleo y la manufactura.
La explotación minera se centra en la obtención de hierro, níquel, cobre, zinc, plata y oro. La producción de petróleo fue de 110 millones de barriles, lo que elevó el PIB un 15%. Por otra parte, la industria pesquera sigue siendo una parte importante de la economía provincial ya que emplea aproximadamente personas y contribuye con más de $ 440 millones de dólares canadienses al PIB. La cosecha combinada de peces como el bacalao, eglefino, halibut, el arenque y la caballa fue de toneladas en 2010, valoradas en aproximadamente en 130 millones $ de dólares canadienses. Los mariscos, como cangrejos, camarones y almejas, representaron toneladas, con un valor de 316 millones $ en el mismo año. El valor de los productos de la caza de focas fue de 55 millones $.

La industria forestal es importante, ya que produce toneladas de madera por año. El valor de las exportaciones de papel prensa es muy variable de año en año, dependiendo del precio del mercado mundial. La madera es producida por numerosos molinos en Terranova.

La acuicultura es una industria nueva para la provincia, que en 2006 produjo más de toneladas de salmón del Atlántico, los mejillones y trucha arco iris de más de 50 millones $. En Terranova se limita a las áreas al sur de San Juan, cerca de Deer Lake y en el Valle de Codroy. Se cultivan principalmente patatas, nabos, zanahorias y el repollo se cultiva para el consumo local. Arándanos silvestres y moras son cultivadas con fines comerciales y se utiliza en mermeladas y vino. La ganadería se reduce a la avícultura y a la producción lechera. Aparte del procesamiento de mariscos, la fabricación de papel y la refinación de petróleo, la industria se complementa con pequeñas industrias productoras de alimentos, la producción de cerveza y otras bebidas, y el calzado. El turismo también está en crecimiento.



</doc>
<doc id="2864" url="https://es.wikipedia.org/wiki?curid=2864" title="Toledo (desambiguación)">
Toledo (desambiguación)

Toledo puede referirse a:







</doc>
<doc id="2867" url="https://es.wikipedia.org/wiki?curid=2867" title="Tesla (unidad)">
Tesla (unidad)

</math>

El tesla (símbolo: T) es la unidad de inducción magnética (o densidad de flujo magnético) del Sistema Internacional de Unidades (SI). Fue nombrada así en 1960 en honor al ingeniero e inventor Nikola Tesla. El nombre de la unidad debe escribirse en minúsculas, mientras que su símbolo se escribe con mayúscula.

Un tesla se define como una inducción magnética uniforme que, repartida normalmente sobre una superficie de un metro cuadrado, produce a través de esta superficie un flujo magnético total de un weber. 

También se define como la inducción de un campo magnético que ejerce una fuerza de 1 N (newton) sobre una carga de 1 C (culombio) que se mueve a velocidad de 1 m/s dentro del campo y perpendicularmente a las líneas de inducción magnética.

Lo que es: 1 T = 1 N·s·m·C

El tesla es el valor del total del flujo magnético dividido por el área, de lo que puede deducirse que si reducimos el área afectada se incrementa la densidad del flujo magnético. Esto continuaría pasando hasta que el material llegara a un punto de saturación magnética.

A continuación una tabla de los múltiplos y submúltiplos del Sistema Internacional de Unidades.

1 tesla es equivalente a:




__FORZAR_TDC__

</doc>
<doc id="2870" url="https://es.wikipedia.org/wiki?curid=2870" title="Tejido cartilaginoso">
Tejido cartilaginoso

El tejido cartilaginoso, o cartílago, es un tipo de tejido conectivo especializado, elástico, carente de vasos sanguíneos, formados principalmente por matriz extracelular y por células dispersas denominadas condrocitos. La parte exterior del cartílago, llamada pericondrio, es la encargada de brindar el soporte vital a los condrocitos.

El cartílago se encuentra revistiendo articulaciones, en las uniones entre las costillas y el esternón, como refuerzo en la tráquea y bronquios, en el oído externo y en el tabique nasal. También se encuentra en embriones de vertebrados y peces cartilaginosos.

Los cartílagos sirven para acomodar las superficies de los cóndilos femorales a las cavidades glenoideas de la tibia, para amortiguar los golpes al caminar y los saltos, para prevenir el desgaste por rozamiento y, por lo tanto, para permitir los movimientos de la articulación. Es una estructura de soporte y da cierta movilidad a las articulaciones.

Existen 3 tipos de tejido cartilaginoso:




Las células propias de este tejido se llaman condrocitos, los cuales provienen de los condroblastos presentes en el pericondrio. Poseen un retículo endoplasmático rugoso bien desarrollado y un aparato de Golgi grande, así como muchas vesículas, las cuales son indicios de su actividad secretora. Los filamentos intermedios, compuestos por vimentina aparecen en abundancia. A menudo las células contienen glucógeno y no con poca frecuencia también inclusiones lipídicas, en parte grandes.

El cartílago hialino se encuentra cubierto externamente por una membrana fibrosa llamada pericondrio, excepto en los extremos articulares de los huesos y también donde se encuentra directamente debajo de la piel, es decir, las orejas y la nariz. Esta membrana contiene vasos que le proporcionan nutrición al cartílago.

Si se observa una delgada capa bajo el microscopio, se encontrará que está formado por células con forma redondeada o sin puntas, en grupos de dos o más en una matriz biológica granular o casi homogénea. Al microscopio óptico no se observa red fibrilar en la matriz extracelular, sin embargo al utilizar luz polarizada pueden visualizarse redes de fibrillas. Este efecto óptico es debido a que el índice de refracción de las fibrillas es similar al de la matriz en las que se encuentran inmersas.

La matriz extracelular es una red 3D muy compleja. Las células se encuentran en las cavidades de la matriz, llamada lagunas del cartílago;3 en torno a estas la matriz está dispuesta en líneas concéntricas, como si se hubiera formado en porciones sucesivas alrededor de las células del cartílago. Esto constituye la llamada cápsula del espacio.

Cada laguna está generalmente ocupada por una sola célula, pero durante la división de las células puede contener dos, cuatro u ocho células, lo que constituye un grupo isogénico o grupo de células isogénicas.4
El cartílago hialino también contiene condrocitos que son las células del cartílago que producen la matriz. La matriz hialina del cartílago se compone sobre todo de colágeno tipo II y sulfato de condroitina, los cuales también se encuentran en el cartílago elástico. Formado principalmente por fibrillas de colágeno tipo I. Posee condrocitos dispuestos en grupos. Existe pericondrio. Es el más abundante del cuerpo. Tiene un aspecto blanquecino azulado. Se encuentra en el esqueleto nasal, la laringe, la tráquea, los bronquios, los arcos costales (costillas) y los extremos articulares de los . Es avascular, nutriéndose por difusión a partir del líquido sinovial. Es de pocas fibras.

Los condrocitos producen la matriz amplia formada por colágeno de tipo II (forma fibrillas finas características), colágeno del tipo IX (une las fibrillas de tipo II), colágeno de tipo X (rodea células hipertróficas), colágeno de tipo XI (función desconocida), hialuronano y el agregado de proteoglucano unido a él. En especial, las cadenas de queratin sulfato y condroitin sulfato del agrecano fijan el agua, un requisito fundamental para la creación de la consistencia elástica característica del cartílago. El hialuronano y la gran cantidad de moléculas de agrecano unidas a el forman un complejo molecular gigante dado que puede alcanzar un tamaño de 3-4 mm. Estos agregados constituyen la mayor parte del cartílago y le imparten su consistencia cartilaginosa. La estabilidad morfológica del cartílago también tiene su origen en estos agregados. En la forma de cartílago articular este material singular puede soportar todo el peso del cuerpo. Un vínculo importante entre la matriz y las células cartilaginosas es la condronectina presente en la membrana de los condrocitos, una proteína semejante a la fibronectina.

Es característico que los condrocitos que han surgido de una célula progenitora por división mitótica se ubiquen en grupos pequeños contiguos (grupos celulares isógenos). En el entorno inmediato de las células cartilaginosas la matriz contiene glucosaminoglucanos muy sulfatados y recibe el nombre de matriz territorial. Esta matriz se tiñe de color azul violeta intenso en los preparados coloreados con H-E. El grupo de condrocitos y su matriz forman un territorio cartilaginoso (= condrona). La matriz entre los territorios se denomina matriz interterritorial. Carece de células y se tiñe de un tono pálido.

Los condrocitos están ubicados en espacios ("lagunas" o condroplastos) de la matriz cuya pared, o sea, el entorno inmediato de las células cartilaginosas también recibe el nombre de cápsula condrocítica. La región capsular con frecuencia posee una capa pericelular con función protectora especial contra la compresión y la tracción. En los preparados histológicos, los condrocitos suelen aparecer retraídos artificialmente dentro de sus lagunas.

El crecimiento ocurre por secreción de matriz en el interior de las piezas cartilaginosas (crecimiento intersticial o por intususcepción) o por formación nueva en la periferia (crecimiento por aposición).

El cartílago maduro no tiene nervios y en la mayoría de los casos es avascular. La nutrición de las células cartilaginosas ocurre por difusión a través de la matriz provista de agua en abundancia. El metabolismo es anaerobio en una proporción considerable.


</doc>
<doc id="2873" url="https://es.wikipedia.org/wiki?curid=2873" title="Tachyglossidae">
Tachyglossidae

Los taquiglósidos o equidnas (Tachyglossidae) son la única familia conocida del suborden Tachyglossa, donde se clasifican a los equidnas actuales y sus ancestros extintos. Estos mamíferos, similares en apariencia a los erizos, habitan en las islas de Nueva Guinea, Salawati, Australia, Tasmania y otras islas menores próximas a las costas de estas. Además de ser muy difíciles de encontrar, su rareza reside en que son los únicos mamíferos, junto con los ornitorrincos, que ponen huevos.

Los equidnas deben su nombre a la ninfa mitológica madre de todos los legendarios monstruos de la Grecia Clásica. Tienen el cuerpo cubierto de espinas, lo que unido a la dieta que llevan, mayoritariamente insectívora, y en algunos casos con predilección por las hormigas y termitas (mirmecofagia), les ha valido el nombre de «hormigueros espinosos».

En la actualidad se reconocen dos géneros vivientes, con solo cuatro especies:


Son animales de cuerpo compacto, cubierto de un denso pelaje del que sobresalen largas púas empleadas como método de defensa. Normalmente miden entre 35 y 45 centímetros de largo, con una cola de 10 centímetros, y un peso promedio de 2 a 7 kilogramos. Los machos son por regla general de mayor tamaño que las hembras.

El cráneo es largo y redondeado, la cara larga con la mandíbula inferior poco desarrollada, constituida por dos delgados y largos huesos. Su dieta, constituida por insectos y lombrices, determina un aparato bucal tubular de estrecha abertura, provisto de una larga lengua pegajosa que puede alcanzar 20 centímetros de longitud, con la que atrapan el alimento, que, al carecer de dientes, será triturado con unas espinas córneas situadas en el paladar al final de la boca. Para localizar los alimentos, además de un agudizado sentido del olfato, están dotados de electrorreceptores táctiles en el rostro con los que les resulta fácil hallar las colonias de hormigas y termitas.

Son poderosos excavadores que emplean pies y manos para construir galerías y oquedades o escarbar en la tierra en busca de alimento. Para ello sus extremidades poseen manos y pies cavadores dotados de poderosas uñas. El segundo dedo de las extremidades posteriores es más largo y lo emplean para rascarse y limpiarse el pelo y la piel.

Los machos y algunas hembras, poseen espolones tras la articulación de la rodilla, pero a diferencia de "Ornithorhynchus sp"., este animal no sintetiza ninguna sustancia tóxica, por lo que se desconoce la función real de los mismos.

Las hembras desarrollan un marsupio temporal mientras dura la incubación y la lactancia. El pene de los machos tiene cuatro cabezas, algo común entre reptiles pero raro en mamíferos. A pesar de ser mamífero, la cría del equidna nace a partir de huevos ya que es uno de los dos mamíferos ovíparos, junto al ornitorrinco ("Ornithorhynchus anatinus"), que existen en la Tierra.

A diferencia de lo que se cree, los equidnas no hibernan como respuesta al frío. El estado de torpor al que se ven sometidos algunos ejemplares aislados, parece estar relacionado más bien con un proceso digestivo anómalo.

Al contrario de lo previamente investigado, los equidnas sí entran en sueño REM, aunque sólo cuando la temperatura del ambiente está alrededor de los 25 °C. A temperaturas de 15 °C y 28 °C, el sueño REM se suprime.

La hembra pone un solo huevo. La incubación tarda diez días; el equidna joven succiona la leche de los poros de las dos glándulas mamarias (los monotremas no tienen pezones) y permanecen en la bolsa durante cuarenta y cinco a cincuenta días, en dicho tiempo comienzan a desarrollar las espinas. La madre cava una madriguera y deposita al pequeño, retornando cada cinco días para amamantarlo hasta el destete, que es a los siete meses.

Los equidnas machos tienen un pene tetracapitado, pero solo dos de las cabezas se usan durante el apareamiento. Las otras dos cabezas "se cierran" y no crecen en tamaño. Las cabezas usadas se intercambian cada vez que el mamífero copula.




</doc>
<doc id="2875" url="https://es.wikipedia.org/wiki?curid=2875" title="Thylacinidae">
Thylacinidae

Thylacinidae fue una familia de marsupiales carnívoros del orden Dasyuromorphia propios de la fauna australiana. Con un rango fósil que comprende el intervalo Oligoceno - Holoceno, su representante más reciente, extinto en 1936, es "Thylacinus cynocephalus", el tilacín o lobo marsupial. Se trata de animales poliprotodontos con una fórmula dentaria 4/3, 1/1, 3/3, 4/4 = 46.

Familia Thylacinidae †



</doc>
<doc id="2877" url="https://es.wikipedia.org/wiki?curid=2877" title="Tipo de dato">
Tipo de dato

En ciencias de la computación, un tipo de dato informático o simplemente tipo, es un atributo de los datos que indica al ordenador (y/o al programador/programadora) sobre la clase de datos que se va a manejar. Esto incluye imponer restricciones en los datos, como qué valores pueden tomar y qué operaciones se pueden realizar.

Los tipos de datos más comunes son: números enteros, números con signo (negativos), números de coma flotante (decimales), cadenas alfanuméricas (y unicodes), estados, etc.

Un tipo de dato es, un espacio en memorias con restricciones. Por ejemplo, el tipo "int" representa, generalmente, un conjunto de enteros de 32 bits cuyo rango va desde el -2.147.483.648 al 2.147.483.647, así como las operaciones que se pueden realizar con los enteros, como son la suma, la resta, y la multiplicación. Los colores, por su parte, se representan como tres bytes denotando la cantidad de rojo, verde y azul, y una cadena de caracteres representando el nombre del color (en este caso, las operaciones permitidas incluyen la adición y la sustracción, pero no la multiplicación).

Este es un concepto propio de la informática, y más específicamente de los lenguajes de programación, aunque también se encuentra relacionado con nociones similares de la matemática y la lógica.

En un sentido amplio, un tipo de datos define un conjunto de valores y las operaciones sobre esos valores. Casi todos los lenguajes de programación explícitamente incluyen la notación del tipo de datos, aunque lenguajes diferentes pueden usar terminologías diferentes. La mayor parte de los lenguajes de programación permiten al programador definir tipos de datos adicionales, normalmente combinando múltiples elementos de otros tipos y definiendo las operaciones del nuevo tipo de dato. Por ejemplo, un programador puede crear un nuevo tipo de dato llamado "Persona", contemplando que el dato interpretado como "Persona" incluya un nombre y una fecha de nacimiento.

Un tipo de dato puede ser también visto como una limitación impuesta en la interpretación de los datos en un sistema de tipificación, describiendo la representación, la interpretación y la estructura de los valores u objetos almacenados en la memoria del ordenador. El sistema de tipificación usa información de los tipos de datos para comprobar la verificación de los programas que acceden o manipulan los datos.

Los tipos de datos hacen referencia al tipo de información que se trabaja, donde la unidad mínima de almacenamiento es el dato, también se puede considerar como el rango de valores que puede tomar una variable durante la ejecución del programa. 

El tipo de dato carácter es un dígito individual el cual se puede representar como numéricos (0 al 9), letras (a-z) y símbolos (!"$&/\).

El tipo de dato carácter unicode es una "extensión" del tipo de dato cadena, permite ampliar los símbolos de escritura, provee exactamente hasta 65535 caracteres diferentes.

Nota: En el lenguaje java la codificación Unicode permite trabajar con todos los caracteres de distintos idiomas.

Este tipo de dato puede ser real o entero, dependiendo del tipo de dato que se vaya a utilizar.

Enteros: son los valores que no tienen punto decimal, pueden ser positivos o negativos y el cero.

Reales: estos caracteres almacenan números muy grandes que poseen parte entera y parte decimal.

estos serían sus rangos y tamaños ordenados

Este tipo de dato se emplea para valores lógicos, los podemos definir como datos comparativos dicha comparación devuelve resultados lógicos (Verdadero o Falso).

Los tipos compuestos se derivan de uno o más datos primitivos. A las distintas maneras de formar o combinar estos datos se les conocen con el nombre de “Estructura de datos”. Al combinarlo podemos crear un nuevo tipo, por ejemplo:

"array-de-enteros" es distinto al tipo "entero".


El lenguaje de programación Pascal permite declarar variables de tipo carácter (Cadena) y numérica.
Como se puede apreciar, todas las variables excepto la de tipo Cadena son de tipo numéricas (incluyendo Booleano).

El lenguaje de programación Java permite declarar variables de tipo primitivo, pero dada que los envoltorios de dichas funciones presentan muchas operaciones útiles, es más común hacer uso de las clases que las tratan.

No hay que confundir estos tipos de datos con los tipos de datos abstractos.

Los TDA siguen una interfaz que especifica que hace ese tipo de datos (la estructura de datos sería la implementación concreta). Formalmente se trata de un modelo matemático para tipos de datos que están definidos por su comportamiento o semántica. A nivel de usuario se puede ver como el esquema de los datos y operaciones para manipular los elementos que componen ese tipo de datos. La estructura de datos sería la representación concreta de los datos.




</doc>
<doc id="2882" url="https://es.wikipedia.org/wiki?curid=2882" title="Identificador de recursos uniforme">
Identificador de recursos uniforme

Un identificador de recursos uniforme o URI —del inglés "uniform resource identifier"— es una cadena de caracteres que identifica los recursos de una red de forma unívoca. La diferencia respecto a un localizador de recursos uniforme (URL) es que estos últimos hacen referencia a recursos que, de forma general, pueden variar en el tiempo.

Normalmente estos recursos son accesibles en una red o sistema. Los URI pueden ser localizador de recursos uniforme (URL), "uniform resource name" (URN), o ambos.

Un URI consta de las siguientes partes:


Aunque se acostumbra llamar URL a todas las direcciones web, URI es un identificador más completo
y por eso es recomendado su uso en lugar de la expresión URL.

Un URI se diferencia de un URL en que permite incluir en la dirección una subdirección, determinada por el “fragmento”.

Cuando la autoridad se divide en forma de usuario, anfitrión y puerto, esta se puede representar mediante el siguiente diagrama sintáctico:




</doc>
<doc id="2883" url="https://es.wikipedia.org/wiki?curid=2883" title="Unicode">
Unicode

Unicode es un estándar de codificación de caracteres diseñado para facilitar el tratamiento informático, transmisión y visualización de textos de numerosos idiomas y disciplinas técnicas, además de textos clásicos de lenguas muertas. El término Unicode proviene de los tres objetivos perseguidos: universalidad, uniformidad y unicidad.

Unicode define cada carácter o símbolo mediante un nombre e identificador numérico, el "code point" (‘punto de código’). Además incluye otras informaciones para el uso correcto de cada carácter, como sistema de escritura, categoría, direccionalidad, mayúsculas y otros atributos. Unicode trata los caracteres alfabéticos, ideográficos y símbolos de forma equivalente, lo que significa que se pueden mezclar en un mismo texto sin utilizar marcas o caracteres de control.

Este estándar es mantenido por el Unicode Technical Committee (UTC), integrado en el Consorcio Unicode, del que forman parte con distinto grado de implicación empresas como: Microsoft, Apple, Adobe, IBM, Oracle, SAP, Google, Facebook o Shopify, instituciones como la Universidad de Berkeley, o el Gobierno de la India y profesionales y académicos a título individual.
El Unicode Consortium mantiene estrecha relación con ISO/IEC, con la que mantiene desde 1991 el acuerdo de sincronizar sus estándares que contienen los mismos caracteres y puntos de código.

La creación de Unicode ha sido un ambicioso proyecto para reemplazar los esquemas de codificación de caracteres ya existentes, muchos de los cuales estaban muy limitados en tamaño y son incompatibles con entornos plurilingües. Unicode se ha convertido en el más extenso y completo esquema de codificación de caracteres, siendo el dominante en la internacionalización y adaptación local del software informático. El estándar ha sido aceptado en un número considerable de tecnologías recientes, como XML, Java y sistemas operativos modernos.

La descripción completa del estándar y las tablas de caracteres están disponibles en la página web oficial de Unicode. La referencia completa se publica, además, en forma de libro cada vez que se completa una nueva versión principal. La versión digital de este libro está disponible de forma gratuita. Las revisiones y adiciones se publican de forma independiente.

Unicode incluye todos los caracteres de uso común en la actualidad. La versión 13.0 contiene 143924 caracteres provenientes de alfabetos, sistemas ideográficos y colecciones de símbolos (matemáticos, técnicos, musicales, iconos...). La cifra crece con cada versión.

Unicode incluye sistemas de escritura modernos como: latino; escrituras históricas extintas, para propósitos académicos, como por ejemplo: cuneiforme, y rúnico. Entre los caracteres no alfabéticos incluidos en Unicode se encuentran símbolos musicales y matemáticos, fichas de juegos como el dominó, flechas, iconos etc.

Además, Unicode incluye los signos diacríticos como caracteres independientes que pueden ser combinados con otros caracteres y dispone de versiones predefinidas de la mayoría de letras con símbolos diacríticos en uso en la actualidad, como las vocales acentuadas del español.

Unicode es un estándar en constante evolución y se agregan nuevos caracteres continuamente. Se han descartado ciertos alfabetos, propuestos por distintas razones, como por ejemplo el alfabeto klingon.

Como ya se ha indicado, Unicode está sincronizado con el estándar ISO/IEC conocido como UCS o juego de caracteres universal. Desde un punto de vista técnico, incluye o es compatible con codificaciones anteriores como ASCII7 o ISO 8859-1, los estándares nacionales ANSI Z39.64, KS X 1001, JIS X 0208, JIS X 0212, JIS X 0213, GB 2312, GB 18030, HKSCS, y CNS 11643, codificaciones particulares de fabricantes de software como Apple, Adobe, Microsoft, IBM, etc. Además, Unicode reserva espacio para fabricantes de software que pueden crear extensiones para su propio uso.

El elemento básico del estándar Unicode es el carácter. Se considera un carácter al elemento más pequeño de un sistema de escritura con significado. El estándar Unicode codifica los caracteres esenciales ―grafemas― definiéndolos de forma abstracta y deja la representación visual (tamaño, dimensión, fuente o estilo) al software que lo trate, como procesadores de texto o navegadores web. Se incluyen letras, signos diacríticos, caracteres de puntuación, ideogramas, caracteres silábicos, caracteres de control y otros símbolos. Los caracteres se agrupan en alfabetos o sistemas de escritura. Se considera que son diferentes los caracteres de alfabetos distintos, aunque compartan forma y significación.

Los caracteres se identifican mediante un número o punto de código y su nombre o descripción. Cuando se ha asignado un código a un carácter, se dice que dicho carácter está codificado. El espacio para códigos tiene 1 114 112 posiciones posibles (0x10FFFF). Los puntos de código se representan utilizando notación hexadecimal agregando el prefijo U+. El valor hexadecimal se completa con ceros hasta 4 dígitos hexadecimales cuando es necesario; si es de longitud mayor que 4 dígitos no se agregan ceros.

Los bloques del espacio de códigos contienen puntos con la siguiente información:


Unicode incluye un mecanismo para formar caracteres y así extender el repertorio de compatibilidad con los símbolos existentes. Un carácter base se complementa con marcas: signos diacríticos, de puntuación o marcos. El tipo de cada carácter y sus atributos definen el papel que pueden jugar en una combinación. Por este motivo, puede haber varias opciones que representen el mismo carácter. Para facilitar la compatibilidad con codificaciones anteriores, se proporcionan caracteres precompuestos; en la definición de dichos caracteres se hace constar qué caracteres intervienen en la composición.

Un grupo de caracteres consecutivos, independientemente de su tipo, forma una secuencia. En caso de que varias secuencias representen el mismo conjunto de caracteres esenciales, el estándar no define una de ellas como 'correcta', sino que las considera equivalentes. Para poder identificar dichas equivalencias, Unicode define los mecanismos de "equivalencia canónica" y de "equivalencia de compatibilidad" basados en la obtención de formas normalizadas de las cadenas a comparar.

En el estándar Unicode, los ideogramas de Asia oriental (popularmente llamados «caracteres chinos») se denominan «ideogramas han». Estos ideogramas se desarrollaron en China y fueron adaptados por culturas próximas para su propio uso.
Japón, Corea y Vietnam desarrollaron sus propios sistemas alfabéticos o silábicos para usar en combinación con los símbolos chinos: hiragana y katakana (en Japón), hangul (en Corea) y yi (en Vietnam). La evolución natural de los sistemas de escritura y los distintos momentos de entrada de los caracteres en las distintas culturas han marcado diferencias en los ideogramas utilizados. Unicode considera las distintas versiones de los ideogramas como variantes de un mismo carácter abstracto, es decir, como resultado de la aplicación de un tipo de letra diferente en cada caso y considera las variantes nacionales como pertenecientes a un mismo sistema de escritura. La versión original del estándar se desarrolló a partir de los estándares industriales existentes en los países afectados.

El organismo encargado de desarrollar el repertorio de caracteres es el Ideographic Rapporteur Group (IRG). IRG es un grupo de trabajo integrado en ISO/IEC JTC1/SC2/WG2, incluyendo a China, Hong Kong, Macao, Taipei Computer Association, Singapur, Japón, Corea del Sur, Corea del Norte, Vietnam y Estados Unidos de América.

La base de datos de caracteres CJK se denomina Unihan y contiene, además, información auxiliar sobre significado, conversiones, datos necesarios para utilizarlos en los diferentes lenguajes que los utilizan. A continuación se muestran los bloques que describen este repertorio. IRG define los caracteres de los siete grupos unificados; los dos grupos siguientes contienen caracteres para compatibilidad con estándares anteriores.

Se admite que nunca se podrá finalizar la tarea de incluir ideogramas en el estándar debido, principalmente, a que la creación de nuevos ideogramas continúa. A fin de suplir eventuales carencias, Unicode ofrece un mecanismo que permite la representación de los símbolos que faltan denominado «secuencias de descripción ideográfica». Se basa en que en la práctica, la totalidad de los ideogramas se puede descomponer en piezas más pequeñas que, a su vez, son ideogramas. Aunque sea posible la representación de un símbolo mediante una secuencia, el estándar especifica que siempre que exista una versión codificada su uso debe ser preferente. No hay un método para la «descomposición canónica» de ideogramas ni algoritmos de equivalencia por lo que las operaciones sobre el texto, como búsqueda u ordenación, pueden fallar.

Unicode define 12 caracteres de control para la descripción de ideogramas representando distintas posibilidades de combinación espacial de otros caracteres han.

El estándar fue diseñado con los siguientes objetivos:


El conjunto de caracteres codificados por Unicode, es la UCD (unicode character database: base de datos de caracteres Unicode). Además de nombre y punto de código, incluye más información: alfabeto al que pertenece, nombre, clasificación, mayúsculas, orientación y otras formas de uso, variantes estandarizadas, reglas de combinación, etc.

Formalmente la base de datos se divide en planos y estos a su vez en áreas y bloques. Con excepciones, los caracteres codificados se agrupan en el espacio de códigos siguiendo categorías como alfabeto o sistema de escritura, de forma que caracteres relacionados se encuentren cerca en las tablas de codificación.

Por conveniencia se ha dividido el espacio de códigos en grandes grupos denominados "planos". Cada plano contiene un máximo de 65 535 caracteres. Dado un punto de código expresado en hexadecimal, los 4 últimos dígitos determinan la posición del carácter en el plano.

Los distintos planos se dividen en áreas de direccionamiento en función de los tipos generales que incluyen. Esta división es convencional, no reglada y puede variar con el tiempo. Las áreas se dividen, a su vez, en bloques. Los bloques están definidos normativamente y son rangos consecutivos del espacio de códigos. Los bloques se utilizan para formar las tablas impresas de caracteres pero no deben tomarse como definiciones de grupos significativos de caracteres.

Los puntos de código de Unicode se identifican por un número entero. Según su arquitectura, un ordenador utilizará unidades de 8, 16 o 32 bits para representar dichos enteros. Las "formas de codificación" de Unicode reglamentan la forma en que los puntos de código se transformarán en unidades tratables por el computador.

Unicode define tres formas de codificación bajo el nombre UTF (Unicode transformation format: formato de transformación Unicode):

Las "formas de codificación" se limitan a describir el modo en que se representan los puntos de código en formato inteligible por la máquina. A partir de las 3 formas identificadas se definen 7 esquemas de codificación.

Los "esquemas de codificación" tratan de la forma en que se serializa la información codificada. La seguridad en los intercambios de información entre sistemas heterogéneos requiere la implementación de sistemas que permitan determinar el orden correcto de los bits y bytes y garantizar que la reconstrucción de la información es correcta. Una diferencia fundamental entre procesadores es el orden de disposición de los bytes en palabras de 16 y 32 bits, lo que se denomina "endianness". Los esquemas de codificación deben garantizar que los extremos de una comunicación saben cómo interpretar la información recibida. A partir de las 3 formas de codificación se definen 7 esquemas. A pesar de que comparten nombres, no debe confundirse esquemas y formas de codificación.

Unicode define una marca especial, la marca de orden de bytes (BOM, "Byte Order Mark"), al inicio de un fichero o una comunicación para hacer explícita la ordenación de bytes. Cuando un protocolo superior especifica el orden de bytes, la marca no es necesaria y puede omitirse dando lugar a los esquemas de la lista anterior con sufijo "BE" o "LE". En los esquemas UTF-16 y UTF-32, que admiten BOM, si este no se especifica se asume que la ordenación de bytes es "big-endian".

La unidad de codificación en UTF-8 es el byte por lo que no necesita una indicación de orden de byte. El estándar ni requiere ni recomienda la utilización de BOM, pero lo admite como marca de que el texto es Unicode o como resultado de la conversión de otros esquemas.

El proyecto Unicode se inició a finales de 1987, tras conversaciones entre Joe Becker, Lee Collins y Mark Davis (ingenieros de las empresas Apple y Xerox). Como resultado de su colaboración, en agosto de 1988 se publicó el primer borrador de Unicode bajo el nombre de Unicode88.
En esta primera versión se consideraba que sólo se codificarían los caracteres necesarios para el uso moderno, por lo que se utilizaron códigos de 16 bits.

Durante el año 1989 se sumaron colaboradores de otras compañías como Microsoft o Sun Microsystems. El 3 de febrero de 1991 se formó el Consorcio Unicode, y en octubre de 1991 se publicó la primera versión del estándar. La segunda versión, que ya incluía la escritura ideográfica han se publicó en junio de 1992. A continuación se muestra una tabla con las distintas versiones del Estándar Unicode con sus adiciones o modificaciones más importantes.



</doc>
<doc id="2885" url="https://es.wikipedia.org/wiki?curid=2885" title="Unidad astronómica">
Unidad astronómica

La unidad astronómica (abreviada ua, au, UA o AU) es una unidad de longitud igual, por definición, a  m, que equivale aproximadamente a la distancia media entre la Tierra y el Sol. Esta definición está en vigor desde la asamblea general de la Unión Astronómica Internacional (UAI) del 31 de agosto de 2012, en la cual se dejó sin efecto la definición “gaussiana” usada desde 1976, que era «el radio de una órbita circular newtoniana y libre de perturbaciones alrededor del Sol descrita por una partícula de masa infinitesimal que se desplaza en promedio a 0,01720209895 radianes por día». 

El símbolo UA es el recomendado por la Oficina Internacional de Pesas y Medidas y por la norma internacional ISO 80000, mientras que au es el único considerado válido por la UAI, y el más común en los países angloparlantes. También es frecuente ver el símbolo escrito en mayúsculas, UA o AU, a pesar de que el Sistema Internacional de unidades utiliza letras mayúsculas solo para los símbolos de las unidades que llevan el nombre de una persona. 

El nombre proviene de los siglos y , cuando todavía no se calculaban con precisión las distancias absolutas entre los cuerpos del sistema solar, y solo se conocían las distancias relativas tomando como patrón la distancia media entre la Tierra y el Sol, que fue denominada unidad astronómica. Se llegó a afirmar que el día en que se midiera este valor, «se conocería el tamaño del universo».

Un antecedente directo de la unidad astronómica se puede encontrar directamente en las demostraciones de Nicolás Copérnico (también conocido como “Copernicus”)para su sistema heliocéntrico en el . En el tomo V de su libro "De Revolutionibus Orbium Coelestium" (1543) calculó, utilizando trigonometría, las distancias relativas entre los planetas conocidos entonces y el Sol, teniendo como base la distancia entre la Tierra y el Sol. Midiendo los ángulos entre la Tierra, el planeta y el Sol en los momentos en que estos forman un ángulo recto, es posible obtener la distancia Sol-planeta en unidades astronómicas. Esta fue una de sus demostraciones para probar que los planetas, incluida la Tierra, giraban alrededor del Sol (heliocentrismo), descartando el modelo de Claudio Ptolomeo que postulaba que la Tierra era el centro alrededor del cual giraban los planetas y el Sol (geocentrismo). Estableció así la primera escala relativa del sistema solar utilizando como patrón la distancia entre la Tierra y el Sol.

Posteriormente Johannes Kepler, basándose en las cuidadosas observaciones de Tycho Brahe, estableció las leyes del movimiento planetario, las cuales se conocen justamente como «leyes de Kepler». La tercera de estas leyes relaciona la distancia de cada planeta al Sol con el tiempo que tarda en recorrer su órbita (es decir el período orbital) y, como consecuencia, establece una escala relativa mejorada para el sistema solar: por ejemplo, basta con medir cuántos años tarda Saturno en orbitar el Sol para saber cuál es la distancia de Saturno al Sol en unidades astronómicas. Kepler estimó con muy buena precisión los tamaños de las órbitas planetarias; por ejemplo, fijó la distancia entre Mercurio y el Sol en 0,387 unidades astronómicas (el valor correcto es 0,389), y la distancia de Saturno al Sol en 9,510 unidades astronómicas (el valor correcto siendo 9,539). Sin embargo, ni Kepler ni ninguno de sus contemporáneos sabían cuánto valía esta unidad astronómica, y por tanto ignoraban completamente la escala real del sistema planetario conocido, que en aquel entonces se extendía hasta Saturno.

Partiendo de las leyes de Kepler, bastaba medir la distancia de un planeta cualquiera al Sol, o a la Tierra, para conocer la unidad astronómica. En 1659 Christian Huygens midió el ángulo que subtiende Marte en el cielo y, atribuyendo un valor al diámetro de este planeta, estimó que la unidad astronómica debía ser 160 millones de kilómetros, es decir, siete veces mayor que lo estimado por Kepler, pero de hecho menos del 10 % por encima del valor real. Sin embargo esta medición no era aceptada, ya que, como el mismo Huygens reconoció, todo dependía del valor que uno atribuyera al tamaño de Marte. Curiosamente, Huygens adivinó con notable exactitud el tamaño de Marte.

Se conocía otro método más fiable, pero que requería mediciones muy difíciles de realizar: el método de la paralaje. Si dos personas situadas en puntos alejados de la Tierra, digamos en París (Francia) y en Cayena (Guayana Francesa), observan simultáneamente la posición de un planeta en el cielo en relación a las estrellas de fondo, sus mediciones dan una pequeña diferencia que corresponde al ángulo que subtendería la línea París-Cayena vista desde el planeta. Conociendo este ángulo, y la distancia París-Cayena, se puede deducir el valor de la unidad astronómica. En la práctica existían tres dificultades: primero, no se conocían bien las distancias sobre la Tierra; segundo, la medición del tiempo no era lo suficientemente precisa como para permitir mediciones simultáneas entre puntos muy alejados; y tercero, la medición de la posición aparente del planeta en el cielo debía ser muy precisa. Pasó más de medio siglo antes de que fuera posible medir la paralaje de un planeta: en 1672 Jean Richer viajó a Cayena para medir la posición de Marte en el cielo en el mismo instante en que sus colegas en París hacían lo mismo. Richer y sus colegas estimaron el valor en 140 millones de kilómetros.

Con el tiempo se desarrollaron métodos más precisos y fiables para estimar la unidad astronómica; en particular, el propuesto por el matemático escocés James Gregory y por el astrónomo británico Edmund Halley (el mismo del cometa), se basa en mediciones del tránsito de Venus o Mercurio sobre el disco solar y fue empleado hasta principios del siglo . Las mediciones contemporáneas se hacen con técnicas láser o de radar y dan el valor 149 597 870 km, con un error aparente de uno o dos kilómetros.

Newton reformuló la tercera ley de Kepler. Un planeta de masa "m", orbitando el sol de masa "M", en una elipse con semi-eje mayor "a" y con un período sideral "T", verifica la ecuación

El matemático alemán Carl Friedrich Gauss (1777-1855) usó para sus cálculos de la dinámica del sistema solar como unidad de masa la masa solar, como unidad de tiempo el día solar medio y como unidad de distancia el semieje mayor de la órbita de la Tierra. Utilizando estas unidades, la ecuación anterior se escribe como

Donde "k" se conoce como la constante gravitacional gaussiana. Gauss utilizó los valores estimados en su época

Gauss reconoció que el problema con esta definición era que cuando se determinaran con mejor precisión el año sidéreo y la masa del Sol, el valor de "k" cambiaría. En 1938, la Unión Astronómica Internacional (UAI) adoptó el valor de la constante gravitacional gaussiana (y la unidad astronómica de ella derivada) como una definición en astronomía. Sin embargo, con la precisión de las medidas actuales, se sabe que el año sidéreo es 56 segundos más corto que el valor conocido en tiempos de Gauss, y que el semieje mayor de la órbita real de la Tierra es unos 17 km más pequeño que la unidad astronómica.

En la asamblea general de la Unión Astronómica Internacional de agosto de 2012 en Pekín se resolvió dejar sin efecto la definición gaussiana y darle a la unidad astronómica el valor actual de 149 597 870 700 metros.


Algunos factores de conversión:


La siguiente tabla muestra algunas distancias tomadas en unidades astronómicas. Incluye algunos ejemplos con distancias aproximadas porque son demasiado pequeños o están demasiado alejados. Las distancias van cambiando con el tiempo. También se puede ordenar según aumente la distancia.



</doc>
<doc id="2886" url="https://es.wikipedia.org/wiki?curid=2886" title="Urano">
Urano

El término Urano puede referirse a:



</doc>
<doc id="2887" url="https://es.wikipedia.org/wiki?curid=2887" title="Unix">
Unix

Unix (registrado oficialmente como UNIX®) es un sistema operativo portable, multitarea y multiusuario; desarrollado en 1969 por un grupo de empleados de los laboratorios Bell de AT&T.

El sistema, junto con todos los derechos fueron vendidos por AT&T a Novell, Inc. Esta vendió posteriormente el software a Santa Cruz Operation en 1995, y esta, a su vez, lo revendió a Caldera Software en 2001, empresa que después se convirtió en el grupo SCO. Sin embargo, Novell siempre argumentó que solo vendió los derechos de uso del software, pero que retuvo el "copyright" sobre "UNIX®". En 2010, y tras una larga batalla legal, esta ha pasado nuevamente a ser propiedad de Novell.

Solo los sistemas totalmente compatibles y que se encuentran certificados por la especificación Single UNIX Specification pueden ser denominados "UNIX®" (otros reciben la denominación «similar a un sistema Unix» o «similar a Unix»). En ocasiones, suele usarse el término "Unix tradicional" para referirse a Unix o a un sistema operativo que cuenta con las características de UNIX Versión 7 o UNIX System V o unix versión 6. 

A finales de la década de 1960 el Instituto Tecnológico de Massachusetts, los Laboratorios Bell de AT&T y General Electric trabajaban en un sistema operativo experimental llamado Multics (Multiplexed Information and Computing Service), desarrollado para ejecutarse en una computadora central (mainframe) modelo GE-645. El objetivo del proyecto era desarrollar un gran sistema operativo interactivo que contase con muchas innovaciones, entre ellas mejoras en las políticas de seguridad. El proyecto consiguió dar a luz versiones para producción, pero las primeras versiones contaban con un pobre rendimiento.
Los laboratorios Bell de AT&T decidieron desvincularse y dedicar sus recursos a otros proyectos.

Uno de los programadores de los laboratorios Bell, Ken Thompson, siguió trabajando para la computadora GE-645 y escribió un juego llamado "Space Travel", (Viaje espacial). Sin embargo, descubrió que el juego era lento en la máquina de General Electric y resultaba realmente caro, algo así como 75 dólares de EE.UU. por cada partida.

De este modo, Thompson escribió nuevamente el programa, con ayuda de Dennis Ritchie, en lenguaje ensamblador, para que se ejecutase en una computadora DEC PDP-7. Esta experiencia, junto al trabajo que desarrolló para el proyecto Multics, condujo a Thompson a iniciar la creación de un nuevo sistema operativo para la DEC PDP-7. Thompson y Ritchie lideraron un grupo de programadores, entre ellos a Rudd Canaday, en los laboratorios Bell, para desarrollar tanto el sistema de ficheros como el sistema operativo multitarea en sí. A lo anterior, agregaron un intérprete de órdenes (o intérprete de comandos) y un pequeño conjunto de programas. El proyecto fue bautizado UNICS, como acrónimo Uniplexed Information and Computing System, pues solo prestaba servicios a dos usuarios (de acuerdo con Andrew Tanenbaum, era solo a un usuario). La autoría de esta sigla se le atribuye a Brian Kernighan, ya que era un "hack" de Multics. Dada la popularidad que tuvo un juego de palabras que consideraba a UNICS un sistema MULTICS castrado (pues "eunuchs", en inglés, es un homófono de UNICS), se cambió el nombre a UNIX, dando origen al legado que llega hasta nuestros días.

Hasta ese instante, no había existido apoyo económico por parte de los laboratorios Bell, pero eso cambió cuando el Grupo de Investigación en Ciencias de la Computación decidió utilizar UNIX en una máquina superior a la PDP-7. Thompson y Ritchie lograron cumplir con la solicitud de agregar herramientas que permitieran el procesamiento de textos a UNIX en una máquina PDP-11/20, y como consecuencia de ello consiguieron el apoyo económico de los laboratorios Bell. Fue así como por vez primera, en 1970, se habla oficialmente del sistema operativo UNIX ejecutado en una PDP-11/20. Se incluía en él un programa para dar formato a textos (runoff) y un editor de texto. Tanto el sistema operativo como los programas fueron escritos en el lenguaje ensamblador de la PDP-11/20. Este "sistema de procesamiento de texto" inicial, compuesto tanto por el sistema operativo como de runoff y el editor de texto, fue utilizado en los laboratorios Bell para procesar las solicitudes de patentes que ellos recibían. Pronto, runoff evolucionó hasta convertirse en troff, el primer programa de edición electrónica que permitía realizar composición tipográfica. El 3 de noviembre de 1971 Thomson y Ritchie publicaron un manual de programación de UNIX (título original en inglés: "UNIX Programmer's Manual").

En 1972 se tomó la decisión de escribir nuevamente UNIX, pero esta vez en el lenguaje de programación C. Este cambio significaba que UNIX podría ser fácilmente modificado para funcionar en otras computadoras (de esta manera, se volvía portable) y así otras variaciones podían ser desarrolladas por otros programadores. Ahora, el código era más conciso y compacto, lo que se tradujo en un aumento en la velocidad de desarrollo de UNIX. AT&T puso a UNIX a disposición de universidades y compañías, también al gobierno de los Estados Unidos, a través de licencias.
Una de estas licencias fue otorgada al Departamento de Computación de la Universidad de California, con sede en Berkeley. En 1975 esta institución desarrolló y publicó su propio sucedáneo de UNIX, conocida como "Berkeley Software Distribution" (BSD), que se convirtió en una fuerte competencia para la familia UNIX de AT&T.

Mientras tanto, AT&T creó una división comercial denominada "Unix Systems Laboratories" para la explotación comercial del sistema operativo. El desarrollo prosiguió, con la entrega de las versiones 4, 5 y 6 en el transcurso de 1975. Estas versiones incluían los "pipes" o "tuberías", lo que permitió dar al desarrollo una orientación modular respecto a la base del código, consiguiendo aumentar aún más la velocidad de desarrollo. Ya en 1978, cerca de 600 o más máquinas estaban ejecutándose con alguna de las distintas encarnaciones de UNIX.

La versión 7, la última versión del UNIX original con amplia distribución, entró en circulación en 1979. Las versiones 8, 9 y 10 se desarrollaron durante la década de 1980, pero su circulación se limitó a unas cuantas universidades, a pesar de que se publicaron los informes que describían el nuevo trabajo. Los resultados de esta investigación sirvieron de base para la creación de Plan 9 from Bell Labs, un nuevo sistema operativo portable y distribuido, diseñado para ser el sucesor de UNIX en investigación por los Laboratorios Bell.

AT&T entonces inició el desarrollo de UNIX System III, basado en la versión 7, como una variante de tinte comercial y así vendía el producto de manera directa. La primera versión del sistema III se lanzó en 1981. A pesar de lo anterior, la empresa subsidiaria Western Electric seguía vendiendo versiones antiguas de Unix basadas en las distintas versiones hasta la séptima. Para finalizar con la confusión con todas las versiones divergentes, AT&T decidió combinar varias versiones desarrolladas en distintas universidades y empresas, dando origen en 1983 al Unix System V Release 1. Esta versión presentó características tales como el editor Vi y la biblioteca curses, desarrolladas por Berkeley Software Distribution en la Universidad de California, Berkeley. También contaba con compatibilidad con las máquinas VAX de la compañía DEC.

Hacia 1991, un estudiante de ciencias de la computación de la Universidad de Helsinki, llamado Linus Torvalds desarrolló un núcleo para computadoras con arquitectura x86 de Intel que emulaba muchas de las funcionalidades de UNIX y lo lanzó en forma de código abierto en 1991, bajo el nombre de Linux. En 1992, el Proyecto GNU comenzó a utilizar el núcleo Linux junto a sus programas.

En 1993, la compañía Novell adquirió la división Unix Systems Laboratories de AT&T junto con su propiedad intelectual. Esto ocurrió en un momento delicado en el que "Unix Systems Laboratories" disputaba una demanda en los tribunales contra BSD por infracción de los derechos de copyright, revelación de secretos y violación de marca de mercado.

Aunque BSD ganó el juicio, Novell descubrió que gran parte del código de BSD fue copiada ilegalmente en UNIX System V. En realidad, la propiedad intelectual de Novell se reducía a unos cuantos archivos fuente. La correspondiente contra-demanda acabó en un acuerdo extrajudicial cuyos términos permanecen bajo secreto a petición de Novell.

A finales de 1993, Novell vendió su división UNIX comercial(es decir, la antigua Unix Systems Laboratories) a "Santa Cruz Operation" (SCO) reservándose, aparentemente, algunos derechos de propiedad intelectual sobre el software. Xinuos (antes UnXis) continúa la comercialización de System V en su producto UnixWare tras adquirir a SCO en abril de 2011.


Las interrelaciones entre estas familias son las siguientes, aproximadamente en orden cronológico:


UNIX® es una marca registrada de Novell, después de una disputa con The Open Group en Estados Unidos y otros países. Esta marca solo se puede aplicar a los sistemas operativos que cumplen la "Single Unix Specification" de esta organización y han pagado las regalías establecidas.

En la práctica, el término UNIX se utiliza en su acepción de familia. Se aplica también a sistemas multiusuario basados en POSIX (tales como GNU/Linux, Mac OS X [el cual, en su versión 10.5 ya ha alcanzado la certificación UNIX], FreeBSD, NetBSD, OpenBSD), los cuales no buscan la certificación UNIX por resultar cara para productos destinados al consumidor final o que se distribuyen libremente en Internet. En estos casos, el término se suele escribir como "UN*X", "UNIX*", "*NIX", o "*N?X". Para referirse a ellos (tanto a Unix, como a los sistema basados en Unix/POSIX) también se utiliza "Unixes", pero "Unices" (que trata la palabra "Unix" como un nombre latino de la tercera declinación) es asimismo popular.

A lo largo de la historia ha surgido una gran multitud de implementaciones comerciales de UNIX. Sin embargo, un conjunto reducido de productos ha consolidado el mercado y prevalece gracias a un continuo esfuerzo de desarrollo por parte de sus fabricantes. Los más importantes son:

Existen sistemas operativos basados en el núcleo Linux, y el conjunto de aplicaciones GNU (también denominado GNU/Linux), entre las más utilizadas encontramos:



También son populares los sistemas operativos descendientes del 4.4BSD:


Las siguientes implementaciones de UNIX tienen importancia desde el punto de vista histórico, no obstante, actualmente están en desuso:


Algunos comandos básicos de UNIX son:

Esta es una lista de los sesenta comandos de usuario de la sección 1 de la Primera Edición:

Otros comandos





</doc>
<doc id="2888" url="https://es.wikipedia.org/wiki?curid=2888" title="Urticaceae">
Urticaceae

Urticaceae es una familia de plantas pertenecientes al orden Rosales.

Son plantas herbáceas, anuales o perennes, raras veces leñosas (en los trópicos), frecuentemente con pelos urticantes (cistolitos). Sin látex. Hojas simples, opuestas o alternas, con frecuencia estipuladas. Flores inconspicuas (verdosas), generalmente unisexuales, de disposición monoica o dioica, monoclamídeas, tetrámeras o pentámeras; gineceo súpero, unicarpelar (un estigma), con un óvulo; reunidas en inflorescencias axilares en panículas, cimas o amentos. Fruto en aquenio o núcula, unas 550 especies, propias sobre todo de las regiones cálidas.

La familia se subdivide en 5 trIbus:


</doc>
<doc id="2889" url="https://es.wikipedia.org/wiki?curid=2889" title="Ulmaceae">
Ulmaceae

Ulmaceae, las ulmáceas, son una familia del Orden Rosales.

Tienen hojas simples alternas, a menudo asimétricas, con estípulas prontamente caducas. Flores en su mayoría hermafroditas (hay plantas poliginas); monoclámideas; cáliz con 4 - 9 sépalos soldados; con 4 - 6 estambres episépalos; gineceo bicarpelar, sincárpico, con dos estigmas patentes; comúnmente agrupadas. Fruto en sámara o en drupa redondeada. Unas 140 especies de las zonas templadas subtropicales y tropicales del hemisferio norte, dos géneros en Europa.

El nombre de la familia Ulmaceae proviene desde 1815 Charles François Brisseau de Mirbel en "Elem. Physiol. Veg. Bot.", 2, S. 905. El género tipo es "Ulmus" L. La familia de las Ulmáceas durante mucho tiempo sólo tuvo dos subfamilias: Ulmoideae y Celtidoideae dentro del ordo Urticales. Estudios de genética molecular revelaron que las seis o siete familias y 2600 especies del anterior ordo "Urticales" pertenecían al orden Rosales. Resultó que la subfamilia "Celtidoideae", con los géneros "Aphananthe, Celtis, Gironniera, Pteroceltis" y "Trema" no se asocian ya a la subfamilia "Ulmoideae", sino que esos géneros que se enmarcaban en la subfamilia "Celtidoideae" pertenecen en realidad a las "Cannabaceae".

Familias relacionadas dentro del orden Rosales:

En la familia de Ulmáceas hay actualmente sólo siete géneros con cerca de 35 especies


</doc>
<doc id="2890" url="https://es.wikipedia.org/wiki?curid=2890" title="Urticales">
Urticales

Urticales fue considerado como un orden de plantas herbáceas y leñosas. 

Tras la clasificación realizada por el sistema de clasificación APG III de 2009, las plantas correspondientes a este orden, incluyendo cuatro familias, pasaron a formar parte del orden Rosales y de la subclase Rosidae. La razón del cambio de subclase se debe al abandono del mismo taxón Hamamelidae, debido también al sistema APG III.
A las siguientes cuatro familias se las considera por tanto dentro del orden Rosales, y debido a la relación que poseen entre ellas tras estudiar los análisis filogenéticos de sus secuencias de ADN, se las denomina también como rósidas urticales.

Hay que destacar otras dos familias consideradas con anterioridad dentro de este taxón: Celtidaceae y Cecropiaceae:



Flores variables, hermafroditas y unisexuales; gineceo frecuentemente súpero, de carpelos abiertos, unilocular; en general anemófilas y solitarias, si hay inflorescencias, estas son aisladas y variables, raramente en amentos monoclamídeos. Los frutos son drupas denominadas también núculas. Chalazogamia hasta porogamia.


</doc>
<doc id="2891" url="https://es.wikipedia.org/wiki?curid=2891" title="Unión Europea">
Unión Europea

La Unión Europea (UE) es una comunidad política de derecho constituida en régimen sui géneris de organización internacional nacida para propiciar y acoger la integración y gobernanza en común de los Estados y los pueblos de Europa. Está compuesta por veintisiete Estados europeos y fue establecida con la entrada en vigor del Tratado de la Unión Europea (TUE) el 1 de noviembre de 1993.

Con ese acto, la supraestructura «Unión Europea» aunaba y se fundaba sobre las tres Comunidades Europeas preexistentes —la Comunidad Europea del Carbón y del Acero (CECA), la Comunidad Europea de la Energía Atómica (Euratom) y la Comunidad Económica Europea (CEE/CE)— y les añadía la política exterior común y la cooperación judicial y policial, formando un sistema complejo conocido como «los tres pilares». Sin embargo, con la entrada en vigor el 1 de diciembre de 2009 del Tratado de Lisboa, la Unión Europea sucedió, por completo aunque con ciertas particularidades, a las Comunidades Europeas y asumió con ello su personalidad jurídica única como sujeto de derecho internacional.

La Unión Europea ha desarrollado un sistema jurídico y político, el comunitario europeo, único en el mundo, que se rige por mecanismos y procedimientos de funcionamiento interno complejos, que se han extendido y evolucionado a lo largo de su historia hasta conformar, en la actualidad, un sistema híbrido de gobierno transnacional difícilmente homologable que combina elementos próximos a la cooperación multilateral, si bien fuertemente estructurada e institucionalizada, con otros de vocación netamente supranacional, regidos ambos por una dinámica de integración regional muy acentuada.

Todo esto desemboca en una peculiarísima comunidad de Derecho, cuya naturaleza jurídica y política es muy discutida, si bien sus elementos fundacionales y su evolución histórica, todavía abierta, apuntan, en el presente, a una especial forma de moderna confederación o gobernanza supranacional, acusadamente institucionalizada y con una inspiración histórico-política de vocación federal —en el sentido de un federalismo internacional nuevo, no de un Estado federal clásico— que se detecta con cierta claridad en ámbitos como la ciudadanía europea, los principios de primacía y efecto directo que le son aplicables a su ordenamiento jurídico en relación con los ordenamientos nacionales, el sistema jurisdiccional o la unión monetaria (el sistema del euro).

La Unión Europea, y antes las Comunidades, promueve la integración continental por medio de políticas comunes que abarcan distintos ámbitos de actuación, en su origen esencialmente económicos y progresivamente extendidos a ámbitos indudablemente políticos. Para alcanzar sus objetivos comunes, los estados de la Unión le atribuyen a esta determinadas competencias, ejerciendo una soberanía en común o compartida que se despliega a través de los cauces comunitarios.

La Unión Europea se rige por un sistema interno en régimen de democracia representativa. Sus instituciones son siete: el Parlamento Europeo, el Consejo Europeo, el Consejo, la Comisión Europea, el Tribunal de Justicia de la Unión Europea, el Tribunal de Cuentas y el Banco Central Europeo. El Consejo Europeo ejerce funciones de orientación política general y de representación exterior, y nombra a los jefes de las altas instituciones constitucionales; el Parlamento Europeo y el Consejo ejercen la potestad legislativa en igualdad de condiciones, tomando decisiones conjuntas —a excepción de los procedimientos legislativos especiales, donde el Parlamento desempeña un papel meramente consultivo—; la Comisión o Colegio de Comisarios aplica el Derecho de la Unión, supervisa su cumplimiento y ejecuta sus políticas, y a ella corresponde en exclusiva la iniciativa legislativa ante el Parlamento y la Comisión; el Tribunal de Justicia ejerce las labores jurisdiccionales supremas en el sistema jurídico comunitario; el Tribunal de Cuentas supervisa y controla el buen funcionamiento y la adecuada administración de las finanzas y de los fondos comunitarios; y el Banco Central Europeo dirige y aplica la política monetaria única de la zona euro.

La Unión cuenta también con otros órganos, instancias y organismos de funciones y atribuciones diversas, como el Comité Económico y Social, el Comité de las Regiones, el Defensor del Pueblo Europeo, el Alto Representante de la Unión para Asuntos Exteriores y Política de Seguridad, entre otros.

En 2012 la Unión Europea ganó el Nobel de la Paz, que fue otorgado por unanimidad de todos los miembros del jurado, «por su contribución durante seis décadas al avance de la paz y la reconciliación, la democracia, y los derechos humanos en Europa». En 2017 fue galardonada con el Premio Princesa de Asturias de la Concordia por lograr «el más largo período de paz de la Europa moderna, colaborando a la implantación y difusión en el mundo de valores como la libertad, los derechos humanos, y la solidaridad».

Tras la salida del Reino Unido el 31 de enero de 2020, el club comunitario se ve abocado a un proceso de refundación.

Con la entrada en vigor del Tratado de Lisboa, los símbolos de la UE como la bandera, el lema, el himno o el Día de Europa no son jurídicamente vinculantes, aunque todos ellos se encuentran en uso. Pese a esto, dieciséis países miembros declararon su lealtad a los símbolos de la Unión Europea en una declaración anexa al tratado.




Tras el final de la Segunda Guerra Mundial, Europa se encontraba sumida en la devastación. Alemania estaba destrozada, en términos de pérdidas de vidas humanas y daños materiales. Si bien Francia y Reino Unido resultaron oficialmente vencedoras frente a Alemania en el conflicto, ambos países sufrieron importantes pérdidas (aunque menores que las de Alemania) que afectaron gravemente a sus economías y su prestigio a nivel mundial.
La declaración de guerra de Francia y Reino Unido a la Alemania nazi tuvo lugar en septiembre de 1939. Una vez finalizado el conflicto en Europa el 8 de mayo de 1945, el régimen alemán fue responsabilizado por el inicio de la guerra, ya que su política expansionista le había llevado a ocupar y en algunos casos anexar territorios de otros países del continente. Alemania, que perdió una parte considerable de su territorio anterior a la guerra, fue ocupada por ejércitos extranjeros que dividieron su superficie territorial en cuatro partes, tal y como se consensuó en la Conferencia de Yalta.

En los años posteriores, los resentimientos y la desconfianza entre las naciones europeas, dificultaban una reconciliación. En este contexto el ministro francés de asuntos exteriores Robert Schuman defendió decididamente la creación de Alemania Occidental, resultado de la unión de las tres zonas de ocupación controladas por las democracias occidentales, dejando de lado la zona ocupada por la URSS. Schuman, de origen germano-luxemburgués, había poseído las tres nacionalidades (francesa, alemana, luxemburguesa) durante diferentes etapas de su vida. Este hecho le hizo comprender la complejidad de los conflictos europeos y desarrollar pronto un interés por la unificación europea.

En 1946, Winston Churchill dio un discurso en la Universidad de Zúrich, considerado por muchos como el primer paso hacia la integración durante la posguerra. Aunque, generalmente se considera que el verdadero primer paso se dio el 9 de mayo de 1950, cinco años después de la rendición del régimen nazi, cuando Schuman lanzó un llamamiento a Alemania Occidental y a los países europeos que lo deseasen para que sometieran bajo una única autoridad común el manejo de sus respectivas producciones de acero y carbón. Este discurso, conocido como Declaración Schuman, fue acogido de manera dispar dentro de los gobiernos europeos y marcó el inicio de la construcción europea, al ser la primera propuesta oficial concreta de integración en Europa. El hecho consistía en que al someter las dos producciones indispensables de la industria armamentística a una única autoridad, los países que participaran en esta organización encontrarían una gran dificultad en el caso de querer iniciar una guerra entre ellos.

La declaración marcó el inicio de la integración de los estados europeos como un movimiento en contraposición a la anterior tendencia nacionalista y las tensas rivalidades que ocasionó entre los estados de Europa. Esta nueva realidad fue propiciada en gran medida por el fin de la tradicional hegemonía europea en el mundo tras la II Guerra Mundial, que concienció a los europeos de su propia debilidad ante el surgimiento de dos nuevas superpotencias, Estados Unidos y la URSS, que tenían un poder superior al del heterogéneo grupo de estados europeos. Además, las consecuencias del conflicto favorecieron el deseo entre los ciudadanos de crear un continente más libre y justo en el que las relaciones entre países se desarrollaran de forma pacífica para evitar por todos los medios un nuevo enfrentamiento entre los estados europeos.

La propuesta de Robert Schuman fue acogida de forma entusiasta por el canciller de la República Federal de Alemania Konrad Adenauer. En la primavera de 1951, se firma en París el Tratado que institucionaliza la Comunidad Europea del Carbón y del Acero (CECA), concretando la propuesta de Schuman. Alemania, Francia, Italia, Países Bajos, Bélgica y Luxemburgo (conocidos como “los seis”), logran un entendimiento que favorece el intercambio de las materias primas necesarias en la siderurgia, acelerando de esta forma la dinámica económica, con el fin de dotar a Europa de una capacidad de producción autónoma. Este tratado fundador buscaba aproximar vencedores y vencidos europeos al seno de una Europa que a medio plazo pudiese tomar su destino en sus manos, haciéndose independiente de entidades exteriores. El tratado expiró en 2002, a pesar de que su función quedó obsoleta tras la fusión de los órganos ejecutivos y legislativos en el seno de la Comunidad Europea, que adquirió personalidad jurídica, y también gracias al Acta Única Europea de 1986.

En mayo de 1952, ya en plena Guerra fría, se firmó en París un tratado estableciendo la Comunidad Europea de Defensa (CED), que permitía el armamento de Alemania Occidental en el marco de un ejército europeo. Cinco miembros de la CECA ratificaron el tratado, pero en agosto de 1954, los parlamentarios franceses lo rechazaron, como consecuencia de la oposición conjunta de gaullistas y comunistas. Es así que el antiguo Tratado de Bruselas de 1948 es modificado para crear la Unión Europea Occidental (UEO) que fue hasta la entrada en vigor del Tratado de Ámsterdam en 1999 la única organización del continente encargada de la defensa y la seguridad europea. Aunque reforzó el antiguo tratado, la UEO fue una entidad a la sombra de la OTAN, pese a lo cual se encargó durante su existencia de la defensa de los países europeos ante un hipotético ataque.

Un impulso de importancia mayor llega en 1957 con la firma de los Tratados de Roma. "Los seis" deciden avanzar en la cooperación en los dominios económico, político y social. La meta planteada fue lograr un “mercado común” que permitiese la libre circulación de personas, mercancías y de capitales. La Comunidad Económica Europea (CEE) es la entidad internacional, de tipo supranacional, dotada de una capacidad autónoma de financiación institucionalizada por este tratado. Este documento formó una tercera comunidad de duración indefinida, el Euratom.

En 1965, se firma un tratado que fusiona los ejecutivos de las tres comunidades europeas por medio de la creación de la Comisión Europea (CE) y el Consejo de la Unión Europea (CUE). El Acta Única Europea firmada en febrero de 1986 entró en aplicación en julio de 1987, y tuvo por misión el redinamizar la construcción europea, fijando la consolidación del mercado interior en 1993 y permitiendo la libre circulación igualmente de capitales y servicios. Por este tratado, las competencias comunitarias son ampliadas a los dominios de la investigación y el desarrollo tecnológico, medio ambiente y política social. El Acta Única consagró también la existencia del Consejo Europeo, que reúne los jefes de estado y de gobierno e impulsa una iniciativa común en materia de política exterior (la Cooperación Política Europea) así como una cooperación en materia de seguridad.

El Tratado de Maastricht o de la Unión Europea, firmado en febrero de 1992 y en vigor a partir de 1993, introdujo una nueva estructura institucional, la cual se mantuvo hasta la entrada en vigor del Tratado de Lisboa. Dicha estructura institucional estaba compuesta por los conocidos tres pilares de la Unión Europea: el primer pilar era el pilar comunitario, que correspondía a las tres comunidades (la Comunidad Europea, la Comunidad Europea de la Energía Atómica y la antigua Comunidad Europea del Carbón y del Acero); el segundo era el pilar correspondiente a la política exterior y de seguridad común, que estaba regulada en el título V del Tratado de la Unión Europea; y el tercero era el pilar correspondiente a la cooperación policial y judicial en materia penal, cubierta por el título VI del Tratado de la Unión Europea. Estos tres pilares funcionaban siguiendo procedimientos de decisión diferentes, ya que el primer pilar funcionaba mediante el procedimiento comunitario, mientras que los otros dos se regían por el procedimiento intergubernamental. El Tratado de Maastricht también creó la ciudadanía europea y permitió circular y residir libremente en los países de la comunidad, así como el derecho de votar y ser elegido en un estado de residencia para las elecciones europeas o municipales. Con este tratado también se decidió la creación de una moneda única europea, el Euro, que entraría en circulación en 2002 bajo control del Banco Central Europeo.

A lo largo de estos años, la CEE/UE comenzó a expandirse por el continente europeo, fundamentalmente entre los países de la Europa occidental: Reino Unido, Irlanda y Dinamarca en 1973; Grecia en 1981; España y Portugal en 1986; Alemania oriental en 1990; y Austria, Finlandia y Suecia en 1995.

En 1999, entró en vigor el Tratado de Ámsterdam que recogía los principios de libertad, democracia y respeto a los derechos humanos, incluyendo explícitamente el principio de desarrollo sostenible. Dos años después se firmó el Tratado de Niza, que entraría en vigor en 2003. Mientras tanto, en 2002, se extinguió la CECA tras finalizar su periodo de validez (50 años), y su ámbito de actuación quedó englobado en el de la Comunidad Europea.

El 1 de mayo de 2004 tuvo lugar la mayor ampliación que se ha dado en la Unión Europea, con la entrada de 10 nuevos miembros de Europa oriental: Estonia, Letonia, Lituania, Polonia, República Checa, Hungría, Eslovaquia, Eslovenia, Malta y Chipre. Más tarde, el 29 de octubre de 2004 se firmó en Roma el tratado constitucional. La ratificación del tratado fue iniciada por la aprobación del Parlamento, pero algunos estados convocaron referendos en 2005. El primero fue el que se celebró en España, donde el documento fue aprobado con el 76,73 % de apoyo. Sin embargo, la ratificación alcanzó un obstáculo importante cuando los votantes de Francia y los Países Bajos rechazaron el documento. Esta ratificación en gran medida se detuvo, con solo unos pocos estados tratando de aprobarlo aún. Luxemburgo siguió adelante con su voto y aprobó la constitución en un 57 %. Esto no cambió las cosas, sin embargo, y los dirigentes anunciaron que entraban en un "período de reflexión" sobre el rechazo.

A comienzos de 2007 se incorporaron Rumania y Bulgaria a la Unión Europea, mientras que el 25 de marzo de 2007 (en el 50.º aniversario de la firma de los Tratados de Roma) los líderes europeos pusieron fin formalmente al "período de reflexión" con la firma de la Declaración de Berlín. La declaración tenía por objeto dar un nuevo impulso a la búsqueda de un nuevo acuerdo institucional antes de realizar las elecciones europeas de 2009. Adentrado ya 2007, el Consejo Europeo acordó que la Constitución había fracasado, a pesar de que la mayoría de las propuestas que incluía el texto se incluyeron posteriormente en la reforma de los tratados de la Unión, en contraposición a la constitución, la cual iba a reemplazar todos los tratados anteriores. De este modo, el 13 de diciembre de 2007, se firmó el conocido como Tratado de Lisboa.

Este tratado tenía como objetivo mejorar el funcionamiento de la Unión Europea mediante la modificación del Tratado de Maastricht y el Tratado constitutivo de la Comunidad Europea (Tratado de Roma). Algunas de las reformas más importantes que introdujo el Tratado de Lisboa fueron la reducción de las posibilidades de estancamiento en la toma de decisiones del Consejo de la Unión Europea mediante el voto por mayoría cualificada, un Parlamento Europeo con mayor peso mediante la extensión del procedimiento de decisión conjunta con el Consejo de la UE, la eliminación de los para entonces obsoletos tres pilares de la Unión Europea, y la creación de las figuras de Presidente del Consejo Europeo y Alto Representante de la Unión para Asuntos Exteriores y Política de Seguridad para dotar de una mayor coherencia y continuidad a las políticas de la UE. El Tratado de Lisboa, que entró en vigor el 1 de diciembre de 2009 también hizo que la Carta de los Derechos Fundamentales de la Unión Europea fuese jurídicamente vinculante para los Estados miembros.

Por otra parte, la Gran Recesión iniciada en 2008 y la Crisis del euro afectaron la economía de la mayoría de los Estados miembros, pese a lo cual Croacia consiguió convertirse el 1 de julio de 2013 en el miembro número 28 de la Unión. En medio de la recesión, la UE y sus Estados miembros enfrentaron la Crisis migratoria en Europa que puso a prueba el sistema europeo común de asilo y el espacio Schengen.

En 2020 la UE ha afrontado la salida del Reino Unido de la Unión Europea "Brexit", efectiva desde enero de ese año. Además, la pandemia de enfermedad por coronavirus ha sumido a la Unión en una crisis sin precedentes. La situación de crisis sanitaria desatada en varios de sus Estados miembros ha abierto un périodo de incertidumbre que previsiblemente marcará la evolución del proyecto de refundación de la Unión Europea. 

<noinclude>

La Unión Europea está formada por 27 países europeos soberanos independientes que se conocen como los estados miembros. La Unión fue fundada por seis países de Europa occidental (Francia, Alemania, Italia, Bélgica, Países Bajos, y Luxemburgo) y se amplió en seis ocasiones, por los cuatro puntos cardinales de la geografía europea. A diferencia de los estados de los Estados Unidos, los estados miembros de la Unión Europea no están obligados a una forma republicana de gobierno. La Unión está compuesta de veintiuna repúblicas y seis monarquías, de las cuales cinco son reinos y una es un ducado (Luxemburgo).

En el territorio de la Unión Europea, los tres países más extensos son 1.º Francia, 2.º España y 3.º Suecia; y los tres países menos extensos son Malta (con una superficie más de dos mil veces inferior al país más grande de la Unión Europea), Luxemburgo y Chipre (ver ). En cuanto a jurisdicción sobre el mar, un país de la Unión Europea, Francia (con más de 11 millones de km²), tiene la más extensa zona económica exclusiva del mundo.

El territorio de la Unión Europea consiste en el conjunto de territorios de sus 27 Estados miembros con algunas excepciones que se exponen a continuación. El territorio de la UE no es el mismo que el de Europa, ya que, en primer lugar, hay estados europeos que se encuentran fuera de la UE, como Reino Unido, Islandia, Suiza, Noruega y Rusia. Además, ciertos territorios europeos de los estados miembros no forman parte de la UE (por ejemplo, las Islas Feroe). Tampoco forman parte de la UE varios territorios situados fuera del continente asociados a los estados miembros (por ejemplo, Groenlandia, Aruba, las Antillas Neerlandesas y las Colectividades Territoriales francesas). Por el contrario, sí hay ciertos territorios de ultramar que son parte de la UE pese a estar situados fuera del continente europeo, como las Azores, Islas Canarias, Ceuta, Melilla, Guayana Francesa, Guadalupe, Madeira, Martinica, San Martín, La Reunión y Mayotte.

La superficie combinada de los estados miembros de la UE cubre un área de 4 237 473 kilómetros cuadrados. El paisaje, el clima, y la economía de la UE se ven influidas por sus costas, que suman 53 563,9 kilómetros de largo. La UE tiene la tercera costa más larga del mundo después de Australia y Canadá. La combinación de los estados miembros comparte fronteras terrestres con 21 estados no miembros para un total de 13 271 kilómetros, la quinta frontera más larga del mundo.

En Europa la UE tiene fronteras con Noruega, Rusia, Bielorrusia, Ucrania, Moldavia, Reino Unido, Suiza, Liechtenstein, Andorra, Mónaco, San Marino, Ciudad del Vaticano, Turquía, Macedonia del Norte, Bosnia y Herzegovina, Albania, Montenegro y Serbia. Por último, tiene fronteras con: San Martín en el Mar Caribe; Brasil y Surinam en América del Sur y con Marruecos en África.

Algunos estados miembros poseen territorios fuera del continente europeo, los cuales pueden formar parte de la Unión; son denominados generalmente regiones ultraperiféricas o territorios de ultramar. Las ciudades españolas de Ceuta y Melilla, que como tales forman parte de la Unión Europea, se encuentran en África, junto a Marruecos, pero no son consideradas regiones ultraperiféricas por parte de la Unión Europea.

Hasta la entrada en vigor del Tratado de Lisboa no se especificaba cómo un país podía salir de la Unión (aunque Groenlandia, un territorio de Dinamarca, se retiró en 1985 siendo necesario para ello la modificación de varios tratados) pero esto ya no ocurre con el Tratado de Lisboa, ya que este contiene un procedimiento formal para la retirada.

Los miembros de la Unión han crecido desde los seis estados fundadores (Alemania, Bélgica, Francia, Italia, Luxemburgo y Países Bajos) a los 27 que conforman la Unión Europea, tras la salida del Reino Unido:

El único estado miembro que ha abandonado la Unión Europea es:

Hay una serie de territorios de ultramar de los Estados miembros, que son legalmente parte de la Unión Europea, pero tienen ciertas exenciones en función de su lejanía de Europa. Estas "regiones ultraperiféricas" disponen de una aplicación parcial de la ley de la UE y en algunos casos se encuentran fuera del Espacio Schengen. Todos estos territorios utilizan el euro como moneda y son:

Las regiones ultraperiféricas están formadas por varias regiones insulares y una región en el noreste del continente sudamericano, a miles de kilómetros de Europa, pero que integran de derecho la Unión Europea y que forman un grupo peculiar y bien definido en el seno de esta. Esta situación compartida ha llevado a las regiones ultraperiféricas a estrechar lazos de unión y afirmar su voluntad de cooperar entre ellas para lograr que la Unión Europea no olvide las características de estas regiones, para conseguir un desarrollo sostenible a largo plazo, y dotarlas de una posición de igualdad respecto del resto del territorio de la Unión.

Los representantes de las regiones ultraperiféricas, están llevando a cabo unas reuniones periódicas de la Conferencia de Presidentes de las regiones ultraperiféricas, con la idea de preparar un programa de cooperación entre las RUP, el RUP PLUS.

Hay territorios de los Estados miembros en los que no se aplica toda la legislación de la Unión Europea, por lo que su estatus es entonces más próximo al de las regiones ultraperiféricas (RUP), aunque sin tener los fondos estructurales que tienen esos territorios asignados. Al igual que las RUP, y a diferencia de los países y territorios de ultramar, estos territorios si que forman parte del territorio de la Unión Europea.

Un caso especial es el Norte de Chipre, donde la legislación de la Unión Europea no se aplica, pese a ser parte del territorio jurídico de la Unión, ya que sus ciudadanos, los cuales votaron a favor de la adhesión de Chipre a la Unión Europea y de la reunificación de Chipre, también votan a los representantes chipriotas del Parlamento Europeo. Este caso, es una excepción del Tratado de adhesión de Chipre, ya que se está esperando una evolución en las negociaciones entre las dos repúblicas chipriotas. El resultado de esta división de la isla es la creación de una línea de demarcación bajo mandato internacional de la ONU, por el norte y el sur, además de dos zonas de soberanía británica (Acrotiri y Dhekelia).

En el Mar Báltico, las Islas Åland de Finlandia también cuentan con un estatus especial. Estas islas, las cuales disfrutan de una amplia autonomía, tuvieron un referéndum separado del de Finlandia relativo a la adhesión de su país a la UE, en el cual se aprobó su adhesión a la Unión, aunque con algunas excepciones.

Otros casos son los municipios que tienen un estatus especial por razones geográficas o históricas. Estos estatus se refieren al uso del euro y del IVA. Sobre todo destacan los casos de los exclaves alemanes e italianos de Büsingen am Hochrhein y Campione d'Italia, respectivamente. También tiene estatus de territorio especial la localidad italiana de Livigno, la cual pese a no ser un exclave se beneficia del estatus extraterritorial regulado desde los siglos XIX y XX.

En el Mar del Norte, la isla alemana de Heligoland, aunque es parte del territorio de la Unión Europea, está excluido de la unión aduanera y no está sujeta a régimen fiscal alemán. Al igual que las ciudades autónomas de Ceuta y Melilla y el resto de plazas de soberanía española en África, las cuales tienen un estatus especial respecto al IVA, la PAC y la PPC.

El Monte Athos o Estado Monástico Autónomo de la Montaña Sagrada también tiene un estatus especial dentro de República Helénica, ya que el Monte Athos forma parte del espacio Schengen y de la Unión Europea, aunque solo están autorizados a entrar en su territorio aquellos varones que cuenten con una autorización, además de esto, el acceso está prohibido a toda "criatura femenina" (excepto gallinas y gatos).

Finalmente, la isla Clipperton, bajo la administración directa del gobierno francés, por lo que es parte del territorio de la Unión. Sin embargo, la isla no es parte del espacio Schengen y, ya que no hay habitantes permanentes, no hay elecciones al Parlamento Europeo.

Los países y territorios de ultramar son países que no forman parte del territorio comunitario (a diferencia de las regiones ultraperiféricas). Los ciudadanos de los países y territorios de ultramar tienen la nacionalidad de los estados miembros de que dependen (sin embargo, en algunos casos sus ciudadanos no poseen una ciudadanía plena de tales estados).

Existen veinticinco países y territorios de ultramar:


Para que un Estado europeo se incorpore a la Unión Europea debe cumplir unas condiciones económicas y políticas conocidas como los criterios de Copenhague, por haberse tomado el correspondiente acuerdo en el Consejo Europeo de 1993 celebrado en la capital danesa. Los criterios de Copenhague establecen cuándo un país candidato está listo para adherirse a la Unión. Entre los principales criterios están los siguientes:
Hay cinco países candidatos oficiales para formar parte de la UE, los cuales son Turquía (desde 2004), Macedonia del Norte (desde 2005), Montenegro (desde 2010), Serbia (desde 2012) y Albania (desde 2014). Un informe de la Comisión Europea de octubre de 2009 valoró positivamente a Macedonia del Norte (por aquel entonces, Antigua República Yugoslava de Macedonia) para una futura ampliación, pero instó a retrasar el proceso con Turquía.

Actualmente son candidatos potenciales Bosnia y Herzegovina y el territorio de Kosovo (bajo administración interina de la ONU), según lo dispuesto en la resolución 1244 del Consejo de Seguridad de las Naciones Unidas. Aunque Bosnia y Herzegovina ha mostrado su interés en pertenecer al grupo europeo, su adhesión se enfrenta a muchos problemas económicos y políticos, lo cual llevará a que el país lleve a cabo grandes reformas en su sistema económico, político y judicial. El caso de Kosovo es diferente, ya que este territorio cuenta con un estatus especial a la hora de su posible entrada en la Unión Europea, ya que la Comisión Europea lo reconoce como candidato potencial pero no como un país independiente, sino que se refiere a él con la denominación "Kosovo según la Resolución 1244", ya que los Estados miembros se encuentran divididos entre aquellos que lo reconocen como un país independiente y los que no han aceptado la declaración de independencia de Kosovo y lo consideran parte integrante de Serbia.

A pesar de que Noruega se encuentra en el Espacio Económico Europeo, es parte del espacio de Schengen, y que participa en muchos de los programas, instituciones y actividades de la UE, los noruegos impidieron el cumplimiento de la agenda de su gobierno para incorporarse a la UE en dos ocasiones, mediante referéndum, en 1972 y 1994. Del mismo modo Islandia es miembro del EEE y del espacio de Schengen, y debido a una grave crisis económica inició los trámites para ser un miembro de la UE, pero tras las elecciones del 2013, donde vencieron los partidos de centroderecha en contra de la adhesión de Islandia a la UE, el nuevo ministro de Exteriores de Islandia informó al comisario de Ampliación de la decisión del nuevo Ejecutivo de no seguir adelante con las negociaciones de adhesión a la Unión Europea hasta la convocatoria de un referéndum sobre esta cuestión. Algo parecido ocurre con Suiza que, aunque también pertenece al espacio de Schengen, rechazó su adhesión en votaciones realizadas en 1994 y 2001.

En los tratados anteriores al de Lisboa no había previsto ningún procedimiento jurídico que regulara la retirada de los estados. Así por ejemplo, en la Convención de Viena no se preveía ni la denuncia ni el retiro de un estado miembro. El propio Tribunal de Justicia de las Comunidades Europeas reconocía el carácter irrevocable de los compromisos asumidos por los Estados. Sin embargo, la no previsión de un procedimiento jurídico de retirada en los Tratados no es motivo suficiente para impedir que un Estado decida sobre su continuidad en la Unión Europea.

El 23 de junio de 2016 se realizó el Referéndum sobre la permanencia del Reino Unido en la Unión Europea, en el cual la opción de «Salir de la UE» gana con un 51,9 % mientras que «Continuar en la UE» obtiene un 48,1 %, sin embargo, en Escocia, Irlanda del Norte y Gibraltar además de la mayoría de Londres, predominó la opción de la permanencia. Tras los resultados del referéndum, el Primer ministro David Cameron anunció su dimisión del cargo, así, el 13 de julio del mismo año, Theresa May asumió el cargo en su reemplazo, David Cameron argumentó que un liderazgo fresco debe llevar al país a la opción elegida en la votación.
Este referéndum inicia el proceso de retirada del Reino Unido de la Unión Europea, siendo este proceso a largo plazo, estimándolo al menos a unos dos años de tramitación concluyendo el 2019.

Las Instituciones de la Unión Europea son los organismos políticos e instituciones en los que los estados miembros delegan parte de sus poderes y soberanía. Con ello se busca que determinadas decisiones y actuaciones institucionales provengan de órganos de carácter supranacional cuya voluntad se aplica en el conjunto de los estados miembros, desapoderando así a los órganos nacionales de cada país.

Las normas y procedimientos que las instituciones deben seguir se establecen en los tratados, negociados por el Consejo Europeo y en conferencias intergubernamentales y ratificadas por los parlamentos nacionales de cada Estado. El Tratado de Lisboa, modifica nuevamente el Tratado de la Unión Europea, pero también el TCE, que pasaría a llamarse "Tratado sobre el Funcionamiento de la Unión Europea" (TFUE).
La legitimidad de la producción normativa de la Unión tiene una doble vertiente: legitimidad internacional en la acción del Consejo y el Consejo Europeo, por un lado, en tanto que la Unión es una organización internacional regida por Derecho Internacional y convencional; y democrática, por otro, ya que el Parlamento Europeo recoge el principio de formación democrática del Derecho, al ser una Institución cuyos miembros son elegidos en unas elecciones directamente por los ciudadanos.

El Tratado de Lisboa ha consolidado la transformación formal del marco institucional supremo con siete instituciones. Las tres principales en el proceso de toma de decisiones son el Parlamento Europeo, el Consejo de la Unión Europea y la Comisión Europea. También cobra gran importancia el Consejo Europeo como institución que determina la dirección y las prioridades de la Unión.

El Parlamento Europeo es el parlamento de la Unión Europea. Desde 1979, es elegido directamente cada cinco años en las elecciones europeas. Por lo tanto, es la primera institución supranacional directamente elegida del mundo y el órgano representativo de alrededor de 385 millones de personas, quienes constituyen el segundo electorado democrático más grande del mundo (después de la India). El Parlamento es considerado la "primera institución" de la Unión Europea: es mencionado en primer lugar en los tratados y su Presidente tiene preferencia protocolaria sobre todas las demás autoridades a nivel europeo. Comparte con el Consejo la competencia legislativa y presupuestaria, teniendo el control sobre el presupuesto de la Unión Europea. La Comisión Europea, el órgano ejecutivo de la Unión, es responsable ante el Parlamento. En concreto, el Parlamento Europeo elige al Presidente de la Comisión, aprueba (o rechaza) la designación de la Comisión en su conjunto, e incluso puede destituirla como órgano presentando una moción de censura.

El actual Presidente del Parlamento Europeo es el italiano Antonio Tajani que fue elegido en enero de 2017 y que preside una cámara compuesta por una gran variedad de partidos asociados en grupos. Los dos principales grupos del Parlamento Europeo (juntos poseen el 61% de los escaños) son el Grupo del Partido Popular Europeo y el Grupo de la Alianza Progresista de Socialistas y Demócratas.

El Consejo, antes Consejo de la Unión Europea (CUE), comúnmente conocido como Consejo de Ministros, reúne en su seno a los representantes de los Gobiernos de los Estados miembros en distintas formaciones, cuyos intereses nacionales incrusta en el proceso decisorio guiado por la búsqueda de un acuerdo común. El Consejo ejerce junto con el Parlamento Europeo el poder legislativo de la Unión. Ostenta la titularidad formal de importantes potestades ejecutivas, pero cuyo ejercicio debe atribuir por imperativo constitucional a la Comisión. Si bien en los últimos tiempos sus funciones legislativas, antes exclusivas, han ido debilitándose en favor de la igualdad con el Parlamento Europeo, el paralelo declive político de la Comisión parece estar propiciando un desplazamiento de retorno simultáneo al Consejo del centro de gravedad del poder decisorio y ejecutivo, que en ocasiones más parece residir en este órgano que en el propio Ejecutivo comunitario. Ello no obstante, el Consejo aparece cada vez más deslumbrado por su "alter ego" en las alturas, el Consejo Europeo.

El Consejo es, pues, cámara co-legisladora donde se hallan representados los Estados de la Unión a través de sus gobiernos nacionales, asegura su plena participación en igualdad de condiciones, en garantía del llamado "principio de representación nacional". Cuando delibera y decide sobre un acto legislativo, las sesiones del Consejo son públicas.

La Presidencia del Consejo cambia entre estados miembros cada seis meses: de enero a junio y de julio a diciembre. Los Gobiernos trabajan aunando fuerzas para manifestarse con una sola voz en cuestiones de política exterior, asistidos por el Alto Representante de la Unión para Asuntos Exteriores y Política de Seguridad.

El Consejo Europeo, que no debe confundirse con el Consejo de Europa o con el Consejo de la Unión Europea, es un organismo político de carácter predominantemente intergubernamental, conformado por los jefes de Estado o de gobierno de los estados miembros de la Unión Europea junto con el presidente permanente del Consejo y el presidente de la Comisión Europea. Sus funciones son de orientación política y de jefatura colectiva simbólica, fijando las grandes directrices y objetivos de la Unión en los ámbitos más relevantes; la potestad legislativa le está expresamente vedada por los Tratados. Los miembros del Consejo Europeo se citan periódicamente en reuniones conocidas como "Cumbres europeas". Sus oficinas se encuentran en el Justus Lipsus de Bruselas, sede del Consejo de la Unión Europea.

El presidente del Consejo Europeo o informalmente el presidente de la Unión, es una de las más altas posiciones institucionales de la Unión Europea, y sin duda la más simbólica. Su proyección exterior se corresponde con la más alta representación de la UE en el ámbito de la política exterior y de seguridad común, al nivel de los jefes de Estado. Su mandato tiene una duración de dos años y medio renovables una sola vez, sustituyéndose así el viejo sistema rotatorio anterior al Tratado de Lisboa.

La Comisión Europea ("Comisión de las Comunidades Europeas" hasta la entrada en vigor del Tratado de Niza) es la rama ejecutiva de la Unión Europea. Este cuerpo es responsable de proponer la legislación, la aplicación de las decisiones, la defensa de los tratados constitutivos y, en general, se encarga del funcionamiento ordinario de la UE. Se le encomienda la vigilancia en el cumplimiento del interés supremo de la Unión, separado del individual de cada Estado miembro.

Una vez nombrada, la Comisión Europea solo puede ser destituida mediante una moción de censura aprobada por una mayoría de dos tercios en el Parlamento Europeo, lo que la dota de un margen de autonomía superior al de la mayoría de los ejecutivos en sistemas parlamentarios.

Al frente de la Comisión se encuentra el Presidente, que ostenta la máxima representación de la misma y ocupa el primer puesto en la cadena de jerarquía, preeminencia que viene reforzada por la legitimidad democrática directa e individualizada que le aporta al cargo su elección directa por el Parlamento Europeo.

Conforme a su posición principal, el Presidente es también quien está al frente de los demás miembros de la Comisión, los denominados Comisarios, que tienen atribuidas las competencias y los servicios que decida asignarles el Presidente de la Comisión a través de la carta de nombramiento que envía a los titulares y al Parlamento Europeo. Cada Comisario europeo es responsable de los departamentos (direcciones generales y servicios) y, en su caso, Agencias ejecutivas que les asigne el Presidente. Estos ámbitos competenciales se conocen en la jerga comunitaria por el nombre de "carteras", y dado que no tienen estructura administrativa propia, son gestionados por el propio Comisario y, en su nombre, por su gabinete.

Además, de las ya mencionadas, el marco institucional de la Unión Europea cuenta con otras tres instituciones no políticas: el Tribunal de Justicia de la Unión Europea, el Tribunal de Cuentas y el Banco Central Europeo.

Los órganos son asimilables a instituciones de menor rango (aunque no gozan de ese estatuto). Aunque sus funciones son concretas, tienen competencias que van mucho más allá de la simple gestión y gozan de independencia en el ejercicio de sus funciones. Los organismos son estructuras subsidiarias de otras instituciones pero con autonomía funcional, generalmente versada en ámbitos especializados de gestión vicaria.


Las agencias europeas son organismos especializados que se encargan de un aspecto específico (científico, técnico, jurídico o social) de la estructura de la Unión Europea. Se encuentran distribuidas en los países miembros de la UE. Su función es proporcionar cooperación entre los estados miembros y ayuda a sus ciudadanos en las áreas de su competencia.

Estas agencias han contribuido de manera significativa al funcionamiento efectivo de la UE, gracias a su especialización en áreas determinadas de la arquitectura comunitaria. Al ser, en su mayoría, instituciones descentralizadas e independientes han servido para fortalecer el carácter plurinacional de la Unión.

Las agencias se dividen en cuatro categorías englobadas en dos tipos: las agencias “reguladoras” y las “ejecutivas”. Las agencias reguladoras, que se dividen a su vez en tres categorías, se encuentran descentralizadas y se encargan de una competencia concreta sin límite de tiempo; se conocen como agencias de los “tres pilares”. Las últimas, las agencias ejecutivas, se encuentran en la sede de la Comisión Europea (Bruselas o Luxemburgo) y se han creado por un tiempo determinado, para realizar una tarea específica.

Actualmente, el presidente del Consejo Europeo es el político polaco Donald Tusk, que fue designado en agosto de 2014 y asumió su mandato el 1 de diciembre del mismo año. Mientras que la comisión actual está presidida por Jean-Claude Juncker desde el 1 de noviembre de 2014. En estos momentos, la Comisión cuenta con 28 comisarios, 7 de ellos son vicepresidentes.

Uno de los rasgos diferenciadores de la Unión Europea frente a otras organizaciones internacionales es el alto grado de desarrollo de sus instituciones de gobierno. El gobierno de la Unión Europea siempre ha oscilado entre el modelo de conferencia intergubernamental, donde los estados conservan el conjunto de sus prerrogativas y el modelo supranacional donde una parte de la soberanía de los estados es delegada a la Unión. En el primer caso, las decisiones comunitarias son de hecho tratadas entre estados y deben adoptarse por unanimidad. Este modelo, cercano al principio de las organizaciones intergubernamentales clásicas, es defendido por la corriente euroescéptica. Según ellos, son los jefes de estado o de gobierno quienes tienen la legitimidad democrática para representar a los ciudadanos y son entonces las naciones quienes deben controlar las instituciones de la Unión. El segundo caso es el de la corriente eurófila, que estima que las instituciones deben representar directamente a los ciudadanos mediante un modelo de federalismo y elecciones directas. Para ellos, una Unión Europea federal resolvería muchos problemas relacionados con la soberanía, la legitimación democrática, la división de poderes comunitaria, el reparto de competencias, la fiscalidad y la aspiración a un modelo de bienestar común.

De este modo, el modelo de gobierno de la Unión es un modelo híbrido: por un lado está el Consejo de la Unión Europea, el cual es el representante de los estados, y en el que las decisiones no requieren unanimidad, y donde los votos de cada estado son ponderados por su peso demográfico; y por otro lado está el Parlamento Europeo, el cual es la única institución europea elegida por sufragio universal, es decir, es la única que representa a los ciudadanos.

Por lo que la Unión Europea, en su calidad de comunidad de Derecho y de acuerdo con su personalidad jurídica única, se ha dotado desde la entrada en vigor del Tratado de Maastricht de un marco institucional y de gobierno único que funciona en régimen de democracia representativa. De acuerdo con el enunciado del artículo 3.1 del Tratado de la Unión, el marco institucional ""tiene como finalidad promover sus valores, perseguir sus objetivos, defender sus intereses, los de sus ciudadanos y los de los estados miembros, así como garantizar la coherencia, eficacia y continuidad de sus políticas y acciones"". En el funcionamiento y la estructura orgánica de la Unión se distinguen los que de acuerdo con la denominación que les otorgan los Tratados son, por este orden, las instituciones, los órganos y los organismos, incluidas las agencias de la Unión.

Las competencias que tiene el gobierno de la Unión Europea, son las que se citan a continuación:

Diputados y grupos de la IX Legislatura del PE:

Un partido político europeo es una organización que sigue un programa político y está formada por partidos e individuos de distintos países y que, por consiguiente, está representada en algún Estado miembro de la Unión Europea. En concreto, según el Tratado de la UE los partidos políticos a escala europea contribuyen a formar la conciencia política europea y a expresar la voluntad de los ciudadanos de la Unión.

Desde julio de 2004 los partidos políticos europeos tienen a su disposición una financiación anual por parte del Parlamento Europeo. Esta financiación está destinada a abarcar aproximadamente hasta el 85% de los gastos de los partidos sufragando gastos que estén directamente relacionados con los objetivos establecidos en el programa político del partido (reuniones, gastos administrativos, campañas relacionadas con las elecciones europeas, publicaciones, etc.). Sin embargo, tal subvención no se puede utilizar en gastos de campañas o financiación de partidos o candidatos de comicios no europeos, así como el pago de las deudas y gastos relacionados con su amortización.

Para ser poder aspirar a dicha financiación pública, las organizaciones antes mencionadas deben poseer personalidad jurídica propia en el Estado miembro donde tenga su sede, tener representación significativa en al menos una cuarta parte de los estados de la Unión, respetar la libertad, la democracia, los derechos humanos, las libertades fundamentales, así como el Estado de derecho (y manifestarlo en particular en su programa, estatutos y actividades); y finalmente haber participado en las elecciones europeas, o haber manifestado su intención de hacerlo.

Los partidos políticos europeos forman los grupos políticos en el Parlamento Europeo, y para ello se necesitan al menos 25 diputados de una quinta parte de los estados miembros. Tras la aprobación de los presidentes de los grupos, los escaños en el hemiciclo del Parlamento Europeo se asignan a los diputados con arreglo a su adscripción política. También puede darse el caso de que algún diputado no pertenezca a ningún grupo, por lo que formará parte de los "no inscritos".

Actualmente el Parlamento está compuesto por 7 grupos políticos: el Grupo del Partido Popular Europeo, el Grupo de la Alianza Progresista de Socialistas y Demócratas, Renovar Europa, Identidad y Democracia, el Grupo de los Conservadores y Reformistas Europeos, el Grupo de Los Verdes / Alianza Libre Europea y el Grupo Confederal de la Izquierda Unitaria Europea / Izquierda Verde Nórdica.

Dado que el Parlamento Europeo no elige a ningún gobierno, en él no hay grupos "gubernamentales" ni "de oposición". En vez de la confrontación predomina la búsqueda de consensos entre los partidos mayoritarios, en los que tradicionalmente tienen un peso especial los dos grupos más grandes, el PPE (democristianos) y el PSE (socialdemócratas). Esto se debe a que ningún grupo político alcanza la mayoría absoluta necesaria para ganar una votación, a diferencia de otros parlamentos donde solo es necesaria la mayoría simple.

El derecho de la Unión Europea es el conjunto de normas y principios que determinan el funcionamiento, corporación y competencias de la Unión Europea. Se caracteriza por tratarse de un orden jurídico "sui generis", diferenciado tanto del derecho internacional como del orden jurídico interno de los estados miembros. El sistema legal comunitario se articula sobre el conjunto de competencias que los estados han atribuido a la Unión por la vía del derecho.

El derecho originario es aquel contenido en los diversos tratados que los Estados miembros suscriben, siendo las fuentes de mayor rango aquellas que posibilitan la aparición del derecho derivado, que está sometido al originario. El derecho derivado no solo cederá en caso de contradicción con el originario, sino que, además, debe estar fundamentado y originado en los diferentes tratados que lo componen.

Los tratados de la Unión Europea son de dos tipos fundamentalmente. De un lado están los tratados fundacionales, en los cuales se incluyen todas las normas contenidas en el Tratado de la Comunidad Europea del Carbón y del Acero (mientras existió), el Tratado de la Comunidad Económica Europea y el Tratado de la Comunidad Europea de la Energía Atómica.

El resto de tratados son modificativos y complementarios, incluyéndose en esta categoría los tratados que han modificado las disposiciones fundacionales. Los más importantes son: el Tratado de Fusión, el Acta Única Europea, el Tratado de la Unión Europea, el Tratado de Ámsterdam, el Tratado de Niza y el Tratado de Lisboa. Aunque también son tratados modificativos los tratados de adhesión de cada uno de los estados que se han ido adhiriendo a la Unión.

El derecho derivado es aquel que se ha desarrollado a través de las distintas normas que han aprobado las distintas instituciones europeas. Las normas que pueden aprobar estas instituciones son: los reglamentos, las directivas y las decisiones.

Los reglamentos son normas jurídicas emanadas de las instituciones europeas que poseen efecto directo en los países miembros, y que prevalecen sobre el Derecho nacional de cada uno de ellos. Existen cuatro procedimientos para la aprobación de reglamentos. En primer lugar, el reglamento será adoptado por el Consejo a propuesta de la Comisión y con la aprobación del Parlamento. Por otro lado, la Comisión podrá dictar reglamentos por iniciativa propia en los casos previstos por los tratados, así como cuando reciba la correspondiente delegación del Consejo para tal emisión reglamentaria.

Las directivas comunitarias son mandatos dirigidos a uno o varios países miembros, siendo competentes para su emisión el Consejo; la Comisión; y el Consejo junto con el Parlamento. Su rasgo más característico es la ausencia de eficacia directa en los Ordenamientos a los que va dirigida, necesitando de una transposición por parte del Estado miembro para que entren en vigor y hagan nacer en los ciudadanos derechos y obligaciones. De esta manera, la directiva contiene unos objetivos que los estados habrán de cumplir usando los medios del derecho interno, dentro del plazo indicado. El incumplimiento del deber de transponer las directivas no hace decaer el derecho del ciudadano de exigirle al Estado el cumplimiento de sus obligaciones (responsabilidad vertical limitada).

Finalmente, las decisiones son más limitadas porque, aun teniendo carácter obligatorio, no suelen tener carácter general, sino que se dirigen a destinatarios precisos. Se pueden comparar con los actos administrativos en el ámbito interno.

La Carta de los derechos fundamentales de la Unión Europea es el texto en el que se recogen todos los derechos civiles, políticos, económicos y sociales de los ciudadanos europeos y de todas las personas que viven en el territorio de la Unión.

La carta no forma parte del Tratado de Lisboa (estaba previsto que formara parte de la Constitución Europea, pero al no aprobarse esta, se modificó la previsión), pero por la remisión en el artículo 6 del Tratado de la Unión Europea tras la reforma de Lisboa se hace vinculante para todos los estados, excepto Polonia. En 2009, el Consejo Europeo aseguró a la República Checa que en la siguiente reforma del Tratado, esa cláusula de excepción se extendiese también a este país.

Los derechos fundamentales son la dignidad, la libertad, la igualdad, la solidaridad, la ciudadanía y la justicia, los cuales ya se recogen en el Convenio Europeo para la Protección de los Derechos Humanos, en la Carta Social Europea del Consejo de Europa, en la Carta Comunitaria de los Derechos Sociales Fundamentales de los Trabajadores, y a su vez en las propias constituciones de los estados miembros de la Unión, así como en otros convenios internacionales que han firmado los estados de la Unión Europea.

Entre las grandes prioridades de la Unión Europea figura la de crear un espacio de justicia, libertad y seguridad. El Tratado de Lisboa introduce cambios importantes en las actuales normas europeas sobre libertad, seguridad y justicia y facilita una actuación más amplia, legítima, eficaz, transparente y democrática de la UE en este campo. Antes de su entrada en vigor, las decisiones importantes en esta materia tenían que adoptarse por unanimidad en el Consejo, mientras que al Parlamento y al Tribunal de Justicia Europeos les correspondía un papel menor.
La actuación de la Unión Europea con respecto a la cooperación policial y judicial en asuntos penales se ve facilitada al suprimirse la distinción entre diferentes ámbitos políticos (los denominados «pilares») que antes caracterizaba a la estructura institucional.

No obstante, los estados miembros tienen la posibilidad de emprender iniciativas legislativas sobre cooperación policial operativa, justicia penal y cooperación administrativa (siempre que cuenten con el respaldo de una cuarta parte del total de países). La Comisión Europea asume el papel como guardiana de los Tratados y como garante, junto al Tribunal de Justicia Europeo, de la correcta aplicación de todas las decisiones. Los Parlamentos nacionales participan de manera más activa en el examen y la elaboración de dictámenes sobre temas de justicia, libertad y seguridad.

El Tratado de Lisboa garantiza las libertades y los principios enunciados en la Carta de los Derechos Fundamentales de la Unión Europea, cuyas disposiciones pasan a ser jurídicamente vinculantes. Por su parte, el Tribunal de Justicia obtiene más poderes para asegurar la correcta aplicación de la Carta. Todos estos cambios contribuyen a un proceso decisorio más amplio, legítimo, eficaz, transparente y democrático para el espacio común de libertad, seguridad y justicia, y ponen fin a los repetidos bloqueos de propuestas a que daba lugar el principio de unanimidad.

Es de señalar, no obstante, que dos estados miembros (Irlanda y Dinamarca) han juzgado necesario negociar o prorrogar ciertas disposiciones particulares sobre aspectos concretos de justicia, libertad y seguridad para mantener algunos puntos de sus normativas nacionales.

La Alta Representante de la Unión para Asuntos Exteriores y Política de Seguridad dirige un nuevo servicio diplomático en el que se integran de inmediato, fusionadas, las delegaciones internacionales del Consejo y de la Comisión presentes en cerca de 125 países, así como las Representaciones especiales de la Política Exterior y de Seguridad Común de la Unión Europea.

Este servicio es el Servicio Europeo de Acción Exterior ("SEAE", o también simplemente "Servicio Exterior"), creado el 1 de diciembre de 2010, según lo previsto por el Tratado de Lisboa. Como servicio diplomático que es, tiene por cometido el apoyar y asistir en el ejercicio de sus funciones al Alto Representante, como máximo responsable de la acción exterior de la Unión, en todos los ámbitos de su actividad.

Además, la Unión Europea también cuenta con una política de vecindad, la cual persigue que la UE no sea un ente ajeno a su entorno ni desvinculado de sus vecinos. Con estas políticas se busca intensificar las relaciones bilaterales con algunas antiguas repúblicas soviéticas así como los estados de la cuenca sur del Mediterráneo. Dentro de este contexto, se está desarrollando un gran proyecto, el , enfocado a largo plazo a buscar una relación de acercamiento entre la UE y la Liga Árabe. En relación con los vecinos del este, existe otra iniciativa, la Asociación Oriental, llevada a cabo entre el bloque comunitario y las antiguas repúblicas soviéticas.

Todo esto hace que el efecto de la política exterior de la Unión Europea se sienta a través del proceso de ampliación, ya que el atractivo que para varios estados tiene adquirir la calidad de miembro es un factor importante que contribuye a la reforma y a la estabilización de los países del antiguo bloque comunista en Europa.

Por lo que en los últimos años la Unión Europea, a través de la Comisión Europea, ha ganado mayor representación en organismos como el G7 o el G20, a través del Alto Representante de la Unión, aunque los estados miembros se representan en la Organización Mundial del Comercio a través de su comisario comercial.

Finalmente también hay que destacar que la Unión Europea es el mayor donante mundial de ayuda humanitaria, y la principal financiadora de las agencias de Naciones Unidas implicadas en la ayuda humanitaria y la cooperación para el desarrollo, a través del Departamento de Ayuda Humanitaria y Protección Civil de la Comisión Europea (ECHO).

La defensa y la seguridad son tradicionalmente materias de soberanía nacional, aunque la política de la Unión Europea en esta área fue establecida como el segundo de los tres pilares de la Unión en el Tratado de Maastricht de 1992, aunque no fue hasta el Tratado de Ámsterdam (1997) cuando se definieron los objetivos de la "Política Exterior y de Seguridad Común" (PESC).

En la cumbre de Helsinki de diciembre de 1999, el Consejo Europeo aprobó la creación de nuevos órganos políticos y militares permanentes, como el Estado Mayor de la Unión Europea ("EMUE"). El EMUE es un departamento de la Unión Europea, responsable de supervisar las operaciones en el ámbito de la Seguridad Común y Política de Defensa. Este departamento depende directamente del gabinete del Alto Representante de la Unión para Asuntos Exteriores y Política de Seguridad, actualmente Federica Mogherini, y se ocupa de la alerta temprana, la evaluación de la situación y el planeamiento estratégico de las misiones Petersberg (misiones humanitarias, mantenimiento de la paz, gestión de crisis) y de todas las operaciones dirigidas por la UE. Formalmente, el EMUE, forma parte de la Secretaría General del Consejo de la Unión Europea, y ha dirigido una serie de despliegues militares desde su creación.

El potente impulso que el Tratado de Lisboa ha supuesto en el ámbito institucional para la política común de seguridad y defensa, parece apuntar, con el fuerte impulso de un núcleo duro de países encabezados por Francia, Italia, España, Reino Unido (hasta 2020), Polonia y Alemania, a un relanzamiento significativo de la política común de seguridad y defensa. Si uno de los Estados miembros de la Unión Europea es atacado, los demás están obligados a protegerlo con cada segundo de la cláusula de defensa mutua contenida en el artículo 42, punto 7, del Tratado de Lisboa.

En 2011, la Unión Europea era, en su conjunto, la primera potencia económica del mundo, superando a los Estados Unidos. Según los datos del FMI ese año, el PIB (nominal) de la UE fue de 15,65 billones de dólares (el estadounidense fue de 15,29 billones). Por su parte, el PIB (nominal) per cápita de la UE en 2011 fue de 34 500 dólares, por lo que se sitúa en el puesto número 38 a escala global.

Aun así, desde 2009 la economía europea se encuentra en una crisis económica, la denominada crisis del euro, que ha provocado que el crecimiento económico en estados como Grecia, Irlanda, Portugal, Chipre, España o Italia haya sido negativo en algunos ejercicios. Las causas de la crisis eran diferentes según el país. En algunos de ellos, la deuda privada surgida como consecuencia de una burbuja en el precio de los activos inmobiliarios fue transferida hacia la deuda soberana, y ello como consecuencia del rescate público de los bancos quebrados y de las medidas de respuesta de los gobiernos a la debilidad económica postburbuja. La estructura de eurozona como una unión monetaria (esto es, una unión cambiaria) sin unión fiscal (esto es, sin reglas fiscales ni sobre las pensiones) contribuyó a la crisis y tuvo un fuerte impacto sobre la capacidad de los líderes europeos para reaccionar. Los bancos europeos tienen en su propiedad cantidades considerables de deuda soberana, de modo que la preocupación sobre la solvencia de los sistemas bancarios europeos o sobre la solvencia de la deuda soberana se refuerzan negativamente.

Como consecuencia de esta crisis económica, la Unión Europea intenta aumentar la integración económica y política entre sus estados miembros, habiendo aprobado para ello medidas comunes de carácter fiscal, una mayor coordinación económica de la eurozona, el refuerzo de los fondos de rescate para países en dificultades económicas y adelantando la puesta en funcionamiento del Mecanismo Europeo de Estabilidad. Así mismo, la mayor parte de los Estados de la UE acordaron adoptar el Pacto del Euro, consistente en una serie de reformas políticas dirigidas a mejorar la solidez fiscal y la competitividad de sus miembros.

Esta política tiene el objetivo declarado de mejorar el bienestar económico de determinadas regiones de la Unión. Alrededor de un tercio del presupuesto de la UE se dedica a esta política. El objetivo que se persigue es la eliminación de las disparidades de riqueza en toda la UE, la reestructuración de las zonas industriales en declive y la diversificación de las zonas rurales con un sector agrícola en declive.

La ampliación más importante de la UE tuvo lugar en mayo de 2004 con diez nuevos estados miembros, en su mayoría procedentes de Europa Central u Oriental, seguida por la adhesión de Bulgaria y Rumanía en enero de 2007. La mayoría de estos países son más pobres que los miembros pretéritos y esto ha significado que la "renta per cápita media" de la UE se ha reducido, lo cual ha hecho que algunas regiones de la anterior UE-15 ya no puedan optar a la ayuda financiera comunitaria, ya que la mayoría de las regiones de los nuevos estados miembros cumplen los requisitos para recibir dichos fondos.

Para conseguir esta convergencia socioeconómica la Unión dispone de varios Fondos Estructurales:

La Unión Europea, dispone además de una serie de iniciativas reservadas para acciones de carácter innovador, las cuales en origen eran 13, y actualmente tan solo se mantienen cuatro:

El presupuesto de la Unión Europea contiene todos los ingresos y todos los gastos de la UE. Si bien ha ido aumentando a lo largo del tiempo, actualmente su límite está fijado en el 1,27 % del PIB de la Unión. El presupuesto anual se fija dentro de un marco financiero plurianual previamente establecido para un período no inferior a cinco años (actualmente 7 años).

Debido a que la Unión Europea tiene un parlamento y una administración distinta e independiente de sus estados miembros, gestiona también de forma independiente los gastos dirigidos a las políticas comunes de la Unión. Para hacer frente a estos gastos, la Unión Europea tiene un presupuesto acordado de más 133 800 millones de euros anuales, lo que equivale a un 1 % de la riqueza que generan cada año los países miembros.

La Unión Europea se nutre de los recursos que le transfieren los estados miembros y que le corresponden por derecho, los conocidos como recursos propios, los cuales provienen fundamentalmente de las exacciones agrícolas, de los derechos de aduanas, de una cuota sobre el IVA y de una cuota en relación con el PIB. Mientras que el resto de recursos tienen fundamentalmente un carácter testimonial, ya que suponen solo un 1 % de los ingresos de la Unión, como son las multas impuestas por la Comisión Europea o el excedente positivo, si hay, del año anterior.

Los gastos de la Unión Europea se dividen en cinco bloques principales: crecimiento sostenible (empleo, innovación, educación, política social, etc.); ciudadanía, seguridad y justicia; política exterior de la UE; gastos de administración y compensaciones (ayudas a los países miembros con menor desarrollo).

El Mercado interior de la Unión Europea (MIUE) es una combinación de unión aduanera y zona de libre comercio. Así, los miembros de la Unión actúan como bloque, definiendo los mismos aranceles al comerciar con el exterior (para evitar la competencia interna), anulando entre ellos los aranceles en frontera y permitiendo el libre tránsito de personas, así como de capitales y servicios (libre prestación de servicios y libertad de establecimiento de empresas).

En 2007, se decidió darle un nuevo rumbo al MIUE otorgándole prioridad al consumidor y a las pequeñas empresas.

El Acuerdo de Schengen, firmado en 1985, tiene como objetivo finalizar con los controles fronterizos dentro del "espacio de Schengen" para armonizar los controles fronterizos externos, con la creación de una zona de libre circulación.<ref name="Espacio/cooperación Schengen"></ref>

Todos los países del espacio de Schengen, con la excepción de Suiza, Noruega, Liechtenstein e Islandia, son miembros de la Unión Europea. Por otra parte, Irlanda ha optado por permanecer fuera del acuerdo de Schengen aunque participa en ciertos asuntos, al igual que el Reino Unido cuando era estado miembro.

El Espacio Económico Europeo (EEE) comenzó a existir el 1 de enero de 1994, con motivo de un acuerdo entre países miembros de la Unión Europea y de la Asociación Europea de Libre Comercio (EFTA). Su creación permitió a los países de la EFTA participar en el mercado único europeo sin tener que adherirse a la UE. Los miembros de la asociación son los 28 países integrantes de la UE, e Islandia, Liechtenstein y Noruega.

Por su parte Suiza, como miembro de la EFTA, también tenía derecho a entrar en el Espacio Económico Europeo, pero tras un resultado negativo a la entrada en un referéndum nacional en diciembre de 1992, no ratificó el acuerdo. Las relaciones de Suiza con la UE están regidas por un conjunto de tratados bilaterales y entró a formar parte del espacio Schengen en noviembre de 2008.

La UE opera una política de competencia destinada a garantizar una sana competencia económica y empresarial en el mercado único. La Comisión como regulador de la competencia en dicho mercado, es responsable de aprobar fusiones, desmontar carteles y busca la liberalización económica y la prevención de las ayudas estatales.

El Comisario de Competencia, actualmente Joaquín Almunia, es una de las posiciones más poderosas de la Comisión, que destaca por la capacidad de afectar los intereses comerciales de las corporaciones transnacionales. Por ejemplo, en 2001 la Comisión por primera vez impidió un fusión entre dos empresas con sede en los Estados Unidos (GE y Honeywell), que ya había sido aprobado por la autoridad nacional. Otro caso de alto perfil dio lugar a que la Comisión multara a Microsoft en más de € 777 millones tras nueve años de acción legal.

El Tratado de la Unión Europea, en vigor desde 1993, prevé la creación de una unión económica y monetaria con la introducción de una moneda única (que por aquel entonces se pensaba llamar ECU). De ella formarían parte los países que cumplieran una serie de condiciones y se introduciría de forma gradual. La fecha inicialmente prevista se fue retrasando hasta que, finalmente, los estados miembros de la Unión Europea acordaron el 15 de diciembre de 1995 en Madrid la creación de una moneda común europea —ya bajo la denominación de «euro»— con fecha de puesta en circulación en enero del año 2002.

El euro es la moneda de la eurozona o zona del Euro, compuesta en 2014 por 18 de los 28 estados miembros de la UE que comparten esta moneda única. Los billetes y monedas de euro se pusieron en circulación el 1 de enero de 2002, fecha en la que 1 euro se cambiaba por 0,9038 dólares estadounidenses (USD). Otros hitos de la moneda europea se dieron en julio de 2002, cuando el euro sobrepasó la paridad con el dólar en el mercado de divisas, y en julio de 2008 cuando el euro alcanzó su valor máximo hasta el momento, al cambiarse 1 euro por 1,5990 dólares.

Por su parte, el Banco Central Europeo (BCE) fue creado en 1998, de conformidad con el TUE, para introducir y gestionar la nueva moneda, efectuar operaciones con divisas y garantizar el buen funcionamiento de los sistemas de pago. Es también responsable de fijar las grandes líneas y ejecutar la política económica y monetaria de la UE. Una de las principales tareas del BCE es mantener la estabilidad de precios en la zona euro, preservando el poder adquisitivo del euro.

En 2006 el Consejo Europeo aprobó la entrada de Eslovenia en el Euro para el 1 de enero de 2007. Un año después, los jefes de Estado y de Gobierno aprobaron la entrada en la zona euro de Malta y Chipre para el 1 de enero de 2008. Después, en 2008, los ministros de Economía y Finanzas de la Unión Europea aprobaron la entrada de Eslovaquia en la zona euro a partir del 1 de enero de 2009, y en 2010, los ministros aprobaron la entrada de Estonia en la zona euro a partir del 1 de enero de 2011. Finalmente, en 2013, se aprueba que Letonia entre en la eurozona a partir del 1 de enero de 2014. El resto de los estados que ingresaron a la UE con las ampliaciones de 2004 y 2007 están tomando las medidas para implementarlo como divisa propia. Por su parte, tanto Dinamarca como el Reino Unido decidieron quedarse fuera (opt-outs) de la zona euro cuando se ratificó el Tratado de Maastricht, aunque se espera que Dinamarca realice un referéndum en los próximos años sobre esta cuestión.

La UE es miembro de la Organización Mundial del Comercio (OMC) desde el 1 de enero de 1995, y a su vez, los 27 estados miembros de la Unión son miembros de la OMC. Es importante destacar que la UE es la primera potencia comercial del planeta, ya que representa más del 20% del comercio internacional (importaciones y exportaciones). En su interior, Alemania tiene el mayor mercado de la Unión atendiendo a su PIB.

La UE es el principal socio comercial de Rusia, la mayoría de países africanos, los países europeos no pertenecientes a la UE y, a partir de 2005, también de la República Popular China, con la que las transacciones superan los 100 000 millones de euros al año.

La UE ha señalado que está interesada en cerrar acuerdos de libre comercio con los países latinoamericanos, los cuales están integrados en varios grupos regionales. Uno es la Comunidad Andina constituida por Bolivia, Colombia, Ecuador, y Perú, y otro es el SICA, a la vez que también ha celebrado acuerdos de cooperación con México y Chile, y está en negociaciones para la liberalización del comercio con el Mercosur. Las negociaciones entre la Unión Europea y el Mercosur comenzaron en 1995 y continúan hasta el día de hoy. El 21 de febrero, ambos bloques se reunieron en Asunción para concretar el acuerdo, sin embargo aún persisten ciertas diferencias.

En 2007, los estados miembros de la UE (27) tenían un consumo interior bruto de energía de millones de toneladas equivalentes de petróleo (tep), de las cuales, alrededor del 46 % de la energía consumida se producía en los propios estados miembros, mientras que el 54 % restante se importó. En estas estadísticas, la energía nuclear es tratada como la energía primaria producida en la UE, independientemente de la fuente del uranio, del que menos del 3 % es producido en la UE.

La UE ha tenido el poder legislativo en el ámbito de la política energética a lo largo de su existencia, teniendo sus raíces en la originaria Comunidad Europea del Carbón y del Acero. La introducción de una política obligatoria e integral de la energía fue aprobada en la reunión del Consejo Europeo en octubre de 2005, y el borrador de la política fue publicado en enero de 2007.

La Comisión tiene cinco puntos clave en su política energética: aumentar la competencia en el mercado interior, fomentar la inversión y aumentar las interconexiones entre las redes de electricidad, diversificar las fuentes de energía con mejores sistemas para responder a una crisis, establecer un nuevo marco para la cooperación energética con Rusia, al tiempo que pretende mejorar las relaciones con los estados ricos en energía de Asia Central y del Norte de África, el uso de las fuentes de energía existentes de manera más eficiente y el aumento del uso de las energías renovables y, finalmente, aumentar la financiación de nuevas tecnologías energéticas.

La UE importaba en 2007 el 82 % del petróleo, el 57 % del gas y el 97,48 % del uranio. Existe la preocupación de que la dependencia de Europa respecto a la energía de Rusia ponga en peligro a la Unión y a sus países miembros. Por lo que la UE está tratando de diversificar su suministro de energía.

El Consejo Europeo de marzo de 2007 aprobó un plan energético obligatorio que incluye un recorte del 20 % de sus emisiones de dióxido de carbono antes del año 2020 y consumir más energías renovables para que representen el 20 % del consumo total de la UE (contra el 7 % en 2006).
Por otra parte se estableció el compromiso de lograr una cuota mínima de un 10 % de biocombustibles en el consumo total de gasolina y gasóleo de transporte en 2020.

El futuro reparto del esfuerzo de ese porcentaje del 20 % tendrá en cuenta las especificidades energéticas de cada estado. Además, la UE se compromete a llegar hasta un 30 % en la reducción de gases de efecto invernadero en caso de compromiso internacional que involucre tanto a otras potencias como a los nuevos países industrializados.

La UE está trabajando para mejorar sus infraestructuras transfronterizas, por ejemplo a través de las redes transeuropeas (RTE). Los proyectos de las RTE incluyen el túnel del Canal, el corredor Mediterráneo, el LGV Est, el túnel ferroviario de Fréjus, el puente de Oresund y el túnel de base del Brennero. En 2001, se calculó que en 2010 la red comprendería 75 200 kilómetros de carreteras, 78 000 kilómetros de vías de ferrocarril, 330 aeropuertos, 270 puertos marítimos y 210 puertos interiores.

Otro proyecto de infraestructura es el sistema global de navegación por satélite Galileo, construido por la Unión Europea y puesto en marcha por la Agencia Espacial Europea (ESA). Tras años de retraso, el proyecto pretende completar una red de 26 satélites en órbita para 2017, más seis de repuesto. Galileo fue lanzado en parte para reducir la dependencia de la UE sobre el Sistema de Posicionamiento Global (GPS) estadounidense, y también para dar una cobertura mundial más completa y permitir una exactitud mucho mayor, dada la antigüedad del sistema GPS. Algunos han criticado al sistema Galileo debido a su elevado coste, sus varios retrasos, y por su percepción de redundancia, dada la existencia del sistema GPS.

La Política Agrícola Común (PAC) es una de las políticas más antiguas de la Unión Europea y uno de sus propósitos originales. La política tiene como objetivos el incrementar la producción agrícola, asegurar la certeza del suministro de los alimentos, mejorar la calidad de vida de los agricultores y estabilizar los mercados al asegurar precios razonables para los consumidores. Hasta hace poco, operaba mediante un sistema de subsidios y de intervención en el mercado; hasta la década de 1990 la política representaba el 60 % del presupuesto anual de la Unión Europea, y hoy en día aún representa el 40 %.

Los controles de precios y la intervención en los mercados tuvieron como resultado la sobreproducción, la cual se almacenaba para mantener los niveles mínimos de precios. Para disponer de este superávit, a menudo se vendían en el mercado mundial internacional a precios por debajo de los precios garantizados por la Unión o, por otra parte, los agricultores a menudo recibían subsidios que equivalían a la diferencia entre los precios mundiales y los de la Unión. Este sistema se ha criticado por vender más barato que la producción de los países en vías de desarrollo. La sobreproducción también ha sido criticada por los ambientalistas por los métodos de producción intensivos. Por otra parte, los que apoyan a la Política Agrícola Común, argumentan que la ayuda económica para los agricultores, les asegura un estándar de vida razonable imposible económicamente si no existiera.

Desde el comienzo de la década de 1990, la política se ha ido reformando. Al principio, estas reformas incluían la política de separar una porción de tierra de la producción, imponer cuotas en la producción lechera, etc. Los gastos agrícolas abandonarán los subsidios relacionados con la producción específica por relacionarlos con el tamaño de las fincas agrícolas, para permitir que el mercado establezca los niveles de producción y a la vez asegurar la renta de los agricultores. Las reformas también incluyen la abolición del régimen de azúcar entre los estados miembros y las naciones africanas y caribeñas y su relación privilegiada.

En el terreno de la investigación y exploración espacial en Europa existe la Agencia Espacial Europea (ESA en sus siglas en inglés). En ella colaboran 18 Estados europeos, aunque se espera que los miembros que trajo la ampliación de la Unión Europea de 2004 y 2007 vayan incorporándose a la Agencia en los próximos años. Su sede central se encuentra en París. El lugar desde el que se efectúan los lanzamientos de los vehículos Ariane de la Agencia es el Puerto espacial de Kourou, situado en la Guayana Francesa.

En el terreno de la física nuclear, destaca la Organización Europea para la Investigación Nuclear (más conocido por sus antiguas siglas, CERN), el mayor laboratorio de investigación en física de partículas a nivel mundial.
Está situado en la frontera entre Francia y Suiza, entre la comuna de Meyrin (en el cantón de Ginebra) y la comuna de Saint-Genis-Pouilly (en el departamento de Ain). En la actualidad hay 20 Estados miembros, y la Comisión Europea actúa como observador. Uno de sus proyectos estrella es el Gran Colisionador de Hadrones, sobre el que los científicos e investigadores han puesto grandes expectativas.

En lo que se refiere a la Política Exterior y de Seguridad Común, la Unión Europea cuenta con una alta representante de la Unión Europea para los Asuntos Exteriores y las Políticas de Seguridad. Es a petición de esta o de un Estado miembro que se toman las decisiones por consenso en el Consejo Europeo de Ministros de Exteriores. En materia de defensa se contemplan mecanismos para la realización de misiones fuera de las fronteras de la Unión. Estas misiones son voluntarias y pueden tener carácter civil y militar. Su origen se halla en las Misiones de Petersberg.

La Política Común de Seguridad y Defensa forma parte de la Política Exterior y de Seguridad Común y se desarrolla en el Tratado de la UE, Título V, Sección 2. Pese al objetivo explícito de desarrollar una política común de defensa, la UE sigue trabajando desde el ámbito de la intergobernabilidad en esta área, es decir, no de modo comunitario sino a través de la cooperación voluntaria de cada Estado miembro. Es por ello que se adoptan mecanismos de doble velocidad, abriendo la puerta a actuaciones conjuntas entre aquellos estados miembros que voluntariamente decidan cooperar en materia de defensa.

Ello se realizará a través de la Agencia Europea de Defensa, que tiene como misión promover una política de defensa común y armonizar las políticas militares y de seguridad y defensa. La AED está compuesta por todos los estados miembros, exceptuando Dinamarca, y cuenta con un presupuesto de alrededor de 30 000 millones de euros anuales.

La Política de Seguridad y Defensa Común ha desarrollado 34 misiones, de las cuales 14 han tenido naturaleza militar. La UE no tiene una fuerza militar propia, pero cuenta con las fuerzas rápidas de despliegue.

Asimismo, el Tratado de la UE establece la alineación de la PCSD con la OTAN, por lo que:

También el Tratado institucionaliza la militarización de los Estados miembros en cuanto que afirma que «[l]os Estados miembros se comprometen a mejorar progresivamente sus capacidades militares». En ese sentido, se promueve la llamada «industria de defensa equilibrada», lo que, traducido, pretende promover el desarrollo de la industria y la tecnología armamentista y la cooperación del negocio de las armas a nivel comunitario. El objetivo es, por ello, doble: por un lado, desarrollar y apoyar el crecimiento de la industria militar, a la vez que una mayor cooperación e integración de la defensa entre los estados miembros. 

Ya en 2007 se creó la Base de Defensa de Tecnología e Industria como estrategia para el desarrollo del complejo militar-industrial y las nuevas decisiones de Bruselas confirman la apuesta por desarrollar una política de defensa común a partir de una mayor integración y desarrollo de la industria armamentística europea. Pese a que la UE ya representa, en términos agregados, una de las primeras fabricantes y exportadoras de armas global, y a que su gasto militar agregado es del 28% en términos globales. Además la UE alberga 30 de las 100 mayores empresas de armamento del mundo, entre ellas BAE Systems, Thales, Grupo Airbus (EADS), Finmeccanica. 

En diciembre de 2013, el Consejo Europeo de Defensa acordó aumentar los esfuerzos de integración militar. Con ello se decidió también aumentar la I+D militar y pese a las propias leyes de la UE, se ha desarrollado la idea de la investigación de doble-uso (con aplicaciones en el ámbito militar y en el civil). El Programa Marco de Investigación de la UE, llamado Horizonte 2020, y con un presupuesto de 30 000 millones de euros ha abierto así la puerta también a la I+D militar y en seguridad, a la que se reservan explícitamente 2000 millones de euros.

La concepción mayoritaria es que la UE es el resultado de los esfuerzos para gestionar los conflictos pacíficamente tras dos guerras mundiales que devastaron el viejo continente. Estos objetivos de paz, que originariamente inspiraban los discursos de integración europea, han dado paso a una voluntad política más abiertamente orientada a la competitividad y a hacer de la Unión Europea un polo económico y financiero mundial integrado (de hecho, ya el principal incentivo de la integración fue a través de las primeras comunidades económicas del carbón y el acero).

La política de defensa europea, además, está supeditada a la OTAN, con el interés por parte de EE. UU. de que el Viejo Continente alivie la carga financiera y militar estadounidense en la Alianza, aumentando su apoyo financiero, militar y político a la organización.

La primera estrategia de desarrollo sostenible en la Unión Europea se realizó en 2001, y posteriormente se actualizó en 2006, ya que se quería mejorar las deficiencias derivadas de los nuevos retos. Esta política se centra fundamentalmente en el cambio climático, en la política energética, así como en la educación, la investigación y la financiación pública para conseguir instalar patrones sostenibles de producción y consumo.<ref name="Europa/Env"> "Actividades de la Unión Europea. Medio ambiente". europa.eu.</ref>

La Unión Europea cuenta con una de las legislaciones de medio ambiente más severas del mundo, la cual se introdujo después de estar varias décadas estudiando los principales problemas medioambientales existentes en la Unión. La UE cuenta con la Agencia Europea de Medio Ambiente, que tiene por misión facilitar a la UE y a los países miembros la toma de decisiones sobre la mejora del medio ambiente, y coordinar la Red europea de información y observación del medio ambiente.

Las actividades prioritarias en relación al medio ambiente se enfocan en la lucha contra el cambio climático, mantener la biodiversidad, reducir los problemas de salud derivados de la contaminación y el uso de los recursos naturales de manera más responsable. De esta forma, lo que se persigue con estas políticas es la protección del medio natural, de una manera que se contribuya al crecimiento económico, impulsando la innovación y la empresa. La Unión Europea ha puesto en marcha a través de un libro de medidas, un paquete ambicioso que marque su propio liderazgo en la preparación para un acuerdo mundial. En ese sentido, se ha convertido en la primera potencia mundial que adopta objetivos jurídicos vinculantes de tal alcance en materia de clima y energía. El primer compromiso que los países miembros han adquirido ha sido la reducción de emisiones de gases de efecto invernadero en un 10%.

La red Natura 2000 es una red ecológica europea de áreas de conservación de la biodiversidad. Consta de Zonas Especiales de Conservación designadas de acuerdo con la Directiva Hábitat, así como de Zonas de Especial Protección para las Aves (ZEPA) establecidas en virtud de la Directiva de Aves. Su finalidad es asegurar la supervivencia a largo plazo de las especies y los hábitats más amenazados de Europa, contribuyendo a detener la pérdida de biodiversidad ocasionada por el impacto adverso de las actividades humanas. Es el principal instrumento para la conservación de la naturaleza en la Unión Europea.

Esta red de espacios coherentes se fundamenta en la política de conservación de la naturaleza de la Unión Europea según su Directiva de Hábitats, que complementa la Directiva de Aves de 1979.

La Red Natura 2000 se creó a través de la Directiva 92/43/CEE sobre la conservación de los hábitats naturales de fauna y flora silvestres (más conocida como Directiva de Hábitats), de 21 de mayo de 1992. Esta red debe permitir alcanzar los objetivos establecidos por el Convenio sobre la Diversidad Biológica, aprobado en la Cumbre de la Tierra en Río de Janeiro en 1992.

El programa LIFE, creado en 1992, financia medidas que contribuyen al desarrollo, la aplicación y actualización de la política y la legislación comunitaria de medio ambiente. Este instrumento financiero pretende igualmente facilitar la integración del medio ambiente en las demás políticas y lograr un desarrollo sostenible en la Unión Europea.

En 2016 se crea un catálogo de especies invasoras de la Unión

La Unión Europea ocupa el 3º puesto en el "ranking" mundial de población, con un total de 501 105 661 personas que se estiman que viven en la UE en 2011 frente a las 313 232 044 en Estados Unidos. Es decir, la Unión Europea tiene aproximadamente 188 millones de habitantes más que los Estados Unidos (En 2020, la UE perdió 67 millones de habitantes al abandonar el Reino Unido la Unión).

El número de habitantes de la Unión podrá incrementarse en el próximo decenio, en parte debido a la inmigración pero sobre todo gracias al proceso de ampliación, que podría dar cabida a Islandia, a Albania, a varios estados de la antigua República Federal Socialista de Yugoslavia (Serbia, Macedonia del Norte, Bosnia y Herzegovina, Montenegro y Kosovo) e incluso a Turquía, con lo cual el total de la población de la Unión aumentará en cerca de 100 millones de habitantes.

Pese a que la población de la UE constituye la tercera potencia demográfica del mundo, por detrás de China y la India, solo contribuyó en 2003 en menos de un 2% al aumento de la población mundial, que se incrementó en 75 millones.

En la mayoría de los países del Sur de Europa se ha producido un cambio desde una situación de altos índices de nacimientos y defunciones a una de bajas tasas de nacimientos y defunciones, aunque este fenómeno apareció décadas después que en otros países europeos más desarrollados. Actualmente ninguno de los países miembros de la Unión Europea registra niveles suficientes de natalidad. Aun así, los niveles de natalidad están creciendo en los últimos años, lo que junto con la inmigración, hace que el crecimiento de la Unión Europea sea positivo.

En España, la natalidad se redujo en más de la mitad entre 1960 y 1990, de 21,7 a 10,2 nacimientos por mil habitantes. En ningún otro país de la Unión la tasa de nacimiento bajó tanto como en España, pero por otra parte este país ostenta la mayor tasa inmigratoria (2003). En 1900 la esperanza de vida en España era de 35 años, la continua caída en la tasa de mortalidad la elevó a 62 años en 1950, para llegar en 1985 casi a los 80 años para las mujeres y 73 para los hombres.

Antes de la ampliación de 2004, la población de la Unión crecía a una tasa anual de 0,23% (2,3 por mil) debido principalmente al incremento de la población inmigrante cuyo saldo adicional en el año 2000 fue de 735 000 personas, mientras que el crecimiento natural de la población, durante el mismo año, fue de 372 000 habitantes.

Sobre la tasa de crecimiento natural de la población debe anotarse que la tasa de natalidad de casi todos los países de la Unión está creciendo, con excepción de Alemania, Italia, Grecia y Suecia. Las tasas más altas de natalidad se observan en Irlanda (16,1 nacidos por mil habitantes), Francia (12,29) y Países Bajos (10,2). En el otro extremo aparecen Alemania (8,3) y Grecia (9,1).

La inmigración es responsable de aproximadamente tres cuartas partes del crecimiento total del número de habitantes de la UE, según datos de 2001. Alemania y España fueron los principales responsables de este crecimiento en términos absolutos con cerca de 230 000 inmigrantes netos cada uno (sumados suponen el 44% del total).

Sin embargo en términos porcentuales, los mayores crecimientos se dan en Luxemburgo y Portugal (ambos con 6,7 inmigrantes por cada 1000 habitantes), seguidos de España (5,6) e Irlanda (5,1). Aunque aún con migración neta positiva, las menores tasas se dan en Francia, Bélgica y Países Bajos. La media de la Unión Europea se cifra en 3 inmigrantes por cada 1000 habitantes.

En los últimos años debido a la crisis económica del 2008-09 el número de inmigrantes ha descendido en países como España, llegando incluso a producirse un proceso migratorio de estos países hacia otros europeos como Alemania.

En la Unión Europea la esperanza de vida es de las más altas del mundo, con 79,4 años de vida media (76,4 para los hombres y 82,4 en el caso de las mujeres), y un Índice de Desarrollo Humano superior al de las potencias emergentes y al de Estados Unidos.

En este contexto la población de la UE, experimenta un proceso marcadamente desigual entre sus regiones. Por una parte países como Alemania, donde durante varios años la población envejece exponencialmente, debido a la disminución del número de nacimientos y el constante aumento en la esperanza de vida. Por otra parte Francia es el único gran estado (en cuanto a número de habitantes se refiere) de toda la unión que ha logrado mantener una tasa de natalidad suficiente. A esta base la situación francesa añade un alto promedio inmigratorio y una reducida tasa de emigración.

Considerando tanto el crecimiento vegetativo como el saldo migratorio, los países que más crecieron en 2010 han sido Irlanda y Luxemburgo, y los que menos Alemania e Italia.

La Unión Europea cuenta, en todas sus instituciones, con 24 idiomas oficiales y de trabajo. Sin embargo, en la Comisión Europea, por ejemplo, el colegio de comisarios negocia sobre la base de documentos presentados en inglés, francés, alemán, italiano y español. De facto, las principales lenguas oficiales y de trabajo son el inglés y el francés, tanto por su uso hablado, así como por su uso en la primera redacción de los documentos oficiales.

En los estados miembros se utilizan, además de los 24 idiomas señalados, unas 60 lenguas más, cooficiales solo en parte del territorio o no oficiales (lenguas regionales y minoritarias). Una de las políticas claves de la UE es la de promover el aprendizaje por todos los ciudadanos de por lo menos dos idiomas aparte de su lengua materna. El objetivo no es únicamente facilitar la comunicación entre ciudadanos, sino también fomentar una mayor tolerancia hacia los demás y un respeto para la diversidad cultural y lingüística de la Unión.

Varios programas de cooperación promueven el aprendizaje de los idiomas y la diversidad lingüística mediante, por ejemplo, intercambios escolares, el desarrollo de nuevos métodos, o becas para el profesorado de idiomas. De alguna manera de esto habla su lema "unida en la diversidad" (latín: «"In varietate concordia"»).

En la Unión Europea, como lenguas maternas las dos más habladas son el alemán (18%) y el francés. El inglés como lengua materna apenas cuenta con hablantes, pero es la primera por el número total de hablantes (51%), seguida por el alemán.

Hay divergencias en el nivel de conocimiento de idiomas entre los estados miembros. Mientras que el 89% de suecos y 86% de daneses es capaz de comunicarse en inglés, cinco de los estados miembros todavía tienen, según un barómetro de 2006, mayoría de población monolingüe: Irlanda (el 66% de habitantes solo habla su lengua materna), Italia (59%), Hungría (58%), Portugal (58%) y España (56%).

Según un estudio de la Comisión Europea en 2005, cuatro de cada cinco ciudadanos de la Unión Europea tienen creencias religiosas o espirituales. Más en concreto, el 52% de la población afirma creer en la existencia de Dios y un 27% creen en la existencia de alguna clase de espíritu o fuerza vital. Sólo el 18% de los ciudadanos declararon no tener ningún tipo de creencia religiosa.

De los 27 estados de la Unión Europea, así como de los estados en negociaciones de adhesión (Turquía, Macedonia del Norte, Islandia, Montenegro y Serbia), existen algunas diferencias entre sus ciudadanos por motivos religiosos. En general, la mayoría de ciudadanos europeos que profesan alguna religión se adscriben al cristianismo, en alguna de sus diferentes ramas (católica, protestante u ortodoxa). Así, mientras que en algunos países como Italia, España, Francia o Irlanda predomina la religión católica; en otros como Suecia o Dinamarca predomina la religión protestante, y en otros como Grecia o Rumania la religión ortodoxa. Asimismo, en otros países como Turquía, la religión musulmana es la mayoritaria.
Además, el porcentaje de agnósticos y ateos en los estados de la UE también varía según el estado en el que se encuentre. Según los últimos eurobarómetros, mientras que tan solo menos de un 2% de malteses no creen en la existencia de Dios, en España, el 24% de la población se declara no creyente o atea y en Francia entre el 30-35%. Los países con porcentajes más altos de ateísmo o agnosticismo son República Checa (60%) y Estonia (76%).

En el conjunto de la UE, según un eurobarómetro de 2006, el 46% de los ciudadanos europeos considera que la religión ocupa una posición muy importante en la sociedad. Así, entre los países donde la religión tendría mayor importancia para la población se encuentran Chipre, Italia, Malta, Eslovaquia, Eslovenia, Polonia, Portugal y España, en ese orden.

Entre los países miembros de la Unión Europea se encuentran casos, como Dinamarca, donde existe una religión de carácter estatal. La aconfesionalidad del estado no es por tanto un requisito para entrar en la Unión (siendo este un aspecto interno de cada país) pero sí que se garantice la libertad religiosa para cualquier credo, según recoge la Carta de los Derechos Fundamentales:

El Tratado de la Unión Europea o Tratado de Maastricht dio reconocimiento oficial a la dimensión cultural de la integración europea, al atribuir ciertas competencias (bastante reducidas) de acción cultural a la Comunidad Europea. Según el tratado, la Comunidad Europea debe impulsar las culturas de los estados miembros, teniendo especial cuidado en preservar la diversidad, pero poniendo también de manifiesto el "patrimonio cultural común".

Sobre cultura, la Comisión Europea dispone de un comisario que agrupa en una sola cartera la educación, la formación, la cultura y la juventud, pero no el multilingüismo.

Uno de los proyectos culturales más importantes es la designación de la Capital Europea de la Cultura. Este es un título conferido por la comisión y el parlamento europeo a una o dos ciudades europeas, que durante un año tienen la posibilidad de mostrar su desarrollo y vida culturales. Algunas ciudades europeas han aprovechado esta designación para transformar completamente sus estructuras culturales y ser reconocidas en el ámbito internacional. Cuando una ciudad es nombrada capital europea de la cultura, en ella se desarrollan todo tipo de manifestaciones artísticas.

Otro proyecto cultural comunitario es el de la Joven Orquesta de la Unión Europea. En ella coinciden músicos jóvenes con talento procedentes de la Unión Europea junto con profesores reconocidos internacionalmente para conformar una orquesta renombrada a escala internacional.
La Unión Europea ha sido siempre conocida como una de las zonas de mayor prestigio educativo y es famosa por sus proyectos y por su gran evolución y experiencia y aunque tenga déficits en algunas de sus características, dedica mucho esfuerzo económico y social para que estas debilidades sean superadas, sobre todo en países que se han cohesionado recientemente.

Aun así, existen grandes diferencias de nivel cultural, social y moral entre los países potenciales y desarrollados de la Unión respecto a los menos desarrollados, e incluso en algunos de los países ricos hay niveles profesionales muy débiles y contraproducentes a los esquemas de la Unión.

En 1995, la Comisión Europea publicó el "Libro blanco sobre la educación y la formación". En él se explicó ampliamente la importancia de que los ciudadanos europeos puedan recibir formación a lo largo de toda la vida, lo que se conoce como aprendizaje permanente. El objetivo es mantener la competitividad y combatir la exclusión social.

Recogiendo estas ideas, en 2000, el "Memorándum sobre el aprendizaje permanente", documento de trabajo de servicios de la Comisión Europea, convoca un debate europeo para hacer realidad el aprendizaje durante toda la vida a nivel individual e institucional. Al final del memorándum, plantean seis claves a tener en cuenta para esta estrategia: garantizar el acceso universal y continuo al aprendizaje para obtener y renovar las cualificaciones de los ciudadanos, aumentar la inversión en recursos humanos, crear métodos eficaces para el aprendizaje permanente, valorar el aprendizaje no formal e informal, asesorar e informar de las oportunidades de aprendizaje durante toda la vida y ofrecer oportunidades próximas de aprendizaje permanente.

Proceso de Bolonia es el nombre que recibe el proceso iniciado a partir de la Declaración de Bolonia, acuerdo que en 1999 firmaron los ministros de Educación de diversos países de Europa (tanto de la UE como de otros países como Rusia o Turquía) en la ciudad italiana de Bolonia. Se trató de una declaración conjunta (la UE no tiene competencias en materia de educación) que dio inicio a un proceso de convergencia que tenía como objetivos facilitar el intercambio de titulados y adaptar el contenido de los estudios universitarios a las demandas del mercado.

La declaración de Bolonia condujo a la creación del Espacio Europeo de Educación Superior, un ámbito que serviría de marco de referencia a las reformas educativas que muchos países, los que se incorporaron a dicho espacio, habrían de iniciar en los primeros años del siglo XXI. Este acuerdo se enmarca dentro del Acuerdo General de Comercio de Servicios, firmado en 1995, y cuyo objetivo declarado es "liberalizar el comercio de servicios" a escala mundial (porque la OMC integra a 151 Estados, incluyendo a toda la Unión Europea) para introducirlos en el mercado, ya que "la financiación pública es un elemento de distorsión de los mercados".

Para muchos sectores de la sociedad, el Proceso de Bolonia va más allá de lo firmado en Bolonia, comprendiendo aspectos relativos a toda la reforma universitaria que se consideran más importantes, especialmente aquellos referidos a la financiación de la universidad pública, y cuenta con muchos detractores y opositores.


Los programas educativos europeos más importantes son Comenius, en el ámbito escolar, Leonardo da Vinci, para la formación profesional, programa Erasmus, para la enseñanza universitaria y Grundtvig, para la enseñanza de adultos.

El programa eLearning promueve la integración efectiva de las tecnologías de la información y la comunicación (TIC) en los sistemas de educación y formación en la UE. La línea más importante de este programa es la iniciativa eTwinning, que pone a disposición de los centros escolares un portal de internet con herramientas y apoyo para facilitar la realización de proyectos de hermanamiento entre centros de diferentes países. Los hermanamientos de eTwinning permiten a los profesores de todas las asignaturas desarrollar proyectos pedagógicos comunes, compartir experiencias y recursos didácticos e introducir la dimensión europea en el aula. Los alumnos tienen la oportunidad de aprender de y con sus compañeros de otros países, de practicar idiomas extranjeros y de desarrollar destrezas relacionadas con las TIC.

Europa ha sido origen a lo largo de la historia de muchos de los deportes más populares, así como del Movimiento Olímpico. En la actualidad, el deporte en la Unión Europea tiende a estar altamente organizado, y las distintas modalidades suelen contar con ligas profesionales. Además, la mayoría de las organizaciones deportivas internacionales de relevancia están situadas en Europa.

En países miembros de la Unión Europea, se han creado diversos deportes que hoy en día son muy populares e importantes en el mundo. El fútbol, deporte creado en Reino Unido, es el más extendido y popular en la Unión Europea y del mundo. Los clubes deportivos de fútbol en Europa son generalmente los que han cosechado mayor éxito a lo largo de la historia en el mundo, así como los mejores pagados. La UEFA Champions League, el campeonato de fútbol a nivel europeo, es uno de las competiciones futbolísticas más prestigiosas. A nivel estatal, las ligas más populares son la Liga SANTANDER española, la competición italiana Serie A, la Ligue 1 en Francia o la Bundesliga en Alemania.

El rugby, por su parte, a nivel profesional es popular en el sur de Francia, Irlanda y el norte de Italia. Otros deportes, como el baloncesto, el balonmano, el ciclismo, el voleibol, el waterpolo, o el hockey también son practicados con popularidad en algunos Estados miembros.

El deporte en la Unión Europea es principalmente una responsabilidad de los distintos Estados miembros o de otras organizaciones internacionales. Sin embargo, algunas políticas de la UE han tenido un impacto sobre el deporte, tales como la libre circulación de trabajadores, a través de la sentencia Bosman, la cual prohíbe a las ligas nacionales de fútbol la imposición de cuotas de jugadores extranjeros con ciudadanía europea.

En el marco del Tratado de Lisboa se ha propuesto un estatuto especial para a los deportes, el cual eximirá a este sector de gran parte de las reglas económicas de la UE. Durante la formulación de la política de deportes de la UE, varias asociaciones deportivas europeas fueron consultados, incluida la FIBA, la UEFA, la Federación Europea de Balonmano, la Federación Internacional de Hockey sobre Hielo, la FIRA y la Confederación Europea de Voleibol. Todos los Estados miembros de la UE y sus respectivas asociaciones nacionales de deporte pueden participar en las organizaciones deportivas europeas como la UEFA.




</doc>
<doc id="2892" url="https://es.wikipedia.org/wiki?curid=2892" title="Universidad de Upsala">
Universidad de Upsala

La Universidad de Upsala (en sueco: "Uppsala universitet") es una universidad ubicada en la ciudad de Upsala, Suecia, y es la casa de estudios más antigua de Escandinavia, habiendo sido fundada en 1477. Constantemente es clasificada entre las mejores universidades de Europa septentrional, y generalmente se la considera como una de las instituciones de educación superior más prestigiosas del viejo continente.

La universidad alcanzó un papel sobresaliente durante el desarrollo del Imperio sueco a fines del siglo XVI, obteniendo más tarde cierta estabilidad financiera a partir de una gran donación del rey Gustavo II a principios del siglo XVII. Upsala, además, representa un importante sitio histórico para la cultura e identidad nacional sueca, e incluso para el surgimiento de la actual nación, ya sea en términos de historiografía, literatura, política y música. Muchos aspectos de la cultura académica del país se originaron en Upsala, como por ejemplo, el birrete blanco.

La Universidad de Upsala pertenece al Grupo Coimbra de universidades europeas. Esta institución cuenta con 9 facultades distribuidas en tres "dominios disciplinarios": el primero corresponde al de humanidades y ciencias sociales; el segundo a las áreas de medicina y farmacia; y finalmente, el último cubre los sectores de ciencia y tecnología. Tiene alrededor de 20 000 estudiantes de tiempo completo, y cerca de 2 000 estudiantes de doctorado. Por otro lado, posee un personal académico de 3 600 profesores e investigadores, de un total de 5 500 empleados.

Su presupuesto anual es de alrededor 4,3 miles de millones de coronas suecas en promedio, lo que equivaldría aproximadamente a unos 715 millones de dólares, del cual cerca del 60% se destina a los estudios de grado e investigación.

En términos de arquitectura, la Universidad de Upsala tradicionalmente ha tenido una fuerte presencia en el área que rodea la catedral de la ciudad, al lado oeste del río Fyris. A pesar del desarrollo de edificios y construcciones más modernas y alejadas del centro, el casco histórico de la ciudad aún es dominado por la presencia de la universidad.

La Universidad de Upsala fue fundada en 1477, convirtiéndose en la primera universidad escandinava. La iniciativa en el asunto se le atribuye al arzobispo de la Iglesia Católica Sueca, Jakob Ulvsson. La nueva casa de estudios era pequeña, teniendo como máximo 50 alumnos y varios profesores. La universidad comenzó a decaer en la primera década del siglo XVI debido a los conflictos políticos de la época.

Entre 1520 y 1530, el nuevo monarca sueco Gustavo I llevó a cabo la reforma luterana, lo que significó que la universidad, dependiente de la Iglesia católica, perdiera su base económica e ideológica.

Sin embargo, esta situación cambió a fines del siglo XVI, cuando el clero protestante había ganado un dominio sólido en la enseñanza de la religión y sintió la necesidad de centrarse en los estudios desde un enfoque más académico, para contrarrestar la reforma católica. Es por esto que en 1593, en el sínodo de la Iglesia luterana, se decidió restaurar los privilegios de la institución. El nuevo decreto se firmó el 15 de marzo de 1595.

Durante el reino de Gustavo II Adolfo, que se extendió entre los años 1611 y 1632, Suecia se consolidó como una potencia militar líder en Europa del Norte, y también se desarrolló como un Estado burocrático avanzado; por lo tanto, el reino necesitaba funcionarios competentes. Junto con su principal asesor, el canciller Axel Oxenstierna, Gustavo II proporcionó muchos subsidios a la universidad, tanto en lo económico como en lo administrativo. En particular, hizo una donación de más de 300 estancias a la casa de estudios, las cuales todavía son administradas por esta institución. A la universidad llegaron profesores del extranjero, y el número de estudiantes aumentó. Durante este tiempo, el sistema de naciones universitarias se importó de las universidades medievales del continente, lo que significaba que los estudiantes provenientes de una misma región se reunían para colaborar y ayudarse mutuamente, y también para tener vida social. Este sistema persiste hasta nuestros días en esta casa de estudios.

Entre los años 1660 y 1670, la institución fue dominada por Olaus Rudbeck, científico y escritor sueco, quien además fue profesor de medicina en la universidad. Rudbeck era un erudito muy versátil, y era una persona que disfrutaba experimentar. Dentro de sus logros podemos encontrar al extraordinario teatro anatómico, el cual erigió en la cumbre del nuevo edificio universitario llamado Gustavianum, el cual hoy es un museo dedicado a la historia de la ciencia y de las ideas.

Carlos Linneo, destacado científico, naturalista, botánico y zoólogo sueco, quien llegó a ser profesor de la universidad en el año 1741 después de haber estudiado en su país de origen y en los Países Bajos, es el nombre que domina el siglo XVIII. Gracias a él, muchos estudiantes provenientes de toda Europa emigraron para estudiar en Upsala. Linneo envió a sus propios alumnos en expediciones de investigación a diferentes partes del mundo, tales como Japón, Sudáfrica y Australia. Así, ya por la mitad del siglo, se produjo un florecimiento de las ciencias naturales en la universidad. Además, también mereciendo mención además de Linneo podemos encontrar a eruditos como Anders Berch, economista sueco; Anders Celsius, el astrónomo que diseñó la escala de medición de temperatura más usada en el mundo; o a Torbern Bergman, químico y exprofesor de la universidad.

A finales de siglo, el rey Gustavo III (1771-1792) se interesó enérgicamente por esta casa de estudios. Una de las formas a través de las cuales demostró este interés fue la donación de un extenso jardín perteneciente al Castillo Real de Upsala. Desde entonces, a esa extensión de terreno se le conoce como el jardín botánico de la universidad, el cual fue erigido en memoria de Linneo y en honor al Gustavo III.

El siglo XIX ha sido llamado como "el siglo de los estudiantes" en Upsala. Previamente, los alumnos habían sido más bien un grupo anónimo, pero bajo las doctrinas de la Revolución francesa y con la creciente importancia, independencia y autoestima de la clase media alta educada, los estudiantes fueron gradualmente involucrándose más en asuntos políticos, y también contaban más en la opinión pública. A mediados de siglo, las tendencias nacionalistas escandinavas tuvieron también una fuerte influencia en los alumnos, la cual se dejó sentir tanto en Upsala como en otras ciudades universitarias.

Si el siglo XVIII fue el de las ciencias naturales, el siglo XIX fue la era de los historiadores, eruditos literarios y escritores; tanto así que se construyó una estatua en frente del edificio principal en honor al más sobresaliente de estos ilustrados, el historiador Erik Gustaf Geijer. Ocurrieron muchos cambios a mitad de siglo, que reformaron la organización de la universidad, y que actualizó el sistema de examen.

La Universidad de Upsala solemnemente su cuatrigentésimo aniversario en el año 1877. Como un presente del Estado sueco, en esta ocasión, la universidad recibió un nuevo edificio, el cual todavía está en uso, aunque fue oficialmente inaugurado diez años más tarde en el año 1887.

A las mujeres se les permitió estudiar en la Universidad al empezar la década de los 1870s. Sin embargo, fue una larga y ardua lucha para que las mujeres que estudiaban en Upsala consiguieran una igualdad de reconocimiento en sus estudios y en sus carreras académicas. La primera mujer en Escandinavia que logró recibir un título de doctorado en investigación fue la historiadora Ellen Fries, quien recibió su grado en la Universidad de Upsala en el año 1833.

Durante este periodo, la universidad albergó a muchos eruditos prominentes, con algunos de ellos laureados con el Premio Nobel. Incluso, Alfred Nobel recibió un grado de doctor honoris causa de parte de la universidad en 1893.

Este periodo se caracteriza por una serie de cambios incluyendo amplias reformas educativas y la expansión radical del número de estudiantes. En los años 50, la universidad poseía alrededor de 5 000 estudiantes, creciendo dramáticamente durante los próximos diez años llegando a 20 000 alumnos en total. En los 90 una nueva expansión tomó lugar, con más de 30 000 esudiantes de pregado en total matriculados en la casa de estudios.

El crecimiento vigoroso de la universidad ha implicado una necesidad de nuevas premisas para la educación y la investigación. Mientras que en la década de 1950 las actividades universitarias se concentraban en la casa central, ubicada cerca de la catedral, hoy la institución se propaga por vastas áreas, con campus de carácter multi e interdisciplinarios.

Al mismo tiempo, el financiamiento externo también ha ganado terreno. Contacto intensivo con el mundo circundante, ya sea a nivel nacional e internacional, amplían el rol de la universidad en la comunidad académica mundial.

Los edificios y ubicación de la universidad se listan a continuación. Algunos de los edificios históricos de la Universidad en el centro de Uppsala han tenido que cambiar ya que su estatus de protección histórica hace imposible llevar a cabo las modificaciones y remodelación necesaria para acceso a personas con discapacidad.
University Park y area de la Catedral

Oeste del Centro de Uppsala
Otros ubicados en el centro de Uppsala
Sur del central Uppsala

La Universidad de Upsala constantemente es considerada como una de las universidades más prestigiosas de Europa y del país. La clasificación académica de universidades mundiales desarrollada por la Universidad de Shanghái Jiao Tong sitúa a la casa de estudios a segundo nivel nacional, después de la universidad médica Instituto Karolinska, posicionándose en el lugar 66 a nivel mundial y decimoctava a nivel continental.

El ranking de universidades de la revista británica "Times Higher Education" sitúa a la universidad en segundo lugar a nivel nacional y la clasifica en el lugar 71-80 a nivel mundial.





</doc>
<doc id="2893" url="https://es.wikipedia.org/wiki?curid=2893" title="Universidad de Lund">
Universidad de Lund

La Universidad de Lund (en sueco "Lunds universitet") es una de las más antiguas y prestigiosas universidades del norte de Europa y consistentemente es considerada dentro de las 100 mejores universidades del mundo. Sus orígenes se remontan al año 1425 cuando un Studium Generale franciscano fue fundado a un costado de la Catedral de Lund, lo que la convertiría en la institución de educación superior más antigua de Escandinavia, seguida por las studia generalia de Upsala en 1477 y Copenhague en 1479. Sin embargo la universidad en su forma actual no fue fundada sino hasta 1666, después de que Suecia adquiriera la región de Escania en 1658 tras el acuerdo de paz firmado con Dinamarca.

La Universidad de Lund cuenta con ocho facultades y dos campus externos ubicados en las ciudades de Malmö y Helsingborg, con una población estudiantil de alrededor de 42 000 estudiantes distribuidos en 276 programas y alrededor de 2200 cursos. La universidad mantiene acuerdos y relaciones internacionales con cerca de 600 otras universidades en más de 70 países y pertenece a la Liga de Universidades de Investigación Europeas así como a la red global Universitas 21.

Dos importantes instalaciones para la investigación en materiales se ubican en la Universidad de Lund: El Laboratorio MAX IV, un laboratorio de radiación sincrotrón líder a nivel mundial y la Fuente Europea de Neutrones por Espalación (ESS), una instalación de la Comunidad Europea que alojará la fuente de neutrones más poderosa del mundo.

La casa central de la universidad y sus edificios más tradicionales se concentran alrededor del parque Lundagård (adyacente a la Catedral de Lund), con departamentos repartidos en diferentes ubicaciones de la ciudad pero que en su mayoría se ubican en una franja que va desde el parque hacia el norte, conectando con la zona del hospital universitario y continuando hasta el campus de la Facultad de Ingeniería en la periferia noreste de la ciudad. 



</doc>
<doc id="2894" url="https://es.wikipedia.org/wiki?curid=2894" title="Universal Networking Language">
Universal Networking Language

El lenguaje universal (Universal Networking Language) es una herramienta informática utilizada para permitir que usuarios que utilizan distintos idiomas puedan comunicarse entre sí a través de la red Internet.

El desarrollo de este lenguaje está auspiciado por el Instituto de las Naciones Unidas (www.ias.unu.edu).



</doc>
<doc id="2895" url="https://es.wikipedia.org/wiki?curid=2895" title="Urbanismo">
Urbanismo

El urbanismo es el conjunto de disciplinas que se encarga del estudio de los asentamientos humanos para su diagnóstico, comprensión e intervención. El urbanismo utiliza a la geografía urbana como herramienta fundamental, e intenta comprender los procesos urbanos a fin de planificar las intervenciones para la cualificación del espacio.

La urbanística es el conjunto de técnicas que derivadas del urbanismo sirven para la intervención urbana, en ellas se sistematizan los procesos urbanos a fin de lograr una eficacia de la intervención urbana. Existen diversas corrientes del pensamiento urbanístico a decir de la planificación estratégica, el planeamiento urbanístico, la renovación urbana, entre otras.

De manera concreta es la acción de urbanización la que interviene en búsqueda de la organización de la ciudad y el territorio.

La denominación de quienes se dedican a esta profesión son los urbanistas, sin embargo de acuerdo a las normas de los países y las regiones estos pueden llamarse planificadores urbanos, peritos en urbanismo, técnicos en urbanismo, ingenieros catastrales. En muchos países, el urbanismo es una especialización o extensión de las profesiones de planificación, geografía, arquitectura o ingeniería civil.

La urbanística es la planificación de los diversos lugares y ambientes en los que se desarrolla la vida material, sentimental y espiritual en todas sus manifestaciones, individuales y colectivas, y comprende tanto los asentamientos urbanos como los rurales.
La urbanística no puede someterse en exclusiva a las normas de un esteticismo gratuito sino que su naturaleza es esencialmente funcional.

Generalmente se entiende que el urbanismo no es más que la práctica de la urbanística, la cual es la disciplina científica correspondiente a la ciencia y arte de la planificación urbana. El urbanismo tradicionalmente se ha asociado a la planificación y la arquitectura en cuanto a que estas disciplinas se aplica al conjunto de conocimientos prácticos que proporcionan las bases fundamentales para resolver los problemas de las ciudades. Esta dualidad permite entrever el carácter descriptivo y explicativo de la urbanística como ciencia frente al carácter prescriptivo del urbanismo como práctica o técnica, incluso como arte, aunque ambos enfoques son parcialmente correctos y se realimentan mutuamente.


El bienestar de la población (residente o forastera) que habita o se encuentra ocasionalmente en la ciudad o el territorio constituye el objeto último de la urbanización, término que fue acuñado por Ildefonso Cerdá el cual describía así la referida actividad:

El urbanismo empezó siendo una teoría compleja que interesó desde el primer momento a los estudiosos de la ciudad, y acabó siendo una disciplina que reúne una suma de conocimientos sustanciales relacionados con la construcción y conservación de las ciudades y con el estudio de las relaciones socio-económico-ambientales que tiene lugar dentro del fenómeno urbano, de la que se ocupa actualmente una multiplicidad de profesionales: planificadores, arquitectos, economistas, geógrafos, ingenieros, sociólogos, y de forma exclusiva los urbanistas.

Hipodamo de Mileto (considerado por muchos el primer urbanista de la historia) hizo el plan urbanístico de El Pireo, el puerto de Atenas, sobre una cuadrícula que ahora se conoce como "hipodámica", y que se ha repetido multitud de veces. Nerón también se comportó como un urbanista cuando, tras el incendio de Roma, hizo reconstruir la ciudad sobre un plan distinto del trazado original.

Felipe II recoge varias ideas urbanísticas en las leyes de Indias, cuando trata de la construcción de nuevas ciudades en el Nuevo Mundo (proceso en el que España llevó a cabo una de las mayores creaciones de ciudades de nueva planta de la historia). Desde el siglo XV en toda Europa también se fundan ciudades, aunque probablemente, en la mayoría, la idea directriz era más demostrar el poder del monarca que hacer ciudades útiles, lo que no quita para que haya unas cuantas de gran belleza.

El barón Haussmann diseñó la renovación de París. Camilo Sitte fue el miembro más destacado de la Escuela Urbana de Viena. Otros urbanistas destacados son Otto Wagner. En 1928 tuvo lugar el primer Congreso Internacional de Arquitectura Moderna y en 1947 se aprueba la ley británica de urbanismo (la Town and Country Planning Act 1947), como resultado del New towns movement.

A iniciativa del Instituto Superior de Urbanismo de la Universidad de Buenos Aires, en 1949 la Organización de las Naciones Unidas (ONU) declaró el 8 de noviembre Día Mundial del Urbanismo como fecha para recordar acciones necesarias para el bien común como el aumento de parques y zonas recreativas, la remodelación de algunas áreas ciudadanas, la terminación de obras de desarrollo urbano, la descongestión de zonas superpobladas y aquellas medidas que disminuyan la contaminación del aire y del agua. Esta fecha es el inicio de diversas iniciativas para el desarrollo urbano sostenible y un hito para las celebraciones de los urbanistas de todo el mundo.

En 1970, surge la ciudad postmoderna y rebasando el marco en el que por etimología y definición estaba constreñido el urbanismo –la ciudad-, hoy es una disciplina de objetivo mucho más amplio y se utiliza para la ordenación integral del territorio. El urbanismo, sinónimo de planificación y ordenación, se ocupa de proporcionar modelos territoriales sectorializados, donde cada uno de esos ámbitos tiene asignado un desarrollo acorde con sus aptitudes. Así, habrá unos suelos netamente urbanos, otros urbanizables, esto es, susceptibles de llegar a ser urbanos cuando las necesidades de crecimiento y expansión lo determinen, y, por fin, suelos no urbanizables sin ninguna expectativa de evolución hacia espacios cívicos.

Con el colapso de la Unión Soviética y el fin de la Guerra Fría simbólicamente representado por la caída del muro de Berlín se fue diluyendo el enfrentamiento bipolar que caracterizó la historia del siglo XX de posguerra. Se acentuaron entonces preocupaciones de índole global, como el Cambio climático o el Calentamiento global que derivaron por ejemplo en la inclusión del Protocolo de Madrid de 1991 como parte del Sistema del Tratado Antártico. Esta multiplicación de las preocupaciones ambientales incentivó la creación de carreras universitarias conexas y la incorporación del planeamiento urbano en una mirada ambientalista. Si bien es tradicional la premisa que expresa la organicidad de las ciudades el concepto no pasó de una simple enunciación hasta el desarrollo de la teoría que en el marco de la Gran Historia las explica como organismos y su justificación ontológica como parte de un proceso de generación de entes de complejidad creciente dentro del campo del planeamiento ambiental y que pretende explicar el porqué hacia la primera década del siglo XXI la población mundial se ha vuelto mayoritariamente urbana, tal como lo plantea Marcelo Pazos, profesor de Planeamiento Ambiental en la Universidad del Salvador, en Buenos Aires.

El término «urbanismo» procede de la palabra latina "urbs" (‘ciudad’), la cual se desarrolló en la antigüedad y se refería por antonomasia a la capital del mundo romano, Roma. Aparece por vez primera en el diccionario de la Real Academia Española en 1956, donde se define como «conjunto de conocimientos que se refieren al estudio de la creación, desarrollo, reforma y progreso de los poblados en orden a las necesidades de la vida urbana». Es claro que la idea de poblado no se ajusta a la dimensión actual del urbanismo, siendo la idea de ciudad, en el sentido moderno del término, la que se adecúa más al campo de esta disciplina.

Lo urbano tiene una condición que más profundamente distingue la vida moderna de aquella tradicional-rural, no es una condición espacial ni una delimitación demográfica o productiva, sino una conducta, una forma de vida, que está determinada por las singulares características de la ciudad en tanto entidad material: específicamente su tamaño, densidad y heterogeneidad. Lo urbano es el efecto que el tamaño, la densidad y la heterogeneidad de la ciudad tienen sobre el carácter social de la vida colectiva.

Aunque el término urbanismo se utilizó inicialmente para designar todos los fenómenos de ordenación urbana, a medida que el fenómeno constructivo y edificatorio ha traspasado el espacio propiamente urbano, dicho término ha sido desplazado en la práctica por el de "Ordenamiento territorial" cuando se quiere hacer referencia a intervenciones en suelos extra urbanos, donde entran en juego intereses supralocales protegidos desde instancias públicas superiores: defensa nacional, carreteras, medio ambiente, etc. En España, el término Ordenación del Territorio se emplea también para la planificación en ámbitos supramunicipales, en los que generalmente existen relaciones funcionales importantes entre los municipios y se aprecia la necesidad de coordinar los planes urbanísticos municipales.

En la actualidad el término urbanismo se aplica a la ordenación urbana; a todos los conocimientos relacionados con la construcción de ciudades o núcleos urbanos, y se distingue del término “urbanización”, el cual está, hoy en día, directamente relacionado con los procesos constructivos, pero no con la ordenación urbana. El término ordenación del territorio se utiliza, en cambio, para designar la actividad urbanística orientada a la planificación del suelo interlocal, desde una óptica más amplia de ordenación espacial, abarcando ámbitos de carácter rural.

En el mundo desde hace varias décadas, el urbanismo se imparte en las universidades como disciplina liberal e independiente de otras profesiones. Podemos encontrar más de 100 universidades de distintos países, que brindan esta carrera universitaria empleando denominaciones como: Urbanismo, Licenciatura en Urbanismo, Planificación del Territorio y Medio Ambiente, Ingeniería Urbana, Planeamiento Urbano, Planificación de Ciudades, Urbanística y Medio Ambiente, Topografía urbana, entre otros.

En Latinoamérica, el primer programa de estudios conducente a Maestría en Planificación en las áreas de Planificación Urbana y Territorial, Sociedad y Ambiente y en Desarrollo Económico y Comunidad fue la Escuela Graduada de Planificación de la Universidad de Puerto Rico - Recinto de Río Piedras, creado el 30 de abril de 1965. Este programa se encuentra acreditado por el Planning Accreditation Board (PAB), el American Institute of Certified Planners (AICP) y el Association of Collegiate Schools of Planning (ACSP), entre otros. Es requisito para ejercer en Puerto Rico pasar examen de revalida para la obtención de licencia otorgada por la Junta Examinadora de Planificadores Profesionales de Puerto Rico. Posteriormente, a nivel licenciatura se implantó la carrera de urbanismo en la Universidad Simón Bolívar de Venezuela;luego la carrera se implementó en México; en la máxima casa de estudios, Universidad Nacional Autónoma de México (UNAM), Universidad Autónoma Metropolitana (UAM)‚ la Universidad Autónoma de la Ciudad de México (UACM)‚ en el Centro Universitario de Arte, Arquitectura y Diseño de la Universidad de Guadalajara, Perú. En la Argentina se dicta la Licenciatura en Urbanismo en la Universidad Nacional de General Sarmiento (UNGS). Brasil y en Colombia se implantó el Programa de Gestión y Desarrollo Urbanos en la Universidad del Rosario y el Programa de Urbanismo en la Universidad de La Salle (Bogotá). En Bolivia existe la Carrera de Planificación Territorial en la y en la Universidad Mayor de San Simón. El caso europeo es liderado por Holanda y Francia y en América del Norte por Canadá.

No obstante, aún perdura la formación de urbanistas como una especialización al nivel de postgrado de disciplinas afines, tales como la Planificación, la Arquitectura, la Ingeniería Civil, la Ecología, la Geografía, la Economía y la Sociología, entre otras.

El Urbanismo Táctico incluye en la forma en la que las ciudades crecen, interviniendo espacios existentes sub utilizados, asignándoles un uso que no es necesariamente de ocupación arquitectónica o permanente, rescatando posibilidades para que los espacios sean utilizados por las personas y valorizar la ciudad. Este tipo de acciones tiene la finalidad de hacer la ciudad un lugar más humano (Mike, 2012, pág. 54)

El Urbanismo Táctico trata de transformar espacios públicos, barrios, vías, en intervenciones con privilegio al peatón, dejando al vehículo en segundo plano. También proponer estrategias para fomentar medios de transportes alternativos y sustentables . Las intervenciones son realizadas por grupos de acciones, rescatando espacios públicos hechos para el vehículo, cada propuesta es de bajo costo y materiales reciclables. Cada intervención revaloriza la ciudad ,propone cuidar el medio ambiente dando un cambio visual a los espacios públicos, también presentar posibles soluciones a problemas de planificación, dando una identidad local de gran impacto.

Mike Lydon (2012) se refiere a esta disciplina como «una aproximación deliberada a hacer ciudad, un ofrecimiento de ideas locales para retos de planificación local con compromisos a corto plazo y expectativas realistas, planteando intervenciones de bajo riesgo con posibilidad de altas recompensas».

En Latinoamérica el Urbanismo Táctico, comprende acciones de corto plazo para generar cambios de largo plazo, trata de solucionar problemáticas ciudadanas, inequidad, y más.

Por otro lado, busca destacar el valor de lugares públicos dando soluciones livianas, rápidas y baratas hechas por personas con creatividad. Ya sea en el ámbito de la vivienda, la economía local de un barrio o el transporte. Éstas nacen como respuesta frente a un histórico escenario de escasez que hoy se encuentra en un cruce de caminos: entre una heredada informalidad y una necesaria formalización de los procesos urbanos. Esto se refleja en múltiples casos de estudio tales como vendedores ambulantes, ferias libres, ocupaciones de predios por juntas de vecinos o prácticas informales que por falta de canales institucionales o voluntad política, operan de forma reactiva buscando acortar la brecha de inequidad y representatividad en la ciudad (Steffens, 2013).

En Latino América lugares como Chile, Perú, Ecuador y más, replantean lugares públicos trasformando en oportunidades para la misma ciudad. Trata de fomentar la participación ciudadana para dar posibles soluciones a las problemáticas sociales, económicas y urbanísticas. Proponer estrategias que impulsen fomentar medios alternativos con materiales reciclables y económicos. Las ciclovías ofrecen rehabilitar calles ocupadas por el vehículo privado y transporte público para mejorar el desarrollo sostenible de barrios, ciudades y más. Es muy importante que en el espacio público los peatones puedan movilizare sin ninguna dificultad como son las barreras arquitectónicas y automóviles.

Uno de los objetivos del urbanismo táctico en latinoamericana es humanizar los espacios públicos urbanos rehabilitando cada sector dando posibles soluciones a problemas de planificación en las ciudades. Esta estrategia no solo lo pueden realizar urbanistas y arquitectos si no personas dispuestas a dar una nueva visual a espacios públicos promoviendo a utilizar la bicicleta o transporte alternativos, rehabilitando áreas verdes, pintar pasos cebras de colores,privilegiando al peatón.

·       Es un enfoque intencionado y progresivo para promover el cambio.

·       Ofrece ideas locales para desafíos en la planificación local

·       Comprende compromisos a corto plazo y expectativas realistas

·       Supone un bajo riesgo, con una posible gran recompensa.

·       Desarrolla el capital social entre ciudadanos y construye la capacidad de organización entre instituciones públicas/privadas, ONG’s y sociedad civil

·       Ayuda a generar ciudadanía, ya que estimula el sentido de colaboración entre vecinos, se convierte en un ejercicio de opinión y trabajo comunitario

      “Bombardeo de sillas” es una táctica urbana de fácil implementación, con el objetivo de activar el espacio público de forma ágil y experimental.

·       “Cine vivo" es una agrupación de cine que trata de un furgón preparado para proyectar una película al aire libre.

·       “Biblioteca Móvil A47" trata de una biblioteca móvil que transporta más de tres mil libros para su consulta gratuita, además de tener incorporada un espacio para conferencias y proyecciones.

·       “Paradero para libros de parques" es un programa de la Secretaría de Cultura, Recreación y Deporte y el Instituto Distrital de las Artes, en convenio con la lectura.

·       “Plantón Móvil" grupo de flores, arbustos y árboles marchan entre los edificios, el tráfico , transportar por carros, carretillas, bicicletas, mochilas, manos o cabezas, en la búsqueda de su lugar en la ciudad.

·       “Malón Urbano” el objetivo es invitar a los vecinos a sacar sus mesas a la calle.

·       “Jardinería de guerrilla” colocar elementos de jardinería en espacios donde no existe y donde no hay un permiso legal para hacerlo.

·       “Calles abiertas” espacios temporales para caminar,ir en bicicleta o asistir a actividades sociales.

·       “Cebra de colores” donde calles enteras se convierten en espacio público.

·       “Des-pavimentado” quitar pavimento innecesario para colocar área verde.

·       “ Parques o Tiendas Pop-up” espacios residuales que se convierten temporalmente en áreas públicas o comerciales.

·       “Iniciativas de mejoras de cuadra” Con materiales baratos o donados en calles comerciales transformándolas en EP, ciclo vías y quitando espacio vehicular.      











</doc>
<doc id="2897" url="https://es.wikipedia.org/wiki?curid=2897" title="Uniola">
Uniola

Uniola es un género de plantas herbáceas, perteneciente a la familia de las poáceas. Es originario de Norteamérica.



</doc>
<doc id="2899" url="https://es.wikipedia.org/wiki?curid=2899" title="Utopía">
Utopía

Por utopía el "Diccionario de la lengua española" entiende dos cosas: en primer lugar, el «plan, proyecto, doctrina o sistema deseables que parecen de muy difícil realización» y en segundo lugar, la «representación imaginativa de una sociedad futura de características favorecedoras del bien humano», esto es, un gobierno político tan perfecto e idealizado que es prácticamente imposible llegar a él.La palabra proviene del helenismo "Utopia", isla imaginaria con un sistema político, social y legal perfecto descrita por Tomás Moro en 1516, durante el renacimiento.

Utopía deriva del griego "οὐ" ("no") y "τόπος" ("lugar") y significa literalmente ""no-lugar"", más el sufijo latino "-ia" o, como glosó Quevedo; "no hay tal lugar". La palabra fue acuñada por Tomás Moro para describir una sociedad ideal, y por lo tanto inexistente. Esta "república" es imaginada como mejor que las conocidas, en especial la europea del Renacimiento, por lo cual el término puede ser interpretado como "Eutopia", también derivado del griego; "εὖ" ("bueno" o "bien") y "τόπος" ("lugar"), significando "el buen lugar", en oposición a la distopía o "mal lugar".
En un sentido estricto, el término hace referencia a la obra homónima de Tomás Moro; "Dē Optimo Rēpūblicae Statu dēque Nova Insula Ūtopia". En ella, "Utopía" es el nombre dado a una isla y a la comunidad ficticia que la habita, cuya organización política, económica y cultural contrasta con la sociedad inglesa de la época. 

Se conocen narraciones antiguas que contienen elementos utópicos, a las cuales se las considera precursoras del género. Entre ellas pueden mencionarse Dilmún de la mitología mesopotámica, Esqueria, la isla de los feacios descripta en la Odisea, y la evocación de la Edad de Oro en Hesiódo. En la Biblia el libro de Ezequiel esboza un Israel utópico y la literatura de Qumrán también describe, sucintamente, una sociedad perfecta, ambas ubicadas en el futuro y realizadas con intervención divina. En el helenismo hay elementos utópicos en la descripción de Pancaya, la isla de la "Inscripción sagrada" del relato de Evémero y de la "isla del Sol" de Yambulo. En otras tradiciones culturales aparecen utopìas como "La fuente del jardín de los duraznos "de Tao Yuanming, "La Ciudad Virtuosa" (una idealización de Medina en tiempos de Mahoma) de Al Farabi y "l"as narraciones sobre Ketumati, el paraíso futuro en algunas escuelas budistas, lleno de palacios hechos de gemas y rodeado de árboles maravillosos, donde no existe el hambre. En la Edad Media, una obra utópica singular es La ciudad de las damas de Christine de Pizan.

El término utopía, sin embargo, se debe a Tomás Moro, quien tituló así una de las obras más importantes de este género. Tomás Moro bautizó con este término una isla idílica, ubicada cerca de las costas (entonces inexploradas) de América de Sur, cuyos habitantes habían logrado el Estado perfecto, caracterizado por la convivencia pacífica, el bienestar físico y moral de sus habitantes, y el disfrute común de los bienes. La obra se basa en los valores del Humanismo y utiliza como marco narrativo las narraciones de Américo Vespucio sobre las tierras americanas, el Nuevo Mundo, en particular la descripción de la isla de Fernando de Noronha y las costas del Brasil. En la utopía de Moro aparece la crítica social puesta en boca de un extranjero o un viajero pero, a diferencia de las narraciones moralizantes o las sátiras, se propone una alternativa en la forma de una comunidad imaginada, la cual se ubica en los límites del espacio habitado. 

Con esta obra Moro dio origen a un género literario en el cual la descripción de una sociedad perfecta es el elemento central de la trama. Estas sociedades aparecen ubicadas en los extremos del mundo o en un futuro más o menos distante, pero siempre se trata de proyectos humanos, exentos de intervenciones sobrenaturales o maravillosas. Por extensión, se denominaron utopías a todas las narraciones precedentes que contenían los elementos básicos de la Utopía de Moro y al mismo concepto de "sociedad ideal". Al final de su libro, el propio Moro considera virtualmente irrealizable una sociedad utópica en Europa, de modo que utopía, desde su origen, establece la premisa de que tal sociedad no puede existir en el mundo del autor y que, por lo tanto, la utopía es meramente imaginaria e imposible. A partir de aquí arranca el uso peyorativo del término.

Derivado de utopìa es el adjetvo "utopismo", que indica aquel pensamiento, propuesta que se concentra en la descripción de comunidades imaginadas igualitarias y armónicas, frecuentemente sin considerar los medios para establecerlas. 

En el siglo XIX, aparece en idioma inglés, la palabra distopía (su primer uso es un discurso de John Stuart Mill sobre la política agraria en Irlanda) modelada expresamente como contraria a Utopía.El español la registra a partir del siglo XX. Relacionado con ella, pero como un género meramente literario, Charles Renouvier acuñó el concepto de ucronía, definida como la utopía en la Historia.

A pesar de este carácter novelado o ficticio de las utopías, a lo largo de la historia del pensamiento se les han atribuido funciones que van más allá del simple entretenimiento.

Todas las utopías posteriores a Moro tienen en común dos rasgos: describen sociedades que están fuera del mundo conocido, en ningún lugar, y siempre se trata de comunidades prácticamente cerradas, sin contacto con el mundo exterior, en especial con las sociedades europeas. Además, si bien tienen una historia previa, aparecen invariables en el tiempo. Todas, por otra parte, presentan elementos de orden, simetría y rigidez. Algunos autores señalan que su premisa es diseñar las condiciones sociales necesarias para llevar a la práctica los valores éticos de justicia e igualdad, agregan que, en ese empeño se ignora el valor de la libertad individual, que ha surgido, precisamente, con el humanismo renacentista.

En Occidente, el primer modelo de sociedad utópica fue imaginado por Platón. En uno de sus diálogos más conocidos, "República", (en griego Πολιτεία Politeia, que proviene de πόλις pólis, denominación dada a las ciudades estados griegas), además de la defensa de una determinada concepción de la justicia, hallamos una detallada descripción de como sería el Estado ideal, es decir, el Estado justo. Platón, profundamente descontento con los sistemas políticos que se habían sucedido en Atenas, especialmente con la democracia, imagina cómo se organizaría un Estado que tuviese como objetivo el logro de la justicia y el bien social.

Según Platón, el Estado perfecto estaría formado por tres clases sociales: los gobernantes, los guardias y los productores. Cada una de estas clases tendría una función, unos derechos y unos deberes rígidamente diferenciados.

A los gobernantes les concerniría la dirección del Estado; a los guardias su protección y defensa; a los productores el abastecimiento de todo lo necesario para la vida: la alimentación, ropa, viviendas, etcétera.

Cada uno sería educado para desempeñar eficientemente las funciones de su grupo: la sabiduría para los gobernantes; el coraje para los guardias, y la apetencia para los productores. Pues para Platón, la buena marcha del Estado depende de que cada clase cumpla eficientemente con su cometido.

En definitiva "La República" de Platón sería, según él, una sociedad justa porque en ella gobernarían los más sabios (filósofos) y las otras dos clases desempeñarían las funciones que les habían sido asignadas.

En "La ciudad de Dios", Agustín de Hipona expresa su interpretación de la utopía siguiendo los preceptos de su visión cristiana. Según este Padre de la Iglesia, la acción terrena (que simboliza para él todos los estados históricos) es fruto del pecado, pues habría sido fundada por Caín y en ella sus habitantes serían esclavos de las pasiones y solo perseguirían bienes materiales. Esta ciudad, por tanto, no podría según él dejar de ser imperfecta e injusta. Sin embargo, Agustín de Hipona concibe la utopía en una ciudad espiritual. Esta habría sido fundada por Dios y en ella reinarían el amor, la paz y la justicia; sin embargo, para Agustín esta utopía solo sería alcanzable de manera espiritual hasta la venida del Reino de Cristo. En tanto, la Iglesia es el mejor ejemplo posible de sociedad perfecta, pero inmersa en el seno de una sociedad imperfecta.

Durante el Renacimiento se produjo un florecimiento espectacular del género utópico. La mayoría de los pensadores consideraba que la influencia del humanismo era la causa de este fenómeno.
El Renacimiento es una época que, además de caracterizarse por el auge espectacular de las artes y las ciencias, destaca también por los cambios sociales y económicos. Sin embargo, estas transformaciones no fueron igual de positivas para todos, ya que ocasionaron enormes desigualdades entre unos miembros y otros de la sociedad.

Muchos de los pensadores de la época, conscientes de estas injusticias, pero también de la capacidad reformadora del ser humano, reaccionaron frente a la cruda realidad de su tiempo. Esta reacción se plasmó en la reivindicación de una racionalización de la organización social y económica que eliminase una gran parte de estas injusticias.

De ésta creencia y confianza en que la capacidad racional puede contribuir a mejorar la sociedad y a hacerla más perfecta, surgen los modelos utópicos renacentistas. El principal y más importante modelo utópico de esta época es, indiscutiblemente, "Utopía" de Tomás Moro.

"Utopía" se divide en dos partes: la primera supone una aguda crítica a la sociedad de la época; la segunda es propiamente la descripción de esa isla localizada en ningún lugar, en la que sus habitantes han logrado construir una comunidad justa y feliz. Básicamente, el secreto de la Utopía se debe a una organización política fundada racionalmente, en la que destaca la abolición de la propiedad privada, considerada la causa de todos los males e injusticias sociales.

La ausencia de propiedad privada comporta que prevalezca el interés común frente a la ambición y el interés personal que rige en las sociedades reales. En "Utopía", además, impera una estricta organización jerárquica de puestos y funciones, a los que se accede como en la república platónica, por capacidad y méritos.

Esta estricta organización es, sin embargo, completamente compatible con la total igualdad económica y social de los utopianos, pues todos disfrutan de los mismos bienes comunes, al margen de su función y su tarea en la comunidad.

También pertenece al Renacimiento la comunidad ideal de Telema, dedicada a cultivar el amor (aunque también incluye una fina sátira de la vida monástica), que brevemente presenta François Rabelais en su "Gargantúa" (1532). Aunque ya del S. XVII, pueden considerarse como utopías renacentistas tardías "La ciudad del Sol", del religioso italiano Tommaso Campanella, y "La Nueva Atlántida", de Francis Bacon. Esta última añade un elemento novedoso e importante, como es el aprovechamiento de los avances científicos y técnicos que entonces empezaban a darse (y más aún quizá, los que se esperaban para el futuro próximo), en la mejora de las condiciones de vida de los seres humanos.

En los siglos XVII y XVIII se asoció la utopía con la literatura de viajes, en la cual las sociedades civilizadas proyectaban solo en ocasiones sus angustias y sus críticas al progreso "El origen de la desigualdad entre los hombres" (1755) de Jean-Jacques Rousseau sería un ejemplo clásico de esta concepción de la historia como un proceso de decadencia.

Pero este no es más que un caso particular en el desarrollo impresionante de las utopías en el siglo XVIII, y en su vinculación a la crítica social (a veces comunista) y a la idea de progreso a finales de la Ilustración.

Otro de los momentos fecundos en la ideación de sociedades utópicas fue principios del siglo XIX. Los profundos cambios sociales y económicos producidos por el industrialismo cada vez más individualista e insolidario abonaron el terreno del descontento y la crítica, así como el deseo de sociedades mejores, más humanas y justas.

De esta época de injusticias y desigualdades proviene el socialismo utópico. El socialismo utópico venía con diseño de soluciones para males e imperfecciones flagrantes. Charles Fourier (1772-1837), Henri de Saint-Simon y Robert Owen tenían en común un interés imperioso por transformar la precaria situación del proletariado de ese momento. A pesar de las diferencias que hay entre ellos, tienen en común su interés por mejorar y transformar la precaria situación del proletariado en ese momento. Para ello, propusieron reformas concretas para hacer de la sociedad un lugar más solidario, en el que el trabajo no fuera una carga alienante y en el que todos tuviesen las mismas posibilidades de auto-realizarse.

A diferencia de muchas de las utopías anteriores, la de estos socialistas fue diseñada con el objetivo inmediato de llevarse a la práctica. Más que relatos fantásticos de mundos perdidos o inalcanzables, constituyeron descripciones detalladas de comunidades igualitarias que, en ocasiones, fueron copiadas en la realidad. Algunos de estos socialistas compaginaron la reflexión teórica con labores prácticas y concretas de reforma social. Así, por ejemplo, Fourier propuso comunidades autosuficientes, a las que llamó falansterios, y Owen llegó a fundar Nueva Armonía, una pequeña comunidad en la que se abrió el primer jardín de infancia y la primera biblioteca pública de EE. UU..

El término viene del "Manifiesto del Partido Comunista" y Friedrich Engels profundizó al respecto en su obra "Del socialismo utópico al socialismo científico."

Muchos autores, como Arnhelm Neusüss, han indicado que las utopías modernas son esencialmente diferentes a sus predecesoras. Otros en cambio, señalan que en rigor las utopías solo se dan en la modernidad y llaman "cronotopías" o "protoutopías" a las utopías anteriores a la obra de Moro. Desde esta perspectiva, las utopías modernas están orientadas al futuro, son teleológicas, progresistas y sobre todo son un reclamo frente al orden cósmico entendido religiosamente, que no explica adecuadamente el mal y la explotación. Así las utopías expresan una rebelión frente a lo dado en la realidad y propondrían una transformación radical, que en muchos casos pasa por procesos revolucionarios, como expresó en sus escritos Karl Marx.

Se ha criticado que las utopías tienen un carácter coercitivo. Pero también se suele añadir que las utopías le otorgan dinamismo a la modernidad, le permiten una ampliación de sus bases democráticas y han sido una especie de sistema reflexivo de la modernidad por la cual esta ha mejorado constantemente. Por ello no sería posible entender la modernidad sin su carácter utópico.

Las utopías han tenido derivaciones en el pensamiento político -como por ejemplo en las corrientes socialistas ligadas al marxismo y el anarquismo-, literario e incluso cinematográfico a través de la ciencia ficción social. La clasificación más usada, hereda la pretensión del marxismo de estar elaborando un socialismo científico y por tanto restringe el nombre de socialismo utópico a las formulaciones ideológicas anteriores a éste, aunque todas ellas comparten su origen en la reacción a la revolución industrial, especialmente a la condición del proletariado, siendo su vinculación al movimiento obrero más o menos próxima o cerca a ello.

Las utopías socialistas y comunistas se centraron en la distribución equitativa de los bienes, con frecuencia anulando completamente la existencia del dinero. Los ciudadanos desempeñan las labores que más les agradan y que se orientan al bien común, permitiéndoles contar con mucho tiempo libre para cultivar las artes y las ciencias. Experiencias prácticas que habían sido plasmadas en Comunidades utópicas en el siglo XIX y XX.

Las utopías capitalistas o de mercado libre se centran en la libre empresa, en una sociedad donde todos los habitantes tengan acceso a la actividad productiva, y unos cuantos (o incluso ninguno) a un gobierno limitado o mínimo. Allí los hombres productivos desarrollan su trabajo, su vida social, y demás actividades pacíficas en libertad, apartados de un Estado intromisorio y expoliador. Se relacionan en especial al ideal del liberalismo libertario.

La utopía ecologista se ha plasmado en el libro Ecotopía, en el cual California y parte de los estados de la costa Oeste se han secesionado de los Estados Unidos, formando un nuevo estado ecologista.

Una utopía global de paz mundial es con frecuencia considerada uno de los finales de la historia.

La visión que tienen tanto el Islam como el cristianismo respecto al paraíso es el de una utopía, en especial en las manifestaciones populares: la esperanza de una vida libre de pobreza, pecado o de cualquier otro sufrimiento, más allá de la muerte (aunque la escatología cristiana del "cielo" al menos, es casi equivalente a vivir con el mismo Dios, en un paraíso que asemeja a la Tierra en el cielo). En un sentido similar, el nirvana del budismo se puede asemejar a una utopía. Las utopías religiosas, concebidas principalmente como un jardín de las delicias, una existencia libre de toda preocupación con calles cubiertas de oro, en una gozosa iluminación con poderes casi divinos.

Las utopías tecnológicas o tecnoutopías se basan en la creencia de que los avances en ciencia y tecnología conducirán a una utopía, o al menos ayudarán a cumplir algún ideal utópico.


Aunque se ha argüido que los ideales utópicos pueden ser realizables, la confianza en la posibilidad y la necesidad de sociedades perfectas sufrió durante el siglo XX un considerable revés. Por varias razones, muchos pensadores defendieron que dedicarse a inventar sociedades utópicas era más perjudicial que beneficioso. Los motivos de esta consideración pueden variar de un pensador a otro.





</doc>
<doc id="2902" url="https://es.wikipedia.org/wiki?curid=2902" title="Sistema astronómico de unidades">
Sistema astronómico de unidades

El sistema astronómico de unidades, llamado formalmente «Sistema de constantes astronómicas de la IAU (1976)» (en inglés, "IAU (1976) System of Astronomical Constants"), es un sistema de unidades desarrollado para su uso en astronomía. Fue adoptado por la Unión Astronómica Internacional (UAI) en 1976, y ha sido ligeramente actualizado desde entonces. 

El sistema fue desarrollado debido a las dificultades en la medición y expresión de los datos astronómicos en el Sistema Internacional de Unidades (unidades SI), al tratar con magnitudes muy grandes. En particular, hay una enorme cantidad de datos muy precisos relativos a la posición de los objetos dentro del sistema solar que no pueden expresarse, o ser tratados convenientemente, en unidades del SI. A través de una serie de modificaciones, el sistema de unidades astronómico reconoce ahora explícitamente las consecuencias de la relatividad general, que es un complemento necesario para el Sistema Internacional de Unidades, a fin de tratar con precisión los datos astronómicos. 

El sistema de unidades astronómico es un sistema tridimensional, en el que están definidas las unidades de longitud, masa y tiempo. Las constantes astronómicas asociadas también fijan los distintos sistemas de referencia que son necesarios para informar sobre las observaciones. El sistema es un sistema convencional, en el que ni la unidad de longitud, ni la unidad de masa son verdaderas constantes físicas, y hay al menos tres medidas diferentes de tiempo. 




Aunque no son unidades SI ni unidades de la UAI, a veces se utilizan en astronomía las siguientes unidades. 

La masa de Júpiter (M o M), es la unidad de masa igual a la masa total del planeta Júpiter, 1,8986 × 10 kg . La masa de Júpiter se utiliza para describir las masas de gigantes gaseosos, como los planetas exteriores y de los planetas extrasolares. También se utiliza en la descripción de las enanas marrones. 

La masa de la Tierra (M) es la unidad de masa igual a la masa de la Tierra. 1 M = 5.9742 × 10 kg . La masa de la Tierra se utiliza a menudo para describir las masas de los planetas rocosos terrestres. La masa de la Tierra es 0,00315 veces la masa de Júpiter. 

Las distancias a las galaxias distantes no suelen ser citadas en unidades de distancia en absoluto, sino más bien en términos de desplazamiento al rojo. Las razones para esto son que la conversión de los corrimientos al rojo a distancias requieren el conocimiento de la constante de Hubble, que no fue medida con precisión hasta principios del siglo XXI, y que, a distancias cosmológicas, la curvatura del espacio-tiempo permite llegar a varias definiciones de distancia. Por ejemplo, la distancia definida como la cantidad de tiempo que tarda un haz de luz en viajar hasta nosotros es diferente de la distancia definida según el tamaño aparente de un objeto. 

Otras unidades de distancias usadas de forma más o menos informal, son las siguientes:

También se emplean algunas unidades complementarias, como el Jansky, que mide el Brillo'




</doc>
<doc id="2905" url="https://es.wikipedia.org/wiki?curid=2905" title="Valladolid">
Valladolid

Valladolid es un municipio y ciudad española situada en el cuadrante noroeste de la península ibérica, capital de la provincia de Valladolid y sede de las Cortes y el Gobierno autonómicos de Castilla y León. Cuenta, según los datos del INE de 2019, con 298 412 habitantes, siendo el municipio más poblado del noroeste español. Por su parte, el área metropolitana de la ciudad, conformada por 23 municipios, es la 20.ª de España, con una población de 414 281 habitantes (INE 2013). Tiene un área de influencia socio-económica directa de más de personas, distando solamente de Palencia y otros municipios importantes.

Aunque existen indicios de asentamientos pertenecientes al Paleolítico inferior, y yacimientos vacceos y tardorromanos, Valladolid no tuvo una población estable hasta la repoblación de la cuenca del Duero, cuando Alfonso VI entregó a su valido Pedro Ansúrez su señorío, en 1072. Durante la Edad Media fue sede de la corte de Castilla siendo dotada de ferias y Fuero Real y de distintas instituciones como Iglesia Colegial, elevada a rango de Catedral en 1595, Universidad, Real Audiencia y Chancillería o Casa de la Moneda.

Carlos I hizo de Valladolid capital política y, posteriormente, entre 1601 y 1606, fue capital del Imperio español hasta que esta función pasó definitivamente a Madrid. A partir de entonces se inicia un período de decadencia hasta la pujanza de la industria harinera y la llegada del ferrocarril a mediados del a cuyo amparo aparecen los primeros establecimientos siderúrgicos y la circulación del capital dando lugar en 1857 a la creación del Banco de Valladolid. En 1854, se funda "El Norte de Castilla", decano de la prensa diaria española. Tras la posguerra, la ciudad experimenta un importante cambio, debido a la instalación de industrias automovilísticas y de otros sectores.

En Valladolid, San Fernando fue proclamado rey de Castilla y se casaron los Reyes Católicos, nacieron Enrique IV, Felipe II, Felipe IV y Ana de Austria, reina de Francia, Magallanes firmó las capitulaciones de la primera circunnavegación del mundo y murió Colón. En la ciudad castellana Cervantes terminó de escribir "el Quijote" y también trabajó Quevedo. Además establecieron sus talleres los más grandes imagineros y orfebres del Renacimiento hispano.

Conserva en su casco antiguo un conjunto histórico compuesto por palacios, casas nobles, iglesias, plazas, avenidas y parques, junto con un patrimonio museístico en el que destacan el Museo Nacional de Escultura, el Museo de Arte Contemporáneo Patio Herreriano o el Museo Oriental, así como las casas-museo de José Zorrilla, Colón y de Cervantes. Entre los acontecimientos que cada año se celebran en la ciudad destacan su Semana Santa, la Semana Internacional de Cine de Valladolid (SEMINCI), la Feria Internacional de Turismo de Interior (INTUR), Pingüinos, el Concurso Nacional de Pinchos y Tapas "Ciudad de Valladolid" o el Festival de Teatro y Artes de Calle (TAC).

Su estratégica posición y comunicación a través de una amplia red de autovías, alta velocidad (AVE), ferrocarril convencional, aeropuerto, y su carácter de nodo logístico en el Corredor Atlántico europeo, seguirán permitiendo su especialización como polo industrial de Castilla y León.

Sobre el origen del nombre hay varias teorías pero poca evidencia. Una teoría afirma que en la época andalusí se llamó Balad al-Walīd بلد الوليد, que significa "puebla de Walid" en alusión quizá al califa omeya Walid I, que gobernaba el Imperio islámico en el momento de la conquista árabe. Relacionadas con esta, existen también las etimologías "Valledolit", "Vallis Oleti" o "Valle de Olit", un árabe que supuestamente poseía la ciudad. Otro posible origen pudiera ser "Vallis olivetum"; es decir, Valle de los Olivos, aunque dado el clima con fríos inviernos y con frecuentes heladas entrada ya la primavera que tiene la ciudad no es muy probable que hubiera gran cantidad de olivos en la zona. Otra teoría, más aceptada que las anteriores, afirma que el origen de la palabra proviene de la expresión celta "Vallis tolitum" (Valle de Aguas), ya que por la ciudad pasan el río Pisuerga y el río Esgueva, que antes de su canalización, en el , se extendía por varios ramales. Otra teoría, y esta más probable, es por el gentilicio "vallisoletano", que se cree que proviene de "valle del sol" o "valle soleado"; en la Edad Media era llamada "Vallisoletum".

También existe la teoría de Valladolid como contracción de "valle de lid", lugar, por su llanura, donde se reunían los clanes y tribus prerromanos para sus enfrentamientos armados.

El historiador Ángel Montenegro Duque sostiene que bien podría ser la "Tola" del itinerario de Antonino de Ptolomeo, y apunta al origen céltico del topónimo, por la raíz "tollo" (lugar de aguas). Pero, siendo un poblado de los vacceos, "Vaccea tollit" (Solevantado de los Vacceos, o lugar elevado de los vacceos) parece un nombre más probable que "valle tollitum", dado que "Tolitum" evoluciona a "Toledo". El origen latino de Valladolid sería así un caso de "falsos amigos" entre "Tollo" y "Tollere". "Vaccea Tollit" parece el origen etimológico de "Vallatolit" (), que fonéticamente evolucionó de forma natural a "Valladolid" .

El término "Pucela" se utiliza también, de forma popular, para denominar a la ciudad. De la procedencia de esta palabra existen varias teorías, que sitúan su aparición en el .


Por último, se encuentra el término de "Pintia", que parece tener un origen mucho más culto. Cerca de Peñafiel, en la localidad de Padilla de Duero, se encuentran las ruinas de una importante ciudad, presuntamente celta: Pintia, perteneciente al pueblo prerromano de los vacceos. El identificar a Valladolid con esta ciudad proviene del Renacimiento y la costumbre que imperaba en aquella época de relacionarlo todo con las civilizaciones griega y romana. Posteriormente, se demostró la inexistente relación entre Valladolid y Pintia.

Hay indicios datables en el Paleolítico Inferior, esencialmente Achelense, recogido en superficie en las terrazas cuaternarias del río Pisuerga, en "Canterac" (que actualmente es un gran parque situado a las afueras); pero no se puede decir que la ciudad tuviera una ocupación estable hasta la Edad Media, que es posiblemente cuando surgió el topónimo que le da nombre.
Los asentamientos posteriores en la actual provincia de Valladolid datan de épocas prerromanas, existiendo en la zona yacimientos de pueblos vacceos, que fueron pobladores de cultura muy avanzada, y, como el resto de pueblos célticos, llegaron a la península procedentes del norte de Europa. El máximo exponente de esta cultura en las cercanías, que fue arrasada por los romanos, es Pincia ("Pintia"), en la actual localidad de Padilla de Duero.

Durante años, se creyó que Valladolid era la antigua Pincia, hasta que las excavaciones arqueológicas demostraron la verdadera ubicación de la ciudad vaccea. En varias zonas del casco antiguo de la ciudad han aparecido restos de época romana: junto a la iglesia de la Antigua aparecieron evidencias constructivas de una villa de cierta entidad (siglos -), así como en las calles Angustias, Arribas, Juan Mambrilla y en las del Empecinado y Padilla, donde se tiene constancia de la aparición de varios mosaicos romanos. También ha habido hallazgos en puntos periféricos de la ciudad; en los alrededores del Monasterio de Nuestra Señora de Prado se descubrió en los años 50 otra villa: la "Villa romana de Prado", la cual acoge un amplio conjunto arquitectónico residencial, acompañado de mosaicos. De hecho, un gran mosaico de mármol y caliza, el "Mosaico de los cantharus" (datado en el ), preside el hemiciclo de las Cortes de Castilla y León (depositado por el Museo de Valladolid).

En el Alfonso III de Asturias consolidó la frontera del Reino de Asturias hasta el Duero, pasando a formar parte del Condado de Castilla. En el , durante la repoblación de la Meseta, el rey Alfonso VI de León encargó al conde de Saldaña y Carrión, Pedro Ansúrez, y a su esposa, doña Eylo Alfonso, el poblamiento y expansión del primitivo núcleo agrario, que ya existía y se organizaba mediante Concejo abierto. Alfonso VI otorgó el señorío de la misma al conde en 1072, fecha a partir de la cual se produjo el crecimiento de la ciudad. Este hizo construir un palacio para él y su esposa, doña Eylo, que no se conserva, así como la Colegiata de Santa María (lo que le otorgó el rango de villa) y la iglesia de La Antigua. En 1208, el rey Alfonso VIII de Castilla la nombró ciudad cortesana y en 1255 Alfonso X le otorgó el Fuero Real.

Tras la temprana muerte de Enrique I de Castilla, nacido en Valladolid, y la abdicación de su madre, Fernando III "el Santo" fue proclamado en 1217 rey de Castilla, en acto realizado en la Plaza Mayor de Valladolid. Durante los siglos y Valladolid experimentó un rápido crecimiento, favorecido por las ferias y privilegios comerciales otorgados por los monarcas Alfonso VIII y Alfonso X "El Sabio". Durante estos siglos, la ciudad servía ocasionalmente como residencia real y sede de las Cortes. El primer "Alcazarejo" fue transformado en Alcázar Real, y la reina María de Molina, reina y regente de Castilla, se hizo edificar un palacio y estableció allí su residencia en torno al 1300. En 1346, el papa Clemente VI otorgó la bula que permitió el paso del Estudio Particular vallisoletano, existente desde la segunda mitad del , a Estudio General o Universidad.

Juan II de Castilla se crio y murió en Valladolid habiendo reinado desde esta ciudad de la que diría que es «la villa más notable de estos mis regnos e aun fuera de ellos». Este rey fue sepultado en la iglesia de San Pablo, hasta el traslado definitivo de sus restos a la Cartuja de Miraflores. En 1425 nacía Enrique IV de Castilla en la desaparecida Casa de las Aldabas de la calle de Teresa Gil. En 1453 Álvaro de Luna, todopoderoso valido de Juan II, es juzgado, condenado y finalmente decapitado en cadalso público en la plaza Mayor. El 7 de diciembre de 1453 se firmó en la ciudad la Concordia de Valladolid, poniendo paz entre Juan de Navarra (futuro rey de Aragón) y su hijo Carlos de Viana.

El 19 de octubre de 1469 Isabel de Castilla y Fernando de Aragón (que sería Fernando II de Aragón) celebraron su matrimonio secreto en el palacio de los Vivero (luego emplazamiento de la Real Audiencia y Chancillería), y pasaron su luna de miel en el castillo de Fuensaldaña. Ya en 1481 contaba Valladolid con imprenta, situada en el monasterio de Prado, de la Orden de San Jerónimo, y bajo los Reyes Católicos la ciudad vivió una etapa de gran dinamismo universitario, que culmina en la creación de los Colegios Mayores de Santa Cruz (por el cardenal Mendoza) y San Gregorio (por fray Alonso de Burgos), lo que hizo de Valladolid uno de los semilleros de la burocracia moderna.

En 1489 se estableció definitivamente el tribunal de Chancillería, y en 1500 el de la Inquisición, para juzgar actos de herejía, dando lugar a la celebración de los Autos de Fe. En 1506 murió en Valladolid Cristóbal Colón, y fue enterrado en la ciudad, en el desaparecido convento de San Francisco. Otro navegante, Magallanes, firmó en Valladolid las capitulaciones con el rey Carlos I de España, antes de iniciar su ruta occidental hacia las Indias, el 22 de marzo de 1518. En 1509 nace en Valladolid Juan de Aragón y Foix, único hijo de Fernando el Católico y su segunda esposa Germana de Foix, que murió a las pocas horas de nacer.

En 1518 las Cortes de Castilla, reunidas en Valladolid, juraron como rey a Carlos I. Durante la Guerra de las Comunidades de Castilla, el incendio de Medina del Campo provocó el levantamiento de Valladolid y, tras la derrota comunera en Tordesillas, los rebeldes comenzaron a reagruparse en la ciudad, donde se estableció la Junta. Tras la victoria del emperador, y el perdón a los sublevados exceptuando sus cabecillas, Valladolid se convirtió en una de las capitales del Imperio español de Carlos I de España y V de Alemania, cobrando gran importancia política, judicial y financiera.

El 21 de mayo de 1527 nació el futuro rey Felipe II en el Palacio de Pimentel.

La célebre controversia de Valladolid tuvo lugar en 1550 y 1551 en el Colegio de San Gregorio y enfrentó dos formas antagónicas de concebir la conquista de América, representadas por Bartolomé de las Casas y Juan Ginés de Sepúlveda. Aquel debate se considera hoy pionero y una vital aportación en la historia a la construcción de los derechos humanos. Su resultado fueron nuevas ordenanzas que regulaban las conquistas, la creación de la figura del defensor de indios y un notable impulso del "derecho de gentes".

En 1559 se celebraron los autos de fe de mayo y octubre, famosos por su severidad. En 1561 la ciudad fue arrasada por un enorme incendio, tras el que Felipe II se comprometió a reconstruir la ciudad, dotándola de la primera plaza Mayor regular de España. Este rey concedió también a su villa natal el título de ciudad el 9 de enero de 1596 en virtud de una Real Provisión, y consiguió del papa Clemente VIII la creación de una diócesis en 1595 (elevada a archidiócesis en 1857).

San Juan de la Cruz y santa Teresa de Jesús coincidieron en Valladolid cuando la religiosa llegó a fundar en 1568 el primer convento de la reforma de la Orden del Carmen que habitó durante un tiempo. También fray Luis de León, que ya había pasado con su familia años de infancia en Valladolid, fue puesto preso en 1572 en las cárceles del Santo Oficio de la ciudad, para hacer frente a un proceso inquisitorial por cuestionar la forma tradicional de entender la Teología.

Los más insignes imagineros del Renacimiento español, Alonso Berruguete, Juan de Juni o Gaspar Becerra, establecieron sus talleres en Valladolid a su llegada de Italia.
En 1601, a instancias del valido del rey Felipe III de España, el duque de Lerma, se trasladó de nuevo la corte a Valladolid, pero se volvió a mudar en 1606. Durante este tiempo nacieron el futuro Felipe IV, y su hermana, Ana de Austria, que sería reina de Francia y madre de Luis XIV. Cabe reseñar que en este periodo llegó, en misión diplomática, el artista Peter Paul Rubens y Cervantes publicó su primera edición del "Quijote", en 1604. También residieron en la ciudad Quevedo y Góngora, y la gran gubia del barroco Gregorio Fernández.

La pérdida de la Corte supuso un gran cambio para la ciudad, que sufrió un grave proceso de decadencia, sólo mitigado a partir de 1670 con la implantación de talleres textiles que anuncian la industrialización posterior. La segunda boda del rey Carlos II, con Mariana de Neoburgo, se llevó a cabo en 1690 en la iglesia del Convento de San Diego, dentro del conjunto del Palacio Real de Valladolid.

Durante la Guerra de Sucesión Española, la ciudad tomó partido por Felipe V de España. En la segunda mitad del , la Ilustración apareció en Valladolid de una forma muy tímida, aunque influyente. Así, se arbolan espacios de la ciudad como Las Moreras, se protegen y estimulan las manufacturas, se alienta el saneamiento urbano, se empiedran calles e intentan racionalizar los vertidos de basuras. El semanario de ideología ilustrada "Diario Pinciano", sale a la luz en 1787. Se crearon la Real Academia Geográfico-Histórica de los Caballeros, la Real Academia de Bellas Artes de la Purísima Concepción en 1779, o la Real Sociedad Económica de Amigos del País de Valladolid en 1783. La economía local y de la meseta se beneficiaría de la construcción del Canal de Castilla, el proyecto más importante de ingeniería civil de la España Ilustrada, iniciativa del marqués de la Ensenada, secretario de Fernando VI, y cuyo Ramal Sur finaliza en Valladolid. En 1746 el franciscano vallisoletano Pedro Regalado fue canonizado. La ciudad sufrió grandes inundaciones en 1788, provocadas al desbordarse el río Esgueva.

Valladolid fue la ciudad elegida para albergar a las tropas francesas a su llegada a España, debido principalmente a su situación en el eje París-Madrid-Lisboa. Durante la estancia de las tropas francesas se sucedieron altercados en la ciudad, entre los vecinos y los soldados, a pesar de los continuos llamamientos a la calma por parte de las autoridades de ambos.

Tras las noticias del motín de Aranjuez, la ciudad también se amotinó desde el 24 de marzo, durante varios días; se humilló la figura de Manuel Godoy (su retrato acabó hecho pedazos y arrojado al Pisuerga), y culminó con el asentamiento del Marqués de Revilla en la regiduría fernandista. El 31 de mayo de 1808 se produce el "dos de mayo vallisoletano": el pueblo se agolpa en plazas y calles al grito de «¡Viva Fernando VII!», exigiendo, frente a las casas consistoriales, el alistamiento general, la entrega de armas, la designación de un jefe, y la proclamación de Fernando VII. El Cabildo condescendió en ello, y los manifestantes pasaron a la Chancillería. La insurrección despertó la preocupación del mariscal de Bessières. Como consecuencia, se preparó la batalla de Cabezón, que se produjo el 12 de julio, con una derrota absoluta y retirada en desbandada del ejército dirigido por García de la Cuesta, reunido en condiciones muy precarias.

Joaquín Blake participó en numerosas acciones de guerra. El 14 de julio fue derrotado junto con Cuesta en la batalla de Medina de Rioseco. Blake, de origen irlandés y presidente del Consejo de Regencia de España e Indias (1810-1811) y jefe del Estado Mayor, murió en Valladolid en 1827.

La ciudad fue finalmente liberada por el ejército mandado por Wellington, en julio de 1812. El vallisoletano Evaristo Pérez de Castro, fue diputado y primer secretario en las Cortes de Cádiz teniendo un papel activo en reclamar la soberanía nacional para las mismas tras la invasión napoleónica. Una placa en el Oratorio de San Felipe Neri en Cádiz le recuerda.

A partir de 1830, con la desamortización de Mendizábal y la reordenación en provincias del territorio español, se reactivan tímidamente el comercio y la administración. Cuando Mendizábal transfiere los inmensos huertos y jardines de los conventos y sus edificios, se aprovecha la oportunidad para abrir nuevas calles o crear servicios públicos en los nuevos edificios.

Con el desarrollo del sistema financiero, aparecieron las primeras sociedades de crédito, y en 1855 se crea el Banco de Valladolid. En 1856 se fundó en Valladolid el decano de la prensa diaria española, "El Norte de Castilla", resultado de la fusión de otros dos diarios: "El Avisador" y "El Correo de Castilla".

La llegada del ferrocarril —Compañía del Norte a partir de 1860 y Compañía de Ferrocarriles Secundarios de Castilla en 1884— a Valladolid supuso un gran impulso y marcó la dirección de crecimiento de la ciudad. Durante este siglo la ciudad no crece notablemente, pero su estructura interna cambia, se abren nuevas calles, se abren nuevas plazas y jardines, como el del Poniente, se reforma el Campo Grande, y se encauza y desvía el río Esgueva, lo que supone el fin de las inundaciones en la ciudad. Todo esto es posible gracias a la gestión de grandes alcaldes, como Miguel Íscar.

El 22 de octubre de 1887 se inauguró el alumbrado eléctrico público en Valladolid: por la noche, tuvo lugar la iluminación del Teatro Zorrilla y del Círculo de Recreo Mercantil, así como de algunos cafés y casas particulares. La central suministradora, de carácter térmico, estaba ubicada en una antigua fábrica de tejidos, en la margen izquierda del río Pisuerga; era popularmente conocida como «La Electra».

Los vallisoletanos Claudio Moyano, Germán Gamazo o José Muro serían importantes políticos en la España del .

La ciudad se expande, creciendo del otro lado de la vía férrea en el barrio que se llamará de Las Delicias. El abogado y político vallisoletano Santiago Alba ocuparía varias carteras ministeriales en diferentes gobiernos entre 1906 y 1923, y sería presidente del Congreso de los Diputados durante la II República. La ciudad vivió la inestabilidad propia de la política española de las primeras décadas del y saludó la instauración de la República en 1931. El 4 de marzo de 1934 se fusionaron Falange Española (el partido de Primo de Rivera) y las JONS (movimiento fundado por el vallisoletano Onésimo Redondo) en un acto celebrado en el Teatro Calderón.

El levantamiento del 18 de julio de 1936 con el que comenzó la Guerra Civil, triunfó en Valladolid, quedando en la zona nacional, siendo uno de los 12 centros del levantamiento militar. La guardia de asalto se sublevó a las 5 de la tarde del 18 de julio y los militares sublevados en la noche del 18 al 19 de julio de 1936 se hicieron con el control de las fuerzas militares tras detener violentamente a su legítimo jefe, el general Molero. 

Valladolid se convirtió en la primera gran ciudad peninsular en la que triunfó la sublevación. Con el importante apoyo de los falangistas y de los monárquicos alfonsinos, controlaron en poco tiempo toda la provincia, procediendo a organizar una columna que marchó sobre Madrid a través de los puertos de Guadarrama (Alto del León) y de Navacerrada. Así, la ciudad quedó desde el principio de la contienda en el interior de la zona sublevada, no perteneciendo al frente en ningún momento de la guerra. 

Durante la guerra y también una vez finalizada ésta, la represión franquista fusiló en Valladolid en torno a 40 personas cada día. Allí, como en otras ciudades de la zona sublevada, los presos eran sacados por la noche en camiones para ser fusilados en las afueras de la ciudad sin siquiera el simulacro de un juicio. El general Mola enviaría un comunicado pidiendo que estas ejecuciones se hiciesen en lugares más discretos y que se enterrase a los muertos, algo que hasta entonces no se hacía. Se estima en al menos 2500 víctimas mortales, y más de 7000 represaliados en toda la provincia. En la capital, destaca como lugar de ejecuciones el Campo de San Isidro. El cementerio del Carmen es por su parte uno de los lugares donde se ubica una de las mayores fosas comunes de la guerra civil a nivel nacional. Entre las víctimas, destaca el caso del propio alcalde de Valladolid durante la Segunda República entre 1932 y 1934, que había sido reelegido nuevamente en las elecciones de 1936, Antonio García Quintana. Tras el levantamiento, permaneció escondido hasta que fue delatado y fusilado en el Campo de San Isidro de Valladolid el 8 de octubre de 1937.

La ciudad también sufrió bombardeos de la aviación republicana, siendo la sexta ciudad de la retaguardia más bombardeada. El ataque más severo se produjo el 25 de enero de 1938, cuando la ciudad fue víctima de un bombardeo republicano, en el que murieron catorce personas y resultaron heridas otras setenta. La ciudad permanecería en el bando sublevado hasta el final de la guerra, en 1939.

En 1940 tiene lugar la peor catástrofe de este siglo en la ciudad cuando explota el polvorín del Pinar de Antequera provocando más de 100 muertos.

Tras la postración de los primeros años de la posguerra, desde los años 1950, Valladolid experimenta un importante cambio, debido a la instalación de industrias automovilísticas (como FASA-Renault) y de otros sectores (Endasa, Michelin, Nicas, Pegaso, Indal...). La absorción de miles de emigrantes procedentes del éxodo rural terracampino provoca un importante crecimiento demográfico y urbanístico. Este hecho provocó la puesta en marcha de un planeamiento urbanístico, proyectado y parcialmente ejecutado en 1938: el Plan César Cort. Como consecuencia de su aprobación, se produce la mayor pérdida de patrimonio urbano en el casco viejo de la ciudad: edificios antiguos, conventos y claustros, incluyendo decenas de palacios renacentistas, fueron demolidos para construir bloques de pisos de gran altura, que rompen la armonía arquitectónica de la ciudad. En los últimos años de la década de 1960, se inicia la construcción del edificio Duque de Lerma, que sería el más alto de Valladolid. Durante tres décadas permaneció deshabitado y en varias ocasiones a punto de ser derribado, convirtiéndose su exterior en un importante muro reivindicativo.

A partir de la década de 1970, la conflictividad social en Valladolid fue incrementándose debido a la cada vez mayor actividad de los movimientos estudiantiles y los trabajadores de la industria del automóvil, principalmente. Trabajadores de FASA promovieron paros laborales con el apoyo de asociaciones obreras de la ciudad. El 20 de enero de 1975 fueron juzgados y condenados en Madrid siete estudiantes vallisoletanos por asociación ilícita. Como respuesta a la condena, tres días después, representantes de todas las Escuelas y Facultades llevaron a cabo un encierro en el Hospital Provincial de Valladolid que terminó con el desalojo y detención por parte de la policía. Manifestaciones frente al rectorado y protestas contra el entonces rector de la Universidad de Valladolid, dieron lugar a una respuesta fulminante por parte del Ministerio de Educación que decretó los cierres de facultades y finalmente, el 8 de febrero, se dio la orden de clausurar la Universidad.

Valladolid continúa su crecimiento con la llegada de la democracia a España. Con las primeras elecciones municipales democráticas (1979), llegan los socialistas a la alcaldía: el socialista Tomás Rodríguez Bolaños se mantiene como alcalde desde 1979 a 1995, en el periodo 1991-1995 gracias a un pacto con IU, ya que el ganador de aquellas elecciones, el Partido Popular, no pudo alcanzar la mayoría absoluta. En 1995 el Partido Popular gana las elecciones por segunda vez, esta vez ya sí con mayoría absoluta y Francisco Javier León de la Riva es nombrado alcalde manteniéndose en el cargo hasta que en 2015 el Partido Popular gana las elecciones locales ( por séptima vez consecutiva ) pero pierde la mayoría absoluta y el socialista Óscar Puente Santiago pasó a ser el nuevo alcalde de la ciudad con el apoyo de Valladolid Toma la Palabra (que entró a formar parte del gobierno municipal) y Sí se Puede Valladolid.

En la década de 1980 surgen nuevos barrios residenciales (como Parquesol), que provocan un crecimiento de la ciudad en su extensión. La ciudad se convierte en sede definitiva de los poderes ejecutivo (Junta) y legislativo (Cortes) de Castilla y León mediante ley aprobada en 1987, aunque las Cortes siguieron ubicadas en el Castillo de Fuensaldaña hasta la inauguración en 2007 de su nueva sede en el barrio de Villa del Prado de la ciudad.

Personas relevantes durante el periodo democrático, muy vinculadas a la capital, son Gregorio Peces-Barba que, como diputado por Valladolid en 1977, fue uno de los «Padres» de la Constitución española, los expresidentes del Gobierno de España José María Aznar, que fue también presidente de la Junta de Castilla y León, y José Luis Rodríguez Zapatero o la vicepresidenta del Gobierno Soraya Sáenz de Santamaría.

Como ciudad significativa en la evolución de la lengua castellana, se celebró entre el 16 y el 19 de octubre de 2001, en el Teatro Calderón, el II Congreso Internacional de la Lengua Española, foro de reflexión sobre el idioma español, presidido por los reyes de España.

Valladolid fue premiada por la asociación internacional LUCI en 2011 con el Premio al Mejor Proyecto de Iluminación Urbana City People Light por la "Ruta Ríos de Luz" y en 2012 con el Premio del Jurado Popular al Mejor Proyecto de Iluminación Urbana de los Premios City People Light Awards. En 2012 Unicef declara a Valladolid Ciudad Amiga de la Infancia. En abril de 2013, Valladolid fue premiada con el Premio Reina Sofía de Accesibilidad de Municipios Españoles por su esfuerzo en la integración, normalización y participación activa de toda la ciudadanía sea cual sea su capacidad funcional.

En el año 2020 la pandemia de coronavirus provocó la declaración del estado de alarma en toda España y el confinamiento de la población en sus hogares.

La corona real es abierta, de origen medieval, más antigua que la corona real cerrada. Habría sido otorgada por los Reyes Católicos, como símbolo de villa de realengo, con fueros propios.

La bordura de gules con los ocho castillos de oro en el escudo de armas de la ciudad aparece por primera vez en la portada de uno de los más de diez ejemplares de la "Historia de Valladolid" de Juan Antolínez de Burgos que data de 1722 (si bien la obra original fue concluida en 1641). Hasta entonces nunca el escudo municipal había figurado con semejante incremento armero. La bordura viene a ser un trasunto historicista, con afán también ornamental, del antiguo sello medieval de la ciudad en donde también aparecían ocho muescas o torres formando parte del cerco o muralla que envolvía simbólicamente a la villa, identificando estos castillos con las ocho puertas de las dos cercas o murallas que llegó a tener la población representadas por la bordura. Esta composición tuvo éxito y fue paulatinamente adoptada por los diferentes gremios de la ciudad y finalmente por el concejo.

Por último, la Cruz Laureada de San Fernando, máxima condecoración militar española, creada en el , le fue otorgada por las nuevas autoridades franquistas por decreto de 17 de julio de 1939 al municipio de Valladolid por acciones de guerra llevadas a cabo por el bando sublevado para controlar la ciudad e inmediaciones en la Guerra Civil española.

La bandera de Valladolid es de color carmesí con el escudo de Valladolid situado en el centro.

Valladolid adquirió la categoría de villa a mediados del para seguir sumando títulos: buenos y leales (Muy leal) en el año 1329; Muy Noble en 1422; Ciudad en 1596; Heroica en 1854 y Laureada en 1939.

Valladolid desborda sus propios límites y salta a municipios del entorno. Esta transformación urbana ha sido definida por el catedrático emérito de Geografía urbana Jesús García como el paso «de la ciudad a la aglomeración».

Valladolid posee una población de habitantes a fecha de 1 de enero de 2017.

Partiendo del primer dato de población recogido por el Instituto Nacional de Estadística, que data de 1842, se observa un crecimiento constante de población en toda la segunda mitad del , que coincide en el tiempo con la construcción del Canal de Castilla y con la llegada del ferrocarril a Valladolid.

A lo largo de los tres primeros tercios del , Valladolid experimentó un importante aumento de población, gracias al éxodo rural. Este crecimiento, lento durante las dos primeras décadas e interrumpido por la Guerra Civil, fue especialmente significativo desde los años sesenta, con la llegada de mano de obra foránea, y supuso el momento de mayor crecimiento demográfico en la historia de la capital. Sin embargo, a partir de los años ochenta se produjo un giro en esta tendencia, que supuso un estancamiento en el crecimiento de la población, debido a dos motivos: al cese de los flujos migratorios que habían impulsado el crecimiento en épocas pasadas y a un descenso brusco en la tasa de natalidad.

En los últimos años, la ciudad de Valladolid ha ido perdiendo población en favor de su franja periurbana, donde prolifera el crecimiento de nuevas áreas residenciales. Esta cuenta con poco más de 400 000 habitantes, y es la 20.ª área de España en población. El encarecimiento de la vivienda en la capital, la falta de una política adecuada de planeamiento urbano y, como consecuencia de ello, el incremento de los problemas asociados al tráfico rodado, originaron cambios residenciales de carácter centrífugo. Las parejas jóvenes que no emigran a otras provincias optan por la adquisición de una vivienda en los municipios de la periferia, cuyo crecimiento demográfico deriva del propio vaciamiento de la ciudad (de 330 700 habitantes en 1991 a 303 905 en 2015) y del asentamiento de familias procedentes, en menor medida, de otros municipios de la provincia.

En el último lustro, Valladolid ha sufrido una paulatina pérdida de población, principalmente debido a movimientos hacia su área metropolitana.


En el año 2005 se produjeron en Valladolid un total de 2600 nacimientos. Esto supone la confirmación de una tendencia ascendente que se remonta al año 1999. Esta tasa de nacimiento es la más alta registrada desde 1992, año en el que se registraron 2658 nacimientos. Por su parte, la tasa bruta de natalidad de Valladolid se sitúa en el 8,10 ‰, que es la cifra más alta desde 1992.

En 2005 se registraron 2735 defunciones en la ciudad de Valladolid, lo que supuso un incremento con respecto a los años anteriores. Es, de hecho, la cifra más alta desde 1920, año en el que se registraron 3206 defunciones. La tasa bruta de mortalidad se cifró en 8,52 ‰, siguiendo la tendencia ascendente reflejada en el número de defunciones, y es la más alta desde 1969.

Según los datos de 2002, llegaron a Valladolid un total de 9072 personas. De este total, 2246 procedían de la propia provincia, 1721 de otras provincias de Castilla y León, 2407 de otra Comunidad Autónoma y por último 2698 personas llegaron procedentes del extranjero.

Por continentes, Europa es el más representado en Valladolid con 8680 residentes en el 2010. En cuanto al país de origen, Bulgaria aporta el mayor número de extranjeros, con 3983 frente a los 3881 de 2009. El colectivo rumano se consolida en el segundo puesto de europeos presentes en la capital con un saldo positivo de 42 habitantes (se ha pasado de los 2490 que residían en el 2009 a los 2532 que lo hacen en la actualidad).

El área metropolitana de Valladolid, como tal, no está constituida ni legal ni administrativamente, aunque existen propuestas de algunos partidos para crearla. No obstante, recibe este nombre el conjunto de municipios, que, centrados en Valladolid, están definidos por las Directrices de Ordenación del Territorio de Valladolid y su Entorno (DOTVAENT), documento realizado por el instituto de urbanística de la Universidad de Valladolid a instancias de la Junta de Castilla y León.

Precisamente esta ausencia de definición legal impide conocer con certeza su tamaño, por lo que las cifras proceden de estudios independientes o de los datos indirectos de los organismos oficiales. De este modo, según el proyecto "AUDES5 - Áreas Urbanas de España 2005", el área metropolitana de Valladolid cuenta con una población de habitantes, mientras que según los datos indirectos procedentes del Instituto Nacional de Estadística (2007) su población sería de 407 148 habitantes.

La ciudad de Valladolid se encuentra en la mitad norte de la península ibérica. Está situada en el centro de la Meseta Norte, división de la Meseta Central, por lo que presenta un paisaje típico, llano y con escasa vegetación. El relieve vallisoletano lo conforma una llanura interrumpida por pequeñas series de colinas que originan un paisaje montañoso de cerros testigos como el de San Cristóbal (), a pocos kilómetros de la capital. Las coordenadas de la ciudad son 41º 38' N 4º 43' O. La altitud del centro de la ciudad es de , mientras que la altitud máxima del municipio es de , la cual se da al noreste del mismo, entre Páramo de Cabezón y Barco de San Pedro; y la altitud mínima es de , la cual se da en el último tramo del río Duero dentro del municipio, a unos metros de su confluencia con el río Pisuerga.. 

El término municipal cuenta con dos exclaves, uno al norte de Villanubla (Navabuena) y otro al oeste de Ciguñuela (El Rebollar). 

Su céntrica situación en la Meseta Norte le hace estar casi equidistante del resto de las ciudades castellanas. Palencia está a 50 kilómetros, Zamora a 104 kilómetros, Segovia a 117 kilómetros, Salamanca a 121 kilómetros, Burgos a 127 kilómetros, Ávila a 138 kilómetros, León a 139 kilómetros y Soria a 208 kilómetros. 

El clima de Valladolid es mediterráneo continentalizado. De acuerdo a la clasificación climática de Köppen el clima de Valladolid en el periodo de referencia 1981-2010 es, en general, de tipo "Csa" (mediterráneo). Sin embargo, la temperatura media en julio y agosto supera solo ligeramente los 22 °C en la zona urbana (concretamente en el observatorio de Valladolid), pero este valor baja de los 22 °C en algunas zonas del municipio de mayor altitud, a las afueras, dándose así en esos lugares un clima de tipo "Csb" (mediterráneo de veranos suaves). El clima de Valladolid está determinado en gran medida por la ubicación de la ciudad en el centro de la cuenca sedimentaria del Duero, que, al estar casi completamente rodeada de montañas que la aíslan del mar, tiene un clima extremado y seco para lo que cabría esperar a casi 700 metros de altitud y a solo 190 kilómetros del mar Cantábrico en línea recta. Las montañas que delimitan la meseta retienen los vientos y las lluvias, excepto por el Oeste, por donde la ausencia de grandes montañas permite un pasillo abierto al océano Atlántico y es por aquí, por Portugal, por donde penetran la mayoría de las precipitaciones que llegan a Valladolid. Los vientos del norte llegan a Valladolid secos y fríos, mientras que los del sur suelen ser cálidos y húmedos, pero es por el oeste y suroeste por donde suele llegar la lluvia a Valladolid. Los vientos predominantes en Valladolid son los del suroeste, y así se ve reflejado por ejemplo en la orientación de la pista del aeropuerto de Villanubla. 

Las precipitaciones están repartidas de forma bastante irregular a lo largo del año, si bien hay un mínimo acusado en verano y un máximo en otoño y primavera. La precipitación anual es de 433 mm y la humedad relativa media a lo largo del año es del 64 %. Al año hay 2624 horas de sol y 67 días de lluvia.

En cuanto a las temperaturas tal vez lo más destacado sea la importante oscilación térmica diaria. Las diferencias térmicas entre el día y la noche superan en muchas ocasiones los 20 grados. La temperatura media anual es de 12,7 °C. Los inviernos son fríos con frecuentes nieblas y heladas (56 días de heladas de media). La ciudad cuenta con 9 días de nieve al año; aunque son infrecuentes las grandes nevadas por la particular situación geográfica de la ciudad. En las calmas anticiclónicas de invierno, principalmente en las largas noches de diciembre y enero, la inversión térmica produce nieblas, que pueden no levantar en todo el día. Es en diciembre y enero cuando puede aparecer la cencellada. La primavera en sus comienzos aún tiene el frío del invierno, para pasar a ser bastante suave y agradable a medida que nos acercamos al verano. Los veranos son, por lo general, calurosos y secos, con máximas entre 30 °C y 35 °C, pero mínimas suaves, superando ligeramente los 14 °C. En la ciudad es posible registrar alguna noche con mínima tropical superior a 20 ºC. Los otoños son por lo general lluviosos. En sus días se alternan las tardes suaves con temperaturas que rondan los 20-23 ºC al principio del otoño, con días más frescos e incluso fríos a medida que se va acercando el invierno. Los récords de temperaturas son los 40,2 °C, del 19 de julio de 1995, y los –11,5 °C del 14 de febrero de 1983, medidos en el observatorio de la Agencia Estatal de Meteorología (AEMET) situado en el barrio de Parquesol, el más alto de la ciudad.

Aunque este dato sea el oficial, en la ola de frío de enero de 1971, concretamente el 3 de dicho mes, se alcanzaron los –16,4 °C en el aeropuerto de Valladolid, situado a las afueras de la ciudad. Sí es así en el caso de Villanubla, cuya mínima absoluta se produce en esta ola de frío, alcanzándose, el día 3 de enero de 1971, –18,8 °C. En el observatorio de Villanubla las temperaturas son más bajas, debido a que se encuentra a 849 metros de altitud, unos 150 metros más que la ciudad.

Como se indicaba anteriormente, el origen más probable del nombre de la ciudad proviene de la expresión celta "Vallis tolitum" (Valle de Aguas), y es que Valladolid se encuentra enmarcada en la confluencia del río Pisuerga con el río Esgueva. Este último atravesaba la ciudad en dos ramales, hasta que a finales del se llevó a cabo su canalización.

La relación de Valladolid con el río Esgueva era ambivalente. Servía de colector de aguas residuales, por lo que impedía beber sus aguas, la insalubridad máxima y contaba con olores fétidos, pero a la vez se utilizaba para lavar y era fuerza motriz para fábricas y talleres.

A partir de 1840 y hasta 1864 Valladolid experimenta un importante desarrollo económico: se pone en servicio el Canal de Castilla y completa la línea ferroviaria Madrid-Irún, por lo que el equilibrio se rompe. De este modo el Esgueva se decide cubrir en las zonas centrales de Valladolid, y encauzar en las zonas periféricas. Además, también el río Duero atraviesa el municipio por el núcleo de Puente Duero, al sur de Valladolid.

El Pisuerga, principal río de la ciudad, ofrece en la actualidad diversas opciones de ocio y cultura. La embarcación "Leyenda del Pisuerga" permite realizar un viaje por el río, desde la Estación de Embarque, situada a la altura del paseo de las Moreras, río abajo, hasta la vecina localidad de Arroyo de la Encomienda. Se trata de un barco de 25 metros de eslora y 6 de puntal. Durante el trayecto se puede observar de cerca la flora y fauna del Pisuerga. Además, Valladolid dispone de una playa artificial, la playa de las Moreras, que permite a los vallisoletanos tomar el sol en pleno centro e incluso darse un chapuzón en el propio Pisuerga.

Valladolid también cuenta con dos canales artificiales: el Canal de Castilla, realizado entre mediados del y el primer tercio del para facilitar el transporte del trigo de Castilla hacia los puertos del norte; y el Canal del Duero, construido en el para asegurar el abastecimiento de agua a la capital y permitir la creación de superficies de regadío al sur de la ciudad.

Valladolid alberga las sedes de las Cortes de Castilla y León y la Junta de Castilla y León, incluyendo la Presidencia de esta y sus diez consejerías.

La actual sede de las Cortes de Castilla y León fue inaugurada en junio de 2007. Se encuentra en la Avenida de Salamanca, en el barrio residencial Villa de Prado, y es obra del arquitecto granadino Ramón Fernández Alonso. La anterior sede se encontraba de forma provisional en el Castillo de Fuensaldaña, en la localidad vallisoletana de Fuensaldaña.

La ubicación del ejecutivo regional, presidido por Alfonso Fernández Mañueco, se encuentra en el colegio de la Asunción. Dicho edificio está ubicado en la plaza de Castilla y León del Barrio de Covaresa, mientras que las sedes de las diferentes consejerías se encuentran repartidas en diferentes puntos de la ciudad.

La Diputación Provincial de Valladolid también tiene su sede en la ciudad, en concreto, en el Palacio de Pimentel. Tras las elecciones municipales del 2019 está presidida por Conrado Íscar del Partido Popular, sustituyendo en el cargo a Jesús Julio Carnero Garcia, del mismo partido.

Valladolid está gobernada por el alcalde y los concejales, que componen la corporación municipal, que tiene a su cargo el municipio. El Ayuntamiento de Valladolid tiene su sede en la plaza Mayor, en el edificio de la Casa Consistorial. Los concejales son elegidos cada cuatro años, mediante sufragio universal, por los mayores de 18 años. El actual alcalde es Óscar Puente Santiago, del Partido Socialista Obrero Español (PSOE) desde el 13 de junio de 2015, que gobierna en coalición junto con Valladolid Toma la Palabra y Sí se Puede Valladolid, de los cuales sólo el primero asume áreas de gobierno.

Los partidos políticos presentes en el ámbito local son el Partido Socialista Obrero Español, a cuyo frente se encuentra Óscar Puente Santiago, el Partido Popular, Valladolid Toma la Palabra (Izquierda Unida, Equo y personas a título individual), Ciudadanos y Vox. Así, tras las elecciones municipales de 2019 la composición del Ayuntamiento de Valladolid es la siguiente:

El término municipal de Valladolid está compuesto por tres territorios separados entre sí: el principal, donde radica la ciudad de Valladolid, y dos exclaves, conocidos como Navabuena y El Rebollar, al noroeste de aquel. El de Navabuena es el más septentrional y el de mayor extensión de los dos, estando el de El Rebollar deshabitado.

Demográficamente hablando, la población del municipio se reparte en cinco entidades singulares de población, que comprenden a su vez siete núcleos de población. Las entidades y sus poblaciones son, según el nomenclátor de 2012:

Con la renovación del padrón municipal de habitantes que se realizó en el año 1986, se procedió a dividir oficialmente el término municipal en diferentes zonas, pues antes de esta fecha ya existía una división popular, en barrios, que no tenía ninguna función administrativa. Para ejecutar esta división se emplearon diferentes criterios, tales como la continuidad física del territorio, criterios sociológicos y la denominación popular de las mismas.

A partir de ese momento Valladolid se divide en un total de doce distritos, que a su vez se subdividen en cuarenta y siete zonas estadísticas, no necesariamente coincidentes con los barrios tradicionales.

Tras su repoblación, y una vez el valle se vio libre de la ocupación árabe, la ciudad comenzó a expandirse. A finales del comenzaron a aparecer una gran variedad de barrios de carácter gremial, que fueron estableciéndose en distintas zonas, abriéndose calles bajo la influencia directa de la cuestión económica. Por estas fechas, se celebraban en Valladolid ferias de periodicidad anual, a las que habitualmente concurrían hombres de negocio de diversos lugares.

En los inicios del , atraídos fundamentalmente por el bullicio comercial, la actividad agrícola y la atención que en la Villa favorecía la Corte, vinieron a Valladolid gentes, no solo de territorios hispánicos, sino también de otros países, de ascendente cristiano, judío o mudéjar, que compartían el mismo espacio geográfico.
En 1359 la ciudad obtuvo el Privilegio de tener Casa de la Moneda, la cual pervivió hasta el cuando Felipe V concentró la fabricación de este metal.

En el , la ciudad fue la capital del Reino, y en ella se centralizaron los principales órganos político-administrativos. A ello se sumó el hecho de que Felipe II, poco antes de morir, otorgó a Valladolid el título de ciudad, y, aunque mediado el se trasladó la capital a Madrid (hasta 1601), Valladolid siguió conociendo un momento de gran esplendor económico.
A partir de la definitiva marcha de la Corte, en tiempos de Felipe III, la ciudad padeció en los siglos siguientes una etapa de cierta decadencia, apenas mitigada por los efectos de la Ilustración, protagonizada por un fuerte descenso demográfico, y sobre todo una paulatina depresión económica.

Valladolid no experimentaría grandes cambios hasta la segunda mitad del , momento en el que renació con la ayuda de la industria harinera y el desarrollo de las comunicaciones, que favorecieron el transporte de la producción y de las importaciones. El funcionamiento del Canal de Castilla y la aparición de los primeros focos industriales en torno a la dársena, y la posterior llegada del ferrocarril a Valladolid, constituyeron la piedra angular de este despegue urbano. También se desarrolló el sistema financiero; aparecieron las primeras sociedades de crédito, y en 1857 se crea el Banco de Valladolid.

En 1864 se da una grave crisis económica, produciéndose el hundimiento del Banco de Valladolid y la aparición de hambrunas. En el último tercio del , la ciudad, aún marcada por la crisis, avanza muy lentamente. El sector secundario es minoritario, mientras que el terciario se sitúa al frente de los sectores productivos.

Ya en la década de 1950, conoció un potente desarrollo industrial, en torno, fundamentalmente, a la fabricación de automóviles; y también comercial, como consecuencia de lo anterior. En la actualidad, la industria vallisoletana continúa fundamentalmente ligada a la industria del automóvil. En paralelo con esa producción de gran escala, varios polígonos urbanizados albergan a pequeñas y medianas empresas, dedicadas a suministros de todo tipo para el mercado español. El comercio es otra de las grandes fuentes económicas de la ciudad, que debido a esa secular tradición, cuenta, desde 1965, con la Feria Internacional de Muestras para exhibir las constantes innovaciones en el sector.

El principal sector económico de Valladolid es el sector servicios, que da trabajo a 104 168 personas, lo que representa el 72,7 % de los trabajadores vallisoletanos afiliados a la Seguridad Social. Asimismo, el 82,5 % de los centros de trabajo de la ciudad corresponde a empresas del sector terciario. La rama con mayor número de establecimientos es la de comercio al por menor de productos no alimenticios, que representa más del 50 % del total.
A continuación se sitúan el sector de la industria y la construcción: 22 013 personas están empleadas en centros de trabajo industriales y 15 710 encuentran trabajo en el sector de la construcción, lo que representa el 15,4 % y el 11 % del total de trabajadores, respectivamente. Por centros de trabajo, el 6,0 % corresponde a centros industriales y el 10,3 % a empresas de la construcción. La industria predominante de la ciudad corresponde a los sectores derivados de las actividades agrarias, metalúrgica, la industria del automóvil, químicas, de la construcción, artes gráficas, etc. El polígono industrial de San Cristóbal es uno de los dos polígonos industriales de la ciudad de Valladolid. Este polígono acoge a gran cantidad de empresas. Está delimitado por la ronda interior (VA-20), por la ronda exterior (VA-30) y por las carreteras de Soria (A-11) y de Segovia (A-601)

Por último, la actividad agrícola, muy minoritaria, da empleo a 1491 personas, apenas el 1 % del total, con tan solo 153 centros de trabajo (el 1,2 %) dedicados a esta actividad. De esta escasa dedicación agrícola, el tipo de cultivo predominante es de secano, representado en la producción de trigo, cebada y remolacha azucarera, principalmente.

Las principales empresas de la ciudad son: Renault-España, Indal, Michelín, Iveco, Ambuibérica, Aquagest, ACOR, Grupo Norte, Panibérica de Levaduras (Lessafre), Helios, Lingotes Especiales, o Queserías Entrepinares.

La educación en Valladolid depende de la Consejería de Educación de la Junta de Castilla y León, que asume las competencias de educación a nivel regional, tanto en los niveles universitarios como en los no universitarios. Según datos de la propia Consejería, se calcula que en el curso académico 2005-2006 el total de estudiantes no universitarios fue superior a los 52 000, los cuales tienen a su disposición 141 centros de enseñanza, con 2399 aulas y 4487 profesores.

En cuanto a la enseñanza universitaria, Valladolid cuenta con dos universidades y un campus de la Universidad Isabel I (la universidad en línea de Castilla y León):

En la actualidad, la Universidad de Valladolid cuenta con cuatro campus en la ciudad: "Huerta del Rey", "Centro", "Río Esgueva" y "Miguel Delibes". Repartidos en sus 25 facultades y centros asociados, unos 2000 profesores dan clase a más de 15 500 alumnos matriculados en 2018.
Dispone, además de los 25 centros, de una serie de edificios administrativos, como por ejemplo el Palacio de Santa Cruz, donde se encuentra el rectorado, y el Museo de la Universidad de Valladolid (MUVa), la Casa del Estudiante, donde están el resto de los servicios administrativos, o el CTI (Centro de Tecnologías de la Información), que se encuentra en el sótano de la Residencia Universitaria Alfonso VIII, junto a la antigua Facultad de Ciencias.



Valladolid cuenta con 410 equipamientos sanitarios, entre los que se encuentran tanto ambulatorios, como centros de salud u hospitales, de carácter tanto público como privado.

Los dos hospitales públicos de Valladolid, ambos dependientes de SACYL (Sanidad de Castilla y León), son el Hospital Clínico Universitario de Valladolid, heredero del histórico Hospital de la Resurrección, con 777 camas, y el Hospital Universitario Río Hortega, con 589. Se ha construido un tercer hospital en el barrio de Las Delicias, el nuevo Río Hortega, que abrió sus puertas en enero de 2009 y reemplazó al antiguo Río Hortega. Valladolid cuenta con los siguientes centros sanitarios: Barrio España, Canterac, Circunvalación, Delicias I, Delicias II, Magdalena, Pilarica, Plaza Circular, Rondilla I, Rondilla II, San Pablo, Tórtola, Arturo Eyríes, Casa del Barco, Gamazo, Huerta del Rey, La Victoria, Parquesol, Plaza del Ejército, Parque Alameda-Covaresa; de los cuales Rondilla, Delicias y Pilarica disponen servicios de urgencias PAC.

El grupo sanitario Recoletas dispone de dos hospitales en la ciudad, el Hospital Felipe II y el Hospital Campo Grande, siendo este último el más importante de Castilla y León de este grupo privado. Además dispone de un tercer centro, el Centro Paracelso que funciona como centro de atención primaria y con algunas especialidades. 

Además de la cobertura sanitaria, la Universidad de Valladolid cuenta con una Escuela Universitaria de Enfermería y una Facultad de Medicina, en la que se imparten los estudios de Medicina, Logopedia y Nutrición y Dietética. Los estudios de Medicina en Valladolid se remontan al , siendo la primera facultad de Medicina erigida en España, y la ciudad cuenta con la segunda Real Academia de Medicina más antigua de España.

Asociados a la institución universitaria, se encuentran diversos centros de investigación sanitaria: el Instituto de Oftalmobiología Aplicada (IOBA), creado en 1994; el Instituto de Farmacoepidemiología (IFE), dedicado a la investigación sobre la seguridad y los efectos de los medicamentos en la población; el Instituto de Ciencias Médicas (ICIME); el Instituto de Biología y Genética Molecular (IBGM), adscrito al Consejo Superior de Investigaciones Científicas (CSIC) o el Centro Nacional de la Gripe.

Cerca del río Pisuerga, junto con el que por mucho tiempo fue el único camino de entrada a la ciudad, el puente Mayor, atravesando las calles de la antigua judería de la ciudad, se disponen una serie de plazas y calles con abundancia de antiguos templos y edificios nobiliarios civiles. En este entorno se emplazan el palacio de los Condes de Benavente, la iglesia de San Nicolás de Bari o el convento de San Quirce, en la plaza de la Trinidad, la conventual calle de Santo Domingo de Guzmán y la iglesia de San Agustín, reconvertida hoy en archivo municipal.

En la Plaza de San Pablo, núcleo de la vida cortesana en tiempos de Felipe III y que vio nacer a su predecesor Felipe II, se halla la Iglesia de San Pablo, que presenta una fachada de Simón de Colonia, en estilo gótico isabelino, que se asemeja a un retablo en piedra. Corresponde al último periodo del estilo gótico. Fue escenario de numerosas ceremonias reales, primera sepultura del infante Alfonso y Juan II, o lugar de bautismo de Enrique IV, Felipe II, Felipe IV y Ana de Austria. Aquí contrajeron matrimonio Maximiliano II y María de Austria, y tomó el capelo cardenalicio Adriano de Utrecht, que sería con el tiempo el papa Adriano VI. Fue lugar predilecto de numerosos obispos que después desempeñaron su actividad pastoral en el Nuevo Mundo.
En el lateral opuesto de la plaza, el Palacio Real, residencia de los monarcas españoles Carlos I, Felipe II y Felipe III y también de Napoleón Bonaparte durante la guerra de Independencia, ha llegado al presente con numerosas alteraciones estructurales de sus primitivas trazas, concluidas en torno a 1528. Aquí nació en 1605 Felipe IV. Fue construido por Luis de Vega arquitecto de Carlos I y su patio renacentista posee decoración de medallones atribuidas a Esteban Jamete y escudos de los diferentes territorios pertenecientes al Imperio español. En el Ventura Rodríguez construyó la escalera neoclásica.

La esquina con la calle de Las Angustias está ocupada por el palacio de Pimentel, en el que, por no contar entonces la emperatriz Isabel con residencia propia en Valladolid, nació, en 1527, Felipe II. El edificio, construido en ladrillo, tiene dos notables detalles en piedra: la portada con arco carpanel y la esquina con ventana angular plateresca. La calle Cadenas de San Gregorio alberga las cuatro dependencias del Museo Nacional de Escultura: el Colegio de San Gregorio, la Iglesia de San Benito el Viejo, el palacio de Villena y el Palacio del Conde de Gondomar (Casa del Sol).

Junto al Palacio de Villena, en la calle Fray Luis de Granada, se encuentra la casa donde nació y vivió el poeta romántico José Zorrilla, y que acoge la Casa Museo de Zorrilla. En las inmediaciones, la iglesia de San Martín destaca por su esbelta torre, realizada en traza románica a principios del . Por su parte el clasicismo impera en la fachada de la Iglesia Penitencial de Nuestra Señora de las Angustias, erigida a principios del , con escultura monumental de Francisco del Rincón.

Frente a este último templo, inaugurado en 1864 según proyecto de Jerónimo de la Gándara, se encuentra el Teatro Calderón. Su emplazamiento y estructura sigue las corrientes del momento. La fachada se mueve dentro del gusto clasicista y en su interior se encuentra la sala de espectáculos, en forma de herradura, a la italiana. Está decorada con pinturas de Augusto Ferri, escenógrafo de la época. En el escenario existe un sistema de tramoya debida al ingeniero italiano Egidio Piccoli. Detrás del teatro se encuentra el Palacio Arzobispal, que fue propiedad del Juan de Villasante y María de Villarroel, construido a mediados del . En 1857 se convirtió en sede del primer arzobispo vallisoletano, Luis de la Lastra y Cuesta.

Con origen en el "trazo a cordel" de las calles con soportales que sucedieron al incendio de 1561, el llamado núcleo histórico de Valladolid se articula a partir de la Plaza Mayor mediante los siete viales que la atraviesan.

Urbanizada en el , la plaza Mayor de Valladolid es la primera plaza mayor regular de España, y sirvió de modelo, desde el , para otras muchas en España y Sudamérica: en el , la antigua "Plaza del Mercado" se convirtió en centro cultural, político, económico y social de la ciudad.

En 1908 abrió sus puertas la actual Casa Consistorial, un palacio de cuatro torres, planta rectangular y patio interior, de cuyo frontal sobresale una tribuna que soporta el balcón principal. Este edificio es obra de Enrique María Repullés, inspirado en el proyecto de Antonio de Iturralde, pero modificándolo para imitar los modelos de la arquitectura renacentista española.

Frente a la Casa Consistorial, en el lugar que ocupó hasta el el convento de San Francisco, donde falleció Cristóbal Colón, se encuentra el Teatro Zorrilla. El teatro fue inaugurado en octubre de 1884, con la obra "Traidor, inconfeso y mártir", contando con la presencia del propio autor de la obra, José Zorrilla, y del poeta vallisoletano Emilio Ferrari.

En uno de los laterales de la Casa Consistorial, la iglesia de Jesús mantiene una fachada de modelo neorrománico de tipo catalán, en ladrillo prensado.

Atravesando la plaza de la Rinconada, a espaldas del edificio del Ayuntamiento, en la que se levanta el Palacio de Correos y Telégrafos, se accede a la iglesia de San Benito el Real, de la orden benedictina, uno de los templos más antiguos de Valladolid. Fue erigido sobre el antiguo Alcázar Real, y está realizado en estilo gótico, aunque la fachada es posterior: fue diseñada por Rodrigo Gil de Hontañón a mediados del . En el interior destaca la reja del mismo siglo, que abarca las tres naves de la iglesia. Junto a ésta se halla el Mercado del Val, que data del .
Muy cerca, la iglesia de San Miguel y San Julián, sobre el punto topográfico más alto de la ciudad, fue templo de la Compañía de Jesús en Valladolid, como lo atestiguan la fachada y la estructura interior, conformes al modelo romano. En su interior se pueden contemplar obras de Gregorio Fernández y relieves de Adrián Álvarez y Francisco de Rincón.

En la misma calle de San Ignacio se conservan algunos de los muchos palacios edificados en esta zona en tiempos de Felipe II, tales como el Palacio del marqués de Valverde, en cuyo exterior destaca una ventana en ángulo y la decoración de medallones, y el Palacio de Fabio Nelli, obra del clasicismo renacentista de Juan de Lastra y Diego de Praves. Junto a estos palacios, por una pequeña entrada, se accede a la plaza del Viejo Coso, la primitiva plaza de toros de Valladolid.

En la plaza de las Brígidas se encuentra el convento de las Brígidas, antigua casa Palacio del Licenciado Butrón, convertido ahora en el Archivo General de Castilla y León. La iglesia adosada tiene una fachada de ladrillo de uniones a regla.

La iglesia Penitencial de Nuestra Señora de la Vera Cruz, en el extremo de la calle de la Platería, fue diseñada por Diego de Praves en 1596. Alberga esculturas procesionales en madera policromada, pertenecientes a la Cofradía de la Vera-Cruz.

A orillas del hoy desviado ramal meridional del río Esgueva, fue erigida por Pedro Ansúrez la colegiata de Santa María, destinada a ser la cabeza religiosa de su nuevo y próspero feudo. Durante la primera mitad del se celebraron en el templo tres Concilios nacionales, y resultando insuficiente o de poco rango el edificio, se levanta uno nuevo a partir del siguiendo la nueva arquitectura del Císter.
La inacabada catedral de Nuestra Señora de la Asunción fue proyectada por Juan de Herrera con una traza de gran monumentalidad, proporción dupla para dos cuadrados iguales con crucero y torres en cuatro esquinas, pero la escasez de rentas del recién creado obispado vallisoletano, la muerte del arquitecto y de Felipe II, principales promotores de la obra, y la falta de recursos y de interés en su término durante los siglos posteriores, dio lugar a que solamente llegara a construirse casi la mitad de lo ideado por Herrera. Adosados a sus muros, perviven los restos románicos y góticos de la colegiata, del , a la que sustituyó como iglesia mayor de la ciudad. El retablo principal de la catedral es obra de Juan de Juni.
En la plaza de la Universidad se levanta el edificio principal de esta institución. El edificio histórico de la Universidad de Valladolid fue construido en el según la traza de fray Pedro de la Visitación; su decoración escultórica es obra de Antonio Tomé e hijos.

Muy cerca se levanta, desde el , la iglesia de Santa María La Antigua, fundación del conde Pedro Ansúrez, señor de Valladolid, con un esbelto campanario de influencia francesa, conocida como "La reina de las torres románicas de Castilla", rematada con un tejado apiramidado y un claustro, ambos de principios del . El resto de la estructura y el interior son de estilo gótico. A comienzos del las naves góticas del templo amenazaban ruina y en 1917 se procedió a su derribo iniciándose la reconstrucción en estilo neogótico.

A medio camino entre estos lugares y la plaza Mayor, se conserva el Pasaje Gutiérrez, galería comercial construida en 1885 al estilo europeo del momento, siguiendo el diseño de Jerónimo Ortiz de Urbina, y que constituye junto al Pasaje de Lodares de Albacete, los únicos ejemplos de este tipo de construcciones en toda España. Cerca del Pasaje, se encuentra la iglesia del Salvador, donde, según la tradición, fue bautizado el patrón de Valladolid, san Pedro Regalado.

Bordeando el Campo Grande, en la Acera de Recoletos, gran arteria de expansión de la burguesía, se mantienen edificios de finales del y principios del : la Casa Mantilla, de 1891, de estilo ecléctico, con inspiración renacentista, o la modernista Casa del Príncipe, de 1906, obra de Jerónimo Arroyo, arquitecto palentino formado en la escuela de Barcelona.
Tras atravesar la Acera de Recoletos se llega a la Plaza de Colón, donde hasta el se ubicó el Convento de San José. A unos metros de la plaza se encuentra la Estación de Valladolid-Campo Grande, la principal estación de ferrocarril de la ciudad.

Rodeando el Campo Grande, en el Paseo de Filipinos, la Iglesia de San Juan de Letrán destaca por su fachada y sus bóvedas cubiertas con yeserías barrocas, ambas del , obra de Matías Machuca; el cuerpo de la iglesia es de finales del .

Del es también el convento de los Agustinos Filipinos. El edificio, cuya fachada encara con la puerta posterior del paseo del Príncipe del Campo Grande, fue diseñado por Ventura Rodríguez.

Siguiendo el recorrido alrededor del parque, encontramos la Academia de Caballería de Valladolid, que data de 1915 y es un edificio historicista rematado por chapiteles similares a los que caracterizaron a los palacios de los Austrias.

Junto a la Academia de Caballería, la plaza de Zorrilla es un punto clave en el trazado urbano vallisoletano. Presidida por una estatua de Zorrilla, obra de Aurelio Carretero, enlaza las principales calles de Valladolid: la anteriormente mencionada Acera de Recoletos, el paseo de Zorrilla, principal bulevar de la ciudad, la calle Santiago, que desemboca en la plaza Mayor y las calles Miguel Íscar, Duque de la Victoria y la plaza España.

Cristóbal Colón murió en Valladolid, el 20 de mayo de 1506. El Ayuntamiento decidió en 1968 levantar una edificación en estilo gótico-isabelino que reprodujese una casa palaciega propiedad de Diego Colón, hermano menor del Almirante, ubicada en la ciudad de Santo Domingo, en la República Dominicana. Este edificio alberga hoy la Casa Museo de Colón.
En la misma calle, en la acera opuesta, la iglesia de la Magdalena, del , luce en su fachada un gran escudo en piedra, blasón de su patrón, el virrey del Perú y obispo, Pedro de la Gasca. En el interior se pueden contemplar el retablo Mayor, el de Santiago, y el sepulcro en alabastro del mencionado obispo, obra de Esteban Jordán El Palacio de Santa Cruz, primer edificio renacentista de España, erigido a partir de 1486 por el arquitecto Lorenzo Vázquez de Segovia con el patrocinio del cardenal Pedro González de Mendoza, presenta una portada de arco de medio punto y un patio de tres pisos, dos en estilo tardogótico y el tercero de matices barrocos, consecuencia de una reforma del . En su biblioteca, a la que se accede por una puerta plateresca, se conservan en sus estanterías de madera dorada en dos pisos, valiosos documentos, entre ellos el "Beato de Valcabado", del año 970.

En las cercanías, el Monasterio de las Huelgas Reales, de estilo palladiano, conserva un arco mudéjar del que fuera palacio de la reina de Castilla María de Molina. Y en el convento de Santa Clara, del , contrasta la severidad franciscana del exterior con las yeserías barrocas del .

El convento de las Descalzas Reales fue encargado por Felipe III y Margarita de Austria, en el ; tiene un torreón de tres alturas, de tipo palacial, con celosías en los balcones. En el interior es posible apreciar el claustro de estilo toscano, y, en la iglesia, el retablo realizado conjuntamente por Juan de Muniátegui, Gregorio Fernández y Santiago Morán.

El Palacio de los Vivero, construido en el , encabeza un complejo de edificios que fue agrandándose a la medida de las necesidades de la administración de justicia. Los Reyes Católicos contrajeron en él matrimonio (1469), y luego decidieron su destino como Real Audiencia y Chancillería.

Durante el y, fundamentalmente, a lo largo del numerosos monumentos históricos fueron derribados a partir de la ejecución de distintos planes urbanísticos diseñados para intentar asumir el descontrolado éxodo rural y el crecimiento demográfico de la ciudad durante este periodo, a lo que contribuyó el estado de ruina en el que se encontraban muchos de ellos. De esta forma, multitud de edificios antiguos como el Hospital de la Resurrección, donde Miguel de Cervantes situó su novela "El coloquio de los perros", conventos y claustros como el de San Francisco o el de San José, iglesias como la de San Julián y Santa Basilisa o la de San Miguel, incluyendo decenas de palacios medievales y renacentistas como el de la Ribera, el palacio Gardoqui o la casa de las Aldabas fueron demolidos para construir bloques de pisos de gran altura que rompían con la armonía arquitectónica de la ciudad.

En julio de 1978, el Consejo de Ministros declaró conjunto histórico-artístico a la ciudad, pero para muchos estudiosos la declaración llegó demasiado tarde y no tuvo repercusión posterior. El arquitecto Fernando Chueca Goitia llegó a afirmar, que la destrucción del patrimonio histórico-artístico de Valladolid era de nueve sobre diez.

La escultura urbana en Valladolid está protagonizada por obras que representan a ilustres personajes a los que se les ha recordado de esta manera. Así, en el año 1887 se instaló en la Plaza de la Universidad la estatua de Miguel de Cervantes en pie, con traje de época, pluma y libro en ristre, realizada por Nicolás Fernández de la Oliva. El escritor José Zorrilla también posee una escultura en la plaza que lleva su nombre. El fundador de la ciudad, el conde Pedro Ansúrez, cuenta con un monumento en el centro de la Plaza Mayor, realizada en 1903 por Aurelio Carretero. El Monumento a Colón, obra del artista Antonio Susillo, e inaugurado en 1905, recuerda a la figura del descubridor.

También destacan las esculturas que adornan el Campo Grande de Valladolid, y cerca de él, a la entrada de la Academia de Caballería, el monumento a los Cazadores de Alcántara, de 1931, obra de Mariano Benlliure.

Lejos de ahí, preside la plaza de San Pablo una estatua de Felipe II; realizada en 1964 por Federico Coullaut-Valera, es copia de la que está en la plaza de la Armería de Madrid e imita el modelo de Pompeo Leoni. A la entrada de la calle Cadenas de San Gregorio, se instaló, en 1982, la escultura en hierro "Lo profundo es el aire", de Eduardo Chillida, un homenaje al poeta vallisoletano Jorge Guillén.

Entre las esculturas a «escala humana» destacan "El Comediante", en la Plaza de Martí Monsó, obra de Eduardo Cuadrado; la escultura de Rosa Chacel que se encuentra en uno de los bancos de los jardines del Poniente y que fue realizada por Luis Santiago Pardo en 1996. Otros ejemplos son "El Encuentro", obra de Feliciano Álvarez Buenaposada, que se encuentra desde 1997 en la Plaza Madrid; la titulada "Candia", situada en el Parque Ribera de Castilla; el monumento al torero Fernando Domínguez, en la plaza de toros; la escultura "Baile en bronce", homenaje al bailarín Vicente Escudero; o las dedicadas a Einstein y a Pío del Río Hortega, en la plaza del Museo de la Ciencia.

Entre las nuevas construcciones cabe destacar las instaladas en la prolongación del paseo de Zorrilla: "Stage Set for a Film" (Decorado para una Película), de Dennis Oppenheim; las "Puertas de Valladolid", de Cristóbal Gabarrón; y la "Columna forma de sonido", de Lorenzo Frechilla. Otra es el "Monumento al IV Centenario de la ciudad de Valladolid", construida en 1999 por Ángel Mateos Bernal, situada frente al recinto ferial de Castilla y León, en la avenida de Salamanca.

En otras ocasiones las esculturas comparten su protagonismo con el agua, en fuentes como la de "Los Colosos" (Pedro Monje, 1996), en la plaza de la Rinconada; la "Fuente de las Sirenas" (Concha Gay, 1996), en la Plaza de Martí Monsó; la titulada "Jorge Guillén y la infancia" (Luis Santiago Pardo, 1998), situada en la glorieta central de los jardines del Poniente; la "Fuente Dorada" (Fernando González Poncio, 1998), en la plaza del mismo nombre; y otras.

El parque más antiguo y más emblemático de la ciudad es el Campo Grande; se trata de un gran jardín romántico, ubicado en pleno centro de Valladolid, ideado en su actual fisonomía por Miguel Íscar, alcalde de Valladolid entre 1877 y 1880. Acoge una gran variedad de árboles que constituyen un verdadero jardín botánico. Habitan diferentes aves y son famosos los pavos reales y, recientemente, las ardillas.
Sobre el antiguo ramal norte del Esgueva se construyeron, también a finales del , los jardines del Poniente:

Se trata de un sencillo jardín en cuyo centro hay dos pérgolas que albergan una pequeña plaza en la que se encuentra una fuente que recuerda la obra del escritor vallisoletano Jorge Guillén.

A lo largo del curso del Pisuerga también abundan las zonas verdes. Comenzando por el norte, el Parque Ribera de Castilla (inaugurado el 20 de marzo de 1988), con una superficie de 12 hectáreas, está poblado de distintas especies de chopos, álamos o tilos. Siguiendo el discurrir del agua, el parque de las Moreras cuenta con varios paseos, zonas deportivas y una playa fluvial. Junto a él se encuentra la Rosaleda Francisco Sabadell, un pequeño jardín exclusivamente formado por rosas.

Otras zonas verdes son el Pinar de Antequera, principal recurso natural de la capital vallisoletana; el parque Forestal de La Fuente del Sol, histórico espacio verde junto al barrio de La Victoria, el parque de Las Norias de Santa Victoria, que ocupa las antiguas instalaciones de la fábrica azucarera Santa Victoria, el Jardín Botánico de la Victoria, que cuenta con 30 especies distintas de árboles junto con una muestra de especies autóctonas, el parque Fuente de la Salud del Barrio Los Pajarillos, el parque de Canterac y de la Paz en Las Delicias o el parque del Mediodía en Parquesol.

Siguiendo el curso del río Pisuerga, lo atraviesan los siguientes puentes:

Sobre el río Esgueva cruzan un importante número de puentes. Como el río es bastante más reducido y la configuración actual obedece a su canalización por un extremo de la ciudad, suprimiendo su paso por el centro y sus varios ramales, estas estructuras no tienen nombre propio, sino que toman el correspondiente a la calle que atraviesa el río.

En el parque situado detrás de la Facultad de Filosofía y Letras, junto al cauce del río, se ha diseñado un estanque en el que se encuentran los restos de una de las cercas de la ciudad.



Además, por su cercanía a la capital es destacable el Archivo General de Simancas que guarda la documentación de la Monarquía Hispánica desde los Reyes Católicos hasta la instauración del Régimen Liberal. En cuanto a privados, el más importante es el Archivo Diocesano de Valladolid que atesora los fondos generados por la colegiata y luego por la catedral, la documentación de la curia diocesana, el conjunto de archivos parroquiales de la diócesis y un gran archivo musical con más de 6000 partituras.














Se pueden visitar tres casas museo:




La Semana Santa es el evento cultural más importante de la ciudad, debido a sus valiosas tallas policromadas de los siglos y de Juan de Juni, Gregorio Fernández o Francisco del Rincón, muchas de ellas expuestas durante el resto del año en el Museo Nacional de Escultura, atrayendo anualmente a visitantes de toda España y el resto del mundo.
Esta celebración fue declarada de Interés Turístico Internacional en 1980, siendo de esta manera la primera celebración de la Semana Santa en España en ostentar dicha declaración. En 2014 se iniciaron los trámites para conseguir su reconocimiento como patrimonio cultural inmaterial de la Humanidad.

En la Semana de Pasión, y siempre que no haya lluvia, las veinte cofradías vallisoletanas procesionan por el casco histórico de la ciudad. La historia de la Semana Santa en Valladolid se remonta al , si bien anteriormente hubo procesiones en el interior de los conventos, donde nacieron las cofradías más antiguas como Santa Vera Cruz, Angustias, La Piedad, La Pasión y Nuestro Padre Jesús Nazareno.

Durante la Semana Santa vallisoletana se pueden contemplar por las calles una de las principales exposiciones de imaginería religiosa del mundo. Pasos como la "Virgen de las Angustias", una de las principales tallas de Juan de Juni, "La Sagrada Cena", de Juan Guraya, "La Oración del Huerto", de Andrés de Solanes, "El Señor Atado a la Columna" y "El Descendimiento", de Gregorio Fernández, o "Las lágrimas de San Pedro", de Pedro de Ávila, recuerdan al ciudadano el vínculo existente entre la religión y el arte.

La Semana Santa de Valladolid no solo se distingue por la singularidad artística y gran valor de sus pasos sino también por la sobriedad, el silencio y el respeto que reina en cada acto.

Dentro de la Semana de Pasión vallisoletana destacan actos como el Pregón y el Sermón de las Siete Palabras que transforma la plaza Mayor de Valladolid en un escenario que parece remontarse al y la Procesión General de la Sagrada Pasión del Redentor del Viernes Santo que hace un recorrido desde la Última Cena hasta la soledad de la Virgen y en el que se pueden contemplar los 32 conjuntos escultóricos más importantes.

El Corpus Christi es una celebración religiosa católica.

Anualmente se celebra la Semana Internacional de Cine de Valladolid (SEMINCI), a finales de octubre. Creada en 1956 como «Semana de cine religioso de Valladolid», se celebraba en Semana Santa, evolucionando hasta convertirse en uno de los principales festivales de cine de España, y el segundo más antiguo, teniendo como objetivo la difusión y promoción de películas de categoría artística, que contribuyan al conocimiento de la cinematografía mundial.

El festival tiene su sede principal en el Teatro Calderón, donde se celebran la gala de inauguración, la proyección de películas de la "Sección Oficial" y la gala de clausura, en la que se entrega la "Espiga de Oro", principal galardón del festival.

Por la SEMINCI han desfilado personajes del mundo cinematográfico, como Ken Loach, Brad Pitt, Kenneth Branagh, Ang Lee, Sophia Loren, Julie Christie, John Cleese, María de Medeiros, Liv Ullmann, Abbas Kiarostami, Atom Egoyan o Mira Sorvino.

A lo largo del año se celebran numerosos eventos culturales en la ciudad. Cronológicamente, el primer fin de semana después de la festividad de Reyes se celebra la concentración motorista invernal Pingüinos, la más numerosa de Europa, en la que se realizan todo tipo de actividades relacionadas con el mundo de las dos ruedas.

Entre finales de mayo y principios del mes de junio se celebra en la Plaza Mayor la Feria del Libro de Valladolid. En su cuadragésimo sexta edición, la feria congregó a más de 130 autores de todo el mundo. Por ella han pasado Fernando Savater, Juan Manuel de Prada o Antonio Gamoneda entre otros. Entre finales de marzo y principios del mes de abril y en el paseo central del Campo Grande se celebra también la Feria del Libro Antiguo y de Ocasión en la que participan más de 20 librerías de toda España.

Durante el mes de mayo se desarrolla la Semana del Renacimiento, con la celebración de un mercado renacentista, con la recreación de sabores, olores y personajes del Valladolid del . Estos días además se teatraliza por las calles "La Ruta del Hereje", popularizada tras la obra de Miguel Delibes, mientras que los restaurantes ofrecen menús gastronómicos rescatados del y actualizados por restauradores vallisoletanos. También en mayo se celebra el Festival Internacional de Teatro y Artes de Calle de Valladolid (TAC). Los espectáculos son nacionales y extranjeros, concebidos para ser exhibidos en espacios sin butacas.
En primavera tenía también lugar el afamado e internacionalmente consolidado festival de música Valladolid Latino en el que desde 2006 y hasta 2015 han intervenido artistas como Alejandro Sanz, Juanes, Paulina Rubio, Julieta Venegas o Marc Anthony entre otros muchos.

Durante los meses veraniegos se celebran Las Noches de San Benito con conciertos y cine al aire libre.

En 2007 se ha creado un ciclo denominado Música en la Catedral, aprovechando la adquisición de un órgano electrónico Allen en detrimento del antiguo órgano de la catedral construido en dos fases (1904 y 1932) por Aquilino Amezua y Leocadio Galdós y que es un valioso instrumento con tres teclados y pedal y 36 juegos, de estilo romántico-sinfónico.

Se ha celebrado en 2007 la primera edición de la Bienal de Escultura de Valladolid, de escultura contemporánea, que en cierta medida viene a complementar al Museo Nacional de Escultura de la ciudad.

Por último, se celebran las Fiestas Patronales de San Pedro Regalado, en plena primavera —13 de mayo—, con una corta celebración en la que en la que el mercado medieval, la gastronomía y la música son los principales protagonistas y, luego, la Feria y Fiestas de Nuestra Señora de San Lorenzo que se celebran a principios de septiembre cuyo día central es el 8 de septiembre festividad de la patrona. El programa da paso a diversas actuaciones musicales, teatro, ferias gastronómicas, corridas de toros, citas artesanales, fuegos de artificio o exposiciones entre otras actividades. Durante esta semana festiva destaca la Feria de Día, los conciertos de la plaza Mayor, las casetas gastronómicas regionales, la Feria de Muestras o los Fuegos Artificiales.

El Concurso Nacional de Pinchos y Tapas "Ciudad de Valladolid", celebrado desde el año 2005, reúne a los representantes de todas las comunidades autónomas de España en torno a la disciplina más característica de la gastronomía española: la elaboración de tapas y pinchos.
El encuentro tiene lugar en la primera quincena de noviembre. En él participan especialistas de máximo nivel y la propia hostelería de Valladolid que ofrece, en sus establecimientos, las creaciones de los finalistas. Además a principios de junio se celebra el Concurso Provincial de Tapas y Pinchos. Desde el año 2018 se celebra en el mes de junio el Festival de música Conexión Valladolid, en los terrenos de la Antigua Hípica Militar.

La Feria de Valladolid dispone de un parque ferial integrado por cuatro pabellones cubiertos, auditorio, centro de congresos, salas de conferencias y espacios al aire libre, en el que es posible celebrar cualquier tipo de la actividad ferial. Durante el año se suceden diferentes eventos o salones: La Feria Internacional de Muestras celebrada durante el mes de septiembre, INTUR (Feria de Turismo de Interior), Expobioenergía (Feria Tecnológica en Bioenergía), AR&PA (Bienal de Restauración y Gestión del Patrimonio), Alimentaria (Salón de Alimentación) (Bienal) o Agraria (de maquinaria agrícola) entre otros. El Centro de Congresos es otro escenario para el desarrollo de diferentes actividades profesionales. Se trata de un conjunto de salas versátiles con aforos que oscilan entre 60 y 240 plazas, un auditorio con capacidad para 600 personas equipado con la tecnología necesaria para atender las demandas de este tipo de reuniones y pabellones en los que tienen cabida hasta 10 000 personas.
Existen alrededor de una treintena de salas expositivas públicas que, durante todo el año, exhiben en Valladolid las diferentes muestras de creatividad artística tanto de artífices vallisoletanos como de los procedentes de otros puntos, españoles o foráneos, de épocas actuales o anteriores. Destacan la Sede de la Fundación Municipal de Cultura, la Sala Municipal de Las Francesas, sita en la iglesia del antiguo convento de las Francesas la moderna Cúpula del Milenio, la Sala de San Benito dedicada desde 1994 exclusivamente a la fotografía con proyectos internacionales en su gran mayoría, o la Sala de la Pasión, en el acondicionado espacio de la antigua iglesia barroca de la Cofradía de la Pasión, dedicada a la pintura, escultura, dibujo, grabado, video diseño, y otras artes plásticas. También la Sala de exposiciones del Teatro Calderón, dedicada a la presentación de obras de artistas locales a partir de una convocatoria pública anual, además de otras exposiciones en colaboración con instituciones, o los espacios reservados a estas funciones en los diferentes Centros Cívicos de la ciudad.

Valladolid es citada tópicamente como lugar donde se habla "el mejor castellano". Esta tradición parece remontarse al , a partir de la referencia que a Marie-Catherine d'Aulnoy (a propósito de su viaje por España, que quedó reflejado en su obra "Relato del viaje a España") se le hizo sobre la pureza del castellano en la ciudad. Este hecho se está concretando en el fomento de iniciativas para la creación de centros especializados en la enseñanza de la lengua española para extranjeros. Pero a pesar de esta fama, el habla de los vallisoletanos se caracteriza por rasgos diatópicos como el leísmo, el laísmo y otros propios del dialecto castellano septentrional.

Valladolid acogió en el año 2001 el II Congreso Internacional de la Lengua Española que se desarrolló bajo el título «El español en la Sociedad de la Información» entre el 16 y el 19 de octubre de dicho año.

Miguel de Cervantes máximo exponente de la literatura española y universalmente conocido, vivió en Valladolid, durante 2 etapas de su vida, la primera ocasión que Miguel de Cervantes residió en Valladolid contaba solamente 4 años, pasados los años volvió a recalar en Valladolid con la llegada a dicha ciudad de la corte del rey Felipe III en 1601, durante esta última estancia escribió parte de su obra culmen, "Don Quijote de la Mancha" y en esta ciudad se encontraba el autor cuando en 1605 se publica dicha novela.

Hasta cuatro premios Cervantes, la máxima distinción literaria en lengua castellana, están vinculados a Valladolid: Miguel Delibes, Jorge Guillén (ambos naturales de la ciudad), Francisco Umbral y José Jiménez Lozano (residentes durante muchos años). Otros autores destacados nacidos en la ciudad o muy vinculados a ella son Miguel de Cervantes, José Zorrilla, Gaspar Núñez de Arce, Rosa Chacel, Francisco Pino, Blas Pajarero, Gustavo Martín Garzo, José María Luelmo, Fernando de Orbaneja o José Manuel de la Huerga.

El Centro Cultural Miguel Delibes, inaugurado en 2007, es sede de la Orquesta Sinfónica de Castilla y León, del Conservatorio Profesional de Música de la ciudad, de la Escuela Superior de Arte Dramático y de la Escuela Profesional de Danza y Teatro Experimental. Además, está equipado con un auditorio con capacidad para 1700 espectadores, una sala para música de cámara y otra más para teatro experimental; su apertura ha sido clave en la cultura musical.

Los teatros Calderón (remodelado en 1999) y Zorrilla (reconstruido entre 2005 y 2009) ofrecen una programación que abarca la mayoría de las artes escénicas y musicales.

El Teatro Carrión, reabierto en 2013, acoge desde 2014 la sede de la Orquesta Filarmónica de Valladolid, con temporada de ópera, zarzuela y conciertos.

Existen numerosas agrupaciones corales y dos jóvenes orquestas: la Joven Orquesta Sinfónica de Valladolid y la Joven Orquesta de la Universidad de Valladolid. Además, proceden de la ciudad formaciones de música folclórica como Candeal o Tradere, de música infantil como La Carraca, y en el ámbito de la música moderna, destacan los Celtas Cortos, grupo de "rock" celta de los años 90, y otros como Greta y los Garbo, Los Mismos, Triquel o Arizona Baby.

Entre los personajes reconocidos dedicados al mundo de la interpretación destacan grandes actores como Lola Herrera, Concha Velasco, Emilio Gutiérrez Caba, Diego Martín, Roberto Enríquez, Elvira Mínguez, Ágata Lys, la actriz y modelo Inés Sastre, Juanjo Pardo, Emilio Laguna, Julia Torres, Paloma Valdés, Daniel Muriel, Sara Rivero, Nacho López, Fernando Cayo, Ana Otero o las hermanas Loreto y Marta Valverde. En el mundo de la televisión han adquirido gran relevancia Patricia Conde, Deborah Ombres o Manu Carreño. También es vallisoletano de nacimiento el expresidente del Gobierno José Luis Rodríguez Zapatero, así como la ex vicepresidenta del Gobierno Soraya Sáenz de Santamaría, la exministra de Agricultura Isabel García Tejerina o el exministro del Interior Jorge Fernández Díaz.

La gastronomía vallisoletana se inserta en la gastronomía castellana. Ocupa un lugar preferente la carne y los asados; uno de los platos más típicos es el asado de lechazo condimentado con agua y sal y cocinado en horno de leña (asado al estilo castellano). Les siguen, el cochinillo o el cabrito y los alimentos de la caza como perdices, codornices y conejo, se cocinan aquí braseados o escabechados. El queso de la zona se elabora con leche de oveja, lo que significa un fuerte sabor en varios grados de curación. Son quesos de Valladolid marcas como Entrepinares o Flor de Esgueva.

Naturalmente se trata de platos que necesitan para su completo disfrute del pan y del vino, dos elaboraciones que desde hace siglos se elaboran en esta zona. Se pueden degustar decenas de texturas de pan de cereal castellano. De ellos, el más famoso es el pan "lechuguino" pero también destacan el pan de picos o el cuatro canteros. En Valladolid pueden degustarse vinos de gran calidad como son los adscritos a las cinco denominaciones de origen de la provincia vallisoletana: los tintos de la Denominación de Origen Ribera del Duero, los blancos de Rueda o los rosados de la Denominación de Origen Cigales, la de Toro y la DO Tierra de León.

Un buen postre empieza en Valladolid por la repostería salida de las manos artesanas de los conventos y se complementa con el "café de puchero". Se puede acompañar de pastas artesanales, como los mantecados de Portillo (popularmente conocidos como "zapatillas") o con los "bizcochos de Santa Clara", los empiñonados, los buñuelos de crema o las almendras garrapiñadas.


Valladolid es el centro del deporte en Castilla y León, así como un referente deportivo de primera categoría a nivel nacional, al poseer equipos de élite en la mayoría de los deportes más populares, destacando particularmente en la ciudad la práctica del rugby, con dos de los equipos punteros de la División de Honor de rugby, El Salvador y el VRAC, que suman entre los dos, quince Campeonatos Nacionales de Liga, doce Copas del Rey y doce Supercopas de España, habiendo aportado históricamente un importante número de jugadores a la selección española de rugby.

El equipo más representativo de la ciudad es el Real Valladolid, con más de cuarenta temporadas en la primera división del fútbol español, campeón de una Copa de la Liga en 1984 y dos veces subcampeón de la Copa del Rey de Fútbol. El club disputa sus partidos locales en el Nuevo José Zorrilla. Anualmente se celebra tanto el Trofeo Ciudad de Valladolid como el Trofeo Diputación de Valladolid. Desde 2020, posee sección de baloncesto con el Real Valladolid Baloncesto tras su fusión con el CBC Valladolid

También destacan el BM Aula Cultural que milita en la máxima categoría del balonmano femenino español y el BM Atlético Valladolid creado en 2014 y que milita desde la temporada 2016/2017 en la Liga Asobal (sustituyendo al desaparecido Club Balonmano Valladolid, que ganó una Recopa de Europa, una Copa ASOBAL y dos ediciones de la Copa del Rey de Balonmano); el extinto Club Baloncesto Valladolid, uno de los equipos históricos de la liga ACB de baloncesto y los dos equipos de rugby antes mencionados, el VRAC y el Club de Rugby El Salvador, siendo este último el que más triunfos ha brindado a los vallisoletanos.

La oferta deportiva de Valladolid se completa con destacados equipos de tenis de mesa (Collosa Telecyl), baloncesto en silla de ruedas (BSR Valladolid), fútbol sala, hockey en línea (CPLV), varios importantes clubs de piragüismo con base en el Pisuerga y con el(Club Atletismo Valladolid) que actualmente está en División de Honor Femenina de Atletismo y Primera División Masculina de Atletismo siendo uno de los clubes más importantes de Castilla y León. Cuenta la ciudad, además, con cuatro campos de golf, múltiples clubs de fútbol, baloncesto, balonmano, tenis, atletismo, natación, ciclismo, voleibol, artes marciales, deportes autóctonos, caza y pesca, así como clubs e instalaciones deportivas de otras disciplinas. Además es la ciudad de deportistas de alto nivel como Mayte Martínez, Rubén Baraja, Laura López Valle, Isaac Viciosa, Miriam Blasco o Roldán Rodríguez (la mayoría ya retirados de la alta competición) y jóvenes atletas como Álvaro Rodríguez o Mohamed Elbendir y la nadadora paralímpica Amaya Alonso.

Valladolid ha acogido varios eventos deportivos relevantes, habiendo sido sede de la Copa Mundial de Fútbol de 1982, del Campeonato Mundial de Gimnasia Rítmica de 1985, de la final del Campeonato de Europa de Fútbol sub-21 de 1986, de la Fase Final de la Willi Brinkmann Eurocup de baloncesto en silla de ruedas en el 2009, de la Liga Europea de voleibol, así como de importantes pruebas ciclistas (entre ellas múltiples etapas de la Vuelta Ciclista a España), campeonatos de tenis, veladas de boxeo, concursos hípicos, etc. En 2016 y 2017 se celebró la final de la Copa del Rey de Rugby en el Estadio José Zorrilla con más de 26.000 espectadores en las gradas, que lo convirtió en el partido entre dos equipos españoles (El Salvador y VRAC) con más público en las gradas. En las cercanías de Valladolid también se encuentran las sedes de dos importantes concentraciones motociclistas anuales de carácter internacional: "Pingüinos" y "Motauros" (Tordesillas).

Los medallistas olímpicos vallisoletanos han sido: Adolfo Mengotti (Plata en fútbol en París 1924, compitiendo con Suiza), Marcelino Gavilán y Ponce de León (Plata en equitación en Londres 1948), Ángel León Gozalo (Plata en pistola libre 50 m. en Helsinki 1952), José Luis Llorente (Plata en Baloncesto en Los Ángeles 1984), Narciso Suárez Amador (Bronce en Piragüismo en aguas tranquilas en Los Ángeles 1984), Miriam Blasco (Oro en Yudo en Barcelona 1992), Fernando Hernández Casado y Raúl González Gutiérrez (ambos Bronce en Balonmano en Atlanta 1996), Laura López Valle (Plata en Natación sincronizada en Pekín 2008) y Juan Carlos Pastor (Bronce en Balonmano en Pekín 2008, como entrenador).

Las principales vías de acceso a la ciudad son:

El ayuntamiento de Valladolid, tiene desde hace años un Sistema de Préstamo de Bicicletas llamado VallaBici, modernizado en 2013, como medio de transporte público individualizado, cómodo, saludable, ecológico y fácil de usar. Así mismo, el sistema es totalmente electrónico y funciona las 24 horas todos los días del año. Hay repartidos por la ciudad 34 puntos de préstamo y 260 bicicletas para poder elegir en cada momento dónde coger o dejar la bicicleta de una forma rápida y sencilla.

Los servicios de autobús sustituyeron en la década de los años 20 a la red de Tranvía de Valladolid. Tras un largo periodo de gestión mediante concesión privada, desde 1982 el transporte urbano de Valladolid está gestionado por la sociedad municipal Autobuses Urbanos de Valladolid, S. A. (AUVASA), encargada del transporte público dentro del término municipal de Valladolid. Además, por un convenio con el Ayuntamiento de Valladolid varias líneas llegan a las localidades de Simancas y La Cistérniga. Tiene un presupuesto anual de unos 31 millones de euros.

Posee 23 líneas ordinarias (2 de ellas circulares), 9 líneas laborables a polígonos industriales, 2 líneas "lanzadera" al Campus Universitario Miguel Delibes, 7 líneas de servicio especial matinal y 5 de nocturno ("Búho"), 6 líneas "F" que dan servicio al estadio José Zorrilla en los días de partido, y 5 líneas especiales para diferentes ferias u otros eventos culturales al Real de la Feria.

Cuenta con una flota de 150 vehículos con una antigüedad media de 12,83 años. Si se tiene en cuenta únicamente los vehículos de las líneas ordinarias su vida media es de 10,89 años, mientras que la de los autobuses que refuerzan las líneas en horas punta o bien sustituyen a los habituales por averías o cualquier otro motivo se eleva a 19,2 años.

En la actualidad, del total de la flota el 58,7% (88 autobuses) funcionan con GLP, 50 con biodiésel (33,3%), 11 son híbrido-eléctricos recargables (7,3%) y un autobús es híbrido (0,7%). El 22% son autobuses articulados de 18 metros (33 unidades), y el resto rígidos de 12 metros. Toda la flota es de piso bajo y 109 autobuses (72,7%) tienen rampa para minusválidos. Todos los autobuses asignados a líneas ordinarias poseen rampa para minusválidos.

Existen varias líneas de autobuses urbanos que conectan la capital con los municipios de su área metropolitana. Estos buses dan servicio a municipios como Zaratán, Laguna de Duero, Simancas, La Cistérniga, Tudela de Duero o Arroyo de la Encomienda, suelen tener una frecuencia de media hora o menos. Estos buses suelen tener su última parada o su inicio en la estación de autobuses de Valladolid que se encuentra en la calle Puente Colgante, en el centro de la ciudad a escasos metros de la estación de ferrocarril y de la gran arteria vial de la ciudad, el paseo de Zorrilla.

A través de los servicios de las diferentes compañías conecta diariamente con varias localidades de la provincia y de otras provincias de España. También se realizan trayectos internacionales a países de Europa, como Francia, Suiza, Holanda, Bélgica, Gran Bretaña o Alemania.

Valladolid es una de las ciudades pioneras en integración del coche eléctrico en España (junto a Madrid, Barcelona, Sevilla y Palencia), mediante la creación en 2010 de un plan piloto para la instalación de puntos de recarga en la ciudad –similar al proyecto Movele–, pero impulsado desde la Junta.

Valladolid fue la primera ciudad española donde se fabricó en serie un coche eléctrico, el Renault Twizy.

El Aeropuerto de Valladolid-Villanubla (IATA: VLL, ICAO: LEVD) está situado a 10 km de Valladolid, en el término municipal de Villanubla, a 846 metros sobre el nivel del mar; fue inaugurado en 1938. La pista del aeropuerto pertenece a la base aérea militar de Villanubla, situada enfrente de la terminal, al otro lado de la pista, y la administración del aeródromo corre a cargo del Ejército del Aire.

Con un tráfico total de 253 271 pasajeros, 5032 operaciones y 149 687 kilogramos de tráfico de carga en el año 2018 según fuentes oficiales de AENA, es el 31º aeropuerto español por volumen de pasajeros.

Cuenta con seis destinos regulares:

En el municipio de Valladolid también se encuentra el aeródromo privado de Torozos, situado al norte del aeropuerto de Villanubla.

A través de la Estación de Valladolid-Campo Grande de ADIF, anteriormente de RENFE (también conocida como "Estación del Norte"), Valladolid queda conectada con diversas localidades de la provincia y de Castilla y León y también con el resto de España, con trenes regulares a Madrid, Barcelona, Santander y Bilbao entre otros.

La estación se sitúa sobre la línea convencional Madrid-Irún, una de las principales líneas de la red española. Desde 2007 es también final de línea de la LAV Madrid-Valladolid, que en el futuro se prolongará hacia el norte (País Vasco, Asturias, Cantabria...), conformando el denominado Eje Norte-Noroeste de Alta Velocidad. Hasta la puesta en funcionamiento se la LAV Valladolid-León se instaló sobre las vías de la estación un cambiador de ancho dual, que permitía que los trenes de ancho variable aprovechen la LAV Madrid-Valladolid y se dirijan posteriormente a otras ciudades del norte de España (Gijón, Santander, Bilbao, Vitoria e Irún).

En 1985, tras 89 años de funcionamiento, se suprimió al tráfico de viajeros el ferrocarril Valladolid-Ariza. Por dicha línea de circulaban los trenes de Barcelona a Salamanca así como el La Coruña-Barcelona. La línea siguió abierta al tráfico de mercancías hasta 1993. Actualmente solo está en servicio hasta La Carrera, para dar servicio a FASA Renault.

Así mismo, al norte de la ciudad existe un apeadero denominado Valladolid-Universidad que da servicio al Campus Miguel Delibes de la Universidad de Valladolid, y a los barrios de Pilarica y Belén. En este apeadero efectúan parada algunos de los trenes regionales y de Media Distancia que desde Valladolid se dirigen hacia Palencia, Burgos o León.

El sindicato Comisiones Obreras ha propuesto un tren de cercanías entre Palencia, Valladolid y Medina del Campo, dando servicio a esta conurbación urbana. Dicho proyecto ha sido respaldado por los alcaldes de las localidades implicadas.

El 22 de diciembre de 2007 se inauguró la línea de Alta Velocidad que une la estación de Campo Grande con Madrid en cincuenta y seis minutos a velocidades de 300 km/h y con el uso de trenes Talgo de la Serie 102, apodados «pato». Desde el 26 de enero de 2009, hay servicios de trenes Avant, conocidos como «lanzaderas», que unen Valladolid con Segovia y Madrid a precios muy inferiores a los de los primeros, y más aún con el uso de bonos de viaje. La duración en lanzadera del viaje entre Valladolid y Madrid es de aproximadamente una hora.

El 29 de septiembre de 2015 se inauguró la línea de alta velocidad Valladolid-Palencia-León por lo que estas tres ciudades quedaron conectadas por AVE. Los tiempos de viaje desde Valladolid a estas capitales se han reducido notablemente: veintinueve minutos a Palencia y setenta a León. Esta línea es empleada por diversos servicios comerciales desde Madrid: un total de cuarenta y cinco servicios semanales por sentido entre Madrid y León, más los veintiuno entre Madrid y Santander, que circulan por la línea hasta el cambiador de Villamuriel. Hay dos servicios AVE diarios entre Madrid y León (con trenes de la Serie 112), cuatro servicios Alvia hasta Gijón (Serie 130), tres servicios Alvia hasta Santander (Serie 130), un servicio Alvia hasta Ponferrada (Serie 121) y un AV City hasta León (Serie 121).

Se encuentra en avanzado estado de construcción la fracción hasta Burgos de la línea de alta velocidad Venta de Baños-Burgos-Vitoria, que dotará de nuevos servicios a Valladolid cuando sea inaugurada. Las obras de plataforma de todos sus tramos fueron adjudicadas a lo largo del año 2009, el montaje de las vías entre 2014 y 2015.

Dado que el trazado del ferrocarril atraviesa el centro del casco urbano, dividiéndolo en dos partes con una barrera de difícil comunicación, se han planteado desde los años 1980 diversas soluciones al problema, y con mayor intensidad desde que ya fue inminente la llegada de la alta velocidad a la capital. Las opciones barajadas iban desde la mejora de la integración urbana del trazado, manteniéndolo en superficie, al desvío de las líneas por un nuevo trazado externo a la ciudad, pasando por el hundimiento del trazado urbano en trinchera o su soterramiento con tuneladora o falso túnel mediante muros pantalla, desde las afueras hasta la estación ferroviaria.

En 2002 se alcanzó un acuerdo entre el Ayuntamiento de Valladolid, la Junta de Castilla y León y el Ministerio de Fomento para soterrar la totalidad del trazado urbano, entre el puente de Daniel del Olmo y el apeadero de la Universidad; el 6 de noviembre de 2002 se firmó el correspondiente convenio de colaboración entre las administraciones implicadas; y el 10 de enero de 2003 se constituyó una sociedad gestora llamada Valladolid Alta Velocidad 2003, con un 50% de capital de las sociedades del Grupo Fomento y un 25% de cada una de las otras dos administraciones. Como objeto de esta sociedad se definió la promoción de la transformación urbanística derivada de las obras de integración de la red arterial ferroviaria en Valladolid. Para ello, la sociedad cuenta como principal activo con un compromiso de cesión de los terrenos donde aún se asienta el Taller Central de Reparaciones de Renfe y el resto de la superficie que se libere de usos ferroviarios y asociados, para su promoción urbanística y venta. La intención era financiar la totalidad de la operación ferroviaria con los beneficios obtenidos de la urbanística.

El soterramiento del tren en Valladolid supondría una importante modificación de los usos del suelo en toda la franja que actualmente ocupa el tendido férreo. La desaparición del mismo eliminaría la línea divisoria que parte actualmente la ciudad, dejando espacio para nuevos usos públicos y áreas residenciales. Así, no solo despejaría un gran espacio, sino que liberaría también un conjunto de construcciones históricas que constituyen un ejemplo de edificación industrial singular, como el Arco de Ladrillo o el Depósito de Locomotoras. Para realizar las obras tendría que desmontarse el arco necesariamente.

Durante el año 2017 se desechó por completo el plan de soterramiento de las vías por parte de las entidades que forman la Sociedad Valladolid Alta Velocidad y se llegó al acuerdo de realizar un proyecto de integración de las vías con nuevos túneles para vehículos, peatones y bicicletas y reforma de los actuales.

La primera obra de la integración vio la luz en el mes de abril de 2019. La Plaza Rafael Cano, en el barrio de la Pilarica, ahora está deprimida bajo las vías del tren. Durante el año 2019 también se desarrollarán los proyectos del nuevo paso de Panaderos-Labradores-Avenida Segovia (peatonal y vehículos) y los pasos peatonales de Unión-Pelícano, Andalucía-Padre Claret y San Isidro. Los talleres de Renfe se trasladarán al nuevo complejo en San Isidro a partir del 8 de abril de 2019

Valladolid participa activamente en la iniciativa de hermanamiento de ciudades promovida, entre otras instituciones, por la Unión Europea. A partir de esta iniciativa se pretenden establecer lazos con las siguientes ciudades con la celebración de ciclos culturales, intercambios o eventos deportivos:





</doc>
<doc id="2906" url="https://es.wikipedia.org/wiki?curid=2906" title="Verticilo">
Verticilo

Se denomina verticilo (del latín "verticillum", 'tortera') a la disposición de tres o más órganos de una planta, como hojas, flores (pétalos, sépalos, estambres o carpelos), que brotan de un mismo nudo del tallo.
Las plantas con hojas verticiladas mantienen esta disposición durante toda su vida; se dan en especies con entrenudos cortos, como en "Aloysia citriodora", "Nerium oleander", "Brabejum stellatifolium" y el género "Banksia" (ambos de la familia Proteaceae) y miembros de las rubiáceas. 

Los órganos que forman la flor de la mayoría de angiospermas también presentan organización verticilada (también llamada cíclica). Según el número de verticilos las flores pueden ser tetracíclicas (4) o pentacíclicas (5); y dependiendo del número de piezas contenido en cada verticilo se denominan dímeras (2 piezas), trímeras (3 piezas), tetrámeras (4 piezas), pentámeras (5 piezas). Cuando solo hay una pieza dispuesta en cada nudo la flor se llama "espiralada" o "acíclica".


</doc>
<doc id="2907" url="https://es.wikipedia.org/wiki?curid=2907" title="Velocidad de la luz">
Velocidad de la luz

La velocidad de la luz en el vacío es una constante universal con el valor de (186 282,397 mi/s),aunque suele aproximarse a 3·10 m/s. Se simboliza con la letra c, proveniente del latín "celéritās" (en español, celeridad o rapidez).

El valor de la velocidad de la luz en el vacío fue incluido oficialmente en el Sistema Internacional de Unidades como constante el 21 de octubre de 1983, pasando así el metro a ser una unidad derivada de esta constante. También se emplea en la definición del año luz, unidad de longitud equivalente a 9,46·10 m, ya que la velocidad de la luz también se puede expresar como 9,46·10 m/año.

La rapidez a través de un medio que no sea el "vacío" depende de su permitividad eléctrica, de su permeabilidad magnética, y otras características electromagnéticas. En medios materiales, esta velocidad es inferior a c y queda codificada en el índice de refracción. En modificaciones del vacío más sutiles, como espacios curvos, efecto Casimir, poblaciones térmicas o presencia de campos externos, la velocidad de la luz depende de la densidad de energía de ese vacío.

De acuerdo con la física moderna toda radiación electromagnética (incluida la luz visible) se propaga o mueve con una rapidez constante en el vacío, conocida como —aunque impropiamente— como "velocidad de la luz" (magnitud vectorial), en vez de "rapidez de la luz" (magnitud escalar). Esta es una constante física denotada como c. La rapidez c es también la rapidez de la propagación de la gravedad en la teoría general de la relatividad.

Una consecuencia que se obtiene a partir de las leyes del electromagnetismo (tales como las ecuaciones de Maxwell) es que la rapidez c de la radiación electromagnética no depende de la rapidez del objeto que emite tal radiación. Así, por ejemplo, la luz emitida por una fuente de luz que se mueve muy rápidamente, viajaría con la misma rapidez que la luz proveniente de una fuente estacionaria (aunque el color, la frecuencia, la energía y el momentum de la luz cambiarán; fenómeno que se conoce como efecto Doppler).

Si se combina esta observación con el principio de relatividad, se concluye que todos los observadores medirán la rapidez de la luz en el vacío como una misma cantidad, sin importar el marco de referencia del observador o la rapidez del objeto que emite la luz. Debido a esto, se puede ver a c como una constante física fundamental. Este hecho, entonces, puede ser usado como base en la teoría de la relatividad especial. La constante es la rapidez c, en vez de la luz en sí misma, lo cual es fundamental para la relatividad especial. De este modo, si la luz es de alguna manera retardada para viajar a una rapidez menor de c, esto no afectará directamente a la teoría de la relatividad especial.

Observadores que viajan con gran rapidez encontrarán que las distancias y los tiempos se distorsionan de acuerdo con la transformación de Lorentz. Sin embargo, las transformaciones distorsionan tiempos y distancias de manera que la rapidez de la luz permanece constante. Una persona viajando con una rapidez cercana a "c" también encontrará que los colores de la luz al frente se tornan azules y atrás se tornan rojos.

Si la información pudiese viajar más rápido que "c" en un marco de referencia, la causalidad sería violada: en otros marcos de referencia, la información sería recibida antes de ser mandada; así, la causa podría ser observada después del efecto. Debido a la dilatación del tiempo de la relatividad especial, el cociente del tiempo percibido entre un observador externo y el tiempo percibido por un observador que se mueve cada vez más cerca de la rapidez de la luz se aproxima a cero. Si algo pudiera moverse más rápidamente que la luz, este cociente no sería un número real. Tal violación de la causalidad nunca se ha observado.

Un cono de luz define la ubicación que está en contacto causal y aquellas que no lo están. Para exponerlo de otro modo, la información se propaga de y hacia un punto de regiones definidas por un cono de luz. El intervalo AB en el diagrama a la derecha es de "tipo tiempo" (es decir, hay un marco de referencia en el que los acontecimientos A y B ocurren en la misma ubicación en el espacio, separados solamente por su ocurrencia en tiempos diferentes, y si A precede a B en ese marco entonces A precede a B en todos los marcos: no hay marco de referencia en el cual el evento A y el evento B ocurren simultáneamente). De este modo, es hipotéticamente posible para la materia (o la información) viajar de A hacia B, así que puede haber una relación causal (con A la causa y B el efecto).

Por otra parte, el intervalo AC es de "tipo espacio" (es decir, existe un marco de referencia donde el evento A y el evento C ocurren simultáneamente). Sin embargo, también existen marcos en los que A precede a C, o en los que C precede a A. Confinando una manera de viajar más rápido que la luz, no será posible para ninguna materia (o información) viajar de A hacia C o de C hacia A. De este modo no hay conexión causal entre A y C.

En acuerdo con la definición actual, adoptada en 1983, la rapidez de la luz es exactamente 299 792 458 m/s (aproximadamente 3 × 10 metros por segundo, 300 000 km/s o 300 m por millonésima de s).

El valor de c define la permitividad eléctrica del vacío (formula_1) en unidades del SIU como:

La permeabilidad magnética del vacío (formula_3) no es dependiente de c y es definida en unidades del SIU como:

Estas constantes aparecen en las ecuaciones de Maxwell, que describen el electromagnetismo y están relacionadas por:

Las distancias astronómicas son normalmente medidas en años luz (que es la distancia que recorre la luz en un año, aproximadamente 9,46×10 km (9,46 billones de km).

Históricamente, el metro había sido definido como la diezmillonésima parte de la longitud del arco de meridiano terrestre comprendido entre el polo norte y el ecuador a través de París, con referencia a la barra estándar, y con referencia a una longitud de onda de una frecuencia particular de la luz.

En 1967 la XIII Conferencia General de Pesos y Medidas definió el segundo del tiempo atómico como la duración de 9 192 631 770  períodos de radiación correspondiente a la transición entre dos niveles hiperfinos del estado fundamental del átomo cesio-133, que en la actualidad sigue siendo la definición del segundo.

En 1983 la Conferencia General de Pesos y Medidas resolvió modificar la definición del "metro" como unidad de longitud del Sistema Internacional, estableciendo su definición a partir de la velocidad de la luz:

En consecuencia, este reajuste efectuado en la definición del "metro" permite que la velocidad de la luz tenga un valor exacto de 299 792 458 m/s cuando se expresa en metros/segundo. Esta modificación aprovecha de forma práctica una de las bases de la teoría de la relatividad de Einstein, que establece que la magnitud de la velocidad de la luz en el vacío es independiente del sistema de referencia utilizado para medirla.

La motivación en el cambio de la definición del metro, así como todos los cambios en la definición de unidades, fue proveer una definición precisa de la unidad que pudiese ser fácilmente usada para calibrar homogéneamente dispositivos en todo el mundo. La no era práctica en este sentido, ya que no podía ser sacada de su cámara o utilizada por dos científicos al mismo tiempo. También era propensa a cambios significativos en su longitud debido a variaciones de temperatura, desgaste de los extremos, oxidación, etc., incompatible con la exactitud necesaria para establecer una de las unidades básicas del Sistema Internacional de unidades.

La rapidez de la luz es de gran importancia para las telecomunicaciones. Por ejemplo, dado que el perímetro de la Tierra es de 40 075 km (en la línea ecuatorial) y "c" es teóricamente la velocidad más rápida en la que un fragmento de información puede viajar, el período más corto de tiempo para llegar al otro extremo del globo terráqueo sería 0.067 s.

En realidad, el tiempo de viaje es un poco más largo, en parte debido a que la velocidad de la luz es cerca de un 30% menor en una fibra óptica, y raramente existen trayectorias rectas en las comunicaciones globales; además se producen retrasos cuando la señal pasa a través de interruptores eléctricos o generadores de señales. En 2004, el retardo típico de recepción de señales desde Australia o Japón hacia los EE.UU. era de 0.18 s. Adicionalmente, la velocidad de la luz afecta al diseño de las comunicaciones inalámbricas.

La velocidad finita de la luz se hizo aparente a todo el mundo en el control de comunicaciones entre el Control Terrestre de Houston y Neil Armstrong, cuando este se convirtió en el primer hombre que puso un pie sobre la Luna: después de cada pregunta, Houston tenía que esperar cerca de 3 s para el regreso de una respuesta aun cuando los astronautas respondían inmediatamente.

De manera similar, el control remoto instantáneo de una nave interplanetaria es imposible debido a que una nave suficientemente alejada de nuestro planeta podría tardar algunas horas desde que envía información al centro de control terrestre y recibe las instrucciones.

La velocidad de la luz también puede tener influencia en distancias cortas. En los superordenadores la velocidad de la luz impone un límite de rapidez a la que pueden ser enviados los datos entre procesadores. Si un procesador opera a 1 GHz, la señal solo puede viajar a un máximo de 300 mm en un ciclo único. Por lo tanto, los procesadores deben ser colocados cerca uno de otro para minimizar los retrasos de comunicación. Si las frecuencias de un reloj continúan incrementándose, la rapidez de la luz finalmente se convertirá en un factor límite para el diseño interno de chips individuales.

Es importante observar que la velocidad de la luz no es un límite de velocidad en el sentido convencional. Un observador que persigue un rayo de luz lo mediría al moverse paralelamente él mismo viajando a la misma velocidad como si fuese un observador estacionario. Esto se debe a que la velocidad medida por este observador depende no solo de la diferencia de distancias recorridas por él y por el rayo, sino también de su tiempo propio que se ralentiza con la velocidad del observador. La ralentización del tiempo o dilatación temporal para el observador es tal que siempre percibirá a un rayo de luz moviéndose a la misma velocidad.

La mayoría de los individuos están acostumbrados a la regla de la adición de velocidades: si dos coches se acercan desde direcciones opuestas, cada uno viajando a una velocidad de 50 km/h, se esperaría (con un alto grado de precisión) que cada coche percibiría al otro en una velocidad combinada de 50 + 50=100 km/h. Esto sería correcto en todos los casos si pudieramos ignorar que la medida física del tiempo transcurrido es relativa según el estado de movimiento del observador.

Sin embargo, a velocidades cercanas a la de la luz, en resultados experimentales se hace claro que esta regla no se puede aplicar por la dilatación temporal. Dos naves que se aproximen una a otra, cada una viajando al 90% de la velocidad de la luz relativas a un tercer observador entre ellas, no se percibirán mutuamente a un 90% + 90%=180% de la velocidad de la luz. En su lugar, cada una percibirá a la otra aproximándose a menos de un 99.5% de la velocidad de la luz. Este resultado se da por la fórmula de adición de la velocidad de Einstein:

donde "v" y "w" son las velocidades de las naves observadas por un tercer observador, y "u" es la velocidad de cualquiera de las dos naves observada por la otra.

Contrariamente a la intuición natural, sin importar la velocidad a la que un observador se mueva relativamente hacia otro observador, ambos medirán la velocidad de un rayo de luz que se avecina con el mismo valor constante, la velocidad de la luz.

La ecuación anterior fue derivada por Einstein de su teoría de relatividad especial, la cual toma el principio de relatividad como premisa principal. Este principio (originalmente propuesto por Galileo Galilei) requiere que actúen leyes físicas de la misma manera en todos los marcos de referencia.

Ya que las ecuaciones de Maxwell otorgan directamente una velocidad de la luz, debería ser lo mismo para cada observador; una consecuencia que sonaba obviamente equivocada para los físicos del siglo XIX, quienes asumían que la velocidad de la luz dada por la teoría de Maxwell es válida en relación al "éter lumínico".

Pero el experimento de Michelson y Morley, puede que el más famoso y útil experimento en la historia de la física, no pudo encontrar este éter, sugiriendo en su lugar que la velocidad de la luz es una constante en todos los marcos de referencia.

Aunque no se sabe si Einstein conocía los resultados de los experimentos de Michelson y Morley, él dio por hecho que la velocidad de la luz era constante, lo entendió como una reafirmación del principio de relatividad de Galileo, y dedujo las consecuencias, ahora conocidas como la teoría de la relatividad especial, que incluyen la anterior fórmula auto-intuitiva.

Debe tenerse presente, especialmente si se consideran sistemas de referencia no inerciales, que la observación experimental de constancia de la luz se refiere a la velocidad física de la luz. La diferencia entre ambas magnitudes ocasionó ciertos malentendidos a los teóricos de principios de siglo XX. Así Pauli llegó a escribir:

Sin embargo, ese comentario es cierto predicado de la velocidad coordenada de la luz (cuya definición no involucra los coeficientes métricos del tensor métrico), sin embargo, una definición adecuada de velocidad física de la luz involucrando las componentes del tensor métrico de sistemas de referencia no inerciales lleva a que la velocidad física sí sea constante.

El índice de refracción de un material indica cuán lenta es la velocidad de la luz en ese medio comparada con el vacío. La disminución de la velocidad de la luz en los materiales puede causar el fenómeno denominado "refracción", como se puede observar en un prisma atravesado por un rayo de luz blanca formando un espectro de colores y produciendo su dispersión.

Al pasar a través de los materiales, la luz se propaga a una velocidad menor que "c", expresada por el cociente denominado «índice de refracción» del material.

La rapidez de la luz en el aire es solo levemente menor que "c". Medios más densos, como el agua y el vidrio, pueden disminuir más la rapidez de la luz, a fracciones como 3/4 y 2/3 de "c". Esta disminución de velocidad también es responsable de "doblar" la luz (modificando su trayectoria según un quiebro con un ángulo dado) en una interfase entre dos materiales con índices diferentes, un fenómeno conocido como refracción. Esto se debe a que dentro de los medios transparentes, la luz en tanto que onda electromagnética interacciona con la materia, que a su vez produce campos de respuesta, y la luz a través del medio es el resultado de la onda inicial y la respuesta de la materia. Esta onda electromagnética que se propaga en el material tiene una velocidad de propagación menor que la luz en el vacío. El índice de refracción "n" de un medio viene dado por la siguiente expresión, donde "v" es la velocidad de la luz en ese medio (debido a que, como ya se ha señalado, la velocidad de la luz en un medio es menor que la velocidad de la luz en el vacío):

Ya que la velocidad de la luz en los materiales depende del índice de refracción, y el índice de refracción depende de la frecuencia de la luz, la luz a diferentes frecuencias viaja a diferentes velocidades a través del mismo material. Esto puede causar distorsión en ondas electromagnéticas compuestas por múltiples frecuencias; un fenómeno llamado dispersión.

Los ángulos de incidencia (i) y de refracción (r) entre dos medios, y los índices de refracción, están relacionados por la Ley de Snell. Los ángulos se miden con respecto al vector normal a la superficie entre los medios:

A escala microscópica, considerando la radiación electromagnética como una partícula, la refracción es causada por una absorción continua y re-emisión de los fotones que componen la luz a través de los átomos o moléculas por los que está atravesando. En cierto sentido, la luz por sí misma viaja solo a través del vacío existente entre estos átomos, y es obstaculizada por los átomos. Alternativamente, considerando la radiación electromagnética como una onda, las cargas de cada átomo (primariamente electrones) interfieren con los campos eléctricos y electromagnéticos de la radiación, retardando su progreso.

Una evidencia experimental reciente demuestra que es posible para la velocidad de grupo de la luz exceder "c". Un experimento hizo que la velocidad de grupo de rayos láser viajara distancias extremadamente cortas a través de átomos de cesio a 300 veces "c". Sin embargo, no es posible usar esta técnica para transferir información más rápido que "c": la rapidez de la transferencia de información depende de la velocidad frontal (la rapidez en la cual el primer incremento de un pulso sobre cero la mueve adelante) y el producto de la velocidad agrupada y la velocidad frontal es igual al cuadrado de la velocidad normal de la luz en el material.

El exceder la velocidad de grupo de la luz de esta manera, es comparable a exceder la velocidad del sonido emplazando personas en una línea espaciada equidistantemente, y pidiéndoles a todos que griten una palabra uno tras otro con intervalos cortos, cada uno midiendo el tiempo al mirar su propio reloj para que no tengan que esperar a escuchar el grito de la persona previa.

La rapidez de la luz también puede parecer superada en cierto fenómeno que incluye ondas evanescentes, tales como túneles cuánticos. Los experimentos indican que la velocidad de fase de ondas evanescentes pueden exceder a "c"; sin embargo, parecería que ni la velocidad agrupada ni la velocidad frontal exceden "c", así, de nuevo, no es posible que la información sea transmitida más rápido que "c".

En algunas interpretaciones de la mecánica cuántica, los efectos cuánticos pueden ser retransmitidos a velocidades mayores que "c" (de hecho, la acción a distancia se ha percibido largamente como un problema con la mecánica cuántica: ver paradoja EPR). Por ejemplo, los estados cuánticos de dos partículas pueden estar enlazados, de manera que el estado de una partícula condicione el estado de otra partícula (expresándolo de otra manera, uno debe tener un espín de +½ y el otro de -½). Hasta que las partículas son observadas, estas existen en una superposición de dos estados cuánticos (+½, –½) y (–½, +½). Si las partículas son separadas y una de ellas es observada para determinar su estado cuántico, entonces el estado cuántico de la segunda partícula se determina automáticamente. Si, en algunas interpretaciones de mecánica cuántica, se presume que la información acerca del estado cuántico es local para una partícula, entonces se debe concluir que la segunda partícula toma su estado cuántico instantáneamente, tan pronto como la primera observación se lleva a cabo. Sin embargo, es imposible controlar qué estado cuántico tomará la primera partícula cuando sea observada, así que ninguna información puede ser transmitida de esta manera. Las leyes de la Física también parecen prevenir que la información sea transmitida a través de maneras más astutas, y esto ha llevado a la formulación de reglas tales como el teorema de no clonación.

El llamado movimiento superluminar también es visto en ciertos objetos astronómicos, tales como los jet de Galaxia activa, galaxias activas y cuásares. Sin embargo, estos jets no se mueven realmente a velocidades excedentes a la de la luz: el movimiento aparente superluminar es una proyección del efecto causado por objetos moviéndose cerca de la velocidad de la luz en un ángulo pequeño del horizonte de visión.

Aunque puede sonar paradójico, es posible que las ondas expansivas se hayan formado con la radiación electromagnética, ya que una partícula cargada que viaja a través de un medio insolado, interrumpe el campo electromagnético local en el medio. Los electrones en los átomos del medio son desplazados y polarizados por el campo de la partícula cargada, y los fotones que son emitidos como electrones se restauran a sí mismos para mantener el equilibrio después de que la interrupción ha pasado (en un conductor, la interrupción puede ser restaurada sin emitir un fotón).

En circunstancias normales, estos fotones interfieren destructivamente unos con otros y no se detecta radiación. Sin embargo, si la interrupción viaja más rápida que los mismos fotones, los fotones interferirán constructivamente e intensificarán la radiación observada. El resultado (análogo a una explosión sónica) es conocido como radiación Cherenkov.

La habilidad de comunicarse o viajar más rápido que la luz es un tema popular en la ciencia ficción. Se han propuesto partículas que viajan más rápido que la luz, taquiones, doblados por la física de partículas, aunque nunca se han observado.

Algunos físicos (entre ellos João Magueijo y John Moffat) han propuesto que en el pasado la luz viajaba mucho más rápido que a la velocidad actual. Esta teoría se conoce como velocidad de la luz variable, y sus proponentes afirman que este fenómeno tiene la habilidad de explicar mejor muchos enigmas cosmológicos que su teoría rival, el modelo inflacionario del universo. Sin embargo, esta teoría no ha ganado suficiente aceptación.

En septiembre de 2011, en las instalaciones del CERN en Ginebra, del laboratorio subterráneo de Gran Sasso (Italia), se observaron unos neutrinos que aparentemente superaban la velocidad de la luz, llegando (60.7 ± 6.9 (stat.) ± 7.4 (sys.)) nanosegundos antes (que corresponde a unos 18 metros en una distancia total de 732 kilómetros). Desde el primer momento, la comunidad científica se mostró escéptica ante la noticia, ya que varios años antes, el proyecto Milos de la Fermilab de Chicago había obtenido resultados parecidos que fueron descartados porque el margen de error era demasiado alto. Y, efectivamente, en este caso también resultó ser un error de medición. En febrero de 2012, los científicos del CERN anunciaron que las mediciones habían sido erróneas debido a una conexión defectuosa.

Fenómenos refractivos tales como el arco iris tienden a retardar la velocidad de la luz en un medio (como el agua, por ejemplo). En cierto sentido, cualquier luz que viaja a través de un medio diferente del vacío viaja a una velocidad menor que "c" como resultado de la refracción. Sin embargo, ciertos materiales tienen un índice de refracción excepcionalmente alto: en particular, la densidad óptica del condensado de Bose-Einstein puede ser muy alta.

En 1999, un equipo de científicos encabezados por Lene Hau pudo disminuir la velocidad de un rayo de luz a cerca de 17 m/s, y en 2001 pudieron detener momentáneamente un rayo de luz.

En 2003, Mijaíl Lukin, junto con científicos de la Universidad Harvard y el Instituto de Física Lébedev (de Moscú), tuvieron éxito en detener completamente la luz al dirigirla a una masa de gas rubidio caliente, cuyos átomos, en palabras de Lukin, se comportaron como «pequeños espejos» debido a los patrones de interferencia en dos rayos de control.

Hasta tiempos relativamente recientes, la velocidad de la luz fue un tema sujeto a grandes conjeturas. Empédocles creía que la luz era algo en movimiento, y que por lo tanto en su viaje tenía que transcurrir algún tiempo.

Por el contrario, Aristóteles creía que «la luz está sujeta a la presencia de algo, pero no es el movimiento». Además, si la luz tiene una velocidad finita, esta tenía que ser inmensa. Aristóteles afirmó: «La tensión sobre nuestro poder de creencias es demasiado grande para creer esto».

Una de las teorías antiguas de la visión es que la luz es emitida por el ojo, en lugar de ser generada por una fuente y reflejada en el ojo. En esta teoría, Herón de Alejandría adelantó el argumento de que la velocidad de la luz debería ser infinita, ya que cuando uno abre los ojos objetos distantes como las estrellas aparecen inmediatamente.

Los filósofos islámicos Avicena y Alhacén creían que la luz tenía una velocidad finita, aunque en este punto otros filósofos convinieron con Aristóteles.

La escuela Ayran de filosofía en la antigua India también mantuvo que la velocidad de la luz era finita.

Johannes Kepler creía que la velocidad de la luz era finita ya que el espacio vacío no representa un obstáculo para ella. Francis Bacon argumentó que la velocidad de la luz no es necesariamente finita, ya que algo puede viajar tan rápido como para ser percibido.

René Descartes argumentó que si la velocidad de la luz era finita, el Sol, la Tierra y la Luna estarían perceptiblemente fuera de alineación durante un eclipse lunar. Debido a que tal desalineación no se ha observado, Descartes concluyó que la velocidad de la luz es infinita. De hecho, Descartes estaba convencido de que si la velocidad de la luz era finita, todo su sistema de filosofía sería refutado. 

La historia de la medición de la velocidad de la luz comienza en el siglo XVII en los albores de la revolución científica. Un estudio histórico relativo a las mediciones de la velocidad de la luz señala una docena de métodos diferentes para determinar el valor de "c". La mayor parte de los primeros experimentos para intentar medir la velocidad de la luz fracasaron debido a su alto valor, y tan solo se pudieron obtener medidas indirectas a partir de fenómenos astronómicos. En el siglo XIX se pudieron realizar los primeros experimentos directos de medición de la velocidad de la luz confirmando su naturaleza electromagnética y las ecuaciones de Maxwell.

En 1629 Isaac Beeckman, un amigo de René Descartes, propuso un experimento en el que se pudiese observar el fogonazo de un cañón reflejándose en un espejo ubicado a una milla (1.6 km) del primero. En 1638, Galileo propuso un experimento para medir la velocidad de la luz al observar la percepción del retraso entre el lapso de destapar una linterna a lo lejos. René Descartes criticó este experimento como algo superfluo, dado el hecho de que la observación de eclipses, los cuales tenían más poder para detectar una rapidez finita, dio un resultado negativo. En 1667, este experimento se llevó a cabo por la Accademia del Cimento de Florencia, con las linternas separadas una milla entre sí, sin observarse ningún retraso. Robert Hooke explicó los resultados negativos tal como Galileo había hecho: precisando que tales observaciones no establecerían la velocidad infinita de la luz, sino tan solo que dicha velocidad debía ser muy grande.

En 1676 Ole Rømer realizó la primera estimación cuantitativa de la velocidad de la luz estudiando el movimiento del satélite Ío de Júpiter con un telescopio. Es posible medir el tiempo de la revolución de Ío debido a sus movimientos de entrada y salida en la sombra arrojada por Júpiter en intervalos regulares. Rømer observó que Ío gira alrededor de Júpiter cada 42.5 h cuando la Tierra esta más cerca de Júpiter. También observó que, cuando la Tierra y Júpiter se mueven separándose, la salida de Ío fuera de la proyección de la sombra comenzaba progresivamente más tarde de lo predicho. Las observaciones detalladas mostraban que estas señales de salida necesitaban más tiempo en llegar a la Tierra, ya que la Tierra y Júpiter se separaban cada vez más. De este modo el tiempo extra utilizado por la luz para llegar a la Tierra podía utilizarse para deducir la rapidez de esta. Seis meses después, las entradas de Ío en la proyección de la sombra se adelantaban, ya que la Tierra y Júpiter se acercaban uno a otro. Con base a estas observaciones, Rømer estimó que la luz tardaría 22 min en cruzar el diámetro de la órbita de la Tierra (es decir, el doble de la unidad astronómica); las estimaciones modernas se acercan más a la cifra de 16 min y 40 s.

Alrededor de la misma época, la unidad astronómica (radio de la órbita de la Tierra alrededor del Sol) se estimaba en cerca de 140 millones de km. Este dato y la estimación del tiempo de Rømer fueron combinados por Christian Huygens, quien consideró que la velocidad de la luz era cercana a 1000 diámetros de la Tierra por minuto, es decir, unos 220 000 km/s, muy por debajo del valor actualmente aceptado, pero mucho más rápido que cualquier otro fenómeno físico entonces conocido.

Isaac Newton también aceptó el concepto de velocidad finita. En su libro "Opticks" expone el valor más preciso de 16 minutos para que la luz recorra el diámetro de la órbita terrestre, valor que al parecer dedujo por sí mismo (se desconoce si fue a partir de los datos de Rømer o de alguna otra manera).

El mismo efecto fue subsecuentemente observado por Rømer en un punto en rotación con la superficie de Júpiter. Observaciones posteriores también mostraron el mismo efecto con las otras tres lunas Galileanas, en las que era más difícil de observar al estar estos satélites más alejados de Júpiter y proyectar sombras menores sobre el planeta.

Aunque por medio de estas observaciones la velocidad finita de la luz no fue establecida para la satisfacción de todos (notablemente Jean-Dominique Cassini), después de las observaciones de James Bradley (1728), la hipótesis de velocidad infinita se consideró totalmente desacreditada. Bradley dedujo que la luz de las estrellas que llega sobre la Tierra parecería provenir en un ángulo leve, que podría ser calculado al comparar la velocidad de la Tierra en su órbita con la velocidad de la luz. Se observó esta llamada aberración de la luz, estimándose en 1/200 de un grado.

Bradley calculó la velocidad de la luz en alrededor de 298 000 km/s. Esta aproximación es solamente un poco menor que el valor actualmente aceptado. El efecto de aberración fue estudiado extensivamente en los siglos posteriores, notablemente por Friedrich Georg Wilhelm Struve y Magnus Nyren.

La segunda medida acertada de la velocidad de la luz, primera mediante un aparato terrestre, fue realizada por Hippolyte Fizeau en 1849. El experimento de Fizeau era conceptualmente similar a aquellos propuestos por Beeckman y Galileo. Un rayo de luz se dirigía a un espejo a cientos de metros de distancia. En su trayecto desde la fuente hacia el espejo, el rayo pasaba a través de un engranaje rotatorio. A cierto nivel de rotación, el rayo pasaría a través de un orificio en su camino de salida y en otro en su camino de regreso. Pero en niveles ligeramente menores, el rayo se proyectaría en uno de los dientes y no pasaría a través de la rueda. Conociendo la distancia hasta el espejo, el número de dientes del engranaje y el índice de rotación, se podría calcular la velocidad de la luz. Fizeau reportó la velocidad de la luz como 313 000 km/s. El método de Fizeau fue refinado más tarde por Marie Alfred Cornu (1872) y Joseph Perrotin (1900), pero fue el físico francés Léon Foucault quien más profundizó en la mejora del método de Fizeau al reemplazar el engranaje por un espejo rotatorio. El valor estimado por Foucault, publicado en 1862, fue de 298 000 km/s. El método de Foucault también fue usado por Simon Newcomb y Albert Michelson, quien comenzó su larga carrera replicando y mejorando este método.

En 1926, Michelson utilizó espejos rotatorios para medir el tiempo que tardaba la luz en hacer un viaje de ida y vuelta entre la montaña Wilson y la montaña San Antonio en California. De las mediciones cada vez más exactas, resultó una velocidad de 299 796 km/s.

Otra forma de obtener la velocidad de la luz es medir independientemente la frecuencia formula_8 y la longitud de onda formula_9 de una onda electromagnética en el vacío. El valor de c puede entonces ser calculado mediante el uso de la relación formula_10. Una opción es medir la frecuencia de resonancia en una "cavidad de resonancia". Si se conocen con precisión sus dimensiones, estas pueden ser utilizadas para determinar la longitud de onda de un haz de luz. En 1946, Louis Essen y AC Gordon-Smith utilizaron este método (las dimensiones de la cavidad de resonancia se establecieron con una precisión de alrededor de ± 0,8 micras utilizando medidores calibrados por interferometría), obteniendo un resultado de 299 792 ±9 kilómetros/s, sustancialmente más preciso que los valores calculados usando técnicas ópticas. En 1950, las mediciones repetidas establecieron un resultado de 299 792,5 ±3,0 kilómetros/s.
La interferometría es otro método para encontrar la longitud de onda de la radiación electromagnética para determinar la velocidad de la luz. Un haz de luz coherente (por ejemplo, un láser), con una frecuencia conocida formula_8, se divide siguiendo dos recorridos distintos y luego se recombina. Mediante el ajuste de la longitud del camino recorrido mientras se observa el patrón de interferencia, midiendo cuidadosamente el cambio en la longitud de la trayectoria, se puede determinar la longitud de onda de la luz formula_9.

La velocidad de la luz se calcula como en el caso anterior, utilizando la ecuación formula_10.

Antes de la llegada de la tecnología láser, se utilizaron fuentes coherentes de radio para las mediciones de interferometría de la velocidad de la luz. Sin embargo el método interferométrico se vuelve menos preciso con longitudes de onda reducidas, y los experimentos fueron por tanto limitados a la precisión de la longitud de onda larga (~ 0,4 cm ) de las ondas de radio. La precisión puede ser mejorada mediante el uso de luz con una longitud de onda más corta, pero a continuación, se hace difícil medir directamente su frecuencia. Una forma de evitar este problema es comenzar con una señal de baja frecuencia (cuyo valor se puede medir con precisión), y a partir de esta señal sintetizar progresivamente señales de frecuencias superiores, cuya frecuencia puede entonces relacionarse con la señal original. La frecuencia de un láser se puede fijar con notable precisión, y su longitud de onda se puede determinar entonces utilizando interferometría. Esta técnica la desarrolló un grupo del National Bureau of Standards (NBS) (que más tarde se convirtió en el NIST). Se utilizó en 1972 para medir la velocidad de la luz en el vacío con una incertidumbre fraccionaria de 3,5 × 10.

Con base en el trabajo de James Clerk Maxwell, se sabe que la velocidad de la radiación electromagnética es una constante definida por las propiedades electromagnéticas del vacío (constante dieléctrica y permeabilidad).

En 1887, los físicos Albert Michelson y Edward Morley realizaron el influyente experimento Michelson-Morley para medir la velocidad de la luz relativa al movimiento de la Tierra. La meta era medir la velocidad de la Tierra a través del éter, el medio que se pensaba en ese entonces necesario para la transmisión de la luz. Tal como se muestra en el diagrama del interferómetro de Michelson, se utilizó un espejo con media cara plateada para dividir un rayo de luz monocromática en dos rayos que viajaban en ángulos rectos uno respecto del otro. Después de abandonar la división, cada rayo era reflejado de ida y vuelta entre los espejos en varias ocasiones (el mismo número para cada rayo para dar una longitud de trayectoria larga pero igual; el experimento Michelson-Morley actual usa más espejos) entonces una vez recombinados producen un patrón de interferencia constructiva y destructiva.

Cualquier cambio menor en la velocidad de la luz en cada brazo del interferómetro cambiaría la cantidad de tiempo utilizada en su tránsito, que sería observado como un cambio en el patrón de interferencia. Durante los ensayos realizados, el experimento dio un resultado nulo.

Ernst Mach estuvo entre los primeros físicos que sugirieron que el resultado del experimento era una refutación a la teoría del éter. El desarrollo en física teórica había comenzado a proveer una teoría alternativa, la contracción de Lorentz, que explicaba el resultado nulo del experimento.

Es incierto si Einstein conocía los resultados de los experimentos de Michelson y Morley, pero su resultado nulo contribuyó en gran medida a la aceptación de su teoría de relatividad. La teoría de Einstein no requirió un elemento etérico sino que era completamente consistente con el resultado nulo del experimento: el éter no existe y la velocidad de la luz es la misma en cada dirección. La velocidad constante de la luz es uno de los postulados fundamentales (junto con el principio de causalidad y la equivalencia de los marcos de inercia) de la relatividad especial.






</doc>
<doc id="2908" url="https://es.wikipedia.org/wiki?curid=2908" title="Vector (desambiguación)">
Vector (desambiguación)

El término vector puede aludir, en esta enciclopedia, a alguno de los siguientes artículos:






</doc>
<doc id="2909" url="https://es.wikipedia.org/wiki?curid=2909" title="Venus (planeta)">
Venus (planeta)

Venus es el segundo planeta del sistema solar en orden de distancia desde el Sol, el sexto en cuanto a tamaño, ordenados de mayor a menor. Al igual que Mercurio, carece de satélites naturales. Recibe su nombre en honor a Venus, la diosa romana del amor (gr. "Afrodita"). Se trata de un planeta de tipo rocoso y terrestre, llamado con frecuencia el planeta hermano de la Tierra, ya que ambos son similares en cuanto a tamaño, masa y composición, aunque totalmente diferentes en cuestiones térmicas y atmosféricas (la temperatura media de Venus es de 463,85ºC). Su órbita es una elipse con una excentricidad de menos del 1%, formando la órbita más circular de todos los planetas; apenas supera la de Neptuno. Su presión atmosférica es 90 veces superior a la terrestre; es, por tanto, la mayor presión atmosférica de las de todos los planetas rocosos del sistema solar.

Pese a situarse más lejos del Sol que Mercurio, Venus posee la atmósfera más caliente del sistema solar; esto se debe a que está principalmente compuesta por gases de efecto invernadero, como el dióxido de carbono, atrapando mucho más calor del Sol. Actualmente carece de agua líquida y sus condiciones en superficie se consideran incompatibles con la vida conocida, aunque en descubrimientos recientes se ha encontrado fosfina en su superficie nebular, una molécula que en la Tierra es generada por microbios, lo que da indicios de una posible existencia de vida. No obstante, el Instituto Goddard de Estudios Espaciales de la NASA y otros han postulado que en el pasado Venus pudo tener océanos con tanta agua como el terrestre y reunir condiciones de habitabilidad planetaria.

Este planeta además posee el día más largo del sistema solar —243 días terrestres—, su movimiento es dextrógiro, es decir, gira en el sentido de las manecillas del reloj, contrario al movimiento de los otros planetas. Por ello, en un "día" venusiano el Sol "sale" por el oeste y "se pone" por el este. Sus nubes, sin embargo, pueden dar la vuelta al planeta en cuatro días terrestres. De hecho, previamente a estudiarlo con naves no tripuladas en su superficie o con radares, se pensaba que el período de rotación de Venus era de unos cuatro días terrestres.

Al encontrarse Venus más cercano al Sol que la Tierra (es un planeta interior), siempre se puede encontrar en las inmediaciones del Sol (su mayor elongación es de 47,8°), por lo que desde la Tierra se puede ver sólo durante unas pocas horas antes del orto (salida del Sol) en unos determinados meses del año; también durante unas pocas horas después del ocaso (puesta del Sol) en el resto del año. A pesar de ello, cuando Venus es más brillante puede ser visto durante el día, siendo uno de los tres únicos cuerpos celestes que pueden ser vistos de día a simple vista además de la Luna y el Sol. Conocido como la estrella de la mañana ("Lucero del alba") o de la tarde ("Lucero vespertino"), cuando es visible en el cielo nocturno es el segundo objeto más brillante del firmamento tras la Luna, por lo que Venus debió ser ya conocido desde los tiempos prehistóricos.

La mayoría de las antiguas civilizaciones conocían los movimientos en el cielo de Venus, por lo que adquirió importancia en casi todas las interpretaciones astrológicas del movimiento planetario. En particular, la civilización maya elaboró un calendario religioso basado en los ciclos astronómicos, incluidos los ciclos de Venus. El símbolo del planeta Venus es una representación estilizada del espejo de la diosa Venus: un círculo con una pequeña cruz debajo, utilizado también hoy para denotar el sexo femenino.

Los adjetivos "venusiano/a", "venusino/a" y "venéreo/a" (poéticamente) son usados para denotar las características habitualmente atribuidas a Venus-Afrodita. El adjetivo "venéreo" suele asociarse a las enfermedades de transmisión sexual. Venus y la Tierra (diosa griega Gea) son los únicos planetas del sistema solar con nombre femenino.

Aunque todas las órbitas planetarias son elípticas, la órbita de Venus es la más parecida a una circunferencia, con una excentricidad inferior a un 1,2%.

El ciclo entre dos elongaciones máximas (período orbital sinódico) dura 584 días. Después de esos 584 días Venus aparece en una posición a 72° de la elongación anterior. Dado que hay cinco períodos de 72° en una circunferencia, Venus regresa al mismo punto del cielo cada ocho años (menos dos días correspondientes a los años bisiestos). Este periodo era conocido como el ciclo Sothis en el Antiguo Egipto.

En la conjunción inferior, Venus puede aproximarse a la Tierra más que ningún otro planeta. El 16 de diciembre de 1850 alcanzó la distancia más cercana a la Tierra desde el año 1800, con un valor de kilómetros (0,26413854UA). Desde entonces nunca ha habido una aproximación tan cercana. Una aproximación casi tan cercana será en el año 2101, cuando Venus alcanzará una distancia de kilómetros (0,26431736UA).

Venus gira sobre sí mismo muy lentamente en un movimiento retrógrado, en el mismo sentido de las manecillas del reloj si se toma como referencia el polo norte, de este a oeste en lugar de oeste a este como el resto de los planetas (excepto Urano, que está muy inclinado), tardando en hacer un giro completo sobre sí mismo 243,187 días terrestres. No se sabe el porqué de la peculiar rotación de Venus. Si el Sol pudiese verse desde la superficie de Venus aparecería subiendo desde el oeste y posándose por el este, con un ciclo día-noche de 116,75 días terrestres y un año venusiano de menos de un día (0,92 días venusianos).

Además de la rotación retrógrada, los periodos orbital y de rotación de Venus están sincronizados de manera que siempre presenta la misma cara del planeta a la Tierra cuando ambos cuerpos están a menor distancia. Esto podría ser una simple coincidencia pero existen especulaciones sobre un posible origen de esta sincronización como resultado de efectos de marea afectando a la rotación de Venus cuando ambos cuerpos están lo suficientemente cerca.

Es el periodo que transcurre entre dos conjunciones inferiores con la Tierra y dura 583,92 días ó 584 días. Es el ciclo sinódico o año aparente. Tales días son terrestres y suman 1 año y 219 días, es decir que durante el periodo orbital sinódico de Venus la Tierra da 1 órbita y el 60% de otra y Venus realiza 2 órbitas y el 60% de otra y a la vez da 2,4 rotaciones. 584 días son estructurables en 8 periodos de 73. A su vez, el ciclo sinódico es la base del siguiente ciclo formado por 5 ciclos sinódicos entre 5 conjunciones inferiores durante las que la Tierra da 8 órbitas (menos 2 días) y Venus 13 (con 12 rotaciones), lo que significa que cada vez que vemos a Venus esté en el mismo punto de la Eclíptica en el que estuvo hace 8 años y estará dentro de 8 años, o que 8 años terrestres equivalen a 13 venusinos.

Es en dirección Este, dura 6 semanas y su momento intermedio es la conjunción inferior. Por tanto se inicia 3 semanas antes de la conjunción inferior abarcando las 3 últimas semanas de un ciclo sinódico y concluye 3 semanas después de la conjunción abarcando las 3 primeras del siguiente. En el resto del ciclo sinódico Venus parece ir en línea recta hacia el Este durante 77 semanas y el momento intermedio es la conjunción superior. La suma de las 6 y las 77 semanas completan las 83 del ciclo sinódico. Este patrón se repite cada 83 semanas (584 días) hasta 5 veces en 8 años (acorde a las 5 conjunciones inferiores en 8 años). Es el efecto de que Venus -que se traslada más rápido que la Tierra- va en la misma dirección que la Tierra. La forma del trazo retrógrado es resultado de la suma del movimiento propio de Venus y del de traslación de la Tierra, y cada uno de los cinco es diferente porque depende del tramo de órbita por el que circule Venus ya que su órbita está inclinada y tiene dos tramos superior e inferior y cada uno un tramo ascendente y descendente. Así, aunque nos parecen movimientos desordenados son el efecto de que observamos de un punto de vista subjetivo y móvil (la Tierra) en una región espacial en la que ambos planetas siguen movimientos circulares a sus velocidades constantes marcando patrones regulares en el tiempo.

Venus tiene una densa atmósfera, compuesta en su mayor parte por dióxido de carbono y una pequeña cantidad de nitrógeno. La presión a nivel de la superficie es noventa veces superior a la presión atmosférica en la superficie terrestre (una presión equivalente en la Tierra a la presión que hay sumergido en el agua a una profundidad de un kilómetro). La enorme cantidad de dióxido de carbono de la atmósfera provoca un fuerte efecto invernadero que eleva la temperatura de la superficie del planeta hasta cerca de 464 °C en las regiones menos elevadas cerca del ecuador. Esto hace que Venus sea más caliente que Mercurio, a pesar de hallarse a más del doble de la distancia del Sol que este y de recibir solo el 25% de su radiación solar ( en la atmósfera superior y en la superficie). Debido a la inercia térmica de su masiva atmósfera y al transporte de calor por los fuertes vientos de su atmósfera, la temperatura no varía de forma significativa entre el día y la noche. A pesar de la lenta rotación de Venus (menos de dos rotaciones por año venusiano, equivalente a una velocidad de rotación en el Ecuador de solo 6,5 km/h), los vientos de la atmósfera superior circunvalan el planeta en un intervalo de solo 4 días, distribuyendo eficazmente el calor. Además del movimiento zonal de la atmósfera de oeste a este, hay un movimiento vertical en forma de célula de Hadley que transporta el calor del ecuador hasta las zonas polares e incluso a latitudes medias del lado no iluminado del planeta.

La radiación solar casi no alcanza la superficie del planeta. La densa capa de nubes refleja al espacio la mayoría de la luz del Sol y la mayor parte de la luz que atraviesa las nubes es absorbida por la atmósfera. Esto impide a la mayor parte de la luz del Sol que caliente la superficie. El albedo bolométrico de Venus es de aproximadamente el 60%, y su albedo visual es aún mayor, lo cual concluye que, a pesar de encontrarse más cercano al Sol que la Tierra, la superficie de Venus no se calienta ni se ilumina como era de esperar por la radiación solar que recibe. En ausencia del efecto invernadero, la temperatura en la superficie de Venus podría ser similar a la de la Tierra. El enorme efecto invernadero asociado a la inmensa cantidad de dióxido de carbono en la atmósfera atrapa el calor provocando las elevadas temperaturas de este planeta.

Los fuertes vientos en la parte superior de las nubes pueden alcanzar los 350 km/h, aunque a nivel del suelo los vientos son mucho más lentos. A pesar de ello, y debido a la altísima densidad de la atmósfera en la superficie de Venus, incluso estos flojos vientos ejercen una fuerza considerable contra los obstáculos. Las nubes están compuestas principalmente por gotas de dióxido de azufre y ácido sulfúrico, y cubren el planeta por completo, ocultando la mayor parte de los detalles de la superficie a la observación externa. La temperatura en la parte superior de las nubes (a 70 km sobre la superficie) es de –45 °C. La medida promedio de temperatura en la superficie de Venus es de 464 °C. La temperatura de la superficie nunca baja de los 400 °C, lo que lo hace el planeta más caliente del sistema solar.

Venus tiene una lenta rotación retrógrada, lo que significa que gira de este a oeste, en lugar de hacerlo de oeste a este como lo hacen la mayoría de los demás planetas mayores (Urano también tiene una rotación retrógrada, aunque el eje de rotación de Urano, inclinado 97.86°, prácticamente descansa sobre el plano orbital). Se desconoce por qué Venus es diferente en este aspecto, aunque podría ser el resultado de una colisión con un asteroide en algún momento del pasado remoto. Además de esta inusual rotación retrógrada, el período de rotación de Venus y su órbita están casi sincronizados, de manera que siempre presenta la misma cara a la Tierra cuando los dos planetas se encuentran en su máxima aproximación (5001 días venusianos entre cada conjunción inferior). Esto podría ser el resultado de las fuerzas de marea que afectan a la rotación de Venus cada vez que los planetas se encuentran lo suficientemente cercanos, aunque no se conoce con claridad el mecanismo.

Venus tiene dos mesetas principales a modo de continentes, elevándose sobre una vasta llanura. La meseta norte se llama Ishtar Terra y contiene la mayor montaña de Venus (aproximadamente dos kilómetros más alta que el monte Everest), llamada Maxwell Montes en honor de James Clerk Maxwell. Ishtar Terra tiene el tamaño aproximado de Australia. En el hemisferio sur se encuentra Aphrodite Terra, mayor que la anterior y con un tamaño equivalente al de Sudamérica. Entre estas mesetas existen algunas depresiones del terreno, que incluyen Atalanta Planitia, Guinevere Planitia y Lavinia Planitia. Con la única excepción del monte Maxwell, todas las características distinguibles del terreno adoptan nombres de mujeres mitológicas.

La densa atmósfera de Venus provoca que los meteoritos se desintegren bruscamente en su descenso, aunque los más grandes pueden llegar a la superficie, originando un cráter si tienen suficiente energía cinética. A causa de esto, no pueden formarse cráteres de impacto más pequeños de 3,2 kilómetros de diámetro.

Aproximadamente el 90% de la superficie de Venus parece consistir en un basalto recientemente solidificado (en términos geológicos) con muy pocos cráteres de meteoritos. Las formaciones más antiguas presentes en Venus no parecen tener más de 800 millones de años, siendo la mayor parte del suelo considerablemente más joven (no más de algunos cientos de millones de años en su mayor parte), lo cual sugiere que Venus sufrió un cataclismo que afectó su superficie no hace mucho tiempo en el pasado geológico.

El interior de Venus es probablemente similar al de la Tierra: un núcleo de hierro de unos 3000 km de radio, con un manto rocoso que forma la mayor parte del planeta. Según datos de los medidores gravitatorios de la sonda Magallanes, la corteza de Venus podría ser más dura y gruesa de lo que se había pensado. Se piensa que Venus no tiene placas tectónicas móviles como la Tierra, pero en su lugar se producen masivas erupciones volcánicas que inundan su superficie con lava «fresca». Otros descubrimientos recientes sugieren que Venus todavía está volcánicamente activo.

El campo magnético de Venus es muy débil comparado con el de otros planetas del sistema solar. Esto se puede deber a su lenta rotación, insuficiente para formar el sistema de «dinamo interno» de hierro líquido. Como resultado de esto, el viento solar golpea la atmósfera de Venus sin ser filtrado. Se supone que Venus tuvo originalmente tanta agua como la Tierra pero que, al estar sometida a la acción del Sol sin ningún filtro protector, el vapor de agua en la alta atmósfera se disocia en hidrógeno y oxígeno, escapando el hidrógeno al espacio por su baja masa molecular. El porcentaje de deuterio (un isótopo pesado del hidrógeno que no escapa tan fácilmente) en la atmósfera de Venus parece apoyar esta teoría. Se supone que el oxígeno molecular se combinó con los átomos de la corteza (aunque grandes cantidades de oxígeno permanecen en la atmósfera en forma de dióxido de carbono). A causa de esta sequedad, las rocas de Venus son mucho más pesadas que las de la Tierra, lo cual favorece la formación de montañas mayores, profundos acantilados y otras formaciones.

Durante algún tiempo se creyó que Venus poseía un satélite natural llamado Neith, llamado así por la diosa Sais del Antiguo Egipto, cuyo velo ningún mortal podía levantar. Fue aparentemente observado por primera vez por Giovanni Cassini en 1672. Otras observaciones esporádicas continuaron hasta 1892, pero estos avistamientos fueron desacreditados (eran en su mayor parte estrellas tenues que parecían estar en el lugar correcto en el momento correcto), y hoy se sabe que Venus no tiene ningún satélite, si bien el asteroide 2002 VE casi lo es.

El Instituto Goddard de Estudios Espaciales de la NASA y otros han postulado que Venus pudo tener un océano poco profundo, con tanta agua como el terrestre, que contribuyera a mantener condiciones de habitabilidad durante un máximo de 2 000 millones de años. Dependiendo de los parámetros suministrados a sus modelos teóricos, el agua líquida venusiana pudo terminar de evaporarse hace 715 millones de años. Hoy en día, toda el agua conocida en Venus está en forma de una pequeña cantidad de vapor atmosférico (20ppm.) No obstante, la sonda Venus Express de la Agencia Espacial Europea detectó en 2008 que Venus todavía está perdiendo cantidades mensurables de hidrógeno, uno de los dos elementos constituyentes del agua.

Sin información sísmica o detalles, momento de inercia, existen pocos datos directos sobre la geoquímica y la estructura interna de Venus. Sin embargo, la similitud en tamaño y densidad entre Venus y la Tierra sugiere que ambos comparten una estructura interna afín: un núcleo, un manto, y una corteza planetaria. Al igual que la Tierra, se especula que el núcleo de Venus es al menos parcialmente líquido. El menor tamaño y densidad de Venus indica que las presiones en su interior son considerablemente menores que en la Tierra. La diferencia principal entre los dos planetas es la carencia de placas tectónicas en Venus, probablemente debido a la sequedad del manto y la superficie. Como consecuencia, la pérdida de calor en el planeta es escasa, evitando su enfriamiento y proporcionando una explicación viable sobre la carencia de un campo magnético interno.

Venus es el astro más característico en los cielos de la mañana y de la tarde de la Tierra (después del Sol y la Luna), y es conocido por el hombre desde la prehistoria. Uno de los documentos más antiguos que sobreviven de la biblioteca babilónica de Asurbanipal, datado sobre el 1600 a. C., es un registro de 21 años del aspecto de Venus (que los primeros babilonios llamaron "Nindaranna"). Los antiguos sumerios y babilonios llamaron a Venus "«Dil-bat»" o "«Dil-i-pat»"; en la ciudad mesopotámica de Akkad era la estrella de la madre-diosa Ishtar, y en chino su nombre es "«Jīn-xīng»" (金星), el planeta del elemento metal. Venus se consideró como el más importante de los cuerpos celestes observados por los mayas, que lo llamaron «Chak ek» (la gran estrella). Los antiguos griegos pensaban que las apariciones matutinas y vespertinas de Venus eran de dos cuerpos diferentes, y les llamaron "Hesperus" cuando aparecía en el cielo del Oeste al atardecer, y "Phosphorus" cuando aparecía en el cielo del Este al amanecer.

Al encontrarse la órbita de Venus entre la Tierra y el Sol, desde la Tierra se pueden distinguir sus diferentes fases de una forma parecida a las de la Luna. Galileo Galilei fue la primera persona en observar las fases de Venus en diciembre de 1610, una observación que sostenía la entonces discutida teoría heliocéntrica de Copérnico. También anotó los cambios en el tamaño del diámetro visible de Venus en sus diferentes fases, sugiriendo que este se encontraba más lejos de la Tierra cuando estaba lleno y más cercano cuando se encontraba en fase creciente. Estas observaciones proporcionaron una sólida base al modelo heliocéntrico.
Venus es más brillante cuando el 25% de su disco (aproximadamente) se encuentra iluminado, lo que ocurre 37 días antes de la conjunción inferior (en el cielo vespertino) y 37 días después de dicha conjunción (en el cielo matutino). Su mayor elongación y altura sobre el horizonte se produce aproximadamente 70 días antes y después de la conjunción inferior, momento en el que muestra justo media fase; entre estos intervalos, Venus es visible durante las primeras o últimas horas del día si el observador sabe dónde buscarlo. El período de movimiento retrógrado es de veinte días en cada lado de la conjunción inferior.

En raras ocasiones, Venus puede verse en el cielo de la mañana y de la tarde el mismo día. Esto sucede cuando se encuentra en su máxima separación respecto a la eclíptica y al mismo tiempo se encuentra en la conjunción inferior; entonces desde uno de los hemisferios terrestres se puede ver en los dos momentos. Esta oportunidad se presentó para los observadores del hemisferio norte durante unos días sobre el 29 de marzo de 2001, y lo mismo sucedió en el hemisferio sur el 19 de agosto de 1999. Estos eventos se repiten cada ocho años conforme al ciclo sinódico del planeta.
En el siglo XIX, muchos observadores atribuyeron a Venus un período de rotación aproximado de 24 horas. El astrónomo italiano Giovanni Schiaparelli fue el primero en predecir un período de rotación significativamente menor, proponiendo que la rotación de Venus estaba bloqueada por el Sol (lo mismo que propuso para Mercurio). Aunque realmente no es verdad para ninguno de los dos cuerpos, era una estimación bastante aproximada. El período de rotación de Venus fue observado por primera vez durante la conjunción de 1961 con radar desde una antena de 26 metros en Goldstone, California, desde el observatorio de radioastronomía Jodrell Bank en el Reino Unido y en las instalaciones de espacio profundo de la Unión Soviética de Yevpatoria. La precisión fue refinada en las siguientes conjunciones, principalmente desde Goldstone y Yevpatoria. El hecho de que la rotación era retrógrada no fue confirmado sino hasta 1964.

Antes de las observaciones de radio de los años sesenta, muchos creían que Venus contenía un entorno como el de la Tierra. Esto era debido al tamaño del planeta y su radio orbital, que sugerían claramente una situación parecida a la de la Tierra, así como por la gruesa capa de nubes que impedían ver la superficie. Entre las especulaciones sobre Venus estaban las de que este tenía un entorno selvático o que poseía océanos de petróleo o de agua carbonatada. Sin embargo, las observaciones mediante microondas en 1956 por C. Mayer "et al.", indicaban una alta temperatura de la superficie (600K). Extrañamente, las observaciones hechas por A. D. Kuzmin en la banda milimétrica indicaban temperaturas mucho más bajas. Dos teorías en competición explicaban el inusual espectro de radio: una de ellas sugería que las altas temperaturas se originaban en la ionosfera y la otra sugería una superficie caliente.

Uno de los fenómenos de la atmósfera de Venus observado por astrónomos desde la Tierra y aún no explicado es el de las llamadas luces Ashen.

Los tránsitos de Venus acontecen cuando el planeta cruza directamente entre la Tierra y el Sol y son eventos astronómicos relativamente raros. La primera vez que se observó este tránsito astronómico fue en 1639 por Jeremiah Horrocks y William Crabtree. El tránsito de 1761, observado por Mijaíl Lomonosov, proporcionó la primera evidencia de que Venus tenía una atmósfera, y las observaciones de paralaje del siglo XIX durante sus tránsitos permitieron obtener por primera vez un cálculo preciso de la distancia entre la Tierra y el Sol. Los tránsitos solo pueden ocurrir en junio o diciembre, siendo estos los momentos en los que Venus cruza la eclíptica (al plano en el que la Tierra orbita alrededor del Sol), y suceden en pares a intervalos de ocho años, separados dichos pares de tránsitos por más de un siglo. El anterior par de tránsitos sucedió en 1874 y 1882, y el presente par de tránsitos son los de 2004 y 2012.

El tránsito de Venus ocurre porque la órbita de Venus está inclinada 3.5 grados respecto a la de la Tierra de modo que el plano de la órbita de Venus se interseca con el de la Tierra en dos puntos que son opuestos, a modo de los puntos equinocciales de la órbita de la Tierra en relación con su propio plano ecuatorial. Venus pasa con frecuencia regular cada 584 días entre la Tierra y el Sol, pero el tránsito ocurre cuando Venus y la Tierra coinciden en alinearse en algo de esos dos puntos de intersección y pueden hacerlo dos veces seguidas en 8 años, como el caso de los tránsitos de 2004 y 2012. Dado que los encuentros de Venus y Tierra al mismo lado del Sol acusan una precesión de unos 2 días cada 8 años, la coincidencia de ambos en el punto de intersección ocurre cada un poco más de un centenar de años.

La órbita de Venus es un 28% más cercana al Sol que la de la Tierra. Por este motivo, las naves que viajan hacia Venus deben recorrer más de 41 millones de kilómetros adentrándose en el pozo gravitatorio del Sol, perdiendo en el proceso parte de su energía potencial. La energía potencial se transforma entonces en energía cinética, lo que se traduce en un aumento de la velocidad de la nave. Por otro lado, la atmósfera de Venus no invita a las maniobras de frenado atmosférico del mismo tipo que otras naves han efectuado sobre Marte, ya que para ello es necesario contar con una información extremadamente precisa de la densidad atmosférica en las capas superiores y, siendo Venus un planeta de atmósfera masiva, sus capas exteriores son mucho más variables y complicadas que en el caso de Marte.

La primera sonda en visitar Venus fue la sonda espacial soviética Venera 1 el 12 de febrero de 1961, siendo la primera sonda lanzada a otro planeta. La nave resultó averiada en su trayecto y la primera sonda exitosa en llegar a Venus fue la americana Mariner 2, en 1962. El 1 de marzo de 1966, la sonda soviética Venera 3 se estrelló sobre Venus, convirtiéndose en la primera nave espacial en alcanzar la superficie del planeta. A continuación diferentes sondas soviéticas fueron acercándose cada vez más en el objetivo de posarse sobre la superficie venusiana. La Venera 4 entró en la atmósfera de Venus el 18 de octubre de 1967 y fue la primera sonda en transmitir datos medidos directamente en otro planeta. La cápsula midió temperaturas, presiones y densidades, y realizó once experimentos químicos para analizar la atmósfera. Sus datos mostraban un 95% de dióxido de carbono, y en combinación con los datos de ocultación de la sonda Mariner 5, mostró que la presión en la superficie era mucho mayor de lo previsto (entre 75 y 100 atmósferas). El primer aterrizaje con éxito en Venus lo realizó la sonda Venera 7 el 15 de diciembre de 1970. Esta sonda reveló unas temperaturas en la superficie de entre 457 y 474 grados Celsius. La Venera 8 aterrizó el 22 de julio de 1972. Además de dar datos sobre presión y temperaturas, su fotómetro mostró que las nubes de Venus formaban una capa compacta que terminaba a 35 kilómetros sobre la superficie.

La sonda soviética Venera 9 entró en la órbita de Venus el 22 de octubre de 1975, convirtiéndose en el primer satélite artificial de Venus. Una batería de cámaras y espectrómetros devolvieron información sobre la capa de nubes, la ionosfera y la magnetosfera, así como mediciones de la superficie realizadas por radar. El vehículo de descenso de 660 kilogramos de la Venera 9 se separó de la nave principal y aterrizó, obteniendo las primeras imágenes de la superficie y analizando la corteza con un espectrómetro de rayos gamma y un densímetro. Durante el descenso realizó mediciones de presión, temperatura y fotométricas, así como de la densidad de las nubes. Se descubrió que las nubes de Venus formaban tres capas distintas. El 25 de octubre, la Venera 10 realizó una serie similar de experimentos.

En 1978, la NASA envió la sonda espacial Pioneer Venus. La misión Pioneer Venus consistía en dos componentes lanzados por separado: un orbitador y una multisonda. La multisonda consistía en una sonda atmosférica mayor y otras tres más pequeñas. La sonda mayor fue desplegada el 16 de noviembre de 1978, y las tres pequeñas lo fueron el 20 de noviembre. Las cuatro sondas entraron en la atmósfera de Venus el 9 de diciembre, seguidas por el vehículo que las portaba. Aunque no se esperaba que ninguna sobreviviera al descenso, una de las sondas continuó operando hasta 45 minutos después de alcanzar la superficie. El vehículo orbitador de la Pioneer Venus fue insertado en una órbita elíptica alrededor de Venus el 4 de diciembre de 1978. Transportaba 17 experimentos y funcionó hasta agotar su combustible de maniobra, momento en el que perdió su orientación. En agosto de 1992 entró en la atmósfera de Venus y fue destruida. Los estudios que se llevaron a cabo con el Pioneer Venus fueron principalmente sobre la Interacción de la Ionosfera de Venus con el Viento Solar.

La exploración espacial de Venus permaneció muy activa durante finales de los 70 y los primeros años de la década de los 80. Se comenzó a conocer en detalle la geología de la superficie de Venus, y se descubrieron volcanes ocultos inusualmente masivos denominados como "coronae" y "arachnoids". Venus no presenta evidencias de placas tectónicas, a menos que todo el tercio norte del planeta forme parte de una sola placa. Las dos capas superiores de nubes resultaron estar compuestas de gotas de ácido sulfúrico, aunque la capa inferior está compuesta probablemente por una solución de ácido fosfórico. Las misiones Vega desplegaron globos aerostáticos que flotaron a unos 53 kilómetros de altitud durante 46 y 60 horas respectivamente, viajando alrededor de un tercio del perímetro del planeta. Estos globos midieron velocidades del viento, temperaturas, presiones y densidad de las nubes. Se descubrió un mayor nivel de turbulencias y convección de lo esperado, incluyendo ocasionales baches con caídas de uno a tres kilómetros de las sondas.

El 10 de agosto de 1990, la sonda estadounidense Magallanes llegó a Venus, realizando medidas por radar de la superficie del planeta y obteniendo mapas de una resolución de 100m en el 98% del planeta. Después de una misión de cuatro años, la sonda Magallanes, tal como estaba planeado, se sumergió en la atmósfera de Venus el 11 de octubre de 1994 y se vaporizó en parte, aunque se supone que algunas partes de la misma alcanzaron la superficie del planeta. Desde entonces, varias sondas espaciales en ruta hacia otros destinos han usado el método de sobrevuelo de Venus para incrementar su velocidad mediante el impulso gravitacional. Esto incluye a las misiones Galileo a Júpiter, la Cassini-Huygens a Saturno (con dos sobrevuelos) y la Messenger a Mercurio (dos sobrevuelos).

La Agencia Espacial Europea maneja una misión llamada Venus Express, que estudia la atmósfera y las características de la superficie desde la órbita. La Venus Express fue lanzada desde el Cosmódromo de Baikonur (Kazajistán) el 9 de noviembre de 2005, y pese a que se esperaba que permaneciese operativa hasta diciembre de 2009, la ESA decidió prolongar oficialmente la misión hasta 2015. La Agencia Japonesa de Exploración Espacial (JAXA) lanzó la misión PLANET-C el 20 de mayo de 2010, pero debido a que la sonda no desaceleró lo suficiente para entrar en la órbita del planeta Venus, pasó de largo y entró en órbita solar. Después de realizar la última serie de maniobras en agosto de 2015, se programó el encuentro de la sonda con Venus para el 7 de diciembre de 2015.​ El segundo intento resultó exitoso, situándose la sonda en órbita de Venus.

La Terraformación de Venus es el proceso teórico por el cual modificar su ambiente para hacerlo habitable por el ser humano.

El Astrónomo Carl Sagan fue quien propuso por primera vez desde un punto de vista científico la terraformación de Venus, en un artículo llamado The Planet Venus, en 1961.

Para terraformar Venus se necesitaría:


Venus es de hecho el segundo blanco para la terraformación y expansión al espacio.

El planeta Venus ha inspirado numerosas referencias religiosas y astrológicas en las civilizaciones antiguas.

La inspiración mitológica de Venus se extiende también a obras de ficción como:


Algunas obras más recientes que tratan de manera más realista el planeta son:







</doc>
<doc id="2911" url="https://es.wikipedia.org/wiki?curid=2911" title="Viella (Siero)">
Viella (Siero)

Viella es una parroquia del concejo de Siero, en el Principado de Asturias (España); y un lugar de dicha parroquia. Alberga una población de 5416 habitantes (INE 2011) en 1548 viviendas. Ocupa una extensión de 2,74 km². 

Está situada en la zona occidental del concejo y limita al norte con la parroquia de Pruvia, en el concejo de Llanera; al oeste, con la de Bobes; al sur, con la de Granda; al oeste, con la de Lugones; y al noreste con la de Lugo, de nuevo en el concejo de Llanera.

El lugar de Viella tiene una población de 676 habitantes (INE 2011), está situado a una altura de 210 msnm y dista 11,6 km de Pola de Siero, capital del concejo.

Según el nomenclátor de 2011 la parroquia está formada por las poblaciones de:


</doc>
<doc id="2912" url="https://es.wikipedia.org/wiki?curid=2912" title="Violaceae">
Violaceae

Violaceae es una familia del orden Malpighiales. Consta de plantas herbáceas (en la península ibérica); en los trópicos hay especies arbustivas, arbóreas y sobre todo lianas. Se reconocen 806 especies en 25 géneros.

Hojas simples, alternas o en roseta (con escapo floral acaule), de ordinario estipuladas y entonces estas llamativas y con valor sistemático. Flores hermafroditas, generalmente zigomorfas, pentámeras; cáliz con 5 sépalos, entre cuyas piezas, es frecuente la presencia de apéndices verdes y membranosos, generalmente reflejos; corola con 5 pétalos, 2 dorsales e iguales entre sí, 2 laterales, iguales entre sí, o no, erguidos o no, y un pétalo ventral, o prolongado en un espolón ("Viola") o en un saco obtuso (Hipantus) de ovario súpero; androceo con 5 o 3 + 2 estambres, con las anteras aplicadas sobre el estilo, en ocasiones 2 ventrales prolongadas en apéndices estaminales que penetran en el espolón, a veces filamentos dilatados y aplicados; gineceo súpero, tricarpelar, de carpelos abiertos, con placentación parietal; el estilo es variable y con carácter taxonómico, globoso, capitado, o/y acodado; flores a veces cleistógamas (entonces, sin néctar), en general solitarias. <br>Fruto en cápsula de dehiscencia valvar (a veces cápsulas elásticas, autocoria) o raramente en baya. Semillas con carúncula (mirmecocoria). Unas 800 especies, por casi todo el mundo.

Se reconocen tres subfamilias: Violoideae, Leonioideae y Fusispermoideae.








Alsodeiaceae , Leoniaceae y Retrosepalaceae 



</doc>
<doc id="2913" url="https://es.wikipedia.org/wiki?curid=2913" title="Violales">
Violales

Violales es un orden de plantas pertenecientes a la clase de las magnoliópsidas, disgregado según la última revisión realizada por el grupo para la Filogenia de las Angiospermas.

Cuentan generalmente con flores dialipétalas. Gineceo en general súpero y sincárpico de carpelos abiertos; placentación parietal (en cistáceas con tabicación interna tabiques placentarios); algunas familias primitivas: Fluconitiaceas con carpelos cerrados y placentación axial (semejante a Teales). Origen desde Teales. Óvulos crasinucelados, endosperma muy rico en aceites y pobre en féculas. Diferencia con Caparales (crucíferas): 2 en Caparales, 3 en Violales, estas nunca tienen células con mirosina. Muy amplio.

En el sistema APG II de 2003, el orden "Violales" no es reconocido y las familias que lo integraban han sido trasladas a otros órdenes según se lista a continuación:


</doc>
<doc id="2914" url="https://es.wikipedia.org/wiki?curid=2914" title="Virus informático">
Virus informático

Un virus o virus informático es un "software" que tiene por objetivo alterar el funcionamiento normal de cualquier tipo de dispositivo informático, sin el permiso o el conocimiento del usuario principalmente para lograr fines maliciosos sobre el dispositivo. Los virus, habitualmente, reemplazan archivos ejecutables por otros infectados con el código de este. Los virus pueden destruir, de manera intencionada, los datos almacenados en una computadora, aunque también existen otros más inofensivos, que solo producen molestias o imprevistos.

Los virus informáticos tienen básicamente la función de propagarse a través de un "software", son muy nocivos y algunos contienen además una carga dañina ("payload") con distintos objetivos, desde una simple broma hasta realizar daños importantes en los sistemas, o bloquear las redes informáticas generando tráfico inútil. El funcionamiento de un virus informático es conceptualmente simple. Se ejecuta un programa que está infectado, en la mayoría de las ocasiones, por desconocimiento del usuario. El código del virus queda residente (alojado) en la memoria RAM de la computadora, incluso cuando el programa que lo contenía haya terminado de ejecutar. El virus toma entonces el control de los servicios básicos del sistema operativo, infectando, de manera posterior, archivos ejecutables que sean llamados para su ejecución. Finalmente se añade el código del virus al programa infectado y se graba en el disco, con lo cual el proceso de replicado se completa.

El primer virus atacó a una máquina IBM Serie 360 (y reconocido como tal). Fue llamado Creeper, (ENMS) creado en 1972. Este programa emitía periódicamente en la pantalla el mensaje: «I'm the creeper... catch me if you can!» («¡Soy una enredadera... Atrápame si puedes!»). Para eliminar este problema se creó el primer programa antivirus denominado "Reaper" (segador).

Sin embargo, el término virus no se adoptaría hasta 1984, pero estos ya existían desde antes. Victor Vyssotsky, Robert Morris Sr. y Doug McIlroy, investigadores de Bell Labs (se cita erróneamente a Dennis Ritchie o Ken Thompson como cuarto coautor) desarrollaron un juego de ordenador llamado "Darwin" (del que derivará "Core Wars") que consiste en eliminar al programa adversario ocupando toda la RAM.

Después de 1984, los virus han tenido una gran expansión, desde los que atacan los sectores de arranque de disquetes hasta los que se adjuntan en un correo electrónico.

Los virus informáticos afectan en mayor o menor medida a casi todos los sistemas más conocidos y usados en la actualidad. Windows, MacOS, Linux...

Cabe aclarar que un virus informático mayoritariamente atacará solo el sistema operativo para el que fue desarrollado, aunque ha habido algunos casos de virus multiplataforma.

Las mayores virus de incidencias se dan en el sistema operativo Windows y Android por estas causas:

En otros sistemas operativos como las distribuciones GNU/Linux, BSD, Solaris, MacOS iOS y otros basados en Unix las incidencias y ataques son raros. Esto se debe principalmente a:

La mayoría de equipos contienen un sistema operativo de disco de la década de 1990 (equipos de 8, 16 y 32 bits) y han sufrido de las diferentes variantes de virus, principalmente de sector de arranque y de ficheros infectados. La única excepción parecen haber sido las versiones de CP/M, CP/M-86 y DOS Plus, pero no así su descendiente DR-DOS. En los directorios de BBS y la incipiente Internet, siempre está presente un apartado de antivirus. Sin embargo las versiones más actualizadas de estos sistemas operativos solo lo contemplan como algo histórico, al no haber desarrollos específicos para el OS (lo que no elimina, por ej., los ataques a través de navegado web). Esta pujanza se basa sobre todo en videojuegos que necesitan tener el disquete desprotegido de escritura para almacenar puntuaciones o estados del juego, o en determinadas protecciones. Varios están situados en ROM, por lo que no es posible infectar al sistema en sí, pero al necesitar cargar parte desde el disquete, no se realiza comprobación.

Dado que una característica de los virus es el consumo de recursos, los virus ocasionan problemas tales como: pérdida de productividad, cortes en los sistemas de información o daños a nivel de datos.

Una de las características es la posibilidad que tienen de diseminarse por medio de réplicas y copias. Las redes en la actualidad ayudan a dicha propagación cuando estas no tienen la seguridad adecuada.

Otros daños que los virus producen a los sistemas informáticos son la pérdida de información, horas de parada productiva, tiempo de reinstalación, etc.

Hay que tener en cuenta que cada virus plantea una situación diferente.

Existen dos grandes clases de contagio. En la primera, el usuario, en un momento dado, ejecuta o acepta de forma inadvertida la instalación del virus. En la segunda, el programa malicioso actúa replicándose a través de las redes. En este caso se habla de gusanos.

En cualquiera de los dos casos, el sistema operativo infectado comienza a sufrir una serie de comportamientos anómalos o imprevistos. Dichos comportamientos pueden dar una pista del problema y permitir la recuperación del mismo.

Dentro de las contaminaciones más frecuentes por interacción del usuario están las siguientes:

Mensajes que ejecutan automáticamente programas (como el programa de correo que abre directamente un archivo adjunto).
Ingeniería social, mensajes como ejecute este programa y gane un premio, o, más comúnmente: Haz 2 clics y gana 2 tonos para móvil gratis..
Entrada de información en discos de otros usuarios infectados.
Instalación de software modificado o de dudosa procedencia.
En el sistema Windows puede darse el caso de que la computadora pueda infectarse sin ningún tipo de intervención del usuario (versiones Windows 2000, XP y Server 2003) por virus como Blaster, Sasser y sus variantes por el simple hecho de estar la máquina conectada a una red o a Internet. Este tipo de virus aprovechan una vulnerabilidad de desbordamiento de buffer y puertos de red para infiltrarse y contagiar el equipo, causar inestabilidad en el sistema, mostrar mensajes de error, reenviarse a otras máquinas mediante la red local o Internet y hasta reiniciar el sistema, entre otros daños. En las últimas versiones de Windows 2000, XP y Server 2003 se ha corregido este problema en su mayoría.

Los métodos para disminuir o reducir los riesgos asociados a los virus pueden ser los denominados activos o pasivos. 




Para no infectar un dispositivo, hay que:

Existen diversos tipos de virus, varían según su función o la manera en que este se ejecuta en nuestra computadora alterando la actividad de la misma, entre los más comunes están:
Otros tipos por distintas características son los que se relacionan a continuación:

La característica principal de estos virus es que se ocultan en la memoria RAM de forma permanente o residente. De este modo, pueden controlar e interceptar todas las operaciones llevadas a cabo por el sistema operativo, infectando todos aquellos ficheros y/o programas que sean ejecutados, abiertos, cerrados, renombrados, copiados.
Algunos ejemplos de este tipo de virus son: Randex, CMJ, Meve, MrKlunky.

Al contrario que los residentes, estos virus no permanecen en memoria. Por tanto, su objetivo prioritario es reproducirse y actuar en el mismo momento de ser ejecutados. Al cumplirse una determinada condición, se activan y buscan los ficheros ubicados dentro de su mismo directorio para contagiarlos.

Estos virus se caracterizan por destruir la información contenida en los ficheros que infectan. Cuando infectan un fichero, escriben dentro de su contenido, haciendo que queden total o parcialmente inservibles.

Los virus de batch lo que hacen generar ficheros batch de lenguajes de script a partir de opciones de configuración propuestas por el programa. Crear este tipo de programa es muy simple por lo que hay multitud de generadores de virus de este tipo.< 

Los términos boot o sector de arranque hacen referencia a una sección muy importante de un disco o unidad de almacenamiento CD, DVD, memorias USB, etc. En ella se guarda la información esencial sobre las características del disco y se encuentra un programa que permite arrancar el ordenador. Este tipo de virus no infecta ficheros, sino los discos que los contienen. Actúan infectando en primer lugar el sector de arranque de los dispositivos de almacenamiento. Cuando un ordenador se pone en marcha con un dispositivo de almacenamiento, el virus de boot infectará a su vez el disco duro. 

Los virus de boot no pueden afectar al ordenador mientras no se intente poner en marcha a este último con un disco infectado. Por tanto, el mejor modo de defenderse contra ellos es proteger los dispositivos de almacenamiento contra escritura y no arrancar nunca el ordenador con uno de estos dispositivos desconocido en el ordenador. 

Algunos ejemplos de este tipo de virus son: Polyboot.B, AntiEXE.

Los ficheros se ubican en determinadas direcciones (compuestas básicamente por unidad de disco y directorio), que el sistema operativo conoce para poder localizarlos y trabajar con ellos.

Los virus de enlace o directorio alteran las direcciones que indican donde se almacenan los ficheros. De este modo, al intentar ejecutar un programa (fichero con extensión EXE o COM) infectado por un virus de enlace, lo que se hace en realidad es ejecutar el virus, ya que este habrá modificado la dirección donde se encontraba originalmente el programa, colocándose en su lugar.

Una vez producida la infección, resulta imposible localizar y trabajar con los ficheros originales.

Más que un tipo de virus, se trata de una técnica utilizada por algunos de ellos, que a su vez pueden pertenecer a otras clasificaciones.
Estos virus se cifran a sí mismos para no ser detectados por los programas antivirus. Para realizar sus actividades, el virus se descifra a sí mismo y, cuando ha finalizado, se vuelve a cifrar.

Son virus que en cada infección que realizan se cifran de una forma distinta (utilizando diferentes algoritmos y claves de cifrado).
De esta forma, generan una elevada cantidad de copias de sí mismos e impiden que los antivirus los localicen a través de la búsqueda de cadenas o firmas, por lo que suelen ser los virus más costosos de detectar.

Virus muy avanzados, que pueden realizar múltiples infecciones, combinando diferentes técnicas para ello. Su objetivo es cualquier elemento que pueda ser infectado: archivos, programas, macros, discos, etc.

Infectan programas o ficheros ejecutables (ficheros con extensiones EXE y COM). Al ejecutarse el programa infectado, el virus se activa, produciendo diferentes efectos.

La tabla de asignación de ficheros o FAT (del inglés "File Allocation Table") es la sección de un disco utilizada para enlazar la información contenida en este. Se trata de un elemento fundamental en el sistema. Los virus que atacan a este elemento son especialmente peligrosos, ya que impedirán el acceso a ciertas partes del disco, donde se almacenan los ficheros críticos para el normal funcionamiento del ordenador.

Son programas que secuestran navegadores de internet principalmente el explorer. Los hijackers alteran las páginas iniciales del navegador e impide que el usuario pueda cambiarla, muestra publicidad en pops ups. Instala nuevas herramientas en la barra del navegador y a veces impiden al usuario acceder a ciertas páginas web. Un ejemplo puede ser no poder acceder a una página de antivirus.

Son programas que secuestran computadoras de forma que es controlada por terceros. Se utiliza para diseminar virus, keyloggers y procedimientos invasivos en general. Esto puede ocurrir cuando la computadora tiene el firewall y su sistema operativo desactualizado. 

Este virus se encarga de registrar cada tecla que sea pulsada, en algunos casos también registran los clics. Son virus que quedan escondidos en el sistema operativo de manera que la víctima no tiene como saber que está siendo monitorizada. Los keyloggers se utilizan usualmente para robar contraseñas de cuentas bancarias, obtener contraseñas personales como las del E-mail, Facebook, etc.

Algunas de las acciones de algunos virus son:

El primer trabajo académico en la teoría de los programas de ordenador auto-replicantes fue publicado por John von Neumann en 1949 quien dio conferencias en la Universidad de Illinois sobre la "Teoría y Organización de Autómatas Complicados" (Theory and Organization of Complicated Automata). El trabajo de von Neumann fue publicado más tarde como la "Teoría de los autómatas autorreproductivos". En su ensayo von Neumann describió cómo un programa de ordenador puede ser diseñado para reproducirse a sí mismo. El diseño de Von Neumann de un programa informático capaz de copiarse a sí mismo se considera el primer virus de computadoras del mundo, y es considerado como el padre teórico de la virología informática.

En 1960 Victor Vyssotsky, Robert Morris Sr. y Doug McIlroy, investigadores de Bell Labs, implementaron un juego de ordenador llamado "Darwin" en un mainframe IBM 7090. En él, dos programas jugadores compiten en la "arena" por controlar el sistema, eliminando a su enemigo, intentado sobreescribir o inutilizar todas sus copias. Una versión mejorada del mismo se conocerá como "Core Wars". Muchos de los conceptos de este se basan en un artículo de Alexander Dewdney en la columna Computer Recreations de la revista Scientific American.

En 1972 Veith Risak publica el artículo "Selbstreproduzierende Automaten mit minimaler Informationsübertragung" (autómata auto reproducible con mínimo intercambio de información). El artículo describe un virus por escrito con fines de investigación. Este contenía todos los componentes esenciales. Fue programado en Lenguaje ensamblador para el equipo SIEMENS 4004/35 y corrió sin problemas.

En 1975 el autor Inglés John Brunner publica la novela El jinete de la onda de shock, en la que anticipa el riesgo de virus de Internet. Thomas Joseph Ryan describió 1979 en " The Adolescence of P-1" (la adolescencia de P-1), como una Inteligencia Artificial se propaga de forma similar a un virus en la red informática nacional.

En 1980, Jürgen Kraus escribió una tesis en la Universidad técnica de Dortmund, en la que compara a algunos programas con los virus biológicos.

En 1982 Rich Skrenta, un estudiante de instituto de 15 años, programa el Elk Cloner para los Apple II, el primer virus informático conocido que tuvo una expansión real y no como un concepto de laboratorio. Puede ser descrito como el primer virus de sector de arranque.

En 1984 Leonard M. Adleman utilizó en una conversación con Fred Cohen por primera vez el término «virus informático».




</doc>
<doc id="2915" url="https://es.wikipedia.org/wiki?curid=2915" title="Vitaceae">
Vitaceae

Las vitáceas (Vitaceae) son una familia de plantas leñosas, principalmente lianas provistas de zarcillos opositifolios. Hojas alternas generalmente palmatilobuladas o palmaticompuestas. Flores pequeñas, hermafroditas o dioicas, actinomorfas, pentámeras o tetrámeras; cáliz gamosétalo, poco desarrollado; corola de pétalos libres o concrescentes por su parte superior, caduca; ovario súpero, con 2 óvulos por lóculo. Inflorescencias diversas, generalmente paniculiformes. Frutos en bayas. Existen unas 600 especies, la mayoría de países cálidos.





</doc>
<doc id="2918" url="https://es.wikipedia.org/wiki?curid=2918" title="Vitamina">
Vitamina

Las vitaminas (del inglés "vitamine", hoy "vitamin", y este del latín "vita" ‘vida’ y el sufijo "amina", término acuñado por el bioquímico Casimir Funk en 1912) son compuestos heterogéneos imprescindibles para la vida, ya que, al ingerirlos de forma equilibrada y en dosis esenciales, promueven el correcto funcionamiento fisiológico. La mayoría de las vitaminas esenciales no pueden ser elaboradas por el organismo, por lo que este no puede obtenerlas más que a través de la ingesta equilibrada de alimentos naturales que las contienen. Las vitaminas son nutrientes que junto con otros elementos nutricionales actúan como catalizadoras de todos los procesos fisiológicos (directa e indirectamente).

Las vitaminas son precursoras de coenzimas, (aunque no son propiamente enzimas) grupos prostéticos de las enzimas. Esto significa que la molécula de la vitamina, con un pequeño cambio en su estructura, pasa a ser la molécula activa, sea ésta coenzima o no.

Los requisitos mínimos diarios de las vitaminas no son muy altos, se necesitan tan solo dosis de miligramos o microgramos contenidas en grandes cantidades (proporcionalmente hablando) de alimentos naturales. Tanto la deficiencia como el exceso de los niveles vitamínicos corporales pueden producir enfermedades que van desde leves a graves e incluso muy graves como la pelagra o la demencia entre otras, e incluso la muerte. Algunas pueden servir como ayuda a las enzimas que actúan como cofactor, como es el caso de las vitaminas hidrosolubles.

La deficiencia de vitaminas se denomina hipovitaminosis mientras que el nivel excesivo de vitaminas se denomina hipervitaminosis.

Está demostrado que las vitaminas del grupo B son imprescindibles para el correcto funcionamiento del cerebro y el metabolismo corporal. Este grupo es hidrosoluble (solubles en agua) debido a esto son eliminadas principalmente por la orina, lo cual hace que sea necesaria la ingesta diaria y constante de todas las vitaminas del complejo “B” (contenidas en los alimentos naturales).

Las vitaminas se clasifican según sean solubles en agua ("hidrosolubles") o si lo son en lípidos ("liposolubles"). En los seres humanos hay 13 vitaminas que se clasifican en dos grupos: 9 hidrosolubles (8 del complejo B y la vitamina C) y 4 liposolubles (A, D, E y K).

Las vitaminas liposolubles, A, D, E y K, se consumen junto con alimentos que contienen grasa.

Son las que se disuelven en grasas y aceites. Se almacenan en el hígado y en los tejidos grasos. Debido a que se pueden almacenar en la grasa del cuerpo no es necesario tomarlas todos los días, por lo que es posible, tras un consumo suficiente, subsistir una época sin su aporte.

Si se consumen en exceso (más de 10 veces las cantidades recomendadas) pueden resultar tóxicas. Esto les puede ocurrir sobre todo a deportistas, que aunque mantienen una dieta equilibrada recurren a suplementos vitamínicos en dosis elevadas, con la idea de que así pueden aumentar su rendimiento físico. Esto es totalmente falso, así como la creencia de que los niños van a crecer más si toman más vitaminas de las necesarias.

Las vitaminas liposolubles son:

Estas vitaminas no contienen nitrógeno, son solubles en grasa, y por tanto, son transportadas en la grasa de los alimentos que la contienen. Por otra parte, son bastante estables frente al calor (la vitamina C se degrada a 90 °C en oxalatos tóxicos). Se absorben en el intestino delgado con la grasa alimentaria y pueden almacenarse en el cuerpo en mayor o menor grado (no se excretan en la orina). Dada a la capacidad de almacenamiento que tienen estas vitaminas no se requiere una ingesta diaria.

Las vitaminas hidrosolubles son aquellas que se disuelven en agua. Se trata de coenzimas o precursores de coenzimas, necesarias para muchas reacciones químicas del metabolismo.

En este grupo de vitaminas, se incluyen las vitaminas B (tiamina), B (riboflavina), B (niacina o ácido nicotínico), B (ácido pantoténico), B (piridoxina), B/B (biotina), B (ácido fólico), B (cobalamina) y vitamina C (ácido ascórbico).

Estas vitaminas contienen nitrógeno en su molécula (excepto la vitamina C) y no se almacenan en el organismo, a excepción de la vitamina B, que lo hace de modo importante en el hígado. El exceso de vitaminas ingeridas se excreta en la orina, por lo cual se requiere una ingesta prácticamente diaria, ya que al no almacenarse se depende de la dieta. Por otro lado, estas vitaminas se disuelven en el agua de cocción de los alimentos con facilidad, por lo que resulta conveniente aprovechar esa agua para preparar caldos o sopas.

La deficiencia de vitaminas puede producir trastornos más o menos graves, según el grado de deficiencia, llegando incluso a la muerte. Respecto a la posibilidad de que estas deficiencias se produzcan en el mundo desarrollado hay posturas muy enfrentadas. Por un lado están los que aseguran que es prácticamente imposible que se produzca una avitaminosis, y por otro los que responden que es bastante difícil llegar a las dosis de vitaminas mínimas, y por tanto, es fácil adquirir una deficiencia, por lo menos leve.

Normalmente, los que alegan que es “poco probable” una avitaminosis son mayoría. Este grupo mayoritario argumenta que:

Por el lado contrario se responde que:

Por estos motivos un bando recomienda consumir suplementos vitamínicos si se sospecha que no se llega a las dosis necesarias. Por el contrario, el otro bando lo ve innecesario, y avisan que abusar de suplementos puede ser perjudicial.

Las vitaminas aunque son esenciales, pueden ser tóxicas en grandes cantidades. Unas son muy tóxicas y otras son inocuas incluso en cantidades muy altas.
La toxicidad puede variar según la forma de aplicar las dosis. Como ejemplo, la vitamina D se administra en cantidades suficientemente altas como para cubrir las necesidades para 6 meses; sin embargo, no se podría hacer lo mismo con vitamina B3 o B6, porque sería muy tóxica.
Otro ejemplo es el que la suplementación con vitaminas hidrosolubles a largo plazo, se tolera mejor debido a que los excedentes se eliminan fácilmente por la orina.

Las vitaminas más tóxicas son la D, y la A, también lo puede ser la vitamina B3.
Otras vitaminas, sin embargo, son muy poco tóxicas o prácticamente inocuas.
La B no posee toxicidad incluso con dosis muy altas. A la tiamina le ocurre parecido, sin embargo con dosis muy altas y durante mucho tiempo puede provocar problemas de tiroides. En el caso de la vitamina E, solo es tóxica con suplementos específicos de vitamina E y con dosis muy elevadas. También se conocen casos de intoxicaciones en esquimales al comer hígado de mamíferos marinos (el cual contiene altas concentraciones de vitaminas liposolubles).

La principal fuente de vitaminas son los vegetales crudos, por ello, hay que igualar o superar la recomendación de consumir 5 raciones de vegetales o frutas frescas al día.

Por eso hay que evitar los procesos que produzcan perdidas de vitaminas en exceso:

Aunque la mayoría de los procesamientos perjudica el contenido vitamínico, algunos procesos biológicos pueden incrementar el contenido de vitaminas en los alimentos, como por ejemplo:

Los procesos industriales, normalmente suelen destruir las vitaminas. Pero alguno puede ayudar a que se reduzcan las pérdidas:
No consumir vitaminas en los niveles apropiados (contenidas en los alimentos naturales) puede causar graves enfermedades.

El valor de comer ciertos alimentos para mantener la salud era reconocido mucho antes de que se identificaran las vitaminas. Los antiguos egipcios sabían que la alimentación de una persona con hígado podía ayudar a curar la ceguera nocturna, una enfermedad que ahora se sabe que es causada por una deficiencia de vitamina A. El avance de los viajes oceánicos durante el Renacimiento dio lugar a que las expediciones pasaran largos periodos sin acceso a frutas frescas y vegetales y a que apareciesen enfermedades por deficiencias vitamínicas, bastante comunes entre las tripulaciones de los buques.

En 1747, el cirujano escocés James Lind descubrió que los alimentos cítricos ayudaban a prevenir el escorbuto, una enfermedad particularmente mortal en la que el colágeno no se forma correctamente, causando mala cicatrización de las heridas, el sangrado de las encías, dolores agudos y, finalmente, la muerte. En 1753, Lind publicó su "Treatise on the Scurvy" [Tratado sobre el escorbuto], que recomendaba el uso de limones y limas para evitarlo, práctica que fue adoptada por la Marina Real británica. (Esto dio lugar al apodo "Limey" para los marineros de la Royal Navy). El descubrimiento de Lind, sin embargo, no fue aceptado por todos y en las expediciones árticas de la misma Royal Navy, en el , en lugar de prevenir el escorbuto con una dieta de alimentos frescos, se creía evitarlo con una buena higiene, el ejercicio regular y el mantenimiento de la moral de la tripulación a bordo. Como resultado, las expediciones árticas continuaron siendo afectadas por el escorbuto y otras enfermedades de deficiencias vitamínicas. A principios del , cuando Robert Falcon Scott realizó sus dos expediciones a la Antártida, la teoría médica que prevalecía en ese momento era que el escorbuto era causado por la comida enlatada «contaminada».

Desde finales del y principios del , el uso de estudios de privación permitió a los científicos aislar e identificar una serie de vitaminas. Los lípidos del aceite de pescado se utilizaron para curar el raquitismo en ratas, y por ello los nutrientes solubles en grasa se llamaron antirraquitismo A ("antirachitic A"). Así, el primer bioactivo “vitamínico” nunca aislado, que curó el raquitismo, se llamó inicialmente “vitamina A”; sin embargo, la bioactividad de este compuesto se llama ahora vitamina D. En 1881, el cirujano ruso Nikolai Lunin (Лунин, Николай Иванович) estudió los efectos del escorbuto mientras estaba en la Universidad de Tartu, en la actual Estonia. Alimentó ratones con una mezcla artificial de todos los constituyentes separados de la leche conocidos en ese momento, a saber, proteínas, grasas, carbohidratos, y sales. Los ratones que recibieron solo los componentes individuales murieron, mientras que los ratones alimentados con la leche en sí se desarrollaron normalmente. Lunin llegó a la conclusión de que «un alimento natural, como la leche, debe por lo tanto contener, además de estos ingredientes principales conocidos, pequeñas cantidades de sustancias desconocidas esenciales para la vida». Sin embargo, sus conclusiones fueron rechazadas por otros investigadores —como su asesor, Gustav von Bunge.— cuando fueron incapaces de reproducir sus resultados. La diferencia fue que él había utilizado el azúcar de mesa (sacarosa), mientras que otros investigadores habían utilizado el azúcar de la leche (lactosa) que todavía contenía pequeñas cantidades de vitamina B. Un resultado similar de Cornelius Pekelharing apareció en una revista médica neerlandesa en 1905, pero no se informó ampliamente.

En Asia oriental, donde el arroz blanco refinado era el alimento básico común de la clase media, el beriberi resultante de la falta de vitamina B era endémico. En 1884, Takaki Kanehiro, un experimentado médico japonés, que había estudiado con otros británicos, de la Marina Imperial Japonesa, observó que el beriberi era endémico entre la tripulación de bajo rango que a menudo solo comía arroz, pero que no aparecía entre los oficiales que consumían una dieta al estilo occidental. Con el apoyo de la marina japonesa, experimentó con las tripulaciones de dos barcos de guerra; una tripulación fue alimentada solo con arroz blanco, mientras que la otra lo fue con una dieta de carne, pescado, cebada, arroz y frijoles. En el grupo que solo comía arroz blanco se documentaron 161 casos de beriberi y 25 muertes en la tripulación, mientras que en el segundo grupo solo se dieron 14 casos de beriberi y ninguna muerte. Esto convenció a Takaki y a la marina de guerra japonesa que la dieta era la causa del beriberi, pero se equivocaron cuando creyeron que con cantidades suficientes de proteínas lo impedirían. Que las enfermedades podrían ser el resultado de algunas deficiencias en la dieta fue además investigado por Christiaan Eijkman, quien en 1897 descubrió que la alimentación con arroz integral en lugar de la variedad refinada para pollos, ayudaba a prevenir una clase de polineuritis que era el equivalent del beriberi en las gallinas. Al año siguiente, Frederick Hopkins postuló que algunos alimentos contenían «factores accesorios» —además de proteínas, carbohidratos, grasas, etc.— que eran necesarios para las funciones del cuerpo humano. Hopkins y Eijkman fueron galardonados con el en 1929 por su descubrimiento de varias vitaminas.

En 1910, el científico japonés Umetaro Suzuki logró aislar el primer complejo vitamínico, extrayendo un complejo hidrosoluble de micronutrientes a partir del salvado de arroz, al que llamó ácido abérico (más tarde "Orizanin"). Publicó este descubrimiento en una revista científica japonesa. Cuando el artículo fue traducido al alemán, en la traducción no se hacía constar que se trataba de un nutriente recién descubierto (afirmación sí hecha en el artículo original en japonés) y por ello su descubrimiento paso inadvertido. En 1912, el bioquímico polaco Casimir Funk, que entonces trabajaba en Londres, aisló el mismo complejo de micronutrientes y propuso que el complejo se llamará «vitamina» (de «vital amina», nombre sugerido por Max Nierenstein un amigo y lector de bioquímica en la Universidad de Bristol.) Más tarde se conocería como vitamina B (niacina), aunque la describió como "anti-beri-beri-factor" (que hoy se llamaría tiamina o vitamina B1). Funk propuso la hipótesis de que otras enfermedades, como el raquitismo, la pelagra, la enfermedad celíaca y el escorbuto, también podrían curarse con vitaminas. El nombre pronto se convirtió en sinónimo de los «factores accesorios» de Hopkins, y, cuando se demostró que no todas las vitaminas eran aminas, la palabra ya estaba en todas partes. En 1920, Jack Cecil Drummond propuso que la “e” final se suprimiera para restarle importancia a la referencia “amina”, cuando los investigadores empezaron a sospechar que no todas las “vitaminas” (en particular, la vitamina A) tenían un componente de amina.

El Premio Nobel de Fisiología o Medicina de 1929 fue otorgado a Christiaan Eijkman y a sir Frederick Gowland Hopkins por sus contribuciones al descubrimiento de las vitaminas. Treinta y cinco años antes, Eijkman había observado que los pollos alimentados con arroz blanco pulido desarrollaban síntomas neurológicos similares a los observados en marineros militares y soldados alimentados con una dieta a base de arroz, y que los síntomas se revertían cuando los pollos cambiaron a arroz integral. Llamó a esto "el factor anti-beriberi", que más tarde fue identificado como B, tiamina.

En 1930, Paul Karrer dilucidó la estructura correcta del beta-caroteno, el principal precursor de la vitamina A, e identificó otros carotenoides. Karrer y Norman Haworth confirmaron el descubrimiento de Albert Szent-Györgyi del ácido ascórbico e hicieron importantes contribuciones a la química de las flavinas, lo que llevó a la identificación de la lactoflavina. Por sus investigaciones sobre los carotenoides, las flavinas y las vitaminas A y B2, ambos recibieron el Premio Nobel de Química en 1937.

En 1931, Albert Szent-Györgyi y uno de sus investigadores Joseph Svirbely sospecharon que el “ácido hexurónico” era en realidad la vitamina C, y dieron una muestra a Charles Glen King, que probó su eficacia contra el escorbuto en ensayos con conejillos de indias. En 1937, Szent-Györgyi fue galardonado con el Premio Nobel de Fisiología o Medicina por su descubrimiento. En 1943, Edward Adelbert Doisy y Henrik Dam fueron galardonados con el Premio Nobel de Fisiología o Medicina por su descubrimiento de la vitamina K y su estructura química. En 1967, George Wald fue galardonado con el Premio Nobel (junto con Ragnar Granit y Haldan Keffer Hartline) por su descubrimiento de que la vitamina A podría participar directamente en un proceso fisiológico.

En 1938, Richard Kuhn recibió el premio Nobel de Química por su trabajo sobre carotenoides y vitaminas, específicamente B y B.

Cinco personas han recibido premios Nobel por estudios directos e indirectos de la vitamina B: George Whipple, George Minot y William P. Murphy (1934), Alexander R. Todd (1957) y Dorothy Hodgkin (1964).

Una vez descubiertas, las vitaminas se promocionaron activamente en artículos y anuncios en "McCall's", "Good Housekeeping", y otros medios de comunicación. Los especialistas en marketing promovieron con entusiasmo el aceite de hígado de bacalao, una fuente de vitamina D, como «sol embotellado», y los plátanos como un «alimento de vitalidad natural». Promovieron alimentos como los pasteles de levadura, una fuente de vitamina B, sobre la base de un valor nutricional determinado científicamente, en lugar del sabor o la apariencia. Los investigadores de la Segunda Guerra Mundial se centraron en la necesidad de garantizar una nutrición adecuada, especialmente en alimentos procesados. A Robert W. Yoder se le atribuye el primer uso del término "vitamania", en 1942, para describir el atractivo de depender de suplementos nutricionales en lugar de obtener vitaminas de una dieta variada de alimentos. La preocupación constante por un estilo de vida saludable ha llevado a un consumo obsesivo de aditivos cuyos efectos beneficiosos son cuestionables.




</doc>
<doc id="2919" url="https://es.wikipedia.org/wiki?curid=2919" title="Velocidad (desambiguación)">
Velocidad (desambiguación)

El término velocidad puede referirse al:


</doc>
