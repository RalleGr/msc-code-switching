<doc id="6887" url="https://es.wikipedia.org/wiki?curid=6887" title="Euphausiacea">
Euphausiacea

Los eufausiáceos (Euphausiacea) son un orden de crustáceos malacostráceos conocidos genéricamente como kril. Pueden encontrarse en todos los océanos del mundo, se alimentan sobre todo de fitoplancton y son un elemento fundamental de la cadena trófica de los ecosistemas oceánicos. En el océano Antártico, una especie, el kril antártico ("Euphausia superba"), constituye una biomasa estimada de alrededor de 379 000 000 de toneladas, lo que la convierte en una de las especies con mayor biomasa del planeta, de la cual más de la mitad es consumida por ballenas barbadas, focas, pingüinos, calamares y peces cada año. La mayoría de las especies de eufausiáceos realizan grandes migraciones verticales diarias, lo que proporciona alimento a los depredadores cerca de la superficie por la noche y en aguas más profundas durante el día.

De comportamiento gregario, se agrupan en enormes cardúmenes que se extienden a lo largo de kilómetros con miles de individuos concentrados en un solo metro cúbico de agua, lo que los hace una especie idónea para su explotación comercial. Se pesca comercialmente en el océano Antártico y en las aguas en torno Japón. La captura total asciende a entre 150 000 y 200 000 toneladas anuales, la mayor parte de la cual proveniente del mar del Scotia. La mayoría se utiliza en la acuicultura, para la confección de alimento para acuarios, como cebo en la pesca deportiva o en la industria farmacéutica. En Japón, Filipinas y Rusia también se usa para el consumo humano.

Su nombre común en español proviene del inglés "krill" y este a su vez del noruego"krill" (alevín, pez pequeño).

Los eufausiáceos (Euphausiacea) son un orden de artrópodos incluidos dentro del gran subfilo Crustacea. El grupo con más familias y más numeroso de crustáceos, la clase Malacostraca, incluye el superorden Eucarida que comprende tres órdenes: Euphausiacea (kril), Decapoda (camarones, cangrejos, langostas) y Amphionidacea.

El orden se divide en dos familias. La más abundante es Euphausiidae, que contiene 10 géneros con un total de 86 especies; de estos, el género "Euphausia" es el mayor, con 31 especies. La familia menos conocida, Bentheuphausiidae, tiene una sola especie, "Bentheuphausia amblyops", un kril batipelágico que vive en aguas por debajo de los 1000 m de profundidad y se considera la especie de kril más primitiva que existe.

Las especies más conocidas, sobre todo por ser objeto de pesca comercial, son el kril antártico ("Euphausia superba"), el kril del Pacífico ("Euphausia pacifica") y el kril del norte ("Meganyctiphanes norvegica").

Se cree que el orden Euphausiacea es monofilético debido a que conserva varias características morfológicas únicas (autoapomorfia), como branquias filamentosas desnudas y toracópodos delgados, y por estudios moleculares.

Ha habido muchas propuestas sobre la ubicación del orden Euphausiacea. Desde la primera descripción de "Thysanopode tricuspide" realizada por Henri Milne-Edwards en 1830, la similitud de sus toracópodos birrámeos había llevado a los zoólogos a agrupar a los eufausiáceos y misidáceos (Mysidacea) en el orden Schizopoda, que fue dividido por Boas en 1883 en dos órdenes separados. En 1904 William Thomas Calman clasificó los misidáceos en el superorden Peracarida y los eufausiáceos en el superorden Eucarida, aunque hasta la década de 1930 se abogó por el orden Schizopoda. Posteriormente también se propuso que el orden Euphausiacea debería agruparse con Penaeidae (familia de langostinos) en Decapoda con base en sus similitudes de desarrollo, tal como lo consideraron Robert Gurney e Isabella Gordon. La razón de este debate es que el kril comparte algunas características morfológicas de los decápodos y otras de los misidáceos.

Los estudios moleculares no hay permitido su agrupación de manera inequívoca, posiblemente debido a la escasez de especies clave escasas como "Bentheuphausia amblyops" en Euphausiacea y "Amphionides reynaudii" en Eucarida. Un estudio apoya la monofilia de Eucarida (con el orden Mysida basal), otros agrupan Euphausiacea con Mysida (Schizopoda), mientras que otros agrupan Euphausiacea con Hoplocarida.

Ningún fósil existente puede asignarse inequívocamente a Euphausiacea. Se ha considerado que algunos taxones eumalacostráceos extintos podrían ser eufausiáceos, como "Anthracophausia", "Crangopsis" —actualmente asignado a Aeschronectida (Hoplocarida)— o "Palaeomysis". Todas las fechas de los procesos de especiación se estimaron mediante la técnica de reloj molecular, que ubicaron al último ancestro común de la familia de krils Euphausiidae (orden Euphausiacea menos "Bentheuphausia amblyops") como que vivió en el Cretácico inferior hace unos 130 millones de años.

El kril se encuentra en todos los océanos del mundo, aunque muchas especies individuales tienen una distribución endémica o nerítica. "Bentheuphausia amblyops", una especie batipelágica, tiene una distribución cosmopolita dentro de su hábitat en aguas profundas.

Las especies del género "Thysanoessa" se encuentran en los océanos Atlántico y Pacífico. "Meganyctiphanes norvegica" se distribuye por el Atlántico, desde una zona aproximadamente a la altura del Mediterráneo hacia el norte. "Euphausia pacifica" se distribuye por el océano Pácífico.

Entre las especies con distribuciones neríticas están las cuatro del género "Nyctiphanes"; son muy abundantes a lo largo de las regiones de surgencia de las corrientes marinas de California, Humboldt, Benguela y Canarias. Otra especie que solo tiene distribución nerítica es "Euphausia crystallorophias", endémica de la costa antártica.

Entre las especies con distribuciones endémicas están "Nyctiphanes capensis", que se encuentra solo en la corriente de Benguela, "Euphausia mucronata" en la corriente de Humboldt, y las seis especies de "Euphausia", nativas del océano Antártico.

En la Antártida se conocen siete especies, una en el género "Thysanoessa" ("T. macrura") y seis en "Euphausia". El kril antártico ("Euphausia superba") vive generalmente a profundidades que alcanzan los 100 m, mientras que "Euphausia crystallorophias" puede alcanzar una profundidad de 4000 m, aunque por lo general viven a profundidades de 300-600 m como máximo. Ambos se encuentran en latitudes de 55°&bsp;S, con "E. crystallorophias" preferentemente al sur de 74° S y en regiones de hielo a la deriva. Otras especies conocidas en el océano Antártico son "E. frigida", "E. longirostris", "E. triacantha" y "E. vallentini".

Como crustáceos, los eufausiáceos tiene un exoesqueleto quitinoso compuesto por tres tagmas: el céfalon (cabeza), el pereion (fusionado al cefalón para formar un cefalotórax) y el pleon; esta capa exterior es transparente en la mayoría de las especies. Como todos los eucáridos, tienen el cuerpo dividido en cinco segmentos cefálicos, ocho torácicos y seis abdominales. Disponen de complejos ojos compuestos. Excepto en el caso del género "Thysanoessa" que cuenta con especies con diversos tipos de ojos, todas las demás especies de un mismo género tienen los ojos o bien redondos o bien bilobulados; la forma y tamaño de los ojos puede ser un factor importante para definir una especie. Cuentan con dos pares de antenas: el par de antenas superiores, las anténulas, están formadas por un pedúnculo antenular de tres segmentos y un par de flagelos antenulares compuestos por múltiples segmentos; las antenas en posición inferior consisten en un segmento basal con una escama y un pedúnculo antenal formado por dos segmentos y terminado en un largo flagelo. Tienen varios pares de patas torácicas en la parte ventral denominadas pereiopodos o toracópodos, por estar unidos al tórax; su número varía entre los distintos géneros y especies. Estas patas torácicas incluyen patas de alimentación y patas de aseo. Todas las especies cuentan con seis segmentos articulados: un telsón con urópodos en el extremo posterior y cinco pares de patas de natación ventrales llamadas pleópodos, muy similares a los de una langosta o los cangrejos de río. La mayoría de las especies tienen una longitud aproximada de 1-2 cm en su etapa adulta, aunque algunos alcanzan tamaños de 6-15 cm. La especie de mayor tamaño es la batipelágica "Thylopoda spinicauda". El kril se puede distinguir fácilmente de otros crustáceos similares, como las gambas y otros decápodos, por sus branquias externas visibles en el margen exterior de la coxa.
A excepción de "Bentheuphausia amblyops" y "Thysanopoda minyops" todas las especies de kril son bioluminiscentes gracias a unos órganos de gran tamaño denominados fotóforos que pueden emitir luz. La luz se genera mediante una reacción de quimioluminiscencia catalizada por enzimas, que activa una luciferina (un tipo de pigmento) mediante una enzima luciferasa. Algunos estudios indican que la luciferina de muchas especies es un tetrapirrol fluorescente similar aunque no idéntica a la de los dinoflagelados y que el kril probablemente no produzca esta sustancia por sí mismo sino que la adquiere porque estos organismos forman parte de su dieta. Los fotóforos del kril son órganos complejos con lentes y capacidad de enfoque y que pueden rotarse con los músculos. La función precisa de estos órganos se desconoce, aunque se cree que pueden estar relacionados con el apareamiento, interacción u orientación social y como una forma de camuflaje de contrailuminación para compensar su sombra contra la luz ambiente proveniente de la superficie.

La mayoría de las especies de kril se alimentan por filtración; sus apéndices anteriores, los toracópodos o patas torácicas, cuentan con unos pelillos muy finos con los que pueden filtrar su comida del agua. Estos filtros pueden ser muy efectivos en aquellas especies (como "Euphausia" spp.) que se alimentan principalmente de fitoplancton, en particular en diatomeas (algas unicelulares) y dinoflagelados. La mayoría omnívoros, aunque algunas especies son herbívoras y otras carnívoras y se alimentan de huevos y larvas de peces, copépodos y otro zooplancton y también detrito. Algunos estudios indican que no tienen un único tipo de alimentación, sino que se acomodan de forma natural a la comida que les es accesible en cada momento, pudiendo cambiar en periodos de tiempo corto de una dieta estrictamente herbívora a una omnívora o carnívora.

Su pequeño tamaño y su alimentación hacen del kril un elemento fundamental de la cadena trófica de los ecosistemas oceánicos, pues convierte la producción primaria que constituye su dieta en una forma adecuada para el consumo de animales más grandes que no pueden alimentarse directamente del plancton. Muchos animales se alimentan de estos pequeños crustáceos, desde animales más pequeños como peces o pingüinos, hasta animales más grandes como focas y ballenas barbadas. Las formas larvarias de kril se consideran generalmente parte del zooplancton.

Las alteraciones de un ecosistema que provoquen una disminución en la población de kril pueden tener efectos de gran alcance. Por ejemplo, durante una proliferación de cocolitóforos en el mar de Bering en 1998, la concentración de diatomeas disminuyó en el área afectada. Como el kril no puede alimentarse de cocolitóforos, su población (principalmente "E. pacifica") en esa región disminuyó drásticamente, lo que afectó a otras especies como las pardelas que tuvo un gran descenso de población. Se cree también que el incidente fue uno de los motivos por los que el salmón no se reprodujo esa temporada.

El cambio climático supone otra amenaza para las poblaciones de kril. Varios ciliados endoparásitos unicelulares del género "Collinia" pueden infectar especies de kril y devastar las poblaciones afectadas. Existen informes de estas enfermedades en "Thysanoessa inermis" en el mar de Bering y también para "E. pacifica", "Thysanoessa spinifera" y "T. gregaria" frente a la costa del Pacífico de América del Norte. Algunos ectoparásitos de la familia Dajidae (isópodos epicarideanos) afectan al kril (y también a gambas y mísidos); uno de estos parásitos es "Oculophryxus bicaulis", que se encontró en el kril "Stylocheiron affine" y "S. longicorne". Este parásito se adhiere al pedúnculo de los ojos del animal y absorbe la sangre de su cabeza; aparentemente inhibe la reproducción del huésped, ya que ninguno de los animales afectados alcanzó la madurez.

El ciclo de vida del kril está relativamente bien estudiado, a pesar de pequeñas variaciones en los detalles de una especie a otra. Después de la eclosión, experimentan varios estadios larvales: nauplio, pseudometanauplio, metanauplio, calyptopis y furcilia, cada uno de los cuales se divide en subetapas. La etapa pseudometanauplio es exclusiva de las especies que ponen sus huevos dentro de un saco ovígero (ovisaco o saco de cría). Las larvas crecen y mudan a medida que se desarrollan, reemplazando su rígido exoesqueleto cuando se vuelve demasiado pequeño. Las reservas de yema dentro de su cuerpo nutren a las larvas a través de la etapa metanauplio. Durante la calyptopis el proceso de diferenciación celular ya ha progresado lo suficiente como para desarrollar la boca y un tracto digestivo y comienzan a comer fitoplancton; en ese momento sus reservas de yema ya se han agotado y las larvas deben haber alcanzado la zona fótica, las capas superiores del océano donde penetra el sol y florecen las algas. Las etapas del estadio furcilia, donde ya son parecidos a adultos de pequeño tamaño, se caracterizan por la adición de segmentos y la emergencia y desarrollo de pleópodos abdominales, comenzando en los segmentos delanteros. Posteriormente desarrolla gónadas y madura sexualmente.

Los sexos están diferenciados y el apareamiento probablemente se realiza mediante la transferencia de uno o dos espermatóforos por parte del macho introduciendo su petasma (órgano de transferencia, modificación de los endopoditos del primer par de pleópodos) en el télico (apertura genital, modificación de la parte ventral del cefalotórax a la altura del 3.º, 4.º y 5.º par de pereiópodos) de la hembra. No hay constancia de observaciones de la cópula del kril, pero probablemente se produce durante la noche y se completa en unos segundos. Las hembras pueden llevar varios miles de huevos, que con el tiempo pueden alcanzar hasta un tercio de la masa corporal del animal. Pueden tener múltiples crías en una sola temporada, con intervalos entre cruces que duran del orden de días.
Utilizan dos tipos de mecanismo de desove. Las 57 especies de los géneros "Bentheuphausia", "Euphausia", "Meganyctiphanes", "Thysanoessa" y "Thysanopoda" son «reproductores por difusión»: la hembra libera los huevos fertilizados en el agua, donde generalmente se hunden, se dispersan y son abandonados. Estas especies liberan de 40 a 500 huevos por puesta, dependiendo de la especie y el tamaño de la hembra, y generalmente eclosionan en la etapa de nauplio 1, pero recientemente se ha descubierto que eclosionan a veces como metanauplio o incluso como etapas de calyptopis; Las 29 especies restantes de los otros géneros son «reproductores de saco», donde la hembra lleva los huevos con ella, unidos a los pares posteriores de los toracópodos hasta que eclosionan como metanauplio, aunque algunas especies como "Nematoscelis difficilis" pueden hacerlo como nauplio o pseudometanauplio.

Los juveniles, que crecen más rápido, mudan más a menudo que los mayores y lo de mayor tamaño. El número de mudas varía entre especies, o incluso dentro de la misma especie dependiendo de la zona, la época y las condiciones ambientales; está sujeta a muchos factores externos, como la latitud, la temperatura del agua y la disponibilidad de alimentos. La especie subtropical "Nyctiphanes simplex", por ejemplo, tiene un período entre mudas de dos a siete días: las larvas mudan cada cuatro días por término medio, mientras que los juveniles y los adultos lo hacen, de media, cada seis días. Para "E. superba" en el mar antártico, se han observado períodos entre mudas que van de 9 a 28 días, dependiendo de temperaturas entre –1 y 4 °C, y para "Meganyctiphanes norvegica" en el mar del Norte los periodos intermuda también varían entre 9 y 28 días, pero a temperaturas entre 2,5 y 15 °C. "E. superba" puede reducir su tamaño corporal cuando no hay suficiente comida disponible, mudando también cuando su exoesqueleto se vuelve demasiado grande. También se ha observado una reducción similar en "E. pacifica", una especie que se encuentra en el océano Pacífico desde zonas polares a templadas, como una adaptación a temperaturas anormalmente altas del agua. La reducción de tamaño también se ha comprobado en otras especies de kril de zonas templadas.

Algunas especies de kril de latitudes elevadas, como "Euphausia superba", pueden vivir más de seis años; otras, como la especie de latitud media "Euphausia pacifica", viven solo durante dos años. La longevidad de las especies subtropicales o tropicales es todavía más corta; por ejemplo, "Nyctiphanes simplex" generalmente vive solo de seis a ocho meses.

La mayoría de las especies de kril son gregarias; el tamaño y densidad de los cardúmenes varía según la especie y la región. En el caso de "Euphausia superba", alcanzan un tamaño de 10 000 a 60 000 individuos por metro cúbico. El agrupamiento es un mecanismo de defensa, que le ayuda a confundir a pequeños depredadores que busquen presas individuales. En 2012 se presentó lo que parece ser un algoritmo estocástico exitoso para modelar el comportamiento de los enjambres de kril. El algoritmo se basa en tres factores principales: «(i) movimiento inducido por la presencia de otros individuos (ii) actividad de alimentación, y (iii) difusión aleatoria».

Fundamentalmente para alimentarse, la mayor parte de especies de kril sigue un patrón de migración vertical diaria; pasan el día a mayor profundidad y suben durante la noche hacia la superficie, a menudo desplazándose más de 200 m. Cuanto más profundo van, más reducen su actividad, aparentemente para reducir los encuentros con depredadores y conservar energía. Su actividad natatoria varía en función de la cantidad de alimento ingerido; los individuos saciados que se habían alimentado en la superficie nadan menos activamente y se hunden hasta la zona mixta. A medida que se hunden producen heces, desempeñando así un papel en el ciclo del carbono antártico. Los individuos con el estómago vacío nadan más activamente y se dirigen hacia la superficie. La migración vertical puede producirse 2 a 3 veces al día. Algunas especies, como "Euphausia superba", "E. pacifica", "E. hanseni", "Pseudeuphausia latifrons" y "Thysanoessa spinifera", forman cardúmenes de superficie durante el día con fines alimenticios y reproductivos, aunque este comportamiento los hace extremadamente vulnerables a los depredadores. Los densos cardúmenes que forman pueden provocar un frenesí de alimentación entre peces, aves y mamíferos depredadores, especialmente cerca de la superficie. Cuando se lo molesta, el cardumen se dispersa e incluso se ha observado que algunos individuos mudan espontáneamente, dejando atrás la exuvia como señuelo.
Normalmente nadan a un ritmo de 5-10 cm/s (2-3 veces su tamaño corporal por segundo), usando sus Pleópodos natatorios para la propulsión. Sus grandes migraciones están sujetas a las corrientes oceánicas. Cuando se sienten en peligro, reaccionan con un comportamiento de huida, sacudiendo sus estructuras caudales, el telson y los urópodos, moviéndose hacia atrás a través del agua con relativa rapidez, alcanzando velocidades de 10 a 27 veces la longitud corporal por segundo, que un kril grande como "E. superba" significa alrededor de 0,8 m/s. Su rendimiento natatorio ha llevado a muchos investigadores a clasificar el kril adulto como formas de vida micro-nectónicas, es decir, pequeños animales capaces de movimiento individual contra corrientes (débiles).

El kril se ha criado artificialmente como una fuente de alimento para humanos y animales domésticos desde al menos el siglo XIX, y posiblemente antes en Japón, donde se lo conocía como "okiami". Su pesca a gran escala se desarrolló a fines de la década de 1960 y principios de la de 1970, y actualmente solo en aguas antárticas y en los mares en torno a Japón. Históricamente, las naciones con mayor volumen de captura de kril fueron Japón y la Unión Soviética o, tras su disolución, Rusia y Ucrania. La pesca alcanzó su punto máximo, que en 1983 era de aproximadamente 528 000 toneladas solo en el océano Antártico (de la cual la Unión Soviética suponía el 93%), actualmente se gestiona con precaución para evitar la sobrepesca.

En 1993, dos hechos provocaron una disminución de la pesca de kril: Rusia abandonó la industria y la Convención para la Conservación de Recursos Vivos Marinos Antárticos (CCRVMA) estableció unos cupos máximos de captura para una explotación sostenible del kril antártico. Tras una revisión de octubre de 2011, la Convención decidió no modificar la cuota.

La captura anual en la zona antártica se estabilizó en torno a las 200 000 toneladas, muy por debajo de la cuota de captura establecida por la CCRVMA de 5,6 millones de toneladas. El principal factor limitante fue probablemente los altos costos junto con cuestiones políticas y legales. En 2014 Japón y Rusia ya no se dedicaban a la pesca del kril, y los países con mayores capturas fueron Noruega, con 165 899 t (58% del total); Corea del Sur, con 55 414 t (19%); y China, con 54 303 t (10%).

Aunque la biomasa total del kril antártico puede alcanzar los 400 millones de toneladas, el impacto humano en esta especie clave está creciendo, con un aumento del 39% en su captura, que pasó de 212 000 a 294 000 toneladas durante el período 2010-2014.

Se encuentra en todos los océanos del mundo, pero se prefiere su pesca en los océanos meridionales porque es más abundante y fácil de capturar en estas regiones. Dado que los mares antárticos se consideran prístinos, el kril está considerado un «producto limpio».

El kril es una fuente rica en proteínas y ácidos grasos omega-3 es también rico en el antioxidante llamado astaxantina, por lo que a principios del está incrementándose su explotación para el consumo humano, como suplementos dietéticos como cápsulas de aceite, alimento para el ganado y alimento para mascotas. Tiene un sabor salado, similar al pescado, algo más fuerte que el camarón. Para su consumo masivo como productos preparados comercialmente, deben pelarse para eliminar su no comestible exoesqueleto.

En 2011 la Administración de Alimentos y Medicamentos de los Estados Unidos publicó un informe de no objeción para que un producto manufacturado de aceite de kril fuera reconocido como generalmente seguro para el consumo humano.





</doc>
<doc id="6889" url="https://es.wikipedia.org/wiki?curid=6889" title="Euphausia superba">
Euphausia superba

El kril antártico (Euphausia superba) es una especie de crustáceo malacostráceo del orden Euphausiacea propia de las aguas frías de los océanos Atlántico y Pacífico en las inmediaciones de la Antártida. Es un crustáceo de pequeño tamaño (hasta 6 cm de longitud y 2 g de peso), que puede vivir hasta seis años y forma enormes cardúmenes de gran densidad (hasta 30 000 ejemplares por m). Se alimenta de fitoplancton, aprovechando la energía que éste toma de la luz solar; por lo que constituye un eslabón esencial en la cadena trófica del ecosistema antártico, y es a la vez alimento de varios animales, entre ellos peces, pingüinos, petreles y ballenas.

Es la especie animal no-humana más exitosa del planeta, ya que su masa corporal total representa más de 500 millones de toneladas (el ser humano, más de 450 millones de toneladas).

Todos los miembros del orden Euphausiacea son crustáceos del superorden Eucarida, en los que la placa pectoral está unida al “caparazón” y forma a cada lado de éste las agallas del kril, visibles al ojo humano. Las patas no forman una estructura mandibular, lo que diferencia a este orden de los decápodos (langostinos, cangrejos).

El kril antártico abunda en las aguas superficiales de los mares del sur: tiene una distribución circumpolar, con las mayores concentraciones en el sector del océano Atlántico.

El límite de los sectores del mar austral, que incluyen al Atlántico, al Pacífico y al Índico se definen en forma aproximada por la convergencia antártica, un frente circumpolar donde el agua fría superficial se sumerge bajo las aguas subantárticas más cálidas. Este frente corre aproximadamente a 55º Sur y desde allí al continente. El océano austral cubre 32 millones de km, lo que representa 65 veces la superficie del mar del Norte. En invierno más de tres cuartas partes de la superficie están cubiertas por hielo, en tanto que en verano unos 24 millones de km² se encuentran libres de él. La temperatura del agua se encuentra en un rango entre -1,3 y 3 °C.

Las aguas del océano austral forman un sistema de corrientes, incluyendo la "corriente circumpolar antártica", que produce la circulación en sentido oeste-este de las aguas superficiales, y la "corriente costera antártica", que corre en sentido antihorario.

En el frente entre ambas, se desarrollan grandes remolinos, como ocurre en el mar de Weddell. El kril se distribuye siguiendo estas masas hídricas, estableciendo una presencia homogénea alrededor de la Antártida, con intercambio genético en toda el área.

Es poco conocido el patrón de migración exacto, debido a que el kril no puede ser monitoreado individualmente para estudiar sus movimientos.

El kril es la especie clave del ecosistema antártico, y constituye una importante fuente de alimento para las ballenas, pinnípedos, focas leopardo, focas peleteras, focas cangrejeras, calamares, peces hielo, pingüinos, albatros y muchas otras especies de aves.

La foca cangrejera ("Lobodon carcinophagus") ha desarrollado dientes especiales como adaptación para capturar al kril, lo que le permite obtenerlos del agua. La dentadura funciona como un colador perfecto, aunque se desconoce la estrategia exacta utilizada por el predador. La "cangrejera" es la foca más abundante del mundo, y el 98% de su dieta está constituida por kril antártico. Según estudios realizados estas focas consumen más de 63 millones de toneladas anuales de kril. La foca leopardo ha desarrollado dientes parecidos, y en su dieta el kril implica el 45% de su dieta. El consumo anual de la cadena trófica representa valores entre 152 y 313 millones de t de kril, de los cuales las focas consumen entre 63 y 130 millones, las ballenas entre 34 y 43 millones, las aves entre 15 y 20 millones, los calamares entre 20 y 100 millones, y los peces entre 10 y 20 millones. Para tener una idea de lo que estas cantidades significan, téngase en cuenta que el total de captura pesquera mundial durante el año 2002 fue de 84,5 millones de toneladas.

La temporada principal de reproducción del kril antártico abarca desde enero hasta marzo, tanto en la placa continental como en las áreas de mar profundo. En la forma típica de todos los "Euphausia", el macho adhiere un paquete de esperma en la abertura genital de la hembra. Con este propósito la primera pata del macho tiene una estructura específica de herramienta de apareamiento. La hembra pone entre 6.000 y 10.000 huevos en cada puesta, que son fertilizados a medida que salen por el canal genital, por el esperma liberado desde el espermatóforo adherido por el macho.

De acuerdo con la hipótesis clásica de Marr, derivada de los resultados de la expedición del barco británico "RSS Discovery", el desarrollo de los huevos luego sigue de la siguiente manera: la gastrulación tiene lugar durante el descenso de las huevas de 6 mm desde la superficie hasta la máxima profundidad, que en áreas oceánicas se encuentra entre 2000 y 3000 m. Desde el momento en que los huevos eclosionan, la primera larva (primera "nauplus") comienza a migrar hacia la superficie con ayuda de sus tres pares de patas, en lo que se denomina ""ascenso del desarrollo"".

En los dos estados larvales siguientes, "segundo nauplius" y "metanauplius" el animal todavía no se alimenta, nutriéndose del remanente de la yema.
Transcurridas tres semanas, el pequeño kril ha completado su ascenso. Pueden aparecer en cantidades enormes, dos ejemplares por litro en una profundidad de hasta 60 metros. Al crecer, se suceden otros estados larvarios: primero y segundo "calytopis", primero a sexto "furcilia". En estos estados larvarios se produce el desarrollo completo de las patas, los ojos compuestos y las cerdas.

Con un tamaño de 15 mm los juveniles ya posen los hábitos de los ejemplares adultos. La madurez se alcanza a una edad de entre dos y tres años. Como todos los crustáceos, el kril debe mudar para poder crecer. Cada trece a veinte días, aproximadamente, pierde su exoesqueleto quitinoso y lo deja atrás como exuvia.

El intestino de "E. superba" puede verse frecuentemente de un color verde brillante a través de la piel transparente del animal, lo que indica que su alimento predominante es el fitoplancton, en especial diatomeas muy pequeñas (20 μm), que filtra del agua mediante una ""canasta de alimentación"".

El caparazón cristalino de las diatomeas es triturado en el tubo gástrico, y digerido en el hepatopáncreas. El kril puede además capturar otros pequeños crustáceos del orden Amphipoda y de la subclase Copepoda, como así también otros componentes del zooplancton.

El intestino forma un tubo recto cuya eficiencia digestiva no es muy grande, por lo que en las heces se puede hallar mucho carbono.

Se ha observado en acuarios que el kril llega a comer a ejemplares de su misma especie. Si no es alimentado, puede reducir su tamaño tras la muda, lo que resulta excepcional en animales de ese tamaño. Se cree que esto se debe a un proceso de adaptación a la estacionalidad del alimento, que está limitado durante el oscuro invierno antártico.

El kril antártico tiene la habilidad de capturar las minúsculas células del fitoplancton de una forma que ninguna otra especie puede lograr. Lo hace utilizando sus muy especializadas patas frontales, que constituyen un eficiente aparato de filtrado y las seis patas unidas al tórax como canasta de recolección. En las zonas más finas, las aberturas de la canasta tienen un diámetro de 1 μm.

La imagen animada muestra un ejemplar de kril suspendido en un ángulo de 55º. En bajas concentraciones de alimento, la canasta de alimentación empuja a través del agua y luego las algas se introducen en la boca mediante cerdas especiales situadas en el lado interior de las patas.

El kril antártico puede raspar la capa verde de algas del lado inferior de la placa de hielo.

La imagen tomada mediante ROV muestra como la mayoría de los ejemplares nadan arriba y abajo directamente bajo el hielo. Solo un ejemplar aislado (en el centro) está recolectando en el agua. El kril ha desarrollado filas especiales de rastrillos de cerda en el extremo de las patas, con las que raspa el hielo en un patrón zigzagueante en forma parecida a una cortadora de césped.

Pueden limpiar las algas a una velocidad de aproximadamente 1,5 cm²/s. Se sabe desde hace relativamente poco que la película de algas bajo el hielo oceánico está muy desarrollada en grandes superficies, y a menudo contiene mucha más materia orgánica que toda la columna de agua por debajo. El kril encuentra una amplia fuente de energía aquí, especialmente en primavera.

El kril es altamente desordenado para alimentarse, y a menudo regurgita materia orgánica de fitoplancton en forma de bolas que contienen miles de células agrupadas. También produce hilos fecales que todavía contienen cantidades significativas de carbono y cristales de los caparazones de diatomea. Ambos materiales son pesados, y caen relativamente rápido al fondo del mar.

Este proceso se denomina bomba biológica. Como el océano alrededor de la Antártida es muy profundo (2000 a 2400 m) el resultado es el hundimiento de grandes cantidades de dióxido de carbono (CO), con lo que se elimina carbono de la biosfera y la fijación resultante se mantiene por unos 1000 años.

Si al fitoplancton lo consumen otros componentes del ecosistema pelágico, la mayoría del carbono permanece en los estratos superiores. Se cree que este proceso es uno de los mayores mecanismos de bio-retroalimentación del planeta, por lo menos el más cuantificable, generado por una gigantesca biomasa. Se requieren todavía otras investigaciones que permitan cuantificar el ecosistema del océano austral.

Suele llamárselo "camarón luminoso" porque puede emitir luz, producida por órganos bioluminiscentes, que se encuentran ubicados en varias partes del cuerpo: pares de órganos detrás de los ojos, y en la articulación de la segunda y séptima pata, y órganos simples en los cuatro esternones.

Emiten periódicamente luz de color amarillo verdoso claro, cada dos o tres segundos. Como muestra de su alto nivel de evolución, incluyen un reflector cóncavo atrás del órgano propiamente dicho, y un lente en su frente para aumentar la luz producida. El órgano completo puede rotarse gracias a músculos específicos.

La función de esta luz no es todavía comprendida cabalmente: algunas hipótesis sugieren que sirven para enmascarar la sombra del kril, de manera que no pueda ser avistado por sus predadores desde abajo. Otras especulaciones sostienen que juega un rol significativo en el apareamiento nocturno.

Los órganos bioluminiscentes del kril contienen varias sustancias fluorescentes. El componente principal adquiere su máxima fluorescencia con una excitación de 355 nm, emitiendo a 510 nm.

El kril usa una reacción de escape para evadir a sus predadores, que consiste en nadar hacia atrás muy rápidamente agitando su telson. Puede alcanzar velocidades de más de 60 cm/s (2 km/h).

El tiempo de inducción biológica para disparar el estímulo fisiológico es, a pesar de las bajas temperaturas, de solo 55 milisegundos.

Aunque la utilidad y los motivos para la evolución de su impresionante ojo compuesto permanecen en el misterio, no existen dudas que el kril antártico posee una de las estructuras para percepción visual más fantásticas de la naturaleza.

Puede disminuir su tamaño de una muda a otra (cuando lo "normal" entre las especies de muda es siempre aumentarlo), en lo que parece ser una estrategia para adaptarse a la escasez estacional de alimento, ya que un cuerpo menor requiere menos energía, y -en consecuencia- menos alimento.

La reducción no alcanza sin embargo a los ojos compuestos. La relación entre el tamaño del ojo y la longitud corporal ha demostrado servir, por lo tanto, como un indicador relativo de inanición.

La biomasa del kril antártico se estima entre 125 y 725 millones de toneladas convirtiendo a "E. superba" en la especie animal más exitosa del planeta. Debe tenerse en cuenta que de todos los animales observables a simple vista algunos biólogos opinan que la hormiga provee la biomasa mayor, pero esta hipótesis suma cientos de especies diferentes de hormigas. Otros sostienen que el récord lo ostentan los copepoda, pero aquí también se trata de una subclase que incluye cientos de especies distribuidas por todo el planeta.

La razón por la que es capaz de llegar a esta biomasa se origina en que en las aguas que rodean la masa continental antártica reside la mayor colonia de plancton del mundo. El océano está repleto de fitoplancton, y como el agua sube desde las profundidades a la luminosa superficie, acarrea nutrientes de todos los océanos del planeta a la zona fótica donde nuevamente están disponibles para los organismos vivientes.
Así, la producción primaria -la conversión de luz solar en biomasa, base de la cadena alimentaria- representa una fijación anual de carbono de entre 1 y 2 g/m² en el océano abierto, y cerca del hielo puede alcanzar de 30 a 50 g/m². Estos valores no son extremadamente altos, comparados con áreas muy productivas como el mar del Norte o las regiones de surgencia, pero la superficie donde se dan es enorme, incluso comparada con otras grandes zonas productoras primarias como las selvas.

Por otro lado, durante el verano austral hay muchas más horas de luz solar para alimentar el proceso. Todos estos factores hacen del plancton y el kril una parte crítica del ciclo ecológico del planeta.

Existen sospechas fundadas de que la biomasa del kril antártico ha disminuido rápidamente en el transcurso de las últimas décadas. Algunos científicos han especulado que tal disminución podría haber alcanzado hasta el 80%. La causa sería la reducción de la placa de hielo debido al calentamiento global.

El gráfico describe el calentamiento del océano austral y la pérdida de la placa de hielo en una escala invertida durante los últimos cuarenta años. El kril antártico, especialmente en sus primeras etapas de desarrollo, parece necesitar la placa de hielo como mejor opción de supervivencia. La placa provee escondites naturales que los ejemplares usan para evadir a sus predadores. En los años en que la placa disminuye de forma notoria, tiende a dejar su nicho ecológico a las salpas, un pequeño predador de plancton que en otras circunstancias no constituye un competidor biológico.

La pesca del kril antártico está en el orden de 100 000 toneladas anuales. Las principales naciones son Japón y Polonia. El producto es muy usado en Japón como alimento de lujo y en todo el mundo para alimento balanceado y cebo de pesca. La captura se dificulta por dos razones principales. En primer lugar, una red para kril debe tener un tejido muy fino, lo que genera un arrastre muy alto y olas de proa que desvían al kril hacia los lados. En segundo lugar, las redes finas tienden a romperse o atascarse más fácilmente.

Un problema adicional es traer el kril capturado a bordo: cuando la red llena es izada del agua, los animales se comprimen de tal forma que pierden mucho de su líquido orgánico. Se ha experimentado bombeándolos desde la red sumergida en el agua, y existen ensayos de redes experimentales.

El procesamiento del kril debe ser muy rápido teniendo en cuenta que luego de la captura se deterioran en pocas horas. El objetivo del procesamiento es separar las patas de la sección frontal, y retirar el caparazón quitinoso, con el fin de producir productos congelados y polvos concentrados. El alto contenido de proteínas y vitaminas lo hace apropiado para el consumo humano y la industria de alimentos balanceados.

A pesar de la falta de conocimientos sobre el ecosistema antártico, las amplias investigaciones efectuadas relacionan íntimamente al kril con la fijación del carbono. En amplias áreas del océano austral abundan los nutrientes, pero —aun así— no hay un crecimiento sostenido del fitoplancton. Se denominan (en inglés) ""HNLC"", por "nutriente alto-clorofila baja", un fenómeno que también ha dado en llamarse la ""paradoja antártica"", causada por la ausencia de hierro.

La inyección de cantidades relativamente pequeñas de hierro desde barcos de investigación soluciona la carencia en varios kilómetros a la redonda. Existe la esperanza de que esta actividad a gran escala pueda disminuir el dióxido de carbono atmosférico, compensando el producido por la quema de combustibles fósiles.

El kril es el protagonista clave de este proceso, al recolectar las diminutas células de plancton que fijan el carbono gracias al rápido hundimiento de la materia orgánica que utiliza para alimentarse. La perspectiva es que en el futuro una flota de buques tanque circunvale el océano antártico inyectando hierro, con lo que un relativamente desconocido animal podría ayudar así a mantener automóviles y acondicionadores de aire funcionando.



</doc>
<doc id="6891" url="https://es.wikipedia.org/wiki?curid=6891" title="Número imaginario">
Número imaginario

En matemáticas, particularmente en álgebra, un número imaginario es un número complejo cuya parte real es igual a cero, por ejemplo: formula_1 es un número imaginario, así como formula_2 o formula_3 son también números imaginarios. En general un número imaginario es de la forma formula_4, donde formula_5 es un número real.

Los números imaginarios pueden expresarse como el producto de un número real por la "unidad imaginaria" i, en donde la letra i denota la raíz cuadrada de -1, es decir:

En raíz cuadrada los números imaginarios son el residuo de una raíz negativa, es decir:
i: la raíz cuadrada de -1,-2,-3,-4,etc.

El género de los números complejos/imaginarios los inventó Raffaelle Bombelli, un matemático e ingeniero italiano del siglo XVI. El término de "números imaginarios" fue creado por René Descartes, en su tratado "Geometría," en oposición a las teorías de Bombelli.

Fue en el año 1777 cuando Leonhard Euler le dio a formula_7 el nombre de i, por imaginario, de manera despectiva dando a entender que no tenían una existencia real. Gottfried Leibniz, en el siglo XVII, decía que formula_7 ""una especie de anfibios entre el ser y la nada"".

En ingeniería eléctrica y campos relacionados, la unidad imaginaria a menudo se indica con j para evitar la confusión con la intensidad de una corriente eléctrica, tradicionalmente denotada por i.


Geométricamente, los números imaginarios se representan en el eje vertical del plano complejo y por tanto perpendicular al eje real que es horizontal, el único elemento que comparten es el cero, ya que formula_9. Este eje vertical es llamado el "eje imaginario" y es denotado como formula_10, formula_11, o simplemente formula_12. En esta representación se tiene que:




En general, multiplicar por un número complejo es lo mismo que sufrir una rotación alrededor del origen por el argumento del número complejo, seguido de un redimensionamiento a escala por su magnitud.

Todo número imaginario puede ser escrito como formula_18 donde formula_19 es un número real e formula_13 es la unidad imaginaria.

Cada número complejo puede ser escrito unívocamente como una suma de un número real y un número imaginario, de esta forma:

Al número imaginario "i" se le denomina también constante imaginaria.

Estos números extienden el conjunto de los números reales formula_21 al conjunto de los números complejos formula_22.

Por otro lado, no podemos asumir que los números imaginarios tienen la propiedad, al igual que los números reales, de poder ser ordenados de acuerdo a su valor. Es decir, es correcto afirmar que formula_23, y que formula_24; esto se debe a que formula_25 y formula_26. Esta regla no aplica a los números imaginarios, debido a una simple demostración:

Recordemos que en los números reales, el producto de dos números reales, supónganse a y b, donde ambos son mayores que cero, es igual a un número mayor que cero. Por ejemplo es justo decir que formula_27, formula_28, por lo tanto, formula_29, entonces tenemos que formula_30, y obviamente formula_31.

Por otro lado, supóngase que formula_32, entonces tenemos que formula_33, lo cual evidentemente es falso.

Y de igual manera, hagamos la errónea suposición de que formula_34, pero si multiplicamos por formula_15 nos queda que formula_36. Por lo tanto tenemos que formula_37. Lo que es, igualmente que la suposición anterior, totalmente falso.

Concluiremos que esta suposición y cualquier otra de intentar dar un valor ordinal a los números imaginarios es completamente errónea.




</doc>
<doc id="6895" url="https://es.wikipedia.org/wiki?curid=6895" title="Mitología mexica">
Mitología mexica

La mitología mexica es una extensión complejo cultural mexica desde antes de la llegada de los aztecas al Valle de México, ya existían antiguos cultos al alma que ellos acuñaron en su afán de adquirir un rostro. Al asimilarlos también cambiaron sus propios dioses, tratando de colocarlos al mismo nivel de los antiguos dioses del panteón Nahua. De esta manera, elevaron sus dioses tutelares, Huitzilopochtli y Coatlicue, al nivel de las antiguas deidades, como Tláloc, Quetzalcóatl y Tezcatlipoca.

Dicho esto, existe un culto dominante sobre los demás dioses azteca el de su dios del Sol y la Guerra, Huitzilopochtli. Los aztecas se consideraban como el pueblo elegido por el Sol, encargados de garantizar su recorrido por el cielo, alimentándolo. Este sentimiento fue reforzado por la reforma social y religiosa de Tlacaélel bajo el reino de los emperadores Itzcóatl, Moctezuma I y Axayácatl a mitad del siglo XV. El mito de la creación del mundo de los aztecas expande esta idea. Las religiones prehispánicas se formaron a través de una lenta evolución y asimilación de los dioses prehispánicos, no tanto como seres de poder ilimitado, sino muchas veces como encarnaciones de las fuerzas de la naturaleza con personalidad humana, por ello varios estudios prefieren traducir el concepto prehispánico de Téotl como ‘señor’ y no como ‘dios’.

Los tlahtimines (sabios nahuas) trataron de dar un poco de orden a esta multitud de dioses, así tenemos en primer lugar a los dioses creadores, o Ipalnemohuani, palabra nahua que significa ‘aquel por quien se vive’ y dado que en náhuatl no existe el plural más que para los nombres de cosas, se ha especulado mucho sobre una posible tendencia monoteísta de los aztecas. Aunque esta interpretación puede estar originada por la influencia monoteísta occidental al no valorar la importancia en la cultura nahuatl del concepto de dualidad creadora. Los dioses creadores eran en primer lugar, Ometéotl ("ome:" ‘dos’; "teotl:" ‘dios’) el principio de la dualidad creadora que a su vez engendraba en sí mismo como origen y efecto a Ometecutli ("ome:" ‘dos’; "tecutli:" ‘señor’), elemento masculino de origen, y Omecihuatl ("ome:" ‘dos’; "cihuatl:" ‘señora’), elemento femenino de origen. A partir de ellos surgían cuatro elementos rectores principales:
Tezcatlipoca (señor del espejo negro) y Quetzalcóatl (serpiente emplumada), creadores del mundo,
Tláloc (señor del agua) y Ehécatl (señor del viento) proveedores de la lluvia y de la vida; otro nombre que se le daba a estos dioses es Tloque Nahuaque ("El inventor de sí mismo" o "El señor del cerca y junto"). La mayor parte de la poesía náhuatl que sobrevive, usa estos nombres para referirse a los dioses creadores.

Después estarían los dioses patronos, que eran los encargados de vigilar a cada pueblo. Según una antigua leyenda, cuando los grupos nahuas (las tribus nahuatlacas) salieron de Aztlan, cada una de ellas llevaba consigo su "bulto sagrado", que contenía las reliquias de su dios patrono. Huitzilopochtli era el dios patrono de los mexicas, pero ellos también respetaban los dioses de los otros pueblos. Junto al templo mayor construyeron un templo especial para los dioses patronos de todos los pueblos conquistados, de manera análoga al Panteón romano.

Existían así mismo, dioses dedicados a cada profesión y aspecto de la vida. Xipe Tótec, dios del reverdecimiento fue adoptado como el dios de los plateros, Nanahuatzin, de las enfermedades de la piel y la humildad, Tlazotéotl, diosa del amor físico y de las prostitutas, etc.

También existían algunos dioses de origen familiar pero se sabe poco de ellos.

La mayoría de estos dioses son anteriores a los aztecas o mexica y son compartidos por los demás pueblos nahua, u otros de las cercanías.

La leyenda de los cinco soles explica las creencias que tenían los aztecas en que otros mundos existían antes del suyo. De acuerdo con los aztecas hubo cuatro mundos anteriores o soles como ellos los llamaban, cada uno regido por un dios específico, una raza humana única y devastada por un fenómeno natural diferente. Cada uno de estos soles estaba ligado con los elementos básicos: tierra, agua, aire y fuego. Cada uno de estos elementos estaba relacionado no solo con la naturaleza y su composición sino también con su destrucción.

Hay varias versiones de este mito ya que la información no es completa y el orden suelen cambiar. Esta versión está basada en la Historia de los mexicanos por sus pinturas donde el orden de los soles es el primer sol, el segundo sol, el tercer sol, el cuarto sol y el quinto sol

Después de las devastaciones de los cuatro soles Quetzalcóatl y Tezcatlipoca son reconocidos por la recreación de la tierra y el cielo, no como enemigos sino como aliados.
Según el mito Azteca de la creación, Quetzalcóatl y Tezcatlipoca crean el cielo y la tierra desmembrando al monstruo de la tierra Tlaltecuhtli, que quiere decir señor de la tierra, a pesar de que en los textos se puede encontrar una descripción femenina de este monstruo. Se dice que Tlaltecuhtli se combinaba con otro monstruo, el gran caimán el cual con su espalda de cocodrilo le dio forma a las montañas del mundo. Este mito fue esparcido por todo México por lo que llegó a la cultura Maya de Yucatán.

Una de las versiones de este mito dice que Quetzalcóatl y Tezcatlipoca descendieron del cielo para observar a Tlaltecuhtli, al hacerlo vieron que su deseo por la carne fresca era tan grande que no solo poseía una fauces llenas de filosos dientes sino que también poseía dentaduras rechinantes en sus hombros, rodillas y otras articulaciones. Al ver esto los dioses concordaron en que la creación no podía ser completada mientras el monstruo estuviera de por medio. Entonces para crear la Tierra Quetzalcóatl y Tezcatlipoca se transformaron en grandes serpientes. Una de ellas tomó la mano izquierda y el pie derecho de Tlaltecuhtli, mientras que el otro la tomó por su mano derecha y su pie izquierdo y entre los dos desmembraron al monstruo. La parte superior del monstruo creó la tierra mientras que la parte inferior fue el cielo.

Este violento acto de desmembramiento al monstruo hizo enojar a los demás dioses por lo que decidieron que para consolar la tierra, todas las plantas que necesitará el hombre para vivir crecerían de ella, de sus cabellos crecieron los árboles, flores y hierbas y de su piel saldría pasto y pequeñas flores; sus ojos serían la fuente de los riachuelos, lagunas y pequeñas cuevas; su boca los grandes ríos y cavernas y su nariz sería la cresta de las montañas y valles. La leyenda dice que se podía escuchar el grito del monstruo en las noches sediento de sangre y por los corazones de la gente y esto solo podía ser calmado por medio de los sacrificios ofreciendo la carne y la sangre para calmar a Tlaltecuhtli y que siguiera dando los frutos necesitados para que la vida humana continuara.

Tezcatlipoca fue el primer sol en alumbrar el mundo y los otros dioses crearon a los gigantes, hombres muy fuertes, que comían bellotas de encinas. Tezcatlipoca fue el sol durante 676 años. Cuando Tezcatlipoca dejó de ser sol, todos los gigantes fueron comidos por jaguares y no quedó ninguno.
Tezcatlipoca dejó de ser sol porque Quetzalcóatl lo golpeó con un gran bastón y tiró al agua, de donde salió convertido en jaguar a comer a los gigantes. Este mundo desapareció por temblores y el hombre fue devorado por jaguares.

Quetzalcóatl fue el sol de la segunda edad, habitada por hombres-mono, que se alimentaban de piñones. Quetzalcóatl fue el sol durante 675 años, hasta que Tezcatlipoca lo derribó y levantó un fuerte viento que se llevó a Quetzalcóatl y los hombres-mono.

Tlalocatecutli, mejor conocido como Tlaloc "el que hace brotar", dios de la lluvia y del rayo fue el sol y duró 364 años. Durante esta edad los hombres-mono comían acicintli es lo que hoy conocemos como teocintle "simiente como de trigo que nace en el agua". Pero Quetzalcóatl hizo llover fuego del cielo, quitó a Tlaloc y fue sustituido por su mujer Chalchiuhtlicue.

Chalchiuhtlicue duró 312 años alumbrando a los hombre-pez, que en ese tiempo comían cincocopi, simiente como maíz. El último año que fue sol Chalchiuhtlicue llovió de tal manera que se cayeron los cielos y los hombre-pez fueron llevados por las aguas y se convirtieron en todos los géneros de peces que hay.

Los dioses decidieron que ya que había un nuevo mundo era necesaria la recreación del hombre para poblar la Tierra.(Taube, 2003, p.37-38)

Según el mito Quetzalcóatl debía ir al inframundo a recuperar los huesos humanos de la última era, es decir, la raza que fue convertida en pez por la inundación. El inframundo era un lugar peligroso conocido como Mictlan, gobernado por Mictlantecuhtli, señor del inframundo. El siguiente verso es la versión de Histoyre du mechique y de La leyenda de los soles. Una vez ahí Quetzalcóatl le pidió a Mictlantecuhtli y a su esposa, Mictecacíhuatl, la señora del inframundo los huesos de los ancestros:“… y entonces Quetzalcóatl fue a Mictlan. Se acercó a Mictlantecuhtli; y le dijo: "He venido por los huesos, los huesos preciosos, los huesos de jade", decía Quetzalcóatl. "¿Puedo con ellos poblar la tierra?"(Taube, 2003, p.37-38)Y Mictlantecuhtli le respondió: "Puedes quitarme lo que guardo con tanto cuidado con una condición - que desfilen cuatro veces alrededor de mi trono al soplar con esta concha”.
De mala gana el Señor de Mictlan, daba su consentimiento. Le entregaba a Quetzalcóatl una concha de caracol que no tenía agujeros para los dedos. Pero los gusanos aburridos creaban los agujeros y las abejas volaban en su interior para hacer el sonido. Quetzalcóatl tenía que actuar con rapidez para tomar los huesos.El Señor de Mictlan, finalmente dio la orden para que los huesos se recuperan, pero Quetzalcóatl pensaba que se trataba de un truco. En consecuencia Quetzalcóatl comenzó a correr. Entonces el señor de Mictlan ordenó que un pozo se excavara en el camino del dios que huía, cayendo en él. Quetzalcóatl revive eventualmente pero los huesos están rotos, y de ahí que haya seres humanos en todos los tamaños.Una vez más allá de la tierra muerta, junto con otros dioses, les roció con su propia sangre, restaurando la vida. Así, la humanidad ha nacido de la penitencia de los dioses.

Y este regalo tuvo que ser cancelado en la sangre del sacrificio. “¿Qué otra cosa podría ser, motivado a los antiguos, desde la muerte precedido a su reaparición, la muerte debe ser la causa de la Vida?”

Se dice que el nacimiento del quinto sol tuvo lugar en Teotihuacán, considerado el lugar donde el tiempo comienza. Después de la creación de la tierra, el hombre, su comida y sus bebidas, los dioses se reunieron en la obscuridad en Teotihuacán para decidir quién sería el nuevo sol“Se dice que cuando todo estaba en oscuridad, cuando no había sol que iluminara el amanecer ni el atardecer, los dioses convocaron una reunión entre ellos en Teotihuacán. Ahí preguntaron: ¡Dioses, vengan aquí!, ¿quién ha de llevar la carga?, ¿quién tomará sobre sí mismo el peso de ser el sol y traer el amanecer?Un dios arrogante llamado Tecuciztecatl se ofreció a ser voluntario rápidamente, sin embargo, los dioses decidieron elegir a un dios humilde de nombre Nanahuatzin (dios que partió la roca del sustento para conseguir el maíz) como segundo contendiente. Como todo un guerrero, acepta su deber y deuda a los otros dioses. Dos pirámides fueron alzadas para Tecuciztecatl y Nanahuatzin para ayunar y hacer penitencia, mientras se preparaba la pira sacrificial. Es lo que hoy conocemos como las pirámides del sol y de la luna.

La ofrenda que hizo Tecuciztecatl fue hecha de los más finos materiales. En vez de ramas de abeto llevaba plumas de quetzal, y bolas de oro en lugar de manojos de hierba atados. En lugar de las espinas de maguey con su sangre ofreció punzones de jade con punta de coral rojo, así como su incienso fue uno de los más finos y raros que había.

En cambio, los materiales de la ofrenda de Nanahuatzin fueron de poco valor; para sus ramas de abeto y sus manojos de hierbas usó haces de caña. Ofreció las espinas de maguey con su sangre, así como para el incienso quemó sus propias costras.

A media noche después de cuatro días de penitencia los dioses prepararon a Tecuciztecatl con grandes adornos mientras que Nanahuatzin vistió con simples vestimentas de papel. Entonces en los dioses formaron un círculo alrededor de la pira sacrificial que había estado ardiendo durante cuatro días. El primero en ser nombrado fue Tecuciztecatl, sin embargo al intentar saltar al fuego, el miedo lo paralizó. Esto sucedió tres veces hasta que los dioses decidieron llamar a Nanahuatzin, quien se echó al fuego sin dudarlo. Fue entonces que Tecuciztécatl, al ver el valor de Nanahuatzin, decidió aventarse, así como también se sacrificaron el águila y el jaguar. Es por eso que la punta de las plumas del águila son blancas y la piel del jaguar está manchada con las quemaduras del fuego. Después de la muerte de Nanahuatzin y Tecuciztécatl los dioses esperaron que alguno apareciera en el cielo, cuando de repente el cielo se empezó a llenar de luz. Los dioses empezaron a buscar el lugar por donde saldría Nanahuatzin y algunos supieron que saldría por el este surgiendo como Tonatiuh, el quinto sol

Los nahuas tenían varios mitos de la creación, resultado de la integración de distintas culturas. En uno de ellos, Tezcatlipoca y Quetzalcóatl se dan cuenta de que los dioses se sentían vacíos y necesitaban compañía. Por ello necesitaban crear la tierra. Existía solo un inmenso mar, donde vivía Tlaltecuhtli, el monstruo de la tierra. Para atraerlo, Tezcatlipoca ofreció su pie como carnada y el monstruo salió y se lo comió. Antes de que pudiera sumergirse, los dos dioses lo tomaron y lo estiraron para dar a la tierra su forma. Sus ojos se convirtieron en lagunas, sus lágrimas en ríos, sus orificios en cuevas. Después de ello, los dioses le dieron el don de la vegetación para confortar su dolor. Y posteriormente se dio a la tarea de crear a los primeros hombres.

Aun así, los dos astros siguen siendo inertes en el cielo y es indispensable alimentarlos para que se muevan. Entonces otros dioses deciden sacrificarse y dar el "agua preciosa" que es necesaria para crear la sangre. Por lo tanto, se obliga a los hombres a recrear eternamente el sacrificio divino original.

Conjuntamente llamados Ometeotl, son la pareja primigenia surgida de la sustancia o principio dual Omeyotl:















"Por orden alfabético véanse en la categoría


















</doc>
<doc id="6896" url="https://es.wikipedia.org/wiki?curid=6896" title="Mitología celta">
Mitología celta

La mitología celta es conocida por una serie de relatos de la religión de los celtas durante la Edad del Hierro. Al igual que otras culturas indoeuropeas durante este periodo, los primeros celtas mantuvieron una mitología politeísta y una estructura religiosa. Entre el pueblo celta hubo algunas tribus en estrecho contacto con Roma, como los galos y los celtíberos. Esta mitología no sobrevivió al Imperio romano, debido a su subsecuente conversión al cristianismo y a la pérdida de sus idiomas originales. Aunque irónicamente fue a través de fuentes romanas y cristianas, contemporáneas, que se conocen detalles sobre sus creencias.

En contraste, la comunidad celta que mantuvo sus identidades políticas o lingüísticas (tales como las tribus de escotos y bretones de las islas británicas) transmitió por lo menos vestigios remanentes de las mitologías de la Edad de Hierro, las cuales fueron registradas a menudo en forma escrita durante la Edad Media.

Debido a la escasez de fuentes sobrevivientes que pongan por escrito el idioma galo, se conjetura que los celtas paganos no eran extensamente alfabetizados. Aunque una forma escrita de la lengua gala - utilizando el alfabeto griego, latino e itálico del Norte - fue trabajada, según lo evidenciado, por los artículos votivos que llevan las inscripciones en lengua gala y el Calendario de Coligny). Julio César da testimonio del alfabetismo de los galos, pero también describe que sus sacerdotes, los druidas, prohibieron utilizar la escritura para registrar ciertos versos de importancia religiosa, haciendo notar también que los Helvecios tenían un censo escrito.

Roma introdujo el hábito más extendido de inscripciones públicas, y debilitó el poder de los druidas en los territorios que conquista. De hecho, la mayor parte de las inscripciones sobre deidades descubierta en Galia (Francia moderna), Britania y en otros lugares que representan territorios celtas posteriores a la conquista romana.

Aunque tempranamente los escotos de Irlanda y partes del moderno Gales usaron la escritura Ogam para registrar inscripciones cortas (en gran parte nombres personales), el alfabetismo más sofisticado no fue introducido a los territorios celtas. Estos no habían sido conquistados por Roma hasta el advenimiento del cristianismo; de hecho, muchos mitos gaélicos primero fueron registrados por monjes cristianos, aunque sin contar con gran parte de sus significados religiosos originales.

La fuente clásica sobre los dioses celtas de la Galia es la sección ""Commentarii de bello Gallico"" de Julio César (52-51 ac; La Guerra de las Galias). En este, él nombra a los cinco dioses principales adorados en la Galia (según la práctica de su tiempo, él nombra a los dioses con el equivalente romano más cercano) y describe sus roles. Mercurio era el más venerado de todas las deidades y numerosas representaciones de él han sido descubiertas. Mercurio es visto como el creador de todas las artes (y a menudo es tomado para referirse a Lugus por esta razón), protector de aventureros y comerciantes y el más poderoso en relación al comercio y el beneficio. Además los galos reverenciaban a Apolo, Marte, Júpiter, y Minerva. Las divinidades celtas son descritas con opiniones aproximadamente iguales a las de otros pueblos: Apolo disipa la enfermedad, Minerva anima habilidades, Júpiter gobierna los cielos, y Marte influye en la guerra. Además de estos cinco, él menciona que los Galos remontan su linaje a Dis Pater.

Debido a que César no describe a estos dioses por sus nombres celtas, sino por los nombres de las divinidades romanas (con los cuales los comparó), este proceso confunde considerablemente la tarea de identificar a estos dioses galos con sus nombres natales en las mitologías insulares. Además retrata por medio de un esquema ordenado la deidad y su rol en una forma que es bastante desconocida y poco familiar a la literatura coloquial de ese tiempo. De todos modos a pesar de las restricciones, su lista final es una observación provechosa y fundamentalmente exacta.

Equilibrando su descripción con la tradición oral, o incluso con la iconografía gala, se puede recolectar los distintos entornos y los roles de estos dioses. Los comentarios de César y la iconografía aluden a períodos bastante distintos en la historia de la religión gala. La iconografía, en los tiempos romanos, es parte de un ajuste de grandes acontecimientos sociales y políticos, y la religión que esto representa se pudo haber mostrado realmente menos obvia que la mantenida por los druidas (orden sacerdotal) durante el período de la autonomía gala de Roma.

Algunos estudiosos conluyen que las deidades celtas y los cultos relacionados eran locales y tribales y no pan-célticos. Los defensores de esta opinión citan la referencia de "Lucan", una divinidad llamada Teutates, que ellos traducen como "espíritu tribal" ("teuta" se cree, puede ser interpretado como "la tribu" en Proto-celta). Sin embargo, la serie evidente de nombres divinos, puede ser justificada de manera diferente: muchos pueden ser meros epítetos aplicados a dioses claves adorados en cultos pan-celtícos.

La mitología celta se encuentra en un número variado, pero relacionado, de subgrupos distintos ampliamente relacionados por las ramificaciones del idioma céltico: 

Aunque el mundo celta en su apogeo abarcara la mayor parte de Europa occidental y central, no estaba políticamente unificada, ni existía alguna fuente central sustancial de influencia cultural; por consiguiente, había mucha variación en las prácticas locales de la religión celta (aunque ciertos motivos, por ejemplo, la adoración al dios Lugh, parece haber difundido en todas partes del mundo Celta). Las inscripciones de más de trescientas deidades, que a menudo se comparan con su contraparte romana, han sobrevivido, pero de estas las más representadas parecen ser los "genii locorum", dioses locales o tribales, de los cuales solo unos pocos fueron extensamente adorados. Sin embargo, de lo que ha llegado a nuestros días de la mitología celta, es posible distinguir las concordancias que insinúan un panteón más unificado de lo que a menudo se cree.

La naturaleza y las funciones de estos dioses antiguos pueden ser deducidas de sus nombres, de la localización de sus inscripciones, su iconografía, y de las deidades romanas con las que han sido comparadas.

El "corpus mítico" de mayor antigüedad lo encontramos en los manuscritos correspondientes a la alta edad media de Irlanda, los cuales fueron escritos por cristianos, por lo que la naturaleza divina de sus dioses fue modificada.

El mito originario parece ser una guerra entre dos razas aparentemente divinas: los Tuatha Dé Danann, literalmente las "Tribus de la diosa Dana" que constituyen lo que se denomina los grandes dioses del panteón irlandés y los Fomoré, pueblo misterioso que aparece constantemente en la tradición irlandesa constituido por gigantes que viven en las islas que rodean Irlanda y que continuamente amenazan con invadirla sin llegar a concretarlo. Estas guerras entre ambas razas representan la base del texto "Cath Maige Tuireadh" (la Batalla de Mag Tuireadh), así como fragmentos de la gran construcción pseudohistórica "Leabhar Ghabhála Érenn" (Libro de la Invasión de Irlanda).

Los Tuatha Dé Dannan representan las funciones de la sociedad humana como la realeza, artes y guerra, mientras que los Fomoré representan la naturaleza salvaje y las fuerzas oscuras siempre dispuestas a llevar al caos a la sociedad humana y divina.

El dios supremo del panteón irlandés parece haber sido Dagda. Es Dios-druida y dios de los druidas, señor de los elementos y del conocimiento, jurista y temible guerrero. Durante la segunda batalla de "Mag Tured", llevó a los Tuatha Dé Danann a la victoria frente a los Fomoré. Se le denomina Dagda porque es el "dios bueno", no bueno en un sentido moral, sino bueno en todo. Ha sido llamado "Eochid" ("padre de todos"), "Lathir" ("padre poderoso")y "Ruadh Rofhessa" ("rojo de la gran ciencia"). Dagda es una figura-paterna, un protector de la tribu y el dios céltico básico del que otras deidades masculinas eran variantes. Los dioses célticos eran entidades mayormente no especializadas, o más bien cuyos campos de dominio abarcaban varias áreas. Al tratarse de un clan y de estar difundida esta idea ampliamente entre la comunidad como reflejo de la sociedad parece que actuaban como una comunidad humana repartiéndose las tareas. Aunque si se pudo dar el sincretismo romano con sus dioses esto se deba a que en la práctica sus campos de actuación estaban claramente acotados.

Debido al carácter particular de Dagda es una figura de la burla ridícula en la mitología irlandesa, algunos autores concluyen que él fue confiado para ser lo suficientemente benévolo (o ineficaz) para tolerar un chiste a sus expensas.

Los cuentos irlandeses retratan a Dagda como una figura de poder, fácil de distinguir por su extrema glotonería y desbordante sexualidad. Lleva un caldero cuyo contenido es inagotable, prototipo del Grial, y un arpa mágica que puede tocar, por sí sola, aires de lamento, de sueño, de muerte o de risa. Posee también, una maza; si golpea a alguien con uno de sus extremos, lo mata; si lo hace con el otro, lo resucita. Es, pues, el dios de la vida y de la muerte, absolutamente ambiguo y poseedor de fuerzas temibles que pueden ser buenas o malas. En Dorset existe una silueta famosa de un gigante itifálico conocido como el Gigante de Cerne Abbas mostrando una maza. Aunque éste fue realizado en tiempos romanos, durante bastante tiempo se ha pensado que representa a Dagda sin embargo, esto ha sido reconsiderado en el último tiempo, por los recientes estudios que muestran que pudo haber una representación de lo que parece ser una amplio paño que cuelga del brazo horizontal de la figura, llevando a la sospecha de que esta realmente representa a Hércules, (Heracles), con la piel del León de Nemea encima de su brazo y llevando la maza que utilizaba para matar. En Galia, se especula que Dagda se asocia con Sucellos, dios de la agricultura, los bosques y las bebidas alcohólicas, provisto de un martillo y una copa.

En los relatos épicos más recientes, así como en las novelas artúricas, el personaje de Dagda aparece a menudo con la forma de un "Hombre de los Bosques", un patán que lleva una maza y que es señor de los animales salvajes.

Balar, Balor o Bolar, fue un dios irlandés que pertenecía a la raza de los gigantes Fomoré. Poseía un ojo en la frente y otro en la parte posterior del cráneo, que era maligno y que habitualmente mantenía cerrado. Cuando lo abría, su mirada era mortal para aquel en quien la fijara. Se conoce principalmente por haber matado al rey de los Tuatha Dé Danann, Nuada, motivo por el que su nieto Lug le dio muerte.

Mórrígan, cuyo nombre significa literalmente "La reina de los fantasmas" era una diosa tripartita de la guerra de los celtas irlandeses antiguos que incitaba a los guerreros a combatir.

Colectivamente era conocida como "Morrigu", pero sus personalidades también eran llamadas; "Nemhain" (pánico), cuyo aspecto espantoso adoptaba solo cuando se presentaba ante los que iban a morir; "Macha" (batalla), que aparece bajo la forma de una hembra de cuervo y "Badb", cuyo nombre deriva del protocelta bodbh, "corneja", aspecto con que incitaba a los guerreros a la batalla. Ella es comúnmente conocida por estar involucrada en la Táin Bó Cúailnge, donde es al mismo tiempo una auxiliadora y un estorbo para el héroe Cúchulainn. A menudo se representa como un cuervo o corneja aunque podía adoptar muchas formas distintas (vaca, lobo o anguila).

La difusión extendida del dios Lug (aparentemente relacionado con la figura mitológica Lúgh en irlandés) en la religión céltica se sustenta por el gran número de lugares en los que aparece su nombre, extendiéndose por todo el mundo celta de Irlanda a Galia. Las más famosos de estas son las ciudades de "Lugdunum" (la ciudad francesa moderna de Lyon), "Lugdunum Batavorum" (la ciudad moderna de Leiden), "Lucus Augusta" (la actual ciudad de Lugo) y "Lucus Asturum (Lugo de Llanera)," además la raíz "Lug" está presente en todo el cantábrico, como ejemplo tenemos la tribu de los Astures de nombre "Lugones" (que da nombre a una localidad asturiana), Lugás que es una aldea en el oeste de Asturias o el término "lugas" que en las tierras interiores de Cantabria se refiere a los rayos de sol que se cuelan entre las nubes.

Lug es descrito en los mitos célticos como un allegado a la lista de deidades, y normalmente se describe teniendo la apariencia de un hombre joven. Aunque es el dios más importante de la mitología irlandesa, no es el dios supremo, sino el "dios sin función" porque las tiene todas. Lug pertenece a los Tuatha Dé Danann por su padre, pero a los Fomoré por su madre y en la segunda batalla de "Mag Tured", se impone como caudillo de los Tuatha dé Danann y los conduce a la victoria, matando a su propio abuelo "Balar", el del ojo pernicioso. Su nombre proviene de una palabra indoeuropea que significa "blanco", "luminoso", pero también "cuervo", por lo que este animal parece estar vinculado de alguna forma con él. Posee un aspecto solar, pero no es un dios del sol, pues esta función era femenina entre los celtas. Sus armas eran la jabalina y la honda, y en Irlanda una fiesta, Lughnasa (irlandés moderno "lúnasa") se conmemora en su honor.

Brigid (o Brigit), gran diosa irlandesa del fuego y la poesía. Se la considera hija de Dagda y pertenece a los Tuatha Dé Danann. Su nombre proviene de un radical que significa "altura", "eminencia", lo que señala su preeminencia. Aparece en la tradición irlandesa con distintos nombres, que simbolizan las funciones sociales que se le atribuyen, esquemáticamente ella es triple, pertenece a las tres clases de la sociedad indoeuropea; diosa de la inspiración y de la poesía (clase sacerdotal), protectora de los reyes y guerreros (clase guerrera) y diosa de las técnicas (clase de los artesanos, pastores y labradores).

Diosas de la naturaleza como Epona, diosa Gala o Galo-Romana, de los caballos. Se trata de la imagen de una antigua diosa-yegua cuyo nombre proviene del galo ("epo" que equivale a caballo, que corresponde al ""hippos"" griego y al ""equus"" latino), además de "Tailtiu" y "Macha".

Los dioses masculinos incluyen a Goibniu, el dios herrero de los Tuatha Dé Danann. Es el señor de los artesanos, forja las armas de los guerreros y preside un extraño festín de inmortalidad, en el que los dioses se regeneran comiéndose los "cochinos mágicos" de Manannán mac Lir. El nombre de Goibniu deriva del nombre "herrero" en celta.

Dian Cecht, dios de la medicina en la tradición irlandesa. Participa en la batalla "Mag Tured" y abre una "fuente de salud" en la que mezcla numerosas hierbas que le permiten devolver la vida a los guerreros heridos o muertos

Angus, dios irlandés del amor, de sobrenombre "Mac Oc" (joven hijo). Hijo de Dagda e hijo adoptivo de Manannan. Posee un manto de invisibilidad con el que envuelve a quienes quiere proteger.

Los dioses de la Britania Prehistórica, también oscurecidos por siglos de cristiandad, llegan a nosotros por los manuscritos de Gales. Aquí existen dos grupos de linajes de dioses; los niños de Dôn y los niños de Llyr, aunque cualquier distinción de función entre los dos grupos no está clara. Dôn, también conocida como Anna, Anu, Ana o Dana es la Diosa-madre de los antiguos celtas. En Irlanda, es la madre de los dioses, los famosos Tuatha Dé Danann. Se trata de una divinidad indoeuropea arcaica, conocida en la India con el nombre de "Anna Purna" (Ana la que provee) y en Roma como "Anna Parenna". Es más que probable que este personaje divino fuera cristianizado bajo la figura de "Santa Ana", madre de la Virgen María. Por otro lado Llyr es padre de un linaje de dioses, entre ellos Manannan, en la tradición irlandesa. Es una divinidad vinculada al mar, pero no es un dios del mar.

Los celtas de la Galia rindieron culto a varias deidades de las que nosotros conocemos poco más que sus nombres. El escritor romano Lucano (siglo I) menciona los dioses Taranis, Teutates y Esus ("Dioses de la noche"), pero existe muy poca evidencia de que éstos fueran deidades célticas importantes. Algunas de estos dioses y diosas pueden haber sido variantes de otros; Epona, por ejemplo, puede haberse convertido en la heroína Rhiannon en Gales, y Macha a quien se le rendía culto principalmente en Ulster. Los pueblos politeístas raramente cuidan y mantienen sus panteones en un orden aseado y ordenado en que a los investigadores les gustaría encontrarlos. Algunas de estas son:

Cernunnos (El Astado), es evidentemente de gran antigüedad, pero nosotros conocemos muy poco de él. Probablemente es él quién aparece realzado en el famoso caldero de plata encontrado en Gundestrup, Dinamarca que data de 1 o 2 siglos adC. Se cree que es el dios de la abundancia y amo de los animales salvajes. Su naturaleza es esencialmente terrenal. Se le representa mayor, tiene las orejas y los cuernos de un ciervo y lleva un "torque", especie de collar galo. Está a menudo acompañado por una serpiente con cabeza de carnero. Aparece como el amo de los animales salvajes, terrestres y acuáticos. Sin duda manifiesta la fuerza, el poder y la perennidad (simbolizada por el ramaje). Se le representa como el donador de un altar con un cesto de vituallas, pasteles y monedas.

Belenus era una deidad regional a la que se le rendia culto principalmente en el norte de Italia y en la costa de la Galia mediterránea. Él era principalmente un dios de agricultura. Una gran fiesta llamada Beltaine es asociada con él. Algunos todavía debaten si él realmente era en absoluto una deidad. Su nombre significa "luminoso y brillante" y algunos creen que 'él' simplemente representa las grandes hogueras de la fiesta de Beltaine. Coincidiendo con esta idea al topónimo asturiano Beleñu proveniente del céltico Belenus, se le añadió el de San Xuan, por ser este el día de la celebración del solsticio de verano en el que se hacen las hogueras coincidente con el día de Beltaine.
Dios guerrero y protector de las tribus. Se le identifica como el Marte romano y Dagda de los irlandeses. Formaba parte de los "dioses de la noche" junto a Esus y Taranis, siendo un dios que recibía muchos sacrificios por parte de los druidas. Se le adoraba sobre todo en la Galia y en la Bretaña romana.

Dios del trueno, de la tormenta y el cielo. Era un dios temido, cuyo culto se extendía por la Galia y parte de Bretaña. En particular, su adoración era muy parecida a la de Teutates, ya que para aplacar su ira se le dedicaban sacrificios y era miembro de la tríada formada por estos dos más Esus. Se le relaciona con Thor, por su similitud con los poderes del rayo y el trueno, y los romanos le identificaban con Júpiter.

Dios sanguinario, señor de los bosques. Agrupado por Lucano junto a Teutates y Taranis como dioses principales de los galos. Recibía sacrificios debido al temor por ser un dios salvaje y ávido de sangre.

Manannan (o Mannawydan) ab Llyr (hijo de Llyr), personaje mitológico irlandés. Es un integrante de los Tuatha Dé Dannann. Es un poderoso mago, dueño de un casco flameante que encandila a sus enemigos, una coraza invulnerable, un manto de invisibilidad, una nave que surca el mar sin remos ni velas y una espada llamada Fragarach que entre sus muchas cualidades es capaz de cortar cualquier armadura y controlar el viento. Nativo de la Isla de Man, que toma su nombre de él; allí aún pueden verse las ruinas de las que se supone su gigantesca tumba, cerca del castillo de Peel.

Representa a la elocuencia, es un anciano todo arrugado, vestido con una piel de león; lleva maza, arco y carcaj. Tira de multitudes de hombres atados por las orejas con una cadenilla de oro cuya extremidad pasa por la lengua agujereada del dios. Ogmios es la elocuencia segura de su poder, el dios que, a través de la magia, atrae a sus fieles. Es también símbolo del poder de la palabra ritual que une el mundo de los hombres con el mundo de los dioses. En su nombre se profieren las bendiciones a favor de los amigos y las maldiciones contra los enemigos.

En Irlanda le llaman "Ogma". Es el inventor del "ogham", conjunto de signos mágicos cuya fuerza es tan grande que puede paralizar al adversario. También es un guerrero que participa eficazmente en la batalla "Mag Tured".

Su nombre significa "brazo de plata" y pertenece a los Tuatha Dé Danann. En el transcurso de la primera batalla de "Mag Tured", perdió un brazo y no podía reinar, el dios Diancecht le fabrica una mano de plata y así asume nuevamente la función de rey y conduce a los Tuatha Dé Danann en la segunda batalla de "Mag Tured".

Heroína galesa, su nombre proviene de "Rigantona": la gran reina. Aparece como una amazona y escoge a "Pwyll" como esposo. Su hijo "Pryderi" le es arrebatado al momento de nacer y es acusada de haber hecho desaparecer al niño, es condenada a llevar sobre su espalda a todos los visitantes que vayan a la fortaleza de su esposo.

Es uno de los héroes más famosos de la tradición galesa. Es hijo de Dana y padre de Lleu Llaw Gyffes. Posiblemente su nombre puede significar "sabio". Representa el poder mágico heredado de los antiguos druidas.

Es el guerrero y mago, es el hijo de Cumail y el padre de Ossian. Temible guerrero, venga a su padre muerto en combate y reconstituye la tropa de los "Fiana". Su nombre significa 
"Blanco, hermoso, rubio y de buena raza". Poeta y mago, conoce los doce libros de poesía y posee el don de la iluminación cuando se mordisquea el pulgar.

Es el personaje más famoso de la epopeya irlandesa. Algunas versiones de su leyenda pretenden que es hijo del propio dios Lug. De verdadero nombre Setanta (donde reconocemos el nombre británico del camino), obtiene su sobrenombre de Cu-Chulainn (perro de Culann) después de matar al perro de los ulates, Culann, y prometer que lo reemplazaría como protector. Su furia guerrera es tal que es capaz de contorsiones inverosímiles, con las cuales deforma completamente su cuerpo, lo que acentúa su aspecto sobrehumano y hace de él un ser ciclópeo. De su cabeza emana la "Luz de Héroe", signo de los semidioses y de personajes inspirados por la divinidad.

Cuchulainn es un "héroe de luz", un héroe civilizador, personificación de la sociedad a la que pertenece, pero a la que él confiere un carácter divino. Representa, también, una especie de culto solar masculino (no existe un dios solar entre los celtas).

Arturo o "Artús" es el personaje más importante de la tradición celta. Originariamente no era más -históricamente- que un modesto caudillo guerrero, un jefe de jinetes que alquilaban en cierto modo sus servicios a los reyes insulares britones hacia el año 500 de nuestra era, en la lucha desesperada que sostenían contra los invasores sajones. Sus éxitos fueron tales, que la leyenda se adueñó del personaje, exagerando notablemente su papel y su poder, y confiriéndole una dimensión mitológica. Así es como Arturo, cuyo nombre (en realidad, sobrenombre) significa “que tiene el aspecto de un oso”, o “hijo del oso guerrero” (del irlandés antiguo "Arto", “oso”; "rīg", “rey guerrero”, que a su vez podría ser la etimología de la "gens" romana "Arturius"), adquirió todas las características de una divinidad de la tradición celta.

Otros mitos, de origen celta, vinieron a añadirse al esquema primitivo, y Arturo se convirtió en el símbolo de un mundo celta ideal que funciona en torno a un eje constituido por el rey. Pero este rey solo tiene poder en la medida en que está presente, aunque sea sin actuar. Arturo y Merlín forman la famosa pareja rey-druida sin la que ninguna sociedad celta puede existir. Su padre fue el rey Uther Pendragon, que con la magia de Merlín engendró un hijo, Arturo, con la esposa del duque de Cornualles. La mujer, que se llamaba Igraine, ya le había dado dos hijas al duque: Morgause, que sería la esposa del rey Lot de Lothian y madre de sir Gawain, y Morgana, que aprendió las artes mágicas de Merlín y fue llamada "Le Fay" o "El Hada". Sucede a su padre al arrancar la espada Excalibur de la roca en la que estaba clavada.

En todas las novelas de la Mesa Redonda, Arturo se distingue por cierta pasividad. Son sus caballeros quienes actúan en su nombre, y en el de la reina Ginebra, que es quien ejerce la soberanía. Ginebra ama a Lanzarote del Lago, mejor caballero de Arturo e hijo adoptivo de la Dama del Lago. El reino se divide en dos cuando todos se enteran del adulterio gracias al Hada Morgana. Mordred (hijo incestuoso de Arturo y Morgana) mata a Arturo en una batalla, y Morgana lo lleva a Ávalon para cuidar de él y enterrarlo.

Uno de los personajes más conocidos de la leyenda artúrica. Merlín tuvo una existencia real, setenta años después del Arturo histórico. Fue un reyezuelo de los bretones del norte, en la Baja Escocia, el cual, habiendo perdido el juicio a consecuencia de una batalla, se refugió en un bosque y se puso a profetizar. La leyenda se apoderó del personaje, y diversos elementos mitológicos vinieron a cristalizar sobre el mismo. Se encuentra en él el mito del loco inspirado por la divinidad, el del "hombre salvaje", señor de los animales y equilibrador de la naturaleza, el del niño que acaba de nacer y que revela el porvenir, y el del mago.

En la leyenda elaborada, Merlín es hijo de un demonio íncubo llamado Belial el Bestial, lo que explica sus poderes. Se opone al rey usurpador Vortigern, sirve y aconseja a Aurelio Ambrosio (Emrys Gwledig) y se convierte en consejero permanente y mago titular de Uther Péndragon. Hace que éste engendre a Arturo, obliga a reconocer a Arturo como rey de los bretones, le aconseja y ayuda en sus empresas, y establece la Mesa Redonda. Acaba sus días en el bosque de Broceliande junto a su amada Nimue, la Dama del Lago.

Para los primeros celtas algunos árboles eran considerados sagrados. 
La importancia de los árboles en la religión celta es mostrada por el hecho de que muchos nombres de la tribu Eburonian contienen alguna referencia al árbol del tejo, mientras que nombres como Mac Cuilinn (hijo del acebo) y Mac Ibar (hijo del tejo) aparecen en los mitos irlandeses.

Los escritores romanos declararon que los celtas practicaron el sacrificio humano en gran escala lo que es apoyado periféricamente por fuentes irlandesas; sin embargo, la mayoría de esta información es de segunda mano y se basa en rumores. 
Existen muy pocos hallazgos arqueológicos que prueben el proceso sacrificatorio por lo que la mayoría de los historiadores contemporáneos tiende a considerar el sacrificio humano como raro dentro de las culturas célticas.

Existía también un culto al guerrero que se centraba en las cabezas cortadas de sus enemigos. 
Los celtas proporcionaban a los muertos las armas y otros equipos que indicarían que ellos creían en otra vida posterior a la muerte. 
Antes del entierro, ellos cortaban también la cabeza de la persona muerta y estrellaban el cráneo, quizás para prevenir que vagara como fantasma.

A menudo se dice que los pueblos celtas no construyeron ningún templo, y que sólo se rendía culto en el exterior, en los bosquecillos de árboles. La arqueología ha mostrado por un largo tiempo que esto es falso, con el descubrimiento de varias estructuras de templos a lo largo del mundo celta conocido. Con la conquista romana de partes del mundo celta, un tipo distinto de templo celta-romano denominado "fanum " también se desarrolló.

Los druidas, quiénes han sido romantizados en la literatura moderna, fueron la gran clase hereditaria de sacerdotes responsables de transmitir y practicar las tradiciones mitológicas y religiosas de los pueblos célticos. El papel del druida puede compararse a la casta hindú de los brahmanes o al mago iraní, y como ellos se especializaron en las prácticas de magia, sacrificio y augurio. Debido a las similitudes entre estas clases y entre las ramas divergentes de descendientes de culturas indoeuropeas, se ha propuesto que los orígenes serían una clase similar entre los proto-indoeuropeos.

Los druidas eran particularmente asociados al roble y al muérdago (hierba parasítica que normalmente crece en estos árboles), y se cree que ellos utilizaban este último para preparar medicinas o pociones alucinógenas. Para ayudar a entender el significado, la palabra druida (galés " derwydd ") se cree a menudo que viene de la raíz de la palabra que significa "roble" (galés "derw "), aunque probablemente esta raíz proto-indo-europea puede haber tenido el significado general de solidez.

Los bardos, por otro lado, eran aquellos que relataban por medio de cantos las historias que recordaban los hechos de los guerreros tribales famosos así como las genealogías e historias de las familias de los estratos gobernantes entre las sociedades celtas.

La cultura céltica era anterior al contacto con las civilizaciones mediterráneas, por tanto no histórica, por lo que no dejaban su historia por escrito. Sin embargo, los pueblos celtas mantuvieron a menudo una intrincada historia hablada comprometida con la memoria y transmitida por los bardos. Similar a otras culturas pre-instruidas (vea, por ejemplo, los Vedas de India, los cuales fueron transmitidos por siglos solamente por la memorización de un formulario arcaico en Sánscrito que no se había hablado como lengua vernácula por cientos de años), los bardos facilitaron la memorización de tales materiales a través del uso de la métrica y rima poética.

Pudo haber existido además una clase de "videntes" o "profetas", los "Strabo", de una palabra celta cuyo significado es "inspirado" o "extático". Es por consiguiente posible que la sociedad celta tuviera, además de la ritualística y taumatúrgica religión de los druidas, un elemento del chamanismo de comunicación extática con el mundo de los muertos.

Diodoro comenta sobre la importancia de profetas en el ritual druídico: Estos hombres predicen el futuro observando el vuelo y llamadas de los pájaros y por el sacrificio de animales sagrados: todas las ramas de la sociedad están en su poder, además en materias muy importantes ellos preparan a una víctima humana, a la que le clavan una daga en el pecho; observando la manera en que sus miembros convulsionan cuando él cae y el fluir de su sangre, de esta forma ellos pueden leer el futuro. Estos comentarios greco-romanos se apoyan de alguna manera en las excavaciones arqueológicas. En Ribemont en Picardy, Francia, se encontraron agujeros llenos de huesos humanos y huesos de muslo deliberadamente arreglados en modelos rectangulares. Se cree que esta urna fue llevada a tierra por Julio César mientras estaba dominando la Galia. En un pantano en Lindow, Cheshire, Inglaterra se descubrió un cuerpo que también puede haber sido la víctima de un ritual druídico. El cuerpo está ahora en exposición en el Museo Británico, Londres.

Las principales festividades eran trimestrales y estaban relacionadas con las estaciones y la fertilidad agropecuaria.

Las creencias y conductas célticas tribales han tenido un gran impacto en las culturas célticas modernas. La mitología basada (aunque, no idéntica) en la religión pre-cristiana, fue de conocimiento común para la cultura céltica y trasmitida oralmente hasta el día de hoy, aunque en la actualidad este menguando. Varios rituales que involucran actos de peregrinación a los sitios como las colinas y pozos sagrados que se cree tienen poder curativo o propiedades beneficiosas todavía se llevan a cabo.. Basado en la evidencia del continente europeo, la adoración de varias figuras que son ampliamente conocidas por el folclóre de los territorios celtas de hoy o que toman parte en la mitología posterior al cristianismo se han descubierto en áreas en las cuales no existen registros antes de la cristiandad. 
Algunos de éstos son: 
Las diferencias en los nombres son consecuencia de la desviación de los idiomas entre los distintos grupos.







</doc>
<doc id="6897" url="https://es.wikipedia.org/wiki?curid=6897" title="Mitología maya">
Mitología maya

Se refiere a las extensivas creencias politeístas de la civilización maya precolombina. Esta cultura mesoamericana siguió con las tradiciones de su religión desde hace 3000 años, e incluso algunas de estas tradiciones continúan siendo contadas como historias fantásticas por los mayas modernos.
En general, los textos mayas originales fueron quemados por los españoles durante la conquista de América. Por lo tanto, el conocimiento de la mitología maya disponible en la actualidad es muy limitado.

Cuando llegaban tormentas terremotos y desastres de agua era la culpa de el dios del fuego Huracán

Tal y como sucede en otras mitologías, los mayas usaban este tipo de relatos para dar explicación a los fenómenos naturales, la diferencia puede ubicarse más bien en sus complejas ceremonias para rendir tributo y en la idea de considerar que el universo cumplía una regeneración cíclica que incluía cataclismos y catástrofes.

La historia maya de la creación de los quiché es el "Popol Vuh". En este se describe la creación del mundo a partir de la nada por la voluntad del panteón maya de dioses. El hombre fue creado del barro sin mucho éxito, posteriormente se crea al hombre a partir de madera con resultados igualmente infructuosos, después de los dos fracasos se crea el hombre en un tercer intento, esta ocasión a partir del maíz y se le asignan tareas que elogiaron a dioses: cortador de gemas, tallador de piedras y otros. Algunos creen que los mayas no apreciaban el arte por sí mismo, sino que todos sus trabajos eran para exaltación de los dioses.

Después de la historia de la creación, el "Popol Vuh" narra las aventuras de los héroes gemelos legendarios, Hunahpú e Ixbalanqué, que consistieron en derrotar a los señores de Xibalbá, del mundo terrenal. Estos son dos puntos focales de la mitología maya y a menudo se encontraron representados en arte maya.

Cuenta el Popol Vuh que los gemelos Hunahpú tenían la costumbre de jugar a la pelota (un juego sagrado) en un campo sobre el Xibalbá o reino de los muertos y eso causaba gran molestia a los señores del Xibalbá. Furiosos, planearon la muerte de los gemelos, y los retaron a una partida del juego en su territorio, finalmente cometieron una equivocación Por ello, fueron sacificados y enterrados, y la cabeza de uno de ellos fue cortada y colgada sobre un árbol seco. Tiempo después, una doncella del inframundo pasó cerca del árbol, y la cabeza le escupió. Ella quedó embarazada y dio a luz a los gemelos Hunahpú e Ixbalanque.

Estos gemelos fueron tratados como esclavos por sus medio hermanos, hasta que un día decidieron que estaban hartos: hicieron que subieran a un árbol, y, con ayuda de la magia, alargaron el tronco hasta dejarlos a una gran altura y convirtiéndolos en monos.

Los hermanos vivieron muchas aventuras donde demostraron su capacidad y poder. Decidieron que iban a vengar la muerte de su padre y de su tío, y para ello trazaron un plan contra los señores del Xibalbá. Aprendieron a jugar a la pelota, y lo hicieron en el mismo campo donde habían jugado sus antepasados, haciendo que los señores del inframundo se pusiesen furiosos. Otra vez, los señores del Xibalbá decidieron retar a los gemelos. Ellos aceptan, pero evitaron las tretas y no se equivocaron de camino.

Los hermanos fueron retados una vez más: deben saltar un gran foso en llamas. Ellos aceptan, y caen en el intento. Los señores del Xibalbá trituraron sus huesos y lanzaron sus cenizas al río, pero se depositaron en una de las orillas y de ellas volvieron a surgir los gemelos. Días después, volvieron disfrazados al Xibalbá y lograron dominar allí a todos sus habitantes, a quienes dejaron vivir a cambio de que renunciasen a todo su poder para hacer el mal.

Por último, Hunahpú e Ixbalanqué se convirtieron en deidades, en la luna y el sol.

En la mitología maya, Tepeu y Kukulkán (Quetzalcóatl para los aztecas) son referidos como los creadores, los fabricantes, y los antepasados. Eran dos de los primeros seres a existir y se dice que fueron tan sabios como antiguos. Huracán, o el ‘corazón del cielo’, también existió y se le da menos personificación. Él actúa más como una tormenta, de la cual él es el dios.
Tepeu y Kukulkán llevan a cabo una conferencia y deciden que, para preservar su herencia, deben crear una raza de seres que puedan adorarlos. Huracán realiza el proceso de creación mientras que Tepeu y Kukulkán dirigen el proceso. La Tierra es creada, junto con los animales. El hombre es creado primero de fango pero este se deshace. Convocan a otros dioses y crean al hombre a partir de la madera, pero este no posee ninguna alma. Finalmente el hombre es creado solamente a partir del maíz.

Estos realizaron el primer intento de la creación del hombre a partir del fango, sin embargo pronto vieron que sus esfuerzos desembocaron en el fracaso, ya que sus creaciones no se sostenían por ser el material muy blando.

Estos dioses que realizaron el segundo intento de crear al hombre a partir de la madera, pero este no poseía ninguna alma

Estos realizaron el tercer intento de construir la humanidad a partir de maíz y finalmente lograron obtener éxito donde los otros dioses habían fracasado.

En los diferentes pueblos tiene papeles importantes como Madre, Diosa, Abuela, Inspiradora, Creadora y Consejera. Se le representa como una anciana.

Se pueden encontrar referencias a los Bacabs en los escritos del historiador del Siglo XVI Diego de Landa y en las historias mayas coleccionadas en el Chilam Balam. En algún momento, los hermanos se relacionaron con la figura de Chac, el dios maya de la lluvia. En Yucatán, Chan Kom se refiere a los cuatro pilares del cielo como los cuatro Chacs. También se cree que fueron dioses jaguar, y que están relacionados con la apicultura. Como muchos otros dioses, los Bacabs eran importantes en las ceremonias de adivinamiento, y se les hacían preguntas sobre los granos, el clima y la salud de las abejas.

Xibalbá es el peligroso inframundo habitado por los señores de la enfermedad y la muerte de la mitología maya. Se decía que el camino hacia esta tierra estaba plagado de peligros, era escarpado, espinoso y prohibido para los extraños. Este lugar era gobernado por los señores demoníacos Vucub-Camé y Hun-Camé. Los habitantes de Xibalbá eran cuatro:







</doc>
<doc id="6899" url="https://es.wikipedia.org/wiki?curid=6899" title="Manihot esculenta">
Manihot esculenta

Manihot esculenta, llamado comúnmente yuca, aipim, mandioca, guacamota, casabe, casava o lumu es un arbusto perenne de la familia de las euforbiáceas extensamente cultivado en América, África y Oceanía por sus raíces con almidones de alto valor alimentario. 

La yuca o mandioca es originaria del centro de América del Sur, se ha cultivado en la mayor parte de las zonas tropicales y subtropicales del continente americano. También se introdujo con gran éxito en zonas africanas de similares condiciones climáticas, y aunque se estima que las variedades hoy conocidas son efecto de la selección artificial, hay variedades generadas por el aislamiento geográfico de la selva (casabe, que es altamente venenosa) o la de los altiplanos (yuca, mínimamente venenosa).

Yuca es una palabra de origen taino en cambio mandioca, deriva del guarani mandiog.

La mandioca es un arbusto perenne que alcanza los dos metros de altura. Está adaptada a condiciones de la zona intertropical, por lo que no resiste las heladas. Requiere altos niveles de humedad —aunque no anegamiento— y de sol para crecer. 

Se reproduce mejor por esquejes que por semilla en las variedades actualmente cultivadas. El crecimiento es lento en los primeros meses, por lo que el control de hierbas es esencial para un correcto desarrollo. En su uso normal, la planta entera se desarraiga al año de edad para extraer las raíces comestibles; si alcanza mayor edad, la raíz se endurece hasta la incomestibilidad. De las plantas desarraigadas se extraen los esquejes para la replantación.

La raíz de la mandioca es cilíndrica y oblonga, y alcanza el metro de largo y los 10 cm de diámetro. La cáscara es dura, leñosa e incomestible. La pulpa es firme —incluso dura— antes de la cocción, surcada por fibras longitudinales más rígidas; muy rica en hidratos de carbono y azúcares, se oxida rápidamente una vez desprovista de la corteza. Según la variedad, puede ser blanca o amarillenta.

La evidencia más antigua del cultivo de la mandioca proviene de datos arqueológicos que indican que hace 4000 años y fue uno de los primeros cultivos domesticados en América. 

Las siguientes referencias al cultivo de yuca provienen de la cultura maya, hace 1400 años en Joya de Cerén (El Salvador). En efecto, recientes investigaciones tienden a demostrar que el complemento alimentario de los mayas, el que les permitió sostener poblaciones muy numerosas, sobre todo durante el periodo clásico, y muy particularmente en la región sur de Mesoamérica en donde se concentraron importantes multitudes (Tikal, Copán, Calakmul), fue la mandioca, una raíz con alto contenido calórico del que se prepara una harina muy nutritiva, en forma de torta redonda, llamada "casabe", que hasta la fecha es parte importante de la dieta en las diversas poblaciones que viven en la región maya y también en la cuenca del mar Caribe, en especial en la República Dominicana, Venezuela y Cuba. 

Una variante se originó posiblemente más al sur, en una zona que actualmente corresponde a la zonas vecinas entre Argentina, Brasil, Bolivia y Paraguay. Con su mayor potencial alimenticio, se había convertido en un alimento básico de las poblaciones nativas del norte de Sudamérica, sur de América central, y las islas del Caribe en la época de la llegada de los españoles, y su cultivo fue continuado con los portugueses y españoles. Del extracto líquido se logra el almidón para planchar las ropas. Las formas modernas de las especies domesticadas pueden seguir creciendo en el sur de Brasil.

En Paraguay y el Noreste de Argentina actualmente la mandioca es una de las especies más consumidas por los habitantes (sobre todo en las zonas rurales, donde su consumo per cápita es uno de los más elevados del mundo), y puede estar presente en la mayoría de las comidas del día (en el desayuno, media mañana, almuerzo y cena), sea hervida, frita o en platillos a base de su almidón. Asimismo, en muchos hogares acompaña todos los días a la comida principal (función similar al que en otras partes cumple el pan), y alimenta al ganado bovino. En este país se cultivan como 300 variedades de la misma. Los paraguayos la llaman principalmente por su nombre en guaraní, "mandi'ó". 

Aunque hay unas cuantas especies salvajes de "mandioca", las variedades de "Manihot esculenta" son seleccionadas por el ser humano para la agricultura.

La producción mundial de la mandioca está estimada en 184 millones de toneladas en 2002, la mayoría de la producción se encuentra en África, donde crecen 99,1 millones de toneladas, 51,5 en Asia y 33,2 en América Latina.

En muchos lugares de América, la mandioca es el alimento básico. Esto se traduce en el abundante uso de imágenes de la mandioca usados en el arte de Perú por la gente de la cultura Moche quienes la representan a menudo en su cerámica.

Al parecer lo que representaban los mochicas fue la Arracacia xanthorrhiza, basados en los mínimos detalles característicos del arte de la cerámica Moche.

La presencia de elementos cianogénicos, como por ejemplo la linamarina en la raíz, hace que la misma sea inutilizable y venenosa en algunas variedades, sin una prolongada cocción, necesaria además para reducir la rigidez de la pulpa. Aunque la variedad llamada "Manihot aipi" (considerada a veces una subespecie de "M. esculenta") contiene concentraciones elevadas de elementos venenosos, estos desaparecen al hervirla.

Alternativamente, la raíz puede rallarse en crudo, tras lo cual es prensada para extraer el jugo potencialmente tóxico (que contiene ácido cianhídrico - HCN). Una vez secada al fuego o al sol, se muele para obtener una harina fina y delicada de la que se obtiene, por sedimentación, el almidón de mandioca y de este se obtiene la tapioca, también llamada casabe. Mediante este procedimiento se hacen comestibles incluso las variedades "amargas" que tienen alto contenido de toxinas. Ciertas culturas africanas maceran la raíz en agua hasta su fermentación para eliminar las toxinas antes de secarla y molerla.

La raíz fresca debe consumirse en un plazo breve, ya que debido a su alto contenido de almidones se descompone rápidamente por la acción de diversos microorganismos. Congelada o envasada al vacío se conserva durante meses en buen estado.

La mandioca se utiliza extensamente en la cocina latinoamericana. Las variedades "dulces" se consumen ampliamente hervidas, o fritas como sustituto de las papas.

Para su preparación en alimentos, la mandioca se somete a varios procesos de escaldado, ebullición, y/o fermentación Ejemplos de productos alimenticios a base de yuca incluyen al "garri" (tostadas las raíces de la yuca), "fufu" (similar a la avena para desayuno), la masa de agbelima y la harina de yuca.

Es consumida tradicionalmente desde época precolombina en lo que hoy es el Noreste argentino. También se usa el almidón/fécula para elaborar la harina de mandioca con la que se elabora el pirón y las tortillas de fariña de mandioca, y el derivado tostado y condimentado de esta, llamado "farofa" en las zonas fronterizas con Brasil.

La harina o fariña tostada y condimentada ("farofa") se emplea para espesar guisos o tostada directamente sobre una plancha. La "feijoada", un cocido de cerdo y alubias negras, se acompaña habitualmente con "farofa" tostada. Otros platos emplean la raíz, como la "vaca atolada", en que esta se cocina hasta disolverse en el caldo. Hervida y pisada hasta hacer un puré se emplea para postres.

Se usa para preparar enyucado, carimañolas, casabe, pandeyuca, pastel de yuca, yucas chorreadas, palitos de yuca, sancochos, pandebono, entre otros. Sancochada o frita se sirve como acompañamiento carnes o de queso en los desayunos del Caribe Colombiano, donde también acompaña arroces, al chicharrón y se prepara en bollo. La yuca es un alimento sagrado para las culturas indígenas que se encuentran ubicadas en la Amazonía colombiana, donde se conocen más de 10 variedades, entre esa la yuca brava que es venenosa pero que los indígenas procesan de tal manera que obtienen bebidas, fariña y casabe, entre otros.

Se prepara hervida, frita, en puré o en tortas para acompañar todo tipo de platillos. Además, sirve para elaborar la olla de carne, enyucados, empanadas y postres como pasteles, atoles y dulces.

Se prepara hervida en trozos, que luego se untan con un mojo de ajo machacado y naranja agria (o limón), y después se le vierte manteca de cerdo limpia y bien caliente, sal al gusto. En el oriente del país también se prepara el casabe, como más arriba se describe.

La yuca es muy típica en platos costeños, de los cuales destaca el "encebollado", del que la yuca es el elemento principal, se acompaña con pescado (albacora, bonito o atún) en un caldo especial, y también se acompaña con verde frito (chifles), limón, aceite, ají y pan. En la Amazonia de Ecuador se usa para beber y comer todos los días.

Existe un plato típico conocido como "yuca con chicharrón", similar al plato cubano de yuca al mojo. Se prepara hervida en trozos y se sirve con lonja de cerdo con carne frita (chicharrón). Se le agrega una salsa de tomate cocido compuesta por trocitos de tomate, cebolla, chile dulce o pimiento verde. Se le puede agregar picante a la salsa de tomate o salsa picante a la preparación. 

En el estado de Tabasco se prepara una comida llamada puchero, la cual contiene carne y verduras que se dejan hervir hasta que la yuca se ablanda, de lo cual resulta un caldo. En el estado de Yucatán se prepara un postre que consiste en hervir durante casi un día entero la yuca en miel de abeja; actualmente se usa azúcar, para la cocción se utiliza leña en lugar de hacerlo en estufa.

Es consumida tradicionalmente de tanto en comidas como postres. En su forma hervida, es un acompañante de las comidas y las sopas o caldos a base de pollo, res y mariscos, además de bastimento en zonas rurales e indígenas. La yuca a su vez es un complemento indispensable de platillos nicaragüenses tradicionales como el vigorón, chancho con yuca, vaho, y rondón. El vigorón es una de las comidas callejeras más tradicionales y es básicamente yuca hervida con chicharrón de cerdo, acompañado de ensalada de repollo, tomate, cebolla y chile; la ciudad de Granada tiene la variante que el chicharrón lleva abundante carne además de la piel. En forma de postres o meriendas están los buñuelos, los cuales son hechos de yuca rayada con queso, bañados con una miel de azúcar hervida con canela.

Se usa para la preparación de carimañolas, también se consume sancochada con culantro al mojo, frita, majada y como ingrediente de las sopas e inclusive del sancocho. Se consume en desayuno, almuerzo o cena ya sea frita o sancochada. La fermentación de la yuca produce una bebida ligeramente alcohólica llamada cauim, consumida con propósitos rituales por los pueblos aborígenes. 

La fécula o almidón de mandioca se mezcla con queso y leche para hacer bollitos horneados llamados chipá, el tentempié más habitual de la región, o se utiliza para dar consistencia a guisados de carne y verdura como el "borí borí" (vorí vorí), el mbeyú (mbejú), o el caburé. La raíz de la mandioca o yuca se come hervida como acompañamiento de las comidas, a modo de papas (patatas), se la emplea frita sola o con huevos ("mandi'o chyryry"). Es también ingrediente principal en la elaboración del lampreado o "payaguá mascada".

En forma hervida es acompañante del cebiche; también se prepara frita, acompañándose con la salsa de la papa a la huancaína; en la zona norte del país, mayormente en Piura y Lambayeque, se prepara un piqueo con base en yuca machacada conocido como majado de yuca. En la Amazonia también se emplea para preparar el masato, bebida alcohólica indígena; además se prepara harina que se usa en sopas y en la preparación de panes. Abraham "fariña" que se utiliza en la preparación de refrescos, como el "shibé", tortillas, frituras, postres, como el "aradú", preparado con huevo de gallina o tortuga fluvial y otras combinaciones. Entre los platos típicos que utilizan la yuca está el picadillo de majaz, chicharrón de lagarto y otros platos exóticos de la gastronomía de la Amazonía peruana.

Se usa hervida como acompañante característico del arroz con habichuelas o al escabeche; también se prepara frita, acompañada de mojo. Al igual que otros países y regiones, se acostumbra también hacer casabe y otros panes. El mayor uso es rallada, en forma de masa para lograr las alcapurrias fritas y los pasteles hervidos de yuca.

Se utiliza para preparar el casabe, buñuelos, dulces llamados naiboas y como acompañante de la carne o pollo en brasas (hervida o frita) y forma parte del sancocho. También es posible, una vez cocida y ablandada, hacer una masa para elaborar arepas de yuca (como alternativa a las arepas tradicionales de maíz). Hervida o frita se puede emplear como acompañante de cualquier tipo de comidas.
El pirón es un alimento tradicional en zonas rurales. Elaborado a partir de fariña y caldo, preferentemente de puchero. La consistencia del pirón es similar a una gacha. Por fariña se conoce localmente a la harina de yuca o mandioca; el nombre deriva del portugués farinha.

La harina de yuca es muy rica en hidratos de carbono y no contiene gluten. Tiene buenas cualidades espesantes por lo que puede ser un recurso en la cocina para espesar salsas. También se puede hacer pirón, farofa o tortillas de yuca, o pan como sustituto de la harina de trigo, galletas, tortas, empanadas, arepas, etc, siempre teniendo en cuenta que la ausencia de gluten nos va a dar masas menos elásticas y esponjosas (por lo cual requiere aditivos como la goma xanthan). En Brasil es tradicional la producción artesanal de la harina de yuca.

Tiene la particularidad de no poseer gluten lo que la convierte en una alternativa para las personas celiacas, siendo a su vez una harina alternativa a la hora de preparar alimentos en nuestros hogares.

La yuca es una de las mayores fuentes de alimentos básicos del mundo. Algunos la califican de "base de la vida" tropical, porque es una de las más importantes fuentes de alimentación en extensas áreas de los trópicos. Es un cultivo apreciado por su fácil y amplia adaptabilidad a diversos ambientes ecológicos, el poco trabajo que requiere, la facilidad con que se cultiva y su gran productividad. Puede prosperar en suelos poco fértiles, en condiciones de poca pluviosidad. En condiciones óptimas la yuca puede producir más calorías alimenticias por hectárea que la mayoría de los demás cultivos alimenticios tropicales. Actualmente es un cultivo con altas expectativas para la producción de etanol y se prevé un crecimiento espectacular en la implantación de este cultivo.
En pruebas de laboratorio se ha determinado que la cáscara seca de yuca sumergida en agua se re-hidrata hasta 160 por ciento de su peso en 25 minutos. Se ha demostrado que la cáscara seca de yuca absorbe en promedio 120 por ciento de su peso de agua de mar. Esta agua no es salobre y es apropiada para uso agrícola. La cáscara seca de la yuca puede ser un modelo estructural para obtener un producto que absorba agua de mar para uso agrícola.

La yuca contiene cantidades pequeñas pero suficientes para causar posibles molestias de sustancias llamadas linamarina y lotaustralina. Estos son glucósidos cianogénicos que se convierten en ácido prúsico (cianuro de hidrógeno), por la acción de la enzima linamarasa, que también se encuentra presente en los tejidos de la raíz.

La concentración del ácido prúsico puede variar de 10 a 490 mg/kg de raíz fresca. Las variedades de yuca "amarga" contienen concentraciones más altas, especialmente cuando estas se cultivan en zonas áridas y en suelos de baja fertilidad. En las variedades llamadas "dulce" la mayor parte de las toxinas se encuentra en la cáscara. Algunas de estas variedades se pueden hasta comer crudas después de pelarlas - como si fueran zanahorias. Sin embargo en muchas de las variedades más frecuentemente cultivadas, que son amargas, la toxina también se halla presente en la carne feculenta de la raíz, especialmente en el núcleo fibroso que se halla en el centro.

Las raíces de yuca también contienen cianuro libre, hasta el 12% del contenido total de cianuro. La dosis letal de cianuro de hidrógeno no combinado para un adulto es de 50 a 60 mg, sin embargo la toxicidad del cianuro combinado no es muy conocida. Los glucósidos se descomponen en el tracto digestivo humano, lo que produce la liberación de cianuro de hidrógeno. Si se hierve la yuca fresca, la toxicidad disminuye muy poco. El glucósido linamarina es resistente al calor, y la enzima linamarasa se inactiva a 75 °C.

En algunos países de África, la llamada enfermedad del konzo se ha producido por el consumo casi exclusivo durante varias semanas de yuca mal procesada.

Los métodos de elaboración de la yuca para desintoxicar las raíces se basan fundamentalmente en la hidrólisis enzimática para reducir la concentración de glucósidos.

Se pueden distinguir los siguientes procesos:







Gastronomía



</doc>
<doc id="6901" url="https://es.wikipedia.org/wiki?curid=6901" title="Campo magnético">
Campo magnético

Un campo magnético es una descripción matemática de la influencia magnética de las corrientes eléctricas y de los materiales magnéticos. El campo magnético en cualquier punto está especificado por dos valores, la "dirección" y la "magnitud"; de tal forma que es un campo vectorial. Específicamente, el campo magnético es un vector axial, como lo son los momentos mecánicos y los campos rotacionales. El campo magnético es más comúnmente definido en términos de la fuerza de Lorentz ejercida en cargas eléctricas. 

El término se usa para dos campos distintos pero estrechamente relacionados, indicados por los símbolos B y H, donde, en el Sistema Internacional de Unidades, H se mide en unidades de amperios por metro y B se mide en teslas o newtons por metro entre amperio. En un vacío, H y B son lo mismo aparte de las unidades; pero en un material con magnetización (denotado por el símbolo M), B es solenoidal (no tiene divergencia en su dependencia espacial) mientras que H es no rotacional (libre de ondulaciones). 

Los campos magnéticos se producen por cualquier carga eléctrica producida por los electrones en movimiento y el momento magnético intrínseco de las partículas elementales asociadas con una propiedad cuántica fundamental, su espín. En la relatividad especial, campos eléctricos y magnéticos son dos aspectos interrelacionados de un objeto, llamado el tensor electromagnético. Las fuerzas magnéticas dan información sobre la carga que lleva un material a través del efecto Hall. La interacción de los campos magnéticos en dispositivos eléctricos tales como transformadores es estudiada en la disciplina de circuitos magnéticos.

Entre las definiciones de campo magnético se encuentra la dada por la fuerza de Lorentz. Esto sería el efecto generado por una corriente eléctrica o un imán, sobre una región del espacio en la que una carga eléctrica puntual de valor (q), que se desplaza a una velocidad formula_1, experimenta los efectos de una fuerza que es secante y proporcional tanto a la velocidad (v) como al campo (B). Así, dicha carga percibirá una fuerza descrita con la siguiente ecuación:

Donde F es la fuerza magnética, v es la velocidad y B el campo magnético, también llamado inducción magnética y densidad de flujo magnético.
(Nótese que tanto F como v y B son magnitudes vectoriales y el "producto vectorial" tiene como resultante un vector perpendicular tanto a "v" como a "B").
El módulo de la fuerza resultante será:

La existencia de un campo magnético se pone de manifiesto gracias a la propiedad de orientar un magnetómetro (laminilla de acero imantado que puede girar libremente). La aguja de una brújula, que evidencia la existencia del campo magnético terrestre, puede ser considerada un magnetómetro.La ley de Lorentz establece que una partícula cargada q que circula a una velocidad v→ por un punto en el que existe una intensidad de campo magnético B→, sufrirá la acción de una fuerza F→ denominada fuerza de Lorentz cuyo valor es proporcional al valor de q, B→ y v→ se obtiene por medio de la siguiente expresión:

Si bien algunos materiales magnéticos han sido conocidos desde la antigüedad, como por ejemplo el poder de atracción que la magnetita ejerce sobre el hierro, no fue sino hasta el siglo XIX cuando la relación entre la electricidad y el magnetismo quedó plasmada, pasando ambos campos de ser diferenciados a formar el cuerpo de lo que se conoce como electromagnetismo.
Antes de 1820, el único magnetismo conocido era el del hierro. Esto cambió con un profesor de ciencias poco conocido de la Universidad de Copenhague, Dinamarca, Hans Christian Oersted. En 1820 Oersted preparó en su casa una demostración científica a sus amigos y estudiantes. Planeó demostrar el calentamiento de un hilo por una corriente eléctrica y también llevar a cabo demostraciones sobre el magnetismo, para lo cual dispuso de una aguja de brújula montada sobre una peana de madera.

Mientras llevaba a cabo su demostración eléctrica, Oersted notó para su sorpresa que cada vez que se conectaba la corriente eléctrica, se movía la aguja de la brújula. Se calló y finalizó las demostraciones, pero en los meses sucesivos trabajó duro intentando explicarse el nuevo fenómeno.¡Pero no pudo! La aguja no era ni atraída ni repelida por la corriente. En vez de eso tendía a quedarse en ángulo recto. Hoy sabemos que esto es una prueba fehaciente de la relación intrínseca entre el campo magnético y el campo eléctrico plasmada en las ecuaciones de Maxwell.

Como ejemplo para ver la naturaleza un poco distinta del campo magnético basta considerar el intento de separar el polo de un imán. Aunque rompamos un imán por la mitad este "reproduce" sus dos polos. Si ahora volvemos a partir otra vez en dos, nuevamente tendremos cada trozo con los polos norte y sur diferenciados. En magnetismo no se han observado los monopolos magnéticos.

El nombre de campo magnético o intensidad del campo magnético se aplica a dos magnitudes:

Desde un punto de vista físico, ambos son equivalentes en el vacío, salvo en una constante de proporcionalidad (permeabilidad) que depende del sistema de unidades: 1 en el sistema de Gauss, formula_2 en el SI. Solo se diferencian en medios materiales con el fenómeno de la magnetización.

El campo H se ha considerado tradicionalmente el campo principal o intensidad de campo magnético, ya que se puede relacionar con unas "cargas", "masas" o "polos magnéticos" por medio de una ley similar a la de Coulomb para la electricidad. Maxwell, por ejemplo, utilizó este enfoque, aunque aclarando que esas cargas eran ficticias. Con ello, no solo se parte de leyes similares en los campos eléctricos y magnéticos (incluyendo la posibilidad de definir un potencial escalar magnético), sino que en medios materiales, con la equiparación matemática de H con E, por un lado, y de B con D, por otro, se pueden establecer paralelismos útiles en las condiciones de contorno y las relaciones termodinámicas; las fórmulas correspondientes en el sistema electromagnético de Gauss son:

En electrotecnia no es raro que se conserve este punto de vista porque resulta práctico.

Con la llegada de las teorías del electrón de Lorentz y Poincaré, y de la relatividad de Einstein, quedó claro que estos paralelismos no se corresponden con la realidad física de los fenómenos, por lo que hoy es frecuente, sobre todo en física, que el nombre de "campo magnético" se aplique a B (por ejemplo, en los textos de Alonso-Finn y de Feynman). En la formulación relativista del electromagnetismo, E no se agrupa con H para el tensor de intensidades, sino con B.

En 1944, F. Rasetti preparó un experimento para dilucidar cuál de los dos campos era el fundamental, es decir, aquel que actúa sobre una carga en movimiento, y el resultado fue que el campo magnético real era B y no H.

Para caracterizar H y B se ha recurrido a varias distinciones. Así, H describe cuan intenso es el campo magnético en la región que afecta, mientras que B es la cantidad de flujo magnético por unidad de área que aparece en esa misma región. Otra distinción que se hace en ocasiones es que H se refiere al campo en función de sus fuentes (las corrientes eléctricas) y B al campo en función de sus efectos (fuerzas sobre las cargas).

Un campo magnético tiene dos fuentes que lo originan. Una de ellas es una corriente eléctrica de conducción, que da lugar a un campo magnético estático, si es constante. Por otro lado una corriente de desplazamiento origina un campo magnético variante en el tiempo, incluso aunque aquella sea estacionaria.

La relación entre el campo magnético y una corriente eléctrica está dada por la ley de Ampère. El caso más general, que incluye a la corriente de desplazamiento, lo da la ley de Ampère-Maxwell.

El campo magnético generado por una única carga en movimiento (no por una corriente eléctrica) se puede calcular de manera aproximada a partir de la siguiente expresión derivada de la ley de Biot-Savart:
Donde formula_3. Esta última expresión define un campo vectorial solenoidal, para distribuciones de cargas en movimiento la expresión es diferente, pero puede probarse que el campo magnético sigue siendo un campo solenoidal. Es una aproximación debido a que, al partir de una corriente continua de cargas e intentar transformar la ley para cargas puntuales, se desprecian las interacciones entre las cargas de la corriente. Esta aproximación es útil para bajas velocidades (respecto a la velocidad de la luz).


A su vez este potencial vector puede ser relacionado con el vector densidad de corriente mediante la relación:

La ecuación anterior planteada sobre formula_5, con una distribución de cargas contenida en un conjunto compacto, la solución es expresable en forma de integral. Y el campo magnético de una distribución de carga viene dado por:
_r} {\|\mathbf{r}-\mathbf{r}_1\|^2}\ \mathrm{d}V_1</math>

Cabe destacar que, a diferencia del campo eléctrico, en el campo magnético no se ha comprobado la existencia de monopolos magnéticos, solo dipolos magnéticos, lo que significa que las líneas de campo magnético son cerradas, esto es, el número neto de líneas de campo que entran en una superficie es igual al número de líneas de campo que salen de la misma superficie. Un claro ejemplo de esta propiedad viene representado por las líneas de campo de un imán, donde se puede ver que el mismo número de líneas de campo que salen del polo norte vuelve a entrar por el polo sur, desde donde vuelven por el interior del imán hasta el norte.
Como se puede ver en el dibujo, independientemente de que la carga en movimiento sea positiva o negativa, en el punto A nunca aparece campo magnético; sin embargo, en los puntos B y C el campo magnético invierte su dirección dependiendo de si la carga es positiva o negativa. La dirección del campo magnético viene dado por la regla de la mano derecha, siendo las pautas las siguientes:


La energía es necesaria para generar un campo magnético, para trabajar contra el campo eléctrico que un campo magnético crea y para cambiar la magnetización de cualquier material dentro del campo magnético. Para los materiales no-dispersivos, se libera esta misma energía tanto cuando se destruye el campo magnético para poder modelar esta energía, como siendo almacenado en el campo magnético.

Para materiales lineales y no dispersivos (tales que formula_6 donde μ es independiente de la frecuencia), la densidad de energía es:

Si no hay materiales magnéticos alrededor, entonces el μ se puede substituir por μ. La ecuación antedicha no se puede utilizar para los materiales no lineales, se utiliza una expresión más general dada abajo. 

Generalmente la cantidad incremental de trabajo por el δW del volumen de unidad necesitado para causar un cambio pequeño del δB del campo magnético es:
δW= H*δB

Una vez que la relación entre H y B se obtenga, esta ecuación se utiliza para determinar el trabajo necesitado para alcanzar un estado magnético dado. Para los materiales como los ferromagnéticos y superconductores el trabajo necesitado también dependerá de cómo se crea el campo magnético.

El campo magnético para cargas que se mueven a velocidades pequeñas comparadas con velocidad de la luz, puede representarse por un campo vectorial. 
Sea una carga eléctrica de prueba formula_7 en un punto P de una región del espacio moviéndose a una cierta velocidad arbitraria v respecto a un cierto observador que no detecte campo eléctrico. Si el observador detecta una deflexión de la trayectoria de la partícula entonces en esa región existe un campo magnético. El valor o intensidad de dicho campo magnético puede medirse mediante el llamado vector de inducción magnética B, a veces llamado simplemente "campo magnético", que estará relacionado con la fuerza F y la velocidad v medida por dicho observador en el punto P: Si se varía la dirección de v por P, sin cambiar su magnitud, se encuentra, en general, que la magnitud de F varía, si bien se conserva perpendicular a v. A partir de la observación de una pequeña carga eléctrica de prueba puede determinarse la dirección y módulo de dicho vector del siguiente modo:


En consecuencia: "Si una carga de prueba positiva formula_7 se dispara con una velocidad v por un punto P y si obra una fuerza lateral F sobre la carga que se mueve, hay una inducción magnética B en el punto P siendo B el vector que satisface la relación:"

La magnitud de F, de acuerdo a las reglas del producto vectorial, está dada por la expresión:

Expresión en la que formula_9 es el ángulo entre v y B.

El hecho de que la fuerza magnética sea siempre perpendicular a la dirección del movimiento implica que el trabajo realizado por la misma sobre la carga, es cero. En efecto, para un elemento de longitud formula_10 de la trayectoria de la partícula, el trabajo formula_11 es formula_12 que vale cero por ser formula_13 y formula_10 perpendiculares. Así pues, un campo magnético estático no puede cambiar la energía cinética de una carga en movimiento. 

Si una partícula cargada se mueve a través de una región en la que coexisten un campo eléctrico y uno magnético la fuerza resultante está dada por:
Esta fórmula es conocida como Relación de Lorentz

La teoría de la relatividad especial probó que de la misma manera que espacio y tiempo no son conceptos absolutos, la parte eléctrica y magnética de un campo electromagnético dependen del observador. Eso significa que dados dos observadores formula_15 y formula_16 en movimiento relativo uno respecto a otro el campo magnético y eléctrico medido por cada uno de ellos no será el mismo. En el contexto de la relatividad especial si los dos observadores se mueven uno respecto a otro con velocidad uniforme "v" dirigida según el eje X, las componentes de los campos eléctricos medidas por uno y otro observador vendrán relacionadas por:
\quad \bar{E}_z = \frac{E_z + v B_y}{\sqrt{1-v^2/c^2}} </math>
Y para los campos magnéticos se tendrá:
\quad \bar{B}_z = \frac{B_z - v E_y/c^2}{\sqrt{1-v^2/c^2}} </math>
Nótese que en particular un observador en reposo respecto a una carga eléctrica detectará solo campo eléctrico, mientras que los observadores que se mueven respecto a las cargas detectarán una parte eléctrica y magnética.

El campo magnético creado por una carga en movimiento puede probarse por la relación general:

que es válida tanto en mecánica newtoniana como en mecánica relativista. Esto lleva a que una carga puntual moviéndose a una velocidad v proporciona un campo magnético dado por:
\mathbf{v}\times\mathbf{u}_r</math>
donde el ángulo formula_17 es el ángulo formado por los vectores formula_18 y formula_19. Si el campo magnético es creado por una partícula cargada que tiene aceleración la expresión anterior contiene términos adicionales (ver potenciales de Liénard-Wiechert).

La unidad de B en el SI es el tesla, que equivale a wéber por metro cuadrado (Wb/m²) o a voltio segundo por metro cuadrado (V s/m²); en unidades básicas es kg s A. Su unidad en sistema de Gauss es el gauss (G); en unidades básicas es cm g s.

La unidad de H en el SI es el amperio por metro, A/m (a veces llamado amperivuelta por metro, Av/m). Su unidad en el sistema de Gauss es el oérsted (Oe), que es dimensionalmente igual al Gauss.

La magnitud del campo magnético terrestre en la superficie de la Tierra es de alrededor de 0.5G. Los imanes permanentes comunes, de hierro, generan campos de unos pocos cientos de Gauss, esto es a corto alcance la influencia sobre una brújula es alrededor de mil veces más intensa que la del campo magnético terrestre; como la intensidad se reduce con el cubo de la distancia, a distancias relativamente cortas el campo terrestre vuelve a dominar. Los imanes comerciales más potentes, basados en combinaciones de metales de transición y tierras raras generan campos hasta diez veces más intensos, de hasta 3000-4000 G, esto es, 0.3-0.4 T. El límite teórico para imanes permanentes es alrededor de diez veces más alto, unos 3 Tesla. Los centros de investigación especializados obtienen de forma rutinaria campos hasta diez veces más intensos, unos 30T, mediante electroimanes; se puede doblar este límite mediante campos pulsados, que permiten enfriarse al conductor entre pulsos. En circunstancias extraordinarias, es posible obtener campos incluso de 150 T o superiores, mediante explosiones que comprimen las líneas de campo; naturalmente en estos casos el campo dura solo unos microsegundos. Por otro lado, los campos generados de forma natural en la superficie de un púlsar se estiman en el orden de los cientos de millones de Tesla.

En el mundo microscópico, atendiendo a los valores del momento dipolar de iones magnéticos típicos y a la ecuación que rige la propagación del campo generado por un dipolo magnético, se verifica que a un nanómetro de distancia, el campo magnético generado por un electrón aislado es del orden de 3 G, el de una molécula imán típica, del orden de 30 G y el de un ion magnético típico puede tener un valor intermedio, de 5 a 15 G. A un angstrom, que es un valor corriente para un radio atómico y por tanto el valor mínimo para el que puede tener sentido referirse al momento magnético de un ion, los valores son mil veces más elevados, esto es, del orden de magnitud del Tesla.



</doc>
<doc id="6902" url="https://es.wikipedia.org/wiki?curid=6902" title="Mitología hinduista">
Mitología hinduista

El término mitología hindú se refiere colectivamente a un largo cuerpo de literatura de la India (esencialmente, mitología y religión del hinduismo) que detalla las vidas y los tiempos de personajes legendarios, deidades y de encarnaciones divinas en la Tierra, a menudo entremezcladas con discursos doctrinarios y éticos

Aunque se clasifican a menudo como «mitología hindú» o «de la India», la etiqueta no capta la centralidad de las afiliaciones religiosas y espirituales de los textos, que perseveran en la actualidad para la mayoría de los hindúes.
Están repletas de largos discursos religiosos y se ven a menudo como fuente para la ética y la práctica hinduista.

Se debe notar que para los hinduistas sus tradiciones no son mitología.
Un ejemplo paralelo sería llamar a la "Biblia" como mitología cristiana.

Entre los textos más importantes se encuentran los "Puranás".
Otros trabajos importantes de la mitología hindú son las dos grandes epopeyas hindúes, el "Ramaiana" y el "Majábharata" (que incluye el texto "Bhagavad Gita", muy sagrado en la India).



</doc>
<doc id="6913" url="https://es.wikipedia.org/wiki?curid=6913" title="Nike">
Nike

Nike, Inc. ( / naiki / )(del griego: Νίκη, "Nike," diosa de la victoria ; ) es una empresa multinacional estadounidense dedicada al diseño, desarrollo, fabricación y comercialización de equipamiento deportivo: balones, calzado, ropa, equipo, accesorios y otros artículos deportivos.

Es uno de los mayores proveedores de material deportivo, con unos ingresos de más de 24,100 millones de dólares estadounidenses y un total en 2019 de unos 76.700 empleados. La marca por sí sola tiene un valor de 47,400 millones de dólares estadounidenses, lo que la convierte en la marca más valiosa entre las corporaciones deportivas.

La empresa fue fundada el 25 de enero de 1964 como "Blue Ribbons Sports" por Phil Knight y Bill Bowerman, y se convirtió oficialmente en "Nike Inc.," el 30 de mayo de 1971. Nike comercializa sus productos bajo su propia marca, así como bajo "Nike Golf", "Nike Pro", "Nike +", "Air Jordan", "Nike Skateboarding", "Hurley InternationalyConverse", "Nike CR7", entre otras. Nike también fue dueño de Bauer Hockey (Nike Bauer) entre 1995 y 2008, y anteriormente propiedad de Cole Haan y Umbro. Además de la ropa deportiva y el equipo de fabricación, la compañía es dueña de las tiendas Niketown. Nike es patrocinador de muchos atletas de alto nivel y equipos deportivos de todo el mundo, con el famoso eslogan «"Just do it."» y el logo, llamado Swoosh, creado por Carolyn Davidson, esquematización de un ala de Niké, diosa que da nombre a la marca.

Fue fundada por el deportista de la Universidad de Oregón Phil Knight y su entrenador, William Jay "Bill" Bowerman. La empresa comenzó distribuyendo calzado de la firma "Onitsuka Tiger" (actualmente ASICS) hasta 1971, cuando BRS lanzó su primer producto propio, con el emblema de la marca diseñado por Carolyn Davidson.

Según Otis Davis, un deportista, a quien Bowerman entrenó en la Universidad de Oregón, que más tarde pasó a ganar dos medallas de oro en los Juegos Olímpicos de Roma 1960. Bowerman hizo el primer par de zapatos Nike para él, lo que contradice la afirmación de que se hicieron por Phil Knight. Dice Davis. “le dijo a Tom Brokaw que yo era el primero. No me importa lo que dicen todos los multimillonarios". 

En 1964, en su primer año en el negocio, BRS vendió 1300 pares de zapatos para correr ganando en total 8000 $. Antes de 1965 la nueva compañía había adquirido un empleado a tiempo completo, y las ventas habían alcanzado los 20.000 $. En 1966, El BRS abrió su primera tienda ubicada en el 3456 Bulevar en Santa Mónica, California, junto a un salón de belleza. En 1967, debido al rápido aumento de las ventas, el BRS expandió sus operaciones al por menor y distribución en la costa este, en Wellesley, Massachusetts.

En 1971, la relación entre BRS y Onitsuka Tiger estaba llegando a su fin. BRS se prepara para lanzar su propia línea de calzado, lo que llevaría el Swoosh de nuevo diseño por Carolyn Davidson. El Swoosh se utilizó por primera vez por Nike el 18 de junio de 1971, y el 22 de enero de 1974 se registró en la Oficina de Patentes y Marcas de Estados Unidos

En 1976, la compañía contrató a John Brown and Partners, con sede en Seattle, como su primera agencia de publicidad. Al año siguiente, la agencia creó el primer "anuncio de marca" de Nike, llamado "No hay línea de meta", en el que no se mostró ningún producto de Nike. En 1980, Nike había alcanzado una cuota de mercado del 50% en el mercado de calzado deportivo de Estados Unidos, y la compañía se hizo pública en diciembre de ese año.
Juntos, Nike y Wieden + Kennedy han creado impresión y anuncios de televisión. Wieden + Kennedy sigue siendo la agencia de publicidad principal de Nike. Fue cofundador de la agencia Dan Wieden, quien acuñó el lema ahora famoso "Just Do It" para una campaña publicitaria de Nike 1988, que fue elegida por Advertising Age como uno de los cinco lemas más importantes del siglo XX y que está consagrado en la Smithsonian Institution. Walt Stack apareció en el primer anuncio de Nike "Just Do It", que se estrenó el 1 de julio de 1988. Wieden atribuye la inspiración para el lema de "hagámoslo" ("let's do it"), las últimas palabras pronunciadas por Gary Gilmore antes de ser ejecutado.

A lo largo de la década de 1980, Nike ha ampliado su línea de productos para abarcar muchos deportes y regiones de todo el mundo.
A principios de los años 80 se popularizaba en Estados Unidos el uso del calzado deportivo para el uso diario y esto junto con las estrategias de patrocinio hace que Nike llegue a los hogares estadounidenses de forma masiva. A mediados de los 80, la empresa atraviesa una crisis de la mano de su competidor Reebok, ésta se superaría gracias a la contratación en 1985 del novato sensación de baloncesto llamado Michael Jordan que llevará a la marca a cotas de mercado inéditas hasta la fecha a costa de ganarse repetidas multas de la NBA por volar sus normas de indumentaria. En este lustro es cuando se creó el eslogan publicitario más conocido de la marca, «"Just Do It"», reconocido incluso como marca autónoma en muchos ámbitos.

En los últimos años, ha desplazado el foco de su negocio desde la producción, que actualmente corre a cargo de empresas externas, a la imagen de marca, como símbolo del espíritu del deporte y la autosuperación.

Nike fabricante de ropa y calzado deportivo, en noviembre de 2008, adquirió la marca Umbro en una operación que valoró a la firma británica en 285 millones de libras esterlinas (340 millones de euros). Umbro vio en esta compra una oportunidad para ampliar su negocio internacional. La marca deportiva tuvo que perfilar sus expectativas de beneficios para 2008 ante la caída en las ventas de la camiseta de la Selección de fútbol de Inglaterra

Umbro ha accedido a la propuesta de Nike tras valorar que su rendimiento financiero tendrá menos desequilibrios al pasar de un año con un gran torneo de fútbol (como un Mundial o la Eurocopa) a otro sin torneos de ese tipo.

Y es que la caída en las ventas de la camiseta de la selección inglesa repercutía en su cotización en Bolsa. Umbro advirtió que los beneficios en 2008 no alcanzarían los pronósticos tras las pobres ventas de equipamiento en 2007. Las acciones se resintieron ante este anuncio y cerraron a 165 peniques.

Nike ha dicho que los accionistas de Umbro recibirán 193,06 peniques por título, lo que representa una prima de aproximadamente un 61% sobre el precio de cierre del valor.

Sin embargo, Nike vendió a Umbro en el mes de mayo del año 2012 por un precio de 174 millones de euros a la principal compañía de gestión de marcas en el mundo Iconix Brand Group ya que la idea de Nike era centrarse en sus marcas con mayor potencial de crecimiento.

El isotipo, denominado "swoosh" se llama así ya que era el sonido que se escuchaba al momento de correr, fue creado por la estudiante de diseño Carolyn Davidson en 1971.

Nike fabrica una amplia gama de equipamiento deportivo. Sus primeros productos fueron zapatos de pista para correr. En la actualidad también se hacen zapatos, camisetas, pantalones cortos, y demás accesorios, para una amplia gama de deportes, incluyendo atletismo, béisbol, hockey sobre hielo, tenis, fútbol, lacrosse, baloncesto, y el cricket. Nike Air Max, es una línea de zapatos lanzados por primera vez en 1987. Las adiciones más recientes a la línea son el Nike 6.0, NYX Nike y Nike SB, zapatos diseñados para el skateboarding. Nike ha presentado recientemente los zapatos de cricket llamado "Zoom Air New Yorker", diseñado para ser un 30% más ligero que sus competidores. En 2009, Nike presentó el Air Jordan XX3, una zapatilla de baloncesto de alto rendimiento.
Nike se asoció recientemente con Apple Inc. para producir el producto Nike+ que monitorea el desempeño de un corredor a través de un dispositivo de radio en el zapato que enlaza con el iPod o iPhone. El producto genera estadísticas útiles, pero ha sido criticado por los investigadores. En 2004, Nike lanzó el Programa de Capacitación SPARQ / División. También lanzó al mercado los tenis Daddy Yankee en honor a dicha celebridad para alcanzar máxima popularidad en Estados Unidos y el mundo entero. Esta producción fue idea de Phil Knight firmando un contrato millonario para usar su prestigio en la venta, al escuchar sus canciones, ya que se vendieron como pan caliente en toda Latinoamérica.

Algunas de las nuevas zapatillas Nike contienen Flywire y espuma Lunarlite para reducir el peso. El 15 de julio de 2009, la pulsera inteligente Nike+ FuelBand fue lanzada en las tiendas. Entre sus características se destaca la ejecución de los registros de productos a distancia, las calorías quemadas, un registro puntual de la hora y también brinda a los corredores nuevas rutas en línea con las cuales pueden seguir corriendo.

El 2010 Nike Pro Combat, colección de camiseta, fueron usados por los equipos de las siguientes universidades: Miami, New York, Boise State University, Florida, Oregón State University, Texas Christian University, Virginia Tech, Virginia Occidental, y Pittsburgh.

Fabricación

Nike tiene más de 500 lugares en todo el mundo y oficinas en 45 países fuera de Estados Unidos. La mayoría de las fábricas están situadas en Asia, incluyendo Indonesia, China, Taiwán, India, Tailandia, Vietnam, Pakistán, Filipinas, Malasia, y la República de Corea. Nike no se atreve a revelar información sobre el contrato con las empresas que funciona. Sin embargo, debido a las duras críticas por parte de algunas organizaciones como Barbie.com, Nike ha divulgado información sobre su contrato de fábricas en su Informe de Gobierno Corporativo.
Nike Plus por su parte, es la alianza de la marca con Apple que permitió en 2006 el lanzamiento de una zapatilla inteligente dotada con un sensor que se conecta con el iPod Nano y permite conocer el rendimiento al correr. A nivel megaglobal, Nike fue responsable el 2008 de organizar la carrera con más participantes en la historia, fue el pasado 31 de agosto y se llamó The Human Race. En ella participaron más de millón de participantes que corrieron 10 km de forma simultánea en 25 ciudades de diversos países.

Otro punto importante a destacar son, claro está, los deportistas patrocinados por la empresa. En toda su existencia, grandes figuras han sido ícono de la “pipa”, como: Michael Jordan, Cristiano Ronaldo, Kyrie Irving, Roger Federer, Tiger Woods, Kobe Bryant, Ronaldinho, Manny Pacquiao, Ronaldo, Rafael Nadal, Serena Williams, María Sharápova, Eric Koston, Radamel Falcao, Roman Torres,
Lebron James, Arturo Vidal, Juan Martín del Potro, entre muchos otros. De igual manera, instituciones deportivas como los equipos de fútbol: FC Barcelona, Sevilla FC, Granada Club de Fútbol, Atlético Nacional, San Lorenzo de Almagro, Atlético de Madrid, Málaga C.F, Paris Saint-Germain, Montpellier, Inter de Milán, A.S. Roma, Chelsea Football Club, Tottenham Hotspur FC, Brighton & Hove Albion F. C., Liverpool FC, Eintracht Frankfurt, Hertha Berlín, RB Leipzig, VfL Wolfsburgo, Club Alianza Lima, BSC Young Boys, Zenit San Petersburgo, FC Spartak de Moscú, AZ Alkmaar, Vitesse, Rio Ave FC, C.S. Marítimo, FC Shakhtar Donetsk, K.R.C. Genk, Red Bull Salzburgo, Galatasaray SK, 
Ankaragücü, Antalyaspor, Denizlispor, Shanghái Shenhua, Shanghai SIPG FC, Kashima Antlers, Molde FK, SK Brann, Partizán de Belgrado, Aris Thessaloniki FC, Corinthians, Club América, Pumas UNAM, selección de fútbol de los Estados Unidos, selección de fútbol de Canadá, selección de fútbol de Australia, selección de fútbol de Brasil, selección de fútbol de Francia, selección de fútbol de Portugal, selección de fútbol de Inglaterra, selección de fútbol de los Países Bajos, selección de fútbol de Polonia, selección de fútbol de Croacia, selección de fútbol de Grecia, selección de fútbol de Noruega, selección de fútbol de Eslovenia, selección de fútbol de Finlandia, selección de fútbol de Turquía, selección de fútbol de Corea del Sur, selección de fútbol de China, selección de fútbol de Arabia Saudita, selección de fútbol de Nigeria, selección de fútbol de Catar, selección de fútbol de Sudáfrica y selección de fútbol de Chile.

Además, se ha encargado de patrocinar eventos o torneos, como la Copa Libertadores de América, el Abierto de Tenis de Australia, el Tour de Francia, Copa América, Liga Panameña de Baloncesto entre otros.

La sede mundial de Nike están rodeada por la ciudad de Beaverton, pero está en un radio no incorporado del Condado de Washington (Oregón). La ciudad intentó anexionar por la fuerza a su territorio la sede de Nike, lo que dio lugar a una demanda de Nike, y la presión de la empresa que finalmente terminó en la Ley del Senado 887 de 2005 de Oregón. Bajo los términos de ese proyecto de ley, le fue específicamente prohibido a Beaverton la anexión por la fuerza la tierra que Nike y Columbia Sportswear ocupan en el condado de Washington durante 35 años, mientras que Electro Scientific Industries y Tektronix reciben la misma protección pero a los 30 años. Nike está planeando la construcción de su sede mundial en Beaverton con una expansión de 3,2 millones de pies cuadrados. El diseño tendrá como objetivo la certificación LEED Platino y estará cubierto por luz natural, y un centro de tratamiento de aguas residuales.

Nike ha contratado más de 700 tiendas en todo el mundo y tiene oficinas ubicadas en 45 países fuera de los Estados Unidos. La mayoría de las fábricas se encuentran en Asia, incluyendo Indonesia, China, Taiwán, India, Tailandia, Vietnam, Pakistán, Filipinas y Malasia. Nike no se atreve a revelar información acerca de las empresas contratistas que trabaja. Sin embargo, debido a las duras críticas por parte de algunas organizaciones como CorpWatch, Nike ha revelado información acerca de sus fábricas contratistas en el Informe de Gobierno Corporativo.

Nike ha sido criticado por la contratación de fábricas (conocidas como fábricas de explotación de Nike) en países como China, Vietnam, Indonesia y México. Labor Watch Vietnam, un grupo de activistas, ha descubierto que las fábricas contratadas por Nike han violado las leyes de salario mínimo y tiempo extra en Vietnam desde 1996, aunque Nike afirma que esta práctica se ha detenido. La empresa ha sido objeto de muchas críticas: las condiciones de trabajo a menudo son muy pobres y la explotación de mano de obra barata en el extranjero empleado en las zonas de libre comercio , donde sus productos se fabrican típicamente. Las fuentes de estas críticas se incluyen en el libro de Naomi Klein, No Logo y en los documentales de Michael Moore. Hay muchas campañas hechas por muchos colegios y universidades, en especial contra la globalización, así como varios grupos antiexplotación como el de Estudiantes Unidos contra la Explotación.

Durante la década de 1990, Nike fue objeto de críticas por el uso de Trabajo infantil en Camboya y Pakistán en las fábricas se contrajo para la fabricación de balones de fútbol. Aunque Nike tomó medidas para frenar o al menos reducir la práctica, siguen contratando su producción a empresas que operan en áreas donde la regulación y la supervisión inadecuada hacen que sea difícil asegurar que no se está utilizando trabajo infantil.

En 2001, un documental de la BBC descubrió ocurrencias de trabajo infantil y malas condiciones de trabajo en una fábrica de Camboya utilizado por Nike. El documental se centró en seis niñas, que trabajaron siete días a la semana, a menudo 16 horas al día.

En abril de 2014, una de las mayores huelgas en China continental se llevó a cabo en el Yue Yuen Holdings Dongguan fábrica de zapatos, produciendo, entre otros, para Nike. Yue Yuen pagó menos de 250 yuanes (40.82 dólares) por mes. El salario medio en Yue Yuen es 3000 yuanes por mes. La fábrica emplea a 70.000 personas. Esta práctica estaba vigente durante casi 20 años.

Según la organización ambientalista Clean Air-Cool Planet, con sede en Nueva Inglaterra, Nike figura entre las tres primeras empresas (de un total de 56) en una encuesta de empresas amigas del clima. Nike también ha sido elogiado por su Nike Grind programa (que cierra el ciclo de vida del producto ) por grupos como Climate Counts. Una campaña que comenzó Nike para el Día de la Tierra 2008 fue un comercial que contó con la estrella de baloncesto Steve Nash que llevaba zapatos de Nike reciclados, que habían sido construidos en febrero de 2008 a partir de piezas de cuero y piel sintética de residuos de pisos de la fábrica. El zapato reciclado también ofreció una suela compuesta del caucho ground-up de un programa del reciclaje del zapato. Nike afirma que este es el primer zapato de baloncesto de rendimiento que se ha creado a partir de residuos de fabricación, pero solo produjo 5.000 pares para la venta.

Otro proyecto que Nike ha comenzado es el programa Nike Reuse-A-Shoe. Este programa, iniciado en 1993, es el programa más largo de Nike que beneficia tanto al medio ambiente como a la comunidad coleccionando zapatos viejos de cualquier tipo para procesarlos y reciclarlos. El material que se produce a continuación, se utiliza para ayudar a crear superficies deportivas, tales como canchas de baloncesto, pistas de carreras y juegos infantiles.

Un proyecto a través de la Universidad de Carolina del Norte en Chapel Hill encontró trabajadores que fueron expuestos a tóxicos isocianatos y otras sustancias químicas en las fábricas de calzado en Tailandia. Además de la inhalación, la exposición dérmica fue el mayor problema encontrado. Esto podría resultar en reacciones alérgicas incluyendo reacciones asmáticas.


</doc>
<doc id="6916" url="https://es.wikipedia.org/wiki?curid=6916" title="Hans Spemann">
Hans Spemann

Hans Spemann (Stuttgart, 27 de junio de 1869-Friburgo de Brisgovia, 12 de septiembre de 1941) fue un embriólogo alemán, galardonado en 1935 con el Premio Nobel de Medicina por el descubrimiento del efecto conocido en la actualidad como inducción embriológica.

Hans Spemann estudió medicina en la Universidad de Heidelberg y zoología en el Instituto Zoológico de la Universidad de Würzburg. Nombrado director del Instituto de Zoología de la Universidad de Rostock en 1908, fue director del Instituto de Biología Kaiser-Wilhelm de Berlín (1914) y profesor de zoología en la Universidad de Friburgo (1919), de la que se convertirá en rector entre 1923-1924.

Una pregunta central en la biología del desarrollo es cómo la forma y el patrón emergen de los principios simples de un óvulo fecundado. Cómo y cuándo las células individuales y los tejidos deciden qué camino de desarrollo escoger, si los destinos de células de alguna manera son predeterminados, o las células y los tejidos interactúan entre sí para orquestar los procesos de desarrollo (Marte, 2004).

Hans Spemann junto con su estudiante Hilde Mangold reconocen un principio fundamental durante el desarrollo: la inducción y otras interacciones célula-célula (Wolpert et al., 2007; Marte 2004). Proporcionaron por primera vez una evidencia inequívoca de que el destino de la célula y tejido puede ser determinada por las señales recibidas de otras células (Wolpert et al., 2007; Marte 2004).

Spemann fue el primer embriólogo en identificar un campo morfogenético en sus experimentos con cristalinos de rana. Luego redirige su investigación fuera de la formación de determinados órganos (ojos, oídos, hígado) hacia mirar el problema de las primeras etapas de la determinación embrionaria (Gilbert, 2006). Esta experiencia le prepararía para la serie crucial de experimentos que le condujeron al descubrimiento del área embrionaria en la gástrula anfibia, conocido como "El Organizador de Spemann" (también conocido como el organizador de Spemann-Mangold). Estas pequeñas regiones fueron llamadas el organizador ya que parecían ser las responsables de controlar la organización del cuerpo embrionario completo (Wolpert,2007). Debido a este descubrimiento, Spemann recibió el Premio Nobel de Medicina en 1935, uno de solo dos premios recibidos en investigación embrionaria (Wolpert et al., 2007).

El experimento fue realizado en embriones de salamandra de la misma especie en la fase de gastrulación, poniendo a prueba el estado de determinación de la gástrula temprana de la salamandra. Con este fin, trasplantó pequeñas regiones de embriones procedentes de una región de la gástrula de la salamandra a una nueva región en otra gástrula. El tejido injertado fue tomado del labio dorsal del blastoporo(Wolpert), al injertar el tejido entre las especies de salamandra de diferente pigmentación, el destino de los tejidos del huésped y el injerto se podían distinguir. Se indujo la formación de los tejidos neurales a partir del ectodermo que de otro modo supone un destino epidérmico y causó dorsalización del mesodermo ventral, lo que lleva a la formación de somitas. Esto dio lugar a la formación de un segundo eje embrionario, y por lo tanto un embrión gemelo, en el sitio del injerto. Todas las estructuras estaban compuestas tanto por el injerto como por las células del huésped. Además, señaló la extensión convergente de la placa neural posterior.

En 1924, Spemann y Mangold publicaron algunas de las conclusiones de sus descubrimientos en los trasplantes del labio dorsal del blastoporo. Encontraron (a) que los trasplantes del labio dorsal del blastoporo se habían invaginado casi por completo, (b) que el tejido trasplantado, provocó la formación de una placa neural secundaria compuesta casi enteramente de los tejidos del huésped, y (c) que, si bien la notocorda se derivó principalmente de tejidos de donantes, el mesodermo acompañante fue una combinación de las células de los donantes y de las células del huésped. Algunos somitas eran quiméricos, algunos completamente del hospedero, algunos completamente del donante (Gilbert, 2006).




</doc>
<doc id="6925" url="https://es.wikipedia.org/wiki?curid=6925" title="Red por radio">
Red por radio

La red por radio es aquella red inalámbrica que emplea la radiofrecuencia como medio de transmisión de las diversas estaciones de la red.

Es un tipo de red usada en distintas empresas dedicadas al soporte de redes en situaciones difíciles para el establecimiento de cableado, como es el caso de edificios antiguos no pensados para la ubicación de los diversos equipos componentes de una red de computadoras.

Los dispositivos inalámbricos que permiten la constitución de estas redes utilizan diversos protocolos como el Wi-Fi: el estándar IEEE 802.11. El cual es para las redes inalámbricas, lo que Ethernet para las redes de área local (LAN) cableadas. Además del protocolo 802.11 del IEEE existen otros estándares como el HomeRF, Bluetooth y ZigBee.




</doc>
<doc id="6929" url="https://es.wikipedia.org/wiki?curid=6929" title="Deméter">
Deméter

Deméter o Demetra (en griego antiguo Δημήτηρ o Δημητρα, ‘diosa madre’ o quizás ‘madre distribuidora’, quizá del sustantivo indoeuropeo "*degom *mater") es la diosa griega de la agricultura, nutricia pura de la tierra verde y joven, ciclo vivificador de la vida y la muerte. Se la venera como la «portadora de las estaciones» en un himno homérico, un sutil signo de que era adorada mucho antes de la llegada de los olímpicos. El himno homérico a Deméter ha sido datado sobre el siglo VII a. C. Junto a su hija Perséfone eran los personajes centrales de los misterios eleusinos que también precedieron al panteón olímpico.

En la mitología romana se asociaba a Deméter con Ceres. Cuando se le dio a Deméter una genealogía, se dijo que era hija de los titanes Crono y Rea (ambos hijos de Gea y Urano), y por tanto hermana mayor de Zeus. A sus sacerdotisas se les daba el título de Melisas.

Es fácil confundir a Deméter con Gea, su abuela, y con Rea, su madre, o Cibeles. Los epítetos de la diosa revelan lo amplio de sus funciones en la vida griega. Deméter y Core (‘la doncella’) solían ser invocadas como "to theo" (‘las dos diosas’), y así aparecen en las inscripciones en lineal B del Pilos micénico en tiempos pre-helénicos. Es bastante probable que existiese una relación con los cultos a diosas de la Creta minoica.

Según el retórico ateniense Isócrates, los mayores dones que Deméter daba a los atenienses eran el grano, que hacía al hombre diferente de los animales salvajes, y los misterios eleusinos, que le daban mayores esperanzas en esta vida y en la otra.

En diversos contextos, se invoca Deméter con diversos epítetos:


Teócrito recordaba un papel más antiguo de Deméter:

En una estatuilla de arcilla de Gazi, la diosa de la amapola minoica lleva las cápsulas de semilla, fuente de nutrición y narcosis, en su diadema. Kerényi señala que «parece probable que la Gran Diosa Madre, que llevaba los nombres de Rea y Deméter, trajese la amapola consigo de su culto cretense a Eleusis, y es seguro que en la esfera religiosa cretense el opio se preparaba a partir de amapolas».

En honor a Deméter de Misia se celebraba una fiesta en Pellene, Arcadia. Pausanias visitó el santuario de Deméter en Misia en su viaje de Micenas a Argos, pero todo lo que pudo averiguar para explicar el arcaico nombre fue un mito de un misio epónimo que veneraba a Deméter.

Los lugares de culto a Deméter más importantes no se concentraban en ninguna región concreta del mundo griego, sino que se repartían por muchos lugares: Eleusis, Hermíone, Megara, Celeas (cerca de Fliunte), Lerna, Égila (actual Anticitera) , Muniquia, Corinto, Delos, Priene, Acragante, Pérgamo, Selinunte, Tegea, Tóricos, Díon, Licosura, Mesembria, Enna y Samotracia.

Deméter enseñó a la humanidad las artes de la agricultura: sembrar semillas, arar, recolectar, etcétera. Era especialmente popular entre las gentes del campo, en parte porque eran los beneficiarios más directos de su ayuda, y en parte porque eran más conservadores a la hora de mantener las viejas costumbres. De hecho Deméter era fundamental en la antigua religión de Grecia. Reliquias propias de su culto, como cerdos votivos de arcilla, se fabricaban ya en el Neolítico. En la época romana, aún se sacrificaba una marrana a Ceres cuando había una muerte en la familia, para purificar el hogar.

Los nombres de Deméter y Poseidón están relacionados en las primeras inscripciones en lineal B halladas en Pilos, donde aparecen como PO-SE-DA-WO-NE y DA-MA-TE en el contexto sagrado de echar a suertes. El elemento «DA» que aparece en ambos nombres no está aparentemente conectado con una raíz protoindoeuropea relacionada con la distribución de tierras y honores (compárese con el latín "dare", ‘dar’). Poseidón (cuyo nombre parece significar ‘consorte de la distribuidora’) persiguió una vez a Deméter, en su forma original de diosa-yegua. Ella se resistió a Poseidón, pero no pudo ocultar su origen divino entre los caballos del rey Oncos. Poseidón se transformó en semental y la cubrió. Deméter se puso literalmente furiosa ("Deméter Erinia") por este asalto, pero lavó su ira en el río Ladón ("Deméter Lusia"). Le dio a Poseidón una hija: Despena, cuyo nombre no podía ser pronunciado fuera de los misterios eleusinos, y un corcel de negras crines llamado Arión. En Arcadia se había adorado históricamente a Deméter como una deidad con cabeza de caballo:

El mito fundamental de Deméter, que constituye el corazón de los misterios eleusinos, es su relación con Perséfone, su hija, y ella misma de joven. En el panteón olímpico, Perséfone era hija de Zeus y consorte de Hades (Plutón para los romanos, dios de la riqueza del inframundo). Perséfone se convirtió en diosa del inframundo cuando Hades la secuestró en la tierra y la llevó con él. Perséfone había estado jugando con algunas ninfas (o Leucipe) a quienes Deméter convirtió en sirenas como castigo por no haber intervenido. La vida se paralizó mientras la deprimida Deméter (diosa de la tierra) buscaba a su hija perdida (descansando en la piedra Agelasta). Finalmente, Zeus no pudo aguantar más la agonía de la tierra y obligó a Hades a devolver a Perséfone enviando a Hermes para rescatarla. Pero antes de liberarla, Hades la engañó para que comiese seis semillas de granada, lo que la obligaba a volver seis meses cada año. Cuando Deméter y su hija estaban juntas, la tierra florecía de vegetación. Pero durante seis meses al año, cuando Perséfone volvía al inframundo, la tierra se convertía de nuevo en un erial estéril.

Por lo tanto, durante los seis meses que Perséfone estaba al lado de su madre, corresponden a la primavera y el verano, cuando la Tierra se llena de flores y calor, mientras que los seis meses restantes, Deméter está triste por la ausencia de su hija y llega el frío durante los meses que corresponden al otoño y al invierno.
Fue durante su viaje para rescatar a Perséfone del inframundo cuando Deméter reveló los misterios eleusinos. En una versión alternativa, Hécate rescató a Perséfone. En otras versiones Perséfone no era engañada para comer las semillas de granada sino que decidía comerlas por sí misma. Algunas versiones afirman que comió cuatro semillas en lugar de seis. En cualquier caso, el resultado final es la sucesión del verano, la primavera, el otoño y el invierno.

Mientras Deméter buscaba a su hija Perséfone, habiendo tomado la forma de una mujer anciana llamada Doso, recibió la hospitalaria bienvenida de Céleo, el rey de Eleusis en Ática. Céleo le pidió que cuidase de Demofonte y Triptólemo, los hijos que había tenido con Metanira.

Como regalo a Céleo por su hospitalidad, Deméter planeó convertir a Demofonte en un dios, cubriéndolo y ungiéndolo con ambrosía, respirando suavemente sobre él mientras lo sostenía entre sus brazos y su pecho, y haciéndolo inmortal quemándolo sobre carbones al rojo vivo en la chimenea del hogar familiar cada noche, a espaldas de sus padres.

Deméter no pudo completar el ritual porque Metanira sorprendió una noche a su hijo en el fuego y chilló asustada, lo que enfureció a Deméter, quien lamentó que los mortales no entendiesen el concepto y el ritual.

En lugar de hacer inmortal a Demofonte, Deméter decidió enseñar a Triptólemo el arte de la agricultura y, a través de él, el resto de Grecia aprendió a plantar y segar cultivos. Triptólemo cruzó el país volando en un carro alado mientras Deméter y Perséfone cuidaban de él, ayudándolo a completar su misión de educar a Grecia entera en el arte de la agricultura.

Más tarde, Triptólemo enseñó a Linco, rey de Escitia, el cultivo del trigo, pero Linco rehusó enseñarlas a sus súbditos, y trató de matar a Triptólemo. Deméter lo transformó en lince.

También Fítalo recibió hospitalariamente a Deméter y como premio la diosa le dio la higuera.

Algunos investigadores creen que la historia de Demofonte está basada en una leyenda popular prototípica anterior.

La identificación con la diosa Isis está en el hecho de que las dos deben emprender una búsqueda, su hija Core en el caso de Deméter y su esposo Osiris en el caso de Isis, produciéndose en los dos casos una paralización de la vida en la naturaleza, por la llegada del invierno en un caso y por el final de la crecida del río en el otro, hasta que se produce el encuentro y la naturaleza vuelve a renacer. Posteriormente, las sacerdotisas grecorromanas de Isis debían formarse previamente en los misterios eleusinos a través del modelo de las sacerdotisas de Deméter, las canéforas.

Su asimilación con la diosa fenicia Astarté, a través de Isis, fue facilitada por las relaciones comerciales entre el Antiguo Egipto y la ciudad fenicia de Biblos, ya que según la mitología egipcia, esta encontraría el cofre con el cadáver de su esposo en esta antigua ciudad.

Yasión fue un hijo de Zeus y Electra que yació con Deméter en un campo arado de Trípolo, en Creta, y fruto de esta unión se dice que nacieron Pluto y Filomelo. Según la "Odisea", Zeus lo fulminó con un rayo, pero que el mito sitúe los hechos en Creta es un indicio de que los helenos sabían que este suceso le ocurrió a una Deméter más antigua.

Deméter puso a Limos, el dios de la hambruna, en las tripas de Erisictón, haciendo que estuviese permanentemente hambriento, como castigo por cortar árboles en una arboleda sagrada.

Se solía retratar a Deméter subida a un carro, y asociada con frecuencia a imágenes de la cosecha, incluyendo flores, fruta y grano. A veces se la pintaba también con Perséfone (su hija).

Era y es célebre la estatua en mármol de esta diosa que se hallaba en la ciudad de Cnido y que actualmente se encuentra en el Museo Británico de Londres.














</doc>
<doc id="6930" url="https://es.wikipedia.org/wiki?curid=6930" title="Prefijos del Sistema Internacional">
Prefijos del Sistema Internacional

Los prefijos del Sistema Internacional se utilizan para nombrar a los múltiplos y submúltiplos de cualquier unidad del SI, ya sean unidades básicas o derivadas. 
Estos prefijos se anteponen al nombre de la unidad para indicar el múltiplo o submúltiplo decimal de la misma; del mismo modo, los símbolos de los prefijos se anteponen a los símbolos de las unidades.

Los prefijos pertenecientes al SI los fija oficialmente la Oficina Internacional de Pesas y Medidas ("Bureau International des Poids et Mesures"), de acuerdo con el cuadro siguiente:

Ejemplos:

Estos prefijos no son exclusivos del SI. Muchos de ellos, así como la propia idea de emplearlos, son anteriores al establecimiento del Sistema Internacional en 1960; por lo tanto, se emplean a menudo en unidades que no pertenecen al SI.





Las siguientes combinaciones de prefijos y cantidades no se emplean regularmente, incluso en los ámbitos de la ciencia y de la ingeniería:


El símbolo "K" (en mayúscula) se emplea a menudo con el significado de múltiplo de mil; por lo tanto, puede escribirse "sueldo de 40K" (de 40000 euros) o "el problema del año 2K". A pesar del empleo habitual, este empleo de la K mayúscula no es correcto en el SI, ya que es el símbolo de unidades de temperatura Kelvin. El empleo de la abreviatura Ki se emplea para representar el prefijo binario kibi (2 = 1024).


Los múltiplos de la unidad son habituales en el ámbito de las computadoras, siendo empleados en la información y unidades de almacenamiento tipo bit y byte. Siendo 2 = 1024 y 10 = 1000, los prefijos del SI se emplean siguiendo la ley de los prefijos binarios, como se observa en las siguientes líneas.

k = 2 = 1 024
M = 2 = 1 048 576
G = 2 = 1 073 741 824
T = 2 = 1 099 511 627 776
P = 2 = 1 125 899 906 842 624

De todas formas, estos prefijos mantienen el significado de las potencias de 1000 cuando de lo que se trata es de expresar la velocidad de la transmisión de datos (cantidad de bits): la red Ethernet de 10 Mbit/s es capaz de transmitir 10 000 000 bit/s, y no 10 485 760 bit/s. El problema se acrecienta por no ser las unidades de información bit y byte unidades del SI. En el SI el bit, el byte, el baudio o la cantidad de signos se darían en hercios. Aunque es más claro emplear "bit" para el bit y "b" para el byte, a menudo se emplea "b" para el bit y "B" para el byte (en el SI, B es la unidad del belio, siendo la del decibelio dB).

De esta forma, la Comisión Electrotécnica Internacional ("International Electrotechnical Commission" —IEC—) eligió nuevos prefijos binarios en 1998, que consisten en colocar un 'bi' tras la primera sílaba del prefijo decimal (siendo el símbolo binario como el decimal más una 'i'). Por lo tanto, ahora un kilobyte (1 kB) son 1000 byte, y un kibibyte=(1 KiB)= 2 bytes = 1024 octetos o bytes. De la misma forma, un mebibyte= MiB= 2bytes, un gibibyte= 1 GiB= 2bytes, tebi (Ti; 2), pebi (Pi; 2) y exbi (Ei; 2). Aunque el estándar del IEC nada diga al respecto, los siguientes prefijos alcanzarían hasta zebi (Zi; 2) y yobi (Yi; 2). Hasta el momento el empleo de estos últimos ha sido muy escaso.

Continuando hacia atrás en el alfabeto, tras "zetta" y "yotta", entre las propuestas para nombrar a los siguientes números grandes se encuentran las palabras "xenta" y "xona" (entre otras), siendo esta última una modificación del sufijo numérico proveniente del latín ; la propuesta para los siguientes números pequeños también comenzaría por "x".

Siguiendo la norma de abreviación de los prefijos (las letras mayúsculas del latín para cantidades grandes y las minúsculas para cantidades pequeñas), y a pesar de no haber un acuerdo en el nombre completo, podrían emplearse, sin ambigüedad, los siguientes prefijos: X, W, V, x, w, v. El símbolo del prefijo de las cantidades pequeñas siguiente en ese orden sería la "u", una sustitución de "µ" (símbolo del "micro" o "micra") basada en el (ISO 2955).

Aun así, no todos los lectores entienden muchos de los prefijos oficiales, y menos aún sus extrapolaciones. Por ello, y a diferencia de la escritura para uso personal, es conveniente escribir una pequeña explicación cuando se trata de un artículo que han de comprender terceros.

Otra propuesta para xenta/xona es "novetta", del italiano "nove". Sin embargo, esta propuesta no tiene en cuenta el orden alfabético

Existen propuestas para llevar más allá la armonización de los símbolos. Según las mismas, los símbolos de los prefijos "deca", "hecto" y "kilo" en lugar de "da", "h" y "k" deberían ser "D", "H" y "K", respectivamente. De la misma forma, algunos hablan de la supresión de todos aquellos prefijos que no entran en el esquema 10, es decir, hecto, deca, deci y centi. La CGPM ha aplazado, de momento, la toma de una decisión concreta sobre ambas propuestas.

Hay que tener precaución en el empleo de los sufijos con unidades cuyas potencias no son ±1. Antes que la potencia siempre se tiene en cuenta el prefijo. Para medir volúmenes aún se emplea el término litro, equivalente a la milésima parte de un metro cúbico (), es decir, un decímetro cúbico (1 dm³). Un centímetro cúbico (cm³) es la millonésima parte de un metro cúbico (). Y un milímetro cúbico (mm³) es la milmillonésima parte de un metro cúbico ().

De acuerdo con los principios generales adoptados por (ISO 31), el Comité internacional de pesos y medidas (CIPM) recomienda que las siguientes reglas sean observadas cuando se utilizan los prefijos antedichos:








</doc>
<doc id="6931" url="https://es.wikipedia.org/wiki?curid=6931" title="Eros">
Eros

En la mitología griega, Eros (en griego antiguo Ἔρως) es el dios primordial responsable de la atracción sexual, el amor venerado también como un dios de la fertilidad. En algunos mitos era hijo de Nicte y Erebo pero también se decía que fue de Afrodita y Ares aunque según "El banquete" de Platón fue concebido por Poros (la abundancia) y Penia (la pobreza) en el cumpleaños de Afrodita. Esto explicaba los diferentes aspectos del amor.

A veces era llamado "Eleuterios" (Ἐλευθερεύς), ‘el libertador’ como Dioniso. Su equivalente romano era Cupido (‘deseo’), también conocido como Amor.

Su estatua podía encontrarse en las palestras, uno de los principales lugares de reunión de los hombres con sus amados, y a él hacían sacrificios los espartanos antes de la batalla. Meleagro recoge este papel en un poema conservado en la "Antología Palatina": «La reina Cipria, una mujer, aviva el fuego que enloquece a los hombres por las mujeres, pero el propio Eros convence la pasión de los hombres por los hombres».

En el pensamiento griego parece haber dos aspectos en la concepción de Eros. En el primero es una deidad primordial que encarna no solo la fuerza del amor erótico sino también el impulso creativo de la siempre floreciente naturaleza, la Luz primigenia que es responsable de la creación y el orden de todas las cosas en el cosmos. En la "Teogonía" de Hesíodo, el más famoso de los mitos de creación griegos, Eros surgió tras el Caos primordial junto con Gea, la Tierra, y Tártaro, el Inframundo. De acuerdo con la obra de Aristófanes "Las aves", Eros brotó de un huevo puesto por la Noche (Nix), quien lo había concebido con la Oscuridad (Érebo). En los misterios eleusinos era adorado como Protógono (Πρωτόγονος), el ‘primero en nacer’.

Posteriormente aparece la versión alternativa que hacía a Eros hijo de Afrodita con Ares (más comúnmente), Hermes o Hefesto, o de Poros y Penia, o a veces de Iris y Céfiro. Este Eros era un ayudante de Afrodita, que dirigía la fuerza primordial del amor y la llevaba a los mortales. 

En algunas versiones tenía dos hermanos llamados Anteros, la personificación del amor correspondido, e Hímero, la del deseo sexual.

La adoración de Eros era poco común en la Grecia más antigua, pero más tarde llegaría a estar muy extendida. Fue adorado fervientemente por un culto a la fertilidad en Tespia y jugó un importante papel en los misterios eleusinos. En Atenas, compartió con Afrodita un culto muy popular y se le consagraba el cuarto día de cada mes.

Eros, muy enfadado con Apolo al haber bromeado este sobre sus habilidades como arquero, hizo que se enamorase de la ninfa Dafne, hija de Ladón, quien lo rechazó. Dafne rezó al dios río Peneo pidiendo ayuda, y fue transformada en un árbol de laurel, que se consagró a Apolo.

La historia de Eros y Psique tiene una larga tradición como cuento popular del antiguo mundo grecorromano mucho antes de que fuera escrita, por primera vez en la novela latina de Apuleyo "El asno de oro", siendo una evidente e interesante combinación de roles. La propia novela tiene el estilo picaresco romano, aunque Psique y Afrodita retienen su carácter griego, siendo Eros el único cuyo papel procede de su equivalente en el panteón romano.

La historia es narrada como digresión y paralelo estructural al argumento principal de la novela de Apuleyo. Narra la lucha por el amor y la confianza entre Eros y la princesa Psique, cuyo nombre es difícil de traducir apropiadamente, pues trasciende los idiomas griego y latino, pero puede considerarse que significa ‘alma’, ‘mente’, o mejor ambas. Afrodita estaba celosa de la belleza de la mortal Psique, pues los hombres estaban abandonando sus altares para adorar en su lugar a una simple mujer, y así ordenó a su hijo Eros que la hiciera enamorarse del hombre más feo del mundo. Pero el propio Eros se enamoró de Psique, y la llevó por arte de magia a su casa. Su frágil paz fue arruinada por una visita de las celosas hermanas de Psique, quienes hicieron que ésta traicionara su confianza. Herido, este la expulsó y Psique vagó por la tierra, buscando a su amor perdido. Apuleyo atribuye en su obra una hija de Eros a Psique, Hedoné, cuyo nombre significa ‘placer’.












</doc>
<doc id="6932" url="https://es.wikipedia.org/wiki?curid=6932" title="Reconquista">
Reconquista

Se denomina Reconquista al período de la historia de la península ibérica de aproximadamente 780 años entre la conquista omeya de Hispania en 711 y la caída del Reino nazarí de Granada ante los reinos cristianos en expansión en 1492. La conquista completa de Granada marca el final del periodo.

La historiografía tradicional utiliza el término «Reconquista» a partir del para lo que previamente se conocía como «restauración» de los reinos cristianos visigodos, entendida como conquista de nuevos terrenos por unas nuevas monarquías que pretendían restablecer un orden político y religioso preexistente.

El comienzo de la Reconquista se marca con la batalla de Covadonga (718 o 722), la primera victoria conocida de las fuerzas militares cristianas en la península ibérica desde la intervención militar de las fuerzas combinadas árabe-bereber de 711. En esa pequeña batalla, un grupo liderado por el noble Pelayo derrotó a una patrulla musulmana en las montañas de la cordillera cantábrica y estableció el reino cristiano independiente de Asturias. La Reconquista terminó con la conquista del emirato de Granada, el último estado musulmán en la península, en 1492, la conquista y caída fue precedida por las Capitulaciones de Granada o Tratado de Granada (1491).

Después de 1492 toda la península fue controlada por gobernantes cristianos. La Reconquista fue seguida por el Edicto de Granada (1492) que expulsó a los judíos de Castilla y Aragón que no se convirtieron al cristianismo, y una serie de edictos (1499-1526) que forzaron las conversiones de los musulmanes en España, y en 1609-1610, su destierro. Desde mediados del , la idea de una «reconquista» se arraigó en España asociada a su creciente nacionalismo y colonialismo.

El término «Reconquista» ha sido muy discutido por algunos académicos e incluso su uso ha sido cuestionado por supuestamente no responder a la realidad histórica medieval peninsular.

La «Reconquista» constituyó para los distintos reinos y señoríos surgidos en el aislamiento del norte montañoso de la Península un proceso restaurador y liberador, no solo del territorio, sino de la numerosa población cristiana hispano-visigoda (mozárabes), que permaneció durante siglos en el territorio ocupado. Resultaban ser los verdaderos herederos del reino visigodo, y su apelación constante al auxilio de los reinos cristianos, suponía para las autoridades musulmanas un problema que surgía periódicamente y que era resuelto con persecuciones y deportaciones de distinto grado.

Según esta misma visión tradicional la temprana reacción en la cornisa cantábrica en contra del islam (Don Pelayo rechazó a los sarracenos en Covadonga apenas siete años después de que atravesaran el estrecho de Gibraltar), y el rechazo del territorio actualmente francés después de la batalla de Poitiers del año 732, sustentan la idea de que la Reconquista sigue casi inmediatamente a la conquista árabe. Incluso, «gran parte de dicha cornisa cantábrica jamás llegó a ser conquistada», lo cual viene a justificar la idea de que la conquista árabe y la reconquista cristiana, de muy diferente duración (muy corta la primera y sumamente larga la segunda), se superponen. Teniendo en cuenta esta posible superposición, podría considerarse como una sola etapa histórica, sobre todo si tenemos en cuenta que la batalla de Guadalete, la primera batalla por defender el reino visigodo en el año 711, marca el inicio de la conquista musulmana.

Sin embargo, algunos académicos han manifestado que el término de «Reconquista» podría ser inexacto, pues los reinos cristianos que «re-conquistaron» el territorio peninsular se constituyeron con posterioridad a la invasión islámica, a pesar de los intentos de estas monarquías por presentarse como herederas directas del antiguo reino visigodo. Se trataría más bien de un afán de legitimación política de estos reinos, que de hecho se consideraban reales herederos y descendientes de los visigodos, así como de un intento por parte de los reinos cristianos de justificar sus conquistas, por otro lado esta versión choca con el hecho indiscutible de la finalidad religiosa de la reconquista por restablecer el catolicismo en toda la península. 
Por otro lado, el término parece confuso, considerando que tras el derrumbe del Califato a comienzos del , los reinos cristianos optaron por una política de dominio tributario –parias– sobre las taifas en lugar de una clara expansión hacia el sur, y las pugnas entre las diferentes coronas –y sus luchas dinásticas–, que solo alcanzaron acuerdos de colaboración contra los musulmanes en momentos puntuales.

Durante el Siglo de Oro algunos poetas definieron y denominaron a los españoles como «godos» (como dijo Lope de Vega: «eah, sangre de los godos»), y durante las guerras de independencia en América, eran también así llamados por los independentistas americanos (de ahí procede el uso despectivo que se emplea en Canarias para referirse al español peninsular). Es por ello, que los críticos del término lo consideran un concepto parcial, pues solo transmite la visión cristiana y europea de este complejo proceso histórico, soslayando el punto de vista de los musulmanes andalusíes; también puede decirse que en el lado cristiano existía conciencia de «reconquista».

En su "España invertebrada" (1922), José Ortega y Gasset, desde la filosofía afirmaba que «Un soplo de aire africano los barre [a los visigodos] de la Península (...) Se me dirá que, a pesar de esto, supimos dar cima a nuestros gloriosos ocho siglos de Reconquista. Y a ello respondo ingenuamente que yo no entiendo cómo se puede llamar reconquista a una cosa que dura ocho siglos». Eloy Benito Ruano, medievalista español, contradijo a Ortega afirmando que la larga duración, ochocientos años, no es un argumento de peso para invalidar la Reconquista como fenómeno: «Argumento que, a nuestro juicio, puede rebatirse con la invocación de tantos procesos y fenómenos históricos como pueden ser, en sus diversas proporciones, el cristianismo, el feudalismo, la institución monárquica... Sujetos todos hoy incluibles en la moderna concepción braudeliana (de Braudel) de la "longue durée".»

En 1965 los historiadores Marcelo Vigil y Abilio Barbero de Aguilera propusieron que los pueblos del norte peninsular presentaban en la Alta Edad Media un bajo nivel de romanización y cristianización. Según estos autores, estos pueblos, que habían resistido tanto a romanos como visigodos, rechazarían del mismo modo la invasión árabe. Teniendo esto en cuenta, estos autores afirmaron que: «el fenómeno histórico llamado reconquista no obedeció en sus orígenes a motivos puramente políticos y religiosos (...). Debió su dinamismo a ser la continuación de un movimiento de expansión de pueblos que iban alcanzando formas de desarrollo económico y social superiores». Aunque tuvo cierta acogida entre algunos historiadores españoles del momento como José Luis Martín Rodríguez, otros como Claudio Sánchez-Albornoz rechazaron esta propuesta desde el mismo momento de su publicación. En 1992, José Miguel Novo Güisán publicó un trabajo donde afirmaba que sí había un alto grado de romanización en los pueblos del norte peninsular ya en el Bajo Imperio Romano, contradiciendo la propuesta de Marcelo Vigil y Abilio Barbero.

Escritores como Ignacio Olagüe Videla, en "La revolución islámica en Occidente" (1974), consideran que la invasión militar árabe es un mito y sostienen que la creación de Al-Ándalus fue el resultado de la conversión de gran parte de la población hispana al islam. Estas tesis han sido estudiadas por el conocido arabista González Ferrín en su obra "Historia general de Al-Andalus", en la que sobre la Reconquista dice «que en verdad nunca existió». También plantea que al-Ándalus «constituye un eslabón insustituible de la historia europea». Olagüe afirma en "La revolución islámica en Occidente": «Creen los historiadores que ha sido invadida España por unos nómadas llegados desde el Hedjaz, sin habérseles ocurrido medir en un mapa el camino que era menester andar, ni tampoco estudiar en obras de geografía los obstáculos que era necesario vencer en tan larguísimo viaje». Las hipótesis de Olagüe no cuentan con ningún apoyo significativo en la historiografía actual. La obra de Olagüe ha sido calificada de «historia ficción» y rechazada en círculos académicos. La arqueología y los textos antiguos desmienten esta teoría, ya que son abundantes las fuentes clásicas y los restos arqueológicos que prueban que la conquista islámica fue violenta, con numerosas batallas y asedios, con poblaciones enteras exterminadas por los ejércitos islámicos, como en Zaragoza o Tarragona durante la Conquista del norte. Además, tanto en fuentes cristianas como musulmanas, aparecen numerosas citas acerca de los elevados impuestos especiales que debían pagar solo los no musulmanes, como la "gizya", "harag," así como leyes que tratan con inferioridad a los no musulmanes.

Los medievalistas franceses Charles-Emmanuel Dufourcq y Jean Gautier-Dalché, en su obra "La España cristiana en la Edad Media" (1983) califican al proceso de conflictos entre cristianos y musulmanes como reconquista:

Derek William Lomax, escritor e hispanista británico especializado en la literatura medieval española, escribió en su libro "La Reconquista" (1984):

El catedrático arabista Serafín Fanjul, en sus libros "Al-Andalus contra España" (2000) y "La quimera de Al-Andalus" (2004), desmonta los mitos de una invasión poco violenta, la idealización de la convivencia de culturas o religiones en Al-Ándalus y usa el término reconquista, entendiéndolo como la recuperación por parte de las comunidades cristianas del territorio invadido por los musulmanes. En "Al-Andalus contra España", Fanjul afirma: «Pero será en el reinado de Alfonso III (866-911) y al socaire de la incipiente reconquista, cuando la "Crónica profética" anuncie ya la vuelta del reino de los godos y la recuperación de todo el suelo de España bajo la égida del mismo rey».

Eloy Benito Ruano, historiador medievalista español, escribió en el año 2002: «Exaltada en general su valoración a lo largo de los siglos, tanto por su propia cronística como por la simple intuición de la masa española, esta versión ha venido siendo objeto de un generalizado en "ingenuo" (sincero) "patriotismo", por lo general perfectamente lícito». Sobre las reacción en contra de la visión tradicional de la Reconquista, opina que no son argumentos válidos la larga duración del proceso, ya que otros fenómenos de la historia han sido igual de largos, ni la supuesta ausencia de una ideolgía reivindicadora en la élite, ya que estuvo presente por escrito a partir de la Crónica albeldense (año 833), ni la falta de continuidad en el proceso, ya que el espíritu de confrontación, según su opinión, siempre estuvo presente. Eloy cita a la historiadora francobelga Adeline Rucquoi «La Reconquista es una realidad y tiene su historia».

Julio Valdeón Baruque, medievalista y catedrático de Historia Medieval de la Universidad de Valladolid, define la Reconquista en su obra "El concepto de España" (2006) como «recuperación»:

El historiador Domínguez Ortiz, en su trabajo "España. Tres milenios de historia" (2000), explica lo dilatado del proceso con una falta de solidaridad del mundo cristiano en la causa peninsular frente a los musulmanes: «La Conquista y posterior Reconquista (...) cuatro años de Conquista, seis siglos de Reconquista. (...) una disimetría tan llamativa ha de buscarse no sólo en la diversa actitud de las poblaciones concernidas, sino en una mayor solidaridad de los musulmanes a uno y otro lado del Estrecho frente a la ayuda muy escasa (...) que a la España cristiana llegó a través de los pasos pirenaicos».

Los medievalistas García de Cortázar y Sesma Muñoz, en su trabajo "Manual de Historia Medieval" (2014), señalan: «Entendido como un proceso de colonización, la Reconquista fue resultado de una combinación de estímulos demográficos, económicos, ideológicos, políticos y militares, y se desarrolló entre comienzos del siglo y finales del XIII».

El medievalista español Ladero Quesada opina sobre el término reconquista que, aunque la palabra comenzó a usarse a comienzos del siglo , ya existía una ideología afín a este concepto empleada por las monarquías de los reinos medievales cristianos en su avance peninsular:

Y en su obra "Lecturas sobre la España histórica" (1998), Ladero sentencia:

Manuel González, historiador español, señaló en 2005: «La Reconquista en manos de unos y de otros se había convertido en un tópico retóricamente exaltado y objeto de culto o en uno de esos conceptos que había que extirpar y combatir. Creo que ambas posturas son igualmente erróneas, porque ambas adolecen del mismo defecto: el de reducir la enorme complejidad del hecho histórico de la Reconquista a una sola de sus múltiples facetas». Y sentencia: «La idea de reconquista, a despecho de modernas teorías y hasta el descrédito que en determinados círculos académicos e intelectuales haya podido tener o tenga, sigue en pie».

Federico Ríos Saloma, doctor en Sociedad y cultura medieval, afirma en un artículo publicado en 2008 que el concepto de reconquista apareció por primera vez en 1646 en la obra "Histórica relación del Reyno de Chile y de las misiones y ministerios que exercita la Compañía de Jesús". Aunque reconoce que en la Crónica profética del año 883 ya se plantea un deseo de expulsión de los musulmanes de la península ibérica, opina que el proyecto de Alfonso III tenía más un carácter más restaurador que de recuperación. Federico señala tres corrientes actuales en el debate en torno a lo que fue la reconquista: La primera corriente está representada por Derek Lomax y Manuel González. Consideran que la conquista militar del territorio andalusí debía interpretarse como una reconquista, ya que desde el reinado de Alfonso III "la reconquista era algo más que un proyecto nebuloso" y, además, un hecho histórico con una dimensión espiritual, material y económica. La segunda corriente es defendida por Thomas Deswarte: deduce que la conquista militar fue una fase previa a la restauración política y eclesiástica promovida por los monarcas astur-leoneses, aferrados a una concepción singular del mundo por la herencia política visigoda y con elementos del pensamiento tardo-romano y agustiniano. La tercera corriente, concebida desde un enfoque materialista (marxista), se halla representada por Abilio Barbero, Marcelo Vigil, José María Mínguez y Joseph Torró, y entiende la conquista militar de al-Ándalus como una fase más del proceso general de expansión del Occidente cristiano ocurrido a lo largo de los siglos del alto y pleno medievales

En 2010 Eduardo Manzano Moreno destacó que las crónicas de la época de Alfonso III de Asturias se escribieron en un momento —la segunda mitad del y principios del — en el que Al-Ándalus estaba atravesando una profunda crisis, lo que les hizo pensar a los cronistas que el fin de la presencia musulmana en la península ibérica estaba cerca. «Cuando esas circunstancias se frustraron, el programa se reajustó de acuerdo con las nuevas condiciones que la historia de los siglos posteriores fue aportando, aunque es evidente que la idea de "pérdida" y de "recuperación" de lo perdido siguió estando presente a lo largo de los siglos». Pero esas menciones posteriores «atenuaron muy considerablemente el componente religioso. Así, en pleno siglo XIV don Juan Manuel decía que "hay guerra entre los cristianos y los moros y la habrá hasta que hayan tomado los cristianos las tierras que los moros tienen capturadas", pero negaba que el conflicto tuviera un trasfondo religioso ya que "ni por la ley, ni por la secta que tienen" [los moros] "habría guerra entre ambos"».

Años más tarde, el mismo Eduardo Manzano Moreno defendió que el término «reconquista» presupone equivocadamente la continuidad entre los reinos y condados cristianos del norte y la Monarquía visigoda anterior a la conquista musulmana de la península ibérica, con lo que Al-Ándalus habría sido simplemente un paréntesis histórico en la evolución peninsular. Este historiador también cuestiona el uso del término «repoblación» pues añade la noción de que Al-Ándalus «fue convenientemente borrado de manera súbita y radical después de la ocupación cristiana». Así, Manzano Moreno propone abandonar el binomio «reconquista»/«repoblación» para referirse a la expansión cristiana, lo que «no implica ―matiza― que la ocupación de enclaves y territorios no alterara sustancialmente situaciones previas, ni que hubiera importantes abandonos de ciertos enclaves, ni que, en fin, se impusieran formas de encuadramiento nuevas que acabaron configurando los caracteres presentes en las sociedades bajomedievales. Hubo, pues, ciertas continuidades, pero también cambios dramáticos, y estos últimos no hicieron más que incrementarse con el paso del tiempo hasta llegar a hacer irreconocible la antigua sociedad andalusí».

En 2018 la revista "Al-Ándalus y la historia" organizó un debate sobre el uso término «Reconquista» en el que participaron Alejandro García Sanjuan, de la Universidad de Huelva, y Carlos de Ayala Martínez, de la Universidad Complutense de Madrid. El primero rechazó el uso del término debido a su carga ideológica «nacional-católica» que imposibilita su aplicación a la realidad medieval peninsular. El segundo, tras reconocer las limitaciones que plantea el uso del término, defendió su validez entendido, no como un hecho histórico, sino como la ideología que crearon los reinos cristianos del norte para legitimar sus conquistas sobre el territorio andalusí.

Por otro lado, para los cristianos peninsulares la reconquista no terminó con la conquista de Granada, sino que continuó en el norte de África septentrional. Con el objeto de "restituir" el territorio de la Mauritania hispana que era parte de Hispania desde la división de Diocleciano. Las conquistas de los Reyes Católicos en el norte de África (Melilla, Cazaza, Mazalquivir, Orán), y anteriormente las de los reyes portugueses, (Ceuta, Tánger) estaban también basadas en el mismo principio de restauración. Sin embargo, la historiografía romántica española del siglo XIX omitió la continuidad que tenía para los cristianos peninsulares el norte de África restringiendo el concepto de Hispania a "la península" y como consecuencia que la reconquista terminó cuando se tomó la ciudad de Granada.

En 711 se produjo en la península ibérica la primera incursión musulmana, compuesta por 7000 bereberes enviados por el gobernador Musa ibn Nusair y comandados por Tárik. Zarparon desde el norte de África y entraron en la península ibérica por Gibraltar (que precisamente debe su nombre actual a Tárik, "«Jebel al-Tarik"»). Roderic o Roderico (Don Rodrigo), uno de los últimos reyes visigodos, intentó rechazar esta incursión, siendo derrotado y perdiendo la vida en la batalla de Guadalete (o laguna de la Janda). Ese mismo año Tarik entró en Toledo, la capital de los visigodos. Tárik fue llamado a informar al califato, viajando a la capital Damasco, y nunca más volvió. Su lugar lo ocupó el gobernador Abd al-Aziz, más conocido por la historiografía como «Musa». En el año 712 Musa cruzó el estrecho con más de guerreros musulmanes, tanto árabes como bereberes, y conquistó Sevilla, Mérida y Zaragoza, realizando además incursiones en Galicia, León y Asturias. Con Musa da comienzo lo que se conoce como Emirato dependiente. Los invasores se sirvieron del sistema de calzadas romano para avanzar entre el año 711 y el 714 por el territorio, dejando guarniciones en puntos clave.

A partir de este momento los musulmanes empezaron una política de tratados con los nobles visigodos, como el de Teodomiro en Murcia, que unido a una política relativamente tolerante con judíos y cristianos, les permitió controlar la mayor parte de la península en pocos años. El pacto entre Teodomiro y Abdelaziz, firmado el 5 de abril de 713, donde se mantenía en el poder a las viejas autoridades hispanogodas a cambio de algunas concesiones, lealtad a Damasco y el pago de tributos:

Poco tiempo después del inicio de la invasión apareció en el seno de los vencedores diversas disputas entre árabes y bereberes, y dentro de los árabes, entre "qaysíes" y yemeníes. En 716 Abd al-Aziz fue asesinado en Sevilla y se inició una crisis tal que en los siguientes cuarenta años (hasta el año 756, con la llegada de Abderramán I), se sucedieron veinte gobernadores, período conocido como Emirato dependiente. Con el fin de acabar una revuelta bereber, quienes se sentían marginados por la mayoría árabe, desde Siria se mandaron tropas árabes con el propósito de sofocarla. 

En el año 716, con el centro de poder ya establecido en Córdoba, los árabes comenzaron a dirigir sus fuerzas hacia los Pirineos para tratar de entrar en el territorio de la antigua Galia romana. Entre los años 711 y 725 los musulmanes ocupan la Península salvo pequeños núcleos cristianos en Asturias y los Pirineos. En el año 720 llegaron incluso a tomar la ciudad de Narbona. Sin embargo su avance por el reino franco se vería frenado por la derrota en Poitiers en el año 732. Entre los años 751 y 756, una serie de malas cosechas fuerzan el repliegue de las tropas musulmanas hacia el sur del Duero, permitiendo la reorganización y recuperación de los cristianos del norte.

Crónica mozárabe del año 754 donde se narra la experiencia del momento de la conquista musulmana de la península ibérica desde el punto de vista cristiano:

La veloz y contundente invasión islámica, además de los factores que propiciaron la expansión mundial del islam, se explica por las debilidades que afectaban al reino visigodo:


Tras la invasión, la resistencia cristiana cristaliza en dos focos de los cuales Asturias fue el más precoz.

Tras la invasión islámica, una minoría cristiana escapó al norte de la península ibérica. Del núcleo asturiano surgió una monarquía cuyo poder les permitiría avanzar en los años siguientes hasta la línea del Duero, entre Oporto y Simancas. En el año 718 se sublevó un noble llamado Pelayo (718-37). Fracasó, fue hecho prisionero y enviado a Córdoba (los escritos usan la palabra «Córdoba», pero esto no implica que fuera la capital, ya que los árabes llamaban "Córdoba" a todo el califato). Sin embargo, consiguió escapar y organizó una segunda revuelta en los montes de Asturias, que empezó con la batalla de Covadonga en 722. Esta batalla se considera el comienzo de la Reconquista. La interpretación es discutida: mientras que en las crónicas cristianas aparece como «una gran victoria frente a los infieles, gracias a la ayuda de Dios», los cronistas árabes la describen como un enfrentamiento con un reducido grupo de cristianos, a los que tras vencer se desiste de perseguir al considerarlos inofensivos. Probablemente fuera una victoria cristiana sobre un pequeño contingente de exploración. La realidad es que esta victoria de Covadonga, por pequeñas que fueran las fuerzas contendientes, tuvo una importancia tal que creó en torno a Don Pelayo, un foco de independencia del poder musulmán que le permitió mantenerse independiente en Oviedo e ir incorporando nuevas tierras a sus dominios. Con Alfonso I de Asturias (739-757) el reino se benefició de las dificultades de al-Andalus y de la inmigración de cristianos venidos del valle del Duero, que quedó prácticamente despoblado. Este aporte humano permitió a los reyes de Asturias ampliar sus dominios.

En cualquier caso, los árabes desistieron de controlar la zona más septentrional de la península, dado que en su opinión, dominar una región montañosa de limitados recursos e inviernos extremos no valía la pena. Además, la fuerte resistencia de los francos en Aquitania y Septimania les impidió destinar fuerzas a la cornisa cantábrica. Los cristianos de la zona no representaban un peligro, y controlar el extremo más alejado supondría más costes que beneficios. Las poblaciones astures y cántabras emprendieron una campaña de resistencia y depredación contra las tierras del Duero. El yerno de Pelayo, Alfonso I de Asturias, aprovechó la crisis interna del emirato de Córdoba para extender el control desde Galicia a Álava. La sorprendente expansión y consolidación del minúsculo reino con el largo reinado de Alfonso II (791-842), quien ya pudo vencer en batalla campal a los musulmanes, recuperó conscientemente la herencia visigoda ("officium palatinum"), favoreció la creación de monasterios y estableció la capital en Oviedo. Esta situación preocupó a las autoridades califales, por lo que se llevaron a cabo sucesivas incursiones (en tiempos de Alfonso II, se hizo una cada año en territorio asturiano), pero el reino sobrevivió y se siguió expandiendo, con sonoras victorias como la batalla de Lutos, Polvoraria y la toma de Lisboa en 798. La aparición del presunto sepulcro del apóstol Santiago en Compostela sirvió para fortalecer la identidad e ideología del reino.

El reino de Asturias era inicialmente de carácter astur, pero fue sometido en sus últimas décadas a una sucesiva gotificación debida a la influencia de los inmigrantes de cultura hispanogoda que huían desde el sur. Asimismo, fue un referente para parte del espacio cultural europeo con la batalla contra el adopcionismo, al romper con el obispado de Toledo. El reino estuvo por épocas muy vinculado al reino de los francos, sobre todo a raíz del «descubrimiento» del supuesto sepulcro del apóstol Santiago. Esta idea «propagandista» consiguió vincular a la Europa cristiana con el pequeño reino del norte, frente al sur islamizado. La emigración de clérigos mozárabes a Asturias permitió crear la doctrina que consideraba al rey como heredero de los visigodos, con derechos a avanzar hacia el sur sobre los territorios de Al-Andalus. Esta doctrina proporcionó a la nueva monarquía elementos propios de las tradiciones godas.

Se originó a partir de la resistencia carolingia (el caudillo franco Carlos Martel había rechazado la invasión musulmana de Aquitania en la batalla de Poitiers en 732). Posteriormente su sucesor, Carlomagno, trató de hacer retroceder a los musulmanes mediante una expedición en el valle del Ebro, consiguiendo conquistar Barcelona y Gerona. Con todo, la expedición fue un desastre tras la derrota ante los vascones en la batalla de Roncesvalles, tal y como narra la "Chanson de Roland". Tras este fracaso creó la Marca Hispánica como barrera defensiva (frontera militar del sur), que con el tiempo dio origen a otros focos cristianos en la península: el reino de Pamplona, los actualmente llamados condados catalanes, y los de Aragón, Sobrarbe y Ribagorza.

El territorio situado entre el oriente de Navarra y el mar se dividió en condados sometidos a los francos. Los condados catalanes fueron divisiones de la zona occidental de la Marca Hispánica y los condados de Aragón, Sobrarbe y Ribagorza ocupaban la zona intermedia. Fue una zona de contención militar que tomaron los francos para frenar las incursiones sarracenas. Si bien la intención inicial de estos era llevar las fronteras hasta el Ebro, la Marca quedó delimitada por los Pirineos en el norte y por el río Llobregat en el Sur. Los francos favorecieron la llegada de mozárabes, entre los que surgió con el tiempo un sentimiento contrario al dominio franco. Posteriormente se independizó del dominio franco gracias a la actuación de los condes Aznar Galíndez, conde de Aragón desde 809 hasta 820 y Wifredo el Velloso, que en el año 874 reunió y gobernó de forma autónoma los condados catalanes para legarlo luego a sus descendientes Borrell II (947-92) y Ramón Borrell (992-1018).

En la zona de los condados catalanes, el Condado de Barcelona se convirtió muy pronto en el dominante de la zona. Tras la unión dinástica entre el Reino de Aragón y el conjunto de condados vinculados al de Barcelona, tendría origen la Corona de Aragón, que extendería sus dominios hacia el sur y el Mediterráneo.

El avance de los reinos cristianos en la península ibérica fue un proceso lento, discontinuo y complejo, en el que se alternaron períodos de expansión con otros de estabilización de fronteras y en el que muchas veces los diferentes reinos o núcleos cristianos siguieron también ritmos de expansión distintos, a la vez que se remodelaban internamente, con uniones, divisiones y reagrupaciones territoriales de signo dinástico.También cambiaba internamente la forma y fuerza del poder musulmán peninsular al que se enfrentaban, experimentando diversas fases de poder centralizado y períodos de disgregación.

La expansión conquistadora estuvo salpicada de continuos conflictos y cambiantes pactos entre reinos cristianos, negociaciones y acuerdos con poderes regionales musulmanes y, puntualmente, alianzas cristianas más amplias, como la que se dio en la batalla de Simancas (939), que aseguró el control cristiano del valle del Duero y del Tormes; o la más sonada (por su excepcionalidad) y de más amplios vuelos en la batalla de las Navas de Tolosa en 1212, que supuso el principio del fin de la presencia almohade en la península ibérica. El estudio de tan dilatado y complejo proceso pasa por el establecimiento de diferentes fases en las que los historiadores han establecido perfiles diferenciados en los ritmos y características de conquista, ocupación y repoblación.

Derrotado el reino visigodo de Toledo entre el año 711 y el 714, al margen de la invasión solo queda una estrecha franja montañosa en el norte peninsular. El principal esfuerzo de estos primeros núcleos de resistencia hasta el siglo irá dirigido a consolidar nuevas estructuras político-institucionales sobre una realidades socio-económica en transformación (el asentamiento masivo de población huida del avance musulmán), configurando las bases del feudalismo en la península ibérica. Al oeste se afianzó el reino asturiano, extendiéndose entre Galicia, el Duero y el Nervión. Al este la Marca defensiva carolingia germinará en diferentes núcleos cristianos pirenaicos. Su precaria situación quedará demostrada durante el reinado de Abd al-Rahman III (912-961), cuando reconocieron la soberanía del Califato y se convirtieron en estados tributarios.

Durante el siglo y a comienzos del , los territorios cristianos asistieron a un incremento de la población y al desarrollo de la colonización y explotación de tierras. Los avances de las conquistas fueron lentos al principio, durante los últimos años del reinado de Alfonso II (Brañosera, 824), para acelerarse con posterioridad desde mediados del siglo , durante los reinados de Ordoño I y Alfonso III (Braga, Tuy, Astorga, León, Amaya, Briviesca, Miranda, Oporto (868), Simancas (889) y Zamora (893). En la zona castellana serían incorporadas a territorio cristiano: Clunia, Roa, San Esteban de Gormaz (912) y Osma. En el año 914 durante el reinado de Sancho Garcés I se añadiría la zona alta de La Rioja.

El avance sobre el valle del Duero a lo largo del parece confirmar la visión goticista iniciada con Alfonso III el Magno (866-910). En el año 856 se produce la toma de León, siendo la nueva sede de los monarcas para administrar mejor los nuevos territorios. Con Alfonso III la frontera quedó fijada en el Duero gracias a la política de colonización llevada adelante con habitantes de las montañas y huidos cristianos de la zona islámica.. Las tierras de repoblación pasan a ser propiedad de los labriegos en lo que se conoce como presuras. Estos campesinos llevaban una vida rudimentaria, basada fundamentalmente en la ganadería y agricultura, constituyendo pequeños núcleos.

El Reino de Asturias tuvo varias escisiones. La primera, a la muerte del rey Alfonso III "el Magno", que repartió sus dominios entre tres de sus cinco hijos: García, Ordoño y Fruela. Estos dominios incluían, además de Asturias, los condados de León, Castilla y Galicia y las marcas de Álava y Portugal (esta última, por aqueil entonces, era solo la frontera sur de Galicia). García se quedó León, Álava y Castilla, fundando el Reino de León. Ordoño se quedó Galicia y Portugal, y Fruela se quedó Asturias. 

En la primera mitad del se llegó a superar la línea del Duero, avanzando hasta Salamanca y Coímbra. En la zona oriental del Duero se produjeron choques más duros contra los musulmanes, entre los que destacamos la derrota de las fuerzas conjuntas de Ordoño II de León y Sancho Garcés I de Pamplona en la Valdejunquera (920) contra el emir Abderramán III y la victoria de Ramiro II (931-951) en Simancas (939). Ramiro II repobló Sepúlveda y la cuenca del Tormes. A Ordoño III de León (951-956) sucede Sancho I (956-958) por presiones de la facción navarra, cuya influencia culmina con Ramiro III.

El avance cristiano al sur del Duero no terminó consolidándose a causa de la reunificación de al-Andalus por Abderramán III, que en el año 929 se autoproclamó califa, iniciando el Califato de Córdoba. Será bajo su gobierno que la zona islámica peninsular alcanzará su cenit político, económico y cultural. El territorio cristiano sufrió ataques con las "aceifas" de Almanzor, canciller del Califato de Córdoba y "hayib" o chambelán del califa Hisham II (976-1009). Se perdieron todas las plazas situadas al sur del Duero y la mayoría de las ciudades importantes del norte peninsular, como Santiago, León y Barcelona, sufrieron asaltos y daños importantes.

Castilla ("territorium Castellae") fue mencionada por primera vez en un documento en el año 800. Era la zona más oriental de León y expuesta a las incursiones islámicas del valle del Ebro y se correspondía al valle alto del río Trueba, al norte de la provincia de Burgos y al pie de la Cordillera Cantábrica. Se trataba de un condado poblado fundamentalmente por vascones cristianizados que había ido adquiriendo autonomía a medida que declinaba el poder de los reyes de León. Se fue consolidando un estilo de vida propio de la zona de frontera: una sociedad fuertemente jerarquizada en lo militar (con unos condes muy autónomos con respecto al poder de los reyes de León), habituada a la guerra y al botín por un lado y a las relaciones mercantiles con al-Andalus por otro. Se independizó de León con Fernán González (930-970) tras la muerte de Ramiro II en el año 951. La expansión castellana, tanto guerrera como pacífica, tuvo como resultado lejano la construcción de un amplio conjunto de territorios desde el Atlántico al Mediterráneo. En su avance hacia territorios despoblados del sur durante los siglos y se definirán dos zonas: «Castilla Vieja», que correspondería a los territorios al norte del Duero, y lo que quedaría al sur hasta la Cordillera Central o "Extrema Dorii", que durante mucho tiempo conservará un derecho propio y unas instituciones urbanas particulares.

El Reino de Pamplona, posteriormente llamado Reino de Navarra, tuvo como origen la propia familia gobernante, que había pactado la expulsión de las tropas francas de Pamplona con los muladíes de Tudela, la familia Banu Qasi. Su primer rey fue Íñigo Arista (820-851). Tras él, el nuevo reino logró mantener la autonomía con García Íñiguez (851-70) y Fortún Garcés (870-905). A principios del siglo , la familia Jimena sustituye a la Arista y el primer rey es Sancho Garcés I (905-26), que tiene un gran éxito militar. 

Referencia a Sancho Garcés I en la Crónica albeldense (881):Le seguirán García Sánchez I (926-70), Sancho Garcés II (970-94) y García Sánchez II (994-1000). La economía del reino estaba basada fundamentalmente en la agricultura y el pastoreo, con algunos contactos comerciales con los musulmanes. Pamplona llegó a controlar lo que actualmente es Navarra (su origen), La Rioja (llamado entonces «Reino de Nájera») y lo que en la actualidad es el País Vasco, y a unir dinásticamente los condados de Castilla, dependiente de León pero muy autónomo, y Aragón (tras haberse constituido como dinastía hereditaria con el conde Aznar Galíndez), Sobrarbe y Ribagorza en los Pirineos en tiempos de Sancho el Mayor (1004-1035). A su muerte legó su reino patrimonial (el Reino de Pamplona) a García Sánchez III de Pamplona (1035-54), a quien de jure, deberían estar subordinados los tenentes de las otras zonas de su reino: Fernando, que recibió el condado de Castilla, Ramiro, que recibió el condado de Aragón y Gonzalo, el menor de los hermanos, que heredó Sobrarbe y Ribagorza. Tras anexionarse Sobrarbe y Ribagorza en 1045, Aragón se independiza.

La disgregación del Califato en una treintena de taifas, coincidirá con la reorganización y consolidación política de los reinos hispano-cristianos y facilitará un lento avance cristiano por la Meseta norte y el valle del Ebro. Ello será financiado con las imposiciones tributarias (parias) a que sometieron a los reinos musulmanes Fernando I de Castilla y León (1035-1065), Sancho Garcés IV de Pamplona (1054-1076), Sancho Ramírez de Aragón (1064-1094) y Ramón Berenguer I de Barcelona (1035-1076), convirtiéndolos virtualmente en protectorados. Es un período de "europeización," con la apertura a las corrientes culturales continentales (Cluny, Císter) y la aceptación de la supremacía religiosa de Roma. La guerra con al-Andalus se plantea ya como una guerra de reconquista, provocando que la frontera adquiera un carácter de provisionalidad permanente. El avance castellano-leonés (Toledo, 1085) provocó sucesivas invasiones norteafricanas –almorávides y almohades– que evitaron el colapso de la España musulmana. La repoblación entre el Duero y el Tajo se produce con colonos libres formando concejos con amplia autonomía (fueros), mientras que en el Ebro, los señoríos cristianos explotarán a la población agrícola musulmana.

El Reino de Aragón tiene su origen en un condado perteneciente a la Marca Hispánica. Se uniría al de Pamplona gracias al enlace dinástico de Andregoto Galíndez con García Sánchez I en el año 943. Tras la muerte de Sancho III de Navarra en 1035, legó a su hijo Ramiro (1035-63) el dominio del condado de Aragón. Tras anexionarse los condados de Sobrarbe y Ribagorza, Ramiro I establecería un reino de facto que comprendía los tres antiguos condados y ocupaba los Pirineos centrales. En 1076, durante el reinado de Sancho Ramírez de Aragón, llegó a anexionarse Navarra. Durante el reinado de Alfonso I el Batallador y tras una dura lucha con las taifas de Zaragoza, el reino aragonés llegó al Ebro, conquistando la capital en 1118. Tras la muerte de Alfonso I, los reinos de Aragón y Navarra se escinden al elegir cada uno a su gobernante.

Incluye Castilla, León, Navarra y el bajo Aragón. Entre los años 1000 y 1035, Sancho el Mayor somete a la Iglesia a Roma con la reforma benedictina con Cluny como referencia. Tras García Sánchez (1035-54) el reino se divide entre castellanos y aragoneses.

Tras ser un condado hereditario con Fernán González (923-970), pasa a ser un reino con Fernando I (1032-1065). Le siguen Sancho II (1065-72) y Alfonso VI (1072-1109). Auge del románico.

La alianza entre los reinos cristianos (Navas de Tolosa, 1212) logra el definitivo derrumbe del Al-Andalus, conquistando con gran celeridad el sur peninsular (salvo Granada), destacando la batalla del Estrecho donde entran en juego el último pueblo norteafricano que intervienen en la península, los benimerines. Esta expansión, protagonizada por las coronas de Castilla y Aragón, generará problemas debido a la absorción de un enorme volumen territorial y poblacional. En Andalucía y Murcia, la imposición de grandes señoríos –nobles guerreros y órdenes militares-, la expulsión de las poblaciones autóctonas –agrícolas y artesanas-, la crisis económica del siglo y las guerras civiles que desangraron a los reinos castellanos bajomedievales, derivará en la decadencia económica del territorio. En Valencia y Alicante, los señoríos cristianos, de menor extensión, se superpondrán a una población musulmana que mantendrá la prosperidad económica. De esta forma se consolida España como la nación que resistió y contuvo los ataques musulmanes en Occidente, del mismo modo que el Reino de Hungría se erige como el guardián de Europa en el Este ante la llegada de los turcos.

Comentario de Antonio Ubieto Arteta sobre la batalla de Las Navas de Tolosa, en el año 1212, que abrió a los reinos cristianos el acceso al valle del Guadalquivir:

La unión dinástica que se produjo con el matrimonio de Petronila (hija única del rey de Aragón, años 1157-1164) y Ramón Berenguer IV, conde de Barcelona (1131-1162), conformó la Corona de Aragón, agrupando al reino y a los condados. A pesar de ello, cada territorio mantuvo sus usos y costumbres consuetudinarios.

La Corona acabaría por unificar con el tiempo lo que hoy es Cataluña, arrebatando a los árabes el resto de Cataluña, la Cataluña Nueva, y anexionándose los restantes territorios.

Los condes de Castilla extendieron su control sobre Álava y Guipúzcoa, incorporada en el siglo . Ambos territorios conservaron su lengua y amplia autonomía. Vizcaya sería incorporada a Castilla en el año 1379, conservando también sus fueros.

La supervivencia del Emirato de Granada responde a varias razones: su condición de vasallo del rey castellano, su conveniencia como refugio de la población musulmana, el carácter montañoso del reino (complementado con una consistente red de fortalezas fronterizas), el apoyo norteafricano, la crisis castellana bajomedieval y la indiferencia de Aragón, que se hallaba ocupada en su expansión mediterránea. Además, la homogeneidad cultural y religiosa (sin población mozárabe) proporcionó al Estado granadino una fuerte cohesión. Su desaparición a finales del siglo –además de por sus interminables luchas dinásticas- se ensarta en el contexto de la construcción de un Estado moderno llevado a cabo por los Reyes Católicos a través de la unificación territorial y el reforzamiento de la soberanía de la Corona.

Sicilia es incorporada a la Corona de Aragón en el año 1479. Esta anexión coincide con las políticas de acercamiento entre Castilla y Aragón que se produce con el matrimonio en el año 1469 entre Isabel I [1451-1504] y Fernando I [1452-1516]. Los Reyes Católicos desarrollarán una política interior autoritaria donde se destaca la incorporación de los maestrazgos de las Órdenes militares a la Corona, la disminución de la autonomía de los municipios con el nombramiento de los corregidores y el aumento de las facultades de las Cortes, el reforzamiento o ampliación de los Concejos, la creación de la sala de Contadores (Hacienda), la reforma militar (nuevos reclutamientos), la mejora de la seguridad pública con la Santa Hermandad y la reforma de la justicia. El 2 de enero de 1492 se produjo la toma de Granada, dando fin al último reino islámico de la península ibérica. El 31 de marzo de ese mismo año se produjo la expulsión salvo bautismo de los judíos. Los conversos darán lugar a una nueva minoría llamada criptojudíos, perseguidos por el Santo Oficio.

En paralelo al avance militar, se produjo un proceso de repoblación con el asentamiento de población cristiana en territorios conquistados, que podía provenir de los núcleos septentrionales (de tierras montañosas, pobres y superpobladas), de las comunidades mozárabes del sur que emigraban al norte debido al incremento de la represión religiosa (al arte mozárabe se le denomina también arte de repoblación), e incluso provenientes de zonas de la Europa al norte de los Pirineos, a los que genéricamente se llamaba "francos". La modalidad de asentamiento de esa población varió en sus características de acuerdo a la forma en que se produjo la conquista, el ritmo de la ocupación y el volumen de la población musulmana preexistente en el territorio. En las zonas que fueron frontera entre cristianos y musulmanes, nunca hubo un "vacío demográfico" o "zona despoblada", a pesar de que algunos documentos (que así lo pretendían, justificando de ese modo la legitimidad de las apropiaciones) dieron origen al concepto de "desierto del Duero", acuñado por la historiografía de comienzos del siglo (Claudio Sánchez Albornoz).

La llegada de los repobladores cristianos se testimonia arqueológicamente no solo en lo más evidente, como edificaciones religiosas o enterramientos, sino también con cambios en la cultura material, como la denominada cerámica de repoblación.

Sirviendo como hitos divisores los valles de los grandes ríos que cruzan la península ibérica de este a oeste, se han definido ciertas modalidades de repoblación, protagonizadas cada una por distintas instituciones y agentes sociales en épocas sucesivas:







Repoblaciones emprendidas tras la toma de Simancas por Ramiro II, en 939. Sampiro fue un cronista del reino de León quien redactó la obra conocida "Crónica de Sampiro", del siglo . Este texto tiene importancia debido a que la Crónica albeldense finaliza su relato en el año 883:

Las comunidades cristianas peninsulares, tanto en territorio musulmán como cristiano, desarrollaron su propio rito diferente al del resto de la cristiandad de Occidente. Esto será reprochado por el papado en el , tal y como lo expresó Gregorio VII:

Los Reyes Católicos acabaron la reconquista de España el 2 de enero de 1492 con la toma de Granada. Esto dio origen a una festividad que se lleva a cabo el 2 de enero de todos los años. El emir Boabdil, de la dinastía Nazarí, tuvo que abandonar Granada. La tolerancia religiosa que había hasta entonces dejó de serlo con la expulsión de los judíos en 1492, y con la prohibición del culto islámico en Granada en 1500, contra los términos pactados. Acabó del todo un siglo después con la expulsión de los moriscos, homogeneizando así toda la península.

Como en otras partes del mundo musulmán, a los cristianos y judíos se les permitió conservar sus religiones, con sus propios sistemas legales y tribunales, pagando un impuesto, el yizia. La sanción por no pagarlo era la prisión.La nueva jerarquía cristiana exigió fuertes impuestos a los no cristianos y les otorgó derechos, como en el Tratado de Granada (1491) solo para los moros en la que fue la Granada islámica reciente. El 30 de julio de 1492, toda la comunidad judía, unas 200 000 personas, fueron expulsadas por la fuerza. Al año siguiente, el decreto de la Alhambra ordenó la expulsión de judíos practicantes, lo que llevó a muchos a convertirse al catolicismo. En 1502, la reina Isabel I declaró obligatoria la conversión al catolicismo dentro del Reino de Castilla. El rey Carlos I hizo lo mismo con los moros en el Reino de Aragón en 1526, forzando las conversiones de su población musulmana durante la rebelión de las Germanías. Muchos funcionarios locales aprovecharon la situación para confiscar propiedades.

Los moriscos, descendientes de aquellos musulmanes que se sometieron a la conversión al cristianismo, en lugar de ir al exilio, durante los primeros tiempos de la Inquisición española y portuguesa fueron expulsados de España después de una grave agitación social, cuando la Inquisición estaba en su apogeo. Las expulsiones se llevaron a cabo de manera más severa en el este de España (Valencia y Aragón) debido a la animosidad local hacia los musulmanes y moriscos, donde los trabajadores locales los consideraron como rivales económicos, ya que eran vistos como mano de obra barata que socavaba su posición de negociación con los propietarios. Las exacciones impuestas a los moriscos allanaron el camino para una importante revuelta de los moriscos que tuvo lugar en 1568, que terminó con la expulsión definitiva de los moriscos de Castilla en 1609, siendo expulsados de Aragón casi al mismo tiempo.




</doc>
<doc id="6933" url="https://es.wikipedia.org/wiki?curid=6933" title="Jean Renoir">
Jean Renoir

Jean Renoir (París, 15 de septiembre de 1894-Beverly Hills, 12 de febrero de 1979) fue un director de cine, guionista y actor francés. Era el segundo hijo del famoso pintor impresionista Pierre-Auguste Renoir.

Sus películas, durante décadas incomprendidas en su verdadera dimensión, se ven desde hace tiempo ya como obras clave dentro del desarrollo de la historia del cine francés entre 1930 y 1950, antes de que se iniciara en Francia la Nouvelle vague. La influencia sobre el cine de François Truffaut, entre otros, es especialmente notable.

Estudió en el Sainte-Marie de Monceau, un colegio católico privado en París.

Tras unos estudios mediocres, Jean Renoir se alistó en el cuerpo de dragones en 1912. Soldado durante la Primera Guerra Mundial, sirvió en la aviación a partir de 1916. Sufrió una herida en la pierna que hará que cojee toda su vida. 

En 1920, se casó con una de las modelos de su padre, Andrée Madeleine Heuchling, y abrió un taller cerámico. El estreno, en 1921, de la película de Erich von Stroheim, "Esposas frívolas" ("Foolish Wives"), fue determinante en su futura carrera como cineasta.

En 1924, hizo el guion y produjo "Catherine ou une vie sans joie"; estaba dirigida por Albert Dieudonné, aunque él asimismo participó en su realización. Trabajaba su joven esposa.

Su primer largometraje, "La Fille de l'eau" (1924), era una fábula bucólica con estética impresionista, en el que participa su mujer, que había adoptado el seudónimo de Catherine Hessling, y su hermano mayor, Pierre Renoir. 

La tibia acogida que se dispensó a la película no desanimó al cineasta, que poco después se aventurará en una costosa producción, "Nana", basada en la novela homónima de Émile Zola, en 1926. Para financiarla venderá algunos de los lienzos de su padre que había heredado. Para él, era su primera película, y el influjo de Stroheim, "Esposas frívolas", está reconocido. Hay un contraste evidente entre criados y señores, y aparece un tema de Renoir, su afición al espectáculo.

Más adelante se lanzará a una serie de películas de inspiración diversa, que no siempre convencieron al público, como "La Petite Marchande d'allumettes", basada en el relato de Hans-Christian Andersen, 1928; "Tire-au-flanc", comedia militar, 1928, que se ha descuidado, a juicio de Bazin, por su mezcla de lo cómico y lo trágico, la fantasía y la crueldad;"On purge Bébé", (basada en Georges Feydeau, 1931).

"La Golfa" (1931) marca un cambio en la obra de Jean Renoir. Es una de las primeras películas sonoras, adaptada partiendo de una novela de Georges de La Fouchardière. Esta bella "La Golfa" dio a Michel Simon uno de sus mejores papeles, el de un pequeño burgués celoso, asesino y torpe (el actor era homenajeado por el cineasta). Fue una empresa desmesurada, dice Renoir, que careció de éxito. 

Tras "La Nuit du carrefour" (basada en Georges Simenon, 1932), en la que Pierre Renoir interpretaba al comisario Maigret, el director dirigirá una serie impresionista de obras maestras: "Boudou salvado de las aguas" (otra vez con Michel Simon, 1932), y lleno de contrasentidos poéticos,"El crimen de Monsieur Lange" (con Jules Berry, 1936), "Una partida de campo" (1936) en la que su sobrino Claude Renoir es el autor de la fotografía, que recuerda al mundo de su padre.

Su "Toni", 1934, está lleno de claves, porque es una especie de manojo de filmes suyos. Pese a sus defectos, es donde lleva más lejos sus hallazgos sobre sí y sobre el cine.

Asimismo hay que contar con "Los bajos fondos" (donde trabaja Louis Jouvet, 1936). Buscando inspiración en las novelas de Gorki, como en este caso, o en los relatos de Maupassant, Jean Renoir demuestra un agudo sentido de la realidad, que pone al servicio de un auténtico naturalismo poético.

Poco a poco irá buscando la colaboración de Jacques Prévert y Roger Blin, que dan a su producción una dimensión abiertamente política, marcada por las ideas del Frente Popular, dado el horror que le inspiraba un personaje como Hitler:"La vie est à nous", (1936), "El crimen de Monsieur Lange", "La Marsellesa", (1938). Esta tendencia abrirá el camino al neorrealismo italiano.

Antes de la Segunda Guerra Mundial, Jean Renoir trata de promover un mensaje de paz con "La gran ilusión" (1937), en la que participan (en un homenaje) su padre espiritual Erich von Stroheim y Jean Gabin. En "La bestia humana" (1938), trata de poner ante la pantalla los compromisos sociales de la época. En su obra maestra, "La regla del juego" (1939), prevé el desmoronamiento de los valores humanistas y traza un cuadro sin ninguna condescendencia sobre las costumbres de la sociedad francesa.

Exiliado en los Estados Unidos en 1940 (dejará inconclusa una adaptación de "Tosca" de Victorien Sardou, que al final rodará su amigo Carl Koch), Jean Renoir adquiere la nacionalidad estadounidense. 

Aunque se adapta difícilmente al universo de Hollywood, dirige a pesar de todo algunas películas por encargo, en especial películas de propaganda, como "Esta tierra es mía", con Charles Laughton en 1943, o "Salute to France" en 1944 y hace adaptaciones literarias ("Memorias de una doncella", basado en Octave Mirbeau, 1946). 

Todo ello sucede antes de viajar a la India para rodar una obra maestra, "El río" ("The River", 1951). Es una película en color, contemplativa y serena, con un humanismo a veces desencantado: es el resultado de una experiencia propia. El influjo de este film en el cine de la India será patente. Y, para Rivette, el viaje a la India se convirtió desde entonces en un referente (como antes lo era a Grecia).

De vuelta a Europa a principios de los años 1950, Jean Renoir aún rodará "Le Carrosse d'or" (basado en Prosper Mérimée, 1952), "French Cancan" (con Jean Gabin y Françoise Arnoul, 1955), "Elena y los hombres" (con Ingrid Bergman y Jean Marais, 1956) y "Le caporal épinglé" (basado en Jacques Perret, 1962). 

Al encontrarse cada vez con mayores dificultades para producir su películas, se dedica a la televisión ("Le petit théâtre de Jean Renoir", 1969-1971) y se dedica con mayor empeño a la escritura: publica un libro sobre su padre, "Renoir, mi padre" (1962); su autobiografía, "Mi vida y mis películas" (1974); un ensayo ("Escritos 1926-1971", 1974), algunas obras de teatro ("Orvet", 1955), así como varias novelas ("Los Cuadernos del capitán Georges", 1966; "El crimen del inglés", 1979). 

En 1970, se retira y se va a vivir a Beverly Hills, en donde muere en 1979.








</doc>
<doc id="6934" url="https://es.wikipedia.org/wiki?curid=6934" title="Ladri di biciclette">
Ladri di biciclette

Ladri di biciclette (en Hispanoamérica, Ladrones de bicicletas; en España y Chile, Ladrón de bicicletas) es una película italiana dramática de 1948 dirigida por Vittorio de Sica. Se considera como una de las películas emblemáticas del neorrealismo italiano. En 1954, la revista "Sight and Sound" publicó su primera lista de las "diez mejores películas jamás hechas", "Ladri di biciclette" estaba en primer lugar en esa lista. En 1962 fue puesta en el séptimo en la misma lista. Ocupa uno de los primeros diez puestos en la lista de "Las 50 películas que deberías ver a los 14 años".

La película está basada en la novela homónima escrita por Luigi Bartolini en 1945 y adaptada a la gran pantalla por Cesare Zavattini. El relato narra un accidente de la vida cotidiana de un trabajador. Este accidente consiste en el robo de la bicicleta con la que va a trabajar. Este acto sería banal si no se tuviera en cuenta el contexto de la sociedad italiana de 1948 en que se sitúa la película. La elección de la bicicleta como objeto clave del drama es característico de las costumbres urbanas italianas, y a la vez, de una época en la que los medios de transporte mecánicos son todavía escasos y costosos.

Roma, segunda posguerra: Antonio Ricci (Lamberto Maggiorani), un desempleado encuentra trabajo pegando carteles, lo cual es un gran logro en la situación de posguerra que vive el país, donde el trabajo escasea y obtenerlo es un éxito excepcional. Pero para trabajar debe poseer una bicicleta. Desafortunadamente, el primer día de trabajo le roban la bicicleta mientras pega un cartel cinematográfico. Antonio persigue al ladrón sin resultado alguno. Decide denunciar el robo ante la policía, pero se da cuenta que las fuerzas del orden no pueden ayudarlo a encontrar su bicicleta.
Desesperado, busca el apoyo de un compañero de partido, que a su vez moviliza a sus amigos basureros. Al alba, Antonio, junto a sus compañeros y a su hijo Bruno comienza su búsqueda, primero en la Piazza Vittorio y más tarde en Porta Portese, donde tradicionalmente van a parar los objetos robados.
Pero no hay nada que hacer: la bicicleta seguramente ya esté desmontada y será imposible de encontrar. En Porta Portese, Antonio ve al ladrón de su bicicleta, mientras negocia con un viejo vagabundo. Lo persigue sin alcanzarlo, regresa a Porta Portese a encontrar al vagabundo, y lo sigue, hasta un comedor social. Allí le pregunta por su bicicleta y por la identidad del ladrón, pero no obtiene ningún resultado. Exasperado, Antonio acude a una vidente, pero la respuesta de ésta es casi una tomadura de pelo: "“o la encuentras ahora o no la encontrarás jamás”". Inmediatamente, al salir de la casa de la vidente, se encuentra con el ladrón de la bicicleta que al final es defendido por todos sus colegas. Antonio habla con un carabinero para explicarle la situación. Entonces éste le contesta que sin testigos del robo no se puede hacer nada.
Finalmente, mientras Antonio y Bruno esperan el autobús para regresar a casa, el padre se percata de la existencia de una bicicleta que nadie parece custodiar. Intenta robarla pero la muchedumbre se lanza a atraparlo. Solo los llantos de Bruno consiguen frenarlos e impedir que su padre vaya a la cárcel. Antonio se encuentra ahora tan pobre como antes pero con la vergüenza de haberse colocado al nivel de quien le había robado. 
La película se cierra con la vuelta a casa de Antonio y Bruno mientras cae la noche sobre la ciudad de Roma.


Un rasgo característico de este filme, y del neorrealismo, es la desaparición de la noción de actor y de la puesta en escena. Los actores que intervienen no son profesionales. Aunque la búsqueda de las personas que interpretarían los personajes fue dura. Un detalle de la búsqueda del niño, fue que De Sica, tras haber visto cantidad de niños, se decantó por uno debido a su forma de andar. Es más, la prueba de selección de los niños se reducía a verlos caminar. Además ninguna escena fue realizada en estudios, sino que todas fueron rodadas en la calle.

Otro rasgo significativo es que todas las angulaciones de cámara están en función de lo que se quiere transmitir. Como por ejemplo, la secuencia en la que con un picado se ve toda la calle mostrándonos la muchedumbre entre la que se pierde el ladrón y la impotencia del trabajador.

La tesis de la película es de una soberbia y tremenda simplicidad, y se eclipsa detrás de una realidad social que a su vez pasa al segundo plano del drama moral y psicológico que bastaría para justificar la película: 

Cuando la película fue lanzado en Italia, fue visto con hostilidad y retratando a los italianos de manera negativa. El crítico italiano Guido Aristarco lo elogió, pero también se quejó de que "el sentimentalismo puede a veces tomar el lugar de la emoción artística". El compañero director de cine neorrealista italiano, Luchino Visconti, criticó la película, diciendo que fue un error utilizar a un actor profesional para doblar sobre el diálogo de Lamberto Maggiorani. Luigi Bartolini, el autor de la novela de la que Sica sacó su título, fue muy crítico con la película, sintiendo que el espíritu de su libro había sido completamente traicionado, ya que su protagonista era un intelectual de clase media y su tema era la ruptura del orden civil frente al comunismo anárquico.

Ladrones de bicicletas ha recibido la aclamación de los críticos desde su lanzamiento, con el agregador de reseñas de películas Rotten Tomatoes que informa el 98% de las críticas positivas de 54 críticas, con un promedio de 9.1 de 10. La imagen también está en la Lista de mejores películas del Vaticano para retratar los valores humanísticos. 

Bosley Crowther , crítico de cine de The New York Times , elogió la película y su mensaje en su crítica. Escribió: "Una vez más, los italianos nos han enviado una película brillante y devastadora en el lamentable drama de Vittorio De Sica sobre la vida urbana moderna. Anunciado fervientemente por quienes lo habían visto en el extranjero (donde ya ha ganado varios premios en varios festivales de cine), esta desgarradora imagen de frustración, que llegó ayer [al World Theatre], es justo para cumplir con todos los pronósticos de su absoluto triunfo aquí. Por una vez más, De Sica, que nos dio el aplastante Betún, esa demostración desesperadamente trágica de corrupción juvenil en la Roma de la posguerra, se ha asentado y ha plasmado con nitidez en términos simples y realistas un tema importante, de hecho, fundamental y universalmente dramático. Es el aislamiento y la soledad del pequeño hombre en este complejo mundo social que irónicamente está bendecido con instituciones para consolar y proteger a la humanidad ". Pierre Leprohon escribió en Cinéma D'Aujourd que" lo que no debe ignorarse en el nivel social es que el personaje no se muestra al comienzo de una crisis sino en su resultado. Basta con mirar su rostro, su paso inseguro, sus actitudes vacilantes o temerosas para comprender que Ricci ya es una víctima, un hombre disminuido que ha perdido su confianza. " Lotte Eisnerlo llamó la mejor película italiana desde la Segunda Guerra Mundial y Robert Winnington lo llamó "el disco más exitoso de cualquier película extranjera en el cine británico". 

Cuando la película se relanzó a finales de la década de 1990, Bob Graham, crítico cinematográfico del San Francisco Chronicle , dio al drama una crítica positiva: "Los papeles son interpretados por no actores, Lamberto Maggiorani como el padre y Enzo Staiola como el Muchacho solemne, que a veces parece ser un hombre en miniatura. Le dan una gran dignidad a la visión sin pestañear de De Sica de la Italia de posguerra. La rueda de la vida gira y hace rechinar a la gente, el hombre que cabalgaba alto en la mañana Al anochecer, es imposible imaginar esta historia en otra forma que la de De Sica. La nueva impresión en blanco y negro tiene una extraordinaria gama de tonos grises que se vuelven más oscuros a medida que la vida se cierra ". 







</doc>
<doc id="6935" url="https://es.wikipedia.org/wiki?curid=6935" title="Ingmar Bergman">
Ingmar Bergman

Ingmar Bergman (Upsala, 14 de julio de 1918-Fårö, 30 de julio de 2007) fue un guionista y director de teatro y cine sueco, considerado uno de los directores de cine clave de la segunda mitad del siglo XX y según muchos académicos el más grande director de la historia del cine.

Segundo hijo del pastor luterano Erik Bergman (1886-1970) y Karin Åkerblom, Ingmar Bergman nació en Upsala. El mundo metafísico de la religión influyó tanto en su niñez como en su adolescencia. Su educación estuvo basada en los conceptos luteranos: «Casi toda nuestra educación estuvo basada en conceptos como pecado, confesión, castigo, perdón y misericordia, factores concretos en las relaciones entre padres e hijos y con Dios», escribe en sus memorias. «Los castigos eran algo completamente natural, algo que jamás se cuestionaba. A veces eran rápidos y sencillos como bofetadas y azotes en el trasero, pero también podían adoptar formas muy sofisticadas, perfeccionadas a lo largo de generaciones».Muchas de sus obras están inspiradas en esos temores y relaciones violentas. El ritual del castigo y otras anécdotas de su infancia aparecen escenificadas en una de sus más reconocidas películas, "Fanny y Alexander", donde Alexander es un niño de diez años que es trasunto del pequeño Bergman.

Progresivamente, el joven Bergman buscó la forma de encauzar sus propios sentimientos y creencias independizándose cada vez más de los valores paternos a fin de buscar su propia identidad espiritual, pero, a lo largo de su vida, Bergman siempre mantuvo un canal abierto con su infancia, y en ella había penetrado con fuerza el cine con el regalo de un cinematógrafo elemental, que le condujo a todo tipo de ensoñaciones y conocimientos técnicos.

A partir de los trece años estudió bachillerato en una escuela privada de Estocolmo; después se licenció en Letras e Historia del Arte en la Universidad. Encontró en el teatro, y luego en el cine, los dos medios más apropiados para expresarse y centrar su capacidad y potencial creativos. Durante los años de la Segunda Guerra Mundial, ya distanciado de su familia, inició su carrera como ayudante de dirección en el Teatro de la Ópera Real de Estocolmo. No obstante, las imágenes y valores de su niñez, que lo seguirían por el resto de su vida, y la proximidad con el quehacer de su padre, lo habían sumergido en las cuestiones metafísicas: la muerte, la autonomía, el dolor y el amor.

La carrera cinematográfica de Bergman comenzó en 1941 trabajando como guionista. Su primer guion lo concibió en 1944 a partir de un cuento suyo, "Tortura" ("Hets"), que fue finalmente un filme dirigido por Alf Sjöberg. Simultáneamente a su trabajo como guionista, ejerció como "script"; y en su segunda autobiografía, "Imágenes", Bergman señala que él hizo el rodaje final de exteriores (fue su inicio como director profesional), y que su historia obsesiva y violenta fue retocada por Sjöbert, siendo este el que dio una tensión interior especial al personaje. La película estuvo producida por Victor Sjöström, por lo que Bergman tuvo de este modo un contacto próximo con dos grandes directores. Sjöström le apoyará, participando como actor en dos filmes suyos.

El éxito internacional de "Tortura" le permitió a Bergman iniciarse como director, un año después, con "Crisis". Durante los siguientes diez años escribió y dirigió más de una docena de películas, que incluyen "Llueve sobre nuestro amor" ("Det regnar på vår kärlek"), "Prisión" ("Fängelse") en 1949, "Noche de circo" ("Gycklarnas afton") y "Un verano con Mónica" ("Sommaren med Monika"), ambas de 1953. La actriz de la última, Harriet Andersson, era a su juicio uno de los «raros ejemplares resplandecientes de la jungla cinematográfica».

Curiosamente, el primer reconocimiento internacional, tanto de público como de crítica, se dio en países periféricos de la industria cinematográfica, con la exhibición de "Sommarlek" ("Juegos de verano" en España y "Juventud divino tesoro" en Uruguay y Argentina) en el Festival de Cine de Punta del Este de 1952. El éxito obtenido en ese festival dio lugar a la exhibición de toda la obra inicial de Bergman en Río de la Plata así como inmediatamente en Brasil, cuando obtuvo una alta valoración tanto por el público como por la crítica, antes de su reconocimiento internacional en Europa y América del Norte. La adhesión del público y la crítica cinematográfica del Cono Sur latinoamericano persistió durante toda la obra posterior de Bergman.

El reconocimiento internacional en Europa y América del Norte le llegó con "Sonrisas de una noche de verano" ("Sommarnattens leende", 1955), donde «hay una porción de nostalgia, una relación padre-hija reflejo de mi vida, la gran confusión y la tristeza», además del complicado amor; con ella ganó el premio «Best poetic humor» y es nominado para la Palma de Oro en el Festival de Cannes de 1956.

Fue seguida por los rodajes de "El séptimo sello" ("Det sjunde inseglet") y "Fresas salvajes" ("Smultronstället"), estrenadas con diez meses de diferencia en Suecia en 1957. "El séptimo sello" —para muchos, su primera obra maestra, aunque Bergman, que la apreciaba, no la considerase impecable— ganó el Premio Especial del Jurado y fue nominada a la Palma de Oro en el Festival de Cannes. Y "Fresas salvajes" ganó numerosos premios, como el Globo de Oro, el Oso de Oro en el Festival de Berlín y estuvo nominada al Óscar al mejor guion. Es el comienzo de la mejor etapa del director, que enlazaría numerosas obras maestras hasta finales de la década de 1960.

A continuación rodó dos películas: "En el umbral de la vida" ("Nära livet", 1958), que recibió numerosos premios —es de las primeras obras de cámara del director (con pocos personajes y desarrollada prácticamente en un solo escenario)— y "El rostro" ("Ansiktet", 1959) —única incursión del director en el cine de misterio mezclado con humor negro— con la que ganó el premio BAFTA. "El rostro", a pesar de no ser un gran éxito de crítica y público, es uno de los títulos más reivindicados de su filmografía, por el Bergman maduro o por su admirador Woody Allen, quien se inspira en su producción.

Rodó "El manantial de la doncella" ("Jungfrukällan", 1960), una cruda fábula medieval basada en una vieja historia sueca de violación y venganza, por la que recibe el Óscar a la mejor película extranjera, el Globo de Oro y un premio especial en el Festival de Cannes. Bergman se encuentra en la cima y, justo en esta época, comienza a pasar largos periodos de tiempo en la isla sueca de Fårö, donde rueda muchos de sus filmes claves.
Tras filmar el divertimento "El ojo del diablo" ("Djävulens öga") —una interesante comedia olvidada con el paso de los años sobre el mito de Don Juan—, Bergman dirigió tres de las películas más importantes de su filmografía: "Como en un espejo" ("Såsom i en spegel", 1961), "Los comulgantes" ("Nattvardsgästerna", 1963) y "El silencio" ("Tystnaden", 1963), en las que explora temas como la soledad, la incomunicación o la ausencia de Dios. Los críticos trataron las obras como un tríptico y Bergman inicialmente desmintió tal afirmación (argumentando que no había planeado sus rodajes como una trilogía y que no veía similitudes entre los tres filmes), pero terminaría aceptando dicho rótulo para los trabajos por su temática. 

"Como en un espejo" ganó nuevamente el Oscar a la mejor película extranjera, además de ser nominada a numerosos premios. La película abordaba con un cuarteto de personajes, un caso de locura histérico-religiosa, como escribiera el autor. Por su parte, "El silencio" se convirtió en una de las obras más aplaudidas del director y su mayor éxito de taquilla hasta la fecha. No obstante el precio de la fama fue caro, debido al contenido argumental desesperado (que anticipaba en su realización parte del estilo formal de obras posteriores de Bergman) y a sus explícitas escenas de sexo; "El silencio" fue prohibida en numerosos países, y Bergman recibió varias amenazas de muerte por parte del sector más conservador y cínico de los espectadores de la época, que veían la película como pornografía. En este periodo de creatividad desaforada y gran éxito de público y crítica, Bergman rodó una comedia menor parodiando el cine de Fellini: "¡Esas mujeres!" ("För att inte tala om alla dessa kvinnor", 1964).

En 1966, tras pasar unos meses hospitalizado, Bergman dirigió "Persona", una película que el propio autor consideró de las más importantes de su carrera, y que condensa de forma magistral todo el trabajo que venía haciendo desde comienzos de los años 1960. La película tuvo una recaudación en taquilla modesta (110 725 suecos vieron "Persona" frente a 1 459 031 que habían visto "El silencio" tres años atrás, tal como apunta Peter Cowe en "Los Archivos Personales de Bergman"); pero a pesar de su aire de cine experimental de arte y ensayo, y de que "Persona" apenas ganó premios, muchos la considerarían desde su estreno la pieza cumbre de su carrera y seguramente es su trabajo hoy más reconocido. Además según escribió: «Durante el rodaje nos alcanzó la pasión a Liv y a mí; una grandiosa equivocación que nos llevó a construir la casa de Fårö, entre 1966 y 1967; ella se quedó allí unos años».

Bergman rodó una de sus obras más crípticas y polémicas, "La hora del lobo" ("Vargtimmen", 1967), un trabajo tan adorado como criticado por su público debido a su compleja narración y simbolismo. Ya en 1968 se despidió del blanco y negro (volvió a él en 1980) con la cruda película bélica "La vergüenza" ("Skammen", 1968) y el filme para la televisión sueca "El rito" ("Riten", 1969). «En el origen de "La vergüenza" hay un horror personal: vi un reportaje sobre Vietnam, antes de la gran escalada, basado en los sufrimientos de civiles; los personajes principales son dos músicos, y él pierde el equilibrio en una invasión bélica».

Más tarde Bergman estrenó la que es oficialmente (si no se tiene en cuenta "¡Esas mujeres!") su primera obra en color, "Pasión" ("En passion", 1969), considerada por un sector otra de sus obras capitales (casi como todas las obras de los 60 de autor), en parte debido al cuidado y hermoso tratamiento de la fotografía. La película es un doloroso análisis del lado más amargo del amor y de las relaciones de pareja; y en ella repiten los mismos actores de "La hora del lobo" y "La vergüenza". En ella, el director se permite la licencia de incluir en medio de su metraje un descarte de su anterior película ("La vergüenza") en forma de sueño. Con "Pasión" se pone fin a una etapa ascendente cargada de experimentación y creatividad para Bergman, y a partir de aquí el director se dedicará a ahondar con mayor desasosiego y crudeza en los temas que ya venía tratando en sus trabajos anteriores, con mayor o peor fortuna.

Después se estrenó "La carcoma" ("Beröringen", 1971), primera película rodada íntegramente en inglés y producto puramente pensado para el mercado hollywoodense, del que el propio director renegó años después y que supuso uno de sus mayores fracasos de crítica. Fracaso que se subsanó con el estreno de "Gritos y susurros" ("Viskningar och rop", "Susurros y gritos", 1972). Obra preciosista y atormentada, de intachable fotografía y escaso diálogo, que se encumbraría entre las más aplaudidas del director, con tres nominaciones a los Oscar y premios en Cannes, y que suponía un regreso más oscuro y onírico a temas tratados en películas anteriores como "El silencio".

En estas fechas, Bergman trabajó para la televisión sueca. Dos de sus trabajos más memorables son "Secretos de un matrimonio" ("Scener ur ett äktenskap", 1973) y "La flauta mágica" ("Trollflöjten", 1975). La primera película tuvo su estreno cinematográfico en versión acortada y sería recordada como uno de los mejores ahondamientos en las relaciones de pareja llevados a la pantalla; mientras que la segunda dio una síntesis teatral sencilla y sabia de Mozart.

En 1976, Bergman dirigió "Cara a cara" ("Ansikte mot ansikte"), una película de una crudeza brutal y sumamente onírica, que ahonda de forma asfixiante en la psique de una protagonista perturbada. Nuevamente, fue nominado al Oscar al mejor director y ganó un Globo de Oro. Ese mismo año fue acusado de evasión de impuestos e internado en un psiquiátrico; con posterioridad se vería que era un problema de su contable y todo se resolvería pagando la diferencia. El escándalo fue internacional y tuvo muchos apoyos.

Tras este episodio, Bergman decidió abandonar Suecia y asentarse en Alemania para rodar "El huevo de la serpiente" ("Ormens ägg/Das Schlangenei", 1977), un curioso análisis del nazismo que quedaría ensombrecido por el éxito de su siguiente trabajo: "Sonata de otoño" ("Höstsonaten", 1978), alabada por muchos como otra de sus cimas artísticas. "Sonata de otoño" recibió nominaciones a los Óscar y los César, y ganó el Globo de Oro a la mejor película extranjera. La película contó con la presencia de Ingrid Bergman y retomó la temática de las relaciones familiares deterioradas que ya había trabajado el director en numerosas obras anteriores como "El silencio" (1963), "Gritos y susurros" (1972) o poco antes "Cara a cara" (1976).

La etapa alemana del director se cerró con "De la vida de las marionetas" ("Aus dem Leben der Marionetten", 1980). Rodada inicialmente para televisión, fue el primer trabajo sin la intervención de Liv Ullman en el reparto desde los años 1960. Un filme severo, apreciado por el director, rodado en blanco y negro, que gira en torno al asesinato de una prostituta.

Posteriormente, Bergman estrenó su última película para cine, "Fanny y Alexander" ("Fanny och Alexander", 1982), que ganó el Óscar, el Globo de Oro y el César a la mejor película extranjera, además de otras nominaciones. Esta película supuso la despedida del director del celuloide y fue considerada por muchos el broche de oro a una carrera llena de obras maestras.

A partir de entonces Bergman se dedicó al teatro, actividad que no había abandonado nunca, y a rodar varias películas para televisión. Tiene especial interés "Saraband" (2003), la última rodada por el director, y en la que retoma los personajes de su obra "Secretos de un matrimonio" para situarlos en la ancianidad. La concibe como un homenaje a Ingrid recién desaparecida.

El director falleció a los 89 años el 30 de julio de 2007 en la isla de Fårö, donde se había retirado. Aquel mismo día falleció también el cineasta italiano Michelangelo Antonioni.

Bergman trabajó en numerosas películas con los mismos actores; entre los que destacan los que le acompañaron a lo largo de toda su carrera, siendo estos:








Otras actrices valiosas que trabajaron con Bergman en varias producciones son Eva Dahlbeck que trabajó en seis de los primeros filmes de Bergman —"Tres mujeres" (1952), "Una lección de amor" (1954), "Sueños" (1955), "Sonrisas de una noche de verano" (1955), "En el umbral de la vida" (1958) y "¡Esas mujeres!" (1964)—; y la bella Gunnel Lindblom, que trabajó en "El séptimo sello" (1957), "El manantial de la doncella" (1960), "Los comulgantes" (1963), "El silencio" (1963) y "Escenas de un matrimonio" (1973).

Además de este grupo de actores y actrices, desde inicios de los años 50, más precisamente desde "Noche de circo" (1953), Bergman tuvo casi como miembro de su equipo de rodaje al fotógrafo Sven Nykvist, quien obtuvo varios premios con las obras dirigidas por Bergman, entre los que se destacan dos Óscar de la Academia de Hollywood a la fotografía de "Gritos y susurros" (1972) y de "Fanny y Alexander". El fruto de esa colaboración con Bergman lanzó la carrera internacional de Nykvist, particularmente en Hollywood.

Dos dramaturgos, Henrik Ibsen y, sobre todo, August Strindberg lo influyeron e introdujeron en un mundo donde se manifestaban los grandes temas que tanto lo atraían, cargados de una atmósfera dramática, agobiante y aún desesperanzada, lo que deja una profunda huella en el espíritu del joven Bergman y una marcada influencia en su obra artística.

Su narrativa visual suele ser deliberadamente lenta, con un montaje y una secuencia de planos mesurados, esto con el fin de lograr un suficiente tiempo de reflexión entre los espectadores, aun cuando ya estén «capturados» en la diégesis; sin embargo tal lentitud está, como en Andrei Tarkovsky, lejos de la monotonía merced a la carga del mensaje o a la excelente marcación actoral; otra característica de su estética fílmica es la limpieza de las imágenes.

Es recurrente el hecho de que en la mayor parte de la filmografía del realizador sueco, sus personajes son atravesados por los mismos caminos en que se internan. Se trata de trayectorias que los reconducen hacia sí mismos, hacia su propia alma, hacia su propia conciencia. Son recorridos íntimos, enigmáticos, que muchas veces se apoderan del espectador transportándolo a una experiencia estrictamente personal e inquietante, en la medida en que sus personajes realizan aquella trayectoria sobrecargada por un denso dramatismo, aquel que implica desnudar el alma humana en forma genérica.

Aquella trayectoria termina en algunos casos en la locura o en la muerte, en otros en un estado de gracia, un momento metafísico que permite a sus personajes comprender más de su realidad, una revelación que los iluminará y modificará el curso de sus vidas. En algunos casos les servirá para exorcizar, conjurar y dominar los fantasmas que perturban el alma del personaje.

Los personajes de Bergman arrastran un pesado lastre en sus mentes, en sus sentimientos. En general son adultos, salvo el caso del niño de "El silencio" (muy revelador, aunque sea Esther la que tiene el 'alumbramiento', el personaje que interpreta Ingrid Thulin). La inquietud que sienten esos personajes es más o menos latente, pero progresivamente irá revelándose ante el espectador produciendo un efecto de iluminación y a veces solo devastador.

La transmisión de esos estados de conflicto interno de sus personajes, originan historias angustiosas y lacerantes, como pocos directores de cine han podido comunicar a su público, y éste es el mayor logro del director sueco.

Los especialistas Jordi Puigdomenech y Charles Moeller clasifican las más de cuarenta obras de Bergman, como director y guionista, en cinco etapas: 



Dirigió teatro desde su juventud, y trabajó en el gran teatro sueco, el Dramaten de Estocolmo, durante decenios. Pero no solo como dramaturgo; en los años 60 pasó al despacho del Dramaten y marcó una época: reorganizó el trabajo interno, abrió los ensayos al público, animó las giras a provincia y el teatro infantil, aumentó los salarios de los actores, rebajó los precios de las entradas atrayendo a los jóvenes, y utilizó su prestigio para, con apoyo del Parlamento, lograr que la cultura sueca repercutiese en el mundo.

Toda la creación de Bergman no puede entenderse sin su constante y paralela dedicación al teatro, de la que destacan:


"La señorita Julia" (1986) y "Casa de muñecas" (1990) fueron representadas en Madrid (otras piezas en Barcelona), bajo su dirección.

En ocasiones, Ingmar Bergman ha dirigido algunas piezas teatrales para televisión: "Llega el señor Sleeman" ("Herr Sleeman kommer") (1957), "La veneciana" ("Venetianskan") (1958), ambas de Hjalmar Bergman; "Rabia" ("Rabies") (1958) de Olle Hedberg, "Tormenta" (1960) y "Un sueño" ("Ett drömspel") (1963), de August Strindberg, y también "La escuela de las mujeres" (1983), de Molière.

En 1951, Bergman hizo nueve cortos publicitarios del jabón Bris para AB Sunlight. La actriz sueca Bibi Andersson intervino en uno de ellos.

No tiene ningún vínculo familiar con la actriz Ingrid Bergman, confusión producida porque Ingmar Bergman se casó con una actriz que también se llamaba Ingrid, Ingrid von Rosen.






</doc>
<doc id="6938" url="https://es.wikipedia.org/wiki?curid=6938" title="La fierecilla domada">
La fierecilla domada

La fierecilla domada, también conocida como La doma de la bravía o La doma de la furia (en inglés, "The Taming of the Shrew"), es una comedia de William Shakespeare. Es una de sus obras más populares, tanto dentro como fuera de su país, como lo demuestra, por ejemplo, el hecho de que sea la quinta obra que más veces ha sido traducida al español de entre las treinta y siete que se conservan de su autor, únicamente precedida por "Romeo y Julieta", "Hamlet", "Macbeth" y "El rey Lear", y por delante de obras como "El sueño de una noche de verano", "Julio César" o incluso "Otelo".








• Tranio, Biondelo. Criados de Lucencio. Para que este pueda entrar en casa de Bautista se cambian de papel: Tranio pasará a ser Lucencio y Lucencio, Tranio.

• Vicencio. Padre de Lucencio. Es un hombre pudiente, que quiere mucho a su hijo.

• Grumio, Curtis. Son los criados de Petruchio. No demuestran mucha estima por su señor pero hacen todo lo que les dice. También colaboran en el maltrato psicológico de Catalina.

La obra se basa, en principio, en el carácter díscolo y malhumorado de Catalina Minola, mujer que ahuyenta, no pocas veces, a golpes a cuantos pretendientes se interesan por ella ante su padre. El asunto no tendría mayor transcendencia si no fuese porque, según la costumbre, el padre de Catalina, el rico Don Bautista Minola, se niega a entregar en matrimonio a su hija menor, Blanca, hasta que no haya casado a la mayor; para desconsuelo de los ambiciosos aspirantes a su mano, Hortensio, Gremio y Lucencio. La llegada a la ciudad de Petruchio, un joven ambicioso y despreocupado y su disposición a cortejar a la áspera Catalina proporcionan a los pretendientes de Blanca una esperanza para la que unen sus esfuerzos a los del ya casi desesperado Bautista. Este planteamiento inicial se desarrolla en forma de diversas situaciones de enredo y abundantes diálogos ocurrentes en los que el ingenio verbal se convierte sin duda en la más contundente de las armas, destacando el doble banquete nupcial con que concluye la obra y que constituye todo un giro inesperado a la situación de partida.

Mucho se ha escrito acerca del origen del argumento de la obra, aunque lo único en lo que la crítica autorizada parece estar de acuerdo es en el hecho de que el argumento principal no es original de Shakespeare. Lo cierto es que el esquema argumental básico de la obra se repite, con muy pequeñas variaciones, en multitud de textos de tradición oral o escrita diseminados por toda Europa, por Asia e incluso en la América precolombina. Uno de los más conocidos de entre esos textos es el cuento "Lo que sucedió a un mancebo que casó con una muchacha muy rebelde", número treinta y cinco de entre los incluidos en el famoso "El conde Lucanor", de don Juan Manuel. No obstante, entre todos esos relatos y el texto de Shakespeare existen diferencias fundamentales, y no solo en lo que concierne a su desenlace, que inducen a pensar que, si bien es muy probable que el dramaturgo isabelino conociera el argumento a partir de esa tradición, no se inspiró en ninguno de esos textos en concreto a la hora de componer su obra. Por lo demás, llama la atención el hecho de que, si bien los textos cuyo argumento se basa en la doma de una mujer bravía por parte de su marido son abundantes en la literatura europea previa a la publicación de "La fierecilla domada" (hacia 1590-1593), con la aparición de la obra de Shakespeare no se vuelve a publicar ninguna obra nueva en que este sea el principal asunto de su argumento, de modo muy similar a como ocurrió con las novelas de caballería y el "Quijote" de Miguel de Cervantes.

Al día siguiente se presentan en casa de Bautista el falso Lucencio (que no es más que el criado Tranio) con un profesor de latín (el verdadero Lucencio) y Petruchio con un profesor de música (Hortensio disfrazado). El falso Lucencio anuncia su interés por desposar a Blanca y Petruchio el suyo por Catalina ante el asombro general.

El encuentro entre Petruchio y Catalina es un desastre, ella se muestra muy arisca y él reacciona con gran ironía. Al final Petruchio anuncia su boda para dentro de una semana y abandona Padua por motivos personales.

Catalina llega a su nueva casa agotada del viaje mientras Petruchio la sigue tratando con muchísima ironía. Cualquier excusa es buena para no dejar comer ni dormir a Catalina y así "domarle" el carácter.

Cuando el verdadero Vincencio llega a Padua acompañado de Petruchio y Catalina, se prende la mecha de la confusión y el lío ante tanto cambio de identidades y tras algunos apuros todo se soluciona cuando el verdadero Lucencio cuenta toda la verdad del asunto y anuncia además, que se ha casado en secreto con Blanca. Todos se alegran y van al banquete donde Lucencio, Hortensio (ahora casado con una viuda tras abandonar su idea de seguir cortejando a Blanca) y Petruchio apuestan 100 coronas para ver quien tiene la mujer más obediente de todas. Dicha apuesta la gana Petruchio al quedar demostrada la obediencia de Catalina que ya no es la bravía que todos creían.





Algunas adaptaciones para la televisión son estas: 







</doc>
<doc id="6946" url="https://es.wikipedia.org/wiki?curid=6946" title="Anglicismo">
Anglicismo

Los anglicismos son préstamos lingüísticos del idioma inglés en otro idioma.

Son muy comunes en el lenguaje empleado por los adolescentes, debido a la influencia que los medios de comunicación regionales y foráneos tienen sobre su manera de hablar y expresarse; y también son frecuentes en el lenguaje técnico (principalmente en ciencias e ingeniería), a causa del montante de artículos de investigación científica que se publican en lengua inglesa y a la presencia de la misma en el desarrollo de las nuevas tecnologías.

Prácticamente, todas las secciones de los medios incorporan anglicismos: en la llamada prensa femenina encontramos términos como "shorts", "jeans", "gloss", "lifting" (como equivalente de Ritidectomía o Ritidosis), "celebrity", "mall", "blue jeans", "happy hour" y "shopping" En la información deportiva, los anglicismos tienen más presencia aún, y se usan en proporción directa con el origen extranjero del deporte, la novedad de este deporte entre hablantes de español, y su internacionalización.

En la adaptación al español de los préstamos lingüísticos deportivos se puede hablar de 3 etapas: incorporación, adaptación y presencia de términos sin traducir:

La analogía con expresiones del inglés ha generado un fenómeno de creación de palabras de apariencia inglesa sobre términos españoles (falsos anglicismos), como por ejemplo "puenting" (aceptada por la RAE.)

En las páginas de información científica y tecnológica de los periódicos, sin duda aparecen muchos préstamos lingüísticos. Los periodistas los usan porque piensan que, si traducen, perderían rigor o precisión; además, la traducción suele implicar el uso de más palabras. Ejemplos: síndrome del "burnout" (síndrome del trabajador quemado); "bluetooth" (dispositivo de transmisión de datos sin cables); "blog" o "weblog" (libro de bitácora en la red, o bitácora digital); Software (Aplicación informática); Windows (Vistas de una aplicación informática).

También hay muchos calcos semánticos en la Informática, cuando se podrían intentar traducir los términos, o usar palabras que ya existan en español. Por ejemplo: de "hard copy" se dice a veces "copia dura", pero lo más preciso es "copia impresa"; de "directory" se dice "directorio", que en castellano podría ser "guía"; se traduce "port" por "puerto" en vez de "vía de entrada"; a veces se dice "remover" por influencia del verbo inglés "to remove", que en realidad significa "eliminar; de hot keys se dice teclas calientes, pero se traduce como teclas rápidas."

La economía es otra sección donde hay muchos préstamos lingüísticos debido, en parte, a la globalización. Actualmente, la información económica tiene una sección especial en todos los periódicos, no como antes cuando era un pequeño recuadro con información bursátil. Obviamente existen muchos anglicismos, ya que el inglés también domina la economía. En muchos casos se suelen emplear términos como "desinversión" ("disinvestment"), "coaseguro" ("coinsurance"), "estanflación" ("stagflation"), "refinanciación" ("refinancing"), o "diseconomía" ("diseconomy"). Pero términos como "cash", "flow", "holding" o "stock", o incluso "dumping", se mantienen sin cambios, así como también "trust",) reflejándose esos usos en definiciones concretas en el "DRAE".

En las páginas dedicadas al ocio también abundan los términos ingleses, tales como "thriller, primetime, celebrity, reality show, singles, hobby", "spoiler", etc. 

Dentro del ámbito educativo, los anglicismos también se han incorporado, como es el caso de "alumni", "coaching", "test", "parenting", "campus", "master", etc. En general, todas las disciplinas se usan muchos anglicismos, lo que refleja el influjo que en general ejerce la cultura anglosajona.




















</doc>
<doc id="6956" url="https://es.wikipedia.org/wiki?curid=6956" title="Papúa Nueva Guinea">
Papúa Nueva Guinea

Papúa Nueva Guinea, oficialmente el Estado Independiente de Papúa Nueva Guinea (en inglés: "Independent State of Papua New Guinea"; en tok pisin: "Independen Stet bilong Papua Niugini"; en hiri motu: "Papua Niu Gini")— es un país soberano de Oceanía que ocupa la mitad oriental de la isla de Nueva Guinea (la otra mitad es parte del estado indonesio de Nueva Guinea Occidental) y una numerosa cantidad de islas situadas alrededor de esta. Su sistema de gobierno es la monarquía parlamentaria. Su territorio está organizado en veintidós provincias y su capital y ciudad más poblada es Puerto Moresby.

Está situado al norte de Australia, al oeste de las Islas Salomón y al suroeste del océano Pacífico, en una región definida desde inicios del siglo XIX como Melanesia. Es el único país de Oceanía que tiene frontera terrestre (Indonesia).

Papúa Nueva Guinea es uno de los países con mayor diversidad cultural del mundo y en donde se han contabilizado hasta 848 idiomas distintos, de los cuales siguen hablándose 836. Aún quedan muchas sociedades que se siguen rigiendo por costumbres tradicionales y aún sigue siendo un país escasamente poblado, solo con 7 millones de habitantes. Además tiene una población ampliamente rural, ya que solo el 18 % población está concentrada en núcleos urbanos.

Es uno de los países menos explorados, geográfica y culturalmente, y muchas especies de plantas y animales están aún sin descubrir dentro del país. Papúa está dentro de la lista de países megadiversos.

El fuerte crecimiento de la minería en Papúa Nueva Guinea ha incrementado el PIB hasta convertirse en el sexto país con el mayor incremento en 2011. A pesar de ello mucha gente vive en la pobreza extrema, con aproximadamente más de un tercio de la población viviendo con menos de 1,25 $ diarios. La mayor parte de la población vive aún de forma muy tradicional y su agricultura es de subsistencia. La constitución del país les reconoce su derecho, considerando que los pueblos tradicionales deben de ser las unidades viables de la sociedad de Papúa Nueva Guinea.
Aunque solo posee tres idiomas oficiales; en Papúa Nueva Guinea y sus alrededores (Indonesia, Islas Salomón, Timor Oriental y demás) se hablan más de 850 lenguas de las cuales 830 son autóctonas de Papúa Nueva Guinea, convirtiéndolo en el país más diverso del mundo a nivel lingüístico.

El doble nombre del país es el resultado de su compleja historia administrativa antes de su independencia. No se conoce el origen exacto de la palabra «Papúa», pero es posible que derive del malayo "papuah", que significa «rizado», en referencia al cabello de los melanesios nativos. Por otro lado, «Nueva Guinea» fue el nombre que recibió por parte del explorador español Yñigo Ortiz de Retez en 1545, por el parecido que notó entre los habitantes de la región y los nativos de la costa de Guinea en África.

En 1905, aquel territorio, ya conocido como la Nueva Guinea Británica, pasó a ser el Territorio de Papúa para diferenciarlo de la Nueva Guinea Alemana. Tras la Primera Guerra Mundial, los dos nombres se juntaron como Territorio de Papúa y Nueva Guinea, que más tarde se simplificó a Papúa Nueva Guinea.

Papúa Nueva Guinea es una monarquía constitucional con una democracia parlamentaria. Como miembro de la Mancomunidad de Naciones, se reconoce como jefe de Estado al rey o reina del Reino Unido, representado por un gobernador general. Este gobernador es elegido por el Parlamento, nombrado por el rey o reina, y participa principalmente en ceremonias oficiales.

El jefe del Gobierno es el primer ministro, elegido por el Parlamento Nacional unicameral de 109 miembros.

Los componentes del Parlamento son elegidos cada cinco años con los votos de las 19 provincias y del distrito de la capital nacional de Puerto Moresby. El primer ministro nombra a su gabinete, compuesto por miembros de su partido o coalición. Desde la independencia sus representantes han sido electos mediante un sistema de representación directa. Muchos puestos son disputados por una gran cantidad de candidatos, en ocasiones el ganador obtiene el triunfo con menos del 15 % de los votos.

Las 22 provincias están agrupadas en cuatro regiones. Si bien constituyen las divisiones geográficas más extensas del país, no tienen funciones administrativas ni políticas. Las cuatro regiones de Papúa Nueva Guinea y sus respectivas provincias son:

Papúa Nueva Guinea está dividida en veinte provincias, una región autónoma (Bougainville) y el Distrito Capital Nacional de Papúa Nueva Guinea.

El país posee gran cantidad de recursos naturales, aunque la explotación de los mismos siempre se ha visto obstaculizada por la carencia de infraestructuras y tecnología de desarrollo. No obstante las fuentes mineras, incluyendo el petróleo, el cobre y el oro, representan las cuatro quintas partes de sus exportaciones.

Mantiene una agricultura de subsistencia que sirve únicamente para el consumo local, si bien ha tomado cierto auge la industria maderera.

La pesca, explotada industrialmente en concesiones a otros países, constituye también una fuente importante de ingresos, pero muy afectada por los cambios climáticos de las corrientes marinas del Pacífico.

Las ayudas al desarrollo provienen en su mayor parte de Australia, si bien son de destacar también las que ofrece Japón y la Unión Europea (UE).

A pesar de las altas potencialidades del país, en 1995 fue necesaria la intervención del Fondo Monetario Internacional y del Banco Mundial para ajustar un programa de desarrollo, que debió rehacerse en 1997 tras los efectos de la sequía que mermó gravemente la producción de café, cacao, té, azúcar y coco. En la actualidad la situación se ha estabilizado, con un crecimiento en la producción agrícola de un 3,9 % de media anual desde el año 1999.

Papúa Nueva Guinea conforma uno de los países más diversos del planeta; existen 836 lenguas indígenas (el 12 % del total mundial) y al menos una mayoría de sociedades indígenas, con una población mayor a los 5 millones. Es también uno de los lugares más rurales, con solo un 18 % de la población viviendo en centros urbanos.

La población nativa está constituida por cientos de grupos étnicos, la mayoría de los cuales son papúes o hablantes de lenguas papúes, que habitan el país desde hace decenas de miles de años y están principalmente en la zona montañosa. El segundo grupo lo forman los hablantes de lenguas austronesias oceánicas, las cuales tienen un origen en antiguas migraciones malayas y habitan sobre todo en las costas. Estos dos grupos están bastante mezclados y constituyen la base de la población melanesia. Otros grupos étnicos presentes en Papúa Nueva Guinea son polinesios, micronesios, chinos, filipinos, europeos y australianos.

Hay tres idiomas oficiales en Papúa Nueva Guinea, el inglés es uno de ellos, aunque poca gente lo habla, y su uso únicamente es cotidiano en las ciudades. La mayoría de la gente en el norte habla la lengua criolla tok pisin, que es un pidgin del inglés usado como lengua franca. En la región sur de Papúa, la gente puede usar el tercer idioma oficial, el hiri motu antes que el tok pisin para este propósito.

Se han firmado acuerdos de entendimiento con Alemania para el estudio del alemán. Se habla también un alemán-criollo denominado unserdeutsch.

Cerca de un tercio de la población se adhiere a creencias indígenas, mientras el resto es cristiana. Cerca de un tercio de los cristianos son católicos, mientras que el resto está dividido entre varias denominaciones protestantes.

El mayor problema demográfico en la actualidad, es el aumento de personas viviendo con VIH/sida, siendo el país con la más alta incidencia en el Pacífico y el cuarto país de la zona que cumple con los criterios de epidemia generalizada para este virus. El principal problema es la ausencia de medidas preventivas para la transmisión de VIH, principalmente en las regiones rurales.

Evolución demográfica:

Papúa Nueva Guinea cuenta con un total de 839 lenguas, con lo cual es el país del mundo en el que se hablan más idiomas.

La cultura de Papúa Nueva Guinea es muy compleja: se estima que existen más de mil grupos culturales. A causa de esta diversidad, se puede encontrar una gran variedad de expresiones culturales; cada grupo ha creado su propia forma de arte, bailes, costumbres, música, etc.

La mayoría de estos grupos tiene su propio lenguaje, y existen muchos casos en los que cada aldea tiene un idioma único. Papúa Nueva Guinea tiene uno de los niveles más altos de diversidad idiomática en proporción a su demografía. Esto tiene que ver con la geografía local, la cual ha permitido que diversas comunidades existan históricamente separadas las unas de las otras desarrollando su propia lengua. La gente acostumbra a vivir en aldeas que subsisten gracias a la agricultura. La caza es una actividad común además de la recolección de algunas plantas salvajes. La gente respeta a la gente que se convierte en buen cazador, pescador y labrador.

En la ribera del río Sepik, un grupo de indígenas son conocidos por sus trabajos tallados en madera. Ellos crean formas de plantas o de animales, según su creencia ya que creen que son sus ancestros.

Las conchas marinas ya no son la moneda corriente en Papúa Nueva Guinea. Estas fueron abolidas como moneda corriente en 1933, pero esta herencia aún está presente en las costumbres locales; por ejemplo, para conseguir una novia, el novio debe conseguir una cierta cantidad de conchas de almejas de borde dorado.


En 2016 acogió la VIII Copa Mundial Femenina Sub-20 de la FIFA.




</doc>
<doc id="6957" url="https://es.wikipedia.org/wiki?curid=6957" title="Disco de marcar">
Disco de marcar

El disco de marcar es un dispositivo mecánico del que están dotados determinados tipos de teléfonos de disco antiguos para la marcación por pulsos.

Consiste en un disco giratorio provisto de diez agujeros numerados del 0 al 9 en los cuales el usuario introduce el dedo para hacer girar el disco hasta un tope denominado "traba". Alcanzada la traba, se libera el disco que retrocede por la acción de un muelle situado alrededor del eje de giro, hasta que el disco regresa a su posición original. La culminación de este procedimiento equivale a marcar una de las cifras del número telefónico completo pulsando un botón, en los teléfonos de botones. Un número telefónico completo se marca repitiendo el mismo procedimiento con cada cifra del mismo, siempre dejando que, marcada una cifra, el disco retorne a la posición inicial, antes de marcar la siguiente.

En este movimiento de retroceso, mediante una leva, se produce la apertura y cierre de la línea telefónica, también denominada bucle local o de abonado, un número de veces igual al dígito marcado (el 0 origina 10 impulsos). Estas aperturas y cierres del bucle son detectados y registrados por la central telefónica y dan lugar al accionamiento de los dispositivos de selección pertinentes con objeto de enlazar al usuario llamante con el llamado.

La pieza giratoria en la que se introduce el dedo se denomina carátula. Algunas carátulas de disco incorporan un portaetiquetas dentro del cual hay insertado un cartón, que queda a la vista, y sobre el cual el abonado al servicio telefónico puede anotar su número de teléfono.


</doc>
<doc id="6959" url="https://es.wikipedia.org/wiki?curid=6959" title="Micrófono">
Micrófono

Un micrófono (término acuñado en el siglo XVIII a partir del prefijo "micro", "pequeño" y el griego antiguo ϕωνήi - "foné", "voz") es un aparato que se usa para transformar las ondas sonoras en energía eléctrica y viceversa en procesos de grabación y reproducción de sonido; consiste esencialmente en un diafragma atraído por un electroimán, que, al vibrar, modifica la corriente transmitida por las diferentes presiones a un circuito. Un micrófono funciona como un transductor o sensor electroacústico y convierte el sonido (ondas sonoras) en una señal eléctrica para aumentar su intensidad, transmitirla y registrarla. Los micrófonos tienen múltiples aplicaciones en diferentes campos como en telefonía, ciencia, salud, transmisión de sonido en conciertos y eventos públicos, trasmisión de sonido en medios masivos de comunicación como producciones audiovisuales (cine y televisión), radio, producción en vivo y grabado de audio profesional, desarrollo de ingeniería de sonido, reconocimiento de voz y VoIP.

Actualmente, la mayoría de los micrófonos utilizan inducción electromagnética (micrófonos dinámicos), cambio de capacitancia (micrófonos de condensador) o piezoelectricidad (micrófonos piezoeléctricos) para producir una señal eléctrica a partir de las variaciones de la presión de aire. Los micrófonos usualmente requieren estar conectados a un preamplificador antes de que su señal pueda ser grabada o procesada y reproducida en altavoces o cualquier dispositivo de amplificación sonora.

Con el tiempo, la humanidad entendió la necesidad de desarrollar herramientas de comunicación más eficientes y de mayor alcance. Así, nació el deseo de aumentar el volumen de las palabras que buscaban ser transmitidas. El dispositivo de mayor antigüedad para lograr esto data de ; era una máscara con aperturas bucales que tenía un diseño acústico especial que incrementaba el volumen de la voz en los anfiteatros. En 1665, el físico inglés Robert Hooke fue el primero en experimentar con un elemento como el aire por medio de la invención del teléfono de lata que consistía en un alambre unido a una taza en cada una de sus extremos.

En 1827, Charles Wheatstone utiliza por primera vez la palabra "micrófono" para describir un dispositivo acústico diseñado para amplificar sonidos débiles. Entre 1870 y 1880 comenzó la historia del micrófono y las grabaciones de audio. El primer micrófono formaba parte del fonógrafo, que en esa época era el dispositivo más común para reproducir sonido grabado, y fue conocido como el primer micrófono dinámico.

El inventor alemán Johann Philipp Reis diseñó un transmisor de sonido rudimentario, que utilizaba una tira metálica unida a una membrana vibrante y producía una corriente intermitente. En 1876 Alexander Graham Bell inventó el teléfono y por primera vez incluyó un micrófono funcional que usaba un electroimán. Este dispositivo era conocido como 'transmisor líquido', con el diafragma conectado a una varilla conductora en una solución de ácido. Estos sistemas, sin embargo, ofrecieron una captación de sonido de muy baja calidad, lo que incitó a los inventores a seguir vías alternativas de diseño.

El primer dispositivo que permitió una comunicación de calidad fue el micrófono de carbón (entonces llamado transmisor), desarrollado independientemente por David Edward Hughes en Inglaterra y Emile Berliner y Thomas Edison en Estados Unidos. Aunque Edison obtuvo la primera patente (después de una larga disputa legal) a mediados de 1877, Hughes logró demostrar que su dispositivo había sido desarrollado durante años, en presencia de muchos testigos. De hecho, la mayoría de los historiadores lo acreditan con su invención.

El dispositivo de Hughes estaba formado por gránulos de carbón empaquetados sin compactar en un recipiente donde solo podía entrar el aire. Las ondas acústicas ejercían presión sobre las partículas de carbón, que reaccionaban y actuaban como un diafragma ejerciendo una resistencia variable al paso de la corriente eléctrica debido a su contenido en carbono, lo que permitía una reproducción relativamente precisa de la señal de sonido. Hizo una demostración de su aparato a la Real Sociedad de Londres magnificando el sonido de insectos a través de una caja de resonancia. La principal desventaja del dispositivo era que con el tiempo perdía sensibilidad. Contrariamente a lo que hizo el inventor estadounidense Thomas Alva Edison, quien solicitó una patente el día 27 de abril de 1877 para su desarrollo, Hughes decidió no registrar la patente donando su invención como un regalo para el mundo.

El micrófono de carbón fue el prototipo que dio origen, de forma directa, a los micrófonos que existen en la actualidad y fue fundamental en el desarrollo de la telefonía, la radiodifusión y la industria del entretenimiento.

Por su parte, Edison perfeccionó el micrófono de carbono en 1886, simplificándolo, consiguiendo una fabricación de bajo costo, y haciéndolo muy eficiente y duradero. Se convirtió en la base para los transmisores telefónicos usados en millones de teléfonos en todo el mundo. Este micrófono fue empleado en la primera emisión de radio de la historia, una actuación en el Metropolitan Opera House en 1910.

El siguiente paso importante en el diseño del transmisor estuvo en manos del inventor inglés Henry Hunnings. Utilizó gránulos de coque entre el diafragma y una placa metálica como soporte. Este diseño se originó en 1878 y fue patentado en 1879. Este transmisor era muy eficiente y podría competir con cualquiera de sus actuales competidores. Su único inconveniente era que tenía una tendencia a perder sensibilidad de captación.

En 1916, los Laboratorios Bell desarrollaron el primer micrófono de condensador.

Con el crecimiento de la industria musical y la radio en los años 1920 se estimuló el desarrollo de los micrófonos de carbón de una calidad mayor. El año 1920 se inició la era de los anuncios comerciales en los medios masivos de comunicación. La mayoría de profesionales en comunicación y los artistas de alto perfil como cantantes y estrellas comenzaron a usar los micrófonos en sus respectivos campos.

En 1923 se construyó el primer micrófono de bobina móvil con un uso práctico. Era denominado el "magnetófono de Marconi-Sykes". Desarrollado por el Capitán Henry Joseph Round, fue utilizado en los estudios de la BBC de Londres. Esta versión de micrófono fue mejorada en 1930 por Alan Blumlein y Herbert Holman, quienes desarrollaron el HB1A, el mejor micrófono en su momento. En el mismo año, se lanzó al mercado el micrófono de cinta, otro tipo de micrófono electromagnético, que se cree fue desarrollado por Harry F. Olson mediante el uso de ingeniería inversa en un altavoz antiguo.

En 1931 la Western Electric presentó el primer micrófono dinámico, el modelo 600, serie 618.

A través de los años, estos micrófonos fueron desarrollados por varias empresas, las mayores aportaciones a esta tecnología los hizo la compañía RCA, que introdujo grandes avances en el control de patrón polar, para dar direccionalidad a la captación del micrófono. Debido al auge del cine y la televisión, se incrementó la demanda de micrófonos de alta fidelidad y una mayor direccionalidad. El primer micrófono que se desarrolló para la industria del cine fue el PB17. Era un cilindro de aluminio de 17 pulgadas de largo y 6 pulgadas de diámetro, su estructura estaba magnetizada y utilizaba un electroimán que requería una corriente de seis voltios y un amperio.

Ya en el año 1947 se produce un evento importante para la historia del micrófono: se fundó la AKG en Viena, una empresa austríaca que empezó a fabricar accesorios profesionales de audio, en especial micrófonos y auriculares. Y en 1948 Neumann lanzó el micrófono de válvulas U47, el primer micrófono de condensador con patrón polar conmutable entre cardioide y omnidireccional. Acabó convirtiéndose en todo un clásico para grabar voces desde que se supo que Frank Sinatra se negaba a cantar sin su U47.

En 1962 Hideo Matsushita estableció la empresa Audio-Technica Corporation en Tokio. La compañía lanzó los modelos AT-1 y AT-3MM de cápsulas estereofónicas y empezó a suministrar cápsulas a fabricantes de audio. Posteriormente, en 1978, Audio-Technica lanzó los auriculares de condensador ATH-8 y ATH-7. Estos auriculares ganaron diversos premios. Este año también se produjo el desarrollo y lanzamiento de la Serie 800 de micrófonos, y la creación de Audio-Technica Ltd. en Leeds, Inglaterra

El fabricante Electro-Voice respondió a las demandas de la industria del cine desarrollando el "Shotgun microphone" o "Micrófono Boom" en 1963, que ofrecía una captación de audio con mayor enfoque gracias que era unidireccional.

Durante la segunda mitad del siglo XX el desarrollo en tecnología de micrófonos avanzó rápidamente, cuando los Hermanos Shure lanzaron al mercado los modelos SM58 y SM57. La compañía Milab fue pionera en la era digital al lanzar en 1999 el DM-1001. Las investigaciones más recientes incluyen el uso de fibras ópticas, láser e interferómetros.

Es la parte más delicada de un micrófono. En algunos lugares también recibe el nombre de pastilla, aunque generalmente este término se refiere al dispositivo que capta las vibraciones en los instrumentos como, por ejemplo, en una guitarra eléctrica. El diafragma es una membrana que recibe las vibraciones sonoras y está unido al sistema que transforma estas ondas en electricidad.

El dispositivo transductor sensible de un micrófono se llama "elemento" o "cápsula". Esta cápsula microfónica puede estar construida de diferentes maneras y, dependiendo del tipo de transductor, se pueden clasificar los micrófonos como dinámicos, de condensador, de carbón o piezoeléctricos.

Protege el diafragma. Evita tanto los golpes de sonido (las “p” y las “b”) así como los daños físicos que sufra por alguna caída.

Es el recipiente donde se colocan los componentes del micrófono. En los de mano, que son los más comunes, esta carcasa es de metales poco pesados, ligeros de portar pero resistentes a la hora de proteger el dispositivo transductor.

Los micrófonos son clasificados según su tipo de transductor, ya sea de condensador o dinámico, y por sus características direccionales. A veces, otras características tales como el tamaño de diafragma, el uso previsto o la orientación de la entrada de sonido principal se utilizan para clasificar el micrófono.

El "micrófono de condensador" (condenser microphone), fue inventado en los Laboratorios Bell en 1916 por Edward Christopher Wente. También llamado "micrófono electroestático" (electrostatic microphone) o "micrófono de capacitancia" (capacitor microphone), en este tipo de micrófonos el diafragma actúa como una placa que "condensa" las vibraciones de las ondas sonoras, que producen cambios debido a la variación de la distancia que hay entre el diafragma y la placa. Hay dos tipos, dependiendo del método de extracción de la señal de audio desde el transductor: micrófonos de polarización de CC, y micrófonos de condensador de frecuencia de radio (RF) o de alta frecuencia (HF).

En un micrófono de polarización de CC, las placas son sesgadas con una carga fija (Q). La tensión que existe entre las placas del condensador cambia con las vibraciones en el aire (de acuerdo con la ecuación de la capacitancia formula_1, donde Q = carga en culombios, C = capacitancia en faradios y V = diferencia de potencial en voltios). La capacitancia de las placas es inversamente proporcional a la distancia entre ellas para un condensador de placas paralelas. El montaje de placas fijas y móviles se llama un "elemento" o "cápsula".

En el condensador se mantiene una carga casi constante. Con los cambios de capacitancia, la carga a través del condensador cambia muy ligeramente, pero a frecuencias audibles es sensiblemente constante. La capacitancia de la cápsula (alrededor de 5 a 100pF) y el valor de la resistencia de polarización (100 mO a decenas de GΩ) forman un filtro que es de paso alto para la señal de audio, y de paso bajo para la tensión de polarización. Téngase en cuenta que la constante de tiempo de un circuito RC es igual al producto de la resistencia y la capacitancia.

Dentro del marco de tiempo de la variación de la capacidad (tanto como 50ms a 20Hz de una señal de audio), la carga es prácticamente constante y el voltaje a través del condensador cambia instantáneamente para reflejar el cambio en la capacitancia. El voltaje a través del condensador varía por encima y por debajo de la tensión de polarización. La diferencia de voltaje entre el sesgo y el condensador se detecta a través de la resistencia en serie. El voltaje a través del resistor es amplificado para mejorar su rendimiento o para su grabación. En la mayoría de los casos, la electrónica del propio micrófono contribuye a la ganancia de tensión, de forma que el diferencial de tensión es bastante significativo, hasta de varios voltios para niveles de sonido altos. Como se trata de un circuito de muy alta impedancia, la ganancia de corriente solo es la necesaria para modificar la tensión constante de referencia.

Utilizan una tensión de RF comparativamente baja, generada por un oscilador de bajo ruido. La señal del oscilador o bien puede ser modulada en amplitud por los cambios de capacitancia producidas por las ondas de sonido al mover el diafragma o cápsula, o la cápsula puede ser parte de un circuito resonante que modula la frecuencia de la señal del oscilador. La demodulación produce una señal de frecuencia de audio de bajo ruido, con una impedancia de fuente muy baja. La ausencia de una tensión de polarización alta permite el uso de un diafragma con la tensión más baja, que puede ser utilizado para lograr la respuesta de frecuencia más amplia debido a una mayor sensibilidad. Los resultados del proceso de polarización de RF en una cápsula de impedancia eléctrica más baja, permite que los micrófonos de condensador de RF pueden funcionar en condiciones climáticas húmedas, que podrían crear problemas en los micrófonos que utilizan una corriente de referencia-DC con superficies aislantes contaminadas. La serie de micrófonos Sennheiser "MKH" utiliza la técnica de empuje de RF.

Los micrófonos de condensador abarcan toda la gama de transmisores de telefonía, así como para otros usos, desde los micrófonos de karaoke de bajo costo hasta los micrófonos de grabación de alta fidelidad. Por lo general, producen una señal de audio de alta calidad y ahora son la elección habitual de laboratorios y estudios de grabación. La idoneidad inherente de esta tecnología se debe a la masa muy pequeña que debe ser movida por la onda sonora incidente, a diferencia de otros tipos de micrófonos que requieren que la onda de sonido realice un mayor trabajo mecánico. Requieren una fuente de alimentación, bien a través de las entradas de micrófono en el equipo como alimentación auxiliar o de una pequeña batería. Esta corriente es necesaria para el establecimiento de la tensión de placa del condensador de potencia, y también es necesaria para alimentar la electrónica de micrófono (conversión de impedancia en el caso de micrófonos electret y polarizadas-DC, demodulación o detección en el caso de micrófonos RF/HF). Los micrófonos de condensador también están disponibles con dos diafragmas que pueden ser conectados eléctricamente para proporcionar una gama de patrones polares (véase más adelante), como cardioide, omnidireccional, y en forma de ocho. También es posible variar el patrón continuamente con algunos micrófonos (por ejemplo el Røde NT2000 o el CAD M179).

Un micrófono de válvula es un micrófono de condensador que utiliza un tubo de vacío amplificador (válvula). Siguen siendo populares entre los entusiastas del sonido procesado por válvulas de vacío.

Un micrófono electret es un tipo de micrófono condensador inventado por Gerhard Sessler y Jim West en los Laboratorios Bell en 1962. La aplicación de una carga externa descrita anteriormente en los micrófonos de condensador se sustituye por una carga permanente en un "material electret", un material ferroeléctrico que ha sido permanentemente cargado eléctricamente o polarizado. El nombre proviene de "electr"ostatic y magn"et"; una carga estática se mantiene asociada en un electret por la alineación de las cargas estáticas en el material, de la misma forma en que un imán el magnetismo se hace permanente mediante la alineación de los dominios magnéticos en una pieza de hierro.

Debido a su buen funcionamiento y facilidad de fabricación, por lo tanto, de bajo coste, la gran mayoría de los micrófonos hechos hoy en día son micrófonos electret; un fabricante de semiconductores estima que la producción anual es de más de mil millones de unidades. Casi todos los teléfonos celulares, ordenadores, PDA y auriculares-micrófonos son del tipo electret. Se utilizan en muchas aplicaciones, desde la grabación de alta calidad y de solapa, hasta en los micrófonos incorporados en pequeños dispositivos de grabación de sonido y teléfonos. Aunque los micrófonos electret fueron considerados inicialmente de baja calidad, los mejores modelos de estos micrófonos pueden ahora competir con los modelos de condensadores tradicionales en todos los aspectos y pueden incluso ofrecer una mayor estabilidad a largo plazo y la respuesta ultra-plana necesaria para un micrófono de medición. A pesar de no requerir tensión de polarización, como otros micrófonos de condensador, a menudo contienen un sistema integrado preamplificador que requiere de energía (a menudo llamado incorrectamente potencia o sesgo de polarización). Este preamplificador es frecuentemente una alimentación fantasma para el refuerzo de sonido y aplicaciones de estudio. Algunos micrófonos monofónicos diseñados para computadoras personales (PC), a veces llamados micrófonos multimedia, utilizan un conector de 3,5mm, como se usa por lo general, sin toma de potencia, para los equipos estéreofónicos; el conector, en lugar de llevar la señal para un segundo canal, lleva la potencia eléctrica a través de una resistencia de (normalmente) un suministro de 5V en el ordenador. Los micrófonos estereofónicos utilizan el mismo conector; no hay forma obvia de determinar qué sistema es utilizado por equipos y micrófonos.

Solo los mejores micrófonos electret pueden rivalizar en términos de nivel de ruido y calidad con otros tipos de micrófonos de calidad. Por el contrario, se prestan a la producción en masa de bajo costo con unas prestaciones aceptables, lo que ha propiciado su uso masivo en todo tipo de dispositivos.

Los micrófonos dinámicos (también conocidos como micrófonos magneto-dinámicos) trabajan a través de la inducción electromagnética. Son robustos, relativamente baratos y resistentes a la humedad. Esto, junto con su potencial de alta ganancia antes de la retroalimentación, los hace ideales para su uso en el escenario.

Los micrófonos de bobina móvil utilizan el mismo principio dinámico que es utilizado en un altavoz, pero invertido. Una pequeña bobina de inducción móvil, situada en el campo magnético de un imán permanente, está unida a la membrana. Cuando el sonido entra a través de la rejilla del micrófono, la onda de sonido mueve el diafragma, desplazando la bobina que se mueve en el campo magnético, que a su vez produce una variación de corriente en la bobina a través de la inducción electromagnética. Una sola membrana dinámica no responde linealmente a todas las frecuencias de audio. Algunos micrófonos por esta razón utilizan múltiples membranas para las diferentes partes del espectro de audio y luego se combinan las señales resultantes. Combinar correctamente las múltiples señales es difícil, y los diseños capaces de hacerlo son raros y tienden a ser caros. Por otra parte, existen varios diseños que se dirigen más específicamente a partes aisladas del espectro de audio. El AKG D 112, por ejemplo, está diseñado para responder a los sonidos graves en lugar de los agudos. En la ingeniería de audio, se utilizan a menudo varios tipos de micrófonos al mismo tiempo para obtener el mejor resultado.

Los micrófonos de cinta utilizan una cinta delgada de metal (por lo general corrugada), suspendida en un campo magnético. La cinta está conectada eléctricamente a la salida del micrófono, y su vibración dentro del campo magnético genera la señal eléctrica. Los micrófonos de cinta son similares a los micrófonos de bobina (ambos producen sonido por medio de la inducción magnética). Detectan el sonido en un patrón bidireccional (también llamado en forma de ocho, como en el diagrama de abajo) porque la cinta está abierta en ambos lados, y porque tiene poca masa, por lo que responde a la velocidad del aire en lugar de a la presión del sonido. Aunque la parte delantera simétrica y la pastilla trasera pueden ser una molestia en la grabación estéreo normal, el rechazo del lado de alta se puede utilizar ventajosamente mediante la colocación de un micrófono de cinta horizontal, por ejemplo, por encima de los platillos de una batería, de modo que el lóbulo trasero recoge únicamente el sonido de los platillos. Las figuras cruzadas en forma de 8 o pares Blumlein, están ganando popularidad en la grabación estereofónica, y la disposición de la respuesta de un micrófono de cinta con forma de ocho es ideal para esa aplicación.

Otros patrones direccionales se pueden producir confinando un lado de la cinta en una "trampa acústica" o deflector, lo que permite que el sonido llegue de un solo lado. El clásico micrófono RCA Tipo 77-DX tiene varias posiciones externamente ajustables del deflector interno, lo que permite la selección de varios patrones de respuesta que van desde la "forma de ocho" a "unidireccional". Estos micrófonos de cinta mayores, algunos de los cuales siguen ofreciendo una reproducción de sonido de alta calidad, fueron en su momento muy valorados por esta razón, pero solo podían obtener una buena respuesta de baja frecuencia cuando la cinta permanecía correctamente suspendida, lo que les hizo relativamente frágiles. Los materiales utilizados en la cinta se han modernizado, incluyendo nuevos nanomateriales, lo que ha permitido hacer estos micrófonos más fiables, e incluso mejorar su rango dinámico efectivo en las frecuencias bajas. Las pantallas anti-viento de protección pueden reducir el peligro de dañar una cinta antigua, y también reducir las explosiones sonoras en la grabación. Las pantallas de viento correctamente diseñadas producen una atenuación de agudos insignificante. Al igual que otros tipos de micrófono dinámico, los micrófonos de cinta no requieren alimentación auxiliar; de hecho, este voltaje puede dañar algunos micrófonos de cinta antiguos. Algunos nuevos diseños modernos de micrófonos de cinta incorporan un preamplificador y, por lo tanto, requieren alimentación auxiliar. Los circuitos de los micrófonos de cinta pasiva modernos, es decir, los que no tienen el preamplificador mencionado, están diseñados específicamente para resistir el daño a la cinta y al transformador de alimentación auxiliar. También hay nuevos materiales de cinta disponibles que son inmunes al viento, a las explosiones sonoras y a la alimentación auxiliar.

Un micrófono de carbono, también conocido como micrófono de botón, utiliza una cápsula o botón que contiene gránulos de carbón prensado entre dos placas de metal como los micrófonos de Berliner y Edison. Aplicando un voltaje a través de las placas de metal, provoca que una pequeña corriente eléctrica fluya hacia el carbono. Una de las placas, el diafragma, vibra en sintonía con las ondas de sonido incidente, aplicando una presión variable a los gránulos de carbón. El cambio de presión deforma los gránulos, causando que el área de contacto entre cada par de gránulos adyacentes cambie, y esto provoca que la resistencia eléctrica de la masa de gránulos cambie. Los cambios en la resistencia produce un cambio correspondiente en el flujo de corriente a través del micrófono, produciendo la señal eléctrica. Hubo una época en que los micrófonos de carbono fueron usados comúnmente en telefonía; tienen una calidad de reproducción de sonido extremadamente baja y un rango de respuesta de frecuencias muy limitado, pero son dispositivos muy robustos. El micrófono de Boudet, que utiliza bolas de carbono relativamente grandes, fue similar a los micrófonos de botón de carbono granular.

A diferencia de otros tipos de micrófonos, el micrófono de carbono también puede ser utilizado como un tipo de amplificador, usando una pequeña cantidad de energía eléctrica. En su inicio, los micrófonos de carbono se utilizaban como repetidores telefónicos, haciendo posible las llamadas de larga distancia en la era anterior a los tubos de vacío. Estos repetidores trabajan mecánicamente, acoplando un receptor telefónico magnético al micrófono de carbono: la débil señal del receptor era transferida al micrófono, donde era modulada en una fuerte corriente eléctrica, produciendo a su vez una fuerte señal eléctrica para enviar por la línea. Una consecuencia de este efecto amplificador era la oscilación producida por retroalimentación, resultando en un chillido audible en los primitivos teléfonos de pared cuando el auricular se colocaba cerca del micrófono de carbono.

Un micrófono de cristal o piezo micrófono utiliza el fenómeno de la piezoelectricidad —la capacidad de algunos materiales para producir un voltaje cuando se someten a presión, para convertir las vibraciones en una señal eléctrica—. Un ejemplo de esto es el tartrato de sodio y potasio, que es un cristal piezoeléctrico que funciona como un transductor (en forma de componente extraplano), indistintamente como un micrófono o como un altavoz. Los "micrófonos de cristal" eran suministrados comúnmente con los equipos de tubos de vacío (válvulas), tales como grabadoras domésticas. Su alta impedancia de salida coincide también con la alta impedancia (típicamente de aproximadamente 10 megaohmios) de la etapa de entrada de los tubos de vacío. Eran difíciles de igualar al comienzo de los equipos transistorizados, pero fueron sustituidos rápidamente por los micrófonos dinámicos durante un tiempo, y más tarde por los pequeños dispositivos de condensador electret. La alta impedancia de los micrófonos de cristal los hizo muy susceptibles a los ruidos parásitos, tanto desde el propio micrófono como desde el cable de conexión.

Los transductores piezoeléctricos se utilizan a menudo como micrófonos de contacto para amplificar el sonido de los instrumentos musicales acústicos, para detectar golpes de tambor, para disparar muestras electrónicas, y para grabar sonido en entornos difíciles, como bajo el agua a alta presión. Las pastillas montadas en guitarras acústicas son generalmente dispositivos piezoeléctricos en contacto con las cuerdas. Este tipo de micrófono es diferente de las pastillas de bobina magnética comúnmente visibles en las típicas guitarras eléctricas, que utilizan la inducción magnética, en lugar del acoplamiento mecánico, para recoger las vibraciones.

Un micrófono de fibra óptica convierte las ondas acústicas en señales eléctricas mediante la detección de cambios en la intensidad de la luz, en lugar de detectar cambios en la capacitancia o en campos magnéticos, como con los micrófonos convencionales.

Durante su funcionamiento, la luz de una fuente láser viaja a través de una fibra óptica para iluminar la superficie de un diafragma reflectante. Las vibraciones del sonido del diafragma modulan la intensidad de la luz que refleja el diafragma en una dirección específica. La luz modulada se transmite entonces a través de una segunda fibra óptica a un fotodetector, que transforma la luz de intensidad modulada en audio analógico o digital para su transmisión o grabación. Los micrófonos de fibra óptica poseen un alto rango dinámico y de frecuencia, similar al de los mejores micrófonos convencionales de alta fidelidad.

Además, no son influidos por campos eléctricos, magnéticos, electrostáticos o radiactivos (esto se llama inmunidad EMI/RFI). El diseño del micrófono de fibra óptica por lo tanto, es ideal para su uso en áreas donde los micrófonos convencionales son ineficaces o peligrosos, como el interior de turbinas industriales o en el entorno de equipos de resonancia magnética (MRI).

Son robustos, resistentes a los cambios ambientales de temperatura y humedad, y se pueden producir para cualquier direccionalidad o adaptación de impedancia. La distancia entre la fuente de luz del micrófono y su fotodetector puede ser de hasta varios kilómetros sin necesidad de preamplificador u de cualquier otro dispositivo eléctrico, por lo que los micrófonos de fibra óptica son adecuados para la monitorización acústica industrial y la vigilancia.

Se utilizan en áreas de aplicación muy específicas, como la detección de infrasonidos y en la cancelación de ruido. Han demostrado ser especialmente útiles en aplicaciones médicas, permitiendo que puedan comunicarse con normalidad los radiólogos, el personal y los pacientes situados dentro del potente campo magnético y del ambiente ruidoso en las salas con equipos de resonancia magnética, así como en las salas de control a distancia. Otros usos incluyen el monitorizado de equipos industriales y detección, calibración y medición de audio, grabación de alta fidelidad y cumplimiento de los niveles sonoros limitados por la ley.

Los micrófonos láser aparecen a menudo en las películas como "gadgets" de espionaje, ya que pueden ser utilizados para recoger el sonido a distancia desde el equipo de microfónico. Un rayo láser se dirige a la superficie de una ventana u otra superficie plana que se ve afectada por el sonido. Las vibraciones de esta superficie cambian el ángulo en el que el haz se refleja, permitiendo detectar el movimiento del punto del haz láser, que tras regresar al equipo se convierte en una señal de audio.

En una aplicación más robusta y cara, la luz devuelta se divide y alimenta un interferómetro, que detecta el movimiento de la superficie por los cambios en la longitud del camino óptico del haz reflejado. Se trata de un desarrollo experimental; puesto que requiere un láser extremadamente estable y ópticas muy precisas.

Un nuevo tipo de micrófono láser es un dispositivo que utiliza un haz de láser y humo o vapor para detectar las vibraciones sonoras al aire libre. El 25 de agosto de 2009, la patente de EE.UU. 7.580.533 expedida para un micrófono de detección de partículas de flujo basado en el acoplamiento de láser y fotocélula, con una corriente en movimiento del humo o vapor en la trayectoria del rayo láser. Las ondas de presión del sonido causan perturbaciones en el humo, que a su vez causan variaciones en la cantidad de luz láser que llega al fotodetector. Un prototipo del dispositivo se demostró en la 127a convención de la Audio Engineering Society en Nueva York del 9 al 12 de octubre de 2009.

Los primeros micrófonos no permitieron reproducir el habla de forma inteligible, hasta que Alexander Graham Bell hizo mejoras incluyendo una resistencia variable entre micrófono y transmisor. El transmisor líquido de Bell consistía en un recipiente de metal lleno de agua con una pequeña cantidad de ácido sulfúrico añadido. Una onda de sonido provocaba que el diafragma se moviera, forzando que una aguja se moviera hacia arriba y hacia abajo en el agua. La resistencia eléctrica entre el alambre y el recipiente era entonces inversamente proporcional al tamaño del menisco de agua alrededor de la aguja sumergida. Elisha Gray presentó el anuncio de una versión con una varilla de bronce en lugar de la aguja. Se presentaron otras variantes y mejoras menores al micrófono líquido (ideadas por Majoranna, Chambers, Vanni, Sykes, y Elisha Gray), y Reginald Fessenden patentó su propia versión en 1903. Estos fueron los primeros micrófonos, pero no eran prácticos para su aplicación comercial. La famosa primera conversación telefónica entre Bell y Watson se llevó a cabo utilizando un micrófono líquido.

Los micrófonos del tipo MEMS ("Microelectromechanical systems" en inglés), también son denominados chips microfónicos o micrófonos de silicio. Un diafragma sensible a la presión se graba directamente en una oblea de silicio mediante técnicas de procesamiento de MEMS, y por lo general se acompaña con un preamplificador integrado. La mayoría de los micrófonos MEMS son variantes del diseño del micrófono de condensador. Los MEMS digitales se han construido integrados en circuitos analógico-a-digital (ADC) en el mismo chip CMOS, haciendo del chip un micrófono digital completo, más fácilmente incorporable a productos digitales modernos. Los principales fabricantes que producen micrófonos MEMS de silicio son Wolfson Microelectrónica (WM7xxx) ahora Cirrus Logic, Analog Devices, Akustica (AKU200x), Infineon (producto SMM310), Knowles Electronics, MemsTech (MSMx), NXP Semiconductors (división comprada por Knowles), Sonion MEMS, Vesper, Tecnologías acústicas AAC y Omron.

Más recientemente, ha aumentado el interés y la investigación en la fabricación de MEMS piezoeléctricos, que suponen un cambio de arquitectura y materiales significativo respecto a los diseños de MEMS existentes, basados en la tecnología del condensador.

Un altavoz es un transductor que convierte una señal eléctrica en ondas de sonido. Funcionalmente, es lo opuesto a un micrófono. Dado que los altavoces convencionales se construyen de forma muy parecida a un micrófono dinámico (con un diafragma, la bobina y el imán), los altavoces en realidad puede trabajar "a la inversa", como micrófonos. El resultado, sin embargo, es un micrófono con mala calidad, respuesta de frecuencia limitada (sobre todo en el extremo superior), y una pobre sensibilidad. En la práctica, los altavoces se utilizan a veces como micrófonos en aplicaciones donde la alta calidad y la sensibilidad no se necesitan, como interfonos, walkie-talkies o videojuegos de chat de voz periféricos, o cuando los micrófonos convencionales son escasos.

Sin embargo, hay al menos otra aplicación práctica de este principio: el uso de un altavoz de tamaño medio colocado muy próximo, frente al pedal del bombo de una batería, para actuar como un micrófono. El uso de altavoces relativamente grandes para transducir fuentes de sonido de baja frecuencia, sobre todo en la producción de música, se está volviendo bastante común. Un ejemplo de un producto de este tipo de dispositivo es el Yamaha SUBKICK, un altavoz de graves de 6,5 pulgadas (170mm) montado frente a los instrumentos de percusión. Al poseer una membrana relativamente pesada, no es capaz de transducir altas frecuencias, por lo que la colocación de un altavoz delante de un bombo a menudo es ideal para captar el sonido del bombo. Con menos frecuencia, los micrófonos en sí mismos se pueden utilizar como altavoces, casi siempre para reproducir sonidos agudos. Los micrófonos, sin embargo, no están diseñados para manejar la potencias que se requieren habitualmente utilizadas para activar los altavoces. Un ejemplo de tal aplicación fue "súper tweeter" STC 4001, derivado de un micrófono. Este dispositivo fue utilizado con éxito en una serie de sistemas de altavoces de alta calidad de la década de 1960 hasta mediados de los años 70.

Los elementos internos de un micrófono son la principal fuente de diferencias en la direccionalidad. Un micrófono de presión usa un diafragma entre un volumen interno fijo de aire y el ambiente, y responde uniformemente a la presión desde todas las direcciones, por lo que se dice que es omnidireccional. Un micrófono de gradiente de presión utiliza un diafragma que está al menos parcialmente abierto en ambos lados. La diferencia de presión entre los dos lados produce sus características direccionales. Otros elementos, como la forma externa del micrófono y los dispositivos externos, como los tubos de interferencia, también pueden alterar la respuesta direccional de un micrófono. Un micrófono de gradiente de presión puro es igualmente sensible a los sonidos que llegan desde adelante o atrás, pero insensible a los sonidos que llegan desde un lado porque el sonido que llega al frente y atrás al mismo tiempo no crea gradiente entre los dos. El patrón direccional característico de un micrófono de gradiente de presión puro es como una figura en forma de 8. Otros patrones polares se generan al crear una cápsula que combina estos dos efectos de diferentes maneras. El cardioide, por ejemplo, presenta una parte trasera parcialmente cerrada, por lo que su respuesta es una combinación de características de presión y gradiente de presión.

(Gráficas de los diferentes tipos de patrones polares):
La direccionalidad de un micrófono o patrón polar indica de qué manera es sensible a los sonidos que llegan en diferentes ángulos alrededor de su eje central. Los patrones polares ilustrados anteriormente representan el lugar geométrico de los puntos que producen la misma salida de nivel de señal en el micrófono si un determinado nivel de presión sonora (SPL) se genera a partir de ese punto. La forma en que el cuerpo físico del micrófono se orienta en relación con los diagramas depende del diseño del micrófono. Para los micrófonos de gran membrana como en el Oktava (foto superior), la dirección hacia arriba en el diagrama polar es generalmente perpendicular al cuerpo del micrófono, comúnmente conocido como "lado de fuego" o "dirección de lado". Para los pequeños micrófonos de diafragma, como el Shure (también en la foto de arriba), por lo general se extiende desde el eje del micrófono comúnmente conocido como "fuego final" o "dirección de la parte superior/fin".

Algunos diseños de micrófonos combinan varios principios en la creación del patrón polar deseado. Esto va desde el blindaje del propio alojamiento (lo que significa difracción/disipación/absorción), hasta combinar electrónicamente membranas duales.

La respuesta de un micrófono omnidireccional (o no direccional) se considera generalmente que es una esfera perfecta en tres dimensiones. En el mundo real, este no es el caso. Como con los micrófonos direccionales, el patrón polar de un micrófono "omnidireccional" es una función de la frecuencia. El cuerpo del micrófono no es infinitamente pequeño y, como consecuencia, tiende a interferir en su propio campo con respecto a los sonidos que llegan desde la parte trasera, provocando un ligero aplanamiento de la respuesta polar. Este aplanamiento aumenta a medida que el diámetro del micrófono (asumiendo que es cilíndrico) llega a la longitud de onda de la frecuencia en cuestión. Por lo tanto, el micrófono de diámetro más pequeño da las mejores características omnidireccionales a altas frecuencias.

La longitud de onda del sonido a 10kHz es poco más de una pulgada (3,4cm). Los micrófonos de medición más pequeños suelen ser de 1/4" (6 mm) de diámetro, lo que prácticamente elimina la direccionalidad incluso hasta de las frecuencias más altas. Los micrófonos omnidireccionales, a diferencia de los cardioides, no emplean cavidades resonantes, por lo que pueden ser considerados los micrófonos "más puros" en términos de baja coloración; agregan muy poca distorsión al sonido original. Ser sensible a la presión puede requerir una respuesta de baja frecuencia muy plana hasta los 20Hz o por debajo, por lo que los micrófonos sensibles a la presión también responden mucho menos al ruido del viento y a las oclusivas (velocidad sensible) que los micrófonos direccionales.

Un ejemplo de un micrófono no direccional es el modelo "8-Ball", cuyo diseño es una esfera de color negro.

Un micrófono unidireccional es sensible a los sonidos de una sola dirección. El diagrama anterior ilustra varios de estos patrones. En cada diagrama, el micrófono está orientado hacia arriba. La intensidad del sonido de una frecuencia particular se mide perimetralmente de 0 a 360°. Los diagramas profesionales muestran estas escalas e incluyen varias gráficas con diferentes frecuencias. Los diagramas anteriores solo proporcionan una visión general de las formas típicas de los patrones habituales, y facilitan sus nombres.

El micrófono unidireccional más común es el micrófono cardioide, llamado así debido a que el patrón de sensibilidad tiene "forma de corazón", es decir, una curva cardioide. La familia de micrófonos cardioides se utilizan comúnmente como micrófonos vocales o del habla, ya que son buenos en el rechazo de los sonidos de otras direcciones. En tres dimensiones, el cardioide tiene la forma de una manzana, centrada alrededor del micrófono que sería el "tallo" de la manzana. La respuesta cardioide reduce la captación trasera y desde los lados, ayudando a evitar la retroalimentación de los monitores. Estos micrófonos son direccionales respecto al gradiente de presión del transductor, por lo que ponerlos muy cerca de la fuente de sonido (a distancias de unos pocos centímetros) se traduce en un refuerzo de los graves. Esto se conoce como el "efecto de proximidad". El SM58 ha sido el micrófono más utilizado para voces en directo durante más de 50 años, lo que demuestra la importancia y la popularidad de los micrófonos cardioides.

Un micrófono cardioide es efectivamente una superposición de un micrófono omnidireccional y de un micrófono en figura en 8. Con esta disposición, las ondas sonoras procedentes de la parte de atrás, la señal negativa del dispositivo con figura en 8, cancela la señal positiva del elemento omnidireccional, mientras que para las ondas de sonido que vienen de la parte delantera, los dos se suman entre sí. Un micrófono hipercardioide es similar, pero con una figura en 8 un poco más grande, lo que produce una zona más estrecha de sensibilidad frontal y un lóbulo menor de sensibilidad trasera. Un micrófono supercardioide es similar a uno hiper-cardioide, excepto en que posee una mayor sensibilidad frontal y una trasera todavía menor. Mientras que cualquier patrón entre el omnidireccional y la figura en 8 es posible mediante el ajuste de su mezcla, las definiciones comunes afirman que un hipercardioide se produce mediante la combinación de ambos en una proporción de 3:1, produciendo sensibilidad nula a 109,5 °, mientras que un supercardioide se genera con una relación 5:3, con sensibilidad nula a 126,9°. El micrófono sub-cardioide no tiene puntos nulos. Se produce con una relación de aproximadamente 7:3, con un nivel de 3-10 dB entre la toma delantera y la posterior.

Los micrófonos en "Figura de 8" o micrófonos bidireccionales, reciben el sonido por igual de las partes delantera y posterior del elemento. La mayoría de los micrófonos de cinta son de este tipo. En principio no responden a la presión sonara en absoluto, excepto para el cambio de presión entre la parte delantera y la parte posterior; desde su llegada, el sonido alcanza la parte delantera y la trasera de igual manera, y no hay diferencia en la presión. Por lo tanto, no responden al sonido de esa dirección. En términos matemáticos, mientras que los micrófonos omnidireccionales son transductores escalares que responden a la presión desde cualquier dirección, los micrófonos bidireccionales son transductores vectoriales que responden al gradiente a lo largo de un eje normal al plano del diafragma. Esto también tiene el efecto de invertir la polaridad de salida para los sonidos que llegan desde el lado posterior.

Los micrófonos "de cañón" son altamente direccionales. Su patrón direccional tiene un lóbulo muy estrecho en la dirección hacia adelante y rechaza el sonido de otras direcciones. Tienen pequeños lóbulos de sensibilidad a la izquierda, a la derecha, y en la parte trasera, pero son mucho menos sensibles en la parte trasera que otros micrófonos direccionales. Esto es consecuencia de la colocación del elemento en el extremo posterior de un tubo con ranuras cortadas a lo largo del lateral; la cancelación de ondas elimina gran parte del sonido fuera del eje. Debido a la estrechez de su área de sensibilidad, los micrófonos de cañón se utilizan comúnmente en las cámaras de televisión y de cine, en los estadios, y para la grabación de campo de la vida silvestre. Los micrófonos parabólicos tienen características similares, pero a menudo tienen una respuesta de graves más pobre.

Varios enfoques han sido desarrollados para la utilización eficaz de un micrófono en espacios acústicos no ideales, que a menudo sufren de reflexiones excesivas de una o más de las superficies (límites) que componen el espacio. Si el micrófono se coloca en, o muy cerca de, uno de estos límites, las reflexiones de superficie no son detectadas por el micrófono. Inicialmente esto se hizo mediante la colocación de un micrófono normal adyacente a la superficie, a veces en un bloque de espuma acústicamente transparente. Los ingenieros de sonido Ed Long y Ron Wickersham desarrollaron el concepto de colocar el diafragma en paralelo y hacia la frontera. Hasta que la patente expire, los términos "Pressure Zone Microphone" y "PZM" siguen siendo marcas activas de Crown International, por lo que es preferible utilizar el término genérico "micrófono de superficie". Mientras que el micrófono de superficie se diseñó inicialmente utilizando un elemento omnidireccional, también es posible montar un micrófono direccional lo suficientemente cerca de la superficie para obtener algunos de los beneficios de esta técnica, al tiempo que conserva las propiedades direccionales del elemento. La marca registrada de Crown de este enfoque se denomina "Phase Coherent Cardioid" o "PCC", pero hay otros fabricantes que emplean esta técnica también.
Un micrófono de solapa está diseñado para operar con manos libres. Estos pequeños micrófonos se usan fijados a la ropa de las personas. Originalmente, se sujetaban con un cordón de seguridad alrededor del cuello, pero más a menudo se colocan sobre la ropa con un clip, alfiler, cinta o imán. El cordón de solapa puede ocultarse en la ropa y conectarse a un transmisor de radiofrecuencia guardado en un bolsillo o sujetarse a una correa (para uso móvil), o puede pasar directamente al mezclador (para aplicaciones en las que permanecen en el mismo sitio).

Un micrófono inalámbrico transmite el audio como una señal de radio u óptica, en vez de a través de un cable. Por lo general, envía su señal usando un pequeño transmisor de radio FM a un receptor cercano conectado al sistema de sonido, pero también puede usar ondas infrarrojas si el transmisor y el receptor están a la vista el uno del otro.

Un micrófono de cerámica recoge las vibraciones directamente de una superficie sólida u objeto, a diferencia de las vibraciones de sonido que se transmiten por el aire. Un uso para estos dispositivos es detectar sonidos de un nivel muy bajo, como los de objetos pequeños o el de los insectos. El micrófono comúnmente consiste en un transductor magnético (bobina móvil), placa de contacto y pin de contacto. La placa de contacto se coloca directamente en la parte vibrante de un instrumento musical u otra superficie, y el pin de contacto transfiere las vibraciones a la bobina. Los micrófonos de contacto se han utilizado para captar el sonido del latido de un caracol y los pasos de las hormigas. Recientemente se ha desarrollado una versión portátil de este micrófono. Un micrófono de garganta es una variante del micrófono de contacto que capta el habla directamente de la garganta de una persona, a la que está sujeto. Esto permite que el dispositivo se use en áreas con sonidos ambientales que de otra manera harían que la voz no fuese audible.

Un micrófono parabólico utiliza un reflector parabólico para recolectar y enfocar las ondas de sonido en un receptor microfónico, de la misma manera que lo hace una antena parabólica (por ejemplo, un plato satelital) con las ondas de radio. Los usos típicos de este micrófono, que tiene una sensibilidad frontal inusualmente enfocada y puede captar sonidos desde muchos metros de distancia, incluyen grabación de naturaleza, eventos deportivos al aire libre, escuchas clandestinas, investigaciones policiales e incluso espionaje. Los micrófonos parabólicos no suelen utilizarse para aplicaciones de grabación estándar, ya que tienden a tener una baja respuesta de baja frecuencia como efecto secundario de su diseño.

Un micrófono estéreo integra dos micrófonos en una unidad para producir una señal estereofónica. Se usan a menudo para aplicaciones de radiodifusión o grabación de campo, donde sería poco práctico configurar dos micrófonos de condensador separados en una configuración X-Y clásica (véase práctica microfónica) para grabación estereofónica. Algunos de estos micrófonos tienen un ángulo de cobertura ajustable entre los dos canales.

Un micrófono cancelador de ruido es un diseño altamente direccional, ideado para entornos ruidosos. Uno de estos usos es en las cabinas de las aeronaves, donde normalmente se instalan como micrófonos de barbilla junto a los auriculares. Otro uso es en eventos musicales en directo, en escenarios de conciertos con música a elevado volumen, donde son utilizados por los vocalistas de conciertos en vivo. Muchos micrófonos con cancelación de ruido combinan las señales recibidas de dos diafragmas que están en polaridad eléctrica opuesta o se procesan electrónicamente. En los diseños de doble diafragma, el diafragma principal se monta más cerca de la fuente deseada y el segundo se ubica más lejos de la fuente para que pueda recoger los sonidos ambientales que se restarán de la señal del diafragma principal. Después de que las dos señales se hayan combinado, los sonidos que no sean la fuente deseada se reducen considerablemente, lo que aumenta sustancialmente la inteligibilidad del sonido procesado. Otros diseños de cancelación de ruido utilizan un diafragma que se ve afectado por los puertos abiertos a los lados y la parte posterior del micrófono, con una suma de 16 dB de rechazo de los sonidos que están más lejos. Vocalistas como Garth Brooks o Janet Jackson han utilizado con frecuencia un diseño de auricular con cancelación de ruido de un solo diafragma. Algunos micrófonos con anulación de ruido son micrófonos de garganta.

Se usan varias técnicas estándar con micrófonos utilizados en sistemas de refuerzo de sonido en actuaciones en vivo, o para grabar en un estudio de sonido o cinematográfico. Mediante la disposición adecuada de uno o más micrófonos, se pueden conservar las características deseables del sonido que se va a recoger, al tiempo que se rechazan los sonidos no deseados.

Los micrófonos que contienen circuitos activos, como la mayoría de los micrófonos de condensador, requieren energía para operar los componentes activos. El primero de estos micrófonos utilizaba circuitos de tubo de vacío con una unidad de fuente de alimentación separada, empleando un cable y un conector multipolo. Con el advenimiento de la amplificación de estado sólido, los requisitos de potencia se redujeron en gran medida y se hizo práctico usar los mismos conductores de cable y conector para audio y potencia. Durante la década de 1960 se desarrollaron varios métodos de alimentación, principalmente en Europa. Los dos métodos dominantes se definieron inicialmente en alemán según los estándares DIN 45595 como o "T-potencia" y DIN 45596 para la alimentación fantasma. Desde la década de 1980, la "alimentación fantasma" se ha vuelto mucho más común, porque la misma entrada se puede utilizar para micrófonos con y sin alimentación. En los productos electrónicos de consumo tales como cámaras réflex digitales y videocámaras, la "conexión con potencia eléctrica" es más común, para micrófonos que usan un conector de enchufe de teléfono de 3.5 mm. Las fuentes "Fantasma" (Phanton), "T-power" y "plug-in power" se describen en la norma internacional IEC 61938.

Los conectores más comunes utilizados por los micrófonos son:

Algunos micrófonos usan otros conectores, como un XLR de 5 pines o un mini XLR para la conexión a equipos portátiles. Algunos micrófonos de solapa utilizan un conector patentado para conectarse a un transmisor inalámbrico, como un "radio pack". Desde 2005, comenzaron a aparecer micrófonos de calidad profesional con conexiones USB, diseñados para la grabación directa para programas de ordenador.

Los micrófonos tienen una característica eléctrica llamada impedancia, medida en ohmios (Ω), que depende de su diseño. En micrófonos pasivos, este valor describe la resistencia eléctrica de la bobina del imán (o mecanismo similar). En los micrófonos activos, este valor describe la resistencia de salida de los circuitos del amplificador. Por lo general, se establece la "impedancia nominal". La baja impedancia se considera en 600 Ω. La impedancia media se considera entre 600 Ω y 10 kΩ. La alta impedancia es superior a 10 kΩ. Debido a su amplificador electrónico incorporado, los micrófonos de condensador típicamente tienen una impedancia de salida entre 50 y 200 Ω.

La salida de un micrófono dado entrega la misma potencia ya sea de baja o alta impedancia. Si un micrófono se fábrica en versiones de alta y baja impedancia, la versión de alta impedancia tiene un voltaje de salida más alto para una entrada de presión de sonido dada, y es adecuado para usar con amplificadores de guitarra de tubo de vacío, por ejemplo, que tienen una impedancia de entrada alta y requieren un voltaje de entrada de señal relativamente alto para superar el ruido inherente de los tubos. La mayoría de los micrófonos profesionales son de baja impedancia, alrededor de 200 Ω o menos. El equipo de sonido profesional de tubo de vacío incorpora un transformador que aumenta la impedancia del circuito del micrófono a la alta impedancia y al voltaje necesario para activar el tubo de entrada. También hay disponibles transformadores de adaptación externos que se pueden usar en línea entre un micrófono de baja impedancia y una entrada de alta impedancia.

Los micrófonos de baja impedancia son preferibles a los de alta impedancia por dos razones: una es que usar un micrófono de alta impedancia con un cable largo produce una pérdida de señal de alta frecuencia debido a la capacitancia del cable, que forma un filtro de paso bajo con la impedancia de salida del micrófono. El otro es que los cables largos de alta impedancia tienden a captar más señales parásitas en forma de interferencias electromagnéticas. No se producen daños si la impedancia entre el micrófono y otros equipos no coincide; lo peor que puede ocurrir es una reducción en la señal o un cambio en la respuesta de frecuencia.

Algunos micrófonos están diseñados para que su impedancia "no" coincida con la carga a la que están conectados. En este caso, puede alterarse su respuesta de frecuencia y causar distorsión, especialmente a niveles altos de presión sonora. Ciertos micrófonos de cinta y dinámicos son las excepciones a este comportamiento, debido a que los diseñadores presuponen que cierta impedancia de carga forma parte del circuito interno de amortiguación electroacústica del micrófono.

El estándar AES42, publicado por la Audio Engineering Society, define una interfaz digital para micrófonos. Los micrófonos que cumplen con este estándar emiten directamente un flujo de audio digital a través de un conector macho XLR o XLD, en lugar de producir una salida analógica. Los micrófonos digitales se pueden usar con equipos nuevos con conexiones de entrada apropiadas que cumplan con el estándar AES42, o bien a través de un dispositivo de interfaz adecuado. Los micrófonos con calidad de estudio que operan de acuerdo con el estándar AES42 están disponibles en las gamas de varios fabricantes.

Debido a las diferencias en su construcción, los micrófonos tienen sus propias respuestas características al sonido. Estas diferencias producen distintas respuesta para fases y frecuencias no uniformes. Además, los micrófonos no son uniformemente sensibles a la presión del sonido, y pueden aceptar diferentes niveles sin distorsión. Aunque para aplicaciones científicas son deseables micrófonos con una respuesta lo más uniforme posible, este no suele ser el caso para la grabación de música, ya que la respuesta no uniforme de un micrófono puede producir una coloración deseable del sonido. Existe un estándar internacional para especificaciones de micrófonos, pero pocos fabricantes se adhieren a él. Como resultado, la comparación de los datos publicados de diferentes fabricantes es difícil, porque se usan distintas técnicas de medición. El sitio web "Microphone Data" ha recopilado las especificaciones técnicas completas con imágenes, curvas de respuesta y datos técnicos de los fabricantes de micrófonos para cada micrófono actualmente listado, e incluso algunos modelos obsoletos, y muestra los datos para todos en un formato común para facilitar la comparación.Sin embargo, se debe tener cuidado al sacar conclusiones de este u otros datos publicados, a menos que se sepa que el fabricante ha suministrado las especificaciones de acuerdo con la norma IEC 60268-4.

Un diagrama de respuesta en frecuencia muestra la sensibilidad del micrófono en decibelios en un rango de frecuencias (típicamente de 20 Hz a 20 kHz), generalmente para un sonido perfectamente alineado con el eje del micrófono (sonido que llega a 0° a la cápsula). La respuesta de frecuencia puede ser de forma orientativa como la siguiente: "30 Hz-16 kHz ± 3 dB". Esto se interpreta como un gráfico casi plano, lineal, entre las frecuencias establecidas, con variaciones en la amplitud no supriores ni inferiores en 3 dB. Sin embargo, no se puede determinar a partir de esta información lo "suaves" que son las variaciones, ni en qué partes del espectro se producen. Téngase en cuenta que valores declarados comúnmente como "20 Hz-20 kHz" no tienen sentido sin una medida de la tolerancia en decibelios. La respuesta de frecuencia de los micrófonos direccionales varía mucho con la distancia desde la fuente de sonido y con la geometría de la fuente de sonido. IEC 60268-4 específica que la respuesta de frecuencia se debe medir en condiciones de "onda progresiva plana" (muy lejos de la fuente), pero esto rara vez es práctico. Los micrófonos para "hablar de cerca" se pueden medir con diferentes fuentes de sonido y distancias, pero no existe un estándar y, por lo tanto, no hay forma de comparar datos de diferentes modelos a menos que se describa la técnica de medición.

El ruido propio, o el nivel de ruido de entrada equivalente, es el nivel de sonido que crea la misma tensión de salida del micrófono en ausencia de sonido. Esto representa el punto más bajo del rango dinámico del micrófono, y es particularmente importante si se desea grabar sonidos que son muy débiles. La medida a menudo se establece en dB(A), que es el volumen equivalente del ruido en una escala de decibelios ponderada en frecuencia de cómo escucha el oído, por ejemplo: "15 dBA SPL" (SPL significa nivel de presión sonora relativo a 20 pascales). Cuanto menor sea el número, mejor. Algunos fabricantes de micrófonos indican el nivel de ruido con el estándar ITU-R 468 ruido ponderado, que representa con mayor precisión la forma en que percibe el ruido el oído humano, pero da una cifra de 11-14 dB más alta. Un micrófono silencioso generalmente proporciona 20 dBA SPL o 32 dB SPL 468 ponderado. Los micrófonos muy silenciosos han existido durante años para aplicaciones especiales, como el Brüel & Kjaer 4179, con un nivel de ruido de 0 dB SPL. Recientemente, se han introducido algunos micrófonos con especificaciones de bajo nivel de ruido en el mercado de estudio / entretenimiento, como los modelos de Neumann y Røde que anuncian niveles de ruido de entre 5-7 dBA. Generalmente, esto se logra alterando la respuesta de frecuencia de la cápsula y la electrónica para producir menos ruido dentro de la curva A-ponderada, mientras que el ruido de banda ancha puede aumentar.

El SPL máximo que el micrófono puede aceptar se mide para valores particulares de distorsión armónica (THD), típicamente del 0.5%. Esta cantidad de distorsión es generalmente inaudible, por lo que se puede usar el micrófono de forma segura en este SPL sin dañar la grabación. Ejemplo: "142 Pico de presión sonora (a 0.5% THD)". Cuanto mayor sea el valor, mejor, aunque los micrófonos con un SPL máximo muy alto también tienen un mayor ruido propio.

El nivel de corte es un indicador importante del nivel máximo utilizable, ya que la cifra de 1% THD generalmente citada bajo SPL máximo es realmente un nivel muy leve de distorsión, bastante inaudible, especialmente en picos altos breves. El corte es mucho más audible. Para algunos micrófonos, el nivel de corte puede ser mucho mayor que el SPL máximo.

El rango dinámico de un micrófono es la diferencia en SPL entre el suelo de ruido y el SPL máximo. Si se establece por sí mismo, por ejemplo, "120 dB", transmite significativamente menos información que el ruido propio y las cifras SPL máximas individualmente.

La sensibilidad indica con qué eficacia el micrófono convierte la presión acústica en voltaje de salida. Un micrófono de alta sensibilidad crea más voltaje y, por lo tanto, necesita menos amplificación en el mezclador o en el dispositivo de grabación. Esta es una preocupación práctica pero no es directamente una indicación de la calidad de un micrófono, y de hecho el término "sensibilidad" es un nombre inapropiado, "ganancia de transducción" es quizás más significativo (o simplemente "nivel de salida") porque la sensibilidad real es generalmente establecida por el ruido base, y demasiada "sensibilidad" en términos de nivel de salida compromete el nivel de corte. Hay dos medidas comunes. El estándar internacional (preferido) se fábrica en milivoltios por pascal a 1 kHz. Un valor más alto indica una mayor sensibilidad. El método americano más antiguo se refiere a un estándar de 1 V/Pa y se mide en decibelios simples, lo que se traduce en un valor negativo. De nuevo, un valor más alto indica una mayor sensibilidad, por lo que -60 dB es más sensible que -70 dB.

Algunos micrófonos están diseñados para probar altavoces, medir niveles de ruido y cuantificar una experiencia acústica. Estos son transductores calibrados y generalmente se suministran con un certificado de calibración que establece la sensibilidad absoluta contra la frecuencia. La calidad de los micrófonos de medición a menudo se refiere a las designaciones "Clase 1", "Tipo 2", etc., que son referencias no a especificaciones de micrófono sino a sonómetros. Se adoptó un estándar más amplio para la descripción del rendimiento de los micrófonos de medición.

Los micrófonos de medición generalmente son sensores escalares de presión; exhiben una respuesta omnidireccional, limitada solo por el perfil de dispersión de sus dimensiones físicas. Las mediciones de intensidad de sonido o potencia de sonido requieren mediciones de gradiente de presión, que normalmente se realizan con matrices de al menos dos micrófonos, o con anemómetros.

Para realizar una medición científica con un micrófono, se debe conocer su sensibilidad precisa (en voltios por cada pascal). Dado que esto puede cambiar en la vida útil del dispositivo, es necesario utilizar micrófonos de medición calibrados con regularidad. Este servicio lo ofrecen algunos fabricantes de micrófonos y laboratorios de pruebas independientes certificados. Todo micrófono de medición calibrado es en última instancia trazable según un patrón primario en un instituto nacional de medición, como el NPL en el Reino Unido, el PTB en Alemania y el Instituto Nacional de Estándares y Tecnología en los Estados Unidos, que más comúnmente calibran utilizando un estándar primario de reciprocidad. Los micrófonos de medición calibrados con este método se pueden usar para calibrar otros micrófonos utilizando técnicas de comparación de calibración.

Dependiendo de la aplicación a la que se destinen, los micrófonos de medición se deben probar periódicamente (cada año o varios meses generalmente) y después de cualquier evento potencialmente dañino, como caídas (la mayoría de estos micrófonos vienen en estuches acolchados con espuma para reducir este riesgo) o la exposición a sonidos más allá del nivel aceptable.

Una matriz de micrófonos es cualquier grupo de micrófonos que funcionan en tandem. Poseen muchas aplicaciones:


Normalmente, una matriz se compone de micrófonos omnidireccionales distribuidos sobre el perímetro de un espacio. Están conectados a un ordenador que registra e interpreta los resultados en una forma coherente.

Los protectores antiviento o antiestallido acústico proporcionan un método para reducir el efecto del viento en los micrófonos. Mientras que las pantallas deflectoras brindan protección contra micro explosiones sonoras unidireccionales, los antiviento de gomaespuma protegen del viento la rejilla desde todas las direcciones. Otros sistemas envuelven por completo el micrófono y protegen también su cuerpo. Esto último es importante porque, dado el contenido de baja frecuencia extrema del ruido del viento, la vibración inducida en la carcasa del micrófono puede contribuir sustancialmente a la formación de ruido.

El material de protección utilizado —tela metálica, tela o espuma— está diseñado para tener una impedancia acústica significativa. Las relativamente bajas variaciones de presión del aire de partículas de baja velocidad que constituyen las ondas de sonido pueden atravesar la pantalla con una mínima amortiguación, pero el aire de partículas de alta velocidad es impedido en mayor medida. Aumentar el grosor del material mejora la amortiguación del viento pero también comienza a comprometer el sonido de alta frecuencia. Esto limita el tamaño práctico de pantallas de gomaespuma simples. Mientras que las espumas y las mallas de alambre pueden ser parcial o totalmente autoportantes, las telas suaves y las mallas de tejido requieren permanecer tensadas sobre un bastidor o ser sujetadas con elementos estructurales más gruesos.

Dado que todo el ruido por viento se genera en la primera superficie en que golpea el aire, cuanto mayor sea el espacio entre la periferia del antiviento o pantalla y la cápsula del micrófono, mayor será la atenuación del ruido. Para un antiviento aproximadamente esférico, la amortiguación aumenta aproximadamente el cubo de esa distancia. Por lo tanto, los antiviento más grandes son siempre mucho más eficientes que los más pequeños. Con los protectores antiviento de canastilla completa hay un efecto de cámara de presión adicional, explicado por primera vez por Joerg Wuttke, que, para micrófonos de dos puertos (gradiente de presión), permite que la combinación antiviento/micrófono actúe como un filtro acústico de paso alto.

Dado que la turbulencia sobre una superficie es la fuente del ruido del viento, reducir la turbulencia bruta puede aumentar la reducción del ruido. Las superficies lisas aerodinámicamente, y las que evitan que se generen poderosos vórtices, ambas suelen usarse con éxito. Históricamente, el pelaje o pelo artificial ha demostrado ser muy útil para este fin, ya que sus fibras producen micro-turbulencias y absorben energía en silencio. Si no están protegidos para resistir el viento y la lluvia, las fibras de pelaje o pelo son muy transparentes acústicamente, pero el respaldo de un tejido puede proporcionar una amortiguación significativa. Como material, es difícil de fabricar con consistencia y de mantener en perfectas condiciones. Es por ello que tiende a evitarse su uso (DPA 5100, Rycote Cyclone).

En el estudio y en el plató, las pantallas deflectoras y los protectores antiviento de espuma pueden ser útiles por razones de higiene y para proteger los micrófonos de la saliva y el sudor. También, con sus colores y personalización pueden ser útiles como identificadores. En su lugar, el protector de canastilla puede contener un sistema de suspensión para aislar al micrófono del ruido producido por los golpes recibidos durante su manejo.

Establecer la eficiencia de la reducción del ruido generado por el viento es una ciencia inexacta, ya que el efecto varía enormemente con la frecuencia y, por lo tanto, con el ancho de banda del micrófono y el canal de audio. A frecuencias muy bajas (10-100 Hz), donde existe una energía por viento masiva, las reducciones son importantes para evitar la sobrecarga de la cadena de audio, particularmente en las primeras fases. Esto puede producir el típico sonido “wump” asociado con el viento, que a menudo es silenciado debido a la limitación del pico de baja frecuencia. A frecuencias más altas - 200 Hz a ~ 3 kHz - la curva de sensibilidad auditiva permite escuchar el efecto del viento como una adición al sonido ambiente, a pesar de que tiene un contenido de energía mucho más bajo. Los antiviento simples pueden permitir que el ruido producido por el viento se reduzca en 10 dB; los mejores pueden lograr una atenuación de más de 50 dB. Sin embargo, también debería indicarse la transparencia acústica, particularmente a alta frecuencia, ya que un nivel muy alto de atenuación del viento podría asociarse con un sonido ensordecido o débil.

Los micrófonos se pueden dividir según varias clasificaciones:

Como se mencionó en las características hay seis tipos de micrófonos:

Se establecen tres grupos:

Los 6 tipos de micrófonos más importantes son:

Existen seis tipos de micrófonos según su uso:




</doc>
<doc id="6964" url="https://es.wikipedia.org/wiki?curid=6964" title="Microchiroptera">
Microchiroptera

Los micromurciélagos o microquirópteros (Microchiroptera) son un suborden del orden Chiroptera. El término de microquirópteros es algo inexacto, ya que algunos de ellos son más grandes que los megaquirópteros.

El tamaño varía entre 4 y 16 cm. La mayoría del alimento de los microquirópteros son los insectos. Algunos cazan especies más grandes: lagartos, ranas o incluso peces. Hay microquirópteros, como el vampiro ("Desmodus rotundus") de América del Sur, que se alimentan de sangre de grandes mamíferos.

Las distinciones entre microquirópteros y megaquirópteros son: 

Esta clasificación es la adoptada por Simmons & Geisler (1998):

También existe una clasificación alternativa:




</doc>
<doc id="6968" url="https://es.wikipedia.org/wiki?curid=6968" title="Kimono">
Kimono

El es el vestido tradicional japonés, que fue la prenda de uso común hasta los primeros años de la posguerra. El término japonés "mono" significa ‘cosa’ y "ki" proviene de "kiru", ‘vestir, llevar puesto’.

Los kimonos llegan hasta las partes bajas del cuerpo, como la canilla con cuellos escote en "tita" y amplias mangas. Hay varios tipos de kimonos usados por hombres, mujeres y niños. El corte, el color, la tela y las decoraciones varían de acuerdo al sexo, la edad, el estado marital, la época del año y la ocasión. El kimono se viste cubriendo el cuerpo en forma envolvente como tipo regalo y sujetado con una faja ancha llamada "obi".

Antiguamente, el kimono se confeccionaba con un material rústico pero a medida que Japón se fue influenciando por la cultura china y coreana, se introdujo la seda, haciendo que el kimono fuera un traje suntuoso. Actualmente, la mayoría de los japoneses utiliza ropa occidental pero acostumbran a vestirse con kimonos en ocasiones especiales como bodas, ceremonias o festivales tradicionales. Los accesorios para acompañar al kimono son los "geta" (chinelas de madera) o los "zori" (sandalias bajas hechas de algodón y cuero) y los "tabi" (calcetines tradicionales que separan el dedo pulgar del resto de los dedos para calzar la sandalia).

Los aficionados a los kimonos en Japón llegan incluso a tomar cursos para aprender a colocarse correctamente un kimono. Las clases abarcan la elección de acuerdo a la temporada, las tramas y figuras a elegir de acuerdo a cada ocasión, la combinación entre la ropa interior y los accesorios de un kimono, el entrenamiento para ubicar cada ropa interior enviando mensajes sutiles, y la selección y prueba del obi, entre otros temas. Existen también clubes devotos a la cultura del kimono, como el Kimono de Ginza.

El nombre original del kimono era , debido a que los primeros kimonos estaban fuertemente influenciados por la ropa china Han tradicional, conocida actualmente como . Desde el siglo V empezó a adoptarse extensamente la cultura china por los japoneses, a través de las embajadas japonesas en China. Durante el siglo VII la moda china, y especialmente el cuello traslapado femenino, obtuvo gran popularidad en Japón. Durante el Periodo Heian (794–1192), los kimonos se volvieron muy estilizados, aunque uno llamado "Mo" siguió usando una mitad delantal encima. Durante el Periodo Muromachi (1392-1573), un kimono de una pieza llamado "Kosode", formalmente considerado como ropa interior, comenzó a llevarse sin pantalones hakama encima, y estos se usaban sujetados por un obi. Durante el periodo Edo (1603-1867), las mangas comenzaron a crecer en longitud y a usarse por solteras, y el obi se hizo más ancho, con varios estilos para sujetarlo.
Desde entonces, la forma básica del kimono femenino y masculino ha permanecido esencialmente sin cambios. Los kimonos hechos con técnicas tradicionales y finos materiales son considerados como grandes obras de arte.

El kimono formal fue reemplazado por ropa europea y el yukata (es solo para hombres), para el uso diario.
Después de un edicto del emperador Meiji, la policía, los ferroviarios y maestros usaron ropa europea. La ropa europea se volvió el uniforme de la armada y las escuelas para varones.
Después del gran terremoto de Kantō, los portadores de Kimonos fueron comúnmente víctimas de robos. "La Asociación de fabricantes de ropa para mujeres y niños de Tokio" (東京婦人子供服組合) promovió el uso de ropa europea.
Entre 1920 y 1930 la indumentaria de marinero sustituyó al hakama de una pieza, como uniforme escolar para niñas.
Se dice que el incendio de 1932 de la tienda Nihonbashi en Shirokiya fue el catalizador para el declive de los kimonos como ropa de uso diario (aunque se sugiere que esto es un mito urbano).
El uniforme nacional "Gokumin-fuku" (que era una variación de ropa europea) fue asignado por mandato para hombres en 1940.
Actualmente las personas suelen llevar ropa de origen europeo, y el yukata en ocasiones especiales.
















Para los hombres hay kimonos con diversos estilos y características. A diferencia de los kimonos femeninos, su indumentaria es bastante simple.

Las mangas del kimono masculino están unidas al cuerpo, solo por unos centímetros independientes en la parte inferior. Las mangas masculinas son menos largas que las femeninas para acommodar el obi alrededor de la cintura bajo ellas. Los kimonos masculinos consisten tradicionalmente en cinco piezas:






La principal distinción entre los kimonos masculinos es la tela de la que están hechos.


En el pasado era común que un kimono tuviera un proceso completo aparte para ser lavado, y vuelto a coser para ser usado, porque las puntadas deben ser sacadas para el lavado. Tradicionalmente los kimonos deben ser cosidos a mano.
Este proceso tradicional de lavado se llama arai hari, y es muy costoso y difícil, por lo que es una de las causas del declive de la popularidad de los kimonos. La telas modernas y los métodos de limpieza desarrollados eliminan este proceso, aunque el lavado tradicional de kimonos aún es practicado, especialmente en los kimonos de mayor valor.

En la actualidad, se pueden encontrar kimonos hechos en seda, de algodón, lana o de materiales sintéticos. Según el tipo de tela de kimono , el cuidado de la prenda será distinto. Por ejemplo, los kimonos de algodón y los sintéticos son los más sencillos de lavar, ya que se pueden lavar a mano o en la lavadora, utilizando programas cortos. Es importante no mezclar la prenda con otras, no utilizar secadora ni tampoco seleccionar temperaturas de lavado elevadas. Si se lavan a mano, también se recomienda no frotar en exceso la prenda. En el caso del kimono de lana se puede llevar a la tintorería. En el caso de querer planchar el kimono, hay que evitar el contacto directo con la plancha, utilizando un paño o tela. 

Nuevos kimonos hechos por encargo son enviados al consumidor con largas puntadas de rociada flojas, alrededor de los bordes exteriores. Estas puntadas son llamadas "shitsuke ito", algunas veces son remplazadas por puntadas de almacenaje. Ayudan a prever su amontonamiento, plegamiento y arrugado; y mantienen las capas del kimono alineadas.

Como muchas otras prendas tradicionales japonesas, los kimonos tienen maneras específicas para ser guardados. Estos métodos ayudan a preservarlos y evitan que se arruguen cuando están almacenados. Los kimonos son comúnmente almacenados envueltos en un papel llamado "tatōshi".

Los kimonos deben ser puestos a airearse, al menos, estacionalmente, y antes y después de ser usados. Algunas personas prefieren lavar en tintorerías sus kimonos, aunque esto puede ser extremadamente costoso, es en general más barato que lavarlos por el "arai hari", además de que no es posible este último método en algunas telas o teñidos.



</doc>
<doc id="6969" url="https://es.wikipedia.org/wiki?curid=6969" title="Rhinolophidae">
Rhinolophidae

Los murciélagos de herradura o rinolófidos (Rhinolophidae) son una gran familia de quirópteros que incluye aproximadamente 130 especies actualmente. Su fórmula dentaria es formula_1 .





</doc>
<doc id="6970" url="https://es.wikipedia.org/wiki?curid=6970" title="Idioma japonés">
Idioma japonés

El es un idioma de Asia oriental hablado por alrededor de 128 millones de personas, principalmente en Japón, donde es la lengua nacional. El japonés es el principal idioma de las lenguas japónicas, y aunque se ha debatido sobre su relación lingüística con el coreano, el ainu, las lenguas altaicas y las lenguas austroasiáticas, no hay consenso en cuanto a su origen.

Poco se sabe acerca el origen del idioma. Hay textos chinos del siglo III que documentan algunas palabras japonesas, pero los primeros textos extensos no surgen hasta el siglo VIII. Durante el período Heian (794-1185), el chino ejerció una profunda influencia en el vocabulario y fonología del japonés antiguo y del medio temprano. El japonés medio tardío (1185-1600) se acerca sustancialmente al japonés contemporáneo. En este período se incorporan también los primeros préstamos europeos. Tras el fin del aislamiento en 1853, la cantidad de préstamos de lenguas europeas aumentó considerablemente, especialmente del inglés.

El japonés no tiene una relación genética con el chino, pero utiliza caracteres chinos llamados "kanji" (漢字), y gran parte del vocabulario proviene del chino. Junto a los "kanji", el japonés utiliza dos alfabetos o silabarios: el "hiragana" (ひらがな) y el "katakana" (カタカナ). El alfabeto latino (en japonés "romaji") se utiliza para escribir acrónimos y el idioma emplea tanto los números árabes como los chinos.

Aunque originario de Asia nororiental, su parentesco filogenético es incierto. Comúnmente se ha clasificado como una lengua aislada, al no haberse podido establecer parentesco con otros idiomas, es decir, un idioma sin relación con ningún otro. Sin embargo, según algunos lingüistas, el japonés estándar moderno no es una lengua aislada sino que es parte de la familia japónica junto a varias lenguas de las islas Ryūkyū (también consideradas dialectos japoneses) todas ellas derivadas del protojapónico. Aunque aparte de sus descendientes modernos no ha podido demostrarse un parentesco filogenético inequívoco entre el protojapónico (o sus descendientes modernos) y ninguna otra lengua de Asia. Sin embargo, aunque no se ha podido establecer firmemente ningún parentesco claro, no faltan las hipótesis que señalan algunas coincidencias con el coreano y con las lenguas altaicas o las lenguas austronesias. Además cabe señalar que el japonés no está relacionado ni filogenéticamente ni tipológicamente con el chino, si bien parte importante del vocabulario del japonés moderno son préstamos y cultismos tomados del chino clásico. Tampoco parece existir ninguna relación con la lengua ainu (con la que comparte rasgos tipológicos, pero no elementos que sugieran origen filogenético común).

Es un hecho comprobado que existen correlaciones sistemáticas entre los fonemas de las lenguas primitivas coreanas y del japonés antiguo. No obstante, aún no está claro si esas correlaciones se deben a un origen común o a préstamos léxicos masivos a lo largo de los siglos, producto del intercambio cultural. Una teoría alternativa adscribe este idioma a la macrofamilia de las lenguas austronesias. Según esta hipótesis, la lengua japonesa conforma el extremo norte de un grupo del que forman parte las lenguas aborígenes de Taiwán, el tagalo y otros idiomas de Filipinas y el malayo-indonesio en todas sus variantes. En general la investigación contemporánea oscila entre ambas hipótesis: reconoce una fuerte influencia continental, posiblemente ligado al coreano, y, al mismo tiempo, considera la posibilidad de la existencia de un sustrato austronésico. Dicho sea de paso, algunos investigadores consideran al coreano como una lengua altaica (si bien la existencia de la categoría de altaico es en sí motivo de controversia). 

Desde el 2500 a. C. los pueblos mongólicos llegados del continente comenzaron a poblar las islas del archipiélago japonés, donde se inició el desarrollo de una lengua arcaica (Yamato kotoba - 倭言葉) de estructura polisilábica, así como una cultura propia. No sería hasta el siglo III d. C. cuando intelectuales coreanos introducen la cultura china en las islas niponas. Esta invasión cultural duró aproximadamente cuatro siglos, durante los cuales se introdujeron ciencias, artes, y religión, así como el sistema de escritura chino. 

Los japoneses comenzaron a usar los caracteres chinos (kanji - "漢字" significa caracteres Han) conservando el sonido original chino, (si bien adaptándolo a su propio sistema fonético) y añadiendo además la pronunciación nativa a esos símbolos. Por ello, hoy en día, al estudiar el sistema de kanji es necesario aprender ambas lecturas, la lectura china (onyomi "音読み") y la lectura japonesa (kunyomi "訓読み"), si bien dichos adjetivos no deben prestarse a errores: Ambas pronunciaciones son propias del japonés, y son diferentes a las del chino moderno, aun así el sonido del onyomi es la aproximación japonesa al sonido chino del entonces y dependía también de la variante hablada que estaba en el poder. Además de los kanji, en el japonés existen dos silabarios para representar todos sus sonidos, creados a partir de la simplificación de ciertos kanji. Los silabarios se denominan hiragana y katakana y son un sistema de escritura único del japonés, ausente en el chino y el coreano. El japonés moderno utiliza los tres sistemas de escritura, kanji, hiragana y katakana, circunscribiendo el uso de cada uno para diferentes funciones, si bien hay ocasiones en que dos de ellos pueden usarse indistintamente.

Debido a la particular historia de Japón, el idioma japonés incluye elementos ausentes en las lenguas indoeuropeas, siendo uno de los más conocidos, un rico sistema de "honoríficos" (keigo "敬語") que resultan en formas verbales y construcciones gramaticales específicas para indicar la jerarquía relativa entre el que habla y el que escucha, así como el nivel de respeto hacia el interlocutor.

El japonés está difundido en su mayor parte, como es lógico, en Japón, donde es hablado por la totalidad de la población. Hay comunidades de inmigrantes japoneses en Hawái que también utilizan el idioma (más de 250 000, el 30% de la población), en California (EE.UU.) unas 300 000; en Brasil unas 400 000 y un número importante en Argentina y en la costa de Perú, así como otras partes del mundo. En las antiguas colonias japonesas como Corea, Manchuria (China), Guam, Taiwán, Filipinas, Islas Marshall y Palaos es conocido también por las personas de edad avanzada que recibieron instrucción escolar en este idioma. No obstante, la mayor parte de estas personas prefiere no utilizarlo. 

Se hablan docenas de dialectos en Japón. La profusión se debe a muchos factores, incluido el período de tiempo que el archipiélago japonés ha estado habitado, el terreno montañoso de la isla y la larga historia de aislamiento externo e interno de Japón. Los dialectos típicamente difieren en términos de acento tonal, morfología inflexional, vocabulario y uso de partículas. Algunos incluso difieren en los inventarios de vocales y consonantes, aunque esto es poco común.

La principal distinción en los acentos japoneses es entre los tipo Tokio (東京 式, Tōkyō-shiki) y los tipo Kioto-Osaka (京阪 式, Keihan-shiki). Dentro de cada tipo hay varias subdivisiones. Los dialectos de tipo Kyoto-Osaka se encuentran en la región central, aproximadamente formados por las regiones de Kansai, Shikoku y Hokuriku occidental.

Los dialectos de regiones periféricas, como Tōhoku o Kagoshima, pueden ser ininteligibles para los hablantes de otras partes del país. Hay algunas islas lingüísticas en pueblos de montaña o islas aisladas, como la isla Hachijō-jima, cuyos dialectos descienden del dialecto oriental del japonés antiguo. Los dialectos de la región de Kansai son hablados o conocidos por muchos japoneses, y el dialecto de Osaka en particular está asociado con la comedia. Los dialectos de Tōhoku y el norte de Kantō están asociados con agricultores típicos regionales.

Respecto a los idiomas ryukyuenses, que se hablan en Okinawa y las Islas Amami (políticamente parte de Kagoshima), el gobierno japonés mantiene la política de que todas las lenguas ryukyuenses son meramente "dialectos del japonés", a pesar de que son lo suficientemente distintas como para ser consideradas una rama separada de la familia japonesa; no solo cada idioma es ininteligible para los hablantes japoneses, sino que la mayoría son ininteligibles para quienes hablan otros idiomas ryukyuenses. Sin embargo, en contraste con los lingüistas, muchos japoneses comunes tienden a considerar los idiomas ryukyuenses como dialectos del japonés. La corte imperial también parece haber hablado una variante inusual de japonés de la época. Lo más probable es que se trate de la forma hablada del idioma japonés clásico, un estilo de escritura que prevaleció durante el período Heian, pero comenzó a declinar a fines de la era Meiji. Los idiomas ryukyuenses son hablados por un número cada vez menor de personas mayores, por lo que la UNESCO los clasificó como en peligro de extinción, ya que podrían extinguirse en 2050. Los jóvenes utilizan principalmente el japonés y no pueden entender los idiomas ryukyuenses. El japonés de Okinawa (no confundir con el idioma okinawense) es una variante del japonés estándar influenciado por los idiomas ryukyuenses. Es el dialecto principal que se habla entre los jóvenes en las islas Ryūkyū.

El japonés moderno se ha generalizado en todo el país (incluidas las islas Ryūkyū) debido a la educación, los medios de comunicación y el aumento de la movilidad dentro de Japón, así como a la integración económica.

Las variantes del idioma han sido confirmadas desde el antiguo Japón, a través del Man'yōshū. Los escritos japoneses más antiguos conocidos que incluyen dialectos orientales cuyas características no fueron heredadas por los dialectos modernos a excepción de algunas partes como la isla de Hachijo.Con la modernización del japonés en la parte tardía del siglo XIX, el gobierno ha promovido el uso del japonés estándar lo cual ha hecho que este dialecto no solo sea altamente conocido solo en Japón sino también en todo el mundo. El idioma ha ido cambiando al fusionarse diversos dialectos y recibir influencia de otras lenguas.

La historia de la lengua japonesa se suele dividir en cuatro periodos diferentes.

Algunas de las similitudes léxicas entre las lenguas austronesias y el japonés podrían deberse a la influencia adstrato de algunas lenguas, aunque la evidencia en favor de dicha influencia prehistórica no es concluyente. A partir del siglo VII sí es notoria la influencia de la cultura china en Japón y la adopción en esta lengua de numerosos préstamos léxicos procedentes del idioma chino para designar conceptos técnicos y culturales asociados a la influencia china. El chino clásico es al japonés, algo similar a lo que las raíces de origen griego son a las lenguas europeas: una fuente de elementos léxicos para formar neologismos. El propio sistema de escritura japonesa es en sí mismo una muestra de la influencia cultural china en el japonés.

A partir del siglo XVI el japonés adoptará algunos términos procedentes del portugués, del español, del neerlandés y de otras lenguas de colonización europea. Y a partir del siglo XX la influencia del inglés como fuente de nuevos préstamos al japonés es hegemónica.

Tiene cinco vocales y dieciséis consonantes y es muy restrictivo en la formación de sílabas. El acento es musical, con dos tonos diferentes: alto y bajo.

El sistema fonológico japonés consta de cinco vocales, que escritas en caracteres latinos son: "a, i, u, e, o", según el orden tradicional. Se pronuncian igual que en castellano /a, e, i, o/ salvo la "u" /ɯ/, que se pronuncia con los labios extendidos, esto es, se trata de una vocal no redondeada. 
Las vocales pueden ser normales o largas, en cuyo caso poseen una duración doble de la normal y se consideran como sílabas separadas. 

Los fonemas consonánticos son dieciséis o quince, dependiendo de si se considera o no que el "sokuon" corresponde a una consonante geminada o al archifonema /Q/, representado en la escritura por el símbolo "sokuon" que adopta el mismo sonido de la consonante que le sigue o en ocasiones se pronuncia como una oclusión glotal. La cuenta de sonidos es mucho más alta si se cuentan los alófonos de aparición consistente, representados entre corchetes en la siguiente tabla. Debe considerarse además que los préstamos tomados de otras lenguas a partir del siglo XX, particularmente del inglés, pueden conservar fonemas ajenos al inventario tradicional.


El japonés es una lengua de estructura aglutinante que combina diversos elementos lingüísticos en palabras simples. Cada uno de estos elementos tiene una significación fija y apta para existir separadamente. El japonés es casi exclusivamente sufijante, con muy pocos prefijos, como por ejemplo, los honoríficos o-(お), go- (ご), por lo que los únicos procesos para la formación de palabras son la composición y la derivación mediante sufijos. La gramática del idioma japonés es muy diferente de la del español. Algunas de sus características son:

 La escritura japonesa está basada en dos sistemas de ortografía:


 y una Categoría que lista sus entradas en japonés 





</doc>
<doc id="6971" url="https://es.wikipedia.org/wiki?curid=6971" title="Fado">
Fado

El fado es la expresión más conocida internacionalmente de la música portuguesa. En el fado se expresan las experiencias de la vida a través del canto. Generalmente lo canta una sola persona, acompañado por la «viola» (guitarra española) y la guitarra portuguesa. Los temas más cantados en el fado son la melancolía, la nostalgia o pequeñas historias del vivir diario de los barrios humildes, pero especialmente el fatalismo y la frustración.

Documentalmente se comprueba la existencia del fado desde 1838, aunque hay quien identifica su origen con los cantos de las gentes del mar, inspirados en la soledad, la nostalgia y los balanceos de los barcos sobre el agua. A pesar de los numerosos investigadores —Gonçalo Sampaio, Mascarenhas Barreto, Pinto de Carvalho o Rodney Gallop— el misterio de sus orígenes todavía no se ha develado. Se cree que nació en los barrios alrededor del puerto de Lisboa, entre clases pobres, marineros, obreros, rufianes, chamiceras, gente bohemia de Alfama, Bairro Alto y otros. 

Sus orígenes, no comprobados, conectan el fado con las "Cantigas d'Amigo" quizás con influencias africanas y más lejanas, traídas de las colonias. 

Una de las mejores definiciones de fado la ofrece Amália Rodrigues (1920-1999), considerada la embajadora artística de Portugal: «el fado es una cosa muy misteriosa, hay que sentirlo y hay que nacer con el lado angustioso de las gentes, sentirse como alguien que no tiene ni ambiciones, ni deseos. Una persona... como si no existiera. Esa persona soy yo y por eso he nacido para cantar el fado». Amália puso emoción y voz de fado a grandes poetas portugueses, como O`Neill, Manuel Alegre, Homem de Melo y Camoens. En una de sus canciones más célebres «Todo esto es fado» canta:
También el escritor portugués Fernando Pessoa escribió: «El fado no es alegre ni triste [...] Formó el alma portuguesa cuando no existía y deseaba todo sin tener fuerza para desearlo [...] El fado es la fatiga del alma fuerte, el mirar de desprecio de Portugal al Dios en que creyó y que también lo abandonó». 

Su origen es sin duda popular y tiene algunos paralelismos con otros estilos relevantes de la misma época, como el tango, el rebetiko y el flamenco. Aunque protegido por las instituciones oficiales durante la dictadura salazarista, los amantes de este cante lo siguieron preservando durante la segunda mitad del siglo pasado. Hoy la popularidad del fado es cada vez mayor, principalmente entre las nuevas generaciones de cantantes portugueses.

La guitarra portuguesa que acompaña siempre el fado tiene doce cuerdas y su origen se remonta a la Edad Media y a un instrumento llamado «cítula». Fue introducido en Portugal en la segunda mitad del siglo XVIII, a través de la colonia inglesa en Oporto. A finales del siglo empezó a ser utilizada en los salones de la burguesía. Su timbre es especial e inconfundible y está vinculado con el fado lisboeta desde 1870. Las guitarras de Lisboa y Coímbra varían ligeramente en su tamaño, afinación y construcción. La estructura del fado se divide en secuencias en las que unas veces la guitarra suena sola y en otras acompaña solamente a la voz del fadista. 

Las casas de fado son restaurantes de los barrios antiguos de Lisboa (Bairro Alto, Alfama, Lapa o Alcântara), que suelen abrir solo por las noches. Después de cenar y tomar un buen vino, se baja la intensidad de la luz, se hace silencio absoluto y uno se deja llevar por el ambiente íntimo y por las voces dulces de los fadistas. Aunque hay fados alegres, que son los más demandados, los melancólicos tienen más admiradores portugueses. Las composiciones de Alfredo Marceneiro y Severa son un clásico. 

En noviembre del 2011, la Unesco inscribió a "El fado, canto popular urbano de Portugal" como integrante de la Lista Representativa del Patrimonio Cultural Inmaterial de la Humanidad.


El fado tradicional de Coímbra está conectado a las tradiciones académicas de la Universidad de Coímbra. Es exclusivamente cantado por hombres. Tanto los cantantes como los músicos visten de negro "de capa y batina". Los temas hacen referencia a amores estudiantiles o a la ciudad. El estilo hace especial hincapié en el componente instrumental. El más conocido de los fados de Coímbra es "Coimbra é uma canção" ("Coímbra es una canción"), que tuvo un notable éxito en toda Europa.



Cantado típicamente en las "casas de fado", tanto por hombres como por mujeres. Las mejores casas de fado se encuentran en los barrios de Alfama, Mouraria, Bairro Alto y Madragoa. Tiene como característica fundamental el cantar con tristeza y con sentimientos de dolor pasados y presentes, pero también puede contar una historia divertida con ironía. Las ornamentaciones vocales y el dramatismo son semejantes a los de otros estilos mediterráneos. Su intérprete más conocida fue Amália Rodrigues.






</doc>
<doc id="6972" url="https://es.wikipedia.org/wiki?curid=6972" title="Tauromaquia en Portugal">
Tauromaquia en Portugal

La tauromaquia en Portugal comparte aspectos generales con la cultura del toro de lidia presentes en la vecina España, como los encierros populares ("largadas") y el de diversas variantes de corridas ("touradas"), Celebradas también en el sur de Francia y en otros países de América Latina. En Portugal concurren aspectos particulares como la preferencia por toreo a caballo o rejoneo y los "forcados", una suerte típica de las corridas de toros lusas. En las corridas de toros de Portugal al toro tras la lidia no se le da muerte en el ruedo sino en los corrales de la plaza desde el 1 de abril de abril de 1928, a excepción de la localidad de Barrancos, en el Alentejo. A pesar de estar despenalizada por el parlamento portugués la muerte del toro en el ruedo desde el año 2000, se mantiene el toro sin muerte en el ruedo. El centro taurino de referencia en Portugal es Ribatejo y la temporada va de abril a octubre. El parlamento de Portugal rechazó en 2018 la propuesta de prohibir las corridas de toros por considerarlas una parte importante de su cultura y tradiciones.

Tras la decisión del ayuntamiento de Póvoa de Varzim de prohibir los festejos taurinos a principios del año 2019, en septiembre de 2019 el Tribunal Administrativo y Fiscal de Oporto declaró inconstitucional dicha decisión, por lo que en Portugal no están prohibidos los espectáculos taurinos.


</doc>
<doc id="6973" url="https://es.wikipedia.org/wiki?curid=6973" title="Cohete espacial">
Cohete espacial

Un cohete espacial es una máquina que, utilizando un motor de combustión, produce la energía cinética necesaria para la expansión de los gases, que son lanzados a través de un tubo propulsor (propulsión a reacción). Por extensión, el vehículo, generalmente espacial, que presenta motor de propulsión de este tipo es denominado cohete o misil. Normalmente, su propósito es enviar artefactos (especialmente satélites artificiales y sondas espaciales) o naves espaciales y hombres al espacio (véase atmósfera). 

Un cohete está formado por una estructura, un motor de propulsión a reacción y una carga útil. La estructura sirve para proteger los tanques de combustible y oxidante y la carga útil. Se llama también cohete al motor de propulsión en sí mismo.

El origen del cohete es probablemente oriental. La primera noticia que se tiene de su uso es del año 1232, en China, donde fue inventada la pólvora. 

Existen algunos relatos del uso de cohetes llamados "flechas de fuego voladoras" en el siglo XIII, en defensa de la capital de la provincia china de Henan. 

Los cohetes fueron introducidos en Europa por los árabes. Durante los siglos XV y XVI fue utilizado como arma incendiaria. Posteriormente, con el perfeccionamiento de la artillería, el cohete bélico desapareció hasta el siglo XIX, y fue utilizado nuevamente durante las Guerras Napoleónicas. Los cohetes del coronel inglés William Congreve fueron usados en España durante el sitio de Cádiz (1810), en la primera Guerra Carlista (1833-1840) y durante la guerra de Marruecos (1860).

A finales del siglo XIX y principios del siglo XX, aparecieron los primeros científicos que convirtieron al cohete en un sistema para impulsar vehículos aeroespaciales tripulados. Entre ellos destacan, el peruano Pedro Paulet, el ruso Konstantín Tsiolkovski, el alemán Hermann Oberth y el estadounidense Robert Hutchings Goddard, y, más tarde los rusos Serguéi Koroliov y Valentin Gruchensko, y el alemán Wernher von Braun. 

Robert Hutchings Goddard fue el responsable del primer vuelo de un cohete propulsado con combustible líquido (gasolina y oxígeno), lanzado el 16 de marzo de 1926, en Auburn, Massachusetts, Estados Unidos. Los cohetes construidos por Goddard, aunque pequeños, ya tenían todos los principios de los modernos cohetes, como orientación por giroscopios, por ejemplo. 

Los alemanes, liderados por Wernher von Braun, desarrollaron durante la Segunda Guerra Mundial los cohetes V-1 y V-2 ("A-4" en la terminología alemana), que fueron la base para las investigaciones sobre cohetes de los EE.UU. y de la URSS en la posguerra. Ambas bombas nazis, usadas para bombardear Londres a finales de la guerra, pueden ser definidas como misiles. Realmente, el V-1 no llega a ser un cohete, sino un misil que vuela como un avión de propulsión a chorro.
Inicialmente se desarrollaron cohetes específicamente destinados para uso militar, normalmente conocidos como misiles balísticos. Los programas espaciales que los estadounidenses y los rusos pusieron en marcha se basaron en cohetes proyectados con finalidades propias para la astronáutica, derivados de estos cohetes de uso militar. Particularmente los cohetes usados en el programa espacial soviético eran derivados del R-7, misil balístico, que acabó siendo usado para lanzar las misiones Sputnik. 

Destacan, por el lado estadounidense, el Astrobee, el Vanguard, el Redstone, el Atlas, el Agena, el Thor-Agena, el Atlas-Centauro, la serie Delta, los Titanes y Saturno (entre los cuales el Saturno V - el mayor cohete de todos los tiempos, que hizo posible el programa Apollo), y, por el lado soviético, los cohetes designados por las letras A, B, C, D y G (estos dos últimos tuvieron un papel semejante a los Saturno estadounidenses), denominados Protón.
Otros países que han construido cohetes, en el marco de un programa espacial propio, son Francia, Gran Bretaña (que lo abandonó), Japón, China, México, Argentina, Brasil y la India, así como el consorcio europeo que constituyó la Agencia Espacial Europea (ESA), que ha construido y explotado el cohete lanzador Ariane.

El principio de funcionamiento del motor de cohete se basa en la tercera ley de Newton, la "ley de la acción y reacción", que dice que "a toda acción le corresponde una reacción, con la misma intensidad, misma dirección y sentido contrario". 

Imaginemos una cámara cerrada donde exista un gas en combustión. La quema del gas producirá presión en todas las direcciones. La cámara no se moverá en ninguna dirección pues las fuerzas en las paredes opuestas de la cámara se anularán. 

Si practicáramos una abertura en la cámara, donde los gases puedan escapar, habrá un desequilibrio. La presión ejercida en las paredes laterales opuestas continuará sin producir fuerza, pues la presión de un lado anulará a la del otro. Ya la presión ejercida en la parte superior de la cámara producirá empuje, pues no hay presión en el lado de abajo (donde está la abertura).
Así, el cohete se desplazará hacia arriba por "reacción" a la presión ejercida por los gases en combustión en la cámara de combustión del motor. Por esto, este tipo de motor es llamado de "propulsión a reacción". 

Como en el espacio exterior no hay oxígeno para quemar el combustible, el cohete debe llevar almacenado en tanques no sólo el "combustible" (carburante), sino también el "oxidante" (comburente). 

La magnitud del "empuje" producido (expresión que designa la fuerza producida por el motor de cohete) depende de la masa y de la velocidad de los gases expelidos por la abertura. Luego, cuanto mayor sea la temperatura de los gases expelidos, mayor será el empuje. Así, surge el problema de proteger la cámara de combustión y la abertura de las altas temperaturas producidas por la combustión. Una manera ingeniosa de hacer esto es cubrir las paredes del motor con un fino chorro del propio propelente usado por el cohete para formar un aislante térmico y refrigerar el motor.

En cuanto al tipo de combustible usado, existen dos tipos de cohete: 


En cuanto al número de fases, un cohete puede ser: 


La importancia de los cohetes como vehículos radica en dos características:

La primera de estas características es la que ha promovido su uso histórico en el campo militar y en los espectáculos pirotécnicos, la segunda no ha sido significativa hasta la aparición de la astronáutica en la década de 1950.

El cohete constituye un medio capaz de transportar una carga útil a grandes velocidades de un punto a otro. Como arma, un cohete puede transportar un explosivo (convencional o nuclear) a grandes distancias en un tiempo corto, a veces tomando al enemigo por sorpresa. El cohete presenta otras ventajas con respecto a los proyectiles: tiene un radio de acción más grande y su trayectoria puede ser controlada.

Existen cohetes militares (también nombrados misiles) de muy variado tamaño, potencia y radio de acción. Los pequeños pueden ser lanzados directamente por los soldados o desde vehículos en movimiento, y suelen ser utilizados para atacar las aeronaves del enemigo. La capacidad de controlar su vuelo también les permite ser usados para atacar objetivos fijos con bastante precisión.

Los misiles de gran tamaño pueden llegar a tener un radio de acción de miles de kilómetros, y se utilizan para bombardear las instalaciones introducidas en territorio enemigo sin necesidad de enviar tropas o aviones. Su gran velocidad también dificulta la intercepción. De especial atención son los misiles balísticos intercontinentales (ICBM en terminología inglesa). Estos cohetes tienen un radio de acción de miles de kilómetros y siguen una trayectoria balística que los lleva, efectivamente, fuera de la atmósfera terrestre. Armados con explosivos nucleares constituyen un medio de disuasión importante, ya que permiten atacar el corazón de la nación enemiga por muy lejos que esté, sin que ésta disponga de ninguno medio para impedir su llegada.

Fuera del campo militar, el uso más importante de los cohetes es el de lanzar objetos al espacio exterior, normalmente poniéndolos en órbita en torno a la Tierra. Para este objetivo, el cohete es el único medio disponible. Por una parte, son los únicos vehículos capaces de alcanzar la velocidad necesaria para esta aplicación, y de la otra sólo el cohete es capaz de propulsarse en el vacío del espacio. Los otros vehículos necesitan un medio material sobre el que desplazarse, o bien obtienen algún elemento esencial para su funcionamiento del medio.

Sin embargo, el cohete no deja de ser un medio ineficaz de lanzar objetos al espacio. Debido a su propia naturaleza el cohete tendrá que ser siempre mucho mayor que el objeto que tiene que transportar, y eso quiere decir que en un lanzamiento la mayor parte de la energía será utilizada para acelerar el propio cohete, y no su carga útil. Por ejemplo, un cohete Ariane 5 cargado de combustible pesa en torno a 750 toneladas, de las cuales sólo 20 pueden ser efectivamente puestas en órbita. Sin embargo, no existen alternativas en el cohete ni a corto ni a largo plazo para esta aplicación.

Otro uso ligeramente diferente de los cohetes se encuentra en los estudios de microgravedad. Un cohete puede poner un objeto en una trayectoria balística fuera de la atmósfera, donde no será sometido a la fuerza de rozamiento del aire y estará, pues, en una situación de caída libre, equivalente a la ausencia de gravedad para muchos fenómenos físicos.

En razón del creciente desarrollo y la alta tecnología que involucra, no puede dejarse de lado la cohetería vocacional, conocida también cohetería amateur.

El cohete convencional deberá pasar por algunos avances en los próximos años, aunque aún será el mayor responsable, por mucho tiempo, del envío de astronautas y satélites artificiales al espacio. 

La adopción de vehículos reutilizables, como el transbordador espacial, de la NASA, debe ampliarse. Los transbordadores espaciales despegan como un cohete convencional, pero aterrizan como aviones, gracias a su aerodinámica especial. 

Un motor revolucionario, que puede hacer avanzar la tecnología astronáutica, es el motor Scramjet, capaz de alcanzar velocidades hipersónicas de hasta 15 veces la velocidad del sonido. El motor Scramjet no posee partes móviles, y obtiene la compresión necesaria para la combustión por el aire que entra de frente, impulsado por la propia velocidad del vehículo en el aire. La NASA probó con éxito un motor de este tipo en 2004. El cohete, llamado X-43A, fue llevado a una altitud de 12 000 m por un avión B-52, y lanzado por un cohete Pegasus a una altitud de 33 000 m. Alcanzó la velocidad récord de 11 000 km/h. 

Otra posibilidad de adelanto en la tecnología de motores de cohetes es el uso de propulsión nuclear, en que un reactor nuclear calienta un gas, produciendo un chorro que se usa para producir empuje. También se ha considerado la idea de construir un cohete en forma de vela, impulsado por la presión de radiación solar, lo que permitiría viajes interplanetarios de larga distancia





</doc>
<doc id="6974" url="https://es.wikipedia.org/wiki?curid=6974" title="Polca">
Polca

La polca (o "polka") es una danza popular aparecida en Bohemia (actual República Checa) hacia 1830, que se comenzó a popularizar en Praga desde 1835. También se usa este término para referirse al género musical asociado a la danza.

En compás de 2/4 y tiempo rápido, se baila con pasos laterales del tipo “paso”, “cierra”, paso, “salto” y evoluciones rápidas, motivo por el que se hizo muy popular en Europa y América.

En Chile, Estados Unidos, México, Puerto Rico, Colombia, Panamá, Perú, Argentina, Uruguay y Paraguay ha devenido, desde su llegada a mediados del siglo XIX, con estilos particulares, en varios casos en respectivos subestilos folclóricos nacionales.

En Paraguay existe la polka paraguaya o purahéi, cuyo nombre deriva de la “polca” europea, pero cuyos ritmos, melodía, armonía y contrapunto no tienen relación con la polka europea; pues la polka paraguaya combina ritmos ternarios, binarios y síncopas. En este estilo sudamericano los instrumentos más populares son la guitarra y el arpa paraguaya.

La polca llega a Uruguay desde Europa alrededor de 1845, primero como danza escénica, pero rápidamente se extiende a los salones de baile de Montevideo, junto con otros ritmos novedosos para bailes en pareja. Hacia fines del siglo, se ha extendido al medio rural, donde se transforma y se generan diversas variantes. 

Entre otras; se hacen populares la polca canaria y la polca de acordeón. Pasa a ser un importante componente de la música de baile en el campo uruguayo como la chamarrita y el gato y la milonga. 

Según el musicólogo Lauro Ayestarán, a partir de 1920 “la polca fue desalojada lentamente del repertorio de los bailes criollos”.

En Argentina existen 3 variedades de polca: la polca (binaria) que se toca en el centro de país, la polca correntina y la polquita rural (del Litoral argentino). La primera posee un compás binario. La polca en la Provincia de Corrientes se vuelve más lenta que la polca original o adquiere un compás ternario, a su vez, esta primero se tocaría con arpa pero ya a principios del siglo XX esta fue sustituida casi completamente por el acordeón (el diatónico y/o el de teclado). Por su parte, la polquita rural, folklórica en especial en la Provincia de Misiones, adquiere un carácter más campestre y sin tantos arreglos y retoques musicales. También, cabe decir que la polca correntina es similar al chamamé, aunque este último es un poco más lento y cadencioso.

En Nicaragua existe el género musical de música folclórica llamada polka neosegoviana -junto a la mazurca, el jamaquello, el palo de mayo ("May Pole") y el Son nica. Fue introducida por inmigrantes de Europa Central (Alemania) que se asentaron principalmente en la zona central norte (Matagalpa y Jinotega) de este país centroamericano. Entre sus recopiladores e intérpretes más destacados se reconoce a Soñadores de Sarawaska (Jinotega), Don Felipe Urrutia y sus Cachorros (Estelí) y el investigador musical Cedrick Dallatorre Zamora (Jinotega).

La polca Checoslovaca, la mazurca y redova Polacas, el chotis escocés, las cuadrillas Inglesas, y el vals Austriaco fueron llevados al Norte de México en calidad de Bailes de Salón a mediados del sigloXIX, principalmente en el norte y noreste de México donde la polca es parte de la música tradicional y fue adoptada por los habitantes de dicha región, siendo parte de su folclor. A finales del sigloXIX había una gran cantidad de composiciones locales, inspiradas en estos ritmos; al extenderse la llama revolucionaria por el norte del país, tanto la polca como el corrido se convirtieron en efectivos periódicos musicales, la mayoría de las polcas y corridos revolucionarios tomaron nombres de soldaderas famosas: Adelita, Marieta, Juana Gallo, Rielera, Revolcada, Jesusita, etc. El baile sin dejar de ser de salón (Por parejas). En la actualidad son muchas las polcas, entre las que se numeran: “El Aguacero”, “El Rancho”, “Carreta”, los pasos son muy movidos, se ejecutan en forma de galope de tiempo en tiempo. El galope se interrumpe para cambiar de paso y de evoluciones.

Los compositores bohemios, Bedřich Smetana (1824-1884) y Antonín Dvořák (1841-1904), compusieron polcas, introduciendo esta danza en la música académica.

También los músicos austríacos de la familia Strauss compusieron muchas polcas. Entre las más conocidas están la Polka Pizzicato o Tritsch-Tratsch. Un ejemplo de polka contemporánea es la “Circus Polka”.



</doc>
<doc id="6975" url="https://es.wikipedia.org/wiki?curid=6975" title="Asteroidea">
Asteroidea

Los asteroideos (Asteroidea) o estrellas de mar, son una clase del filo Echinodermata (equinodermos) de simetría pentarradial, cuerpo aplanado formado por un disco pentagonal con cinco brazos o más. El nombre «estrella de mar» se refiere esencialmente a los miembros de la clase Asteroidea. Sin embargo, en su uso común el nombre es a veces incorrectamente aplicado a los ofiuroideos. La clase Asteroidea se compone de cerca de 1900 especies existentes que se distribuyen en todos los océanos del mundo, incluyendo el Atlántico, Pacífico, Índico, Ártico y Antártico. Estrellas de mar se producen en un amplio rango de profundidad, desde la zona intermareal hasta la abisal a profundidades superiores a 6000 m.

Las estrellas de mar forman uno de los grupos de animales marinos más conocidos del fondo marino. Por lo general tienen un disco central y cinco brazos, aunque algunas especies pueden tener muchos brazos más. La superficie aboral o superior puede ser lisa, granular o espinosa, y está cubierta con placas superpuestas. Muchas especies son de colores brillantes en varios tonos de rojo o naranja, mientras que otros son de color azul, gris, o marrón. Tienen pies ambulacrales operados por un sistema hidráulico y una boca en el centro de la superficie oral o inferior. Se alimentan de forma oportunista, depredando sobre todo a invertebrados bentónicos. Varias especies tienen un comportamiento de alimentación especial, incluyendo alimentación por suspensión y adaptaciones para alimentarse de presas específicas. Tienen ciclos de vida complejos y pueden reproducirse tanto sexual como asexualmente. La mayoría tiene la capacidad de regenerar brazos dañados o perdidos.
Tienen varias funciones importantes en la ecología y la biología. Especies como "Pisaster ochraceus" llegaron a ser ampliamente conocidas como ejemplos del concepto de las especies claves en la ecología. La especie tropical "Acanthaster planci" es un depredador voraz de coral a lo largo de la región del Indo-Pacífico. Otra especies de estrellas de mar, como los miembros de la familia Asterinidae, se utilizan con frecuencia en la biología del desarrollo.

Asteroidea es una clase dentro del filo Echinodermata, que se compone de un gran número de especies. Al igual que otras clases de ese grupo, sus miembros se caracterizan por tener simetría radial como adultos, generalmente simetría pentaradial. Sin embargo, durante sus primeras fases de desarrollo, las larvas tienen simetría bilateral. Otras características de los adultos incluyen un sistema vascular acuífero, y esqueletos calcáreos que consisten en placas planas conectadas por una malla de colágeno mutable. Los Asteroideos se caracterizan por un disco central con un número de brazos radiantes, típicamente cinco. Los osículos que forman el elemento duro de la estructura esquelética se extienden desde el disco sobre los brazos en una disposición continua que forman una amplia base para los brazos. En cambio, en los ofiuroideos el disco se distingue claramente de los brazos largos y delgados.

Los Asteroidea están escasamente representados en el registro fósil, parcialmente porque las partes duras del esqueleto se separan cuando el animal se descompone o porque los tejidos blandos se descomponen en restos distorsionados e irreconocibles. Otra razón puede ser que la mayoría de los Asteroidea vive en sustratos duros donde las condiciones para la fosilización no son favorables. Los primeros asteroides conocidos se remontan al Ordovícico. En los dos principales eventos de extinción masiva durante el Devónico Tardío y el Pérmico Tardío, muchas especies se extinguieron, pero otros lograron sobrevivir. Estos se diversificaron rápidamente en un plazo de 60 millones de años durante el Jurásico Temprano y la primera parte del Jurásico Medio.

La clase de los asteroideos se compone de los siguientes órdenes: 






Las estrellas de mar suelen tener una apariencia radialmente simétrica y por lo general tienen una simetría pentarradial en la edad adulta. Sin embargo, se cree que los ancestros evolutivos de los equinodermos tenían una simetría bilateral. En la actualidad, las estrellas de mar, así como otros equinodermos, solo exhiben simetría bilateral en sus formas larvales.

La mayoría de las estrellas de mar tiene cinco brazos que irradian desde un disco central. Sin embargo, varios grupos de asteroideos, tales como la familia Solasteridae, tienen 10 a 15 brazos, mientras que algunas especies, tales como "Labidiaster annulatus" de la Antártida, pueden tener hasta 50 brazos. No es inusual que las especies que típicamente solo tienen cinco brazos, tengan seis o más brazos debido a anomalías en su desarrollo.

La superficie de las estrellas de mar tiene componentes que se parecen a placas de carbonato de calcio conocidos como osículos. Estos forman el endoesqueleto, que puede tener varias formas, externamente expresadas como una variedad de estructuras tales como espinas y gránulos. Estos pueden ser dispuestos en patrones o series características, y su arquitectura, formas individuales y ubicaciones se utilizan para clasificar los diferentes grupos dentro de la clase Asteroidea. La terminología que se refiere a la ubicación de los componentes corporales de las estrellas de mar se basa generalmente en referencia a la boca, para evitar suposiciones incorrectas de homología con las superficies dorsales y ventrales de animales bilaterales. La superficie inferior suele denominarse como la superficie oral o actinal, mientras que la superficie superior se conoce como el lado aboral o abactinal.
La superficie corporal de estrellas de mar tiene varias estructuras que comprenden la anatomía básica del animal, y a veces puede ayudar en su identificación. La placa madreporita puede fácilmente ser identificada por el círculo claro que se encuentra ligeramente fuera del centro del disco central. Esta placa porosa está conectada, a través de un canal calcificado, al sistema vascular hidráulica en el disco. Su función es, al menos en parte, de proporcionar agua adicional para las necesidades del animal, incluyendo el agua de relleno para el sistema vascular acuífero. El ano se encuentra también un poco fuera del centro del disco, cerca de la placa madreporita. En la superficie oral, un surco ambulacral corre por cada brazo, a cada lado del cual se extiende una doble fila de osículos no fusionados. Los pies ambulacrales se extienden en estos a través de muescas y son internamente conectados al sistema vascular acuífero.

Varios grupos de asteroideos, incluyendo los órdenes Valvatida y Forcipulatida, poseen pequeñas estructuras conocidas como pedicelarios, que tienen alguna semejanza con válvulas. Estos ocurren ampliamente sobre la superficie del cuerpo. En los asteroideos del orden Forcipulatida, tales como los del género "Asterias" y "Pisaster", los pedicelarios se producen en mechones parecidas a pompones en la base de cada espina, mientras que en especies de la familia Goniasteridae, tales como "Hippasteria phrygiana", los pedicelarios se distribuyen de forma más dispersa sobre la superficie del cuerpo. Aunque no se conoce todas las funciones de estas estructuras, se piensa que algunas ayudan en la defensa del animal, mientras que otras ayudan en la alimentación o en la eliminación de organismos que intentan establecerse en la superficie de la estrella de mar. La especie "Labidiaster annulatus" de Antártica tiene grandes pedicelarios que utiliza para capturar presas activas de krill. "Stylasterias forreri" del Pacífico Norte ha sido observado capturando pequeños peces con su pedicelarios.

También existen otros tipos de estructuras cuya ocurrencia varía por taxón. Por ejemplo, los miembros de la familia Porcellanasteridae poseen órganos adicionales, que parecen a tamices, situados entre las series de placas laterales, se cree que generan corrientes en las madrigueras hechas por estas estrellas de mar.

Como equinodermos, las estrellas de mar poseen un sistema vascular acuífero hidráulico que ayuda en la locomoción. Este sistema incluye muchas proyecciones, conocidas como pies ambulacrales, en los brazos de la estrella de mar; estas sirven para la locomoción y la alimentación. Los pies ambulacrales emergen a través de aberturas en el endoesqueleto y se expresan externamente mediante surcos abiertos que se encuentran a lo largo de la superficie oral de cada brazo.

La cavidad del cuerpo también contiene un sistema circulatorio, conocido como el sistema hemal. Canales hemales forman anillos alrededor de la boca (el anillo hemal oral), más cerca de la superficie aboral, y alrededor del sistema digestivo (el anillo hemal gástrico). Una porción de la cavidad del cuerpo, el seno axial, conecta los tres anillos. Cada brazo también tiene canales hemales corriendo al lado de las gónadas. Estos canales tienen extremos ciegos sin circulación continua de la sangre.

En el extremo de cada brazo se encuentra un pequeño ojo simple (u ocelo), que permite percibir la diferencia entre luz y oscuridad, lo que sirve para la detección de objetos en movimiento. Solo una parte de cada célula del ocelo es pigmentado (por lo tanto un color rojo o negro), sin córnea o iris.

La pared corporal se compone de una epidermis exterior delgada, una dermis espesa formada de tejido conectivo y un peritoneo interior delgado que contiene músculos circulares y longitudinales. La dermis contiene osículos (placas óseas) libremente organizados. Algunos tienen gránulos externos, tubérculos y espinas, a veces organizados en patrones definidos, y algunos son especializados como pedicelarios. También puede incluir pápulas, protuberancias con paredes finas que pasan a través de la pared coporal, se extienden hacia el agua circundante, y que tienen una función respiratoria. Estas estructuras son soportadas por fibras colágenas orientadas perpendicularmente entre sí y dispuestos en una red tridimensional con los osículos y las pápulas en los intersticios. Esta disposición permite tanto la flexión de los brazos de la estrella de mar, como el rápido inicio de la rigidez necesaria para las acciones realizadas bajo presión.
La boca de una estrella de mar se encuentra en el centro de la superficie oral y se abre, a través de un esófago corto, en un estómago cardíaco en primera instancia, y luego en un estómago pilórico en segunda instancia. Cada brazo también contiene dos ciegos pilóricos, que forman largos tubos de ramificación del estómago pilórico hacia el exterior. Cada ciego pilórico está recubierto por una serie de glándulas digestivas, que secretan enzimas digestivas y absorben los nutrientes de los alimentos. Un intestino corto se extiende desde la superficie superior del estómago pilórico hasta la apertura del ano cerca del centro de la parte superior del cuerpo.

Muchas estrellas de mar, como las del género "Astropecten" y "Luidia", tragan su presa entera y empiezan a digerirlas en el estómago antes de pasarla al ciego pilórico. Sin embargo, un gran número de especies tienen la capacidad de evertir el estómago cardíaco hacia afuera para engullir y digerir los alimentos. En estas especies, el estómago cardíaco recupera la presa, y luego la pasa al estómago pilórico que siempre permanece interna. Los residuos se excretan a través del ano en la superficie aboral del cuerpo.

Esta capacidad de digerir alimentos fuera de su cuerpo, permite a la estrella de mar de cazar presas mucho más grandes que el tamaño de su boca. Se alimenta de almejas, ostras, artrópodos, pequeños peces y moluscos gastrópodos. Algunas estrellas de mar no son totalmente carnívoro, y pueden complementar su dieta con algas o detritus orgánico. Algunas de estas especies pastan, pero otros atrapan partículas de alimentos del agua en pegajosas hebras mucosas que pueden ser arrastradas hacia la boca a lo largo de ranuras ciliosas.

Aunque las estrellas de mar carecen de un cerebro centralizado, sus cuerpos tienen sistemas nerviosos complejos bajo la coordinación de lo que podría denominarse un cerebro distribuido. Tienen una red de nervios entrelazados, un plexo nervioso, que se encuentra dentro así como por debajo de la piel. El esófago también está rodeado por un anillo nervioso central, que envía los nervios radiales en cada uno de los brazos, a menudo en paralelo con las ramas del sistema vascular acuífero. Todos ellos se conectan para formar un cerebro. Los nervios del anillo nervioso central y los nervios radiales son responsables de la coordinación del equilibrio de la estrella de mar y de sus sistemas de dirección.

Aunque las estrellas de mar no tienen muchas entradas sensoriales bien definidas, son sensibles al tacto, la luz, la temperatura, la orientación y el estado de las aguas circundantes. Los pies ambulacrales, las espinas y los pedicelarios de las estrellas de mar son sensibles al tacto, mientras que los ocelos en los extremos de los brazos son sensibles a la luz. Los pies ambulacrales, especialmente aquellas que se encuentran en las puntas de los brazos, también son sensibles a sustancias químicas, y esta sensibilidad se utiliza en la localización de las fuentes de olor, tales como alimentos.

Los ocelos constan de una masa compuesta de células epiteliales pigmentadas que responden a la luz, y células sensoriales estrechas situadas entre ellas. Cada ocelo está cubierto por una gruesa cutícula transparente que da protección y que actúa como una lente. Muchas estrellas de mar también poseen células fotorreceptoras individuales distribuidas sobre sus cuerpos, y son capaces de responder a la luz, incluso cuando sus ocelos están cubiertos.

Las estrellas de mar se mueven utilizando un sistema vascular acuífero. El agua ambiental entra en el sistema a través de la placa madreporita. Luego circula desde el conducto pétreo hacia el canal anular y los canales radiales. Los canales radiales llevan el agua a la ampolla (depósito) en los pies ambulacrales. Cada pie está formado por una ampolla interna y un podio externo, o «pie». Cuando la ampolla se comprime, fuerza el agua en el podio, que se expande hasta hacer contacto con el substrato. En algunas circunstancias, los pies ambulacrales parecen funcionar como palancas, pero cuando se mueve sobre superficies verticales, forman un sistema de tracción. Aunque el podio se asemeja a una ventosa, la acción de agarre se lleva a cabo con la secreción de químicos adhesivos en lugar de aspiración. Otros químicos y la contracción podial permiten que se libera del sustrato.

Los pies ambulacrales se adhieren a la superficie y se mueven en una onda, con una sección del cuerpo conectándose a la superficie, como otros soltándola. La mayoría de las estrellas de mar no puede moverse con mucha velocidad; por ejemplo, "Dermasterias imbricata" sólo puede desplazarse sobre 15 cm en un minuto. Algunas especies de los géneros excavadores "Astropecten" y "Luidia" tienen puntos en lugar de ventosas en sus largos pies ambulacrales y son capaces de desplazarse con una velocidad mucho más alta sobre el fondo marino. "Luidia foliolata" por ejemplo puede desplazarse a una velocidad de 2,8 m por minuto.

La respiración ocurre principalmente a través de los pies ambulacrales y a través de las pápulas que salpican la superficie del cuerpo. El oxígeno del agua se distribuye por el cuerpo principalmente por el fluido de la cavidad corporal principal; el sistema hemal también puede tener un papel menor.

Como no tiene órganos excretores distintos, la excreción de desechos nitrogenados se realiza a través de los pies ambulacrales y las pápulas. El fluido corporal contiene células fagocíticas, celomocitos, que también se encuentran dentro del sistema hemal y sistema vascular acuífero. Estas células engullen materiales residuales, que eventualmente se trasladan hacia las puntas de las pápulas donde son expulsadas en el agua circundante. Una parte de los desechos puede ser excretada por las glándulas pilóricas con los heces.

Estrellas de mar no parecen tener un mecanismo para la osmorregulación, y mantienen sus fluidos corporales en la misma concentración de sal que el agua circundante. Aunque algunas especies pueden tolerar una salinidad relativamente baja, la falta de un sistema de osmorregulación probablemente explica porqué las estrellas de mar no se encuentran en agua dulce, ni incluso en ambientes estuarinos.

Se extrajeron varias toxinas y metabolitos secundarios de un número de especies de estrellas de mar. La investigación sobre la eficacia de estos compuestos para su posible uso farmacológico o industrial se lleva a cabo en muchos países.
Las estrellas de mar tienen la capacidad de reproducirse sexual y asexualmente.

La mayoría de las especies de estrellas de mar son dioicas, es decir que existen machos y hembras. Por lo general no es posible distinguirlos externamente, ya que no se puede ver las gónadas, pero su sexo es aparente durante el desove. Algunas especies son hermafroditas simultáneas (producen óvulos y espermatozoides al mismo tiempo). En algunos de ellos, la misma gónada, llamada ovotestis, produce tanto huevos como esperma. Otras estrellas de mar son hermafroditas secuenciales, de las cuales algunas son protándricas. Es decir, los juveniles son inicialmente machos pero se convierten en hembras a medida que envejecen, como ocurre en la especie "Asterina gibbosa" por ejemplo. Otros son protóginos y se convierten de hembras en machos al envejecer. En algunas especies, cuando una hembra grande se reproduce por división, los individuos menores que produce se convierten en machos. Cuando crezcan lo suficientemente, cambian de nuevo en hembras.

Cada brazo tiene dos gónadas que liberan gametos a través de aberturas, llamadas gonoductos, ubicadas en el disco central entre los brazos. En la mayoría de las especies la fecundación es externa, aunque algunas especies conocen fecundación interna. En la mayoría de las especies, se emite los huevos y el esperma en el agua (desove libre) y los embriones y larvas que resultan de la fecundación externa forman parte del plancton. En otras especies, los huevos se desarrollan pegados a la parte inferior de rocas. En ciertas especies, las hembras incuban sus huevos, cubriéndolos con su cuerpo, o sosteniéndolos en estructuras especiales. Estas estructuras incluyen cámaras en la superficie aboral, el estómago pilórico ("Leptasterias tenera") o incluso las propias gónadas. Las estrellas de mar que incuban sus huevos cubriéndolos con su cuerpo, suelen elevar su disco central, asumiendo una postura encorvada. También existe una especie que incuba una parte de sus crías y emite los huevos restantes que no caben en la bolsa. En estas especies incubadoras, los huevos son relativamente grandes y provistos de yema, y por lo general, aunque no siempre, se desarrollan directamente en una pequeña estrella de mar, sin pasar por una etapa larval. Las crías en desarrollo se denominan «lecitotróficas», ya que obtienen su nutrición de la yema del huevo, a diferencia de las larvas que se alimentan de manera planctotrófica. En una especie incubadora intragonadal, las crías obtienen su nutrición alimentándose de otros huevos y embriones en la bolsa de incubación gonadal. La incubación es particularmente común en las especies polares y de aguas profundas, que viven en ambientes menos favorables para el desarrollo larvario, y en las especies más pequeñas que producen pocos huevos.

La reproducción tiene lugar en diferentes momentos del año según la especie. Para aumentar las posibilidades de que sus óvulos sean fecundados, estrellas de mar pueden sincronizar el desove, reunidos en grupos, o formando parejas. Este último comportamiento se denomina pseudo-cópula y el macho se sube encima de la hembra, colocando sus brazos entre los suyos, y emite esperma al agua. Esto estimula la emisión de los huevos. Las estrellas de mar pueden utilizar señales del medio ambiente para coordinar el momento del desove (la duración del día para indicar la hora correcta del año, el amanecer o el atardecer para indicar la hora correcta del día), y señales químicas para indicar su disposición a los demás. En algunas especies, hembras adultas producen sustancias químicas para atraer a los espermatozoides en el agua de mar.

Algunas especies de estrellas de mar también se reproducen asexualmente como adultos, ya sea por fisión de sus discos centrales o por la autotomía de sus brazos. El tipo de reproducción depende del género. Entre las estrellas de mar que regeneran un cuerpo completo a partir de brazos cortados, algunos pueden hacerlo, incluso a partir de fragmentos de tan sólo 1 cm de largo. La división de la estrella de mar, o bien a través de sus discos o en sus brazos, suele ser acompañada de cambios que facilitan la partición.

Las larvas de varias especies de estrellas de mar también tienen la capacidad de reproducirse asexualmente. Pueden hacerlo mediante la autotomía de algunas partes de sus cuerpos o por gemación. Cuando existe una abundancia de alimentos, las larvas favorecen la reproducción asexual en lugar de desarrollarse directamente. Aunque esto le cuesta tiempo y energía, permitirá que una sola larva se reproduzca en varios adultos si las condiciones son buenas. Varias otras razones desencadenan fenómenos similares en las larvas de otros equinodermos. Estos incluyen el uso de tejidos que se pierde durante la metamorfosis, o la presencia de depredadores que cazan las larvas más grandes.

Al igual que los demás equinodermos, estrellas de mar conocen un desarrollo (embrionario) deuterostómico, una característica que comparten con los cordados (incluyendo los vertebrados), pero no con la mayoría de otros invertebrados. Sus embriones desarrollan inicialmente una simetría bilateral, lo que parece reflejar su probable ascendencia común con los cordados. Sin embargo, el desarrollo posterior toma un camino muy diferente, cuando la larva se asienta fuera del zooplancton y desarrolla la característica simetría radial. A medida que el organismo crece, un lado de su cuerpo crece más que el otro, y eventualmente absorbe el lado más pequeño. Después de eso, el cuerpo se forma en cinco partes alrededor de un eje central hasta que la estrella de mar tenga su simetría radial.

Las larvas de estrellas de mar son organismos ciliados, que nadan libremente. Los huevos fecundados se convierten en organismos bipinarios y más tarde (en la mayoría de los casos) en larvas brachiolarias que, o bien crecen alimentándose de la yema del huevo, o atrapando y comiendo otro plancton. En ambos casos, viven como plancton, suspendidas en el agua y nadan batiendo los cilios. Las larvas son bilateralmente simétricas y tienen un lado izquierdo y derecho distinto. Con el tiempo, se instalan en el fondo del mar, experimentan una metamorfosis completa, y se convierten en adultos.

La esperanza de vida de estrellas de mar varía considerablemente entre las especies, y es por lo general más largo en las especies mayores. Por ejemplo, "Leptasterias hexactis", cuyo peso como adulto es 20 g, alcanza la madurez sexual en dos años y tiene una esperanza de vida de unos 10 años, mientras que "Pisaster ochraceus", cuyo peso adulto es 80 g, alcanza la madurez en cinco años y puede vivir hasta 34 años.

Algunas especies de estrellas de mar tienen la capacidad de regenerar brazos perdidos y pueden crecer nuevos miembros. Algunas especies también tienen la capacidad de volver a crecer un nuevo disco central a partir de un solo brazo, mientras que otras necesitan que por lo menos una porción del disco central esté conectado a la parte separada. La regeneración puede durar varios meses o años. Las estrellas de mar son vulnerables a infecciones durante las primeras etapas tras la pérdida de un brazo. Una extremidad separada vive de nutrientes almacenados hasta que vuelve a crecer un disco central y una boca y es capaz de alimentarse de nuevo. Aparte de la fragmentación que se lleva a cabo para fines de reproducción, la división del cuerpo puede ocurrir accidentalmente tras el desprendimiento por un depredador, o el brazo puede ser activamente expulsado durante una respuesta de escape, un proceso conocido como autotomía. La pérdida de partes del cuerpo se logra mediante el ablandamiento rápido de un tipo de tejido conectivo especial en respuesta a señales nerviosas. La mayoría de los equinodermos cuenta con este tipo de tejido.

La mayoría de las especies de estrellas de mar son depredadores generalistas, que se alimentan de moluscos, como almejas, ostras, caracoles, o cualquier otro animal demasiado lento para poder evadir su ataque (por ejemplo, otros equinodermos o peces casi muertos). Algunas especies son detritívoros, y se alimentan de animales y materia vegetal en estado de descomposición, o de láminas orgánicas adheridas a sustratos. Otros, como los miembros del orden Brisingida, se alimentan de esponjas o de plancton y partículas orgánicas en suspensión. "Acanthaster planci" consume pólipos de corales, y es parte de la cadena alimenticia en los arrecifes de coral. De vez en cuando, ocurren brotes explosivos de estas estrellas que pueden causar graves daños a los ecosistemas de los arrecifes de coral.

Algunas estrellas de mar cuentan con órganos especiales que facilitan la captura de presas y la alimentación; "Pisaster brevispinus", de la costa del Pacífico de los Estados Unidos, puede utilizar un conjunto de pies ambulacrales especializados para cavar profundamente en los sustratos blandos y extraer sus presas (generalmente almejas). Agarrando el marisco, la estrella de mar abre lentamente la cáscara de la presa, agotando su músculo aductor, y luego inserta su estómago evertido hacia una apertura para devorar los tejidos blandos. Para que el estómago evertido pueda ganar entrada, la brecha entre las válvulas sólo necesita ser una fracción de un milímetro de ancho.

En la actualidad se conoce más de 1900 especies de estrellas de mar existentes. Equinodermos mantienen un delicado equilibrio electrólito interno en sus cuerpos, y esto sólo es posible en un ambiente marino. Esto significa que las estrellas de mar se producen en todos los océanos de la Tierra, pero no se encuentran en ningún hábitat de agua dulce. La mayor variedad de especies se encuentra en la zona tropical del Indo-Pacífico. Otras regiones conocidas por su gran diversidad incluyen las zonas tropicales y templadas de Australia, el Pacífico oriental tropical y el agua frío-templado del Pacífico Norte (desde California hasta Alaska). Todas las estrellas de mar viven en el fondo del mar, pero sus larvas son planctonicas, lo que las permite dispersarse a nuevas ubicaciones. Los hábitats varían desde arrecifes de coral tropicales, rocas, barro, grava, y arena, hasta bosques de algas marinas, praderas marinas y el fondo oscuro de aguas profundas.

Estrellas de mar y otros equinodermos bombean agua directamente en sus cuerpos a través del sistema vascular acuífero. Esto les hace vulnerables a todas las formas de contaminación del agua, ya que tienen poca capacidad para filtrar las toxinas y los contaminantes que contiene. Derrames de petróleo y eventos similares suelen afectar las poblaciones de equinodermos y tienen consecuencias de largo alcance para el ecosistema.

Además son utilizadas como recuerdos o souvenirs y se extraen para ser vendidas. Son utilizadas en acuarofilia.



</doc>
<doc id="6986" url="https://es.wikipedia.org/wiki?curid=6986" title="10 de mayo">
10 de mayo

El 10 de mayo es el 130.º (centésimo trigésimo) día del año del calendario gregoriano y el 131.º en los años bisiestos. Quedan 235 días para finalizar el año.









</doc>
<doc id="6989" url="https://es.wikipedia.org/wiki?curid=6989" title="Red por microondas">
Red por microondas

Una red por microondas es un tipo de red inalámbrica que utiliza microondas como medio de transmisión.

El protocolo más frecuente es el IEEE 802.11b y transmite a 2,4 GHz, alcanzando velocidades de 11 Mbps (Megabits por segundo). Otras redes utilizan el rango de 5,4 a 5,7 GHz para el protocolo IEEE 802.11a.

Las etapas de comunicación son:





</doc>
<doc id="6991" url="https://es.wikipedia.org/wiki?curid=6991" title="Árbol">
Árbol

Un árbol es una planta, de tallo leñoso, que se ramifica a cierta altura del suelo. El término hace referencia habitualmente a aquellas plantas cuya altura supera un determinado límite en la madurez, diferente según las fuentes: dos metros, tres metros, cinco metros o los seis metros. Además, producen ramas secundarias nuevas cada año, que parten de un único fuste o tronco, con clara dominancia apical, dando lugar a una nueva copa separada del suelo. Algunos autores establecen un mínimo de 10 cm de diámetro en el tronco (la longitud de la circunferencia sería de unos 30 cm). Las plantas leñosas que no reúnen estas características por tener varios troncos o por ser de pequeño tamaño son consideradas arbustos. 

Los árboles presentan una mayor longitud que otros tipos de plantas. Ciertas especies de vegetales (como las secuoyas) pueden superar los 100 m de altura, y llegar a vivir durante miles de años. Los árboles han existido desde hace 370 millones de años. Se estima que hay poco más de 3 billones de árboles maduros en el mundo.

Un estudio realizado por la Universidad de Yale y luego publicado en la revista Nature, estima que en la Tierra hay alrededor de 3 billones de árboles, y su cantidad se redujo un 46% desde que comenzó la civilización humana, dando en promedio 422 árboles por persona, pero, cada año se pierden 15.000 millones de ejemplares.
Los árboles son un importante componente del paisaje natural debido a que previenen la erosión y proporcionan un ecosistema protegido de las inclemencias del tiempo en su follaje y por debajo de él. También desempeñan un papel importante a la hora de producir oxígeno y reducir el dióxido de carbono en la atmósfera, así como moderar las temperaturas en el suelo. También, son elementos en el paisajismo y la agricultura, tanto por su atractivo aspecto como por su producción de frutos en huertos de frutales como el manzano. La madera de los árboles es un material de construcción, así como una fuente de energía primaria en muchos países en vías de desarrollo. Los árboles desempeñan también un importante papel en muchas mitologías del mundo.

Los árboles están formados por tres partes: la raíz, el tronco y la copa. Los dos primeros son los que diferencian, fundamentalmente, a un árbol de un arbusto. Los arbustos son más pequeños y no tienen un único tallo sino que están formados por varios. No obstante, ha de señalarse que algunas especies se pueden desarrollar como árboles pequeños o como arbustos, dependiendo de las circunstancias ambientales.

Las raíces fijan el árbol al suelo. Las raíces pueden tener una raíz principal, o bien, ser numerosas raíces en las que ninguna de ellas predomina, adoptando la forma de raíz ramificada fasciculada. Muchas raíces se combinan simbióticamente con micelios de hongos. Los hongos pueden conectar diferentes árboles y formar una red que transmite nutrientes y señales. Las raíces aéreas son más raras dentro de los árboles, pero se dan en algunas especies que viven en entornos pantanosos, por ejemplo el mangle "(Rhizophora)."

El tronco sostiene la copa. Su capa exterior se llama corteza o súber, de espesor y color variables, que sirve para proteger la savia. Sus características (color, forma en que se desescama, etc.) son una ayuda a la hora de diferenciar las especies arbóreas. A modo de ejemplo, puede señalarse que el haya común la tiene gris y lisa hasta edades muy avanzadas; el pino piñonero la tiene de color pardo gris o pardo rojizo, es escuamiforme, forma surcos oscuros y grandes planchas; y el olmo común, por ejemplo tiene color pardo gris, cuarteado por grietas, tanto horizontales como transversales.

Si se corta un tronco de manera longitudinal, por ejemplo en un tocón, pueden verse los anillos, que delatan la forma en que ha ido desarrollándose ese árbol. Cada año se forma un anillo. Contándolos puede saberse la edad del árbol, si bien esto es más fácil en los árboles de zonas templadas, ya que en los trópicos con un clima regular a lo largo del año, no se aprecia la formación de anillos anuales. Los anillos estrechos evidencian años de dificultades y pobre alimentación de manera que el crecimiento es retardado. Los años de crecimiento más rápido se ven en anillos más anchos. Hay un centro del tronco más oscuro, el duramen o corazón, son células leñosas muertas de donde procede la mejor madera para usar como combustible, y luego unos anillos más claros hacia el exterior, la albura. Entre la albura y la corteza hay una sola capa de células por la que el tronco está creciendo, llamada "cambium;" se divide a su vez en dos partes: la interior formará el xilema (albura y duramen) y la exterior forma la corteza interna (floema).

Las ramas suelen brotar a cierta altura del suelo, de manera que dejan una franja de tronco libre. Las ramas y hojas forman la copa. La copa adopta formas diversas, según las especies, distinguiéndose básicamente tres tipos: la alargada y vertical, la redondeada o la que se extiende de manera horizontal, como si fuera una sombrilla. Las ramas salen del tronco, se subdividen en ramas menores y en estas están las yemas y las hojas. De la yema nacerá una flor, una rama, u hojas. Las yemas que quedan en el extremo de las ramitas se llaman yemas terminales. Suelen estar cubiertas por escamas o catafilos como forma de protección.

A través de las hojas el árbol realiza la fotosíntesis y puede por lo tanto debe alimentarse. Las raíces absorben el agua con minerales disueltos en ella. Suben por el tronco hasta las hojas. Allí reaccionan con el carbono procedente del anhídrido carbónico y forman azúcares. Luego el azúcar se transforma en celulosa, que es la materia prima de la madera. La hoja tiene una parte superior (haz) y otra inferior (envés), en el que se encuentran los estomas, pequeñas aberturas por las que penetra el anhídrido carbónico y por los que sale el agua sobrante y el oxígeno.

Las hojas son un elemento primordial a la hora de diferenciar entre las distintas especies arbóreas. Pueden señalarse cuatro tipos básicos de hojas: 

Pueden tener una (aovada, acorazonada, sagitadas, reniformes, lanceoladas, etc.) o bien ser recortada, lobulada, con entrantes más o menos marcados. El borde de la hoja (borde foliar) también es un elemento de distinción, pues puede ser entero (liso), crenado, dentado (con pequeños picos), aserrado y doble aserrado (como dientes de sierra), sinuado y lobulado; además, el borde puede ser espinoso (con espinas en el borde, como en el borde dentado punzante).


Algunos árboles, las coníferas, son gimnospermas y se caracterizan por portar estructuras reproductivas llamadas conos, pero la mayoría de las especies son 
angiospermas (actualmente "Magnopliophyta") y tienen algún tipo de flor. El gingko es un caso particular, ya que aunque es gimnosperma, no es una conífera. Algunas son flores aisladas, como se ve en las magnolias, pero otras están juntas formando ramilletes llamados inflorescencias. No todos los árboles tienen flores completas, con órganos reproductores masculinos y femeninos, sino que algunos tienen flores femeninas y flores masculinas (abedul, nogal, roble); es más, en algunas especies, hay ejemplares que solo tienen flores masculinas y las femeninas están en otros ejemplares distintos (Dioico), como por ejemplo en el gingko.

El tamaño de los árboles va desde los 3 metros de altura hasta los más de cien que pueden alcanzar las secuoyas, la especie que se considera de mayor tamaño. Las alturas de los árboles más altos del mundo han sido objeto de controversia y exageración. Modernas medidas verificadas hechas con aparatos láser, otros métodos de medida, o con medidas de cinta corrida realizada por investigadores o miembros de grupos como la U.S. Eastern Native Tree Society, han demostrado que los antiguos métodos de medición a menudo no son fiables, a veces producen exageraciones de 5 % a 15 % o más por encima de la verdadera altura. Pretensiones históricas de árboles que crecieron hasta más de 130 metros o incluso 150 ahora se consideran en gran medida poco fidedignas, y atribuidas al error humano. Mediciones históricas de árboles caídos realizadas con el tronco postrado en el suelo se consideran algo más fidedignas. Actualmente se acepta que las especies más altas son:

En cuanto a la edad, los árboles son los seres vivos que pueden vivir mayor cantidad de años. Los árboles más longevos son las secuoyas, que pueden llegar a vivir 2000-3000 años. Le siguen algunas especies pináceas propias de la alta montaña y el drago canario. Se ha calculado que el drago de Icod de los Vinos, aunque se le llama "milenario", tiene una edad 500 y los 600 años. Los árboles más antiguos se determinan por la dendrocronología o crecimiento de los anillos, que puede verse si el árbol es cortado, o en catas tomadas desde la corteza hacia el centro del tronco. La determinación exacta solo es posible para árboles que producen anillos de crecimiento, generalmente en climas con estaciones diferenciadas. Los árboles en climas tropicales, que no diferencia entre estaciones no tienen anillos distintivos. También es solo posible en árboles que son sólidos por el centro. Muchos árboles viejos se van vaciando por dentro cuando están muertos al decaer la madera muerta. Para alguna de estas especies, la edad estimada se ha hecho sobre la base de extrapolar los ritmos de crecimiento actuales, pero los resultados son normalmente en gran medida fruto de la especulación. White (1998) propone un método de estimar la edad de árboles grandes y antiguos en el Reino Unido, a través de la correlación entre el diámetro de la rama del árbol, carácter de crecimiento y edad.

Los dos árboles más antiguos son:


El grosor de un árbol es normalmente más fácil de medir que la altura, pues se trata solo de medir con cinta alrededor del tronco, tensarlo y así hallar la circunferencia. El árbol con el tronco más grueso del mundo es un baobab africano: 15,9 m, Glencoe Baobab (medido cerca del suelo), provincia de Limpopo, Sudáfrica. El célebre árbol del Tule en Oaxaca, México que es una especie de ahuehuete "(Taxodium mucronatum)": 11,62, Árbol del Tule, Santa María del Tule, Oaxaca, México.

Hay árboles por todo el mundo, siendo particularmente ricas en diversidad de especies arbóreas las franjas tropicales. Los árboles tropicales se hallan en las selvas tropicales y ecuatoriales de América Central, América del Sur, África y Asia. Pero también hay árboles en las zonas templadas y llega hasta latitudes muy altas. En este último caso, los bosques suelen presentar menos diversidad de especies y estar formados por una o pocas especies.

Los árboles son parte predominante del ecosistema de los continentes debido a que previenen la erosión, constituyendo los elementos primordiales del paisaje, la agricultura, los llamados ecosistemas forestales, los bosques y las selvas, además de encontrarse dispersos en ambientes como las sabanas o las orillas fluviales. Los árboles tienen gran importancia ecológica, puesto que fijan el suelo, impidiendo que la delgada capa fértil quede barrida por las lluvias o los vientos. Proporciona refugio y alimento a numerosas especies animales.
El grado de humedad y la naturaleza del terreno suelen determinar qué tipo de bosque se dará, y no solo la temperatura o la latitud. Cuanto mayor sea la humedad, más espeso será el bosque. La aridez determina que los árboles se encuentren en ejemplares aislados o bosquecillos en torno a una fuente de agua, como un pozo o un río. Dependiendo de la altura se darán unas especies u otras. Normalmente en las partes bajas habrá bosques de frondosas como robles, hayas y castaños, y más arriba aparecerán las coníferas. Cuanta mayor sea la altura, más empezará a ralear el terreno, hasta que llegue un momento en que desaparezcan los árboles y solo queden hierbas perennes y líquenes. Esa línea máxima que pueden alcanzar los árboles es la llamada línea de árboles. Dependiendo de la exposición al sol, los vientos o la pluviosidad, puede darse la circunstancia de que en una ladera crezcan los árboles hasta una altura y en la otra, más expuesta, la línea de árboles esté a menor altura.

Varios biotopos se definen en gran medida por los árboles que los habitan, como por ejemplo el bosque templado de caducifolios. Un paisaje de árboles disperso por un amplio espacio es la sabana. Un bosque de gran edad se llama bosque primario.

Hay diversos tipos de clasificaciones dentro de las especies arbóreas. Por el tipo de hoja, se puede distinguir entre árboles caducifolios o planifolios, que pierden su follaje durante una parte del año, normalmente la estación fría en los climas templados, y la árida en los climas cálidos y áridos, y árboles perennifolios, que no es que no pierdan las hojas, sino que no las pierden todas a la vez ni tampoco con ritmo anual, sino más largo.

La principal distinción es la que se establece entre árboles de crecimiento monopódico y árboles de crecimiento simpódico. En los monopódicos el crecimiento en longitud se basa en un tallo principal vertical del que salen, con ángulos marcados, ramas laterales subordinadas, de menor grosor. El crecimiento monopódico da lugar a un porte piramidal, como el que es característico de las coníferas. En el crecimiento simpódico, las ramas derivadas se desarrollan cerca del ápice (extremo) de aquellas en que se asientan, sustituyéndolas en el crecimiento. Las copas de estos árboles suelen ser más esféricas o cilíndricas y menos piramidales.

En inglés, pero habitualmente no en castellano, se trata de árboles a las palmeras ("palm trees"). El biotipo palmeroide se presenta en varios grupos de plantas, destacando las cícadas (Cycadophyta) y, especialmente, las angiospermas de la familia arecáceas (Arecaceae).

Un árbol es una forma de planta que aparece en muchos órdenes y familias de plantas diferentes. Los árboles muestran una variedad de formas de crecimiento, formas de hojas, características de la corteza y órganos reproductivos.

La forma de árbol ha evolucionado separadamente en clases de plantas sin parentesco, en respuesta a unos desafíos medioambientales similares, haciendo de él un ejemplo clásico de evolución en paralelo. Con unas 100 000 especies arbóreas aproximadas, el número de especies en todo el mundo puede suponer el 25 % de todas las especies de plantas vivas. La mayoría de las especies arbóreas crecen en regiones tropicales del mundo y muchas de estas áreas no han sido aún investigadas por los botánicos, haciendo de la diversidad de especies y áreas de distribución se entienden de manera fragmentaria.
Actualmente (abril de 2007) la datación de los primeros árboles conocidos es del rango de los 380 millones de años antes del presente, en pleno período devónico cuando los animales vertebrados apenas comenzaban a colonizar las tierras emergidas. Esos árboles, del género "Wattieza", que poblaban zonas actualmente correspondientes a Sur y Norteamérica, probablemente enriquecieron la atmósfera con oxígeno producido mediante la fotosíntesis favoreciendo de este modo el desarrollo de especies superiores de animales fuera de los mares. Los árboles más antiguos eran helechos arborescentes, equisetáceas y licofitas, que crecieron en bosques en el período carbonífero; aún sobreviven helechos arborescentes, pero las únicas equisetáceas y licofitas que quedan no tienen forma de árbol. Más tarde, en el período Triásico, aparecieron las coníferas, los ginkgos, las cícadas y otras gimnospermas, y posteriormente las plantas con flor en el período Cretácico. La mayor parte de las especies actuales son plantas con flor (angiospermas) y coníferas.

Plantas con el biotipo de árbol se encuentran en todas las clases de la superdivisión Spermatophyta (las antes llamadas fanerógamas), salvo en las cícadas (Cycadophyta), que son de biotipo Palmeroide.

Se llama dendrología al estudio de los árboles en aquello que les es propio como tales, y silvicultura al estudio científico y la práctica de su cuidado o cultivo, del que se ocupan los ingenieros forestales.

El humano explota los árboles de diferentes maneras. Desde la antigüedad, la madera se ha usado como combustible. Se habla de especies forestales, que son aquellas que suministran madera y productos derivados. La madera de los árboles es un material común de construcción de edificios y de muebles. La pulpa se emplea para la industria papelera.

Hay árboles frutales, que se caracterizan por producir frutos comestibles y con tal finalidad se plantan por el hombre.

Un tercer tipo de uso es el adorno u ornamento de fincas particulares y espacios públicos. Se habla así de especies ornamentales. Los árboles forman parte del mobiliario urbano: en las ciudades se utilizan los árboles en calles, parques y jardines, como algo ornamental y creando así puntos de descanso, refresco y esparcimiento para los ciudadanos.

Los árboles han jugado un importante papel en la religión, en la magia y la industria, como por ejemplo el árbol de Navidad, y tienen también un gran simbolismo en la filosofía y la cultura, por ejemplo el árbol de la sabiduría. Asimismo tienen un gran protagonismo en relación al calentamiento global.

En diversas culturas el árbol se ha considerado sagrado. En la iconografía cristiana tiene asociada toda una iconografía. Es el eje entre los mundos inferior, terrestre y celeste. Coincide con la cruz de la Redención. La cruz está representada muchas veces como "árbol de la vida". Este árbol de la vida surge por primera vez en el arte de los pueblos orientales; es el "hom" o árbol central colocado entre dos animales afrontados o dos seres fabulosos; es un tema mesopotámico que pasó a Extremo Oriente y Occidente por medio de los persas, árabes y bizantinos. Para las teogonías orientales el "hom" tiene un sentido cósmico, está situado en el centro del Universo y se mueve con la idea del dios creador. Dos árboles míticos o simbólicos mencionados por primera vez en la Biblia en el libro del Génesis. Estos árboles serían llamados "árbol del conocimiento del bien y el mal" y el "árbol de la vida". En el paraíso el árbol de la vida estaba en medio del huerto, pero protegido de los hombres. En el claustro de la iglesia de Santa María la Real de Nieva en la provincia de Segovia (España), en algunos capiteles se encuentra la representación del hom oriental como símbolo del árbol de la vida:

Los budistas, hinduistas y jainistas consideran sagrado cierto tipo de higuera llamada por ello higuera sagrada bajo la cual, creen, Buda alcanzó el nirvana. Yggdrasil es el árbol mítico de los nórdicos, un fresno perenne al que consideraban el "árbol de la vida", o "fresno del universo". Los antiguos sajones tenían también un árbol sagrado, Irminsul, que Carlomagno ordenó destruir cuando los atacó.

En la mitología grecorromana, distintos tipos de árboles y otras plantas han sido consagrados a diferentes divinidades:


Los árboles están desapareciendo de forma masiva de la superficie de la tierra en un proceso de deforestación sin precedentes. Se calcula que un tercio de los bosques del mundo han desaparecido. Se debe en parte a la sobreexplotación que padecen, por ejemplo las selvas tropicales, pero también a los incendios forestales, la mayor parte de los cuales son producidos por el hombre, bien de forma intencionada, bien por negligencia. Además, el hombre efectúa talas intensivas para hacer sitio a otro tipo de cultivo que da un rendimiento económico mayor a corto plazo, por ejemplo, para abrir pastos para la ganadería o para el cultivo de grandes extensiones de soja. Las consecuencias negativas son: la pérdida de hábitats para diversas especies animales y vegetales, la erosión, al dejar el terreno libre a la acción desecante del viento y la libre circulación de las aguas, lo que provoca que se pierda la capa fértil de suelo y ocasiona que el terreno se vaya desertificando.

La solución, además del abandono de determinadas prácticas, como la quema intencionada del bosque para obtener pastos, pasa por una explotación racional, que implique no solo tala sino también reforestación con ejemplares jóvenes que constituyan el bosque del futuro. El Programa de las Naciones Unidas para el Medio Ambiente ha iniciado una campaña mundial Plantemos para el Planeta con el objetivo de plantar 7000 millones de árboles, o sea 1 árbol por habitante de la tierra para finales de 2009. Además, se protegen extensiones de aquellas áreas más ricas en biodiversidad, o de las especies endémicas, muchas de ellas en peligro de extinción.

También hay riesgos naturales que amenazan los bosques, como el fuego, las plagas de insectos y enfermedades.




</doc>
<doc id="6992" url="https://es.wikipedia.org/wiki?curid=6992" title="Poda">
Poda

Podar es el proceso de recortar un árbol o arbusto. Hecho con cuidado y correctamente, la poda puede incrementar el rendimiento del fruto; así, es una práctica agrícola común. En producción forestal se emplea para obtener fustes más rectos y con menos ramificaciones, por tanto de mayor calidad. En arbolado urbano su utilidad es, por un lado, prevenir el riesgo de caída de ramas, y por otro controlar el tamaño de árboles cuya ubicación no permite su desarrollo completo. 

Con frecuencia, en jardinería, se utiliza la poda para conseguir formas artificiales en los árboles o arbustos. Bien ejecutada y repetida con la periodicidad adecuada puede aumentar el valor ornamental de los mismos. Sin embargo con frecuencia se practica de forma inadecuada (mutilaciones como el desmoche), ocasionando pudriciones de la madera que acortan la vida de los árboles e incrementan el riesgo de rotura de ramas. Por otra parte, una tala demasiado radical del árbol a menudo compromete su supervivencia.

Cada árbol exige un tipo de poda diferente. Como norma general las podas más importantes son: 

Las podas de ramas viejas y secas se realizan para prevenir que exista una excesiva cantidad de madera seca que permita una gran combustión en caso de incendio. Son podas de limpieza.

El proceso por el cual un árbol se protege de la entrada de organismos parásitos en los cortes de poda fue descrito en 1977 por Alex Shigo, denominándose modelo CODIT ("Compartmentalization Of Decay In Trees").

En algunos países como México, específicamente en el Distrito Federal, existe legislación ambiental que protege el arbolado urbano y que se remite a normas locales que regulan la actividad de poda como lo es la Norma Ambiental para el Distrito Federal NADF-001-RNAT-2006, que aunque se encuentra sujeta a revisión y modificación, es un buen intento en la protección de estos individuos, y sobre todo la prevención de riesgos a los habitantes de esta ciudad.

El Real Decreto 2177/2004 de 12 de noviembre de la Legislación Española regula el trabajo de altura que es aquel que se realiza a más de dos metros de altura. Con base en esto se denominan podas en altura aquellas realizadas a más de dos metros de altura. Para llevarla a cabo es necesario disponer de elementos elevadores para llegar a las ramas de mayor altura, generalmente a bastantes metros, o hacer uso de técnicas de escalada que permitan al podador subir y podar las rama.

Los principales útiles utilizados para podar son: 





</doc>
<doc id="6997" url="https://es.wikipedia.org/wiki?curid=6997" title="Jardinería">
Jardinería

La jardinería es el arte y la práctica de cultivar los jardines. Consiste en cultivar, tanto en un espacio abierto como cerrado (arriates), flores, árboles, hortalizas, o verduras (huertos), ya sea por estética, por gusto o para la alimentación, y en cuya consecución el objetivo económico es algo secundario.

El término "jardín", conocido desde el siglo XII, parece provenir del compuesto latino-germánico "hortus gardinus" que significa, literalmente, "jardín rodeado de una valla", del latín "hortus", jardín "fráncico", o gart o gardo "cerrado", como si el jardín tuviera que defenderse contra los animales e incluso de los ladrones.

El término "jardinería" se conoce desde finales del siglo XIII (con él se designaba el conjunto de los jardines). Pero adquiere su rango de nobleza con el célebre tratado publicado por vez primera en 1709 titulado "Teoría y práctica de la jardinería", de Dézallier d’Argenville, abogado y secretario del rey, gran amante de los jardines. Hizo una síntesis de los conocimientos del Gran Siglo para el arte de los jardines y para las técnicas hortícolas. Por otra parte, Olivier de Serres, agrónomo, escribió en 1599 «El Teatro de la Agricultura y Cuidado de los Campos» detallando todo lo que se necesita para cuidar, enriquecer y embellecer la casa rústica. Se trata, indudablemente, de un manual agrícola en el que se explica la manera de gestionar una propiedad rural (la propiedad de O. de Serres era de 150 ha.) y en el que la finalidad económica primaba sobre el hecho estético y el placer, pero la obra contiene un capítulo titulado «La Jardinería», con unos subtítulos: «Para tener Hierbas y Frutos: las Hierbas y flores olorosas: las Hierbas medicinales: los Frutos de los Árboles: el Azafrán, el Lino, el Cáñamo, la Granza, los Cardos, los Rozeaux y, además: la Manera de hacer las Conservas para la conservación de los frutos en general».

El término jardinería se usa, especialmente, para el uso, goce y consumición de los particulares mientras que el término horticultura designa la actividad profesional dedicada a la producción de frutos, flores, legumbres y otros productos vegetales. Sus principales denominaciones son: la horticultura para las legumbres, fruticultura para los frutos, floricultura para las flores y arboricultura para los árboles y arbustos. Pese a todo, puede ser utilizado para actividades de tipo lucrativo, si la producción no es muy importante, por ejemplo, cuando un horticultor vende, directamente, en un mercado. Esta situación es corriente en países donde los mercados continúan abasteciéndose por medio de pequeños productores que podrían denominarse "jardineros".

La diferencia entre la jardinería y la horticultura es una diferencia de valores y de medios: la jardinería puede ser un entretenimiento o un medio para complementar los ingresos, mientras que la agricultura o la horticultura se inscriben en los grandes circuitos económicos, con grandes superficies, cantidades y prácticas bien diferentes. La jardinería requiere, casi siempre, la mano de obra y utiliza poco capital y medios mecánicos, son típicos algunos útiles: una pala, un rastrillo, una cesta, una regadera, una carretilla. En comparación, la agricultura se sirve de tractores, segadoras, fertilizantes químicos, sistemas de irrigación, etc.

La jardinería está asociada, generalmente, al cuidado de un jardín, no solo a su creación. Se habla de paisajismo, o de arquitectura de jardín, cuando se trata del arte de pensar o crear un jardín. Es preciso recordar que este término no existía en la época de André Le Nôtre, no se hablaba de paisajismo se utilizaba sólo el término de jardinismo. Por último, es interesante constatar que, un determinado grupo de paisajistas contemporáneos prefieren el término "jardinero-paisajista". El más conocido es Gilles Clément, autor del Jardín Planetario. Esto denota, evidentemente, una determinada filosofía con respecto a la naturaleza, respeto a sus ritmos, y la economía de medios, de energías y recursos que caracterizan a la jardinería: el jardinero ¿no es el que hace suya la divisa "Semper festina lente" (crece lentamente)?

Desde el nacimiento de la jardinería se pueden constatar los primeros signos de sedentarismo de seres humanos con intereses económicos en la jardinería, pero aquí se trata de evaluar los primeros pasos de su nacimiento en el Antiguo Egipto que tenían una connotación política y social. La fecha elegida puede explicarse por el hecho de que el nacimiento de esta cultura y la jardinería denotan, ambas, un mismo factor: un aumento de la prosperidad. Esto permite la utilización de tierras, tiempo y técnicas agrícolas más por razones de estética y entretenimiento que de otra índole. A partir de este momento es cuando se puede empezar a hablar de jardinería propiamente dicha. Los jardines permiten demostrar, a algunos, su prosperidad, lo que demuestra que la jardinería juega también, en cierto sentido, un papel socio-político.

Este cometido va creciendo con el tiempo. En Europa y en América del Norte, la gente pone de manifiesto sus opiniones políticas o sociales en el jardín, de manera intencionada o no. Por ejemplo, el mensaje político de los partidos ecologistas, o algunas ONG, como Greenpeace aconsejando los jardines silvestres y en contra de los prolijos céspedes bien verdes.

Como todas las actividades humanas en las sociedades occidentales, la jardinería no escapa a un cierto mercantilismo y toda una actividad económica se desarrolla alrededor de esta práctica. En su origen sustentada por el comercio del grano, la comercialización de las plantas y granos se incrementa asegurada por la jardinería a la que acompaña una oferta de accesorios y productos de tratamientos diversos que forman parte, en la actualidad, del paisaje de las zonas comerciales y de las grandes ciudades. Viveros y empresas dedicadas a los espacios verdes completan la oferta de servicios accesibles al particular.

Aunque se puede admitir que, por lo general, la jardinería ha estado al alcance de las clases sociales superiores, no se puede decir lo mismo respecto al resto de la sociedad. A medida que va creciendo la prosperidad, los marginados de la jardinería reivindican sus derechos. En Europa, más en concreto en España, en el siglo XVI bajo los postulados del Renacimiento se construyó el primer jardín público del continente, en unos terrenos hasta entonces inundables en el centro de la ciudad de Sevilla conocidos como "la Laguna". En el lugar se abrieron acequias para drenarlo y se concibió un gran jardín público arbolado, con fuentes, un monumento y esculturas que todavía se conserva, es la Alameda de Hércules (1574). Más tarde, se puede decir que fue Inglaterra, durante la época victoriana, el país en el que el Estado empezó a conceder tierras para la construcción de jardines públicos. 

Actualmente, y en Europa en particular, ante la falta, cada vez más creciente, de terrenos vírgenes, especialmente en las ciudades y alrededor de las mismas, un jardín es casi un lujo. Pero se pueden conseguir ingresos suplementarios para las personas menos favorecidas, impulsando la utilización de las tecnologías intermediarias (sobre todo la jardinería ecológica). Los jardines comunitarios que ofrecen el acceso a la jardinería para los ciudadanos, han conseguido, así como con las ideas para este tipo de jardines, poder alimentar hasta 100 ciudadanos.

En algunos países otros movimientos se han puesto en práctica, tales como el Slow Food, que han propuesto, por ejemplo, la creación de jardines alimentarios en las escuelas.

Tras los estragos que la era posindustrial ha causado en la naturaleza, los movimientos político-ecologistas y sus derivados, han ejercido su influencia sobre el campo de la jardinería (también sobre la arquitectura y la vida en general). Así han nacido los jardines silvestres (o jardines naturales), de modo que las plantas ornamentales y los frutos se cultivan junto con las especies nativas. Las especies cultivadas se incluyen en una especie de ecología natural preexistente, no perturbándola, todo lo contrario, favoreciéndose con el proceso de la jardinería. Como en otras formas de jardinería, estos jardines juegan un papel central decidiendo lo que es correcto, sin otras coacciones.

Los jardines silvestres son, por definición, ejemplo de una jardinería que sabe administrar los recursos del agua, dado que las especies naturales presentes en una ecorregión o en un microclima se adaptan por sí mismas a los recursos locales.

El césped, más que el jardín, es un punto importante en la planificación urbana, puesto que establece el derecho a la existencia de la naturaleza silvestre, antes que la naturaleza dominante. Para algunos, el derecho a aceptar en los jardines toda clase de especies, incluso las nocivas o alérgicas, representa un derecho de expresión.

En algunas eco-construcciones, que generan por sí mismas el agua y sus residuos, las cubiertas vegetales han sido creadas. Este principio es lo más próximo al de una máquina viviente, la cual descansa sobre:

En la mayor parte del mundo este tipo de jardines es corriente, a pesar de la existencia de riesgos sanitarios, ya que no se utilizan las tecnologías y métodos modernos.

En China, por ejemplo, los agricultores ponen sus aseos en el exterior, en las carreteras, para favorecer su uso por parte de los turistas y abastecerse de materias orgánicas. Con este método se obtienen calorías, agua y minerales, pero choca con las consideraciones estéticas y sanitarias de la mayor parte de los occidentales que no aceptarían la utilización de los residuos humanos en sus jardines o la alimentación de los animales. Se establece, de este modo, el conflicto entre la jardinería por razones personales o estéticas y razones prácticas de producción de alimentos.

La pared de cultivo es una variación poco habitual de una máquina viviente y convertida en un jardín vertical; el agua resbala por una superficie sobre la cual se desarrolla el musgo y otras plantas, algunos insectos y bacterias, al final de la pared se forma un charco que vuelve a reinyectarse ascendiendo por la pared. Este tipo de jardín es perfecto para el interior de las habitaciones, ayuda a reducir el estrés de la vida en las zonas urbanas o sirve para aumentar el contenido en oxígeno en la atmósfera reciclada. Otros jardines de interior forman parte de los sistemas de calefacción o de aire acondicionado. La pared de cultivo o pared viva forma parte de lo que se denomina jardinería urbana.

El arte de la jardinería está considerado como un arte absolutamente esencial en la mayor parte de las culturas. Se conocen infinidad de evoluciones diferentes por todos los continentes e incluso por países. 


Se observan, no obstante, dos evoluciones paralelas y perfectamente diferenciadas en la jardinería, derivadas de los principales estilos paisajísticos. Algunas culturas han desarrollado una jardinería simétrica y rectilínea, otras una jardinería espontánea y desordenada. Esta disociación tiene su explicación en la historia de la jardinería que nace, principalmente, de dos lugares: en el Egipto Antiguo y en China. La enorme diferencia climática entre ambos países provoca las dos corrientes. Las condiciones áridas del norte de África obligan a los egipcios a "adaptar" sus plantaciones a fin de facilitar su irrigación. Por el contrario, el clima de China, y su lujuriosa vegetación inspiran una jardinería mucho más descuidada en sus habitantes. Los griegos importaron los jardines rectilíneos a Europa, al mismo tiempo que la jardinería "a la China" se imponía en Asia.

Gracias a la reducción del tiempo de trabajo y al aumento del tiempo libre, el número de jardineros aficionados ha crecido notablemente, y el sector dedicado a la jardinería ha experimentado un incremento. Los comercios, (grandes superficies dedicadas a este sector) han proliferado o han sido ampliados gracias a la informática para jardinería. Su crecimiento es considerable.

Los Sistemas de riego más usados comúnmente en el sector de la [jardinería suelen ser los siguientes mencionados:




</doc>
<doc id="7000" url="https://es.wikipedia.org/wiki?curid=7000" title="Planta carnívora">
Planta carnívora

Una planta carnívora (también llamada planta insectívora) es una planta que obtiene parte o la mayoría de sus necesidades nutricionales mediante la captura y el consumo de protozoos y animales, especialmente insectos (además de otros artrópodos). Estas plantas crecen generalmente en lugares donde el suelo es pobre, especialmente en nitrógeno, como las tierras ácidas pantanosas y los farallones rocosos. Charles Darwin escribió el primer tratado conocido sobre estas plantas en 1875.

Se piensa que el hábito carnívoro ha evolucionado en, al menos, 11 linajes separados que se encuentran representados por más de una docena de géneros en cinco familias. Estas incluyen alrededor de 630 especies que atraen y atrapan a sus presas, producen enzimas o poseen bacterias digestivas y absorben los nutrientes resultantes. Además, más de 300 especies de plantas protocarnívoras en varios géneros muestran algunas, aunque no todas.

Existen distintos tipos de plantas carnívoras, dependiendo del mecanismo de captura que utilizan.

Es el mecanismo de la venus atrapamoscas ("Dionaea muscipula"), junto con "Aldrovanda vesiculosa". Son las dos únicas especies que tienen tal mecanismo. El insecto o animal pequeño es atraído por un néctar dulce, se posa en la hoja y cuando roza al menos dos de los cilios detectores dentro de un lapso máximo de cinco segundos, se cierra automáticamente como una pinza o tenaza. La razón por la que deben tocarse dos cilios detectores es para evitar la confusión con gotas de agua. Las espinas de los bordes impiden el escape de su presa.

La presa una vez dentro se mueve, y estimula la secreción de jugos digestivos para su desintegración, que dura varios días. Una vez digerido el insecto, la hoja se abre nuevamente hasta encontrar a otra presa.

Es el mecanismo usado por "Drosera", "Byblis", "Drosophyllum" y "Pinguicula", entre otras. "Drosera" posee hojas en rosetas pegadas al suelo que segregan un fluido viscoso con un aroma similar al de la miel.

Cuando un insecto se posa en la hoja, queda atrapado en los pelos pegajosos. Después, los tentáculos de "Drosera" se curvan hacia adentro hasta que se cierran. Puede tardar desde un minuto a varias horas en cerrar y transcurren entre 7 a 14 días hasta que los tentáculos se vuelven a abrir completamente.

Son las utilizadas por los géneros "Heliamphora, Nepenthes, Sarracenia, Cephalotus, Darlingtonia" y "Brocchinia reducta". Estas plantas también se conocen como plantas odre o planta jarra. Estas trampas tienen forma de jarrón o de copa y al fondo tienen líquido acuoso donde los insectos se ahogan. Estos son atraídos por aromas que producen los bordes de la trampa y cuando se posan, resbalan y caen adentro, y una vez ahogados, la planta los digiere. En algunos géneros como "Nepenthes", las trampas tienen además tapaderas, que actúan como sombrillas, evitando que el agua de lluvia las llene completamente.

"Darlingtonia californica" es única porque no atrapa el agua de lluvia en su jarra, sino que la regula dentro bombeando desde sus raíces o expulsándola, según convenga. Sus hojas no producen enzimas digestivas, ya que las células que absorben los alimentos son idénticas a las raíces del suelo, confiando en bacterias simbióticas. También comparte su mecanismo de captura con "Sarracenia psittacina", en la que el insecto atraído por un néctar que produce la entrada de la trampa es confundido por "falsas salidas" o "falsas ventanas", que sólo harán que el animal vaya más abajo hasta que quede agotado y caiga en el líquido.

Este mecanismo es exclusivo del género "Utricularia" y es el más complejo y rápido del mundo de las plantas carnívoras. Estas plantas acuáticas poseen numerosas trampas en cada tallo que se asemejan a globos diminutos. Cada trampa tiene una trampilla muy pequeña que suele estar bien cerrada. Para tender las trampas, la planta bombea hacia afuera una parte del agua, para que la presión en el interior sea menor que la exterior. Si un diminuto animal nada demasiado cerca de la trampa, roza unas cerdas que se encuentran pegadas a la trampilla. La trampilla se abre y la trampa absorbe agua, arrastrando al animal hacia adentro. Luego la trampilla nunca se cierra. Cuando la planta ya ha digerido a su víctima, vuelve a tender la trampa y dispone a capturar otra presa.

Son trampas fáciles de entrar y difíciles de salir. La salida está obstruida por pelos rígidos que apuntan hacia adentro. Se dan en el género "Genlisea" que se especializan en cazar protozoos, a los que atraen de forma química. Una hoja en forma de Y permite la entrada de la presa pero no la salida. Los pelos que apuntan hacia adentro obligan a que la presa se mueva en una sola dirección. La presa pasa por la entrada en espiral que la va llevando alrededor de los dos brazos superiores de la Y, por ello se ven obligados a moverse inexorablemente hacia un estómago en el brazo inferior de la Y. El movimiento de la presa se cree que está ayudado por el movimiento del agua a través de la trampa, producido de una manera similar a la de vacío en las trampas de la vejiga, y probablemente evolutivamente relacionado con ella.

Este mecanismo recuerda mucho al visto en "Sarracenia psittacina, Darlingtonia californica" y "Nepenthes aristolochioides".

Un mecanismo utilizado únicamente por "Drosera glanduligera", en la que combina características de las trampas de pinza y de sus trampas de pelos pegajosos, estas últimas comunes en las demás droseras.

Aunque las distintas especies de plantas carnívoras poseen diferentes necesidades de luz solar, mezcla de sustrato o humedad, todas comparten algunas de estas necesidades. La mayoría requiere agua de lluvia, de pH 6,5 con ácido sulfúrico. El agua corriente contiene minerales (en especial sales de calcio) que se acumularían hasta matar la planta. Esto ocurre porque la mayoría de las plantas carnívoras ha evolucionado en sustratos ácidos y pobres en nutrientes y son, en consecuencia, extremadamente calcífugas. Por lo tanto son demasiado sensibles al aporte continuado de nutrientes en el suelo. Ya que la mayoría vegeta en pantanos, casi todas son muy intolerantes a la sequía, por lo que en verano hay que colocar la maceta sobre un platillo con agua. Sin embargo, hay excepciones, como las tuberosas "Drosera" que requieren un período seco en verano (reposo) y "Drosophyllum lusitanicum" que requiere condiciones mucho más secas que la mayoría.

Las plantas cultivadas en el exterior normalmente obtienen insectos más que suficientes para alimentarse adecuadamente, aunque en ocasiones se les deben suministrar manualmente para suplementar la dieta. Sin embargo, estas plantas son incapaces de digerir otro tipo de alimentos que no sean insectos, como trozos de carne, por ejemplo, ya que estos se pudrirían en el interior de la trampa causando la muerte de toda la planta. Es raro que una carnívora muera aunque no atrape ningún insecto, lo que puede afectar es a su crecimiento. En general, lo mejor es dejar que estas plantas utilicen sus propios recursos: las causas más comunes de muerte para una Venus atrapamoscas, son, además de regarla con agua del grifo, intentar forzar su trampa para alimentarla.

Salvo un par de especies, "Nepenthes" y "Pinguicula", que vegetan bien a la sombra, la mayoría requiere luz brillante o pleno sol, para estimularles a sintetizar los pigmentos rojo y púrpura de la antocianina.

La mayoría vive en los pantanos y las demás, generalmente, en regiones tropicales, por lo que requieren un alto grado de humedad. Estas condiciones se pueden imitar en el cultivo doméstico o a pequeña escala colocando las plantas dentro de un recipiente mayor con el fondo cubierto de guijarros que se mantengan constantemente húmedos. Las especies pequeñas de "Nepenthes" vegetan bien en un terrario.

Muchas especies son originarias de regiones frías, por lo que pueden cultivarse en un jardín húmedo durante todo el año. La mayoría de las especies de "Sarracenia" tolera temperaturas por debajo de 0 °C, a pesar de que casi todas son oriundas de la zona suroriental de Estados Unidos. Las especies de "Drosera" y "Pinguicula" también toleran estas temperaturas. Sin embargo, el género "Nepenthes", que es tropical, requiere entre 20 a 30 °C para prosperar.

El sustrato más adecuado para plantas carnívoras es una mezcla 3:1 de turba de "Sphagnum" con arena ácida del tipo usado para horticultura (la fibra de coco es un sustituto más ecológico que la turba). "Nepenthes" crece bien en un compuesto para orquídeas o simplemente en musgo de "Sphagnum".

Irónicamente, estas plantas son propensas a sufrir infestaciones parasitarias de áfidos o cochinillas. Los ataques menores se pueden eliminar a mano, sin embargo las infestaciones masivas requieren un insecticida. El alcohol isopropílico es efectivo como insecticida topical, particularmente para cocoideos. El diazinón es un excelente insecticida sistémico tolerado por la mayoría de las carnívoras, así como el malation y el acefato.

Pero aunque las plagas de insectos puedan ser un problema, el mayor peligro para plantas carnívoras (además del maltrato humano) es el moho gris ("Botrytis cinerea"). Este medra en condiciones cálidas y húmedas, convirtiéndose en una seria amenaza en invierno. En cierta medida, se puede proteger a las carnívoras de regiones frías, manteniéndolas frescas y bien ventiladas y asegurándose de retirar las hojas muertas con regularidad. Si aun así el hongo ataca, será necesario un fungicida.




</doc>
<doc id="7002" url="https://es.wikipedia.org/wiki?curid=7002" title="Bandera de Cataluña">
Bandera de Cataluña

La bandera de Cataluña, adoptada como bandera oficial de la comunidad autónoma de Cataluña en el Estatuto de autonomía de Cataluña de 1979 en su artículo 4, se define y regula actualmente según el artículo 8.2 del Estatuto de Autonomía de Cataluña de 2006:

Se trata de la tradicional señera de los Reyes de la Corona de Aragón, que era antiguamente usada únicamente por el Rey, como expresiva de su soberanía. 

Existe documentación que prueba fehacientemente que la misma fue usada desde los tiempos de Alfonso II Rey de Aragón y Conde de Barcelona, siendo universalmente conocida como "de Aragón", dada la preeminencia del reino de Aragón en la titulación, como reconoce el propio Pedro IV el Ceremonioso:

Con el paso del tiempo, el emblema de los reyes de la Corona de Aragón pasó a identificarse con los territorios que gobernaban. Su identificación con el condado de Barcelona y, por extensión, con el Principado de Cataluña, parece originarse también en tiempos de Pedro el Ceremonioso y se prolonga en los siglos posteriores, aunque sigue siendo utilizado también por otros territorios de la Corona. Ya a mediados del siglo XVI, el historiador valenciano Pere Antoni Beuter narra la leyenda de Wifredo el Velloso y los dedos de sangre en su "Crónica General de España". Posteriormente, en un poema de 1644, Francesc Fontanella aludía a las «barras» en uno de sus poemas:

Desde mediados del siglo XIX, y particularmente a partir de la eclosión del catalanismo como movimiento cultural y político con la Renaixença, las antiguas armas del rey de Aragón adquieren un simbolismo político de afirmación identitaria. Tras períodos alternativos de tolerancia y represión del uso de la bandera cuatribarrada, fue izada como «bandera de la Patria» en la Diputación de Barcelona el 27 de mayo de 1930 y de nuevo el 14 de abril de 1931 al ser proclamada la República Catalana por Francesc Macià.

Son elementos comunes de la Bandera y el Escudo los "palos de gules" o "barras de Aragón", elemento histórico común de las actuales cuatro comunidades autónomas que en su día estuvieron integradas en la Corona de Aragón, en cuya emblemática se encuentran todavía, y que en su representación se incorporaron al Escudo de España.

No se ha hallado ninguna referencia documental hasta el año 1150, en que aparece como escudo preheráldico en un sello de Ramón Berenguer IV, aunque la escasa nitidez del sello, y su monocromía, hacen dudosa la atribución, pues el escudo palado y blocado refleja los habituales refuerzos defensivos de los escudos de mediados del siglo XII, por lo que este no sería un signo de linaje, sino el mero escudo de tablas almendrado que simbolizaba el poder real. El primer testimonio seguro son los sellos de la cancillería de Alfonso II, datados a partir de 1167.

No está demostrada la prueba de su vinculación a la casa condal barcelonesa en sendos sarcófagos de 1082 de Ramón Berenguer II y Ermesenda de Carcasona, donde aparecen pintados 15 palos de oro y gules, lo que ha llevado a pensar a algunos heraldistas (A. Fluvià, M. de Riquer) que este es el origen de las cuatro barras en tanto que emblema pre-heráldico; pero no puede ser una prueba de la antigüedad del emblema asociado al linaje condal. Según Faustino Menéndez-Pidal y otros autores, se trataría de una decoración impostada con motivo de su traslado en 1385 al interior de la Catedral de Gerona por iniciativa de Pedro IV de Aragón, por lo tanto, la pintura aludida sería 300 años posterior, ya que en su emplazamiento original a la intemperie durante tres siglos es imposible que el sarcófago conservara la pintura del siglo XI, como demuestra Alberto Montaner Frutos en "El señal del rey de Aragón: historia y significado" (1995). Menéndez-Pidal también arguye que es aún más difícil demostrar que se realizasen ornamentaciones emblemáticas en las tumbas del siglo XI, y que en el sepulcro posterior de Ramón Berenguer III no se encuentra ninguna de estas ornamentaciones.

Otros historiadores (Guillermo Fatás y Guillermo Redondo) refuerzan el argumento de que el emblema de las barras de gules en campo de oro proviene de la temprana vinculación del Reino de Aragón con la Santa Sede. En todo caso, el linaje de los reyes de Aragón es el único linaje que podía ostentar las mentadas franjas de gules en campo dorado, puesto que Alfonso II lo hereda del derecho sucesorio que le concede un lugar como miembro de la Casa de Aragón, y lo heredan sus hijos como dignidad familiar y siempre vinculado al título principal de Rey de Aragón (en el derecho aragonés medieval conocido como "Matrimonio en Casa"), linaje al que, según algunos historiadores como Ubieto o Fatás, accede por el matrimonio con la heredera de la casa, Petronila de Aragón, al haberse cortado la posibilidad sucesoria por vía masculina. Esta teoría ha sido cuestionada recientemente por J. Serrano Daura, dada la ausencia de referencias a esta institución consuetudinaria del derecho aragonés antes del siglo XV, y que las cláusulas que fueron establecidas por Ramiro II sobre la sucesión a la corona de Aragón no se ajustan a las peculiaridades de esta institución, por lo que no sería trasladable al siglo XII.

Otra teoría sobre el origen del señal de la Casa de Aragón lo relaciona con el viaje de Sancho Ramírez (1064-1094) a Roma en 1068 para consolidar el joven reino de Aragón ofreciéndose en vasallaje al Papa, vasallaje documentado incluso en la cuantía del tributo de 600 marcos de oro al año. De ahí que se haya aducido que Alfonso II, conocedor de ese viaje, tomara como emblema del vínculo vasallático las conocidas franjas rojas y doradas, inspiradas en los colores propios de la Santa Sede, que eran bien conocidos y están bien documentados en las cintas de lemnisco de los sellos de la Santa Sede, y son visibles hoy todavía en la "umbrella" vaticana.

Hay que decir que en la segunda mitad del s. XII, el señal de la Casa de Aragón era un mero distintivo familiar, y no territorial, de manera que no era posible la identificación con él de sus súbditos, que lo reconocerían solo como atributo de su rey o de la autoridad de él emanada. La confusión sobre su origen condal fue difundida durante la Renaixença en el siglo XIX de estos símbolos, utilizando historiografía del s. XVI y apoyada en la creencia de que Pedro IV tenía conocimientos heráldicos rigurosos en el siglo XIV. 

Además, Pedro IV usó con profusión otros símbolos, ya caballerescos en el sentido que cobrarían en el siglo XV, ceremoniales y ornamentales, y los adoptó por primera vez (por ejemplo la llamada Cruz de Íñigo Arista) en la creencia de que era el señal de los antiguos reyes de Aragón. También fue introducida por él la cruz de San Jorge, incluso fundó una orden de caballería valenciana caracterizada por estas armas. Asimismo, fue él quien usó una cimera con un dragón (probablemente emblema parlante: D'Aragón=dragón y de ahí el entramado que hizo a San Jorge patrón de Aragón, por serlo de su rey, y con ello de todos los reinos (Valencia, Mallorca, Sicilia) y condados (Barcelona) que componían la Corona de Aragón. De ese dragón en cimera debió surgir, por deformación, el murciélago de Valencia.

A finales del siglo XIV, la atribución de los palos al condado de Barcelona era cada vez más extendida. Así, en el "Armorial d'Urfé", fechado alrededor de 1380, se afirma que "«Le roy d'Arragon d'or à iij paux de gueules, et son les armes de le Conte de Cathalogne (...) Le royaume d'Arragon d'azur à la croix d'argent patée.»" En las Cortes de 1396 se dispone que "«les galees no porten banderas, cendals ne panys de senyal alcú sinó del comptat de Barchelona, ço es, barres grogues e vermelles tant solament»", y en 1406, el rey de Aragón Martín el Humano declara ante las Cortes reunidas en Perpiñán: "«Fil, yo us do la bandera nostra antiga del principat de Cathalunya (...) la dita nostra bandera reyal.»"

La designación de la documentación medieval es "El senyal real del Rei d'Aragón", sin que aparezca como sustantivo la denominación de "señera". En ningún caso se documente el presunto linaje de los "Condes-Reyes", ambas denominaciones surgidas en la historiografía decimonónica al amparo del nacionalismo catalán. Por otra parte, rey de Aragón es el título principal de esta Casa, que utilizaron todos los reyes de Aragón arriba mencionados, incluso Martín I El Humano o Alfonso V El Magnánimo. Solo si se consignaba el título completo aparecía el de "Conde de Barcelona", que es la única denominación posible en la Edad Media.

El historiador valenciano Pere Antoni Beuter incluyó en su "Crónica general de España", por primera vez, la leyenda que atribuye su origen a Wifredo el Velloso (Guifré el Pilós), conde de Barcelona, Gerona y Osona en el siglo IX. Wifredo el Velloso era hijo de Sunifredo I de Urgel-Cerdaña, y reunió bajo su gobierno los condados de Barcelona, Urgel, Cerdaña, Besalú y Gerona; reconquistó Montserrat, fundó el monasterio de San Juan de las Abadesas y vivificó el de Ripoll. Repobló todo el centro de Cataluña y con esto consolidó su unidad interior. Inició la casa de Barcelona, la dinastía catalana que se subordinaría, según algunos historiadores como Ubieto o Fatás, con la firma del "Matrimonio en Casa" con Petronila de Aragón desde 1150, a la Casa de Aragón. Recientemente, el profesor J. Serrano Daura ha cuestionado la teoría del "casamiento en casa" aplicada a los esponsales de Ramón Berenguer IV y Petronila de Aragón, basándose en la ausencia de referencias a esta institución consuetudinaria del derecho aragonés antes del siglo XV, y que las cláusulas que fueron establecidas por Ramiro II sobre la sucesión a la corona de Aragón no se ajustan a las peculiaridades de esta institución, por lo que no sería trasladable al siglo XII.

Esta explicación legendaria, presente en otros lugares de Europa, incluida una anécdota de la Castilla del siglo XIII, refiere que en una de sus gestas decidió, con sus seguidores, una victoria de los francos sobre los normandos. El premio que habría recibido por ello, sería un escudo con fondo de oro de manos del rey Carlos el Calvo. Explica la leyenda que el mismo rey pintó, con los dedos manchados de sangre de las heridas del conde, las cuatro franjas rojas.

Fluvià propone como apoyo documental una inicial miniada de la versión catalana de la "Crónica de San Juan de la Peña" donde aparece el conde Guifredo (no su hijo, Guifredo el Velloso) rindiendo vasallaje al emperador Carlomagno, pero los escudos que portan son apócrifos. El de Carlomagno, que nunca usó, es claramente legendario. El de Guifredo sería fruto del hecho de que la Crónica fue compuesta en el taller de Pedro IV en la segunda mitad del siglo XIV. Como vemos la labor de rearme emblemático y heráldico de Pedro IV, que necesitaba hacer prevalecer su dignidad frente a la nobleza en la crisis de la sociedad estamental del S. XIV, fue ingente.

En la Edad Moderna comenzaron a usarse de modo vacilante diversos escudos asociados privativamente a los distintos territorios que componían la Corona de Aragón, y para este propósito se usaron tanto las armas del rey de Aragón (los cuatro palos), como otros emblemas aparecidos a lo largo de la Edad Media. Como escudo privativo de Cataluña se usaron dos blasones: el propio señal real y la cruz de San Jorge, que formaba parte del escudo de Barcelona y era el emblema de la Generalidad de Cataluña. De forma similar ocurrió en Aragón, donde se alternó el uso del señal real como símbolo privativo con otros dos emblemas, difundidos como armas privativas de Aragón desde fines del siglo XV: la cruz de Alcoraz y la cruz de Íñigo Arista. La utilización de los palos de gules en múltiples instituciones asociadas al Principado de Cataluña no implica que este emblema fuera considerado en la Edad Moderna exclusivo de Cataluña. También se usaron en Valencia y en Aragón con similares funciones. No fue hasta el siglo XIX, debido a la "Renaixença" y a las interpretaciones inexactas de la heráldica de ese tiempo, que comenzó a tenerse el emblema de los palos de oro y gules como único propio y exclusivo de Cataluña, desechando otros emblemas privativos de Cataluña basados en la Cruz de San Jorge.

La ambivalencia de los símbolos se manifestó, por ejemplo, en la Corte convocada por Felipe V en 1701, en la que se discutió cuál debía ser su sello: mientras unos propusieron «el glorioso San Jorge llevando el escudo de la Cruz», la mayoría se inclinó por «el escudo de las cuatro barras y sobre él la corona real». Incluso después de los Decretos de Nueva Planta, una exposición de la Real Audiencia de Cataluña de 1755 se refería a «las armas de Cataluña (...), las cuatro barras de gules o rojas en campo de oro».

Durante la invasión napoleónica, las autoridades francesas acuñaron monedas en Barcelona en 1808 que, según la descripción del barón de Maldà, tenían «armes d'Aragó en una cara i en l'altra ab la inscripció de "Valga per la província", i res d'armes de França». En 1810, Napoleón mandó al mariscal Augereau izar «en lugar de la bandera española, la bandera francesa y la catalana», en la que es la primera referencia pública moderna.

Las Cortes Catalanas adoptaron la bandera de la Cruz de San Jorge en 1359 bajo el reinado de Pedro IV por considerar este a la Cruz de San Jorge como "las antiguas armas de Barcelona". Esto se debe a que eran las armas del brazo eclesiástico, es decir, el escudo de la diócesis de Barcelona por correspondencia a San Jorge, el patrón de esta.

El historiador español Lluís Domènech i Muntaner así lo testifica:

Años después, en 1701, adopta las llamadas barras de Aragón. Durante la Segunda República española, después de reinstaurado el autogobierno, se siguen adoptando como símbolo, que sigue siendo el utilizado en la actualidad.





</doc>
<doc id="7007" url="https://es.wikipedia.org/wiki?curid=7007" title="John Napier">
John Napier

John Napier de Merchiston, llamado también Johannes Neper o Nepair ( Edimburgo, 1 de febrero de 1550-"ibídem", 4 de abril de 1617), fue un matemático e inventor escocés, reconocido por ser el primero en definir los logaritmos. También hizo común el uso del punto decimal en las operaciones aritméticas.

El padre de Napier era Sir Archibald Napier del Castillo de Merchiston y su madre era Janet Bothwell (hija del político y juez Francis Bothwell, Lord of Session y hermana de Adam Bothwell, quien llegaría a ser Obispo de Orkney).

A los 13 años comenzó a asistir a la Universidad de Saint Andrews, en Fife. Permaneció allí menos de un año. Su madre murió durante ese período de tiempo. Su tío Adam Bothwell recomendó en una carta al padre de John que lo hiciera viajar por Europa para educarse: "Señor, le ruego que envíe a John a la escuela de Francia o la de Flandes porque no puede aprender nada bueno en casa". Se cree probable que el consejo haya sido seguido y que Napier viajó por Países Bajos, Francia e Italia para formarse.

De regreso a Merchiston en 1571 contrajo matrimonio al año siguiente con Elizabeth Stirling, con quien tuvo dos hijos. Vivieron en un castillo en Gartland (Stirling). Ella murió en 1579. Napier se casó poco después con Agnes Chisholm, con quien tuvo diez hijos.

Cuando su padre murió en 1608, John pasó a vivir en el castillo de Merchiston hasta que falleció nueve años después debido a causas naturales.

En su vida, Napier mostró gran interés en la búsqueda de técnicas para simplificar las tareas de cálculo. Ya en la década de 1570 escribió su primer tratado, en el que muestra diversos métodos eficientes de cálculo, describe notaciones más sencillas e investiga acerca de las raíces imaginarias de ecuaciones. El trabajo no fue publicado sino hasta 1838, cuando estas ideas ya habían sido superadas por otros matemáticos.

Sin dudas, su mayor aporte en el campo de la matemática fue el concepto de logaritmo. Napier estudió acerca de ellos entre 1590 y 1617. La primera obra que publicó en ese sentido fue "Mirifici Logarithmorum Canonis Descriptio" (Descripción de una admirable tabla de logaritmos) en 1614. Allí describe cómo utilizar los logaritmos para resolver problemas con triángulos y da una tabla de logaritmos. En 1619 su hijo Robert publica póstumamente "Mirifici logarithmorum canonis constructio" (Construcción de una admirable tabla de logaritmos), donde se explica cómo se construye la tabla de logaritmos.

Si bien en el comienzo denominó «números artificiales» a los logaritmos, él mismo crearía luego el nombre con el que se los conoce actualmente, al combinar las palabras griegas «logos» (proporción) y «arithmos» (número).

El descubrimiento de Napier tuvo un éxito inmediato, tanto en matemática como en astronomía. Algunos de los pioneros en seguir su trabajo fueron Henry Briggs y John Speidell. Johannes Kepler dedicó una publicación de 1620 a Napier, afirmando que los logaritmos fueron la idea central para poder descubrir la tercera ley del movimiento de los planetas.

Una cita de Pierre-Simon Laplace hace mención y honor al descubrimiento y aplicación de los logaritmos por Napier:

Otro aporte, aunque de forma lateral, de Napier es la utilización de la notación decimal actual. Gracias a la difusión de su obra "Mirifici logarithmorum canonis constructio" por Europa, en la que se utilizaba la coma para separar la parte entera de la decimal en un número, esta notación se volvió popular. Si bien no fue él quien la creó, sí fue el responsable de que se popularizara.

Napier diseñó tres aparatos para facilitar cálculos, descritos en su obra de 1617 "Rabdologiae". Si bien el más famoso es su ábaco neperiano, puede considerarse a su "promptuario" como una de las primeras máquinas de cálculo de la historia.

Toda su vida se dedicó a pelear por sus ideas religiosas, siendo un protestante apasionado.

Ya desde sus primeros años Napier estuvo interesado en el estudio del Apocalipsis. En 1594 publicó "Plaine Discovery of the Whole Revelation of Saint John", obra muy influyente en su época, siendo traducida al francés y alemán y reeditada en varias ocasiones. Allí, entre otras cosas, muestra que el Papa es el Anticristo y urge a el rey de Escocia a expulsar de su corte a todos los papistas y ateos. Además, predice el fin del mundo.

Napier fue también un inventor en diversas ramas:






</doc>
<doc id="7008" url="https://es.wikipedia.org/wiki?curid=7008" title="Teorema fundamental del álgebra">
Teorema fundamental del álgebra

El teorema fundamental del álgebra establece que todo polinomio de grado mayor que cero tiene una raíz. El dominio de la variable es el conjunto de los números complejos, que es una extensión de los números reales.

Aunque este enunciado, en principio, parece ser una declaración débil, implica que todo polinomio de grado "n" de una variable con grado mayor que cero con coeficientes complejos tiene, contando las multiplicidades, exactamente "n" raíces complejas. La equivalencia de estos dos enunciados se realiza mediante la división polinómica sucesiva por factores lineales.

Hay muchas demostraciones de esta importante proposición, que requieren bastantes conocimientos matemáticos para formalizarlas.

Pedro Rothe (Petrus Roth), en su libro "Arithmetica Philosophica" (publicado en 1608), escribió que una ecuación polinómica de grado formula_1 (con coeficientes reales) "puede" tener formula_1 soluciones. Alberto Girardo, en su libro "L'invention nouvelle en l'Algebre" (publicado en 1629), aseveró que una ecuación de grado formula_1 tiene formula_1 soluciones, pero no menciona que dichas soluciones deban ser números reales. Más aún, él agrega que su aseveración es válida "salvo que la ecuación sea incompleta", con lo que quiere decir que ninguno de los coeficientes del polinomio sea igual a cero. Sin embargo, cuando explica en detalle a qué se está refiriendo, se hace evidente que el autor piensa que la aseveración siempre es cierta; en particular, muestra que la ecuación
a pesar de ser incompleta, tiene las siguientes cuatro soluciones (la raíz 1 tiene multiplicidad 2):
Leibniz en 1702 y más tarde Nikolaus Bernoulli, conjeturaron lo contrario.

Como se mencionará de nuevo más adelante, se sigue del teorema fundamental del álgebra que todo polinomio con coeficientes reales y de grado mayor que cero se puede escribir como un producto de polinomios con coeficientes reales del cual sus grados son 1 o 2. De todas formas, en 1702 Leibniz dijo que ningún polinomio de tipo formula_7 (con "a" real y distinto de 0) se puede escribir en tal manera. Luego, Nikolaus Bernoulli hizo la misma afirmación concerniente al polinomio formula_8, pero recibió una carta de Euler en 1742 en el que le decía que su polinomio pasaba a ser igual a:

con α igual a raíz cuadrada de 4 + 2√7. Igualmente mencionó que:

El primer intento que se hizo para demostrar el teorema lo hizo d'Alembert en 1746. Su demostración tenía un fallo, en tanto que asumía implícitamente como cierto un teorema (actualmente conocido como el teorema de Puiseux) que no sería demostrado hasta un siglo más tarde. Entre otros Euler (1749), de Foncenex (1759), Lagrange (1772) y Laplace (1795) intentaron demostrar este teorema.

A finales del siglo XVIII, se presentaron dos nuevas pruebas, una por James Wood y otra por Gauss (1799), pero ambas igualmente incorrectas. Finalmente, en 1806 Argand publicó una prueba correcta para el teorema, enunciando el teorema fundamental del álgebra para polinomios con coeficientes complejos. Gauss produjo otro par de demostraciones en 1816 y 1849, siendo esta última otra versión de su demostración original.

El primer libro de texto que contiene la demostración de este teorema fue escrito por Cauchy. Se trata de "Course d'anlyse de l'École Royale Polytechnique" (1821). La prueba es la debida a Argand, sin embargo, en el texto no se le da crédito.

Ninguna de las pruebas mencionadas más arriba son constructivas. Es Weierstrass quien por primera vez, a mediados del siglo XIX, menciona el problema de encontrar una prueba constructiva del teorema fundamental del álgebra. En 1891 publica una demostración de este tipo. En 1940 Hellmuth Knesser consigue otra prueba de este estilo, que luego sería simplificada por su hijo Marin Kneser en 1981.

El teorema se enuncia comúnmente de la siguiente manera:

Es ampliamente conocido también el enunciado: Un polinomio en una variable, no constante y con coeficientes complejos, tiene tantas raíces como indica su grado, contando las raíces con sus multiplicidades. En otras palabras, dado un polinomio complejo "p"("z") de grado "n" ≥ 1, la ecuación "p"("z") = 0 tiene exactamente "n" soluciones complejas, contando multiplicidades.

Otras formas equivalentes del teorema son:

Sea formula_12 un polinomio de grado formula_1. formula_12 es una función entera. Para cada constante positiva formula_15, existe un número real positivo formula_16 tal que

Si formula_12 no tiene raíces, la función formula_19, es una función entera con la propiedad de que para cualquier número real formula_20 mayor que cero, existe un número positvo formula_16 tal que

Concluimos que la función formula_23 es acotada. Pero el teorema de Liouville dice que si formula_23 es una función entera y acotada, entonces, formula_23 es constante y esto es una contradicción.

De manera que formula_23 no es entera y por tanto formula_12 tiene al menos una raíz. formula_12 se puede escribir por tanto como el producto

donde formula_30 es una raíz de formula_12 y formula_32 es un polinomio de grado formula_33. Por el argumento anterior, el polinomio formula_32 a su vez tiene al menos una raíz y se lo puede factorizar nuevamente.

Repitiendo este proceso formula_33 veces, concluimos que el polinomio p puede escribirse como el producto

donde formula_30 … formula_38 son las raíces de formula_39 (no necesariamente distintas) y formula_40 es una constante.

Como el teorema fundamental del álgebra puede ser visto como la declaración de que el cuerpo de los números complejos es algebraicamente cerrado, se sigue que cualquier teorema concerniente a cuerpos algebraicamente cerrados aplican al cuerpo de los números complejos. Se muestran aquí algunas consecuencias del teorema, acerca del cuerpo de los números reales o acerca de las relaciones entre el cuerpo de los reales y el cuerpo de los complejos:








</doc>
<doc id="7010" url="https://es.wikipedia.org/wiki?curid=7010" title="Filosofía del derecho">
Filosofía del derecho

La filosofía del derecho y los valores es una rama de la filosofía que estudia los fundamentos filosóficos del derecho y los valores como orden normativo e institucional de la conducta humana en sociedad sin perder el punto de vista ético y los valores respectivos que conlleva el mismo.

El contenido de la filosofía del derecho en un sentido amplio trata de aglutinar el estudio filosófico no solo de la norma jurídica positiva, sino de todas las corrientes de pensamiento que sirven de fundamento al propio derecho, entendido éste como el orden normativo e institucional de la sociedad. Sus campos de estudio se pueden dividir en:


Junto con el derecho natural, la parte más importante de la filosofía del derecho lo ha constituido el estudio de la norma jurídica desde el punto de vista positivo (iuspositivismo).

La filosofía del derecho aparece, con este preciso nombre, a finales del siglo XVIII e inicios del XIX. Hasta entonces, la reflexión de carácter filosófico sobre el fenómeno jurídico se había enmarcado dentro de la tradición de la Filosofía política del derecho natural, bien de corte escolástico o racionalista. Al lado de las leyes positivas, el derecho natural se presentaba como un orden válido por sí mismo, evidente, e invariable, que constituía la regla última de toda comunidad humana. El derecho natural no es obra de los seres humanos, y no es producto de la historia.

Frente a esta concepción, las corrientes iuspositivistas consideran a los ordenamientos jurídicos como creaciones humanas que se desarrollan y cambian en el tiempo histórico. El derecho es siempre un sistema normativo, coactivo e institucional efectivamente válido y vigente en un grupo social determinado. No es un orden lógico y racional, surgido de la naturaleza, sino un conjunto de normas de conducta elaborado artificialmente como respuesta a los conflictos y a las necesidades de una comunidad específica en un momento histórico.

De este modo, durante la segunda mitad del siglo XVIII el concepto «derecho natural» va perdiendo su primacía y comienzan a aparecer numerosas obras en que la reflexión teórica sobre las instituciones jurídicas se reviste de otra terminología. En 1797 Kant emplea la expresión «teoría del derecho»; en 1798 Gustav Hugo utiliza «filosofía del derecho positivo»; en 1803 Jakob Friedrich Fries emplea «teoría filosófica del derecho», y el término «filosofía del derecho» es empleado desde 1800 en adelante por autores como W. T. Krug, Chr. Weiss o Karl Christian Friedrich Krause. En 1821 Hegel publicó en Berlín sus "Principios de la filosofía del derecho", que adquieren una resonancia decisiva.


La epistemología jurídica (o teoría de la ciencia jurídica) estudia los métodos y los procedimientos intelectuales que los juristas emplean para identificar, interpretar, integrar, y aplicar las normas jurídicas. También se ocupa del estudio sistemático de la argumentación jurídica. La disciplina central es, en este terreno, la dogmática jurídica, que toma como punto de partida el dogma de la «racionalidad del legislador» y se ocupa de la descripción de un sistema jurídico positivo entendido como un conjunto de normas, sin ponerlas en discusión, presentándolas como un orden lógico, coherente y completo, integrando sus lagunas y resolviendo sus antinomias. La dogmática es la disciplina que caracteriza la mayor parte de los estudios impartidos en las facultades de derecho.

La discusión sobre si este saber jurídico tiene o no carácter científico es muy larga. La crítica más célebre contra la ciencia del derecho fue expuesta por Kirchmann en una conferencia de 1847: «tres palabras del legislador convierten bibliotecas enteras en basura». Algunos autores (como A. Calsamiglia) se inclinan por considerar que lo que usualmente se denomina ciencia jurídica es, con más propiedad, una técnica social.

La ciencia jurídica moderna tiene su origen en el siglo XV. De forma paralela en el tiempo se van desarrollando: en las Islas Británicas, la Escuela Analítica de Jurisprudencia; en Francia, la Escuela de la Exégesis, y en Alemania, la Escuela Histórica del derecho (Friedrich Karl von Savigny) y la Jurisprudencia de Conceptos (Ihering).

Un modelo de ciencia jurídica que conserva gran parte de su validez es el propuesto por John Austin, discípulo de Jeremy Bentham. Para él, la ciencia general del derecho se clasifica en dos grandes áreas:







</doc>
<doc id="7017" url="https://es.wikipedia.org/wiki?curid=7017" title="Ábaco neperiano">
Ábaco neperiano

El ábaco de Napier es un ábaco inventado por John Napier quien publicó la descripción del mismo en una obra impresa en Edimburgo a finales de 1617 titulada "Rhabdologia". Por este método, los productos se reducen a operaciones de suma y los cocientes a restas; al igual que con las tablas de logaritmos, inventadas por él mismo se transforman las potencias en productos y las raíces en divisiones.

El ábaco consta de un tablero con reborde en el que se colocarán las varillas neperianas para realizar las operaciones de multiplicación o división. El tablero tiene su reborde izquierdo dividido en 9 casillas en las que se escriben los números 1 a 9.

Las varillas neperianas son tiras de madera, metal o cartón grueso. La cara anterior está dividida en 9 cuadrados, salvo el superior, divididos en dos mitades por un trazo diagonal.

En la primera casilla de cada varilla se escribe el número, rellenando las siguientes con el duplo, triplo, cuádruplo y así sucesivamente hasta el nónuplo del número al que corresponda la varilla.

Los dígitos resultados del producto se escriben uno a cada lado de la diagonal y en aquellos casos en los que sea inferior a 10, se escriben en la casilla inferior, escribiendo en la superior un cero.

Un juego consta de 9 varillas correspondientes a los dígitos 1 a 9. En la figura se ha representado además la varilla 0, que realmente no es necesaria para los cálculos.

Provistos del conjunto descrito, supongamos que deseamos calcular el producto del número 46785399 por 7.

En el tablero colocaremos las varillas correspondientes al número, tal como muestra la figura, haciendo posteriormente la lectura del resultado en la faja horizontal correspondiente al 7 del casillero del tablero, operación que solo requiere sencillas sumas, naturalmente con acarreo de los dígitos situados en diagonal.

Comenzando por la derecha obtendremos las unidades (3), las decenas (6+3=9), las centenas (6+1=7), etc.

Si algún dígito del número que deseamos multiplicar fuera cero, bastaría dejar un hueco entre las varillas.

Supongamos que queremos multiplicar el número anterior por 96.431; operando análogamente al caso anterior obtendremos rápidamente los productos parciales del número por 9, 6, 4, 3 y 1, colocándolos correctamente y sumando, obtendremos el resultado total.

Igualmente podrían realizarse divisiones una vez conocidos los 9 productos parciales del dividendo; determinados éstos mediante el ábaco, basta seleccionar el inmediatamente inferior al resto sin necesidad de realizar los molestos tanteos que requieren las divisiones realizadas a mano.

En el ejemplo, para hacer la operación anterior, se sigue el método siguiente:

El resultado es por tanto el siguiente (como se puede ver en la tabla):

Como sabemos, para extraer una raíz cuadrada primeramente, debe agruparse los dígitos de dos en dos desde la coma, tanto hacia la derecha como la izquierda, quedando el número de la forma siguiente:

... xx xx xx xx, xx xx xx...

Por ejemplo: el número 458938,34 quedaría 45 89 38, 34.

Tomando el par (que podrá ser un solo dígito) de la izquierda (xx), se obtiene la cifra a entera tal que su cuadrado sea igual o menor que el par. Ésta será la primera cifra de la solución. Restando del par el cuadrado del entero así encontrado, obtenemos el resto:

Posteriormente, y de forma iterativa, se añade al resto el siguiente par, quedando un número de la forma yxx (y, el resto anterior, xx el par añadido) que llamaremos Ra. La siguiente cifra de la solución deberá ser tal que el cuadrado de la solución parcial ab (siendo ab un número de dos dígitos, no un producto) sea menor que xxxx (los dos primeros pares del radicando):

Despejando:

Operando de igual modo una vez conocidas las cifras ab, deberá determinarse la tercera cifra de la solución (c) y siguientes (d, e, ...) que, como fácilmente se puede demostrar operando análogamente al caso anterior, deberán cumplir:

Los productos indicados pueden obtenerse fácilmente con el ábaco de Napier, pero para ello es necesaria una varilla auxiliar tal que en cada faja horizontal recoja los cuadrados de los números correspondientes.

Conocida la primera cifra a, colocamos en el ábaco la (o las) varillas correspondientes al duplo de a. Hecho esto, bastará añadir la varilla de los cuadrados para encontrar el número tal que se cumpla la ecuación (I), que será el correspondiente a la faja b. Dicho número deberá sustraerse de Ra para encontrar Rb.

Encontrado b, retiramos la varilla auxiliar de los cuadrados y colocamos en el tablero la varilla correspondiente a 2·b; pueden darse dos casos, si b es menor que 5, el doble tendrá solo una cifra con la que bastará colocar la varilla; en caso contrario (igual o mayor que 5) el duplo será mayor de 10, por lo que será necesario incrementar la última varilla colocada en una unidad. 

Veámoslo con un ejemplo. Deseamos obtener la raíz cuadrada del número 46 78 53 99. Tomamos el primer par (46) y determinamos el cuadrado inmediatamente inferior, que resulta ser 36 (49 que es el siguiente es mayor que 46), de modo que la primera cifra de la solución es 6, y el resto: 46 - 6·6 = 46 - 36 = 10.

Colocamos las varillas de 6·2 = 12 en el tablero, y seguidamente la varilla auxiliar de los cuadrados. Componemos el resto y el siguiente par obteniendo el número 1078 que no deberá ser superado por el cuadrado de (6b). Leemos en el ábaco (1) el valor 1024, encontrando que b= 8 y el nuevo resto 1078 - 1024 = 54, descendiendo el siguiente par, obtenemos un valor de 545312.

Colocamos las varillas correspondientes al doble de 8; por ser 16 (>10), retiraremos la última varilla, la del 2, sustituyéndola por la del 3 (es decir, le sumamos una unidad) y añadimos la varilla del 6. El ábaco queda como se muestra en (2a). Como puede observarse, las cifras colocadas son las correspondientes al doble de la solución encontrada hasta el momento (68·2 = 136); es decir, el 2abc de las ecuaciones anteriores.

Hecho esto, volvemos a colocar la varilla auxiliar, y operando como en el caso anterior, obtenemos (2b) la tercera cifra: 3, siendo el resto 1364. Descendemos el siguiente par obteniendo un valor 136499, colocamos la varilla 6 (3·2) y encontramos el siguiente dígito 9 y el resto 13478. Mientras el resto sea distinto de cero se puede seguir obteniendo cifras significativas.

Por ejemplo, para obtener el primer decimal, bajaríamos el par 00 obteniendo el número 1347800 y colocaríamos las varillas del 9·2 = 18, quedando en el tablero las siguientes: 1-3-6-7(6+1)-8-auxiliar. Haciendo la comprobación, se obtiene el primer decimal = 9.

Durante el siglo XIX, el ábaco neperiano sufrió una transformación para facilitar la lectura. Las varillas comenzaron a fabricarse con una inclinación del orden de 65º, de modo que los triángulos que debían sumarse quedaran alineados verticalmente. En este caso, en cada casilla de la varilla se consigna la unidad a la derecha y la decena (o el cero) a la izquierda.

Las varillas estaban fabricadas de modo tal que el grabado vertical y horizontal era más visible que las juntas entre las varillas, facilitándose mucho la lectura al quedar el par de componentes de cada dígito del resultado en un rectángulo.

Así, en la figura se aprecia inmediatamente que:

Además del ábaco anterior, Napier construyó un ábaco de fichas. Ambos reunidos en un único aparato constituyen una joya histórica, única en Europa, que posee el Museo Arqueológico Nacional español.

El aparato es una magnífica caja de madera con incrustaciones de hueso. En la parte superior contiene el ábaco rabdológico, mientras que en la inferior se encuentra el segundo ábaco que consta de 300 fichas almacenadas en 30 cajones de las que 100 están cubiertas de cifras y doscientas muestran pequeños taladros triangulares que permiten ver únicamente ciertas cifras de las fichas de números cuando se superponen a aquellas, de forma tal que merced a la hábil colocación de unos y otros pueden realizarse multiplicaciones hasta el asombroso límite de un número de 100 cifras por otro de 200.

En las portezuelas de la caja se encuentran además las primeras potencias de los números dígitos, los coeficientes de los términos de las primeras potencias del binomio y los datos numéricos de los poliedros regulares.

Se desconoce quién fue el autor de esta riquísima joya, ni si es de autoría española o vino del extranjero, aunque es probable que originalmente perteneciera a la Academia de Matemáticas creada por Felipe II o que la trajese como regalo el Príncipe de Gales. Lo único que puede asegurarse es que se conservaba en Palacio, de donde pasó a la Biblioteca Nacional y posteriormente al Museo Arqueológico Nacional, donde aún se conserva.

En 1876, el gobierno español envió el aparato a la exposición de instrumentos científicos celebrada en Kensington, donde llamó extraordinariamente la atención, hasta el punto de que varias sociedades consultaron a la representación española acerca del origen y uso del aparato, lo que motivó que D. Felipe Picatoste escribiera una monografía que fue posteriormente enviada a todas las naciones, sorprendiendo el hecho de que el ábaco solo fuera conocido en Inglaterra, país de origen de su inventor.


</doc>
<doc id="7020" url="https://es.wikipedia.org/wiki?curid=7020" title="DOS">
DOS

DOS (sigla de "Disk Operating System", "Sistema Operativo de Disco" o "Sistema Operativo en Disco") es una familia de sistemas operativos para computadoras personales (PC). Creado originalmente para computadoras de la familia IBM PC, que utilizaban los procesadores Intel 8086 y 8088, de 16 bits, siendo el primer sistema operativo popular para esta plataforma. Contaba con una interfaz de línea de comando en modo texto o alfanumérico, vía su propio intérprete de órdenes, command.com. Probablemente la más popular de sus variantes sea la perteneciente a la familia MS-DOS, de Microsoft, suministrada con buena parte de los ordenadores compatibles con IBM PC, en especial aquellos de la familia Intel, como sistema operativo independiente o nativo, hasta la versión 6.22, frecuentemente adjunto a una versión de la interfaz gráfica de Windows de 16 bits, como las 3.1x.

En las versiones nativas de Microsoft Windows, basadas en NT (y este a su vez en OS/2 2.x) (véase Windows NT, 2000, 2003, XP o Vista o Windows 7) MS-DOS desaparece como sistema operativo (propiamente dicho) y entorno base, desde el que se arrancaba el equipo y sus procesos básicos y se procedía a ejecutar y cargar la interfaz gráfica o entorno operativo de Windows. Todo vestigio del mismo queda relegado, en tales versiones, a la existencia de un simple intérprete de comandos, denominado símbolo del sistema, ejecutado como aplicación mediante cmd.exe, a partir del propio entorno gráfico (elevado ahora a la categoría de sistema).

Esto no es así en las versiones no nativas de Windows, que sí están basadas en MS-DOS, cargándose a partir del mismo. Desde los 1.0x a las versiones 3.1(1), de 16 bits, Ms Windows tuvo el planteamiento de una simple aplicación de interfaz o entorno gráfico, complementaria al propio intérprete de comandos, desde el que era ejecutado. Fue a partir de las versiones de 32 bits, de nuevo diseño y mayor potencia, basadas en Windows 95 y 98, cuando el MS-DOS comienza a ser deliberadamente camuflado por el propio entorno gráfico de Windows, durante el proceso de arranque, dando paso, por defecto, a su automática ejecución, lo que acapara la atención del usuario medio y atribuye al antiguo sistema un papel más dependiente y secundario, llegando a ser por muchos olvidado y desconocido, y paulatinamente abandonado por los desarrolladores de software y hardware, empezando por la propia Microsoft (esta opción puede desactivarse alterando la entrada BootGUI=1 por BootGUI=0, del archivo de sistema, ahora de texto, MSDOS. SYS). Sin embargo, en tales versiones, Windows no funcionaba de forma autónoma, como sistema operativo. Tanto varias de las funciones primarias o básicas del sistema como su arranque se deben aún en las versiones de 32 bits, a los distintos módulos y archivos de sistema que componían el modesto armazón del DOS, requiriendo aquellas un mínimo de los archivos básicos de este, para poder ejecutarse (tales como IO.SYS, DRVSPACE. BIN, EMM386.EXE e HIMEM. SYS).

Existen varias versiones de DOS:

Con la aparición de los sistemas operativos con interfaz gráfica de usuario (GUI), del tipo Windows, en especial aquellos de 32 bits, del tipo Windows 95, el DOS ha ido quedando relegado a un segundo plano, hasta verse reducido al mero intérprete de órdenes, y a las líneas de comandos (en especial en ficheros de tipo .PIF y .BAT), como ocurre en los sistemas derivados de Windows NT.

La historia comienza en 1981 con la compra, por parte de Microsoft, del sistema operativo QDOS ("Quick Disk Operating System"), que tras realizarle pocas modificaciones, se convierte en la primera versión del sistema operativo de Microsoft, denominado MS-DOS 1.0 ("MicroSoft Disk Operating System"). 

A partir de aquí, suceden una serie de modificaciones del sistema operativo, hasta llegar a la versión 7.1, a partir de la cual MS-DOS deja de existir como tal y se convierte en una parte integrada del sistema operativo Microsoft Windows.

En 1982, aparece la versión 1.25 que se añade soporte para disquetes de doble cara.

En 1983, el sistema comienza a tener más funcionalidad, con su versión 2.0, que añade soporte a discos duros IBM de 10 MB, y la posibilidad de lectura-escritura de disquetes de 5¼" con capacidad de 360 Kb. En la versión 2.11 del mismo año, se añaden nuevos caracteres de teclado.

En 1984, Microsoft lanzaría su versión MS-DOS 3.0, y es entonces cuando se añade soporte para discos de alta densidad de 1,2 MB y posibilidad de instalar un disco duro con un máximo de 32 MB.
En ese mismo año, se añadiría en la versión 3.1 el soporte para redes Microsoft.
Tres años más tarde, en 1987, se lanza la versión 3.3 con soporte para los disquetes de 3½", y se permite utilizar discos duros mayores de 32 MB.

En 1988, Microsoft saca al mercado su versión 4.0 y con ella el soporte para especificación de memoria extendida (XMS) y la posibilidad de incluir discos duros de hasta 2 GB, cabe destacar que esta versión fue la mayor catástrofe realizada por la empresa, porque estaba llena de "bugs", fallos, etcétera, que arreglaron en 1989 con el lanzamiento de la versión 4.01 que solucionaba todos estos problemas y fallos.

En 1991, uno de los avances más relevantes de la historia de MS-DOS, es el paso de la versión 4.01 a la versión 5.0, en la que DOS, es capaz ya de cargar programas en la parte de la memoria alta del sistema utilizando la memoria superior (de los 640 Kb a los 1024 Kb). En la versión 5.0 se añade el programador BASIC y el famoso editor EDIT. También se añadieron las utilidades UNDELETE (recuperación de ficheros borrados), FDISK (administración de particiones) y una utilidad para hacer funcionar los programas diseñados para versiones anteriores de MS-DOS, llamada SETVER. A finales de 1992 se resuelven unos problemas con UNDELETE y CHKDSK en la versión 5.0a.

En 1993, aparece MS-DOS 6.0 con muchas novedades, entre ellas la utilidad "Doublespace" que se encargaba de comprimir el disco y así tener más espacio disponible, también se incluyó un antivirus básico (MSAV), un defragmentador (DEFRAG), un administrador de memoria (MEMMAKER) y se suprimieron ciertas utilidades antiguas, que haciendo un mal uso de ellas podían destruir datos, estas utilidades eran JOIN y RECOVER, entre otras.
En el mismo año sale la versión 6.2 que añade seguridad a la pérdida de datos de "Doublespace", y añade un nuevo escáner de discos, SCANDISK, y soluciona problemas con DISKCOPY y SmartDrive. En la versión 6.21 aparecida en 1993, Microsoft suprime Doublespace y busca una nueva alternativa para esta utilidad.

En 1994, aparece la solución al problema de "Doublespace", es la utilidad de la compañía Stac Electronics, Drivespace, la elegida para incluirse en la versión 6.22.

En 1995 aparece Microsoft Windows 95, y que con la aparición del mismo, supone apartar a MS-DOS a un plano secundario.

El sistema MS-DOS no obstante sigue siendo en 1995 una nueva versión, la 7.0, con la que se corrigen multitud de utilidades y proporciona soporte para nombres largos. Las utilidades borradas del anterior sistema operativo las podemos encontrar en el directorio del CD de windows 95 \other\oldmsdos.

En 1997 aparece Windows 95 OSR2, y con él una revisión exhaustiva del sistema DOS, añadiendo el soporte para particiones FAT32. A partir de entonces, MS-DOS deja de existir como sistema operativo.

Fueron varias las compañías que desarrollaron versiones del DOS, generalmente muy similares entre sí. PC-DOS y MS-DOS, por ejemplo, empezaron siendo prácticamente idénticos, aunque acabaron siendo muy distintos. Las versiones más conocidas son QDOS, PC-DOS, MS-DOS y FreeDOS, entre otras.

Con el sistema operativo GNU/Linux es posible ejecutar copias de DOS bajo , una máquina virtual nativa de GNU/Linux para ejecutar programas en modo real. Hay otros muchos emuladores para diferentes versiones de UNIX, incluso para plataformas diferentes a la arquitectura de procesador x86.

El DOS carece por completo de interfaz gráfica, y no utiliza el ratón, aunque a partir de ciertas versiones solía incluir controladoras para detectarlo, inicializarlo y hacerlo funcionar bajo diversas aplicaciones de edición y de interfaz y entorno gráfico, además de diversos juegos que tendían a requerirlo (como juegos de estrategia, aventuras gráficas y Shoot 'em up subjetivos, entre otros). Por sí solo es incapaz de detectar el hardware, a menos que las mencionadas controladoras incluyan en su núcleo de sistema, como residentes en memoria, el código, instrucciones y funciones necesarias. En cualquier caso, el intérprete de comandos y la mayoría de sus aplicaciones y mandatos de edición debían o podían ser fácilmente controlados manualmente, a través del teclado, ya fuera mediante comandos, o introduciendo teclas de acceso rápido para activar los distintos menús y opciones desde el editor (un buen ejemplo de esto último son el editor de texto edit.com, el menú de ayuda help.exe, o el intérprete de BASIC qbasic.exe, incluidos en las últimas versiones del MS-DOS). Tales opciones siguen, de hecho, encontrándose presentes en los Windows, en versiones muy posteriores.

El DOS no es ni multiusuario ni multitarea. No puede trabajar con más de un usuario ni en más de un proceso a la vez. En sus versiones nativas (hasta la 6.22 en el MS-DOS), no puede trabajar con particiones de disco demasiado grandes, superiores a los 2 GB, que requieren formatos y sistemas de archivos tales como el FAT32, propio de Windows de 32 bits (a partir del 98), o el NTFS, propio de Windows de tipo NT. Originalmente, por limitaciones del software, no podía manejar más de 64KB de memoria RAM. En las versiones anteriores a la 4.0, el límite, a su vez, era de 32 MB por partición, al no soportar aún el formato FAT16 (desarrollado en 1987). Poco a poco, con las mejoras en la arquitectura de los PC, llegó primero a manejar hasta 640 KB de RAM (la llamada "memoria convencional", o base), y luego hasta 1 megabyte (agregando a la memoria convencional la "memoria superior" o UMB). Más tarde, aparecieron mecanismos como la memoria expandida (EMS) y la memoria extendida (XMS), que permitían ya manejar varios megabytes.

Desde el punto de vista de los programadores, este sistema operativo permitía un control total de la computadora, libre de las capas de abstracción y medidas de seguridad a las que obligan los sistemas multiusuario y multitarea. Así, hasta la aparición del DirectX, y con el fin de aprovechar al máximo el hardware, la mayoría de videojuegos para PC funcionaban directamente bajo DOS.

La necesidad de mantener la compatibilidad con programas antiguos, hacía cada vez más difícil programar para DOS, debido a que la memoria estaba segmentada, es decir, la memoria apuntada por un puntero tenía como máximo el tamaño de un segmento de 64KB. Para superar estas limitaciones del modo real de los procesadores x86, se recurría al modo protegido de los procesadores posteriores (80386, 80486...), utilizando programas extensores que hacían funcionar programas de 32 bits sobre DOS.

Aunque este sistema operativo sea uno de los más antiguos, aún los entornos operativos Windows de 32 bits, hasta el 98, tenían como plataforma base "camuflada" u oculta el DOS. Su intérprete de comandos, denominado, por lo general, "Command Prompt" o Símbolo del Sistema, puede invocarse desde la interfaz como command.com, ó, en versiones posteriores, basadas en NT, que ya no se basan ni parten de MS-DOS, mediante cmd.exe, esto pasa también en Windows ME a pesar de estar aún basado en la antigua arquitectura 9x. También existen, para sistemas actuales, emuladores como DOSBox, o entornos de código abierto como el FreeDOS, comunes ambos en GNU/Linux; ello permite recuperar la compatibilidad perdida con ciertas aplicaciones nativas para este antiguo sistema, que ya no pueden funcionar desde los nuevos Windows, basados en NT, o bajo sistemas operativos de arquitectura dispar, como los UNIX y GNU/Linux.


Algunas de estas órdenes admiten el uso de parámetros, también llamados modificadores.

Modificadores de la orden codice_1:

Los modificadores pueden combinarse, por ejemplo:

Ciertas órdenes, como codice_3, pueden recibir parámetros que permiten una manipulación de archivos ciertamente avanzada, en particular el modificador codice_48, que efectúa una copia binaria. Por ejemplo, la secuencia:
codice_49: copiará el contenido de archivo1, archivo2 y archivo3 en un nuevo archivo, denominado archivo4.

Además, el DOS permitía escribir archivos de proceso por lotes (pequeños scripts para COMMAND.COM), cuya extensión era .BAT, que admitían órdenes como codice_50, codice_51 y codice_52 (que pedía la entrada de un carácter entre los especificados). Así, se podían hacer menús, automatizar tareas, etcétera.




</doc>
<doc id="7021" url="https://es.wikipedia.org/wiki?curid=7021" title="IBM PC DOS">
IBM PC DOS

El IBM PC DOS (nombre completo: The IBM Personal Computer Disk Operating System) es un Sistema operativo de disco (DOS) para el IBM Personal Computer y los sistemas compatibles. Fue uno de los sistemas operativos que dominó el mercado de los computadores personales entre 1985 y 1995. Manufacturado y vendido por IBM desde el año 1981 al 2000.

Tiene las mismas raíces que el MS-DOS. De hecho, el MS DOS y el PC DOS son dos variantes del mismo sistema operativo con algunas diferencias. Mientras que el PC DOS fue hecho originalmente para los computadores personales de IBM, el MS DOS apuntaba al mercado de los clones.

El grupo de trabajo de IBM reunido para desarrollar el IBM PC decidió que los componentes críticos de la máquina, incluyendo el sistema operativo, pudieran venir de vendedores externos. Esta ruptura radical de la tradición de la compañía, de desarrollo interno, fue la decisión clave que hizo el IBM PC un estándar industrial, pero esto fue hecho por necesidad para ahorrar tiempo. Microsoft fue seleccionado para el sistema operativo. IBM quería que Microsoft conservara la propiedad de cualquier software que desarrollara, y no quería tener nada que ver en ayudar a Microsoft, con excepción de hacer sugerencias desde lejos. Según el miembro del grupo de trabajo Jack Sams, "Las razones eran internas. Teníamos un terrible problema siendo demandados por gente clamando que habíamos robado sus cosas. Podría ser horriblemente costoso para nosotros tener nuestros programadores mirando el código que perteneció a algún otro, porque entonces ellos podrían volver y decir que nosotros les robamos e hicimos todo este dinero. Habíamos perdido una serie de disputas legales en esto, así que no quisimos tener, trabajado por la gente de IBM, un producto que fuera claramente de algún otro. Fuimos a Microsoft con la proposición que queríamos que éste fuera su producto". IBM primero entró en contacto con Microsoft para ver la compañía por julio de 1980. Las negociaciones continuaron durante los meses siguientes, y el papeleo fue oficialmente firmado a principios de noviembre.

Microsoft compró una licencia no exclusiva para el 86-DOS (anteriormente llamado QDOS) a Seattle Computer Products (SCP) en diciembre de 1980 por 25.000 dólares. En mayo de 1981, se contrató a Tim Paterson para portar QDOS al IBM-PC, que utilizaba el procesador Intel 8088, que era más lento y menos costoso, y que tenía su propia familia específica de periféricos. IBM observó los progresos diariamente y presentó más de 300 peticiones de cambio antes de aceptar el producto y escribir el manual de usuario para él.

En julio de 1981, un mes antes de que lanzaran el IBM PC, Microsoft compró todos los derechos del 86-DOS de SCP por 50.000 dólares. Esto cumplió los criterios principales de IBM: Parecía CP/M y era fácil adaptar los programas de 8 bits existentes de CP/M para funcionar bajo este, sobre todo gracias al comando TRANS del QDOS, que permitía traducir código fuente del Intel 8080 al lenguaje de máquina del 8086.

Microsoft licenció QDOS a IBM, y se convirtió en el PC-DOS 1.0. Esta licencia también permitió que Microsoft vendiera el DOS a otras compañías, lo cual hizo posteriormente cuando aparecieron los clones llamándolo MS DOS. El acuerdo fue espectacularmente exitoso, y SCP demandó posteriormente en los juzgados que Microsoft había encubierto su relación con IBM para comprar el sistema operativo más barato (incluso aunque Microsoft todavía estaba bajo los términos de un acuerdo de no revelación y el grado de éxito del PC no estaba previsto ampliamente). SCP recibió en última instancia un millón de dólares como acuerdo de pago.

El PC DOS estaba formado por cuatro componentes principales:

Adicionalmente hay una serie de programas ejecutables, algunos archivos de configuración, y otros. 

El hermano del PC DOS, el MS DOS, tenía los archivos IO.SYS y DOS.SYS que eran los equivalentes respectivamente del IBMBIO.COM y el IBMDOS.COM.

El IBMBIO.COM era el nombre de archivo del DOS-BIOS en muchos sistemas operativos DOS, y como tal, parte de PC-DOS, versiones anteriores del MS-DOS, y DR DOS 5.0 y posteriores (a excepción del DR-DOS 7.06). Sirve el mismo propósito que el IO.SYS en MS-DOS, o el DRBIOS.SYS en DR DOS 3.31 al 3.41.

El archivo residía en el sector de arranque del disco (el primer sector) y era cargado por el boot loader después de ejecutarse el POST al encender el computador.

En la secuencia del bootup del PC, es cargado en memoria el primer sector del disco de arranque y se ejecuta el código almacenado allí. Si este es el sector de arranque del DOS, este carga los primeros tres sectores del IBMBIO.COM en la memoria y transfiere el control a este. El IBMBIO.COM entonces realiza lo siguiente:


Bajo el DR-DOS, se salta el primer paso, puesto que un sector de arranque del DR-DOS monta el sistema de archivos FAT, localiza el archivo IBMBIO.COM (o DRBIOS.SYS) en el directorio raíz y lo carga en memoria por sí mismo. No es necesario que el archivo IBMBIO.COM resida en una posición física fija o sea almacenado en sectores consecutivos. En lugar de ello, simplemente puede ser copiado al disco (sin el SYS), dado un sector de arranque de DR-DOS ya resida en el disco.

El IBMDOS.COM era el nombre de archivo del kernel del PC DOS. El archivo estaba situado en el directorio raíz en el disco de sistema del sistema operativo PC-DOS. Cuando Microsoft lanzó el MS-DOS, este tenía un archivo equivalente llamado MSDOS.SYS. Posteriormente, en el sistema operativo DR-DOS había también un IBMDOS.COM.

El kernel inicializaba al sistema operativo e interpretaba el contenido del archivo CONFIG.SYS, que también debía estar situado en el directorio raíz. Un comando en el CONFIG.SYS especificaba la localización del interpretador de línea de comandos, típicamente el COMMAND.COM.

El PC-DOS tenía una serie de funciones que podían ser llamadas por los programas por medio de interrupciones. Había funciones para entrada por teclado, salida por pantalla, entrada y salida por consola (la cual era la combinación del teclado y la pantalla tratados en conjunto), entrada y salida por el puerto serial, manejo de memoria, manejo de archivos, manejo de directorios, manejo del disco, fecha y hora, etc.

El interpretador de comandos para el PC-DOS y el MS-DOS corre después de que finaliza la aplicación que se está ejecutando (o después de que un programa TSR devuelve el control después de instalarse). Si después de que la aplicación finalice o devuelva el control, el interpretador de comandos residente en memoria hubiera sido sobreescrito, el PC-DOS lo recargará desde el disco de nuevo. El interpretador de comandos es almacenado usualmente en un archivo llamado COMMAND.COM.

Algunos comandos son internos y están construidos dentro del COMMAND.COM, otros están almacenados en el disco de la misma forma que los programas de aplicación. Cuando el usuario teclea una línea de texto en el prompt de comandos del sistema operativo, el COMMAND.COM parseará la línea, e intentará encontrar un nombre de comando construido internamente. Si no lo encuentra, entonces busca un archivo de programa ejecutable o un archivo batch en el disco con el nombre del comando. Si en cualquiera de los dos casos lo encuentra lo ejecuta y le pasa los parámetros que hubieran en la línea de texto escrita por el usuario. Si no se encuentra, un mensaje de error es impreso y el prompt de comando es refrescado de nuevo.

Los comandos residentes variaron levemente entre las diferentes versiones del PC-DOS. Típicamente, las funciones DIR (lista directorio), el ERASE o DEL (borra un archivo o un directorio), COPY (copia archivos), DATE (exhibe o ajusta la fecha), TIME (exhibe o ajusta la hora), CD (cambia el directorio de trabajo), MD (hacer un directorio en el disco actual), REN (renombrar un archivo o un directorio) y algunos otros, eran residentes en COMMAND.COM.

Los comandos transitorios eran, o demasiado grandes para mantenerse en el procesador de comandos, o eran usados con menos frecuencia. Tales programas utilitarios serían almacenados en el disco y cargados justo como los programas de aplicación regulares, pero eran distribuidos con el sistema operativo. Las copias de estos programas de comando utilitarios tenían que estar en un disco accesible, en la unidad de disco floppy actual o en la ruta de comandos fijada en el interpretador de comandos.

El archivo CONFIG.SYS es el principal archivo de configuración del PC DOS. Contiene instrucciones de configuración y de inicialización del sistema.

Los archivos con la extensión .BAT son archivos de procesamiento por lotes que contienen un conjunto de comandos que son procesados como si se entraran en la línea de comandos por el usuario. Sirven para automatizar la ejecución de una serie de comandos. Algunas palabras claves adicionales son reconocidas por el interpretador de comandos COMMAND.COM para hacer a los archivos batch más útiles. Estos comandos adicionales no son útiles si están mecanografiados interactivamente en el prompt de comandos, pero permiten un procesamiento flexible en un archivo bach.

El archivo Autoexec.bat es un archivo de procesamiento por lotes que se encuentra en el directorio raíz del disco de arranque y se ejecuta al iniciar el computador después de que el DOS fuera cargado y el CONFIG.SYS procesado.

Microsoft primero licenció, luego compró el 86-DOS de Seattle Computer Products (SCP), el cual fue modificado para el IBM PC por el empleado de Microsoft Bob O'Rear con asistencia de Tim Paterson de SCP y luego empleado de Microsoft. O'Rear consiguió que el 86-DOS corriera en el prototipo del IBM PC en febrero de 1981. El 86-DOS tuvo que ser convertido de los discos floppy de 8 pulgadas a los de 5,25 pulgadas, y ser integrado con el BIOS que Microsoft estaba ayudando a IBM a escribir. IBM tenía más gente escribiendo requisitos para la computadora que los que tenía Microsoft escribiendo código. O'Rear a menudo se sentía abrumado por el número de personas con las que tuvo que tratar en el Entry Level Systems facility en Boca Raton. Al 86-DOS se le cambió el nombre por PC DOS 1.0 para su lanzamiento con el IBM PC, en agosto de 1981. Hacia finales de 1981, Paterson fue a trabajar en una mejora, que fue llamada PC DOS 1.1. Ésta permitía que los datos fueran escritos en ambos lados de un disquete, doblando así la capacidad de la máquina de IBM, y fue finalizado en marzo de 1982.

Posteriormente, un grupo de programadores de Microsoft (principalmente Paul Allen, Mark Zbikowski y Aaron Reynolds)

comenzaron el trabajo en el PC DOS 2.0, la siguiente versión para el IBM PC/XT, el primer PC en almacenar los datos en un disco duro. Era un programa mucho más sofisticado que la versión 1.0, tenía 20.000 líneas de código en lenguaje ensamblador, comparado con cerca de 4.000 líneas de la primera versión. Fue oficialmente anunciado en marzo de 1983 o a fines de 1984. Luego, en marzo de 1984, se despachó el IBM PCjr. Corría el PC DOS 2.1, que soportaba la capacidad del PCjr de correr programas desde cartuchos de ROM y una arquitectura de controlador de disco ligeramente diferente.

En agosto de 1984, IBM introdujo el IBM PC/AT, un computador construido alrededor del procesador 80286 de Intel. Corría sobre el PC DOS 3.0, que soportaba las unidades de disco más grandes y los disquetes de más alta densidad (1,2 MB) del nuevo computador. El PC DOS 3.1 soportaba la tarjeta adaptadora de red de IBM en el IBM PC-Network. El PC DOS 3.2 añade soporte para las unidades de disco floppy de doble densidad de 720 KiB y 3½ pulgadas, soportando el IBM PC Convertible, el primer computador de IBM en usar discos floppy de 3½ pulgadas, lanzado en abril de 1986.

En junio de 1985, IBM y Microsoft firmaron un acuerdo de desarrollo conjunto de largo plazo para compartir código de DOS especificado y crear un nuevo sistema operativo desde cero, conocido en ese entonces como Advanced DOS (DOS avanzado). El 2 de abril de 1987, el OS/2 fue anunciado como el primer producto producido bajo los términos del acuerdo.

Al mismo tiempo, IBM lanzó su siguiente generación de computadores personales, el IBM Personal System/2. El PC DOS 3.3, lanzado con la línea PS/2, agregó soporte para unidades de disco floppy de "1,44 MB" (con capacidad de 1440 KiB) de alta densidad de 3½ pulgadas, que IBM introducido en sus modelos de PS/2 basados en el 80286 y más avanzados. La mejora del DOS 3.2 a 3.3 fue escrita completamente por IBM, sin esfuerzo de desarrollo de parte de Microsoft, el cual trabajaba en el "Advanced DOS 1.0".

El PC DOS 4.0, despachado en julio de 1988, fue un DOS sin éxito que presentó IBM probando ideas para su DOS 5, que estaba en desarrollo, y más tarde se convirtió en el OS/2.

Digital Research lanzó un DOS 5.0, que cogió a Microsoft por sorpresa, pero la combinación de vaporware, y alguna codificación apresurada, permitió a Microsoft evitar la competencia. Este DOS también fue el último DOS en el que IBM y Microsoft compartieron el código completo, y el DOS que fue integrado en el Virtual DOS Machine del OS/2 2.0, y posteriormente del Windows NT. El DOS en estos sistemas operativos para el computador i386 nunca progresó más allá de esto.

Bajo los términos de la división, a IBM se le permitió quedarse (y comprar los derechos) para su propio DOS, lo cual hicieron. También les permitieron quedarse con Win-OS/2 (básicamente Windows 3.10 para OS/2). Microsoft fue algo específico en cuál DOS era, puesto que los disquetes OEM fueron etiquetados "MS-DOS y herramientas adicionales", es decir dos productos. IBM lanzó su propio DOS, con un nuevo editor, y un número de utilidades que eran versiones anteriores?? completas de PC Tools. Las herramientas de Microsoft eran herramientas de Norton con características limitadas.

El PC DOS permaneció como una versión del MS DOS hasta 1993. IBM y Microsoft se separaron - MS DOS 6 fue lanzado en marzo y el PC DOS 6.1 (desarrollado por separado) lo siguió en junio. El QBasic fue eliminado y el MS DOS Editor fue reemplazado por E.

El PC DOS 6.3 siguió en diciembre. El PC DOS 6.30 también fue usado en el OS/2 para el Power PC.

La división final vino después del DOS 6.30. Se nota que el 6.30 tiene las mejoras que tuvo el 6.20, y que comenzando con el 6.22 y Windows 3.11, el sistema operativo preferido cambió del OS/2 a Windows NT.

El PC DOS 7.0 fue lanzado en noviembre de 1994. El lenguaje de programación REXX fue añadido, al igual que un soporte para un nuevo formato de disco floppy, el XDF, que extiende el estándar de disco floppy de 1,44 MB a 1.86 MB.

El DOS 7.0 de IBM, el último lanzamiento antes de que Boca Ratón cerrara, incluía características SAA (como el REXX, vista IPF para la ayuda, y unpack2 - todo viniendo del OS/2), junto con la remoción de la versión incorrecta del DOS de la mayoría, pero no todas, las utilidades.

El PC DOS 2000 - lanzado en Austin en 1998 - es básicamente un slipstream del 7.0 con el año 2k y otro arreglos aplicados. Para las aplicaciones, el PC DOS 2000 se reportaba como "IBM PC DOS 7.00, revisión 1", en contraste con el PC DOS 7.0 original, que se reportaba como la revisión 0. IBM continúa utilizando el código del PC DOS para compilar los discos de arranque de DOS para sus servidores.

El lanzamiento al por menor más reciente fue el PC DOS 2000, que encontró su nicho en el mercado de programas empotrados y otros lados. Fue basado en el PC DOS 7.0, y corrigió cuestiones con el problema del año 2000. El mercadeo del PC DOS 2000 incluyó la frase "incluye el PC DOS 7.0".

Los productos ThinkPad actualmente tienen una copia de la última versión del PC DOS en su partición de rescate y recuperación.

Desde 2003, hay también una versión OEM del PC DOS que tiene activada el LBA/FAT32, reportándose a sí misma a las aplicaciones como "IBM PC DOS 7.10". No debe ser confundida con el OEM DR-DOS 7.04 y superiores, que también se reportan como "IBM DOS 7.10" para propósitos de compatibilidad.

El PC DOS seguía siendo un cambio de marca de la versión del MS-DOS hasta 1993. IBM y Microsoft se separaron - El MS-DOS 6 fue lanzado en marzo, y el PC DOS 6.1 (desarrollado separadamente) lo siguió en junio. El QBasic fue retirado y el MS-DOS Editor fue reemplazado con E. El PC DOS 6.3 siguió en diciembre.

El PC DOS 7.0 fue lanzado en noviembre de 1994. El lenguaje de programación REXX fue agregado, así como el soporte para un nuevo formato de disco floppy, XDF, que extendió un disco floppy estándar de 1440 KiB a 1860 KiB.

El lanzamiento al por menor más reciente fue el PC DOS 2000, que encontró su nicho en el mercado de programas empotrados y otros lados. Fue basado en el PC DOS 7.0, y corrigió cuestiones con el problema del año 2000. El mercadeo del PC DOS 2000 incluyó la frase "incluye el PC DOS 7.0".

Los productos ThinkPad actualmente tienen una copia de la última versión del PC DOS en su partición de rescate y recuperación.

Desde 2003, hay también una versión OEM del PC DOS que tiene activada el LBA/FAT32, reportándose a sí misma a las aplicaciones como "IBM PC DOS 7.10". No debe ser confundida con el OEM DR-DOS 7.04 y superiores, que también se reportan como "IBM DOS 7.10" para propósitos de compatibilidad.







</doc>
<doc id="7022" url="https://es.wikipedia.org/wiki?curid=7022" title="DR-DOS">
DR-DOS

El DR-DOS es un sistema operativo compatible con el MS-DOS para computadoras personales compatibles con el IBM PC. Fue desarrollado originalmente por Digital Research de Gary Kildall y derivado del Concurrent PC DOS 6.0, el cual a su vez era un sucesor avanzado del CP/M-86. Debido a que cambió varias veces de dueño, se produjeron varias versiones posteriores como Novell DOS, Caldera OpenDOS, etc.

El original CP/M de Digital Research para sistemas de 8 bits basados en los procesadores Intel 8080 y Z-80 engendró varias versiones derivadas, la más notable CP/M-86 para la familia de procesadores Intel 8086/8088. Aunque CP/M había dominado el mercado, en 1981 la IBM PC produjo un cambio masivo.

IBM se acercó originalmente a Digital Research, buscando una versión x86 de CP/M. Sin embargo, por desacuerdos por el contrato IBM desechó el trato y firmó con Microsoft, que compró a Seattle Computer Products su sistema operativo 86-DOS para convertirlo en MS-DOS y IBM PC DOS. La estructura de las instrucciones y el API de 86-DOS imitaban a CP/M así que Digital Research demandó. IBM acordó vender CP/M-86 a la vez que PC DOS. Sin embargo, mientras vendía PC DOS en $40 dólares, CP/M-86 lo vendía en $240.

Digital Research trató de promover CP/M-86 y su sucesor multitareas multiusuario Concurrent CP/M-86 pero eventualmente dieron la batalla por perdida y modificaron Concurrent CP/M-86 para que permitiera correr las mismas aplicaciones que MS-DOS y PC DOS.

Inicialmente Digital Research desarrolló DOS Plus como una versión recortada y monousuario de Concurrent DOS pero resultó tener mal desempeño. Digital Research hizo un segundo intento esta vez creando un sistema nativo. El nuevo sistema operativo fue lanzado en 1988 como DR DOS.

Puesto que Digital Research no podía competir con el predominio de MS-DOS, decidió modificar su sistema operativo para que fuera compatible con el de Microsoft, y así, en 1988, nació DR DOS 3.31, compatible con Compaq MS-DOS 3.31. En aquel momento, MS-DOS sólo se vendía preinstalado, y DR-DOS trató de competir por dos frentes: por un lado, salió a la venta en tiendas; por otro, ofreció a los fabricantes licencias más baratas.

La versión más importante de DR DOS fue la versión 5.0, en 1990. Lanzada para competir con el MS-DOS 4.x, incluía un administrador de archivos gráfico (ViewMAX), y la capacidad de cargar el sistema en memoria alta en ordenadores con procesador 286 y cargar los dispositivos en bloques UMB, algo muy útil para los usuarios que cada vez tenían que manejar más hardware pero seguían limitados a 640 KB de memoria convencional, que a veces quedaban limitadas a 400 KB tras instalar los controladores. Estas características sólo eran ofrecidas, hasta aquel momento, por aplicaciones como QEMM, y no por sistemas operativos.

El mismo mes en que apareció DR-DOS 5.0, se anunció la aparición de MS-DOS 5.0, que al final se retrasaría hasta el año siguiente. El sistema de Microsoft presentaba las mismas capacidades de manejo de memoria que DR-DOS 5.0, pero la sintaxis de sus comandos no era totalmente compatible (por ejemplo, DR-DOS usaba XDEL para lo que en MS-DOS 5 sería DELTREE y en Windows NT es DEL /S).

Digital Research respondió con DR-DOS 6.0 en 1991. Sus principales características eran el compresor de disco SuperStor (en aquella época eran habituales los discos duros de 40 MB) y la capacidad multitarea proporcionada por TaskMax. Si bien inferior a aplicaciones como DesqView, el introducir multitarea suponía una importante mejora respecto de MS-DOS.

Como respuesta, Microsoft incluiría utilidades de terceros, tales como un compresor de archivos (DoubleSpace, luego llamado DriveSpace por problemas legales), en su MS-DOS 6.0.

Digital Research fue comprada por Novell en su estrategia de competir con Microsoft. Como resultado de ello, apareció Novell DOS 7.0, cuya principal ventaja sobre MS-DOS era ofrecer una versión personal del sistema de red Novell, sistema que comenzaba a perder popularidad a causa de la aparición de Windows para Trabajo en Grupo. Finalmente, DR-DOS fue vendido a Caldera en 1996. Posiblemente el principal interés de Caldera en el producto era una antigua demanda contra Microsoft por competencia desleal, ya que aunque el producto era altamente compatible a nivel binario con MS-DOS, Microsoft se esforzó en introducir código en Windows específicamente para hacerlo incompatible con DR-DOS.

El DR-DOS 7.01 de Caldera fue distribuido como freeware para uso no comercial, incluyendo el código fuente, con el nombre de OpenDOS, pero en la versión 7.02 se volvió a un modelo completamente cerrado.

En 2002, la división de Caldera dedicada a DR-DOS (Caldera Thinclients, luego Lineo), tras sacar la versión 7.03 en 1999, decidió centrarse en Linux y vendió DR-DOS a la empresa DeviceLogics, que en 2004 produjo DR-DOS 8.0. Los DR-DOS de Lineo y DeviceLogics se han licenciado habitualmente para su uso en sistemas integrados o para utilidades que necesitan usar un disco de arranque (por ejemplo, las utilidades de disco de Seagate).

Mientras tanto, el proyecto DR-DOS enhancement ha tratado de crear un sistema operativo a partir de la fuente abierta del DR-DOS 7.01.



</doc>
<doc id="7023" url="https://es.wikipedia.org/wiki?curid=7023" title="FreeDOS">
FreeDOS

FreeDOS (anteriormente Free-DOS y PD-DOS) es un sistema operativo libre para computadoras compatible IBM PC. Pretende proporcionar un entorno completo compatible con DOS para la ejecución de software heredado y el soporte de sistemas integrados.

FreeDOS puede ser arrancado desde una memoria USB. Está diseñado para funcionar bajo virtualización o emulación x86.

A diferencia de la mayoría de las versiones de MS-DOS, FreeDOS está compuesto por software libre y de código abierto, licenciado bajo los términos de la GNU General Public License. Por lo tanto, su distribución básica no requiere derechos de licencia o derechos de autor y se permite la creación de distribuciones personalizadas. Sin embargo, otros paquetes que forman parte del proyecto FreeDOS incluyen software no GPL que se considera conveniente preservar, como el 4DOS, que se distribuye bajo licencia MIT modificada.

El físico Jim Hall, egresado de la Universidad de Wisconsin-River Falls, tuvo en su juventud un clon de Apple II que inicialmente le sirvió de campo de juegos electrónicos para luego pasar a la etapa de programación al aprender por sí mismo el lenguaje Applesoft BASIC. Transcurrido el tiempo el relevo del equipo fue una IBM PC la cual también tenía una versión de BASIC al cual migró sus programas sin mayor problema durante sus estudios en el liceo. Durante su pregrado en la universidad aprendió lenguaje C y programó bajo el ambiente MS-DOS. Aunque en esa casa de estudio utilizaban UNIX, Jim Hall siguió utilizando en sus ordenadores personales el MS-DOS para sus trabajos académicos. En 1993 descubre que GNU/Linux es compatible con sus computadores y lo instaló en arranque doble, reconoce la potencialidad del nuevo sistema operativo sin embargo dada la gran cantidad de programas utilitarios -y de juegos- decide quedarse con MS-DOS.
En 1994, tras haber probado Windows 3.1, y ante los anuncios de prensa que se irían solamente en entorno gráfico y la empresa Microsoft abandonaría el desarrollo y soporte de futuras versiones de MS-DOS, Jim Hall decide publicar el anuncio de su proyecto el 29 de junio en la página web comp.os.msdos.apps. Confundido con los conceptos de software libre y dominio público decide colocarle el nombre "PD-DOS" (Public Domain-Disk Operating System por sus iniciales en idioma inglés) y es hasta finales de julio que es relanzado con el nombre de "Free-DOS" y bajo licencia pública general (GNU). Finalmente eliminaron el guion y quedó con el nombre actual "FreeDOS".

FreeDOS incluye algunas características que no estaban presentes en MS-DOS:

No es posible iniciar ninguna versión de Windows basada en MS-DOS en modo extendido del 386, solo se puede iniciar Windows 3.0 en modo estándar, Windows 3.1 en modo estándar y las versiones de Windows 1.x y 2.x desde FreeDOS. No obstante, es posible sortear este problema usando un gestor de arranque o una herramienta similar, como la que viene con FreeDOS, para hacer una instalación paralela (con arranque dual) de FreeDOS y la versión de Windows en cuestión (decidiendo entre un sistema operativo y otro al arrancar).

Estas versiones de Windows están enlazadas al propio DOS que incorporan. No es posible ejecutarlas desde FreeDOS, pero se pueden instalar Windows y FreeDOS en la misma unidad C:, con la ayuda de un gestor de arranque tal como se ha descrito anteriormente, o con un gestor de arranque de GNU/Linux como LiLo o Grub.

Una instalación paralela con Windows NT y ReactOS no causa problemas porque estas versiones ya no usan un sistema DOS como sistema base. El núcleo de FreeDOS se puede añadir simplemente al gestor de arranque que estos sistemas operativos incluyen.

El programa de gestión de memoria EMM386 incluido con soporta VCPI, que permite ejecutar programas que utilizan DPMI.
FreeDOS también contiene un controlador UDMA para un acceso a disco más rápido, que además también se puede usar en otras versiones de DOS. La memoria intermedia de disco "LBAcache" almacena los datos del disco a los que se ha accedido recientemente en la memoria XMS para proporcionar un acceso aún más rápido y reducir el acceso directo al disco duro (lo que causa menos ruido)

Gracias a que el intérprete de línea de comandos FreeCOM se puede mover a sí mismo a la memoria extendida, es posible liberar mucha memoria convencional: Con el núcleo almacenado en la memoria alta y los controladores cargados en los bloques de memoria superior, se pueden disponer de 620KB (620*1024 bytes) de memoria convencional, lo que es útil para programas y juegos de DOS exigentes en este aspecto.

La licencia es libre. Tiene soporte para particiones FAT32, desde las que puede arrancar. Dependiendo de el BIOS usado, se pueden utilizar discos duros LBA de hasta 128 Gb o incluso 2 TB. Algunos BIOS tienen soporte para LBA pero tienen un fallo con los discos mayores de 32 GB; controladores como OnTrack o EzDrive pueden "reparar" ese problema.
FreeDOS también se puede usar con un controlador llamado "DOSLFN" que soporta nombres de archivo largos (ver VFAT), pero la mayoría de los programas de FreeDOS NO soportan nombres de archivo largos, incluso si el controlador está cargado (EDIT.COM para Windows 9x sí soporta nombres largos si el controlador está cargado).

No hay planes para añadir soporte NTFS o ext2fs a FreeDOS, pero hay varios controladores shareware disponibles para tal propósito. Para acceder a particiones ext2fs, se pueden usar la herramienta "LTOOLS", que puede copiar información desde y hacia particiones ext2fs. Si se ejecuta FreeDOS en (un emulador de PC/DOS para sistemas GNU/Linux) es posible instalar aplicaciones DOS en cualquier sistema de archivos y disco duro que soporte GNU/Linux. 

Tampoco está planeado el soporte de USB, sólo los dispositivos USB reconocidos por el BIOS están disponibles de primera mano para FreeDOS. Se pueden usar controladores gratuitos, o ejecutar FreeDOS en una ventana de DOSEmu y dejar que use cualquier unidad que sea accesible a GNU/Linux.

Otros emuladores populares de PC y DOS son Bochs (simula un PC completo) y DOSBox, que simula un PC con un núcleo DOS y su intérprete: Los programas dentro de DOSBox "ven" un DOS, pero no se puede instalar FreeDOS u otro núcleo. No obstante, las herramientas de FreeDOS son plenamente funcionales en DOSBox

El núcleo de FreeDOS también se suministra con DOSEmu. DOSEmu simula de manera optimizada un PC que permite el uso de controladores simplificados (proporcionados con DOSEmu). El sistema se ejecuta mucho más rápido que con el simulador de PC GNU Bochs o el emulador comercial VMware. Sin embargo, la simulación del hardware carece de realismo en algunos aspectos: El acceso al disco simulado a través de la BIOS virtual funciona bien, pero los programas DOS no pueden programar los controladores del disco virtual. No obstante, sí que hay hardware gráfico y de sonido virtual

Debido a un acuerdo con Microsoft, que impedía a los vendedores de ordenadores venderlos sin sistema operativo instalado, Dell Computer ofreció algunos de sus sistemas de su "serie n" con FreeDOS preinstalado.

El proyecto FreeDOS comenzó a proporcionar una alternativa a MS-DOS cuando Microsoft anunció en 1994 que dejaría de vender y dar soporte a su MS-DOS.

Una alternativa a FreeDOS es OpenDOS y EDR-DOS Enhanced DR-DOS. Este DOS es más compatible con Windows, pero la licencia es más restrictiva. OpenDOS está basado en el DR-DOS, propiedad de DeviceLogics y que se ofrece como shareware, y Enhanced DR-DOS basado en el OpenDOS.

Desde 2014 la mascota de este sistema operativo tiene un nombre concreto: "Binkly".




</doc>
<doc id="7024" url="https://es.wikipedia.org/wiki?curid=7024" title="Ecuación diferencial">
Ecuación diferencial

Una ecuación diferencial es una ecuación matemática que relaciona una función con sus derivadas. En las matemáticas aplicadas, las funciones usualmente representan cantidades físicas, las derivadas representan sus razones de cambio, y la ecuación define la relación entre ellas. Como estas relaciones son muy comunes, las ecuaciones diferenciales juegan un rol primordial en diversas disciplinas, incluyendo la ingeniería, la física, la química, la economía, y la biología.

En las matemáticas puras, las ecuaciones diferenciales se estudian desde perspectivas diferentes, la mayoría concernientes al conjunto de las soluciones de las funciones que satisfacen la ecuación. Solo las ecuaciones diferenciales más simples se pueden resolver mediante fórmulas explícitas; sin embargo, se pueden determinar algunas propiedades de las soluciones de una cierta ecuación diferencial sin hallar su forma exacta.

Si la solución exacta no puede hallarse, esta puede obtenerse numéricamente, mediante una aproximación usando computadoras. La teoría de sistemas dinámicos hace énfasis en el análisis cualitativo de los sistemas descritos por ecuaciones diferenciales, mientras que muchos métodos numéricos han sido desarrollados para determinar soluciones con cierto grado de exactitud.

Las ecuaciones diferenciales aparecieron por primera vez en los trabajos de cálculo de Newton y Leibniz. En 1671, en el Capítulo 2 de su trabajo "Método de las fluxiones y series infinitas", Isaac Newton hizo una lista de tres clases de ecuaciones diferenciales:

Resolvió estas ecuaciones y otras usando series infinitas y discutió la no unicidad de las soluciones.

Jakob Bernoulli propuso la ecuación diferencial de Bernoulli en 1695. Esta es una ecuación diferencial ordinaria de la forma

para la que luego, en los siguientes años, Leibniz obtuvo sus soluciones mediante simplificaciones.

Históricamente, el problema de una cuerda vibrante tal como la de un instrumento musical, fue estudiado por Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, y Joseph-Louis Lagrange.

Las ecuaciones de Euler-Lagrange fueron desarrolladas en la década de 1750 por Euler y Lagrange en relación con sus estudios del problema de la tautócrona. Este es el problema de determinar una curva en la cual una partícula con peso caerá en un punto fijo en cierta cantidad fija de tiempo, independiente del punto de partida.

Lagrange resolvió este problema en 1755 y envió la solución a Euler. Ambos desarrollaron el método de Lagrange y lo aplicaron a la mecánica, lo que los condujo a la mecánica Lagrangiana.

En 1822 Fourier publicó su trabajo de transferencia de calor en "" (Teoría analítica del calor), en la que basó su razonamiento en la ley del enfriamiento de Newton, esto es, que la transferencia de calor entre dos moléculas adyacentes es proporcional a diferencias extremadamente pequeñas de sus temperaturas. En este libro Fourier expone la ecuación del calor para la difusión conductiva del calor. Esta ecuación en derivadas parciales es actualmente objeto de estudio en la física matemática.

Las ecuaciones diferenciales estocásticas, que amplían tanto la teoría de las ecuaciones diferenciales como la teoría de la probabilidad, fueron introducidas con un tratamiento riguroso por Kiyoshi Itō y Ruslán Stratónovich durante los años 1940 y 1950.

Las ecuaciones diferenciales pueden dividirse en varios tipos. Aparte de describir las propiedades de la ecuación en si, las clases de las ecuaciones diferenciales pueden ayudar a buscar la elección de la aproximación a una solución. Es muy común que estas distinciones incluyan si la ecuación es: Ordinaria/Derivadas Parciales, Lineal/No lineal, y Homogénea/Inhomogénea. Esta lista es demasiado grande; hay muchas otras propiedades y subclases de ecuaciones diferenciales las cuales pueden ser muy útiles en contextos específicos.

Una ecuación diferencial ordinaria ("EDO") es una ecuación que contiene una función de una variable independiente y sus derivadas. El término ""ordinaria"" se usa en contraste con la ecuación en derivadas parciales la cual puede ser respecto a "más de" una variable independiente.

Las ecuaciones diferenciales lineales, las cuales tienen soluciones que pueden sumarse y ser multiplicadas por coeficientes, están bien definidas y comprendidas, y tienen soluciones exactas que pueden hallarse. En contraste, las EDOs cuyas soluciones no pueden sumarse son no lineales, y su solución es más intrincada, y muy pocas veces pueden hallarse en forma exacta de funciones elementales: las soluciones suelen obtenerse en forma de series o forma integral. Los métodos numéricos y gráficos para EDOs, pueden realizarse manualmente o mediante computadoras, se pueden aproximar las soluciones de las EDOs y su resultado puede ser muy útil, muchas veces suficientes como para prescindir de la solución exacta y analítica.

Una ecuación en derivadas parciales ("EDP") es una ecuación diferencial que contiene una función multivariable y sus derivadas parciales. Estas ecuaciones se utilizan para formular problemas que involucran funciones de varias variables, y pueden resolverse manualmente, para crear una simulación por computadora.

Las EDPs se pueden usar para describir una amplia variedad de fenómenos tal como el sonido, el calor, la electroestática, la electrodinámica, la fluidodinámica, la elasticidad, o la mecánica cuántica. Estos distintos fenómenos físicos se pueden formalizar en términos de EDPs. Con ecuaciones diferenciales ordinarias es muy común realizar modelos unidimensionales de sistemas dinámicos, y las ecuaciones diferenciales parciales se pueden utilizar para modelos de sistemas multidimensionales. Las EDPs tienen una generalización en las ecuaciones en derivadas parciales estocásticas.

Una ecuación diferencial es "lineal" cuando sus soluciones pueden obtenerse a partir de combinaciones lineales de otras soluciones. Si es lineal, la ecuación diferencial tiene sus derivadas con máxima potencia de 1 y no existen términos en donde haya productos entre la función desconocida y/o sus derivadas. La propiedad característica de las ecuaciones lineales es que sus soluciones tienen la forma de un subespacio afín de un espacio de soluciones apropiados, cuyo resultado se desarrolla en la teoría de ecuaciones diferenciales lineales.

Las ecuaciones diferenciales lineales homogéneas son una subclase de las ecuaciones diferenciales lineales para la cual el espacio de soluciones es un subespacio lineal, es decir, la suma de cualquier conjunto de soluciones o múltiplos de soluciones, es también una solución. Los coeficientes de la función desconocida, y sus derivadas en una ecuación diferencial lineal pueden ser funciones de la variable o variables independientes, si estos coeficientes son constantes, entonces se habla de "ecuaciones diferenciales lineales a coeficientes constantes".

Se dice que una ecuación es lineal si tiene la forma:

Es decir:

Ejemplos:

Existen muy pocos métodos para resolver ecuaciones diferenciales no lineales en forma exacta; aquellas que se conocen es muy común que dependan de la ecuación teniendo simetrías particulares. Las ecuaciones diferenciales no lineales pueden exhibir un comportamiento muy complicado 
en intervalos grandes de tiempo, característica del caos. Cada una de las cuestiones fundamentales de la existencia, unicidad, y extendibilidad de las soluciones para ecuaciones diferenciales no lineales, y el problema bien definido de los problemas de condiciones iniciales y de contorno para EDPs no lineales son problemas difíciles y su resolución en casos especiales se considera que es un avance significativo en la teoría matemática (por ejemplo la existencia y suavidad de Navier-Stokes). Sin embargo, si la ecuación diferencial es una representación de un proceso físico significativo formulado correctamente, entonces se espera tener una solución. 

Las ecuaciones diferenciales no lineales suelen aparecer por medio de aproximaciones a ecuaciones lineales. Estas aproximaciones son válidas únicamente bajo condiciones restringidas. Por ejemplo, la ecuación del oscilador armónico es una aproximación de la ecuación no lineal de un péndulo que es válida para pequeñas amplitudes de oscilación (ver más adelante).

No existe un procedimiento general para resolver ecuaciones diferenciales no lineales. Sin embargo, algunos casos particulares de no linealidad sí pueden ser resueltos. Son de interés el caso semilineal y el caso cuasilineal.

Una ecuación diferencial ordinaria de orden "n" se llama cuasilineal si es "lineal" en la derivada de orden "n". Más específicamente, si la ecuación diferencial ordinaria para la función formula_11 puede escribirse en la forma:

Se dice que dicha ecuación es cuasilineal si formula_12 es una función afín, es decir, formula_13.

Una ecuación diferencial ordinaria de orden "n" se llama semilineal si puede escribirse como suma de una función "lineal" de la derivada de orden "n" más una función cualquiera del resto de derivadas. Formalmente, si la ecuación diferencial ordinaria para la función formula_11 puede escribirse en la forma:

Se dice que dicha ecuación es semilineal si formula_15 es una función lineal.

Las ecuaciones diferenciales se describen por su orden, determinado por el término con derivadas de mayor orden. Una ecuación que contiene solo derivadas simples es una "ecuación diferencial de primer orden", una ecuación que contiene hasta derivadas segundas es una "ecuación diferencial de segundo orden", y así sucesivamente.

Ejemplos de orden en ecuaciones: 
formula_19

Es la potencia de la derivada de mayor orden que aparece en la ecuación, siempre y cuando la ecuación esté en forma polinómica, de no ser así se considera que no tiene grado.

En el primer grupo de ejemplos, sea "u" una función desconocida que depende de "x", y "c" y "ω" son constantes conocidas. Observar que tanto las ecuaciones diferenciales ordinarias como parciales pueden clasificarse como "lineales" y "no lineales".



Una solución esta dada por formula_22




En el siguiente grupo de ejemplos, la función desconocida "u" depende de dos variables "x" y "t" o "x" e "y".




1° Paso:
La resolución de ecuaciones diferenciales no es como aquellas resoluciones de las ecuaciones algebraicas. Puesto que a pesar de que en ocasiones sus soluciones son poco claras, también puede ser de interés si estas son únicas o existen.

Para problemas de primer orden con valores iniciales, el teorema de existencia de Peano nos da un conjunto de condiciones en el cual la solución existe. Para cualquier punto dado formula_29 en el plano "xy", y definida una región rectangular formula_30, tal que formula_31 y formula_29 está en el interior de formula_30. Si tenemos una ecuación diferencial formula_34 y la condición que formula_35 cuando formula_36, entonces hay una solución local a este problema si formula_37 y formula_38 son ambas continuas en formula_30. La solución existe en algún intervalo con su centro en formula_40. La solución puede no ser única. (Ver Ecuación diferencial ordinaria para otros resultados.)

Sin embargo, esto solo nos ayuda con problemas de primer orden con condiciones iniciales. 
Supongamos que tenemos un problema lineal con condiciones iniciales de orden enésimo:

tal que

Para cualquier formula_43 no nulo, si formula_44 y formula_45 son continuos sobre algún intervalo conteniendo formula_46, formula_47 es único y existe.

Una solución de una ecuación diferencial es una función que al reemplazar a la función incógnita, en cada caso con las derivaciones correspondientes, verifica la ecuación, es decir, la convierte en una identidad. Hay tres tipos de soluciones:

La solución general es una solución de tipo genérico, expresada con una o más constantes. Es un haz de curvas. Tiene un orden de infinitud de acuerdo a su cantidad de constantes (una constante corresponde a una familia simplemente infinita, dos constantes a una familia doblemente infinita, etc). En caso de que la ecuación sea lineal, la solución general se logra como combinación lineal de las soluciones (tantas como el orden de la ecuación) de la ecuación homogénea (que resulta de hacer el término no dependiente de formula_48 ni de sus derivadas igual a 0) más una solución particular de la ecuación completa

Si fijando cualquier punto formula_49 por donde debe pasar necesariamente la solución de la ecuación diferencial, existe un único valor de C, y por lo tanto de la curva integral que satisface la ecuación, éste recibirá el nombre de solución particular de la ecuación en el punto formula_49, que recibe el nombre de condición inicial. 

Es un caso particular de la solución general, en donde la constante (o constantes) recibe un valor específico.

La solución singular es una función que verifica la ecuación, pero que no se obtiene particularizando la solución general. Es solución de la ecuación no consistente en una particular de la general, en otras palabras, esta solución no pertenece a la solución general pero aun así verifica la ecuación diferencial.

Sea la ecuación diferencial ordinaria de orden n formula_51, es fácil verificar que la función "y= f(x)" es su solución. Basta calcular sus derivadas de f(x), luego reemplazarlas en la ecuación , junto con f(x) y probar que se obtiene una identidad en x.

Las soluciones de E.D.O. se presentan en forma de funciones implícitamente definidas, y a veces imposibles de expresar de manera explícita. Por ejemplo
La forma más simple de todas las ecuaciones diferenciales es formula_52 cuya solución es formula_53
En algunos casos es posible resolverla por métodos elementales del cálculo. Sin embargo, en otros casos, la solución analítica requiere técnicas de variable compleja o más sofisticadas como sucede con las integrales:

no puede estructurase mediante un número finito de funciones elementales.

El estudio de ecuaciones diferenciales es un campo extenso en matemáticas puras y aplicadas, en física y en la ingeniería. Todas estas disciplinas se interesan en las propiedades de ecuaciones diferenciales de varios tipos. Las matemáticas puras se focalizan en la existencia y unicidad de las soluciones, mientras que las matemáticas aplicadas enfatiza la justificación rigurosa de los métodos de aproximación de las soluciones. Las ecuaciones diferenciales juegan un rol muy importante en el modelado virtual de cualquier proceso físico, técnico, o biológico, por ejemplo, tanto el movimiento celeste, como el diseño de un puente, o la interacción entre neuronas. Las ecuaciones diferenciales que se plantean para resolver problemas de la vida real, no necesariamente son resolubles directamente, es decir, sus soluciones no tienen una expresión en forma cerrada. Cuando sucede esto, las soluciones se pueden aproximar usando métodos numéricos.

Muchas leyes de la física y la química se formalizan con ecuaciones diferenciales. En biología y economía, las ecuaciones diferenciales se utilizan para el modelado del comportamiento de sistemas complejos. La teoría matemática de las ecuaciones diferenciales se desarrolló inicialmente con las ciencias donde las ecuaciones se originaban y donde se encontraban resultados para las aplicaciones. Sin embargo, algunas veces se originaban problemas diversos en campos científicos distintos, de los cuales resultaban ecuaciones diferenciales idénticas. Esto sucedía porque, detrás de la teoría matemática de las ecuaciones, puede verse un principio unificado detrás de los fenómenos. Como por ejemplo, si se considera la propagación de la luz y el sonido en la atmósfera, y de las ondas sobre la superficie de un estanque. Todos estos fenómenos pueden describirse con la misma ecuación en derivadas parciales de segundo orden, la ecuación de onda, la cual nos permite pensar a la luz y al sonido como formas de onda, y en forma similar a las ondas en el agua. La conducción de calor, la teoría que fue desarrollada por Joseph Fourier, está gobernada por otra ecuación en derivadas parciales de segundo orden, la ecuación de calor. Resulta que muchos procesos de difusión, aunque aparentan ser diferentes, están descritos por la misma ecuación. La ecuación de Black-Scholes en las finanzas, está por ejemplo, relacionada con la ecuación del calor.


Siempre que se conozca la fuerza actuante sobre una partícula, la Segunda ley de Newton es suficiente para describir el movimiento de una partícula. Una vez que están disponibles las relaciones independientes para cada fuerza que actúa sobre una partícula, se pueden sustituir en la segunda ley de Newton para obtener una ecuación diferencial ordinaria, la cual se denomina "ecuación de movimiento".

Las ecuaciones de Maxwell son un conjunto de ecuaciones en derivadas parciales que, junto con la ley de la fuerza de Lorentz , forman los fundamentos de la electrodinámica clásica, óptica clásica, y la teoría de los circuitos eléctricos. Estos campos se volvieron fundamentales en las tecnologías eléctricas, electrónicas y de comunicaciones. Las ecuaciones de Maxwell describen cómo los campos eléctrico y magnético se generan alterando uno y otro por cargas y corrientes eléctricas. Estas ecuaciones deben su nombre al físicomatemático escocés James Clerk Maxwell, quien publicó sus trabajos sobre estas ecuaciones entre 1861 y 1862.

Las ecuaciones de campo de Einstein (conocidas también como "ecuaciones de Einstein") son un conjunto de diez ecuaciones en derivadas parciales de la teoría de la relatividad general donde se describe la interacción fundamental de la gravitación como un resultado de que el espacio-tiempo es curvado por la materia y la energía. Publicado por primera vez por Einstein en 1915 
como una ecuación tensorial, las ecuaciones equiparan una curvatura espacio-tiempo local (expresada por el tensor de Einstein) con la energía y momentum local dentro del espacio-tiempo (expresado por el tensor de energía-impulso).

En la mecánica cuántica, el análogo a la ley de Newton es la Ecuación de Schrödinger (una ecuación en derivadas parciales) para un sistema cuantificado (usualmente átomos, moléculas, y partículas subatómicas que pueden estar libres, ligadas, o localizadas). No es una ecuación algebraica simple, pero es, en general, una ecuación en derivadas parciales y lineal, que describe la evolución en el tiempo de una función de onda (también llamada una "función de estado").


Las ecuaciones Lotka–Volterra, también conocidas como las ecuaciones predador-presa, son un par de ecuaciones diferenciales no lineales de primer orden frecuentemente utilizadas para describir la dinámica de sistemas biológicos en los cuales interactúan dos especies, una el predador, y la otra, la presa.






</doc>
<doc id="7030" url="https://es.wikipedia.org/wiki?curid=7030" title="Michael Faraday">
Michael Faraday

Michael Faraday, FRS (; Newington Butt, 22 de septiembre de 1791-Hampton Court, 25 de agosto de 1867), fue un físico y químico británico que estudió el electromagnetismo y la electroquímica. Sus principales descubrimientos incluyen la inducción electromagnética, el diamagnetismo y la electrólisis.

A pesar de la escasa educación formal recibida, Faraday es uno de los científicos más influyentes de la historia. Mediante su estudio del campo magnético alrededor de un conductor por el que circula corriente continua, fijó las bases para el desarrollo del concepto de campo electromagnético. También estableció que el magnetismo podía afectar a los rayos de luz y que había una relación subyacente entre ambos fenómenos.

Descubrió asimismo el principio de inducción electromagnética, el diamagnetismo, las leyes de la electrólisis e inventó algo que él llamó "dispositivos de rotación electromagnética", que fueron los precursores del actual motor eléctrico.

En el campo de la química, Faraday descubrió el benceno, investigó el clatrato de cloro, inventó un antecesor del mechero de Bunsen, el sistema de números de oxidación e introdujo términos como ánodo, cátodo, electrodo e ion. Finalmente, fue el primero en recibir el título de Fullerian Professor of Chemistry en la Royal Institution de Gran Bretaña, que ostentaría hasta su muerte.

Faraday fue un excelente experimentador, que transmitió sus ideas en un lenguaje claro y simple. Sus habilidades matemáticas, sin embargo, no abarcaban más allá de la trigonometría y el álgebra básica. James Clerk Maxwell tomó el trabajo de Faraday y otros y lo resumió en un grupo de ecuaciones que representan las actuales teorías del fenómeno electromagnético. El uso de líneas de fuerza por parte de Faraday llevó a Maxwell a escribir que "demuestran que Faraday ha sido en realidad un gran matemático. Del cual los matemáticos del futuro derivarán valiosos y prolíficos métodos".

La unidad de capacidad eléctrica en el Sistema Internacional de Unidades (SI), el faradio (F), se denomina así en su honor.

Albert Einstein tenía colgado en la pared de su estudio un retrato de Faraday junto a los de Isaac Newton y James Clerk Maxwell.

El físico neozelandés Ernest Rutherford declaró: "Cuando consideramos la extensión y la magnitud de sus descubrimientos y su influencia en el progreso de la ciencia y de la industria, no existen honores que puedan retribuir la memoria de Faraday, uno de los mayores descubridores científicos de todos los tiempos".

Faraday nació en la aldea de Newington Butt, que es ahora parte del municipio de Southwark (prácticamente en el centro de Londres), pero que, en aquel entonces, era una zona suburbana del condado de Surrey. No provenía de una familia rica. 

De niño, la madre de Faraday lo sacó del colegio porque los métodos y castigos para los niños eran terribles; la maestra de Faraday se burlaba y le castigaba por no pronunciar bien la "R". Faraday comenzó a estudiar por su cuenta, pero su creatividad e ingenio lo llevarían a la fama a pesar de no tener una formación rigurosa en ciertos campos de la ciencia. 

Su padre, James, se trasladó junto a su esposa y sus dos hijos a Londres durante el invierno de 1791, desde Outhgill, en Westmorland, donde trabajó como aprendiz del herrero del pueblo. Michael nació durante el otoño de ese año. El joven Michael Faraday, el tercero de cuatro hermanos, llegó a ser, a la edad de 14, aprendiz de George Riebau, encuadernador y vendedor de libros de la ciudad.

Durante los siete años que duró su aprendizaje, Faraday leyó muchos libros, entre ellos "The improvement of the Mind", de Isaac Watts, implementando con gran entusiasmo los principios y sugerencias ahí escritos. Durante esta época también desarrolló su interés por la ciencia, especialmente por el fenómeno eléctrico.

En 1812, a la edad de 20 años, y ya en el fin de su proceso de aprendizaje de encuadernador, Faraday comenzó a asistir a las conferencias del destacado químico inglés Humphry Davy, de la Royal Institution y de la Royal Society, y de John Tatum, fundador de la City Philosophical Society. La mayoría de las invitaciones para las conferencias fueron ofrecidas a Faraday por William Dance, uno de los fundadores de la Royal Philharmonic Society. Faraday, posteriormente, envió a Davy un libro de 300 páginas basado en notas que él mismo había tomado durante esas conferencias. La respuesta de Davy fue inmediata, amable y favorable. Davy, durante un experimento con tricloruro de nitrógeno, se dañó gravemente la vista, por lo que decidió contratar a Faraday como su secretario. Cuando uno de los asistentes de la Royal Institution, John Payne, fue despedido, Humphry Davy se vio en la necesidad de buscar un sustituto para el puesto, designando a Faraday asistente de química de la Royal Institution, el 1 de marzo de 1813.

En la clasista sociedad inglesa de la época, Faraday no era considerado un caballero. Cuando Davy decidió emprender un viaje por el continente en 1813-1815, su sirviente prefirió no ir. Faraday, que iba en calidad de asistente científico, se vio forzado a suplir las tareas del sirviente hasta que se pudiera encontrar uno nuevo en París. La esposa de Davy, Jane Apreece, se negaba a tratar a Faraday como un igual (le obligaba a viajar fuera del carruaje, comer con los sirvientes, etcétera), le hacía que su vida resultase tan miserable, que lo llevó a contemplar la idea de regresar a Inglaterra solo y abandonar la ciencia. El viaje, sin embargo, le dio acceso a la élite científica europea y sus fascinantes y estimulantes ideas.

Faraday se casó con Sarah Barnard (1800-1879) el 12 de junio de 1821. Se conocieron a través de sus familias en la iglesia Sandemaniana, confesando su fe a esta congregación el mes siguiente a su matrimonio. No tuvieron hijos.

Faraday fue un cristiano devoto; su congregación Sandemaniana era una filial de la Iglesia de Escocia. Una vez casado, sirvió como diácono y, durante dos períodos, como presbítero. Su iglesia estaba ubicada en Paul's Alley en Barbican Estate. Este lugar de reuniones fue trasladado a Barnsbury Grove, Islington, en 1862. Aquí fue donde Faraday cumplió los últimos dos años de su segundo período de presbítero, antes de dimitir de su cargo. Biógrafos del científico han señalado que "un fuerte sentimiento de unidad entre Dios y la naturaleza impregnó la vida y el trabajo de Faraday".

En junio de 1832, la Universidad de Oxford concedió a Faraday el grado de "Doctor of Civil Law" (honorario). Durante su vida, la corona británica le ofreció un título de caballero, en reconocimiento a sus servicios a la ciencia, el cual fue rechazado por motivos religiosos. Faraday creía que acumular riquezas y perseguir recompensas mundanas atentaba contra la palabra sagrada de la Biblia, prefiriendo seguir siendo llamado "simplemente Sr. Faraday, hasta el final".
Rechazó dos veces convertirse en presidente de la Royal Society.

Fue elegido miembro extranjero de la Real Academia de las Ciencias de Suecia en 1838, y fue uno de los ocho miembros extranjeros elegidos por la Academia de Ciencias de Francia en 1844.

Faraday sufrió un colapso nervioso en 1839, pero regresaría posteriormente a sus investigaciones sobre electromagnetismo.

En 1848, como resultado de las gestiones del príncipe consorte Alberto, se le concedió una casa de Gracia y Favor en Hampton Court en Middlesex, libre de gastos y costos de mantenimiento. En 1858, Faraday se retiró a vivir a ese lugar.

Al ser consultado por el gobierno británico con el fin de ayudar en la producción de armas químicas para la Guerra de Crimea (1853-1856), Faraday rechazó participar, alegando motivos éticos.

Faraday murió en su casa en Hampton Court, a 35 km al suroeste de Londres, el 25 de agosto de 1867, a la edad de 75 años. A pesar de haber rechazado una sepultura en la Abadía de Westminster, existe ahí una placa conmemorativa en su nombre, cerca de la tumba de Isaac Newton. Faraday fue sepultado en la sección de disidentes del Cementerio de Highgate.

Desde 1935 el cráter lunar "Faraday" lleva este nombre en su memoria.

El primer trabajo de Faraday en el área de la química fue como asistente de Humphry Davy. Estaba especialmente interesado en el estudio del cloro, descubriendo dos nuevos compuestos de cloro y carbono. También condujo los primeros rudimentarios experimentos sobre difusión de gases, fenómeno que había sido previamente identificado por John Dalton. La importancia física de este fenómeno fue enteramente revelada por Thomas Graham y Johann Josef Loschmidt. Tuvo éxito al lograr licuar diversos gases, investigó la aleación del acero y produjo varios nuevos tipos de vidrio destinados a fines ópticos. Un ejemplar de estos pesados cristales tomaría posteriormente una gran importancia histórica; cuando Faraday ubicó el vidrio en un campo magnético descubrió la rotación del plano de polarización de la luz. Este ejemplar fue también la primera sustancia que se encontró que era repelida por los polos de un imán.

Faraday inventó una temprana forma del mechero de Bunsen, usado en todos los laboratorios de ciencia del mundo como una buena fuente de calor.

Trabajó ampliamente en el campo de la química, descubriendo sustancias tales como el benceno y condensando gases como el cloro. La licuefacción de gases ayudó a establecer que estos corresponden a vapores de líquidos con bajo punto de ebullición, otorgando una base más sólida al concepto de agregación molecular. En 1820, Faraday reportó la primera síntesis de compuestos de cloro y carbono, el hexacloroetano (CCl) y el tetracloroetileno (CCl), publicando sus resultados al año siguiente.

También descubrió la composición del clatrato hidrato de cloro, que había sido descubierto por Humphry Davy en 1810. Asimismo, es responsable del descubrimiento de las leyes de la electrólisis y de introducir términos como ánodo, cátodo, electrodo e ion, propuestos en gran parte por William Whewell.

Faraday fue el primero en descubrir lo que posteriormente serían llamadas nanopartículas metálicas. En 1847 descubrió que las propiedades ópticas del coloide de oro diferían de las del metal macizo. Esta fue, probablemente, la primera observación registrada sobre los efectos del tamaño cuántico, y podría ser considerado como el nacimiento de la nanociencia.

Faraday es mejor conocido por su trabajo relacionado con la electricidad y el magnetismo. Su primer experimento registrado fue la construcción de una pila voltaica con siete monedas de medio penique, apiladas junto a siete discos chapados en zinc y seis trozos de papel humedecidos con agua salada. Con esta pila pudo descomponer el sulfato de magnesio (primera carta a Abbott, 12 de julio de 1812).

En 1821, poco después del descubrimiento del fenómeno electromagnético por parte del físico y químico danés Hans Christian Ørsted, Davy y el científico británico William Hyde Wollaston intentaron, sin éxito, diseñar un motor eléctrico. Faraday, habiendo discutido el problema con los dos hombres, persistió y logró construir dos dispositivos que producían, lo que él denominó, "rotación electromagnética". Uno de ellos, conocido ahora como motor homopolar, producía un movimiento circular continuo ocasionado por la fuerza magnética circular en torno a un alambre que se extendía hasta un recipiente con mercurio que tenía un imán en su interior; el alambre rota alrededor del imán cuando se le suministra una corriente eléctrica desde una batería química. Estos experimentos e inventos conformaron las bases de la tecnología electromagnética moderna. La emoción debida a estos descubrimientos llevó a Faraday a publicar sus trabajos sin haberlos presentado previamente a Davy o Wollaston. La controversia resultante dentro de la Royal Society tensó la relación con su mentor Davy y pudo haber contribuido a que Faraday fuera designado para otras tareas, impidiendo su participación en investigación electromagnética durante varios años.

Desde su primer descubrimiento en 1821, Faraday continuó su trabajo de laboratorio, explorando las propiedades electromagnéticas de distintos materiales y desarrollando la experiencia requerida. En 1824, diseñó un circuito para estudiar si el campo magnético podía regular el flujo eléctrico de un cable adyacente, pero no encontró tal relación.
Durante los siguientes siete años, Faraday ocupó la mayor parte de su tiempo perfeccionando la fórmula de un cristal con cualidades ópticas, el borosilicato de plomo, el cual utilizaría en sus posteriores experimentos que lo llevarían a relacionar el fenómeno electromagnético con la luz.

En su tiempo libre continuó publicando sus trabajos experimentales en óptica y electromagnetismo; mantuvo también correspondencia con científicos que había conocido en su viaje a través de Europa con Davy y que también se encontraban investigando el electromagnetismo. Dos años después de la muerte de Davy, en 1831, Faraday dio inicio a la gran serie de experimentos que lo llevarían a descubrir la inducción electromagnética.

El gran descubrimiento de Faraday surgió cuando enrolló dos solenoides de alambre alrededor de un aro de hierro, y encontró que cuando hacía pasar corriente por un solenoide, otra corriente era temporalmente inducida en el otro solenoide. Este fenómeno se conoce como inducción mutua. Este aparato aún se expone en la Royal Institution. En experimentos posteriores, observó que si hacía pasar un imán a través de una espira de alambre, una corriente eléctrica circularía a través de este alambre. La corriente también fluía si la espira era movida sobre el imán en reposo. Sus demostraciones establecieron que un campo magnético variable generaba un campo eléctrico; esta relación fue modelada matemáticamente por James Clerk Maxwell como la Ley de Faraday, que posteriormente se convertiría en una de las cuatro ecuaciones de Maxwell, y que a su vez evolucionarían a un modelo más general conocido como teoría de campos. Faraday usaría después los principios que había descubierto para construir el dínamo eléctrico, ancestro de los actuales generadores y motores eléctricos.

En 1832, realizó una serie de experimentos con el objetivo de estudiar la naturaleza fundamental de la electricidad. Faraday utilizó "estática", baterías y "electricidad animal" para producir el fenómeno de atracción eléctrica, electrólisis, magnetismo, etc. Concluyó que, al contrario de la opinión científica de la época, la división entre varios "tipos" de electricidad era irreal. En vez de eso, propuso que sólo existe un "tipo" de electricidad, y que valores variables de cantidad e intensidad (corriente y voltaje) producirían diferentes grupos de fenómenos.

Cerca del final de su carrera, Faraday propuso que la fuerza electromagnética podía extenderse en el espacio vacío alrededor de un conductor. Esta idea fue rechazada por sus pares científicos, no pudiendo vivir lo suficiente como para ver la aceptación de su proposición por parte de la comunidad científica. El concepto de Faraday de líneas de flujo saliendo desde cuerpos cargados e imanes proveyó una forma de visualizar los campos eléctrico y magnético; ese modelo conceptual fue crucial para el exitoso desarrollo de dispositivos electromecánicos que dominarían la industria y la ingeniería por el resto del siglo XIX.

En 1845, Faraday descubrió que muchos materiales exhibían una débil repulsión frente a campos magnéticos: un fenómeno que denominó diamagnetismo.

También descubrió que el plano de polarización de la luz linealmente polarizada podía rotarse debido a la aplicación de un campo magnético externo alineado con la dirección de propagación de la luz. Este fenómeno es llamado en la actualidad efecto Faraday. Así lo hace constar en su libro de notas: "He, al fin, tenido éxito en "iluminar una curva magnética" o "línea de fuerza" y en "magnetizar un rayo de luz"".

En los últimos años de su vida, en 1862, Faraday utilizó un espectroscopio para estudiar la alteración de las líneas espectrales en presencia de un campo magnético. El equipamiento disponible, sin embargo, no fue suficiente como para mostrar una determinación precisa del cambio espectral. Posteriormente, el físico neerlandés Pieter Zeeman utilizaría un aparato mejorado para estudiar el mismo fenómeno, publicando sus resultados en 1897 y recibiendo el premio Nobel de Física en 1902. Tanto en su publicación de 1897 como en su discurso de aceptación del Nobel en 1902,
Zeeman hizo referencia al trabajo de Faraday.

En su trabajo en electricidad estática denominado La cubeta de Faraday, se demostró que la carga eléctrica se acumula sólo en el exterior de un conductor cargado, sin importar lo que hubiera en su interior. Esto es debido a que las cargas se distribuyen en la superficie exterior de tal manera que los campos eléctricos internos se cancelan. Este efecto de barrera es conocido como jaula de Faraday.

De una obra de Isaac Watts titulada "The Improvement of the Mind" —"La mejora de la mente"—, leída a sus catorce años, Michael Faraday adquirió estos seis constantes principios de su disciplina científica:

Faraday llevó a cabo este descubrimiento en 1845. Consiste en la desviación del plano de polarización de la luz como efecto de un campo magnético, al atravesar un material transparente como el vidrio. Se trataba del primer caso conocido de interacción entre el magnetismo y la luz.

Michael Faraday inició la primera serie de Conferencias de Navidad en 1825. Esto llegó en un momento en el que la educación organizada para jóvenes era escasa. Presentó un total de diecinueve series de conferencias.



</doc>
<doc id="7032" url="https://es.wikipedia.org/wiki?curid=7032" title="Batalla de Chacabuco">
Batalla de Chacabuco

La batalla de Chacabuco fue una decisiva contienda de la Independencia de Chile en la cual combatieron el Ejército de los Andes, formado por tropas de las Provincias Unidas del Río de la Plata y chilenas exiliadas en Mendoza, y el Ejército Realista, resultando en una firme victoria para el bando independentista comandado por el general José de San Martín. La batalla tuvo lugar el 12 de febrero de 1817, en la hacienda de Chacabuco (Colina), a 55 km al norte de la ciudad de Santiago 

Tras el desastre de Rancagua, que causó el fin de la Patria Vieja, los exiliados chilenos se trasladaron a Cuyo, donde se pusieron bajo las órdenes del general José de San Martín, gobernador de la provincia, que había desarrollado un plan para derrotar a los realistas atacando el Virreinato del Perú por mar desde Chile. La ocupación realista de Chile le obligaba a liberar primeramente ese país.

Los chilenos Bernardo O'Higgins y Ramón Freire ayudaron a organizar y adiestrar al llamado Ejército de los Andes.

San Martín liberó a los esclavos negros siempre que se enrolasen en las tropas, e incorporó en ellas a los patriotas chilenos que seguían a O'Higgins (ya que no existía un ejército propiamente chileno, pasando a formar parte íntegra del ejército libertador) y aquellos soldados de Carrera que estuviesen dispuestos a servir bajo sus banderas.

Entre chilenos y argentinos el ejército llegó a contar con alrededor de 4000 hombres perfectamente armados y disciplinados.

Luego del Cruce de los Andes las fuerzas patriotas dirigidas por San Martín marcharon por la ladera poniente del macizo, llevando consigo las piezas de artillería, alimento y ropajes.

Debido a la dispersión de sus fuerzas (estimadas en abril de 1817 en 4317 hombres), a Francisco Casimiro Marcó del Pont, se le hizo muy difícil reunir un ejército, el que finalmente sería de 1500 hombres. La moral de éstos no era la mejor, pues estaban mal pagos y no se les había reconocido los grados ganados en la campaña de reconquista al mando de Mariano Osorio.

Tras reunirse el 9 de febrero en el "Campamento de Curimón" las columnas que cruzaron los Andes por "camino de Los Patos" junto con las que cruzaron por el "camino de Uspallata", se resolvió atacar en la madrugada del día 12. Con el fin de emplear una táctica de pinzas por el frente y la retaguardia, se dividieron a las tropas disponibles en dos:


Mientras Soler rodeaba a los realistas por el camino de Montenegro, más suave pero mucho más largo, O'Higgins lo hacía por "Cuesta Vieja", más corto pero en pendiente y mucho más peligroso, dirigiéndose en dos columnas, y enfrentándose con los adelantados realistas hasta encontrarse frente a frente con el grueso del ejército realista, por lo que decidió avanzar hacia el cerro "Los Halcones" y desplegar allí sus fuerzas, al tiempo que despachaba un mensajero para informar de la situación al general San Martín.

Las fuerzas realistas, inferiores en número, estaban compuestas por el batallón Talavera, de soldados peninsulares, más otros dos provenientes principalmente de Chiloé y Valdivia. Inicialmente Maroto, consciente de la debilidad de sus tropas había conseguido que el gobernador apoyase la idea de retirarse al Maule y unir sus fuerzas a las de Concepción para presentar batalla a San Martín. Pero Marco del Pont cambio de opinión rápidamente y le ordenó impedir que los republicanos avanzaran sobre Santiago. El general realista escogió la cuesta de Chacabuco como una posición defensiva, esperando detener a los patriotas mientras llegaban los refuerzos desde el sur. Sin embargo, en un reconocimiento efectuado el día 12 Maroto notó que la cuesta estaba ocupada por los patriotas, e incapaz de tomarla tuvo que escoger entre retroceder a Colina o defender las posiciones donde estaba su ejército, delante del cerro de Victoria, cerca de la Hacienda de Chacabuco. Optó por esto último, lo que permitió a San Martín rodearlo con sus fuerzas más numerosas.

El plan de San Martín era que O'Higgins atacara por el este, Soler por el oeste y San Martín de frente. Llegada la batalla O'Higgins se desespera al no recibir órdenes de San Martín e inicia el ataque; cuando San Martín se da cuenta de esto envía a un mensajero para que Soler comience el ataque. No había tiempo hasta que Soler ataque y San Martín decide ir él por el frente junto a O'Higgins, hasta que luego una división de adelantados de Soler arribó produciéndose el envolvimiento completo del flanco izquierdo y de la espalda, y destrozando la retaguardia realista, consolidándose así una aplastante victoria a favor de los patriotas. La batalla concluyó a las 14:00 horas. El sorpresivo avance de Maroto cambiaba por completo el panorama. Ahora O'Higgins, sin ayuda de Soler, tendría que batirse con la totalidad de las fuerzas realistas o retroceder a una catástrofe segura. 
O'Higgins al no recibir respuesta ante esta situación a las 11:45 y contraviniendo las órdenes de San Martín de no comprometer fuego, aconsejado por Crámer, (exoficial de Napoleón), ordenó a la infantería cargar a la bayoneta, organizando dos columnas de ataque, siguiendo el modelo napoleónico y lanzándolas sobre el ala derecha enemiga (Batallón Talavera) apoyada por la caballería del coronel José Matías Zapiola, pero los granaderos tropezaron con el profundo cauce de Las Margaritas, que no habían visto, no pudiendo pasar en formación de ataque y retrocedieron tras una andanada de fuego enemigo, sin sufrir muchas bajas, hasta el cerro de los Halcones, donde se reorganizaron. De nuevo O'Higgins y Cramer las lanzaron al asalto, dirigiendo ahora la caballería contra el flanco derecho y la infantería contra el centro. Un pelotón de caballería rompía la línea realista entre la extrema izquierda del Talavera y la derecha del grueso del batallón Chiloé, arrollando a los artilleros. La infantería ya casi vencedora, acudió en auxilio de la caballería. Zapiola, después de romper el cuadro formado por los talaveras, rebasó el ala derecha realista y una segunda carga sobre la infantería y la caballería enemigas produjo la dispersión. Los restos del ejército realista huyeron a la desbandada hacia las casas de Chacabuco distante a pocos kilómetros, y dejando en el campo la tercera parte de sus efectivos. En medio de la batalla San Martín llama a Osorio, el General Realista, para que saque a sus heridos de la batalla, dando así San Martín un ejemplo de hacer una campaña con el menor costo de sangre posible.

Poco después de finalizar la batalla el general San Martín dirigió al Director Supremo argentino Juan Martín de Pueyrredón el parte oficial:
Los patriotas muertos fueron 12 y 120 heridos, mientras que los realistas sufrieron 500 muertos, quedando prisioneros 32 oficiales y 600 soldados. Fue capturada la bandera del Regimiento de Chiloé, junto con aproximadamente mil fusiles, dos piezas de artillería, seis espadas, 16 cajones de municiones, dos barriles de pólvora, cuatro fardos de vestuarios, una treintena de equipajes y la correspondencia. Sin embargo, San Martín cometió el error de no perseguir a los realistas vencidos, dando la oportunidad de reembarcar hacia el Perú a 1.600 soldados, que serían la base de la expedición de Mariano Osorio en el siguiente. De otro modo, estiman varios historiadores, Chacabuco hubiera sido la batalla decisiva de la independencia y la expedición al Perú no se hubiera atrasado tres años.

El 16 de febrero la victoria fue conocida en Mendoza y el 24 de febrero a las 9 horas llegó a Buenos Aires en un pliego enviado por cuenta del gobernador de Cuyo Toribio de Luzuriaga. El 26 de febrero el sargento mayor Manuel Escalada llegó a Buenos Aires conduciendo el parte oficial de San Martín y la bandera tomada a los realistas. Un día antes de la llegada de Escalada el director Pueyrredón -habiéndose puesto en conocimiento de la victoria del ejército a través de Luzuriaga- le envió a San Martín la siguiente comunicación:

Reunida la asamblea bajo la presidencia del gobernador don Francisco Ruiz-Tagle, elegido interinamente por el pueblo al tiempo de la fuga de Marcó del Pont, los concurrentes declararon por aclamación que a la voluntad unánime era nombrar a don José de San Martín gobernador de Chile con omnímoda facultad, y así lo hicieron constar en el acta que se levantó y todos firmaron ante escribano público. El general fiel a sus instrucciones y a su plan político, se negó a aceptar el mando que se le ofreció, y convocó por intermedio del Cabildo una nueva asamblea popular a que concurrieron 210 vecinos notables. El auditor del ejército de los Andes, Dr. Bernardo de Vera y Pintado, reiteró públicamente la renuncia de San Martín, y fue aclamado en el acto el general O’Higgins Director Supremo del Estado de Chile, declarando Vera que la elección era del agrado del General San Martín. El nuevo Director nombró por ministro del interior a don Miguel Zañartu, carácter entero y decidido partidario de la alianza chileno-argentina, y en el departamento de guerra y marina al teniente coronel don José Ignacio Zenteno, secretario de San Martín. Su primer acto de gobierno, el 17 de febrero de 1817, fue dirigirse al pueblo en una proclama con alusión honorífica al general San Martín:

Gracias a la Batalla de Chacabuco, en la que los patriotas salieron victoriosos, pudieron entrar en Santiago de Chile el 14 de febrero. Finalizando el período de la Reconquista o "Restauración" y empezando el período de la Patria Nueva.




</doc>
<doc id="7033" url="https://es.wikipedia.org/wiki?curid=7033" title="Dato">
Dato

Un dato es una representación simbólica (numérica, alfabética, algorítmica, espacial, etc.) de un atributo o variable cuantitativa o cualitativa. Los datos describen hechos empíricos, sucesos y entidades.
Es un valor o referente que recibe el computador por diferentes medios, los datos representan la información que el programador manipula en la construcción de una solución o en el desarrollo de un algoritmo.

Los datos aisladamente pueden no contener información humanamente relevante. Solo cuando un conjunto de datos se examina conjuntamente a la luz de un enfoque, hipótesis o teoría se puede apreciar la información contenida en dichos datos. Los datos pueden consistir en números, estadísticas o proposiciones descriptivas. Los conceptos de datos, información, conocimientos y sabiduría están inter-relacionado, los datos convenientemente agrupados, estructurados e interpretados se han considerado que son la base de la información humanamente relevante que se pueden utilizar en la toma de las decisiones, la reducción de la incertidumbre o la realización de cálculos. Es de empleo muy común en el ámbito... informático En general, prácticamente en cualquier investigación científica. En programación, un dato es la expresión general que describe las características de las entidades sobre las cuales opera un algoritmo. En estructura de datos, es la parte mínima de la información. Se ha dicho que datos son el nuevo petróleo de la economía digital.

La virtualización de datos es un enfoque para unificar datos de múltiples fuentes en una sola capa para que las aplicaciones, las herramientas de informes y los usuarios finales puedan acceder a los datos sin requerir detalles sobre la fuente original, la ubicación y las estructuras de datos.


</doc>
<doc id="7034" url="https://es.wikipedia.org/wiki?curid=7034" title="Mediatriz">
Mediatriz

La mediatriz de un segmento es la línea recta perpendicular a dicho segmento trazada por su punto medio. Equivalentemente se puede definir como el lugar geométrico — la recta — cuyos puntos son equidistantes a los extremos del segmento. 

En efecto, sea formula_1 el segmento que sea, determinado por los puntos formula_2 y formula_3. Sea formula_4 el punto medio del segmento y formula_5 la recta perpendicular al segmento por dicho punto. Sea formula_6 un punto sobre la recta formula_5. En la simetría axial respecto de la recta formula_5, el punto formula_6 es invariante y los puntos formula_2 y formula_3 son uno el simétrico del otro. Por tanto, en esta simetría, el segmento formula_12 se transforma en el segmento formula_13, ambos segmentos son congruentes y el punto formula_6 equidista de los puntos formula_2 y formula_3. En consecuencia, todo punto que se encuentre sobre la recta formula_5 pertenece a la mediatriz del segmento en cuestión.

Recíprocamente, sea formula_1 un segmento y sea formula_6 un punto que equidista de formula_2 y de formula_3, esto es que los segmentos formula_12 y formula_13 son iguales. Consideremos la bisectriz formula_24 del ángulo formula_25 y sea formula_4 la intersección de dicha bisectriz con el segmento formula_1.

Por construcción, los ángulos formula_28 y formula_29 son iguales y en la simetría axial respecto de la recta formula_5 se transforman uno en el otro. Como los segmentos formula_31 y formula_32 son iguales, en esta simetría, los puntos formula_2 y formula_3 son uno la imagen del otro. Concluimos que el punto formula_4 es punto medio del segmento formula_1 y que dicho segmento es perpendicular a la recta formula_5.

Para trazar la mediatriz de un segmento dado "AB", se trazarán dos arcos de igual radio arbitrario (siempre mayores que la mitad de la longitud del segmento) con centros en los extremos del segmento. Los dos arcos se cortarán en dos puntos "C" y "D" que pertenecen a la mediatriz, puesto que cumplen la condición de equidistar de los extremos del segmento.
Las mediatrices de un polígono cíclico son las mediatrices de sus lados, es decir, las perpendiculares a los lados que pasan por sus puntos medios. Estas se cortan en un punto que se denomina circuncentro, el cual es el centro de la circunferencia que pasa por los vértices del polígono, es decir, de la circunferencia circunscrita al polígono.

Esto se debe a que la mediatriz de una cuerda dada en cualquier circunferencia pasa necesariamente por el centro de la misma. Aplicando las mediatrices a los lados del polígono cíclico como si de cuerdas de circunferencia se tratara, obtenemos que las intersecciones de las mismas constituyen el centro de la circunferencia que contiene todas ellas y por tanto, la circunferencia circunscrita.

No todos los polígonos simples convexos son polígonos cíclicos, entre los polígonos cíclicos se encuentran todos los triángulos, los cuadriláteros cíclicos y todos los polígonos regulares simples.

Por la propiedad antes mencionada, en todo triángulo "ABC" las mediatrices de sus tres lados concurren en un mismo punto, llamado el circuncentro ("O") del triángulo. Dicho punto equidista de los vértices del triángulo. La circunferencia de centro "O" y de radio "OA", pasa por los otros dos vértices del triángulo. Se dice que dicha circunferencia es circunscrita al triángulo y que el triángulo está inscrito en la circunferencia.





</doc>
<doc id="7043" url="https://es.wikipedia.org/wiki?curid=7043" title="Asteroide">
Asteroide

Un asteroide es un cuerpo celeste rocoso, más pequeño que un planeta y mayor que un meteoroide. La mayoría orbita entre Marte y Júpiter, en la región del sistema solar conocida como cinturón de asteroides; otros se acumulan en los puntos de Lagrange de Júpiter, y la mayor parte del resto cruza las órbitas de los planetas.

La palabra asteroide procede del griego, "ἀστεροειδής", y significa «de figura estelar», en referencia al aspecto que presentan cuando son vistos con un telescopio. Fue acuñada por William Herschel en 1802, aunque durante la mayor parte del siglo XIX los astrónomos los denominaran planetas. Hasta el 24 de marzo de 2006 a los asteroides se les llamaba también planetoides o planetas menores. Sin embargo, estos términos han caído en desuso.

Durante más de dos siglos, Ceres fue el primer asteroide descubierto. Tras la redefinición de planeta de 2006, que reclasificó a este cuerpo como planeta enano, técnicamente es Palas, encontrado en 1802, el primer asteroide descubierto. En estos dos siglos el número de asteroides conocidos no ha dejado de crecer, alcanzando valores de varios cientos de miles. No obstante, si se sumara toda su masa, el equivalente solo daría para un porcentaje del 5 % de toda la masa de la Luna.

Los asteroides se clasifican en función de su ubicación, composición o agrupamiento. Para la ubicación se toma como referencia la posición relativa de estos cuerpos respecto al Sol y los planetas. Para la composición se usan los datos extraídos de los espectros de absorción. Los agrupamientos se basan en los valores nominales similares del semieje mayor, la excentricidad y la inclinación de la órbita. Debido a su diminuto tamaño y gran distancia de la Tierra, casi todo lo que sabemos de ellos procede de medidas astrométricas y radiométricas, curvas de luz y espectros de absorción. Gaspra, en 1991, fue el primer asteroide visitado por una sonda espacial, mientras que dos años después Ida fue el primero en el que se confirmó la existencia de un satélite.

«Asteroide» es una palabra de origen griego, "ἀστεροειδής", que se puede traducir al español como «de forma estelar». Hace alusión al aspecto que ofrecen estos cuerpos vistos a través de un telescopio. Fue Herschel quien el 6 de mayo de 1802 propuso ante la Royal Society de Londres que tanto Ceres como Palas, únicos asteroides descubiertos hasta ese momento, eran un nuevo tipo de cuerpos, a los que llamó asteroides. Sin embargo, la mayoría de los astrónomos de la época rechazó la propuesta de Herschel por considerarla indigna, ridícula o sin precedentes, y continuaron considerándolos planetas. Giuseppe Piazzi, descubridor de Ceres, empleó el término planetoide y solo Heinrich Olbers secundó a Herschel. «Asteroide» no empezó a generalizarse hasta principios del siglo XX.

En 2013, Clifford Cunningham, en un encuentro de la división planetaria de la Sociedad Astronómica Americana, argumentó que la propuesta original procedía del especialista en griego Charles Burney. Según Cunningham, Herschel pidió sugerencias a varios amigos entre los que estaban Joseph Banks y Charles Burney. A su vez, Banks escribió a Stephen Weston, quien propuso el nombre «aorate», y Burney escribió a su hijo proponiendo nombres como «stellula» en clara alusión al diminuto tamaño de estos cuerpos. Posteriormente, Burney escribiría a Frances Crewe: "They are not allowed by Herschel to be either Planets or Comets, but asteroids, italick, a kind of star —a name my son, the Grecian, furnished". Finalmente Herschel se decidió por «asteroide» por ser "la mejor de un montón de malas ideas".

Durante siglos, astrónomos, físicos y matemáticos se preguntaron por el enorme vacío que había entre las órbitas de Marte y Júpiter, pero no fue hasta el siglo XIX que Piazzi dio una primera respuesta al descubrir Ceres. En el siglo siguiente, los astrónomos ya conocían miles de asteroides, principalmente agrupados en el cinturón de asteroides. Con la llegada de las búsquedas automatizadas a finales del siglo XX y principios del XXI, el número de asteroides conocido se disparó. En 2012, había más de seiscientas mil órbitas computadas.

El primer investigador que se ocupó del hueco que había entre las órbitas de Marte y Júpiter fue Johannes Kepler. Kepler formuló la hipótesis de que debía existir un planeta desconocido en ese espacio, aunque agregó que quizá no fuese suficiente con uno. Posteriormente otros científicos retomaron la cuestión. Isaac Newton opinaba que tanto Júpiter como Saturno habían sido puestos por influencia divina en el exterior del sistema solar para no perturbar las órbitas de los planetas interiores. El filósofo Immanuel Kant dijo que el espacio vacío estaba en proporción a la masa de Júpiter. Johann Heinrich Lambert pensaba que el hueco era quizá el resultado de la expulsión de algún hipotético planeta debido a la influencia gravitatoria de Júpiter y Saturno.

Ya en el siglo XVIII varios astrónomos estaban dispuestos a creer en la existencia de múltiples planetas desconocidos en el sistema solar. Sin embargo, fue Johann Daniel Titius, en 1766, el primero en aportar la explicación para la distancia entre las órbitas de Marte y Júpiter que con el tiempo se conocería como ley de Titius-Bode. La relación numérica atrajo la atención de Johann Elert Bode, quien no dudó de su validez y la publicó en 1772. El descubrimiento de Urano por William Herschel en 1781 a la distancia que vaticinaba la ley fue la confirmación definitiva de su fiabilidad y reforzó la creencia en la existencia de un planeta entre Marte y Júpiter.
Uno de los astrónomos que más interés se tomó en la localización del planeta fue el barón Franz Xaver von Zach, director del observatorio de Seeberg. Zach seleccionó la región zodiacal, preparó una mapa de estrellas que le permitiera determinar la presencia de nuevos objetos y calculó incluso una hipotética órbita para el desconocido planeta. En 1800, tras estériles resultados, convenció a otros astrónomos para que le ayudaran en la búsqueda. El 20 de septiembre de 1800 se constituyó la Vereinigte Astronomische Gesellschaft, conocida como Sociedad de Lilienthal, con el propósito de cartografiar la región del Zodiaco hasta las más débiles estrellas. Entre los miembros fundadores estaban Karl Ludwig Harding y Olbers, quienes más adelante descubrirían uno y dos asteroides respectivamente.

Para lograr sus fines, dividieron el Zodiaco en veinticuatro partes iguales y escogieron a otros astrónomos hasta completar la cifra de las divisiones. A estos astrónomos se les conoce como la policía celeste, aunque varios no llegaron a participar activamente en la búsqueda. Entre los seleccionados estaban Herschel y Piazzi, quien no recibió una invitación formal para unirse a la empresa, aunque a la postre fue el descubridor del nuevo planeta.

La noche del 1 de enero de 1801, mientras trabajaba en la composición de un catálogo de estrellas, Piazzi encontró un objeto en la constelación del Toro. Observó, en las noches sucesivas, que el objeto se movía sobre el fondo estelar. Al principio pensó que se trataba de un error, pero luego llegó a la conclusión de que había descubierto un cometa. El 4 de enero anunció a la prensa el hallazgo, gracias a lo cual varios astrónomos europeos, entre ellos Joseph Lalande quien pidió a Piazzi que le enviara sus observaciones, supieron la noticia a finales de febrero. Más adelante compartió sus observaciones por sendas cartas con Bode y Barnaba Oriani en las que mencionaba la ausencia de nebulosidad alrededor del objeto.

Con los datos que le aportaba Piazzi en su carta, Bode calculó una órbita preliminar. El 26 de marzo comunicó en la Academia Prusiana de las Ciencias que la órbita era consistente con el planeta que faltaba entre Marte y Júpiter y posteriormente informó a Zach para que lo publicase en "Monatliche Correspondenz". Llegó incluso a proponer el nombre de Juno para el nuevo planeta. Piazzi ya había bautizado su descubrimiento como Cerere Ferdinandea en honor a la diosa patrona de Sicilia y al rey Fernando. A la larga, la comunidad astronómica aceptó el nombre de Ceres para el nuevo objeto.
Lalande pasó las observaciones de Piazzi a Johann Karl Burckhardt quien calculó una órbita elíptica con ellas y envió sus resultados a Zach a primeros de junio. A finales del mismo mes, la comunidad astronómica estaba convencida de que Ceres era un nuevo planeta. Sin embargo, la tardanza de Piazzi en proporcionar los datos de sus observaciones frustraron los intentos de recuperarlo. Zach, en carta enviada a Oriani el 6 de julio, criticó a Piazzi por haber mantenido en secreto su trabajo. Para finales de agosto muchos astrónomos, en especial en Francia, dudaban de la existencia del objeto.

En septiembre se publicaron todas las observaciones de Piazzi. Carl Friedrich Gauss calculó una nueva órbita elíptica que mejoraba mucho la anteriormente obtenida por Burckhardt, quien en realidad trabajó con pocas observaciones. El 7 de diciembre Zach llegó a ver el planeta enano, pero el mal tiempo de los siguientes días le impidió continuar con sus observaciones. Finalmente, el 31 de diciembre Zach y el 2 de enero Olbers observaron independientemente Ceres en la posición predicha por los cálculos de Gauss, con lo que se confirmaba la existencia del objeto.

Unos meses después de la recuperación de Ceres, el 28 de marzo de 1802, Olbers encontraba otro objeto de características parecidas, pero con inclinación y excentricidad mayores. Dos días después estaba seguro de que se hallaba ante un nuevo planeta, al que denominó Palas, pues observó que se desplazaba respecto a las estrellas de fondo. El 4 de abril, Zach confirmó el descubrimiento de Olbers y extendió la noticia que fue enseguida aceptada por la mayoría de astrónomos europeos. Para tratar de casar la ley de Bode-Titius, cuyo fundamento físico, aunque desconocido, no había sido puesto en duda, con la presencia de dos cuerpos en lugar de uno, Olbers propuso que Ceres y Palas eran trozos de un planeta mayor que se había fragmentado por fuerzas internas o por un impacto.

La consecuencia inmediata de la teoría de Olbers fue que podrían existir más objetos entre las órbitas de Marte y Júpiter aún por descubrir. Así, Harding, tras constantes observaciones de la región del firmamento donde se cruzaban las órbitas de Ceres y Palas, terminó por encontrar a Juno el 1 de septiembre de 1804. Días después, Hofrath Huth, en una carta enviada a Bode, aventuraba que no sería el último descubrimiento y que estos cuerpos podrían haberse originado a la vez que el resto de planetas y de la misma forma, en contra de lo que postulaba Olbers.

Casi tres años después, Olbers descubrió un cuarto asteroide, Vesta, en la misma región del cielo y que ha resultado ser el más brillante. El nombre fue propuesto por Gauss. Estos cuatro descubrimientos reforzaron la teoría olbersiana, a pesar de ser objetivamente pocos. Sin embargo, ya en 1812, Joseph-Louis de Lagrange la cuestionaba, afirmando que era extraordinaria, pero improbable.

Tras los primeros descubrimientos, pasaron cerca de cuarenta años hasta que Karl Ludwig Hencke encontró el quinto tras cinco lustros de intensa búsqueda. Este largo lapso de tiempo se puede explicar por tres causas principales. En primer lugar, la mayoría de astrónomos, influidos por la teoría de Olbers, hicieron sus búsquedas en la misma región del espacio en las que se descubrieron los primeros cuerpos. En segundo, la búsqueda sistemática de nuevos planetas no fue considerada una prioridad astronómica, puesto que los primeros cuerpos se encontraron por accidente. Por último, la ausencia de buenas cartas celestes, donde se mostrase de forma inequívoca la posición de las estrellas, desalentó a los astrónomos porque no se tenía certeza de hallarse ante un nuevo planeta o una estrella.
Con el acceso a un número cada vez mayor de cartas celestes, los astrónomos dispusieron de medios para emprender la tarea con suficientes garantías. Así, en 1857 ya se habían descubierto cincuenta y el número cien se catalogó en 1868. El 22 de diciembre de 1891, Maximilian Franz Wolf descubrió Brucia mediante la astrofotografía, técnica que aceleró el aumento de la nómina de asteroides. Para 1923 ya había mil asteroides catalogados y en 1985 se registró el número tres mil. A finales del siglo XX, el refinamiento de las técnicas de observación y el empleo de programas automatizados, como Linear y Spacewatch, incrementó exponencialmente la cantidad de asteroides conocidos. En 1999 eran diez mil; en 2002, cincuenta mil; el número cien mil se catalogó en 2005; para 2014 ya eran cuatrocientos mil los cuerpos catalogados. Algunas estimaciones permiten suponer que haya más de un millón de asteroides con tamaños superiores a un kilómetro.

Mientras aumentaba el número de asteroides, los astrónomos se cuestionaban su origen. François Arago observó que las órbitas no se intersecaban en la misma región del espacio, lo que ponía en duda la teoría de Olbers, aunque admitió que el entrelazamiento de las órbitas sugería algún tipo de relación. Más adelante, en 1867, Daniel Kirkwood postuló que los asteroides se habían originado a partir de un anillo de materia que no llegó a formar un planeta debido a la influencia gravitatoria de Júpiter. Esta teoría terminó por convertirse en la dominante en los círculos astronómicos. El mismo Kirkwood encontró que no existían asteroides cuyos periodos de traslación tuviesen una relación de números enteros sencillos con Júpiter por lo que se producían huecos en la distribución de los asteroides. En 1918, Kiyotsugu Hirayama encontró similitudes en los parámetros orbitales de varios asteroides, concluyó que tenían un origen común, probablemente tras colisiones catastróficas, y llamó a estas agrupaciones familias de asteroides.

Los asteroides son cuerpos menores, rocosos y que orbitan alrededor del Sol a distancias inferiores a la de Neptuno. La mayoría está situada entre las órbitas de Marte y Júpiter. Tienen tamaños reducidos y formas irregulares, salvo algunos de mayor tamaño como Palas, Vesta o Higía que tienen formas ligeramente redondeadas.

Se originaron a partir de la colisión de cuerpos mayores que no llegaron a conformar un planeta por la influencia gravitatoria de Júpiter.

El tamaño de los asteroides varía entre los 1000 km del más grande hasta rocas de apenas una decena de metros. Los tres más grandes son similares a planetas en miniatura: Son más o menos esféricos, su interior está parcialmente diferenciado y se cree que son protoplanetas. Sin embargo, la gran mayoría son mucho más pequeños, de forma irregular y, o bien son restos supervivientes de los primitivos planetésimos, o bien fragmentos de cuerpos más grandes producidos tras colisiones catastróficas.

Ceres antes considerado el más grande asteroide, ha ingresado en la categoría de planeta enano. Por tanto, ahora los de mayor tamaño son Palas y Vesta, ambos con diámetros poco mayores de 500 km. Vesta, además, es el único asteroide del cinturón principal que, en ocasiones, puede verse a simple vista. En contadas ocasiones, asteroides cercanos a la Tierra como Apofis pueden verse con el ojo desnudo.

La masa de todos los asteroides del cinturón principal está estimada entre 2,8 y 3,2×10 kg; o, lo que es igual, un 4 % de la masa de la Luna. Ceres, con 9,5×10 kg, representa la tercera parte del total. Junto a Vesta (9 %), Palas (7 %) e Higía (3 %) alcanza a más de la mitad de la masa. Los siguientes tres asteroides Davida (1,2 %), Interamnia (1 %) y Europa (0,9 %) solo añaden otro 3 % a la masa total. A partir de aquí, el número de asteroides aumenta rápidamente al tiempo que sus masas individuales disminuyen.

El número de asteroides disminuye notablemente conforme aumenta el tamaño. Aunque esto sigue una distribución de potencias, hay saltos para los 5 y 100 km donde se encuentran más asteroides de lo esperado según una distribución logarítmica.

En la Figura se comparan los tamaños relativos entre un plantea, un planetesimal, cometas, asteroides, meteoroides y granos de polvo. En esta diagrama se compara con el asteroide Chicxulub, que según las simulaciones más recientes han acotado su tamaño entre 10 y 15 km . 

Los asteroides cercanos a la Tierra (NEA, acrónimo inglés de "Near-Earth Asteroids") son todos aquellos objetos astronómicos que tienen una órbita cercana a la Tierra y no son cometas. Hay más de asteroides conocidos con estas características con diámetros que varían desde un metro a los aproximadamente de Ganimedes. Los que superan el kilómetro se acercan a los 1000. Eros fue el primer asteroide de este grupo en ser descubierto.

Parte de estos cuerpos son residuos de cometas extinguidos. Otros NEA se cree que se originan en el cinturón de asteroides donde la influencia gravitatoria de Júpiter expulsa al sistema solar interior a los asteroides que caen en los huecos de Kirkwood. El efecto Yarkovsky contribuye a que el suministro de asteroides a las resonancias jovianas sea continuo.

La duración estimada de los NEA es de unos pocos millones de años. Su composición es comparable a la de los asteroides del cinturón principal o a la de los cometas de periodo corto.

Los NEA se dividen en tres grupos principales atendiendo al semieje mayor, perihelio y afelio.

Son aquellos que tienen un semieje mayor inferior a . El asteroide Atón da nombre al grupo. Si además no cruzan la órbita terrestre se les denomina asteroides Apohele, asteroides Atira u objetos interiores a la Tierra. Algunos asteroides de este grupo, como Cruithne, tienen órbitas similares a la terrestre.

Son aquellos que tienen un semieje mayor superior a y cruzan la órbita de la Tierra. El asteroide Apolo da nombre al grupo.

Son aquellos cuyo perihelio es mayor que el afelio terrestre e inferior a . El asteroide Amor.

Se llaman asteroides potencialmente peligrosos (PHA, acrónimo en inglés de "Potentially Hazardous Asteroids") a aquellos que se aproximan a la Tierra a menos de 0,05 ua y tienen una magnitud absoluta inferior a 22. El más grande de estos cuerpos es Toutatis.

El cinturón de asteroides es una región del sistema solar comprendida entre las órbitas de Marte y Júpiter. La mayor parte de los asteroides forma parte de él, a distancias comprendidas entre 2 y . Más de la mitad de la masa la constituyen el planeta enano Ceres y los grandes asteroides Palas, Vesta, Juno e Higía, aunque la masa total del cinturón apenas supone un de la masa de la Luna.

El cinturón de asteroides se formó en la nebulosa protosolar junto con el resto del sistema solar. Los fragmentos de material contenidos en la región del cinturón habrían podido formar un planeta, pero las perturbaciones gravitacionales de Júpiter, el planeta más masivo, produjeron que estos fragmentos colisionaran entre sí a grandes velocidades y no pudieran agruparse, resultando en el residuo rocoso que se observa en la actualidad. Una consecuencia de estas perturbaciones son los huecos de Kirkwood, zonas donde no se encuentran asteroides debido a resonancias orbitales con Júpiter que provocan que sus órbitas se tornen inestables.

El cinturón de asteroides está dividido en varias regiones según los límites que marcan las resonancias jovianas. Sin embargo, no todos los autores se ponen de acuerdo. Para la mayoría se divide en interior, exterior y medio o principal propiamente dicho, cuyos límites son las resonancias 4:1 y 2:1. A su vez, el cinturón principal se divide en tres zonas designadas con números romanos y limitadas por las resonancias 3:1 y 5:2. Una última resonancia, 7:3, marca una interrupción en la zona III. Algunos asteroides tienen órbitas tan excéntricas que llegan a cruzar la de Marte (en inglés, "Mars-crossing asteroids").

Los hungarias o asteroides del grupo de Hungaria son cuerpos menores situados entre 1,78 y 2,06 ua, con inclinaciones orbitales elevadas y excentricidad menor de 0,18. Son el resultado de una colisión catastrófica producida hace menos de quinientos millones de años y cuyo fragmento mayor es Hungaria, que da nombre al grupo, la región y la familia. La mayoría de los cuerpos de este conjunto pertenecen al grupo asteroidal de Hungaria. Son objetos muy brillantes, con magnitudes absolutas inferiores a 18 y pertenecen a los tipos espectrales E y X.

Los hildas o asteroides del grupo de Hilda son cuerpos menores que tienen resonancia orbital 3:2 con Júpiter y un semieje mayor comprendido entre 3,8 y 4,1 ua aproximadamente. Los miembros centrales del grupo tienen una alta estabilidad orbital y pertenecen a los tipos espectrales D y P. La mayoría está agrupado en las familias de Hilda y de Schubart.

Los asteroides troyanos son asteroides que comparten órbita con un planeta. Se distribuyen en dos regiones alargadas y curvas alrededor de los puntos estables de Lagrange y , situados 60° delante y detrás del planeta respectivamente. El nombre troyano se debe a que se estableció la convención de bautizar a los asteroides que ocupaban dichos puntos de la órbita de Júpiter con el nombre de los personajes de la guerra de Troya.

Tradicionalmente el término se ha referido a los asteroides troyanos de Júpiter, los primeros en ser descubiertos y los más numerosos hasta la fecha con diferencia. Sin embargo, con el descubrimiento de asteroides en los puntos de Lagrange de otros planetas del sistema solar, el término se ha extendido para englobarlos a todos. Solo Saturno y los planetas interiores a la Tierra no tienen asteroides troyanos confirmados. En el caso de los troyanos de Júpiter, los que anteceden al planeta pertenecen al grupo del campo griego y los que siguen al planeta al grupo del campo troyano.

Existen dos teorías para explicar su origen y ubicación. La primera indica que se formaron durante la última etapa de acreción planetaria en la misma región en la que se encuentran. La segunda establece que, durante la migración planetaria, el primitivo cinturón de Kuiper se desestabilizó y millones de objetos fueron expulsados al interior del sistema solar donde se incorporaron a los puntos de Lagrange de los planetas gaseosos.

, el 22 de febrero de 1906 en el punto de la órbita joviana, fue el primer troyano en ser descubierto. Hubo de transcurrir casi un siglo para descubrir troyanos de otros planetas. El 20 de junio de 1990 se encontró , primer troyano de Marte, y el 21 de agosto de 2001 se halló a , el primero de Neptuno. Más tarde se descubrieron sendos troyanos en las órbitas de la Tierra y Urano.

Se denominan centauros a un grupo de cuerpos menores que se encuentran en la parte exterior del sistema solar orbitando entre los grandes planetas. Quirón orbita entre Saturno y Urano, Damocles entre Marte y Urano.

Estos cuerpos tienen órbitas inestables, muy influidos gravitatoriamente por la gran masa de Júpiter.

El estudio de la luz reflejada por los asteroides proporciona indicios de sus composiciones superficiales. El análisis de los espectros de absorción de cientos de asteroides ha permitido clasificarlos en diferentes tipos atendiendo a diversos criterios, siendo los principales tipos “S”, “C”, “M”, “V” y “D”. Sin embargo, distintos materiales pueden tener similares espectros de absorción que, a su vez, pueden estar afectados por el estado de la superficie: si es porosa o compacta; si las rocas están más o menos fragmentadas; si está cubierta de polvo; o si ha sufrido una larga exposición a las radiaciones solar y cósmica. Los principales modelos de clasificación espectral son Tholen y SMASS.

Los asteroides del tipo S representan alrededor del de los asteroides conocidos y tienen un albedo promedio de 0,14. Contienen metales en su composición y son formados fundamentalmente por silicio. Abundan en la parte interna del Cinturón.

Los asteroides del tipo C tienen un albedo menor que 0,04 y constituyen más de la mitad de los asteroides conocidos. Son extremadamente oscuros y semejantes a meteoritos. Contienen rocas con un elevado porcentaje de carbono.

Los asteroides del tipo M son brillantes (albedos entre 0,10 y 0,18), ricos en metales (principalmente níquel y hierro) y parecen proceder del núcleo de asteroides diferenciados.

Estos asteroides, también llamados vestoides, son objetos astronómicos cuyo espectro es muy similar al de Vesta, el más grande con diferencia. La mayoría tiene valores de excentricidad e inclinación de la órbita parecidos a los de Vesta y un rango del semieje mayor entre 2,18 y 2,5 unidades astronómicas (hueco de Kirkwood 3:1). Esto permite suponer un origen común tras un gran impacto sobre Vesta. Son relativamente brillantes y en composición están equiparados a los asteroides del tipo S, pero contienen más piroxeno. Están relacionados con los meteoritos HED.

Los asteroides del tipo D tienen un albedo muy bajo (comprendido entre 0,02 y 0,05). Son muy rojos en longitudes de onda largas, debido quizás a la presencia de materiales con gran cantidad de carbono. Son muy raros en el cinturón principal y se les encuentra con mayor frecuencia en distancias superiores a 3,3 unidades astronómicas del Sol, donde su período orbital es del orden de la mitad del de Júpiter; es decir, están en las proximidades de la resonancia 2:1.

Las familias de asteroides son agrupaciones de asteroides que comparten similares valores de semieje mayor, excentricidad e inclinación orbital. Generalmente, se nombran a partir del asteroide con menor número que forma parte de la familia. Fueron definidas por primera vez en 1918 por Kiyotsugu Hirayama quien identificó las cinco familias que aún se llaman familias de Hirayama: la familia de Coronis, la familia de Eos, la familia de Temis, la familia de Flora y la familia de María.

Se originan por las colisiones entre los asteroides. La edad media de las familias de asteroides es del orden de mil millones de años.

Algunos asteroides tienen satélites a su alrededor como Ida y su satélite Dactyl; o Silvia y sus dos satélites, Rómulo y Remo. Rómulo, descubierto el 18 de febrero de 2001 en el telescopio W. M. Keck II de 10 m en Mauna Kea, tiene |km]] de diámetro y su órbita, a una distancia de de Silvia, tarda en completarse 87,6 horas. Remo, la segunda luna, tiene de diámetro y gira a una distancia de tardando 33 horas en completar una órbita alrededor de Silvia.

Los Asteroides Cercanos a la Tierra ("Near Earth Asteroids" o "NEA") se dividen en tres categorías: Atones, Apolos y Amores, siguiendo el nombre de cada prototipo (Atón, Apolo y Amor). Bajo ciertas condiciones sería posible un impacto con nuestro planeta. Si además consideramos a los cometas, generalmente menos masivos pero igualmente con gran poder destructor, el grupo que los incluye a todos se llama Objetos Cercanos a la Tierra, en inglés "Near Earth Objects" ("NEO").

Actualmente existen unos 4000 objetos catalogados como NEO, según «NeoDys» ("Near Earth Objects - Dynamic Site"), un proyecto de la Universidad de Pisa que proporciona información actualizada de este tipo de astros. Finalmente, si un NEA se aproxima a menos de 0,05 unidades astronómicas (7 millones y medio de kilómetros) de la Tierra, se le denomina PHA (asteroide potencialmente peligroso, por sus siglas en inglés). De ellos hay clasificados unos 800 en la actualidad y son los que representan un peligro para la civilización si en verdad alguno llegara a chocar contra nuestro planeta, ya que afectaría de manera global al mismo. Sin embargo, los cálculos de las trayectorias y de cada aproximación a la Tierra tienen grandes incertidumbres, debido a que los elementos orbitales (semiejes mayor y menor, distancia mínima al Sol, excentricidad, entre otros) no se conocen con total precisión, de manera que cualquier predicción está sujeta a un margen de error considerable.

De hecho, el PHA que durante los pasados años ha representado el mayor peligro, denominado 1950 DA, ya no se clasifica como tal y dejó recientemente de ser un PHA. Hasta hace poco se pensaba que existía cierta posibilidad de que impactara contra nuestro planeta el año 2880; sin embargo, el refinamiento de los elementos orbitales ha permitido que nos demos cuenta de que tal evento no ocurrirá. Otros PHA conocidos poseen probabilidades muy bajas de llegar a chocar con la Tierra. De hecho ninguno está por encima del umbral de ruido (esto es, la posibilidad no es significativa). Lo que no quiere decir que en cualquier momento un cálculo más preciso de la trayectoria de uno de ellos, lo cual requiere observaciones precisas y continuadas, o el descubrimiento de un nuevo PHA, indique que el impacto llegue a ocurrir. De ahí la importancia de los grandes proyectos que coordinen observaciones sistemáticas del cielo y el mantenimiento de bases de datos actualizadas.

En España existe un centro dedicado casi exclusivamente a este tema que está ubicado en el Observatorio Astronómico de La Sagra, situado en plena montaña (a una altura de cerca de Puebla de Don Fadrique, en la provincia de Granada, miembro de la asociación internacional "Spaceguard Foundation".

Por otro lado, dependiendo de la distancia relativa entre la superficie de la Tierra y la roca espacial (asteroide o comenta o meteoroide) tienen diferentes nombres. En la Figura se muestra que a los restos de la roca que quedaron después de quemarse en la atmósfera y chocar con la superficie se llama meteorito. Si la roca está en la atmósfera se llama Meteoro o lluvia de estrellas o Bólido cuando es muy brillante.
Hasta la llegada de los viajes espaciales, los objetos del cinturón de asteroides no eran más que simples puntos de luz, incluso para los más grandes telescopios, y sus formas y composición eran meramente especulativas. Los mejores telescopios terrestres y el telescopio espacial Hubble, en órbita terrestre, son capaces de resolver unos pocos detalles de las superficies de los asteroides más grandes, pero aun en este caso la mayoría de esos detalles solo son manchas borrosas. Algo más de información sobre la composición y la forma se consigue deducir de la curva de luz y de las características espectrales. El tamaño del asteroide se puede saber midiendo el tiempo que duran las ocultaciones estelares —cuando un asteroide pasa delante de una estrella— y calculando la distancia del asteroide a la Tierra. Las imágenes de radar proporcionan excelentes datos de las formas y los parámetros orbitales y rotacionales, especialmente de los asteroides cercanos a la Tierra. En cuanto a los requisitos de delta-v y propulsión, los NEO son cuerpos más accesibles que la Luna.

Las primeras imágenes en primer plano de objetos similares a los asteroides se tomaron en 1971 cuando la sonda espacial sacó fotografías de Fobos y Deimos, los dos pequeños satélites de Marte, que son probablemente asteroides capturados. Estas imágenes, al igual que las obtenidas por las Voyager de los pequeños satélites de los gigantes gaseosos, revelaron la forma irregular de estos cuerpos.
La sonda Galileo en ruta hacia Júpiter tomó las primeras fotografías cercanas a un asteroide el 29 de octubre de 1991 durante el sobrevuelo del asteroide . Posteriormente, el 28 de marzo de 1993, hizo lo propio con donde además descubrió , el primer satélite asteroidal confirmado. La primera sonda espacial dedicada exclusivamente a la exploración asteroidal fue la NEAR Shoemaker. Sobrevoló el 27 de junio de 1997 y entró en órbita de el 14 de febrero de 2000 para aterrizar en su superficie un año más tarde, el 12 de febrero de 2001. Otros asteroides visitados por sondas de camino a sus objetivos han sido por la Deep Space 1 el 28 de julio de 1999, por la Stardust el 2 de noviembre de 2002, y por la Rosetta el 5 de septiembre de 2008 y el 10 de julio de 2010 respectivamente y el 13 de diciembre de 2012 por la Chang'e 2.

El 13 de junio de 2010 la sonda Hayabusa trajo a la Tierra material del asteroide , lo que permitió establecer un vínculo entre los meteoritos condríticos y los asteroides de. Esta fue la primera vez que una misión espacial traía a la Tierra materiales de un asteroide. Anteriormente, los meteoritos habían sido la única fuente de muestras procedentes de los asteroides.

La sonda Dawn fue lanzada el 27 de septiembre de 2007 con destino y Ceres. Estuvo en órbita alrededor de Vesta entre el 16 de julio de 2011 y el 5 de septiembre de 2012. En este periodo descubrió un enorme cráter en el hemisferio sur cuyo pico central es una de las montañas conocidas más altas del sistema solar. Tras abandonar Vesta, emprendió viaje a Ceres. El 6 de marzo de 2015 entró en órbita alrededor del planeta enano. Está previsto que la misión primaria concluya en julio del mismo año.

La Agencia Japonesa de Exploración Aeroespacial (JAXA) lanzó el 3 de diciembre de 2014 la sonda Hayabusa 2 con el objetivo de traer a la Tierra una muestra de material del asteroide Ryugu, un objeto perteneciente a la clase de los asteroides de y considerado por el Minor Planet Center como un asteroide potencialmente peligroso. Está previsto que alcance el asteroide en 2018, que abandone la órbita asteroidal un año más tarde y que retorne a la Tierra en 2020. Esta será la segunda vez que una sonda espacial retorna con muestras materiales de un asteroide.

A finales de 2016 está previsto el lanzamiento de la sonda OSIRIS-REx de la NASA con destino al asteroide Bennu, perteneciente al grupo de los asteroides Apolo. El objetivo de la misión es ampliar los conocimientos científicos en formación planetaria y origen de la vida, así como traer material superficial para mejorar la comprensión de los asteroides que podrían impactar contra la Tierra. La llegada de la sonda al asteroide y su inserción en órbita están previstas para finales de 2018 y el regreso a la Tierra para 2023. Esta será la tercera vez que una sonda espacial retorna con muestras materiales de un asteroide.

En principio, cuando un asteroide es descubierto recibe del «Centro de Planetas Menores» (Minor Planet Center (MPC) un nombre provisional compuesto de una clave que indica el año, el mes y orden del descubrimiento. Esta denominación consta de un número, que es el año, y de dos letras: la primera indicando la quincena en que aconteció el avistamiento y la segunda reflejando la secuencia dentro de la quincena. De este modo, 1989 AC indica que fue descubierto en la primera quincena de enero (A) de 1989, y que fue el tercero (C) descubierto en ese período.

Una vez que la órbita se ha establecido con la suficiente precisión como para poder predecir su futura trayectoria, se le asigna un número (no necesariamente el del orden en que fue descubierto) y, más tarde, un nombre permanente elegido por el descubridor y aprobado por un comité de la Unión Astronómica Internacional (International Astronomical Union (IAU). Inicialmente, todos los nombres con los que se bautizaba a los asteroides eran de personajes femeninos de la mitología griega y romana pero pronto se optó por formas más modernas. El primer asteroide que recibió un nombre no mitológico fue el número 125 de la serie, Liberatrix ("liberadora" en latín) que le fue otorgado en honor a Juana de Arco, aunque también se especula con que tal nombre es un homenaje al primer presidente de la República Francesa, Adolphe Thiers. Por su parte, el primer nombre masculino, lo recibió el número 433, Eros. Hoy en día, las denominaciones son mucho menos restringidas y van desde nombres de ciudades y países como Barcelona (945), Hiroshima (2247), Austria (132), China (1125) y Uganda (1279) hasta nombres de personas famosas como Zamenhof (1462) o Piazzia (1000) en honor a Piazzi, personajes de ficción como Mr. Spock (2309) y otros conceptos como razas, género géneros de animales y plantas, etc. Sin embargo se ha acordado que hay ciertos nombres y temas que están prohibidos: por ejemplo el de militares, personajes o lugares de la II Guerra Mundial ya que la referencia a los mismos puede ser molesta o incluso insultante para los demás. Actualmente con la propuesta del nombre se acompaña una corta nota que informa a la comunidad internacional del porqué de dicha denominación: p. ej. «Snoopy: nombre de un personaje de ficción, concretamente un perro blanco de orejas colgantes, que acompaña a Charlie Brown y suele reflexionar sobre el tejado de la caseta en la que vive».

Las efemérides de los asteroides se recogen anualmente en un volumen titulado "Ephemerides of Minor Planets", que publica el Institute of Theoretical Astronomy, Russian Academy of Sciences, Naberezhnaya Kutuzova 10, 191187 San Petersburgo, Rusia.

En ocasiones una serie de asteroides numerados consecutivamente se nombran siguiendo un patrón o en recuerdo de un acontecimiento. Así, las iniciales de los numerados del (1227) al (1234) forman el apellido de Gustav Stracke y las del (8585) al (8600) la frase latina «per aspera ad astra». Los asteroides numerados del (3350) al (3356) y los del (51823) al (51829) se nombraron en recuerdo de los astronautas fallecidos en los accidentes del Challenger y el Columbia respectivamente.




</doc>
<doc id="7044" url="https://es.wikipedia.org/wiki?curid=7044" title="Observatorio Fabra">
Observatorio Fabra

El Observatorio Fabra (en catalán "Observatori Fabra") es un observatorio astronómico situado en Barcelona, sobre un contrafuerte de la montaña del Tibidabo, encarado al sur, a 415 msnm. Es propiedad de la Real Academia de Ciencias y Artes de Barcelona y debe su nombre al mecenas que hizo posible su construcción, el industrial Camil Fabra.

Su actividad científica se centra en la actualidad en el estudio de asteroides y cometas. Es el cuarto observatorio más antiguo del mundo que sigue en activo.

En 1894 la Real Academia de Ciencias y Artes de Barcelona puso en marcha el proyecto de un nuevo observatorio astronómico situado en el Tibidabo. Pocos años antes había completado el nuevo edificio central de la Academia, situado en La Rambla y sobre el que se instalaron dos observatorios diseñados por José Joaquín de Landerer. Sin embargo, pronto se vio que los nuevos observatorios eran obsoletos casi desde el momento de su inauguración. La Academia propuso el proyecto a la Diputación de Barcelona, que lo rechazó. Entonces se inició una campaña de prensa para concienciar al público de la necesidad de una institución de investigación de este tipo y de nivel internacional.

El impulso definitivo llegó cuando, en 1901, el marqués de Alella, Camilo Fabra, industrial de prestigio y exalcalde de Barcelona, donó 250 000 pesetas a la Academia con el objetivo de hacer realidad el proyecto. En 1902 comenzaron las obras, no exentas de polémica, ya que el proyecto original de Eduard Fontserè fue revisado por José Comas y Solá, en una época de enfrentamiento entre ambos científicos. El edificio es obra del arquitecto José Doménech y Estapá y fue inaugurado el 7 de abril de 1904, con la presencia del rey Alfonso XIII. Al principio se llamó 'Observatorio del Tibidabo', pero al poco tiempo se cambió el nombre por el de 'Observatorio Fabra'.

Inicialmente el observatorio se organizó en dos secciones: la Sección Astronómica y la Sección Meteorológica y Sísmica. Comas y Solá fue nombrado director de la Sección Astronómica y también, interinamente, director de la Sección Meteorológica y Sísmica hasta que en 1912 se hizo cargo de ella Eduard Fontserè. Hasta la Guerra Civil el observatorio (que durante la Segunda República quedó integrado en el Servicio de Astronomía de Cataluña, del cual Comas y Solá fue director) disfrutó del período de mayor relevancia científica. En cuanto a la sismología, inició el estudio sistemático de la sismología catalana y pirenaica. En el aspecto meteorológico comenzó las observaciones meteorológicas diarias en octubre de 1913, en una serie ininterrumpida hasta la actualidad, incluso durante la Guerra Civil (hasta 1981 se hacía una observación diaria a las 8 de la mañana, en 1982 se pasó a 3 y finalmente a 4 observaciones diarias). En cuanto a la astronomía, se realizaron importantes observaciones entre las que cabe destacar el estudio del planeta Marte, el descubrimiento de once planetas menores y de un cometa (el 32P/Comas Solá) y el estudio de estrellas dobles y variables.

En 1937, tras la muerte de Comas y Solá, la dirección del observatorio pasó a Isidro Pólit y, posteriormente, a Joaquín Febrer. A partir de los años 40, la actividad científica se vio reducida, en parte por la falta de presupuesto y de condiciones adecuadas y en parte por la evolución de la observación astronómica, que dejó obsoletos los instrumentos del observatorio para la investigación de primer nivel. Actualmente centra su actividad de investigación astronómica en la observación de estrellas dobles y en la participación en programas internacionales centrados en el estudio y seguimiento de asteroides y cometas (coordinados por el Minor Planet Center y por el Observatorio de Púlkovo. También recopila datos meteorológicos y sismológicos. Además, en la línea inaugurada por el propio Comas y Solá, en los últimos años ha recuperado el papel de centro de divulgación de la astronomía, con visitas y charlas aprovechando la doble vertiente de centro astronómico en activo y museo de historia de la astronomía.

Instrumentos astronómicos:

Instrumentos sísmicos:




</doc>
<doc id="7052" url="https://es.wikipedia.org/wiki?curid=7052" title="Renoir">
Renoir

Renoir puede referirse a:


</doc>
<doc id="7054" url="https://es.wikipedia.org/wiki?curid=7054" title="Generación de energía eléctrica">
Generación de energía eléctrica

En general, la generación de energía eléctrica consiste en transformar alguna clase de energía (química, cinética, térmica, lumínica, nuclear, solar entre otras), en energía eléctrica. Para la generación industrial se recurre a instalaciones denominadas centrales eléctricas, que ejecutan alguna de las transformaciones citadas.
Estas constituyen el primer escalón del sistema de suministro eléctrico.
La generación eléctrica se realiza, básicamente, mediante un generador eléctrico; si bien estos no difieren entre sí en cuanto a su principio de funcionamiento, varían en función a la forma en que se accionan.

Desde que se descubrió la corriente alterna y la forma de producirla en los alternadores, se ha llevado a cabo una inmensa actividad tecnológica para llevar la energía eléctrica a todos los lugares habitados del mundo, por lo que, junto a la construcción de grandes y variadas centrales eléctricas, se han construido sofisticadas redes de transporte y sistemas de distribución. Sin embargo, el aprovechamiento ha sido y sigue siendo muy desigual en todo el planeta. Así, los países industrializados o del primer mundo son grandes consumidores de energía eléctrica, mientras que los países en vías de desarrollo apenas disfrutan de sus ventajas.

La demanda de energía eléctrica de una ciudad, región o país tiene una variación a lo largo del día. Esta variación es función de muchos factores, entre los que destacan: tipos de industrias existentes en la zona y turnos que realizan en su producción, climatología extremas de frío o calor, tipo de electrodomésticos que se utilizan más frecuentemente, tipo de calentador de agua que haya instalado en los hogares, la estación del año y la hora del día en que se considera la demanda. La generación de energía eléctrica debe seguir la curva de demanda y, a medida que aumenta la potencia demandada, se debe incrementar la potencia suministrada. Esto conlleva el tener que iniciar la generación con unidades adicionales, ubicadas en la misma central o en centrales reservadas para estos períodos. En general los sistemas de generación se diferencian por el periodo del ciclo en el que está planificado que sean utilizados; se consideran de base la nuclear y la eólica, de valle la termoeléctrica de combustibles fósiles, y de pico la hidroeléctrica principalmente (los combustibles fósiles y la hidroeléctrica también pueden usarse como base si es necesario).

Dependiendo de la fuente primaria de energía utilizada, las centrales generadoras se clasifican en químicas cuando se utilizan plantas de radioactividad, que generan energía eléctrica con el contacto de esta, termoeléctricas (de carbón, petróleo, gas, nucleares y solares termoeléctricas), hidroeléctricas (aprovechando las corrientes de los ríos o del mar: mareomotrices), eólicas y solares fotovoltaicas. La mayor parte de la energía eléctrica generada a nivel mundial proviene de los dos primeros tipos de centrales reseñados. Todas estas centrales, excepto las fotovoltaicas, tienen en común el elemento generador, constituido por un alternador de corriente, movido mediante una turbina que será distinta dependiendo del tipo de energía primaria utilizada.

Por otro lado, un 64 % de los directivos de las principales empresas eléctricas consideran que en el horizonte de 2018 existirán tecnologías limpias, WN, accesibles y renovables de generación local, lo que obligará a las grandes corporaciones del sector a un cambio de mentalidad.

Una central termoeléctrica es un lugar empleado para la generación de energía eléctrica a partir de calor. Este calor puede obtenerse tanto de la combustión, de la fisión nuclear del uranio u otro combustible nuclear, del sol o del interior de la Tierra. Las centrales que en el futuro utilicen la fusión también serán centrales termoeléctricas. Los combustibles más comunes son los combustibles fósiles (petróleo, gas natural o carbón), sus derivados (gasolina, gasóleo), biocarburantes, residuos sólidos urbanos, metano generado en algunas estaciones depuradoras de aguas residuales.

Las centrales termoeléctricas consisten en una caldera en la que se quema el combustible para generar calor que se transfiere a unos tubos por donde circula agua, la cual se evapora. El vapor obtenido, a alta presión y temperatura, se expande a continuación en una turbina de vapor, cuyo movimiento impulsa un alternador que genera la electricidad. Luego el vapor es enfriado en un condensador donde circula por tubos agua fría de un caudal abierto de un río o por torre de refrigeración.

En las centrales termoeléctricas denominadas de ciclo combinado se usan los gases de la combustión del gas natural para mover una turbina de gas. En una cámara de combustión se quema el gas natural y se inyecta aire para acelerar la velocidad de los gases y mover la turbina de gas. Como, tras pasar por la turbina, esos gases todavía se encuentran a alta temperatura (500 °C), se reutilizan para generar vapor que mueve una turbina de vapor. Cada una de estas turbinas impulsa un alternador, como en una central termoeléctrica común. El vapor luego es enfriado por medio de un caudal de agua abierto o torre de refrigeración como en una central térmica común. Además, se puede obtener la cogeneración en este tipo de plantas, al alternar entre la generación por medio de gas natural o carbón. Este tipo de plantas está en capacidad de producir energía más allá de la limitación de uno de los dos insumos y pueden dar un paso a la utilización de fuentes de energía por insumos diferentes.

Las centrales térmicas que usan combustión liberan a la atmósfera dióxido de carbono (CO), considerado el principal gas responsable del calentamiento global. También, dependiendo del combustible utilizado, pueden emitir otros contaminantes como óxidos de azufre, óxidos de nitrógeno, partículas sólidas (polvo) y cantidades variables de residuos sólidos. Las centrales nucleares generan residuos radiactivos de diversa índole que requieren una disposición final de máxima seguridad y pueden contaminar en situaciones accidentales (véase accidente de Chernóbil).

Una central térmica solar o central termosolar es una instalación industrial en la que, a partir del calentamiento de un fluido mediante radiación solar y su uso en un ciclo termodinámico convencional, se produce la potencia necesaria para mover un alternador para la generación de energía eléctrica como en una central térmica clásica. En ellas es necesario concentrar la radiación solar para que se puedan alcanzar temperaturas elevadas, de 300 °C hasta 1000 °C, y obtener así un rendimiento aceptable en el ciclo termodinámico, que no se podría obtener con temperaturas más bajas. La captación y concentración de los rayos solares se hacen por medio de espejos con orientación automática que apuntan a una torre central donde se calienta el fluido, o con mecanismos más pequeños de geometría parabólica. El conjunto de la superficie reflectante y su dispositivo de orientación se denomina heliostato. Su principal problema medioambiental es la necesidad de grandes extensiones de territorio que dejan de ser útiles para otros usos (agrícolas, forestales, etc.).

La energía geotérmica es aquella energía que puede obtenerse mediante el aprovechamiento del calor del interior de la Tierra. El término "geotérmico" viene del griego "geo" (Tierra), y "thermos" (calor). Este calor interno calienta hasta las capas de agua más profundas: al ascender, el agua caliente o el vapor producen manifestaciones, como los géiseres o las fuentes termales, utilizadas para calefacción desde la época de los romanos. Hoy en día, los progresos en los métodos de perforación y bombeo permiten explotar la energía geotérmica en numerosos lugares del mundo. Para aprovechar esta energía en centrales de gran escala, es necesario que se den temperaturas muy elevadas a poca profundidad.

Una central o planta nuclear o atómica es una instalación industrial empleada para la generación de energía eléctrica a partir de energía nuclear. Se caracteriza por el empleo de combustible nuclear fisionable que mediante reacciones nucleares proporciona calor que a su vez es empleado, a través de un ciclo termodinámico convencional, para producir el movimiento de alternadores que transforman el trabajo mecánico en energía eléctrica. Estas centrales constan de uno o más reactores.

Una central hidroeléctrica es aquella que se utiliza para la generación de energía eléctrica mediante el aprovechamiento de la energía potencial del agua embalsada en una presa situada a más alto nivel que la central. El agua se lleva por una tubería de descarga a la sala de máquinas de la central, donde mediante enormes turbinas hidráulicas se produce la electricidad en alternadores y el agua regresa a su cauce natural tras la salida de las turbinas. Las dos características principales de una central hidroeléctrica, desde el punto de vista de su capacidad de generación de electricidad son:

La potencia de una central hidroeléctrica puede variar desde unos pocos MW, hasta varios GW. Hasta 10 MW se consideran minicentrales. En China se encuentra la mayor central hidroeléctrica del mundo (la Presa de las Tres Gargantas), con una potencia instalada de 22.500 MW. La segunda es la Represa de Itaipú (que pertenece a Brasil y Paraguay), con una potencia instalada de 14.000 MW en 20 turbinas de 700 MW cada una.

Esta forma de energía, en ocasiones, posee problemas medioambientales al necesitar la construcción de grandes embalses en los que acumular el agua, que es sustraída de otros usos, incluso urbanos en algunas ocasiones; sin embargo, otra alternativa utilizada, sobre todo en minihidroeléctricas, son las minicentrales al filo de agua, las cuales consisten en la realización de una obra de toma mediante rejillas, compuertas y una estructura desarenadora, en la que se embalsa el caudal de agua necesario para la operación de las turbinas, dejando correr el caudal restante por el cauce natural del río, afectando así mínimamente el entorno.

Las centrales mareomotrices utilizan el flujo y reflujo de las mareas. En general, pueden ser útiles en zonas costeras donde la amplitud de la marea sea alta y las condiciones morfológicas de la costa permitan la construcción de una presa que corte la entrada y salida de la marea en una bahía. Se genera energía tanto en el momento del llenado como en el momento del vaciado de la bahía.

Actualmente se encuentra en desarrollo la explotación comercial de la conversión en electricidad del potencial energético que tiene el oleaje del mar, en las llamadas centrales undimotrices.

La energía eólica se obtiene mediante el movimiento del aire, es decir, de la energía cinética generada por efecto de las corrientes de aire o de las vibraciones que el dicho viento produce. Los molinos de viento se han usado desde hace muchos siglos para moler el grano, bombear agua u otras tareas que requieren una energía. En la actualidad se usan aerogeneradores para generar electricidad, especialmente en áreas expuestas a vientos frecuentes, como zonas costeras, alturas montañosas o islas. La energía del viento está relacionada con el movimiento de las masas de aire que se desplazan de áreas de alta presión atmosférica hacia áreas adyacentes de baja presión, con velocidades proporcionales al gradiente de presión.

El impacto medioambiental de este sistema de obtención de energía es relativamente bajo, pudiéndose nombrar el impacto estético, porque deforman el paisaje, la muerte de aves por choque con las aspas de los molinos o la necesidad de extensiones grandes de territorio que se sustraen de otros usos. Además, este tipo de energía, al igual que la solar o la hidroeléctrica, están fuertemente condicionadas por las condiciones climatológicas, siendo aleatoria la disponibilidad de las mismas.

Se denomina energía solar fotovoltaica a la obtención de energía eléctrica a través de paneles fotovoltaicos. Los paneles, módulos o colectores fotovoltaicos están formados por dispositivos semiconductores tipo diodo que, al recibir radiación solar, se excitan y provocan saltos electrónicos, generando una pequeña diferencia de potencial en sus extremos. El acoplamiento en serie de varios de estos fotodiodos permite la obtención de voltajes mayores en configuraciones muy sencillas y aptas para alimentar pequeños dispositivos electrónicos. A mayor escala, la corriente eléctrica continua que proporcionan los paneles fotovoltaicos se puede transformar en corriente alterna e inyectar en la red eléctrica.

La implantación de la energía solar fotovoltaica ha avanzado considerablemente en los últimos años.

Los principales problemas de este tipo de energía son la necesidad de extensiones grandes de territorio que se sustraen de otros usos y su dependencia con las condiciones climatológicas. Este último problema hace que sean necesarios sistemas de almacenamiento de energía para que la potencia generada en un momento determinado, pueda usarse cuando se solicite su consumo. Se están estudiando sistemas como el almacenamiento cinético, bombeo de agua a presas elevadas y almacenamiento químico, entre otros.

Un grupo electrógeno es una máquina que mueve un generador de energía eléctrica a través de un motor de combustión interna. Es comúnmente utilizado cuando hay déficit en la generación de energía de algún lugar, o cuando hay corte en el suministro eléctrico y es necesario mantener la actividad. Una de sus utilidades más comunes es en aquellos lugares donde no hay suministro a través de la red eléctrica, generalmente son zonas agrícolas con pocas infraestructuras o viviendas aisladas. Otro caso es en locales de pública concurrencia, hospitales, fábricas, etc., que, a falta de energía eléctrica de red, necesiten de otra fuente de energía alterna para abastecerse en caso de emergencia. Un grupo electrógeno consta de las siguientes partes:

Se denomina ordinariamente pila eléctrica a un dispositivo que genera energía eléctrica por un proceso químico transitorio, tras de lo cual cesa su actividad y han de renovarse sus elementos constituyentes, puesto que sus características resultan alteradas durante el mismo. Se trata de un generador primario. Esta energía resulta accesible mediante dos terminales que tiene la pila, llamados polos, electrodos o bornes. Uno de ellos es el polo negativo o cátodo y el otro es el polo positivo o ánodo. En español es habitual llamarla así, mientras que las pilas recargables o acumuladores, se ha venido llamando batería.

La primera pila eléctrica fue dada a conocer al mundo por Volta en 1800, mediante una carta que envió al presidente de la "Royal Society" londinense, por tanto son elementos provenientes de los primeros tiempos de la electricidad. Aunque la apariencia de una pila sea simple, la explicación de su funcionamiento dista de serlo y motivó una gran actividad científica en los siglos XIX y XX, así como diversas teorías, y la demanda creciente que tiene este producto en el mercado sigue haciendo de él objeto de investigación intensa.

El funcionamiento de una pila se basa en el potencial de contacto entre dos sustancias, mediado por un electrolito. Cuando se necesita una corriente mayor que la que puede suministrar un elemento único, siendo su tensión en cambio la adecuada, se pueden añadir otros elementos en la conexión llamada en paralelo. La capacidad total de una pila se mide en amperios-hora (A•h); es el número máximo de amperios que el elemento puede suministrar en una hora. Es un valor que no suele conocerse, ya que no es muy claro dado que depende de la intensidad solicitada y la temperatura.

Un importante avance en la calidad de las pilas ha sido la pila denominada seca, al que pertenecen prácticamente todas las utilizadas hoy día. Las pilas eléctricas, baterías y acumuladores se presentan en unas cuantas formas normalizadas en función de su forma, tensión y capacidad que tengan.

Los metales y productos químicos constituyentes de las pilas pueden resultar perjudiciales para el medio ambiente, produciendo contaminación química. Es muy importante no tirarlas a la basura (en algunos países no está permitido), sino llevarlas a centros de reciclado. En algunos países, la mayoría de los proveedores y tiendas especializadas también se hacen cargo de las pilas gastadas. Una vez que la envoltura metálica que recubre las pilas se daña, las sustancias químicas que contienen se ven liberadas al medio ambiente causando contaminación. Con mayor o menor grado, las sustancias son absorbidas por la tierra pudiéndose filtrar hacia los mantos acuíferos y de estos pueden pasar directamente a los seres vivos, entrando con esto en la cadena alimenticia. Las pilas son residuos peligrosos por lo que desde el momento en que se empiezan a reunir, deben ser manejadas por personal capacitado que siga las precauciones adecuadas empleando todos los procedimientos técnicos y legales para el manejo de dicho residuos.

Estas pilas suelen utilizarse en los aparatos eléctricos portátiles, que son una gran cantidad de dispositivos que se han inventado y que se nutren para su funcionamiento de la energía facilitada por una o varias pilas eléctricas o de baterías recargables. Entre los dispositivos de uso masivo destacan juguetes, linternas, relojes, teléfonos móviles, marcapasos, audífonos, calculadoras, ordenadores personales portátiles, reproductores de música, radio transistores, mando a distancia, etc.

Una celda, célula o pila de combustible es un dispositivo electroquímico de generación de electricidad similar a una batería, que se diferencia de esta en estar diseñada para permitir el reabastecimiento continuo de los reactivos consumidos. Esto permite producir electricidad a partir de una fuente externa de combustible y de oxígeno, en contraposición a la capacidad limitada de almacenamiento de energía de una batería. Además, la composición química de los electrodos de una batería cambia según el estado de carga, mientras que en una celda de combustible los electrodos funcionan por la acción de catalizadores, por lo que son mucho más estables.

En las celdas de hidrógeno los reactivos usados son hidrógeno en el ánodo y oxígeno en el cátodo. Se puede obtener un suministro continuo de hidrógeno a partir de la electrólisis del agua, lo que requiere una fuente primaria de generación de electricidad, o a partir de reacciones catalíticas que desprenden hidrógeno de hidrocarburos. El hidrógeno puede almacenarse, lo que permitiría el uso de fuentes discontinuas de energía como la solar y la eólica. El hidrógeno gaseoso (H) es altamente inflamable y explosivo, por lo que se están desarrollando métodos de almacenamiento en matrices porosas de diversos materiales.

Un generador termoeléctrico de radioisótopos es un generador eléctrico simple que obtiene su energía liberada por la desintegración radiactiva de determinados elementos. En este dispositivo, el calor liberado por la desintegración de un material radiactivo se convierte en electricidad directamente gracias al uso de una serie de termopares, que convierten el calor en electricidad gracias al efecto Seebeck en el llamado Unidad de calor de radioisótopos (o RHU en inglés).

Los RTG se pueden considerar un tipo de batería y se han usado en satélites, sondas espaciales no tripuladas e instalaciones remotas que no disponen de otro tipo de fuente eléctrica o de calor.

Los RTG son los dispositivos más adecuados en situaciones donde no hay presencia humana y se necesitan potencias de varios centenares de vatios durante largos períodos de tiempo, situaciones en las que los generadores convencionales como las pilas de combustible o las baterías no son viables económicamente y donde no pueden usarse células fotovoltaicas.



</doc>
<doc id="7057" url="https://es.wikipedia.org/wiki?curid=7057" title="Leonhard Euler">
Leonhard Euler

Leonhard Paul Euler (pron. en alemán, en español) (Basilea, Suiza; 15 de abril de 1707-San Petersburgo, Imperio ruso; 18 de septiembre de 1783), conocido como Leonhard Euler, fue un matemático y físico suizo. Se trata del principal matemático del siglo XVIII y uno de los más grandes y prolíficos de todos los tiempos, muy conocido por el número de Euler (e), número que aparece en muchas fórmulas de cálculo y física.

Vivió en San Petersburgo (Rusia), y también en Berlín (Prusia) la mayor parte de su vida adulta y realizó importantes descubrimientos en áreas tan diversas como el cálculo o la teoría de grafos. También introdujo gran parte de la moderna terminología y notación matemática, particularmente para el área del análisis matemático, como, por ejemplo, la noción de función matemática. Asimismo se le conoce por sus trabajos en los campos de la mecánica, la óptica y la astronomía.

Euler ha sido uno de los matemáticos más prolíficos, y se calcula que sus obras completas reunidas podrían ocupar entre 60 y 80 volúmenes. Una afirmación atribuida a Pierre Simon Laplace expresa la influencia de Euler en los matemáticos posteriores: «Lean a Euler, lean a Euler, él es el maestro de todos nosotros».

Leonhard Euler nació en Basilea (Suiza), hijo de Paul Euler, un pastor calvinista, y de Marguerite Brucker, hija de otro pastor. Tuvo dos hermanas pequeñas llamadas Anna Maria y Maria Magdalena. Poco después de su nacimiento, su familia se trasladó de Basilea al cercano pueblo de Riehen, en donde Euler pasó su infancia. Por su parte, Paul Euler era amigo de los Bernoulli, famosa familia de matemáticos entre los que destacaba Johann Bernoulli, que en ese momento era ya considerado el principal matemático europeo y que ejercería una gran influencia sobre el joven Leonhard.

La educación formal de Euler comenzó en la ciudad de Basilea, donde le enviaron a vivir con su abuela materna. A la edad de 13 años se matriculó en la Universidad de Basilea y en 1723 recibió el título de maestro de Filosofía tras una disertación comparativa de las filosofías de René Descartes e Isaac Newton. Por entonces, Euler tomaba lecciones particulares todos los sábados por la tarde con Johann Bernoulli, quien descubrió rápidamente el increíble talento para las matemáticas de su nuevo pupilo.

En aquella época, Euler se dedicaba a estudiar teología, griego y hebreo, siguiendo los deseos de su padre, y con la vista puesta en llegar a ser también pastor. Johann Bernoulli intervino para convencer a Paul Euler de que Leonhard estaba destinado a ser un gran matemático. En 1726 Euler finalizó su Doctorado con una tesis sobre la propagación del sonido bajo el título "De Sono" y en 1727 participó en el concurso promovido por la Academia de las Ciencias francesa por el cual se solicitaba a los concursantes que encontraran la mejor forma posible de ubicar el mástil en un buque. Ganó el segundo puesto, detrás de Pierre Bouguer, que es conocido por ser el padre de la arquitectura naval. Más adelante, Euler conseguiría ganar ese premio hasta en doce ocasiones.

En aquel tiempo, los dos hijos de Johann Bernoulli, Daniel y Nicolás, se encontraban trabajando en la Academia de las ciencias de Rusia en San Petersburgo. En julio de 1726, Nicolás murió de apendicitis tras haber vivido un año en Rusia y, cuando Daniel asumió el cargo de su hermano en el departamento de Matemáticas y Física, recomendó que el puesto que había dejado vacante en Fisiología fuese ocupado por su amigo Euler. En noviembre de ese mismo año, Euler aceptó la oferta, aunque retrasó su salida hacia San Petersburgo mientras intentaba conseguir, sin éxito, un puesto de profesor de Física en la Universidad de Basilea.
Euler llegó a la capital rusa el 17 de mayo de 1727. Fue ascendido desde su puesto en el departamento médico de la Academia a otro en el departamento de matemáticas, en el que trabajó con Daniel Bernoulli, a menudo en estrecha colaboración. Euler aprendió el ruso y se estableció finalmente en San Petersburgo a vivir. Llegó incluso a tomar un trabajo adicional como médico de la Armada de Rusia.

La Academia de San Petersburgo, creada por Pedro I de Rusia, tenía el objetivo de mejorar el nivel educativo en Rusia y de reducir la diferencia científica existente entre ese país y la Europa Occidental. Como resultado, se implementaron una serie de medidas para atraer a eruditos extranjeros como Euler. La Academia poseía amplios recursos financieros y una biblioteca muy extensa, extraída directamente de las bibliotecas privadas de Pedro I y de la nobleza. La Academia admitía a un número muy reducido de estudiantes para facilitar la labor de enseñanza, a la vez que se enfatizaba la labor de investigación y se ofrecía a la facultad tanto el tiempo como la libertad necesarios para resolver cuestiones científicas.

Sin embargo, la principal benefactora de la Academia, la emperatriz Catalina I de Rusia, que había continuado con la política progresista de su marido, murió el mismo día de la llegada de Euler a Rusia. Su muerte incrementó el poder de la nobleza, puesto que el nuevo emperador pasó a ser Pedro II de Rusia, por entonces un niño de tan solo 12 años de edad. La nobleza sospechaba de los científicos extranjeros de la Academia, por lo que cortó la cuantía de recursos dedicados a la misma y provocó otra serie de dificultades para Euler y sus colegas.

Las condiciones mejoraron ligeramente tras la muerte de Pedro II, y Euler fue poco a poco ascendiendo en la jerarquía de la Academia, convirtiéndose en profesor de Física en 1731. Dos años más tarde, Daniel Bernoulli, harto de las dificultades que le planteaban la censura y la hostilidad a la que se enfrentaban en San Petersburgo, dejó la ciudad y volvió a Basilea. Euler le sucedió como director del departamento de Matemáticas.

El 7 de enero de 1734, Euler contrajo matrimonio con Katharina Gsell (1707–1773) (hija del pintor suizo de la Academia de San Petersburgo Georg Gsell, y cuya madrastra era la pintora Dorothea Maria Graff, hija a su vez de la famosa naturalista holandesa Maria Sibylla Merian). La joven pareja compró una casa al lado del río Neva y llegó a concebir trece hijos, si bien solo cinco sobrevivieron hasta la edad adulta. El mayor de estos hijos, Johann Euler, fue matemático y astrónomo y miembro de la Academia de Berlín desde 1754.

Preocupado por los acontecimientos políticos que estaban teniendo lugar en Rusia, Euler partió de San Petersburgo el 19 de junio de 1741 para aceptar un cargo en la Academia de Berlín, cargo que le había sido ofrecido por Federico II el Grande, . Vivió veinticinco años en Berlín, en donde escribió más de 380 artículos. También publicó aquí dos de sus principales obras: la "Introductio in analysin infinitorum", un texto sobre las funciones matemáticas publicado en 1748, y la "Institutiones calculi differentialis", publicada en 1755 y que versaba sobre el cálculo diferencial.

Además, se le ofreció a Euler un puesto como tutor de la princesa de Anhalt-Dessau, la sobrina de Federico. Euler escribió más de 200 cartas dirigidas a la princesa que más tarde serían recopiladas en un volumen titulado "Cartas de Euler sobre distintos temas de Filosofía Natural dirigidas a una Princesa alemana". Este trabajo recopilaba la exposición de Euler sobre varios temas de física y matemáticas, así como una visión de su personalidad y de sus creencias religiosas. El libro se convirtió en el más leído de todas sus obras, siendo publicado a lo largo y ancho del continente europeo y en los Estados Unidos. La popularidad que llegaron a alcanzar estas "Cartas" sirve de testimonio sobre la habilidad de Euler de comunicar cuestiones científicas a una audiencia menos cualificada.

A pesar de la inmensa contribución de Euler al prestigio de la Academia, fue obligado finalmente a dejar Berlín. El motivo de esto fue, en parte, un conflicto de personalidad entre el matemático y el propio rey Federico, que llegó a ver a Euler como una persona muy poco sofisticada, y especialmente en comparación con el círculo de filósofos que el rey alemán había logrado congregar en la Academia. Voltaire, en particular, era uno de esos filósofos y gozaba de una posición preeminente en el círculo social del rey. Euler, como un simple hombre de carácter religioso y trabajador, era muy convencional en sus creencias y en sus gustos, representando en cierta forma lo contrario que Voltaire. Euler tenía conocimientos limitados de retórica y solía debatir cuestiones sobre las que tenía pocos conocimientos, lo cual le hacía un objetivo frecuente de los ataques del filósofo. Por ejemplo, Euler protagonizó varias discusiones metafísicas con Voltaire, de las que solía retirarse enfurecido por su incapacidad en la retórica y la metafísica. Federico también mostró su descontento con las habilidades prácticas de ingeniería de Euler:

La vista de Euler fue empeorando a lo largo de su vida. En el año 1735 Euler sufrió una fiebre casi fatal, y tres años después de dicho acontecimiento quedó prácticamente ciego del ojo derecho. Euler, sin embargo, prefería acusar de este hecho al trabajo de cartografía que realizaba para la Academia de San Petersburgo.

La vista de ese ojo empeoró a lo largo de su estancia en Alemania, hasta el punto de que Federico II hacía referencia a él como el "Cíclope". Euler más tarde sufrió cataratas en su ojo sano, el izquierdo, lo que le dejó prácticamente ciego pocas semanas después de haber sido diagnosticadas. A pesar de ello, parece que sus problemas de visión no afectaron a su productividad intelectual, dado que lo compensó con su gran capacidad de cálculo mental y su memoria fotográfica. Por ejemplo, Euler era capaz de repetir la Eneida de Virgilio desde el comienzo hasta el final y sin dudar en ningún momento, y en cada página de la edición era capaz de indicar qué línea era la primera y cuál era la última. También se sabía de memoria las fórmulas de trigonometría y las primeras 6 potencias de los primeros 100 números primos.

Pasó los últimos años de su vida ciego, pero siguió trabajando. Muchos trabajos se los dictó a su hijo mayor. Esto incrementó el respeto que la comunidad científica ya tenía por él. El matemático francés François Arago (1786–1853) se refirió en cierta ocasión a él diciendo:

La situación en Rusia había mejorado enormemente tras el ascenso de Catalina la Grande, por lo que en 1766 Euler aceptó una invitación para volver a la Academia de San Petersburgo y pasar allí el resto de su vida. Su segunda época en Rusia, sin embargo, estuvo marcada por la tragedia: un incendio en San Petersburgo en 1771 le costó su casa y casi su vida, y en 1773 perdió a su esposa Katharina Gsell, después de 40 años de matrimonio. Euler se volvió a casar tres años más tarde con Salome Abigail Gsell (1723-1794), hermana de padre de su primera mujer. Este segundo matrimonio duró hasta la muerte del matemático.

El 18 de septiembre de 1783, Euler falleció en la ciudad de San Petersburgo tras sufrir un accidente cerebrovascular y fue enterrado junto con su primera esposa en el Cementerio Luterano ubicado en la isla Vasilievski. Sus restos fueron trasladados por los soviéticos al Monasterio de Alejandro Nevski (también conocido como Leningradsky Nikropol).

El matemático y filósofo francés Nicolas de Condorcet escribió su elogio funerario para la Academia francesa:

Por su parte, Nikolaus von Fuss, ahijado de Euler y secretario de la Academia Imperial de San Petersburgo, escribió un relato de su vida junto con un listado de sus obras.

Euler trabajó prácticamente en todos los ámbitos de las matemáticas: geometría, cálculo, trigonometría, álgebra, teoría de números, además de física continua, teoría lunar y otras áreas de la física. Adicionalmente, hizo aportaciones relevantes a la lógica matemática con su diagrama de conjuntos.

Ha sido uno de los matemáticos más prolíficos de la historia. Su actividad de publicación fue incesante (un promedio de 800 páginas de artículos al año en su época de mayor producción, entre 1727 y 1783), y una buena parte de su obra completa está sin publicar. La labor de recopilación y publicación completa de sus trabajos, llamados "Opera Omnia", comenzó en 1911 y hasta la fecha ha llegado a publicar 76 volúmenes. El proyecto inicial planeaba el trabajo sobre 887 títulos en 72 volúmenes. Se le considera el ser humano con mayor número de trabajos y artículos en cualquier campo del saber, solo equiparable a Gauss. Si se imprimiesen todos sus trabajos, muchos de los cuales son de una importancia fundamental, ocuparían entre 60 y 80 volúmenes. Además, y según el matemático Hanspeter Kraft, presidente de la "Comisión Euler" de la Universidad de Basilea, no se ha estudiado más de un 10 % de sus escritos. Por todo ello, el nombre de Euler está asociado a un gran número de cuestiones matemáticas.

Se cree que fue el que dio origen al pasatiempos Sudoku creando una serie de pautas para el cálculo de probabilidades.

Euler introdujo y popularizó varias convenciones referentes a la notación en los escritos matemáticos en sus numerosos y muy utilizados libros de texto. Posiblemente lo más notable fue la introducción del concepto de función matemática, siendo el primero en escribir "f"("x") para hacer referencia a la función "f" aplicada sobre el argumento "x". Esta nueva forma de notación ofrecía más comodidad frente a los rudimentarios métodos del cálculo infinitesimal existentes hasta la fecha, iniciados por Newton y Leibniz, pero desarrollados basándose en las matemáticas del último.

También introdujo la notación moderna de las funciones trigonométricas, la letra "e" como base del logaritmo natural o neperiano (el número "e" es conocido también como el número de Euler), la letra griega Σ como símbolo de los sumatorios y la letra formula_1 para hacer referencia a la unidad imaginaria. El uso de la letra griega π para hacer referencia al cociente entre la longitud de la circunferencia y la longitud de su diámetro también fue popularizado por Euler, aunque él no fue el primero en usar ese símbolo.

El desarrollo del cálculo era una de las cuestiones principales de la investigación matemática del siglo XVIII, y la familia Bernoulli había sido responsable de gran parte del progreso realizado hasta entonces. Gracias a su influencia, el estudio del cálculo se convirtió en uno de los principales objetos del trabajo de Euler. Si bien algunas de sus demostraciones matemáticas no son aceptables bajo los estándares modernos de rigor matemático, es cierto que sus ideas supusieron grandes avances en ese campo.

Euler definió la constante matemática conocida como número formula_2 como aquel número real tal que el valor de la derivada (la pendiente de la línea tangente) de la función formula_3formula_4formula_2 en el punto formula_6 es exactamente 1. Es más, es el número real tal que la función formula_3formula_4formula_2 se tiene como derivada a sí misma. La función formula_2 es también llamada función exponencial y su función inversa es el logaritmo natural o logaritmo en base formula_2, mal llamado logaritmo neperiano.

El número formula_2 puede ser representado como un número real en varias formas: como una serie infinita, un producto infinito, una fracción continua o como el límite de una sucesión. La principal de estas representaciones, particularmente en los cursos básicos de cálculo, tiene como forma el límite:

y también la serie:

formula_14

Además, Euler es muy conocido por su análisis y su frecuente utilización de la serie de potencias, es decir, la expresión de funciones como una suma infinita de términos como la siguiente:

Uno de los famosos logros de Euler fue el descubrimiento de la expansión de series de potencias de la función arcotangente. Su atrevido, aunque, según los estándares modernos, técnicamente incorrecto uso de las series de potencias le permitieron resolver el famoso problema de Basilea en 1735, por el cual quedaba demostrado que:

Euler introdujo el uso de la función exponencial y de los logaritmos en las demostraciones analíticas. Descubrió formas para expresar varias funciones logarítmicas utilizando series de potencias, y definió con éxito logaritmos para números negativos y complejos, expandiendo enormemente el ámbito de la aplicación matemática de los logaritmos. También definió la función exponencial para números complejos, y descubrió su relación con las funciones trigonométricas. Para cualquier número real φ, la fórmula de Euler establece que la función exponencial compleja puede establecerse mediante la siguiente fórmula:

Siendo un caso especial de la fórmula (cuando formula_18 = formula_19), lo que se conoce como la identidad de Euler:

Esta fórmula fue calificada por Richard Feynman como «la fórmula más reseñable en matemáticas», porque relaciona las principales operaciones algebraicas con las importantes constantes 0, 1, formula_2, formula_1 y π, mediante la relación binaria más importante. En 1988, los lectores de la revista especializada "The Mathematical Intelligencer" votaron la fórmula como «la más bella fórmula matemática de la historia». En total, Euler fue el responsable del descubrimiento de tres de las cinco primeras fórmulas del resultado de la encuesta.

Además de eso, Euler elaboró la teoría de las funciones trascendentes (aquellas que no se basan en operaciones algebraicas) mediante la introducción de la función gamma, e introdujo un nuevo método para resolver ecuaciones de cuarto grado. También descubrió una forma para calcular integrales con límites complejos, en lo que sería en adelante el moderno análisis complejo, e inventó el cálculo de variaciones incluyendo dentro de su estudio a las que serían llamadas las ecuaciones de Euler-Lagrange.

Euler también fue pionero en el uso de métodos analíticos para resolver problemas teóricos de carácter numérico. Con ello, Euler unió dos ramas separadas de las matemáticas para crear un nuevo campo de estudio, la teoría analítica de números. Para ello, Euler creó la teoría de las series hipergeométricas, las series q, las funciones hiperbólicas trigonométricas y la teoría analítica de fracciones continuas. Por ejemplo, demostró que la cantidad de números primos es infinita utilizando la divergencia de series armónicas, y utilizó métodos analíticos para conseguir una mayor información sobre cómo los números primos se distribuyen dentro de la sucesión de números naturales. El trabajo de Euler en esta área llevaría al desarrollo del teorema de los números primos.

El interés de Euler en la teoría de números procede de la influencia de Christian Goldbach, amigo suyo durante su estancia en la Academia de San Petersburgo. Gran parte de los primeros trabajos de Euler en teoría de números se basan en los trabajos de Pierre de Fermat. Euler desarrolló algunas de las ideas de este matemático francés pero descartó también algunas de sus conjeturas.

Euler unió la naturaleza de la distribución de los números primos con sus ideas del análisis matemático. Demostró la divergencia de la suma de los inversos de los números primos y, al hacerlo, descubrió la conexión entre la función zeta de Riemann y los números primos. Esto se conoce como el producto de Euler para la función zeta de Riemann.

Euler también demostró las identidades de Newton, el pequeño teorema de Fermat, el teorema de Fermat sobre la suma de dos cuadrados e hizo importantes contribuciones al teorema de los cuatro cuadrados de Joseph-Louis de Lagrange. También definió la función φ de Euler que, para todo número entero positivo, cuantifica el número de enteros positivos menores o iguales a n y coprimos con n. Más tarde, utilizando las propiedades de esta función, generalizó el pequeño teorema de Fermat a lo que se conoce como el teorema de Euler.

Contribuyó de manera significativa al entendimiento de los números perfectos, tema que fascinó a los matemáticos desde los tiempos de Euclides, y avanzó en la investigación de lo que más tarde se concretaría en el teorema de los números primos. Los dos conceptos se consideran teoremas fundamentales de la teoría de números, y sus ideas pavimentaron el camino del matemático Carl Friedrich Gauss.

En el año 1772, Euler demostró que 2 - 1 = 2 147 483 647 es un número primo de Mersenne. Esta cifra permaneció como el número primo más grande conocido hasta el año 1867.

En 1736, Euler resolvió el problema conocido como "problema de los puentes de Königsberg". La ciudad de Königsberg, en Prusia Oriental (actualmente Kaliningrado, en Rusia), estaba localizada en el río Pregel, e incluía dos grandes islas que estaban conectadas entre ellas por un puente, y con las dos riberas del río mediante seis puentes (siete puentes en total). El problema que se planteaban sus habitantes consistía en decidir si era posible seguir un camino, y cómo hacerlo, que cruzase todos los puentes una sola vez y que finalizase llegando al punto de partida. Euler logró demostrar matemáticamente que no lo hay, porque con esta configuración no es posible conformar lo que se denomina hoy un ciclo euleriano en el grafo que modela el recorrido, debido a que el número de puentes es impar en más de dos de los bloques (representados por vértices en el grafo correspondiente).

A esta solución se la considera el primer teorema de teoría de grafos y de grafos planares. Euler también introdujo el concepto conocido como característica de Euler del espacio, y una fórmula que relacionaba el número de lados, vértices y caras de un polígono convexo con esta constante: el teorema de Euler para poliedros, que básicamente consiste en buscar una relación entre número de caras, aristas y vértices en los poliedros. Utilizó esta idea para demostrar que no existían más poliedros regulares que los sólidos platónicos conocidos hasta entonces. El estudio y la generalización de esta fórmula, especialmente por Cauchy y L'Huillier, supuso el origen de la topología.

Dentro del campo de la geometría analítica descubrió además que tres de los puntos notables de un triángulo —baricentro, ortocentro y circuncentro— podían obedecer a una misma ecuación, es decir, a una misma recta. A la recta que contiene el baricentro, ortocentro y circuncentro se le denomina «Recta de Euler» en su honor.

Algunos de los mayores éxitos de Euler fueron en la resolución de problemas del mundo real a través del análisis matemático, en lo que se conoce como matemática aplicada, y en la descripción de numerosas aplicaciones de los números de Bernoulli, las series de Fourier, los diagramas de Venn, el número de Euler, las constantes e y π, las fracciones continuas y las integrales. Integró el cálculo diferencial de Leibniz con el método de fluxión de Newton, y desarrolló herramientas que hacían más fácil la aplicación del cálculo a los problemas físicos. Euler ya empleaba las series de Fourier antes de que el mismo Fourier las descubriera y las ecuaciones de Lagrange del cálculo variacional, las ecuaciones de Euler-Lagrange.

Hizo grandes avances en la mejora de las aproximaciones numéricas para resolver integrales, inventando lo que se conoce como las aproximaciones de Euler. Las más notables de estas aproximaciones son el método de Euler para resolver ecuaciones diferenciales ordinarias, y la fórmula de Euler-Maclaurin. Este método consiste en ir incrementando paso a paso la variable independiente y hallando la siguiente imagen con la derivada. También facilitó el uso de ecuaciones diferenciales, en particular mediante la introducción de la constante de Euler-Mascheroni:

Por otro lado, uno de los intereses más llamativos de Euler fue la aplicación de las ideas matemáticas sobre la música. En 1739 escribió su obra "Tentamen novae theoriae musicae", esperando con ello poder incorporar el uso de las matemáticas a la teoría musical. Esta parte de su trabajo, sin embargo, no atrajo demasiada atención del público, y llegó a ser descrita como "demasiado matemática para los músicos y demasiado musical para los matemáticos".

Euler ayudó a desarrollar la ecuación de la curva elástica, que se convirtió en el pilar de la ingeniería. Aparte de aplicar con éxito sus herramientas analíticas a los problemas de mecánica clásica, Euler también las aplicó sobre los problemas de los movimientos de los astros celestes. Su trabajo en astronomía fue reconocido mediante varios premios de la Academia de Francia a lo largo de su carrera, y sus aportes en ese campo incluyen cuestiones como la determinación con gran exactitud de las órbitas de los cometas y de otros cuerpos celestes, incrementando el entendimiento de la naturaleza de los primeros, o el cálculo del paralaje solar. Formuló siete leyes o principios fundamentales sobre la estructura y dinámica del Sistema Solar y afirmó que los distintos cuerpos celestes y planetarios rotan alrededor del Sol siguiendo una órbita de forma elíptica. Sus cálculos también contribuyeron al desarrollo de tablas de longitud más exactas para la navegación. También publicó trabajos sobre el movimiento de la Luna.

Además, Euler llevó a cabo importantes contribuciones en el área de la óptica. No estaba de acuerdo con las teorías de Newton sobre la luz, desarrolladas en su obra "Opticks", y que eran la teoría prevalente en aquel momento. Sus trabajos sobre óptica desarrollados en la década de 1740 ayudaron a que la nueva corriente que proponía una teoría de la luz en forma de onda, propuesta por Christiaan Huygens, se convirtiese en la teoría hegemónica. La nueva teoría mantendría ese estatus hasta el desarrollo de la teoría cuántica de la luz.

En el campo de la mecánica Euler, en su tratado de 1739, introdujo explícitamente los conceptos de partícula y de masa puntual y la notación vectorial para representar la velocidad y la aceleración, lo que sentaría las bases de todo el estudio de la mecánica hasta Lagrange. En el campo de la mecánica del sólido rígido definió los llamados «tres ángulos de Euler para describir la posición» y publicó el teorema principal del movimiento, según el cual siempre existe un eje de rotación instantáneo, y la solución del movimiento libre (consiguió despejar los ángulos en función del tiempo).

En hidrodinámica estudió el flujo de un fluido ideal incompresible, detallando las ecuaciones de Euler de la hidrodinámica.

Adelantándose más de cien años a Maxwell previó el fenómeno de la presión de radiación, fundamental en la teoría unificada del electromagnetismo. En los cientos de trabajos de Euler se encuentran referencias a problemas y cuestiones tremendamente avanzadas para su tiempo, que no estaban al alcance de la ciencia de su época.

En el campo de la lógica, se atribuye a Euler el uso de curvas cerradas para ilustrar el razonamiento silogístico (1768). Las representaciones de este tipo reciben el nombre de diagramas de Euler.

En este campo, Euler desarrolló la ley que lleva su nombre sobre el pandeo de soportes verticales y generó una nueva rama de ingeniería con sus trabajos sobre la carga crítica de las columnas.

Euler y su amigo Daniel Bernoulli se oponían al monismo de Leibniz y a la corriente filosófica representada por Christian Wolff. Euler insistía en que el conocimiento se basa en parte en la existencia de leyes cuantitativas precisas, algo que el monismo y las teorías filosóficas de Wolff no eran capaces de proveer. Sus inclinaciones religiosas también pueden haber contribuido a que le desagradase ese tipo de doctrinas, hasta el punto de que llegó a catalogar las ideas de Wolff como «paganas y ateas».. Sin embargo, tuvo una inmensa influencia debido al racionalismo temprano del filósofo René Descartes. 

Gran parte del conocimiento que tenemos de las creencias religiosas de Euler se deduce de su obra "Cartas a una Princesa alemana", así como de un trabajo anterior llamado "Rettung der Göttlichen Offenbahrung Gegen die Einwürfe der Freygeister" (en español, "Defensa de la revelación divina frente a las objeciones de los librepensadores"). Estos trabajos muestran a Euler como un cristiano convencido que defendía la interpretación literal de la Biblia (por ejemplo, su obra "Rettung" era principalmente una discusión en defensa de la inspiración divina de las escrituras).

Euler cuenta con una extensísima bibliografía, en esta sección se puede encontrar alguna referencia sobre algunas de sus obras más conocidas o importantes.


En 1911, la Academia Suiza de las Ciencias comenzó a publicar una colección definitiva de los trabajos de Euler titulada "Opera Omnia". Existe un plan para la ampliación de la obra a la publicación de la correspondencia (en el año 2008 se han publicado ya tres volúmenes de correspondencia) y los manuscritos de Euler, aunque no se ha especificado ninguna fecha para su edición.






</doc>
<doc id="7066" url="https://es.wikipedia.org/wiki?curid=7066" title="5 de mayo">
5 de mayo

El 5 de mayo es el 125.º (centésimo vigesimoquinto) día del año en el calendario gregoriano y el 126.º en los años bisiestos. Quedan 240 días para finalizar el año.











</doc>
<doc id="7067" url="https://es.wikipedia.org/wiki?curid=7067" title="6 de mayo">
6 de mayo

El 6 de mayo es el 126.º (centésimo vigésimo sexto) día del año en el calendario gregoriano y el 127.º en los años bisiestos. Quedan 239 días para finalizar el año.












</doc>
<doc id="7068" url="https://es.wikipedia.org/wiki?curid=7068" title="7 de mayo">
7 de mayo

El 7 de mayo es el 127.º (centésimo vigesimoséptimo) día del año en el calendario gregoriano y el 128.º en los años bisiestos. Quedan 238 días para finalizar el año.













</doc>
<doc id="7069" url="https://es.wikipedia.org/wiki?curid=7069" title="8 de mayo">
8 de mayo

El 8 de mayo es el 128.º (centésimo vigesimoctavo) día del año en el calendario gregoriano y el 129.º en los años bisiestos. Quedan 237 días para finalizar el año.








</doc>
<doc id="7070" url="https://es.wikipedia.org/wiki?curid=7070" title="9 de mayo">
9 de mayo

El 9 de mayo es el 129.º (centésimo vigesimonoveno) día del año del calendario gregoriano y el 130.º en los años bisiestos. Quedan 236 días para finalizar el año.








</doc>
<doc id="7071" url="https://es.wikipedia.org/wiki?curid=7071" title="11 de mayo">
11 de mayo

El 11 de mayo es el 131.º (centésimo trigésimo primer) día del año en el calendario gregoriano y el 132.º en los años bisiestos. Quedan 234 días para finalizar el año.














</doc>
<doc id="7072" url="https://es.wikipedia.org/wiki?curid=7072" title="12 de mayo">
12 de mayo

El 12 de mayo es el 132.º (centésimo trigésimo segundo) día del año en el calendario gregoriano y el 133.º en los años bisiestos. Quedan 233 días para finalizar el año.









</doc>
<doc id="7073" url="https://es.wikipedia.org/wiki?curid=7073" title="13 de mayo">
13 de mayo

El 13 de mayo es el 133.º (centésimo trigésimo tercer) día del año del calendario gregoriano y el 134.º en los años bisiestos. Quedan 232 días para finalizar el año.























 Guatemala:





</doc>
<doc id="7075" url="https://es.wikipedia.org/wiki?curid=7075" title="17 de mayo">
17 de mayo

El 17 de mayo es el 137.º (centésimo trigésimo séptimo) día del año del calendario gregoriano y el 138.º en los años bisiestos. Quedan 228 días para finalizar el año.









</doc>
<doc id="7076" url="https://es.wikipedia.org/wiki?curid=7076" title="19 de mayo">
19 de mayo

El 19 de mayo es el 139.º (centésimo trigésimo noveno) día del año del calendario gregoriano y el 140.º en los años bisiestos. Quedan 226 días para finalizar el año.










</doc>
<doc id="7077" url="https://es.wikipedia.org/wiki?curid=7077" title="22 de mayo">
22 de mayo

El 22 de mayo es el 142.º (centésimo cuadragésimo segundo) día del año en el calendario gregoriano y el 143.º en los años bisiestos. Quedan 223 días para finalizar el año.








</doc>
<doc id="7078" url="https://es.wikipedia.org/wiki?curid=7078" title="23 de mayo">
23 de mayo

El 23 de mayo es el 143.º (centésimo cuadragésimo tercer) día del año en el calendario gregoriano y el 144.º en los años bisiestos. Quedan 222 días para finalizar el año.








</doc>
<doc id="7079" url="https://es.wikipedia.org/wiki?curid=7079" title="24 de mayo">
24 de mayo

El 24 de mayo es el 144.º (centésimo cuadragésimo cuarto) día del año en el calendario gregoriano y el 145.º en los años bisiestos. Quedan 221 días para finalizar el año.








</doc>
<doc id="7081" url="https://es.wikipedia.org/wiki?curid=7081" title="25 de mayo">
25 de mayo

El 25 de mayo es el 145.º (centésimo cuadragésimo quinto) día del año en el calendario gregoriano y el 146.º en los años bisiestos. Quedan 220 días para finalizar el año.









</doc>
<doc id="7082" url="https://es.wikipedia.org/wiki?curid=7082" title="26 de mayo">
26 de mayo

El 26 de mayo es el 146.º (centésimo cuadragésimo sexto) día del año en el calendario gregoriano y el 147.º en los años bisiestos. Quedan 219 días para finalizar el año.









</doc>
<doc id="7083" url="https://es.wikipedia.org/wiki?curid=7083" title="27 de mayo">
27 de mayo

El 27 de mayo es el 147.º (centésimo cuadragésimo séptimo) día del año en el calendario gregoriano y el 148.º en los años bisiestos. Quedan 218 días para finalizar el año.









</doc>
<doc id="7084" url="https://es.wikipedia.org/wiki?curid=7084" title="28 de mayo">
28 de mayo

El 28 de mayo es el 148.º (centésimo cuadragésimo octavo) día del año en el calendario gregoriano y el 149.º en los años bisiestos. Quedan 217 días para finalizar el año.



















</doc>
<doc id="7085" url="https://es.wikipedia.org/wiki?curid=7085" title="29 de mayo">
29 de mayo

El 29 de mayo es el 149.º (centésimo cuadragésimo noveno) día del año en el calendario gregoriano y el 150.º en los años bisiestos. Quedan 216 días para finalizar el año.








</doc>
<doc id="7086" url="https://es.wikipedia.org/wiki?curid=7086" title="30 de mayo">
30 de mayo

El 30 de mayo es el 150.º (centésimo quincuagésimo) día del año en el calendario gregoriano y el 151.º en los años bisiestos. Quedan 215 días para finalizar el año.










</doc>
<doc id="7087" url="https://es.wikipedia.org/wiki?curid=7087" title="31 de mayo">
31 de mayo

El 31 de mayo es el 151.º (centésimo quincuagésimo primer) día del año en el calendario gregoriano y el 152.º en los años bisiestos. Quedan 214 días para finalizar el año.








</doc>
<doc id="7089" url="https://es.wikipedia.org/wiki?curid=7089" title="Academia Panameña de la Lengua">
Academia Panameña de la Lengua

La Academia Panameña de la Lengua (APL) es una corporación conformada por académicos de número, expertos en el buen uso y la creación con la palabra de la lengua española en Panamá.

Fue establecida en Panamá el 12 de mayo de 1926. Dicha institución, que pertenece a la Asociación de Academias de la Lengua Española, fomenta la difusión de publicaciones y libros panameños valiosos, da forma a un léxico de panameñismos aceptables, en el que figuran también neologismos y arcaísmos con citas e indicaciones, con el fin de presentarlos a la Real Academia Española (RAE) para su posible incorporación en las ediciones del Diccionario Oficial de la Lengua. El 9 de agosto de 1926 fue fundada mediante el nombramiento, por parte de la Real Academia Española, de dieciocho notables intelectuales panameños: Samuel Lewis García de Paredes (director), Eduardo Chiari (tesorero), Ricardo Miró (secretario perpetuo), Ricardo J. Alfaro, Guillermo Andreve, Abel Bravo, Jeptha Duncan, Demetrio Fábrega, Narciso Garay, Julio J. Fábrega, José de la Cruz Herrera, Melchor Lasso de la Vega, Octavio Méndez Pereira, Eusebio A. Morales, José Dolores Moscote, Belisario Porras, Samuel Quintero y Nicolás Victoria Jaén.
La sede de la Academia Panameña de la Lengua se encuentra en una casona con arquitectura del primer cuarto del siglo en el sector de Calle 50, en el corregimiento de Bella Vista, ubicada en el área urbana y económica de la Ciudad de Panamá 
En Panamá, la Academia organiza conferencias sobre temas lingüísticos, literarios, humanísticos y su Director y uno o dos más de los académicos asisten a los congresos que periódicamente celebra la Asociación de Academias de la Lengua Española, cuyas 22 academias que la conforman presentan, cada seis meses, términos que son evaluados por una comisión de la RAE. 

El actual director Arístides Royo tiene una mención de las funciones que tiene la institución:

Los Directores de la Academia Panameña de la Lengua son elegidos por 3 años. Ha sido dirigida por Nicolás Victoria Jaén en 1939, Ricardo J. Alfaro en 1950, Baltasar Isaza Calderón en 1960, Ernesto de la Guardia Navarro en 1973, Ismael García S. en 1979, Elsie Alvarado de Ricord en 1991, Pablo Pinilla Chiari en el 2003, en 2006 el Dr. José Guillermo Ros-Zanet, en 2009, Berna Pérez Ayala de Burrell, en 2015, Margarita Vásquez Quirós y desde 2017 Arístides Royo. Los Directores son elegidos por 3 años.





</doc>
<doc id="7096" url="https://es.wikipedia.org/wiki?curid=7096" title="Teoría de la gran unificación">
Teoría de la gran unificación

Una teoría de gran unificación (TGU, , GUT) es una teoría que unificaría tres de las cuatro fuerzas fundamentales en la naturaleza: la fuerza nuclear débil, fuerza nuclear fuerte y la fuerza electromagnética. La fuerza de gravedad no es considerada en las teoría de gran unificación, pero sí en una eventual teoría del todo ("Theory of Everything", ToE), que consideraría las cuatro interacciones fundamentales.

Steven Weinberg y Abdus Salam elaboraron en 1967-1968, una teoría relativista del campo cuántico, que permitía expresar las interacciones electromagnéticas y débiles de una manera unificadas (modelo electrodébil), y que predijo hechos que luego fueron comprobados experimentalmente. Posteriormente, Howard Georgi y Sheldon Lee Glashow desarrollaron una nueva teoría, que aportaba nuevas características y corregía algunos errores y omisiones de la anterior teoría. Sin embargo de las ecuaciones se desprendía el decaimiento del protón. Esto llevó a algunos famosos experimentos para detectar este efecto: pero como el tiempo de vida de un protón es muy largo, en el orden de 10 años, no es posible observar la partícula el tiempo suficiente como para presenciar la descomposición. En reemplazo de esto, quizás el efecto podría ser observado si se examinan suficientes protones. Algunos intentos de medición conocidos se realizaron en piscinas subterráneas (para proteger el experimento de radiaciones) de grandes dimensiones, en las que el decaimiento del protón sería visualizado como un destello en una serie de fotosensores.

El modelo estándar de la física de partículas es una teoría de campo de gauge que describe a fermiones elementales (leptones y quarks) en interacción mutua mediante una serie de campos de Yang-Mills de bosones intermediarios. Puesto que el modelo electrodébil (que describe la interacción electromagnética y débil) está basado en una teoría de gauge con grupo gauge de simetría SU(2)xU(1) y la cromodinámica cuántica (que describe la interacción fuerte) está basada en una teoría con grupo gauge SU(3); los físicos han encontrado prometedor describir todas estas interacciones mediante una teoría gauge con un grupo de simetría que tenga como subgrupos a los grupos gauge mencionados.

Un candidato obvio para grupo de simetría es SU(5) en el que se basa el modelo de Georgi-Glashow de 1974. En ese modelo se incluía un mecanismo de ruptura espontánea de la simetría por el cual la simetría original completa, se volvía una simetría menos general U(1)xSU(2)xSU(3) a bajas energía por fenómenos que rompían la simetría. Aunque a grandes energías los factores de ruptura se vuelven irrelevantes y los tres tipos de interacción debían aparecer como manifestaciones del mismo campo. Una de las predicciones de este modelo es que existirían interacciones que transformarían quarks en leptones violando la conservación de número bariónico (aunque aún se conservaría la suma del número bariónico más el número leptónico). Una de esas interacciones mencionadas permitiría la desintegración del protón en otras partículas leptónicas. Como la propia teoría permite calcular la tasa de desintegración en principio es más o menos directo someter a prueba la teoría. Desgraciadamente la desintegración del protón no ha sido observada y los límites de error experimental permiten descartar la teoría, razón por la cual se han buscado otros grupos de simetría gauge que den lugar a predicciones de acuerdo con lo observado. Aunque la elegancia de esta teoría ha hecho que sea la base de muchas otras propuestas posteriores algo más complicadas.

Se han propuesto muchas teorías de gran unificación con grupo gauge que tiene como subgrupos al grupo gauge del modelo estándar (U(1)xSU(2)xSU(3)), aunque ninguna de ellas tiene aceptación general. Algunas teorías de gran unificación convencionales son:





</doc>
<doc id="7102" url="https://es.wikipedia.org/wiki?curid=7102" title="Tamil Nadu">
Tamil Nadu

Tamil Nadu ( en tamil: தமிழ் நாடு "tamiḻ nāṭu" «Tierra de los Tamiles») es uno de los veintinueve estados que, junto con los siete territorios de la Unión, forman la República de la India. Su capital es Chennai, antigua Madrás.

Se ubica en el extremo sur del país, limitando al norte con Karnataka y Andhra Pradesh, al este con la bahía de Bengala (océano Índico), cerca de su costa sureste se encuentra Sri Lanka y al oeste con Kerala. Con 72 147 030 habitantes en 2011 es el sexto estado más poblado —por detrás de Uttar Pradesh, Maharastra, Bihar, Bengala Occidental y Madhya Pradesh— y con 555 hab/km², el sexto más densamente poblado, por detrás de Bihar, Bengala Occidental, Kerala, Uttar Pradesh y Haryana.

Como estado, se fundó en 1956 para acoger dentro de sus fronteras a los hablantes del idioma tamil, que tienen una larga historia literaria y es uno de los idiomas clásicos de la India. Recibió su nombre actual en 1969.

Tamil Nadu fue conocido como la "presidencia de Madrás" durante el periodo de dominación británica de la India. Tras la independencia del país se convirtió en el "estado de Madrás"; en 1953 la zona nordeste del estado, con mayoría de hablantes del télugu se convirtió en el nuevo estado de Andhra Pradesh mientras que el distrito de Bellary se convirtió en parte integrante del estado de Mysore.

En 1956 las fronteras actuales del estado quedaron definidas cuando la zona oeste del estado de Madrás, en el Mar Arábigo, quedó dividida entre Mysore (más tarde Karnataka) y el nuevo estado de Kerala. En julio de 1967 el estado fue renombrado como Tamil Nadu, nombre que recibió la aprobación del parlamento de la India en agosto de 1968. Las políticas del estado protegen la cultura, el idioma y las tradiciones de los tamiles.

Al contrario de otras partes del país, Tamil Nadu recibe las principales precipitaciones gracias al monzón del noroeste que llega al estado en los meses de octubre a diciembre. La agricultura depende casi exclusivamente de estas lluvias cíclicas para su mantenimiento. Tamil Nadu fue uno de los estados afectados por el tsunami que asoló las costas de Asia el 26 de diciembre de 2004. Murieron más de 4000 personas y quedaron dañadas casi la totalidad de las zonas más al sur del estado.

Madrás, llamada oficialmente desde 1996 como "Chennai", es la cuarta ciudad de la India y la capital del estado. En Madrás se encuentra Playa Marina, la segunda playa más larga del mundo. Coimbatore, Cuddalore, Madurai, Tiruchirapalli, Salem y Tirunelveli son otras ciudades importantes de Tamil Nadu.

Tamil Nadu es conocido por su rica literatura, música y danzas que siguen floreciendo hoy en día. Es uno de los estados más progresistas y más industrializados de la India. El idioma oficial es el tamil.

Tamil Nadu es el más sureño de los estados de la India. Limita al oeste con el estado de Kerala, al noroeste con el de Karnataka, y al norte con el de Andhra Pradesh. También se encuentran en el interior de su territorio dos de los enclaves que constituyen el territorio de Puducherry: Puducherry (Pondicherry) y Karaikal. Al este y al sur, Tamil Nadu limita con el Océano Índico: la Bahía de Bengala al este, y el Golfo de Mannar al sudeste. El estrecho de Palk separa Tamil Nadu del estado insular de Sri Lanka. La longitud total de costa del estado de Tamil Nadu es de 1.076 kilómetros.

Los 32 distritos de Tamil Nadu son los siguientes, con la numeración correspondiente en el mapa de la derecha. 

En Tamil Nadu destacan los numerosos templos hindúes basados en la arquitectura dravidiana. Los templos tienen un estilo característico famoso por su estructura en forma de torre. Algunas ciudades con templos destacados son Madurai, Trichy (Tiruchirapalli), Kanchipuram, Palani, Swamithoppe, Tiruvallur y Mahabalipuram. Estaciones de montaña como Kodaikanal y Nilgiris tienen algunos de los paisajes más espectaculares del país. En Nilgiris está también uno de los dos ferrocarriles de montaña de la India que han sido declarados por la Unesco como Patrimonio de la Humanidad.




</doc>
<doc id="7103" url="https://es.wikipedia.org/wiki?curid=7103" title="Idioma tamil">
Idioma tamil

El tamil o támil (autoglotónimo தமிழ் "támiḻ" ) es una lengua drávida que se habla principalmente en Tamil Nadu (India) y en el noreste de Sri Lanka. Es la lengua drávida con el segundo mayor número de hablantes después del telugu y es la de mayor desarrollo literario. Actualmente se calcula que tiene unos 70 millones de hablantes. 

Existen también destacados núcleos de hablantes en Malasia, Vietnam, Singapur (donde es una de las cuatro lenguas oficiales del país), la isla de Zanzíbar (Tanzania) y, recientemente, en diversas ciudades europeas importantes, entre las que destaca Londres.

Cuenta con una rica y antigua tradición literaria: los primeros escritos de esta lengua, consistentes en poesía lírica y una gramática, se remontan al siglo I d. C.

El sistema de escritura del tamil desciende del alfabeto brahmi. Combina consonantes con vocales para crear caracteres silábicos únicos pero, al contrario que las otras escrituras derivadas del brahmi, no tiene caracteres que representen sonidos con más de una consonante. Actualmente, como el tamil está recibiendo gran influencia de otras lenguas, su sistema de escritura está teniendo problemas a la hora de representar los nuevos vocablos.

El tamil es una de las lenguas actualmente habladas más antiguas del mundo, con 2.200 años de historia. Sus orígenes no son bien conocidos, pero se desarrolló y floreció en India como una lengua independiente con una rica literatura. Más del 55% de las inscripciones epigráficas encontradas en el Indostán están escritas en lengua tamil. La literatura en tamil es la más antigua que existe entre las lenguas drávidas, pero es difícil datar el lenguaje y la literatura de manera precisa. Muchos trabajos literarios indios fueron preservados en manuscrito (copiando y recopilando) o a través de la transmisión oral, y es imposible datarlos. Evidencias lingüísticas internas y referencias cronológicas externas indican que el trabajo más antiguo conocido fue realizado probablemente entre el siglo II a. C. y el X d. C.

El tamil es la lengua oficial del Estado de Tamil Nadu, en la India. Es uno de los idiomas oficiales de los Territorios de la Unión de Pondicherry y las Islas Andamán y Nicobar. Se trata de uno de los 23 idiomas en la Constitución de la India. El tamil es, asimismo, uno de los idiomas oficiales de Sri Lanka y Singapur. En Malasia, la educación primaria en las escuelas públicas también se desarrolla totalmente en tamil.

Además, con la creación en 2004 de un estatuto jurídico de lenguas clásicas por el Gobierno indio y después de una campaña política apoyada por varias asociaciones tamiles, este idioma se convirtió en la primera lengua clásica reconocida legalmente en el subcontinente. El reconocimiento fue anunciado por el entonces Presidente de la India, el doctor Abdul Kalam, en una sesión conjunta de ambas cámaras del Parlamento indio el 6 de junio de 2004.

<nowiki>*</nowiki> "Sonidos no nativos del tamil"

"Las formas de u y ū pueden cambiar en cada letra"


Pronunciación:

Escritura tamil:

Traducción al español:




</doc>
<doc id="7105" url="https://es.wikipedia.org/wiki?curid=7105" title="(2675) Tolkien">
(2675) Tolkien

(2675) Tolkien es un asteroide que forma parte del cinturón de asteroides y fue descubierto por Martin Watt el 14 de abril de 1982 desde la Estación Anderson Mesa (condado de Coconino, cerca de Flagstaff, Arizona, Estados Unidos).

Su denominación provisional fue , y su descubridor le dio su nombre definitivo en honor al filólogo, profesor y escritor británico J. R. R. Tolkien, autor de las novelas de alta fantasía "El hobbit" y "El Señor de los Anillos", entre otras obras.

Tolkien está situado a una distancia media del Sol de 2,213 ua, pudiendo alejarse hasta 2,439 ua y acercarse hasta 1,987 ua. Su excentricidad es 0,1019 y la inclinación orbital 2,754 grados. Emplea 1202 días en completar una órbita alrededor del Sol.

La magnitud absoluta de Tolkien es 12,1 y el periodo de rotación de 1060 horas. Está asignado al tipo espectral S de la clasificación SMASSII.



</doc>
<doc id="7106" url="https://es.wikipedia.org/wiki?curid=7106" title="(467) Laura">
(467) Laura

(467) Laura es un asteroide perteneciente al cinturón de asteroides descubierto el 9 de enero de 1901 por Maximilian Franz Wolf desde el observatorio de Heidelberg-Königstuhl, Alemania.
Está posiblemente nombrado por Laura, un personaje de la ópera "La Gioconda" del compositor italiano Amilcare Ponchielli.

Laura está situado en la zona II del cinturón de asteroides, en el borde del hueco de Kirkwood 7:3, y emplea 1843 días en cubrir una órbita alrededor del Sol.

Laura tiene un diámetro aproximado de y una masa estimada de , suponiendo una densidad similar a la de (835) Olivia que orbita en la misma región. Esta masa representa aproximadamente el del total del cinturón de asteroides. Su albedo es de 0,063 por lo que es muy oscuro y se supone que pertenece a los asteroides del tipo espectral C. La gravedad superficial es unas 800 veces menor a la equivalente en la Tierra.


</doc>
<doc id="7107" url="https://es.wikipedia.org/wiki?curid=7107" title="Hathor">
Hathor

Hathor es una de las principales diosas de la religión del Antiguo Egipto que desempeñaba una gran variedad de papeles. Como deidad del cielo, era la madre o consorte del dios celeste Horus y del dios solar Ra, ambos relacionados con la realeza, por lo que Hathor era la madre simbólica de sus representantes terrenales, los faraones. Fue una de las muchas diosas que asumió el papel del Ojo de Ra, la contraparte femenina de Ra, y en esta forma tenía un carácter vengativo que lo protegía de sus enemigos. Su lado benefactor representaba la música, la danza, la alegría, el amor, la sexualidad y el cuidado materno, y actuaba como consorte de varias deidades masculinas y madre de sus hijos. Estos dos aspectos de la diosa ejemplificaban la concepción egipcia de la feminidad. Cruzó la frontera entre los mundos, ayudando a las almas fallecidas en su transición a la vida después de la muerte.

Con frecuencia era representada como una vaca, símbolo de su carácter maternal y celestial, aunque su forma más común era la de una mujer con un tocado de cuernos de vaca y un disco solar. También podría representarse como leona, ureo o sicomoro.

Existen representaciones de diosas bovinas similares en el arte egipcio del cuarto milenio a. C., pero Hathor posiblemente no apareció hasta el Imperio Antiguo (c. 2686-2181 a. C.). Con el auspicio de los gobernantes del Imperio Antiguo se convirtió en una de las deidades más importantes de Egipto. Se le dedicaron más templos que a cualquier otra diosa, de los cuales el más destacado fue el de Dendera, en el Alto Egipto. También era adorada en los templos de sus consortes masculinos. Los egipcios la vincularon con tierras extranjeras como Nubia y Canaán y sus valiosos bienes, como el incienso y las gemas semipreciosas, y algunos de los pueblos de esas tierras adoptaron su culto. En Egipto fue una de las deidades más invocadas en las oraciones privadas y en las ofrendas votivas, especialmente por las mujeres que deseaban tener hijos. 

Durante el Imperio Nuevo (c. 1550-1070 a. C.), diosas como Mut e Isis ocuparon la posición de Hathor en la ideología de la realeza, pero siguió siendo una de las deidades más adoradas. Después del fin del Imperio Nuevo, Hathor fue cada vez más eclipsada por Isis, pero continuó siendo adorada hasta la extinción de la antigua religión egipcia en los primeros siglos de nuestra era.

Imágenes de vacas aparecen con frecuencia en el arte del Egipto predinástico (antes de 3100 a. C.), al igual que imágenes de mujeres con los brazos alzados y curvados que recuerdan la forma de los cuernos de los bovinos. Ambos tipos de imágenes pueden representar diosas relacionadas con el ganado. Las vacas son veneradas en muchas culturas, incluido el antiguo Egipto, como símbolos de maternidad y alimentación, porque cuidan de sus terneros y suministran leche a los seres humanos. La Paleta de Gerzeh, una paleta de piedra del periodo prehistórico de Naqada II (c. 3500-3200 a. C.), muestra la silueta de la cabeza de una vaca con cuernos curvados hacia adentro rodeados de estrellas. La paleta sugiere que esta vaca estaba ligada al cielo, al igual que varias diosas de épocas posteriores que fueron representadas de esta forma: Hathor, Meheturet y Nut.

A pesar de estos precedentes primitivos, Hathor no es mencionada o representada sin lugar a dudas hasta la Cuarta Dinastía (c. 2613-2494 a. C.) del Imperio Antiguo, aunque varios objetos que se refieren a ella pueden ser datados en el Periodo Arcaico (c. 3100-2686 a. C.). Cuando Hathor aparece de forma clara, sus cuernos se curvan hacia afuera, en lugar de hacia adentro como los del arte predinástico. Una deidad bovina con cuernos curvados hacia adentro aparece en la Paleta de Narmer, de cerca de los inicios de la historia egipcia, tanto en la parte superior de la paleta como en el cinturón del rey Narmer. El egiptólogo Henry George Fischer planteó que esta deidad podría ser Bat, una diosa que posteriormente fue representada con el rostro de una mujer y antenas que se doblaban hacia adentro, reflejando aparentemente la curva de los cuernos de vaca. Por su parte, la egiptóloga Lana Troy identifica un pasaje de los "Textos de las Pirámides" del Imperio Antiguo tardío que relaciona a Hathor con el «delantal» del rey, que recuerda a la diosa del cinturón de Narmer y que sugiere que esta diosa no es Bat sino Hathor.

Durante la Cuarta Dinastía, Hathor se convirtió rápidamente en un personaje destacado. Reemplazó a un dios cocodrilo primitivo que era adorado en Dendera, en el Alto Egipto, para convertirse en la deidad patrona de esta ciudad, y fue asumiendo cada vez más el culto a Bat en la vecina región de Hu, hasta que, en el Imperio Medio (c. 2055-1650 a. C.), las dos divinidades se unificaron. La teología en torno al faraón en el Imperio Antiguo, a diferencia de la de épocas anteriores, se centraba en gran medida en el dios sol Ra como rey de los dioses y padre y patrón del rey terrenal. Hathor ascendió con Ra y se convirtió en su esposa mitológica, y por tanto la divina madre del faraón.

Hathor adoptó muchas formas y desempeñó una gran variedad de funciones. El egiptólogo Robyn A. Gillam sugiere que esta diversidad de formas surgieron cuando la diosa real promovida por la corte del Imperio Antiguo sustituyó a muchas diosas locales adoradas por la población, a las que luego se trató como manifestaciones de ella. Los textos egipcios hablan a menudo de las manifestaciones de la diosa como las «Siete Hathores» o, con menos frecuencia, de muchas más —incluso hasta 362—. Por ello, Gillam la considera «un tipo de deidad y no una entidad única». Esta diversidad refleja la gran variedad de atributos que los egipcios asociaban con las diosas. Más que ninguna otra deidad, ella ejemplifica la concepción egipcia de la feminidad.

Se le asignaron los epítetos «señora del cielo» y «señora de las estrellas» y se decía que habitaba en el cielo con Ra y otros dioses solares. Los egipcios creían que el cielo era un cuerpo de agua a través del cual navegaba el dios sol y lo asociaban con las aguas de las que, según sus mitos sobre la creación, el sol emergió al principio de los tiempos. Esta diosa madre cósmica se representaba a menudo como una vaca. Se consideraba tanto a Hathor como a Meheturet como la vaca que dio a luz al dios sol y lo puso entre sus cuernos. Como Nut, se decía que Hathor daba a luz al dios sol cada amanecer.

Su nombre egipcio era "ḥwt-ḥrw" o "ḥwt-ḥr". Se suele traducir como «casa de Horus», pero también se puede traducir como «mi casa es el cielo». El dios halcón Horus representaba, entre otras cosas, el sol y el cielo. La «casa» a la que se hace referencia puede ser el cielo en el que vive Horus, o el vientre de la diosa desde el que él, como dios del sol, nace cada día.

Era en sí misma una deidad solar, una contraparte femenina de dioses solares como Horus y Ra, y formaba parte del séquito divino que acompañaba a Ra mientras navegaba por el cielo en su barca. Se la conocía habitualmente como «La de Oro», en referencia al resplandor del sol, y los textos de su templo en Dendera dicen que «sus rayos iluminan toda la tierra». A veces se unía a otra diosa, Nebethetepet, cuyo nombre puede significar «Señora de las ofrendas», «Señora de la alegría» o «Señora de la vulva». En el centro de culto de Ra de Heliópolis, Hathor-Nebethetepet era adorada como su consorte, y el egiptólogo Rudolf Anthes consideraba que el nombre de Hathor hacía referencia a una mítica «casa de Horus» en Heliópolis que estaba ligada a la ideología de la monarquía.

Fue una de las muchas diosas que asumió el papel del Ojo de Ra, una personificación femenina del disco del sol y una extensión del propio poder de Ra, que a veces se representaba dentro del disco, lo que Troya interpreta como que la diosa Ojo estaba considerada como un vientre del que nació el dios sol. Las funciones aparentemente contradictorias de Hathor como madre, esposa e hija de Ra reflejaban el ciclo diario del sol. Al atardecer el dios se introducía en el cuerpo de la diosa, fecundándola y engendrando a las deidades nacidas de su vientre al amanecer: él mismo y la diosa Ojo, que más tarde lo alumbraría a él. Ra dio origen a su hija, la diosa Ojo, quien a su vez lo originó a él, su hijo, en un ciclo de regeneración constante.

El Ojo de Ra protegía al dios sol de sus enemigos y a menudo era representado como un ureo o cobra erguida, o como una leona. Una forma del Ojo de Ra conocida como «Hathor de las Cuatro Caras», representada por un conjunto de cuatro cobras, se decía que estaba de cara a cada uno de los puntos cardinales para vigilar las amenazas contra el dios sol. Un grupo de mitos, conocidos a partir del Imperio Nuevo (c. 1550-1070 a. C.), describen lo que sucede cuando la diosa Ojo se enfurece sin control. En el texto funerario conocido como el "Libro de la Vaca Sagrada", Ra envía a Hathor como el Ojo de Ra para castigar a los humanos por tramar una rebelión contra su gobierno. Se convierte en la diosa leona Sejmet y masacra a los humanos rebeldes, pero Ra decide impedir que mate a toda la humanidad. Ordena que la cerveza se tiña de rojo y sea distribuida por toda la tierra. La diosa Ojo bebe la cerveza, confundiéndola con sangre, y en su estado de ebriedad vuelve a ser la benévola y hermosa Hathor. Relacionado con esta historia está el mito de la Diosa Distante, de los periodos tardío y ptolemaico. La diosa Ojo, a veces en forma de Hathor, se rebela contra el control de Ra y hace estragos libremente en un país extranjero: Libia, al oeste de Egipto, o Nubia, al sur. Debilitado por la pérdida de su Ojo, Ra envía a otro dios, Tot, para que la traiga de vuelta. Una vez pacificada, la diosa regresa para convertirse en la consorte del dios sol o del dios que la hace volver. Los dos aspectos de la diosa Ojo, violento y peligroso en lugar de bello y alegre, reflejaban la creencia egipcia de que las mujeres, tal como lo plantea la egiptóloga Carolyn Graves-Brown, «abarcaban las pasiones extremas de la furia y el amor».

La religión egipcia celebraba los placeres de los sentidos en la vida, que se consideraban entre los regalos de los dioses a la humanidad. Los egipcios comían, bebían, bailaban y tocaban música en sus festivales religiosos. Perfumaban el aire con flores e incienso. Muchos de los epítetos de Hathor la asocian con las celebraciones; se la menciona como la señora de la música, la danza, las guirnaldas, la mirra y la embriaguez. En himnos y relieves de templos, los músicos tocan panderos, arpas, liras y sistros en honor de Hathor. El sistro, un instrumento parecido a un sonajero, fue particularmente importante en su adoración; este instrumento tenía connotaciones eróticas y, por extensión, aludía a la creación de nueva vida.

Estos aspectos de la diosa estaban relacionados con el mito del Ojo de Ra. El Ojo fue pacificado por la cerveza en el mito de la destrucción de la humanidad. En algunas versiones del mito de la Diosa Distante, la naturaleza salvaje del Ojo vagabundo disminuyó cuando fue apaciguada con productos de la civilización como la música, la danza y el vino. El agua de la crecida anual del Nilo, coloreada de rojo por los sedimentos, se comparó con el vino y con la cerveza teñida de rojo del mito de la destrucción. Los festivales que se celebraban durante la inundación incorporaban, en consecuencia, la bebida, la música y la danza como una forma de apaciguar a la diosa que regresaba. Un texto del Templo de Edfu dice de Hathor: «Los dioses tocan el sistro para ella, las diosas danzan para que ella se deshaga de su mal genio». Un himno a la diosa Rattaui como una forma de Hathor en el templo de Medamud describe el «Festival de la embriaguez» como parte de su mítico regreso a Egipto. Las mujeres llevan ramos de flores, los juerguistas borrachos tocan tambores y personas y animales de otros países bailan para ella mientras entra en el palco del festival del templo. El ruido de la celebración aleja los poderes hostiles y asegura que la diosa permanezca en su forma alegre mientras espera al dios masculino del templo, su mitológico consorte Montu, cuyo hijo dará a luz.

Su lado alegre y exultante indica su poder femenino y procreador. En algunos mitos de la creación contribuyó a crear el propio mundo. Se creía que Atum, un dios creador que contenía todas las cosas dentro de sí mismo, había generado, mediante una masturbación metafórica, a Shu y Tefnut y así comenzó el proceso de creación. La mano que utilizó para este acto, la «Mano de Atum», representaba el aspecto femenino de sí mismo y podía ser personificada por Hathor, Nebethetepet o Iusaaset. En un antiguo mito de la creación del periodo ptolemaico (332-30 a. C.), el dios Jonsu desempeña un papel fundamental, y Hathor es la diosa con la que Jonsu se empareja para hacer posible la creación.

Hathor podía ser la consorte de muchos dioses masculinos, de los cuales Ra era solo el más importante. Mut era la consorte habitual de Amón, la principal deidad durante el Imperio Nuevo, que a menudo estaba relacionada con Ra. Pero a Mut se le representaba en pocas ocasiones junto a Amón en contextos relacionados con el sexo o la fertilidad, y en esas situaciones, Hathor o Isis estaban a su lado. En los últimos períodos de la historia egipcia, la forma de Hathor en Dendera y la de Horus en Edfu eran consideradas marido y mujer, y en diferentes versiones del mito de la Diosa Distante, Hathor-Rattaui era la consorte de Montu y Hathor-Tefnut la de Shu.

Su aspecto sexual se aprecia en algunos relatos cortos. En un fragmento críptico de un cuento del Imperio Medio, conocido como "La historia del pastor", un pastor se encuentra con una diosa peluda, parecida a un animal, en un pantano y reacciona con terror. Otro día la encuentra como una mujer desnuda y seductora. La mayoría de los egiptólogos que estudiaron esta historia opinan que esta mujer es Hathor o una diosa como ella, una que puede ser salvaje y peligrosa o benigna y erótica. Thomas Schneider interpreta el texto en el sentido de que entre sus dos encuentros con la diosa, el pastor ha hecho algo para apaciguarla.

Fue ensalzada por su hermoso cabello. La literatura egipcia incluye alusiones a un mito que no está claramente descrito en ningún texto que haya sobrevivido, en el que Hathor perdió un mechón de pelo que representaba su encanto sexual. Un texto compara esta pérdida con la pérdida de Horus de su Ojo divino y la de Seth de sus testículos durante la lucha entre los dos dioses, dando a entender que la pérdida del bucle de Hathor fue tan catastrófica para ella como la mutilación de Horus y Set lo fue para ellos.

Fue conocida como «señora del amor», como una prolongación de su aspecto sexual. En la serie de poemas de amor del papiro de Chester Beatty I, de la dinastía XX (c. 1189-1077 a. C.), hombres y mujeres le piden a Hathor que les traiga a sus amantes: «Le recé a ella [Hathor] y ella escuchó mi plegaria. Ella destinó a mi señora [amada] para mí. Y ella vino por su propia voluntad a verme.»

Estaba considerada la madre de varias deidades infantiles. Como indica su nombre, a menudo se la consideraba tanto la madre como la consorte de Horus. Como esposa del rey y madre de su heredero, Hathor era la contraparte divina de las reinas humanas.

Isis y Osiris se consideraban los padres de Horus en el "Mito de Osiris" ya en el Imperio Antiguo, pero la relación entre Horus y Hathor puede que sea incluso más antigua. Si es así, Horus sólo se relacionó con Isis y Osiris cuando surgió el "Mito de Osiris" durante el Imperio Antiguo. Incluso después de que Isis se consolidó claramente como la madre de Horus, Hathor continuó apareciendo en este papel, especialmente cuando amamantaba al faraón. Imágenes de Hathor representada como vaca con un niño en un matorral de papiros representaban su educación mitológica en un pantano aislado. La leche de las diosas era un signo de divinidad y estatus real. Así, las imágenes en las que Hathor cuida al faraón representan su derecho a gobernar. La relación de Hathor con Horus le confirió un aspecto curativo a su personalidad, ya que se decía que había restaurado el ojo u ojos perdidos de Horus después de que Set lo atacara. En la versión de este episodio de "Las disputas de Horus y Seth", Hathor encuentra a Horus con los ojos arrancados y cura las heridas con leche de gacela.

A partir del período tardío (664-323 a. C.), los templos se centraron en la adoración de una familia divina: una deidad masculina adulta, su esposa y su hijo todavía pequeño. Los edificios auxiliares conocidos como mammisis, fueron construidos para celebrar el nacimiento de la deidad infantil local. El dios niño representaba la renovación cíclica del cosmos y un heredero arquetípico de la realeza. Hathor fue la madre en muchas de estas tríadas locales de dioses. En Dendera, el Horus adulto de Edfu era el padre y Hathor la madre, mientras que su hijo era Ihy, un dios cuyo nombre significaba «músico del sistro» y que personificaba el júbilo asociado con este instrumento. Entre otros hijos de Hathor había una deidad menor de la ciudad de Hu, llamada Neferhotep, y varias formas infantiles de Horus.

La savia lechosa del sicomoro, que los egipcios consideraban un signo de vida, se convirtió en uno de sus símbolos. La leche se equiparaba con las aguas de la inundación del Nilo y, por lo tanto, con la fertilidad. A finales de los periodos ptolemaico y romano, muchos templos incluían un mito de la creación que adaptaba ideas ancestrales sobre la creación. La versión del templo de Hathor en Dendera enfatiza que ella, como deidad solar femenina, fue el primer ser en emerger de las aguas primordiales que precedieron a la creación, y que su luz y su leche vivificantes nutrieron a todos los seres vivos.

Al igual que Mesjenet, otra diosa relacionada con la maternidad, Hathor estaba ligada al "shai", el concepto egipcio del destino, sobre todo cuando adoptó la forma de las «Siete Hathores». En dos obras de ficción del Imperio Nuevo, la "Historia de los dos hermanos" y la "Historia del príncipe predestinado", las Hathores aparecen en los nacimientos de los personajes principales y predicen la forma en la que mueren.

Sus facetas maternales pueden compararse con las de Isis y Mut, pero existen muchos matices diferentes entre ellas. La devoción de Isis hacia su esposo y el cuidado de su hijo representaba una forma de amor socialmente más aceptable que la sexualidad desinhibida de Hathor, y el carácter de Mut era más autoritario que sexual. En el "Papyrus Insinger", un texto del siglo I d. C., se compara a una esposa fiel, la señora de un hogar, con Mut, mientras que a Hathor la compara con una mujer extraña que seduce a un hombre casado.

Egipto mantenía relaciones comerciales con las ciudades costeras de Siria y Canaán, especialmente Biblos, lo que ponía a la religión egipcia en contacto con las de esa región. En algún momento, quizás ya durante el Imperio Antiguo, los egipcios comenzaron a hacer referencia a la diosa patrona de Biblos, Baalat Gebal, como una forma local de Hathor. Su vínculo con Biblos era tan fuerte que los textos de Dendera dicen que residía allí. En algunas ocasiones los egipcios equiparaban a Hathor con Anat, una diosa cananea también agresiva y sensual que fue adorada en Egipto durante el Imperio Nuevo. Algunas obras de arte cananeas representan a una diosa desnuda con una peluca rizada que proviene de la iconografía de Hathor. No se sabe qué diosa representan estas imágenes, pero los egipcios adoptaron su iconografía y llegaron a considerarla como una deidad independiente, Qetesh, a la que relacionaron con Hathor.

Su carácter solar puede que haya jugado un papel importante en su vinculación con el comercio: se creía que protegía a los barcos en el Nilo y en los mares más allá de Egipto, ya que protegía la barca de Ra en el cielo. El peregrinaje mitológico de la diosa Ojo en Nubia o Libia también la ligó con esas tierras.

Estaba estrechamente relacionada con la península del Sinaí, que no se consideraba parte de Egipto propiamente dicho, sino que era el emplazamiento de minas egipcias de cobre, turquesa y malaquita durante los imperios Medio y Nuevo. Uno de los epítetos de Hathor, «Señora de la turquesa», puede hacer referencia específica a la turquesa o a todos los minerales de color verde azulado. También se la llamaba «Señora de la fayenza» ", una cerámica azul verdosa que los egipcios comparaban con la turquesa. También era adorada en varias canteras y emplazamientos mineros del desierto arábigo de Egipto, como las minas de amatista de Wadi el-Hudi, donde a veces se la llamaba «Señora de la amatista».

En el sur de Egipto, se cree que su influencia se extendió hasta Punt, que se encontraban a lo largo de la costa del mar Rojo y que fueron la principal fuente de incienso con el que se ligaba a Hathor, así como a Nubia, al noroeste de Punt. La autobiografía de Hirjuf, un funcionario de la dinastía VI (c. 2345-2181 a. C.), describe su expedición a una tierra en o cerca de Nubia, de la que trajo grandes cantidades de ébano, pieles de pantera e incienso para el rey; el texto describe estos bienes exóticos como un regalo de Hathor al faraón. Las expediciones egipcias para extraer oro en Nubia introdujeron su culto a la región durante los imperios Medio y Nuevo, y los faraones del Imperio Nuevo le construyeron varios templos en las regiones de Nubia en las que gobernaban.

Era una de las varias diosas que se creía que ayudaban a las almas fallecidas en la otra vida. Una de ellas era Amentit, la diosa del oeste, que personificaba las necrópolis o grupos de tumbas en la margen oeste del Nilo y el reino de la vida después de la muerte en sí mismo. A menudo se la consideraba una manifestación específica de Hathor.

Del mismo modo que cruzó la frontera entre Egipto y otras tierras, Hathor traspasó la frontera entre los vivos y la Duat, el reino de los muertos. Ayudaba a los espíritus de los humanos fallecidos a entrar en la Duat y estaba estrechamente vinculada a las tumbas, donde se iniciaba esa transición. La Necrópolis tebana, por ejemplo, se presentaba a menudo como una montaña estilizada de la que salía la vaca Hathor. Su papel como diosa del cielo también estaba ligado a la vida después de la muerte. Como diosa del cielo, ya fuera Nut o Hathor, que asistía a Ra en su renacer diario, tuvo un papel importante en las creencias egipcias sobre la vida después de la muerte, según las cuales los humanos fallecidos renacían como el dios sol. Ataúdes, tumbas y el propio inframundo se interpretaron como el vientre de esta diosa, de la que renacería el alma fallecida.

Nut, Hathor y Amentit podían, en diferentes textos, llevaban a los difuntos a un lugar donde recibirían comida y bebida para el sustento eterno. Así Hathor, como Amentit, aparece a menudo en las tumbas, dando la bienvenida a la persona fallecida como a su hijo en una vida después de la muerte. En los textos funerarios y obras de arte del Imperio Nuevo, la otra vida se ilustraba a menudo como un jardín agradable y fértil, que a veces era presidido por Hathor. La acogedora diosa de la vida después de la muerte fue representada a menudo como una diosa en forma de árbol, dando agua al difunto. Nut cumplía más habitualmente este papel, pero la diosa del árbol a veces era llamada Hathor.

La vida después de la muerte también tenía un componente sexual. En el "mito de Osiris", el dios asesinado resucitó cuando copuló con Isis y concibió a Horus. En la ideología solar, la unión de Ra con la diosa del cielo permitió su propio renacer. Por lo tanto, el sexo permitió el renacimiento del difunto, y diosas como Isis y Hathor contribuyeron a despertar al difunto a una nueva vida. No obstante, simplemente estimulaban los poderes regenerativos de las deidades masculinas, en lugar de desempeñar el papel central.

Los antiguos egipcios precedían los nombres del difunto con el nombre de Osiris para conectarlos con su resurrección. Por ejemplo, una mujer llamada Henutmehyt sería «Osiris-Henutmehyt». Con el tiempo, se fue asociando cada vez más a los difuntos con poderes divinos masculinos y femeninos. Ya en el Imperio Antiguo, en ocasiones se decía que las mujeres se unían a los adoradores de Hathor en la otra vida, al igual que los hombres se unían a los seguidores de Osiris. En el Tercer periodo intermedio (c. 1070-664 a. C.), los egipcios comenzaron a añadir el nombre de Hathor al de las mujeres fallecidas en lugar del de Osiris. En algunos casos, las mujeres fueron llamadas «Osiris-Hathor», lo que indica que se beneficiaban del poder revivificante de ambas deidades. Durante estos períodos tardíos, a veces se decía que Hathor gobernaba la vida después de la muerte como lo hizo Osiris.

Con frecuencia se la representa como una vaca con el disco solar entre los cuernos, especialmente cuando se la veía amamantando al rey. También podría aparecer como una mujer con cabeza de vaca. Sin embargo su representación más habitual era la de una mujer que llevaba un tocado con los cuernos y el disco solar, a menudo con un vestido tubo rojo o turquesa, o uno que combinaba ambos colores. A veces los cuernos se situaban sobre un modio bajo o el tocado de buitre típico de las reinas egipcias del Imperio Nuevo. Como Isis adoptó el mismo tocado durante el Imperio Nuevo, las dos diosas solo pueden distinguirse si la imagen tiene un rótulo escrito. En su papel de Amentit, Hathor lucía en su cabeza el emblema del oeste en lugar del tocado con cuernos. Las Siete Hathores se representaban en ocasiones como un conjunto de siete vacas, acompañadas por una deidad menor del cielo y de la vida después de la muerte llamada el Toro del Oeste.

También podía ser representada en la figura de otros animales, además de la vaca. El ureo era un motivo habitual en el arte egipcio y podía representar a diversas diosas que se identificaban con el Ojo de Ra. Cuando se la mostraba como un ureo, representaba los aspectos más violentos y protectores de su carácter; también aparece en ocasiones como una leona, con un sentido similar. Por el contrario, el gato doméstico, que a veces se asociaba a Hathor, a menudo representaba la forma pacífica de la diosa Ojo. Cuando se la representaba como un sicomoro, aparecía con la parte superior de su cuerpo humano emergiendo del tronco.

Como otras diosas, podía aparecer con un tallo de papiro como bastón, aunque en su lugar a veces sostenía un cetro uas, símbolo de poder que normalmente estaba restringido a las deidades masculinas. Las únicas diosas que utilizaron el uas eran las que, como Hathor, estaban relacionadas con el Ojo de Ra. También solía portar un sistro o lucir un collar "menat". El sistro se muestra con dos variantes: una con forma de nudo simple, o el más complejo sistro "naos", que se asemeja a una cela o naos de un templo y está flanqueada por volutas que recuerdan a las antenas del símbolo de Bat. El collar "menat", con forma de platillo compuesto de muchas filas de cuentas, se agitaba en ceremonias en honor de Hathor, de forma similar al sistro. Imágenes de este objeto a veces se consideraban personificaciones de la propia Hathor. Otro de sus símbolos eran los espejos, porque en Egipto a menudo estaban elaborados con oro o bronce y por lo tanto simbolizaban el disco solar, y también porque estaban relacionados con la belleza y la feminidad. Algunos mangos de espejos incluían la cara de Hathor.

A veces se representaba como un rostro humano con orejas de vacuno, visto de frente y no desde la perspectiva de perfil típica del arte egipcio. Cuando se representa en esta forma, el pelo a cada lado de su cara a menudo se enrosca formando bucles. Este rostro en forma de máscara apareció en capiteles de columnas a partir del Imperio Antiguo. Columnas de este estilo se utilizaron en muchos templos para Hathor y otras diosas. Estas columnas tienen dos o cuatro caras, que pueden representar la dualidad entre los diferentes aspectos de la diosa o la vigilancia de la Hathor de las Cuatro Caras. Los diseños de las columnas hathóricas tienen una relación compleja con los de los sistros. Ambos estilos de sistro pueden incluir la máscara Hathor en el mango, y las columnas hathóricas a menudo incorporan la forma de un sistro "naos" sobre la cabeza de la diosa.

Durante el Periodo arcaico Neit era la diosa dominante en la corte real, mientras que en la dinastía IV, Hathor se convirtió en la diosa más estrechamente vinculada con el rey. El fundador de esta dinastía, Seneferu, es posible que le construyera un templo, y una hija de Dyedefra fue la primera sacerdotisa de Hathor de la que se tiene constancia. Los gobernantes del Imperio Antiguo únicamente hacían contribuciones a templos dedicados a reyes particulares o a deidades estrechamente relacionadas con la realeza. Hathor fue una de las pocas deidades que recibió esa clase de donaciones. Los últimos gobernantes del Imperio Antiguo promovieron especialmente el culto a Hathor en las provincias, como una forma de vincular a esas regiones a la corte real. Es posible que Hathor haya asumido los atributos de las diosas provinciales contemporáneas.

Muchas mujeres de la realeza, aunque no fueran monarcas reinantes, ocuparon cargos en la administración del culto a Hathor durante el Imperio Antiguo. Mentuhotep II, que se convirtió en el primer faraón del Imperio Medio a pesar de no tener ninguna relación con los gobernantes del Imperio Antiguo, trató de legitimar su gobierno representándose a sí mismo como hijo de Hathor. Las primeras imágenes de la vaca Hathor amamantando al rey datan de su reinado, y varias de sus sacerdotisas fueron representadas como si fueran sus esposas, aunque puede que en realidad no se hubiera casado con ellas. En el curso del Imperio Medio, las reinas se veían cada vez más como la encarnación directa de la diosa, del mismo modo que el rey encarnaba a Ra. El interés en representar a la reina como Hathor continuó a lo largo del Imperio Nuevo. Las reinas se representaron con el tocado de Hathor a partir de finales de la dinastía XVIII. Una imagen del Heb Sed de Amenofis III, destinada a celebrar y renovar su reinado, muestra al rey junto a Hathor y su esposa la reina Tiy, lo que podría significar que el rey se casó simbólicamente con la diosa en el transcurso de la fiesta.

Hatshepsut, una mujer que gobernó como faraón en los primeros tiempos del Imperio Nuevo, destacó su relación con Hathor de una manera distinta. Utilizó nombres y títulos que la relacionaban con varias diosas, entre ellas Hathor, para legitimar su gobierno, que normalmente era una función masculina. Le construyó varios templos a Hathor y erigió su propio templo funerario, que incorporaba una capilla dedicada a la diosa, en Deir el-Bahari, que había sido un lugar de culto a Hathor desde el Imperio Medio.

La importancia de Amón durante el Imperio Nuevo dio mayor visibilidad a su consorte Mut y en el transcurso de este período, Isis comenzó a aparecer en funciones que tradicionalmente pertenecían únicamente a Hathor, como la de la diosa en la barca solar. A pesar de la creciente relevancia de estas deidades, Hathor siguió siendo importante durante todo el Imperio Nuevo, particularmente en relación con la fertilidad, la sexualidad y la realeza.

Después del Imperio Nuevo Isis eclipsó cada vez más a Hathor y a otras diosas al asumir sus funciones. Durante el Período helenístico de Egipto, cuando los griegos gobernaron el país y su religión desarrolló una compleja relación con la de Egipto, la dinastía ptolemaica adoptó y modificó la ideología egipcia sobre la divinidad de la realeza. Comenzando con Arsínoe II, esposa de Ptolomeo II, los ptolomeos asociaron estrechamente a sus reinas con Isis y con varias diosas griegas, en particular con su propia diosa del amor y la sexualidad, Afrodita. Sin embargo, cuando los griegos hacían referencia a los dioses egipcios con los nombres de sus propios dioses ("interpretatio graeca"), en ocasiones llamaban a Hathor Afrodita. Los rasgos de Isis, Hathor y Afrodita se combinaron para justificar el tratamiento de las reinas ptolemaicas como diosas. Así, el poeta Calímaco aludió al mito del mechón de pelo perdido de Hathor al elogiar a Berenice II por sacrificar su propio pelo a Afrodita, y los rasgos iconográficos que compartían Isis y Hathor, como los cuernos bovinos y el tocado de buitre, aparecieron en las imágenes que retratan a las reinas ptolemaicas como Afrodita.

Se dedicaron más templos a Hathor que a cualquier otra diosa egipcia. Durante el Imperio Antiguo, su centro de culto más importante estaba en la región de Menfis, donde la «Hathor del sicomoro» era adorada en muchos lugares a lo largo de la Necrópolis Menfita. Durante el Imperio Nuevo, el templo de la Hathor del sicomoro del sur fue su principal templo en Menfis. Allí se la describió como hija de la principal deidad de la ciudad, Ptah. El culto de Ra y Atum en Heliópolis, al noreste de Menfis, incluía un templo a Hathor-Nebethetepet que probablemente fue construido en el Imperio Medio. Un sauce y un sicomoro estaban cerca del santuario y es posible que hayan sido adorados como manifestaciones de la diosa. Algunas ciudades más al norte del delta del Nilo, como Yamu y Terenuthis, también tenían templos dedicados a ella.

Cuando los gobernantes del Imperio Antiguo se dedicaron a establecer ciudades en el Alto y Medio Egipto, se fundaron varios centros de culto de Hathor en toda la región, en lugares como Cusae, Akhmim y Naga ed-Der. En el Primer periodo intermedio (c. 2181-2055) su estatua de culto en Dendera se trasladaba periódicamente a la Necrópolis tebana. A comienzos del Imperio Medio, Mentuhotep II le erigió un centro de culto permanente en la necrópolis de Deir el-Bahari. Un pueblo próximo, Deir el-Medina, hogar de los trabajadores de las tumbas de la necrópolis durante el Imperio Nuevo, también contaba con templos dedicados a Hathor. Uno de ellos siguió operativo y fue reconstruido periódicamente hasta el periodo ptolemaico, siglos después de que el pueblo fuera abandonado.

Dendera, el templo de Hathor más antiguo del Alto Egipto, data al menos de la dinastía IV. Tras el fin del Imperio Antiguo superó en importancia a sus templos menfitas. Muchos reyes realizaron ampliaciones al complejo del templo a lo largo de la historia egipcia. La última versión del templo fue construida en los periodos ptolemaico y romano y hoy en día es uno de los templos egipcios mejor conservados de esa época.

Durante el Imperio Antiguo la mayoría de los sacerdotes de Hathor, incluidos los de más alto rango, eran mujeres. Muchas de estas mujeres eran miembros de la familia real. A lo largo del Imperio Medio las mujeres fueron progresivamente excluidas de los más altos cargos sacerdotales, mientras que las reinas estaban cada vez más vinculadas al culto de Hathor. Así, las mujeres que no eran de la realeza desaparecieron de los altos puestos de su sacerdocio, aunque las mujeres continuaron sirviendo como músicas y cantoras en los cultos de los templos en todo Egipto.

El rito más frecuente en los templos para cualquier deidad era el ritual de la ofrenda diaria, en el que la imagen o estatua de culto se vestía y se le daba de comer. En general, el rito diario era el mismo en todos los templos egipcios, aunque los elementos que se ofrecían como ofrendas podían variar según la deidad que los recibiera. El vino y la cerveza eran ofrendas comunes en todos los templos, pero especialmente en los rituales en honor de Hathor, y tanto ella como las diosas relacionadas con ella a menudo recibían sistros y collares "menat". En los períodos tardío y ptolemaico también se les ofrecía un par de espejos, que representaban el sol y la luna. 

Muchas de las fiestas anuales en su honor se celebraban con bebidas y danzas que tenían un propósito ritual. Los que participaban en estos festivales es posible que trataran de alcanzar un estado de éxtasis religioso, que de otra manera era muy poco común o inexistente en la antigua religión egipcia. La egiptóloga Graves-Brown señala que los celebrantes en los festivales de Hathor buscaban alcanzar un estado alterado de conciencia que les permitiera interactuar con el reino divino. Un ejemplo sería la Fiesta de la Ebriedad, que conmemoraba el regreso del Ojo de Ra, que se celebraba el vigésimo día del mes de Tot en los templos de Hathor y de otras diosas Ojo. Se celebraba ya durante el Imperio Medio, pero era más conocido en la época ptolemaica y romana. El baile, la comida y la bebida que se realizaba durante la Fiesta de la Ebriedad representaba lo contrario del dolor, el hambre y la sed que los egipcios asociaban con la muerte. Mientras que la violencia del Ojo de Ra trajo la muerte a los humanos, la Fiesta de la Ebriedad celebraba la vida, la abundancia y la alegría.

En una fiesta tebana local conocida como la Bella Fiesta del Valle, que comenzó a celebrarse en el Imperio Medio, la imagen de culto de Amón del templo de Karnak visitaba los templos de la Necrópolis tebana mientras los miembros de la comunidad se dirigían a las tumbas de sus parientes fallecidos para beber, comer y divertirse. Hathor no intervino en esta fiesta hasta principios del Imperio Nuevo, tras lo cual la presencia de Amón en los templos de Deir el-Bahari se consideró como su unión sexual con la diosa.

Varios templos de la época ptolemaica, entre ellos el de Dendera, celebraban el año nuevo egipcio con una serie de ceremonias en las que se suponía que las imágenes de la deidad del templo se revitalizarían por el contacto con el dios sol. En los días previos al año nuevo, la estatua de Hathor de Dendera se llevaba al "wabet", una sala específica del templo dedicada a la unión de las imágenes de culto con el disco solar, y se colocaba bajo un techo decorado con imágenes del cielo y del sol. El primer día del año nuevo (el primer día del mes de Tot) la imagen de Hathor se llevaba hasta el techo para que fuera bañada por la auténtica luz del sol.

La celebración mejor documentada centrada en su culto es otra festividad ptolemaica, la Fiesta de la Bella Reunión. Tenía lugar durante catorce días en el mes de Apep. La imagen de Hathor en Dendera se llevaba en barco a varios templos para visitar a los dioses de esos templos; el punto final del viaje era el templo de Horus en Edfu, donde su imagen se encontraba con la de Horus y las dos se colocaban juntas. Durante un día de la fiesta, estas imágenes se llevaban a un santuario donde se decía que estaban enterradas deidades primordiales como el dios sol y la Enéada. Los textos afirman que la pareja divina realizaba ritos de ofrenda a estos dioses enterrados. Muchos egiptólogos consideran esta fiesta como un matrimonio ritual entre Horus y Hathor, aunque Martin Stadler cuestiona esta idea y en su opinión representaba el rejuvenecimiento de los dioses creadores enterrados. C. J. Bleeker consideraba que la Bella Reunión era otra celebración del regreso de la Diosa Distante, citando alusiones al mito del ojo solar en los textos del templo sobre la fiesta. Barbara Richter sostiene que la fiesta representaba las tres cosas a la vez; señala que el nacimiento del hijo de Horus y Hathor, Ihy, se celebraba en Dendera nueve meses después de la Fiesta de la Bella Reunión, por lo que la visita de Hathor a Horus representaba la concepción de Ihy.

El tercer mes del calendario egipcio, Hathor o Athyr, debe su nombre a la diosa. Las fiestas en su honor se celebraban durante todo el mes, aunque no se recogen en los textos de Dendera.

Ya en los tiempos del Imperio Antiguo, los reyes egipcios ofrecían bienes al templo de Baalat Gebal en Biblos, utilizando el sincretismo de Baalat con Hathor para afianzar su intensa relación comercial con Biblos. Durante el reinado de Tutmosis III se construyó un templo dedicado a Hathor como Señora de Biblos, aunque es posible que simplemente fuera un santuario dentro del templo de Baalat. Tras la caída del Imperio Nuevo, la relevancia de Hathor en Biblos disminuyó junto con los vínculos comerciales de Egipto con la ciudad. Algunos objetos de principios del primer milenio antes de Cristo parecen indicar que en esa época los egipcios comenzaron a equiparar a Baalat con Isis. Un mito sobre la presencia de Isis en Biblos, relatado por el autor griego Plutarco en su obra "e Isis y Osiris" en el siglo II d. C., parece indicar que en su época Isis ya había sustituido por completo a Hathor en la ciudad.

Los egipcios del Sinaí construyeron algunos templos en la región. El más grande era un complejo en Sarabit al-Jadim, en el lado oeste de la península, dedicado fundamentalmente a Hathor como patrona de la minería. Se ocupó desde mediados del Imperio Medio hasta cerca del final del Nuevo. Al este de la península el valle de Timna, en los límites del imperio egipcio, fue el lugar de las expediciones mineras estacionales durante el Nuevo Reino; incluía un santuario a Hathor que probablemente fue abandonado durante la temporada baja. Los madianitas locales, a quienes los egipcios utilizaban como parte de la mano de obra minera, puede que le hayan realizado ofrendas a Hathor como lo hicieron sus superiores. Sin embargo, después de que los egipcios abandonaron el lugar durante la dinastía XX, los madianitas convirtieron el santuario en una capilla dedicada a sus propias deidades.

En cambio, los nubios del sur la incorporaron plenamente a su religión. Durante el Imperio Nuevo, cuando la mayor parte de Nubia estaba bajo control egipcio, los faraones le dedicaron a Hathor varios templos en Nubia, como los de Faras y Mirgissa. Amenofis III y Ramsés II construyeron templos en Nubia que honraban a sus respectivas reinas como manifestaciones de las deidades femeninas, incluida Hathor: la esposa de Amenofis, Tiy, en Sedeinga, y la de Ramsés, Nefertari, en el Templo menor de Abu Simbel. El reino independiente de Kush, que surgió en Nubia tras el fin del Imperio Nuevo, centró sus creencias sobre los en la ideología de la realeza egipcia. Así, Hathor, Isis, Mut y Nut fueron consideradas como la madre mitológica de cada rey kushita y equiparadas con sus parientes femeninos, como las "kandake", la reina kushita o la reina madre, que desempeñaban un papel destacado en la religión kushita. En Gebel Barkal, un lugar sagrado para Amón, el rey kushita Taharqo construyó un par de templos, uno dedicado a Hathor y otro a Mut como consortes de Amón, remplazando a los templos egipcios del Imperio Nuevo, que es posible que se dedicaban a estas mismas diosas. No obstante Isis era la más destacada de las diosas egipcias adoradas en Nubia y su posición allí aumentó con el tiempo. Así, en el período meroítico de la historia de Nubia (c. 300 a. C.-400 d. C.), Hathor aparecía en los templos sobre todo como una compañera de Isis.

Además de los rituales formales y públicos en los templos, los egipcios adoraban en privado a las deidades por razones personales, incluso en sus hogares. El parto era peligroso tanto para la madre como para el niño en el antiguo Egipto, pero los niños eran muy deseados, de ahí que la fertilidad y un parto seguro estuviesen entre las principales inquietudes de su religión popular y las diosas de la fertilidad como Hathor y Tueris eran adoradas con frecuencia en los santuarios de los hogares. Las mujeres egipcias para dar a luz se ponían de rodillas o en cuclillas sobre una «silla de partos» confeccionada con ladrillos de adobe con un agujero central, y el único ladrillo de parto conocido que se conserva del antiguo Egipto está decorado con una imagen de una mujer que sostiene a su hijo flanqueada por imágenes de Hathor. En la época romana, algunas figurillas de terracota, en ocasiones encontradas en un ámbito doméstico, representaban a una mujer con un elaborado tocado que exponía sus genitales, como hizo Hathor para animar a Ra. Se desconoce el significado de estas figuras, pero se cree que representan a Hathor o Isis combinadas con Afrodita haciendo un gesto que representaba la fertilidad o la protección contra el mal.

Hathor era una de las pocas deidades, incluidas Amón, Ptah y Tot, a las que habitualmente se les oraba solicitando ayuda con los problemas personales. Muchos egipcios dejaban ofrendas en templos o pequeños santuarios dedicados a los dioses a los que oraban. La mayoría de las ofrendas a Hathor se utilizaban por su simbolismo, no por su valor material. Eran frecuentes las telas pintadas con imágenes de Hathor, así como placas y figuras que representaban sus formas animales. Es posible que los distintos tipos de ofrendas hayan simbolizado diferentes objetivos por parte del donante, pero por lo general se desconoce su significado. Algunas imágenes hacen alusión a sus funciones míticas, como las representaciones de la vaca materna en el pantano. Las ofrendas de sistros puede que se hicieran para apaciguar los aspectos peligrosos de la diosa y sacar a relucir sus cualidades positivas, mientras que el falo representaba una oración por la fertilidad, como demuestra una inscripción encontrada en una talla casera en piedra de una figurilla realizada por un trabajador pidiendo una familia.

Algunos egipcios también dejaron oraciones escritas a Hathor, grabadas en estelas o escritas como grafitis. Oraciones a algunas deidades, como Amón, indican que se creía que castigaban a los malhechores y curaban a las personas que se arrepentían de su mal comportamiento. En cambio, en las oraciones a Hathor solo se mencionan los beneficios que podía otorgar, como la abundancia de alimentos durante la vida y un entierro bien provisto después de la muerte.

Como una diosa de la vida después de la muerte, aparece frecuentemente en textos y arte funerario. Junto con Osiris y Anubis, Hathor era una de las deidades más comunes en la decoración de tumbas reales durante el Imperio Nuevo temprano. En esa época aparecía a menudo como la diosa que recibía a los muertos en la otra vida. Algunas imágenes hacían referencia a ella de forma más indirecta. Relieves en tumbas del Imperio Antiguo muestran a hombres y mujeres realizando un ritual llamado «agitar el papiro»; se desconoce el significado de este rito, pero algunas inscripciones indican que se realizó «para Hathor», y al sacudir los tallos de papiro se produce un crujido que puede compararse con el sonido de un sistro. Entre otras imágenes hathóricas en tumbas están la vaca que emerge de la montaña de la necrópolis y la figura de la diosa sentada presidiendo un jardín en la vida después de la muerte. A menudo se pintaban o grababan imágenes de Nut en el interior de los ataúdes, indicando que el ataúd era su vientre, desde el que el ocupante renacería en la otra vida. Durante el Tercer periodo intermedio Hathor comenzó a colocarse en el fondo del ataúd, con Nut en el interior de la tapa.

El arte funerario de la Dinastía XVIII muestra a menudo a gente bebiendo, bailando y tocando música, y también sosteniendo collares "menat" y sistros, una imaginería que hace alusión a Hathor. Estas imágenes pueden representar fiestas privadas que se celebraban frente a las tumbas para conmemorar a las personas enterradas allí, o pueden mostrar reuniones en fiestas del templo como la Bella Fiesta del Valle. Se creía que las fiestas permitían el contacto entre los reinos humano y divino y, por extensión, entre los vivos y los muertos. Así, los textos de las tumbas expresaban a menudo el deseo de que los difuntos pudieran participar en las fiestas, sobre todo las dedicadas a Osiris. Sin embargo, las imágenes de las fiestas de las tumbas pueden referirse a las fiestas en las que participa Hathor, como la Fiesta de la Ebriedad, o a las fiestas privadas, que también estaban estrechamente relacionadas con ella. Beber y bailar en estas fiestas puede que fuera para embriagar a los celebrantes, como en la Fiesta de la Ebriedad, permitiéndoles entrar en comunión con los espíritus de los difuntos.

Se decía que Hathor proporcionaba ofrendas a personas fallecidas ya en el Imperio Antiguo, y los conjuros que permitían tanto a hombres como a mujeres unirse a su séquito en la vida después de la muerte aparecieron ya en los "Textos de los sarcófagos" en el Imperio Medio. Algunos objetos funerarios que muestran a las mujeres fallecidas como diosas es posible que representen a estas mujeres como seguidoras de Hathor, aunque no se sabe si las imágenes se refieren a Hathor o a Isis. El vínculo entre Hathor y las mujeres fallecidas se mantuvo en el período romano de Egipto, la última etapa de la antigua religión egipcia antes de su desaparición.





</doc>
<doc id="7109" url="https://es.wikipedia.org/wiki?curid=7109" title="Turbina hidráulica">
Turbina hidráulica

Una turbina hidráulica es una turbomáquina motora hidráulica, que aprovecha la energía de un fluido que pasa a través de ella para producir un movimiento de rotación que, transferido mediante un eje, mueve directamente una máquina o bien un generador eléctrico que transforma la energía mecánica en eléctrica, así son el órgano fundamental de una central hidroeléctrica.

Fue inventada por Benoît Fourneyron en 1827, que instaló su primera máquina en Pont-sur-l'Ognon.

Por ser turbomáquinas siguen la misma clasificación de estas, y pertenecen, obviamente, al subgrupo de las turbomáquinas hidráulicas y al subgrupo de las turbomáquinas motoras. En el lenguaje común de las turbinas hidráulicas se suele hablar en función de las siguientes clasificaciones:


Para clasificar a una turbina dentro de esta categoría se requiere calcular el grado de reacción de la misma. Las turbinas de acción aprovechan únicamente la velocidad del flujo de agua, mientras que las de reacción aprovechan además la pérdida de presión que se produce en su interior.

Esta clasificación es la más determinista, ya que entre las distintas de cada género las diferencias sólo pueden ser de tamaño, ángulo de los àlabes o cangilones, o de otras partes de la turbomáquina distinta al rodete. Los tipos más importantes son:



</doc>
<doc id="7113" url="https://es.wikipedia.org/wiki?curid=7113" title="Palíndromo">
Palíndromo

Un palíndromo (del griego "palin dromein", ‘volver a ir atrás’), también llamado palindromo, palíndroma o palindroma, es una palabra o frase que se lee igual en un sentido que en otro (por ejemplo, Ana). Si se trata de números en lugar de letras, se llama capicúa. Habitualmente, las frases palindrómicas se resienten en su significado cuanto más largas son.

Habitualmente se entiende por palíndromo aquel que toma por unidad la letra, es decir, cuya última letra es la misma que la primera, la penúltima es la misma que la segunda, etc. Es el caso de palabras tales como "reconocer" o "anilina". Sin embargo, también se puede tomar como unidad la sílaba (por ejemplo, "gato" con "toga", aunque en este caso podría ser calificado como anagrama), la palabra o incluso el renglón.

"Ababa", "Abalaba", "aibofobia", "Ana", "ala", "arenera", "arepera", "anilina", "aviva", "Malayalam", "Menem", "Neuquén", "oso", "Oruro", "ojo", "radar", "reconocer", "rotor", "salas", "seres", "somos", "sometemos", "solos", "sosos".























S A T O R
A R E P O
T E N E T
O P E R A
R O T A S

Algunos sugieren que las letras de este cuadrado se puede reordenar para que se lea «paternoster» dos veces y quedaría A y O (alfa y omega). Es decir, se trataría de una oración cristiana:

P

A

A T O

E

R

PATERNOSTER

O

S

O T A

E

R

Se reconocen estructuras externas a la lingüística y al estudio de los signos (semiótica) que admiten lectura en ambos sentidos y son, pues, considerados palíndromos.

Son aquellas palabras que leídas al revés tienen distinto significado. Por ejemplo:

Un día palíndromo es cuando la fecha contiene un palíndromo, como el 2 de febrero de 2020 (02-02-2020); no importa cual formato de fecha el país tenga, la fecha será un palíndromo.




</doc>
<doc id="7114" url="https://es.wikipedia.org/wiki?curid=7114" title="20 de noviembre">
20 de noviembre

El 20 de noviembre es el 324.º (tricentésimo vigésimo cuarto) día del año en el calendario gregoriano y el 325.º en los años bisiestos. Quedan 41 días para finalizar el año.









</doc>
<doc id="7124" url="https://es.wikipedia.org/wiki?curid=7124" title="Sistro">
Sistro

El sistro es un instrumento musical antiguo, con forma de aro o de herradura, que contiene platillos metálicos insertados en unas varillas, y se hace sonar agitándolo. 

El sistro está considerado un instrumento de percusión de la familia de los idiófonos, en la gama de los indirectamente percutidos, o sacudidos, como las maracas, las castañuelas o los cascabeles.

Era muy usado con las diosas Isis, Bat y Bastet.

Hoy todavía se conservan ejemplares y representaciones cerámicas de este instrumento en varios museos, como el Museo Arqueológico Nacional de España, Museo Británico, Louvre, etc. 

El nombre proviene del griego "σείω (seio)", agitar, así que "σείστρον (seistron)" es lo que está siendo agitado.

El sistro fue un instrumento sagrado en el Antiguo Egipto. Quizás originario de la adoración a Bat, se utilizaba en las danzas y ceremonias religiosas, en particular en el culto de la diosa Hathor: la forma de U del sistro recuerda a la cara y cuernos de la vaca diosa. También se agitaba para evitar las inundaciones del Nilo y para asustar a distancia a Seth. Isis en su papel de madre y creadora se representa con un cubo que simboliza las inundaciones del Nilo en una mano y el sistro en la otra. La diosa Bastet muy a menudo se representa con un sistro, que simboliza su papel como diosa de la danza, la alegría y la fiesta.

Los sistros todavía se utilizan en los ritos de la Iglesia Copta tanto en Egipto como en Etiopía. Además de la representación en el arte egipcio en los bailes y como expresión de alegría, el sistro también se menciona en la literatura egipcia, como en las Instrucciones de Amenemhat.




</doc>
<doc id="7130" url="https://es.wikipedia.org/wiki?curid=7130" title="Manufactura">
Manufactura

Manufactura o fabricación es una fase de la producción económica de los bienes. El término puede referirse a un rango de actividad humana, desde la artesanía hasta la alta tecnología, pero se aplica más comúnmente a la producción industrial, que consiste en la transformación de materias primas en productos manufacturados, productos elaborados o productos terminados para su distribución y consumo a gran escala. También involucra procesos de elaboración de productos semi-manufacturados o productos semielaborados.

La ingeniería de fabricación o el proceso de fabricación son los pasos a través de los cuales las materias primas se transforman en un producto final. El proceso de fabricación comienza con el diseño del producto y la especificación de los materiales con los que se fabrica el producto. Estos materiales se modifican a través de procesos de fabricación para convertirse en la parte requerida.

La manufactura es la actividad del sector secundario de la economía, también denominado sector industrial, sector fabril, o simplemente fabricación o industria.

El sector manufacturero está estrechamente relacionado con la ingeniería y el diseño industrial. Algunos ejemplos de los principales fabricantes en América del Norte son General Motors Corporation, General Electric, Procter & Gamble, General Dynamics, Boeing, Pfizer y Precision Castparts. Ejemplos en Europa incluyen Volkswagen Group, Siemens, FCA y Michelin. Ejemplos en Asia incluyen Toyota, Yamaha, Panasonic, Mitsubishi, LG y Samsung.

Inicialmente la manufactura significa una etapa del desarrollo del capitalismo en que la producción era a mano, es decir, producción de los objetos sin intervención de las máquinas; en esta forma de producción a diferencia del taller artesanal, el objeto no es producido por una sola persona, sino por un grupo de ellas, cada una de las cuales ejecuta una u otra operación, lo que conduce a un rápido incremento de la productividad del trabajo, en comparación con el artesano. 

El término puede referir a una variedad de la actividad humana, desde la artesanía a la alta tecnología, pero es más comúnmente aplicado a la producción industrial, en la cual las materias primas son transformadas en bienes terminados a gran escala y con la utilización de máquinas y fuentes de energía más allá del simple trabajo del hombre.

En el Antiguo Régimen, la denominación "manufactura", y específicamente las Manufacturas Reales, se oponía en la práctica tanto a las instalaciones propias de los talleres gremiales como a las primeras fábricas (que fueron el ámbito donde se desarrolló la Revolución industrial).
La "manufactura" en el sentido de "fabricación" se produce bajo todos los tipos de sistemas económicos, y es una actividad tan propia del ser humano que lo define como especie, siendo los restos de cultura material del Paleolítico, los primeros testimonios de la presencia humana sobre la tierra, al ser más resistentes incluso que los restos anatómicos.

En el sistema económico capitalista, la fabricación se dirige, a través del mercado libre y la libre empresa, hacia la fabricación en serie de productos para la venta a un mercado masivo de consumidores (sociedad de consumo). En los países del denominado socialismo real, que pretendían la construcción de un modo de producción socialista, la fabricación estaba dirigida por una agencia estatal (planificación), y se privilegiaba la industria pesada sobre la de bienes de consumo. En las economías modernas, la fabricación discurre bajo algún grado de regulación gubernamental.

La fabricación moderna incluye todos los procesos intermedios requeridos para la producción y la integración de los componentes de un producto. El sector industrial está estrechamente relacionado con la ingeniería y el diseño industrial.

El proceso puede ser manual (origen del término) o con la utilización de máquinas. Para obtener mayor volumen de producción es aplicada la técnica de la división del trabajo, donde cada trabajador ejecuta sólo una pequeña porción de la tarea. Así, se especializa y economiza movimientos, lo que va a repercutir en una mayor velocidad de producción.

Aunque la producción artesanal ha formado parte de la humanidad desde hace mucho tiempo (desde la Edad Media), se piensa que la manufactura moderna surge alrededor de 1780 con la Revolución industrial británica, expandiéndose a partir de entonces a toda la Europa Continental, luego a América del Norte y finalmente al resto del mundo.

El Arsenal de Venecia ofrece uno de los primeros ejemplos de una empresa manufacturera en el sentido moderno, fundada en 1104 en Venecia; producía casi un barco todos los días y en su apogeo tenía 16.000 empleados especializados.

La manufactura se ha convertido en una porción inmensa de la economía del mundo moderno. Según algunos economistas, la fabricación es un sector que produce riqueza en una economía, mientras que el sector servicios tiende a ser el consumo de la riqueza.




3. Mijailov, M.I "La Revolución Industrial"


</doc>
<doc id="7134" url="https://es.wikipedia.org/wiki?curid=7134" title="Sendmail">
Sendmail

Sendmail es un popular agente de transporte de correo (MTA - Mail Transport Agent) en Internet, cuya tarea consiste en encaminar los mensajes o correos de forma que estos lleguen a su destino. Soporta muchos tipos de métodos de entrega, incluyendo el protocolo "Simple Mail Transfer Protocol" (SMTP) utilizado para el transporte de emails sobre Internet. 

Es un sucesor de "delivermail", un programa escrito por Eric Allman. Sendmail es un proyecto bien conocido por comunidades de software libre y de código abierto y comunidades Unix. Existe en versión de software libre y propietario.

Allman escribió "delivermail" para ARPANET que fue incluido en la versión 4.0 y 4.1 de BSD en 1979. Allman escribió Sendmail como un derivado de delivermail a principios de 1980 para la Universidad de California en Berkeley y fue incluido con BSD 4.1c en 1983, la primera versión BSD que incluía protocolos TCP/IP.

En 1996, aproximadamente el 80% de los servidores de emails públicamente alcanzables corrían Sendmail Encuestas más recientes han sugerido una disminución. Para agosto de 2019 solo el 4.18% de servidores de emails corren Sendmail según un estudio llevado a cabo por E-Soft, Inc.

Otras encuestas han sugerido una disminución menor, con el 24% de servidores de emailscorriendo Sendmail en agosto del 2015, según un estudio llevado a cabo por Mail Radar.

Allman diseño Sendmail para que tenga mucha flexibilidad pero puede su configuración puede ser desalentadora para principiantes. Los paquetes de configuración estándar entregados con el código fuente requieren el uso del lenguaje de macros M4 que esconde gran parte de la complejidad de la configuración. La configuración define las opciones de entrega de emails, sus parámetros de acceso y el mecanismo de reenvío de emails a sitios remotos tanto como parámetros de ajuste de la aplicación.

Sendmail soporta una variedad de protocolos de transferencia incluyendo SMTP, ESMTP, Mail-11 de DECnet, HylaFax, QuickPage y UUCP. Adicionalmente, Sendmail v8.12 en septiembre de 2001 introdujo el soporte para milters - programas de filtrado de emails externos que pueden participar en cada paso de la conversación SMTP.

Según un anuncio del 1 de octubre de 2013 Sendmail, Inc fue adquirido por Proofpoint, Inc.


"La información deriva del archivo RELEASE_NOTES file de la distribución de sendmail ."

Sendmail se originó en los primeros días de internet, una era donde las consideraciones de seguridad no jugaban un rol primario en el desarrollo del software de red. Las primeras versiones de Sendmail sufrieron un número de vulnerabilidades de seguridad que fueron corregidas a lo largo de los años.

Sendmail en sí incorporó cierta cantidad de separación de privilegios para evitar la exposición de los problemas de seguridad. Para el 2009, las versiones de Sendmail, como otros MTAs modernos, incorporan muchas mejoras de seguridad y características opcionales que pueden ser configuradas para mejorar la seguridad y prevenir el abuso.

Vulnerabilidades de Sendmail en avisos y alertas del CERT.

Página Principal de Sendmail


</doc>
<doc id="7137" url="https://es.wikipedia.org/wiki?curid=7137" title="Manga">
Manga

El manga japonés constituye una de las tres grandes tradiciones historietísticas a nivel mundial, junto con la estadounidense y la franco-belga. Abarca una extensa variedad de géneros y llega a públicos diversos. Es una parte muy importante del mercado editorial de Japón y motiva múltiples adaptaciones a distintos formatos: series de animación, conocidas como "anime", o de imagen real, películas, videojuegos y novelas. Cada semana o mes se editan nuevas revistas con entregas de cada serie, al más puro estilo del folletín, protagonizadas por héroes cuyas aventuras en algunos casos seducen a los lectores durante años. Desde los años ochenta han ido conquistando también los mercados occidentales.

Hokusai Katsushika, un representante del ukiyo-e, acuñó el término "manga" combinando los "kanji" correspondientes a "informal" (漫 man) y "dibujo" (画 ga). Se traduce, literalmente, como «dibujos caprichosos» o «garabatos». Los japoneses llaman también al manga «imágenes insignificantes», pues compran al año más de mil millones de volúmenes en blanco y negro, impresos en papel barato. Al profesional que escribe o dibuja mangas se le conoce como "mangaka". Algunos autores producen asimismo sus mangas en vídeo.

Actualmente, la palabra manga se usa en Japón para referirse a "historietas", de forma general. Fuera de Japón, esta palabra se emplea más concretamente para referirse de estilo japonés de dibujar y contar historias.

En el manga las viñetas y páginas se leen de derecha a izquierda, la mayoría de los mangas originales que se traducen a otros idiomas respetado este orden. El más popular y reconocido estilo de manga tiene otras características distintivas, muchas de ellas por influencia de Osamu Tezuka, considerado el padre del manga moderno.

Scott McCloud señala, por ejemplo, la tradicional preeminencia de lo que denomina "efecto máscara", es decir, la combinación gráfica de unos personajes caricaturescos con un entorno realista, como sucede en la "línea clara". En el manga es frecuente, sin embargo, que se dibujen de forma más realista algunos de los personajes u objetos (estos últimos para indicar cuando sea necesario sus detalles). McCloud detecta asimismo una mayor variedad de las transiciones entre viñetas que en los cómics occidentales, con una presencia más sustancial del tipo que denomina «aspecto a aspecto», en la que el tiempo no parece avanzar. Igualmente, hay que destacar el gran tamaño de los ojos de muchos de los personajes, más propio de individuos occidentales que japoneses, y que tiene su origen en la influencia que ejerció el estilo de la franquicia Disney sobre Osamu Tezuka.

A pesar de ello, el manga es muy variado y no todas las historietas son asimilables a las más popularizadas en Occidente, abordando de hecho todo tipo de estilos y temáticas, y comprendiendo autores de dibujo realista como Ryōichi Ikegami, Katsuhiro Otomo o Takeshi Obata.

El manga comienza su vida entre los años 1790-1912 debido a la llegada de personas de Occidente a Japón, y este estilo de dibujo fue tomando pronto mayor popularidad entre los japoneses.
El manga nace de la combinación de dos tradiciones: la del arte gráfico japonés, producto de una larga evolución a partir del siglo XI, y la de la historieta occidental, afianzada en el siglo XIX. Sólo cristalizaría con los rasgos que hoy conocemos tras la Segunda Guerra Mundial y la labor pionera de Osamu Tezuka.

Las primeras características del manga pueden encontrarse en el "Chōjugiga" (dibujos satíricos de animales), atribuidos a Toba no Sōjō (siglos XI-XII), del que apenas se conservan actualmente unos escasos ejemplares en blanco y negro.

Durante el período Edo, el ukiyo-e se desarrolló con vigor y produjo las primeras narraciones remotamente comparables a los géneros actuales del manga, que van de la historia y el erotismo a la comedia y la crítica. Hokusai, una de sus figuras, implantaría el uso del vocablo "manga" en uno de sus libros, "Hokusai Manga", recopilado a lo largo del siglo XIX. Otros dibujantes, como Gyonai Kawanabe, destacaron también en este período artístico.

Durante el siglo XIX, en plena transición de la era feudal a la industrializada, los artistas occidentales se maravillaban del ukiyo-e por la exótica belleza que transmitía. Con todo, los verdaderos inicios del manga moderno no se debieron al esteticismo del arte del período Edo, sino a la expansión de la influencia cultural europea en Japón.

Fueron Charles Wirgman y George Bigot (ambos críticos de la sociedad japonesa de su tiempo) quienes sentaron las bases para el desarrollo ulterior del manga. La revista británica "Punch" (1841) fue el modelo para la revista "The Japan Punch" (1862-87) de Wirgman, como lo había sido antes para otras revistas similares en otros países. Además en 1877 se publicó el primer libro infantil extranjero: "Max y Moritz" del alemán Wilhelm Busch.

La expansión de las técnicas historietísticas europeas se tradujo en una producción lenta pero segura por parte de artistas autóctonos japoneses, como Kiyochika Kayashi, Takeo Nagamatsu, Ippei Okomoto, Ichiro Suzuki y sobre todo Rakuten Kitazawa, cuya historieta "Tagosaku to Mokube no Tokyo Kenbutsu" 『田吾作と杢兵衛の東京見物』 se considera el primer manga en su sentido moderno. Todos ellos oficiaron de pioneros, difundiendo su obra a través de publicaciones, como "Tokyo Puck" (1905), aunque, igual que en Europa, el uso de globos de diálogo, que ya era habitual en la prensa estadounidense desde "The Yellow Kid" (1894), todavía no se había generalizado. Desde 1915 se empezó a ensayar simultáneamente la adaptación del manga a la animación, lo que más tarde devendría en el surgimiento anime.

Los años 1920 y 1930 fueron muy halagüeños, con la aparición y triunfo del "kodomo manga" (historietas infantiles), como "Las aventuras de Shochan" (1923) de Shousei Oda/Tofujin y "Los Tres Mosqueteros con botas en la cabeza" (1930) de Taisei Makino/Suimei Imoto. Curiosamente, la primera historieta de estilo manga aparecida en España fue un cómic infantil de esta época, publicado en el número 35 bis de "Bobín" en 1931, un mes antes de que se proclamase la Segunda República Española.

El cómic estadounidense -en especial "Bringing up father" (1913) de George McManus - se imita mucho en los años 20, lo que ayuda a implantar el globo de diálogo en series como "Speed Taro" (1930-33) de Sako Shishido, "Ogon Bat" (1930, un primer superhéroe) de Ichiro Suzaki/Takeo Nagamatsu y "Las aventuras de Dankichi" (1934), de Keizo Shimada, así como la tira cómica "Fuku-Chan" (1936-), de Ryuichi Yokohama. Para entonces, habían surgido historietas bélicas como Norakuro (1931-41) de Suihou Tagawa, ya que el manga sufría la influencia de las políticas militaristas que preludiaban la Segunda Guerra Mundial, durante la cual fue usado con fines propagandísticos. En 1945, las autoridades de ocupación estadounidenses prohibieron de manera generalizada este género.

Tras su rendición incondicional, Japón entraría en una nueva era. El entretenimiento emergió como industria, respondiendo a la necesidad psicológica de evasión ante una cruda posguerra. La falta de recursos de la población en general requería de medios baratos de entretenimiento, y la industria tokiota de mangas basados en revistas vio surgir competidores. Apareció así el Kamishibai, una especie de leyendas de ciego, que recorría los pueblos ofreciendo su espectáculo a cambio de la compra de caramelos. El "Kamishibai" no competía con las revistas, pero sí otros dos nuevos sistemas de distribución centrados en Osaka:

Osamu Tezuka, un estudiante de medicina veinteañero apasionado de los dibujos animados de Fleischer y Disney, cambiaría la faz de la historieta nipona con su primer libro rojo: "La nueva isla del tesoro", que vendió de súbito entre 400 000 y 800 000 ejemplares, gracias a la aplicación a la historia de un estilo cinematográfico que descomponía los movimientos en varias viñetas y combinaba este dinamismo con abundantes efectos sonoros.

El gran éxito de Tezuka lo llevó a las revistas de Tokio, particularmente a la nueva "Manga Shōnen" (1947), que fue la primera revista infantil dedicada en exclusiva al manga, y en la que Tezuka publicó "Astroboy". En estas revistas impuso su esquema de epopeya en forma de serie de relatos y diversificó su producción en múltiples géneros, de los que destacan sus adaptaciones literarias y el manga para chicas o "shōjo manga". A mediados de la década de 1950 Tezuka se trasladó a un edificio de la capital llamado Tokiwasi, al que peregrinarían los nuevos autores. Hay espacio, sin embargo, para autores como Machiko Hasegawa, creadora de la tira cómica Sazae-san (1946-1974), Kon Shimizu o Shigeru Sugiura con un grafismo muy diferente, nada disneyano.

Un año después, Shōnen desapareció y los libros rojos agonizaron. Entre ambos, y por obra de Osamu Tezuka, habían puesto los pilares de la industria del manga y anime contemporáneos.

El triunfo de las revistas de manga acabó con el "Kamishibai", y muchos de sus autores se refugiaron en el sistema de bibliotecas. Las revistas de manga eran todas infantiles y las bibliotecas encontraron su nicho creando un manga orientado hacia un público más adulto: el "gekiga". Abandonaron el estilo de Disney por otro más realista y fotográfico y se abrieron a nuevos géneros más violentos, escatológicos o sensuales como el horror, las historias de samuráis, los mangas sobre yakuzas, el erotismo, etc. Entre ellos cabe destacar a Sanpei Shirato que en 1964 patrocinaría la única revista "underground" de la historia del manga, Garo. La competencia en el terreno gráfico del gekiga obligó a las revistas a reducir la presencia del texto, aumentando el número de páginas y el tamaño para mejorar su visión.

Con el comienzo del auge económico, el pueblo nipón exigía más manga. En respuesta, una de las principales editoras de libros, Kōdansha, se introdujo en 1959 en el mercado de revistas. Su título Shōnen Magazine cambió la pauta de periodicidad mensual a semanal, multiplicando la producción e imponiendo a los autores el estajanovismo, aunque esta vez con sueldos millonarios. Pronto, otros grupos editores como Shueisha, Shōgakukan o Futabasha se le unirían. Este sistema de producción sacrificaba el color, la calidad del papel y la sofisticación temática, llevándose también de paso la crítica política, pero aumentaría vertiginosamente las ventas hasta cifras astronómicas y con ellas los beneficios empresariales, convirtiendo al manga en el medio de comunicación más importante del país.

Otros importantes autores de estos años son Fujio Akatsuka, Tetsuya Chiba, Fujiko F. Fujio, Riyoko Ikeda, Kazuo Koike, Leiji Matsumoto, Shigeru Mizuki, Gō Nagai, Keiji Nakazawa, Monkey Punch y Takao Saito.

En 1988, gracias al éxito de la versión cinematográfica de "Akira", basada en el "manga" homónimo del dibujante Katsuhiro Otomo, publicado en 1982 en la revista "Young Magazine" de la editorial Kōdansha, la difusión internacional del manga comenzó a aumentar de forma explosiva. El gran éxito de esta película en Occidente venía precedido de una tradición en aumento de emitir anime japonés en las cadenas de televisión europeas y estadounidenses. Ya en los años 60, Osamu Tezuka había vendido los derechos de emisión de su primer "Astro Boy" a la cadena estadounidense NBC consiguiendo un éxito notable entre la audiencia infantil. Posteriormente, se sucedieron las series de animación "Mazinger Z", "Great Mazinger" o "Grendizer", siendo esta última un estallido mediático en Francia, donde se la conocería como "Goldorak". Todas ellas se basaban en las historietas del mangaka Gō Nagai, actual magnate de un imperio de distribución editorial. En la década de 1980 empezaron a destacarse series de otra índole, como "The Super Dimension Fortress Macross", parte del compendio de series conocido en Occidente por "Robotech", obra de Carl Macek, o la revisión de Osamu Tezuka de "Astroboy" pero en esta ocasión vuelta a filmar en color y con aires más modernos. A ésta se sumó la saga épica "Gundam.

Otro de los autores más relevantes en este apogeo mediático de finales de los ochenta y principios de los noventa, fue el "mangaka" Akira Toriyama, creador de las famosas series "Dragon Ball" y "Dr. Slump", ambas caracterizadas por un humor picante, irreverente y absurdo (aunque la primera de ellas se caracterizó más por un contenido de acción más que de humor). Tal fue el éxito de estas dos obras que en algunos países europeos llegaron a desbancar de las listas de ventas al cómic estadounidense y nacional durante bastantes años. Este fenómeno fue más marcado en España, donde "Dragon Ball" vendió tantos ejemplares que se la considera la historieta de origen extranjero más vendida de la historia. En el propio Japón, la revista "Shōnen Jump" —en momentos puntuales, especialmente durante algunas semanas que coincidía con episodios decisivos de la serie "Dragon Ball"— llegó a aumentar su tirada semanal en 6 millones de ejemplares. En España, el éxito de las producciones de anime a principios de los años noventa derivó en una «fiebre editorial entre 1993 y 1995» que llenó las librerías especializadas de todo tipo de manga.

Otros importantes autores de estos años son Tsukasa Hōjō, Ryōichi Ikegami, Masakazu Katsura, Masamune Shirow, Mitsuru Adachi, Hirohiko Araki, Yuzo Takada, Rumiko Takahashi, CLAMP, Jirō Taniguchi, Yoshihiro Togashi, Takehiko Inoue, Nobuhiro Watsuki, Tite Kubo, Eiichirō Oda, Masashi Kishimoto, Masami Kurumada,Kosuke Fujishima, Naoko Takeuchi, Wataru Yoshizumi, Kentaro Miura, Yuu Watase, y Chiho Saito.

Cuando se comenzaron a traducir algunos títulos de manga, se les añadía color y se invertía el formato en un proceso conocido como «flopping» para que pudieran ser leídos a la manera occidental, es decir de izquierda a derecha, también conocido como «espejado». Sin embargo, varios creadores (como Akira Toriyama), no aprobaron que sus trabajos fueran modificados de esa forma, ya que se perdería la esencia de la imagen y el encuadre original, y exigieron que mantuvieran el formato original. Pronto, como consecuencia de la demanda de los fanáticos y la exigencia de los creadores, la mayoría de las editoriales comenzó a ofrecer el formato original de derecha a izquierda, que ha llegado a convertirse en un estándar para los lectores de manga fuera de Japón. También es frecuente que las traducciones incluyan notas de detalles acerca de la cultura del Japón que no resultan familiares a las audiencias extranjeras y que facilitan el entendimiento de las publicaciones.

La cantidad de mangas que han sido traducidos a un múltiples idiomas y vendidos en diferentes países sigue en aumento. Han surgido grandes casas editoriales fuera de Japón como la estadounidense VIZ Media centradas únicamente en la comercialización de manga. La francesa Glénat vive una segunda juventud gracias a publicación de cómic japonés. Los mercados que importan más manga son Francia (siendo este país el segundo del mundo en edición de cómics de origen japonés por detrás solamente del mismo Japón), los Estados Unidos, España y el Reino Unido.

Francia sobresale por poseer un mercado sumamente variado a lo que manga se refiere. Muchos trabajos publicados en Francia caen en géneros que usualmente no tienen mucho mercado en otros países fuera de Japón, como el drama orientado a adultos o los trabajos experimentales y alternativos. Artistas como Jirō Taniguchi que resultaba desconocido para la mayor parte de los países occidentales ha recibido mucho predicamento en Francia. La diversidad de manga en Francia se debe en gran parte a que este país tiene un mercado de historietas conocido como franco-belga muy bien establecido. En el sentido contrario, autores franceses, como Jean Giraud se han quejado de «"el manga llega a Europa, pero el cómic europeo no va a Japón"».

La compañía TOKYOPOP, se ha dado a conocer en los Estados Unidos acreditando el auge en ventas de manga, particularmente para un público de chicas adolescentes. Muchos críticos coinciden en que sus publicaciones agresivas hacen énfasis en la cantidad sobre la calidad siendo responsables de algunas traducciones de dudosa calidad.

Aunque el mercado de historietas en Alemania resulta pequeño en comparación con otros países de Europa, el manga ha favorecido cierto auge de las mismas. Luego de un imprevisto comienzo temprano en los años 1990, el movimiento manga tomo velocidad con la publicación de Dragon Ball en 1997. Hoy, el manga mantiene un 75 a 80% de las ventas de historietas publicadas en Alemania, con las mujeres sobrepasando como lectoras a los varones.

La compañía "Chuang Yi" publica manga en inglés y chino en Singapur; algunos de los títulos de "Chuang Yi" son importados a Australia y Nueva Zelanda.

En Corea, se puede encontrar manga en la mayoría de las librerías. Sin embargo, es común la práctica de leer manga «en línea» ya que resulta más económico que una versión impresa. Casas editoriales como Daiwon y Seoul Munhwasa publican la mayor parte del manga en Corea.

En Tailandia antes de 1992-1995 la mayoría del manga disponible salía de forma rápida, sin licencia, de baja calidad. Recientemente, las traducciones licenciadas han comenzado a aparecer, pero continúan siendo baratas comparadas con otros países. Entre las editoriales de manga en Tailandia se encuentran Vibunkij, Siam Inter Comics, Nation Edutainment y Bongkouh.

En Indonesia, se ha producido un crecimiento rápido en las industrias de este tipo, hasta convertirse en uno de los mercados más grandes de manga fuera de Japón. El manga en Indonesia es publicado por Elex Media Komputindo, Acolyte, Gramedia.

La influencia del manga es muy destacable en la industria de historietas original de casi todos los países del Extremo Oriente e Indonesia. Al día de hoy el "manga" también se ha consolidado en la sociedad occidental debido al éxito cosechado durante las décadas pasadas, dejando de ser algo exclusivo de un país para constituirse en un fenómeno comercial y cultural global, en competencia directa con la hegemonía narrativa estadounidense y europea.

El ejemplo más claro de la influencia internacional del manga se encuentra en el denominado "amerimanga", es decir, el conjunto de artistas fuera del Japón que han creado historietas bajo la influencia del manga y el anime japonés pero para un público estadounidense. Y es que el manga se ha vuelto tan popular que muchas compañías fuera del Japón han lanzado sus propios títulos basados en el manga como Antarctic Press, Oni Press, Seven Seas Entertainment, TOKYOPOP e incluso Archie Comics que mantienen el mismo tipo de historia y estilo que los mangas originales. El primero de estos títulos salió al mercado en 1985 cuando Benn Dunn, fundador de Antartic Press, lanzó Magazine y Ninja High School. Artistas como los estadounidenses Brian Wood (Demo) y Becky Cloonan así como el canadiense Bryan Lee O'Malley (Lost At Sea) están en gran parte influenciados por el estilo de manga comercial y han sido alabados por sus trabajos fuera del círculo de fanáticos de manga y anime. Mientras que Antarctic Press se refería a sus trabajos como «amerimanga», no todos estos trabajos inspirados en el manga son creados por estadounidenses. Muchos de estos artistas que trabajan en Seven Seas Entertainment en series como Last Hope o Amazing Agent Luna son de origen filipino y TOKYOPOP tiene una gran variedad de artistas coreanos y japoneses en algunos de sus títulos como Warcraft y Princesa Ai. Otros artistas estadounidenses con influencia del manga en algunas de sus obras son Frank Miller, Scott McCloud y sobre todo Paul Pop. Este último trabajó en Japón para Kōdansha en la antología de manga Afternoon y luego de ser despedido (debido a un cambio editorial en Kodansha) continuó con las ideas que había desarrollado para la antología, publicando en los Estados Unidos bajo el nombre de Heavy Liquid. Su trabajo contiene, por tanto, una gran influencia del manga sin las influencias internacionales de la cultura otaku. En el otro sentido, la editorial estadounidense Marvel Comics llegó a contratar al "mangaka" japonés Kia Asamiya para una de sus series bandera, "Uncanny X-Men".

En Francia existe el movimiento llamado «La nouvelle manga» iniciado por Frédéric Boilet, que trata de combinar la sofisticación madura del manga con el estilo artístico de las historietas franco-belgas. Mientras que el movimiento envuelve a artistas japoneses, un puñado de artistas franceses han adoptado la idea de Boilet.

En Europa, de hecho, se está desarrollando actualmente a marchas forzadas los «mangakas» españoles. Tanto es así, que las editoriales extranjeras están buscando a mangakas españoles para la publicación de mangas en sus respectivos países. Ejemplos como Sebastián Riera, Desireé Martínez, Studio Kôsen, y muchos otros están consiguiendo poco a poco posicionar este nuevo manga, llamado Iberomanga, o Euromanga, cuando engloba a los autores que se están dando a conocer en Europa. Además, existen muchos artistas aficionados que son influidos exclusivamente por el estilo del manga. Muchos de estos artistas se han vuelto muy populares haciendo pequeñas publicaciones de historietas y mangas utilizando mayormente Internet para dar a conocer sus trabajos.

Sin embargo, lo más importante de todo es que gracias a la irrupción del manga en Occidente, la población juvenil de estas regiones ha vuelto a interesarse masivamente por la Historieta como medio, algo que no sucedía desde la implantación de otras formas de ocio como la TV.

El "manga" en Japón es un auténtico fenómeno de masas. Un único dato sirve para ilustrar la magnitud de este fenómeno: En 1989, el 38% de todos los libros y revistas publicados en Japón eran de "manga".

Como se puede suponer por esta cifra, el "manga" no es solo cosa de jóvenes. En Japón hay "manga" para todas las edades, profesiones y estratos sociales, incluyendo amo/as de casa, oficinistas, adolescentes, obreros, etc. El "manga" erótico y pornográfico (hentai) supone una cuarta parte de las ventas totales. Desde 2006 existe en la ciudad de Kioto el Museo Internacional del Manga de Kioto, que constituye una novedad al ser el primero de su género. En la actualidad cuenta con 300.000 artículos y objetos relacionados con la materia, de los que se distinguen especialmente los 50.000 volúmenes con los que cuenta la colección del museo.

Y en cuanto a las revistas de manga, conocidas también como «revistas manga» o «revistas antológicas», hay que decir que sus tiradas son espectaculares: Al menos diez de ellas pasan del millón de ejemplares semanales. Shōnen Jump es la revista más vendida, con 6 millones de ejemplares cada semana. Shōnen Magazine le sigue con 4 millones. Otras conocidas revistas de "manga" son Shōnen Sunday, Big Comic Original, Shonen Gangan, Ribon, Nakayoshi, Margaret, Young Animal, Shojo Beat y Lala.

Las revistas de manga son publicaciones semanales o mensuales de entre 200 y 900 páginas en las que concurren muchas series distintas que constan a su vez de entre veinte y cuarenta páginas por número. Estas revistas suelen estar impresas en papel de baja calidad en blanco y negro con excepción de la portada y usualmente algunas páginas del comienzo. También contienen varias historietas de cuatro viñetas.

Si las series mangas resultan ser exitosas se publican durante varios años. Sus capítulos pueden ser recogidos en tomos de unas 200 páginas conocidos como "tankōbon", que recopilan 10 u 11 capítulos que aparecieron antes en revista. El papel y las tintas son de mejor calidad, y quien haya sido atraído por una historia concreta de la revista la comprará cuando salga a la venta en forma de "tankōbon". Recientemente han sido impresas versiones «de lujo» para aquellos lectores que buscan un impreso de mayor calidad y que buscan algo especial.

De forma orientativa, las revistas cuestan en torno a 200 o 300 yenes (algo menos de 2 o 3 euros) y los "tankōbon" cuestan unos 400 yenes (3,50 euros).

Otra variante que ha surgido por la proliferación del intercambio de archivos a través del Internet es el formato digital que permite la lectura en un computador o similar; denominándose e-comic. Los formatos más comúnmente usados para ello son el .cbr y .cbz, que realmente son archivos comprimidos (en rar y en zip, respectivamente) con imágenes en formatos comunes (jpeg y gif sobre todo) en su interior. También se suelen distribuir como imágenes sueltas o también en formato pdf o lit.

El manga se clasifica en función del segmento de población al que se dirigen. Para ello usan términos nipones como los siguientes:


La clasificación de los mangas por género se vuelve extremadamente ardua, dada la riqueza de la producción nipona, en la que una misma serie puede abarcar varios géneros y mutar a lo largo del tiempo. De ahí que la clasificación por segmento de población sea mucho más frecuente. El aficionado occidental al manga usa, sin embargo, algunos términos nipones que permiten designar a algunos de los subgéneros -que no géneros- más específicos, y que no tienen un equivalente preciso en castellano. Son los siguientes:



Feria típica japonesa donde las personas aficionadas al manga pueden disfrutar de su hobbie.

Los aficionados se visten de Cosplay, estas ferias únicamente eran típicas de Japón, pero se fueron extendiendo hasta alcanzar todos los continentes.

Otra forma de clasificar al manga es mediante la temática, estilo o gag que se utiliza como centro de la historia. Así, tenemos:





</doc>
<doc id="7138" url="https://es.wikipedia.org/wiki?curid=7138" title="Luis García Berlanga">
Luis García Berlanga

Luis García-Berlanga Martí (Valencia, 12 de junio de 1921-Somosaguas, Pozuelo de Alarcón, Madrid, 13 de noviembre de 2010) fue un director de cine y guionista español.

Luis García-Berlanga Martí nació en Valencia el 12 de junio de 1921, en una familia de terratenientes de Camporrobles, provincia de Valencia. Su abuelo, Fidel García Berlanga (1859-1914), era miembro activo del Partido Liberal de Sagasta, a finales del siglo XIX, llegando a ser diputado en Cortes en Madrid y presidente de la diputación de Valencia. Su padre, José García-Berlanga (1886-1952), comenzó también su militancia en el Partido Liberal, para luego pasar al partido de centro derecha de Lerroux, el Partido Radical, y más tarde afiliarse al partido de centro izquierda burgués de Martínez Barrio, Unión Republicana.

Los orígenes de su madre, Amparo Martí, fueron mucho más humildes, ya que venía de una familia de emigrantes de Teruel que se establecieron en Valencia. Su tío materno, Luis Martí Alegre, llegó a ser presidente de la Caja de Ahorros de Valencia.

El propio Luis García Berlanga cuenta a su biógrafo Antonio Gómez Rufo en relación a su padre: «"Y así fue que cuando llegó 1936 mi padre estaba en Unión Republicana, en el Frente Popular. Pero resultaba que era muy perseguido por determinadas facciones de la ultraizquierda, concretamente por aquellos con los que más simpatizaba yo, los anarquistas, a causa no recuerdo qué follones en Utiel y en Requena, por lo que no le quedó más remedio que huir de Valencia para salvarse de la persecución. Y se fue a Tánger, donde vivió un año, hasta que lo detuvieron los nacionales"».

Durante su juventud se unió a la División Azul para evitar represiones políticas por el cargo de gobernador civil que su padre había desempeñado en Valencia durante la República española. En 1990, el propio Luis reconoce que se alistó pues muchos de sus amigos eran miembros jóvenes destacados de FE de las JONS. Sobre su ideología azul en aquellos años son muchos los testimonos de divisionarios que compartieron con él las trincheras en Rusia, como, por ejemplo, José Luis Amador de los Ríos. 

En marzo de 1943 ganaba el premio «Luis Fuster» dado por el SEU —sindicato universitario falangista— de Valencia por su artículo aparecido en la "Hoja de Campaña de la División Azul" titulado "Fragmentos de una primavera". Escribía:

De joven, decidió estudiar Derecho y luego Filosofía y Letras, pero más tarde, en 1947, cambió su vocación e ingresó en el Instituto de Investigaciones y Experiencias Cinematográficas de Madrid, donde realizó sus primeros cortometrajes.

Fue un gran aficionado al erotismo, recabando una enorme colección de material sobre el tema (principalmente literatura) y llegó a codirigir de 1979 a 2004 en Tusquets Editores una colección literaria de esta temática que otorgaba un premio, el Premio La Sonrisa Vertical, en el que era presidente del jurado.

Debutó como director en 1951 con la película "Esa pareja feliz", en la que colaboraba con Juan Antonio Bardem. Junto a éste, se lo considera uno de los renovadores del cine español de posguerra. Entre sus películas destacan títulos célebres de la historia del cine español, como "El verdugo" o "Bienvenido, Mister Marshall". Trabajó en siete ocasiones con el guionista Rafael Azcona, y de esta asociación surgieron algunas de las películas más célebres del cine español, además de las citadas, como "La escopeta nacional".
Su cine se caracteriza por su mordaz ironía y sus ácidas sátiras sobre diferentes situaciones sociales y políticas. En la etapa de la dictadura franquista despuntó su habilidad para burlar la censura de la época con situaciones y diálogos no excesivamente explícitos pero de inteligente contralectura y consiguió llevar a cabo proyectos tan atrevidos como "Los jueves, milagro".

Su película "Plácido" fue nominada para el Óscar a la mejor película de habla no inglesa en 1961. En 1980 obtuvo el Premio Nacional de Cinematografía, en 1981 la Medalla de Oro de las Bellas Artes, en 1986 el Premio Príncipe de Asturias de las Artes y en 1993 el Goya al mejor director por su película "Todos a la cárcel". El 25 de abril de 1988 fue elegido miembro de la Real Academia de Bellas Artes de San Fernando, e ingresó al año siguiente con un discurso titulado "El cine, sueño inexplicable".

Obtuvo premios y galardones internacionales en los más importantes festivales, como Cannes, Venecia, Montreal y Berlín. En el Festival de Karlovy Vary fue elegido como uno de los diez cineastas más relevantes del mundo. Además, poseía un incontable número de reconocimientos nacionales.

Se casó en 1954 con María Jesús Manrique de Aragón (n. 1931) y fueron padres de cuatro hijos: José Luis García-Berlanga, productor de televisión, hostelero y cocinero; Jorge Berlanga (1958-2011), periodista, escritor y guionista (participó en el guion de varias películas de su padre) y director de la Mostra de Valencia entre 2001 y 2002; Carlos Berlanga (1959-2002), músico, compositor e importante precursor de la corriente cultural conocida como la movida madrileña, además de la música pop de los años 80; y Fernando García-Berlanga, locutor y presidente de la desaparecida cadena española Somosradio.

Sus dos hijos más conocidos fallecieron en Madrid relativamente jóvenes por enfermedades hepáticas: Carlos el 5 de junio de 2002, a los 42 años, y Jorge el 9 de junio de 2011, a los 52 años.

Luis García Berlanga falleció a los 89 años por causas naturales en su casa de la urbanización de Somosaguas (Pozuelo de Alarcón, Madrid) el 13 de noviembre de 2010.

En 2008, teniendo ya un delicado estado de salud, depositó en la Caja 1.034 de las Letras del Instituto Cervantes de la calle Alcalá un sobre donde contenía un secreto, el cual pidió que no se revelase hasta el 12 de junio de 2021, cuando se cumpliera el centenario de su nacimiento.

Tras el fallecimiento de Luis García Berlanga, Álex de la Iglesia, presidente de la Academia de las Artes y las Ciencias Cinematográficas de España, escribió en "El País" un obituario donde reconoce que la película "Plácido", del citado director, le cambió la vida:
Santiago Segura también explicó que su obra influyó en la suya diciendo que "el cine de Berlanga ha influido en mi filmografía de la mejor manera posible, impactando fuertemente en mi cerebro y dejando su poso tras visionarlo", al igual que Óscar Aibar quien afirmó que su película "El gran Vázquez" tiene influencias de Berlanga. Berlanga fue uno de los 25 primeros cineastas españoles elegidos por la Junta Directiva de la Academia de las Artes y las Ciencias Cinematográficas de España que tuvieron su estrella en el paseo de la fama de Madrid, situado en la calle de Martín de los Heros, y realizada por Óscar Mariné.
























</doc>
<doc id="7139" url="https://es.wikipedia.org/wiki?curid=7139" title="José Saramago">
José Saramago

José de Sousa Saramago (Azinhaga, 16 de noviembre de 1922-Tías, 18 de junio de 2010) fue un escritor, novelista, poeta, periodista y dramaturgo portugués. En 1998 se le otorgó el . La Academia Sueca destacó su capacidad para «volver comprensible una realidad huidiza, con parábolas sostenidas por la imaginación, la compasión y la ironía».

José Saramago nació en la "freguesia" portuguesa de la Azinhaga (municipio de Golegã, en el distrito central del Ribatejo), cerca del río Tajo, a 120 km al noreste de Lisboa.

Sus padres fueron José de Sousa y Maria da Piedade, una pareja campesina sin tierras y de escasos recursos económicos. Este estilo de vida influirá notablemente en los pensamientos del escritor, especialmente en lo que se refiere a sus ideas políticas, cimentadas sobre una vasta cultura formal y popular, y una experiencia vital hiperestésica. El apodo de la familia paterna era Saramago («Jaramago» en español, nombre de una planta herbácea silvestre de la familia de las crucíferas).

El niño debería haberse llamado José Sousa, pero el funcionario del registro civil cometió un error de pluma y lo anotó como José «Saramago», aunque hay quienes dicen que fue una broma del funcionario, conocido de su padre. El registro oficial menciona el día 18 de noviembre, aunque fue el 16. En 1925, la familia de Saramago se mudó a Lisboa, tras un breve paso por Azinhaga, donde su padre comenzó a trabajar de policía. Pocos meses después de la mudanza, falleció su hermano Francisco, dos años mayor.

En 1934, a la edad de doce años, entró en una escuela industrial. En aquellos años incluso los estudios técnicos contenían asignaturas humanísticas. En los libros de texto gratuitos de aquellos años Saramago se encontró con los clásicos. Incluso en sus últimos años aún podía recitar de memoria algunos de esos textos. Aunque Saramago era buen alumno, no pudo finalizar sus estudios porque sus padres ya no pudieron pagarle la escuela, por lo que para mantener a su familia Saramago trabajó durante dos años en una herrería mecánica.

Pronto cambió de trabajo y comenzó a trabajar de empleado administrativo en la Seguridad Social. Tras casarse en 1944 con Ilda Reis, Saramago comenzó a escribir su primera novela, "Tierra de pecado", que se publicó en 1947, pero que no tuvo éxito. Ese año nació su primera hija, Violante.

Saramago escribió una segunda novela, "Claraboya", que no fue publicada hasta el 2012 (dos años después de haber fallecido) por su viuda esposa. Los siguientes veinte años no se dedicó a la literatura. «Sencillamente no tenía algo que decir y cuando no se tiene algo que decir lo mejor es callar».

Entró a trabajar en una compañía de seguros. Simultáneamente colaboró como periodista en "Diário de Notícias", un periódico de alcance nacional, pero por razones políticas pronto fue expulsado. Luego, colaboró como crítico literario de la revista "Seara Nova" y fue comentarista cultural. Formó parte de la primera dirección de la Asociación Portuguesa de Escritores, y también desempeñó la subdirección del "Diário de Notícias". Desde 1966 se dedicó con exclusividad a su trabajo literario. Sufrió censura y persecución durante los años de la dictadura de Salazar. Consiguió trabajo en una editorial en la cual trabajó durante doce años. En su tiempo libre tradujo varias obras de autores como Maupassant, Tolstoi, Baudelaire y Colette.

En 1966 publicó "Os poemas possíveis". En 1969 se afilió al por aquel entonces clandestino Partido Comunista Portugués. Ese mismo año se divorció de Ilda y abandonó su trabajo en la editorial para dedicarse plenamente a vivir de la escritura, bien como articulista, bien como novelista. En 1970 publicó "Provavelmente alegria." Entre 1972 y 1973 fue redactor del "Diário de Lisboa". En 1974 se sumó a la Revolución de los Claveles, que llevó la democracia a Portugal. En 1975 publicó "O Año de 1993".

Su primera gran novela fue "Levantado do chão" (1980), un retrato fresco y vívido de las condiciones de vida de los trabajadores de Lavre, en la provincia de Alentejo. Con este libro Saramago consigue encontrar su voz propia, ese estilo inconfundible, límpido y casi poético que lo distingue. En los siguientes años, Saramago publicó casi sin descanso:

"Memorial do convento" (1982), donde cuenta las más duras condiciones de vida del pueblo llano en el oscuro mundo medieval, en épocas de guerra, hambre y supersticiones. Este libro fue adaptado como ópera por Azio Corghi, y estrenado en el Teatro de la Scala de Milán, con el título de "Blimunda", el inolvidable personaje femenino de la novela.

También Corghi adaptó su obra teatral "In nómine Dei", que con el nombre de "Divara" fue estrenada en Münster. De Azio Corghi es también la música de la cantata "La muerte de Lázaro", sobre textos de "Memorial del convento", "El Evangelio según Jesucristo" e "In nómine Dei". Fue interpretada por vez primera en la iglesia de San Marco, de Milán.

En 1984 Saramago publicó "El año de la muerte de Ricardo Reis" y en 1986, "A jangada de pedra" ("La balsa de piedra"), donde cuenta qué sucedería si la península ibérica se desprendiera del continente europeo. Ese año, cuando tenía sesenta y tres, conoció a quien sería su esposa hasta el final de sus días, la periodista española Pilar del Río, natural de la localidad granadina de Castril, nacida en 1950, quien finalmente se convirtió en su traductora oficial en castellano.

La novela "El Evangelio según Jesucristo" (1991) lo catapultó a la fama a causa de una polémica sin precedentes en Portugal —que se considera una república laica—, cuando el gobierno vetó su presentación al Premio Literario Europeo de ese año, alegando que «ofendía a los católicos».

Como acto de protesta, Saramago abandonó Portugal y se instaló en la isla canaria de Lanzarote. En 1995 publicó una de sus novelas más conocidas, "Ensayo sobre la ceguera," novela que fue llevada al cine en el 2008 bajo la dirección de Fernando Meirelles. En 1997 publicó su novela "Todos los nombres", que gozó también de gran reconocimiento.

En 1998 ganó el Premio Nobel de Literatura, convirtiéndose en el primer escritor —y hasta ahora el único— de lengua portuguesa en ganar este premio. Desde entonces, compartió su residencia entre Lisboa y la isla canaria, participando en la vida social y cultural de ambos países, cuyas estrechas relaciones justificó en una entrevista para proponer su idea utópica de creación de una Iberia unida. Ateo declarado, colaboró ocasionalmente en prensa, aportando su punto de vista, siempre agudo y comprometido. En definición suya, «Dios es el silencio del universo, y el ser humano, el grito que da sentido a ese silencio». Una de sus últimas obras fue "Las intermitencias de la muerte", donde cuenta de un país cuyo nombre no será mencionado y se produce algo nunca visto desde el principio del mundo: la muerte decide suspender su trabajo letal, la gente deja de morir. De ahí en adelante, se relatarán situaciones inimaginables o no, ya que nadie muere pero siguen envejeciendo. La frase que cierra su última novela, "Caín", es «"La historia ha acabado, no habrá más que contar"».

Tanto su casa como la biblioteca privada se encuentran abiertas al público todo el año en el pueblo de Tías. Por los pasillos es fácil toparse con Pilar del Río, su mujer, quien aún frecuenta la vivienda y preside la Fundación José Saramago. Allí, entre otras cosas, está la colección de relojes que el escritor portugués detuvo a las cuatro, como símbolo de amor hacia ella, pues se conocieron a esa hora.

Falleció a los ochenta y siete años de edad, el día 18 de junio de 2010, en su residencia de la localidad de Tías (Lanzarote), a causa de una leucemia crónica que derivó en un fallo multiorgánico. Había hablado con su esposa y pasado una noche tranquila. Saramago escribió hasta el final de su vida, incluso se dice que llevaba treinta páginas de una próxima novela.

Las cenizas del novelista portugués fueron depositadas el 18 de junio de 2011 al pie de un olivo centenario, traído de su pueblo natal y trasplantado en el Campo das Cebolas, frente a la Fundación José Saramago Casa dos Bicos de Lisboa, al cumplirse el primer aniversario de su muerte.


















</doc>
<doc id="7140" url="https://es.wikipedia.org/wiki?curid=7140" title="Gō Nagai">
Gō Nagai

, mejor conocido bajo su nombre artístico de , es un mangaka y prolífico autor de ciencia ficción, fantasía, terror y erotismo japonés, reconocido como un innovador del género de manga. Hizo su debut profesional en 1967 con "Meakashi Polikichi", pero sería más conocido por crear "Cutey Honey", "Devilman" y "Mazinger Z". También fue pionero en el género ecchi con "Harenchi Gakuen". Se le atribuye la creación del género "Super Robot" y el diseño de los primeros mecha piloteados por un usuario desde una cabina con "Mazinger" Z. En 2005, se convirtió en profesor de diseño de personajes en la Universidad de Arte de Osaka. Desde 2009, también es miembro del comité de nominación del Premio Cultural Tezuka Osamu. En España, también se han publicado algunas de sus obras.

La decisión de convertirse en mangaka le vino a la cabeza cuando su hermano le prestó el manga de "Lost World" de Osamu Tezuka y "La Princesa Caballero")- A los 25 años, tras una grave enfermedad, creó "Kuro no Shishi" (黒の獅士, "El león negro").

En 1962, Go Nagai estableció Dynamic Productions, una compañía para desarrollar sus proyectos de manga y anime. Los primeros títulos de esta nueva empresa fueron "Getter Robo" y "Abashiri Ikka" ('La familia Abashiri').

En 1968, mientras que Shueisha estaba preparando el lanzamiento de su primera publicación manga, Shonen Jump, para competir con Shonen Magazine de Kodansha, entre otras... Nagai fue invitado a ser uno de los primeros artistas de manga que publicarán en dicha revista. Aquí obtuvo su primera serie de larga duración que fue todo un éxito.

Esta serie se llamó "Harenchi Gakuen" (ハレンチ学園, "La escuela indecente", 1968-1972, revista "Shōnen Jump") y aquí Nagai fue el primero en introducir el erotismo en el manga moderno. Su obra entraría en la polémica ya que Go Nagai se convertiría en el primer mangaka que introduciría el erotismo en historias infantiles, arremetiendo contra tabúes sobre el hablar de sexo abiertamente. La violencia y humor grosero también fueron reprobados por muchos rincones de la sociedad japonesa. La serie concluyó dramáticamente: todos los personajes mueren durante una masacre.

Tras "Harenchi Gakuen", Nagai creó la serie que lo llevaría a la fama mundial. "Mazinger Z" (マジンガーZ, Majinga Z), que después continuaría con las secuelas "Great Mazinger" y "UFO Robo Grendizer", en las que desarrolló el concepto de robot (mecha) gigante pilotado desde una cabina. Más tarde introduciría el concepto de robot tranformable con Groizer X y Getter Robo. Estas ideas resultaron sorprendentemente productivas en muchos programas de televisión posteriores. El reconocimiento mundial llegaría tras la trasmisión del anime de la historieta el 3 de diciembre de 1972 por el canal de televisión Fuji TV.

Al tiempo que "Mazinger", creó una de sus series más populares, "Debiruman" (デビルマン, "Devilman" u "Hombre diablo"), sobre un anti-héroe que combate contra impresionantes hordas de demonios. "Debiruman" aún es una serie de culto en Japón. Devilman está inspirado en "mao dante" (demon lord dante) el primer manga de Go Nagai sobre demonios. Más tarde tuvo su anime de 13 episodios.

A pesar de las críticas, Go Nagai siguió desarrollando mangas de corte erótico como "Kekko Kamen" en 1974, historia que cuenta las aventuras de una justiciera que combate el crimen con una máscara roja, bufanda, capa, guantes y botas, pero nada más.

La mente creadora de Go Nagai no tenía cuando parar y crea "Cutie Honey" y cambia por completo el género del manga con "Magical Girl" (anime que narra las aventuras de chicas con poderes mágicos), añadiéndole una personalidad más fuerte al personaje femenino, ya que luchaba contra la injusticia sin la ayuda de un personaje masculino y, claro, añadiéndole el correspondiente toque de erotismo.

Nagai ha trabajado con Shōtarō Ishinomori y Ken Ishikawa. Actualmente está produciendo más manga que nunca. Muchas de sus series manga tienen versiones anime.

Nagai se ha superado mucho en el aspecto del contenido erótico en sus mangas, empezando a dibujar escenas de bondage en sus mangas primerizos (como en "Cutey Honey" y "Kekko Kamen"), llegando a dibujar escenas de sexo oral y escenas de sexo en general como en "Hanappe Bazooka" y "Kamasutra" (también ha hecho mucha percusión en el hecho de hermafroditas y chistes de origen sexual o flatulento). Cabe aclarar que el autor siempre se ha ""autocensurado"" en las partes genitales tanto de personajes femeninos como masculinos.

Luego de publicar obras como "Cutey Honey" y "Kekko Kamen" (en esta incluía temas como humillación sexual, violencia y mutilación, desnudos por doquier, personajes ambiguos, etc.) muchos autores de esa generación (mediados de los 60s y los 70s) siguieron con esta línea de incluir erotismo en sus mangas, cada vez más explícito, hasta llegar a la actualidad. Ejemplo puede ser el propio Masakazu Katsura, que en entrevistas confiesa que eligió el "Shonen Jump" para mostrar sus obras ya que lo consideraba un exponente de erotismo.

Go Nagai es uno de los autores de manga más influyentes del mundo y para los fanes de este género está a la altura de Stan Lee o Jack Kirby de Marvel Comics. Actualmente no tiene descanso y está serializando el manga "Shin Mazinger Zero vs Ankoku Daishougun" y preparando un nuevo manga llamado "Devilman Saga", que tiene como fecha de estreno el 25 de diciembre de este año por la revista Big Comic.

Para ver un listado actualizado de los artículos relacionados con este autor consulte la categoría de .

Go Nagai a parte de producir sus propios proyectos, ha participado en algunas películas en calidad de actor, cabe destacar su participación en la comedia de 1989 "The Toxic Avenger PartII" (el Vengador Tóxico II) donde hacia un pequeño papel interpretando a un experto en Tsukudani.
Es digno de mencionarse también su colaboración en el film "Metrópolis de Osamu Tezuka" donde presta su voz en un cameo.
También ha participado con cameos en un par de películas japonesas de consumo interno japonés, en Kekkô Kamen (2004) y en Cutey Honey: The Live (2004) esta última una adaptación de Cutey Honey.

Junto a Osamu Tezuka y Shotaro Ishinomori, Nagai es uno de los artistas más prolíficos en la historia del manga, y del comic en general.
Sus obras han sido adaptadas a numerosos otros formatos, incluyendo televisión, cine, radionovelas, literatura y videojuegos.


</doc>
<doc id="7142" url="https://es.wikipedia.org/wiki?curid=7142" title="2 de junio">
2 de junio

El 2 de junio es el 153.º (centésimo quincuagésimo tercer) día del año en el calendario gregoriano y el 154.º en los años bisiestos. Quedan 212 días para finalizar el año.


A partir de aquel día inició el movimiento a favor de los derechos de las trabajadoras sexuales.


























</doc>
<doc id="7143" url="https://es.wikipedia.org/wiki?curid=7143" title="Casarabonela">
Casarabonela

Casarabonela es un municipio español de la provincia de Málaga, Andalucía. Está situada en el centro de la provincia, en la comarca Sierra de las Nieves. Declarada Reserva de la Biosfera por la Unesco e integrada en el parque natural Sierra de las Nieves y su entorno. En 2016 contaba con una población total de 2.573 habitantes.

El área de Casarabonela ya estaba habitada desde tiempos prehistóricos. Consta la presencia humana en varios yacimientos de talleres líticos, enterramientos y grutas con útiles cotidianos. Los yacimientos históricos más importantes que se han encontrado pertenecen a la época romana, cuando la ciudad se denominaba "Castra Vinaria" (Castillo del Vino).

Con la llegada de los árabes se respetaron la mayoría de los monumentos romanos, incluso Omar ibn Hafsún, quien consiguió conquistar la ciudad y la usó como frente defensivo de Bobastro, reforzó su fortaleza y cambiaron el nombre del pueblo a "Qasr Bunaira". En el 922 el Califato de Córdoba consigue recuperar la ciudad y la vuelve a reforzar contra los rebeldes. Estos refuerzos en las fortificaciones hicieron que Casarabonela fuera de los últimos asentamientos en capitular durante la Reconquista, el 2 de junio de 1485.

En 1574, Felipe II le concede el estatus de villa. Tras la expulsión de los moriscos (1609-1614), la mayoría de las tierras quedaron deshabitadas y fueron repobladas por ciudadanos de otras áreas de Andalucía y Extremadura. 

En el año 1810 José Bonaparte pernocta en la ciudad en su viaje a Málaga.

Casarabonela está situado en el borde occidental de la comarca del Valle del Guadalhorce, adentrándose en la comarca natural de Ronda por Alcaparaín (1.200 m) y Prieta (1.521 m), hasta acercarse al río Turón en su límite con el municipio de El Burgo. 

En el centro de su término municipal, rodeado de olivares y campos de cereal, se levanta la sierra de La Robla (563 m), que se asoma entre pinares al resto de las formaciones de la Serranía y a los campos de Zalea, en el corazón del Guadalhorce. Terreno calizo. Horticultura en bancales que producen fruta y hortalizas.

La administración política del municipio se realiza a través de un Ayuntamiento cuyos componentes se eligen cada cuatro años por sufragio universal. El censo electoral está compuesto por todos los residentes empadronados en Casarabonela mayores de 18 años y nacionales de España y de los restantes estados miembros de la Unión Europea. Según lo dispuesto en la Ley del Régimen Electoral General, que establece el número de concejales elegibles en función de la población del municipio, la Corporación Municipal de Casarabonela está formada por 11 concejales.

Tras las elecciones municipales de 2007, el PSOE consiguió la mayoría absoluta con 6 concejales y 954 votos, el PP consiguió 3 concejales y 429 votos y AGIC (Agrupación Independiente de Casarabonela) consiguió 2 escaños y 307 votos. Al conseguir mayoría absoluta gobernó el partido socialista durante esta legislatura.

Tras las elecciones municipales de 2011, el PSOE consiguió 5 escaños y 759 votos, el PP 3 escaños y 422 votos y AGIC 3 escaños y 407 votos. Al no conseguir el partido socialista una mayoría absoluta, PP y AGIC se repartieron la legislatura gobernando durante 2 años cada uno.

Tras la celebración de las elecciones municipales de 2015, el PSOE consiguió la mayoría absoluta con 6 concejales y 815 votos, el PP 4 concejales y 480 votos y AGIC (Agrupación Independiente de Casarabonela) consiguió 1 concejal y 222 votos. El partido socialista con su mayoría absoluta recuperó la alcaldía del municipio con Antonio Campos Campos como alcalde.

Jardín botánico dedicado al cactus e inaugurado en 2011, con más de 2 500 especies adaptadas para vivir en zonas áridas. La colección de plantas suculentas fue formada por Joan Mora y Edwige Bravard, matrimonio que desarrolló su afición por estas plantas en la isla de Mallorca, hasta que en 1995 se trasladaron a Andalucía y establecieron su colección en la actual ubicación del jardín en Casarabonela.


Artículos de esparto, artículos de madera, artículos de palma, artículos de pita, cerámica artística, macramé, sillería de anea y talabartería.

El plato más conocido de la gastronomía morisca es el pipeo, olla cocinada con lechuga y pipas de haba, a la que se le añade un majado de ajos y pan frito y se acompaña con tortillitas de pan. Así mismo tienen mucha fama las distintas formas de cocinar el conejo y el chivo.
Además de los dulces: Tortas de aceite, las tortas de almendra, los roscos de vino, los bizcochos, las empanadillas de batata, los polvorones, los mantecados, los mostachones.

Semana Santa 

Viernes santo: Hermandad de las Servitas. 

Casarabonela no está integrado formalmente en el Consorcio de Transporte Metropolitano del Área de Málaga, aunque existen rutas de autobuses interurbanos que operan en su territorio. Pueden consultarse en el siguiente enlace.



</doc>
<doc id="7144" url="https://es.wikipedia.org/wiki?curid=7144" title="Miranda (satélite)">
Miranda (satélite)

Miranda, también designado como Urano I, es el menor de los cinco satélites principales del planeta Urano y el último en ser descubierto hasta el sobrevuelo de Urano por la sonda espacial "Voyager 2". Descubierto por Gerard Kuiper el 16 de febrero de 1948, Miranda recibe su nombre de un personaje —la hija del mago Próspero— de la obra de William Shakespeare "La Tempestad".

La inclinación de la órbita de Miranda (4,338°) es muy alta para un cuerpo tan próximo a su planeta. Es posible que en algún momento estuviese en resonancia orbital 3:1 con Umbriel. La fricción provocada por las fuerzas de marea podría haber causado un calentamiento en el interior del satélite y ser el origen de la actividad geológica y criovolcánica.

Miranda es un cuerpo cuasiesférico de 472 km de diámetro. Todo parece indicar que el nacimiento del satélite fue extremadamente violento, a tenor de su extraordinaria orografía. La superficie de Miranda está formada en su mayoría por agua helada, estando el interior posiblemente formado por rocas silíceas y compuestos ricos en metano. Geológicamente, Miranda ha sido el cuerpo más activo del sistema solar.

La superficie está atravesada por grandes cañones de hasta 20 km de profundidad con regiones de terreno resquebrajado que indican una muy intensa actividad geológica en el pasado. Se piensa que esta actividad geológica podría estar relacionada con efectos de marea producidos por Urano. Sin embargo, es más aceptada la teoría de que en el pasado Miranda sufrió un fuerte impacto que estuvo a punto de destruir el satélite. Otra teoría, que ahora ya no se considera tan válida, dice que en el pasado Miranda sufrió un fuerte impacto que la partió en trozos. Con el tiempo, los fragmentos se volvieron a juntar, dando el aspecto de cuerpo remendado que tiene actualmente.




</doc>
<doc id="7145" url="https://es.wikipedia.org/wiki?curid=7145" title="22 de octubre">
22 de octubre

El 22 de octubre es el 295.º (ducentésimo nonagésimo quinto) día del año en el calendario gregoriano y el 296.º en los años bisiestos. Quedan 70 días para finalizar el año.







. Acámbaro, Guanajuato:





</doc>
<doc id="7146" url="https://es.wikipedia.org/wiki?curid=7146" title="1934">
1934

1934 () fue un año normal comenzado en lunes según el calendario gregoriano.































</doc>
<doc id="7148" url="https://es.wikipedia.org/wiki?curid=7148" title="Baba Yagá">
Baba Yagá

Baba Yagá (en ruso: Бáба Ягá) es un personaje recurrente en el folclore y la mitología eslava.

Baba Yagá es vieja, huesuda y arrugada, con la nariz azul y los dientes de acero, posee una pierna normal y una de hueso por lo que a menudo se le da el apelativo de "Baba Yagá Pata de Hueso". Estas dos piernas representan al mundo de los vivos y el mundo de los muertos por los que deambula. Baba Yagá es un ser perverso y cruel, pero no totalmente malvado; come personas, generalmente niños. Sus dientes le permiten romper huesos y desgarrar la carne con facilidad. Pese a que consume diariamente grandes cantidades de carne, siempre tiene ese aspecto delgado y huesudo. Baba Yagá vuela montada en un almirez (a veces una olla) y rema el aire con una escoba plateada. Baba Yagá no permite que ninguna persona "bendecida" permanezca dentro de su propiedad, siempre y cuando ella sepa que la persona tiene una bendición.

Vive en una choza que se levanta sobre dos enormes patas de gallina que le sirven para desplazarse por toda Rusia. La valla de su choza está adornada con cráneos, en cuyo interior coloca velas. La idea de una casa con patas de gallina podría derivar de las cabañas de ciertos pueblos finoúgricos, que las construían de esta manera para protegerse de los animales. Para entrar en la casa, Baba Yagá dice el conjuro "Casita Casita, da la espalda al bosque y voltea hacia mí". El interior de la choza siempre está repleto de carne y vino. También es custodiado por los sirvientes invisibles de Baba Yagá, que aparecen como manos espectrales. Baba Yagá también tiene a su servicio a los caballeros blanco, rojo y negro, que controlan el día, el atardecer y la noche.

Baba Yagá ha aparecido en diferentes historias del folclore ruso, y algunas de ellas muestran distintas facetas suyas. En algunas, ayuda a la gente que le sirve. En otras se dice que guarda las "Aguas de la Vida y de la Muerte", pues es "la Dama Blanca de la Muerte y del Renacimiento". En otras dice que tiene dos hermanas, llamadas como ella y con su mismo aspecto. 

En Bulgaria, a los niños se les cuenta que si se portan mal, vendrá Baba Yagá (o Dyado Yag, Дядо Яг) para llevárselos con un saco y comérselos. También se le asocia con magia negra.

También se cuenta que envejece un año cada vez que le hacen una pregunta y que para rejuvenecer bebe un té hecho de las extrañas rosas azules, por lo que recompensa enormemente a las personas que le traen alguna de estas rosas.

La figura de Baba Yagá probablemente deriva de "la Bruja", tercer componente de la Diosa Tripartita (Virgen, Madre y Bruja), símbolo de las tres edades de la mujer.

Baba Yagá es ampliamente usada por los autores de cuentos de hadas del ruso moderno, y desde los años noventa del siglo XX, en la "Fantasía rusa". En particular, Baba Yagá aparece en el ciclo de libros de Andréi Belanin "La Agencia de detectives del Zar Goroj (Царь Горох)". La infancia y juventud de Baba Yagá fueron descritas por primera vez en el cuento "La bahía" ("Lukomorie") de A. Aliverdíev.

Debido a su popularidad, Baba Yagá ha aparecido en historias no eslavas. Se pueden mencionar sus apariciones en el mundo del cómic, especialmente del cómic adulto, en historietas como Hellboy, de Mike Mignola, perteneciente al sello Dark Horse, donde es antagonista en una de las historias, y una aparición fuerte en Fábulas, de Bill Willingham, perteneciente a Editorial Vertigo, donde es aliada de El Adversario. Además, toma la forma de Caperucita Roja para introducirse como espía en Villa Fábula en la saga La Marcha de los Soldados de Madera, enfrentándose finalmente a Frau Totenkinder. También hace una breve aparición en el tercer volumen del cómic "Los Libros de la Magia", de Neil Gaiman, atrapando al protagonista hasta que es rescatado por De. Occult cuando éste amenaza a la bruja con revelar su nombre en la Tierra de las Hadas.
También aparece en el cuento infantil Las Aventuras de Vania el Forzudo, de Otfried Preussler, donde Baba Yagá constituye el segundo gran enemigo de Vania, la cual derrota con su lanza, y como recompensa ella le da el caballo Varón, el más rápido de toda Rusia.

En la película Inocencia rebelde, la protagonista utiliza constantes paralelismos sobre Baba Yaga.

En 1990 hace una aparición como parte de una línea de juguetes lanzados por Matchbox en su línea Monster in My Pocket o "Monstruos de bolsillo", que eran figuras coleccionables de plástico de no más de 5 cm de altura y de variados colores, los cuales se colocaban dentro de un volcán negro igualmente de plástico. En esa ocasión se la representaba como una horrible bruja robusta de aspecto grotesco, nada huesuda como en la mitología rusa; viajaba a toda velocidad dentro de un caldero negro impulsado por una vieja escoba voladora y usaba un collar con dos cráneos humanos como colguijes. Entre la información detallada sobre esta variante de "Baba Yagá" se conoce que medía 1.80 metros, tenía la espalda y el resto del cuerpo cubierto por llagas o ámpulas y también se sabe que nació en el siglo XII en algún bosque remoto de Rusia.

Además, la figura de Baba Yagá aparece en el film de dibujos animados "Bartok el Magnífico", siendo una bruja con una personalidad más humanizada a como se la suele representar y, si bien durante la película sí que se la presenta como un ser al que temer, cruel, carente de sentimientos, cuya casa se alza sobre unas largas patas y para acceder a la cual se requiere resolver un acertijo, al final la bruja revela sus buenos sentimientos y adopta una perspectiva más benigna.

Baba Yagá es uno de los números de la obra de Modest Músorgski Cuadros de una exposición, así como el op. 56 del también compositor ruso Anatoli Liádov.

En España apareció en 2010 un cómic llamado "Baba" del autor catalán Lluís Moreno, protagonizado por la bruja, aunque en esta ocasión en forma de niña y con corte de humor. Este cómic fue publicado por Norma Editorial.

En el Mundo de Tinieblas (mundo donde se desarrollan los juegos de rol de la editorial White Wolf), Baba Yagá es el nombre de una vampira perteneciente al clan Nosferatu.
Una de sus últimas apariciones fue en la serie "Lost Girl" de Showcase.ca en donde Baba Yagá es convocada inocentemente por una joven, llamada Kenzi, en un acto de solidaridad para vengar el corazón roto de su más querida amiga, Bo.

También hace su aparición en la cuarta novela del autor David Safier, Happy Family; en español, Una familia feliz.

En el anime y manga "Soul Eater" existe un castillo que lleva su nombre.

Además, aparece en el cortometraje de Studio Ghibli "Pandane to tamago hime" (パン種とタマゴ姫?; literalmente, "Mr. Dough and the Egg Princess"), el cual se puede ver únicamente de manera oficial en el Museo Ghibli.

La bruja también aparece como protagonista indiscutible en el libro-juego: "En las garras de Bába Yagá", de David Ruiz del Portal y Juapi, publicado por Alfasur Editorial en 2013.

La bruja también aparece en la serie de libros y serie animada "Ever After High" como consejera académica y profesora junto con otros personajes famosos de los cuentos de hadas.

En la película de 2014 "John Wick", el personaje del mismo nombre, interpretado por Keanu Reeves, es referido por el líder mafioso, Viggo Tarasov, como "la persona que envías para matar a la maldita Baba Yagá".

Aparece también en la película "Don’t Knock Twice", de 2016, siendo el principal peligro sobrenatural para las protagonistas de dicho filme.

Aparece en "El extraño caso de la mujer sin memoria", libro del autor español Juan Pascal, en el cual sirve de guía al personaje principal.

La bruja de la serie animada "Dragon Ball" con el mismo nombre -Baba- puede ser una referencia a ella, puesto que ésta también vaga entre ambos mundos (el de los vivos y el de los muertos).

En la serie animada "Las Leyendas", producida por Netflix, la bruja aparece como antagonista de la historia y está representada de la misma manera que en el folclore ruso.

Además, en un momento de la serie de Netflix "Trollhunters", se oye la voz de un personaje con muchos nombres, entre los que se encuentra el de Baba Yagá.

En la película de Marvel "Ant-Man and the Wasp", de 2018, se menciona al personaje de Baba Yagá, relacionándolo con la villana Ghost.

Baba Yagá aparece en la película "Hellboy" de 2019.





</doc>
<doc id="7149" url="https://es.wikipedia.org/wiki?curid=7149" title="Aguas de la Vida y de la Muerte">
Aguas de la Vida y de la Muerte

Las Aguas de la Vida y de la Muerte son, según el folclore y la mitología rusa, aguas con poderes mágicos de resurrección que pocos buscaban porque eran custodiadas por Baba Yagá.

Se podían recibir las Aguas de la Vida y de la Muerte de manos de su temida guardiana si se le demostraba fuerza singular o mediante alguna artimaña; aunque también podía ser otorgada como regalo. Su mítico y legendario poder conseguiría que a un hombre descuartizado, si se le aspergía con el Agua de la Muerte, los trozos volvían a ensamblarse y, tras esto, si se aspergía el cadáver con el Agua de la Vida, renacía.


</doc>
<doc id="7152" url="https://es.wikipedia.org/wiki?curid=7152" title="Babbage (cráter)">
Babbage (cráter)

Babbage es un antiguo cráter de impacto que se encuentra cerca del limbo noroeste de la Luna. Se adjunta al borde sureste del cráter prominente Pitágoras. El remanente de un impacto que recibe el nombre de cráter South invade el suelo sureste de Babbage.

El borde exterior de Babbage ha sido erosionado y modificado por una multitud de impactos posteriores, hasta que todo lo que queda es un anillo de colinas redondeadas. La más notable de estas modificaciones es el cráter satélite "Babbage E", que se superpone al borde suroeste. El borde noreste de este cráter satelital ha desaparecido y forma una bahía en el perímetro de Babbage. Oenopides es otra formación desgastada unida al borde suroeste de esta protuberancia.

Las rampas exteriores de Pitágoras se superponen sobre el suelo de Babbage, formando una región de terreno accidentado en la parte noroeste de su interior. El resto del suelo del cráter es relativamente plano, aunque está marcado por muchos pequeños impactos. La característica más notable en el suelo interior es el cráter satélite "Babbage A", que se encuentra en la parte sureste de la plataforma interna. Este impacto no se ha desgastado significativamente, y parece mucho más reciente que el resto de la formación. Justo al oeste de "Babbage A" se halla el pequeño "Babbage C", una formación en forma de cuenco.

El cráter debe su nombre al matemático británico Charles Babbage.

Por convención estos elementos son identificados en los mapas lunares poniendo la letra en el lado del punto medio del cráter que está más cercano a Babbage.



</doc>
<doc id="7153" url="https://es.wikipedia.org/wiki?curid=7153" title="Fulgurita">
Fulgurita

La fulgurita (del latín "fulgur", 'relámpago') es una roca metamórfica en forma de tubo compuesta por lechatelierita (sílice vitrificada) que se puede encontrar en arenas o areniscas. Su formación se debe a la caída de rayos atmosféricos.

La caída del rayo sobre un terreno arenoso provoca la fusión de los granos de sílice, ya que la temperatura de este puede alcanzar los 4000 grados Celsius, hasta una profundidad de más de un metro, pero en una zona muy estrecha. La sílice queda así vitrificada en forma de "tubos de rayo", de 2 a 50 mm de diámetro, a veces retorcidos o ramificados. Esas fulguritas pueden encontrarse en los desiertos de arena y en las dunas litorales.

Pueden ser de diferentes colores dependiendo de la composición de la arena, en donde se formaron, incluyendo negro, bronce, verde y blanco translúcido. El interior de la fulgurita es comúnmente liso o delineado de unas finas burbujas; el exterior está generalmente cubierto de ásperas partículas de arena. Tienen apariencia de raíz y a menudo muestran pequeños agujeros. Las fulguritas algunas veces forman conjuntos vítreos en rocas sólidas. 

La formación de fulguritas es un fenómeno poco frecuente, y extraer una íntegra es complicado por causa de la fragilidad del mineral, que se quiebra fácilmente. Posiblemente el ejemplar mejor conservado se encuentra en la Academy of Natural Sciences de Filadelfia, descubierto en 1940.




</doc>
<doc id="7154" url="https://es.wikipedia.org/wiki?curid=7154" title="Tasa Tobin">
Tasa Tobin

La tasa Tobin o ITF (Impuesto a las transacciones financieras) es un tipo de tasa sobre las transacciones financieras que fue propuesta por el economista estadounidense James Tobin en sus "Janeway Lectures" en la Universidad de Princeton en el año 1971. Este tipo de impuesto recuperó la atención pública cuando, en los años 1990, propuso su aplicación el movimiento antiglobalización, en especial la organización ATTAC, y de nuevo en los años 2000 con motivo de la crisis económica de 2008. James Tobin ha considerado que se ha abusado de su nombre y de su idea, dado que en su origen la tasa sólo tenía por objetivo frenar la volatilidad de los mercados cambiarios internacionales. En su nueva formulación se propone que su recaudación se destine a fines sociales o que tenga por objetivo el control de crisis financieras como la crisis de la deuda soberana europea. Desde 2011 se relanzó la reivindicación de esta tasa, tanto desde autoridades políticas y monetarias como diferentes discusiones en el seno de la Unión Europea, así como desde ciertas ONG como Oxfam, quien la rebautizó como Impuesto Robin Hood.

En 2001, después de las crisis económicas de los noventa en México, Rusia y el Sureste Asiático, James Tobin describió la tasa que ideó a comienzos de los años setenta:

El objetivo de James Tobin al desarrollar su idea de una tasa sobre las transacciones de divisas era encontrar una vía para gestionar la volatilidad de tipo de cambio. En su visión, "los intercambios de tipo de cambio trasmiten alteraciones a los mercados financieros internacionales. Las economías nacionales y los gobiernos nacionales no son capaces de ajustar los movimientos masivos de fondos en los tipos de cambio extranjeros sin gran trabajo y un alto sacrificio de los objetivos de política económica nacional en relación al empleo, producto e inflación". 

Tobin encontró dos soluciones a este asunto. La primera era desplazarse "hacia una divisa común, una política monetaria y fiscal común y la integración económica". La segunda era desplazarse "hacia una mayor segmentación financiera entre naciones y áreas monetarias, permitiendo a los bancos centrales y a los gobiernos una mayor autonomía en sus políticas destinadas a sus específicas instituciones económicas y objetivos". La solución preferida de Tobin era la anterior, pero no la encontraba políticamente viable de modo que abogó por la segunda opción: "por lo tanto recomiendo, lamentándome por ello, la segunda, y mi propuesta es introducir algún tipo de palo en las ruedas de nuestros excesivamente eficientes mercados internacionales de dinero".

El método de Tobin para introducir un "palo en las ruedas" fue sugerir una tasa sobre todas las conversiones "spot" de una moneda en otra divisa, proporcional al tamaño de la transacción. En este sentido, afirmó:

Sería una tasa acordada de modo uniforme internacionalmente, administrada por cada gobierno en su propia jurisdicción. Gran Bretaña, por ejemplo, sería responsable por gravar todas las transacciones intra divisa en la divisa Europea de los bancos y "brokers" basados en Londres, incluso aunque la libra esterlina no participara. La recaudación del impuesto serían apropiadamente satisfecha al Fondo Monetario Internacional (FMI) o al Banco Mundial. El impuesto se aplicaría a todas las compras de instrumentos financieros denominados en otra divisa, desde divisa y moneda hasta títulos sobre activos. Debería aplicarse, creo, a todos los pagos en una divisa de bienes, servicios y activos reales vendidos por un residente de otra área monetaria. No pretendo añadir ni siquiera una mínima barrera al comercio. Pero no veo otro modo de evitar que las transacciones financieras se disfracen de comercio.

En el desarrollo de su idea, Tobin estuvo influido por el trabajo previo de John Maynard Keynes sobre la teoría general de las tasas sobre transacciones financieras:

Soy un discípulo de Keynes, y él, en el famoso capítulo XII de la "Teoría general del empleo, el interés y el dinero", ya había prescrito la idea de una tasa sobre las transacciones, con el objeto de alinear a los inversores con sus acciones de un modo duradero. En 1971, yo transferí esta idea a los mercados cambiarios.

El concepto de Keynes deriva de su observación en 1936, cuando propuso que una tasa sobre las transacciones financieras fuera impuesta a las operaciones de Wall Street, donde argumentó que la excesiva especulación de "traders" financieros sin información aumentaba la volatilidad de los mercados.

De acuerdo a Paul Bernd Spahn en 1995, el "análisis ha mostrado que la tasa Tobin tal y como fue propuesta originalmente no es viable y debería dejarse de lado". Más allá, añadió:
Spahn sugirió una alternativa, que incluiría
El 19 de septiembre de 2001, el especulador jubilado George Soros avanzó la propuesta de unos Derechos Especiales de Giro (DEG) que los países ricos comprometerían al objeto de proveer con asistencia internacional, sin necesariamente desestimar la idea de la tasa Tobin. Afirmó: "creo que se da el caso para una tasa Tobin... [pero] no está nada claro que una tasa Tobin fuera a reducir la volatilidad en los mercados de cambio. Es cierto que desincentivaría la especulación de divisas pero también reduciría la liquidez del mercado".

El término "tasa Tobin" se ha utilizado con frecuencia de modo intercambiable tanto para denominar una tasa sobre las transacciones sobre divisas (TTD) en el sentido de la idea original de Tobin, como para hacer referencia a diferentes formas de una más general tasa sobre las transacciones financieras (TTF). En ambos casos, las diferentes ideas propuestas han incluido tanto conceptos nacionales como multinacionales.

Un ejemplo de esta asociación con una específica tasa sobre transacciones con divisas se muestra en este ejemplo de 2001:

"El concepto de una tasa Tobin ha experimentado un resurgir en las discusiones destinadas a reformar el sistema financiero internacional. Además de diferentes iniciativas legislativas nacionales a favor de una tasa Tobin en los parlamentos nacionales, otras vías para introducir una tasa Tobin sobre las transacciones de divisas (TTD) están siendo examinadas por Naciones Unidas".

Un ejemplo de su asociación con una tasa sobre las transacciones financieras en general se muestra en este otro ejemplo de 2009:

"Los líderes de la Unión Europea han urgido al Fondo Monetario Internacional (FMI) el viernes a considerar una tasa global sobre las transacciones financieras en contra de la oposición a esta idea de EEUU y del propio FMI. En un comunicado emitido tras un encuentro de dos días, los 27 líderes de la EU no llegaron a lanzar un llamamiento formal para la introducción de la llamada "tasa Tobin" pero expresaron claramente que la veían como un instrumento potencialmente útil para aumentar los ingresos".

El concepto específico de Tobin de una "tasa sobre las transacciones en divisas" de 1972 se mantuvo dormido durante más de 20 años, pero volvió a la vida con la llegada de la crisis financiera asiática de 1997. En diciembre de 1997, Ignacio Ramonet, editor de "Le Monde Diplomatique", recuperó el debate en torno a la tasa Tobin con un editorial titulado "Disarming the markets". Ramonet proponía la creación de una asociación para la introducción de esta tasa, que fue llamada ATTAC (Asociación para la Tasación de las Transacciones financieras para la Ayuda a los Ciudadanos). La tasa se convirtió entonces en un asunto clave del movimiento por la justifica global o antiglobalización y un objeto de discusión no solo entre las instituciones académicas sino incluso en las calles y parlamentos de Reino Unido, Francia y alrededor del mundo.

En una entrevista, concedida a la red de radios independientes italiana Radio Popolare en julio de 2001, James Tobin se distanció del movimiento antiglobalización. «"Existen agencias y grupos en Europa que han utilizado la tasa Tobin como un elemento para campañas más amplias, con razones que van más allá de mi propuesta. Mi propuesta fue realizada ha sido convertida en una suerte de pieza clave del programa antiglobalización"». La entrevista de James Tobin con Radio Popolare fue citada por el Ministro de Exteriores italiano de la época y por el antiguo director de la OMC Renato Ruggiero durante un debate parlamentario tras la reunión del G8 en Génova. Posteriormente, James Tobin se distanció del propio movimiento antiglobalización, y siguió afirmando la validez de su propuesta:

Tobin observó que, mientras que su propuesta original solo tenía por objetivo "poner un freno en el tráfico de divisas extranjeras", el movimiento antiglobalización ha puesto el hincapié en "el ingreso de las tasas con el que quieren financiar sus proyectos para mejorar el mundo". El economista no se declaró contrario al uso de la recaudación del impuesto, pero llamó la atención sobre que ése no era el aspecto más importante de la tasa.

ATTAC y otras organizaciones han reconocido que aunque todavía consideran la idea original de Tobin como primoridal, piensan que el impuesto podría generar fondos para las necesidades de desarrollo del Sur (como los Objetivos de Desarrollo del Milenio), y permitir a los gobiernos, y por lo tanto a los ciudadanos, reclamar parte del espacio democrático concedido a los mercados financieros.

En marzo de 2002, el profesor Willem Buiter de la London School of Economics, quien estudió bajo la tutela de James Tobin, escribió un encendido obituario del hombre, que también remarcó que "esta [tasa Tobin]... ha sido en años recientes adoptada por algunos de los más determinados enemigos de la liberalización del comercio, de la globalización y de la sociedad abierta". Buiter añadió que "la propuesta para usar la tasa Tobin como un medio de aumentar los ingresos para la asistencia al desarrollo fue rechazada por Tobin, y forzadosamente repudió el mantra antiglobalización de la masa de Seattle". En septiembre de 2009, Buiter también escribió en Financial Times que "Tobin fue un genio... pero probablemente la tasa Tobin fue su idea más rídicula".

Se asumió originalmente que la tasa Tobin requeriría una implementación internacional, dado que para un país actuando solo sería muy difícil implementar la tasa. Desde entonces, muchos han argumentado que sería mejor implementarla por una institución internacional. Se ha propuesto que fuera Naciones Unidas quien gestionara la tasa Tobin, resolviendo esta cuestión y dando al mismo tiempo a la ONU una importante fuente de fondos independiente de las donaciones de los estados miembros. No obstante, también han existido iniciativas sugiriendo la posibilidad de dotar a la tasa de una dimensión nacional.

Aunque encontró algo de apoyo en países con movimientos políticos de izquierda importantes como Francia y Latinoamérica, la propuesta de tasa Tobin sufrió importantes críticas tanto de economistas como de gobiernos, especialmente aquellos con mercados liberales y un importante sector bancario internacional, quienes afirmaron que sería imposible implementar la tasa porque desestabilizaría los mercados de divisas extranjeras.

La mayor parte de las implementaciones prácticas de la tasa Tobin, tanto en la forma de una tasa sobre las transacciones en divisas específica o una más general tasa sobre las transacciones financieras ha tenido lugar a nivel nacional. En julio de 2006, el analista Marion G. Wrobel examinó las experiencias internacional de varios países con tasas sobre las transacciones financieras.

El trabajo de Wrobel llamó la atención sobre la experiencia de Suecia con diferentes tasas sobre las transacciones financieras. En enero de 1984, Suecia introdujo una tasa del 0.5% sobre la compra o venta de un activo de capital. De modo que una transacción de ida y vuelta (compra y venta) resultaría en un gravamen del 1%. En julio de 1986 el tipo fue doblado. En enero de 1989, una tasa considerablemente más baja del 0.002% sobre los activos de renta fija fue introducida para todos los valores con madurez superior a los 90 días o inferior. Para los bonos con un vencimiento de cinco años o superior, la tasa era del 0.003%.

La recaudación de estos impuestos no fueron buenos. Por ejemplo, se preveía inicialmente que los ingresos de la tasa sobre los valores de renta fija fueran de 1.500 millones de coronas suecas al año. En realidad, no superaron los 80 millones de coronas suecas ningún año y la media estuvo cerca de los 50 millones. Además, al caer los volúmenes negociados imponibles, la recaudación de los impuestos sobre las ganancias del capital también cayeron, eliminando por completo el efecto recaudador positivo de la tasa sobre las transacciones de capital.

El día que la tasa fue anunciada, los precios de las acciones cayeron un 2.2%. Pero existió filtración de información antes del anuncio, lo que explicaría la caída del 5.35% en el precio durante los 30 días anteriores al anuncio. Cuando la tasa fue doblada, los precios volvieron a caer otro 1%. Estas caídas estuvieron en línea con el valore capitalizado de los pagos impositivos futuros que se esperaban de las operaciones. Así, se percibió que las tasas sobre los valores de renta fija solo sirvieron para incrementar el coste del endeudamiento público, dando otro argumento contra la tasa.

Incluso aunque la tasa sobre los valores de renta fija fue mucho más baja que sobre el capital, el impacto en la negociación de mercado fue más dramático. Durante la primera semana del impuesto, el volumen de negociación de bonos cayó un 85%, incluso aunque el tipo de la tasa para el bono a cinco años fuera de tan solo el 0.003%. El volumen de la negociación de futuros cayó un 98% y el mercado de opciones desapareció. El 15 de abril de 1990, se abolió la tasa sobre los activos de renta fija. En enero de 1991, los tipos de las demás tasas fueron recortados a la mitad y al final de ese año fueron abolidos completamente. Una vez que las tasas fueron eliminadas, los volúmenes de negociación volvieron y crecieron sustancialmente durante los años noventa.

Un tipo de tasa sobre las transacciones financieras (FTT) es la "Stamp Duty Reserve Tax" (SDRT) y el impuesto de sello. Los impuestos de sello fueron introducidos como un impuesto ad valorem sobre la compra de acciones en 1808, precediendo en cerca de 150 años la tasa Tobin sobre las transacciones en divisas. En 1963 se introdujeron modificaciones. Ese año, el tipo de la "UK Stamp Duty" era del 2%, fluctuando entre 1% y 2% a partir de entonces, hasta que un proceso de reducción gradual comenzó en 1984, cuando el tipo fue reducido a la mitad, primero del 2% al 1%, y de nuevo en 1986 del 1% al nivel actual del 0.5%.

Los cambios en los tipos del impuesto de sello en 1974, 1984 y 1986 facilitaron a los investigadores con "experimentos naturales", permitiéndoles medir el impacto de la tasa sobre las transacciones en los volúmenes negociados en el mercado, en la volatilidad, en los rendimientos y en las valoraciones de los valores de la Bolsa de Londres. Jackson y O'Donnel (1985), utilizando datos para el Reino Unido trimestrales, encontraron que una reducción de un 1% en la "Stamp Duty" en abril de 1984 del 2% al 1% condujo a un "incremento dramático del 70% en el volumen de valores". Analizando los tres cambios en los tipos de la "Stamp Duty", Saporta y Kan (1997) encontraron que los anuncios de incremento (o reducción) del tipo de la tasa fueron seguidos por rendimientos negativos (o positivos), pero incluso aunque estos resultados fueron estadísticamente significativos, podrían estar influidos por otros factores, dado que los anuncios fueron realizados en "día de presupuesto". Bond et al. (2005) confirmaron los hallazgos de los anteriores estudios, señalando además que el impacto del anuncio del recorte del tipo de la tasa fue más beneficioso (aumentando el valor de mercado más significativamente) en caso de las empresas más grandes, que disponían de mayores volúmenes, y estaban por lo tanto más afectadas por la tasa sobre las transacciones que los valores de empresas más pequeñas y, por tanto, menos frecuentemente negociados.

En 1996, el Programa de Naciones Unidas para el Desarrollo patrocinó un estudio comprensivo sobre la viabilidad y el análisis coste beneficio de la tasa Tobin.

A finales de 2001, una tipo de tasa Tobin fue adoptada por la Asamblea Nacional de Francia. Sin embargo, fue depuesta en marzo de 2002 por el Senado francés.

El 15 de junio de 2004, la Comisión de Finanzas y Presupuestos del Parlamento Federal de Bélgica aprobó una ley implementando una tasa Spahn. De acuerdo a esta normativa, Bélgica introduciría una tasa Tobin una vez que todos los países de la eurozona introdujeran una ley similar. En julio de 2005, el antiguo canciller austríaco Wolfgang Schüssel hizo un llamamiento para aprobar una tasa Tobin de la Unión Europea que basa la estructura financiera de las comunidades de modo más estable e independiente. No obstante, la propuesta fue rechazada por la Comisión Europea.

El 23 de noviembre de 2009, el Presidente del Consejo Europeo, Herman Van Rompuy, tras atender una reunión del Grupo Bilderberg, argumentó a favor de una versión europea de la tasa Tobin. Este impuesto se aplicaría más allá de las puras transacciones financieras: "todas las compras y el petróleo serían tasadas". En contra se pronunció su hermana, Christine Van Rompuy, quien dijo, "cualquier nuevo impuesto afectará directamente a los pobres".

El 29 de junio de 2011, la Comisión Europea realizó un llamamiento para aplicar tasas similares a la tasa Tobin al sistema financiero europeo para genera ingresos directos desde 2014. Al mismo tiempo, sugirió la reducción de los impuestos existentes de los 27 estados miembros. El 28 de septiembre de 2011 la Comisión Europea aprobó un proyecto de directiva con el fin de establecer un impuesto sobre la mayoría de las transacciones financieras realizadas en la Unión Europea. La iniciativa partió de la Comisión con el apoyo de Alemania, Francia y España, entre otros, y el rechazo expreso de Londres.

A raíz de la crisis del euro se ha considerado la creación de una tasa a las transacciones financieras que podría alejar la volatilidad de los mercados, disminuir las inversiones meramente especulativas y generar beneficios de unos 55.000 millones de euros anuales permitiendo disminuir además las aportaciones de los países a la Unión Europea.

El 9 de octubre de 2012 11 países de la Unión Europea (Alemania, Francia, Portugal, Grecia, Eslovenia, Bélgica, Austria, España, Italia, Estonia y Eslovaquia) acordaron avanzar en la creación de una TTF mediante una "cooperación reforzada" para evitar el veto de países como Reino Unido. La propuesta de Bruselas gravaría con un 0,1% las compraventes de acciones y bonos y con un 0,01% las de derivados. Si se implantara en toda la Unión Europea generaría unos ingresos de 55.000 millones de Euros. La propuesta deberá discutirse en la reunión de ministros de Economía del 12 de noviembre.

El primer país del grupo del G20 en aceptar formalmente la tasa Tobin fue Canadá. El 23 de marzo de 1999, la Cámara de los Comunes de Canadá aprobó una resolución instando a su Gobierno a "poner en marcha una tasa sobre las transacciones financieras en concierto con la comunidad internacional." No obstante, diez años más tarde, en noviembre de 2009, durante el encuentro de ministros de finanzas del G20 en Escocia, el representante del gobierno de minoría de Canadá se expresó abiertamente en contra de la resolución de su Cámara de los Comunes.

En septiembre de 2009, el presidente francés Nicolas Sarkozy relanzó el asunto de la tasa Tobin de nuevo, sugiriendo su adopción por el G20. El 7 de noviembre de 2009, el primer ministro inglés Gordon Brown afirmó que el G20 debería considerar una tasa sobre la especulación, aunque no especificó si esta debería ser solo sobre la negociación de divisas. La BBC informó que existía una respuesta negativa por parte del G20. El 11 de diciembre de 2009, los líderes de la Unión Europea expresaron un apoyo amplio para una tasa Tobin en un comunicado enviado al Fondo Monetario Internacional.

John Dillon sostiene que no es necesario disponer del acuerdo unánime sobre la viabilidad de una TTF internacional antes de avanzar. Propone su introducción gradual, comenzando seguramente en Europa donde el apoyo a la misma es mayor. El primer paso podría incluir un impuesto sobre los instrumentos financieros en unos pocos países. Stephan Schulmeister del "Austrian Institute for Economic Re-search" ha sugerido que inicialmente Gran Bretaña y Alemania podrían implementar el impuesto en un amplio conjunto de instrumentos financieros dado que el 97% de todas las transacciones sobre divisas en la Unión Europea tiene lugar en estos dos países.





</doc>
<doc id="7162" url="https://es.wikipedia.org/wiki?curid=7162" title="Milton Friedman">
Milton Friedman

Milton Friedman (Nueva York, 31 de julio de 1912-San Francisco, 16 de noviembre de 2006) fue un estadístico, economista e intelectual estadounidense de origen judío ganador del Premio Nobel de Economía de 1976. Profesor en la Universidad de Chicago, fue uno de los fundadores de la Escuela de Economía de Chicago, una escuela económica de economía clásica defensora del libre mercado. Junto a John Maynard Keynes y F. A. Hayek, Friedman es considerado uno de los economistas más influyentes del siglo 

Ideológicamente liberal, Friedman dedicó buena parte de su carrera a la crítica del keynesianismo dominante, a mediados del siglo , calificando la teoría keynesiana de «ingenua». No obstante, Friedman incorporó el lenguaje keynesiano en su obra, aunque siempre fue crítico con las conclusiones del keynesianismo. Su alternativa macroeconómica se centra en los factores monetarios y se conoce como monetarismo. Friedman propuso como política monetaria ideal una expansión suave y gradual de la oferta monetaria. También desarrolló el concepto de la tasa natural de desempleo, y predijo la crisis de la estanflación en Estados Unidos diez años antes de que ocurriera.

Friedman fue asesor para los gobiernos de Ronald Reagan en Estados Unidos y Margaret Thatcher en el Reino Unido. También, su pensamiento económico ha sido muy influyente en las políticas de algunos Estados postsoviéticos. Su teoría monetaria inspiró las medidas que tomó la Reserva Federal de Estados Unidos como respuesta a la crisis financiera de 2008.

Friedman nació en el barrio neoyorquino de Brooklyn fue el cuarto y último hijo de una familia judía humilde formada por Sarah Ethel y Jeno Saul Friedman, inmigrantes judíos procedentes del centro de Europa. Su padre murió teniendo Friedman la edad de quince años. A pesar de ello, y debido al gran esfuerzo económico realizado por su familia y él mismo costeándose sus estudios, Friedman cursó Economía en la Universidad Rutgers.Tras ello regresó a Chicago para colaborar como investigador con Henry Schultz en mediciones de la demanda.

En 1935 empezó a trabajar para la Asociación Económica del Comité de Recursos Naturales y en 1937 dejó el puesto para incorporarse a la Oficina Nacional de Investigación Económica, en la que estudió las estructuras de ingresos de las profesiones liberales.

Fue alumno de Simon Kuznets y Arthur Burns. Trabajó como economista para varias agencias federales en la ciudad de Washington de 1935 a 1940 y de 1941 a 1943. En 1946 presentó su tesis. Profesor de la Universidad de Chicago desde 1946 a 1976. También dio clases en las universidades de Wisconsin, Princeton, Columbia y Stanford. En 1977 se jubiló como docente.

Investigador del National Bureau of Economic Research, de 1937 a 1981. Miembro activo del Partido Republicano, fue asesor económico especial, entre otros, de Richard Nixon, Ronald Reagan, Margaret Tatcher y George W. Bush. Presidente de la American Economic Association en 1967. En 1947 funda junto a Hayek y otros la Sociedad Mont Pèlerin, de la que fue elegido presidente en 1972.

Desde 1977 hasta su muerte, fue Senior Research Fellow en el Instituto Hoover de la Universidad Stanford.

También perteneció a la American Philosophical Society y la National Academy of Sciences.

En 1938 se casó con la también economista Rose Director Friedman, a la que conoció cuando ambos estudiaban en la Universidad de Chicago en una clase impartida por Jacob Viner. Juntos crearon la Milton and Rose D. Friedman Foundation for Educational Choice. También firmaron conjuntamente varios libros.

En 1941, con la entrada estadounidense en la Segunda Guerra Mundial, fue destinado al Departamento del Tesoro, donde se encargó de la política fiscal durante el periodo de guerra. En 1943 fue nombrado director de la Asociación Estadística de la Universidad de Columbia, en la que se ocupó de problemas relacionados con la producción militar. En 1953 obtuvo una beca Fullbright, que le permitió una estancia en la Universidad de Cambridge, donde existía entonces un amplio debate en torno a las ideas keynesianas.

Asesoró a multitud de gobiernos, muchos de los cuales aplicaron sus propuestas económicas liberales, apareciendo en los medios de comunicación. Escribió en la revista "Newsweek" (1966-1984).

Una de las contribuciones de Friedman a la economía es su estudio de la función de consumo. A diferencia de Keynes, que decía que el consumo de un periodo dependía exclusivamente del ingreso del mismo periodo, Friedman postuló que este dependía del ingreso permanente, es decir, del ingreso a largo plazo.

Este nuevo enfoque tenía un énfasis en las expectativas y proyecciones de los consumidores. Junto a Edmund Phelps, corrigió la curva de Philips. Introdujo el rol de las expectativas en este modelo.

Milton Friedman fue un monetarista. Propuso resolver los problemas de inflación limitando el crecimiento de la oferta monetaria a una tasa constante y moderada. 

Economista empírico, era especialista en estadística y econometría. Defensor del libre mercado, fue el más conocido líder de la Escuela de Chicago debido, en parte, a que sus escritos son muy fáciles de leer por el hombre de la calle. Se opuso al keynesianismo en el momento de máximo apogeo de este, en los años cincuenta y sesenta.En su explicación de la demanda de dinero (1956) Friedman plantea que, la demanda de dinero es función de la proporción entre la riqueza humana y no humana, el tipo de interés nominal, la inflación esperada, el nivel de precios real, la función de preferencia del dinero ante otros bienes y, naturalmente, de la renta. Pero a diferencia de Keynes, Friedman, más centrado en dar una explicación a largo plazo, considera la renta permanente; es decir, el valor actualizado a fecha actual de los capitales futuros originados de un stock de riqueza dado que engloba no solo aspectos cuantitativos o materiales.

Otro aporte de Friedman es la revisión de curva de Phillips, de inspiración keynesiana, que relaciona inversamente niveles de paro e inflación. Considera Friedman que el paro sería voluntario de no ser por la existencia de una tasa de paro natural, la NAIRU ("non accelerating inflation rate of unemployment)," consecuencia de las limitaciones impuestas por gobiernos y otras instituciones públicas. Un ejemplo de ello es la prohibición de ciertos tipos de contratos. La visión de Friedman concibe al mercado como un sistema racional de asignación de recursos, capaz de corregir sus desequilibrios a largo plazo.

En "Capitalism and Freedom" (1962) y "Free to Choose" (1980) aseguraba que solo una institución como el mercado podía garantizar la libertad de los individuos y proponía dejar áreas prioritarias como la educación y la salud en manos de la libre competencia. La teoría de Friedman dice que el consumo de la gente no se ve afectado por los ingresos actuales. Si los consumidores reciben un ingreso imprevisto, es totalmente ahorrado para consumirlo luego. Esta idea planteaba que los estímulos fiscal del Estado no tenían efecto significativo. Esta teoría que fue un pilar central del modelo monetarista sería refutada a la luz de nuevos estudios ya que el comportamiento de los consumidores es más cortoplacista que lo que predecía Friedman ya que las personas que recibe algún beneficio imprevisto, tienden a gastar parte de ello inmediatamente. Diversos economistas demostraron que los consumidores son cortoplacistas cuando dejan de recibir el dinero, reducen el gasto, al contrario de los planteos de Friedman.

Friedman planteaba soluciones de mercado a todo tipo de problemas -educación, atención sanitaria, tráfico de drogas ilegales- que en opinión de casi todos los demás exigían una intervención estatal extensa. Algunas de sus ideas eran sustituir las normas sobre contaminación por un sistema de permisos de contaminación que las empresas pueden comprar y vender. Y algunas de sus propuestas, como eliminar los procedimientos de concesión de licencia para los médicos y abolir la Administración de Alimentos y Medicamentos.

Cuando un gobierno intenta disminuir el paro por debajo de esa tasa natural mediante políticas monetarias muy expansivas, a corto plazo puede conseguirlo. Pero los agentes económicos se acaban dando cuenta de que, si con iguales salarios hay inflación, ven menguada su capacidad de adquisición de bienes y servicios. De tal modo que descuentan ese efecto, y en la próxima revisión de sus contratos elevarán sus salarios al alza, lo que incita a un nivel de paro mayor, todo intento sistemático por parte de los gobiernos de reducir el paro acaba creando inflación sin resolver el desempleo. Incluso puede haber un punto a partir del cual la curva de Phillips se torne en una curva de pendiente positiva, de tal modo que paro e incremento de inflación estén ligados. Eso sucedió en las crisis del petróleo de los años 70, situación que la teoría keynesiana era incapaz de explicar.

Según Friedman, el éxito de la intervención de los gobiernos es muy limitado, y lo que deben de hacer es eliminar las restricciones que impiden que la tasa de paro natural se instale en una cota más reducida. Friedman consideraba que, al igual que una política monetaria expansiva puede crear crisis económicas, una política restrictiva también puede ser perjudicial, mediante una deflación de precios. Así lo puso de manifiesto en 1963 cuando publicó, junto a Anna Schwartz, un voluminoso tomo titulado "A Monetary History of the United States, 1897-1958". Donde argumenta que la Gran Depresión fue consecuencia de la implantación de políticas equivocadas por parte de la Reserva Federal. 

Friedman también planteaba que la política monetaria tiene efectos reales sobre el empleo a corto plazo, pero a largo plazo solo tiene efectos nominales sobre los precios. Posteriormente parte de esta teoría sería refutada por diversos estudios.

Fue uno de los principales impulsores de la política conocida como monetarismo. Décadas después Reserva Federal y el Banco de Inglaterra adoptaban su doctrina a finales de la década de 1970, pero la abandonarían dada su inviabilidad pocos años más tarde. Friedman sostenía que el crecimiento constante de la oferta monetaria mantendría la economía estable. 

Estados Unidos y Reino Unido intentaron poner en práctica el monetarismo predicado por Friedman a finales de los setenta, los resultados fueron decepcionantes: en ambos países, la oferta monetaria no consiguió impedir recesiones graves. La Reserva Federal adoptó oficialmente objetivos monetarios planteados por Friedman en 1979, pero los abandonó en 1982 y oficialmente en 1984, cuando la tasa de desempleo superó el 10%. Desde entonces la Reserva federal adoptaría políticas contrarias a los postulados de Friedman. En segundo lugar contribuyó al desprestigio de su teoría el hecho que desde comienzos de la década de 1980, la inflación se mantenía baja, las recesiones eran breves y leves. Y todo ello a pesar de las fuertes fluctuaciones de la oferta monetaria, que criticaban los monetaristas y que los llevaron -incluso a Friedman- a predecir desastres económicos que no llegaron a materializarse. David Warsh, de "The Boston Globe" señalaba que "Friedman despuntó su lanza prediciendo la inflación en la década de 1980, durante la que se equivocó profunda y frecuentemente".

Propugnó medidas de corte liberal. Una de ellas fue el establecimiento del bono educativo, en la idea de incentivar la demanda educativa según las preferencias de los padres. Propuso la flexibilización de precios, desregulaciones y privatizaciones, sistemas de pensiones individualizadas, la legalización del consumo de drogas y hasta de la prostitución.

Defendió la abolición del servicio militar obligatorio, de los salarios mínimos y del seguro social. Sostenía que la economía de Hong Kong era el mejor ejemplo de una economía de capitalismo "laissez faire".

En un contexto general de revolución conservadora, Milton Friedman participó en el Partido Republicano. Aconsejó a Ronald Reagan en su campaña presidencial y durante sus dos mandatos, la política económica de Reagan se baso en las ideas de Friedman, que tenían como base a una reducción de peso del estado, bajas tasas marginales de impuestos a los ingresos más altos, la desregulación de la economía y la política monetaria como única herramienta para reducir la inflación. 

Para Friedman la libertad económica la tiende a generar el capitalismo, y que por eso mismo el socialismo la intenta imposibilitar y el estatismo a debilitar.

En 1980, la cadena pública de televisión estadounidense PBS emitió una serie de diez capítulos, escrita por Friedman, llamada "Free to choose".

En 1991, Friedman afirmó que Colombia no debía seguir pagando el precio de la ineptitud de las leyes norteamericanas con el tema de las drogas. Consideraba que la legalización de la droga, transformaría la situación de Colombia, pues consideraba que por ser ilegal el vicio y no poder ejecutar la ley, se crea un ambiente de crimen, de guerras, de pandillas. También afirmó que no solo se debería legalizar la droga, sino, que sería necesario introducirla en el campo industrial, de manera que con ella se produzcan medicinas que ayuden a aliviar enfermedades como el glaucoma. También considera que se deben procesar las sustancias alucinógenas en farmacias para así garantizar su calidad.

En 2003, afirmó que “El medio ambiente es un problema ampliamente sobrevalorado. [...] Contaminamos por el sólo hecho de respirar. No vamos a cerrar las fábricas con el pretexto de eliminar todas las emisiones de dióxido de carbono en la atmósfera. ¡Sería como ahorcarse ahora mismo!”.

Friedman influyó en los asistentes económicos de la dictadura del general Augusto Pinochet incluso desde antes del golpe de Estado. Dos años después de este, en 1975, visitó Chile como invitado especial de la Escuela de Negocios de Valparaíso, por sus exalumnos chilenos de la Escuela de Chicago ("Chicago Boys"). El programa económico puesto en práctica en Chile por el régimen de Pinochet por un grupo de economistas chilenos, tenía una fuerte impronta de Friedman, la mayoría de ellos entrenados en la Universidad de Chicago por Milton Friedman y Arnoldo Harberger. 

Los generales que habían perpetrado el golpe de Estado carecían de las habilidades técnicas para intervenir en la economía y los Chicago Boys asumieron esta tarea de tratamiento "de shock̈́" de la inestable economía chilena. El Comité Especial del Senado Norteamericano sobre Inteligencia revelaría más tarde que las medidas económicas que implementó la Junta Militar de Gobierno en Chile inmediatamente después del golpe de estado fueron diseñadas con ayuda de «colaboradores de la CIA». Según el diario "The Wall Street Journal" (2 de noviembre, 1973), los Chicago Boys estaban ansiosos del golpe e "impacientes por lanzarse" sobre la economía chilena. El modelo teórico que habían aprendido en Chicago de su profesor Friedman se implantó sin oposición política posible un año más tarde. Friedman visitó Chile por este motivo y, junto a Harberger, realizó varias charlas y declaraciones de prensa sobre su recomendado «tratamiento de shock» para la economía chilena, a su juicio, gravemente enferma.

Los efectos iniciales en la economía chilena fueron graves. El PIB cayó en un 12 %, la tasa de desempleo creció hasta el 16,5 %, y el valor de las exportaciones se redujo en un 40 %. Pero el sistema se empezó a afianzar a partir de 1977, iniciándose lo que se ha llamado el ""boom"", con cifras positivas en muchos ámbitos, pero con una constante alta tasa de desempleo, de 17-15 %, debido entre otras cosas, a los despidos masivos de empleados públicos, de funcionarios de las empresas privatizadas y la pérdida de empleo en los sectores manufacturero y exportador debido a las políticas cambiarías y de apertura de la economía. El ""boom"" duraría hasta la crisis económica de 1982, fuertemente influida por la recesión mundial de 1980 y que formó parte de la crisis de la deuda latinoamericana que provocaron un alza en tasas de interés y dificultades para acceder a nuevo crédito, debilitamiento de actividad real y una caída de términos de intercambio (el cobre tuvo una abrupta caída de precio a inicios de 1980). Chile quedó desprotegido a esta crisis internacional por su "excesiva dependencia del mercado externo", el "excesivo endeudamiento" privado (el crédito doméstico subió de 25 %, en 1976, al 64% del PIB en 1982) y la "fijación del dólar" ("switch" a tipo de cambio fijo) lo que provocó una de las crisis más profundas que afectaran a la nación en conjunto a la de 1930 y la de principios de los años setenta. Esto provocó una caída del PIB de un 13,6 % (la caída más alta registrada por Chile desde la crisis de 1929), un notable incremento del desempleo con tasas en torno al 20 % por varios años y la quiebra e intervención de numerosos bancos e instituciones financieras (fue intervenido el 60 % del mercado del crédito).

Friedman abogó por la reducción del gasto público (20%) y del número de empleos públicos (30%), la eliminación de políticas sociales y un inicial aumento de impuestos al consumo constituyeron el paquete inicial de medidas del ministro Sergio de Castro.

Su colaboración con la dictadura militar de Pinochet le sería reprochada, en una entrevista en el año 2000 Friedman lo atribuyó «a los comunistas que intentaron desacreditar a cualquier persona que hubiese tenido la conexión más leve con el Presidente Pinochet». Más adelante Friedman se refirió a este tema haciendo analogía entre la dictadura militar chilena y la dictadura china, donde dictó conferencias a los estudiantes de economía y se reunió con el secretario del Partido Comunista de China Zhao Ziyang, diciendo: «Dicté tanto en China como en Chile exactamente las mismas conferencias. He visto muchas manifestaciones contra mí por lo que dije en Chile, pero nadie ha hecho objeciones a lo que dije en China. ¿cómo se explica?»A pesar de ello se publicaría una carta que fue escrita por Friedman el 15 de abril del año 1975 dirigida al dictador Augusto Pinochet, donde Friedman expresaba un fuerte apoyo a la dictadura pinochetista y a su régimen. Después de haber visitado Chile el 21 de marzo de 1975, le escribe a Pinochet una carta donde agradece la hospitalidad del dictador: “Nos hicieron sentir como si realmente estuviéramos en casa”.
El 14 de octubre de 1976 se anunció que Friedman recibiría el Premio Nobel de Economía "por sus logros en los campos del análisis del consumo, historia y teoría monetaria y por su demostración de la complejidad de la política de" estabilización"." El diario estadounidense" The New York Times" publicó una carta de dos Premio Nobel, George Wald (Medicina) y Linus Pauling (Química y Paz), en la que criticaban al comité de premiación por “una deplorable exhibición de insensibilidad” al otorgarle el premio a Friedman. Se sumaría también el nobel Salvador Edward Luria que calificaba la decisión del comité como “perturbadora” y “un insulto para la gente de Chile que cargaba con las reaccionarias medidas económicas avaladas por Friedman” (Friedman y Friedman 1998, 596-7). En diciembre, cuando Friedman viaja para recibir el premio, se produjeron múltiples y masivas manifestaciones. El 14 de diciembre de 1976, cuatro días después el Premio Nobel Gunnar Myrdal publicó una larga columna en el diario sueco "Dagens Nyheter", la que pronto aparecería reproducida en la popular revista americana "Challenge", donde critica a la Academia Sueca de las Ciencias por sus prácticas secretas en la elección de dicho nobel y argumentó que la entrega del Premio Nobel de Economía debería descontinuarse ya que era un acto político.

Uno de los primeros en criticar la teoría de Friedman y sus postulados fue Orlando Letelier, quien en una columna en "The Nation" criticaba a Friedman y a los Chicago Boys por aconsejar a Pinochet la imposición de su programa económico. Por otra parte, recalcaba la imagen de Friedman como el “arquitecto intelectual y asesor no oficial” del régimen militar chileno. Letelier vinculó las transformaciones económicas "neoliberales" que por entonces llevaban adelante los Chicago Boys, con la sostenida y sistemática violación de los derechos humanos en Chile.
Días después el crítico de Friedman y del régimen de Pinochet en Chile, habría sido asesinado por agentes del régimen pinochetista en la ciudad de Washington. El 21 de septiembre de 1976, en pleno centro de Washington, un atentado explosivo desintegró el automóvil de Letelier y causó su muerte y la de su secretaria Ronni Moffitt.

"La doctrina del shock: el auge del capitalismo del desastre" es un libro y documental escrito por la periodista canadiense Naomi Klein. en 2007, donde se habla de forma crítica sobre la influencia de Friedman y su teoría del libre mercado en diferentes países, cómo fue puesta a prueba en países que se encontraban lejos de ser libres, cómo lo poco popular de la medidas que recomendaba hacían necesario crear caos para poder implementarlas y cómo dichas medidas fracasaron, estableciendo un paralelismo con el fracaso de los experimentos con electroshock financiados por la CIA en los años 50.

La doctrina de Friedman ha sido criticada además por los resultados de la aplicación de su teoría, entre ellos que : en la primera mitad de 1975, como parte del proceso de suprimir las regulaciones de la economía, el precio de la leche en Chile fue eximido de control, al contrarío de las teorías de Friedman que postulaban que el libre mercado aumentaría la competencia y haría bajar los precios, en la práctica el precio al consumidor subió un 40% y el precio pagado al productor cayó 22%. También se le atribuyo un fracaso económico a sus teorías. A fines de 1975 la tasa anual de inflación en Chile había alcanzado 341%, la más alta inflación en el mundo, producto de la aplicación de la teoría monetarista. Los precios al consumidor aumentaron ese mismo año en un promedio de 375%; los precios al por mayor crecieron en 440%.El Producto Real Nacional Bruto se contrajo durante 1975 en casi 15% a su más bajo nivel desde 1969, mientras, de acuerdo con el Fondo Monetario Internacional, el ingreso nacional real "cayó en un 26%, dejando el ingreso real per cápita más bajo que su nivel de diez años anteriores. La disminución en el Producto Doméstico Bruto de 1975 refleja una caída de un 8.1% en el sector minero, una disminución de un 27% en la industria manufacturera y una caída de 35% en la construcción. La extracción de petróleo bajó en un 11% estimado, mientras transportes, almacenamiento y comunicaciones bajaron 15.3%, y el comercio disminuyó en 21.5.

El activista liberal económico Johan Norberg, autor de varios libros y documentales en favor de la globalización económica, considerando una falsificación histórica su lectura de las palabras de Friedman así como su reinterpretación de lo que Friedman interpretaba como "crisis" (consecuencia de las políticas estatistas) y como "shock" (la aplicación inmediata de un paquete de medidas de desregulación).

También acusó a Klein y los antiglobalistas de imponer una premisa de impopularidad de las economías de libre mercado, apelando a una falaz generalización con el fin de asociar la libertad económico-individual con la autocracia político-colectiva. Para el autor, así como para Friedman, la libertad política necesaria para hacer ejercicio de una democracia política, requiere de la libertad económica (sea que se elijan políticas socializantes o liberalizadoras de la actividad empresarial); Dentro de los economistas liberales, Friedman, ha sido criticado por Joshep Salerno en su libro "Money: Sound and Unsound", publicado en 2010 por el Instituto Ludwig von Mises.

En 1976 se traslada a San Francisco para integrarse a la Institución Hoover, donde siguió defendiendo la libertad económica. En 1998 escribe un libro junto a su esposa, titulado "Dos personas con suerte", donde relata sus memorias. A los 91 años en una nota periodística para el "Financial Times" Friedman renegaría de su propia teoría del monetarismo, y diría: "El control sobre la masa monetaria como un objetivo en sí mismo no ha sido un éxito. Hoy en día ya no creo en ello, como lo hice alguna vez".

Falleció a causa de un ataque cardíaco el 16 de noviembre de 2006, a los 94 años, en un hospital próximo a la ciudad de San Francisco, donde residía desde finales de la década de los años 70. Fue sobrevivido por su esposa Rose y por su dos hijos: Janet y David tenía cuatro nietos y tres bisnietos. Tras su muerte su familia pidió que, en lugar de recibir flores o regalos, todas las aportaciones deseadas se destinarán a la Milton and Rose D. Friedman Foundation.Fue incinerado y sus cenizas fueron esparcidas en la bahía de San Francisco-







</doc>
<doc id="7164" url="https://es.wikipedia.org/wiki?curid=7164" title="Brigitte Bardot">
Brigitte Bardot

Brigitte Anne-Marie Bardot (París, 28 de septiembre de 1934) es una actriz, cantante, y escritora francesa actualmente retirada del espectáculo y entretenimiento. Reconocida por ser icono de la moda y símbolo sexual de mediados del siglo XX. Además es activista de derechos de los animales, fundadora y presidente de la fundación que lleva su nombre. 

En su trayectoria como actriz, inició a partir de 1952. Su primer éxito fue al protagonizar "Y Dios creó a la mujer" dirigida por Roger Vadim en 1957, la cual obtuvo reconocimiento internacional y aunque no es su primera película, es ampliamente reconocida como el vehículo que logró llevarla al centro de atención pública. Luego, protagonizó la película de Jean-Luc Godard de 1963 Le Mépris. Por su papel en la película Viva Maria! de 1965 dirigida por Louis Malle, fue nominada en los Premios BAFTA como Mejor Actriz extranjera. .

Bardot se retiró de la industria del entretenimiento en 1973. Actuó en 47 películas, actuó en varios musicales y grabó más de 60 canciones. Fue galardonada con la Legión de Honor en 1985, pero se negó a aceptarlo. Después de retirarse, se convirtió en activista de los Derechos de los animales. Durante la década de 2000, generó controversia al criticar la inmigración y el islam en Francia y ha sido multada cinco veces por incitar al odio racial.

Hasta en la actualidad, sigue siendo un importante ícono de la cultura popular.

Brigitte Bardot nació en París, el 28 de septiembre de 1934. Vivió inicialmente en el distrito número tres, en un ambiente burgués. Hija de Louis "Pilou" Bardot (1896 - 1975) y Anne-Marie Mucel (1912 - 1978). Su padre era un empresario originario de Ligny-en-Barrois, Lorena (Francia), dueño de una empresa llamada Usines Bardot (propiedad de Air Liquide), con sede en la calle Vineuse en la ciudad de París. Su madre, conocida como "Toty" pasó su infancia en Italia. Brigitte y su hermana menor, Marie-Jeanne (llamada Mijanou), nacida el 5 de mayo de 1938, recibieron una educación estricta. Desde una edad temprana, Bardot sufrió ambliopía, lo que le impidió ver por su ojo izquierdo.

Durante el transcurso su infancia, se convirtió en una niña apasionada por el ballet y la danza clásica. A los siete años entró a la escuela de danza de madame Bougart.

En 1947, Bardot fue admitida en el Conservatorio Nacional Superior de Música y Danza, a pesar de la rígida selección de estudiantes y un número limitado de lugares. Durante tres años asistió a las clases de ballet de Jeanne Schwartz y más tarde al coreógrafo ruso Boris Knyazev. También estudió con Leslie Caron, quien más tarde se hizo famosa por la película " American in Paris" (1951) con la participación de Gene Kelly. De acuerdo con el recuerdo de su compañero de estudios, Brigitte era elegante y plástica, pero al mismo tiempo lenta, no demasiado fuerte y no trabajadora. Karon creía que Bardot podría haberse convertido en una bailarina "hermosa" si trabajara más duro en el aula. Knyazev no solo era un maestro de primera clase, sino también un verdadero tirano: golpeaba a los estudiantes negligentes con un látigo, y Brigitte a menudo obtenía más que el resto. Karon recordó: "Knyazev generalmente caminaba alrededor de la clase con una pila en sus manos y, si la niña no cumplía con sus requisitos, no dudó en usarla". Sin embargo, fue gracias a Knyazev y su severa exactitud que Bardot aprendió a moverse con gracia, desarrolló una figura plegable y creó su andar famoso en el futuro.

Al comienzo de la temporada de 1948, Leslie Caron y varias otras chicas de la clase de Knyazev recibieron una invitación para el "Ballet en los Campos Elíseos" . Brigitte miró desde detrás de las escenas, como ensaya un grupo, y así tomó lecciones de artesanía. Uno de los directores de teatro, Jean Robin, se acordó bien de la niña: “Tenía entre 13 y 14 años. Me recordó a un tallo, alto y tan delgado, lejos de ser hermoso y terriblemente tímido. Tenía miedo incluso de pronunciar una palabra. Bardot nunca recibió una invitación para participar en la compañía, que el próximo año realizó una gira a Egipto. Más tarde, un amigo de los padres de Bardot, Christian Foy, el bailarín principal del "Ballet en los Campos Elíseos", la invitó a ir con su conjunto de gira a la ciudad de Fougères y Rennes . Así, por primera vez, Bardot tuvo la oportunidad de actuar con una compañía de ballet profesional. De vuelta en París, ella continuó asistiendo a clases Knyazev.

Brigitte está considerada como un mito erótico y sex-symbol de los años 1950 y 1960. Su gran belleza y sensualidad natural comenzaron a mostrarse en la adolescencia, etapa en la que apareció por primera vez en el cine: tenía 18 años, era 1952 y se trataba de la película "Le trou normand". Ese mismo año, se casó con el primero de sus cuatro maridos, el director de cine Roger Vadim. Sería una de las películas dirigidas por su primer marido, "Et Dieu... créa la femme" (1956) la que la lanzaría a la fama de la mano de Jean-Louis Trintignant. Una de las escenas protagonizadas por Bardot muestra a su personaje bailando descalza sobre una mesa y está considerada como una de las escenas más eróticas de la historia del cine.

Bardot es una de las pocas actrices europeas que han recibido la atención de los medios de comunicación estadounidenses. Cada vez que hacía una aparición pública en los Estados Unidos, era perseguida por una horda de periodistas que tomaban nota de todos y cada uno de sus movimientos.

En 1954 realizó su primera película en los Estados Unidos, "Un acte d'amour", coprotagonizada por Kirk Douglas. En 1965 se representó a sí misma en la película "Dear Brigitte" con James Stewart. Sin embargo, debido a lo limitado de su inglés, la actriz fue doblada en muchas de sus películas.

En 1974, justo después de su 40º cumpleaños, Bardot anunció su retirada de las pantallas, tras haber protagonizado cerca de 50 películas y grabado varios discos, uno de ellos, el que mejores críticas obtuvo, con el "chico malo" de la música francesa, Serge Gainsbourg.

A partir de entonces, Bardot se ha dedicado a la promoción de los derechos de los animales. En 1986 creó la Fundación Brigitte Bardot para la protección de los animales en peligro. 

Durante la década de 1990 generó controversia al criticar la inmigración, la islamización y el Islam en Francia, y ha sido multada cinco veces por "incitar al odio racial".

En 2010 el partido "Alianza Ecologista Independiente" le ofrece presentarse como candidata a la Presidencia de Francia.

En enero del 2013 anunció que pediría la nacionalidad rusa tal como lo hizo Gérard Depardieu si las autoridades de su país sacrificaban a dos elefantes enfermos, pues Brigitte es defensora de los derechos de los animales. La princesa Estefanía de Mónaco también se declaró en contra de la matanza de esta pareja de elefantes.

En la película "Estrellas del Futuro" (1955) Bardot aparece cantando por primera vez en pantalla. A finales de los años 50, en España, Bardot compró una guitarra y aprendió tres acordes. A principios de la década de 1960, sus amigos, los artistas Jean-Max Rivière y Claude Bolling la aconsejaron para poder cantar y grabar varias canciones. Eddie Barclay, propietario de un gran estudio de grabación en Francia, donde Bardot estaba grabando en varias ocasiones, dijo lo siguiente acerca de sus habilidades: "Ella no cantó tanto como contando una canción. A decir verdad, su voz es más bien débil. Pero Dios le dio a su rara habilidad de traer una canción para el oyente y de hecho, se refiere muy en serio para hacer estallar la carrera ".

En 1960, Bardot lanzó el álbum pop Behind Brigitte Bardot. Posteriormente, se publicaron varios registros más de la actriz, entre ellos Brigitte Bardot Sings (1963) [99] , B.B en (1964) y Special Bardot (1968) (las publicaciones francesas e internacionales tienen nombres diferentes y una lista alternativa de composiciones). BardoT cantó canciones en francés, inglés y español. Después de unas vacaciones en Brasil con Bob Zaguri, regresando a París, grabó la canción Maria Nimguem en portugués. Bardot cantó canciones en películas en las que ella interpretó, incluyendo "¡Viva, María!", "Roma Boulevard", "Doctor en el mar", "Futuras estrellas" y otras.

En 1967, Bob Zaguri se convirtió en el productor del espectáculo musical "Unique Bardot" (Bardot Especial). Fue una serie de diecisiete videoclips de canciones interpretadas por ella misma, cada una de las cuales contó su historia corta. Entre esas canciones estaba la famosa "Harley Davidson", escrita por el compositor francés Serge Gainsbourg. Más tarde Gainsbourg escribió para Brigitte algunas canciones más, y alguna de ellas como un dúo clásico.
En el Festival de Cine de Cannes de 1956, una verdadera estrella, Brigitte Bardot, eclipsa a Sophia Loren y Gina Lollobrigida, las mejores estrellas de la época y su atractivo sexual conmueve a La Croisette.

Al mismo tiempo, Roger Vadim y Raoul Levy terminaron de escribir un guion titulado "Et Dieu... créa la femme". Después de no haberse realizado por falta de medios financieros, la película se filmó en Saint-Tropez. Esta producción permitirá a Brigitte Bardot entrar en la leyenda del cine mundial y convertirse en un mito viviente, un modelo social y un símbolo sexual internacional.

Brigitte Bardot desempeña el papel de "Juliette Hardy", frente a Curd Jürgens, Christian Marquand y Jean-Louis Trintignant, con quien entra en una unión. Vadim, quien aún es su marido, define el carácter que interpreta: "Quería, a través de Brigitte, restaurar el clima de una era. Juliette es una chica de su tiempo, que se ha liberado de todos los sentimientos de culpa, de todos los tabúes impuestos por la sociedad y cuya sexualidad es completamente gratuita. En la literatura y las películas de antes de la guerra, ella habría sido comparada con una prostituta. Es en esta película una mujer muy joven, generosa, a veces sesgada y en definitiva difícil de alcanzar, sin otra excusa de su generosidad. Las escenas están censuradas, especialmente la de un "cunnilingus"."

Cuando se estrenó en Francia, la película fue recibida con cierta reserva por los críticos y despertó la hostilidad de los círculos conservadores. Brigitte Bardot es criticada sin piedad por su verbo arrastrando y su articulación considera cuestionable. Paul Reboux dice de ella que tiene "el físico de un boniche y la forma de hablar analfabeta". Raoul Levy y Roger Vadim deciden la explotación de la película en el extranjero y esperan que habrá un éxito. Más tarde triunfó en Estados Unidos y Bardot se convirtió en una de las francesas más famosas en Norteamérica. Los norteamericanos incluso inventaron el término "bardolâtrie" para describir el entusiasmo que llegó a generar. La película luego sale a la venta en Francia y tiene un triunfo rotundo. Cinemonde escribe: "El atractivo sexual es Marlene Dietrich, el glamour es Ava Gardner, Brigitte Bardot mezcla todos estos ingredientes explosivos, agrega un toque de fantasía personal".

Durante tres años consecutivos, por sus propios medios, Brigitte Bardot trabajó por el bienestar de los animales. Fue portavoz de la SPA (Société pour la prévention de la cruauté envers les animaux/Sociedad para la Prevención de la Crueldad contra los Animales) y comenzó a realizar proyectos para los perros abandonados en las calles. Posteriormente se alió con Allain Bougrain-Dubourg.

A comienzos de 1976, se une con Brian Davis, quien ostentaba un cargo en la IFAW (International Fund for Animal Welfare), y creó una campaña para denunciar la caza de focas después de ver un documental sobre este tema. Dicha práctica es tradicional entre los Inuit, quienes se encuentran en la región del Ártico, con el fin de obtener la carne, la piel, la grasa (o aceite) y los huesos de los animales. La caza de focas permitía alimentar durante siete meses a unas 15.000 familias de pescadores.

Lo que condenaba la actriz era los métodos empleados por los cazadores, quienes generalmente atacaban crías de pocos días de vida, las cuales eran despojadas de sus pieles estando todavía conscientes en algunos casos. Bardot dirigió una manifestación frente a la embajada noruega y llegó a conmover la opinión pública, pero no lo suficiente como para hacer cambiar de opinión a los responsables de la caza.

El 15 de marzo de 1977, el presidente francés Valéry Giscard d'Estaing prohibió la importación de pieles de foca en Francia. El 20 de marzo de 1977, Bardot, que seguía siendo una estrella en los ojos de todo el mundo, se dirigió a Canadá con el fin de denunciar la caza de crías de foca por su piel. A continuación, comenzó una pelea que cambiaría su vida. Su viaje duró cinco días bajo la presión sin precedentes de los medios de comunicación. A su llegada, reprendió a los cazadores y les gritó "canadienses asesinos", añadiendo luego en una conferencia de prensa:

En su lucha, Bardot ha sido apoyada por numerosas personalidades famosas tales como Isabelle Adjani, Kim Basinger, Tippi Hedren, Ursula Andress y Johnny Hallyday.
El 21 de diciembre de 1952, a los 18 años, Bardot se casó con el director Roger Vadim. Se divorciaron en 1957, después de menos de cinco años de matrimonio; no tuvieron hijos juntos, pero se mantuvieron en contacto e incluso colaboraron en proyectos posteriores. La razón declarada del divorcio fueron los asuntos de Bardot con otros dos hombres. Mientras estaba casado con Vadim, Bardot tuvo una aventura amorosa con Jean-Louis Trintignant, quien fue su coprotagonista en "Et Dieu... créa la femme". Trintignant en ese momento estaba casado con la actriz Stéphane Audran.Los dos vivieron juntos durante unos dos años, abarcando el período anterior y posterior al divorcio de Bardot de Vadim, pero nunca se casaron. Su relación se complicó por la frecuente ausencia de Trintignant debido al servicio militar y el romance de Bardot con el músico Gilbert Bécaud.

A principios de 1958, su divorcio de Vadim fue seguido rápidamente por su ruptura con Trintignant y un colapso nervioso reportado en Italia, según informes periodísticos. También se notó un intento de suicidio con pastillas para dormir dos días antes, pero su gerente de relaciones públicas lo negó. Se recuperó en cuestión de semanas y comenzó una aventura con el actor Jacques Charrier. Quedó embarazada mucho antes de casarse el 18 de junio de 1959. El único hijo de Bardot, su hijo Nicolas-Jacques Charrier, nació el 11 de enero de 1960. Después de que ella y Charrier se divorciaron en 1962, Nicolas creció en la familia Charrier y tuvo poco contacto. con su madre biológica hasta su edad adulta. 

El tercer matrimonio de Bardot fue con el playboy millonario alemán Gunter Sachs, y duró del 14 de julio de 1966 al 1 de octubre de 1969. En 1968, comenzó a salir con Patrick Gilles, quien la coprotagonizó con ella en "El oso y la muñeca" (1970); pero ella terminó su relación en la primavera de 1971.

En los años siguientes, Bardot salió en sucesión con el barman / instructor de esquí Christian Kalt, después con Luigi Rizzi un propietario de un club, el músico (productor posterior) Bob Zagury, el cantante Serge Gainsbourg, el escritor John Gilmore y los actores Warren Beatty y Laurent Vergez, su coprotagonista en "Si Don Juan fuese mujer". La más larga de estas relaciones fue con el escultor Miroslav Brozek; ella vivió con él desde 1975 hasta diciembre de 1979 y posó para algunas de sus esculturas. Después de romper con Brozek, tuvo una relación a largo plazo con el productor de televisión francés Allain Bougrain-duBourg.

El cuarto y actual esposo de Bardot es Bernard d'Ormale, exasesor de Jean-Marie Le Pen, exlíder del Frente Nacional del partido de extrema derecha (ahora Rally Nacional a partir de junio de 2018); se casaron el 16 de agosto de 1992.

En 1974, Bardot apareció en una sesión de fotos desnuda en la revista Playboy, que celebró su 40 cumpleaños. El 28 de septiembre de 1983, cuando cumplía 49 años, Bardot tomó una sobredosis de somníferos o sedantes con vino tinto. Tuvieron que llevarla de urgencia al hospital, donde le salvaron la vida después de usar una bomba estomacal para evacuar las píldoras de su cuerpo. Bardot también es una sobreviviente del cáncer de mama.

Bardot incursionó en la música, especialmente en los años 60.




Brigitte Bardot también ha escrito cinco libros:






</doc>
<doc id="7165" url="https://es.wikipedia.org/wiki?curid=7165" title="Tardigrada">
Tardigrada

Los tardígrados (Tardigrada), llamados comúnmente osos de agua debido a su aspecto y movimientos, constituyen un filo de ecdisozoos dentro del reino animal, caracterizado por ser invertebrados, protóstomos, segmentados y microscópicos (de 500 µm de media). Además se agrupan dentro del gran grupo de los panartrópodos por presentar caracteres que sugieren que comparten un antecesor común con los artrópodos, junto a los onicóforos.

Fueron descritos por primera vez por Johann August Ephraim Goeze en 1773, el cual los denominó como oso de agua (del alemán "Kleine Wasser-Bären", literalmente ‘ositos de agua’) y hace referencia a la manera en la que caminan, similar al andar de un oso. Más tarde, el término tardígrado (que significa ‘de paso lento’) fue dado por Lazzaro Spallanzani en 1777 justamente debido a la lentitud de este animal.

Son organismos extremófilos (resistentes a condiciones extremas), con características únicas en el reino animal como poder sobrevivir en el vacío del espacio o soportar presiones muy altas de casi 6000 atm; pueden sobrevivir a temperaturas de -200 °C y hasta los 150 °C, a la deshidratación prolongada (pueden pasar hasta 10 años sin obtener agua) o a la radiación ionizante.

La mayoría de los tardígrados son terrestres y habitan fundamentalmente en los musgos, líquenes o helechos, aunque también pueden llegar a habitar aguas oceánicas o de agua dulce, no habiendo rincón del mundo que no habiten. Los especímenes adultos más grandes pueden verse a simple vista porque llegan a alcanzar un largo de 0,5 mm de media, sin embargo, los más pequeños pueden medir 0,05 mm solamente.

Son de forma ovalada o alargada, pueden entrar en criptobiosis (metabolismo reducido) y se alimentan succionando líquidos vegetales o animales además de tener células eucariotas, poseen cutícula no quitinosa aunque pueden mudar.
Se conocen más de 1000 especies de tardígrados. Algunos autores todavía los consideran una clase de artrópodos.

Los tardígrados están formados por unas mil células y algunas especies son eutélicas, es decir, mantienen constante el número de células durante su desarrollo. Sin embargo, según otras fuentes, el número de células es de unos 40000.

Algunos tardígrados ponen sus huevos a la vez que mudan la cutícula (cubierta externa), de tal forma que la puesta queda alojada en la cutícula de la que acaban de desprenderse, que le servirá de protección.

Dotados de simetría bilateral, con la zona ventral aplanada y la dorsal convexa, los tardígrados constan de cinco segmentos no diferenciados. Un segmento cefálico poco diferenciado de forma roma que contiene la boca (con un par de estiletes internos) y, en ocasiones, puntos o manchas oculares y cirros sensoriales. Los cuatro segmentos restantes tienen cada uno un par de patas ventrolaterales terminadas con garras (entre cuatro y ocho) o con ventosas; normalmente los primeros tres pares se destinan a la locomoción mientras que el cuarto sirve para anclarse al sustrato dado que los tardígrados son extremadamente ligeros e incluso una leve brisa puede arrastrarlos fácilmente. La cutícula no quitinosa exterior que los recubre puede ser de una gran variedad de colores. Los tardígrados son ovíparos, dioicos y experimentan un desarrollo directo, sin fases larvarias. Carecen de sistemas circulatorio y respiratorio, pero sí disponen de aparatos nervioso, excretor y reproductor. Poseen unas células (matoxistemas) que les permiten sobrevivir en cualquier medio ya sea: agua, aire, vacío, etc.

Lo más destacado del aparato digestivo es su estructura bucal. Se caracteriza por una abertura bucal o probóscide formada por unos tres anillos de cutícula incrustada hacia la cavidad interior, que continúa con una faringe tubular y después una succionadora, en la que hay unos potentes músculos circulares que hacen los movimientos de succión. En esta musculatura hay unas estructuras esclerotizadas denominadas macroplacoides, que dan rigidez a la estructura y además suponen un punto de inserción para los músculos suctores. A la estructura de la boca va asociada dos estiletes punzantes que están asociados a músculos retractores y protractores. Su función es atravesar las paredes de los vegetales de los que se alimenta y succionar los fotosintatos. Los estiletes en reposo se encuentran embebidos en las glándulas salivales, las cuales son las encargadas de secretarlos de nuevo, junto con el resto de la estructura bucal, tras la ecdisis (proceso de muda).

Los tardígrados se alimentan de bacterias, algas, criptógamas, rotíferos, nemátodos y otros invertebrados microscópicos. Normalmente sorben sus células pero en ocasiones ingieren los organismos completos.

Tal vez la cualidad más fascinante de los tardígrados es su resistencia y capacidad, en situaciones medioambientales extremas, de entrar en un estado de animación suspendida conocido como criptobiosis o estado anhidrobiótico. Mediante un proceso de deshidratación, pueden pasar de tener el habitual 85 % de agua corporal a quedarse con tan solo un 3 %. En este estado el crecimiento, la reproducción y el metabolismo se reducen o cesan temporalmente y así pueden pasar hasta 4,4 años. En 2016 científicos del Instituto Nacional de Investigación Polar de Japón (NIPR) consiguieron reanimar a ejemplares que llevaban más de 30 años congelados.

Esta resistencia permite a los tardígrados sobrevivir a temporadas de frío y sequedad extremos, radiorresistencia a la radiación ionizante y resistencia al calor y la polución. Existen estudios que demuestran que, en estado de metabolismo indetectable, pueden sobrevivir a temperaturas que oscilan entre los –20 °C y los 100 °C. En condiciones de laboratorio extremas parece que pueden sobrevivir a temperaturas entre -273 °C, casi el cero absoluto, y 151 °C. Asimismo se indica que pueden sobrevivir a la inmersión en alcohol puro y en éter. Científicos rusos afirman haber encontrado tardígrados vivos en la cubierta de los cohetes recién llegados de vuelta del espacio exterior. Recientes investigaciones demuestran que son capaces de sobrevivir en el espacio exterior.

En 1948 la bióloga italiana Tina Franceschi rehidrató unos tardígrados procedentes de una muestra de musgo seca, conservada en un museo desde 1828. Al cabo de doce días, uno de los ejemplares mostró algunos ligeros signos de movimiento, después nada. Las observaciones de Franceschi fueron muy exageradas en las citas subsecuentes, afirmándose en numerosos trabajos, aunque sin fundamento real, que los tardígrados pueden revivir tras permanecer 120 años en estado de criptobiosis.

En septiembre de 2007 se lanzó la sonda espacial Foton M3 de Rusia y la ESA, y en ella fue colocado un grupo de tardígrados. Se comprobó que no solo sobrevivieron a las condiciones del espacio exterior, sino que incluso mantuvieron su capacidad reproductiva, por lo que se les considera el ser vivo más resistente. Además, pueden soportar 100 veces más radiación que los seres vivos más resistentes y pueden pasar años en un estado de hibernación sin agua, y reactivarse en cuanto se les suministre.

El filo de los tardígrados se compone de tres clases: heterotardígrados, eutardígrados y mesotardígrados, aunque este último taxón se basa solo en una descripción de la especie "Thermozodium esakii" (Rahn, 1937) de un manantial japonés de agua caliente cerca de Nagasaki. Los especímenes y el manantial fueron destruidos por un terremoto de modo que la clase y la especie es dudosa ("nomen dubium").

Las relaciones filogenéticas de los tardígrados no están claras. Considerados a veces un filo pseudocelomado, o miembros de un grupo denominado Pararthropoda (grupo en el que también se incluían los onicóforos y que se ha demostrado parafilético), la tendencia actual es la de situarlos junto a onicóforos y artrópodos en un clado denominado Panarthropoda dentro de Ecdysozoa, algunas filogenias recientes los consideraron más próximos a los nematodos que a onicóforos y artrópodos; sin embargo ahora se cree que esta filogenia es consecuencia de la atracción de ramas largas. Según los análisis moleculares constituyen el primer grupo divergente del clado Panarthropoda.



</doc>
<doc id="7166" url="https://es.wikipedia.org/wiki?curid=7166" title="Criptobiosis">
Criptobiosis

La criptobiosis es un estado que consiste en la suspensión de los procesos metabólicos, en la que algunos seres vivos entran cuando las condiciones ambientales llegan a ser extremas. Un organismo en estado criptobiótico puede vivir indefinidamente hasta que las condiciones vuelvan a ser de nuevo tolerables. 

Los criptobiontes han sido clasificados en: a) aquellos en los que el estado criptobiótico puede aparecer solo en un específico estadio de desarrollo ontogenético y b) aquellos que pueden entrar en criptobiosis en cualquier estadio de su ciclo de vida. La primera categoría incluye especies de artrópodos, crustáceos, braquiópodos, insectos, esporas de ciertos hongos y bacterias, así como polen y semillas de algunas plantas; en tanto que la segunda categoría principalmente incluye especies de protozoarios, rotíferos, nematodos, tardígrados, ortópteros y varias especies de musgos, líquenes y algas, así como también algunas plantas superiores.

El ejemplo más conocido de seres que entran en este estado son los tardígrados (del latín "tardígradus", que significa movimientos lentos), conocidos como ositos de agua; son animales invertebrados capaces de vivir en cualquier parte del mundo, desde las profundidades abisales del mar hasta los lugares terrestres más inhóspitos. Existen desde hace 600 millones de años con más de 1000 especies identificadas.

Existen varios tipos de criptobiosis:

Durante la criptobiosis se activan potentes mecanismos de reparación del ADN, así como la producción de membranas biológicas con fosfolípidos específicos, enzimas antioxidantes, carbohidratos y proteínas que protegen a las células y los tejidos. Estos carbohidratos y proteínas se conocen como bioprotectores e interactúan directamente con el ADN, membranas celulares y otras proteínas. 

Se ha observado que durante la deshidratación, los tardígrados acumulan un disacárido llamado trehalosa. Éste actúa como estabilizador molecular al formar puentes de hidrógeno con las membranas, reemplazando al agua y preservando las estructuras. También forma “cristales”, un estado en el que la movilidad de las macromoléculas se reduce enormemente. Hay además otros organismos que también entran en estado criptobiótico, como las artemias salinas que utilizan hasta cinco veces más trehalosa que la encontrada en los osos de agua; muchos bacilos tienen la capacidad de formar esporas, separando su nucleoide del resto del citoplasma y formando a su alrededor una doble membrana que incluye pared celular y que constituye la cubierta externa de la espora. Las esporas pueden conservar su viabilidad durante mucho tiempo.

También se cree que la entrada y salida de la criptobiosis depende de la síntesis de las proteínas de choque térmico, unas moléculas que responden a diferentes tipos de estrés externo. Éstas ayudan a que las nuevas proteínas que se están sintetizando se plieguen de forma apropiada, las protegen del desdoblamiento y las llevan al lugar dentro de la célula donde cumplirán su función. Actúan por lo tanto como estabilizadores bioquímicos en condiciones ambientales adversas, y en el proceso de recuperación y reparación cuando estos organismos regresan al estado activo. 



</doc>
<doc id="7171" url="https://es.wikipedia.org/wiki?curid=7171" title="Arquitectura Intel Itanium">
Arquitectura Intel Itanium

Intel Itanium, antes conocida como IA-64 (Intel Architecture-64), es una arquitectura de 64 bits desarrollada por Intel en cooperación con Hewlett-Packard para su línea de procesadores Itanium e Itanium 2. Usa direcciones de memoria de 64 bits y está basada en el modelo EPIC (Explicitly Parallel Instruction Computing, procesamiento de instrucciones explícitamente en paralelo). 

Los procesadores Intel Itanium 2 representan un complejo diseño con más de 1700 millones de transistores, lo que les permite ofrecer muy altas capacidades de virtualización, mejoramiento de confiabilidad y niveles de rendimiento.

La serie de procesadores Intel Itanium 2 ofrece al usuario final una amplia gama de opciones de software con más de 8000 aplicaciones en producción.

Actualmente, los servidores y sistemas de cómputo de alto desempeño basados en el procesador Itanium ofrecen soporte de misión crítica para Windows, Linux, Unix y otros sistemas operativos.


</doc>
<doc id="7172" url="https://es.wikipedia.org/wiki?curid=7172" title="DEC Alpha">
DEC Alpha

DEC Alpha es una arquitectura de microprocesadores diseñada por DEC e introducida en 1992 bajo el nombre AXP, como reemplazo a la serie VAX. Cuenta con un conjunto de instrucciones RISC de 64 bits especialmente orientada a cálculo de coma flotante. 

La arquitectura Alpha se caracteriza por seguir la filosofía RISC (Conjunto de instrucciones reducidas). El primer procesador que hizo gala de la tecnología Alpha fue el 21064.

La organización de sus registros es de uso general con una arquitectura que se puede encuadrar como de registro-registro. Esto hace que la mayoría de sus instrucciones operen sobre los registros, haciendo uso de la memoria RAM sólo para instrucciones de carga y almacenamiento. La razón es que se intenta minimizar los accesos a memoria, puesto que suponen el cuello de botella para los procesadores actuales. La longitud de palabra de los registros es de 64 bits, ya sea desde el PC (contador de programa), pasando por los registros de enteros, coma flotante, etc.

Está preparado para manejar datos de 64 bits, pero también puede manejar datos de 32, 16 bits y por último de 8 bits.

La primera versión, el Alpha 21064 se lanzó en 1992 corriendo a 200 MHz.

El procesador de 64-bit fue un diseño supersegmentado (segmentación) y superescalar, como otros diseños RISC, pero sin embargo superándolos a todos y DEC lo promulgó como el procesador más rápido del mundo. 

En comparación, el Intel Pentium, de menor costo, salió a 66 MHz en el lanzamiento de esa primavera.

El Alpha 21164 estuvo disponible en 1995 a una frecuencia de 333 MHz. En julio de 1996 se elevó a 500 MHz, en marzo de 1998 a 666 MHz, y en mayo de 1998 el 21264 fue lanzado a 731 MHz. Los de 1 GHz, y otros más veloces, fueron anunciados en 2001 (el 21364 o EV7), y estuvieron disponibles desde 2003 a 1,1 GHz+. Alrededor de 500.000 sistemas basados en Alpha se vendieron hacia el final de 2000.

La producción de los chips Alpha fue licenciada a Samsung. Seguidamente la compra de DEC por Compaq puso a los productos en su mayoría bajo la firma API NetWorks, Inc. (previamente Alpha Processor Inc.), compañía fundada por Samsung y Compaq. En octubre del 2001 Microway comenzaba la venta exclusiva y provisión de servicios para la línea de Alpha de API NetWorks'. Después Compaq anuncia que los computadores que utilizan Alpha sufrirían un cambio en favor de Intel's Itanium en 2004. 

HP, nuevo dueño de Compaq, anuncia más tarde que el soporte continuaría varios años más, incluyendo el lanzamiento del chip EV7z (EV79 y EV8 ambos cancelados), pero esta sería la última instancia del chip. El supone un reemplazo a esta serie. 
HP continuará manteniendo y vendiendo Tru64 hasta 2006, y ha extendido el soporte hasta 2011. 

Irónicamente a mediados del 2003 cuando se pensaba pasarlo a retiro el Alpha encabezaba la lista de los computadores más rápidos de Estados Unidos. El 16 de agosto de 2004 HP anunció el lanzamiento del EV7z a 1,3 GHz, y que éste es el último modelo Alpha que van a producir.

Extensiones ISA :


</doc>
<doc id="7179" url="https://es.wikipedia.org/wiki?curid=7179" title="Pueblo yanomami">
Pueblo yanomami

Los yanomamiös o yanomamis son una etnia indígena americana dividida en tres grandes grupos: sanumá, yanomam y yanam. Aunque hablan lenguas diferentes, se entienden entre ellos. Se denominan también "la nación yanomami". Habitan principalmente en el Estado Amazonas de Venezuela y en los estados brasileños de Amazonas y Roraima.

Se ha apuntado que la razón por la que en muchos idiomas se conocen como "yanomami" se debe a que fueron los misioneros salesianos de origen italiano los que se encargaron de las misiones católicas en la región de los yanomamos y en italiano el plural de "yanomamo" es "yanomami". Así el padre Cocco, misionero italiano que pasó muchos años viviendo entre los yanomamos, los señalaba con este nombre y no con el plural en italiano. Por otra parte existen dos autónimos nativos que son "yąnomamö" y "yąnomami" que son formas de singular.

Varios investigadores están de acuerdo que los yanomamos tienen un origen poligénico, y que no son el resultado de la fusión de diferentes etnias de orígenes heterogéneos.

Alrededor de 20 000 individuos que integran los yanomamis viven desperdigados por la selva tropical, en aldeas separadas por muchos kilómetros de tierra deshabitada. Alrededor del 70 por ciento de esta población ocupa el sur de Venezuela, en el estado Amazonas, mientras que el resto se distribuye por zonas adyacentes a Brasil, en concreto en una zona que comprende parte del estado de Roraima y del Amazonas. Las comunidades yanomamis se concentran en la zona de la cuenca del río Mavaca, en los afluentes del Orinoco, y en la sierra Parima.

A pesar de que los contactos del pueblo yanomami con la sociedad dominante se iniciaron hace más de dos siglos, a consecuencia de la colonización de los portugueses en el Amazonas y el río Negro, estos permanecieron relativamente aislados en territorios de refugio, hasta mediados del siglo XX (década de los cincuenta) cuando comenzaron contactos más directos y permanentes con población no indígena. Expertos antropólogos como Jacques Lizot y otros autores afirman que los yanomamis migraron de la zona entre el río Blanco y el río Negro en Brasil, y de alguna manera se refugiaron en un territorio más seguro como la sierra Parima, cadena montañosa entre Venezuela y Brasil. Tras este asentamiento se dieron ciertas condiciones para que la población yanomami creciera númericamente y se expandiera hacia ciertas zonas del alto Orinoco y sus afluentes.

A mitad del siglo XX los yanomamis mantuvieron encuentros tensos y no amigables con criollos venezolanos y brasileños que se internaron en su territorio para la explotación cauchera, los cuales condujeron a varios enfrentamientos violentos con saldos de personas muertas y el rapto de otras por parte de los yanomamis. En la segunda mitad de dicho siglo, sobre todo a partir de la década del cincuenta, se realiza la expedición venezolano-francesa que descubrió las fuentes del Orinoco y se comienzan a establecer en el territorio yanomami un grupo de misiones religiosas que representan la primera presencia permanente y estructurada de personas no indígenas con actividades directas en la zona; en consecuencia los contactos son cada vez más crecientes entre estos y las comunidades yanomamis ubicadas en áreas de difícil acceso.

Viven en aldeas pequeñas, de entre 40 o 50 personas, que se construyen en círculo completamente abiertas. Sus viviendas tienen forma cónica y viven en grupos de familias. La situación de las cabañas puede variar y, en numerosas ocasiones, en lugar de formar un círculo, forman una hilera. Las familias comparten con las otras familias de la comunidad los productos obtenidos de la caza, la pesca o la cosecha (dentro de cada shabono conviven varias familias como una comunidad).

Los yanomamis tienen una tradición mitológica muy rica que continúa hasta el día de hoy, pese a la conversión de muchos pemones al catolicismo o al protestantismo. Varios de los mitos más importantes describen los orígenes del Sol y de la Luna, la creación de los tepuyes (monte Rorarima o Dodoima en pemón) y las actividades del héroe creador Makunaima y sus hermanos.

Una de las costumbres de esta etnia es la práctica del canibalismo endogámico como ritual sagrado: en una colectiva ceremonia funeraria se comen las cenizas de los huesos de su pariente muerto. Creen que en los huesos reside la energía vital de la persona fallecida y que al ingerir sus cenizas la reintegran al grupo familiar.

Utilizan la sustancia tóxica de unas plantas para impregnar las puntas de sus flechas. Este veneno (curare) paraliza al animal cazado sin alterar su consciencia ni la sensibilidad.

Las mujeres se adornan atravesando con un palo pequeño su tabique nasal y las comisuras de los labios. Utilizan también pinturas corporales. La etnia lleva siempre el mismo corte de pelo, con flequillo y la coronilla rasurada (estilo capuchino). Las cicatrices son muestra de valor y madurez. Tienen una pequeña estatura y solo se visten con un cinturón tubular los hombres y un pequeño fleco las mujeres.

La primera persona que estudió de manera formal los mitos y el lenguaje de los yanomamis fue el etnólogo Theodor Koch-Grünberg, quien visitó Roraima en 1912. Posteriormente, en los años setenta, por medio de la beca de la Fundação de Auxílio à Pesquisa do Estado de São Paulo (Fapesp) para estudiar a la tribu, la fotoperiodista húngaro-brasileña Claudia Andujar realizó sus primeros trabajos con la comunidad, lo que la llevaría a desarrollar un comprometido trabajo de registro y conservación del pueblo.

Los yanomamis se desplazan continuamente, es decir, son nómadas. Estos desplazamientos están motivados por el corto periodo de la productividad de sus cultivos. Cultivan en sus huertos la mayoría de alimentos: plátano, ñame, batata y malanga. Un cultivo dura dos o tres años. Cuando la tierra se agota, el poblado crea una nueva plantación en otro lugar. También recolectan productos silvestres y comen ranas.

Practican la caza todo el año, individualmente o en grupos, y utilizan el arco y la flecha. La pesca se practica con menos frecuencia y para pescar utilizan la flecha y el "timbó", es una especie de planta que pulverizan y esparcen en el agua para aturdir a los peces y de esta manera poder capturarlos fácilmente.

Al basar su economía en principios básicos de autoconsumo (elaboración de sus propias pertenencias, como cestas, garrotes, arcos y flechas), no tienen relaciones comerciales con pueblos vecinos. Actualmente siguen utilizando motivos "decorativos" ancestrales en sus cuerpos, los cuales se estampan con ciertos pigmentos naturales. Utilizan un veneno llamado curare, que untan en la punta de las flechas para cazar su alimento. También consumen la planta "epená" o virola, que contiene una sustancia alucinógena que utilizan en rituales curativos por los chamanes para comunicarse con los espíritus. Se utiliza en poca cantidad y en polvo, y se introduce en el chamán por medio de las fosas nasales soplándola a través de un palo hueco.

Debido a las condiciones climáticas, su vestimenta es muy ligera. Se visten con fines ornamentales más que protectores; un hombre bien vestido no lleva nada más que unas cuantas cuerdas de algodón en muñecas, tobillos y cintura, y el prepucio sujeto a la cuerda de esta última. 
También usan ramas enrolladas al cuerpo que tienen el nombre de "guayuco".

La vestimenta de las mujeres es igualmente escueta. Generalmente, se pintan el cuerpo con muchos colores, principalmente rojo y negro además se ponen collares, plumas en la cabeza y atadas a los brazos y pendientes.

La vida social se organiza en torno a los principios tribales tradicionales: relaciones de parentesco, descendencia de los antepasados, intercambios matrimoniales entre familiares o grupos con un parentesco común y la autoridad transitoria de jefes distinguidos que intentan mantener el orden en la aldea y son responsables de establecer las relaciones de la comunidad con otras aldeas. El liderazgo suele estar vinculado al parentesco y los vínculos matrimoniales: los "hombres grandes" o líderes, proceden de las familias más numerosas de las aldeas. Según su ingenio, sabiduría y carisma pueden convertirse en autócratas, aunque la mayoría de los jefes se limitan a actuar como superiores ante sus iguales. No están exentos de limpiar los huertos, recolectar, cosechar, plantar y cazar. 

Son al mismo tiempo pacificadores y valientes guerreros. La pacificación pasa a menudo por la amenaza o el uso de la fuerza, de ahí que la mayoría de los jefes tengan fama de "waiteri" o fieros.



Fue un conflicto armado en el año 1993, en Haximu, Brasil, cerca de la frontera con Venezuela. Sin embargo, el informe 32/12 de la Comisión Interamericana de Derechos Humanos, indica que las autoridades de Brasil y Venezuela se trasladaron al lugar de los hechos en los meses de agosto y septiembre de 1993, y determinaron que la aldea Haximu se encontraban en territorio venezolano. Unos dieciséis yanomamis fueron asesinados por un grupo de garimpeiros. 

En un boletín de noticias publicado el 7 de agosto de 2006 el Consejo Indigenista Misionario (CIMI) informa que:
Al comentar este caso, la ONG Survival International dijo: “La convención de la ONU sobre genocidio, ratificada por Brasil, dice que la matanza ‘con la intención de destruir, enteramente o en partes, un grupo nacional, étnico, racial, o religioso’ es genocidio. La sentencia de la Corte Suprema es muy significativa y sirve como advertencia importante para aquellos que continúan cometiendo crímenes contra pueblos indígenas en Brasil.”

Survival International y Wataniba, organizaciones que defienden los derechos de pueblos indígenas, alertaron en junio de 2018 sobre un brote de sarampión que afecta a varias comunidades amazónicas de yanomamis residentes en Brasil y Venezuela. Al haber entrado en contacto con la sociedad industrializada hace pocas décadas, los yanomami son altamente vulnerables a enfermedades contagiosas por no haber desarrollado inmunidad contra enfermedades comunes.

La Organización Panamericana de la Salud (OPS) confirmó en junio el brote de sarampión, señalando en su informe epidemiológico 995 casos notificados en Brasil: 611 en Amazonas y 384 en Roraima, los dos estados con mayor presencia de indígenas yanomami.




</doc>
<doc id="7181" url="https://es.wikipedia.org/wiki?curid=7181" title="Solifluxión">
Solifluxión

La solifluxión es el proceso geomorfológico característico de zonas de clima periglaciar (aunque puede darse incluso en los trópicos), consistente en el desplazamiento masivo y lento por gravedad de formaciones arcillosas u otros tipos de suelo sobre el permafrost a causa de la plasticidad y fluidez adquirida por aquellos cuando absorben gran cantidad de agua.

La solifluxión es propia de los suelos que han sido debilitados por la acción recurrente de heladas y, en consecuencia, las características originales del terreno a menudo están muy alteradas. En los climas periglaciares, la alternancia del hielo y del deshielo hace que la arcilla se precipite en forma de capas muy finas, en las cuales es más fácil el deslizamiento. Este puede generalizarse a toda una vertiente de pendiente moderada (solifluxión laminar) o se limita a una parte que, al despegarse, forma un nicho de desprendimiento. En los climas menos fríos, la solifluxión requiere mayores proporciones de arcilla o de marga en el terreno, y las coladas suelen ser de poca extensión.

El agua que empapa al terreno puede provenir del deshielo; en ese caso el fenómeno es calificado de gelifluxión. También puede proceder de infiltraciones del manto freático, pero la mayoría de las veces se trata de aguas pluviales o níveas.


</doc>
<doc id="7183" url="https://es.wikipedia.org/wiki?curid=7183" title="Sudáfrica">
Sudáfrica

Sudáfrica o, en uso minoritario, Suráfrica (oficialmente la República de Sudáfrica) es un país soberano, miembro de la Unión Africana, situado en África austral y cuya forma de gobierno es la república parlamentaria modificada. Su territorio está organizado en nueve provincias. Su capitalidad tiene un estatus especial, pues la componen tres ciudades: Pretoria, sede del poder ejecutivo; Bloemfontein, sede del poder judicial: y Ciudad del Cabo, sede del poder legislativo. Asimismo, la ciudad más poblada del país es Johannesburgo, siendo esta, además, una de las 40 áreas metropolitanas más grandes del mundo.

Posee 2798 kilómetros de costa en los océanos Atlántico e Índico.Limita al norte con Namibia, Botsuana y Zimbabue, al este con Mozambique y Suazilandia, mientras que Lesoto es un país rodeado por el territorio sudafricano.

Sudáfrica es conocido por su diversidad de culturas, idiomas y creencias religiosas, por lo que se la conoce como "la nación del arco iris". Once idiomas son reconocidos como oficiales por la Constitución de Sudáfrica. Dos de los once idiomas son de origen europeo: el afrikáans, idioma que proviene directamente del neerlandés y es hablado por la mayoría de la población blanca y mestiza, y el inglés. Aunque el inglés tiene un importante rol en la vida pública y comercial es, sin embargo, el quinto idioma por hablantes nativos.

Sudáfrica es un país étnicamente diverso. El 79,5 % de la población sudafricana es negra, la cual está dividida en diferentes grupos étnicos que hablan diferentes lenguas bantúes, nueve de las cuales son oficiales. Además cuenta con las mayores comunidades de habitantes de procedencia europea e india, así como de comunidades multirraciales del continente.

Sudáfrica es uno de los miembros fundadores de la Unión Africana, y tiene la mayor economía del continente entre todos los miembros. Es también miembro fundador de la ONU y del NEPAD. El país es miembro de la Mancomunidad de Naciones, el Tratado Antártico, el G77, la ZPCAS, la SACU, la OMC, el FMI, el G20, el G8+5, los CIVETS, los BRICS, entre otros.

Sudáfrica es también un país en el que existen grandes desigualdades entre los distintos grupos sociales; mientras existen grandes fortunas y las capitales están entre los principales centros de negocio de África, aproximadamente una cuarta parte de la población sudafricana se encuentra desempleada y vive con menos de 1,25 dólares estadounidenses al día.

Posee una rica fauna y flora por lo que se encuentra dentro de la lista de países megadiversos.

Sudáfrica cuenta con algunos de los yacimientos paleoantropológicos más antiguos de África, hace millones de años, estuvo habitada por grupos de "Australopithecus africanus" que sobrevivían recolectando raíces, frutos secos y vegetales, moluscos, cazando y pescando. Les sucedieron varias especies de "Homo", incluyendo "Homo habilis", "Homo ergaster" y el hombre moderno ("Homo sapiens"). Los bosquimanos desde hace 100.000 años y posteriormente los hotentotes actuales, fueron los primeros grupos humanos afincados. Durante la edad de hierro y hasta la actualidad, grupos humanos de etnia negra se extendieron por el territorio.

Agricultores y ganaderos bantúes se establecieron en el s. IV y V al sur del río Limpopo. Más tarde se trasladaron más al sur, a la actual provincia de KwaZulu-Natal, donde se encuentra la fundición más antigua, que data del 1050. En el periodo histórico, el grupo étnico de los xhosa se estableció incluso más al sur, alcanzando el río Fish en lo que actualmente es la provincia Oriental del Cabo. Estas poblaciones más avanzadas desplazaron a los pobladores nativos cazadores-recolectores.

En el momento de la llegada de los europeos, la población indígena era el resultado de una inmigración procedente de otras partes de África, entre las que destacaban los pueblos xhosa y zulú. Si bien desde finales del siglo XV los europeos habían navegado cerca de las costas sudafricanas, solo en 1652, la Compañía Neerlandesa de las Indias Orientales estableció un pequeño asentamiento que se convertiría en Ciudad del Cabo. La ciudad se convirtió en colonia británica en 1806, lo que provocó que los bóeres (colonos originarios de Holanda, Flandes, Francia y Alemania) y los colonos británicos se adentraran hacia el norte y el este del territorio, lo que desencadenó en una serie de conflictos entre los afrikáner, los xhosa y los zulúes por la posesión del terreno.

El descubrimiento de yacimientos de diamantes y minas de oro ocasionó el conflicto conocido con el nombre de Segunda Guerra Bóer, que enfrentó a británicos y bóeres por el control de los recursos minerales del país. Aunque los bóeres resultaron perdedores de la guerra, el Reino Unido concedió en 1910 una independencia limitada a Sudáfrica como colonia británica. En el interior del país, la élite blanca antibritánica llevó a cabo una serie de políticas con la intención de lograr la independencia total. La segregación racial fue tomando fuerza e impregnando la legislación sudafricana, instituyéndose el régimen que se conocería posteriormente con el nombre de "apartheid", que estableció tres clases de estratificación racial.

El país alcanzó finalmente la independencia en 1961, cuando fue declarada la República de Sudáfrica. El gobierno continuó legislando según el régimen del "apartheid", a pesar de la oposición tanto exterior como interior. En 1990, el gobierno sudafricano comenzó una serie de negociaciones que terminaron con las leyes discriminatorias y con la convocatoria de las primeras elecciones democráticas en 1994. Tras las elecciones el país volvió a unirse a la Mancomunidad de Naciones.

La historia escrita de Sudáfrica comienza con la llegada de los portugueses. En 1487 Bartolomé Díaz fue el primer europeo en alcanzar el punto más austral de África, y lo denominó "Cabo das Tormentas" (Cabo de las Tormentas) debido al mal tiempo que experimentó en la región. Sin embargo, cuando volvió a Lisboa cargado de noticias sobre el descubrimiento, el monarca Juan II de Portugal quiso cambiarle el nombre por el de "Cabo da Boa Esperança " (Cabo de Buena Esperanza) y prometió establecer desde ese punto una ruta marítima para que los portugueses pudieran ir a buscar las riquezas de la India. Más tarde el gran poeta portugués Luís de Camões inmortalizó el viaje de Bartolomé Díaz en el poema épico Os Lusíadas, concretamente con el personaje mitológico Adamastor, el cual simboliza las fuerzas de la naturaleza que los navegantes portugueses tuvieron que superar durante la circunnavegación de los cabos.

Los primeros relatos escritos de la historia de Sudáfrica se obtuvieron de los primeros navegantes y los náufragos supervivientes. Durante los dos siglos posteriores a 1488 los marineros portugueses realizaron algunos pequeños acuerdos de pesca en dicha costa, pero no se conserva ningún documento escrito sobre estos.

El 6 de abril de 1652, Jan van Riebeeck estableció un puesto de avituallamiento en el cabo de Buena Esperanza para la Compañía Neerlandesa de las Indias Orientales en el continente africano, para abastecer a los barcos de alimentos, agua y atención para los marineros enfermos.

Durante los siglos XVII y XVIII la pequeña colonia se fue extendiendo lentamente, principalmente bajo la soberanía neerlandesa. Los colonos finalmente se toparon con los pueblos Xhosa en expansión en la región del río Fish. Es entonces cuando se desencadenaron una serie de guerras llamadas las guerras de Fronteras del Cabo, originadas por conflictos por la tierra y los víveres. Para aliviar la escasez de trabajadores en el Cabo, se trajeron esclavos de Indonesia, Madagascar e India. Los descendientes de estos esclavos, que a menudo se casaron con colonos neerlandeses, fueron luego clasificados junto con los descendientes de los san como "mestizos del cabo" y "malayos del Cabo", constituyendo casi la mitad de la población de la provincia del Cabo Occidental.

Gradualmente, los neerlandeses se impusieron a los khoikhoi arrebatándoles sus arroyos, tierras y ganado. Incorporaron a los nativos como mano de obra agrícola o miembros de las milicias. La estructura política de los khoikhoi no era suficientemente fuerte como para resistir. El esclavo no tenía derecho legal alguno, y a diferencia de los esclavos de América, muy pocas posibilidades de alcanzar la libertad mediante su conversión al cristianismo.
La Compañía tenía tal necesidad de mano de obra que en la primera década de asentamiento trajo esclavos de su imperio oriental y de regiones a ambos lados de África.

En el Cabo, la liberación obligatoria de los conversos al cristianismo sirvió de impedimento a la conversión al cristianismo y volvió más atractiva la conversión al islamismo por razones no solo religiosas sino también políticas. Los esclavos de la Compañía o los residentes en los pueblos tenían cierta posibilidad de practicar oficios. Los esclavos de los granjeros estaban bajo un control más estricto. Las esclavas no tenían la opción del matrimonio, pero sí era frecuente el concubinato con hombres blancos.

El Cabo se convirtió en una sociedad dividida en grupos claramente definidos desiguales ante la ley, y los negros libres nunca fueron lo suficientemente numerosos u organizados como para romper las barreras. Los empleados de la Compañía y los granjeros blancos establecieron un predominio defendido por la ley y reforzado por la libre inmigración. Los blancos mantendrían su predominio por más de tres siglos y medio, pese a varios intentos de emancipación.

Gran Bretaña ocupó el área del cabo de Buena Esperanza en 1795 durante la guerra de la Primera Coalición. Los neerlandeses declararon la bancarrota de la Compañía Neerlandesa de las Indias Orientales en 1798. Por el Tratado de Amiens (1802) la colonia fue devuelta a la República Bátava y los británicos se anexionaron la Colonia del Cabo en enero de 1806, conquista legalizada por el Congreso de Viena. Los británicos continuaron con sus guerras contra los Amaxhosa, empujando la frontera oriental al este, siguiendo una línea de fuertes establecidos a lo largo del río Fish y consolidándola promoviendo nuevos asentamientos británicos. Debido a la presión de las sociedades abolicionistas de Gran Bretaña, el parlamento británico primero paró su comercio de esclavos en 1806, para posteriormente abolir definitivamente la esclavitud en todas sus colonias en 1833.

Los descubrimientos de diamantes en 1867 y el oro en 1886 animaron el crecimiento de la economía y la inmigración, lo que intensificó la subyugación de los nativos. Los bóeres resistieron con éxito el asedio de los británicos en la primera guerra bóer (1880-1881), basada en tácticas que aprovechaban mejor las condiciones locales. Por ejemplo, los británicos llevaban brillantes uniformes rojos, haciéndoles objetivos más fáciles para los tiradores Bóer. Durante la segunda guerra Bóer (1899-1902) los británicos regresaron en mayor número. El intento de los Bóer para aliarse con los alemanes de África del Sudoeste fue otra razón para controlar a las repúblicas bóeres. Los bóeres se resistieron con fiereza, pero los británicos finalmente derrotaron a las fuerzas bóeres, usando su superioridad numérica y el abastecimiento externo de equipamiento, además de la controvertida táctica de tierra quemada. El tratado de Vereeniging declaraba la soberanía británica sobre la totalidad de las repúblicas sudafricanas, y el gobierno británico acordó asumir la deuda de 3 millones de libras de los gobiernos Afrikáner. Una de las principales disposiciones del tratado era que a los negros no se les permitiría votar, salvo en la Colonia del Cabo.

Después de cuatro años, se creó la Unión Sudafricana a partir de la Colonia del Cabo, la Colonia de Natal y las repúblicas del Estado Libre de Orange y el Transvaal (estas dos últimas anexionadas después de la Guerra de los Bóeres en la Colonia del Río Orange y el Transvaal), el 31 de mayo de 1910, justo diez años después del fin de la segunda guerra bóer. En 1934, el partido sudafricano y los partidos nacionales se fusionaron para formar el partido unificado, que buscaba la reconciliación entre los Afrikaners y los blancos angloparlantes, pero se escindió en 1939 a raíz de la decisión sobre la entrada del país en la Segunda Guerra Mundial como aliado del Reino Unido.

El ala más conservadora del Partido Nacional simpatizaba con la Alemania nazi durante la guerra, y buscó una segregación racial o "apartheid" mayor después de la guerra. En 1948, el Partido Nacional llegó al poder. Abogando por un sistema segregacionista y racista, este inició el "apartheid", palabra que en Afrikáans significa "separación". Se creó un vasto sistema jurídico y social para separar a las etnias blanca y negra, con ventaja para la primera, a la que se le otorgaban privilegios políticos, económicos y sociales:


En 1960, después de la masacre de Sharpeville, Verwoerd llevó a cabo un referéndum pidiendo a la población blanca que se pronunciara a favor o en contra de la unión con el Reino Unido. El 52% votó en contra. Sudáfrica se independizó del Reino Unido, pero siguió siendo miembro de la Commonwealth. Su permanencia en esta organización se hizo cada vez más difícil, pues los estados africanos y asiáticos, indignados por el apartheid, intensificaron su presión para expulsar a Sudáfrica, que finalmente se retiró de la Commonwealth el 31 de mayo de 1961, fecha en que se declaró la República de Sudáfrica. 

Con el paso de los años, el apartheid provocó repudio, rechazo e indignación en el mundo entero. Numerosos países rompieron relaciones diplomáticas y comerciales con Sudáfrica, generando un creciente aislamiento del gobierno sudafricano. El país fue excluido de Miss Universo, de Miss Mundo, de los Juegos Olímpicos, de las Copas Mundiales de fútbol, rugby y otras competiciones deportivas. Dentro de Sudáfrica, los movimientos anti-apartheid, especialmente el Congreso Nacional Africano o CNA, iniciaron campañas de resistencia, huelgas, marchas, protestas y sabotajes que fueron reprimidos con dureza por las fuerzas del gobierno.

En 1989 se produjo un golpe palaciego dentro del Partido Nacional. En él, el Presidente Pieter Botha fue desplazado por Frederik De Klerk, y este inició el desmantelamiento del apartheid. Se levantó la proscripción que pesaba sobre el Congreso Nacional Africano y otras organizaciones políticas de izquierda, y se liberó a Nelson Mandela tras 27 años de prisión. La legislación del Apartheid fue gradualmente retirada. En un referéndum de 1993, los blancos aceptaron otorgarle el derecho al voto a la mayoría negra, y al año siguiente, en 1994, se realizaron las primeras elecciones democráticas del país. Nelson Mandela fue elegido presidente por mayoría absoluta en representación del CNA, partido que se ha mantenido en el poder desde entonces. El aislamiento internacional que pesaba sobre el país llegó a su fin; fue readmitido en la Mancomunidad de Naciones (Commonwealth) en ese mismo año de 1994. 

Mandela se convirtió en un símbolo de la lucha contra el apartheid dentro y fuera del país y una figura legendaria que representaba la falta de libertad de todos los habitantes negros de Sudáfrica.

La transición democrática se vio facilitada por un proceso único de la reconciliación y Ubuntu: la Comisión para la verdad y la reconciliación (Sudáfrica), presidida por el premio Nobel de la Paz (1984), el arzobispo anglicano de Ciudad del Cabo, Desmond Tutu, fue creado en 1995 y cerró su informe en 1998. La Comisión examinó los delitos graves contra los Derechos Humanos cometidos por todas las partes bajo el Apartheid, y podría conceder la amnistía a los criminales (“perpetradores”).

A pesar del fin del apartheid, millones de sudafricanos negros continúan actualmente viviendo en la pobreza, en parte a causa de los problemas heredados del régimen del apartheid y debido también a que los gobiernos post-apartheid han tenido las manos atadas en los temas económicos, los cuales durante la transición fueron gestionados casi exclusivamente por los miembros del gobierno saliente. De esta forma los blancos pierden el control político pero se aseguran mantener sus privilegios económicos. Sin embargo, la política de vivienda llevada a cabo por el CNA ha producido alguna mejora en las condiciones de vida en muchas regiones, si bien la desigualdad entre las distintas clases sociales es todavía muy grande, comparada con los estándares de otros países.

Aunque la economía está más diversificada, la exportación de oro y diamantes sigue siendo la fuente de ingresos más importante del país. Actualmente el gobierno de Sudáfrica está también empeñado en realizar una vasta reforma agraria para aliviar la tensión social y las desigualdades raciales. Esta reforma consiste en la devolución de tierras por parte de los blancos a los negros, a los cuales se las arrebataron en la época colonial (cerca de un 80% de las tierras cultivables aún están en manos de los blancos). La reforma avanza con lentitud: menos del 10% de las tierras han sido devueltas, por lo que el Gobierno del país ha decidido obligar a los blancos a vender las tierras por un precio razonable o expropiarlas en un corto período. Pero existe en algunos sectores de la sociedad, también, un gran temor a que la impaciencia de la población negra por tener tierras lleve a una reforma desordenada y caótica, lo que podría repetir la desastrosa reforma agraria realizada en la vecina Zimbabue, que arruinó la agricultura y causó una terrible hambruna en ese país.

El futuro de Sudáfrica parece incierto. La alarmante ola de criminalidad (50,000 homicidios por año, proporcionalmente, 8 veces más que en los EE. UU.) y la nueva legislación creada por el CNA, que prohíbe a los blancos ocupar numerosos puestos de trabajo, ahora reservados a los negros, están empujando a miles de blancos a abandonar el país. Hoy en día sigue siendo uno de los países con las tasas de desigualdad más altas del mundo. Desde el fin del Apartheid en 1994 hasta la actualidad ya han emigrado casi un millón de blancos. Los altos índices de delincuencia y la creciente sensación de que el CNA no ha sabido gobernar bien el estado, no hacen más que agravar la incertidumbre.

Jacob Zuma, actual líder del CNA, un político que fue acusado de corrupción y violación, los cuales fueron retirados), fue presidente de Sudáfrica desde el 9 de mayo de 2009, cuando fue elegido por la Asamblea Nacional después de que su partido obtuviese el 70% de los votos en las elecciones hasta febrero de 2018, cuando Zuma debió dimitir en medio de diversos escándalos de corrupción.

El 17 de agosto del 2012 se produjo una masacre de mineros en Lanmin, quienes reclamaban mejores condiciones de trabajo, fueron asesinados por un grupo de policías armados con fusiles automáticos y ametralladoras, pereciendo en el incidente 34 trabajadores mineros, siendo la peor masacre desde el fin del Apartheid.

Casi un centenario, Nelson Mandela falleció el 5 de diciembre de 2013. Al año siguiente se cumplieron 20 años del fin del Apartheid, con alrededor de 20 millones de sudafricanos (un 40% de la población) nacidos libres, que son la primera generación en crecer sin recuerdos del Apartheid.

El gobierno de Sudáfrica actúa bajo un sistema parlamentario modificado inspirado en el británico de Westminster, aunque notablemente distinto al de otros sistemas de países de la Commonwealth.

El Consejo Nacional de las Provincias (NCoP), que reemplazó al Senado en 1997, está formado por 90 miembros que representan a cada una de las nueve provincias de Sudáfrica, al mismo tiempo que también tienen representación las grandes ciudades. Cada provincia de Sudáfrica tiene una Legislatura Provincial Unicameral, y un Consejo Ejecutivo encabezado por un "Premier".

Tras el fin del "apartheid" el país fue readmitido en la Mancomunidad de Naciones, las relaciones exteriores de Sudáfrica se han enfocado en la Comunidad de Desarrollo de África Austral (SADC) y en la Unión Africana. Sudáfrica ha desempeñado un papel fundamental como mediador en conflictos internos africanos durante la última década, en Burundi, la República Democrática del Congo, Comoras y Zimbabue.

La Unión Sudafricana, precusora de la actual República, fue uno de los miembros fundadores de la Organización de las Naciones Unidas. El entonces primer ministro Jan Smuts escribió el preámbulo de la Carta de las Naciones Unidas. Sudáfrica fue un miembro no permanente del Consejo de Seguridad de las Naciones Unidas entre 2007 y 2008, generando controversia su actuación al votar en contra de la resolución que criticaba al gobierno birmano en 2006 y contra la aprobación de sanciones contra Zimbabue en 2008. Sudáfrica es miembro del Grupo de los 77, siendo el país presidente en 2006. Así mismo, Sudáfrica es miembro de la Zona de Paz y Cooperación del Atlántico Sur, la Unión Aduanera de África Austral, la Organización Mundial del Comercio, el Fondo Monetario Internacional, el G20 y el G8+5.
La Fuerza Nacional de Defensa Sudafricana fue creada en 1994, como una fuerza de voluntarios compuesta por antiguos miembros de las Fuerzas de Defensa de Sudáfrica, las fuerzas de grupos nacionalistas (Umkhonto we Sizwe y el Ejército de Liberación del Pueblo Azanian), y las fuerzas de defensa de los antiguos bantustanes. Las SANDF (por su siglas en inglés) se dividen en cuatro áreas: el Ejército Sudafricano, la Fuerza Aérea Sudafricana, la Marina Sudafricana, y el Servicio de Salud Militar Sudafricano.

En años recientes, la SANDF ha realizado mayoritariamente operaciones de mantenimiento de paz en África, involucrándose en Lesoto, la República Democrática del Congo y Burundi, entre otros. Además, ha tomado parte en operaciones multinacionales de las fuerzas de paz de la ONU.

Por otra parte, la Sudáfrica del apartheid contó con un programa de desarrollo de armas nucleares en la década de 1970 y pudo llevar a cabo unas pruebas nucleares en el océano Atlántico en 1979 (ver Incidente Vela). Sudáfrica es el único país africano que ha logrado desarrollar con éxito armas nucleares. Igualmente, fue el primer país con capacidad nuclear que renunció a su programa de forma voluntaria y desmanteló sus instalaciones nucleares, tras firmar el Tratado de No Proliferación Nuclear en 1991.

Sudáfrica es una república con un sistema democrático de gobierno, el cual está comprometido a lograr la igualdad entre hombres, mujeres y gentes de todas razas. La constitución es la ley suprema del país, aplicable a todos los órganos del Estado en todos los niveles de gobierno. Existe una separación entre el poder ejecutivo, legislativo y judicial, que mantiene equilibrio de poder entre ellos.

Las fuentes principales de las leyes de Sudáfrica fueron las leyes mercantiles romano-neerlandesas, junto con la ley Común inglesa, traídas por los colonos neerlandeses y británicos. La primera ley sudafricana basada en principios europeos fue traída por la Compañía Neerlandesa de las Indias Orientales y se denominó ley romano-neerlandesa. Fue importada antes de la codificación de la ley europea según el Código Napoleónico y es comparable en muchos aspectos a la ley escocesa. A ello siguió en el siglo XIX por la ley británica común y estatutaria. Comenzando en 1910 por la unificación, Sudáfrica tenía su propio parlamento, el cual legislaba específicamente para Sudáfrica, basándose en las leyes aprobadas anteriormente por los miembros individuales de las colonias.

La Sudáfrica posterior al "apartheid" es el país más avanzado de África en cuanto a su política de derechos respecto al colectivo homosexual. El 30 de noviembre de 2006, Sudáfrica se convirtió en el primer país del continente en legalizar el matrimonio entre personas del mismo sexo, entre protestas por parte de colectivos democristianos.


El presidente es la cabeza del Estado, jefe de Gobierno y jefe de la fuerza de la defensa de Sudáfrica. Este es elegido por el parlamento bicameral, que consiste en la Asamblea Nacional o cámara baja y el Consejo Nacional de las Provincias, o cámara alta. En la práctica, el presidente es el líder del partido mayoritario la Asamblea Nacional, que cuenta con 400 parlamentarios elegidos a través de un sistema electoral proporcional. Está encargado de conducir al país en los intereses de la unidad nacional de acuerdo a la Constitución y tiene las siguientes facultades: Otorgar distinciones, nombrar, acreditar, recibir y reconocer embajadores, representantes diplomáticos y otros funcionarios consulares, conceder el perdón o suspender la ejecución de una pena, firmar, ratificar convenios y tratados internacionales, anular o proclamar la ley Marcial, así como declarar la guerra o firmar la paz y nombrar a su gabinete para desempeñar las funciones que le concede la ley.


El parlamento es la autoridad legislativa de Sudáfrica, tiene la facultad de elaborar leyes para el país de acuerdo a la Constitución, compuesto de una Asamblea Nacional y un Senado. El Senado está compuesto de diez miembros por cada una de las provincias, cada legislador nomina a senadores proporcionalmente al soporte del partido en la provincia y sesiona por lo menos una vez al año, la primera a los treinta días después del día de la votación de las elecciones. El parlamento estará en funciones durante cinco años a partir de su primera sesión.


El poder judicial goza de autonomía e independencia con respecto al Gobierno. A la cabeza de este poder se encuentra la Corte Suprema de Justicia, compuesta por la división de apelación.

Cuando finalizó el "apartheid" en 1994, el Gobierno integró los bantustanes anteriormente independientes y semindependientes a la estructura política del país. Con este fin, abolió las cuatro antiguas provincias de Sudáfrica (El Cabo, Natal, Estado Libre de Orange y Transvaal) y las remplazó por nueve provincias totalmente integradas. Las nuevas provincias eran mucho más pequeñas que sus antecesoras, lo cual teóricamente daba a los gobiernos locales más recursos para distribuir en áreas de menor tamaño. Las nueve provincias fueron posteriormente subdivididas en 52 distritos, seis metropolitanos y 46 municipales. Los puertos principales de Sudáfrica son: Durban, Ciudad del Cabo, Puerto Elizabeth, East London, Richards Bay, Saldanha Bay y Mossel Bay.

Con una superficie de 1 219 090 km², la República de Sudáfrica se extiende en el extremo sur del continente africano. El país limita con Namibia al Noroeste; al norte con Botsuana y al este con Zimbabue, Mozambique y Suazilandia. El país es rodeado por los océanos Atlántico e Índico. Lesoto, país independiente pero con importantes lazos con Sudáfrica, se encuentra completamente rodeado de territorio sudafricano.

Debido a la extensión del país, el clima es muy variable en función de las zonas climáticas. En el sur y las zonas altas, el clima es templado, mientras que en el noreste y este el clima es tropical y en la parte occidental del país es semiárido. El promedio anual de precipitaciones es de 464 mm.

Los ríos principales son el río Orange, que desemboca en el océano Atlántico; el río Vaal, su principal afluente, y el Limpopo, que desemboca en el océano Índico, nace cerca de Johannesburgo y luego marca en el norte la frontera con Botsuana y Zimbabue.

El punto más austral del país, y por lo tanto del continente africano, es el cabo Agulhas.

En respuesta a la sequía, en octubre de 2019, las autoridades introdujeron restricciones de agua en las principales ciudades del país. Varias regiones del centro y norte del país ya habían sufrido cortes de agua, en particular debido al fallo de las instalaciones del principal proveedor de agua de Sudáfrica, Rand Water. En algunas provincias, como el Cabo Oriental y el Cabo Occidental, la sequía ha arruinado las cosechas y ha causado la muerte de rebaños de ganado.

Sudáfrica es el mayor contaminador del continente africano y el decimocuarto más grande del mundo en términos de emisiones de carbono. En 2019, el gobierno introdujo un impuesto al carbono para tratar de alentar a las empresas a hacer esfuerzos. A pesar del apoyo de las organizaciones ecologistas, esta iniciativa sigue considerándose insuficiente y poco disuasiva. La contaminación atmosférica representaría un coste anual de dos mil millones de euros.

Desde principios del siglo XX, 37 especies de plantas han desaparecido en Sudáfrica, principalmente víctimas de deforestación..

Sudáfrica cuenta con más de 20.000 plantas diferentes, que representan cerca del 10% de todas las especies conocidas del mundo, por lo que es considerada un área particularmente rica en biodiversidad vegetal.

El bioma prevalente en el país es la pradera, especialmente en el Highveld, donde la flora predominante son los pastos, los arbustos bajos y las acacias, principalmente las de espina blanca y camel. La vegetación es más escasa hacia el noroeste, debido a las bajas precipitaciones de lluvia.

WWF distingue cuatro ecorregiones de pradera de montaña dentro del país:
El pasto y los espinos de la sabana dan paso progresivamente a los arbustos de la sabana hacia el noreste del país, con un crecimiento más lento. Existe un número significativo de árboles baobabs en esta área, cerca del extremo norte del parque nacional Kruger.

En el Bushveld se encuentran numerosos hábitats de mamíferos como el león, el leopardo, el ñu azul, el kudu, el impala, la hiena, el hipopótamo y la jirafa. El hábitat del Bushveld se extiende de forma significativa hacia el nordeste, incluyendo los territorios que pertenecen al parque nacional Kruger y la Reserva Mala Mala, así como la Biósfera de Waterberg, más al norte. WWF divide la región de sabana del nordeste en tres ecorregiones, de este a oeste: sabana arbolada de mopane del Zambeze, sabana arbolada de África austral y sabana arbolada del Kalahari.

La región desértica del Karoo, en el oeste del país, se divide en tres ecorregiones: el Karoo suculento, cerca de la costa; el Karoo nama, en el interior (Namaqualand), donde existen varias especies de plantas que almacenan agua, como los aloes y euforbias; y por último, y más al norte, la sabana xerófila del Kalahari.
El bioma mediterráneo del fynbos, uno de los seis reinos florales, está ubicado en una pequeña región del Cabo Occidental y consta de más de 9000 de estas especies, lo que lo convierte en una de las regiones más ricas del mundo en términos de Biodiversidad Floral. La mayor parte de las plantas son perennes y de hojas duras con hojas finas como agujas, como por ejemplo las plantas esclerófilas. Otra planta exclusiva de Sudáfrica es el género de las proteas, de las cuales existen alrededor de 130 especies diferentes en este país. WWF divide esta región en tres ecorregiones: fynbos y renosterveld de tierras bajas, fynbos y renosterveld de montaña y matorral de Albany.

Si bien Sudáfrica cuenta con una gran cantidad de plantas florales, posee pocos bosques. Solo el 1% de Sudáfrica es bosque, que se encuentra casi exclusivamente en el plano costero húmedo del Océano Índico en KwaZulu-Natal: la selva mosaico costera de KwaZulu y El Cabo y la selva mosaico costera de Maputaland y, más al sur, la selva montana de Knysna y los montes Amatole. Existen reservas incluso más pequeñas de bosques que se encuentran fuera del alcance del fuego. Las plantaciones de especies de árboles importados son predominantes, en particular del eucalipto no nativo y el pino. Sudáfrica ha perdido una extensa superficie de hábitat natural en las últimas cuatro décadas, debido a la sobrepoblación, a los patrones descontrolados de desarrollo y a la deforestación del siglo XIX.

Sudáfrica es uno de los países más afectados por la invasión de especies foráneas (por ejemplo la Acacia mearnsii, Port Jackson, Hakea, Lantana y Jacarandá) que son una gran amenaza a la biodiversidad nativa y la actual escasez de recursos hídricos. El bosque templado original que encontraron los primeros europeos que se establecieron en este país, fue explotado despiadadamente hasta que sólo quedaron unas pocas y pequeñas áreas. Actualmente los árboles de maderas nobles en Sudáfrica como el Podocarpus latifolius, el Ocotea bullata y el Olea laurifolia se encuentran bajo protección gubernamental. Finalmente, en la costa del Índico se encuentran varios enclaves de manglar de África austral.

Se espera que el cambio climático conlleve un incremento en el calentamiento y la sequedad de forma considerable para esta región que ya es semiárida, con mayor frecuencia e intensidad de eventos climáticos extremos, como olas de calor, inundaciones y sequías. De acuerdo a los modelos computacionales realizados por el Instituto de Biodiversidad Nacional de Sudáfrica (SANBI, por sus siglas en inglés) (junto con muchas de sus instituciones asociadas), algunas partes de África del sur verán un incremento en su temperatura de cerca de 1 °C a lo largo de la costa, que podrá llegar a superar los 4 °C en las ya calurosas tierras interiores, como en Cabo del Norte a fines de la primavera y el verano del año 2050.

El Reino Floral del Cabo ha sido identificado como uno de los puntos más sensibles de la biodiversidad sudafricana, ya que será seriamente afectado por el cambio climático y cuenta con una enorme diversidad de vida. Las sequías, el aumento de la intensidad y la frecuencia de los incendios y el incremento de la temperatura llevarán a la extinción de muchas de estas especies exóticas. El libro utiliza gran parte del modelo realizado por la SANBI y presenta una recopilación de ensayos de estilo narración de viajes.

Sudáfrica alberga muchas especies animales endémicas, como el conejo ribereño ("Bunolagus monticularis") que se encuentra en peligro crítico de extinción en el Karoo.

La economía de Sudáfrica es la más potente e importante del continente africano, concentra casi el 25 % de todo el PIB de continente, y desempeña un papel importante en el desarrollo de la región. Está considerada como una economía de renta media-alta por el Banco Mundial. La economía sudafricana cuenta con un gran volumen de capital nacional —público y privado— en estrecha relación con las grandes economías mundiales. Pese a esto, el desempleo es extremadamente alto y Sudáfrica está dentro de los diez países con más desigualdad social según el coeficiente de Gini, alrededor de un cuarto de la población está desempleada y la misma proporción vive con menos de 1,25 dólares por día. En 2010, se estimaba que la mano de obra total del país ascendía a 17 390 000 personas. En 2007, el 9 % de los trabajadores se ocupaba en la agricultura, el 26 % en la industria y el 65 % en servicios.Desde 2004 en adelante, el crecimiento económico ha ido en aumento, tanto en el empleo como en la formación de capital. En el año 2011 fue oficialmente nombrado como uno de los integrantes de los países BRICS. Sudáfrica es un destino turístico muy popular, y una cantidad sustancial de los ingresos proviene del turismo.

La moneda nacional es el rand sudafricano. Su código ISO 4217 es ZAR. Esta moneda también es usada en otros países del Área Monetaria Común de África del Sur. La Bolsa de Johannesburgo es la más importante bolsa de valores de África y ocupa el puesto 17º dentro de los mayores mercados bursátiles del mundo. Entre los principales socios comerciales internacionales de Sudáfrica —además de otros países africanos— se encuentran Alemania, los Estados Unidos, China, Japón, el Reino Unido y España.

En 2019, el salario medio de los sudafricanos blancos es 3,5 veces superior al de los sudafricanos negros. El desempleo afecta al 27% de la población

La minería en Sudáfrica ha sido la principal causa por la cual el país está dentro de las grandes economías mundiales. Actualmente es el segundo productor de oro a nivel mundial. Históricamente, la minería en el país comienza con el descubrimiento de diamantes a orillas del río Orange en 1867. Cercana a esa época, la extracción de oro se haría famosa en la región de Witwatersrand, dando origen a la fiebre del oro de 1886. Ciudades como Kimberley, surgieron gracias a la explotación de recursos mineros en la zona norte del país. A partir de 2007, la industria minera de Sudáfrica cuenta con 493 000 trabajadores.

Sudáfrica es el mayor productor de platino del mundo, el quinto de oro y quinto de carbón y uno de los mayores exportadores de diamantes. La producción nacional de diamantes está controlada en un 94 % por la De Beers Consolidated Mines Ltd., que también está presente en otros países de África. También es importante la explotación de recursos como cromo, antimonio, manganeso, níquel, fosfatos, uranio, cobre, vanadio, sal y gas natural.

Después de la Segunda Guerra de los Bóer (1899-1902), se institucionalizó el saqueo de las poblaciones negras. En 1913, la Ley de Tierras Indígenas limitó la propiedad de la tierra de los negros al 7% del territorio (ampliada al 13% en 1936). Cuatro millones de campesinos pierden la tierra que todavía poseen y generalmente se convierten en aparceros o mineros, una mano de obra barata para los propietarios 
En 2007, el trabajo relacionado con la agricultura y la ganadería, ocupaban el 9 % de la mano de obra del país.Cerca de un 80 % de las tierras son utilizadas para la agricultura, pero solo un 15 % de estas son cultivables, el resto es utilizada para pastoreo y ganadería.

La agricultura representa el 8 % de las exportaciones del país. Los principales productos de exportación son los cereales —maíz, trigo— las frutas —manzanas, peras, duraznos, damascos, aguacates, pomelos, mandarinas, ciruelas y uvas de mesa— productos hortícolas, patatas, semillas de girasol, carne —vacuno, pollo, cordero y cerdo — y huevos.

Las condiciones climáticas que presenta Sudáfrica, especialmente el Cabo Occidental, hacen que se produzca el mejor vino del continente africano, de tan buena calidad como el que se hace en Europa, los Estados Unidos, Australia y los países del Cono Sur. El país está dentro de los grandes exportadores de vino del mundo.

En 2018, 30.000 granjas comerciales empleaban a unos 840.000 trabajadores agrícolas. Las condiciones de vida de estos últimos suelen ser difíciles; muchos viven en barrios marginales sin agua corriente. La directora de la asociación para el desarrollo rural, Laurel Oettle, señala que "los temporeros no tienen ingresos desde hace meses. Algunos se pagan a veces con productos agrícolas. Hay muchos casos de abuso sexual. El acceso a las tumbas de los antepasados da lugar a conflictos con los propietarios.

La mayoría de la población está compuesta por negros de origen africano en un 79,6 %, xhosa, zulú, y otros 8 grupos. El porcentaje sin embargo es el más bajo del África subsahariana, la diversidad étnica y la multiculturalidad existentes le ha valido el nombre del país del arcoíris.
El 9,2 % de los sudafricanos son de raza blanca, de origen neerlandés (bóeres), francés (hugonotes) o británico. Un 8,8 % son mestizados llamados "coloured", descendientes de los bóeres y esclavos de origen malayo o africano. Un cuarto grupo es el de los asiáticos (indostaníes en un 91 %) que viven sobre todo alrededor de Durban, representan el 2,4 % de la población.
De los cuatro grupos étnicos, solo la población blanca se está reduciendo debido a la baja tasa de fecundidad y a la emigración de sudafricanos blancos hacia Europa, América del Norte y Oceanía.

Desde la caída del régimen del apartheid en 1994, unos 850 000 sudafricanos blancos (un 16 % del total) han emigrado, sobre todo a Reino Unido y Australia ante el incremento de la inseguridad y de las medidas de discriminación racial contra los blancos. Ante esta situación, desde 2006 el gobierno ha comenzado a tomar medidas incentivas para reducir la emigración de la cualificada población blanca, del mismo modo se han iniciado medidas a favor del retorno de los emigrados.
Durante los últimos años ha habido signos de que algunos de estos emigrantes han empezado a volver a Sudáfrica y según los más recientes informes del instituto de estadística de Sudáfrica (StatsSA) el número de blancos en el país ha crecido por primera vez desde hace muchos años (108 000 entre 2009 y 2010).

La principal religión es el cristianismo, mayoritariamente protestante y evangélico. Pero también se practican cultos tradicionales africanos, en ocasiones mezclados con propuestas cristianas de carácter independiente, el islam y el hinduismo, forman minorías superiores al 1 %. También destaca una comunidad judía de en torno a 70 000 personas. Resultan también relevantes los porcentajes de no religiosos y ateos. El último censo con información religiosa fue en el año 2001, en el que los datos son más detallados.

En 2016 los porcentajes eran:


Sudáfrica es el país del mundo con mayor número de infectados por sida, lo que unido a una baja tasa de natalidad para los estándares africanos (2,16 hijos por mujer), ha hecho que su población haya disminuido durante 2003, según el CIA World Factbook. La difusión del sida (síndrome de la inmunodeficiencia adquirida) es un problema alarmante en Sudáfrica al descubrirse en 2005 que más del 31 % de las mujeres embarazadas estaban infectadas por el VIH y la tasa de infección entre los adultos se situaba próxima al 20 %. El vínculo entre el VIH, un virus de transmisión principalmente por un contacto sexual, y el sida fue largamente rechazado por el antiguo presidente Thabo Mbeki y la entonces ministra de Sanidad Manto Tshabalala-Msimang, quienes insistían en que la mayoría de muertes en el país estaban asociadas a la malnutrición y al elevado nivel de pobreza, y no al VIH.

En 2007, en respuesta a la presión social, el gobierno comenzó a volcar sus esfuerzos en combatir el sida. En septiembre de 2008 Thabo Mbeki fue forzado a dimitir por el congreso de Sudáfrica y Kgalema Motlanthe fue nombrado presidente en funciones. Una de las primeras acciones del Sr. Motlanthe fue reemplazar a la ministra de sanidad Tshabalala-Msimang por la actual ministra, Barbara Hogan.

El sida afecta principalmente a aquellas personas sexualmente activas y es mucho más frecuente en la población negra. La mayoría de personas que mueren son también personas que están trabajando, por lo que el resultado es que en muchas familias se pierde la fuente de ingresos al hogar. Esto ha supuesto así mismo la fundación de muchos 'Orfanatos sida' quienes en muchos casos dependen del estado para su cuidado y soporte económico. Se calcula que hay a día de hoy unos 1,2 millones de huérfanos en Sudáfrica. Muchas de las personas mayores han perdido así mismo el apoyo de los miembros más jóvenes de sus familias. Aproximadamente cinco millones de personas están infectadas por dicho virus infeccioso.

Sudáfrica es el país con más lenguas oficiales del mundo. Reconoce once lenguas como idiomas oficiales, aunque los dos principales son de origen europeo: el inglés usado como vehículo de comunicación entre todos los sudafricanos y el afrikáans derivado del neerlandés, es usado por los bóer y también por los "coloured." Los otros idiomas oficiales son ndebele, sesotho (sotho austral), sotho septentrional, tswana (estos tres idiomas del grupo sotho), swazi, tsonga, venda, xhosa y zulú.

En Sudáfrica no existe una cultura única, sino que hay diversidad de culturas que han enriquecido la música, el arte y la gastronomía sudafricana.
La variedad racial del país es muy grande. A principios del siglo XX el 60 % de la población era de raza negra, el 30 % de raza blanca y el resto en su mayor parte mestizos o surasiáticos. La religión predominante es la cristiana: 55 % de la población protestante, 9 % católica. El resto de habitantes son hindúes, musulmanes o de otras confesiones.

Debido al "apartheid", se ha producido un desarrollo cultural desigual entre los distintos grupos raciales y étnicos, históricamente separados. Entre la población de origen europeo, la cultura inglesa ha emergido últimamente como dominante tras el fin del "apartheid" y del aislamiento internacional. La antigua distinción entre los afrikáneres, más nacionalistas y religiosos, y los anglosajones, más liberales y cosmopolitas, se está borrando entre las generaciones jóvenes y urbanas. En cambio en las zonas rurales los afrikáneres todavía se resisten a abandonar la cultura tradicional, aislada durante siglos de la evolución de Europa.


Sudáfrica tiene 11 de los 18 premios Nobel africanos: Michael Levitt (Química 2013), J. M. Coetzee (Literatura, 2003), Sydney Brenner (Medicina 2002), Frederik Willem de Klerk (Paz, 1993), Nelson Mandela (Paz, 1993), Nadine Gordimer (Literatura, 1991), Desmond Tutu (Paz, 1984), Aaron Klug (Química, 1982), Allan M. Cormack (Medicina, 1979), Albert Lutuli (Paz, 1960) y Max Theiler (Medicina, 1951); Sudáfrica tiene en su haber dos de los cuatro premios Nobel de literatura africanos: Nadine Gordimer y J. M. Coetzee y cinco de los seis premios Nobel «científicos».

Así mismo Sudáfrica cuenta con las dos únicas universidades africanas entre las 400 principales del mundo según el Academic Ranking of World Universities (Shanghai University), entre ellas la Universidad del Ciudad del Cabo (University of Cape Town), la primera de África en le puesto 156 según el QS World University Rankings.

Los artistas sudafricanos debieron acatar la censura que puso el régimen político a su obra. Por ejemplo, la palabra apartheid estaba prohibida y los cantautores no podían ponerla en sus letras. Grandes talentos fueron asfixiados. Otros grandes talentos encontraron gran reconocimiento fuera de sus fronteras, o como mínimo, libertad para cantar las canciones que querían cantar. Gran talento de reconocimiento mundial y en su tierra fue el cantautor Lucky Dube.

En Sudáfrica se editan numerosos periódicos. Entre ellos se cuentan: The Star, The Sowetan y This Day (editados en Johannesburgo), Isolezwe (editado en Ciudad del Cabo) y Daily Sun (editado en Gauteng). Además, cuenta con numerosas televisoras, como la MSNBC Africa, la señal local de MSNBC en territorio africano; además de CNBC Africa, un canal operado conjuntamente con CNBC (una división de NBC Universal) y Africa Business News, con sede en Sandton, Johannesburgo. También se dispone de la televisora pública South African Broadcasting Corporation. La cadena de televisión de deportes SuperSport es la principal de África.

Las divisiones raciales y étnicas se notan incluso en los deportes: los blancos afrikáner son fanáticos del rugby, mientras que los blancos angloparlantes prefieren el críquet. Los negros (mayoría de la población), en cambio, son más aficionados al fútbol, por lo que es el deporte más popular del país. Durante el apartheid, numerosas federaciones deportivas internacionales aplicaron sanciones a las asociaciones sudafricanas, en particular en los tres deportes mencionados.

En el rugby, Sudáfrica es una potencia, de hecho, es considerado el deporte nacional. La selección nacional, conocida como los "Springboks" (Gacelas), ha ganado la Copa Mundial de Rugby tres veces (1995, la 2007) y la más reciente la de 2019, el Torneo de las Tres Naciones tres veces (1998, 2004 y 2009). Los Bulls han logrado tres campeonatos de Super Rugby ante equipos provinciales de Australia y Nueva Zelanda.

En cuanto al fútbol, la FIFA prohibió que la selección participara en torneos oficiales desde 1974 hasta el 7 de junio de 1992. La Premier Soccer League es una de las ligas de clubes más destacadas del continente.

El críquet es otro deporte importante en la vida sudafricana, siendo el país una de las sedes de la Copa Mundial de Críquet en el año de 2003. Asimismo, la selección nacional logró acceder tres veces a la semifinal del torneo en 1992, 1999 y 2007, además de ganar la medalla de oro en los Juegos de la Mancomunidad de 1998.

Los sudafricanos también dejaron sus huellas en el tenis, el exponente más conocido en el deporte blanco es Kevin Anderson, quien fue finalista del Abierto de los Estados Unidos en 2017 y del Campeonato de Wimbledon en 2018, en ambas ocasiones siendo derrotado por los número 1 del ranking en su momento (Rafael Nadal y Novak Djokovic), además de obtener el ATP 500 de Viena en 2018.

El golf es otro deporte que le ha dado grandes triunfos al país. Gary Player está considerado uno de los tres mejores jugadores de la historia. En tanto, Bobby Locke, Ernie Els y Retief Goosen han ganado múltiples torneos mayores, a la vez que Trevor Immelman, Louis Oosthuizen y Charl Schwartzel han ganado uno.

El automovilismo tuvo historia sudafricana, en especial el retirado piloto de Fórmula 1 Jody Scheckter, que logró el título en 1979 con Ferrari, y una de las únicas pilotos femeninas, Desiré Wilson, que triunfó en la Fórmula 1 Británica. También en el calendario de Fórmula 1 figuraba el Gran Premio de Sudáfrica, pero este se dejó de realizar debido al apartheid en 1985. También hubo un campeonato nacional de Fórmula 1 de Sudáfrica disputado entre 1960 y 1975. Luego la F1 volvería en 1992 y 1993 cuando el promotor de la carrera quebró. Actualmente ha habido especulaciones sobre el regreso de la máxima categoría al país.

El Ciclismo es también un deporte que le ha aportado alegrías al país, con el ciclista profesional Daryl Impey uno de los pocos nacionales en ganar etapas en la prueba más prestigiosa del mundo sobre ruedas, el Tour de Francia, además de ser el único que ha logrado ser líder de la "Grande Boucle".

La delegación de Sudáfrica en los Juegos Olímpicos ha conseguido 86 medallas, entre ellas 25 oros. Obtuvo el séptimo puesto en 1912, 11º en 1920 y 12º en 1952. Se ubica en el cuarto puesto histórico en tenis, 11º en boxeo y 14º en natación. La ciudad de Durban fue designada como sede de los Juegos de la Mancomunidad de 2022.

La gran mayoría de las competiciones deportivas nacionales e internacionales se transmiten por la cadena de televisión por suscripción SuperSport, cuya señal se puede captar en la mayor parte del continente.

El día nacional del deporte en Sudáfrica se celebra cada año el 19 de diciembre.

Se celebró en Sudáfrica entre el 25 de mayo y el 24 de junio de 1995. Aquí regresó la selección anfitriona a las competiciones oficiales, tras no competir en 1987 y 1991 por las sanciones impuestas al país debido a su política de apartheid. La selección de Sudáfrica ganó la final.

Se celebró en Sudáfrica, Zimbabue y Kenia en el año 2003, esta fue la primera edición celebrada en África, la selección sudafricana quedó eliminada en primera ronda.

La final se disputó en la ciudad de Johannesburgo, enfrentando a las selecciones de la India y Australia, ganando esta última.

Entre el 11 de junio y 11 de julio Sudáfrica albergó la Copa Mundial de Fútbol de 2010, por ello presentó la candidatura para realizarlo en el 2006, perdiendo por un solo voto con Alemania. En el año 2004 se hizo realidad, cuando el máximo rector del fútbol, la FIFA, escogió a Sudáfrica como sede para 2010.






</doc>
