<doc id="11449" url="https://es.wikipedia.org/wiki?curid=11449" title="Antillas">
Antillas

Antillas(; en criollo haitiano "Zantiy"; ; en papiamento "Antias"; ; en patois jamaiquino "Antiliiz"), forma parte de la América Insular, constituyen un numeroso grupo de archipiélagos, conformado por las islas: Antillas Mayores y Antillas Menores , ubicadas en el mar Caribe, océano Atlántico. Estas islas dibujan un arco que se extiende en forma de medialuna desde el sureste de la península de la Florida (Estados Unidos), al sur del Archipiélago de las Lucayas, al noreste de la península de Yucatán (México), en Norteamérica, hasta la costa oriental de Venezuela, en Suramérica. Todas las islas de las Antillas juntas, tienen una superficie total de unos 299 000 km².

El nombre hace referencia a la mítica isla de Antilia.

Los idiomas predominantes en la región son el español (hablado por casi 25 millones de personas y predominante en las Grandes Antillas, se habla en Cuba, República Dominicana y Puerto Rico y minoritario en las Pequeñas Antillas (Islas de Venezuela)), el francés y el criollo haitiano (en total, 9 millones de hablantes en Haití y otras islas), el inglés (hablado en Jamaica, en las Bahamas y en la mayor parte de las Pequeñas Antillas, en total unos 5 millones) y, en menor medida, el neerlandés y el papiamento (unas 300 000 personas en conjunto).

Estas islas pertenecieron inicialmente en su totalidad a España, quien fue la potencia dominante en las Grandes Antillas; sin embargo, su escaso interés en conservarlas, especialmente las Antillas Menores, motivó que estas últimas pudieran ser conquistadas sin mayores problemas por británicos, franceses y neerlandeses. Fueron plazas que, posteriormente, estos utilizaron como punto de partida para otras conquistas, lo que ha dado lugar al rico mosaico actual de nacionalidades, lenguas y culturas.

Varias de las islas son independientes, pero muchas siguen siendo posesiones o dependencias de otros países. Algunas, como Guadalupe y Martinica, ambas regiones de Francia, forman parte del territorio nacional de países en otros continentes.

El turismo domina la economía de Antigua y Barbuda, produciendo casi el 60 % del PIB y el 40 % de las inversiones. La disminución de turistas desde el año 2000 obligó al gobierno a transformar el país en un paraíso fiscal. La producción agrícola está centrada en el ámbito doméstico y limitada por el reducido suministro de agua y una disminución de la mano de obra debido a los mejores salarios en los sectores de turismo y construcción.

Es importante también la producción agrícola de caña de azúcar, algodón y frutas; así como el refino de petróleo y las manufacturas textiles, de carpintería y de producción de ron. Produce algo de cerveza, ropas, cemento, artesanías locales y muebles.

La moneda oficial es el dólar del Caribe Oriental (East Caribean Dollar), con una paridad fija de 2,7:1 con el dólar estadounidense desde el año 1976. El producto bruto interno fue de 180 100 dólares "per capita" en 2009, año de una retracción del 6,5 % del PIB. La tasa de inflación anual es muy baja (1,5 % en 2007).

Las Bahamas es un país en desarrollo estable, dependiente de la economía basada en el turismo y actividades bancarias. El turismo solamente supone más del 60 % del PIB y emplea directa o indirectamente a la mitad de la mano de obra del archipiélago. El crecimiento constante del turismo y el auge en la construcción de hoteles, de recursos y de nuevas residencias han conducido al crecimiento sólido del PIB durante algunos años hasta el 2006, aunque desde dicho año se produjo una caída en el número de turistas.

Los servicios financieros constituyen el segundo sector en importancia de la economía de Bahamas, cerca del 15 % del PIB. Sin embargo, desde diciembre de 2000, cuando el gobierno decretó nuevas regulaciones sobre el sector financiero, muchos negocios internacionales han salido de las Bahamas. La industria y la agricultura, contribuyen aproximadamente una décima parte del PIB y muestran poco crecimiento, a pesar de los incentivos que el gobierno destino a estos sectores. En suma, el crecimiento depende del funcionamiento del sector del turismo, que depende del crecimiento en los EE. UU., la fuente de más del 80 % de los visitantes. Además del turismo y de las actividades bancarias, el gobierno apoya el desarrollo de un "tercer pilar", el comercio.

Barbados es el país más rico y más desarrollado del Caribe Oriental y tiene una de las rentas per cápita más altas de América. Su economía tradicional se basaba en la producción de azúcar, principal materia de exportación. Con la explosión del turismo, se produjo una reorientación de la actividad. Ahora mantiene un sistema muy dependiente de Estados Unidos y Europa, que son los lugares de procedencia de la mayoría de los turistas, lo que debilita su economía en los periodos de contención en los países de origen. En la actualidad ha diversificado parcialmente su economía con algo de industria ligera. Es así mismo sede de importantes empresas, sobre todo financieras, dado el alto nivel de protección del secreto bancario que ofrece y los bajos impuestos que soportan. Por la comunidad internacional es considerado un paraíso fiscal.

La economía cubana está sustentada en los recursos naturales del país, que son muy variados y van desde minerales como el níquel y el cobalto a los paisajes tropicales que atraen a millones de turistas todos los años. El capital humano es el otro pilar fundamental de la economía del país, que cuenta con las tasas más elevadas de alfabetización, esperanza de vida y cobertura sanitaria de toda la América Latina y el Caribe.

El gobierno cubano mantiene su adhesión a los principios socialistas a la hora de organizar su economía, lo que ha llevado a que su política económica se base en la planificación, con opciones diferentes y cerradas a las que serían dictadas por el mercado; aunque después del derrumbe de la URSS y de los países socialistas del este de Europa, la iniciativa privada y el papel del mercado hayan aumentado, aunque no al nivel de lo sucedido en la Europa del Este.

Por otro lado, y según datos de la ONU, Cuba sería el único país del mundo que cumple los dos criterios que, para la organización WWF, significan la existencia del desarrollo sostenible: desarrollo humano alto (IDH > 0,8) y huella ecológica sostenible (huella < 1'8 ha/p). Según el informe EPI de 2010, realizado por las universidades de Yale y Columbia en Estados Unidos el país está en la posición 9.ª en el mundo con mejor desempeño ambiental.

La economía de Dominica depende principalmente de los servicios financieros offshore. El crecimiento de su industria de servicios financieros offshore deriva de un proceso gubernamental en donde se han realizado cambios estructurales con la finalidad de diversificar sus fuentes de ingreso, el gobierno busca promover activamente la isla como centro bancario internacional, y recientemente firmó un acuerdo con la Unión Europea con la finalidad de explorar sus potenciales de energía geotérmica.

Aunque anteriormente dependía en gran medida de la agricultura - especialmente banana - sus medios de ingreso se han diversificado. Su segunda fuente de ingreso es el turismo, en especial el ecoturismo.

El 2003 el gobierno comenzó una extensa reestructuración de la economía, con la eliminación del control de precios, privatización del sector bananero, y aumento de impuestos, con miras a enfrentar una crisis económica y atender las recomendaciones del Fondo Monetario Internacional. Esta reestructuración permitió la recuperación económica - en 2006 el crecimiento ultrapasó los dos dígitos - y ayudó a reducir la deuda pública.

El progreso económico de Granada, debido a las reformas fiscales y una macroeconomía prudente, ha disparado el crecimiento anual del país al 5-6 % en 1998-1999. El incremento de la actividad económica ha estado liderado por la construcción y el comercio. Actualmente el país depende del turismo como su principal fuente de ingresos de capital extranjero, especialmente después de la inauguración de su aeropuerto internacional el 1985. Las instalaciones turísticas se han aumentado desde entonces.

Los huracanes Iván (2004) y Emily (2005) han severamente damnificado su sector agrícola - especialmente el cultivo de cacao. Después de la devastación, el país confronta un enorme déficit presupuestario, ampliado durante el proceso de reconstrucción, y que llega actualmente a 110 % del PIB.

La economía de Haití es la más pobre de América y del Hemisferio Occidental, es decir, Haití es el país con menor PIB per cápita y uno de los más desiguales del mundo. Su renta per cápita es alrededor de una décima parte de la de sus vecinos de la región del Caribe. Tiene una tasa de desempleo superior al 50% de su población, sus ingresos anuales per cápita son menores al salario mínimo de otros países latinoamericanos y la pobreza extrema alcanza casi el 70 % de la población.

La economía de Kingston es principalmente agrícola y minera, aunque desde la década de los 90 del siglo XX tomó impulso especial el turismo. El crecimiento sostenido desde las reformas de 1982 ha permitido alcanzar un PIB de 7400 millones de dólares. La agricultura emplea a más de un 20 % de la población, siendo el azúcar el principal producto. Esto supone una dependencia excesiva del precio del azúcar en los mercados internacionales. Además, se cultivan plátanos, café y tabaco, que en buena medida se dirigen a la exportación, además de los productos para el consumo interno, como la patata y el maíz. La cabaña ganadera asciende a más de 800 000 cabezas entre ganado vacuno y caprino, siendo el cerdo residual con apenas 150 000 cabezas.

La alúmina y la bauxita constituyen la espina dorsal de la minería desde su descubrimiento en 1940, cuya producción íntegra se destina a la exportación. La industria ha alcanzado cierto nivel de importancia, sobre todo la manufacturera (textil y calzado) y las de refino petrolífero.

A partir del 2000 Jamaica empezó a experimentar crecimientos positivos de su economía tras un periodo de cuatro años de crisis. La inflación terminó por controlarse a niveles aceptables en 2001, si bien en 2003 y 2004 ha vuelto a repuntar hasta niveles preocupantes.

La economía de la República Dominicana es la primera economía del Caribe y la octava economía más grande de América Latina después de Brasil, México, Argentina, Colombia, Chile, Perú y Ecuador. Es un país en desarrollo de ingresos altos según el Banco Mundial, dependiendo, principalmente, de la agricultura, el comercio exterior, los servicios, la minería, la industria y el turismo. Aunque el sector servicios ha sobrepasado a la agricultura como el principal proveedor de empleos debido, sobre todo, al auge y crecimiento del turismo y la industria, la agricultura todavía se mantiene como el sector más importante en términos de consumo doméstico y está en segundo lugar (detrás de la minería) en términos de exportación. El turismo aporta más de US$ 7000 millones al año. La industria y el turismo son los sectores de mayor crecimiento. Las remesas de los ciudadanos dominicanos viviendo en el exterior se estiman en unos US$ 5500 millones por año.

San Cristóbal y Nieves fue el último lugar en practicar el monocultivo de azúcar en las Antillas Menores. Pero debido a que la industria azucarera encontraba cada vez mayores dificultades para conseguir beneficios, el gobierno decidió realizar un programa de diversificación para el sector agricultor y estimulación del desarrollo en otros sectores de la economía, particularmente el turismo.

El gobierno instituyó un programa de incentivos a la inversión, alentando tanto la inversión privada doméstica como extranjera. Las políticas gubernamentales incluían exenciones fiscales, importación de equipamiento y materiales libres de impuestos y subsidios para la capacitación a personal local. El turismo ha mostrado un gran incremento. Hacia 1987, había sobrepasado al azúcar como fuente de ingreso de divisas.

La economía de San Vicente depende en una gran medida de la agricultura. El cultivo de la banana representa un 60% del empleo y un 50 % de las exportaciones. Ésta muy fuerte dependencia de un solo cultivo hace que la economía sea vulnerable a múltiples factores externos. Los agricultores de banana de San Vicente poseen acceso preferencial al mercado europeo. Dado que la Unión Europea ha anunciado que dicho acceso preferencial será discontinuado, es que la diversificación de la actividad económica se torna una prioridad para San Vicente.

El turismo ha crecido, convirtiéndose en un elemento importante de la actividad económica. En 1993, el turismo desplazó a las exportaciones de banana como el principal elemento generador de ingreso de divisas. Las Granadinas se han convertido en un mercado favorito de los fanáticos con altos niveles de ingreso que practican el yachting. La tendencia de crecimiento del turismo es muy probable continúe. En 1996, se inauguraron nuevos atracaderos y amarraderos para cruceros y buques, con el consecuente aumento de la cantidad de pasajeros arribados. En 1998, arribaron un total de 202,109 visitantes, y la mayoría de los turistas provinieron de otros países del Caribe y el Reino Unido.

La economía del país depende en gran parte del cultivo de plátanos. Sin embargo, los cambios en el régimen de importaciones de la Unión Europea y la creciente competencia de los productores de América Latina han forzado la diversificación. En años recientes la industria del turismo y las finanzas internacionales han adquirido un papel preponderante en la composición de su Producto Interno Bruto y ahora casi el 73 % del mismo es generado por la industria de servicios (2002). Su sector de manufactura, aunque menos importante, es uno de los más diversificados del Caribe Oriental.

Los principales productos de exportación de la isla son el plátano y algunos productos textiles que vende al Reino Unido y a los Estados Unidos por un monto cercano a los USD $30 millones, casi la mitad de sus exportaciones totales. Debido a las condiciones geográficas y demográficas de la isla, gran parte de sus insumos son importados, siendo sus principales proveedores Brasil (41,7 %), Estados Unidos (21,4 %) y Trinidad y Tobago (11,9 %).

La isla fue uno de los países fundadores de la Organización Mundial del Comercio y se integró al Fondo Monetario Internacional el 15 de noviembre de 1979. Su moneda, el dólar del Caribe Oriental, es la moneda de curso legal en otros seis países. Santa Lucía ha firmado algunos convenios de libre comercio y cooperación económica con la Organización de Estados del Caribe Oriental y con la Comunidad del Caribe. En materia técnica recibe asesoría por parte de la Mancomunidad Británica de Naciones y de la CEPAL y también recibe apoyos económicos del Banco de Desarrollo del Caribe.

El año fiscal en la isla comienza el 1 de abril y finaliza el 31 de marzo del año siguiente.

La economía de Trinidad y Tobago experimentó, durante el año 2002, un índice de crecimiento del 3,2 %. Esto se debe a 9 años consecutivos de verdadero crecimiento después de 8 años de recesión. El gobierno del primer ministro Patrick Manning ha seguido la política macroeconómica del gobierno anterior, tratando de atraer las inversiones en el país. A largo plazo, parece que va a producirse un gran crecimiento, un crecimiento que irá estrechamente ligado al desarrollo de los hidrocarburos, la petroquímica, y el sector siderúrgico, que supondrá aumentos significativos en las exportaciones de Trinidad y Tobago. Además, el país sigue sus esfuerzos en la diversificación de servicios, el turismo, la industria y la agricultura.

Así pues, el gran índice de crecimiento de Trinidad y Tobago ha producido excedentes que son exportados, aún sin dejar de importar, ya que la extensión industrial y el aumento de consumo así lo requieren. Por consiguiente, el ratio de deudas ha pasado de un 15,4 % en 1997 a un 4,4 % en 2002. El paro disminuye lentamente: ha pasado de un 12,1 % en 2001 a un 10,4 % en 2002.

La actividad agrícola más importante es el cultivo de la caña de azúcar, al que se asocia la producción de azúcar en los seis ingenios del oeste de la isla, así como de mieles y de ron. Le siguen en importancia el cacao, el grano y su beneficio, las frutas cítricas y el café. La ganadería es poco importante: 65 000 cabezas de ganado bovino, 6000 de ovino, etc.




</doc>
<doc id="11450" url="https://es.wikipedia.org/wiki?curid=11450" title="Archipiélago">
Archipiélago

Un archipiélago es aquel conjunto, generalmente numeroso, de islas agrupadas en una superficie más o menos extensa del mar. Estas islas se encuentran cercanas entre sí y pueden tener su origen de diferentes maneras, y por eso hay varios tipos.

Estas islas son segmentos de territorios fértiles rodeados totalmente por mar, y juntos forman un archipiélago. Además de islas, los archipiélagos pueden contener otras masas de tierra menores como islotes, arrecifes y cayos.

La palabra viene del griego άρχι (quiere decir, "superior") y πέλαγος ("mar").Antiguamente la palabra "archipielgus significaba "Mar principal" y hacía referencia al Mar Egeo porque estaba lleno de islas." En italiano, comenzó siendo el nombre propio para el mar Egeo desde 1268 y más adelante, el uso pasó a referirse a las islas del mar Egeo. Desde ese momento, se utiliza para referirse a cualquier grupo de islas o, a veces, a un mar que contiene un pequeño número de islas dispersas.

Los archipiélagos más frecuentes son aquellos de naturaleza volcánica asociados a grandes erupciones de magma pero también pueden ser resultado de erosión o deposición de tierra.

Dependiendo del origen geológico, las islas que forman los archipiélagos se pueden considerar como islas oceánicas o islas continentales, y por consiguiente, los archipiélagos pueden ser oceánicos o continentales.

Los archipiélagos oceánicos están formados por islas que no pertenecen a una placa tectónica continental y son principalmente de origen volcánico. La formación de estos archipiélagos es mucho más rápida que la formación de archipiélagos continentales.

El archipiélago de Hawái es el claro ejemplo de este tipo de archipiélago.
La masa de tierra que forman las islas continentales pertenecen a fragmentos continentales que se han separado de la masa principal del continente mediante el movimiento de las placas téctonicas y otros fenómenos geológicos. A diferencia de los archipiélagos oceánicos, su formación es muy lenta.

Islas Baleares, Groenlandia o las Islas Británicas son ejemplos de archipiélagos continentales.

La gran mayoría de los archipiélagos más conocidos se sitúan en el sudeste asiático, a este grupo podríamos añadir las Islas Hawái (pertenece a Estados Unidos) y las Islas Canarias (pertenecen España). El archipiélago Malay es considerado uno de los más grandes del mundo, siendo este compuesto por más de 25.000 islas. A continuación, una lista de los archipiélagos más conocidos. No todos están incluidos en esta lista, debido a la dificultad de determinar todos los existentes en el mundo.



</doc>
<doc id="11451" url="https://es.wikipedia.org/wiki?curid=11451" title="Mar">
Mar

El mar —considerado de forma genérica, como el conjunto de los mares y océanos—, también llamado océano mundial o simplemente el océano, es el cuerpo de agua salada interconectada que cubre más del 70% de la superficie de la Tierra (, con un volumen total de aproximadamente ). Modera el clima del planeta y tiene papeles importantes en los ciclos del agua, del carbono y del nitrógeno. Se ha viajado y explorado desde la antigüedad, mientras que el estudio científico del mar, la oceanografía, se remonta a los viajes del capitán James Cook para explorar el océano Pacífico entre 1768 y 1779. 

La palabra «mar» también se usa para indicar secciones más pequeñas del océano, en parte interiores, y para algunos grandes lagos salados, totalmente interiores, como el mar Caspio, el mar Muerto o el mar de Aral. Se habla entonces de mar cerrado o interior, aunque el término correcto sería el de lago endorreico.

La salinidad varía ampliamente, siendo más baja cerca de la superficie y en las desembocaduras de los grandes ríos y más alta en las profundidades del océano; sin embargo, las proporciones relativas de sales disueltas varían poco en los océanos. El sólido disuelto en agua de mar más abundante es el cloruro de sodio. El agua también contiene sales de magnesio, calcio y potasio, entre muchos otros elementos, algunos en concentraciones mínimas. Los vientos que soplan sobre la superficie del mar producen olas que se rompen cuando entran en aguas poco profundas. Los vientos también crean corrientes superficiales a través de la fricción, estableciendo circulaciones de agua lentas pero estables a través de los océanos. Las direcciones de la circulación se rigen por factores que incluyen las formas de los continentes y la rotación de la Tierra (el efecto Coriolis). Las corrientes de aguas profundas, conocidas como la cinta transportadora global, transportan agua fría desde cerca de los polos a todos los océanos. Las mareas, generalmente el aumento y la caída del nivel del mar dos veces al día, son causadas por la rotación de la Tierra y los efectos gravitacionales de la Luna y, en menor medida, del Sol. Las mareas pueden tener un rango muy alto en bahías o estuarios. Los terremotos submarinos que surgen de los movimientos de las placas tectónicas debajo de los océanos pueden provocar tsunamis destructivos, al igual que los volcanes, los grandes deslizamientos de tierra o el impacto de grandes meteoritos.

Una gran variedad de organismos, incluyendo bacterias, protistas, algas, plantas, hongos y animales, viven en el mar, que ofrece una amplia gama de hábitats y ecosistemas marinos, que se extienden, verticalmente, desde la superficie iluminada por el Sol y la costa hasta las grandes profundidades y presiones de la fría y oscura zona abisal, y latitudinalmente desde las frías aguas bajo los casquetes polares hasta la colorida diversidad de los arrecifes de coral en las regiones tropicales. Muchos de los principales grupos de organismos evolucionaron en el mar y la vida pudo haber comenzado allí.

El mar proporciona suministros sustanciales de alimentos para los humanos, principalmente peces, pero también mariscos, mamíferos y algas, ya sea capturados por pescadores o cultivados bajo el agua. Otros usos humanos del mar incluyen el comercio, los viajes, la extracción de minerales, la generación de energía, la guerra y actividades de ocio como la natación, la vela y el buceo. Muchas de estas actividades crean contaminación marina.

El mar es importante en la cultura humana, con importantes apariciones en la literatura al menos desde "La Odisea" de Homero, en el arte marino, en el cine, en el teatro y en la música clásica. Simbólicamente, el mar aparece como monstruos como Scylla en la mitología y representa a la mente inconsciente en la interpretación de los sueños.
Ateniéndose al uso que de la palabra se hace en español, cabe observar que la gente de mar y los poetas tienden a atribuirle el género femenino (la mar). Fuera de esos dos ámbitos, se ha generalizado el uso masculino de la palabra («el mar»).

El día internacional del mar es el 8 de junio y el Día Marítimo Mundial es el 26 de septiembre. En 2008, la Comisión Europea propuso la fecha del 20 de mayo para celebrar el mar en Europa, con el fin de promover la cultura y el patrimonio marítimo. Ese día podrá resultar en operaciones de «puertas abiertas» (puertos abiertos), acciones ambientales que involucren a museos y acuarios, conferencias, etc. La Comisión organiza cada año un Día Marítimo Europeo (, DME) en una ciudad diferente.

El mar es el sistema interconectado de todas las aguas oceánicas de la Tierra, incluidos los océanos Atlántico, Pacífico, Índico, Meridional y Ártico. Sin embargo, la palabra «mar» también puede usarse para designar ciertos cuerpos de agua específicos, mucho más pequeños, como el mar del Norte o el mar Rojo. Los océanos serían las mayores extensiones y vendrían luego, de diferentes tamaños, los mares. No existe una distinción clara entre mares y océanos, aunque en general los mares son más pequeños y a menudo están en parte (como los mares marginales) o totalmente (como los mares interiores) bordeados por tierra. Sin embargo, el mar de los Sargazos no tiene costas y se encuentra dentro de una corriente circular, el giro del Atlántico Norte. Los mares generalmente son más grandes que los lagos y contienen agua salada, pero el mar de Galilea es un lago de agua dulce.
La distinción entre mar y océano obedece a diversas causas, sobre todo cuando se habla de mares abiertos en que suele distinguirse atendiendo a la situación geográfica, generalmente enclavada entre dos masas terrestres o, a veces, las menos, a la posición de la plataforma continental. Algunos ejemplos de esto son: el mar del canal de la Mancha, que comunica con el océano Atlántico por el mar Céltico, pero que se distingue por su posición entre la costa sur de Inglaterra y la costa norte de Francia; el mar Mediterráneo, que comunica con el océano Atlántico por el estrecho de Gibraltar,y se distingue claramente por estar enclavado entre Europa, Asia y África, al punto de que tiene unas condiciones marinas propias (diferentes temperaturas, diferente fauna y flora, y mareas de diferente amplitud). Otro mar abierto, en este caso el de los Sargazos, con su acumulación de algas a lo largo de la Florida, se distingue del océano Atlántico de forma totalmente arbitraria.

La Convención de las Naciones Unidas sobre el Derecho del Mar establece que todo el océano es «mar».

Atendiendo al contacto con el océano, los mares se consideran de tres clases principales: mares litorales (o costeros), mares continentales y los mares cerrados.




La máxima autoridad internacional en materia de delimitación de mares, a efectos de regular el tráfico y segurídad marítima, es la Organización Hidrográfica Internacional (IHO/OHI), siendo la referencia mundial su publicación "Limits of oceans and seas" ("Límites de océanos y mares") (3.ª edición de 1953).
Dicha publicación no establece diferencia alguna entre océanos y mares, sino que se limita a enumerar todos los océanos y mares del mundo, asignándoles un número, llegando hasta el 66, aunque como utiliza a veces números con letra, en realidad son 73. Son un total de 5 los océanos (el Atlántico y el Pacífico están divididos cada uno en dos, Norte y Sur) y 67 los mares, de ellos, dos divididos a su vez en dos cuencas: el mar Mediterráneo y el mar de China. Además algunos mares tienen mares interiores (que se numeran con una letra minúscula) como el Báltico (con 3), el Mediterráneo (con 8) y los mares del archipiélago de la India Oriental (con 13). La publicación considera además algunas grandes extensiones de agua salada, como golfos, bahías, canales y estrechos, y muchas veces, no resulta muy claro cuál es el criterio utilizado, ya que a veces es el simple uso de esos accidentes desde tiempos pasados.

Aunque la mencionada publicación del IHO no considera los mares incluidos en los océanos —sino como algo aparte de modo que entre todos cubren toda la superficie marina— habitualmente siempre se han considerado así, obedeciendo a una consideración de ámbito más geográfico. A veces en algunos mares situados en los bordes entre dos océanos, hay discrepancia entre asignarlos a uno u otro, y depende de la publicación consultada. Por eso parece más oportuno clasificarlos de acuerdo al continente al que bañan, con las mismas salvedades en cuanto a la situación de borde. Se muestran primero los mares que se nombran como tales y luego los cuerpos marinos (golfos, bahías, estrechos...); entre paréntesis se recoge el número asignado por la IHO: los que no lo tienen no aparecen en su publicación, formando parte de otros más amplios.

Los "mares lunares" son vastas planicies basálticas en la Luna que fueron llamadas "mares" porque los primeros astrónomos pensaban que eran grandes masas de agua, por lo que se refirieron a ellas como mares.

Se estima que hay agua líquida sobre la superficie de muchos satélites naturales, como en Europa, una luna de Júpiter. También se piensa que hay hidrocarburos en estado líquido en la superficie de Titán, aunque han de ser considerados más bien «lagos» que «mares». La distribución de esas regiones líquidas será mejor comprendida después de la llegada de la sonda espacial Cassini-Huygens.

La Tierra es el único planeta conocido con mares de agua líquida en su superficie, aunque Marte tiene capas de hielo y varios planetas análogos en otros sistemas solares pueden tener océanos. Todavía no está claro de dónde proviene el agua de la Tierra, pero, visto desde el espacio, nuestro planeta aparece como una «canica azul» con sus diversas formas: océanos, casquetes polares, nubes. Los de mar de la Tierra contienen aproximadamente el 97.2% de su agua conocida y cubren más del 70% de su superficie. Otro 2.15% del agua de la Tierra está congelada, y se encuentra en el hielo marino que cubre el océano Ártico, en la capa de hielo que cubre la Antártida y sus mares adyacentes, y en varios glaciares y depósitos superficiales por todo el mundo. El resto (alrededor del 0,65% del total) forma depósitos subterráneos o varias etapas del ciclo del agua, que contiene el agua dulce encontrada y utilizada por la mayoría de la vida terrestre: vapor en el aire, las nubes que se forman lentamente, la lluvia que cae de ellas, y los lagos y ríos que se formaron espontáneamente a medida que sus aguas fluían una y otra vez hacia el mar. El dominio del mar sobre el planeta es tal que el autor británico Arthur C. Clarke señaló una vez que la «Tierra» habría sido mejor llamada «Océano». 

El estudio científico del agua y del ciclo del agua de la Tierra es la hidrología; la hidrodinámica estudia la física del agua en movimiento. El estudio del mar en particular, más reciente, corresponde a la oceanografía. Esta comenzó como el estudio de la forma de las corrientes oceánicas pero desde entonces se ha expandido y ahora tiene un campo mayor y multidisciplinar: examina las propiedades del agua de mar; estudia las olas, las mareas y las corrientes; traza las líneas de costa y cartografía los fondos marinos; y estudia la vida marina. El subcampo que se ocupa del movimiento del mar, de sus fuerzas y de las fuerzas que actúan sobre él se conoce como oceanografía física. La biología marina (oceanografía biológica) estudia las plantas, los animales y otros organismos que habitan en los ecosistemas marinos. Ambas están informadas por la oceanografía química, que estudia el comportamiento de los elementos y moléculas dentro de los océanos: particularmente, en este momento, el papel del océano en el ciclo del carbono y el papel del dióxido de carbono en la creciente acidificación del agua de mar. La geografía marina y marítima traza la forma y formación del mar, mientras que la geología marina (oceanografía geológica) ha proporcionado evidencias de la deriva continental y de la composición y estructura de la Tierra, ha aclarado el proceso de sedimentación y ha ayudado al estudio del volcanismo y de los terremotos.

Se pensaba que el agua en el mar provenía de los volcanes de la Tierra, un proceso que habría comenzado hace en el que el agua era liberada por la desgasificación de la roca fundida. Un trabajo más reciente sugiere que gran parte del agua de la Tierra puede provenir de cometas. Una característica del agua de mar es que es salada. La salinidad generalmente se mide en partes por mil (‰), y el océano abierto tiene aproximadamente de sólidos por litro, una salinidad del . El mar Mediterráneo es ligeramente más alto, con , mientras que la salinidad del norte del mar Rojo puede llegar hasta el . Los componentes de la sal de mesa, sodio y cloruro, constituyen aproximadamente el 85% de los sólidos en solución, también hay otros iones metálicos como el magnesio y el calcio y los iones negativos, incluidos el sulfato, el carbonato y el bromuro. A pesar de las variaciones en los niveles de salinidad en diferentes mares, la composición relativa de las sales disueltas es estable en todos los océanos del mundo. El agua de mar es demasiado salina para que los humanos la ingieran de manera segura, ya que los riñones no pueden excretar orina tan salada como el agua de mar. En contraste, algunos lagos hipersalinos interiores tienen una salinidad mucho mayor, por ejemplo, el mar Muerto tiene de sólidos disueltos por litro ().

Aunque la cantidad de sal en el océano permanece relativamente constante dentro de la escala de millones de años, varios factores afectan a la salinidad de un cuerpo de agua. La evaporación y el subproducto de la formación de hielo (conocido como "«rechazo de salmuera»") aumentan la salinidad, mientras que la precipitación, el derretimiento del hielo marino y la escorrentía de la tierra lo reducen.. El mar Báltico, por ejemplo, es un mar poco profundo que tiene muchos ríos que fluyen hacia él, por lo que el mar podría considerarse salobre. Mientras tanto, el mar Rojo es muy salado debido a su alta tasa de evaporación.

La temperatura del mar depende de la cantidad de radiación solar que cae sobre su superficie. En los trópicos, con el sol casi por encima, la temperatura de las capas superficiales puede elevarse a más de , mientras que cerca de los polos la temperatura, en equilibrio con el hielo marino, es de aproximadamente . Hay una circulación continua de agua en los océanos. Las corrientes cálidas superficiales se enfrían a medida que se alejan de los trópicos, y el agua se vuelve más densa y se hunde. El agua fría retrocede hacia el ecuador como una corriente de aguas profundas, impulsada por los cambios en la temperatura y en la densidad del agua, antes de volver a salir finalmente hacia la superficie. El agua del mar profundo tiene una temperatura entre y en todas partes del globo.
El agua de mar con una salinidad típica del tiene un punto de congelación de aproximadamente . Cuando su temperatura baja lo suficiente, se forman cristales de hielo en la superficie. Estos se rompen en pequeñas piezas y coalescen en discos planos que forman una suspensión gruesa conocida como frazil. En condiciones marinas tranquilas, este se congela en una delgada lámina plana conocida como nilas, que se espesa a medida que se forma hielo nuevo en su parte inferior. En mares más turbulentos, los cristales de frazil se unen en discos planos conocidos como panqueques. Estos se deslizan uno debajo del otro y coalencen para formar témpanos. En el proceso de congelación, el agua salada y el aire quedan atrapados entre los cristales de hielo. Las nilas puede tener una salinidad de , pero para cuando el hielo marino tiene un año, este cae a .

La cantidad de oxígeno que se encuentra en el agua de mar depende principalmente de las plantas que crecen en él. Estas son principalmente algas, incluido el fitoplancton, con algunas plantas vasculares como los pastos marinos. A la luz del día, la actividad fotosintética de estas plantas produce oxígeno, que se disuelve en el agua de mar y es utilizado por los animales marinos. Por la noche, la fotosíntesis se detiene y la cantidad de oxígeno disuelto disminuye. En las profundidades del mar, donde la luz penetra de forma insuficiente para que crezcan las plantas, hay muy poco oxígeno disuelto. En su ausencia, el material orgánico se descompone por bacterias anaeróbicas que producen sulfuro de hidrógeno. Es probable que el calentamiento global reduzca los niveles de oxígeno en las aguas superficiales, ya que la solubilidad del oxígeno en el agua cae a temperaturas más altas.. La cantidad de luz que penetra en el mar depende del ángulo del sol, de las condiciones climáticas y de la turbidez del agua. Mucha luz se refleja en la superficie, y la luz roja se absorbe en los primeros metros. La luz amarilla y verde alcanza mayores profundidades, y la luz azul y violeta puede penetrar hasta . No hay suficiente luz para la fotosíntesis y el crecimiento de las plantas más allá de una profundidad de aproximadamente.

El viento que sopla sobre la superficie de un cuerpo de agua forma olas que son perpendiculares a la dirección del viento. La fricción entre el aire y el agua causada por una suave brisa en un estanque hace que se formen ondas. Un fuerte golpe sobre el océano provoca olas más grandes cuando el aire en movimiento empuja contra las crestas elevadas del agua. Las olas alcanzan su altura máxima cuando la velocidad a la que viajan casi coincide con la velocidad del viento. En aguas abiertas, cuando el viento sopla continuamente, como sucede en el hemisferio sur en los Rugientes Cuarentas, largas y organizadas masas de agua llamadas oleaje cruzan el océano. Si el viento disminuye, la formación de olas se reduce, pero las olas ya formadas continúan viajando en su dirección original hasta que se encuentran con la tierra. El tamaño de las olas depende del fetch, de la distancia que el viento ha soplado sobre el agua y de la fuerza y ​​duración de ese viento. Cuando las olas se encuentran con otras que provienen de diferentes direcciones, la interferencia entre ambas puede producir mares rotos e irregulares. La interferencia constructiva puede causar olas individuales vagabundas o gigantes (inesperadas) mucho más altas de lo normal. La mayoría de esas olas tienen menos de de altura aunque no es inusual que en fuertes tormentas dupliquen o tripliquen esa altura; la construcción en alta mar, como en los parques eólicos y las plataformas petrolíferas, utilizan estadísticas meteocéanicas (abreviatura silábica de meteorología y oceanografía) a partir de mediciones para calcular las fuerzas de esas olas (debido, por ejemplo, a la ola de cien años) contra las que están diseñadas. Sin embargo, se han documentado olas vagabundas de alturas superiores a los .

La parte superior de una ola se conoce como la cresta, el punto más bajo entre las olas es el valle y la distancia entre las crestas es la longitud de onda. El viento empuja la ola a través de la superficie del mar, pero esto representa una transferencia de energía y no un movimiento horizontal del agua. A medida que las olas se acercan a la tierra y se mueven en aguas poco profundas, cambian su comportamiento. Si se acerca en ángulo, las olas pueden doblarse (refracción) o envolver rocas y promontorios (difracción). Cuando la ola alcanza un punto donde sus oscilaciones más profundas del agua entran en contacto con el fondo marino, comienzan a disminuir. Esto atrae las crestas más juntas y aumenta la altura de las olas, lo que se conoce como asomeramiento. Cuando la relación entre la altura de la ola y la profundidad del agua aumenta por encima de un cierto límite, se "rompe" y cae en una masa de agua espumosa. Esta se precipita en una hoja por la playa antes de retirarse en el mar bajo la influencia de la gravedad.

Un tsunami es una forma inusual de ola causada por un evento poderoso poco frecuente, como un terremoto submarino, un deslizamiento de tierra, el impacto de un meteorito, una erupción volcánica o un colapso de tierra en el mar. Estos eventos pueden elevar o bajar temporalmente la superficie del mar en el área afectada, generalmente unos pocos pies. La energía potencial del agua de mar desplazada se convierte en energía cinética, creando una ola poco profunda, un tsunami, que irradia hacia afuera a una velocidad proporcional a la raíz cuadrada de la profundidad del agua y que, por lo tanto, viaja mucho más rápido en el océano abierto que en el océano sobre la placa continental. En el mar abierto, los tsunamis tienen longitudes de onda de alrededor de , viajan a velocidades de más de y generalmente tienen una altura de menos de , por lo que a menudo pasan desapercibidos en esa etapa. En contraste, las olas de la superficie del océano causadas por los vientos tienen longitudes de onda de unos pocos cientos de metros, viajan a hasta y tienen hasta de altura.

Un evento desencadenante en la plataforma continental puede causar un tsunami local en el lado terrestre y un tsunami distante que viaja a través del océano. La energía de la ola se disipa solo gradualmente, pero se extiende sobre el frente de la ola, de modo que a medida que la ola se aleja de la fuente, el frente se alarga y la energía promedio se reduce, por lo que las costas distantes, en promedio, serán golpeadas por olas más débiles Sin embargo, como la velocidad de la ola está controlada por la profundidad del agua, no viaja a la misma velocidad en todas las direcciones, y eso afecta a la dirección del frente de la ola —un efecto conocido como refracción— que puede enfocar la fuerza del avance del tsunami en algunas áreas y debilitarlo en otras según sea la topografía submarina.

A medida que un tsunami se mueve hacia aguas poco profundas, su velocidad disminuye, su longitud de onda se acorta y su amplitud aumenta enormemente, comportándose de la misma manera que una ola generada por el viento en aguas poco profundas, pero a una escala mucho mayor. Tanto el sumidero como la cresta de un tsunami pueden llegar primero a la costa. En el primer caso, el mar retrocede y deja expuestas las áreas submareales cercanas a la costa, lo que proporciona una advertencia útil para las personas en tierra. Cuando llega la cresta, generalmente no se rompe sino que se precipita tierra adentro, inundando todo a su paso. Gran parte de la destrucción puede ser causada por el agua de la inundación que regresa al mar después del tsunami, arrastrando escombros y personas con ella. A menudo, varios tsunamis son causados ​​por un solo evento geológico y llegan a intervalos de entre ocho minutos y dos horas. La primera ola en llegar a la costa puede no ser la más grande ni la más destructiva. Ocasionalmente, un tsunami puede transformarse en un macareo, generalmente en una bahía poco profunda o en un estuario.

El viento que sopla sobre la superficie del mar causa fricción en la interfaz entre el aire y el mar. Esto no solo hace que se formen olas, sino que también hace que el agua de mar superficial se mueva en la misma dirección que el viento. Aunque los vientos son variables, en cualquier lugar soplan predominantemente desde una única dirección y, por lo tanto, se puede formar una corriente superficial. Los vientos del oeste son más frecuentes en las latitudes medias, mientras que los vientos del este dominan en los trópicos. Cuando el agua se mueve de esta manera, otra agua fluye para llenar el vacío y se forma un movimiento circular de corrientes superficiales conocido como giro oceánico. Hay cinco giros principales en los océanos del mundo: dos en el Pacífico, dos en el Atlántico y uno en el océano Índico. Otros giros más pequeños se encuentran en mares menores y un solo giro fluye alrededor de la Antártida. Estos giros han seguido las mismas rutas durante milenios, guiados por la topografía de la tierra, la dirección del viento y el efecto Coriolis. Las corrientes superficiales fluyen en sentido horario en el hemisferio norte y en sentido antihorario en el hemisferio sur. El agua que se aleja del ecuador es cálida, y la que fluye en la dirección inversa ha perdido la mayor parte de su calor. Estas corrientes tienden a moderar el clima de la Tierra, enfriando la región ecuatorial y calentando regiones en latitudes más altas. El clima global y los pronósticos del tiempo se ven fuertemente afectados por el océano mundial, por lo que el modelado climático global utiliza modelos de circulación oceánica, así como modelos de otros componentes importantes como la atmósfera, las superficies terrestres, los aerosoles y el hielo marino. Los modelos oceánicos utilizan una rama de la física, la dinámica de fluidos geofísicos, que describe el flujo a gran escala de fluidos como el agua de mar.

Las corrientes superficiales solo afectan a los primeros cientos de metros superiores del mar, pero también hay flujos a gran escala en las profundidades del océano causados ​​por el movimiento de las masas de aguas profundas. Una corriente principal del océano profundo fluye a través de todos los océanos del mundo y se conoce como circulación termohalina o cinta transportadora global. Este movimiento es lento y está impulsado por diferencias en la densidad del agua causadas por variaciones en la salinidad y en la temperatura. En latitudes altas, el agua se enfría por la baja temperatura atmosférica y se vuelve más salada a medida que el hielo marino se cristaliza. Ambos factores la hacen más densa y el agua se hunde. Desde las profundidades del mar, cerca de Groenlandia, esa agua fluye hacia el sur entre las masas continentales a ambos lados del Atlántico. Cuando llega a la Antártida, se le unen más masas de agua fría que se hunde y fluye hacia el este. Luego se divide en dos corrientes que se mueven hacia el norte en los océanos Índico y Pacífico. Aquí se calienta gradualmente, se vuelve menos densa, se eleva hacia la superficie y se enrolla sobre sí mismo. Algunos vuelven al Atlántico. Se necesitan mil años para completar este patrón de circulación.

Además de los giros, hay corrientes superficiales temporales que ocurren bajo condiciones específicas. Cuando las olas se encuentran con una costa en ángulo, se crea una deriva litoral a medida que el agua es empujada paralelamente a la costa. El agua se arremolina en la playa en ángulo recto con las olas que se aproximan, pero se drena directamente por la pendiente bajo el efecto de la gravedad. Cuanto más grandes sean las olas, más largas las playas y más oblicuo el acercamiento de la ola, más fuertes serán la corrientes de la costa. Estas corrientes pueden desplazar grandes volúmenes de arena o de guijarros, crear cordones litorales y hacer que las playas desaparezcan y los canales de agua se llenen de sedimentos. Una corriente de resaca puede ocurrir cuando el agua se acumula cerca de la costa de las olas que avanzan y se canaliza hacia el mar a través de un canal en el fondo del mar. Puede ocurrir en una brecha en un banco de arena o cerca de una estructura hecha por el hombre, como un espigón. Estas fuertes corrientes pueden tener una velocidad de , pueden formarse en diferentes lugares en diferentes etapas de la marea y pueden llevarse a bañistas desprevenidos. Las corrientes temporales de surgencia ocurren cuando el viento empuja el agua fuera de la tierra y el agua más profunda sube para reemplazarla. Esta agua fría a menudo es rica en nutrientes y crea una floración de fitoplancton y un gran aumento en la productividad del mar.

Las mareas son el aumento y la caída regulares del nivel del agua que experimentan los mares y los océanos en respuesta a las influencias gravitacionales de la Luna y del Sol, y de los efectos de la rotación de la Tierra. Durante cada ciclo de marea, en cualquier lugar dado, el agua sube a una altura máxima conocida como "marea alta o pleamar" antes de disminuir nuevamente al nivel mínimo de "marea baja o bajamar". A medida que el agua retrocede, descubre más y más de la playa, también conocida como la zona intermareal. La diferencia de altura entre la pleamar y la bajamar se conoce como rango o amplitud de marea.

La mayoría de los lugares experimentan dos mareas altas cada día, que ocurren a intervalos de aproximadamente 12 horas y 25 minutos. Este tiempo es la mitad del período de 24 horas y 50 minutos que le llevaa a la Tierra hacer una revolución completa y devolver la Luna a su posición anterior en relación a un observador. La masa de la Luna es unos 27 millones de veces más pequeña que el Sol, pero está 400 veces más cerca de la Tierra. La fuerza de las marea disminuye rápidamente con la distancia, por lo que la luna tiene un efecto sobre las marea más del doble que el del Sol. Se forma una protuberancia en el océano en el lugar donde la Tierra está más cerca de la Luna, porque también es donde el efecto de la gravedad de la Luna es más fuerte. En el lado opuesto de la Tierra, la fuerza lunar está en su punto más débil y esto hace que se forme otro bulto. A medida que la Luna gira alrededor de la Tierra, esos bultos oceánicos se mueven alrededor de la Tierra. La atracción gravitacional del Sol también está trabajando en los mares, pero su efecto en las mareas es menos poderoso que el de la Luna, pero cuando el Sol, la Luna y la Tierra están alineados (luna llena y luna nueva), el efecto combinado resulta en las altas "mareas vivas". Por el contrario, cuando el Sol está a 90° de la Luna como se ve desde la Tierra, el efecto gravitacional combinado en las mareas es menos fuerte, causante de las "mareas bajas" más bajas.

Los flujos de marea de agua de mar son resistidos por la inercia del agua y pueden verse afectados por las masas de tierra. En lugares como el golfo de México, donde las tierras que lo delimitan constriñe el movimiento de las protuberancias, solo puede ocurrir un conjunto de mareas cada día. En la costa de una isla puede haber un ciclo diario complejo con cuatro mareas altas. Los estrechos de las islas en Chalkis en Eubea experimentan fuertes corrientes que cambian abruptamente de dirección, generalmente cuatro veces al día, pero hasta 12 veces al día cuando la luna y el sol están separados 90 grados. Donde hay una bahía o estuario en forma de embudo, el rango de marea puede ampliarse. La bahía de Fundy es el ejemplo clásico de esto y puede experimentar mareas de primavera de . Aunque las mareas son regulares y predecibles, la altura de las mareas altas puede reducirse con los vientos marinos y elevarse con los vientos terrestres. La alta presión en el centro de un anticiclón empuja hacia abajo el agua y se asocia con mareas anormalmente bajas, mientras que las áreas de baja presión pueden causar mareas extremadamente altas. Una tormenta puede ocurrir cuando los fuertes vientos acumulan agua contra la costa en un área poco profunda y esto, junto con un sistema de bajas presiones, puede elevar la superficie del mar con marea alta dramáticamente. En 1900, Galveston, Texas, experimentó una oleada de durante un huracán que arrasó la ciudad, matando a más de 3500 personas y destruyendo 3636 hogares.

La Tierra está compuesta por un núcleo central magnético, un manto en su mayoría líquido y una pesada capa externa rígida (o litosfera), que se compone de la corteza rocosa de la Tierra y de la capa externa más profunda y sólida del manto. En las partes que noy agua, en tierra, la corteza se conoce como corteza continental, mientras que bajo el mar se conoce como corteza oceánica. Esta última está compuesta de basalto relativamente denso y tiene un grosor de unos cinco a diez kilómetros. La litosfera relativamente delgada flota sobre el manto más débil y más caliente debajo y se fractura en varias placas tectónicas. En medio del océano, el magma está siendo empujado constantemente a través del lecho marino entre las placas adyacentes para formar las dorsales mediooceánicas y aquí las corrientes de convección dentro del manto tienden a separar las dos placas. Paralelamente a estas dorsales y más cerca de las costas, una placa oceánica puede deslizarse debajo de otra placa oceánica en un proceso conocido como subducción. Aquí se forman profundas fosas y el proceso se acompaña de fricción a medida que las placas se trituran juntas. El movimiento continúa en sacudidas que causan terremotos, produce calor y el magma se ve forzado a crear montes submarinos, algunas de las cuales pueden formar cadenas de islas volcánicas cerca de las fosas profundas. Cerca de algunos de los límites entre la tierra y el mar, las placas oceánicas ligeramente más densas se deslizan debajo de las placas continentales y se forman más fosas de subducción. A medida que se juntan, las placas continentales se deforman y se doblan causando la elevación de montañas y la actividad sísmica.

La fosa más profunda de la Tierra es la fosa de las Marianas, que se extiende por unos a través del fondo del mar. Está cerca de las islas Marianas, un archipiélago volcánico en el Pacífico occidental, y aunque tiene un promedio de solo de ancho, su punto más profundo está a debajo de la superficie del mar. Una fosa aún más larga corre a lo largo de la costa de Perú y Chile, alcanzando una profundidad de y extendiéndose aproximadamente unos . Ocurre donde la placa oceánica de Nazca se desliza bajo la placa continental de América del Sur y está asociada con el empuje y la actividad volcánica de los Andes.

La zona donde la tierra se encuentra con el mar se conoce como la costa y la parte entre las mareas de primavera más bajas y el límite superior alcanzado por las olas es la orilla. Una playa es la acumulación de arena o de guijarros en la orilla. Un promontorio es un punto de tierra que se adentra en el mar y un promontorio más grande se conoce como un cabo La hendidura de una costa, especialmente entre dos promontorios, es una bahía, una pequeña bahía con una entrada estrecha es una ensenada y una gran bahía se puede denominar golfo. Las líneas de costa están influenciadas por una serie de factores que incluyen la fuerza de las olas que llegan a la costa, el gradiente del margen terrestre, la composición y la dureza de la roca costera, la inclinación de la pendiente de la costa y los cambios del nivel de la tierra debido a la elevación o la inmersión locales. Normalmente, las olas circulan hacia la orilla a una velocidad de seis a ocho por minuto y se conocen como olas constructivas, ya que tienden a mover el material hacia la playa y tienen poco efecto erosivo. Las olas de tormenta llegan a la costa en rápida sucesión y se conocen como olas destructivas ya que el rebalaje —reflujo y escurrimiento— mueve el material de la playa hacia el mar. Bajo su influencia, la arena y los guijarros en la playa se trituran y desgastan. Alrededor de la marea alta, el poder de una ola de tormenta que impacta en el pie de un acantilado tiene un efecto devastador, ya que el aire en las grietas y fisuras se comprime y luego se expande rápidamente con la liberación de presión. Al mismo tiempo, la arena y los guijarros tienen un efecto erosivo al ser arrojados contra las rocas. Esto tiende a socavar el acantilado, y los procesos de meteorización normales como la acción de las heladas siguen, causando una mayor destrucción. Gradualmente, se desarrolla una plataforma de corte de olas al pie del acantilado y esto tiene un efecto protector, reduciendo aún más la erosión de las olas.

El material usado desde los márgenes de la tierra finalmente termina en el mar. Aquí está sujeto a atrición debido a que las corrientes que fluyen paralelas a la costa recorren los canales y transportan arena y guijarros lejos de su lugar de origen. Los sedimentos transportados al mar por los ríos se depositan en el lecho marino y hacen que se formen deltas en los estuarios. Todos esos materiales se mueven de un lado a otro bajo la influencia de las olas, las mareas y las corrientes. El dragado elimina material y profundiza los canales, pero puede tener efectos inesperados en otras partes de la costa. Los gobiernos hacen esfuerzos para evitar la inundación de la tierra mediante la construcción de rompeolas, diques marinos y otras defensas contra el mar. Por ejemplo, la barrera del Támesis está diseñada para proteger a la ciudad de Londres de una marejada ciclónica, mientras que el fracaso de los diques y diques alrededor de Nueva Orleans durante el huracán Katrina causó una crisis humanitaria en los Estados Unidos. La recuperación de tierras en Hong Kong también permitió la construcción del Aeropuerto Internacional de Hong Kong mediante la nivelación y expansión de dos islas más pequeñas.

Durante la mayor parte del tiempo geológico, el nivel del mar ha estado más alto de lo que está hoy. El principal factor que afecta al nivel del mar a lo largo del tiempo es el resultado de los cambios en la corteza oceánica, con una tendencia descendente que se espera que continúe a muy largo plazo. En el último máximo glacial, hace unos , el nivel del mar estaba por debajo de su nivel actual. Durante al menos los últimos 100 años, el nivel del mar ha aumentado a una tasa promedio de aproximadamente por año. La mayor parte de este aumento se puede atribuir a un aumento en la temperatura del mar y a la ligera expansión térmica resultante de los superiores de agua. Contribuciones adicionales, hasta una cuarta parte del total, provienen de las fuentes de agua sobre la tierra, como el derretimiento de la nieve y de los glaciares y la extracción de agua subterránea para riego y otras necesidades agrícolas y humanas. Se espera que la tendencia al alza del calentamiento global continúe al menos hasta el final del siglo XXI.

El mar desempeña un papel en el ciclo hidrológico o del agua, en el que el agua se evapora del océano, viaja a través de la atmósfera en forma de vapor, se condensa, cae como lluvia o nieve, manteniendo así la vida en tierra y en gran medida regresa al mar. Incluso en el desierto de Atacama, donde cae muy poca lluvia, densas nubes de niebla conocidas como la camanchaca soplan desde el mar y sostienen la vida vegetal. 

En Asia central y otras grandes masas de tierra, hay cuencas endorreicas que no tienen salida al mar, separadas del océano por montañas u otros accidentes geológicos naturales que impiden que el agua drene. El mar Caspio es el mayor de ellos. Su afluencia principal proviene del río Volga, no hay flujo de salida y la evaporación del agua lo hace salino a medida que se acumulan los minerales disueltos. El mar de Aral, en Kazajistán y Uzbekistán, y el lago Pyramid, en el oeste de los Estados Unidos, son otros ejemplos de grandes cuerpos de agua salina interiores sin drenaje. Algunos lagos endorreicos son menos salados, pero todos son sensibles a las variaciones en la calidad del agua entrante.
Los océanos contienen la mayor cantidad de carbono de ciclo activo en el mundo y solo son superados por la litosfera en la cantidad de carbono que almacenan. La capa superficial de los océanos contiene grandes cantidades de carbono orgánico disuelto que se intercambia rápidamente con la atmósfera. La concentración de carbono inorgánico disuelto en las capas profundas es aproximadamente un 15% más alta que la de la capa superficial y permanece allí durante períodos de tiempo mucho más largos. La circulación termohalina intercambia carbono entre estas dos capas.

El carbono ingresa en el océano a medida que el dióxido de carbono atmosférico se disuelve en las capas superficiales y se convierte en ácido carbónico, carbonato y bicarbonato:

También puede ingresar a través de los ríos como carbono orgánico disuelto y es convertido por los organismos fotosintéticos en carbono orgánico. Este puede intercambiarse a lo largo de la cadena alimentaria o precipitarse en las capas más profundas y ricas en carbono como tejido blando muerto o en conchas y huesos como carbonato de calcio. Circula en esta capa durante largos períodos de tiempo antes de depositarse como sedimento o regresar a las aguas superficiales a través de la circulación termohalina.
El agua de mar es ligeramente alcalina y tuvo un pH promedio de aproximadamente 8.2 en los últimos 300 millones de años. Más recientemente, las actividades antropogénicas han aumentado constantemente el contenido de dióxido de carbono de la atmósfera; Los océanos absorben alrededor del 30-40% del CO agregado, formando ácido carbónico y bajando el pH (ahora por debajo de 8.1) a través de un proceso llamado acidificación oceánica.Se espera que el pH alcance 7,7 (lo que representa un aumento de 3 veces en la concentración de iones de hidrógeno) para el año 2100, que es un cambio significativo en un siglo.

Un elemento importante para la formación de material esquelético en los animales marinos es el calcio, pero el carbonato de calcio se vuelve más soluble con la presión, por lo que los depósitos y esqueletos de carbonato se disuelven por debajo de su profundidad de compensación. El carbonato de calcio también se vuelve más soluble con un pH más bajo, por lo que es probable que la acidificación del océano tenga efectos profundos en los organismos marinos con conchas calcáreas, como las ostras, almejas, erizos de mar y corales, debido a que su capacidad para formar conchas se reducirá, y la profundidad de compensación del carbonato se elevará más cerca de la superficie del mar. Los organismos planctónicos afectados incluirán a los moluscos en forma de caracol conocidos como pterópodos y a las algas unicelulares llamadas coccolitofóridos y foraminíferos. Todos estos son partes importantes de la cadena alimentaria y una disminución en su número tendrá consecuencias significativas. En las regiones tropicales, es probable que los corales se vean gravemente afectados, ya que se hace más difícil construir sus esqueletos de carbonato de calcio, que a su vez afectan negativamente a otros habitantes de los arrecifes.
La tasa actual de cambio en la química del océano parece no tener precedentes en la historia geológica de la Tierra, por lo que no está claro cómo de bien podrán adaptarse los ecosistemas marinos a las condiciones cambiantes del futuro cercano. De particular preocupación es la forma en que la combinación de acidificación con los estresores adicionales esperados de temperaturas más altas y niveles de oxígeno más bajos impactarán en los mares.

Los océanos albergan una colección diversa de formas de vida que lo utilizan como hábitat. Dado que la luz solar ilumina solo las capas superiores, la mayor parte del océano está en la oscuridad permanente. Como las diferentes zonas de profundidad y de temperatura proporcionan hábitat para un conjunto único de especies, el entorno marino en su conjunto abarca una inmensa diversidad de vida. Los hábitats marinos varían desde las aguas superficiales hasta las fosas oceánicas más profundas, incluidos los arrecifes de coral, los bosques de algas, las praderas marinas, los pozas de marea, fondos marinos fangosos, arenosos y rocosos, y la zona pelágica abierta. Los organismos que viven en el mar varían desde ballenas de 30 metros de largo hasta fitoplancton y zooplancton microscópicos, hongos y bacterias and viruses, including recently discovered marine bacteriophages which live parasitically inside bacteria. La vida marina juega un papel importante en el ciclo del carbono, ya que los organismos fotosintéticos convierten el dióxido de carbono disuelto en carbono orgánico y esto es económicamente importante para los humanos al proporcionar pescado para su uso como alimento.

La vida puede haberse originado en el mar y todos los principales grupos de animales están representados allí. Los científicos difieren en cuanto a dónde surgió la vida en el mar: el temprano experimento de Miller y Urey sugería una "sopa" química diluida en aguas abiertas, pero las evidencias más recientes apuntan a las aguas termales volcánicas, a los sedimentos de arcilla de grano fino o a las fumarolas negras de aguas profundas, todos ellos ambientes que habrían proporcionado protección contra la radiación ultravioleta dañina que no era bloqueada por la atmósfera de la Tierra primitiva.

Los hábitats marinos se pueden dividir:



Los arrecifes de coral, los llamados «bosques tropicales del mar», ocupan menos del 0.1% de la superficie oceánica del mundo, pero sus ecosistemas incluyen el 25% de todas las especies marinas. Los más conocidos son los arrecifes de coral tropicales como la Gran Barrera de Coral de Australia, pero los arrecifes de agua fría albergan una amplia gama de especies, incluidos los corales (solo seis de los cuales contribuyen a la formación de arrecifes).
Los productores primarios marinos —plantas y organismos microscópicos en el plancton—están ampliamente extendidos y son esenciales para el ecosistema. Se ha estimado que la mitad del oxígeno del mundo es producido por el fitoplancton y alrededor del 45% de la producción primaria de material vivo del mar es aportada por las diatomeas. Las algas mucho mayores, comúnmente conocidas como macroalgas, son importantes a nivel local; "Sargassum" forma derivas flotantes, mientras que el kelp forma bosques de fondos marinos. Las plantas con flores en forma de pastos marinos crecen en "praderas" en aguas poco profundas arenosas, los manglares se alinean en la costa en las regiones tropicales y subtropicales y las plantas tolerantes a la sal prosperan en las marismas salinas regularmente inundadas. Todos estos hábitats son capaces de secuestrar grandes cantidades de carbono y mantener un rango biodiverso de vida animal cada vez más grande.

La luz solo puede penetrar en los superiores, por lo que esa es la única parte del mar donde pueden crecer las plantas. Las capas superficiales a menudo son deficientes en compuestos de nitrógeno biológicamente activos. El ciclo del nitrógeno marino consiste en transformaciones microbianas complejas que incluyen la fijación de nitrógeno, su asimilación, la nitrificación, anammox y desnitrificación. Algunos de estos procesos tienen lugar en aguas profundas, de modo que donde hay una corriente de aguas frías, y también cerca de los estuarios donde hay nutrientes de origen terrestre, el crecimiento de las plantas es mayor. Esto significa que las áreas más productivas, ricas en plancton y, por lo tanto, también en peces, son principalmente costeras.
Hay un espectro más amplio de taxones de animales superiores en el mar que en la tierra, muchas especies marinas aún no se han descubierto y el número conocido por la ciencia aumenta anualmente. Algunos vertebrados como las aves marinas, las focas y las tortugas marinas regresan a la tierra para reproducirse, pero los peces, los cetáceos y las serpientes marinas tienen un estilo de vida completamente acuático y muchos phyla de invertebrados son completamente marinos. De hecho, los océanos están llenos de vida y proporcionan muchos microhábitats diferentes. Una de estos es la película de superficie que, aunque se mueve por el movimiento de las olas, proporciona un ambiente rico y alberga bacterias, hongos, microalgas, protozoos, huevos de peces y varias larvas.

La zona pelágica contiene macro y microfauna y una miríada de zooplancton que derivan con las corrientes. La mayoría de los organismos más pequeños son las larvas de peces e invertebrados marinos que liberan sus huevos en grandes cantidades porque la posibilidad de que un embrión sobreviva hasta la madurez es mínima. El zooplancton se alimenta del fitoplancton y unos de otros, entre sí, y forma una parte básica de la compleja cadena alimentaria que se extiende a través de peces de diversos tamaños y otros organismos nectónicos hasta los grandes calamares, los tiburones, las marsopas, los delfines y las ballenas. Algunas criaturas marinas realizan grandes migraciones, ya sea a otras regiones del océano de forma estacional o migraciones verticales diariamente, a menudo ascendiendo para alimentarse por la noche y descendiendo a un lugar seguro durante el día. Los barcos pueden introducir o propagar especies invasoras a través de la descarga de agua de lastre o por el transporte de organismos que se han acumulado como parte de la comunidad de incrustaciones en los cascos de los buques.

La zona demersal soporta muchos animales que se alimentan de organismos bentónicos o que buscan protección contra los depredadores, ya que el fondo marino proporciona una variedad de hábitats en, o debajo de, la superficie del sustrato que utilizan las criaturas adaptadas a estas condiciones. La zona mareal, con su exposición periódica al aire deshidratante, es el hogar de percebes, moluscos y crustáceos. La zona nerítica tiene muchos organismos que necesitan luz para prosperar. Aquí, entre las rocas incrustadas de algas viven esponjas, equinodermos, gusanos poliquetos, anémonas de mar y otros invertebrados. Los corales a menudo contienen simbiontes fotosintéticos y viven en aguas poco profundas donde penetra la luz. Los extensos esqueletos calcáreos que extruyen se acumulan en los arrecifes de coral que son una característica importante del fondo marino. Estos proporcionan un hábitat biodiverso para los organismos que viven en los arrecifes. Hay menos vida marina en el fondo de los mares más profundos, pero la vida marina también florece alrededor de las montes submarinas que se elevan desde las profundidades, donde los peces y otros animales se congregan para desovar y alimentarse. Cerca del fondo marino viven peces demersales que se alimentan principalmente de organismos pelágicos o invertebrados bentónicos. La exploración de las profundidades del mar mediante sumergibles reveló un nuevo mundo de criaturas que viven en el fondo del mar que los científicos no sabían que existían anteriormente. Algunos como los detritívoros dependen del material orgánico que cae al fondo del océano. Otros se agrupan alrededor de respiraderos hidrotermales de aguas profundas donde los flujos de agua ricos en minerales emergen del fondo marino, soportando comunidades cuyos productores primarios son bacterias quimioautotróficas oxidantes de sulfuro, y cuyos consumidores incluyen bivalvos especializados, anémonas de mar, percebes, cangrejos, gusanos y peces,que a menudo no se encuentra en ningún otro lugar. Una ballena muerta que se hunde en el fondo del océano proporciona alimento para un conjunto de organismos que también dependen en gran medida de las acciones de las bacterias reductoras de azufre. Dichos lugares soportan biomas únicos donde se han descubierto muchos nuevos microbios y otras formas de vida.

Los humanos han viajado por los mares desde la primera vez que construyeron embarcaciones marítimas. Los mesopotámicos usaban bitumen para calafatear sus botes de caña y, un poco más tarde, velas con mástiles. Hacia , los austronesios en Taiwán comenzaron a extenderse por el sudeste marítimo de Asia. Posteriormente, los pueblos "lapita" austronesios mostraron grandes hazañas de navegación, llegando desde el archipiélago de Bismarck hasta lugares tan lejanos como las isla Fiyi, Tonga y Samoa. Sus descendientes continuaron viajando miles de millas entre pequeñas islas en canoas con balancín, y en el proceso encontraron muchas islas nuevas, incluyendo Hawái, Isla de Pascua (Rapa Nui) y Nueva Zelanda.

Los antiguos egipcios y fenicios exploraron el Mediterráneo y el mar Rojo con el egipcio Hannu llegando a la península arábiga y a la costa africana alrededor del . En el , los fenicios y los griegos establecieron colonias en todo el Mediterráneo y el mar Negro. Alrededor de , el navegante cartaginés Hanno dejó un detallado periplo de un viaje por el Atlántico que llegó al menos a Senegal y posiblemente al monte Camerún. En el período medieval temprano, los vikingos cruzaron el Atlántico Norte e incluso podrían haber llegado a las franjas del noreste de América del Norte. Los novgorodianos también habían estado navegando por el mar Blanco desde el siglo XIII o antes. Mientras tanto, los mares a lo largo de la costa oriental y del sur de Asia fueron utilizados por los comerciantes árabes y chinos. La dinastía china Ming tenía una flota de 317 barcos con bajo Zheng He a principios del siglo XV, navegando por los océanos Índico y Pacífico. A finales del siglo XV, los marineros de Europa occidental comenzaron a realizar viajes más largos de exploración en busca de comercio. Bartolomeu Dias rodeó el cabo de Buena Esperanza en 1487 y Vasco da Gama llegó a India a través del Cabo en 1498. Cristóbal Colón zarpó de Cádiz en 1492, intentando llegar a las tierras orientales de India y Japón por los nuevos medios de viajar hacia el oeste. En su lugar, tocó tierra en una isla en el mar Caribe y unos años más tarde, el navegante veneciano John Cabot llegó a Terranova. El italiano Amerigo Vespucci, por quien fue nombrada América, exploró la costa sudamericana en viajes realizados entre 1497 y 1502, descubriendo la desembocadura del río Amazonas. En 1519, el navegante portugués Fernando Magallanes dirigió la primera expedición para navegar alrededor del mundo.
En cuanto a la historia de los instrumentos de navegación, los antiguos griegos y chinos usaron un compás por primera vez para mostrar dónde se encontraba el norte y la dirección a la que se dirigía el barco. La latitud (un ángulo que varía desde 0° en el ecuador a 90° en los polos) se determinaba midiendo el ángulo entre el Sol, la Luna o una estrella específica con respecto al horizonte mediante el uso de un astrolabio, del bastón de Jacob o del sextante. La longitud (una línea en el globo que une los dos polos) solo se pudo calcular con un cronómetro preciso para mostrar la diferencia horaria exacta entre el barco y un punto fijo como el meridiano de Greenwich. En 1759, John Harrison, un relojero, diseñó dicho instrumento y James Cook lo usó en sus viajes de exploración. Hoy en día, el Sistema de Posicionamiento Global (GPS) que utiliza más de treinta satélites permite una navegación precisa en todo el mundo.

Con respecto a los mapas, vitales para la navegación, Ptolomeo trazó en el siglo II un mapa de todo el mundo conocido desde las "Insulas Fortunatae", Cabo Verde o Canarias, hacia el este hasta el golfo de Tailandia. Este mapa se usó en 1492 cuando Cristóbal Colón emprendió sus viajes de descubrimiento. Posteriormente, Gerardus Mercator hizo un mapa práctico del mundo en 1538, con una proyección de mapa que convenientemente volvía rectas las líneas de rumbo. En el siglo XVIII se habían hecho mejores mapas y parte del objetivo de James Cook en sus viajes era seguir cartografiando el océano. El estudio científico ha continuado con los registros en profundidad del "Tuscarora", la investigación oceánica de los viajes del Challenger (1872-1876), el trabajo de los marineros escandinavos Roald Amundsen y Fridtjof Nansen, la expedición de Michael Sars en 1910, la expedición alemana de meteoritos de 1925, el trabajo de reconocimiento antártico del "Discovery II" en 1932, y otros desde entonces. Además, en 1921, se creó la Organización Hidrográfica Internacional, que constituye la autoridad en topografía hidrográfica y cartografía náutica.

La oceanografía científica comenzó con los viajes del capitán James Cook de 1768 a 1779, describiendo el Pacífico con una precisión sin precedentes desde los 71ºS a los 71ºN. Los cronómetros de John Harrison apoyaron la navegación precisa de Cook y la cartografía en dos de estos viajes, mejorando permanentemente el estándar alcanzable para los trabajos posteriores. Otras expediciones siguieron en el siglo XIX, desde Rusia, Francia, los Países Bajos y los Estados Unidos, así como más de Gran Bretaña. En el HMS "Beagle", que proporcionó a Charles Darwin ideas y materiales para su libro de 1859 "On the Origin of Species" [Sobre el origen de las especies], el capitán del barco, Robert FitzRoy, cartografió los mares y las costas y publicó su informe en cuatro volúmenes sobre los tres viajes del barco en 1839. El libro de Edward Forbes de 1854, "Distribution of Marine Life" [Distribución de la vida marina], argumentó que no podría existir vida por debajo de unos 600 m. Esto fue demostrado erróneoe por los biólogos británicos W. B. Carpenter y C. Wyville Thomson, quienes en 1868 descubrieron la vida en aguas profundas mediante el dragado. Wyville Thompson se convirtió en el científico jefe de la expedición Challenger de 1872-1876, que efectivamente creó la ciencia de la oceanografía.
En su viaje de alrededor del mundo, el "HMS Challenger" descubrió alrededor de marinas nuevas, e hizo 492 sondeos de aguas profundas, 133 dragas de fondo, 151 redes de arrastre en aguas abiertas y 263 observaciones en serie de la temperatura del agua. En el Atlántico sur, en 1898-1899, Carl Chun en el "Valdivia" trajo a la superficie muchas formas de vida nuevas desde profundidades de más de . Las primeras observaciones de animales de aguas profundas en su entorno natural fueron hechas en 1930 por William Beebe y Otis Barton, quienes descendieron a en el esférico Bathysphere de acero. Esto fue descendido mediante un cable, pero en 1960 un sumergible autopropulsado, Trieste, desarrollado por Jacques Piccard, llevó a Piccard y a Don Walsh a la parte más profunda de los océanos de la Tierra, la fosa de las Marianas en el Pacífico, alcanzando una profundidad récord de aproximadamente , una hazaña que no se repitió hasta 2012 cuando el director de cine canadiense James Cameron pilotó el Deepsea Challenger a profundidades similares. Se puede usar un traje de buceo atmosférico para operaciones en aguas profundas, con un nuevo récord mundial establecido en 2006 cuando un buzo de la Marina de los EE. UU. Descendió a en uno de estos trajes articulados y presurizados.

A grandes profundidades, la luz desde arriba no penetra a través de las capas de agua y la presión es extrema. Para la exploración en aguas profundas es necesario utilizar vehículos especializados, ya sea vehículos submarinos operados de forma remota con luces y cámaras o sumergibles tripulados. Los sumergibles Mir que funcionan con baterías tienen una tripulación de tres hombres y pueden descender a . Tienen puertos de visualización, luces de , equipos de video y brazos manipuladores para recoger muestras, colocar sondas o empujar el vehículo a través del lecho marino cuando los propulsores agitarían el sedimento en exceso.

La batimetría es la cartografía y estudio de la topografía del fondo del océano. Los métodos utilizados para medir la profundidad del mar incluyen ecosondas monohaz o multihaz, sondas de profundidad aerotransportadas por láser y el cálculo de profundidades a partir de datos de teledetección satelital. Esta información se utiliza para determinar el tendido de cables y tuberías submarinas, para elegir las ubicaciones adecuadas para emplazar las plataformas petroleras y las turbinas eólicas en alta mar y para identificar posibles nuevas pesquerías.

La investigación oceanográfica en curso incluye el estudio de las formas de vida marina, la conservación, el medio marino, la química del océano, el estudio y modelado de la dinámica del clima, el límite aire-mar, los patrones climáticos, los recursos oceánicos, la energía renovable, las olas y corrientes, y el diseño y desarrollo de nuevas herramientas y tecnologías para investigar en profundidad. Mientras que en los años 1960 y 1970 la investigación estaba centrada en la taxonomía y en la biología básica, en los años 2010 la atención se centra en temas más amplios como el cambio climático.. Los investigadores utilizan la teledetección satelital para las aguas superficiales, con barcos de investigación, observatorios amarrados y vehículos autónomos submarinos para estudiar y monitorear todas las partes del mar.

La «libertad de los mares» es un principio del derecho internacional que data del siglo XVII. Hace hincapié en la libertad de navegar por los océanos y desaprueba la guerra librada en aguas internacionales.Hoy, este concepto está consagrado en la Convención de las Naciones Unidas sobre el Derecho del Mar ("United Nations Convention on the Law of the Sea", UNCLOS), suscrita en 1982 y cuya tercera versión entró en vigor en 1994. Es calificada como la "Constitución de los océanos". El artículo 87(1) establece: «La alta mar está abierta a todos estados, ya sean costeros o sin litoral». El artículo 87(1) (a) a (f) ofrece una lista no exhaustiva de las libertades, que comportan la navegación, el sobrevuelo, el tendido de cables submarinos, la construcción de islas artificiales, la pesca y la investigación científica. La seguridad del transporte marítimo está regulada por la Organización Marítima Internacional ("International Maritime Organization"). Sus objetivos incluyen el desarrollo y mantenimiento de un marco regulatorio para el transporte marítimo, la seguridad marítima, las preocupaciones ambientales, los asuntos legales, la cooperación técnica y la seguridad marítima.

UNCLOS define varias áreas de agua. Las «aguas interiores» están en el lado de tierra de una línea de base y las embarcaciones extranjeras no tienen derecho de paso en ellas. Las «aguas territoriales» se extienden a 12 millas náuticas (22 km) de la costa y en estas aguas, el estado costero es libre de establecer leyes, regular el uso y explotar cualquier recurso. Una «zona contigua» que se extiende otras 12 millas náuticas permite el seguimiento de buques sospechosos de infringir las leyes en cuatro áreas específicas: aduanas, impuestos, inmigración y contaminación. Una «zona económica exclusiva» se extiende 200 millas náuticas (370 km) desde la línea de base. Dentro de esta área, la nación costera tiene derechos exclusivos de explotación sobre todos los recursos naturales. La «plataforma continental» es la prolongación natural del territorio terrestre hasta el borde exterior del margen continental, o 200 millas náuticas desde la línea de base del estado costero, la que sea mayor. Aquí la nación costera tiene el derecho exclusivo de extraer minerales y también recursos vivos «ligados» al fondo marino.

De la Prehistoria y las primeras épocas de la Historia Antigua se conservan numerosas referencias sobre la guerra en el mar. Destacan especialmente en las leyendas homéricas: la "Ilíada", sobre la Guerra de Troya, y su continuación, la "Odisea". El control del mar es importante para la seguridad de una nación marítima, y ​​el bloqueo naval de un puerto se puede utilizar para cortar el abastecimiento de alimentos y suministros en tiempo de guerra. En el mar se han librado batallas durante más de 3000 años, estando datada la primera batalla naval registrada en documentos escritos hacia el año : Suppiluliuma II, rey de los hititas, se enfrentó con sus naves a una flota procedente de Alashiya (moderno Chipre), derrotándola e incendiando los barcos chipriotas en el mar.
El Imperio persa —fuerte y unido, pero sin un poder marítimo propio— no pudo vencer a los débiles y desunidos griegos, debido al poder de la flota ateniense. Reforzada por las flotas de otras "polis" (ciudades) más pequeñas, siempre consiguió frustrar los intentos persas de subyugar a las "polei" (ciudades-estado griegas). En la decisiva batalla de Salamina del , el general griego Temistocles atrapó a la flota mucho mayor del rey persa Jerjes II en un canal estrecho y la atacó vigorosamente, destruyendo 200 barcos persas por la pérdida de 40 buques griegos.

El poder y la influencia de las civilizaciones fenicia y egipcia, los de la púnica (basada en Cartago), e incluso los de Roma, dependieron en gran medida de su respectiva capacidad de controlar los mares (talasocracia). También la República de Venecia consiguió destacar sobre sus rivales entre las ciudades-estado de Italia, por su desarrollo naval. Pero su pujanza comercial se eclipsó por el declive del Mediterráneo en la Edad Moderna (las grandes rutas del comercio internacional se desarrollaron lejos de Venecia, en el océano Atlántico).

Algo similar le sucedió al poderío del Imperio otomano, ligado a la decadencia de la Ruta de la Seda y del Mediterráneo en general, durante los siglos XVII y XVIII. En otras épocas, el dominio del mar dio una gran relevancia a pueblos pequeños y comparativamente atrasados: durante tres siglos (del VI al IX), los hombres del norte, llamados comúnmente vikingos, asaltaron, saquearon e infestaron las costas europeas, llegando incluso a la Rusia central, a Ucrania y a Constantinopla. Remontaron los grandes ríos tributarios del mar Negro, el Danubio, el Don y el Volga, y cruzaron innumerables veces el estrecho de Gibraltar, considerado entre los grandes reinos europeos del momento, centrados en el Mediterráneo y menos avezados en los viajes oceánicos por el Atlántico, como las Columnas de Hércules, la puerta hacia un mar indómito, desconocido y lleno de peligros.

Al final de la Era de la navegación a vela, la armada inglesa, liderada por Horacio Nelson, rompió el poder de las flotas combinadas francesa y española en la batalla de Trafalgar en 1805.

Con el vapor y la producción industrial de chapa de acero, se produjo un gran aumento de la potencia de fuego en forma de barcos acorazados (dreadnought) armados con cañones de largo alcance. En 1905, la flota japonesa derrotó decisivamente a la flota rusa, que había viajado más de , en la batalla de Tsushima. Los acorazados lucharon de forma inconcluyente en la Primera Guerra Mundial en la batalla de Jutlandia de 1916 entre la Gran Flota de la Marina Real británica y la Flota de Alta Mar de la Marina Imperial alemana. En la Segunda Guerra Mundial, la victoria británica en la batalla de Taranto de 1940 mostró que el poder aéreo naval era suficiente para vencer a los mayores buques de guerra, presagiando las decisivas batallas navales de la Guerra del Pacífico, incluidas las batallas del Mar de Coral (942), de Midway (1942), del mar de Filipinas (1944) y la culminante batalla del Golfo de Leyte (1944), en la que los barcos dominantes ya fueron los portaaviones.

Los submarinos se hicieron importantes en la guerra naval en la Primera Guerra Mundial, cuando los submarinos alemanes, conocidos como U-boats, hundieron a cerca de mercantes aliados, incluyendo sin embargo al "RMS Lusitania", ayudando así a entrar en guerra a los Estados Unidos. En la Segunda Guerra Mundial, casi aliados fueron hundidos por submarinos que intentaban bloquear el flujo de suministros a Gran Bretaña, pero los Aliados rompieron el bloqueo en la batalla del Atlántico, que duró toda la longitud de la guerra, hundiendo 783 U-boats. Desde 1960, varias naciones han mantenido flotas de submarinos de misiles balísticos de propulsión nuclear, embarcaciones equipadas para lanzar misiles balísticos con ojivas nucleares desde el mar. Algunos de estos se mantienen permanentemente en patrulla.

Los veleros y paquebotes transportaban correo al extranjero, siendo uno de los primeros el servicio neerlandés a Batavia en la década de 1670. Pronto añadieron alojamiento para pasajeros, pero en condiciones de hacinamiento. Más tarde, se ofrecieron servicios programados, aunque el tiempo de viaje dependía mucho del clima. Cuando los barcos de vapor reemplazaron a los veleros, los navíos transatlánticos asumieron la tarea de transportar a las personas. A principios del siglo XX, cruzar el Atlántico tomaba alrededor de cinco días y las compañías navieras competían por tener los barcos más grandes y rápidos. El Blue Riband era un galardón no oficial otorgado al transatlántico más rápido que cruzase el Atlántico en servicio regular. Entregado por vez primera en 1830 al "Columbia" por una travesía de casi 16 días, el "Mauretania" retuvó el título con 4 d y 19 h () durante casi veinte años desde 1909. El Trofeo Hales, otro premio por la travesía comercial más rápida del Atlántico, fue ganado por el "SS United States" en 1952 por un viaje que llevó tres días, diez horas y cuarenta minutos.

Los grandes barcos de línea eran cómodos pero caros en combustible y en personal. La edad de los transatlánticos disminuyó a medida que se disponía de vuelos intercontinentales baratos. En 1958, un servicio aéreo regular programado entre Nueva York y París que demoraba siete horas condenó al servicio de ferry del Atlántico al olvido. Uno a uno, los barcos fueron apartados, algunos fueron desguazados, otros se convirtieron en cruceros para la industria del ocio y otros, incluso, en hoteles flotantes. El mar sigue siendo una ruta por la que los refugiados viajan en pequeñas embarcaciones, a veces poco aptas para navegar, a menudo habiendo pagado dinero a los traficantes de personas por su pasaje. Algunos pueden estar huyendo de la persecución, pero la mayoría son inmigrantes económicos que intentan llegar a países donde creen que sus perspectivas son mejores.

El comercio marítimo ha existido durante milenios. La dinastía ptolemaica había desarrollado el comercio con India utilizando los puertos del mar Rojo y en el los árabes, fenicios, israelitas e indios ya comerciaban con artículos de lujo como especias, oro y piedras preciosas. Los fenicios fueron conocidos comerciantes de mar y bajo los griegos y romanos, el comercio continuó prosperando. Con el colapso del Imperio romano, el comercio europeo disminuyó pero continuó floreciendo entre los reinos de África, Oriente Medio, India, China y el sudeste de Asia. Desde los siglos XVI al XIX, alrededor de 13 millones de personas fueron enviadas a través del Atlántico para ser vendidas como esclavas en las Américas.

Hoy en día, grandes cantidades de mercancías se transportan por mar, especialmente a través del Atlántico y alrededor de la cuenca del Pacífico. Una importante ruta comercial pasa por los Pilares de Hércules, cruza el Mediterráneo y el canal de Suez hasta el océano Índico y el estrecho de Malaca; gran parte del comercio también pasa por el canal de la Mancha. Las rutas marítimas son las rutas en mar abierto utilizadas por los buques de carga, que tradicionalmente utilizan vientos alisios y la corrientes. Más del 60% del tráfico mundial de contenedores circula por una de las veinte rutas comerciales más importantes. El aumento de la fusión del hielo del Ártico desde 2007 permite a los barcos viajar por el Paso del Noroeste durante algunas semanas en verano, evitando las rutas más largas a través del canal de Suez o el canal de Panamá. El flete se complementa con el flete aéreo, un envío más costoso reservado principalmente para cargas particularmente valiosas o perecederas. El comercio marítimo transportaba en 2013 más de 4 billones de dólares en bienes por año.

Hay dos tipos principales de carga, carga a granel ("bulk cargo") y carga fraccionada o carga general ("break bulk"), la mayoría de las cual ahora se transporta en portacontenedores. Las mercancías en forma de líquidos, polvo o partículas se transportan sueltas en las bodegas de graneleros e incluyen petróleo, granos, carbón, mineral, chatarra, arena y grava. Los cargueros de carga a granel suele ser productos manufacturados y se transporta en paquetes, a menudo apilados en pallets. Antes de la llegada de la contenedorización en la década de 1950, estos productos se cargaban, transportaban y descargaban pieza a pieza. El uso de contenedores ha aumentado considerablemente la eficiencia y ha disminuido el costo de moverlos viajando ahora la mayoría de la carga en contenedores de tamaño estándar con cerradura, cargados en portacontenedores especialmente diseñados para atracar en terminales dedicadas exclusivamente a ellos. Las empresas de transporte de carga reservan la carga, organizan la recogida y entrega y gestionan la documentación.

El pescado y otros productos pesqueros se encuentran entre las fuentes más importantes de proteínas y otros nutrientes que son esenciales para una dieta humana equilibrada y una buena salud. En 2009, el 16,6% de la ingesta mundial de proteínas animales y el 6,5% de todas las proteínas consumidas provenían del pescado. Para satisfacer esta necesidad, los países costeros han explotado los recursos marinos en su zona económica exclusiva, aunque los buques pesqueros se aventuran cada vez más lejos para explotar las poblaciones en aguas internacionales. En 2011, la producción mundial total de pescado, incluida la acuicultura, se estimó en 154 Mto, de las cuales la mayoría fue para consumo humano. La captura de peces silvestres representó 90,4 Mto, mientras que el aumento anual de la acuicultura contribuye con el resto. El Pacífico noroccidental es, con mucho, el área más productiva con 20,9 Mto (el 27% de las capturas marinas mundiales) en 2010. Además, el número de buques pesqueros en 2010 alcanzó los 4,36 millones, mientras que el número de personas empleadas en el sector primario de producción pesquera, en el mismo año, fue de 54.8 millones.
Los buques pesqueros modernos incluyen arrastreros con una pequeña tripulación, arrastreros de popa, cerqueros, palangreros y grandes buques factoría diseñados para permanecer en el mar durante semanas, procesando y congelando grandes cantidades de pescado. El equipo utilizado para capturar los peces pueden ser redes de cerco, redes de arrastre, redes de enmalle y palangres y las especies de peces más capturadas con frecuencia son el arenque, el bacalao, la anchoa, el atún, el lenguado, el salmonete, los calamares y el salmón. La sobreexplotación misma se ha convertido en una seria preocupación y no solo porque causa el agotamiento de las poblaciones de peces, sino que también reduce sustancialmente la población de peces depredadores. Myers & Wworm estimó que «las pesquerías industrializadas generalmente reducen la biomasa comunitaria en un 80% a los 15 años de la explotación». Para evitar la sobreexplotación, muchos países han introducido cuotas en sus propias aguas. Sin embargo, los esfuerzos de recuperación a menudo conllevan costos sustanciales para las economías locales o el suministro de alimentos. No obstante, una investigación publicada en "Nature" en abril de 2018 encontró que el esfuerzo agresivo para reducir la pesca ilegal del Ministro de Asuntos Marítimos y Pesca de Indonesia, Susi Pudjiastuti, ha «reducido el esfuerzo total de pesca en al menos un 25%, (...) [potencialmente] generado un aumento del 14% en la captura y un aumento del 12% en las ganancias». Por lo tanto, el documento concluía que «muchas naciones pueden recuperar sus pesquerías al tiempo que evitan esos costos a corto plazo abordando con dureza la pesca ilegal, no declarada y no reglamentada (IUU)».

Los métodos de pesca artesanal incluyen la caña y la línea, los arpones, el buceo, las trampas y las redes de tiro y redes de arrastre. Los barcos de pesca tradicionales funcionan con motores de paleta, viento o fueraborda y operan en aguas cercanas a la costa. La Organización de las Naciones Unidas para la Agricultura y la Alimentación está fomentando el desarrollo de la pesca local para proporcionar seguridad alimentaria a las comunidades costeras y ayudar a aliviar la pobreza.

Además del stock silvestre, la acuicultura produjo alrededor de de productos alimenticios y no alimenticios en 2010, un máximo histórico. Se cultivaron alrededor de seiscientas especies de plantas y animales, algunas para su uso en la siembra de poblaciones silvestres. Los animales criados incluyen peces, reptiles acuáticos, crustáceos, moluscos, pepinos y erizos de mar, ascidias y medusas. La maricultura integrada tiene la ventaja de que hay un suministro fácilmente disponible de alimentos planctónicos y los desechos se eliminan naturalmente. Se emplean varios métodos: los recintos de malla, para peces, se suspendenen mar abierto, las jaulas se usan en aguas más protegidas o los estanques se pueden refrescar con agua en cada marea alta; los camarones se crían en estanques poco profundos conectados al mar abierto; se cuelgan cuerdas en el agua para cultivar algas, ostras y mejillones; las ostrastambién se crían en bandejas o en tubos de malla; los pepinos de mar se crían en el fondo marino. Los programas de cría en cautividad han criado larvas de langosta para la liberación de alevines en la naturaleza, lo que ha aumentado la cosecha de langosta en Maine. Al menos 145 especies de algas marinas —algas rojas, verdes y marrones— se comen en todo el mundo, y algunas han sido cultivadas durante mucho tiempo en Japón y otros países asiáticos; hay un gran potencial para la algacultura adicional. Pocas plantas de floración marítima se usan ampliamente como alimento, pero un ejemplo es el samphire de pantano que se come crudo y cocido.. Una gran dificultad para la acuicultura es la tendencia hacia el monocultivo y el riesgo asociado de enfermedad generalizada. En la década de 1990, la enfermedad acabó con las vieiras y los camarones blancos cultivados en China y requirió su reemplazo por otras especies. La acuicultura también está asociada con riesgos ambientales; por ejemplo, la camaronicultura ha causado la destrucción de importantes manglares en todo el sudeste asiático.

El uso del mar para el ocio se desarrolló en el siglo XIX y se convirtió en una industria importante en el siglo XX. Las actividades de ocio marítimo son variadas e incluyen viajes autoorganizados de crucero, navegación de recreo o deportiva, regatas a motor y pesca recreativa; viajes organizados en cruceros comerciales; y viajes en embarcaciones más pequeñas para el ecoturismo como el avistamiento de cetáceos y la observación de aves costeras.

Los humanos gozan al aventurarse en el mar; los niños reman y chapotean en las aguas poco profundas y muchas personas disfrutan bañándose y relajándose en la playa. Este no fue siempre el caso, ya que el baño de mar se convirtió en moda en Europa en el siglo XVIII después de que el doctor William Buchan defendiera la práctica de la natación por razones de salud. El surfing es un deporte en el que un surfista cabalga a una ola, con o sin tabla de surf. Otros deportes acuáticos marinos son el kite surf, donde una cometa impulsa una tabla tripulada a través del agua, el windsurf, en el que la tracción es provista por una vela fija y maniobrable y el esquí acuático, donde se usa una lancha motora para tirar de un esquiador.

Bajo la superficie, el buceo libre está necesariamente restringido a descensos poco profundos. Los buceadores de perlas tradicionalmente se han engrasado la piel, se han puesto algodón en las orejas y pinzas en la nariz y se han zambullido hasta los 12 m con cestas para recolectar ostras. Los ojos humanos no están adaptados para su uso bajo el agua, pero la visión se puede mejorar usando una máscara de buceo. Otro equipo útil incluye aletas y tubos de snorkel, y el equipo de buceo permite la respiración bajo el agua y, por lo tanto, se puede pasar más tiempo debajo de la superficie. Las profundidades que pueden alcanzar los buzos y el tiempo que pueden permanecer bajo el agua está limitado por el aumento de la presión que experimentan a medida que descienden y la necesidad de prevenir la enfermedad de descompresión a medida que regresan a la superficie. Se aconseja a los buzos recreativos que se limiten a profundidades de más allá de las cuales aumenta el peligro de narcosis de nitrógeno. Las inmersiones más profundas se pueden realizar con equipos especializados y entrenamiento.

El mar ofrece un suministro muy grande de energía transportada por las olas oceánicas, las mareas, las diferencias de salinidad y las diferencias de temperatura del océano que se pueden aprovechar para generar electricidad. Formas de energía verde marina son la energía mareomotriz, energía de las corrientes marinas, energía osmótica, la energía maremotérmica y energía de las olas.
La energía de las mareas usa generadores para producir electricidad a partir de los flujos de las mareas, a veces usando una presa para almacenar y luego liberar el agua de mar. La presa de Rance, de 1 km de largo, cerca de Saint-Malo, en Bretaña, se inauguró en 1967, que genera alrededor de 0.5 GW, ha sido seguida por algunos esquemas similares.
La energía grande y altamente variable de las olas les da una enorme capacidad destructiva, lo que hace que el desarrollo de máquinas de olas asequibles y confiables sea problemático. Una pequeña planta de energía de olas comercial de 2 MW, "Osprey", se construyó en el norte de Escocia en 1995 a unos 300 metros de la costa. Pronto fue dañado por las olas, luego destruida por una tormenta. La energía de la corriente marina podría proporcionar a las zonas pobladas cercanas al mar una parte importante de sus necesidades energéticas. En principio, podría ser aprovechada por turbinas de flujo abierto; Los sistemas de fondos marinos están disponibles, pero limitados a una profundidad de aproximadamente .

La energía eólica marina es capturada por aerogeneradores ubicados en el mar; tiene la ventaja de que las velocidades del viento son más altas que en tierra, aunque los parques eólicos son más costosos de construir en alta mar. El primer parque eólico marino se instaló en Dinamarca en 1991, y la capacidad instalada de los parques eólicos marinos europeos alcanzó los 3 GW en 2010.

Las centrales eléctricas a menudo se encuentran en la costa o al lado de un estuario para que el mar se pueda usar como un disipador de calor. Un disipador de calor más frío permite una generación de energía más eficiente, lo cual es importante para las costosas centrales nucleares en particular.

El fondo marino contiene enormes reservas de minerales que pueden explotarse mediante dragado. Esto tiene ventajas sobre la minería terrestre, ya que los equipos se pueden construir en astilleros especializados y los costos de infraestructura son más bajos. Las desventajas incluyen problemas causados ​​por las olas y las mareas, la tendencia de las excavaciones a sedimentarse y el lavado de los montones de escombros. Existe el riesgo de erosión costera y daños ambientales.

Los depósitos de sulfuro masivos del fondo marino son fuentes potenciales de plata, oro, cobre, plomo y zinc y metales traza desde su descubrimiento en la década de 1960. Se forman cuando se emite agua calentada geotérmicamente por respiraderos hidrotermales de aguas profundas conocidos como "fumarolas negras". Los minerales son de alta calidad pero de extracción prohibitiva. La minería a pequeña escala del fondo marino se está desarrollando frente a la costa de Papúa Nueva Guinea utilizando técnicas robóticas, pero los obstáculos son formidables.

Hay grandes depósitos de petróleo y gas natural en las rocas debajo del fondo marino. Las plataformas marinas y las perforaciones rotatorias extraen el petróleo o el gas y lo almacenan para su transporte a tierra. La producción de petróleo y de gas en alta mar puede ser difícil debido al entorno remoto y hostil. La perforación de petróleo en el mar tiene impactos ambientales. Los animales pueden estar desorientados por las ondas sísmicas que se utilizan para localizar los depósitos, lo que probablemente provoque el varamiento de las ballenas. Se pueden liberar sustancias tóxicas como el mercurio, el plomo y el arsénico. La infraestructura puede causar daños y se puede derramar petróleo.

Hay grandes cantidades de clatrato de metano en el lecho marino y en los sedimentos oceánicos a una temperatura de alrededor de y estos son de interés como fuente potencial de energía. Algunas estimaciones establecen la cantidad disponible entre 1 y 5 millones de km³. También en el fondo marino hay nódulos de manganeso formados por capas de hierro, manganeso y otros hidróxidos alrededor de un núcleo. En el Pacífico, estos pueden cubrir hasta el 30% del fondo del océano profundo. Los minerales precipitan del agua de mar y crecen muy lentamente. La extracción comercial de níquel se investigó en la década de 1970, pero se abandonó en favor de fuentes más convenientes. En lugares adecuados, los diamantes se recogen del fondo marino utilizando mangueras de succión para llevar la grava a tierra. En aguas más profundas, se utilizan rastreadores móviles del fondo marino y los depósitos se bombean a una embarcación arriba. En Namibia, ahora se recolectan más diamantes de fuentes marinas que por métodos convencionales en tierra.

El mar contiene enormes cantidades de valiosos minerales disueltos. El más importante, la sal para uso industrial y de mesa, se ha cosechado por evaporación solar de estanques poco profundos desde tiempos prehistóricos. El bromo, acumulado después de ser lixiviado de la tierra, se recupera económicamente en el mar Muerto, donde se produce a 55,000 partes por millón (ppm).

La desalinización es la técnica de eliminar sales del agua de mar para dejar agua fresca apta para beber o irrigar. Los dos métodos principales de procesamiento, destilación al vacío y ósmosis inversa, utilizan grandes cantidades de energía. La desalinización normalmente solo se realiza cuando el agua dulce de otras fuentes es escasa o la energía es abundante, como en el exceso de calor generado por las centrales eléctricas. La salmuera producida como subproducto contiene algunos materiales tóxicos y se devuelve al mar.

Muchas sustancias acaban en el mar como resultado de actividades humanas. Los productos de la combustión se transportan en el aire y se depositan en el mar por precipitación; los efluentes industriales y las aguas residuales contribuyen con metales pesados, pesticidas, PCBs, desinfectantes, productos de limpieza del hogar y otros productos químicos sintéticos. Estos se concentran en la película superficial y en los sedimentos marinos, especialmente en el lodo estuarino. El resultado de toda esa contaminación es en gran parte desconocido debido a la gran cantidad de sustancias involucradas y a la falta de información sobre sus efectos biológicos. Los metales pesados ​​de mayor preocupación son el cobre, el plomo, el mercurio, el cadmio y el zinc que pueden ser bioacumulados por los invertebrados marinos. Son toxinas acumulativas y pasan a la cadena alimentaria.

Mucha de la basura plástica flotante no se biodegrada, sino que se desintegra con el tiempo y finalmente se descompone a nivel molecular. Los plásticos rígidos pueden flotar durante años. En el centro del giro del Pacífico hay una acumulación flotante permanente de residuos plásticos en su mayoría y hay un mancha de basura similar en el Atlántico. Las aves marinas que se alimentan, como el albatros y el petrel, pueden confundir los desechos con la comida y acumular plástico no digerible en sus sistemas digestivos. Se han encontrado tortugas y ballenas con bolsas de plástico y sedal en sus estómagos. Los microplásticos puede hundirse, amenazando los alimentadores de filtro en el fondo marino.

La mayor parte de la contaminación por petróleo en el mar proviene de las ciudades y la industria. El petróleo es peligroso para los animales marinos. Puede obstruir las plumas de las aves marinas, reduciendo su efecto aislante y la flotabilidad de las aves, y puede ingerirse cuando se acicalan para intentar eliminar el contaminante. Los mamíferos marinos se ven menos afectados pero pueden enfriarse mediante la eliminación de su aislamiento, cegarse, deshidratarse o envenenarse. Los invertebrados bentónicos se inundan cuando el aceite se hunde, los peces se envenenan y la cadena alimentaria se interrumpe. A corto plazo, los derrames de petróleo provocan que las poblaciones de vida silvestre disminuyan y se desequilibren, las actividades de ocio se vean afectadas y los medios de vida de las personas que dependen del mar sean devastados. El ambiente marino tiene propiedades de autolimpieza y las bacterias naturales actuarán con el tiempo para eliminar el petróleo del mar. En el golfo de México, donde las bacterias que consumen petróleo ya están presentes, solo toma unos días consumir el petróleo derramado.

La escorrentía de fertilizantes de tierras agrícolas es una fuente importante de contaminación en algunas áreas y la descarga de aguas residuales sin tratar tiene un efecto similar. Los nutrientes adicionales proporcionados por estas fuentes pueden causar un crecimiento excesivo de las plantas. El nitrógeno es a menudo el factor limitante en los sistemas marinos, y con el nitrógeno añadido, las floraciones de algas y las mareas rojas pueden reducir el nivel de oxígeno del agua y matar a los animales marinos. Tales eventos ya han creado zonas muertas en el mar Báltico y en el golfo de México. Algunas proliferaciones de algas son causadas por cianobacterias que hacen que los mariscos que filtran los alimentos se vuelvan tóxicos y dañen a los animales como las nutrias de mar. Las instalaciones nucleares también pueden contaminar. El mar de Irlanda fue contaminado por cesio-137 radioactivo de la antigua planta de procesamiento de combustible nuclear de Sellafield y los accidentes nucleares también pueden hacer que el material radiactivo se filtre en el mar, al igual que el desastre en la central nuclear de Fukushima Daiichi en 2011.
En 1881, el escritor y geógrafo John Francon Williams publicó un trabajo seminal sobre "The Geography of the Oceans" [Geografía de los océanos] en el que afirmaba: «Por lo tanto, puede decirse realmente que el océano actual no es más que un vasto taller, donde se elaboran y conservan los materiales de los futuros continentes.» Una declaración profunda que nunca ha sonado más relevante dada la amenaza actual para el equilibrio ecológico de los océanos que plantean los contaminantes modernos como los desechos plásticos, los derrames de petróleo y otras toxinas. El vertido de desechos (incluidos petróleo, líquidos nocivos, aguas residuales y basura) en el mar se rige por el derecho internacional. La Convención de Londres ("Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter", 1972) es un acuerdo de las Naciones Unidas para controlar el vertido en el océano, que había sido ratificado por 89 países el 8 de junio de 2012. MARPOL 73/78 es una convención para minimizar la contaminación de los mares por parte de los barcos. En mayo de 2013, 152 naciones marítimas habían ratificado MARPOL.

Varios grupos indígenas nómadas en el sudeste marítimo asiático viven en botes y obtienen casi todo lo que necesitan del mar. El pueblo moken vive en las costas de Tailandia y Birmania y en las islas del mar de Andaman. El pueblo bajau es originario del archipiélago de Sulu, Mindanao y el norte de Borneo. Algunos gitanos del mar son buceadores libres, capaces de descender a profundidades de , aunque muchos están adoptando una forma de vida más asentada y terrestre.

Los pueblos indígenas del Ártico, como los chukchi, inuit, inuvialuit y yupik, cazan mamíferos marinos como focas y ballenas, y los isleños del estrecho de Torres de Australia incluyen la propiedad de la Gran Barrera de Coral entre sus posesiones. Viven una vida tradicional en las islas que implica la caza, la pesca, la jardinería y el comercio con los pueblos vecinos de Papúa y los aborígenes australianos continentales.

El mar aparece en la cultura humana de maneras contradictorias, como poderoso pero sereno y como bello pero peligroso. Tiene su lugar en la literatura, el arte, la poesía, el cine, el teatro, la música clásica, la mitología y la interpretación de sueños. Los antiguos lo personificaron, creyendo que estaba bajo el control de un ser que necesitaba ser apaciguado, y simbólicamente, ha sido percibido como un ambiente hostil poblado por criaturas fantásticas; el Leviatán de la Biblia, Escila en la mitología griega, Isonade en la mitología japonesa,y el kraken de la mitología nórdica tardía. Las civilizaciones han avanzado a través del comercio marítimo y el intercambio de ideas.

El mar y los barcos, como tema, han sido abundantemente abordados en la pintura, habiéndose creado un vasto género de origen muy antiguo, el de la marina, que comprende toda obra pictórica cuyo tema principal es el mar. Abarca desde simples dibujos en las paredes de las cabañas en el archipiélago de Lamu hasta los paisajes marinos de Joseph Turner. En la pintura del Siglo de oro neerlandés, artistas como Jan Porcellis, Hendrick Dubbels, Willem van de Velde el Viejo y su hijo, y Ludolf Bakhuizen celebraron el mar y la Marina neerlandesa en la cima de su destreza militar. El artista japonés Katsushika Hokusai creó impresiones en color de los estados de ánimo del mar, incluyendo La gran ola de Kanagawa. 

La música también ha sido inspirada por el océano, a veces de compositores que vivieron o trabajaron cerca de la costa y vieron sus muchos aspectos diferentes. Las salomas o canciones de mar, canciones que fueron cantadas por los marineros para ayudarlos a realizar tareas arduas, se han entretejido en composiciones y se han creado impresiones en la música de aguas tranquilas, de las olas y de las tormentas en el mar. Destacadas composiciones de música clásica relacionada con el mar son la ópera "El holandés errante" (1843) de Richard Wagner; "La mer, trois esquisses symphoniques pour orchestre" (1903-1905), de Claude Debussy; las "Songs of the Sea" (1904), de Charles Villiers Stanford; la "Sea Pictures" (1899), de Edward Elgar; y la "A Sea Symphony" (1903-1909) de Ralph Vaughan Williams. En 1946 el compositor francés Charles Trenet graba el tema titulado "La mer", que supuso su mayor éxito, y que tuvo numerosas versiones (más de cuatrocientas).

Como símbolo, el mar ha desempeñado durante siglos un papel en la literatura, en la poesía y en los sueños. A veces aparece allí como un fondo suave, pero a menudo introduce temas como las tormentas, los naufragios, las batallas, las dificultades y desastres, la carrera de las esperanzas o la muerte. En su poema épico "La Odisea", escrito en el , Homero describe el viaje de diez años del héroe griego Odiseo que lucha por regresar a casa a través de los numerosos peligros del mar después de la guerra descrita en la Ilíada. El mar es un tema recurrente en los poemas haiku del poeta japonés del período Edo Matsuo Bashō (松尾 芭蕉) (1644-1694). En la literatura moderna, Joseph Conrad ha escrito novelas inspiradas en el mar, extraídas de su experiencia en el mar, Herman Wouk y Herman Melville. En las obras del psiquiatra Carl Jung, el mar simboliza el inconsciente colectivo y personal en la interpretación de los sueños, y las profundidades del mar simbolizan las profundidades de la mente inconsciente. Aunque el origen de la vida en la Tierra todavía es un tema de debate, la científica y escritora Rachel Carson, en su galardonado libro de 1951 "The Sea Around Us", escribió: «Es una situación curiosa que el mar, de donde la vida surgió primero, ahora debería verse amenazado por las actividades de una forma de esa vida. Pero el mar, aunque cambiado de una manera siniestra, seguirá existiendo: la amenaza es más bien la vida misma».

El mar aparece como objeto en algunos de los ensayos de la historiografía, por ejemplo: "El mar" de Jules Michelet o en "El Mediterráneo y el mundo mediterráneo en la época de Felipe II" (1946) de Fernand Braudel. Dice Michelet: «Mucho antes de vislumbrarse el mar, se oye y se adivina el temible elemento. Primero un rumor lejano, sordo y uniforme. Poco a poco cesan todos los ruidos dominados por aquel. No tarda en notarse la solemne alternativa, la vuelta invariable de la misma nota, fuerte y profunda, que corre más y más, y brama».



</doc>
<doc id="11452" url="https://es.wikipedia.org/wiki?curid=11452" title="Mar Cantábrico">
Mar Cantábrico

El mar Cantábrico es el mar litoral del océano Atlántico que baña parte de la costa norte de España y el extremo suroeste de la costa atlántica de Francia; supone la zona sur del golfo de Vizcaya. Se extiende desde el cabo Estaca de Bares, en la provincia de La Coruña, hasta la desembocadura del río Adur, cerca de la ciudad de Bayona, en la costa del departamento de Pirineos Atlánticos, en el País Vasco Francés. Baña 800 kilómetros de costa compartida por las provincias de La Coruña y Lugo (Galicia), Asturias, Cantabria, Vizcaya y Guipúzcoa (País Vasco), y el departamento francés de Pirineos Atlánticos.

Constituye un mar de transición entre los mares fríos del norte y los templados del trópico, lo que hace que sea ecotono de especies vegetales y animales de aguas frías. El afloramiento de aguas profundas y frías existente frente a las costas gallegas hace que la temperatura del agua aumente conforme nos desplazamos hacia el Este. Esa temperatura del agua superficial presenta una acusada estacionalidad, así durante el invierno la temperatura del agua puede bajar hasta los 11 °C, mientras que en verano alcanza los 22 °C aproximadamente. A partir de 35 o 40 m de profundidad la temperatura del agua se mantiene prácticamente estable durante todo el año. Estas temperaturas son inusualmente altas dada la región geográfica que ocupa el mar Cantábrico, y se deben a los efectos cálidos de la corriente del Golfo.

Los fuertes vientos, del Noroeste preferentemente, que soplan sobre el mar Cantábrico tienen su origen en las bajas presiones centradas sobre las islas británicas y el mar del Norte en combinación con el anticiclón de las Azores. La distancia recorrida por el viento y el mantenimiento de su dirección y velocidad constantes hacen que se generen olas de 2 a 3 m de altura, lo que origina un mar bastante agitado. En condiciones muy particulares, más propicias en los meses de abril-mayo y septiembre-octubre, los vientos del Oeste pueden alcanzar magnitudes de galerna con olas que llegan a superar los 7 m de altura.

La salinidad media del Cantábrico es de 35 g/L, aunque varía ligeramente en función del régimen de lluvias, la mayor o menor cercanía a la costa y la presencia de desembocaduras de ríos caudalosos.

Tiene una significativa amplitud de marea, pudiendo ser de 4,5 m como máximo, especialmente en las mareas vivas de marzo.

El mar Cantábrico fue bautizado por los romanos en el siglo I a. C. como "Cantabricus Oceanus" en referencia a uno de los pueblos que poblaban sus costas: los cántabros. En otras citas clásicas más antiguas aparece con el nombre de "Britannicus Oceanus" y "Gallicus Oceanus". En sus costas se crearon asentamientos humanos de tribus de astures, cántabros, autrigones, caristios y várdulos que hicieron de la pesca su principal actividad económica, aunque esta en esos tiempos primitivos no fue muy importante. No obstante la presencia de determinados restos arqueológicos anteriores a la conquista romana, así como investigaciones en el campo de la genética, demuestran la existencia de contactos marítimos más allá de las costas cantábricas.
Hallazgos como el del caldero de Cabárceno, similar a los encontrados cerca de Battersea (Londres) o en Dublín (Irlanda), o la diadema de Moñes, con representaciones de la mitología celta irlandesa y galesa, parecen evidenciar que existieron intercambios comerciales marítimos al menos 600 años antes de la llegada de los romanos entre los pobladores de la zona norte de la Península y los de la zona atlántica europea, especialmente con los habitantes de las Islas Británicas.

Con la decadencia romana se inicia un periodo convulso y con escasas fuentes historiográficas. Hidacio señala que hacia el año 456 d. C. la costa fue devastada por expediciones de hérulos. Durante este periodo el litoral cantábrico eran costas indefensas frente a las "razzias" de los pueblos del norte de Europa. Aunque el mar probablemente sí estaría abierto a la navegación de cabotaje, no se tiene constancia de la existencia de comercio o intercambios marítimos. Se desconoce si los puertos romanos seguían aun activos, estaban abandonados o únicamente daban servicio a intercambios locales.
Posteriormente se registra la primera llegada de los vikingos a la península ibérica a través del mar Cantábrico en el siglo IX, quienes intentaron asaltar y saquear numerosos pueblos costeros aunque fueron derrotados en numerosas ocasiones e inicialmente por Ramiro I de Asturias.
La pesca se convirtió en una importante actividad económica en el mar Cantábrico, especialmente las capturas de ballenas, hoy extintas en la región.

El mar Cantábrico ha sido considerado tradicionalmente un "mare tenebrosum", cerrado, peligroso y de difícil tránsito. No obstante, las investigaciones arqueológicas actualmente están cuestionando esta visión. Desde finales del siglo I d. C., de sus resguardadas bahías y ensenadas surgieron asentamientos que con el tiempo llegaron a tener gran importancia, como demuestra el surgimiento de la Hermandad de las Cuatro Villas o de la de las Marismas, federaciones de puertos que conformaron un poder naval y económico de primer orden en el Arco Atlántico.

Principales municipios a la orilla del mar Cantábrico:


</doc>
<doc id="11453" url="https://es.wikipedia.org/wiki?curid=11453" title="Corriente del Golfo">
Corriente del Golfo

La Corriente del Golfo es una corriente oceánica cálida y rápida del Océano Atlántico que se origina en el Golfo de México y se extiende hasta la punta de Florida, y sigue las costas orientales de los Estados Unidos y Terranova antes de cruzar El Océano Atlántico como la Corriente del Atlántico Norte. Es una corriente superficial (por la temperatura cálida de sus aguas) y disminuye gradualmente en profundidad y velocidad hasta prácticamente anularse a unos 100 m, cota donde la influencia del calentamiento por los rayos solares desaparece en la práctica. Tiene una anchura de más de 1000 km en gran parte de su larga trayectoria, lo que da una idea aproximada de la enorme cantidad de energía que transporta y de las consecuencias tan beneficiosas de la misma. Se desplaza a 1,8 m/s aproximadamente y su caudal es enorme: unos 80 millones de m³/s.

La circulación de esta corriente asegura a Europa un clima cálido para la latitud en que se encuentra. También determina en buena parte la flora y la fauna marina de los lugares por los que pasa (por ejemplo, los artrópodos y cefalópodos abundan más en las costas de Galicia que en las del País Vasco, donde su influencia es menor).

Es provocada por la acción combinada del movimiento de rotación terrestre (y en menor grado el de traslación) y de la configuración de las costas tanto americanas como europeas.

La corriente del Golfo se forma en el Golfo de México (de ahí su nombre) desde donde sale al Atlántico por el estrecho de Florida. Es la corriente de borde oeste de la circulación anticiclónica del Atlántico norte. El punto donde termina ha sido motivo de controversia, pero se considera que la corriente del Golfo propiamente dicha finaliza a aproximadamente 40°N y 50°O donde el flujo no cesa sino que sus aguas cálidas y saladas siguen fluyendo por un lado hacia el norte, en la corriente del Atlántico Norte (también llamada deriva del Atlántico Norte) y la corriente de Noruega que la prolongan, y por otro lado hacia el sur vía la corriente de las Islas Canarias.
Ya desde el primer viaje de Colón, los españoles comprobaron la dificultad de navegar hacia el oeste a una latitud superior al trópico de Cáncer, rumbo que los hizo retrasar considerablemente en su recorrido. De hecho, en los cuatro viajes que Colón realizó, el primero fue el único que siguió este rumbo. Esta ruta atravesaba el cinturón de altas presiones de lo que ahora se conoce como el anticiclón de las Azores, donde los vientos son relativamente débiles y abundan los días de calma. Y cuando ya estaban relativamente cerca de las tierras americanas (que en un primer momento se llamaron las Indias Occidentales), se encontraron con el mar de los Sargazos (nombre de origen griego, empleado por Aristóteles para denominar una parte del mar en la que abundan las algas y utilizado posteriormente para indicar un área extensa ubicada al este-noreste de las Grandes Antillas), donde abundan las algas de este nombre, lo cual fue interpretado, erróneamente, como un obstáculo que frenaba el viaje de las embarcaciones. Como después se pudo comprobar, las algas, que son más ligeras que el agua para flotar, no ofrecen ninguna clase de resistencia a la navegación.

En la misma obra de Anglería se intuye la existencia de una fuerza superior a la de los vientos que hacía desviar las embarcaciones, indicando las experiencias de Bartolomé Colón (hermano de Cristóbal) en las costas de La Española (ahora Santo Domingo), tan temprano como en 1497. 

El descubrimiento por parte de los europeos de la corriente del Golfo data de 1513, año de la expedición de Juan Ponce de León, fundador de la provincia de la Florida y explorador de las costas de esta península. Navegando hacia el sur a lo largo de las costas orientales de la Florida, con viento en popa (aunque débil) se dieron cuenta de que su embarcación retrocedía en lugar de avanzar. El descubrimiento se atribuye a su piloto, Antón de Alaminos. A partir de dicha fecha fue ampliamente utilizada por los barcos españoles en su viaje de vuelta del Caribe a España.

Otro autor que identifica con precisión la naturaleza de la corriente del Golfo es Jerry Wilkinson, en un artículo reciente sobre este tema:

El primero que publicó descripciones detalladas y mapas precisos de la corriente del Golfo fue Benjamin Franklin en su obra de 1786 "Sundry Maritime Observations".

Los pescadores norteamericanos, en particular los balleneros cuya área de pesca se extendía de Terranova a las Bahamas y las Azores, se habían dado cuenta de que las ballenas evitaban las aguas cálidas de la Corriente y se mantenían en sus bordes. Transmitieron sus conocimientos a los capitanes norteamericanos de navíos que modificaron su ruta ganando así dos semanas en el trayecto de América a Gran Bretaña. En 1769, la Oficina de Aduanas de Boston se quejó a las autoridades británicas de que los navíos británicos tardaban más que los americanos en realizar el trayecto. Franklin, por entonces Responsable General de Correos de Nueva Inglaterra, consultó a su primo, Thomas Folger, capitán de navío y antiguo ballenero basado en Londres. Siguiendo sus indicaciones, mandó a cartografiar la corriente del Golfo en 1669-1770 pero el mapa fue rechazado por el Almirantazgo y los capitanes ingleses que mantenían que el camino más corto tenía que ser el más rápido y se negaban a aceptar los consejos de simples pescadores americanos.

Franklin decidió estudiar el fenómeno y en sus numerosos viajes entre América y Europa tomó medidas sistemáticas de la temperatura de las aguas. Constató que las corrientes norte-sur eran más frías que las que fluían en sentido contrario, y concluyó que el termómetro podía ser también una útil herramienta de navegación.
Con Franklin la corriente del Golfo adquirió nombre propio ("Gulf Stream" en inglés) y abrió el camino al estudio de la oceanografía física. Las autoridades británicas empezaron a dar instrucciones a sus navíos para que tomaran mediciones de la Corriente, y los cuadernos de bitácora de los buques se convirtieron en la principal fuente de información sobre las corrientes marinas. Entre 1810 y 1830 el geógrafo británico James Rennell compiló estos datos para cartografiar las corrientes del océano Atlántico con un interés particular en la del Golfo. Su obra "Currents of the Atlantic Ocean", publicada en 1832, es la primera síntesis científica exhaustiva sobre el tema. 

El geógrafo Matthew Fontaine Maury, del Observatorio Naval de los Estados Unidos, retomó los trabajos de Rennell y realizó mapas de vientos y de corrientes para la navegación, promediando datos recogidos en cuadernos de bitácora entre 1840 y 1850. Impulsó en ese aspecto la cooperación internacional, en la primera conferencia internacional de meteorología de Bruselas en 1853, del que fue el iniciador, y dio los primeros pasos hacia la «oceanografía sinóptica». A partir de 1844 se realizaron estudios sistemáticos de la corriente del Golfo sobre la base de mediciones de la temperatura en superficie y en profundidad, desde la costa hacia el mar abierto. Pero los medios disponibles, ligados a barcos oceanográficos lentos y de autonomía limitada, no permitían apreciar la variabilidad de la dinámica oceánica ni medir la velocidad de las corrientes a gran profundidad. Hubo que esperar hasta los años 1960-1970, cuando se desarrollaron sistemas espaciales que permitieron cubrir en tiempo real todos los océanos y desplegar instrumentos de medida localizados y comunicados por satélite.

El avance tecnológico del siglo XIX con el desarrollo de la Revolución Industrial produjo una era de optimismo y fe en el poder de la ciencia y de la técnica, que fue muy importante en la Gran Bretaña, el país donde la revolución industrial tuvo mayor desarrollo inicial. Y como ya hemos visto, fueron geógrafos ingleses y norteamericanos los primeros en desarrollar la cartografía del océano Atlántico para incluir la dirección de las corrientes marinas que hubieran permitido, ya entonces, el diseño de una ruta segura entre Europa y América del Norte. Sin embargo, no se hizo así, y la catástrofe del Titanic, de la que se están cumpliendo en el 2020, 108 años, vino a convertirse en un recordatorio, no del fracaso de la ciencia sino del desfase casi siempre inevitable entre el conocimiento científico y sus aplicaciones técnicas. El lugar de dicho hundimiento ( se encuentra al sureste de la Isla de Terranova, donde la Corriente del Labrador (no la del Golfo) llevó un gran iceberg desde el noroeste de Groenlandia hasta el punto donde ocasionó el hundimiento del transatlántico más grande y moderno del inicio del siglo XX. En esta imagen satelital a pequeña escala también puede verse la ubicación de Nueva York, adonde se dirigía el Titanic. Se trata de una zona costera que abarca la isla Larga (Long Island, visible algo más al sur, en la costa americana, y una parte en otra isla (Manhattan) que queda a la izquierda, es decir, al oeste de Long Island. La extensión de Nueva York se distingue por la zona edificada de color claro, en comparación a la zona verde (vegetación) y azul del océano Atlántico. Se hace esta aclaratoria para que se entienda la larga trayectoria de un iceberg en cuanto a su latitud desde la zona de su procedencia hasta el sitio donde hundió al Titanic (varios miles de km). Este largo recorrido contrasta con el reducido trayecto de los grandes icebergs de la Antártida (algunos alcanzan más de 200 km de longitud) y que desaparecen (en muchos casos, después de moverse cerca del continente antártico durante varios años) al encontrarse con las aguas mucho más cálidas de la corriente circumpolar antártica. Después de la tragedia del Titanic, se tomaron muchas precauciones con el desplazamiento de los icebergs y en algunos casos, se bombardeaban con los barcos de guerra para romperlos y destruir su poder destructivo. La segunda guerra mundial vino a cambiar radicalmente la situación, especialmente, por la invención del radar y por el desarrollo de las nuevas técnicas de comunicación (imágenes aéreas y, más recientemente, imágenes satelitales).



</doc>
<doc id="11454" url="https://es.wikipedia.org/wiki?curid=11454" title="Golfo de Vizcaya">
Golfo de Vizcaya

El golfo de Vizcaya (; ) es un amplio golfo del océano Atlántico Norte localizado en la parte occidental de Europa. Se extiende desde el cabo Ortegal en Galicia (España) hasta la punta de Pern en la isla de Ouessant, en Bretaña (Francia). Baña las costas de las comunidades autónomas españolas de Galicia, Asturias, Cantabria y el País Vasco, así como las regiones francesas de Nueva Aquitania, Países del Loira y Bretaña.

En España se considera el golfo de Vizcaya como la parte más oriental del mar Cantábrico, con la que se designa el mar litoral que baña la costa norte de España y la costa suroeste de Francia y que correspondería con lo que los romanos en el siglo I a. C. nombraron como "Sinus Cantabrorum" («bahía de los cántabros»). La parte más septentrional era denominada "Sinus Aquitanus" o "Mare Aquitanicum" (mar de los aquitanos).

Su costa en el sur es escarpada, con multitud de acantilados entre los que se abren playas y pequeñas bahías, generalmente en las desembocaduras de los ríos (que suelen ser en forma de rías). Destacan la rasa mareal de la costa guipuzcoana, entre Deba y Zumaya (que se extiende en menor medida hasta Ondárroa), y las playas de Gijón, Llanes, Santander, Laredo, Comillas, Laga, Deba, Zarauz o San Sebastián, mientras que la costa francesa es recta, baja y arenosa, con dunas y numerosos pantanos, del sur hasta la desembocadura del Loira; en el tercio norte, en la costa de Bretaña, alternan tramos rocosos y elevados, con numerosas ensenadas, bahías y amplias playas de arena.

Los ríos que desembocan en la costa sur de este golfo son de corto recorrido, como todos los de la vertiente cantábrica. En cambio los de la costa este (Garona que desemboca en la ciudad de Burdeos o Loira) tienen un gran recorrido, volviendo a ser de curso corto al norte (en Bretaña). Entre los españoles destacan el Nalón, siendo el río más largo y caudaloso del área española, el Nervión, que forma la ría de Bilbao, y el Bidasoa, que marca en parte la frontera entre Francia y España.

La corriente de Navidad son las aguas cálidas superficiales que fluyen por la costa atlántica de la península ibérica de sur a norte y la costa cantábrica de oeste a este, hasta chocar con la costa continental francesa para desplazarse al norte. Los meses de noviembre a marzo. en una franja de aproximadamente 50 km de ancho.

Los vientos fuertes del suroeste que la generan se originan con las bajas presiones centradas sobre las Islas Británicas y el mar del Norte, combinadas con el anticiclón de las Azores.

Esta corriente es la culpable de que parte del vertido de chapapote o galipot del naufragio del Prestige llegara hasta las costas francesas desde más allá de la ciudad de La Coruña en Galicia.

Respecto a la batimetría, el golfo de Vizcaya presenta dos plataformas continentales separadas por una amplia llanura abisal con una profundidad máxima de 2789 m. Esta llanura fue creada por la separación y rotación de la placa ibérica con respecto a la eurasiática. El límite entre ambas plataformas está definido por la fosa de Capbreton, un estrecho fiordo submarino de 2100 m de profundidad y 150 km de largo que se acerca frente al puerto de Capbreton, en el departamento francés de las Landas.

Esta disposición batimétrica y la orientación noroeste del Golfo de Vizcaya hacen que esté especialmente expuesto a los temporales provenientes del Atlántico Norte. La estrechez de la plataforma continental de la parte sur hace que las olas puedan alcanzar una altura considerable en esta costa, especialmente frente a la fosa de Capbreton.

La pesquería del golfo de Vizcaya es similar a la del mar Cantábrico. Está muy explotada y hay muchas especies que están en peligro serio de extinción, como el besugo, la merluza e incluso la anchoa. Otras, como el verdel y el bonito, gozan de buena salud y se explotan regularmente. Algunas especies ya han desaparecido del golfo de Vizcaya, como la ballena franca glacial, cuyo último ejemplar se cazó en Orio a principios del siglo XX. No obstante, es una buena región para la observación de mamíferos marinos.

Desde la antigüedad el golfo de Vizcaya ha sido una zona de tránsito naval importante. La característica de su ubicación y forma ha potenciado que se usara para la navegación entre la costa occidental continental y el norte peninsular, y entre estos puntos y las Islas Británicas (a diferencia de la región occidental del mar Cantábrico, las costas gallega y asturiana, donde se propiciaron las rutas hacia y desde Irlanda). Hay servicios regulares de ferry entre Bilbao y Portsmouth, Gijón y Nantes, y entre Santander y Plymouth.

Comprende importantes puertos comerciales, como los de Avilés, Gijón, Santander, Bilbao, Pasajes, Bayona (Francia), Burdeos, La Rochelle, Nantes o Lorient, y pesqueros como los de Vivero, Burela, Luarca, Cudillero, Candás, Llanes, San Vicente de la Barquera, Santoña, Laredo, Castro-Urdiales, Bermeo, Ondárroa, Guetaria, San Juan de Luz, Les Sables-d'Olonne o Concarneau.

La máxima autoridad internacional en materia de delimitación de mares, la Organización Hidrográfica Internacional («International Hydrographic Organization, IHO), considera el golfo de Vizcaya («Bay of Biscay») como un mar. En su publicación de referencia mundial, «Limits of oceans and seas» (Límites de océanos y mares, 3ª edición de 1953), le asigna el número de identificación 22 y lo define de la forma siguiente:


 


</doc>
<doc id="11455" url="https://es.wikipedia.org/wiki?curid=11455" title="Vizcaya">
Vizcaya

Vizcaya (en euskera y oficialmente Bizkaia) es una de las tres provincias españolas que componen la comunidad autónoma del País Vasco. Su capital y ciudad más poblada es Bilbao. Está situada en el norte de la península ibérica y limita al norte con el mar Cantábrico, al este con Guipúzcoa, al sur con Álava y con Burgos y al oeste con Cantabria. Vizcaya es una provincia muy montañosa y cuenta con un clima oceánico.

Se compone de organizados en siete comarcas. Con una superficie de 2217 km², es la segunda provincia más pequeña de España, pero es la novena más poblada () y la tercera en densidad de población (518,55 hab/km²). La mayoría de la población vive en el área metropolitana de Bilbao, que es la sexta área metropolitana más grande de España. Los idiomas oficiales son el español y el euskera; es autóctono del territorio vizcaíno el dialecto occidental del euskera.

El territorio de Vizcaya tiene su origen en el medieval señorío de Vizcaya, del que derivan derechos históricos y un régimen foral que son reconocidos por la Constitución española. Así, la Diputación Foral de Vizcaya, como el resto de las diputaciones forales vascas, cuenta con una autonomía mucho mayor que la del resto de diputaciones provinciales. Como otras provincias españolas, cuenta con un derecho foral propio. Vizcaya fue uno de los primeros focos industriales de España y contó desde finales del con una industria siderometalúrgica muy desarrollada; Altos Hornos de Vizcaya fue la empresa más grande del país durante casi todo el . No obstante, la crisis del petróleo de 1973 tuvo consecuencias económicas muy profundas en Vizcaya, y la provincia sufrió un proceso de profunda reconversión industrial.

El término "Vizcaya" o "Bizkaia" tiene una etimología discutida. Para algunos significa "cima", y sería un sinónimo de la actual palabra vasca "bizkar" (‘loma’). En el año 1141 en referencia a la cima del monte Igueldo de San Sebastián, aparece inscrito el siguiente topónimo: "Iheldo Bizchaya" (‘cima de Igueldo’). También en Navarra hay una comarca denominada «La Vizcaya», que puede significar ‘La cima’. Se han propuesto también otras etimologías, como "bits-kaia" (‘puerto de espuma’) o "bizi-kaia" ("puerto vivo"), menos probables, pues hay topónimos similares en lugares alejados del mar, como el citado de Navarra o el de Labets-Biscay.

"Bizkaia" es la denominación en euskera recomendada por la Real Academia de la Lengua Vasca, usada habitualmente en documentos oficiales en este idioma. Es usada en documentos oficiales de la Administración española (es desde 2011 la única denominación oficial), también en documentos en castellano, y es la más empleada por los medios de comunicación en castellano del País Vasco. Es también la denominación utilizada en la versión en euskera de la Constitución española y en la versión en euskera del Estatuto de Autonomía para el País Vasco.

A pesar de que es la única denominación oficial aprobada para el territorio histórico por sus Juntas Generales mediante Norma Foral 12/1986, de 15 de diciembre, de las Juntas Generales de Vizcaya, sobre "Signos de Identidad del Territorio Histórico de Bizkaia", como dicha Norma Foral no afectaba a la denominación de la provincia, ya que el Real Decreto legislativo 781/1986, de 18 de abril, por el que se aprobaba el texto refundido de las disposiciones legales vigentes en materia de régimen local, disponía en su artículo 25.2 que «sólo mediante ley aprobada por las Cortes Generales puede modificarse la denominación y capitalidad de las provincias», Vizcaya continuó siendo la única denominación oficial de la provincia según el Real Decreto de 30 de noviembre de 1833 , hasta el año 2011.

El día 29 de junio de 2004, el Grupo Parlamentario Partido Nacionalista Vasco en el Congreso de los Diputados presentó la proposición de ley núm. 122/000084 en la que proponía establecer como denominación oficial única la de "Bizkaia". Esta propuesta fue eliminada por ese mismo Grupo Parlamentario el día 9 de mayo de 2006 por falta de apoyos.

En 2011, el acuerdo presupuestario alcanzado por el PSOE y el PNV en el Congreso de los Diputados, incluyó el cambio de denominación actual, mediante el cual, la única denominación oficial del territorio vizcaíno sería la de "Bizkaia".

Vizcaya es la denominación en castellano, recomendada por la Real Academia Española. Puede ser usada en documentos no oficiales y, en general, en el ámbito escrito hispanohablante. Es también la denominación que fue utilizada en la versión en castellano de la Constitución española de 1978 y en la versión en castellano del Estatuto de Autonomía del País Vasco de 1979.

Hay constancia de poblamiento en el Paleolítico en varios lugares de Vizcaya, como en las cuevas de Bolinkoba (Abadiano), Arenaza (Galdames), Atxeta (Forua), Santimamiñe (Cortézubi) y Lumentxa (Lequeitio).

La presencia de varios castros de la Edad de los Metales,como los de Arrola, Malmasín o Bolumburu, hacen pensar en una ocupación del territorio por indoeuropeos. La arqueología actual opina: 

En la distribución de tribus prerromanas de Ptolomeo, engloba parte norte de las antiguas Caristia y Autrigonia prerromanas. En las de Estrabón, Pomponio Mela y Plinio el Viejo, corresponde a la parte occidental del territorio de los várdulos. La filiación de estas tres tribus es desconocida. Los historiadores discuten sobre su origen cántabro, vascón, indoeuropeo, celta o celtibérico sin que haya pruebas concluyentes en favor de ninguna de estas hipótesis.

Los autrigones, que en Vizcaya ocuparían las Encartaciones, no fueron mencionados por Estrabón. Otros historiadores romanos como Pomponio Mela y Plinio el Viejo los sitúan en el interior, en la zona norte de la actual provincia de Burgos (Briviesca). Plinio el Viejo alrededor del año 77 citaba «entre las diez ciudades de los autrigones "Tricio" (Tritium Autrigonum) y "Virovesca" (Briviesca) como capital del los autrigones».

Ptolomeo los sitúa lindando con cántabros al oeste y turmogos al sur, y con caristios y berones al este, y, según esta distribución, se extenderían entre el río Asón y el río Nervión. Su ciudad principal era Virovesca (Briviesca), una de las cecas de las monedas del jinete ibérico. Otras ciudades importantes fueron Tricio, en la Rioja; Deóbriga (Miranda de Ebro) y en la costa Flaviobriga (Castro-Urdiales) (aunque Plinio asigna esta ciudad a los várdulos) la última colonia fundada por los romanos en Hispania. Otros asentamientos fueron Osma de Valdegovia, Poza de la Sal y es posible que en la desembocadura del río Nerua (Nervión) tuvieron un puerto ya que se encontraron monedas romanas en la barra de Portugalete y en Bilbao. Floro y Orosio cuentan que eran frecuentemente atacados por los cántabros, por lo que posiblemente colaborasen con Augusto en las guerras cántabras y como premio obtuviesen el dominio de nuevos territorios en la cornisa cantábrica llegando casi hasta el río Deva.

Se discute si estaban emparentados a cántabros, celtíberos o vascones. Lo primero es dudoso ya que fue el ataque de cántabros contra autrigones y turmódigos lo que inició la guerra romano-cántabra. El hecho de que algunas de sus ciudades tengan la terminación "briga" parece indicar un origen céltico.

Los caristios ocupaban el resto de Vizcaya, según Ptolomeo. No son mencionados por Estrabón, ni por Pomponio Mela, pero sí por Plinio el Viejo, que les llama "Carietes" y les sitúa en el interior, en la zona sur del actual País Vasco.

Ptolomeo los sitúa entre el río Deva, en la provincia de Guipúzcoa y lo que actualmente es Bilbao, llegando por el sur hasta el Ebro. Su territorio limitaba con los de los várdulos y el de los autrigones. Sus ciudades eran "Tullica" (quizás Tuyo a la orilla del Zadorra), "Suessatio" (que podría ser la actual Zuazo) y "Veleia" (que podría ser la actual Iruña-Veleia), las dos últimas se encontraban en la calzada romana de Burdeos a Astorga.

También en este caso se discute si estaban emparentados a cántabros, celtíberos o vascones.

Se han encontrado numerosos restos de la romanización de Vizcaya, como los restos de los asentamientos costeros de Lequeitio, Portuondo o Bermeo, siendo quizás el más importante el puerto fluvial de Forua y los restos de su poblado romano.

Desde la caída del Imperio romano hasta las proximidades del año 1000, hay muy pocas noticias históricas de Vizcaya. Probablemente sufrió las devastaciones de los hérulos, ya que el cronista Hidacio, relata que 400 hérulos en siete naves atacaron la costa cántabra y de Vardulia en el año 456.

Las últimas investigaciones arqueológicas parecen indicar una expansión francoaquitana en Vizcaya a partir del , lo que se contradice con las propuestas historiográficas que se basan en una continuidad de la cultura desde la protohistoria hasta los inicios de la Edad Media:

La relación con los francos merovingios se explicaría a través del Ducado de Vasconia.

Ni las invasiones de los visigodos ni las de los musulmanes parecen haber llegado a Vizcaya, aunque probablemente sus costas fueron desoladas por los vikingos, especulándose con la posibilidad de un asentamiento vikingo en las cercanías de Mundaca, que podría ser el origen de la leyenda de Jaun Zuria.

Tras la invasión musulmana, se cree que Vizcaya quedó bajo la órbita del reino de Asturias, con algunos enfrentamientos cuyo reflejo sería la también mítica batalla de Padura. En la crónica de Alfonso III de Asturias, escrita en el , y refiriéndose al reinado de Alfonso I, es donde se hace por primera vez referencia a Vizcaya: «"Alava, Vizcaya, Alaon y Orduña siempre habían sido poseídas por sus habitantes"», por lo que no hubo necesidad de repoblarla; en cambio sí «pobló» Sopuerta y Carranza (es decir colocó bajo su control el oeste, las Encartaciones).

Tras la anexión del condado de Castilla por Sancho III de Navarra (1029), Vizcaya queda bajo la influencia del Reino de Pamplona; hasta el año 1040, cuando Íñigo López Ezquerra, conde de Vizcaya que gobernaba la Vizcaya nuclear (sin las Encartaciones ni el Duranguesado), en los enfrentamientos entre Castilla y Navarra, se alía al rey de Castilla, aprovechando para hacerse señor hereditario de Vizcaya.

En 1135 la Vizcaya nuclear vuelve a estar bajo órbita navarra, pasando definitivamente a la de Castilla el año 1180. Las Encartaciones siguen en el Reino de Castilla y el Duranguesado sigue en el de Navarra, hasta el año 1200, en que pasa a depender de Castilla. A fines del y en el la sucesión en el señorío se altera por diferencias dinásticas y por la intervención en los asuntos del reino de Castilla, donde los señores de la casa de Haro tenían feudos y emparentaban con casas nobiliarias como los Lara o la casa real.

El Señorío de Vizcaya, por herencia materna, en 1370 recae en el Infante don Juan de Castilla, que hereda de su padre el reino de Castilla, como Juan I, permaneciendo desde entonces ligado a la corona, primero a la de Castilla y luego, desde Carlos I, a la de España, siempre con la condición de que el Señor de turno jurase defender y mantener los fueros del señorío (leyes propias vizcaínas) que en su texto afirmaban que los vizcaínos, al menos en teoría, podían desobedecer al Señor que así no lo hiciera.

La crisis bajomedieval afectó a Vizcaya produciéndose una disminución de la producción agrícola, hambrunas, etc. A esta crisis se sumó la epidemia de la peste negra de 1348. Muchos campesinos murieron, y otros se refugiaron en las villas, lo cual afectó a las rentas de las casas principales.

Los intentos de mantener su prestigio y la búsqueda de ingresos llevó a los jefes de linaje a luchas de poder que dieron lugar a dos bandos: oñacinos y gamboínos, liderados los oñacinos por el Señor de la Casa de Mújica, y los gamboínos por la Casa de Urquizu de Abendaño. Así comenzaron las guerras de banderizos que asolaron Vizcaya desde la Baja Edad Media hasta principios de la Edad Moderna. Los demás linajes de Vizcaya se adscribían a uno u otro bando en función de sus intereses, siendo normal el cambio de bando. Los señores no dudaban en robar en la villas consideradas enemigas, en saquear y extorsionar a sus campesinos, ni en asaltar los convoyes de los mercaderes de Burgos que se dirigían a los puertos para exportar sus géneros.

Las Encartaciones, en 1394, adoptan el Fuero de Avellaneda, para luchar contra la conflictividad social generada por la violencia de los banderizos. Los labradores de la Tierra Llana y las Villas acudieron al rey Enrique III de Castilla, Señor de Vizcaya, para pedirle autorización para formar una Hermandad para protegerse de las tropelías de los "jaunchos". El rey, en 1393, comisiona al corregidor Gonzalo Moro, para redactar unas nuevas Ordenanzas de Hermandad, lo que se hace en Junta General, pero estas ordenanzas no llegan a aplicarse por la oposición de algunos señores del bando oñacino.

A mediados del , las peleas entre señores banderizos se transforman en una lucha de poder entre estos, que dominan la Tierra Llana, por una parte, y las Villas y la Ciudad por otra. En 1479, en la Junta de Villas reunida en Durango, acuerda formar una nueva hermandad para las Villas. Y en 1480 se acuerda que, para pacificar Vizcaya, se formará una comisión con apoderados de las villas de Bilbao, Bermeo, Lequeitio y Durango y miembros de los linajes de Butrón, Múgica, Abendaño y Arteaga para dirimir las querellas y dar fin a las peleas.

Las guerras de banderizos acaban a finales del . La puesta de las Villas bajo control administrativo de la Corona, la pujanza de las Hermandades de las villas y el reconocimiento de la hidalguía universal a todos los vizcaínos fueron elementos importantes en la pérdida de poder de los señores. Por otro lado, los valores de los señores banderizos (nobleza, honor, honra) pasaron a ser considerados propios de la sociedad vizcaína, y, añadidos a otras costumbres importadas, como el mayorazgo castellano, reforzaron la idea de una identidad propia vizcaína.

La Vizcaya medieval estaba dividida en tres partes con gobierno y jurisdicción propios:

Al ir siendo dotadas las Villas y la Ciudad de cartas pueblas y fueros particulares durante los siglos y , éstas dejaban de depender de los fueros de Vizcaya, Encartaciones o Durango, y pasaban a celebrar sus Juntas separadamente. Las villas y el año de concesión de fueros eran: Valmaseda (1199), Bermeo (1236), Lanestosa (1287), Plencia (1299), Bilbao (1301), Ochandiano (1304), Portugalete (1322), Lequeitio (1325), Ondárroa (1327), Areatza (1338), Marquina (1355), Elorrio (1356), Guernica (1366), Durango (1372), Ermua (1372), Miravalles (1375), Munguía (1376), Larrabezua (1376) y Rigoitia (1376); la ciudad es Orduña (1228).

Las Villas y la Ciudad, las Encartaciones y la merindad de Durango sólo acudían a las Juntas Generales de Guernica enviando representantes cuando se iban a tratar temas comunes que les afectasen.

El año 1068 Sancho II de Castilla concedió a la sede episcopal de Oca permiso para pescar en varios puertos cántabros. Se cree que, entre finales del y principios del la población, que había abandonado la franja costera vizcaína por temor a los ataques vikingos, vuelve a ocuparla y van apareciendo las localidades y asentamientos que conocemos en la actualidad. Pero no hay referencias escritas hasta el año 1082, en la donación de la ermita de San Miguel en Bermeo: «"et illa ecclesia S. Micaelis arcangeli in portu de Vermelio, in ora maris, cum suos morturos ad illa pertinente"». Esta repoblación es lenta. Los puertos pesqueros y comerciales se irán desarrollando a partir de los siglos y .

Bermeo recibe su fuero en 1236, convirtiéndose en Cabeza de Vizcaya, y en 1296 pasa a formar parte de la Hermandad de las Villas de la Marina de Castilla con Vitoria.

Refiriéndose a Bermeo, un documento de 1269 menciona «cinco cabañas» a orillas del mar, lo que parece indicar que la actividad pesquera todavía era estacional. Pero también describe instalaciones más importantes, y dice que hay dos puertos, mayor y menor, y que el menor se puede cerrar con una cadena. También menciona otros dos fondeaderos llamados Arcaeta y Portuondo, que probablemente estarían en la ría de Mundaca. Bermeo y otras villas costeras irán convirtiéndose en importantes centros pesqueros y comerciales hasta el desastre de la peste negra del verano y otoño de 1348.

En esos puertos, la actividad pesquera fue adquiriendo cada vez más importancia, especialmente la caza de la ballena. Y el hecho de ser los puertos naturales para la exportación de hierro vizcaíno y lana castellana hacia Inglaterra, Francia, Flandes y los países bálticos los convirtió también en puertos comerciales.

En el hay constancia de fábricas en la ría de Bilbao y en los puertos mayores. Ya en el se plantean pleitos entre Ondárroa y Lequeitio y entre Lequeitio y Marquina por el aprovechamiento de los bosques, cuyos árboles son necesarios para la construcción naval. Aunque no se puedan considerar astilleros, se construyen barcos en Ondárroa en "Icaran", en Lequeitio junto a la orilla del río Lea. En Bermeo en 1357 el convento de San Francisco se encuentra cerca «del arrabal donde se labran las nabes», o sea en la zona que se denomina Ribera y en Bilbao se construyen barcos en las orillas de la ría. En el se van consolidando los astilleros, y su funcionamiento pasa a ser regulado al tiempo que aparecen las industrias auxiliares como ferrerías o cordelerías que se instalan en sus proximidades. En Lequeitio el astillero está en la parte sur de la Plaza del Astillero, en Bermeo en la Ribera, en Plencia en el camposanto, Ondárroa y Berriatua comparten astillero y se carenaban barcos en Amallo, Rentería y Asánsolo. Pero el mayor auge de los astilleros es en Bilbao, donde desde el actual Puente de San Antón hasta Portugalete hay multitud de gradas, fábricas y playas, pasando a ser a partir del el centro de la construcción naval de Vizcaya. Al ser punto de paso obligado entre Orduña y Bermeo, Bilbao le va quitando protagonismo a Bermeo para convertirse en el puerto y la villa más importante de Vizcaya. En el , los astilleros de Bilbao y el comercio lanero con Francia y Flandes eran muy importantes. Y en el , Portugalete rivalizaba con el puerto de Bilbao.

Durante la guerra de las Comunidades de Castilla Vizcaya se mantuvo en completa calma. El 30 de julio de 1520 una de sus ciudades, Bilbao, le envió una carta a Carlos I asegurándole la fidelidad de toda la provincia. Siglos después, sí hubo algunas revueltas en Vizcaya, por ejemplo: Rebelión de la sal (), Matxinada ().

La Guerra de la Convención con la monarquía española supuso la invasión francesa de Vizcaya en 1794 y su retirada con la Paz de Basilea en 1795. Poco más tarde (1808) la Guerra de Independencia española llevó a nuevos enfrentamientos. La población se decantó en su abrumadora mayoría por Fernando VII y durante toda la guerra, la provincia fue escenario de violentos combates. En 1808 en el lapso de tres meses (6 de agosto-2 de noviembre) Bilbao cambió de manos seis veces y sufrió una revolución, una gran batalla y dos saqueos a fondo. En 1812 se repitió la situación, por la ofensiva del Séptimo Ejército español, que ocupó y perdió Bilbao varias veces. Al mismo tiempo, las guerrillas de la provincia llegaron a sumar varios miles de combatientes que hostigaban sin cesar a los invasores.

La sucesión del rey Fernando VII inició las guerras carlistas; en Vizcaya hubo dos, entre 1833-1840 y entre 1872-1876. La mayor parte de Vizcaya apoyó al carlismo pero Bilbao apoyó al gobierno liberal y fue asediada por los carlistas que no consiguieron tomarla (véase: sitios de Bilbao de 1835, 1836 y 1874).

El final de la última guerra supuso para Vizcaya la pérdida de la mayor parte de la autonomía (la «abolición foral» de 1876), aunque la provincia fue compensada con el «Concierto económico», un régimen fiscal y administrativo propio similar al que Navarra gozaba desde 1841. Fue también el inicio de la explotación ilimitada de las minas de hierro (que el régimen foral limitaba como un tesoro); así Vizcaya se transformó en un país industrial con consecuencias importantes: Agotamiento de las minas al cabo de un siglo (una gran parte del mineral va a Inglaterra, de donde viene carbón); Inmigración masiva de obreros venidos de otras regiones de España que viven en condiciones miserables sobre todo en la margen izquierda de la ría (conflictos entre obreros y patrones y formación de dos comunidades: nativa = vasca, e inmigrada = «maketos»); Formación de grandes fortunas (Ybarra, Chávarri, Lezama-Leguizamón..)(además de los capitales ingleses, franceses o belgas) que inviertan una gran parte fuera de Vizcaya (que se transforma en un centro capitalista de España: Banco Bilbao y Banco Vizcaya hoy unidos en el BBVA).

Tras la derrota carlista, Sabino Arana, un vizcaíno de familia carlista acomodada, creó el Partido Nacionalista Vasco a finales del (de derecha, católico) que llega a ser uno de los grandes partidos de Vizcaya (junto con la derecha y la izquierda «españolistas») cuando cae la monarquía española con la proclamación de la Segunda República (1931). En 1936 se llega a la aprobación del estatuto de autonomía.

La guerra civil española, entre 1936 y 1939, dejó casi aislada a Vizcaya del gobierno de Madrid, lo que permitió al Gobierno de Euzkadi, con sede en Bilbao, una amplia autonomía. Pero la derrota de las fuerzas leales a la república (bombardeo de Guernica), trajo el régimen de Franco: represión de todo lo que era de izquierda, «rojo», o «separatista» (es decir, nacionalista vasco); declarada «provincia traidora» por el franquismo, Vizcaya perdió el resto de autonomía que le quedaba. 

Durante los años 50 y, sobre todo, 60, coincidiendo con la etapa del Desarrollismo, se produjo la segunda gran oleada de inmigrantes provenientes del resto de España que se trasladaron al País Vasco en busca de trabajo. Su elevado número y su mezcla con los autóctonos (parte de los cuales provenían de la primera oleada de inmigración de finales del ) produjo la actual sociedad vasca.

Muerto el franquismo, como Franco, de vejez en 1975, con una sociedad que quería ser «europea», y no «diferente» como el franquismo, llega la democracia actual: Constitución Española de 1978 y autonomía con el Estatuto de Autonomía del País Vasco de 1979, que establece a Vizcaya como «territorio histórico» con una cierta autonomía.

El escudo tradicional del Señorío de Vizcaya muestra los lobos de la familia Haro, que recuerdan la batalla de Padura, y un roble, que se identifica como el Árbol de Guernica sede de las Juntas, al que se añadió una cruz.

El escudo oficial, definido por una Norma Foral prescinde de los lobos, pero dice «"No obstante lo dispuesto en la presente Norma Foral, se mantendrán los escudos existentes en aquellos edificios caracterizados por su valor histórico-artístico, así como las enseñas actualmente existentes como integrantes de la Historia del Pueblo de Vizcaya"», lo que permite la coexistencia de los dos escudos, el tradicional y el oficial.

La heráldica municipal de Vizcaya incorpora en numerosas armerías el motivo del árbol y los lobos, asociadas con elementos originales así como también, alusiones religiosas, especialmente cruces de San Andrés y figuras de San Miguel Arcángel; motivos vegetales como 
las típicas panelas, las ramas de roble, encinas o acebos; animales heráldicos, en particular el león, usado en ocasiones como soporte exterior, pero también marinos como la ballena; construcciones como torres, castillos y puentes, o las armas de familias nobiliarias asociadas a la historia de cada municipio. En algunos casos también se combinan las tradicionales armas vizcaínas con las de la corona de Castilla. Las actividades económicas de la agricultura y la pesca también son evocadas, junto con la industria pesada y naval.

Vizcaya está situada al norte de la península ibérica, limita al oeste con la comunidad autónoma de Cantabria, al sur con la provincia de Burgos y el territorio histórico de Álava, al este con Guipúzcoa y al norte con el mar Cantábrico (golfo de Vizcaya). Su orografía es muy accidentada al encontrarse en la zona de unión de la Cordillera Cantábrica con los Pirineos. Es la quinta provincia más montañosa de España si se tiene en cuenta la pendiente media del terreno.

La extensión de la provincia es de 2217 km². Perímetro terrestre: 167 km, marítimo: 80 km.

El clima es templado, oceánico, con mucha nubosidad a lo largo de todo el año. Precipitaciones abundantes y frecuentes, especialmente en otoño e invierno, con una media anual de 1200 mm. Las temperaturas son suaves tanto en verano como en invierno (14-15 °C de media anual).

Entre las principales vías de acceso se encontrarían la AP-8 Autovía/Autopista del Cantábrico y la AP-68 Autopista Vasco-Aragonesa.

Según el censo INE 2019, Vizcaya cuenta con 1 152 651 habitantes, y su densidad de población es de 519,91 hab/km²; solo superada por la Comunidad de Madrid y Barcelona. Quinta provincia española por población en 1981, a pesar de la crisis demográfica que viene sufriendo desde la Transición, es aún la novena provincia española en número de habitantes. Debido a que desde principios de los años 1980 ha presentado un fuerte saldo migratorio negativo hacia otras regiones de España, contaba en el 2009 tan solo con un 5,7 % de extranjeros, al menos seis puntos menos que la media nacional.

Los veinte municipios más poblados de Vizcaya son los indicados en la tabla. Datos obtenidos del Instituto Nacional de Estadística en 2019. Nota: la denominación oficial de los municipios no es la actualmente oficial según el Instituto Nacional de Estadística. Vizcaya ocupa la situación 32.ª del conjunto estatal en que existe un mayor porcentaje de habitantes concentrados en su capital (30,10 %, frente a 31,96 % del total).

Las instituciones y órganos forales de Vizcaya, como territorio histórico del País Vasco, son las Juntas Generales de Vizcaya y la Diputación Foral de Vizcaya.

Las Juntas Generales de Vizcaya son la asamblea unicameral que ejerce la potestad normativa del territorio histórico de Vizcaya. Sus miembros, denominados "apoderados", son elegidos mediante sufragio popular directo. Las elecciones tiene lugar cada cuatro años, coincidiendo con las elecciones municipales, en cuatro circunscripciones. Las Juntas Generales de Vizcaya tienen su salón de plenos en la Casa de Juntas de Guernica, y oficinas en Bilbao. La composición de las Juntas tras las elecciones de 2015 es la siguiente:

La Diputación Foral ejerce la función ejecutiva y la potestad reglamentaria dentro del ámbito de competencias de Vizcaya.

La Diputación Foral de Vizcaya está formada por un diputado general, actualmente Unai Rementería (PNV), elegido por las Juntas Generales, y por los demás diputados (no necesariamente miembros de las Juntas Generales) que designe el diputado general, hasta un máximo de diez.

Las normas emanadas de las Juntas Generales se denominan normas forales y son producto del poder legislativo con el que las Juntas Generales están investidas. Por su parte, la Diputación Foral puede emitir decretos forales, de similar categoría a los reales decretos del gobierno central. Tanto las normas forales como los decretos forales tienen rango reglamentario y están sometidas, por tanto, a control de legalidad por parte de los tribunales ordinarios, a excepción de las normas forales fiscales, que, por decisión del Tribunal Constitucional, tienen rango de ley y solo pueden ser controladas en consecuencia por este mismo tribunal.

La gastronomía vasca goza de merecida reputación, no solo por el prestigio que en tiempos recientes han alcanzado cocineros como Juan Mari Arzak, sino sobre todo por la variedad y arraigo de un extensísimo recetario popular. Por lo que respecta a la cocina vizcaína, íntimamente emparentada con su vecina guipuzcoana, destacan platos tan conocidos como el bacalao al pil-pil o a la vizcaína, el marmitako, el pisto a la bilbaína, los chipirones en su tinta o la merluza a la ondarresa. Buena muestra de la importancia de la gastronomía en la sociedad vizcaína y vasca es la abundancia de sociedades gastronómicas (conocidas como txokos en Vizcaya).

Su capital, Bilbao, es famosa por el Museo Guggenheim Bilbao y su ría.

Monumentos y lugares de interés:





</doc>
<doc id="11458" url="https://es.wikipedia.org/wiki?curid=11458" title="Provincia de Nueva Vizcaya">
Provincia de Nueva Vizcaya

Nueva Vizcaya (tagalo: "Bagong Biskaya"; inglés: "New Viscaya") es una provincia de Filipinas, perteneciente a la región de Valle del Cagayán.

La provincia de Nueva Vizcaya, tiene como cabecera a Bayombong, situada a 268 kilómetros al norte de Manila. 

Tiene una superficie de 437,880 hectáreas distribuida entre quince municipios con 275 barangays. Debido a sus características topográficas, menos de 20 por ciento de la tierra se usa para la agricultura.

La provincia tiene un clima relativamente fresco. La temperatura mínima registrada en Bayombong es de 12 grados Celsius y la máxima de 25 grados Celsius. Los meses más fríos son diciembre y enero y los más templados abril y mayo. 

Suele llover entre mayo y octubre. La estación seca abarca los meses de noviembre a febrero. 

La provincia de Nueva Vizcaya está compuesta de quince municipios con Bayombong como capital. Solano y Bambang son los centros comerciales mientras que Kayapa, debido a su clima muy fresco, es considerado su capital de verano. 

La provincia consiste en un distrito del congreso con dos sectores, el Norte y el Sur. El Sector Norte comprende siete municipios a saber Ambaguio, Bagabag, Bayombong (capital), Diadi, Quezón, Solano y Villaverde. El Sector Sur tiene por otro lado ocho municipios: Alfonso Castañeda, Aritao, Bambang, Dupax del Norte, Dupax del Sur, Kasibu, Kayapa y Santa Fe. 

La población de la provincia es de 334,965 personas según censo de 1995. El 56% de la población se dedica a la agricultura. Hay un 20 por ciento de desempleados.

Políticamente se divide en 15 municipios y ninguna ciudad. Cuenta con 275 barangays. 
Consta de un único distrito para las elecciones al Congreso.
La provincia se creó como una provincia político-militar separada en tiempos del Gobernador español, Luis Lardizabal a través de un Decreto Real por el Rey de España el 10 de abril de 1841.
Se trataba entonces de uno de los Gobiernos político militares de la isla de Luzón. "Confinaba al norte con La Isabela, al este y al sur con el Distrito de El Príncipe, al sur con Nueva Ecija y al oeste con Benguet y Pangasinán. Era su capital Bayombong". 

Habitada por los Igorotes, Ifugaos, Ilongotes, y Alaguetes (Aetas o Negritos). Las tribus salvajes llevaron una vida nómada y sobrevivían recorriendo la zona desértica de la Cordillera, las regiones de Ilongote y la Sierra Madre.

Estas tribus fueron objeto de expediciones religiosas dirigidas por los dominicos y los misioneros agustinos. España administró la provincia a través de las misiones de Ituy y Paniqui. La misión de Ituy que cubría la parte sur estaba principalmente habitada por el Isinays mientras la misión de Paniqui que cubría la parte norte por el Gaddangs. 

El año 1877 tenía una extensión de 40.650 hectáreas y estaba habitada por 32,209 humanos distribuidos en 8 pueblos: Bayombong (capital con 2,897 almas), Bambang, Dupax, Aritao, Solano, Bagabag, Diadi e Ibung y 3 rancherías: Silipan, Ibuag y Lagavia, sin contar las tribus alzadas que pueden clasificarse como sigue:

A finales del siglo XIX la provincia de Nueva Vizcaya comprendía la Comandancia de Cayapa.

Debido a los recursos naturales ricos, Nueva Vizcaya atrajo a corrientes migratorias de otras partes del país. Ilocanos, Tagalos, Pangasinenses, Pampangos, Ibanas, Ibontocs y Batangueños. Otros incluso vinieron de lugares tan lejanos como las islas Bisayas y de Mindanao.


</doc>
<doc id="11459" url="https://es.wikipedia.org/wiki?curid=11459" title="Valle del Cagayán">
Valle del Cagayán

El Valle del Cagayán (ilocano: "Tanap ti Cagayán") es una región de Filipinas, también denominada Región II. Está compuesta por las provincias de Batanes, Cagayán, Isabela, Nueva Vizcaya y de Quirino. La capital regional es Tuguegarao.

La mayoría de la región se encuentra en un gran valle en el noreste de Luzón, entre la Cordillera Central y la Sierra Madre. El río Cagayán, el más largo del país, fluye por su centro y desemboca en el estrecho de Luzón en el norte, en el pueblo de Aparri. Los grupos de islas de Babuyan y Batanes pertenecen también a la región.

Las cinco provincias cuentan con 3 ciudades, 90 municipios y 2.311 Barangays.
A finales del siglo XIX la provincia de Cagayán comprendía las comandancias de Apayaos, Cabugaoan e Itaves.


</doc>
<doc id="11462" url="https://es.wikipedia.org/wiki?curid=11462" title="Trópico de Cáncer">
Trópico de Cáncer

El trópico de Cáncer es uno de los paralelos del planeta que están ubicados en el hemisferio norte. Es uno de los paralelos situado a una latitud de 23º 26′ 14″ al norte del ecuador.

Se está desplazando hacia el sur a un ritmo de casi medio segundo (0,46 s) por año (en el año 1917 estuvo en 23° 27').

Esta línea imaginaria delimita los puntos más septentrionales en los que el Sol alcanza el cénit (la vertical del lugar), lo que ocurre entre el 20 y el 21 de junio de cada año, a lo que se le denomina como solsticio de junio. En tablas astronómicas, la fecha y la hora de este evento se señalan en tiempo universal coordinado (UTC).

En el instante en que ocurre el solsticio de junio, los rayos solares caen verticalmente sobre el suelo en la línea imaginaria del trópico del hemisferio norte. En el solsticio de diciembre, lo hacen sobre el trópico del hemisferio sur

El trópico de Cáncer señala el límite septentrional de la llamada zona intertropical, comprendida entre los trópicos de Cáncer y Capricornio.

Se le denomina «de Cáncer» porque en la Antigüedad, cuando se producía el solsticio de verano en el hemisferio norte, el Sol estaba en la constelación de Cáncer. En la actualidad está en la constelación de Tauro, muy cerca del borde de la constelación de Géminis. La palabra "tropos" proviene del griego y significa "volver atrás", señalando así que en los solsticios, el Sol aparenta invertir su camino.

Según la dinámica de la precesión de los solsticios (y de los equinoccios) hace un poco más de 20 siglos el punto solsticial estaba al final de la constelación de Cáncer. La siguiente constelación a la de Cáncer en sentido precesional es la de Géminis que abarca 28º de la Eclíptica, y el punto solsticial avanza 1º cada 71,6 años. Así, durante los últimos 20 siglos el punto solsticial ha atravesado Géminis y actualmente ha ingresado ya en Tauro, situándose en su primer grado.

Por esto en el trópico a mediodía se reciben los rayos procedentes del Sol situado hacia Tauro y por ello este sería el nombre astronómico del trópico, un nombre que nos aporta información astronómica y de la dinámica de la precesión del punto solsticial estival de la Tierra. Así mismo podemos llamar al solsticio como «Solsticio de Tauro». Igualmente ocurriría con el otro solsticio y el trópico de Capricornio.

El trópico de Cáncer pasa a través de los siguientes países, partiendo del océano Pacífico hacia el este:



Para calcular la longitud del trópico:

De acuerdo a las reglas establecidas por la Fédération Aéronautique Internationale, para que un vuelo califique para competir por el récord de velocidad alrededor del mundo son necesarias las siguientes condiciones:




</doc>
<doc id="11463" url="https://es.wikipedia.org/wiki?curid=11463" title="Trópico">
Trópico

Trópico proviene del latín "tropĭcus", y este del griego "τροπικός" ["tropikós"], que significa ‘vuelta’. El plano horizontal en el cual se produce el movimiento de traslación de la Tierra alrededor del Sol se conoce como plano de la eclíptica. Ya que el eje de rotación de la Tierra no es perpendicular al plano de la elíptica, la intersección de este plano con la esfera no coincide con el plano ecuatorial terrestre. La latitud máxima a la que la eclíptica corta a la esfera terrestre es de 23°26′14″N y 23°26′14″S (en 2015); por lo que los paralelos que pasan por estas latitudes tienen una relevancia especial y se los conoce como:

La región comprendida entre los dos trópicos se conoce como zona intertropical, tórrida o tropical aunque la primera denominación tiende a reemplazar a las dos últimas en aras de la exactitud; desde el punto de vista biogeográfico, los trópicos pueden extenderse más allá de los paralelos de Cáncer y Capricornio, v. gr., la península de la Florida en los Estados Unidos yace en los subtrópicos (latitud mayor a 23°26′N) pero aloja muchas especies características de los trópicos del nuevo mundo, en parte debido al efecto atemperante del clima que brinda la corriente del Golfo. Igualmente, los valles medio y bajo del río Paraná son subtropicales pero forman parte de la región biogeográfica neotropical que incluye la Patagonia y demás regiones australes del continente. De la misma forma, y en sentido inverso, también los climas secos de las zonas subtropicales (inmediatamente al norte del trópico de Cáncer o al sur del trópico de Capricornio) pueden extenderse dentro de la zona intertropical en las costas occidentales de los continentes. Estas observaciones se justifican por el hecho de que las líneas de los trópicos constituyen un concepto matemático (geométrico, más propiamente), mientras que los conceptos de clima o de la biogeografía, son netamente geográficos y la latitud apenas es uno de los cinco factores que modifican estos conceptos.


Son los paralelos que se encuentran a 66°33´ al Sur o al norte respecto al Ecuador, donde los rayos del sol llegan de forma oblicua haciendo que en esta latitud se produzcan 24 horas seguidas de oscuridad o luz una vez al año, aumentando hasta el máximo de 3 meses de luz u oscuridad seguidos en los Polos (90° Norte y 90° Sur). Las latitudes 66º 33' 46" N y 66º 33' 46" S corresponden respectivamente a los círculos polares ártico y antártico.

La zona intertropical húmeda es un mundo con mucha lluvia que es bueno para la agricultura. Toda esta zona tropical ocupa el 20% de la tierra emergida, representando el 40% de la tierra útil para el ser humano, y acoge a algo más del 40% de la población mundial, aunque con una desigual distribución en el territorio.



</doc>
<doc id="11465" url="https://es.wikipedia.org/wiki?curid=11465" title="Islas de Barlovento">
Islas de Barlovento

Las islas de Barlovento son un grupo de islas de América que acotan al este la cuenca del mar Caribe, integrado por las islas septentrionales de las Pequeñas Antillas. Algunas de las islas principales son Granada, Martinica, Santa Lucía, Barbados, Guadalupe, Dominica, Trinidad y Tobago.

Las islas de Barlovento fueron llamadas así porque están más a barlovento (en la dirección de origen del viento) de las naves que venían al Nuevo Mundo que las islas de Sotavento, dado que los vientos que prevalecen en esa zona soplan en el sentido de este a oeste. Las corrientes transatlánticas y los vientos que producían la ruta más rápida a través del océano llevaban a las naves primero a estas islas ubicadas en el sureste de las Antillas para luego continuar a su destino final en el Caribe septentrional.

Existe una importante diferencia en la consideración de que islas pertenecen al grupo de Barlovento o Sotavento, según sea el ámbito lingüístico, de un lado el inglés y de otro el español, francés y neerlandés.

Las islas de Barlovento son, de norte a sur, las siguientes:



</doc>
<doc id="11466" url="https://es.wikipedia.org/wiki?curid=11466" title="Islas de Sotavento">
Islas de Sotavento

Las islas de Sotavento son un grupo de islas de las Antillas Menores, integrado por diversas islas repartidas entre los Países Bajos y Venezuela, y situadas frente a las costas de este último país y sobre la plataforma continental sudamericana. 

Es importante resaltar que la clasificación anglosajona de estas islas varía con respecto a la del resto del mundo, por lo que algunas islas clasificadas como de Sotavento o Barlovento pueden variar según la fuente que se consulte.

Las islas de Sotavento fueron llamadas así porque estaban más abajo de las corrientes dominantes de vientos (abajo de donde sopla el viento) y por lo tanto eran las más alejadas de las naves que venían desde Europa que se encontrarían primero con las islas de Barlovento.

Existe una importante diferencia en la consideración de que islas pertenecen al grupo de Barlovento o Sotavento, según sea el ámbito lingüístico, de un lado el inglés y de otro el español, francés y neerlandés.

Las islas de Sotavento son, de occidente a oriente, las siguientes:



</doc>
<doc id="11467" url="https://es.wikipedia.org/wiki?curid=11467" title="Antillas Mayores">
Antillas Mayores

Las Antillas Mayores o Grandes Antillas (en francés: "Grandes Antilles"; en inglés: "Greater Antilles": en Criollo haitiano "Gwo Zantiy") son un grupo de islas en el norte del mar Caribe, localizadas al este de Yucatán (México) y sureste de la Florida (Estados Unidos) y al oeste de las Antillas Menores. El grupo está compuesto por la Isla de Cuba, Isla de la Juventud, Jamaica, La Española (que incluye a República Dominicana y Haití) y Puerto Rico. Las Bahamas, si bien vecinas, no integran esta unidad geográfica junto con las Islas Turcas y Caicos, Islas Caimán y las Bermudas.

Las Antillas Mayores constituyen casi el 90% de la masa de tierra de todas las Indias Occidentales, así como más del 90% de su población. El resto de la tierra pertenece a las Bahamas y al archipiélago de las Antillas Menores, que es una cadena de islas al este (que se extiende de norte a sur y abarca el borde oriental del Mar Caribe, donde se encuentra con el Océano Atlántico) y al sur (que se extiende de este a oeste desde la costa norte de América del Sur).

La palabra "Antillas" se originó en el período anterior a la conquista europea del Nuevo Mundo. Los europeos usaron el término "Antilia" como una de las tierras misteriosas que aparecen en las cartas medievales, a veces como archipiélago, a veces como tierra continua de mayor o menor extensión, fluctuando su ubicación en medio del océano entre las Islas Canarias y Eurasia.

Las Grandes Antillas descansan sobre un macizo submarino común y están atravesadas por una cadena abrupta y elevada de montañas, cuyos picos más altos oscilan entre 2.000 y 3.000msnm (metros sobre el nivel del mar) que culminan en la República Dominicana (en el Pico Duarte con 3.087msnm) y declina, a ambos lados, en Cuba, Jamaica y Puerto Rico. Esas montañas están compuestas de piedra caliza, con afloramiento de otras rocas, todas ellas mucho más antiguas que las de origen eruptivo de las Pequeñas Antillas y sin huellas de actividad volcánica reciente. Las Bahamas, por el contrario, son islas de origen coralino. Antiguamente a las Bahamas ni siquiera se las consideraba como parte de las Antillas, aunque en la actualidad está difundido su englobamiento como un tercer grupo dentro de las mismas (Grandes Antillas, Pequeñas Antillas y las Bahamas).
La República Dominicana posee la montaña más alta del Caribe el Pico Duarte con 3.087 msnm) y el lago más grande de todas las Antillas (Lago Enriquillo). Cuba, sin embargo, tiene el río más largo de las Antillas, el Río Cauto, con 343 km y es la isla más extensa de la región. 

Las Grandes Antillas están en una parte de América Central llamada Archipiélago Antillano y están bañadas por el mar Caribe y el océano Atlántico.

Algunos de los recursos minerales que podemos encontrar en las Antillas Mayores son: bauxita,
oro, cobre, hierro, plata, mármol, entre otros.

Las grandes Antillas están divididas desde el punto de vista administrativo en 4 países independientes y 1 dependencia.


</doc>
<doc id="11470" url="https://es.wikipedia.org/wiki?curid=11470" title="Artesano">
Artesano

Un artesano es la persona que realiza objetos artesanales o artesanías. Los artesanos realizan su trabajo a mano o con distintos instrumentos propios de manualidades, por lo que hay que tener cierta destreza y habilidad para realizar su trabajo. Pueden trabajar solos o junto a otras personas que les pueden servir de ayudantes o aprendices.

Los objetos producidos suelen tener un valor estético y/o utilitario. El artesano puede vender, a título personal o a terceros sus creaciones, las cuales produce en el "taller", a pie de calle, en un puesto de artesanía o en el taller de un maestro artesano, cuando trabaja como empleado. 

Los artesanos y su trabajo suelen formar parte del folklore de su lugar de origen, utilizan materiales típicos de su zona para fabricar sus productos o se inspiraran en motivos tradicionalmente lugareños. Cada cual suele tener sus materiales preferentes, que en muchos casos imprimen un estilo especial a sus creaciones; entre los materiales que utilizan se incluyen: conchas marinas spondylus, algas, granos de arroz, cuarzo, maderas específicas, piedras, huesos, incluso fósiles u otros elementos que el propio artesano recoge y elige en playas o campos, etc.

La palabra "artesano" viene del italiano "artigiano" (significando 'que ejerce un arte mecánico'), y este término viene del latín "ars", "artis". El latín nos dio arte en castellano, el que viene de la raíz indoeuropea "ar" (significando 'mover, ajustar, hacer actuar'). Es solo a partir de finales del siglo XV, durante el renacimiento Italiano, cuando por primera vez se hace la distinción entre el artesano y el artista, esta diferencia da lugar a la figura del artista.




</doc>
<doc id="11472" url="https://es.wikipedia.org/wiki?curid=11472" title="Cerámica">
Cerámica

La cerámica (procedente del griego antiguo "κεραμική" (keramiké), femenino de "κεραμικός" ("keramikós", ‘hecho de arcilla’), "cerámico", que designaba al barrio de los alfareros de la antigua Atenas, al noroeste de la Acrópolis), es el arte de fabricar vasijas y otros objetos de arcilla u otro material cerámico por acción del calor, es decir cocida a una temperatura superior a los 900 grados. El resultado es una diversa variedad de piezas u objetos de terracota —o alfarería «de basto»—, de loza y del conjunto de porcelanas. Además de denominar la técnica y su actividad, también da nombre al conjunto de objetos y producción. 

Su uso inicial fue la fabricación de recipientes empleados para contener alimentos o bebidas. Más adelante se utilizó para modelar figurillas de posible carácter simbólico, mágico, religioso o funerario. También se empleó como material de construcción en forma de ladrillo, teja, baldosa o azulejo, conformando muros o revistiendo paramentos. La técnica del vidriado aumentó su atractivo suntuario y su uso arquitectónico. A partir del siglo XIX se aplicó a la industria como aislante eléctrico y térmico en hornos, en motores y en blindajes. La moderna cerámica se aplica a las industrias de silicatos (grupo de minerales de mayor abundancia, pues constituyen más del 95 % de la corteza terrestre) y como complemento en tecnologías de construcción asociada al cemento. También es la base de las técnicas de esmaltes sobre metal.

Existe cierta confusión, provocada desde el propio contexto de la investigación a partir del siglo XVIII, entre los conceptos alfarería y cerámica, llegando a generar un incómodo conflicto semántico (semántica lógica). Las dos palabras se usan indistintamente para nombrar las actividades artesanales, artísticas e industriales a partir del barro cocido, así como el producto o los productos de las mismas y su cultura.

La propuesta de los diccionarios (ideológicos y de sinónimos) y los manuales léxicos no ayuda a resolver la disyuntiva cuando «alfarería» aparece redirigida o referida a «cerámica», dándosele así a esta última mayor valor troncal. En el capítulo de las etimologías se indica que "Alfarería", como alfar, provienen del árabe hispánico "alfah hár", y este del árabe clásico "fah har" ‘alfarería’, y a su vez del hebreo "hhafar" ‘tierra, barro’. Por su parte, "Cerámica" procede del griego antiguo "κεραμική" (keramiké), femenino de "κεραμικός", "keramikós" ‘hecho de arcilla’; “cerámico”, que designaba originalmente al barrio de los alfareros de la antigua Atenas, al noroeste de la Acrópolis.
A comienzos del siglo XVI, el humanista Antonio de Nebrija ya mencionaba el término griego "ceramion" en un contexto amplio. Pero se ha atribuido al arqueólogo Giovanni Battista Passeri la responsabilidad de incluir la voz "cerámica" en el contexto lingüístico moderno, al usarlo en una obra impresa en Venecia en 1768. Joan Corominas completa el seguimiento del término y su uso explicando que dicho vocablo llegó a España en 1869, justo un siglo después de la propuesta de Passeri.

Las definiciones con más peso oficial, tras admitir que ambos términos designan el arte de elaborar objetos de barro, relacionan la alfarería con los espacios de fabricación y venta, y a la cerámica con el conjunto de objetos y sus vertientes científicas asociadas a la arqueología.

En un manual clásico de términos de arte, ambos términos se relacionan con el «arte y técnicas del barro y la arcilla»; dándole preferencia a la alfarería en esta acepción y reservando a cerámica la definición de los objetos fabricados con dichas características y haciéndolo extensivo a otros términos más concretos como: loza, porcelana, mayólica y terracota.


Además de las diferenciaciones según aspectos geográficos, lingüísticos, sociológicos, económicos, se ha clasificado:

La base y los materiales arqueológicos para dichas clasificaciones y su investigación, por convención, son los diferentes productos del trabajo alfarero. Estructuralmente se han propuesto tres fases de investigación: la "histórico-artística" (del siglo XV a 1880) cuando se trata de vasos completos, la "tipológica" (de 1880 a 1956) en el caso de que sean fragmentos, y la fase "contextual" (de 1956-60 en adelante) cuando se parte de muestras microscópicas o se trabaja con conjuntos de muestras.

Existen varias razones por las que se considera muy importante el estudio de las cerámicas en comparación con el resto del registro arqueológico:


Cuando nos enfrentamos al análisis de un objeto cerámico debemos tener en cuenta que este va a ser una aproximación a la historia total del artefacto, desde su producción a su deposición y alteraciones posteriores, y que esta historia contiene información desde un nivel puramente estético a un nivel relacionado con el grado de tecnología de estas comunidades, las posibles funciones de las cerámicas (uso doméstico, ritual, simbólico...), la procedencia de las mismas (intercambio, producción autónoma, etc.).

Este argumento ha traído como consecuencia la superación de la fase llamada crono-tipológica en los estudios cerámicos, que había llegado a un punto de estancamiento por no ser capaz de dar más información que la puramente descriptiva.

De este modo, se recurrió a otras disciplinas para poder llegar a incrementar los niveles de información recuperable que no podían extraerse con ningún otro medio arqueológico.

Por otro lado es muy importante tener en cuenta que el estudio de las cerámicas ha de realizarse siempre teniendo en cuenta el contexto en el que han sido halladas (con qué otros elementos arqueológicos estaban, que disposición en el espacio tenían respecto a los demás elementos y su posición estratigráfica, en qué tipo de estructuras estaban, si están en un asentamiento, en una necrópolis, en un área de producción, etc.). Aisladas del mismo, la información es mucho más reducida y prácticamente se limita a su datación relativa y posible función.

La importancia de los datos proporcionados por las distintas técnicas de análisis no tienen relevancia arqueológica directa si no es porque se estudian como fruto de un sistema humano de conducta, como un producto humano (se han llegado a hacer análisis del tipo de medidas de diámetro, estadísticas, reagrupamientos con análisis de grupos, etc, para al final decir que tal conjunto cerámico es de tal período o tal cultura), una conducta que puede inferirse de ellos, y que en última instancia son los que interesan en la investigación arqueológica.

Por ello, en la investigación hay que partir en primer lugar de un marco teórico que sea el que dote de significado los estudios analíticos que se emprendan, en un intento de integrar la información de la composición de las cerámicas y la información cultural, buscando así la interrelación entre las aproximaciones experimentales y las arqueológicas.

La caracterización de una cerámica, al igual que la tipología, no tiene un valor más que puramente descriptivo si no tiene un marco teórico que dote de significado a estos estudios analíticos.

Los estudios tecnológicos de las cerámicas fueron aplicados en un primer momento fuera de España. Estos no solo se pueden quedar en darle un carácter científico a una publicación, sino que hay que interpretar los datos para poder responder a hipótesis previas.

Con la caracterización de un objeto cerámico se intenta determinar los constituyentes de su materia prima con el fin de poder llegar a realizar inferencias sobre aspectos tecnológicos que nos informan sobre su proceso de manufactura, y también son susceptibles de informarnos sobre la posible procedencia de los mismos.

Esta información puede ser muy valiosa para detectar patrones de producción o de intercambio y comercio, así como para documentar datos sobre factores socio-económicos y culturales. Los resultados serán más valiosos si trabajamos con cerámicas bien contextualizadas.

Al estudiar la naturaleza de la materia prima, el fin principal por lo tanto es la tecnología y su procedencia.

Aunque ambas cuestiones debieran jugar un papel semejante, se ha puesto mayor énfasis en los aspectos relacionados con la procedencia de las cerámicas, (más de un tercio de los trabajos en todo el mundo). Para ello se lleva a cabo un estudio del entorno geológico en donde se ha hallado la cerámica y se recogen sedimentos arcillosos potencialmente utilizables dentro del área geográfica del estudio, como apoyo y contrastación de los resultados analíticos obtenidos con la caracterización de las cerámicas.

Las materias primas de la cerámica son la arcilla, el desgrasante o clastos y el agua.

La arcilla es llamada fracción fina de un suelo o sedimento, siendo el conjunto de partículas minerales que tienen un diámetro de dos micras o menos. Algunos autores prefieren denominar la materia prima de la cerámica como tierras, porque las arcillas seleccionadas nunca son puras, están mezcladas con elementos minerales de mayor tamaño o fracciones gruesas, no plásticos o desgrasante. Es decir, aunque el mayor porcentaje de material sea arcilla, no lo es todo. También contienen limos y arenas en cantidades variables que serán factores determinantes respecto al tipo de textura.

La razón de que se use la arcilla es por su propiedad plástica, sus facultades de moldeo en el estado pastoso pero dureza en el estado cocido.

El desgrasante se añade o ya va incluido en las arcillas para que sirva de armazón y de solidez a la parte plástica de la cerámica (arcilla y agua). Las arcillas tienen una gran capacidad de absorción de agua, no solo la intrínseca sino también la añadida por el alfarero para darle plasticidad y poder moldearla (supone el 18-25 % del total). Si se le echa poca agua se fragmenta y si se le echa mucha ya no es plástica.

El desgrasante suele ser más visible en la pared interior, ya que en la exterior normalmente se procede a un acabado final de alisamiento por motivos estéticos y prácticos (por ejemplo para evitar en lo posible la porosidad).

Los desgrasantes pueden ser minerales (cuarzo, calcita, feldespato, esquisto, mica, etc), orgánicos (carbón, vegetales, cereales, hojas), animales (conchas, fragmentos de hueso), y trozos de cerámica, fragmentos de sílex, etc.

Su tamaño puede ser de fracción gruesa, 2mm, media, de 2 a 1mm, o fina, 1mm.

El tipo de desgrasante en ocasiones era seleccionado según la función que fuese a cumplir la vasija. Para las que tenían que soportar altas temperaturas, como los crisoles por ejemplo, añadían gran cantidad de cuarzo; para las de actividad de cocina le añadían mayor cantidad de minerales desgrasantes que a las rituales o de enterramientos (estas últimas suelen tener unas pastas con el desgrasante más fino). Si requerían alta porosidad para transpirar (para contener agua, aceite, leche) se utilizaban desgrasantes orgánicos, ya que estos al cocerse la cerámica desaparecen y dejan los huecos.

Se pueden hacer estudios, incluso dentro de un mismo yacimiento, sobre cómo va variando la cerámica a lo largo del tiempo en relación a su mayor o menor calidad, su forma, su función, etc., y deducir, por ejemplo, que el cambio está motivado por un cambio en la dieta o por otros aspectos y el por qué (por contacto con otros grupos, por evolución interna en el tipo de producción inducido por un cambio en el medio, por nuevas técnicas de producción, por un nuevo modo de vida nómada o sedentaria, etc.).

Asimismo se pueden hacer estudios de la procedencia de los minerales: si se trata de esquinas redondeadas o cantos desbastados procede normalmente de las márgenes de un río o de depósitos fluviales. En el estudio concreto de la cerámica neolítica granadina, la presencia de mica dorada era un detector clave del lugar de procedencia del sedimento (Sierra Nevada).

El tipo de resistencia mecánica de la cerámica puede ser también un indicador importante: si se trata de una cerámica con arcilla muy fina y cocida a altas temperaturas, su resistencia es alta en tanto que si la densidad es baja y tiene alta porosidad, puede indicar cierto grado de arcaísmo.

Existen distintas técnicas de modelado:


La historia de la cerámica va unida a la historia de casi todos los pueblos del mundo. Abarca sus mismas evoluciones y fechas y su estudio está unido a las relaciones de los seres humanos que han permitido el progreso de este arte.

La invención de la cerámica se produjo durante el neolítico, cuando se hicieron necesarios recipientes para almacenar el excedente de las cosechas producido por la práctica de la agricultura. En un principio esta cerámica se modelaba a mano, con técnicas como el "pellizco", el "colombín" o la placa (de ahí las irregularidades de su superficie), y tan solo se dejaba secar al sol en los países cálidos y cerca de los fuegos tribales en los de zonas frías. Más adelante comenzó a decorarse con motivos geométricos mediante incisiones en la pasta seca, cada vez más compleja, perfecta y bella elaboración determinó, junto con la aplicación de cocción, la aparición de un nuevo oficio: el del alfarero.

Según las teorías difusionistas, los primeros pueblos que iniciaron la elaboración de utensilios de cerámica con técnicas más sofisticadas y cociendo las piezas en hornos fueron los chinos. Desde China pasó el conocimiento hacia Corea y Japón por el Oriente, y hacia el Occidente, a Persia y el norte de África hasta llegar a la península ibérica. En todo este recorrido, las técnicas fueron modificándose. Esto fue debido a ciertas variantes; una de ellas fue porque las arcillas eran diferentes. En China se utilizaba una arcilla blanca muy pura, el caolín, para elaborar porcelana, mientras que en Occidente estas arcillas eran difíciles de encontrar. Otras variantes fueron los motivos decorativos y los diferentes métodos utilizados para la cocción.

El invento del torno de alfarero, ya en la Edad de los Metales, vino a mejorar su elaboración y acabado, como también su cocción al horno que la hizo más resistente y amplió la gama de colores y texturas. En principio, el torno era solamente una rueda colocada en un eje vertical de madera introducido en el terreno, y se la hacía girar hasta alcanzar la velocidad necesaria para elaborar la pieza. Poco a poco fue evolucionando, se introdujo una segunda rueda superior y se hacía girar el torno mediante un movimiento del pie; posteriormente se añadió un motor, que daba a la rueda diferente velocidad según las necesidades.

A menudo la cerámica ha servido a los arqueólogos para datar los yacimientos e, incluso, algunos tipos de cerámica han dado nombre a culturas prehistóricas. Uno de los primeros ejemplos de cerámica prehistórica es la llamada cerámica cardial. Surgió en el Neolítico, debiendo su denominación a que estaba decorada con incisiones hechas con la concha del "cardium edule", una especie de berberecho. La cerámica campaniforme, o de vaso campaniforme, es característica de la edad de los metales y, más concretamente, del calcolítico, al igual que la cerámica de El Argar (argárica) lo es de la Edad del Bronce.

Los ceramistas griegos trabajaron la cerámica influenciados por las civilizaciones del Antiguo Egipto, Canaán y Mesopotamia. Crearon recipientes con bellas formas que cubrieron de dibujos que narraban la vida y costumbres de su época. La estética griega fue heredada por la Antigua Roma y Bizancio, que la propagaron hasta el Extremo Oriente. Se unió después a las artes del mundo islámico, de las que aprendieron los ceramistas chinos el empleo del bello azul de cobalto.

Desde el norte de África penetró el arte de la cerámica en la península ibérica, dando pie a la creación de la loza hispano-morisca, precedente de la cerámica mayólica con esmaltes metálicos, de influencia persa, y elaborada por primera vez en Europa en Mallorca (España), introducida después con gran éxito en Sicilia y en toda Italia, donde perdió la influencia islámica y se europeizó.

El torno y el horno son los elementos fundamentales e importantes para la fabricación de la cerámica. Se necesita además pinceles y varillas para la decoración. Las principales herramientas o utensilios son:

Las distintas técnicas que se han ido utilizando han dado como resultado una gran variedad de acabados:

La materia prima es la arcilla. Se emplea agua, sílice, plomo, estaño y óxidos metálicos. Para la cerámica llamada gres se utiliza una arcilla no calcárea y sal. Otro material importante para otro tipo de cerámica es el caolín mezclado con cuarzo y feldespato. También se emplea el polvo de alabastro y mármol. Para las porcelanas se utilizan los óxidos de potasio, magnesio y aluminio.

Tanto antes como después de ser cocida, la pieza de alfarería puede ser adornada sometiéndola a diferentes técnicas de decoración:

La fabricación de componentes cerámicos tiene lugar de la siguiente manera:

Los materiales son buenos aislantes térmicos y que además tienen la propiedad de tener una temperatura de fusión y resistencia en compresión elevadas. Asimismo, su módulo de Young (pendiente hasta el límite elástico que se forma en un ensayo de tracción) también es muy elevado (lo que llamamos fragilidad).

Todas estas propiedades hacen que los materiales sean imposibles de fundir y de mecanizar por medios tradicionales (fresado, torneado, brochado, etc). Por esta razón, en las cerámicas realizamos un tratamiento de sinterización. Este proceso, por la naturaleza en la cual se crea, produce poros que pueden ser visibles a simple vista. Un ensayo a tracción, por los poros y un elevado módulo de Young (fragilidad elevada) y al tener un enlace iónico covalente, es imposible de realizar.

Existen materiales cuya tensión mecánica en un ensayo de compresión puede llegar a ser superior a la tensión soportada por el acero. La razón, viene dada por la compresión de los poros/agujeros que se han creado en el material. Al comprimir estos poros la fuerza por unidad de sección es mayor que antes del colapso de los poros.

Las propiedades de un material cerámico dependen de la naturaleza de la arcilla empleada, de la temperatura y de las técnicas de cocción a las que ha sido sometido. Así tenemos:




</doc>
<doc id="11473" url="https://es.wikipedia.org/wiki?curid=11473" title="Botijo">
Botijo

Un botijo (también boteja en Hispanoamérica y búcaro en gran parte de la España meridional) es un recipiente de barro cocido poroso, diseñado para beber y conservar fresca el agua. En alfarería se define como vasija de cuerpo esferoide, un asa en su parte superior, y con dos o más orificios. Por lo general se llama "boca" al más ancho —por el que se llena—, y "pitón o pitorro" al otro, que produce un fino chorrillo ideal para beber sin demasiado desperdicio.

El botijo es un objeto típico de la cultura española, habitual en Castilla, Aragón y el tercio sur de la Península (Extremadura, La Mancha, Levante y Andalucía), como en las zonas más húmedas del norte o el territorio insular.

El ejemplar más antiguo aparecido en la península ibérica pertenece a la cultura argárica y fue hallado en la necrópolis de Puntarrón Chico (Beniaján), cercana a la capital de la región murciana, en cuyo museo arqueológico se conserva; pieza importante en la historiografía de la cerámica por tratarse de una 'obra cerrada', con un solo orificio de 2 cm y el asa colocada en la parte superior; la medida del botijo es de 11 x 9,5 cm.

El principio de funcionamiento del botijo es el siguiente: el agua almacenada se filtra por los poros de la arcilla y en contacto con el ambiente seco exterior (característica del clima mediterráneo) se evapora, produciendo un enfriamiento (2,219 kilojulios por gramo de agua evaporada). La clave del enfriamiento está, por lo tanto, en la evaporación del agua exudada, ya que ésta, para evaporarse, extrae parte de la energía térmica del agua almacenada dentro del botijo.

En algunas regiones, antes de usarlo por primera vez, se "cura" dejándolo durante un par de días lleno de agua y con un poco de anís. En zonas de litoral, para curar el barro se introducen algunos cantos marinos por la boca de carga, se dejan en agua unos días y se enjuaga bien para que no quede regusto a sal. 

Al margen de la RAE, se sabe que existió el término latino "buttis", botella, tonel, y luego el latín medieval "butticula". Probablemente la mezcla de voces romances de origen latino con otras de la cultura mozárabe formaron su tronco etimológico.

Sebastián de Covarrubias en el año 1611, describe "botija", como "vaso de tierra ventrudo con la boca y cuello angosto. Los niños cuando están para llorar hinchan los carrillos y a esto le llaman embotijarse".

En la geografía española, el botijo recibe distintos nombres: en el sur y suroeste de España se alterna con términos como boteja o botejo, "búcaro", "cachucho", "pimporro" o piporro, "pipo" o pipote, "pirulo" en las vegas de Granada y el Guadalquivir, "ñañe" y "pichilín" en Huelva, "piche" en Extremadura. En Aragón "rallo". En valenciano, la "botija", documentada en obras de teatro desde 1850, se pronuncia [botíʤa], plural "botiges" y diminutivo "botigeta". En el País Vasco, el más afín sería la "txongila" de Cegama (Guipúzcoa). Y en Cataluña, "càntir", que aglutina una variada familia de modelos y tipos.
La alfarería catalana celebra una feria anual en la localidad barcelonesa de Argentona que cuenta con uno de los mejores museos monográficos dedicados a botijos cerámicos.
Otras piezas de alfarería de agua de la familia del botijo son, por ejemplo, el barril y la botija de carro, con su forma panzuda y el pitorro como si fuera el ombligo, con la espalda plana, para poder colgarla del carro. Existe, asimismo, una gran variedad de cantarillas y botijas.

En 1995, Gabriel Pinto y José Ignacio Zubizarreta de la Universidad Politécnica de Madrid desarrollaron un modelo matemático para un botijo esférico. Dos ecuaciones diferenciales describen el proceso:

donde:

Joaquín Sorolla, pintor luminista valenciano, pintó al menos en dos ocasiones un botijo blanco, similar a los de Agost. Hacia 1905, en un óleo titulado precisamente "El botijo", en el que una muchacha ayuda a un niño a beber de él. El cuadro, vendido originalmente a un particular y a pesar de no ser especialmente representativo de su obra, fue una de las pinturas seleccionadas en 1964 para la serie filatélica dedicada a Sorolla, llevando la imagen del botijo español hasta los más recónditos confines postales del planeta. Años después, un botijo similar aparece en el lado izquierdo de una de las muchas escenas íntimas captadas por Sorolla en las playas valencias: "Madre e hija. Playa de Valencia", de 1916. 




</doc>
<doc id="11474" url="https://es.wikipedia.org/wiki?curid=11474" title="Real Fábrica de Sargadelos">
Real Fábrica de Sargadelos

La cerámica de Sargadelos es una famosa cerámica elaborada en Sargadelos (en el municipio lucense de Cervo, Galicia, España). La primera fábrica (Real Fábrica de Sargadelos) fue creada a principios del siglo XIX por Antonio Raimundo Ibáñez y tras varias generaciones acabó cerrando en 1875. 

A partir de mediados del siglo XX una nueva iniciativa cerámica en Sargadelos, basada en coloraciones en tonos azulados , forma parte de un grupo de empresas del sector, el "grupo Sargadelos" —al que pertenece igualmente la cerámica de Castro-Sada— gracias al impulso del ceramista Isaac Díaz Pardo
Antonio Raimundo Ibáñez, notable enciclopedista de familia hidalga de escasos recursos, dedicado desde muy joven a negocios de importación y exportación, introdujo innovaciones tecnológicas para sus fábricas. Asentado en Ribadeo, inició una industria siderúrgica, y en el año 1806 creó conjuntamente una manufactura de cerámica que estuvo en sus primeros tiempos dedicada a la fabricación de loza fina para vajillas con estampación e influida de la loza inglesa, en aquel tiempo muy valorada. En 1808, tras el éxito de la inauguración de la fábrica de cerámica, Carlos IV le condecoró con la cruz de la Orden de Carlos III, otorgándole los títulos de marqués de Sargadelos y conde de Orbaiceta.

Como consecuencia de la guerra de la Independencia y sus sucesos revolucionarios, Antonio Raimundo Ibáñez fue asesinado el año 1808, sucediéndole en el cargo de la fábrica su cuñado Francisco Acevedo, quien contrató para la dirección de la misma al portugués Antonio Correa de Saa. 

A partir de esta nueva dirección la fábrica produjo un tipo de cerámica decorada con filetes en rojo y azul y escudos en oro. Correa decidió en 1829 montar su propia fábrica, por lo que la dirección de Sargadelos recayó en esa fecha en Hilario Marcos. Las vajillas de ese tiempo eran realizadas en blanco, sin ser posible la competencia con las piezas inglesas, deseo de su fundador. Al pasar la propiedad de la fábrica a José Ibáñez por la muerte de su padre en 1832, se formó una sociedad con el sevillano Antonio de Tapia con el fin de emprender de nuevo «la fabricación de loza fina», contratándose como director al inglés Richard, cargo que ejerció hasta 1842. Durante este tiempo murió prematuramente José Ibáñez y su viuda debió ponerse al frente, ya que su hijo era menor; debido a la débil situación económica de la empresa tuvo que arrendarla en 1845.

La fábrica de cerámica pasó por cuatro etapas, cada una con sus características, cerrándose definitivamente en 1875. En el último tercio del siglo XX resurgió la manufactura de cerámica en Sargadelos, ocupando edificios nuevos y respetando las ruinas antiguas como conjunto Histórico–Artístico, nombramiento que le fue dado en 1972

Ibáñez contó en primer lugar con la materia prima necesaria que se encontraba además muy cercana: arcillas, caolines, leña, cursos de agua. Tenía además muy próximo el puerto de San Ciprián para enviar desde allí los productos a lugares lejanos, imitando así la iniciativa inglesa.

La creación de la fábrica de cerámica en 1806 supuso un nuevo sistema de producir, introduciendo el proceso mecánico que vendría a sustituir la pieza hecha a mano, con lo que se intentaba además abaratar los precios. La pintura a mano fue sustituida igualmente por el moderno sistema de estampación, importado de Bristol (Inglaterra). Los objetos se hicieron en loza fina, un producto intermedio entre la loza y la porcelana, un material duro y ligero a la vez, de paredes delgadas con cocción entre 1.100 °C y 1.200 °C. Tras la cocción el color resulta blanco y es entonces cuando se le aplica un barniz de plomo. Este sistema se diferencia de la loza en que en lugar de añadir arena a la pasta se añade sílice, feldespato, caolín y calcio, obteniendo resultados diferentes según las cantidades añadidas. Sobre esta pasta se superpone el estampado cuya técnica habían perfeccionado en 1761 John Sadler y Guy Green, en la fábrica de Liverpool.

En 1806 Antonio Raimundo Ibáñez consiguió del Gobierno un privilegio exclusivo para la explotación de las minas de cuarzo descubiertas hasta la fecha y para las que estuvieran aún sin descubrir. De este modo tuvo asegurada la materia prima para la fábrica de loza, que había comenzado su andadura ese mismo año, paralela a la ya existente de fundición. El complejo fabril constaba de dos patios, varios hornos, oficinas, máquinas para romper las rocas y un molino para los barnices.

El primer director, Juan Antonio Pérez estuvo al frente de la fábrica un año. Después, en 1807, le sustituyó el portugués José Antonio Correa de Saa, con la experiencia de haber dirigido la fábrica de Vale da Piedade. Correa de Saa se mantuvo al frente hasta 1829; en este año pasó a ser el director Hilario Marco cuya gestión duró hasta 1832, cuando fue necesario cerrar la empresa.

En 1809 había muerto Antonio Raimundo Ibáñez y la dirección administrativa había pasado a su cuñado Francisco Azevedo (escrito con zeta). Fueron años difíciles y de apuros económicos, con un almacén de piezas sin vender, aunque hubo un cierto movimiento gracias a los encargos recibidos desde La Coruña, Ferrol, Rías Bajas, Vizcaya y Castilla. En esta época todavía no se estampaba la corona real en las marcas, aunque a la fábrica se la empieza a llamar Real Fábrica. Se cree que el título lo autorizó Fernando VII desde el exilio. 

A Francisco Azevedo le sustituyó en 1832 el hijo del fundador José Ibáñez que se vio obligado a cerrar, dadas las circunstancias económicas desfavorables. 

El proceso de fabricación fue de loza común, algo más fina para las vajillas en blanco con un ligero tinte azulado y un cuarteado característico que es el resultado de la diferencia entre el punto de cocción de la pasta y el esmalte. Para la forma se utilizaron moldes de yeso. Una de las piezas más características fue el "florero de dedos", siendo también muy apreciados los otros floreros de peces y árboles cuyos ejemplares supervivientes se encuentran en colecciones particulares de Galicia y en los museos de A Coruña, Lugo y Pontevedra. También se fabricó el jarrón de jardín con pedestal inspirado en la cerámica de la fábrica de Wedgwood así como las jarras de peregrino. Los botes de farmacia estaban decorados con relieves y tenían un pequeño estrechamiento en el centro.

También corresponde a esta época el relieve que representa la defensa del parque de Monleón en Madrid, que el propio Ibáñez dedicó a Fernando VII y que se conserva en el Museo Arqueológico Nacional más la serie del Apostolado, más los relieves que representan héroes de la Independencia y bustos de hombres célebres de la Antigüedad que adornan el pazo de Antonio Raimundo Ibáñez.

Tras los dos años de inactividad se reabrió la fábrica cuando José Ibáñez consiguió un socio capitalista en la persona del empresario sevillano Antonio de Tapia y Piñeiro. El director técnico fue el francés Richard que consiguió formar una plantilla de profesionales venidos de otros puntos de la península. Cuando José Ibáñez murió en 1836 quedó al frente del negocio su viuda Anita Varela. Fue época de ampliaciones con nuevos hornos y molinos; incluso se construyó una nave para estampados. 


Las piezas novedosas que se fabricaron se inspiraron en el Libro de Formas editado por la fábrica Hartley Greens and Company de Leeds. Las vajillas y muchas otra piezas como relojes y candelabros siguieron saliendo en blanco, así como las placas de información con motivos mitológicos o religiosos. Hacia 1838 empezó un cambio en la producción en blanco; se hicieron las primeras estampaciones con temas populares gallegos y empezaron a salir algunas vajillas con estampación de flores alemanas. Estos primeros momentos de coloreado tuvieron mucho éxito en los bibelots, siendo muy famosa y apreciada la jarra de cerveza "Mambrú", inspirada en los "tobies" ingleses.

Una vez más la situación económica obligó a cerrar la fábrica en 1842. A partir de 1845 bajo la gestión del nuevo arrendatario Luis de la Riba de Santiago de Compostela, la empresa tomo un nuevo rumbo y llegó a contratar a numerosas familias que permitieron crear grandes cantidades de piezas de buena calidad tanto estética como de técnica y se llegó a fabricar algunas vajillas reales para Isabel II —parte de las cuales se exhiben en el Museo de Pontevedra—, este período tuvo como director a Edwing Forestier y un grupo de ceramistas ingleses llegados en 1847 desde Staffordshire. Las vajillas eran de loza fina pero de gran dureza llamadas de «pedernal» de estilo isabelino. En esta época la fábrica daba trabajo a mil familias; poseía trescientos pares de bueyes y veintidós buques de cabotaje, fue el momento de máximo esplendor de que gozó la fabricación de esta cerámica.

En estos años se trazaron los paseos llamados Paseo de la Presa y Paseo de los Enamorados. Con la elevación de la presa se consiguió una mayor caída de agua. El uno de abril de 1848 se inauguró la carretera que unió las fábricas con el cercano puerto de San Ciprián.

Las vajillas y jarros presentaban una decoración estampada en gran parte de forma industrial, la técnica empleada era grabar una plancha de cobre o estaño con el dibujo a copiar —las primeras planchas fueron importadas de Inglaterra—, posteriormente se entintaba con color mezclado con grasa. Esta plancha se imprimía en un papel fino que se pegaba a la superficie de la pieza bizcochada. Esta pieza absorbía la tinta y el dibujo o decoración quedaba impreso, se eliminaba el papel con agua y se cubría con un barniz plumbífero y se pasaba a una segunda cocción. La transparencia de este barniz permitía ver con la máxima claridad el dibujo impreso además de darle un brillo especial. La decoración más frecuente en esta época en Sagardelos fue la llamada de «góndola», que consiste en un dibujo paisajístico, en el que el primer término está formado por una balaustrada con un gran jarrón, tras el que se ve un río con una góndola; el fondo está compuesto por unas colinas con arquitecturas y árboles; otras decoraciones presentaban cisnes, pavos reales, o temas chinescos. Los colores empleados fueron, el negro, el violeta, el rojo, el verde y el azul cobalto claro. Otro tipo o serie fueron las decoraciones de «lozas iluminadas» que consistía, que una vez realizada la estampación se policromaban las piezas a pincel. Los temas más numerosos en este caso, fueron los florales en rojo, verde, amarillo y azul. En las jarras y piezas altas se hacía la decoración en la parte abombada y si el cacharro tenía tapa, ésta iba también decorada.

Hubo otra innovación de mucho éxito que fue la serie llamada "china opaca", a imitación de la inglesa "flown blue". Consistía en manipular la plancha del estampado para que el dibujo quedase ligeramente corrido o desenfocado, dando así un aspecto enigmático. El tema era siempre chinesco.

Las piezas de tipo popular que tanto éxito habían tenido en la etapa anterior no se perdieron y siguió la fabricación de las jarras "Mambrús", y las figuras de macacos sentados, osos, perros, patos, palomas etc con los que se fabricaban también palilleros y salseras. Otras obras eran pequeñas pilas para el agua bendita, tinteros o centros de mesa. Se fabricaron además objetos que dejaron de tener utilidad con el paso de más de un siglo, como los aguamaniles, escupideras, orinales y pediluvios.

La empresa volvió a la dirección de la familia Ibáñez en 1862, a partir de entonces sufrió una decadencia hasta llegar a su cierre definitivo en 1875.
En 1862 terminó el contrato con Luis de la Riba, después de lo cual transcurrieron unos años sin actividad. En 1870 volvió a abrir la fábrica Carlos Ibáñez Varela, ingeniero de minas y nieto del fundador, pero a los cinco años se cerró definitivamente y sus instalaciones fueron desmanteladas hasta tal punto que no dejaron rastro.

Durante estos cinco años las piezas salieron con los mismos dibujos y estampaciones aunque desapareció la "china opaca" y resurgieron las vajillas de la primera época, blancas y fileteadas de azul y verde.

Toda la loza de Sargadelos salía con su marca correspondiente, incisa en la primera y segunda época y grabada en la tercera y cuarta. Junto a las marcas se podían ver las iniciales de los artistas y a veces unos números que se relacionaban con el tamaño de las piezas. Sargadelos llegó a utilizar alrededor de veinticinco marcas.
En 1949 el ceramista Isaac Díaz Pardo creó una fábrica en El Castro, —O Castro de Samoedo—, un lugar de la parroquia de Osedo, en el municipio de Sada. La fábrica se llamó Cerámicas do Castro y comenzó con una serie limitada de obras de destacados artistas. Sus vajillas tuvieron una gran demanda, no solo en Galicia sino en el resto del país, creciendo así la producción y el número de trabajadores.

En pleno apogeo de producción, en 1955 Díaz Pardo hizo un viaje a Argentina donde se encontraba un grupo de artistas e intelectuales españoles exiliados: Luis Seoane, Andrés Albalat (arquitecto) y Fernando Arranz entre otros. Juntos crearon el Laboratorio de Formas, una iniciativa encaminada a recuperar y estudiar las formas cerámicas de tiempos pasados y las que perduraban todavía en aquellos años. Como consecuencia de estos estudios y proyectos, fundaron una fábrica de porcelana en la ciudad de Magdalena, ubicada al este de la provincia de Buenos Aires, a unos 100 km de la capital. La fábrica se llamó La Magdalena. Fue un proyecto muy moderno, con una ordenación de trabajo circular pues los obreros cambiaban de cometido aprendiendo todas las fases del proceso de fabricación, e incluso tomaban parte en los diseños. Cerró sus puertas y actividad en 1980.

La continuación y puesta en marcha de la idea del Laboratorio de Formas se manifestó en la recuperación de la antigua fábrica de Sargadelos cuya planta circular se inauguró en 1970. Este edificio se levantó fuera de los restos del antiguo complejo industrial de fundición con el interés de conservar las ruinas para las que se obtuvo en 1972 el nombramiento de conjunto Histórico Artístico, lo que después se llamaría Bien de Interés Cultural.

Comenzó la nueva época fabricando servicios de mesa y piezas de decoración, empleando como colores básicos el azul y el marrón dorado, incorporando el rojo en las piezas muy especiales pues el proceso de este color encarecía el producto. Se dio importancia a las formas clásicas incorporando además nuevas formas vanguardistas salidas del estudio de Laboratorio de Formas y diseñadas por Luis Seoane. Tuvieron especial interés los retratos de personajes célebres de las letras y el arte, tanto en forma escultórica como en jarras "Mambrú". La primera de esta serie fue la obra dedicada a Rosalía de Castro seguida por Antonio Machado, León Felipe, Castelao, Unamuno, Valle Inclán y Pérez Galdós y el pintor Picasso. Personajes del medioevo como el maestro Mateo, el obispo Gelmírez o la popular heroína, María Pita. En otro momento salió la serie fauna con reproducciones de toda clase de pájaros propios de Galicia, gatos, vacas, etc. 

Otra serie que tuvo una aceptación popular y de gran éxito fue la de los amuletos, figuras pequeñas para colgar del cuello, cada una con su leyenda particular, inspiradas en las historias de las meigas y la forma de defenderse de sus hechizos. 

Las joyas de Sargadelos fueron también muy apreciadas en su combinación de plata y cerámica; se fabricaron sortijas, pulseras, dijes, collares, pendientes, etc.

En 1988 se instaló un museo donde está expuesto todo el material recuperado de las antiguas fábricas de fundición además de la zona dedicada a la cerámica española y en especial a la cerámica de Sargadelos de cada época.

Dependiendo económicamente de la fábricas de Sargadelos y de Castro se puso en marcha una editorial llamada Ediciós do Castro; un museo de arte contemporáneo situado en Sada, llamado Museo Carlos Maside; un centro de comunicación que recibe el nombre de Instituto Galego de Información; el Seminario de Estudos Galegos. Para la difusión de la cerámica existen las Galerías Sargadelos distribuidas por toda Galicia, Madrid y Barcelona.





</doc>
<doc id="11476" url="https://es.wikipedia.org/wiki?curid=11476" title="Coloide">
Coloide

En física y química un coloide, sistema coloidal, suspensión coloidal o dispersión coloidal es un sistema conformado por dos o más fases, normalmente una fluida (líquido o gas) y otra dispersa en forma de partículas generalmente sólidas muy finas, de diámetro comprendido entre 10 y 10 m. La fase dispersa es la que se halla en menor proporción. Normalmente la fase continua es líquida, pero pueden encontrarse coloides cuyos componentes se encuentran en otros estados de agregación de la materia. 

El nombre de coloide proviene de la raíz griega "kolas" que significa «que puede pegarse». Este nombre se refiere a una de las principales propiedades de los coloides: su tendencia espontánea para agregar o formar coágulos. De ahí viene también la palabra "cola", el fluido pastoso que sirve para pegar. Los coloides también afectan al punto de ebullición del agua y son contaminantes.

Los coloides se diferencian de las suspensiones químicas, principalmente en el tamaño de las partículas de la fase dispersa. Las partículas en los coloides no son visibles directamente, son visibles a nivel microscópico (entre 1 nm y 1 µm), y en las suspensiones químicas sí son visibles a nivel macroscópico (mayores de 1 µm). Además, al reposar, las fases de una suspensión química se separan, mientras que las de un coloide no lo hacen. La suspensión química es filtrable, mientras que el coloide no es filtrable.

Los sistemas coloidales son sistemas no homogéneos en los que las partículas constituyentes de uno o varios de sus componentes (fase dispersa o dispersoide) tienen tamaños comprendidos entre 10 y 2000 Å, mientras que los restantes componentes están constituidos por partículas con tamaño inferior a unos 10 Å (fase dispersante o medio de dispersión).

Las partículas coloides tienen propiedades intermedias entre las disoluciones y las suspensiones químicas; se encuentran dispersas sin que estén unidas a las moléculas del disolvente y no se sedimenta al dejarlas en reposo. 

En algunos casos las partículas son moléculas grandes, como proteínas. En la fase acuosa, una molécula se pliega de tal manera que su parte hidrofílica se encuentra en el exterior, es decir la parte que puede formar interacciones con moléculas de agua a través de fuerzas ión-dipolo o fuerzas puente de hidrógeno se mueven a la parte externa de la molécula. Los coloides pueden tener una determinada viscosidad (la viscosidad es la resistencia interna que presenta un fluido: líquido o gas, al movimiento relativo de sus moléculas).

Los coloides se clasifican según la magnitud de la atracción entre la fase dispersa y la fase continua o dispersante. Si esta última es líquida, los sistemas coloidales se catalogan como «soles» y se subdividen en «liófobos» (poca atracción entre la fase dispersa y el medio dispersante) y «liófilos» (gran atracción entre la fase dispersa y el medio dispersante). En los coloides liófilos la fase dispersa y el medio dispersante son afines, por lo tanto forman soluciones verdaderas y tienen carácter termodinámicamente estable; en tanto que los coloides liófobos son aquellos donde la fase dispersa y el medio dispersante no son afines, pueden formar dos fases y tienen carácter cinéticamente estable. Una característica fundamental de los coloides liófobos es que no son termodinámicamente estables, como ya se mencionó anteriormente, aunque poseen una estabilidad de tipo cinético que les permite mantenerse en estado disperso durante largos períodos de tiempo. Las partículas coloidales son lo suficientemente pequeñas como para que su comportamiento esté controlado por el movimiento browniano y no por efectos macroscópicos, como las fuerzas gravitatorias. Al agregarles cierta cantidad de electrolito pueden coagular, la cantidad depende de la valencia y la naturaleza del electrolito.
Respecto a la clasificación de coloides, cabe destacar también que, si el medio dispersante es agua se denominan «hidrófobos» (repulsión al agua) e «hidrófilos» (atracción al agua).

En la siguiente tabla se recogen los distintos tipos de coloides según el estado de sus fases continua y dispersa:

En principio, no existe una regla fija que establezca el estado de agregación en el que se tienen que encontrar, tanto la fase dispersa como el medio dispersante. Por lo tanto, son posibles todas las combinaciones imaginables, como se mostró en la tabla anterior.

Actualmente, y debido a sus aplicaciones industriales y biomédicas, el estudio de los coloides ha cobrado una gran importancia dentro de la fisicoquímica y de la física aplicada. Así, numerosos grupos de investigación de todo el mundo se dedican al estudio de las propiedades ópticas, acústicas, de estabilidad y de su comportamiento frente a campos externos. En particular, el comportamiento electrocinético (principalmente las medidas de movilidad electroforética) o la conductividad eléctrica de la suspensión completa. 

Por lo general, el estudio de los coloides es experimental, aunque también se realizan grandes esfuerzos en los estudios teóricos, así como en desarrollo de simulaciones informáticas de su comportamiento. En la mayor parte de los fenómenos coloidales, como la conductividad y la movilidad electroforética, estas teorías tan solo reproducen la realidad de manera cualitativa, pero el acuerdo cuantitativo sigue sin ser completamente satisfactorio.

Por su tamaño, las partículas coloidales tienen una relación área/masa extremadamente grande, por ello son excelentes materiales adsorbentes.
En la superficie de las partículas existen fuerzas llamadas de Van der Waals e incluso enlaces inter-atómicos que al estar insatisfechos pueden atraer y retener átomos, iones o moléculas de sustancias extrañas. A esta adherencia de sustancias ajenas en la superficie de una partícula se le llama adsorción. Las sustancias adsorbidas se mantienen firmemente unidas en capas que suelen tener no más de una o dos moléculas (o iones) de espesor. Aunque la adsorción es un fenómeno general de los sólidos, resulta especialmente eficiente en dispersiones coloidales, debido a la enorme cantidad de área superficial.

Consiste en que un haz luminoso se hace visible cuando atraviesa un sistema coloidal. Este fenómeno se debe a que las partículas coloidales dispersan la luz en todas las direcciones haciéndola visible. Los rayos de luz pueden ser vistos al pasar a través de un bosque, por ejemplo, como resultado de la dispersión de la luz por las partículas coloidales suspendidas en el aire del bosque.
Aunque todos los gases y líquidos dispersan la luz, la dispersión por una sustancia pura o por una solución es muy pequeña, que generalmente no es detectable.

Son ejemplos de este fenómenos los movimientos observados en partículas de polvo que se desplazan libres al azar en un rayo de sol que ingresa a través de una ventana (o una cortina abierta), o partículas de polvo y humo moviéndose en un rayo de luz proveniente del cuarto de proyección de una sala de cine. 
El movimiento desordenado de dichas partículas coloidales es debido al bombardeo o choque con las moléculas del medio dispersante, y en los ejemplos citados seria por las moléculas presentes en el aire (N², O²,Ar, Cr², etc).
El movimiento se conoce como movimiento browniano en memoria del botánico inglés Robert Brown, quien observó por primera vez este movimiento irregular de partículas en 1827, mientras estudiaba con el microscopio el comportamiento de los granos de polen suspendidos en agua. El movimiento browniano impide que las partículas coloidales se asienten o formen sedimentos.

Consiste en la migración de partículas coloidales cargadas dentro de un campo eléctrico. Las partículas coloidales absorben iones en su superficie cargándose positiva o negativamente, aunque todo el sistema coloidal es eléctricamente neutro, estas partículas viajan hacia los electrodos (cátodo y ánodo) mediante fuerzas eléctricas de atracción.

Se define como el movimiento de iones y moléculas pequeñas a través de una membrana porosa, llamada membrana dialítica o dializante, pero no de moléculas grandes o partículas coloidales. La diálisis no es una propiedad exclusiva de los coloides, puesto que ciertas soluciones también se pueden dializar, por ejemplo, en bioquímica se utiliza con frecuencia la diálisis para separar moléculas proteínicas de iones acuosos.
En los coloides, la diálisis permite purificar el sistema coloidal, puesto que se eliminan iones y otras moléculas pequeñas consideradas impurezas. Se utilizan como membranas dialíticas, el celofán y las membranas de origen animal.




</doc>
<doc id="11477" url="https://es.wikipedia.org/wiki?curid=11477" title="Aluminosilicato">
Aluminosilicato

Un aluminosilicato es un mineral que contiene óxido de aluminio (AlO) y sílice (SiO). Son alumninosilicatos el feldespato, las cloritas, los minerales de la arcilla, algún tipo de puzolana, etc. Los aluminosilicatos se suelen considerar como derivados de los silicatos debido al reemplazo de los iones Si por los de Al, los cuales, debido a la diferencia de cargas positivas, requieren cationes adicionales con la finalidad de poder alcanzar la neutralidad eléctrica.

Algunos aluminosilicatos, como por ejemplo los feldespatos, se encuentran dentro de los minerales más abundantes de la corteza de la tierra.
Entre ellos, el aluminio reemplaza a uno de cada cuatro átomos de silicio, e incluso a uno de cada dos, como ocurre por ejemplo en la anortita (CaAlSiO). Como consecuencia de la erosión, los feldespastos se suelen ver transformados en silicatos de tipo laminar, los cuales constituyen las arcillas que se encuentran presente en los suelos.
Se clasifican en:
Pueden ser de origen natural o sintético. En México hay minas de aluminosilicatos y zeolitas en diferentes estados como: Oaxaca, Michoacán, Veracruz y Sonora.

La composición química de los aluminosilicatos es variable, también la de los diferentes tipos de micotoxinas, por lo tanto su capacidad fijadora de micotoxinas será diferente en cada caso.

Algunos silicatos son utilizados como materias primas en la fabricación de materiales usados diariamente, como pueden ser por ejemplo, el cemento, el vidrio, la cerámica, etc. Por otro lado, las zeolitas han sido empleadas en la agricultura desde los 60s debido a sus propiedades de intercambio catiónico, tamizado molecular y adsorción.



</doc>
<doc id="11479" url="https://es.wikipedia.org/wiki?curid=11479" title="Elasticidad">
Elasticidad

Elasticidad puede hacer referencia a:




</doc>
<doc id="11480" url="https://es.wikipedia.org/wiki?curid=11480" title="Dureza">
Dureza

La dureza es la oposición que ofrecen los materiales a alteraciones físicas como la penetración, la abrasión y el rayado.

En metalurgia la dureza se mide utilizando un durómetro para el ensayo de penetración de un indentador. Dependiendo del tipo de punta empleada y del rango de cargas aplicadas, existen diferentes escalas, adecuadas para distintos rangos de dureza.

El interés de la determinación de la dureza en los aceros estriba en la correlación existente entre la dureza y la resistencia mecánica, siendo un método de ensayo más económico y rápido que el ensayo de tracción, por lo que su uso está muy extendido.

Hasta la aparición de la primera máquina Brinell para la determinación de la dureza, ésta se medía de forma cualitativa empleando una lima de acero templado que era el material más duro que se empleaba en los talleres.

Las escalas de uso industrial actuales son las siguientes:


La nanoindentación es un ensayo de dureza llevado a cabo a la escala de longitudes nanométricas. Se utiliza una punta pequeña para indentar el material objeto de estudio. La carga impuesta y el desplazamiento se miden de manera continua con una resolución de micronewtons y subnanómetros, respectivamente. La carga y el desplazamiento se miden simultáneamente durante el proceso de indentación y por ello también se la denomina «nanoindentación instrumentada». Las técnicas de nanoindentación son importantes para la medición de las propiedades mecánicas en aplicaciones microelectrónicas y para la deformación de estructuras a micro y nanoescala. Los nanoindentadores incorporan microscopios ópticos para la localización del area a estudiar. Sin embargo, a diferencia de los métodos de indentacion a macro y microescala, en la técnica de nanoindentación instrumentada no esposible medir directamente el área de la indentación.

Las puntas de los nanopenetradores vienen en una variedad de formas. A una forma común se le conoce como penetrador de Berkovich, el cual es una pirámide con 3 lados.

Oliver y Pharr inventaron un método para calcular el área proyectada de la indentación formula_1 durante la máxima carga. La primera etapa de una prueba de nanoindentación involucra el desarrollo de indentaciones sobre un patrón de calibración. La sílice fundida es un patrón de calibración común, debido a que tiene propiedades mecánicas homogéneas y bien caracterizadas. El propósito de efectuar indentaciones sobre el estándar de calibración es determinar el área de contacto proyectada de la punta del penetrador A como una función de la profundidad de la indentación. Para una punta de Berkovich perfecta,

formula_2

Sin embargo, en general la punta no perfecta, se desgasta y cambia de forma con cada uso. Por tanto, debe llevarse a cabo regularmente una calibración de la punta que se utiliza. Para ello es necesario encontrar la función relaciona el área A de la sección transversal del penetrador a máxima carga con la distancia de la punta h que está en contacto con el material que se está indentando. 

La profundidad total de la indentación h es la suma de la profundidad de contacto h y la profundidad h en la periferia de la indentación donde el indentador no hace contacto con la superficie del material, es decir,

formula_3

donde,

formula_4 Ɛ formula_5
donde P es la carga máxima y Ɛ es una constante geométrica igual a 0.75 para un penetrador de Berkovich. S es la rigidez al descargar, que se calcula en la curva de nanoindentación: formula_6

La dureza de un material determinada por la nanoindentación instrumentada se calcula entonces como:

formula_7

La dureza (determinada por la nanoindentación) se reporta con unidades de GPa y los resultados de indentaciones múltiples por lo general se promedian para incrementar la precisión.

Este análisis permite el cálculo del módulo elástico y la dureza durante la carga máxima y es conocido como nanoindentación instrumentada; sin embargo, actualmente se emplea de modo normal una técnica experimental conocida como nanoindentación dinámica. Durante ésta, se superpone una carga oscilante pequeña sobre la carga total en la muestra. De esta manera, la muestra se descarga de manera elástica continuamente a medida que se incrementa la carga total. Esto permite mediciones continuas del módulo elástico y de la rigidez como una función de la profundidad de la indentación.

En mineralogía se utiliza la escala de Mohs, creada por el alemán Friedrich Mohs en 1820, que mide la resistencia al rayado de los materiales.

A un nivel profesional, se utilizan en mineralogía, las escala de Rosiwal y de Knoop, ya que estas permiten realizar la valoración de medias con una cuantificación absoluta.

Lista de equivalencias aproximadas para escalas de dureza de aceros no austeníticos (en el rango de la escala "Rockwell C"):

Para aceros no aleados y fundiciones, existe una relación aproximada y directa entre la dureza Vickers y el límite elástico, siendo el límite elástico aproximadamente 3,3 veces la dureza Vickers.

Rp0,2==3,3*HV



</doc>
<doc id="11481" url="https://es.wikipedia.org/wiki?curid=11481" title="Industria">
Industria

La industria es la actividad que tiene como propósito transformar las materias primas en 
productos elaborados o semielaborados, utilizando una fuente de energía. Además de materiales, para su desarrollo la industria necesita maquinaria y recursos humanos organizados habitualmente en empresas por su especialización laboral. Existen diferentes clases de industrias en virtud del propósito ético fundacional de su actividad (ecológicas: fundamentos ecologistas) y tipos que la demarcan en ámbitos sectoriales según sean los productos que fabrican. Por ejemplo, la industria alimentaria se dedica a la elaboración de productos destinados a la alimentación, como el queso, los embutidos y las conservas, entre otros.

Desde el origen del ser humano, este ha tenido la necesidad de transformar los elementos de la naturaleza para poder aprovecharse de ellos, en sentido estricto ya existía la industria, pero a finales del siglo XVIII, y durante el siglo XIX, cuando el proceso de transformación de los recursos de la naturaleza sufre un cambio radical, que se conoce como revolución industrial.

Este cambio se basa en la disminución del tiempo de trabajo necesario para transformar un recurso en un producto útil, gracias a la utilización de un modo de producción capitalista, que pretende la consecución de un beneficio aumentando los ingresos y disminuyendo los gastos. Con la revolución industrial el capitalismo adquiere una nueva dimensión, y la transformación de la naturaleza alcanza límites insospechados hasta entonces.

La industria fue el sector motor de la economía desde el siglo XIX y, hasta la Segunda Guerra Mundial, la industria era el sector económico que más aportaba al Producto Interior Bruto (PIB), y el que más mano de obra ocupaba. Desde entonces, y con el aumento de la productividad por la mejora de las máquinas y el desarrollo de los servicios, ha pasado a un segundo término. Sin embargo, continúa siendo esencial, puesto que no puede haber servicios sin desarrollo industrial.

El capital de inversión en Europa procede de la acumulación de riqueza en la agricultura. El capital agrícola se invertirá en la industria y en los medios de transporte necesarios para poner en el mercado los productos elaborados.

En principio los productos industriales aumentan la productividad de la tierra, con lo que se disminuye fuerza de trabajo para la industria y se obtienen productos agrícolas excedentarios para alimentar a una creciente población urbana, que no vive del campo. La agricultura, pues, proporciona a la industria capitales, fuerza de trabajo y mercancías. Todo ello es una condición necesaria para el desarrollo de la revolución industrial. 

Gracias a la revolución industrial las regiones se pueden especializar, sobre todo debido a la creación de medios de transporte eficaces, en un mercado nacional y otro mercado internacional, lo más libre posible de trabas arancelarias y burocráticas. 

Una nueva estructura económica, y la destrucción de la sociedad tradicional, garantizaron la disponibilidad de suficiente fuerza de trabajo asalariada y voluntaria.

En los países del Tercer Mundo, y en algunos países de industrialización tardía, el capital lo proporciona la inversión extranjera, que monta las infraestructuras necesarias para extraer la riqueza y las plusvalías que genera la fuerza de trabajo; sin liberar de las tareas agrícolas a la mano de obra necesaria, sino solo a la imprescindible. En un principio hubo de recurrirse a la esclavitud para garantizar la mano de obra. Pero el cambio de la estructura económica, y la destrucción de la sociedad tradicional, garantizó la disponibilidad de suficientes capitales.

La manufactura es la forma más elemental de la industria; la palabra significa "hacer a mano" 
pero en economía significa transformar la materia prima en un producto de utilidad concreta. Casi todo lo que usamos es un fruto de este proceso, y casi todo lo que es manufactura se elabora en grandes fábricas. Los artesanos también fabrican mercancías, bien sea solos o en pequeños grupos.
Hay mercancías que necesitan fabricarse en varias etapas, por ejemplo los automóviles, que se construyen con piezas que se han hecho en otras, por lo general de otros países y del mismo.
O está constituida por empresas desde muy pequeñas (tortillerías, panaderías y molinos, entre otras) hasta grandes conglomerados (armadoras de automóviles, embotelladoras de refrescos, empacadoras de alimentos, laboratorios farmacéuticos y fábricas de juguetes).





</doc>
<doc id="11484" url="https://es.wikipedia.org/wiki?curid=11484" title="Pulvimetalurgia">
Pulvimetalurgia

La pulvimetalurgia o metalurgia de polvos es un proceso de fabricación de objetos metálicos que, partiendo de polvos finos y tras su compactación para darles una forma determinada, se calientan en una atmósfera controlada para la obtención de la pieza.

Este proceso es adecuado para la fabricación de grandes series de piezas pequeñas de gran precisión, para materiales o mezclas poco comunes y para controlar el grado de porosidad o permeabilidad.
Algunos productos típicos son rodamientos, árboles de levas, herramientas de corte, segmentos de pistones, guías de válvulas, filtros, etc.

Generalmente se realiza de metales puros, principalmente hierro, cobre, estaño, aluminio, níquel y titanio, aleaciones como latones, bronces, aceros y aceros inoxidables o polvos pre-aleados. Procesos típicos son:






Generalmente, para obtener las características requeridas será necesario mezclar polvos de tamaños y composiciones diferentes. Igualmente se puede añadir aditivos que actúen como lubricantes durante el compactado o aglutinantes que incrementen la resistencia del compactado podrido.

Debido a la elevada relación área superficial/volumen esto quiere decir que cuanto más dividido esté el polvo, más área de exposición al medio ambiente posee este. La mayoría de los polvos metálicos tienden a reaccionar con el oxígeno del ambiente generando así una flama en la mayoría de los casos, además de otros como el magnesio que es explosivo, por lo que deberán manejarse con precaución, y para contenerlos (los polvos) se utilizan normalmente cuartos de ambientes controlados.

El polvo suelto se comprime mediante prensas mecánicas o hidráulicas en una matriz, resultando una forma que se conoce como pieza en verde o compactado crudo. Las prensas más utilizadas son uniaxiales, en la que la presión se aplica al polvo en una sola dirección. Mediante compactación uniaxial pueden obtenerse piezas en verde con dimensiones y acabados precisos, obteniéndose una alta productividad en la industria mediante esta técnica. Un inconveniente de la compactación uniaxial es la baja relación longitud/diámetro que puede obtenerse en las piezas debido al gradiente de densidad que se produce entre el centro de la pieza y las zonas más próximas al punzón. Para obtener un compacto con mayor densidad se emplean prensas de doble émbolo. 

Variantes: "Prensado isostático en frío ("Cold Isostatic Pressing, CIP")". Es un método de compactación que se realiza encerrando herméticamente el polvo en moldes elásticos típicamente de goma, látex o PVC, aplicándoles presión hidrostática mediante un fluido que puede ser agua o aceite. Las piezas en verde obtenidas por este sistema tienen propiedades uniformes e isótropas. Una de las principales ventajas de este método de compactación es la alta relación longitud/diámetro que puede obtenerse en las piezas con respecto a la compactación uniaxial. Es un método muy utilizado para la compactación de piezas cerámicas.

Consiste en el calentamiento en horno de mufla con atmósfera controlada a una temperatura en torno al 75 % de la de fusión. En general, los hornos son continuos con tres cámaras:




En todo el proceso, es fundamental mantener una atmósfera controlada para evitar la rápida oxidación de las pequeñas partículas metálicas al elevarse las temperaturas en presencia de oxígeno. Para ello, se emplean atmósferas reductoras basadas en hidrógeno, amoníaco disociado y nitrógeno.

Variantes: Prensado isostático en caliente ("Hot Isostatic Pressing"', HIP). La compactación y el sinterizado se realizan en una única etapa encerrando herméticamente el polvo en un recipiente flexible y exponiéndolo seguidamente a alta temperatura y presión. Los productos obtenidos por este sistema tienen propiedades uniformes e isótropas. Pueden obtenerse valores elevados de densidad en las piezas debido a la baja porosidad residual que queda en las piezas tras el proceso, con valores en muchos casos superiores al 99 % de la densidad teórica del material completamente denso (sin porosidad). 

Por otro lado, también es posible, cuando desee realizarse algún mecanizado, realizar un presinterizado del compactado de forma que pueda manipularse y mecanizarse sin dificultad. Tras el sinterizado definitivo, el mecanizado posterior puede minimizarse e incluso eliminarse.

Si el sinterizado se efectúa durante un tiempo prolongado puede eliminarse los poros y el material se hace más denso. La velocidad de sinterizado depende de la Temperatura, energía de activación, coeficiente de difusión, tamaño de las partículas originales.





</doc>
<doc id="11485" url="https://es.wikipedia.org/wiki?curid=11485" title="Cobre">
Cobre

El cobre (del latín "cuprum", y este del griego "kypros", "Chipre"), cuyo símbolo es Cu, es el elemento químico de número atómico 29. Se trata de un metal de transición de color cobrizo, es decir, rojizo anaranjado de brillo metálico que, junto con la plata, el oro y el roentgenio forma parte de la llamada familia del cobre, se caracteriza por ser uno de los mejores conductores de electricidad (el segundo después de la plata). Gracias a su alta conductividad eléctrica, ductilidad y maleabilidad, se ha convertido en el material más utilizado para fabricar cables eléctricos y otros elementos eléctricos y componentes electrónicos.

El cobre forma parte de una cantidad muy elevada de aleaciones que generalmente presentan mejores propiedades mecánicas, aunque tienen una conductividad eléctrica menor. Las más importantes son conocidas con el nombre de bronces y latones. Por otra parte, el cobre es un metal duradero porque se puede reciclar un número casi ilimitado de veces sin que pierda sus propiedades mecánicas.

Fue uno de los primeros metales en ser utilizado por el ser humano en la prehistoria. El cobre y su aleación con el estaño, el bronce, adquirieron tanta importancia que los historiadores han llamado Edad del cobre y Edad del Bronce a dos periodos de la Antigüedad. Aunque su uso perdió importancia relativa con el desarrollo de la siderurgia, el cobre y sus aleaciones siguieron siendo empleados para hacer objetos tan diversos como monedas, campanas y cañones. A partir del siglo XIX, concretamente de la invención del generador eléctrico en 1831 por Faraday, el cobre se convirtió de nuevo en un metal estratégico, al ser la materia prima principal de cables e instalaciones eléctricas.

El cobre posee un importante papel biológico en el proceso de fotosíntesis de las plantas, aunque no forma parte de la composición de la clorofila. El cobre contribuye a la formación de glóbulos rojos y al mantenimiento de los vasos sanguíneos, nervios, sistema inmunitario y huesos y por tanto es un oligoelemento esencial para la vida humana.

El cobre se encuentra en una gran cantidad de alimentos habituales de la dieta tales como ostras, mariscos, legumbres, vísceras y nueces entre otros, además del agua potable y por lo tanto es muy raro que se produzca una deficiencia de cobre en el organismo. El desequilibrio de cobre ocasiona en el organismo una enfermedad hepática conocida como enfermedad de Wilson.

El cobre es el tercer metal más utilizado en el mundo, por detrás del hierro y el aluminio. La producción mundial de cobre refinado se estimó en 15,8 Mt en el 2006, con un déficit de 10,7 % frente a la demanda mundial proyectada de 17,7 Mt. Los pórfidos cupríferos constituyen la principal fuente de extracción de cobre en el mundo.


El cobre es uno de los pocos metales que pueden encontrarse en la naturaleza en forma de cobre nativo, es decir, sin combinar con otros elementos. Por ello fue uno de los primeros en ser utilizado por el ser humano. Los otros metales nativos son el oro, el platino, la plata y el hierro proveniente de meteoritos.

Se han encontrado utensilios de cobre nativo en torno al 7000 a. C. en Çayönü Tepesí (en la actual Turquía) y en Irak). El cobre de Çayönü Tepesí fue recocido pero el proceso aún no estaba perfeccionado. En esta época, en Oriente Próximo también se utilizaban carbonatos de cobre (malaquita y azurita) con motivos ornamentales. En la región de los Grandes Lagos de América del Norte, donde abundaban los yacimientos de cobre nativo, desde el 4000 a. C. los indígenas acostumbraban a golpearlas hasta darles forma de punta de flecha, aunque nunca llegaron a descubrir la fusión.

Los primeros crisoles para producir cobre metálico a partir de carbonatos mediante reducciones con carbón datan del V milenio a. C. Es el inicio de la llamada Edad del cobre, apareciendo crisoles en toda la zona entre los Balcanes e Irán, incluyendo Egipto. Se han encontrado pruebas de la explotación de minas de carbonatos de cobre desde épocas muy antiguas tanto en Tracia (Ai Bunar) como en la península del Sinaí. De un modo endógeno, no conectado con las civilizaciones del Viejo Mundo, en la América precolombina, en torno al siglo IV a. C. la cultura Moche desarrolló la metalurgia del cobre ya refinado a partir de la malaquita y otros carbonatos cupríferos.

Hacia el 3500 a. C. la producción de cobre en Europa entró en declive a causa del agotamiento de los yacimientos de carbonatos. Por esta época se produjo la irrupción desde el este de unos pueblos, genéricamente denominados kurganes, que portaban una nueva tecnología: el uso del cobre arsenical. Esta tecnología, quizás desarrollada en Oriente Próximo o en el Cáucaso, permitía obtener cobre mediante la oxidación de sulfuro de cobre. Para evitar que el cobre se oxidase, se añadía arsénico al mineral. El cobre arsenical (a veces llamado también "bronce arsenical") era más cortante que el cobre nativo y además podía obtenerse de los muy abundantes yacimientos de sulfuros. Uniéndolo a la también nueva tecnología del molde de dos piezas, que permitía la producción en masa de objetos, los kurganes se equiparon de hachas de guerra y se extendieron rápidamente.

Ötzi, el cadáver hallado en los Alpes y datado hacia el 3300 a. C., llevaba un hacha de cobre con un 99,7 % de cobre y un 0,22 % de arsénico. De esta época data también el yacimiento de Los Millares (Almería, España), centro metalúrgico cercano a las minas de cobre de la sierra de Gádor.

No se sabe cómo ni dónde surgió la idea de añadir estaño al cobre, produciendo el primer bronce. Se cree que fue un descubrimiento imprevisto, ya que el estaño es más blando que el cobre y, sin embargo, al añadirlo al cobre se obtenía un material más duro cuyos filos se conservaban más tiempo. El descubrimiento de esta nueva tecnología desencadenó el comienzo de la Edad del Bronce, fechado en torno a 3000 a. C. para Oriente Próximo, 2500 a. C. para Troya y el Danubio y 2000 a. C. para China. En el yacimiento de Bang Chian, en Tailandia, se han datado objetos de bronce anteriores al año 2000 a. C. Durante muchos siglos el bronce tuvo un papel protagonista y cobraron gran importancia los yacimientos de estaño, a menudo alejados de los grandes centros urbanos de aquella época.

El declive del bronce empezó hacia el 1000 a. C., cuando surgió en Oriente Próximo una nueva tecnología que posibilitó la producción de hierro metálico a partir de minerales férreos. Las armas de hierro fueron reemplazando a las de cobre en todo el espacio entre Europa y Oriente Medio. En zonas como China la Edad del Bronce se prolongó varios siglos más. Hubo también regiones del mundo donde nunca llegó a utilizarse el bronce. Por ejemplo, el África subsahariana pasó directamente de la piedra al hierro.

Sin embargo, el uso del cobre y el bronce no desapareció durante la Edad del Hierro. Reemplazados en el armamento, estos metales pasaron a ser utilizados esencialmente en la construcción y en objetos decorativos como estatuas. El latón, una aleación de cobre y zinc fue inventado hacia el 600 a. C. También hacia esta época se fabricaron las primeras monedas en el estado de Lidia, en la actual Turquía. Mientras que las monedas más valiosas se acuñaron en oro y plata, las de uso más cotidiano se hicieron de cobre y bronce.

La búsqueda de cobre y metales preciosos por el Mediterráneo condujo a los cartagineses a explotar el gran yacimiento de Río Tinto, en la actual provincia de Huelva. Tras las guerras púnicas los romanos se apoderaron de estas minas y las siguieron explotando hasta agotar todo el óxido de cobre. Debajo de él quedó una gran veta de sulfuro de cobre, el cual los romanos no sabían aprovechar eficazmente. A la caída del Imperio romano la mina había sido abandonada y solo fue reabierta cuando los andalusíes inventaron un proceso más eficaz para extraer el cobre del sulfuro.

La resistencia a la corrosión del cobre, el bronce y el latón permitió que estos metales hayan sido utilizados no solo como decorativos sino también como funcionales desde la Edad Media hasta nuestros días. Entre los siglos X y XII se hallaron en Europa Central grandes yacimientos de plata y cobre, principalmente Rammelsberg y Joachimsthal. De ellos surgió una gran parte de la materia prima para realizar las grandes campanas, puertas y estatuas de las catedrales góticas europeas. Además del uso bélico del cobre para la fabricación de objetos, como hachas, espadas, cascos o corazas; también se utilizó el cobre en la Edad Media en luminarias como candiles o candelabros; en braseros y en objetos de almacenamiento, como arcas o estuches.

Los primeros cañones europeos de hierro forjado datan del siglo XIV, pero hacia el siglo XVI el bronce se impuso como el material casi único para toda la artillería y mantuvo ese dominio hasta bien entrado el siglo XIX. En el Barroco, durante los siglos XVII y XVIII, el cobre y sus aleaciones adquirieron gran importancia en la construcción de obras monumentales, la producción de maquinaria de relojería y una amplia variedad de objetos decorativos y funcionales. Las monarquías autoritarias del Antiguo Régimen utilizaron el cobre en aleación con la plata (denominada vellón) para realizar repetidas devaluaciones monetarias, llegando a la emisión de monedas puramente de cobre, características de las dificultades de la Hacienda de la Monarquía Hispánica del siglo XVII (que lo utilizó en tanta cantidad que tuvo que recurrir a importarlo de Suecia).

Durante 1831 y 1832, Michael Faraday descubrió que un conductor eléctrico moviéndose perpendicularmente a un campo magnético generaba una diferencia de potencial. Aprovechando esto, construyó el primer generador eléctrico, el disco de Faraday, empleando un disco de cobre que giraba entre los extremos de un imán con forma de herradura, induciendo una corriente eléctrica. El posterior desarrollo de generadores eléctricos y su empleo en la historia de la electricidad ha dado lugar a que el cobre haya obtenido una importancia destacada en la humanidad, que ha aumentado su demanda notablemente.

Durante gran parte del siglo XIX, Gran Bretaña fue el mayor productor mundial de cobre, pero la importancia que fue adquiriendo el cobre motivó la explotación minera en otros países, llegando a destacarse la producción en Estados Unidos y Chile, además de la apertura de minas en África. De esta forma, en 1911 la producción mundial de cobre superó el millón de toneladas de cobre fino.

La aparición de los procesos que permitían la producción masiva de acero a mediados del siglo XIX, como el convertidor Thomas-Bessemer o el horno Martin-Siemens dio lugar a que se sustituyera el uso del cobre y de sus aleaciones en algunas aplicaciones determinadas donde se requería un material más tenaz y resistente. Sin embargo, el desarrollo tecnológico que siguió a la Revolución industrial en todas las ramas de la actividad humana y los adelantos logrados en la metalurgia del cobre han permitido producir una amplia variedad de aleaciones. Esto ha dado lugar a que se incrementen los campos de aplicación del cobre, lo cual, añadido al desarrollo económico de varios países, ha conllevado un notable aumento de la demanda mundial.

Desde principios del siglo XIX existió producción de cobre en los Estados Unidos, primero en Míchigan y más tarde en Arizona. Se trataba de pequeñas minas que explotaban mineral de alta ley.

El desarrollo del proceso de flotación, más eficaz, hacia finales del siglo XIX permitió poner en explotación grandes yacimientos de baja ley, principalmente en Arizona, Montana y Utah. En pocos años Estados Unidos se convirtió en el primer productor mundial de cobre.

En 1916 las minas estadounidenses produjeron por vez primera más de un millón de toneladas de cobre, representando en torno a las tres cuartas partes de la producción mundial. La producción minera bajó fuertemente a partir de la crisis de 1929, no solo por la reducción del consumo sino porque se disparó el reciclaje de metal. La demanda se recuperó a finales de los años 30, volviendo a superar las minas estadounidenses el millón de toneladas en 1940. Sin embargo, esta cifra ya representaba "solo" la mitad de la producción mundial y no llegaba a cubrir la demanda interna, por lo que en 1941 el país se convirtió por primera vez en importador neto de cobre.

Desde los años 1950 hasta la actualidad la producción de Estados Unidos ha oscilado entre uno y dos millones de toneladas anuales, lo cual representa una fracción cada vez menor del total mundial (27 % en 1970, 17 % en 1980, 8 % en 2006). Mientras tanto, el consumo ha seguido creciendo continuamente y ello ha obligado a importar cantidades cada vez mayores de metal, superándose el millón de toneladas importadas por vez primera en 2001.

En 1810, año de su primera junta nacional, Chile producía unas 19 000 toneladas de cobre al año. A lo largo del siglo, la cifra fue creciendo hasta convertir al país en el primer productor y exportador mundial. Sin embargo, a finales del siglo XIX, comenzó un periodo de decadencia, debido por un lado al agotamiento de los yacimientos de alta ley y por otro al hecho de que la explotación del salitre acaparaba las inversiones mineras. En 1897, la producción había caído a 21 000 toneladas, casi lo mismo que en 1810.

La situación cambió a comienzos del siglo XX, cuando grandes grupos mineros dotados de este país obtuvieron avances tecnológicos que permitieron la recuperación de cobre en yacimientos de baja concentración, iniciando la explotación de los yacimientos chilenos.

El Estado chileno recibió pocos beneficios de la minería del cobre durante toda la primera mitad del siglo XX. La situación empezó a cambiar en 1951 con la firma del Convenio de Washington, que le permitió disponer del 20 % de la producción. En 1966, el Congreso Nacional de Chile impuso la creación de Sociedades Mineras Mixtas con las empresas extranjeras en las cuales el Estado tendría el 51 % de la propiedad de los yacimientos. El proceso de chilenización del cobre culminó en julio de 1971, bajo el mandato de Salvador Allende, cuando el Congreso aprobó por unanimidad la nacionalización de la Gran Minería del cobre.

En 1976, ya bajo la dictadura militar de Pinochet, el Estado fundó la Corporación Nacional del Cobre de Chile (Codelco) para gestionar las grandes minas de cobre.

La mina de Chuquicamata, en la cual se han encontrado evidencias de la extracción de cobre por culturas precolombinas, inició su construcción para la explotación industrial en 1910; su explotación se inició el 18 de mayo de 1915. Chuquicamata es la mina a cielo abierto más grande del mundo y fue durante varios años la mina de cobre de mayor producción del mundo. En 2002, se fusionaron las divisiones de Chuquicamata y Radomiro Tomic, creando el complejo minero Codelco Norte, que consta de dos minas a cielo abierto, "Chuquicamata" y "Mina Sur". Aunque el yacimiento de Radomiro Tomic fue descubierto en los años 1950, sus operaciones comenzaron en 1995, una vez actualizados los estudios de viabilidad técnica y económica.

En 1995, se inició la construcción de la mina de Minera Escondida, en la II Región de Antofagasta, y en 1998 se iniciaron las operaciones de extracción. Es la mina de mayor producción del mundo. La Huelga de la Minera Escondida en el 2006 paralizó la producción durante 25 días y alteró los precios mundiales del cobre. La producción de Minera Escondida alcanzó en 2007 las 1 483 934 t. Esta producción representa el 9,5 % de la producción mundial y el 26 % de la producción chilena de cobre, según estimaciones para 2007.

En las últimas décadas, Chile se ha consolidado como el mayor productor mundial de cobre, pasando del 14 % de la producción mundial en 1960 al 36 % en 2006.

En la naturaleza se encuentran dos isótopos estables: Cu y Cu. El más ligero de ellos es el más abundante (69,17 %). Se han caracterizado hasta el momento 25 isótopos radiactivos de los cuales los más estables son el Cu, el Cu y el Cu con periodos de semidesintegración de 61,83 horas, 12,70 horas y 3,333 horas respectivamente. Los demás radioisótopos, con masas atómicas desde 54,966 uma (Cu) a 78,955 uma (Cu), tienen periodos de semidesintegración inferiores a 23,7 minutos y la mayoría no alcanzan los 30 segundos. Los isótopos Cu y Cu presentan estados metaestables con un periodo de semidesintegración mayor al del estado fundamental.

Los isótopos más ligeros que el Cu estable se desintegran principalmente por emisión beta positiva, originando isótopos de níquel, mientras que los más pesados que el isótopo Cu estable se desintegran por emisión beta negativa dando lugar a isótopos de cinc. El isótopo Cu se desintegra generando Zn, por captura electrónica y emisión beta positiva en un 69 % y por desintegración beta negativa genera Ni en el 31 % restante.

El cobre posee varias propiedades físicas que propician su uso industrial en múltiples aplicaciones, siendo el tercer metal, después del hierro y del aluminio, más consumido en el mundo. Es de color rojizo y de brillo metálico y, después de la plata, es el elemento con mayor conductividad eléctrica y térmica. Es un material abundante en la naturaleza; tiene un precio accesible y se recicla de forma indefinida; forma aleaciones para mejorar las prestaciones mecánicas y es resistente a la corrosión y oxidación.

La conductividad eléctrica del cobre puro fue adoptada por la Comisión Electrotécnica Internacional en 1913 como la referencia estándar para esta magnitud, estableciendo el International Annealed Copper Standard (Estándar Internacional del Cobre Recocido) o IACS. Según esta definición, la conductividad del cobre recocido medida a 20 °C es igual a 5,80 × 10S/m. A este valor de conductividad se le asigna un índice 100 % IACS y la conductividad del resto de los materiales se expresa en porcentaje de IACS. La mayoría de los metales tienen valores de conductividad inferiores a 100 % IACS pero existen excepciones como la plata o los cobres especiales de muy alta conductividad designados C-103 y C-110.

Tanto el cobre como sus aleaciones tienen una buena maquinabilidad, es decir, son fáciles de mecanizar. El cobre posee muy buena ductilidad y maleabilidad lo que permite producir láminas e hilos muy delgados y finos. Es un metal blando, con un índice de dureza 3 en la escala de Mohs (50 en la escala de Vickers) y su resistencia a la tracción es de 210 MPa, con un límite elástico de 33,3 MPa. Admite procesos de fabricación de deformación como laminación o forja, y procesos de soldadura y sus aleaciones adquieren propiedades diferentes con tratamientos térmicos como temple y recocido. En general, sus propiedades mejoran con bajas temperaturas lo que permite utilizarlo en aplicaciones criogénicas.

En la mayoría de sus compuestos, el cobre presenta estados de oxidación bajos, siendo el más común el +2, aunque también hay algunos con estado de oxidación +1.

Expuesto al aire, el color rojo salmón, inicial se torna rojo violeta por la formación de óxido cuproso (CuO) para ennegrecerse posteriormente por la formación de óxido cúprico (CuO). La coloración azul del Cu se debe a la formación del ion [Cu (OH)].

Expuesto largo tiempo al aire húmedo, forma una capa adherente e impermeable de carbonato básico (carbonato cúprico) de color verde y venenoso. También pueden formarse pátinas de cardenillo, una mezcla venenosa de acetatos de cobre de color verdoso o azulado que se forma cuando los óxidos de cobre reaccionan con ácido acético, que es el responsable del sabor del vinagre y se produce en procesos de fermentación acética. Al emplear utensilios de cobre para la cocción de alimentos, deben tomarse precauciones para evitar intoxicaciones por cardenillo que, a pesar de su mal sabor, puede ser enmascarado con salsas y condimentos y ser ingerido.

Los halógenos atacan con facilidad al cobre, especialmente en presencia de humedad. En seco, el cloro y el bromo no producen efecto y el flúor solo le ataca a temperaturas superiores a 500 °C. El cloruro cuproso y el cloruro cúprico, combinados con el oxígeno y en presencia de humedad producen ácido clorhídrico, ocasionando unas manchas de atacamita o paratacamita, de color verde pálido a azul verdoso, suaves y polvorientas que no se fijan sobre la superficie y producen más cloruros de cobre, iniciando de nuevo el ciclo de la erosión.

Los ácidos oxácidos atacan al cobre, por lo cual se utilizan estos ácidos como decapantes (ácido sulfúrico) y abrillantadores (ácido nítrico). El ácido sulfúrico reacciona con el cobre formando un sulfuro, CuS (covelina) o CuS (calcocita) de color negro y agua. También pueden formarse sales de sulfato cúprico (antlerita) con colores de verde a azul verdoso. Estas sales son muy comunes en los ánodos de los acumuladores de plomo que se emplean en los automóviles.

El ácido cítrico disuelve el óxido de cobre, por lo que se aplica para limpiar superficies de cobre, lustrando el metal y formando citrato de cobre. Si después de limpiar el cobre con ácido cítrico, se vuelve a utilizar el mismo paño para limpiar superficies de plomo, el plomo se bañará de una capa externa de citrato de cobre y citrato de plomo con un color rojizo y negro.

En las plantas, el cobre posee un importante papel en el proceso de la fotosíntesis y forma parte de la composición de la plastocianina. Alrededor del 70 % del cobre de una planta está presente en la clorofila, principalmente en los cloroplastos. Los primeros síntomas en las plantas por deficiencia de cobre aparecen en forma de hojas estrechas y retorcidas, además de puntas blanquecinas. Las panículas y las vainas pueden aparecer vacías por una deficiencia severa de cobre, ocasionando graves pérdidas económicas en la actividad agrícola.

El cobre contribuye a la formación de glóbulos rojos y al mantenimiento de los vasos sanguíneos, nervios, sistema inmunitario y huesos y por tanto es esencial para la vida humana. El cobre se encuentra en algunas enzimas como la "citocromo c" oxidasa, la "lisil oxidasa" y la "superóxido dismutasa".

El desequilibrio de cobre en el organismo cuando se produce en forma excesiva ocasiona una enfermedad hepática conocida como enfermedad de Wilson, el origen de esta enfermedad es hereditario, y aparte del trastorno hepático que ocasiona también daña al sistema nervioso. Se trata de una enfermedad poco común.

Puede producirse deficiencia de cobre en niños con una dieta pobre en calcio, especialmente si presentan diarreas o desnutrición. También hay enfermedades que disminuyen la absorción de cobre, como la enfermedad celiaca, la fibrosis quística o al llevar dietas restrictivas.

El cobre se encuentra en una gran cantidad de alimentos habituales de la dieta tales como ostras, mariscos, legumbres, vísceras y nueces entre otros, además del agua potable y por lo tanto es muy raro que se produzca una deficiencia de cobre en el organismo.

A pesar de que el cobre es un oligoelemento necesario para la vida, unos niveles altos de este elemento en el organismo pueden ser dañinos para la salud. La inhalación de niveles altos de cobre puede producir irritación de las vías respiratorias. La ingestión de niveles altos de cobre puede producir náuseas, vómitos y diarrea. Un exceso de cobre en la sangre puede dañar el hígado y los riñones, e incluso causar la muerte. Ingerir por vía oral una cantidad de 30 g de sulfato de cobre es potencialmente letal en los humanos.

Para las actividades laborales en las que se elaboran y manipulan productos de cobre, es necesario utilizar medidas de protección colectiva que protejan a los trabajadores. El valor límite tolerado es de 0,2 mg/m³ para el humo y 1 mg/m³ para el polvo y la niebla. El cobre reacciona con oxidantes fuertes tales como cloratos, bromatos y yoduros, originando un peligro de explosión. Además puede ser necesario el uso de equipos de protección individual como guantes, gafas y mascarillas. Además, puede ser recomendable que los trabajadores se duchen y se cambien de ropa antes de volver a su casa cada día.

La Organización Mundial de la Salud (OMS) en su "Guía de la calidad del agua potable" recomienda un nivel máximo de 2 mg/l. El mismo valor ha sido adoptado en la Unión Europea como valor límite de cobre en el agua potable, mientras que en Estados Unidos la Agencia de Protección Ambiental ha establecido un máximo de 1,3 mg/l. El agua con concentraciones de cobre superiores a 1 mg/l puede ensuciar la ropa al lavarla y presentar un sabor metálico desagradable. La Agencia para Sustancias Tóxicas y el Registro de Enfermedades de Estados Unidos recomienda que, para disminuir los niveles de cobre en el agua potable que se conduce por tuberías de cobre, se deje correr el agua por lo menos 15 segundos antes de beberla o usarla por primera vez en la mañana.

Las actividades mineras pueden provocar la contaminación de ríos y aguas subterráneas con cobre y otros metales durante su explotación así como una vez abandonada la minería en la zona. El color turquesa del agua y las rocas se debe a la acción que el cobre y otros metales desarrollan durante su explotación minera.
Desde el punto de vista físico, el cobre puro posee muy bajo límite elástico (33 MPa) y una dureza escasa (3 en la escala de Mohs o 50 en la escala de Vickers). En cambio, unido en aleación con otros elementos adquiere características mecánicas muy superiores, aunque disminuye su conductividad. Existe una amplia variedad de aleaciones de cobre, de cuyas composiciones dependen las características técnicas que se obtienen, por lo que se utilizan en multitud de objetos con aplicaciones técnicas muy diversas. El cobre se alea principalmente con los siguientes elementos: Zn, Sn, Al, Ni, Be, Si, Cd, Cr y otros en menor cuantía.

Según los fines a los que se destinan en la industria, se clasifican en aleaciones para forja y en aleaciones para moldeo. Para identificarlas tienen las siguientes nomenclaturas generales según la norma ISO 1190-1:1982 o su equivalente UNE 37102:1984. Ambas normas utilizan el sistema UNS (del inglés "Unified Numbering System").

El latón, también conocido como "cuzin", es una aleación de cobre, cinc (Zn) y, en menor proporción, otros metales. Se obtiene mediante la fundición de sus componentes en un crisol o mediante la fundición y reducción de menas sulfurosas en un horno de reverbero o de cubilote. En los latones industriales, el porcentaje de Zn se mantiene siempre inferior a 50 %. Su composición influye en las características mecánicas, la fusibilidad y la capacidad de conformación por fundición, forja y mecanizado. En frío, los lingotes obtenidos se deforman plásticamente produciendo láminas, varillas o se cortan en tiras susceptibles de estirarse para fabricar alambres. Su densidad depende de su composición y generalmente ronda entre 8,4 g/cm³ y 8,7 g/cm³.

Las características de los latones dependen de la proporción de elementos que intervengan en la aleación de tal forma que algunos tipos de latón son maleables únicamente en frío, otros exclusivamente en caliente, y algunos no lo son a ninguna temperatura. Todos los tipos de latones se vuelven quebradizos cuando se calientan a una temperatura próxima al punto de fusión.

El latón es más duro que el cobre, pero fácil de mecanizar, grabar y fundir. Es resistente a la oxidación, a las condiciones salinas y es maleable, por lo que puede laminarse en planchas finas. Su maleabilidad varía la temperatura y con la presencia, incluso en cantidades mínimas, de otros metales en su composición.

Un pequeño aporte de plomo en la composición del latón mejora la maquinabilidad porque facilita la fragmentación de las virutas en el mecanizado. El plomo también tiene un efecto lubricante por su bajo punto de fusión, lo que permite ralentizar el desgaste de la herramienta de corte.

El latón admite pocos tratamientos térmicos y únicamente se realizan recocidos de homogeneización y recristalización. El latón tiene un color amarillo brillante, con parecido al oro, característica que es aprovechada en joyería, especialmente en bisutería, y en el galvanizado de elementos decorativos. Las aplicaciones de los latones abarcan otros campos muy diversos, como armamento, calderería, soldadura, fabricación de alambres, tubos de condensadores y terminales eléctricos. Como no es atacado por el agua salada, se usa también en las construcciones de barcos y en equipos pesqueros y marinos.

El latón no produce chispas por impacto mecánico, una propiedad atípica en las aleaciones. Esta característica convierte al latón en un material importante en la fabricación de envases para la manipulación de compuestos inflamables, cepillos de limpieza de metales y en pararrayos.

Las aleaciones en cuya composición predominan el cobre y el estaño (Sn) se conocen con el nombre de bronce y son conocidas desde la antigüedad. Hay muchos tipos de bronces que contienen además otros elementos como aluminio, berilio, cromo o silicio. El porcentaje de estaño en estas aleaciones está comprendido entre el 2 y el 22 %. Son de color amarillento y las piezas fundidas de bronce son de mejor calidad que las de latón, pero son más difíciles de mecanizar y más caras.

La tecnología metalúrgica de la fabricación de bronce es uno de los hitos más importantes de la historia de la humanidad pues dio origen a la llamada Edad de Bronce. El bronce fue la primera aleación fabricada voluntariamente por el ser humano: se realizaba mezclando el mineral de cobre (calcopirita, malaquita, etc.) y el de estaño (casiterita) en un horno alimentado con carbón vegetal. El resultante de la combustión del carbón, que se oxidaba formando anhídrido carbónico, producía la reducción los minerales de cobre y estaño a metales. El cobre y el estaño que se fundían, se aleaban entre un 5 y un 10 % en peso de estaño.

El bronce se emplea especialmente en aleaciones conductoras del calor, en baterías eléctricas y en la fabricación de válvulas, tuberías y uniones de fontanería. Algunas aleaciones de bronce se usan en uniones deslizantes, como cojinetes y descansos, discos de fricción; y otras aplicaciones donde se requiere alta resistencia a la corrosión como rodetes de turbinas o válvulas de bombas, entre otros elementos de máquinas. En algunas aplicaciones eléctricas es utilizado en resortes.

Las alpacas o platas alemanas son aleaciones de cobre, níquel (Ni) y zinc (Zn), en una proporción de 50-70 % de cobre, 13-25 % de níquel, y 13-25 % de zinc. Sus propiedades varían de forma continua en función de la proporción de estos elementos en su composición, pasando de máximos de dureza a mínimos de conductividad. Estas aleaciones tienen la propiedad de rechazar los organismos marinos ("antifouling"). Si a estas aleaciones de cobre-níquel-zinc se les añaden pequeñas cantidades de aluminio o hierro constituyen aleaciones que se caracterizan por su resistencia a la corrosión marina, por lo que se utilizan ampliamente en la construcción naval, principalmente en condensadores y tuberías, así como en la fabricación de monedas y de resistencias eléctricas.

Las aleaciones de alpaca tienen una buena resistencia a la corrosión y buenas cualidades mecánicas. Su aplicación se abarca materiales de telecomunicaciones, instrumentos y accesorios de fontanería y electricidad, como grifos, abrazaderas, muelles, conectores. También se emplea en la construcción y ferretería, para elementos decorativos y en las industrias químicas y alimentarias, además de materiales de vajillas y orfebrería.

El monel es una aleación que se obtiene directamente de minerales canadienses y tiene una composición de Cu=28-30 %, Ni=66-67 %, Fe=3-3,5 %. Este material tiene gran resistencia a los agentes corrosivos y a las altas temperaturas.

Otro tipo de alpaca es el llamado platinoide, aleación de color blanco compuesta de 60 % de cobre,14 % de níquel, 24 % de cinc y de 1-2 % de wolframio.

Otras aleaciones de cobre con aplicaciones técnicas son las siguientes:

Algunas aleaciones de cobre tienen pequeños porcentajes de azufre y de plomo que mejoran la maquinabilidad de la aleación. Tanto el plomo como el azufre tienen muy baja solubilidad en el cobre, separándose respectivamente como plomo (Pb) y como sulfuro cuproso (CuS) en los bordes de grano y facilitando la rotura de las virutas en los procesos de mecanizado, mejorando la maquinabilidad de la aleación.

El cobre nativo suele acompañar a sus minerales en bolsas que afloran a la superficie explotándose en minas a cielo abierto. El cobre se obtiene a partir de minerales sulfurados (80 %) y de minerales oxidados (20 %), los primeros se tratan por un proceso denominado pirometalurgia y los segundos por otro proceso denominado hidrometalurgia. Generalmente en la capa superior se encuentran los minerales oxidados (cuprita, melaconita), junto a cobre nativo en pequeñas cantidades, lo que explica su elaboración milenaria ya que el metal podía extraerse fácilmente en hornos de fosa. A continuación, por debajo del nivel freático, se encuentran las piritas (sulfuros) primarias calcosina (CuS) y covellina (CuS) y finalmente las secundarias calcopirita (FeCuS) cuya explotación es más rentable que la de las anteriores. Acompañando a estos minerales se encuentran otros como la bornita (CuFeS), los cobres grises y los carbonatos azurita y malaquita que suelen formar masas importantes en las minas de cobre por ser la forma en la que usualmente se alteran los sulfuros.

La tecnología de obtención del cobre está muy bien desarrollada aunque es laboriosa debido a la pobreza de la ley de los minerales. Los yacimientos de cobre contienen generalmente concentraciones muy bajas del metal. Ésta es la causa de que muchas de las distintas fases de producción tengan por objeto la eliminación de impurezas.

La metalurgia del cobre depende de que el mineral se presente en forma de sulfuros o de óxidos (cuproso u cúprico).

Para los sulfuros se utiliza para producir cátodos la vía llamada pirometalurgia, que consiste en el siguiente proceso: Conminución del mineral -> Concentración (flotación) -> fundición en horno -> paso a convertidores -> afino -> moldeo de ánodos -> electrorefinación -> cátodo. El proceso de refinado produce unos cátodos con un contenido del 99,9 % de cobre. Los cátodos son unas planchas de un metro cuadrado y un peso de 55 kg.

Otros componentes que se obtienen de este proceso son hierro (Fe) y azufre (S), además de muy pequeñas cantidades de plata (Ag) y oro (Au). Como impurezas del proceso se extraen también plomo (Pb), arsénico (As) y mercurio (Hg).

Como regla general una instalación metalúrgica de cobre que produzca 300.000 t/año de ánodos, consume 1.000.000 t/año de concentrado de cobre y como subproductos produce 900.000 t/año de ácido sulfúrico y 300.000 t/año de escorias.

Cuando se trata de aprovechar los residuos minerales, la pequeña concentración de cobre que hay en ellos se encuentra en forma de óxidos y sulfuros, y para recuperar ese cobre se emplea la tecnología llamada hidrometalurgia, más conocida por su nomenclatura anglosajona Sx-Ew.

El proceso que sigue esta técnica es el siguiente: Mineral de cobre-> lixiviación-> extracción-> electrólisis-> cátodo

Esta tecnología se utiliza muy poco porque la casi totalidad de concentrados de cobre se encuentra formando sulfuros, siendo la producción mundial estimada de recuperación de residuos en torno al 15 % de la totalidad de cobre producido.

El cobre y sus aleaciones permiten determinados tratamientos térmicos para fines muy determinados siendo los más usuales los de recocido, refinado y temple.

El cobre duro recocido se presenta muy bien para operaciones en frío como son: doblado, estampado y embutido. El recocido se produce calentando el cobre o el latón a una temperatura adecuada en un horno eléctrico de atmósfera controlada, y luego se deja enfriar al aire. Hay que procurar no superar la temperatura de recocido porque entonces se quema el cobre y se torna quebradizo y queda inutilizable.

El refinado es un proceso controlado de oxidación seguida de una reducción. El objetivo de la oxidación es eliminar las impurezas contenidas en el cobre, volatilizándolas o reduciéndolas a escorias. A continuación la reducción es mejorar la ductilidad y la maleabilidad del material.

Los tratamientos térmicos que se realizan a los latones son principalmente recocidos de homogeneización, recristalización y estabilización. Los latones con más del 35 % de Zn pueden templarse para hacerlos más blandos.

Los bronces habitualmente se someten a tratamientos de recocidos de homogeneización para las aleaciones de moldeo; y recocidos contra dureza y de recristalización para las aleaciones de forja. El temple de los bronces de dos elementos constituyentes es análogo al templado del acero: se calienta a unos 600 °C y se enfría rápidamente. Con esto se consigue disminuir la dureza del material, al contrario de lo que sucede al templar acero y algunos bronces con más de dos componentes.

Ya sea considerando la cantidad o el valor del metal empleado, el uso industrial del cobre es muy elevado. Es un material importante en multitud de actividades económicas y ha sido considerado un recurso estratégico en situaciones de conflicto.

El cobre se utiliza tanto con un gran nivel de pureza, cercano al 100 %, como aleado con otros elementos. El cobre puro se emplea principalmente en la fabricación de cables eléctricos.

El cobre es el metal no precioso con mejor conductividad eléctrica. Esto, unido a su ductilidad y resistencia mecánica, lo han convertido en el material más empleado para fabricar cables eléctricos, tanto de uso industrial como residencial. Asimismo se emplean conductores de cobre en numerosos equipos eléctricos como generadores, motores y transformadores. La principal alternativa al cobre en estas aplicaciones es el aluminio.

También son de cobre la mayoría de los cables telefónicos, los cuales además posibilitan el acceso a Internet. Las principales alternativas al cobre para telecomunicaciones son la fibra óptica y los sistemas inalámbricos. Por otro lado, todos los equipos informáticos y de telecomunicaciones contienen cobre en mayor o menor medida, por ejemplo en sus circuitos integrados, transformadores y cableado interno.

El cobre se emplea en varios componentes de coches y camiones, principalmente los radiadores (gracias a su alta conductividad térmica y resistencia a la corrosión), frenos y cojinetes, además naturalmente de los cables y motores eléctricos. Un coche pequeño contiene en total en torno a 20 kg de cobre, subiendo esta cifra a 45 kg para los de mayor tamaño.

También los trenes requieren grandes cantidades de cobre en su construcción: 1 - 2 toneladas en los trenes tradicionales y hasta 4 toneladas en los de alta velocidad. Además las catenarias contienen unas 10 toneladas de cobre por kilómetro en las líneas de alta velocidad.

Por último, los cascos de los barcos incluyen a menudo aleaciones de cobre y níquel para reducir el ensuciamiento producido por los seres marinos.

Una gran parte de las redes de transporte de agua están hechas de cobre o latón, debido a su resistencia a la corrosión y sus propiedades anti-bacterianas, habiendo quedado las tuberías de plomo en desuso por sus efectos nocivos para la salud humana. Frente a las tuberías de plástico, las de cobre tienen la ventaja de que no arden en caso de incendio y por tanto no liberan humos y gases potencialmente tóxicos.

El cobre y, sobre todo, el bronce se utilizan también como elementos arquitectónicos y revestimientos en tejados, fachadas, puertas y ventanas. El cobre se emplea también a menudo para los pomos de las puertas de locales públicos, ya que sus propiedades anti-bacterianas evitan la propagación de epidemias.

Dos aplicaciones clásicas del bronce en la construcción y ornamentación son la realización de estatuas y de campanas.

El sector de la construcción consume actualmente (2008) el 26 % de la producción mundial de cobre.

Desde el inicio de la acuñación de monedas en la Edad Antigua el cobre se emplea como materia prima de las mismas, a veces puro y, más a menudo, en aleaciones como el bronce y el cuproníquel.

Ejemplos de monedas que incluyen cobre puro:

Ejemplos de monedas de cuproníquel:

Ejemplos de monedas de otras aleaciones de cobre:

El cobre participa en la materia prima de una gran cantidad de diferentes y variados componentes de todo tipo de maquinaria, tales como casquillos, cojinetes, embellecedores, etc. Forma parte de los elementos de bisutería, bombillas y tubos fluorescentes, calderería, electroimanes, monedas, instrumentos musicales de viento, microondas, sistemas de calefacción y aire acondicionado. El cobre, el bronce y el latón son aptos para tratamientos de galvanizado para cubrir otros metales.

El sulfato de cobre (II) también conocido como sulfato cúprico es el compuesto de cobre de mayor importancia industrial y se emplea como abono y pesticida en agricultura, alguicida en la depuración del agua y como conservante de la madera.

El sulfato de cobre está especialmente indicado para suplir funciones principales del cobre en la planta, en el campo de las enzimas: oxidasas del ácido ascórbico, polifenol, citocromo, etc. También forma parte de la plastocianina contenida en los cloroplastos y que participa en la cadena de transferencia de electrones de la fotosíntesis. Su absorción se realiza mediante un proceso activo metabólicamente. Prácticamente no es afectado por la competencia de otros cationes pero, por el contrario, afecta a los demás cationes. Este producto puede ser aplicado a todo tipo de cultivo y en cualquier zona climática en invernaderos.

Para la decoración de azulejos y cerámica, se realizan vidriados que proporcionan un brillo metálico de diferentes colores. Para decorar la pieza una vez cocida y vidriada, se aplican mezclas de óxidos de cobre y otros materiales y después se vuelve a cocer la pieza a menor temperatura. Al mezclar otros materiales con los óxidos de cobre pueden obtenerse diferentes tonalidades. Para las decoraciones de cerámica, también se emplean películas metálicas de plata y cobre en mezclas coloidales (cobre coloidal) de barnices cerámicos que proporcionan tonos parecidos a las irisaciones metálicas del oro o del cobre.

Un pigmento muy utilizado en pintura para los tonos verdes es el cardenillo, también conocido en este ámbito como "verdigris", que consiste en una mezcla formada principalmente por acetatos de cobre, que proporciona tonos verdosos o azulados.

El cobre blíster, también llamado ampollado o anódico, tiene una pureza de entre 98 y 99,5 %, y su principal aplicación es la fabricación por vía electrolítica de cátodos de cobre, cuya pureza alcanza el 99,99 %. También se puede emplear para sintetizar sulfato de cobre y otros productos químicos. Su principal aplicación es su transformación en ánodos de cobre.

El paso intermedio en la transformación de cobre blíster en cátodos de cobre es la producción de ánodos de cobre, con cerca de 99,6 % de pureza. Un ánodo de cobre tiene unas dimensiones aproximadas de 100x125 cm, un grosor de 5 cm y un peso aproximado de 350 kg.

El cátodo de cobre constituye la materia prima idónea para la producción de alambrón de cobre de altas especificaciones. Es un producto, con un contenido superior al 99,99 % de cobre, es resultante del refino electrolítico de los ánodos de cobre. Su calidad está dentro de la denominación Cu-CATH-01 bajo la norma EN 1978:1998. Se presenta en paquetes corrugados y flejes, cuya plancha tiene unas dimensiones de 980x930 mm y un grosor de 7 mm con un peso aproximado de 47 kg. Su uso fundamental es la producción de alambrón de cobre de alta calidad, aunque también se utiliza para la elaboración de otros semitransformados de alta exigencia.

Después del proceso de elaborar ánodo de cobre y cátodo de cobre se obtienen los siguientes subproductos: Ácido sulfúrico. Escoria granulada. Lodos electrolíticos. Sulfato de níquel. Yeso

El alambrón de cobre es un producto resultante de la transformación de cátodo en la colada continua. Su proceso de producción se realiza según las normas ASTM B49-92 y EN 1977.

Las características esenciales del alambrón producido por la empresa Atlantic-copper son:

El alambrón se comercializa en bobinas flejadas sobre palet de madera y protegidas con funda de plástico. Cuyas dimensiones son: Peso bobina 5000 kg, diámetro exterior 1785 mm, diámetro interior 1150 mm y altura 900 mm. Las aplicaciones del alambrón son para la fabricación de cables eléctricos que requieran una alta calidad, ya sean esmaltados o multifilares de diámetros de 0,15/0,20 mm.

El alambre de cobre desnudo se produce a partir del alambrón y mediante un proceso de desbaste y con un horno de recocido. Se obtiene alambre desnudo formado por un hilo de cobre electrolítico en tres temples, duro, semiduro y suave y se utiliza para usos eléctricos se produce en una gama de diámetros de 1 mm a 8 mm y en bobinas que pueden pesar del orden de 2250 kg. Este alambre se utiliza en líneas aéreas de distribución eléctrica, en neutros de subestaciones, conexiones a tierra de equipos y sistemas y para fabricar hilos planos, esmaltados y multifilares que pueden tener un diámetros de 0,25/0,22 mm. Está fabricado a base de cobre de alta pureza con un contenido mínimo de 99,9 % de Cu. Este tipo de alambre tiene una alta conductividad, ductilidad y resistencia mecánica así como gran resistencia a la corrosión en ambientes salobres.

Se denomina trefilado al proceso de adelgazamiento del cobre a través del estiramiento mecánico que se ejerce al mismo al partir de alambrón de 6 u 8 mm de diámetro con el objetivo de producir cables eléctricos flexibles con la sección requerida. Un cable eléctrico se compone de varios hilos que mediante un proceso de extrusión se le aplica el aislamiento exterior con un compuesto plástico de PVC o polietileno. Generalmente el calibre de entrada es de 6 a 8 mm, para luego adelgazarlo al diámetro requerido. Como el trefilado es un proceso continuo se van formando diferentes bobinas o rollos que van siendo cortados a las longitudes requeridas o establecidos por las normas y son debidamente etiquetados con los correspondientes datos técnicos del cable.

Se llama apantallado al cubrimiento de un conductor central debidamente aislado por varios hilos conductores de cobre, que entrelazados alrededor forman una pantalla. Cuando es necesario aislar un hilo conductor mediante esmaltado se le aplica una capa de barniz (poliesterimida). Estas mezclas de resinas son usadas para recubrir el conductor metálico quedando aislados del medio ambiente que lo rodea y logrando de esta forma conducir el flujo eléctrico sin problemas.

Un tubo es un producto hueco, de sección normalmente redonda, que tiene una periferia continua y que es utilizado en gasfitería, fontanería y sistemas mecánicos para el transporte de líquidos o gases. Los tubos de cobre se utilizan masivamente en edificios residenciales, comerciales e industriales.

Para la fabricación de tubo se parte, por lo general, de una mezcla de cobre refinado y de chatarra de calidad controlada, que se funde en un horno. De la colada de cobre se obtienen lingotes conocidos como «billets», que tienen forma cilíndrica, unos 300 mm de diámetro y 8 m de largo y un peso de unas 5 toneladas. Los tubos sin costura se fabrican a partir de estos lingotes mediante las operaciones siguientes:


Una de las propiedades fundamentales del cobre es su maleabilidad que permite producir todo tipo de láminas desde grosores muy pequeños, tanto en forma de rollo continuo como en planchas de diversas dimensiones, mediante las instalaciones de laminación adecuadas.

El cobre puro no es muy adecuado para fundición por moldeo porque produce "galleo". Este fenómeno consiste en que el oxígeno del aire se absorbe sobre el cobre a altas temperaturas y forma burbujas; cuando después se enfría el metal, se libera el oxígeno de las burbujas y quedan huecos microscópicos sobre la superficie de las piezas fundidas.

Sus aleaciones sí permiten fabricar piezas por cualquiera de los procesos de fundición de piezas que existen dependiendo del tipo de pieza y de la cantidad que se tenga que producir. Los métodos más usuales de fundición son por moldeo y por centrifugado.

Se denomina fundición por moldeo al proceso de fabricación de piezas, comúnmente metálicas pero también de plástico, consistente en fundir un material e introducirlo en una cavidad, llamada molde, donde se solidifica. El proceso tradicional es la fundición en arena, por ser ésta un material refractario muy abundante en la naturaleza y que, mezclada con arcilla, adquiere cohesión y moldeabilidad sin perder la permeabilidad que posibilita evacuar los gases del molde al tiempo que se vierte el metal fundido

El proceso de fundición centrifugada consiste en depositar una capa de fundición líquida en un molde de revolución girando a gran velocidad y solidificar rápidamente el metal mediante un enfriamiento continuo del molde o coquilla. Las aplicaciones de este tipo de fundición son muy variadas.

El forjado en caliente de una pieza consiste en insertar en un molde una barra de metal, calentarla a la temperatura adecuada y obligarla a deformarse plásticamente hasta adoptar la forma del molde. La ventaja de forjar en caliente es que se reduce la potencia mecánica que debe suministrar la prensa para la deformación plástica.

Los productos del cobre y sus aleaciones reúnen muy buenas condiciones para producir piezas por procesos de estampación en caliente, permitiendo el diseño de piezas sumamente complejas gracias a la gran ductilidad del material y la escasa resistencia a la deformación que opone, proporcionando así una vida larga a las matrices. Una aleación de cobre es “forjable” en caliente si existe un rango de temperaturas suficientemente amplio en el que la ductilidad y la resistencia a la deformación sean aceptables. Este rango de temperaturas depende de composición química que tenga, en la que influyen los elementos añadidos y de las impurezas.

Las piezas de cobre o de sus aleaciones que van a someterse a trabajos de mecanizado por arranque de viruta tienen en su composición química una pequeña aportación de plomo y azufre que provoca una fractura mejor de la viruta cortada.

Actualmente (2008) el mecanizado de componentes de cobre, se realiza bajo el concepto de mecanizado rápido en seco con la herramienta refrigerada por aire si es necesario. Este tipo de mecanizado rápido se caracteriza porque los cabezales de las máquinas giran a velocidades muy altas consiguiendo grandes velocidades de corte en herramientas de poco diámetro.

Asimismo, las herramientas que se utilizan suelen ser integrales de metal duro, con recubrimientos especiales que posibilitan trabajar con avances de corte muy elevados. Los recubrimientos y materiales de estas herramientas son muy resistentes al desgaste, pueden trabajar a temperaturas elevadas, de ahí que no sea necesario muchas veces su refrigeración, tienen un coeficiente de fricción muy bajo y consiguen acabados superficiales muy finos y precisos.

Para soldar uniones de cobre o de sus aleaciones se utilizan dos tipos de soldadura diferentes: soldadura blanda y soldadura fuerte.

La soldadura blanda es aquella que se realiza a una temperatura de unos 200 °C y se utiliza para la unión de los componentes de circuitos impresos y electrónicos. Se utilizan soldadores de estaño y el material de aporte es una aleación de estaño y plomo en forma de alambre en rollo y que tiene resina desoxidante en su alma. Es una soldadura poco resistente y sirve para asegurar la continuidad de la corriente eléctrica a través del circuito.

Las soldaduras de tuberías de agua y gas realizadas por los fontaneros son de diversos tipos en función de los materiales que se quieran unir y de la estanqueidad que se quiera conseguir de la soldadura. Actualmente, la mayoría de los tubos de instalaciones de fontanería son de cobre, aunque en ocasiones se usan también otros materiales.

La soldadura de tuberías de cobre se realiza con sopletes de gas que proporcionan la llama para fundir el material soldante. El combustible del soplete puede ser butano o propano.

El cobre se utiliza también como aglutinante en la soldadura fuerte de fontanería, utilizada para conducciones de gas y canalizaciones complejas de agua caliente. Un metal alternativo para esta aplicación es la plata.

Se llama calderería a una especialidad profesional de la rama de fabricación metálica que tiene como función principal la construcción de depósitos aptos para el almacenaje y transporte de sólidos en forma de granos o áridos, líquidos y gas así como todo tipo de construcción naval y estructuras metálicas. Gracias a la excelente conductividad térmica que tiene la chapa de cobre se utiliza para fabricar alambiques, calderas, serpentines, cubiertas, etc.

Se denomina embutición al proceso de conformado en frío por el que se transforma un disco o piezas recortada, según el material, en piezas huecas, e incluso partiendo de piezas previamente embutidas, estirarlas a una sección menor con mayor altura.

El objetivo es conseguir una pieza hueca de acuerdo con la forma definida por la matriz de embutición que se utilice, mediante la presión ejercida por la prensa. La matriz de embutición también es conocida como molde.

Se trata de un proceso de conformado de chapa por deformación plástica en el curso del cual la chapa sufre simultáneamente transformaciones por estirado y por recalcado produciéndose variaciones en su espesor. Para la embutición se emplean, casi exclusivamente, prensas hidráulicas.

La chapa de cobre y sus aleaciones tienen unas propiedades muy buenas para ser conformados en frío. La embutición es un buen proceso para la fabricación en chapa fina de piezas con superficies complejas y altas exigencias dimensionales, sustituyendo con éxito a piezas tradicionalmente fabricadas por fundición y mecanizado.

Se conoce con el nombre de estampación a la operación mecánica que se realiza para grabar un dibujo o una leyenda en la superficie plana de una pieza que generalmente es de chapa metálica. Las chapas de cobre y sus aleaciones reúnen condiciones muy buenas para realizar en ellas todo tipo de grabados.

Los elementos claves de la estampación lo constituyen una prensa que puede ser mecánica, neumática o hidráulica; de tamaño, forma y potencia muy variada, y una matriz llamada estampa o troquel, donde está grabado el dibujo que se desea acuñar en la chapa, y que al dar un golpe seco sobre la misma queda grabado.

El estampado de los metales se realiza por presión o impacto, donde la chapa se adapta a la forma del molde. La estampación es una de las tareas de mecanizado más fáciles que existen, y permite un gran nivel de automatismo del proceso cuando se trata de realizar grandes cantidades de piezas.

La estampación se puede realizar en frío o en caliente, la estampación de piezas en caliente se llama forja, y tiene un funcionamiento diferente a la estampación en frío que se realiza en chapas generalmente. Las chapas de acero, aluminio, plata, latón y oro son las más adecuadas para la estampación. Una de las tareas de estampación más conocidas es la que realiza el estampado de las caras de las monedas en el proceso de acuñación de las mismas.

Se denomina troquelado a la operación mecánica que se realiza para producir piezas de chapa metálica o donde sea necesario realizar diversos agujeros en las mismas. Para realizar esta tarea, se utilizan desde simples mecanismos de accionamiento manual hasta sofisticadas prensas mecánicas de gran potencia.

Los elementos básicos de una prensa troqueladora lo constituyen el troquel que tiene la forma y dimensiones exteriores de la pieza o de los agujeros que se quieran realizar, y la matriz de corte por donde se inserta el troquel cuando es impulsado de forma enérgica por la potencia que le proporciona la prensa mediante un accionamiento de excéntrica que tiene y que proporciona un golpe seco y contundente sobre la chapa, produciendo un corte limpio de la misma.

Según el trabajo que se tenga que realizar, así son diseñadas y construidas las prensas. Hay matrices simples y progresivas donde la chapa, que está en forma de grandes rollos, avanza automáticamente provocando el trabajo de forma continuado, y no requiriendo otros cuidados que cambiar de rollo de chapa cuando se termina e ir retirando las piezas troqueladas así como vigilar la calidad del corte que realizan.

Cuando el corte se deteriora por desgaste del troquel y de la matriz, se desmontan de la máquina y se les rectifica en una rectificadora plana, estableciendo un nuevo corte. Una matriz y un troquel permiten muchos reafilados hasta que se desgastan totalmente.

Hay troqueladoras que funcionan con un cabezal donde puede llevar insertado varios troqueles de diferentes medidas y una mesa amplia donde se coloca la chapa que se quiere mecanizar. Esta mesa es activada mediante CNC y se desplaza a lo largo y ancho de la misma a gran velocidad, produciendo las piezas con rapidez y exactitud.

Los mecanismos subyacentes a los efectos de intoxicación por Cu en humanos no son muy comprendidos. El Cu es un metal de transición que, al igual que el resto de este tipo de metales (excepto el Zn), tiene electrones desapareados en sus orbitales externos. Por este motivo es que estos metales pueden ser considerados radicales libres.

El cobre puede ser encontrado en muchas clases de comidas, en el agua potable y en el aire. Debido a que absorbemos una cantidad eminente de cobre cada día por la comida, bebiendo y respirando. La absorción del cobre es necesaria, porque el cobre es un elemento traza que es esencial para la salud de los humanos. Aunque los humanos pueden manejar concentraciones de cobre proporcionalmente altas, mucho cobre puede también causar problemas de salud.

Las concentraciones del cobre en el aire son usualmente bastante bajas, así que la exposición al cobre por respiración es insignificante. Pero gente que vive cerca de fundiciones que procesan el mineral cobre en metal pueden experimentar esta clase de exposición.

La gente que vive en casas que todavía tiene tuberías de cobre están expuestas a más altos niveles de cobre que la mayoría de la gente, porque el cobre es liberado en sus aguas a través de la corrosión de las tuberías.

La producción mundial de cobre está todavía creciendo. Esto básicamente significa que más y más cobre termina en el medio ambiente. Los ríos están depositando barro en sus orillas que están contaminados con cobre, debido al vertido de aguas residuales contaminadas con cobre. El cobre entra en el aire, mayoritariamente a través de la liberación durante la combustión de fuel. El cobre en el aire permanecerá por un periodo de tiempo eminente, antes de depositarse cuando empieza a llover. Este terminará mayormente en los suelos, como resultado los suelos pueden también contener grandes cantidades de cobre después de que esté sea depositado desde el aire.

El cobre puede ser liberado en el medio ambiente tanto por actividades humanas como por procesos naturales. Ejemplo de fuentes naturales son las tormentas de polvo, descomposición de la vegetación, incendios forestales y aerosoles marinos. El cobre es a menudo encontrado cerca de minas, asentamientos industriales, vertederos y lugares de residuos.

Cuando el cobre termina en el suelo este es fuertemente atado a la materia orgánica y minerales.

El cobre no se rompe en el ambiente y por eso se puede acumular en plantas y animales cuando este es encontrado en suelos. En suelos ricos en cobre sólo un número pequeño de plantas pueden vivir. El cobre puede seriamente influir en el proceso de ciertas tierras agrícolas, dependiendo de la acidez del suelo y la presencia de materia orgánica. A pesar de esto el estiércol que contiene cobre es todavía usado.

El cobre puede interrumpir la actividad en el suelo, su influencia negativa en la actividad de microorganismos y lombrices de tierra. La descomposición de la materia orgánica puede disminuir debido a esto.

Cuando los suelos de las granjas están contaminados con cobre, los animales pueden absorber concentraciones de cobre que dañan su salud. Principalmente las ovejas sufren un gran efecto por envenenamiento con cobre, debido a que los efectos del cobre se manifiestan a bajas concentraciones.

A pesar de que en los trabajos químicos de referencia se indica que las sales de cobre son tóxicas, en la práctica esto sólo es cierto cuando las disoluciones se utilizan de forma incontrolada, con fines suicidas o como tratamiento tópico de áreas con quemaduras graves. Cuando se ingiere sulfato de cobre, también conocido como piedra azul o azul vitriolo, en cantidades del orden de gramos, se producen náuseas, vómitos, diarrea, sudoración, hemólisis intravascular y posible fallo renal; en raras ocasiones, se observan también convulsiones, coma y la muerte. Cuando se beben aguas carbonatadas o zumos de cítricos que han estado en contacto con recipientes, cañerías, grifos o válvulas de cobre se puede producir irritación del tracto gastrointestinal, que pocas veces llega a ser grave. Este tipo de bebidas son suficientemente ácidas para disolver niveles de cobre irritantes. Existe un informe de úlceras corneales e irritación cutánea, con baja toxicidad de otro tipo, en un minero de cobre que cayó en un baño electrolítico, aunque la causa pudo haber sido la acidez más que el cobre. En algunos casos en que se utilizaron sales de cobre para el tratamiento de quemaduras, se observaron concentraciones elevadas de cobre sérico y manifestaciones tóxicas. La inhalación de polvos, humos o nieblas de sales de cobre puede causar congestión nasal y de las mucosas, y ulceración con perforación del tabique nasal. Los humos desprendidos durante el calentamiento del cobre metálico pueden producir fiebre, náuseas, gastralgias y diarrea.

Efectos tóxicos crónicos atribuibles al cobre sólo parecen existir en personas que han heredado una pareja específica de genes recesivos autosómicos y que, como consecuencia, desarrollan una degeneración hepatolenticular (enfermedad de Wilson). Es una enfermedad rara. La mayor parte de la alimentación diaria que consume el hombre contiene de 2 a 5 mg de cobre, que prácticamente no se retiene en el organismo. 

El contenido corporal de cobre en una persona adulta es de 100 a 150 mg y es casi constante. En individuos normales (sin enfermedad de Wilson), casi todo el cobre está presente como parte integrante y funcional de una docena de proteínas y sistemas enzimáticos, como la citocromo oxidasa, la dopa-oxidasa y la ceruloplasmina sérica. 

En personas que ingieren grandes cantidades de ostras o mariscos de concha, hígado, setas, nueces y chocolate, alimentos todos ellos ricos en cobre, o en mineros que trabajan y comen durante 20 años o más en un ambiente cargado con un 1 o 2 % de polvo de minerales de cobre, pueden llegar a observarse concentraciones hasta 10 veces superiores a lo normal. 

Sin embargo, aún no se ha descrito ningún caso de toxicidad crónica primaria por cobre (perfectamente definida a partir de las observaciones de pacientes con toxicosis por cobre crónica heredada "la enfermedad de Wilson" como disfunción y lesiones estructurales hepáticas, del sistema nervioso central, de los riñones, los huesos y los ojos) excepto en personas que padecen la enfermedad de Wilson. Sin embargo, los depósitos excesivos de cobre hallados en el hígado de pacientes con cirrosis biliar primaria, colestasis y cirrosis infantil de la India pueden contribuir a la gravedad de la enfermedad hepática característica de estos procesos.

Los mecanismos subyacentes a los efectos de intoxicación por Cu en humanos no son muy comprendidos. El Cu es un metal de transición que, al igual que el resto de este tipo de metales (excepto el Zn), tiene electrones desapareados en sus orbitales externos. Por este motivo es que estos metales pueden ser considerados radicales libres. El Cu, al igual que el hierro puede participar en las reacciones tipo Fenton (1) y Häber-Weiss (2) produciendo ROS:
Las sales de Cu+ reaccionan con el HO con mayor eficiencia que el Fe. De modo que el principal mecanismo de toxicosis mediada por cobre puede descansar en su habilidad para provocar sobreproducción de ROS y subsecuente daño pro-oxidativo a lípidos, ácidos nucleicos y proteínas

El cobre tiene importantes efectos como agente citotóxico y genotóxico desarrollando un papel importante en la etiopatogénesis de las neoplasias . Este último mecanismo consiste en dañar la estructura molecular del ADN por vía indirecta (ROS) o directamente por formación de complejos con grupos funcionales de las bases nitrogenadas que las modifican introduciendo mutaciones, o dificultando el proceso de reparación.

Se cree que una de las vías por las que los iones Cu ejercen su efecto tóxico es produciendo un aumento del estrés oxidativo en múltiples tejidos del organismo.

El cobre es uno de los pocos materiales que no se degradan ni pierden sus propiedades químicas o físicas en el proceso de reciclaje. Puede ser reciclado un número ilimitado de veces sin perder sus propiedades, siendo imposible distinguir si un objeto de cobre está hecho de fuentes primarias o recicladas. Esto hace que el cobre haya sido, desde la Antigüedad, uno de los materiales más reciclados.

El reciclado proporciona una parte fundamental de las necesidades totales de cobre metálico. Se estima que en 2004 el 9 % de la demanda mundial se satisfizo mediante el reciclado de objetos viejos de cobre. Si también se considera "reciclaje" el refundido de los desechos del proceso de refinado del mineral, el porcentaje de cobre reciclado asciende al 34 % en el mundo y hasta un 41 % en la Unión Europea.

El reciclado del cobre no requiere tanta energía como su extracción minera. A pesar de que el reciclado requiere recoger, clasificar y fundir los objetos de metal, la cantidad de energía necesaria para reciclar el cobre es solo alrededor de un 25 % de la requerida para convertir el mineral de cobre en metal.

La eficacia del sistema de reciclado depende de factores tecnológicos como el diseño de los productos, económicos como el precio del cobre y sociales como el concienciamiento de la población acerca del desarrollo sostenible. Otro factor clave es la legislación. Actualmente existen más de 140 leyes, regulaciones, directivas y guías nacionales e internacionales que tratan de favorecer la gestión responsable del final del ciclo de vida de los productos que contienen cobre como por ejemplo electrodomésticos, teléfonos y vehículos.

En la Unión Europea, la directiva 2002/96/CE sobre residuos de aparatos eléctricos y electrónicos (RAEE, o "WEEE" del inglés "Waste Electrical and Electronic Equipment") propicia una política de minimización de desperdicios, que incluye una obligatoria y drástica reducción de los desechos industriales y domiciliarios, e incentivos para los productores que producen menos residuos. El objetivo de esta iniciativa era reciclar 4 kilos por habitante al año a fines de 2006.

Un ejemplo de reciclaje masivo de cobre lo constituyó la sustitución de las monedas nacionales de doce países europeos por el euro en 2002, el cambio monetario más grande de la historia. Se eliminaron de la circulación unas 260.000 toneladas de monedas, conteniendo aproximadamente 147.496 toneladas de cobre, que fueron fundidas y recicladas para su uso en una amplia gama de productos, desde nuevas monedas hasta diferentes productos industriales.

La producción mundial de cobre durante el 2014 alcanzó un total de 18.7 millones de toneladas métricas de cobre fino. El principal país productor es Chile, con casi un tercio del total, seguido por China y Perú:
De entre las diez mayores minas de cobre del mundo, tres se encuentran en Chile (Escondida, Codelco Norte, Collahuasi, El Teniente y Los Pelambres), dos en Indonesia, una en Estados Unidos, una en Rusia y otra en Perú (Antamina, Toquepala, Cuajone, Cerro verde ).

De acuerdo a información entregada en el informe anual del United States Geological Survey (USGS), las estimaciones señalan que las reservas conocidas de cobre en el 2011 a nivel mundial alcanzarían 690 millones de toneladas métricas de cobre fino. Y según las estimaciones de USGS, en Chile existirían del orden de 190 millones de toneladas económicamente explotables, equivalentes al 28 % del total de reservas mundiales del mineral; seguido de Perú con 90 millones de toneladas económicamente explotables, equivalentes al 13 % del total de reservas mundiales del mineral:

El cobre es el tercer metal más utilizado en el mundo, por detrás del hierro y el aluminio.
Existe un importante comercio mundial de cobre que mueve unos 30.000 millones de dólares anuales.

Los tres principales mercados de cobre son el LME de Londres, el COMEX de Nueva York y la Bolsa de Metales de Shanghái. Estos mercados fijan diariamente el precio del cobre y de los contratos de futuros sobre el metal. El precio de suele expresar en dólares / libra y en la última década ha oscilado entre los 0,65 $/lb de finales de 2001 y los más de 4,00 $/lb alcanzados en 2006 y en 2008. El fuerte encarecimiento del cobre desde 2004, debido principalmente al aumento de la demanda de China y otras economías emergentes, ha provocado una oleada de robos de objetos de cobre (sobre todo cables) en todo el mundo, con los consiguientes riesgos para la infraestructura eléctrica.

Los principales productores de mineral de cobre son también los principales exportadores, tanto de mineral como de cobre refinado y derivados. Los principales importadores son los países industrializados: Japón, China, India, Corea del Sur y Alemania para el mineral y Estados Unidos, Alemania, China, Italia y Taiwán para el refinado.





</doc>
<doc id="11487" url="https://es.wikipedia.org/wiki?curid=11487" title="Ductilidad">
Ductilidad

La ductilidad es una propiedad que presentan algunos materiales, como las aleaciones metálicas o materiales asfálticos, los cuales bajo la acción de una fuerza, pueden deformarse plásticamente de manera sostenible sin romperse, permitiendo obtener alambres o hilos de dicho material. A los materiales que presentan esta propiedad se les denomina "dúctiles". Los materiales no dúctiles se califican como frágiles. Aunque los materiales dúctiles también pueden llegar a romperse bajo el esfuerzo adecuado, esta rotura solo sucede tras producirse grandes deformaciones.

Algunos ejemplos de materiales muy dúctiles son el bronce y el latón.

En otros términos, un material es dúctil cuando la relación entre el alargamiento longitudinal producido por una tracción y la disminución de la sección transversal es muy elevada.

En el ámbito de la metalurgia se entiende por metal dúctil aquel que sufre grandes deformaciones antes de romperse, siendo el opuesto al metal frágil, que se rompe sin apenas deformación. Nótese que la ductilidad es un fenómeno observable solo en régimen plástico.

No debe confundirse dúctil con blando, ya que la ductilidad es una propiedad que como tal se manifiesta una vez que el material está soportando una fuerza considerable, suficiente para producir plastificación. Esto es, mientras la carga sea pequeña, la deformación también lo será y en general la deformación será elástica y reversible, sin embargo, alcanzado cierto punto el material cede fluye por plastificación, deformándose en mucha mayor medida de lo que lo había hecho hasta entonces pero sin llegar a romperse.

En un ensayo de tracción, los materiales dúctiles presentan una fase de fluencia caracterizada por una gran deformación sin apenas incremento de la carga. Desde un punto de vista tecnológico, al margen de consideraciones económicas, el empleo de materiales dúctiles presenta ventajas:

La ductilidad de un metal se valora de forma indirecta a través de la resiliencia. La ductilidad es la propiedad de los metales para formar alambres o hilos de diferentes grosores. Los metales se caracterizan por su elevada ductilidad, la que se explica porque los átomos de los metales se disponen de manera tal que es posible que se deslicen unos sobre otros y por eso se pueden estirar sin romperse.

Tras una prueba de tensión, o prueba de tracción, son dos las "medidas" que nos proporcionan información acerca de la "ductilidad" de un material: el porcentaje de elongación y la reducción porcentual en el área.

Donde "L" es la distancia entre las marcas calibradas tras la falla de la muestra.

donde Af es el área de la sección transversal final en la superficie de la fractura.




</doc>
<doc id="11494" url="https://es.wikipedia.org/wiki?curid=11494" title="Cayo">
Cayo

Un cayo, término de origen antillano, es una pequeña isla con una playa de baja profundidad, formada en la superficie de un arrecife de coral.

Los cayos por lo general se encuentran en ambientes tropicales de los océanos Pacífico, Atlántico e Índico (incluidos el mar Caribe, la Gran barrera de coral y el arrecife de barrera de Belice), donde pueden proporcionar tierra habitable y agrícola para cientos de miles de personas. Sus ecosistemas de arrecifes que lo rodean también proporcionan alimentos y materiales de construcción para los habitantes de la isla. Un inconveniente habitual en estas superficies terrestres es la falta de agua potable.

Al conjunto de cayos se le llama cayería. Algunos de ellos pueden ser de considerable extensión territorial, como es el caso del cayo Coco (aproximadamente 370 kilómetros cuadrados), al norte de la isla de Cuba, que constituye así la cuarta mayor isla del archipiélago cubano, después de la isla de Cuba y de la isla de Pinos o isla de la Juventud y del cayo Romano (vecino a cayo Coco).

Un cayo se forma cuando las corrientes oceánicas transportan sedimento suelto a través de la superficie de un arrecife hacia un nodo de depósitos, lugar donde la corriente disminuye o converge con otra corriente, liberando su carga de sedimentos. Poco a poco, las capas de la acumulación de sedimentos son depositadas en la superficie del arrecife.

Estos nodos se producen en áreas de superficies de arrecifes en barlovento o sotavento, aunque a veces surgen alrededor de un afloramiento de un antiguo arrecife emergente o en una playa rocosa.

La isla que resulta de la acumulación de sedimentos se compone casi enteramente de sedimento biogénico –restos de esqueletos de plantas y animales– de los ecosistemas de arrecifes circundantes. Si los sedimentos acumulados son predominantemente arena, la isla se llama "cayo", y si son predominantemente de grava, la isla se llama "islote".

Los sedimentos de un cayo están compuestos principalmente de carbonato de calcio (CaCO), aragonito, calcita y calcita magnésica. Estos son producidos por diversas plantas (por ejemplo, algas coralinas y especies del alga verde Halimeda) y animales (por ejemplo, el coral, moluscos o foraminíferos). Suelen encontrarse pequeñas cantidades de silicato de sedimento también aportado por esponjas de mar y otros animales similares. Con el tiempo, el suelo y la vegetación se pueden desarrollar en la superficie de un cayo, asistidos por el depósito del guano de las aves marinas.

Una gama de influencias físicas, biológicas y químicas determinan el desarrollo en curso o la erosión del entorno de los cayos. Estas influencias son: la extensión de acumulaciones de arrecifes de arena superficiales, los cambios en las olas del mar, las corrientes, las mareas, los niveles del mar y las condiciones meteorológicas, la forma del arrecife subyacente, los tipos y abundancia de la biota carbonato de producción y otros organismos, tales como aglutinantes, bioerosionadores y bioturbadores (criaturas que se unen, erosionan y mezclan sedimentos) que viven en los alrededores de los ecosistemas de arrecifes.

Los cambios significativos en los cayos y sus ecosistemas circundantes pueden ser resultado de fenómenos naturales como el Fenómeno del Niño o los severos ciclos del Fenómeno del Enos. Además, los ciclones tropicales pueden ayudar a construir o destruir estas islas.

Hay un gran debate y preocupación por la estabilidad futura de los cayos frente a las crecientes poblaciones humanas y las presiones sobre los ecosistemas de los arrecifes que las componen, así como las predicciones de cambios climáticos y el aumento del nivel del mar. A esto se suma que se desconoce con certeza cuántos años poseen los cayos en cuanto a su formación actual para tomar decisiones conservacionistas al respecto.

Para ello, es necesario entender el potencial de cambio en las fuentes de sedimentos para la creación de playas en los cayos, con vistas a determinar si el cambio del medio ambiente es un factor importante para predecir su estabilidad presente y futura. A pesar de ello, hay consenso en que estos ambientes insulares son muy complejos y algo frágiles a los elementos externos.

Los cayos suelen tener mayor variedad de insectos y reptiles que las islas normales.

La preservación del ambiente de los cayos se ve comprometida por el avance de los proyectos turísticos, que aprovechan la belleza de las playas, la riqueza de la vegetación y los excelentes sitios de buceo. Los cayos son reductos frágiles de flora y fauna, que deben ser respetados y cuidados. Así, en muchos proyectos se incentiva una interacción respetuosa con el ambiente, como son el buceo contemplativo y las excursiones ecológicas.









</doc>
<doc id="11496" url="https://es.wikipedia.org/wiki?curid=11496" title="Mesosfera">
Mesosfera

En meteorología se denomina mesosfera o mesósfera a la parte de la atmósfera terrestre situada por encima de la estratosfera y por debajo de la termosfera. Es la capa de la atmósfera en la que la temperatura va disminuyendo a medida que se aumenta la altura, hasta llegar a unos −80 °C a los 80kilómetros aproximadamente. Se extiende desde la estratopausa (zona de contacto entre la estratosfera y la mesosfera). La mesosfera es la tercera capa de la atmósfera de la Tierra. Es la zona más fría de la atmósfera.

Contiene sólo cerca del 0,1 % de la masa total del aire. Es importante por la ionización y las reacciones químicas que ocurren en ella. La baja densidad del aire en la mesosfera determinan la formación de turbulencias y ondas atmosféricas que actúan a escalas espaciales y temporales muy grandes. La mesosfera es la región donde las naves espaciales que vuelven a la Tierra empiezan a notar la estructura de los vientos de fondo, y no solo el freno aerodinámico. También en esta capa se observan las estrellas fugaces que son meteoroides que se han desintegrado en la termosfera.

En ella se desintegran los meteoritos que se dirigen a la Tierra provocando destellos de luz llamados estrellas fugaces.

Debido a que la mesosfera se encuentra por encima de la altitud máxima de globos y aviones, pero demasiado baja para los satélites artificiales, sólo puede estudiarse con cohetes sonda durante tiempo limitado. Por esta razón, es la zona peor entendida de la atmósfera y entre los científicos ha dado lugar al apodo humorístico "ignorosfera".


</doc>
<doc id="11497" url="https://es.wikipedia.org/wiki?curid=11497" title="Efecto Coriolis">
Efecto Coriolis

El efecto Coriolis, descrito en 1836 por el científico francés Gaspard-Gustave Coriolis, es el efecto que se observa en un sistema de referencia en rotación cuando un cuerpo se encuentra en movimiento respecto de dicho sistema de referencia. Este efecto consiste en la existencia de una aceleración "relativa" del cuerpo en dicho sistema en rotación. Esta aceleración es siempre perpendicular al eje de rotación del sistema y a la velocidad del cuerpo.

El efecto Coriolis hace que un objeto que se mueve sobre el radio de un disco en rotación tienda a acelerarse con respecto a ese disco según si el movimiento es hacia el eje de giro o alejándose de este. Por el mismo principio, en el caso de una esfera en rotación, el movimiento de un objeto sobre los meridianos también presenta este efecto, ya que dicho movimiento reduce o incrementa la distancia respecto al eje de giro de la esfera.

Debido a que el objeto sufre una aceleración desde el punto de vista del observador en rotación, es como si para este existiera una fuerza sobre el objeto que lo acelera. A esta fuerza se le llama "fuerza de Coriolis", y no es una fuerza real en el sentido de que no hay nada que la produzca. Se trata pues de una fuerza inercial o ficticia, que se introduce para explicar, desde el punto de vista del sistema en rotación, la aceleración del cuerpo, cuyo origen está en realidad, en el hecho de que el sistema de observación está rotando.

Un ejemplo canónico de efecto Coriolis es el experimento imaginario en el que disparamos un proyectil desde el Ecuador en dirección norte. El cañón está girando con la tierra hacia el este y, por tanto, imprime al proyectil esa velocidad (además de la velocidad hacia adelante al momento de la impulsión). Al viajar el proyectil hacia el norte, sobrevuela puntos de la tierra cuya velocidad lineal hacia el este va disminuyendo con la latitud creciente. La inercia del proyectil hacia el este hace que su velocidad angular aumente y que, por tanto, adelante a los puntos que sobrevuela. Si el vuelo es suficientemente largo (ver cálculos al final del artículo), el proyectil caerá en un meridiano situado al este de aquel desde el cual se disparó, a pesar de que la dirección del disparo fue exactamente hacia el norte. Finalmente, el efecto Coriolis, al actuar sobre masas de aire (o agua) en latitudes intermedias, induce un giro al desviar hacia el este o hacia el oeste las partes de esa masa que ganen o pierdan latitud o altitud en su movimiento.

La fuerza de Coriolis es una fuerza ficticia que aparece cuando un cuerpo está en movimiento con respecto a un sistema en rotación y se describe su movimiento en ese referencial. La fuerza de Coriolis es diferente de la fuerza centrífuga. La fuerza de Coriolis siempre es perpendicular a la dirección del eje de rotación del sistema y a la dirección del movimiento del cuerpo vista desde el sistema en rotación. La fuerza de Coriolis tiene dos componentes:
La componente del movimiento del cuerpo paralela al eje de rotación no engendra fuerza de Coriolis. El valor de la fuerza de Coriolis formula_1 es:

donde:

En 1835, Gaspard-Gustave de Coriolis, en su artículo "Sur les équations du mouvement relatif des systèmes de corps", describió matemáticamente la fuerza que terminó llevando su nombre. En ese artículo, la fuerza de Coriolis aparece como una componente suplementaria a la fuerza centrífuga experimentada por un cuerpo en movimiento relativo a un referencial en rotación, como puede producirse, por ejemplo, en los engranajes de una máquina.
El razonamiento de Coriolis se basaba sobre un análisis del trabajo y de la energía potencial y cinética en los sistemas en rotación. Ahora, la demostración más utilizada para enseñar la fuerza de Coriolis utiliza las herramientas de la cinemática.

Esta fuerza no comenzó a aparecer en la literatura meteorológica y oceanográfica hasta finales del siglo XIX. El término "fuerza de Coriolis" apareció a principios del siglo XX.

Para demostrar la expresión analítica expresada en la introducción, pueden usarse dos aproximaciones diferentes: por conservación del momento angular o por derivación en base móvil. A continuación se explican ambas.

Es preciso recordar que cuando un observador en un sistema no inercial (como lo es un sistema en rotación) trata de comprender el comportamiento de su sistema como si fuese un sistema inercial ve aparecer fuerzas ficticias. En el caso de un sistema en rotación, el observador ve que todos los objetos que no están sujetos se alejan de manera radial como si actuase sobre ellos una fuerza proporcional a sus masas y a la distancia a una cierta recta (el eje de rotación). Esa es la fuerza centrífuga que hay que compensar con la fuerza centrípeta para sujetar los objetos. Por supuesto, para un observador externo, situado en un sistema inercial (sistema fijo), la única fuerza que existe es la fuerza centrípeta, cuando los objetos están sujetos. Si no lo están, los objetos tomarán la tangente y se alejarán del eje de rotación.

Si los objetos no están inmóviles con respecto al observador del sistema en rotación, otra fuerza ficticia aparece: la fuerza de Coriolis. Visto desde el sistema en rotación, el movimiento de un objeto se puede descomponer en una componente paralela al eje de rotación, otra componente radial (situada sobre una línea que pasa por el eje de rotación y perpendicular a este), y una tercera componente tangencial (tangente a un círculo centrado en el eje y perpendicular a este) (ver gráfica).

Un objeto que se desplaza paralelamente al eje de rotación, visto de un sistema fijo, gira con el sistema en rotación a la misma velocidad angular y con radio constante. La única fuerza que actúa sobre el objeto es la fuerza centrípeta. El observador del sistema en rotación sólo nota la fuerza centrífuga contra la cual hay que oponerse para que se quede a la misma distancia del eje.

Supóngase que un observador en el sistema en rotación mantiene una masa formula_6 a una distancia formula_7 del eje de rotación mediante un hilo de masa despreciable. El observador tira del hilo y modifica ligeramente el radio de rotación de la masa de formula_8. Eso le ha tomado un tiempo formula_9. Como el momento dinámico es nulo, el momento angular de la masa se conserva. Si formula_10 es la velocidad de la masa, la conservación del momento angular expresa:

El signo menos indica que cuando el radio aumenta la velocidad tangencial disminuye.

Si la masa se moviese siguiendo una trayectoria radial, fija con respecto al sistema en rotación, conservando en consecuencia la misma velocidad angular formula_13 del sistema en rotación, su velocidad lineal habría aumentado de formula_14 (o disminuido, si formula_8 es negativo). Para un observador fijo, entre la velocidad de la masa que se ve obligada a seguir una trayectoria radial y la velocidad de la masa que conserva su momento angular hay una diferencia de:

Como el objeto no está sujeto al sistema en rotación, el observador en ese sistema ve la masa tomar una velocidad lateral formula_17. Eso se interpreta como la aplicación de una fuerza lateral (de Coriolis). Si el cambio de velocidad tomó formula_9 segundos, la aceleración de Coriolis será (en valor absoluto):

donde formula_20 es la velocidad radial. Esa aceleración corresponde a una fuerza (de Coriolis) de:

Considerando un objeto con velocidad tangencial formula_22 vista por el observador en el sistema en rotación. Esta vez, la misma masa tenida por un hilo tiene una velocidad angular diferente del sistema en rotación. Para el observador en el sistema en rotación, las fuerzas que nota aplicadas a la masa para que siga una trayectoria circular son: la fuerza centrífuga formula_23 que ve aplicada en todos los objetos, más la fuerza centrípeta debido a la rotación aparente de la masa formula_24. Pero eso no basta. Hay aún otra fuerza aparente, y es precisamente la fuerza de Coriolis. Se calcula ahora la fuerza centrípeta que ve un observador fijo: la velocidad tangencial es formula_25. Para este observador, la fuerza centrípeta que mantiene la masa a distancia constante será:

El primer término es la fuerza centrífuga común a todos los objetos que giran con el sistema en rotación. El tercero es la fuerza centrípeta debida a la rotación de la masa con respecto al sistema en rotación. Y el segundo término es la fuerza de Coriolis. Es un término suplementario debido al hecho de que la fuerza centrípeta depende del cuadrado de la velocidad tangencial y no puede obtenerse sumando las fuerzas centrífuga y centrípeta debido a velocidades parciales. La fuerza de Coriolis es:

Como se ha dicho , esa fuerza es radial.

Para esta demostración se utilizará el subíndice abs para indicar magnitudes vistas desde el sistema de referencia inercial, es decir, uno donde el espacio sea homogéneo e isótropo y donde el tiempo sea constante. El subíndice rel (relativa) se refiere a magnitudes vistas desde una referencia no galileana o no inercial. El subíndice ar (arrastre) hace referencia al movimiento de la base móvil respecto a la base fija. También es necesario conocer cómo se deriva en una base móvil:

Una aceleración es un cambio en la magnitud o en la orientación de la velocidad respecto del tiempo. Para esa demostración se considera un movimiento que no varía la magnitud de su velocidad, es decir, que no está sometido a fuerzas que tengan alguna componente en la dirección del movimiento. Entonces:

Por una parte:

Por otra:
donde:
Como no se considera el movimiento alrededor del Sol, sino sólo el giro de la tierra en torno a sí misma:

Además, como se está imaginando un movimiento sin aceleración relativa (como el de un proyectil):

quedando así:
Pero:
Entonces:
Volviendo al principio:
La aceleración de Coriolis es el primer sumando:
La aceleración centrípeta es el segundo:

El ejemplo más notorio de manifestación del efecto Coriolis se da cuando masas de aire o de agua se desplazan siguiendo meridianos terrestres, y su trayectoria y velocidad se ven modificadas por él. En efecto, los vientos o corrientes oceánicas que se desplazan siguiendo un meridiano se desvían acelerando en la dirección de giro (este) si van hacia los polos o al contrario (oeste) si van hacia el ecuador. Se puede añadir, que por consecuencia, en el Ecuador, no hay efecto de Coriolis. La manifestación de estas desviaciones produce, de manera análoga al giro de la bolita mostrado al principio, que las borrascas tiendan a girar en el hemisferio sur en el sentido de las agujas del reloj y, en el hemisferio norte, en sentido contrario.

El efecto de la fuerza de Coriolis deberá considerarse siempre que se estudie el movimiento de fluidos y también el de cualquier objeto móvil sobre esferas o superficies planas en rotación. Esto incluye a los planetas gaseosos del sistema solar, el Sol y todas las estrellas y, en el planeta Tierra, el movimiento de las aguas de los ríos, los lagos, los océanos y, por supuesto, de la atmósfera. El efecto de Coriolis predice que siempre que se observen los movimientos giratorios de esos cuerpos, los vórtices seguirán la norma descrita para las borrascas y anticiclones terrestres.

Además de su influencia sobre la atmósfera, es muy notoria la que tiene también sobre la circulación oceánica. En las cuencas que tienen la forma apropiada (como, por ejemplo, la cuenca del Atlántico norte y la del Atlántico sur), el efecto Coriolis desvía a las corrientes marinas hacia la derecha en el hemisferio norte y hacia la izquierda en el hemisferio sur, de la misma manera que sucede con la circulación general de los vientos.

Las excepciones o modificaciones de este patrón general de la circulación general de los océanos tienen que ver con la disposición de las costas y la compensación introducida por las corrientes cálidas que van, en los océanos, de las costas orientales de la zona intertropical hacia las occidentales de las zonas templadas de los continentes (corriente del Golfo y de Kuro Shivo, especialmente). Además, en los océanos, lo mismo que sucede en la atmósfera, se produce una especie de convergencia en las latitudes ecuatoriales por la fuerza centrífuga del movimiento de rotación: tanto el océano como la atmósfera tienen un abombamiento ecuatorial por la rotación terrestre, de varios kilómetros de altura en el caso de los océanos y aún mayor en el caso de la atmósfera debido a su menor densidad. A su vez, este "abombamiento" ocasiona una especie de obstáculo a la libre circulación y al libre intercambio de energía (oceánica y atmosférica) entre los dos hemisferios. La circulación en la zona ecuatorial es, por lo tanto, de este a oeste, tanto en lo que respecta a las corrientes ecuatoriales del norte y del sur como con respecto a los alisios del noreste en el hemisferio norte y del sureste en el hemisferio sur. Por último, lo que se ha denominado abombamiento ecuatorial de los océanos tiene varias consecuencias: entre ellas, la formación de lo que se ha denominado contracorrientes ecuatoriales también del norte y del sur, definidas e identificadas en muchos atlas y libros de geografía y de ciencias de la Tierra, y la desviación hacia las zonas subtropicales y templadas: de nuevo, hacia la derecha en el hemisferio norte y hacia la izquierda en el hemisferio sur.

Una de las raras ocasiones en la cual una persona puede sentir la fuerza de Coriolis es cuando trata de caminar siguiendo una trayectoria radial en un tiovivo (o carrusel). Cuando la persona se aleja del eje de rotación, sentirá una fuerza que la empuja en el sentido contrario a la rotación: es la fuerza de Coriolis. Cuando una persona se aleja o se acerca del eje de rotación a una velocidad de 1 m/s en un tiovivo que gira a 10 vueltas por minuto, la aceleración de Coriolis es:

Se trata, por consiguiente, de una aceleración lateral 4,6 veces más pequeña que la gravedad, pero que para una persona de 70 kg, eso corresponde a una fuerza lateral igual al peso de 15 kg. que es perfectamente percibida.

La Tierra gira mucho más lentamente que un carrusel. Su velocidad angular es de formula_36 radianes por día sideral (23 h, 56 m, 4,1 s) es decir formula_37. La aceleración de Coriolis debido a la rotación de la Tierra es mucho menor.

Cuando un cuerpo sigue una trayectoria norte-sur sobre la Tierra (siguiendo un meridiano), la componente radial de su velocidad (la velocidad a la cual el cuerpo se acerca o se aleja del eje de rotación terrestre) depende de la latitud del cuerpo. Es fácil ver que la componente radial es formula_38. Cuando el cuerpo está cerca del ecuador, su distancia respecto al eje de la Tierra no cambia. Si la trayectoria del cuerpo es este-oeste y sigue un paralelo, su distancia respecto al eje terrestre no varía, pero ya hemos visto que sentirá una aceleración de Coriolis dirigida hacia el eje de la Tierra que vale formula_39. La componente paralela a la superficie de la Tierra depende de la latitud y es: formula_40.

Vemos que en los dos casos, visto desde la Tierra, un cuerpo que se desplaza sobre la superficie de la Tierra siente una aceleración lateral de valor formula_41 dirigida hacia la derecha de la velocidad.

Un cuerpo que se desplaza con una velocidad de 1 m/s, sin interacción con el suelo, a una latitud de 45° encuentra una aceleración lateral de Coriolis igual a:

lo cual corresponde a una fuerza lateral aproximadamente 100 000 veces menor que su propio peso. Dicho de otra manera, la trayectoria se desvía hacia la derecha como si el terreno estuviese inclinado hacia la derecha 1 milímetro cada 100 metros.

Si se trata de un avión cuya velocidad es 900 km/h (250 m/s), la aceleración será 250 veces mayor. El efecto será darle al avión una trayectoria circular de 4850 km de diámetro (a una latitud de 45°):
Por supuesto, el piloto corregirá esta desviación, pero no parece posible que pueda distinguirla de los efectos del viento o de los errores de reglaje de la posición neutra de los alerones de dirección y de profundidad.

Tomemos el caso de un obús, situado a una latitud de 45° y que tira un proyectil a 110 km de distancia. El ángulo de tiro para esa distancia es de 45°. Si se desprecia el efecto de los rozamientos con el aire, la velocidad horizontal del proyectil es de 734 m/s, y el tiempo de vuelo es de 150 segundos. La aceleración de Coriolis será:

La distancia lateral de desvío provocada por la aceleración de Coriolis es:

Esa distancia corresponde a un error en el ángulo de tiro de 0,44°. Las opiniones divergen sobre la importancia de este error, comparado con la influencia de otras fuerzas y, sobre todo, con la fuerza provocada por el efecto Magnus sobre proyectiles que giran axialmente.

Para cañones de menor alcance, el error en el ángulo de tiro es aún menor. Por ejemplo, para un proyectil cuyo alcance es de 20 km y cuya velocidad media es la misma, el error del ángulo es 25 veces menor.

La versión simplificada del efecto Coriolis está ligada a su componente horizontal causada por movimientos horizontales con respecto a la superficie terrestre.

Pero también hay componentes verticales del efecto Coriolis que son significativos. Los objetos que viajen hacia el este a gran velocidad se desviarán hacia arriba (parecerán más ligeros), mientras que los que lo hagan hacia el oeste se desviarán hacia abajo (parecerán más pesados). Esto se conoce como el efecto Eötvös. Este componente vertical del efecto Coriolis es mayor en el ecuador, y se reduce a cero en los polos.

Otro caso a tener en cuenta es el de objetos que viajan en dirección perpendicular al plano terrestre. Aquellos que se desplacen arriba a gran velocidad se desviarán hacia el oeste y los que lo hagan hacia abajo se desviarán hacia el este. El efecto de nuevo alcanza su máximo en el ecuador y es 0 en los polos (en el ecuador un movimiento vertical es perpendicular al eje de rotación y en los polos sin embargo es paralelo y por lo tanto el efecto causado por Coriolis en ese caso es 0). 

Imaginemos un tren que viaja por una vía sin rozamiento alrededor del ecuador de la Tierra a la velocidad necesaria para completar una vuelta al mundo en un día (465 m/s). Analizamos el efecto Coriolis en tres casos:
Para cada uno de estos casos calculamos el efecto Coriolis, primero desde el punto de vista de nuestro sistema de referencia en rotación en la Tierra para a continuación comprobar que el resultado es el mismo observando el tren en un sistema de referencia inercial. En la siguiente imagen podemos observar los tres casos en el sistema de referencia inercial vistos desde un punto fijo sobre la tierra en su eje de rotación: 

Esto explica por qué los proyectiles a alta velocidad que se disparan hacia el este se desvían hacia arriba mientras que si son disparados hacia el oeste la desviación es hacia abajo. Esta componente vertical del efecto de Coriolis se denomina el Efecto Eötvös.

Podemos usar el ejemplo para explicar por qué el efecto Eötvös comienza a reducirse en objetos que viajan hacia el oeste una vez que su velocidad tangencial supera la velocidad de rotación de la tierra (465 m/s en el ecuador). Si el tren que viaja hacia el oeste en el ejemplo incrementa su velocidad en esa dirección y lo observamos desde el sistema de referencia inercial en el espacio veremos que empieza a rotar alrededor de la tierra que gira debajo en dirección contraria. Para mantener esa trayectoria circular, parte de la fuerza de la gravedad que empuja al tren contra las vías actuaría como fuerza centrípeta. Una vez que el tren doblara su velocidad a 930 m/s la fuerza centrípeta sería igual a la experimentada cuando el tren se encuentra parado. Desde el punto de vista del sistema de referencia inercial en ambos casos el tren está rotando a la misma velocidad (465 m/s) solo que en direcciones opuestas. Por lo tanto la fuerza es la misma y por tanto el efecto Eötvös se cancelaría completamente a esa velocidad. Cualquier objeto que se mueva hacia el oeste a una velocidad superior a 930 m/s no experimentara una desviación hacia abajo, sino hacia arriba. El gráfico de la derecha ilustra la fuerza causada por el efecto Eötvös que experimentaría un objeto de 10 gramos en el tren del ejemplo en función de su velocidad. La forma parabólica del gráfico se explica porque la fórmula de la fuerza centrípeta es proporcional al cuadrado de la velocidad tangencial. En el sistema de referencia inercial la parte de abajo de la parábola estaría centrada en el origen. El desplazamiento del origen se explica porque estamos usando el sistema de referencia en rotación de la tierra. Observando el gráfico podemos comprobar que el efecto Eötvös no es simétrico, y que la fuerza hacia abajo experimentada por un objeto viajando hacia el oeste a gran velocidad es menor que la fuerza hacia arriba experimentada por el mismo objeto viajando en dirección al este a la misma velocidad.

Una aplicación práctica de la fuerza de Coriolis es el caudalímetro másico, un instrumento que mide el caudal másico de un fluido que circula a través de una tubería. Este instrumento fue comercializado en 1977 por Micro Motion Inc.

Los caudalímetros normales miden el caudal volumétrico, el cual es proporcional al caudal másico sólo cuando la densidad del fluido es constante. Si el fluido tiene una variación de densidad o contiene burbujas, entonces el caudal volumétrico, multiplicado por la densidad, no será exactamente igual al caudal másico. El caudalímetro másico de Coriolis funciona aplicando una fuerza de vibración a un tubo curvado a través del cual pasa el fluido. El efecto Coriolis crea una fuerza en el tubo perpendicular a ambas direcciones: la de vibración y la dirección de la corriente. Esta fuerza se mide para obtener el caudal másico. Los caudalímetros de Coriolis pueden usarse además con fluidos no newtonianos, en los cuales los caudalímetros normales tienden a dar resultados erróneos. El mismo instrumento puede usarse para medir la densidad del fluido. Este instrumento tiene una novedad adicional, que consiste en que el fluido está en un tubo liso, sin partes móviles, que no necesita limpieza ni mantenimiento y presenta una caída de presión muy baja.





</doc>
<doc id="11499" url="https://es.wikipedia.org/wiki?curid=11499" title="Bioma">
Bioma

Un bioma (del griego «bios», vida), también llamado paisaje bioclimático o área biótica es una determinada parte del planeta que comparte el clima, flora y fauna. Un bioma es el conjunto de ecosistemas característicos de una zona biogeográfica que está definido a partir de su vegetación y de las especies animales que predominan. Es la expresión de las condiciones ecológicas del lugar en el plano regional o continental: el clima y el suelo determinan las condiciones ecológicas a las que responden las comunidades de plantas y animales del bioma en cuestión.

En función de la latitud, la temperatura, las precipitaciones y la altitud, en definitiva, y de las características básicas del clima, se puede dividir la tierra en zonas de características semejantes; en cada una de esas zonas se desarrolla una vegetación (fitocenosis) y una fauna (zoocenosis) que cuando están relacionadas, definen un bioma, que comprende las nociones de comunidad y la interacción entre suelo, plantas y animales. 

Hay diferentes sistemas para la clasificación de biomas, que en general suelen dividir la tierra en dos grandes grupos —biomas terrestres y biomas acuáticos-, con un número no demasiado grande de biomas. A escala planetaria, la selva, la sabana, la estepa, el bosque y la tundra son los grandes biomas que caracterizan la biósfera y que tienen un reparto zonal, es decir, que no superan ciertos valores latitudinales. A escala regional o continental, los biomas son difíciles de definir, en parte porque existen diferentes patrones y también porque sus fronteras suelen ser difusas (véase el concepto de ecotono).

Los biomas a menudo son conocidos por sus nombres locales. Por ejemplo, un bioma del herbazal se conoce como pradera en Norteamérica, sabana en África, estepa en Asia, pampa en Sudamérica y veld en Sudáfrica.

Los biomas terrestres son descritos por la ciencia de la biogeografía. Por extensión, se habla de microbioma para designar la esfera de la vida microbiana.

El concepto de bioma no debe confundirse con otros conceptos similares como el de ecozona , hábitat o ecosistema. Las distintas ecorregiones del mundo se agrupan tanto en biomas como en ecozonas.

Los biomas son áreas definidas climática y geográficamente, con similares condiciones ecológicas, tales como las comunidades de plantas y animales, (que a menudo se nombran como ecosistemas). Los biomas están definidos por factores tales como la estructura de las plantas (árboles, arbustos y hierbas), los tipos de hojas (hoja ancha y hoja acicular o agujas), el espaciado de las plantas (cerrado, abierto) y el clima. A diferencia de las ecozonas, los biomas no están definidos por semejanzas genéticas, taxonómicas o históricas. Los biomas con frecuencia se identifican con patrones particulares de sucesión ecológica y vegetación clímax (casi-estado de equilibrio del ecosistema local). Un ecosistema tiene muchos biotopos y un bioma es un tipo mayor de hábitat. Un tipo principal de hábitats, sin embargo, es un compromiso ya que posee una falta de homogeneidad intrínseca. 

La biodiversidad característica de cada bioma, especialmente la diversidad de la flora y fauna, está en función de factores abióticos que determinan la productividad de la biomasa de la vegetación dominante. En los biomas terrestres, la diversidad de especies tiende a correlacionarse positivamente con la producción primaria neta, con la disponibilidad de humedad y con la temperatura. 

El bioma está caracterizado fundamentalmente por el clima. Fue de hecho la distribución zonal de los climas lo que llevó a poner de relieve la zonificación de las tierras a finales del siglo XIX, y después, los biomas. 

Los parámetros físicos particularmente involucrados son temperatura y las precipitaciones. A su vez la temperatura está determinada especialmente con la latitud y la altitud. 


Los sistemas de clasificación de los biomas más utilizados corresponden a la latitud (o la zonificación de temperaturas) y la humedad. De hecho, el agua y la temperatura —cuya distribución a escala global está en gran medida condicionada por la rotación de la Tierra sobre su eje— son los dos factores clave para el establecimiento de un clima que presentan, a escala global y continental, variaciones según la latitud. Esta distribución está, por tanto, en correlación con bandas de vegetación homogéneas. Estas bandas latitudinales fueron observadas por primera vez por Vasili Dokucháyev, padre de la edafología rusa, y se llamaron zonas (del griego «zonê» que significa cintura), lo que dio a luz al concepto de zonificación, fundamental en la geografía del medio natural. Así por ejemplo, la biodiversidad es creciente, en general, desde los polos al ecuador, ya sea desde un punto de vista animal o vegetal, como en el caso de la selva ecuatorial densa que es el bioma más rico y diverso.

El término bioma a menudo suele confundirse con otros semejantes, como:

Un bioma, en general, agrupa más de un ecosistema y se puede clasificar dentro de niveles de organización biológica: 

El WWF organiza los grupos biológicos del siguiente modo:

Las características primarias de esta región son temperaturas bajas (entre –15 °C y 5 °C) y gran brevedad de la estación favorable. La precipitación pluvial es más bien escasa (unos 300 mm al año), pero el agua no suele ser factor limitante, ya que el ritmo de evaporación es también muy bajo. 
Existe una tundra ártica, también llamada "desierto polar", que se extiende por encima de los 60º de latitud N y una "tundra antártica", por encima de los 50ºS, que comprende la Antártida, las islas subantárticas y parte de la Patagonia.

Se da en pocas regiones del mundo: El Sur de Europa, el Norte de África, el Sur de Estados Unidos y parte de Sudamérica (Centro de Chile y Argentina). Cuando las temperaturas son más templadas y la humedad más abundante y repartida a lo largo del año, el bosque de coníferas es sustituido por el bosque caducifolio. En el Hemisferio Norte este bioma está dominado por hayas, robles, avellanos, olmos, castaños y numerosos arbustos que generan un suelo profundo y fértil. En las zonas templadas, si la pluviosidad es baja y la estación seca muy marcada, se instala otro tipo de bosque, de hoja perenne y resistente a la sequía estival. Es el bosque mediterráneo, con vegetación xerófila, dominado en Europa por la encina, el alcornoque o el roble quejigo.

Clima de bosque caducifolio: Encontramos el bosque caducifolio en torno a los 35º 55º de latitud . El clima típico tiene un régimen térmico moderado, precipitaciones abundantes, y bien distribuidas a lo largo del año y 4 estaciones bien definidas. En el predominan los suelos pardos poco o nada lixiviados y con humus mull o moder (degradación del bosque a la pradera alpina). En las pendientes aparecen suelos ranker o rendzina, más o menos ácidos, causados por la erosión sobre roca madre carbonatada.

El bioma de la pradera se encuentra en parajes con lluvia de 300 a 1500 mm por año, cifra insuficiente para el sustento de un bosque, y superior a la normal en un desierto verdadero. Algunas praderas se han desertificado por la acción del hombre. Se encuentra terreno de prado en el interior de los continentes y son bien conocidas las praderas del occidente de Estados Unidos y las de Argentina, Uruguay y parte de la región sur del Brasil, Australia, Rusia meridional y Siberia. El suelo de las praderas es muy rico en capas por virtud del rápido crecimiento y descomposición de los vegetales, y muy apropiado para el crecimiento de plantas alimenticias como trigo y maíz. Otras de sus características pueden ser:




El chaparral es también conocido como bosque mediterráneo. En las regiones del mundo de clima dócil, con lluvias relativamente abundantes en invierno pero con veranos muy secos, la comunidad culminante incluye árboles y arbustos de hojas gruesas y duras. Este tipo de vegetación se llama ""xerófila"". Durante los veranos secos y calurosos es constante el peligro de fuego que puede invadir rápidamente los lomeríos del chaparral.

Las comunidades de chaparral son muy extensas en California y costa noroccidental de México, a lo largo del Mediterráneo, en Chile y a lo largo de la costa sur de Australia.La diversidad del chaparral, un medio ambiente bastante uniforme, soporta relativamente pocas especies, pero muchas de sus plantas producen bayas comestibles y dan vida a vasta poblaciones de insectos y lo que el chaparral pierde en diversidad lo gana en número de individuos. Algunos vertebrados residentes característicos son los pequeños, ratas del bosque, ardillas listadas, lagartos y otros. Un ave característica del chaparral es el chochín herrerillo (Chamaea fasciata), una especie callada cuya área coincide casi exactamente con los límites del chaparral.

En el Mediterráneo, aunque la diversidad animal residente no es grande, la de aves migratorias es muy grande ya que esta región queda a mitad del camino entre los trópicos y las zonas más templadas. Durante el verano, la población de aves es menor, encontrándose solamente algunas aves tropicales, adaptadas al hábitat arbustivo y a condiciones de aridez. Llegan al Mediterráneo en primavera para nidificar, abandonándolo antes del comienzo del invierno. Entre los visitantes invernales, predominan las paseriformes (tales como las currucas y zorzales) y los patos.

El desierto se desarrolla en regiones con menos de 225 mm de lluvia anual. Lo característico de estas zonas es: 

Son poco productivos (menos de 500 g de carbono por año) y su productividad depende proporcionalmente de la lluvia que cae.
Algunos desiertos son cálidos, como el del Sahara, mientras que otros son fríos como el de Gobi. En algunos la lluvia es prácticamente inexistente, como en el de Atacama, en la cordillera de los Andes. Atacama está rodeado de altas montañas que bloquean la entrada de humedad desde el mar y favorecen la aparición de vientos catabáticos, secos y descendentes; este fenómeno se conoce como efecto Foehn. Otro mecanismo climático que forma desiertos en zonas cercanas a las costas es el ascenso de corrientes marinas frías cerca de los bordes continentales occidentales de África y América del Sur. El agua fría baja la temperatura del aire y son lugares en donde el aire desciende y no sopla hacia tierra. En el mar serán frecuentes las nieblas, pero en la tierra cercana no lloverá.

La vegetación se encuentra muy espaciada y las plantas suelen tener mecanismos repelentes para asegurar que en su cercanía no se sitúan otros ejemplares.

Hay cuatro formas principales de vida vegetal adaptadas al desierto: 

1. Plantas que sincronizan sus ciclos de vida, con los periodos de lluvia y crecen solo cuando hay humedad. Cuando llueve con intensidad suficiente, sus semillas germinan y con gran rapidez crecen las plantas y forman vistosas flores. Los insectos son atraídos por las flores y las polinizan al viajar de unas a otras. Muchos de estos insectos poseen también unos ciclos vitales muy cortos, adaptados a los de las plantas de las que se alimentan.

2. Matorrales de largas raíces que penetran en el suelo hasta llegar a la humedad. Se desarrollan especialmente en desiertos fríos. Sus hojas se suelen caer antes que la planta se marchite totalmente y de esta forma pasa a un estado de vida latente, hasta que vuelva a haber humedad en el subsuelo.

3. Plantas que acumulan agua en sus tejidos. Son de formas suculentas, como los cactus o las euforbias y tienen paredes gruesas, púas y espinas para protegerse de los fitófagos. Su rigidez es otra forma de protegerse contra la desecación producida por el viento.

4. Microflora, que permanece latente hasta que se producen buenas condiciones para su desarrollo.

La vida animal también ha desarrollado adaptaciones muy específicas para sobrevivir en un medio tan seco. Las excreciones de los animales que viven en el desierto contienen muy poca agua y muchos son capaces de obtener agua de los alimentos. Son de hábitos de vida nocturnos y durante el día permanecen en cuevas y madrigueras bajo tierra. 
El hombre ha desarrollado culturas que, con mucho ingenio, le han permitido vivir en los límites de los desiertos o en las mismas zonas desérticas.

Cuando el terreno desértico se riega, en los lugares en los que los suelos son adecuados, puede convertirse en uno de los sistemas agrícolas más productivos. Pero la puesta en cultivo de los terrenos áridos suele traer problemas de agotamiento de las fuentes de agua y salinización, como sucedió en las antiguas culturas mesopotámicas, si no se aplican sistemas para evitar esta dificultad. Para su explotación es necesario tener conocimientos del ecosistema y actuar en consecuencia.

Ocupa una franja de más de 1500 km de anchura en el hemisferio norte (América del norte, Europa y Asia) y también se encuentra en zonas montañosas.

Temperaturas invernales muy bajas (menos de -40 °C) y un verano relativamente corto. Escasez de agua (250 mm-500 mm anuales) y además permanece helada muchos meses.


La estepa es un bioma que comprende un territorio llano y extenso, de vegetación herbácea, propio de climas extremos y escasas precipitaciones. También se lo asocia a un desierto frío para establecer una diferencia con los desiertos tórridos. Estas regiones se encuentran lejos del mar, con clima árido continental, una gran amplitud térmica entre verano e invierno y precipitaciones que no llegan a los 500 mm anuales. Predominan las hierbas bajas y matorrales. El suelo contiene muchos minerales y poca materia orgánica; también hay zonas de la estepa con un alto contenido en óxido de hierro lo que le otorga una tonalidad rojiza a la tierra.

Las selvas tropicales ocupan extensas superficies cercanas al centro del Ecuador, Sudamérica, África, Asia y Oceanía, y prosperan en climas muy húmedos y calurosos, estando provistas no solo de lluvias abundantes, sino también de ríos caudalosos que experimentan crecidas violentas en otoño. Una selva de lluvia no es una "jungla". La jungla es una vegetación arbustiva muy densa que crece a lo largo de las riberas de los ríos. Puede aparecer en tierra cuando la selva lluviosa ha sido talada por los humanos o por un evento natural como una inundación o un incendio. La mayor parte de las junglas se transforman en selvas lluviosas. Por lo tanto, la jungla es una selva húmeda.

Las sabanas son praderas tropicales con una pequeña cantidad de árboles o arbustos dispersos. Se desenvuelven en regiones de alta temperatura, que tienen marcada diferencia entre las estaciones seca y húmeda. En la estación húmeda el crecimiento de las plantas es rápido, pero éstas se secan y bajan en calidad durante la estación seca. Las sabanas tropicales cubren áreas extensas en América del Sur, África, India, Sudeste Asiático y Australia Septentrional. 
El crecimiento animal y vegetal en la sabana tropical, depende de las distintas alteraciones periódicas. Los grandes animales emigran en busca de agua, y sus ciclos reproductivos corresponden a la disponibilidad de crecimiento de nuevas plantas. Muchos animales se reúnen en grandes manadas. Es necesario una gran área de producción fotosintética para alimentar a estos grandes animales. El fuego regular es importante para este ecosistema, de él depende el mantenimiento de las praderas en lugares donde las manadas no son tan numerosas.

Los biomas acuáticos pueden ser marinos (agua salada) o dulceacuícolas. Los biomas marinos son básicamente el oceánico o pelágico y el litoral o nerítico, caracterizados por la diferente profundidad que alcanza el agua y por la distancia a la costa. La zona litoral se caracteriza por la luminosidad de sus aguas, escasa profundidad y abundancia de nutrientes. En ella se concentran algas, moluscos, equinodermos y arrecifes de coral. Las tortugas, focas y peces óseos son muy comunes en esta zona. La zona pelágica se caracteriza por tener una banda iluminada pero también grandes profundidades sin luz. En estas regiones los seres acuáticos se han adaptado a vivir sin ella y a estar sometidos a grandes presiones.

Los biomas dulceacuícolas son básicamente las aguas quietas (lénticas) de lagos y lagunas y las aguas corrientes (lóticas) de ríos y arroyos. De la superficie del planeta, el 70% de su superficie está ocupado por los océanos. Del restante 30%, que corresponde a tierras emergidas, un 11% de esa superficie se halla cubierto por hielo, lo que se puede clasificar como desierto helado, y el 10% lo ocupa la tundra.

Los manglares son biomas de árboles que toleran la sal y crecen en las costas, donde baja y sube el nivel del mar. Estos árboles generan tierras firmes de forma natural al acumular partículas de arena y hojas de mangle en el suelo y cuando baja la marea formando tierras pantanosas.

La necesidad de disponer de un sistema de clasificación de los biomas surgió después de la creación de los sistemas de clasificación de climas, que se basaban solamente en criterios meteorológicos como la pluviometría y la insolación. Las primeras clasificaciones bioclimáticas nacieron en la década de 1950 con la clasificación de Holdridge. Los sistemas de clasificación pioneros trataban de definir los biomas utilizando las mediciones climáticas. Después, en los años 1970 y 1980 se produjo un importante impulso para entender las relaciones entre estos parámetros y las propiedades energéticas de los ecosistemas, porque tales descubrimientos permitirían la predicción de las tasas de captura de energía y la transferencia entre los distintos componentes de los ecosistemas. 

Un estudio de ese tipo fue realizado por Sims et al. (1978) sobre las praderas de América del Norte. El estudio encontró una correlación positiva entre la evapotranspiración, en mm/año y la producción primaria neta por encima del suelo en g/m²/año. Otros resultados generales del estudio fueron que la precipitación y el uso del agua llevan a la producción primaria sobre el terreno; que la radiación solar y la temperatura llevan a una producción primaria subterránea (raíces); y que la temperatura y el agua llevan a hábitats de crecimiento estacional de temporada fría y caliente. Estos resultados ayudan a explicar las categorías utilizadas en el sistema de bioclasificación de Holdridge, que luego fueron simplificados en la de Whittaker.

Las clasificaciones ecológicas se fueron haciendo cada vez más precisas y detalladas y varios países quisieron tener su propio sistema de clasificación. El número de sistemas de clasificación y la amplia variedad de los factores determinantes utilizados debe tomarse como un indicador de que no todos los biomas encajan perfectamente en los sistemas de clasificación creados y que las clasificaciones realizadas no son equivalentes, ya que los criterios elegidos para la definición de las zonas cumplen diferentes objetivos según sean los Estados o las organizaciones que los eligen. 
Así los Estados Unidos han establecido clasificaciones como la Clasificación Estándar de la vegetación nacional de los Estados Unidos («United States National Vegetation Classification Standard») en el marco de la Comisión para la Cooperación Ambiental («Commission de coopération environnementale») que ayudará a definir los biomas. 

Los biomas definidos son enumerados de manera precisa, lo que permite definir una política de protección precisa. Los lugares importantes para cada bioma fueron listados en bases de datos del tipo de la base europea Corine Biotopo («Corine Biotope»), hoy reemplazada por la del European Union Nature Information System (EUNIS). Los biomas utilizados por la Unión Europea figuran en el Mapa Digital de la Región Ecológica Europea («Digital Map European Ecological Region», DMEER) o por la Clasificación medioambiental de Europa (Environmental classification of Europe, CNE). A veces, todo un bioma puede ser objeto de protección, especialmente por la acción individual de una nación, mediante la elaboración de un Plan de Acción sobre la Biodiversidad («Biodiversity Action Plan», BAP).

El Sistema de clasificación de Holdridge es un proyecto para la clasificación de las diferentes áreas terrestres según su comportamiento global bioclimático. Fue desarrollado por el botánico y climatólogo estadounidense Leslie Holdridge (1907-99) y fue publicado por vez primera en 1947 (con el título de Determination of World Plant Formations from Simple Climatic Data) y posteriormente actualizado en 1967 (Life Zone Ecology).
Utiliza el concepto de zona de vida y se basa en los siguientes factores: 

En este sistema las zonas biogeográficas se clasifican según los efectos biológicos de la temperatura y las precipitaciones en la vegetación, en el supuesto de que estos dos factores abióticos son los principales determinantes del tipo de vegetación que se encuentra en una zona. Holdridge utiliza 4 ejes (biotemperatura, precipitación, piso altitudinal y región latitudinal) para definir las llamadas 30 «provincias de humedad», que son claramente visibles en el diagrama de Holdridge. Ya que su clasificación ignora en gran medida el suelo y la exposición al sol, Holdridge reconoció que estos elementos, eran factores importantes, demasiado, en la determinación de los biomas.

Robert Harding Whittaker (1920-80), ecólogo y botánico estadounidense, apreció la existencia de los tipos de bioma como una representación de la gran diversidad del mundo viviente, y vio la necesidad de establecer una manera sencilla de clasificar esos tipos de biomas. Whittaker basó su sistema de clasificación en dos factores abióticos: la temperatura y la precipitación. Su esquema puede considerarse como una simplificación del de Holdridge, más fácilmente accesible, pero quizás echando en falta la mayor especificidad que proporciona el de Holdrige. 

Whittaker basa su representación de los biomas mundiales en las dos anteriores afirmaciones teóricas, así como en una toma de muestras empíricas cada vez mayor de los ecosistemas mundiales. Whittaker se encontraba en una posición única para hacer tal afirmación holística ya que previamente había compilado una revisión de la clasificación de biomas.

Los conceptos clave para la comprensión del esquema de Whittaker son los siguientes:


La distinción de Whittaker entre bioma y formación se puede simplificar: la formación se utiliza cuando se aplica sólo a las comunidades vegetales, mientras que el bioma se utiliza cuando se trata de plantas y animales. La convención de Whittaker de tipo de bioma o tipo de formación es simplemente un método más amplio para clasificar comunidades similares. Los tipos de bioma del mundo, mostrados en un mapa del mundo, se puede ver en el siguiente enlace: here 

Whittaker, viendo la necesidad de disponer de una manera más sencilla de expresar la relación de la estructura de la comunidad con el medio ambiente, utiliza lo que él llamó «análisis de gradiente» («gradient analysis») de patrones de ecoclinas para relacionar las comunidades con el clima a una escala mundial. Whittaker considera cuatro grandes ecoclinas en el reino terrestre:


A lo largo de estos gradientes, Whittaker encontró varias tendencias que le permitieron establecer cualitativamente los tipos de bioma. 


Whittaker resume los efectos de los gradientes (3) y (4) disponiendo un gradiente de temperatura conjunto y combina éste con el gradiente de humedad (2), para expresar las conclusiones anteriores en lo que se conoce como el Esquema de Clasificación de Whittaker («Whittaker Classification Scheme»). El esquema representa gráficamente la precipitación media anual (eje x) versus la temperatura media anual (eje Y) para clasificar los tipos de biomas.

El sistema de clasificación de Heinrich Walter fue desarrollada por Heinrich Walter, un ecologista alemán. Se diferencia tanto de los regímenes de Holdridge y Whittaker porque tiene en cuenta la estacionalidad de la temperatura y las precipitaciones. El sistema, también basado en la precipitación y temperatura, encuentra 9 grandes biomas, cuyos rasgos más importantes de clima y tipos de vegetación se resume en el cuadro adjunto. Los límites de cada bioma se correlacionan con las condiciones de humedad y frío que son determinantes importantes de la forma de las plantas y, por tanto, de la vegetación que define la región. 

Robert G. Bailey casi desarrolló un sistema de clasificación biogeográfica para los Estados Unidos en un mapa publicado en 1976. Bailey posteriormente amplió el sistema para incluir el resto de América del Sur en 1981 y en el mundo en 1989. El sistema de Bailey se basa en el clima y está dividido en siete dominios (polar, templado húmedo, seco, húmedo y húmedo tropical), con otras divisiones basadas en otras características climáticas (subártica, cálido templado, caliente templado y subtropical; marinos y continental; tierras bajas y montaña).

Un equipo de biólogos convocado por el Fondo Mundial para la Naturaleza (WWF) ha desarrollado un sistema de clasificación ecológico en el que se identificaron los llamados «tipos principales de hábitat» («Major Habitat Types», semejantes a los biomas) después de analizar las 867 ecorregiones terrestres en que se dividió la Tierra. Cada una de esas ecorregiones terrestres tiene un número de identificación o EcoID, con un formato del tipo XXnnNN (en el que XX es la Ecozona, nn es el número del bioma y NN es el número individual de la ecorregión). Esta clasificación se utiliza para definir la lista Global 200 de las ecorregiones identificadas por el WWF como prioridades para la conservación. 

El WWF organiza los biomas en dos grandes grupos, los biomas terrestres y los marinos, y los terrestres, a su vez, en dos subgrupos, los biomas terrestres propiamente y los biomas de agua dulce. Aunque existen biomas marinos, responden mucho menos a los criterios de zonificación —debido a las grandes corrientes que atraviesan los océanos a todos los niveles de profundidad— y son más difíciles de definir en el espacio. En el sentido de bioma según ha sido definido, el estudio de los ambientes acuáticos recaería preferentemente en la oceanografía —estudio de los mares— o de la limnología —estudio de las aguas dulces—. 

El WWF ha identificado 14 tipos de hábitat principales terrestres, 12 de aguas dulces y 7 marinos. Los terrestres se recogen en el siguiente mapa y todos ellos la tabla correspondiente (el código de colores no posee una norma que lo unifique por lo que se adecúa al mapa adjunto).

La forma de clasificar las ecorregiones del WWF responde al siguiente esquema:

La Organización para la Alimentación y la Agricultura (FAO) de las Naciones Unidas, ha desarrollado mapas ecológicos y forestales mundiales que dan una definición espacial y estadística, proporcionando una visión de la cubierta forestal mundial, lo que proporciona un medio importante para agregar información global sobre los recursos naturales de acuerdo con sus características ecológicas.

El siguiente mapa está inspirado en las zonas ecológicas o ecozonas de la FAO:



</doc>
<doc id="11500" url="https://es.wikipedia.org/wiki?curid=11500" title="François Truffaut">
François Truffaut

François Roland Truffaut (; París, 6 de febrero de 1932 - 21 de octubre de 1984) fue un director, crítico y actor francés. Fue uno de los iniciadores del movimiento llamado "la Nouvelle vague", si bien luego evolucionó de un modo muy personal.

Reconocido en el registro civil como hijo por Roland Truffaut, un delineante (o arquitecto y decorador), François Truffaut nunca llegó a conocer a su verdadero padre. Su madre Jeanine de Montferrand, que era secretaria en el periódico "L'Illustration", será recordada en su cine conflictivamente. 

Sus padres se despreocuparon de él, y fue atendido por sus abuelos maternos hasta los diez años. La orfandad forma parte de sus personajes esenciales y también originó esa "novela familiar" que rodea varias de sus historias. De todos modos, una agencia privada de detectives, encargada por Truffaut, señaló en 1968, que las indagaciones sobre su origen conducían a Roland Levy, un dentista de origen judío, de Bayona (al que vio de lejos); ese hallazgo lo rechazó la familia materna, pero a Truffaut le pareció posible, y hoy viene recogida en las monografías más autorizadas, así en la muy extensa que fue escrita por dos autores clave de "Cahiers du Cinéma".

François Truffaut, cuya infancia fue más bien desgarrada y fantasiosa, estudió en la escuela de la rue Clauzel y en el liceo Rollin, aunque nunca fue un alumno ejemplar. A partir de 1939, el joven Truffaut, que era un lector apasionado de literatura, también se pasaba la vida en el cine, a veces durante las horas en las que debería estar en clase (destacó pronto a Renoir, Rossellini, Hitchcock, Vigo, Buñuel, Bresson, Welles, N. Ray, K. Vidor, Ophuls, Sternberg, Stroheim).

Desde 1946, una vez que dejó sus estudios, sobrevivió con pequeños trabajos, como mozo de almacén, y fundó un cine-club en 1947, pero algunos problemas económicos (el alquiler de películas le condujo al impago por fracaso) hacen que sea enviado por su padre adoptivo a un correccional en Villejuif, del que fue sacado por André Bazin, que conoció en ese trabajo de divulgación cinematográfica.

Gracias de nuevo al crítico de cine André Bazin, su referencia vital, François Truffaut empieza a trabajar en "Travail et Culture". Escribe sus primeros artículos desde 1950. Tras alistarse en el ejército, se le envía a Alemania, pero deserta y pasa por la prisión militar; es liberado por Bazin, alegando inestabilidad de carácter. 

A continuación publica críticas en los "Cahiers du cinéma" en 1953, con sus colegas innovadores, pero también ese trabajo, que llega hasta 1959, lo hace en "Le Parisienne", en "Arts, Radio, Cinéma" y "Le Bulletin de Paris".La persona de cine de su momento más influyente en él fue Jacques Rivette.

Dirige ya al año siguiente, 1954, su primer cortometraje: "Une visite".

En 1956, Truffaut fue ayudante de dirección de Roberto Rossellini. Se casó en 1957 con Madeleine Morgenstern, hija de un distribuidor de cine, siendo testigos André Bazin y Roberto Rossellini; tuvo dos hijas, Laura y Eva. Se separaron en 1965, aunque mantuvieron relación y hasta convivencia toda la vida, por amistad y por las hijas (ella se casó pero se separó enseguida). Madeleine le cuidó al final de sus días, y ha sido su albacea (aunque Truffaut tuvo otra hija, Josephine, con Fanny Ardant, en 1983, que fue la mujer del final). Ese mismo año dirigió otro cortometraje, "Les Mistons" ("Los golfillos").

En 1958, rueda "Los cuatrocientos golpes", que servirá de carta de presentación al mundo del movimiento de la Nouvelle vague, que encabeza junto a Claude Chabrol, Éric Rohmer, Jean-Luc Godard, Alain Resnais o Jacques Rivette. Tendrá un éxito espectacular (Cannes, Acapulco, Fémina de Bélgica, Crítica de Nueva York, el Meliès, el Laurel de David Selznick, Valladolid).

Colaboró con Godard (guion de "Al final de la escapada"), y en los inicios de Rivette. Aparece ya una característica de Truffaut, su preocupación por la infancia, tan conflictiva en su caso, y por los más desamparados; e irá desde su primer largo "Los cuatrocientos golpes" (documento que radiografía autobiográficamente la realidad francesa tras la 2.ª Guerra Mundial), pasando por la revisión de las teorías de Jean-Jacques Rousseau en "El pequeño salvaje", hasta la sensibilidad que demuestra en la visión que un adulto puede llegar a tener de los niños y su mundo acometida en "La piel dura".

En 1968, cuando el gobierno destituyó a Henry Langlois de la Cinemateca francesa, se creó un comité de defensa, presidido por Jean Renoir, del que él fue tesorero, con Doniol-Valcroce, y organizó protestas. Fue el momento de mayor intervención social de Truffaut, en general replegado, pero firme defensor de libertades, como hizo con su admirado Jean-Paul Sartre.

Dirigirá Truffaut películas hasta su muerte a los 52 años, el 21 de octubre de 1984 en Neuilly-sur-Seine debido a un tumor cerebral. Está enterrado en el cementerio de Montmartre en París.

Hay que destacar que François Truffaut aparece como actor en varias de sus películas, como "La habitación verde", "La noche americana o" "El pequeño salvaje." También realizó un papel en "Close Encounters of the Third Kind," película de 1977 de Steven Spielberg, en la que interpretaba al sabio francés "Claude Lacombe".

Es conocido también por ser el autor de un extenso libro de entrevistas a Alfred Hitchcock, "El cine según Hitchcock", que se ha convertido en una referencia en los estudios de cine. 

Destaca también el Truffaut escritor. Además de ser un cuidadoso guionista, escribió muchas y buenas críticas, así como el prólogo a la obra de André Bazin, muerto prematuramente como él mismo, o el prefacio al libro de su gran director de fotografía, Néstor Almendros, con el que hizo nueve películas.

Truffaut es el director de su generación más aceptado en Estados Unidos. Woody Allen siempre se declaró admirador suyo. Pasó varias temporadas en California, para aprender inglés o para descansar y visitar amigos.

Entre las muchas películas de Truffaut, cabe destacar la serie en la que aparece el personaje de Antoine Doinel, interpretado por el actor Jean-Pierre Léaud, quien inicia con 14 años su carrera de actor en "Los cuatrocientos golpes": será el actor-fetiche y "alter ego" del propio Truffaut, con el que le confundieron alguna vez, según aparece en "Les aventures de Antoine Doinel", libro prologado por Truffaut que recoge sus guiones de toda esa secuencia de filmes. Esta serie seguirá hasta "El amor en fuga", y pasando por un episodio de "El amor a los 20 años", "Besos robados" y "Domicilio conyugal" junto a Claude Jade en el papel de Christine, amiga y mujer de Doinel. Una hija de Truffaut, Eva Truffaut, seguía en 2004 buscando las últimas escenas de su padre y ha producido un serial radiofónico "El diario de Alphonse", en donde aparecen Christine Doinel (Claude Jade) y su hijo Alphonse (Stanislas Merhar). 

Lector apasionado, Truffaut llevará al cine muchas novelas: a) policiales estadounidenses ("La novia vestía de negro" y "La sirena del Mississippi" de William Irish, "Vivamente el domingo" (o bien, más cercano al francés, "Ojalá el domingo llegue pronto", de Charles Williams, "Disparen al pianista" de David Goodis y "Una chica tan decente como yo" de Henry Farrell); b) satírico-costumbristas, destacadamente de Henri-Pierre Roché "Jules y Jim" y "Las dos inglesas y el continente"; c) de ciencia-ficción "Fahrenheit 451" de Ray Bradbury; d) un relato de fantasmas de Henry James, en "La habitación verde", que es un film que revela lo más profundo de sus inquietudes: la amistad, la pasión, la muerte.

El resto de las películas de Truffaut surgen de guiones originales, a menudo en colaboración con su gran colaboradora, Suzanne Schiffman, o Jean Gruault. Son películas de temas muy diversos, que van desde "Diario íntimo de Adèle H.", basada en la vida de la hija de Víctor Hugo, con Isabelle Adjani, o "La noche americana", un auténtico homenaje al cine, que fue premiado en la ceremonia de los Óscar de Hollywood con el Óscar a la mejor película de habla no inglesa en 1973), y también "El último metro", película que se desarrolla durante la ocupación alemana de Francia y con la que ganó diez Premios César concedidos por la Academia del Cine Francés. 

Pero no hay que olvidar que el propio director decía que "no hay buenas historias, sólo hay buenas películas".

Los inicios del movimiento cinematográfico y del propio realizador son una crítica al academicismo y a los convencionalismos del cine francés hasta mediados de los años 50, a los que acusaban de ser caducos reflejos del arte de narrar visualmente una historia, un sentimiento, etc. Ellos creían que el cine tenía que renovarse enfocando cada historia, personaje o situación desde una perspectiva más cercana, humana y, dentro de lo posible, real. Concretamente, Truffaut señaló que solo unos pocos directores franceses trabajaban de un modo más personal, como Jacques Tati, Robert Bresson, Max Ophuls, Jacques Becker y Jean Renoir.

Por ello, muchos de estos jóvenes teóricos cinematográficos, reconvertidos en directores, se adhieren al género documental en sus primeras realizaciones, tomando incluso elementos del entonces en declive "neorrealismo italiano", del realismo francés de los años 30 de Jean Vigo, Renoir o Carné, además del cine experimental y de vanguardia de los años 20.

Tras debutar en el largometraje, estos directores rápidamente entendieron que el nuevo movimiento (al igual que ocurrirá con el "free cinema" británico o el "nuevo cine" alemán), tenía unas claras limitaciones en cuanto a estructuras narrativo-visuales, y que al intentar salir de la independencia artística e integrar sus motivos conceptuales en un cine más comercial —y con mejores medios a raíz del éxito comercial de sus primeras cintas—, el estilo se diluía en parte desde su propia base y era percibido por crítica y público como un cine igual de clásico que el de la etapa anterior, si bien suponía un clasicismo renovado y una reflexión sobre un presente muy distinto ya de los años de posguerra. 

El propio Truffaut comienza siendo renovador y rupturista, pero con su estilo más moderado (desde "Jules y Jim" hasta "Domicilio conyugal") que otros, para ser además un autor lúcido y brillante y con un clasicismo formal tan descollante como insospechado, desde "Las dos inglesas y el amor" hasta su último film. 

De ahí las muy polémicas páginas sobre él escritas por Godard, su amigo-enemigo, que le alabó cariñosamente en su estreno y que decía en 1965 que era el "cineasta cada vez más y más serio". En 1973, Godard atacó absurda y violentamente a Truffaut (éste solo escribiría "historias"), y la respuesta implacable de Truffaut, que denunciaba a Godard por su frialdad y cobardía, su ideologismo coyuntural y elitista, su impostura vanidosa, les llevó a la ruptura.Truffaut le había ayudado en el primer guion y también como productor. Hicieron una carrera en paralelo en vida de Truffaut, con dos ángulos de visión casi opuestos; incluso su roce indica admiración, pese a tantas diferencias radicales.












</doc>
<doc id="11501" url="https://es.wikipedia.org/wiki?curid=11501" title="Bioestadística">
Bioestadística

La bioestadística es la rama de la estadística aplicada a las ciencias de la vida, como la biología o la medicina, entre otras.

El primer médico que utilizó métodos matemáticos para cuantificar variables de pacientes y sus enfermedades fue el francés Pierre Charles-Alexandre Louis (1787-1872). La primera aplicación del Método numérico (que es como tituló a su obra y llamó a su método) en su clásico estudio de la tuberculosis, que influyó en toda una generación de estudiantes. Sus discípulos, a su vez, reforzaron la nueva ciencia de la epidemiología con base en el método estadístico. En las recomendaciones de Louis para evaluar diferentes métodos de tratamiento están las bases de los ensayos clínicos que se hicieron un siglo después. En Francia Louis René Villermé (1782-1863) y en Inglaterra William Farr (1807-1883) —que había estudiado estadística médica con Louis— hicieron los primeros mapas epidemiológicos usando métodos cuantitativos y análisis epidemiológicos. Francis Galton (1822-1911), basado en el darwinismo social, fundó la biometría estadística.

Pierre Simon Laplace (1749-1827), astrónomo y matemático francés, publicó en 1812 un tratado sobre la teoría analítica de las probabilidades, Théorie analytique des probabilités, sugiriendo que tal análisis podría ser una herramienta valiosa para resolver problemas médicos.

Los primeros intentos de hacer coincidir las matemáticas de la teoría estadística con los conceptos emergentes de la infección
bacteriana tuvieron lugar a comienzos del siglo XX. Tres diferentes problemas cuantitativos fueron estudiados por otros tantos autores. William Heaton Hamer (1862-1936) propuso un modelo temporal discreto en un intento de explicar la ocurrencia regular
de las epidemias de sarampión; John Brownlee (1868-1927), primer director del British Research Council, luchó durante veinte
años con problemas de cuantificación de la infectividad epidemiológica, y Ronald Ross (1857-1932) exploró la aplicación matemática
de la teoría de las probabilidades con la finalidad de determinar la relación entre el número de mosquitos y la incidencia de malaria en situaciones endémicas y epidémicas. Pero el cambio más radical en la dirección de la epidemiología se debe a Austin
Bradford Hill (1897-1991) con el ensayo clínico aleatorizado y, en colaboración con Richard Doll (n. 1912), el épico trabajo que correlacionó el tabaco y el cáncer de pulmón.

Los primeros trabajos bioestadísticos en enfermería los realizó, a mediados del siglo XIX la enfermera inglesa Florence Nightingale. Durante la guerra de Crimea, Florence Nightingale observó que eran mucho más numerosas las bajas producidas en el hospital que en el frente. Por lo tanto, recopiló información y dedujo que la causa de la elevada tasa de mortalidad se debía a la precariedad higiénica existente. Así, gracias a sus análisis estadísticos, se comenzó a tomar conciencia de la importancia y la necesidad de unas buenas condiciones higiénicas en los hospitales.

El razonamiento y la modelización bioestadísticas fueron fundamentales en la fundación de la Síntesis Moderna de la evolución. A principios de los años noventa, después del redescubrimiento de la obra de Mendel, los problemas conceptuales ligados a la comprensión de la relación entre la genética y el darwinismo condujeron a un acalorado debate entre "biométricos" (Weldon, Pearson) y "mendelianos" (Davenport, Bateson). En los años 30, tres grandes estadísticos, Ronald Fisher, quién desarrolló varios métodos básicos de la estadística en su libro "The Genetical Theory of Natural Selection, " Sewall G. Wright y J. B. S. Haldane lograron resolver el conflicto e introdujeron la bioestadística y, en particular, la genética de poblaciones, como una de las ramas esenciales de la Síntesis evolutiva moderna.

La aplicación resulta hoy en día necesaria, en los campos:

La colaboración de la bioestadística ha sido clave en el desarrollo de nuevos fármacos, en el entendimiento de enfermedades crónicas como el cáncer y el sida, y estos son solo algunos de los miles de ejemplos posibles.

La estrecha relación de la Estadística con el método científico hace de la Bioestadística una disciplina imprescindible en la mayoría de los proyectos en el área tecnológica.

El pensamiento estadístico no solo resuelve y entiende la compleja metodología para dar respuesta a hipótesis, sino que es capaz de organizar el “sistema” que involucra la investigación desde el diseño general, diseño de muestreo, control de calidad de la información, análisis y presentación de resultados.

Siendo como es, parte fundamental del desarrollo del conocimiento en todas las áreas de la salud, la estadística no está exenta de dificultades. Lo cierto es que, como han puesto de manifiesto numerosos autores, la mayor parte de los trabajos científicos que se publican en la actualidad están aquejados de defectos en su metodología, graves en numerosas ocasiones, a veces debidos a la falta de formación de los autores y los revisores, pero también, en otras ocasiones, debidos a la intencionalidad de transmitir algún mensaje concreto a través de los resultados del trabajo.




</doc>
<doc id="11503" url="https://es.wikipedia.org/wiki?curid=11503" title="Atmósfera terrestre">
Atmósfera terrestre

La atmósfera terrestre es la parte gaseosa de la Tierra, siendo por esto la capa más externa y menos densa del planeta. Está constituida por varios gases que varían en cantidad según la presión a diversas alturas. Esta mezcla de gases que forma la atmósfera recibe genéricamente el nombre de aire. El 75 % de masa atmosférica se encuentra en los primeros 11 km de altura, desde la superficie del mar. Los principales gases que la componen son: el oxígeno (20 %) y el nitrógeno (80 %), seguidos del argón, el dióxido de carbono y el vapor de agua.

La atmósfera y la hidrosfera constituyen el sistema de capas fluidas superficiales del planeta, cuyos movimientos dinámicos están estrechamente relacionados. Las corrientes de aire reducen drásticamente las diferencias de temperatura entre el día y la noche, distribuyendo el calor por toda la superficie del planeta. Este sistema cerrado evita que las noches sean gélidas o que los días sean extremadamente calientes.

La atmósfera protege la vida sobre la Tierra, absorbiendo gran parte de la radiación solar ultravioleta en la capa de ozono. Además, actúa como escudo protector contra los meteoritos, los cuales se desintegran en polvo a causa de la fricción que sufren al hacer contacto con el aire.

Durante millones de años, la vida ha transformado, una y otra vez, la composición de la atmósfera. Por ejemplo; su considerable cantidad de oxígeno libre es posible gracias a las formas de vida —como son las plantas— que convierten el dióxido de carbono en oxígeno, el cual es a su vez respirable por las demás formas de vida, tales como los seres humanos y los animales en general.

En la atmósfera terrestre se pueden distinguir dos regiones con distinta composición, la homosfera y la heterosfera.

La homosfera ocupa los 100 km inferiores y tiene una composición constante y uniforme.

La heterosfera se extiende desde los 80 km hasta el límite superior de la atmósfera (unos 10 000 km); está estratificada, es decir, formada por diversas capas con composición diferente.

La variación con la altura de la presión atmosférica con el conocimiento que se tiene del magnetismo o de la densidad atmosférica es lo que se conoce como "ley barométrica". La diferencia de presión entre dos capas separadas por un formula_1 es:

pues se supone la densidad constante. La "ley de la densidad" suponiendo el aire como un gas ideal

aplicada a la superficie de la Tierra resulta una densidad del aire formula_2.

En una atmósfera isoterma la presión varía con la altura siguiendo la ley:
donde M es la masa molecular, g la aceleración de la gravedad, h-h es la diferencia de alturas entre los niveles con presiones P y P y T es la temperatura absoluta media entre los dos niveles, y R la constante de los gases perfectos. El hecho de que la temperatura varíe sí limita la validez de la fórmula. Por el contrario, la variación de la aceleración de la gravedad es tan mínima que no afecta.

La escala de altura es la altura a la que hay que elevarse en una atmósfera para que la presión atmosférica disminuya en un factor "e" = 2,718182. Es decir la disminución de presión es 1-1/"e" = 0,632 (= 63,2 %). Para calcularla basta con poner en la Ley barométrica formula_4 resulta:

Para la atmósfera de la Tierra la escala de alturas H es de 8,42 km. En función de la escala de alturas H la presión puede expresarse:
y análogamente para la densidad:

La temperatura de la atmósfera terrestre varía con la altitud. La relación entre la altitud y la temperatura es distinta dependiendo de la capa atmosférica considerada: troposfera, estratosfera, mesosfera, termosfera y exosfera. A esto se le llama gradiente térmico atmosférico.

Las divisiones entre una capa y otra se denominan respectivamente tropopausa, estratopausa, mesopausa y termopausa.

Sus principales características son:

Su nombre obedece a que está dispuesta en capas más o menos horizontales (o estratos).
Se extiende entre los 9 o 18 km hasta los 50 km de altitud. La estratosfera es la segunda capa de la atmósfera de la Tierra. A medida que se sube, la temperatura en la estratosfera aumenta. Este aumento de la temperatura se debe a que los rayos ultravioleta transforman al oxígeno en ozono, proceso que involucra calor: al ionizarse el aire, se convierte en un buen conductor de la electricidad y, por ende, del calor. Es por ello que a cierta altura existe una relativa abundancia de ozono (ozonosfera) lo que implica también que la temperatura se eleve a unos –3 °C o más. Sin embargo, se trata de una capa muy enrarecida, muy tenue.

Se denomina capa de ozono, u ozonosfera, a la zona de la estratosfera terrestre que contiene una concentración relativamente alta de ozono. Esta capa, que se extiende aproximadamente de los 15 km a los 40 km de altitud, reúne el 90 % del ozono presente en la atmósfera y absorbe del 97 % al 99 % de la radiación ultravioleta de alta frecuencia.

Es la tercera capa de la atmósfera de la Tierra. Se extiende entre los 50 y 80 km de altura, contiene solo el 0.1 % de la masa total del aire. Es la zona más fría de la atmósfera, pudiendo alcanzar los –80 °C. Es importante por la ionización y las reacciones químicas que ocurren en ella. La baja densidad del aire en la mesosfera determina la formación de turbulencias y ondas atmosféricas que actúan a escalas espaciales y temporales muy grandes.

En la termosfera (de 69/90 a los 600/800 km), la temperatura aumenta con la altitud, de ahí su nombre. Coincide prácticamente con la la región llamada ionosfera. Es la cuarta capa de la atmósfera de la Tierra. Se encuentra encima de la mesosfera. A esta altura, el aire es muy tenue y la temperatura cambia con la mayor o menor radiación solar tanto durante el día como a lo largo del año. Si el sol está activo, las temperaturas en la termosfera pueden llegar a 1500 °C e incluso más altas. En ella se encuentra el 0.1 % de los gases.

La última capa de la atmósfera de la Tierra es la exosfera (600/800-2000/). Esta es el área donde los átomos se escapan hacia el espacio.
Como su nombre indica, es la región atmosférica más distante de la superficie terrestre. Su límite superior se localiza a altitudes que alcanzan los 960 e incluso 1000 km, y está relativamente indefinida. Es la zona de tránsito entre la atmósfera terrestre y el espacio interplanetario.

Es la región de la atmósfera donde se concentra la mayor parte del ozono. Está situada en la estratosfera, entre los 15 y 32 km, aproximadamente. Esta capa protege a la Tierra de la radiación ultravioleta del Sol.

Es la región ionizada por el bombardeo producido por la radiación solar. La ionización de esta capa produce la reflexión de las ondas de radio emitidas desde la superficie terrestre, lo que permite su recepción a grandes distancias. Se corresponde aproximadamente con la termosfera.

Es la región exterior a la Tierra donde el campo magnético, generado por el núcleo terrestre, actúa como protector de los vientos solares.

Son capas situadas cerca de la mesopausa, que se caracterizan por la luminiscencia, incluso nocturna, causada por la reestructuración de átomos en forma de moléculas que habían sido ionizadas por la luz solar durante el día, o por rayos cósmicos. Las principales capas son la del OH, a unos 85 km, y la de O, situada a unos 95 km de altura, ambas con un grosor aproximado de unos 10 km.

Se llama dinámica de la atmósfera o dinámica atmosférica a una parte de la Termodinámica que estudia las leyes físicas y los flujos de energía involucrados en los procesos atmosféricos. Estos procesos presentan una gran complejidad por la enorme gama de interacciones posible tanto en el mismo seno de la atmósfera como con las otras partes (sólida y líquida) de nuestro planeta.

La termodinámica establece tres leyes, además de lo que se conoce como principio cero de la termodinámica. Estas tres leyes rigen en todo el mundo físico-natural y constituyen la base científica de los procesos que constituyen el campo de la dinámica de la atmósfera. Así pues, la dinámica atmosférica involucra a todos los movimientos que se presentan en el seno de la atmósfera terrestre y estudia también las causas de dichos movimientos, los efectos de los mismos y, en general todos los flujos de energía térmica, eléctrica, físico-química, y de otros tipos que ocurren en la capa de aire que rodea a la Tierra.

La atmósfera funciona como un escudo protector contra los impactos de enorme energía que pueden provocar los pequeños objetos espaciales al colisionar a altísima velocidad contra la superficie del planeta.

Sin atmósfera, la velocidad de colisión de estos objetos sería la suma de su propia velocidad inercial espacial (medida desde nuestro planeta) más la aceleración provocada por la gravitación terrestre.

La energía cinética de los meteoritos se transforma en calor por la fricción de los mismos en el aire y desde la superficie vemos un meteoro, meteorito o también estrella fugaz.

La fricción es la manifestación macroscópica de una transferencia de energía cinética, o su transformación en otro tipo de energía, por la que un cuerpo "pierde" movimiento cediéndoselo a otro ya sea transfiriéndole parte de su propio movimiento o transformándose en movimientos moleculares (calor, vibración sonora, etc.)

Un cuerpo en caída libre dentro de la atmósfera puede tener velocidad decreciente, dado que la atracción gravitacional produce un movimiento uniformemente acelerado solamente en el vacío.

Si un cuerpo comienza a caer atravesando la atmósfera, se va acelerando hasta que su peso es igual a la fuerza de fricción que se produce por el desplazamiento dentro del aire. En ese momento deja de acelerar, y su velocidad comienza a decrecer a medida que la atmósfera aumenta su densidad, provocando una fuerza de fricción mayor.

Puede desacelerar la velocidad de caída no solo por la densidad de la atmósfera sino también por la variación del área de sección atravesada, lo que aumenta la fricción. Los acróbatas aéreos de caída libre pueden variar su velocidad de caída acelerando o desacelerando: si se desplazan de cabeza aceleran hasta equilibrar su peso, y si abren los brazos y piernas desaceleran.

La atmósfera tiene una gran importancia en los ciclos biogeoquímicos. La composición actual de la atmósfera es debida a la actividad de la biosfera (fotosíntesis), controla el clima y el ambiente en el que vivimos y engloba dos de los tres elementos esenciales (nitrógeno y carbono); aparte del oxígeno.

La actividad del hombre está modificando su composición, como el aumento del dióxido de carbono o el metano, causando el efecto invernadero o el óxido de nitrógeno, causando la lluvia ácida.

Las radiaciones solares nocivas, como la ultravioleta, son absorbidas casi en un 90 % por la capa de ozono de la estratosfera. La actividad mutágena de dicha radiación es muy elevada, originado dímeros de timina que inducen la aparición de melanoma en la piel. Sin ese filtro, la vida fuera de la protección del agua no sería posible.

Gracias a la atmósfera, la Tierra no tiene grandes contrastes térmicos; debido al efecto invernadero natural, que está producido por todos los componentes gaseosos del aire, que absorben gran parte de la radiación infrarroja re-emitida por la superficie terrestre; este calor queda retenido en la atmósfera en vez de perderse en el espacio gracias a dos características físicas del aire: su compresibilidad, que comprime el aire en contacto con la superficie terrestre por el propio peso de la atmósfera lo que, a su vez, determina la mayor absorción de calor del aire sometido a mayor presión y la diatermancia, que significa que la atmósfera deja pasar a la radiación solar casi sin calentarse (la absorción directa de calor procedente de los rayos solares es muy escasa), mientras que absorbe gran cantidad del calor oscuro () reenviado por la superficie terrestre y, sobre todo, acuática de nuestro planeta. Este efecto invernadero tiene un papel clave en las suaves temperaturas medias del planeta. Así, teniendo en cuenta la constante solar (calorías que llegan a la superficie de la Tierra por centímetro cuadrado y por minuto), la temperatura media del planeta sería de -27 °C, incompatible con la vida tal y como la conocemos; en cambio, su valor real es de unos 15 °C debido precisamente al efecto invernadero.

La composición de la atmósfera terrestre no ha sido siempre la misma, sino que ha variado a lo largo de la vida del planeta por diversas causas. Además, los elementos ligeros escapan continuamente de la gravedad terrestre; de hecho, en la actualidad se fugan unos tres kilogramos de hidrógeno y 50 gramos de helio cada segundo, cifras que en tiempos geológicos (millones de años) resultan decisivas, aunque compensan, al menos en gran parte, la materia recibida del sol en forma de energía. Esta compensación también tiende a equilibrarse en el tiempo, de acuerdo a la mayor o menor energía solar recibida, generando un ciclo complejo, diario, estacional y de ciclos más largos (de acuerdo con la mayor o menor actividad solar) y una respuesta equivalente de la atmósfera en el almacenamiento de dicha energía y su posterior liberación en el espacio. Por ejemplo, la formación del ozono (O) en la capa denominada precisamente, ozonosfera, absorbe la mayor parte de la radiación ultravioleta recibida del sol pero cede esa energía al volverse a transformar durante la noche en oxígeno (O).

Se pueden establecer diferentes etapas evolutivas de la atmósfera según su composición:

La atmósfera se deriva de diversas fuentes, estaba y está condicionada por factores como:

Antes de la vida, la atmósfera sufrió algunos cambios importantes:

Etapa con la aparición de las primeras bacterias anaeróbicas (que usaban H y HS) y fotosintéticas (Bacterias del azufre y cianobacterias):

Etapa con la aparición de organismos eucariotas con fotosíntesis más eficiente:




</doc>
<doc id="11504" url="https://es.wikipedia.org/wiki?curid=11504" title="Inglés">
Inglés

Inglés puede referirse a:

Además, puede hacer referencia a:


</doc>
<doc id="11505" url="https://es.wikipedia.org/wiki?curid=11505" title="Baisers volés">
Baisers volés

Baisers volés, ("Besos robados" en España; "La hora del amor" en Argentina), es una película francesa de 1968, dirigida por François Truffaut. Protagonizada por Jean-Pierre Léaud y Claude Jade en los papeles principales. 

Es la continuación de la historia del personaje Antoine Doinel, que inició con la película "Los 400 golpes" y el cortometraje "Antoine y Colette" y que sería seguida por "Domicilio conyugal" y "El amor en fuga". En esta película "Antoine" inicia su relación amorosa con "Christine". El personaje "de Antoine siempre" fue interpretado por Jean-Pierre Léaud.

La historia comienza cuando Antoine Doinel, quien de pequeño había huido de su hogar como es descrito en "Los 400 golpes", es licenciado del servicio militar y se reencuentra con su amiga Christine ("Claude Jade"). Ahora en la vida civil, Antoine busca un trabajo para sobrevivir, empezando por el de vigilante nocturno, pero por infortunadas situaciones y por su ineptitud y mala suerte termina siendo despedido. Consigue entonces un trabajo como detective privado y de este modo termina infiltrado en una tienda de zapatos como vendedor, a pedido del dueño de la tienda, quien busca averiguar porqué las personas que lo rodean ( sus empleados y demás) lo detestan. Antoine termina seducido por la esposa del dueño de la tienda, con quien tiene una aventura. La relación de Antoine y Christine se deteriora y terminan separándose, él pierde además su trabajo como detective privado. Antoine ahora trabajando como reparador de electrodomésticos, es enviado a la casa de Christine quien intencionalmente ha descompuesto su televisor para poder ver a Antoine mientras sus padres están fuera de casa.

Finalmente Antoine y Christine terminan reconciliándose y comprometidos. Su relación como pareja se describe en los filmes posteriores.

"Es una de las mejores comedias románticas de todos los tiempos. A partir de allí, sus aventuras estarían siempre vinculadas al amor de pareja, siempre con esa quijotesca hidalguía que lo acompaña aun en la más completa desprotección, a través de Besos robados, que explota la relación con su amiga Christine (Claude Jade); Domicilio conyugal (1971) -que lo encuentra casado con Christine- y El amor en fuga (1979), donde el matrimonio termina. Al menos un par de generaciones de cinéfilos crecieron con estas películas. [...] Besos robados se rodó en meses turbulentos, que incluyeron los sucesos del mayo francés. Truffaut sería criticado por el tono ligero de la película, a pesar de iniciarla con la imagen de la Cinemateca Francesa cerrada, y de dedicarla a su director, Henri Langlois, que había sido echado del cargo por el ministro André Malraux. En la ficción las manifestaciones de esos días aparecen en imágenes televisivas e interesan a Christine, pero no así a Antoine, lo que fue visto por algunos como falta de compromiso por parte del cineasta. Pero Truffaut lo tenía muy claro: Antoine es un joven desprotegido que debe trabajar para vivir, es decir, pertenece a la clase trabajadora, mientras que Christine vive con sus padres y no trabaja. Para la clase obrera, la política es un lujo." () 




</doc>
<doc id="11514" url="https://es.wikipedia.org/wiki?curid=11514" title="Claude Jade">
Claude Jade

Claude Marcelle Jorré ( Dijon, 8 de octubre de 1948 - Boulogne-Billancourt, 1 de diciembre de 2006), conocida como Claude Jade, fue una actriz francesa.

Hija de profesores de inglés, pasó tres años en la Escuela de Arte Dramático. Se trasladó a París y comenzó a actuar en producciones de televisión y en teatro. Fue descubierta por François Truffaut, quien le dio el papel de Christine Darbon, novia de su álter ego Antoine Doinel en "Besos robados". Su debut en la pantalla causó gran sensación y dio a Claude Jade una proyección internacional. Repitió su papel como Christine en "Domicilio conyugal" y " El amor en fuga", también de Truffaut. La crítica estadounidense Pauline Kael considera, no sin razón, que Claude Jade «parece una Catherine Deneuve menos etérea, más práctica». Detectado el éxito de Besos robados, François Truffaut decidió seguir la saga de Antoine e Christine comprometiendo a los dos principales actores, Jean-Pierre Léaud y Claude Jade; además. La escena en "Domicilio conyugal" en que Claude Jade se viste a la japonesa es de una inteligencia que roza la perfección. Su irrupción en la gran pantalla de la mano de Truffaut le abrió la puerta de otros grandes directores, como el británico Alfred Hitchcock, con quien en 1969 trabajó en "Topaz", donde interpretó el personaje de Michèle Picard, hija de un agente secreto y esposa de un periodista.

Más tarde, Edouardo Molinaro le ofreció interpretar en "Mon oncle Benjamin" el papel de la esposa de Jacques Brel, que permitió a la actriz trabajar junto al popular cantante e incrementar así su popularidad.

"Le bateau sur l'herbe", de Gérard Brach, "Home sweet Home", de Benoît Lamy, son otras de su treintena de filmes, una serie que concluyó en 1998 con "Le Radeau de la méduse", de Iradj Azimi.

La televisión le permitió mantener contacto con la interpretación, con trabajos en series policiacas, y donde interpretó su papel más popular como Véronique d'Hergemont, heroína de "L'île aux trente cercueils" ("La isla de los treinta ataúdes") (1979), Suzan Frend en "El gran secreto" (1989) y desde 1998 hasta 2000 como Anna en la serie de la televisión "Cap des pins".
La última actuación de Jade fue en la obra de Rampal "Celimene et le Cardinal", que se exhibió en París y en algunos festivales en los últimos meses.

La actriz Claude Jade, que ya padecía cáncer, tenía un ojo plástico para sus actuaciones, dijo Rampal. Antes de morir, estuvo leyendo un guion para una película de televisión que esperaba filmar en los próximos meses, indicó. Falleció el 1 de diciembre de 2006 a los 58 años de edad por un cáncer en el ojo.





</doc>
<doc id="11515" url="https://es.wikipedia.org/wiki?curid=11515" title="Amor en rebeldía (película)">
Amor en rebeldía (película)

Amor en rebeldía (en francés "Les feux de la chandeleur") es una película francesa de Serge Korber estrenada en 1972.
Con su mujer Marie-Louise (Annie Girardot) y sus dos niños Juan-Paul y Laura, Alexandre Boursault (Jean Rochefort), notario de provincia, habría sido el más feliz de los hombres si su mujer no indicara abiertamente sus opiniones gauchistes y no participara en toda clase de manifestaciones. La vida común que no es ya posible, el 2 de febrero de 1962, día del Chandeleur, Alexandre se separa de su mujer... Pero se encuentra pronto reducida a la inacción, sin marido, sin oficio, sin niños. Diez años después, el 2 de febrero de 1972, sale con un sombrero de paja rojo mientras que nieva a grandes copos. Alexandre Boursault le envía una carta - la primera desde años - para aconsejarle otro peinado. Este encuentro actúa como una centella. En este remolino Laura (Claude Jade) se enamora de Marc Champenois (Bernard Fresson), un amigo de su madre. Marie-Louise encuentra - inspirada por la felicidad de Laura - su oportunidad: convencida de que Alexandre no regresará, ya no vive más que en la esperanza de una reconciliación...


</doc>
<doc id="11517" url="https://es.wikipedia.org/wiki?curid=11517" title="Jean Rochefort">
Jean Rochefort

Jean Rochefort (París, 29 de abril de 1930-Ibídem, 9 de octubre de 2017) fue un actor francés.

Nacido en París de padres bretones, asistió al Instituto Pierre Corneille de Ruan. Con 19 años entró en el Centro de Arte Dramático de la calle Blanche y posteriormente se incorporó al Conservatorio Nacional. Trabajó con la Compañía Grenier Hussenot como actor de teatro desde 1953 y durante siete años.

Tras algunos papeles secundarios interpretó su primer papel importante en el drama "Amor en rebeldía" (1972) con Annie Girardot y Claude Jade. En esta película interpretó a un padre de 41 años de edad con una familia de niños adultos (la joven Claude Jade tenía ya 23). Tenía un bigote, su marca registrada "Moustache", que solo se quitó una vez desde entonces (1992 en "Ridicule"). Ese mismo año también apareció en "El gran rubio con un zapato negro" y en su continuación "La vuelta del gran rubio (con un zapato rojo)" (1974). Aumentó notablemente su popularidad al protagonizar la comedia "Un elefante se equivoca enormemente" (1976).

Falleció el 9 de octubre de 2017 a los 87 años de edad.




</doc>
<doc id="11518" url="https://es.wikipedia.org/wiki?curid=11518" title="Número transfinito">
Número transfinito

En teoría de conjuntos, número transfinito es el término original que el matemático alemán Georg Cantor introdujo para referirse a los ordinales infinitos, que son mayores que cualquier número natural.

En la terminología moderna, al referirse a ordinales o cardinales, «transfinito» e «infinito» son sinónimos.

Al igual que con los números naturales, puede pensarse en los números transfinitos como cardinales u ordinales:

Asumiendo el axioma de elección, todo lo que puede demostrarse con los axiomas de Zermelo-Fraenkel es:

La hipótesis del continuo afirma que de hecho "c" = . Sin embargo, el trabajo de Gödel y Paul Cohen demuestra que la hipótesis es independiente de dichos axiomas: no puede ser refutada o demostrada a partir de ellos. Es decir, usando los axiomas de Zermelo-Fraenkel (ZF) puede comprobarse que los tres cardinales anteriores cumplen formula_1. La hipótesis del continuo afirma que de hecho formula_3. Gödel probó en 1938 que esta hipótesis es consistente con los axiomas ZF, y por tanto puede ser tomado como un axioma nuevo para la teoría de conjuntos. Sin embargo, en 1963 Paul Cohen probó que la negación de la hipótesis del continuo también es consistente con los axiomas ZF, lo cual prueba que dicha hipótesis es totalmente independiente de los axiomas ZF. Es decir, pueden construirse tanto "teorías de conjuntos cantorianas" (en las que la hipótesis del continuo es una afirmación cierta), como "teorías de conjuntos no cantorianas" (en las que la hipótesis del continuo sea falsa). Esta situación es similar a la de las geometrías no euclidianas.

Para los números transfinitos se pueden extender sin ambigüedad la suma, la multiplicación y la potenciación. Sean por ejemplo dos conjuntos disjuntos formula_4 y formula_5, la suma y la multiplicación puede construirse a partir del cardinal de la unión y del producto cartesiano de estos dos conjuntos:

Es sencillo comprobar que estas operaciones están bien definidas ya que:

Aunque la suma y la multiplicación no presentan problemas, la resta y la división no están definidas. A diferencia de lo que sucede con los cardinales finitos no pueden definirse sin ambigüedad operaciones equivalentes a la resta o la división. La resta y la división pueden introducirse entre los cardinales finitos gracias a que a partir del conjunto de los cardinales finitos, que coinciden con los números naturales formula_6, pueden construirse el conjunto de los enteros y de los racionales. La construcción de los enteros y los racionales es posible debido a que todo cardinal finito es regular respecto a la suma, es decir, para cualesquiera cardinales "a, b" y "c" > 0, finitos se cumple:

Esas dos últimas propiedades de hecho no se cumplen nunca cuando uno de los cardinales es transfinito, si formula_7 tenemos las siguientes igualdades:

Los cardinales transfinitos dotados de la suma o la multiplicación constituyen un monoide conmutativo. Debido a la falta de regularidad de los cardinales transfinitos no es aplicable el teorema de simetrización de un monoide que permitiría definir la resta y la división.
La potenciación requiere construir un conjunto más complicado, pero resulta igualmente bien definida. Si "A" y "B" son dos conjuntos cualesquiera y formula_4 y formula_5 se puede definir la exponenciación formula_10 como el cardinal del conjunto de funciones de "B" en "A":

Un caso particular interesante se da cuando "a" = 2, en este caso podemos por ejemplo "A" = {0,1}, y el conjunto A se puede identificar naturalmente con el conjunto de partes de B o conjunto potencia.

La potenciación también tiene propiedades de saturación curiosas, así para cardinales de tipo alef se tiene:
Cantor se percató de que era posible hablar de la cantidad de elementos de un conjunto infinito tal y como se habla de la cantidad de elementos de un conjunto finito. Es decir, encontró que era posible “medir” el tamaño de un conjunto infinito y, de hecho, comparar el tamaño de dos conjuntos infinitos para encontrar que el de uno era “mayor” que el del otro, y elaboró una teoría hasta cierto punto rigurosa respecto de estas ideas: la teoría de números transfinitos.

Cantor argumentaba que el desprecio de los matemáticos por el infinito y su naturaleza se debía a un abuso de este concepto. Lo que Cantor quería decir era que el término infinito se aplicaba sin distinción a cualesquiera conjuntos no finitos, siendo que, de entre ellos, era posible tomar algunos que son, de alguna manera, medibles y de tamaños comparables. Las reflexiones y posterior estudio de Cantor acerca de todo esto comenzaron cuando, intuyendo éste algún resultado no trivial, se preguntó si era posible poner en correspondencia uno a uno el conjunto de los números naturales con el conjunto de los números reales. Pronto pudo Cantor demostrar que no existía tal correspondencia, revelando así una diferencia entre la infinitud de dos conjuntos infinitos, lo que constituyó, en definitiva, un resultado de mucho interés. Cantor probó también que, contrario a lo que pudiera pensarse, el conjunto de los números racionales, que tiene propiedad de densidad, se corresponde uno a uno con el conjunto de los números naturales.

Es fácil dar un ejemplo de dos conjuntos que, uno teniendo todos los elementos del otro y más, se corresponden uno a uno. Tomemos, por ejemplo, a los números naturales:

y tomemos ahora solo aquellos números que son el cuadrado de algún número natural (claramente no todos los números naturales cumplen con esta característica, por lo que se descartan muchos de ellos):

Apenas es necesario explicar más para percatarse de que existe una correspondencia uno a uno entre formula_6 y su subconjunto

Además, Cantor encontró que la medición de un conjunto (ya sea finito o infinito), puede realizarse de dos maneras: una de ellas no considera nada más que la cantidad de elementos de un conjunto, mientras que la otra toma en cuenta el orden de los elementos de un conjunto. De esta distinción surgen los números cardinales y los números ordinales, los cuales pueden ser también transfinitos. Para conjuntos finitos, estos dos conceptos son equivalentes. Sin embargo, los dos conceptos difieren en el momento de aplicarse a conjuntos infinitos.




</doc>
<doc id="11520" url="https://es.wikipedia.org/wiki?curid=11520" title="Topaz (película)">
Topaz (película)

Topaz es una película estadounidense de 1969 del llamado género del suspense, dirigida por Alfred Hitchcock y con Frederick Stafford, Dany Robin, Claude Jade, Michel Subor, Karin Dor, John Vernon, Michel Piccoli, Philippe Noiret y John Forsythe en los papeles principales. 

La película está basada en la novela homónima escrita por Leon Uris y publicada en 1967 por McGraw-Hill.

La trama de la película comienza en 1962, en el ámbito de la Guerra Fría, con un imponente desfile militar del Ejército Rojo en la Plaza Roja. Mientras, en Copenhague, un oficial de los servicios de espionaje soviéticos, Boris Kusenov (Per-Axel Arosenius), con la ayuda de los agentes de la CIA, deserta a Estados Unidos junto con su esposa e hija.

El desertor soviético revela a los agentes de la CIA dos datos fundamentales: Cuba va a recibir misiles de la Unión Soviética y existe dentro de los servicios de inteligencia franceses una organización llamada Topaz, que traspasa información de la OTAN a la Unión Soviética mediante un «"topo"».

El agente estadounidense Nordstrom (John Forsythe), que había sido el interrogador de Kusenov, informa a su colega y amigo 
André Devereaux (Frederick Stafford), del Servicio de Documentación Exterior y Contraespionaje francés, de la existencia de un documento firmado por los gobiernos de Cuba y la Unión Soviética en el que se establecían las bases de las relaciones militares y todo lo concerniente a los misiles soviéticos en Cuba. Dicho documento estaba en poder de Enrique Parra (John Vernon), delegado de Cuba ante la Organización de las Naciones Unidas que estaba en Nueva York para asistir a una asamblea, por lo que le pide a Devereaux que intente hacerse con una copia de dicho documento.

Andre acompañó a su hija Michèle (Claude Jade) y su hijo Francois (Michel Subor) en su viaje de luna de miel a Nueva York. En Nueva York, André contacta a uno de sus agentes, Philippe Dubois ("Roscoe Lee Browne"), que consigue una entrevista con Parra bajo el pretexto de escribir un reportaje para la revista "Ebony". Aprovechando el barullo en la embajada de Cuba (ubicada en Harlem), la venalidad de un secretario y la soberbia de Parra que acepta saludar desde el balcón a las masas, Dubois consigue sacar unas fotos del texto del acuerdo. Pero al ser descubierto, debe darse a la fuga perseguido por los guardias de la embajada. Ya en la calle, finge atropellar a André para pasarle la cámara y desaparece entre la muchedumbre. Un guardaespaldas de Parra llega, pone a André de pie, lo observa de arriba a abajo y finalmente le suelta.

Ahora a los agentes de la CIA solo les faltan fotos de los misiles en Cuba y Nordstrom propone a André ir a Cuba para conseguirlas

André vuela a Cuba como agregado comercial de la embajada francesa, a pesar de la ira y el ansia de su esposa Nicole (Dany Robin) quien sospecha que él tiene una amante en Cuba. En efecto, André tiene una amante, la hermosa morena Juanita de Córdoba (Karin Dor), joven viuda de un héroe de la revolución. Juanita es ahora una espía contrarrevolucionaria, comanda una red anti-castrista e incluso aprovecha sus estrechas relaciones con Parra para conseguir información de primera mano.

Al llegar a casa de Juanita, André encuentra a Parra que se despide y los dos rivales se desafían mutuamente. Luego, en la alcoba y antes de acostarse, el agente francés pide a su amante consegui, si es posible, fotos de los misiles.

Juanita envía a una pareja de sus sirvientes al puerto bajo el pretexto de una comida campestre. Allí sacan fotos de los buques rusos descargando los misiles, pero son descubiertos, detenidos y llevados a la sede de la Dirección General de Inteligencia (DGI). Donde serán torturados, pero no sin antes esconder su cámara en la viga hueca de un puente, donde un peón de Juanita la recoge. La película es rápidamente revelada y las imágenes transferidas a un microfilm que es escondido en la máquina de escribir de André.

Entretanto, André presencia, como empleado de la embajada, una gran arenga de Fidel Castro al pueblo, donde el guardaespaldas de Parra lo reconoce. A su vez, en el sótano de la DGI la criada de Juanita balbucea en la oreja de Parra el nombre de su jefe: Juanita de Córdoba. Inmediatamente se organiza una redada en la casa de Córdoba en la cual se hacen pesquisas y se confirma la traición. Parra toma en sus brazos a Juanita, la estrecha y susurrando le dice que no quiere que los verdugos descuarticen tan hermoso cuerpo. Tras lo cual le dispara una bala en el corazón. Seguidamente llama al aeropuerto para que detengan a André, pero el francés acaba de despegar. Los aduaneros no hallaron nada extraño en su equipaje.

Para su sorpresa, en Washington D. C. no halla a su esposa, Nicole ha vuelto sola a París. Además sus jefes están furiosos, ¿qué hizo en Cuba para que La Habana se quejara de la «actitud inamistosa» del agregado francés? André decide inmediatamente ir tras los pasos de su esposa.

En París halla a su esposa y su hija Michèle, en un intento por reconciliarse. También organiza una reunión para sus amigos y colegas en el restaurante de lujo Chez Pierre, buscando revelar la identidad del topo. El servicio empieza y André declara que hay un espía sentado a la mesa. En el silencio repentino solo Jarré (Philippe Noiret) empieza a comer vorazmente el paté de hígado y a beber grandes tragos de vino. En su estado de nerviosismo, dice que a él le parece que esto es solo un intento de desinformación por parte de los rusos, pues que sabe de fuente fidedigna que Kusenov ha muerto hace dos años. Y se va.

Pero en la tarde, Jarré, muy inquieto, va a ver su jefe en la red Topaz, Granville (Michel Piccoli). Un alto funcionario francés, viejo amigo de André. Granville en vez de calmar a Jarré, le reta. Ha atraído la atención y además le molesta porque está esperando a una visita galante. Al desperdirse se encuentra a Nicole, era ella a quien Granville esperaba. Y mientras se abrazan en el vestíbulo se puede ver colgada al muro una vieja foto en un cuadro, Nicole entre Granville y André, con cazadoras, boinas y fusiles, juveniles y amistosos, tomados durante sus años en la Resistencia francesa, unas dos décadas atrás. 

André, que ahora sospecha de Jarré, le envía a su yerno François (un periodista y caricaturista) para que le hostigue con el pretexto de hacerle una entrevista para un periódico; pero Jarré, acorralado, llama a sus guardaespaldas. André y Michèle, alarmados, llegan poco después al piso y lo hallan muerto, caído de su balcón sin una huella de François. Después, el padre y su hija regresan a su piso, donde Nicole les espera. De repente llega François vacilante, los guardaespaldas le golpearon, pero pudo recobrar el sentido. Les oyó llamar a un número de teléfono y se dio a la fuga. Pero el joven recuerda el número y André averigua que éste es el de su viejo amigo Granville. Nicole ve al retrato de Jarré y entiende que su amante es el jefe de la red Topaz. Sin poder soportarlo, con sus ojos llenos de lágrimas le confiesa todo a su familia.




Hitchcock aparece en el aeropuerto con una mujer que le lleva en silla de ruedas. Hitchcock ve a un conocido, se levanta a saludarle y ambos se retiran caminando.



</doc>
<doc id="11525" url="https://es.wikipedia.org/wiki?curid=11525" title="Ceferino">
Ceferino

Ceferino (Roma, Imperio Romano; ¿?-Ib.; 20 de diciembre de 217) fue el papa n.º 15 de la Iglesia católica de 199 a 217.

Hijo de Abundio, al iniciar su pontificado nombró a Calixto, que sería su sucesor, archidiácono de Roma, cargo equivalente al actual de secretario de estado, y que supuso convertirlo en su principal consejero, lo que dada la escasa formación teológica de Ceferino lo hizo depender totalmente de aquel.

Conocemos mucho más de Ceferino que de cualquier otro pontífice de los primeros años de la Iglesia, aunque esta información está posiblemente sesgada al proceder del teólogo romano Hipólito, quien, en su obra "Philosopheumena", acusaba al papa de adepto al monarquianismo modalístico o modalismo, doctrina que negaba la Trinidad y que posiblemente aceptó Ceferino, influenciado por su consejero Calixto, como respuesta al montanismo. En dicha obra, Hipólito describe a Ceferino como torpe e ignorante, lo cual puede ser cierto, ya que no gozaba de los conocimientos teológicos de su archidiácono y consejero. 

Ceferino estableció que los jóvenes, cumplidos los 14 años, hiciesen la comunión por Pascua y que los cálices no fueran de madera, sino de vidrio. Introdujo, además, el uso de la patena. También excomulgó a Tertuliano.

Ordenó que los sacerdotes y diáconos se ordenasen públicamente en presencia de muchos clérigos y legos para que fuera manifiesta la inocencia y que fueran dotados para estos oficios personas de vida irreprensible.

Durante su pontificado, iniciado bajo el gobierno del emperador Septimio Severo, se reanudaron las persecuciones contra los cristianos, persecuciones que se suavizaron a su muerte y durante el mandato de su sucesor, Caracalla, pero que se reactivaron al ascender, en 217, al trono del imperio Macrino, lo que hace afirmar a ciertas fuentes que Ceferino murió en ese mismo año tras sufrir martirio, aunque, al no existir pruebas documentales de tal afirmación, lo más probable es que falleciera de muerte natural.

Ceferino fue enterrado en una cámara sepulcral suya propia cerca del cementerio de Calixto en la Vía Apia.

La oración propia de las misas que se celebran por San Ceferino es el "Praesta quaesumus", propia de los pontífices mártires.



</doc>
<doc id="11526" url="https://es.wikipedia.org/wiki?curid=11526" title="Espora">
Espora

En biología, el término espora designa un cuerpo microscópico unicelular o pluricelular que se forma con fines de dispersión y supervivencia por largo tiempo (dormancia) en condiciones adversas, y que generalmente es una célula haploide. En muchos seres eucariotas, es parte fundamental de su reproducción, originándose un nuevo organismo al dividirse por mitosis (especialmente en hongos) o meiosis (plantas), sin tener que fusionarse con otra célula, mientras que en algunas bacterias se trata en cambio de una etapa inactiva, resistente a la desecación y con fines de supervivencia no reproductivos. El término deriva del griego σπορά ("sporá"), "semilla".

La espora es un elemento importante en los ciclos vitales biológicos de plantas, hongos, algas y algunos protozoos, los cuales suelen producir las esporas en estructuras denominadas esporangios. En las plantas, las esporas son los gametofitos dentro de su ciclo de vida y permiten al mismo tiempo la dispersión de los propágulos. La mayoría de los hongos producen esporas; aquellos que no lo hacen se denominan hongos asporógenos.

Las esporas se pueden clasificar según su función, estructura, origen del ciclo vital o por su movilidad:

Las diásporas son unidades de dispersión de los hongos, musgos y algunas otras plantas. En hongos, las clamidosporas son esporas multicelulares de pared gruesa resultado de reproducción asexual y las zygosporas son la parte sexual, pues se dividen por meiosis cuando logra condiciones para germinar. Los hipnozigotos de los hongos zigomicetos son producidos por vía sexual y pueden dar lugar a una conidiospora (“zygosporangium”) asexual.

Una meiospora es el producto de la meiosis (la etapa citogenética crítica de la reproducción sexual), lo que significa que es haploide y que dará lugar a una célula o individuo haploide. Esto es característico en los ciclos vitales de plantas y algas.

Una mitospora se produce por un mecanismo de esporulación y se propaga por un medio asexual como resultado de la mitosis. La mayoría de los hongos producen mitoesporas.

La motilidad es la capacidad de moverse autónoma y espontáneamente. Las esporas se dividen según puedan moverse o no. La zoospora puede moverse por medio de uno o más flagelos y se pueden encontrar en algunas algas y hongos. En tanto la autoespora no puede moverse y no tiene el potencial de desarrollar ningún flagelo. Las balistosporas se descargan activamente del cuerpo fructífero (tal como la seta). La estatismospora no se descarga activamente del cuerpo fructífero, como en el pedo de lobo.

Las plantas se caracterizan por tener un ciclo vital con alternancia de generaciones, una generación esporofítica y una gametofítica. Ciertas células de los esporófitos producen esporas haploides por meiosis. Estas esporas se desarrollarán hasta convertirse en gametofitos. Un ejemplo es el gametofito de las plantas vasculares más altas (angiospermas y gimnospermas), que son meiosporas de dos tipos:

En el caso de las plantas vasculares como los helechos, la dispersión anemócora proporciona una gran capacidad de distribución de las esporas. También, las esporas son menos propicias para la depredación animal que las semillas porque no contienen casi ninguna reserva de alimento, pero son más propicias para la depredación por hongos y bacterias. Su principal ventaja es que, de todas las formas de reproducción, las esporas requieren menos energía y materiales para producirse. Las esporas de las plantas vasculares son siempre haploides, pudiendo ser isosporas (homosporas) o heterosporas. Las heterosporas, presentes por ejemplo en las selaginellas, isoetes y algunos helechos acuáticos, producen esporas de dos tamaños: las esporas más grandes (megasporas) producen gametófitos femeninos y las más pequeñas (microsporas) producen gametófitos masculinos. Las esporas pueden ser de dos tipos atendiendo a las marcas de desarrollo: monoletas o triletas. En las monoletas, hay una sola línea elevada en la espora que indica el eje a lo largo del cual la célula madre de las esporas se dividió. En las esporas triletas, las cuatro esporas comparten un origen común, se disponen según un tetraedro y entran en contacto en tres caras separadas por tres carenas que irradian de un punto central (en Y).

Esporas de gran diversidad y otros propágulos juegan un rol central en la mayoría de algas multicelulares, afectando su ecología. Los fósiles más antiguos son esporangios de algas rojas de hace 1200 millones de años. Se presentan esporas en algas verdes, rodofitas, heterocontofitas y cloraracniofitas.

En los hongos y pseudohongos, son a menudo clasificados por las estructuras productoras de esporas. Estas esporas suelen tener características propias de un taxón en particular. Principales tipos de esporas:

Las oosporas forman parte del ciclo sexual de los oomicetes.

Las esporas bacterianas son características de ciertas bacterias, que por lo general desarrollan una sola espora por cada célula. En este caso la formación de esporas no es un tipo de reproducción definitiva; estas células pueden resistir la destrucción en un medio hostil o desfavorable. Son diversas bacterias terrestres, especialmente Gram positivas, las que pueden inducirse al estado de espora mediante un mecanismo llamado esporulación, logrando así resistencia contra la desecación, trituración, escasez de nutrientes, frío, calor, radiación (UV, X, γ), sal, oxidantes, desinfectantes, pH extremo, etc., debido a su cubierta dura e impermeable. Es un estado inactivo o latente en el que no crece y no hay reproducción, pues de una bacteria se produce una sola espora. Su activación en condiciones favorables se denomina germinación. Hay 3 tipos de esporas bacterianas:


</doc>
<doc id="11528" url="https://es.wikipedia.org/wiki?curid=11528" title="Errores de tipo I y de tipo II">
Errores de tipo I y de tipo II

En un estudio de investigación, el error de tipo I, también denominado error de tipo alfa (α) o falso positivo, es el error que se comete cuando el investigador rechaza la hipótesis nula (formula_1: el supuesto inicial) siendo esta verdadera en la población. Es equivalente a encontrar un resultado falso positivo, porque el investigador llega a la conclusión de que existe una diferencia entre las hipótesis cuando en realidad no existe. Se relaciona con el nivel de significancia estadística.

La hipótesis de la que se parte formula_1 aquí es el supuesto de que la situación experimental presentaría un «estado normal». Si no se advierte este «estado normal», aunque en realidad existe, se trata de un error estadístico tipo I. Algunos ejemplos para el error tipo I serían:

En un estudio de investigación, el error de tipo II, también llamado error de tipo beta (β) (β es la probabilidad de que exista este error) o falso negativo, se comete cuando el investigador no rechaza la hipótesis nula siendo esta falsa en la población. Es equivalente a la probabilidad de un resultado falso negativo, ya que el investigador llega a la conclusión de que ha sido incapaz de encontrar una diferencia que existe en la realidad.

De forma general y dependiendo de cada caso, se suele aceptar en un estudio que el valor del error beta esté entre el 5 y el 20%.

Contrariamente al error tipo I, en la mayoría de los casos no es posible calcular la probabilidad del error tipo II. La razón de esto se encuentra en la manera en que se formulan las hipótesis en una prueba estadística. Mientras que la hipótesis nula representa siempre una afirmación enérgica (como por ejemplo formula_3 «"Promedio μ = 0"») la hipótesis alternativa, debido a que engloba todas las otras posibilidades, es generalmente de naturaleza global (por ejemplo formula_4 «"Promedio μ ≠ 0"» ). El gráfico de la derecha ilustra la probabilidad del error tipo II (rojo) en dependencia del promedio μ desconocido.

El poder o potencia del estudio representa la probabilidad de observar en la muestra una determinada diferencia o efecto, si existe en la población. Es el complementario del error de tipo II (1 − β).

Una vez realizado el contraste de hipótesis, se habrá optado por una de las dos hipótesis, la hipótesis nula o base formula_5 o la hipótesis alternativa formula_6, y la decisión escogida coincidirá o no con la que en realidad es cierta. Se pueden dar los cuatro casos que se exponen en el siguiente cuadro:

Si la probabilidad de cometer un error de tipo I está unívocamente determinada, su valor se suele denotar por la letra griega α, y en las mismas condiciones, se denota por β la probabilidad de cometer el error de tipo II, esto es:

En este caso, se denomina Potencia del contraste al valor 1-β, esto es, a la probabilidad de escoger formula_6 cuando esta es cierta

Cuando es necesario diseñar un contraste de hipótesis, sería deseable hacerlo de tal manera que las probabilidades de ambos tipos de error fueran tan pequeñas como fuera posible. Sin embargo, con una muestra de tamaño prefijado, disminuir la probabilidad del error de tipo I, α, conduce a incrementar la probabilidad del error de tipo II, β.

Usualmente, se diseñan los contrastes de tal manera que la probabilidad α sea el 5% (0,05), aunque a veces se usan el 10% (0,1) o 1% (0,01) para adoptar condiciones más relajadas o más estrictas. El recurso para aumentar la potencia del contraste, esto es, disminuir β, probabilidad de error de tipo II, es aumentar el tamaño muestral, lo que en la práctica conlleva un incremento de los costes del estudio que se quiere realizar.




</doc>
<doc id="11533" url="https://es.wikipedia.org/wiki?curid=11533" title="Folíolo">
Folíolo

En botánica, se llama pinna o folíolo a cada una de las piezas separadas en que a veces se encuentra dividido el limbo de una hoja.
Cuando el limbo foliar está formado por un solo folíolo, es decir no está dividido, se dice que la hoja es una hoja simple.
Cuando el limbo foliar está dividido en folíolos se dice que la hoja es hoja compuesta.

Según el número de folíolos o pinnas de una hoja compuesta podemos diferenciar:

Una hoja pinnada puede tener a su vez las pinnas divididas en pínnulas, estas hojas frecuentes, por ejemplo en muchos helechos, se denominan bipinnadas.



</doc>
<doc id="11534" url="https://es.wikipedia.org/wiki?curid=11534" title="Epicureísmo">
Epicureísmo

El epicureísmo es un movimiento que abarca la búsqueda de una vida feliz mediante la búsqueda inteligente de placeres, la ataraxia (ausencia de turbación) y las amistades entre sus correligionarios. Fue enseñada por Epicuro de Samos, filósofo griego del siglo IV a. C. (341 a. C.), el cual fundó una escuela llamada Jardín y cuyas ideas fueron seguidas por otros filósofos, llamados "".

El gusto, para el epicureísmo, no debía conformarse al cuerpo, como preconizaba el hedonismo cirenaico, sino que debía ser también intelectual. Además, para Epicuro la presencia de placer o felicidad era un sinónimo de la ausencia de dolor o de cualquier tipo de aflicción: el hambre, la tensión sexual, el aburrimiento, etc. Era un equilibrio perfecto entre la mente y el cuerpo que proporcionaba la serenidad o ataraxia. 

La física epicúrea sostenía que todo el universo consistía en dos cosas: materia y vacío. La materia está formada por átomos, que son cuerpos diminutos que solo tienen las cualidades inmutables de la forma, el tamaño y el peso. Se consideró que los átomos no cambiaban porque los epicúreos creían que el mundo estaba ordenado y que los cambios debían tener fuentes específicas y consistentes. Epicuro sostiene que debe haber un suministro infinito de átomos, aunque solo un número finito de tipos de átomos, así como una cantidad infinita de vacío. Epicuro explica esta posición en su carta a Heródoto: 

Debido al suministro infinito de átomos, hay una cantidad infinita de mundos, o "cosmoi". Algunos de estos mundos podrían ser muy diferentes a los nuestros, otros bastante similares, y todos los mundos estaban separados unos de otros por vastas áreas de vacío ("metakosmia").

El epicureísmo afirma que los átomos no se pueden dividir en partes más pequeñas, y los epicúreos ofrecieron múltiples argumentos para apoyar esta posición. Los epicúreos argumentan que debido a que el vacío es necesario para que la materia se mueva, cualquier cosa que consista tanto en el vacío como en la materia puede descomponerse, mientras que si algo no contiene ningún vacío, entonces no hay forma de separarse porque ninguna parte de la sustancia podría estar desglosado en una subsección más pequeña de la sustancia. También argumentaron que para que el universo persista, lo que está compuesto en última instancia no debe poder cambiarse o, de lo contrario, el universo se destruiría esencialmente.

Los átomos se mueven constantemente de una de cuatro maneras diferentes. Los átomos pueden simplemente chocar entre sí y luego rebotar entre sí. Cuando se unen entre sí y forman un objeto más grande, los átomos pueden vibrar a medida que se mantienen uno al otro mientras se mantiene la forma general del objeto más grande. Cuando no son prevenidos por otros átomos, todos los átomos se mueven a la misma velocidad naturalmente hacia abajo en relación con el resto del mundo. Este movimiento hacia abajo es natural para los átomos; sin embargo, como cuarto medio de movimiento, los átomos a veces pueden desviarse aleatoriamente de su camino descendente habitual. Este movimiento de desvío es lo que permitió la creación del universo, ya que a medida que más y más átomos se desviaban y chocaban entre sí, los objetos podían tomar forma a medida que los átomos se unían. Sin el viraje, los átomos nunca habrían interactuado entre sí, y simplemente habrían continuado moviéndose hacia abajo a la misma velocidad.

Epicuro también intuyó que el viraje era lo que explicaba el libre albedrío de la humanidad. Si no fuera por el viraje, los humanos estarían sujetos a una cadena interminable de causa y efecto. Este fue un punto que los epicúreos utilizaron a menudo para criticar a Demócrito.

Los epicúreos creían que los sentidos también dependían de los átomos. Cada objeto emitía continuamente partículas de sí mismo que luego interactuaban con el observador. Todas las sensaciones, como la vista, el olfato o el sonido, se basaban en estas partículas. Si bien los átomos que se emitían no tenían las cualidades que los sentidos percibían, la manera en que se emitían provocaba que el observador experimentara esas sensaciones; por ejemplo, las partículas rojas no eran en sí mismas rojas sino que se emitían de una manera tal que hacían que el espectador experimentara el color rojo. Los átomos no se perciben individualmente, sino más bien como una sensación continua debido a la rapidez con que se mueven.

Al contrario que sus contemporáneos, creía que el universo era ilimitado con un número ilimitado de átomos y una cantidad infinita de vacío. La Tierra no era el centro del cosmos y se cree que sostuvo la forma de la Tierra como plana como hizo Demócrito. También sostuvo que los cuerpos celestes eran tan pequeños como se observaban. El epicúreo Lucrecio () cuestionó la Tierra esférica al encontrar absurda la idea de animales andando boca abajo en las antípodas.

Según Adolfo Sánchez Vázquez, «el epicúreo alcanza el bien, retirado de la vida social, sin caer en el temor a lo sobrenatural, encontrando en sí mismo, o rodeado de un pequeño círculo de amigos, la tranquilidad de ánimo y la autosuficiencia». 

Epicuro afirmó que es bueno todo lo que produce placer, pues el placer, según él, es el principio y el fin de una vida feliz. Pero para que el placer sea real debe ser moderado, controlado y racional.

Él definió el placer como la satisfacción de las necesidades del cuerpo y la tranquilidad del alma. Para el epicureísmo, lo malo es todo aquello que le produce dolor al ser humano. Son las cosas que nos hacen o nos afectan en el sentido espiritual y corporal, Epicuro señaló que el placer no tiene que ser necesariamente un placer sexual, sino también algo que nos produzca placer el hacerlo, como: respirar tranquilamente en el campo o tomar helado, pero sobre todo los placeres espirituales como la música de calidad o un buen libro también.

El ser humano está compuesto de cuerpo y alma, y los placeres del alma son superiores a los del cuerpo. En su opinión, la paz interior puede alcanzarse al reducir las necesidades del cuerpo y acabar con las inquietudes y temores. La ética epicúrea dice que para vivir una vida feliz, es muy importante superar el miedo a la muerte; Epicuro dijo "La muerte no nos concierne, pues mientras existimos, la muerte no está presente y cuando llega la muerte, nosotros ya no existimos"

Para Epicuro, los placeres y sufrimientos son consecuencia de la realización o impedimento de los "apetitos". Epicuro distingue entre tres clases de apetitos, por tanto placeres:

Es importante aclarar que Epicuro no era dualista, es decir, no postulaba la oposición cuerpo-alma; el alma, igual que el cuerpo, es material y está compuesta de átomos.
También distinguía entre dos tipos de placeres, basados en la división del hombre entre dos diferentes pero unidos, el cuerpo y el alma:

Los epicúreos dividieron aún más cada uno de estos tipos de placeres en dos categorías: "placer cinético" o móvil y "placer catastemático" o estable.


Pese a que el placer es un bien y el dolor un mal, hay que administrar inteligentemente el placer y el dolor: en ocasiones debemos rechazar placeres a los que les siguen sufrimientos mayores y aceptar dolores cuando se siguen de placeres mayores. La razón representa un papel decisivo en lo que respecta a nuestra felicidad, nos permite alcanzar la total imperturbabilidad (ataraxia), la cual compara Epicuro «con un mar en calma», cuando ningún viento lo azota y nos da libertad ante las pasiones.

La finalidad de la filosofía de Epicuro no era teórica, sino más bien práctica, que buscaba sobre todo procurar el sosiego necesario para una vida feliz y placentera en la que los temores al destino, los dioses o la muerte quedaran definitivamente eliminados.

Para ello se fundamentaba en una teoría empirista del conocimiento, en una física atomista inspirada en las doctrinas de Leucipo y Demócrito y en una ética hedonista.

El "Tetrafármaco", o "La cura en cuatro partes", es una guía básica de Filodemo de Gadara de cómo vivir la vida más feliz posible, sobre la base de las primeras cuatro "Doctrinas Principales" de Epicuro. Esta doctrina poética fue transmitida por un epicúreo anónimo que resumió la filosofía de Epicuro sobre la felicidad en cuatro líneas simples:"No temas a los dioses;"

"no te preocupes por la muerte;" 

"Lo que es bueno es fácil de obtener, y" 

"lo que es terrible es fácil de soportar."

La filosofía epicúrea emplea una epistemología empírica. Los epicúreos creían que todas las percepciones sensoriales eran verdaderas, y que surgen errores en cómo juzgamos esas percepciones. Cuando formamos juicios sobre las cosas ("hupolepsis"), pueden verificarse y corregirse a través de más información sensorial. Por ejemplo, si alguien ve una torre desde lejos que parece ser redonda, y al acercarse a la torre ven que en realidad es cuadrada, se daría cuenta de que su juicio inicial era erróneo, y puede corregir su error.

Epicuro propuso tres criterios de verdad : sensaciones ( "aisthêsis" ), preconcepciones ( "prolepsis" ) y sentimientos ( "pathê" ). Se dice que un cuarto criterio llamado "aplicaciones de presentación de la mente" ( "phantastikai epibolai tês dianoias" ) fue agregado por epicúreos posteriores. Estos criterios formaron el método a través del cual los epicúreos pensaron que obtuvimos conocimiento.

Dado que los epicúreos pensaban que las sensaciones no podían engañar, las sensaciones son el primer y principal criterio de verdad para los epicúreos. Incluso en los casos en que la información sensorial parece inducir a error, la información en sí misma es verdadera y el error surge de nuestros juicios sobre la información. Por ejemplo, cuando uno coloca un remo recto en el agua, parece doblado. El epicúreo argumentaría que la imagen del remo, es decir, los átomos que viajan desde el remo a los ojos del observador, se han desplazado y, por lo tanto, realmente llegan a los ojos del observador en forma de remo doblado. El observador comete el error al suponer que la imagen que recibe correctamente representa el remo y no ha sido distorsionada de alguna manera. Para no hacer juicios erróneos sobre cosas perceptibles y, en cambio, verificar el propio juicio, los epicúreos creían que era necesario obtener una "visión clara" ( "enargeia" ) de lo perceptible mediante un examen más detallado.  Esto actuó como una justificación de los juicios sobre lo que se percibe. La "Enargeia" se caracteriza como la sensación de un objeto que no ha sido modificado por juicios u opiniones y es una percepción clara y directa de ese objeto.

Las ideas preconcebidas de un individuo son sus conceptos de lo que son las cosas, por ejemplo, cuál es la idea de un caballo de alguien, y estos conceptos se forman en la mente de una persona a través del aporte sensorial a lo largo del tiempo. Cuando se usa la palabra que se relaciona con la preconcepción, la mente convoca estas ideas preconcebidas en los pensamientos de la persona. Es a través de nuestras ideas preconcebidas que podemos hacer juicios sobre las cosas que percibimos.  ideas preconcebidas también fueron utilizadas por los epicúreos para evitar la paradoja propuesta por Platón en el "Menón" sobre el aprendizaje. Platón argumenta que el aprendizaje requiere que ya tengamos conocimiento de lo que estamos aprendiendo, de lo contrario no podríamos reconocer cuándo habíamos aprendido con éxito la información, ideas preconcebidas, argumentan los epicúreos, proporcionan a las personas ese conocimiento previo requerido para el aprendizaje.

Nuestros sentimientos o emociones ( "pathê" ) son cómo percibimos el placer y el dolor.  Son análogos a las sensaciones en que son un medio de percepción, pero perciben nuestro estado interno en oposición a las cosas externas. Según Diógenes Laercio, los sentimientos son cómo determinamos nuestras acciones. Si algo es placentero, buscamos esa cosa, y si algo es doloroso, lo evitamos.

La idea de "aplicaciones de presentación de la mente" es una explicación de cómo podemos discutir y preguntar sobre cosas que no podemos percibir directamente. Recibimos impresiones de tales cosas directamente en nuestras mentes, en lugar de percibirlas a través de otros sentidos. El concepto de "aplicaciones de presentación de la mente" puede haberse introducido para explicar cómo aprendemos sobre cosas que no podemos percibir directamente, como los dioses.

El epicureísmo no niega la existencia de los dioses; más bien niega su participación en el mundo. Según el epicureísmo, los dioses no interfieren con las vidas humanas ni con el resto del universo de ninguna manera. La manera en que existen los dioses epicúreos todavía se discute.

Algunos eruditos dicen que el epicureísmo cree que los dioses existen fuera de la mente como objetos materiales (la posición realista), mientras que otros afirman que los dioses solo existen en nuestras mentes como ideales (la posición idealista).

El epicureísmo también ofreció argumentos contra la existencia de los dioses en la forma propuesta por otros sistemas de creencias. La "Paradoja de Epicuro", o el "Problema del mal", es un famoso argumento en contra de la existencia de un Dios o dioses todopoderosos y providenciales. Según Lactancio:Este tipo de argumento de "trilema" (Dios es omnipotente, Dios es bueno, pero el mal existe) fue favorecido por los antiguos escépticos griegos, y este argumento puede haber sido erróneamente atribuido a Epicuro por Lactancio, quien, desde su perspectiva cristiana, consideró a Epicuro como un ateo.

Las ideas epicúreas sobre política están en desacuerdo con las tradiciones filosóficas estoicas, platónicas y aristotélicas. Según Emilio Lledó, Epicuro entendía la política como expansión de la felicidad, justicia, sabiduría, belleza...

Para los epicúreos, todas nuestras relaciones sociales son una cuestión de cómo nos percibimos mutuamente, de las costumbres y tradiciones. Nadie es inherentemente de mayor valor o está destinado a dominar a otro. Esto se debe a que no hay una base metafísica para la superioridad de un tipo de persona, todas las personas están hechas del mismo material atómico y, por lo tanto, son naturalmente iguales. Los epicúreos también desalientan la participación política y otra participación en la política. Sin embargo, los epicúreos no son apolíticos, es posible que alguna asociación política pueda ser vista como beneficiosa por algunos epicúreos. Algunas asociaciones políticas podrían conducir a ciertos beneficios para el individuo que ayudarían a maximizar el placer y evitar la angustia física o mental.

Si bien esta evasión o libertad podría lograrse por medios políticos, Epicuro insistió en que la participación en la política no lo liberaría del miedo y él aconsejó no llevar una vida política. En cambio, Epicuro alentó la formación de una comunidad de amigos fuera del estado político tradicional. Esta comunidad de amigos virtuosos se enfocaría en asuntos internos y justicia. Se cree que la opinión acerca del matrimonio en Epicuro es positiva y consideró las relaciones sexuales como naturales pero innecesarias.

Sin embargo, el epicureísmo es adaptable a las circunstancias, como lo es el enfoque epicúreo de la política. Los mismos enfoques no siempre funcionarán en la protección contra el dolor y el miedo. En algunas situaciones será más beneficioso tener una familia y en otras situaciones será más beneficioso participar en política. En última instancia, depende de los epicúreos analizar sus circunstancias y tomar las medidas que correspondan a la situación.

Epicuro también fue uno de los primeros pensadores en desarrollar la noción de justicia como un contrato social. Él definió la justicia como un acuerdo hecho por la gente para no dañarse unos a otros. El punto de vivir en una sociedad con leyes y castigos es protegerse del daño para que uno sea libre de perseguir la felicidad. Debido a esto, las leyes que no contribuyen a promover la felicidad humana no son justas. Dio su propia versión única de la ética de la reciprocidad, que difiere de otras formulaciones al enfatizar en minimizar el daño y maximizar la felicidad para uno mismo y para los demás:"Es imposible vivir una vida placentera sin vivir sabiamente, bien y justamente, y es imposible vivir sabiamente, bien y justamente sin vivir una vida placentera".
El epicureísmo incorporó un relato relativamente completo de la teoría del contrato social y, en parte, intenta abordar los problemas con la sociedad descrita en la "República" de Platón. La teoría del contrato social establecida por el epicureísmo se basa en un acuerdo mutuo, no en un decreto divino.

Diógenes Laercio fue un biógrafo, que vivió en el último siglo del epicureísmo (siglo III), en su obra "Vida y opiniones de los filósofos ilustres", dedica el libro 10 y último en textos epicúreos. Según Diógenes, Epicuro dejó más de 300 manuscritos, incluyendo 37 tratados sobre física y numerosas obras sobre el amor, la justicia, los dioses y otros temas, según refiere Diógenes Laercio en el siglo III. Lo que queda de la filosofía epicúrea está disponible a través de estas fuentes:


El epicureísmo es una doctrina de un paganismo típicamente laico y mediterráneo, y en este ámbito ganó gran número de seguidores que la consideraron una doctrina verdadera que solucionaba todos los problemas. Fue conocida por toda Grecia e Italia, y hasta llegó a Asia y Egipto, a pesar de estar siempre bajo la sombra del por entonces predominante estoicismo. 

Entre los seguidores de las enseñanzas de Epicuro en la Antigua Roma figuran los poetas Horacio, cuya famosa declaración "Carpe Diem" ("aprovecha el día") ilustra su filosofía, Virgilio. Lucrecio, el epicúreo romano más importante, quien vivió durante el siglo I a.C. , escribió una obra, "Sobre la naturaleza" ("De rerum natura"), en siete libros, que representa sin duda el texto más importante del epicureísmo fuera de Epicuro. Los temas básicos tratados por Lucrecio son la constitución atómica del universo, una teoría sobre la sensación empírica, la pasión amorosa, una alabanza de la persona y la obra de Epicuro, los fenómenos de la astronomía epicúrea, y de otros. Sin embargo, en contra de la creencia popular, Lucrecio no copia textualmente a Epicuro, sino que se diferencia en algunos aspectos, por ejemplo en la doctrina de la felicidad, pues Lucrecio elabora una teoría pesimista y dramática de la vida.

Su escuela de pensamiento perduró largamente durante siete siglos tras la muerte de Epicuro; sin embargo, en el siglo IV (según el testimonio de San Agustín) han desaparecido totalmente las escuelas epicúreas y los escritos de Epicuro permanecen dispersos por el mundo antiguo, o bien en algunos fragmentos de obras de escritores como Séneca, Plutarco, etcétera. Fue olvidado al advenir la Edad Media, periodo en el que se perdió o fue destruida la mayoría de los escritos de este filósofo griego a causa del rechazo que por sus ideas experimentó el Cristianismo, que no pudo adaptarlas a su sistema de creencias por la visión cristiana del dolor. 

Sin embargo, a través de autores del humanismo (como Cosimo Raimondi) y renacentistas (como Pierre Gassendi) el epicureísmo se da a conocer por toda Europa. Baruch Spinoza y John Locke, por ejemplo, reconocen la importancia (tanto desde el punto de vista histórico como por una cierta influencia en los mismos escritos) de Epicuro y Lucrecio. Incluso se encuentran resonancias (o sólo menciones) del epicureísmo en autores ya más contemporáneos como Jeremy Bentham, John Stuart Mill, Auguste Comte, Friedrich Hegel, Marx y Nietzsche.








</doc>
<doc id="11537" url="https://es.wikipedia.org/wiki?curid=11537" title="Función continua">
Función continua

En matemáticas, una función continua es aquella para la cual, intuitivamente, para puntos cercanos del dominio se producen pequeñas variaciones en los valores de la función; aunque en rigor, en un espacio métrico como en variable real, significa lo contrario, que pequeñas variaciones de la función implican que deben estar cercanos los puntos. Si la función no es continua, se dice que es discontinua. Informalmente, una función continua de ℝ en ℝ es aquella cuya gráfica puede dibujarse sin levantar el lápiz del papel (más formalmente su grafo es un conjunto conexo).

La continuidad de funciones es uno de los conceptos básicos del análisis matemático y de la topología general. El artículo describe principalmente la continuidad de funciones reales de una variable real.

Informalmente hablando, una función f definida sobre un intervalo I es continua si la curva que la representa, es decir el conjunto de los puntos (x, f(x)), con x en I, está constituida por un trazo continuo, es decir un trazo que no está roto, ni tiene "hoyos" ni "saltos", como en la figura de la derecha.

El intervalo I de x es el dominio de definición de f, definido como el conjunto de los valores de x para los cuales f(x) existe.

El intervalo J de y es el rango (también conocido como imagen) de f, el conjunto de los valores de y, tomados como y = f(x). Se escribe J = f(I). Notar que en general, no es igual que el codominio (sólo si la función en cuestión es suprayectiva.)

El mayor elemento de J se llama el máximo absoluto de f en I, y el menor valor de J es su mínimo absoluto en el dominio I.

Definición de continuidad en un punto
si:

tal que para toda x

Esto se puede escribir en términos de límites de la siguiente manera; si "x" es punto del dominio de la función que es punto de acumulación del mismo, entonces "f" es continua en "x" si y sólo si formula_3.Cuando "x" es un punto del dominio que no es de acumulación del mismo, es decir, es punto aislado del dominio, se cumple trivialmente la definición, luego toda función es continua en los puntos aislados de su dominio. Por ejemplo, las sucesiones de números reales son un caso de función real de variable real cuyo dominio es el conjunto de los números naturales. Como todos los puntos del dominio de una sucesión son puntos aislados del mismo, se concluye que toda sucesión es una función continua. Por otro lado, no tiene sentido hablar de si una función es o no continua en un punto que no pertenezca al dominio de la misma. Por ejemplo, a función f(x)=1/x es continua en todos los puntos de su dominio. En cero, como no está en el dominio, no podemos hablar ni de si es continua ni de si no lo es. Lo que sí podemos afirmar en este caso es que la función 1/x no puede extenderse con continuidad a cero, pero no podemos afirmar ni que 1/x es continua en 0 ni que 1/x no es continua en 0 (y, por tanto, ni que 1/x es discontinua en 0).

OBSERVACIÓN:<br>
En el caso de aplicaciones de formula_4 en formula_4, es común ver que se dice que una función formula_6 es continua en un punto x si existe f (x), si existe el límite de f (x) cuando x tiende hacia x por la derecha, si existe el límite de f (x) cuando x tiende hacia x por la izquierda, y además ambos coinciden con f (x). Esto implicaría que, dada una función, si no está definida en un punto, ésta no es continua en él, llegando a una situación como la siguiente: La función formula_7 definida como formula_8 no es continua en 0 porque no está definida en dicho punto, pero tampoco es continua en 3 ni en 5. Esta definición, no satisfactoria, de continuidad está muy extendida, pero hay que recordar el requisito indispensable para poder hablar de continuidad de que el punto en el que se estudia la continuidad pertenezca al dominio. Si no está en el dominio, pero es punto de acumulación del mismo, podemos hablar de si puede o no extenderse con continuidad a dicho punto, pero no podemos decir que la función es discontinua en dicho punto (la función extendida sí podría ser discontinua, puesto que al incorporar dicho punto al dominio, tiene sentido plantearse el estudio de la continuidad en él).

Así pues, una función f continua en un punto de su dominio x que, además, es punto de acumulación del mismo, implica lo siguiente:

1. existe el límite por la derecha:

2. existe el límite por la izquierda:

3. El límite por la derecha, el límite por la izquierda coinciden:

4. Si existen el límite por la derecha y por la izquierda y sus valores coinciden, la función tiene límite en este punto:

5. Existe f(x):

6. El límite y el valor de la función coinciden:
Se dice que una función es continua en un intervalo si es continua en todos sus puntos.

Si f(x)= y, la continuidad en x se expresa así:

parafraseando, cuando x se aproxima a x, f(x) se aproxima a y. Por definición de los límites, esto significa que para todo intervalo abierto J, centrado en y, existe un intervalo abierto I, centrado en x, tal que formula_17.

Si f no es continua en un punto, el teorema cae en falta. En efecto no todo intervalo I alrededor de x tiene su imagen en un intervalo J centrado en y, con un radio inferior al salto de f, no importa lo pequeño que este intervalo sea, hay valores de x del intervalo I alrededor de x que tiene su imagen en un intervalo K centrado en y, siendo y y y valores distintos, esto es: x tiene imágenes que se salen de J.

La ventaja de esta definición es que se puede generalizar a cualquier espacio topológico.

Una función formula_18 es continua por la izquierda en el punto formula_19 si el límite lateral por la izquierda y el valor de la función en el punto son iguales. Es decir:

como en la figura.

Una función formula_18 es continua por la derecha en el punto formula_19 si su límite lateral por la derecha y el valor de la función en el punto son iguales. Es decir:

Una función formula_18 es continua en un punto si es continua por la izquierda y es continua por la derecha. Esto es:
Un valor c, pertenece a un intervalo abierto I, de extremo izquierdo a y extremo derecho b, representado I= (a,b) si:

Una función, f es continua en un intervalo abierto I= (a,b), si y solo si la función es continua en todos los puntos del intervalo, es decir:

Un valor c, pertenece a un intervalo cerrado I, de extremo izquierdo a y extremo derecho b, representado I= [a,b] si:

Una función f es continua en un intervalo cerrado [a, b] si la función es continua en el intervalo abierto (a,b) y es continua por la derecha de a y continua por la izquierda de b:
Las funciones polinomiales, trigonométricas: seno y coseno, las exponenciales y los logaritmos son continuas en sus respectivos dominios de definición.

La parábola, como función polinómica, es un ejemplo de función continua a lo largo de todo el dominio real.

En la gráfica se ve la función seno que es periódica, acotada y continua en todo el domino real, dado su carácter periódico, con ver uno solo de los ciclos es suficiente para comprobar la continuidad, porque el resto de los ciclos son exactamente iguales.

Las funciones definidas para distintos intervalos de x, pueden ser discontinuas en los puntos de cambio de intervalo, como por ejemplo:


Su gráfica es una sucesión de segmentos horizontales a distintas alturas. Esta función no es continua en los enteros, pues los límites a la izquierda y a la derecha son diferentes, pero es continua en los segmentos abiertos (n, n+1) donde es constante.

Las funciones racionales son continuas en un intervalo adecuado. Un ejemplo de esto es la función inverso de x:

Esta función es una hipérbola compuesta por dos tramos. x < 0 y x > 0. Como se puede ver, es continua en todo el dominio formula_31 porque no está definida en x= 0. Si se extiende el dominio de la función a R (dándole un valor arbitrario a f(0) la función será discontinua.

Estos son algunos de los teoremas más importantes sobre funciones continuas.


Las funciones derivables son continuas. Si una función es derivable en x=a entonces es continua en x=a. De modo que la continuidad es una condición necesaria para la derivabilidad.

Es importante notar que lo recíproco no es válido; es decir que nada se puede afirmar sobre la derivabilidad de una función continua. Un ejemplo claro de esta situación es la función valor absoluto f(x)= |x| que si bien es continua en todo su dominio no es derivable en x= 0. Incluso hay funciones continuas en todo formula_44 pero no derivables en ningún punto (las funciones del movimiento browniano verifican esto con probabilidad 1).

Una función formula_45, se dice que: 
Cualquier función polinómica de una variable es una función de clase formula_56. La función generalizada denomiada delta de Dirac es una función de clase formula_57 ya que es la derivada segunda de la función rampa que es continua, y la derivada primera de la función escalón de Heaviside que es de clase formula_58

Se puede dar ejemplos que muestran que hay funciones de clase formula_48
pero no lo son de clase formula_60. Los ejemplos clásicos son formula_61.

Sean formula_62 e formula_63 dos espacios topológicos. Una aplicación formula_64 se dice que es continua si:

Esta definición se reduce a la definición ordinaria de continuidad de una función formula_69 si sobre formula_70 y formula_71 se considera la topología inducida por la distancia euclídea.

Con la misma notación anterior, si formula_72, diremos que formula_6 es continua en formula_74 cuando se obtiene que formula_75 es un entorno de formula_74, cualquiera que sea el entorno formula_77 de formula_78.

Es posible entonces comprobar que formula_6 es continua si y solo si es continua en formula_80, cualquiera que sea este, es decir, cuando sea continua en cada uno de los puntos de su dominio.

El término función continua en la parte de la teoría de conjuntos que se refiere a los números ordinales tiene un sentido diferente al referido a las funciones sobre espacios topológicos. Concretamente una función "F" definida sobre la clase de los números ordinales formula_81 es continua si para cada ordinal límite se cumple la siguiente propiedad:



</doc>
<doc id="11538" url="https://es.wikipedia.org/wiki?curid=11538" title="Rioja (vino)">
Rioja (vino)

Rioja es una Denominación de origen calificada (DOCa) de España con la que se distinguen a ciertos vinos elaborados en áreas de las comunidades autónomas de La Rioja y el País Vasco mayoritariamente y, en menor medida, en Navarra y Castilla y León, (menos de 2,5 km²). Por la diversidad orográfica y climática se distinguen tres subzonas de producción —Rioja Alta, Rioja Oriental (Rioja Baja) y Rioja Alavesa— donde se originan vinos de distintas características. La producción media anual de vino de Rioja es de 336 millones de litros (85% vino tinto y 15% vino blanco y rosado). Los vinos de Rioja son frescos, aromáticos, de composición equilibrada y excelente buqué.

El vino de la DOCa Rioja se identifica por sus contraetiquetas y precintos numerados. Es la denominación de origen más elegida por los consumidores tanto dentro como fuera de casa, con un 36% y un 27%, respectivamente, de las consumiciones.


Las variedades tradicionales autorizadas por el Consejo Regulador de la D.O.Ca. Rioja desde su creación en 1925 han sido siete, cuatro tintas y tres blancas:

Las variedades preferentes son la Tempranillo en tinto y la Viura en blanco.

En el año 2007, el Consejo Regulador de la D.O.Ca. Rioja autorizó, por primera vez desde 1925, la incorporación de nueve variedades nuevas dentro de los límites de la denominación.
, cambios que se reflejaron en dos modificaciones del Reglamento existente aprobado en 2004: "BOE-A-2008-4991" y "BOE-A-2009-8950". Son las siguientes:


Estas nuevas variedades autorizadas solo se pueden plantar en sustitución de arranques, para no incrementar la masa vegetal de la Denominación.

En el caso de las nuevas variedades autóctonas, tanto tintas como blancas, no se establece un límite en el porcentaje que deben llevar los vinos, por lo que se permite la elaboración de vinos monovarietales de estas uvas. Por el contrario, en las variedades blancas foráneas (Chardonnay, Sauvignon blanc y Verdejo) se establece que no podrán ser las predominantes en la composición final del vino. Por tanto, si se indican las variedades en la etiqueta, deberá figurar siempre en primer lugar la variedad blanca autóctona (Viura, Garnacha blanca, Malvasía de Rioja, Maturana blanca, Tempranillo blanco o Turruntés).

La incorporación de estas nuevas variedades se hizo con la finalidad de recuperar el patrimonio vitícola riojano -en el caso de las uvas autóctonas- y para incrementar la competitividad de los blancos de Rioja en el mercado internacional -en el caso de las variedades blancas foráneas.

Además de estas uvas, el Consejo Regulador autoriza en ocasiones la incorporación de otras variedades bajo la denominación de ""experimentales"" siempre y cuando no sea la variedad predominante y no se especifique su nombre en la etiqueta o simplemente se nombren como ""otras variedades"". El caso más conocido es el de la Bodega Herederos del Marqués de Riscal a la que se le permite incluir la uva Cabernet sauvignon en algunos de su "coupages" (""Gran Reserva"" y ""Barón de Chirel"") ya que la utiliza desde su fundación en 1858, muchos años antes de la formación del propio Consejo Regulador. No obstante, a pesar de los más de 150 años transcurridos, legalmente se sigue considerando una variedad "experimental". Otras muchas bodegas utilizan esta casta de uva como variedad "experimental" como: Bodegas Marqués de Murrieta (""Dalmau""), Bodegas Alicia Rojas (""Colección Privada""), Bodegas y Viñedos Marqués de Vargas, Martínez Bujanda (""Finca Valpiedra ""), Bodegas Paternina, Solarce, Barón de Ley(""Finca Monasterio""), Izadi o la La Rioja Alta (""Barón de Oña""). Otras variedades tintas cultivadas como "experimentales" son Merlot y Sirah (Bodegas Enartis, Bodegas Bagordi, Bodegas Campo Viejo, Hacienda de Susar, etc.). En cuanto a las variedades blancas, algunas han dejado de ser "experimentales" como la Chardonnay y la Sauvignon blanc y otras lo siguen siendo. Por ejemplo, el vino ""Remelluri Blanco"" de Granja Ntra. Sra. de Remelluri contiene además de Chardonnay y Sauvignon blanc, Viognier, Roussane, Marsanne y Moscatel.

La superficie cultivada en 2018, en hectáreas, según las variedades de uva y por Comunidades Autónomas es la siguiente:

Como puede verse, las uvas tintas representan el 90,85% y las blancas el 9,15%.

En cuanto a variedades, las uvas tintas se distribuyen de las siguiente manera: Tempranillo: 87,67%, Garnacha: 7,58%, Mazuelo: 2,07%, Graciano: 2,15%, Maturana tinta: 0,30% y otras: 0,23%.

El porcentaje entre las blancas queda determinado así: Viura: 69,17%, Malvasía: 2,22%, Garnacha blanca: 3,61%, Tempranillo blanco: 12,48%, Maturana blanca: 0,58%, Verdejo: 5,44%, Turruntés: 0,08%, Chardonnay: 2,49%, Sauvignon Blanc: 3,31% y otras: 0,61%.

La poda consiste en formar la cepa con tres brazos y dos pulgares en cada brazo. Cada pulgar tendrá dos yemas de las que brotarán los sarmientos. La vendimia se hace manualmente en el mes de octubre. Para garantizar la calidad se restringe la cantidad a 6500 kg/ha para variedades tintas y 9000 kg/ha para blancas.

Tradicionalmente el vino de Rioja se ha elaborado ensamblando diferentes tipos de variedades de uva, aunque hoy en día también es muy común encontrar vinos monovarietales.

De acuerdo con el Reglamento de la D.O.Ca. Rioja (BOE-A-2004-18384) y su última modificación (BOE-A-2009-8950), los diferentes tipos de vino deben emplear las variedades autorizadas en las siguientes proporciones:







La elaboración se realiza en barricas de roble de 225 litros durante un tiempo que oscila entre 1 y 3 años, y posteriormente en la propia botella durante un período de 6 meses a 6 años.

Dependiendo del tiempo que el vino permanece en barrica se clasifica como:


Las calificaciones de las cosechas de la D.O.Ca. Rioja otorgadas por su Consejo Regulador desde su fundación en 1926 son las siguientes:



Los términos municipales englobados en la D.O.Ca. Rioja, desglosados por subzonas y provincias son los siguientes:


El Reglamento actual de la D.O.Ca. Rioja viene recogida en la Orden Ministerial:

Este Reglamento ha sido modificado posteriormente por las siguientes Órdenes Ministeriales:

Otro documento importante es


La Universidad de La Rioja (UR) imparte el Grado en Enología en la Facultad de Ciencias, Estudios Agroalimentarios e Informática desde 1996 y es el segundo campus español que creó esta titulación oficial de segundo ciclo.

Los alumnos de la UR completan su formación teórica con un "practicum" de tres meses en alguna de las bodegas de la DOCa Rioja y, gracias a las instalaciones del Complejo Científico Tecnológico -dotadas con Bodega Experimental y Sala de Análisis Sensorial-, completan el ciclo de la vid, desde la vendimia hasta el embotellado de sus propios vinos.

Además, la Universidad de La Rioja imparte el programa de Doctorado en Enología, el Máster en Turismo Enológico, el Máster en Viticultura, Enología y Dirección de Empresas y varios cursos de Especialista Universitario.

El vino impregna múltiples facetas de la vida en La Rioja y dispone de variadas manifestaciones culturales, como el ciclo de conciertos en bodega "Catarsis", el programa "El Rioja y los 5 Sentidos", el Museo Vivanco de la Cultura del Vino en Briones, la popular "Batalla del Vino de Haro" o la "Batalla del clarete" de San Asensio, las Fiestas de la Vendimia en Logroño y, organizado de forma independiente, el Festival Mariquitina's Day.




</doc>
<doc id="11545" url="https://es.wikipedia.org/wiki?curid=11545" title="Resta">
Resta

La resta o la sustracción es una operación de aritmética que se representa con el signo (–); representa la operación de eliminación de objetos de una colección. Por ejemplo, en la imagen de la derecha hay 5-2 manzanas—significando 5 manzanas con 2 quitadas, con lo cual hay un total de 3 manzanas. Por lo tanto, 5 – 2 = 3. Además de contar frutas, la sustracción también puede representar combinación de otras magnitudes físicas y abstractas usando diferentes tipos de objetos: números negativos, fracciones, números irracionales, vectores, decimales, funciones, matrices y más.

La sustracción sigue varios patrones importantes. Es anticonmutativa, lo que significa que el cambio del orden cambia el signo de la respuesta. No es asociativa, lo que significa que cuando se restan más de dos números, importa el orden en el que se realiza la resta. Restar 0 no cambia un número. La sustracción también obedece a reglas predecibles relativas a las operaciones relacionadas, tales como la adición y la multiplicación. Todas estas reglas pueden probarse a partir de la sustracción de números enteros y generalizarlas mediante los números reales y más allá. Las operaciones binarias generales que siguen estos patrones se estudian en el álgebra abstracta.

Realizar sustracciones es una de las tareas numéricas más simples. La sustracción de números muy pequeños es accesible para los niños pequeños. En la educación primaria, a los estudiantes se les enseña a restar números en el sistema decimal, comenzando con un solo dígito y progresivamente abordando problemas más difíciles. Las ayudas mecánicas van desde el antiguo ábaco a la computadora moderna.

Imagine un segmento de recta de longitud "b" con el extremo izquierdo etiquetado "a" y el extremo derecho etiquetado "c". Partiendo de "a", se toma "b" posiciones a la derecha para llegar a c. Este movimiento hacia la derecha se modela matemáticamente mediante la adición:

De "c", se toman "b" posiciones a la izquierda para volver a "a". Este movimiento a la izquierda se modela por sustracción:

Ahora, un segmento de la línea marcada con los números 1, 2 y 3. Desde la posición 3, no se toma ningún paso hacia la izquierda para permanecer en el 3, por lo que 3 − 0 = 3. Se necesitan 2 pasos a la izquierda para llegar a la posición 1, por lo que 3 − 2 = 1. Esta imagen es inadecuada para describir lo que sucedería después de pasar 3 pasos a la izquierda de la posición 3. Para representar dicha operación, la línea debe extenderse.

Para restar números naturales arbitrarios, uno comienza con una línea que contiene cada número natural (0, 1, 2, 3, 4, 5, 6, ...). Del 3, se toman 3 pasos a la izquierda para llegar a 0, por lo que 3 - 3 = 0. Pero 3 − 4 todavía es inválido, puesto que una vez más sale de la línea. Los números naturales no son un contexto útil para la resta.

La solución es considerar la línea numérica entera (..., −3, −2, −1, 0, 1, 2, 3, ...). Del 3, se toman 4 pasos a la izquierda para llegar a −1:

Hay algunos casos donde la resta como una operación separada se vuelve problemática. Por ejemplo, 3 - (-2) (es decir, restar -2 de 3) no es inmediatamente obvia desde un punto de vista del número natural o una vista de línea de números, porque no está claro de inmediato lo que significa mover -2 pasos a la izquierda o quitar -2 manzanas. Una solución es ver la resta como la suma de números con signo. Un signo menos extra simplemente denota inversión aditiva. Entonces tenemos 3 - (-2) = 3 + 2 = 5. Esto también ayuda a mantener el anillo de los enteros "simple" al evitar la introducción de "nuevos" operadores como la resta. Por lo general un anillo solo tiene dos operaciones definidas en el mismo; en el caso de los números enteros, éstos son la suma y la multiplicación. Un anillo ya tiene el concepto de inversiones aditivas, pero no tiene ninguna noción de una operación de sustracción separada, así que el uso de la suma como la resta firmada nos permite aplicar los axiomas de anillo para la resta sin necesidad de demostrar nada.

Hay varios algoritmos para la resta, y difieren en su idoneidad para diversas aplicaciones. Para el cálculo a mano, se adaptan un número de métodos; por ejemplo, al hacer el cambio, no se realiza la resta real, sino que más bien sigue subiendo el cambio de cuentas.

Para cálculo en máquina, se prefiere el método de complementos, por lo que la resta se sustituye por una adición en una aritmética modular.

Los métodos utilizados para enseñar la resta para la escuela primaria varían de país en país, y dentro de un país, están de moda diferentes métodos en diferentes momentos.
Algunas escuelas europeas emplean un método de sustracción llamado método austriaco, también conocido como el método de adiciones. En este método no hay préstamo. En cambio, existen muletas (marcas para ayudar a la memoria), que varían de acuerdo con el país.

Este método separa la sustracción como un proceso de sustracciones de un dígito por valor de posición. A partir de un dígito menos significativo, una sustracción de sustraendo:

desde el minuendo
donde cada "s" y "m" es un dígito, procediendo a escribir
abajo "m" − "s", "m" − "s", y así sucesivamente, siempre y cuando "s" no exceda "m". En caso contrario, "m" se incrementa en 10 y algunos otros dígitos se modifica para corregir de este aumento. El método americano lo corrige intentando disminuir el dígito minuendo "m" por uno (o continuar el préstamo hacia la izquierda hasta que no sea un dígito distinto de cero desde el que presta). El método europeo corrige incrementado el dígito sustraendo "s" por uno.

Ejemplo: 704 − 512. 

El minuendo es 704, el sustraendo es 512. Los dígitos del minuendo son "m" = 7, "m" = 0
y "m" = 4. Los dígitos sustraendo son "s" = 5, "s" = 1 y "s" = 2. Comenzando en el lugar de las unidades, 4 es no menos de 2 por lo que se escribe 2 la diferencia en el lugar del resultado. En el lugar de las decenas, 0 es menor que 1, por lo que el 0 se incrementa en 10, y la diferencia con 1, que es 9, se escribe en lugar de las decenas. El método americano corrige el aumento de diez reduciendo el dígito en el lugar de la centena del minuendo en uno. Es decir, el 7 está tachado y se sustituye por un 6. Entonces, la resta procede en el lugar de las centenas, donde 6 no es inferior a 5, lo que la diferencia se reduce en el lugar del resultado de cien. Ahora hemos terminado, el resultado es 192.

El método austriaco no reduce la 7 a 6. Más bien aumenta el dígito de las centenas del sustraendo en uno. Se hace una pequeña marca cerca o por debajo de esta cifra (dependiendo de la escuela). A continuación, la restas procede por preguntar qué número cuando aumenta en 1, y 5, se añade a la misma, hace 7. La respuesta es 1, y se anota el resultado en el lugar de las centenas.

Hay una sutileza adicional en que el estudiante siempre emplea una tabla de sustracción mental en el método americano. Muchas veces, el método austriaco alienta al estudiante a usar mentalmente la tabla de sumar a la inversa. En el ejemplo anterior, en lugar de la adición de 1 a 5, consiguiendo 6, y resta este desde el 7, el estudiante se le pide que considere qué número, cuando aumenta en 1, y 5, se añade al mismo, haciendo 7.

Ejemplo:

Ejemplo:

En este método, cada dígito del sustraendo se sustrae del dígito por encima de él comenzando de derecha a izquierda. Si el número superior es demasiado pequeño para restar el número inferior del mismo, se le suma 10 al mismo; este 10 es 'prestado' desde el dígito superior hacia la izquierda, lo que se resta 1. Luego se pasa a restar el siguiente dígito y el préstamo como sea necesario, hasta que se haya restado cada dígito.
Ejemplo:
Una variante del método americano, donde todos los préstamos se realizan antes de que toda resta.

Ejemplo:

El método de las diferencias parciales se diferencia de otros métodos de sustracción verticales porque ningún préstamo o o acarreo se realiza. En su lugar, se usan unos lugares más o signos de menos en función de si el minuendo es mayor o menor que el sustraendo. La suma de las diferencias parciales es la diferencia total.

Ejemplo:

En lugar de encontrar diferencia dígito por dígito, puede contar los números entre el sustraendo y el minuendo.

Ejemplo:

1234 − 567 = puede ser encontrada en los siguientes pasos:
Se suma el valor de cada paso para obtener la diferencia total: 3 + 30 + 400 + 234 = 667.

Otro método que es útil para el cálculo mental es dividir la resta en pequeños pasos.

Ejemplo:

1234 − 567 = puede ser resuelta de la siguiente manera:

El mismo método de cambio se basa en el hecho de que sumar o restar el mismo número del minuendo y sustraendo no cambia la respuesta. Se añade la cantidad necesaria para obtener ceros en el sustraendo.

Ejemplo:

«1234 − 567 =» puede ser resuelta de la siguiente manera:

Al restar dos números con unidades de medida, tales como kilogramos o libras, deben tener la misma unidad. En la mayoría de casos, la diferencia tendrá la misma unidad que los números originales.

Una excepción es cuando se restan dos números con porcentaje como unidad. En este caso, la diferencia tendrá puntos porcentuales como unidad; la diferencia es que los porcentajes deben ser positivos, mientras que los puntos porcentuales pueden ser negativos.




</doc>
<doc id="11548" url="https://es.wikipedia.org/wiki?curid=11548" title="Tegucigalpa">
Tegucigalpa

Tegucigalpa, oficialmente Tegucigalpa, Municipio del Distrito Central y abreviado como , es la capital y sede de gobierno de la República de Honduras, junto a su ciudad gemela Comayagüela, según los artículos 8 y 295 de la actual Constitución de Honduras. Es una de las .

Aunque ya desde 1536 se le conocía al pequeño poblado a las orillas de la cuenca del río Choluteca (hoy en día el Centro Histórico) por el peculiar nombre de "Taguzgalpa", es con la llegada de los españoles a la región en busca de minerales que se reconoce el 29 de septiembre de 1578 como el día que marca su fundación bajo el nombre de "Real de Minas de San Miguel de Tegucigalpa". Tres siglos después, el 30 de octubre de 1880 se convierte en la capital del país, durante la presidencia de Marco Aurelio Soto.

Durante la corta existencia de la Constitución Política de la República Federal de Centro América, entre 1824 y 1839, Tegucigalpa fue declarada un distrito federal y capital de los entonces unidos en una sola nación: los estados de El Salvador, Guatemala y Honduras. Después de este fallido intento de preservar una república centroamericana, Honduras regresa a ser un país individual e independiente y el 30 de enero de 1937, se reforma el Artículo 179 de la Constitución de Honduras de 1936 bajo el Decreto N.º 53 y se establece a Tegucigalpa y Comayagüela como el Distrito Central. El 9 de diciembre del mismo año se ratifica bajo el Decreto N.º 2.

El Distrito Central se encuentra en la región montañosa sur central de Honduras en el departamento de Francisco Morazán, del cual es también la cabecera departamental. El área metropolitana de Tegucigalpa y Comayagüela se encuentra en un valle, rodeado por montañas y ambas, siendo ciudades gemelas, están geológicamente separadas por la cuenca del río Choluteca que les atraviesa. El Distrito Central es el municipio más grande y más poblado de Francisco Morazán y el decimocuarto más grande de Honduras. Tegucigalpa y Comayagüela, juntas, es la ciudad más grande y más poblada de Honduras.

La capital es el centro político y administrativo del país donde se ubican 23 embajadas y 16 consulados representando diplomática y consularmente a 39 países de alrededor del mundo. Es la sede de la mayoría de las agencias públicas y empresas estatales, entre ellas, la ENEE y Hondutel, las compañías nacionales de energía y telecomunicaciones, respectivamente. Es también el hogar de la selección nacional de fútbol y del plantel principal y rectoría de la Universidad Nacional Autónoma de Honduras (UNAH), la máxima casa de estudios del país. El aeropuerto internacional, Toncontín, ha adquirido fama e infamia mundial por su pista de aterrizaje extremadamente corta para un aeropuerto de categoría internacional lo cual obliga a los aviadores a emprender maniobras algo irregulares durante el despegue y aterrizaje para evadir las montañas aledañas.

La Alcaldía Municipal del Distrito Central (AMDC) es la autoridad gubernamental de la ciudad y municipio, encabezada por un alcalde y 10 regidores quienes forman la Corporación Municipal, él órgano ejecutivo-legislativo del municipio. Siendo cabecera departamental, es sede del gobernador político departamental de Francisco Morazán. Para 2013, la Alcaldía aprobó un presupuesto de más de tres mil millones de lempiras (US$153.5 millones), y acumuló una deuda arriba de los mil millones de lempiras (US$50 millones), en parte para financiar los proyectos de infraestructura que está emprendiendo la presente administración municipal.

La infraestructura capitalina no se ha mantenido al ritmo de su explosión demográfica. La falta de planificación adecuada, la urbanización densa y desordenada sumados con fenómenos socioeconómicos como la delincuencia, son azotes de la vida cotidiana. Las principales vías de circulación son el escenario de embotellamientos ya que la presente red vial no se da abasto con los más de 400 mil vehículos que circulan por ellas diariamente. Tanto el gobierno nacional como el municipal han desarrollado proyectos para incrementar la infraestructura.

La mayoría de las fuentes sugieren que el origen y significado del vocablo "Tegucigalpa" deriva de la lengua náhuatl. Su significado exacto está abierto a diferentes interpretaciones, pero la versión más difundida entre la creencia popular es que deriva del vocablo nahua "Taguz-galpa" en el cual significa "cerros de plata". Entre historiadores como el hondureño Jesús Aguilar Paz, dicha interpretación es incierta ya que los pobladores aborígenas ignoraban la presencia de yacimientos minerales en la región. Se ha perdido en la historia quién o cuándo se determinó lo de "cerros de plata", una teoría es que fueron los españoles, y no los nativos, quienes le llamaron así a la región tras el descubrimiento de sus riquezas minerales.

El polígrafo mexicano Antonio Peñafiel, en su libro "Nomenclatura geográfica de México" (1897), define el vocablo "Tegucigalpa" como una corrupción de "Tecutli-cal-pa" que significa "[señor] en los palacios reales". Otra creencia entre historiadores mexicanos como José Ignacio Dávila Garibi y Alfredo Barrera Vásquez, contempla que "Tegucigalpa" es del vocablo nahua "Tecuztlicallipan" que se traduce como "lugar de residencia de los nobles", o posiblemente del vocablo "Tecuhtzincalpan" que significa "lugar sobre la casa del amado señor" o "cerro de los sabios".

El filólogo hondureño Alberto de Jesús Membreño, en su libro "Nombres Geográficos Indígenas de la República de Honduras" (1901) (republicado en 1994 como “Toponimias indígenas de Centroamérica”), descarta por completo el tradicional "cerros de plata" y argumenta que "Tegucigalpa" deriva del vocablo nahua "Teguycegalpa" que significa "en las casas de las piedras puntiagudas". Membreño defiende su interpretacíón haciendo apunte de que "Taguzgalpa" era el nombre antiguo de la zona oriental de Honduras y cuyo vocablo significa "en las casas de la tierra amarilla".

El lingüista americanista austríaco, Rodolfo R. Schuller, propone que el vocablo "Tegucigalpa" significa "lugar donde está la casa de la aurora", mientras que el investigador guatemalteco Flavio Rodas Noriega, promovió una discusión sobre el origen etimológico de "Tegucigalpa" y propuso que el término deriva de "Totogalpa", lo que es una referencia a "Tototi", vocablo nahua que significa "pájaro" y/o a "toncontín" que es otro término nahua cuyo significado es "baile de los indios mexicanos". Por otra parte, el escritor hondureño Rafael Heliodoro Valle, escribió que el nombre es "Teguiazkalpa", cuya etimología significa "la región de los cerros de los venerables ancianos".

La autora e historiadora hondureña Leticia de Oyuela, en su libro “Historia Mínima de Tegucigalpa” (1989), contempla la idea de que la palabra "Tegucigalpa" deriva de otra lengua en el cual significa "piedras pintadas". La antropóloga hondureña Gloria Lara Pinto, en su colaboración titulada "Dicotomía de una ciudad: Las raíces indígenas de Tegucigalpa y Comayagüela" (2011) para la Revista Paradigma de la Universidad Pedagógica Nacional Francisco Morazán, propone que es una derivación de "Teguzigalpa" o "Tecuzincalpan" y significa "en la tierra del pequeño señor". El historiador hondureño Mario Felipe Martínez Castillo, en su libro de bolsillo "Lecturas de la capital de Honduras" (2012), hace hincapié de que "Tegucigalpa" no puede significar "cerros de plata" por las razones ya indagadas por previos investigadores y por lo cual sugiere que viene de la lengua lenca y significa "lugar donde se reúnen los señores".

En sus inicios fue poblada por un grupo de españoles que buscaban vetas de plata en el lugar cerca de 1560, posteriormente con el crecimiento del poblado minero se le conoció con el nombre de "Real Villa de San Miguel de Tegucigalpa de Heredia" el 29 de septiembre de 1578 sobre un antiguo poblado indígena existente. En esa época el área de Tegucigalpa era un centro de actividad minera donde se extraía especialmente plata y oro.

El primer alcalde de Tegucigalpa, Juan de la Cueva, nunca se imaginó que aquella encantadora y pintoresca ciudad, se convertiría años más tarde en la zona más importante de Honduras, funcionando las principales oficinas del Estado y sector privado.
Fue fundada en 1578, como centro minero, y de la Cueva fue nombrado alcalde en 1579. La población fue denominada "Real Minas de Tegucigalpa", obteniendo el título de Villa de San Miguel de Heredia.
Durante todo el período colonial la villa tuvo un carácter minero, extrayéndose minerales desde el cerro El Picacho y en la zona montañosa de San Juancito. Al devenir la independencia del país la capitalidad de la República de Honduras pasó de Tegucigalpa a Comayagua y viceversa en varias ocasiones, hasta que en 1880 quedó definitivamente establecida en Tegucigalpa.

En 1817 se inicia, por iniciativa del alcalde Narciso Mallol, la construcción de un puente sobre el río Choluteca, de mampostería en siete arcos. La obra, terminada cuatro años más tarde, unió a Tegucigalpa con la vecina ciudad de Comayagüela, sobre la margen opuesta del río. Hoy se la conoce, popularmente, como el "Puente Mallol".

En 1821 fue elevada al rango de ciudad. En 1824, el primer congreso de la República de Honduras decretó que Tegucigalpa y Comayagua, las dos ciudades principales del país, se alternaran como capital del Estado, hasta que el 30 de octubre de 1880 se trasladó la sede del Gobierno definitivamente a la ciudad de Tegucigalpa como capital del Estado, se decretó que residirán en ella autoridades civiles, excepto la Corte Suprema de Justicia que residirá en Comayagua, trasladando de inmediato las oficinas dependientes del Gobierno Supremo.

En el año 1847 se funda la primera universidad en el país, con el nombre de "La Sociedad del Genio Emprendedor y del Buen Gusto", siendo su primer rector el sacerdote José Trinidad Reyes.

En 1875 durante la presidencia del Capitán general José María Medina se ordenó la construcción de un nuevo Cementerio General de Tegucigalpa ubicándolo en Comayagüela y terminado en 1877, inaugurado durante la presidencia de Marco Aurelio Soto, en marzo de 1995 el Congreso Nacional decretó al Cementerio General como "Patrimonio Cultural Nacional" dejándose de vender nuevos lotes y ordenándose su protección y cuidado. 

Desde 1898 se dispuso que Tegucigalpa y Comayagüela, las dos ciudades vecinas, a ambas orillas del Choluteca, formasen la capital, pero manteniendo nombres separados, con dos gobiernos municipales. Contaban, en aquel entonces, con unos 40.000 habitantes y el Distrito Central, incluyendo poblaciones circunvecinas, reunía más de 50.000 almas.

Tegucigalpa ha sido la cuna de ilustres patriotas y estadistas hondureños, como Dionisio de Herrera, Francisco Morazán, José Trinidad Cabañas, José Trinidad Reyes, General José Santos Guardiola, Doctor Marco Aurelio Soto, entre otros.

Tegucigalpa ha crecido en los últimos 50 años, pero se ha convertido en una ciudad desorganizada debido a la falta de planificación de la misma. La migración del campo hacia la ciudad ha venido a incrementar la población capitalina, especialmente en los terrenos aledaños ubicados en las laderas de los numerosos cerros, muchos de ellos, carentes de urbanización.

La ciudad de Tegucigalpa, se encuentra en constante crecimiento. En estos momentos los polos de desarrollo residencial apuntan hacia el sur de la ciudad, desde el aeropuerto Toncontín hasta la zona de la represa Los Laureles y como ciudades dormitorios, tenemos en la zona noreste, a los municipios de Santa Lucía y Valle de Ángeles.

El 30 de octubre de 1998 la ciudad sufrió daños importantes tras el paso del huracán Mitch, que destruyó una parte de Comayagüela y los lugares bordeados por el río Grande o Choluteca. El huracán se mantuvo sobre el territorio hondureño por cinco días lo que ocasionó que la tierra no pudiera absorber tanta lluvia y aunado a la deforestación, provocara graves inundaciones en todo el país, principalmente en Tegucigalpa.

La crecida de los afluentes del río Grande o Choluteca hicieron que este rebasará la altura del puente Juan Ramón Molina, el cual fue arrastrado por la corriente y sustituido rápidamente por un puente Bailey.

Las lluvias también provocaron deslizamientos de tierra en el sector del cerro El Berrinche. Estos deslizamientos arrastraron la mayor parte de la colonia Soto, cuyos escombros cayeron sobre la cuenca del río haciendo que se formara un dique a la altura de dicha colonia, este dique estancó las aguas del río Grande o Choluteca y ocasionó la inundación en las partes bajas de Comayagüela destruyendo los viejos establecimientos ubicados a inmediaciones de la Calle Real. En otros sectores la corriente derrumbó colinas, cerros y laderas de montañas, llevándose consigo barrios enteros, edificios, parques, automóviles, etc. Las áreas mayormente afectadas fueron las situadas cerca de ríos.

El centro histórico de Tegucigalpa conserva algunos interesantes exponentes de la arquitectura colonial que datan de los siglos XVIII, XIX, los edificios religiosos más importantes son:



Entre algunas edificaciones de principios del XX, tenemos:


Según los resultados del Censo de Población y de Vivienda del 2001 del Instituto Nacional de Estadísticas (INE), la población del Distrito Central, que comprende las ciudades de Tegucigalpa y Comayagüela, era de 906.129 habitantes. En 2010 la población del Distrito Central contaba 1.126.534 habitantes de acuerdo con las proyecciones del INE.

Tegucigalpa se encuentra en una cadena de montañas a alturas de 935 metros (3.068 pies) en sus puntos más bajos y 1.463 metros (4.800 pies) en su nivel más alto en áreas suburbanas. Al igual que la mayoría de las tierras altas del interior de Honduras, la mayoría de la superficie actual de Tegucigalpa fue ocupada por bosques abiertos. El área que rodea la ciudad sigue siendo el apoyo a los bosques abiertos bosque de pinos mezclados con algunos de roble, matorral y claros de hierba, así como la aguja de hoja perenne de hojas y bosques caducifolios de hoja ancha.

Tegucigalpa limita al norte con los municipios de Cedros y Talanga, al sur con los municipios de Maraita, San Buenaventura, Santa Ana y Lepaterique, al este con los municipios de Santa Lucía, San Antonio de Oriente, Valle de Ángeles y San Juan de Flores y al oeste con los municipios de Ojojona, Lepaterique, Lamaní y San Antonio de Flores.

Tegucigalpa, junto con Comayagüela, constituyen la capital de Honduras. El río Choluteca, que cruza la ciudad de norte a sur, separa físicamente de Tegucigalpa y Comayagüela, mientras Tegucigalpa se encuentra a la margen derecha del río Grande o Choluteca, Comayagüela está en el sector occidental de la ciudad y próxima al aeropuerto. Ambas ciudades se localizan en el municipio del Distrito Central, sede constitucional del Gobierno de la República de Honduras y de la Arquidiócesis de Tegucigalpa.
La ciudad se compone de suaves colinas, y el anillo de montañas que rodean la ciudad tiende a atrapar la contaminación.
Hay una reserva conocida como "Embalse Los Laureles", al oeste de la ciudad ofreciendo un 30 por ciento del suministro de agua de la ciudad, así como una planta de tratamiento de aguas al sur de la ciudad alrededor de 7,3 kilómetros (4,5 millas) del aeropuerto, parte del embalse de la Concepción a solo 6 km (3,7 millas) al suroeste de la planta de agua.

Tegucigalpa es la ciudad que cuenta el mayor Índice de desarrollo humano de Honduras, el cual es de 0.859, muy superior al de San Pedro Sula (0.720), aunque se mantiene menor que el resto de capitales centroamericanas superando únicamente a Managua, Nicaragua.

La ciudad tiene un clima tropical de sabana (clasificación climática de Köppen: "Aw"), sin embargo cuenta con temperaturas más amenas por causa de la altitud, menos húmedo que los valles más bajos y las regiones costeras. Cuenta con dos temporadas, la temporada seca y fría que comienza en noviembre y finaliza en marzo y la temporada lluviosa y cálida que inicia en abril y finaliza en octubre.

El promedio de horas de sol por mes durante el año es 211,2 y el promedio de días lluviosos por mes es de 8,9. El promedio de horas de sol durante la estación seca es de 228 por mes, mientras que 182,5 milímetros (7,19 pulgadas) es el promedio de precipitación mensual durante la estación húmeda. Los meses más lluviosos de la temporada de lluvias es de mayo-junio y septiembre-octubre, con un promedio 16.2 días de lluvia durante esos períodos.
En Tegucigalpa, la temporada de lluvia es húmeda y nublada, la temporada seca es mayormente despejada y es caliente durante todo el año. Durante el transcurso del año, la temperatura generalmente varía de 15 °C a 30 °C y rara vez baja a menos de 12 °C o sube a más de 31 °C.

La contaminación es uno de los principales problemas que afectan a la ciudad, desde ya algunas décadas. Durante algunos meses del año la ciudad se ve cubierta por una extensa capa de smog, el cual puede ser producido debido a la quema de desechos tóxicos o bosques cercanos a la ciudad, así como el humo expulsado por la gran cantidad de vehículos que circulan por la metrópoli. Aunque también puede ser causada por la combustión de carbón, madera o biomasa.

En mayo de 2014, un estudio reveló que Tegucigalpa es la ciudad con el aire más contaminado en Centroamérica, y una de las capitales más contaminadas de Latinoamérica (junto a Ciudad de México, Lima, Buenos Aires, Bogotá, Santiago de Chile y Ciudad de Guatemala entre otras). La contaminación urbana en Tegucigalpa provoca también un declive económico, ya que anualmente el país sufre pérdidas de hasta mil millones de lempiras.

Como capital de Honduras, como cabecera departamental y como municipio, el Distrito Central es la sede de tres gobiernos: el gobierno nacional, el gobierno departamental y el gobierno municipal. Antes de 1991, el gobierno central ejercía mayor jurisdicción sobre la ejecución del manejo municipal en todo el país, lo cual causaba una representación admistrativa dispareja y una distribución inadecuada de recursos y de gobernabilidad. Como resultado, a finales de 1990, bajo el Decreto No. 134-90, el Congreso Nacional de Honduras aprobó la Ley de Municipalidades, dándole una definición más clara a las instituciones municipales y departamentales, sus representantes y sus funciones para así darle autonomía a los gobiernos locales y descentralizarlos del gobierno nacional.

Aunque autónomo, el Distrito Central es influenciado por el gobierno nacional siendo que el territorio es la sede del gobierno de la república. Cambios importantes en la estructura política del municipio y el financiamiento de grandes proyectos deben ser presentados ante el Despacho Presidencial y aprobados por el Congreso Nacional antes de ser ejecutados por el gobierno local.

El Municipio del Distrito Central es constitucionalmente la capital de Honduras mientras que Tegucigalpa y Comayagüela son dos entidades dentro del municipio, que en su tiempo fueron ciudades y municipios propios hasta ser incorporados en un solo municipio y una sola ciudad, por la necesidad de poder asentar oficinas gubernamentales de ambos lados de la cuenca del río Choluteca.

La actual Constitución de Honduras, en su Título I, Capítulo I, Artículo 8 declara:
Además, en su Título V, Capítulo XI, Artículo 295 declara:

Popularmente se identifica a Tegucigalpa como la capital, siendo que fue Tegucigalpa quien ocupara dicho título antes de compartirlo con Comayagüela. Aunque legal y políticamente hablando son una sola ciudad, tradicionalmente, se les sigue identificando como dos ciudades hermanas o gemelas, dado a la historia detrás de sus inicios.

Hoy en día, es correcto decir que "Tegucigalpa" es la capital de Honduras, también es correcto decir que "Tegucigalpa y Comayagüela", juntas, son la capital de Honduras; y por último, a partir de su formación el 30 de enero de 1937 bajo el Decreto No. 53 del reformado Artículo 179 de la Constitución de Honduras de 1936 es correcto decir que el "Distrito Central" es la capital.

Cabe resaltar que en Honduras los municipios se definen como divisiones administrativas de los departamentos y no necesariamente como una sola ciudad, por lo cual un municipio puede contener más de una ciudad o poblado, incluyendo su cabecera municipal que en el caso de Honduras tiende a ser la ciudad homónima y más poblada del municipio; aunque para propósitos administrativos y legales, la alcaldía municipal es la autoridad dentro de la ciudad cabecera y el resto del municipio.

Un ejemplo, el Municipio del Distrito Central es el municipio más poblado de Honduras y el área metropolitana de Tegucigalpa y Comayagüela forman la ciudad más grande y más poblada del país, aun así, el Distrito Central no es el municipio más grande de Honduras cuando en términos de superficie administrativa se habla (territorio). Aunque es el municipio más grande de Francisco Morazán, es únicamente el decimocuarto más grande de Honduras, habiendo 13 municipios más grandes en términos de área administrativa pero no en población. Los dos municipios más grandes de Honduras son Puerto Lempira y Catacamas en los departamentos de Gracias a Dios y Olancho, respectivamente.

Otro ejemplo se observa en la ciudad de La Ceiba, cabecera municipal del municipio homónimo, uno de los ocho municipios del Departamento de Atlántida. La Ceiba es la ciudad más grande de Atlántida y la tercera ciudad más grande de Honduras, en términos de población y área urbana, pero el Municipio de La Ceiba es solo el segundo municipio más grande de Atlántida, siendo el Municipio de Tela el más grande de dicho departamento, aunque menos poblado, por lo cual la ciudad de Tela no se considera la ciudad más grande de Atlántida.

Además del urbe de Tegucigalpa y Comayagüela, el Distrito Central también cuenta con 41 aldeas y 293 caseríos, éstos son pequeños poblados en las zonas rurales del municipio cuyas poblaciones varían entre unas cuantas docenas en los caseríos más pequeños hasta unos cuantos miles, en las aldeas más pobladas. Para propósitos administrativos, la ley municipal permite asignarles alcaldes auxiliares y/o patronatos para actuar como representantes locales.

El Distrito Central es el centro político y administrativo de Honduras. Es la sede de los tres poderes del gobierno nacional y sus dependencias, incluyendo de la mayoría de las agencias públicas y empresas estatales. La capital es sede de todas las misiones diplomáticas que mantienen presencia en Honduras, con la excepción de Filipinas, cuyo consulado en el país se encuentra en San Pedro Sula. Actualmente se ubican 23 embajadas y 16 consulados en Tegucigalpa, representando a todos los países centroamericanos, la mayoría de los sudamericanos, 14 países europeos incluyendo Rusia, tres países caribeños, dos países asiáticos, Canadá, Estados Unidos, México y Sudáfrica.

La residencia oficial y despacho del Ejecutivo, la Casa Presidencial, se encuentra sobre el Bulevar Juan Pablo II en la Colonia Los Profesionales, el Congreso Nacional se encuentra en el Centro Histórico de Tegucigalpa sobre la Calle Bolívar del Barrio La Merced y la Corte Suprema de Justicia se encuentra en el Centro Cívico Gubernamental localizado a un costado del distribuidor vial del Bulevar Fuerzas Armadas y el Bulevar Kuwait, al sur del Centro Comercial Mall las Cascadas. La capital es también la sede de la Policía Nacional, de las Fuerzas Armadas y de la mayoría de las instituciones financieras del país, tanto públicas como privadas.

El Distrito Central es también la sede del gobierno del departamento de Francisco Morazán encabezado por el gobernador político departamental. El gobernador es nombrado por el presidente de la república y a su vez éste primero es el representante del ejecutivo nacional a nivel departamental. Históricamente, el gobernador político departamental ha tenido un papel menos visible en el marco político y administrativo del país a nivel departamental y municipal al grado que sus incumbentes han declarado que el gobernador necesita una autonomía y autoridad más evidente así como sucede en otros países como México y los Estados Unidos. El gobernador político actual es Rigoberto Herrera del Partido Nacional durante el periodo de 2010-2014 y devenga un sueldo base mensual de 35,000 lempiras (US$1,725).

A nivel nacional, el departamento de Francisco Morazán está representado por 23 de los 128 diputados que constituyen el Congreso Nacional. Así mismo, el Distrito Central como el resto de los municipios, es miembro de la Asociación de Municipios de Honduras (AMHON), quien observa los intereses municipales dentro del marco civil y político del país y actúa como enlace a nivel nacional.

El gobierno local toma forma dentro de un sistema donde los poderes del ejecutivo y el legislativo son representativo (electos por voto popular) y compartido (alcalde y regidores), regidos por la Ley de Municipalidades que entró en vigor el 1.º de enero de 1991. La Alcaldía Municipal del Distrito Central (AMDC) es la autoridad gubernamental de la ciudad y municipio cuya sede de gobierno se encuentra en el Centro Histórico de Tegucigalpa frente al Parque Central. Como lo establece la Ley de Municipalidades, la AMDC está estructurada en una Corporación Municipal, cual es el órgano deliberativo de la municipalidad, electa por el pueblo y máxima autoridad dentro del término municipal.

La Corporación Municipal está formada por un alcalde quien actúa como jefe ejecutivo, administrador general y representante legal del municipio y un vice alcalde quien asume el puesto del alcalde al ser requerido y supervisa las funciones dentro de la AMDC según le indique el alcalde. El alcalde y vice alcalde devengan un sueldo base mensual de L.61,000 (US$3.000) y L. 55,000 (US$2.700), respectivamente.

La Corporación Municipal también la integran diez regidores quienes constituyen el poder legislativo y deliberativo dentro del municipio y juntos con el alcalde ejecutan sus obligaciones delegadas por la Ley de Municipalidades, incluyendo la administración del municipio, sus normas presupuestarias y la legislación de leyes y ordenanzas a nivel municipal. Los regidores devengan un sueldo base mensual de L.55,000 (US$2,700).

Un gerente general, nombrado por el alcalde, actúa como jefe de auditoría encargado del manejo, recaudación y repartición de fondos de la comuna municipal. Una secretaria municipal, también nombrada por el alcalde, actúa como oficiadora legal encargada del registro oficial de todos los procedimientos legales. La Corporación Municipal también cuenta con un Consejo de Desarrollo Municipal el cual actúa como gabinete de asesoría en todas las áreas de enfoque de la ciudad, como el desarrollo social, la seguridad, servicios públicos, etc.

El actual alcalde del Distrito Central es Nasry Juan Asfura Zablahz quien pertenece al Partido Nacional (PNH), después de ganar la elección en noviembre de 2013. Asfura es la octava persona electa en ocupar el puesto de Alcalde del Distrito Central desde que las elecciones locales fueron restablecidas en 1986. Antes de 1986, el gobierno local del Distrito Central, conocido como Consejo Metropolitano, era designado por el Presidente de la República. Ésta es la novena elección desde entonces.

De los diez regidores actuales, siete son hombres y tres son mujeres. Cinco pertenecen al Partido Nacional mientras que otros dos pertenecen al Partido Libertad y Refundación (Libre), dos pertenecen al Partido Liberal (PLH) y uno pertenece al Partido Anti Corrupción (PAC).

Tanto el alcalde y como los regidores son electos a un término de cuatro años por los votantes del Distrito Central. La destitución del alcalde o cualquier regidor por cualquier causa queda reservada a la Secretaría del Interior y Población (antes Secretaría de Gobernación y Justicia).

El Alcalde de Tegucigalpa es el Ingeniero Nasry Asfura, fue elegido para administrar el gobierno de la capital en el periodo 2014-2018 (actual).

La presente administración cuenta con la siguiente organización:

El presupuesto de la alcaldía para el año 2016 es de 4,200 millones de lempiras. El PIB per cápita es de ciento cincuenta mil Lempiras.

La Policía Nacional de Honduras a través de la Policía Nacional Preventiva es la autoridad uniformada encargada de preservar el orden público y el cumplimiento de la ley. La Policía Nacional mantiene su sede en el Distrito Central en la Colonia Casamata. La Jefatura Metropolitana No. 1 es la designación al departamento de policía del municipio. Este cuenta con siete distritos policiales dentro de la zona metropolitana.

Para 2011, la Secretaría de Seguridad designó dos mil 162 millones de Lempiras (US$ 114 millones de dólares) a seguridad pública e investigación criminal en el Distrito Central.

Según lo establecido por la Ley de Policía y Convivencia Social, los municipios pueden financiar sus propias policías municipales y el Distrito Central opera en la actualidad una Policía Municipal compuesta por aproximadamente 160 oficiales, algunos de ellos asignados a las zonas turísticas de la ciudad. Existe también la Policía de Tránsito, dependencia de la Policía Nacional, encargada de observar el cumplimiento de las leyes de la vía pública. El Departamento Municipal de Justicia a través del Juzgado de Policía Municipal procesa las infracciones locales.

El Ministerio Público de Honduras, con sede en el Distrito Central y jurisdicción a nivel nacional a través de sus fiscalías regionales, está a cargo de la investigación de los delitos y el ejercicio de la acción penal pública representando a la sociedad hondureña. La Procuraduría General de la República, también con la misma sede domiciliar, ejerce la representación legal del Estado en defensa de sus intereses.

Tegucigalpa se caracteriza por su variado e interesante carácter cultural. La ciudad posee un gran valor cultural que ha estado atrayendo a muchas personas en las últimas décadas, se caracteriza por su bello estilo colonial y moderno a la vez lo que la hace un sitio muy especial.

La ciudad celebra varias ferias como la Feria del Aniversario de Tegucigalpa el 29 de septiembre esta festividad pretende obtener lo mejor de los capitalinos y es una época del año que siempre llena de mucha satisfacción para reunirse.

También se celebran otras ferias como AGAFAM (Feria de Asociación de Ganaderos y Agricultores de Francisco Morazán), la Feria del Caballo y la Feria del Emprendedor.

En Tegucigalpa encontramos muchos centros culturales, entre ellos destacan:


Tegucigalpa cuenta con algunas bibliotecas, presentes en solo algunas zonas de la capital, quedando la mayor parte de la capital y áreas escolares sin acceso a bibliotecas públicas. En Tegucigalpa se han realizado también varias "ferias del libro".

Además Honduras cuenta con varios museos con diferentes temáticas, entre ellos:


La capital cuenta con un limitado número de bibliotecas, entre ellas se encuentran:

La Biblioteca Nacional de Honduras (1880): La Biblioteca Nacional de Honduras es una biblioteca que contiene más de cuarenta mil volúmenes, concita el sueño de su fundador el doctor Antonio Ramón Vallejo y los sueños de su propiciador, secretario de gobierno de Marco Aurelio Soto, doctor Ramón Rosa. Es una institución de utilidad pública, dependiente de la Secretaría de Cultura, Artes y Deportes, cuya misión es recopilar, catalogar, clasificar, conservar y difundir toda la producción documental publicada en el país y en el extranjero. Se encuentra en la antigua casa donde nació el General Francisco Morazán, esta antigua casa, fue también: "Casa de la Moneda" (1780) y en 1830 sirvió para las reuniones del Congreso Nacional. Entre los años: 1859-1876 sirvió como cuartel y más tarde en 1898 se instaló la Tipografía Nacional; y como información hasta 1926 se dejó de acuñar monedas.

La ciudad de Tegucigalpa cuenta también con la Biblioteca de la Universidad Nacional Autónoma de Honduras para apoyar el desarrollo de la investigación, la docencia y el estudio, proporcionando el acceso a los recursos de información necesarios, propios de la Universidad o ajenos a ella. Los diferentes puntos de servicio atienden a toda la comunidad universitaria y se ofrecen en 10 centros universitarios, cada uno con varias bibliotecas de facultad, Centro de Recursos de Aprendizaje y museos especializados.

Tegucigalpa tiene varios museos, muchos de los cuales se ubican en su centro histórico. Algunos de sus museos más famosos son:







Tegucigalpa se caracteriza por sus parques como:





De categoría privada:
De categoría pública:


El centro recreativo y deportivo más amplio en Tegucigalpa es la Villa Olímpica. Es de acceso gratuito, cuenta con un estadio olímpico, parque de béisbol, piscinas, varios gimnasios, canchas de baloncesto, de tenis, amplios aparcamientos para automóviles y áreas verdes para descanso. Se puede acceder fácilmente por el anillo periférico de la ciudad.

Es un estadio destinado para partidos de béisbol, ubicado frente al estadio metropolitano Tiburcio Carias Andino, es el segundo estadio en importancia para este deporte después del ubicado en las instalaciones de la Villa Olímpica de Tegucigalpa. creado en 1951

La manera más rápida de trasladarse de un punto a otro en la ciudad, es a través del anillo periférico "Quinto Centenario". Este anillo, cubre el lado este de la ciudad de Tegucigalpa y también está conectada a la carretera nacional, donde están los ramales nacionales hacia el norte del país, (hacia San Pedro Sula), al sur (hacia Nicaragua) y otras salidas hacia pueblos cercanos.
Tegucigalpa está comunicada con el resto del país y el exterior por diversas carreteras pavimentadas. La principal de ellas se dirige hacia el norte del país, otra hacia el sur -conectando con la Carretera Interamericana- y una tercera hacia el este.

La ciudad tiene en construcción el proyecto de transporte público Metrobús Tegucigalpa, iniciado en el año 2010 e inaugurado una etapa en fecha 24 de enero de 2014.

El aeropuerto principal de Tegucigalpa es el Aeropuerto Internacional Toncontín, ubicado en el sur de la ciudad. Este aeropuerto es considerado uno de los más peligrosos del mundo por la cercanía de una sierra y una pista de aterrizaje relativamente corta; durante varios años se ha tratado de sustituirlo por el aeropuerto de Palmerola, en Comayagua, donde actualmente opera una base aérea de los Estados Unidos.

Toncontín ha sido mejorado significativamente por medio de las obras realizadas por la Corporación Aeroportuaria de Tegucigalpa y Interairports, empresa privada contratada por el gobierno para administrar los cuatro aeropuertos internacionales del país. Anteriormente, Toncontín solo contaba con una pista de aterrizaje de 1863 metros de longitud. En 2009 fue ampliada con 300 metros, de las cuales 150 metros de pista útil, 60 metros de franja de seguridad de pista, y 90 metros de área de seguridad nivelado al extremo de la pista. La altitud de la pista es de 1033 msnm.

Toncontín cuenta, además, con servicios de migración, aduana, meteorología y control del tráfico aéreo. 
Es atendido por compañías aéreas internacionales como American Airlines, Avianca, Continental Airlines, Copa Airlines, DELTA y varias líneas aéreas locales, que la conectan con el resto del país, El Salvador y Estados Unidos.

Tegucigalpa cuenta con muchos centros deportivos en cada colonia, barrio o centro, además con muchos estadios, y complejos deportivos.

Tegucigalpa es sede de los dos clubes más grandes de la Liga Nacional de Fútbol de Honduras: El Club Deportivo Olimpia, y el Club Deportivo Motagua. Entre ambos suman más de 40 títulos de liga. El deportivo Olimpia, es el más popular y el que posee más campeonatos de liga, seguido de cerca de su archirrival Motagua. Ambos equipos han sido campeones de Centroamérica. Además la ciudad cuenta con una complejo de entrenamiento para selecciones juveniles denominado proyecto GOAL que fue financiado por la FIFA y construido en los terrenos de la Universidad Nacional Autónoma de Honduras (UNAH).

1. Sistema Agropecuario 
2. Bosque tropical siempre verde estacional aciculifoliado montano inferior 
3. Bosque tropical siempre verde estacional mixto montano inferior y 
4. Bosque tropical siempre verde estacional latifoliado montano superior. 
Este último está en la parte más elevada que posee el parque.

El bosque seco subtropical está ubicado en la zona norte del parque y es el que tiene menor cantidad de área. La Tigra tiene una gran variedad de flora y fauna, entre la vegetación podemos mencionar especies de árboles como ser: Pino de Ocote, Robles, Encinos, Liquidámbar, Aguacatillos entre otros, también hay diversidad de helechos seis de ellos en peligro de extinción los cuales son protegidos por AMITIGRA.

Entre la fauna hay especies de mamíferos, anfibios, reptiles y aves que son los grupos más comunes y que podemos observar dentro del parque, se sabe que viven animales como: Tigrillos, Guatusas, Venados Cola Blanca, Micos de Noche, Pumas y Yaguaroundis. Aves como Jilgueros, Pavas de Montaña, Quetzales, Gavilanes, Tucanes entre otras más.


Destaca un jaguar, felino más grande de América y que es nativo de Honduras, así como una colección muy completa de venados cola blanca y monos araña.
Con un total de 310 animales, entre los que podemos contar 20 especies diferentes de mamíferos, 23 de aves y 7 de reptiles.

El área total del zoológico es de unas 22 manzanas.

En las cercanías de Tegucigalpa hay muchos pueblos coloniales, entre ellos encontramos:

Tegucigalpa es una ciudad de hotelería y negocios. Hay un amplio abanico de hoteles, entre los más grandes se encuentran los siguientes:

Tienen 15 capitales hermanadas con:

Tienen 5 ciudades hermanadas con:



</doc>
<doc id="11549" url="https://es.wikipedia.org/wiki?curid=11549" title="Proyección de Mollweide">
Proyección de Mollweide

La proyección de Mollweide, también conocida como Babinet, es una proyección cartográfica equitativa y pseudocilíndrica, usada generalmente para mapas de la Tierra o del cielo nocturno.

La proyección fue publicada por primera ocasión por el matemático y astrónomo Karl Mollweide (1774–1825) de Leipzig en 1805. Fue reinventada y popularizada en 1857 por Jacques Babinet, quien le dio el nombre de proyección homalográfica.

Su propósito es representar la proporción de las áreas con la máxima exactitud posible.

El ecuador tiene el doble de longitud que el eje corto, el meridiano central o tipo, es recto. Los meridianos a 90° son arcos circulares. Los paralelos son rectos pero desigualmente espaciados. La escala es casi verdadera sólo a lo largo de los paralelos estándar de 40:44N y 40:44S, por lo que tiene una mayor representación por la zona ecuatorial.
La proyección de Mollweide es usada para mapas del mundo, especialmente para representar zonas de latitudes bajas.

La latitud y longitudes en coordenadas "x" e "y" por medio de las siguientes ecuaciones:

donde "θ" es un ángulo auxiliar definido por

y "λ" es la longitud, "λ" es el meridiano central, "φ" es la latitud, y "R" es el radio del globo a ser proyectado. El mapa tiene como área 4π"R", conforme a la superficie de globo generador. La coordenada "x" tiene un rango de [−2"R", 2"R"], y la coordenada "y" tiene un rango de [−"R", "R"].

La ecuación (1) puede ser resuelta con una convergencia rápida (pero lenta cerca de los polos) usando la iteración del Método de Newton–Raphson:

Si "φ" = ±, entonces también "θ" = ±. En ese caso la iteración debe evitarse; de otro modo, podría resultar en división por cero.

Existe una forma cerrada de transformación inversa:

donde "θ" se puede encontrar por la relación

Las transformaciones inversas permiten encontrar la latitud y longitud correspondientes a las coordenadas "x" e "y".




</doc>
<doc id="11551" url="https://es.wikipedia.org/wiki?curid=11551" title="Media geométrica">
Media geométrica

En matemáticas y estadística, la media geométrica de una cantidad arbitraria de números (por decir "n" números) es la raíz n-ésima del producto de todos los números; es recomendada para datos de progresión geométrica, para promediar razones, interés compuesto y números índice.
\sqrt[n]{x_1 \cdot x_2 \cdots x_n} 

Por ejemplo, la media geométrica de 2 y 18 es la raíz cuadrada del producto de ambos formula_1.
Otro ejemplo, la media geómetrica de 1, 3 y 9 sería la raíz cúbica del producto de los tres números formula_2.

Frecuentemente se usa una media geométrica cuando se comparan diferentes aspectos, cuyos rendimientos tienen unidades de medida en diferentes rangos numéricos. Por ejemplo, la media geométrica puede dar un valor serio para comparar dos empresas que tienen una calificación entre 0 a 5 por su sostenibilidad ambiental, y una calificación entre 0 a 100 por su viabilidad financiera. Si se usara la media aritmética en lugar de la media geométrica, la viabilidad financiera tendría mayor peso porque su rango numérico es mayor. Es decir, un pequeño cambio porcentual en la calificación financiera (por ejemplo, pasar de 80 a 90) haría una diferencia mucho mayor en la media aritmética que un gran cambio porcentual en la sostenibilidad ambiental (por ejemplo, pasar de 2 a 5). El uso de la media geométrica normaliza los valores de rango diferente, lo que significa que un cambio de porcentaje dado en cualquiera de las propiedades tiene el mismo efecto en la media geométrica. Entonces, un cambio del 20% en la sostenibilidad ambiental de 4 a 4.8 tiene el mismo efecto en la media geométrica que un cambio del 20% en la viabilidad financiera de 60 a 72.

Esta media se puede entender en términos geométricos. La media de dos números, formula_3 y formula_4, es la longitud del lado de un cuadrado cuya área es igual al área de un rectángulo con lados de longitudes formula_3 y formula_4. De manera similar, la media de tres números, formula_3, formula_4, y formula_9, es la longitud de la arista de un cubo cuyo volumen es el mismo que el de un ortoedro cuyos lados son iguales a los tres números dados.

La media geométrica es también una de las tres medias pitagóricas, junto con la media aritmética, mencionada anteriormente, y la media armónica. Para todos los conjuntos de datos positivos que contienen al menos un par de valores desiguales, la media armónica es siempre la menor de las tres medias, mientras que la media aritmética es siempre la mayor de las tres y la media geométrica siempre está en el medio (ver Desigualdad de las medias aritmética y geométrica.)
 \le \frac{x_1+ x_2 +\dots + x_n}{n}</math>
La igualdad sólo se alcanza si formula_10.


Solo es relevante la media geométrica si todos los números son positivos. Como hemos visto, si uno de ellos es 0, entonces el resultado es 0. Si hubiera un número negativo (o una cantidad impar de ellos) entonces la media geométrica sería o bien negativa, o bien inexistente en los números reales.

En muchas ocasiones se utiliza su trasformación en el manejo estadístico de variables con distribución no normal.

La media geométrica es relevante cuando varias cantidades son multiplicadas para producir un total.

Al igual que en una media aritmética pueden introducirse pesos como valores multiplicativos para cada uno de los valores con el fin de ponderar o hacer pesar más en el resultado final ciertos valores, en la media geométrica pueden introducirse pesos como exponentes:
\right)^{\frac{1}{\sum_i{\alpha_i}}}=
Donde las formula_12 son los «pesos».

Una cadena de expendedores de gasolina el año pasado aumentó sus ingresos respecto al año anterior en 21%; y han proyectado que este año van a llegar a un aumento de 28% con respecto al año pasado. ¿Cuánto es el promedio anual del aumento porcentual?

Definitivamente no es (21% + 28%):2 = 24,5%.

El monto de la producción, al final de dos años, es 100(1,21)(1,28)= 154,88. Si en cada año se tuviera una tasa anual de aumento de i% resulta

Entonces


El peso w de una sustancia que tiene pesos hallados por dos balanzas u y v , resulta formula_19





</doc>
<doc id="11556" url="https://es.wikipedia.org/wiki?curid=11556" title="Gerardus Mercator">
Gerardus Mercator

Gerard Kremer, conocido por su nombre latinizado Gerardus Mercator (Rupelmundo, Flandes; 5 de marzo de 1512-Duisburgo, Sacro Imperio Romano Germánico; 2 de diciembre de 1594), también llamado "Mercator" o "Gerardo Mercator", fue un geógrafo, matemático y cartógrafo flamenco, famoso por idear la llamada proyección de Mercator, un sistema de proyección cartográfica conforme, en el que se respetan las formas de los continentes pero no los tamaños. Fue uno de los primeros en utilizar el término «atlas» para designar una colección de mapas

Nació en Rupelmundo, Flandes. Su nombre era Gerard de Cremere (o Kremer). "Mercator" es la latinización de su nombre, que significa 'mercader'. Recibió educación del humanista Macropedius en Bolduque y en la Universidad Católica de Lovaina. 

En 1534, Mercator se dedicó al estudio de las matemáticas, la astronomía y la geografía bajo la tutela del matemático Gemma Frisius. También aprendió a hacer grabados gracias a la ayuda de Gaspard van der Heyden, grabador y constructor de globos terráqueos (mapas esféricos). A principios del siglo XVI, los cartógrafos, o dibujantes de mapas, empleaban gruesos caracteres góticos que limitaban el espacio disponible para añadir información en los mapas. No obstante, Mercator adoptó un nuevo estilo italiano de escritura cursiva —o letra itálica— que resultó muy útil en la fabricación de globos terráqueos y un tipo de letra más adecuado para los grabados en cobre de los mapas.. Escribió al respecto un libro que fue el primero que trataba sobre este tema (Europa del Norte). Trabajó como grabador con Frisius y van der Heyden en la elaboración de un mapa esférico en 1536.

Su primer trabajo en solitario fue la elaboración de un mapa de Palestina en 1537, después de lo que dedicó tres años a su "Exactissima Flandriae Descriptio" ("La descripción más exacta de Flandes"), el mejor mapa de Flandes confeccionado hasta el momento. 

En 1544 por mostrarse tolerante al protestantismo es acusado de herejía y pasó en prisión siete meses. En 1552, se trasladó a Duisburgo donde abre un taller de cartografía. Trabajó en la elaboración de un mapa de Europa, compuesto por seis paneles, que completó en 1554; también se dedicó a enseñar matemática. Asimismo realizó otros mapas. Fue nombrado cosmógrafo de la corte por el duque Guillermo de Cléveris en 1564. Durante estos años, concibió la idea de una nueva proyección aplicable en los mapas, que utilizó por primera vez en 1569, la cual sería conocida posteriormente como proyección de Mercator; lo novedoso en su propuesta del nuevo sistema de proyección era que las líneas de longitud eran paralelas, lo cual facilitaba la navegación por mar al poderse marcar las direcciones de las brújulas con líneas rectas.

Estimuló a Abraham Ortelius a hacer el primer atlas moderno, "Theatrum Orbis Terrarum" en 1570. Posteriormente Mercator comenzó a elaborar su propio atlas, organizado en varios tomos, el primero de los cuales fue publicado en 1578 y consistía en una versión corregida de los mapas de Ptolomeo, aunque esta edición también incluía algunos errores propios de Mercator. En 1585, se publicaron mapas de Francia, Alemania y Holanda, y en 1588 se agregaron mapas de los Balcanes y Grecia.

En el título de su obra "Atlas sive Cosmographicae meditationes de fabrica mvndi et fabricati figura" ("Atlas, o meditaciones cosmográficas sobre la creación del universo y el universo en tanto creación") es donde aparece por primera vez el término "Atlas" para describir una publicación de ese tipo. Los dos primeros tomos aparecieron en 1594 y el tercero al año siguiente completado por su hijo Rumold.

Mercator contribuyó al campo de la musicología a partir de su medición de las distancias entre los tonos y los semitonos de la escala diatónica. Dicho campo se relaciona con la cartografía en lo que hace a las mediciones que tienden a la percepción de lo continuo. Se llega al límite del continuo cuando se le dificulta a la mente distinguir con claridad dos puntos contiguos. El interés de Mercator son los límites de la percepción auditiva de los intervalos de altura, lo cual le lleva a definir el noveno de tono como el intervalo más pequeño. Llega a dicho intervalo a partir de la medición sistemática de los 5 tonos enteros de la escala diatónica –Do-Re, Re-Mi, Fa-Sol, Sol-La, La-Si–. Mientras, para los semitonos naturales –Mi-Fa, Si-Do– observa que cada uno mide solamente 4 novenos de tono. A nivel micro, la escala que encuentra Mercator contendría un total de 53 micro-divisiones de noveno de tono: 45 novenos de tono en los 5 tonos eneros y 8 novenos de tono en los 2 semitonos.







Mercator falleció sin haber terminado su atlas. Fue su hijo Rumold Mercator, quien concluiría la obra publicando más mapas en 1595.

El Museo Mercator, en Sint-Niklaas, Bélgica, tiene una exposición permanente con trabajos sobre la vida y el legado de Mercator.






</doc>
<doc id="11558" url="https://es.wikipedia.org/wiki?curid=11558" title="Campeonato Mundial de Atletismo">
Campeonato Mundial de Atletismo

El Campeonato Mundial de Atletismo es la máxima competición de atletismo a nivel internacional. Es organizado por World Athletics desde 1983; las tres primeras ediciones (de 1983 a 1991) se disputaron de forma cuatrienal, pero a partir de entonces, se convirtió en bienal.

En la siguiente tabla se muestran los atletas —hombres y mujeres— que han conseguido al menos cinco medallas de oro en sus participaciones en los Campeonatos Mundiales:
En esta tabla aparecen los atletas —hombres y mujeres— que han conseguido al menos cuatro medallas de oro en las pruebas individuales (no se incluyen las carreras de relevos):
Con dieciocho medallas (trece de oro, tres de plata y dos de bronce), la estadounidense Allyson Felix es la atleta con más medallas en los campeonatos del mundo. 

Con once medallas de oro, el jamaicano Usain Bolt es el atleta con mayor número de condecoraciones áureas en la historia de los campeonatos mundiales de atletismo. 

Asimismo, con catorce medallas totales (once de oro, dos de plata y una de bronce), Usain Bolt es el atleta masculino más condecorado en la historia de la competición; superando a Carl Lewis, que consiguió ocho medallas de oro, una de plata y una de bronce.

Con siete medallas de oro individuales: 200 m (cuatro) y 100 m (tres), Usain Bolt es el atleta que ha obtenido más victorias individuales. Con sus dos medallas de oro individuales obtenidas en el Mundial de Pekín 2015, Usain Bolt supera a quienes con seis medallas de oro, Serguei Bubka en salto con pértiga y Michael Johnson en 400 m (cuatro) y 200 m (dos), eran los atletas que habían obtenido más victorias individuales hasta antes de la edición de 2015.





</doc>
<doc id="11563" url="https://es.wikipedia.org/wiki?curid=11563" title="Aleación">
Aleación

Una aleación es una mezcla homogénea de dos o más elementos, de los cuales al menos uno debe ser un metal. El compuesto resultante generalmente presenta unas propiedades muy diferentes de las de los elementos constitutivos por separado, y a veces basta con añadir una muy pequeña cantidad de uno de ellos para que aparezcan. La técnica de la aleación se utiliza para mejorar algunas propiedades de los metales puros, como la resistencia mecánica, la dureza o la resistencia a la corrosión. Así, el acero es mucho más duro que el hierro, que lo compone en casi su totalidad y al que se ha añadido un poco de carbono. Por otra parte, algunas propiedades físicas como la densidad, la reactividad, el módulo elástico o la conductividad térmica y eléctrica de la aleación no difieren mucho de la de los elementos primarios. Algunas aleaciones comunes son, además del acero, el latón, compuesto de cobre y zinc, o el bronce, formado por cobre y estaño.

Mayormente las aleaciones son consideradas mezclas, al no producirse enlaces estables entre los átomos de los elementos involucrados. Excepcionalmente, algunas aleaciones generan compuestos químicos.

Se clasifican teniendo en cuenta el elemento que se halla en mayor proporción (aleaciones férricas, aleaciones base cobre, etc.). Cuando los aleantes no tienen carácter metálico suelen hallarse en muy pequeña proporción, mientras que si únicamente se mezclan metales, los aleantes pueden aparecer en proporciones similares

Las aleaciones presentan brillo metálico y alta conductividad eléctrica y térmica, aunque usualmente menor que los metales puros. Las propiedades físicas y químicas son, en general, similares a la de los metales, sin embargo las propiedades mecánicas tales como dureza, ductilidad, tenacidad y otras pueden ser muy diferentes, de ahí el interés que despiertan estos materiales.

Las aleaciones no tienen una temperatura de fusión única, dependiendo de la concentración, cada metal puro funde a una temperatura, coexistiendo simultáneamente la fase líquida y fase sólida como se puede apreciar en los diagramas de fase. Hay ciertas concentraciones específicas de cada aleación para las cuales la temperatura de fusión se unifica. Esa concentración y la aleación obtenida reciben el nombre de eutéctica, y presenta un punto de fusión más bajo que los puntos de fusión de los componentes.

Históricamente, la mayoría de las aleaciones se preparaban mezclando los materiales fundidos. Más recientemente, la pulvimetalurgia ha alcanzado gran importancia en la preparación de aleaciones con características especiales. En este proceso, se preparan las aleaciones mezclando los materiales secos en polvo, prensándolos a alta presión y calentándolos después a temperaturas justo por debajo de sus puntos de fusión. El resultado es una aleación sólida y homogénea. Los productos hechos en serie pueden prepararse por esta técnica abaratando mucho su costo. Entre las aleaciones que pueden obtenerse por pulvimetalurgia están los cermets. Estas aleaciones de metal y carbono (carburos), boro (boruros), oxígeno (óxidos), silicio (siliciuros) y nitrógeno (nitruros) combinan las ventajas del compuesto cerámico, estabilidad y resistencia a las temperaturas elevadas y a la oxidación, con las ventajas del metal, ductilidad y resistencia a los golpes. Otra técnica de aleación es la implantación de ion, que ha sido adaptada de los procesos utilizados para fabricar chips de ordenadores o computadoras. Sobre los metales colocados en una cámara de vacío, se disparan haces de iones de carbono, nitrógeno y otros elementos para producir una capa de aleación fina y resistente sobre la superficie del metal. Bombardeando titanio con nitrógeno, por ejemplo, se puede producir una aleación idónea para los implantes de prótesis.

La plata de ley, el oro de 18 quilates y el oro blanco son aleaciones de metales preciosos. La aleación antifricción, el latón, el bronce, el metal Dow, la plata alemana, el bronce de torpedo, el monel, el peltre y la soldadura son aleaciones de metales menos preciosos. Debido a sus impurezas, el aluminio comercial es en realidad una aleación. Las aleaciones de mercurio con otros metales se llaman amalgamas.

Las aleaciones más comunes utilizadas en la industria son:



</doc>
<doc id="11564" url="https://es.wikipedia.org/wiki?curid=11564" title="Acero">
Acero

El acero es una aleación de hierro y carbono en un porcentaje de este último elemento variable entre el 0,08% y el 2% en masa de su composición. La rama de la metalurgia que se especializa en producir acero se denomina siderurgia o acería.

El acero producido antes de la detonación de las primeras bombas atómicas es acero de bajo fondo, no contaminado por radionucleidos. 

No se debe confundir el acero con el hierro, que es un metal duro y relativamente dúctil, con diámetro atómico (dA) de 2,48 Å, con temperatura de fusión de 1535 °C y punto de ebullición 2740 °C. Por su parte, el carbono es un no metal de diámetro menor (dA = 1,54 Å), blando y frágil en la mayoría de sus formas alotrópicas (excepto en la forma de diamante). La difusión de este elemento en la estructura cristalina del anterior se logra gracias a la diferencia en diámetros atómicos, formándose un compuesto intersticial.

La diferencia principal entre el hierro y el acero se halla en el porcentaje del carbono: el acero es hierro con un porcentaje de carbono de entre el 0,03% y el 1,075%; a partir de este porcentaje se consideran otras aleaciones con hierro.

Cabe destacar que el acero posee diferentes constituyentes según su temperatura, concretamente, de mayor a menor dureza, perlita, cementita y ferrita; además de la austenita (para mayor información consultar el artículo Diagrama hierro-carbono).

El acero conserva las características metálicas del hierro en estado puro, pero la adición de carbono y de otros elementos tanto metálicos como no metálicos mejora sus propiedades físico-químicas. Sin embargo, si la aleación posee una concentración de carbono mayor del 1,8%, se producen fundiciones, que son mucho más frágiles que el acero y no es posible forjarlas, sino que tienen que ser moldeadas.

Existen muchos tipos de acero en función del elemento o los elementos aleantes que estén presentes. La definición en porcentaje de carbono corresponde a los aceros al carbono, en los cuales este no metal es el único aleante, o hay otros pero en menores concentraciones. Otras composiciones específicas reciben denominaciones particulares en función de múltiples variables como por ejemplo los elementos que predominan en su composición (aceros al silicio), de su susceptibilidad a ciertos tratamientos (aceros de cementación), de alguna característica potenciada (aceros inoxidables) e incluso en función de su uso (aceros estructurales). Usualmente estas aleaciones de hierro se engloban bajo la denominación genérica de aceros especiales, razón por la que aquí se ha adoptado la definición de los comunes o "al carbono" que además de ser los primeros fabricados y los más empleados, sirvieron de base para los demás. Esta gran variedad de aceros llevó a Siemens a definir el acero como «un compuesto de hierro y otra sustancia que incrementa su resistencia».

El término acero procede del latín «"aciarius"», y este de la palabra «"acies"», que es como se denomina en esta lengua el filo de un arma blanca. «"Aciarius"» sería, por tanto, el metal adecuado, por su dureza y resistencia, para ponerlo en la parte cortante de las armas y las herramientas.
Se desconoce la fecha exacta en que se descubrió la técnica para obtener hierro a partir de la fusión de minerales. Sin embargo, los primeros restos arqueológicos de utensilios de hierro datan del 3000 a. C. y fueron descubiertos en Egipto, aunque hay vestigios de adornos anteriores. Algunos de los primeros aceros provienen del este de África, cerca de 1400 a. C.
Durante la dinastía Han de China se produjo acero al derretir hierro forjado con hierro fundido, en torno al siglo I a. C. También adoptaron los métodos de producción para la creación de acero wootz, un proceso surgido en India y en Sri Lanka desde aproximadamente el año 300 a. C. y exportado a China hacia el siglo V. Este temprano método utilizaba un horno de viento, soplado por los monzones. También conocido como acero Damasco, era una aleación de hierro con gran número de diferentes materiales, incluyendo trazas de otros elementos en concentraciones menores a 1000 partes por millón o 0,1% de la composición de la roca. Estudios realizados por Peter Paufler sugirieron que en su estructura se incluían nanotubos de carbono, lo que podría explicar algunas de las cualidades de este acero -como su durabilidad y capacidad de mantener un filo-, aunque debido a la tecnología de la época es posible que las mismas se hayan obtenido por azar y no por un diseño premeditado.

Entre los siglos IX y X se produjo en Merv el acero de crisol, en el cual el acero se obtenía calentando y enfriando el hierro y el carbón por distintas técnicas. Durante la dinastía Song del siglo XI en China, la producción de acero se realizaba empleando dos técnicas: la primera producía acero de baja calidad por no ser homogéneo —método «berganesco»— y la segunda, precursora del método Bessemer, quita el carbón con forjas repetidas y somete la pieza a enfriamientos abruptos.

El hierro para uso industrial fue descubierto hacia el año 1500 a. C., en Medzamor y el monte Ararat, en Armenia. La tecnología del hierro se mantuvo mucho tiempo en secreto, difundiéndose extensamente hacia el año 1200 a. C.

No hay registros de que la templabilidad fuera conocida hasta la Edad Media. Los métodos antiguos para la fabricación del acero consistían en obtener hierro dulce en el horno, con carbón vegetal y tiro de aire, con una posterior expulsión de las escorias por martilleo y carburación del hierro dulce para cementarlo. Luego se perfeccionó la cementación fundiendo el acero cementado en crisoles de arcilla y en Sheffield (Inglaterra) se obtuvieron, a partir de 1740, aceros de crisol. La técnica fue desarrollada por Benjamin Huntsman.

En 1856, Henry Bessemer, desarrolló un método para producir acero en grandes cantidades, pero dado que solo podía emplearse hierro que contuviese fósforo y azufre en pequeñas proporciones, fue dejado de lado. Al año siguiente, Carl Wilhelm Siemens creó otro, el procedimiento Martin-Siemens, en el que se producía acero a partir de la descarburación de la fundición de hierro dulce y óxido de hierro como producto del calentamiento con aceite, gas de coque, o una mezcla este último con gas de alto horno. Este método también quedó en desuso.

Aunque en 1878 Siemens también fue el primero en emplear electricidad para calentar los hornos de acero, el uso de hornos de arco eléctricos para la producción comercial comenzó en 1902 por Paul Héroult, quien fue uno de los inventores del método moderno para fundir aluminio. En este método se hace pasar dentro del horno un arco eléctrico entre chatarra de acero cuya composición se conoce y unos grandes electrodos de carbono situados en el techo del horno.

En 1948 se inventa el proceso del oxígeno básico L-D. Tras la segunda guerra mundial se iniciaron experimentos en varios países con oxígeno puro en lugar de aire para los procesos de refinado del acero. El éxito se logró en Austria en 1948, cuando una fábrica de acero situada cerca de la ciudad de Linz, Donawitz desarrolló el proceso del oxígeno básico o L-D.

En 1950 se inventa el proceso de colada continua que se usa cuando se requiere producir perfiles laminados de acero de sección constante y en grandes cantidades. El proceso consiste en colocar un molde con la forma que se requiere debajo de un crisol, el cual con una válvula puede ir dosificando material fundido al molde. Por gravedad el material fundido pasa por el molde, que está enfriado por un sistema de agua; al pasar el material fundido por el molde frío se convierte en pastoso y adquiere la forma del molde. Posteriormente el material es conformado con una serie de rodillos que al mismo tiempo lo arrastran hacia la parte exterior del sistema. Una vez conformado el material con la forma necesaria y con la longitud adecuada el material se corta y almacena.

En la actualidad se utilizan algunos metales y metaloides en forma de ferroaleaciones, que, unidos al acero, le proporcionan excelentes cualidades de dureza y resistencia.

Actualmente, el proceso de fabricación del acero se completa mediante la llamada metalurgia secundaria. En esta etapa se otorgan al acero líquido las propiedades químicas, temperatura, contenido de gases, nivel de inclusiones e impurezas deseados. La unidad más común de metalurgia secundaria es el horno cuchara. El acero aquí producido está listo para ser posteriormente colado, en forma convencional o en colada continua.

El uso intensivo que tiene y ha tenido el acero para la construcción de estructuras metálicas ha conocido grandes éxitos y rotundos fracasos que al menos han permitido el avance de la ciencia de materiales. Así, el 7 de noviembre de 1940 el mundo asistió al colapso del puente Tacoma Narrows al entrar en resonancia con el viento. Ya durante los primeros años de la Revolución industrial se produjeron roturas prematuras de ejes de ferrocarril que llevaron a William Rankine a postular la fatiga de materiales y durante la Segunda Guerra Mundial se produjeron algunos hundimientos imprevistos de los cargueros estadounidenses Liberty al fragilizarse el acero por el mero descenso de la temperatura, problema inicialmente achacado a las soldaduras.
Aceros ordinarios.
Aceros aleados o especiales.
Los aceros aleados o especiales contienen otros elementos, además de carbono, que modifican sus propiedades. Estos se clasifican según su influencia:

Elementos que aumentan la dureza: fósforo, níquel, cobre, aluminio. En especial aquellos que conservan la dureza a elevadas temperaturas: titanio, vanadio, molibdeno, wolframio, cromo, manganeso y cobalto.
Elementos que limitan el crecimiento del tamaño de grano: aluminio, titanio y vanadio.
Elementos que determinan en la templabilidad: aumentan la templabilidad: manganeso, molibdeno, cromo, níquel y silicio. Disminuye la templabilidad: el cobalto.
Elementos que modifican la resistencia a la corrosión u oxidación: aumentan la resistencia a la oxidación: molibdeno y wolframio. Favorece la resistencia a la corrosión: el cromo.
Elementos que modifican las temperaturas críticas de transformación: Suben los puntos críticos: molibdeno, aluminio, silicio, vanadio, wolframio. Disminuyen las temperaturas críticas: cobre, níquel y manganeso. En el caso particular del cromo, se elevan los puntos críticos cuando el acero es de alto porcentaje de carbono pero los disminuye cuando el acero es de bajo contenido de carbono.

Los dos componentes principales del acero se encuentran en abundancia en la naturaleza, lo que favorece su producción a gran escala. Esta variedad y disponibilidad lo hace apto para numerosos usos como la construcción de maquinaria, herramientas, edificios y obras públicas, contribuyendo al desarrollo tecnológico de las sociedades industrializadas. A pesar de su densidad (7850 kg/m³ de densidad en comparación a los 2700 kg/m³ del aluminio, por ejemplo) el acero es utilizado en todos los sectores de la industria, incluso en el aeronáutico, ya que las piezas con mayores solicitaciones (ya sea al impacto o a la fatiga) solo pueden aguantar con un material dúctil y tenaz como es el acero, además de la ventaja de su relativo bajo costo.

Las clasificaciones normalizadas de aceros como la AISI, ASTM y UNS, establecen valores mínimos o máximos para cada tipo de elemento. Estos elementos se agregan para obtener unas características determinadas como templabilidad, resistencia mecánica, dureza, tenacidad, resistencia al desgaste, soldabilidad o maquinabilidad. A continuación se listan algunos de los efectos de los elementos aleantes en el acero:



Se denomina impurezas a todos los elementos indeseables en la composición de los aceros. Se encuentran en los aceros y también en las fundiciones como consecuencia de que están presentes en los minerales o los combustibles. Se procura eliminarlas o reducir su contenido debido a que son perjudiciales para las propiedades de la aleación. En los casos en los que eliminarlas resulte imposible o sea demasiado costoso, se admite su presencia en cantidades mínimas.



Los aceros aleados o especiales contienen otros elementos, además de carbono, que modifican sus propiedades. Estos se clasifican según su influencia:


Aunque es difícil establecer las propiedades físicas y mecánicas del acero debido a que estas varían con los ajustes en su composición y los diversos tratamientos térmicos, químicos o mecánicos, con los que pueden conseguirse aceros con combinaciones de características adecuadas para infinidad de aplicaciones, se pueden citar algunas propiedades genéricas:

Es la degradación física (pérdida o ganancia de material, aparición de grietas, deformación plástica, cambios estructurales como transformación de fase o recristalización, fenómenos de corrosión, etc.) debido al movimiento entre la superficie de un material sólido y uno o varios elementos de contacto.

Para homogeneizar las distintas variedades de acero que se pueden producir, existen sistemas de normas que regulan la composición de los aceros y las prestaciones de los mismos en cada país, en cada fabricante de acero, y en muchos casos en los mayores consumidores de aceros.

Por ejemplo, en España están regulados por la norma UNE-EN 10020:2001 y antiguamente estaban reguladas por la norma UNE-36010, ambas editadas por AENOR.

Existen otras normas reguladoras del acero, como la clasificación de AISI (de uso mucho más extendido internacionalmente), ASTM, DIN, o la ISO 3506.

Debido a la facilidad que tiene el acero para oxidarse cuando entra en contacto con la atmósfera o con el agua, es necesario y conveniente proteger la superficie de los componentes de acero para protegerles de la oxidación y corrosión. Muchos tratamientos superficiales están muy relacionados con aspectos embellecedores y decorativos de los metales.

Los tratamientos superficiales más usados son los siguientes:

Un proceso de tratamiento térmico adecuado permite aumentar significativamente las propiedades mecánicas de dureza, tenacidad y resistencia mecánica del acero. Los tratamientos térmicos cambian la microestructura del material, con lo que las propiedades macroscópicas del acero también son alteradas.

Los tratamientos térmicos que pueden aplicarse al acero sin cambiar en su composición química son:

Son tratamientos térmicos en los que, además de los cambios en la estructura del acero, también se producen cambios en la composición química de la capa superficial, añadiendo diferentes productos químicos hasta una profundidad determinada. Estos tratamientos requieren el uso de calentamiento y enfriamiento controlados en atmósferas especiales. Entre los objetivos más comunes de estos tratamientos están aumentar la dureza superficial de las piezas dejando el núcleo más blando y tenaz, disminuir el rozamiento aumentando el poder lubrificante, aumentar la resistencia al desgaste, aumentar la resistencia a fatiga o aumentar la resistencia a la corrosión.

Entre los factores que afectan a los procesos de tratamiento térmico del acero se encuentran la temperatura y el tiempo durante el que se expone a dichas condiciones al material. Otro factor determinante es la forma en la que el acero vuelve a la temperatura ambiente. El enfriamiento del proceso puede incluir su inmersión en aceite o el uso del aire como refrigerante.

El método del tratamiento térmico, incluyendo su enfriamiento, influye en que el acero tome sus propiedades comerciales.

Según ese método, en algunos sistemas de clasificación, se le asigna un prefijo indicativo del tipo. Por ejemplo, el acero O-1, o A2, A6 (o S7) donde la letra "O" es indicativo del uso de aceite (del inglés: "oil quenched"), y "A" es la inicial de aire; el prefijo "S" es indicativo que el acero ha sido tratado y considerado resistente al golpeo ("shock resistant").

El acero que se utiliza para la construcción de estructuras metálicas y obras públicas, se obtiene a través de la laminación de acero en una serie de perfiles normalizados.

El proceso de laminado consiste en calentar previamente los lingotes de acero fundido a una temperatura que permita la deformación del lingote por un proceso de estiramiento y desbaste que se produce en una cadena de cilindros a presión llamado tren de laminación. Estos cilindros van formando el perfil deseado hasta conseguir las medidas que se requieran. Las dimensiones de las secciones conseguidas de esta forma no se ajustan a las tolerancias requeridas y por eso muchas veces los productos laminados hay que someterlos a fases de mecanizado para ajustar sus dimensiones a la tolerancia requerida.

La forja es el proceso que modifica la forma de los metales por deformación plástica cuando se somete al acero a una presión o a una serie continuada de impactos. La forja generalmente se realiza a altas temperaturas porque así se mejora la calidad metalúrgica y las propiedades mecánicas del acero.

El sentido de la forja de piezas de acero es reducir al máximo posible la cantidad de material que debe eliminarse de las piezas en sus procesos de mecanizado. En la forja por estampación la fluencia del material queda limitada a la cavidad de la estampa, compuesta por dos matrices que tienen grabada la forma de la pieza que se desea conseguir.

El acero corrugado es una clase de acero laminado usado especialmente en construcción, para emplearlo en hormigón armado. Se trata de barras de acero que presentan resaltos o «corrugas» que mejoran la adherencia con el hormigón. Está dotado de una gran ductilidad, la cual permite que a la hora de cortar y doblar no sufra daños, y tiene una gran soldabilidad, todo ello para que estas operaciones resulten más seguras y con un menor gasto energético.

Las barras de acero corrugado están normalizadas. Por ejemplo, en España están cubiertas por las Normas UNE 36068:2011, UNE 36065:2011 y UNE 36811:1998 IN.

Las barras de acero corrugados se producen en una gama de diámetros que van de 6 a 40 mm, en la que se cita la sección en cm² que cada barra tiene así como su peso en kg.

Las barras inferiores o iguales a 16 mm de diámetro se pueden suministrar en barras o rollos, para diámetros superiores a 16 siempre se suministran en forma de barras.

Las barras de producto corrugado tienen unas características técnicas que deben cumplir, para asegurar el cálculo correspondiente de las estructuras de hormigón armado. Entre las características técnicas destacan las siguientes, todas ellas se determinan mediante el ensayo de tracción:

La estampación del acero consiste en un proceso de mecanizado sin arranque de viruta donde a la plancha de acero se la somete por medio de prensas adecuadas a procesos de embutición y estampación para la consecución de determinadas piezas metálicas. Para ello en las prensas se colocan los moldes adecuados.

La troquelación del acero consiste en un proceso de mecanizado sin arranque de viruta donde se perforan todo tipo de agujeros en la plancha de acero por medio de prensas de impactos donde tienen colocados sus respectivos troqueles y matrices.

Las piezas de acero permiten mecanizarse en procesos de arranque de virutas en máquinas-herramientas (taladro, torno, fresadora, centros de mecanizado CNC, etc.) luego endurecerlas por tratamiento térmico y terminar los mecanizados por procedimientos abrasivos en los diferentes tipos de rectificadoras que existen.

El proceso de rectificado permite obtener muy buenas calidades de acabado superficial y medidas con tolerancias muy estrechas, que son muy beneficiosas para la construcción de maquinaria y equipos de calidad. Pero el tamaño de la pieza y la capacidad de desplazamiento de la rectificadora pueden presentar un obstáculo.

En ocasiones especiales, el tratamiento térmico del acero puede llevarse a cabo antes del mecanizado en procesos de arranque de virutas, dependiendo del tipo de acero y los requerimientos que deben ser observados para determinada pieza. Con esto, se debe tomar en cuenta que las herramientas necesarias para dichos trabajos deben ser muy fuertes por llegar a sufrir desgaste apresurado en su vida útil. Estas ocasiones peculiares, se pueden presentar cuando las tolerancias de fabricación son tan estrechas que no se permita la inducción de calor en tratamiento por llegar a alterar la geometría del trabajo, o también por causa de la misma composición del lote del material (por ejemplo, las piezas se están encogiendo mucho por ser tratadas). En ocasiones es preferible el mecanizado después del tratamiento térmico, ya que la estabilidad óptima del material ha sido alcanzada y, dependiendo de la composición y el tratamiento, el mismo proceso de mecanizado no es mucho más difícil.

En algunos procesos de fabricación que se basan en la descarga eléctrica con el uso de electrodos, la dureza del acero no hace una diferencia notable.

En muchas situaciones, la dureza del acero es determinante para un resultado exitoso, como por ejemplo en el taladrado profundo al procurar que un agujero mantenga su posición referente al eje de rotación de la broca de carburo. O por ejemplo, si el acero ha sido endurecido por ser tratado térmicamente y por otro siguiente tratamiento térmico se ha suavizado, la consistencia puede ser demasiado suave para beneficiar el proceso, puesto que la trayectoria de la broca tenderá a desviarse.

El doblado del acero que ha sido tratado térmicamente no es muy recomendable pues el proceso de doblado en frío del material endurecido es más difícil y el material muy probablemente se haya tornado demasiado quebradizo para ser doblado; el proceso de doblado empleando antorchas u otros métodos para aplicar calor tampoco es recomendable puesto que al volver a aplicar calor al metal duro, la integridad de este cambia y puede ser comprometida.

Para su uso en construcción, el acero se distribuye en perfiles metálicos, siendo éstos de diferentes características según su forma y dimensiones y debiéndose usar específicamente para una función concreta, ya sean vigas o pilares.

El acero en sus distintas clases está presente de forma abrumadora en nuestra vida cotidiana en forma de herramientas, utensilios, equipos mecánicos y formando parte de electrodomésticos y maquinaria en general así como en las estructuras de las viviendas que habitamos y en la gran mayoría de los edificios modernos. En este contexto existe la versión moderna de perfiles de acero denominada Metalcón.
Los fabricantes de medios de transporte de mercancías (camiones) y los de maquinaria agrícola son grandes consumidores de acero.

También son grandes consumidores de acero las actividades constructoras de índole ferroviario desde la construcción de infraestructuras viarias así como la fabricación de todo tipo de material rodante.

Otro tanto cabe decir de la industria fabricante de armamento, especialmente la dedicada a construir armamento pesado, vehículos blindados y acorazados.

También consumen mucho acero los grandes astilleros constructores de barcos especialmente petroleros, y gasistas u otros buques cisternas.

Como consumidores destacados de acero cabe citar a los fabricantes de automóviles porque muchos de sus componentes significativos son de acero.

A modo de ejemplo cabe citar los siguientes componentes del automóvil que son de acero:

Cabe destacar que cuando el automóvil pasa a desguace por su antigüedad y deterioro se separan todas las piezas de acero, son convertidas en chatarra y son reciclados de nuevo en acero mediante hornos eléctricos y trenes de laminación o piezas de fundición de hierro.

Cuando un técnico proyecta una estructura metálica, diseña una herramienta o una máquina, define las calidades y prestaciones que tienen que tener los materiales constituyentes. Como hay muchos tipos de aceros diferentes y, además, se pueden variar sus prestaciones con tratamientos térmicos, se establecen una serie de ensayos mecánicos para verificar principalmente la dureza superficial, la resistencia a los diferentes esfuerzos que pueda estar sometido, el grado de acabado del mecanizado o la presencia de grietas internas en el material, lo cual afecta directamente al material pues se pueden producir fracturas o roturas.

Hay dos tipos de ensayos, unos que pueden ser destructivos y otros no destructivos.

Todos los aceros tienen estandarizados los valores de referencia de cada tipo de ensayo al que se le somete.

Los ensayos no destructivos son los siguientes:


Los ensayos destructivos son los siguientes:


El consumo mundial de productos acabados de acero acabados en 2005 superó los mil millones de toneladas. La evolución del consumo resulta sumamente dispar entre las principales regiones geográficas. China registró un incremento del consumo aparente del 23% y representa en la actualidad prácticamente un 32% de la demanda mundial de acero. En el resto, tras un año 2004 marcado por un significativo aumento de los stocks motivado por las previsiones de incremento de precios, el ejercicio 2005 se caracterizó por un fenómeno de reducción de stocks, registrándose la siguiente evolución: −6% en Europa (UE25), −7% en Norteamérica, 0% en Sudamérica, +5% en CEI, +5% en Asia (excluida China), +3% en Oriente Medio.

La producción mundial de acero bruto en 2005 ascendió a 1129,4 millones de toneladas, lo que supone un incremento del 5,9% con respecto a 2004. Esa evolución resultó dispar en las diferentes regiones geográficas. El aumento registrado se debe fundamentalmente a las empresas siderúrgicas chinas, cuya producción se incrementó en un 24,6%, situándose en 349,4 millones de toneladas, lo que representa el 31% de la producción mundial, frente al 26,3% en 2004. Se observó asimismo un incremento en India (+16,7%). La contribución japonesa se ha mantenido estable. Asia en conjunto produce actualmente la mitad del acero mundial. Mientras que el volumen de producción de las empresas siderúrgicas europeas y norteamericanas se redujo en un 3,6% y un 5,3% respectivamente.

La distribución de la producción de acero en 2005 fue la siguiente según cifras estimadas por el International Iron and Steel Institute (IISI) en enero de 2006:

El acero, al igual que otros metales, puede ser reciclado. Al final de su vida útil, todos los elementos construidos en acero como máquinas, estructuras, barcos, automóviles, trenes, etc., se pueden desguazar, separando los diferentes materiales componentes y originando unos desechos seleccionados llamados comúnmente chatarra. La misma es prensada en bloques que se vuelven a enviar a la acería para ser reutilizados. De esta forma se reduce el gasto en materias primas y en energía que deben desembolsarse en la fabricación del acero. Se estima que la chatarra reciclada cubre el 40% de las necesidades mundiales de acero (cifra de 2006).

El proceso de reciclado se realiza bajo las normas de prevención de riesgos laborales y las medioambientales. El horno en que se funde la chatarra tiene un alto consumo de electricidad, por lo que se enciende generalmente cuando la demanda de electricidad es menor. Además, en distintas etapas del reciclaje se colocan detectores de radiactividad, como por ejemplo en la entrada de los camiones que transportan la chatarra a las industrias de reciclaje.

El personal que manipula chatarra debe estar siempre vacunado contra la infección del tétanos, pues puede infectarse al sufrir alguna herida con la chatarra. Cualquier persona que sufra un corte con un elemento de acero, debe acudir a un centro médico y recibir dicha vacuna, o un refuerzo de la misma si la recibió con anterioridad.





</doc>
<doc id="11565" url="https://es.wikipedia.org/wiki?curid=11565" title="Mineral">
Mineral

Un mineral es una sustancia natural, de composición química definida, normalmente sólido e inorgánico, y que tiene una cierta estructura cristalina. Es diferente de una roca, que puede ser un agregado de minerales o no minerales y que no tiene una composición química específica. La definición exacta de un mineral es objeto de debate, especialmente con respecto a la exigencia de ser abiogénico, y en menor medida, a que debe tener una estructura atómica ordenada. El estudio de los minerales se llama mineralogía. 

Hay más de minerales conocidas, de ellas más de 5090 aprobadas por la Asociación Internacional de Mineralogía ("International Mineralogical Association", o IMA). Continuamente se descubren y describen nuevos minerales, entre 50 y 80 al año. La diversidad y abundancia de especies minerales es controlada por la química de la Tierra. El silicio y el oxígeno constituyen aproximadamente el 75% de la corteza terrestre, lo que se traduce directamente en el predominio de los minerales de silicato, que componen más del 90% de la corteza terrestre. Los minerales se distinguen por diversas propiedades químicas y físicas. Diferencias en la composición química y en la estructura cristalina distinguen varias especies, y estas propiedades, a su vez, están influidas por el entorno geológico de la formación del mineral. Cambios en la temperatura, la presión, o en la composición del núcleo de una masa de roca causan cambios en sus minerales.
Los minerales pueden ser descritos por varias propiedades físicas que se relacionan con su estructura química y composición. Las características más comunes que los identifican son la estructura cristalina y el hábito, la dureza, el lustre, la diafanidad, el color, el rayado, la tenacidad, la exfoliación, la fractura, la partición y la densidad relativa. Otras pruebas más específicas para la caracterización de ciertos minerales son el magnetismo, el sabor o el olor, la radioactividad y la reacción a los ácidos fuertes.

Los minerales se clasifican por sus componentes químicos clave siendo los dos sistemas dominantes la clasificación de Dana y la clasificación de Strunz. La clase de silicatos se subdivide en seis subclases según el grado de polimerización en su estructura química. Todos los silicatos tienen una unidad básica en forma de tetraedro de sílice , es decir, un catión de silicio unido a cuatro aniones de oxígeno. Estos tetraedros pueden ser polimerizados para dar las subclases: neosilicatos (no polimerizados, y por lo tanto, solo tetraedros), sorosilicatos (dos tetraedros enlazadados entre sí), ciclosilicatos (anillos de tetraedros), inosilicatos (cadenas de tetraedros), filosilicatos (láminas de tetraedros), y tectosilicatos (redes en tres dimensiones de tetraedros). Otros grupos minerales importantes son los elementos nativos, sulfuros, óxidos, haluros, carbonatos, sulfatos y fosfatos.

La definición general de un mineral comprende los siguientes criterios:


Las tres primeras características generales son menos debatidas que las dos últimas. El primer criterio significa que un mineral se tiene que formar por un proceso natural, lo que excluye compuestos antropogénicos. La estabilidad a temperatura ambiente, en el sentido más simple, es sinónimo de que el mineral sea sólido. Más específicamente, un compuesto tiene que ser estable o metaestable a 25°C. Son ejemplos clásicos de excepciones a esta regla el mercurio nativo, que cristaliza a -39°C, y el hielo de agua, que es sólido solo por debajo de 0°C; puesto que estos dos minerales se habían descrito con anterioridad a 1959, fueron adoptados por la Asociación Internacional de Mineralogía (IMA). Los avances modernos suponen un amplio estudio de los cristales líquidos, que también concierne ampliamente a la mineralogía. Los minerales son compuestos químicos, y, como tales, pueden ser descritos por una fórmula fija o una variable. Muchos grupos de minerales y especies están compuestos por una solución sólida; las sustancias puras generalmente no se encuentran debido a la contaminación o sustitución química. Por ejemplo, el grupo del olivino se describe por la fórmula variable , que es una solución sólida de dos especies de miembro extremo, la forsterita rica en magnesio y la fayalita rica en hierro, que se describen mediante una fórmula química fija. Otras especies minerales podrían tener composiciones variables, tales como el sulfuro de mackinawita, , que es principalmente un sulfuro ferroso, pero que tiene una impureza de níquel muy significativa que se refleja en su fórmula.

El requisito de que una especie mineral para ser válida ha de ser abiogénica también se ha descrito como similar a que sea inorgánica; sin embargo, este criterio es impreciso y a los compuestos orgánicos se les ha asignado una rama de clasificación separada. Por último, la exigencia de tener una disposición atómica ordenada es generalmente sinónimo de cristalinidad; sin embargo, los cristales también son periódicos, por lo que se utiliza en su lugar el criterio más amplio. Una disposición atómica ordenada da lugar a una variedad de propiedades físicas macroscópicas, como la forma cristalina, la dureza y la exfoliación. Ha habido varias propuestas recientes para modificar la definición para considerar las sustancias biogénicas o amorfas como minerales. La definición formal de un mineral aprobada por la IMA en 1995:

Además, las sustancias biogénicas fueron excluidas explícitamente: 

Los sistemas de clasificación de minerales y sus definiciones están evolucionando para recoger los últimos avances de la ciencia mineral. Los cambios más recientes han sido la adición de una clase orgánica, tanto en el nuevo Dana y en los esquemas de la clasificación de Strunz. La clase orgánica incluye un grupo muy raro de minerales con hidrocarburos. La «Comisión sobre nuevos minerales y nombres de minerales» de la IMA aprobó en 2009 un esquema jerárquico para la denominación y clasificación de los grupos minerales y de los nombres de los grupos y estableció siete comisiones y cuatro grupos de trabajo para revisar y clasificar los minerales en una lista oficial de sus nombres publicados. De acuerdo con estas nuevas reglas,

La exclusión de Nickel (1995) de las sustancias biogénicas no fue universalmente respetada. Por ejemplo, Lowenstam (1981) declaró que «los organismos son capaces de formar una gran variedad de minerales, algunos de los cuales no se pueden formar inorgánicamente en la biosfera.»La distinción es una cuestión de clasificación y tiene menos que ver con los constituyentes de los minerales mismos. Skinner (2005) considera todos los sólidos como minerales potenciales e incluye los biominerales en el reino mineral, que son aquellos creados por las actividades metabólicas de los organismos. Skinner amplió la definición previa de un mineral para clasificar como mineral cualquier «elemento o compuesto, amorfo o cristalino, formado a través de los procesos biogeoquímicos».

Los recientes avances en la genéticas de alta resolución y espectroscopía de absorción de rayos X están proporcionando revelaciones sobre las relaciones biogeoquímicas entre microorganismos y minerales que pueden hacer obsoleta la exclusión biogénica de Nickel (1995) y una necesidad la inclusión biogénica de Skinner (2005). Por ejemplo, el IMA encargó al «Grupo de trabajo de Mineralogía ambiental y Geoquímica» tratar de los minerales en la hidrosfera, atmósfera y biosfera. El alcance del grupo incluye microorganismos formadores de minerales, que existen en casi todas las rocas, en el suelo y en la superficie de las partículas que atraviesan el globo hasta una profundidad de al menos 1600 metros por debajo del fondo del mar y 70 kilómetros en la estratosfera (posiblemente se introduzcan en la mesosfera). Los ciclos biogeoquímicos han contribuido a la formación de minerales durante miles de millones de años. Los microorganismos pueden precipitar los metales de la disolución, contribuyendo a la formación de yacimientos de mineral. También pueden catalizar la disolución de los minerales.

Antes de la lista de la Asociación Internacional de Mineralogía, más de 60 biominerales ya habían sido descubiertos, nombrados y publicados. Estos minerales (un subconjunto tabulado en Lowenstam (1981)) se consideran propiamente minerales de acuerdo con la definición de Skinner (2005). Estos biominerales no figuran en la lista oficial de nombres de minerales de la IMA, aunque muchos de estos biominerales representativos se distribuyen entre las 78 clases minerales que figuran en la clasificación de Dana. Otra clase rara de minerales (principalmente de origen biológico) incluye los cristales líquidos minerales que tienen propiedades tanto de líquidos y cristales. Hasta la fecha se han identificado más de 80.000 compuestos cristalinos líquidos.

La definición de mineral de Skinner (2005) toma en cuenta esta cuestión afirmando que un mineral puede ser cristalino o amorfo, incluyendo en este último grupo los cristales líquidos. Aunque los biominerales y los cristales líquidos no son la forma más común de minerales, ayudan a definir los límites de lo que constituye propiamente un mineral. La definición formal de Nickel (1995) menciona explícitamente la cristalinidad como una clave para la definición de una sustancia como un mineral. Un artículo de 2011 define la icosahedrita, una aleación de hierro-cobre-aluminio, como mineral; llamada así por su singular simetría icosaédrica natural, es un cuasi cristal. A diferencia de un verdadero cristal, los cuasicristales están ordenados pero no de forma periódica.

Los minerales no son equivalentes a las rocas. Una roca puede ser un agregado de uno o más minerales, o no tener ningún mineral. Rocas como la caliza o la cuarcita se componen principalmente de un mineral —calcita o aragonito en el caso de la caliza, y cuarzo, en la última. Otras rocas pueden ser definidas por la abundancia relativa de los minerales clave (esenciales); un granito está definido por las proporciones de cuarzo, feldespato alcalino y plagioclasa. Los otros minerales de la roca se denominan accesorios, y no afectan en gran medida la composición global de la roca. Las rocas también pueden estar compuestas enteramente de material no mineral; el carbón es una roca sedimentaria compuesta principalmente de carbono derivado de manera orgánica.

En las rocas, algunas especies y grupos minerales son mucho más abundantes que otros; estos se denominan minerales formativos. Los principales ejemplos son el cuarzo, feldespatos, las micas, los anfíboles, los piroxenos, los olivinos, y la calcita; excepto la última, todos son minerales silicatos. En general, alrededor de unos 150 minerales se consideran particularmente importantes, ya sea en términos de su abundancia o valor estético en términos de coleccionismo.

Los minerales y rocas comercialmente valiosos se conocen como minerales industriales y rocas industriales. Por ejemplo, la moscovita, una mica blanca, puede ser utilizada para ventanas (a veces conocida como isinglass), como material de relleno o como un aislante. Las menas son minerales que tienen una alta concentración de un determinado elemento, normalmente de un metal. Ejemplos de ello son el cinabrio (), un mineral de mercurio, esfalerita (), un mineral de zinc, o la casiterita (), un mineral de estaño. Las gemas son minerales con un alto valor ornamental, y se distinguen de las no gemas por su belleza, durabilidad, y por lo general, rareza. Hay alrededor de 20 especies minerales que se califican como minerales gema, que constituyen alrededor de las 35 piedras preciosas más comunes. Los minerales gema están a menudo presentes en diversas variedades, y así un mineral puede dar cuenta de varias piedras preciosas diferentes; por ejemplo, rubí y el zafiro son ambas corindón, .

Los minerales se solían clasificar en la antigüedad con criterios de su aspecto físico; Teofrasto, en el siglo III a. C., creó la primera lista sistemática cualitativa conocida; Plinio el Viejo (siglo I), en su “Historia Natural”, realizó una sistemática mineral, trabajo que, en la Edad Media, sirvió de base a Avicena; Linneo (1707-1778) intentó idear una nomenclatura fundándose en los conceptos de género y especie, pero no tuvo éxito y dejó de usarse en el siglo XIX; con el posterior desarrollo de la química, el químico sueco Axel Fredrik Cronstedt (1722-1765) elaboró la primera clasificación de minerales en función de su composición; el geólogo estadounidense James Dwight Dana, en 1837, propuso una clasificación considerando la estructura y composición química. La clasificación más actual se funda en la composición química y la estructura cristalina de los minerales. Las clasificaciones más empleadas son las de Strunz y Kostov.

Los minerales se clasifican según la variedad, especie, serie y grupo, en orden creciente de generalidad. El nivel básico de definición es el de las especies minerales, que se distinguen de otras especies por sus propiedades químicas y físicas específicas y únicas. Por ejemplo, el cuarzo se define por su fórmula química, , y por una estructura cristalina específica que lo distingue de otros minerales con la misma fórmula química (denominados polimorfos). Cuando existe un rango de composición entre dos especies minerales, se define una serie mineral. Por ejemplo, la serie de la biotita está representada por cantidades variables de la endmembers flogopita, siderofilita, annita, y eastonita. Por contraste, un grupo mineral es una agrupación de especies minerales con algunas propiedades químicas comunes que comparten una estructura cristalina. El grupo piroxeno tiene una fórmula común de , en donde X e Y son ambos cationes, siendo X generalmente mayor que Y (radio iónico); los piroxenos son silicatos de cadena sencilla que cristalizan en cualquiera de los sistemas cristalinos monoclínico o ortorrómbico. Finalmente, una variedad mineral es un tipo específico de especies minerales que difieren por alguna característica física, como el color o el hábito del cristal. Un ejemplo es la amatista, que es una variedad púrpura del cuarzo.

Para ordenar minerales dos son las clasificaciones más comunes, la de Dana y la de Strunz, ambas basadas en la composición, en especial respecto a los grupos químicos importantes, y en la estructura. James Dwight Dana, un geólogo principal de su tiempo, publicó por primera vez su "System of Mineralogy" [Sistema de Mineralogía] en 1837; en 1997 se editó su octava edición. La clasificación de Dana asigna un número de cuatro partes a una especie mineral. Su número de clase se basa en los grupos de composición importantes; el número de tipo da la relación de cationes/aniones en el mineral; y los dos últimos números corresponden al grupo de minerales por similitud estructural dentro de un tipo o clase determinada. La clasificación de Strunz —utilizada con menor frecuencia y llamada así por el mineralogista alemán Karl Hugo Strunz— se basa en el sistema de Dana, pero combina tanto criterios químicos como estructurales, estos últimos con respecto a la distribución de los enlaces químicos.

En enero de 2016, la IMA había aprobado 5.090 especies minerales. Se han nombrado en general en honor de una persona (45%) — ver: —, seguidos por la ubicación del lugar, mina o yacimiento del descubrimiento (23%); otras etimologías comunes son los nombres basados en la composición química (14%) y en las propiedades físicas (8%). El sufijo común "-ita" usado en los nombres de las especies minerales desciende del antiguo sufijo griego - ί τ η ς (-ites), que significa 'relacionado con' o 'que pertenece a'.

La abundancia y diversidad de minerales es controlada directamente por su composición química, que a su vez, depende de la abundancia de los elementos en la Tierra. La mayoría de los minerales observados derivan de la corteza terrestre. Ocho elementos representan la mayor parte de los componentes clave de los minerales, debido a su abundancia en la corteza terrestre. Estos ocho elementos suponen más del 98% de la corteza en peso, y son, en orden decreciente: oxígeno, silicio, aluminio, hierro, magnesio, calcio, sodio y potasio. El oxígeno y el silicio son, con mucho, los dos más importantes —el oxígeno compone, en peso, el 46,6% de la corteza terrestre, y el silicio un 27,7%.

Los minerales que se forman son controlados directamente por la química mayor del cuerpo matriz. Por ejemplo, un magma rico en hierro y magnesio formará minerales máficos, como el olivino y los piroxenos; por el contrario, un magma más rico en sílice cristalizará para formar minerales que incorporen más , como los feldespatos y cuarzos. La caliza, la calcita o la aragonita (todas ) se forman porque la roca es rica en calcio y carbonato. Un corolario es que no se encontrará un mineral en una roca cuya química mayor no se parezca a la química mayor del mineral dado, con la excepción de algunas trazas de minerales. Por ejemplo, la cianita, , se forma a partir del metamorfismo de lutitas ricas en aluminio; no sería probable que ocurriera en rocas pobres en aluminio, como la cuarcita.

La composición química puede variar entre las especies terminales de una serie de solución sólida. Por ejemplo, los feldespatos plagioclasa comprenden una serie continua que va desde el miembro extremo de la albita, rica en sodio (), hasta la anortita, rica en calcio (), con cuatro variedades intermedias reconocidas entre ellas (recogidas en orden de riqueza del sodio al calcio): oligoclasa, andesina, labradorita y bytownita. Otros ejemplos de serie son la serie del olivino, desde la forsterita, rica en magnesio, a la fayalita, rica en hierro, y la serie del wolframita, desde la hübnerita, rica en manganeso, hasta la ferberita, rica en hierro.

La sustitución química y la coordinación de poliedros explican esta característica común de los minerales. En la naturaleza, los minerales no son sustancias puras, y se contaminan por otros elementos que están presentes en el sistema químico dado. Como resultado, es posible que un elemento sea sustituido por otro. La sustitución química se producirá entre iones de un tamaño y carga similares; por ejemplo, no sustituirá a debido a las incompatibilidades químicas y estructurales causadas por la gran diferencia en tamaño y carga. Un ejemplo común de sustitución química es el del > por , que están próximos en carga, tamaño y abundancia en la corteza terrestre. En el ejemplo de la plagioclasa, hay tres casos de sustitución. Los feldespatos son todos armazones de sílice, que tienen una relación de silicio-oxígeno de 2:1, y el espacio para otros elementos se da por la sustitución del ion por el ion para dar una unidad de base de ; sin la sustitución, la fórmula puede ser cargada-equilibrada como , dando cuarzo. La importancia de esta propiedad estructural se explica además por los poliedros de coordinación. La segunda sustitución se produce entre el ion y el ion ; sin embargo, la diferencia en la carga tiene que contabilizarse haciendo una segunda sustitución del ion por el ion .

La coordinación de poliedros es una representación geométrica de cómo un catión está rodeado por un anión. En mineralogía, debido a su abundancia en la corteza terrestre, los poliedros de coordinación se consideran generalmente en términos del oxígeno. La unidad base de los minerales de silicato es el tetraedro de sílice —un ion rodeado de cuatro —. Una forma alternativa de describir la coordinación del silicato es mediante un número: en el caso del tetraedro de sílice, se dice que tiene un número de coordinación de 4. Diversos cationes tienen un rango específico de posibles números de coordinación; para el silicio, es casi siempre 4, excepto para minerales de muy altas presiones en los que los compuestos se comprimen de tal manera que el silicio está seis veces (octaédrico) coordinado con el oxígeno. Los cationes mayores tienen un número de coordinación más grande debido al aumento en el tamaño relativo en comparación con el oxígeno (la última subcapa orbital de los átomos más pesados es diferente también). Los cambios en los números de coordinación conduce a diferencias físicas y mineralógicas; por ejemplo, a alta presión, tal como en el manto, muchos minerales, especialmente algunos silicatos como el olivino y los granates cambiarán a una estructura de perovskita, en el que el silicio está en coordinación octaédrica. Otro ejemplo son los aluminosilicatos cianita, andalucita y silimanita (polimorfos, ya que comparten la fórmula ), que se diferencian por el número de coordinación del ; estos minerales transitan de uno al otro como una respuesta a los cambios en la presión y en la temperatura. En el caso de materiales de silicato, la sustitución del ion por permite una variedad de minerales, debido a la necesidad de equilibrar las cargas.

Los cambios de temperatura, de presión y de composición alteran la mineralogía de una roca simple: los cambios en la composición pueden ser causados por procesos como la erosión o metasomatismo (alteración hidrotérmica); los cambios en la temperatura y en la presión se producen cuando la roca madre se somete a movimientos tectónicos o magmáticos en diferentes regímenes físicos; y los cambios en las condiciones termodinámicas favorecen que algunas asociaciones de minerales reaccionen entre sí para producir nuevos minerales. Como tal, es posible que dos rocas tengan una química de roca base idéntica, o muy similar, sin tener una mineralogía similar. Este proceso de alteración mineralógica está relacionado con el ciclo de las rocas. Un ejemplo de una serie de reacciones minerales se ilustra como sigue.

El feldespato ortoclasa () es un mineral que se encuentra comúnmente en el granito, una roca ígnea plutónica. Cuando se expone a la intemperie, reacciona para formar caolinita (, un mineral sedimentario, y ácido silícico):

Bajo condiciones metamórficas de bajo grado, la caolinita reacciona con el cuarzo para formar pirofilita ():

A medida que aumenta el grado metamórfico, la pirofilita reacciona para formar cianita y cuarzo:

Alternativamente, un mineral puede cambiar su estructura cristalina como consecuencia de cambios de temperatura y de presión sin reaccionar. Por ejemplo, el cuarzo se convertirá en una variedad de sus polimorfos de , como la tridimita y la cristobalita a altas temperaturas, y en coesita a altas presiones.

La caracterización de los minerales puede variar de ser muy simple a muy difícil. Un mineral puede ser identificado por varias propiedades físicas, siendo algunos de ellas suficientes para una plena identificación sin ambigüedades. En otros casos, los minerales sólo se pueden clasificar mediante análisis más complejos, ópticos, químicos o de difracción de rayos X; estos métodos, sin embargo, pueden ser costosos y consumen mucho tiempo. Las propiedades físicas que se estudian para la clasificación son la estructura cristalina y el hábito, la dureza y el lustre, la diafanidad, el color, el rayado, la exfoliación y la fractura, y la densidad relativa. Otras pruebas menos generales son la fluorescencia y fosforescencia, el magnetismo, la radioactividad, la tenacidad (respuesta a los cambios mecánicos inducidos de forma), la piezoelectricidad y la reactividad para diluir ácidos.

La estructura cristalina resulta de la disposición espacial geométrica ordenada de los átomos en la estructura interna de un mineral. Esta estructura cristalina se basa en una disposición atómica o iónica interna regular, que se expresa a menudo en la forma geométrica que el cristal toma. Incluso cuando los granos minerales son demasiado pequeños para ser vistos o son de forma irregular, la estructura cristalina subyacente siempre es periódica y se puede determinar por difracción de rayos X. Los minerales por lo general son descritos por su contenido de simetría. Los cristales están cristalográficamente restringidos a 32 grupos de puntos, que se diferencian por su simetría. Estos grupos se clasifican a su vez en categorías más amplias, siendo las de mayor alcance seis familias de cristales. (a veces una de las familias, la hexagonal, también se divide en dos "sistemas" cristalinos: el trigonal, que tiene un eje tres veces simétrico, y el hexagonal, que tiene un eje seis veces simétrico.)

Estas familias pueden ser descritas por las longitudes relativas de los tres ejes cristalográficos, y los ángulos que forman entre ellos; estas relaciones corresponden a las operaciones de simetría que definen los grupos de puntos más estrechos. Se resumen a continuación; a, b, y c representan los ejes, y α, β, y γ representan el ángulo opuesto al eje cristalográfico respectivo (por ejemplo, α es el ángulo opuesto al eje a, es decir el ángulo entre los ejes b y c.):
La química y la estructura cristalina, en conjunto, definen un mineral. Con una restricción a grupos de 32 puntos, los minerales de diferente química pueden tener una estructura cristalina idéntica. Por ejemplo, la halita (), la galena () y la periclasa () pertenecen todas al grupo de puntos hexaoctahedral (familia isométrica), ya que tienen una estequiometría similar entre sus diferentes elementos constitutivos. En contraste, los polimorfos son agrupaciones de minerales que comparten una fórmula química, pero que tienen una estructura diferente. Por ejemplo, la pirita y la marcasita, ambos sulfuros de hierro, tienen la fórmula ; sin embargo, el primero es isométrico mientras que el último es ortorrómbico. Este polimorfismo se extiende a otros sulfuros de fórmula genérica ; estos dos grupos son conocidos colectivamente como los grupos de la pirita y marcasita.

El polimorfismo se puede extender más allá del contenido de la pura simetría. Los aluminosilicatos son un grupo de tres minerales —cianita, andalucita y silimanita— que comparten la fórmula química . La cianita es triclınica, mientras que la andalucita y la silimanita son ambas ortorrómbicas y pertenecen al grupo de puntos bipiramidal. Estas diferencias surgen en correspondencia a cómo el aluminio se coordina dentro de la estructura cristalina. En todos los minerales, un ion de aluminio está siempre seis veces coordinado con el oxígeno; el silicio, por regla general está en coordinación de cuatro veces en todos los minerales; una excepción es un caso como la stishovita (, un polimorfo de cuarzo de ultra-alta presión con estructura de rutilo). En la cianita, el segundo aluminio está en coordinación seis veces; su fórmula química se puede expresar como , para reflejar su estructura cristalina. La andalucita tiene el segundo aluminio en coordinación cinco veces () y la silimanita lo tiene en coordinación de cuatro veces (().

Las diferencias en la estructura cristalina y la química influyen mucho en otras propiedades físicas del mineral. Los alótropos del carbono, el diamante y el grafito, tienen propiedades muy distintas; el diamante es la sustancia natural más dura, tiene un lustre adamantino, y pertenece a la familia isométrica, mientras que el grafito es muy blando, tiene un lustre grasiento, y cristaliza en la familia hexagonal. Esta diferencia se explica por diferencias en el enlace. En el diamante, los átomos de carbono están en orbitales híbridos sp, lo que significa que forman un marco o armazón en el que cada carbono está unido covalentemente a cuatro vecinos de una manera tetraédrica. Por otro lado, el grafito forma láminas de átomos de carbono en orbitales híbridos sp, en los que cada átomo de carbono está unido covalentemente a sólo otros tres. Estas hojas se mantienen unidas por fuerzas mucho más débiles que las fuerzas de van der Waals, y esta discrepancia se traduce en grandes diferencias macroscópicas.

La macla es la interpenetración entre dos o más cristales de una única especie mineral. La geometría de la macla está controlada por la simetría del mineral y, como resultado, hay varios tipos: de contacto, reticuladas, geniculadas, de penetración, cíclicas y polisintéticas. Las maclas de contacto, o maclas simples, constan de dos cristales unidos en un plano; este tipo de maclas es común en la espinela; las maclas reticuladas, comunes en forma de rutilo, son cristales entrelazados que se asemejan a un reticulado. Las maclas geniculadas tienen una mezcla en el medio que es causada por el comienzo del maclado. Las maclas de penetración constan de dos cristales individuales que han crecido uno dentro de otro; ejemplos de este hermanamiento son las maclas en forma de cruz de la estaurolita y las maclas de Carlsbad en la ortoclasa. Las maclas cíclicas son causadas por el maclado repetido en torno a un eje de rotación. Se produce alrededor de tres, cuatro, cinco, seis, o ocho ejes de plegado. Las maclas polisintéticas son similares a las maclas cíclicas por la presencia de maclados repetitivos aunque, en lugar de producirse alrededor de un eje de rotación, lo hacen siguiendo planos paralelos, por lo general en una escala microscópica.

El hábito cristalino se refiere a la forma general de cristal. Se utilizan varios términos para describir esta propiedad: acicular, que describe cristales en forma de aguja como en la natrolita; acuchillado; arborescente o dendrítica (patrón de árbol, común en el cobre nativo); equante, que es típico del granate; prismático (alargado en una dirección); y tabular, que se diferencia de acuchillado en que el primero es plano mientras que este último tiene un alargamiento definido. En relación con la forma cristalina, la calidad de las caras del cristal es diagnóstico de algunos minerales, especialmente con un microscopio petrográfico. Los cristales euhedrales tienen una forma externa definida, mientras que los cristales anhedrales no la tienen; las formas intermedias se denominan subhedrales.

La dureza de un mineral define cuánto puede resistir el rayado. Esta propiedad física depende de la composición química y de la estructura cristalina, y por ello no es necesariamente constante en todas las cara; la debilidad cristalográfica hace que algunas direcciones sean más blandas que otras. Un ejemplo de esta propiedad se muestra en la cianita, que tiene una dureza de Mohs de 5½ en la dirección paralela a [001], pero de 7 paralela a [100].

La escala más común de medición es la escala de dureza de Mohs ordinaria. Definida por diez indicadores, un mineral con un índice más alto raya los minerales que están por debajo de él en la escala. La escala va desde el talco, un silicato estratificado, hasta el diamante, un polimorfo de carbono que es el material natural más duro.

El lustre o brillo indica cómo se refleja la luz que incide sobre la superficie del mineral, una propiedad que no depende del color y sí de su naturaleza química: es más intenso en sustancias que tienen enlaces metálicos y menor en las de enlaces iónicos o covalentes. El tipo y la intensidad del brillo dependen del índice de refracción y de la relación entre la luz absorbida y la reflejada. Hay numerosos términos cualitativos para su descripción, que se agrupan en tres:


La diafanidad de un mineral describe la capacidad de la luz de pasar a través de él. Los minerales transparentes no disminuyen la intensidad de la luz que pasa a través de ellos. Un ejemplo de estos minerales es la moscovita (mica de potasio); algunas variedades son lo suficientemente claras como para haber sido utilizadas como vidrios en las ventanas. Los minerales translúcidos permiten pasar algo de luz, pero menos que los que son transparentes. La jadeíta y nefrita (formas minerales del jade) son ejemplos de minerales con esta propiedad. Los minerales que no dejan pasar la luz se denominan opacos.

La diafanidad de un mineral depende del espesor de la muestra. Cuando un mineral es suficientemente delgado (por ejemplo, en una lámina delgada para petrografía) puede llegar a ser transparente, incluso si esa propiedad no se ve en la muestra de mano. Por el contrario, algunos minerales, como la hematita o la pirita son opacos incluso en láminas delgadas.

El color es la propiedad más obvia de un mineral, pero a menudo no sirve para caracterizarlo. Es causada por la radiación electromagnética que interactúa con los electrones (excepto en el caso de incandescencia, que no se aplica a los minerales). Por su contribución en el color, se definen tres grandes clases de minerales:




Algunos metales, como el hierro, pueden ser tanto alocromático como idiocromático: en el primer caso es considerado como una impureza, mientras que en el segundo forma parte intrínseca del mineral coloreado.

El color de algunos minerales puede cambiar, ya sea de manera natural o con un poco de ayuda. Los bajos niveles de radiación, que se dan a menudo en la naturaleza, pueden contribuir a oscurecer algunos minerales incoloros. Los mismos berilos de color amarillo verdoso se tratan artificilamente ahora con calor para darles una coloración más azulada.

Además del simple color del cuerpo, los minerales pueden tener otras propiedades ópticas distintivas que pueden implican variabilidad del color:
 
La raya de un mineral se refiere al color de un mineral en forma de polvo, que puede o no ser idéntico al color de su cuerpo. La forma más común de evaluar esta propiedad se hace con una placa de raya, que está hecha de porcelana y es de color blanco o negro. La raya de un mineral es independiente de los elementos traza o de cualquier alteración de la superficie a causa de la intemperie. Un ejemplo común de esta propiedad se ilustra con la hematita, que es de color negro, plata o rojo en la muestra, pero que tiene una raya de color rojo cereza. a marrón rojizo. La raya es más a menudo distintiva de los minerales metálicos, en contraste con los minerales no metálicos, cuyo color de cuerpo está creada por elementos alocromáticos. La prueba de la raya se ve limitada por la dureza del mineral, ya que los minerales de dureza superior a siete rayan ellos la placa.

Por definición, los minerales tienen una disposición atómica característica y cualquier debilidad de esa estructura cristalina es la causa de la existencia de los planos de debilidad. La rotura del mineral a lo largo de esos planos se denomina exfoliación. La calidad de la exfoliación puede ser descrita en función de cómo de limpia y fácilmente se rompa el mineral; los términos con los que se describen comúnmente esa calidad, en orden decreciente , son «perfecto», «bueno», «distinto» y «pobre». En particular en los minerales transparentes, o en una sección delgada, la exfoliación se puede ver como una serie de líneas paralelas que señalan las superficies planas cuando se ven de lado. La exfoliación no es una propiedad universal de los minerales; por ejemplo, el cuarzo, compuesto por tetraedros de sílice muy interconectados, no tiene ninguna debilidad cristalográfica que le permitiría exfoliarse. Por el contrario, las micas, que tienen una exfoliación basal perfecta, consisten en láminas de tetraedros de sílice que se mantienen juntas muy débilmente .
Como la exfoliación es función de la cristalografía, hay gran variedad de tipos de exfoliación produciéndose en uno, dos, tres, cuatro o seis direcciones. La exfoliación basal en una única dirección es una característica distintiva de las micas. La exfoliación en dos direcciones, denominada prismática, se produce en anfíboles y piroxenos. Los minerales como la galena o la halita tienen exfoliación cúbica (o isométrica) en tres direcciones, a 90°; cuando hay tres direcciones de exfoliación, pero no a 90°, como en la calcita o en la rodocrosita, se denomina exfoliación romboédrica. La exfoliación octaédrica (cuatro direcciones) está presente en la fluorita y en el diamante, y la esfalerita tiene seis direcciones de exfoliación del dodecaedro.

Los minerales con muchas exfoliaciones pueden no romper igual de bien en todas las direcciones; por ejemplo, la calcita tiene buena exfoliación en tres direcciones, pero el yeso solo tiene una exfoliación perfecta en una dirección, y pobre en las otras dos. Los ángulos entre los planos de exfoliación varían entre los minerales. Por ejemplo, dado que los anfíboles son silicatos de cadena doble y los piroxenos son silicatos de cadena única, el ángulo entre sus planos de exfoliación es diferente: los piroxenos exfolian en dos direcciones a aproximadamente 90°, mientras que los anfíboles lo hacen claramente en dos direcciones separadas aproximadamente a 120° y 60°. Los ángulos de exfoliación se pueden medir con un goniómetro de contacto, que es similar a un transportador.

La partición, a veces llamada "falsa exfoliación", es similar en apariencia a la exfoliación pero se produce por defectos estructurales en el mineral en lugar de por una debilidad sistemática. La partición varía de cristal a cristal de un mismo mineral, mientras que todos los cristales de un mineral determinado exfoliaran si la estructura atómica permite tal propiedad. En general, la partición es causada por una cierta tensión aplicada a un cristal. Las fuentes de las tensiones incluyen la deformación (por ejemplo, un aumento de la presión), exsolution o maclado. Los minerales que a menudo muestran partición son los piroxenos, la hematita, la magnetita y el corindón.

Cuando un mineral se rompe en una dirección que no corresponde a un plano de exfoliación, se habla de fractura. Hay varios tipos:


La tenacidad está relacionada tanto con la exfoliación y la fractura. Mientras que la fractura y la exfoliación describen las superficies que se crean cuando el mineral se rompe, la tenacidad describe la resistencia que ofrece el mineral a tal ruptura. Los minerales pueden ser:


La densidad relativa (a veces llamada gravedad específica) describe numéricamente la densidad de un mineral. Las dimensiones de la densidad son unidades de masa divididas por unidades de volumen: kg/m³ o en g/cm³. La densidad relativa mide la cantidad de agua desplazada por una muestra mineral. Se define como el cociente de la masa de la muestra y la diferencia entre el peso de la muestra en el aire y su correspondiente peso en agua; la densidad relativa es una relación adimensional, sin unidades. Para la mayoría de los minerales, esta propiedad no sirve para caracterizarlos. Los minerales que forman las rocas —normalmente silicatos y occasionalmente carbonatos— tienen una densidad relativa de 2.5–3.5.<

Una alta densidad relativa si permite diagnosticar algunos minerales. La variación química (y por consiguiente, en la clase mineral) se correlaciona con un cambio en la densidad relativa. Entre los minerales más comunes, los óxidos y sulfuros tienden a tener una alta densidad relativa, ya que incluyen elementos con mayor masa atómica. Una generalización es que los minerales metálicos o con brillo diamantino tienden a tener densidades relativas más altas que las que tienen los minerales no-metálicos o de brillo mate. Por ejemplo, la hematita, , tiene una densidad relativa de 5.26 mientras que la galena, , tiene una gravedad específica de 7.2–7.6, que es el resultado de su alto contenido en hierro y en plomo, respectivamente. La densidad relativa es muy alta en los metales nativos; la kamacita, una aleación de hierro-níquel común en los meteoritos de hierro, tiene una densidad relativa de 7.9, y el oro tiene una densidad relativa observada entre 15 y 19.3.

Se pueden utilizar otras propiedades para identificar minerales, aunque son menos generales y solo aplicables a ciertos minerales.

La inmersión en ácido diluido (a menudo en HCl al 10%) ayuda a distinguir los carbonatos de otras clases de minerales. El ácido reacciona con el grupo del carbonato ([CO3] 2-), lo que causa que el área afectada sufra efervescencia, con desprendimiento de gas dióxido de carbono. Esta prueba se puede ampliar para poner a prueba el mineral en su forma original de cristal o en polvo. Un ejemplo de esta prueba se realiza para distinguir la calcita de la dolomita, especialmente dentro de las rocas (caliza y dolomía, respectivamente). La efervescencia de la calcita es inmediata en ácido, mientras que para que lo haga la dolomita el ácido debe aplicarse a muestras en polvo o sobre una superficie rayada en una roca. Los minerales de zeolita no sufren efervescencia en ácido; en vez de eso, se vuelven esmerilados después de 5-10 minutos, y si se dejan en ácido durante un día, se disuelven o se convierten en un gel de sílice.

El magnetismo es una propiedad muy notable de ciertos minerales. Entre los minerales comunes, la magnetita muestra esta propiedad con fuerza, y también está presente, aunque no con tanta intensidad, en la pirrotita y la ilmenita.

Algunos minerales también pueden identificarse mediante la prueba del sabor u olor. La halita, NaCl, es la sal de mesa; su homólogo de potasio, la silvita, tiene un sabor amargo pronunciado. Los sulfuros tienen un olor característico, sobre todo cuando las muestras están fracturadas, reaccionando o en polvo.

La radiactividad es una propiedad poco frecuente, aunque algunos minerales pueden integrar elementos radiactivos. Pueden ser constituyentes que los definen, como el uranio en la uraninita, la autunita y la carnotita, o como impurezas traza. En este último caso, la desintegración de los elementos radiactivos daña el cristal mineral; el resultado, denominado "halo radiactivo" o "halo pleocroico", es observable mediante diversas técnicas, en especial en las láminas finas de petrografía.

Dado que la composición de la corteza terrestre está dominada por el silicio y el oxígeno, los elementos con silicatos son, con mucho, la clase de minerales más importante en términos de formación de rocas y diversidad: la mayoría de las rocas se componen en más de un 95% de minerales de silicato, y más del 90% de la corteza terrestre está compuesta por estos minerales. Además de los componentes principales, silicio y oxígeno, son comunes en los minerales de silicato otros elementos comunes en la corteza terrestre, como el aluminio, el magnesio, el hierro, el calcio, el sodio y el potasio. Los silicatos más importantes que forman rocas son los feldespatos, los cuarzos, los olivinos, los piroxenos, los anfíboles, los granitos y las micas.

A su vez, los minerales no-silicatos se subdividen en varias clases por su química dominante: elementos nativos, sulfuros, haluros, óxidos e hidróxidos, carbonatos y nitratos, boratos, sulfatos, fosfatos y compuestos orgánicos. La mayoría de las especies minerales no silicatos son extremadamente raras (constituyen en total un 8% de la corteza terrestre), aunque algunas son relativamente comunes, como la calcita, pirita, magnetita y hematita. Hay dos estilos estructurales principales observados en los no-silicatos: el empaquetamiento compacto y los tetraedros enlazados como aparecen en los silicatos. Las estructuras compactas son una manera de empaquetar densamente átomos y reducir al mínimo el espacio intersticial. El empaquetado compacto hexagonal consiste en apilar capas en las que cada capa es la misma ("ababab"), mientras que el empaquetado cúbico consiste en grupos de apilamiento de tres capas ("abcabcabc"). Análogos a los tetraedros de sílice enlazados son los tetraesdros que forman los iones (sulfato), (fosfato), (arseniato), y (vanadato).

Los minerales no-silicatos tienen una gran importancia económica, ya que concentran más elementos que los minerales de silicato y se explotan especialmente como menas. 

Los silicatos son sales que combinan la sílice con otros óxidos metálicos. La base de la unidad de un mineral de silicato es el tetraedro : en la mayoría de casos, el silicio se encuentra coordinado cuatro veces, o en coordinación tetraédrica, con el oxígeno; en situaciones de muy altas presiones, el silicio estará coordinado seis veces, o en coordinación octaédrica, como en la estructura de perovskita o en el cuarzo polimorfo stishovita (). (En el último caso, el mineral ya no tiene una estructura de silicato, si no de rutilo () y su grupo asociado, que son óxidos simples.) Estos tetraedros de sílice son luego polimerizados en algún grado para crear otras estructuras, como cadenas unidimensionales, láminas bidimensionales o armazones tridimensionales. El mineral de un silicato básico sin polimerización de tetraedros requiere de otros elementos que equilibren la base cargada 4-. En las otras estructuras de silicato son varias las combinaciones de elementos que equilibran esa carga negativa. Es común que el sea sustituido por debido a la similitud en radio iónico y en carga; en otros casos, los tetraedros de forman las mismas estructuras que lo hacían los tetraedros no sustituidos, pero los requisitos del equilibrio de cargas son diferentes.

El grado de polimerización puede ser descrito tanto por la estructura formada como por el número de vértices tetraédricos (u oxígenos de coordinación) compartidos (por el aluminio y el silicio en sitios tetraédricos): los ortosilicatos (o nesosilicatos) no tienen ninguna vinculación de poliedros, así que los tetraedros no comparten vértices; los disilicatos (o sorosilicatos) tienen dos tetraedros que comparten un átomo de oxígeno; los inosilicatos son silicatos en cadena: los de cadena simple tienen dos vértices compartidos y los de cadena doble dos o tres; los filosilicatos forman una estructura de lámina que requiere tres oxígenos compartidos (en el caso de silicatos de cadena doble, algunos tetraedros deben compartir dos vértices en lugar de tres como harían si resultase una estructura de lámina); los silicatos "en armazón" o tectosilicatos, tienen tetraedros que comparten los cuatro vértices; los silicatos de anillo, o ciclosilicatos, solo necesitan tetraedros que compartan dos vértices para formar la estructura cíclica.

Se describen a continuación en orden decreciente de polimerización, las subclases de silicato.

Los tectosilicatos son muy abundantes, constituyendo aproximadamente el 64% de los minerales de la corteza terrestre.
También conocidos como silicatos de "estructura en armazón", tienen el grado de polimerización más alto y tienden a ser químicamente estables como resultado de la fuerza de los enlaces covalentes. Son ejemplos el cuarzo, los feldespatos, los feldespatoides, y las zeolitas. 

Tienen una estructura basada en un entramado tridimensional de tetraedros () con los cuatro vértices ocupados por el ion O compartidos, lo que implica relaciones Z:O=1:2. La Z es silicio (Si) (la fórmula resultante es , sílice), pero parte del puede ser reemplazado por (en raras ocasiones por , y ). Al suceder esto, las cargas negativas resultantes se compensan con la entrada de cationes grandes, como el , el o el (y con menos frecuencia , y ).También pueden tener aniones complementarios F, Cl, S, CO, SO.

El cuarzo () es la especie mineral más abundante, formando el 12% de la corteza terrestre. Se caracteriza por su alta resistividad química y física. Tiene varios polimorfos, incluyendo la tridimita y la cristobalita a altas temperaturas, la coesita a alta presión y la stishovita a ultra-alta presión. Este último mineral solo puede formarse en la Tierra por impacto de meteoritos, y su estructura está tan compuesta que había cambiado de una estructura de silicato a la de rutilo (). El polimorfo de sílice que es más estable en la superficie de la Tierra es el α-cuarzo. Su homólogo, el cuarzo-β, está presente solo a altas temperaturas y presiones (a 1 bar, cambia a cuarzo-α por debajo de 573°C). Estos dos polimorfos difieren en un "retorcimiento" de los enlaces; este cambio en la estructura da al cuarzo-β mayor simetría que al cuarzo-α, y por lo tanto también se les llama cuarzo alto (β) y cuarzo bajo (α).

Los feldespatos son el grupo más abundante en la corteza terrestre, en torno al 50%. En los feldespatos, los sustitutos de los crean un desequilibrio de carga que debe ser explicado por la adición de cationes. La estructura de base se convierte ya en , ya en . Hay 22 especies minerales de feldespatos, subdivididas en dos grandes subgrupos —alcalino y plagioclasa— y dos grupos menos comunes —celsiana y banalsita. Los feldespatos alcalinos son los más comunes en una serie que va desde la entre ortoclasa, rica en potasio, a la albita, rica en sodio; en el caso de las plagioclasas, la serie más común varía desde la albita a la anortita, rica en calcio. El maclado de cristales es común en los feldespatos, especialmente con maclas polisintéticas en las plagioclasas y maclas de Carlsbad en los feldespatos alcalinos. Si el último subgrupo se enfría lentamente a partir de una masa fundida, se forma laminillas de exsolution porque los dos componentes —ortoclasa y albita— son inestables en solución sólida. La exsolution puede darse desde una escala microscópica hasta ser fácilmente observable en la muestra de mano; se forma una textura pertitica cuando un feldespato rico en Na exsolve en un huésped rico en K. La textura opuesta (antipertitica), cuando un feldespato rico en K exsolve en un huésped rico en Na, es muy rara.
Los feldespatoides son estructuralmente similares a los feldespatos, pero se diferencian en que se forman en condiciones de carencia de silicio lo que permite una mayor sustitución por . Como resultado, los feldespatoides no se pueden asociar con cuarzo. Un ejemplo común de un feldespatoide es la nefelina (); comparada con los feldespatos alcalinos, la nefelina tiene una relación : de 1: 2, en lugar de 1:6 en el feldespato.

Las zeolitas a menudo tienen hábitos de cristal distintivos, produciendo agujas, placas o bloques masivos. Se forman en presencia de agua a bajas temperaturas y presiones, y tienen canales y huecos en su estructura. Las zeolitas tienen varias aplicaciones industriales, especialmente en el tratamiento de aguas residuales.

Los filosilicatos son un grupo de minerales muy extendidos en la corteza terrestre, integrantes de muchos tipos de rocas, ígneas, metamórficas y sedimentarias. Las arcillas están formadas fundamentalmente por filosilicatos.

La característica principal de los filosilicatos es su disposición en capas, que ocasiona hábitos típicos fácilmente reconocibles (minerales hojosos o escamosos). Además suelen ser minerales blandos y poco densos.

Los filosilicatos consisten en apilamientos de láminas de tetraedros polimerizados. Las láminas, desde el punto de vista estructural, son de dos tipos: tetraédricas y octaédricas. Los tetraédricas están enlazados a tres sitios de oxígeno, lo que da una relación característica de silicio:oxígeno de 2:5. Ejemplos importantes son la mica, el grupo de las cloritas y los grupos de caolinita-serpentina. Las láminas están débilmente enlazadas por fuerzas de van der Waals o enlaces de hidrógeno, lo que provoca una debilidad cristalográfica, que a su vez conduce a una prominente exfoliación basal entre los filosilicatos. Además de los tetraedros, los filosilicatos tienen una hoja de octaedros (elementos de coordinación seis con oxígeno) que equilibran los tetraedros de base, que tienen una carga negativa (por ejemplo, ) Estas hojas de tetraedros (T) y octaedros (O) se apilan en una gran variedad de combinaciones para crear los distintos grupos de los filosilicatos. En una capa octaédrica, hay tres sitios octaédricos en una estructura única; sin embargo, no todos los sitios pueden estar ocupados. En ese caso, el mineral se denomina dioctahédrico, mientras que en otro caso se denomina trioctaédrico.

El grupo de la caolinita-serpentina consiste en pilas de T-O (minerales de arcilla 1:1); su dureza varía de 2 a 4, cuando las láminas están retenidas por enlaces de hidrógeno. Los minerales de arcilla 2:1 (pirofilita-talco) consisten en pilas T-O-T, pero son más blandos (dureza 1-2), ya que están se mantienen unidos por fuerzas de van der Waals. Estos dos grupos de minerales están divididos en subgrupos según la ocupación octaédrica; específicamente, la caolinita y la pirofilita son dioctaédricos mientras que la serpentina y el talco son trioctaédricos.

Las micas son también filosilicatos T-O-T apilados, pero difieren de los otro miembros de las subclases apiladas T-O-T y T-O en que incorporan aluminio en las láminas tetraédricas (los minerales de arcilla tienen en los sitios octaédricos). Ejemplos comunes de micas son la moscovita y las series de la biotita. El grupo de la clorita se relaciona con el grupo de la mica, pero con una capa similar a la brucita () entre la de las pilas T-O-T.<

A causa de su estructura química, los filosilicatos típicamente tienen capas flexibles, elásticas, transparentes que son aislantes eléctricos y se pueden dividir en escamas muy finas. Las micas se puede utilizar en la electrónica como aislantes, en la construcción, como relleno óptico, o incluso en cosméticos. Crisotila, una especie de serpentina, es la especie mineral más común en el amianto industrial, ya que es menos peligrosa en términos de la salud que los asbestos anfíboles.

Los inosilicatos son metasilicatos que consisten en tetraedros unidos repetidamente en cadenas. Estas cadenas pueden ser simples —cuando un tetraedro está unido a otros dos para formar una cadena continua —o dobles, cuando dos cadenas sencillas se combinan entre ellas. Los silicatos de cadena individuales tienen una relación de silicio:oxígeno de 1:3 (por ejemplo, ), mientras que las variedades de doble cadena tiene una proporción de 4:11, por ejemplo . Los inosilicatos tienen dos importantes grupos de minerales que forman rocas; los piroxenos, generalmente silicatos de cadena simple, y los anfiboles, de cadena doble. Hay cadenas de orden superior (por ejemplo, cadenas de tres, cuatro o cinco miembros) pero son raras.

El grupo de los piroxenos consta de 21 especies minerales. Los piroxenos tienen una fórmula de estructura general (), siendo X un sitio octaédrico e Y otro que puede variar en número de coordinación de seis a ocho. La mayoría de las variedades de los piroxenos consisten en permutaciones de , y que equilibran la carga negativa de la cadena principal. Los piroxenos son comunes en la corteza terrestre (aproximadamente el 10%) y son un componente clave de las rocas ígneas máficas.

Los anfiboles tienen una gran variabilidad química, por ello descritos a veces como un «cesto mineralógico» o un «tiburón mineralógico nadando en un mar de elementos». La columna vertebral de los anfíboles es la ; está equilibradoa por cationes en tres posiciones posibles, aunque la tercera posición no siempre se utiliza y un elemento puede ocupar las restantes. Los anfíboles están generalmente hidratados, es decir, que tienen un grupo hidroxilo (), aunque puede ser reemplazado por un fluoruro, un cloruro, o un ion de óxido. Debido a su química variable, hay más de 80 especies de anfíboles, aunque las variaciones más comunes, como en los piroxenos, implican mezclas de , y . Varias especies minerales de los anfíboles pueden tener un hábito cristalino asbestiforme. Estos minerales de asbesto forman fibras largas, delgadas, flexibles y fuertes, que son aislantes eléctricos, químicamente inertes y resistentes al calor; como tal, tienen varias aplicaciones, especialmente en materiales de construcción. Sin embargo, los asbestos son conocidos carcinógenos, y causan varias enfermedades más, como la asbestosis; los asbestos anfíboles (antofilita, tremolita, actinolita, grunerita y riebeckita) se consideran más peligrosos que el asbesto serpentina crisotilo.

La clase de los ciclosilicatos corresponde a la clase 9.C de la clasificación de Strunz y tiene 16 familias. Está integrada por tres o más tetraedros de [SiO] unidos por sus vértices, formando un anillo cerrado, simple o doble, el cual puede tener enlaces iónicos con un metales como por ejemplo sodio, calcio, hierro, aluminio, potasio, magnesio, etc. Algunos ejemplos de ciclosilicatos son la turmalina, cordierita, rubelita, benitoita, dioptasa, etc.

Los ciclosilicatos, o silicatos de anillo, tienen una relación de silicio a oxígeno de 1:3. Los anillos de seis miembros son los más comunes, con una estructura de base de ; ejemplos del grupo son la turmalina y el berilo. Hay otras estructuras de anillo, habiendo sido descritas las de 3, 4, 8, 9 y 12 . Los ciclosilicatos tienden a ser fuertes, con cristales alargados y estriados. Los anillos pueden ser simples o ramificados, aislados unos de otros o agrupados en dos. Estos anillos están generalmente apilados en la estructura y determinar canales que puede estar vacíos u ocupados por iones o moléculas. Los ciclosilicates se clasifican según el tipo de anillos, y en particular por el número de tetraedros en el anillo. 

Las turmalinas tienen una química muy compleja que puede ser descrita por una fórmula general . El es la estructura básica del anillo, donde T es generalmente , pero pueden ser sustituidos por o . Las turmalinas pueden dividirse en subgrupos por el sitio que ocupe el X, y de ahí se subdividen por la química del sitio W. Los sitios Y y Z pueden acomodar una variedad de cationes, especialmente diversos metales de transición; esta variabilidad en el contenido del metal de transición estructural da al grupo de la turmalina mayor variabilidad en color. Otro ciclosilicato es el berilo, , cuyas variedades incluyen piedras preciosas como la esmeralda (verde) y la aguamarina (azulado). La cordierita es estructuralmente similar al berilo, y es un mineral metamórfico común.

La clase de los sorosilicatos corresponde a la clase 9.B de la clasificación de Strunz y tiene 10 familias, de dos tipos, el de las de las epidotas y el de las idocrasas.

Los sorosilicatos, también denominados disilicatos, tienen un enlace tetraedro-tetraedro en un oxígeno, lo que resulta en una relación de 2:7 de silicio al oxígeno. El elemento estructural común resultante es el grupo . Los disilicatos más comunes son, con mucho, los miembros del grupo de la epidota. Las epidotas se encuentran en diversos entornos geológicos, que van desde las cordilleras oceánicas a los granitos y hasta las metapelitas. Las epidotas se construyen alrededor de la estructura ; por ejemplo, las especies minerales de epidota tiene calcio, aluminio y hierro férrico para equilibrar las cargas: . La presencia de hierro como y ayuda a entender la fugacidad de oxígeno, que a su vez es un factor significativo en petrogénesis.

Otros ejemplos de sorosilicatos son la lawsonita, un mineral metamórfico que forma las facies blueschist (ajuste de zona de subducción con baja temperatura y alta presión), la vesuvianita, que ocupa una cantidad significativa de calcio en su estructura química.

La clase de los ortosilicatos corresponde a la clase 9.A de la clasificación de Strunz y tiene 10 familias con cerca de 120 especies.

Los ortosilicatos consisten en tetraedros aislados que tienen las cargas equilibrada por otros cationes. También denominados nesosilicatos, este tipo de silicatos tiene una relación silicio:oxígeno de 1:4 (por ejemplo, ). Los ortosilicatos típicos tienden a formar bloques de cristales equantes, y son bastante pesados. Varios minerales que forman rocas son parte de esta subclase, como los aluminosilicatos, el grupo del olivino o el grupo del granate.

Los aluminosilicatos —cianita, andalucita, y silimanita, todos — están estructuralmente compuestos por un tetraedro y un en coordinación octaédrica. El restante puede estar en coordinación de seis (cianita), cinco (andalucita) o cuatro (silimanita); qué mineral se forma en un entorno dado depende de las condiciones de presión y temperatura. En la estructura del olivino, la serie principal de olivino consisten en forsterita, rica en magnesio, y fayalita, rica en hierro. Tanto el hierro como el magnesio están en coordinación octaédrica con el oxígeno. Existen otras especies minerales que tienen esta estructura, como la tefroita, . El grupo del granate tiene una fórmula general de , donde X es un gran catión ocho veces coordinado, e Y es un catión menor seis veces coordinado. Hay seis miembros terminales ideales de granate, divididos en dos grupos. Los granates piralspita tienen en la posición Y: piropo (), almandino (), y espesartina (). Los granates ugrandita tienen en la posición X: uvarovita (), grossular () y andradita (). Si bien hay dos subgrupos de granate, existen soluciones sólidas entre los seis miembros finales.

Otros ortosilicatos son el circón, la estaurolita y el topacio. El zirconio () es útil en geocronología ya que el puede ser sustituido por ; además, debido a su estructura muy resistente, es difícil resetearlo como un cronómetro. La estaurolita es un común mineral índice de grado intermedio metamórfico. Tiene una estructura cristalina particularmente complicada que solo fue descrita plenamente en 1986. El topacio (, que se encuentra a menudo en pegmatitas graníticas asociadas con turmalina, piedra preciosa es un mineral común.

Los elementos nativos son aquellos minerales integrados por elementos que no están unidos químicamente a otros elementos. Este grupo incluye minerales metales nativos, semi-metales y no metales, y varias aleaciones sólidas y soluciones. Los metales se mantienen unidos por enlaces metálicos, lo que les confiere propiedades físicas distintivas, como su lustre metálico brillante, ductilidad y maleabilidad, y conductividad eléctrica. Los elementos nativos se subdividen en grupos por su estructura o atributos químicos.

El grupo del oro, con una estructura cercana al empaquetamiento cúbico, incluye metales como el oro, la plata y el cobre. El grupo del platino es similar en estructura al grupo de oro. El grupo del hierro-níquel se caracteriza por tener varias especies de aleaciones de hierro-níquel. Dos ejemplos son la kamacita y la taenita, que se encuentran en meteoritos de hierro; estas especies difieren en la cantidad de Ni en la aleación; la kamacita tiene menos de 5–7% de níquel y es una variedad de hierro nativo, mientras que el contenido de níquel de la taenita es del 7–37%. Los minerales del grupo del arsénico se componen de semi-metales, que tienen solamente algunos metálicos; por ejemplo, carecen de la maleabilidad de los metales. El carbono nativo aparece en dos alótropos, el grafito y el diamante; el último se forma a muy alta presiones en el manto, lo que le confiere una estructura mucho más fuerte que el grafito.

La clase de los minerales sulfuros y sulfosales —denominación engañosa pues los sulfuros solo son una parte del grupo— corresponde a la clase 2 de la clasificación de Strunz y en ella se incluyen: minerales sulfuros —con el ion —-, los seleniuros, teluriuros, arseniuros, antimoniuros, bismutiuros, sulfoarseniuros y sulfosales. Los sulfuros se clasifican por la relación del metal o del semimetal con el azufre, M:S igual a 2:1, o 1:1. A pesar de que los sulfuros son mucho menos abundantes que los silicatos, su química y sus estructuras son muy variadas, lo que explica porque el número de minerales de sulfuro es muy alto en relación a su abundancia.

Se agrupan entre los sulfuros los minerales compuestos de uno o más metales o semimetales con un azufre, que tienen una fórmula de tipo general de , donde M es un metal (Ag, Cu, Pb, Zn, Fe, Ni, Hg, As, Sb, Mo, Hg, Tl, V). Los arseniuros, los antimoniuros, los telurios... se clasifican entre los «sulfuros» "sensu lato" debido a su similitud estructural con los sulfuros.

Los sulfuros minerales se caracterizan por la unión covalente, la opacidad y el brillo metálico; se estudian con el microscopio de reflexión. Los sulfuros tienden a ser blandos y frágiles, con un alto peso específico y la mayoría son semiconductores. Muchos sulfuros en polvo, como la pirita, tienen un olor sulfuroso cuando son pulverizados. Los sulfuros son susceptibles a la intemperie, y muchos se disuelven fácilmente en agua; estos minerales disueltos se pueden después volver a redepositar, lo que crea yacimientos de menas secundarias.
Muchos minerales de sulfuro son importantes económicamente como minerales metálicos; son ejemplos la esfalerita (), una mena de zinc; la galena (), una mena de plomo; el cinabrio (), una mena de mercurio; y la molibdenita (, una mena de molibdeno. La pirita () es el sulfuro que aparece más y se puede encontrar en la mayoría de entornos geológicos. No es, sin embargo, una mena de hierro, pero puede ser oxidada para producir ácido sulfúrico. Relacionados con los sulfuros están las raras sulfosales, en las que un elemento metálico está unido al azufre y a un semimetal, como antimonio, arsénico o bismuto. Al igual que los sulfuros, las sulfosales son típicamente minerales blandos, pesados y frágiles.

La clase de los minerales óxidos e hidróxidos corresponde a la clase 4 de la clasificación de Strunz y en ella se incluyen: óxidos, hidróxidos, vanadatos, arsenitos, antimonitos, bismutitos, sulfitos, selenitos, teluritos y yodatos.

Los minerales óxidos se dividen en tres categorías: óxidos simples, hidróxidos y óxidos múltiples. Los óxidos simples se caracterizan por como anión principal y enlace principalmente iónico. Se pueden subdividir además por la relación del oxígeno a los cationes. El grupo de la periclasa consta de minerales con una relación 1:1. Óxidos con una relación 2:1 incluyen la cuprita () y el hielo de agua. minerales del grupo del corindón tienen una proporción de 2:3, e incluye minerales como el corindón () y la hematita (). Los minerales del grupo del rutilo tienen una proporción de 1:2; la especie del mismo nombre, rutilo () es el principal mena del titanio; Otros ejemplos incluyen la casiterita (, mena de estaño), y pirolusita (, mena de manganeso).

En hidróxidos, el anión dominante es el ion hidroxilo, . Las bauxitas son la mena principal del aluminio, y son una mezcla heterogénea de minerales de hidróxido de diáspora, gibbsita, y bohmita; se forman en áreas con una alta tasa de meteorización química (principalmente condiciones tropicales). Por último, varios óxidos son compuestos de dos metales con oxígeno. Un grupo importante dentro de esta clase son las espinelas, con una fórmula general de . Ejemplos de especies incluyen la propia espinela (), la cromita () y la magnetita (). Esta última es fácilmente distinguible por su fuerte magnetismo, que se produce ya que tiene hierro en dos estados de oxidación (), lo que hace que sea un óxido múltiple en lugar de un óxido simple.

La clase de los minerales haluros corresponde a la clase 3 de la clasificación de Strunz y en ella se incluyen: haluros o halogenuros simples o complejos, con HO o sin ella, así como derivados oxihaluros, hidroxihaluros y haluros con doble enlace.

Los minerales haluros son compuestos en los que un halógeno (flúor, cloro, yodo y bromo) es el anión principal. Estos minerales tienden a ser blandos, débiles, quebradizos y solubles en agua. Los ejemplos más comunes de haluros son la halita (, sal de mesa), la silvita () y la fluorita (). La halita y la silvita se forman comúnmente como evaporitas, y pueden ser minerales dominantes en las rocas sedimentarias químicas. La criolita, , es un mineral clave en la extracción de aluminio a partir de la bauxita; Sin embargo, dado que la única ocurrencia significativa está en Ivittuut, Groenlandia, en una pegmatita granítica, ya agotada, la criolita sintética se puede hacer a partir de la fluorita.

La clase de los minerales carbonatos y nitratos corresponde a la clase 5 de la clasificación de Strunz y en ella se incluyen carbonatos, carbonatos de uranilo y nitratos.

Los minerales carbonatos son aquellos en los que el grupo aniónico principal es un carbonato, . Los carbonatos tienden a ser frágiles, muchos tienen exfoliación romboédrica, y todos reaccionan con ácido. Debido a la última característica, los geólogos de campo a menudo llevan ácido clorhídrico diluido para distinguir los carbonatos de los no-carbonatos. La reacción del ácido con los carbonatos, que se encuentra más comúnmente como los polimorfos calcita y aragonita (), se refiere a la disolución y precipitación del mineral, que es un elemento clave en la formación de las cuevas de caliza —con elementos como estalactitas y estalagmitas— y los accidentes geográficos kársticos. Los carbonatos se forman con mayor frecuencia en forma de sedimentos biogénicos o químicos en ambientes marinos. El grupo carbonato es estructuralmente un triángulo, donde un catión central de está rodeado por tres aniones ; diferentes grupos de minerales se forman a partir de diferentes disposiciones de estos triángulos.

El mineral de carbonato más común es la calcita, que es el componente principal de la sedimentaria caliza y del mármol metamórfico. La calcita, , puede tener una impureza de alto contenido en magnesio; en condiciones de alto magnesio, se formará en su lugar su polimorfo, la aragonita; la geoquímica marina se puede describir, en este sentido, como un mar de aragonito o mar de calcita, dependiendo de qué mineral se forme preferentemente. La dolomita es un carbonato doble, de fórmula . La dolomitization secundaria de la caliza es común, en la que la calcita o la aragonita se convierten en dolomita; esta reacción aumenta el espacio de los poros (el volumen de la celda unidad de la dolomita es el 88% del de la calcita), lo que puede crear un yacimiento de petróleo y gas. Estas dos especies minerales son miembros de los grupos de minerales del mismo nombre: el grupo de la calcita incluye carbonatos con fórmula general y el de la dolomita la de .

La clase de los minerales sulfatos corresponde a la clase 7 de la clasificación de Strunz y en ella se incluyen: sulfatos, selenatos, teluratos, cromatos, molibdatos y wolframatos.
Los minerales sulfatos tienen todos el anión sulfato, . Tienden a ser de transparentes a translúcidos, blandos, y muchos son frágiles. Los minerales de sulfato se forman comúnmente como evaporitas, donde se precipitan de la evaporación de las aguas salinas; alternativamente, los sulfatos también se pueden encontrar en los sistemas de vetas hidrotermales asociados con sulfuros, o como productos de oxidación de sulfuros. Los sulfatos se pueden subdividir en minerales anhidros e hidratados. El sulfato hidratado más común, con mucho, es el yeso, ⋅. Se forma como un evaporita, y se asocia con otros evaporitas como la calcita y la halita; si incorpora granos de arena cuando cristaliza, el yeso puede formar rosas del desierto. El yeso tiene muy baja conductividad térmica y mantiene una temperatura baja cuando se calienta a medida que pierde el calor por deshidratación; como tal, el yeso se utiliza como aislante en materiales de construcción. El equivalente anhidro del yeso es la anhidrita; se puede formar directamente de agua de mar en condiciones muy áridas. El grupo de la barita tiene la fórmula general , donde X es un catión grande 12-enlazado. Son ejemplos la barita (), la celestina (), y la anglesita (); la anhidrita no es parte del grupo de la barita, ya que el más pequeño sólo tiene enlace ocho veces.

La clase de los minerales fosfatos corresponde a la clase 8 de la clasificación de Strunz y en ella se incluyen fosfatos, arseniatos y vanadatos. Son 51 familias agrupadas en 7 divisiones, un grupo grande y diverso, que sin embargo, tiene solo unas pocas especies relativamente comunes. 

Los minerales fosfatos se caracterizan por el anión fosfato coordinado tetraédricamente , aunque la estructura se puede generalizar siendo el fósforo sustituido por antimonio (), arsénico (), o vanadio (). Los aniones de cloro (), flúor () e hidróxido () también encajan en la estructura cristalina. 

El fosfato más común es el grupo de la apatita, un nombre genérico que designa fosfatos hexagonales de composición bastante variable, . Las especies más comunes del grupo son la fluorapatita (), la clorapatita () y la hidroxiapatita (). Los minerales de este grupo son los principales constituyentes cristalinos de los dientes y de los huesos de los vertebrados.

Otro grupo relativamente abundante es el grupo de la monacita, que tiene una estructura general de , donde T es el fósforo o arsénico, y A es, a menudo, un elemento de las tierras raras. La monacita es importante en dos sentidos: en primer lugar, como "sumidero" de tierras raras, puede concentrar la cantidad suficiente de estos elementos para convertirse en una mena; en segundo lugar, los elementos del grupo de la monacita pueden incorporar cantidades relativamente grandes de uranio y torio, que pueden ser utilizadas para datar una roca basándose en la desintegración del U y Th en plomo.

La clase de los minerales compuestos orgánicos corresponde a la clase 10 de la clasificación de Strunz y en ella se incluyen sales y ácidos orgánicos que aparezcan en minas y los hidrocarburos. Son 7 familias agrupadas en 3 divisiones, un grupo escaso.

Estos raros compuestos contienen carbono orgánico, pero se pueden formar también mediante un proceso geológico. Por ejemplo, la whewellita, ⋅ es un oxalato que se puede depositar en las venas de menas hidrotermales. Mientras el oxalato de calcio hidratado se puede encontrar en las vetas de carbón y en otros depósitos sedimentarios que comprenden materia orgánica, la ocurrencia hidrotérmica no se considera que está relacionada con la actividad biológica.

Los minerales tienen gran importancia por sus múltiples aplicaciones en los diversos campos de la actividad humana. 
La industria moderna depende directa o indirectamente de los minerales.

Algunos minerales se utilizan prácticamente tal como se extraen; por ejemplo el azufre, el talco, la sal de mesa, etc. Otros, en cambio, deben ser sometidos a diversos procesos para obtener el producto deseado, como el hierro, cobre, aluminio, estaño, etc. Los minerales constituyen la fuente de obtención de los diferentes metales, base tecnológica de la sociedad actual. Así, de distintos tipos de cuarzo y silicatos, se produce el vidrio. Los nitratos y fosfatos son utilizados como abono para la agricultura. Ciertos materiales, como el yeso, son utilizados profusamente en la construcción. Los minerales que entran en la categoría de piedras preciosas o semipreciosas, como los diamantes, topacios, rubíes, se destinan a la confección de joyas.

Se ha sugerido que los biominerales podrían ser indicadores importantes de vida extraterrestre y que por lo tanto podrían jugar un papel importante en la búsqueda de vida pasada o presente en el planeta Marte. Por otra parte, se cree que los componentes orgánicos (biofirmas), que a menudo se asocian con los biominerales, juegan un papel crucial tanto en reacciones pre-bióticas como bióticas.

El 24 de enero de 2014, la NASA informó que los estudios actuales de los rovers "Curiosity" y "Opportunity" en Marte estarán ahora destinados a la búsqueda de evidencia de vida antigua, incluyendo una biosfera basada en microorganismos autótrofos, quimiótrofos y/o quimiolitoautotróficos, así como en agua antigua, incluyendo ambientes fluvo-lacustres (llanuras relacionadas con antiguos ríos o lagos) que pueden haber sido habitables. La búsqueda de evidencia de habitabilidad, tafonomía (relacionada con los fósiles), y el carbono orgánico en el planeta Marte son ahora un objetivo primordial de la NASA.





</doc>
<doc id="11566" url="https://es.wikipedia.org/wiki?curid=11566" title="Ágata (mineral)">
Ágata (mineral)

El ágata no es un mineral específico, sino un conjunto de variedades microcristalinas del cuarzo (sílice). En realidad, son variedades de calcedonia que presentan bandas de varios colores poco contrastados. La diferencia de colores aparece porque en cada zona la estructura y el número de inclusiones en la calcedonia varía, con lo que cambian sus propiedades.

El ágata se encuentra en rocas volcánicas cuyo tamaño puede variar desde milímetros a varios metros. Se caracteriza por presentar una serie de bandas concéntricas de colores similares, opacos y translúcidos, que recuerdan el corte de un tronco de árbol en sentido circular. Puede adoptar diversas formas y presentarse en muchas variedades. Es una roca dura y resistente a los reactivos químicos.

Existen algunas variedades, que en realidad son calcedonias con distintas inclusiones, como ágata dendrítica, ágata musgosa o piedra mocha, ágata de paisaje, ónix u ónice, ágata de fuego, sardónix o sardónica, ónix negro, ágata azul acebo, entre otras. Reciben estos nombres por los colores y dibujos que forman sus bandas.

Los yacimientos más importantes de ágatas se encuentran en Estados Unidos, Brasil, departamento de Artigas en el Uruguay, Argentina, India y Madagascar.

El magma es expulsado desde el interior de la Tierra hasta la superficie por medio de los volcanes. Este, al tomar contacto con el aire combina sus elementos químicos, pasando a denominarse lava. Esta, sobre la superficie de la tierra genera calor y su superficie se enfría más rápidamente que su interior. En su interior presenta burbujas de gas. Con el paulatino enfriamiento de la lava, los distintos gases presentes en las burbujas se van enfriando y combinando hasta enfriarse totalmente y formarse las piedras. Si la burbuja presenta poco volumen de gases se formará una ágata de lo contrario se producirá un amatista.

El ágata se forma en las cavidades de las rocas volcánicas, por donde se filtran y depositan por capas las soluciones calientes ricas en sílice. Las variaciones en la solución o en las condiciones en que se deposita son las que provocan las variaciones en las sucesivas capas (con lo que a veces la calcedonia alterna con el cuarzo cristalino).

La primera capa que se deposita suele ser una sustancia grisácea oscura, que proviene de la descomposición de ciertos minerales presentes en la roca en la que se va a formar el ágata. Además, cuando el ágata se desprende de su matriz, esta capa queda rugosa y basta. Todo esto le da a este mineral un aspecto exterior de pedrusco.

Muchas ágatas son huecas, ya que a menudo no se deposita la cantidad suficiente de solución silícea como para llenar toda la cavidad. En estos casos, la última deposición suele ser cuarzo o amatista, y se produce de forma tal que los cristales apuntan al interior del hueco. Se dice entonces que se ha formado una geoda.

Cuando la roca que la contiene se desintegra, el ágata, que es extremadamente resistente a la erosión,permanece como gravilla en la tierra o en las orillas de los ríos.

El nombre "ágata" proviene del río Achates, actualmente río Dirillo, al sur de Sicilia, en Italia, donde se dice que se encontró la primera de estas piedras.

El ágata fue muy venerada por los antiguos y se la consideraba como la piedra de la ciencia. Se creía que el ágata de la India era el mejor remedio para las enfermedades de los ojos, y que el ágata egipcia era muy efectiva contra las mordeduras de arañas y picaduras de escorpiones.

Las ágatas de Aleppo, en Arabia, recibieron el nombre de “ágatas de ojo”, debido a que parecían pupilas rodeadas del iris. Eran muy estimadas y se usaban como ojos en las imágenes de los dioses. También se han encontrado en las cuencas oculares de las momias del viejo Egipto.

En el Islam las ágatas también son piedras muy preciadas. Según la tradición, un anillo de ágata, por ejemplo, protege a su portador de ciertos percances y le garantiza la longevidad, entre otros beneficios.

A menudo, para comercializarlas, las ágatas se tiñen para resaltar el dibujo que forman sus bandas. De esta manera, se obtienen colores mucho más vivos.

El ágata, y las variedades de que es tipo, han suministrado las piedras duras más adecuadas para el grabado. Uno de los más notables y al mismo tiempo una de las mayores piedras de esta especie representa a Alejandro Magno. La cabeza tiene un relieve muy particular y la piedra está montada en un magnífico engarce de oro esmaltado.

Otra figura en ágata calcedonia representa el toro dionisíaco, el cuerpo ceñido con una guirnalda de hiedra, la cabeza baja y el tirso a sus pies. Arriba, en el campo, se lee la firma del famoso grabador Hillo. Célebre por la belleza de su trabajo, este camafeo es uno de los monumentos de primer orden que nos ha legado la antigüedad.

Como muestra de grabado moderno en cornalina se puede citar la piedra célebre conocida como sello de Miguel Ángel. En el mismo sátiros y bacantes de ambos sexos celebran al dios del vino: unos beben, otros escancian, otros llevan canastillos llenos de uvas. Dos genios alados tienden un velo que atan a troncos de vid. En medio de la composición se distingue la cabeza de un caballo. A la izquierda se ve un grupo de dos mujeres cargando una a otra una canasta a la cabeza. En el exergo, un bello paisaje representa un río encauzado entre dos colinas y un hombre sentado a la margen de este río está pescando a la caña. 

Miguel Ángel pintó al fresco en la Capilla Sixtina una Judit entregando a su sirvienta la cabeza de Holofernes. El grabador se inspiró en la obra de Miguel Ángel y el grabado, por tanto, es posterior a este ilustre artista.

Industrialmente se utiliza principalmente para realizar ornamentos de distintos tipos: pines, broches, pisapapeles etcétera. Además debido a su dureza y resistencia a los ácidos se utiliza en la realización de morteros destinados a la mezcla de reactivos químicos. Debido a sus características físicas también es óptimo para el acabado de materiales de cuero.



</doc>
<doc id="11568" url="https://es.wikipedia.org/wiki?curid=11568" title="Aluminio">
Aluminio

El aluminio es un elemento químico, de símbolo Al y número atómico 13. Se trata de un metal no ferromagnético. Es el tercer elemento más común encontrado en la corteza terrestre. Los compuestos de aluminio forman el 8 % de la corteza de la tierra y se encuentran presentes en la mayoría de las rocas, de la vegetación y de los animales. En estado natural se encuentra en muchos silicatos (feldespatos, plagioclasas y micas). Este metal se extrae únicamente del mineral conocido con el nombre de bauxita, por transformación primero en alúmina mediante el proceso Bayer y a continuación en aluminio metálico mediante electrólisis.
Este metal posee una combinación de propiedades que lo hacen muy útil en ingeniería de materiales, tales como su baja densidad (2812,5 kg/m³) y su alta resistencia a la corrosión. Mediante aleaciones adecuadas se puede aumentar sensiblemente su resistencia mecánica (hasta los 690 MPa). Es buen conductor de la electricidad y del calor, se mecaniza con facilidad y es muy barato. Por todo ello es desde mediados del siglo XX es el metal que más se utiliza después del acero.

Fue aislado por primera vez en 1825 por el físico danés H. C. Ørsted. El principal inconveniente para su obtención reside en la elevada cantidad de energía eléctrica que requiere su producción. Este problema se compensa por su bajo coste de reciclado, su extendida vida útil y la estabilidad de su precio.

El aluminio se utilizaba en la antigüedad clásica en tintorería y medicina bajo la forma de una sal doble, conocida como alumbre y que se sigue usando hoy en día. En el siglo XIX, con el desarrollo de la física y la química, se identificó el elemento. Su nombre inicial, "aluminum", fue propuesto por el británico sir Humphrey Davy en el año 1809. A medida que se sistematizaban los nombres de los distintos elementos, se cambió por coherencia a la forma "aluminium", que es la preferida hoy en día por la IUPAC debido al uso uniforme del sufijo "-ium". No es, sin embargo, la única aceptada, ya que la primera forma es muy popular en los Estados Unidos. En el año 1825, el físico danés Hans Christian Ørsted, descubridor del electromagnetismo, logró aislar por electrólisis unas primeras muestras, bastante impuras. El aislamiento total fue conseguido dos años después por Friedrich Wöhler.
La extracción del aluminio a partir de las rocas que lo contenían se reveló como una tarea ardua. A mediados de siglo, podían producirse pequeñas cantidades, reduciendo con sodio un cloruro mixto de aluminio y sodio, gracias a que el sodio era más electropositivo. Durante el siglo XIX, la producción era tan costosa que el aluminio llegó a considerarse un material exótico, de precio exorbitante, y tan preciado o más que la plata o el oro. Durante la Exposición Universal de 1855 se expusieron unas barras de aluminio junto a las joyas de la corona de Francia. El mismo emperador Napoleón III había pedido una vajilla de aluminio para agasajar a sus invitados. De aluminio se hizo también el vértice del Monumento a Washington, a un precio que rondaba en 1884 el de la plata.

Diversas circunstancias condujeron a un perfeccionamiento de las técnicas de extracción y un consiguiente aumento de la producción. La primera de todas fue la invención de la dinamo en 1866, que permitía generar la cantidad de electricidad necesaria para realizar el proceso. En el año 1889, Karl Bayer patentó un procedimiento para extraer la alúmina u óxido de aluminio a partir de la bauxita, la roca natural. Poco antes, en 1886, el francés Paul Héroult y el norteamericano Charles Martin Hall habían patentado de forma independiente y con poca diferencia de fechas un proceso de extracción, conocido hoy como proceso Hall-Héroult. Con estas nuevas técnicas se incrementó vertiginosamente la producción de aluminio. Si en 1882, la producción anual alcanzaba apenas las 2 toneladas, en 1900 alcanzó las 6700 toneladas, en 1939 las 700 000 toneladas, 2 000 000 en 1943, y en aumento desde entonces, llegando a convertirse en el metal no férreo más producido en la actualidad.

La abundancia conseguida produjo una caída del precio y que perdiese la vitola de metal preciado para convertirse en metal común. Ya en 1895 abundaba lo suficiente como para ser empleado en la construcción, como es el caso de la cúpula del edificio de la secretaría de Sídney, donde se utilizó este metal. Hoy en día las líneas generales del proceso de extracción se mantienen, aunque se recicla de manera general desde 1960, por motivos medioambientales pero también económicos, ya que la recuperación del metal a partir de la chatarra cuesta un 5% de la energía de extracción a partir de la piedra.

El aluminio tiene como número atómico 13. Los 13 protones que forman el núcleo están rodeados de 13 electrones dispuestos en la forma:

La valencia es 3 y las energías de ionización de los tres primeros electrones son, respectivamente: 577,5kJ/mol, 1816,7kJ/mol y 2744,8kJ/mol. Existen en la naturaleza dos isótopos de este elemento, el Al y el Al. El primero de ellos es estable mientras que el segundo es radiactivo y su vida media es de 7,2×10 años. Además de esto existen otros siete isótopos cuyo peso está comprendido entre 23 y 30 unidades de masa atómica.

El Al se produce a partir del argón a causa del bombardeo por la radiación altamente energética de los rayos cósmicos, que inciden en la atmósfera sobre los núcleos de este elemento. Al igual que el C, la medida de las abundancias del Al es utilizada en técnicas de datación, por ejemplo en procesos orogenéticos cuya escala es de millones de años o para determinar el momento del impacto de meteoritos. En el caso de estos últimos, la producción de aluminio radiactivo cesa cuando caen a la tierra, debido a que la atmósfera filtra a partir de ese momento los rayos cósmicos.

El aluminio posee tres radios iónicos en su estado de oxidación +3, dependiendo del número de coordinación del átomo. Dicho esto, tenemos que para un número de 4 el radio es 53,0pm, para 5 es 62,0pm y para 6 es 67,5pm.

El aluminio es un elemento muy abundante en la naturaleza, solo aventajado por el oxígeno y el silicio. Se trata de un metal ligero, con una densidad de 2700 kg/m³, y con un bajo punto de fusión (660 °C). Su color es grisáceo y refleja bien la radiación electromagnética del espectro visible y el térmico. Es buen conductor eléctrico (entre 35 y 38m/(Ωmm²)) y térmico (80 a 230W/(m·K)).

Es un material blando (escala de Mohs: 2-3-4) y maleable. En estado puro tiene un límite de resistencia en tracción de 160-200N/mm² (160-200MPa). Todo ello le hace adecuado para la fabricación de cables eléctricos y láminas delgadas, pero no como elemento estructural. Para mejorar estas propiedades se alea con otros metales, lo que permite realizar sobre las operaciones de fundición y forja, así como la extrusión del material. También de esta forma se utiliza como soldadura.

La capa de valencia del aluminio está poblada por tres electrones, por lo que su estado normal de oxidación es III. Esto hace que reaccione con el oxígeno de la atmósfera formando con rapidez una fina capa gris mate de alúmina AlO, que recubre el material, aislándolo de posteriores corrosiones. Esta capa puede disolverse con ácido cítrico. A pesar de ello es tan estable que se usa con frecuencia para extraer otros metales de sus óxidos. Por lo demás, el aluminio se disuelve en ácidos y bases. Reaccionan con facilidad con el ácido clorhídrico y el hidróxido sódico.

La utilización industrial del aluminio ha hecho de este metal uno de los más importantes, tanto en cantidad como en variedad de usos, siendo hoy un material polivalente que se aplica en ámbitos económicos muy diversos y que resulta estratégico en situaciones de conflicto. Hoy en día, tan solo superado por el hierro/acero. El aluminio se usa en forma pura, aleado con otros metales o en compuestos no metálicos. En estado puro se aprovechan sus propiedades ópticas para fabricar espejos domésticos e industriales, como pueden ser los de los telescopios reflectores. Su uso más popular, sin embargo, es como papel aluminio, que consiste en láminas de material con un espesor tan pequeño que resulta fácilmente maleable y apto por tanto para embalaje alimentario. También se usa en la fabricación de latas y tetrabriks.

Por sus propiedades eléctricas es un buen conductor, capaz de competir en coste y prestaciones con el cobre tradicional. Dado que, a igual longitud y masa, el conductor de aluminio tiene menos conductividad, resulta un componente poco útil para utilidades donde el exceso de peso es importante. Es el caso de la aeronáutica y de los tendidos eléctricos donde el menor peso implica en un caso menos gasto de combustible y mayor autonomía, y en el otro la posibilidad de separar las torres de alta tensión.

Además de eso, aleado con otros metales, se utiliza para la creación de estructuras portantes en la arquitectura y para fabricar piezas industriales de todo tipo de vehículos y calderería. También está presente en enseres domésticos tales como utensilios de cocina y herramientas. Se utiliza asimismo en la soldadura aluminotérmica y como combustible químico y explosivo por su alta reactividad. Como presenta un buen comportamiento a bajas temperaturas, se utiliza para fabricar contenedores criogénicos. Cuanto más puro, será más liviano y en algunas piezas de aviación, tendrá una alta resistencia gracias al oxígeno que lo compone. Es conocido como "Aluminio oxigenado o Aero Aluminio".

El uso del aluminio también se realiza a través de compuestos que forma. La misma alúmina, el óxido de aluminio que se obtiene de la bauxita, se usa tanto en forma cristalina como amorfa. En el primer caso forma el corindón, una gema utilizada en joyería que puede adquirir coloración roja o azul, llamándose entonces rubí o zafiro, respectivamente. Ambas formas se pueden fabricar artificialmente. y se utilizan como el medio activo para producir la inversión de población en los láser. Asimismo, la dureza del corindón permite su uso como abrasivo para pulir metales. Los medios arcillosos con los cuales se fabrican las cerámicas son ricos en aluminosilicatos. También los vidrios participan de estos compuestos. Su alta reactividad hace que los haluros, sulfatos, hidruros de aluminio y la forma hidróxida se utilicen en diversos procesos industriales tales como mordientes, catálisis, depuración de aguas, producción de papel o curtido de cueros. Otros compuestos del aluminio se utilizan en la fabricación de explosivos.

El aluminio es uno de los elementos más abundantes de la corteza terrestre (8%) y uno de los metales más caros en obtener. La producción anual se cifra en unos 33,1 millones de toneladas, siendo China y Rusia los productores más destacados, con 8,7 y 3,7 millones respectivamente. Una parte muy importante de la producción mundial es producto del reciclaje. En 2005 suponía aproximadamente un 20% de la producción total. A continuación se lista unas cifras de producción:

La materia prima a partir de la cual se extrae el aluminio es la bauxita, que recibe su nombre de la localidad francesa de Les Baux, donde fue extraída por primera vez. Actualmente los principales yacimientos se encuentran en el Caribe, Australia, Brasil y África porque la bauxita extraída allí se disgrega con más facilidad. Es un mineral rico en aluminio, entre un 20% y un 30% en masa, frente al 10% o 20% de los silicatos alumínicos existentes en arcillas y carbones. Es un aglomerado de diversos compuestos que contiene caolinita, cuarzo óxidos de hierro y titania, y donde el aluminio se presenta en varias formas hidróxidas como la gibbsita Al (OH), la boehmita AlOOH y la diásporo AlOOH.

La obtención del aluminio se realiza en dos fases: la extracción de la alúmina a partir de la bauxita (proceso Bayer) y la extracción del aluminio a partir de esta última mediante electrolisis. Cuatro toneladas de bauxita producen dos toneladas de alúmina y, finalmente, una de aluminio. El proceso Bayer comienza con el triturado de la bauxita y su lavado con una solución caliente de hidróxido de sodio a alta presión y temperatura. La sosa disuelve los compuestos del aluminio, que al encontrarse en un medio fuertemente básico, se hidratan:

Los materiales no alumínicos se separan por decantación. La solución cáustica del aluminio se enfría luego para recristalizar el hidróxido y separarlo de la sosa, que se recupera para su ulterior uso. Finalmente, se calcina el hidróxido de aluminio a temperaturas cercanas a 1000 °C, para formar la alúmina.

El óxido de aluminio así obtenido tiene un punto de fusión muy alto (2000 °C) que hace imposible someterlo a un proceso de electrolisis. Para salvar este escollo se disuelve en un baño de criolita, obteniendo una mezcla eutéctica con un punto de fusión de 900 °C. A continuación se procede a la electrólisis, que se realiza sumergiendo en la cuba unos electrodos de carbono (tanto el ánodo como el cátodo), dispuestos en horizontal. Cada tonelada de aluminio requiere entre 17 y 20MWh de energía para su obtención, y consume en el proceso 460 kg de carbono, lo que supone entre un 25% y un 30% del precio final del producto, convirtiendo al aluminio en uno de los metales más caros de obtener. De hecho, se están buscando procesos alternativos menos costosos que el proceso electrolítico. El aluminio obtenido tiene un pureza del 99,5% al 99,9%, siendo las impurezas de hierro y silicio principalmente. De las cubas pasa al horno donde es purificado mediante la adición de un fundente o se alea con otros metales con objeto de obtener materiales con propiedades específicas. Después se vierte en moldes o se hacen lingotes o chapas.

El aluminio puro es un material blando y poco resistente a la tracción. Para mejorar estas propiedades mecánicas se alea con otros elementos, principalmente magnesio, manganeso, cobre, zinc y silicio, a veces se añade también titanio y cromo. La primera aleación de aluminio, el popular duraluminio fue descubierta casualmente por el metalúrgico alemán Alfred Wilm y su principal aleante era el cobre. Actualmente las aleaciones de aluminio se clasifican en series, desde la 1000 a la 8000, según el siguiente cuadro.

Las series 2000, 6000 y 7000 son tratadas térmicamente para mejorar sus propiedades. El nivel de tratamiento se denota mediante la letra T seguida de varias cifras, de las cuales la primera define la naturaleza del tratamiento. Así T3 es una solución tratada térmicamente y trabajada en frío.

La extrusión es un proceso tecnológico que consiste en dar forma o moldear una masa haciéndola salir por una abertura especialmente dispuesta para conseguir perfiles de diseño complicado.

Se consigue mediante la utilización de un flujo continuo de la materia prima, generalmente productos metalúrgicos o plásticos. Las materias primas se someten a fusión, transporte, presión y deformación a través de un molde según sea el perfil que se quiera obtener.

El aluminio debido a sus propiedades es uno de los metales que más se utiliza para producir variados y complicados tipos de perfiles que se usan principalmente en las construcciones de carpintería metálica. Se puede extruir tanto aluminio primario como secundario obtenido mediante reciclado.

Para realizar la extrusión, la materia prima, se suministra en lingotes cilíndricos también llamados «tochos». El proceso de extrusión consiste en aplicar una presión al cilindro de aluminio (tocho) haciéndolo pasar por un molde (matriz), para conseguir la forma deseada. Cada tipo de perfil, posee un molde adecuado llamado «matriz», que es el que determinará su forma.

El tocho es calentado (aproximadamente a 500 °C, temperatura en que el aluminio alcanza un estado plástico) para facilitar su paso por la matriz, y es introducido en la prensa. Luego, la base del tocho es sometida a una llama de combustión incompleta, para generar una capa fina de carbono. Esta capa evita que el émbolo de la prensa quede pegado al mismo. La prensa se cierra, y un émbolo comienza a empujar el tocho a la presión necesaria, de acuerdo con las dimensiones del perfil, obligándolo a salir por la boca de la matriz. La gran presión a la que se ve sometido el aluminio hace que este eleve su temperatura ganando en maleabilidad.

Los componentes principales de una instalación de extrusión son: el contenedor donde se coloca el tocho para extrusión bajo presión, el cilindro principal con pistón que prensa el material a través del contenedor, la matriz y el portamatriz.

Del proceso de extrusión y temple, dependen gran parte de las características mecánicas de los perfiles, así como la calidad en los acabados, sobre todo en los anodizados. El temple, en una aleación de aluminio, se produce por efecto mecánico o térmico, creando estructuras y propiedades mecánicas características.

A medida que los perfiles extrusionados van saliendo de la prensa a través de la matriz, se deslizan sobre una bancada donde se les enfría con aire o agua, en función de su tamaño y forma, así como las características de la aleación involucrada y las propiedades requeridas. Para obtener perfiles de aluminio rectos y eliminar cualquier tensión en el material, se les estira. Luego, se cortan en longitudes adecuadas y se envejecen artificialmente para lograr la resistencia apropiada. El envejecimiento se realiza en hornos a unos 200 °C y están en el horno durante un periodo que varía entre 4 a 8 horas. Todo este proceso de realiza de forma automatizada.

Los procesos térmicos que aumentan la resistencia del aluminio. Hay dos proceso de temple que son el tratamiento térmico en solución, y el envejecimiento. El temple T5 se consigue mediante envejecimiento de los perfiles que pasan a los hornos de maduración, los cuales mantienen una determinada temperatura durante un tiempo dado. Normalmente 185 °C durante 240 minutos para las aleaciones de la familia 6060, de esta forma se consigue la precipitación del silicio con el magnesio en forma de siliciuro de magnesio (MgSi) dentro de las dendritas de aluminio, produciéndose así el temple del material. La temperatura de salida de extrusión superior a 510 °C para las aleaciones 6060 más el correcto enfriamiento de los perfiles a 250 °C en menos de cuatro minutos, es fundamental para que el material adquiera sus propiedades,

El temple es medido por durómetros, con la unidad de medida llamada Webster o grados Websters.

La fundición de piezas consiste fundamentalmente en llenar un molde con la cantidad de metal fundido requerido por las dimensiones de la pieza a fundir, para, después de la solidificación, obtener la pieza que tiene el tamaño y la forma del molde.

Existen tres tipos de procesos de fundición diferenciados aplicados al aluminio:
En el proceso de fundición con molde de arena se hace el molde en arena consolidada por una apisonadora manual o mecánico alrededor de un molde, el cual es extraído antes de recibir el metal fundido. A continuación se vierte la colada y cuando solidifica se destruye el molde y se granalla la pieza. Este método de fundición es normalmente elegido para la producción de:
La fundición en molde metálico permanente llamados coquillas, sirven para obtener mayores producciones. En este método se vierte la colada del metal fundido en un molde metálico permanente bajo gravedad y bajo presión centrífuga.Puede resultar caro, difícil o imposible fundirlas por moldeo.

En el método de fundición por inyección a presión se funden piezas idénticas al máximo ritmo de producción forzando el metal fundido bajo grandes presiones en los moldes metálicos.

Mediante el sistema de fundición adecuado se pueden fundir piezas que puede variar desde pequeñas piezas de prótesis dental, con peso de gramos, hasta los grandes bastidores de máquinas de varias toneladas, de forma variada, sencilla o complicada, que son imposibles de fabricar por otros procedimiento convencionales, como forja, laminación, etc.

El proceso de fundición se puede esquematizar de la siguiente manera:

Las aleaciones de aluminio para fundición han sido desarrolladas habida cuenta de que proporcionan calidades de fundición idóneas, como fluidez y capacidad de alimentación, así como valores optimizados para propiedades como resistencia a la tensión, ductilidad y
resistencia a la corrosión. Difieren bastante de las aleaciones para forja. El silicio en un rango entre el 5 al 12% es el elemento aleante más importante porque promueve un aumento de la fluidez en los metales fundidos. En menores cantidades se añade magnesio, o cobre con el fin de aumentar la resistencia de las piezas.

El mecanizado del aluminio y sus aleaciones en máquinas herramientas de arranque de virutas en general, es fácil y rápido y está dando paso a una nueva concepción del mecanizado denominada genéricamente mecanizado rápido. Durante el arranque de viruta, las fuerzas de corte que tienen lugar son considerablemente menores que en el caso de las generadas con el acero (la fuerza necesaria para el mecanizado del aluminio es aproximadamente un 30% de la necesaria para mecanizar acero). Por consiguiente, los esfuerzos sobre los útiles y herramientas así como la energía consumida en el proceso es menor para el arranque de un volumen igual de viruta.

El concepto de mecanizado rápido se refiere al que se produce en las modernas máquinas herramientas de Control Numérico con cabezales potentes y robustos que les permiten girar a muchos miles de revoluciones por minuto hasta del orden de 30000rpm, y avances de trabajo muy grandes cuando se trata del mecanizado de materiales blandos y con mucho vaciado de viruta tal y como ocurre en la fabricación de moldes o de grandes componentes de la industria aeronáutica.

El aluminio tiene unas excelentes características de conductividad térmica, lo cual es una importante ventaja, dado que permite que el calor generado en el mecanizado se disipe con rapidez. Su baja densidad hace que las fuerzas de inercia en la piezas de aluminio giratorio (torneados) sean asimismo mucho menores que en otros materiales.

Ocurre, sin embargo, que el coeficiente de fricción entre el aluminio y los metales de corte es, comparativamente con otros metales, elevado. Este hecho unido a su baja resistencia hace que se comporte como plastilina, pudiendo causar el embotamiento de los filos de corte, deteriorando la calidad de la superficie mecanizada a bajas velocidades de corte e incluso a elevadas velocidades con refrigeración insuficiente. Siempre que la refrigeración en el corte sea suficiente, hay una menor tendencia al embotamiento con aleaciones más duras, con velocidades de corte mayores y con ángulos de desprendimiento mayores.

El desarrollo del mecanizado rápido permite que muchas piezas complejas no sea necesario fundirlas previamente sino que se mecanicen a partir de unos prismas a los cuales se les realiza todo el vaciado que sea necesario.

El mecanizado rápido puede representar una reducción de costes en torno al 60%. En este tipo de mecanizado rápido se torna crítico la selección de las herramientas y los parámetros de corte. La adopción del mecanizado de alta velocidad es un proceso difícil para el fabricante, ya que requiere cambios importantes en la planta, una costosa inversión en maquinaria y "software", además de una formación cualificada del personal.

Para el mecanizado rápido que se realiza en las máquinas herramientas de Control Numérico es conveniente que se utilicen herramientas especiales para el mecanizado del aluminio. Se distinguen de las empleadas en el mecanizado del acero en que tienen mayores ángulos de desprendimiento y un mayor espacio para la evacuación de la viruta, así como unos rebajes para que la viruta fluya mejor. La mayoría de las herramientas de filo múltiple como por ejemplo las fresas, tienen pocos dientes.

Hay tres grandes familias de herramientas de corte para el mecanizado del aluminio:

Como lubricante de corte para el aluminio es recomendable que se utilicen productos emulsionables en agua con aditivos de lubricación específicamente formulados a tal fin que estén exentos de compuestos en base cloro y azufre La lubricación se utiliza en operaciones de taladrado, torneado, fresado, brochado, escariado y deformación.

Las aleaciones de aluminio permiten su mecanizado por procedimientos de electroerosión que es un método inventado para el mecanizado de piezas complejas. No obstante, este método no es del todo adecuado para el aluminio, pues su elevada conductividad térmica reducen notablemente la velocidad de eliminación del material, ya de por sí bastante lenta para este método.

Se conoce como electroerosión a un proceso de mecanizado que utiliza la energía suministrada a través de descargas eléctricas entre dos electrodos para eliminar material de la pieza de trabajo, siendo ésta uno de los electrodos.
Al electrodo que hace las funciones de herramienta se le suele denominar simplemente electrodo mientras que al electrodo sobre el cual se desea llevar a cabo el arranque se le conoce como pieza de trabajo. Este sistema permite obtener componentes con tolerancias muy ajustadas a partir de los nuevos materiales que se diseñan.

Los procedimientos de soldeo en aluminio pueden ser al arco eléctrico, bajo atmósfera inerte que puede ser argón, helio, por puntos o por fricción.

La soldadura TIG ("Tungsten Inert Gas"), se caracteriza por el empleo de un electrodo permanente de tungsteno, aleado a veces con torio o zirconio en porcentajes no superiores a un 2%. Dada la elevada resistencia a la temperatura del tungsteno (funde a 3.410 °C), acompañada de la protección del gas, la punta del electrodo apenas se desgasta tras un uso prolongado. Los gases más utilizados para la protección del arco en esta soldadura son el argón y el helio, o mezclas de ambos. Una varilla de aportación alimenta el baño de fusión. Esta técnica es muy utilizada para la soldadura de aleaciones de aluminio y se utiliza en espesores comprendidos entre 1 y 6 mm y se puede robotizar el proceso.


La soldadura por fricción es un proceso de penetración completa en fase sólida, que se utiliza para unir chapas de metal, principalmente de aluminio, sin alcanzar su punto de fusión. El método está basado en el principio de obtener temperaturas suficientemente altas para forjar dos componentes de aluminio, utilizando una herramienta giratoria que se desplaza a lo largo de una unión a tope. Al enfriarse deja una unión en fase sólida entre las dos piezas. La soldadura por fricción, puede ser utilizada para unir chapas de aluminio sin material de aportación. Se consiguen soldaduras de alta calidad e integridad con muy baja distorsión, en muchos tipos de aleaciones de aluminio, incluso aquellas consideradas de difícil soldadura por métodos de fusión convencionales.

El aluminio se presenta en el mercado en diversas formas, ya sean estas barras con diversos perfiles u hojas de varios tamaños y grosores entre otras. Cuando se trabaja con aluminio, específicamente en crear algún doblez en una hoja, o en una parte de ésta, es importante considerar la "dirección del grano"; esto significa que la composición en el metal, después de haber sido fabricado, ha tomado una tendencia direccional en su microestructura, mostrando así una mayor longitud hacia una dirección que hacia otra. Así es que el aluminio puede quebrarse si la dirección del grano no es considerada al crear algún doblez, o si el doblez es creado con un radio demasiado pequeño, el cual sobrepase la integridad elástica del tipo de aluminio.

Este metal, después de extruido o decapado, para protegerse de la acción de los agentes atmosféricos, forma por sí solo una delgada película de óxido de aluminio; esta capa de AlO, tiene un espesor más o menos regular del orden de 0,01 micras sobre la superficie de metal que le confiere unas mínimas propiedades de inoxidacción y anticorrosión.

Existe un proceso químico electrolítico llamado anodizado que permite obtener de manera artificial películas de óxido de mucho más espesor y con mejores características de protección que las capas naturales.

El proceso de anodizado llevado a cabo en un medio sulfúrico produce la oxidación del material desde la superficie hacia el interior, aumentando la capa de óxido de aluminio, con propiedades excelentes por resistencia a los agentes químicos, dureza, baja conductividad eléctrica y estructura molecular porosa, esta última junto con las anteriores, que permite darle una excelente terminación, que es un valor determinante a la hora de elegir un medio de protección para este elemento.

Según sea el grosor de la capa que se desee obtener existen dos procesos de anodizados:

Las ventajas que tiene el anodizado son:

Los anodizados más comerciales son los que se utilizan coloreados por motivos decorativos. Se emplean diversas técnicas de coloración tanto orgánicas como inorgánicas.

Anodizado duro

Cuando se requiere mejorar de forma sensible la superficie protectora de las piezas se procede a un denominado anodizado duro que es un tipo de anodizado donde se pueden obtener capas de alrededor de 150 micras, según el proceso y la aleación. La dureza de estas capas es comparable a la del cromo-duro, su resistencia a la abrasión y al frotamiento es considerable.

Las propiedades del anodizado duro son:

Es muy importante a la hora de seleccionar el material para un anodizado duro, verificar la pieza que se vaya a mecanizar y seleccionar la aleación también en función de sus características y resistencia mecánica.

El proceso de pintura de protección que se da al aluminio es conocido con el nombre de lacado y consiste en la aplicación de un revestimiento orgánico o pintura sobre la superficie del aluminio. Existen diferentes sistemas de lacado para el aluminio

El lacado, que se aplica a los perfiles de aluminio, consiste en la aplicación electrostática de una pintura en polvo a la superficie del aluminio. Las pinturas más utilizadas son las de tipo poliéster por sus características de la alta resistencia que ofrecen a la luz y a la corrosión.

Los objetivos del lacado son:
El proceso de lacado, puede dividirse en tres partes:

El proceso de lacado exige una limpieza profunda de la superficie del material, con disoluciones acuosas ácidas, para eliminar suciedades de tipo graso. Este proceso consigue una mayor adherencia a las pinturas. Mejora la resistencia a la corrosión y a los agentes atmosféricos.

La imprimación con la pintura deseada se realiza en cabinas equipadas con pistolas electrostáticas. La pintura es polvo de poliéster, siendo atraído por la superficie de la pieza que se laca. Combinando todos los parámetros de la instalación se consiguen las capas de espesor requeridas que en los casos de carpintería metálica suele oscilar entre 60/70 micras.

El polimerizado se realiza en un horno de convención de aire, de acuerdo con las especificaciones de tiempo y temperatura definidos por el fabricante de la pintura.

El sistema industrial de lacado puede estar robotizado.

El aluminio metálico se recubre espontáneamente de una delgada capa de óxido que evita su corrosión. Sin embargo, esta capa desaparece en presencia de ácidos, particularmente del perclórico y clorhídrico; asimismo, en soluciones muy alcalinas de hidróxido potásico (KOH) o hidróxido sódico (NaOH) ocurre una enérgica reacción. La presencia de CuCl o CuBr también destruye el óxido y hace que el aluminio se disuelva enérgicamente en agua. Con mercurio y sales de éste, el aluminio reacciona si está limpio formando una amalgama que impide su pasivación. Reacciona también enérgicamente en frío con bromo y en caliente con muchas sustancias, dependiendo de la temperatura, reduciendo a casi cualquier óxido (proceso termita). Es atacado por los haloalcanos. Las reacciones del aluminio a menudo van acompañadas de emisión de luz.

No obstante, las aleaciones de aluminio se comportan bastante peor a corrosión que el aluminio puro, especialmente si llevan tratamientos de recocido, con los que presentan problemas graves de corrosión intercristalina y bajo tensiones debido a la microestructura que presentan en estos estados.

El aluminio es 100% reciclable sin merma de sus cualidades físicas, y su recuperación por medio del reciclaje se ha convertido en una faceta importante de la industria del aluminio. El proceso de reciclaje del aluminio necesita poca energía. El proceso de refundido requiere solo un 5% de la energía necesaria para producir el metal primario inicial.

El reciclaje del aluminio fue una actividad de bajo perfil hasta finales de los años sesenta, cuando el uso creciente del aluminio para la fabricación de latas de refrescos trajo el tema al conocimiento de la opinión pública.

Al aluminio reciclado se le conoce como aluminio secundario, pero mantiene las mismas propiedades que el aluminio primario.

La fundición de aluminio secundario implica su producción a partir de productos usados de dicho metal, los que son procesados para recuperar metales por pretratamiento, fundición y refinado.

Se utilizan combustibles, fundentes y aleaciones, mientras que la remoción del magnesio se practica mediante la adición de cloro, cloruro de aluminio o compuestos orgánicos clorados.

Las mejores técnicas disponibles incluyen:
Durante el año 2002 se produjeron en España 243.000 toneladas de aluminio reciclado y en el conjunto de Europa occidental esta cifra ascendió a 3,6 millones de toneladas.

Para proceder al reciclaje del aluminio primero hay que realizar una revisión y selección de la chatarra según su análisis y metal recuperable para poder conseguir la aleación deseada. La chatarra preferiblemente se compactará, generalmente en cubos o briquetas o se fragmentará, lo cual facilita su almacenamiento y transporte. La preparación de la chatarra descartando los elementos metálicos no deseados o los inertes, llevarán a que se consiga la aleación en el horno de manera más rápida y económica.

El residuo de aluminio es fácil de manejar porque es ligero, no arde y no se oxida y también es fácil de transportar. El aluminio reciclado es un material cotizado y rentable. El reciclaje de aluminio produce beneficios ya que proporciona ocupación y una fuente de ingresos para mano de obra no cualificada.

Este metal fue considerado durante muchos años como inocuo para los seres humanos. Debido a esta suposición se fabricaron de forma masiva utensilios de aluminio para cocinar alimentos, envases para alimentos, y papel de aluminio para el embalaje de alimentos frescos. Sin embargo, su impacto sobre los sistemas biológicos ha sido objeto de mucha controversia en las décadas pasadas y una profusa investigación ha demostrado que puede producir efectos adversos en plantas, animales acuáticos y seres humanos.

La exposición al aluminio por lo general no es dañina, pero la exposición a altos niveles puede causar serios problemas para la salud.

La exposición al aluminio se produce principalmente cuando:

Cualquier persona puede intoxicarse con aluminio o sus derivados, pero algunas personas son más propensas a desarrollar toxicidad por aluminio.

Absorción: Las características químicas de los compuestos de aluminio hacen que de las tres vías por las que una sustancia puede entrar al organismo (oral, dérmica y respiratoria) la vía dérmica sea la menos importante. Menos del 1% del aluminio de la dieta es absorbido; esta absorción en el intestino depende mayoritariamente del pH y de la presencia de ligandos complejos, ácidos carboxílicos a través del cual el aluminio se vuelve absorbible. La fracción absorbible por vía inhalatoria puede acceder directamente al cerebro a través de la vía olfatoria. Los compuestos de aluminio pueden alterar la absorción de otros elementos en el tracto gastrointestinal. Por ejemplo, el aluminio inhibe la absorción de fluoruro y puede disminuir la absorción de calcio, compuestos de hierro y ácido salicílico (el cual este último también disminuye la del aluminio).

Distribución y excreción: la especie influye en estos procesos. La principal vía de excreción es la vía biliar pero si se ingiere en abundancia es más importante la vía renal. En cuanto a la distribución, en el plasma más del 50% del aluminio se une a la albumina y transferrina, a través de la cual puede ser transportando a los diferentes tejidos. Las mayores concentraciones de aluminio se han observado en pulmones, hígado y huesos. En los huesos reduce los efectos positivos de la vitamina D, bloquea los depósitos de calcio lo que puede dar origen a una osteomalacia.La toma de medicamentos que contienen aluminio como son antiácidos, analgésicos, antidiarreicos y antiulcerosos favorecen la absorción intestinal del metal, y predisponen a la toxicidad del aluminio en niños con insuficiencia renal.

La toxicidad aguda del aluminio es rara. La mayoría de los casos de toxicidad del aluminio se observan en personas con insuficiencia renal crónica, en personas expuestas al aluminio en su ámbito laboral y como factor etiológico de la enfermedad del alzheimer.

Aunque el aluminio no figure en el cuadro de las enfermedades profesionales, se han descrito cuadros tóxicos en trabajadores que lo trabajan (expuestos a dosis bajas de forma crónica) en su actividad profesional. Algunos estudios señalan efectos adversos en las vías respiratorias con síntomas similares al asma, que abarcan disnea, sibilancias, fibrosis pulmonar y enfermedad pulmonar obstructiva crónica (Donoghue, 2011; Taiwo, 2006). A estas manifestaciones se pueden asociar otras a nivel del sistema nervioso central como pérdida de memoria, coordinación y problemas de equilibrio (Meyer-Baron, 2007).

En estudios de experimentación, la neurotoxicidad depende de la especie y la edad. En animales susceptibles como conejos y gatos, se caracteriza por el deterioro neurológico progresivo con resultado de muerte asociada con el estatus epiléptico (WHO, 1997). El primer cambio patológico destacado es la acumulación de ovillos neurofibrilares (NFTs) en neuronas grandes, axones proximales y dendritas de las neuronas de muchas regiones del cerebro; esto ha sido observado en los monos, sin embargo en las ratas no se ha presenciado ni NFTs ni encefalopatías. Esto se asocia con la pérdida de sinapsis y la atrofia del árbol dendrítico. También se ha observado deterioro de la función cognitiva y motora y alteraciones del comportamiento. La relación de la neurotoxicidad por aluminio y enfermedades humanas es todavía incierta.

Los pacientes que necesitan someterse a hemodiálisis presentan una sobrecarga de aluminio que procede de la entrada directa en la circulación sanguínea a través del líquido de diálisis (tras 3-7 años de tratamiento) o a través de la ingestión de sales de aluminio (sobre todo de hidróxido de aluminio) que se emplea como quelantes del fósforo; por lo que se aconseja tratar el agua mediante deionización y ósmosis inversa. Las consecuencias clínicas por intoxicación con aluminio son diversas, en primer lugar puede producir dolor óseo por depósito directo, lo que es difícil de diagnosticar en el estudio radiológico y únicamente puede establecerse por biopsia óptica y análisis químico. En segundo lugar se puede producir anemia microcítica que no responde a la terapéutica con hierro. Otras posibles manifestaciones son convulsiones focales, mioclonías, demencia y miopatía proximal.

Diferentes estudios han demostrado un aumento de los niveles de aluminio en los cerebros de personas que padecían Alzheimer, tras realizar la autopsia en comparación con individuos sanos, así como lesiones neurofibrilares en animales de experimentación.

Otros estudios epidemiológicos establecen una relación con la zona de residencia por las altas concentraciones de aluminio en el agua. (Krewski et al.2007) La eficacia reducida de la barrera hematoencefálica en la enfermedad de Alzheimer podría permitir mayor concentración de aluminio en el cerebro por lo que hay controversia de si es causa o consecuencia de la enfermedad. Además, estudios recientes han planteado la posibilidad de que los métodos de tinción en estudios anteriores pueden haber llevado a la contaminación de aluminio (Makjanic "et al.", 1998). Actualmente la etiología más aceptada es que el Alzheimer se produce por un virus de desarrollo lento y procesos autoinmunes.

Los niveles séricos de aluminio son una ayuda en la investigación toxicológica pero no reflejan el contenido corporal total puesto que se encuentra fuertemente ligado con proteínas. Los niveles normales de aluminio se encuentran por debajo de 10µg/ml mientras que en pacientes con diálisis crónica sin manifestaciones tóxicas pueden llegar a 50µg/ml. Los niveles de este metal por encima de 60µg/ml indican absorción incrementada, por encima de 100µg/ml toxicidad potencial y por encima de 200µg/ml se presentan los síntomas clínicos. Los niveles se determinan por espectrofotometría de absorción atómica.

La deferoxamina se ha usado para tratar la encefalopatía y osteomalacia de la diálisis, se sugiere el uso cuando se detectan niveles del metal entre 100-200µg/ml. También se ha utilizado en el diagnóstico de la oteodistrofia debida al aluminio. El EDTA cálcico disódico no parece ser tan efectivo como quelante de aluminio.

No hay medidas especiales para la prevención de la intoxicación por aluminio. Pero en el medio laboral se aconseja mantener las concentraciones del aluminio a niveles por debajo del TLV recomendado. La cerveza, debido a su contenido en silicio podría ejercer un papel protector frente a la toxicidad del aluminio, siendo la cerveza con alcohol y a unas dosis moderadas la que parece disminuir la biodisponibilidad del aluminio de una forma más eficaz. Por tanto un aporte moderado de cerveza podría ser un posible factor protector frente a la neurotoxicidad del aluminio, siendo necesarios la realización de estudios crónicos para confirmar dicho estudio.

En algunos suelos del planeta el aluminio tiende a concentrarse en algunos de los horizontes del perfil, otorgándole características muy particulares. De los 11 órdenes de suelos que se reconocen según la clasificación del Departamento de Agricultura de los Estados Unidos, dos de ellos presentan una alta concentración de aluminio: los oxisoles, que se desarrollan en latitudes tropicales y subtropicales y los spodosoles, que se hallan en climas fríos y bajo vegetación de coníferas. En este tipo de suelos el contenido en nutrientes disponibles para las plantas es bajo, solo el magnesio puede ser abundante en algunos casos; además su elevado contenido en aluminio agrava el problema por su toxicidad para las plantas. En las regiones tropicales y subtropicales en las que se presentan estos suelos lo habitual es que se cultiven plantas con bajas necesidades nutritivas y con fuerte resistencia al aluminio, tales como el té, el caucho y la palma de aceite.





</doc>
<doc id="11569" url="https://es.wikipedia.org/wiki?curid=11569" title="Bauxita">
Bauxita

La bauxita es una roca sedimentaria con un contenido de aluminio relativamente alto. Es la principal fuente de aluminio del mundo. La bauxita consiste principalmente en los minerales de aluminio (Al(OH)), (γ-AlO(OH)) y diásporo (α-AlO(OH)), mezclados con dos óxidos de hierro, goetita y hematita, minerales de la arcilla de aluminio, caolinita, y pequeñas cantidades de anatasa (TiO ) e ilmenita (FeTiO o FeO.TiO).

Puede ser tanto blanda como dura, compuesta por óxidos de aluminio hidratados. Se origina como residuo producido por la meteorización química de una amplia gama de rocas comúnmente ricas en arcilla. Algunas bauxitas tienen un origen más complejo que esto pudiendo ser precipitados químicos reprocesados. Comúnmente se forma en los trópicos en zonas de clima cálido y húmedo.

La bauxita recibió su nombre en alusión a la ciudad de "Les Baux", en Provenza, Francia. En dicho lugar fue identificada por el geólogo Pierre Berthier en 1821 quién la llamó "bauxite", su nombre en francés.

La bauxita puede tener variados colores entre ellos rosado, rojo, crema, café, gris y amarillo. Cuando es de color rojizo esto se debe a óxidos de hierro. La estructura también es variable pudiendo ser porosa, compacta, estratificada, sin estructuras, pisolítica o con estructuras semejantes a vainas. Otras bauxitas preservan la estructura de la roca original siendo seudomórficas. 

De manera simplificada la química de la bauxita se puede expresar en la siguiente fórmula: 

Donde "X" puede ser un número entre 0 y 1.

Es la principal mena del aluminio utilizada por la industria. La bauxita es generalmente extraída por un sistema de minería a cielo abierto, aproximadamente a unos 4-6 metros de profundidad de la tierra. Entre el 85 y 95% de la bauxita extraída por la minería es usada en la producción de aluminio. Hay numerosos depósitos de bauxita, principalmente en las regiones tropicales y subtropicales, así como también en Europa. Entre los principales países donde se extrae la bauxita están Brasil, Jamaica, Australia. El contenido de hierro en las bauxitas eleva el costo de producción de aluminio por lo que las bauxitas con mucho hierro no son deseables para producir aluminio.




</doc>
<doc id="11570" url="https://es.wikipedia.org/wiki?curid=11570" title="Electrólisis">
Electrólisis

La electrólisis es el proceso que separa los elementos de un compuesto por medio de la electricidad. En ella ocurre la liberación de electrones por los aniones en el ánodo (una oxidación) y la captura de electrones por los cationes en el cátodo (una reducción).

Fue descubierta accidentalmente en 1800 por William Nicholson mientras estudiaba el funcionamiento de las baterías. En 1834 el físico y químico inglés Michael Faraday desarrolló y publicó las leyes de la electrólisis que llevan su nombre y acuñó los términos.


En definitiva lo que ocurre es una reacción de oxidación-reducción, donde la fuente de alimentación eléctrica se encarga de aportar la energía necesaria.

Si el agua no es destilada, la electrólisis no solo separa el oxígeno y el hidrógeno, sino los demás componentes que estén presentes como sales, metales y algunos otros minerales (lo que hace que el agua conduzca la electricidad no es el HO, sino que son los minerales. Si el agua estuviera destilada y fuera 100% pura, no tendría conductividad).

Es importante hacer varias consideraciones:



</doc>
<doc id="11571" url="https://es.wikipedia.org/wiki?curid=11571" title="Radio atómico">
Radio atómico

El radio atómico identifica la distancia que existe entre el núcleo, y el orbital más externo de un átomo. Por medio del radio atómico, es posible determinar el tamaño del átomo.

En 1920, poco después de que ya era posible determinar los tamaños de los átomos utilizando la difracción de rayos X, se sugirió que todos los átomos de un mismo elemento tienen el mismo radio. Sin embargo, en 1923, cuando hubo más datos disponibles, se determinó que la aproximación de un átomo como una esfera no se mantiene necesariamente cuando se compara el mismo átomo en cristales con diferentes estructuras.

Definiciones ampliamente usadas de radio atómico incluyen:



En la tabla siguiente figuran los valores en ángstroms publicados por J. C. Slater, con una incertidumbre de 0.12 Å: 


</doc>
<doc id="11572" url="https://es.wikipedia.org/wiki?curid=11572" title="Radio iónico">
Radio iónico

El radio iónico es, al igual que el radio atómico, la distancia entre el centro del núcleo del átomo y el electrón estable más alejado del mismo, pero haciendo referencia no al átomo, sino al ion.
Este aumenta en la tabla de derecha a izquierda en los periodos y de arriba hacia abajo en los grupos.
En el caso de los cationes, la ausencia de uno o varios electrones disminuye la fuerza eléctrica de repulsión mutua entre los electrones restantes, provocando el acercamiento de los mismos entre sí y al núcleo positivo del átomo del que resulta un radio iónico menor que el atómico.

En el caso de los aniones, el fenómeno es el contrario, el "exceso" de carga eléctrica negativa obliga a los electrones a alejarse unos de otros para restablecer el equilibrio de fuerzas eléctricas, de modo que el radio iónico es mayor que el atómico.



</doc>
<doc id="11573" url="https://es.wikipedia.org/wiki?curid=11573" title="Catión">
Catión

Un catión es un ion con carga eléctrica positiva, es decir, que ha perdido electrones. Los cationes se describen con un estado de oxidación positivo. En términos químicos, es cuando un átomo neutro pierde uno o más electrones de su dotación original, este fenómeno se conoce como ionización.

Ion: En química, se define al ion (del griego "ión" ("ιών"), participio presente de "ienai" ("ιεναι") "ir", de ahí "el que va") como una especie química, ya sea un átomo o una molécula, cargada eléctricamente.

Las sales típicamente están formadas por cationes y aniones (aunque el enlace nunca es puramente iónico, siempre hay una contribución covalente).

También los cationes están presentes en el organismo en elementos conocidos como, el sodio (Na) y el potasio (K) en forma de sales ionizadas.

Ejemplo:
El catión K es un K que perdió un electrón para quedar isoelectrónico con el argón.
El Mg es un Mg que perdió 2 electrones para quedar isoelectrónico con el neón.

Desde la publicación en 2005 del "Libro Rojo" de la IUPAC de recomendaciones para la nomenclatura y formulación inorgánicas, la nomenclatura tradicional o antigua con las terminaciones "-oso" e "-ico" es desaconsejada salvo para los oxoácidos.
Estos cationes pueden ser identificados por medio de distintas reacciones en medios tanto ácidos como básicos, dependiendo de las propiedades de los mismos.

Los cationes juegan muchos papeles en los procesos biológicos. Los gradientes de concentración de diversos cationes (Na, K, etc.) a través de las membranas celulares mantienen diferentes potenciales electroquímicos que son empleados para transportar diferentes moléculas orgánicas al interior de las células por difusión facilitada. También promueven la contracción muscular, la transmisión de impulsos nerviosos, etc. Además, los cationes metálicos están presentes en los sitios activos de muchas enzimas formando parte de funciones catalíticas, etc.

En medicina se emplean complejos de cationes paramagnéticos como el Gd como agentes de contraste en RMI (resonancia magnética de imágenes) de tejidos blandos.
También se emplea el cisplatino como medicamento contra el cáncer debido a su coordinación al DNA impidiendo su replicación y, por tanto, impidiendo el crecimiento de células tumorales que son las de mayor tasa de crecimiento.



</doc>
<doc id="11574" url="https://es.wikipedia.org/wiki?curid=11574" title="Anión">
Anión

Un anión es un ion (o ión) con carga eléctrica negativa, es decir, que ha ganado más electrones. Los aniones monoatómicos se describen con un estado de oxidación negativo. Los aniones poliatómicos se describen como un conjunto de átomos unidos con una carga eléctrica global negativa, variando sus estados de oxidación individuales.

Hay tres tipos de aniones: monoatómicos, poliatómicos y ácidos.

Se pueden considerar como procedentes de una molécula que ha ganado electrones, o de un ácido que ha perdido protones.

Se nombran con la palabra ion o anión, seguida del nombre del no metal terminado en "-ito" si actúa con la valencia menor o en "-ato" si actúa con la valencia mayor. Ejemplo:

Se nombran como los ácidos pero anteponiendo la palabra ion o anión, y quitando ""de hidrógeno"". Ejemplo:

Proceden de un ácido poliprótico que ha perdido parte de sus átomos de hidrógeno como protones.

Se nombran como el ion correspondiente pero añadiendo la palabra ácido y usando prefijos multiplicativos cuando haya más de uno.

Para los ácidos dipróticos (con dos hidrógenos en su fórmula) se mantiene aún en el comercio y la industria un sistema de nomenclatura antiguo pero no recomendado. Consiste en nombrar el anión con el prefijo bi-.

Se nombran como el ion correspondiente pero anteponiendo el prefijo "hidrógeno-" con el prefijo multiplicativo correspondiente.

Para un mejor entendimiento realizamos un esquema de clasificación puesto que no es una clasificación rígida.

Desprenden gases con el ácido clorhídrico o sulfúrico diluido: carbonato, bicarbonato, sulfito, tiosulfato, sulfuro, nitrito, hipoclorito, cianuro y cianato. Están incluidos los del (I) con el agregado de los siguientes: floruro, cloruro, bromuro, yoduro, nitrato, clorato, perclorato, bromato y yodato, borato *, ferrocianuro, ferricianuro, tiocianato, formiato, acetato, oxalato , tartrato y citrato.

Reacciones de precipitación: sulfato, persulfato **, fosfato, fosfito, hipofosfito, arseniato, arsenito, silicato, fluorosilicato, salicilato, benzoato y succinato. Reacciones de oxidación y reducción en disolución: manganato, permanganato, cromato y dicromato.

Los aniones más frecuentes en un laboratorio no se pueden separar de forma tan clara como los cationes. La mayor parte de las veces se van a identificar de forma directa, mientras que otras se van a separar en grandes grupos precipitando con cationes y, a partir de estos precipitados, se identifican esos aniones. Sin embargo, en laboratorio es bastante más difícil analizar los aniones presentes que los cationes. Generalmente en el laboratorio la marcha analítica de aniones se hace primero eliminando todos los cationes existentes precipitando con hidróxido de sodio o carbonato de sodio. A continuación se hacen tres ensayos preliminares.

Las sales típicamente están formadas por cationes y aniones (aunque el enlace nunca es puramente iónico, siempre hay una contribución covalente).


</doc>
<doc id="11575" url="https://es.wikipedia.org/wiki?curid=11575" title="Energía de ionización">
Energía de ionización

La energía de ionización ("E") es la energía necesaria para separar un electrón en su estado fundamental de un átomo de un elemento en estado gaseoso. La reacción puede expresarse de la siguiente forma:

Siendo formula_2 los átomos en estado gaseoso de un determinado elemento químico; formula_3, la energía de ionización y formula_4 un electrón.

Esta energía corresponde a la primera ionización. La segunda energía de ionización representa la energía precisa para sustraer el segundo electrón; esta segunda energía de ionización es siempre mayor que la primera, pues el volumen de un ion positivo es menor que el del átomo y la fuerza electrostática atractiva que soporta este segundo electrón es mayor en el ion positivo que en el átomo, ya que se conserva la misma carga nuclear.

La energía de ionización se expresa en electronvoltios, julios o en kilojulios por mol (kJ/mol).

En los elementos de una misma familia o grupo, la energía de ionización disminuye a medida que aumenta el número atómico, es decir, de arriba abajo.

Sin embargo, el aumento no es continuo, pues en el caso del berilio se obtienen valores más altos que lo que podía esperarse por comparación con los otros elementos del mismo periodo. Este aumento se debe a la estabilidad que presentan las configuraciones s y sp, respectivamente.

La energía de ionización más elevada corresponde a los gases nobles, ya que su configuración electrónica es la más estable, y por tanto habrá que proporcionar más energía para arrancar los electrones.

El potencial de ionización ("P") es la energía mínima requerida para separar un electrón de un átomo o molécula específica a una distancia tal que no exista interacción electrostática entre el ion y el electrón.Inicialmente se definía como el potencial mínimo necesario para que un electrón saliese de un átomo que queda ionizado. El potencial de ionización se medía en voltios. En la actualidad, sin embargo, se mide en electronvoltios (aunque no es una unidad del SI) aunque está aceptada o en julios por mol.
El sinónimo de energía de ionización (E) se utiliza con frecuencia. La energía para separar el electrón unido más débilmente al átomo es el primer potencial de ionización; sin embargo, hay alguna ambigüedad en la terminología. Así, en química, el segundo potencial de ionización del litio es la energía del proceso.

En física, el segundo potencial de ionización es la energía requerida para separar un electrón del nivel siguiente al nivel de energía más alto del átomo neutro o molécula, p.

Se puede estudiar como pi=q/r, siendo "q" la carga del elemento.

La forma más directa es mediante la aplicación de la espectroscopia atómica. A base del espectro de radiación de luz, que desprende básicamente colores en el rango de la luz visible, se pueden determinar los niveles de energía necesarios para desprender cada electrón de su órbita.

Lo más destacado de las propiedades periódicas de los elementos se observa en el incremento de las energías de ionización cuando recorremos la tabla periódica de izquierda a derecha, lo que se traduce en un incremento asociado de la electronegatividad, contracción del tamaño atómico y aumento del número de electrones de la capa de valencia. La causa de esto es que la carga nuclear efectiva se incrementa a lo largo de un periodo, generando, cada vez, más altas energías de ionización. Existen discontinuidades en esta variación gradual tanto en las tendencias horizontales como en las verticales, que se pueden razonar en función de las especificidades de las configuraciones electrónicas.

Vamos a destacar algunos aspectos relacionados con la primera energía de ionización que se infieren por el bloque y puesto del elemento en la tabla periódica:





En general, las energías de ionización descienden a lo largo de las columnas de la tabla periódica y crecen de izquierda a derecha a lo largo de un período de la tabla. La energía de ionización muestra una fuerte anti-correlación con el radio atómico. La siguiente tabla muestra los valores de la primera energía de ionización de los elementos expresada en eV:
Cuanto más nos desplacemos hacia la derecha y hacia arriba en la tabla periódica, mayor es la energía de ionización.



</doc>
<doc id="11576" url="https://es.wikipedia.org/wiki?curid=11576" title="Redes de Bravais">
Redes de Bravais

En geometría y cristalografía las redes de Bravais son una disposición infinita de puntos discretos cuya estructura es invariante bajo cierto grupo de traslaciones. En la mayoría de casos también se da una invariancia bajo rotaciones o simetría rotacional. Estas propiedades hacen que desde todos los nodos de una red de Bravais se tenga la misma perspectiva de la red. Se dice entonces que los puntos de una red de Bravais son equivalentes.

Mediante teoría de grupos se ha demostrado que solo existe una única red de Bravais unidimensional, 5 redes bidimensionales y 14 modelos distintos de redes tridimensionales.

La red unidimensional es elemental siendo ésta una simple secuencia de nodos equidistantes entre sí. En dos o tres dimensiones las cosas se complican más y la variabilidad de formas obliga a definir ciertas estructuras patrón para trabajar cómodamente con las redes.

Para generar estas normalmente se usa el concepto de celda primitiva. Las celdas unitarias, son paralelogramos (2D) o paralelepípedos (3D) que constituyen la menor subdivisión de una red cristalina que conserva las características generales de toda la retícula, de modo que por simple traslación de la misma, puede reconstruirse la red al completo en cualquier punto.

Una red típica R en formula_1 tiene la forma:
donde {"a"..., "a"} es una base en el espacio R. Puede haber diferentes bases que generen la misma red pero el valor absoluto del determinante de los vectores "a" vendrá siempre determinado por la red por lo que se lo puede representar como d(R).

Las celdas unitarias se pueden definir de forma muy simple a partir de dos vectores (2D) o tres vectores (3D). La construcción de la celda se realiza trazando las paralelas de estos vectores desde sus extremos hasta el punto en el que se cruzan. Existe un tipo de celda unitaria que se construye de un modo distinto y que presenta ciertas ventajas en la visualización de la red ya que posee la misma simetría que la red, es la celda de Wigner-Seitz. Una celda unitaria se caracteriza principalmente por contener un único nodo de la red de ahí el adjetivo de "unitaria". Si bien en muchos casos existen distintas formas para las celdas unitarias de una determinada red el volumen de toda celda unitaria es siempre el mismo.

En ocasiones resulta más sencillo construir otro tipo de celdas que sin ser unitarias describen mejor la estructura de la red que tratamos. Este tipo de celdas se denominan celdas convencionales. Estas tienen, a su vez, sus propios parámetros de red y un volumen determinado.

Todas estas celdas se consideran celdas primitivas ya que son capaces de cubrir todo el espacio mediante traslaciones sin que queden huecos ni solapamientos. Sus diferencias o características son las siguientes:

Empaquetamiento compacto: Esto es cuando los átomos de la celda están en contacto unos con otros. No siempre será así y en muchos casos mediará una distancia mínima entre las nubes electrónicas de los diferentes átomos.

Parámetro de red: Es la longitud de los lados de la celda unitaria. Puede haber tan solo uno, dos o hasta tres parámetros de red distintos dependiendo del tipo de red de Bravais que tratemos. En las estructuras más comunes se representa con la letra "a" y con la "c" en caso de haber dos.

Nodos o átomos por celda: Tal y como dice el nombre es el número de nodos o átomos que posee cada celda. Una celda cuadrada, por ejemplo, poseerá un nodo por celda ya que cada esquina la comparte con cuatro celdas más. De hecho si una celda posee más de un nodo de red es que no es unitaria, en cambio si posee más de un átomo por celda pudiera ser que estuviésemos en una celda unitaria pero con una base atómica de más de un átomo.

Número de coordinación: Es el número de puntos de la red más cercanos, los primeros vecinos, de un nodo de la red. Si se trata de una estructura con empaquetamiento compacto el número de coordinación será el número de átomos en contacto con otro. El máximo es 12.

Factor de empaquetamiento atómico: Fracción del espacio de la celda unitaria ocupada por los átomos, suponiendo que estos son esferas sólidas.

formula_3

Donde "f" es el factor de empaquetamiento o fracción de volumen ocupado, "n" el número de átomos por celda, v el volumen del átomo y V el volumen de la celda. Normalmente se suele dar el factor de empaquetamiento compacto para las diferentes celdas como indicador de la densidad de átomos que posee cada estructura cristalina. En este caso los átomos se tratan como esferas rígidas en contacto con sus vecinos más cercanos.

Densidad: A partir de las características de la red, puede obtenerse la densidad teórica del material que conforma la red mediante la siguiente expresión.

formula_4

Donde ρ es la densidad, "N" el número de Avogadro y "m" la masa atómica.

Volumen de la celda unitaria primitiva: Toda celda unitaria tiene el mismo volumen representado por la siguiente fórmula.
formula_5
Donde "a" son los vectores de la base de la red.

Según los ángulos y la distancia entre los nodos se distinguen 5 redes distintas.

En función de los parámetros de la celda unitaria, longitudes de sus lados y ángulos que forman, se distinguen 7 sistemas cristalinos.

Para determinar completamente la estructura cristalina elemental de un sólido, además de definir la forma geométrica de la red, es necesario establecer las posiciones en la celda de los átomos o moléculas que forman el sólido cristalino; lo que se denominan puntos reticulares. Las alternativas son las siguientes:

Combinando los 7 sistemas cristalinos con las disposiciones de los puntos de red mencionados, se obtendrían 28 redes cristalinas posibles. En realidad, como puede demostrarse, solo existen 14 configuraciones básicas, pudiéndose obtener el resto a partir de ellas. Estas estructuras se denominan redes de Bravais.

En el caso más sencillo, a cada punto de red le corresponderá un átomo, pero en estructuras más complicadas, como materiales cerámicos y compuestos, cientos de átomos pueden estar asociados a cada punto de red formando celdas unitarias extremadamente complejas. La distribución de estos átomos o moléculas adicionales se denomina base atómica y esta nos da su distribución dentro de la celda unitaria.

Existen dos casos típicos de bases atómicas. La estructura del diamante y la hexagonal compacta. Para redes bidimensionales un caso ejemplar sería el grafito cuya estructura sigue un patrón de red en panal.

Los nombres (BCC, HCP, FCC) están en nomenclatura internacional o inglesa.



</doc>
<doc id="11577" url="https://es.wikipedia.org/wiki?curid=11577" title="Sistema cristalino">
Sistema cristalino

Un sólido cristalino se construye a partir de la repetición en el espacio de una estructura elemental paralelepipédica denominada celda unitaria. En función de los parámetros de red, es decir, de las longitudes de los lados o ejes del paralelepípedo elemental y de los ángulos que forman, se distinguen siete sistemas cristalinos:

En función de las posibles localizaciones de los átomos en la celda unitaria se establecen 14 estructuras cristalinas básicas, las denominadas redes de Bravais.

El tipo de sistema normal cristalino depende de la disposición simétrica y repetitiva de las caras que forman el cristal. Dicha disposición es consecuencia del ordenamiento interno de sus átomos y, por lo tanto, característico de cada mineral. Las caras se dispondrán según los elementos de simetría que tenga ese sistema, siendo uno de ellos característico de cada uno de los siete sistemas:


</doc>
<doc id="11579" url="https://es.wikipedia.org/wiki?curid=11579" title="Experimento de Rutherford">
Experimento de Rutherford

Los experimentos de Rutherford fueron una serie de experimentos históricos mediante los cuales los científicos descubrieron que cada átomo tiene un núcleo donde tiene las cargas positivas y la mayor parte de su masa se concentran. Ellos dedujeron esto midiendo cómo un haz de partículas alfa se dispersa cuando golpea una delgada hoja metálica. Los experimentos se realizaron entre 1908 y 1924 por Hans Geiger y Ernest Marsden bajo la dirección de Ernest Rutherford en los laboratorios de la Universidad de Mánchester.

La teoría popular de la estructura atómica fue la de JJ Thomson. Thomson fue el científico que descubrió el electrón que forma parte de cada átomo. Thomson creía que el átomo era una esfera de carga positiva en la cual estaban dispuestos los electrones. Los protones y los neutrones eran desconocidos en esa época.

El modelo de Thomson no fue universalmente aceptado. Thomson mismo no fue capaz de desarrollar un modelo estable y completo de su concepto. Hantaro Nagaoka, un científico japonés, lo rechazó alegando que las cargas eléctricas opuestas no pueden penetrar entre sí. En cambio, propuso que los electrones orbitaban la carga positiva como los anillos de Saturno.

Una partícula alfa es una partícula sub-microscópica con una carga positiva. Según el modelo de Thomson, si una partícula alfa chocara un átomo, pasaría directamente a través. A escala atómica, el concepto de «materia sólida» carece de sentido, por lo que la partícula alfa no rebotaría en el átomo como si fueran canicas. Solo se vería afectada por los campos eléctricos del átomo, y en el modelo de Thomson los campos eléctricos eran demasiado débiles para afectar una partícula alfa pasajera en un grado significativo. Ambas cargas negativas y positivas dentro del átomo de Thomson se extienden sobre todo el volumen del átomo. De acuerdo con la Ley de Coulomb, cuanto menos concentrada es una esfera de carga eléctrica, más débil será su campo eléctrico en su superficie.
Como ejemplo trabajado, considere una partícula alfa que pasa tangencialmente a un átomo de oro de Thomson, donde experimentará el campo eléctrico en su punto más fuerte y, de este modo, experimentará la máxima deflexión "θ". Puesto que los electrones son muy ligeros comparados con la partícula alfa, su influencia puede ser despreciada y el átomo puede ser visto como una esfera de carga positiva.

Usando la física clásica, el cambio lateral de la partícula alfa en el momento "Δp" puede ser aproximado usando el impulso de la relación de fuerza y la expresión fuerza de Coulomb.

El cálculo anterior no es más que una aproximación, pero está claro que la deflexión a lo sumo estará en el orden de una pequeña fracción de un grado. Si la partícula alfa pasara a través de una lámina de oro de unos 400 átomos de espesor y experimentara una deflexión máxima en la misma dirección (poco probable), seguiría siendo una pequeña deflexión.

A petición de Rutherford, Geiger y Marsden realizaron una serie de experimentos en los que dirigieron un haz de partículas alfa en una fina lámina de oro y midieron el patrón de dispersión usando una pantalla fluorescente. Detectaron partículas alfa rebotando en la hoja de oro en todas las direcciones, algunas de vuelta en la fuente. Esto debía ser imposible según el modelo de Thomson. Obviamente, esas partículas habían encontrado una fuerza electrostática mucho mayor que el modelo de Thomson, lo que a su vez implicaba que la carga positiva del átomo se concentraba en un volumen mucho más pequeño de lo que Thomson imaginaba.

Cuando Geiger y Marsden dispararon partículas alfa en sus láminas, se dieron cuenta de que solo una pequeña fracción de las partículas alfa se desvió en más de 90°. La mayoría voló directamente a través de la lámina. Esto sugirió que esas esferas minúsculas de la carga positiva intensa fueron separadas por vastos golfos del espacio vacío. La mayoría de las partículas pasaron a través del espacio vacío con una desviación mínima, y una pequeña fracción golpeó los núcleos y se desvió fuertemente.

Rutherford rechazó así el modelo de Thomson, y en cambio propuso un modelo en el que el átomo consistía en su mayoría espacio vacío, con toda su carga positiva concentrada en el centro de un volumen muy pequeño, rodeado por una nube de electrones.
En resumen: la mayoría de los rayos alfa atravesaron la lámina sin dividirse, la mayor parte del espacio de un átomo es espacio vacío. Hay una densa y diminuta región que llamó núcleo, que contiene carga positiva y casi toda la masa del átomo; algunos rayos se desviaron porque pasan muy cerca del centro con carga eléctrica del mismo tipo que los rayos alfa (carga positiva); muy pocos rebotaron porque chocaron frontalmente contra ejes centros de carga positiva.

Ernest Rutherford fue profesor de física en la Universidad de Mánchester. Ya había recibido numerosos honores por sus estudios de radiación. Había descubierto la existencia de rayos alfa; rayos beta y rayos gamma, y había demostrado que estos eran la consecuencia de la desintegración de los átomos. En 1906, recibió la visita de un físico alemán llamado Hans Geiger, y quedó tan impresionado que le pidió a Geiger que se quedara y le ayudara en sus investigaciones. Ernest Marsden era un estudiante de licenciatura en física que estudiaba bajo Geiger.

Las partículas alfa son pequeñas partículas positivamente cargadas que son emitidas espontáneamente por ciertas sustancias como el uranio y el radio. El propio Rutherford los había descubierto en 1899. En 1908 estaba tratando de medir con precisión su relación de carga-masa. Para hacer esto, primero necesitaba saber cuántas partículas alfa su muestra de radio estaba emitiendo (después de lo cual mediría su carga total y dividiría una por la otra). Las partículas alfa son demasiado pequeñas para ser vistas incluso con un microscopio, pero Rutherford sabía que las partículas alfa ionizan las moléculas de aire, y si el aire está dentro de un campo eléctrico, los iones producirán una corriente eléctrica. En este principio, Rutherford y Geiger diseñaron un dispositivo de conteo simple que consistió en dos electrodos en un tubo de cristal. Cada partícula alfa que pasaba por el tubo creaba un pulso electricidad que podía ser contado. Era una versión temprana del contador Geiger.

Los experimentos que diseñaron involucraron bombardear una lámina metálica con partículas alfa para observar cómo la lámina los dispersó en relación con su espesor y material. Utilizaron una pantalla fluorescente para medir las trayectorias de las partículas. Cada impacto de una partícula alfa en la pantalla produjo un pequeño destello de luz. Geiger trabajó en un laboratorio oscurecido durante horas y horas, contando estos pequeños centellos con un microscopio. Rutherford carecía de la resistencia para este trabajo, por lo que se lo dejó a sus colegas más jóvenes. Para la lámina metálica, probaron una variedad de metales, pero preferían el oro porque podían hacer que la lámina fuera muy fina, ya que el oro es muy maleable. Como fuente de partículas alfa, la sustancia de elección de Rutherford era el radio, una sustancia varios millones de veces más radiactiva que el uranio.

Un artículo de 1908 por Geiger, «Sobre la Dispersión de Partículas por Materia», describe el siguiente experimento. Geiger construyó un largo tubo de vidrio de casi dos metros de longitud. En un extremo del tubo había una cantidad de "emanación de radio" (R) que servía como fuente de partículas alfa. El extremo opuesto del tubo se cubrió con una pantalla fosforescente (Z). En el centro del tubo había una hendidura de 0,9 mm de ancho. Las partículas alfa de R pasaron a través de la hendidura y crearon un parche brillante de luz en la pantalla. Se utilizó un microscopio (M) para contar los centelleos en la pantalla y medir su propagación. Geiger bombeó todo el aire del tubo para que las partículas alfa estuvieran desobstruidas y dejaron una imagen limpia y apretada en la pantalla que correspondía a la forma de la hendidura. Geiger entonces dejó un poco de aire en el tubo, y el parche brillante se hizo más difuso. Geiger luego bombeó el aire y colocó una hoja de oro sobre la ranura en AA. Esto también hizo que el parche de luz en la pantalla se extendiera más. Este experimento demostró que tanto el aire como la materia sólida podrían dispersar notablemente las partículas alfa. El aparato, sin embargo, solo podía observar pequeños ángulos de deflexión. Rutherford quería saber si las partículas alfa estaban siendo esparcidas por ángulos aún mayores-quizás más de 90°.

En un artículo de 1909, «En una Reflexión Difusa de las Partículas Alfa», Geiger y Marsden describieron el experimento mediante el cual demostraron que las partículas alfa pueden ser dispersadas por más de 90°. En su experimento prepararon un pequeño tubo de vidrio cónico (AB) que contenía radio, y su apertura fue sellada con mica. Esto fue su emisor de partículas alfa. Ellos montaron una placa de plomo (P), detrás de la cual se colocó una pantalla fluorescente (S). Ellos posicionaron el tubo de radio en el otro lado de la placa, de tal manera que las partículas alfa que emitió no pudieron golpear directamente la pantalla. Ellos notaron unos cuanto centelleos en la pantalla. Se debía a que algunas partículas alfa evitaron la placa de plomo rebotando en las moléculas de aire. Luego colocaron una lámina de metal (R) en el lado de la placa de plomo. Se dieron cuenta de más centelleos en la pantalla porque las partículas alfa estaban rebotando en la lámina. Contando los centelleos, notaron que los metales con mayor masa atómica, como el oro, reflejaban más partículas alfa que las más ligeras como el aluminio.

Geiger y Marsden entonces querían estimar el número total de partículas alfa que se estaban reflejando. La configuración anterior no era adecuada para ello porque el tubo contenía varias sustancias radiactivas (radio y sus productos de desintegración) y, por lo tanto, las partículas alfa emitidas tenían rangos variables y porque era difícil para ellos determinar a qué velocidad emitía el tubo partículas alfa. Esta vez, colocaron una pequeña cantidad de radio C (bismuto-214) sobre una placa de plomo, que rebotó sobre un reflector de platino (R) y sobre la pantalla. Ellos encontraron que solo una pequeña fracción de las partículas alfa que golpeó el reflector rebotó en la pantalla (1 en 8000).

Un artículo de 1910 de Geiger, «La dispersión de las α-partículas por materia», describe un experimento mediante el cual intentó medir cómo el ángulo más probable a través del cual se desvía una partícula alfa varía con el material por el que pasa, el espesor de dicho material, y la velocidad de las partículas alfa. Geiger construyó un tubo de vidrio hermético del que se bombeaba el aire. En un extremo había un bulbo (B) que contenía "emanción de radio" (radón-222). Por medio de mercurio, el radón en B fue bombeado por el estrecho del tubo hacia una pantalla de sulfuro de zinc fluorescente (S). El microscopio que utilizó para contar los centelleos en la pantalla fue fijado a una escala de milímetro vertical con un vernier, lo que permitió a Geiger para medir con precisión donde los destellos de luz apareció en la pantalla y así calcular los ángulos de las partículas de deflexión. Las partículas alfa emitidas desde A se estrecharon a una viga por un pequeño orificio circular en D. Geiger colocó una lámina de metal en la trayectoria de los rayos en D y E para observar cómo cambió la zona de destellos. También podría variar la velocidad de las partículas alfa colocando hojas extra de mic o aluminio en A.

A partir de las meidiciones que tomó, Geiger llegó a las siguientes conclusiones:


En 1911, Rutherford publicó un documento histórico en 1911 titulado «La dispersión de partículas alfa y beta por materia y la estructura del átomo» en el que propuso que el átomo contenga en su centro un volumen de carga eléctrica que es muy pequeño e intenso (Rutherford lo trató como una carga puntual en sus ecuaciones). A los efectos de sus ecuaciones, supuso que esta carga central era positiva, pero admitió que no podía probar esto todavía.

Rutherford desarrolló un ecuación que modelaba cómo la lámina debía dispersar las partículas alfa si toda la carga positiva y la mayor parte de la masa atómica se concentraban en un solo punto en el centro de un átomo.
formula_4

En un artículo de 1913, «Las leyes de la deflexión de las partículas α mediante ángulos grandes», Geiger y Marsden describen una serie de experimentos mediante los cuales intentaron verificar experimentalmente la ecuación anterior que desarrolló Rutherford. La ecuación de Rutherford predijo que el número de centelleos por minuto ("s") que se observará en un ángulo dado ("Φ") debería ser proporcional a:


Su artículo de 1913 describe cuatro experimentos por los cuales demostraron cada una estas cuatro relaciones.

Para probar cómo la dispersión varió con el ángulo de deflexión (i.e., si "s ∝ cscΦ/2") Geiger y Marsden construyeron un aparato que consistía en un cilindro de metal hueco montado en un plato giratorio. Dentro del cilindro había una lámina metálica (F) y una fuente de radiación que contenía radón (R), montada sobre una columna separada (T) que permitía que el cilindro girara independientemente. La columna era también un tubo por el cual se bombeaba aire fuera de cilindro. Un microscopio (M) con su objetivo cubierto por una pantalla fluorescente de sulfuro de zinc (S) penetró en la pared del cilindro y apuntó a la hoja metálica. Al girar la mesa, el microscopio se puede mover un círculo alrededor de la lámina, permitiendo que Geiger observe y cuente las partículas alfa desviadas hasta 150°. Corrigiendo el error experimental, Geiger y Marsden encontraron que el número de partículas alfa que son desviadas por un ángulo "Φ" es en efecto proporcional a "cscΦ/2".

Geiger y Marsden luego probabron cómo la dispersión varió con el espesor de la lámina (i.e. if "s ∝ t"). Construyeron un disco (S) con seis orificios perforados en él. Los orificios fueron cubiertos con láminas de metal de espesor variable, or ninguno para el control. Este disco se selló entonces en un anillo de latón (A) entre dos pplacas de vidrio (B y C). El disco podría ser girado por media de una barra (P) para llevar cada ventana delante de la fuente de partículas alfa (R). En el panel de vidrio trasero se encontraba una pantalla de sulfuro de zinc (Z). Geiger y Marsden observaron que el número de centelleos que aparecieron en la pantalla era en realidad proporcional al espesor, siempre y cuando dicho espesor fuera pequeño.

Geiger y Marsden reutilizaron el aparato anterior para medir cómo el patrón de dispersión varió con el cuadrado de la carga nuclear (i.e. si "s ∝ Q"). Geiger y Marsden supusieron que la carga del núcleo era proporcional al peso atómico del elemento, por lo que probaron si la dispersión era proporcional al peso atómico al cuadrado. Geiger y Marsden cubrían los agujeros del disco con láminas de oro, estaño, plata, cobre y aluminio. Medían el poder de frenado de cada lámina al equipararlo a un espesor equivalente de aire. Contaron el número de centelleos por minuto que cada lámina produjo en la pantalla. Dividieron el número de centelleos por minuto por el equivalente de aire. Contaron el número de centelleos por minuto que cada lámina produjo en la pantalla. Dividieron el número de centelleos por minuto por el quivalente de aire de la lámina respectiva, luege se dividieron de nuevo por la raíz cuadrada del peso atómico (ellos sabían que para las láminas de igual poder de frenado, el número de átomos por unidad de área es proporcional a la raíz cuadrada del peso atómico). Así, para cada metal, Geiger y Marsden obtuvieron el número de centelleos que produce un número fijo de átomos. Para cada metal, entonces dividieron este número por el cuadrado del peso atómico, y encontraron que las proporiciones eran más o menos iguales. Así probaron que "s ∝ Q".

Por último, Geiger y Marsden probado cómo la dispersión varió con la velocidad de las partículas alfa (i.e. si "s α 1/v"). Utilizando de nuevo el mismo aparato, ellos retardaron las partículas alfa colocando hojas adicionales de mica delante de la fuente de partículas alfa. Observaron que, dentro del rango de error experimental, que el número de escintilaciones era en realidad proporcional a "1/v".

En su artículo de 1911, Rutherford supuso que la carga central del átomo estaba cargada positivamente, pero reconoció que no podía decir con seguridad, ya que una carga negativa o positiva habría sido adecuada a su modelo de dispersión. Los resultados de otros experimentos confirmaron su hipótesis. En un artículo de 1913, Rutherford declaró que el «núcleo» estaba cargado positivamente, basado en el resultado de experimento que exploraban la dispersión de partículas alfa en varios gases.

En 1917, Rutherford y su asistente William Kay comenzaron a explorar el paso de las partículas alfa a través de gases como el hidrógeno y el nitrógeno. En un experimento en el que dispararon un haz de partículas alfa a través del hidrógeno, las partículas alfa golpearon los núcleos de hidrógeno hacia adelante en la dirección de la viga, no hacia atrás. En un experimento en el que dispararon partículas alfa a través de nitrógeno, descubrió que las partículas alfa golpearon a núcleos de hidrógeno (i.e. protones) fuera de los núcleos de nitrógeno.

El descubrimiento del núcleo fue uno de los descubrimientos científicos más importantes de todos los tiempos. Debido a que reveló la estructura de toda la materia, afectó a todos los campos científicos y de ingeniería.





</doc>
<doc id="11581" url="https://es.wikipedia.org/wiki?curid=11581" title="Cuanto">
Cuanto

En física, el término cuanto o cuantio (del latín "quantum", plural "quanta", que significa «cantidad») denota en la física cuántica tanto el valor mínimo que puede tomar una determinada magnitud en un sistema físico, como la mínima variación posible de este parámetro al pasar de un estado discreto a otro. Se habla de que una determinada magnitud está cuantizada según el valor de cuanto. Es decir, el cuanto es una proporción determinada por la magnitud dada.

Un ejemplo del modo en que algunas cantidades relevantes de un sistema físico están cuantizadas se encuentra en el caso de la carga eléctrica de un cuerpo, que solo puede tomar un valor que sea un múltiplo entero de la carga del electrón. En la moderna teoría cuántica aunque se sigue hablando de cuantización el término cuanto ha caído en desuso. El hecho de que las magnitudes estén cuantizadas se considera ahora un hecho secundario y menos definitorio de las características esenciales de la teoría.

En informática, un cuanto de tiempo es un pequeño intervalo de tiempo que se asigna a un proceso para que ejecute sus instrucciones. El cuanto es determinado por el planificador de procesos utilizando algún algoritmo de planificación.

La palabra «quantum» viene del latín «quantus», por «cuánta [cantidad]». «Quanta», abreviatura de «quanta de electricidad» (electrones) fue utilizada por Philipp Lenard en un artículo de 1902 sobre el efecto fotoeléctrico, quien acreditó a Hermann von Helmholtz el uso de la palabra en el campo de la electricidad. Sin embargo, la palabra «quantum» en general era bien conocida antes de 1900, y a menudo era utilizada por médicos, como por ejemplo en el término quantum satis. Tanto Helmholtz como Julius von Mayer eran médicos, además de físicos. Helmholtz utilizó «quantum» haciendo referencia a calentar en su artículo sobre el trabajo de Mayer, y de hecho, la palabra «quantum» se puede encontrar en la formulación de la primera ley de la termodinámica de Mayer en su carta de 24 de julio de 1841. Max Planck utilizó «quanta» para significar «quanta de materia y electricidad», de gas y de calor. En 1905, en respuesta al trabajo de Planck y al trabajo experimental de Lenard, que había explicado sus resultados usando el término «quanta de electricidad», Albert Einstein sugirió que la radiación existía en paquetes espacialmente localizados que llamó "cuantos de luz" ("Lightquanta").
El concepto de cuantización de la radiación fue descubierto en 1900 por Max Planck, que había estado tratando de entender la emisión de radiación de los objetos calientes, conocida como radiación del cuerpo negro. Al asumir que la energía solo puede ser absorbida o liberada en paquetes discretos, pequeños, diferenciales, que llamó "paquetes" o "elementos de energía", Planck explicó el hecho de que ciertos objetos cambiaban de color cuando se calentaban. El 14 de diciembre de 1900, Planck informó de sus hallazgos revolucionarios a la Deutsche Physikalische Gesellschaft, e introdujo la idea de la cuantificación por primera vez como parte de su investigación sobre la radiación del cuerpo negro. Como resultado de sus experimentos, Planck dedujo el valor numérico de "h", conocido como constante de Planck, y también pudo reportar un valor más preciso para el número de Avogadro-Loschmidt, el número de moléculas reales en un mol y la unidad de carga eléctrica, a la Sociedad Física Alemana. Después de que se validase su teoría, Planck fue galardonado con el Premio Nobel de Física en 1918 por su descubrimiento.


</doc>
<doc id="11583" url="https://es.wikipedia.org/wiki?curid=11583" title="Número de coordinación">
Número de coordinación

En química y física del estado sólido, el número de coordinación de un átomo en un compuesto químico es el número de átomos unidos directamente a él. Por ejemplo, en el metano el número de coordinación del átomo de carbono es 4.

En química orgánica, el número de coordinación, que se representa por las letras griegas sigma (σ) o delta (δ) con un superíndice, y se aplica al caso de los compuestos organometálicos es el número de átomos a los que está directamente enlazado el átomo central, o al número de enlaces σ del átomo central, 

En ciencia de materiales y en química del estado sólido, el número de coordinación (NC) es el número de vecinos que están en contacto directo con un átomo o ion en particular en una red o estructura cristalina.

En química inorgánica el número de coordinación es el número de átomos, iones o moléculas que un átomo o ion central mantienen como sus vecinos cercanos en un complejo de coordinación o un cristal. Puede variar desde 2 hasta 12, siendo 6 el más común. Podemos definir también el número de coordinación como el número de pares electrónicos que acepta un ácido de Lewis (por lo general un centro metálico), es decir, si un compuesto de coordinación tiene dos especies que estén donando pares de electrones, entonces tendrá un número de coordinación 2. El número de coordinación de un complejo está influenciado por los tamaños relativos del ion metálico y de los ligandos, así como de los factores electrónicos, los cuales cambiarán dependiendo de la configuración electrónica del ion metálico.

Dependiendo de la relación de radio se puede observar que cuando mayor sea la carga del ion metálico, más atracción habrá hacia ligandos negativamente cargados, sin embargo al mismo tiempo, cuanto mayor sea la carga más pequeño se vuelve el ion, el cual después limita el número de grupos con el cual se puede coordinar. Es importante reconocer que cada geometría tiene un número específico de coordinación, pero cada complejo con determinado número de coordinación tendrá distintas opciones geométricas a elegir.

Los factores que determinan el número de coordinación son:

Son aquellos compuestos cuyo centro metálico está unido a unos, dos, o tres ligandos.

Son sólo compuestos organometálicos con ligandos muy impedidos. Se encuentran en fase gaseosa a altas temperaturas, pero son raros bajo circunstancias ordinarias. Dos elementos que hacen compuestos organometálicos con número de coordinación 1 son Cu(I) y Ag(I).

Son elementos de los grupos 11 y 12 con configuración d tales como el Cu(I), Ag (I), Au(I), Hg(I). Elementos con este número de coordinación son poco comunes, aunque a altas temperaturas se encuentran en fase gaseosa.Algunos ejemplos incluyen [CuCl], [Ag(NH)], [Au(CN)], (RP)AuCl (donde R es un grupo alquilo o arilo), en cada uno de los cuales el centro metálico está en un entorno lineal.

Los complejos de coordinación 3 no son muy comunes. Normalmente se observan estructuras trigonales-planas y los ejemplos con centros metálicos d incluyen:


Las estructuras más comunes con compuestos con este número de coordinación son tetraédricas y cuadradas –planas, siendo el tetraedro la estructura observada con más frecuencia.

El tetraedro a veces está “aplanado” y las distorsiones se atribuyen a efectos estéricos o de empaquetamiento cristalino, en algunos casos, a efectos electrónicos. Las especies tetraédricas sencillas incluyen

Estos complejos son más raros que los tetraedricos y con frecuencia están asociados a configuraciones d8 en las que los factores electrónicos favorecen considerablemente una disposición cuadrado-plana. Como ejemplo se pueden mencionar

Las estructuras limitantes para número de coordinación 5 son la bipirámide trigonal y la pirámide de base cuadrada. La diferencia energética entre ambas estructuras es muy baja. De hecho muchas moléculas con cinco ligantes ya sea que tengas una de estas dos estructuras o pueden cambiar de una a otra muy fácilmente. Entre los complejos sencillos con coordinación 5 y estructura bipiramidal-trigonal están: 
Algunos complejos con estructura de pirámide de base cuadrada son: 

Seis, es el número de coordinación más común. La estructura más común es la octaédrica, sin embargo son conocidas también los prismas trigonales. Compuestos con este número de coordinación surgen de metales de transición con configuraciones d y d.

Si un ion metálico es lo suficientemente grande para permitir seis ligandos alrededor y los electrones de la capa d son ignorados, resulta este tipo de geometría es la más común para metales de transición de la primera fila, incluyendo a los iones aqua. En algunos casos son observadas algunas distorsiones tetragonales para iones metálicos d y d, las cuales se pueden explicar en términos del efecto Jahn Teller.

Algunos ejemplos de este tipo de geometrías son:


La mayoría de los compuestos con esta estructura tienen tres ligantes bidentados.Este geometría ocurre cuando dos caras triangulares son eclipsadas, como por ejemplo:



No es un número muy común para complejos de la primera fila de metales de transición. La diferencia de energía entre las estructuras es pequeña por lo que pueden ocurrir distorsiones para estabilizarse. Las distorsiones pueden dificultar la determinación de la geometría de los compuestos. Los números de coordinación iguales y mayores a 7 se observan con más frecuencia en iones de los primeros metales de la segunda y terceras filas del bloque "d" y para los lantánidos y actínidos.

En formas cubiertas, el séptimo ligando es simplemente añadido a la cara de la estructura, con los ajustes adecuados en el resto de los ángulos de manera que todos quepan. Aunque no es número de coordinación muy común, se han encontrado tres formas geométricas, con diferencias aparentemente resultantes de los distintos contraiones y los requerimientos estéricos de los ligandos. Las tres posibles formas gemétricas son: bipirámide pentagonal, prisma trigonal cubierto, octaedro cubierto.

Al ir aumentando el número de vértices de un poliedro lo hace también el número de estructuras posibles. Posiblemente el poliedro de ocho vértices más conocido es el cubo, pero apenas se observa como disposición de los átomos dadores en un complejo. Los pocos ejemplos que existen incluyen los aniones de los complejos actínidos Na[PaF], Na[UF]. El impedimento estérico entre ligandos puede reducirse convirtiendo una disposición cúbica en otra antiprismática cuadrada, es decir, pasar de cuadrados eclipsados a cuadrados girados. Se componen principalmente por metales pesados de los grupos 4 al 6 en estado de oxidación +4 o +5. Puede tener formas geométricas de antiprisma cuadrado, dodecaedro y bipirámide hexagonal.

Se conocen números de coordinación hasta 16, sin embargo aquellos mayores a 8 son muy raros de encontrar. Los datos de los que se dispone actualmente indican que una coordinación superior está limitada a iones metálicos del bloque "f".

La mayoría de los compuestos con este número de coordinación tienen una geometría trigonal triapuntado, "e.g". [ReH], [TcH] Este número de coordinación está asociado con más frecuencia al itrio, lantano, y elementos del bloque "f".

Su geometría más estable es: antiprisma cuadrada de bicapa.10 Este número de coordinación exige, tanto un átomo central de gran tamaño, como un ligando muy compacto, por lo que sólo se presenta en los complejos de los cationes de los lantánidos y actínidos en combinación con átomos dadores unidentados de pequeño tamaño. Un ejemplo de este tipo es el [Th(CO)].

Tomando por ejemplo en un cristal el átomo central de una celda cúbica centrada en el cuerpo (BCC), éste claramente está en contacto con 4 átomos vecinos en la cara superior y 4 átomos abajo, por lo tanto:

Recordando que un cristal HCP (Hexagonal Compacta) está formado por planos hexagonales compactos en orden ABC entonces se puede apreciar que tomando un átomo cualquiera del cristal, éste tiene 6 vecinos en el mismo plano, 3 vecinos arriba y 3 abajo.
Y por la misma razón anterior:


</doc>
<doc id="11584" url="https://es.wikipedia.org/wiki?curid=11584" title="Color">
Color

El color es la impresión producida por un tono de luz en los órganos visuales, o más exactamente, es una percepción visual que se genera en el cerebro de los humanos y otros animales al interpretar las señales nerviosas que le envían los fotorreceptores en la retina del ojo, que a su vez interpretan y distinguen las distintas longitudes de onda que captan de la parte visible del espectro electromagnético.

Todo cuerpo iluminado absorbe una parte de las ondas electromagnéticas y refleja las restantes. Las ondas reflejadas son captadas por el ojo e interpretadas en el cerebro como distintos colores según las longitudes de ondas correspondientes.

El ojo humano solo percibe las longitudes de onda cuando la iluminación es abundante. Con poca luz se ve en blanco y negro. En la superposición de colores luz (denominada "síntesis aditiva de color"), el color blanco resulta de la superposición de todos los colores, mientras que el negro es la ausencia de luz. En la mezcla de pigmentos (denominada "síntesis sustractiva de color"), trátese de pinturas, tintes, tintas o colorantes naturales para crear colores, el blanco solo se da si el pigmento o el soporte son de ese color, reflejando toda la luz blanca, mientras que el negro es resultado de la superposición completa de los colores cian, magenta y amarillo, una mezcla que en cierta medida logra absorber todas las longitudes de onda de la luz.

La luz blanca puede ser descompuesta en todos los colores del espectro visible por medio de un prisma (dispersión refractiva). En la naturaleza esta descomposición da lugar al arcoíris.

En el arte de la pintura, el diseño gráfico, el diseño visual, la fotografía, la imprenta y en la televisión, la teoría del color es un grupo de reglas básicas en la mezcla de colores para conseguir el efecto deseado combinando colores de luz o pigmento.
El color negro se puede producir combinando los colores pigmento: cian, magenta, amarillo; y mientras que combinando los colores luz: rojo, verde y azul se produce el color blanco.

En resumen la combinación de los colores pigmento (cian, magenta, amarillo) sustraen luz, como su nombre lo indica, y se obtiene el color negro. Y la combinación de los colores luz (verde, rojo, azul) suman luz, y se obtiene el color blanco.

La visión es el sentido de la percepción que consiste en la habilidad de detectar la luz y de interpretarla. Es propia de los animales teniendo estos un sistema dedicado a ella llamado sistema visual. La primera parte del sistema visual se encarga de formar la imagen óptica del estímulo visual en la retina (sistema óptico), donde sus células son las responsables de procesar la información. Las primeras en intervenir son los fotorreceptores, los cuales capturan la luz que incide sobre ellos. Los hay de dos tipos: los conos y los bastones. Otras células de la retina se encargan de transformar dicha luz en impulsos electroquímicos y en transportarlos hasta el nervio óptico. Desde allí, se proyectan al cerebro. En el cerebro se realiza el proceso de formar los colores y reconstruir las distancias, movimientos, formas de los objetos observados y distinción de los colores.

La percepción del color en el ojo humano se produce en las células sensibles de la retina que reaccionan de forma distinta a la luz según su longitud de onda. Los bastones perciben las tonalidades de oscuridad, y solo permiten distinguir las distintas tonalidades de grises entre el negro y el blanco. Los conos son medidores de cuantos de luz, radiaciones electromagnéticas, que se transforma en información de impulsos eléctricos que más tarde darán lugar a impresiones ópticas. Hay tres clases de conos, cada uno de ellos posee un fotopigmento opsina que solo detecta unas longitudes de onda concretas, que transformadas en el cerebro se corresponden aproximadamente a los colores azul, rojo y verde, es decir, los tres colores primarios con cuya combinación podemos percibir toda la gama de colores. En el sistema de la tricromática los tres grupos de conos combinados permiten cubrir el espectro completo de luz visible y son los siguientes:

Esta actividad retiniana ya es cerebral, puesto que los fotorreceptores, aunque simples, son células neuronales. La información de los conos y bastones es procesada por otras células situadas inmediatamente a continuación y conectadas detrás de ellos (horizontales, bipolares, amacrinas y ganglionares). El procesamiento en estas células es el origen de dos dimensiones o canales de pares antagónicos cromáticos: rojo-verde, azul-amarillo y de una dimensión acromática o canal de claroscuro. Dicho de otra manera, estas células se excitan o inhiben ante la mayor intensidad de la señal del rojo frente a la del verde, y del azul frente a la combinación de rojo y verde (amarillo), generando además un trayecto acromático de información relativa a la luminosidad.

La información de este procesamiento se traslada, a través del nervio óptico, a los núcleos geniculados laterales (situados a izquierda y derecha del tálamo), donde la actividad neuronal es específica respecto a la sugerencia del color y del claroscuro. Esta información precisa se transfiere al córtex visual por las vías denominadas radiaciones ópticas. La percepción del color es consecuencia de la actividad de las neuronas complejas del área de la corteza visual V4/V8, específica para el color. Esta actividad determina que las cualidades vivenciales de la visión del color puedan ser referidas mediante los atributos: luminosidad, tono y saturación.
Se denomina visión fotópica a la que tiene lugar con buenas condiciones de iluminación. Esta visión posibilita la correcta interpretación del color por el cerebro.

Muchos primates de origen africano (catarrinos), como el ser humano, comparten las características genéticas descritas: por eso se dice que tenemos percepción tricromática. Sin embargo, los primates de origen sudamericano únicamente tienen dos genes para la percepción del color. Existen pruebas que confirman que la aparición de este tercer gen fue debida a una mutación que duplicó uno de los dos originales. Posiblemente esta mutación esté relacionada con la capacidad para distinguir los frutos maduros de los que no lo están, debido a la evolución natural.

En el reino animal los mamíferos no suelen diferenciar bien los colores, las aves en cambio, sí; aunque suelen tener preferencia por los colores rojizos. Los insectos, por el contrario, suelen tener una mejor percepción de los azules e incluso ultravioletas. Por regla general los animales nocturnos ven en blanco y negro. Algunas enfermedades como el daltonismo o la acromatopsia impiden ver bien los colores. "Véase también:" Percepción del color.

Dentro del espectro electromagnético se constituyen todos los posibles niveles de energía de la luz. Hablar de energía es equivalente a hablar de longitud de onda; por ello, el espectro electromagnético abarca todas las longitudes de onda que la luz puede tener. De todo el espectro, la porción que el ser humano es capaz de percibir es muy pequeña en comparación con todas las existentes. Esta región, denominada espectro visible, comprende longitudes de onda desde los 380 nm hasta los 780 nm (1 nm = 1 nanómetro = 0,000001 mm). La luz de cada una de estas longitudes de onda es percibida en el cerebro humano como un color diferente. Por eso, en la descomposición de la luz blanca en todas sus longitudes de onda, mediante un prisma o por la lluvia en el arcoíris, el cerebro percibe todos los colores.
Por tanto, del Espectro visible, que es la parte del espectro electromagnético de la luz solar que podemos notar, cada longitud de onda es percibida en el cerebro como un color diferente.

Newton usó por primera vez la palabra "espectro" (del latín, "apariencia" o "aparición") en 1671 al describir sus experimentos en óptica. Newton observó que cuando un estrecho haz de luz solar incide sobre un prisma de vidrio triangular con un ángulo, una parte se refleja y otra pasa a través del vidrio y se desintegra en diferentes bandas de colores. También Newton hizo converger esos mismos rayos de color en una segunda lente para formar nuevamente luz blanca. Demostró que la luz solar tiene todos los colores del arcoíris.

Cuando llueve y hay sol, cada gota de lluvia se comporta de igual manera que el prisma de Newton y de la unión de millones de gotas de agua se forma el fenómeno del arcoíris.

A pesar de que el espectro es continuo y por lo tanto no hay cantidades vacías entre uno y otro color, se puede establecer la siguiente aproximación:

Cuando la luz incide sobre un objeto, su superficie absorbe ciertas longitudes de onda y refleja otras. Solo las longitudes de onda reflejadas podrán ser vistas por el ojo y por tanto en el cerebro solo se percibirán esos colores. Es un proceso diferente a luz natural que tiene todas las longitudes de onda, allí todo el proceso nada más tiene que ver con luz, ahora en los colores que percibimos en un objeto hay que tener en cuenta también el objeto en si, que tiene capacidad de absorber ciertas longitudes de onda y reflejar las demás.

Consideremos una manzana "roja". Cuando es vista bajo una luz blanca, parece roja. Pero esto no significa que emita luz roja, que sería el caso una síntesis aditiva. Si lo hiciese, seríamos capaces de verla en la oscuridad. En lugar de eso, absorbe algunas de las longitudes de onda que componen la luz blanca, reflejando solo aquellas que el humano ve como rojas. Los humanos ven la manzana roja debido al funcionamiento particular de su ojo y a la interpretación que hace el cerebro de la información que le llega del ojo.

Los colores armónicos son aquellos que funcionan bien juntos, es decir, que producen un esquema de color sensible al mismo sentido (la armonía nace de la percepción de los sentidos y, a la vez, esta armonía retroalimenta al sentido, haciéndolo lograr el máximo equilibrio que es hacer sentir al sentido). El círculo cromático es una herramienta útil para determinar armonías de color. Los colores complementarios son aquellos que se contraponen en dicho círculo y que producen un fuerte contraste. Así, por ejemplo, en el modelo RGB el verde es complementario del rojo, mientras que en el modelo CMY el verde es el complementario del magenta.

Un pigmento o un tinte es un material que cambia el color de la luz que refleja debido a que selectivamente absorben ciertas ondas luminosas. La luz blanca es aproximadamente igual a una mezcla de todo el espectro visible de luz. Cuando esta luz se encuentra con un pigmento, algunas ondas son absorbidas por los enlaces químicos y sustituyentes del pigmento, mientras otras son reflejadas. Este nuevo espectro de luz reflejado crea la apariencia del color. Por ejemplo, un pigmento azul ultramar refleja la luz azul, y absorbe los demás colores.

La apariencia de los pigmentos o tintes está íntimamente ligada a la luz que reciben. La luz solar tiene una temperatura de color alta y un espectro relativamente uniforme, y es considerada un estándar para la luz blanca. La luz artificial, por su parte, tiende a tener grandes variaciones en algunas partes de su espectro. Vistos bajo estas condiciones, los pigmentos o tintes lucen de diferentes colores.

Los tintes sirven para colorear materiales, como los tejidos, mientras que los pigmentos sirven para cubrir una superficie, como puede ser un cuadro. Desde las glaciaciones los humanos empleaban plantas y partes de animales para lograr tintes naturales con los que coloreaban sus tejidos. Luego los pintores han preparado sus propios pigmentos. Desde 1856 aparecieron los tintes sintéticos.

Durante varios siglos, los artistas han intentado entender las variaciones de los colores y han experimentado con mezclas para así obtener o sintetizar la mayor gama posible para sus obras; por lo que se concluyó que existe un número pequeño de colores -a los que se llamó colores primarios o "primitivos"- con cuya mezcla se pensó que se podría obtener todos los demás colores existentes y se propuso varias teorías. Sin embargo, a pesar de que la existencia de los colores primarios está comprobada, se debió esperar a que la ciencia defina en qué consiste la física de la luz y la parte biológica de su percepción, para así definir exactamente cuales son los verdaderos colores primarios. 

Los colores primarios dependen de la fuente del color, ya que puede ser una fuente luminosa que emite una luz con un color determinado o puede tratarse de un objeto que absorbe una parte y refleja otra de la luz que recibe y que es lo que vemos e interpretamos. Tomando en cuenta estas dos fuentes de color, se puede resumir los modelos más difundidos para la síntesis del color del siguiente modo:

De estos tipos de síntesis, la columna de la derecha donde se representa a la coloración tradicional, es parte del conocimiento empírico y no científico, ya que en realidad sus colores primarios no pueden considerarse como los verdaderos porque, a pesar de la creencia popular, con la mezcla de los mismos no es posible sintetizar toda la gama de colores. Los colores secundarios así obtenidos son limitados, en especial el morado y el verde, los cuales se presentan opacos y con tendencia hacia tonos grisáceos. Es por esto que en la actualidad los profesionales, tanto los artistas plásticos como los pintores decorativos, tienden a reemplazar a colores primarios como el azul y el rojo, por el cian o azul cian y por el magenta o rojo magenta, obteniendo mejores resultados.

Se llama síntesis aditiva a obtener un color de luz determinado por la suma de otros colores. Thomas Young partiendo del descubrimiento de Newton que la suma de los colores del espectro visible formaba luz blanca realizó un experimento con linternas con los seis colores del espectro visible, proyectando estos focos y superponiéndolos llegó a un nuevo descubrimiento: para formar los seis colores del espectro solo hacían falta tres colores y además sumando los tres se formaba luz blanca.
El proceso de reproducción aditiva normalmente utiliza luz roja, verde y azul para producir el resto de los colores. Combinando uno de estos colores primarios con otro en proporciones iguales produce los colores aditivos secundarios, más claros que los anteriores: cian, magenta y amarillo. Variando la intensidad de cada luz de color finalmente deja ver el espectro completo de estas tres luces. La ausencia de los tres da el negro, y la suma de los tres da el blanco. Estos tres colores se corresponden con los tres picos de sensibilidad de los tres sensores de color en nuestros ojos.
Los colores primarios no son una propiedad fundamental de la luz, sino un concepto biológico, basado en la respuesta fisiológica del ojo humano a la luz. Un ojo humano normal solo contiene tres tipos de receptores, llamados conos. Estos responden a zonas del espectro que corresponden con longitudes de onda específicas de luz roja, verde y azul. Las personas y los miembros de otras especies que tienen estos tres tipos de receptores se llaman tricrómatas. Aunque la sensibilidad máxima de los conos no se produce exactamente en las frecuencias roja, verde y azul, son los colores que se eligen para definirlos como primarios, porque con ellos es posible estimular los tres receptores de color de manera casi independiente, proporcionando una amplia gama de color. Para generar rangos de color óptimos para otras especies aparte de los seres humanos se tendrían que usar otros colores primarios aditivos. Por ejemplo, para las especies conocidas como tetracrómatas, con cuatro receptores de color distintos, se utilizarían cuatro colores primarios (como los humanos solo pueden ver hasta 400 nanómetros (violeta), pero los tetracrómatas pueden ver parte del ultravioleta, hasta los 300 nanómetros aproximadamente, este cuarto color primario estaría situado en este rango y probablemente sería un violeta espectral puro, en lugar del violeta que vemos). Muchas aves y marsupiales son tetracrómatas, y se ha sugerido que algunas mujeres nacen también tetracrómatas, con un receptor extra para el amarillo. Por otro lado, la mayoría de los mamíferos tienen solo dos tipos de receptor de color y por lo tanto son dicrómatas; para ellos, solo hay dos colores primarios.
Las televisiones, los monitores de ordenador y las pantallas de los teléfonos celulares, son las aplicaciones prácticas más comunes de la síntesis aditiva.

Todo lo que no es color aditivo es color sustractivo. En otras palabras, todo lo que no es luz directa es luz reflejada en un objeto, la primera se basa en la síntesis aditiva de color, la segunda en la síntesis sustractiva de color.

La síntesis sustractiva explica la teoría de la mezcla de pigmentos y tintes para crear color. El color que parece que tiene un determinado objeto depende de qué partes del espectro electromagnético son reflejadas por él, o dicho a la inversa, qué partes del espectro son absorbidas.

Se llama síntesis sustractiva porque a la energía de radiación se le sustrae algo por absorción. En la síntesis sustractiva el color de partida siempre suele ser el color acromático blanco, el que aporta la luz (en el caso de una fotografía el papel blanco, si hablamos de un cuadro es el lienzo blanco), es un elemento imprescindible para que las capas de color puedan poner en juego sus capacidades de absorción. En la síntesis sustractiva los colores primarios son el amarillo, el magenta y el cian, cada uno de estos colores tiene la misión de absorber el campo de radiación de cada tipo de conos. Actúan como filtros, el amarillo, no deja pasar las ondas que forman el azul, el magenta no deja pasar el verde y el cian no permite pasar al rojo.

En los sistemas de reproducción de color según la síntesis sustractiva, la cantidad de color de cada filtro puede variar del 0% al 100%. Cuanto mayor es la cantidad de color mayor es la absorción y menos la parte reflejada, si de un color no existe nada, de ese campo de radiaciones pasará todo. Por ello, a cada capa de color le corresponde modular un color sensación del órgano de la vista: al amarillo le corresponde modular el azul, al magenta el verde y al cian el rojo.
Así mezclando sobre un papel blanco cian al 100% y magenta al 100%, no dejaran pasar el color rojo y el verde con lo que el resultado es el color azul. De igual manera el magenta y el amarillo formaran el rojo, mientras el cian y el amarillo forman el verde. El azul, verde y rojo son colores secundarios en la síntesis sustractiva y son más oscuros que los primarios. En las mezclas sustractivas se parte de tres primarios claros y según se mezcla los nuevos colores se van oscureciendo, al mezclar estamos restando luz. Los tres primarios mezclados dan el negro.

La aplicación práctica de la síntesis sustractiva es la impresión en color, fotografía a color y la pintura.

En la impresión en color, las tintas que se usan principalmente como primarios son el cian, magenta y amarillo. Como se ha dicho, el Cian es el opuesto al rojo, lo que significa que actúa como un filtro que absorbe dicho color. La cantidad de cian aplicada a un papel controlará cuanto rojo mostrará. Magenta es el opuesto al verde y amarillo el opuesto al azul. Con este conocimiento se puede afirmar que hay infinitas combinaciones posibles de colores. Así es como las reproducciones de ilustraciones son producidas en grandes cantidades, aunque por varias razones también suele usarse una tinta negra. Esta mezcla de cian, magenta, amarillo y negro se llama "modelo de color CMYK". CMYK es un ejemplo de espacio de colores sustractivos, o una gama entera de espacios de color.

El origen de los nombres magenta y cian procede de las películas de color inventadas en 1936 por Agfa y Kodak. El color se reproducía mediante un sistema de tres películas, una sensible al amarillo, otro sensible a un rojo púrpura y una tercera a un azul claro. Estas casas comerciales decidieron dar el nombre de magenta al rojo púrpura y cian al azul claro. Estos nombres fueron admitidos como definitivos en la década de 1950 en las normas DIN que definieron los colores básicos de impresión.

Aunque los dos extremos del espectro visible, el rojo y el violeta, son diferentes en longitud de onda, visualmente tienen algunas similitudes, Newton propuso que la banda recta de colores espectrales se distribuyese en una forma circular uniendo los extremos del espectro visible. Este fue el primer círculo cromático, un intento de fijar las similitudes y diferencias entre los distintos matices de color. Muchos estudiosos admitieron el círculo de Newton para explicar las relaciones entre los diferentes colores. Los colores que están juntos corresponden a longitud de onda similar.

Desde un punto de vista teórico un círculo cromático de doce colores estaría formado por los tres primarios, entre ellos se situarían los tres secundarios y entre cada secundario y primario el terciario que se origina de su unión. Así en actividades de síntesis aditiva, se pueden distribuir los tres primarios, rojo, verde y azul uniformemente separados en el círculo; en medio entre cada dos primarios, el secundario que forman ellos dos; entre cada primario y secundario se pondría el terciario que se origina en su mezcla. Así tenemos un círculo cromático de síntesis aditiva de doce colores. Se puede hacer lo mismo con los tres primarios de síntesis sustractiva y llegaríamos a un círculo cromático equivalente.

El círculo cromático suele representarse como una rueda dividida en doce partes. Los colores primarios se colocan de modo que uno de ellos esté en la porción superior central y los otros dos en la cuarta porción a partir de esta, de modo que si unimos los tres con unas líneas imaginarias formarían un triángulo equilátero con la base horizontal. Entre dos colores primarios se colocan tres tonos secundarios de modo que en la porción central entre ellos correspondería a una mezcla de cantidades iguales de ambos primarios y el color más cercano a cada primario sería la mezcla del secundario central más el primario adyacente.

Los círculos cromáticos actuales utilizados por los artistas se basan en el modelo CMY, si bien los colores primarios utilizados en pintura difieren de las tintas de proceso en imprenta en su intensidad. Los pigmentos utilizados en pintura, tanto en óleo como acrílico y otras técnicas pictóricas suelen ser el azul de ftalocianina (PB15 en notación "Color Index") como cian, el magenta de quinacridona (PV19 en notación "Color Index") y algún amarillo arilida o bien de cadmio que presente un tono amarillo neutro (existen varios pigmentos válidos o mezclas de ellos utilizables como primarios amarillos). Varias casas poseen juegos de colores primarios recomendados que suelen venderse juntos y reciben nombres especiales en los catálogos, tales como «azul primario» o «rojo primario» junto al «amarillo primario», pese a que ni el azul ni el rojo propiamente dichos son en realidad colores primarios según el modelo CMYK utilizado en la actualidad.

No obstante, como los propios nombres dados por los fabricantes a sus colores primarios evidencian, existe una tradición todavía anclada en el modelo RYB y que ocasionalmente se encuentra todavía en libros y en cursos orientados a aficionados a la pintura. Pero la enseñanza reglada, tanto en escuelas de arte como en la universidad, y los textos de referencia importantes ya han abandonado tal modelo hace décadas. La prueba la tenemos en los colores orientados a la enseñanza artística de diferentes fabricantes, que sin excepción utilizan un modelo de color basado en CMYK, que además de los tres colores primarios CMY incluyen negro y blanco como juego básico para el estudiante.

El blanco y el negro no suelen considerarse colores y no aparecen en un círculo cromático, pues el blanco es la presencia de todos los colores y el negro la ausencia total de color. Sin embargo, también se les llama colores neutros: el negro y el blanco al combinarse forman el gris, el cual también se marca en escalas; esto forma un círculo propio llamado "círculo cromático en escala de grises" o "círculo de grises".

En el círculo cromático se llaman colores complementarios o colores opuestos a los pares de colores ubicados diametralmente opuestos en la circunferencia, unidos por su diámetro. Al situar juntos y no mezclados colores complementarios el contraste que se logra es máximo.

La denominación "complementario" depende en gran medida del modelo de círculo cromático empleado. Así en el círculo cromático natural (sistemas RGB, CMY), el complementario del color verde es el color magenta, el del azul es el amarillo y del rojo el cian. En el círculo cromático tradicional (RYB), el amarillo es el complementario del violeta y el naranja el complementario del azul. 

Hoy, los científicos saben que el conjunto correcto es el de los modelos RGB y CMY. En la teoría del color actual, dos colores se denominan "complementarios" si, al ser mezclados en una proporción dada el resultado de la mezcla es un color neutral (gris, blanco, o negro).


El grado en que uno o dos de los tres colores primarios RGB (esta clasificación es referente a los colores básicos en la composición luminosa de una pantalla informática R=Red, G=Green, B=Blue, con los que se componen por medio de adición lumínica, distinta a la clasificación de los colores básicos o primarios de la pintura, en la que se mezclan por adición de pigmentos matéricos o físicos) predominan en un color. A medida que las cantidades de RGB se igualan, el color va perdiendo saturación hasta convertirse en gris o blanco.

Estas 3 propiedades combinadas entre sí, son capaces de sintetizar toda la gama de colores existente, por un camino diferente del de la combinación de los colores primarios aditivos (RGB). Esto constituye la base de la síntesis del color de los modelos HSL y HSV.

La "saturación" bien entendida tiene que ver con la cantidad de materia que se aplica sobre una superficie, por ende saturar significa colmar una superficie con pigmento. El agregado de gris a los colores como forma de saturar, no hace otra cosa que obtener un nuevo color producto de la mezcla. Puede probarse por experimentación. Por ende un color, inclusive al que se le agregara gris, puede saturar una superficie con mayor o menor efectividad dependiendo de la técnica utilizada y de la calidad de los materiales con los que se ha fabricado. Por ejemplo, la técnica de acuarela tiene menor capacidad para saturar que la del acrílico.

En su libro "Teoría de los colores", el poeta y científico alemán Johann Wolfgang von Goethe propuso un círculo de color simétrico, el cual comprende el establecido por el matemático y físico inglés Isaac Newton y los espectros complementarios. En contraste, el círculo de color de Newton, con siete ángulos de color desiguales y subtendidos, no exponía la simetría y la complementariedad que Goethe consideró como característica esencial del color. Para Newton, solo los colores espectrales podían considerarse como fundamentales. El enfoque más empírico de Goethe le permitió admitir el papel esencial del color magenta, que no es espectral, en un círculo de color. Posteriormente, los estudios de la percepción del color definieron el estándarCIE 1931, el cual es un modelo perceptual que permite representar colores primarios con precisión y convertirlos a cada modelo de color de forma apropiada.

La teoría del color propuesta por el químico y filósofo alemán Wilhelm Ostwald consta de cuatro sensaciones cromáticas elementales (amarillo, rojo, azul y verde) y dos sensaciones acromáticas intermedias.
Un espacio de color define un modelo de composición del color. Por lo general un espacio de color lo define una base de N vectores (por ejemplo, el espacio RGB lo forman 3 vectores: rojo, verde y azul), cuya combinación lineal genera todo el espacio de color. Los espacios de color más generales intentan englobar la mayor cantidad posible de los colores visibles por el ojo humano, aunque existen espacios de color que intentan aislar tan solo un subconjunto de ellos.

'Existen espacios o modelos de color de:'

De los cuales, los espacios de color de tres dimensiones son los más extendidos y los más utilizados. Entonces, un color se especifica usando tres coordenadas, o atributos, que representan su posición dentro de un espacio de color específico. Estas coordenadas no nos dicen cuál es el color, sino que muestran dónde se encuentra un color dentro de un espacio de color en particular.

Para representar y cuantificar cada color se usan diferentes modelos:

En la síntesis aditiva usada en pantallas y monitores, el modelo de color RGB (del inglés Red-rojo, Green-verde, Blue-azul), cada color se representa mediante la mezcla de los tres colores luz primarios, en términos de intensidad de cada color primario con que se forma. Para indicar con qué proporción mezclamos cada color, se asigna un valor a cada uno de los colores primarios, de manera que el valor 0 significa que no interviene en la mezcla y la intensidad de cada una de las componentes se mide según una escala que va del 0 al 255 (cada píxel 16x16=256). Por lo tanto, el rojo se obtiene con (255,0,0), el verde con (0,255,0) y el azul con (0,0,255). La ausencia de color —lo que conocemos como color negro— se obtiene cuando los tres componentes son 0, (0,0,0). La combinación de dos colores a nivel máximo, 255, con un tercero en nivel 0 da lugar a los tres colores secundarios. De esta forma el amarillo es (255,255,0), el cyan (0,255,255) y el magenta (255,0,255). El color blanco se forma con los tres colores primarios a su máximo nivel (255,255,255).

Se debe tener en cuenta que sólo con unos colores «primarios» ficticios se pueden llegar a conseguir todos los colores posibles. Estos colores primarios son conceptos idealizados utilizados en modelos de color matemáticos que no representan las sensaciones de color reales o incluso los impulsos nerviosos reales o procesos cerebrales. En otras palabras, todos los colores «primarios» perfectos son completamente imaginarios, lo que implica que todos los colores primarios que se utilizan en las mezclas son incompletos o imperfectos.

Existe también el espacio derivado RGBA, que añade el canal alfa (de transparencia) al espacio RGB original.

En el modelo de color RYB, el rojo, el amarillo y el azul se consideran colores primarios, y en teoría, el resto de colores puros (color materia) puede ser creados mezclando pintura roja, amarilla y azul. A pesar de su obsolescencia e imprecisión, mucha gente aprende algo sobre este modelo en los estudios de educación primaria, mezclando pintura o lápices de colores con estos colores primarios.

Este modelo tradicional es aún utilizado en general en conceptos de arte y pintura tradicionales, pero ha sido totalmente dejado de lado en la mezcla industrial de pigmentos de pintura. Aún siendo usado como guía para la mezcla de pigmentos, el modelo tradicional no representa con precisión los colores que resultan de mezclar los tres colores tradicional primarios, puesto que el azul y el rojo son tonalidades verdaderamente secundarias. A pesar de la imprecisión de este modelo –su corrección es el modelo CMYK–, se sigue utilizando en las artes visuales, el diseño gráfico y otras disciplinas afines, por tradición del modelo original de Goethe de 1810 y otros autores anteriores.

El sistema de representación de colores HTML, también de síntesis aditiva, usado en las páginas web, se descompone también de la misma forma en los tres colores primarios aditivos: Rojo-Verde-Azul. La intensidad de cada una de las componentes se mide también en una escala que va del 0 al 255. Sin embargo utiliza la numeración hexadecimal, lo que le permite representar el número 255 en base decimal con solo dos dígitos en base hexadecimal. En el sistema de codificación hexadecimal, además de los números del 0 al 9 se utilizan seis letras con un valor numérico equivalente; a=10, b=11, c=12, d=13, e=14 y f=15. La correspondencia entre la numeración hexadecimal y la decimal u ordinaria viene dada por la siguiente fórmula:

La intensidad máxima es ff, que se corresponde con (15*16)+15= 255 en decimal, y la nula es 00, también 0 en decimal. De esta manera, cualquier color queda definido por tres pares de dígitos.

CMY trabaja mediante la absorción de la luz (colores secundarios). 

En la mezcla sustractiva en la impresión de colores se utiliza el modelo de color CMYK (acrónimo de Cyan, Magenta, Yellow-amarillo y Key-negro). La mezcla de colores CMY es sustractiva y al imprimir conjuntamente cyan, magenta y amarillo sobre fondo blanco resulta el color negro. Por varias razones, el negro generado al mezclar los colores primarios sustractivos no es adecuado y se emplea también la tinta negra como color inicial además de los tres colores primarios sustractivos amarillo, magenta y cyan. El modelo CMYK se basa en la absorción de la luz por un objeto: el color que presenta un objeto corresponde a la parte de la luz que incide sobre este y se refleja no siendo absorbida por el objeto, en este caso el papel blanco.

Los colores que se ven son la parte de luz que no es absorbida. En CMY, magenta más amarillo producen rojo, magenta más cian producen azul, cian más amarillo generan verde y la combinación de cian, magenta y amarillo forman negro.

El negro generado por la mezcla de colores primarios sustractivos no es tan denso como el color negro puro (uno que absorbe todo el espectro visible). Es por esto que al CMY original se ha añadido un canal clave ("key"), que normalmente es el canal negro ("black"), para formar el espacio CMYK o CMYB. Actualmente las impresoras de cuatro colores utilizan un cartucho negro además de los colores primarios de este espacio, lo cual genera un mejor contraste. Sin embargo el color que una persona ve en una pantalla de computador difiere del mismo color en una impresora, debido a que los modelos RGB y CMY son distintos. El color en RGB está hecho por la reflexión o emisión de luz, mientras que el CMY, mediante la absorción de ésta.

Fue una recodificación de color realizada para la norma de televisión cromática estadounidense NTSC, que debía ser compatible con la televisión en blanco y negro. Los nombres de los componentes de este modelo son Y por luminancia ("luminance"), I fase ("in-phase") y Q cuadratura ("quadrature"). La primera es la señal monocromática de la televisión en blanco y negro y las dos últimas generan el tinte y saturación del color. Los parámetros I y Q son nombrados en relación con el método de modulación utilizado para codificar la señal portadora. Los valores de las señales RGB son sumados para producir una única señal Y’ que representa la iluminación o brillo general de un punto en particular. La señal I es creada al restar el Y' de la señal azul de los valores RGB originales y luego el Q se realiza restando la señal Y' del rojo.

Son modelos de síntesis aditiva basados en las propiedades del color. Sus códigos son coordenadas cilíndricas que se desarrollaron en los años 1970 para la computación gráfica y se usa hoy para la edición digital de imágenes. Los parámetros son H=matiz o tono (del inglés "hue"), S=saturación ("saturation"), V=valor ("value") y L=luminosidad ("lightness"). Se pueden representar geométricamente mediante conos, cilindros o cubos, y su numeración es la siguiente:

Es un espacio cilíndrico, pero normalmente asociado a un cono o cono hexagonal, debido a que es un subconjunto visible del espacio original con valores válidos de RGB.




El uso de ciertos colores impacta gradualmente en el estado de ánimo de las personas, muchos de ellos son utilizados con esa intención en lugares específicos, por ejemplo en los restaurantes es muy común que se utilice decoración de color naranja ya que abre el apetito, en los hospitales se usa colores neutros para dar tranquilidad a los pacientes, y para las entrevistas de trabajo es recomendable llevar ropa de colores oscuros, ya que da la impresión de ser una persona responsable y dedicada; así como los colores en la ropa nos pueden favorecer y hacer lucir el rostro más radiantes o más opacos; estos son algunos ejemplos de la relación entre los colores y las emociones.

Los ocho colores elementales corresponden a las ocho posibilidades extremas de percepción del órgano de la vista. Las posibilidades últimas de sensibilidad de color que es capaz de captar el ojo humano. Estos resultan de las combinaciones que pueden realizar los tres tipos de conos del ojo, o lo que es lo mismo las posibilidades que ofrecen de combinarse los tres primarios. Estas ocho posibilidades son los tres colores primarios, los tres secundarios que resultan de la combinación de dos primarios, más los dos colores acromáticos, el blanco que es percibido como la combinación de los tres primarios (síntesis aditiva: colores luz) y el negro es la ausencia de los tres.

Por tanto, colores tradicionales como el violeta, el naranja o el marrón no son colores elementales.

Cada color determinado está originado por una mezcla o combinación de diversas longitudes de onda. El matiz es la cualidad que permite diferenciar un color de otro: permite clasificarlo en términos de rojizo, verdoso, azulado, etc. Se refiere a la ligera variación que existe entre un color y el color contiguo en el círculo cromático (o dicho de otra forma la ligera variación en el espectro visible). Así un verde azulado o a un verde amarillo son matices del verde cuando la longitud de onda dominante en la mezcla de longitudes de onda es la que corresponde al verde, y hablaremos de un matiz del azul cuando tenemos un azul verdoso o un azul magenta donde la longitud de onda dominante de la mezcla corresponda al azul.

Los siguientes son los principales colores del círculo cromático y sus derivados oscuros (hacia el negro), agrisados (semisaturados o hacia el gris) y claros (hacia el blanco):

Son aquellos que no poseen colorido, es decir, que su saturación es igual a 0. En conjunto conforman la escala de grises, la cual va desde el blanco hasta el negro. Poseen un equilibrio o igualdad entre los colores primarios que lo componen. Entre los principales tenemos: 

No existe una homología completa entre las distintas lenguas que cubra completamente la paleta del color. Algunas lenguas ni siquiera poseían el concepto. Platón consideraba que existían cuatro colores básicos: el blanco, el negro, el rojo y el brillante ¿?, algo que para nosotros no es ni siquiera un color, y definió también al color como: «una emanación de las figuras, proporcionado a la vista y, por tanto, perceptible» ; William Ewart Gladstone (1809-1898), quien no solo fue un gran político, sino un experto helenista, estudió el color en Homero (quien describía el mar como "vino oscuro" y el cielo "como bronce") y contó que los colores más citados en sus obras eran el negro (200 veces) y el blanco (100); el rojo solo 15 y el verde y el amarillo menos de diez. No se mencionaban nada ni el azul, ni el índigo, ni el añil. Y no había tampoco mención alguna del color azul en el resto de los autores griegos. 

El filósofo y lingüista alemán Lazarus Geiger encontró que tampoco en muchas otras civilizaciones antiguas se conocía el color azul: en el Corán, en antiguas historias chinas, en la versión antigua de la Biblia hebrea, en las sagas islandesas y hasta en los Vedas indios no aparece el color del cielo. La única cultura antigua que sabía representar el color azul fue la egipcia, e incluso así les costó mucho fabricar el pigmento de forma sintética, pues no se encuentra fácilmente en la naturaleza (por ejemplo, no hay flores azules: las primeras las fabricaron los humanos).

Guy Deutscher descubrió con diversos experimentos que, en general, el color del cielo es el último que suelen aprender los niños. Para los pueblos antiguos el cielo era blanco, y, el mar, negro. Por otra parte, las denominaciones de los colores antiguos fue evolucionando con la lengua y, por ejemplo, el color negro entre los hebreos ("kajol") pasó a significar azul con el tiempo, y lo mismo ocurrió con el color "kuanos" de los griegos, que en Homero significa negro y en la actualidad azul.

Esta noción de que los conceptos y el lenguaje limitan la percepción cognitiva se llama relativismo lingüístico, y describe las maneras en que diferentes culturas pueden tener dificultades para recordar o retener información sobre los objetos o conceptos para los que carecen de identificación de idioma. Los inuit poseen, por ejemplo, 50 formas de decir blanco. Y, aunque la tribu Himba de Namibia no posee palabra alguna para el azul, si saben distinguir matices del color verde que a nosotros nos cuesta notar. Existen también diferencias biológicas entre los sexos humanos en cuanto a la percepción del color: el naranja puede parecer más rojizo para los hombres que las mujeres, y el verde siempre aparece más brillante para las mujeres que para los hombres, que siempre ven un tono más amarillento, entre otras disimilitudes.




</doc>
<doc id="11585" url="https://es.wikipedia.org/wiki?curid=11585" title="Espectro electromagnético">
Espectro electromagnético

Se denomina espectro electromagnético a la distribución energética del conjunto de las ondas electromagnéticas. Referido a un objeto se denomina "espectro electromagnético" o simplemente "espectro" a la radiación electromagnética que emite (espectro de emisión) o absorbe (espectro de absorción) una sustancia. Dicha radiación sirve para identificar la sustancia de manera análoga a una huella dactilar. Los espectros se pueden observar mediante espectroscopios que, además de permitir ver el espectro, permiten realizar medidas sobre el mismo, como son la longitud de onda, la frecuencia y la intensidad de la radiación.

El espectro electromagnético se extiende desde la radiación de menor longitud de onda, como los rayos gamma y los rayos X, pasando por la radiación ultravioleta, la luz visible y la radiación infrarroja, hasta las ondas electromagnéticas de mayor longitud de onda, como son las ondas de radio. Si bien el límite para la longitud de onda más pequeña posible no sería la longitud de Planck (porque el tiempo característico de cada modalidad de interacción es unas 10 veces mayor al instante de Planck y, en la presente etapa cosmológica, ninguna de ellas podría oscilar con la frecuencia necesaria para alcanzar aquella longitud de onda), se cree que el límite máximo sería el tamaño del Universo (véase Cosmología física) aunque formalmente el espectro electromagnético es infinito y continuo.

El espectro electromagnético cubre longitudes de onda muy variadas. Existen frecuencias de 30 Hz y menores que son relevantes en el estudio de ciertas nebulosas. Por otro lado se conocen frecuencias cercanas a 2,9×10 Hz, que han sido detectadas provenientes de fuentes astrofísicas.

La energía electromagnética en una particular longitud de onda λ (en el vacío) tiene una frecuencia "f" asociada y una energía de fotón "E". Por tanto, el espectro electromagnético puede ser expresado igualmente en cualquiera de esos términos. Se relacionan en las siguientes ecuaciones:

formula_1, o lo que es lo mismo formula_2 

formula_3, o lo que es lo mismo formula_4

Donde formula_5 (velocidad de la luz) y formula_6 es la constante de Planck, formula_7.

Por lo tanto, las ondas electromagnéticas de alta frecuencia tienen una longitud de onda corta y mucha energía mientras que las ondas de baja frecuencia tienen grandes longitudes de onda y poca energía.

Por lo general, las radiaciones electromagnéticas se clasifican basándose en su longitud de la onda en ondas de radio, microondas, infrarrojos, visible –que percibimos como luz visible–, ultravioleta, rayos X y rayos gamma.

El comportamiento de las radiaciones electromagnéticas depende de su longitud de onda. Cuando la radiación electromagnética interactúa con átomos y moléculas puntuales, su comportamiento también depende de la cantidad de energía por quantum que lleve. Al igual que las ondas de sonido, la radiación electromagnética puede dividirse en octavas.

La espectroscopia puede detectar una región mucho más amplia del espectro electromagnético que el rango visible de 400 a 700 nm. Un espectrómetro de laboratorio común y corriente detecta longitudes de onda de 2 a 2500nm.

Para su estudio, el espectro electromagnético se divide en segmentos o bandas, aunque esta división es inexacta. Existen ondas que tienen una frecuencia, pero varios usos, por lo que algunas frecuencias pueden quedar en ocasiones incluidas en dos rangos.

En radiocomunicaciones, los rangos se abrevian con sus siglas en inglés. Los rangos son:

Frecuencias menores no han sido detectadas o hasta ahora (y tal vez nunca) no se pueden usar(estas son las TLF)











La definición del espectro de microondas depende de la fuente. Varios autores consideran que las microondas abarcan las frecuencias entre 300 MHz y 300 GHz, pero los estándares IEC 60050 e IEEE 100 sitúan el espectro entre 1 GHz y 300GHz. Estas frecuencias abarcan parte del rango de UHF y todo el rango de SHF y EHF. Estas ondas se utilizan en numerosos sistemas, como múltiples dispositivos de transmisión de datos, radares y hornos microondas.

Las ondas infrarrojas están en el rango de 0,7 a 1000 micrómetros. La radiación infrarroja se asocia generalmente con el calor. Ellas son producidas por cuerpos que generan calor, aunque a veces pueden ser generadas por algunos diodos emisores de luz y algunos láseres.

Las señales son usadas para algunos sistemas especiales de comunicaciones, como en astronomía para detectar estrellas y otros cuerpos en los que se usan detectores de calor para descubrir cuerpos móviles en la oscuridad. También se usan en los mandos a distancia de los televisores y otros aparatos, en los que un transmisor de estas ondas envía una señal codificada al receptor del televisor. En últimas fechas se ha estado implementando conexiones de área local LAN por medio de dispositivos que trabajan con infrarrojos, pero debido a los nuevos estándares de comunicación estas conexiones han perdido su versatilidad.

Por encima de la frecuencia de las radiaciones infrarrojas se encuentra lo que comúnmente es llamado luz, un tipo especial de radiación electromagnética que tiene una longitud de onda en el intervalo de 0,4 a 0,8 micrómetros. Este es el rango en el que el sol y las estrellas similares emiten la mayor parte de su radiación. Probablemente, no es una coincidencia que el ojo humano sea sensible a las longitudes de onda que emite el sol con más fuerza. Las unidades usuales para expresar las longitudes de onda son el Angstrom y el nanómetro. La luz que vemos con nuestros ojos es realmente una parte muy pequeña del espectro electromagnético. La radiación electromagnética con una longitud de onda entre 380 nm y 760 nm (790-400 terahercios) es detectada por el ojo humano y se percibe como luz visible. Otras longitudes de onda, especialmente en el infrarrojo cercano (más de 760 nm) y ultravioleta (menor de 380 nm) también se refiere a veces como la luz, aun cuando la visibilidad a los seres humanos no es relevante. Si la radiación que tiene una frecuencia en la región visible del espectro electromagnético se refleja en un objeto, por ejemplo, un tazón de fruta, y luego golpea los ojos, esto da lugar a la percepción visual de la escena. Nuestro sistema visual del cerebro procesa la multitud de frecuencias que se reflejan en diferentes tonos y matices, y a través de este fenómeno psicofísico, no del todo entendido, la mayoría de la gente percibe un tazón de fruta; Un arco iris muestra la óptica (visible) del espectro electromagnético. En la mayoría de las longitudes de onda, sin embargo, la radiación electromagnética no es visible directamente, aunque existe tecnología capaz de manipular y visualizar una amplia gama de longitudes de onda.

La luz puede usarse para diferentes tipos de comunicaciones. Las ondas electromagnéticas pueden modularse y transmitirse a través de fibras ópticas, lo cual resulta en una menor atenuación de la señal con respecto a la transmisión por el espacio libre.

La luz ultravioleta cubre el intervalo de 4 a 400 nm. El Sol es una importante fuente emisora de rayos en esta frecuencia, los cuales causan cáncer de piel a exposiciones prolongadas. Este tipo de onda no se usa en las telecomunicaciones, sus aplicaciones son principalmente en el campo de la medicina.

La denominación rayos X designa a una radiación electromagnética, invisible, capaz de atravesar cuerpos opacos y de impresionar las películas fotográficas. La longitud de onda está entre 10 a 0,01 nanómetros, correspondiendo a frecuencias en el rango de 30 a 30 000 PHz (de 50 a 5000 veces la frecuencia de la luz visible).

La radiación gamma es un tipo de radiación electromagnética producida generalmente por elementos radiactivos o procesos subatómicos como la aniquilación de un par positrón-electrón. Este tipo de radiación de tal magnitud también es producida en fenómenos astrofísicos de gran violencia.

Debido a las altas energías que poseen, los rayos gamma constituyen un tipo de radiación ionizante capaz de penetrar en la materia más profundamente que la radiación alfa o beta. Dada su alta energía pueden causar grave daño al núcleo de las células, por lo que son usados para esterilizar equipos médicos y alimentos.

Cuando se analiza el espectro electromagnético de la luz de una estrella o galaxia, se puede apreciar en este un corrimiento al rojo o un corrimiento al azul es decir los colores visibles se desplazan hacia un extremo u otro del espectro visible. Esto ocurre gracias al efecto Doppler, llamado así por el físico austríaco Christian Andreas Doppler, es el aparente cambio de frecuencia de una onda producido por el movimiento relativo de la fuente respecto a su observador. Doppler propuso este efecto en 1842 en su tratado "Über das farbige Licht der Doppelsterne und einige andere Gestirne des Himmels" ("Sobre el color de la luz en estrellas binarias y otros astros"). 

En el caso del espectro visible de la radiación electromagnética, si el objeto se aleja, su luz se desplaza a longitudes de onda más largas, desplazándose hacia el rojo. Si el objeto se acerca, su luz presenta una longitud de onda más corta, desplazándose hacia el azul. Esta desviación hacia el rojo o el azul es muy leve incluso para velocidades elevadas, como las velocidades relativas entre estrellas o entre galaxias, y el ojo humano no puede captarlo, solamente medirlo indirectamente utilizando instrumentos de precisión como espectrómetros. Si el objeto emisor se moviera a fracciones significativas de la velocidad de la luz, sí sería apreciable de forma directa la variación de longitud de onda. 

El primer corrimiento al rojo Doppler fue descrito en 1848 por el físico francés Hippolyte Fizeau, que indicó que el desplazamiento en líneas espectrales visto en las estrellas era debido al efecto Doppler. En 1868, el astrónomo británico William Huggins fue el primero en determinar la velocidad de una estrella alejándose de la Tierra mediante este método.

La abundancia de corrimiento al rojo en el universo ha permitido crear la teoría de la expansión del universo. El corrimiento al azul del espectro, se observa en la galaxia de Andrómeda lo que indica que se acerca y en algunos brazos de galaxias lo que permite descubrir su giro.




</doc>
<doc id="11587" url="https://es.wikipedia.org/wiki?curid=11587" title="Colores web">
Colores web

Los colores web son aquellos colores que aparecen en una página web. Se pueden basar sobre los sistemas de color RGB o HSL. En el código CSS (y antiguamente en HTML) son especificados como valores numéricos, aunque hay algunos colores que son nombrados por nombres propios en inglés.

La paleta de colores RGB (RVA en español) consta, básicamente, de tres colores primarios aditivos: Rojo, Verde, Azul.
Estos colores primarios aditivos, en HTML, están representados por tres pares hexadecimales del tipo 0xHH-HH-HH según el siguiente formato: (los colores básicos o primarios, no aquellos que son resultantes de mezclas).<nowiki>#RRGGBB (= #RRVVAA)</nowiki>

Los valores que puede adaptar cada uno de los tres pares hexadecimales van del 0x00 (0 decimal) al 0xFF (255 decimal). Cuanto mayor sea el valor del par, tanto mayor será también la intensidad (matiz, brillo o claridad) del color correspondiente a ese par (y viceversa).
Esto implica que el extremo inferior de la escala cromática parte de una intensidad (grado) de color mínima (nulo = par 0x00), pasa por una intensidad de color media (mediano = par 0x80 [128 decimal]) hasta llegar a una intensidad de color máxima (saturado = par 0xFF).
El grado de más alta pureza (absoluto) de un color primario aditivo estará determinado por la presencia total del mismo (saturación = 0xFF) junto con la ausencia total (nulidad = 0x00) de los otros dos colores primarios aditivos.

Además de estos tres colores primarios aditivos (RVA), existen tres colores primarios sustractivos o CMY (CMA en español): Cian, Magenta, Amarillo. Estos colores surgen de la siguiente combinación (mezcla) de los primarios aditivos:

En cuanto a su grado de pureza, ocurre algo inverso a los colores primarios aditivos, ya que el grado absoluto estará determinado por la nulidad de uno de sus componentes y la saturación de los otros dos.
Los colores complementarios de los primarios, tanto aditivos como sustractivos, serán recíprocamente:

La combinación simultánea de los tres primarios aditivos saturados produce el blanco (0xFFFFFF). Contrariamente, la combinación simultánea de los tres primarios sustractivos nulos produce el negro (0x000000).
Resulta claro también que la combinación de dos colores mutuamente complementarios producirá el blanco, de igual modo que la sustracción (absorción) de ambos dará lugar al negro (ausencia total de color). Así, p.ej., el rojo (0xFF0000) más su complemetario que es el cian (0x00FFFF), generan el blanco (0xFFFFFF). De hecho, el cian no es otra cosa que la sustracción del rojo al blanco. Obsérvese el siguiente gráfico:

Por su parte, el gris mediano (0x808080), que es el exacto término medio entre el negro y el blanco, se obtendrá a partir de la combinación simultánea de los tres primarios aditivos medianos. Análogamente, el gris semisaturado (claro) se obtendrá a partir de la combinación 0xC0C0C0, mientras que el gris seminulo (oscuro) mediante 0x404040.
De este modo, tenemos que el resto de los colores, que están comprendidos entre el negro (0x000000) y el blanco (0xFFFFFF), surgen de la combinación de los tres primarios aditivos en distintos grados. En otras palabras: bastará con reemplazar cada uno de los pares 0xHH-HH-HH por un valor comprendido entre 0x00 y 0xFF para obtener cualquiera de los colores posibles.

Existen 16777216 combinaciones distintas en el sistema RGB de 24 bits y, por lo tanto, 16777216 colores: 256 × 256 × 256 = 16777216. En la práctica, sin embargo, puede haber algunas combinaciones que no sean válidas. Eso pasaba antiguamente, cuando la paleta de colores más grande tenía 256 colores (8 bits). Por eso, existen 216 colores seguros, que serán visibles en cualquier dispositivo sin necesidad de tramado, reservándose los otros 40 colores para el sistema, de los cuales algunos se muestran aquí.

En el sistema de colores HSL los colores se miden por tres parámetros ("hue", "saturation" y "light") que determinan la posición del color en el cilindro de colores HSL. El primer parámetro es el ángulo horizontal, el segundo es la distancia horizontal del centro de la base y el tercero es la distancia vertical (altura) del centro de la base.

Un color puede mostrarse con una opacidad determinada en pantalla, que se determina por el parámetro alpha, que, añadido a RGB y HSL, los convierte en RGBa y HSLa.

Entre los sistemas web hay algunas contradicciones que se pueden resumir en las siguiente tabla:

Esta tabla de colores ha sido adoptada para su uso por W3C/CSS, HTML/X11, Mozilla, SVG, Internet Explorer (IE)/Microsoft Windows, etc.




</doc>
<doc id="11588" url="https://es.wikipedia.org/wiki?curid=11588" title="Conductividad eléctrica">
Conductividad eléctrica

La conductividad eléctrica (símbolo σ) es la medida de la capacidad de un material o sustancia para dejar pasar la corriente eléctrica a través de él. La conductividad depende de la estructura atómica y molecular del material. Los metales son buenos conductores porque tienen una estructura con muchos electrones con vínculos débiles, y esto permite su movimiento. La conductividad también depende de otros factores físicos del propio material, y de la temperatura.

La conductividad es la inversa de la resistividad (símbolo ρ); por tanto, formula_1, y su unidad es el S/m (siemens por metro) o Ω·m. Usualmente, la magnitud de la conductividad es la proporcionalidad entre el campo eléctrico formula_2 y la densidad de corriente de conducción formula_3:

El físico inglés Stephen Gray (1666-1736) estudió principalmente la conductividad eléctrica de los cuerpos y, después de muchos experimentos, fue el primero en 1729 en transmitir electricidad a través de un conductor. En sus experimentos descubrió que para que la electricidad, o los "efluvios" o "virtud eléctrica", como él la llamó, pudiera circular por el conductor, éste tenía que estar aislado de tierra. Posteriormente estudió otras formas de transmisión y, junto con los científicos G. Wheler y J. Godfrey, clasificó los materiales en conductores y aislantes de la electricidad.

Los mecanismos de conductividad difieren entre los tres estados de la materia. Por ejemplo en los sólidos los átomos como tal no son libres de moverse y la conductividad se debe a los electrones. En los metales existen electrones cuasi-libres que se pueden mover muy libremente por todo el volumen, en cambio en los aislantes, muchos de ellos son sólidos iónicos.

La conductividad electrolítica en medios líquidos está relacionada con la presencia de sales en disoluciones, cuya disociación genera iones positivos y negativos capaces de transportar la energía eléctrica si se somete el líquido a un campo eléctrico. Estos conductores iónicos se denominan electrolitos o conductores electrolíticos.

Las determinaciones de la conductividad reciben el nombre de determinaciones conductométricas y tienen muchas aplicaciones como, por ejemplo:


La base de las determinaciones de la solubilidad es que las soluciones saturadas de electrólitos escasamente solubles pueden ser consideradas como infinitamente diluidas. Midiendo la conductividad específica de semejante solución y calculando la conductividad equivalente según ella, se halla la concentración del electrólito, es decir, su solubilidad.

Un método práctico sumamente importante es el de la titulación conductométrica, o sea la determinación de la concentración de un electrólito en solución por la medición de su conductividad durante la titulación. Este método resulta especialmente valioso para las soluciones turbias o fuertemente coloreadas que con frecuencia no pueden ser tituladas con el empleo de indicadores.

La conductividad eléctrica se utiliza para determinar la salinidad (contenido de sales) de suelos y substratos de cultivo, para lo que se disuelven en agua y se mide la conductividad del medio líquido resultante. Suele estar referenciada a 25 °C y el valor obtenido debe corregirse en función de la temperatura. Coexisten muchas unidades de expresión de la conductividad para este fin, aunque las más utilizadas son dS/m (deciSiemens por metro), mmhos/cm (milimhos por centímetro) y según los organismos de normalización europeos mS/m (miliSiemens por metro). El contenido de sales de un suelo o substrato también se puede expresar por la resistividad (se solía expresar así en Francia antes de la aplicación de las normas INEN).

Según la teoría de bandas de energía en sólidos cristalinos, son materiales conductores aquellos en los que las bandas de valencia y conducción se superponen, formándose una «nube» de electrones libres causante de la corriente al someter al material a un campo eléctrico. Estos medios conductores se denominan conductores eléctricos.

La Comisión Electrotécnica Internacional definió como patrón de la conductividad eléctrica:

Conductividad eléctrica de metales puros a temperaturas entre 273 y 300K (10 S⋅m): 
Antes del advenimiento de la mecánica cuántica, la teoría clásica empleada para explicar la conductividad de los metales era el modelo de Drude-Lorentz, donde los electrones se desplazan a una velocidad media aproximadamente constante que es la velocidad límite asociada al efecto acelerador del campo eléctrico y el efecto desacelerador de la red cristalina con la que chocan los electrones produciendo el efecto Joule.

Sin embargo, el advenimiento de la mecánica cuántica permitió construir modelos teóricos más refinados a partir de la teoría de bandas de energía que explican detalladamente el comportamiento de los materiales conductores.

Fenomenológicamente la interacción de los electrones libres de los metales con la red cristalina se asimila a una fuerza "viscosa", como la que existe en un fluido que tiene rozamiento con las paredes del conducto por el que fluye. La ecuación de movimiento de los electrones de un metal por tanto se puede aproximar por una expresión del tipo:
Así la velocidad de arrastre de la corriente, es aquella en la que se iguala el efecto acelerador del campo eléctrico con la resistencia debida a la red, esta velocidad es la que satisface:
Para un conductor que satisface la ley de Ohm y con un número "n" de electrones por unidad de volumen que se mueven a la misma velocidad se puede escribir:

Introduciendo el tiempo de relajación formula_4 y comparando las últimas expresiones se llega a que la conductividad puede expresarse como:

A partir de los valores conocidos de formula_5 se puede estimar el tiempo de relajación formula_6 y compararlo con el tiempo promedio entre impactos de electrones con la red. Suponiendo que cada átomo contribuye con un electrón y que "n" es del orden de 10 electrones por m³ en la mayoría de metales. Usando además los valores de la masa del electrón formula_7 y la carga del electrón formula_8 el tiempo de relajamiento 10 s.

Para juzgar si ese modelo fenomenológico explica adecuadamente la ley de Ohm y la conductividad en los metales debe interpretarse el tiempo de relajamiento con las propiedades de la red. Si bien el modelo no puede ser teóricamente correcto porque el movimiento de los electrones en un cristal metálico está gobernado por la mecánica cuántica, al menos los órdenes de magnitud predichos por el modelo son razonables. Por ejemplo es razonable relacionar el tiempo de relajamiento formula_6 con el tiempo medio entre colisiones de un electrón con la red cristalina. Teniendo en cuenta que la separación típica entre átomos de la red es "l" = 5·10 m y usando la teoría de gases ideales aplicada a los electrones libres la velocidad de los mismos sería formula_10 = 10 m/s, por lo que formula_11 = 5·10 s, que está en buen acuerdo con los valores inferidos para la misma magnitud a partir de la conductividad de los metales.

Según el modelo de Drude-Lorentz la velocidad de los electrones debería variar con la raíz cuadrada de la temperatura, pero cuando se compara el tiempo entre colisiones estimado por el modelo de Drude-Lorentz con la conductividad a bajas velocidades, no se obtienen valores coherentes, ya que esas predicciones del modelo solo son compatibles con distancias interiónicas mucho mayores que las distancias reales.

En el modelo cuántico los electrones son acelerados por el campo eléctrico, y también interaccionan con la red cristalina transfiriéndole parte de su energía y provocando el efecto Joule. Sin embargo, al ser dispersados en una colisión con la red, por el principio de exclusión de Pauli los electrones deben acabar después de la colisión con el momentum lineal de un estado cuántico que previamente estuviera vacío; eso hace que los electrones dispersados con mayor probabilidad sean los más energéticos. Tras ser dispersados pasan a estados cuánticos con un momentum negativo de menor energía; esa dispersión continua hacia estados de momentum opuesto es lo que contrarresta el efecto acelerador del campo. En esencia este modelo comparte con el modelo clásico de Drude-Lorentz la idea de que es la interacción con la red cristalina lo que hace que los electrones se muevan a una velocidad estacionaria y no se aceleren más allá de un cierto límite. Aunque cuantitativamente los dos modelos difieren especialmente a bajas temperaturas.

Dentro del modelo cuántico la conductividad viene dada por una expresión superficialmente similar al modelo clásico de Drude-Lorentz:

Donde:
Si por un razonamiento cuántico se trata de calcular la probabilidad de dispersión se tiene:
Donde:
De acuerdo con los cálculos cuánticos, la sección eficaz de los dispersores es proporcional al cuadrado de la amplitud de su vibración térmica, y como dicho cuadrado es proporcional a la energía térmica, y esta es proporcional a la temperatura "T" se tiene que a bajas temperaturas:

Este comportamiento predicho correctamente por el modelo no podía ser explicado por el modelo clásico de Drude-Lorentz, por lo que dicho modelo se considera superado por el correspondiente modelo cuántico especialmente para bajas temperaturas.





</doc>
<doc id="11589" url="https://es.wikipedia.org/wiki?curid=11589" title="Metalurgia">
Metalurgia

La metalurgia es la técnica de la obtención y tratamiento de los metales a partir de minerales metálicos. También estudia la producción de aleaciones. El control de calidad de los procesos. La metalurgia es la rama que aprovecha la ciencia, la tecnología y el arte de obtener metales y minerales industriales, partiendo de sus minas, de una manera eficiente, económica y con resguardo del ambiente, a fin de adaptar dichos recursos en beneficio del desarrollo y bienestar de la humanidad.

El cobre fue uno de los primeros minerales trabajados por el hombre, ya que se encuentra en estado casi puro (cobre nativo) en la naturaleza. Junto al oro y la plata fue utilizado desde finales del Neolítico, golpeándolo, al principio, hasta dejarlo plano como una lámina. Después, como consecuencia del perfeccionamiento de las técnicas cerámicas, se aprendió a fundirlo en horno y vaciarlo en moldes, lo que permitió fabricar mejores herramientas y en mayor cantidad. Esto originó la Edad del Cobre de la Humanidad (también conocida como Calcolítico).

Posteriormente se experimentó con diversas aleaciones, como la de arsénico, que produjo cobre arsenicado, o la de estaño, que dio lugar al bronce e inició la Edad del Bronce de la Humanidad. El bronce, más duro y cortante que el cobre, apareció hacia el 3000 a. C.

El proceso de adquisición de los conocimientos metalúrgicos fue diferente en las distintas partes del mundo, siendo las evidencias más antiguas de fundición del plomo y el cobre del VII milenio a. C., en Anatolia y en Kurdistán. En América no hay constancia hasta el I milenio a. C. y en África el primer metal que se consiguió fundir fue el hierro, durante el II milenio a. C.

El hierro, que inauguró la Edad del Hierro de la Humanidad, comenzó a ser trabajado en Anatolia hacia el tercer milenio a. C.. Este mineral requiere altas temperaturas para su fundición y moldeado, para ser así es más maleable, duro y resistente que el cobre. Algunas técnicas usadas en la antigüedad fueron el moldeo a la cera perdida, la soldadura o el templado del acero. Las primeras fundiciones conocidas empezaron en China en el siglo I a. C., pero no llegaron a Europa hasta el siglo XIII, cuando aparecieron los primeros altos hornos.

El empleo de los metales se debió, inicialmente, a la necesidad que se creó el hombre de utilizar objetos de prestigio y ostentación (adornos de cobre), para, posteriormente, pasar a sustituir sus herramientas de piedra, hueso y madera por otras mucho más resistentes al calor y al frío (hechas en bronce y, sobre todo, hierro). Los utensilios elaborados con metales fueron muy variados: armas, herramientas, vasijas, adornos personales, domésticos y religiosos. El uso de los metales repercutió, a partir de la generalización del hierro, de diversas formas en la conformación de la civilización humana:

En la Edad Media la metalurgia estaba muy ligada a las técnicas de purificación de metales preciosos y la acuñación de moneda.

La metalurgia extractiva es el área de la metalurgia en donde se estudian y aplican operaciones y procesos para el tratamiento de minerales o materiales que contengan una especie útil (oro, plata, cobre, etc.), dependiendo el producto que se quiera obtener, se realizarán distintos métodos de tratamiento.



Se define como la técnica de producción de polvos de un metal para poder emplearlos en la elaboración de objetos útiles. Los primeros en utilizar esta técnica fueron los egipcios desde el año 3000 a. C. en la producción de utensilios de hierro.

Como principales procesos se tienen el compactado y sinterizado. El compactado consiste en preparar adecuadamente mezclas de polvos, a temperatura ambiente o a temperatura elevada y a una presión considerablemente alta. Se obtiene un comprimido manipulable, pero relativamente frágil, al que se le llamara aglomerado verde. El sinterizado es la operación donde el aglomerado verde es expuesto a una fuente de calor inferior al punto de fusión del metal en atmósferas inertes. Este proceso le otorga las resistencias mecánicas que se requieren.

Se pueden aplicar en la elaboración de metales compuestos, combinaciones de metales-no metales, metales refractarios. Por ejemplo: magnetos, filtros de metal, escobillas para motor.

Los procesos metalúrgicos comprenden las siguientes fases:

Operaciones básicas de obtención de metales: 

Dependiendo el producto que se quiera obtener, se realizarán distintos métodos de tratamiento. Uno de los tratamientos más comunes es la mena, consiste en la separación de los materiales de desecho. Normalmente entre el metal está mezclado con otros materiales como arcilla y silicatos, a esto se le suele denominar ganga.

Uno de los métodos más usuales es el de la flotación que consiste en moler la mena y mezclarla con agua, aceite y detergente. Al batir esta mezcla líquida se produce una espuma que, con ayuda de la distinta densidad que proporciona el aceite va a ir arrastrando hacia la superficie las partículas de mineral y dejando en el fondo la ganga.

Otra forma de flotación puede emplearse en la separación de minerales ferromagnéticos, utilizando imanes que atraen las partículas de mineral y dejando intacta la ganga.

Otro sistema de extracción de la mena es la amalgama formada con la aleación de mercurio con otro metal o metales. Se disuelve la plata o el oro contenido en la mena para formar una amalgama líquida, que se separa con facilidad del resto. Después el metal de oro y plata se purifican eliminando el mercurio mediante la destilación.



</doc>
<doc id="11590" url="https://es.wikipedia.org/wiki?curid=11590" title="Siderurgia">
Siderurgia

"Acería redirige aquí, para otros usos ver Acería (desambiguación)"

La siderurgia (del griego "σίδερος", "síderos", "hierro") o siderometalurgia es la técnica del tratamiento del mineral de hierro para obtener diferentes tipos de este o de sus aleaciones tales como el acero. El proceso de transformación del mineral de hierro comienza desde su extracción en las minas. El hierro se encuentra presente en la naturaleza en forma de óxidos, hidróxidos, carbonatos, silicatos y sulfuros. Los más utilizados por la siderurgia son los óxidos, hidróxidos y carbonatos. Los procesos básicos de transformación son los siguientes:

Estos minerales se encuentran combinados en rocas, las cuales contienen elementos indeseados denominados gangas. Parte de la ganga puede ser separada del mineral de hierro antes de su envío a la siderurgia, existiendo principalmente dos métodos de separación:



Una vez realizada la separación, el mineral de hierro es llevado a la planta siderúrgica donde será procesado para convertirlo primeramente en arrabio y posteriormente en acero.

Se denomina siderurgia o siderurgia integral a una planta industrial dedicada al proceso completo de producir acero a partir del mineral de hierro, mientras que se denomina acería a una planta industrial dedicada exclusivamente a la producción y elaboración de acero partiendo de otro acero o de hierro.

El acero es una aleación de hierro y carbono. Se produce en un proceso de dos fases. Primero el mineral de hierro es reducido o fundido con coque y piedra pómez, produciendo hierro fundido que es moldeado como arrabio o conducido a la siguiente fase como hierro fundido. La segunda fase, la de aceración, tiene por objetivo reducir el alto contenido de carbono introducido al fundir el mineral y eliminar las impurezas tales como azufre y fósforo, al mismo tiempo que algunos elementos como manganeso, cromo, níquel, hierro o vanadio son añadidos en forma de ferro-aleaciones para producir el tipo de acero demandado.

En las instalaciones de colada y laminación se convierte el acero bruto fundido en lingotes o en desbastes cuadrados (slabs) o planos (flog) y posteriormente en perfiles o chapas, laminadas en caliente.

Una planta integral tiene todas las instalaciones necesarias para la producción de acero en diferentes formatos.


Las materias primas para una planta integral son mineral de hierro, caliza y coque. Estos materiales son cargados en capas sucesivas y continuas en un horno alto donde la combustión del carbón ayudada por soplado de aire y la presencia de caliza funde el hierro contenido en el mineral, que se transforma en hierro líquido con un alto contenido en carbono. 

A intervalos, el hierro líquido acumulado en el alto horno es transformado en lingotes de arrabio o llevado líquido directamente en contenedores refractarios a las acerías. Históricamente el proceso desarrollado por Henry Bessemer ha sido la estrella en la producción económica de acero, pero actualmente ha sido superado en eficacia por los procesos con soplado de oxígeno, especialmente los procesos conocidos como Acerías LD.

El acero fundido puede seguir dos caminos: la colada continua o la colada clásica. En la colada continua el acero fundido es colado en grandes bloques de acero conocidos como "tochos". Durante el proceso de colada continua puede mejorarse la calidad del acero mediante adiciones como, por ejemplo, aluminio, para que las impurezas “floten” y salgan al final de la colada pudiéndose cortar el final del último lingote que contiene las impurezas. 
La colada clásica pasa por una fase intermedia que vierte el acero líquido en lingoteras cuadradas o rectangulares (petacas) según sea el acero se destine a producir perfiles o chapas. Estos lingotes deben ser recalentados en hornos antes de ser laminados en trenes desbastadores para obtener bloques cuadrados ("bloms") para laminar perfiles o planos rectangulares ("slabs") para laminar chapas planas o en bobinas pesadas.

Debido al coste de la energía y a los esfuerzos estructurales asociados con el calentamiento y coladas de un alto horno, estas instalaciones primarias deben operar en campañas de producción continua de varios años de duración. Incluso durante periodos de caída de la demanda de acero no es posible dejar que un alto horno se enfríe, aun cuando son posibles ciertos ajustes de la producción.

Las plantas siderúrgicas integrales son rentables con una capacidad de producción superior a los 2.000.000 de toneladas anuales y sus productos finales son, generalmente, grandes secciones estructurales, chapa pesada, redondos pesados, rieles de ferrocarril y, en algunos casos, palanquillas y tubería pesada.

Un grave inconveniente ambiental asociado a las plantas siderúrgicas integrales es la contaminación producida por sus hornos de coque, producto esencial para la reducción del mineral de hierro en el alto horno.

Por otra parte, con el fin de reducir costes de producción las plantas integrales pueden tener instalaciones complementarias características de las acerías especializadas: hornos eléctricos, coladas continuas, trenes de laminación comerciales o laminación en frío.

La capacidad mundial de producción de acero en plantas integrales está cerca de la demanda global, así la competencia entre productores hace que solo sean viables los más eficaces. Sin embargo, debido al alto nivel de empleo de estas instalaciones, los gobiernos a menudo las ayudan económicamente antes de correr el riesgo de enfrentarse a una situación de desempleo masivo. Estas medidas llevan, a escala internacional, a acusaciones de prácticas comerciales incorrectas ("dumping") y a conflictos entre países.

Estas plantas son productoras secundarias de aceros comerciales o plantas de producción de aceros especiales. Generalmente obtienen el hierro del proceso de chatarra de acero, especialmente de automóviles, y de subproductos como sinterizados o pellets de hierro (DRI). Estos últimos son de mayor coste y menor rentabilidad que la chatarra de acero por lo que su empleo se trata siempre de reducir a cuando sea estrictamente necesario para lograr el tipo de producto a conseguir por razones técnicas. Una acería especializada debe tener un horno eléctrico y “cucharas” u hornos al vacío (convertidores) para controlar la composición química del acero. El acero líquido pasa a lingoteras ligeras o a coladas continuas para dar forma sólida al acero fundido. También son necesarios hornos para recalentar los lingotes y poder laminarlos.

Originalmente estas acerías fueron adoptadas para la producción de grandes piezas fundidas (cigüeñas, grandes ejes, cilindros de motores náuticos, etc.) que posteriormente se mecanizan, y para productos laminados estructurales ligeros, tales como hierros redondos de hormigonar, vigas, angulares, tubería, rieles ligeros, etc. A partir de los años 1980 el éxito en el moldeado directo de barras en colada continua ha hecho productiva esta modalidad. Actualmente estas plantas tienden a reducir su tamaño y especializarse. Con frecuencia, con el fin de tener ventajas en los menores costes laborales, se empiezan a construir acerías especializadas en áreas que no tienen otras plantas de proceso de aceros, orientándose a la fabricación de piezas para transportes, construcción, estructuras metálicas, maquinaria, etc. 

Las capacidades de estas plantas pueden alcanzar alrededor del millón de toneladas anuales, siendo sus dimensiones más corrientes en aceros comerciales o de bajas aleaciones del rango 200.000 a 400.000 toneladas anuales. Las plantas más antiguas y las de producción de aceros con aleaciones especiales para herramientas y similares pueden tener capacidades del orden de 50.000 toneladas anuales o menores. 

Dadas sus características técnicas, los hornos eléctricos pueden arrancarse o parar con cierta facilidad lo que les permite trabajar 24 horas al día con alta demanda o cortar la producción cuando la demanda cae.

Las laminadoras son las máquinas encargadas de laminar, es decir, de aplanar el acero surgido del proceso de metalurgia y fundición para crear materia prima de acero en forma de planchas o láminas, que pueden ser estampadas, troqueladas y/o enchapadas para obtener productos secundarios del acero como automóviles o autopartes, ferrajes y otros.

Estas solo comprenden las siguientes clases de máquinas para el proceso: trenes de laminación, tren de alambrón, de perfiles comerciales o chapa fría. 
Para satisfacer las necesidades del proceso, esta clase de acero usado en este proceso contiene un bajo porcentaje de carbono, para darle mayor maleabilidad.



</doc>
<doc id="11591" url="https://es.wikipedia.org/wiki?curid=11591" title="Bronce">
Bronce

El bronce es toda aleación de cobre y estaño, en la que el primero constituye su base y el segundo aparece en una proporción del tres al veinte por ciento. Puede incluir otros metales.

Las aleaciones constituidas por cobre y zinc se denominan propiamente latón; sin embargo, dado que en la actualidad el cobre se suele alear con el estaño y zinc al mismo tiempo, en el lenguaje no especializado la diferencia entre bronce y latón es bastante imprecisa.

El bronce fue la primera aleación de importancia obtenida por el hombre y da su nombre a la Edad del Bronce. Durante milenios fue la aleación básica para la fabricación de armas y utensilios, y orfebres de todas las épocas lo han utilizado en joyería, medallas y escultura. Las monedas acuñadas con aleaciones de bronce tuvieron un protagonismo relevante en el comercio.

Cabe destacar entre sus aplicaciones actuales su uso en partes mecánicas resistentes al roce y a la corrosión, en instrumentos musicales de buena calidad como campanas, gongs, platillos de acompañamiento, saxofones, y en la fabricación de cuerdas de arpas, guitarras y pianos

El término «bronce» deriva probablemente del persa "berenj" («latón»). Otras versiones lo relacionan con el latín "aes brundisium" («mineral de Brindisi») por el antiguo puerto de Brundisium. Se cree que la aleación pudiera haber sido enviada por mar a este puerto, y desde allí era distribuida a todo el Imperio romano.

La introducción del bronce resultó significativa en cualquier civilización que lo halló, constituyendo la aleación más innovadora en la historia tecnológica de la humanidad. Herramientas, armas, y varios materiales de construcción como mosaicos y placas decorativas consiguieron mayor dureza y durabilidad que sus predecesores en piedra o cobre calcopirítico.

La técnica consistía en mezclar el mineral de cobre —por lo general calcopirita o malaquita— con el de estaño (casiterita) en un horno alimentado con carbón vegetal. El carbono del carbón vegetal reducía los minerales a cobre y estaño que se fundían y aleaban con el 5 al 10% en peso de estaño. El conocimiento metalúrgico de la fabricación de bronce dio origen en las distintas civilizaciones a la llamada Edad de Bronce.

Inicialmente las impurezas naturales de arsénico permitían obtener una aleación natural superior, denominada "bronce arsenical". Esta aleación, con no menos del 2% de arsénico, se utilizaba durante la Edad de Bronce para la fabricación de armas y herramientas, teniendo en cuenta que el otro componente, el estaño, no era frecuente en muchas regiones, y debía ser importado de parajes lejanos.
La presencia de arsénico hace a esta aleación altamente tóxica, ya que produce —entre otros efectos patológicos— atrofia muscular y pérdida de reflejos.

Las aleaciones basadas en estaño más antiguas que se conocen datan del cuarto milenio a. C. en Susa (actual Irán) y otros sitios arqueológicos en Luristán y Mesopotamia.

Aunque el cobre y el estaño pueden alearse con facilidad, raramente se encuentran minas mixtas, si bien existen algunas pocas excepciones en antiguos yacimientos en Irán y Tailandia. El forjado regular del bronce involucró desde siempre el comercio del estaño. De hecho, algunos arqueólogos sospechan que uno de los disparadores de la Edad del hierro, con el subsecuente y progresivo reemplazo del bronce en las aplicaciones más importantes, se debió a alguna interrupción seria en el comercio de ese mineral alrededor de 1200 a. C., en coincidencia con las grandes migraciones del Mediterráneo. La principal fuente de estaño en Europa fue Gran Bretaña, que posee depósitos de importancia en Cornualles. Se sabe que ya los fenicios llegaron hasta sus costas con mercancías del Mediterráneo para intercambiarlas por estaño.

En el Antiguo Egipto la mayoría de los elementos metálicos que se elaboraban consistían en aleaciones de cobre con arsénico, estaño, oro y plata. En tumbas del Imperio Nuevo, o en el templo de Karnak, se encuentran bajorrelieves mostrando una fundición datada en el siglo XV a. C.

En el caso de la Grecia clásica, conocida por su tradición escultórica en mármol, se sabe que desarrollaron igualmente técnicas de fundición de bronce avanzadas, como lo prueban los bronces de Riace, originados en el siglo V a. C.
En India, la plenitud artística de la Dinastía chola produjo esculturas notables entre los siglos X y XI de nuestra era, representando las distintas formas del dios Shivá y otras deidades.

Las civilizaciones de la América prehispánica conocían todas el uso de las aleaciones de bronce, si bien muchos utensilios y herramientas continuaban fabricándose en piedra. Se han hallado objetos fabricados con aleaciones binarias de cobre-plata, cobre-estaño, cobre al plomo e incluso aleaciones poco usuales de latón. Ya en la época colonial, las fundiciones más importantes se encontraban en Perú y en Cuba, dedicadas principalmente a la fabricación de campanas y cañones.

El bronce siguió en uso porque el acero de calidad no estuvo ampliamente disponible hasta muchos siglos después, con las mejoras de las técnicas de fundición a inicios de la Edad Media en Europa, cuando se obtuvo acero más barato y resistente, eclipsando al bronce en muchas aplicaciones.

Exceptuando el acero, las aleaciones de bronce son superiores a las de hierro en casi todas las aplicaciones. Por su elevado calor específico, el mayor de todos los sólidos, se emplea en aplicaciones de transferencia del calor.

Aunque desarrollan pátina no se oxidan bajo la superficie, son más frágiles y tienen menor punto de fusión. Son aproximadamente un 10% más pesadas que el acero, a excepción de las compuestas por aluminio o sílice. También son menos rígidas, por lo tanto en aplicaciones elásticas como resortes acumulan menos energía que las piezas similares de acero. Resisten la corrosión, incluso la de origen marino, el umbral de fatiga metálica es menor, y son mejores conductores del calor y la electricidad.

Otra característica diferencial de las aleaciones de bronce respecto al acero es la ausencia de chispas cuando se las golpea contra superficies duras. Esta propiedad ha sido aprovechada para fabricar martillos, mazas, llaves ajustables y otras herramientas para uso en atmósferas explosivas o en presencia de gases inflamables.

El cobre y sus aleaciones tienen una amplia variedad de usos como resultado de la versatilidad de sus propiedades mecánicas, físicas y químicas. Téngase en cuenta, por ejemplo, la conductividad eléctrica del cobre puro, la excelente maleabilidad de los cartuchos de munición fabricados en latón, la baja fricción de aleaciones cobre-plomo, las sonoridad del bronce para campanas y la resistencia a la corrosión de la mayoría de sus aleaciones.

Datos para una aleación promedio con 89% de cobre y 11% de estaño:


La aleación básica de bronce contiene aproximadamente el 88% de cobre y el 12% de estaño. El bronce "«alfa»" es la mezcla sólida de estaño en cobre. La aleación alfa de bronce con un 4 a 5% de estaño se utiliza para acuñar monedas y para fabricar resortes, turbinas, y herramientas de corte.

En muchos países se denomina incorrectamente "«bronce comercial»" al latón, que contiene 90% de cobre y 10% de zinc, pero no estaño. Es más duro que el cobre, y tiene una ductilidad similar. Se utiliza en tornillos y alambres.

La aleación de cobre con arsénico es el primer bronce utilizado por el hombre. Es una aleación blanquecina, muy dura y frágil. Se fabrica en una proporción del 70% de cobre y el 30% de arsénico, aunque es posible fundir bronces con porcentajes de arsénico de hasta 47,5%. En estos casos, el resultado es un material gris brillante, fusible al rojo y no alterado por el agua hirviente.

La simple exposición al aire del bronce arsenical produce una pátina oscura. Esta circunstancia, y la alta toxicidad del arsénico la convirtieron en una aleación muy poco utilizada, especialmente a partir del descubrimiento de la "alpaca", "plata alemana" o "bronce blanco", conocida desde tiempos antiguos en China y fabricada en Alemania desde finales del siglo XVIII.

El denominado bronce sol (en alemán; "Sonnenbronze") es una aleación utilizada en joyería, tenaz, dúctil y muy dura, que funde a temperaturas próximas a las del cobre (1357 °C) y está constituida hasta por el 60% de cobalto.

El cuproaluminio es un tipo de bronce, de color similar al del oro, en el cual el aluminio es el metal de aleación principal que se agrega al cobre. Una variedad de bronces de aluminio, de composiciones diferentes, han encontrado uso industrial.

A partir del descubrimiento de la pólvora se utilizó un bronce para cañones compuesto por un 90 a 91% de cobre y un 9 a 10% de estaño, proporción que se denomina comúnmente «bronce ordinario». Estas armas eran conocidas en China en épocas tan tempranas como el siglo XI a. C., y en Europa se utilizaron a partir del siglo XIII tanto para cañones como en falconetes.

Para el siglo XV la artillería del Imperio otomano contaba con grandes bombardas de bronce. Construidas en dos piezas, con un largo total de 5,20 m y 16,8 toneladas de peso, lanzaban balas de 300 kg a una distancia de hasta 1600 metros. De operación difícil, con una capacidad de tiro de no más de 15 disparos diarios, fueron utilizadas en el sitio de Constantinopla en 1453.

La fundición para campanas es generalmente frágil: las piezas nuevas presentan una coloración que varía del ceniza oscuro al blanco grisáceo, con tonos rojo amarillento o incluso rojo azulado en las aleaciones con mayor contenido de cobre.

La mayor proporción de cobre produce tonos más graves y profundos a igualdad de masa, mientras que el agregado de estaño, hierro o cinc produce tonos más agudos. Para obtener una estructura más cristalina y producir variantes en la sonoridad, los fundidores han utilizado también otros metales como antimonio o bismuto en pequeñas cantidades. 

La aleación con mayor sonoridad para fabricar campanas es el denominado metal de campana, que consta de 78% de cobre y de 22% de estaño. Es relativamente fácil para fundir, tiene una estructura granulosa compacta con fractura vítreo-concoidea de color rojizo. Este tipo de bronce era conocido desde la antigüedad en la India para fabricar gongs. Aunque poco frecuente por su coste, la adición de plata es una de las pocas que mejora aún más la sonoridad.

También se han utilizado aleaciones con hasta el 2% de antimonio. En China se conocía una aleación con 80% de cobre y 20% de estaño para fabricar campanas, grandes gongos y timbales. 

En Inglaterra se utilizó una aleación constituida por 80% de cobre, 10,25% de estaño, 5,50% de cinc y 4,25% de plomo. Es de sonoridad menor, teniendo en cuenta que el plomo no se homogeneiza con la aleación.

Para campanillas e instrumentos pequeños se utilizó frecuentemente una aleación del 68% de cobre y el 32% de estaño, que resulta en un material frágil, de fractura cenicienta.

Para platillos y gongs se usan varias aleaciones que van desde una aleación templada con el 80% de cobre y el 20% de estaño (B20), 88% de cobre y 12% estaño (B12, por ejemplo, ZHT Zildjian, Alpha Paiste), y la más económica B8, la cual consiste en solo el 8% de estaño por el 92% de cobre (Ejemplo, B8 Sabian, Paiste 201, Zildjian ZBT). El temple se logra volviendo a calentar la pieza fundida y enfriándola rápidamente.

La mayor campana que se conserva, llamada "Tsar Kólokol", fue fundida en 1733 por Iván Motorin, por encargo de la emperatriz Ana de Rusia, sobrina del Zar Pedro el Grande. Con un peso de 216 toneladas, 6,14 m de altura y 6,6 de diámetro. Nunca fue utilizada como instrumento, ya que un incendio en 1737 destruyó sus grandes soportes de madera. Desde 1836 se exhibe en el Kremlin de Moscú.

El "kara-kane" («metal chino» en japonés) es un bronce para campanas y orfebrería tradicional de Japón constituido por un 60% de cobre, 24% de estaño y 9% de cinc, con agregados de hierro y plomo.

Muchos orfebres suelen agregarle pequeñas cantidades de arsénico y antimonio para endurecer el bronce sin perder fusibilidad, y lograr mayor detalle en la impresión de los moldes.

El "kara-kane" es muy utilizado para artesanía y estatuaria no solo por su bajo punto de fusión, gran fluidez y buenas características de relleno de molde, sino también por su superficie suave que rápidamente desarrolla una fina pátina.

Existe una variedad singular denominada "seniokuthis", o bronce dorado, originada en la época de la dinastía Ming en China, que destaca por su textura lustrosa y su tonalidad dorada. En su fabricación tienen especial importancia las técnicas de pátina.

Las grandes esculturas de Buda realizadas por los orfebres japoneses demuestran el alto dominio técnico que poseían y teniendo en cuenta su gran tamaño, la mayoría de ellas debió ser fundida en el lugar de emplazamiento por medio de sucesivas etapas.


Para la fabricación de cojinetes y otras piezas sometidas a fricción suelen utilizarse aleaciones de bronce con hasta un 10% de plomo, que le otorga propiedades autolubricantes.

La característica distintiva del plomo es que no forma aleación con el cobre; de allí que queda distribuido de acuerdo a la técnica de fundido en la masa de la aleación, sin mezclarse íntimamente. Por este motivo, el calentamiento excesivo de una pieza de maquinaria construida con este material puede llevar a la «exudación» de plomo que queda aparente como barro o lodo. El reciclaje de estas piezas es también dificultoso, porque el plomo se funde y se separa de la aleación mucho antes de que el cobre llegue al punto de fusión.

El método más utilizado para la fundición artística del bronce es el de la «cera perdida» o microfusión, que —con diversas variantes— sigue los pasos siguientes:







</doc>
<doc id="11593" url="https://es.wikipedia.org/wiki?curid=11593" title="Electroerosión">
Electroerosión

La electroerosión es un proceso de fabricación también conocido como mecanizado por descarga eléctrica o EDM (por su nombre en inglés, "electrical discharge machining"). 
El proceso de electroerosión consiste en la generación de un arco eléctrico entre una pieza y un electrodo en un medio dieléctrico para arrancar partículas de la pieza hasta conseguir reproducir en ella las formas del electrodo. Ambos, pieza y electrodo, deben ser conductores, para que pueda establecerse el arco eléctrico que provoque el arranque de material.

Básicamente tiene dos variantes:
El proceso de erosión térmica en el cual se extrae metal mediante una serie de descargas eléctricas recurrentes entre una herramienta de corte que actúa como electrodo y una pieza conductora, en presencia de un fluido dieléctrico. Esta descarga se produce en un hueco (“gap”) de voltaje entre el electrodo y la pieza. El calor de la descarga vaporiza partículas diminutas del material de la pieza y del electrodo, que seguidamente se eliminan del hueco por el dieléctrico que fluye continuamente. La expansión del mecanizado por electroerosión en los últimos 45 años ha dado origen a los tres tipos principales que se enumeran a continuación, aunque los más utilizados son los dos primeros. 
Este es el tipo convencional que emplearon las primeras máquinas electroerosionadoras y se basa en el proceso que ya describimos oportunamente.

Durante el proceso de electroerosión la pieza y el electrodo se sitúan muy cercanos entre sí, dejando un hueco que oscila entre 0,01 y 0,05 mm, por el que circula un líquido dieléctrico (normalmente aceite de baja conductividad). Al aplicar una diferencia de tensión continua y pulsante entre ambos, se crea un campo eléctrico intenso que provoca el paulatino aumento de la temperatura, hasta que el dieléctrico se vaporiza.
Al desaparecer el aislamiento del dieléctrico salta la chispa, incrementándose la temperatura hasta los 20 000 °C, vaporizándose una pequeña cantidad de material de la pieza y el electrodo formando una burbuja que hace de puente entre ambas.
Al anularse el pulso de la fuente eléctrica, el puente se rompe separando las partículas del metal en forma gaseosa de la superficie original. Estos residuos se solidifican al contacto con el dieléctrico y son finalmente arrastrados por la corriente junto con las partículas del electrodo.
Dependiendo de la máquina y ajustes en el proceso, es posible que el ciclo completo se repita miles de veces por segundo. También es posible cambiar la polaridad entre el electrodo y la pieza.

El resultado deseado del proceso es la erosión uniforme de la pieza, reproduciendo las formas del electrodo. En el proceso el electrodo se desgasta, por eso es necesario desplazarlo hacia la pieza para mantener el hueco constante. En caso que el desgaste sea severo, el electrodo es reemplazado. Si se quiere un acabado preciso (tolerancia de forma +-0,05 mm es preciso la utilización de dos electrodos).

Entre las características principales de la electroerosión por penetración podemos mencionar: El fluido dieléctrico es aceite mineral, aunque algunas máquinas pueden usar agua u otros líquidos especiales. Pueden obtenerse tanto formas pasantes como formas ciegas de geometrías complicadas. Capacidad de extracción en aceros: hasta 2000 mm3/min. Rugosidad mínima en aceros: hasta menos de 0,4 m Ra. Aplicaciones: fabricación de moldes y troqueles de embutición
El electrodo es comúnmente hecho de grafito pues este, por tener una elevada temperatura de vaporización, es más resistente al desgaste. Puede ser trabajado en una fresadora específica con el fin de crear ya sea un electrodo macho o un electrodo hembra, lo que significa que el electrodo tendrá la forma opuesta a la forma deseada y resultante en la pieza de trabajo.
Es buena práctica tener un electrodo de erosión en bruto y uno que consuma en forma fina y final, mas esto puede ser determinado por las dimensiones y características de la pieza a ser lograda.
Los electrodos pueden ser manufacturados en forma que múltiples formas pertenezcan al mismo pedazo de grafito.
También el cobre es un material predilecto para la fabricación de electrodos precisos, por su característica conductividad, aunque por ser un metal suave su desgaste es más rápido. El electrodo de cobre es ideal para la elaboración de hoyos o agujeros redondos y profundos. Comúnmente estos electrodos se encuentran de diámetros con tamaños milimétricos en incrementos de medio milímetro y longitudes variadas. Este proceso en particular es muy utilizado para antes del proceso de electroerosión con hilo, para producir el agujero inicial donde pase el hilo a través de un grosor de material que es inconveniente al taladro convencional.
Si deseamos un buen acabado en el objeto a erosionar, sea cual sea el material en que se construya el electrodo este debe ser repasado a mano después ser mecanizado en la fresadora o torno debido a las marcas que las herramientas de corte utilizadas en estas máquinas producen pequeñas marcas en los electrodos.



A modo de ejemplo se puede citar el agujereado de las boquillas de los inyectores en la industria automotriz, así como en la fabricación de moldes y matrices para procesos de moldeo o deformación plástica.

Es un desarrollo del proceso anteriormente descrito, nacido en los años de la década de 1970, y por consiguiente, más moderno que el anterior, que sustituye el electrodo por un hilo conductor; además, este proceso tiene mejor movilidad. 
Las tasas de arranque de material con hilo rondan los 350 cm/h.
La calidad, material y diámetro del hilo, en conjunción al voltaje y amperaje aplicado, son factores que influyen directamente la velocidad con que una pieza pueda ser trabajada. También, el grosor y material de la pieza dictan ajustes para el cumplimiento del corte.
El acabado deseado en el proceso también es un factor de consideración que afecta el tiempo de ciclo de manufactura, pues el acabado que este proceso deja en la pieza puede ser mejorado cuanto más pases semi-repetitivos de corte sobre la misma superficie son ejecutados.
El hilo metálico puede ser fabricado de latón o de zinc (y molibdeno, en caso de máquinas de hilo recirculante). En prácticas de protección al medio ambiente, después del uso y descarte del hilo empleado y sus residuos, el material del hilo, ya sea en forma de hilo o éste pulverizado, es acumulado separadamente con el fin de ser reciclado.
Existen varios diámetros en el mercado, incluyendo 0,010” (0,25mm) y 0,012” (0,30mm). Generalmente el hilo se vende en rollos y por peso, más que por su longitud.
La tensión del hilo es importante para producir un corte efectivo, y por consiguiente una mejor parte; la sobretensión del hilo resulta en que este se rompa cuando no sea deseado. Mas la ruptura del hilo es común durante el proceso, y también es necesaria. En unos talleres, los encendedores comunes se utilizan como una forma práctica de cortar el hilo.
Inicialmente, la posición de una cabeza superior y una cabeza inferior por las cuales pasa el hilo están en un alineamiento vertical y concéntrico una a la otra; el hilo en uso se encuentra entre estos dos componentes mecánicos.
El hilo mayormente es un hilo fino.

A diferencia de las máquinas de electroerosión con electrodo de forma a las que la polaridad aplicada puede ser invertida, la polaridad en el proceso de electroerosión con hilo es constante, o sea que la "mesa" o marco donde las piezas son montadas para ser trabajadas es tierra; esto significa que es de polaridad negativa. El hilo, por consiguiente, es el componente mecánico al que la carga positiva es dirigida.
Todas las máquinas reciben un hilo a modo que este se tensione en forma vertical (axial "Z"), para producir cortes y movimientos en axiales "X" e "Y". Mas en su mayoría, las máquinas de electroerosión con hilo tienen la capacidad de mover sus componentes para ajustar el hilo vertical y producir un ángulo limitado de corte (axiales "U" y "V").

En el corte interno, el hilo sujeto por sus extremos comenzando por un agujero previamente taladrado y mediante un movimiento de vaivén, como el de una sierra, va socavando la pieza hasta obtener la geometría deseada.
En el corte externo, el hilo puede empezar el movimiento desde el exterior del perímetro de la pieza hasta entablar el arco; continúa su movimiento hasta que consigue la periferia deseada.



En el siglo XXI se puede producir un proceso parecido al de torneado a alta velocidad utilizando el hilo para configuraciones caprichosas, dimensiones difíciles y acabados satisfactorios.

Cuando una de estas dos formas de proceso es escogida a ser aplicada, se debe buscar como finalidad que el ciclo de manufactura sea lo más breve posible (reducción de tiempo de ciclo), que el acabado en la pieza tenga la aspereza y calidad deseada, y que la precisión en dimensiones y tolerancias geométricas sean las planeadas, todo esto incluido con las prácticas generales y aceptadas en la buena manufactura, fabricación y producción.
La plantación de un ciclo inteligente y, cuando sea posible, una preparación de múltiples piezas en orden y montadas con el fin de ser trabajadas en ciclos que requieran atención mínima, son dos formas que contribuyen al ahorro de tiempo y recursos. Obviamente, la protección y seguridad del operador es lo más importante y, por consiguiente, contribuye también a la prosperidad y ahorro.

Siempre se debe observar precauciones y consideraciones preventivas, y regulaciones dictadas por las buenas prácticas, por instructivos y manuales de las máquinas y demás equipo, y por el taller o fábrica de trabajo donde el proceso de electroerosión sea practicado.




</doc>
<doc id="11594" url="https://es.wikipedia.org/wiki?curid=11594" title="Fatiga de materiales">
Fatiga de materiales

En ingeniería y, en especial, en ciencia de los materiales, la fatiga de materiales se refiere a un fenómeno por el cual la rotura de los materiales bajo cargas dinámicas cíclicas se produce más fácilmente que con cargas estáticas. Aunque es un fenómeno que, sin definición formal, era reconocido desde la antigüedad, este comportamiento no fue de interés real hasta la revolución industrial, cuando a mediados del siglo XIX se comenzaron a producir las fuerzas necesarias para provocar la rotura de los materiales con cargas dinámicas muy inferiores a las necesarias en el caso estático y a desarrollar métodos de cálculo para el diseño de piezas confiables. Este no es el caso de materiales de aparición reciente, para los que es necesaria la fabricación y el ensayo de prototipos.

La amplitud de la tensión varía alrededor de un valor medio, el promedio de las tensiones máxima y mínima en cada ciclo:

El intervalo de tensiones es la diferencia entre tensión máxima y mínima

La amplitud de tensión es la mitad del intervalo de tensiones

El cociente de tensiones R es el cociente entre las amplitudes mínima y máxima

Por convención, los esfuerzos a tracción son positivos y los de compresión son negativos. Para el caso de un ciclo con inversión completa de carga, el valor de R es igual a -1.

La curva s-n, también llamada curva de Wöhler, se obtienen a través de una serie de ensayos donde una probeta del material se somete a tensiones cíclicas con una amplitud máxima relativamente grande (aproximadamente 2/3 de la resistencia estática a tracción). Se cuentan los ciclos hasta rotura. Este procedimiento se repite en otras probetas a amplitudes máximas decrecientes.

Los resultados se representan en un diagrama de tensión, S, frente al logaritmo del número N de ciclos hasta la rotura para cada una de las probetas. Los valores de S se toman normalmente como amplitudes de la tensión formula_5.

Se pueden obtener dos tipos de curvas S-N. A mayor tensión, menor número de ciclos hasta rotura. En algunas aleaciones férreas y en aleaciones de titanio, la curva S-N se hace horizontal para valores grandes de N, es decir, existe una tensión límite, denominada límite de fatiga, por debajo del cual la rotura por fatiga no ocurrirá.
Suele decirse, de manera muy superficial, que muchas de las aleaciones no férreas (aluminio, cobre, magnesio, etc.) no tienen un límite de fatiga, dado que la curva S-N continúa decreciendo al aumentar N. Según esto, la rotura por fatiga ocurrirá independientemente de la magnitud de la tensión máxima aplicada, y por tanto, para estos materiales, la respuesta a fatiga se especificaría mediante la resistencia a la fatiga que se define como el nivel de tensión que produce la rotura después de un determinado número de ciclos. Sin embargo, esto no es exacto: es ingenuo creer que un material se romperá al cabo de tantos ciclos, no importa cúan ridículamente pequeña sea la tensión presente.

En rigor, todo material cristalino (metales, …) presenta un límite de fatiga. Ocurre que para materiales como la mayoría de los férricos, dicho límite suele situarse en el entorno del millón de ciclos (para ensayos de probeta rotatoria), para tensiones internas que rondan 0,7-0,45 veces el límite elástico del material; mientras que para aquellos que se dicen sin límite de fatiga, como el aluminio, se da incluso para tensiones muy bajas (en el alumnio, de 0,1-0,2 veces dicho límite), y aparece a ciclos muy elevados (en el aluminio puede alcanzar los mil millones de ciclos; en el titanio pueden ser, según aleaciones, cien millones de ciclos o incluso, excepcionalmente el billón de ciclos). Como en general no se diseñan máquinas ni elementos de manera que las máximas tensiones sean de 0,1-0,2 veces el límite elástico del material, pues en ese caso se estarían desaprovechando buena parte de las capacidades mecánicas del material, y como tampoco se suele diseñar asumiendo valores de vida por encima del millón de ciclos, en la práctica este tipo de materiales no van a poder presentar su límite de fatiga, aunque sí lo tienen.

Esta confusión surge de la propia naturaleza de las curvas S-N de Wöhler, que fueron concebidas en el siglo XIX para los aceros. Al ampliarse el tipo de materiales metálicos usuales en ingeniería, los mismos conceptos y las mismas curvas se trasladaron a otros metales cuyo comportamiento a fatiga es esencialmente diferente (de hecho, es una característica propia de la fatiga la gran variabilidad de comportamientos que presenta en los distintos tipos de materiales). Y como quiera que el acero ha sido y es la piedra angular de la ingeniería, interesaba comparar las propiedades de los demás metales con respecto al mismo: es y era común que, al ensayar materiales, los ensayos se suspendieran una vez superado el millón de ciclos, considerando que no interesaba caracterizar materiales por encima de ese límite temporal.
Otro parámetro importante que caracteriza el comportamiento a fatiga de un material es la vida a fatiga N. Es el número de ciclos para producir una rotura a un nivel especificado de tensiones.

Además, el conocimiento del comportamiento a fatiga no es igual en todos los materiales: el material mejor conocido, más ensayado y más fiable en cuanto a predicciones a fatiga es la familia de los aceros. De otros materiales metálicos de uso común como el aluminio, el titanio, aleaciones de cobre, níquel, magnesio o cromo, se dispone de menos información (decreciente esta con la novedad de la aleación), aunque la forma de los criterios de cálculo a fatiga y de las curvas S-N parece regular, y es parecida a la de los de los aceros, y se considera que su fiabilidad es alta. Para materiales cerámicos, por el contrario, se dispone de muy poca información, y de hecho, el estudio de la fatiga en ellos y en polímeros y materiales compuestos es un tema de candente investigación actual.

En todo caso, existe una diferencia notable entre la teoría y la realidad. Esto conduce a incertidumbres significativas en el diseño cuando la vida a fatiga o el límite de fatiga son considerados. La dispersión en los resultados es una consecuencia de la sensibilidad de la fatiga a varios parámetros del ensayo y del material que son imposibles de controlar de forma precisa. Estos parámetros incluyen la fabricación de las probetas y la preparación de las superficies, variables metalúrgicas, alineamiento de la probeta en el equipo de ensayos, tensión media y frecuencia de carga del ensayo.

Aproximadamente la mitad de las probetas ensayadas se rompen a niveles de tensión que están cerca del 25% por debajo de la curva. Esto suele asociarse a la presencia de fuentes de concentración de tensiones internas, tales como defectos, impurezas, entallas, ralladuras, …, que han permanecido indetectadas.

Se han desarrollado técnicas estadísticas y se han utilizado para manejar este fallo en términos de probabilidades. Una manera adecuada de presentar los resultados tratados de esta manera es con una serie de curvas de probabilidad constante.
Fatiga de bajo número de ciclos (oligofatiga) < formula_6 ciclos.

Fatiga de alto número de ciclos > formula_6 ciclos.

El proceso de rotura por fatiga se desarrolla a partir del inicio de la grieta y se continúa con su propagación y la rotura final.

Las grietas que originan la rotura o fractura casi siempre nuclean sobre la superficie en un punto donde existen concentraciones de tensión (originadas por diseño o acabados, ver Factores).

Las cargas cíclicas pueden producir discontinuidades superficiales microscópicas a partir de escalones producidos por deslizamiento de dislocaciones, los cuales actuarán como concentradores de la tensión y, por tanto, como lugares de nucleación de grietas.



Al mismo tiempo que la grieta aumenta en anchura, el extremo avanza por continua deformación por cizalladura hasta que alcanza una configuración enromada. Se alcanza una dimensión crítica de la grieta y se produce la rotura.

La región de una superficie de fractura que se formó durante la etapa II de propagación puede caracterizarse por dos tipos de marcas, denominadas marcas de playa y estrías. Ambas indican la posición del extremo de la grieta en diferentes instantes y tienen el aspecto de crestas concéntricas que se expanden desde los puntos de iniciación. Las marcas de playa son macroscópicas y pueden verse a simple vista.

Las marcas de playa y estrías no aparecen en roturas rápidas.

Los resultados de los estudios de fatiga han mostrado que la vida de un componente estructural puede relacionarse con la velocidad de crecimiento de la grieta. La velocidad de propagación de la grieta es una función del nivel de tensión y de la amplitud de la misma.

Dónde:

El valor de m normalmente está comprendido entre 1 y 6.

o bien

Desarrollando estas expresiones a partir de gráficas generadas por ellas mismas, se puede llegar a la siguiente ecuación:

Dónde:

formula_14 se puede calcular por

donde formula_18 es la tenacidad de fractura de deformaciones planas.

Estas fórmulas fueron generadas por Paul C. Paris en 1961 realizando una gráfica logarítmica log-log de la velocidad de crecimiento de grieta contra el factor de intensidad de tensiones mostrando una relación lineal en la gráfica. Utilizando esta gráfica se pueden realizar predicciones cuantitativas sobre la vida residual de una probeta dado un tamaño de grieta particular. Se encuentra así el comienzo de la iniciación o iniciación rápida de grieta.

Son diversos los factores que intervienen en un proceso de rotura por fatiga aparte de las tensiones aplicadas. Así pues, el diseño, tratamiento superficial y endurecimiento superficial pueden tener una importancia relativa.

El diseño tiene una influencia grande en la rotura de fatiga. Cualquier discontinuidad geométrica actúa como concentradora de tensiones y es por donde puede nuclear la grieta de fatiga. Cuanto más aguda es la discontinuidad, más severa es la concentración de tensiones.

La probabilidad de rotura por fatiga puede ser reducida evitando estas irregularidades estructurales, o sea, realizando modificaciones en el diseño, eliminando cambios bruscos en el contorno que conduzcan a cantos vivos, por ejemplo, exigiendo superficies redondeadas con radios de curvatura mayores.

Las dimensiones de la pieza también influyen, aumentando el tamaño de la misma obtenemos una reducción en el límite de fatiga.

En las operaciones de mecanizado, se producen pequeñas rayas y surcos en la superficie de la pieza por acción del corte. Estas marcas limitan la vida a fatiga pues son pequeñas grietas las cuales son mucho más fáciles de aumentar. Mejorando el acabado superficial mediante pulido aumenta la vida a fatiga.

Uno de los métodos más efectivos de aumentar el rendimiento es mediante esfuerzos residuales de compresión dentro de una capa delgada superficial. Cualquier tensión externa de tracción es parcialmente contrarrestada y reducida en magnitud por el esfuerzo residual de compresión. El efecto neto es que la probabilidad de nucleación de la grieta, y por tanto de rotura por fatiga se reduce.

Este proceso se llama «granallado» o «perdigonado». Partículas pequeñas y duras con diámetros del intervalo de 0,1 a 1,0 mm son proyectadas a altas velocidades sobre la superficie a tratar. Esta deformación induce tensiones residuales de compresión.

Es una técnica por la cual se aumenta tanto la dureza superficial como la vida a fatiga de los aceros aleados. Esto se lleva a cabo mediante procesos de carburación y nitruración, en los cuales un componente es expuesto a una atmósfera rica en carbono o en nitrógeno a temperaturas elevadas. Una capa superficial rica en carbono en nitrógeno es introducida por difusión atómica a partir de la fase gaseosa. Esta capa es normalmente de 1mm de profundidad y es más dura que el material del núcleo. La mejora en las propiedades de fatiga proviene del aumento de dureza dentro de la capa, así como de las tensiones residuales de compresión que se originan en el proceso de cementación y nitruración.

El medio puede afectar el comportamiento a fatiga de los materiales. Hay dos tipos de fatiga por el medio: fatiga térmica y fatiga con corrosión.

La fatiga térmica se induce normalmente a temperaturas elevadas debido a tensiones térmicas fluctuantes; no es necesario que estén presentes tensiones mecánicas de origen externo. La causa de estas tensiones térmicas es la restricción a la dilatación y o contracción que normalmente ocurren en piezas estructurales sometidas a variaciones de temperatura. La magnitud de la tensión térmica resultante debido a un cambio de temperatura depende del coeficiente de dilatación térmica y del módulo de elasticidad. Se rige por la siguiente expresión:

La fatiga con corrosión ocurre por acción de una tensión cíclica y ataque químico simultáneo. Lógicamente los medios corrosivos tienen una influencia negativa y reducen la vida a fatiga, incluso la atmósfera normal afecta a algunos materiales. A consecuencia pueden producirse pequeñas fisuras o picaduras que se comportarán como concentradoras de tensiones originando grietas. La de propagación también aumenta en el medio corrosivo puesto que el medio corrosivo también corroerá el interior de la grieta produciendo nuevos concentradores de tensión.






</doc>
<doc id="11595" url="https://es.wikipedia.org/wiki?curid=11595" title="Grafito">
Grafito

El grafito es una de las formas polimórficas en las que se puede presentar el carbono en la naturaleza. Otras formas son diamante, la chaoita y la lonsdaleita. A presión atmosférica y temperatura ambiente el polimorfo más estable es el grafito. Sin embargo, la transformación del diamante en grafito es tan extremadamente lenta que solo es apreciable a escala geológica. 

Fue nombrado por Abraham Gottlob Werner en el año 1789. El término grafito deriva del griego γραφειν (graphein) que significa "escribir", ya que se usa principalmente para crear la punta de los lápices. Antiguamente recibía otros muchos nombres, como plombagina y "molibundasna" (que no debe confundirse con el mineral molibdeno).

Aunque puede producirse artificialmente, a partir de carbono amorfo, existen muchos yacimientos naturales de este mineral. Se encuentra en rocas metamórficas, en las que los procesos asociados con su formación han transformado el carbono presente en la materia orgánica que contenía la roca original. A escala industrial, el principal productor mundial de grafito es China, seguida de India y Brasil.

El grafito cristaliza en el sistema hexagonal. En el grafito los átomos de carbono presentan hibridación sp, esto significa que forma tres enlaces covalentes en el mismo plano a un ángulo de 120° (estructura hexagonal) y que un orbital "Π" perpendicular a ese plano quede libre (estos orbitales deslocalizados son fundamentales para definir el comportamiento eléctrico del grafito). El enlace covalente entre los átomos de una capa es extremadamente fuerte. Sin embargo, las uniones entre las diferentes capas se realizan por fuerzas de Van der Waals e interacciones entre los orbitales "Π", y son mucho más débiles.Se podría decir que el grafito está constituido por láminas de grafeno superpuestas. Esta estructura laminar hace que el grafito sea un material marcadamente anisótropo.

El grafito es de color negro y gris con brillo metálico, refractario y se exfolia con mucha facilidad. En la dirección perpendicular a las capas presenta una conductividad de la electricidad baja, que aumenta con la temperatura, comportándose pues como un semiconductor. A lo largo de las capas la conductividad es mayor y aumenta proporcionalmente a la temperatura, comportándose como un conductor semimetálico. Aunque tanto el grafito como el diamante están formados exclusivamente por átomos de carbono, el grafito es muy blando y opaco, mientras que el diamante es el mineral más duro según la escala de Mohs y además deja pasar la luz a través de sí. Estas marcadas diferencias físicas se deben exclusivamente a las diferentes redes cristalinas o retículos sobre las que se disponen los átomos de carbono en el grafito (átomos de carbono en los vértices de prismas hexagonales) y en el diamante (la red cristalina está hecha de tetraedros regulares cuyos vértices son átomos de carbono).


Distintas moléculas o iones pueden penetrar en las capas del grafito. Por ejemplo, el potasio puede ceder un electrón al grafito, quedando el ion de potasio, K, entre las capas. Este electrón contribuye a aumentar la conductividad que presentaba el carbono.

Se pueden preparar diferentes compuestos de intercalación con distintas estequiometrías y distintas especies. En algunos casos la conductividad resultante es mayor, como en el caso del potasio, y es lo que ocurre generalmente, pero en otros, como por ejemplo con flúor, es en menor medida.

Existen otras formas llamadas de "carbón amorfo" que tienen una estructura relacionada con la del grafito: 



</doc>
<doc id="11596" url="https://es.wikipedia.org/wiki?curid=11596" title="Quilate">
Quilate

El quilate es un término que se utiliza para cuantificar la masa de gemas y perlas, y también el grado de pureza de los metales preciosos.


El término proviene de la antigua palabra griega "keration" (κεράτιον), que significa algarroba ("Ceratonia siliqua"), porque las semillas de este fruto eran utilizadas en la antigüedad para pesar gemas y joyas debido al tamaño y peso notoriamente uniformes de las semillas. Cuando los árabes adoptaron esta unidad de masa el nombre se deformó a "quirat" y esta se deformó a "quilate" al saltar al español.

En el año 309 d.C. el Emperador Romano Constantino I ordenó acuñar el primer "solidus", una moneda de 24 quilates (masa) de oro (unos 4,5 gramos), integrada en el sistema duodecimal romano de pesos y medidas como de libra, siendo cada quilate o "siliqua" de libra. El "solidus" (de donde procede la palabra castellana "sueldo"), y su equivalente árabe dinar (del latín "denario", de donde procede "dinero"), fueron la referencia del peso del oro hasta al menos el siglo XII, y su pureza se convirtió en la "ley" del oro, es decir, cuántos quilates-masa (de los 24 totales de la moneda) son oro puro: 24 quilates significa una pureza del 100%, 18 quilates significa que la aleación contiene un 75%, etc.

La ortografía «kilate» es incorrecta y no aceptada por la Real Academia Española, al no estar relacionada la palabra con el prefijo "kilo-". La abreviatura de "quilate masa" es "ct" según la grafía francesa "carat", mientras la del "quilate pureza" es "K" debido al término griego καθαρότητα ("katharótita", “pureza”).




</doc>
<doc id="11597" url="https://es.wikipedia.org/wiki?curid=11597" title="Kimberlita">
Kimberlita

La kimberlita es un tipo roca ígnea volcánica, potásica, conocida porque a veces contiene diamantes. Lleva el nombre de la ciudad de Kimberley, Sudáfrica, donde el descubrimiento de un diamante de 83,5 quilates (16,7 g) en 1871 dio lugar a una fiebre de diamantes, y con el tiempo a la excavación del Big Hole.

Existe un consenso de que esta roca se formó bajo la superficie de la Tierra con magma fundido a gran profundidad, presión y temperatura hace más de 100 millones de años, donde la forma más estable para el carbono es el diamante y no el grafito. La formación se produce a profundidades de entre 150 y 450 kilómetros en el manto, partiendo de composiciones del manto anormalmente enriquecidas. Posteriormente la kimberlita ascendió mediante erupciones rápidas y violentas, a menudo con considerable cantidad dióxido de carbono y otros componentes volátiles. La profundidad de fusión y generación hace a la kimberlita propensa a alojar diamantes como xenolitos. Las estructuras verticales por donde salió a la superficie reciben el nombre de chimeneas de kimberlita y son la fuente más importante de diamantes en la actualidad, ya sea incrustado en esta roca o en la peridotita y liberado en la superficie de la Tierra por los agentes atmosféricos.

La kimberlita recibe una gran atención aun a pesar de su relativamente pequeña cantidad. Esto se debe principalmente porque sirve para sacar a la superficie de la Tierra diamantes y granates peridotitas del manto xenolitos. Probablemente, proviene de profundidades mayores que cualquier otro tipo de roca ígnea y la composición extrema del magma refleja un bajo contenido de sílice y altos niveles de oligoelementos, hacen importante la comprensión de la petrogénesis de la kimberlita. En este sentido, el estudio de kimberlita puede proporcionar información sobre la composición del manto profundo y sobre los procesos de fusión que ocurren en, o cerca de, la interfase entre el cratón de la litosfera continental y la astenosfera subyacente del manto convectivo.

Las formaciones de kimberlita son intrusiones verticales, con forma de zanahoria, llamadas 'chimeneas'. Esta forma clásica de zanahoria se debe a un complejo proceso intrusivo del magma de la kimberlita que contiene una gran proporción de CO y HO. Esto produce a cierta profundidad una etapa de ebullición explosiva que causa una cantidad significativa de combustión vertical (Bergman, 1987). La clasificación de la kimberlita se basa en el reconocimiento de las diferentes facies de la roca. Cada facies se asocian a un estilo particular de actividad magmática, es decir, cráter, diatrema y rocas filonianas (Clemente y Skinner 1985, y Clemente, 1982).

La morfología de las chimeneas de kimberlita, y su forma clásica de zanahoria, es el resultado de un volcanismo explosivo diatrema desde muy profundas fuentes en el manto derivados. Estas explosiones volcánicas producen columnas verticales de roca que se elevan desde profundas cámaras de magma. La morfología de las chimeneas de kimberlita es variada, pero generalmente incluye un complejo de sábanas dique de tabular, sumergiendo verticalmente los diques de alimentación en la raíz de la tubería que se extiende hasta el manto. De 1,5 a 2 km a la superficie, el magma altamente presionados estalla hacia arriba y se expande para formar una diatrema de cónica a cilíndrica, que erupciona en la superficie. La expresión en la superficie no suele ser preservado, pero suele ser similar a un volcán maar.

El diámetro de una chimenea de kimberlita en la superficie es generalmente de unos pocos cientos de metros hasta un kilómetro.

Tanto la ubicación y el origen de los magmas de kimberlita son debatidos. Su enriquecimiento extremo y la geoquímica ha dado lugar a una gran cantidad de especulaciones sobre su origen, con los modelos de colocación de la fuente en el manto litosférico subcontinental (SCLM) o incluso a la profundidad de la zona de transición. El mecanismo de enriquecimiento ha sido también el tema de debate, con modelos que incluyen la fusión parcial, la asimilación de sedimentos subduccidos o derivar de una fuente de magma primario.

Históricamente, las kimberlitas se han subdividido en dos variedades distintas llamadas 'basáltica' y 'micácea ": Basándose principalmente en observaciones petrográficas (Wagner, 1914). Esta clasificación fue revisado posteriormente por Smith (1983) que rebautizó estos divisiones como Grupo I y Grupo II sobre la base de las afinidades isotópicas de las rocas utilizando sistemas de Nd, Sr y Pb. Más tarde, Mitchell (1995), propuso que estos grupos I y II kimberlitas ver diferencias tan distintas, que pueden no ser tan estrechamente relacionados como se pensaba. Demostró que las kimberlitas del Grupo II en realidad muestran una mayor afinidad con las lamproítas que con las kimberlitas del Grupo I. Por lo tanto, se reclasificó el Grupo II como kimberlitas orangeites para evitar confusiones.

El grupo I de kimberlitas son rocas ígneas potásicas, ultramáficas ricas en CO. La composición dominante es una asociación principal de minerales: olivino rico en forsterita, ilmenita de magnesio, piropo de cromo, piropo almandino, diópsido de cromo (en algunos casos subcalcic), flogopita, enstatita y cromita pobre en titanio. Las kimberlitas del grupo I presentan una textura distintiva inequigranular causada por de macrocristales (0.5 a 10 mm) a megacristales (10-200 mm) fenocristales de olivino, piropo, diópsido de cromo, ilmenita magnesiano y flogopita, en una matriz de granos finos a medios.

Esta matriz mineralogía, se asemeja más a la verdadera composición de una roca ígnea, contiene olivino rico en forsterita, piropo granate, diópsido rico en cromo, ilmenita rica en magnesio y espinela.

El grupo II de kimberlitas (o orangeites) son rocas ultrapotásicas y peralcalinos ricas en volátiles (predominantemente HO). La característica distintiva de las orangeites son macrocristales y microfenocristales de flogopita, junto con una matriz de micas que varían en composición de flogopita a "tetraferriphlogopite" (flogopita anormalmente rica en hierro). Macrocristales de olivino y cristales primarios euhedrales reabsorbidos en una matriz de olivino es común pero no esenciales.

Las características principales de la matriz son: zonas de piroxenos (núcleos de diópsido bordeada por aegirina con titáneo); minerales del grupo de la espinela (magnesiano cromita de titanífero magnetita), perovskitas ricas en tierras raras y estroncio, apatita rica en estroncio; fosfatos ricos en tierras raras (monacita, daqingshanite); minerales del grupo del Barian de potasio holandita;rutilo con Niobio y ilmenita con manganeso.

Las kimberlitas son rocas ígneas peculiares debido a que contienen una variedad de especies minerales con composiciones químicas peculiares. Estos minerales como el richterita potásica, diópsido de cromo(un piroxeno), espinelas de cromo, ilmenita magnesiano, y los granates ricos en piropo más cromo, suelen estar ausentes en la mayoría de las rocas ígneas, lo que son especialmente útiles como indicadores de kimberlitas.

Estos minerales indicadores generalmente se busca en los sedimentos aluviales actuales. Su presencia puede indicar la presencia de kimberlita dentro de la cuenca de erosión que produjo el aluvión.

Las kimberlitas son la fuente primaria más importante de diamantes. Muchas chimeneas de kimberlita también producen ricos aluviales o eluvial depósitos placer de diamantes. Solo 1 de cada 200 chimeneas de kimberlita contienen diamantes de calidad.

Los depósitos situados en Kimberley, Sudáfrica fueron los primeros y dieron nombre a la roca. Los diamantes de Kimberley se encontraron inicialmente en kimberlita meteorolizada, que es de color amarillo por la limonita, y se bautizó como tierra amarilla. Trabajos a mayor profundidad encontraron roca menos alterada, kimberlita serpentinizada, que los mineros llamaron tierra azul.

La tierra azul y amarilla fueron prolíficas productoras de diamantes. Después de agotar la tierra amarilla, los mineros de finales del siglo XIX excavaron accidentalmente en el suelo azul y encontraron diamantes de calidad gema en cantidad. La importancia económica del momento se vio afectada por la cantidad de diamantes que encontraron en poco tiempo. Los mineros, al competir unos con otros, fueron abaratando el precio de los diamantes que disminuyeron en su valor en un corto período.





</doc>
<doc id="11599" url="https://es.wikipedia.org/wiki?curid=11599" title="Reloj">
Reloj

Se denomina reloj al instrumento capaz de medir, mantener e indicar el tiempo en unidades convencionales (horas, minutos o segundos). Fundamentalmente permite conocer la hora actual, aunque puede tener otras funciones, como medir la duración de un suceso o activar una señal en cierta hora específica.

Los relojes se utilizan desde la antigüedad y a medida que ha ido evolucionando la tecnología de su fabricación han ido apareciendo nuevos modelos con mayor precisión, mejores prestaciones y presentación y menor coste de fabricación. Es uno de los instrumentos más populares, ya que prácticamente muchas personas disponen de uno o varios relojes, principalmente de pulsera, de manera que en muchos hogares puede haber varios relojes, muchos electrodomésticos los incorporan en forma de relojes digitales y en cada computadora hay un reloj.

El reloj, además de su función práctica, se ha convertido en un objeto de joyería, símbolo de distinción y valoración.

La mayor precisión conseguida hasta ahora es la del último reloj atómico desarrollado por la Oficina Nacional de Normalización (NIST) de los Estados Unidos, el NIST-F1, puesto en marcha en 1999, es tan exacto que tiene un margen de error de solo un segundo cada 45 millones de años

En la antigüedad se conocieron varias especies de relojes. Vitruvio habla del reloj de agua o clepsidra, el de aire, el de sol y de otras especies que son desconocidas.

Los egipcios medían con el gnomon los movimientos del Sol. De igual medio se valía el ilustre astrónomo para sus observaciones. Las clepsidras y los relojes de sol fueron inventados en Egipto en tiempos de los Ptolomeos; las clepsidras fueron después perfeccionadas por Escipión Nasica o según otros por Ctesibio (discípulo de los oradores romanos medían con ellas la duración de sus discursos.)

Se cree que los grandes relojes de pesas y ruedas fueron inventados en Occidente por el monje benedictino Gerberto (papa, con el nombre de Silvestre II, hacia finales del siglo X) aunque ya con alguna anterioridad se conocían en el Imperio bizantino.
Dante, en La divina comedia, canto X de El paraiso, antes del año 1321dc, cuenta acerca de relojes mecánicos con función alarma, "cuyas ruedas se mueven unas a otras, y apresuran a la que va delante hasta que se oye "tin tin" con notas tan dulces", como algo normal. 

Según otras fuentes, el primer reloj de que habla la historia construido sobre principios de mecánica es el de Richard de Wallingford, abad de San Albano, que vivió en Inglaterra hacia 1326, pues al parecer la invención de Gerberto (después Silvestre II) no era más que un reloj de sol. El segundo es el que Santiago Dondis mandó construir en Padua hacia 1344 y en el cual según refieren se veía el curso del sol y de los planetas. El tercero fue el que había en el Louvre de París, mandado traer de Alemania por el rey Carlos V de Francia. El antepasado directo de estos instrumentos podría ser el complejo mecanismo de Anticitera, datado entre 150 a. C. y 100 a. C.
En España, la noticia más antigua de la instalación de un reloj de torre data de 1378, cuando se recogen en un documento las condiciones establecidas entre el cabildo de la catedral de Valencia y Juan Alemany, maestro de relojes procedente de Alemania, para realizar un reloj de esfera grande para ubicarlo en el antiguo campanario. Dentro de los relojes mecánicos considerados los más antiguos del país se localiza el reloj «seny de les hores» que fue instalado en la catedral de Barcelona en 1393; el del campanario de la iglesia de San Miguel de la villa de Cuéllar (Segovia) que fue arreglado en el año 1395 y finalmente en la catedral de Sevilla otro en 1396, cuya inauguración tuvo lugar el 22 de julio de 1400 en presencia del rey Enrique III de Castilla.
El primero que imaginó construir relojes de bolsillo fue Pedro Bell de Núremberg; su aspecto les valió el nombre de «huevos de Núremberg». En 1647, Christiaan Huygens aplicó a los relojes de torre o de pared el péndulo, cuyo descubrimiento se debe a Galileo. El mismo físico aplicó en 1665 el muelle de espiral a los relojes de bolsillo. En 1647, el ginebrino Gruet, residente en Londres, aplicó al reloj la cadenilla de acero que sirve para transmitir el movimiento del tambor al cono, sustituyendo a las cuerdas de vihuela empleadas hasta entonces. Dos años después se inventaron los relojes de repetición.

Hay una gran variedad de tipos diferentes de relojes. Actualmente los relojes personales son en su mayoría mecánicos y electrónicos, ya sean analógicos o digitales, funcionan con una pequeña pila eléctrica que mediante impulsos hace girar las agujas (relojes analógicos) o marca los números (relojes digitales).

Existen gran cantidad de relojes mecánicos para uso personal (de pulsera o de bolsillo) o general (relojes de pared y antesala). 
Los relojes mecánicos se estiman y valoran más que los electrónicos a pesar de su menor exactitud y mayor precio; ya que son considerados por los expertos como obras de arte mecánicas.

Hoy en día existen una gran cantidad de compañías relojeras, fabricantes de relojes mecánicos, tanto personales como fijos, países como Alemania, Suiza, Japón, China, Reino Unido, Estados Unidos y Rusia, albergan importantes compañías del sector.
En el formato analógico existe una escala fija y dos agujas que giran a velocidad constante; la aguja más corta y ancha indica las horas, y tarda doce horas en completar una vuelta completa, la aguja más delgada y larga, el minutero, indica los minutos y tarda una hora en completar una vuelta completa a la esfera del reloj. Puede existir una tercera aguja en el mismo eje o con un eje distinto que señala los segundos y tarda un minuto en dar una vuelta completa.

En los relojes digitales, hay dos grupos de dos dígitos cada uno, separados por el signo de dos puntos (:), los dos primeros indican la hora en formato de 24 horas de 0 a 23 o en formato de 12 horas de 1 a 12; el segundo grupo de dígitos indica los minutos en un rango de 0 a 59, en algunos casos puede existir un tercer grupo de dos dígitos que indica los segundos en un rango de 0 a 59 segundos.

Al principio, sólo los llevaban las mujeres, hasta la Primera Guerra Mundial (1914-1918), en que se hicieron populares entre los hombres de las trincheras.

Los relojes de pulsera vienen todos con dos correas ajustables que se colocan en alguna de las muñecas para su lectura. Son de tipo analógico y digital. Aunque la carátula de la mayoría de ellos es generalmente redonda, también existen de carátula cuadrada, hexagonal y hasta pentagonales.

En los relojes analógicos (de variable continua) la hora se indica en la carátula mediante dos o tres manecillas: una corta para la hora, una larga para los minutos y, opcionalmente, una tercera manecilla también larga que marca los segundos. En los relojes digitales (de variable discreta) se lee la hora directamente en números sobre la pantalla. También existen relojes mixtos, es decir, analógicos y digitales en la misma carátula.

Los relojes calendarios son relojes mecánicos o digitales que marcan el año en vigor, el mes, el día de la semana, la hora, los minutos e incluso los segundos.

Los cronógrafos son relojes muy precisos (normalmente hasta las milésimas de segundo) utilizados para medir intervalos de tiempo, por ejemplo en pruebas deportivas o en experimentos científicos.

En alta relojería se refiere a instrumentos de precisión certificados por el COSC (control oficial suizo de cronometría).

Antes de inventarse los relojes personales de pulsera y de bolsillo se inventaron relojes muy grandes de mecanismos complicados y pesados que se colocaban en lo alto de las torres y campanarios de los pueblos y ciudades para que los ciudadanos tuviesen conocimiento de la hora del día. A estos relojes se les conectaba a una campana grande y sonora y es la que iba indicando con un toque peculiar las horas y cuartos de hora cuando se iban cumpliendo. A lo largo de los años hay relojes de este tipo que se han hecho muy famosos, como el "Gran Reloj de Westminster" situado en la Torre de Isabel del palacio del Parlamento británico o el situado en la Puerta del Sol de Madrid.

Los relojes han figurado durante siglos como piezas importantes en el amueblamiento de salones, para lo cual se construían con diversas formas decorativas. Prescindiendo del reloj de arena, que viene usándose desde las civilizaciones griega y romana para medir lapsos cortos y prefijados, los relojes fueron usados en cantidad muy pequeña hasta finales del siglo XIII o mediados del siglo XIV, época en la cual se inventó el motor de resorte o muelle real, difundiéndose el uso del reloj-mueble en el siglo XVI.

De esta época se conservan algunos ejemplares muy curiosos en los Museos del Louvre, Berlín y Viena, que tienen la forma exterior de un edificio coronado con una pequeña cúpula donde se halla el timbre o campana de las horas.

Los relojes de bolsillo se inventaron en Francia a mediados del siglo XV, poco después de aplicarse a la relojería el muelle espiral. Al principio tenían forma cilíndrica, variando mucho y con raros caprichos, y desde el comienzo del siglo XVI se construyeron en Núremberg con profusión y en forma ovoidea, de donde deriva el nombre de "huevos de Núremberg", creyéndose inventados en esta ciudad alemana e italiana.

El reloj nuclear podría ser útil para algunas comunicaciones confidenciales y para el estudio de teorías fundamentales de la física. Asimismo podría añadir precisión al sistema de posicionamiento global (GPS por su sigla en inglés), que se sustenta ahora en relojes atómicos. La precisión extrema de este reloj, cien veces superior a la de los actuales relojes atómicos, proviene del núcleo de un solo ion de torio.

Otros tipos de relojes según su forma o empleo son:

El reloj con esfera tradicional suele contar con manecillas para la hora, minutero (para los minutos) y segundero (para los segundos) y el horario (para la hora). Además, puede contar adicionalmente con despertador o calendario.

Un reloj electrónico es un reloj en el que la base de tiempos es electrónica o electromecánica, al igual que la división de frecuencia. La exactitud del reloj depende de la base de tiempos, que puede consistir en un oscilador o en un adaptador que, a partir de una referencia, genera una señal periódica.

El divisor de frecuencia es un circuito digital formado por una sucesión de contadores hasta obtener una frecuencia de 1 Hz, que permite mostrar segundos. Si se quiere mostrar décimas, la división se detiene al llegar a los 10 Hz. Esta frecuencia pasa al módulo de presentación, que puede ser de carácter electrónico o mecánico, donde otros divisores van separando los segundos, minutos y horas para presentarlas mediante algún tipo de pantalla.

Los relojes mecánicos carecen en la mayoría de los casos de componentes electrónicos; este tipo de relojes cuentan con un sistema mecánico fabricado generalmente en metal, en donde la fuerza motriz necesaria para poner en marcha la maquinaria es proporcionada por un muelle motor o por medio de pesas conectadas por cadenas o cables.

En la cultura popular es común referirse a la carga del muelle motor como «dar cuerda», no obstante este término es erróneo, y solo es aplicable a los relojes de pesas, en donde literalmente se le da cuerda a un cilindro dentro del reloj para que de esa manera continúe el descenso de la pesa que da vida al mismo. Dentro de un muelle motor se encuentra una banda o cinta de acero templado que, al enrollarse, genera una fuerza de torsión usada por el reloj para mover el mecanismo, bien sea la marcha o la sonería. Por medio de un tren de engranajes se reduce la fuerza y aumenta la velocidad, finalizando en una rueda dentada de manera especial, llamada rueda de escape, la cual conecta con una pieza llamada ancora. Esta pieza es la encargada de convertir el movimiento rotatorio de los engranajes en un desplazamiento lateral de izquierda a derecha que se trasmite a un volante o a un péndulo para proveerles la energía suficiente para oscilar. Es el contacto entre estas dos piezas, rueda de escape y ancora el que produce el famoso "tic-tac". Finalmente, el péndulo o el volante marcan el paso del tiempo y se les conoce con el nombre de órgano regulador. El reloj usa sus oscilaciones o alternancias constantes para determinar el paso del tiempo: cuanto más preciso sea el mecanismo, menos variaciones habrá en la periodicidad de las oscilaciones.

Cabe resaltar que, aunque los relojes de pulsera, que usan volantes como órgano regulador, han logrado niveles de exactitud sorprendentes; el péndulo y su oscilación periódica regular continúan siendo el patrón de medición del tiempo u órgano regulador más exacto en los relojes mecánicos.

Normalmente el número de engranajes o ruedas que posee un reloj mecánico es consecuencia directa del tiempo estimado en el que el muelle o la pesa le proveerá energía suficiente para funcionar; así, si un reloj mecánico, por ejemplo un despertador, está construido para almacenar 24 horas de marcha, el número de ruedas será generalmente de cinco, desde el engranaje del muelle hasta la rueda de escape; por otro lado, si se trata de un reloj de pared, en donde la reserva de marcha está diseñada para durar 192 horas (ocho días), entonces se añadirá una rueda extra justo después del muelle motor para de esta forma aumentar la velocidad del mecanismo de escape en relación a la velocidad de rotación del muelle motor, expandiendo así la autonomía de funcionamiento del mecanismo, aunque en estos casos se requiere de muelles más poderosos, para compensar la pérdida de fuerza causada por el aumento en la relación de los engranajes; finalmente, la hora se muestra siempre en formato analógico, por medio de manecillas, que usan el giro de los engranajes internos, usualmente la rueda primera para los relojes de 1 día, y la rueda segunda para los de 8 días, para convertir el movimiento del tren de engranajes, controlado por el sistema de escape, en indicaciones comprensibles para las personas, quienes realizan la lectura de la hora fijándose en la posición de las manecillas frente a una escala horaria fija en el frente del reloj.

Cabe resaltar, que el minutero en el reloj mecánico, a diferencia del horario, no posee un tren de engranajes independiente que ajuste la relación para marcar la hora, este se encuentra fijo a la rueda que usualmente engrana con el muelle motor, dicha rueda posee un eje que sobresale, en frente de la maquinaria, y que es de hecho el eje conocido como “cañón”, donde se conecta el minutero, por lo tanto esta rueda gira una vez cada 60 minutos exactamente, el cañón horario realiza una reducción de velocidad, usando un pequeño tren de engranajes ubicado en la parte frontal del reloj justo entre el minutero y el horario, la relación entre ambos seria entonces de 1/12, en donde por cada vuelta de la manecilla horaria, la minutera ha debido girar 12 veces, este mecanismo también se encuentra en todos los relojes electrónicos con lectura analógica.

El tipo de base de tiempos utilizada es tan importante que suele dar nombre al tipo de reloj. Las más habituales son:

El reloj mecánico se basa en un pulsador que puede ser de 1 Hz o submúltiplo. Por lo general este pulsador era un mecanismo de escape mecánico en el cual la energía almacenada en un muelle era liberada de manera constante y lenta. El sonido de tic-tac del reloj corresponde a este sistema de escape que es el responsable de generar la base de tiempo del reloj y brinda movimiento al segundero; tanto el minutero como el horario son movidos mediante trenes de engranajes que transforman la relación del segundero en 1/60 para el minutero y de este 1/60 para el horario( ver imagen).

Un reloj digital consta de un oscilador, generalmente de cuarzo el cual mediante divisor de frecuencia, a similitud de los trenes de engranajes, genera las señales de 1 Hz, 1/60 Hz y 1/3600 Hz para el segundero, minutero y horario respectivamente. En este caso los distintos pulsos eléctricos pasan a 3 contadores en cascada que se corresponden en la pantalla a los segundos, minutos y horas respectivamente. Estos contadores están acoplados para permitir la secuencia necesaria de conteo y de señalización entre un contador y otro, a saber 0 al 59 para los segundos y los minutos y 0 a 24 o 1 a 12 para las horas, según el diseño particular o la configuración en modelos que permiten ambas.



</doc>
<doc id="11600" url="https://es.wikipedia.org/wiki?curid=11600" title="Instrumento">
Instrumento

El término instrumento puede referirse a:




</doc>
<doc id="11602" url="https://es.wikipedia.org/wiki?curid=11602" title="Mecanismo (desambiguación)">
Mecanismo (desambiguación)

El término mecanismo puede aplicarse en los siguientes contextos:


</doc>
<doc id="11603" url="https://es.wikipedia.org/wiki?curid=11603" title="Máquina hidráulica">
Máquina hidráulica

Una máquina hidráulica es una variedad de máquina de fluidos que para su funcionamiento se vale de las propiedades de un fluido incompresible o que se comporta como tal, debido a que su densidad en el interior del sistema no sufre variaciones importantes.

Convencionalmente se especifica para los gases un límite de 100 mbar para el cambio de presión; de modo que si este es inferior, la máquina puede considerarse hidráulica. Dentro de las máquinas hidráulicas el fluido experimenta un proceso adiabático, es decir no existe intercambio de calor con el entorno.

Las máquinas hidráulicas pueden clasificarse atendiendo a diferentes criterios.

En los motores hidráulicos, la energía del fluido que atraviesa la máquina disminuye, obteniéndose energía mecánica, mientras que en el caso de generadores hidráulicos, el proceso es el inverso, de modo que el fluido incrementa su energía al atravesar la máquina.

Atendiendo al tipo de energía fluidodinámica que se intercambia a través de la máquina, se tienen:




Teniendo en cuenta el modo en el que se intercambia la energía dentro de la máquina su clasificación puede ser así:


Atendiendo a la presencia o no de carcasa:



Existen otros criterios, como la división en rotativas y alternativas, dependiendo de si el órgano intercambiador de energía tiene un movimiento rotativo o alternativo, esta clasificación es muy intuitiva pero no atiende al principio básico de funcionamiento de estas máquinas.

En la siguiente tabla se muestra un resumen de la clasificación de las máquinas hidráulicas (l=líquido, g=gas).




</doc>
<doc id="11604" url="https://es.wikipedia.org/wiki?curid=11604" title="Máquina de fluido">
Máquina de fluido

Se denominan máquinas de fluido aquellas que tienen como función principal intercambiar energía con un fluido que las atraviesa. Este intercambio implica directamente una transformación de energía. 

Las máquinas de fluido se suelen clasificar según varios principios. Las tres clasificaciones presentadas a continuación son complementarias de modo que, por ejemplo, un ventilador es una turbomáquina hidráulica generadora, mientras que un motor de explosión es un motor térmico alternativo (de desplazamiento positivo).



El estudio de los intercambios de energía en las máquinas térmicas es objeto de la termodinámica. Las máquinas de fluido también se clasifican atendiendo a dos criterios, la cantidad de fluido y el movimiento de la máquina.



Si en el proceso el fluido incrementa su energía, la máquina se denomina "generadora" (compresores, bombas), mientras que si la disminuye, la máquina se denomina "motora" (turbinas, motores de explosión).



</doc>
<doc id="11605" url="https://es.wikipedia.org/wiki?curid=11605" title="Máquina térmica">
Máquina térmica

Una máquina térmica es un conjunto de elementos mecánicos que permite intercambiar
energía, generalmente a través de un eje, mediante la variación de energía de un fluido que varía su densidad significativamente al atravesar la máquina. Se trata de una máquina de fluido en la que varía el volumen específico del fluido en tal magnitud que los efectos mecánicos y los efectos térmicos son interdependientes.

En una máquina térmica, la compresibilidad del fluido no es despreciable y es necesario considerar su influencia en la transformación de energía.

En un principio se podría definir a una máquina térmica como un dispositivo, equipo o una instalación destinada a la producción de trabajo en virtud de un aporte calórico.
Aunque en algunas definiciones se identifican como sinónimos los términos «máquina térmica motora» y «motor térmico», en otras se diferencian ambos conceptos. Al diferenciarlos, se considera que un motor térmico es un conjunto de elementos mecánicos que permite obtener energía mecánica a partir de la energía térmica obtenida mediante una reacción de combustión o una reacción nuclear. Un motor térmico dispone de lo necesario para obtener energía térmica, mientras que una máquina térmica motora necesita energía térmica para funcionar, mediante un fluido que dispone de más energía a la entrada que a la salida.

Las máquinas térmicas pueden clasificarse, según el sentido de transferencia de energía, en:

Atendiendo al principio de funcionamiento, las máquinas térmicas se clasifican en:

Teniendo en cuenta lo anterior, podemos clasificar las máquinas térmicas tal como se recoge en el cuadro siguiente.

Un sistema abierto es aquel que intercambia materia y energía con el entorno. Aplicando el primer principio de la termodinámica para un sistema abierto, el incremento de energía del sistema en un intervalo de tiempo se expresa en:
donde;

Haciendo la derivada de la expresión anterior respecto al tiempo, se obtiene:

Debe tenerse en cuenta que en máquinas generadoras, puede aparecer esta expresión con el signo de "W" cambiado, para que se exprese el trabajo entregado por la máquina y así "W" sea positivo.

La ecuación que expresa el balance de energía puede simplificarse en los siguientes casos:
Cuando el sistema está en reposo, tal como en máquinas estacionarias, las variaciones de energía potencial y energía cinética serán nulas.
Cuando la máquina funciona en régimen permanente, las cantidades de masa y energía que entran son iguales a las que salen, pues de lo contrario variarían esa cantidades dentro del sistema.
En la mayoría de las máquinas térmicas, diferencia de energía potencial del flujo que sale respecto al que entra es poco significativo en comparación con los otros términos asociados a la energía del flujo.
En la mayoría de las máquinas térmicas, la transferencia de calor es despreciable frente a otros intercambios de energía. Teniendo en cuenta la transmisión de calor por conducción y convección:
donde "Q" es el calor intercambiado, "U" es el coeficiente global de transferencia de calor, "A" es la superficie del sistema y formula_6 es la diferencia de temperaturas media logarítmica, puede considerarse que el sistema es adiabático cuando se da alguna de las siguientes condiciones:

En una máquina térmica que funciona en régimen permanente en la cual se desprecie la variación de energía potencial, la expresión el primer principio de la termodinámica puede expresarse como
donde h es la entalpía de parada.

En los ciclos termodinámicos asociados a la turbina de vapor, la energía cinética específica puede considerarse despreciable frente a la entalpía, resultando 

Para el cálculo del rendimiento, se relaciona la energía obtenida, ya sea en forma de incremento de energía en el fluido o de energía mecánica suministrada por la máquina, entre la máxima energía que se podría obtener en las condiciones de contorno. 

El trabajo específico máximo que puede obtenerse en la expansión de un fluido está definido por la diferencia de entalpías entre el fluido a la entrada y las condiciones isoentrópicas a la presión de salida. En cambio el trabajo real es menor a éste debido al aumento de la entropía.

El rendimiento mecánico es la relación entre potencia efectiva ((formula_8), que es la potencia obtenida en el eje, y la potencia interna ((formula_9), que es la variación por unidad de tiempo de la energía del fluido. La potencia efectiva resulta de restar a la potencia indicada menos la potencia de pérdidas mecánicas (formula_10), que es disipada el rozamiento de elementos mecánicos (cojinetes, retenes, etc.) y en el accionamiento de elementos auxiliares (bomba de aceite, ventiladores, etc.)

El rendimiento isoentrópico relaciona la potencia obtenida en el eje con potencia máxima del proceso isoentrópico en las mismas condiciones de contorno.

El trabajo específico mínimo para comprimir un fluido desde un estado térmico hasta una presión determinada es igual al salto entálpico del correspondiente proceso isoentrópico, de forma que un proceso real presentará mayor diferencia de entalpías del fluido entre la entrada y la salida.

El rendimiento mecánico es la relación entre potencia efectiva ((formula_8), que es la potencia obtenida en el eje, y la potencia interna ((formula_9), que es la variación por unidad de tiempo de la energía del fluido. La potencia efectiva resulta de restar a la potencia indicada menos la potencia de pérdidas mecánicas (formula_10), que es disipada el rozamiento de elementos mecánicos (cojinetes, retenes, etc.) y en el accionamiento de elementos auxiliares (bomba de aceite, ventiladores, etc.)

El rendimiento isoentrópico relaciona potencia mínima del proceso isoentrópico en las mismas condiciones de contorno con la potencia suministrada en el eje.
La fuerza de la energía térmica varía con el material que se utiliza para emitir la energía térmica



</doc>
<doc id="11606" url="https://es.wikipedia.org/wiki?curid=11606" title="Reloj de sol">
Reloj de sol

El reloj de sol o reloj solar es un instrumento usado desde tiempos muy remotos con el fin de medir el paso de las horas, minutos y segundos (tiempo). En castellano se le denomina también cuadrante solar. Emplea la sombra arrojada por un gnomon o "estilo" sobre una superficie con una escala para indicar la posición del Sol en el movimiento diurno. Según la disposición del gnomon y la forma de la escala se pueden medir diferentes tipos de tiempo; el más habitual es el tiempo solar aparente (el reloj solar del rey Acab).

Los conocimientos astronómicos de los egipcios les permitieron orientar la pirámide de Keops, c. 2550 a. C. mediante referencias estelares. Mil años después, en la época del faraón Tutmosis III (c. 1500 a. C.), se diseña un instrumento denominado "sechat"; se trata de un pequeño reloj solar para medir el tiempo mediante la longitud de las sombras, que constaba de dos piezas prismáticas, pétreas, de unos tres decímetros de longitud, situadas perpendicularmente, donde una tenía marcadas las horas y otra servía de "aguja". Debió ser un instrumento muy popular entre los sacerdotes egipcios, pues, por sus dimensiones, cabe que fuese un instrumento portátil.

Hacia 2400 a. C. los escribas sumerios ya utilizaban un calendario: dividieron el año en 12 partes, también dividieron el día, y lo hicieron siguiendo el mismo patrón de divisiones. Su año constaba de 12 meses y cada uno de ellos de 30 días. Sus días constaban de doce "danna" (cada "danna" duraría dos de nuestras horas) de 30 "ges" cada uno (cada "ges" duraría 4 de nuestros minutos).

La mayoría de los instrumentos empleados en la Antigüedad no eran portátiles. En Mesopotamia encontramos los zigurats, que eran construcciones con peldaños en los que se podían visualizar las horas mediante el conteo de los peldaños que estaban oscurecidos por la sombra de sus propios bordes. La primera referencia literaria conocida a un reloj de sol es el famoso Cuadrante de Achaz hacia el siglo VII a. C.

La percepción acerca del tiempo de la sociedad griega del siglo V a. C. resulta patente de la lectura de varios escritores griegos y romanos de la época que describen, y dan referencias, de instrumentos identificados como los primeros relojes de sol. El autor griego más antiguo, y tal vez más importante, ha sido Heródoto de Halicarnaso (484-426 a. C.), que hace una pequeña reseña en su "Historia" (II, 109, 3) de los conocimientos griegos del tiempo, diciendo que adquirieron de los babilonios la división del día en doce partes.

Por lo tanto,el sistema horario de los griegos era temporal: con ello se quiere decir que la hora se entendía como la doceava parte del arco diurno recorrido por el Sol, pero como tal arco varía durante el año, la hora también varía.
Por esta razón, a este sistema se le denomina también de horas desiguales. Los romanos, a su vez, heredaron este sistema de división del día de los griegos.

Plinio el Viejo (ca. 23-79) en su "Historia Natural" (Libro XXXVI, Capítulo XIV) relata la historia del reloj que el emperador Augusto hizo construir en el Campo de Marte, aprovechando un obelisco egipcio del faraón Psamético II, el denominado Reloj Solar de Augusto.

A finales del siglo I a. C. y reinando ya en Roma el emperador Augusto, un ingeniero militar llamado Marco Vitruvio Polión escribió el único tratado sobre arquitectura que, de la antigüedad, haya llegado hasta nosotros. Se sabe que fue arquitecto en Roma, donde construyó y dirigió diversas obras, entre ellas la Basílica de Fanum. El tratado está dividido en diez libros y se titula "De Architectura". Los primeros siete libros tratan de arquitectura, el octavo de construcciones hidráulicas, con especial aplicación a los métodos para alumbrar y conducir el agua, el noveno trata de la gnomónica y el décimo de la maquinaria. En el Libro IX, Capítulos VIII-IX describe un método geométrico para diseñar relojes de sol denominado analema. El autor no se atribuye la invención de este método, sino que lo asigna a los que él denomina como sus maestros.

En los primeros siglos de la era cristiana, la gnomónica, débilmente iluminada por los estudios de la astronomía helénica, entra en una decadencia que caracteriza a toda la ciencia de la Europa cultural y económica del medioevo. Son pocos los elementos (sobre todo arqueológicos) que podemos encontrar: apenas existen escritos que muestren nuevos avances.

Aunque en este periodo la medida del tiempo interesaba poco a la población general, tampoco existen descripciones científicas precisas. No obstante, como rarezas de la época, se encuentran los agrimensores Beda el Venerable e Higinio Gromático (siglo II).

Paladio en el siglo IV escribe una obra denominada "Re Agrícola" compuesta de catorce libros, divididos de tal forma que cada libro corresponde a las tareas agrícolas típicas de cada mes. Al final de cada libro pone una especie de tabla que denomina "horologium" típico del mes en cuestión. En dicho "horologium" indica la longitud de las sombras en pies para cada hora durante los días del mes en cuestión. Indica así el uso que se hacía del cuerpo humano para substituir a los relojes de sol. En gnomónica se les denomina reloj de pie.

En el siglo VII tomaron relevancia las órdenes benedictinas. En el año 529, el fundador de esta orden religiosa, san Benito, prescribe desde su monasterio unas "Reglas" precisas por las que todos los monjes benedictinos de Europa deben regirse. Ya desde sus orígenes, la Iglesia católica quiso santificar determinadas horas del día con una oración común. San Benito denominó a estas horas de rezo "horas canónicas", y así se haría desde el siglo VI. El nombre proviene de las normas o cánones proporcionados por la Iglesia.

La gnomónica de estos siglos derivó a la construcción de relojes de misa o relojes de horas canónicas, en ellos se indicaban las horas de rezo. Estos relojes se encuentran ubicados generalmente en las fachadas meridionales de iglesias o monasterios. También se desarrollaron relojes personales para tal fin, como el reloj anular.

En este periodo medieval, en el que la gnomónica "oficial" era la impuesta por la Iglesia Católica, mediante el uso de las horas canónicas, existieron autores innovadores como Cayo Julio Solino que en el siglo IV escribió un libro titulado "Tractatus de umbra et luce" (‘Tratado de la sombra y la luz’) que mantiene el enlace de conocimiento de la cultura grecolatina. Existe también otro oscuro autor del siglo VI, Antemio, al que se le atribuye el códice titulado "Problema Sciatericum".

Ya a comienzos del siglo I los estudios realizados acerca de las obras Vitruvio y Ptolomeo permiten reconocer por primera vez que hay dos parámetros importantes para el diseño de un reloj de sol:


En el siglo IX entra en escena la astronomía árabe. El califato de Al-Mamún marca el comienzo de una intensa actividad cultural que continuaría en siglos sucesivos con autores como Averroes, Thábit Ibn Qurra (826-901), Costa Ebn Luca, Abulphetano, Hazemio, Al-Biruni (973-1048). Mientras la Europa cristiana de la época seguía la obra del venerable Beda, los árabes tenían una actividad intelectual muy agitada continuada a partir de la destrucción de la Biblioteca de Alejandría. Es sólo a partir del siglo X cuando en Europa se empieza a ver tímidamente la inmensa labor recopilatoria del conocimiento antiguo realizada por los árabes.

Los relojes árabes de esta época medieval eran todos planos, por lo menos en su gran mayoría, y se denominaban "al-basit" (‘superficie plana’); estaban construidos en mármol (Ruchâmet), o en placas de cobre. Todos ellos eran sin inclusiones de elementos esféricos, y con indicación de la dirección del santuario de la Kaaba en La Meca, debido al precepto religioso de rezar con el rostro dirigido a ese lugar independientemente del lugar en el que se hallara ubicado. Tal dirección se denomina Al Qibla. Todos ellos con curiosas curvas para los rezos cotidianos.

En el año 1000 en España se emplea por primera vez el "Quadrans vetus cum cursorem" del que se desconoce el inventor. Pero este cuadrante será la primera avanzadilla de los instrumentos de navegación que empleará Cristóbal Colón.

Fue Ermanno Contratto (1013-1054), matemático alemán conocedor del idioma árabe, el que escribe el primer tratado sobre el astrolabio cerca del año 1026, conservando algunas de las terminologías árabes. En este libro "De mensura astrolabii líber" se encuentran algunas indicaciones para realizar el reloj de pastor. En el terreno de la gnomónica la traducción de dos códices árabes fue el punto de traspaso cultural más importante.

En España, el rey de Castilla y León Alfonso X apodado ""el Sabio"" (1224-1284) reúne en la ciudad de Toledo un numeroso grupo de astrónomos cristianos, griegos, hebreos y árabes. Con esta mezcla de sabios pudo traducir al latín gran parte de las obras escritas en árabe. De esta manera se abrirá aún más la puerta del saber árabe de los siglos anteriores a Europa. Ni que decir tiene que este fenómeno permitió a la gnomónica europea salir del retraso medieval en que se hallaba inmersa. De todas formas esta absorción fue lenta.

A comienzos del siglo XIV aparecen unos instrumentos mecánicos capaces de medir regularmente el tiempo durante el día. De esta forma en el año 1386 se coloca un reloj en la Catedral de Salisbury y en 1400, durante el reinado de Enrique III ""el Doliente"", se instala en Sevilla, en la torre de la iglesia de Santa María, el primer reloj mecánico con campanas.

Entre las obras maestras en el campo de la medición del tiempo hay que destacar joyas de arte como el Reloj solar de Schniep, artesano renacentista alemán, obtenido por Fernando Álvarez de Toledo y Pimentel (1507-1582), III Duque de Alba, en 1545.

En las colonias europeas de América también se construyeron muchos relojes de sol, algunos de los cuales todavía se conservan. En el caso de la zona Intertropical hay que construirlos con un doble disco horario, como el que se ve en la imagen: el disco que queda hacia el sur (el que aparece en la foto) se emplea durante una parte del año (de agosto a abril) y el disco del otro lado, que mira hacia el norte se usaría el resto del año, cuando el sol se encuentra entre la latitud de La Asunción (Isla de Margarita, Venezuela) y el trópico de Cáncer. Dos días al año, a fines del mes de abril y a comienzos de agosto, el sol pasa por la vertical del lugar (el cenit) y entonces, como es lógico, pueden verse las horas en ambos lados.

Existen diferentes tipos de relojes de sol; algunos de ellos son:

En este modelo, el gnomon que proyecta la sombra tiene la siguiente orientación espacial:


Para determinar la dirección del plano meridiano del lugar para colocar posteriormente el gnomon, lo mejor es determinar la meridiana del lugar, es decir la intersección de dicho plano meridiano con el plano horizontal. La meridiana coincide con la dirección SUR- NORTE. La meridiana del lugar coincide también con la sombra que produce una varilla colocada verticalmente en el momento del paso del Sol por el meridiano del lugar (en ese momento el Sol está situado hacia el sur, en el hemisferio norte, y hacia el norte en el hemisferio sur y en el punto más alto de su trayectoria diaria). Para saber a qué hora oficial ocurre dicha situación es posible recurrir a las tablas de efemérides de los observatorios oficiales.

La superficie sobre la que se proyecta la sombra es plana y perpendicular al gnomon y por tanto es paralela al ecuador.
El trazado de las líneas horarias es sencillo. En el cuadrante, se dibuja un círculo con el centro en el polo del cuadrante y se divide dicho círculo en 24 partes de 15º cada una y posteriormente se trazan los 24 radios correspondientes a la división anterior. De todos ellos, el radio que coincide con la intersección del plano meridiano del lugar con el plano del cuadrante y que se dirige hacia el horizonte es la recta horaria de las 12.00. Los diferentes radios espaciados de 15 en 15º indican las horas anteriores a las 12h cuando están al oeste de la línea de las 12 h y las horas posteriores cuando están al este de la línea de las 12 h. No es necesario trazar todos los radios, puesto que las horas anteriores a la 4 h y las posteriores a las 20.00 no son necesarias. Los radios de las 6.00 y de las 18.00 determinan la dirección este–oeste si está correctamente orientado el cuadrante.

Durante medio año, desde el inicio de la primavera hasta la finalización del verano, período durante el cual la declinación solar es positiva, la sombra del gnomon se proyecta en la cara superior del plano ecuatorial del lugar. Durante el otro medio año la sombra aparece en la cara inferior y por tanto es necesario:

El cuadrante solar horizontal se obtiene mediante la proyección ortogonal oblicua de las rectas horarias de un reloj ecuatorial sobre un plano horizontal.

Las rectas horarias del reloj ecuatorial, están uniformemente distribuidas y además el ángulo horario de cada hora ecuatorial (Hecut) aumenta de 15º en 15º a izquierda y derecha de la recta horaria de las 12 de la mañana.

La recta horaria de las 12 de la mañana está contenida en el plano meridiano del lugar. Así el ángulo horario para las 11 de la mañana sería de Hecut=15º, para las 10 h de la mañana Hecut=30º y así sucesivamente.

Además, el gnomon del reloj ecuatorial que es perpendicular al plano del reloj ecuatorial, es paralelo al eje terrestre y por tanto forma con el plano horizontal un ángulo que coincide con la latitud Φ del lugar de asentamiento del reloj ecuatorial y está contenido en el plano meridiano del lugar.

El punto P, representa el extremo de una recta horaria del reloj ecuatorial (en la figura podría ser la relativa a las 11.00). Si elegimos un sistema de coordenadas cartesiano de forma que el eje X coincida con la recta que contiene a las líneas horarias de las 18.00 y 6.00 y con sentido positivo el que resulta al ir del extremo de las 18.00 hacia el extremo de las 6.00 y como eje Y la recta que contiene a la línea de las 12.00 y con sentido positivo el que resulta de ir desde el centro O hasta el extremo de las 12.00, entonces las coordenadas del punto P serían:

R representa el radio del círculo que pasa por los extremos de las rectas horarias del reloj ecuatorial.

Coordenadas del punto P' extremo de la recta horaria correspondiente del reloj horizontal: el punto extremo P se proyecta en el punto P', cuyas coordenadas se obtiene al realizar la proyección ortogonal oblicua sobre el plano horizontal: El segmento OP1 se encuentra sobre el eje X y es paralelo al plano horizontal (ver figura2) donde se realiza la proyección ortogonal oblicua, por tanto, la proyección O'P'1 coincide con el segmento proyectado OP1.

La proyección ortogonal oblicua del segmento OP2, que se encuentra sobre el eje Y, sobre el plano horizontal es de mayor tamaño y concretamente resulta ser la hipotenusa del triángulo P'2 O' O" y consecuentemente O' P'2=R. cos Hecut)/ sen Φ. Por tanto, las coordenadas de P' serán:

El ángulo que forman las nuevas rectas horarias horizontal con la línea meridiana (es decir con la recta horaria de la 12 h), tendrá una tangente que cumple la relación:


La proyección ortogonal oblicua del gnomon coincide con O'. La dirección del gnomon debe mantenerse paralela al eje terrestre y por tanto continuará formando un ángulo Φ con la horizontal y al mismo tiempo contenido en el plano meridiano del lugar. En resumen, es una prolongación del gnomon del reloj ecuatorial.

Se trata de un reloj de sol con un cuadrante horizontal elíptico asociado a un gnomon móvil vertical a lo largo del eje menor de la elipse, estando dicho eje menor en dirección NORTE-SUR. El cuadrante se construye directamente sobre el suelo y en este caso será el propio observador el que, haciendo de gnomon móvil, se desplaza hasta unas posiciones sobre el eje menor de la elipse dependientes de la fecha, desde las cuales proyecta su sombra sobre la elipse. El punto de partida es el reloj de cuadrante ecuatorial.

De este tipo es un reloj de sol que se encuentra en el Puerto de Cotos (Madrid, España), a unos 300 metros al norte de la carretera y otro en la población de Alfambra en la provincia de Teruel. En este reloj de sol, la indicación de los días y de los meses en el suelo (donde el observador se ubica para ver la hora que indica la proyección de su propia sombra) está acompañada de los signos del Zodíaco, lo cual es motivo de confusión porque las personas no suelen identificar que se hallan ante un reloj de sol. Y en este caso, tenemos que considerar la hora legal de España, que es una hora después de la solar en invierno y dos horas después durante el verano.

Las rectas horarias de un reloj vertical no declinante se calculan a través de una proyección ortogonal oblicua de las rectas horarias de un reloj ecuatorial sobre un plano vertical.

La rectas horarias del reloj ecuatorial están uniformemente distribuidas, y además el ángulo horario de cada hora (Hecut), aumenta de 15º en 15º a izquierda y derecha de la recta horaria de las 12 de la mañana. La recta horaria de las 12 de la mañana está contenida en el plano meridiano del lugar. Así el ángulo horario para las 11 de la mañana sería de Hecut=15º, para las 10 h de la mañana Hecut=30º y así sucesivamente. Además el gnomon del reloj ecuatorial que es perpendicular al plano del reloj ecuatorial, es paralelo al eje terrestre y por tanto forma con el plano vertical un ángulo complementario a la latitud Φ del lugar de asentamiento del reloj ecuatorial, es decir, 90º- Φ y además está contenido en el plano meridiano del lugar.


El punto P, representa el extremo de una recta horaria del reloj ecuatorial (en la figura podría ser la relativa a la 11 h). Si elegimos un sistema de coordenadas cartesiano de forma que el eje X coincida con la recta que contiene a las líneas horarias de las 18h y 6h y con sentido positivo el que resulta al ir del extremo de las 18h hacia el extremo de las 6h y como eje Y la recta que contiene a la línea de las 12 h y con sentido positivo el que resulta de ir desde el centro O hasta el extremo de las 12h, entonces las coordenadas del punto P serían:

R representa el radio del círculo que pasa por los extremos de las rectas horarias del reloj ecuatorial.


El punto extremo P se proyecta en el punto P', cuyas coordenadas se obtiene al realizar la proyección ortogonal oblicua sobre el plano vertical. El segmento OP1 se encuentra sobre el eje X y es paralelo al plano vertical sobre el que se realiza la proyección ortogonal oblicua; por tanto, la proyección O'P'1 coincide con el segmento proyectado OP1.

La proyección ortogonal oblicua del segmento OP2, que se encuentra sobre el eje Y, sobre el plano vertical es de mayor tamaño y concretamente resulta ser la hipotenusa del triángulo P'2 O' O" y consecuentemente

por tanto, las coordenadas de P' serán:


El ángulo que forman las nuevas rectas horarias verticales (Hvert) con la línea de la 12 h (la línea de las 12 h es la vertical que pasa por el polo), tendrá una tangente que cumple la relación:


La proyección ortogonal oblicua del gnomon coincide con O'. La dirección del gnomon debe mantenerse paralela al eje terrestre y por tanto continuará formando un ángulo (90º-Φ) con el plano vertical y al mismo tiempo contenido en el plano meridiano del lugar. En resumen, la posición coincide con la prolongación del gnomon del reloj ecuatorial.

 El reloj de sol cilíndrico portátil, llamado "de pastor" (utilizado por los pastores de los Pirineos y los Alpes), mide la inclinación del sol, la cual varía según la latitud para un mismo instante del día y del año. Por lo tanto, cada reloj debe ser construido para una latitud determinada. En el momento del paso del sol por el meridiano local (mediodía verdadero), su altura varía respecto al horizonte según las estaciones. Como ejemplo, para un lugar ubicado a 42° de latitud (norte o sur): *Solsticio de verano: 42º sobre el horizonte + 23º 27'=65º 27' *Equinoccios: 90º - Latitud=42º sobre el horizonte *Solsticio de invierno: 42º sobre el horizonte - 23º 27'=18º 33' A lo largo del día, la altura del sol varía en función de la hora. En el ecuador terrestre: * A mediodía: para una ángulo horario (AH)=0, la inclinación del sol es de 90º - 0º=90º respecto a la horizontal del lugar. * A las 10.00: para una ángulo horario (AH)=30º, la inclinación del sol es de 90º - 30º=60º respecto a la horizontal del lugar. * A las 8.00: para una ángulo horario (AH)=60º, la inclinación del sol es de 90º - 60º=30º respecto a la horizontal del lugar. La altura del sol (HS) a una hora concreta (AH=ángulo horario) se determina en función de la posición del sol (declinación en función de la fecha=D) y de la latitud del lugar (L). :HS (en grados)= arco seno [(sen L · sen D) + (cos L · cos D · cos AH)] La proyección de la sombra del estilo del reloj de pastor indica la hora según la altura del sol en el momento de la medida. Puesto que la altura del sol varía con la fecha, hay que girar la tapa del reloj hasta que coincidan la posición del estilo con la fecha del día y orientar el cilindro hacia el sol hasta obtener un trazo de sombra vertical cuya longitud indicará la hora en la trama de curvas del cilindro. La relación entre la longitud del estilo y la altura del sol viene dada por la fórmula: :Hora = longitud del estilo (ls) * tan HS

En los relojes de sol convencionales el gnomon proyecta la sombra sobre un cuadro de referencia, el reloj negativo de sol es el que proyecta los rayos de luz a través de una hendidura.

En el gráfico de la derecha se puede observar los rayos de luz proyectados a través de los cuatro domos sobre la pared que mira al sur.



</doc>
<doc id="11607" url="https://es.wikipedia.org/wiki?curid=11607" title="Gema">
Gema

Una gema, también llamada piedra preciosa, es una roca, mineral, vidrio o producto orgánico de origen natural, que al ser cortado o pulido se puede usar en la confección de joyas u objetos artísticos. Son piedras preciosas minerales como por ejemplo rubí, espinela, alejandrita, diamante, esmeralda, zafiro, tanzanita o granate; vidrios naturales como la obsidiana; rocas como la malaquita, el lapislázuli o el ónix, productos fósiles de origen orgánico como el azabache o el ámbar y productos biogénicos como las perlas (producida por una ostra) o el coral (formado por la secreción calcárea de pequeños pólipos marinos). 

Para la Confederación Mundial de Joyería una gema es, en sentido estricto, una piedra preciosa —siempre de origen natural—, de muy alta calidad o perfección.

Algunas piedras son manufacturadas para imitar a otras gemas. Sin embargo, las gemas sintéticas no son necesariamente una imitación. Por ejemplo el diamante, el rubí, el zafiro y la esmeralda creadas en laboratorios poseen las mismas características físicas y químicas que el artículo original. Pequeños diamantes artificiales han sido manufacturados masivamente durante varios años, aunque sólo recientemente han sido creados grandes diamantes de calidad, especialmente los de colores variados y llamativos.

Una gema es evaluada principalmente por su belleza y perfección. De hecho, la apariencia es lo más importante. La belleza también debe ser duradera; si una gema es dañada de alguna manera, pierde su valor instantáneamente. Las características que hacen a una piedra hermosa son su color, un fenómeno óptico inusual, una incrustación como con un fósil, su rareza y, algunas veces, la forma peculiar del cristal.

Tradicionalmente, las gemas eran divididas en dos grandes grupos: las piedras preciosas y las piedras semipreciosas, sin más. Se consideran, hasta la actualidad,
cuatro gemas preciosas:


Y semipreciosas, como:

Sin embargo, hoy en día, las gemas son descritas y diferenciadas por los especialistas por ciertas especificaciones técnicas. Entre ellas, de qué están hechas, su composición química e incluso su color. Los diamantes, por ejemplo, son de carbono (C). En el caso de los diamantes tallados, por ejemplo, su valor dependerá de los llamados «cuatro C», por sus siglas en inglés: "carat" (quilate), "cut" (talla), "colour", (color) y "clarity" (transparencia).

Por otro lado, muchas gemas y cristales son clasificados por su forma, en distintos grupos, especies y variedades. Por ejemplo, la esmeralda es de la variedad verde; aguamarina, es de la azul, la bixbita es de la variedad roja y la morganita, rosa. Todas estas variedades son de la especie del berilo.













</doc>
<doc id="11609" url="https://es.wikipedia.org/wiki?curid=11609" title="Proyección de Peters">
Proyección de Peters

La proyección de mayo (llamada así por Arno Peters), también llamada proyección de Gall-Peters es una proyección cartográfica que fue descrita por primera vez en 1855 por James Gall, que en 1885 la dio a conocer más ampliamente mediante un artículo en el "Scottish Geographical Magazine". 

La proyección de Peters es equivalente, es decir que conserva la proporción entre las áreas de las distintas zonas de la Tierra. Esta es su principal diferencia con la proyección Mercator, la más utilizada y en la actualidad, que conserva los ángulos pero no las áreas.

La proyección de Peters trata de huir de la imagen eurocéntrica del mundo, ya que la proyección Mercator otorga gran espacio a las tierras más cercanas a los polos y hace por ello, parecer al norte de Europa, Rusia y Canadá, mucho más grandes de lo que son realmente. También, es capaz de representar las latitudes altas hasta los mismos polos norte y sur, algo imposible matemáticamente en la proyección Mercator. Las distorsiones menores se encuentran en las latitudes medias, donde vive la mayor parte de la población.

donde:

Por lo tanto la esfera se proyecta sobre un cilindro vertical, y el cilindro se estira al doble de su longitud. El factor de estiramiento en este caso es 2.

Las diversas variantes de la proyección cilíndrica equidistante difieren solamente en el ratio del eje vertical al horizontal. Esta proporción determina el paralelo estándar de la proyección, es decir, aquel en el que no hay distorsión y las distancias coinciden con la escala especificada. Los paralelos estándar de la Gall-Peters son el 45°N y el 45°S.




</doc>
<doc id="11610" url="https://es.wikipedia.org/wiki?curid=11610" title="Proyección acimutal de Lambert">
Proyección acimutal de Lambert

La proyección acimutal equivalente de Lambert conserva deliberadamente las áreas.

Es una proyección particular de esfera a disco. No debe ser confundida con la Proyección Conforme Cónica de Lambert que es muy utilizada en navegación aérea.
La proyección acimutal equivalente de Lambert no es conforme, es decir, no mantiene el valor real de los ángulos tras realizar la proyección. La escala disminuye a medida que nos acercamos al borde exterior, pero en menor medida que en la proyección ortográfica. Este sistema es muy adecuado para trazar mapas de pequeña escala.

El inventor de esta proyección es el matemático alemán Johann Heinrich Lambert que en el año 1759 publicó un libro con reflexiones diversas acerca de la proyección, el escrito se titulaba "Freye Perspective" (hubo en 1774 una segunda edición mejorada). Los escritos de perspectiva y proyección fueron ampliados en 1943 por Max Steck reuniéndolos en una obra completa.
La distancia desde el punto de tangencia sobre el mapa es proporcional a la distancia en línea recta sobre la superficie de la tierra: r(d) = c sen(d/(2R)).

Este sistema de proyección presenta como gran ventaja que las áreas representadas en los mapas no sufren deformación y son proporcionales a las formas originales, cumpliéndose la regla siguiente: «superficies iguales representan ángulos sólidos iguales».



</doc>
<doc id="11611" url="https://es.wikipedia.org/wiki?curid=11611" title="Proyección cónica múltiple">
Proyección cónica múltiple

La proyección cónica múltiple o policónica es una proyección cartográfica que consiste en utilizar como base de proyección no un cono, sino varios superpuestos. El resultado es un mapa dividido en franjas. El único meridiano que tendrá la misma escala es el central, que aparece como una línea recta. Los demás meridianos son curvas, y la escala aumenta con la distancia. También la línea del Ecuador es una línea recta, perpendicular al meridiano central. Los demás paralelos son arcos concéntricos.

Esta proyección ni es conforme ni conserva las áreas, pero en la zona central las variaciones de escala son mínimas.

La proyección fue de uso común por muchas agencias cartográficas de los Estados Unidos desde el momento de su propuesta por Ferdinand Rudolph Hassler en 1825 hasta mediados del siglo XX.

La proyección se define por:

donde:

Para evitar la división por cero, las fórmulas anteriores se extienden de manera que si formula_6 entonces formula_7 y formula_8.




</doc>
<doc id="11612" url="https://es.wikipedia.org/wiki?curid=11612" title="Proyección conforme de Lambert">
Proyección conforme de Lambert

La proyección conforme cónica de Lambert, o, más sencillamente, proyección de Lambert es una de las proyecciones cartográficas presentadas por el matemático, físico, filósofo y astrónomo mulhousiano Johann Heinrich Lambert en 1772.

En esencia, la proyección superpone un cono sobre la esfera de la Tierra, con dos paralelos de referencia secantes al globo e intersecándolo. Esto minimiza la distorsión proveniente de proyectar una superficie tridimensional a una bidimensional. La distorsión es nula a lo largo de los paralelos de referencia, y se incrementa fuera de los paralelos elegidos. Como el nombre lo indica, esta proyección es conforme.

Los pilotos utilizan estas cartas debido a que una línea recta dibujada sobre una carta cuya proyección es conforme cónica de Lambert muestra la distancia verdadera entre puntos. Sin embargo, los aviones deben volar rutas que son arcos de círculos máximos para recorrer la distancia más corta entre dos puntos de la superficie, que en una carta de Lambert aparecerá como una línea curva que debe ser calculada en forma separada para asegurar de identificar los puntos intermedios correctos en la navegación.

Sobre la base de la proyección cónica simple con dos meridianos de referencia Lambert ajustó matemáticamente la distancia ente paralelos para crear un mapa conforme. Como los meridianos son líneas rectas y los paralelos arcos de círculo concéntricos las diferentes hojas encajan perfectamente.

Las coordenadas de un sistema de referencia geodésico esférico se pueden transformar a coordenadas de la proyección cónica conforme de Lambert con las siguientes fórmulas, donde formula_1 es la longitud, formula_2 la longitud de referencia, formula_3 la latitud, formula_4 la latitud de referencia y formula_5 y formula_6 los paralelos estándar:

donde:




</doc>
<doc id="11613" url="https://es.wikipedia.org/wiki?curid=11613" title="Proyección cónica simple">
Proyección cónica simple

La proyección cónica simple se obtiene proyectando los elementos de la superficie esférica terrestre sobre una superficie cónica secante, tomando el vértice en el eje que une los dos polos. 

La "proyección cónica simple" puede tener uno o dos paralelos de referencia. 
La malla de meridianos y paralelos se dibuja proyectándolos sobre el cono suponiendo un foco de luz que se encuentra en el centro del globo. El cono sí es una figura geométrica que pueda desarrollarse en un plano. 

El resultado es un mapa semicircular en el que los meridianos son líneas rectas dispuestas radialmente y los paralelos arcos de círculos concéntricos. La escala aumenta a medida que nos alejamos del paralelo de contacto entre el cono y la esfera.

El cono secante corta el globo. A medida que nos alejamos de ellos la escala aumenta pero en la región comprendida entre los dos paralelos la escala disminuye. 
Esto es una representación de la tierra que muestra que la disposición de los paralelos es que puede tener uno o dos de diferencia 




</doc>
<doc id="11614" url="https://es.wikipedia.org/wiki?curid=11614" title="Línea internacional de cambio de fecha">
Línea internacional de cambio de fecha

La línea internacional de cambio de fecha es una línea imaginaria superficial terrestre trazada sobre el océano Pacífico y próxima con el meridiano 180°. Por conveniencia de algunos países cuyo territorio atraviesa, la hora legal o local y la fecha, en ellos, pueden ser la correspondiente al otro hemisferio.
Pasar de un lado al otro de la línea implica cambiar de fecha, exactamente un día. En 1612, un historiador francés de nombre Nicolás Bergier vio la necesidad de tener un meridiano donde cambiase la fecha. Como en aquel momento el meridiano de referencia para la navegación era el de las islas Canarias, propuso el que se encontraba a 180° de ese meridiano.

El empleo del meridiano 180° como la línea internacional del cambio de fecha fue ideada en 1879 por sir Sandford Fleming, quien la defendió en numerosos congresos, incluyendo el de 1884 en Washington, Estados Unidos, donde se decidió establecer como origen tanto para la longitud geográfica como para los husos horarios, al meridiano de Greenwich.

La elección del meridiano 180° como la línea internacional de cambio de fecha se basa en la característica conveniente de que atraviesa zonas oceánicas prácticamente despobladas.

La línea internacional de cambio de fecha corresponde al meridiano 180º del planeta en su mayor parte, excepto en el territorio cercano al estrecho de Bering donde se define que Siberia y Alaska tengan diferentes fechas. La mayor parte de esta línea se ubica sobre el océano Pacífico y define la fecha local en los territorios cercanos a ella.

Al atravesar la línea internacional de cambio de fecha de este a oeste (desde América a Asia por el océano Pacífico) la fecha debe adelantarse un día en todos los relojes, es decir, se pierde un día. En cambio, si un viajero cruza dicha línea de oeste a este la fecha deberá ser retrasada un día (ganando un día). Esto se debe a que la tierra gira de oeste a este y por cada huso horario cruzado en esa dirección (hacia el Oriente) se añade una hora hasta acumular 24 horas justo antes de atravesar dicha línea, resultando la misma hora pero del día siguiente.

En esta idea se basó Julio Verne para escribir su famosa novela "La vuelta al mundo en ochenta días", en la que el protagonista viaja al encuentro del Sol, de oeste a este, motivo por el cual debía retrasar un día de su propia cuenta de viaje (81 días) para coincidir con la fecha de quienes lo esperaban "al otro lado de la línea" y contaron sólo 80 días desde su partida.

Tal como se explica en el concepto de huso horario, cada meridiano múltiplo de 15° es el centro de un huso horario que abarca 7° y medio a cada lado. La aplicación de esta idea, quizá también errónea, conduce a confusiones originadas por dos paradojas:
Así, cuando es mediodía en el meridiano 0°, ya hace media hora que pasó el sol por el meridiano 7.5° de longitud este, lo cual equivale a indicar que el avance del movimiento solar aparente siempre tiene efecto de media hora retroactiva en cada huso horario, algo sin sentido: cada hora debe comenzar en los meridianos múltiplos de 15° hacia el oeste, de la misma forma que las horas del reloj comienzan en el número correspondiente y avanzan en sentido horario hasta el número siguiente: treinta grados en los relojes de dos agujas y doce horas en un día.




</doc>
<doc id="11615" url="https://es.wikipedia.org/wiki?curid=11615" title="Paralelo">
Paralelo

Se denomina paralelo al círculo formado por la intersección del geoide terrestre con un plano imaginario perpendicular al eje de rotación de la Tierra.

Sobre los paralelos, y a partir del ecuador o paralelo 0, se mide la latitud —el arco de circunferencia expresado en grados sexagesimales—, que podrá ser norte o sur, en función del sentido de medida de la misma. A diferencia de los meridianos, los paralelos no son circunferencias máximas, salvo el ecuador, no contienen el centro de la Tierra. 

El ángulo formado (con vértice en el centro de la Tierra) sobre cualquier plano meridiano por un paralelo y la línea ecuatorial se denomina latitud y es la misma para todos los puntos del paralelo, la cual se discrimina entre "latitud Norte" y "latitud Sur" según el hemisferio.

Tanto meridianos como paralelos forman el sistema de coordenadas geográficas basado en latitud y longitud.

Existen cinco paralelos notables o principales que se corresponden con una posición concreta de la Tierra en su órbita alrededor del Sol y que, por ello, reciben un nombre particular:

Estos ángulos son determinados por la oblicuidad de la eclíptica (aprox. 23° 27').

A partir de estos paralelos principales, la Tierra queda dividida en tres zonas conocidas como zonas geoastronómicas:


La zona intertropical es el espacio de la superficie de la Tierra comprendido entre los dos trópicos, a quien divide por medio el ecuador o la línea y distando cada uno 23º y 27', será toda su latitud de aprox. 47º que reducidos a leguas españolas son 822,5 y en leguas francesas 940; la longitud de esta zona es toda la redondez de la Tierra o 360º de ecuador igual a 6300 leguas españolas o bien 7200 francesas. La superficie y solidez de esta zona se hallará por los preceptos de la geometría.

Los antiguos llamaron a esta zona tórrida porque teniendo los habitantes de ella el Sol en su cenit y siéndoles sus rayos perpendiculares, juzgaron que sería en la mayor parte inhabitada por su excesivo calor, pero los modernos han encontrado en ella países frescos, templados y saludables en donde se goza casi de primavera y otoño perpetuos, porque siendo las noches de casi 12 horas y corriendo en el día vientos frescos que pasan sobre muchas leguas de mar, templan los rayos del Sol causando frecuentes lluvias y por esto en muchas partes de esta zona se hacen dos cosechas de fruto cada año y los árboles en todo tiempo tienen flor y fruto.

Las regiones situadas en la línea ecuatorial, por tener su cenit en este círculo, tienen la esfera recta y sus propiedades son las siguientes:


Se denomina "zonas templadas" a las zonas entre cada uno de los trópicos y su correspondiente círculo polar en el hemisferio. Estas zonas se caracterizan por:

También llamadas zonas subtropicales, estas presentan una serie de núcleos de alta presión, en ambos hemisferios, alineados siguiendo aproximadamente los 35° de latitud. Los ejes de cada cinturón experimentan un débil desplazamiento meridiano anual.

Las zonas polares están situadas al norte del círculo polar ártico y al sur del círculo polar antártico. En el caso de la zona polar sur, se suele tomar como punto de partida el paralelo 58, para incluir la totalidad del continente antártico. 



</doc>
<doc id="11617" url="https://es.wikipedia.org/wiki?curid=11617" title="Loxodrómica">
Loxodrómica

Se denomina loxodrómica o loxodromia (del griego "λοξóς" -oblicuo- y "δρóμος" -carrera, curso-) a la línea que une dos puntos cualesquiera de la superficie terrestre cortando a todos los meridianos con el mismo ángulo. La loxodrómica, por tanto, es fácil de seguir manteniendo el mismo rumbo marcado por la brújula. Su representación en el mapa dependerá del tipo de proyección del mismo; por ejemplo, en la de Mercator es una recta.

La loxodrómica es, junto a la ortodrómica y la isoazimutal, una de las tres líneas que pueden trazarse entre dos puntos cualesquiera de la superficie terrestre.

Pedro Nunes, un matemático portugués, publicó en el "Tratado de la navegación" (1546) un descubrimiento con grandes implicaciones para la navegación. Antes de él se creía que, marchando sobre la superficie terrestre con un rumbo fijo, es decir, formando un ángulo constante con la meridiana, la línea recorrida era un círculo máximo. Dicho con otras palabras, que un navío que siguiese este derrotero daría la vuelta al mundo y volvería al punto de partida. Nunes señaló la falsedad de este concepto al demostrar que la curva recorrida se va acercando al polo, alrededor del cual da infinitas vueltas, sin llegar nunca a él; o, dicho en lenguaje técnico, que tiene el polo por punto asintótico.




</doc>
<doc id="11618" url="https://es.wikipedia.org/wiki?curid=11618" title="Materia prima">
Materia prima

Se conoce como materia prima a la materia extraída de la naturaleza y que se transforma para elaborar materiales que más tarde se convertirán en bienes de consumo.



Las actividades relacionadas con la extracción de productos de origen animal, vegetal y mineral se les llama materias primas en crudo. En el sector primario se agrupan la agricultura, la ganadería, la explotación forestal, la pesca y la minería, así como todas las actividades dónde se aprovechan los recursos sin modificarlos, es decir, tal como se extraen de la naturaleza.

Las materias primas sirven para fabricar o producir un producto, siendo necesario, por lo general que sean refinadas para poder ser usadas en el proceso de elaboración de un producto. Por ejemplo, la magnetita, o la pirita serían una materia prima en crudo, y el hierro refinado y el acero serían materias primas refinadas, o elaboradas.

De los cinco grupos de materias primas en crudo, tres se consideran renovables, el grupo vegetal, el animal y el líquido y gaseoso, al "volver" al lugar de partida por sí solos, cerrando el ciclo, puede fabricar o producir un producto necesario.

Las materias primas minerales consideradas superabundantes, las abundancia de los elementos químicos en la superficie terrestre son: oxígeno, silicio (SiO2-60 %), aluminio, hierro, calcio, magnesio (MgO-3,1 %), sodio, potasio, y agua, dióxido de carbono, titanio,(TiO2-0,7) y (fósforo, P2O5-0,2 %) (de la capa superficial, principalmente ya en las plantas, pues es limitante para su crecimiento, junto con el agua, el sol y la temperatura). 

Distinguiendo entre "materia prima" para un proceso de fabricación (esta clasificación), y una materia prima en crudo que necesita ser previamente procesado/elaborado/refinado para poder ser usado en un proceso de fabricación. (Los fluidos, energía y vectores de esta quedan excluidos de esta clasificación), esta es exclusivamente para las materias primas de aplicación directa a la producción (refinadas o no), y que formarán parte del producto final (formarán parte, estarán incorporados al producto final, esto es, excluyendo los consumibles).

Materias primas estructurales listas para su uso o "materias primas estructurales industriales" (sin necesidad de ser refinadas, procesadas o válidas en crudo para ser trabajadas)










Son aquellas necesarias para el proceso de elaboración de un producto sin llegar a formar parte del producto, esto es, que luego quedan excluidas de la composición de este.


Algunas materias forman parte de la vida cotidiana tales como:

Petróleo: Se emplea para la fabricación de aceites, para la fabricación de plásticos, para la fabricación de combustible para aviones y automóviles comúnmente.

Madera: Se emplea para las Construcciones, también se utiliza para fabricación de artesanías e inmuebles.

Cuero: Se emplea para la fabricación de materiales como ropas, inmuebles y entre otras cosas.

Agua : El agua es una de las materias primas más usadas ya que se utiliza para uso domésticos. El agua se emplea para los cultivos y su riega también se utiliza para generar energía.



</doc>
<doc id="11619" url="https://es.wikipedia.org/wiki?curid=11619" title="Rumbo">
Rumbo

El rumbo es la dirección considerada en el plano del horizonte y, principalmente, cualquiera de las comprendidas en el meridiano. Precisamente la palabra procede del latín "rhombus" (‘rombo’), que son las formas geométricas que unidas señalan las diferentes direcciones posibles en la rosa de los vientos.

Rumbo es también la dirección en la que nos movemos o navegamos, o en la cual nos dirigimos o miramos y suele expresarse en forma del ángulo que forma esta dirección con otra tomada como referencia. Según que esta dirección de referencia sea el meridiano terrestre que pasa por la posición en la que nos encontramos o la dirección en que señala la brújula magnética hablaremos de "rumbo geográfico" o de "rumbo magnético".

En navegación se define el rumbo como el ángulo medido en el plano horizontal entre el norte y la dirección de avance del barco, medido en círculo, es decir, de 0º a 360º. El rumbo se expresa siempre con tres dígitos y, si es necesario, se añaden ceros a la izquierda. Así, al decir "rumbo 028º" se evitan errores de interpretación, evitando la confusión con rumbo 128º o 228º. Anteriormente el rumbo se expresaba "en cuadrantal", por referencia a un cuadrante de la rosa náutica: "rumbo S 30º E" significa 30 grados hacia el este contados desde el sur, lo que equivale a rumbo circular 150º.

En las cartas de navegación se representan los rumbos principales mediante la rosa náutica, compuesta por 32 rombos (deformados) unidos en el centro, cuyas puntas exteriores señalan el rumbo sobre el círculo del horizonte. Sobre el mismo, a partir del siglo XVII, se representa la flor de lis que señala el Norte. También se representa la intensidad media del viento en los diferentes sectores en los que se divide el círculo del horizonte.

En náutica se distinguen varios rumbos: 

Para convertir un rumbo a un acimut es necesario primero conocer la declinación magnética. De esta forma si la declinación magnética es al Este, entonces el acimut va a ser el rumbo más la declinación magnética (Az = Rm+Dm), en cambio, si la declinación magnética es al oeste entonces el acimut es igual al rumbo menos la declinación magnética (Az = Rm-Dm). Para facilitar las ecuaciones y que se utilice una sola, se usa la ecuación donde el acimut es el rumbo más la declinación magnética teniendo en cuenta la convención de signos en donde el Este es positivo y el Oeste es negativo. Ejemplo: necesito encontrar el azimut en un punto donde el rumbo es de 60° y la declinación magnética es de 5°Oeste (-5°). Utilizando la fórmula: Az = Rm+Dm = 60° + (-5°) = 55°


</doc>
<doc id="11620" url="https://es.wikipedia.org/wiki?curid=11620" title="Instalación industrial">
Instalación industrial

Se entiende por instalación industrial al conjunto de medios o recursos necesarios para llevar a cabo los procesos de fabricación y de servicio dentro de un sistema de organización industrial.

La instalación industrial comprende:

Por lo que respecta al conjunto de la instalación en sí, dos aspectos deben considerarse:

La localización de una instalación, representa un elemento fundamental que se ha de tomar en cuenta en el momento de planificar las futuras operaciones de cualquier empresa; debido a que representa el arreglo de los recursos y actividades dentro de cualquier organización; con la finalidad de evitar la acumulación de inventario de productos en proceso, las sobrecargas en los sistemas de manejo de materiales y las largas trayectorias que han de realizar para transportar los productos de un equipo a otro que influyen directamente en los costos totales de producción; y de esa forma contribuir con la eficiencia total de las operaciones de producción y de servicio.

Una buena localización de una instalación requiere de un estudio detallado de los factores que pueden afectar desde el punto de vista mundial, nacional, o departamental; debido a que la misma obedece al grado de desarrollo de las organizaciones, ya que mientras más grandes sean, más cuidadosos serán los estudios que se deben tomar en cuenta a la hora de ampliar sus operaciones. Partiendo de este criterio, los factores que intervienen en el estudio de ubicación de una instalación son las siguientes:
Cada país presenta sus propias normas y restricciones jurídicas. Una empresa transnacional que tenga intenciones de extenderse hacia una nación específica, tiene que respetar los reglamentos y edictos gubernamentales propios de legislación; si existe un proceso productivo que viole en su infraestructura estas condiciones, evidentemente que todo esfuerzo de instalación sería inútil. 



</doc>
<doc id="11621" url="https://es.wikipedia.org/wiki?curid=11621" title="Ortodrómica">
Ortodrómica

La ortodrómica (del griego "orthos" "recto" y "dromos" "carrera") es el camino más corto entre dos puntos de la superficie terrestre; es el arco del círculo máximo que los une, menor de 180 grados. Entre dos puntos de la superficie terrestre pueden trazarse tres líneas diferentes: ortodrómica, loxodrómica e isoazimutal.

Si los puntos estuvieran separados 180 grados, serían puntos opuestos, también conocidos como antípodas, y entre ellos se podrían trazar infinitos arcos de 180 grados de igual longitud. 

Una característica de la ortodrómica es que presenta un ángulo diferente con cada meridiano, ("excepto cuando dicha ortodrómica coincide con un meridiano o con el ecuador"). Esta característica representó un grave inconveniente para la navegación, solucionado hacia los últimos años del Siglo XX con el sistema GPS, porque antes del mismo, era difícil trazar una ruta de navegación que siguiera la ortodrómica ya que obligaría a continuos cambios de rumbo. Cuando las distancias eran grandes y seguir el camino más corto suponía un ahorro significativo, se realizaba una aproximación marcando una serie de puntos intermedios, en los cuales se cambiaba de rumbo, y de esta manera se lograba una aproximación a las correspondientes loxodrómicas.

La ortodrómica posee tres puntos relevantes que son:

En los últimos años del , las dificultades de realizar trayectos que siguieran la curva ortodrómica se vio enormemente facilitada, como consecuencia de la posibilidad de navegar sin utilizar brújulas. Fue la implementación de los sistemas de posicionamiento global tipo "GPS" lo que otorgó nuevas posibilidades de referencia extremadamente precisas. Si además se piensa en los avances de los sistemas de control de navegación por ordenador, totalmente interactivos con los GPS, uno se dará cuenta que a partir de esto, que el seguir una trayectoria ortodrómica dejó de ser un inconveniente.

Existe ("o puede existir") una diferencia entre los "caminos ideales" como podría ser una curva ortodrómica y los "caminos posibles". Los caminos posibles tienen que lidiar con factores de la realidad como pueden ser: mareas, corrientes, vientos y bloqueos directos como son las islas, los continentes, las montañas, y hasta los edificios en una zona urbana. De cualquier manera para caminos muy largos suele ser conveniente ("en tiempo y economía"), el aproximarse lo máximo posible a la curva ortodrómica.




</doc>
<doc id="11622" url="https://es.wikipedia.org/wiki?curid=11622" title="Normalización">
Normalización

La normalización (también denominada estandarización) es el proceso de elaborar, aplicar y mejorar las normas que se emplean en distintas actividades científicas, industriales o económicas, con el fin de ordenarlas y mejorarlas. Por su parte, la Sociedad Americana para Pruebas y Materiales (ASTM), define la estandarización como el proceso de formular y aplicar reglas, para una aproximación ordenada a una actividad específica, para el beneficio y con la cooperación de todos los involucrados.

Según la ISO ("International Organization for Standardization"), la normalización es la actividad que tiene por objeto establecer, ante problemas tales como reales o potenciales, disposiciones destinadas a usos comunes y repetidos, con el fin de obtener un nivel de ordenamiento óptimo en un contexto dado, que puede ser tecnológico, político o económico.

La normalización persigue fundamentalmente tres objetivos:


Las elevadas sumas de dinero que los países desarrollados invierten en los organismos normalizadores, tanto nacionales como internacionales, es un indicio o una prueba de la importancia que se da a esta cuestión.



Algunos ejemplos de organismos nacionales de normalización son:
<nowiki>* Miembro correspondiente de ISO




</doc>
<doc id="11623" url="https://es.wikipedia.org/wiki?curid=11623" title="Isoazimutal">
Isoazimutal

Tres son las curvas más importantes entre dos puntos cualesquiera de la superficie terrestre: la ortodrómica, la loxodrómica y la isoazimutal.

La línea o curva isoazimutal, IsoZ(X,Z), es el lugar geométrico de los puntos sobre la superficie terrestre cuyo rumbo inicial ortodrómico respecto a un punto fijo X es constante e igual a Z.
Por ejemplo, si el rumbo inicial ortodrómico desde S hasta X es de 80 grados, la línea isoazimutal asociada es la formada por todos los puntos cuyo rumbo ortodrómico inicial al punto X es de 80º.

Sea X un punto fijo de la Tierra de coordenadas latitud: B2, y longitud: L2. En un modelo esférico terrestre, la ecuación de la isoazimutal de rumbo inicial Z que pasa por el punto S(B, L) es:

formula_1

En este caso el punto X es el polo de iluminación del astro observado y el ángulo θ es su azimut. La ecuación de la curva isoazimutal, o arco capaz esférico, para un astro de coordenadas (dec, gha), declinación y ángulo horario en Greenwich, observado bajo un azimut Z, viene dada por
formula_2

donde lha es el ángulo horario local y los puntos de latitud B, y longitud L, pertenecen a la curva.




</doc>
<doc id="11626" url="https://es.wikipedia.org/wiki?curid=11626" title="Blitzkrieg">
Blitzkrieg

Blitzkrieg (; en alemán, literalmente ‘guerra relámpago’) es el nombre popular que recibe una táctica militar de ataque que implica un bombardeo inicial, seguido del uso de fuerzas móviles atacando con velocidad y sorpresa para impedir que un enemigo pueda llevar a cabo una defensa coherente. Los principios básicos de este tipo de operaciones se desarrollaron en el siglo por varias naciones, y se adaptaron años después de la Primera Guerra Mundial, principalmente por la Wehrmacht, para incorporar armas y vehículos modernos como un método para evitar la guerra de trincheras y la guerra en frentes fijos en futuros conflictos.

"Blitzkrieg" es una palabra alemana que literalmente se puede traducir como "guerra relámpago", significando "una guerra tan rápida como un relámpago". La palabra no entró en la terminología oficial de la Wehrmacht (el Ejército alemán de la época de 1935 a 1945) ni antes ni durante la guerra, aunque fue utilizada por la publicación militar "Deutsche Wehr" en 1935, en el contexto de un artículo que exponía cómo Estados con insuficiente comida y materias primas podían ganar una guerra. Blitzkrieg apareció de nuevo en 1938 en el "Militär-Wochenblatt", donde se definió como un "ataque estratégico" llevado a cabo por el empleo de blindados, fuerzas aéreas y fuerzas aerotransportadas. En el libro "Blitzkrieg Legende" de Karl-Heinz Frieser, que investigó el origen del término y encontró los ejemplos antes mencionados, señala que el uso de la palabra antes de la guerra era raro y que prácticamente nunca entró en la terminología oficial durante la guerra.

En el mundo anglosajón se hizo popular el término por un periodista de la revista estadounidense "Time", al describir la invasión de Polonia en 1939. Publicado el 25 de septiembre de 1939, cuando la campaña ya estaba desarrollada, el relato del periodista menciona:

Los historiadores han definido la Blitzkrieg como el empleo de conceptos de maniobras y guerra de fuerzas combinadas desarrollada en Alemania durante el periodo de entreguerras y la Segunda Guerra Mundial. Desde el punto de vista estratégico, la idea era conseguir un derrumbamiento rápido del adversario con una campaña corta librada por un ejército pequeño y profesional. Desde el punto de vista operacional, su meta se conseguía por medios indirectos, tales como la movilidad y la sorpresa, dejando los planes del adversario impracticables o irrelevantes. Para alcanzarlo se combinaron las fuerzas de formaciones de blindados, infantería motorizada, ingenieros, artillería y cazabombarderos.

El significado de Blitzkrieg se ha ampliado para usos más populares. A partir de su significado original, Blitzkrieg se ha empleado para hacer referencia a cualquier operación militar que enfatiza la sorpresa, la velocidad o la concentración. Durante la guerra, los bombardeos a la ciudad de Londres de la Luftwaffe se conocieron como Blitz. Existe también una modalidad de juego de ajedrez rápido o relámpago.

Los primeros ejemplos prácticos de este concepto, junto a la tecnología moderna, fueron los establecidos por la Wehrmacht alemana en las batallas iniciales de la Segunda Guerra Mundial. Mientras que las operaciones en Polonia fueron bastante convencionales, las siguientes batallas (particularmente las invasiones de Francia, los Países Bajos y las primeras operaciones en la Unión Soviética) fueron efectivas debido a las penetraciones por sorpresa, la falta de preparación general del enemigo y la incapacidad de reaccionar rápidamente a las ofensivas alemanas. La victoria del ejército alemán frente a un enemigo técnicamente superior y más numeroso en Francia llevó a muchos analistas a creer que se había inventado un nuevo sistema de guerra.

La definición generalmente aceptada de las operaciones en forma de "Blitzkrieg" incluye el uso de maniobras en lugar de desgaste para derrotar a un oponente, y traza operaciones utilizando la concentración de fuerzas combinadas de recursos móviles en un punto central, los blindados apoyados estrechamente por activos de infantería móvil, artillería y apoyo aéreo. Estas tácticas necesitaban el desarrollo de vehículos de apoyo especializados, nuevos métodos de comunicación, nuevas tácticas militares y una descentralización efectiva de la estructura de mandos.

En términos generales, la "Blitzkrieg" necesitaba la formación de la infantería mecanizada, la artillería autopropulsada y cuerpos de ingenieros que pudiesen mantener en buenas condiciones el equipo y la movilidad de los carros. Las fuerzas alemanas evitaban el combate directo con el fin de interrumpir las comunicaciones, la toma de decisiones, la logística y reducir el estado de ánimo del enemigo. En el combate, la Blitzkrieg dejaba poca elección a las fuerzas defensoras, lentas, más allá de romperse en bolsas aisladas, que eran rodeadas y posteriormente destruidas por la infantería alemana.

Por primera vez se utilizó la estrategia durante la guerra polaco-soviética (1919-1920). La Fuerzas Armadas polacas eran menores que las soviéticas. Para mover las tropas, se utilizó por primera vez la estrategia, que ganó la guerra y, en consecuencia, la independencia polaca se prolongó 19 años.

El inminente desarrollo de la Blitzkrieg comenzó con la derrota alemana en la Primera Guerra Mundial. Poco después del conflicto, la Reichswehr creó comités de oficiales veteranos para evaluar 57 cuestiones de la guerra. Los informes de estos comités dieron forma a publicaciones de doctrinas y entrenamientos que serían estándares en la Segunda Guerra Mundial. La Reichswehr estaba influida por su análisis del pensamiento militar alemán de la preguerra, en particular de sus tácticas de infiltración y la guerra de maniobras que dominó el frente oriental.

La historia militar alemana estaba muy influida por Carl von Clausewitz, Alfred von Schlieffen y Helmuth von Moltke, que eran partidarios de la maniobra, la masa y la maniobra envolvente. Sus conceptos fueron aplicados con éxito en la Guerra franco-prusiana y en el intento del Plan Schlieffen. Durante la guerra, estos conceptos fueron modificados por la Reichswehr. Su jefe de Estado Mayor, Hans von Seeckt, se alejó de la doctrina argumentando que se centraba demasiado en el envolvimiento basado en la velocidad. La velocidad daba sorpresa, y ésta permitía su explotación si las decisiones se tomaban rápidamente y la movilidad daba flexibilidad y velocidad. Von Seeckt abogó por efectuar rupturas contra el centro del enemigo cuando era más rentable que los envolvimientos, o donde los envolvimientos no eran prácticos.

Bajo el mando de Von Seeckt, la actualización moderna del sistema doctrinal recibió el nombre de "Bewegungskrieg", en alemán guerra de movimiento, y su sistema de tácticas denominado "Auftragstaktik", en alemán Misión-tipo táctica, fue desarrollado dando lugar al conocido efecto Blitzkrieg. Además, rechazó la noción de masa que habían defendido Von Schlieffen y Von Moltke.

Mientras que las reservas ocupaban cuatro décimas partes de las fuerzas alemanas en las campañas de la preguerra, Von Seeckt buscó la creación de una fuerza militar de voluntarios pequeña y profesional apoyada por una milicia defensiva. En la guerra moderna, sostenía que una fuerza pequeña era más capaz de la acción ofensiva, más rápida en estar preparada y menos cara de equipar con armas modernas. La Reichswehr estaba forzada a adoptar un pequeño ejército profesional debido a las condiciones del Tratado de Versalles que limitaba el ejército alemán a un máximo de cien mil soldados.

La "Bewegungskrieg" necesitaba una nueva jerarquía de mando que permitiese que las decisiones militares fueran tomadas lo más próximas al nivel de la unidad militar. Esto permitía a las unidades reaccionar y hacer efectivas las decisiones más rápidamente, que era una ventaja crítica y una de las razones principales para el éxito de la Blitzkrieg.

El liderazgo alemán también había sido criticado por no comprender los avances tecnológicos de la Primera Guerra Mundial, dejando la producción de carros de combate como una prioridad mínima y no realizando estudios de la ametralladora antes de la guerra. Como respuesta, los oficiales alemanes asistieron a escuelas técnicas durante el periodo de reconstrucción tras la guerra.

Las tácticas de infiltración, creadas por el ejército alemán durante la Primera Guerra Mundial, se convirtieron en la base de las tácticas posteriores. La infantería alemana había evolucionado a pequeños grupos descentralizados, que evitaban la resistencia y trataban de alcanzar los puntos débiles y atacar las comunicaciones de retaguardia. Se ayudaba de artillería coordinada y bombardeos aéreos, seguidos por fuerzas terrestres mayores con armas pesadas que destruían los puntos de resistencia. Estos conceptos formaron la base de las tácticas de la Wehrmacht durante la Segunda Guerra Mundial.

El frente oriental de la guerra no se estancó en una guerra de trincheras. Los ejércitos alemanes y rusos combatieron en una guerra de maniobras sobre miles de kilómetros, dando a los líderes alemanes la experiencia única que el frente occidental no tenía. Los estudios de las operaciones en el este llevaron a la conclusión de que pequeñas fuerzas coordinadas poseían más capacidad de combate que grandes fuerzas descoordinadas.

Durante este periodo, los combatientes principales de la guerra desarrollaron teorías propias sobre las fuerzas mecanizadas, siendo las de los Aliados occidentales sustancialmente distintas de las del "Reichswehr". Las doctrinas británicas, francesas y estadounidenses al principio de la Primera Guerra Mundial planteaban un papel de los carros blindados reducido a la función de meros apoyos a fuerzas de infantería y supeditados a las mismas, con escaso enfoque en grupos combinados y la concentración de fuerzas blindadas. Eso influyó de forma decisiva en el diseño de los modelos de carro aliados en servicio: lentos y pesados, con fuerte blindaje y un armamento pensado para el fuego de apoyo. Los alemanes tendrían, por el contrario, menor blindaje y potencia de fuego a cambio de una velocidad y maniobrabilidad mucho mayores, por lo menos en las fases iniciales de la guerra y hasta la aparición de los modelos de panzer más pesados.

Las primeras publicaciones del "Reichswehr" contenían muchos artículos traducidos procedentes de los países aliados, aunque cuanto más diferían las líneas doctrinales, menos interés recibían por parte del Estado Mayor alemán. Los avances técnicos de los países extranjeros fueron, sin embargo, vigilados y utilizados en parte por la Oficina de Armamento. En general, las doctrinas externas tuvieron poca influencia, con cuatro posibles excepciones: el francés Charles de Gaulle, el soviético Mijaíl Tujachevski y los británicos J.F.C. Fuller y Basil Liddell Hart.

De Gaulle, que por entonces era coronel en el ejército francés, era un conocido defensor de la concentración de blindados y aviones, opinión menospreciada por su alto mando, pero que algunos afirman que influyó a Heinz Guderian. En 1934 De Gaulle había escrito en su libro "L'armée de metier" unas teorías donde defendía el uso combinado de carros e infantería, en colaboración con la aviación. Los mandos superiores del ejército francés rechazaron tales ideas, pero muchos extractos del texto de De Gaulle fueron citados literalmente como teoría útil en los manuales militares alemanes de esa época.

Se asocia a Fuller y Liddell Hart con el desarrollo de la "Blitzkrieg" por el mismo Guderian en su libro de memorias. A propuesta de ambos, la Oficina de Guerra británica permitió una Fuerza Mecanizada Experimental, formada el 1 de mayo de 1927, que estaba completamente motorizada e incluía artillería autopropulsada e ingenieros motorizados. Sus artículos con las conclusiones extraídas tuvieron una amplia difusión en Alemania, e incluso fue el propio Guderian el encargado de traducirlos. Ambos autores eran ampliamente conocidos por el cuerpo de oficiales alemán anterior al rearme (Erwin Rommel, por ejemplo, tenía en su casa ejemplares originales y algunas de las traducciones de Guderian). Sin embargo, los Aliados (y especialmente Gran Bretaña) descartaron esos estudios iniciales y adoptaron completamente la doctrina del carro como apoyo de la infantería.

De lo que no hay duda, por tanto, es de que fueron Guderian y otros generales alemanes los primeros en diseñar y poner en práctica esta doctrina en una amplia y exitosa gama de escenarios durante la Segunda Guerra Mundial. Desde los cruces de ríos por las primeras fuerzas combinadas y la explotación de la penetración durante el avance en Francia en 1940 a los masivos avances envolventes en Rusia en 1942, el ejército alemán mostró una maestría e innovación que le permitió superar su inferioridad numérica y material. En gran parte se debe a la decidida labor de Guderian como impulsor incansable del arma acorazada; su liderazgo fue apoyado y fomentado por el Estado Mayor del "Reichswehr", promoviendo tanto el diseño del arma como la mejora en su uso a través de juegos de guerra durante los años 1930.

Por otra parte, el "Reichswehr" y el Ejército Rojo colaboraron en ejercicios militares y pruebas en Kazán y Lípetsk a comienzos de 1926. Durante este periodo, el Ejército Rojo estaba desarrollando la teoría de "Operaciones de Profundidad", que guiaría la doctrina del Ejército Rojo durante la Segunda Guerra Mundial. Situados dentro de la Unión Soviética, estos dos centros fueron usados para pruebas de aviación y vehículos blindados hasta un nivel de batallón, así como para alojar escuelas de blindados y aéreas. Estas pruebas iniciales se realizaron en secreto en territorio de la Unión Soviética como parte de un programa de intercambio mediante el cual los alemanes pretendían evitar las imposiciones del Tratado de Versalles en materia de investigación bélica. Pese a ello, la Gran Purga lanzada por Stalin en 1935 significó que muchos jefes militares soviéticos defensores de la "guerra en profundidad" fueran arrestados y luego fusilados, con la consecuente prohibición gubernamental de seguir estudiando conceptos bélicos cuyos autores habían perdido el favor del régimen. Irónicamente, serían precisamente los soviéticos los que más sufrirían la maestría técnica conseguida por las fuerzas alemanas gracias a esta colaboración inicial encubierta.

Siguiendo las reformas militares de Alemania en los años 1920, Heinz Guderian apareció como un decidido partidario de las fuerzas mecanizadas. Dentro de la Inspección de Transporte de Tropas, Guderian y sus colegas realizan trabajos teóricos y de ejercicio en el campo. Había una oposición por muchos oficiales que daban primicia a la infantería o simplemente dudaban de la utilidad del blindado. Entre ellos estaba el jefe del Estado Mayor Ludwig Beck (1935-1938), que desconfiaba de que las fuerzas blindadas pudieses ser decisivas. No obstante, durante su mandato se crearon las divisiones panzer.

Guderian defendió que el carro era el arma decisiva de la guerra. Afirmó en uno de sus escritos que "si los carros tienen éxito, entonces se consigue la victoria". En un artículo dirigido a los críticos de la guerra blindada, Guderian escribió "hasta que nuestros críticos puedan aportar un nuevo y mejor método para realizar un ataque terrestre con éxito distinta de una matanza indiscriminada, continuaremos manteniendo nuestras creencias en que los blindados —empleados apropiadamente, no hace falta decirlo— son ahora el mejor medio disponible para un ataque por tierra".

Tratando sobre el mayor ritmo en el que los defensores podrían reforzar una zona en que los atacantes hubieran penetrado durante la Primera Guerra Mundial, Guderian escribió que "ya que las fuerzas de reserva estarán ahora motorizadas, la creación de nuevos frentes defensivos es más fácil de lo que solía ser; las posibilidades de una ofensiva basadas en la cooperación de la artillería e infantería son, consecuentemente, más sencillas de lo que fueron en la última guerra". Continuó con que "creemos que atacando con blindados podemos alcanzar un índice de movimiento mayor que el posible hasta ahora, y —lo que es quizás incluso más importante— podemos mantenerlo una vez que se abra una brecha en el frente". Además, Guderian pidió que la radio fuese utilizada de forma generalizada para facilitar la coordinación y mando.

La "Blitzkrieg" no sería posible sin la modificación del ejército permanente de Alemania, que estaba limitado por el Tratado de Versalles a 100 000 hombres, su fuerza aérea disuelta y el desarrollo del tanque prohibido. Tras convertirse en jefe de estado, Adolf Hitler ignoró estas obligaciones.

Se creó un mando de tropas blindadas dentro del "Heer" (Ejército) alemán, las "Panzertruppen". La Luftwaffe, o Fuerza Aérea, fue restablecida, y comenzó el desarrollo de cazabombarderos y doctrinas. Hitler era un fuerte partidario de esta nueva estrategia. Leyó el libro de Guderian "Achtung! Panzer!" y observó los ejercicios de campo de los blindados en Kummersdorf, donde comentó "Esto es lo que quiero: y esto es lo que tendré".

Los voluntarios alemanes utilizaron por primera vez los blindados en campos de batalla reales durante la Guerra Civil Española de 1936. Los cuerpos blindados consistían en el Batallón 88, una fuerza creada con tres compañías de carros Panzer I que funcionaron como cuadro de entrenamiento para el ejército nacional. La Luftwaffe desplegó escuadrones de cazas, bombarderos en picado y transportes bajo el nombre de la Legión Cóndor.

Guderian dijo que el despliegue de carros fue "en una escala demasiado pequeña para permitir realizar valoraciones exactas". La verdadera prueba para su "idea blindada" debería esperar hasta la Segunda Guerra Mundial. Sin embargo, la Fuerza Aérea alemana también proporcionó voluntarios para probar tácticas y aviones en combate, incluyendo el primer uso del Stuka.

La "Blitzkrieg" siempre perseguía acciones decisivas. Con este fin, se desarrolló la teoría del "Schwerpunkt" o punto focal: se trataba del punto de máximo esfuerzo. Las fuerzas "panzer" y la Luftwaffe eran utilizadas únicamente en este punto de máximo esfuerzo siempre que fuera posible. Mediante el éxito local en el "Schwerpunkt", una pequeña fuerza lograba una rotura de la línea y conseguía ventajas al luchar en la retarguardia del enemigo. Fue resumido por Guderian como «Nicht kleckern, klotzen!» (¡Sin hacer cosquillas, golpeando!).

Para conseguir una rotura del frente, la infantería, y con menor frecuencia, las propias fuerzas blindadas, atacarían la línea defensiva del enemigo, apoyada por fuego de artillería y bombardeos para crear una brecha en la línea enemiga por la que pasaría la totalidad de las fuerzas mecanizadas. La fuerza atacante abre los flancos para aumentar la seguridad con la distancia. Este momento de la rotura ha sido etiquetado de "bisagra", porque las fuerzas mecanizadas maniobraban hacia el interior y creaban un efecto de palanca contra las fuerzas defensoras.

En esto, la fase inicial de la operación, las fuerzas aéreas intentaban ganar la superioridad aérea sobre las fuerzas enemigas atacando los aviones situados en tierra, bombardeando sus aeródromos e intentando destruirlos en combates aéreos.

Un último elemento era el uso de fuerzas aerotransportadas más allá de las líneas enemigas para interrumpir las actividades enemigas y tomar posiciones importantes, como ocurrió en Eben Emael.

Abriendo una brecha hacia las zonas de retaguardia adversarias, las fuerzas alemanas intentaban paralizar el proceso de toma de decisiones y de puesta en práctica del enemigo. Moviéndose más rápidamente que sus oponentes, los elementos mecanizados explotaban esta debilidad y actuaban anticipando cualquier respuesta contraria. Guderían escribió que «el éxito debe ser explotado sin respiro y con cada pizca de fuerza disponible, incluso de noche. El enemigo derrotado no debe estar tranquilo».

Un punto principal para esto era el ciclo de decisiones. Cada decisión tomada por los alemanes o las fuerzas enemigas necesitaba de tiempo para recopilar información, tomar una decisión, repartir las órdenes entre los subordinados, y luego poner en práctica la decisión a través de la acción. Gracias a la movilidad superior y los ciclos más rápidos de toma de decisiones, las fuerzas mecanizadas podían realizar acciones en una situación antes que sus oponentes.

El control directo ("Auftragstaktik") fue un método de mando rápido y flexible. En lugar de recibir una orden explícita, un comandante sería informado de la intención de su superior y el papel que tendría su unidad dentro de ese concepto. El método exacto de ejecución sería entonces un asunto que el comandante determinaría como mejor se ajustase a la situación. La carga del personal se reducía a repartir y extender junto con las órdenes más información sobre su propia situación. Además, fomentar la iniciativa a todos los niveles ayudaba a su puesta en práctica. Consecuentemente, las decisiones importantes podían ser ejecutadas rápidamente bien de forma verbal, bien con órdenes escritas de poca longitud.

La fase final de una operación se denominada "Kesselschlacht" o "batalla de la caldera". Consistía en un ataque concéntrico a una fuerza cercada. Era donde se infligía la mayor parte de las pérdidas al enemigo, sobre todo con la captura de prisioneros y armamento.

A pesar de que el término "Blitzkrieg" fue acuñado durante la invasión de Polonia de 1939, los historiadores mantienen generalmente que las operaciones alemanas fueron más coherentes con métodos más tradicionales. La estrategia de la Wehrmacht estaba más en línea con el "Vernichtungsgedanke", centrarse en envolvimientos para crear bolsas. Las fuerzas "Panzer" fueron desplegadas repartidas entre las tres concentraciones alemanas sin un fuerte énfasis en su uso independiente, siendo usadas para crear o destruir bolsas de fuerzas polacas y capturar puntos estratégicos para apoyar a la infantería a pie que le seguía.

La Luftwaffe ganó la superioridad aérea con una combinación de tecnología superior y cantidad. Se afirma erróneamente que la Fuerza Aérea polaca fue destruida al inicio de la campaña mientras estaba en tierra. Los aviones polacos fueron trasladados a aeródromos ocultos aproximadamente 48 horas después del comienzo de las hostilidades.

La comprensión de las operaciones en Polonia han cambiado considerablemente desde la Segunda Guerra Mundial. Muchas de las primeras crónicas de la posguerra atribuían incorrectamente la victoria alemana a «un desarrollo enorme en la técnica militar que ocurrió entre 1918 y 1940», citando incorrectamente que «Alemania, que tradujo teorías a la práctica... llamando al resultado "Blitzkrieg"». Historias más recientes identifican las operaciones alemanas en Polonia como relativamente cautelosas y tradicionales. Matthew Cooper escribió:

Cooper llegó a decir que el uso de los tanques «dejó mucho que desear... El miedo de la acción enemiga contra los flancos del avance, el miedo que fue comprobado tan desastroso a las posibilidades alemanas en el frente occidental en 1940 y en la Unión Soviética en 1941, estaba presente desde el principio de la guerra». John Ellis afirmó que «hay una considerable justicia en la afirmación de Matthew Cooper que las divisiones "Panzer" no tuvieron el tipo de misión estratégica que era característico en la auténtica "Blitzkrieg" de blindados, y que estaban subordinadas casi siempre a varios ejércitos de infantería».

De hecho, «mientras que los informes occidentales de la campaña polaca hacían hincapié en el poder de choque de los tanques y los ataques de los Stuka, tendían a subestimar el efecto castigador de la artillería alemana en las unidades polacas. Móvil y disponible en cantidades significativas, la artillería destruyó tantas unidades como las otras ramas de la Wehrmacht».

La invasión de Francia constó de dos fases: el Plan Amarillo ("Fall Gelb") y el Plan Rojo ("Fall Rot"). Fall Gelb comenzó con una finta dirigida contra los Países Bajos y Bélgica con dos cuerpos blindados y paracaidistas. Tres días más tarde el "Panzergruppe von Kleist" atacó a través de las Ardenas y consiguió una rotura del frente con el apoyo aéreo. El grupo se movió rápidamente por la costa del canal de la Mancha, copando a la Fuerza Expedicionaria Británica ("British Expeditionary Force", BEF), el Ejército belga y algunas divisiones del Ejército francés.

Las unidades motorizadas avanzaron inicialmente mucho más lejos que las divisiones que les seguían. Cuando las fuerzas mecanizadas alemanas se encontraron con el contraataque en la Batalla de Arras, los tanques pesados británicos crearon un breve pánico en el Alto Mando alemán. Más tarde, las fuerzas motorizadas fueron detenidas a las puertas de la ciudad portuaria de Dunkerque, que estaba siendo utilizada para evacuar las fuerzas aliadas. Hermann Göring había prometido que su Luftwaffe terminaría el trabajo pero las operaciones aéreas no detuvieron la evacuación de la mayoría de las tropas aliadas, unos 300 000 franceses y británicos, en una operación llamada Dynamo.

El Plan Rojo comenzó con el XV Cuerpo Panzer atacando hacia Brest y el XIV Cuerpo Panzer atacando el sureste de París, hacia Lyon y el XIX Cuerpo Panzer completando el envolvimiento de la Línea Maginot. Las fuerzas defensoras estaban demasiado presionadas como para organizar cualquier tipo de contraataque. Se ordenó continuamente a las fuerzas francesas formar nuevas líneas de defensa junto a los ríos, encontrándose a menudo que las fuerzas alemanas ya habían pasado.

La utilización de fuerzas blindadas fue crucial para ambas partes del frente oriental. La Operación Barbarroja, la invasión alemana de la Unión Soviética en 1941, implicó una cantidad de roturas de frentes y envolvimientos por parte de fuerzas motorizadas. Su objetivo era "destruir las fuerzas rusas desplegadas en el Oeste y evitar su huida hacia los espacios abiertos de Rusia". Esto se consiguió con cuatro ejércitos "Panzer" que cercaron a las sorprendidas y desorganizadas fuerzas soviéticas, seguidos por la infantería a pie que completaba los envolvimientos y derrotaba las fuerzas atrapadas. El primer año de la ofensiva en el frente oriental puede ser considerada como la última "Blitzkrieg" importante con éxito.

Tras no haber conseguido destruir a los soviéticos antes del invierno de 1941, los límites de la superioridad táctica alemana llegaron a ser evidentes. Aunque la invasión alemana conquistó con éxito extensas zonas del territorio soviético, los efectos estratégicos generales fueron más limitados. El Ejército Rojo pudo reagruparse más allá de la línea principal de batalla, y finalmente derrotar a las fuerzas alemanas por primera vez en la Batalla de Moscú. A ello se unió que las tácticas alemanas se dificultaban por el clima y debido a que el frente de combate se alejaba cada vez más de los centros industriales de Alemania y tal rasgo no había sido adecuadamente previsto.

En el verano de 1942, cuando Alemania lanzó otra ofensiva contra el sur de la Unión Soviética sobre Stalingrado y el Cáucaso, los soviéticos perdieron una cantidad importante de territorio, sólo contraatacando una vez más durante el invierno. Los triunfos alemanes fueron limitados por el desvío por parte de Hitler de fuerzas para el ataque de Stalingrado e intentar alcanzar los campos petrolíferos del Cáucaso simultáneamente en lugar de seguidamente como se había considerado en el plan original. El frente estaba más sobreextendido que nunca y ello dificultaba el abastecimiento. El Ejército Rojo, por su parte, poseía una vastísima retaguardia que le permitía planificar maniobras y movimientos que no pudieron intentar franceses o polacos contra la Wehrmacht.

Con el transcurso de la guerra, los ejércitos aliados empezaron a utilizar formaciones de fuerzas combinadas y estrategias de penetración en profundidad que Alemania había intentado usar en los primeros años de la guerra. Muchas operaciones aliadas en el Desierto Occidental y en el frente oriental confiaron en las concentraciones masivas de potencia de fuego para obtener roturas del frente por unidades blindadas móviles. Estas tácticas basadas en la artillería fueron también decisivas en las operaciones del frente occidental tras la Operación Overlord y tanto los ejércitos de la Commonwealth como de Estados Unidos desarrollaron sistemas flexibles y fuertes utilizando apoyo de artillería.

Tras los desembarcos aliados de Normandía, Alemania hizo intentos de aplastar la fuerza del desembarque con ataques de blindados pero no logró su objetivo por la falta de coordinación y la superioridad aérea aliada. La tentativa más significativa del uso de operaciones en profundidad en Normandía fue en Mortain, que acabó con la creación de la Bolsa de Falaise y la destrucción final de las fuerzas alemanas de Normandía. El contraataque de Mortain fue lanzado contra las fuerzas aliadas que actuaron en la Operación Cobra, el XII Grupo de Ejércitos de Estados Unidos. El 7.º Ejército alemán atacó hacia las costas de Saint-Lô, intentando cortar al 3. Ejército de Estados Unidos, mandado por George S. Patton en la Operación Lüttich. No pudo alcanzar la rotura de la línea contra la infantería defensora y, atascado, fue cercado y destruido por el XII Grupo de Ejércitos.

La ofensiva aliada en el centro de Francia, encabezada por las unidades blindadas del III Ejército de Patton, utilizó técnicas de rotura y penetración que eran esencialmente idénticas a la "idea de blindados" de la preguerra de Guderian. Patton reconoció que había leído a Guderian y a Rommel antes de la guerra, y sus tácticas compartían sus ideas de velocidad y ataque.

La última ofensiva alemana en el frente occidental, la batalla de las Ardenas, denominada Operación "Wacht Am Rhein" por los alemanes, fue una ofensiva lanzada hacia el puerto vital de Amberes en diciembre de 1944. Lanzada con mal tiempo atmosférico contra un sector débil aliado, fue una sorpresa y un éxito inicial mientras las fuerzas aéreas aliadas estuvieron bloqueadas por la nubosidad. Sin embargo, las bolsas defensivas en lugares clave a través de las Ardenas, la escasez de carreteras útiles y un mal plan logístico provocaron retrasos a los alemanes. Las fuerzas aliadas desplegadas en los flancos de la penetración alemana y la aviación aliada pudieron atacar de nuevo a las columnas blindadas. Mientras que la estrategia había sido sólida, la capacidad de las tropas alemanas se había reducido hasta el punto de no poder explotar los beneficios iniciales.

Los conceptos asociados con la denominación "Blitzkrieg", penetraciones en profundidad por blindados, grandes envolvimientos y ataques de fuerzas combinadas, tenían una dependencia importante del terreno y las condiciones meteorológicas. Donde no había capacidad para el movimiento rápido, la penetraciones de blindados fueron evitadas a menudo o resultaron un fracaso.

El terreno debía ser idealmente plano, firme, sin obstáculos naturales o fortificaciones e intercalado de carreteras y vías de ferrocarril. Si en su lugar era accidentado, arbolado, con pantanos o zonas urbanas, los blindados serían vulnerables a la infantería en combate próximo y sin posibilidad de salir a toda velocidad. Además, las unidades podían detenerse por el fango o la nieve. La artillería y el apoyo aéreo también dependían del tiempo atmosférico.

La superioridad aérea aliada se convirtió en un impedimento significativo en las operaciones alemanas durante los últimos años de la guerra. Los primeros éxitos alemanes disfrutaron de superioridad aérea, apoyo aéreo cercano y reconocimiento aéreo. Sin embargo, los cazabombaderos aliados fueron temidos por sus éxitos tácticos, de manera que tras la Operación Overlord, las tripulaciones de los vehículos alemanes mostraban reticencia a moverse en masa a la luz del día.

De hecho, la última operación "Blitzkrieg" alemana, la Batalla de las Ardenas, fue planeada para que tuviese lugar con mal tiempo y la aviación aliada en tierra. Bajo esas condiciones, fue difícil para los comandantes alemanes emplear la "idea de blindados" a su potencial previsto.

La "Blitzkrieg" era muy efectiva contra las doctrinas de defensa estática que la mayoría de los países desarrollaron al final de la Primera Guerra Mundial. Los primeros intentos de derrotar a la "Blitzkrieg" pueden ser fechados durante la invasión de Polonia en 1939, donde el general polaco Stanisław Maczek, comandante de la 10.ª Brigada de Caballería Motorizada, preparó un informe detallado de las tácticas alemanas, su uso, efectividad y posibles precauciones para el Ejército francés. Sin embargo, el personal francés hizo caso omiso de este informe, que fue capturado por los alemanes, sin abrir.

Durante la Batalla de Francia en 1940, la 4.ª División Blindada de De Gaulle y elementos de la Brigada Blindada de la Fuerza Expedicionaria Británica realizaron ataques contra el flanco alemán, llegando a empujar hacía atrás a las columnas blindadas avanzadas durante la Batalla de Arras. Ésta pudo haber sido la razón para que Hitler ordenase la detención del avance alemán.

Esos ataques, combinados con la "defensa de erizo" de Maxime Weygand, se convirtieron en la base principal para responder a la "Blitzkrieg" en el futuro: despliegue en profundidad, permitir a las fuerzas enemigas circunvalar las concentraciones defensivas, dependencia de la artillería anticarro, empleo de la mayor fuerza en los flancos del ataque enemigo, seguido de contraataques en la base para destruir el avance enemigo. Mantener los flancos era esencial para encauzar el ataque enemigo, y la artillería, empleada apropiadamente, causaría un número mayor de bajas a los atacantes.

Mientras que las fuerzas aliadas en 1940 carecían de la experiencia para desarrollar con éxito esas estrategias, teniendo como resultado la capitulación de Francia con muchas pérdidas, fueron características en las operaciones aliadas posteriores. En la Batalla de Kursk, el Ejército Rojo empleó una combinación de defensa en gran profundidad, campos de minas extensos y una defensa tenaz en los flancos de la rotura de la línea. De esta forma, redujeron la capacidad de combate de los alemanes incluso mientras las fuerzas alemanas avanzaban.

Aunque efectiva en las campañas rápidas contra Polonia y Francia, Alemania no podía mantener la "Blitzkrieg" en los últimos años de la guerra. La "Blitzkrieg" tiene el peligro inherente de extender demasiado sus líneas de abastecimiento, y la estrategia podía ser derrotada por un enemigo determinado, que esté dispuesto a sacrificar territorio durante el tiempo necesario para reagruparse y rearmarse, como hicieron los soviéticos en el frente oriental, la conocida estrategia de ceder terreno a cambio de ganar tiempo.

La producción de tanques y vehículos era un problema constante para Alemania. De hecho, a final de la guerra, muchas divisiones "panzer" no tenían más que algunas docenas de carros. Conforme se acercaba el final de la guerra, Alemania también tuvo una escasez crítica de combustible y munición debido a los bombardeos estratégicos aliados . Aunque la producción de aviones de combate continuaba, no podían volar debido a la falta de combustible. El combustible era enviado a las divisiones "Panzer", que incluso así no podían operar de forma normal. De los Tiger I que se perdieron contra el Ejército de los Estados Unidos, casi la mitad de ellos fueron abandonados por falta de combustible.

La influencia más amplia de la Blitzkrieg estuvo dentro de la dirección aliada occidental de la guerra, algunos de los que tomaron inspiración de la propuesta alemana. El general estadounidense Patton resaltaba la persecución rápida, el uso de una punta de lanza de blindados para realizar una rotura del frente, y aislar y desbaratar las fuerzas enemigas antes de que se dieran a la fuga. También puso en práctica la idea atribuida al líder de caballería Nathan Bedford Forrest de "llegar allí más rápido, con la mayoría de las fuerzas".

La Blitzkrieg también ha influido sobre otros militares y doctrinas. El Ejército de Defensa de Israel puede haber sido influido por la Blitzkrieg al crear puntas de lanza flexibles y apoyo cercano aéreo. En los años 1990, los teóricos estadounidenses del Shock y pavor afirmaron que la Blitzkrieg era un subconjunto de estrategias que denominaron "dominio rápido".




</doc>
